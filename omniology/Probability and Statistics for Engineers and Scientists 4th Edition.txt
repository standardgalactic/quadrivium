
       This is an electronic version of the print textbook. Due to electronic rights restrictions,
some third party content may be suppressed. Editorial review has deemed that any suppressed 
content does not materially affect the overall learning experience. The publisher reserves the right 
to remove content from this title at any time if subsequent rights restrictions require it. For
valuable information on pricing, previous editions, changes to current editions, and alternate 
formats, please visit www.cengage.com/highered to search by ISBN#, author, title, or keyword for 
materials in your areas of interest.

■
Probability and Statistics
for Engineers and
Scientists

This page intentionally left blank 

■
Probability and Statistics
for Engineers and
Scientists
FOURTH EDITION
Anthony Hayter
University of Denver
Australia • Brazil • Japan • Korea • Mexico • Singapore • Spain • United Kingdom • United States

Probability and Statistics for Engineers
and Scientists, Fourth Edition
Anthony Hayter
Vice-President, Editorial Director: PJ Boardman
Publisher: Richard Stratton
Senior Sponsoring Editor: Molly Taylor
Assistant Editor: Shaylin Walsh
Editorial Assistant: Alexander Gontar
Associate Media Editor: Andrew Coppola
Senior Marketing Manager: Barb Bartoszek
Marketing Coordinator: Michael Ledesma
Marketing Communications Manager:
Mary Anne Payumo
Content Project Manager: Susan Miscio
and Jill A. Quinn
Senior Art Director: Linda Helcher
Print Buyer: Diane Gibbons
Permissions Editor: Shalice Shah-Caldwell
Production Service and Compositor:
MPS Limited, a Macmillan Company
Cover Designer: Rokusek Design
Cover Image: elwynn/©shutterstock
© 2012 Brooks/Cole, Cengage Learning
ALL RIGHTS RESERVED. No part of this work covered by the copyright
herein may be reproduced, transmitted, stored, or used in any form or
by any means graphic, electronic, or mechanical, including but not
limited to photocopying, recording, scanning, digitizing, taping, Web
distribution, information networks, or information storage and
retrieval systems, except as permitted under Section 107 or 108 of
the 1976 United States Copyright Act, without the prior written
permission of the publisher.
For product information and technology assistance, contact us at
Cengage Learning Customer & Sales Support, 1-800-354-9706.
For permission to use material from this text or product,
submit all requests online at www.cengage.com/permissions.
Further permissions questions can be e-mailed to
permissionrequest@cengage.com.
Library of Congress Control Number: 2011933683
ISBN-13: 978-1-111-82704-5
ISBN-10: 1-111-82704-4
Brooks/Cole
20 Channel Center Street
Boston, MA 02210
USA
Cengage Learning is a leading provider of customized learning solutions
with ofﬁce locations around the globe, including Singapore, the United
Kingdom, Australia, Mexico, Brazil and Japan. Locate your local ofﬁce at:
international.cengage.com/region.
Cengage Learning products are represented in Canada by
Nelson Education, Ltd.
For your course and learning solutions, visit www.cengage.com.
Purchase any of our products at your local college store or at our
preferred online store www.cengagebrain.com
Printed in the United States of America
1 2 3 4 5 6 7 15 14 13 12 11

ABOUT THE AUTHOR
Dr. Anthony Hayter obtained a triple ﬁrst-class degree in mathematics from Cambridge
University in England and a Ph.D. in statistics from Cornell University. His doctoral thesis
included the proof of a famous mathematical conjecture that had remained unsolved for
thirty years, and he is the author of numerous research publications in the ﬁelds of
experimental design and applied data analysis.
Dr. Hayter is passionate about empowering students and researchers with the skills and
knowledge that they need to perform effective and accurate data analysis. With his
experience as a teacher, researcher, and consultant in a wide variety of settings, Dr. Hayter
knows how essential these skills are in today’s workplace. Dr. Hayter’s work has shown him
the substantial advantages that accrue from a clear understanding of the concepts of
probability and statistics, together with the pitfalls that can arise from their misuse.
Dr. Hayter collaborates on many research projects, including work to improve
wheelchair designs and to provide better assistive technologies to disabled people. He has
served as a site review team member at the National Institutes of Health. In addition, he has
worked on projects to improve the safety of bridges, to monitor air pollution levels, and to
promote equitable taxation rates, along with various other projects that are used as examples
and data sets in this textbook. Dr. Hayter has been a keynote speaker at international research
conferences, and has been a panelist at symposia on business information and analytics.
He has been an invited researcher at universities in England, Japan, Hong Kong and
Singapore, and he has received a Fulbright Scholarship to assist the government and
businesses in Thailand with data collection and analysis.
In his spare time, Dr. Hayter likes to read the detective stories of Timothy Hemion. In
fact, Dr. Hayter tells his students that conducting a good data analysis is like being part
of a detective story. A well-designed experiment provides pertinent evidence, and the
statistician’s job is to know how to extract the relevant clues from the data set. These clues
can then be used to piece together a picture of the true state of affairs, and they may be
used to disprove or substantiate the theories and hypotheses that have been put forward.

This page intentionally left blank 

CONTENTS
Preface
x
Continuing Case Studies: Microelectronic Solder Joints
and Internet Marketing
xv
C H A P T E R 1
P R O B A B I L I T Y T H E O R Y
1
1.1
Probabilities
1
1.2
Events
8
1.3
Combinations of Events
15
1.4
Conditional Probability
33
1.5
Probabilities of Event Intersections
40
1.6
Posterior Probabilities
50
1.7
Counting Techniques
56
1.8
Case Study: Microelectronic Solder Joints
64
1.9
Case Study: Internet Marketing
66
1.10 Supplementary Problems
66
C H A P T E R 2
R A N D O M V A R I A B L E S
71
2.1
Discrete Random Variables
71
2.2
Continuous Random Variables
81
2.3
The Expectation of a Random Variable
93
2.4
The Variance of a Random Variable
102
2.5
Jointly Distributed Random Variables
114
2.6
Combinations and Functions
of Random Variables
129
2.7
Case Study: Microelectronic Solder Joints
142
2.8
Case Study: Internet Marketing
143
2.9
Supplementary Problems
143
C H A P T E R 3
D I S C R E T E P R O B A B I L I T Y
D I S T R I B U T I O N S
147
3.1
The Binomial Distribution
147
3.2
The Geometric and Negative
Binomial Distributions
160
3.3
The Hypergeometric Distribution
168
3.4
The Poisson Distribution
173
3.5
The Multinomial Distribution
179
3.6
Case Study: Microelectronic
Solder Joints
183
3.7
Case Study: Internet Marketing
184
3.8
Supplementary Problems
184
C H A P T E R 4
C O N T I N U O U S P R O B A B I L I T Y
D I S T R I B U T I O N S
186
4.1
The Uniform Distribution
186
4.2
The Exponential Distribution
190
4.3
The Gamma Distribution
199
4.4
The Weibull Distribution
204
4.5
The Beta Distribution
209
4.6
Case Study: Microelectronic
Solder Joints
213
4.7
Case Study: Internet Marketing
213
4.8
Supplementary Problems
214
C H A P T E R 5
T H E N O R M A L D I S T R I B U T I O N
216
5.1
Probability Calculations Using the
Normal Distribution
216
5.2
Linear Combinations of Normal
Random Variables
229
5.3
Approximating Distributions with the
Normal Distribution
240
5.4
Distributions Related to the
Normal Distribution
251
5.5
Case Study: Microelectronic
Solder Joints
262
5.6
Case Study: Internet Marketing
263
5.7
Supplementary Problems
263
vii

viii
CONTENTS
C H A P T E R 6
D E S C R I P T I V E S T A T I S T I C S
267
6.1
Experimentation
267
6.2
Data Presentation
272
6.3
Sample Statistics
280
6.4
Examples
288
6.5
Case Study: Microelectronic Solder Joints
292
6.6
Case Study: Internet Marketing
293
6.7
Supplementary Problems
293
C H A P T E R 7
S T A T I S T I C A L E S T I M A T I O N A N D
S A M P L I N G D I S T R I B U T I O N S
296
7.1
Point Estimates
296
7.2
Properties of Point Estimates
301
7.3
Sampling Distributions
311
7.4
Constructing Parameter Estimates
320
7.5
Case Study: Microelectronic Solder Joints
327
7.6
Case Study: Internet Marketing
327
7.7
Supplementary Problems
327
Guide to Statistical Inference Methodologies
C H A P T E R 8
I N F E R E N C E S O N A
P O P U L A T I O N M E A N
333
8.1
Conﬁdence Intervals
333
8.2
Hypothesis Testing
349
8.3
Summary
381
8.4
Case Study: Microelectronic Solder Joints
383
8.5
Case Study: Internet Marketing
384
8.6
Supplementary Problems
384
C H A P T E R 9
C O M P A R I N G T W O
P O P U L A T I O N M E A N S
389
9.1
Introduction
389
9.2
Analysis of Paired Samples
397
9.3
Analysis of Independent Samples
402
9.4
Summary
422
9.5
Case Study: Microelectronic Solder Joints
424
9.6
Case Study: Internet Marketing
426
9.7
Supplementary Problems
427
C H A P T E R 1 0
D I S C R E T E D A T A A N A L Y S I S
432
10.1 Inferences on a Population Proportion
432
10.2 Comparing Two Population Proportions
455
10.3 Goodness of Fit Tests for One-Way
Contingency Tables
466
10.4 Testing for Independence in Two-Way
Contingency Tables
478
10.5 Case Study: Microelectronic Solder Joints
488
10.6 Case Study: Internet Marketing
489
10.7 Supplementary Problems
490
C H A P T E R 1 1
T H E A N A L Y S I S O F V A R I A N C E
494
11.1 One-Factor Analysis of Variance
494
11.2 Randomized Block Designs
520
11.3 Case Study: Microelectronic Solder Joints
537
11.4 Case Study: Internet Marketing
539
11.5 Supplementary Problems
540
C H A P T E R 1 2
S I M P L E L I N E A R R E G R E S S I O N
A N D C O R R E L A T I O N
543
12.1 The Simple Linear Regression Model
543
12.2 Fitting the Regression Line
551
12.3 Inferences on the Slope Parameter β1
561
12.4 Inferences on the Regression Line
569
12.5 Prediction Intervals for Future
Response Values
575
12.6 The Analysis of Variance Table
579
12.7 Residual Analysis
585
12.8 Variable Transformations
590
12.9 Correlation Analysis
594
12.10 Case Study: Microelectronic Solder
Joints
600
12.11 Case Study: Internet Marketing
601
12.12 Supplementary Problems
602

CONTENTS
ix
C H A P T E R 1 3
M U L T I P L E L I N E A R R E G R E S S I O N
A N D N O N L I N E A R R E G R E S S I O N
608
13.1 Introduction to Multiple Linear
Regression
608
13.2 Examples of Multiple Linear Regression
618
13.3 Matrix Algebra Formulation of Multiple
Linear Regression
628
13.4 Evaluating Model Adequacy
637
13.5 Nonlinear Regression
643
13.6 Case Study: Internet Marketing
647
13.7 Supplementary Problems
648
C H A P T E R 1 4
M U L T I F A C T O R E X P E R I M E N T A L
D E S I G N A N D A N A L Y S I S
650
14.1 Experiments with Two Factors
650
14.2 Experiments with Three or More Factors
679
14.3 Case Study: Internet Marketing
692
14.4 Supplementary Problems
693
C H A P T E R 1 5
N O N P A R A M E T R I C S T A T I S T I C A L
A N A L Y S I S
694
15.1 The Analysis of a Single Population
695
15.2 Comparing Two Populations
716
15.3 Comparing Three or More Populations
726
15.4 Case Study: Internet Marketing
732
15.5 Supplementary Problems
733
C H A P T E R 1 6
Q U A L I T Y C O N T R O L M E T H O D S
736
16.1 Introduction
736
16.2 Statistical Process Control
736
16.3 Variable Control Charts
742
16.4 Attribute Control Charts
752
16.5 Acceptance Sampling
758
16.6 Case Study: Internet Marketing
763
16.7 Supplementary Problems
764
C H A P T E R 1 7
R E L I A B I L I T Y A N A L Y S I S
A N D L I F E T E S T I N G
766
17.1 System Reliability
766
17.2 Modeling Failure Rates
772
17.3 Life Testing
777
17.4 Case Study: Internet Marketing
785
17.5 Supplementary Problems
786
Tables
787
Answers to Odd-Numbered Problems
796
Index
818

PREFACE
Unlikely events happen all the time to somebody somewhere—it’s just
that it would be strange if they happened to you or me.
—Inspector Morimoto and the Sushi Chef by Timothy Hemion
As before, the primary guidelines governing the development of the fourth edition of this
textbook have been to extend the strengths of the previous editions that have resulted in its
adoption worldwide for teaching probability and statistics at both undergraduate and graduate
levels. The cornerstone of the success of this textbook has been that it is full of real examples,
which I believe is the best way to teach and capture the interests of students. It is clearly
important to include examples that are relevant to engineering. However, it is just as important
to incorporate examples that are interesting to both the instructor and the students. In the
fourth edition new examples have been included relating to internet use together with green
and sustainable practices.
This textbook has been built around three main pedagogical tenets: (1) talk to students with
a language and vocabulary that students ﬁnd familiar from their other science and engineering
courses, (2) provide clear explanations and expositions of the statistical concepts that students
need for their work and research, and (3) provide a ﬁrm reinforcement of the theoretical
concepts in interesting examples to which the students can relate. Moreover, the foundation
underlying all my teaching activities is my conviction that teaching is a valuable and noble
enterpriseandthatitisworthwhiletodevotetimeandenergytowarddoingitaswellaspossible.
The education of the following generations is an important way for us to pay back something in
response to the many advantages and opportunities that we have been afforded in our own lives.
This book has been adopted for undergraduate sequences providing an introduction to
data analysis from probability theory through basic statistical techniques and leading to more
advanced statistical inference methods. The book has also been used for graduate-level service
courses, and it provides a useful handbook for researchers in engineering and the sciences. It
is intended for students with reasonable quantitative abilities, although it is designed mainly
to provide an applied rather than a theoretical exposition.
Highlights of the Book
■
The book has been developed from extensive teaching experience with undergraduate
and graduate engineering, science, and business students.
■
Real examples from the engineering sciences and from general internet areas are
developed throughout the book.
■
The applied presentation stresses the comprehension of the underlying concepts and
the application of statistical methodologies.
■
A large number of interesting data sets from a wide range of ﬁelds such as internet
usage and green and sustainable practices are included.
x

PREFACE
xi
■
A guide to matching statistical inference methodologies to data sets and research
questions is presented.
■
Two motivating case studies on Microelectronic Solder Joints and Internet Marketing
are included at the beginning of the book and are continued at the ends of the chapters.
■
A large number and variety of exercise problems of various levels of difﬁculty and
format are included.
■
The book provides a handbook of statistical methodologies for undergraduate and
graduate engineering students.
■
Computer notes offer help and tips for data analysis with statistical software packages.
■
The composition of the book allows ﬂexibility in the order in which the material is
taught.
■
Historical notes are provided for famous probabilists and statisticians.
Motivation and Goals
The primary goal of this book is to provide a means of leading the reader through the impor-
tant issues of data collection, data presentation, data analysis and decision making. These are
tremendously important topics for students and researchers today, and the book is designed to
show the reader how to think about these issues properly and accurately. A key issue in this
goal is illustrating to the reader why statistical analysis methods are relevant and useful for
engineering and the sciences.
The topics are presented in the context of a wide range of engineering and scientiﬁc
examples that provide a motivation for the development of the material. The examples show
how the techniques can be used to gain an understanding of the data set under consideration.
The examples are intended to be readily understood by the reader and to be interesting and
thought provoking. This book can also be used as a handbook of statistical techniques and
probability distributions for all scientists, engineers, and anybody involved in data analysis.
A guide to matching statistical inference methodologies to data sets and research questions is
presented. Many students have commented that it is a valuable resource that they have used
long after ﬁnishing their statistics course.
The book concentrates on allowing the reader to obtain an understanding of the concepts
behind the methodologies presented, rather than providing an unnecessary amount of theory.
The reader is encouraged to look at a formula and to understand what it is doing and how it
works. The reader is then able to use statistical software packages properly and knows which
analysis techniques to employ and how to use and interpret the results.
Presentation of Topics
Each of the topics presented in this book is introduced with reference to several examples
from different engineering and scientiﬁc areas and with reference to some data sets. After the
technical development of the topic has been described, the important points are summarized
in a highlighted box.
The examples are then used to show the proper application of the new methodology. These
examples are built on and developed throughout the chapters as increasingly sophisticated
methodologies are considered. This presentation provides ties and connections between the
differentchapters,anditalsoshowshoweachoftheindividualtopicsﬁtsintothewiderrangeof
statisticalmethodologiesthatcanbeappliedtoanyparticularproblem.Moreover,therelevance

xii
PREFACE
and importance of the statistical analysis to these problems are demonstrated. A list of the ex-
amplesandthesectionswheretheyappearinthetextisprovidedontheinsideofthefrontcover.
Continuing Case Studies
Two motivating case studies on Microelectronic Solder Joints and Internet Marketing are
presented at the beginning of the book. These show how the subjects of probability and
statistics can be applied to the issues that arise in computer chip construction and internet
usage. The analyses are decomposed into several core constituents that are tied to the different
chapters of the book. The case studies provide students with an immediate explanation for
why it is necessary and important to study probability and statistics. Instructors may choose
to use the case studies at the beginning of the course as motivating material and as a road map
to show students where they will be going. Instructors can also refer back to the case studies
before each new chapter or topic is started in order to show how the different topics all ﬁt
together with each other.
Composition of the Book
The ﬁgure on the next page illustrates the composition of this book. The chapters are arranged
from general probability theory and probability distributions to descriptive statistics, basic
statistical inference techniques, and more advanced statistical inference techniques. The book
also includes chapters on nonparametric methods, quality control methods, and reliability
analysis and life testing. Instructors can be very ﬂexible in the selection and order in which
the chapters are actually taught in a course.
Data Sets
All the data sets used in the book can be found on the companion website through
www.CengageBrain.com. The data sets of the worked examples are included so that read-
ers can replicate the results with their own statistical software package. Many of the exercise
problems also involve data sets included on www.CengageBrain.com that readers can analyze
with their own statistical software packages.
A list of the data sets and the sections where they are used is provided on the inside of the
back cover.
Exercise Problems
This book contains a large number of exercise problems of varying difﬁculty levels and
formats. The problems are presented at the end of every section within the chapters, and in
addition a set of supplementary problems is provided at the end of each chapter. The initial
problems take the reader through the steps of the new material that has been presented and
allow the reader to practice the material. The subsequent problems become more difﬁcult
and more open-ended. Most of the problems are presented in the context of engineering and
scientiﬁc problems together with data sets. Some multiple choice and true/false problems
are also included. Answers to all the odd-numbered problems at the ends of the chapter
sections are given at the back of this book, and worked solutions can be found in the Student
Solution Manual for these odd-numbered problems and in the Instructor Solution Manual for
all of the problems. Instructors can also access Solution Builder, an online instructor database
offering complete, worked-out solutions to all exercises in the text, which allows you to create
customized, secure solutions printouts (in PDF format) matched exactly to the problems you
assign in class. Sign up for access at www.cengage.com/solutionbuilder.

PREFACE
xiii
Accompanying Materials
■
All the data sets in this book are available on www.CengageBrain.com.
■
Worked solutions and answers to all the problems are presented in the Instructor
Solution Manual and on SolutionBuilder at www.cengage.com/SolutionBuidler.
■
Worked solutions and answers to all the odd-numbered problems at the ends of the
chapter sections are presented in the Student Solution Manual.
■
A password protected, Single-Sign-On Instructor site has datasets, a link to Solution-
Builder, and a multimedia manager of all the art ﬁgures in the text. You can ﬁnd the
instructor site through www.CengageBrain.com or by accessing the Cengage catalog
at www.cengage.com/statistics/hayter.

xiv
PREFACE
Acknowledgments
I would like to express my heartfelt thanks to my editor, Molly Taylor, and to all members
of the team that have contributed to this fourth edition with their wonderful talents, wisdom,
experience, and energy. This especially includes Andrew Coppola, Alexander Gontar, Linda
Helcher, Charu Khanna, Jill A. Quinn, Shaylin Walsh Hogan, and Richard Stratton.
Finally, I would also like to thank various reviewers who have helped with the de-
velopment of this book, especially Christopher Scott Brown of the University of South
Alabama for his work on the fourth edition. The reviewers of the ﬁrst edition include Mary
R. Anderson, Arizona State University; Charles E. Antle, The Pennsylvania State University,
University Park; Sant Ram Arora, University of Minnesota—Twin Cities Campus; William
R. Astle, Colorado School of Mines; Lee J. Bain, University of Missouri—Rolla; Douglas
M. Bates, University of Wisconsin—Madison; Rajan Batta, The State University of New
York at Buffalo; Alan C. Bovik, University of Texas at Austin; Don B. Campbell, Western
Illinois University; M. Jeya Chandra, Pennsylvania State University—University Park; Yueh-
Jane Chang, Idaho State University; Chung-Lung Chen, Mississippi State University; Inchan
Choi, Wichita State University; John R. Cook, North Dakota State University; Rianto A.
Djojosugito, South Dakota School of Mines and Technology; Lucien Duckstein, University
of Arizona; Earnest W. Fant, University of Arkansas; Richard F. Feldman, Texas A & M
University; Sam Gutmann, Northeastern University; Carol O’Connor Holloman, University
of Louisville; Chi-Ming Ip, University of Miami; Rasul A. Khan, Cleveland State Univer-
sity; Stojan Kotefski, New Jersey Institute of Technology; Walter S. Kuklinski, University
of Massachusetts—Lowell; S. Kumar, Rochester Institute of Technology; Gang Li, Univer-
sity of North Carolina at Charlotte; Jiye-Chyi Lu, North Carolina State University; Ditlev
Monrad, University of Illinois at Urbana—Champaign; John Morgan, California Polytechnic
State University—Pomona; Paul J. Nahin, University of New Hampshire; Larry Ringer, Texas
A & M University; Paul L. Schillings, Montana State University; Ioannis Stavrakakis, The
University of Vermont; and James J. Swain, University of Alabama—Huntsville.
The reviewers of the second edition were Alexander Dukhovny, San Francisco State Uni-
versity; Marc Genton, Massachusetts Institute of Technology; Diwakar Gupta, University of
Minnesota; Joseph J. Harrington, Harvard University; and Jim Rowland, University of Kansas.
Survey respondents for the third edition include Mostafa S. Aminzadeh, Towson Univer-
sity; Barb Barnet, University of Wisconsin—Platteville; Ronald D. Bennett, Bethel College;
Shannon Brewer, Northeast State Community College; Frank C. Castronova, Lawrence Tech-
nological University; Mike Doviak, Old Dominion University; Natarajan Gautam, Penn State
University; Peggy Hart, Doane College; Wei-Min Huang, Lehigh University; Xiaoming Huo,
Georgia Tech; Bruce N. Janson, University of Colorado at Denver; Scott Jilek, University
of St. Thomas; Michael Kostreva, Clemson University; Paul Kvam, Georgia Tech; David
W. Matolak, Ohio University; Gary C. McDonald, Oakland University; Megan Meece, Uni-
versity of Florida; Luke Miller, University of San Diego; Steve Patch, University of North
Carolina at Asheville; Robi Polikar, Rowan University; Andrew M. Ross, Lehigh University;
Manuel D. Rossetti, University of Arkansas; Robb Sinn, North Georgia College and State
University; Bradley Thiessen, St. Ambrose University; Dolores Tichenor, Tri-State Univer-
sity; Lewis VanBrackle, Kennesaw State University; Jerry Weyand, Cleary University; Ed
Wheeler, University of Tennessee at Martin; Elaine Zanutto, The Wharton School, University
of Pennsylvania; and Kathy Zhong, University of Detroit Mercy.
The reviewers for the fourth edition include Georgiana Baker, University of South
Carolina; Arthur Cohen, Rutgers University; Diane Evans, Rose-Hulman Institute of Technol-
ogy; Piotr Kokoszka, Utah State University; Nikolay Strigul, Stevens Institute of Technology;
and Daniela Szatmari Voicu, Kettering University.
Anthony Hayter

CASE STUDIES
These running case studies continue through the chapters of the book. They
demonstrate how probability and statistical inference can be applied to the important
engineering problem of microelectronic solder joints and the developing ﬁeld
concerning the analysis of internet usage.
(1) Continuing Case Study: Microelectronic Solder Joints
Solder joints are an important component of microelectronic assemblies. Figure CS.1 shows
a cross-section of a typical assembly known as a ﬂip chip in which as series of conductive
bump-shaped solder joints are used to attach a silicon chip to a printed circuit board, which is
known as the substrate. These solder joints provide the conductive path from the silicon ship
to the substrate, and fatigue in the solder joints is responsible for almost all of the mechanical
and electrical failures of the assembly.
The area surrounding the solder joints between the silicon chip and the substrate is ﬁlled
with a substance known as the underﬁll, which is non-conductive epoxy that helps to protect
the solder joints from moisture as well as adding strength to the assembly. The underﬁll
also helps to minimize the stress in the solder joints that arises from the different thermal
expansions of the silicon chip and the substrate. This helps ensure that the connections are
not damaged or broken.
It is important to investigate the reasons behind the development of cracks in the solder
joints which can affect the operation of the assembly. The development of these cracks can
be related to the shapes of the solder joints, which are illustrated in Figure CS.2. While most
of the joints turn out to be barrel shaped, some may have cylinder shapes or hourglass shapes.
In addition to cracks in the solder joints, failures can also be caused by solder extrusions that
connect two adjacent solder joints as shown in Figure CS.3, or which leave only a very small
gap between two adjacent solder joints.
Inaddition,acriticalcomponentoftheassemblyisthebondingbetweenthesolderjointand
the substrate which is achieved through suitable metallization of the substrate. As Figure CS.4
shows, a bond pad is created in the substrate made of copper, which is coated with thin layers
of nickel and gold. The thickness of the gold layer has an important effect on the reliability
of the electrical connection between the solder joint and the substrate.
FIGURE CS.1
Cross section of a typical ﬂip chip
microelectronic assembly
Silicon chip
Printed circuit board
(Substrate)
Solder joint
Epoxy underfill
xv

xvi
CASE STUDIES
Barrel shape
Cylinder shape
Hourglass shape
FIGURE CS.2
Shapes of solder joints in a microelectronic assembly
Extrusion
FIGURE CS.3
An extrusion between solder joints
in a microelectronic assembly
FIGURE CS.4
Diagram of a substrate bond pad in
a microelectronic assembly
Gold layer
Nickel layer
Substrate
Copper
Solder joint
Reliability assessment of these microelectronic assemblies is often performed with accel-
erated life tests. Since it is known that temperature changes cause stress in the solder joints that
can ultimately lead to failure, the accelerated life tests often consist of subjecting the assembly
to alternate periods at low and high temperatures. For example, the assembly may be alter-
nately immersed in liquid at −55 degrees Centigrade for ﬁve minutes, and then switched to a
liquid at 125 degrees Centigrade for ﬁve minutes. This cycle is repeated many times until the
assembly eventually fails. These accelerated life tests are designed to mimic (at an accelerated
speed) the conditions to which the assembly will be subjected in its everyday operation, and it
is anticipated that designs which survive the most cycles of the accelerated life test will have
the best reliability in real world applications.
A considerable amount of research is conducted on the fatigue life of the solder joints
together with their optimal production method, and the areas of probability and statistics play
major roles in this research. This case study will be continued through the ﬁrst twelve chapters
of the book to show how probability theory and statistical inference can be applied to this
important engineering problem.
Chapter 1 Probability Theory
For a given production method, it is important to know the probabilities that the solder joints
will be formed according to each of the three shape proﬁles. Additionally, the probability that
cracking occurs in each of the different solder joint shapes is investigated, and the probability
of ﬁnding a given number of cracked solder joints within a random sample of solder joints is
calculated.
Chapter 2 Random Variables
The number of extrusions in an assembly with a large number of solder joints is critical
to the overall lifetime of the assembly. In this chapter the average number of extrusions in

CASE STUDIES
xvii
an assembly is investigated, and an assessment is made of the amount of variability in the
number of extrusions. In addition, the total number of extrusions in a batch of 250 assemblies
is considered.
Chapter 3 Discrete Probability Distributions
An analysis is conducted of the number of hourglass shaped solder joints on an assembly
which contains 64 solder joints. A calculation is also performed concerning the amount of
work that is necessary if a researcher intends to examine solder joints one at a time until two
cracked hourglass shaped solder joints are discovered. Finally, the distribution of the number
of solder joints of each shape on an assembly consisting of 16 solder joints is discussed.
Chapter 4 Continuous Probability Distributions
Accelerated life tests are considered and attention is directed to the question of how many
temperature cycles an assembly can survive before it fails. A probability distribution is used to
model this failure time. An investigation is also conducted into how different types of epoxies
employed in the underﬁll can affect the reliability of the assembly.
Chapter 5 The Normal Distribution
The thickness of the gold layer at the top of the bond pad has an important effect on the
reliability of the bond between the solder joint and the substrate. In this chapter these thick-
nesses are modelled with a normal probability distribution, and it is shown how to calculate
the probability that the thickness of the gold layer lies within an optimal range. The average
thickness of the gold layers on an assembly consisting of 16 bond pads is also considered.
Finally, calculations are made concerning the number of hourglass shaped solder joints that
will be produced on an assembly comprised of 512 solder joints.
Chapter 6 Descriptive Statistics
A data set is obtained by measuring the thicknesses of the nickel layers deposited by a new
method on each of the bond pads of an assembly. Summary statistics are calculated for the
data set. Also, a categorical data set is obtained from the frequencies of the solder joint shapes
in a large assembly produced by a particular methodology.
Chapter 7 Statistical Estimation and Sampling Distributions
The data sets given in Chapter 6 are used to obtain estimates of the quantities of interests. The
average amount of nickel deposited by the new method is estimated, and an assessment of
the estimate’s accuracy is made. Also, for the methodology under consideration an estimate
is obtained for the probability that a solder joint will have a barrel shape, and the precision of
this estimate is also considered.
Chapter 8 Inferences on a Population Mean
The data set of nickel layer thicknesses is used to examine whether the new method is applying
nickel according to the desired target value. The information pertinent to this question is
extracted from the data set and is summarized in various ways. Finally, calculations are
performed to estimate how many additional nickel layer thickness measurements are needed
in order to increase the sensitivity of the statistical inferences to a speciﬁed level.

xviii
CASE STUDIES
Chapter 9 Comparing Two Population Means
A comparison is made between the original method of applying the nickel layer to the substrate
bond pads and the new method. A data set of nickel layer thicknesses achieved from the original
method is compared with the data set of nickel layer thicknesses achieved with the new method
in order to test whether there is any evidence of a difference between the two methods. Analysis
methods are introduced which allow the researcher to quantify this difference.
Chapter 10 Discrete Data Analysis
A test is performed to determine whether the frequencies of the solder joint shapes observed
in the analysis of a large assembly are consistent with some theoretical probability values.
The data set is also used to calculate a range for one of these probability values. In addition,
an experiment is conducted to investigate which of two epoxy formulations for the underﬁll
results in the smallest failure probability after 2000 temperature cycles of the accelerated life
test, and the analysis of the resulting data set is performed.
Chapter 11 The Analysis of Variance
In this chapter it is shown how a comparison can be made between four different companies
in terms of the thicknesses of the gold layers on the substrate bond pads. A random selection
is made of bond pads on assemblies produced by each of the four companies, and a data set is
formed by measuring the gold layer thicknesses. An analysis of the data set is then performed
which allows the determination of which company has the thinnest gold layers and which
company has the thickest gold layers.
Chapter 12 Simple Linear Regression and Correlation
The researcher is interested in whether the heights of the solder joints have any inﬂuence on the
reliability of the microelectronic assembly. An experiment is conducted whereby assemblies
with different solder joint heights are subjected to an accelerated life test until they fail. The
number of temperature cycles that the assemblies can withstand before failing is measured,
and an analysis is performed to investigate whether there is any evidence that the solder joint
heights have any effect on the reliability of the assembly.
(2) Continuing Case Study: Internet Marketing
An organisation’s website is obviously an important way for it to market itself and to drive its
business. Data are often automatically collected when an individual uses the organisation’s
website, and concern information on how the individual reached the websites, as well as the
actual website activities. The analysis of this data can reveal important information to the
organisation about how effective different website designs are, and how successful its various
online marketing campaigns and methods have been. This case study is included at the end of
each chapter, and the following questions are addressed.

CASE STUDIES
xix
Chapter 1
Probability Theory
An organisation’s website may be accessed directly or through various links. How does the
probability that an online purchase is made depend upon these different ways of accessing the
website?
Chapter 2
Random Variables
An organisation can incur a cost when its website is accessed through sponsored advertise-
ments on other websites, or through search engines. What costs can the organisation expect
to pay for marketing its website in these ways?
Chapter 3
Discrete Probability Distributions
How can the organisation predict the proportion of times that its website will be accessed
directly without incurring any cost?
Chapter 4
Continuous Probability Distributions
When an individual is logged on to the organisation’s website, the length of the idle periods
is monitored. For security purposes, the individual is automatically logged out when the idle
period reaches a certain time. How can the organization balance security with convenience
and ease-of-use?
Chapter 5
The Normal Distribution
How can the organisation estimate how many visitors there will be to its website over certain
periods of time?
Chapter 6
Descriptive Statistics
How can the organisation present and analyze data on the number of visits to its website?
Chapter 7
Statistical Estimation and Sampling Distributions
How can the organization estimate the effectiveness of the banner advertisements that it has
on certain websites?
Chapter 8
Inferences on a Population Mean
What inferences can the organisation make about the average number of visits per week to its
website?
Chapter 9
Comparing Two Population Means
Which of two different search engines are generating more trafﬁc to the organisation’s website?

xx
CASE STUDIES
Chapter 10
Discrete Data Analysis
The organisation has two different designs for the banner advertisement that it employs on
other websites. Which banner design is more effective?
Chapter 11
The Analysis of Variance
How does the amount of trafﬁc generated to the organisation’s website by a banner advertise-
ment depend upon the design of the banner and the day of the week?
Chapter 12
Simple Linear Regression and Correlation
The organisation conducts various advertising campaigns and monitors their costs and produc-
tivity in terms of the amount of trafﬁc generated to its website. What are the most cost-effective
advertising campaigns?
Chapter 13
Multiple Linear Regression and Nonlinear Regression
How many of the visits to the organisation’s website result in an online purchase? How does
this change when there is a promotional campaign in operation?
Chapter 14
Multifactor Experimental Design and Analysis
Over a two month experimental period, the organisation monitors how many visits there are to
its website under different advertising strategies. In some weeks the organisation pays to have
sponsored advertisements on three leading search engines, and in addition, in some weeks the
company has television advertising. The four combinations of advertising types are run twice,
for one week periods each in a random order. What does the data tell us about the effectiveness
of the online advertising on the search engines and the television advertising?
Chapter 15
Nonparametric Statistical Analysis
Further analysis is presented concerning which of two different search engines are generating
more trafﬁc to the organisation’s website, and how the amount of trafﬁc directed to the
organisation’s website by a banner advertisement depends upon the design of the banner and
the day of the week.
Chapter 16
Quality Control Methods
Over a six month period the organisation monitors the number of visits to its website and the
number of resulting purchases on a weekly basis. How can control charts be used to present
and analyze this data?
Chapter 17
Reliability Analysis and Life Testing
Further analysis of an individual being automatically logged out when the online idle period
reaches a certain time.

C H A P T E R O N E
Probability Theory
1.1
Probabilities
1.1.1
Introduction
Jointly with statistics, probability theory is a branch of mathematics that has been developed
to deal with uncertainty. Classical mathematical theory had been successful in describing
the world as a series of ﬁxed and real observable events, yet before the seventeenth century
it was largely inadequate in coping with processes or experiments that involved uncertain or
random outcomes. Spurred initially by the mathematician’s desire to analyze gambling games
and later by the scientiﬁc analysis of mortality tables within the medical profession, the theory
of probability has been developed as a scientiﬁc tool dealing with chance.
Today, probability theory is recognized as one of the most interesting and also one of the
most useful areas of mathematics. It provides the basis for the science of statistical inference
through experimentation and data analysis—an area of crucial importance in an increasingly
quantitative world. Through its applications to problems such as the assessment of system
reliability, the interpretation of measurement accuracy, and the maintenance of suitable quality
controls, probability theory is particularly relevant to the engineering sciences today.
1.1.2
Sample Spaces
An experiment can in general be thought of as any process or procedure for which more than
one outcome is possible. The goal of probability theory is to provide a mathematical structure
for understanding or explaining the chances or likelihoods of the various outcomes actually
occurring. A ﬁrst step in the development of this theory is the construction of a list of the
possible experimental outcomes. This collection of outcomes is called the sample space or
state space and is denoted by S.
Sample Space
The sample space S of an experiment is a set consisting of all of the possible
experimental outcomes.
The following examples help illustrate the concept of a sample space.
Example 1
Machine Breakdowns
An engineer in charge of the maintenance of a particular machine notices that its breakdowns
can be characterized as due to an electrical failure within the machine, a mechanical failure
of some component of the machine, or operator misuse. When the machine is running, the
1

2
CHAPTER 1
PROBABILITY THEORY
engineer is uncertain what will be the cause of the next breakdown. The problem can be
thought of as an experiment with the sample space
S = {electrical, mechanical, misuse}
Example 2
Defective Computer
Chips
A company sells computer chips in boxes of 500, and each chip can be classiﬁed as either
satisfactory or defective. The number of defective chips in a particular box is uncertain, and
the sample space is
S = {0 defectives, 1 defective, 2 defectives, 3 defectives, 4 defectives, . . . ,
499 defectives, 500 defectives}
Example 3
Software Errors
The control of errors in computer software products is obviously of great importance. The
number of separate errors in a particular piece of software can be viewed as having a sample
space
S = {0 errors, 1 error, 2 errors, 3 errors, 4 errors, 5 errors, . . .}
In practice there will be an upper bound on the possible number of errors in the software,
although conceptually it is all right to allow the sample space to consist of all of the positive
integers.
S
(0, 0, 0)
(1, 0, 0)
(0, 0, 1)
(1, 0, 1)
(0, 1, 0)
(1, 1, 0)
(0, 1, 1)
(1, 1, 1)
FIGURE 1.1
Sample space for power plant
example
Example 4
Power Plant Operation
A manager supervises the operation of three power plants, plant X, plant Y, and plant Z. At
any given time, each of the three plants can be classiﬁed as either generating electricity (1)
or being idle (0). With the notation (0, 1, 0) used to represent the situation where plant Y is
generating electricity but plants X and Z are both idle, the sample space for the status of the
three plants at a particular point in time is
S = {(0, 0, 0) (0, 0, 1) (0, 1, 0) (0, 1, 1) (1, 0, 0) (1, 0, 1) (1, 1, 0) (1, 1, 1)}
It is often helpful to portray a sample space as a diagram. Figure 1.1 shows a dia-
gram of the sample space for this example, where the sample space is represented by a
box containing the eight individual outcomes. Diagrams of this kind are known as Venn
diagrams.
GAMES OF CHANCE
Games of chance commonly involve the toss of a coin, the roll of a die, or the use of a pack
of cards. The toss of a single coin has a sample space
S = {head, tail}
and the toss of two coins (or one coin twice) has a sample space
S = {(head, head) (head, tail) (tail, head) (tail, tail)}
where (head, tail), say, represents the event that the ﬁrst coin resulted in a head and the second
coin resulted in a tail. Notice that (head, tail) and (tail, head) are two distinct outcomes since
observing a head on the ﬁrst coin and a tail on the second coin is different from observing a
tail on the ﬁrst coin and a head on the second coin.
A usual six-sided die has a sample space
S = {1, 2, 3, 4, 5, 6}

1.1 PROBABILITIES
3
FIGURE 1.2
Sample space for rolling two dice
S
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
(3, 1)
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 3)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 2)
(5, 3)
(5, 4)
(5, 5)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
FIGURE 1.3
Sample space for choosing one card
S
A♥
2♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
J♥
Q♥
K♥
A♣
2♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
J♣
Q♣
K♣
A♦
2♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
J♦
Q♦
K♦
A♠
2♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
J♠
Q♠
K♠
FIGURE 1.4
Sample space for choosing two
cards with replacement
S
(A♥, A♥)
(A♥, 2♥)
(A♥, 3♥)
· · ·
(A♥, Q♠)
(A♥, K♠)
(2♥, A♥)
(2♥, 2♥)
(2♥, 3♥)
· · ·
(2♥, Q♠)
(2♥, K♠)
(3♥, A♥)
(3♥, 2♥)
(3♥, 3♥)
· · ·
(3♥, Q♠)
(3♥, K♠)
...
...
...
...
...
(Q♠, A♥)
(Q♠, 2♥)
(Q♠, 3♥)
· · ·
(Q♠, Q♠)
(Q♠, K♠)
(K♠, A♥)
(K♠, 2♥)
(K♠, 3♥)
· · ·
(K♠, Q♠)
(K♠, K♠)
If two dice are rolled (or, equivalently, if one die is rolled twice), then the sample space is
shown in Figure 1.2, where (1, 2) represents the event that the ﬁrst die recorded a 1 and the
second die recorded a 2. Again, notice that the events (1, 2) and (2, 1) are both included in
the sample space because they represent two distinct events. This can be seen by considering
one die to be red and the other die to be blue, and by distinguishing between obtaining a 1 on
the red die and a 2 on the blue die and obtaining a 2 on the red die and a 1 on the blue die.
If a card is chosen from an ordinary pack of 52 playing cards, the sample space consists
of the 52 individual cards as shown in Figure 1.3. If two cards are drawn, then it is necessary
to consider whether they are drawn with or without replacement. If the drawing is performed
with replacement, so that the initial card drawn is returned to the pack and the second drawing
is from a full pack of 52 cards, then the sample space consists of events such as (6♥, 8♣),
where the ﬁrst card drawn is 6♥and the second card drawn is 8♣. Altogether there will be
52 × 52 = 2704 elements of the sample space, including events such as (A♥, A♥), where
the A♥is drawn twice. This sample space is shown in Figure 1.4.

4
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.5
Sample space for choosing two
cards without replacement
S
(A♥, 2♥)
(A♥, 3♥)
· · ·
(A♥, Q♠)
(A♥, K♠)
(2♥, A♥)
(2♥, 3♥)
· · ·
(2♥, Q♠)
(2♥, K♠)
(3♥, A♥)
(3♥, 2♥)
· · ·
(3♥, Q♠)
(3♥, K♠)
...
...
...
...
...
(Q♠, A♥)
(Q♠, 2♥)
(Q♠, 3♥)
· · ·
(Q♠, K♠)
(K♠, A♥)
(K♠, 2♥)
(K♠, 3♥)
· · ·
(K♠, Q♠)
If two cards are drawn without replacement, so that the second card is drawn from a
reduced pack of 51 cards, then the sample space will be a subset of that above, as shown in
Figure 1.5. Speciﬁcally, events such as (A♥, A♥), where a particular card is drawn twice,
will not be in the sample space. The total number of elements in this new sample space will
therefore be 2704 −52 = 2652.
1.1.3
Probability Values
The likelihoods of particular experimental outcomes actually occurring are found by assigning
a set of probability values to each of the elements of the sample space. Speciﬁcally, each
outcome in the sample space is assigned a probability value that is a number between zero
and one. The probabilities are chosen so that the sum of the probability values over all of the
elements in the sample space is one.
Probabilities
A set of probability values for an experiment with a sample space
S = {O1, O2, . . . , On} consists of some probabilities
p1, p2, . . . , pn
that satisfy
0 ≤p1 ≤1, 0 ≤p2 ≤1, . . . , 0 ≤pn ≤1
and
p1 + p2 + · · · + pn = 1
The probability of outcome Oi occurring is said to be pi, and this is written
P(Oi) = pi.
An intuitive interpretation of a set of probability values is that the larger the probability
value of a particular outcome, the more likely it is to happen. If two outcomes have identical
probability values assigned to them, then they can be thought of as being equally likely to
occur. On the other hand, if one outcome has a larger probability value assigned to it than
another outcome, then the ﬁrst outcome can be thought of as being more likely to occur.

1.1 PROBABILITIES
5
FIGURE 1.6
Probability values for machine
breakdown example
S
Electrical
Mechanical
Misuse
0.2
0.5
0.3
If a particular outcome has a probability value of one, then the interpretation is that it is
certain to occur, so that there is actually no uncertainty in the experiment. In this case all of
the other outcomes must necessarily have probability values of zero.
The following examples illustrate the assignment of probability values.
Example 1
Machine Breakdowns
Suppose that the machine breakdowns occur with probability values of P(electrical) = 0.2,
P(mechanical) = 0.5, and P(misuse) = 0.3. This is a valid probability assignment since
the three probability values 0.2, 0.5, and 0.3 are all between zero and one and they sum to
one. Figure 1.6 shows a diagram of these probabilities by recording the respective probability
value with each of the outcomes. These probability values indicate that mechanical failures
are most likely, with misuse failures being more likely than electrical failures.
In addition, P(mechanical) = 0.5 indicates that about half of the failures will be at-
tributable to mechanical causes. This does not mean that of the next two machine breakdowns,
exactly one will be for mechanical reasons, or that in the next ten machine breakdowns, exactly
ﬁvewillbeformechanicalreasons.However,itmeansthatinthe longrun,themanagercanrea-
sonably expect that roughly half of the breakdowns will be for mechanical reasons. Similarly,
in the long run, the manager will expect that about 20% of the breakdowns will be for electrical
reasons, and that about 30% of the breakdowns will be attributable to operator misuse.
Example 3
Software Errors
Suppose that the number of errors in a software product has probabilities
P(0 errors) = 0.05,
P(1 error) = 0.08,
P(2 errors) = 0.35,
P(3 errors) = 0.20,
P(4 errors) = 0.20,
P(5 errors) = 0.12,
P(i errors) = 0,
for i ≥6
These probabilities show that there are at most ﬁve errors since the probability values are
zero for six or more errors. In addition, it can be seen that the most likely number of errors is
two and that three and four errors are equally likely.
It is reasonable to ask how anybody would ever know the probability assignments in the
above two examples. In other words, how would the engineer know that there is a probability
of 0.2 that a breakdown will be due to an electrical fault, or how would a computer programmer
know that the probability of an error-free product is 0.05? In practice these probabilities would
have to be estimated from a collection of data and prior experiences. Later in this book, in
Chapters 7 and 10, it will be shown how statistical analysis techniques can be employed to
help the engineer and programmer conduct studies to estimate probabilities of these kinds.
In some situations, notably games of chance, the experiments are conducted in such a
way that all of the possible outcomes can be considered to be equally likely, so that they
must be assigned identical probability values. If there are n outcomes in the sample space
that are equally likely, then the condition that the probabilities sum to one requires that each
probability value be 1/n.

6
CHAPTER 1
PROBABILITY THEORY
GAMES OF CHANCE
For a coin toss, the probabilities will in general be given by
P(head) = p,
P(tail) = 1 −p
for some value of p with 0 ≤p ≤1. A fair coin will have p = 0.5 so that
P(head) = P(tail) = 0.5
with the two outcomes being equally likely. A biased coin will have p ̸= 0.5. For example, if
p = 0.6, then
P(head) = 0.6,
P(tail) = 0.4
as shown in Figure 1.7, and the coin toss is more likely to record a head.
S
Head
Tail
0.6
0.4
FIGURE 1.7
Probability values for a biased coin
A fair die will have each of the six outcomes equally likely, with each being assigned the
same probability. Since the six probabilities must sum to one, this implies that each of the six
outcomes must have a probability of 1/6, so that
P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = 1
6
This case is shown in Figure 1.8. An example of a biased die would be one for which
P(1) = 0.10,
P(2) = 0.15,
P(3) = 0.15,
P(4) = 0.15,
P(5) = 0.15,
P(6) = 0.30
as in Figure 1.9. In this case the die is most likely to score a 6, which will happen roughly
three times out of ten as a long-run average. Scores of 2, 3, 4, and 5 are equally likely, and a
score of 1 is the least likely event, happening only one time in ten on average.
If two dice are thrown and each of the 36 outcomes are equally likely (as will be the
case with two fair dice that are shaken properly), the probability value of each outcome will
necessarily be 1/36. This is shown in Figure 1.10.
If a card is drawn at random from a pack of cards, then there are 52 possible outcomes in
the sample space, and each one is equally likely so that each would be assigned a probability
value of 1/52. Thus, for example, P(A♥) = 1/52, as shown in Figure 1.11. If two cards
are drawn with replacement, and if both the cards can be assumed to be chosen at random
through suitable shufﬂing of the pack before and between the drawings, then each of the
52×52 = 2704 elements of the sample space will be equally likely and hence should each be
assigned a probability value of 1/2704. In this case P(A♥, 2♣) = 1/2704, for example, as
shown in Figure 1.12. If the drawing is performed without replacement but again at random,
then the sample space has only 2652 elements and each would have a probability of 1/2652,
as shown in Figure 1.13.
S
6
1
1/6
2
1/6
3
1/6
5
1/6
1/6
4
1/6
FIGURE 1.8
Probability values for a fair die
S
1
2
3
4
5
6
0.10
0.15
0.15
0.15
0.15
0.30
FIGURE 1.9
Probability values for a biased die

1.1 PROBABILITIES
7
FIGURE 1.10
Probability values for rolling
two dice
S
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1)
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(4, 1)
(4, 2)
(4, 3)
(4, 4)
(4, 5)
(4, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(5, 1)
(5, 2)
(5, 3)
(5, 4)
(5, 5)
(5, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
1/36
1/36
1/36
1/36
1/36
1/36
FIGURE 1.11
Probability values for choosing one
card
S
A♥
2♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
J♥
Q♥
K♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
A♣
2♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
J♣
Q♣
K♣
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
A♦
2♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
J♦
Q♦
K♦
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
A♠
2♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
J♠
Q♠
K♠
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
FIGURE 1.12
Probability values for choosing two
cards with replacement
S
(A♥, A♥)
(A♥, 2♥)
(A♥, 3♥)
· · ·
(A♥, Q♠)
(A♥, K♠)
1/2704
1/2704
1/2704
· · ·
1/2704
1/2704
(2♥, A♥)
(2♥, 2♥)
(2♥, 3♥)
· · ·
(2♥, Q♠)
(2♥, K♠)
1/2704
1/2704
1/2704
· · ·
1/2704
1/2704
(3♥, A♥)
(3♥, 2♥)
(3♥, 3♥)
· · ·
(3♥, Q♠)
(3♥, K♠)
1/2704
1/2704
1/2704
· · ·
1/2704
1/2704
...
...
...
...
...
(Q♠, A♥)
(Q♠, 2♥)
(Q♠, 3♥)
· · ·
(Q♠, Q♠)
(Q♠, K♠)
1/2704
1/2704
1/2704
· · ·
1/2704
1/2704
(K♠, A♥)
(K♠, 2♥)
(K♠, 3♥)
· · ·
(K♠, Q♠)
(K♠, K♠)
1/2704
1/2704
1/2704
· · ·
1/2704
1/2704

8
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.13
Probability values for choosing two
cards without replacement
S
(A♥, 2♥)
(A♥, 3♥)
· · ·
(A♥, Q♠)
(A♥, K♠)
1/2652
1/2652
· · ·
1/2652
1/2652
(2♥, A♥)
(2♥, 3♥)
· · ·
(2♥, Q♠)
(2♥, K♠)
1/2652
1/2652
· · ·
1/2652
1/2652
(3♥, A♥)
(3♥, 2♥)
· · ·
(3♥, Q♠)
(3♥, K♠)
1/2652
1/2652
· · ·
1/2652
1/2652
...
...
...
...
...
(Q♠, A♥)
(Q♠, 2♥)
(Q♠, 3♥)
· · ·
(Q♠, K♠)
1/2652
1/2652
1/2652
· · ·
1/2652
(K♠, A♥)
(K♠, 2♥)
(K♠, 3♥)
· · ·
(K♠, Q♠)
1/2652
1/2652
1/2652
· · ·
1/2652
1.1.4
Problems
1.1.1 What is the sample space when a coin is tossed three
times?
1.1.2 What is the sample space for counting the number of
females in a group of n people?
1.1.3 What is the sample space for the number of aces in a hand
of 13 playing cards?
1.1.4 What is the sample space for a person’s birthday?
1.1.5 A car repair is performed either on time or late and either
satisfactorily or unsatisfactorily. What is the sample space
for a car repair?
1.1.6 A bag contains balls that are either red or blue and either
dull or shiny. What is the sample space when a ball is
chosen from the bag?
1.1.7 A probability value p is often reported as an odds ratio,
which is p/(1 −p). This is the ratio of the probability
that the event happens to the probability that the event
does not happen.
(a) If the odds ratio is 1, what is p?
(b) If the odds ratio is 2, what is p?
(c) If p = 0.25, what is the odds ratio?
1.1.8 An experiment has ﬁve outcomes, I, II, III, IV, and V. If
P(I) = 0.13, P(II) = 0.24, P(III) = 0.07, and
P(IV) = 0.38, what is P(V)?
1.1.9 An experiment has ﬁve outcomes, I, II, III, IV, and V. If
P(I) = 0.08, P(II) = 0.20, and P(III) = 0.33, what are
the possible values for the probability of outcome V? If
outcomes IV and V are equally likely, what are their
probability values?
1.1.10 An experiment has three outcomes, I, II, and III. If
outcome I is twice as likely as outcome II, and outcome II
is three times as likely as outcome III, what are the
probability values of the three outcomes?
1.1.11 A company’s advertising expenditure is either low with
probability 0.28, average with probability 0.55, or high
with probability p. What is p?
1.2
Events
1.2.1
Events and Complements
Interest is often centered not so much on the individual elements of a sample space, but rather
on collections of individual outcomes. These collections of outcomes are called events.

1.2 EVENTS
9
FIGURE 1.14
P(A) = 0.10 + 0.15 + 0.30 = 0.55
A
S
0.10
0.10
0.10
0.15
0.30
0.05
0.05
0.15
A′
Events
An event A is a subset of the sample space S. It collects outcomes of particular
interest. The probability of an event A, P(A), is obtained by summing the
probabilities of the outcomes contained within the event A.
An event is said to occur if one of the outcomes contained within the event occurs.
Figure 1.14 shows a sample space S consisting of eight outcomes, each of which is
labeled with a probability value. Three of the outcomes are contained within the event A. The
probability of the event A is calculated as the sum of the probabilities of these three events,
so that
P(A) = 0.10 + 0.15 + 0.30 = 0.55
The complement of an event A is taken to mean the event consisting of everything in
the sample space S that is not contained within the event A. The notation A′ is used for the
complement of A. In this example, the probability of the complement of A is obtained by
summing the probabilities of the ﬁve outcomes not contained within A, so that
P(A′) = 0.10 + 0.05 + 0.05 + 0.15 + 0.10 = 0.45
Notice that P(A) + P(A′) = 1, which is a general rule.
Complements of Events
The event A′, the complement of an event A, is the event consisting of everything in
the sample space S that is not contained within the event A. In all cases
P(A) + P(A′) = 1
It is useful to consider both individual outcomes and the whole sample space as also being
events. Events that consist of an individual outcome are sometimes referred to as elementary
events or simple events. If an event is deﬁned to be a particular single outcome, then its

10
CHAPTER 1
PROBABILITY THEORY
probability is just the probability of that outcome. If an event is deﬁned to be the whole
sample space, then obviously its probability is one.
1.2.2
Examples of Events
Example 2
Defective Computer
Chips
Consider the following probability values for the number of defective chips in a box of
500 chips:
P(0 defectives) = 0.02,
P(1 defective) = 0.11,
P(2 defectives) = 0.16,
P(3 defectives) = 0.21,
P(4 defectives) = 0.13,
P(5 defectives) = 0.08
and suppose that the probabilities of the additional elements of the sample space (6 defectives,
7 defectives, . . . , 500 defectives) are unknown. The company is thinking of claiming that each
box has no more than 5 defective chips, and it wishes to calculate the probability that the claim
is correct.
The event correct consists of the six outcomes listed above, so that
correct = {0 defectives, 1 defective, 2 defectives, 3 defectives,
4 defectives, 5 defectives} ⊂S
The probability of the claim being correct is then
P(correct) = P(0 defectives) + · · · + P(5 defectives)
= 0.02 + 0.11 + 0.16 + 0.21 + 0.13 + 0.08 = 0.71
Consequently, on average, only about 71% of the boxes will meet the company’s claim that
there are no more than 5 defective chips. The complement of the event correct is that there
will be at least 6 defective chips so that the company’s claim will be incorrect. This has a
probability of 1 −0.71 = 0.29.
Example 3
Software Errors
Consider the event A that there are no more than two errors in a software product. This event
is given by
A = {0 errors, 1 error, 2 errors} ⊂S
and its probability is
P(A) = P(0 errors) + P(1 error) + P(2 errors)
= 0.05 + 0.08 + 0.35 = 0.48
The probability of the complement of the event A is
P(A′) = 1 −P(A) = 1 −0.48 = 0.52
which is the probability that a software product has three or more errors.
Example 4
Power Plant Operation
Consider the probability values given in Figure 1.15, where, for instance, the probability that
all three plants are idle is P((0, 0, 0)) = 0.07, and the probability that only plant X is idle is
P((0, 1, 1)) = 0.18. The event that plant X is idle is given by
A = {(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1)}

1.2 EVENTS
11
S
(0, 0, 0)
(1, 0, 0)
0.07
0.16
(0, 0, 1)
(1, 0, 1)
0.04
0.18
(0, 1, 0)
(1, 1, 0)
0.03
0.21
(0, 1, 1)
(1, 1, 1)
0.18
0.13
FIGURE 1.15
Probability values for power
plant example
(1, 1, 1)
0.13
(1, 1, 0)
0.21
(1, 0, 1)
0.18
(1, 0, 0)
0.16
(0, 1, 1)
0.18
(0, 1, 0)
0.03
(0, 0, 1)
0.04
(0, 0, 0)
0.07
S
A
FIGURE 1.16
Event A: plant X idle
(1, 1, 1)
0.13
(1, 1, 0)
0.21
(1, 0, 1)
0.18
(1, 0, 0)
0.16
(0, 1, 1)
0.18
(0, 1, 0)
0.03
(0, 0, 1)
0.04
(0, 0, 0)
0.07
S
B
FIGURE 1.17
Event B: at least two plants
generating electricity
as illustrated in Figure 1.16, and it has a probability of
P(A) = P((0, 0, 0)) + P((0, 0, 1)) + P((0, 1, 0)) + P((0, 1, 1))
= 0.07 + 0.04 + 0.03 + 0.18 = 0.32
The complement of this event is
A′ = {(1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)}
which corresponds to plant X generating electricity, and it has a probability of
P(A′) = 1 −P(A) = 1 −0.32 = 0.68
Suppose that the manager is interested in the proportion of the time that at least two out
of the three plants are generating electricity. This event is given by
B = {(0, 1, 1), (1, 0, 1), (1, 1, 0), (1, 1, 1)}
as illustrated in Figure 1.17, with a probability of
P(B) = P((0, 1, 1)) + P((1, 0, 1)) + P((1, 1, 0)) + P((1, 1, 1))
= 0.18 + 0.18 + 0.21 + 0.13 = 0.70
This result indicates that, on average, at least two of the plants will be generating electricity
about 70% of the time. The complement of this event is
B′ = {(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)}
which corresponds to the situation in which at least two of the plants are idle. The probability
of this is
P(B′) = 1 −P(B) = 1 −0.70 = 0.30

12
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.18
Event A: sum equal to 6
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 4)
(5, 5)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
A
(4, 3)
(5, 2)
GAMES OF CHANCE
The event that an even score is recorded on the roll of a die is given by
even = {2, 4, 6}
For a fair die this event would have a probability of
P(even) = P(2) + P(4) + P(6) = 1
6 + 1
6 + 1
6 = 1
2
Figure 1.18 shows the event that the sum of the scores of two dice is equal to 6. This event
is given by
A = {(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)}
If each outcome is equally likely with a probability of 1/36, then this event clearly has a
probability of 5/36. A sum of 6 will be obtained with two fair dice roughly 5 times out of 36
on average, that is, on about 14% of the throws. The probabilities of obtaining other sums can
be obtained in a similar manner, and it is seen that 7 is the most likely score, with a probability
of 6/36 = 1/6. The least likely scores are 2 and 12, each with a probability of 1/36.
Figure 1.19 shows the event that at least one of the two dice records a 6, which is seen to
have a probability of 11/36. The complement of this event is the event that neither die records
a 6, with a probability of 1 −11/36 = 25/36.
Figure 1.20 illustrates the event that a card drawn from a pack of cards belongs to the
heart suit. This event consists of the 13 outcomes corresponding to the 13 cards in the heart
suit. If the drawing is done at random, with each of the 52 possible outcomes being equally
likely with a probability of 1/52, then the probability of drawing a heart is clearly 13/52 =
1/4. This result makes sense since there are four suits that are equally likely. Figure 1.21
illustrates the event that a picture card (jack, queen, or king) is drawn, with a probability of
12/52 = 3/13.

1.2 EVENTS
13
FIGURE 1.19
Event B: at least one 6 recorded
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
B
(5, 2)
(5, 4)
(5, 5)
(4, 3)
FIGURE 1.20
Event A: card belongs to heart suit
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
A
S
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥
FIGURE 1.21
Event B: picture card is chosen
S
B
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥

14
CHAPTER 1
PROBABILITY THEORY
1.2.3
Problems
1.2.1 Consider the sample space in Figure 1.22 with outcomes
a, b, c, d, and e. Calculate:
(a) P(b)
(b) P(A)
(c) P(A′)
A
S
a
0.13
c
0.48
d
0.02
e
0.22
b
?
FIGURE 1.22
1.2.2 Consider the sample space in Figure 1.23 with outcomes
a, b, c, d, e, and f . If P(A) = 0.27, calculate:
(a) P(b)
(b) P(A′)
(c) P(d)
A
S
a
0.09
c
0.11
d
f
0.29
b
?
?
e
0.06
FIGURE 1.23
1.2.3 If birthdays are equally likely to fall on any day, what is
the probability that a person chosen at random has a
birthday in January? What about February?
1.2.4 When a company introduces initiatives to reduce its carbon
footprint, its costs will either increase, stay the same, or de-
crease. Suppose that the probability that the costs increase
is 0.03, and the probability that the costs stay the same
is 0.18. What is the probability that costs will decrease?
What is the probability that costs will not increase?
1.2.5 An investor is monitoring stocks from Company A and
Company B, which each either increase or decrease each
day. On a given day, suppose that there is a probability
of 0.38 that both stocks will increase in price, and a
probability of 0.11 that both stocks will decrease in price.
Also, there is a probability of 0.16 that the stock from
Company A will decrease while the stock from Company
B will increase. What is the probability that the stock from
Company A will increase while the stock from Company
B will decrease? What is the probability that at least
one company will have an increase in the stock price?
1.2.6 Two fair dice are thrown, one red and one blue. What is the
probability that the red die has a score that is strictly greater
than the score of the blue die? Why is this probability
less than 0.5? What is the complement of this event?
1.2.7 If a card is chosen at random from a pack of cards, what
is the probability that the card is from one of the two
black suits?
1.2.8 If a card is chosen at random from a pack of cards, what
is the probability that it is an ace?
1.2.9 A winner and a runner-up are decided in a tournament of
four players, one of whom is Terica. If all the outcomes
are equally likely, what is the probability that
(a) Terica is the winner?
(b) Terica is either the winner or the runner-up?
1.2.10 Three types of batteries are being tested, type I, type II, and
type III. The outcome (I, II, III) denotes that the battery of
type I fails ﬁrst, the battery of type II next, and the battery
of type III lasts the longest. The probabilities of the six out-
comes are given in Figure 1.24. What is the probability that
(a) the type I battery lasts longest?
(b) the type I battery lasts shortest?
(c) the type I battery does not last longest?
(d) the type I battery lasts longer than the type II battery?
(This problem is continued in Problem 1.4.9.)
S
(I, II, III)
(I, III, II)
0.11
0.07
(II, I, III)
(II, III, I)
0.24
0.39
(III, I, II)
(III, II, I)
0.16
0.03
FIGURE 1.24
Probability values for battery lifetimes

1.3 COMBINATIONS OF EVENTS
15
S
(S, S)
(S, P)
(S, F)
0.02
0.06
0.05
(P, S)
(P, P)
(P, F)
0.07
0.14
0.20
(F, S)
(F, P)
(F, F)
0.06
0.21
0.19
FIGURE 1.25
Probability values for assembly line operations
1.2.11 A factory has two assembly lines, each of which is shut
down (S), at partial capacity (P), or at full capacity (F).
The sample space is given in Figure 1.25, where, for
example, (S, P) denotes that the ﬁrst assembly line is
shut down and the second one is operating at partial
capacity. What is the probability that
(a) both assembly lines are shut down?
(b) neither assembly line is shut down?
(c) at least one assembly line is at full capacity?
(d) exactly one assembly line is at full capacity?
What is the complement of the event in part (b)? What is
the complement of the event in part (c)?
(This problem is continued in Problem 1.4.10.)
1.2.12 A fair coin is tossed three times. What is the probability
that two heads will be obtained in succession?
1.2.13 A company’s revenue is considerably below expectation
with probability 0.08, is slightly below expectation with
probability 0.19, exactly meets expectation with
probability 0.26, is slightly above expectation with
probability 0.36, and is considerably above expectation
with probability 0.11. What is the probability that the
company’s revenue is not below expectation?
1.2.14 An advertising campaign is canceled before launch with
probability 0.10, is launched but canceled early with
probability 0.18, is launched and runs its targeted length
with probability 0.43, and is launched and is extended
beyond its targeted length with probability 0.29. What is
the probability that the advertising campaign is launched?
1.3
Combinations of Events
In general, more than one event will be of interest for a particular experiment and sample space.
For two events A and B, in addition to the consideration of the probability of event A occurring
and the probability of event B occurring, it is often important to consider other probabilities
such as the probability of both events occurring simultaneously. Other quantities of interest
may be the probability that neither event A nor event B occurs, the probability that at least
one of the two events occurs, or the probability that event A occurs, but event B does not.
1.3.1
Intersections of Events
Consider ﬁrst the calculation of the probability that both events occur simultaneously. This can
be done by deﬁning a new event to consist of the outcomes that are in both event A and event B.
Intersections of Events
The event A ∩B is the intersection of the events A and B and consists of the
outcomes that are contained within both events A and B. The probability of this event,
P(A ∩B), is the probability that both events A and B occur simultaneously.
Figure 1.26 shows a sample space S that consists of nine outcomes. Event A consists of
three outcomes, and its probability is given by
P(A) = 0.01 + 0.07 + 0.19 = 0.27
Event B consists of ﬁve outcomes, and its probability is given by
P(B) = 0.07 + 0.19 + 0.04 + 0.14 + 0.12 = 0.56

16
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.26
Events A and B
S
0.18
0.22
A
B
0.04
0.19
0.14
0.01
0.12
0.07
0.03
FIGURE 1.27
The event A ∩B
S
0.18
0.22
A
B
0.04
0.19
0.14
0.01
0.12
0.07
0.03
The intersection of these two events, shown in Figure 1.27, consists of the two outcomes that
are contained within both events A and B. It has a probability of
P(A ∩B) = 0.07 + 0.19 = 0.26
which is the probability that both events A and B occur simultaneously.
Event A′, the complement of the event A, is the event consisting of the six outcomes that
are not in event A. Notice that there are obviously no outcomes in A ∩A′, and this is written
A ∩A′ = ∅
where ∅is referred to as the “empty set,” a set that does not contain anything. Consequently,
P(A ∩A′) = P(∅) = 0
and it is impossible for the event A to occur at the same time as its complement.
A more interesting event is the event A′ ∩B illustrated in Figure 1.28. This event consists
of the three outcomes that are contained within event B but that are not contained within
event A. It has a probability of
P(A′ ∩B) = 0.04 + 0.14 + 0.12 = 0.30
which is the probability that event B occurs but event A does not occur. Similarly, Figure 1.29
shows the event A ∩B′, which has a probability of
P(A ∩B′) = 0.01
This is the probability that event A occurs but event B does not.

1.3 COMBINATIONS OF EVENTS
17
FIGURE 1.28
The event A′ ∩B
S
0.18
0.22
A
B
0.04
0.19
0.14
0.01
0.12
0.07
0.03
FIGURE 1.29
The event A ∩B′
S
0.18
0.22
A
B
0.04
0.14
0.01
0.12
0.03
0.19
0.07
Notice that
P(A ∩B) + P(A ∩B′) = 0.26 + 0.01 = 0.27 = P(A)
and similarly that
P(A ∩B) + P(A′ ∩B) = 0.26 + 0.30 = 0.56 = P(B)
The following two equalities hold in general for all events A and B:
P(A ∩B) + P(A ∩B′) = P(A)
P(A ∩B) + P(A′ ∩B) = P(B)
Two events A and B that have no outcomes in common are said to be mutually exclusive
events. In this case A ∩B = ∅and P(A ∩B) = 0.
Mutually Exclusive Events
Two events A and B are said to be mutually exclusive if A ∩B = ∅so that they have
no outcomes in common.
Figure 1.30 illustrates a sample space S that consists of seven outcomes, three of which are
contained within event A and two of which are contained within event B. Since no outcomes
are contained within both events A and B, the two events are mutually exclusive.

18
CHAPTER 1
PROBABILITY THEORY
S
B
A
FIGURE 1.30
A and B are mutually exclusive events
S
A
B
FIGURE 1.31
A ⊂B
Finally, Figure 1.31 illustrates a situation where an event A is contained within an event B,
that is, A ⊂B. Each outcome in event A is also contained in event B. It is clear that in this
case A ∩B = A.
Some other simple results concerning the intersections of events are as follows:
A ∩B = B ∩A
A ∩A = A
A ∩S = A
A ∩∅= ∅
A ∩A′ = ∅
A ∩(B ∩C) = (A ∩B) ∩C
1.3.2
Unions of Events
The event that at least one out of two events A and B occurs, shown in Figure 1.32, is denoted
by A ∪B and is referred to as the union of events A and B. The probability of this event,
P(A ∪B), is the sum of the probability values of the outcomes that are in either of events
A or B (including those events that are in both events A and B).
Unions of Events
The event A ∪B is the union of events A and B and consists of the outcomes that are
contained within at least one of the events A and B. The probability of this event,
P(A ∪B), is the probability that at least one of the events A and B occurs.
Notice that the outcomes in the event A ∪B can be classiﬁed into three kinds. They are
1. in event A, but not in event B
2. in event B, but not in event A
3. in both events A and B

1.3 COMBINATIONS OF EVENTS
19
A
S
B
FIGURE 1.32
The event A ∪B
S
B
A
FIGURE 1.33
Decomposition of the event A ∪B
The outcomes of type 1 form the event A ∩B′, the outcomes of type 2 form the event A′ ∩B,
and the outcomes of type 3 form the event A∩B, as shown in Figure 1.33. Since the probability
of A ∪B is obtained as the sum of the probability values of the outcomes within these three
(mutually exclusive) events, the following result is obtained:
P(A ∪B) = P(A ∩B′) + P(A′ ∩B) + P(A ∩B)
This equality can be presented in another form using the relationships
P(A ∩B′) = P(A) −P(A ∩B)
and
P(A′ ∩B) = P(B) −P(A ∩B)
Substituting in these expressions for P(A ∩B′) and P(A′ ∩B) gives the following result:
P(A ∪B) = P(A) + P(B) −P(A ∩B)
This equality has the intuitive interpretation that the probability of at least one of the events A
and B occurring can be obtained by adding the probabilities of the two events A and B and
then subtracting the probability that both the events occur simultaneously. The probability
that both events occur, P(A ∩B), needs to be subtracted because the probability values of the
outcomes in the intersection A ∩B have been counted twice, once in P(A) and once in P(B).
Notice that if events A and B are mutually exclusive, so that no outcomes are in A ∩B
and P(A ∩B) = 0 as in Figure 1.30, then P(A ∪B) can just be obtained as the sum of the
probabilities of events A and B.
If the events A and B are mutually exclusive so that P(A ∩B) = 0, then
P(A ∪B) = P(A) + P(B)

20
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.34
The event A ∪B
S
0.18
0.22
A
B
0.04
0.14
0.01
0.12
0.03
0.19
0.07
FIGURE 1.35
The event A′ ∪B′
S
0.18
0.22
A
B
0.04
0.14
0.01
0.12
0.03
0.19
0.07
The sample space of nine outcomes illustrated in Figure 1.26 can be used to demonstrate
some general relationships between unions and intersections of events. For this example, the
event A∪B consists of the six outcomes illustrated in Figure 1.34, and it has a probability of
P(A ∪B) = 0.01 + 0.07 + 0.19 + 0.04 + 0.14 + 0.12 = 0.57
The event (A ∪B)′, which is the complement of the union of the events A and B, consists of
the three outcomes that are neither in event A nor in event B. It has a probability of
P((A ∪B)′) = 0.03 + 0.22 + 0.18 = 0.43 = 1 −P(A ∪B)
Notice that the event (A∪B)′ can also be written as A′ ∩B′ since it consists of those outcomes
that are simultaneously neither in event A nor in event B. This is a general result:
(A ∪B)′ = A′ ∩B′
Furthermore, the event A′ ∪B′ consists of the seven outcomes illustrated in Figure 1.35,
and it has a probability of
P(A′ ∪B′) = 0.01 + 0.03 + 0.22 + 0.18 + 0.12 + 0.14 + 0.04 = 0.74
However, this event can also be written as (A ∩B)′ since it consists of the outcomes that are
in the complement of the intersection of sets A and B. Hence, its probability could have been

1.3 COMBINATIONS OF EVENTS
21
calculated by
P(A′ ∪B′) = P((A ∩B)′) = 1 −P(A ∩B) = 1 −0.26 = 0.74
Again, this is a general result:
(A ∩B)′ = A′ ∪B′
Finally, if event A is contained within event B, A ⊂B, as shown in Figure 1.31, then
clearly A ∪B = B.
Some other simple results concerning the unions of events are as follows:
A ∪B = B ∪A
A ∪A = A
A ∪S = S
A ∪∅= A
A ∪A′ = S
A ∪(B ∪C) = (A ∪B) ∪C
1.3.3
Examples of Intersections and Unions
Example 4
Power Plant Operation
Consider again Figures 1.15, 1.16, and 1.17, and recall that event A, the event that plant X
is idle, has a probability of 0.32, and that event B, the event that at least two out of the three
plants are generating electricity, has a probability of 0.70.
The event A ∩B consists of the outcomes for which plant X is idle and at least two out
of the three plants are generating electricity. Clearly, the only outcome of this kind is the one
where plant X is idle and both plants Y and Z are generating electricity, so that
A ∩B = {(0, 1, 1)}
as illustrated in Figure 1.36. Consequently,
P(A ∩B) = P((0, 1, 1)) = 0.18
The event A ∪B consists of outcomes where either plant X is idle or at least two plants
are generating electricity (or both). Seven out of the eight outcomes satisfy this condition, so
that
A ∪B = {(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0), (1, 1, 1)}
as illustrated in Figure 1.37. The probability of the event A ∪B is thus
P(A ∪B) = P((0, 0, 0)) + P((0, 0, 1)) + P((0, 1, 0)) + P((0, 1, 1))
+ P((1, 0, 1)) + P((1, 1, 0)) + P((1, 1, 1))
= 0.07 + 0.04 + 0.03 + 0.18 + 0.18 + 0.21 + 0.13 = 0.84
Another way of calculating this probability is
P(A ∪B) = P(A) + P(B) −P(A ∩B)
= 0.32 + 0.70 −0.18 = 0.84

22
CHAPTER 1
PROBABILITY THEORY
B
(1, 1, 1)
0.13
(1, 1, 0)
0.21
(1, 0, 1)
0.18
(1, 0, 0)
0.16
(0, 1, 1)
0.18
(0, 1, 0)
0.03
(0, 0, 1)
0.04
(0, 0, 0)
0.07
S
A
FIGURE 1.36
The event A ∩B
B
(1, 1, 1)
0.13
(1, 1, 0)
0.21
(1, 0, 1)
0.18
(1, 0, 0)
0.16
(0, 1, 0)
0.03
(0, 0, 1)
0.04
(0, 0, 0)
0.07
S
A
(0, 1, 1)
0.18
FIGURE 1.37
The event A ∪B
Still another way is to notice that the complement of the event A ∪B consists of the single
outcome (1, 0, 0), which has a probability value of 0.16, so that
P(A ∪B) = 1 −P((A ∪B)′) = 1 −P((1, 0, 0)) = 1 −0.16 = 0.84
Example 5
Television Set Quality
A company that manufactures television sets performs a ﬁnal quality check on each appliance
before packing and shipping it. The quality check has two components, the ﬁrst being an
evaluation of the quality of the picture obtained on the television set, and the second being
an evaluation of the appearance of the television set, which looks for scratches or other
visible deformities on the appliance. Each of the two evaluations is graded as Perfect, Good,
Satisfactory, or Fail. The 16 outcomes are illustrated in Figure 1.38 together with a set of
probability values, where the notation (P, G), for example, means that an appliance has a
Perfect picture and a Good appearance.
The company has decided that an appliance that fails on either of the two evaluations will
not be shipped. Furthermore, as an additional conservative measure to safeguard its reputation,
it has decided that appliances that score an evaluation of Satisfactory on both accounts will
also not be shipped.
An initial question of interest concerns the probability that an appliance cannot be shipped.
This event A, say, consists of the outcomes
A = {(F, P), (F, G), (F, S), (F, F), (P, F), (G, F), (S, F), (S, S)}
as illustrated in Figure 1.39. The probability that an appliance cannot be shipped is then
P(A) = P((F, P)) + P((F, G)) + P((F, S)) + P((F, F)) + P((P, F))
+ P((G, F)) + P((S, F)) + P((S, S))
= 0.004 + 0.011 + 0.009 + 0.008 + 0.007 + 0.012 + 0.010 + 0.013
= 0.074
In the long run about 7.4% of the television sets will fail the quality check.
From a technical point of view, the company is also interested in the probability that
an appliance has a picture that is graded as either Satisfactory or Fail. This event B, say, is

1.3 COMBINATIONS OF EVENTS
23
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
FIGURE 1.38
Probability values for television set example
A
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
FIGURE 1.39
Event A: appliance not shipped
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
B
FIGURE 1.40
Event B: picture Satisfactory or Fail
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
A
B
FIGURE 1.41
Event A ∩B
illustrated in Figure 1.40, and it has a probability of
P(B) = P((F, P)) + P((F, G)) + P((F, S)) + P((F, F)) + P((S, P))
+ P((S, G)) + P((S, S)) + P((S, F))
= 0.004 + 0.011 + 0.009 + 0.008 + 0.067 + 0.056 + 0.013 + 0.010
= 0.178
The event A ∩B consists of outcomes where the appliance is not shipped and the picture
is evaluated as being either Satisfactory or Fail. It contains the six outcomes illustrated in
Figure 1.41, and it has a probability of
P(A ∩B) = P((F, P)) + P((F, G)) + P((F, S)) + P((F, F))
+ P((S, S)) + P((S, F))
= 0.004 + 0.011 + 0.009 + 0.008 + 0.013 + 0.010 = 0.055

24
CHAPTER 1
PROBABILITY THEORY
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
A
B
FIGURE 1.42
Event A ∪B
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
A
B
FIGURE 1.43
Event A ∩B′
The event A ∪B consists of outcomes where the appliance was either not shipped or
the picture was evaluated as being either Satisfactory or Fail. It contains the ten outcomes
illustrated in Figure 1.42, and its probability can be obtained either by summing the individual
probability values of these ten outcomes or more simply as
P(A ∪B) = P(A) + P(B) −P(A ∩B)
= 0.074 + 0.178 −0.055 = 0.197
Television sets that have a picture evaluation of either Perfect or Good but that cannot be
shipped constitute the event A ∩B′. This event is illustrated in Figure 1.43 and consists of the
outcomes
A ∩B′ = {(P, F), (G, F)}
It has a probability of
P(A ∩B′) = P((P, F)) + P((G, F)) = 0.007 + 0.012 = 0.019
Notice that
P(A ∩B) + P(A ∩B′) = 0.055 + 0.019 = 0.074 = P(A)
as expected.
GAMES OF CHANCE
The event A that an even score is obtained from a roll of a die is
A = {2, 4, 6}
If the event B, a high score, is deﬁned to be
B = {4, 5, 6}

1.3 COMBINATIONS OF EVENTS
25
then
A ∩B = {4, 6}
and
A ∪B = {2, 4, 5, 6}
If a fair die is used, then P(A ∩B) = 2/6 = 1/3, and P(A ∪B) = 4/6 = 2/3.
If two dice are thrown, recall that Figure 1.18 illustrates the event A, that the sum of the
scores is equal to 6, and Figure 1.19 illustrates the event B, that at least one of the two dice
records a 6. If all the outcomes are equally likely with a probability of 1/36, then P(A) = 5/36
and P(B) = 11/36. Since there are no outcomes in both events A and B,
A ∩B = ∅
and P(A ∩B) = 0. Consequently, the events A and B are mutually exclusive.
The event A ∪B consists of the ﬁve outcomes in event A together with the 11 outcomes
in event B, and its probability is
P(A ∪B) = 16
36 = 4
9 = P(A) + P(B)
If one die is red and the other is blue, then Figure 1.44 illustrates the event C, say, that an
even score is obtained on the red die, and Figure 1.45 illustrates the event D, say, that an even
score is obtained on the blue die. Figure 1.46 then illustrates the event C ∩D, which is the
event that both dice have even scores. If all outcomes are equally likely, then this event has a
probability of 9/36 = 1/4. Figure 1.47 illustrates the event C ∪D, the event that at least one
die has an even score. This event has a probability of 27/36 = 3/4. Notice that (C ∪D)′, the
complement of the event C ∪D, is just the event that both dice have odd scores.
Recall that Figure 1.20 illustrates the event A, that a card drawn from a pack of cards
belongs to the heart suit, and Figure 1.21 illustrates the event B, that a picture card is drawn.
FIGURE 1.44
Event C: even score on red die
C
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
(5, 4)
(5, 5)
(5, 2)
(4, 3)

26
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.45
Event D: even score on blue die
D
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
(5, 5)
(5, 4)
(5, 2)
(4, 3)
FIGURE 1.46
Event C ∩D
D
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
C
(5, 5)
(5, 4)
(5, 2)
(4, 3)
If all outcomes are equally likely, then P(A) = 13/52 = 1/4, and P(B) = 12/52 = 3/13.
Figure 1.48 then illustrates the event A ∩B, which is the event that a picture card from the
heart suit is drawn. This has a probability of 3/52. Figure 1.49 illustrates the event A ∪B,
the event that either a heart or a picture card (or both) is drawn, which has a probability of
22/52 = 11/26. Notice that, as expected,
P(A) + P(B) −P(A ∩B) = 13
52 + 12
52 −3
52 = 22
52 = P(A ∪B)

1.3 COMBINATIONS OF EVENTS
27
FIGURE 1.47
Event C ∪D
D
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
C
(5, 5)
(5, 4)
(5, 2)
(4, 3)
FIGURE 1.48
Event A ∩B
A
B
S
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥
FIGURE 1.49
Event A ∪B
A
B
S
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥

28
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.50
Event A′ ∩B
A
B
S
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥
FIGURE 1.51
Three events decompose the
sample space into eight regions
A
B
C
S
1
2
3
4
5
6
7
8
Finally, Figure 1.50 illustrates the event A′ ∩B, which is the event that a picture card from
a suit other than the heart suit is drawn. It has a probability of 9/52. Again, notice that
P(A ∩B) + P(A′ ∩B) = 3
52 + 9
52 = 12
52 = P(B)
as expected.
1.3.4
Combinations of Three or More Events
Intersections and unions can be extended in an obvious manner to three or more events.
Figure 1.51 illustrates how three events A, B, and C can divide a sample space into eight
distinct and separate regions. The event A, for example, is composed of the regions 2, 3, 5,
and 6, and the event A ∩B is composed of the regions 3 and 6.
The event A ∩B ∩C, the intersection of the events A, B, and C, consists of the outcomes
that are simultaneously contained within all three events A, B, and C. In Figure 1.51 it
corresponds to region 6. The event A ∪B ∪C, the union of the events A, B, and C, consists
of the outcomes that are in at least one of the three events A, B, and C. In Figure 1.51 it
corresponds to all of the regions except for region 1. Hence region 1 can be referred to as
(A ∪B ∪C)′ since it is the complement of the event A ∪B ∪C.

1.3 COMBINATIONS OF EVENTS
29
In general, care must be taken to avoid ambiguities when specifying combinations of three
or more events. For example, the expression
A ∪B ∩C
is ambiguous since the two events
A ∪(B ∩C)
and
(A ∪B) ∩C
are different. In Figure 1.51 the event B ∩C is composed of regions 6 and 7, so A ∪(B ∩C)
is composed of regions 2, 3, 5, 6, and 7. In contrast, the event A ∪B is composed of regions
2, 3, 4, 5, 6, and 7, so (A ∪B) ∩C is composed of just regions 5, 6, and 7.
Figure 1.51 can also be used to justify the following general expression for the probability
of the union of three events:
Union of Three Events
The probability of the union of three events A, B, and C is the sum of the probability
values of the simple outcomes that are contained within at least one of the three
events. It can also be calculated from the expression
P(A ∪B ∪C) = [P(A) + P(B) + P(C)]
−[P(A ∩B) + P(A ∩C) + P(B ∩C)] + P(A ∩B ∩C)
The expression for P(A ∪B ∪C) can be checked by matching up the regions in Figure 1.51
with the various terms in the expression. The required probability, P(A ∪B ∪C), is the sum
of the probability values of the outcomes in regions 2, 3, 4, 5, 6, 7, and 8. However, the sum
of the probabilities P(A), P(B), and P(C) counts regions 3, 5, and 7 twice, and region 6
three times. Subtracting the probabilities P(A ∩B), P(A ∩C), and P(B ∩C) removes the
double counting of regions 3, 5, and 7 but also subtracts the probability of region 6 three times.
The expression is then completed by adding back on P(A∩B∩C), the probability of region 6.
Figure 1.52 illustrates three events A, B, and C that are mutually exclusive because no
two events have any outcomes in common. In this case,
P(A ∪B ∪C) = P(A) + P(B) + P(C)
because the event intersections all have probabilities of zero. More generally, for a sequence
A1, A2, . . . , An of mutually exclusive events where no two of the events have any outcomes
in common, the probability of the union of the events can be obtained by summing the
probabilities of the individual events.
FIGURE 1.52
Three mutually exclusive events
A
B
C
S

30
CHAPTER 1
PROBABILITY THEORY
Union of Mutually Exclusive Events
For a sequence A1, A2, . . . , An of mutually exclusive events, the probability of the
union of the events is given by
P(A1 ∪· · · ∪An) = P(A1) + · · · + P(An)
If a sequence A1, A2, . . . , An of mutually exclusive events has the additional property
that their union consists of the whole sample space S, then they are said to be an exhaustive
sequence. They are also said to provide a partition of the sample space.
Sample Space Partitions
A partition of a sample space is a sequence A1, A2, . . . , An of mutually exclusive
events for which
A1 ∪· · · ∪An = S
Each outcome in the sample space is then contained within one and only one of the
events Ai.
Figure 1.53 illustrates a partition of a sample space S into eight mutually exclusive events.
Example 5
Television Set Quality
In addition to the events A and B discussed before, consider also the event C that an appliance
is of “mediocre quality.” The event is deﬁned to be appliances that score either Satisfactory
or Good on each of the two evaluations, so that
C = {(S, S), (S, G), (G, S), (G, G)}
The three events A, B, and C are illustrated in Figure 1.54.
Notice that
A ∩C = {(S, S)}
and
B ∩C = {(S, S), (S, G)}
Also, the intersection of the three events is
A ∩B ∩C = {(S, S)}
The fact that the events A ∩C and A ∩B ∩C are identical, both consisting of the single
outcome (S, S), is a consequence of the fact that A ∩B′ ∩C = ∅. There are no outcomes that
FIGURE 1.53
A partition of the sample space
A
S
8
A1
A2
A5
A4
A6
A7
A3

1.3 COMBINATIONS OF EVENTS
31
FIGURE 1.54
Events A, B, and C
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
A
B
C
are not shipped (in event A), have a picture rating of Good or Perfect (in event B′), and are of
mediocre quality (in event C).
The company may be particularly interested in the event D, that an appliance is of “high
quality,” deﬁned to be the complement of the union of the events A, B, and C:
D = (A ∪B ∪C)′
Notice that this event can also be written as
D = A′ ∩B′ ∩C′
since it consists of the outcomes that are shipped (in event A′), have a picture rating of Good
or Perfect (in event B′), and are not of mediocre quality (in event C′). Speciﬁcally, the event D
consists of the outcomes
D = {(G, P), (P, P), (P, G), (P, S)}
and it has a probability of
P(D) = P((G, P)) + P((P, P)) + P((P, G)) + P((P, S))
= 0.124 + 0.140 + 0.102 + 0.157 = 0.523
1.3.5
Problems
1.3.1 Consider the sample space S = {0, 1, 2} and the event
A = {0}. Explain why A ̸= ∅.
1.3.2 Consider the sample space and events in Figure 1.55.
Calculate the probabilities of the events:
(a) B
(b) B ∩C
(c) A ∪C
(d) A ∩B ∩C
(e) A ∪B ∪C
(f) A′ ∩B
(g) B′ ∪C
(h) A ∪(B ∩C)
(i) (A ∪B) ∩C
(j) (A′ ∪C)′
(This problem is continued in Problem 1.4.1.)
1.3.3 Use Venn diagrams to illustrate the equations:
(a) A ∪(B ∩C) = (A ∪B) ∩(A ∪C)
(b) A ∩(B ∪C) = (A ∩B) ∪(A ∩C)
(c) (A ∩B ∩C)′ = A′ ∪B′ ∪C′
1.3.4 Let A be the event that a person is female, let B be the
event that a person has black hair, and let C be the event
that a person has brown eyes. Describe the kinds of
people in the following events:
(a) A ∩B
(b) A ∪C′
(c) A′ ∩B ∩C
(d) A ∩(B ∪C)
1.3.5 A card is chosen from a pack of cards. Are the events that
a card from one of the two red suits is chosen and that a
card from one of the two black suits is chosen mutually
exclusive? What about the events that an ace is chosen
and that a heart is chosen?
1.3.6 If P(A) = 0.4 and P(A ∩B) = 0.3, what are the
possible values for P(B)?

32
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.55
A
B
C
0.04
0.07
0.05
0.08
0.04
0.01
0.08
0.06
0.13
0.11
0.11
0.07
0.05
0.03
0.02
0.05
S
1.3.7 If P(A) = 0.5, P(A ∩B) = 0.1, and P(A ∪B) = 0.8,
what is P(B)?
1.3.8 An evaluation of a small business by an accounting ﬁrm
either reveals a problem with the accounts or it doesn’t
reveal a problem. Also, the evaluation is either done
accurately or incorrectly. The probability that the
evaluation is done accurately is 0.85. Furthermore, the
probability that the evaluation is done incorrectly and that
it reveals a problem is 0.10. If the probability that the
evaluation is done accurately and it does not reveal a
problem is 0.25, what is the probability that the
evaluation does not reveal a problem?
A. 0.10
B. 0.20
C. 0.30
D. 0.40
1.3.9 A card is drawn at random from a pack of cards. A is the
event that a heart is obtained, B is the event that a club is
obtained, and C is the event that a diamond is obtained.
Are these three events mutually exclusive? What is
P(A ∪B ∪C)? Explain why B ⊂A′.
1.3.10 A card is drawn from a pack of cards. A is the event that
an ace is obtained, B is the event that a card from one of
the two red suits is obtained, and C is the event that a
picture card is obtained. What cards do the following
events consist of?
(a) A ∩B
(b) A ∪C
(c) B ∩C′
(d) A ∪(B′ ∩C)
1.3.11 A car repair can be performed either on time or late and
either satisfactorily or unsatisfactorily. The probability
of a repair being on time and satisfactory is 0.26. The
probability of a repair being on time is 0.74. The
probability of a repair being satisfactory is 0.41. What is
the probability of a repair being late and unsatisfactory?
1.3.12 A bag contains 200 balls that are either red or blue and
either dull or shiny. There are 55 shiny red balls, 91 shiny
balls, and 79 red balls. If a ball is chosen at random,
what is the probability that it is either a shiny ball or a
red ball? What is the probability that it is a dull blue ball?
1.3.13 In a study of patients arriving at a hospital emergency
room, the gender of the patients is considered, together
with whether the patients are younger or older than
30 years of age, and whether or not the patients are
admitted to the hospital. It is found that 45% of the
patients are male, 30% of the patients are younger than
30 years of age, 15% of the patients are females older
than 30 years of age who are admitted to the hospital, and
21% of the patients are females younger than 30 years of
age. What proportion of the patients are females older
than 30 years of age who are not admitted to the hospital?
1.3.14 Recall that a company’s revenue is considerably
below expectation with probability 0.08, is slightly
below expectation with probability 0.19, exactly meets
expectation with probability 0.26, is slightly above
expectation with probability 0.36, and is considerably
above expectation with probability 0.11. Let A be the
event that the revenue is not below expectation. Let B be
the event that the revenue is not above expectation. What
is the probability of the intersection of these two events?
What is the probability of the union of these two events?
1.3.15 Recall that an advertising campaign is canceled before
launch with probability 0.10, is launched but canceled
early with probability 0.18, is launched and runs its
targeted length with probability 0.43, and is launched and

1.4 CONDITIONAL PROBABILITY
33
is extended beyond its targeted length with probability
0.29. Let A be the event that the advertising campaign is
launched. Let B be the event that the advertising campaign
is launched and runs at least as long as targeted. What
is the probability of the intersection of these two events?
What is the probability of the union of these two events?
1.4
Conditional Probability
1.4.1
Deﬁnition of Conditional Probability
For experiments with two or more events of interest, attention is often directed not only at the
probabilities of individual events, but also at the probability of an event occurring conditional
on the knowledge that another event has occurred. Probabilities such as these are important and
very useful since they provide appropriate revisions of a set of probabilities once a particular
event is known to have occurred.
The probability that event A occurs conditional on event B having occurred is written
P(A|B)
Its interpretation is that if the outcome occurring is known to be contained within the event B,
then this conditional probability measures the probability that the outcome is also contained
within the event A. Conditional probabilities can easily be obtained using the following
formula:
Conditional Probability
The conditional probability of event A conditional on event B is
P(A|B) = P(A ∩B)
P(B)
for P(B) > 0. It measures the probability that event A occurs when it is known that
event B occurs.
One simple example of conditional probability concerns the situation in which two events
A and B are mutually exclusive. Since in this case events A and B have no outcomes in com-
mon, it is clear that the occurrence of event B precludes the possibility of event A occurring, so
that intuitively, the probability of event A conditional on event B must be zero. Since A∩B = ∅
for mutually exclusive events, this intuitive reasoning is in agreement with the formula
P(A|B) = P(A ∩B)
P(B)
=
0
P(B) = 0
Another simple example of conditional probability concerns the situation in which an
event B is contained within an event A, that is B ⊂A. Then if event B occurs, it is clear that
event A must also occur, so that intuitively, the probability of event A conditional on event B
must be one. Again, since A ∩B = B here, this intuitive reasoning is in agreement with the
formula
P(A|B) = P(A ∩B)
P(B)
= P(B)
P(B) = 1
For a less obvious example of conditional probability, consider again Figure 1.26 and the
events A and B shown there. Suppose that event B is known to occur. In other words, suppose
that it is known that the outcome occurring is one of the ﬁve outcomes contained within the
event B. What then is the conditional probability of event A occurring?

34
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.56
P(A|B) = P(A ∩B)/P(B)
S
0.18
0.22
A
B
0.04
0.14
0.01
0.12
0.03
0.07
0.19
Since two of the ﬁve outcomes in event B are also in event A (that is, there are two outcomes
in A ∩B), the conditional probability is the probability that one of these two outcomes occurs
rather than one of the other three outcomes (which are in A′ ∩B). As Figure 1.56 shows, the
conditional probability is calculated to be
P(A|B) = P(A ∩B)
P(B)
= 0.26
0.56 = 0.464
Notice that this conditional probability is different from P(A) = 0.27. If the event B is known
not to occur, then the conditional probability of event A is
P(A|B′) = P(A ∩B′)
P(B′)
= P(A) −P(A ∩B)
1 −P(B)
= 0.27 −0.26
1 −0.56
= 0.023
In the same way that P(A) + P(A′) = 1, it is also true that
P(A|B) + P(A′|B) = 1
This is reasonable because if event B occurs, it is still the case that either event A occurs or
it does not, and so the two conditional probabilities should sum to one. Formally, this result
can be shown by noting that
P(A|B) + P(A′|B) = P(A ∩B)
P(B)
+ P(A′ ∩B)
P(B)
=
1
P(B)(P(A ∩B) + P(A′ ∩B)) =
1
P(B) P(B) = 1
However, there is no general relationship between P(A|B) and P(A|B′).
S
0 defectives
0.02
1 defective
0.11
2 defectives
0.16
3 defectives
0.21
4 defectives
0.13
5 defectives
0.08
correct
6 defectives
?
...
500 defectives
?
FIGURE 1.57
Sample space for computer chips
example
Finally, the event conditioned on can be represented as a combination of events. For
example,
P(A|B ∪C)
represents the probability of event A conditional on the event B ∪C, that is conditional on
either event B or C occurring. It can be calculated from the formula
P(A|B ∪C) = P(A ∩(B ∪C))
P(B ∪C)
1.4.2
Examples of Conditional Probabilities
Example 2
Defective Computer
Chips
Consider Figure 1.57 that illustrates the sample space for the number of defective chips in a
box of 500 chips, and recall that the event correct, with a probability of P(correct) = 0.71,
consists of the six outcomes corresponding to no more than ﬁve defectives.

1.4 CONDITIONAL PROBABILITY
35
The probability that a box has no defective chips is
P(0 defectives) = 0.02
so that if a box is chosen at random, it has a probability of only 0.02 of containing no
defective chips. If the company guarantees that a box has no more than ﬁve defective chips,
then customers can be classiﬁed as either satisﬁed or unsatisﬁed depending on whether the
guarantee is met. Clearly, an unsatisﬁed customer did not purchase a box containing no
defective chips. However, it is interesting to calculate the probability that a satisﬁed customer
purchased a box that contained no defective chips. Intuitively, this conditional probability
should be larger than the unconditional probability 0.02.
The required probability is the probability of no defectives conditional on there being no
more than ﬁve defectives, which is calculated to be
P(0 defectives|correct) = P(0 defectives ∩correct)
P(correct)
= P(0 defectives)
P(correct)
= 0.02
0.71 = 0.028
This conditional probability indicates that whereas 2% of all the boxes contain no defectives,
2.8% of the satisﬁed customers purchased boxes that contained no defectives.
Example 4
Power Plant Operation
The probability that plant X is idle is P(A) = 0.32. However, suppose it is known that at
least two out of the three plants are generating electricity (event B). How does this change the
probability of plant X being idle?
The probability that plant X is idle (event A) conditional on at least two out of the three
plants generating electricity (event B) is
P(A|B) = P(A ∩B)
P(B)
= 0.18
0.70 = 0.257
as shown in Figure 1.58. Therefore, whereas plant X is idle about 32% of the time, it is idle
only about 25.7% of the time when at least two of the plants are generating electricity.
Example 5
Television Set Quality
Recall that the probability that an appliance has a picture graded as either Satisfactory or Fail
is P(B) = 0.178. However, suppose that a technician takes a television set from a pile of sets
that could not be shipped. What is the probability that the appliance taken by the technician
has a picture graded as either Satisfactory or Fail?
The required probability is the probability that an appliance has a picture graded as either
Satisfactory or Fail (event B) conditional on the appliance not being shipped (event A). As
Figure 1.59 shows, this can be calculated to be
P(B|A) = P(A ∩B)
P(A)
= 0.055
0.074 = 0.743
so that whereas only about 17.8% of all the appliances manufactured have a picture graded
as either Satisfactory or Fail, 74.3% of the appliances that cannot be shipped have a picture
graded as either Satisfactory or Fail.

36
CHAPTER 1
PROBABILITY THEORY
B
(1, 1, 1)
0.13
(1, 1, 0)
0.21
(1, 0, 1)
0.18
(1, 0, 0)
0.16
(0, 1, 0)
0.03
(0, 0, 1)
0.04
(0, 0, 0)
0.07
S
A
(0, 1, 1)
0.18
FIGURE 1.58
P(A|B) = P(A ∩B)/P(B)
(P, P)
(G, P)
(F, P)
(P, G)
(P, S)
(F, S)
(G, S)
(S, S)
(F, G)
(S, F)
(F, F)
(P, F)
0.140
0.102
0.157
0.007
0.124
0.139
0.012
0.013
0.010
0.004
0.011
0.009
0.008
(G, F)
S
(G, G)
0.141
(S, P)
(S, G)
0.067
0.056
A
B
FIGURE 1.59
P(B|A) = P(A ∩B)/P(A)
GAMES OF CHANCE
If a fair die is rolled the probability of scoring a 6 is P(6) = 1/6. If somebody rolls a die
without showing you but announces that the result is even, then intuitively the chance that a 6
has been obtained is 1/3 since there are three equally likely even scores, one of which is a 6.
Mathematically, this conditional probability is calculated to be
P(6|even) = P(6 ∩even)
P(even)
=
P(6)
P(even)
=
P(6)
P(2) + P(4) + P(6) =
1/6
1/6 + 1/6 + 1/6 = 1
3
as expected.
If a red die and a blue die are thrown, with each of the 36 outcomes being equally likely,
let A be the event that the red die scores a 6, so that
P(A) = 6
36 = 1
6
Also, let B be the event that at least one 6 is obtained on the two dice (see Figure 1.19) with
a probability of
P(B) = 11
36
Suppose that somebody rolls the two dice without showing you but announces that at least
one 6 has been scored. What then is the probability that the red die scored a 6? As Figure 1.60
shows, this conditional probability is calculated to be
P(A|B) = P(A ∩B)
P(B)
= P(A)
P(B) =
1/6
11/36 = 6
11
As expected, this conditional probability is larger than P(A) = 1/6. It is also slightly larger
than 0.5, which is accounted for by the outcome (6, 6) where both dice score a 6.
Contrast this problem with the situation where the announcement is that exactly one 6 has
been scored, event C, say. In this case, it is intuitively clear that the 6 obtained is equally likely

1.4 CONDITIONAL PROBABILITY
37
FIGURE 1.60
P(A|B) = P(A ∩B)/P(B)
B
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 4)
(5, 5)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
(6, 6)
A
(5, 2)
(4, 3)
FIGURE 1.61
P(A|C) = P(A ∩C)/P(C)
C
S
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(1, 5)
(1, 6)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(2, 5)
(2, 6)
1/36
1/36
1/36
1/36
1/36
1/36
(3, 1) 
(3, 2)
(3, 3)
(3, 4)
(3, 5)
(3, 6)
(4, 1)
(4, 2)
(4, 4)
(4, 5)
(4, 6)
(5, 1)
(5, 3)
(5, 4)
(5, 5)
(5, 6)
(6, 1)
(6, 2)
(6, 3)
(6, 4)
(6, 5)
A
(5, 2)
(4, 3)
1/36
(6, 6)
to have been scored on the red die or the blue die, so that the conditional probability P(A|C)
should be equal to 1/2. As Figure 1.61 shows, this is correct since
P(A|C) = P(A ∩C)
P(C)
= 5/36
10/36 = 1
2
If a card is drawn from a pack of cards, let A be the event that a card from the heart suit is
obtained, and let B be the event that a picture card is drawn. Recall that P(A) = 13/52 = 1/4
and P(B) = 12/52 = 3/13. Also, the event A ∩B, the event that a picture card from the
heart suit is drawn, has a probability of P(A ∩B) = 3/52.

38
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.62
P(C|A) = P(C ∩A)/P(A)
C
S
K♠
Q♠
J♠
A♠
3♠
4♠
5♠
6♠
7♠
8♠
9♠
10♠
2♠
K♦
Q♦
J♦
A♦
3♦
4♦
5♦
6♦
7♦
8♦
9♦
10♦
2♦
K♣
Q♣
J♣
A♣
3♣
4♣
5♣
6♣
7♣
8♣
9♣
10♣
2♣
K♥
Q♥
J♥
A♥
3♥
4♥
5♥
6♥
7♥
8♥
9♥
10♥
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
1/52
2♥
A
Suppose that somebody draws a card and announces that it is from the heart suit. What
then is the probability that it is a picture card? This conditional probability is
P(B|A) = P(A ∩B)
P(A)
= 3/52
1/4 = 3
13
Notice that in this case, P(B|A) = P(B) because the proportion of picture cards in the heart
suit is identical to the proportion of picture cards in the whole pack. The events A and B are
then said to be independent events, which are discussed more fully in Section 1.5.
Finally, let C be the event that the A♥is chosen, with P(C) = 1/52. If it is known that
a card from the heart suit is obtained, then intuitively the conditional probability of the card
being A♥is 1/13 since there are 13 equally likely cards in the heart suit. As Figure 1.62
shows, this is correct because
P(C|A) = P(C ∩A)
P(A)
= P(C)
P(A) = 1/52
1/4 = 1
13
1.4.3
Problems
1.4.1 Consider again Figure 1.55 and calculate the probabilities:
(a) P(A|B)
(b) P(C|A)
(c) P(B|A ∩B)
(d) P(B|A ∪B)
(e) P(A|A ∪B ∪C)
(f) P(A ∩B|A ∪B)
1.4.2 When a company receives an order, there is a probability
of 0.42 that its value is over $1000. If an order is valued at
over $1000, then there is a probability of 0.63 that the
customer will pay with a credit card.
(a) What is the probability that the next three
independent orders will each be valued at over $1000?
(b) What is the probability that the next order will
be valued at over $1000 but will not be paid with a
credit card?
1.4.3 A card is drawn at random from a pack of cards. Calculate:
(a) P(A♥|card from red suit)
(b) P(heart|card from red suit)
(c) P(card from red suit|heart)
(d) P(heart|card from black suit)
(e) P(king|card from red suit)
(f) P(king|red picture card)
1.4.4 If A ⊂B and B′ ̸= ∅, is P(A) larger or smaller than
P(A|B)? Provide some intuitive reasoning for your
answer.
1.4.5 A ball is chosen at random from a bag containing
150 balls that are either red or blue and either dull or
shiny. There are 36 red shiny balls and 54 blue balls.
What is the probability of the chosen ball being
shiny conditional on it being red? What is the probability
of the chosen ball being dull conditional on it being red?
1.4.6 A car repair is either on time or late and either
satisfactory or unsatisfactory. If a repair is made on time,
then there is a probability of 0.85 that it is satisfactory.

1.4 CONDITIONAL PROBABILITY
39
There is a probability of 0.77 that a repair will be made
on time. What is the probability that a repair is made on
time and is satisfactory?
1.4.7 Assess whether the probabilities of the events (i) increase,
decrease, or remain unchanged when they are conditioned
on the events (ii).
(a) (i) It rains tomorrow, (ii) it is raining today.
(b) (i) A lottery winner has black hair, (ii) the lottery
winner has brown eyes.
(c) (i) A lottery winner has black hair, (ii) the lottery
winner owns a red car.
(d) (i) A lottery winner is more than 50 years old, (ii) the
lottery winner is more than 30 years old.
1.4.8 Suppose that births are equally likely to be on any day.
What is the probability that somebody chosen at random
has a birthday on the ﬁrst day of a month? How does this
probability change conditional on the knowledge that the
person’s birthday is in March? In February?
1.4.9 Consider again Figure 1.24 and the battery lifetimes.
Calculate the probabilities:
(a) A type I battery lasts longest conditional on it not
failing ﬁrst
(b) A type I battery lasts longest conditional on a type II
battery failing ﬁrst
(c) A type I battery lasts longest conditional on a type II
battery lasting the longest
(d) A type I battery lasts longest conditional on a type II
battery not failing ﬁrst
1.4.10 Consider again Figure 1.25 and the two assembly lines.
Calculate the probabilities:
(a) Both lines are at full capacity conditional on neither
line being shut down
(b) At least one line is at full capacity conditional on
neither line being shut down
(c) One line is at full capacity conditional on exactly one
line being shut down
(d) Neither line is at full capacity conditional on at least
one line operating at partial capacity
1.4.11 The length, width, and height of a manufactured part are
classiﬁed as being either within or outside speciﬁed
tolerance limits. In a quality inspection 86% of the parts
are found to be within the speciﬁed tolerance limits for
width, but only 80% of the parts are within the speciﬁed
tolerance limits for all three dimensions. However, 2% of
the parts are within the speciﬁed tolerance limits for
width and length but not for height, and 3% of the parts
are within the speciﬁed tolerance limits for width and
height but not for length. Moreover, 92% of the parts are
within the speciﬁed tolerance limits for either width or
height, or both of these dimensions.
(a) If a part is within the speciﬁed tolerance limits for
height, what is the probability that it will also be
within the speciﬁed tolerance limits for width?
(b) If a part is within the speciﬁed tolerance limits for
length and width, what is the probability that it will
be within the speciﬁed tolerance limits for all three
dimensions?
1.4.12 A gene can be either type A or type B, and it can be either
dominant or recessive. If the gene is type B, then there
is a probability of 0.31 that it is dominant. There is also
a probability of 0.22 that a gene is type B and it is
dominant. What is the probability that a gene is of
type A?
1.4.13 A manufactured component has its quality graded on its
performance, appearance, and cost. Each of these three
characteristics is graded as either pass or fail. There is a
probability of 0.40 that a component passes on both
appearance and cost. There is a probability of 0.31 that a
component passes on all three characteristics. There is a
probability of 0.64 that a component passes on
performance. There is a probability of 0.19 that a
component fails on all three characteristics. There is a
probability of 0.06 that a component passes on
appearance but fails on both performance and cost.
(a) What is the probability that a component passes on
cost but fails on both performance and appearance?
(b) If a component passes on both appearance and cost,
what is the probability that it passes on all three
characteristics?
1.4.14 An agricultural research establishment grows vegetables
and grades each one as either good or bad for its taste,
good or bad for its size, and good or bad for its
appearance. Overall 78% of the vegetables have a good
taste. However, only 69% of the vegetables have both a
good taste and a good size. Also, 5% of the vegetables
have both a good taste and a good appearance, but a bad
size. Finally, 84% of the vegetables have either a good
size or a good appearance.
(a) If a vegetable has a good taste, what is the probability
that it also has a good size?
(b) If a vegetable has a bad size and a bad appearance,
what is the probability that it has a good taste?
1.4.15 There is a 4% probability that the plane used for a
commercial ﬂight has technical problems, and this causes
a delay in the ﬂight. If there are no technical problems

40
CHAPTER 1
PROBABILITY THEORY
with the plane, then there is still a 33% probability that
the ﬂight is delayed due to all other reasons. What is the
probability that the ﬂight is delayed?
1.4.16 In a reliability test there is a 42% probability that a
computer chip survives more than 500 temperature
cycles. If a computer chip does not survive more than
500 temperature cycles, then there is a 73% probability
that it was manufactured by company A. What is the
probability that a computer chip is not manufactured by
company A and does not survive more than 500
temperature cycles?
1.4.17 Recall that a company’s revenue is considerably below
expectation with probability 0.08, is slightly below
expectation with probability 0.19, exactly meets
expectation with probability 0.26, is slightly above
expectation with probability 0.36, and is considerably
above expectation with probability 0.11. If revenue is not
below expectation, what is the probability that it exactly
meets expectation?
1.4.18 Recall that an advertising campaign is canceled before
launch with probability 0.10, is launched but canceled
early with probability 0.18, is launched and runs its
targeted length with probability 0.43, and is launched and
is extended beyond its targeted length with probability
0.29. If the advertising campaign is launched, what is the
probability that it runs at least as long as targeted?
1.5
Probabilities of Event Intersections
1.5.1
General Multiplication Law
It follows from the deﬁnition of the conditional probability P(A|B) that the probability of the
intersection of two events A ∩B can be calculated as
P(A ∩B) = P(B) P(A|B)
That is, the probability of events A and B both occurring can be obtained by multiplying the
probability of event B by the probability of event A conditional on event B. It also follows
from the deﬁnition of the conditional probability P(B|A) that
P(A ∩B) = P(A) P(B|A)
so that the probability of events A and B both occurring can also be obtained by multiplying
the probability of event A by the probability of event B conditional on event A. Therefore, it
does not matter which of the two events A or B is conditioned upon.
More generally, since
P(C|A ∩B) = P(A ∩B ∩C)
P(A ∩B)
the probability of the intersection of three events can be calculated as
P(A ∩B ∩C) = P(A ∩B) P(C|A ∩B) = P(A) P(B|A) P(C|A ∩B)
Thus, the probability of all three events occurring can be obtained by multiplying together
the probability of one event, the probability of a second event conditioned on the ﬁrst event,
and the probability of the third event conditioned on the intersection of the ﬁrst and second
events. This formula can be extended in an obvious way to the following multiplication law
for the intersection of a series of events.
Probabilities of Event Intersections
The probability of the intersection of a series of events A1, . . . , An can be calculated
from the expression
P(A1 ∩· · · ∩An) = P(A1) × P(A2|A1) × P(A3|A1 ∩A2) ×
· · · × P(An|A1 ∩· · · ∩An−1)

1.5 PROBABILITIES OF EVENT INTERSECTIONS
41
This expression for the probability of event intersections is particularly useful when the
conditional probabilities P(Ai|A1 ∩· · · ∩Ai−1) are easily obtainable, as illustrated in the
following example.
Suppose that two cards are drawn at random without replacement from a pack of cards.
Let A be the event that the ﬁrst card drawn is from the heart suit, and let B be the event that
the second card drawn is from the heart suit. What is P(A∩B), the probability that both cards
are from the heart suit?
Figure 1.13 shows the sample space for this problem, which consists of 2652 equally likely
outcomes, each with a probability of 1/2652. One way to calculate P(A ∩B) is to count the
number of outcomes in the sample space that are contained within the event A ∩B, that is,
for which both cards are in the heart suit. In fact,
A ∩B = {(A♥, 2♥), (A♥, 3♥), . . . , (A♥, K♥), (2♥, A♥), (2♥, 3♥), . . . ,
(2♥, K♥), . . . (K♥, A♥), (K♥, 2♥), . . . , (K♥, Q♥)}
which consists of 13 × 12 = 156 outcomes. Consequently, the required probability is
P(A ∩B) = 156
2652 = 3
51
However, a more convenient way of calculating this probability is to note that it is the
product of P(A) and P(B|A). When the ﬁrst card is drawn, there are 13 heart cards out of a
total of 52 cards, so
P(A) = 13
52 = 1
4
Conditional on the ﬁrst card being a heart (event A), when the second card is drawn there will
be 12 heart cards remaining in the reduced pack of 51 cards, so that
P(B|A) = 12
51
The required probability is then
P(A ∩B) = P(A) P(B|A) = 1
4 × 12
51 = 3
51
as before.
1.5.2
Independent Events
Two events A and B are said to be independent events if
P(B|A) = P(B)
so that the probability of event B remains the same whether or not the event A is conditioned
upon. In other words, knowledge of the occurrence (or lack of occurrence) of event A does
not affect the probability of event B. In this case
P(A ∩B) = P(A) P(B|A) = P(A) P(B)
and
P(A|B) = P(A ∩B)
P(B)
= P(A)P(B)
P(B)
= P(A)
Thus, in a similar way, the probability of event A remains the same whether or not event B is
conditioned upon, and the probability of both events occurring, P(A ∩B), is obtained simply
by multiplying together the individual probabilities of the two events P(A) and P(B).

42
CHAPTER 1
PROBABILITY THEORY
Independent Events
Two events A and B are said to be independent events if
P(A|B) = P(A),
P(B|A) = P(B),
and
P(A ∩B) = P(A) P(B)
Any one of these three conditions implies the other two. The interpretation of two
events being independent is that knowledge about one event does not affect the
probability of the other event.
The concept of independence is most easily understood from a practical standpoint, with
two events being independent if they are “unrelated” to each other. For example, suppose that
a person is chosen at random from a large group of people, as in a lottery, for instance. Let A
be the event that the person is over 6 feet tall, and let B be the event that the person weighs
more than 200 pounds. Intuitively, these two events are not independent because knowledge
of one event inﬂuences our perception of the likelihood of the other event. For example, if the
lottery winner is known to be over 6 feet tall, then this fact increases the likelihood that the
person weighs more than 200 pounds.
On the other hand, if event C is that the person owns a red car, then intuitively the events A
and C are independent, as are the events B and C. The knowledge that the lottery winner is
over 6 feet tall does not change our perception of the probability that the person owns a red
car. Conversely, the knowledge that the lottery winner owns a red car does not change our
perception of the probability that the person is over 6 feet tall.
From a mathematical standpoint, two events A and B can be proven to be independent by
establishing one of the conditions
P(A|B) = P(A),
P(B|A) = P(B),
or
P(A ∩B) = P(A) P(B)
In practice, however, an assessment of whether two events are independent or not is usually
made by the practical consideration of whether the two events are “unrelated.”
Events A1, A2, . . . , An are said to be independent if conditioning on combinations of some
of the events does not affect the probabilities of the other events. In this case, the expression
given earlier for the probability of the intersection of the events simpliﬁes to the product of
the probabilities of the individual events.
Intersections of Independent Events
The probability of the intersection of a series of independent events A1, . . . , An is
simply given by
P(A1 ∩· · · ∩An) = P(A1) P(A2) · · · P(An)
Consider again the problem discussed above where two cards are drawn from a pack of
cards, and where A is the event that the ﬁrst card drawn is from the heart suit and B is the
event that the second card drawn is from the heart suit. Suppose that now the drawings are
made with replacement. What is P(A ∩B) in this case?
Figure 1.12 shows the sample space for this problem, which consists of 2704 equally
likely outcomes, each with a probability of 1/2704. As before, one way to calculate P(A∩B)
is to count the number of outcomes in the sample space that are contained within the event

1.5 PROBABILITIES OF EVENT INTERSECTIONS
43
A ∩B. This event is now
A ∩B = {(A♥, A♥), (A♥, 2♥), . . . , (A♥, K♥), (2♥, A♥), (2♥, 2♥), . . . ,
(2♥, K♥), . . . (K♥, A♥), (K♥, 2♥), . . . , (K♥, K♥)}
It consists of 13 × 13 = 169 outcomes, so that the required probability is
P(A ∩B) = 169
2704 = 1
16
However, it is easier to notice that events A and B are independent with P(A) = P(B) =
1/4, so that
P(A ∩B) = P(A) P(B) = 1
4 × 1
4 = 1
16
The independence follows from the fact that with the replacement of the ﬁrst card and with
appropriate shufﬂing of the pack to ensure randomness, the outcome of the second drawing
is not related to the outcome of the ﬁrst drawing.
If the drawings are performed without replacement, then clearly events A and B are not
independent. This can be conﬁrmed mathematically by noting that
P(B|A) = 12
51
and
P(B|A′) = 13
51
which are different from P(B) = 1/4.
1.5.3
Examples and Probability Trees
Example 2
Defective Computer
Chips
Suppose that 9 out of the 500 chips in a particular box are defective, and suppose that 3 chips
are sampled at random from the box without replacement. If each of the 3 chips sampled is
tested to determine whether it is defective (1) or satisfactory (0), the sample space has eight
outcomes. For example, the outcome (0, 1, 0) corresponds to the ﬁrst and third chips being
satisfactory and the second chip being defective.
The probability values of the eight outcomes can be calculated using a probability tree
as illustrated in Figure 1.63. The events A, B, and C are, respectively, the events that the
ﬁrst, second, and third chips sampled are defective. These events are not independent since
the sampling is conducted without replacement. The probability tree starts at the left with two
branches corresponding to the events A and A′. The probabilities of these events
P(A) =
9
500
and
P(A′) = 491
500
are recorded at the ends of the branches.
Each of these two branches then splits into two more branches corresponding to the
events B and B′, and the conditional probabilities of these events are recorded. These condi-
tional probabilities are
P(B|A) =
8
499,
P(B′|A) = 491
499,
P(B|A′) =
9
499,
P(B′|A′) = 490
499
which are constructed by considering how many of the 499 chips left in the box are defective
when the second chip is chosen. For example, P(B|A) = 8/499 since if the ﬁrst chip chosen
is defective (event A), then 8 out of 499 chips in the box are defective when the second chip
is chosen.
The probability tree is completed by adding additional branches for the events C and C′,
and by recording the probabilities of these events conditional on the outcomes of the ﬁrst two

44
CHAPTER 1
PROBABILITY THEORY
Outcome
(1,1,1)
(1,1,0)
(1,0,1)
(1,0,0)
(0,1,1)
(0,1,0)
(0,0,1)
(0,0,0)
Probability
First chip satisfactory
First chip defective
Second chip defective
Third chip defective
A
B
'A
B
B
B'
'
C'
C'
C'
C
C'
C
C
C
Second chip satisfactory
Third chip satisfactory
9
500
491
500
8
499
491
499
9
499
490
499
7
498
491
498
9
498
489
498
8
498
490
498
8
498
490
498
≃4.0
10 6
499
490
498 ≃0.0174
499
≃2.85
10 4
≃2.85
10 4
491
500
9
499
8
498 ≃2.85
10 4
491
500
9
499
490
498 ≃0.0174
491
500
491
9
500
499
8
498
491
9
500
491
498
8
9
500
499
7
498
8
499
9
500
490
499
9
498 ≃0.0174
491
500
490
499
489
498 ≃0.9469
1
FIGURE 1.63
Probability tree for computer chip sampling
choices. For example,
P(C|A ∩B′) =
8
498
because conditional on the event A ∩B′ (the ﬁrst choice is defective and the second is
satisfactory), 8 out of the 498 chips in the box are defective when the third choice is made.
The probability values of the eight outcomes are found by multiplying the probabilities
along the branches. Thus, the probability of choosing 3 defective chips is
P((1, 1, 1)) = P(A ∩B ∩C) = P(A)P(B|A)P(C|A ∩B)
=
9
500 ×
8
499 ×
7
498 =
21
5,177,125 ≃4.0 × 10−6
The probability of choosing 2 satisfactory chips followed by a defective chip is
P((0, 0, 1)) = P(A′ ∩B′ ∩C) = P(A′)P(B′|A′)P(C|A′ ∩B′)
= 491
500 × 490
499 ×
9
498 =
72,177
4,141,700 ≃0.0174

1.5 PROBABILITIES OF EVENT INTERSECTIONS
45
Notice that the probabilities of the outcomes (1, 0, 0), (0, 1, 0), and (0, 0, 1) are identical,
although they are calculated in different ways. Similarly, the probabilities of the outcomes
(1, 1, 0), (0, 1, 1), and (1, 0, 1) are identical. The probability of exactly 1 defective chip being
found is
P(1 defective) = P((1, 0, 0)) + P((0, 1, 0)) + P((0, 0, 1))
≃3 × 0.0174 = 0.0522
In fact, if attention is focused solely on the number of defective chips in the sample, then the
required probabilities can be found from the hypergeometric distribution which is discussed
in Section 3.3. Finally, it is interesting to note that, in practice, the number of defective chips
in a box will not usually be known, but probabilities of these kinds are useful in estimating
the number of defective chips in the box. In later chapters of this book, statistical techniques
will be employed to use the information provided by a random sample (here the number of
defective chips found in the sample) to make inferences about the population that is sampled
(here the box of chips).
Example 6
Satellite Launching
A satellite launch system is controlled by a computer (computer 1) that has two identical
backup computers (computers 2 and 3). Normally, computer 1 controls the system, but if it
has a malfunction then computer 2 automatically takes over. If computer 2 malfunctions then
computer 3 automatically takes over, and if computer 3 malfunctions there is a general system
shutdown.
The state space for this problem consists of the four situations
S = {computer 1 in use, computer 2 in use, computer 3 in use, system failure}
Suppose that a computer malfunctions with a probability of 0.01 and that malfunctions of the
three computers are independent of each other. Also, let the events A, B, and C be, respectively,
the events that computers 1, 2, and 3 malfunction.
Figure 1.64 shows the probability tree for this problem, which starts at the left with
two branches corresponding to the events A and A′ with probabilities P(A) = 0.01 and
0.01
0.99
0.99
0.01
0.01
Computer 1 malfunctions
Computer 2 malfunctions
Computer 3 malfunctions
A
B
C
A
'
B'
C'
Outcome
Probability
Computer 1 in use
Computer 2 in use
Computer 3 in use
System failure
0.99
0.01 ×  0.99 
   = 0.0099
0.01 ×  0.01 ×  0.99 
   = 0.000099
0.01 ×  0.01 ×  0.01 
   = 10 − 6
1
0.99
FIGURE 1.64
Probability tree for computer backup system

46
CHAPTER 1
PROBABILITY THEORY
P(A′) = 0.99. The top branch (event A′) corresponds to computer 1 being in use, and there
is no need to extend it further. However, the bottom branch (event A) extends into two further
branches for the events B and B′.
Since events A and B are independent, the probabilities of these second-stage branches
(events B and B′) do not need to be conditioned on the ﬁrst-stage branch (event A), and so
their probabilities are just P(B) = 0.01 and P(B′) = 0.99. The probability tree is completed
by adding branches for the events C and C′ following on from the events A and B.
The probability values of the four situations are obtained by multiplying the probabilities
along the branches, so that
P(computer 1 in use) = 0.99
P(computer 2 in use) = 0.01 × 0.99 = 0.0099
P(computer 3 in use) = 0.01 × 0.01 × 0.99 = 0.000099
P(system failure) = 0.01 × 0.01 × 0.01 = 10−6
The design of the system backup capabilities is obviously conducted with the aim of mini-
mizing the probability of a system failure. Notice that a key issue in the determination of this
probability is the assumption that the malfunctions of the three computers are independent
events. In other words, a malfunction in computer 1 should not affect the probabilities of the
other two computers malfunctioning. An essential part of such a backup system is ensuring
that these events are as independent as it is possible to make them.
In particular, it is sensible to have three teams of programmers working independently
to supply software to the three computers. If only one piece of software is written and then
copied onto the three machines, then the computer malfunctions will not be independent since a
malfunction due to a software error in computer 1 will be repeated in the other two computers.
Finally, it is worth noting that this system can be thought of as consisting of three computers
connected in parallel, as discussed in Section 17.1.2, where system reliability is considered
in more detail.
Example 7
Car Warranties
A company sells a certain type of car that it assembles in one of four possible locations. Plant I
supplies 20% of the cars; plant II, 24%; plant III, 25%; and plant IV, 31%. A customer buying
a car does not know where the car has been assembled, and so the probabilities of a purchased
car being from each of the four plants can be thought of as being 0.20, 0.24, 0.25, and 0.31.
Each new car sold carries a one-year bumper-to-bumper warranty. The company has
collected data that show
P(claim|plant I) = 0.05
P(claim|plant II) = 0.11
P(claim|plant III) = 0.03
P(claim|plant IV) = 0.08
For example, a car assembled in plant I has a probability of 0.05 of receiving a claim on
its warranty. This information, which is a closely guarded company secret, indicates which
assembly plants do the best job. Plant III is seen to have the best record, and plant II the worst
record. Notice that claims are clearly not independent of assembly location because these four
conditional probabilities are unequal.
Figure 1.65 shows a probability tree for this problem. It is easily constructed because the
probabilities of the second-stage branches are simply obtained from the conditional probabil-
ities above. The probability that a customer purchases a car that was assembled in plant I and
that does not require a claim on its warranty is seen to be
P(plant I, no claim) = 0.20 × 0.95 = 0.19

1.5 PROBABILITIES OF EVENT INTERSECTIONS
47
FIGURE 1.65
Probability tree for car warranties
example
0.20 × 0.05  =  0.0100
0.20 × 0.95  =  0.1900
0.24 × 0.11  =  0.0264
0.24 × 0.89  =  0.2136 
0.25 × 0.03  =  0.0075
0.25 × 0.97  =  0.2425
0.31 × 0.08  =  0.0248
0.31 × 0.92  =  0.2852
0.05
0.95
0.11
0.89
0.03
0.97
0.08
0.92
No claim
Plant I 
Plant II
Plant III
Plant IV
0.20
0.24
0.25
0.31
Claim
Claim
Claim
Claim
No claim
No claim
No claim
Probability
1
From a customer’s point of view, the probability of interest is the probability that a claim
on the warranty of the car will be required. This can be calculated as
P(claim) = P(plant I, claim) + P(plant II, claim) + P(plant III, claim)
+ P(plant IV, claim)
= (0.20 × 0.05) + (0.24 × 0.11) + (0.25 × 0.03) + (0.31 × 0.08)
= 0.0687
In other words, about 6.87% of the cars purchased will have a claim on their warranty. Notice
that this overall claim rate is a weighted average of the four individual plant claim rates, with
weights corresponding to the supply proportions of the four plants.
GAMES OF CHANCE
In the roll of a fair die, consider the events
even = {2, 4, 6}
and
high score = {4, 5, 6}
Intuitively, these two events are not independent since the knowledge that a high score is
obtained increases the chances of the score being even, and vice versa, the knowledge that
the score is even increases the chances of the score being high. Mathematically, this may be
conﬁrmed by noting that the probabilities
P(even) = 1
2
and
P(even|high score) = 2
3
are different.

48
CHAPTER 1
PROBABILITY THEORY
If a red die and a blue die are rolled, consider the probability that both dice record even
scores. In this case the scores on the two dice will be independent of each other since the score
on one die does not affect the score that is obtained on the other die. If A is the event that the
red die has an even score, and B is the event that the blue die has an even score, the required
probability is
P(A ∩B) = P(A) P(B) = 1
2 × 1
2 = 1
4
A more tedious way of calculating this probability is to note that 9 out of the 36 outcomes in
the sample space (see Figure 1.46) have both scores even, so that the required probability is
9/36 = 1/4.
Suppose that two cards are drawn from a pack of cards without replacement. What is the
probability that exactly one card from the heart suit is obtained? A very tedious way to solve
this problem is to count the number of outcomes in the sample space (see Figure 1.13) that
satisfy this condition. A better way is
P(exactly one heart) = P(ﬁrst card heart, second card not heart)
+ P(ﬁrst card not heart, second card heart)
=
13
52 × 39
51

+
39
52 × 13
51

= 13
34 = 0.382
Since the second drawing is made without replacement, the events “ﬁrst card heart” and
“second card heart” are not independent. However, notice that if the second card is drawn with
replacement, then the two events are independent, and the required probability is
P(exactly one heart) = P(ﬁrst card heart, second card not heart)
+ P(ﬁrst card not heart, second card heart)
=
1
4 × 3
4

+
3
4 × 1
4

= 3
8 = 0.375
It is interesting that the probability is slightly higher when the second drawing is made without
replacement.
1.5.4
Problems
1.5.1 Two cards are chosen from a pack of cards without
replacement. Calculate the probabilities:
(a) Both are picture cards.
(b) Both are from red suits.
(c) One card is from a red suit and one card is from a
black suit.
1.5.2 Repeat Problem 1.5.1, except that the second drawing is
made with replacement. Compare your answers with
those from Problem 1.5.1.
1.5.3 Two cards are chosen from a pack of cards without
replacement. Are the following events independent?
(a) (i) The ﬁrst card is a picture card, (ii) the second card
is a picture card.
(b) (i) The ﬁrst card is a heart, (ii) the second card is a
picture card.
(c) (i) The ﬁrst card is from a red suit, (ii) the second
card is from a red suit.
(d) (i) The ﬁrst card is a picture card, (ii) the second card
is from a red suit.
(e) (i) The ﬁrst card is a red picture card, (ii) the second
card is a heart.
1.5.4 Four cards are chosen from a pack of cards without
replacement. What is the probability that all four cards
are hearts? What is the probability that all four cards are
from red suits? What is the probability that all four cards
are from different suits?

1.5 PROBABILITIES OF EVENT INTERSECTIONS
49
FIGURE 1.66
Switch diagram
1
2
3
?
0.88
0.92
0.90
1.5.5 Repeat Problem 1.5.4, except that the drawings are made
with replacement. Compare your answers with those from
Problem 1.5.4.
1.5.6 Show that if the events A and B are independent events,
then so are the events
(a) A and B′
(b) A′ and B
(c) A′ and B′
1.5.7 Consider the network given in Figure 1.66 with three
switches. Suppose that the switches operate independently
of each other and that switch 1 allows a message through
with probability 0.88, switch 2 allows a message through
with probability 0.92, and switch 3 allows a message
through with probability 0.90. What is the probability that
a message will ﬁnd a route through the network?
1.5.8 Suppose that birthdays are equally likely to be on any day
of the year (ignore February 29 as a possibility). Show
that the probability that two people chosen at random
have different birthdays is 364/365. Show that the
probability that three people chosen at random all
have different birthdays is
364
365 × 363
365
and extend this pattern to show that the probability that n
people chosen at random all have different birthdays is
364
365 × · · · × 366 −n
365
What then is the probability that in a group of n people, at
least two people will share the same birthday? Evaluate this
probability for n = 10, n = 15, n = 20, n = 25, n = 30,
and n = 35. What is the smallest value of n for which
the probability is larger than a half? Do you think that
birthdays are equally likely to be on any day of the year?
1.5.9 Suppose that 17 lightbulbs in a box of 100 lightbulbs are
broken and that 3 are selected at random without
replacement. Construct a probability tree for this
problem. What is the probability that there will be no
broken lightbulbs in the sample? What is the probability
that there will be no more than 1 broken lightbulb in the
sample? (This problem is continued in Problem 1.7.8.)
1.5.10 Repeat Problem 1.5.9, except that the drawings are made
with replacement. Compare your answers with those from
Problem 1.5.9.
1.5.11 Suppose that a bag contains 43 red balls, 54 blue balls,
and 72 green balls, and that 2 balls are chosen at random
without replacement. Construct a probability tree for this
problem. What is the probability that 2 green balls will be
chosen? What is the probability that the 2 balls chosen
will have different colors?
1.5.12 Repeat Problem 1.5.11, except that the drawings are made
with replacement. Compare your answers with those from
Problem 1.5.11.
1.5.13 A biased coin has a probability p of resulting in a head.
If the coin is tossed twice, what value of p minimizes the
probability that the same result is obtained on both throws?
1.5.14 If a fair die is rolled six times, what is the probability that
each score is obtained exactly once? If a fair die is rolled
seven times, what is the probability that a 6 is not
obtained at all?
1.5.15 (a) If a fair die is rolled ﬁve times, what is the
probability that the numbers obtained are all even
numbers?
(b) If a fair die is rolled three times, what is the probability
that the three numbers obtained are all different?
(c) If three cards are taken at random from a pack of
cards with replacement, what is the probability that
there are two black cards and one red card?
(d) If three cards are taken at random from a pack of
cards without replacement, what is the probability
that there are two black cards and one red card?
1.5.16 Suppose that n components are available, and that each
component has a probability of 0.90 of operating
correctly, independent of the other components. What
value of n is needed so that there is a probability of

50
CHAPTER 1
PROBABILITY THEORY
at least 0.995 that at least one component operates
correctly?
1.5.17 Suppose that an insurance company insures its clients for
ﬂood damage to property. Can the company reasonably
expect that the claims from its clients will be independent
of each other?
1.5.18 A system has four computers. Computer 1 works with a
probability of 0.88; computer 2 works with a probability
of 0.78; computer 3 works with a probability of 0.92;
computer 4 works with a probability of 0.85. Suppose
that the operations of the computers are independent of
each other.
(a) Suppose that the system works only when all four
computers are working. What is the probability that
the system works?
(b) Suppose that the system works only if at least one
computer is working. What is the probability that the
system works?
(c) Suppose that the system works only if at least three
computers are working. What is the probability that
the system works?
1.5.19 Suppose that there are two companies such that for each
one the revenue is considerably below expectation
with probability 0.08, is slightly below expectation
with probability 0.19, exactly meets expectation with
probability 0.26, is slightly above expectation with
probability 0.36, and is considerably above expectation
with probability 0.11. Furthermore, suppose that the
revenues from both companies are independent. What is
the probability that neither company has a revenue below
expectation?
1.5.20 Consider four advertising campaigns where for each one
it is canceled before launch with probability 0.10, it is
launched but canceled early with probability 0.18, it is
launched and runs its targeted length with probability
0.43, and it is launched and is extended beyond its
targeted length with probability 0.29. If the advertising
campaigns are independent, what is the probability that
all four campaigns will run at least as long as they are
targeted?
1.6
Posterior Probabilities
1.6.1
Law of Total Probability
Let A1, . . . , An be a partition of a sample space S so that the events Ai are mutually exclusive
with
S = A1 ∪· · · ∪An
Suppose that the probabilities of these n events, P(A1), . . . , P(An), are known. In addition,
consider an event B as shown in Figure 1.67, and suppose that the conditional probabilities
P(B|A1), . . . , P(B|An) are also known.
FIGURE 1.67
A partition A1, . . . , An and an
event B
A
B
1
2
3
n
S
A
A
A

1.6 POSTERIOR PROBABILITIES
51
An initial question of interest is how to use the probabilities P(Ai) and P(B|Ai) to
calculate P(B), the probability of the event B. In fact, this is easily achieved by noting that
B = (A1 ∩B) ∪· · · ∪(An ∩B)
where the events Ai ∩B are mutually exclusive, so that
P(B) = P(A1 ∩B) + · · · + P(An ∩B)
= P(A1) P(B|A1) + · · · + P(An) P(B|An)
This result, known as the law of total probability, has the interpretation that if it is known that
one and only one of a series of events Ai can occur, then the probability of another event B can
be obtained as the weighted average of the conditional probabilities P(B|Ai), with weights
equal to the probabilities P(Ai).
Law of Total Probability
If A1, . . . , An is a partition of a sample space, then the probability of an event B can
be obtained from the probabilities P(Ai) and P(B|Ai) using the formula
P(B) = P(A1) P(B|A1) + · · · + P(An) P(B|An)
Example 7
Car Warranties
The law of total probability was tacitly used in the previous section when the probability of a
claim being made on a car warranty was calculated to be 0.0687. If A1, A2, A3, and A4 are,
respectively, the events that a car is assembled in plants I, II, III, and IV, then they provide a
partition of the sample space, and the probabilities P(Ai) are the supply proportions of the
four plants.
If B is the event that a claim is made, then the conditional probabilities P(B|Ai) are the
claim rates for the four individual plants, so that
P(B) = P(A1) P(B|A1) + P(A2) P(B|A2) + P(A3) P(B|A3) + P(A4) P(B|A4)
= (0.20 × 0.05) + (0.24 × 0.11) + (0.25 × 0.03) + (0.31 × 0.08)
= 0.0687
as obtained before.
1.6.2
Calculation of Posterior Probabilities
An additional question of interest is how to use the probabilities P(Ai) and P(B|Ai) to
calculate the probabilities P(Ai|B), the revised probabilities of the events Ai conditional on
the event B. The probabilities
P(A1), . . . , P(An)
can be thought of as being the prior probabilities of the events A1, . . . , An. However, the
observation of the event B provides some additional information that allows the revision of
these prior probabilities into a set of posterior probabilities
P(A1|B), . . . , P(An|B)
which are the probabilities of the events A1, . . . , An conditional on the event B.
From the law of total probability, these posterior probabilities are calculated to be
P(Ai|B) = P(Ai ∩B)
P(B)
= P(Ai) P(B|Ai)
P(B)
=
P(Ai) P(B|Ai)
n
j=1 P(A j) P(B|A j)
which is known as Bayes’ theorem.

52
CHAPTER 1
PROBABILITY THEORY
Bayes’ Theorem
If A1, . . . , An is a partition of a sample space, then the posterior probabilities of the
events Ai conditional on an event B can be obtained from the probabilities P(Ai) and
P(B|Ai) using the formula
P(Ai|B) =
P(Ai) P(B|Ai)
n
j=1 P(A j) P(B|A j)
HISTORICAL NOTE
Thomas Bayes was born in
London, England, in 1702. He
was ordained and ministered at
a Presbyterian church in
Tunbridge Wells, about 35
miles outside London. He was
elected a Fellow of the Royal
Society in 1742 and died on
April 17, 1761. His work on
posterior probabilities was
discovered in his papers after
his death.
Bayes’ theorem is an important result in probability theory because it shows how new
information can properly be used to update or revise an existing set of probabilities. In some
cases the prior probabilities P(Ai) may have to be estimated based on very little information
or on subjective feelings. It is then important to be able to improve these probabilities as more
information becomes available, and Bayes’ theorem provides the means to do this.
1.6.3
Examples of Posterior Probabilities
Example 7
Car Warranties
When a customer buys a car, the (prior) probabilities of it having been assembled in a particular
plant are
P(plant I) = 0.20
P(plant II) = 0.24
P(plant III) = 0.25
P(plant IV) = 0.31
If a claim is made on the warranty of the car, how does this change these probabilities? From
Bayes’ theorem, the posterior probabilities are calculated to be
P(plant I|claim) = P(plant I)P(claim|plant I)
P(claim)
= 0.20 × 0.05
0.0687
= 0.146
P(plant II|claim) = P(plant II)P(claim|plant II)
P(claim)
= 0.24 × 0.11
0.0687
= 0.384
P(plant III|claim) = P(plant III)P(claim|plant III)
P(claim)
= 0.25 × 0.03
0.0687
= 0.109
P(plant IV|claim) = P(plant IV)P(claim|plant IV)
P(claim)
= 0.31 × 0.08
0.0687
= 0.361
which are tabulated in Figure 1.68. Notice that plant II has the largest claim rate (0.11), and its
posterior probability 0.384 is much larger than its prior probability of 0.24. This is expected
since the fact that a claim is made increases the likelihood that the car has been assembled in
a plant that has a high claim rate. Similarly, plant III has the smallest claim rate (0.03), and
its posterior probability 0.109 is much smaller than its prior probability of 0.25, as expected.
FIGURE 1.68
Prior and posterior probabilities for
car warranties example
Posterior Probabilities
Prior Probabilities
Claim
No claim
Plant I
0.200
0.146
0.204
Plant II
0.240
0.384
0.229
Plant III
0.250
0.109
0.261
Plant IV
0.310
0.361
0.306
1.000
1.000
1.000

1.6 POSTERIOR PROBABILITIES
53
On the other hand, if no claim is made on the warranty, the posterior probabilities are
calculated to be
P(plant I|no claim) = P(plant I)P(no claim|plant I)
P(no claim)
= 0.20 × 0.95
0.9313
= 0.204
P(plant II|no claim) = P(plant II)P(no claim|plant II)
P(no claim)
= 0.24 × 0.89
0.9313
= 0.229
P(plant III|no claim) = P(plant III)P(no claim|plant III)
P(no claim)
= 0.25 × 0.97
0.9313
= 0.261
P(plant IV|no claim) = P(plant IV)P(no claim|plant IV)
P(no claim)
= 0.31 × 0.92
0.9313
= 0.306
as tabulated in Figure 1.68. In this case when no claim is made, the probabilities decrease
slightly for plant II and increase slightly for plant III.
Finally, it is interesting to notice that when a claim is made the probabilities are revised
quite substantially, but when no claim is made the posterior probabilities are almost the same
as the prior probabilities. Intuitively, this is because the claim rates are all rather small, and so
a claim is an “unusual” occurrence, which requires a more radical revision of the probabilities.
Example 8
Chemical Impurity
Levels
A chemical company has to pay particular attention to the impurity levels of the chemicals that
it produces. Previous experience leads the company to estimate that about one in a hundred
of its chemical batches has an impurity level that is too high.
To ensure better quality for its products, the company has invested in a new laser-based
technology for measuring impurity levels. However, this technology is not foolproof, and its
manufacturers warn that it will falsely give a reading of a high impurity level for about 5% of
batches that actually have satisfactory impurity levels (these are “false-positive” results). On
the other hand, it will falsely indicate a satisfactory impurity level for about 2% of batches that
have high impurity levels (these are “false-negative” results). With this in mind, the chemical
company is interested in questions such as these:
■
If a high impurity reading is obtained, what is the probability that the impurity level
really is high?
■
If a satisfactory impurity reading is obtained, what is the probability that the impurity
level really is satisfactory?
To answer these questions, let A be the event that the impurity level is too high. Event A
and its complement A′ form a partition of the sample space, and they have prior probability
values of
P(A) = 0.01
and
P(A′) = 0.99

54
CHAPTER 1
PROBABILITY THEORY
Let B be the event that a high impurity reading is obtained. The false-negative rate then
indicates that
P(B|A) = 0.98
and
P(B′|A) = 0.02
and the false-positive rate indicates that
P(B|A′) = 0.05
and
P(B′|A′) = 0.95
If a high impurity reading is obtained, Bayes’ theorem gives
P(A|B) =
P(A) P(B|A)
P(A) P(B|A) + P(A′) P(B|A′)
=
0.01 × 0.98
(0.01 × 0.98) + (0.99 × 0.05) = 0.165
and
P(A′|B) =
P(A′) P(B|A′)
P(A) P(B|A) + P(A′) P(B|A′)
=
0.99 × 0.05
(0.01 × 0.98) + (0.99 × 0.05) = 0.835
If a satisfactory impurity reading is obtained, Bayes’ theorem gives
P(A|B′) =
P(A) P(B′|A)
P(A) P(B′|A) + P(A′) P(B′|A′)
=
0.01 × 0.02
(0.01 × 0.02) + (0.99 × 0.95) = 0.0002
and
P(A′|B′) =
P(A′) P(B′|A′)
P(A) P(B′|A) + P(A′) P(B′|A′)
=
0.99 × 0.95
(0.01 × 0.02) + (0.99 × 0.95) = 0.9998
These posterior probabilities are tabulated in Figure 1.69.
We can see that if a satisfactory impurity reading is obtained, then the probability of the
impurity level actually being too high is only 0.0002, so that on average, only 1 in 5000 batches
testing satisfactory is really not satisfactory. However, if a high impurity reading is obtained,
there is a probability of only 0.165 that the impurity level really is high, and the probability
is 0.835 that the batch is really satisfactory. In other words, only about 1 in 6 of the batches
testing high actually has a high impurity level.
At ﬁrst this may appear counterintuitive. Since the false-positive and false-negative error
rates are so low, why is it that most of the batches testing high are really satisfactory? The
answer lies in the fact that about 99% of the batches have satisfactory impurity levels, so that
99% of the time there is an “opportunity” for a false-positive result, and only about 1% of the
time is there an “opportunity” for a genuine positive result.
FIGURE 1.69
Prior and posterior probabilities for
the chemical impurities example
Posterior Probabilities
Prior Probabilities
B: high reading
B′: satisfactory reading
A: impurity level too high
0.0100
0.1650
0.0002
A′: impurity level satisfactory
0.9900
0.8350
0.9998
1.0000
1.0000
1.0000

1.6 POSTERIOR PROBABILITIES
55
In conclusion, the chemical company should realize that it is wasteful to disregard off-hand
batches that are indicated to have high impurity levels. Further investigation of these batches
should be undertaken to identify the large proportion of them that are in fact satisfactory
products.
1.6.4
Problems
1.6.1 Suppose it is known that 1% of the population suffers from
a particular disease. A blood test has a 97% chance of
identifying the disease for diseased individuals, but also
has a 6% chance of falsely indicating that a healthy person
has the disease.
(a) What is the probability that a person will have a
positive blood test?
(b) If your blood test is positive, what is the chance that
you have the disease?
(c) If your blood test is negative, what is the chance that
you do not have the disease?
1.6.2 Bag A contains 3 red balls and 7 blue balls. Bag B contains
8 red balls and 4 blue balls. Bag C contains 5 red balls and
11 blue balls. A bag is chosen at random, with each bag
being equally likely to be chosen, and then a ball is chosen
at random from that bag. Calculate the probabilities:
(a) A red ball is chosen.
(b) A blue ball is chosen.
(c) A red ball from bag B is chosen.
If it is known that a red ball is chosen, what is the probability
that it comes from bag A? If it is known that a blue ball
is chosen, what is the probability that it comes from bag B?
1.6.3 A class had two sections. Section I had 55 students of
whom 10 received A grades. Section II had 45 students
of whom 11 received A grades. Now 1 of the 100 students
is chosen at random, with each being equally likely to be
chosen.
(a) What is the probability that the student was in
section I?
(b) What is the probability that the student received an
A grade?
(c) What is the probability that the student received an
A grade if the student is known to have been in
section I?
(d) What is the probability that the student was in section I
if the student is known to have received an A grade?
1.6.4 An island has three species of bird. Species 1 accounts for
45% of the birds, of which 10% have been tagged. Species
2 accounts for 38% of the birds, of which 15% have been
tagged. Species 3 accounts for 17% of the birds, of which
50% have been tagged. If a tagged bird is observed, what
are the probabilities that it is of species 1, of species 2, and
of species 3?
1.6.5 After production, an electrical circuit is given a quality
score of A, B, C, or D. Over a certain period of time,
77% of the circuits were given a quality score A,
11% were given a quality score B, 7% were given a
quality score C, and 5% were given a quality score D.
Furthermore, it was found that 2% of the circuits given a
quality score A eventually failed, and the failure rate was
10% for circuits given a quality score B, 14% for circuits
given a quality score C, and 25% for circuits given a
quality score D.
(a) If a circuit failed, what is the probability that it had
received a quality score either C or D?
(b) If a circuit did not fail, what is the probability that it
had received a quality score A?
1.6.6 The weather on a particular day is classiﬁed as either cold,
warm, or hot. There is a probability of 0.15 that it is cold
and a probability of 0.25 that it is warm. In addition, on
each day it may either rain or not rain. On cold days there is
a probability of 0.30 that it will rain, on warm days there is
a probability of 0.40 that it will rain, and on hot days there
is a probability of 0.50 that it will rain. If it is not raining
on a particular day, what is the probability that it is cold?
1.6.7 A valve can be used at four temperature levels. If the valve
is used at a cold temperature, then there is a probability of
0.003 that it will leak. If the valve is used at a medium
temperature, then there is a probability of 0.009 that it will
leak. If the valve is used at a warm temperature, then there
is a probability of 0.014 that it will leak. If the valve is used
at a hot temperature, then there is a probability of 0.018 that
it will leak. Under standard operating conditions, the valve
is used at a cold temperature 12% of the time, at a medium
temperature 55% of the time, at a warm temperature 20%
of the time, and at a hot temperature 13% of the time.
(a) If the valve leaks, what is the probability that it is
being used at the hot temperature?

56
CHAPTER 1
PROBABILITY THEORY
(b) If the valve does not leak, what is the probability that it
is being used at the medium temperature?
1.6.8 A company sells ﬁve types of wheelchairs, with type A
being 12% of the sales, type B being 34% of the sales,
type C being 7% of the sales, type D being 25% of the
sales, and type E being 22% of the sales. In addition,
19% of the type A wheelchair sales are motorized, 50% of
the type B wheelchair sales are motorized, 4% of the
type C wheelchair sales are motorized, 32% of the type D
wheelchair sales are motorized, and 76% of the type E
wheelchair sales are motorized.
(a) If a motorized wheelchair is sold, what is the
probability that it is of type C?
(b) If a nonmotorized wheelchair is sold, what is the
probability that it is of type D?
1.6.9 A company’s revenue is considerably below expectation
with probability 0.08, in which case there is a probability
of 0.03 that the CEO receives a bonus; is slightly below
expectation with probability 0.19, in which case there is a
probability of 0.14 that the CEO receives a bonus; exactly
meets expectation with probability 0.26, in which case
there is a probability of 0.60 that the CEO receives a
bonus; is slightly above expectation with probability 0.36,
in which case there is a probability of 0.77 that the CEO
receives a bonus; and is considerably above expectation
with probability 0.11, in which case there is a probability
of 0.99 that the CEO receives a bonus. What is the
probability that the CEO receives a bonus? If the CEO
receives a bonus, what is the probability that company has
a revenue below expectation?
1.6.10 An advertising campaign is canceled before launch with
probability 0.10, in which case the marketing company is
ﬁred with probability 0.74; is launched but canceled early
with probability 0.18, in which case the marketing
company is ﬁred with probability 0.43; is launched and
runs its targeted length with probability 0.43, in which case
the marketing company is ﬁred with probability 0.16; and
is launched and is extended beyond its targeted length with
probability 0.29, in which case the marketing company is
ﬁred with probability 0.01. What is the probability that the
marketing company is ﬁred? If the marketing company is
ﬁred, what is the probability that the advertising campaign
was not canceled before launch?
1.7
Counting Techniques
In many situations the sample space S consists of a very large number of outcomes that
the experimenter will not want to list in their entirety. However, if the outcomes are equally
likely, then it sufﬁces to know the number of outcomes in the sample space and the number of
outcomes contained within an event of interest. In this section, various counting techniques
are discussed that can be used to facilitate such computations. Remember that if a sample
space S consists of N equally likely outcomes, of which n are contained within the event A,
then the probability of the event A is
P(A) = n
N
1.7.1
Multiplication Rule
Suppose that an experiment consists of k “components” and that the ith component has ni
possible outcomes. The total number of experimental outcomes will then be equal to the
product
n1 × n2 × · · · × nk
This is known as the multiplication rule and can easily be seen by referring to the tree diagram
in Figure 1.70. The n1 outcomes of the ﬁrst component are represented by the n1 branches at
the beginning of the tree, each of which splits into n2 branches corresponding to the outcomes
of the second component, and so on. The total number of experimental outcomes (the size of
the sample space) is equal to the number of branch ends at the end of the tree, which is equal
to the product of the ni.

1.7 COUNTING TECHNIQUES
57
FIGURE 1.70
Probability tree illustrating the
multiplication rule
branches
ends
n
n
n branches
branches
1
2
n1
n1   n2 ends
k
1
2
 n   n        nk   ends
... 
Multiplication Rule
If an experiment consists of k components for which the number of possible outcomes
are n1, . . . , nk, then the total number of experimental outcomes (the size of the sample
space) is equal to
n1 × n2 × · · · × nk
Example 9
Car Body Assembly
Line
A side panel for a car is made from a sheet of metal in the following way. The metal sheet is
ﬁrst sent to a cleaning machine, then to a pressing machine, and then to a cutting machine.
The process is completed by a painting machine followed by a polishing machine. Each of the
ﬁve tasks can be performed on one of several machines whose number and location within
the factory are determined by the management so as to streamline the whole manufacturing
process.
In particular, suppose that there are six cleaning machines, three pressing machines,
eight cutting machines, ﬁve painting machines, and eight polishing machines, as illustrated in
Figure 1.71. As a quality control procedure, the company attaches a bar code to each panel that

58
CHAPTER 1
PROBABILITY THEORY
FIGURE 1.71
Manufacturing process for car side
panels
Metal 
sheet
Car panel
6 cleaning 
3 pressing
8 cutting
5 painting 
8 polishing
Total number of pathways is  6 × 3 × 8 × 5 × 8 = 5760.
machines
machines
machines
machines
machines
identiﬁes which of the machines have been used in its construction. The number of possible
“pathways” through the manufacturing process is
6 × 3 × 8 × 5 × 8 = 5760
The number of pathways that include a particular pressing machine are
6 × 8 × 5 × 8 = 1920
If the 5760 pathways can be considered to be equally likely, then a panel chosen at random
will have a probability of 1/5760 of having each of the pathways. However, notice that the
pathways will probably not be equally likely, since, for example, the factory layout could
cause a panel coming out of one pressing machine to be more likely to be passed on to a
particular cutting machine than panels from another pressing machine.
Example 10
Fiber Coatings
Thin ﬁbers are often coated by passing them through a cloud chamber containing the coating
material. The ﬁber and the coating material are provided with opposite electrical charges to
provide a means of attraction. Among other things, the quality of the coating will depend on
the sizes of the electrical charges employed, the density of the coating material in the cloud
chamber, the temperature of the cloud chamber, and the speed at which the ﬁber is passed
through the chamber.
A chemical engineer wishes to conduct an experiment to determine how these four factors
affect the quality of the coating. The engineer is interested in comparing three charge levels,
ﬁve density levels, four temperature levels, and three speed levels, as illustrated in Figure 1.72.
The total number of possible experimental conditions is then
3 × 5 × 4 × 3 = 180
In other words, there are 180 different combinations of the four factors that can be investigated.
However, the cost of running 180 experiments is likely to be prohibitive, and the engineer
may have a budget sufﬁcient to investigate, say, only 30 experimental conditions. Nevertheless,

1.7 COUNTING TECHNIQUES
59
FIGURE 1.72
Experimental conﬁgurations for
ﬁber coatings
3 × 5 × 4 × 3 = 180     possible experimental configurations
3 speed levels
5 density levels
4 temperature levels
3 charge levels
with an appropriate experimental design and statistical analysis, the engineer can carefully
choose which experimental conditions to investigate in order to provide a maximum amount
of information about the four factors and how they inﬂuence the quality of the coating. The
analysis of experiments of this kind is discussed in Chapter 14.
Often the k components of an experiment are identical because they are replications of
the same process. In such cases n1 = · · · = nk = n, say, and the total number of experimental
outcomes will be nk. For example, if a die is rolled twice, there are 6 × 6 = 36 possible
outcomes. If a die is rolled k times, there are 6k possible outcomes.
Computer codes consist of a series of binary digits 0 and 1. The number of different strings
consisting of k digits is then 2k. For example, a string of 20 digits can have
220 = 1,048,576
possible values. Calculations such as these indicate how much “information” can be carried
by the strings.
Computer passwords typically consist of a string of eight characters, say, which are either
1 of the 26 letters or a numerical digit. The possible number of choices for a password is then
368 ≃2.82 × 1012
If a password is chosen at random, the chance of somebody “guessing” it is thus negligibly
small. Nevertheless, a feeling of security could be an illusion for at least two reasons. First,
if a computer can be programmed to search repeatedly through possible passwords quickly
in an organized manner, it may not take it long to hit on the correct one. Second, few people
choose passwords at random, since they themselves have to remember them.
1.7.2
Permutations and Combinations
Often it is important to be able to calculate how many ways a series of distinguishable k objects
can be drawn from a pool of n objects. If the drawings are performed with replacement, then
the k drawings are identical events, each with n possible outcomes, and the multiplication rule
shows that there are nk possible ways to draw the k objects.

60
CHAPTER 1
PROBABILITY THEORY
Ifthedrawingsaremadewithoutreplacement,thentheoutcomeissaidtobeapermutation
of k objects from the original n objects. If only one object is chosen, then clearly there are
only n possible outcomes. If two objects are chosen, then there will be
n(n −1)
possible outcomes, since there are n possibilities for the ﬁrst choice and then only n −1
possibilities for the second choice. More generally, if k objects are chosen, there will be
n(n −1)(n −2) · · · (n −k + 1)
possible outcomes, which is obtained by multiplying together the number of choices at each
drawing.
For dealing with expressions such as these, it is convenient to use the following notation:
Factorials
If n is a positive integer, the quantity n! called “n factorial” is deﬁned to be
n! = n(n −1)(n −2) · · · (1)
Also, the quantity 0! is taken to be equal to 1.
The number of permutations of k objects from n objects is given the notation Pn
k .
Permutations
A permutation of k objects from n objects (n ≥k) is an ordered sequence of
k objects selected without replacement from the group of n objects. The number of
possible permutations of k objects from n objects is
Pn
k = n(n −1)(n −2) · · · (n −k + 1) =
n!
(n −k)!
Notice that if k = n, the number of permutations is
Pn
n = n(n −1)(n −2) · · · 1 = n!
which is just the number of ways of ordering n objects.
Example 11
Taste Tests
A food company has four different recipes for a potential new product and wishes to compare
them through consumer taste tests. In these tests, a participant is given the four types of food
to taste in a random order and is asked to rank various aspects of their taste. This ranking
procedure simply provides an ordering of the four products, and the number of possible ways
in which it can be performed is
P4
4 = 4! = 4 × 3 × 2 × 1 = 24
In a different taste test, each participant samples eight products and is asked to pick the
best, the second best, and the third best. The number of possible answers to the test is then
P8
3 = 8 × 7 × 6 = 336
Notice that with permutations, the order of the sequence is important. For example, if
the eight products in the taste test are labeled A–H, then the permutation ABC (in which

1.7 COUNTING TECHNIQUES
61
product A is judged to be best, product B second best, and product C third best) is considered
to be different from the permutation ACB, say. That is, each of the six orderings of the products
A, B, and C is considered to be a different permutation.
Sometimes when k objects are chosen from a group of n objects, the ordering of the
drawing of the k objects is not of importance. In other words, interest is focused on which k
objects are chosen, but not on the order in which they are chosen. Such a collection of objects
is called a combination of k objects from n objects. The notation Cn
k is used for the total
possible number of such combinations, and it is calculated using the formula
Cn
k =
n!
(n −k)! k!
Thisformulaforthenumberofcombinationsfollowsfromthefactthateachcombinationof
k objects can beassociatedwiththek!permutationsthatconsistofthoseobjects.Consequently,
Pn
k = k! × Cn
k
so that
Cn
k = Pn
k
k! =
n!
(n −k)! k!
A common alternative notation for the number of combinations is
Cn
k =

n
k

Combinations
A combination of k objects from n objects (n ≥k) is an unordered collection of k
objects selected without replacement from the group of n objects. The number of
possible combinations of k objects from n objects is
Cn
k =

n
k

=
n!
(n −k)! k!
Notice that
Cn
1 =

n
1

=
n!
(n −1)! 1! = n
and
Cn
2 =

n
2

=
n!
(n −2)! 2! = n(n −1)
2
so that there are n ways to choose one object from n objects, and n(n −1)/2 ways to choose
two objects from n objects (without attention to order). Also,
Cn
n−1 =

n
n −1

=
n!
1! (n −1)! = n
and
Cn
n =

n
n

=
n!
0! n! = 1
This last equation just indicates that there is only one way to choose all n objects. It is also
useful to note that Cn
k = Cn
n−k.

62
CHAPTER 1
PROBABILITY THEORY
Example 11
Taste Tests
Suppose that in the taste test, each participant samples eight products and is asked to select
the three best products, but not in any particular order. The number of possible answers to the
test is then

8
3

=
8!
5! 3! = 8 × 7 × 6
3 × 2 × 1 = 56
Example 2
Defective Computer
Chips
Suppose again that 9 out of 500 chips in a particular box are defective, and that 3 chips are
sampled at random from the box without replacement. The total number of possible samples
is

500
3

=
500!
497! 3! = 500 × 499 × 498
3 × 2 × 1
= 20,708,500
which are all equally likely.
The probability of choosing 3 defective chips can be calculated by dividing the number
of samples that contain 3 defective chips by the total number of samples. Since there are 9
defective chips, the number of samples that contain 3 defective chips is

9
3

=
9!
6! 3! = 9 × 8 × 7
3 × 2 × 1 = 84
so that the probability of choosing 3 defective chips is

9
3


500
3
 =
 9!
6! 3!

 500!
497! 3!
 =
9 × 8 × 7
500 × 499 × 498 ≃4.0 × 10−6
as obtained before.
Also, the number of samples that contains exactly 1 defective chip is
9 ×

491
2

since there are 9 ways to choose the defective chip and C491
2
ways to choose the 2 satisfactory
chips. Consequently, the probability of obtaining exactly 1 defective chip is
9 ×

491
2


500
3

=

9 ×
491!
489! 2!

 500!
497! 3!

= 9 × 491 × 490 × 3
500 × 499 × 498 = 0.0522
as obtained before. These calculations are examples of the hypergeometric distribution that is
discussed in Section 3.3.
GAMES OF CHANCE
Suppose that four cards are taken at random without replacement from a pack of cards. What
is the probability that two kings and two queens are chosen?
The number of ways to choose four cards is

52
4

=
52!
48! 4! = 52 × 51 × 50 × 49
4 × 3 × 2 × 1
= 270,725
The number of ways of choosing two kings from the four kings in the pack as well as the
number of ways of choosing two queens from the four queens in the pack is

4
2

=
4!
2! 2! = 4 × 3
2 × 1 = 6

1.7 COUNTING TECHNIQUES
63
so that the number of hands consisting of two kings and two queens is

4
2

×

4
2

= 36
The required probability is thus
36
270,725 ≃1.33 × 10−4
which is a chance of about 13 out of 100,000.
1.7.3
Problems
1.7.1 Evaluate:
(a) 7!
(b) 8!
(c) 4!
(d) 13!
1.7.2 Evaluate:
(a) P7
2
(b) P9
5
(c) P5
2
(d) P17
4
1.7.3 Evaluate:
(a) C6
2
(b) C8
4
(c) C5
2
(d) C14
6
1.7.4 A menu has ﬁve appetizers, three soups, seven main
courses, six salad dressings, and eight desserts. In how
many ways can a full meal be chosen? In how many ways
can a meal be chosen if either an appetizer or a soup is
ordered, but not both?
1.7.5 In an experiment to test iron strengths, three different ores,
four different furnace temperatures, and two different
cooling methods are to be considered. Altogether,
how many experimental conﬁgurations are possible?
1.7.6 Four players compete in a tournament and are ranked
from 1 to 4. They then compete in another tournament
and are again ranked from 1 to 4. Suppose that their
performances in the second tournament are unrelated to
their performances in the ﬁrst tournament, so that the two
sets of rankings are independent.
(a) What is the probability that each competitor receives
an identical ranking in the two tournaments?
(b) What is the probability that nobody receives the same
ranking twice?
1.7.7 Twenty players compete in a tournament. In how
many ways can rankings be assigned to the top ﬁve
competitors? In how many ways can the best ﬁve
competitors be chosen (without being in any order)?
1.7.8 There are 17 broken lightbulbs in a box of 100 lightbulbs.
A random sample of 3 lightbulbs is chosen without
replacement.
(a) How many ways are there to choose the sample?
(b) How many samples contain no broken lightbulbs?
(c) What is the probability that the sample contains no
broken lightbulbs?
(d) How many samples contain exactly 1 broken
lightbulb?
(e) What is the probability that the sample contains no
more than 1 broken lightbulb?
1.7.9 Show that Cn
k = Cn−1
k
+ Cn−1
k−1. Can you provide an
interpretation of this equality?
1.7.10 A poker hand consists of ﬁve cards chosen at random
from a pack of cards.
(a) How many different hands are there?
(b) How many hands consist of all hearts?
(c) How many hands consist of cards all from the same
suit (a “ﬂush”)?
(d) What is the probability of being dealt a ﬂush?
(e) How many hands contain all four aces?
(f) How many hands contain four cards of the same
number or picture?
(g) What is the probability of being dealt a hand
containing four cards of the same number or picture?
1.7.11 In an arrangement of n objects in a circle, an object’s
neighbors are important, but an object’s place in the circle
is not important. Thus, rotations of a given arrangement
are considered to be the same arrangement. Explain why
the number of different arrangements is (n −1)!
1.7.12 In how many ways can six people sit in six seats in a line
at a cinema? In how many ways can the six people sit
around a dinner table eating pizza after the movie?
1.7.13 Repeat Problem 1.7.12 with the condition that one of the
six people, Andrea, must sit next to Scott. In how many
ways can the seating arrangements be made if Andrea
refuses to sit next to Scott?
1.7.14 A total of n balls are to be put into k boxes with the
conditions that there will be n1 balls in box 1, n2 balls in

64
CHAPTER 1
PROBABILITY THEORY
box 2, and so on, with nk balls being placed in box k
(n1 + · · · + nk = n). Explain why the number of ways of
doing this is
n!
n1! × · · · × nk!
Explain why this is just Cn
n1 = Cn
n2 when k = 2.
1.7.15 Explain why the following two problems are identical and
solve them.
(a) In how many ways can 12 balls be placed in 3 boxes,
when the ﬁrst box can hold 3 balls, the second box
can hold 4 balls, and the third box can hold 5 balls.
(b) In how many ways can 3 red balls, 4 blue balls, and 5
green balls be placed in a straight line?
(See Problem 1.7.14.)
1.7.16 A garage employs 14 mechanics, of whom 3 are needed
on one job and, at the same time, 4 are needed on another
job. The remaining 7 are to be kept in reserve. In how
many ways can the job assignments be made?
(See Problem 1.7.14.)
1.7.17 A company has 15 applicants to interview, and 3 are to be
invited on each day of the working week. In how many
ways can the applicants be scheduled?
(See Problem 1.7.14.)
1.7.18 A quality inspector selects a sample of 12 items at
random from a collection of 60 items, of which 18 have
excellent quality, 25 have good quality, 12 have poor
quality, and 5 are defective.
(a) What is the probability that the sample only contains
items that have either excellent or good quality?
(b) What is the probability that the sample contains three
items of excellent quality, three items of good quality,
three items of poor quality, and three defective items?
1.7.19 A salesman has to visit ten different cities. In how many
different ways can the ordering of the visits be made? If
he decides that ﬁve of the visits will be made one week,
and the other ﬁve visits will be made the following week,
in how many different ways can the ten cities be split into
two groups of ﬁve cities?
1.7.20 Suppose that 5 cards are taken without replacement from
a deck of 52 cards. How many ways are there to do this so
that there are 2 red cards and 3 black cards?
1.7.21 A hand of 8 cards is chosen at random from an ordinary
deck of 52 playing cards without replacement.
(a) What is the probability that the hand does not have
any hearts?
(b) What is the probability that the hand consists of two
hearts, two diamonds, two clubs, and two spades?
1.7.22 A box contains 40 batteries, 5 of which have low
lifetimes, 30 of which have average lifetimes, and 5 of
which have high lifetimes. A consumer requires 8
batteries to run an appliance and randomly selects them
all from the box. What is the probability that among the
8 batteries ﬁtted into the consumer’s appliance, there are
exactly 2 low, 4 average and 2 high lifetimes batteries?
1.7.23 In each of 3 years a company’s revenue is classiﬁed as
being either considerably below expectation, slightly
below expectation, exactly meeting expectation, slightly
above expectation, or considerably above expectation.
How many different sequences of revenue results are
possible?
1.7.24 A marketing company is hired to manage four advertising
campaigns sequentially, one after the other. Each
advertising campaign may be canceled before launch,
launched but canceled early, launched and meets its
targeted length, or launched and extended beyond its
targeted length. How many different sequences of
campaign results are possible?
1.8
Case Study: Microelectronic Solder Joints
Suppose that using a particular production method there is a probability of 0.85 that a solder
joint has a barrel shape, there is a probability of 0.03 that a solder joint has a cylinder shape,
and there is a probability of 0.12 that a solder joint has an hourglass shape. If it is known
that a particular solder joint does not have a barrel shape, what is the probability that it has a
cylinder shape? This is a conditional probability that can be calculated as
P(cylinder|not barrel) = P(cylinder and not barrel)
P(not barrel)
=
P(cylinder)
P(cylinder) + P(hourglass)
=
0.03
0.03 + 0.12 = 0.2

1.8 CASE STUDY: MICROELECTRONIC SOLDER JOINTS
65
Furthermore, suppose that after a certain number of temperature cycles in an accelerated
life test there is a probability of 0.002 that a solder joint is cracked if it has a barrel shape, there
is a probability of 0.004 that a solder joint is cracked if it has a cylinder shape, and there is a
probability of 0.005 that a solder joint is cracked if it has an hourglass shape. This information
can be represented by the conditional probabilities shown in Figure 1.73.
If a solder joint is known to be cracked, Bayes’ theorem can be used to calculate the
probabilities of it having each of the three shapes. For example, the probability that it has a
barrel shape is
P(barrel|cracked)
=
P(barrel)P(cracked|barrel)

P(barrel)P(cracked|barrel) + P(cylinder)P(cracked|cylinder)
+P(hourglass)P(cracked|hourglass)

=
0.85 × 0.002
(0.85 × 0.002) + (0.03 × 0.004) + (0.12 × 0.005) = 0.70248
Similarly, if a solder joint is known not to be cracked, then Bayes’ theorem can be used to
calculate the probability that it has a cylinder shape, for example, as
P(cylinder|not cracked)
=
P(cylinder)P(not cracked|cylinder)

P(barrel)P(not cracked|barrel) + P(cylinder)P(not cracked|cylinder)
+P(hourglass)P(not cracked|hourglass)

=
0.03 × 0.996
(0.85 × 0.998) + (0.03 × 0.996) + (0.12 × 0.995) = 0.02995
Figure 1.74 shows all of the shape probabilities conditional on whether the solder joint is
known to be cracked or not cracked. Notice that the probabilities in each column sum to one,
and that whereas the knowledge that the solder joint is not cracked has little effect on the
shape probabilities, the knowledge that the solder joint is cracked (which is a considerably
rarer event) has much more effect on the shape probabilities.
Finally, suppose that an assembly consists of 16 solder joints and that unknown to the
researcher 5 of these solder joints are cracked. If the researcher randomly chooses a sample
of 4 of the solder joints for inspection, then the state space of the number of cracked joints in
FIGURE 1.73
Conditional probabilities of
cracking for solder joints
P(cracked|barrel) = 0.002
P(not cracked|barrel) = 0.998
P(cracked|cylinder) = 0.004
P(not cracked|cylinder) = 0.996
P(cracked|hourglass) = 0.005
P(not cracked|hourglass) = 0.995
FIGURE 1.74
Shape probabilities conditional on
whether the solder joint is cracked
or not
No information on whether
Solder joint
Solder joint
the solder joint is
is known to be
is known not to be
cracked or not
cracked
cracked
P(barrel) = 0.85
P(barrel|cracked) = 0.70248
P(barrel|not cracked) = 0.85036
P(cylinder) = 0.03
P(cylinder|cracked) = 0.04959
P(cylinder|not cracked) = 0.02995
P(hourglass) = 0.12
P(hourglass|cracked) = 0.24793
P(hourglass|not cracked) = 0.11969

66
CHAPTER 1
PROBABILITY THEORY
the sample is {0, 1, 2, 3, 4}. The total number of different samples that can be chosen is

16
4

=
16!
12!4! = 1820
and the probability that there will be exactly two cracked solder joints in the researcher’s
sample is

5
2

×

11
2


16
4

= 10 × 55
1820
= 0.302
This hypergeometric distribution is discussed more comprehensively in Section 3.3.
1.9
Case Study: Internet Marketing
When a organisation’s website is accessed, there is a probability of 0.07 that the web ad-
dress was typed in directly. In such a case, there is a probability of 0.08 that an online purchase
will be made. On the other hand, when the website is accessed indirectly, which occurs with
probability 0.93, then there is only a 0.01 chance that an online purchase will be made.
The probability that the website is accessed directly and that a purchase is made is
P(direct access) × P(purchase|direct access) = 0.07 × 0.08 = 0.0056
Similarly, the probability that the website is accessed indirectly and that a purchase is made is
P(indirect access) × P(purchase|indirect access) = 0.93 × 0.01 = 0.0093
What proportion of online purchases are from individuals who access the website directly?
Using Bayes’ Theorem this is calculated as
P(direct access) × P(purchase|direct access)
(P(direct access) × P(purchase|direct access)) + (P(indirect access) × P(purchase|indirect access))
=
0.0056
0.0056 + 0.0093 = 0.376
so that the proportion is 37.6%.
1.10
Supplementary Problems
1.10.1 What is the sample space for the average score of two
dice?
1.10.2 What is the sample space when a winner and a runner-up
are chosen in a tournament with four contestants.
1.10.3 A biased coin is known to have a greater probability of
recording a head than a tail. How can it be used to
determine fairly which team in a football game has the
choice of kick-off?
1.10.4 If two fair dice are thrown, what is the probability that
their two scores differ by no more than one?
1.10.5 If a card is chosen at random from a pack of cards,
what is the probability of choosing a diamond picture
card?
1.10.6 Two cards are drawn from a pack of cards. Is it more
likely that two hearts will be drawn when the drawing is
with replacement or without replacement?
1.10.7 Two fair dice are thrown. A is the event that the sum of
the scores is no larger than four, and B is the event that
the two scores are identical. Calculate the probabilities:
(a) A ∩B
(b) A ∪B
(c) A′ ∪B

1.10
SUPPLEMENTARY PROBLEMS
67
FIGURE 1.75
Switch diagram
1
2
3
?
4
5
0.85
0.85
0.85
0.85
0.85
1.10.8 Two fair dice are thrown, one red and one blue. Calculate:
(a) P(red die is 5|sum of scores is 8)
(b) P(either die is 5|sum of scores is 8)
(c) P(sum of scores is 8|either die is 5)
1.10.9 Consider the network shown in Figure 1.75 with
ﬁve switches. Suppose that the switches operate
independently and that each switch allows a message
through with a probability of 0.85. What is the probability
that a message will ﬁnd a route through the network?
1.10.10 Which is more likely: obtaining at least one head in two
tosses of a fair coin, or at least two heads in four tosses
of a fair coin?
1.10.11 Bag 1 contains six red balls, seven blue balls, and three
green balls. Bag 2 contains eight red balls, eight blue
balls, and two green balls. Bag 3 contains two red balls,
nine blue balls, and eight green balls. Bag 4 contains
four red balls, seven blue balls, and no green balls.
Bag 1 is chosen with a probability of 0.15, bag 2 with a
probability of 0.20, bag 3 with a probability of 0.35, and
bag 4 with a probability of 0.30, and then a ball is chosen
at random from the bag. Calculate the probabilities:
(a) A blue ball is chosen.
(b) Bag 4 was chosen if the ball is green.
(c) Bag 1 was chosen if the ball is blue.
1.10.12 A fair die is rolled. If an even number is obtained, then
that is the recorded score. However, if an odd number is
obtained, then a fair coin is tossed. If a head is obtained,
then the recorded score is the number on the die, but if a
tail is obtained, then the recorded score is twice the
number on the die.
(a) Give the possible values of the recorded score.
(b) What is the probability that a score of ten is
recorded?
(c) What is the probability that a score of three is
recorded?
(d) What is the probability that a score of six is
recorded?
(e) What is the probability that a score of four is
recorded if it is known that the coin is tossed?
(f) If a score of six is recorded, what is the probability
that an odd number was obtained on the die?
1.10.13 How many sequences of length 4 can be made when
each component of the sequence can take ﬁve different
values? How many sequences of length 5 can be made
when each component of the sequence can take four
different values? In general, if 3 ≤n1 < n2, are there
more sequences of length n1 with n2 possible values for
each component, or more sequences of length n2 with n1
possible values for each component?
1.10.14 Twenty copying jobs need to be done. If there are four
copy machines, in how many ways can ﬁve jobs be
assigned to each of the four machines? If an additional
copier is used, in how many ways can four jobs be
assigned to each of the ﬁve machines?
1.10.15 A bag contains two counters with each independently
equally likely to be either black or white. What is the
distribution of X, the number of white counters in the
bag? Suppose that a white counter is added to the bag
and then one of the three counters is selected at random
and taken out of the bag. What is the distribution of X
conditional on the counter taken out being white? What
if the counter taken out of the bag is black?
1.10.16 It is found that 28% of orders received by a company are
from ﬁrst-time customers, with the other 72% coming
from repeat customers. In addition, 75% of the orders
from ﬁrst-time customers are dispatched within one day,
and overall 30% of the company’s orders are from repeat
customers whose orders are not dispatched within
one day. If an order is dispatched within one day, what is
the probability that it was for a ﬁrst-time customer?

68
CHAPTER 1
PROBABILITY THEORY
1.10.17 When asked to select their favorite opera work, 26% of
the respondents selected a piece by Puccini, and 22%
of the respondents selected a piece by Verdi. Moreover,
59% of the respondents who selected a piece by Puccini
were female, and 45% of the respondents who selected a
piece by Verdi were female. Altogether, 62% of the
respondents were female.
(a) If a respondent selected a piece that is by neither
Puccini nor Verdi, what is the probability that the
respondent is female?
(b) What proportion of males selected a piece by
Puccini?
1.10.18 A random sample of 10 ﬁbers is taken from a collection
of 92 ﬁbers that consists of 43 ﬁbers of polymer A,
17 ﬁbers of polymer B, and 32 ﬁbers of polymer C.
(a) What is the probability that the sample does not
contain any ﬁbers of polymer B?
(b) What is the probability that the sample contains
exactly one ﬁber of polymer B?
(c) What is the probability that the sample contains
three ﬁbers of polymer A, three ﬁbers of polymer B,
and four ﬁbers of polymer C?
1.10.19 A fair coin is tossed ﬁve times. What is the probability
that there is not a sequence of three outcomes of the
same kind?
1.10.20 Consider telephone calls made to a company’s
complaint line. Let A be the event that the call is
answered within 10 seconds. Let B be the event that the
call is answered by one of the company’s experienced
telephone operators. Let C be the event that the call
lasts less than 5 minutes. Let D be the event that the
complaint is handled successfully by the telephone
operator. Describe the following events.
(a) B ∩C′
(b) (A ∪B′) ∩D
(c) A′ ∩C′ ∩D′
(d) (A ∩C) ∪(B ∩D)
1.10.21 A manager has 20 different job orders, of which 7 must
be assigned to production line I, 7 must be assigned to
production line II, and 6 must be assigned to production
line III.
(a) In how many ways can the assignments be made?
(b) If the ﬁrst job and the second job must be assigned
to the same production line, in how many ways can
the assignments be made?
(c) If the ﬁrst job and the second job cannot be assigned
to the same production line, in how many ways can
the assignments be made?
1.10.22 A hand of 3 cards (without replacement) is chosen at
random from an ordinary deck of 52 playing cards.
(a) What is the probability that the hand contains only
diamonds?
(b) What is the probability that the hand contains one
ace, one king, and one queen?
1.10.23 A hand of 4 cards (without replacement) is chosen
at random from an ordinary deck of 52 playing
cards.
(a) What is the probability that the hand does not have
any aces?
(b) What is the probability that the hand has exactly one
ace?
Suppose now that the 4 cards are taken with
replacement.
(c) What is the probability that the same card is
obtained four times?
1.10.24 Are the following statements true or false?
(a) If a fair coin is tossed three times, the probability of
obtaining two heads and one tail is the same as the
probability of obtaining one head and two tails.
(b) If a card is drawn at random from a deck of cards,
the probability that it is a heart increases if it is
conditioned on the knowledge that it is an ace.
(c) The number of ways of choosing ﬁve different
letters from the alphabet is more than the number of
seconds in a year.
(d) If two events are independent, then the probability
that they both occur can be calculated by
multiplying their individual probabilities.
(e) It is always true that P(A|B) + P(A′|B) = 1.
(f) It is always true that P(A|B) + P(A|B′) = 1.
(g) It is always true that P(A|B) ≤P(A).
1.10.25 There is a probability of 0.55 that a soccer team will win
a game. There is also a probability of 0.85 that the
soccer team will not have a player sent off in the game.
However, if the soccer team does not have a player sent
off, then there is a probability of 0.60 that the team will
win the game. What is the probability that the team has a
player sent off but still wins the game?
1.10.26 A warehouse contains 500 machines. Each machine is
either new or used, and each machine has either good
quality or bad quality. There are 120 new machines that
have bad quality. There are 230 used machines. Suppose
that a machine is chosen at random, with each machine
being equally likely to be chosen.

1.10
SUPPLEMENTARY PROBLEMS
69
(a) What is the probability that the chosen machine is a
new machine with good quality?
(b) If the chosen machine is new, what is the probability
that it has good quality?
1.10.27 A class has 250 students, 113 of whom are male, and
167 of whom are mechanical engineers. There are
52 female students who are not mechanical engineers.
There are 19 female mechanical engineers who are
seniors.
(a) If a randomly chosen student is not a mechanical
engineer, what is the probability that the student is a
male?
(b) If a randomly chosen student is a female mechanical
engineer, what is the probability that the student is a
senior?
1.10.28 A business tax form is either ﬁled on time or late, is
either from a small or a large business, and is either
accurate or inaccurate. There is an 11% probability that
a form is from a small business and is accurate and on
time. There is a 13% probability that a form is from a
small business and is accurate but is late. There is a 15%
probability that a form is from a small business and is on
time. There is a 21% probability that a form is from a
small business and is inaccurate and is late.
(a) If a form is from a small business and is accurate,
what is the probability that it was ﬁled on time?
(b) What is the probability that a form is from a large
business?
1.10.29 (a) If four cards are taken at random from a pack of
cards without replacement, what is the probability
of having exactly two hearts?
(b) If four cards are taken at random from a pack
of cards without replacement, what is the
probability of having exactly two hearts and
exactly two clubs?
(c) If four cards are taken at random from a pack of
cards without replacement and it is known that there
are no clubs, what is the probability that there are
exactly three hearts?
1.10.30 Applicants have a 0.26 probability of passing a test
when they take it for the ﬁrst time, and if they pass it
they can move on to the next stage. However, if they
fail the test the ﬁrst time, they must take the test a
second time, and when applicants take the test for the
second time there is a 0.43 chance that they will pass
and be allowed to move on to the next stage. Applicants
are rejected if the test is failed on the second attempt.
(a) What is the probability that an applicant moves on
to the next stage but needs two attempts at the test?
(b) What is the probability that an applicant moves on
to the next stage?
(c) If an applicant moves on to the next stage, what is
the probability that he or she passed the test on the
ﬁrst attempt?
1.10.31 A fair die is rolled ﬁve times. What is the probability
that the ﬁrst score is strictly larger than the second score,
which is strictly larger than the third score, which is
strictly larger than the fourth score, which is strictly
larger than the ﬁfth score (i.e., the ﬁve scores are strictly
decreasing).
1.10.32 A software engineer makes two backup copies of his
ﬁle, one on a CD and another on a ﬂash drive. Suppose
that there is a probability of 0.05% that the ﬁle is
corrupted when it is backed-up onto the CD, and a
probability of 0.1% that the ﬁle is corrupted when it is
backed-up onto the ﬂash drive, and that these events are
independent of each other. What is the probability that
the engineer will have at least one uncorrupted copy of
the ﬁle?
1.10.33 A warning light in the cockpit of a plane is supposed to
indicate when a hydraulic pump is inoperative. If the
pump is inoperative, then there is a probability of 0.992
that the warning light will come on. However, there is a
probability of 0.003 that the warning light will come on
even when the pump is operating correctly. Furthermore,
there is a probability of 0.996 that the pump is operating
correctly. If the warning light comes on, what is the
probability that the pump really is inoperative?
1.10.34 A hand of 10 cards is chosen at random without
replacement from a deck of 52 cards. What is the
probability that the hand contains exactly two aces,
two kings, three queens, and three jacks?
1.10.35 There are 11 items of a product on a shelf in a retail outlet,
and unknown to the customers, 4 of the items are overage.
Suppose that a customer takes 3 items at random.
(a) What is the probability that none of the overage
products are selected by the customer?
(b) What is the probability that exactly 2 of the items
taken by the customer are overage?
1.10.36 Among those people who are infected with a certain
virus, 32% have strain A, 59% have strain B, and the
remaining 9% have strain C. Furthermore, 21% of
people infected with strain A of the virus exhibit

70
CHAPTER 1
PROBABILITY THEORY
symptoms, 16% of people infected with strain B of the
virus exhibit symptoms, and 63% of people infected
with strain C of the virus exhibit symptoms.
(a) If a person has the virus and exhibits symptoms
of it, what is the probability that they have
strain C?
(b) If a person has the virus but doesn’t exhibit any
symptoms of it, what is the probability that they
have strain A?
(c) What is the probability that a person who has the
virus does not exhibit any symptoms of it?
1.10.37 The marketing division of a company proﬁles its
potential customers and grades them as either likely or
unlikely purchasers. Overall, 16% of the potential
customers are graded as likely purchasers. In reality,
81% of the potential customers graded as likely
purchasers actually make a purchase, while only 9% of
the potential customers graded as unlikely purchasers
actually make a purchase. If somebody made a purchase,
what is the probability that they had been graded as a
likely purchaser?
“When solving mysteries like this one, it’s always a question of prior probabilities
and posterior probabilities.” (From Inspector Morimoto and the Two Umbrellas,
by Timothy Hemion)

C H A P T E R T W O
Random Variables
After the general discussion of probability theory presented in Chapter 1, random variables
are introduced in this chapter. They are one of the fundamental building blocks of probability
theoryandstatisticalinference.ThebasicprobabilitytheorydevelopedinChapter1isextended
for outcomes of a numerical nature, and distinctions are drawn between discrete random
variables and continuous random variables.
Useful properties of random variables are discussed, and summary measures such as the
mean and variance are considered, together with combinations of several random variables.
Speciﬁc examples of common families of random variables are given in Chapters 3, 4, and 5.
2.1
Discrete Random Variables
2.1.1
Deﬁnition of a Random Variable
A random variable is formed by assigning a numerical value to each outcome in the sample
space of a particular experiment. The state space of the random variable consists of these
numerical values. Technically, a random variable can be thought of as being generated from
a function that maps each outcome in a particular sample space onto the real number line R,
as illustrated in Figure 2.1.
Random Variables
A random variable is obtained by assigning a numerical value to each outcome of a
particular experiment.
A random variable is therefore a special kind of experiment in which the outcomes are
numerical values, either positive or negative or possibly zero. Sometimes the experimental
outcomes are already numbers, and then these may just be used to deﬁne the random variable.
In other cases the experimental outcomes are not numerical, and then a random variable may
be deﬁned by assigning “scores” or “costs” to the outcomes.
Example 1
Machine Breakdowns
The sample space for the machine breakdown problem is
S = {electrical, mechanical, misuse}
and each of these failures may be associated with a repair cost. For example, suppose that
electrical failures generally cost an average of $200 to repair, mechanical failures have an
average repair cost of $350, and operator misuse failures have an average repair cost of only
$50. These repair costs generate a random variable cost, as illustrated in Figure 2.2, which
has a state space of {50, 200, 350}.
71

72
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.1
A random variable is formed by
assigning a numerical value to each
outcome in a sample space
.  .  .
−1
−2
−3
1
2
3
0
.  .  .
R
S
FIGURE 2.2
The random variable “cost” for
machine breakdowns
0
50
200
350
Misuse
Electrical
Mechanical
0.3
0.2
0.5
Cost ($)
S
Notice that cost is a random variable because its values 50, 200, and 350 are numbers. The
breakdown cause, deﬁned to be electrical, mechanical, or operator misuse, is not considered
to be a random variable because its values are not numerical.
Example 4
Power Plant Operation
Figure 1.15 illustrates the sample space for the power plant example, where the outcomes
designate which of the three power plants are generating electricity (1) and which are idle (0).
Suppose that interest is directed only at the number of plants that are generating electricity.
This creates a random variable
X = number of power plants generating electricity
which can take the values 0, 1, 2, and 3, as shown in Figure 2.3.
Example 12
Personnel Recruitment
A company has one position available for which eight applicants have made the short list. The
company’sstrategyistointerviewtheapplicantssequentiallyandtomakeanofferimmediately

2.1 DISCRETE RANDOM VARIABLES
73
FIGURE 2.3
X= number of power plants
generating electricity
S
0
1
2
3
(0, 0, 0)
(1, 0, 0)
(0, 0, 1)
(1, 0, 1)
(0, 1, 0)
(1, 1, 0)
(0, 1, 1)
(1, 1, 1)
0.07
0.16
0.04
0.18
0.03
0.18
0.21
0.13
X
to anyone they feel is outstanding (without interviewing the additional applicants). If none
of the ﬁrst seven applicants interviewed is judged to be outstanding, the eighth applicant is
interviewed and then the best of the eight applicants is offered the job.
The company is interested in how many applicants will need to be interviewed under this
strategy. A random variable
X = number of applicants interviewed
can be deﬁned taking the values 1, 2, 3, 4, 5, 6, 7, and 8.
Example 13
Factory Floor
Accidents
For safety and insurance purposes, a factory manager is interested in how many factory ﬂoor
accidents occur in a given year. A random variable
X = number of accidents
can be deﬁned, which can hypothetically take the value 0 or any positive integer.
GAMES OF CHANCE
The score obtained from the roll of a die can be thought of as a random variable taking the
values 1 to 6. If two dice are rolled, a random variable can be deﬁned to be the sum of the scores,
taking the values 2 to 12. Figure 2.4 illustrates a random variable deﬁned to be the positive
difference between the scores obtained from two dice, taking the values 0 to 5.
It is usual to refer to random variables generically with uppercase letters such as X, Y,
or Z. The values taken by the random variables are then labeled with lowercase letters. For
example, it may be stated that a random variable X takes the values x = −0.6, x = 2.3,
and x = 4.0. This custom helps clarify whether the random variable is being referred to, or a
particular value taken by the random variable, and it is helpful in the subsequent mathematical
discussions.

74
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.4
X= positive difference between the
scores of two dice
3
4
2
0
1
5
(5, 5)
(6, 6)
(5, 6)
(6, 5)
(4, 5)
(4, 6)
(4, 4)
(6, 4)
(5, 4)
(3, 4)
(3, 6)
(3, 5)
(3, 3)
(6, 3)
(5, 3)
(4, 3)
(2, 3)
(6, 2)
(5, 2)
(4, 2)
(3, 2)
(2, 6)
(2, 5)
(2, 4)
(2, 2)
(1, 2)
(1, 6)
(1, 5)
(1, 4)
(1, 3)
(1, 1)
(6, 1)
(5, 1)
(4, 1)
(3, 1)
(2, 1)
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
1/36
X
S
The examples given above all concern discrete random variables as opposed to continuous
random variables, which are discussed in the next section. A continuous random variable is
one that may take any value within a continuous interval. For example, a random variable
that can take any value between 0 and 1 is a continuous random variable. Mathematically
speaking, continuous random variables can take uncountably many values.
In contrast, discrete random variables can take only certain discrete values, as their name
suggests. There may be only a ﬁnite number of values, as in Examples 1, 4, and 12, or
inﬁnitely many values, as in Example 13. The distinction between discrete and continuous
random variables can most easily be understood by comparing the previous examples with the
examples of continuous random variables in the next section. The distinction is important since
the probability properties of discrete and continuous random variables need to be handled in
two different ways.
2.1.2
Probability Mass Function
The probability properties of a discrete random variable are based upon the assignment of a
probability value pi to each of the values xi taken by the random variable. These probability
values, which are known as the probability mass function of the random variable, must each
be between 0 and 1 and must sum to 1.

2.1 DISCRETE RANDOM VARIABLES
75
Probability Mass Function
The probability mass function (p.m.f.) of a random variable X is a set of probability
values pi assigned to each of the values xi taken by the discrete random variable.
These probability values must satisfy 0 ≤pi ≤1 and 
i pi = 1. The probability that
the random variable takes the value xi is said to be pi, and this is written
P(X = xi) = pi.
The probabilistic properties of a discrete random variable are deﬁned by specifying its
probability mass function, that is, by specifying what values the random variable can take and
the probability values of its taking each of those values. The probability mass function is also
referred to as the distribution of the random variable, and the abbreviation p.m.f. is often used.
The probability mass function may typically be given in either tabular or graphical form
as illustrated in the following examples. In addition, it will be seen in Chapter 3 that some
common and useful discrete random variables have their probability mass functions speciﬁed
by succinct formulas.
Example 1
Machine Breakdowns
It follows from Figure 2.2 that P(cost = 50) = 0.3, P(cost = 200) = 0.2, and P(cost =
350) = 0.5. This probability mass function is given in tabular form in Figure 2.5 and as a line
graph in Figure 2.6.
Example 4
Power Plant Operation
The probability mass function for the number of plants generating electricity can be in-
ferred from Figure 2.3 and is given in Figures 2.7 and 2.8. For example, the probability
50
0.3
200
350
0.2
0.5
pi
xi
FIGURE 2.5
Tabular presentation of the
probability mass function for
machine breakdown costs
Probability
Cost ($)
50
200
350
0.3
0.2
0.5
FIGURE 2.6
Line graph of the probability mass function for machine breakdown
costs
x
p
i
i
0
1
2
3
0.07 0.23 0.57 0.13
FIGURE 2.7
Tabular presentation of the probability
mass function for power plant example
0
1
2
3
Probability
Number of power plants
0.07
0.23
0.57
0.13
FIGURE 2.8
Line graph of the probability mass function for power plant example

76
CHAPTER 2
RANDOM VARIABLES
that no plants are generating electricity (X = 0) is simply the probability of the outcome
(0, 0, 0), namely 0.07. The probability that exactly one plant is generating electricity (X = 1)
is the sum of the probabilities of the outcomes (1, 0, 0), (0, 1, 0), and (0, 0, 1), which is
0.04 + 0.03 + 0.16 = 0.23.
Example 13
Factory Floor
Accidents
Suppose that the probability of having x accidents is
P(X = x) =
1
2x+1
This is a valid probability mass function since
∞

x = 0
P(X = x) =
∞

x = 0
1
2x+1 = 1
2 + 1
4 + 1
8 + 1
16 + · · · = 1
so that the probability values sum to 1 (recall that in general for 0 < p < 1, p+ p2+ p3+· · · =
p/(1−p)). A line graph of this probability mass function is given in Figure 2.9. It is a special
case of the geometric distribution discussed in further detail in Chapter 3.
GAMES OF CHANCE
The probability mass function for the positive difference between the scores obtained from
two dice can be inferred from Figure 2.4 and is given in Figures 2.10 and 2.11. For example,
since there are four outcomes, (1, 5), (5, 1), (2, 6), and (6, 2), for which the positive difference
FIGURE 2.9
Line graph of the probability mass
function for factory ﬂoor accidents
example
0
1
2
3
4
5
Probability
.  .  .
.  .  .
1/2
1/4
1/8
1/32
1/64
1/16
x
xi
pi
0
1
2
3
4
5
1/6
5/18 2/9
1/6
1/9 1/18
FIGURE 2.10
Tabular presentation of the probability
mass function for dice example
1/6
5/18
2/9
1/6
1/9
1/18
0
1
2
3
4
5
Probability
x
FIGURE 2.11
Line graph of the probability mass function for dice example

2.1 DISCRETE RANDOM VARIABLES
77
is equal to 4, and each is equally likely with a probability of 1/36, the probability of having
a positive difference equal to 4 is P(X = 4) = 4/36 = 1/9.
2.1.3
Cumulative Distribution Function
An alternative way of specifying the probabilistic properties of a random variable X is through
the function
F(x) = P(X ≤x)
which is known as the cumulative distribution function, for which the abbreviation c.d.f. is
often used. One advantage of this function is that it can be used for both discrete and continuous
random variables.
Cumulative Distribution Function
The cumulative distribution function (c.d.f.) of a random variable X is the function
F(x) = P(X ≤x)
Like the probability mass function, the cumulative distribution function summarizes the
probabilistic properties of a random variable. Knowledge of either the probability mass func-
tion or the cumulative distribution function allows the other function to be calculated.
For example, suppose that the probability mass function is known. The cumulative distri-
bution function can then be calculated from the expression
F(x) =

y:y≤x
P(X = y)
In other words, the value of F(x) is constructed by simply adding together the probabilities
P(X = y) for values y that are no larger than x.
The cumulative distribution function F(x) is an increasing step function with steps at
the values taken by the random variable. The heights of the steps are the probabilities of
taking these values. Mathematically, the probability mass function can be obtained from the
cumulative distribution function through the relationship
P(X = x) = F(x) −F(x−)
where F(x−) is the limiting value from below of the cumulative distribution function. If
there is no step in the cumulative distribution function at a point x, then F(x) = F(x−)
and P(X = x) = 0. If there is a step at a point x, then F(x) is the value of the cumulative
distribution function at the top of the step, and F(x−) is the value of the cumulative distribution
function at the bottom of the step, so that P(X = x) is the height of the step.
These relationships are illustrated in the following examples.
Example 1
Machine Breakdowns
The probability mass function given in Figures 2.5 and 2.6 can be used to construct the
cumulative distribution function as follows:
−∞< x < 50 ⇒F(x) = P(cost ≤x) = 0
50 ≤x < 200 ⇒F(x) = P(cost ≤x) = 0.3
200 ≤x < 350 ⇒F(x) = P(cost ≤x) = 0.3 + 0.2 = 0.5
350 ≤x < ∞⇒F(x) = P(cost ≤x) = 0.3 + 0.2 + 0.5 = 1.0
This cumulative distribution function is illustrated in Figure 2.12.

78
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.12
Cumulative distribution function
for machine breakdown costs
1.0
0.5
0.3
50
200
350
0
F(x)
Cost ($)
x
FIGURE 2.13
Cumulative distribution function
for power plant example
0
F(x)
x
0.07
0.30
0.87
1.00
1
2
3
Notice that the cumulative distribution function is a step function that starts at a value of
0 for small values of x and increases to a value of 1 for large values of x. The steps occur
at the points x = 50, x = 200, and x = 350, which are the possible values of the cost, and the
sizes of the steps at these points 0.3, 0.2, and 0.5 are simply the values of the probability mass
function.
Example 4
Power Plant Operation
The cumulative distribution function for the number of plants generating electricity can be
inferred from the probability mass function given in Figure 2.7 and is given in Figure 2.13.
For example, the probability that no more than one plant is generating electricity is simply
F(1) = P(X ≤1) = P(X = 0) + P(X = 1) = 0.07 + 0.23 = 0.30
Example 12
Personnel Recruitment
Suppose that Figure 2.14 provides the cumulative distribution function for the random variable
X, the number of applicants interviewed, which is graphed in Figure 2.15. The probability
mass function of X can be obtained by measuring the heights of the steps of the cumulative
distribution function. For example,
P(X = 1) = F(1) −F(1−) = 0.18 −0.00 = 0.18
P(X = 2) = F(2) −F(2−) = 0.28 −0.18 = 0.10
A line graph of the probability mass function is presented in Figure 2.16.

2.1 DISCRETE RANDOM VARIABLES
79
F(x)
−∞< x < 1
0.00
1 ≤x < 2
0.18
2 ≤x < 3
0.28
3 ≤x < 4
0.37
4 ≤x < 5
0.43
5 ≤x < 6
0.48
6 ≤x < 7
0.52
7 ≤x < 8
0.54
8 ≤x < ∞
1.00
FIGURE 2.14
Tabular representation of the
cumulative distribution function for
personnel recruitment example
1
F(x)
x
2
3
4
5
6
7
8
1.00
0.54
0.48
0.43
0.37
0.28
0.18
0.52
FIGURE 2.15
Graphical representation of the cumulative distribution function for personnel recruitment
example
FIGURE 2.16
Line graph of the probability mass
function for personnel recruitment
example
Probability
0.18
0.10
0.09
0.05
0.04
0.02
0.46
0.06
1
2
3
4
5
6
7
8
x
Notice that the probability values P(X = 1) to P(X = 7) are decreasing, but the most
likely outcome is that all eight applicants need to be interviewed. This could happen if the
company interviews the applicants in order of decreasing merit based upon prior information.
The chance of an applicant being judged as good enough to be offered the job immediately
then decreases the further down the list that he or she is.

80
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.17
Cumulative distribution function
for dice example
0
F(x)
x
1
2
3
4
5
1
17/18
15/18
12/18
8/18
3/18
GAMES OF CHANCE
The cumulative distribution function for the positive difference between the scores obtained
from two dice can be inferred from the probability mass function given in Figure 2.10 and is
graphed in Figure 2.17. For example, the probability that the difference is no larger than 2 is
F(2) = P(X ≤2) = P(X = 0) + P(X = 1) + P(X = 2) = 1
6 + 5
18 + 2
9 = 2
3
2.1.4
Problems
2.1.1 An ofﬁce has four copying machines, and the random
variable X measures how many of them are in use
at a particular moment in time. Suppose that
P(X = 0) = 0.08, P(X = 1) = 0.11, P(X = 2) = 0.27,
and P(X = 3) = 0.33.
(a) What is P(X = 4)?
(b) Draw a line graph of the probability mass function.
(c) Construct and plot the cumulative distribution function.
(This problem is continued in Problems 2.3.1 and 2.4.2.)
2.1.2 Figure 2.18 presents the cumulative distribution function
of a random variable. Make a table and line graph of its
probability mass function.
2.1.3 Suppose that two fair dice are rolled and that the two
numbers recorded are multiplied to obtain a ﬁnal score.
Construct and plot the probability mass function and the
cumulative distribution function of the ﬁnal score. (This
problem is continued in Problem 2.3.2.)
2.1.4 Two cards are drawn at random from a pack of cards with
replacement. Let the random variable X be the number of
cards drawn from the heart suit.
(a) Construct the probability mass function.
(b) Construct the cumulative distribution function.
F(x)
−∞≤x < −4
0.00
−4 ≤x < −1
0.21
−1 ≤x < 0
0.32
0 ≤x < 2
0.39
2 ≤x < 3
0.68
3 ≤x < 7
0.81
7 ≤x < ∞
1.00
FIGURE 2.18
Cumulative distribution function of a random variable
(c) What is the most likely value of the random
variable X?
Repeat parts (a)–(c) when the second drawing is made
without replacement. (This problem is continued in
Problem 2.3.3.)
2.1.5 Two fair dice, one red and one blue, are rolled. A score is
calculated to be twice the value of the blue die if the red
die has an even value, and to be the value of the red die

2.2 CONTINUOUS RANDOM VARIABLES
81
minus the value of the blue die if the red die has an odd
value. Construct and plot the probability mass function and
the cumulative distribution function of the score.
2.1.6 A fair coin is tossed three times. A player wins $1 if the
ﬁrst toss is a head, but loses $1 if the ﬁrst toss is a tail.
Similarly, the player wins $2 if the second toss is a head,
but loses $2 if the second toss is a tail, and wins or
loses $3 according to the result of the third toss. Let the
random variable X be the total winnings after the three
tosses (possibly a negative value if losses are incurred).
(a) Construct the probability mass function.
(b) Construct the cumulative distribution function.
(c) What is the most likely value of the random
variable X?
2.1.7 Consider Example 5 and the probability values given in
Figure 1.38. The company has decided that each television
set should be given a quality score calculated in the
following manner. A perfect picture scores 4, a good
picture scores 2, a satisfactory picture scores 1, and a failed
picture scores 0. Also, a perfect appearance scores 3, a
good appearance scores 2, a satisfactory appearance scores
1, and a failed appearance scores 0. The ﬁnal quality score
is obtained by multiplying the picture score and the
appearance score. For example, the outcome (P, G),
where the picture is perfect and the appearance is good,
would be assigned a quality score of 4 × 2 = 8.
(a) Construct the probability mass function of the quality
score.
(b) Construct the cumulative distribution function of the
quality score.
(c) What is the most likely value of the quality score?
Recall that an appliance is not shipped if it fails on either
the picture or the appearance evaluation, or if it is
evaluated as only satisfactory in both cases. How can the
probability that an appliance is not shipped be obtained
from the cumulative distribution function?
2.1.8 Four cards are labeled $1, $2, $3, and $6. A player pays $4,
selects two cards at random, and then receives the sum of
the winnings indicated on the two cards. Calculate the
probability mass function and the cumulative distribution
function of the net winnings (that is, winnings minus the
$4 payment).
2.1.9 A company has ﬁve warehouses, only two of which have
a particular product in stock. A salesperson calls the ﬁve
warehouses in a random order until a warehouse with the
product is reached. Let the random variable X be the
number of calls made by the salesperson, and calculate its
probability mass function and cumulative distribution
function. (This problem is continued in Problems 2.3.4
and 2.4.3.)
2.1.10 Suppose that a random variable X can take the value 1, 2,
or any other positive integer. Is it possible that
P(X = i) = c
i2
for some value of the constant c? Is it possible that
P(X = i) = c
i
for some value of c?
2.1.11 A consultant has six appointment times that are open,
three on Monday and three on Tuesday. Suppose that when
making an appointment a client randomly chooses one of
the remaining open times, with each of those open times
equally likely to be chosen. Let the random variable X be
the total number of appointments that have already been
made over both days at the moment when Monday’s
schedule has just been completely ﬁlled.
(a) What is the state space of the random variable X?
(b) Calculate the probability mass function and the
cumulative distribution function of X.
(This problem is continued in Problems 2.3.16 and 2.4.12.)
2.2
Continuous Random Variables
2.2.1
Examples of Continuous Random Variables
Random variables are classiﬁed as either discrete or continuous according to the set of values
that they can take. Continuous random variables can take any value within a continuous region,
as illustrated in the following examples.
Example 14
Metal Cylinder
Production
A company manufactures metal cylinders that are used in the construction of a particular type
of engine. These cylinders, which must slide freely within an outer casing, are designed to
have a diameter of 50 mm. The company discovers, however, that the cylinders it manufactures
can have a diameter anywhere between 49.5 and 50.5 mm.

82
CHAPTER 2
RANDOM VARIABLES
Suppose that the random variable X is the diameter of a randomly chosen cylinder man-
ufactured by the company. Since this random variable can take any value between 49.5 and
50.5, it is a continuous random variable.
Example 15
Battery Failure Times
Suppose that a random variable X is the time to failure of a newly charged battery. Failure
can be deﬁned to be the moment at which the battery can no longer supply enough energy
to operate a certain appliance. This random variable is continuous since it can hypothetically
take any positive value. Its state space can be thought of as the interval [0, ∞).
Example 16
Concrete Slab Breaking
Strengths
The strength of a concrete slab is measured by a machine that applies increasing amounts of
pressure to the center of the slab. The pressure at which the slab ﬁrst cracks is then said to be
the breaking strength of the concrete. If X is a random variable corresponding to the breaking
strength of a randomly chosen concrete slab, then it is a continuous random variable taking
any value between certain practical limits.
Example 17
Milk Container
Contents
A machine-ﬁlled milk container is labeled as containing 2 liters. However, the actual amount
of milk deposited into the container by the ﬁlling machine varies between 1.95 and 2.20 liters.
If the random variable X measures the amount of milk in a randomly chosen container, it is a
continuous random variable taking any value in the interval [1.95, 2.20].
GAMES OF CHANCE
Suppose that a dial is spun and the angle θ that it makes with a ﬁxed mark is measured once
it has come to a halt, as illustrated in Figure 2.19. Let the angle θ be measured so that it
lies between 0◦and 180◦. The value of θ obtained from a spin is then a continuous random
variable taking any value within the interval [0, 180].
Also, suppose that a player spins the dial and then wins an amount corresponding to
$1000 ×
θ
180
Then it is convenient to consider the amount won as a continuous random variable taking
values within the interval [0, 1000].
An astute reader may have noticed that the distinction between discrete and continuous
random variables is sometimes not all that clear. For instance, in Example 15 above, if the
failure time is recorded to the nearest minute or nearest hour, then it may be thought of as
a discrete random variable taking a positive integer value. Similarly, in the spinning game
above, if the angle θ is measured to the nearest degree, then it can be thought of as being a
discrete random variable taking one of the values 0, 1, 2, . . . , 180. In addition, the monetary
amount won in the game would actually be a discrete number of cents.
FIGURE 2.19
Dial-spinning game

2.2 CONTINUOUS RANDOM VARIABLES
83
In practice, the real distinction between discrete and continuous random variables lies in
how their probabilistic properties are deﬁned. Whereas the probabilistic properties of discrete
random variables are deﬁned through a probability mass function, as described in the previous
section, the probabilistic properties of a continuous random variable are deﬁned through a
probability density function, as described below.
Finally,itisusefultonotethatadiscreterandomvariablecanbeobtainedfromacontinuous
random variable by grouping the elements of the state space in a certain manner. For example,
the milk container contents in Example 17 may be deﬁned to be inadequate if the contents
are less than 2.0 liters, adequate if the contents are between 2.0 and 2.1 liters, and excessive if
the contents are greater than 2.1 liters. If some scores are assigned to these three categories,
then a discrete random variable has been generated that can take three values.
2.2.2
Probability Density Function
The probabilistic properties of a continuous random variable are deﬁned through a function
f (x) known as the probability density function, for which the abbreviation p.d.f. is often
used. The probability that the random variable lies between two values a and b is obtained by
integrating the probability density function between these two values, so that
P(a ≤X ≤b) =
 b
a
f (x) dx
This probability is the area under the probability density function between the points a and b
as illustrated in Figure 2.20. A valid probability density function f (x) cannot take negative
values and must integrate to one over the whole sample space, so that the total probability is
equal to 1.
Probability Density Function
A probability density function f (x) deﬁnes the probabilistic properties of a
continuous random variable. It must satisfy f (x) ≥0 and

state space
f (x) dx = 1
The probability that the random variable lies between two values is obtained by
integrating the probability density function between the two values.
FIGURE 2.20
P(a ≤x ≤b) is the area under the
probability density function f (x)
between the points a and b
a
b
x
f(x)

84
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.21
Probability density function for
metal cylinder diameters
49.5
50.5
x
Cylinder diameter (mm)
f (x) = 
1.5 − 6(x − 50.0)2
It is useful to notice that the probability that a continuous random variable X takes any
speciﬁc value a is always 0! Technically, this can be seen by noting that
P(X = a) =
 a
a
f (x) dx = 0
Of course, this is in contrast to discrete random variables, which can have nonzero probabil-
ities of taking speciﬁc values. Actually, this may be the best way of distinguishing between
discrete and continuous random variables. Even though continuous random variables have
zero probabilities of taking speciﬁc values, they can have nonzero probabilities of falling
within certain continuous regions.
Example 14
Metal Cylinder
Production
Suppose that the diameter of a metal cylinder has a probability density function
f (x) = 1.5 −6(x −50.0)2
for 49.5 ≤x ≤50.5 and f (x) = 0 elsewhere, as shown in Figure 2.21. This is a valid
probability density function because it is positive within the state space [49.5, 50.5] and
because
 50.5
49.5
(1.5 −6(x −50.0)2) dx = [1.5x −2(x −50.0)3]50.5
49.5
= [1.5 × 50.5 −2(50.5 −50.0)3]
−[1.5 × 49.5 −2(49.5 −50.0)3]
= 75.5 −74.5 = 1.0
The probability that a metal cylinder has a diameter between 49.8 and 50.1 mm can be
calculated to be
 50.1
49.8
(1.5 −6(x −50.0)2) dx = [1.5x −2(x −50.0)3]50.1
49.8
= [1.5 × 50.1 −2(50.1 −50.0)3]
−[1.5 × 49.8 −2(49.8 −50.0)3]
= 75.148 −74.716 = 0.432

2.2 CONTINUOUS RANDOM VARIABLES
85
49.5
50.5
x
Cylinder diameter (mm)
49.8
50.1
f(x) = 
1.5 − 6(x − 50.0)2
FIGURE 2.22
Probability that a metal cylinder diameter is between 49.8 and
50.1 mm
Failure time (hrs)
x
0
7
1
2
3
4
5
6
2
1
3
f x( ) =
2
+1
(x
)
FIGURE 2.23
Probability density function for battery failure times and the area corresponding
to the probability of failing within ﬁve hours
as illustrated in Figure 2.22. Consequently, about 43% of the cylinders will have diameters
within these limits.
Example 15
Battery Failure Times
Suppose that the battery failure time, measured in hours, has a probability density function
given by
f (x) =
2
(x + 1)3
for x ≥0 and f (x) = 0 for x < 0. This is a valid probability density function because it is
positive and because
 ∞
0
2
(x + 1)3 dx =

−1
(x + 1)2
∞
0
= [0] −[−1] = 1
The probability density function is illustrated in Figure 2.23 along with the area representing
the probability thatthebatteryfailswithintheﬁrstﬁvehours.Thisprobabilitycanbe calculated
to be
P(0 ≤X ≤5) =
 5
0
2
(x + 1)3 dx =

−1
(x + 1)2
5
0
=
−1
36

−
−1
1

= 35
36
The probability that a battery lasts longer than ﬁve hours is consequently 1/36.

86
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.24
Probability density function for
milk container contents and the
area corresponding to the
probability of underweight
containers
Underweight
Overweight
Milk content (liters)
1.95
2.00
2.10
2.20
x
−x
f (x) = 
40.976 − 16x − 30e
Example 17
Milk Container
Contents
Suppose that the probability density function of the amount of milk deposited in a milk
container is
f (x) = 40.976 −16x −30e−x
for 1.95 ≤x ≤2.20 and f (x) = 0 elsewhere. This is a valid probability density function
since it is positive within the state space [1.95, 2.20] and
 2.20
1.95
(40.976 −16x −30e−x) dx = [40.976x −8x2 + 30e−x]2.20
1.95
= 54.751 −53.751 = 1
Figure 2.24 illustrates the area that corresponds to the probability that the actual amount
of milk is less than the advertised 2.00 liters. The area can be calculated to be
 2.00
1.95
(40.976 −16x −30e−x) dx = [40.976x −8x2 + 30e−x]2.00
1.95
= 54.012 −53.751 = 0.261
Consequently, about 26% of the milk containers are underweight.
GAMES OF CHANCE
Consider again the game that involves the spinning of a dial. If the dial is fair, so that after
spinning it is no more likely to point in any particular direction than another, then conceptually
all possible values of θ within the state space [0, 180] should be “equally likely.” This can be
achieved with a “ﬂat” probability density function as illustrated in Figure 2.25. Since the total
area under the probability density function must be equal to 1, its height must be 1/180, so
that
f (θ) =
1
180

2.2 CONTINUOUS RANDOM VARIABLES
87
FIGURE 2.25
Probability density function for
dial angle θ and area corresponding
to P(a ≤θ ≤b)
0
a
b
180
q
1
180
1
180
f(x) =
for 0 ≤θ ≤180 and f (θ) = 0 elsewhere. This is an example of the uniform probability
density function, which is discussed in greater detail in Chapter 4.
Notice that the probability that the angle θ lies between two values a and b is
 b
a
1
180 dθ = b −a
180
In other words, the probability that θ lies within the interval [a, b] is proportional to the length
of the interval b −a. Thus, for example, P(0 ≤θ ≤10) = 10/180 = 1/18, which is twice
as large as P(10 ≤θ ≤15) = 5/180 = 1/36, which is expected for a fair dial.
The winnings
X = $1000 ×
θ
180
0
1
x
State space
F(x)
FIGURE 2.26
Thecumulativedistributionfunction
increases from 0 at the beginning of
the state space to 1 at the end of the
state space
are intuitively “equally likely” to be anywhere between $0 and $1000, and so the probability
density function should be ﬂat between these two limits. Since the total area must be 1, the
probability density function must be
f (x) =
1
1000
for 0 ≤x ≤1000 and f (x) = 0 elsewhere.
2.2.3
Cumulative Distribution Function
The cumulative distribution function of a continuous random variable X is deﬁned in exactly
the same way as for a discrete random variable, namely
F(x) = P(X ≤x)
For a continuous random variable, the cumulative distribution function F(x) is a continuous
nondecreasing function that takes the value 0 prior to and at the beginning of the state space
and increases to a value of 1 at the end of and after the state space, as illustrated in Figure 2.26,
although the shape of the cumulative distribution function will vary.

88
CHAPTER 2
RANDOM VARIABLES
Like the probability density function, the cumulative distribution function summarizes the
probabilistic properties of a continuous random variable, and knowledge of either function
allows the other function to be constructed. For example, if the probability density function
f (x) is known, then the cumulative distribution function can be calculated from the expression
F(x) = P(X ≤x) =
 x
−∞
f (y) dy
In practice, the lower integration limit −∞can be replaced by the lower endpoint of the
state space since the probability density function is 0 outside the state space. The probability
density function can be obtained by differentiating the cumulative distribution function
f (x) = dF(x)
dx
In addition, notice that the cumulative distribution function provides a convenient way of
obtaining the probability that a random variable lies within a certain region, since
P(a ≤X ≤b) = P(X ≤b) −P(X ≤a) = F(b) −F(a)
Example 14
Metal Cylinder
Production
The cumulative distribution function of the metal cylinder diameters can be constructed from
the probability density function as
F(x) = P(X ≤x) =
 x
49.5
(1.5 −6(y −50.0)2) dy
= [1.5y −2(y −50.0)3]x
49.5
= [1.5x −2(x −50.0)3] −[1.5 × 49.5 −2(49.5 −50.0)3]
= 1.5x −2(x −50.0)3 −74.5
for 49.5 ≤x ≤50.5. As expected, the cumulative distribution function is an increasing
function between the limits x = 49.5 and x = 50.5, as illustrated in Figure 2.27, with
F(49.5) = 0 and F(50.5) = 1. Technically, in addition F(x) = 0 for x < 49.5 and F(x) = 1
for x > 50.5.
FIGURE 2.27
Cumulative distribution function
for metal cylinder diameters
illustrating P(49.7 ≤X ≤50.0)
49.5
49.7
50.0
50.5
Cylinder diameter (mm)
x
1
P(X ≤ 50.0) = 0.500
P(X ≤ 49.7) = 0.104
P(49.7 ≤ X ≤ 50.0) = 0.396
3
F(x) = 1.5x − 2(x − 50)   − 74.5

2.2 CONTINUOUS RANDOM VARIABLES
89
FIGURE 2.28
Cumulative distribution function
for battery failure times illustrating
P(1 ≤X ≤2)
0
1
2
3
Failure time (hrs)
(
P X
(
)
≤1 = 3/4
P X
(
)
≤2 = 8/9
P 1≤
≤2)
X
= 5
36
(
F
)
x =
)
1−
1
(
2
+
x 1
Figure 2.27 also illustrates the probability that a metal cylinder has a diameter between
49.7 and 50.0 mm, which is
P(49.7 ≤X ≤50.0) = F(50.0) −F(49.7)
= (1.5 × 50.0 −2(50.0 −50.0)3 −74.5)
−(1.5 × 49.7 −2(49.7 −50.0)3 −74.5)
= 0.5 −0.104 = 0.396
Consequently, about 40% of the cylinders have diameters between these two limits.
Example 15
Battery Failure Times
The cumulative distribution function of the battery failure times is
F(x) = P(X ≤x) =
 x
0
2
(y + 1)3 dy
=

−1
(y + 1)2
x
0
=

−1
(x + 1)2

−[−1] = 1 −
1
(x + 1)2
for x ≥0. This is illustrated in Figure 2.28, together with the probability that a battery lasts
between one and two hours, which is
P(1 ≤X ≤2) = F(2) −F(1)
=

1 −
1
(2 + 1)2

−

1 −
1
(1 + 1)2

= 8
9 −3
4 = 5
36
Example 16
Concrete Slab Breaking
Strengths
Suppose that the concrete slab breaking strengths measured in certain units are between 120
and 150, with a cumulative distribution function of
F(x) = 3.856 −12.8e−x/100
for 120 ≤x ≤150. This is a valid cumulative distribution function since
F(120) = 3.856 −12.8e−120/100 = 0
F(150) = 3.856 −12.8e−150/100 = 1

90
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.29
Cumulative distribution function
for concrete breaking strengths
illustrating P(X ≤130)
0
120
130
140
150
Breaking strength
1
P(X ≤ 130) =  0.368
F(x) =  3.856 − 12.8e− x/100
and the function is increasing between these two values. Figure 2.29 illustrates the cumulative
distribution function together with the probability that a concrete slab has a strength less than
130, which is
F(130) = 3.856 −12.8e−130/100 = 0.368
The probability density function of the breaking strengths can be calculated from the
cumulative distribution function to be
f (x) = dF(x)
dx
= d
dx (3.856 −12.8e−x/100) = 0.128e−x/100
for 120 ≤x ≤150 and f (x) = 0 elsewhere.
GAMES OF CHANCE
Consider again the game that involves the spinning of a dial. The cumulative distribution
function of the angle θ is
F(θ) =
 θ
0
f (y) dy =
 θ
0
1
180 dy =
θ
180
for 0 ≤θ ≤180. Similarly, the cumulative distribution function of the winnings X is
F(x) =
 x
0
f (y) dy =
 x
0
1
1000 dy =
x
1000
for 0 ≤x ≤1000.
Figure 2.30 illustrates the cumulative distribution function of the winnings together with
the probability that the winnings are between $250 and $750, which is
P(250 ≤X ≤750) = F(750) −F(250) = 750
1000 −250
1000 = 1
2
This result is expected because these winnings correspond to values of θ between 45◦and
135◦, which make up half of the dial, as illustrated in Figure 2.31.

2.2 CONTINUOUS RANDOM VARIABLES
91
1000
750
250
0
Winnings ($)
F(x) =
x
1000
P(X ≤ 750) = 3/4
P(X ≤ 250) = 1/4
P(250 ≤ X ≤ 750) = 1/2
FIGURE 2.30
Cumulative distribution function for dial-spinning game illustrating
P(250 ≤X ≤750)
135
45
°
°
FIGURE 2.31
Dial region corresponding to winnings
between $250 and $750
2.2.4
Problems
2.2.1 Consider a random variable measuring the following
quantities. In each case state with reasons whether you
think it more appropriate to deﬁne the random variable as
discrete or as continuous.
(a) A person’s height
(b) A student’s course grade
(c) The thickness of a metal plate
(d) The purity of a chemical solution
(e) The type of personal computer a person owns
(f) A person’s age
2.2.2 A random variable X takes values between 4 and 6 with a
probability density function
f (x) =
1
x ln(1.5)
for 4 ≤x ≤6 and f (x) = 0 elsewhere.
(a) Make a sketch of the probability density function.
(b) Check that the total area under the probability density
function is equal to 1.
(c) What is P(4.5 ≤X ≤5.5)?
(d) Construct and sketch the cumulative distribution
function.
(This problem is continued in Problems 2.3.10 and 2.4.5.)
2.2.3 A random variable X takes values between −2 and 3 with
a probability density function
f (x) = 15
64 + x
64,
−2 ≤x ≤0
f (x) = 3
8 + cx,
0 ≤x ≤3
and f (x) = 0 elsewhere.
(a) Find the value of c and sketch the probability density
function.
(b) What is P(−1 ≤X ≤1)?
(c) Construct and sketch the cumulative distribution
function.
2.2.4 A random variable X takes values between 0 and 4 with a
cumulative distribution function
F(x) = x2
16
for 0 ≤x ≤4.
(a) Sketch the cumulative distribution function.
(b) What is P(X ≤2)?
(c) What is P(1 ≤X ≤3)?
(d) Construct and sketch the probability density function.
(This problem is continued in Problems 2.3.11 and 2.4.6.)
2.2.5 A random variable X takes values between 0 and ∞with
a cumulative distribution function
F(x) = A + Be−x
for 0 ≤x ≤∞.

92
CHAPTER 2
RANDOM VARIABLES
(a) Find the values of A and B and sketch the cumulative
distribution function.
(b) What is P(2 ≤X ≤3)?
(c) Construct and sketch the probability density function.
2.2.6 A car panel is spray-painted by a machine, and the
technicians are particularly interested in the thickness of
the resulting paint layer. Suppose that the random
variable X measures the thickness of the paint in
millimeters at a randomly chosen point on a randomly
chosen car panel, and that X takes values between 0.125
and 0.5 mm with a probability density function of
f (x) = A(0.5 −(x −0.25)2)
for 0.125 ≤x ≤0.5 and f (x) = 0 elsewhere.
(a) Find the value of A and sketch the probability density
function.
(b) Construct and sketch the cumulative distribution
function.
(c) What is the probability that the paint thickness at a
particular point is less than 0.2 mm?
(This problem is continued in Problems 2.3.12 and 2.4.7.)
2.2.7 Suppose that the random variable X is the time taken by a
garage to service a car. These times are distributed between
0 and 10 hours with a cumulative distribution function
F(x) = A + B ln(3x + 2)
for 0 ≤x ≤10.
(a) Find the values of A and B and sketch the cumulative
distribution function.
(b) What is the probability that a repair job takes longer
than two hours?
(c) Construct and sketch the probability density function.
2.2.8 The bending capabilities of plastic sheets are investigated
by bending sheets at increasingly large angles until a
deformity appears in the sheet. The angle θ at which the
deformity ﬁrst appears is then recorded. Suppose that this
angle takes values between 0◦and 10◦with a probability
density function
f (θ) = A(e10−θ −1)
for 0 ≤θ ≤10 and f (θ) = 0 elsewhere.
(a) Find the value of A and sketch the probability density
function.
(b) Construct and sketch the cumulative distribution
function.
(c) What is the probability that a plastic sheet can be bent
up to an angle of 8◦without deforming?
(This problem is continued in Problems 2.3.13 and 2.4.8.)
2.2.9 An archer shoots an arrow at a circular target with a
radius of 50 cm. If the arrow hits the target, the distance r
between the point of impact and the center of the target is
measured. Suppose that this distance has a cumulative
distribution function
F(r) = A +
B
(r + 5)3
for 0 ≤r ≤50.
(a) Find the values of A and B and sketch the cumulative
distribution function.
(b) What is the probability that the arrow hits within
10 cm of the center of the target?
(c) What is the probability that the arrow hits more than
30 cm away from the center of the target?
(d) Construct and sketch the probability density function.
(This problem is continued in Problems 2.3.14 and 2.4.9.)
2.2.10 Sometimes a random variable is a mix of discrete and
continuous components. For example, suppose that the
dial-spinning game is modiﬁed in the following way. First
a fair coin is tossed and if a head is obtained, the player
wins $500 and the dial is not spun. However, if a tail is
obtained, the player spins the dial and receives winnings of
$1000 ×
θ
180
as before. In this game there is a probability of 0.5 of
winning $500, with all the other possible winnings
between $0 and $1000 being equally likely. The coin toss
provides a discrete element to the winnings, and the dial
spin provides a continuous element. The best way to
describe the probabilistic properties of mixed random
variables such as this is through a cumulative distribution
function. The cumulative distribution function of the
winnings from this game is given in Figure 2.32.
(a) What is the probability of winning less than $200?
(b) What is the probability of winning between $400 and
$700?
Interpret your answers.
2.2.11 The resistance X of an electrical component has a
probability density function
f (x) = Ax(130 −x2)
for resistance values in the range 10 ≤x ≤11.
(a) Calculate the value of the constant A.
(b) Calculate the cumulative distribution function.
(c) What is the probability that the electrical component
has a resistance between 10.25 and 10.5?
(This problem is continued in Problems 2.3.17, 2.4.13,
and 2.6.12.)

2.3 THE EXPECTATION OF A RANDOM VARIABLE
93
FIGURE 2.32
The cumulative distribution
function of the winnings of the
game in Problem 2.2.10.
1.0
0.50
0.25
0
250
500
750
1000
x
F(x)
Winnings ($)
0.75
2.3
The Expectation of a Random Variable
Whereas the probability mass function or the probability density function provides complete
information about the probabilistic properties of a random variable, it is often useful to employ
some summary measures of these properties. One of the most basic summary measures is the
expectation or mean of a random variable, which is denoted by E(X) and represents an
“average” value of the random variable. Two random variables with the same expected value
can be thought of as having the same average value, although their probability mass functions
or probability density functions may be quite different.
2.3.1
Expectations of Discrete Random Variables
A discrete random variable X taking the values xi with probability values pi has an expected
value of
E(X) =

i
pixi
This value can be interpreted as a weighted average of the values in the state space xi, where
the weights are the probability values pi.
Expected Value of a Discrete Random Variable
The expected value or expectation of a discrete random variable with a probability
mass function P(X = xi) = pi is
E(X) =

i
pixi
E(X) provides a summary measure of the average value taken by the random variable
and is also known as the mean of the random variable.

94
CHAPTER 2
RANDOM VARIABLES
50
200
350
0.3
0.2
0.5
Cost ($)
E(X) = 230
Probability
FIGURE 2.33
Expected value of the machine breakdown costs
0
1
2
3
0.07
0.23
0.57
0.13
Number of power plants
Probability
E(X) = 1.76
FIGURE 2.34
Expected value for power plant example
The calculation and interpretation of the expected value are illustrated in the following
examples.
Example 1
Machine Breakdowns
The expected repair cost is
E(cost) = ($50 × 0.3) + ($200 × 0.2) + ($350 × 0.5) = $230
which is illustrated with the probability mass function in Figure 2.33. Over a long period of
time, the repairs will cost an average of about $230 each. Notice that this expected value is
not equal to one of the values $50, $200, and $350 in the state space, but it is the average of
these values weighted by their probabilities.
Example 4
Power Plant Operation
The expected number of power plants generating electricity is
E(X) = (0 × 0.07) + (1 × 0.23) + (2 × 0.57) + (3 × 0.13) = 1.76
which is illustrated in Figure 2.34. This expected value, 1.76, provides a summary measure
of the average number of power plants generating electricity at particular points in time.
Example 12
Personnel Recruitment
The expected number of applicants interviewed is
E(X) = (1 × 0.18) + (2 × 0.10) + (3 × 0.09) + (4 × 0.06) + (5 × 0.05)
+ (6 × 0.04) + (7 × 0.02) + (8 × 0.46)
= 5.20
which is illustrated in Figure 2.35. This expected value provides some indication of how many
of the eight applicants will actually have to be interviewed under the company’s interviewing
strategy.
If another strategy is employed, whereby the weakest two candidates on the short list
are dropped so that a maximum of only six applicants are interviewed, then the probability
that X = 6 increases to 0.04 + 0.02 + 0.46 = 0.52, and the expected number of applicants

2.3 THE EXPECTATION OF A RANDOM VARIABLE
95
FIGURE 2.35
Expected value for personnel
recruitment example
0.18
0.10
0.09
0.05
0.04
0.02
0.46
0.06
1
2
3
4
5
6
7
8
x
Probability
E(X) = 5.2
interviewed falls to
E(X) = (1 × 0.18) + (2 × 0.10) + (3 × 0.09) + (4 × 0.06) + (5 × 0.05) + (6 × 0.52)
= 4.26
The drop in the expected value from 5.20 to 4.26 illustrates the potential savings under the
new strategy, which is at the expense of not having the chance to hire one of the two applicants
dropped from the short list.
Finally, notice that if the strategy is to interview all eight applicants and then to hire the
best one, then obviously P(X = 8) = 1, and the expected number of applicants interviewed
is E(X) = 8.
GAMES OF CHANCE
If a fair die is rolled, the expected value of the outcome is
E(X) =

1 × 1
6

+

2 × 1
6

+

3 × 1
6

+

4 × 1
6

+

5 × 1
6

+

6 × 1
6

= 3.5
Since each outcome is equally likely, this is just the normal arithmetic average of the six
outcomes.
The probability mass function of the positive difference between the scores obtained from
two dice is given in Figure 2.36 together with the expected difference, which is calculated
to be
E(X) =

0 × 1
6

+

1 × 5
18

+

2 × 2
9

+

3 × 1
6

+

4 × 1
9

+

5 × 1
18

= 35
18 = 1.94
Suppose that you organize a game whereby a player rolls two dice and you pay the player
the dollar amount of the difference in the scores. How much should you charge a person to
play? Since your expected or long-run average payment is $1.94 per game, a “fair” charge
would be $1.94. However, if you want to make a proﬁt, you must charge more than this. If you

96
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.36
Expected value for dice example
0
1
2
3
4
5
1/6
5/18
2/9
1/6
1/9
1/18
x
E(X) = 1.94
Probability
charge $2.00 per game, you will lose $3, $2, or $1, break even, or win $1 or $2 on each game,
but your expected long-run proﬁt will be $2.00 −$1.94 per game, that is, about 6 cents per
game. The more people who play the better, because your total expected proﬁts will increase
and the chance of an unlucky net loss will diminish. Of course, if the dice are not fair, things
may not be as they appear to be!
2.3.2
Expectations of Continuous Random Variables
The expectation of a continuous random variable X with a probability density function f (x)
is given by
E(X) =

state space
x f (x) dx
Again, E(X) can be interpreted as a weighted average of the values within the state space,
with weights corresponding to the probability density function f (x).
Expected Value of a Continuous Random Variable
The expected value or expectation of a continuous random variable with a
probability density function f (x) is
E(X) =

state space
x f (x) dx
The expected value provides a summary measure of the average value taken by the
random variable, and it is also known as the mean of the random variable.
The calculation and interpretation of the expected value of continuous random variables
are illustrated in the following examples.
Example 14
Metal Cylinder
Production
The expected diameter of a metal cylinder is
E(X) =
 50.5
49.5
x(1.5 −6(x −50.0)2) dx

2.3 THE EXPECTATION OF A RANDOM VARIABLE
97
x
E(X) = 50.0
49.5
50.5
Cylinder diameter (mm)
f (x) = 1.5 − 6(x − 50.0)2
FIGURE 2.37
Expected value for metal cylinder diameters
x
f(x)
E(X) = µ
µ
FIGURE 2.38
Expected value for a symmetric probability density function
The evaluation of this integral can be simpliﬁed using the transformation y = x −50.0, so
that
E(X) =
 0.5
−0.5
(y + 50.0)(1.5 −6y2) dy
=
 0.5
−0.5
(−6y3 −300y2 + 1.5y + 75) dy
= [−3y4/2 −100y3 + 0.75y2 + 75y]0.5
−0.5
= [25.09375] −[−24.90625] = 50.0
Consequently, the metal cylinders have an average diameter of 50.0 mm.
The probability density function and the expected value are illustrated in Figure 2.37.
Notice that the probability density function is symmetric about the value 50.0 mm, which is
the expected value. It is a general result that a random variable with a symmetric probability
density function has an expectation equal to the point of symmetry.
Symmetric Random Variables
If a continuous random variable X has a probability density function f (x) that is
symmetric about a point μ, so that f (μ + x) = f (μ −x) for all x ∈R, then
E(X) = μ, so that the expectation of the random variable is equal to the point of
symmetry.
This general result is illustrated in Figure 2.38.
Example 15
Battery Failure Times
The expected battery failure time is
E(X) =
 ∞
0
x
2
(x + 1)3 dx =
 ∞
0

2
(x + 1)2 −
2
(x + 1)3

dx
=

−2
(x + 1) +
1
(x + 1)2
∞
0
= [0] −[−1] = 1

98
CHAPTER 2
RANDOM VARIABLES
Failure time (hrs)
x
0
7
1
2
3
4
5
6
2
1
E(X) = 1
3
f x( ) =
2
+1
(x
)
FIGURE 2.39
Expected value for battery failure times
E(X) = 134.1
x
120
150
Breaking strength
f(x) = 0.128e−x/100
FIGURE 2.40
Expected value for concrete breaking strengths
which is illustrated in Figure 2.39. This expected value indicates that the batteries fail on
average after one hour of operation.
Example 16
Concrete Slab Breaking
Strengths
Figure 2.40 illustrates the probability density function for the concrete breaking strengths
together with the expected breaking strength, which is
E(X) =
 150
120
x0.128e−x/100 dx
= [−12.8(100 + x)e−x/100]150
120 = [−714.02] −[−848.16] = 134.1
The relative advantage of another type for concrete might be judged by determining whether
it has an expected breaking strength smaller or larger than this value of 134.1.
GAMES OF CHANCE
The probability density function for the winnings in the dial-spinning game is illustrated in
Figure 2.41. Since it is symmetric about the middle value of $500, the expected winnings are
immediately known to be $500. Formally, this can be checked by noting that
E(X) =
 1000
0
x
1
1000 dx =
 x2
2000
1000
0
= [500] −[0] = 500
A fair price for playing the game is thus $500.
Finally, the concept of expectation can be extended in a natural way from the expectation
E(X) of a random variable X to the expectation E(g(X)) of a function g(X) of the random
variable. An example of this, which is useful in the next section concerning the variances of
random variables, is when g(X) = X2. The expectation of the square of the random variable X,

2.3 THE EXPECTATION OF A RANDOM VARIABLE
99
FIGURE 2.41
Expected winnings for
dial-spinning game
0
1000
E(X) = 500
x
Winnings ($)
1
1000
f(x) = 
1
1000
E(X2), can be interpreted as an average value of the squares of the values taken by the random
variable.
Expectations of functions of random variables can be calculated from the formulas
E(g(X)) =

i
pig(xi)
for discrete random variables and
E(g(X)) =

state space
g(x) f (x) dx
for continuous random variables. Notice that in general E(g(X)) ̸= g(E(X)), and in particular
E(X2) is in general not equal to E(X)2. Functions and combinations of random variables are
discussed in more detail in Section 2.6.
2.3.3
Medians of Random Variables
The median is another summary measure of the distribution of a random variable that provides
information about the “middle” value of the random variable. It is deﬁned to have the property
that the random variable is equally likely to be either smaller or larger than the median. The
median is most often used with continuous random variables and is the value of x for which
F(x) = 0.5.
Median
The median of a continuous random variable X with a cumulative distribution
function F(x) is the value x in the state space for which
F(x) = 0.5
The random variable is then equally likely to fall above or below the median value.
Example 14
Metal Cylinder
Production
The median value of the metal cylinder diameters is the solution to
F(x) = 1.5x −2(x −50.0)3 −74.5 = 0.5

100
CHAPTER 2
RANDOM VARIABLES
which is x = 50.0. This is not really a surprise because the probability density function is
symmetric about the point x = 50.0, so that a diameter is equally likely to be smaller than
50 mm and larger than 50 mm.
In general, it is clear that a random variable with a symmetric probability density function
has a median as well as an expectation equal to the point of symmetry. The following statement
generalizes the previous result on the expectation of symmetric random variables.
Symmetric Random Variables
If a continuous random variable X has a probability density function f (x) that is
symmetric about a point μ, then both the median and the expectation of the random
variable are equal to μ.
The following examples illustrate that in general the expectation and the median of a
random variable can take different values.
Example 15
Battery Failure Times
The median of the battery failure times is the solution to
F(x) = 1 −
1
(x + 1)2 = 0.5
which is x =
√
2−1 = 0.414. Figure 2.42 illustrates this median value relative to the expected
failure time of 1 hour. Whereas the batteries will operate for an average of 1 hour, half of them
will fail before 0.414 hour, or about 25 minutes. The reason the expected lifetime is so much
longer than the median lifetime is that those batteries that last longer than 25 minutes have a
chance of lasting a considerable length of time.
FIGURE 2.42
Comparison of median and mean
battery failure times
Failure time (hrs)
x
0
7
1
2
3
4
5
6
2
1
f x
2
( )
+1 3
=
(x
)
F(0.414) = 0.5
E(X) = 1

2.3 THE EXPECTATION OF A RANDOM VARIABLE
101
FIGURE 2.43
Comparison of median and mean
concrete breaking strengths
x
120
150
Breaking strength
F(133.9) = 0.5
f(x) = 0.128e−x/100
E(X) = 134.1
This example illustrates that the median and the mean can both provide useful information
about a random variable. If an engineer has one appliance and wants to know how many
batteries will be required to keep it running over a long period of time, then the mean value
of one hour provides the most useful information. To keep the appliance running for one day,
the engineer can plan on needing about 24 batteries on average.
On the other hand, if the engineer has many appliances and wishes to know how many
of them will last more than 30 minutes on newly charged batteries, then a median value of
about 25 minutes indicates that the engineer can expect fewer than half of the appliances to
last more than 30 minutes.
Example 16
Concrete Slab Breaking
Strengths
The median concrete breaking strength is the solution to
F(x) = 3.856 −12.8e−x/100 = 0.5
which is x = 133.9. Figure 2.43 illustrates this together with the expected breaking strength
of 134.1. The median and the mean strengths are about the same here since the probability
density function is almost ﬂat.
2.3.4
Problems
2.3.1 Consider again the four copying machines discussed in
Problem 2.1.1. What is the expected number of copying
machines in use at a particular moment in time?
2.3.2 Consider again Problem 2.1.3 where the numbers
obtained on two fair dice are multiplied to obtain a ﬁnal
score. What is the expected value of this score?
2.3.3 Consider again Problem 2.1.4 where two cards are drawn
from a pack of cards. Is the expected number of hearts
drawn larger when the second drawing is made with or
without replacement? Does this answer surprise you?
2.3.4 Consider again the salesperson discussed in Problem 2.1.9
who is trying to locate a particular product. What is the
expected number of warehouses called by the salesperson?
2.3.5 Suppose that a player draws a card at random from a pack
of cards, and wins $15 if an ace, king, queen, or jack is
obtained, and otherwise wins the face value of the card in
dollars. What is the expected amount won by the player?
Would you pay $9 to play this game?
2.3.6 Two fair dice, one red and one blue, are rolled, and a fair
coin is tossed. If a head is obtained on the coin toss, then

102
CHAPTER 2
RANDOM VARIABLES
a player wins the sum of the scores on the two dice. If a
tail is obtained on the coin toss, then the player wins
the score on the red die. What are the expected
winnings?
2.3.7 A player pays $1 to play a game where three fair dice are
rolled. If three 6s are obtained the player wins $500, and
otherwise the player wins nothing. What are the expected
net winnings of this game? Would you want to play this
game? Does your answer depend upon how many times
you can play the game?
2.3.8 A state lottery generally consists of many tickets being
sold at prices of about $1, each with a chance to win a
large jackpot which is often over $1 million. Would you
expect the expected net winnings on each ticket to be
positive or negative? Why do people play lotteries?
2.3.9 Suppose that you are organizing the game described at
the end of Section 2.3.1, where you charge players $2 to
roll two dice, and then you pay them the difference in the
scores. If you ﬁx the dice so that each die has a probability
of 0.2 of scoring a 3 and equal probabilities of 0.16 of
scoring the other ﬁve numbers, do your expected winnings
increase beyond 6 cents per game? Is this a surprise?
2.3.10 Consider again the random variable described in
Problem 2.2.2 with a probability density function of
f (x) =
1
x ln(1.5)
for 4 ≤x ≤6 and f (x) = 0 elsewhere.
(a) What is the expected value of this random variable?
(b) What is the median of this random variable?
2.3.11 Consider again the random variable described in
Problem 2.2.4 with a cumulative distribution function of
F(x) = x2
16
for 0 ≤x ≤4.
(a) What is the expected value of this random variable?
(b) What is the median of this random variable?
2.3.12 Consider again the car panel painting machine discussed
in Problem 2.2.6. What is the expected paint thickness?
What is the median paint thickness?
2.3.13 Consider again the plastic bending capabilities discussed
in Problem 2.2.8. What is the expected deformity angle?
What is the median deformity angle?
2.3.14 Consider again the archery problem discussed in
Problem 2.2.9. What is the expected deviation from the
center of the target? What is the median deviation?
2.3.15 Prove that a continuous random variable with a
probability density function that is symmetric about a
point μ has an expected value equal to the point of
symmetry μ.
2.3.16 Recall Problem 2.1.11 concerning the scheduling of
appointments with a consultant. What is the expected
value of the total number of appointments that have
already been made over both days at the moment when
Monday’s schedule has just been completely ﬁlled?
2.3.17 Recall Problem 2.2.11 concerning the resistance of an
electrical component.
(a) What is the expected value of the resistance?
(b) What is the median value of the resistance?
2.3.18 A random variable has a probability density function
f (x) = A(x −1.5) over the state space 2 ≤x ≤3.
(a) What is the value of A?
(b) What is the median of the random variable?
2.3.19 A manager notices that the number of items purchased by
visitors to a store is zero with probability 0.38, one with
probability 0.44, two with probability 0.15, and three with
probability 0.03. What is the expected number of items
purchased by a visitor to the store?
2.4
The Variance of a Random Variable
2.4.1
Deﬁnition and Interpretation of Variance
Another important summary measure of the distribution of a random variable is the variance,
which measures the spread or variability in the values taken by the random variable. Whereas
the mean or expectation measures the central or average value of the random variable, the
variance measures the spread or deviation of the random variable about its mean value.
Speciﬁcally, the variance of a random variable is deﬁned as
Var(X) = E
	
(X −E(X))2

2.4 THE VARIANCE OF A RANDOM VARIABLE
103
Thus, it is deﬁned to be the expected value of the squares of the deviations of the random
variablevaluesabouttheexpectedvalue E(X).Necessarily,thevarianceisalwayspositive,and
larger values of the variance indicate a greater spread in the distribution of the random variable
aboutthemeanvalue.Analternativeandoftensimplerexpressionforcalculatingthevarianceis
Var(X) = E((X −E(X))2)
= E(X2 −2X E(X) + (E(X))2)
= E(X2) −2E(X)E(X) + (E(X))2
= E(X2) −(E(X))2
Variance
The variance of a random variable X is deﬁned to be
Var(X) = E((X −E(X))2)
or equivalently
Var(X) = E(X2) −(E(X))2
The variance is a positive quantity that measures the spread of the distribution of the
random variable about its mean value. Larger values of the variance indicate that the
distribution is more spread out.
The concept of variance can be illustrated graphically. Figure 2.44 shows two probability
density functions that have different mean values, but identical variances. The variances are
the same because the shape or spread of the density functions about their mean values is the
same. On the other hand, Figure 2.45 shows two probability density functions that have the
FIGURE 2.44
Two distributions with different
mean values but identical variances
x
f(x)
FIGURE 2.45
Two distributions with identical
mean values but different variances
x
f(x)

104
CHAPTER 2
RANDOM VARIABLES
same mean values, but different variances. The density function that is ﬂatter and more spread
out has the larger variance.
It is common to use the symbol μ to denote the mean or expectation of a random variable
and the symbol σ 2 to denote the variance. The square root of the variance, σ, is known as the
standard deviation of the distribution of the random variable and is often used in place of
the variance to describe the spread of the distribution.
Standard Deviation
The standard deviation of a random variable X is deﬁned to be the positive square
root of the variance. The symbol σ 2 is often used to denote the variance of a random
variable, so that σ represents the standard deviation.
Notice that the standard deviation has the same units as the random variable X, but the
variance has the square of these units. For example, if the random variable X is measured in
seconds, then the standard deviation will also be measured in seconds but the variance will be
measured in seconds2.
2.4.2
Examples of Variance Calculations
Example 1
Machine Breakdowns
Recall that the repair costs are $50, $200, and $350 with respective probability values of 0.3,
0.2, and 0.5, and that the expected repair cost is E(X) = $230. The variance of the repair cost
can be calculated from the formula
Var(X) = E((X −E(X))2) =

i
pi(xi −E(X))2
and the calculations are shown in Figure 2.46. The variance is 17,100 so that the standard
deviation is √17,100 = $130.77.
Alternatively, since
E(X2) =

i
pix2
i = (0.3 × 502) + (0.2 × 2002) + (0.5 × 3502) = 70,000
FIGURE 2.46
Mean and standard deviation of
machine breakdown costs
50
200
350
0.3
0.2
0.5
E(X) = 230
Cost ($)
Probability
    = 0.3 (50 − 230)   + 0.2 (200 − 230)   + 0.5 (350 − 230)    = 17,100
2
2
2
2
 = 130.77
 = 130.77

2.4 THE VARIANCE OF A RANDOM VARIABLE
105
the variance can be calculated from the simpler formula to be
Var(X) = E(X2) −(E(X))2 = 70,000 −2302 = 17,100
as before.
Example 12
Personnel Recruitment
The expected value of the square of the number of applicants interviewed is
E(X2) =

i
pix2
i
= (0.18 × 12) + (0.10 × 22) + (0.09 × 32) + (0.06 × 42) + (0.05 × 52)
+ (0.04 × 62) + (0.02 × 72) + (0.46 × 82) = 34.96
Since the expected value of the number of applicants interviewed is E(X) = 5.20, the variance
can be calculated to be
Var(X) = E(X2) −(E(X))2 = 34.96 −5.202 = 7.92
The standard deviation is therefore σ =
√
7.92 = 2.81 people, which is illustrated in
Figure 2.47.
Example 14
Metal Cylinder
Production
Recall that the mean cylinder diameter is E(X) = 50.0 mm, so that the variance is
Var(X) = E((X −E(X))2) =

(x −50.0)2 f (x) dx
=
 50.5
49.5
(x −50.0)2(1.5 −6(x −50.0)2) dx
= [0.5(x −50.0)3 −1.2(x −50.0)5]50.5
49.5 = [0.025] −[−0.025] = 0.05
FIGURE 2.47
Mean and standard deviation for
personnel recruitment example
σ = 2.81
Probability
0.18
0.10
0.09
0.05
0.04
0.02
0.46
0.06
1
2
3
4
5
6
7
8
x
σ = 2.81
E(X) = 5.2

106
CHAPTER 2
RANDOM VARIABLES
49.5
50.5
x
Cylinder diameter (mm)
f (x) = 1.5 − 6 (x − 50.0)2
E(X) = 50.0
σ = 0.224 σ = 0.224
FIGURE 2.48
Mean and standard deviation of metal cylinder diameters
150
Breaking strength
σ = 10.05
σ = 10.05
E(X) = 134.1 
x
f(x) = 0.128e −x/100
120
FIGURE 2.49
Mean and standard deviation of concrete breaking strengths
The standard deviation of the metal cylinder diameters is therefore σ =
√
0.05 = 0.224 mm,
which is illustrated in Figure 2.48.
Example 16
Concrete Slab Breaking
Strengths
The expected squared breaking strength is
E(X2) =

x2 f (x)dx =
 150
120
x20.128e−x/100dx
= [−12.8(20,000 + 200x + x2)e−x/100]150
120
= [−207,064.79] −[−225,148.70] = 18,083.91
Since the mean breaking strength is E(X) = 134.1, the variance of the breaking strengths is
Var(X) = E(X2) −(E(X))2 = 18,083.91 −134.12 = 101.10
The standard deviation of the breaking strengths is thus σ =
√
101.10 = 10.05, which is
illustrated in Figure 2.49.
GAMES OF CHANCE
Consider two games. In game I a fair die is rolled and a player wins the dollar amount of the
score obtained. The probability mass function of the winnings is given in Figure 2.50, and
the expected winnings are $3.50. In game II the same die is rolled, but the player wins $3 if a
score of 1, 2, or 3 is obtained and wins $4 if a score of 4, 5, or 6 is obtained. The probability
mass function of the winnings from this game is given in Figure 2.51, and again the expected
winnings are $3.50.
In both games the expected winnings are the same, and so both games produce the same
average revenue. However, the variability in the winnings of game I is clearly larger than the
variability in the winnings of game II, and consequently the winnings should have a larger
variance in game I than in game II.

2.4 THE VARIANCE OF A RANDOM VARIABLE
107
1
2
3
4
5
6
Winnings ($)
1/6
1/6
1/6
1/6
1/6
1/6
σ = 1.71
E(X) = 3.5
σ = 1.71
Probability
FIGURE 2.50
Winnings from game I
3
4
E(X) = 3.5
Winnings ($)
1/2
1/2
σ = 0.5
σ = 0.5
Probability
FIGURE 2.51
Winnings from game II
For Game I,
Var(X) = E
	
(X −E(X))2
=

i
pi(xi −3.5)2
= 1
6(1 −3.5)2 + 1
6(2 −3.5)2 + 1
6(3 −3.5)2 + 1
6(4 −3.5)2
+ 1
6(5 −3.5)2 + 1
6(6 −3.5)2 = 35
12
whereas for Game II,
Var(X) = E
	
(X −E(X))2
=

i
pi(xi −3.5)2
= 1
2(3 −3.5)2 + 1
2(4 −3.5)2 = 1
4
Thus, the standard deviation of the winnings for game I is σ = √35/12 = 1.71, which,
as expected, is larger than the standard deviation of the winnings for game II, which is
σ = √1/4 = 0.50.
2.4.3
Chebyshev’s Inequality
Chebyshev’s inequality is a general result that underlines the importance of the variance
and standard deviation of a distribution. It provides general bounds on the probability that
a random variable can take values greater than so many standard deviations away from its
expected value.
Chebyshev’s Inequality
If a random variable has a mean μ and a variance σ 2, then
P(μ −cσ ≤X ≤μ + cσ) ≥1 −1
c2
for c ≥1.

108
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.52
Illustration of Chebyshev’s
inequality
x
f(x)
E(X) = µ
σ
σ
σ
σ
x
f(x)
E(X) = µ
σ
σ
σ
σ
σ
σ
Area ≥ 0.75
Area ≥ 0.89
This result indicates that regardless of the actual distribution of a random variable, there
are very large probabilities that it will take a value within a few standard deviations of its
mean value. For example, taking c = 2 gives
P(μ −2σ ≤X ≤μ + 2σ) ≥1 −1
22 = 0.75
and taking c = 3 gives
P(μ −3σ ≤X ≤μ + 3σ) ≥1 −1
32 = 0.89
Thus any random variable has a probability of at least 75% of taking a value within two
standard deviations of its mean and has a probability of at least 89% of taking a value within
three standard deviations of its mean, as illustrated in Figure 2.52.
HISTORICAL NOTE
Pafnutii Lvovich Chebyshev
(1821–1894) was the leader
of a mathematical school
in Russia known as the
St. Petersburg School, which
was of paramount importance
in the development of
mathematics in Russia. In
particular this school provided
a solid mathematical
background for probability
theory. The inequality
presented here appears in
Chebyshev’s master’s
dissertation entitled “An essay
on elementary analysis of
probability theory” presented at
Moscow University in 1846.
The importance of Chebyshev’s inequality is that these results are true regardless of the
exact distribution of a random variable, and they require knowledge of only the mean and
the standard deviation of the distribution. However, if the exact distribution is known or can
reasonably be approximated, then the probabilities can be calculated more exactly and may
be much larger than the lower bounds provided by Chebyshev’s inequality. For example, the
exact values for a normal distribution are shown in Figure 5.16.

2.4 THE VARIANCE OF A RANDOM VARIABLE
109
Example 18
Tomato Plant Heights
A researcher is interested in how tomato plants are affected by different growing conditions.
It is found that under particular growing conditions, three weeks after planting, the heights of
the plants have a mean of 29.4 cm and a standard deviation of 2.1 cm. Two standard deviations
either side of the mean is
[29.4 −(2 × 2.1), 29.4 + (2 × 2.1)] = [25.2, 33.6]
and three standard deviations either side of the mean is
[29.4 −(3 × 2.1), 29.4 + (3 × 2.1)] = [23.1, 35.7]
Consequently, the researcher can infer that tomato plants grown under these conditions have a
probability of at least 75% of having a height between 25.2 cm and 33.6 cm after three weeks,
and have a probability of at least 89% of having a height between 23.1 cm and 35.7 cm after
three weeks.
Notice that these conclusions can be drawn without knowing the actual distribution of the
plant heights, since only the mean and standard deviation are required. However, an important
question is: How does the researcher estimate these values? The discussion on statistical
estimation in Chapter 7 will indicate how the researcher can estimate the mean value to be
29.4 cm and the standard deviation to be 2.1 cm.
Example 14
Metal Cylinder
Production
Recall that the metal cylinder diameters have a cumulative distribution function of
F(x) = 1.5x −2(x −50.0)3 −74.5
for 49.5 ≤x ≤50.5, and that the diameters have a mean of 50.0 mm and a standard deviation
of 0.224 mm. Two standard deviations either side of the mean is therefore
[50.0 −(2 × 0.224), 50.0 + (2 × 0.224)] = [49.552, 50.448]
so that Chebyshev’s inequality indicates that there is at least a 75% probability that a cylinder
will have a diameter between 49.552 and 50.448 mm.
However, this probability can be calculated exactly to be
F(50.448) −F(49.552) = 0.992 −0.008 = 0.984
While this result conﬁrms that Chebyshev’s inequality is correct in this case, it also illustrates
that it provides a very poor lower bound on the true probability. The estimation of the actual
probability distribution can allow substantially more accurate inferences to be made. Notice
also that for this example there is in fact a probability of 1 that a cylinder diameter falls within
three standard deviations of the mean.
2.4.4
Quantiles of Random Variables
Quantiles of random variables are additional summary measures that can provide information
about the spread or variability of the distribution of the random variable. The pth quantile of
a random variable X with a cumulative distribution function F(x) is deﬁned to be the value
x for which
F(x) = p
so that there is a probability of p that the random variable takes a value smaller than the pth
quantile. The probability p is often written as a percentage, and the resulting quantiles are
then called percentiles, so that, for instance, the 70th percentile of a distribution is the value
x for which F(x) = 0.70, as illustrated in Figure 2.53. Notice that the 50th percentile of a
distribution is the median value.

110
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.53
Illustration of 70th percentile
x
f(x)
Area = 0.70
70th percentile
FIGURE 2.54
Illustration of quartiles and median
x
f(x)
Lower
quartile
Median
Upper
quartile
Each area = 0.25
Quantiles
The pth quantile of a random variable X with a cumulative distribution function F(x)
is deﬁned to be the value x for which
F(x) = p
This is also referred to as the p × 100th percentile of the random variable. There is a
probability of p that the random variable takes a value less than the pth quantile.
An idea of the spread of a distribution can be obtained by calculating its quartiles. The
upper quartile of a distribution is deﬁned to be the 75th percentile of the distribution, and
the lower quartile of a distribution is deﬁned to be the 25th percentile. Notice that the two
quartiles, together with the median, partition the state space of a random variable into four
“quarters,” each of which has a probability of 0.25, as illustrated in Figure 2.54.
The interquartile range, deﬁned to be the distance between the two quartiles as shown
in Figure 2.55, can be used similar to the variance to provide an indication of the spread of a
distribution. Larger values of the interquartile range obviously indicate that a random variable
has a distribution that is more spread out.

2.4 THE VARIANCE OF A RANDOM VARIABLE
111
FIGURE 2.55
Illustration of the interquartile
range
Area = 0.5
Interquartile range
x
f(x)
Lower
quartile
Upper
quartile
Quartiles and Interquartile Range
The upper quartile of a distribution is deﬁned to be the 75th percentile of the
distribution, and the lower quartile of a distribution is deﬁned to be the 25th
percentile. The interquartile range is the distance between the two quartiles and like
the variance provides an indication of the spread of the distribution.
Example 14
Metal Cylinder
Production
The cumulative distribution function of the metal cylinder diameters is
F(x) = 1.5x −2(x −50.0)3 −74.5
for 49.5 ≤x ≤50.5. The upper quartile of the distribution is the value of x for which
F(x) = 0.75
which is 50.17 mm. The lower quartile satisﬁes
F(x) = 0.25
and is 49.83 mm. The interquartile range is therefore 50.17 −49.83 = 0.34 mm, and half of the
cylinders will have diameters between 49.83 mm and 50.17 mm, as illustrated in Figure 2.56.
Example 15
Battery Failure Times
The cumulative distribution function of the battery failure times is
F(x) = 1 −
1
(x + 1)2
The upper quartile of the distribution is thus seen to be 1 hour, and the lower quartile is seen
to be (2/
√
3) −1 = 0.155 hour, which is about 9.3 minutes. The interquartile range is thus
60 −9.3 = 50.7 minutes, and half of the batteries will fail between 9.3 minutes and 1 hour,
as illustrated in Figure 2.57.
Recall that the expected failure time is also 1 hour. However, even though the average
failure time is 1 hour, on average three out of four batteries will fail before 1 hour.

112
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.56
Interquartile range for metal
cylinder diameters
49.5
50.5
x
Cylinder diameter (mm)
49.83
50.17
Area = 0.5
Interquartile range = 0.34
quartile
Upper
Lower
quartile
f(x) = 1.5 − 6(x − 50)
2
FIGURE 2.57
Interquartile range for battery
failure times
Failure time (hrs)
x
7
1
2              3              4            5              6 
2
1
f x
2
( )
+1 3
= (x
)
Lower 
quartile
Upper 
quartile
0.155
Interquartile range = 0.845
Area = 0.5

2.4 THE VARIANCE OF A RANDOM VARIABLE
113
2.4.5
Problems
2.4.1 Suppose that the random variable X takes the values −2,
1, 4, and 6 with probability values 1/3, 1/6, 1/3, and 1/6,
respectively.
(a) Find the expectation of X.
(b) Find the variance of X using the formula
Var(X) = E((X −E(X))2)
(c) Find the variance of X using the formula
Var(X) = E(X 2) −(E(X))2
2.4.2 Consider again the four copying machines discussed in
Problems 2.1.1 and 2.3.1. Calculate the variance and
standard deviation of the number of copying machines in
use at a particular moment.
2.4.3 Consider again the salesperson discussed in Problems
2.1.9 and 2.3.4 who is trying to locate a particular
product. Calculate the variance and standard deviation of
the number of warehouses called by the salesperson.
2.4.4 Suppose that you are organizing the game described at the
end of Section 2.3.1, where you charge players $2 to roll
two dice and then you pay them the difference in the
scores. What is the variance in your proﬁt from each
game? If you are playing a game in which you have
positive expected winnings, would you prefer a small or a
large variance in the winnings?
2.4.5 Consider again the random variable described in Problems
2.2.2 and 2.3.10 with a probability density function of
f (x) =
1
x ln(1.5)
for 4 ≤x ≤6 and f (x) = 0 elsewhere.
(a) What is the variance of this random variable?
(b) What is the standard deviation of this random
variable?
(c) Find the upper and lower quartiles of this random
variable.
(d) What is the interquartile range?
2.4.6 Consider again the random variable described in
Problems 2.2.4 and 2.3.11 with a cumulative distribution
function of
F(x) = x2
16
for 0 ≤x ≤4.
(a) What is the variance of this random variable?
(b) What is the standard deviation of this random
variable?
(c) Find the upper and lower quartiles of this random
variable.
(d) What is the interquartile range?
2.4.7 Consider again the car panel painting machine discussed
in Problems 2.2.6 and 2.3.12.
(a) What is the variance of the paint thickness?
(b) What is the standard deviation of the paint thickness?
(c) Find the upper and lower quartiles of the paint
thickness.
(d) What is the interquartile range?
2.4.8 Consider again the plastic bending capabilities discussed
in Problems 2.2.8 and 2.3.13.
(a) What is the variance of the deformity angle?
(b) What is the standard deviation of the deformity
angle?
(c) Find the upper and lower quartiles of the deformity
angle.
(d) What is the interquartile range?
2.4.9 Consider again the archery problem discussed in
Problems 2.2.9 and 2.3.14.
(a) What is the variance of the deviation from the center
of the target?
(b) What is the standard deviation of the deviation from
the center of the target?
(c) Find the upper and lower quartiles of the deviation
from the center of the target.
(d) What is the interquartile range?
2.4.10 The time taken to serve a customer at a fast-food
restaurant has a mean of 75.0 seconds and a standard
deviation of 7.3 seconds. Use Chebyshev’s inequality to
calculate time intervals that have 75% and 89%
probabilities of containing a particular service time.
2.4.11 A machine produces iron bars whose lengths have a mean
of 110.8 cm and a standard deviation of 0.5 cm. Use
Chebyshev’s inequality to obtain a lower bound on the
probability that an iron bar chosen at random has a length
between 109.55 cm and 112.05 cm.
2.4.12 Recall Problems 2.1.11 and 2.3.16 concerning the
scheduling of appointments with a consultant. What
is the standard deviation of the total number of
appointments that have already been made over both
days at the moment when Monday’s schedule has just
been completely ﬁlled?

114
CHAPTER 2
RANDOM VARIABLES
2.4.13 Recall Problems 2.2.11 and 2.3.17 concerning the
resistance of an electrical component.
(a) What is the standard deviation of the resistance?
(b) What is the 80th percentile of the resistance? What is
the 10th percentile of the resistance?
2.4.14 A continuous random variable has a probability density
function f (x) = Ax2.5 for 2 ≤x ≤3.
(a) What is the value of A?
(b) What is the expectation of the random variable?
(c) What is the standard deviation of the random
variable?
(d) What is the median of the random variable?
2.4.15 In a game a player either loses $1 with a probability 0.25,
wins $1 with a probability 0.4, or wins $4 with a
probability 0.35. What are the expectation and the
standard deviation of the winnings?
2.4.16 A random variable X has a probability density function
f (x) = A/√x for 3 ≤x ≤4.
(a) What is the value of A?
(b) What is the cumulative distribution function of X?
(c) What is the expected value of X?
(d) What is the standard deviation of X?
(e) What is the median of X?
(f) What is the upper quartile of X?
2.4.17 When a construction project is opened for bidding,
two proposals are received with probability 0.11, three
proposals are received with probability 0.19, four
proposals are received with probability 0.55, and
ﬁve proposals are received with probability 0.15.
(a) What is the expectation of the number of proposals
received?
(b) What is the standard deviation of the number of
proposals received?
2.4.18 A random variable X has a distribution given by the
probability density function f (x) = (1 −x)/2 with a
state space −1 ≤x ≤1.
(a) What is the expected value of X?
(b) What is the standard deviation of X?
(c) What is the upper quartile of X?
2.5
Jointly Distributed Random Variables
2.5.1
Joint Probability Distributions
Instead of considering one random variable X and its probability distribution, it is often
appropriatetoconsidertworandomvariables X andY andtheirjointprobabilitydistribution.
If the random variables are discrete, then the joint probability mass function consists of
probability values P(X = xi, Y = y j) = pi j ≥0 satisfying

i

j
pi j = 1
If the random variables are continuous, then the joint probability density function is a
function f (x, y) ≥0 satisfying
 
state space
f (x, y) dx dy = 1
The probability that a ≤X ≤b and c ≤Y ≤d is obtained from the joint probability density
function as
 b
x = a
 d
y = c
f (x, y) dy dx
The joint cumulative distribution function is deﬁned to be
F(x, y) = P(X ≤x, Y ≤y),
which is
F(x, y) =

i:xi≤x

j:y j≤y
pi j

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
115
for discrete random variables and
F(x, y) =
 x
w=−∞
 y
z=−∞
f (w, z) dz dw
for continuous random variables.
Joint Probability Distributions
The joint probability distribution of two random variables X and Y is speciﬁed by a
set of probability values P(X = xi, Y = y j) = pi j for discrete random variables, or a
joint probability density function f (x, y) for continuous random variables. In either
case, the joint cumulative distribution function is deﬁned to be
F(x, y) = P(X ≤x, Y ≤y)
The following two examples illustrate jointly distributed random variables.
Example 19
Air Conditioner
Maintenance
A company that services air conditioner units in residences and ofﬁce blocks is interested
in how to schedule its technicians in the most efﬁcient manner. Speciﬁcally, the company is
interested in how long a technician takes on a visit to a particular location, and the company
recognizes that this mainly depends on the number of air conditioner units at the location that
need to be serviced.
If the random variable X, taking the values 1, 2, 3, and 4, is the service time in hours taken
at a particular location, and the random variable Y, taking the values 1, 2, and 3, is the number
of air conditioner units at the location, then these two random variables can be thought of as
jointly distributed.
Suppose that their joint probability mass function pi j is given in Figure 2.58. The ﬁgure
indicates, for example, that there is a probability of 0.12 that X = 1 and Y = 1, so that there
is a probability of 0.12 that a particular location chosen at random has one air conditioner
unit that takes a technician one hour to service. Similarly, there is a probability of 0.07 that
a location has three air conditioner units that take four hours to service. Notice that this is a
valid probability mass function since

i

j
pi j = 0.12 + 0.08 + · · · + 0.07 = 1.00
FIGURE 2.58
Joint probability mass function for
air conditioner maintenance
example
0.12
0.08
0.05
0.08
0.01
0.15
0.01
0.21
0.02
0.13
0.07
0.07
1
2
3
4
1
2
3
X = service time (hrs)
Y = number of
air conditioner units

116
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.59
Joint cumulative distribution
function for air conditioner
maintenance example
0.12
0.20
0.32
0.20
0.21
0.43
0.45
0.71
0.75
0.89
0.27
1.00
1
2
3
4
1
2
3
X = service time (hrs)
Y = number of
air conditioner units
The joint cumulative distribution function
F(x, y) = P(X ≤x, Y ≤y) =
x

i=1
y

j=1
pi j
is given in Figure 2.59. For example, the probability that a location has no more than two air
conditioner units that take no more than two hours to service is
F(2, 2) = p11 + p12 + p21 + p22 = 0.12 + 0.08 + 0.08 + 0.15 = 0.43
Example 20
Mineral Deposits
In order to determine the economic viability of mining in a certain area, a mining company
obtains samples of ore from the location and measures their zinc content and their iron content.
Suppose that the random variable X is the zinc content of the ore, taking values between 0.5
and 1.5, and that the random variable Y is the iron content of the ore, taking values between
20.0 and 35.0. Furthermore, suppose that their joint probability density function is
f (x, y) = 39
400 −17(x −1)2
50
−(y −25)2
10,000
for 0.5 ≤x ≤1.5 and 20.0 ≤y ≤35.0.
The validity of this joint probability density function can be checked by ascertaining that
f (x, y) ≥0 within the state space 0.5 ≤x ≤1.5 and 20.0 ≤y ≤35.0, and that
 1.5
x = 0.5
 35.0
y = 20.0
f (x, y) dx dy = 1
The joint probability density function provides complete information about the joint prob-
abilistic properties of the random variables X and Y. For example, the probability that a
randomly chosen sample of ore has a zinc content between 0.8 and 1.0 and an iron content
between 25 and 30 is
 1.0
x = 0.8
 30.0
y = 25.0
f (x, y) dx dy
which can be calculated to be 0.092. Consequently only about 9% of the ore at the location
has mineral levels within these limits.

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
117
2.5.2
Marginal Probability Distributions
Even though two random variables X and Y may be jointly distributed, if interest is focused on
only one of the random variables, then it is appropriate to consider the probability distribution
of that random variable alone. This is known as the marginal distribution of the random
variable and can be obtained quite simply by summing or integrating the joint probability
distribution over the values of the other random variable.
For example, for two discrete random variables X and Y, the probability values of the
marginal distribution of X are
P(X = xi) = pi+ =

j
pi j
and for two continuous random variables, the probability density function of the marginal
distribution of X is
fX(x) =
 ∞
−∞
f (x, y) dy
where in practice the summation and integration limits can be curtailed at the appropriate
boundaries of the state space. Note that the marginal distribution of a random variable X
and the marginal distribution of a random variable Y do not uniquely determine their joint
distribution.
Marginal Probability Distributions
The marginal distribution of a random variable X is obtained from the joint
probability distribution of two random variables X and Y by summing or integrating
over the values of the random variable Y. The marginal distribution is the individual
probability distribution of the random variable X considered alone.
The expectations and variances of the random variables X and Y can be obtained from
their marginal distributions in the usual manner, as illustrated in the following examples.
Example 19
Air Conditioner
Maintenance
The marginal probability mass function of X, the time taken to service the air conditioner units
at a particular location, is given in Figure 2.60 and is obtained by summing the appropriate
values of the joint probability mass function. For example,
P(X = 1) =
3

j=1
p1 j = 0.12 + 0.08 + 0.01 = 0.21
The expected service time is
E(X) =
4

i=1
i P(X = i)
= (1 × 0.21) + (2 × 0.24) + (3 × 0.30) + (4 × 0.25) = 2.59
Since
E(X2) =
4

i=1
i2P(X = i)
= (1 × 0.21) + (4 × 0.24) + (9 × 0.30) + (16 × 0.25) = 7.87

118
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.60
Marginal probability mass
functions for air conditioner
maintenance example
0.21
0.25
0.24
0.30
  Marginal
distribution
     of Y
Marginal distribution of X
1
2
0.01
0.01
0.02
0.07
0.11
3
4
0.12
0.08
0.05
0.07
0.32
1
3
X = service time (hrs)
0.08
0.15
0.21
0.13
0.57
2
Y = number of
air conditioner units
1
2
3
4
Probability
Service time (hrs)
0.21
0.24
0.30
0.25
σ = 1.08
σ = 1.08
E(X) = 2.59
FIGURE 2.61
Marginal probability mass function of the service time
2
1
3
Probability
E(X) = 1.79
Number of units
0.32
0.11
0.57
 σ = 0.62
 σ = 0.62
FIGURE 2.62
Marginal probability mass function of the number of air
conditioner units
the variance in the service times is
Var(X) = E(X2) −(E(X))2 = 7.87 −2.592 = 1.162
The standard deviation is therefore σ =
√
1.162 = 1.08 hours, or about 65 minutes, as
indicated in Figure 2.61.
The marginal probability mass function of Y, the number of air conditioner units at a
particular location, is given in Figure 2.62. Here,
P(Y = 1) =
4

i=1
pi1 = 0.12 + 0.08 + 0.07 + 0.05 = 0.32

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
119
FIGURE 2.63
Marginal probability density
function of zinc content
x
=
σ
0.5
1.5
0.23
Zinc content
(x) = 57
40
51
10
x
−
1 2
(
)
−
fX
=
σ
0.23
E(X) = 1.0
for example. The expected number of air conditioner units can be calculated to be E(Y) =
1.79, and the standard deviation is σ = 0.62.
Example 20
Mineral Deposits
The marginal probability density function of X, the zinc content of the ore, is
fX(x) =
 y=35.0
y=20.0
f (x, y) dy
=
 y=35.0
y=20.0
 39
400 −17(x −1)2
50
−(y −25)2
10,000

dy
=
39y
400 −17y(x −1)2
50
−(y −25)3
30,000
y=35.0
y=20.0
= 57
40 −51(x −1)2
10
for 0.5 ≤x ≤1.5. This is shown in Figure 2.63, and since it is symmetric about the point
x = 1, the expected zinc content is E(X) = 1. The variance of the zinc content is
Var(X) = E((X −E(X))2)
=
 1.5
0.5
(x −1)2 fX(x)dx =
 1.5
0.5
(x −1)2
57
40 −51(x −1)2
10

dx
=
19
40(x −1)3 −51
50(x −1)5
1.5
0.5
= [0.0275] −[−0.0275] = 0.055
and the standard deviation is therefore σ =
√
0.055 = 0.23.
Theprobabilitythatasampleoforehasazinccontentbetween0.8and1.0canbecalculated
from the marginal probability density function to be
P(0.8 ≤X ≤1.0) =
 1.0
0.8
fX(x)dx =
 1.0
0.8
57
40 −51(x −1)2
10

dx
=
57x
40 −17(x −1)3
10
1.0
0.8
= [1.425] −[1.1536] = 0.2714
Consequently about 27% of the ore has a zinc content within these limits.

120
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.64
Marginal probability density
function of iron content
y
20
35
 = 4.27
E(Y) = 27.36
Iron content
 = 4.27
f ( ) =
y
10,000
(y − 25)2
83
1200 −
Y
The marginal probability density function of Y, the iron content of the ore, is
fY(y) =
 x=1.5
x = 0.5
f (x, y)dx =
 x =1.5
x = 0.5
 39
400 −17(x −1)2
50
−(y −25)2
10,000

dx
=
39x
400 −17(x −1)3
150
−x(y −25)2
10,000
x =1.5
x = 0.5
=
83
1200 −(y −25)2
10,000
for 20.0 ≤y ≤35.0. This is shown in Figure 2.64 together with the expected iron content
and the standard deviation of the iron content, which can be calculated to be E(Y) = 27.36
and σ = 4.27.
2.5.3
Conditional Probability Distributions
If two random variables X and Y are jointly distributed, then it is sometimes useful to consider
the distribution of one random variable conditional on the other random variable having taken
a particular value. Conditional probabilities were discussed in Section 1.4, and they allow
probabilities, or more generally random variable distributions, to be revised following the
observation of a certain event.
If two discrete random variables X and Y are jointly distributed, then the conditional
distribution of random variable X conditional on the event Y = y j consists of the probability
values
pi|Y=y j = P(X = xi|Y = y j) = P(X = xi, Y = y j)
P(Y = y j)
= pi j
p+ j
where p+ j = P(Y = y j) = 
i pi j. If two continuous random variables X and Y are jointly
distributed, then the conditional distribution of random variable X conditional on the event
Y = y has a probability density function
fX|Y=y(x) = f (x, y)
fY(y)
where the denominator fY(y) is the marginal distribution of the random variable Y. Condi-
tional expectations and variances can be calculated in the usual manner from these conditional
distributions.

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
121
Conditional Probability Distributions
The conditional distribution of a random variable X conditional on a random
variable Y taking a particular value summarizes the probabilistic properties of the
random variable X under the knowledge provided by the value of Y. It consists of the
probability values
pi|Y=y j = P(X = xi|Y = y j) = P(X = xi, Y = y j)
P(Y = y j)
= pi j
p+ j
for discrete random variables or the probability density function
fX|Y=y(x) = f (x, y)
fY(y)
for continuous random variables, where fY(y) is the marginal distribution of the
random variable Y.
It is important to recognize the difference between a marginal distribution and a con-
ditional distribution. The marginal distribution for X is the appropriate distribution for the
random variable X when nothing is known about the random variable Y. In contrast, the condi-
tional distribution for X conditional on a particular value y of Y is the appropriate distribution
for the random variable X when the random variable Y is known to take the value y. This
difference is illustrated in the following examples.
Example 19
Air Conditioner
Maintenance
Suppose that a technician is visiting a location that is known to have three air conditioner
units, an event that has a probability of
P(Y = 3) = p+3 = 0.01 + 0.01 + 0.02 + 0.07 = 0.11
The conditional distribution of the service time X consists of the probability values
p1|Y=3 = P(X = 1|Y = 3) = p13
p+3
= 0.01
0.11 = 0.091
p2|Y=3 = P(X = 2|Y = 3) = p23
p+3
= 0.01
0.11 = 0.091
p3|Y=3 = P(X = 3|Y = 3) = p33
p+3
= 0.02
0.11 = 0.182
p4|Y=3 = P(X = 4|Y = 3) = p43
p+3
= 0.07
0.11 = 0.636
These values are shown in Figure 2.65, and they are clearly different from the marginal
distribution of the service time given in Figure 2.61. Conditioning on a location having three
air conditioner units increases the chances of a large service time being required.
The conditional expectation of the service time is
E(X|Y = 3) =
4

i=1
ipi|Y=3
= (1 × 0.091) + (2 × 0.091) + (3 × 0.182) + (4 × 0.636) = 3.36
which, as expected, is considerably larger than the “overall” expected service time of 2.59
hours. The difference between these expected values can be interpreted in the following way.

122
CHAPTER 2
RANDOM VARIABLES
4
3
2
1
Probability
E(X|Y = 3) = 3.36
Service time (hrs)
0.182
0.091
0.091
0.636
FIGURE 2.65
Conditional probability mass function of service time when Y = 3
y
20
35
Iron content
σ = 4.14
σ = 4.14
f
( ) =
y
(y − 25)2
−
0.073
3922.5
Y|X = 0.55
E(Y|X = 0.55) = 27.14
FIGURE 2.66
Conditional probability density function of iron content when
X = 0.55
If a technician sets off for a location for which the number of air conditioner units is not
known, then the expected service time at the location is 2.59 hours. However, if the technician
knows that there are three air conditioner units at the location that need servicing, then the
expected service time is 3.36 hours.
Example 20
Mineral Deposits
Suppose that a sample of ore has a zinc content of X = 0.55. What is known about its iron
content? The information about the iron content Y is summarized in the conditional probability
density function for the iron content, which is
fY|X= 0.55(y) = f (0.55, y)
fX(0.55)
where the denominator is the marginal distribution of the zinc content X evaluated at 0.55.
Since
fX(0.55) = 57
40 −51(0.55 −1.00)2
10
= 0.39225
the conditional probability density function is
fY|X= 0.55(y) = f (0.55, y)
0.39225
=
39
400 × 0.39225 −17(0.55 −1.00)2
50 × 0.39225
−
(y −25)2
10,000 × 0.39225
= 0.073 −(y −25)2
3922.5
for 20.0 ≤y ≤35.0. This is shown in Figure 2.66 with the conditional expectation of the
iron content, which can be calculated to be 27.14, and with the conditional standard deviation,
which is 4.14.

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
123
2.5.4
Independence and Covariance
In the same way that two events A and B were said to be independent in Chapter 1 if they
are “unrelated” to each other, two random variables X and Y are said to be independent if
the value taken by one random variable is “unrelated” to the value taken by the other random
variable. More speciﬁcally, the random variables are independent if the distribution of one of
the random variables does not depend upon the value taken by the other random variable.
Independent Random Variables
Two random variables X and Y are deﬁned to be independent if their joint probability
mass function or joint probability density function is the product of their two marginal
distributions. If the random variables are discrete, then they are independent if
pi j = pi+ p+ j
for all values of xi and y j. If the random variables are continuous, then they are
independent if
f (x, y) = fX(x) fY(y)
for all values of x and y. If two random variables are independent, then the probability
distribution of one of the random variables does not depend upon the value taken by
the other random variable.
Notice that if the random variables X and Y are independent, then their conditional
distributions are identical to their marginal distributions. If the random variables are discrete,
this is because
pi|Y=y j = pi j
p+ j
= pi+ p+ j
p+ j
= pi+
and if the random variables are continuous, this is because
fX|Y=y(x) = f (x, y)
fY(y) = fX(x) fY(y)
fY(y)
= fX(x)
In either case, the conditional distributions do not depend upon the value conditioned upon, and
they are equal to the marginal distributions. This result has the interpretation that knowledge
of the value taken by the random variable Y does not inﬂuence the distribution of the random
variable X, and vice versa.
As a simple example of two independent random variables, suppose that X and Y have a
joint probability density function of
f (x, y) = 6xy2
for 0 ≤x ≤1 and 0 ≤y ≤1 and f (x, y) = 0 elsewhere. The fact that this joint density
function is a function of x multiplied by a function of y (and that the state spaces of the random
variables [0, 1] do not depend upon each other) immediately indicates that the two random
variables are independent. Speciﬁcally, the marginal distribution of X is
fX(x) =
 1
y= 0
6xy2 dy = 2x
for 0 ≤x ≤1, and the marginal distribution of Y is
fY(y) =
 1
x= 0
6xy2 dx = 3y2

124
CHAPTER 2
RANDOM VARIABLES
1
2
1
X
0
0
1/8
1/4
1/8
1/8
1/4
1/8
1/2
1/4
1/2
1/4
1/2
  Marginal
distribution
    of Y
Y
Marginal distribution of X
FIGURE 2.67
Joint probability mass function and marginal
probability mass functions for X and Y in
coin-tossing game
  Marginal
distribution
    of Z
1
2
1
2
X
0
Z
0
1/8
1/4
0
0
1/8
1/8
1/8
1/8
1/8
1/4
1/2
1/4
1/4
1/2
1/4
Marginal distribution of X
FIGURE 2.68
Joint probability mass function and marginal
probability mass functions for X and Z in
coin-tossing game
for 0 ≤y ≤1. The fact that f (x, y) = fX(x) fY(y) conﬁrms that the random variables X
and Y are independent.
GAMES OF CHANCE
Suppose that a fair coin is tossed three times so that there are eight equally likely outcomes,
and that the random variable X is the number of heads obtained in the ﬁrst and second tosses,
the random variable Y is the number of heads in the third toss, and the random variable Z is
the number of heads obtained in the second and third tosses.
The joint probability mass function of X and Y is given in Figure 2.67 together with the
marginal distributions of X and Y. For example, P(X = 0, Y = 0) = P(T T T ) = 1/8 and
P(X = 0) = P(T T T ) + P(T T H) = 1/4. It is easy to check that
P(X = i, Y = j) = P(X = i)P(Y = j)
for all values of i = 0, 1, 2 and j = 0, 1, so that the joint probability mass function is equal
to the product of the two marginal probability mass functions. Consequently, the random
variables X and Y are independent, which is not surprising since the outcome of the third coin
toss should be unrelated to the outcomes of the ﬁrst two coin tosses.
Figure2.68showsthejointprobabilitymassfunctionof X and Z togetherwiththemarginal
distributions of X and Z. For example, P(X = 1, Z = 1) = P(HT H) + P(T HT ) = 1/4.
Notice, however, that
P(X = 0, Z = 0) = P(T T T ) = 1
8
P(X = 0) = P(T T H) + P(T T T ) = 1
4
and
P(Z = 0) = P(HT T ) + P(T T T ) = 1
4

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
125
so that
P(X = 0, Z = 0) ̸= P(X = 0)P(Z = 0)
This result indicates that the random variables X and Z are not independent. In fact, their
dependence is a result of their both depending upon the result of the second coin toss.
The strength of the dependence of two random variables on each other is indicated by
their covariance, which is deﬁned to be
Cov(X, Y) = E((X −E(X))(Y −E(Y)))
The covariance can be any positive or negative number, and independent random variables
have a covariance of zero. It is often convenient to calculate the covariance from an alternative
expression
Cov(X, Y) = E((X −E(X))(Y −E(Y)))
= E(XY −X E(Y) −E(X)Y + E(X)E(Y))
= E(XY) −E(X)E(Y) −E(X)E(Y) + E(X)E(Y)
= E(XY) −E(X)E(Y)
Covariance
The covariance of two random variables X and Y is deﬁned to be
Cov(X, Y) = E((X −E(X))(Y −E(Y))) = E(XY) −E(X)E(Y)
The covariance can be any positive or negative number, and independent random
variables have a covariance of 0.
In practice, the most convenient way to assess the strength of the dependence between
two random variables is through their correlation.
Correlation
The correlation between two random variables X and Y is deﬁned to be
Corr(X, Y) =
Cov(X, Y)
√Var(X)Var(Y)
The correlation takes values between −1 and 1, and independent random variables
have a correlation of 0.
Random variables with a positive correlation are said to be positively correlated, and in
such cases there is a tendency for high values of one random variable to be associated with
high values of the other random variable. Random variables with a negative correlation are
said to be negatively correlated, and in such cases there is a tendency for high values of one
random variable to be associated with low values of the other random variable. The strength
of these tendencies increases as the correlation moves further away from 0 to 1 or to −1.
As an illustration of the calculation of a covariance, consider again the simple example
where
f (x, y) = 6xy2

126
CHAPTER 2
RANDOM VARIABLES
for 0 ≤x ≤1 and 0 ≤y ≤1. The expectation of the random variable X can be calculated
from its marginal distribution to be
E(X) =
 1
x = 0
x fX(x) dx =
 1
x = 0
2x2 dx = 2
3
and similarly
E(Y) =
 1
y = 0
y fY(y) dy =
 1
y = 0
3y3 dy = 3
4
Also,
E(XY) =
 1
x = 0
 1
y = 0
xy f (x, y) dy dx =
 1
x = 0
 1
y = 0
6x2y3 dy dx = 1
2
so that
Cov(X, Y) = E(XY) −E(X)E(Y) = 1
2 −
2
3 × 3
4

= 0
This result is expected since the random variables X and Y were shown to be independent.
Example 19
Air Conditioner
Maintenance
The expected service time is E(X) = 2.59 hours, and the expected number of units serviced
is E(Y) = 1.79. In addition,
E(XY) =
4

i=1
3

j=1
i j pi j
= (1 × 1 × 0.12) + (1 × 2 × 0.08) + · · · + (4 × 3 × 0.07) = 4.86
so that the covariance is
Cov(X, Y) = E(XY) −E(X)E(Y) = 4.86 −(2.59 × 1.79) = 0.224
Since Var(X) = 1.162 and Var(Y) = 0.384, the correlation between the service time and the
number of units serviced is
Corr(X, Y) =
Cov(X, Y)
√Var(X)Var(Y) =
0.224
√1.162 × 0.384 = 0.34
As expected, the service time and the number of units serviced are not independent but are
positively correlated. This makes sense because there is a tendency for locations with a large
number of air conditioner units to require relatively long service times.
GAMES OF CHANCE
Consider again the tossing of three coins and the random variables X, Y, and Z. It is easy to
check that E(X) = 1 and E(Y) = 1/2, and that
E(XY) =
2

i = 0
1

j = 0
i j pi j =

1 × 1 × 1
4

+

2 × 1 × 1
8

= 1
2
Consequently,
Cov(X, Y) = E(XY) −E(X)E(Y) = 1
2 −

1 × 1
2

= 0
which is expected because the random variables X and Y are independent.

2.5 JOINTLY DISTRIBUTED RANDOM VARIABLES
127
However, E(Z) = 1 and
E(X Z) =
2

i = 0
2

j = 0
i j pi j
=

1 × 1 × 1
4

+

2 × 1 × 1
8

+

1 × 2 × 1
8

+

2 × 2 × 1
8

= 5
4
so that
Cov(X, Z) = E(X Z) −E(X)E(Z) = 5
4 −(1 × 1) = 1
4
Also, since Var(X) = Var(Z) = 1/2,
Corr(X, Z) =
Cov(X, Z)
√Var(X)Var(Z) =
1
4

1
2 × 1
2
= 1
2
so that the random variables X and Z are positively correlated.
2.5.5
Problems
2.5.1 Consider Example 20 on mineral deposits.
(a) Show that P(0.8 ≤X ≤1, 25 ≤Y ≤30) = 0.092.
(b) Show that the iron content has an expected value of
27.36 and a standard deviation of 4.27.
(c) Show that conditional on X = 0.55, the iron content
has an expected value of 27.14 and a standard
deviation of 4.14.
2.5.2 Consider Example 19 on air conditioner maintenance.
(a) Suppose that a location has only one air conditioner
that needs servicing. What is the conditional
probability mass function of the service time
required, and the conditional expectation and
standard deviation of the service time?
(b) Suppose that a location requires a service time of
two hours. What is the conditional probability mass
function of the number of air conditioner units
serviced, and the conditional expectation and
standard deviation of the number of air conditioner
units serviced?
2.5.3 Suppose that two continuous random variables X and Y
have a joint probability density function
f (x, y) = A(x −3)y
for −2 ≤x ≤3 and 4 ≤y ≤6, and f (x, y) = 0
elsewhere.
(a) What is the value of A?
(b) What is P(0 ≤X ≤1, 4 ≤Y ≤5)?
(c) Construct the marginal probability density functions
fX(x) and fY(y).
(d) Are the random variables X and Y independent?
(e) If Y = 5, what is the conditional probability density
function of X?
2.5.4 A fair coin is tossed four times, and the random variable
X is the number of heads in the ﬁrst three tosses and the
random variable Y is the number of heads in the last three
tosses.
(a) What is the joint probability mass function of X
and Y?
(b) What are the marginal probability mass functions of
X and Y?
(c) Are the random variables X and Y independent?
(d) What are the expectations and variances of the
random variables X and Y?
(e) What is the covariance of X and Y?
(f) If there is one head in the last three tosses, what is the
conditional probability mass function of X? What are
the conditional expectation and variance of X?
2.5.5 Suppose that two continuous random variables X and Y
have a joint probability density function
f (x, y) = A	
ex+y + e2x−y
for 1 ≤x ≤2 and 0 ≤y ≤3, and f (x, y) = 0
elsewhere.

128
CHAPTER 2
RANDOM VARIABLES
(a) What is the value of A?
(b) What is P(1.5 ≤X ≤2, 1 ≤Y ≤2)?
(c) Construct the marginal probability density functions
fX(x) and fY(y).
(d) Are the random variables X and Y independent?
(e) If Y = 0, what is the conditional probability density
function of X?
2.5.6 Two cards are drawn without replacement from a pack of
cards, and the random variable X measures the number of
hearts drawn and the random variable Y measures the
number of clubs drawn.
(a) What is the joint probability mass function of X
and Y?
(b) What are the marginal probability mass functions of
X and Y?
(c) Are the random variables X and Y independent?
(d) What are the expectations and variances of the
random variables X and Y?
(e) What is the covariance of X and Y?
(f) What is the correlation between X and Y?
(g) If no hearts are drawn, what is the conditional
probability mass function of Y? If one heart is drawn,
what is the conditional probability mass function
of Y?
2.5.7 Repeat Problem 2.5.6 when the second card is drawn with
replacement.
2.5.8 The random variable X measures the concentration of
ethanol in a chemical solution, and the random variable Y
measures the acidity of the solution. They have a joint
probability density function
f (x, y) = A(20 −x −2y)
for 0 ≤x ≤5 and 0 ≤y ≤5, and f (x, y) = 0
elsewhere.
(a) What is the value of A?
(b) What is P(1 ≤X ≤2, 2 ≤Y ≤3)?
(c) Construct the marginal probability density functions
fX(x) and fY(y).
(d) Are the ethanol concentration and the acidity
independent?
(e) What are the expectation and the variance of the
ethanol concentration?
(f) What are the expectation and the variance of the
acidity?
(g) If the ethanol concentration is 3, what is the
conditional probability density function of the
acidity?
(h) What is the covariance between the ethanol
concentration and the acidity?
(i) What is the correlation between the ethanol
concentration and the acidity?
2.5.9 Two safety inspectors inspect a new building and assign it
a “safety score” of 1, 2, 3, or 4. Suppose that the random
variable X is the score assigned by the ﬁrst inspector and
the random variable Y is the score assigned by the second
inspector, and that they have a joint probability mass
function given in Figure 2.69.
1
2
4
0.09
0.03
0.01
0.01
1
3
X
Y
3
0.01
0.01
0.24
0.04
0.15
2
0.01
0.02
0.03
0.00
0.01
0.02
0.32
4
FIGURE 2.69
Joint probability mass function for safety scores
(a) What is the probability that both inspectors assign the
same safety score?
(b) What is the probability that the second inspector
assigns a higher safety score than the ﬁrst inspector?
(c) What are the marginal probability mass function,
expectation, and variance of the score assigned by the
ﬁrst inspector?
(d) What are the marginal probability mass function,
expectation, and variance of the score assigned by the
second inspector?
(e) Are the scores assigned by the two inspectors
independent of each other? Would you expect them to
be independent? How would you interpret the
situation if they were independent?
(f) If the ﬁrst inspector assigns a score of 3, what is the
conditional probability mass function of the score
assigned by the second inspector?
(g) What is the covariance of the scores assigned by the
two inspectors?
(h) What is the correlation between the scores assigned
by the two inspectors? If you are responsible for

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
129
training the safety inspectors to perform proper safety
evaluations of buildings, what correlation value
would you like there to be between the scores of two
safety inspectors?
2.5.10 Joint probability distributions of three or more random
variables can be interpreted by extending the ideas in this
section. For example, suppose that three continuous
random variables X, Y, and Z have a joint probability
density function
f (x, y, z) = 3xyz2
32
for 0 ≤x ≤2, 0 ≤y ≤2, and 0 ≤z ≤2, and
f (x, y, z) = 0 elsewhere.
(a) Establish that this is a valid joint probability density
function by showing that it is always positive within
the state space 0 ≤x ≤2, 0 ≤y ≤2, and 0 ≤z ≤2,
and that the total probability is equal to 1.
(b) In general,
P(a ≤X ≤b, c ≤Y ≤d, e ≤Z ≤f )
=
 b
x = a
 d
y = c

f
z = e
f (x, y, z) dx dy dz
What is P(0 ≤X ≤1, 0.5 ≤Y ≤1.5, 1 ≤Z ≤2)?
(c) The marginal probability density function of a
particular random variable can be calculated from the
joint probability density function by integrating over
all values of the other random variables. What is the
marginal probability density function of the random
variable X?
2.6
Combinations and Functions of Random Variables
2.6.1
Linear Functions of a Random Variable
A linear function of a random variable X is another random variable
Y = aX + b
for some numbers a, b ∈R. It is a general result that
E(Y) = aE(X) + b
so that the expectation of the random variable Y is just the same linear function of the
expectation of the random variable X. However,
Var(Y) = a2Var(X)
and the standard deviation of Y, σY, is given by
σY = |a|σX
where σX is the standard deviation of X. Notice that the “shift” amount b does not inﬂuence
the variance of Y and that the “scale” amount a is squared in the relationship between the two
variances.
Linear Functions of a Random Variable
If X is a random variable and Y = aX + b for some numbers a, b ∈R, then
E(Y) = aE(X) + b
and
Var(Y) = a2Var(X)
An important application of this result concerns the “standardization” of a random variable
to have a zero mean and a unit variance. If a random variable X has an expectation of μ and

130
CHAPTER 2
RANDOM VARIABLES
a variance of σ 2, notice that the result above implies that the random variable
Y = X −μ
σ
= 1
σ X +

−μ
σ

has an expectation of 0 and a variance of 1. This standardization of a random variable is often
useful, and notice that it is performed by ﬁrst subtracting the mean from the random variable
and then dividing by the standard deviation of the random variable.
The actual cumulative distribution function FY(y) of the random variable Y = aX + b
can be obtained from the cumulative distribution function FX(x) of the random variable X by
noting that
FY(y) = P(Y ≤y) = P(aX + b ≤y) = P(aX ≤y −b)
If a > 0, this gives
FY(y) = P

X ≤y −b
a

= FX
 y −b
a

and if a < 0, this gives
FY(y) = P

X ≥y −b
a

= 1 −FX
 y −b
a

Example 21
Test Score
Standardization
Suppose that the raw scores X from a particular testing procedure are distributed between −5
and 20 with an expected value of 10 and a variance of 7. In order to standardize the scores so
that they lie between 0 and 100, the linear transformation
Y = 4X + 20
is applied to the scores. This means, for example, that a raw score of x = 12 corresponds to
a standardized score of y = (4 × 12) + 20 = 68.
The expected value of the standardized scores is then known to be
E(Y) = 4E(X) + 20 = (4 × 10) + 20 = 60
with a variance of
Var(Y) = 42Var(X) = 42 × 7 = 112
The standard deviation of the standardized scores is σY =
√
112 = 10.58, which is 4 × σX =
4 ×
√
7.
Example 22
Chemical Reaction
Temperatures
The temperature X in degrees Fahrenheit of a particular chemical reaction is known to be
distributed between 220◦and 280◦with a probability density function of
fX(x) = x −190
3600
This is shown in Figure 2.70 together with the expectation and standard deviation of the
chemical reaction temperature, which are easily calculated to be E(X) = 255◦and σX =
16.58◦. The variance of the chemical reaction temperature is Var(X) = 275. In addition, the
cumulative distribution function of the chemical reaction temperature is
FX(x) =
 x
z=220
fX(z) dz = (x −190)2
7200
−1
8
Suppose that a chemist wishes to convert the temperatures to degrees Centigrade. If the
random variable Y measures the reaction temperature in degrees Centigrade, then it is obtained

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
131
Temperature  (°F)
220
280
E(X) = 255
x
f( ) =
x
x − 190
3600
σ = 16.58
σ = 16.58
FIGURE 2.70
Probability density function for chemical reaction temperature in
degrees Fahrenheit
137.78
104.44
y
Temperature(∞C)
(
)=
E
123.9
Y
 = 9.21
 = 9.21
f( ) =
y
9y
10,000 1000
−79
FIGURE 2.71
Probability density function for chemical reaction temperature in degrees
Centigrade
as the following linear function of the random variable X
Y = 5
9 X −160
9
Notice that x = 220◦F corresponds to
y = 5
9 × 220 −160
9
= 104.44◦C
and that x = 280◦F corresponds to
y = 5
9 × 280 −160
9
= 137.78◦C
Since a = 5/9 is positive, the cumulative distribution function of Y is
FY(y) = FX
 y −b
a

= FX

y + 160
9
5
9

= (9y −790)2
180,000
−1
8
The probability density function of Y is obtained by differentiation to be
fY(y) = d
dy FY(y) =
9y
10,000 −
79
1000
for 104.44 ≤y ≤137.78, as shown in Figure 2.71. The expectation and variance of Y can be
obtained directly from its probability density function or from the relationships
E(Y) = 5
9 E(X) −160
9
= 5
9 × 255 −160
9
= 123.9
and
Var(Y) =
5
9
2
Var(X) =
5
9
2
× 275 = 84.88

132
CHAPTER 2
RANDOM VARIABLES
2.6.2
Linear Combinations of Random Variables
Given two random variables X1 and X2, it is often useful to consider the random variable
obtained as their sum. It is a general result that
E(X1 + X2) = E(X1) + E(X2)
The expectation of the sum of two random variables is equal to the sum of the
expectations of the two random variables.
Also, in general,
Var(X1 + X2) = Var(X1) + Var(X2) + 2Cov(X1, X2)
Notice that if the two random variables are independent so that their covariance is 0, then the
variance of their sum is equal to the sum of their two variances.
The variance of the sum of two independent random variables is equal to the sum of
the variances of the two random variables.
These are nice simple results, but it is important to remember that whereas the expectation
of the sum of two random variables is always equal to the sum of the expectations of the two
random variables, the variance of the sum of two random variables is equal to the sum of their
variances only when they are independent random variables.
Sums of Random Variables
If X1 and X2 are two random variables, then
E(X1 + X2) = E(X1) + E(X2)
and
Var(X1 + X2) = Var(X1) + Var(X2) + 2Cov(X1, X2)
If X1 and X2 are independent random variables so that Cov(X1, X2) = 0, then
Var(X1 + X2) = Var(X1) + Var(X2).
Consider now a sequence of random variables X1, . . . , Xn together with some constants
a1, . . . , an and b, and deﬁne a new random variable Y to be the linear combination
Y = a1X1 + · · · + an Xn + b
Linear combinations of random variables like this are important in many situations, and it is
useful to obtain some general results for them.
The expectation of the linear combination is
E(Y) = a1E(X1) + · · · + an E(Xn) + b
which is just the linear combination of the expectations of the random variables Xi. Further-
more, if the random variables X1, . . . , Xn are independent of one another, then
Var(Y) = a2
1 Var(X1) + · · · + a2
n Var(Xn)

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
133
Again, notice that the “shift” amount b does not inﬂuence the variance of Y and that the
coefﬁcients ai are squared in this expression.
Linear Combinations of Random Variables
If X1, . . . , Xn is a sequence of random variables and a1, . . . , an and b are constants,
then
E(a1X1 + · · · + an Xn + b) = a1E(X1) + · · · + an E(Xn) + b
If, in addition, the random variables are independent, then
Var(a1X1 + · · · + an Xn + b) = a2
1 Var(X1) + · · · + a2
n Var(Xn)
As an illustration of these results, suppose that two independent random variables X1 and
X2 both have an expectation of μ and a variance of σ 2. Suppose that
Y1 = X1 + X2
is the sum of the two random variables and that
Y2 = X1 −X2
is their difference. Then
E(Y1) = E(X1) + E(X2) = 2μ
and
E(Y2) = E(X1) −E(X2) = 0
so that the sum has an expectation of 2μ and the difference has an expectation of 0.
Since X1 and X2 are independent random variables,
Var(Y1) = Var(X1) + Var(X2) = 2σ 2
In addition,
Var(Y2) = Var(X1 + (−1)X2) = Var(X1) + (−1)2Var(X2) = 2σ 2
so that the variances of the sum and the difference of X1 and X2 are both equal to 2σ 2. It is
also interesting to note that there is “more” variability in the random variables Y1 and Y2 than
in the random variables X1 and X2 since their variances are twice as large. This illustrates the
general rule:
Adding or subtracting independent random variables increases variability.
As another illustration, suppose that X1, . . . , Xn is a sequence of independent random
variables each with an expectation μ and a variance σ 2, and consider the average
¯X = X1 + · · · + Xn
n
In particular, the random variables may represent the observations in a sample of size n from
some population of interest. Then
E( ¯X) = E
1
n X1 + · · · + 1
n Xn

= 1
n E(X1) + · · · + 1
n E(Xn)
= 1
n μ + · · · + 1
n μ = μ

134
CHAPTER 2
RANDOM VARIABLES
and
Var( ¯X) = Var
1
n X1 + · · · + 1
n Xn

=
1
n
2
Var(X1) + · · · +
1
n
2
Var(Xn)
=
1
n
2
σ 2 + · · · +
1
n
2
σ 2 = σ 2
n
Notice that whereas the expectation of the average is equal to the expectations of the individual
random variables μ, the variance of the average is reduced to σ 2/n. This illustrates the general
rule:
Averaging independent random variables reduces variability.
Averaging Independent Random Variables
Suppose that X1, . . . , Xn is a sequence of independent random variables each with an
expectation μ and a variance σ 2, and with an average
¯X = X1 + · · · + Xn
n
Then
E( ¯X) = μ
and
Var( ¯X) = σ 2
n
Figure 2.72 summarizes how the mean and the variance change for simple combinations
of random variables. Notice that (1) doubling a random variable has more variability than
adding two independent realizations of the random variable; (2) the sum of two independent
realizations of a random variable has the same variability as the difference between the two
random variables, and these are larger than the variability of the individual random variable;
(3) averaging two realizations of a random variable has smaller variability than the individual
random variable.
In summary, while the expectation of a combination of random variables is intuitively
obvious and is easy to calculate, be careful when calculating the variance because it can be
less intuitive.
Example 23
Piston Head
Construction
A circular piston head is designed to slide smoothly within a cylinder. However, there is some
variability in the piston head and cylinder sizes about their speciﬁed dimensions, and the
company that manufactures them is interested in how well the piston heads actually ﬁt within
the cylinders.
Suppose that the random variable X1 measures the radius of a piston head, and that it
has an expected value of 30.00 mm and a standard deviation of 0.05 mm. Also, suppose that
the random variable X2 measures the inside radius of a cylinder, with an expected value of
30.25 mm and a standard deviation of 0.06 mm. The “gap” between the piston head and the
cylinder is Y = X2 −X1, as illustrated in Figure 2.73.

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
135
FIGURE 2.72
Expectations and variances of
combinations of a random variable
μ − 3σ
μ + 3σ
μ
0
10
20
μ + 3σ
μ
0
10
20
μ
0
10
20
μ
0
10
20
2 ×
+
−
2
+
μ
μ + 3σ
0
10
20
Var(X) = 1
standard deviation = 1
Var(2X)
= 22 × Var(X)
= 4 × 1 = 4
standard deviation = 2
E(2X)
= 2 × E(X)
= 2 × 10 = 20
Var(X1 + X2)
= Var(X1) + Var(X2)
= 1 + 1 = 2
standard deviation = 1.414
E(X1 + X2)
= E(X1) + E(X2)
= 10 + 10 = 20
Var(X1 – X2)
= Var(X1) + Var(X2)
= 1 + 1 = 2
standard deviation = 1.414
E(X1 – X2)
= E(X1) – E(X2)
= 10 – 10 = 0
Var((X1 + X2)/2)
= Var(X1)/22
 + Var(X2)/22
= 1/4 + 1/4 = 1/2
standard deviation = 0.707
E((X1 + X2)/2)
= E(X1)/2 + E(X2)/2
= 5 + 5 = 10
E(X) = 10
μ − 3σ
μ – 3σ
μ − 3σ
μ +  3σ
μ − 3σ
μ + 3σ
The expected value of the gap is
E(Y) = E(X2) −E(X1) = 30.25 −30.00 = 0.25 mm
Also, since the sizes of a piston head and a cylinder can reasonably be expected to be inde-
pendent, the variance of the gap is
Var(Y) = Var(X2 + (−1)X1) = Var(X2) + (−1)2Var(X1)
= 0.062 + 0.052 = 0.0061
The standard deviation of the gap is therefore σY =
√
0.0061 = 0.078 mm, which is larger
than the standard deviations of both the piston heads and the cylinders.

136
CHAPTER 2
RANDOM VARIABLES
FIGURE 2.73
Cross-sectional view of piston head
within cylinder
Cylinder
Piston 
head
Gap=X2
1
X
−
X2
1
X
Example 21
Test Score
Standardization
Suppose that in a certain examination procedure candidates must take two tests, and that X1
measures the score of a candidate on test 1 and X2 measures the score of a candidate on
test 2. Furthermore, suppose that the scores on test 1 are distributed between 0 and 30 with an
expectation of 18 and a variance of 24, and that the scores on test 2 are distributed between
−10 and 50 with an expectation of 30 and a variance of 60.
The examining board wishes to standardize each test score to lie between 0 and 100, and
to then calculate a ﬁnal score out of 100 that is weighted 2/3 from test 1 and 1/3 from test 2.
The standardized scores of the two tests are
Y1 = 10
3 X1
and
Y2 = 5
3 X2 + 50
3
so that the ﬁnal score is
Z = 2
3Y1 + 1
3Y2 = 20
9 X1 + 5
9 X2 + 50
9
Thus a candidate receiving a score of x1 = 11 on test 1 and a score of x2 = 2 on test 2 receives
a ﬁnal score of
z =
20
9 × 11

+
5
9 × 2

+ 50
9 = 31.11
The expected value of the ﬁnal score is
E(Z) = 20
9 E(X1) + 5
9 E(X2) + 50
9
=
20
9 × 18

+
5
9 × 30

+ 50
9 = 62.22
What is the variance of the ﬁnal score? At this point it is important to remember that
the scores a candidate receives on the two tests may not be independent. In fact, they will be
independentonlyiftheymeasuretwounrelatedqualitiesofthecandidate.If,forexample,test1

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
137
measures a candidate’s proﬁciency at probability and test 2 measures a candidate’s proﬁciency
at statistics, then the test scores should not be independent and the calculation of the variance
of the ﬁnal score actually requires knowledge of the covariance between the two test scores.
On the other hand, suppose that test 1 measures a candidate’s proﬁciency at probability
but that test 2 measures a candidate’s athletic abilities, then it is reasonable to assume that the
two test scores are independent of each other. In this case, the variance of the ﬁnal score is
Var(Z) = Var
20
9 X1 + 5
9 X2 + 50
9

=
20
9
2
× Var(X1) +
5
9
2
× Var(X2)
=
20
9
2
× 24 +
5
9
2
× 60 = 137.04
The standard deviation of the ﬁnal score is therefore σZ =
√
137.04 = 11.71.
GAMES OF CHANCE
Suppose that a fair coin is tossed 100 times. What are the expectation and the variance of the
number of heads obtained? This problem can easily be solved by deﬁning the random variable
Xi to be equal to 1 if the ith coin toss results in a head, and to be equal to 0 if the ith coin toss
results in a tail. The number of heads obtained is therefore
Y = X1 + · · · + X100
The expectation of Xi is clearly 1/2, and the variance can easily be shown to be 1/4.
Therefore,
E(Y) = E(X1) + · · · + E(X100) = 1
2 + · · · + 1
2 = 100
2
= 50
and since the coin tosses are independent,
Var(Y) = Var(X1) + · · · + Var(X100) = 1
4 + · · · + 1
4 = 100
4
= 25
Hence, the expected number of heads obtained is 50 with a standard deviation of 5. This is an
example of the binomial distribution that is discussed in Section 3.1.
Suppose now that a die is rolled 10 times and the average of the 10 scores is recorded. The
probability mass function of the average score is tedious to compute, but again, the expectation
and variance of the average score are easily calculated.
Since the score from the roll of a single die has an expected value of μ = 3.5 and a
variance of σ 2 = 35/12 (see Section 2.4.2), the average score also has an expected value of
3.5 but has a variance of σ 2/10 = 35/120 = 7/24. The standard deviation of the average
score is

7/24 = 0.54.
2.6.3
Nonlinear Functions of a Random Variable
A nonlinear function of a random variable X is another random variable Y = g(X) for some
nonlinear function g. Some useful examples might be Y = X2, Y =
√
X, Y = eX, and Y =
1/(1 + X). There are no general results that relate the expectation and variance of the random
variable Y to the expectation and variance of the random variable X. Usually, the easiest way
to construct the probability distribution of the random variable Y is to construct its cumulative
distribution function from the cumulative distribution function of the random variable X.
For example, suppose that the random variable X is distributed between 0 and 1 with a
probability density function of
fX(x) = 1

138
CHAPTER 2
RANDOM VARIABLES
for 0 ≤x ≤1 and f (x) = 0 elsewhere. Since the probability density function is symmetric
about x = 0.5, the expectation of X is 0.5. Also, the cumulative distribution function of X is
FX(x) = x
for 0 ≤x ≤1.
Consider the random variable Y deﬁned to be
Y = eX
Clearly, Y takes values between 1 and e = 2.718, and its cumulative distribution function is
FY(y) = P(Y ≤y) = P(eX ≤y) = P(X ≤ln(y)) = FX(ln(y)) = ln(y)
The probability density function of Y is obtained by differentiation to be
fY(y) = dFY(y)
dy
= 1
y
for 1 ≤y ≤2.718, which is shown together with the probability density function of X in
Figure 2.74. Notice that
E(Y) =
 2.718
z=1
z fY(z) dz =
 2.718
z=1
1 dz = 2.718 −1 = 1.718
FIGURE 2.74
Comparison of the probability
density functions of X and Y = ex
E(Y) = 1.718
1.000
2.718
y
fY(y) = 1y
0
1
x
E(X) = 0.5
fX(x) = 1

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
139
and that, even though Y = eX,
E(Y) ̸= eE(X) = e0.5 = 1.649
Example 23
Piston Head
Construction
Suppose that X1, the radius of the piston heads, actually takes values between 29.9 mm and
30.1 mm with a probability density function of
fX1(x) = 5
for 29.9 ≤x ≤30.1 and f (x) = 0 elsewhere. The cumulative distribution function of X1 is
therefore
FX1(x) =
 x
z=29.9
fX1(z) dz = 5x −149.5
for 29.9 ≤x ≤30.1.
The piston manufacturers are particularly interested in the area of the piston head since
this directly affects the performance of the piston. The area of the piston head is
Y = π X2
1
which takes values between π29.92 = 2808.6 mm2 and π30.12 = 2846.3 mm2. The cumu-
lative distribution function of the piston head area is
FY(y) = P(Y ≤y) = P
	
π X2
1 ≤y

= P
	
X1 ≤

y/π

= FX1
	
y/π

= 2.821√y −149.5
so that the probability density function is
fY(y) = dFY(y)
dy
= 1.410
√y
for 2808.6 ≤y ≤2846.3, as shown in Figure 2.75.
FIGURE 2.75
Probability density function of
piston head area
2808.6
2846.3
Piston head area
fY (y)
1.410
_____
y
=
√
y

140
CHAPTER 2
RANDOM VARIABLES
2.6.4
Problems
2.6.1 Suppose that the random variables X, Y, and Z are
independent with E(X) = 2, Var(X) = 4, E(Y) = −3,
Var(Y) = 2, E(Z) = 8, and Var(Z) = 7. Calculate the
expectation and variance of the following random
variables.
(a) 3X + 7
(b) 5X −9
(c) 2X + 6Y
(d) 4X −3Y
(e) 5X −9Z + 8
(f) −3Y −Z −5
(g) X + 2Y + 3Z
(h) 6X + 2Y −Z + 16
2.6.2 Recall that for any function g(X) of a random variable X,
E(g(X)) =

g(x) f (x) dx
where f (x) is the probability density function of X. Use
this result to show that
E(aX + b) = aE(X) + b
and
Var(aX + b) = a2Var(X)
2.6.3 Suppose that X1, X2, and X3 are independent random
variables each with a mean of μ and a variance of σ 2.
Compare the means and variances of
Y = 3X1
and
Z = X1 + X2 + X3
2.6.4 A machine part is assembled by fastening two
components of type A and one component of type B end
to end. Suppose that the lengths of components of type A
have an expectation of 37.0 mm and a standard deviation
of 0.7 mm, whereas the lengths of components of type B
have an expectation of 24.0 mm and a standard deviation
of 0.3 mm. What are the expectation and variance of the
length of the machine part?
2.6.5 In a game a player either wins $10 with a probability of
1/8 or loses $1 with a probability of 7/8. What are the
expectation and standard deviation of the total net
winnings of a player after 50 turns at the game?
2.6.6 The weight of a certain type of brick has an expectation
of 1.12 kg with a standard deviation of 0.03 kg.
(a) What are the expectation and variance of the average
weight of 25 bricks randomly selected?
(b) How many bricks need to be selected so that their
average weight has a standard deviation of no more
than 0.005 kg?
2.6.7 Ten cards are drawn with replacement from a pack of
cards. What are the expectation and variance of the
number of aces drawn? If the drawings are made without
replacement, what is the expected number of aces drawn?
2.6.8 Suppose that the random variable X has a probability
density function
f (x) = 2x
for 0 ≤x ≤1. Find the probability density function and
the expectation of the random variable Y in the following
cases.
(a) Y = X3
(b) Y =
√
X
(c) Y = 1/(1 + X)
(d) Y = 2X
2.6.9 The radius of a soap bubble has a probability density
function
f (r) = A(1 −(r −1)2)
for 0 ≤r ≤2.
(a) What is the value of A?
(b) What is the probability density function of the
volume of the soap bubble?
(c) What is the expected value of the volume of the soap
bubble?
2.6.10 A rod of length L is bent until it snaps in two. The point
of breakage X, as measured from one end of the rod, has
a probability density function
f (x) = Ax(L −x)
for 0 ≤x ≤L.
(a) What is the value of A?
(b) What is the probability density function of the
difference in the lengths of the two pieces of the rod?
(c) What is the expected difference in the lengths of the
two pieces of the rod?
2.6.11 If $x is invested in mutual fund A, the annual return has an
expectation of $0.1x and a standard deviation of $0.02x.
If $x is invested in mutual fund B, the annual return has
an expectation of $0.1x and a standard deviation of
$0.03x. Suppose that the returns on the two funds are
independent of each other and that I have $1000 to invest.
(a) What are the expectation and variance of my annual
return if I invest all my money in fund A?
(b) What are the expectation and variance of my annual
return if I invest all my money in fund B?
(c) What are the expectation and variance of my total
annual return if I invest half of my money in fund A
and half in fund B?

2.6 COMBINATIONS AND FUNCTIONS OF RANDOM VARIABLES
141
(d) Suppose I invest $x in fund A and the rest of my
money in fund B. What value of x minimizes the
variance of my total annual return?
Explain why your answers illustrate the importance of
diversity in an investment strategy.
2.6.12 Recall Problems 2.2.11, 2.3.17, and 2.4.13 concerning
the resistance of an electrical component. Suppose that
ﬁve of the components are used in an electrical circuit
so that the total resistance is the sum of the ﬁve individual
resistances. Furthermore, suppose that it is reasonable
to assume that the resistances of the ﬁve components
are independent of each other. What will be the expected
value and the standard deviation of the total resistance?
2.6.13 Suppose that items from a manufacturing process are
subject to three separate evaluations, and that the results
of the ﬁrst evaluation X1 have a mean value of 59 with a
standard deviation of 10, the results of the second
evaluation X2 have a mean value of 67 with a standard
deviation of 13, and the results of the third evaluation X3
have a mean value of 72 with a standard deviation of 4. In
addition, suppose that the results of the three evaluations
can be taken to be independent of each other.
(a) If a ﬁnal evaluation score is obtained as the average
of the three evaluations X = (X1 + X2 + X3)/3,
what are the mean and the standard deviation of the
ﬁnal evaluation score?
(b) If a ﬁnal evaluation score is obtained as the weighted
average of the three evaluations X = 0.4X1 +
0.4X2 + 0.2X3, what are the mean and the standard
deviation of the ﬁnal evaluation score?
2.6.14 The random variable X has an expectation of 77 and a
standard deviation of 9. Find the values of a and b such
that the random variable Y = a + bX has an expectation
of 1000 and a standard deviation of 10.
2.6.15 Suppose that components are manufactured such that
their heights are independent of each other with
μ = 65.90 and σ = 0.32.
(a) What are the mean and the standard deviation of the
average height of ﬁve components?
(b) If eight components are stacked on top of each other,
what are the mean and the standard deviation of the
total height?
2.6.16 An object has a weight W. When it is weighed with
machine 1, a value X1 is obtained. When it is weighed
with machine 2, a value X2 is obtained. The value X1 has
a mean W and a standard deviation 3. The value X2 has a
mean W and a standard deviation 4. The values X1 and
X2 are independent.
(a) Suppose that an engineer uses the value A =
(X1 + X2)/2 to estimate the weight. What are the
expectation and the standard deviation of A?
(b) Suppose that an engineer uses the value
B = δX1 + (1 −δ)X2 to estimate the weight. What
value of δ gives an estimate B with the smallest
standard deviation? What is this smallest standard
deviation?
2.6.17 A fair six-sided die is rolled 80 times and the sum of the
80 scores is calculated. What are the expectation and the
standard deviation of this total score?
2.6.18 A relay race is run between team A and team B. Each
team has four runners, who successively run around a
track and pass a baton to the next runner in their team
when they have ﬁnished. The team ﬁnishes when the
fourth runner has completed the run. The runners on
team A take times with an expectation of 33.2 seconds
and a standard deviation of 1.4 seconds, while the runners
on team B take times with an expectation of 33.0 seconds
and a standard deviation of 1.3 seconds. The times of all
eight runners are independent of each other.
(a) What are the expectation and the standard deviation
of the total time taken by team A?
(b) An ofﬁcial measures the total time taken by team A
and subtracts from it the total time taken by team B?
What are the expectation and the standard deviation
of this difference?
(c) The manager of team A measures the time taken by
the ﬁrst runner on team A and subtracts from it the
average of the times of the other three runners on
team A. What are the expectation and the standard
deviation of this difference?
2.6.19 Suppose that a temperature has a mean of 110◦F and a
standard deviation of 2.2◦F. What are the mean and the
standard deviation in degrees Centigrade?
(F = 9C/5 + 32)
2.6.20 A person’s cholesterol level C can be measured by three
different tests. Test-α returns a value Xα with a mean C
and a standard deviation 1.2, test-β returns a value Xβ
with a mean C and a standard deviation 2.4, and test-γ
returns a value Xγ with a mean C and a standard
deviation 3.1. Suppose that the three test results are
independent. If a doctor decides to use the weighted
average 0.5Xα + 0.3Xβ + 0.2Xγ what is the standard
deviation of the cholesterol level obtained by the doctor?

142
CHAPTER 2
RANDOM VARIABLES
2.6.21 Suppose that the thicknesses of electrical components
have a standard deviation of 56 microns. How many
components need to be taken so that the standard
deviation of their average thickness is no larger than
10 microns?
2.6.22 Suppose that the impurity levels of water samples taken
from a particular source are independent with a mean
value of 3.87 and a standard deviation of 0.18.
(a) What are the mean and the standard deviation of
the sum of the impurity levels from two water
samples?
(b) What are the mean and the standard deviation of the
sum of the impurity levels from three water samples?
(c) What are the mean and the standard deviation of the
average of the impurity levels from four water
samples?
(d) If the impurity levels of two water samples are
averaged, and the result is subtracted from the
impurity level of a third sample, what are the
mean and the standard deviation of the resulting
value?
2.6.23 Suppose that the amount of money requested on a loan
has a standard deviation of $14,000.
(a) If a loan ofﬁcer receives two loan requests from two
independent sources, what is the standard deviation
of the total amount of money requested?
(b) If a loan ofﬁcer receives two loan requests from two
independent sources, what is the standard deviation
of the average amount of money requested?
2.6.24 An investment in company A has an expected return
of $30,000 with a standard deviation of $4000. An
investment in company B has an expected return of
$45,000 with a standard deviation of $3000. If
investments are made in both companies, what are the
expectation and standard deviation of the total return?
2.7
Case Study: Microelectronic Solder Joints
Figure 2.76 shows the probability mass function of the number of extrusions on an assembly
with a large number of solder joints. These extrusions are deﬁned to be cases where the gap
between the solder of two adjacent solder joints is less than a speciﬁed value, and their presence
is likely to reduce the lifetime of the assembly. The expected number of extrusions is
E(X) = (0 × 0.721) + (1 × 0.133) + (2 × 0.083) + (3 × 0.055)
+ (4 × 0.007) + (5 × 0.001) = 0.497
and since
E(X 2) = (02 × 0.721) + (12 × 0.133) + (22 × 0.083)
+ (32 × 0.055) + (42 × 0.007) + (52 × 0.001) = 1.097
the variance is
Var(X) = 1.097 −0.4972 = 0.850
which gives a standard deviation of
√
0.850 = 0.922. Consequently, the average number of
extrusions per assembly is about 0.5, with a standard deviation of just less than 1. Furthermore,
it is clear from the probability mass function that there is a probability of more than 99% that
an assembly will have no more than three extrusions.
FIGURE 2.76
The probability mass function of
the number of solder joint
extrusions in an assembly
x
pi
i
0
1
2
3
4
5
0.721
0.133
0.083
0.055
0.007
0.001

2.9
SUPPLEMENTARY PROBLEMS
143
If a batch of these products consist of 250 assemblies, then the total number of extrusions
in the complete batch has an expected value
E(X) = 250 × 0.497 = 124.25
and a variance
Var(X) = 250 × 0.850 = 212.50
so that the standard deviation is
√
212.50 = 14.58.
2.8
Case Study: Internet Marketing
When a organisation’s website is accessed, there is a probability of 0.07 that the web address
was typed in directly, and the organisation incurs no charge for this. However, there is a
probability of 0.26 that the website was accessed through a sponsored advertisement on a
search engine, which costs the organisation $0.12. In addition, there is a probability of 0.49
that the website was accessed through a search engine without a sponsored advertisement,
which costs the organisation $0.02. Finally, there is a probability of 0.18 that the website was
accessed through a banner advertisement on another web page, which costs the organisation
$0.06. What is the average cost to the organisation when its web page is accessed?
This expectation can be calculated as
(0.07 × $0.00) + (0.26 × $0.12) + (0.49 × $0.02) + (0.18 × $0.06) = $0.0518
2.9
Supplementary Problems
2.9.1 A box contains four red balls and two blue balls. Balls are
drawn at random without replacement, and the random
variable X measures the total number of balls selected up
to the point when both blue balls have been selected.
(a) What is the probability mass function of X?
(b) What is the expected value of X?
2.9.2 The probability mass function of the number of calls taken
by a switchboard within 1 minute is given in Figure 2.77.
x
pi
i
0
1
2
3
4
5
6
0.21
0.39
0.18
0.16
0.03
0.02
0.01
FIGURE 2.77
Probability mass function of the number of calls taken by a switchboard
(a) Compute and sketch the cumulative distribution
function of the number of calls taken within a minute.
(b) What is the expected number of calls taken within a
minute?
(c) What is the variance of the number of calls taken
within a minute?
(d) If the numbers of calls taken in two different minutes
are independent, what are the expectation and
variance of the number of calls taken within one hour?
2.9.3 A box initially contains two red balls and two blue balls.
At each turn, one of the balls within the box is selected at
random and then replaced together with another ball of
the opposite color.
(a) After three turns, what is the distribution of the
number of red balls in the box?
(b) After three turns, what are the expectation and
variance of the number of red balls in the box?
(c) Repeat parts (a) and (b) if at each turn the selected
ball is replaced with a ball of the same color.
2.9.4 Suppose that an ace, a king, a queen, and a jack are all
worth 15 points, and that other cards are all worth their
face value. If a hand of 13 cards is dealt, what is the
expected value of the total score of the hand?
2.9.5 The acidity level X of a soil sample has a probability
density function
f (x) = A

3
2
x
for 1 ≤x ≤11.
(a) What is the value of A?
(b) Compute and sketch the cumulative distribution
function of X.

144
CHAPTER 2
RANDOM VARIABLES
(c) What is the median soil acidity level?
(d) What is the interquartile range of the soil acidity
levels?
2.9.6 Suppose that the random variables X and Y have a joint
probability density function
f (x, y) = 4x(2 −y)
for 0 ≤x ≤1 and 1 ≤y ≤2.
(a) What is the marginal probability density function of
X?
(b) Are the random variables X and Y independent?
(c) What is Cov(X, Y)?
(d) What is the probability density function of X
conditional on Y = 1.5?
2.9.7 The density X of a chemical solution is
f (x) = A

x + 2
x

for 5 ≤x ≤10.
(a) What is the value of A?
(b) Compute and sketch the cumulative distribution
function of the density.
(c) What is the mean density level?
(d) What is the variance of the density level?
(e) What is the median density level?
(f) What is the interquartile range of the density
level?
(g) Suppose that 10 chemical solutions are made
independently of each other. What are the mean
and variance of the average density of the ten
solutions?
2.9.8 Recall that
Var(aX + b) = a2Var(X)
and
Var(X1 + X2) = Var(X1) + Var(X1)
if the random variables X1 and X2 are independent. Use
these results to show that
Var(a1X1 + · · · + an Xn + b)
= a2
1Var(X1) + · · · + a2
nVar(Xn)
if the random variables Xi are independent.
2.9.9 Suppose that the scores from a test have a mean of 75 and
a standard deviation of 12. How should the scores be
standardized so that they have a mean of 100 and a
standard deviation of 20?
2.9.10 If the random variables X and Y are related through the
expression
Y = aX + b
show that they have a correlation of 1 if a > 0 and of −1
if a < 0.
2.9.11 An insurance company charges a customer an annual
premium of $100, and there is a probability of 0.9 that the
customer will not need to make a claim. If the customer
does make a claim, the amount of the claim $X has a
probability density function
f (x) = x(1800 −x)
972,000,000
for 0 ≤x ≤1800. Each customer also incurs
administrative costs to the insurance company of $5. If
the insurance company has 10,000 customers, what is its
expected annual proﬁt? Would you expect the customers’
claims to be independent of each other?
2.9.12 Suppose that telephone calls on a particular line have an
expected length of 320 seconds with a standard deviation
of 63 seconds, and suppose that the call lengths are
independent of each other.
(a) What are the expectation and the standard deviation
of the total length of ﬁve calls?
(b) What are the expectation and the standard deviation
of the average length of ten calls?
2.9.13 In an evaluation procedure, n items are ranked in order of
effectiveness. No ties are allowed, so that the ranks are
the positive integers from 1 to n. Suppose that all of the
possible rankings are equally likely.
(a) What is the probability mass function of the rank of a
particular item?
(b) What is the expectation of the rank of a particular
item?
(c) What is the variance of the rank of a particular item?
2.9.14 Nancy and Tom have to take a bus ride. The time taken by
a bus for the speciﬁed journey has an expectation of 87
minutes with a standard deviation of 3 minutes, and the
times taken by different buses are independent of each
other. Consider the random variable X, which is deﬁned
to be the sum of the time that Nancy spends on a bus and
the time that Tom spends on a bus.
(a) What are the expectation and the standard deviation
of X if Nancy and Tom ride on different buses?
(b) What are the expectation and the standard deviation
of X if Nancy and Tom ride together on the same bus?

2.9 SUPPLEMENTARY PROBLEMS
145
2.9.15 When a fair coin is tossed, 10 points are scored if a head
is obtained and 20 points are scored if a tail is obtained.
Suppose that the coin is tossed two times, and let the
random variable X be the total score obtained.
(a) What is the state space of X?
(b) What is the probability mass function of X?
(c) What is the cumulative distribution function of X?
(d) What is the expectation of X?
(e) What is the standard deviation of X?
2.9.16 A continuous random variable has a probability density
function
f (x) = Ax
for 5 ≤x ≤6.
(a) What is the value of A?
(b) What is the cumulative distribution function of the
random variable?
(c) What is the expectation of the random variable?
(d) What is the standard deviation of the random variable?
2.9.17 Components have a weight with an expectation of 438
and a standard deviation of 4.
(a) What are the expectation and the standard deviation
of the sum of the weights of three randomly selected
components?
(b) What are the expectation and the standard deviation
of the average of the weights of eight randomly
selected components?
2.9.18 In a game a player rolls a fair six-sided die. If the score is
even, the player receives an amount of dollars equal to the
score. If the score is odd, the player receives an amount of
dollars equal to the score multiplied by three. It costs the
player $5 to play the game.
(a) What is the probability mass function of the player’s
net winnings?
(b) What are the expectation and the standard deviation
of the net winnings?
(c) Suppose that a player plays the game 10 times. What
are the expectation and the standard deviation of the
total net winnings from all 10 games?
2.9.19 Are the following statements true or false?
(a) The variance of a random variable is measured in the
same units as the random variable.
(b) In a diving competition, the scores awarded by judges
for a particular type of dive have an expected value
of 78 with a standard deviation of 5. If the scores
are doubled so that they can be compared with scores
from an easier type of dive, the new scores will have an
expected value of 156 and a standard deviation of 10.
(c) The variance of the difference between two
independent random variables cannot be smaller than
the larger of their two variances.
(d) If a continuous random variable has a symmetric
probability density function, then the mean and the
median are identical.
(e) If X is a continuous random variable, then
P(X ≥x) = P(X > x) for any value of x.
(f) If X is a discrete random variable, then
P(X ≥x) = P(X > x) for any value of x.
2.9.20 Suppose that the time taken to download a ﬁle of a certain
kind onto a computer has an expected value of 22 minutes
and a standard deviation of 1.8 minutes. If a technician
needs to download ﬁve of these ﬁles onto a computer
one after the other, and the downloading times are
independent of each other, what are the expected value and
the standard deviation of the total time taken to download
all ﬁve ﬁles? What are the expected value and the standard
deviation of the average downloading time of the ﬁve ﬁles?
2.9.21 When a computer chip is examined to discover how many
of the solder joints have become cracked, there is a
probability of 0.12 that none of the joints are cracked, a
probability of 0.43 that one of the joints is cracked, a
probability of 0.28 that two of the joints are cracked, and
a probability of 0.17 that three of the joints are cracked.
(a) What is the mean number of cracked joints in a
computer chip?
(b) What is the standard deviation of the number of
cracked joints in a computer chip?
(c) Suppose that two different chips are randomly
selected. What are the mean and the standard
deviation of the total number of cracked joints in the
two chips combined?
2.9.22 A discrete random variable takes the values −22, 3, 19,
and 23 with probabilities 0.3, 0.2, 0.1, and 0.4
respectively.
(a) What is the expectation of the random variable?
(b) What is the standard deviation of the random
variable?
2.9.23 A random variable has a probability density function
f (x) = A/x2 for 2 ≤x ≤4.
(a) What is the value of A?
(b) What is the lower quartile of the distribution?
2.9.24 Bricks have weights that have a mean 250 and a standard
deviation 4.
(a) Suppose X is the weight of a randomly chosen brick.
Let Y = c + d X. What are the values of c and d

146
CHAPTER 2
RANDOM VARIABLES
such that Y has a mean 100 and a standard
deviation 1.
(b) Suppose 10 bricks are chosen at random. What are
the mean and the standard deviation of the total
weight of these 10 bricks?
2.9.25 An evaluation score X1 of a candidate using method 1 has
a mean of 100 and a standard deviation of 12, while an
evaluation score X2 of a candidate using method 2 has a
mean of 100 and a standard deviation of 13. If the two
evaluation scores are independent, what values of c1 and
c2 can be chosen so that the combined score
Y = c1X1 + c2X2 has a mean of 100 and a standard
deviation of 10?
2.9.26 Wafers of type A have thicknesses with a mean of 134.9
and a standard deviation of 0.7, while wafers of type B
have thicknesses with a mean of 138.2 and a standard
deviation of 1.1. The thicknesses of the wafers are all
independent of each other.
(a) Three wafers of type A are stacked on top of each
other. What are the mean and the standard deviation
of the total thickness?
(b) Two wafers of type A and two wafers of type B are
stacked on top of each other. What are the mean and
the standard deviation of the total thickness?
(c) Four wafers of type A and three wafers of type B are
taken. What are the mean and the standard deviation
of the average thickness of these seven wafers?
2.9.27 The standard deviation is a measure of the average value.
A. True
B. False
2.9.28 An investment in company I has an expected return of
$150,000 and a standard deviation of $30,000. An
investment in company II has an expected return of
$175,000 and a standard deviation of $40,000. What are
the expectation and standard deviation of the difference
between the returns from the two companies?
“So anyway, what exactly do you think that means . . . a 50% chance of rain?”
“Perhaps that there is a 75% chance that it will rain in 66.6% of the places?” (From
Inspector Morimoto and the Japanese Cranes, by Timothy Hemion)

C H A P T E R T H R E E
Discrete Probability Distributions
Several important discrete probability distributions are discussed in this chapter, and they
allow the concise description of many commonly occurring phenomena. The probability mass
functions of these distributions are described by formulas that depend upon some parameter
values, and their expectations and variances are functions of these parameters. Familiarity
with these distributions leads to an understanding of many probabilistic problems and to the
development of statistical inference methods discussed in later chapters.
The binomial, geometric, negative binomial, hypergeometric, Poisson, and multinomial
distributions are discussed in this chapter. With the exception of the Poisson distribution,
each of these distributions is generated by the speciﬁc problem under consideration. In other
words, the nature of the problem determines which probability distribution is appropriate
for its analysis, and it is important to be able to recognize which probability distribution is
appropriate for a particular problem. On the other hand, the Poisson distribution is chosen by
the experimenter as being likely to provide a useful analysis.
3.1
The Binomial Distribution
3.1.1
Bernoulli Random Variables
The most simple discrete random variable is one that can take just two values. Such a random
variable can be used to model the outcome of a coin toss, whether a valve is open or shut,
whether an item is defective or not, or any other process that has only two possible outcomes.
Typically,theoutcomesarelabeled0and1,andtherandomvariableisdeﬁnedbytheparameter
p, 0 ≤p ≤1, which is the probability that the outcome is 1.
These simple random variables are known as Bernoulli random variables, and their
expectation is
E(X) = (0 × P(X = 0)) + (1 × P(X = 1)) = (0 × (1 −p)) + (1 × p) = p
Also, since
E(X2) = (02 × P(X = 0)) + (12 × P(X = 1)) = p
their variance is
Var(X) = E(X2) −(E(X))2 = p −p2 = p(1 −p)
147

148
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
Bernoulli Random Variables
A Bernoulli random variable with parameter p, 0 ≤p ≤1, takes the values 0 and 1
with P(X = 0) = 1 −p and P(X = 1) = p. The expectation and variance of the
random variable are
E(X) = p
and
Var(X) = p(1 −p)
An experiment that has only two outcomes is often referred to as a Bernoulli trial.
3.1.2
Deﬁnition of the Binomial Distribution
Many processes can be thought of as consisting of a sequence of Bernoulli trials, such as, for
example, the repeated tossing of a coin or the repeated examination of objects to determine
whether or not they are defective. In such cases, a random variable of interest is the number
of successes obtained within a ﬁxed number of trials n, where a “success” is deﬁned in an
appropriate manner. Such a random variable is called a binomial random variable and it is
probably the most important of all discrete probability distributions.
HISTORICAL NOTE
James (Jakob, Jacques)
Bernoulli (1654–1705) was one
of several famous Swiss
mathematicians from the same
family. He obtained a degree in
theology from the University of
Basel in 1676 and held a chair
in mathematics at the same
university from 1687. One of
his early interests was how to
teach mathematics to the blind.
He is reported to have been an
excellent teacher, and students
from all over Europe came to
study under him. Much of his
work on probability was
contained in his manuscript Ars
Conjectandi (The Art of
Conjecturing), which was
published (in Latin) after his
death in 1713.
Speciﬁcally, if n independent Bernoulli trials X1, . . . , Xn are performed, each with a
probability p of recording a 1, then the random variable
X = X1 + · · · + Xn
is said to have a binomial distribution with parameters n and p, which is written
X ∼B(n, p)
Notice that the random variable X takes the values 0, 1, . . . , n and counts the number of the
Bernoulli random variables that take the value 1 or, in other words, counts the number of
“successes” in the n trials.
The probability mass function of a B(n, p) random variable is given by the formula
P(X = x) =

n
x

px(1 −p)n−x
for x = 0, 1, . . . , n. This formula is illustrated by the case n = 4, say, where the 24 = 16
possible outcomes of the four Bernoulli trials are listed in Figure 3.1. The probability that the
binomial random variable takes the value 3 is the probability that exactly three of the Bernoulli
random variables take the value 1, which is
P(X = 3) = P(0111) + P(1011) + P(1101) + P(1110)
= (1 −p)ppp + p(1 −p)pp + pp(1 −p)p + ppp(1 −p)
= 4p3(1 −p)
Also, the probability that the binomial random variable takes the value 2 is the probability
that exactly two of the Bernoulli random variables take the value 1, which is
P(X = 2) = P(0011) + P(0101) + P(0110) + P(1001) + P(1010) + P(1100)
= (1 −p)(1 −p)pp + (1 −p)p(1 −p)p + (1 −p)pp(1 −p)
+ p(1 −p)(1 −p)p + p(1 −p)p(1 −p) + pp(1 −p)(1 −p)
= 6p2(1 −p)2

3.1 THE BINOMIAL DISTRIBUTION
149
FIGURE 3.1
Illustration of the random variable
X ∼B(4, p)
Outcome of four
Bernoulli trials
Probability
X = number of successes
(1 −p)(1 −p)(1 −p)(1 −p)
0
(1 −p)(1 −p)(1 −p)p
1
(1 −p)(1 −p)p(1 −p)
1
(1 −p)(1 −p)pp
2
(1 −p)p(1 −p)(1 −p)
1
(1 −p)p(1 −p)p
2
(1 −p)pp(1 −p)
2
(1 −p)ppp
3
p(1 −p)(1 −p)(1 −p)
1
p(1 −p)(1 −p)p
2
p(1 −p)p(1 −p)
2
p(1 −p)pp
3
pp(1 −p)(1 −p)
2
pp(1 −p)p
3
ppp(1 −p)
3
0 0 0 0
0 0 0 1
0 0 1 0
0 0 1 1
0 1 0 0
0 1 0 1
0 1 1 0
0 1 1 1
1 0 0 0
1 0 0 1
1 0 1 0
1 0 1 1
1 1 0 0
1 1 0 1
1 1 1 0
1 1 1 1
pppp
4
These calculations illustrate the general rule that if X ∼B(n, p), then P(X = x) is
obtained as the sum of a number of probabilities that are each equal to px(1 −p)n−x since
they correspond to exactly x of the Bernoulli random variables taking the value 1 and (n −x)
of the Bernoulli random variables taking the value 0. Furthermore, the number of these
separate terms is simply the number of ways in which x 1s and (n −x) 0s can be arranged,
which is

n
x

so that the probability mass function is as indicated.
Notice that in particular
P(X = 0) =

n
0

p0(1 −p)n−0 =
n!
n! 0!(1 −p)n = (1 −p)n
which is the probability that each Bernoulli trial results in a 0, and
P(X = n) =

n
n

pn(1 −p)n−n =
n!
0! n! pn = pn
which is the probability that each Bernoulli trial results in a 1.
Since the Bernoulli random variables each have an expectation of p, the expectation of a
B(n, p) random variable is easily calculated to be
E(X) = E(X1) + · · · + E(Xn) = p + · · · + p = np
Also, because the Bernoulli random variables each have a variance of p(1 −p) and are
independent, the variance of a B(n, p) random variable is
Var(X) = Var(X1) + · · · + Var(Xn)
= p(1 −p) + · · · + p(1 −p) = np(1 −p)

150
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
The Binomial Distribution
Consider an experiment consisting of
■
n Bernoulli trials
■
that are independent and
■
that each have a constant probability p of success.
Then the total number of successes X is a random variable that has a binomial
distribution with parameters n and p, which is written
X ∼B(n, p)
The probability mass function of a B(n, p) random variable is
P(X = x) =

n
x

px(1 −p)n−x
for x = 0, 1, . . . , n, with
E(X) = np
and
Var(X) = np(1 −p)
Figure 3.2 illustrates the probability mass function and cumulative distribution function
of a B(8, 0.5) random variable. In this case,
P(X = 3) =

8
3

× 0.53 × (1 −0.5)5 =
8!
3! 5! × 0.58 = 0.219
FIGURE 3.2
Probability mass function and
cumulative distribution function of
a B(8, 0.5) random variable
Probability
0
1
2
3
4
5
6
7
8
0.004
0.031
0.109
0.219
0.219
0.109
0.031
0.004
0.273
x
0
1
2
3
4
5
6
7
8
0.004
0.035
0.144
0.363
0.636
0.855
0.965
0.996
1.000
x
P(X Ä x)

3.1 THE BINOMIAL DISTRIBUTION
151
and
P(X ≤1) = P(X = 0) + P(X = 1)
=

8
0

× 0.50 × (1 −0.5)8 +

8
1

× 0.51 × (1 −0.5)7
=
8!
0! 8! × 0.58 +
8!
1! 7! × 0.58 = 0.004 + 0.031 = 0.035
Notice that this is a symmetric distribution, and in fact it is straightforward to establish that
a B(n, 0.5) distribution is symmetric for any value of the parameter n.
Symmetric Binomial Distributions
A B(n, 0.5) distribution is a symmetric probability distribution for any value of the
parameter n. The distribution is symmetric about the expected value n/2.
The B(8, 0.5) distribution is, for example, the distribution of the number of heads
obtained in eight tosses of a fair coin. The probability of obtaining exactly four heads in
eight tosses is
P(X = 4) = 0.273
and the probability of obtaining at least six heads in eight tosses is
P(X ≥6) = 1 −P(X ≤5) = 1 −0.855 = 0.145
The expected number of heads obtained in eight tosses is
E(X) = np = 8 × 0.5 = 4
and the variance is
Var(X) = np(1 −p) = 8 × 0.5 × 0.5 = 2
Figure 3.3 illustrates the probability mass function and cumulative distribution function
of a B(8, 1/3) random variable. In this case,
P(X = 3) =

8
3

×
1
3
3
×

1 −1
3
5
=
8!
3! 5! ×
1
3
3
×
2
3
5
= 0.273
and
P(X ≤1) = P(X = 0) + P(X = 1)
=

8
0

×
1
3
0
×

1 −1
3
8
+

8
1

×
1
3
1
×

1 −1
3
7
=
8!
0! 8! ×
2
3
8
+
8!
1! 7! ×
1
3

×
2
3
7
= 0.039 + 0.156 = 0.195

152
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
FIGURE 3.3
Probability mass function and
cumulative distribution function of
a B(8, 1/3) random variable
Probability
0
1
2
3
4
5
6
7
8
x
0.039
0.156
0.273
0.273
0.171
0.068
0.017
0.002
0.001
0
1
2
3
4
5
6
7
8
0.039
0.195
0.468
0.741
0.912
0.980
0.997
0.999
1.000
x
P(X Ä x)
Suppose that a fair die is rolled eight times and that $1 is won each time a 5 or a 6 is scored.
The distribution of the winnings is then given by the B(8, 1/3) distribution, since there is a
probability of p = 1/3 of winning $1 on each of the eight rolls of the die. The probability
that exactly $4 is won is therefore
P(X = 4) = 0.171
and the probability that no more than $2 is won is
P(X ≤2) = 0.468
The expected winnings are
E(X) = np = 8 × 1
3 = $2.67
and the variance is
Var(X) = np(1 −p) = 8 × 1
3 × 2
3 = 1.78
Figure 3.4 illustrates the probability mass function and cumulative distribution function of
a B(8, 2/3) random variable, which is seen to be a “reﬂection” of the B(8, 1/3) distribution.
In general, if
X ∼B(n, p)
and if the random variable Y is deﬁned by Y = n −X, then
Y ∼B(n, 1 −p)

3.1 THE BINOMIAL DISTRIBUTION
153
FIGURE 3.4
Probability mass function and
cumulative distribution function of
a B(8, 2/3) random variable
0
1
2
3
4
5
6
7
8
x
0.156
0.273
0.273
0.171
0.068
0.017
0.002
0.001
0.039
x
P(X Ä x)
0
1
2
3
4
5
6
7
8
1.000
0.001
0.003
0.020
0.088
0.259
0.532
0.805
0.961
Probability
This makes sense since if X counts the number of “successes” in n Bernoulli trials where the
success probability is p, then Y = n −X counts the number of “failures” in n Bernoulli trials,
where the failure probability is 1 −p.
COMPUTER NOTE
Statistical software packages provide a convenient way of ﬁnding binomial probabilities with-
out having to compute them by hand or to look them up in tables. You will need to indicate
which parameter values n and p you are interested in, and you are usually given the op-
tion of being told the probability mass function or the cumulative distribution function. The
statistical software package should also provide information on the other distributions in this
chapter.
3.1.3
Examples of the Binomial Distribution
Example 17
Milk Container
Contents
Recall that there is a probability of 0.261 that a milk container is underweight. Suppose that the
milk containers are shipped to retail outlets in boxes of 20 containers. What is the distribution
of the number of underweight containers in a box?
If the milk contents of two different milk containers are independent of each other, the
number of underweight containers in a box has a binomial distribution with parameters n = 20
and p = 0.261. This is because each individual milk container represents a Bernoulli trial
with a constant probability p = 0.261 of being underweight, as illustrated in Figure 3.5. The
expected number of underweight cartons in a box is
E(X) = np = 20 × 0.261 = 5.22
and the variance is
Var(X) = np(1 −p) = 20 × 0.261 × 0.739 = 3.86
so that the standard deviation is σ =
√
3.86 = 1.96.

154
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
FIGURE 3.5
The total number of underweight
milk containers in the box has a
binomial distribution with
parameters n = 20 and
p = 0.261
Number of underweight milk containers ~ B(20, 0.261)
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
P(underweight) = 0.261
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
Milk
The probability mass function of the number of underweight containers is shown in
Figure 3.6. The probability that a box contains exactly seven underweight containers, for
example, is
P(X = 7) =
20
7

× 0.2617 × 0.73913 =
20!
7! 13! × 0.2617 × 0.73913 = 0.125
The probability that a box contains no more than three underweight containers is
P(X ≤3) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)
=

20
0

× 0.2610 × 0.73920 +

20
1

× 0.2611 × 0.73919
+

20
2

× 0.2612 × 0.73918 +

20
3

× 0.2613 × 0.73917
= 0.0024 + 0.0167 + 0.0559 + 0.1185 = 0.1935

3.1 THE BINOMIAL DISTRIBUTION
155
0.119
0.201
0.178
0.125
0.072
0.034
0.013
0.001
0.000
0.004
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
Number of underweight containers
16
17
18
19
20
0.056
0.017
0.002
0
0.178
x
x
P(X Ä x)
0
1
2
3
4
5
6
7
8
9
10
0.002 0.019 0.075 0.194 0.372 0.573 0.751 0.876 0.948 0.982 0.995
11
12
13
20
0.999 1.000 1.000
1.000
. . .
. . .
FIGURE 3.6
Probability mass function and cumulative distribution function of the number of underweight milk containers
Example 24
Air Force Scrambles
An Air Force intercept squadron consists of 16 planes that should always be ready for imme-
diate launch. However, a plane’s engines are troublesome, and there is a probability of 0.25
that the engines of a particular plane will not start at a given attempt. If this happens, then the
mechanics must wait ﬁve minutes before trying to start the engines again.
The squadron commander is interested in how many planes will immediately become
airborne if the squadron is scrambled. As Figure 3.7 illustrates, the number of planes suc-
cessfully launched has a binomial distribution with parameters n = 16 and p = 0.75.
Each plane represents a Bernoulli trial with a constant probability p = 0.75 of launching
on time.
The expected number of planes launched is
E(X) = np = 16 × 0.75 = 12
and the variance is
Var(X) = np(1 −p) = 16 × 0.75 × 0.25 = 3
so that the standard deviation is σ =
√
3 = 1.73.

156
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
FIGURE 3.7
The number of planes launched on
time has a binomial distribution
with parameters n = 16 and
p = 0.75
Number of planes launched ~ B(16, 0.75)
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
P(launch) = 0.75
The probability mass function of the number of planes that scramble successfully is shown
in Figure 3.8. The probability that exactly 12 planes scramble successfully is
P(X = 12) =

16
12

× 0.7512 × 0.254 =
16!
12! 4! × 0.7512 × 0.254 = 0.225
The probability that at least 14 planes scramble successfully is
P(X ≥14) = P(X = 14) + P(X = 15) + P(X = 16)
=

16
14

× 0.7514 × 0.252 +

16
15

× 0.7515 × 0.251
+

16
16

× 0.7516 × 0.250
= 0.134 + 0.054 + 0.010 = 0.198
GAMES OF CHANCE
Suppose that a fair coin is tossed n times. The distribution of the number of heads obtained,
X, is binomial with parameters n and p = 0.5. The expected number of heads obtained is
therefore
E(X) = np = n
2

3.1 THE BINOMIAL DISTRIBUTION
157
FIGURE 3.8
Probability mass function and
cumulative distribution function of
the number of planes launched
on time
Probability
0.0010.006
0.020
0.052
0.110
0.180
0.225
0.208
0.134
0.054
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
x
P(X Ä x)
0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.007 0.027 0.079 0.189 0.369 0.594 0.802 0.936 0.990 1.000
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1
16
0
Number of planes launched on time
0.010
x
0.000
and the variance is
Var(X) = np(1 −p) = n
4
The proportion of heads obtained is Y = X/n, which has an expectation and variance of
E(Y) = E(X)
n
= 1
2
and
Var(Y) = Var(X)
n2
= 1
4n
Notice that the proportion of heads has an expected value equal to the probability of obtaining
a head on one coin toss, which is not surprising, and that the variance of the proportion of
heads decreases as the number of tosses n increases. In practice, as the coin is tossed more
and more times, the number of heads may vary widely about the expected value of n/2,
but there will be a tendency for the proportion of heads to become closer and closer to the
value 1/2.
Figure 3.9 shows the results of 80 coin tosses, and the proportion of heads obtained
is graphed against the number of tosses in Figure 3.10. As expected, the ﬂuctuation in the
proportion of heads decreases as the number of tosses increases, and there is a tendency toward
the expected value of p = 1/2. The calculation presented at the end of Section 5.3.1 provides
further information on this point.

158
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
 0
 1
 2
 3
 3
 4
 5
 6
 7
 7
 8
 8
 8
 8
 9
10
10
10
10
11
12
13
14
14
14
14
14
14
14
14
14
14
14
14
15
15
16
16
17
17
 tail
head
head
head
 tail
head
head
head
head
 tail
head
 tail
 tail
 tail
head
head
 tail
 tail
 tail
head
head
head
head
 tail
 tail
 tail
 tail
 tail
 tail
 tail
 tail
 tail
 tail
 tail
head
 tail
head
 tail
head
 tail
0.000
0.500
0.667
0.750
0.600
0.667
0.714
0.750
0.778
0.700
0.727
0.667
0.615
0.571
0.600
0.625
0.588
0.556
0.526
0.550
0.571
0.591
0.609
0.583
0.560
0.538
0.519
0.500
0.483
0.467
0.452
0.438
0.424
0.412
0.429
0.417
0.432
0.421
0.436
0.425
Result of
nth coin
toss
Number of heads
in first n coin 
tosses
 Proportion of heads
in first n coin 
tosses
n
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
15
16
16
17
17
18
18
18
19
20
20
20
20
20
20
21
21
21
21
22
22
23
23
24
25
26
26
27
28
29
29
29
30
31
32
32
32
33
33
34
35
35
36
36
37
 tail
head
 tail
head
 tail
head
 tail
 tail
head
head
 tail
 tail
 tail
 tail
 tail
head
 tail
 tail
 tail
head
 tail
head
 tail
head
head
head
 tail
head
head
head
 tail
 tail
head
head
head
 tail
 tail
head
 tail
head
head
 tail
head
 tail
head
0.417
0.432
0.421
0.436
0.425
0.439
0.429
0.419
0.432
0.444
0.435
0.426
0.417
0.408
0.400
0.412
0.404
0.396
0.389
0.400
0.393
0.404
0.397
0.407
0.417
0.426
0.419
0.429
0.438
0.446
0.439
0.433
0.441
0.449
0.457
0.451
0.444
0.452
0.446
0.453
0.461
0.455
0.462
0.456
0.463
Result of
nth coin
toss
Number of heads
in first n coin 
tosses
 Proportion of heads
in first n coin 
tosses
n
FIGURE 3.9
Results of 80 coin tosses
In general, the following result holds for the proportion of successes in a series of inde-
pendent Bernoulli trials.
Proportion of Successes in Bernoulli Trials
If the random variable X counts the number of successes in n independent Bernoulli
trials with a constant success probability p, so that X ∼B(n, p), then the proportion
of successes Y = X/n has an expected value and variance of
E(Y) = p
and
Var(Y) = p(1 −p)
n
The variance of the proportion Y decreases as the number of trials n increases, so that
there is a tendency for Y to become closer and closer to the success probability p as
the number of trials n increases.
This result will be useful in Chapter 7 when the problem of estimating an unknown
success probability p is addressed. Also, it should be noted that for large values of n, the

3.1 THE BINOMIAL DISTRIBUTION
159
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
10
20
30
40
50
60
70
80
Number of coin tosses
Proportion of heads
FIGURE 3.10
Graph of proportion of heads against number of coin tosses for 80 coin tosses
probability mass function of a B(n, p) random variable becomes tedious to calculate. A simple
approximate method of calculating binomial probabilities for large values of n based on the
normal distribution is discussed in Section 5.3.1.
3.1.4
Problems
3.1.1 Suppose that X ∼B(10, 0.12). Calculate:
(a) P(X = 3)
(b) P(X = 6)
(c) P(X ≤2)
(d) P(X ≥7)
(e) E(X)
(f) Var(X)
3.1.2 Suppose that X ∼B(7, 0.8). Calculate:
(a) P(X = 4)
(b) P(X ̸= 2)
(c) P(X ≤3)
(d) P(X ≥6)
(e) E(X)
(f) Var(X)
3.1.3 Draw line graphs of the probability mass functions of a
B(6, 0.5) distribution and a B(6, 0.7) distribution. Mark
the expected values of the distributions on the line graphs
and calculate the standard deviations of the two
distributions.
3.1.4 An archer hits a bull’s-eye with a probability of 0.09, and
the results of different attempts can be taken to be
independent of each other. If the archer shoots nine arrows,
calculate the probability that:
(a) Exactly two arrows score bull’s-eyes.
(b) At least two arrows score bull’s-eyes.
What is the expected number of bull’s-eyes scored?
(This problem is continued in Problems 3.2.5 and 3.5.3.)
3.1.5 A fair die is rolled eight times. Calculate the probability
that there are:
(a) Exactly ﬁve even numbers
(b) Exactly one 6
(c) No 4s
3.1.6 A multiple-choice quiz consists of ten questions, each with
ﬁve possible answers of which only one is correct. A
student passes the quiz if seven or more correct answers
are obtained. What is the probability that a student who
guesses blindly at all of the questions will pass the quiz?
What is the probability of passing the quiz if, on each

160
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
question, a student can eliminate three incorrect answers
and then guesses between the remaining two?
3.1.7 A ﬂu virus hits a company employing 180 people.
Independent of the other employees, there is a probability
of p = 0.35 that each person needs to take sick leave.
What are the expectation and variance of the proportion of
the workforce who need to take sick leave? In general,
what value of the sick rate p produces the largest variance
for this proportion?
3.1.8 Consider the two independent binomial random variables
X1 ∼B(n1, p) and X2 ∼B(n2, p). If Y = X1 + X2,
explain why Y ∼B(n1 + n2, p).
3.1.9 A company receives 60% of its orders over the Internet.
Within a collection of 18 independently placed orders,
what is the probability that
(a) between eight and ten of the orders are received over
the Internet?
(b) no more than four of the orders are received over the
Internet?
(This problem is continued in Problems 3.2.9, 3.5.5, and
5.3.10.)
3.1.10 When a company receives an order, there is a probability of
0.42 that its value is over $1000. What is the probability
that exactly four out of the next seven orders will be valued
at over $1000?
A. 0.212
B. 0.232
C. 0.252
D. 0.272
3.1.11 Investments are made in ten companies, and for each
company there is a probabiltiy of 0.65 that the investment
will deliver a proﬁt. What is the probability that at least
half of the investments will deliver a proﬁt?
3.1.12 There is a probability of 0.93 that a visitor to a website will
bounce (leave the website without clicking on any links).
What is the probability that at least 10 of the next 12
visitors to the website will bounce?
3.2
The Geometric and Negative Binomial Distributions
3.2.1
Deﬁnition of the Geometric Distribution
Consider a sequence of independent Bernoulli trials with a constant success probability p.
Whereas the binomial distribution is the distribution of the number of successes occurring in
a ﬁxed number of trials n, it is sometimes of interest to count instead the number of trials
performed until the ﬁrst success occurs. Such a random variable is said to have a geometric
distribution with parameter p, and it has a probability mass function given by
P(X = x) = (1 −p)x−1 p
for x = 1, 2, 3, 4 . . . . It is easy to see that the probability mass function is of this form, because
if the ﬁrst success occurs on the xth trial, then the ﬁrst x −1 trials must all be failures. The
probability of these x −1 failures is
(1 −p)x−1
which is then multiplied by p, the probability that the xth trial is a success.
It can be shown that a geometric distribution with parameter p has an expected value and
a variance of
E(X) = 1
p
and
Var(X) = 1 −p
p2
Also, the cumulative distribution function can be calculated to be
P(X ≤x) =
x

i=1
P(X = i) =
x

i=1
(1 −p)i−1 p
= p

1 + (1 −p) + (1 −p)2 + · · · + (1 −p)x−1
= p × 1 −(1 −p)x
1 −(1 −p) = 1 −(1 −p)x

3.2 THE GEOMETRIC AND NEGATIVE BINOMIAL DISTRIBUTIONS
161
Figure 3.11 illustrates a geometric distribution with parameter p = 1/2, and Figure 3.12
illustrates a geometric distribution with parameter p = 1/5. The former has an expected value
of E(X) = 2, while the latter has an expected value of E(X) = 5. Notice that the probability
values form a decreasing geometric series, as the name of the distribution suggests.
The Geometric Distribution
The number of trials up to and including the ﬁrst success in a sequence of independent
Bernoulli trials with a constant success probability p has a geometric distribution
with parameter p. The probability mass function is
P(X = x) = (1 −p)x−1 p
for x = 1, 2, 3, 4 . . . , and the cumulative distribution function is
P(X ≤x) = 1 −(1 −p)x
The geometric distribution with parameter p has an expected value and a variance of
E(X) = 1
p
and
Var(X) = 1 −p
p2
The distribution with p = 1/2 is appropriate for modeling the number of tosses of a fair
coin made until a head is obtained for the ﬁrst time, since in this case the “success” probability
(the probability of obtaining a head) is p = 1/2. The probability that a head is obtained for
FIGURE 3.11
Probability mass function and
cumulative distribution function of
a geometric distribution with
parameter p = 1/2
Probability
x
1
2
3
4
5
6
7
8
1/2
1/4
1/8
1/16
1/32
1/64
1/128 1/256
. . .  
1
2
3
4
5
6
7
8
P(X Ä x)
x
1/2
3/4
7/8
15/16 31/32
63/64 127/128 255/256
. . .
. . .

162
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
0.128
0.102
0.082
0.066
0.052
0.042
0.034
0.027
0.014
0.017
0.021
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
0.011 0.009
. . .
0.160
0.200
x
11
6
x
P(X Ä x)
0.200 0.360 0.488 0.590 0.672 0.738 0.790 0.832 0.866 0.893 0.914 0.931 0.945 0.956 0.965 . . .
. . .
1
2
3
4
5
7
8
9
10
12
13
14
15
FIGURE 3.12
Probability mass function and cumulative distribution function of a geometric distribution with parameter p = 1/5
the ﬁrst time on the fourth coin toss is therefore
P(X = 4) = (1 −p)4−1 p =
1
2
3
× 1
2 = 1
16
which is simply the probability of obtaining three tails followed by a head.
3.2.2
Deﬁnition of the Negative Binomial Distribution
The geometric distribution can be generalized to situations in which the quantity of interest
is the number of trials up to and including the rth success. In this case, the appropriate distri-
bution is the negative binomial distribution with parameters p and r, which has a probability
mass function given by
P(X = x) =

x −1
r −1

(1 −p)x−r pr
for x = r,r + 1,r + 2,r + 3, . . . .
This probability mass function can be understood by noting that if the rth success occurs
on the xth trial, then the ﬁrst x −1 trials consist of r −1 successes and x −r failures. The
probability of this event is obtained from the binomial distribution as

x −1
r −1

(1 −p)x−r pr−1
The required probability value of the negative binomial distribution is obtained by multiplying
this probability by p, the probability that the xth trial results in a success.

3.2 THE GEOMETRIC AND NEGATIVE BINOMIAL DISTRIBUTIONS
163
The Negative Binomial Distribution
The number of trials up to and including the rth success in a sequence of independent
Bernoulli trials with a constant success probability p has a negative binomial
distribution with parameters p and r. The probability mass function is
P(X = x) =

x −1
r −1

(1 −p)x−r pr
for x = r,r + 1,r + 2,r + 3, . . . , with an expected value and a variance of
E(X) = r
p
and
Var(X) = r(1 −p)
p2
Figure 3.13 illustrates a negative binomial distribution with parameter values p = 1/2
and r = 2. This distribution is appropriate for modeling the number of tosses of a fair coin
made until a head is obtained for the second time. For example, the probability that only two
0.250 0.250
0.188
0.125
0.078
0.047
0.027
0.016
0.009
0.005
0.001
0.001
0.003
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
x
2
3
4
5
6
7
8
9
10
11
12
0.250 0.500 0.688 0.813 0.891 0.938 0.965 0.981 0.990 0.995 0.998
14
15
0.000
. . .
. . .
13
14
0.999 1.000
x
15
1.000
. . .
. . .
P(X Ä x)
FIGURE 3.13
Probability mass function and cumulative distribution function of a negative binomial distribution with parameters p = 1/2
and r = 2

164
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
coin tosses are required is
P(X = 2) =

1
1

× 0.50 × 0.52 = 1
4
which is simply the probability that the ﬁrst two coin tosses result in heads. The probability
that ﬁve coin tosses are required is
P(X = 5) =

4
1

× 0.53 × 0.52 = 1
8
which is the sum of the probabilities of the four outcomes T T T H H, T T HT H, T HT T H,
and HT T T H. The expected number of coin tosses required to obtain two heads is
E(X) = r
p = 2
0.5 = 4
3.2.3
Examples of the Geometric and Negative Binomial Distributions
Example 24
Air Force Scrambles
Recall that a plane’s engines start successfully at a given attempt with a probability of 0.75.
Any time that the mechanics are unsuccessful in starting the engines, they must wait ﬁve
minutes before trying again. What is the distribution of the number of attempts needed to start
a plane’s engines?
A “success” here is the event that the plane’s engines start, so that the success probability
is p = 0.75. Furthermore, the geometric distribution is appropriate since attention is directed
at the number of trials until the ﬁrst success. The probability that the engines start on the third
attempt is therefore
P(X = 3) = 0.252 × 0.75 = 0.047
The probability that the plane is launched within 10 minutes of the ﬁrst attempt to start the
engines is the probability that no more than three attempts are required, which is
P(X ≤3) = 1 −0.253 = 0.984
The expected number of attempts required to start the engines is
E(X) = 1
p =
1
0.75 = 1.33
Example 25
Telephone Ticket Sales
Telephone ticket sales for a popular event are handled by a bank of telephone salespersons
who start accepting calls at a speciﬁed time. In order to get through to an operator, a caller has
to be lucky enough to place a call at just the time when a salesperson has become free from a
previous client. Suppose that the chance of this is 0.1. What is the distribution of the number
of calls that a person needs to make until a salesperson is reached?
In this problem, the placing of a call represents a Bernoulli trial with a “success” probabil-
ity, that is, the probability of reaching a salesperson, of p = 0.1, as illustrated in Figure 3.14.
The geometric distribution is appropriate since the quantity of interest is the number of calls
made until the ﬁrst success. The probability that a caller gets through on the ﬁfth attempt, say,
is therefore
P(X = 5) = 0.94 × 0.1 = 0.066
The expected number of calls needed to get through to a salesperson is
E(X) = 1
p = 1
0.1 = 10

3.2 THE GEOMETRIC AND NEGATIVE BINOMIAL DISTRIBUTIONS
165
0.081
0.073
0.066
0.060
0.053
0.048
0.043 0.039
0.028
0.031
0.035
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
0.0250.023
Number of calls made
0.021 0.019
16
17
18
0.090
0.017
0.100
Telephone call made
Salesperson
reached
Busy signal
Probability p = 0.1
Probability 1 − p = 0.9
. . .
. . .
FIGURE 3.14
Probability mass function of a geometric distribution with parameter p = 0.1, the distribution of the number of calls made until a ticket
salesperson is reached
and the probability that 15 or more calls are needed is
P(X ≥15) = 1 −P(X ≤14) = 1 −

1 −0.914
= 0.914 = 0.229
which is simply the probability that the ﬁrst 14 calls are unsuccessful.
Example 12
Personnel Recruitment
Suppose that a company wishes to hire three new workers and that each applicant interviewed
has a probability of 0.6 of being found acceptable, as illustrated in Figure 3.15. What is the
distribution of the total number of applicants that the company needs to interview?
In this problem, an interview represents a Bernoulli trial with a “success” being that
the applicant is found to be acceptable for the position. The success probability is therefore
p = 0.6. Furthermore, the negative binomial distribution with r = 3 is appropriate since the
quantity of interest is the number of interviews that must be undertaken until three suitable
applicants have been found.
The negative binomial distribution with parameters p = 0.6 and r = 3 is illustrated in
Figure 3.16. For example, the probability that exactly six applicants need to be interviewed is
P(X = 6) =

5
2

× 0.43 × 0.63 = 0.138
If the company has a budget that allows up to six applicants to be interviewed, then the
probability that the budget is sufﬁcient is
P(X ≤6) = P(X = 3) + P(X = 4) + P(X = 5) + P(X = 6)
= 0.216 + 0.259 + 0.207 + 0.138 = 0.820

FIGURE 3.15
The number of applicants who
need to be interviewed to ﬁll three
positions has a negative binomial
distribution with parameters
p = 0.6 and r = 3
Applicants
Interview
Is applicant acceptable for a position?
1 − p = 0.4
Yes
 p = 0.6
No
Position 1
Position 2
Position 3
. . .  
0.216
0.259
0.207
0.138
0.083
0.046
0.025
0.013
0.002
0.003
0.006
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
0.001 0.001
. . .
. . .
Number of applicants interviewed
0.000 0.000
16
17
FIGURE 3.16
Probability mass function of a negative binomial distribution with parameters p = 0.6 and r = 3, the distribution of the number of
applicants interviewed
166

3.2 THE GEOMETRIC AND NEGATIVE BINOMIAL DISTRIBUTIONS
167
The expected number of interviews required is
E(X) = r
p = 3
0.6 = 5
GAMES OF CHANCE
If a fair die is repeatedly thrown, the number of throws made until a 6 is obtained has a
geometric distribution with parameter p = 1/6. The expected number of throws made until
a 6 is obtained is therefore
E(X) = 1
p = 6
The probability that a 6 is not obtained in the ﬁrst six throws is
P(X ≥7) = 1 −P(X ≤6) = 1 −

1 −
5
6
6 
=
5
6
6
= 0.335
The distribution of the number of throws made until a 6 is obtained for the second time has
a negative binomial distribution with parameters p = 1/6 and r = 2. The expected number
of throws required is
E(X) = r
p = 12
The probability that two 6s are obtained in the ﬁrst four throws is
P(X ≤4) = P(X = 2) + P(X = 3) + P(X = 4)
=

1
1

×
5
6
0
×
1
6
2
+

2
1

×
5
6
1
×
1
6
2
+

3
1

×
5
6
2
×
1
6
2
= 0.028 + 0.046 + 0.058 = 0.132
3.2.4
Problems
3.2.1 If X has a geometric distribution with parameter p = 0.7,
calculate:
A. P(X = 4)
B. P(X = 1)
C. P(X ≤5)
D. P(X ≥8)
3.2.2 If X has a negative binomial distribution with parameters
p = 0.6 and r = 3, calculate:
A. P(X = 5)
B. P(X = 8)
C. P(X ≤7)
D. P(X ≥7)
3.2.3 If X has a geometric distribution with parameter p = 0.7,
show that
E(X) =
∞

x=1
x P(X = x) = 1
p
Also, show that
E(X 2) =
∞

x=1
x2P(X = x) = 2
p2 −1
p
and deduce that
Var(X) = 1 −p
p2
3.2.4 Suppose that X1, . . . , Xr are independent random
variables, each with a geometric distribution with
parameter p. Explain why
Y = X1 + · · · + Xr
has a negative binomial distribution with parameters p
and r. Use this relationship to establish the mean and
variance of a negative binomial distribution.
3.2.5 Recall Problem 3.1.4 where an archer hits a bull’s-eye
with a probability of 0.09, and the results of different
attempts can be taken to be independent of each other.
(a) If the archer shoots a series of arrows, what is the
probability that the ﬁrst bull’s-eye is scored with the
fourth arrow?

168
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
(b) What is the probability that the third bull’s-eye is
scored with the tenth arrow?
(c) What is the expected number of arrows shot before
the ﬁrst bull’s-eye is scored?
(d) What is the expected number of arrows shot before
the third bull’s-eye is scored?
3.2.6 A supply container dropped from an aircraft by parachute
hits a target with a probability of 0.37.
(a) What is the expected number of container drops
needed to hit a target?
(b) If hits from three containers are required to provide
sufﬁcient supplies, what is the expected number of
containers dropped before sufﬁcient supplies have
been provided?
(c) What is the probability that sufﬁcient supplies are
provided by ten container drops?
(d) What is the probability that it is the tenth container
dropped that completes the provision of supplies?
3.2.7 Cards are chosen randomly from a pack of cards with
replacement. Calculate the probability that:
(a) The ﬁrst heart is obtained on the third drawing.
(b) The fourth heart is obtained on the tenth drawing.
What is the expected number of cards drawn before the
fourth heart is obtained? If the ﬁrst two cards drawn are
spades, what is the probability that the ﬁrst heart is
obtained on the ﬁfth drawing?
3.2.8 When a ﬁsherman catches a ﬁsh, it is a young one with a
probability of 0.23 and it is returned to the water. On the
other hand, an adult ﬁsh is kept to be eaten later.
(a) What is the expected number of ﬁsh caught by the
ﬁsherman before an adult ﬁsh is caught?
(b) What is the probability that the ﬁfth ﬁsh caught is the
ﬁrst young ﬁsh?
Suppose that the ﬁsherman wants three ﬁsh to eat for
lunch.
(a) What is the probability that the ﬁrst time the
ﬁsherman can stop for lunch is immediately after the
sixth ﬁsh has been caught?
(b) If the ﬁsherman catches eight ﬁsh, what is the
probability that there are sufﬁcient ﬁsh for lunch?
3.2.9 Recall Problem 3.1.9, in which a company receives 60%
of its orders over the Internet. Within a certain period of
time:
(a) What is the probability that the ﬁfth order received is
the ﬁrst Internet order?
(b) What is the probability that the eighth order received
is the fourth Internet order?
3.2.10 Consider a fair six-sided die. The die is rolled until a 6 is
obtained for the third time. What is the expectation of the
number of die rolls needed?
3.2.11 A fair coin is tossed until the ﬁfth head is obtained. What
is the probability that the coin is tossed exactly ten times?
3.2.12 (Problem 3.1.10 continued) A manager monitors orders
that the company receives and waits for the ﬁrst one that
is valued over $1000. What is the probability that the ﬁfth
order is the ﬁrst one valued over $1000?
A. 0.108
B. 0.078
C. 0.048
D. 0.018
3.2.13 Suppose that there is a probability of 0.65 that each of a
series of investments will deliver a proﬁt. What is the
expected number of investments that need to be made in
order to get ﬁve that deliver a proﬁt?
3.2.14 There is a probability of 0.93 that a visitor to a website
will bounce (leave the website without clicking on any
links). What is the expected number of visitors required
to get ten that do not bounce?
3.3
The Hypergeometric Distribution
3.3.1
Deﬁnition of the Hypergeometric Distribution
Consider a collection of N items of which r are of a certain kind. For example, it may be
useful to think of r of the N items as being “defective.” Alternatively, it may be useful to
envision a box containing N balls of which exactly r are red. If one of the items is chosen at
random, the probability that it is of the special kind is clearly
p = r
N
Consequently, if n items are chosen at random with replacement, then clearly the distribution
of X, the number of defective items chosen, is
X ∼B(n,r/N)

3.3 THE HYPERGEOMETRIC DISTRIBUTION
169
However, if n items are chosen at random without replacement, then the binomial distri-
bution cannot be applied because the success probability, that is, the probability of selecting
an item of the special kind, is no longer constant. Instead, the appropriate distribution for the
number of defective items chosen is the hypergeometric distribution. The probability mass
function of the hypergeometric distribution is
P(X = x) =

r
x

×

N −r
n −x


N
n

for max{0, n + r −N} ≤x ≤min{n,r}. Notice that the number of defective items x must
take a value at least as large as n + r −N if this is positive. This is because when n + r −N
is positive, the sample size n is larger than the number of nondefective items N −r, so that
the sample must contain at least n −(N −r) defective items. Also, the number of defective
items x cannot be larger than either the sample size n or the total number of defective items r,
whichever is smaller.
The Hypergeometric Distribution
The hypergeometric distribution has a probability mass function given by
P(X = x) =

r
x

×

N −r
n −x


N
n

for max {0, n + r −N} ≤x ≤min{n,r}, with an expected value of
E(X) = nr
N
and a variance of
Var(X) =
 N −n
N −1

× n × r
N ×

1 −r
N

It represents the distribution of the number of items of a certain kind in a random
sample of size n drawn without replacement from a population of size N that contains
r items of this kind.
The hypergeometric distribution was encountered in Section 1.7 during the discussion of
Example 2 on defective computer chips. In this example, a box of N = 500 computer chips
contains r = 9 defective chips, and n = 3 chips are selected at random without replacement.
The total number of possible samples that can be taken is

500
3

which in general is

N
n

The number of samples containing exactly one defective chip is

9
1

×

491
2


170
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
where the ﬁrst term represents the number of ways of choosing one defective chip and the
second term represents the number of ways of choosing two satisfactory chips. In general, the
number of samples containing exactly x defective items can similarly be shown to be

r
x

×

N −r
n −x

The probability mass function of the hypergeometric distribution is then derived by noting
that since all possible samples are equally likely to be chosen, the probability of choosing x
defective items is obtained by dividing the number of samples containing x defective items
by the total possible number of samples.
Finally, it is useful to note that if the total number of items N is much larger than the
sample size n, then sampling without replacement is very similar to sampling with replace-
ment. This is because the reduction of the total population by the items selected does not
appreciably change the selection probabilities of additional items. In such a case, while the
hypergeometric distribution is technically correct, it is usually convenient to use the binomial
distribution
X ∼B(n,r/N)
to approximate the distribution of the number of defective items chosen.
3.3.2
Examples of the Hypergeometric Distribution
Example 17
Milk Container
Contents
Suppose that milk is shipped to retail outlets in boxes that hold 16 milk containers. One
particular box, which happens to contain 6 underweight containers, is opened for inspection,
and 5 containers are chosen at random. What is the distribution of the number of underweight
milk containers in the sample chosen by the inspector?
The required distribution is the hypergeometric distribution with N = 16, r = 6, and
n = 5, which is illustrated in Figure 3.17. For example, the probability that the inspector
FIGURE 3.17
Probability mass function of the
number of underweight milk
containers in the inspector’s sample
Probability
1
2
3
4
5
0
Number of underweight milk containers found by inspector
0.058
0.288
0.412
0.206
0.034
0.002

3.3 THE HYPERGEOMETRIC DISTRIBUTION
171
FIGURE 3.18
Probability mass function of the
number of tagged ﬁsh that are
recaptured
Probability
1
2
3
4
5
6
7
8
Number of tagged fish recaptured
0.143
0.347
0.322
0.147
0.035 0.005
0.001
0.000
0.000
0
chooses exactly two underweight containers is
P(X = 2) =

6
2

×

10
3


16
5

=
 6!
2! 4!

×
 10!
3! 7!

 16!
5! 11!

= 0.412
The expected number of underweight containers chosen by the inspector is
E(X) = nr
N = 5 × 6
16
= 1.875
Example 26
Fish Tagging and
Recapture
A small lake contains 50 ﬁsh. One day a ﬁsherman catches 10 of these ﬁsh and tags them so
that they can be recognized if they are caught again. The tagged ﬁsh are released back into
the lake. The next day the ﬁsherman goes out and catches 8 ﬁsh, which are kept in the ﬁshing
boat until they are all released at the end of the day. What is the distribution of the number of
tagged ﬁsh caught by the ﬁsherman on the second day?
The second day’s ﬁshing can be thought of as a sample of size eight taken without re-
placement from the ﬁsh stock. The sample is taken without replacement since the ﬁsh that are
caught are kept in the ﬁshing boat until all 8 ﬁsh have been caught (thereby eliminating the
possibility of the same ﬁsh being caught twice on the second day). Consequently, given that
all 50 ﬁsh are equally likely to be caught, the number of tagged ﬁsh caught on the second day
has a hypergeometric distribution with N = 50, r = 10, and n = 8.
This distribution is illustrated in Figure 3.18. For example, the probability that 3 tagged
ﬁsh are caught on the second day is
P(X = 3) =

10
3

×

40
5


50
8

=
 10!
3! 7!

×
 40!
5! 35!

 50!
8! 42!

= 0.147
The expected number of tagged ﬁsh recaptured on the second day is
E(X) = nr
N = 8 × 10
50
= 1.6

172
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
Suppose that there is also a much larger lake containing 5000 ﬁsh of which 80 have been
tagged over a period of time. If a ﬁsherman goes out one day and catches 8 ﬁsh (without
replacement), the distribution of the number of tagged ﬁsh caught will be a hypergeometric
distribution with N = 5000, r = 80, and n = 8. However, because of the large number of ﬁsh
in the lake, it is appropriate to approximate the distribution as a B(8, p) distribution, where
the success probability is
p = r
N =
80
5000 = 0.016
In practice, tagging and recapture experiments of this kind can be used to estimate the
total size of the ﬁsh stock. For example, if 100 ﬁsh have been tagged and the ﬁshermen ﬁnd
that tagged ﬁsh tend to represent about 10% of their catches, this suggests that the total ﬁsh
stock is about 1000 ﬁsh.
GAMES OF CHANCE
If six cards are randomly drawn without replacement from a pack of cards, the number of
cards chosen from the heart suit has a hypergeometric distribution with N = 52, r = 13, and
n = 6. The expected number of hearts chosen is
E(X) = nr
N = 6 × 13
52
= 1.5
The number of aces chosen has a hypergeometric distribution with N = 52, r = 4, and n = 6,
with an expected value of
E(X) = nr
N = 6 × 4
52
= 0.462
3.3.3
Problems
3.3.1 Let X have a hypergeometric distribution with N = 11,
r = 6, and n = 7. Calculate:
(a) P(X = 4)
(b) P(X = 5)
(c) P(X ≤3)
3.3.2 A committee consists of eight right-wing members and
seven left-wing members. A subcommittee is formed by
randomly choosing ﬁve of the committee members. Draw
a line graph of the probability mass function of the
number of right-wing members serving on the
subcommittee.
3.3.3 A box contains 17 balls of which 10 are red and 7 are
blue. A sample of 5 balls is chosen at random and placed
in a jar. Calculate the probability that:
(a) The jar contains exactly 3 red balls.
(b) The jar contains exactly 1 red ball.
(c) The jar contains more blue balls than red balls.
3.3.4 A jury of 12 people is selected at random from a group of
16 men and 18 women. What is the probability that the
jury contains exactly 7 women? Suppose that the jury
is selected at random from a group of 1600 men and
1800 women. Use the binomial approximation to the
hypergeometric distribution to calculate the probability
that in this case the jury contains exactly 7 women.
3.3.5 Five cards are selected at random from a pack of cards
without replacement. What is the probability that exactly
three of them are picture cards (kings, queens, or jacks)?
If a hand of 13 cards is dealt from the pack, what are the
expectation and variance of the number of picture cards
that it contains?
3.3.6 Consider a collection of N items of which ri are of type i,
for 1 ≤i ≤k, where
r1 + · · · + rk = N
Suppose that a sample of size n is chosen at random from
the N items without replacement. If Xi is the number of
items of type i in the sample, 1 ≤i ≤k, with
X1 + · · · + Xk = n
then they are said to have a multivariate
hypergeometric distribution. Explain why their

3.4 THE POISSON DISTRIBUTION
173
probability mass function is
P(X1 = x1, . . . , Xk = xk) =

r1
x1

× · · · ×

rk
xk


N
n

for suitable values of x1, . . . , xk.
A box contains four red balls, ﬁve blue balls, and six
green balls. Five balls are chosen at random and placed in
a jar. What is the probability that the jar contains one red
ball, two blue balls, and two green balls?
3.3.7 There are 11 items of a product on a shelf in a retail outlet,
and unknown to the customers, 4 of the items are outdated.
Suppose that a customer takes 3 items at random.
(a) What is the probability that none of the outdated
products are selected by the customer?
(b) What is the probability that exactly 2 of the items
taken by the customer are outdated?
3.3.8 A plate has 15 cupcakes on it, of which 9 are chocolate
and 6 are strawberry. A child randomly selects 5 of the
cupcakes and eats them. What is the probability that the
number of chocolate cupcakes remaining on the plate is
between 5 and 7 inclusive?
3.3.9 (a) A box contains 8 red balls and 8 blue balls, and 4 balls
are taken at random without replacement. What is the
probability that 2 red balls and 2 blue balls are taken?
(b) A box contains 50,000 red balls and 50,000 blue
balls, and 4 balls are taken at random without
replacement. Estimate the probability that 2 red balls
and 2 blue balls are taken.
3.3.10 In a ground water contamination study, the researchers
identify 25 possible sites for drilling and sample
collection. Unknown to the researchers, 19 of these sites
have ground water with a high contamination, while the
other 6 sites have ground water with a low contamination.
The researchers have a budget that only allows them to
drill at 5 sites, so they randomly choose these 5 sites from
their list of 25 sites. What is the probability that at least 4
out of the 5 sites that the researchers examine have
ground water with a high contamination?
3.3.11 An investor is considering making investments in
ten companies. Unknown to the investor, only three of
these companies will be successful. If the investor
randomly chooses four of the companies, what is the
probability that at least two will be successful?
3.4
The Poisson Distribution
3.4.1
Deﬁnition of the Poisson Distribution
It is often useful to deﬁne a random variable that counts the number of “events” that occur
within certain speciﬁed boundaries. For example, an experimenter may be interested in the
number of defects in an item, the number of radioactive particles emitted by a substance, or
the number of telephone calls received by an operator within a certain time limit. The Poisson
distribution is often appropriate to model such situations.
A random variable X with a Poisson distribution takes the values x = 0, 1, 2, 3, . . . with
a probability mass function
P(X = x) = e−λ λx
x!
where λ is the parameter of the distribution. This can be written
X ∼P(λ)
which should be read “X is distributed as a Poisson random variable with parameter λ.”
Notice that the series expansion of eλ ensures that these probability values sum to 1 since
∞

x = 0
P(X = x) =
∞

x = 0
e−λ λx
x!
= e−λ
1
1 + λ
1 + λ2
2! + λ3
3! + λ4
4! + · · ·

= e−λ × eλ = 1
It can also be shown that if X ∼P(λ), then
E(X) = Var(X) = λ

174
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
The Poisson Distribution
A random variable X distributed as a Poisson random variable with parameter λ,
which is written
X ∼P(λ)
has a probability mass function
P(X = x) = e−λ λx
x!
for x = 0, 1, 2, 3, . . . . The Poisson distribution is often useful to model the number of
times that a certain event occurs per unit of time, distance, or volume, and it has a
mean and variance both equal to the parameter value λ.
HISTORICAL NOTE
The French mathematician
Simeon Denis Poisson
(1781–1840) produced a vast
amount of mathematical work.
His main work on probability
theory, Recherches sur la
Probabilit´e des Jugements en
Mati`ere Criminelle et en
Mati`ere Civile, was published
in 1837. The probability
distribution named after him
was later applied by the Polish
statistician L. Bortkiewicz
(1868–1931) to the modeling
of rare events such as deaths by
horse-kick in the Prussian
Army.
Figures 3.19 and 3.20 illustrate and contrast the probability mass functions and cumulative
distribution functions of Poisson distributions with parameters λ = 2 and λ = 5. It can be seen
that since the mean and variance of the Poisson distribution are both equal to the parameter
value, the distribution with the larger parameter value has a larger expected value and is more
0.135
0.271 0.271
0.180
0.090
0.036
0.012 0.004 0.001
0
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
. . .
. . .
x
P(X Ä x)
0
1
2
3
4
5
6
7
8
9
0.135 0.406 0.677 0.857 0.947 0.983 0.995 0.999 1.000 1.000
. . .
. . .
x
0.000
FIGURE 3.19
Probability mass function and cumulative distribution function of a Poisson random variable with mean λ = 2.0

3.4 THE POISSON DISTRIBUTION
175
0.007
0.034
0.084
0.140
0.176 0.176
0.146
0.104
0.065
0.036
0.018
0.001
0.004
0.008
0
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
14
15
0.0010.000 . . .
. . .
x
P(X Ä x)
0
1
2
3
4
5
6
7
8
9
10
0.007 0.041 0.125 0.265 0.441 0.617 0.763 0.867 0.932 0.968 0.986
. . .
. . .
11
12
13
14
15
0.994 0.998 0.999
1.000
1.000
x
FIGURE 3.20
Probability mass function and cumulative distribution function of a Poisson random variable with mean λ = 5.0
spread out. As a check on some of the probability values given, notice that for λ = 2,
P(X = 3) = e−2 × 23
3!
= 0.135 × 8
6
= 0.180
and for λ = 5,
P(X ≤2) = P(X = 0) + P(X = 1) + P(X = 2)
= e−5 × 50
0!
+ e−5 × 51
1!
+ e−5 × 52
2!
= e−5
1
1 + 5
1 + 25
2

= 0.125
As a ﬁnal point, it may be useful to note that the Poisson distribution can be used to
approximate the B(n, p) distribution when n is very large (larger than 150, say) and the
success probability p is very small (smaller than 0.01, say). A parameter value of λ = np
should be used for the Poisson distribution, so that it has the same expected value as the
binomial distribution.

176
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
3.4.2
Examples of the Poisson Distribution
Example 3
Software Errors
Suppose that the number of errors in a piece of software has a Poisson distribution with
parameter λ = 3. This parameter immediately implies that the expected number of errors is
three and that the variance in the number of errors is also equal to three.
The distribution of the number of errors is illustrated in Figure 3.21, and the probability
that a piece of software has no errors is
P(X = 0) = e−3 × 30
0!
= e−3 = 0.050
The probability that there are three or more errors in a piece of software is
P(X ≥3) = 1 −P(X = 0) −P(X = 1) −P(X = 2)
= 1 −e−3 × 30
0!
−e−3 × 31
1!
−e−3 × 32
2!
= 1 −e−3
1
1 + 3
1 + 9
2

= 1 −0.423 = 0.577
0.050
0.149
0.224 0.224
0.168
0.101
0.050
0.022
0.008 0.003 0.001
0
1
2
3
4
5
6
7
8
9
10
11
12
13
Probability
Number of software errors
. . .
. . .
x
P(X Ä x)
0
1
2
3
4
5
6
7
8
9
10
0.050 0.199 0.423 0.647 0.815 0.916 0.966 0.988 0.996 0.999
1.000
. . .
. . .
11
12
13
1.000
1.000
1.000
0.000
FIGURE 3.21
Probability mass function and cumulative distribution function of a Poisson distribution with parameter λ = 3, the distribution of
the number of software errors

3.4 THE POISSON DISTRIBUTION
177
Example 27
Glass Sheet Flaws
A quality inspector at a glass manufacturing company inspects sheets of glass to check for
any slight imperfections. Suppose that the number of these ﬂaws in a glass sheet has a Poisson
distribution with parameter λ = 0.5. This implies that the expected number of ﬂaws per sheet
is only 0.5.
The distribution of the number of ﬂaws per sheet is shown in Figure 3.22. The probability
that there are no ﬂaws in a sheet is
P(X = 0) = e−0.5 × 0.50
0!
= e−0.5 = 0.607
so that about 61% of the glass sheets are in “perfect” condition. Sheets with two or more ﬂaws
are scrapped by the company, and this happens with a probability of
P(X ≥2) = 1 −P(X = 0) −P(X = 1) = 1 −e−0.5 × 0.50
0!
−e−0.5 × 0.51
1!
= 1 −e−0.5
1
1 + 0.5
1

= 1 −0.910 = 0.090
Consequently, about 9% of the glass sheets need to be scrapped and recycled through the
company’s manufacturing process.
FIGURE 3.22
Probability mass function and
cumulative distribution function of
a Poisson distribution with
parameter λ = 0.5, the distribution
of the number of ﬂaws in a
glass sheet
Probability
0
1
2
3
4
5
6
0
1
2
3
4
x
Number of flaws in a glass sheet
0.607
0.303
0.076
0.0130.001
0.607 0.910 0.986 0.999 1.000
5
6
1.000 1.000
. . .
. . .
P(X Ä x)
. . .
. . .
0.000

178
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
Example 28
Hospital Emergency
Room Arrivals
A hospital emergency room accepts an average of about 47 bone fracture patients per week.
How might the number of bone fracture patients arriving in a certain day be modeled? As-
suming that bone fracture accidents are equally likely to occur on any day of the week, it is
reasonable to expect an average of 47/7 = 6.71 patients per day. If the Poisson distribution
is chosen to model the number of bone fracture patients, then it is appropriate to choose a
parameter value of λ = 6.71 because this is the expected value of the distribution.
The hospital manager has decided to allocate emergency room resources that are sufﬁcient
to comfortably cope with up to ten bone fracture patients per day. The Poisson distribution
can be used to predict the probability that on any given day these resources will be inadequate.
The probability that no more than ten bone fracture patients arrive at the emergency room is
predicted to be
P(X ≤10) =
10

x = 0
e−6.71 × 6.71x
x!
= e−6.71
1
1 + 6.71
1
+ 6.712
2!
+ 6.713
3!
+ · · · + 6.7110
10!

= 0.921
Consequently the manager should expect the emergency room to require additional assistance
on about 8% of days, which is an average of about 29 days per year.
3.4.3
Problems
3.4.1 If X ∼P(3.2), calculate:
(a) P(X = 1)
(b) P(X ≤3)
(c) P(X ≥6)
(d) P(X = 0|X ≤3)
3.4.2 If X ∼P(2.1), calculate:
(a) P(X = 0)
(b) P(X ≤2)
(c) P(X ≥5)
(d) P(X = 1|X ≤2)
3.4.3 If X ∼P(λ), show that
E(X) =
∞

x = 0
x P(X = x) = λ
Also show that
E(X 2) −E(X) = E(X(X −1))
=
∞

x = 0
x(x −1)P(X = x) = λ2
and use this result to show that Var(X) = λ.
3.4.4 The number of cracks in a ceramic tile has a
Poisson distribution with a mean of λ = 2.4. What
is the probability that a tile has no cracks? What is
the probability that a tile has four or more cracks?
3.4.5 On average there are about 25 imperfections in 100 meters
of optical cable. Use the Poisson distribution to estimate
the probability that there are no imperfections in 1 meter of
cable. What is the probability that there is no more than
one imperfection in 1 meter of cable?
3.4.6 On average there are four trafﬁc accidents in a city during
one hour of rush-hour trafﬁc. Use the Poisson distribution
to calculate the probability that in one such hour there are
(a) no accidents
(b) at least six accidents
3.4.7 Recall that the Poisson distribution with a parameter
value of λ = np can be used to approximate the B(n, p)
distribution when n is very large and the success
probability p is very small.
A box contains 500 electrical switches, each one of
which has a probability of 0.005 of being defective. Use
the Poisson distribution to make an approximate
calculation of the probability that the box contains no more
than 3 defective switches.
3.4.8 In a scanning process, the number of misrecorded pieces of
information has a Poisson distribution with parameter
λ = 9.2.
(a) What is the probability that there are between six and
ten misrecorded pieces of information?
(b) What is the probability that there are no more than
four misrecorded pieces of information?
(This problem is continued in Problem 5.7.10.)
3.4.9 Suppose that the number of errors in a company’s accounts
has a Poisson distribution with a mean on 4.7. What is the
probability that there will be exactly three errors?
A. 0.127
B. 0.157
C. 0.187
D. 0.217
3.4.10 (Problem 3.4.9 continued) What is the probability that
there will be three or more errors?
A. 0.818
B. 0.848
C. 0.878
D. 0.908

3.5 THE MULTINOMIAL DISTRIBUTION
179
3.5
The Multinomial Distribution
3.5.1
Deﬁnition of the Multinomial Distribution
Whereas the binomial distribution is generated from a sequence of Bernoulli trials, each with
only two possible outcomes, the multinomial distribution is a more general distribution that
arises when each of the trials can have three or more possible outcomes.
The Multinomial Distribution
Consider a sequence of n independent trials where each individual trial can have
k outcomes that occur with constant probability values p1, . . . , pk with
p1 + · · · + pk = 1. The random variables X1, . . . , Xk that count the number of
occurrences of each outcome are said to have a multinomial distribution, and their
joint probability mass function is
P(X1 = x1, . . . , Xk = xk) =
n!
x1! · · · xk! × px1
1 × · · · × pxk
k
for nonnegative integer values of the xi satisfying x1 + · · · + xk = n.
The random variables X1, . . . , Xk have expectations and variances given by
E(Xi) = npi
and
Var(Xi) = npi(1 −pi)
but they are not independent.
The justiﬁcation of this probability mass function is similar to that of the binomial dis-
tribution. If X1 = x1, . . . , Xk = xk, then this means that xi of the trials take outcome i with
probability pi, so that a particular realization of the k trials has a probability of
px1
1 × · · · × pxk
k
Since the total number of possible rearrangements of the n trial outcomes satisfying
X1 = x1, . . . , Xk = xk is
n!
x1! · · · xk!
the probability mass function is as indicated.
Since the random variable Xi counts the number of trials that take outcome i, which has
a constant probability of pi, the marginal distribution of Xi is
Xi ∼B(n, pi)
Consequently, E(Xi) = npi and Var(Xi) = npi(1 −pi). However, the random variables
X1, . . . , Xk are not independent.
3.5.2
Examples of the Multinomial Distribution
Example 1
Machine Breakdowns
Recall that the machine breakdowns are attributable to electrical faults, mechanical faults, and
operator misuse, and that these causes occur with probabilities of 0.2, 0.5, and 0.3, respectively.
The engineer in charge of the maintenance of the machine is interested in predicting the causes
of the next ten breakdowns. If X1 is the number of breakdowns due to electrical reasons, X2

180
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
is the number of breakdowns due to mechanical reasons, and X3 is the number of breakdowns
due to operator misuse, then
X1 + X2 + X3 = 10
and if the breakdown causes can be assumed to be independent of one another, the random
variables X1, X2, and X3 have a multinomial distribution with a probability mass function
P(X1 = x1, X2 = x2, X3 = x3) =
10!
x1! x2! x3! × 0.2x1 × 0.5x2 × 0.3x3
The probability that there will be three electrical breakdowns, ﬁve mechanical breakdowns,
and two misuse breakdowns is therefore
P(X1 = 3, X2 = 5, X3 = 2) =
10!
3! 5! 2! × 0.23 × 0.55 × 0.32 = 0.057
The expected number of electrical breakdowns is
E(X1) = np1 = 10 × 0.2 = 2
the expected number of mechanical breakdowns is
E(X2) = np2 = 10 × 0.5 = 5
and the expected number of misuse breakdowns is
E(X3) = np3 = 10 × 0.3 = 3
If the engineer is interested in the probability of there being no more than two electrical
breakdowns, then this can be calculated by noting that X1 ∼B(10, 0.2), so that
P(X1 ≤2) = P(X1 = 0) + P(X1 = 1) + P(X1 = 2)
=

10
0

× 0.20 × 0.810 +

10
1

× 0.21 × 0.89
+

10
2

× 0.22 × 0.88
= 0.107 + 0.268 + 0.302 = 0.677
Example 29
Drug Allergies
Patients being treated with a particular drug run the risk of being allergic to the drug. A
patient’s reaction to the drug is characterized by doctors as being hyperallergic, allergic,
mildly allergic, or not allergic, and these have probability values of 0.12, 0.28, 0.33, and 0.27,
respectively.
Suppose that nine patients are administered the drug, as illustrated in Figure 3.23. How
can the doctors predict the types of allergy that will be encountered? If X1, X2, X3, and X4 are
the numbers of patients exhibiting the four types of reaction, then assuming that one patient’s
reaction can reasonably be assumed to be independent of another patient’s (this might not
be the case, for example, if the patients are related to one another), the random variables
X1, X2, X3, and X4 will have a multinomial distribution with a probability mass function
P(X1 = x1, X2 = x2, X3 = x3, X4 = x4)
=
9!
x1! x2! x3! x4! × 0.12x1 × 0.28x2 × 0.33x3 × 0.27x4

3.5 THE MULTINOMIAL DISTRIBUTION
181
P(allergic) = 0.28
P(hyperallergic) = 0.12
P(not allergic) = 0.27
P(mildly allergic) = 0.33
X1
X2
X3
X4
Patients
Reaction to drug
Number of patients 
exhibiting each reaction
FIGURE 3.23
The number of patients exhibiting each kind of drug reaction has a multinomial distribution
The probability that two patients are hyperallergic, one patient is allergic, four patients
are mildly allergic, and two patients exhibit no reaction is therefore
P(X1 = 2, X2 = 1, X3 = 4, X4 = 2)
=
9!
2! 1! 4! 2! × 0.122 × 0.281 × 0.334 × 0.272 = 0.013
The probability that no patients are hyperallergic, one patient is allergic, four patients are
mildly allergic, and four patients exhibit no reaction is
P(X1 = 0, X2 = 1, X3 = 4, X4 = 4)
=
9!
0! 1! 4! 4! × 0.120 × 0.281 × 0.334 × 0.274 = 0.011
The expected number of hyperallergic reactions is
E(X1) = np1 = 9 × 0.12 = 1.08

182
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
the expected number of allergic reactions is
E(X2) = np2 = 9 × 0.28 = 2.52
the expected number of mildly allergic reactions is
E(X3) = np3 = 9 × 0.33 = 2.97
and the expected number of patients not exhibiting a reaction is
E(X4) = np4 = 9 × 0.27 = 2.43
GAMES OF CHANCE
Suppose that eight cards are chosen at random from a pack of cards with replacement. What
is the distribution of the number of cards obtained from each of the four suits?
Since the drawings are made with replacement, each of the four suits is equally likely to
be chosen at each of the drawings, so that the numbers of hearts X1, clubs X2, diamonds X3,
and spades X4 have a multinomial distribution with probability mass function
P(X1 = x1, X2 = x2, X3 = x3, X4 = x4)
=
8!
x1! x2! x3! x4! ×
1
4
x1
×
1
4
x2
×
1
4
x3
×
1
4
x4
However, since X1 + X2 + X3 + X4 = 8, this simpliﬁes to
P(X1 = x1, X2 = x2, X3 = x3, X4 = x4) =
8!
x1! x2! x3! x4! ×
1
4
8
The probability of drawing three hearts, two clubs, two diamonds, and one spade is there-
fore
P(X1 = 3, X2 = 2, X3 = 2, X4 = 1) =
8!
3! 2! 2! 1! ×
1
4
8
= 105
4096 = 0.026
and the expected number of cards from each suit is two. The probability of actually drawing
two cards from each suit is
P(X1 = 2, X2 = 2, X3 = 2, X4 = 2) =
8!
2! 2! 2! 2! ×
1
4
8
= 315
8192 = 0.038
3.5.3
Problems
3.5.1 A garage sells three types of tires, type A, type B, and
type C. A customer purchases type A with probability
0.23, type B with probability 0.48, and type C with
probability 0.29.
(a) What is the probability that the next 11 customers
purchase four sets of type A, ﬁve sets of type B, and
two sets of type C?
(b) What is the probability that fewer than three sets
of type A are sold to the next seven customers?
3.5.2 A fair die is rolled 15 times. Calculate the probability that
there are:
(a) Exactly three 6s and three 5s
(b) Exactly three 6s, three 5s, and four 4s
(c) Exactly two 6s
What is the expected number of 6s obtained?
3.5.3 Recall Problems 3.1.4 and 3.2.5, where an archer hits a
bull’s-eye with a probability of 0.09. Suppose also that the
archer misses the target completely with a probability of
0.12. If the archer shoots eight arrows whose performances
are independent of each other, calculate the probability of:
(a) Scoring exactly two bull’s-eyes and missing the target
exactly once
(b) Scoring exactly one bull’s-eye and missing the target
exactly twice

3.6 CASE STUDY: MICROELECTRONIC SOLDER JOINTS
183
(c) Scoring at least two bull’s-eyes
What is the expected number of times the archer misses the
target?
3.5.4 A researcher plants 22 seedlings. After one month,
independent of the other seedlings, each seedling has a
probability of 0.08 of being dead, a probability of 0.19 of
exhibiting slow growth, a probability of 0.42 of exhibiting
medium growth, and a probability of 0.31 of exhibiting
strong growth. What is the expected number of seedlings in
each of these four categories after one month? Calculate
the probability that after one month:
(a) Exactly three seedlings are dead, exactly four exhibit
slow growth, and exactly six exhibit medium growth.
(b) Exactly ﬁve seedlings are dead, exactly ﬁve exhibit
slow growth, and exactly seven exhibit strong growth.
(c) No more than two seedlings have died.
3.5.5 Recall Problems 3.1.9 and 3.2.9, where a company
receives 60% of its orders over the Internet. Suppose that
30% of the orders received over the Internet are large
orders, and 40% of the orders received by other means
are large orders. Out of eight independently placed
orders, what is the probability that two will be large
orders received over the Internet, two will be small orders
received over the Internet, two will be large orders not
received over the Internet, and two will be small orders
not received over the Internet?
3.5.6 In a consumer satisfaction survey the responses are “very
unsatisfactory,” “unsatisfactory,” “average,” “satisfactory,”
and “very satisfactory.” If each of these responses are
equally likely, what is the probability that in ten surveys
each answer will be selected twice?
3.6
Case Study: Microelectronic Solder Joints
Recall that there is a probability of 0.12 that a solder joint will have an hourglass shape. If an
assembly is comprised of 64 solder joints, and if their shapes can be taken to be independent
of each other, then the total number of hourglass-shaped solder joints on the assembly is
distributed as a B(64, 0.12) random variable. This has an expectation of 64 × 0.12 = 7.68
with a standard deviation √64 × 0.12 × 0.88 = 2.60. The probability that there are at most
two hourglass-shaped solder joints on the assembly is
P(X = 0) + P(X = 1) + P(X = 2)
= 0.8864 + (64 × 0.12 × 0.8863) +
64 × 63
2
× 0.122 × 0.8862

= 0.0132
The probability that a solder joint is hourglass shaped and is cracked is
P(hourglass) × P(cracked|hourglass) = 0.12 × 0.005 = 0.0006
Consequently, if a researcher examines solder joints one at a time until a cracked hourglass-
shaped solder joint is found, the number of solder joints that need to be examined has a
geometric distribution with parameter p = 0.0006. The probability that more than 1000
solder joints will need to be examined is
1 −P(X ≤1000) = 1 −(1 −(1 −0.0006)1000) = 0.99941000 = 0.549
If the researcher wishes to ﬁnd two cracked hourglass-shaped joints, then the number of solder
joints that need to be examined has a negative binomial distribution with parameters r = 2
and p = 0.0006. This has an expectation of 2/0.0006 = 3333.3.
Finally, the number of barrel-shaped, cylinder-shaped, and hourglass-shaped solder joints
on an assembly comprising of 16 solder joints has a multinomial distribution with n = 16 and
probabilities p1 = 0.85, p2 = 0.03, and p3 = 0.12 as given in Chapter 1. The probability
that the assembly will have 12 barrel-shaped solder joints, 1 cylinder-shaped solder joint, and
3 hourglass-shaped solder joints is therefore
16!
12! 1! 13! × 0.8512 × 0.031 × 0.123 = 0.054

184
CHAPTER 3
DISCRETE PROBABILITY DISTRIBUTIONS
3.7
Case Study: Internet Marketing
Recall that when a organisation’s website is accessed, there is a probability of 0.07 that the
web address was typed in directly, and the organisation incurs no charge for this. Consider the
next 250 visits to the organisation’s website. The number of these visits that will be as a result
of a direct typing of the web address has a binomial distribution with n = 250 and p = 0.07.
The expected number of this type of visits is
np = 250 × 0.07 = 17.5
The probability that at least ten of the visits will be through a direct typing of the web address is
P(B(250, 0.07) ≥10) = 1 −P(B(250, 0.07) ≤9) = 1 −0.017 = 0.983
3.8
Supplementary Problems
3.8.1 An integrated circuit manufacturer produces wafers that
contain 18 chips. Each chip has a probability of 0.085 of
not being placed quite correctly on the wafer.
(a) Find the probability that a wafer contains at least
three incorrectly placed chips.
(b) What is the probability that a wafer contains no more
than one incorrectly placed chip?
(c) What is the expected number of incorrectly placed
chips on a wafer?
3.8.2 A biologist has a culture consisting of 13 cells. In a
period of 1 hour, independent of the other cells, there is
a probability of 0.4 that each of these cells splits into
2 cells. What is the probability that after 1 hour the
biologist has at least 16 cells? What is the expected
number of cells after 1 hour?
3.8.3 A beverage company has three different formulas for its
soft drink product. Tests reveal that 40% of consumers
prefer formula I, 25% of consumers prefer formula II, and
35% of consumers prefer formula III. If eight consumers
are chosen at random, calculate the probability that:
(a) Exactly two prefer formula I and exactly three prefer
formula II.
(b) Exactly three prefer formula I and exactly four prefer
formula III.
(c) No more than two prefer formula III.
3.8.4 A company’s toll-free complaints line receives an
average of about 40 calls per hour. Use the Poisson
distribution to estimate the probability that in one minute
there are
(a) no calls
(b) exactly one call
(c) three or more calls
3.8.5 The number of radioactive particles passing
through a counter in one minute has a Poisson
distribution with λ = 3.3. What is the probability
that in one minute there are exactly two particles passing
through the counter? What is the probability that in
one minute there are at least six particles passing through
the counter?
3.8.6 In a typical sports playoff series, two teams play a
sequence of games until one team, the eventual winner,
has won four games. Suppose that in each game team A
beats team B with a probability of 0.55, and that the
results of different games are independent.
(a) Explain how the negative binomial distribution can
be used to analyze this problem.
(b) What is the probability that team A wins the series in
game seven?
(c) What is the probability that team A wins the series in
game six?
(d) What is the probability that the series is over after
game ﬁve?
(e) What is the probability that team A wins the series?
3.8.7 A golf shop sells both right-handed and left-handed sets
of clubs, and 42% of customers purchase right-handed
sets whereas 58% of customers purchase left-handed sets.
The owner opens the shop one day and waits for
customers to arrive.
(a) What is the probability that the ninth set of clubs sold
in the day is the third set of left-handed clubs sold
that day?
(b) What is the probability that four sets of right-handed
clubs are sold before four sets of left-handed clubs
are sold?

3.8 SUPPLEMENTARY PROBLEMS
185
3.8.8 Box A contains six red balls and ﬁve blue balls. Box B
contains ﬁve red balls and six blue balls. A fair coin is
tossed, and if a head is obtained, three balls are taken at
random from box A and placed in a jar. If a tail is
obtained, three balls are taken at random from box B and
placed in a jar. If the jar contains two red balls and one
blue ball after the toss, what is the probability that a head
had been obtained?
3.8.9 Suppose that a box contains 40 items of which 4 are
defective. If a random sample of 5 items is chosen, what
is the probability that it contains no more than 1 defective
item? If a random sample of 5 items is chosen from a
collection of 4,000,000 items of which 400,000 are
defective, what is the probability that it contains no more
than 1 defective item?
3.8.10 (a) If a fair die is rolled 22 times, what is the probability
that a 6 is obtained exactly 3 times?
(b) If a fair die is rolled, what is the probability that
the third time that a 6 is obtained is on the tenth
roll?
(c) If a fair coin is tossed 11 times, what is the
probability that three or fewer heads are obtained?
3.8.11 A box contains 11 red balls and 8 blue balls. Six balls are
taken at random from the box without replacement. What
is the probability that out of these 6 balls, exactly 3 are
red and exactly 3 are blue?
3.8.12 Are the following statements true or false?
(a) An unfair coin, for which the probabilities of a head
and a tail are different, is tossed seven times. The
probability of getting three heads and four tails
cannot be the same as the probability of getting four
heads and three tails.
(b) A fair die is rolled. The probability that a 6 is not
obtained until the eighth roll is 78,125/1,679,616.
(c) A fair coin is tossed. The probability that the fourth
head occurs on the seventh toss is 5/32.
(d) The proportion of heads in 16 tosses of a fair coin has
a standard deviation 1/8.
3.8.13 (a) A fair die is rolled ten times. What is the probability
of obtaining the outcome 6 exactly three times?
(b) A fair die is repeatedly rolled. What is the probability
that the outcome 6 is obtained for the fourth time on
the twentieth roll?
(c) A fair die is rolled nine times. What is the probability
of obtaining the outcomes 5 and 6 both exactly two
times each?
3.8.14 The number of imperfections in an object has a Poisson
distribution with a mean λ = 8.3. If the number of
imperfections is 4 or less, the object is called “top
quality.” If the number of imperfections is between 5 and
8 inclusive, the object is called “good quality.” If the
number of imperfections is between 9 and 12 inclusive,
the object is called “normal quality.” If the number of
imperfections is 13 or more, the object is called “bad
quality.” The number of imperfections in different objects
are independent of each other.
(a) A set of seven articles is taken. What is the
probability that the set has exactly two top-quality,
two good-quality, two normal-quality and one
bad-quality objects?
(b) A set of ten articles is taken. What are the expectation
and the standard deviation of the number of normal
quality objects in the set.
(c) A set of eight articles is taken. What is the probability
that the sum of the number of top quality and good
quality objects is three or less?
3.8.15 A utility company calculates that there is a probability of
0.82 that a customer service problem can be successfully
resolved within 2 hours. A managers monitors how the
problems are resolved throughout the day. What is the
probability that the sixth problem of the day is the second
problem that cannot be resolved within 2 hours?
A. 0.013
B. 0.033
C. 0.053
D. 0.073
3.8.16 (Problem 3.8.15 continued) Suppose that eight problems
are reported during the morning. What is the probability
that exactly two of these problems cannot be resolved
within 2 hours?
A. 0.276
B. 0.296
C. 0.316
D. 0.336

C H A P T E R F O U R
Continuous Probability Distributions
Most of the common continuous probability distributions are presented in Chapters 4 and
5. The probability density functions of these distributions are described by formulas that
depend on some parameter values. The expectations and variances of the distributions are
speciﬁed in terms of these parameters. The probability values associated with these continuous
distributions are sometimes straightforward to calculate, although some distributions require
the use of a software package.
In this chapter the most simple continuous distribution, the uniform distribution, is
considered ﬁrst, followed by the exponential distribution, which is often appropriate for
modeling failure rates or waiting times. The exponential distribution can be generalized to
both the gamma distribution and the Weibull distribution. Finally, the beta distribution that
can be useful for modeling proportions is discussed. The exponential and gamma distributions
are used to discuss the Poisson process, which is a simple stochastic process.
However, the most important continuous probability distribution for statistical data
analysis—the distribution that will be used the most frequently in the remainder of this book—
is the normal distribution. This distribution is discussed in Chapter 5 together with some
other continuous probability distributions that are related to the normal distribution.
4.1
The Uniform Distribution
4.1.1
Deﬁnition of the Uniform Distribution
The simplest continuous probability distribution has a ﬂat probability density function
between two points a and b, say, as illustrated in Figure 4.1. It is called a uniform distribution
between a and b and can be written
X ∼U(a, b)
In order for the area under the probability density function to be equal to 1, it must have a
height of 1/(b −a), so that
f (x) =
1
b −a
for a ≤x ≤b and f (x) = 0 elsewhere. The cumulative distribution function is
F(x) =
 x
y=a
1
b −a dy = x −a
b −a
for a ≤x ≤b.
A random variable X ∼U(a, b) has the simple interpretation that it is “equally” likely
to take values anywhere between a and b. More precisely, the random variable is equally
186

4.1 THE UNIFORM DISTRIBUTION
187
FIGURE 4.1
Probability density function of a
U(a, b) distribution
a
b
f(x) =
1
b − a
E(X)
a + b
2
=
b − a
12
σ =
x
b − a
12
σ =
likely to take a value within any interval of length δ that is contained between a and b.
Moreover,theprobabilitythattherandomvariabledoesfallwithinagivenintervaloflengthδ is
δ/(b −a).
A U(0, 1) distribution is often thought of as being a “standard” uniform distribution. A
standard uniform distribution can be obtained from any uniform distribution using a linear
transformation, since if
X ∼U(a, b)
then
Y = X −a
b −a ∼U(0, 1)
A uniform distribution is a symmetric distribution, so that its expectation is its middle
value
E(X) = a + b
2
Also,
E(X2) =
 b
a
x2
1
b −a dx = b3 −a3
3(b −a) = a2 + ab + b2
3
and the variance of a U(a, b) random variable is therefore
Var(X) = E(X2) −(E(X))2 = a2 + ab + b2
3
−(a + b)2
4
= (b −a)2
12
Notice that the variance increases as b −a gets larger and the uniform distribution becomes
more spread out.

188
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
The Uniform Distribution
A random variable X with a ﬂat probability density function between two points a
and b, so that
f (x) =
1
b −a
for a ≤x ≤b and f (x) = 0 elsewhere, is said to have a uniform distribution, which
is written X ∼U(a, b). The cumulative distribution function is
F(x) = x −a
b −a
for a ≤x ≤b, and the expectation and variance are
E(X) = a + b
2
and
Var(X) = (b −a)2
12
.
Since the distribution is symmetric, the median of a U(a, b) distribution is, like the ex-
pectation, equal to the middle value (a+b)/2. In general, the pth quantile of the distribution is
(1 −p)a + pb
and the interquartile range is (b −a)/2.
4.1.2
Examples of the Uniform Distribution
Example 30
Pearl Oyster Farming
When pearl oysters are opened, pearls of various sizes are found. Suppose that each oyster
contains a pearl with a diameter in mm that has a U(0, 10) distribution. The expected pearl
diameter is therefore 5 mm, with a variance of
Var(X) = (10 −0)2
12
= 8.33
and a standard deviation of σ =
√
8.33 = 2.89 mm, as shown in Figure 4.2.
FIGURE 4.2
Distribution of pearl diameters
f(x) = 0.1
0 mm
E(X) = 5 mm
10 mm
σ = 2.89 mm
Pearl diameter
σ = 2.89 mm

4.1 THE UNIFORM DISTRIBUTION
189
Pearls with a diameter of at least 4 mm have commercial value. The probability that an
oyster contains a pearl of commercial value is therefore
P(X ≥4) = 1 −F(4) = 1 −0.4 = 0.6
Suppose that a farmer retrieves ten oysters out of the water and that the random variable Y
represents the number of them containing pearls of commercial value. If the oysters grow
pearls independently of one another, Y has a binomial distribution with parameters n = 10
and p = 0.6, and the probability that at least 8 of the oysters contain pearls of commercial
value is
P(Y ≥8) = P(Y = 8) + P(Y = 9) + P(Y = 10)
=

10
8

× 0.68 × 0.42 +

10
9

× 0.69 × 0.41 +

10
10

× 0.610 × 0.40
= 0.121 + 0.040 + 0.006 = 0.167
GAMES OF CHANCE
In the dial-spinning game discussed in Section 2.2, both the angle θ and the winnings X have
uniform distributions. Speciﬁcally, θ ∼U(0, 180) and X ∼U(0, 1000). Clearly,
E(θ) = 90
and
E(X) = 500
and the formula for the variance gives
Var(θ) = (180 −0)2
12
= 2700
with a standard deviation of σθ =
√
2700 = 51.96, and
Var(X) = (1000 −0)2
12
= 83,333
with a standard deviation of σX = √83,333 = 288.7.
4.1.3
Problems
4.1.1 Suppose that X ∼U(−3, 8). Find:
(a) E(X)
(b) The standard deviation of X
(c) The upper quartile of the distribution
(d) P(0 ≤X ≤4)
4.1.2 A new battery supposedly with a charge of 1.5 volts
actually has a voltage with a uniform distribution between
1.43 and 1.60 volts.
(a) What is the expectation of the voltage?
(b) What is the standard deviation of the voltage?
(c) What is the cumulative distribution function of the
voltage?
(d) What is the probability that a battery has a voltage less
than 1.48 volts?
(e) If a box contains 50 batteries, what are the expectation
and variance of the number of batteries in the box with
a voltage less than 1.5 volts?
4.1.3 A computer random-number generator produces numbers
that have a uniform distribution between 0 and 1.
(a) If 20 random numbers are generated, what are the
expectation and variance of the number of them that
lie in each of the four intervals [0.00, 0.30),
[0.30, 0.50), [0.50, 0.75), and [0.75, 1.00)?
(b) What is the probability that exactly ﬁve numbers lie in
each of the four intervals?
4.1.4 The lengths in meters of pieces of scrap wood found on a
building site are uniformly distributed between 0.0 and 2.5.

190
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
(a) What are the expectation and variance of the lengths?
(b) What is the probability that at least 20 out of 25 pieces
of scrap wood are longer than 1 meter?
4.1.5 Suppose that a metal pin has a diameter that has a uniform
distribution between 4.182 mm and 4.185 mm.
(a) What is the probability that a pin will ﬁt into a hole
that has a diameter of 4.184 mm?
(b) If a pin does ﬁt into the hole, what is the probability
that the difference between the diameter of the hole
and the diameter of the pin is less than 0.0005 mm?
4.1.6 When employees undergo an evaluation, their scores are
independent and uniformly distributed between 60 and 100.
(a) If six employees take the evaluation, what is the
probability that half of them score more than 85 and
half less?
(b) If six employees take the evaluation, what is the
probability that two of them score less than 80, two of
them score between 80 and 90, and the remaining two
score more than 90?
(c) If the employees are tested sequentially, what is
the expected number of employees who need to be
tested before three are found with scores higher
than 90?
4.2
The Exponential Distribution
4.2.1
Deﬁnition of the Exponential Distribution
The exponential distribution has a state space x ≥0 and is often used to model failure or
waiting times and interarrival times. It has a probability density function
f (x) = λe−λx
for x ≥0 and f (x) = 0 for x < 0, which depends upon a parameter λ > 0. The cumulative
distribution function is
F(x) =
 x
0
λe−λy dy = 1 −e−λx
for x ≥0.
The expectation of an exponential distribution with parameter λ can be calculated using
integration by parts as
E(X) =
 ∞
0
xλe−λx dx = 1
λ
Similarly, integration by parts reveals that
E(X2) =
 ∞
0
x2λe−λx dx = 2
λ2
so that
Var(X) = E(X2) −E(X)2 = 2
λ2 −1
λ2 = 1
λ2
Notice that the standard deviation of the distribution is consequently 1/λ, which is also the
expectation of the distribution.

4.2 THE EXPONENTIAL DISTRIBUTION
191
The Exponential Distribution
An exponential distribution with parameter λ > 0 has a probability density function
f (x) = λe−λx
for x ≥0 and f (x) = 0 for x < 0, and a cumulative distribution function
F(x) = 1 −e−λx
for x ≥0. It is useful for modeling failure times and waiting times. Its expectation and
variance are
E(X) = 1
λ
and
Var(X) = 1
λ2
Figure 4.3 shows the probability density function of an exponential distribution with
parameter λ = 1, and Figure 4.4 shows the probability density function of an exponential
distribution with parameter λ = 1/2. The ﬁrst distribution has a mean and standard deviation
of 1, and the second distribution has a mean and standard deviation equal to 2. Notice that the
shapes of the probability density functions are smooth exponential decays.
The pth quantile of an exponential distribution, that is, the value of x that satisﬁes
F(x) = p, is
−ln(1 −p)
λ
In particular, the median of the distribution is
−ln(1.0 −0.5)
λ
= 0.693
λ
= 0.693 × E(X)
0.5
1.0
1
2
3
4
f(x) = e−x
x
FIGURE 4.3
Probability density function of an exponential distribution
with parameter λ = 1
0.5
1
2
3
4
f(x) =      e− x/2
1
2
x
FIGURE 4.4
Probability density function of an exponential distribution with
parameter λ = 1/2

192
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
The fact that the median is smaller than the expected value is because of the long right tail of
the probability density function.
COMPUTER NOTE
Theprobabilityvaluesofanexponentialdistributionareeasilycalculatedbecauseofthesimple
form of the cumulative distribution function. Nevertheless, you should also ﬁnd that they are
available on your computer package. Make sure that you specify the distribution correctly since
some packages deﬁne the parameter of the exponential distribution to be 1̸ λ instead of λ.
4.2.2
The Memoryless Property of the Exponential Distribution
An important aspect of the exponential distribution is its memoryless property. This property
states that if X has an exponential distribution with parameter λ, then conditional on X ≥x0 for
some ﬁxed value x0, the quantity X −x0 also has an exponential distribution with parameter λ.
In other words, if X measures the time until a certain event occurs and the event has not
occurred by time x0, the additional waiting time for the event to occur beyond x0 has the
same exponential distribution as X. The process seems to “forget” that a time x0 has already
elapsed and acts as though it is just starting afresh at time zero.
The memoryless property can be shown in the following manner. Notice that if X has an
exponential distribution with parameter λ, then
P(X ≥x) = 1 −F(x) = e−λx
Then if the random variable Y represents the additional time beyond x0 that elapses before
the event occurs,
P(Y ≥y) = P(X ≥x0 + y | X ≥x0) = P(X ≥x0 + y)
P(X ≥x0)
= e−λ(x0+y)
e−λx0
= e−λy
so that Y also has an exponential distribution with parameter λ. In graphical terms, the mem-
oryless property follows from the fact that the section of the probability density function of
an exponential distribution beyond a certain point x0 is just a scaled version of the whole
probability density function, as illustrated in Figure 4.5.
FIGURE 4.5
Illustration of the memoryless
property of the exponential
distribution. The part of the
probability density function beyond
x0 is a scaled version of the whole
probability density function
x
x0
f(x) = λ e− λ x

4.2 THE EXPONENTIAL DISTRIBUTION
193
The implications of the memoryless property can be rather confusing when ﬁrst encoun-
tered. Suppose that you are waiting at a bus stop and that the time in minutes until the arrival
of the bus has an exponential distribution with λ = 0.2. The expected time that you will wait
is consequently 1/λ = 5 minutes. However, if after 1 minute the bus has not yet arrived, what
is the expectation of the additional time that you must wait?
Unfortunately, it has not been reduced to 4 minutes but is still, as before, 5 minutes. This is
because the additional waiting time until the bus arrives beyond the ﬁrst minute during which
you know the bus did not arrive still has an exponential distribution with λ = 0.2. In fact,
as long as the bus has not arrived, no matter how long you have waited, you always have an
expected additional waiting time of 5 minutes! This is true right up until the time you ﬁrst
spot the bus coming.
The memoryless property of the exponential distribution makes it attractive for modeling
many processes. However, at the same time it reveals that the exponential distribution is unsuit-
ableformodelingprocessesthatclearlydonotpossesssuchaproperty.Thegammadistribution
and the Weibull distribution, which are discussed in subsequent sections, are generalizations
of the exponential distribution that do not possess this memoryless property (in fact, the expo-
nential distribution is the only continuous distribution with a memoryless property) and which
may be more suitable than the exponential distribution for modeling certain waiting times.
In particular, if an electronic component fails due to a random voltage ﬂuctuation, then
it may be appropriate to model the failure time with an exponential distribution since the
memoryless property is plausible in this case. However, if the failure is due to wearout, then
the memoryless property is not plausible and the exponential distribution would not be appro-
priate to model the failure times. A further discussion of modeling failure times is presented
in Section 17.2, and Example 76 discusses how the exponential distribution is often used to
model the time until an electrical discharge occurs.
4.2.3
The Poisson Process
A stochastic process can be thought of as being a series of random events. In particular,
a simple stochastic process may consist of a sequence of events occurring over time, in
which case it can be deﬁned by specifying the distributions of the time intervals between the
occurrences of adjacent events. A Poisson process (with parameter λ) is one such process
where these time intervals are independent random variables having exponential distributions
with parameter λ, as shown in Figure 4.6.
A Poisson process may, for example, be used to model the arrival of calls at a switchboard,
the addition of new elements to a queue, or the positions of deformities within a substance.
Some examples of Poisson processes are discussed in this section. In all cases, the modeling
FIGURE 4.6
A Poisson process. The time
intervals between events have
independent exponential
distributions with parameter λ
X1
X2
X3
X4
X5
Events
Time
Independent exponential random variables
f(xi) = λe− λxi

194
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.7
A Poisson process. The number of
events occurring in a time interval
of length t has a Poisson
distribution with mean λt
Time
t
Number of events ~ Poisson (λt)
is based upon the assumption that the “distances” or times between events are independently
distributed with identical exponential distributions.
The expected waiting time between two events in a Poisson process is 1/λ because it is
simply the expected value of an exponential distribution with parameter λ. Furthermore, the
expected number of events occurring within a ﬁxed time interval of length t is λt. Moreover,
the number of events occurring within such a time interval has a Poisson distribution with
mean λt. In other words, if the random variable X counts the number of events occurring
within a ﬁxed time interval of length t, then
X ∼P(λt)
as illustrated in Figure 4.7. This is why these processes are called Poisson processes.
4.2.4
Examples of the Exponential Distribution
Example 31
Shipwreck Hunts
A team of underwater salvage experts sets sail to search the ocean ﬂoor for the wreckage of a
ship that is thought to have sunk within a certain area. Their boat is equipped with underwater
sonar with which they hope to detect unusual objects lying on the ocean ﬂoor.
The captain’s experience is that in similar situations it has taken an average of 20 days to
locate a wreck. Consequently, the captain surmises that the time in days taken to locate the
wreck can be modeled by an exponential distribution with parameter
λ =
1
E(X) = 1
20 = 0.05
The captain considers the memoryless property of the exponential distribution to be suitable
since, with such vast areas of the ocean ﬂoor to be searched, the unfruitful searching of certain
areas does not appreciably alter the chance of ﬁnding the wreck in the future.
The captain’s contractors have offered a sizeable bonus if it is possible to reduce searching
costs by locating the wreck within the ﬁrst week. The captain estimates the probability of this
to be
P(X ≤7) = F(7) = 1 −e−0.05×7 = 0.30
On the other hand, the captain is only authorized to search for at most 4 weeks before calling
off the search. The probability that the captain has to call off the search without success is

4.2 THE EXPONENTIAL DISTRIBUTION
195
FIGURE 4.8
Probability density function for
shipwreck hunt
20
40
60
28
Probability of bonus = 0.30
Probability search called off = 0.25
7
Days
f(x) = 0.05 e− 0.05 x
Time taken to locate wreck
estimated to be
P(X ≥28) = 1 −F(28) = e−0.05×28 = 0.25
These probabilities are illustrated in Figure 4.8.
Example 32
Steel Girder Fractures
An engineer examines the edges of steel girders for hairline fractures. The girders are 10 m
long, and it is discovered that they have an average of 42 fractures each. If a girder has
42 fractures, then there are 43 “gaps” between fractures or between the ends of the girder
and the adjacent fractures. The average length of these gaps is therefore 10/43 = 0.23 m.
The fractures appear to be randomly spaced on the girders, so the engineer proposes that the
location of fractures on a particular girder can be modeled by a Poisson process with
λ =
1
0.23 = 4.3
According to this model, the length of a gap between any two adjacent fractures has an
exponential distribution with λ = 4.3, as illustrated in Figure 4.9. In this case, the probability
that a gap is less than 10 cm long is
P(X ≤0.10) = F(0.10) = 1 −e−4.3×0.10 = 0.35
The probability that a gap is longer than 30 cm is
P(X ≥0.30) = 1 −F(0.30) = e−4.3×0.30 = 0.28

196
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.9
Poisson process modeling fracture
locations on a steel girder
X1
X7
X6
X5
X4
X3
X2
Fractures
Independent exponential random variables
with λ = 4.3
FIGURE 4.10
The number of fractures in a 25-cm
segment of the steel girder has
a Poisson distribution with
mean 1.075
25 cm
Number of fractures ~ Poisson (1.075)
If a 25-cm segment of a girder is selected, the number of fractures it contains has a Poisson
distribution with mean
λ × 0.25 = 4.3 × 0.25 = 1.075
as illustrated in Figure 4.10. The probability that the segment contains at least two fractures
is therefore
P(X ≥2) = 1 −P(X = 0) −P(X = 1)
= 1 −e−1.075 × 1.0750
0!
−e−1.075 × 1.0751
1!
= 1 −0.341 −0.367 = 0.292
Example 9
Car Body Assembly
Line
The engineer in charge of the car panel manufacturing process pays particular attention to the
arrival of metal sheets at the beginning of the panel construction lines. These metal sheets are
brought one by one from other parts of the factory ﬂoor, where they have been cut into the
required sizes. On average, about 96 metal sheets are delivered to the panel construction lines
in 1 hour.

4.2 THE EXPONENTIAL DISTRIBUTION
197
  Car panel
construction
      lines
Times between deliveries
of metal sheets are 
independently distributed 
exponential random 
variables with λ = 1.6
FIGURE 4.11
Poisson process modeling arrival times
of metal sheets to car panel construction lines
x
7
8
9
10
11
12
13
14
15
16
0.0000
0.0002
0.0004
0.0011
0.0025
0.0054
0.0107
0.0198
0.0344
0.0563
P(x    Ä x)
X ∼ Poisson (24.0)
x
P(x    Ä x)
X ∼ Poisson (24.0)
17
18
19
20
21
22
23
24
25
26
0.0871
0.1283
0.1803
0.2426
0.3139
0.3917
0.4728
0.5540
0.6319
0.7038
27
28
29
30
31
32
33
34
35
36
0.7677
0.8225
0.8679
0.9042
0.9322
0.9533
0.9686
0.9794
0.9868
0.9918
37
38
39
40
41
42
43
44
45
0.9950
0.9970
0.9983
0.9990
0.9995
0.9997
0.9998
0.9999
1.0000
FIGURE 4.12
The cumulative distribution function of a
Poisson random variable with mean 24.0
The engineer decides to model the arrival of the metal sheets with a Poisson process. The
average waiting time between arrivals is 60/96 = 0.625 minute, so a value of
λ =
1
0.625 = 1.6
is used. This model assumes that the waiting times between arrivals of metal sheets are
independently distributed as exponential distributions with λ = 1.6, as shown in Figure 4.11.
For example, the probability that there is a wait of more than 3 minutes between arrivals is
P(X ≥3) = 1 −F(3) = e−1.6×3 = 0.008
The number of metal sheets arriving at the panel construction lines during a speciﬁc
15-minute period has a Poisson distribution with mean
λ × 15 = 1.6 × 15 = 24.0
Figure 4.12 shows the cumulative distribution function of this Poisson distribution. The prob-
ability that no more than 16 sheets arrive during the 15-minute period, for example, is about

198
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
0.056. On the other hand, the engineer can be about 95% conﬁdent that no more than 32 metal
sheets will arrive during the period under consideration.
4.2.5
Problems
4.2.1 Use integration by parts to show that if X has an
exponential distribution with parameter λ, then
(a) E(X) = 1/λ
(b) E(X 2) = 2/λ2
4.2.2 Suppose that you are waiting for a friend to call you and
that the time you wait in minutes has an exponential
distribution with parameter λ = 0.1.
(a) What is the expectation of your waiting time?
(b) What is the probability that you will wait longer than
10 minutes?
(c) What is the probability that you will wait less than
5 minutes?
(d) Suppose that after 5 minutes you are still waiting for
the call. What is the distribution of your additional
waiting time? In this case, what is the probability that
your total waiting time is longer than 15 minutes?
(e) Suppose now that the time you wait in minutes for
the call has a U(0, 20) distribution. What is the
expectation of your waiting time? If after 5 minutes
you are still waiting for the call, what is the
distribution of your additional waiting time?
4.2.3 The time in days between breakdowns of a machine is
exponentially distributed with λ = 0.2.
(a) What is the expected time between machine
breakdowns?
(b) What is the standard deviation of the time between
machine breakdowns?
(c) What is the median time between machine
breakdowns?
(d) What is the probability that after the machine is
repaired it lasts at least a week before failing
again?
(e) If the machine has performed satisfactorily for six
days, what is the probability that it lasts at least two
more days before breaking down?
4.2.4 A researcher plants 12 seeds whose germination times in
days are independent exponential distributions with
λ = 0.31.
(a) What is the probability that a given seed germinates
within ﬁve days?
(b) What are the expectation and variance of the number
of seeds germinating within ﬁve days?
(c) What is the probability that no more than nine seeds
have germinated within ﬁve days?
4.2.5 A double exponential distribution, often called a Laplace
distribution, has a probability density function
f (x) = 1
2 λ e−λ|x−θ|
for −∞≤x ≤∞, depending on two parameters λ and θ.
Sketch the probability density function and cumulative
distribution function of this distribution. What is the
expectation of the distribution? If λ = 3 and θ = 2,
calculate:
(a) P(X ≤0)
(b) P(X ≥1)
4.2.6 Imperfections in an optical ﬁber are distributed according
to a Poisson process such that the distance between
imperfections in meters has an exponential distribution
with parameter λ = 2m−1.
(a) What is the expected distance between imperfections?
(b) What is the probability that the distance between two
imperfections is longer than 1 meter?
(c) What is the distribution of the number of
imperfections in a 3-meter stretch of ﬁber?
(d) What is the probability that a 3-meter stretch of ﬁber
has no more than four imperfections?
(This problem is continued in Problem 4.3.5.)
4.2.7 The arrival times of workers at a factory ﬁrst-aid room
satisfy a Poisson process with an average of 1.8 per
hour.
(a) What is the value of the parameter λ of the Poisson
process?
(b) What is the expectation of the time between two
arrivals at the ﬁrst-aid room?
(c) What is the probability that there is at least 1 hour
between two arrivals at the ﬁrst-aid room?
(d) What is the distribution of the number of workers
visiting the ﬁrst-aid room during a 4-hour
period?
(e) What is the probability that at least four workers
visit the ﬁrst-aid room during a 4-hour period?
(This problem is continued in Problem 4.3.6.)

4.3 THE GAMMA DISTRIBUTION
199
4.2.8 Engineers observe that about 90% of graphite samples
fracture within 5 hours when subjected to a certain stress.
(a) If the time to fracture is modeled with an exponential
distribution, what would be a suitable value for the
parameter λ?
(b) Use the model to estimate the probability that a
fracture occurs within 3 hours.
4.2.9 Consider a Poisson process with parameter λ = 0.8.
(a) What is the probability that the time between two
adjacent events is longer than 1.5?
(b) What is the probability that there will be at least three
events in a period of length 2?
4.2.10 The lengths of telephone calls can be modeled by an
exponential distribution with parameter λ = 0.3 per
minute, with the call lengths being independent. What is
the probability that out of ten telephone calls, two will be
shorter than 1 minute, four will last between 1 minute
and 3 minutes, and the other four will last longer than
3 minutes?
4.2.11 Customers arrive at a service window according to
a Poisson process with parameter λ = 0.2 per minute.
(a) What is the probability that the time between two
successive arrivals is less than 6 minutes?
(b) What is the probability that there will be exactly three
arrivals during a given 10-minute period?
4.2.12 Suppose that components have failure times that are
independent and that can be modeled with an
exponential distribution with λ = 0.0065 per day. If
a box contains ten components, what is the probability
that the box has at least eight components that last longer
than 150 days?
4.2.13 As a metal detector is passed over the ground, signals are
received according to a Poisson process with λ = 0.022
per meter. What is the probability that there is no more
than one signal in a 100-meter stretch?
4.2.14 Ninety identical electrical circuits are monitored at an
extreme temperature to see how long they last before
failing. The 50th failure occurs after 263 minutes. If
the failure times are modeled with an exponential
distribution, when would you predict that the 80th failure
will occur?
4.3
The Gamma Distribution
4.3.1
Deﬁnition of the Gamma Distribution
The gamma distribution has many important applications in areas such as reliability theory,
and it is also used in the analysis of a Poisson process. It has a state space x ≥0 and a
probability density function
f (x) = λk xk−1 e−λx
(k)
for x ≥0 and f (x) = 0 for x < 0, which depends upon two parameters k > 0 and λ > 0.
The function (k) is known as the gamma function. It provides the correct scaling to ensure
that the total area under the probability density function is equal to 1.
The Gamma Function
The gamma function is deﬁned to be
(k) =
 ∞
0
xk−1e−x dx
Some special cases are (1) = 1 and (1/2) = √π, and in general,
(k) = (k −1) (k −1)

200
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
The Gamma Function, continued
for k > 1. If n is a positive integer, then
(n) = (n −1)!
but except for these special cases there is in general no closed-form expression for the
gamma function.
Notice that if k = 1, the gamma distribution simpliﬁes to the exponential distribution with
parameter λ. The expectation and variance of a gamma distribution are given in the following
box.
The Gamma Distribution
A gamma distribution with parameters k > 0 and λ > 0 has a probability density
function
f (x) = λk xk−1 e−λx
(k)
for x ≥0 and f (x) = 0 for x < 0, where (k) is the gamma function. It has an
expectation and variance of
E(X) = k
λ
and
Var(X) = k
λ2
The parameter k is often referred to as the shape parameter of the gamma distribution, and
λ is referred to as the scale parameter. Figure 4.13 shows the probability density functions
of gamma distributions with λ = 1 and k = 1, 3, and 5. As the shape parameter increases,
the peak of the density function is seen to move farther to the right. Figure 4.14 shows the
probability density functions of gamma distributions with λ = 1, 2, and 3 and k = 3. This
illustrates how the parameter λ “scales” the distribution function.
One important property of a random variable that has a gamma distribution with an integer
value of the parameter k is that it can be obtained as the sum of a set of independent exponential
random variables. Speciﬁcally, if X1, . . . , Xk are independent random variables each having
an exponential distribution with parameter λ, then the random variable
X = X1 + · · · + Xk
has a gamma distribution with parameters k and λ. This fact implies that for a Poisson process
with parameter λ, the time taken for k events to occur has a gamma distribution with parameters
k and λ, since the time taken until the ﬁrst event occurs, and the times between subsequent
events, each have independent exponential distributions with parameter λ.
COMPUTER NOTE
The probability values of gamma distributions are generally intractable and are best obtained
from software packages. However, when you do this it is important to ensure that you know
what parameterization the package is using so that you deﬁne the distribution properly. In
particular, many packages use parameters k and 1/λ instead of k and λ.

4.3 THE GAMMA DISTRIBUTION
201
FIGURE 4.13
Probability density functions of
gamma distributions
0.5
1.0
2.5
5.0
7.5
10.0
f(x)
x
λ = 1, k = 3
λ = 1, k = 5
λ = 1, k = 1
FIGURE 4.14
Probability density functions of
gamma distributions
0.5
1.0
2.5
5.0
7.5
10.0
f(x)
x
λ = 2, k = 3
λ = 1, k = 3
λ = 3, k = 3

202
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.15
Distance to ﬁfth fracture has a
gamma distribution with
parameters k = 5 and λ = 4.3
X
X ~ Gamma   k = 5, λ = 4.3
4.3.2
Examples of the Gamma Distribution
Example 32
Steel Girder Fractures
Suppose that the random variable X measures the length between one end of a girder and the
ﬁfth fracture along the girder, as shown in Figure 4.15. If the fracture locations are modeled
by a Poisson process as discussed previously, X has a gamma distribution with parameters
k = 5 and λ = 4.3. The expected distance to the ﬁfth fracture is therefore
E(X) = k
λ = 5
4.3 = 1.16 m
A software package can be used to show that the 0.05 quantile point of this distribution is
x = 0.458 m, so that
F(0.458) = 0.05
Consequently, the engineer can be 95% sure that the ﬁfth fracture is at least 46 cm away from
the end of the girder. A software package can also be used to calculate the probability that the
ﬁfth fracture is within 1 m of the end of the girder, which is
F(1) = 0.4296
It is interesting to note that this latter probability can also be obtained using the Poisson
distribution. The number of fractures within a 1-m section of the girder has a Poisson distri-
bution with mean
λ × 1 = 4.3
The probability that the ﬁfth fracture is within 1 m of the end of the girder is the probability
that there are at least ﬁve fractures within the ﬁrst 1-m section, which is therefore
P(Y ≥5) = 0.4296
where Y ∼P(4.3).
Example 9
Car Body Assembly
Line
Suppose that the engineer in charge of the car panel manufacturing process is interested in
how long it will take for 20 metal sheets to be delivered to the panel construction lines. Under
the Poisson process model, this time X has a gamma distribution with parameters k = 20 and
λ = 1.6. The expected waiting time is consequently
E(X) = k
λ = 20
1.6 = 12.5 minutes
The variance of the waiting time is
Var(X) = k
λ2 = 20
1.62 = 7.81
so that the standard deviation is σ =
√
7.81 = 2.80 minutes, as illustrated in Figure 4.16.

4.3 THE GAMMA DISTRIBUTION
203
FIGURE 4.16
Probability density function of the
time taken to deliver 20 metal
sheets to the car panel construction
lines
5
10
15
20
f(x)
k = 20, λ = 1.6
Gamma distribution
Minutes
E(X) = 12.5 min
Time taken to deliver 20 metal sheets
σ = 2.80 min σ = 2.80 min
A software package can be used to show that for this distribution,
F(17.42) = 0.95
and
F(15) = 0.8197
The engineer can therefore be 95% conﬁdent that 20 metal sheets will have arrived within
18 minutes, say. Furthermore, there is a probability of about 0.82 that they will all arrive
within 15 minutes. This latter probability can also be obtained from the probabilities of a
Poisson distribution with mean 24.0, which is shown in Figure 4.12. This Poisson distri-
bution is the distribution of the number of sheets arriving during a 15-minute period, and
1−0.1803 = 0.8197 is seen to be the probability that at least 20 sheets arrive during this time
interval.
4.3.3
Problems
4.3.1 Use integration by parts to show that
(k) = (k −1) (k −1)
for k > 1. Use the fact that (0.5) = √π to evaluate
(5.5).
4.3.2 Recall that if X1, . . . , Xk are independent random
variables each having an exponential distribution with
parameter λ, then the random variable
X = X1 + · · · + Xk
has a gamma distribution with parameters k and λ.
(a) Use this fact to verify the expectation and variance of
a gamma distribution. Check that you get the same
answer for the expectation of a gamma random
variable using the formula
E(X) =
 ∞
0
x f (x) dx
where f (x) is the probability density function of the
gamma distribution.
(b) If X has a gamma distribution with parameters k1 and
λ, and Y has a gamma distribution with parameters k2

204
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
and λ, where k1 and k2 are both positive integers and X
and Y are independent random variables, explain why
Z = X + Y has a gamma distribution with parameters
k1+ k2 and λ.
4.3.3 Use a computer package to ﬁnd both the probability
density function and cumulative distribution function at
x = 3 and the median of gamma distributions with the
following parameter values:
(a) k = 3.2, λ = 0.8
(b) k = 7.5, λ = 5.3
(c) k = 4.0, λ = 1.4
In part (c), check the value of the probability density
function from its formula.
4.3.4 A day’s sales in $1000 units at a gas station have a gamma
distribution with parameters k = 5 and λ = 0.9.
(a) What is the expectation of a day’s sales?
(b) What is the standard deviation of a day’s sales?
(c) What are the upper and lower quartiles of a day’s sales?
(d) What is the probability that a day’s sales are more than
$6000?
(This problem is continued in Problem 5.3.9.)
4.3.5 Recall Problem 4.2.6 concerning imperfections in an
optical ﬁber. Suppose that ﬁve adjacent imperfections are
located on a ﬁber.
(a) What is the distribution of the distance between the
ﬁrst imperfection and the ﬁfth imperfection?
(b) What is the expectation of the distance between the
ﬁrst imperfection and the ﬁfth imperfection?
(c) What is the standard deviation of the distance between
the ﬁrst imperfection and the ﬁfth imperfection?
(d) Consider the probability that the distance between the
ﬁrst imperfection and the ﬁfth imperfection is longer
than 3 meters. Show how this probability can be
obtained using the gamma distribution and also by
using the Poisson distribution.
4.3.6 Recall Problem 4.2.7 concerning the arrivals at a factory
ﬁrst-aid room.
(a) What is the distribution of the time between the ﬁrst
arrival of the day and the fourth arrival?
(b) What is the expectation of this time?
(c) What is the variance of this time?
(d) By using (i) the gamma distribution and (ii) the
Poisson distribution, show how to calculate the
probability that this time is longer than 3 hours.
4.3.7 Suppose that the time in minutes taken by a worker on an
assembly line to complete a particular task has a gamma
distribution with parameters k = 44 and λ = 0.7.
(a) What are the expectation and standard deviation of the
time taken to complete the task?
(b) Use a software package to ﬁnd the probability that the
task is completed within an hour.
4.4
The Weibull Distribution
4.4.1
Deﬁnition of the Weibull Distribution
The Weibull distribution is often used to model failure and waiting times. It has a state space
x ≥0 and a probability density function
f (x) = a λa xa−1 e−(λx)a
for x ≥0 and f (x) = 0 for x < 0, which depends upon two parameters a > 0 and λ > 0.
Notice that taking a = 1 gives the exponential distribution as a special case.
The cumulative distribution function of a Weibull distribution is
F(x) =
 x
0
a λa ya−1 e−(λy)a dy = 1 −e−(λx)a
for x ≥0. The expectation and variance of a Weibull distribution depend upon the gamma
function and are given in the following box.

4.4 THE WEIBULL DISTRIBUTION
205
The Weibull Distribution
A Weibull distribution with parameters a > 0 and λ > 0 has a probability density
function
f (x) = a λa xa−1 e−(λx)a
for x ≥0 and f (x) = 0 for x < 0, and a cumulative distribution function
F(x) = 1 −e−(λx)a
for x ≥0. It has an expectation
E(X) = 1
λ 

1 + 1
a

and a variance
Var(X) = 1
λ2



1 + 2
a

−

1 + 1
a
2 
HISTORICAL NOTE
Ernst Hjalmar Waloddi Weibull
(1887–1979) was a Swedish
engineer and scientist. He was
a member of the Swedish Coast
Guard for many years before he
obtained his doctorate from the
University of Uppsala in 1932.
His paper on the Weibull
distribution was published in
1939, and he subsequently
conducted a lot of research and
published many papers in
mechanical engineering. He
presented a paper on the
Weibull distribution to the
American Society of
Mechanical Engineers in 1951,
and later was awarded a gold
medal by that society. Actually,
the Weibull distribution had
been identiﬁed by Maurice
Fréchet in 1927 prior to
Weibull’s work.
As with the gamma distribution, λ is called the scale parameter of the distribution, and a is
called the shape parameter. A useful property of the Weibull distribution is that the probability
density function can exhibit a wide variety of forms, depending on the choice of the two
parameters. Figure 4.17 illustrates some probability density functions with λ = 1 and various
values of the shape parameter a. Figure 4.18 illustrates some probability density functions
with a = 3 and with various values of λ.
FIGURE 4.17
Probability density functions of the
Weibull distribution
0.5
1.0
0.5
1.0
1.5
2.0
f(x)
x
λ = 1, a = 3
λ = 1, a = 2
λ = 1, a = 0.5
1.5

206
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.18
Probability density functions of the
Weibull distribution
1.0
2.0
0.5
1.0
1.5
2.0
x
λ = 1, a = 3
λ = 0.8, a = 3
λ = 2, a = 3
f(x)
Notice that the pth quantile of the Weibull distribution is easily calculated to be
(−ln(1 −p))1/a
λ
COMPUTER NOTE
The probability values of a Weibull distribution are easy to calculate because of the simple
form of the cumulative distribution function. However, they should also be available on your
computer package. As with the exponential and gamma distributions, make sure that you check
on the parameterization used by your package which may be a and 1/λ instead of a and λ.
4.4.2
Examples of the Weibull Distribution
Example 33
Bacteria Lifetimes
Suppose that the random variable X measures the lifetime of a bacterium at a certain high
temperature, and that it has a Weibull distribution with a = 2 and λ = 0.1. This distribution
is illustrated in Figure 4.19.
The expected survival time of a bacterium is
E(X) =
1
0.1 × 

1 + 1
2

= 10 × 1
2 × 
1
2

= 10 × 1
2 × √π = 8.86 minutes
The variance of the bacteria lifetimes is
Var(X) =
1
0.12 ×



1 + 2
2

−

1 + 1
2
2 
= 100 ×

1 −
√π
2
2 
= 21.46
so that the standard deviation is σ =
√
21.46 = 4.63 minutes.

4.4 THE WEIBULL DISTRIBUTION
207
FIGURE 4.19
Distribution of bacteria survival
times
5
10
15
20
f(x)
Minutes
Survival time of a bacterium
E(X) = 8.86 min
Weibull distribution
a = 2.0, λ = 0.1
σ = 4.63 min σ = 4.63 min
The probability that a bacterium dies within 5 minutes is
P(X ≤5) = F(5) = 1 −e−(0.1×5)2 = 0.22
and the probability that a bacterium lives longer than 15 minutes is
P(X ≥15) = 1 −F(15) = e−(0.1×15)2 = 0.11
Notice that if F(x) = 0.95, then
0.95 = 1 −e−(0.1×x)2
which can be solved to give x = 17.31 minutes. Consequently, within a large group of bacteria,
it will take about 17.3 minutes for 95% of the bacteria to die.
Example 34
Car Brake Pad Wear
A brake pad made from a new compound is tested in cars that are driven in city trafﬁc. The
random variable X, which measures the mileage in 1000-mile units that the cars can be driven
before the brake pads wear out, has a Weibull distribution with parameters a = 3.5 and
λ = 0.12. This distribution is shown in Figure 4.20.
The median car mileage is the value x satisfying
0.5 = F(x) = 1 −e−(0.12×x)3.5
which can be solved to give x = 7.50. Consequently, it should be expected that about half of
the brake pads will last longer than 7500 miles. The probability that a set of brake pads last
longer than 10,000 miles is
P(X ≥10) = 1 −F(10) = e−(0.12×10)3.5 = 0.15.

208
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.20
Distribution of brake pad mileage
2500
5000
7500
10,000
f(x)
Miles
Weibull distribution
a = 3.5, λ = 0.12
12,500
Probability mileage more
 than 10,000 = 0.15
Median = 7500 miles
Brake pad mileage
4.4.3
Problems
4.4.1 Use the deﬁnition of the gamma function to derive the
expectation and variance of a Weibull distribution.
4.4.2 Suppose that the random variable X has a Weibull
distribution with parameters a = 4.9 and λ = 0.22. Find:
(a) The median of the distribution
(b) The upper and lower quartiles of the distribution
(c) P(2 ≤X ≤7)
4.4.3 Suppose that the random variable X has a Weibull
distribution with parameters a = 2.3 and λ = 1.7. Find:
(a) The median of the distribution
(b) The upper and lower quartiles of the distribution
(c) P(0.5 ≤X ≤1.5)
4.4.4 The time to failure in hours of an electrical circuit
subjected to a high temperature has a Weibull distribution
with parameters a = 3 and λ = 0.5.
(a) What is the median failure time of a circuit?
(b) The circuit engineers can be 99% conﬁdent that a
circuit will last as long as what time?
(c) What are the expectation and variance of the circuit
failure times?
(d) If a circuit has three equivalent backup circuits that
have independent failure times, what is the probability
that at least one circuit is working after 3 hours?
4.4.5 A biologist models the time in minutes between the
formation of a cell and the moment at which it splits into
two new cells using a Weibull distribution with parameters
a = 0.4 and λ = 0.5.
(a) What is the median value of this distribution?
(b) What are the upper and lower quartiles of this
distribution?
(c) What are the 95th and 99th percentiles of this
distribution?
(d) What is the probability that the cell “lifetime” is
between 3 and 5 minutes?
4.4.6 The lifetime in minutes of a mechanical component has a
Weibull distribution with parameters a = 1.5 and λ = 0.03.
(a) What are the median, upper quartile, and 99th
percentile of the lifetime of a component?
(b) If 500 independent components are considered, what
are the expectation and variance of the number of
components still operating after 30 minutes?

4.5 THE BETA DISTRIBUTION
209
4.4.7 Suppose that the time in days taken for bacteria cultures to
develop after they have been prepared can be modeled by a
Weibull distribution with parameters λ = 0.3 and a = 0.6.
A biologist prepares several sets of cultures at the same
time, and after four days opens them one by one until ﬁve
developed cultures have been found. What is the
probability that the biologist opens exactly ten cultures?
4.4.8 A physician conducts a study to investigate the time taken
to recover from an ailment under a certain treatment. A
group of 82 patients with the ailment are given the
treatment, and when they are checked 7 days later, it is
found that 9 of them have recovered. The remaining
73 patients are checked 14 days after receiving the
treatment, and an additional 15 of them are found to have
recovered. If the time to recovery is modeled with a
Weibull distribution, estimate the median time to
recovery.
4.5
The Beta Distribution
4.5.1
Deﬁnition of the Beta Distribution
The beta distribution has a state space 0 ≤x ≤1 and is often used to model proportions.
The Beta Distribution
A beta distribution with parameters a > 0 and b > 0 has a probability density
function
f (x) = (a + b)
(a)(b) xa−1 (1 −x)b−1
for 0 ≤x ≤1 and f (x) = 0 elsewhere. It is useful for modeling proportions. Its
expectation and variance are
E(X) =
a
a + b
and
Var(X) =
ab
(a + b)2(a + b + 1)
Figure 4.21 illustrates the probability density functions of beta distributions with a = b =
0.5 and a = b = 2. While their shapes are quite different, they are both symmetric about
x = 0.5. In fact, all beta distributions with a = b are symmetric. Figure 4.22 illustrates the
probability density functions of beta distributions with a = 0.5, b = 2 and with a = 4, b = 2.
COMPUTER NOTE
Unless the parameters a and b take integer values, the cumulative distribution function of the
beta distribution is generally intractable so that a software package is essential to calculate the
probability values of beta distributions. As before, check on the parameterization that your
package employs.
4.5.2
Examples of the Beta Distribution
Example 35
Stock Prices
A Wall Street analyst has built a model for the performance of the stock market. In this model
the proportion of listed stocks showing an increase in value on a particular day has a beta
distribution with parameter values a and b, which depend upon various economic and political
factors. On each day the analyst predicts suitable values of the parameters for modeling the
subsequent day’s stock prices. Suppose that on Monday the analyst predicts that parameter
values a = 5.5 and b = 4.2 are suitable for the next day. What does this indicate about stock
prices on Tuesday?

210
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.21
Probability density functions of the
beta distribution
0.25
0.50
1.00
f(x)
0.75
0.5
1.0
1.5
a = b = 2
a = b = 0.5
x
0.00
FIGURE 4.22
Probability density functions of the
beta distribution
0.25
0.50
1.00
f(x)
0.75
1.5
3.0
a = 4, b = 2
x
0.00
a = 0.5, b = 2

4.5 THE BETA DISTRIBUTION
211
FIGURE 4.23
Distribution of the proportion of
stocks that increase in value
0.00
0.25
0.50
0.75
1.00
E(X) = 0.57
Beta distribution
a = 5.5,  b = 4.2
f(x)
Proportion of stocks increasing in value
σ = 0.15 σ = 0.15
x
The distribution of the proportion of stocks increasing in value on Tuesday is shown in
Figure 4.23. The expected proportion of stocks increasing in value on Tuesday is
E(X) =
5.5
5.5 + 4.2 = 0.57
The variance in the proportion is
Var(X) =
5.5 × 4.2
(5.5 + 4.2)2 × (5.5 + 4.2 + 1) = 0.0229
so that the standard deviation is σ =
√
0.0229 = 0.15. A software package can be used to
calculate the probability that more than 75% of the stocks increase in value as
P(X ≥0.75) = 1 −F(0.75) = 1 −0.881 = 0.119
Example 36
Bee Colonies
When a queen bee leaves a bee colony to start a new hive, a certain proportion of the worker
bees take ﬂight and follow her. An entomologist models the proportion X of the worker bees
that leave with the queen using a beta distribution with parameters a = 2.0 and b = 4.8. This
distribution is illustrated in Figure 4.24.
The expected proportion of bees leaving is
E(X) =
2.0
2.0 + 4.8 = 0.29
The variance in the proportion is
Var(X) =
2.0 × 4.8
(2.0 + 4.8)2 × (2.0 + 4.8 + 1) = 0.0266
so that the standard deviation is σ =
√
0.0266 = 0.16. The probability that more than half of
the bee colony leaves with the queen can be calculated from a software package to be
P(X ≥0.5) = 1 −F(0.5) = 1 −0.878 = 0.122

212
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.24
Distribution of the proportion of
worker bees leaving with queen
0.25
f(x)
E (X) = 0.29
Beta distribution
a = 2.0, b = 4.8
0.00
0.50
0.75
1.00
σ = 0.16
σ = 0.16
Proportion of worker bees that leave the hive
x
4.5.3
Problems
4.5.1 Consider the probability density function
f (x) = Ax3 (1 −x)2
for 0 ≤x ≤1 and f (x) = 0 elsewhere.
(a) Find the value of A by direct integration.
(b) Find by direct integration the expectation and variance
of this distribution.
(c) What are the parameter values a and b of a beta
distribution for which this is the probability density
function? Check your answers to part B using the
general formulas for beta distributions.
4.5.2 Consider the beta probability density function
f (x) = Ax9 (1 −x)3
for 0 ≤x ≤1 and f (x) = 0 elsewhere.
(a) What are the values of the parameters a and b?
(b) Use the answer to part A to calculate the value of A.
(c) What is the expectation of this distribution?
(d) What is the standard deviation of this distribution?
(e) Calculate the cumulative distribution function of this
distribution.
4.5.3 Use a computer package to ﬁnd the probability density
function and cumulative distribution function at x = 0.5,
and the upper quartile, of beta distributions with the
following parameter values:
(a) a = 3.3, b = 4.5
(b) a = 0.6, b = 1.5
(c) a = 2, b = 6
In part C, check the value of the probability density
function using the general formula.
4.5.4 Suppose that the random variable X has a beta distribution
with parameters a = b = 2.1, and consider the random
variable
Y = 3 + 4X
(a) What is the state space of the random variable Y?
(b) What are the expectation and variance of the random
variable Y?
(c) What is P(Y ≤5)?
4.5.5 The purity of a chemical batch, expressed as a percentage,
is equal to 100X, where the random variable X has a beta
distribution with parameters a = 7.2 and b = 2.3.
(a) What are the expectation and variance of the purity
levels?
(b) What is the probability that a chemical batch has a
purity of at least 90%?

4.7
CASE STUDY: INTERNET MARKETING
213
4.5.6 The proportion of tin in a metal alloy has a beta
distribution with parameters a = 8.2 and b = 11.7.
(a) What is the expected proportion of tin in the alloy?
(b) What is the standard deviation of the proportion of tin
in the alloy?
(c) What is the median proportion of tin in the alloy?
4.6
Case Study: Microelectronic Solder Joints
A Weibull distribution can be used to model the number of temperature cycles that an assembly
can be subjected to before it fails. In this case, experience dictates that it is best to deﬁne the
cumulative distribution function of the failure time distribution in terms of the logarithm of
the number of cycles, so that
P(assembly fails within t cycles) = P(X ≤t) = 1 −e−(λ ln(t))a
The values of the parameters a and λ will depend upon the speciﬁc design of the assembly.
Suppose that if an epoxy of type I is used for the underﬁll, then a = 25.31 and λ = 0.120,
whereas if an epoxy of type II is used, then a = 27.42 and λ = 0.116. The solution of
P(X ≤t) = 1 −e−(0.120 ln(t))25.31 = 0.01
is t = 1041, and
P(X ≤t) = 1 −e−(0.120 ln(t))25.31 = 0.5
is solved with t = 3691. Consequently, if epoxy of type I is used for the underﬁll, then 99% of
the assemblies can survive 1041 temperature cycles, whereas half of them can survive 3691
temperature cycles. In addition, the solution of
P(X ≤t) = 1 −e−(0.116 ln(t))27.42 = 0.01
is t = 1464 and
P(X ≤t) = 1 −e−(0.116 ln(t))27.42 = 0.5
is solved with t = 4945, so that if epoxy of type II is used for the underﬁll, then 99% of
the assemblies can survive 1464 temperature cycles, whereas half of them can survive 4945
temperature cycles. These calculations reveal that an underﬁll with epoxy of type II produces
an assembly with better reliability.
4.7
Case Study: Internet Marketing
When a individual has logged on to the organisation’s website, the length of the idle periods
in minutes is distributed as a gamma distribution with k = 1.1 and λ = 0.9. Consequently, the
idle periods have an expectation of k/λ = 1.1/0.9 = 1.22 minutes, and the standard deviation
is
√
k/λ =
√
1.1/0.9 = 1.17 minutes.
Suppose that the individual is automatically logged out when the idle period reaches
5 minutes. What proportion of the idle periods result in the individual being automatically
logged out?
This can be calculated as
P(Gamma(k = 1.1, λ = 0.9) ≥5) = 1 −P(Gamma(k = 1.1, λ = 0.9) ≤5)
= 1 −0.986 = 0.014
so that the proportion is 1.4%.

214
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
4.8
Supplementary Problems
4.8.1 A dial is spun and an angle θ is measured, which can be
taken to be uniformly distributed between 0◦and 360◦. If
0 ≤θ ≤90, a player wins nothing; if 90 ≤θ ≤270, then
a player wins $(2θ −180); and if 270 ≤θ ≤360, then a
player wins $(θ2 −72,540). Draw the cumulative
distribution function of the player’s winnings.
4.8.2 A commercial bleach eventually becomes ineffective
because the chlorine in it becomes attached to other
molecules. The company that manufactures the bleach
estimates that the median time for this to happen is about
one and a half years.
(a) If an exponential distribution is used to model
the time taken for a sample of bleach to become
ineffective, what is a suitable value for the
parameter λ?
(b) Estimate the probability that a sample of bleach is
still effective after 2 years, and the probability that a
sample of bleach becomes ineffective within 1 year.
4.8.3 A ship navigating through the southern regions of the
North Atlantic ice ﬂoes encounters icebergs according to
a Poisson process. The distances between icebergs in
nautical miles are exponentially distributed with a
parameter λ = 0.7.
(a) What is the expected distance between iceberg
encounters?
(b) What is the probability that there is a distance of at
least 3 nautical miles between iceberg
encounters?
(c) What is the median distance between icebergs?
(d) What is the distribution of the number of icebergs
encountered in a stretch of 10 nautical miles?
(e) What is the probability that at least ﬁve icebergs are
encountered in a 10-nautical-mile stretch?
(f) What is the distribution of the distance traveled by
the ship before encountering ten icebergs? What are
the expectation and variance of this distance?
4.8.4 Calls arriving at a switchboard follow a Poisson process
with parameter λ = 5.2 per minute.
(a) What is the expected waiting time between the
arrivals of two calls?
(b) What is the probability that the waiting time between
the arrivals of two calls is less than 10 seconds?
(c) What is the distribution of the time taken for ten calls
to arrive at the switchboard?
(d) What is the expectation of the time taken for ten calls
to arrive at the switchboard?
(e) What is the probability that more than ﬁve calls arrive
at the switchboard during a 1-minute period?
4.8.5 Figure 4.25 shows the probability density function of a
triangle distribution T (a, b) with endpoints a and b.
(a) What is the height of the probability density function
at (a + b)/2?
(b) If the random variable X has a T (a, b) distribution,
what is P(X ≤a/4 + 3b/4)?
(c) What is the variance of a T (a, b) distribution?
(d) Calculate the cumulative distribution function of a
T (a, b) distribution.
4.8.6 The fermentation time in weeks required by a brewery for
a particular kind of beer has a Weibull distribution with
parameters a = 4 and λ = 0.2.
(a) What are the median, upper quartile, and 95th
percentile of the fermentation times?
(b) What are the expectation and variance of the
fermentation times?
(c) What is the probability that a batch requires a
fermentation time between 5 and 6 weeks?
FIGURE 4.25
The probability density function of
a triangle distribution, T (a, b),
with endpoints a and b
a
b
x
f(x)
a + b
2

4.8 SUPPLEMENTARY PROBLEMS
215
4.8.7 The proportion of a day that a tiger spends hunting for
food has a beta distribution with parameters a = 2.7 and
b = 2.9.
(a) What is the expected amount of time per day that the
tiger spends hunting for food?
(b) What is the standard deviation of the amount of time
per day that the tiger spends hunting for food?
(c) What is the probability that on a particular day the
tiger spends more than half the day hunting for
food?
4.8.8 The starting time of a class is uniformly distributed
between 10:00 and 10:05. If a student arrives early and
has to wait t minutes for the class to start, then the student
incurs a penalty of A1t, which accounts for the waste in
the student’s time. On the other hand, if the student
arrives t minutes after the class has started, then the
student incurs a penalty of A2t, which accounts for the
information the student has missed. If the student arrives
at x minutes after 10:00, what is the expected penalty
incurred by the student? What value of x minimizes the
expected penalty?
4.8.9 An herbalist ﬁnds that about 25% of plants sprout within 35
days, and that about 75% of plants sprout within 65 days.
(a) If the time of sprouting is modeled with a Weibull
distribution, what parameter values would be
appropriate?
(b) Use this model to estimate the time by which 90% of
plants sprout.
4.8.10 The strength of a chemical solution is measured on a
scale between 0 and 1, with values smaller than 0.5 being
too weak, values between 0.5 and 0.8 being satisfactory,
and values larger than 0.8 being too strong. If chemical
batches have strengths that are independently distributed
according to a beta distribution with parameters a = 18
and b = 11, what is the probability that if ten batches are
produced, exactly one batch will be weak, one batch
will be strong, and the other eight batches will all be
satisfactory?
(This problem is continued in Problem 5.3.11.)
4.8.11 Suppose that visits to a website can be modeled by a
Poisson process with parameter λ = 4 per hour.
(a) What is the probability that there are exactly ten
visits within a given 2-hour interval?
(b) A supervisor starts to monitor the website from the
start of a new shift. What is the distribution of the
time waited by the supervisor until the tenth visit to
the website during that shift?
4.8.12 A hole is drilled into the Antarctic ice shelf and a core is
extracted that provides information on the climate when
the ice was formed at different times in the past. Suppose
that a researcher is interested in high-temperature years,
and that the places in the core corresponding to
high-temperature years occur according to a Poisson
process with parameter λ = 0.48 per cm.
(a) What is the expected distance in cm between adjacent
high-temperature years?
(b) What is the expected distance in cm between one
high-temperature year and the tenth high-temperature
year that followed after it?
(c) What is the probability that the distance between two
adjacent high-temperature years is less that 0.5 cm?
(d) Suppose that a 20-cm section of core is analyzed.
What is the probability that the number of
high-temperature years in this section of core is
between 8 and 12 inclusive?
4.8.13 Are the following statements true or false?
(a) If a Beta distribution has the parameter a larger
than the parameter b, then its expectation is smaller
than 1/2.
(b) The uniform distribution is a symmetric distribution.
(c) In a Poisson process the distances between events are
identically distributed.
(d) The exponential distribution is a special case of the
Weibull distribution.
4.8.14 Suppose that after operation, the electrical charge
remaining on a circuit component has a uniform
distribution between 50 and 100, and that these charges
are independent of each other for different operations.
If the machine is operated ﬁve times, what is the
probability that the residual charge is between 50 and 70
exactly two times, between 70 and 90 exactly two times,
and between 90 and 100 exactly one time?
4.8.15 Consider a Poisson process with parameter λ = 8.
(a) Consider an interval of length 0.5. What is the
probability of obtaining exactly four events within
this interval?
(b) What is the probability that the interval between two
adjacent events is shorter than 0.2?
4.8.16 Suppose that customer waiting times are independent and
can be modeled by a Weibull distribution with a = 2.3
and λ = 0.09 per minute. What is the probability that out
of ten customers, exactly three wait less than 8 minutes,
exactly four wait between 8 and 12 minutes, and exactly
three wait more than 12 minutes?

C H A P T E R F I V E
The Normal Distribution
In this chapter the normal or Gaussian distribution is discussed. It is the most important of
all continuous probability distributions and is used extensively as the basis for many statistical
inference methods. Its importance stems from the fact that it is a natural probability distribution
for directly modeling error distributions and many other naturally occurring phenomena. In
addition, by virtue of the central limit theorem, which is discussed in Section 5.3, the normal
distribution provides a useful, simple, and accurate approximation to the distribution of general
sample averages.
5.1
Probability Calculations Using the Normal Distribution
5.1.1
Deﬁnition of the Normal Distribution
The Normal Distribution
The normal or Gaussian distribution has a probability density function
f (x) =
1
σ
√
2π
e−(x−μ)2/2σ 2
for −∞≤x ≤∞, depending upon two parameters, the mean and the variance
E(X) = μ
and
Var(X) = σ 2
of the distribution. The probability density function is a bell-shaped curve that is
symmetric about μ. The notation
X ∼N(μ, σ 2)
denotes that the random variable X has a normal distribution with mean μ and
variance σ 2. In addition, the random variable X can be referred to as being “normally
distributed.”
HISTORICAL NOTE
Carl Friedrich Gauss
(1777–1855) ranks as one of
the greatest mathematicians
of all time. He studied
mathematics at the University
of G¨ottingen, Germany,
between 1795 and 1798 and
later in 1807 became professor
of astronomy at the same
university, where he remained
until his death. His work on
the normal distribution was
performed around 1820. He is
reported to have been deeply
religious, aristocratic, and
conservative. He did not enjoy
teaching and consequently had
only a few students.
The probability density function of a normal random variable is symmetric about the mean
value μ and has what is known as a “bell-shaped” curve. Figure 5.1 shows the probability
density functions of normal distributions with μ = 5, σ = 2 and with μ = 10, σ = 2 and
illustrates the fact that as the mean value μ is changed, the shape of the density function
remains unaltered while the location of the density function changes. On the other hand,
Figure 5.2 shows the probability density functions of normal distributions with μ = 5, σ = 2
and with μ = 5, σ = 0.5. The central location of the density function has not changed, but
the shape has. Large values of the variance σ 2 result in long, ﬂat bell-shaped curves, whereas
small values of the variance σ 2 result in thinner, sharper bell-shaped curves.
216

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
217
FIGURE 5.1
The effect of changing the mean of
a normal distribution
5
10
N(10, 4)
μ = 10
μ = 5
N(5, 4)
FIGURE 5.2
The effect of changing the variance
of a normal distribution
π = 5
5
N(5, 0.25)
N(5, 4)
COMPUTER NOTE
There is no simple closed-form solution for the cumulative distribution function of a normal
distribution. Nevertheless, in this section it is shown how probability values of normal random
variables are easily calculated from tables of the standard normal distribution. In addition, it
is worthwhile to discover how your computer software package can be used to obtain such
values.
5.1.2
The Standard Normal Distribution
A normal distribution with mean μ = 0 and variance σ 2 = 1 is known as the standard
normal distribution. Its probability density function has the notation φ(x) and is given by
φ(x) =
1
√
2π
e−x2/2
for −∞≤x ≤∞, as illustrated in Figure 5.3.
The notation (x) is used for the cumulative distribution function of a standard normal
distribution, which is calculated from the expression
(x) =
 x
−∞
φ(y) dy
as illustrated in Figure 5.4. The cumulative distribution function is shown in Figure 5.5 and is
often referred to as an “S-shaped” curve. Notice that (0) = 0.5 because the standard normal
distribution is symmetric about x = 0, and that the cumulative distribution function (x)
approaches 1 as x tends to ∞and approaches 0 as x tends to −∞.

218
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.3
The standard normal distribution
−3
−2
−1
0
1
2
3
 = 1
 = 1
π = 0 
N(0, 1)
x
f (x) =
1
√
2
e x2/2
FIGURE 5.4
(x) is the cumulative distribution
function of a standard normal
distribution
x
Φ(x)
N(0, 1)
FIGURE 5.5
The cumulative distribution
function of a standard normal
distribution
−3
−2
−1
0
1
2
3
x
1.0
0.5
Φ(x)

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
219
The symmetry of the standard normal distribution about 0 implies that if the random
variable Z has a standard normal distribution, then
1 −(x) = P(Z ≥x) = P(Z ≤−x) = (−x)
as illustrated in Figure 5.6. This equation can be rearranged to provide the easily remembered
relationship
(x) + (−x) = 1
The cumulative distribution function of the standard normal distribution (x) is tabulated
in Table I at the end of the book. This table provides values of (x) to four decimal places
for values of x between −3.49 and 3.49. For values of x less than −3.49, (x) is very close
to 0, and for values of x greater than 3.49, (x) is very close to 1.
As an example of the use of Table I, suppose that the random variable Z has a standard
normal distribution. Table I then indicates that
P(Z ≤0.31) = (0.31) = 0.6217
as illustrated in Figure 5.7. Table I also reveals that
P(Z ≥1.05) = 1 −(1.05) = 1 −0.8531 = 0.1469
as illustrated in Figure 5.8, and that
P(−1.50 ≤Z ≤1.18) = (1.18) −(−1.50) = 0.8810 −0.0668 = 0.8142
as illustrated in Figure 5.9.
FIGURE 5.6
(−x) = 1 −(x)
0
x
N(0, 1)
−x
Φ(−x)
1 − Φ(x)
FIGURE 5.7
Probability calculations for a
standard normal distribution
0
N(0, 1)
Φ(0.31) = 0.6217
0.31

220
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.8
Probability calculations for a
standard normal distribution
0
N(0, 1)
1 −Φ(1.05) = 0.1469
1.05
FIGURE 5.9
Probability calculations for a
standard normal distribution
0
N(0, 1)
Φ(1.18) − Φ(−1.50) = 0.8142
1.18
−1.50
Table I can also be used to ﬁnd percentiles of the standard normal distribution. For example,
the 80th percentile satisﬁes
(x) = 0.8
and can be found by using the table “backward” by searching for the value 0.8 in the body
of the table. It is found that (0.84) = 0.7995 and that (0.85) = 0.8023, so that the
80th percentile point is somewhere between 0.84 and 0.85. (If further accuracy is required,
interpolation may be attempted or a computer software package may be utilized.) If the value
x is required for which
P(|Z| ≤x) = 0.7
as illustrated in Figure 5.10, notice that the symmetry of the standard normal distribution
implies that
(−x) = 0.15
Table I then indicates that the required value of x lies between 1.03 and 1.04.
The percentiles of the standard normal distribution are used so frequently that they have
their own notation. For α < 0.5, the (1 −α)× 100th percentile of the distribution is denoted
by zα, so that
(zα) = 1 −α
as illustrated in Figure 5.11, and some of these percentile points are given in Table I. The
percentiles zα are often referred to as the “critical points” of the standard normal distribution.

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
221
FIGURE 5.10
Symmetric tails of the normal
distribution
−x
0
x
N(0, 1)
Φ(−x) = 0.15
1 − Φ(x) = 0.15
P(| Z | ≤ x) = 0.70
FIGURE 5.11
The critical points zα of the
standard normal distribution
N(0, 1)
1 − Φ(z  ) = α
 α   
Φ(z   ) = 1 − α
  α   
  α
z
FIGURE 5.12
The critical points zα/2 of the
standard normal distribution
N(0, 1)
P( |Z| ≤        ) = 1 − α
α/2
z
α/2
z
α/2
z
−
Notice that the symmetry of the standard normal distribution implies that if Z ∼N(0, 1),
then
P(|Z| ≤zα/2) = P(−zα/2 ≤Z ≤zα/2)
= (zα/2) −(−zα/2) = (1 −α/2) −α/2 = 1 −α
as illustrated in Figure 5.12.

222
CHAPTER 5
THE NORMAL DISTRIBUTION
5.1.3
Probability Calculations for General Normal Distributions
A very important general result is that if
X ∼N(μ, σ 2)
then the transformed random variable
Z = X −μ
σ
has a standard normal distribution. This result indicates that any normal distribution can be
related to the standard normal distribution by appropriate scaling and location changes. Notice
that the transformation operates by ﬁrst subtracting the mean value μ and by then dividing
by the standard deviation σ. The random variable Z is known as the “standardized” version
of the random variable X.
A consequence of this result is that the probability values of any normal distribution can
be related to the probability values of a standard normal distribution and, in particular, to the
cumulative distribution function (x). For example,
P(a ≤X ≤b) = P
a −μ
σ
≤X −μ
σ
≤b −μ
σ

= P
a −μ
σ
≤Z ≤b −μ
σ

= 
b −μ
σ

−
a −μ
σ

as illustrated in Figure 5.13. In this way Table I can be used to calculate probability values for
any normal distribution.
FIGURE 5.13
Relating normal probabilities to
(x)
a
μ
b
0
P(a ≤X ≤b) = P
a −μ
σ
≤Z ≤b −μ
σ

= 
b −μ
σ

−
a −μ
σ

X ∼N(μ, σ2)
Z ∼N(0, 1)
a −μ
σ
b −μ
σ

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
223
Probability Calculations for Normal Distributions
If X ∼N(μ, σ 2), then
Z = X −μ
σ
∼N(0, 1)
The random variable Z is known as the “standardized” version of the random variable
X. This result implies that the probability values of a general normal distribution can
be related to the cumulative distribution function of the standard normal distribution
(x) through the relationship
P(a ≤X ≤b) = 
b −μ
σ

−
a −μ
σ

As an illustration of this result, suppose that X ∼N(3, 4). Then
P(X ≤6) = P(−∞≤X ≤6) = 
6 −3
2

−
−∞−3
2

= (1.5) −(−∞) = 0.9332 −0 = 0.9332
as illustrated in Figure 5.14, and
P(2.0 ≤X ≤5.4) = 
5.4 −3.0
2.0

−
2.0 −3.0
2.0

= (1.2) −(−0.5) = 0.8849 −0.3085 = 0.5764
as illustrated in Figure 5.15.
FIGURE 5.14
Using the standard normal
distribution to calculate the
probabilities of a normal
distribution
3
6
0
1.5
P(X ≤6) = (1.5)
X ∼N(3, 4)
Z ∼N(0, 1)

224
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.15
Using the standard normal
distribution to calculate the
probabilities of a normal
distribution
2.0
3.0
5.4
P(2.0 ≤  X ≤ 5.4) = (1.2) − (−0.5)
0.0
1.2
X ∼N(3, 4)
Z ∼N(0, 1)
 0.5
−
In general, if X ∼N(μ, σ 2), notice that
P(μ −cσ ≤X ≤μ + cσ) = P(−c ≤Z ≤c)
where Z ∼N(0, 1). Table I reveals that when c = 1 this probability is about 68%, when
c = 2 this probability is about 95%, and when c = 3 this probability is about 99.7%, as shown
in Figure 5.16. These calculations can be summarized in the following general rules.
Normal Random Variables
■
There is a probability of about 68% that a normal random variable takes a
value within one standard deviation of its mean.
■
There is a probability of about 95% that a normal random variable takes a
value within two standard deviations of its mean.
■
There is a probability of about 99.7% that a normal random variable takes a
value within three standard deviations of its mean.
The percentiles of a N(μ, σ 2) distribution are related to the percentiles of a standard
normal distribution through the relationship
P(X ≤μ + σzα) = P(Z ≤zα) = 1 −α
For example, since the 95th percentile of the standard normal distribution is z0.05 = 1.645,
the 95th percentile of a N(3, 4) distribution is
μ + σz0.05 = 3 + (2 × z0.05) = 3 + (2 × 1.645) = 6.29

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
225
FIGURE 5.16
The probability values of lying
within one, two, and three standard
deviations of the mean of a normal
distribution
0
−1
1
−3
−2
3
2
Z ∼N(0, 1)
P(|Z| ≤1) ≃0.68
0
−1
1
−3
−2
3
2
P(|Z| ≤2) ≃0.95
Z ∼N(0, 1)
0
−1
1
−3
−2
3
2
P(|Z| ≤3) ≃0.997
Z ∼N(0, 1)
5.1.4
Examples of the Normal Distribution
Example 18
Tomato Plant Heights
Recall that three weeks after planting, the heights of tomato plants have a mean of 29.4 cm
and a standard deviation of 2.1 cm. Chebyshev inequality was used to show that there is at
least a 75% chance that a tomato plant has a height within two standard deviations of the
mean, that is, within the interval [25.2, 33.6].
However, if the tomato plant heights are taken to be normally distributed, then this proba-
bility can be calculated much more precisely. In fact, the probability is about 95%. Moreover,
there is a probability of about 99.7% that a tomato plant has a height within three standard
deviations of the mean, that is, within the interval [23.1, 35.7].
More generally, there is a probability of 1 −α that a tomato plant has a height within the
interval
[μ −σzα/2, μ + σzα/2] = [29.4 −2.1zα/2, 29.4 + 2.1zα/2]

226
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.17
Probability density function of
tomato plant heights
29.4
29.0
30.0
Tomato plant heights (cm)
X ∼N(29.4, 2.12)
P(29.0 ≤X ≤30.0) = (0.29) −(−0.19) = 0.1894
Since z0.05 = 1.645, there is therefore a 90% chance that a tomato plant has a height within
the interval
[29.4 −(2.1 × 1.645), 29.4 + (2.1 × 1.645)] = [25.95, 32.85]
Consequently, the researcher can predict that about 9 out of 10 tomato plants will have a height
between 26 cm and 33 cm three weeks after planting.
The probability that a tomato plant height is between 29 cm and 30 cm can be calculated
to be
P(29.0 ≤X ≤30.0) = 
30.0 −μ
σ

−
29.0 −μ
σ

= 
30.0 −29.4
2.1

−
29.0 −29.4
2.1

= (0.29) −(−0.19) = 0.6141 −0.4247 = 0.1894
as illustrated in Figure 5.17.
Example 37
Concrete Block
Weights
A company manufactures concrete blocks that are used for construction purposes. Suppose
that the weights of the individual concrete blocks are normally distributed with a mean value
of μ = 11.0 kg and a standard deviation of σ = 0.3 kg.
Since z0.005 = 2.576 the company can be 99% conﬁdent that a randomly selected concrete
block has a weight within the interval
[μ −σz0.005, μ + σz0.005] = [11.0 −(0.3 × 2.576), 11.0 + (0.3 × 2.576)]
= [10.23, 11.77]
The probability that a concrete block weighs less than 10.5 kg is
P(X ≤10.5) = P(−∞≤X ≤10.5) = 
10.5 −μ
σ

−
−∞−μ
σ

= 
10.5 −11.0
0.3

−
−∞−11.0
0.3

= (−1.67) −(−∞) = 0.0475 −0 = 0.0475
as illustrated in Figure 5.18. Consequently, only about 1 in 20 concrete blocks weighs less
than 10.5 kg.

5.1 PROBABILITY CALCULATIONS USING THE NORMAL DISTRIBUTION
227
FIGURE 5.18
Probability density function of
concrete block weights
P(X ≤ 10.5) = (−1.67) 
                     = 0.0475
11.0
Concrete block weights (kg)
10.5
X ∼N(11.0, 0.32)
FIGURE 5.19
Probability density function of
annual return from stock of
company A
8.0
5.0
10.0
P(X  ≥ 10.0) = 1 − (1.33) 
                      = 0.0918
Stock “excellent”
X ∼N(8.0, 1.52)
( 2.
−
P(X ≤5.0) = 
0)
Annual return (%)
Stock “unsatisfactory”
= 0.0228 
Example 35
Stock Prices
A Wall Street analyst estimates that the annual return from the stock of company A can be
considered to be an observation from a normal distribution with mean μ = 8.0% and standard
deviation σ = 1.5%. The analyst’s investment choices are based upon the considerations that
any return greater than 5% is “satisfactory” and a return greater than 10% is “excellent.”
The probability that company A’s stock will prove to be “unsatisfactory” is
P(X ≤5.0) = P(−∞≤X ≤5.0)
= 
5.0 −μ
σ

−
−∞−μ
σ

= 
5.0 −8.0
1.5

−
−∞−8.0
1.5

= (−2.00) −(−∞) = 0.0228 −0 = 0.0228
and the probability that company A’s stock will prove to be “excellent” is
P(10.0 ≤X) = P(10.0 ≤X ≤∞)
= 
∞−μ
σ

−
10.0 −μ
σ

= 
∞−8.0
1.5

−
10.0 −8.0
1.5

= (∞) −(1.33) = 1 −0.9082 = 0.0918
These probabilities are illustrated in Figure 5.19.

228
CHAPTER 5
THE NORMAL DISTRIBUTION
5.1.5
Problems
5.1.1 Suppose that Z ∼N(0, 1). Find:
(a) P(Z ≤1.34)
(b) P(Z ≥−0.22)
(c) P(−2.19 ≤Z ≤0.43)
(d) P(0.09 ≤Z ≤1.76)
(e) P(|Z| ≤0.38)
(f) The value of x for which P(Z ≤x) = 0.55
(g) The value of x for which P(Z ≥x) = 0.72
(h) The value of x for which P(|Z| ≤x) = 0.31
5.1.2 Suppose that Z ∼N(0, 1). Find:
(a) P(Z ≤−0.77)
(b) P(Z ≥0.32)
(c) P(−3.09 ≤Z ≤−1.59)
(d) P(−0.82 ≤Z ≤1.80)
(e) P(|Z| ≥0.91)
(f) The value of x for which P(Z ≤x) = 0.23
(g) The value of x for which P(Z ≥x) = 0.51
(h) The value of x for which P(|Z| ≥x) = 0.42
5.1.3 Suppose that X ∼N(10, 2). Find:
(a) P(X ≤10.34)
(b) P(X ≥11.98)
(c) P(7.67 ≤X ≤9.90)
(d) P(10.88 ≤X ≤13.22)
(e) P(|X −10| ≤3)
(f) The value of x for which P(X ≤x) = 0.81
(g) The value of x for which P(X ≥x) = 0.04
(h) The value of x for which P(|X −10| ≥x) = 0.63
5.1.4 Suppose that X ∼N(−7, 14). Find:
(a) P(X ≤0)
(b) P(X ≥−10)
(c) P(−15 ≤X ≤−1)
(d) P(−5 ≤X ≤2)
(e) P(|X + 7| ≥8)
(f) The value of x for which P(X ≤x) = 0.75
(g) The value of x for which P(X ≥x) = 0.27
(h) The value of x for which P(|X + 7| ≤x) = 0.44
5.1.5 Suppose that X ∼N(μ, σ2) and that
P(X ≤5) = 0.8
and
P(X ≥0) = 0.6
What are the values of μ and σ 2?
5.1.6 Suppose that X ∼N(μ, σ 2) and that
P(X ≤10) = 0.55
and
P(X ≤0) = 0.40
What are the values of μ and σ 2?
5.1.7 Suppose that X ∼N(μ, σ 2). Show that
P(X ≤μ + σzα) = 1 −α
and that
P(μ −σzα/2 ≤X ≤μ + σzα/2) = 1 −α
5.1.8 What are the upper and lower quartiles of a N(0, 1)
distribution? What is the interquartile range? What is the
interquartile range of a N(μ, σ 2) distribution?
5.1.9 The thicknesses of glass sheets produced by a certain
process are normally distributed with a mean of μ =
3.00 mm and a standard deviation of σ = 0.12 mm.
(a) What is the probability that a glass sheet is thicker
than 3.2 mm?
(b) What is the probability that a glass sheet is thinner
than 2.7 mm?
(c) What is the value of c for which there is a 99%
probability that a glass sheet has a thickness within
the interval [3.00 −c, 3.00 + c]?
(This problem is continued in Problem 5.2.8.)
5.1.10 The amount of sugar contained in 1-kg packets is actually
normally distributed with a mean of μ = 1.03 kg and a
standard deviation of σ = 0.014 kg.
(a) What proportion of sugar packets are underweight?
(b) If an alternative package-ﬁlling machine is used for
which the weights of the packets are normally
distributed with a mean of μ = 1.05 kg and a
standard deviation of σ = 0.016 kg, does this result
in an increase or a decrease in the proportion of
underweight packets?
(c) In each case, what is the expected value of the excess
package weight above the advertised level of 1 kg?
(This problem is continued in Problem 5.2.9.)
5.1.11 The thicknesses of metal plates made by a particular
machine are normally distributed with a mean of 4.3 mm
and a standard deviation of 0.12 mm.
(a) What are the upper and lower quartiles of the metal
plate thicknesses?
(b) What is the value of c for which there is 80%
probability that a metal plate has a thickness within
the interval [4.3 −c, 4.3 + c]?
(This problem is continued in Problem 5.2.4.)
5.1.12 The density of a chemical solution is normally distributed
with mean 0.0046 and variance 9.6 × 10−8.

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
229
(a) What is the probability that the density is less than
0.005?
(b) What is the probability that the density is between
0.004 and 0.005?
(c) What is the 10th percentile of the density level?
(d) What is the 99th percentile of the density level?
5.1.13 The resistance in milliohms of 1 meter of copper cable at
a certain temperature is normally distributed with mean
μ = 23.8 and variance σ 2 = 1.28.
(a) What is the probability that a 1-meter segment of
copper cable has a resistance less than 23.0?
(b) What is the probability that a 1-meter segment of
copper cable has a resistance greater than 24.0?
(c) What is the probability that a 1-meter segment of
copper cable has a resistance between 24.2 and 24.5?
(d) What is the upper quartile of the resistance level?
(e) What is the 95th percentile of the resistance level?
5.1.14 The weights of bags ﬁlled by a machine are normally
distributed with a standard deviation of 0.05 kg and a
mean that can be set by the operator. At what level should
the mean be set if it is required that only 1% of the bags
weigh less than 10 kg?
5.1.15 Suppose a certain mechanical component produced by a
company has a width that is normally distributed with a
mean μ = 2600 and a standard deviation σ = 0.6.
(a) What proportion of the components have a width
outside the range 2599 to 2601?
(b) If the company needs to be able to guarantee to its
purchaser that no more than 1 in 1000 of the
components have a width outside the range 2599 to
2601, by how much does the value of σ need to be
reduced?
(This problem is continued in Problem 5.2.10.)
5.1.16 Bricks have weights that are independently distributed
with a normal distribution that has a mean 1320 and a
standard deviation of 15. A set of ten bricks is chosen at
random. What is the probability that exactly three bricks
will weigh less than 1300, exactly four bricks will weigh
between 1300 and 1330, and exactly three bricks will
weigh more than 1330?
5.1.17 Manufactured items have a strength that has a normal
distribution with a standard deviation of 4.2. The mean
strength can be altered by the operator. At what value
should the mean strength be set so that exactly 95% of
the items have a strength less than 100?
5.1.18 An investment in company A has an expected return of
$30,000 with a standard deviation of $4000. What is the
probability that the return will be at least $25,000 if it has
a normal distribution?
5.2
Linear Combinations of Normal Random Variables
5.2.1
The Distribution of Linear Combinations of Normal Random Variables
In this section an attractive feature of the normal distribution is discussed, which is that
linear combinations of normal random variables are also normally distributed. The means
and variances of these linear combinations can be found from the general results presented in
Section 2.6.
In the simplest case, Section 2.6.1 provides general results for the mean and variance of
a linear function
Y = aX + b
of a random variable X. If the random variable X has a normal distribution, then an additional
point is that the linear function Y also has a normal distribution. This result is summarized in
the following box and is illustrated in Figure 5.20.
Linear Functions of a Normal Random Variable
If X ∼N(μ, σ 2) and a and b are constants, then
Y = aX + b ∼N(aμ + b, a2σ 2)

230
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.20
A linear function of a normal
random variable
X
X ∼N(μX, σ 2 )
Y
Y ∼N(μY , σ 2 )
μX
σX
σX
Y = aX + b
μY = aμX + b
σY =
σX
a| |
σY =
σX
a| |
Notice that if a = 1/σ and b = −μ/σ, the resulting linear function of X has a standard
normal distribution, as discussed in Section 5.1.3.
Section 2.6.2 provides some general results for the mean and variance of a linear combi-
nation of random variables. An additional point now is that a linear combination of normally
distributed random variables is also normally distributed. In the simple case involving the
summation of two independent random variables, the following result is obtained, which is
illustrated in Figure 5.21.
The Sum of Two Independent Normal Random Variables
If X1 ∼N(μ1, σ 2
1 ) and X2 ∼N(μ2, σ 2
2 ) are independent random variables, then
Y = X1 + X2 ∼N

μ1 + μ2, σ 2
1 + σ 2
2

It is also worth noting that if two normal random variables are not independent, then their sum
is still normally distributed, but the variance of the sum depends on the covariance of the two
random variables.
The two results presented so far can be synthesized into the following general result.

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
231
σ1
σ1
μ1
μ2
σ2
σ2
X1
N(μ1, σ 2
1 )
(
    )
X2
N μ2, σ 2
2
∼
∼
Y ∼N(μ1 + μ2, σ 2
1 + σ 2
2 )

σ 2
1 + σ 2
2

σ 2
1 + σ 2
2
μ1 + μ2
Y = X 1 + X 2
FIGURE 5.21
The sum of two independent normal random variables
Linear Combinations of Independent Normal Random Variables
If Xi ∼N(μi, σ 2
i ), 1 ≤i ≤n, are independent random variables and if ai, 1 ≤i ≤n,
and b are constants, then
Y = a1X1 + · · · + an Xn + b ∼N(μ, σ 2)
where
μ = a1μ1 + · · · + anμn + b
and
σ 2 = a2
1σ 2
1 + · · · + a2
nσ 2
n
A special case of this result concerns the situation in which interest is directed toward the
average ¯X of a set of independent identically distributed N(μ, σ 2) random variables. With
b = 0, ai = 1/n, μi = μ, and σ 2
i = σ 2, the result above implies that ¯X is normally distributed

232
CHAPTER 5
THE NORMAL DISTRIBUTION
with
E( ¯X) = μ
and
Var( ¯X) = σ 2
n
Averaging Independent Normal Random Variables
If Xi ∼N(μ, σ 2), 1 ≤i ≤n, are independent random variables, then their average ¯X
is distributed
¯X ∼N

μ, σ 2
n

Notice that averaging reduces the variance to σ 2/n, so that the average ¯X has a tendency
to be closer to the mean value μ than do the individual random variables Xi. This tendency in-
creases as n increases and the average of more and more random variables Xi is taken.
As an illustration of this idea, recall that there is a probability of about 68% that a normal
random variable takes a value within one standard deviation of its mean, so that
P(μ −σ ≤Xi ≤μ + σ) = 0.68
If n = 10 so that an average of ten of these random variables is taken, then
¯X ∼N

μ, σ 2
10

and
P(μ −σ ≤¯X ≤μ + σ) = 

σ

σ 2/10
	
−

−
σ

σ 2/10
	
= (3.16) −(−3.16) = 0.9992 −0.0008 = 0.9984
In other words, while there is only a 68% chance that a N(μ, σ 2) random variable lies within
the interval [μ −σ, μ + σ], the average of ten independent random variables of this kind has
more than a 99.8% chance of taking a value within this interval.
5.2.2
Examples of Linear Combinations of Normal Random Variables
Example 23
Piston Head
Construction
Recall that the radius of a piston head X1 has a mean value of 30.00 mm and a standard
deviation of 0.05 mm, and that the inside radius of a cylinder X2 has a mean value of 30.25
mm and a standard deviation of 0.06 mm. The gap between the piston head and the cylinder
Y = X2 −X1 therefore has a mean value of
μ = 30.25 −30.00 = 0.25
and a variance of
σ 2 = 0.052 + 0.062 = 0.0061
If the piston head radius and the cylinder radius are taken to be normally distributed, then
the gap Y is also normally distributed since it is obtained as a linear combination of the normal
random variables X1 and X2. Speciﬁcally,
Y ∼N(0.25, 0.0061)

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
233
30.25
30.00
Piston head radius (mm)
Cylinder radius (mm)
1
X ∼N(30.00, 0.052)
2
X ∼N(30.25, 0.062)
Y = X 2 −X 1
P(Y ≤ 0) = Φ(−3.20) = 0.0007
Y ∼ N(0.25, 0.0061)
Gap (mm)
0.25
0.35
0.10
0.00
P(0.10 ≤Y ≤0.35) = (1.28) −(−1.92)
= 0.8723
Optimal performance
FIGURE 5.22
Probability density functions for piston head construction
The probability that a piston head will not ﬁt within a cylinder can then be calculated to be
P(Y ≤0) = 
 0 −0.25
√
0.0061

= (−3.20) = 0.0007
as illustrated in Figure 5.22.
SupposethatapistonperformsoptimallywhenthegapY isbetween0.10mmand0.35mm.
The probability that a piston performs optimally is then
P(0.10 ≤Y ≤0.35) = 
0.35 −0.25
√
0.0061

−
0.10 −0.25
√
0.0061

= (1.28) −(−1.92) = 0.8997 −0.0274 = 0.8723
Example 18
Tomato Plant Heights
Suppose that 20 tomato plants are planted. What is the distribution of the average tomato
plant height after three weeks of growth?
The distribution of the individual tomato plant heights is
Xi ∼N(29.4, 2.12) = N(29.4, 4.41)

234
CHAPTER 5
THE NORMAL DISTRIBUTION
Consequently, the average of 20 of these heights is distributed
¯X ∼N

29.4, 4.41
20

= N(29.4, 0.2205)
Since z0.025 = 1.96, there is therefore a 95% chance that the average tomato plant height lies
within the interval
[29.4 −(1.96 ×
√
0.2205), 29.4 + (1.96 ×
√
0.2205)] = [28.48, 30.32]
Notice that the standard deviation of the average tomato plant height σ =
√
0.2205 = 0.47 cm
is considerably smaller than the standard deviation of the individual tomato plant heights,
which is σ = 2.10 cm.
Example 37
Concrete Block
Weights
Suppose that a wall is constructed from 24 concrete blocks as illustrated in Figure 5.23. What
is the distribution of the total weight of the wall?
The weights of the individual concrete blocks Xi, 1 ≤i ≤24, are distributed
Xi ∼N(11.0, 0.32) = N(11.0, 0.09)
Consequently, the weight of the wall Y is distributed
Y = X1 + · · · + X24 ∼N(μ, σ 2)
where
μ = 11.0 + · · · + 11.0 = 264.0
FIGURE 5.23
Distribution of the total weight of
the wall
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
N(11.0, 0.09)
μ  =  11.0 +      + 11.0 = 264.0
     =  0.09 +      + 0.09 = 2.16
Total weight of wall is N(264.0, 2.16).
…
…
σ 2

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
235
and
σ 2 = 0.09 + · · · + 0.09 = 2.16
Thus the wall has an expected weight of 264.0 kg with a standard deviation of
√
2.16 =
1.47 kg.
There is about a 99.7% chance that the wall has a weight within three standard deviations
of its mean value, that is, within the interval
[264.0 −(3 × 1.47), 264.0 + (3 × 1.47)] = [259.59, 268.41]
Consequently, the builders can be conﬁdent that the wall weighs somewhere between 259 kg
and 269 kg.
Example 35
Stock Prices
Recall that the annual return from the stock of company A, X A say, is distributed
X A ∼N(8.0, 1.52) = N(8.0, 2.25)
In addition, suppose that the annual return from the stock of company B, X B say, is distributed
X B ∼N(9.5, 4.00)
independent of the stock of company A.
The probability that company B’s stock proves to be “unsatisfactory” is
P(X B ≤5.0) = 
5.0 −9.5
2.0

= (−2.25) = 0.0122
and the probability that company B’s stock proves to be “excellent” is
P(10.0 ≤X) = 1 −
10.0 −9.5
2.0

= 1 −(0.25) = 1 −0.5987 = 0.4013
These probabilities are illustrated in Figure 5.24.
FIGURE 5.24
Probability density function of
annual return from stock of
company B
P(X ≥ 10.0) = 1 − Φ(0.25) 
 
    = 0.4013
Annual return (%)
9.5  10.0
5.0
X ∼ N(9.5, 4.0)
Stock “unsatisfactory”
Stock “excellent”
P(X ≤ 5.0) = Φ(−2.25) 
                   = 0.0122

236
CHAPTER 5
THE NORMAL DISTRIBUTION
What is the probability that company B’s stock performs better than company A’s stock?
If Y = X B −X A, then
Y ∼N(9.5 −8.0, 4.00 + 2.25) = N(1.5, 6.25)
The required probability is therefore
P(Y ≥0) = 1 −
0 −1.5
√
6.25

= 1 −(−0.6) = 1 −0.2743 = 0.7257
The probability that company B’s stock performs at least two percentage points better than
company A’s stock is
P(Y ≥2.0) = 1 −
2.0 −1.5
√
6.25

= 1 −(0.2) = 1 −0.5793 = 0.4207
Example 38
Chemical
Concentration Levels
A chemist has two different methods for measuring the concentration level C of a chemical
solution. Method A produces a measurement X A that is distributed
X A ∼N(C, 2.97)
so that the chemist can be 99.7% certain that the measured value lies within the interval
[C −(3 ×
√
2.97), C + (3 ×
√
2.97)] = [C −5.17, C + 5.17]
Method B involves a different kind of analysis and produces a measurement X B that is
distributed
X B ∼N(C, 1.62)
In this case the chemist can be 99.7% certain that the measured value lies within the interval
[C −(3 ×
√
1.62), C + (3 ×
√
1.62)] = [C −3.82, C + 3.82]
The variability in method B is smaller than the variability in method A, and so the chemist
correctly feels that the measurement reading xB obtained from method B is more “accurate”
than the measurement reading xA obtained from method A. Should the measurement reading
xA therefore be completely ignored?
In fact, the most sensible course for the chemist to take is to combine the two measurement
values xA and xB into one value
y = pxA + (1 −p)xB
for a suitable value of p between 0 and 1. The ﬁnal measurement value y is thus a weighted
average of the two measurement values xA and xB and has a distribution
Y = pX A + (1 −p)X B ∼N

μY, σ 2
Y

where
μY = pE(X A) + (1 −p)E(X B) = pC + (1 −p)C = C
and
σ 2
Y = p2Var(X A) + (1 −p)2Var(X B) = p22.97 + (1 −p)21.62
It is sensible to choose p in a way that minimizes the variance σ 2
Y. The derivative of σ 2
Y
with respect to p is
dσ 2
Y
dp = 5.94p −3.24(1 −p)

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
237
Setting this expression equal to 0 gives an “optimal” value of p = 0.35, in which case
σ 2
Y = (0.352 × 2.97) + (0.652 × 1.62) = 1.05
Notice that this variance is smaller than the variances of both method A and method B, as
illustrated in Figure 5.25.
In conclusion, the chemist’s best estimate of the concentration level is
y = 0.35xA + 0.65xB
and the chemist can be 99.7% certain that this value lies within the interval
[C −(3 ×
√
1.05), C + (3 ×
√
1.05)] = [C −3.07, C + 3.07]
FIGURE 5.25
Probability density functions for
chemical concentration level
measurements
C
C
C
X A
N(C, 2.97
Method A
Method B
)
X B
N(C, 1.62)
Y = 0.35X A + 0.65X B
Combination of
methods A and B
Y
N(C, 1.05)

238
CHAPTER 5
THE NORMAL DISTRIBUTION
5.2.3
Problems
5.2.1 Suppose that X ∼N(3.2, 6.5), Y ∼N(−2.1, 3.5), and
Z ∼N(12.0, 7.5) are independent random variables.
Find the probability that
(a) X + Y ≥0
(b) X + Y −2Z ≤−20
(c) 3X + 5Y ≥1
(d) 4X −4Y + 2Z ≤25
(e) |X + 6Y + Z| ≥2
(f) |2X −Y −6| ≤1
5.2.2 Suppose that X ∼N(−1.9, 2.2), Y ∼N(3.3, 1.7), and
Z ∼N(0.8, 0.2) are independent random variables. Find
the probability that
(a) X −Y ≥−3
(b) 2X + 3Y + 4Z ≤10
(c) 3Y −Z ≤8
(d) 2X −2Y + 3Z ≤−6
(e) |X + Y −Z| ≥1.5
(f) |4X −Y + 10| ≤0.5
5.2.3 Consider a sequence of independent random variables Xi,
each with a standard normal distribution.
(a) What is P(|Xi| ≤0.5)?
(b) If ¯X is the average of eight of these random variables,
what is P(| ¯X| ≤0.5)?
(c) In general, if ¯X is the average of n of these random
variables, what is the smallest value of n for which
P(| ¯X| ≤0.5) ≥0.99?
5.2.4 Recall Problem 5.1.11 where metal plate thicknesses are
normally distributed with a mean of 4.3 mm and a
standard deviation of 0.12 mm.
(a) If one metal plate is placed on top of another, what is
the distribution of their combined thickness?
(b) What is the distribution of the average thickness of
12 metal plates?
(c) What is the smallest number of metal plates required
in order for their average thickness to be between 4.25
and 4.35 mm with a probability of at least 99.7%?
5.2.5 A machine part is assembled by fastening two
components of type A and three components of type B
end to end. The lengths of components of type A in mm
are independent N(37.0, 0.49) random variables, and the
lengths of components of type B in mm are independent
N(24.0, 0.09) random variables. What is the probability
that a machine part has a length between 144 and
147 mm?
5.2.6 (a) Suppose that X1 ∼N(μ1, σ 2
1 ) and X2 ∼N(μ2, σ 2
2 )
are independently distributed. What is the variance of
Y = pX1 + (1 −p)X2?
Show that the variance is minimized when
p =
1
σ 2
1
1
σ 2
1 +
1
σ 2
2
What is the variance of Y in this case?
(b) More generally suppose that Xi ∼N(μi, σ 2
i ),
1 ≤i ≤n, are independently distributed, and that
Y = p1X1 + · · · + pn Xn
where p1 + · · · + pn = 1. What values of the pi minimize
the variance of Y, and what is the minimum variance?
5.2.7 If $x is invested in mutual fund I, its worth after one year
is distributed
X I ∼N(1.05x, 0.0002x2)
and if $x is invested in mutual fund II, its worth after
one year is distributed
X I I ∼N(1.05x, 0.0003x2)
Suppose that you have $1000 to invest and that you place
$y in mutual fund I and $(1000 −y) in mutual fund II.
(a) What is the expected value of the total worth of your
investments after one year?
(b) What is the variance of the total worth of your
investments after one year?
(c) What value of y minimizes the variance of the total
worth of your investments after one year? If you
adopt this “conservative” strategy, what is the
probability that after one year the total worth of your
investments is more than $1060?
5.2.8 Recall Problem 5.1.9 where glass sheets have a
N(3.00, 0.122) distribution.
(a) What is the probability that three glass sheets placed
one on top of another have a total thickness greater
than 9.50 mm?
(b) What is the probability that seven glass sheets have
an average thickness less than 3.10 mm?
5.2.9 Recall Problem 5.1.10 where sugar packets have weights
with N(1.03, 0.0142) distributions. A box contains
22 sugar packets.
(a) What is the distribution of the total weight of sugar in
a box?

5.2 LINEAR COMBINATIONS OF NORMAL RANDOM VARIABLES
239
(b) What are the upper and lower quartiles of the total
weight of sugar in a box?
5.2.10 Recall Problem 5.1.15, where mechanical components
have a width that is normally distributed with a mean
μ = 2600 and a standard deviation σ = 0.6. In an
assembly procedure, four of these components need to be
ﬁtted side by side into a slot in another part.
(a) Suppose that the slots have a width of 10,402.5. What
proportion of the time will four randomly selected
components be able to ﬁt into a slot?
(b) More generally, suppose that the widths of the slots
vary according to a normal distribution with mean
μ = 10,402.5 and standard deviation σ = 0.4. In this
case, what proportion of the time will four randomly
selected components be able to ﬁt into a randomly
selected slot?
5.2.11 Let X1, . . . , X15 be independent identically distributed
N(4.5, 0.88) random variables, with an average ¯X.
(a) Calculate P(4.2 ≤¯X ≤4.9).
(b) Find the value of c for which
P(4.5 −c ≤¯X ≤4.5 + c) = 0.99.
5.2.12 Five students are waiting to talk to the TA when ofﬁce
hours begin. The TA talks to the students one at a time,
starting with the ﬁrst student and ending with the ﬁfth
student, with no breaks between students. Suppose that
the time taken by the TA to talk to a student has a normal
distribution with a mean of 8 minutes and a standard
deviation of 2 minutes, and suppose that the times taken
by the students are independent of each other.
(a) What is the probability that the total time taken by
the TA to talk to all ﬁve students is longer than
45 minutes?
(b) Suppose that the time that elapses between when the
TA starts talking to the ﬁrst student, and when the TA
starts to have a headache, has a normal distribution
with a mean of 28 minutes and a standard deviation
of 5 minutes, which is independent of the times taken
to talk to the students. What is the probability that
the TA’s headache starts at a time after the TA has
ﬁnished talking to the third student?
5.2.13 Components of type A have heights that are
independently distributed as a normal distribution with a
mean 190 and a standard deviation of 10. Components of
type B have heights that are independently distributed as
a normal distribution with a mean 150 and a standard
deviation of 8. What is the probability that a stack of four
components of type A placed one on top of the other will
be taller than a stack of ﬁve components of type B placed
one on top of the other?
5.2.14 The times taken for worker 1 to perform a task are inde-
pendently distributed as a normal distribution with mean
13 minutes and standard deviation 0.5 minutes. The times
taken for worker 2 to perform a task are independently dis-
tributed as a normal distribution with mean 17 minutes and
standard deviation 0.6 minutes, and they are independent
of the times taken by worker 1. At the beginning of the
day, both workers start their ﬁrst task at the same time, and
when they have ﬁnished a task, they immediately start an-
other task. What is the probability that worker 1 will ﬁnish
his fourth task before worker 2 has ﬁnished his third task?
5.2.15 Bricks’ weights are independently distributed as a normal
distribution with mean 110 and standard deviation 2.
What is the smallest value of n such that there is a
probability of at least 99% that the average weight of n
randomly selected bricks is less than 111?
5.2.16 A piece of wire is cut, and the length of the wire has a
normal distribution with a mean 7.2 m and a standard
deviation 0.11 m. If the piece of wire is then cut exactly in
half, what are the mean and the standard deviation of the
lengths of the two pieces?
5.2.17 The amount of timber available from a certain type of
fully grown tree has a mean of 63400 with a standard
deviation of 2500.
(a) What are the mean and the standard deviation of the
total amount of timber available from 20 trees?
(b) What are the mean and the standard deviation of the
average amount of timber available from 30 trees?
5.2.18 A chemist can set the target value for the elasticity of a
polymer compound. The resulting elasticity is normally
distributed with a mean equal to the target value and a
standard deviation of 47.
(a) What target value should be set if it is required that
there is only a 10% probability that the elasticity is
less than 800?
(b) Suppose that a target value of 850 is used. What is the
probability that the average elasticity of ten samples
is smaller than 875?
5.2.19 An investment in company A has an expected return of
$30,000 with a standard deviation of $4000. An
investment in company B has an expected return of
$45,000 with a standard deviation of $3000. If the returns
are normally distributed and independent, what is the
probability that the total return from both investments
will be at least $85,000?

240
CHAPTER 5
THE NORMAL DISTRIBUTION
5.3
Approximating Distributions with the Normal Distribution
A very useful property of the normal distribution is that it provides good approximations to
the probability values of certain other distributions. In these special cases, the cumulative
distribution function of a rather complicated distribution can sometimes be related to the
cumulative distribution function of a normal distribution that is easily evaluated.
The most common example is when the normal distribution is used to approximate the bi-
nomial distribution. If the parameter n of the binomial distribution is large, then its cumulative
distribution function is tedious to compute. However, these binomial probabilities are approx-
imated very well by the probabilities of a corresponding normal distribution. Some more
general theory provided by the central limit theorem indicates that the normal distribution
is appropriate to approximate the distribution of an average of a set of identically distributed
random variables, irrespective of the distribution of the individual random variables.
5.3.1
The Normal Approximation to the Binomial Distribution
An examination of the probability mass functions of the binomial distributions that are graphed
in Section 3.1 reveals that they have a “bell-shaped” curve similar to the probability density
function of a normal distribution. In fact, it turns out that the normal distribution is very
good at providing an approximation to the probability values of a binomial distribution when
the parameter n is reasonably large and when the success probability p is not too close to 0
or to 1.
Recall that a B(n, p) distribution has an expected value of np and a variance of np(1−p).
This distribution can be approximated by a normal distribution with the same mean and
variance, that is, a
N(np, np(1 −p))
distribution.
For example, suppose that X ∼B(16, 0.5), in which case X has a mean of 8 and a variance
of 4. The probability mass function of this distribution is shown in Figure 5.26 together with
the probability density function of the random variable Y ∼N(8, 4). Even though the random
variable X has a discrete distribution and the random variable Y has a continuous distribution,
the shapes of their respective probability mass function and probability density function are
FIGURE 5.26
Comparison of the probability
mass function of a B(16, 0.5)
random variable and the probability
density function of a N(8, 4)
random variable
0
1
2
3
X ∼ B(16, 0.5)
4
5
6
7
8
9
10
11
12
13
14
15
16
Y ∼ N(8, 4)
0.000
0.000
0.002
0.009
0.028
0.067
0.122
0.175
0.197
0.175
0.122
0.067
0.028
0.009
0.002 0.000 0.000

5.3 APPROXIMATING DISTRIBUTIONS WITH THE NORMAL DISTRIBUTION
241
quite similar. The lines of the probability mass function of X are not exactly the same height
as those of the probability density function of Y, but they are very close.
It is interesting to compute some probability values of the B(16, 0.5) and N(8, 4) distri-
butions to see how well they compare. The probability that the binomial distribution takes a
value no larger than 5 is
P(X ≤5) =
5

x = 0

16
x

× 0.5x × 0.516−x = 0.1051
The best way to approximate this probability using the normal distribution is to compute the
probability that Y ≤5.5. Notice that a “continuity correction” of 0.5 is added to the value 5
in order to improve the approximation of the discrete binomial distribution by a continuous
distribution, as illustrated in Figure 5.27. The approximate probability value obtained from
the normal distribution is
P(Y ≤5.5) = 
5.5 −μ
σ

= 
5.5 −8.0
2.0

= (−1.25) = 0.1056
It is seen that the approximation provided by the normal distribution is remarkably good.
Asanotherexample,illustratedinFigure5.28,theprobabilitythatthebinomialdistribution
takes a value between 8 and 11 inclusive is
P(8 ≤X ≤11) =
11

x=8

16
x

× 0.5x × 0.516−x = 0.5598
FIGURE 5.27
Approximating P(X ≤5) with a
probability from a normal
distribution
0
1
2
3
X ∼ B(16, 0.5)
4
5
6
7
8
9
10
11
12
13
14
15
16
Y ∼ N(8, 4)
0.002
0.009
0.028
0.067
P(X ≤ 5) = 0.1051
P(Y ≤ 5.5) = Φ(−1.25) 
                  = 0.1056
FIGURE 5.28
Approximating P(8 ≤X ≤11)
with a probability from a normal
distribution
0
1
2
3
X ∼ B(16, 0.5)
4
5
6
7
8
9
10
11
12
13
14
15
16
Y ∼ N(8, 4)
0.067
P(7.5 ≤ Y ≤ 11.5) = 0.5586
  P(8 ≤ X ≤ 11) = 0.5598
0.1970.175
0.122

242
CHAPTER 5
THE NORMAL DISTRIBUTION
The normal approximation to this, with continuity corrections, is
P(7.5 ≤Y ≤11.5) = 
11.5 −μ
σ

−
7.5 −μ
σ

= 
11.5 −8.0
2.0

−
7.5 −8.0
2.0

= (1.75) −(−0.25) = 0.9599 −0.4013 = 0.5586
Again, the normal approximation is seen to be remarkably good.
The normal approximation to the binomial distribution improves in accuracy as the param-
eter n of the binomial distribution increases. However, for a given value of n, the approximation
may not be very good if the success probability p is too close to 0 or to 1. How close is “too
close” depends upon the value of n. A general rule is that the approximation is reasonable as
long as both
np ≥5
and
n(1 −p) ≥5
It should also be noted that the exact probability values of binomial distributions are
easily obtainable from computer software packages, so that in practice there may be no need
to approximate their probability values unless n is larger than, say, 100. In such cases the
software package that you use may actually be using the normal approximation to calculate
the binomial probabilities.
Normal Approximations to the Binomial Distribution
The probability values of a B(n, p) distribution can be approximated by those of a
N(np, np(1 −p)) distribution. If X ∼B(n, p), then
P(X ≤x) ≃
 x + 0.5 −np
√np(1 −p)

and
P(X ≥x) ≃1 −
 x −0.5 −np
√np(1 −p)

These approximations work well as long as np ≥5 and n(1 −p) ≥5.
GAMES OF CHANCE
Suppose that a fair coin is tossed n times. The distribution of the number of heads obtained,
X, is B(n, 0.5). If n = 100, what is the probability of obtaining between 45 and 55 heads?
With a normal approximation this probability is easily calculated as
P(45 ≤X ≤55) ≃P(44.5 ≤Y ≤55.5)
where Y ∼N(50, 25), which can be evaluated to be

55.5 −50.0
√
25

−
44.5 −50.0
√
25

= (1.1) −(−1.1)
= 0.8643 −0.1357 = 0.7286
Consequently, in 100 coin tosses there is a probability of about 0.73 that the proportion of
heads is between 0.45 and 0.55.

5.3 APPROXIMATING DISTRIBUTIONS WITH THE NORMAL DISTRIBUTION
243
FIGURE 5.29
The probabilities that in n tosses of
a fair coin the proportion of heads
lies between 0.45 and 0.55
P(0.45 ≤ X/n ≤ 0.55)
Number of tosses
n = 100
n = 150
n = 200
n = 500
n = 750
n = 1000
73%
81%
86%
98%
99.4%
99.9%
For a general value of n, the distribution of X ∼B(n, 0.5) can be approximated by
Y ∼N(n/2, n/4). In general, the probability that the proportion of heads lies between 0.45
and 0.55 is therefore equal to
P(0.45n ≤X ≤0.55n) ≃P(0.45n −0.5 ≤Y ≤0.55n + 0.5)
= 
0.55n + 0.5 −0.5n
√n/4

−
0.45n −0.5 −0.5n
√n/4

= 

0.1√n + 1
√n

−

−0.1√n −1
√n

These probability values are given in Figure 5.29 for various values of n. Notice that they are
increasing and tend toward a limiting value of one as the number of tosses n increases.
5.3.2
The Central Limit Theorem
Consider a sequence X1, . . . , Xn of independent identically distributed random variables.
Suppose that these random variables have an expectation and variance
E(Xi) = μ
and
Var(Xi) = σ 2
If
¯X = X1 + · · · + Xn
n
then it was shown in Section 2.6.2 that
E( ¯X) = μ
and
Var( ¯X) = σ 2
n
Furthermore, it was also shown in Section 5.2.1 that if Xi ∼N(μ, σ 2), then
¯X ∼N

μ, σ 2
n

In summary, it is known that if a set of independent random variables is obtained and each has
the same distribution with mean μ and variance σ 2, then their average always has mean μ and
variance σ 2/n, and their average is normally distributed if the individual random variables
are normally distributed.
The central limit theorem provides an important extension to these results by stating that
regardless of the actual distribution of the individual random variables Xi, the distribution of
their average ¯X is closely approximated by a N(μ, σ 2/n) distribution. In other words, the
average of a set of independent identically distributed random variables is always approxi-
mately normally distributed. The accuracy of the approximation improves as n increases and
the average is taken over more random variables.

244
CHAPTER 5
THE NORMAL DISTRIBUTION
A general rule is that the approximation is adequate as long as n ≥30, although the
approximation is often good for much smaller values of n, particularly if the distribution of
the random variables Xi has a probability density function with a shape reasonably similar to
the normal bell-shaped curve.
Notice that if
¯X ∼N

μ, σ 2
n

then
X1 + · · · + Xn ∼N(nμ, nσ 2)
and so the central limit theorem can also be used to show that the distribution of the sum
X1 + · · · + Xn can be approximated by a N(nμ, nσ 2) distribution.
The central limit theorem is regarded as one of the most important theorems in the whole of
probability theory and, in fact, in the whole of mathematics. It may explain why many naturally
occurring phenomena are observed to have distributions similar to the normal distribution,
because they may be considered to be composed of the aggregate of many smaller random
events.
The central limit theorem also explains why the normal distribution provides a good
approximation to the binomial distribution, since if X ∼B(n, p), then
X = X1 + · · · + Xn
where the random variables Xi have independent Bernoulli distributions with parameter p.
Since E(Xi) = p and Var(Xi) = p(1 −p), the central limit theorem indicates that the
distribution of X can be approximated by a N(np, np(1 −p)) distribution, as discussed in
Section 5.3.1.
The Central Limit Theorem
If X1, . . . , Xn is a sequence of independent identically distributed random variables
with a mean μ and a variance σ 2, then the distribution of their average ¯X can be
approximated by a
N

μ, σ 2
n

distribution. Similarly, the distribution of the sum X1 + · · · + Xn can be approximated
by a
N(nμ, nσ 2)
distribution.
In conclusion, the central limit theorem provides a very convenient way of approximating
the probability values of an average of a set of identically distributed random variables. The
exact distribution of this average will in general have a complicated distribution, whereas the
approximate probability values are easily obtained from the appropriate normal distribution.
The central limit theorem has important consequences for statistical analysis methods (which
are discussed in later chapters) because it indicates that a sample average may be taken to be
normally distributed regardless of the actual distribution of the individual sample observations.

5.3 APPROXIMATING DISTRIBUTIONS WITH THE NORMAL DISTRIBUTION
245
5.3.3
Simulation Experiment 1: An Investigation of the Central Limit Theorem
It is instructive to check the central limit theorem by simulating sets of random numbers on a
computer. Most computer packages allow you to simulate random numbers from a speciﬁed
distribution. If the distribution is discrete, then these random numbers are the actual elements
of the state space that are produced with frequencies corresponding to their probability values.
For continuous distributions, the random numbers produced have the property that they fall
into speciﬁc intervals with probability values governed by the probability density function of
the distribution being used.
An idea of the shape of the probability mass function or probability density function of
a distribution can be obtained by simulating a large number m of random variables from the
distribution and then constructing a histogram of their values (histograms are discussed in
more detail in Chapter 6). The shape of the histogram approximates the shape of the underlying
density function of the random variables, and the accuracy of the approximation improves as
the number of simulated observations m increases.
Suppose that we wish to investigate the probability density function of the random variable
Y = X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10
10
where the random variables Xi are independent exponential random variables with parameter
λ = 0.1. We can do this by simulating m = 500 values yi, 1 ≤i ≤500, from this distribution.
Each value yi can be obtained by simulating ten values xi j, 1 ≤j ≤10, having the speciﬁed
exponential distribution, and then by taking their average. (In practice, a convenient way to
do this is to form ten columns of random numbers, each of length 500, and then to form a new
column equal to their average.)
Notice that overall this requires the simulation of 5000 exponential random variables.
The probability density function of the random variable Y is approximated by a histogram of
the values yi. Figure 5.30 presents a histogram obtained from this simulation experiment. Of
course, if you try it yourself, you’ll produce a slightly different histogram.
FIGURE 5.30
Histogram of the averages of
simulated exponential random
variables
6
8
10
14
22
12
4
2
Frequency
16
18
20

246
CHAPTER 5
THE NORMAL DISTRIBUTION
4
5
6
7
3
Frequency
FIGURE 5.31
Histogram of the averages of simulated beta random variables
8
7
9
10
11
12
13
14
Frequency
FIGURE 5.32
Histogram of the averages of simulated Poisson random variables
The random variables Xi have a mean of 10 and a variance of 100, so that the random
variable Y has a mean of 10 and a variance of 10. Therefore, the central limit theorem indicates
that the distribution of Y can be approximated by a N(10, 10) distribution. Figure 5.30 clearly
illustrates that the distribution of Y is starting to take on a normal bell-shaped curve, and that
it is quite different from the exponential distribution.
Figure 5.31 presents a histogram obtained from a similar simulation experiment where
the random variables Xi are taken to have beta distributions with parameter values a = b = 2
(this distribution is illustrated in Figure 4.21). This beta distribution has a mean of 0.5 and
a variance of 0.05, so that the central limit theorem indicates that the distribution of Y can
be approximated by a N(0.5, 0.005) distribution, and this is conﬁrmed by the histogram in
Figure 5.31, which clearly has the shape of a normal distribution.
Finally, Figure 5.32 presents a histogram obtained from this simulation experiment when
the random variables Xi are taken to have a Poisson distribution with parameter λ = 10.
This Poisson distribution has a mean and a variance of 10, so that the central limit theorem
indicates that the distribution of Y can be approximated by a N(10, 1) distribution. Again,
this is conﬁrmed by the histogram in Figure 5.32, which clearly has the shape of a normal
distribution.
When you perform simulations of this kind, it is useful to remember that the closeness of
the histogram that you obtain to the shape of a normal distribution depends on three factors
(m, n, and the distribution of Xi) in the following ways.
■
As the number m of simulated values yi of the random variable Y increases, the
histogram of the yi becomes a more and more accurate representation of the true
probability density function of Y.
■
As the number n increases, so that the random variable Y is obtained as the average
of a larger number of random variables, then the true probability density function of
Y becomes closer to the probability density function of a normal distribution.

5.3 APPROXIMATING DISTRIBUTIONS WITH THE NORMAL DISTRIBUTION
247
■
In general, for a given value of n, the true probability density function of Y becomes
closer to the probability density function of a normal distribution as the probability
density function of the random variables Xi has a shape that looks more like a normal
probability density function.
5.3.4
Examples of Employing Normal Approximations
Example 17
Milk Container
Contents
Recall that there is a probability of 0.261 that a milk container is underweight. Consequently,
the number of underweight containers X in a box of 20 containers has a B(20, 0.261) distri-
bution. This distribution may be approximated by
Y ∼N(20 × 0.261, 20 × 0.261 × (1 −0.261)) = N(5.22, 3.86)
Figure 5.33 shows the probability mass function of X together with the probability density
function of Y, and they are seen to match well. The probability that a box contains no more
than three underweight containers was calculated in Section 3.1.3 to be
P(X ≤3) = 0.1935
The normal approximation to this probability is
P(Y ≤3.5) = 
3.5 −5.22
√
3.86

= (−0.87) = 0.1922
which is very close.
Suppose that 25 boxes of milk are delivered to a supermarket. What is the distribution of
the number of underweight containers? Altogether there are now 500 milk containers so that
the number of underweight containers X has a B(500, 0.261) distribution. This distribution
may be approximated by
Y ∼N(500 × 0.261, 500 × 0.261 × (1 −0.261)) = N(130.5, 96.44)
12
0
1
2
3
X ∼ B(20, 0.261)
4
5
6
7
8
9
10
11
Y ∼ N(5.22, 3.86)
13
14
15
16
17
18
19
20
0.201
0.178
0.125
0.072
0.034
0.013
0.004 0.001 0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.178
0.119
0.056
0.017
0.002
Number of underweight containers
P(Y ≤3.5) = 0.1922
P(X ≤3) = 0.1935
FIGURE 5.33
Approximating the distribution of the number of underweight containers with a normal distribution

248
CHAPTER 5
THE NORMAL DISTRIBUTION
The probability that at least 150 out of the 500 milk containers are underweight can then be
calculated to be
P(X ≥150) ≃P(Y ≥149.5) = 1 −
149.5 −130.5
√
96.44

= 1 −(1.93) = 1 −0.9732 = 0.0268
Example 39
Cattle Inoculations
Cattle are routinely inoculated to help prevent the spread of diseases among them. Suppose
that a particular vaccine has a probability of 0.0005 of provoking a serious adverse reaction
when administered to an animal. This probability value implies that, on average, only 1 in
2000 animals suffers this reaction.
Suppose that the vaccine is to be administered to 500,000 head of cattle. The number of
animals X that will suffer an adverse reaction has a binomial distribution with parameters
n = 500,000 and p = 0.0005. Since
E(X) = 500,000 × 0.0005 = 250
and
Var(X) = 500,000 × 0.0005 × 0.9995 = 249.9
this distribution can be approximated by
Y ∼N(250, 249.9)
An interval of three standard deviations about the mean value for this normal distribution is
[250 −(3 ×
√
249.9), 250 + (3 ×
√
249.9)] = [202.6, 297.4]
Consequently, the veterinarians can be conﬁdent (at least 99.7% certain) that if they inoculate
500,000 head of cattle, then the number of animals that suffer a serious adverse reaction will
be somewhere between 200 and 300.
Example 27
Glass Sheet Flaws
Recall that the number of ﬂaws in a glass sheet has a Poisson distribution with a parameter
value λ = 0.5. This implies that both the expectation and variance of the number of ﬂaws in
a sheet are
μ = σ 2 = 0.5
What is the distribution of the total number of ﬂaws X in 100 sheets of glass?
The expected number of ﬂaws in 100 sheets of glass is 100 × μ = 50, and the variance of
the number of ﬂaws in 100 sheets of glass is 100 × σ 2 = 50. The central limit theorem then
implies that the distribution of X can be approximated by a
N(50, 50)
distribution. The probability that there are fewer than 40 ﬂaws in 100 sheets of glass can
therefore be approximated as
P(X ≤40) ≃
40 −50
√
50

= (−1.41) = 0.0793
or about 8%.

5.3 APPROXIMATING DISTRIBUTIONS WITH THE NORMAL DISTRIBUTION
249
The central limit theorem indicates that the average number of ﬂaws per sheet in 100
sheets of glass, ¯X, has a distribution that can be approximated by a
N

μ, σ 2
100

= N(0.5, 0.005)
distribution. The probability that this average is between 0.45 and 0.55 is therefore
P(0.45 ≤¯X ≤0.55) ≃
0.55 −0.50
√
0.005

−
0.45 −0.50
√
0.005

= (0.71) −(−0.71) = 0.7611 −0.2389 = 0.5222
which is about 52%.
The key point in these probability calculations is that even though the number of ﬂaws in
an individual sheet of glass follows a Poisson distribution, the central limit theorem indicates
that probability calculations concerning the total number or average number of ﬂaws in 100
sheets of glass can be found using the normal distribution.
Example 30
Pearl Oyster Farming
Recall that there is a probability of 0.6 that an oyster produces a pearl with a diameter of at least
4 mm, which is consequently of commercial value. How many oysters does an oyster farmer
need to farm in order to be 99% conﬁdent of having at least 1000 pearls of commercial value?
If the oyster farmer farms n oysters, then the distribution of the number of pearls of
commercial value is
X ∼B(n, 0.6)
This distribution can be approximated by
Y ∼N(0.6n, 0.24n)
The probability of having at least 1000 pearls of commercial value is then
P(X ≥1000) ≃P(Y ≥999.5) = 1 −
999.5 −0.6n
√
0.24n

Now if
1 −(x) ≥0.99
or equivalently
(x) ≤0.01
it can be seen from Table I that x ≤−2.33. Therefore, it follows that the farmer’s requirements
can be met as long as
999.5 −0.6n
√
0.24n
≤−2.33
which is satisﬁed for n ≥1746.
In conclusion, the farmer should farm about 1750 oysters in order to be 99% conﬁdent of
having at least 1000 pearls of commercial value. In this case, the expected number of pearls
of commercial value is 1750 × 0.6 = 1050, and the standard deviation is
√
1750 × 0.6 × 0.4 = 20.5
Recall that each pearl has an expected diameter and variance of
μ = 5.0
and
σ 2 = 8.33

250
CHAPTER 5
THE NORMAL DISTRIBUTION
If 1050 pearls are actually obtained, then the average diameter of the pearls will have an
expectation and variance of
μ = 5.0
and
σ 2
1050 = 8.33
1050 = 0.00793
The central limit theorem indicates that the average diameter has approximately a normal
distribution (with such a large value of n the approximation will be very accurate), so that
there is about a 99.7% chance that the average pearl diameter will lie within three standard
deviations of the mean value, that is, within the interval
[5.0 −(3 ×
√
0.00793), 5.0 + (3 ×
√
0.00793)] = [4.7, 5.3]
In other words, the farmer can be very conﬁdent of harvesting a collection of pearls with an
average diameter between 4.7 mm and 5.3 mm.
5.3.5
Problems
5.3.1 Calculate the following probabilities both exactly and by
using a normal approximation:
(a) P(X ≥8) where X ∼B(10, 0.7)
(b) P(2 ≤X ≤7) where X ∼B(15, 0.3)
(c) P(X ≤4) where X ∼B(9, 0.4)
(d) P(8 ≤X ≤11) where X ∼B(14, 0.6)
5.3.2 Calculate the following probabilities both exactly and by
using a normal approximation:
(a) P(X ≥7) where X ∼B(10, 0.3)
(b) P(9 ≤X ≤12) where X ∼B(21, 0.5)
(c) P(X ≤3) where X ∼B(7, 0.2)
(d) P(9 ≤X ≤11) where X ∼B(12, 0.65)
5.3.3 Suppose that a fair coin is tossed n times. Estimate the
probability that the proportion of heads obtained lies
between 0.49 and 0.51 for n = 100, 200, 500, 1000, and
2000.
5.3.4 Suppose that a fair die is rolled 1000 times.
(a) Estimate the probability that the number of 6s is
between 150 and 180.
(b) What is the smallest value of n for which there is a
probability of at least 99% of obtaining at least 50 6s
in n rolls of a fair die?
5.3.5 The number of cracks in a ceramic tile has a Poisson
distribution with parameter λ = 2.4.
(a) How would you approximate the distribution of the
total number of cracks in 500 ceramic tiles?
(b) Estimate the probability that there are more than
1250 cracks in 500 ceramic tiles.
5.3.6 In a test for a particular illness, a false-positive result is
obtained about 1 in 125 times the test is administered. If
the test is administered to 15,000 people, estimate the
probability of there being more than 135 false-positive
results.
5.3.7 Despite a series of quality checks by a company that
makes television sets, there is a probability of 0.0007
that when a purchaser unpacks a newly purchased
television set it does not work properly. If the company
sells 250,000 television sets a year, estimate the
probability that there will be no more than 200 unhappy
purchasers in a year.
5.3.8 A multiple-choice test consists of a series of questions,
each with four possible answers.
(a) If there are 60 questions, estimate the probability that
a student who guesses blindly at each question will
get at least 30 questions right.
(b) How many questions are needed in order to be 99%
conﬁdent that a student who guesses blindly at each
question scores no more than 35% on the test?
5.3.9 Recall Problem 4.3.4 in which a day’s sales in $1000 units
at a gas station have a gamma distribution with k = 5 and
λ = 0.9. If the sales on different days are distributed
independently of each other, estimate the probability that
in one year the gas station takes in more than $2 million.
5.3.10 Recall Problem 3.1.9, where a company receives 60% of
its orders over the Internet. Estimate the probability that
at least 925 of the company’s next 1500 orders will be
received over the Internet.
5.3.11 Consider again Supplementary Problem 4.8.10, where the
strength of a chemical solution has a beta distribution
with parameters a = 18 and b = 11. Estimate the
probability that the average strength of 20 independently
produced chemical solutions is between 0.60 and 0.65.

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
251
5.3.12 Suppose that a course has a capacity of at most 240
people, but that 1550 invitations are sent out. If each
person who receives an invitation has a probability of
0.135 of attending the course, independently of
everybody else, what is the probability that the number
of people attending the course will exceed the
capacity?
5.3.13 The lifetimes of batteries are independent with an
exponential distribution with a mean of 84 days. Consider
a random selection of 350 batteries. What is the
probability that at least 55 of the batteries have lifetimes
between 60 and 100 days?
5.3.14 The time to failure of an electrical component has a
Weibull distribution with parameters λ = 0.056 and
a = 2.5. A random collection of 500 components is
obtained. Estimate the probability that at least 125 of the
500 components will have failure times larger than 20.
5.3.15 Suppose that components have weights that are
independent and uniformally distributed between 890
and 892.
(a) Suppose that components are weighed one by one.
What is the probability that the sixth component
weighed is the third component that weighs more
than 891.2?
(b) If a box contains 200 components, what is the
probability that at least half of them weigh more than
890.7?
5.3.16 Suppose that the time taken for food to spoil using a
certain packaging method has an exponential distribution
with a mean of 8 days. If a random sample of 100 packets
are tested after 10 days, what are the expectation and the
standard deviation of the number of packets that will be
found with spoiled food? What is the probability that at
least 75 of the packets will contain spoiled food?
5.4
Distributions Related to the Normal Distribution
The normal distribution is the basis for the construction of various other important probability
distributions. The lognormal distribution has a positive state space and can be used to
model response times and failure times as well as many other phenomena. The chi-square
distribution, the t-distribution, and the F-distribution are important tools in statistical data
analysis, as will be seen in later chapters. Finally, the multivariate normal distribution is
used to develop much of the theory behind statistical inference methods.
5.4.1
The Lognormal Distribution
A random variable X has a lognormal distribution with parameters μ and σ 2 if the transformed
random variable Y = ln(X) has a normal distribution with mean μ and variance σ 2.
The Lognormal Distribution
A random variable X has a lognormal distribution with parameters μ and σ 2 if
Y = ln(X) ∼N(μ, σ 2)
The probability density function of X is
f (x) =
1
√
2πσ x
e−(ln(x)−μ)2/2σ 2
for x ≥0 and f (x) = 0 elsewhere, and the cumulative distribution function is
F(x) = 
ln(x) −μ
σ

A lognormal distribution has expectation and variance
E(X) = eμ+σ 2/2
and
Var(X) = e2μ+σ 2
eσ 2 −1


252
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.34
Probability density functions of the
lognormal distribution
0
10
20
μ = 2, σ = 1
x
f(x)
μ = σ = 1
μ = 1, σ = 0.5
Notice that the cumulative distribution function of a lognormal distribution is easily cal-
culated using the cumulative distribution function of a standard normal distribution (x),
since
P(X ≤x) = P(ln(X) ≤ln(x)) = P
Y −μ
σ
≤ln(x) −μ
σ

= 
ln(x) −μ
σ

Figure 5.34 shows the probability density functions of lognormal distributions with pa-
rameter values μ = σ = 1, μ = 2 and σ = 1, and μ = 1 and σ = 0.5. It can be seen that these
distributions all have long, gradually decreasing right-hand tails, which is a general property
of lognormal distributions. The cumulative distribution function of a lognormal distribution
indicates that the median value is eμ, which is always smaller than the mean value. This is a
consequence of the long right-hand tail of the distribution.
Notice that, in general, F(x) = 1 −α implies that

ln(x) −μ
σ

= 1 −α
so that
ln(x) −μ
σ
= zα
where zα is the critical point of the standard normal distribution. This implies that the (1−α)th
quantile of a lognormal distribution is
x = eμ+σzα
COMPUTER NOTE
Probability values of the lognormal distribution are easily calculated due to the simple form
of the cumulative distribution function, and they should also be available on your computer
package.
Example 40
Testing Reaction Times
Suppose that the reaction time in seconds of a person, that is, the time elapsing between the
arrival of a certain stimulus and a consequent action by the person, can be modeled by a

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
253
FIGURE 5.35
Probability density function of
reaction times
0.25
0.50
0.75
0.60
1.00
x
P(X ≤ 0.6) = 0.2119
Lognormal, μ = −0.35, σ = 0.2
Reaction time (Seconds)
lognormal distribution with parameter values μ = −0.35 and σ = 0.2. This distribution is
illustrated in Figure 5.35.
The mean reaction time is
E(X) = e−0.35+0.22/2 = 0.719
and the variance of the reaction times is
Var(X) = e−(2×0.35)+0.22
e0.22 −1

= 0.021
Also, the median reaction time is e−0.35 = 0.705 seconds.
The probability that a reaction time is less than 0.6 seconds is
P(X ≤0.6) = 
ln(0.6) + 0.35
0.2

= (−0.80) = 0.2119
The ﬁfth percentile of the reaction times is the value x that satisﬁes
P(X ≤x) = 
ln(x) + 0.35
0.2

= 0.05
This equation is satisﬁed when
ln(x) + 0.35
0.2
= −1.645
so that x = 0.507 seconds. Consequently, only about 5% of the reaction times are less than
0.51 seconds.
5.4.2
The Chi-Square Distribution
If the random variable X has a standard normal distribution, then the random variable
Y = X2
is said to have a chi-square distribution with one degree of freedom. More generally, if the
random variables Xi ∼N(0, 1), 1 ≤i ≤n, are independent, then the random variable
Y = X2
1 + · · · + X2
n
is said to have a chi-square distribution with n degrees of freedom.

254
CHAPTER 5
THE NORMAL DISTRIBUTION
The degrees of freedom of a chi-square distribution are usually denoted by the Greek
letter ν and can take any positive integer value. The notation
X ∼χ2
ν
is used to denote that the random variable X has a chi-square distribution with ν degrees of
freedom. Notice that if the random variables
X1 ∼χ2
ν1
and
X2 ∼χ2
ν2
are independently distributed, then it follows from their representation as the sum of squares
of standard normal random variables that
Y = X1 + X2 ∼χ2
ν1+ν2
The chi-square distribution is in fact a gamma distribution with parameter values λ = 1/2
and k = ν/2. Its probability density function is
f (x) =
1
2ν/2 
(ν/2) xν/2−1 e−x/2
for x ≥0 and f (x) = 0 elsewhere. Its expectation and variance are given in the following
box. It is worth noting that a chi-square distribution with noninteger (but positive) degrees of
freedom can also be deﬁned from the gamma distribution.
The Chi-Square Distribution
A chi-square random variable with ν degrees of freedom, X, can be generated as
X = X2
1 + · · · + X2
ν
where the Xi are independent standard normal random variables. A chi-square
distribution with ν degrees of freedom is a gamma distribution with parameter values
λ = 1/2 and k = ν/2, and it has an expectation of ν and a variance of 2ν.
Figure 5.36 illustrates chi-square distributions with degrees of freedom ν = 5, 10, and
15. Notice that as the degrees of freedom increase, the distribution becomes more symmetric
and more spread out. In fact, since a chi-square distribution with ν degrees of freedom is
generated as the sum of ν independent, identically distributed random variables (i.e., X2
i
where Xi ∼N(0, 1)), the central limit theorem implies that for large values of ν a chi-square
distribution can be approximated by a N(ν, 2ν) distribution.
The critical points of chi-square distributions are denoted by χ2
α,ν and are deﬁned by
P

X ≥χ2
α,ν

= α
where X has a chi-square distribution with ν degrees of freedom, as illustrated in Figure 5.37.
TableIIcontainsthevaluesofthesecriticalpointsforvariousα levelsanddegreesoffreedomν.
The chi-square distribution and these critical points will be used in the statistical inference
methodologies discussed in Chapters 8, 10, 15, and 17.
COMPUTER NOTE
The critical points and other probability values of chi-square distributions should be available
from your software package. Usually, there is a “chi-square” command for which you need
to specify only the degrees of freedom ν, but in any case do not forget that the chi-square
distribution is just a special case of a gamma distribution.

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
255
10
20
30
ν = 5
x
f(x)
ν = 15
ν = 10
0
FIGURE 5.36
Probability density functions of the chi-square distribution
x
f(x)
χ2
α,ν
α
χ2
ν
FIGURE 5.37
The critical points χ2
α,ν of the chi-square distribution
5.4.3
The t-distribution
If a standard normal random variable is divided by the square root of an independent χ2
ν /ν
random variable, then the resulting random variable is said to have a t-distribution with
ν degrees of freedom. This can be written
tν ∼N(0, 1)

χ2ν /ν
The t-distribution is often referred to as “Student’s t-distribution” (see the Historical Note).
HISTORICAL NOTE
William Sealey Gosset
(1876–1937) studied
mathematics and chemistry at
Oxford University in England.
In 1899 he moved to Dublin,
Ireland, and worked as a
statistician for the Guinness
brewery. During his work on
the quality of barley and hops,
Gosset proposed the use of the
t-distribution, and in 1908 he
published his ideas in an
academic article using the
pseudonym “Student” because
Guinness forbade its
employees from publishing
their own research results.
Consequently, the t-distribution
is often referred to as Student’s
t-distribution.
Figure 5.38 shows a t-distribution with ﬁve degrees of freedom superimposed upon a
standard normal distribution. The t-distribution has a shape very similar to that of a standard
normal distribution. It has a symmetric bell-shaped curve centered at 0, but it is actually a
little “ﬂatter” than the standard normal distribution. However, as the degrees of freedom ν
increase, the t-distribution becomes closer and closer to a standard normal distribution, and
the standard normal distribution is in fact the limiting value of the t-distribution as ν →∞.
The t-distribution
A t-distribution with ν degrees of freedom is deﬁned to be
tν ∼N(0, 1)

χ2ν /ν
where the N(0, 1) and χ2
ν random variables are independently distributed. The
t-distribution has a shape similar to a standard normal distribution but is a little ﬂatter.
As ν →∞, the t-distribution tends to a standard normal distribution.
The critical points of a t-distribution are denoted by tα,ν and are deﬁned by
P(X ≥tα,ν) = α

256
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.38
Comparison of a t-distribution and
the standard normal distribution
N(0, 1) distribution
0
1
2
3
t5 distribution
FIGURE 5.39
The critical points tα,ν of the
t-distribution
0
α
tα ν
tν distribution
where the random variable X has a t-distribution with ν degrees of freedom, as illustrated in
Figure 5.39. Some of these critical points are given in Table III for various values of ν and
α ≤0.1.
Notice that the symmetry of the t-distribution implies that t1−α,ν = −tα,ν. Furthermore,
notice that if X has a t-distribution with ν degrees of freedom, then
P(|X| ≤tα/2,ν) = P(−tα/2,ν ≤X ≤tα/2,ν) = 1 −α
as illustrated in Figure 5.40.
The last row in Table III with ν = ∞corresponds to the standard normal distribution,
and it is seen that for a ﬁxed value of α, the critical points tα,ν decrease to tα,∞= zα as the
degrees of freedom ν increase. For example, with α = 0.05, t0.05,5 = 2.015, t0.05,10 = 1.812,
t0.05,25 = 1.708, and t0.05,∞= z0.05 = 1.645. The t-distribution and these critical points will
be used in the statistical inference methodologies discussed in Chapters 8, 9, 12, and 13.
COMPUTER NOTE
Check to see how the critical values given in Table III and additional probability values of the
t-distribution are obtained from your computer package.

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
257
FIGURE 5.40
The critical points tα/2,ν of the
t-distribution
0
tα/2,ν
t   distribution
ν
−tα/2,ν
α/2
t
α/2
P(|X| ≤tα/2,ν) = 1 −α
FIGURE 5.41
Probability density functions of the
F-distribution
1
5
4
2
3
0
x
f(x)
v   = v  = 25
1
2
v   = 5, v  = 25
1
2
v   = v  = 5
1
2
5.4.4
The F-distribution
The ratio of two independent chi-square random variables that have been divided by their
respective degrees of freedom is deﬁned to be an F-distribution. This ratio can be written
Fν1,ν2 ∼χ2
ν1/ν1
χ2ν2/ν2
An F-distribution has degrees of freedom ν1 and ν2 that correspond to the degrees of freedom
of ﬁrst the numerator chi-square distribution and then the denominator chi-square distribution.
Notice that in general an Fν1,ν2 distribution is not the same as an Fν2,ν1 distribution.
An F-distribution has a state space x ≥0 and is unimodal with a long right-hand tail.
Figure 5.41 shows the probability density functions of F-distributions with degrees of freedom
ν1 = ν2 = 5, ν1 = 5 and ν2 = 25, and ν1 = ν2 = 25. The expectation of an Fν1,ν2 distribution
is ν2/(ν2 −2) (for ν2 ≥3), which is roughly equal to one for reasonably large values of ν2.
In addition, the variance of the F-distribution decreases as the degrees of freedom ν1 and ν2
become larger, in which case the probability density function becomes more and more sharply
spiked about the value one.

258
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.42
The critical point Fα,ν1,ν2 of the
F-distribution
x
f(x)
α
         distribution
Fν1,ν2
Fα,ν1,ν2
The F-distribution
An F-distribution with degrees of freedom ν1 and ν2 is deﬁned to be
Fν1,ν2 ∼χ2
ν1/ν1
χ2ν2/ν2
where the two chi-square random variables are independently distributed. The
F-distribution has a positive state space, an expectation close to one, and a variance
that decreases as the degrees of freedom ν1 and ν2 increase.
The critical points of F-distributions are denoted by Fα,ν1,ν2, as illustrated in Figure 5.42.
Table IV contains some of these critical points for α = 0.10, 0.05, and 0.01. In addition, it
follows from the deﬁnition of the F-distribution that
F1−α,ν1,ν2 =
1
Fα,ν2,ν1
so that Table IV can also be used to ﬁnd the values Fα,ν1,ν2 for α = 0.90, 0.95, and 0.99. The
F-distribution and these critical points will be used in the statistical inference methodologies
discussed in Chapters 11, 12, 13, and 14.
COMPUTER NOTE
Check to see how the critical values given in Table IV and additional probability values of the
F-distribution are obtained from your computer package.
5.4.5
The Multivariate Normal Distribution
A bivariate normal distribution for a pair of random variables (X, Y) has ﬁve parameters.
These are the means μX and μY and the variances σ 2
X and σ 2
Y of the marginal distributions of
the random variables X and Y, together with the correlation ρ (−1 ≤ρ ≤1) between the two
random variables. A useful property of the bivariate normal distribution is that both marginal
distributions and any conditional distributions are all normal distributions.

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
259
FIGURE 5.43
A bivariate normal distribution
with correlation ρ = 0
y
x
The random variables X and Y are independent when ρ = 0, in which case the joint
probability density function of X and Y is the product of their normal marginal distributions.
If the two marginal distributions are standard normal distributions, so that μX = μY = 0 and
σ 2
X = σ 2
Y = 1, then the joint probability density function of X and Y is
f (x, y) = 1
2π e−(x2+y2)/2
for −∞< x, y < ∞, which is shown in Figure 5.43. This density function is rotationally
symmetric, and any “slice” of it on a plane that is perpendicular to the (x, y)-plane produces
a curve that is proportional to a normal probability density function.
If the two marginal distributions are standard normal distributions, so that μX = μY = 0
and σ 2
X = σ 2
Y = 1, then the joint probability density function of X and Y for a general
correlation value ρ is
f (x, y) =
1
2π

1 −ρ2 e−(x2+y2−2ρxy)/2(1−ρ2)
for −∞< x, y < ∞. Two views of this probability density function are shown in Figure 5.44
when the correlation is ρ = 0.8.
Notice that this positive correlation tends to associate large values of X with large values
of Y, and similarly small values of X with small values of Y, so that the probablity density
function is concentrated close to the line x = y. Again, both marginal distributions and any
conditional distributions are normally distributed, so that, as before, any “slice” of the joint
probability density function on a plane that is perpendicular to the (x, y)-plane produces a
curve that is proportional to a normal probability density function.
The ideas of a bivariate normal distribution can be extended to a more general multivariate
normal distribution for any dimension. The notation
X ∼Nk(μ, )
indicates that the vector of random variables
X = (X1, . . . , Xk)

260
CHAPTER 5
THE NORMAL DISTRIBUTION
FIGURE 5.44
A bivariate normal distribution
with correlation ρ = 0.8
x
y
x
y
has a k-dimensional multivariate normal distribution with a mean vector
μ = (μ1, . . . , μk)
and a variance-covariance matrix . The variance-covariance matrix is a symmetric k × k
matrix with diagonal elements σ 2
i equal to the variances of the marginal distributions of
the random variables Xi, and off-diagonal terms equal to the covariances of the random
variables.
The joint probability density function of X is
f (x) =
 1
2π
k/2 1
||
1/2
e−(x−μ)′−1(x−μ)/2
where || is the determinant of the matrix . The marginal distributions of the random
variables Xi are
Xi ∼N

μi, σ 2
i

and all of the conditional distributions are also normally distributed. The multivariate normal
distribution is important in the theoretical development of statistical methodologies.

5.4 DISTRIBUTIONS RELATED TO THE NORMAL DISTRIBUTION
261
5.4.6
Problems
5.4.1 Suppose that the random variable X has a lognormal
distribution with parameter values μ = 1.2 and σ = 1.5.
Find:
(a) E(X)
(b) Var(X)
(c) The upper quartile of X
(d) The lower quartile of X
(e) The interquartile range
(f) P(5 ≤X ≤8)
5.4.2 Suppose that the random variable X has a lognormal
distribution with parameter values μ = −0.3 and
σ = 1.1. Find:
(a) E(X)
(b) Var(X)
(c) The upper quartile of X
(d) The lower quartile of X
(e) The interquartile range
(f) P(0.1 ≤X ≤7.0)
5.4.3 Consider a sequence of random variables Xi that are
independently identically distributed with a positive state
space. Explain why the central limit theorem implies that
the random variable
X = X1 × · · · × Xn
has approximately a lognormal distribution for large
values of n.
5.4.4 A researcher grows cultures of bacteria. Suppose that
after one day’s growth, the size of the culture has a
lognormal distribution with parameters μ = 2.3 and
σ = 0.2.
(a) What is the expected size of the culture after one day?
(b) What is the median size of the culture after one day?
(c) What is the upper quartile of the size of the culture
after one day?
(d) What is the probability that the size of the culture
after one day is greater than 15?
(e) What is the probability that the size of the culture
after one day is smaller than 6?
5.4.5 Use your computer package to ﬁnd the following critical
points, and check that they match the values given in
Table II.
(a) χ2
0.10,9
(b) χ2
0.05,20
(c) χ2
0.01,26
(d) χ2
0.90,50
(e) χ2
0.95,6
5.4.6 Use your computer package to ﬁnd the following critical
points:
(a) χ2
0.12,8
(b) χ2
0.54,19
(c) χ2
0.023,32
If the random variable X has a chi-square distribution
with 12 degrees of freedom, use your computer package
to ﬁnd:
(d) P(X ≤13.3)
(e) P(9.6 ≤X ≤15.3)
5.4.7 Use your computer package to ﬁnd the following critical
points, and check that they match the values given in
Table III.
(a) t0.10,7
(b) t0.05,19
(c) t0.01,12
(d) t0.025,30
(e) t0.005,4
5.4.8 Use your computer package to ﬁnd the following critical
points:
(a) t0.27,14
(b) t0.09,22
(c) t0.016,7
If the random variable X has a t-distribution with
22 degrees of freedom, use your computer package to
ﬁnd:
(d) P(X ≤1.78)
(e) P(−0.65 ≤X ≤2.98)
(f) P(|X| ≥3.02)
5.4.9 Use your computer package to ﬁnd the following critical
points, and check that they match the values given in
Table IV.
(a) F0.10,9,10
(b) F0.05,6,20
(c) F0.01,15,30
(d) F0.05,4,8
(e) F0.01,20,13
5.4.10 Use your computer package to ﬁnd the following critical
points:
(a) F0.04,7,37
(b) F0.87,17,43
(c) F0.035,3,8
If the random variable X has an F-distribution with
degrees of freedom ν1 = 5 and ν2 = 33, use your
computer package to ﬁnd:
(d) P(X ≥2.35)
(e) P(0.21 ≤X ≤2.92)
5.4.11 If the random variable X has a t-distribution with
ν degrees of freedom, explain why the random variable
Y = X2 has an F-distribution with degrees of freedom 1
and ν.
5.4.12 (a) There is a probability of 0.90 that a t random variable
with 23 degrees of freedom lies between −x and x.
Find the value of x.

262
CHAPTER 5
THE NORMAL DISTRIBUTION
(b) There is a probability of 0.975 that a t random
variable with 60 degrees of freedom is larger than y.
Find the value of y.
(c) What is the probability that a chi-square random
variable with 29 degrees of freedom takes a value
between 19.768 and 42.557?
5.4.13 The probability that an F5,20 random variable takes
a value greater than 4.00 is A. greater than 10%,
B. between 5% and 10%, C. between 1% and 5%,
or D. less than 1%?
5.4.14 The probability that a t35 random variable takes
a value greater than 2.50 is A. greater than 10%,
B. between 5% and 10%, C. between 1% and 5%,
or D. less than 1%?
5.4.15 Use the tables to put bounds on these probabilities.
(a) P(F10,50 ≥2.5)
(b) P(χ2
17 ≤12)
(c) P(t24 ≥3)
(d) P(t14 ≥−2)
5.4.16 Use the tables to put bounds on these probabilities.
(a) P(t21 ≤2.3)
(b) P(χ2
6 ≥13.0)
(c) P(t10 ≤−1.9)
(d) P(t7 ≥−2.7)
5.4.17 Use the tables to put bounds on these probabilities.
(a) P(t16 ≤1.9)
(b) P(χ2
25 ≥42.1)
(c) P(F9,14 ≤1.8)
(d) P(−1.4 ≤t29 ≤3.4)
5.4.18 A t-distribution with 24 degrees of freedom has a larger
variance than a standard normal distribution.
A. True
B. False
5.5
Case Study: Microelectronic Solder Joints
The thickness of the gold layer at the top of the bonding pad has an important effect on the
quality of the conductive bond that is established between the solder joint and the substrate.
Suppose that a certain manufacturing process produces a gold layer thickness that is normally
distributed with a mean of 0.08 microns and a standard deviation of 0.01 microns. The prob-
ability that the gold layer thickness on a particular bond is within the range 0.075 to 0.085
microns can be calculated as
P(0.075 ≤N

0.08, 0.012
≤0.085)
= P
0.075 −0.08
0.01
≤N(0, 1) ≤0.085 −0.08
0.01

= (0.5) −(−0.5) = 0.6915 −0.3085 = 0.3830
Furthermore, if an assembly consists of 16 solder joints and the thicknesses of the gold layers
on the bond pads are independent of each other, the probability that the average gold layer
thickness lies within the range 0.075 to 0.085 microns can be calculated to be
P

0.075 ≤N

0.08, 0.012
16

≤0.085

= P
0.075 −0.08
0.0025
≤N(0, 1) ≤0.085 −0.08
0.0025

= (2) −(2) = 0.9772 −0.0228 = 0.9544
Recall that there is a probability of 0.12 that a solder joint has an hourglass shape, and
suppose that an assembly consists of 512 solder joints. Then if the solder joint shapes are
independent of each other, the number of hourglass-shaped solder joints on the assembly will
have a B(512, 0.12) distribution. This has a mean of 512 × 0.12 = 61.44 and a variance
of 512 × 0.12 × 0.88 = 54.0672, and so it can be approximated by a N(61.44, 54.0672)

5.7
SUPPLEMENTARY PROBLEMS
263
distribution. The probability that there will be no more than 50 hourglass-shaped solder joints
on the assembly can therefore be estimated to be
P(B(512, 0.12) ≤50) ≃P(N(61.44, 54.0672) ≤50.5)
= P

N(0, 1) ≤50.5 −61.44
√
54.0672

= (−1.488) = 0.068
5.6
Case Study: Internet Marketing
The number of visitors to a start-up organisation’s website within a week is normally dis-
tributed with a mean of 1200 and a standard deviation of 130. What is the probability that the
organisation will get at least 5250 visitors over a 4-week period?
The number of visitors over a 4-week period has an expectation of 4×1200 = 4800 and a
variance of 4 × 1302 = 67600 so that the standard deviation is
√
67600 = 260. The required
probability is therefore
P(N(4800, 2602) ≥5250) = 1 −P(N(4800, 2602) ≤5250)
= 1 −P

N(0, 1) ≤5250 −4800
260

= 1 −P (N(0, 1) ≤1.731) = 1 −0.958 = 0.042
5.7
Supplementary Problems
5.7.1 The amount of sulfur dioxide escaping from the ground in
a certain volcanic region in one day is normally
distributed with a mean μ = 500 tons and a standard
deviation σ = 50 tons under ordinary conditions.
However, if a volcanic eruption is imminent, there are
much larger sulfur dioxide emissions.
(a) Under ordinary conditions, what is the probability of
there being a daily sulfur dioxide emission larger
than 625 tons?
(b) What is the 99th percentile of daily sulfur dioxide
emissions under ordinary conditions?
(c) If your instruments indicate that 700 tons of sulfur
dioxide have escaped from the ground on a particular
day, would you advise that an eruption is imminent?
Why? How sure would you be?
5.7.2 The breaking strengths of nylon ﬁbers in dynes are
normally distributed with a mean of 12,500 and a
variance of 200,000.
(a) What is the probability that a ﬁber strength is more
than 13,000?
(b) What is the probability that a ﬁber strength is less
than 11,400?
(c) What is the probability that a ﬁber strength is
between 12,200 and 14,000?
(d) What is the 95th percentile of the ﬁber strengths?
5.7.3 Adult salmon have lengths that are normally distributed
with a mean of μ = 70 cm and a standard deviation of
σ = 5.4 cm.
(a) What is the probability that an adult salmon is longer
than 80 cm?
(b) What is the probability that an adult salmon is shorter
than 55 cm?
(c) What is the probability that an adult salmon is
between 65 and 78 cm long?
(d) What is the value of c for which there is a 95%
probability that an adult salmon has a length within
the interval [70 −c, 70 + c]?
5.7.4 Consider again Problem 5.6.3 where the lengths of adult
salmon have N(70, 5.42) distributions.
(a) If you go ﬁshing with a friend, what is the probability
that the ﬁrst adult salmon you catch is longer than the
ﬁrst adult salmon your friend catches?

264
CHAPTER 5
THE NORMAL DISTRIBUTION
(b) What is the probability that the ﬁrst adult salmon you
catch is at least 10 cm longer than the ﬁrst adult
salmon your friend catches?
(c) What is the probability that the average length of the
ﬁrst two adult salmon you catch is at least 10 cm
longer than the ﬁrst adult salmon your friend catches?
5.7.5 Suppose that the lengths of plastic rods produced by a
machine are normally distributed with a mean of 2.30 m
and a standard deviation of 2 cm. If two rods are placed
side by side, what is the probability that the difference in
their lengths is less than 3 cm?
5.7.6 A new 1.5-volt battery has an actual voltage that is
uniformly distributed between 1.43 and 1.60 volts.
Estimate the probability that the sum of the voltages from
120 new batteries lies between 180 and 182 volts.
5.7.7 The germination time in days of a newly planted seed is
exponentially distributed with parameter λ = 0.31. If the
germination times of different seeds are independent of
one another, estimate the probability that the average
germination time of 2000 seeds is between 3.10 and
3.25 days.
5.7.8 A publisher sends out advertisements in the mail
asking people to subscribe to a magazine. Suppose that
there is a probability of 0.06 that a recipient of the
advertisement does subscribe to the magazine. If 350,000
advertisements are mailed out, estimate the probability
that the magazine gains at least 20,800 new subscribers.
5.7.9 Suppose that if I invest $1000 today in a high-risk
new-technology company, my return after 10 years has a
lognormal distribution with parameters μ = 5.5 and
σ = 2.0.
(a) What are the median, upper, and lower quartiles of
my 10-year return?
(b) What is the probability that my 10-year return is at
least $75,000?
(c) What is the probability that my 10-year return is less
than $1000?
5.7.10 Recall Problem 3.4.8, where the number of misrecorded
pieces of information in a scanning process has a Poisson
distribution with parameter λ = 9.2. Estimate the
probability that there are fewer than 1000 total pieces of
misrecorded information when 100 different scans are
performed.
5.7.11 When making a connection at an airport, Jasmine arrives
on a plane that is due to arrive at 2:15 P.M. However, the
amount by which her plane arrives late has a normal
distribution with a mean μ = 32 minutes and a standard
deviation σ = 11 minutes. Jasmine wants to transfer to a
plane that is due to depart at 3:25 P.M., although the actual
departure time is late by an amount that is normally
distributed with a mean μ = 10 minutes and a standard
deviation σ = 3 minutes. If Jasmine needs 30 minutes at
the airport to get from the arrival gate to the departure
gate, what is the probability that she will be able to make
her connection?
5.7.12 A clinic has four different physicians, A, B, C, and D, one
of whom is selected by each new patient. If the new
patients are equally likely to choose each of the four
physicians independently of each other, estimate the
probability that physician A will get at least 25 out of the
next 80 new patients. If physician D leaves the clinic and
the new patients are equally likely to select each of the
remaining three physicians, what then is the probability
that physician A will get at least 25 out of the next 80 new
patients?
5.7.13 An aircraft can seat 220 passengers, and each of the
passengers booked on the ﬂight has a probability of 0.9 of
actually arriving at the gate to board the plane,
independent of the other passengers.
(a) Suppose the airline books 235 passengers on the
ﬂight. What is the probability that there will be
insufﬁcient seats to accommodate all of the
passengers who wish to board the plane?
(b) If the airline wants to be 75% conﬁdent that there will
be no more than 220 passengers who wish to board
the plane, how many passengers can be booked on
the ﬂight?
5.7.14 (a) What is the probability that a random variable with a
standard normal distribution takes a value between
0.6 and 2.2?
(b) What is the probability that a random variable with a
normal distribution with μ = 4.1 and σ = 0.25 takes
a value between 3.5 and 4.5?
(c) What is the probability that a random variable with a
chi-square distribution with 28 degrees of freedom
takes a value between 16.928 and 18.939?
(d) What is the probability that a random variable with a
t-distribution with 22 degrees of freedom takes a
value between −1.717 and 2.819?
5.7.15 Components have lifetimes in minutes that are
independent of each other with a lognormal distribution
with parameters μ = 3.1 and σ = 0.1. Suppose that a

5.7 SUPPLEMENTARY PROBLEMS
265
random sample of 200 components is taken. What is the
probability that 30 or more of the components will have a
lifetime of at least 25 minutes?
5.7.16 Are the following statements true or false?
(a) A t-distribution with 60 degrees of freedom has a
larger variance than a standard normal distribution.
(b) The probability that a normal random variable with
mean 10 and standard deviation 2 is less than 14 is
equal to the probability that a normal random
variable with mean 20 and standard deviation 3 is
greater than 14.
(c) P(χ2
30 ≤42) ≥0.90
(d) P(−2 ≤t9 ≤2) ≤0.95
(e) P(F10,15 ≥6.5) ≤0.01
5.7.17 When an order is placed with a company, there is a
probability of 0.2 that it is an express order. Estimate the
probability that 90 or more of the next 400 orders will be
express orders.
5.7.18 In genetic proﬁling, the expression of a gene is measured
for a set of different samples. Suppose that the
expressions are modeled as being independently normally
distributed with a mean 0.768 and a standard deviation
0.083.
(a) If six samples are measured, what is the probability
that at least half of them have expressions larger than
0.800?
(b) If six samples are measured, what is the probability
that two of them have expressions smaller than 0.700,
two of them have expressions between 0.700 and
0.750, and the remaining two have expressions larger
than 0.780?
(c) If the samples are tested sequentially, what is the
probability that the sixth sample tested is the
third sample with an expression smaller
than 0.760?
(d) If the samples are tested sequentially, what is the
probability that the ﬁfth sample tested is the ﬁrst
sample with an expression smaller than 0.680?
(e) Suppose that ten samples are tested and exactly ﬁve
of them have expressions smaller than 0.750.
Furthermore, six samples are randomly selected from
these ten samples and are sent to another laboratory.
What is the probability that exactly half of the
samples sent to the other laboratory have an
expression smaller than 0.750?
5.7.19 Suppose that electrical components have lifetimes that are
independent and that come from a normal distribution
with a mean of 8200 minutes and a standard deviation of
350 minutes.
(a) If three components are selected, what is the
probability that one lasts for less than 8000 minutes,
one lasts between 8000 and 8300 minutes, and
one lasts for more than 8300 minutes?
(b) A consumer buys a box of ten components. What is
the probability that the sixth component that the
consumer uses is the second one to last less than
7900 minutes?
(c) If seven components are selected, what is the
probability that exactly three of them last for more
than 8500 minutes?
5.7.20 The time taken by operator A to ﬁnish a task has a normal
distribution with a mean 220 minutes and a standard
deviation 11 minutes. The time taken by operator B to
ﬁnish a task has a normal distribution with a mean 185
minutes and a standard deviation 9 minutes, independent
of operator A. Operator A began working at 9 A.M. The
probability that operator A ﬁnishes before operator B is
0.90. What time did operator B start working?
5.7.21 When users connect to a server, the lengths of time in
minutes that they are connected are independently
distributed with a Weibull distribution with λ = 0.03 and
a = 0.8.
(a) Suppose that 5 users connect to the server. What is
the probability that 2 of the users are connected for a
time less than 30 minutes and that 3 of the users are
connected for a time greater than 30 minutes?
(b) Suppose that 500 users connect to the server. What is
the probability that no more than 210 of the users are
connected for a time greater than 30 minutes?
5.7.22 Tiles have weights that are independently normally
distributed with a mean of 45.3 and a standard deviation
of 0.02. What is the probability that the total weight of
three tiles is no more than 135.975?
5.7.23 Components of type A have lengths that are
independently normally distributed with a mean of 67.2
and a standard deviation of 1.9. Components of type B
have lengths that are independently normally distributed
with a mean of 33.2 and a standard deviation of 1.1.
What is the probability that two components of type B
will have a total length shorter than one component of
type A?
5.7.24 Suppose that the failure time of a component is modeled
with an exponential distribution with a mean of 32 days.

266
CHAPTER 5
THE NORMAL DISTRIBUTION
A company acquires a batch of 240 components. If the
failure times of these components are taken to be
independent of each other, estimate the probability that
at least half of the components will last longer than
25 days.
5.7.25 Machine A produces components with holes whose
diameter is normally distributed with a mean 56,000 and
a standard deviation 10. Machine B produces components
with holes whose diameter is normally distributed with a
mean 56,005 and a standard deviation 8. Machine C
produces pins whose diameter is normally distributed
with a mean 55,980 and a standard deviation 10.
Machine D produces pins whose diameter is normally
distributed with a mean 55,985 and a standard deviation 9.
(a) What is the probability that a pin from machine C will
have a larger diameter than a pin from machine D?
(b) What is the probability that a pin from machine C will
ﬁt inside the hole of a component from machine A?
(c) If a component is taken from machine A and a
component is taken from machine B, what is the
probability that both holes will be smaller than
55,995?
5.7.26 (a) What is the probability that a t random variable with
40 degrees of freedom lies between −1.303 and
2.021?
(b) Use Table III to put bounds on the probability that a t
random variable with 17 degrees of freedom is
greater than 2.7.
5.7.27 Use the tables to put bounds on these probabilities.
(a) P(F16,20 ≤2)
(c) P(t29 ≥1.5)
(e) P(t10 ≥−2)
(b) P(χ2
28 ≥47)
(d) P(t7 ≤−1.3)
5.7.28 Use the tables to put bounds on these probabilities.
(a) P(χ2
40 > 65.0)
(c) P(t26 < 3.0)
(b) P(t20 < −1.2)
(d) P(F8,14 > 4.8)
5.7.29 A patient has a doctor’s appointment that is scheduled
for 9:40 A.M. However, the amount of time after the
scheduled time that the doctor’s consultation actually
starts has a normal distribution with a mean of 22 minutes
and a standard deviation of 4 minutes. The doctor’s
consultation lasts for a period that has a normal
distribution with a mean of 17 minutes and a standard
deviation of 5 minutes. After the doctor’s consultation
has ﬁnished, the patient visits the laboratory and then the
pharmacy. It takes the patient 1 minute to go from the
doctor’s consultation to the laboratory, and 1 minute to go
from the laboratory to the pharmacy. The amount of time
spent at the laboratory has a normal distribution with a
mean of 11 minutes and a standard deviation of 3
minutes, and the amount of time spent at the pharmacy
has a normal distribution with a mean of 15 minutes and
a standard deviation of 5 minutes. If the times taken by
each component of the patient’s visit are all independent,
what is the probability that the patient will be ﬁnished by
11:00 A.M?
5.7.30 Suppose that the time taken by a salesperson to close a
deal is normally distributed with a mean of 3 hours
and a standard deviation of 20 minutes. What is the
probability that a deal can be closed in less than 3 and
a half hours?
A. 0.893
C. 0.933
B. 0.913
D. 0.953
5.7.31 (Problem 5.7.30 continued) Suppose that a salesperson
works on one deal, and as soon as that is closed
immediately starts working on another deal. What is the
probability that the total time taken to close both deals is
less than 7 hours?
A. 0.931
C. 0.983
B. 0.952
D. 0.995
5.7.32 An investment in company I has an expected return of
$150,000 and a standard deviation of $30,000. If the
return is normally distributed, what is the probability that
it will be no more than $175,000?
5.7.33 An investment in company I has an expected return of
$150,000 and a standard deviation of $30,000. An
investment in company II has an expected return of
$175,000 and a standard deviation of $40,000. If the
returns are normally distributed and independent, what is
the probability that the return from company II is at least
$50,000 more than the return from company I?

C H A P T E R S I X
Descriptive Statistics
Now that probability theory has been presented, an important change occurs at this point in the
book. The ﬁrst ﬁve chapters on probability theory described how the properties of a random
variable can be understood using the probability mass function or probability density function
of the random variable. For these purposes the probability mass function or probability density
function was taken to be known.
Of course, in most applications the probability mass function or probability density func-
tion of a random variable is not known by an experimenter, and one of the ﬁrst tasks of the
experimenter is to ﬁnd out as much as possible about the probability distribution of the random
variable under consideration. This is done through experimentation and the collection of a
data set relating to the random variable. The science of deducing properties of an underlying
probability distribution from such a data set is known as the science of statistical inference.
In this chapter the collection of a data sample from an overall population is discussed,
together with various basic data investigations. These include initial graphical presentations
of the data set and the calculation of useful summary statistics of the data set.
6.1
Experimentation
6.1.1
Samples
Consider Example 1 concerning machine breakdowns, which are classiﬁed as due either to
electrical causes, to mechanical causes, or to operator misuse. The probability mass function
of the breakdowns, that is, the probability values of each of the three causes, summarizes the
breakdown characteristics of the machine. In other words, the probability mass function can
be thought of as summarizing the “true state of nature.”
In practice, however, this underlying probability mass function is unknown. Consequently,
an obvious task of an experimenter or manager who wishes to understand as fully as possible
the breakdowns of the machine is to estimate the probability mass function. This can be done
by collecting a data set relating to the machine breakdowns, which in this case would just be a
record of how many machine breakdowns are actually attributable to each of the three causes.
In general, suppose that a (continuous) random variable X of interest to an experimenter
has a probability density function f (x). With reference to Example 17, X may represent the
amount of milk in a milk container. The probability density function f (x) provides complete
information about the probabilistic properties of the random variable X and is unknown to the
experimenter. Again, it represents the “true state of nature” that the experimenter wishes to
ﬁnd out about.
The experimenter proceeds by obtaining a sample of observations of the random variable
X, which may be written
x1, x2, . . . , xn
For the milk container example, this sample or data set is obtained by weighing the contents
of n milk containers. Since these data observations are governed by the unknown underlying
267

268
CHAPTER 6
DESCRIPTIVE STATISTICS
FIGURE 6.1
The relationship between
probability theory and statistical
inference
Experimentation
Probability theory
Data analysis
Statistical inference
Data set x1,...,xn
Probability distribution f(x)
Estimate properties of f(x)
probabilitydensityfunction f (x),anappropriateanalysisofthedataaffordstheexperimentera
glimpse of f (x). Such an analysis is known as statistical inference, as illustrated in Figure 6.1.
A great deal of care needs to be taken to ensure that the data set is obtained in an ap-
propriate manner. The expression “garbage in, garbage out” is often used by statisticians to
make the point that any statistical analysis based upon inaccurate or poor quality data is nec-
essarily misleading. To judge the quality of a sample of data observations it is often useful to
envision a population of potential observations from which the sample should be drawn in a
representative manner.
For example, in the milk container example the population can be thought of as all of the
milk containers. A representative sample can then be obtained by taking a random sample,
whereby milk containers are chosen at random and weighed. If the milk company has three
machines that ﬁll milk containers, an example of a sample that is potentially not representative
is one in which only containers from two of the three machines are selected and weighed,
since it is possible that the amount of milk in a container may depend upon which machine it
comes from. Indeed, one purpose of the statistical analysis may be to investigate whether there
is any difference between the three machines, using the techniques described in Chapter 11.
The notion of a population is a fairly ﬂexible one, and its deﬁnition depends upon the
particular question being investigated by the experimenter. For example, if the experimenter
speciﬁcally wishes to investigate whether the three ﬁlling machines are actually operating
differently from one another, then conceptually it is appropriate to envision three populations
and three samples. The milk containers ﬁlled by each of the three machines constitute three
different populations, and a random sample can be obtained from each of the three populations
by in turn selecting at random milk containers from each of the three machines.
Populations and Samples
A population consists of all possible observations available from a particular
probability distribution. A sample is a particular subset of the population that an
experimenter measures and uses to investigate the unknown probability distribution. A
random sample is one in which the elements of the sample are chosen at random
from the population, and this procedure is often used to ensure that the sample is
representative of the population.

6.1 EXPERIMENTATION
269
The data observations x1, . . . , xn can be of several general types. Categorical or nominal
data record which of several categories or types an observation takes. A machine breakdown
classiﬁed as either mechanical, electrical, or misuse is an example of a categorical data ob-
servation. If a categorical variable has only two levels, then it is known as a binary variable.
Numerical data may be either integers or real numbers. Such a variable may be referred to as
a continuous variable. It is important to be aware of the type of data that one is dealing with
since it affects the choice of an appropriate analysis, as illustrated in the following examples.
6.1.2
Examples
Example 1
Machine Breakdowns
The engineer in charge of the maintenance of the machine keeps records on the breakdown
causes over a period of a year. Altogether there are 46 breakdowns, of which 9 are attributable
to electrical causes, 24 are attributable to mechanical causes, and 13 are attributable to operator
misuse. This data set is shown in Figure 6.2 and can be used to estimate the probabilities of
the breakdowns being attributable to each of the three causes.
Breakdown cause
Frequency
Electrical
9
Mechanical
24
Misuse
13
Total
46
FIGURE 6.2
Data set of machine breakdowns
Notice that this data set actually consists of 46 categorical observations, x1, . . . , x46, with
each observation taking one of the values
{electrical, mechanical, misuse}
However, the data set can be summarized by simply recording the frequencies of occurrence
of the three categories, as in Figure 6.2.
The 46 breakdowns that occurred over the year constitute the sample of observations
available to the engineer. What is the population from which this sample is drawn? This is a
rather confusing question, but it may be helpful to envisage the population as consisting of
all breakdowns during the year under consideration together with breakdowns from previous
years and future years.
In practice, the most sensible thing for the engineer to do here is to concentrate directly on
whether the sample obtained is a representative one. This judgment can be made only after the
purpose of the data analysis has been decided. If the engineer is conducting the data analysis
in order to predict the types and frequencies of machine breakdowns that will occur in the
future, then the appropriate question is
How representative is this year’s data set of future years?
This question is most easily answered by noticing whether there are any factors that
suggest that the data may not be representative. For example, if the machine was operated this
year by a skilled operator but will be operated next year by an inexperienced trainee, then it is
probably reasonable to anticipate a greater proportion of breakdowns due to operator misuse
next year. Similarly, if it is anticipated that next year the machine must be operated at higher
speeds than this year in order to meet larger production targets, then a greater proportion of
breakdowns due to mechanical reasons may be expected. These are the kinds of issues that
a good statistician will investigate in order to assess the quality and representativeness of the
data set being dealt with.
Example 2
Defective Computer
Chips
Recall that a company sells computer chips in boxes of 500 chips. How can the com-
pany investigate the probability distribution of the number of defective chips per box? Fig-
ure 6.3 shows a data set of 80 observations corresponding to the number of defective chips
found in a random sample of 80 boxes. An appropriate analysis of this data set will reveal

270
CHAPTER 6
DESCRIPTIVE STATISTICS
FIGURE 6.3
Data set of the number of defective
computer chips in a box
1
3
4
7
2
7
5
5
2
2
4
2
4
3
2
2
7
1
3
3
2
5
0
0
1
2
5
5
4
1
3
2
6
3
8
2
2
3
1
6
3
4
1
2
5
1
3
3
3
2
1
2
5
5
4
1
4
3
1
0
2
1
2
4
4
5
3
3
4
0
5
2
5
6
2
5
3
3
3
1
properties of the underlying unknown probability distribution of the number of defective chips
per box.
The population of interest here can be thought of as all the boxes produced by the company
within a certain time period. The representativeness of the sample of 80 boxes examined can
be justiﬁed on the basis that it is a random sample, that is, the 80 boxes have been selected on
some random basis. There are various ways in which this might have been done. For example,
if each box has a code number assigned to it, a table of random numbers or a random-number
generator on a computer may be used to identify the 80 boxes that can be chosen to make up
the random sample.
In contrast, if the boxes are all stored in a warehouse, then boxes may be selected by
randomly choosing aisles and shelves and then randomly selecting a particular box on a shelf.
Alternatively, a random sample may be obtained by selecting every 100th box, say, to come
off a production line.
Number of 
defective chips
0
1
2
3
4
5
6
7
8
≥ 9
Total
Frequency
4
12
18
17
10
12
3
3
1
0
80
FIGURE 6.4
Data set of defective computer chips
For each box selected into the sample, all 500 chips must be tested to determine whether or
not they are defective. The data observations x1, . . . , x80 will then take integer values between
0 and 500. A useful way to summarize the data set is to record the frequencies of the number
of defective chips found, as illustrated in Figure 6.4.
Example 14
Metal Cylinder
Production
In order to investigate the actual probability distribution of the diameters of the cylinders
that it produces, the company selects a random sample of 60 cylinders and measures their
diameters. This data set is shown in Figure 6.5. The population may be the set of all cylinders
produced by the company (within a certain time period) or, if attention is directed at only
one production line, say, the population may be all cylinders produced from that produc-
tion line. The random selection of the 60 cylinders that constitute the sample should ensure
that it is a representative sample. Notice that in this case the data observations x1, . . . , x60,
which represent the diameters in mm of the cylinders, are numbers recorded to two decimal
places.
Example 17
Milk Container
Contents
A random sample of 50 milk containers is selected and their milk contents are weighed. This
data set is shown in Figure 6.6 and it can be used to investigate the unknown underlying
probability distribution of the milk container weights. The population in this experiment is
the collection of all the milk containers produced and, again, the random selection of the
sample should ensure that it is a representative one. Notice that for this experiment, the data
observations x1, . . . , x50, which represent the milk contents in liters, are numbers recorded to
three decimal places.
COMPUTER NOTE
Remember that all of the data sets discussed in the examples and problems are available on
the book’s CD and website.

6.1 EXPERIMENTATION
271
FIGURE 6.5
Data set of metal cylinder
diameters in mm
50.08
49.78
50.02
50.02
50.13
49.74
49.84
49.97
49.93
50.02
50.05
49.94
50.19
49.86
50.03
50.04
49.96
49.90
49.87
50.13
49.81
50.02
50.26
49.90
50.01
50.04
50.01
49.79
50.36
50.21
50.17
50.12
50.00
50.01
49.85
49.93
49.84
50.20
49.94
49.74
50.00
50.03
49.92
50.07
49.89
49.99
50.01
50.09
49.90
50.05
49.95
50.20
50.03
49.92
50.02
49.97
50.27
49.77
50.07
50.07
FIGURE 6.6
Data set of milk weights in liters
1.958
1.951
2.107
2.092
1.955
2.162
2.168
2.134
1.971
2.072
2.049
2.017
2.117
1.977
2.034
2.062
2.110
1.974
1.992
2.018
2.135
2.107
2.084
2.169
2.085
2.018
1.977
2.116
1.988
2.066
2.126
2.167
1.969
2.198
2.078
2.119
2.088
2.172
2.133
2.112
2.066
2.128
2.142
2.042
2.050
2.102
2.000
2.188
1.960
2.128
6.1.3
Problems
Answer these questions for each data set. The data sets
labeled DS are on the accompanying CD.
(a) Deﬁne the population from which the sample is taken.
Do you think that it is a representative sample?
(b) Are there any other factors that should be taken into
account in interpreting the data set? Are there any
issues pertaining to the way in which the sample has
been collected that you would want to investigate
before interpreting the data set?
6.1.1 DS 6.1.1 shows the outcomes obtained from a series of
rolls of a six-sided die. (This problem is continued in
Problems 6.2.5, 6.3.4, and 7.3.10.)
6.1.2 Television Set Quality
One Friday morning at a television manufacturing
company the quality inspector recorded the grades
assigned to the pictures on the television sets that were
ready to be shipped. The grades, presented in DS 6.1.2, are
“perfect,” “good,” “satisfactory,” or “fail.” (This problem is
continued in Problems 6.2.6 and 7.3.11.)
6.1.3 Eye Colors
DS 6.1.3 presents the eye colors of a group of students who
are registered for a course on computer programming.
(This problem is continued in Problems 6.2.7 and 7.3.12.)
6.1.4 Restaurant Service Times
One Saturday a researcher recorded the times taken to
serve customers at a fast-food restaurant. DS 6.1.4 shows
the service times in seconds for all the customers who were
served between 2:00 and 3:00 in the afternoon. (This
problem is continued in Problems 6.2.8, 6.3.5, 7.3.13,
8.1.18, 8.2.16, and 9.3.21.)
6.1.5 Fruit Spoilage
Every day in the summer months a supermarket receives a
shipment of peaches. The supermarket’s quality inspector
arranges to have one box randomly selected from each
shipment for which the number of “spoiled” peaches (out
of 48 peaches in the box), which cannot be put out on the
supermarket shelves, is recorded. DS 6.1.5 shows the data
set obtained after 55 days. (This problem is continued in
Problems 6.2.9, 6.3.6, and 7.3.14.)
6.1.6 Telephone Switchboard Activity
A researcher records the number of calls received by a
switchboard during a 1-minute period. These 1-minute
intervals are chosen at evenly spaced times during a
working week. The data set obtained by the researcher is
shown in DS 6.1.6. (This problem is continued in Problems
6.2.10, 6.3.7, 7.3.15, 8.1.19, and 8.2.17.)

272
CHAPTER 6
DESCRIPTIVE STATISTICS
6.1.7 Paving Slab Weights
A builder orders a large shipment of paving slabs from a
particular company. The weights of a sample of randomly
selected slabs are given in DS 6.1.7. (This problem is
continued in Problems 6.2.11, 6.3.8, 7.3.16, 8.1.20, 8.2.18,
and 9.3.17.)
6.1.8 Spray Painting Procedure
Car panels are spray painted by a machine. An inspector
selects 1 in every 20 panels coming off a production line
and measures the paint thickness at a speciﬁed point on the
panels. The resulting data set is given in DS 6.1.8. (This
problem is continued in Problems 6.2.12, 6.3.9, 7.3.17,
8.1.21, 8.2.19, and 9.3.18.)
6.1.9 Plastic Panel Bending Capabilities
The bending capabilities of plastic panels are investigated
by measuring the angle of bend at which a deformity ﬁrst
appears in the panel. A researcher collects 80 plastic panels
made by a machine and measures their deformity angles.
The resulting data set is shown in DS 6.1.9. (This problem
is continued in Problems 6.2.13, 6.3.10, 7.3.18, 8.1.22, and
8.2.20.)
6.1.10 An analysis is performed of a company’s monthly proﬁts,
which are $1.378 million, $0.837 million, $1.963 million,
and so on.
A. The proﬁts should be analyzed as a continuous
variable.
B. The proﬁts should be analyzed as a categorical
variable.
6.1.11 In market research, a potential new product is tested on
a sample of consumers who rate it as “deﬁnitely
undesirable,” “mixed feelings,” or “deﬁnitely desirable.”
A. The ratings should be analyzed as a continuous
variable.
B. The ratings should be analyzed as a categorical
variable.
6.2
Data Presentation
Once a data set has been collected, the experimenter’s next task is to ﬁnd an informative
way of presenting it. In this chapter various graphical techniques for presenting data sets are
discussed. In general, a table of numbers is not very informative, whereas a picture or graphical
representation of the data set can be quite informative. If “a picture is worth a thousand words,”
then it is worth at least a million numbers.
6.2.1
Bar Charts and Pareto Charts
A bar chart is a simple graphical technique for illustrating a categorical data set. Each
category has a bar whose length is proportional to the frequency associated with that category.
A Pareto chart is a bar chart that is popular in quality control (see Chapter 16) where the
categories are arranged in order of decreasing frequency.
Example 1
Machine Breakdowns
Figure 6.7 shows a bar chart for the data set of 46 machine breakdowns.
Example 41
Internet Commerce
A manager in an Internet-based company that sells a certain range of products from its website
is interested in the causes of customer dissatisfaction. The complaints that the company
received over a certain period of time are classsiﬁed as being due to the late delivery of an
order, to the delivery of a damaged product, to the delivery of a wrong order, to errors in the
billing procedure, or to any other type of complaint, as shown in Figure 6.8. A Pareto chart

6.2 DATA PRESENTATION
273
Electrical Mechanical
Misuse
9
24
13
Frequency
FIGURE 6.7
Bar chart of machine breakdown data set
Cause of complaint
Frequency
Late delivery
Damaged product
Billing error
Other
Wrong product
Total
481
134
44
21
83
763
FIGURE 6.8
Data set of customer complaints for
Internet company
FIGURE 6.9
Pareto chart of customer
complaints for Internet company
Late delivery
Damaged product
Wrong product
Billing error
Other
800
700
600
500
400
300
200
100
0
Count
100
80
60
40
20
0
Defect
Count
Percent
Cumulative %
481 
63.0 
63.0
134 
17.6 
80.6
83 
10.9 
91.5
44 
5.8 
97.2
21 
2.8 
100.0
Percent
for this data set is shown in Figure 6.9. The Pareto chart arranges the causes of the complaints
in order of decreasing frequency and presents a bar chart together with a line representing
the cumulative count. For example, the two most common causes of complaint are the late
delivery of a product and the delivery of a damaged product, which together account for 80.6%
of all of the complaints.
HISTORICAL NOTE
Vilfredo Pareto (1848–1923)
was an Italian economist and
sociologist who was interested
in the application of
mathematics to economic
analysis. He studied
engineering at the University of
Turin in Italy and graduated in
1870 with a thesis entitled “The
fundamental principles of
equilibrium in solid bodies.”
After working as an engineer,
Pareto was appointed in 1893
to the chair of political
economy at the University of
Lausanne, Switzerland. He
died in Geneva in 1923.

274
CHAPTER 6
DESCRIPTIVE STATISTICS
A
B
C
80
90
100
Frequency
FIGURE 6.10
Bar chart with truncated frequency axis
A
B
C
80
90
100
70
60
50
40
30
20
10
Frequency
FIGURE 6.11
Bar chart without truncated frequency axis
When one looks at bar charts it is always prudent to check the frequency axis to make
sure that it has not been truncated. The bar chart shown in Figure 6.10 conveys the visual
impression that category C has a frequency at least twice as large as the frequencies of the
other categories. However, the frequency axis has been truncated and does not start at zero.
Figure 6.11 shows the bar chart without truncation of the axis.
COMPUTER NOTE
Find out how to obtain bar charts on your software package. Do not confuse bar charts with
histograms, which are discussed in Section 6.2.3 and which look similar. Most spreadsheet
and data management packages will produce bar charts for you.
6.2.2
Pie Charts
Pie charts are an alternative way of presenting the frequencies of categorical data in a
graphical manner. A pie chart emphasizes the proportion of the total data set that is taken
up by each of the categories. If a data set of n observations has r observations in a speciﬁc
category, then that category receives a “slice” of the pie with an angle of
r
n × 360◦
This means that the angles of the pie slices are proportional to the frequencies of the various
categories. Even though pie charts are a very simple graphical tool, their effectiveness should
not be underestimated.
Example 1
Machine Breakdowns
Figure 6.12 shows a pie chart for the data set of 46 machine breakdowns. Notice that this chart
immediately conveys the information that more than half of the breakdowns were attributable
to mechanical causes.
Example 41
Internet Commerce
Figure 6.13 shows a pie chart of the customer complaints for the Internet company. The late
delivery of orders is clearly seen to generate the most customer complaints.
COMPUTER NOTE
Find out how to obtain pie charts on your software package. Again, most spreadsheet and data
management packages will produce pie charts for you.

6.2 DATA PRESENTATION
275
Mechanical
Electrical
Misuse
24
9
13
FIGURE 6.12
Pie chart for machine breakdowns data set
Late (63.0%)
Other (2.8%)
Billing (5.8%)
Wrong (10.9%)
Damaged (17.6%)
FIGURE 6.13
Pie chart of customer complaints for Internet company
6.2.3
Histograms
Histograms look similar to bar charts, but they are used to present numerical or continuous
data rather than categorical data. In bar charts the “x-axis” lists the various categories under
consideration, whereas in histograms the “x-axis” is a numerical scale. A histogram consists
of a number of bands whose length is proportional to the number of data observations that
take a value within that band. An important consideration in the construction of a histogram
is an appropriate choice of the bandwidth.
Example 2
Defective Computer
Chips
Figure 6.14 shows a histogram of the data set in Figure 6.4, which consists of the number of
defective chips found in a sample of 80 boxes. Since the data observations are the integers
0, 1, 2, . . . , 8, the bands of the histogram are chosen to be
(−0.5, 0.5], (0.5, 1.5], . . . , (7.5, 8.5]
The histogram shows that no defectives were found in 4 of the boxes, exactly one defective
was found in 12 of the boxes, and so on.
The histogram in Figure 6.14 provides a graphical indication of the shape of the probability
mass function of the number of defective chips in a box. It suggests that the probability mass
function increases to a peak value at either 2 or 3 and then decreases rapidly. In other words,
the actual probability mass function can be thought of as being a smoothed version of the
histogram. An important question to ask is
How close is the shape of the histogram to the actual shape of the probability mass
function?
This question is addressed in later chapters using some technical statistical inference tools.
Example 14
Metal Cylinder
Production
Figure 6.15 shows a histogram of the data set of metal cylinder diameters given in Figure 6.5.
The bandwidth is chosen to be 0.04 mm. Figure 6.16 shows how the histogram changes when
bandwidths of 0.10 mm and 0.02 mm are employed.

276
CHAPTER 6
DESCRIPTIVE STATISTICS
0 1
5 6
2 3 4
7 8
5
10
15
20
Frequency
FIGURE 6.14
Histogram of computer chips data set
49.9
50.1
49.7
50.3
2
4
6
8
10
Frequency
Diameter (mm)
FIGURE 6.15
Histogram of metal cylinder diameter data set
FIGURE 6.16
Histograms of metal cylinder
diameter data set with different
bandwidths
50.2
49.8
49.9
50.1
50.4
50.0
49.7
50.3
10
20
Frequency
Frequency
Diameter (mm)
50.1
49.8
49.9
50.4
50.0
49.7
50.2
50.3
2
4
6
Diameter (mm)

6.2 DATA PRESENTATION
277
Common sense should be used to decide what is the best bandwidth when constructing a
histogram. If the bandwidth is too large, then the histogram fails to convey all of the “structure”
within the data set. On the other hand, if the bandwidth is too small, then the histogram becomes
too “spiky” and may have gaps in it. For this data set, a bandwidth of 0.04 mm seems to be
about right.
The shape of the histogram in Figure 6.15 provides an indication of the shape of the
unknown underlying probability density function of the cylinder diameters. It is to be expected
that the actual probability density function is some smooth curve that mimics the shape of the
histogram.
Would you guess that the true probability density function is symmetric? This seems to be
possible, since the histogram appears to be fairly symmetric about a value close to 50.00 mm.
In Section 2.2.2, for illustrative purposes the probability density function of the metal cylinders
was taken to be
f (x) = 1.5 −6(x −50.0)2
for 49.5 ≤x ≤50.5, which is drawn in Figure 2.21. In view of the data that are now available,
does this look like a plausible probability density function? The answer is probably not, since
the histogram appears to have longer, ﬂatter tails than the probability density function drawn
in Figure 2.21. In fact, the histogram in Figure 6.15 appears to have a shape fairly similar to
that of a normal distribution.
Example 17
Milk Container
Contents
Figure 6.17 shows a histogram of the data set of milk weights given in Figure 6.6. The band
intervals employed are
(1.95, 1.97], (1.97, 1.99], . . . , (2.19, 2.21]
In Section 2.2.2 a probability density function of
f (x) = 40.976 −16x −30e−x
for 1.95 ≤x ≤2.20 was assumed for the milk content weights, which is drawn in Figure 2.24.
In retrospect, given the data set at hand, does this density function appear reasonable? It’s
difﬁcult to say with this sample size. The histogram in Figure 6.17 appears to have a shape
FIGURE 6.17
Histogram of milk weights
1.99
6
4
2.03
2.15
2.07 2.11
1.95
2.19
2
8
Frequency
Weight (liters)

278
CHAPTER 6
DESCRIPTIVE STATISTICS
Frequency
FIGURE 6.18
A histogram with positive skewness
Frequency
FIGURE 6.19
A histogram with negative skewness
fairly similar to the probability density function drawn in Figure 2.24, except for the “spike”
at about 2.12 liters and the “dip” between 2.00 and 2.04 liters. The shape of the probability
density function will become clearer if a histogram is constructed from a larger sample size.
When one looks at a histogram, one of the ﬁrst considerations is usually to determine
whether or not it appears to be symmetric. For histograms that are not symmetric, it is often
useful to talk about skewness. The histogram in Figure 6.18 is said to be right-skewed or
positively skewed because its right-hand tail is much longer and ﬂatter than its left-hand
tail. Similarly, the histogram in Figure 6.19 is said to be left-skewed or negatively skewed
because its left-hand tail is much longer and ﬂatter than its right-hand tail. Skewness can
also be used to describe probability density functions. For example, the Weibull distribution
shown in Figure 4.19 is slightly positively skewed, whereas the beta distribution (a = 4,
b = 2) shown in Figure 4.22 is negatively skewed.
Of course, not all histograms are unimodal. Figure 6.20 shows a histogram that is bimodal
since it has two separate peaks. How should such a histogram be interpreted? It may be that the
data set is actually a combination of two data sets corresponding to two different probability
distributions. For example, a data set measuring some attribute of “people” may more usefully
be separated into one data set for men and one for women.
COMPUTER NOTE
Find out how to draw histograms using your software package. Again, most spreadsheet and
data management packages will also produce histograms for you. Find out how to change
the bandwidths and band center points of histograms. You’ll probably ﬁnd that your package
chooses these for you automatically unless you specify them.
6.2.4
Outliers
Graphical presentations of a data set sometimes indicate odd-looking data points that don’t
seem to ﬁt in with the rest of the data set. For example, consider the histogram shown in
Figure 6.21. This indicates a fairly symmetric distribution centered close to zero, except for
one strange data point lying at about 4.5. Such a data point may be considered to be an outlier,

6.2 DATA PRESENTATION
279
FIGURE 6.20
A histogram for a bimodal
distribution
Frequency
FIGURE 6.21
Histogram of a data set with a
possible outlier
4.5
0
Outlier ?
Frequency
and in general, outliers can be deﬁned to be data points that appear to be separate from the
rest of the data set.
Should outliers be removed from the data set? It is usually sensible to remove outliers
from the data set before any statistical inference techniques (discussed in subsequent chapters)
are applied. The experimenter should certainly notice the outlier and investigate the data
observation to see whether there is anything special about it. In many cases an outlier will be
discovered to be a misrecorded data observation and can be corrected. In other cases it may be
discovered that the data point corresponds to some special conditions that were not in effect
when the other data points were collected.
The important lesson here is that an experimenter should be aware of outliers in a data set,
as identiﬁed through a graphical presentation of the data set, and should take steps to deal with
them in an appropriate manner. The basic issue is whether the outlier represents true variation
in the population under consideration or whether it is caused by an “outside” inﬂuence.

280
CHAPTER 6
DESCRIPTIVE STATISTICS
6.2.5
Problems
6.2.1 Fabric Types
DS 6.2.1 shows a data set of fabric types. Construct a bar
chart and a pie chart for the data set.
6.2.2 Software Evaluations
DS 6.2.2 shows the evaluations of a new piece of software
from a group of 60 trial users. Construct a bar chart and a
pie chart for the data set.
6.2.3 Piston Rod Lengths
DS 6.2.3 shows the lengths of 30 piston rods. Construct a
histogram of the data set with appropriate band widths.
Do you think that there are any outliers in the data set?
(This problem is continued in Problem 6.3.2.)
6.2.4 Physical Training Course Completion Times
DS 6.2.4 shows the times taken by 25 students to ﬁnish a
physical training course. Construct a histogram of the
data set with appropriate band widths. Do you think that
there are any outliers in the data set? (This problem is
continued in Problem 6.3.3.)
Use a statistical software package to obtain appropriate graphical
presentations of each of the following data sets. Obtain more than
one graphical presentation where appropriate. Indicate any data
observations that might be considered to be outliers. What do
your pictures tell you about the data sets?
6.2.5 The data set of die rolls given in DS 6.1.1.
6.2.6 Television Set Quality
The data set of television picture grades given in DS 6.1.2.
6.2.7 Eye Colors
The data set of eye colors given in DS 6.1.3.
6.2.8 Restaurant Service Times
The data set of service times given in DS 6.1.4.
6.2.9 Fruit Spoilage
The data set of spoiled peaches given in DS 6.1.5.
6.2.10 Telephone Switchboard Activity
The data set of calls received by a switchboard given in
DS 6.1.6.
6.2.11 Paving Slab Weights
The data set of paving slab weights given in DS 6.1.7.
6.2.12 Spray Painting Procedure
The data set of paint thicknesses given in DS 6.1.8.
6.2.13 Plastic Panel Bending Capabilities
The data set of plastic panel bending capabilities given in
DS 6.1.9.
6.2.14 Explain the difference between a bar chart and a
histogram.
6.2.15 A categorical data set is obtained from a marketing study
where consumers were asked to rate a new product as
either “poor,” “satisfactory,” “good,” or “excellent.” A
boxplot would be a useful way to investigate whether
there is any skewness in this data set.
A. True
B. False
6.2.16 An outlier is:
A. A data point that the experimenter does not like
B. An unusually large or an unusually small data point
that should always be removed from the data set
C. An unusually large or an unusually small data point
that does not have to be removed from the data set
D. A sign that there will be problems with the data
analysis
6.2.17 When choosing an appropriate graphical method for a
data set:
A. Neither a pie chart nor a bar chart should be used for
categorical data.
B. Either a boxplot or a bar chart can be used for
categorical data, but not a histogram.
C. Either a pie chart or a bar chart can be used for
categorical data, but not a histogram.
D. Either a pie chart, a histogram, or a bar chart can be
used for categorical data.
6.3
Sample Statistics
Sample statistics such as the sample mean, the sample median, and the sample standard
deviation provide numerical summary measures of a data set in the same way that the expec-
tation, median, and standard deviation provide numerical summary measures of a probability
distribution.

6.3 SAMPLE STATISTICS
281
FIGURE 6.22
Illustrative data set
0.9
1.3
1.8
2.5
2.6
2.8
3.6
4.0
4.1
4.2
4.3
4.3
4.6
4.6
4.6
4.7
4.8
4.9
4.9
5.0
Sample mean
x = 0.9 + ··· + 5.0
20
= 3.725
Sample median
4.2 + 4.3
2
= 4.25
Sample trimmed mean
1.8 + ··· + 4.9
16
= 3.90
¯
6.3.1
Sample Mean
The sample mean of a data set ¯x is simply the arithmetic average of the data observations.
Speciﬁcally, if a data set consists of the n observations x1, . . . , xn, then the sample mean is
¯x =
n
i=1 xi
n
The sample mean can be thought of as indicating a “middle value” of the data set in the
same way that the expectation E(X) of a random variable X indicates a “middle value” of
the probability distribution of X. Moreover, the sample mean ¯x can be thought of as being
an estimate of the expectation of the unknown underlying probability distribution of the
observations in the data set. Statistical estimation is discussed in more detail in Chapter 7.
Figure 6.22 shows a data set of 20 observations that have a sample mean ¯x = 3.725.
6.3.2
Sample Median
The sample median is the value of the “middle” of the ordered data points. For example, if a
data set consists of 31 observations, the sample median is the 16th largest data point, so that
there are at least 15 data points no larger than the sample median and at least 15 data points no
smaller than the sample median. If a data set consists of 30 observations, say, then the sample
median is usually taken to be the average of the 15th and 16th largest data points.
The sample median can be considered to be an estimate of the median value of the unknown
underlying probability distribution of the observations in the data set. The relationship between
a sample mean and a sample median is similar to the relationship between the expectation and
median of a probability distribution. A symmetric sample has a sample mean and a sample
median roughly equal. However, a sample with positive skewness has a sample mean larger
than the sample median, and a sample with negative skewness has a sample mean smaller than
the sample median.
For example, consider the sample of salaries of professional athletes in a major sport.
Typically, a small group of athletes earn salaries vastly higher than their fellow athletes so
that the sample of salaries has positive skewness. What then is an “average salary”? The mean
salary is inﬂuenced by the few very large salaries, so that considerably fewer than half of the
athletes earn more than the mean salary. However, the median salary, which is smaller than
the mean salary, may be more appropriate as an “average salary” since half of the athletes
earn less than the median amount and half of the athletes earn more than the median amount.
In Figure 6.22, the 10th and the 11th largest data observations are 4.2 and 4.3, so that
the sample median is 4.25. Notice that this is larger than the sample mean 3.725 due to the
negative skewness of the sample.

282
CHAPTER 6
DESCRIPTIVE STATISTICS
6.3.3
Sample Trimmed Mean
A trimmed mean is obtained by deleting some of the largest and some of the smallest data
observations, and by then taking the mean of the remaining observations. Usually a 10%
trimmed mean is employed, whereby the top 10% of the data observations are removed
together with the bottom 10% of the data points. For example, if there are n = 50 data
observations, then the largest 5 and the smallest 5 are removed, and the mean is taken of the
remaining 40 data points.
The advantage of a trimmed mean compared with a general sample mean is that the
trimmed mean is not as sensitive to the tails of the data set as the overall mean. In particular,
if the data set contains an outlier, then this affects the sample mean but does not affect the
trimmed mean since the outlier will be one of the points removed. On the other hand, the
tails of the sample may consist of valid data points, in which case the trimmed mean is
“wasteful” because it does not use these data points. The trimmed mean is often referred to
as a robust estimator of the expectation of the unknown underlying probability distribution
of the observations in the data set since it is not sensitive to the largest and smallest elements
of the data set.
The trimmed mean in Figure 6.22 is calculated as the average of the 16 data points when
the 2 largest and 2 smallest data points have been removed. It has a value of 3.90, which is
larger than the overall sample mean 3.725 but smaller than the sample median 4.25. Again, this
is due to the negative skewness of the data set. In general, a trimmed mean can be considered
to be a compromise between the sample mean and the sample median, as shown in Figure 6.23
for positively and negatively skewed data sets.
6.3.4
Sample Mode
For categorical or discrete data, the sample mode may be used to denote the category or data
value that contains the largest number of data observations. In other words, the sample mode
is the value with the highest frequency and can be thought of as estimating the category or
value that has the highest probability. Figure 6.14, which shows the histogram of the number
of defective chips in a box, reveals that the sample mode for this data set is two defective chips
per box.
6.3.5
Sample Variance
The sample variance of a set of data observations x1, . . . , xn is deﬁned to be
s2 =
n
i=1(xi −¯x)2
n −1
and the sample standard deviation is s. Notice that the numerator of the formula for s2 is
composed of the sum of the squares of the deviances of the data observations xi about the
sample average ¯x. Also, notice that the denominator of the formula for s2 is n −1 and not n,
although for reasonably large data sets there is very little difference between using n −1 and
n (an explanation of the use of n −1 rather than n is provided in Chapter 7).
The sample variance s2 can be thought of as an estimate of the variance σ 2 of the unknown
underlying probability distribution of the observations in the data set. It provides an indication
of the variability in the sample in the same way that the variance σ 2 provides an indication of
the variability of a probability distribution.

6.3 SAMPLE STATISTICS
283
FIGURE 6.23
Relationship between the sample
mean, median, and trimmed mean
for positively and negatively
skewed data sets
Median
Trimmed mean
Mean
Negative skewness
Median
Trimmed mean
Mean
Positive skewness
Frequency
Frequency
Alternative computational formulas for the sample variance s2 are
s2 =
 n
i=1 x2
i

−n ¯x2
n −1
and
s2 =
 n
i=1 x2
i

−
 n
i=1 xi
2/n
n −1
These are usually the easiest way to calculate s2 by hand, since they require knowledge of
only n
i=1 x2
i and either ¯x or n
i=1 xi.

284
CHAPTER 6
DESCRIPTIVE STATISTICS
For the data set given in Figure 6.22
¯x = 3.725
and
20

i=1
x2
i = 0.92 + · · · + 5.02 = 308.61
so that the sample variance is
s2 = 308.61 −(20 × 3.7252)
19
= 1.637
The sample standard deviation is therefore s =
√
1.637 = 1.279.
6.3.6
Sample Quantiles
The pth sample quantile is a value that has a proportion p of the sample taking values smaller
than it and a proportion 1 −p taking values larger than it. Clearly, it is an estimate of the pth
quantile of the unknown underlying probability distribution of the sample observations. The
terminology sample percentile is often used in place of sample quantile in the obvious manner.
The sample median is the 50th percentile of the sample, and the upper and lower sample
quartiles are respectively the 75th percentile and 25th percentile of the sample. The sample
interquartile range denotes the difference between the upper and lower sample quartiles.
Sample quantiles usually take a value between two data observations and are usually
presented as an appropriate weighted average of the two data observations. For example,
what is the upper sample quartile of the data set given in Figure 6.22? The 15th largest data
observation is 4.6, and the 16th largest data observation is 4.7. The upper sample quartile is
then usually given as
1
4 × 4.6

+
3
4 × 4.7

= 4.675
On the other hand, the ﬁfth largest data observation is 2.6, and the sixth largest data observation
is 2.8, so that the lower sample quartile is usually given as
3
4 × 2.6

+
1
4 × 2.8

= 2.65
Notice that the weighting of the two data observations may be performed in accordance with
the proportion p of the quantile being calculated, although different software packages may
do the weighting in different ways. The important point is that the sample quartile is between
the appropriate two adjacent data values.
Finally, it is worth remarking that the empirical cumulative distribution function discussed
in Section 15.1.1 is a simple graphical representation of a data set from which the sample
quantiles can easily be found.
6.3.7
Boxplots
A boxplot is a schematic presentation of the sample median, the upper and lower sample
quartiles, and the largest and smallest data observations. As Figure 6.24 shows, a box is
constructed whose ends are the lower and upper sample quartiles, and a vertical line in the
middle of the box represents the sample median. Horizontal lines stretch out from the ends of
the box to the largest and smallest data observations.

6.3 SAMPLE STATISTICS
285
FIGURE 6.24
Boxplot of a data set
Lower sample
quartile (25%)
Sample median
(50%)
Upper sample
quartile (75%)
Smallest data
observation
Largest data
observation
FIGURE 6.25
Boxplot for data set in Figure 6.22
1.0
2.0
3.0
4.0
5.0
FIGURE 6.26
Boxplot with possible outlier
Possible
outlier
*
A boxplot provides a simple and immediate graphical representation of the shape of a
data set. Notice that half of the data observations lie within the box and half lie outside. If
the sample histogram is fairly symmetric, then the two lines on the ends of the box should be
roughly the same length, and the median should lie roughly in the center of the box. If the
data are skewed, then the two lines are not the same length, and the median does not lie in the
center of the box.
Figure 6.25 shows a boxplot for the data set given in Figure 6.22. Notice that the line on
the left of the box is much longer than the line on the right of the box, and the median lies to
the right of the center of the box. This illustrates the negative skewness of the data set.
Various additions to a boxplot are often employed to convey more information. If very
large or very small data observations are considered to be possible outliers, then they may
be represented by an asterisk and the line does not extend all the way to them, as shown in
Figure 6.26. Some statistical packages allow you to custom design boxplots by, for example,
adding various notches on the lines to indicate sample percentiles such as the 10th percentile
and the 90th percentile.
6.3.8
Coefﬁcient of Variation
Recall that the sample mean ¯x and the sample standard deviation s are both measured in the
same units as the data observations, and that they provide information on the middle value and
the spread of the data set respectively. Sometimes it may be useful to consider the spread of

286
CHAPTER 6
DESCRIPTIVE STATISTICS
the data relative to the middle value, which can be measured by the coefﬁcient of variation
deﬁned by
CV = s
¯x
This is a positive unitless quantity that can be useful to make comparisons between different
data sets in terms of their variabilities expressed relative to their sample averages. Large values
of the coefﬁcient of variation imply that the variability is large relative to the sample average,
while small values indicate that the variability is small relative to the sample average.
It is also worth noting that the coefﬁcient of variation can be applied to probability distri-
butions where it is calculated as CV = σ/μ and measures the standard deviation relative to
the mean. For example, it takes a value of √(1 −p)/np for a binomial distribution, 1/
√
λ for
a Poisson distribution, 1 for an exponential distribution, and 1/
√
k for a gamma distribution.
Example 42
Elephants and Mice
A zoologist is interested in the variations in the weights of different kinds of animals. A data
set of adult male African elephants provided weights with a sample average of ¯xe = 4550 kg
and a sample standard deviation of se = 150 kg, while a data set concerning a certain kind of
mouse provided weights with a sample average of ¯xm = 30 g and a sample standard deviation
of sm = 1.67 g.
Obviously, the variation in the elephant weights is larger than the variation in the mice
weights when compared directly because the elephant weights are so much larger. However,
the coefﬁcient of variation for the elephant weights is
CVe = se
¯xe
= 150
4550 = 0.033
while the coefﬁcient of variation for the mice weights is
CVm = sm
¯xm
= 1.67
30 = 0.056
Consequently, it can be seen that the mice have more variability in their weights than the
elephants relative to their respective average weights.
6.3.9
Examples
Example 17
Milk Container
Contents
Figure 6.27 shows a boxplot of the data set of milk container weights together with a set of
sample statistics. The sample mean of 2.0727 liters, the sample median of 2.0845 liters, and
the sample 10% trimmed mean of 2.0730 liters are all close together, which conﬁrms that the
data set is fairly symmetric, as suggested by the histogram in Figure 6.17. Furthermore, the
boxplot is also fairly symmetric.
FIGURE 6.27
Boxplot and summary statistics for
milk weights data set
1.95
2.00
2.05
Milk weights
2.15
2.20
2.10
Sample size = 50
Sample mean = 2.0727
Sample trimmed mean = 2.0730
Sample standard deviation = 0.0711
Sample maximum = 2.1980
Sample upper quartile = 2.1280
Sample median = 2.0845
Sample lower quartile = 2.0127
Sample minimum = 1.9510

6.3 SAMPLE STATISTICS
287
FIGURE 6.28
Dotplot and summary statistics for
computer chips data set
0
1
3
5
7
2
4
Number of defective chips
8
6
Sample size = 80
Sample mean = 3.075
Sample trimmed mean = 3.014
Sample standard deviation = 1.813
Sample maximum = 8
Sample upper quartile = 4
Sample median = 3
Sample lower quartile = 2
Sample minimum = 0
FIGURE 6.29
Boxplot and summary statistics for
metal cylinder diameters data set
49.7
49.9
50.0
Metal cylinder diameters
50.3
50.4
50.2
50.1
49.8
Sample size = 60
Sample mean = 49.999
Sample trimmed mean = 49.996
Sample standard deviation = 0.134
Sample maximum = 50.360
Sample upper quartile = 50.070
Sample median = 50.010
Sample lower quartile = 49.905
Sample minimum = 49.740
*
Example 2
Defective Computer
Chips
Figure 6.28 shows a dotplot and the summary statistics of the computer chips data set. The
dotplot of the data set simply records the data observations on a linear scale, and for this data
set it provides a visual representation similar to the histogram in Figure 6.14. The sample
median is 3, and the lower and upper sample quartiles are 2 and 4, which indicates that at
least half of the boxes examined contained between two and four defective chips. The sample
mean is 3.075, and the sample standard deviation is 1.813.
Example 14
Metal Cylinder
Production
Figure 6.29 presents sample statistics and a boxplot for the data set of metal cylinder diameters.
This boxplot has been drawn by the statistical software package Minitab that has indicated
that the largest data observation may be an outlier by representing it with an asterisk and by
curtailing the top line. Notice also that the sample mean of 49.999 mm and the sample median
of 50.01 mm are very close, which conﬁrms the suggestion from the histogram in Figure 6.15
that the data set is fairly symmetric.
Finally, it worth remarking that boxplots are useful tools for providing a graphical com-
parison of samples from two or more populations. Figure 9.25 shows boxplots of two samples
drawn to the same scale, and Figure 11.12 presents a comparison of six samples using boxplots.
COMPUTER NOTE
Find out how to obtain sample statistics and boxplots on your software package.
6.3.10
Problems
6.3.1 Consider the data set given in DS 6.3.1. Calculate by hand
the sample mean, sample median, sample trimmed mean,
and sample standard deviation. Calculate the upper and
lower sample quartiles, and draw a boxplot of the data set.
6.3.2 Piston Rod Lengths
Consider the data set of 30 piston rod lengths given in
DS 6.2.3. Calculate the sample mean, sample median,
sample trimmed mean, and sample standard deviation.

288
CHAPTER 6
DESCRIPTIVE STATISTICS
Calculate the upper and lower sample quartiles, and draw
a boxplot of the data set.
6.3.3 Physical Training Course Completion Times
Consider the data set of physical training course
completion times given in DS 6.2.4. Calculate the sample
mean, sample median, sample trimmed mean, and
sample standard deviation. Calculate the upper and
lower sample quartiles, and draw a boxplot of the
data set.
Use a statistical software package to obtain sample statistics and
boxplots for the following data sets. What do the sample statistics
and boxplots tell you about the data set?
6.3.4 The data set of die rolls given in DS 6.1.1.
6.3.5 Restaurant Service Times
The data set of service times given in DS 6.1.4.
6.3.6 Fruit Spoilage
The data set of spoiled peaches given in DS 6.1.5.
6.3.7 Telephone Switchboard Activity
The data set of calls received by a switchboard given in
DS 6.1.6.
6.3.8 Paving Slab Weights
The data set of paving slab weights given in DS 6.1.7.
6.3.9 Spray Painting Procedure
The data set of paint thicknesses given in DS 6.1.8.
6.3.10 Plastic Panel Bending Capabilities
The data set of plastic panel bending capabilities given in
DS 6.1.9.
6.3.11 Consider the data set
6 7 12 18 22
together with a sixth value x. What value of x minimizes
the sample standard deviation of all six data points?
6.3.12 The standard deviation is measured in the same units as
the quantity under consideration, and larger values of the
standard deviation imply that there is more variability in
the quantity under consideration.
A. True
B. False
6.3.13 Consider a data set consisting of 43 different numbers.
A. If the smallest data value is decreased by one unit,
then the sample mean will decrease and the sample
median will decrease.
B. If the smallest data value is decreased by one unit,
then the sample mean will decrease and the sample
median will stay the same.
C. If the smallest data value is decreased by one unit,
then the sample mean will stay the same and the
sample median will decrease.
D. If the smallest data value is decreased by one unit,
then the sample mean will stay the same and the
sample median will stay the same.
6.3.14 The sample mean is always deﬁnitely a better measure
than the sample median of the “average” of a data set
because it takes into account all of the data values.
A. True
B. False
6.3.15 Consider the data set 7, 9, 14, 15, 22. The sample
standard deviation is:
A. 5.86
B. 5.88
C. 5.90
D. 5.92
E. 5.94
6.3.16 If a histogram is skewed with a long left tail, which of the
following must be correct?
A. The sample mean is larger than the sample median.
B. The sample mean is smaller than the sample median.
C. Some data points should be classiﬁed as outliers.
6.4
Examples
Example 43
Rolling Mill Scrap
In a rolling mill process, illustrated in Figure 6.30, ingots of bronze metals such as brass or
copper are repeatedly heated, rolled, and cooled until a desired thickness and hardness of
metal plate are obtained. After each pass through the rolling machines, the metal plates are
trimmed on the sides and ends to remove material that has cracked or is otherwise damaged.
Much of this scrap material can be recycled, although some is lost.
It is useful for the company to be able to predict the amount of scrap obtained from each
order. Figure 6.31 shows a data set of 95 observations which are the % scrap for 95 orders that
required only one pass through the rolling machines. The variable % scrap is deﬁned to be
% scrap =

1 −ﬁnished weight of plate
input ingot weight

× 100%

6.4 EXAMPLES
289
FIGURE 6.30
Rolling mill process
Scrap
Cooled
Rolled
Bronze ingots
Heated
FIGURE 6.31
% scrap data set from rolling mill
process
20.00
17.64
23.21
22.22
13.33
23.17
22.56
15.79
20.00
28.57
27.27
23.53
23.81
21.95
27.73
13.51
27.73
21.05
21.05
15.79
23.81
21.05
22.22
28.57
26.83
20.00
25.47
17.50
21.05
19.05
11.11
21.05
15.67
28.57
27.27
21.95
16.67
25.00
21.57
29.41
16.50
20.96
20.96
17.65
22.22
11.76
11.76
14.77
14.77
15.79
23.81
20.69
10.71
21.95
20.00
22.48
16.19
12.98
20.00
20.00
21.05
7.69
23.53
16.67
25.00
16.67
20.00
24.44
18.70
18.70
18.70
18.70
21.59
29.17
12.50
16.49
25.00
26.31
30.56
22.49
24.24
21.05
20.00
14.63
25.47
23.53
25.00
26.00
25.00
18.74
25.00
25.00
28.57
15.49
15.57
FIGURE 6.32
Histogram of rolling mill scrap
data set
7 9
15
21
11 13
25
17
27
19
23
29 31
% scrap
20
15
10
5
Frequency
Figures 6.32 and 6.33 show a histogram and a boxplot of this data set together with
summary statistics. The histogram and the boxplot suggest that the data set has a slight
negative skewness. This is consistent with the sample mean 20.81% being slightly smaller
than the sample median 21.05%. As expected, the trimmed mean 20.91% lies between these
two values. However, the smallest observation 7.69% might be considered to be an outlier. If
it is removed from the data set, then the sample looks much more symmetric.

290
CHAPTER 6
DESCRIPTIVE STATISTICS
FIGURE 6.33
Boxplot and summary statistics for
rolling mill scrap data set
10
20
25
% scrap
30
15
Sample size = 95
Sample mean = 20.810
Sample trimmed mean = 20.913
Sample standard deviation = 4.878
Sample maximum = 30.560
Sample upper quartile = 24.440
Sample median = 21.050
Sample lower quartile = 16.670
Sample minimum = 7.690
2 minutes of pushups
2 minutes of situps
2-mile run
FIGURE 6.34
The Army Physical Fitness Test
847
887
879
919
816
814
814
855
980
954
1078
1001
766
916
798
782
836
837
791
838
853
840
740
763
778
855
875
868
880
905
895
720
712
703
741
792
808
761
785
801
810
1013
882
861
845
865
883
881
921
816
837
1056
1034
774
821
850
870
931
930
808
828
719
707
934
939
977
896
921
815
838
854
1063
1024
780
813
850
902
906
865
886
881
825
821
832
FIGURE 6.35
Data set of run times in seconds
Finally, notice that the sample standard deviation is 4.878% and that half the data obser-
vations lie between the lower sample quartile 16.67% and the upper sample quartile 24.44%.
Example 44
Army Physical Fitness
Test
The Army Physical Fitness Test, illustrated in Figure 6.34, consists of two minutes of pushups
followed by two minutes of situps, and is completed with a two-mile run. Figure 6.35 presents
84 run times in seconds for a group of male army ofﬁcers. Figure 6.36 shows a boxplot,
histogram, and summary statistics for this data set.
The histogram and boxplot reveal that the data set has a slight positive skewness due to
a long right tail made up of some relatively slow runners, which the boxplot has indicated
might be considered as outliers. Correspondingly, the mean run time, which is 857.7 seconds =
14 minutes 17.7 seconds, is larger than the median run time, which is 850.0 seconds = 14 min-
utes 10.0 seconds.

6.4 EXAMPLES
291
700
800
Run times (seconds)
1100
1000
900
Sample size = 84
Sample mean = 857.70
Sample trimmed mean = 854.93
Sample standard deviation = 81.98
Sample maximum = 1078.00
Sample upper quartile = 900.50
Sample median = 850.00
Sample lower quartile = 808.50
Sample minimum = 703.00
700
750
800
850
900
950
1000
1050
1100
Run times (seconds)
Frequency
***
FIGURE 6.36
Boxplot, histogram, and summary statistics of run times data set
The sample standard deviation is 81.98 seconds, and half of the run times are between
the lower sample quartile of 808.5 seconds = 13 minutes 28.5 seconds and the upper sample
quartileof900.5seconds=15minutes0.5second.Thequickestrunrecordedis703.0seconds=
11 minutes 43.0 seconds, and the slowest is 1078.0 seconds = 17 minutes 58.0 seconds.
Example 45
Fabric Water
Absorption Properties
An experiment is conducted to investigate the water absorption properties of cotton fabric. This
absorption level is important in understanding the dyeing behavior of the fabric. A diagram
of the experimental apparatus employed is shown in Figure 6.37. The cotton fabric is scoured
and bleached and then run vertically between two rollers containing a bath of water. The water
pickup of the fabric is deﬁned to be
% pickup =
 ﬁnal fabric weight
initial fabric weight −1

× 100%
Figure 6.38 contains 15 data observations of % pickup obtained when the two rollers
rotated at 24 revolutions per minute with a pressure of 10 pounds per square inch between
them. Figure 6.39 shows a boxplot of the data set, a dotplot, and summary statistics.
Again, the dotplot of the data set simply records the data observations on a linear scale.
With so few data observations, it provides a better representation of the data set than a his-
togram. The dotplot indicates that the largest observation 70.2% appears to be far away from
the rest of the data points and may perhaps be considered an outlier. Finally, notice that the
sample mean, median, and trimmed mean are all roughly equal.

292
CHAPTER 6
DESCRIPTIVE STATISTICS
FIGURE 6.37
Apparatus for fabric water
absorption experiment
Water
Pressure
Pressure
Damp cotton
fabric
Dry cotton
fabric
51.8
61.8
57.3
54.5
64.0
59.5
61.2
64.9
54.5
70.2
59.1
55.8
65.4
60.4
56.7
FIGURE 6.38
% pickup data set
50
55
60
% pickup
70
65
Sample size = 15 
Sample mean = 59.81
Sample trimmed mean = 59.62
Sample standard deviation = 4.94
Sample maximum = 70.20
Sample upper quartile = 64.00
Sample median = 59.50
Sample lower quartile = 55.80
Sample minimum = 51.80
52.5
57.5
62.5
% pickup
70.0
65.0
67.5
60.0
55.0
FIGURE 6.39
Boxplot, dotplot, and summary statistics for fabric water absorption data set
6.5
Case Study: Microelectronic Solder Joints
A researcher is investigating a new method for applying the nickel layer onto the bond pads in
the substrate, and the thickness of the nickel layer is of particular interest. An assembly with
16 bond pads is examined and the nickel layer thickness is measured for each pad, resulting
in the data set shown in Figure 6.40.
This data set has a sample size n = 16, a sample mean ¯x = 2.7688 microns, and a sample
standard deviation s = 0.0260 microns. The sample median is the average of the eighth largest
data point 2.76 and the ninth largest 2.77, and so it is 2.765 microns. The minimum data point
is 2.72 microns and the largest is 2.81 microns.

6.7
SUPPLEMENTARY PROBLEMS
293
2.72
2.79
2.81
2.75
2.77
2.76
2.75
2.75
2.81
2.75
2.74
2.77
2.79
2.78
2.80
2.76
FIGURE 6.40
Data set of nickel layer thicknesses on substrate
bond pads (microns)
Barrel
Cylinder
Hourglass
Total
shape
shape
shape
451
8
53
512
FIGURE 6.41
Data set of solder joint shape frequencies from
an assembly with 512 solder joints
Website Visits
254028
304394
354765
348410
331162
263029
352599
354808
212230
206321
FIGURE 6.42
Visits per week to a
company’s website
FIGURE 6.43
Boxplot of website visits
360,000
340,000
320,000
300,000
280,000
260,000
220,000
200,000
24,0000
In a separate experiment, the researcher examines each of 512 solder joints on an assembly
to determine their shapes, and the categorical data set shown in Figure 6.41 is obtained.
6.6
Case Study: Internet Marketing
Figure 6.42 shows the number of visits per week to the organisation’s website over a 10-week
period. A boxplot of this data set is shown in Figure 6.43, and the mean number of visits per
week is 298,175 with a standard deviation of 59,656.
6.7
Supplementary Problems
The following data sets can be used to practice the generation
and interpretation of summary statistics and graphical
representations.
6.7.1 Bird Species Identiﬁcation
Three species of bird inhabit an island and they are
classiﬁed as having either brown, grey, or black markings.
DS 6.7.1 shows the types of birds observed by an
ornithologist during a stay on the island. (This problem is
continued in Problem 7.7.6.)
6.7.2 Oil Rig Accidents
DS 6.7.2 presents the number of accidents occurring on a
collection of oil rigs for each month during a two-year
span. (This problem is continued in Problem 7.7.7.)
6.7.3 Programming Errors
A software development company keeps track of the
number of errors found in the programs written by the
company employees. DS 6.7.3 shows the number of
errors found in the 30 programs that were written during
a particular month. (This problem is continued in
Problem 7.7.8.)
6.7.4 Osteoporosis Patient Heights
DS 6.7.4 shows the heights in inches of 60 adult males
with osteoporosis who visit a medical clinic during a
particular week. (This problem is continued in
Problems 7.7.9 and 8.6.5.)

294
CHAPTER 6
DESCRIPTIVE STATISTICS
6.7.5 Bamboo Cultivation
A researcher grows bamboo under controlled conditions
in a greenhouse. DS 6.7.5 presents the heights of a set of
bamboo shoots 40 days after planting. (This problem is
continued in Problems 7.7.10, 8.6.6, and 9.7.5.)
6.7.6 Soil Compressibility Tests
The knowledge of soil behavior is an important issue in
civil engineering. When soil is subjected to a load, there
is a change in the volume of the soil due to drainage of
water. A consolidation test can be performed to evaluate
the compressibility of soil, so that the amount of
settlement of buildings and other structures can be
estimated. DS 6.7.6 contains the measurements of
compressibility of 44 soil samples taken from a
construction site. (This problem is continued in
Problems 7.7.14 and 8.6.9.)
6.7.7 Glass Fiber Reinforced Polymer Tensile Strengths
Specimens of a glass ﬁber reinforced polymer were
placed in a tension testing machine. Increasing amounts
of tensile stress were applied until failure, and the
maximum loads are shown in DS 6.7.7.
6.7.8 Infant Blood Levels of Hydrogen Peroxide
High blood levels of hydrogen peroxide in infants can be
indicative of a dangerous infection. In order to understand
what levels of hydrogen peroxide are unusually high, the
data set in DS 6.7.8 was collected of hydrogen peroxide
levels in the blood of infants who were known to be free
of infection.
6.7.9 Paper Mill Operation of a Lime Kiln
A lime kiln is a large cylinder made of metal that is used
at a paper mill to extract lime from calcium carbonate
by heating it to a high temperature. An engineer was
interested in variations in the temperature of the kiln, and
DS 6.7.9 shows the temperature of the kiln every
10 minutes during a 5-hour period.
6.7.10 River Salinity Levels
DS 6.7.10 shows the salinity levels in parts per trillion
(ppt) at various points along a river.
6.7.11 Dew Point Readings from Coastal Buoys
Buoys ﬂoating in the ocean provide important
information on weather conditions, including the dew
point measurement which is deﬁned to be the temperature
at which water will condense in the air. DS 6.7.11 shows
the dew point measurements from a set of buoys at a
certain time.
6.7.12 Brain pH Levels
Psychiatrists are interested in how the pH levels of
brains may change for patients with mental illnesses.
DS 6.7.12 shows the pH levels of brains of 20 healthy
individuals which the psychiatrists hope to use as a
reference point.
6.7.13 Silicon Dioxide Percentages in Ocean Floor
Volcanic Glass
DS 6.7.13 shows the silicon dioxide percentages for
samples of volcanic glass found in the Atlantic Ocean.
6.7.14 Network Server Response Times
DS 6.7.14 shows the times in milliseconds taken by a
server to fulﬁll a standard task.
6.7.15 Are the following statements true or false?
(a) The shape of a boxplot provides some information on
the amount of skewness in the data set.
(b) Histograms are not a good way to detect skewness in a
data set.
(c) Outliers may be misrecorded data points.
(d) A boxplot indicates the value of the sample mean.
6.7.16 A histogram is used to represent categorical data, while a
bar chart is used to represent continuous numerical data.
A. True
B. False
6.7.17 Carbon Footprints
Analyze the data in DS 6.7.15, which contains estimates
of the pounds of carbon dioxide released when making
several types of car.
6.7.18 Data Warehouse Design
Power consumption represents a large proportion of a
data center’s costs. Analyze the data in DS 6.7.16, which
shows monthly electricty costs as a percentage of the data
center’s total costs.
6.7.19 Customer Churn
Customer churn is a term used for the attrition of a
company’s customers. DS 6.7.17 contains information
from an Internet service provider on the length of days
that its customers were signed up before switching to
another provider. Provide graphical representations and
summary statistics of these data.
6.7.20 Mining Mill Operations
DS 6.7.18 contains daily data for the mill operations of a
mining company over a period of a month. Each day, the
company keeps track of the carbon concentration in the
waste material. Provide graphical representations and
summary statistics of this data.

6.7 SUPPLEMENTARY PROBLEMS
295
6.7.21 Mercury Levels in Coal
DS 6.7.19 shows the mercury levels of coal samples that
are taken periodically as the coal is mined further and
further into the seam. Provide graphical representations
and summary statistics of this data.
6.7.22 Natural Gas Consumption
DS 6.7.20 contains data on the total daily natural gas
consumption for a region during the summer. Provide
graphical representations and summary statistics of this
data.
6.7.23 Boxplots are a graphical technique for a continuous
variable.
A. True
B. False
6.7.24 Consider a data set consisting of the seven numbers 56,
32, 47, 80, 28, 49, 71.
A. The sample mean is smaller than the sample median.
B. The sample mean and the sample median are equal.
C. The sample mean is larger than the sample median.
6.7.25 The word “average” can refer to either the sample mean
or the sample median.
A. True
B. False
6.7.26 A categorical data set is obtained from a satisfaction
survey where consumers were asked to rate their
experience as either “very unsatisfactory,”
“unsatisfactory,” “OK,” “satisfactory,” or “very
satisfactory.”
A. A bar chart could not be used to represent this data set.
A pie chart could not be used to represent this data set.
B. A bar chart could be used to represent this data set. A
pie chart could not be used to represent this data set.
C. A bar chart could not be used to represent this data set.
A pie chart could be used to represent this data set.
D. A bar chart could be used to represent this data set. A
pie chart could be used to represent this data set.
6.7.27 An analysis is performed of a company’s daily revenues,
which are $2.54 million, $0.87 million, $1.66 million,
and so on.
A. A boxplot could not be used to represent this data set.
A pie chart could not be used to represent this data set.
B. A boxplot could be used to represent this data set. A
pie chart could not be used to represent this data set.
C. A boxplot could not be used to represent this data set.
A pie chart could be used to represent this data set.
D. A boxplot could be used to represent this data set. A
pie chart could be used to represent this data set.
6.7.28 A categorical data set is obtained from a satisfaction
survey where consumers were asked to rate their
experience as either “very unsatisfactory,”
“unsatisfactory,” “OK,” “satisfactory,” or “very
satisfactory,” and these responses are then coded as
1 point, 2 points, 3 points, 4 points, and 5 points,
respectively.
A. After coding, the results can be analyzed as a
continuous variable.
B. After coding, the results cannot be analyzed as a
continuous variable.
6.7.29 About half of the data points lie between the sample
lower quartile and the sample upper quartile.
A. True
B. False

C H A P T E R S E V E N
Statistical Estimation and Sampling Distributions
Estimators provide the basis for the ﬁrst technical discussion of statistical inference, which is
presented in this chapter. Based on the ideas discussed in the previous chapter on descriptive
statistics, an important distinction is made between population properties (parameters) and
sample properties (statistics). The basic statistical inference problem of estimating param-
eters is formulated, and various desirable properties of estimators are considered. Finally,
the sampling distributions of common estimators are discussed, and general techniques for
constructing good estimators are described.
7.1
Point Estimates
7.1.1
Parameters
It is very important to have a clear understanding of the difference between a parameter and
a statistic. A parameter, which can be generically denoted by θ, is a property of an underlying
probability distribution governing a particular observation. Parameters of obvious interest are
the mean μ and variance σ 2 of the probability distribution. For continuous probability distribu-
tions other parameters of interest may be the various quantiles of the probability distribution,
and for discrete probability distributions the probability values of particular categories may
be parameters of interest.
Parameters can be thought of as representing a quantity of interest about a general pop-
ulation. In Chapters 1–5 probability calculations were made based on given values of the
parameters of the probability distributions, but in practice the parameters are unknown since
the probability distribution that characterizes observations from the population is unknown.
An experimenter’s goal is to ﬁnd out as much as possible about these parameters since they
provide an understanding of the underlying probability distribution that characterizes the
population.
Parameters
In statistical inference, the term parameter is used to denote a quantity θ, say, that is a
property of an unknown probability distribution. For example, it may be the mean,
variance, or a particular quantile of the probability distribution. Parameters are
unknown, and one of the goals of statistical inference is to estimate them.
Example 1
Machine Breakdowns
Let po be the probability that a machine breakdown is due to operator misuse. This is a
parameter because it depends upon the probability distribution that governs the causes of the
machine breakdowns. In practice po is an unknown quantity, but it may be estimated from the
records of machine breakdown causes.
296

7.1 POINT ESTIMATES
297
Example 43
Rolling Mill Scrap
Let μ and σ 2 be the mean and variance of the probability distribution of % scrap when an
ingot is passed once through the rollers. These are unknown parameters that are properties
of the unknown underlying probability distribution governing the % scrap obtained from the
rolling process. Other parameters of interest may be the upper quartile and the lower quartile
of the % scrap distribution.
Example 45
Fabric Water
Absorption Properties
In this example, suppose that the parameters μ and σ 2 are the mean and variance of the
unknown probability distribution governing % pickup. In other words, a particular observation
of % pickup is considered to be an observation from a probability distribution with mean μ
and variance σ 2. These parameters are unknown but may be estimated from the sample of
observations of % pickup.
7.1.2
Statistics
Whereas a parameter is a property of a population or a probability distribution, a statistic
is a property of a sample from the population. Speciﬁcally, a statistic is deﬁned to be any
function of a set of data observations. In contrast to parameters, statistics take observed values
and consequently can be thought of as being known. However, in the discussion of statistical
estimation it is useful to remember that statistics are actually observations of random variables
with their own probability distributions.
For example, suppose that a sample of size n is collected of observations from a particular
probability distribution f (x). The data values recorded, x1, . . . , xn, are the observed values
of a set of n random variables X1, . . . , Xn, and each has the probability distribution f (x). In
general, a statistic is any function
A(X1, . . . , Xn)
of these random variables. The observed value of the statistic
A(x1, . . . , xn)
can be calculated from the observed data values x1, . . . , xn.
Common statistics are, of course, the sample mean
¯X = X1 + · · · + Xn
n
and the sample variance
S2 =
n
i=1(Xi −¯X)2
n −1
For a given data set x1, . . . , xn, these statistics take the observed values
¯x = x1 + · · · + xn
n
and
s2 =
n
i=1(xi −¯x)2
n −1
as discussed in Chapter 6. For continuous data, other useful statistics are the sample quantiles
and perhaps the sample trimmed mean. For discrete data, the cell frequencies are statistics of
obvious interest.

298
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
Statistics
In statistical inference, the term statistic is used to denote a quantity that is a property
of a sample. For example, it may be a sample mean, a sample variance, or a particular
sample quantile. Statistics are random variables whose observed values can be
calculated from a set of observed data observations. Statistics can be used to estimate
unknown parameters.
7.1.3
Estimation
Estimation is a procedure by which the information contained within a sample is used to
investigate properties of the population from which the sample is drawn. In particular, a point
estimate of an unknown parameter θ is a statistic ˆθ that is in some sense a “best guess” of
the value of θ. The relationship between a point estimate ˆθ calculated from a sample, and the
unknown parameter θ is illustrated in Figure 7.1. Notice that a caret or “hat” placed over a
parameter signiﬁes a statistic used as an estimate of the parameter.
Of course, an experimenter does not in general believe that a point estimate ˆθ is exactly
equal to the unknown parameter θ. Nevertheless, good point estimates are chosen to be good
indicators of the actual values of the unknown parameter θ. In certain situations, however,
there may be two or more good point estimates of a certain parameter which could yield
slightly different numerical values.
Remember that point estimates can only be as good as the data set from which they are
calculated. Again, this is a question of how representative the sample is of the population
relating to the parameter that is being estimated. In addition, if a data set has some obvious
outliers, then these observations should be removed from the data set before the point estimates
are calculated.
Point Estimates of Parameters
A point estimate of an unknown parameter θ is a statistic ˆθ that represents a “best
guess” at the value of θ. There may be more than one sensible point estimate of a
parameter.
FIGURE 7.1
The relationship between a point
estimate ˆθ and an unknown
parameter θ
is the “best guess” of the parameter .
The statistic ˆ
Unknown parameter 
Probability distribution f(x, )
ˆ
Point estimate  (statistic)
Not known by
the experimenter 
Probability
theory
Known by 
the experimenter
 Statistical
 inference
Data observations x1, ..., xn (sample)

7.1 POINT ESTIMATES
299
FIGURE 7.2
Estimation of the population mean
by the sample mean
Data observations (known)
from probability density function
Sample mean
(known)
Probability density function 
(unknown)
Population mean
(unknown)
μ
ˆμ = ¯x 
FIGURE 7.3
Estimating the probability that a
machine breakdown is due to
operator misuse
n machine breakdowns
ˆ = xo
n
po
1−po
po: unknown
n, xo: known
Point estimate: po
xo breakdowns due to
operator misuse 
n − xo breakdowns due to
electrical and mechanical
failures
As a simple example of a point estimate, notice that an obvious point estimate of the mean
μ of a probability distribution is the sample mean ¯x of data observations obtained from the
probability distribution. In this case ˆμ = ¯x, as illustrated in Figure 7.2.
Example 1
Machine Breakdowns
Consider the unknown parameter po, which represents the probability that a machine break-
down is due to operator misuse. Suppose that a representative sample of n machine breakdowns
is recorded, of which xo are due to operator misuse. As illustrated in Figure 7.3, the statistic
xo/n is an obvious point estimate of the unknown parameter po, and this may be written
ˆpo = xo
n
For the data set shown in Figure 6.2, n = 46 and xo = 13. Consequently, based upon this
data set a point estimate
ˆpo = 13
46 = 0.28
is obtained.
Example 43
Rolling Mill Scrap
Given a representative sample x1, . . . , xn of % scrap values, obvious point estimates of the
unknown parameters μ and σ 2, the mean and variance of the probability distribution of %

300
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
FIGURE 7.4
Estimating the population mean
and variance of the rolling mill
scrap
Sample (known)
Point estimates
       (known)
ˆμ = x = 20.81
ˆσ 2 = s2 = 23.79
 Probability
 density
 function
 (unknown)
Population mean 
Population variance
μ
σ 2
(unknown)
% scrap
¯
scrap when an ingot is passed once through the rollers, are
ˆμ = ¯x = x1 + · · · + xn
n
and
ˆσ 2 = s2 =
n
i=1(xi −¯x)2
n −1
In other words, the sample mean and sample variance can be used as point estimates of the
population mean and population variance.
For the data set given in Figure 6.31, these point estimates take the values
ˆμ = 20.81
and
ˆσ 2 = 4.8782 = 23.79
as shown in Figure 7.4. In addition, the upper quartile θ0.75 and the lower quartile θ0.25 of the
% scrap distribution may be estimated by the upper and lower sample quartiles, so that
ˆθ0.75 = 24.44
and
ˆθ0.25 = 16.67
Is it sensible to use the sample trimmed mean as a point estimate of μ instead of the sample
mean ¯x? In fact, this is a situation where there is more than one sensible point estimate of the
parameter μ, since both the sample mean and the trimmed sample mean provide a good point
estimate of μ. Actually, the sample mean usually has a smaller variance than the trimmed
sample mean, but, as discussed in the previous chapter, the trimmed sample mean is a more
robust estimator and is not sensitive to data observations that may be outliers.
Example 45
Fabric Water
Absorption Properties
Consider the data set of % pickup observations given in Figure 6.38. Should any outliers be
removed before point estimates of the mean μ and variance σ 2 of the fabric absorption are
calculated? The dotplot in Figure 6.39 suggests that the largest data observation is suspect.
However, since the data set has only 15 observations, it is not clear whether this data point is
really “unusual” or not.

7.2 PROPERTIES OF POINT ESTIMATES
301
The experimenter checked this data observation and could not ﬁnd anything unusual about
it, so it is probably best to leave it in the data set. In this case the point estimates are
ˆμ = 59.81
and
ˆσ 2 = 4.942 = 24.40
7.2
Properties of Point Estimates
This section considers two basic criteria for determining good point estimates of a particular
parameter, namely, unbiased estimates and minimum variance estimates. These criteria help
us decide which statistics to use as point estimates. In general, when there is more than one
obvious point estimate for a parameter, these criteria can be used to compare the possible
choices of point estimate.
7.2.1
Unbiased Estimates
A point estimate ˆθ for a parameter θ is said to be unbiased if
E(ˆθ) = θ
Remember that a point estimate is an observation of a random variable with a probability dis-
tribution. The property of unbiasedness requires a point estimate ˆθ to have a probability distri-
bution with a mean equal to θ, the value of the parameter being estimated. If a point estimate
has a symmetric probability distribution, then as Figure 7.5 illustrates, it is unbiased if the
probability distribution is centered at the parameter value θ.
Unbiasedness is clearly a nice property for a point estimate to possess. If a point estimate
is not unbiased, then its bias can be deﬁned to be
bias = E(ˆθ) −θ
Figure 7.6 illustrates the bias of a point estimate with a symmetric distribution. If two different
point estimates are being compared, then the one with the smaller absolute bias is usually
preferable (although their variances may also affect the choice between them).
Probability density
function of point 
estimate   
ˆ

FIGURE 7.5
An unbiased point estimate ˆθ
Probability density
function of point 
estimate  
E( )
Bias
ˆ
ˆ

FIGURE 7.6
A biased point estimate ˆθ

302
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
Unbiased and Biased Point Estimates
A point estimate ˆθ for a parameter θ is said to be unbiased if
E(ˆθ) = θ
Unbiasedness is a good property for a point estimate to possess. If a point estimate is
not unbiased, then its bias can be deﬁned to be
bias = E(ˆθ) −θ
All other things being equal, the smaller the absolute value of the bias of a point
estimate, the better.
The common point estimates discussed in the previous section can now be investigated
to determine whether or not they are unbiased. Consider ﬁrst a sequence of Bernoulli trials
with a constant unknown success probability p. This unknown parameter p can be estimated
by conducting a sequence of trials and by observing how many of them result in a success.
Suppose that n trials are conducted and that the random variable X counts the number of
successes observed. The obvious point estimate of p is
ˆp = X
n
Is it an unbiased point estimate?
Notice that the number of successes X has a binomial distribution
X ∼B(n, p)
Therefore, the expected value of X is
E(X) = np
Consequently,
E( ˆp) = E
 X
n

= 1
n E(X) = 1
n np = p
so that ˆp = X/n is indeed an unbiased point estimate of the success probability p.
Point Estimate of a Success Probability
Suppose that X ∼B(n, p). Then
ˆp = X
n
is an unbiased point estimate of the success probability p. This result implies that the
proportion of successes in a sequence of Bernoulli trials with a constant success
probability p is an unbiased point estimate of the success probability.

7.2 PROPERTIES OF POINT ESTIMATES
303
Example 1
Machine Breakdowns
Notice that the number of machine breakdowns due to operator misuse, Xo, has the binomial
distribution
Xo ∼B(n, po)
Consequently, the point estimate
ˆpo = Xo
n
is an unbiased point estimate of po.
Now suppose that X1, . . . , Xn is a sample of observations from a probability distribution
with a mean μ and a variance σ 2. Is the sample mean
ˆμ = ¯X
an unbiased point estimate of the population mean μ? Clearly it is since
E(Xi) = μ,
1 ≤i ≤n
so that
E( ˆμ) = E( ¯X) = 1
n E
 n

i=1
Xi

= 1
n
n

i=1
E(Xi) = 1
n nμ = μ
Point Estimate of a Population Mean
If X1, . . . , Xn is a sample of observations from a probability distribution with a
mean μ, then the sample mean
ˆμ = ¯X
is an unbiased point estimate of the population mean μ.
Isatrimmedsamplemeananunbiasedestimateofthepopulationmeanμ?Iftheprobability
distribution of the data observations is symmetric, then the answer is yes. However, a trimmed
sample mean is in general not an unbiased point estimate of the population mean when the
probability distribution is not symmetric. Furthermore, a sample median is in general not
an unbiased point estimate of the population median when the probability distribution is not
symmetric.
However, this does not necessarily imply that the sample median should not be used as a
point estimate of the population median. Whether it should or not depends on whether there
are “better” point estimates than the sample median. For example, other point estimates may
have a smaller bias than the bias of the sample median. The important point to notice here
is that sometimes obvious point estimates may not be unbiased, although this does not imply
that better point estimates are available.

304
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
The sample variance S2 is an unbiased estimate of the population variance σ 2. This is
because
E(S2) =
1
n −1 E
 n

i=1
(Xi −¯X)2

=
1
n −1 E
 n

i=1
((Xi −μ) −( ¯X −μ))2

=
1
n −1 E
 n

i=1
(Xi −μ)2 −2( ¯X −μ)
n

i=1
(Xi −μ) + n( ¯X −μ)2

=
1
n −1 E
 n

i=1
(Xi −μ)2 −n( ¯X −μ)2

=
1
n −1
 n

i=1
E((Xi −μ)2) −nE(( ¯X −μ)2)

Now notice that
E(Xi) = μ
so that
E((Xi −μ)2) = Var(Xi) = σ 2
Furthermore,
E( ¯X) = μ
so that
E(( ¯X −μ)2) = Var( ¯X) = σ 2
n
Putting this all together gives
E(S2) =
1
n −1
 n

i=1
σ 2 −n
σ 2
n

= σ 2
so that S2 is indeed an unbiased estimate of σ 2.
Point Estimate of a Population Variance
If X1, . . . , Xn is a sample of observations from a probability distribution with a
variance σ 2, then the sample variance
ˆσ 2 = S2 =
n
i=1(Xi −¯X)2
n −1
is an unbiased point estimate of the population variance σ 2.
In fact, unbiasedness is the reason the denominator of S2 is chosen to be n −1 rather than
the perhaps more obvious choice of n. If the denominator is chosen to be n, so that the point

7.2 PROPERTIES OF POINT ESTIMATES
305
estimate is
ˆσ 2 =
n
i=1(Xi −¯X)2
n
then this estimate has an expectation of
E( ˆσ 2) =
n −1
n

σ 2
so that it is not unbiased. It has a bias of
E( ˆσ 2) −σ 2 =
n −1
n

σ 2 −σ 2 = −σ 2
n
Notice that as the sample size n increases, the bias becomes increasingly small, and clearly
for large sample sizes it is unimportant whether n −1 or n is used in the calculation of the
sample variance. However, in general the unbiasedness criterion dictates the use of n −1 in
the denominator of S2.
Example 43
Rolling Mill Scrap
The point estimates
ˆμ = 20.81
and
ˆσ 2 = 23.79
which are the sample mean and sample variance, are the observed values of unbiased point
estimates. They are good and sensible estimates of the true mean and variance of the % scrap
amounts.
7.2.2
Minimum Variance Estimates
As well as looking at the expectation E(ˆθ) of a point estimate ˆθ, it is important to consider the
variance Var(ˆθ) of the point estimate. It is generally desirable to have unbiased point estimates
with as small a variance as possible.
For example, suppose that two point estimates ˆθ1 and ˆθ2 have symmetric distributions as
shown in Figure 7.7. Moreover, suppose that their distributions are both centered at θ so that
they are both unbiased point estimates of θ. Which is the better point estimate? Since
Var(ˆθ1) > Var(ˆθ2)
FIGURE 7.7
The unbiased point estimate ˆθ2 is
better than the unbiased point
estimate ˆθ1 because it has a smaller
variance
Probability density
function of 
Probability density
function of ˆ1
2

ˆ

306
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
FIGURE 7.8
ˆθ2 is a better point estimate than ˆθ1
Probability density
function of 
Probability density
function of 1
ˆ2
P(| 1 − | ≤δ) < P(| 2 − | ≤δ)
 −δ


+ δ
 −δ


+ δ
ˆ
ˆ
ˆ
ˆθ2 is clearly a better point estimate than ˆθ1. It is better in the sense that it is likely to provide
an estimate closer to the true value θ than the estimate provided by ˆθ1.
In mathematical terms, this can be written
P(|ˆθ1 −θ| ≤δ) < P(|ˆθ2 −θ| ≤δ)
for any value of δ > 0, as illustrated in Figure 7.8. This inequality says that the probability
that the point estimate ˆθ2 provides an estimate no more than an amount δ away from the real
value of θ is larger than the corresponding probability for the point estimate ˆθ1.
The best possible situation is to be able to construct a point estimate that is unbiased and
that also has the smallest possible variance. An unbiased point estimate that has a smaller
variance than any other unbiased point estimate is called a minimum variance unbiased
estimate (MVUE). Such estimates are clearly good ones. A great deal of mathematical theory
has been developed to detect and investigate MVUEs for various problems. For our purpose
it is sufﬁcient to note that if X1, . . . , Xn is a sample of observations that are independently
normally distributed with a mean μ and a variance σ 2, then the sample mean ¯X is a minimum
variance unbiased estimate of the mean μ.
The efﬁciency of an unbiased point estimate is calculated as the ratio of the variance of
the point estimate and the variance of the MVUE. In this sense the MVUE can be described
as the “most efﬁcient” point estimate. More generally, the relative efﬁciency of two unbiased
point estimates is deﬁned to be the ratio of their variances.

7.2 PROPERTIES OF POINT ESTIMATES
307
Relative Efﬁciency
The relative efﬁciency of an unbiased point estimate ˆθ1 to an unbiased point estimate
ˆθ2 is
Var(ˆθ2)
Var(ˆθ1)
As a simple example of the calculation and interpretation of relative efﬁciency, suppose
that X1, . . . , X20 are independent, identically distributed random variables with an unknown
mean μ and a variance σ 2. If a point estimate of μ is required, then the sample mean
¯X = X1 + · · · + X20
20
is known to provide an unbiased point estimate. However, suppose that it is suggested that the
point estimate
¯X10 = X1 + · · · + X10
10
should be used to estimate μ. Is this a sensible point estimate?
Intuitively we feel uncomfortable with the point estimate ¯X10 because it uses only half
the data set. Consequently, it is not utilizing all the “information” available for estimating μ.
Nevertheless, ¯X10 is an unbiased point estimate of μ, so the criterion of unbiasedness does
not allow us to distinguish between the point estimates ¯X and ¯X10.
Of course, the reason that ¯X is a better point estimate than ¯X10 is that it has a smaller
variance, since
Var( ¯X) = σ 2
20
while
Var( ¯X10) = σ 2
10
In fact, the relative efﬁciency of the point estimate ¯X10 to the point estimate ¯X is
Var( ¯X)
Var( ¯X10) = 1
2
In conclusion, in this example our intuitive desire to use all of the data available to estimate
μ corresponds in mathematical terms to obtaining a point estimate with as small a variance
as possible.
Example 38
Chemical
Concentration Levels
Recall that a chemist has two independent measurements X A and X B available to estimate a
concentration level C, and that
X A ∼N(C, 2.97)
and
X B ∼N(C, 1.62)
With our present knowledge of estimation theory, we are in a position to understand more
fully how the chemist arrives at an optimum point estimate of the concentration level C.

308
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
Notice that ˆCA = X A, say, and ˆCB = X B are both point estimates of the unknown
concentration level C. Moreover, they are both unbiased point estimates since
E( ˆCA) = C
and
E( ˆCB) = C
However, since
Var( ˆCA) = 2.97
and
Var( ˆCB) = 1.62
ˆCB is a more efﬁcient estimate than ˆCA. In fact the relative efﬁciency of ˆCA to ˆCB is
Var( ˆCB)
Var( ˆCA) = 1.62
2.97 = 0.55
In summary, Var( ˆCB) is a better point estimate than Var( ˆCA).
Consider now the point estimate
ˆC = a ˆCA + b ˆCB
where a and b are two constants. Can a and b be chosen to make this a better point estimate
than ˆCB? First, notice that
E( ˆC) = aE( ˆCA) + bE( ˆCB) = (a + b)C
Therefore, in order for ˆC to be an unbiased point estimate of C, it is necessary to choose
a + b = 1
Setting a = p and b = 1 −p gives the point estimate
ˆC = p ˆCA + (1 −p) ˆCB
Finally, how should the value of p be chosen? Now the objective is to minimize the variance
of ˆC. The calculations made before showed that this goal is met by taking p = 0.35, so that
ˆC = 0.35 ˆCA + 0.65 ˆCB
which has a variance
Var( ˆC) = 1.05
This is a better point estimate than ˆCB, and the relative efﬁciency of ˆCB to ˆC is
Var( ˆC)
Var( ˆCB) = 1.05
1.62 = 0.65
In some circumstances it may be useful to compare two point estimates that have different
expectations and different variances. For example, in Figure 7.9, the point estimate ˆθ1 has a
smaller bias than the point estimate ˆθ2, but it also has a larger variance. In such cases, it is
usual to prefer the point estimate that minimizes the value of mean square error (MSE),
which is deﬁned to be
MSE(ˆθ) = E((ˆθ −θ)2)
Notice that the mean square error is simply the expectation of the squared deviation of the
point estimate about the value of the parameter of interest. Moreover, notice that
MSE(ˆθ) = E((ˆθ −θ)2)
= E(((ˆθ −E(ˆθ)) + (E(ˆθ) −θ))2)
= E((ˆθ −E(ˆθ))2) + 2(E(ˆθ) −θ)E(ˆθ −E(ˆθ)) + (E(ˆθ) −θ)2

7.2 PROPERTIES OF POINT ESTIMATES
309
FIGURE 7.9
Comparing point estimates with
different biases and different
variances
Probability density
function of 
Probability density
function of 1
ˆ2

1
Bias of
Bias of
2
ˆ
ˆ
ˆ
However,
E(ˆθ −E(ˆθ)) = E(ˆθ) −E(ˆθ) = 0
so that
MSE(ˆθ) = E((ˆθ −E(ˆθ))2) + (E(ˆθ) −θ)2 = Var(ˆθ) + bias2
Thus, the mean square error of a point estimate is the sum of its variance and the square of its
bias. For unbiased point estimates, the mean square error is simply equal to the variance of
the point estimate.
For example, suppose that in Figure 7.9
ˆθ1 ∼N(1.1θ, 0.04θ2)
and
ˆθ2 ∼N(1.2θ, 0.02θ2)
Then ˆθ1 has a bias of 0.1θ and a variance of 0.04θ2, so that its mean square error is
MSE(ˆθ1) = 0.04θ2 + (0.1θ)2 = 0.05θ2
Similarly, ˆθ2 has a mean square error
MSE(ˆθ2) = 0.02θ2 + (0.2θ)2 = 0.06θ2
so that, based upon this criterion, the point estimate ˆθ1 is preferable to ˆθ2.
Finally, it is worth remarking that the properties of a point estimate generally depend
on the size n of the sample from which they are constructed. In particular, the variances of
sensible point estimates decrease as the sample size n increases. Notice that it is reassuring
if the variance of a point estimate tends to 0 as the sample size becomes larger and larger,
and if the point estimate is either unbiased or has a bias that also tends to 0 as the sample
size becomes larger and larger (such point estimates are said to be consistent), since in this
case the point estimate can be made to be as accurate as required by taking a sufﬁciently large
sample size.

310
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
7.2.3
Problems
7.2.1 Suppose that E(X1) = μ, Var(X1) = 10, E(X2) = μ, and
Var(X2) = 15, and consider the point estimates
ˆμ1 = X1
2 + X2
2
ˆμ2 = X1
4 + 3X2
4
ˆμ3 = X1
6 + X2
3 + 9
(a) Calculate the bias of each point estimate. Is any one of
them unbiased?
(b) Calculate the variance of each point estimate. Which
one has the smallest variance?
(c) Calculate the mean square error of each point estimate.
Which point estimate has the smallest mean square
error when μ = 8?
7.2.2 Suppose that E(X1) = μ, Var(X1) = 7, E(X2) = μ,
Var(X2) = 13, E(X3) = μ, and Var(X3) = 20, and
consider the point estimates
ˆμ1 = X1
3 + X2
3 + X3
3
ˆμ2 = X1
4 + X2
3 + X3
5
ˆμ3 = X1
6 + X2
3 + X3
4 + 2
(a) Calculate the bias of each point estimate. Is any one of
them unbiased?
(b) Calculate the variance of each point estimate. Which
one has the smallest variance?
(c) Calculate the mean square error of each point estimate.
Which point estimate has the smallest mean square
error when μ = 3?
7.2.3 Suppose that E(X1) = μ, Var(X1) = 4, E(X2) = μ, and
Var(X2) = 6.
(a) What is the variance of
ˆμ1 = X1
2 + X2
2
(b) What value of p minimizes the variance of
ˆμ = pX1 + (1 −p)X2?
(c) What is the relative efﬁciency of ˆμ1 to the point
estimate with the smallest variance that you have
found?
7.2.4 Repeat Problem 7.2.3 with Var(X1) = 1 and Var(X2) = 7.
7.2.5 Suppose that a sequence of independent random variables
X1, . . . , Xn each has an expectation μ and variance σ 2,
and consider the point estimate
ˆμ = a1X1 + · · · + an Xn
for some constants a1, . . . , an.
(a) What is the condition on the constants ai for this to be
an unbiased point estimate of μ?
(b) Subject to this condition, what value of the constants
ai minimizes the variance of the point estimate?
7.2.6 If
ˆθ1 ∼N(1.13θ, 0.02θ2)
ˆθ2 ∼N(1.05θ, 0.07θ2)
ˆθ3 ∼N(1.24θ, 0.005θ2)
which point estimate would you prefer to estimate θ? Why?
7.2.7 Suppose that X ∼N(μ, σ 2) and consider the point
estimate
ˆμ = X + μ0
2
for some ﬁxed value μ0. Show that this point estimate has
a smaller mean square error than X when
|μ −μ0| ≤
√
3σ
Explain why it is not surprising that ˆμ has a smaller mean
square error than X when μ is close to μ0.
7.2.8 Suppose that X ∼B(10, p) and consider the point estimate
ˆp = X
11
(a) What is the bias of this point estimate?
(b) What is the variance of this point estimate?
(c) Show that this point estimate has a mean square error
of
10p −9p2
121
(d) Show that this mean square error is smaller than the
mean square error of X/10 when p ≤21/31.
7.2.9 Suppose that X1 is an estimate of a parameter θ with a
standard deviation 5.39, and that X2 is an estimate of θ
with a standard deviation 9.43. If the estimates X1 and X2
are independent, what is the standard deviation of the
estimate (X1 + X2)/2?

7.3 SAMPLING DISTRIBUTIONS
311
7.3
Sampling Distributions
Theprobabilitydistributionsorsamplingdistributionsofthesampleproportion ˆp,thesample
mean ¯X, and the sample variance S2 are now considered in more detail.
7.3.1
Sample Proportion
If X ∼B(n, p), then an unbiased estimate of the success probability p is
ˆp = X
n
This estimate can be referred to as a sample proportion since it represents the proportion
of successes observed in a sample of n trials. For large enough values of n, the normal ap-
proximation to the binomial distribution (discussed in Section 5.3.1) implies that X, and
similarly ˆp, may be taken to have normal distributions. Notice that because Var(X) =
np(1 −p), it follows that
Var( ˆp) = p(1 −p)
n
Sample Proportion
If X ∼B(n, p), then the sample proportion ˆp = X/n has the approximate distribution
ˆp ∼N

p, p(1 −p)
n

The standard deviation of ˆp is referred to as its standard error and is
s.e.( ˆp) =

p(1 −p)
n
The standard error provides an indication of the “accuracy” of the point estimate ˆp. Smaller
values of the standard error indicate that the point estimate is likely to be more accurate because
its variability about the true value of p is smaller. Notice that the standard error is inversely
proportional to the square root of the sample size n, so that as the sample size increases,
the standard error decreases and ˆp becomes a more accurate estimate of the success prob-
ability p.
Of course, since the success probability p is unknown, the standard error is really also
unknown since it depends upon p. However, it is customary to estimate the standard error by
replacing p by the observed value ˆp = x/n, so that
s.e.( ˆp) =

ˆp(1 −ˆp)
n
= 1
n

x(n −x)
n
Example 1
Machine Breakdowns
Recall that 13 out of 46 machine breakdowns are attributable to operator misuse. The point
estimate of the probability of a breakdown being attributable to operator misuse is
ˆpo = 13
46 = 0.28

312
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
which has a standard error of
s.e.( ˆpo) = 1
n

xo(n −xo)
n
= 1
46

13 × (46 −13)
46
= 0.066
Example 39
Cattle Inoculations
Suppose that the probability p that a vaccine provokes a serious adverse reaction is unknown.
If the vaccine is administered to n = 500,000 head of cattle and then x = 372 are observed
to suffer the reaction, the point estimate of p is
ˆp =
372
500,000 = 7.44 × 10−4
with a standard error of
s.e.( ˆp) =
1
500,000

372 × (500,000 −372)
500,000
= 3.86 × 10−5
A comparison of this calculation with the previous discussion of this example in Section
5.3.3 provides a distinct contrast between the different uses of probability theory and statistical
inference. In Section 5.3.3 the probability of an adverse reaction p is taken to be known,
and probability theory then allows the number of cattle suffering a reaction to be predicted.
However, the situation is now reversed. In this discussion, the number of cattle suffering a
reaction is observed, and hence is known, and statistical inference is used to estimate the
probability of an adverse reaction p.
GAMES OF CHANCE
A coin that is suspected of being biased is tossed many times in order to investigate the possible
bias. Consider the following two scenarios:
■
Scenario I : The coin is tossed 100 times and 40 heads are obtained.
■
Scenario II : The coin is tossed 1000 times and 400 heads are obtained.
What is the difference, if any, between the interpretations of these two sets of experimental
results?
In either case, the probability of obtaining a head is estimated to be ˆp = 0.4. However, in
scenario II the total number of coin tosses is larger than in scenario I, and so we feel that the
point estimate obtained from scenario II is more “accurate” than the point estimate obtained
from scenario I.
Mathematically, this is reﬂected in the point estimate having a smaller standard error in
scenario II than in scenario I. The standard error in scenario I is
s.e.( ˆp) =
1
100

40 × (100 −40)
100
= 0.0490
whereas the standard error in scenario II is
s.e.( ˆp) =
1
1000

400 × (1000 −400)
1000
= 0.0155
As a result of increasing the sample size by a factor of 10, the standard error has been reduced
by a factor of
√
10 = 3.16. This problem is analyzed further in Section 10.1.
7.3.2
Sample Mean
Consider a set of independent, identically distributed random variables X1, . . . , Xn with a
mean μ and a variance σ 2. The central limit theorem (discussed in Section 5.3.2) indicates

7.3 SAMPLING DISTRIBUTIONS
313
that the sample mean ¯X has the approximate distribution
¯X ∼N

μ, σ 2
n

This distribution is exact if the random variables Xi are normally distributed.
Sample Mean
If X1, . . . , Xn are observations from a population with a mean μ and a variance σ 2,
then the central limit theorem indicates that the sample mean ˆμ = ¯X has the
approximate distribution
ˆμ = ¯X ∼N

μ, σ 2
n

The standard error of the sample mean is
s.e.( ¯X) = σ
√n
which again is inversely proportional to the square root of the sample size. Thus, if the sample
size is doubled, the standard error is reduced by a factor of 1/
√
2 = 0.71. Similarly, in order
to halve the standard error, the sample size needs to be multiplied by four.
If a sample size n = 20 is used, what is the probability that the value of ˆμ = ¯X lies within
σ/4 of the true mean μ? From the properties of the normal distribution, this probability can
be calculated to be
P
	
μ −σ
4 ≤¯X ≤μ + σ
4

= P

μ −σ
4 ≤N

μ, σ 2
20

≤μ + σ
4

= P

−
√
20
4
≤N(0, 1) ≤
√
20
4

= (1.12) −(−1.12)
= 0.8686 −0.1314 = 0.7372
However, if a sample size of n = 40 is used, this probability increases to
P

−
√
40
4
≤N(0, 1) ≤
√
40
4

= (1.58) −(−1.58)
= 0.9429 −0.0571 = 0.8858
These probability values, which are illustrated in Figure 7.10, demonstrate the increase in
accuracy obtained with a larger sample size.
Since the standard deviation σ is usually unknown, it can be replaced by the observed
value s, so that in practice the standard error of an observed sample mean ¯x is calculated as
s.e.(¯x) =
s
√n
Example 43
Rolling Mill Scrap
The standard error of the sample mean ˆμ = ¯x = 20.81 is
s.e.(¯x) =
s
√n = 4.878
√
95
= 0.500

314
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
FIGURE 7.10
The increase in accuracy of ˆμ = ¯X
as the sample size increases
Probability density function
of 
−σ/4
+ σ/4
ˆ =
n = 20
when
74%
Probability density function
of 
= X when n = 40
μ
μ
μ
μ
−σ/4
+ σ/4
μ
μ
μ
ˆμ
89%
X
Example 44
Army Physical Fitness
Test
The standard error of the sample mean ˆμ = ¯x = 857.7 is
s.e.(¯x) =
s
√n = 81.98
√
84
= 8.94
Example 45
Fabric Water
Absorption Properties
The standard error of the sample mean ˆμ = ¯x = 59.81 is
s.e.(¯x) =
s
√n = 4.94
√
15
= 1.28
7.3.3
Sample Variance
For a sample X1, . . . , Xn obtained from a population with a mean μ and a variance σ 2,
consider the variance estimate
ˆσ 2 = S2 =
n
i=1(Xi −¯X)2
n −1
Sample Variance
If X1, . . . , Xn are normally distributed with a mean μ and a variance σ 2, then the
sample variance S2 has the distribution
S2 ∼σ 2 χ2
n−1
(n −1)

7.3 SAMPLING DISTRIBUTIONS
315
Thus, S2 is distributed as a scaled chi-square random variable with n −1 degrees of
freedom, where the scaling factor is σ 2/(n −1). Notice that for a sample of size n, the degrees
of freedom of the chi-square random variable are n −1.
This distributional result turns out to be very important for the problem of estimating a
normal population mean. This is because, as shown above, the standard error of the sample
mean ˆμ = ¯X is σ/√n. The dependence of the standard error on the unknown variance σ 2 is
rather awkward, but the sample variance S2 can be used to overcome the problem.
The elimination of the unknown variance σ 2 is accomplished as follows using the
t-distribution. The distribution of the sample mean
¯X ∼N

μ, σ 2
n

can be rearranged as
√n
σ ( ¯X −μ) ∼N (0, 1)
Also, notice that
S
σ ∼

χ2
n−1
(n −1)
so that
√n( ¯X −μ)
S
=
√n
σ ( ¯X −μ)
 S
σ

∼N (0, 1)

χ2
n−1
(n−1)
∼tn−1
Again, notice that the degrees of freedom of the t-distribution are one fewer than the sample
size n. For a given value of μ, the quantity
√n( ¯X −μ)
S
is known as a t-statistic.
t-statistic
If X1, . . . , Xn are normally distributed with a mean μ, then
√n( ¯X −μ)
S
∼tn−1
This result is very important since in practice an experimenter knows the values of n and
the observed sample mean ¯x and sample variance s2, and so knows everything in the quantity
√n(¯x −μ)
s
except for μ. This allows the experimenter to make useful inferences about μ, as described
in Chapter 8.

316
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
7.3.4
Simulation Experiment 2: An Investigation of Sampling Distributions
Suppose that an experimenter can measure some variables that are taken to be normally
distributed with an unknown mean and variance. Using simulation methods, for speciﬁc
values of the mean and variance we can simulate the data values that the experimenter might
obtain. More interestingly, we can simulate lots of possible samples of which, in reality, the
experimenter would observe only one. Performing this simulation experiment allows us to
check on the sampling distributions of the parameter estimates that we have discussed in this
section.
Let us suppose that μ = 10 and σ 2 = 3. This is something that the experimenter does
not know, and indeed the experimenter is conducting the experiment in order to ﬁnd out what
these parameter values are. Suppose that the experimenter decides to take a sample of n = 30
observations. We can use the computer to simulate the data values that the experimenter
might observe. This simply involves obtaining 30 random observations from a N(10, 3)
distribution.
When this is done, a data set of 30 observations is obtained with ¯x = 10.589 and s2 =
3.4622, as illustrated in Figure 7.11. An experimenter who obtained these data values would
therefore estimate the parameters as ˆμ = 10.589 and ˆσ 2 = 3.4622. With our knowledge of
the true parameter values, we can see that the experimenter is not doing too badly.
Suppose that we now simulate lots of different samples of data observations. Speciﬁcally,
let’s simulate 500 samples. Since each sample contains 30 data observations, notice that this
requires a total of 15,000 random observations from a N(10, 3) distribution. For each sample
we can calculate a value of ¯x and s2.
Figure 7.12 shows a histogram of the 500 values of the sample mean ¯x obtained from the
simulation. The sampling distribution theory discussed in this section tells us that the sample
means ¯x are observations from a normal distribution with a mean of μ = 10 and a variance of
σ 2/n = 3/30 = 0.1. The histogram in Figure 7.12 is seen to have a shape similar to a normal
distribution, and in fact the average of the 500 ¯x values is 10.006 and they have a (sample)
variance of 0.091, so that the simulation results agree well with the theory.
Random variable X ∼N (10, 3)
Sample x1, . . . , x30
Sample statistics
ˆ = x = 10.589
ˆσ 2 = s2 = 3.4622
¯
μ
FIGURE 7.11
A simulated sample of 30 observations
9.2
9.6
10.0
10.4
10.8
Frequency
FIGURE 7.12
Histogram of 500 simulated values of ¯x

7.3 SAMPLING DISTRIBUTIONS
317
In reality, the experimenter obtains just one sample, and exactly how close the estimate
ˆμ = ¯x is to the true value of μ is a matter of luck. The point estimate obtained by the
experimenter is a random observation from the normal distribution indicated by the histogram
in Figure 7.12. In our 500 simulated samples, it turns out that the largest value of ¯x obtained
is 10.876 and the smallest is 9.073. However, about 250 of the simulations produced a value
of ¯x between 9.8 and 10.2.
Figure 7.13 shows a histogram of the 500 values of the sample variance s2 obtained from
the simulation. The sampling distribution theory presented in this section tells us that the
distribution of the sample variance is
σ 2 χ2
n−1
(n −1) = 3 × χ2
29
29
The histogram in Figure 7.13 exhibits the positive skewness that a chi-square distribution
possesses. The average of the 500 simulated values of s2 is 3.068 and half of them take values
between 2.54 and 3.56. The largest simulated value is 6.75 and the smallest is 1.33.
How close will the experimenter’s value of ˆσ 2 be to the true value σ 2 = 3? Again, it’s a
matter of luck. The point estimate obtained by the experimenter is a random observation from
the scaled chi-square distribution indicated by the histogram in Figure 7.13.
Finally, let’s look at the values of the t-statistics
√n(¯x −μ)
s
=
√
30(¯x −10)
s
Figure 7.14 shows a histogram of the 500 simulated values of these t-statistics. The theory
presented in this section tells us that the t-statistics should have a t-distribution with n−1 = 29
degrees of freedom, which is very similar to a standard normal distribution but with a slightly
larger variance. In fact, the histogram in Figure 7.14 is seen to have a shape similar to a
normal distribution, and the 500 t-statistics have an average of 0.0223 and a variance of 0.96,
which is in general agreement with the theory. This simulation experiment is continued in
Section 8.1.4.
1.25
2.25 3.25
4.25 5.25
6.25
Frequency
FIGURE 7.13
Histogram of 500 simulated values of s2
−1
−2
−3
2
3
1
0
Frequency
FIGURE 7.14
Histogram of 500 simulated t-statistics

318
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
7.3.5
Problems
7.3.1 Suppose that X1 ∼B(n1, p) and X2 ∼B(n2, p). What is
the relative efﬁciency of the point estimate
X1
n1
to the point estimate
X2
n2
for estimating the success probability p?
7.3.2 Consider a sample X1, . . . , Xn of normally distributed
random variables with mean μ and variance σ 2 = 1.
(a) If n = 10, what is the probability that |μ −¯X| ≤0.3?
(b) What is this probability when n = 30?
7.3.3 Consider a sample X1, . . . , Xn of normally distributed
random variables with mean μ and variance σ 2 = 7.
(a) If n = 15, what is the probability that |μ −¯X| ≤0.4?
(b) What is this probability when n = 50?
7.3.4 Consider a sample X1, . . . , Xn of normally distributed
random variables with variance σ 2 = 5. Suppose that
n = 31.
(a) What is the value of c for which P(S2 ≤c) = 0.90?
(b) What is the value of c for which P(S2 ≤c) = 0.95?
7.3.5 Repeat Problem 7.3.4 with n = 21 and σ 2 = 32.
7.3.6 Consider a sample X1, . . . , Xn of normally distributed
random variables with mean μ. Suppose that n = 16.
(a) What is the value of c for which
P(|4( ¯X −μ)/S| ≤c) = 0.95?
(b) What is the value of c for which
P(|4( ¯X −μ)/S| ≤c) = 0.99?
7.3.7 Consider a sample X1, . . . , Xn of normally distributed
random variables with mean μ. Suppose that n = 21.
(a) What is the value of c for which
P(|( ¯X −μ)/S| ≤c) = 0.95?
(b) What is the value of c for which
P(|( ¯X −μ)/S| ≤c) = 0.99?
7.3.8 In a consumer survey, 234 people out of a representative
sample of 450 people say that they prefer product A to
product B. Let p be the proportion of all consumers who
prefer product A to product B. Construct a point estimate
of p. What is the standard error of your point estimate?
7.3.9 The breaking strengths of 35 pieces of cotton thread are
measured. The sample mean is ¯x = 974.3 and the sample
variance is s2 = 452.1. Construct a point estimate of the
average breaking strength of this type of cotton thread.
What is the standard error of your point estimate?
7.3.10 Consider the data set of die rolls given in DS 6.1.1.
Construct a point estimate of the probability of scoring
a 6. What is the standard error of your point estimate?
7.3.11 Television Set Quality
Consider the data set of television picture grades given in
DS 6.1.2. Construct a point estimate of the probability
that a television picture is satisfactory. What is the
standard error of your point estimate?
7.3.12 Eye Colors
Consider the data set of eye colors given in DS 6.1.3.
Construct a point estimate of the probability that a student
has blue eyes. What is the standard error of your point
estimate?
7.3.13 Restaurant Serving Times
Consider the data set of service times given in DS 6.1.4.
Construct a point estimate of the average service time.
What is the standard error of your point estimate?
7.3.14 Fruit Spoilage
Consider the data set of spoiled peaches given in
DS 6.1.5. Construct a point estimate of the average
number of spoiled peaches per box. What is the standard
error of your point estimate?
7.3.15 Telephone Switchboard Activity
Consider the data set of calls received by a switchboard
given in DS 6.1.6. Construct a point estimate of the
average number of calls per minute. What is the standard
error of your point estimate?
7.3.16 Paving Slab Weights
Consider the data set of paving slab weights given in
DS 6.1.7. Construct a point estimate of the average slab
weight. What is the standard error of your point estimate?
7.3.17 Spray Painting Procedure
Consider the data set of paint thicknesses given in DS 6.1.8.
Construct a point estimate of the average paint thickness.
What is the standard error of your point estimate?
7.3.18 Plastic Panel Bending Capabilities
Consider the data set of plastic panel bending capabilities
given in DS 6.1.9. Construct a point estimate of the
average deformity angle. What is the standard error of
your point estimate?

7.3 SAMPLING DISTRIBUTIONS
319
7.3.19 Unknown to an experimenter, the probability of a
prototype etching procedure producing a defective
part is p = 0.24. The experimenter examines
100 randomly selected parts and ﬁnds out whether
or not each one is defective. What is the probability
that the experimenter’s point estimate of p is within
0.05 of the true value? How does this probability change if
the experimenter examines 200 randomly selected parts?
7.3.20 The capacitances of certain electronic components have a
normal distribution with a mean μ = 174 and a standard
deviation σ = 2.8. If an engineer randomly selects a
sample of n = 30 components and measures their
capacitances, what is the probability that the engineer’s
point estimate of the mean μ will be within the interval
(173, 175)?
7.3.21 Unknown to an experimenter, when a coin is tossed there
is a probability of p = 0.63 of obtaining a head. The
experimenter tosses the coin 300 times in order to
estimate the probability p. What is the probability that the
experimenter’s point estimate of p will be within the
interval (0.62, 0.64)?
7.3.22 The weights of bricks are normally distributed with
μ = 110.0 and σ = 0.4. If the weights of 22 randomly
selected bricks are measured, what is the probability that
the resulting point estimate of μ will be in the interval
(109.9, 110.1)?
7.3.23 A scientist reports that the proportion of defective items
from a process is 12.6%. If the scientist’s estimate is
based on the examination of a random sample of 360
items from the process, what is the standard error of the
scientist’s estimate?
7.3.24 Suppose that components have weights that are normally
distributed with μ = 341 and σ = 2. An experimenter
measures the weights of a random sample of 20
components in order to estimate μ. What is the
probability that the experimenter’s estimate of μ will
be less than 341.5?
7.3.25 Unknown to an experimenter, the corrosion rate of a
certain type of chilled cast iron has a standard deviation
of 5.2. The experimenter measures the corrosion rates of
18 random samples of the chilled cast iron and estimates
the mean corrosion rate. What is the probability that the
experimenter’s estimate will be more than 2 away from
the correct value?
7.3.26 In a poll a random sample of 1400 respondents are asked
whether they are in support or against a proposal. What
is the largest possible value of the standard error of the
estimate of the overall proportion in favor of the proposal?
7.3.27 Unknown to an experimenter, the failure time of a
component has an exponential distribution with parameter
λ = 0.02 per minute. The experimenter takes 110
components, and ﬁnds out how many of them last longer
than one hour. This allows the experimenter to estimate
the probability that a component will last longer than one
hour. What is the probability that the experimenter’s
estimate is within 0.05 of the correct answer?
7.3.28 The pH levels of food items prepared in a certain way are
normally distributed with a standard deviation of
σ = 0.82. An experimenter estimates the mean pH level
by averaging the pH levels of a random sample of n items.
(a) If n = 5, what is the probability that the experimenter’s
estimate is within 0.5 of the true mean value?
(b) If n = 10, what is the probability that the
experimenter’s estimate is within 0.5 of the true
mean value?
(c) What sample size n is needed to ensure that there
is a probability of at least 99% that the experimenter’s
estimate is within 0.5 of the true mean value?
7.3.29 A company has installed 3288 ﬂow meters throughout an
extensive sewer system. Unknown to the company, 592 of
these meters are operating outside acceptable tolerance
limits, whereas the other 2696 meters are operating
satisfactorily. The company decides to estimate the
unknown proportion p of the meters that are operating
outside acceptable tolerance limits based on the
inspection of a random sample of 20 meters.
(a) What is the probability that the company’s estimate
of p will be within 0.1 of the correct value?
(b) Suppose that 2012 of the meters are easily accessible,
whereas the other 1276 meters are not easily acces-
sible. In addition, suppose that only 184 of the easily
accessible meters are operating outside acceptable
tolerance limits. If the company’s sample of 20
meters is biased due to the fact that the meters were
randomly chosen from the subset of easily accessible
meters, what is the probability that the company’s
estimate of p will be within 0.1 of the correct value?
7.3.30 In a survey of a random selection of 17 companies, 11 of
them had introduced a new IT initiative in the last 6
months. The estimate of the proportion of all companies
that have introduced a new IT initiative in the last 6
months has a standard error of:
A. 0.146
B. 0.136
C. 0.126
D. 0.116

320
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
7.3.31 A manager selects a random sample of 12 employees
from the company’s total workforce and gives them a test
that measures job related tension on a scale from 0 to 100.
The scores from the 12 employees have a sample mean of
63.32 and a sample standard deviation of 18.40. The
manager makes a report and states that based on these
data, the average job related tension score for the whole
workforce can be estimated as 63.32. This estimate has a
standard error of about:
A. 2.3
B. 3.3
C. 4.3
D. 5.3
7.3.32 Increasing the sample size:
A. Tends to increase the standard error of the estimates
with an increased experimental cost
B. Tends to decrease the standard error of the estimates
with a decreased experimental cost
C. Tends to increase the standard error of the estimates
with a decreased experimental cost
D. Tends to decrease the standard error of the estimates
with an increased experimental cost
7.3.33 Suppose that bricks have weights that are normally
distributed with a mean of 100 and a standard deviation of
1. If a random sample of ten bricks is taken, the sample
mean ¯x of the brick weights:
A. Will be equal to 100
B. Is equally likely to be smaller or larger than 100
C. Is more likely to be larger than 100 than smaller than
100
D. Is less likely to be larger than 100 than smaller than
100
7.3.34 Consider the data set 7, 9, 14, 15, 22. The standard error
of the sample mean is:
A. 2.53
B. 2.56
C. 2.59
D. 2.62
E. 2.65
7.3.35 In a political poll, the margin of error is related to the
standard error of the published number.
A. True
B. False
C. May be true or false depending on the number of
people sampled
7.4
Constructing Parameter Estimates
In this chapter the obvious point estimates for a success probability, a population mean,
and a population variance have been considered in detail. However, it is often of interest to
estimate parameters that require less obvious point estimates. For example, if an experimenter
observes a data set that is taken to consist of observations from a beta distribution, how should
the parameters of the beta distribution be estimated?
Two general methods of estimation can be used to solve questions of this kind. They are
the method of moments and maximum likelihood estimation. They are described next and
are illustrated on the standard problems of estimating a success probability and a normal mean
and variance. The two methods are then applied to some more complicated examples.
7.4.1
The Method of Moments
Method of Moments Point Estimate for One Parameter
If a data set consists of observations x1, . . . , xn from a probability distribution that
depends upon one unknown parameter θ, the method of moments point estimate ˆθ of
the parameter is found by solving the equation
¯x = E(X)
In other words, the point estimate is found by setting the sample mean equal to the
population mean.

7.4 CONSTRUCTING PARAMETER ESTIMATES
321
As an example of the implementation of this estimation method, suppose that x1, . . . , xn
are a set of Bernoulli observations, with each taking the value 1 with probability p and the
value 0 with probability 1 −p. The expectation of the Bernoulli distribution is E(X) = p, so
that the method of moments point estimate of p is found from the equation
¯x = p
This simply provides the usual point estimate ˆp = x/n, where x is the number of data
observations that take the value 1.
Method of Moments Point Estimates for Two Parameters
If a data set consists of observations x1, . . . , xn from a probability distribution that
depends upon two unknown parameters, the method of moments point estimates of the
parameters are found by solving the equations
¯x = E(X)
and
s2 = Var(X)
This is an intuitively reasonable method of estimation since it simply sets the
population mean and variance equal to the sample mean and variance. Some
practitioners may use n instead of n −1 in the denominator of s2 here, but it generally
makes little difference to the point estimates.
Normally distributed data provide a simple example of estimating two parameters by the
method of moments. Since E(X) = μ and Var(X) = σ 2 for a N(μ, σ 2) distribution, the
method of moments immediately gives the usual point estimates
¯x = ˆμ
and
s2 = ˆσ 2
In general, the method of moments is a simple, easy-to-use method for obtaining sensible
point estimates. However, it is not foolproof. Suppose that the data observations
2.0
2.4
3.1
3.9
4.5
4.8
5.7
9.9
are obtained from a U(0, θ) distribution. In this case the upper endpoint of the uniform dis-
tribution is the unknown parameter to be estimated. Since the expectation of a U(0, θ) distri-
bution is
E(X) = θ
2
and the sample mean is ¯x = 4.5375, the method of moments point estimate of θ is obtained
from the equation
4.5375 = θ
2
This gives
ˆθ = 2 × 4.5375 = 9.075
The problem with this point estimate is that it is clearly impossible! One of the data
observations 9.9 exceeds the value ˆθ, whereas the true value of θ must necessarily be larger
than all the data values. Nevertheless, even though this example shows that point estimation
using the method of moments may be unsuitable in certain cases, in general it is a simple and
sensible method.

322
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
The method of moments can be generalized to problems with three or more unknown
parameters by equating additional population moments
E(X −μ)k
where k ≥3, with the corresponding sample moments. However, examples of this kind are
rare.
7.4.2
Maximum Likelihood Estimates
Maximum likelihood estimation is a more technical method of obtaining point estimates, yet it
is a very powerful method with a great deal of theoretical justiﬁcation behind its use. Consider
a set of data values x1, . . . , xn that are taken to be observations with a probability density
function f (x, θ) depending on one unknown parameter θ. The joint density function of the
data observations is therefore
f (x1, . . . , xn, θ) = f (x1, θ) × · · · × f (xn, θ)
which can be thought of as the “likelihood” of observing the data values x1, . . . , xn for a given
value of θ.
Maximum Likelihood Estimate for One Parameter
If a data set consists of observations x1, . . . , xn from a probability distribution f (x, θ)
depending upon one unknown parameter θ, the maximum likelihood estimate ˆθ of
the parameter is found by maximizing the likelihood function
L(x1, . . . , xn, θ) = f (x1, θ) × · · · × f (xn, θ)
This method of estimation has an intuitive appeal to it, since it asks the question
For what parameter value is the observed data “most likely” to have arisen?
In practice, the maximization of the likelihood function is usually performed by taking the
derivative of the likelihood function with respect to the parameter value. Often, however, it is
convenient to take the natural log of the likelihood function before differentiating. Since the
naturallogisamonotonicfunction,maximizingthelog-likelihoodisequivalenttomaximizing
the likelihood.
To illustrate this estimation method, suppose again that x1, . . . , xn are a set of Bernoulli
observations, with each taking the value 1 with probability p and the value 0 with probability
1 −p. In this case, the probability distribution (actually a probability mass function) is
f (1, p) = p
and
f (0, p) = 1 −p
A succinct way of writing this is
f (xi, p) = pxi (1 −p)1−xi
The likelihood function is therefore
L(x1, . . . , xn, p) =
n
i=1
pxi (1 −p)1−xi = px(1 −p)n−x

7.4 CONSTRUCTING PARAMETER ESTIMATES
323
where x = x1 +· · ·+ xn, and the maximum likelihood estimate ˆp is the value that maximizes
this. The log-likelihood is
ln(L) = x ln(p) + (n −x) ln(1 −p)
and
d ln(L)
dp
= x
p −n −x
1 −p
Setting this expression equal to 0 and solving for p produce
ˆp = x
n
which can be checked to be a true maximum of the likelihood function. Consequently, the
method of maximum likelihood estimation is seen to produce the usual estimate of the success
probability p, which is the proportion of the sample that are successes.
Maximum Likelihood Estimate for Two Parameters
If a data set consists of observations x1, . . . , xn from a probability distribution
f (x, θ1, θ2) depending upon two unknown parameters, the maximum likelihood
estimates ˆθ1 and ˆθ2 are the values of the parameters that jointly maximize the
likelihood function
L(x1, . . . , xn, θ1, θ2) = f (x1, θ1, θ2) × · · · × f (xn, θ1, θ2)
Again, the best way to perform the joint maximization is usually to take derivatives of the
log-likelihood with respect to θ1 and θ2 and to set the two resulting expressions equal to 0.
The normal distribution is an example of a distribution with two parameters, with a prob-
ability density function
f (x, μ, σ 2) =
1
√
2πσ
e−(x−μ)2/2σ 2
The likelihood of a set of normal observations is therefore
L(x1, . . . , xn, μ, σ 2) =
n
i=1
f (xi, μ, σ 2)
=

1
2πσ 2
n/2
exp

−
n

i=1
(xi −μ)2/2σ 2

so that the log-likelihood is
ln(L) = −n
2 ln(2πσ 2) −
n
i=1(xi −μ)2
2σ 2
Taking derivatives with respect to the parameter values μ and σ 2 gives
d ln(L)
dμ
=
n
i=1(xi −μ)
σ 2
and
d ln(L)
dσ 2
= −n
2σ 2 +
n
i=1(xi −μ)2
2σ 4

324
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
Setting d ln(L)/dμ = 0 gives
ˆμ = ¯x
and setting d ln(L)/dσ 2 = 0 then gives
ˆσ 2 =
n
i=1(xi −ˆμ)2
n
=
n
i=1(xi −¯x)2
n
which are consequently the maximum likelihood estimates of the parameters. It is interesting
to notice that these point estimates come out to be the usual estimates that have been discussed
in this chapter, except that the variance estimate uses n rather than n −1 in the denominator.
As with point estimates produced by the method of moments, maximum likelihood es-
timates are generally sensible point estimates, and theoretical results show that they have
very good properties when the sample size n is reasonably large. If there are three or more
unknown parameters to be estimated, then the method of maximum likelihood estimation can
be generalized in the obvious manner.
In most cases the two methods of estimation produce identical point estimates, although
in certain cases the estimates may differ slightly. In certain cases the point estimates obtained
from these methods may not be unbiased, as was seen with the maximum likelihood estimate
of the normal variance, but any bias is usually small and decreases as the sample size n
increases.
7.4.3
Examples
Example 27
Glass Sheet Flaws
Suppose that the quality inspector at the glass manufacturing company inspects 30 randomly
selected sheets of glass and records the number of ﬂaws found in each sheet. These data values
are shown in Figure 7.15. If the distribution of the number of ﬂaws per sheet is taken to have
a Poisson distribution, how should the parameter λ of the Poisson distribution be estimated?
If the random variable X has a Poisson distribution with parameter λ, then
E(X) = λ
Consequently, the method of moments immediately suggests that the parameter estimate
should be
ˆλ = ¯x
This is also the maximum likelihood estimate, which can be shown as follows. The probability
mass function of a data observation xi is
f (xi, λ) = e−λλxi
xi!
so that the likelihood is
L(x1, . . . , xn, λ) =
n
i=1
f (xi, λ) =
e−nλλ(x1+···+xn)
(x1! × · · · × xn!)
The log-likelihood is therefore
ln(L) = −nλ + (x1 + · · · + xn) ln(λ) −ln(x1! × · · · × xn!)
FIGURE 7.15
Glass sheet ﬂaws data set
0
1
1
1
0
0
0
2
0
1
0
1
0
0
0
0
0
1
0
2
0
0
3
1
2
0
0
1
0
0

7.4 CONSTRUCTING PARAMETER ESTIMATES
325
so that
d ln(L)
dλ
= −n + (x1 + · · · + xn)
λ
Setting this expression equal to 0 gives ˆλ = ¯x.
The sample average of the 30 data observations in Figure 7.15 is 0.567, so that the quality
inspector should use the point estimate
ˆλ = 0.567
In addition, since each data observation has a variance of λ,
Var( ¯X) = λ
n
so that the standard error of the estimate of a Poisson parameter can be calculated as
s.e.(ˆλ) =

ˆλ
n
The quality inspector’s point estimate ˆλ = 0.567 consequently has a standard error of
s.e.(ˆλ) =

0.567
30
= 0.137
Example 26
Fish Tagging and
Recapture
Fish tagging and recapture present a way to estimate the size of a ﬁsh population. Suppose that
a ﬁsherman wants to estimate the ﬁsh stock N of a lake and that 34 ﬁsh have been tagged and
released back into the lake. If, over a period of time, the ﬁsherman catches 50 ﬁsh (without
release) and 9 of them are tagged, an intuitive point estimate of the total number of ﬁsh in the
lake is
ˆN = 34 × 50
9
≃189
This point estimate is based upon the reasoning that the proportion of ﬁsh in the lake that are
tagged should be roughly equal to the proportion of the ﬁsherman’s catch that is tagged.
This point estimate is also the method of moments point estimate. Under the assumption
that all the ﬁsh are equally likely to be caught, the distribution of the number of tagged ﬁsh X
in the ﬁsherman’s catch of 50 ﬁsh is a hypergeometric distribution with r = 34, n = 50, and
N unknown. The expectation of X is therefore
E(X) = nr
N = 50 × 34
N
and the method of moments point estimate of N is found by equating this to the observed
value x = 9. Notice that here there is only one data observation x, which is therefore ¯x.
A similar point estimate is arrived at if the binomial approximation to the hypergeometric
distribution is employed. In this case the success probability p = r/N is estimated to be
ˆp = x
n = 9
50
with
ˆN = r
ˆp

326
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
FIGURE 7.16
Bee colony data set
0.28
0.32
0.09
0.35
0.45
0.41
0.06
0.16
0.16
0.46
0.35
0.52
0.29
0.31
Example 36
Bee Colonies
An entomologist collects data on the proportion of worker bees that leave a colony with a
queen bee. Calculations from 14 colonies provide the data values given in Figure 7.16. If
the entomologist wishes to model this proportion with a beta distribution, how should the
parameters be estimated?
The simplest way to answer this question is to use the method of moments. Recall that a
beta distribution with parameters a and b has an expectation and variance
E(X) =
a
a + b
and
Var(X) =
ab
(a + b)2(a + b + 1)
The 14 data observations have a mean of 0.3007 and a variance of 0.01966.
The point estimates ˆa and ˆb are consequently the solutions to the equations
a
a + b = 0.3007
and
ab
(a + b)2(a + b + 1) = 0.01966
which are ˆa = 2.92 and ˆb = 6.78.
7.4.4
Problems
7.4.1 Suppose that 23 observations are collected from a Poisson
distribution, and the sample average is ¯x = 5.63.
Construct a point estimate of the parameter of the
Poisson distribution and calculate its standard error.
7.4.2 Suppose that a set of observations is collected from a beta
distribution, with an average of ¯x = 0.782 and a variance
of s2 = 0.0083. Obtain point estimates of the parameters
of the beta distribution.
7.4.3 Consider a set of independent data observations x1, . . . , xn
that have an exponential distribution with an unknown
parameter λ. Show that the method of moments and
maximum likelihood estimation both produce the point
estimate
ˆλ = 1
¯x
7.4.4 If the random variables X1, . . . , Xk have a multinomial
distribution with parameters n and p1, . . . , pk, the
likelihood function is
L(x1, . . . , xk, p1, . . . , pk) =
n!
x1! · · · xk! px1
1 · · · pxk
k
Maximize this likelihood subject to the condition that
p1 + · · · + pk = 1
in order to ﬁnd the maximum likelihood estimates ˆpi,
1 ≤i ≤k.
7.4.5 Consider a set of independent data observations x1, . . . , xn
that have a gamma distribution with k = 5 and an unknown
parameter λ. Show that the method of moments and
maximum likelihood estimation both produce the point
estimate
ˆλ = 5
¯x

7.7
SUPPLEMENTARY PROBLEMS
327
7.5
Case Study: Microelectronic Solder Joints
Recall the data set in Figure 6.40 of the nickel layer thicknesses on the substrate bond pads
produced by a new method. If μ represents the average amount of nickel deposited by this
new method, then it can be estimated by
ˆμ = ¯x = 2.7688
with a standard error
s.e.( ˆμ) =
s
√n = 0.0260
√
16
= 0.0065
The data set in Figure 6.41 can be used to estimate pb, the probability that a solder joint
will have a barrel shape for that production method, as
ˆpb = 451
512 = 0.881
which has a standard error
s.e.( ˆpb) =

ˆpb(1 −ˆpb)
n
=

0.881(1 −0.881)
512
= 0.014
7.6
Case Study: Internet Marketing
When a particular banner advertisement is employed on a web page, there are 8548 clicks on
the banner over a certain period of time directing the user to the organisation’s own website,
and these lead to 332 purchases. What does this tell us about the true effectiveness of the
banner advertisement in terms of the proportion of purchases to clicks?
This proportion can be estimated as
ˆp = 332
8548 = 3.88%
Furthermore, information about the accuracy of this estimate is contained in its standard error,
which is
s.e.( ˆp) =
1
8548

332 × (8548 −332)
8548
= 0.21%
7.7
Supplementary Problems
7.7.1 Suppose that X1 and X2 are independent random
variables with
E(X1) = E(X2) = μ
and
Var(X1) = Var(X2) = 1
Show that the point estimate
ˆμ1 = X1 + X2
4
+ 5
has a smaller mean square error than the point estimate
ˆμ2 = X1 + X2
2
when
|μ −10| ≤
√
6
2
Why would you expect ˆμ1 to have a smaller mean square
error than ˆμ2 when μ is close to 10?

328
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
7.7.2 Suppose that X ∼B(12, p) and consider the point
estimate
ˆp = X
14
(a) What is the bias of this point estimate?
(b) What is the variance of this point estimate?
(c) Show that this point estimate has a mean square
error of
3p −2p2
49
(d) Show that this mean square error is smaller than the
mean square error of X/12 when p ≤0.52.
7.7.3 Let X1, . . . , Xn be a set of independent random variables
with a U(0, θ) distribution, and let
T = max{X1, . . . , Xn}
(a) Explain why the cumulative distribution function of
T is
F(t) =
	 t
θ

n
for 0 ≤t ≤θ.
(b) Show that the probability density function of T is
f (t) = n tn−1
θn
for 0 ≤t ≤θ.
(c) Show that
ˆθ = n + 1
n
T
is an unbiased point estimate of θ.
(d) What is the standard error of ˆθ?
(e) Suppose that n = 10 and that the following data
values are obtained:
1.2
6.3
7.3
6.4
3.5
0.2
4.6
7.1
5.0
1.8
What are the values of ˆθ and the standard error of ˆθ?
7.7.4 As in Problem 7.6.3, let X1, . . . , Xn be a set of
independent random variables with a U(0, θ) distribution,
and let
T = max{X1, . . . , Xn}
Explain why the likelihood function L(x1, . . . , xn, θ) is
equal to
1
θn
if θ ≥t = max{x1, . . . , xn}, and is equal to 0 otherwise.
Sketch the likelihood function against θ, and deduce that
the maximum likelihood estimate of θ is ˆθ = t. What is
the bias of this point estimate?
7.7.5 Consider a set of independent data observations
x1, . . . , xn that have a geometric distribution with an
unknown parameter p. Show that the method of moments
and maximum likelihood estimation both produce the
point estimate
ˆp = 1
¯x
7.7.6 Bird Species Identiﬁcation
Consider the data set of bird species given in DS 6.7.1.
Construct a point estimate of the probability that a bird
has black markings. What is the standard error of your
point estimate?
7.7.7 Oil Rig Accidents
Consider the data set of monthly accidents given in
DS 6.7.2. Construct a point estimate of the average
number of accidents per month. What is the standard
error of your point estimate?
7.7.8 Programming Errors
Consider the data set of programming errors given in
DS 6.7.3. Construct a point estimate of the average
number of errors per month. What is the standard error of
your point estimate?
7.7.9 Osteoporosis Patient Heights
Consider the data set of osteoporosis patient heights
given in DS 6.7.4. Construct a point estimate of the
average height. What is the standard error of your point
estimate?
7.7.10 Bamboo Cultivation
Consider the data set of bamboo shoot heights given in
DS 6.7.5. Construct a point estimate of the average
height. What is the standard error of your point
estimate?
7.7.11 Consider the usual point estimates s2
1 and s2
2 of the
variance σ 2 of a normal distribution based on sample
sizes n1 and n2, respectively. What is the relative
efﬁciency of the point estimate s2
1 to the point estimate s2
2?
7.7.12 Suppose that among 24,839 customers of a certain
company, exactly 11,842 feel “very satisﬁed” with the
service they received. In order to estimate the satisfaction
levels of the customers, a manager contacts a random
sample of 80 of these customers and ﬁnds out how many
of them were “very satisﬁed.” What is the probability that
the manager’s estimate of the proportion of “very

7.7 SUPPLEMENTARY PROBLEMS
329
satisﬁed” customers in this group is within 0.10 of the
true value?
7.7.13 The viscosities of chemical infusions obtained from a
speciﬁc production technique are normally distributed
with a standard deviation σ = 3.9. If a chemist is able to
measure the viscosities of 15 independent samples of the
infusions, what is the probability that the resulting point
estimate of the mean μ will be within 0.5 of the true
value? How does this probability change if a sample of
40 independent infusions is obtained?
7.7.14 Soil Compressibility Tests
Recall the data set of soil compressibility measurements
given in DS 6.7.6. Construct a point estimate of the
average soil compressibility, and ﬁnd its standard error.
What is a point estimate of the upper quartile of the
distribution of soil compressibilities?
7.7.15 An engineer assumes that the distribution of the
breaking strengths of ﬁbers is N(280, 2.5) and uses this
distribution to perform an analysis of whether the average
breaking strength of a collection of 20 ﬁbers will exceed
a speciﬁed value. Is the engineer doing probability theory
or statistical inference?
7.7.16 An experimenter assumes a probability distribution for
the lengths of telephone calls arriving at a hotline, and
predicts the lengths of calls that will be obtained in a
random sample of calls. Is the experimenter using
probability theory or statistical inference?
7.7.17 Suppose that an engineer wishes to estimate the
proportion of defective products from a production line.
A random sample of 220 products are tested, of which
39 are found to be defective. What is the standard error
of the engineer’s estimate of the proportion of defective
products?
7.7.18 The probability that a medical treatment is effective is
0.68, unknown to a researcher. In an experiment to
investigate the effectiveness of the treatment, the
researcher applies the treatment in 140 cases and
measures whether the treatment is effective or not. What
is the probability that the researcher’s estimate of the
probability that the medical treatment is effective is
within 0.05 of the correct answer?
7.7.19 The biomass of 12 samples was measured, and the
following values were obtained:
78
67
58
93
63
70
59
82
88
66
50
73
(a) What is the estimate of the mean biomass?
(b) What is the standard error of the estimate of the mean
biomass?
(c) What is the sample median?
7.7.20 An experimenter measures the weights of a random
sample of 20 items and uses the information to estimate
the overall population mean. Is the experimenter using
probability theory or statistical inference?
7.7.21 A random sample of components from a supplier is tested
in order to estimate the probability that a component from
that supplier satisﬁes the design requirements. Is this
probability theory or statistical inference?
7.7.22 Are the following statements true or false?
(a) Statistical inference uses the results of an experiment
to make inferences on some properties of an
unknown underlying probability distribution.
(b) The margin of error in a political poll is based on the
standard error of the estimate obtained.
(c) An experimenter collects some data from a process
and uses it to estimate some properties of the process.
The experimenter is using statistical inference, not
probability theory, because the known data is used to
make inferences about the unknown parameters of
the process.
(d) The standard error of a point estimate provides an
indication of its accuracy.
7.7.23 Components have lengths that are independently
distributed as a normal distribution with μ = 723 and
σ = 3. If an experimenter measures the lengths of a
random sample of 11 components, what is the probability
that the experimenter’s estimate of μ will be between
722 and 724?
7.7.24 An experimenter wishes to estimate the mean weight of
some components where the weights have a normal
distribution with a standard deviation of 40.0.
(a) If the experimenter has a sample size of 10, what is
the probability that the estimate is within 20.0 of the
correct value?
(b) What is the probability if the sample size is 20?
7.7.25 In a political poll, responses were obtained from a sample
of 1962 people about which candidate they preferred.
There were 852 people who reported that they preferred
candidate A. What is the estimate of the proportion of the
overall electorate who prefer candidate A? What is the
standard error of this estimate?

330
CHAPTER 7
STATISTICAL ESTIMATION AND SAMPLING DISTRIBUTIONS
For Problems 7.7.26–7.7.33 use the data sets to practice ﬁnding
parameter point estimates and their standard errors.
7.7.26 Glass Fiber Reinforced Polymer Tensile Strengths
The data set in DS 6.7.7.
7.7.27 Infant Blood Levels of Hydrogen Peroxide
The data set in DS 6.7.8.
7.7.28 Paper Mill Operation of a Lime Kiln
The data set in DS 6.7.9.
7.7.29 River Salinity Levels
The data set in DS 6.7.10.
7.7.30 Dew Point Readings from Coastal Buoys
The data set in DS 6.7.11.
7.7.31 Brain pH levels
The data set in DS 6.7.12.
7.7.32 Silicon Dioxide Percentages in Ocean Floor
Volcanic Glass
The data set in DS 6.7.13.
7.7.33 Network Server Response Times
The data set in DS 6.7.14.
7.7.34 When presented with an estimate of an unknown quantity
based upon the analysis of some data, the sophisticated
statistician (like us) would want to know the standard
error of that estimate because the standard error would
provide information about the accuracy of the estimate.
A. True
B. False
7.7.35 A researcher assumed that 0.25% of all companies have
introduced a new IT initiative in the last 6 months and
predicted how many companies in a random sample of
10 companies would have introduced a new IT initiative
in the last 6 months.
A. The researcher was doing probability theory rather
than statistical inference.
B. The researcher was doing statistical inference rather
than probability theory.
7.7.36 A researcher selects a group of 20 companies in a certain
sector and calculates their annual price increases. The
20 price increases have a sample mean of 12.72% and a
sample standard deviation of 2.29%. The researcher states
that the average annual price increase within this sector
can be estimated at 12.72%. This estimate has a standard
error of about:
A. 0.21
B. 0.31
C. 0.41
D. 0.51
7.7.37 In a survey of a random selection of 25 customers,
8 indicated that they would deﬁnitely upgrade their
service. The estimate of the proportion of all customers
who would deﬁnitely upgrade their service has a standard
error of about:
A. 0.09
B. 0.10
C. 0.11
D. 0.12
7.7.38 Consider the standard error of an estimate.
A. The standard error of an estimate provides
information about its accuracy.
B. A larger standard error implies that the estimate is
more accurate.
C. Both of the above.
D. Neither of the above.

Guide to Statistical Inference Methodologies
This table can be used to match a statistical inference methodology to a data set and a research question. The examples listed
are typical problems that can be used with that methodology.
One-Sample Analyses
You have a measurement variable that is continuous. The objective is to make inferences about the average of
the variable.
See Chapter 8 (also
Sections 15.1 and 17.3).
• Example 48—Car Fuel Efﬁciency
The continuous variable is the car fuel efﬁciency. The objective is make inferences about the average
fuel efﬁciency.
You have a binary variable. The objective is to make inferences about the related probability.
See Section 10.1.
• Example 39—Cattle Inoculations
The binary variable is whether or not there is a serious adverse reaction. The objective is make inferences
about the probability of a serious adverse reaction.
You have a categorical variable with three or more levels. The objective is to make inferences about the
probabilities of the different levels.
See Section 10.3.
• Example 13—Factory Floor Accidents
The categorical variable is the day of the week of the accident. The objective is to investigate whether
accidents are more likely to occur on some days than others.
Two-Sample Analyses
You have a measurement variable that is continuous, and a categorical variable with two levels. The objective
is to see whether and how the measurement variable is different for the two groups.
See Section 9.2 for paired
data (also section 15.1.2).
• Example 56—Radar Detection Systems
The detection distance is the continuous variable, and the two systems are the two levels of the categorical
variable. The objective is to compare the two systems in terms of their detection distances.
See Section 9.3 for unpaired
independent data (also
section 15.2).
You have two binary variables. The objective is to compare the two related probabilities.
• Example 61—Political Polling
See Section 10.2.
The binary variables are whether or not the mayor is supported for the two age groups. The objective
is to compare the probabilities of supporting the mayor for the two age groups.
Analysis of Variance
This is similar to two-sample analyses, except that now there are three or more groups being compared. You
have a measurement variable that is continuous and a categorical variable with three or more values. The
objective is to see whether and how the measurement variable is different for the various groups.
See Section 11.1 without a
blocking variable (also
Section 15.3).
• Example 63—Roadway Base Aggregates
The resilient modulus of the aggregate material is the continuous variable, and the four suppliers are
the categorical variable. The objective is to compare the four suppliers in terms of the resilient modulus
of their aggregate material.
See Section 11.2 with a
blocking variable (also
Section 15.3).

Simple Linear Regression
You have two continuous variables that are paired together, so that you would want to view the data with a
scatter plot. Each dot on the scatter plot is an “experimental unit.” The objective is to see whether there is any
evidence that the two variables are related, and if so, to use one variable to predict the other.
• Example 67—Car Plant Electricity Usage
See Chapter 12.
The factory’s electricity consumption is one continuous variable, and production level is the other
continuous variable. The calendar months are the “experimental units. The objective is to use production
level to predict electricity consumption.
Multiple Linear Regression
Multiple linear regression is an extension of simple linear regression where there is again one continuous output
variable but there is now more than one input variable. The objective is to see how the set of input variables can
be used to model and predict the single output variable. The input variables are continuous variables, except
that “dummy variables” may be used to model simple categorical variables.
• Example 71—Supermarket Deliveries
See Chapter 13.
The unloading time of a truck is the continuous output variable, and the objective is to see how it
depends upon the volume and the weight of the load (which are two continuous input variables) together
with whether it is the day or night shift (a binary categorical input variable that is coded as a
dummy variable).
Multifactor Analyses
You have a continuous variable, and the objective is to see how it depends upon a set of categorical input
variables.
• Example 74—Company Transportation Costs
See Chapter 14.
The driving time is the continuous variable, and the objective is to see how it depends upon the period
of day and the route, which are both categorical variables.
Contingency Table Analyses
You have an experimental unit from which several categorical variables are obtained. The objective is to
investigate the relationships between the categorical variables.
See Section 10.4.
• Example 29—Drug Allergies
The experimental unit is a patient, and the two categorical variables are which drug is administered and
the allergic reaction type. The objective is to see whether the drugs are identical in terms of the allergic
reaction they generate.
(Section 10.3 when there is
only one variable.)
Control Charts
You have either a continuous or a categorical variable that you want to monitor over time to see whether any
changes occur in the underlying process.
• Example 32—Steel Girder Fractures
See Chapter 16.
Steel girders are sampled periodically, and the numbers of fractures are measured. The control
chart is employed to detect if there is a sudden jump in the number of fractures.

C H A P T E R E I G H T
Inferences on a Population Mean
Random variable theory and estimation methods are combined in this chapter to provide an
analysis of a single sample of continuous data observations taken from a particular population.
Inference procedures designed to investigate the population mean μ are described. These
inference procedures are conﬁdence interval construction and hypothesis testing, which
are two fundamental techniques of statistical inference. The methodologies discussed are
commonly referred to as “t-intervals” and “t-tests,” and these are among the most basic and
widely employed of all statistical inference methods.
8.1
Confidence Intervals
8.1.1
Conﬁdence Interval Construction
The discussions in this chapter concern the analysis of a sample of data observations x1, . . . , xn
that are independent observations from some unknown continuous probability distribution.
Statistical methodologies for investigating the unknown population mean are described. The
data set of metal cylinder diameters given in Figure 6.5 is a typical data set of this kind for
which μ is the average diameter of cylinders produced in this manner.
A conﬁdence interval for μ is an interval that contains “plausible” values of the parameter
μ (the notion of plausibility is given a rigorous deﬁnition in Section 8.2). It is a simple
combination of the point estimate ˆμ = ¯x together with its estimated standard error s/√n. A
conﬁdence interval is associated with a conﬁdence level, which is usually written as 1 −α,
and which indicates the conﬁdence that the experimenter has that the parameter μ actually lies
within the given conﬁdence interval. Conﬁdence levels of 90%, 95%, and 99% are typically
used, which correspond to α values of 0.10, 0.05, and 0.01, respectively.
Conﬁdence Intervals
A conﬁdence interval for an unknown parameter θ is an interval that contains a set of
plausible values of the parameter. It is associated with a conﬁdence level 1 −α, which
measures the probability that the conﬁdence interval actually contains the unknown
parameter value.
The t-intervals discussed in this section, and more generally any t-procedure such as
these t-intervals or the hypothesis tests discussed in Section 8.2, are appropriate for making
inferences on a population mean in a wide variety of settings. Technically, the implementation
of these procedures requires that the sample mean be an observation from a normal distribution,
and for sample sizes n ≥30 the central limit theorem ensures that this will be a reasonable
assumption.Forsmallersamplesizestherequirementismetifthedataarenormallydistributed,
333

334
CHAPTER 8
INFERENCES ON A POPULATION MEAN
and in fact the t-intervals provide a sensible analysis unless the data observations are clearly
not normally distributed. In this latter case the general nonparametric inference methods
discussed in Chapter 15 may be employed, or alternative procedures for speciﬁc distributions
may be used such as the procedure described in Section 17.3.1 for data from an exponential
distribution.
Inferences on a Population Mean
Inference methods on a population mean based upon the t-procedure are appropriate
for large sample sizes n ≥30 and also for small sample sizes as long as the data
can reasonably be taken to be approximately normally distributed. Nonparametric
techniques can be employed for small sample sizes with data that are clearly not
normally distributed.
The most commonly used conﬁdence interval for a population mean μ based on a sample
of n continuous data observations with a sample mean ¯x and a sample standard deviation s is
a two-sided t-interval, which is constructed as
μ ∈

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

As illustrated in Figure 8.1, the interval is centered at the “best guess” ˆμ = ¯x and extends
on either side by an amount equal to a critical point tα/2,n−1 (as deﬁned in Section 5.4.3)
multiplied by the standard error of ˆμ. Thus, it is useful to understand that the conﬁdence
interval is constructed as
μ ∈( ˆμ −critical point × s.e.( ˆμ), ˆμ + critical point × s.e.( ˆμ))
FIGURE 8.1
A two-sided t-interval
x
ˆμ
Critical point × s.e. (   )
−
√
n
x
ˆμ
¯
¯
s
tα/2,n−1 × √n
tα/2,n−1s
+
√
n
x¯
tα/2,n−1s

8.1 CONFIDENCE INTERVALS
335
Two-Sided t-Interval
A conﬁdence interval with conﬁdence level 1 −α for a population mean μ based upon
a sample of n continuous data observations with a sample mean ¯x and a sample
standard deviation s is
μ ∈

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

The interval is known as a two-sided t-interval or variance unknown conﬁdence
interval.
The length of the conﬁdence interval is
L = 2 tα/2,n−1s
√n
= 2 × critical point × s.e.( ˆμ)
which is proportional to the standard error of ˆμ. As the standard error of ˆμ decreases, so
that ˆμ = ¯x becomes a more “accurate” estimate of μ, the length of the conﬁdence interval
decreases so that there are fewer plausible values for μ. In other words, a more accurate
estimate of μ allows the experimenter to eliminate more values of μ from contention.
The length of the conﬁdence interval L also depends upon the critical point tα/2,n−1. Recall
that this critical point is deﬁned by
P(X ≥tα/2,n−1) = α/2
where the random variable X has a t-distribution with degrees of freedom n −1. The con-
ﬁdence interval depends upon the conﬁdence level 1 −α through this critical point. As the
conﬁdence level increases, so that α decreases, the critical point tα/2,n−1 also increases so that
the conﬁdence interval becomes longer. This relationship is illustrated in Figure 8.2 and may
be summarized as
Higher conﬁdence levels require longer conﬁdence intervals.
Finally, notice that the degrees of freedom of the critical point are one fewer than the sample
size n.
FIGURE 8.2
Higher conﬁdence levels require
longer conﬁdence intervals
Confidence level 1 − α1
Confidence level 1 − α2
1 − α1 > 1 − α2
x¯
x¯

336
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Effect of the Conﬁdence Level on the Conﬁdence Interval Length
The length of a conﬁdence interval depends upon the conﬁdence level 1 −α through
the critical point. As the conﬁdence level 1 −α increases, the length of the conﬁdence
interval also increases.
Example 17
Milk Container
Contents
The data set of milk container weights is given in Figure 6.6, and summary statistics are given
in Figure 6.27.
Suppose that a conﬁdence interval is required with conﬁdence level 95%. In this case
α = 0.05, so that the relevant critical point is tα/2,n−1 = t0.025,49 = 2.0096 (which can
be obtained exactly from the computer or approximately from Table III). Consequently, the
conﬁdence interval is

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

=

2.0727 −2.0096 × 0.0711
√
50
, 2.0727 + 2.0096 × 0.0711
√
50

= (2.0525, 2.0929)
This result has the interpretation that the experimenter is 95% conﬁdent that the average milk
container content is between about 2.053 and 2.093 liters.
Since t0.005,49 = 2.680, a conﬁdence interval with conﬁdence level 99% is

2.0727 −2.680 × 0.0711
√
50
, 2.0727 + 2.680 × 0.0711
√
50

= (2.0457, 2.0996)
Similarly, t0.05,49 = 1.6766, so that a conﬁdence interval with conﬁdence level 90% is

2.0727 −1.6766 × 0.0711
√
50
, 2.0727 + 1.6766 × 0.0711
√
50

= (2.0558, 2.0895)
The three conﬁdence intervals are shown in Figure 8.3, and clearly the conﬁdence interval
length increases as the conﬁdence level rises.
FIGURE 8.3
Conﬁdence intervals for the mean
milk container weight
Confidence level 90%
Confidence level 95%
Confidence level 99%
2.0558
2.0895
2.0525
2.0929
2.0457
2.0996
x = 2.0727
¯
x = 2.0727
¯
x = 2.0727
¯

8.1 CONFIDENCE INTERVALS
337
Example 14
Metal Cylinder
Production
A data set of 60 metal cylinder diameters is given in Figure 6.5, and summary statistics are
given in Figure 6.29. The critical points required for conﬁdence interval construction are given
in Figure 8.4.
With α = 0.10, t0.05,59 = 1.671 so that a conﬁdence interval with conﬁdence level 90% is

49.999 −1.671 × 0.134
√
60
, 49.999 + 1.671 × 0.134
√
60

= (49.970, 50.028)
Similarly, t0.025,59 = 2.001 so that a conﬁdence interval with conﬁdence level 95% is

49.999 −2.001 × 0.134
√
60
, 49.999 + 2.001 × 0.134
√
60

= (49.964, 50.034)
and t0.005,59 = 2.662 so that a conﬁdence interval with conﬁdence level 99% is

49.999 −2.662 × 0.134
√
60
, 49.999 + 2.662 × 0.134
√
60

= (49.953, 50.045)
These conﬁdence intervals are illustrated in Figure 8.5.
A sensible way to summarize these results might be to notice that based upon this sample
of 60 randomly selected cylinders, the experimenter can conclude (with over 99% certainty)
that the average cylinder diameter lies within 0.05 mm of 50.00 mm, that is, within the interval
(49.95, 50.05). Of course, it is important to remember that this conﬁdence interval is for the
mean cylinder diameter, and not for the actual diameter of a randomly selected cylinder. In
fact, the sample contains a cylinder as thin as 49.737 mm and as thick as 50.362 mm.
The justiﬁcation of the two-sided t-intervals introduced in this section is straightforward.
The result at the end of Section 7.3.3 states that
√n( ¯X −μ)
S
∼tn−1
Sample size n = 60
Conﬁdence level 90%:
t0.05,59 = 1.671
Conﬁdence level 95%:
t0.025,59 = 2.001
Conﬁdence level 99%:
t0.005,59 = 2.662
FIGURE 8.4
Critical points for the construction of two-sided
conﬁdence intervals for the mean metal cylinder
diameter
90%
49.970
50.028
x = 49.999
¯
49.964
50.034
49.953
50.045
95%
x = 49.999
¯
99%
x = 49.999
¯
FIGURE 8.5
Conﬁdence intervals for the mean metal cylinder diameter

338
CHAPTER 8
INFERENCES ON A POPULATION MEAN
and so the deﬁnition of the critical points of the t-distribution ensures that
P

−tα/2,n−1 ≤
√n( ¯X −μ)
S
≤tα/2,n−1

= 1 −α
However, the inequality
−tα/2,n−1 ≤
√n( ¯X −μ)
S
can be rewritten
μ ≤¯X + tα/2,n−1S
√n
and the inequality
√n( ¯X −μ)
S
≤tα/2,n−1
can be rewritten
¯X −tα/2,n−1S
√n
≤μ
so that
P

¯X −tα/2,n−1S
√n
≤μ ≤¯X + tα/2,n−1S
√n

= 1 −α
This probability expression indicates that there is a probability of 1−α that the parameter value
μ lies within the two-sided t-interval. A subtle but important point to remember is that μ is a
ﬁxed value but that the conﬁdence interval limits are random quantities. Thus, the probability
statement should be interpreted as saying that there is a probability of 1 −α that the random
conﬁdence interval limits take values that “straddle” the ﬁxed value μ. This interpretation of
a conﬁdence interval is further clariﬁed by the simulation experiment in Section 8.1.4.
Technically speaking, √n( ¯X −μ)/S has a t-distribution only when the random variables
Xi are normally distributed. Nevertheless, as discussed at the beginning of this section, the
central limit theorem ensures that the distribution of ¯X is approximately normal for reasonably
large sample sizes, and in such cases it is sensible to construct t-intervals regardless of the
actual distribution of the data observations. Alternative nonparametric conﬁdence intervals
are discussed in Chapter 15 for situations where the sample size is small (less than 30, say)
and the data observations are evidently not normally distributed.
8.1.2
Effect of the Sample Size on Conﬁdence Intervals
The sample size n has an important effect on the conﬁdence interval length
L = 2 tα/2,n−1 s
√n
Effect of the Sample Size on the Conﬁdence Interval Length
For a ﬁxed critical point, a conﬁdence interval length L is inversely proportional to the
square root of the sample size n
L ∝
1
√n
Thus a fourfold increase in the sample size reduces the conﬁdence interval length by half.

8.1 CONFIDENCE INTERVALS
339
The critical point tα/2,n−1 also depends upon the sample size n, although this dependence
is generally minimal. Recall that as the sample size n increases, the critical point tα/2,n−1
decreases to the standard normal critical point zα/2. For example, with α = 0.05, it can be
seen from Table III that
t0.025,10 = 2.228
t0.025,20 = 2.086
t0.025,30 = 2.042
t0.025,∞= z0.025 = 1.960
Notice that this dependence of the critical point on the sample size also serves to produce
smaller conﬁdence intervals with larger sample sizes.
Some simple calculations can be made to determine what sample size n is required to
obtain a conﬁdence interval of a certain length. Speciﬁcally, if a conﬁdence interval with a
length no larger than L0 is required, then a sample size
n ≥4 ×
tα/2,n−1s
L0
2
must be used. This inequality can be used to ﬁnd a suitable sample size n if approximate
values or upper bounds are used for tα/2,n−1 and s.
For example, suppose that an experimenter wishes to construct a 95% conﬁdence interval
with a length no larger than L0 = 2.0 mm for the mean thickness of plastic sheets produced
by a particular process. Previous experience with the process enables the experimenter to be
certain that the standard deviation of the sheet thicknesses cannot be larger than 4.0 mm, and
a large enough sample size is expected so that the critical point t0.025,n−1 will be less than 2.1,
say. Consequently, the experimenter can expect that a sample size
n ≥4 ×
tα/2,n−1s
L0
2
= 4 ×
2.1 × 4.0
2.0
2
= 70.56
is sufﬁcient. A random sample of at least 71 plastic sheets should then meet the experimenter’s
requirement.
Similarcalculationscanalsobeemployedtoascertainwhatadditionalsamplingisrequired
to reduce the length of a conﬁdence interval that has been constructed from an initial sample.
In this case, the values of tα/2,n−1 and s employed in the initial conﬁdence interval can be used
as approximate values.
For example, if an initial sample of n1 observations is obtained that has a sample standard
deviation s, then a conﬁdence interval of length
L = 2 tα/2,n1−1s
√n1
can be constructed. If the experimenter decides that additional sampling is required in order
to reduce the conﬁdence interval length to L0 < L, the experimenter can expect that a total
sample size n will be sufﬁcient as long as
n ≥4 ×
tα/2,n1−1s
L0
2
The difference n−n1 is the size of the additional sample required, which can then be combined
with the initial sample of size n1.

340
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Example 17
Milk Container
Contents
With a sample of n = 50 milk containers, a conﬁdence interval for the mean container content
with conﬁdence level 99% is constructed to be
(2.0457, 2.0996)
This interval has a length of 2.0996 −2.0457 = 0.0539 liters. Suppose that the engineers
decide that they need a 99% conﬁdence interval that has a length no larger than 0.04 liters.
How much additional sampling is required?
Using the values t0.005,49 = 2.680 and s = 0.0711 employed in the initial analysis, it
appears that a total sample size
n ≥4 ×
tα/2,n−1s
L0
2
= 4 ×
2.680 × 0.0711
0.04
2
= 90.77
is required. The engineers can therefore predict that if an additional random sample of at least
91 −50 = 41 milk containers is obtained, a conﬁdence interval based upon the combination
of the two samples will have a length no larger than 0.04 liters.
Example 14
Metal Cylinder
Production
With a sample of n = 60 metal cylinders, a 99% conﬁdence interval
(49.953, 50.045)
has been obtained with a length of 50.045 −49.953 = 0.092 mm. How much additional
sampling is required to provide the increased precision of a conﬁdence interval with a length
of 0.08 mm at the same conﬁdence level?
Using t0.005,59 = 2.662 and s = 0.134, a total sample size of
n ≥4 ×
tα/2,n−1s
L0
2
= 4 ×
2.662 × 0.134
0.08
2
= 79.53
is required. Therefore, the engineers can anticipate that an additional sample of at least
80 −60 = 20 cylinders is needed to meet the speciﬁed goal.
8.1.3
Further Examples
Example 43
Rolling Mill Scrap
Recall that a random sample of n = 95 ingots that were passed through the rolling machines
provided % scrap observations with a sample mean of ¯x = 20.810 and a sample standard
deviation of s = 4.878. Since t0.05,94 = 1.6612, a conﬁdence interval for the mean % scrap
with a conﬁdence level of 90% is

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

=

20.810 −1.6612 × 4.878
√
95
, 20.810 + 1.6612 × 4.878
√
95

= (19.978, 21.641)
With a conﬁdence level of 99%, the conﬁdence interval increases in length to (19.494,
22.126).
Perhaps a good summary of these results is to say that there is a high degree of conﬁdence
that the mean value of % scrap is somewhere between about 19.5% and 22%. This is very
useful information for the rolling mill managers because it indicates the amount of scrap that
they should expect over a certain period of time. Even though the amount of scrap obtained
from each ingot varies considerably (in the sample of 95 ingots % scrap varied from about
7% to 31%), if a large number of ingots are to be rolled over a reasonably long period of time,

8.1 CONFIDENCE INTERVALS
341
then the managers can be fairly conﬁdent that the amount of scrap obtained during the period
will be about 19.5%–22% of the total weight of the ingots used.
Example 44
Army Physical Fitness
Test
The sample of n = 84 run times has a sample mean ¯x = 857.70 and a sample standard
deviation s = 81.98. Since t0.025,83 = 1.9890, a conﬁdence interval for the mean run time
with a conﬁdence level of 95% is

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

=

857.70 −1.9890 × 81.98
√
84
, 857.70 + 1.9890 × 81.98
√
84

= (839.91, 875.49)
With t0.005,83 = 2.6364, a conﬁdence interval with conﬁdence level 99% is

857.70 −2.6364 × 81.98
√
84
, 857.70 + 2.6364 × 81.98
√
84

= (834.12, 881.28)
Consequently, with 99% conﬁdence, the mean run time is found to lie between 834 and 882
seconds, which is between 13 minutes 54 seconds and 14 minutes 42 seconds. With 95%
conﬁdence this interval can be reduced to 839 and 876 seconds, that is, between 13 minutes
59 seconds and 14 minutes 36 seconds.
Example 45
Fabric Water
Absorption Properties
The sample of n = 15 fabric % pickup observations has a sample mean ¯x = 59.81 and a
sample standard deviation s = 4.94. Since t0.005,14 = 2.9769, a conﬁdence interval for the
mean % pickup value with a conﬁdence level of 99% is

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

=

59.81 −2.9769 × 4.94
√
15
, 59.81 + 2.9769 × 4.94
√
15

= (56.01, 63.61)
This analysis reveals that the mean water pickup of the cotton fabric under examination
lies between about 56% and 64%. Suppose that the textile engineers decide that they need
more precision, and that speciﬁcally they require a 99% conﬁdence interval with a length no
larger than L0 = 5%. Using the values t0.005,14 = 2.9769 and s = 4.94, the engineers can
predict that a total sample size of
n ≥4 ×
tα/2,n−1s
L0
2
= 4 ×
2.9769 × 4.94
5
2
= 34.6
will sufﬁce. Therefore, a second sample of at least 35 −15 = 20 observations is required.
8.1.4
Simulation Experiment 3: An Investigation of Conﬁdence Intervals
The simulation experiment described in Section 7.3.4 can be extended to illustrate the prob-
abilistic properties of conﬁdence intervals. Recall that when an initial sample of 30 obser-
vations was simulated, a sample mean of ¯x = 10.589 and a sample standard deviation of
s =
√
3.4622 = 1.861 were obtained. With t0.025,29 = 2.0452, these values provide a 95%
conﬁdence interval

10.589 −2.0452 × 1.861
√
30
, 10.589 + 2.0452 × 1.861
√
30

= (9.89, 11.28)

342
CHAPTER 8
INFERENCES ON A POPULATION MEAN
We know that the 30 observations were simulated with a mean of μ = 10, so we know that this
conﬁdence interval does indeed contain the true value of μ. In fact, any conﬁdence interval
calculated in this fashion using simulated observations has a probability of 0.95 of containing
the value μ = 10. Notice that the value of the mean μ = 10 is ﬁxed, and that the upper and
lower endpoints of the conﬁdence interval, in this case 9.89 and 11.28, are random variables
that depend upon the simulated data set.
Figure 8.6 shows 95% conﬁdence intervals for some of the 500 samples of simulated
data observations. For example, in simulation 1 the sample statistics are ¯x = 10.3096 and
s = 1.25211, so that the 95% conﬁdence interval is

10.3096 −2.0452 × 1.25211
√
30
, 10.3096 + 2.0452 × 1.25211
√
30

= (9.8421, 10.7772)
which again does indeed contain the true value μ = 10. However, notice that in simulation
24 the conﬁdence interval
(10.2444, 11.2548)
does not include the correct value μ = 10, as is the case with simulation 37 where the
conﬁdence interval is
(8.6925, 9.8505)
Remember that each simulation provides a 95% conﬁdence interval, which has a probability
of 0.05 of not containing the value μ = 10. Since the simulations are independent of each
other, the number of simulations out of 500 for which the conﬁdence interval does not contain
μ = 10 has a binomial distribution with n = 500 and p = 0.05. The expected number of
simulations where this happens is therefore np = 500 × 0.05 = 25.
Figure 8.7 presents a graphical illustration of some of the simulated conﬁdence intervals
showing how they generally straddle the value μ = 10, although simulations 24 and 37
are exceptions, for example. Notice that the lengths of the conﬁdence intervals vary from one
simulation to another due to changes in the value of the sample standard deviation s. Remember
that, in practice, an experimenter observes just one data set, and it has a probability of 0.95 of
providing a 95% conﬁdence interval that does indeed straddle the true value μ.
8.1.5
One-Sided Conﬁdence Intervals
One-sided conﬁdence intervals can be useful if only an upper bound or only a lower bound
on the population mean μ is of interest. Since
√n( ¯X −μ)
S
∼tn−1
the deﬁnition of the critical point tα,n−1 implies that
P

−tα,n−1 ≤
√n( ¯X −μ)
S

= 1 −α
This may be rewritten
P

μ ≤¯X + tα,n−1S
√n

= 1 −α

8.1 CONFIDENCE INTERVALS
343
FIGURE 8.6
Conﬁdence interval construction
from simulation results
Simulation
xx
s
Lower bound
Upper bound
1
10.3096
1.25211
10.7772
2
10.0380
1.85805
9.3442
10.7318
3
9.4313
1.52798
8.8608
10.0019
4
9.5237
1.99115
8.7802
10.2671
5
9.8644
1.78138
9.1993
10.5296
6
9.9980
1.75209
9.3438
10.6523
7
10.0151
1.67648
9.3891
10.6410
8
9.5657
2.02599
8.8092
10.3222
9
9.9897
1.96631
9.2555
10.7239
10
9.8570
1.73049
9.2108
10.5032
11
10.2514
2.16112
9.4444
11.0584
12
10.1104
1.80382
9.4368
10.7839
13
10.0157
1.41898
9.4859
10.5455
14
10.3813
1.38312
9.8648
10.8977
15
9.4689
1.94572
8.7432
10.1954
16
9.7135
1.62459
9.1069
10.3201
17
10.2732
1.82360
9.5923
10.9542
18
9.6372
1.74860
8.9842
10.2901
19
10.1828
1.83197
9.4987
10.8668
20
10.2726
1.45160
9.7305
10.8146
21
9.9432
1.74806
9.2905
10.5959
22
10.1797
1.84898
9.4893
10.8701
23
10.2311
1.59639
9.6351
10.8272
24
10.7496
1.35291
10.2444
11.2548
25
10.2216
1.85089
9.5305
10.9127
26
10.3936
1.80703
9.7189
11.0684
27
10.1002
1.79356
9.4305
10.7699
28
10.0762
1.53233
9.5040
10.6484
29
10.2444
1.87772
9.5433
10.9456
30
10.2307
2.22136
9.4012
11.0601
31
10.3165
1.95971
9.5848
11.0483
32
9.9352
2.15672
9.1298
10.7405
33
10.1056
1.36460
9.5961
10.6152
34
10.3469
1.53834
9.7725
10.9213
35
10.0949
1.62171
9.4893
10.7004
36
10.0132
1.81789
9.3344
10.6920
37
9.2715
1.55059
8.6925
9.8505
38
10.0483
1.40464
9.5238
10.5728
39
9.8531
2.07755
9.0773
10.6289
40
9.8680
1.18785
9.4244
10.3115
41
10.0369
1.64264
9.4236
10.6503
42
9.8179
1.91046
9.1046
10.5313
43
10.4725
1.57621
9.8839
11.0610
44
9.8642
1.68885
9.2336
10.4949
45
10.2753
2.02283
9.5199
11.0306
46
9.8033
1.95644
9.0728
10.5338
47
10.0100
1.57821
9.4207
10.5993
48
9.9486
1.75946
9.2916
10.6056
49
9.9372
1.55260
9.3574
10.5169
50
9.7926
1.52264
9.2128
10.3724
...
...
...
...
...
500
9.6115
1.59441
9.0161
10.2068
Confidence interval
does not contain
π = 10
¯
Confidence interval
9.8421

344
CHAPTER 8
INFERENCES ON A POPULATION MEAN
FIGURE 8.7
Conﬁdence intervals from
simulation experiment
Simulation 1
Simulation 2
Simulation 3
Simulation 4
Simulation 5
Simulation 6
Simulation 7
Simulation 24
Simulation 37
...
...
...
μ = 10
so that
μ ∈

−∞, ¯x + tα,n−1s
√n

is a one-sided conﬁdence interval for μ with a conﬁdence level of 1 −α. This conﬁdence
interval provides an upper bound on the population mean μ.
Similarly, the result
P
√n( ¯X −μ)
S
≤tα,n−1

= 1 −α
implies that
P

¯X −tα,n−1S
√n
≤μ

= 1 −α
so that
μ ∈

¯x −tα,n−1s
√n
, ∞

is also a one-sided conﬁdence interval for μ with a conﬁdence level of 1 −α. This conﬁdence
interval provides a lower bound on the population mean μ. These conﬁdence intervals are
known as one-sided t-intervals.

8.1 CONFIDENCE INTERVALS
345
FIGURE 8.8
Comparison of two-sided and
one-sided conﬁdence intervals
One-sided (upper bound)
Two-sided
One-sided (lower bound)
x
+ tα,n−1s
√n
¯
x + tα
,n−1s
√n
/2
¯
x¯
tα
,n−1s
√n
x −
/2
¯
tα,n−1s
√n
x¯
x −
¯
x¯
One-Sided t-Interval
One-sided conﬁdence intervals with conﬁdence levels 1 −α for a population mean μ
based on a sample of n continuous data observations with a sample mean ¯x and a
sample standard deviation s are
μ ∈

−∞, ¯x + tα,n−1s
√n

which provides an upper bound on the population mean μ, and
μ ∈

¯x −tα,n−1s
√n
, ∞

which provides a lower bound on the population mean μ. These conﬁdence intervals
are known as one-sided t-intervals.
Figure 8.8 compares the one-sided t-intervals with a two-sided t-interval. Notice that
tα,n−1 < tα/2,n−1, so that the one-sided t-intervals provide a lower or an upper bound that is
closer to ˆμ = ¯x than the limits of the two-sided t-interval.
Example 46
Hospital Worker
Radiation Exposures
Hospital workers who are routinely involved in administering radioactive tracers to patients
are subject to a radiation exposure emanating from the skin of the patient. In an experiment to
assess the amount of this exposure, radiation levels were measured at a distance of 50 cm from
n = 28 patients who had been injected with a radioactive tracer, and a sample mean ¯x = 5.145
and sample standard deviation s = 0.7524 are obtained. With a critical point t0.01,27 = 2.473,
a 99% one-sided conﬁdence interval providing an upper bound for μ is
μ ∈

−∞, ¯x + tα,n−1s
√n

=

−∞, 5.145 + 2.473 × 0.7524
√
28

= (−∞, 5.496)
Consequently, with a conﬁdence level of 0.99 the experimenter can conclude that the average
radiation level at a 50-cm distance from a patient is no more than about 5.5.
8.1.6
z-Intervals
In some circumstances an experimenter may wish to use a “known” value of the population
standard deviation σ in a conﬁdence interval in place of the sample standard deviation s. In
this case, the standard normal critical point zα/2 is used in place of tα/2,n−1 for a two-sided
conﬁdence interval.

346
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Two-Sided z-Interval
If an experimenter wishes to construct a conﬁdence interval for a population mean μ
based on a sample of size n with a sample mean ¯x and using an assumed known value
for the population standard deviation σ, then the appropriate conﬁdence interval is
μ ∈

¯x −zα/2σ
√n , ¯x + zα/2σ
√n

which is known as a two-sided z-interval or variance known conﬁdence interval.
If a one-sided conﬁdence interval using a “known” value of the population standard de-
viation σ is required, then a critical point zα should be used in place of tα,n−1.
One-Sided z-Interval
One-sided 1 −α level conﬁdence intervals for a population mean μ based on a sample
of n observations with a sample mean ¯x and using a “known” value of the population
standard deviation σ are
μ ∈

−∞, ¯x + zασ
√n

and
μ ∈

¯x −zασ
√n , ∞

These conﬁdence intervals are known as one-sided z-intervals.
When a conﬁdence interval is required for a population mean μ, as discussed in this
chapter, it is almost always the case that a t-interval should be used rather than a z-interval.
A z-interval might be appropriate if an experimenter has “prior” information from previous
experimentation on the population standard deviation σ and wishes to use this value in the
conﬁdence interval. However, in most cases, whereas an experimenter may have some idea as
to the value of the population standard deviation σ, it is proper to estimate it with the sample
standard deviation s and to construct a t-interval.
Nevertheless, for a reasonably large sample size n there is little difference between the
critical points tα,n−1 and zα, and so with s = σ there is little difference between z-intervals
and t-intervals. Consequently, it may be helpful to think of the z-intervals as large-sample
conﬁdence intervals and the t-intervals as small-sample conﬁdence intervals.
As with t-intervals, the z-intervals require that the sample mean ¯x is an observation from
a normal distribution, and for small sample sizes with data observations that are obviously not
normally distributed it is best to use the nonparametric procedures discussed in Chapter 15.
COMPUTER NOTE
Find out how to obtain conﬁdence intervals using your computer package. Software packages
usually allow you to choose whether you want a “t-interval” or a “z-interval.” The intervals
may be alternatively described as “variance unknown” or “variance known” intervals. If you
select a z-interval, you also have to specify the assumed value of σ. You should also have the
option of specifying whether you want a “two-sided” or a “one-sided” conﬁdence interval.
Finally, do not forget that you will need to specify a conﬁdence level 1 −α.

8.1 CONFIDENCE INTERVALS
347
8.1.7
Problems
8.1.1 A sample of 31 data observations has a sample
mean ¯x = 53.42 and a sample standard deviation
s = 3.05. Construct a 95% two-sided t-interval for
the population mean. (This problem is continued in
Problem 8.1.9.)
8.1.2 A random sample of 41 glass sheets is obtained and
their thicknesses are measured. The sample mean is
¯x = 3.04 mm and the sample standard deviation is
s = 0.124 mm. Construct a 99% two-sided t-interval for
the mean glass thickness. Do you think it is plausible that
the mean glass thickness is 2.90 mm? (This problem is
continued in Problems 8.1.10 and 8.6.12.)
8.1.3 The breaking strengths of a random sample of 20 bundles
of wool ﬁbers have a sample mean ¯x = 436.5 and a
sample standard deviation s = 11.90. Construct 90%,
95%, and 99% two-sided t-intervals for the average
breaking strength μ. Compare the lengths of the
conﬁdence intervals. Do you think it is plausible that
the average breaking strength is equal to 450.0?
(This problem is continued in Problems 8.1.11 and
8.6.13.)
8.1.4 A random sample of 16 one-kilogram sugar packets is
obtained and the actual weights of the packets are
measured. The sample mean is ¯x = 1.053 kg and the
sample standard deviation is s = 0.058 kg. Construct a
99% two-sided t-interval for the average sugar packet
weight. Do you think it is plausible that the average
weight is 1.025 kg? (This problem is continued in
Problem 8.6.14.)
8.1.5 A sample of 28 data observations has a sample mean
¯x = 0.0328. If an experimenter wishes to use a “known”
value σ = 0.015 for the population standard deviation,
construct an appropriate 95% two-sided conﬁdence
interval for the population mean μ.
8.1.6 The resilient moduli of 10 samples of a clay mixture are
measured and the sample mean is ¯x = 19.50. If an
experimenter wishes to use a “known” value σ = 1.0
for the standard deviation of the resilient modulus
measurements based upon prior experience, construct
appropriate 90%, 95%, and 99% two-sided conﬁdence
intervals for the average resilient modulus μ. Compare
the lengths of the conﬁdence intervals. Do you think it is
plausible that the average resilient modulus is equal to
20.0?
8.1.7 An experimenter feels that a population standard
deviation is no larger than 10.0 and would like to
construct a 95% two-sided t-interval for the population
mean that has a length at most 5.0. What sample size
would you recommend?
8.1.8 An experimenter would like to construct a 99% two-sided
t-interval, with a length at most 0.2 ohms, for the average
resistance of a segment of copper cable of a certain
length. If the experimenter feels that the standard
deviation of such resistances is no larger than 0.15 ohms,
what sample size would you recommend?
8.1.9 Consider the sample of 31 data observations discussed in
Problem 8.1.1. How many additional data observations
should be obtained to construct a 95% two-sided
t-interval for the population mean μ with a length no
larger than L0 = 2.0?
8.1.10 Consider the sample of 41 glass sheets discussed in
Problem 8.1.2. How many additional glass sheets should
be sampled to construct a 99% two-sided t-interval for
the average sheet thickness with a length no larger than
L0 = 0.05 mm?
8.1.11 Consider the sample of 20 breaking strength
measurements discussed in Problem 8.1.3. How many
additional data observations should be obtained to
construct a 99% two-sided t-interval for the average
breaking strength with a length no larger than L0 = 10.0?
8.1.12 A sample of 30 data observations has a sample mean
¯x = 14.62 and a sample standard deviation s = 2.98.
Find the value of c for which μ ∈(−∞, c) is a one-sided
95% t-interval for the population mean μ. Is it plausible
that μ ≥16?
8.1.13 A sample of 61 bottles of chemical solution is obtained
and the solution densities are measured. The sample mean
is ¯x = 0.768 and the sample standard deviation is
s = 0.0231. Find the value of c for which μ ∈(c, ∞) is a
one-sided 99% t-interval for the average solution density
μ. Is it plausible that the average solution density is less
than 0.765?
8.1.14 A sample of 19 data observations has a sample mean of
¯x = 11.80. If an experimenter wishes to use a “known”
value σ = 2.0 for the population standard deviation, ﬁnd
the value of c for which μ ∈(c, ∞) is a one-sided 95%
conﬁdence interval for the population mean μ.

348
CHAPTER 8
INFERENCES ON A POPULATION MEAN
8.1.15 A sample of 29 measurements of radiation levels
in a research laboratory taken at random times has a
sample mean of ¯x = 415.7. If an experimenter wishes to
use a “known” value σ = 10.0 for the standard deviation
of these radiation levels based upon prior experience, ﬁnd
the value of c for which μ ∈(−∞, c) is a one-sided 99%
conﬁdence interval for the mean radiation level μ. Is it
plausible that the mean radiation level is larger than 418.0?
8.1.16 The pH levels of a random sample of 16 chemical
mixtures from a process were measured, and a sample
mean ¯x = 6.861 and a sample standard deviation
s = 0.440 were obtained. The scientists presented a
conﬁdence interval (6.668, 7.054) for the average pH
level of chemical mixtures from the process. What is the
conﬁdence level of this conﬁdence interval?
8.1.17 Chilled cast iron is used for mechanical components that
need particularly high levels of hardness and durability. In
an experiment to investigate the corrosion properties of a
particular type of chilled cast iron, a collection of n = 10
samples of this chilled cast iron provided corrosion rates
with a sample mean of ¯x = 2.752 and a sample standard
deviation of s = 0.280. Construct a two-sided 99%
conﬁdence interval for the average corrosion rate for
chilled cast iron of this type. Is 3.1 a plausible value for
the average corrosion rate? (This problem is continued in
Problem 8.2.15.)
For Problems 8.1.18–8.1.22 use the summary statistics that you
calculated for the data sets to construct by hand conﬁdence
intervals for the appropriate population mean. Use your statistical
software package to obtain conﬁdence intervals and check your
answers. Show how you would describe what you have learned
from your analysis.
8.1.18 Restaurant Service Times
The data set of service times given in DS 6.1.4.
8.1.19 Telephone Switchboard Activity
The data set of calls received by a switchboard given in
DS 6.1.6.
8.1.20 Paving Slab Weights
The data set of paving slab weights given in DS 6.1.7.
8.1.21 Spray Painting Procedure
The data set of paint thicknesses given in DS 6.1.8.
8.1.22 Plastic Panel Bending Capabilities
The data set of plastic panel bending capabilities given in
DS 6.1.9.
8.1.23 The yields of nine batches of a chemical process were
measured and a sample mean of 2.843 and a sample
standard deviation of 0.150 were obtained. The
experimenter presented a conﬁdence interval of
(2.773, ∞) for the average yield of the process. What is
the conﬁdence level of this conﬁdence interval?
8.1.24 Consider the data set
34
45
27
33
38
41
45
29
30
39
34
40
28
33
36
(a) What is the sample median?
(b) Construct a 99% two-sided conﬁdence interval for
the population mean.
8.1.25 A random sample of 14 chemical solutions is obtained,
and their strengths are measured. The sample mean is
5437.2 and the sample standard deviation is 376.9.
(a) Construct a two-sided 95% conﬁdence interval for
the average strength.
(b) Estimate how many additional chemical solutions
need to be measured in order to obtain a two-sided
95% conﬁdence interval for the average strength with
a length no larger than 300.
8.1.26 A boot manufacturer is testing the quality of leather
provided by a potential supplier. The manufacturer wants
to construct a two-sided conﬁdence interval with a
conﬁdence level of 95% that has a length no larger than
0.1, and from previous experience it is believed that the
variability in the leather is such that the standard
deviation is no larger than 0.2031. What sample size
would you recommend?
8.1.27 Suppose that a two-sided t-interval for a population mean
is obtained at conﬁdence level 99% with n = 15,
¯x = 69.71, and s = 3.92.
A. The values 67 and 73 are both contained within the
conﬁdence interval.
B. The value 67 is contained within the conﬁdence
interval but 73 is not.
C. The value 73 is contained within the conﬁdence
interval but 67 is not.
D. Neither of the values 67 and 73 are contained within
the conﬁdence interval.

8.2 HYPOTHESIS TESTING
349
8.2
Hypothesis Testing
8.2.1
Hypotheses
So far statistical inferences about an unknown population mean μ have been based upon the
calculation of a point estimate and the construction of a conﬁdence interval. An additional
methodology discussed in this section is hypothesis testing, which allows an experimenter
to assess the plausibility or credibility of a speciﬁc statement or hypothesis.
For example, an experimenter may be interested in the plausibility of the statement μ = 20,
say. In other words, an experimenter may be interested in the plausibility that the population
mean is equal to a speciﬁc ﬁxed value. If this ﬁxed value is denoted by μ0, then the experi-
menter’s statement may formally be described by a null hypothesis
H0 : μ = μ0
The word hypothesis indicates that this statement will be tested with an appropriate data set.
It is useful to associate a null hypothesis with an alternative hypothesis, which is deﬁned
to be the “opposite” of the null hypothesis. The null hypothesis above has an alternative
hypothesis
HA : μ ̸= μ0
This is known as a two-sided problem since the alternative hypothesis concerns values of
μ both larger and smaller than μ0. In a one-sided problem the experimenter allows the null
hypotheses to be broader so as to indicate that the speciﬁed value μ0 provides either an upper
or a lower bound for the population mean μ.
Hypothesis Tests of a Population Mean
A null hypothesis H0 for a population mean μ is a statement that designates possible
values for the population mean. It is associated with an alternative hypothesis HA,
which is the “opposite” of the null hypothesis. A two-sided set of hypotheses is
H0 : μ = μ0
versus
HA : μ ̸= μ0
for a speciﬁed value of μ0, and a one-sided set of hypotheses is either
H0 : μ ≤μ0
versus
HA : μ > μ0
or
H0 : μ ≥μ0
versus
HA : μ < μ0
Example 47
Graphite-Epoxy
Composites
A supplier claims that its products made from a graphite-epoxy composite material have a
tensile strength of 40. An experimenter may test this claim by collecting a random sample of
products and measuring their tensile strengths. The experimenter is interested in testing the
hypothesis
H0 : μ = 40
where μ is the actual mean of the tensile strengths, against the two-sided alternative hypothesis
HA : μ ̸= 40

350
CHAPTER 8
INFERENCES ON A POPULATION MEAN
In this case, the null hypothesis states that the supplier’s claim concerning the tensile strength
is correct.
Example 14
Metal Cylinder
Production
The machine that produces metal cylinders is set to make cylinders with a diameter of 50 mm.
Is it calibrated correctly? Regardless of the machine setting there is always some variation
in the cylinders produced, so it makes sense to conclude that the machine is calibrated cor-
rectly if the mean cylinder diameter μ is equal to the set amount. Consequently, the two-sided
hypotheses of interest are
H0 : μ = 50
versus
HA : μ ̸= 50
where the null hypothesis states that the machine is calibrated correctly.
Example 48
Car Fuel Efﬁciency
A manufacturer claims that its cars achieve an average of at least 35 miles per gallon in
highway driving. A consumer interest group tests this claim by driving a random selection
of the cars in highway conditions and measuring their fuel efﬁciency. If μ denotes the true
average miles per gallon achieved by the cars, then the consumer interest group is interested
in testing the one-sided hypotheses
H0 : μ ≥35
versus
HA : μ < 35
For this experiment, the null hypothesis states that the manufacturer’s claim regarding the fuel
efﬁciency of its cars is correct.
Example 45
Fabric Water
Absorption Properties
Suppose that a fabric is unsuitable for dyeing if its water pickup is less than 55%. Is the cotton
fabric under consideration suitable for dyeing? This question can be formulated as a set of
one-sided hypotheses
H0 : μ ≤55%
versus
HA : μ > 55%
where μ is the mean water pickup of the cotton fabric. These hypotheses have been chosen
so that the null hypothesis corresponds to the fabric being unsuitable for dyeing and the
alternative hypothesis corresponds to the fabric being suitable for dyeing.
With one-sided sets of hypotheses, considerable care needs to be directed toward deciding
which should be the null hypothesis and which should be the alternative hypothesis. For
instance, in the fabric absorption example, why not take the null hypothesis to be that the cotton
fabric is suitable for dyeing? This matter is addressed below with the discussion of p-values.
8.2.2
Interpretation of p-Values
The plausibility of a null hypothesis is measured with a p-value, which is a probability that
takes a value between 0 and 1. The p-value is sometimes referred to as the observed level of
signiﬁcance. A p-value is constructed from a data set as illustrated in Figure 8.9. A useful way
of interpreting a p-value is to consider it as the plausibility or credibility of the null hypothesis.
The p-value is directly proportional to the plausibility of the null hypothesis, so that
The smaller the p-value, the less plausible is the null hypothesis.
p-Values
A data set can be used to measure the plausibility of a null hypothesis H0 through the con-
struction of a p-value. The smaller the p-value, the less plausible is the null hypothesis.

8.2 HYPOTHESIS TESTING
351
Hypotheses 
H0, HA
Data set 
x1, …, xn
p-value 
Plausibility of H0 based on
the data set x1, …, xn
FIGURE 8.9
P-value construction
p-Value
0.10
Intermediate
area
0
0.01
H0 
not plausible
1
H0 
plausible
FIGURE 8.10
P-value interpretation
Figure 8.10 shows how an experimenter can interpret different levels of a p-value. If
the p-value is very small, less than 1% say, then an experimenter can conclude that the null
hypothesis is not a plausible statement. In other words, a p-value less than 0.01 indicates to
the experimenter that the null hypothesis H0 is not a credible statement. The experimenter can
then consider the alternative hypothesis HA to be true. In such situations, the null hypothesis
is said to have been rejected in favor of the alternative hypothesis.
Rejection of the Null Hypothesis
A p-value smaller than 0.01 is generally taken to indicate that the null hypothesis H0
is not a plausible statement. The null hypothesis H0 can then be rejected in favor of
the alternative hypothesis HA.
If a p-value larger than 10% is obtained, then an experimenter should conclude that there
is no substantial evidence that the null hypothesis is not a plausible statement. In other words,
a p-value larger than 0.10 implies that there is no substantial evidence that the null hypothesis
H0 is false. The experimenter has learned that the null hypothesis is a credible statement
based upon the fact that there is no strong “inconsistency” between the data set and the null
hypothesis. In these situations, the null hypothesis is said to have been accepted.
It is important to realize that when a p-value larger than 0.10 is obtained, the experimenter
should not conclude that the null hypothesis has been proven. If a null hypothesis is accepted,
then this simply means that the null hypothesis is a plausible statement. However, there will
be many other plausible statements and consequently many other different null hypotheses
that can also be accepted. The acceptance of a null hypothesis therefore indicates that the data
set does not provide enough evidence to reject the null hypothesis, but it does not indicate that
the null hypothesis has been proven to be true.
Acceptance of the Null Hypothesis
A p-value larger than 0.10 is generally taken to indicate that the null hypothesis H0 is
a plausible statement. The null hypothesis H0 is therefore accepted. However, this
does not mean that the null hypothesis H0 has been proven to be true.

352
CHAPTER 8
INFERENCES ON A POPULATION MEAN
A p-value in the range 1%–10% is in an intermediate area. There is some evidence that the
nullhypothesisisnotplausible,buttheevidenceisnotoverwhelming.Inasensetheexperiment
is inconclusive but suggests that perhaps a further look at the problem is warranted. If it is
possible, the experimenter may wish to collect more information, that is, a larger data set, to
help clarify the matter. Sometimes a cutoff value of 0.05 is employed (see Section 8.2.4 on
signiﬁcance levels) and the null hypothesis is accepted if the p-value is larger than 0.05 and
is rejected if the p-value is smaller than 0.05.
Intermediate p-Values
A p-value in the range 1%–10% is generally taken to indicate that the data analysis is
inconclusive. There is some evidence that the null hypothesis is not plausible, but the
evidence is not overwhelming.
With a two-sided hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
rejection of the null hypothesis allows the experimenter to conclude that μ ̸= μ0. Acceptance
of the null hypothesis indicates that μ0 is a plausible value of μ, together with many other
plausible values. Acceptance of the null hypothesis does not prove that μ is equal to μ0.
With the one-sided hypothesis testing problem
H0 : μ ≤μ0
versus
HA : μ > μ0
rejection of the null hypothesis allows the experimenter to conclude that μ > μ0. Acceptance
of the null hypothesis, however, indicates that it is plausible that μ ≤μ0, but that this has not
been proven. Consequently, it is seen that the “strongest” inference is available when the null
hypothesis is rejected.
The preceding consideration is important when an experimenter decides which should be
the null hypothesis and which should be the alternative hypothesis for one-sided problems. In
order to “prove” or establish the statement μ > μ0 it is necessary to take it as the alternative
hypothesis. It can then be established by demonstrating that its opposite μ ≤μ0 is implausible.
Remember that
A null hypothesis cannot be proven to be true; it can only be shown to be implausible.
Example 47
Graphite-Epoxy
Composites
For this problem, the onus is on the experimenter to disprove the supplier’s claim that μ = 40.
That is why it is appropriate to take the null hypothesis as
H0 : μ = 40
A small p-value (less than 0.01) will demonstrate that this null hypothesis is not plausible
and consequently will establish that the supplier’s claim is not credible. If the p-value is not
small, then the experimenter must conclude that there is not enough evidence to disprove the
supplier’s claim.
Itmaybehelpfultorealizethatthesupplierisbeinggiventhebeneﬁtofthedoubtor,putting
it in legal terms, the supplier is “innocent” until proven “guilty.” In this sense, “guilt” (the
alternative hypothesis HA : μ ̸= 40) is established by showing that the supplier’s “innocence”
(the null hypothesis) is implausible. If “innocence” is plausible (a large p-value), then the null
hypothesis is accepted and the supplier is acquitted. The important point is that the acquittal
is as a result of the failure to prove guilt, and not as a result of a proof of innocence.

8.2 HYPOTHESIS TESTING
353
Example 14
Metal Cylinder
Production
For this problem, the question is whether the machine can be shown to be calibrated incorrectly.
It is therefore appropriate to take the alternative hypothesis to be
HA : μ ̸= 50
which corresponds to a miscalibration. With a small p-value, the null hypothesis H0 : μ = 50
is rejected and the machine is demonstrated to be miscalibrated. With a large p-value, the
null hypothesis is accepted and the experimenter concludes that there is no evidence that the
machine is calibrated incorrectly.
Example 48
Car Fuel Efﬁciency
The onus is on the consumer interest group to demonstrate that the car manufacturer’s claim
is incorrect. The manufacturer’s claim is incorrect if μ < 35, and so this should be taken as
the alternative hypothesis. The one-sided hypotheses that should be tested are therefore
H0 : μ ≥35
versus
HA : μ < 35
If a small p-value is obtained, the null hypothesis is rejected and the consumer interest group
has demonstrated that the manufacturer’s claim is incorrect. A large p-value indicates that
there is insufﬁcient evidence to establish that the manufacturer’s claim is incorrect.
Example 45
Fabric Water
Absorption Properties
How can the experimenter establish that the cotton fabric is suitable for dyeing? In other
words, how can the experimenter establish that μ > 55%? With this question in mind, it is
appropriate to use the one-sided hypotheses
H0 : μ ≤55%
versus
HA : μ > 55%
If a small p-value is obtained, the null hypothesis is rejected and the cotton fabric is demon-
strated to be ﬁt for dyeing. A large p-value indicates that it is plausible that the cotton fabric
is unﬁt for dyeing.
8.2.3
Calculation of p-Values
A p-value for a particular null hypothesis based on an observed data set is deﬁned in the
following way:
The p-value is the probability of obtaining this data set or worse when the null
hypothesis is true.
There are two important components of this deﬁnition:
(i) this data set or worse
and
(ii) when the null hypothesis is true.
In the ﬁrst component, worse is interpreted as meaning to have less afﬁnity with the null
hypothesis. In other words, a “worse” data set is one for which the null hypothesis is less
plausible than it is for the actual observed data set. The second component of the deﬁni-
tion indicates that the probability calculation is made under the assumption that the null

354
CHAPTER 8
INFERENCES ON A POPULATION MEAN
hypothesis is true, which in practice means calculating a probability under the assumption
that μ = μ0.
Deﬁnition of a p-Value
A p-value for a particular null hypothesis H0 based on an observed data set is deﬁned
to be “the probability of obtaining the data set or worse when the null hypothesis is
true.” A “worse” data set is one that has less afﬁnity with the null hypothesis.
Thisdeﬁnitionofa p-valueexplainstheinterpretationof p-valuesdiscussedintheprevious
section. A p-value smaller than 0.01 reveals that if the null hypothesis H0 is true, then the
chance of observing the kind of data observed (or “worse”) is less than 1 in 100. If the null
hypothesis is true, then it is unlikely that the experimenter obtains the kind of data set that
has been obtained. It is this argument that leads the experimenter to conclude that the null
hypothesis is implausible.
On the other hand, a p-value larger than 0.10 reveals that if the null hypothesis H0 is true,
then the chance of observing the kind of data observed is at least 1 in 10. In other words, if
the null hypothesis is true, then it is not at all unlikely that the experimenter obtains the kind
of data set that has been obtained. Consequently, the null hypothesis is a plausible statement
and should be accepted.
Two-Sided Problems
Consider the two-sided hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
Suppose that a data set of n observations is obtained, and the observed sample mean and
standard deviation are ¯x and s, respectively. The “discrepancy” between the data set and the
null hypothesis is measured through a t-statistic
t =
√n(¯x −μ0)
s
The discrepancy is smallest when ¯x = μ0, which gives t = 0, because this indicates that the
samplemeancoincidesexactlywiththehypothesizedvalueμ0 ofthepopulationmean.Thedis-
crepancy between the data set and the null hypothesis increases as the absolute value of the
t-statistic |t| increases, as illustrated in Figure 8.11. Consequently, a data set is considered to
be “worse” (to have less afﬁnity with the null hypothesis) than the observed data set if it has
a t-statistic with an absolute value larger than |t|, as shown in Figure 8.11.
The p-value is therefore calculated as the probability that a data set generated with μ = μ0
(that is, under the null hypothesis H0) has a t-statistic with an absolute value larger than |t|.
However, if ¯x and s are the sample mean and sample standard deviation of a data set generated
with μ = μ0, the t-statistic
√n(¯x −μ0)
s
is known to be an observation from a t-distribution with n −1 degrees of freedom. Therefore,
the p-value is
p-value = P(X ≥|t|) + P(X ≤−|t|)

8.2 HYPOTHESIS TESTING
355
FIGURE 8.11
Measuring discrepancy between a
data set and H0 for a two-sided
problem
There is more discrepancy between data set II and H0 than between data set I and H0.
|t|
|t|
Data set I
Data set II
|t|
−
0
0
0
|t|
“Worse” data sets
than data set II have values of the t-statistic in these regions.
FIGURE 8.12
P-value for two-sided t-test
0
p-value
−|t|
|t|
tn−1
= P(X ≤−|t|) + P(X ≥|t|)
versus
H0 : μ = μ0
HA : μ ̸= μ0
 distribution
where the random variable X has a t-distribution with n −1 degrees of freedom, as illustrated
in Figure 8.12. However, the symmetry of the t-distribution ensures that
P(X ≥|t|) = P(X ≤−|t|)
and so the p-value may be calculated as
p-value = 2 × P(X ≥|t|)
as illustrated in Figure 8.13. This testing procedure is known as a two-sided t-test.

356
CHAPTER 8
INFERENCES ON A POPULATION MEAN
FIGURE 8.13
P-value for two-sided t-test
0
p-value = 2 × P(X ≥|t|)
 distribution
|t|
tn−1
versus
H0 : μ = μ0
HA : μ ̸= μ0
Two-Sided t-Test
The p-value for the two-sided hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
based on a data set of n observations with a sample mean ¯x and a sample standard
deviation s, is
p-value = 2 × P(X ≥|t|)
where the random variable X has a t-distribution with n −1 degrees of freedom, and
t =
√n(¯x −μ0)
s
which is known as the t-statistic. This testing procedure is called a two-sided t-test.
Asanillustrationofthecalculationofa p-valueforatwo-sidedhypothesistestingproblem,
consider the hypotheses
H0 : μ = 10.0
versus
HA : μ ̸= 10.0
Suppose that a data set is obtained with n = 15, ¯x = 10.6, and s = 1.61. The t-statistic is
t =
√n(¯x −μ0)
s
=
√
15(10.6 −10.0)
1.61
= 1.44
Therefore any data set with a t-statistic larger than 1.44 or smaller than −1.44 is “worse” than
the observed data set. The p-value is
p-value = 2 × P(X ≥1.44)
where the random variable X has a t-distribution with n −1 = 14 degrees of freedom. A
computer package can be used to show that this value is
p-value = 2 × 0.086 = 0.172
as illustrated in Figure 8.14.

8.2 HYPOTHESIS TESTING
357
FIGURE 8.14
Two-sided p-value calculation
0
distribution
p-value = 2× P(X ≥
)
t14
|t| = 1.44
1.44 = 2×0.086 = 0.172
versus
H0 : μ = 10.0
HA : μ ̸= 10.0
0.086
FIGURE 8.15
Two-sided p-value calculation
0
distribution
p-value = 2 × P(X ≥
t14
0.0014
|t| = 3.61
3.61) = 2×0.0014= 0.0028
versus
H0 : μ = 10.0
HA : μ ̸= 10.0
This large p-value (greater than 0.10) indicates that the null hypothesis should be accepted.
There is not enough evidence to conclude that the null hypothesis is implausible. In other
words, based upon the data set observed, it is plausible that μ = 10.0. More speciﬁcally, if
μ = 10.0, there is a probability of over 17% of observing a data set with a t-statistic larger
than 1.44 or smaller than −1.44, and so this data set does not cast any doubt on the plausibility
of the null hypothesis.
Suppose instead that the data set has ¯x = 11.5. In this case the t-statistic is
t =
√
15(11.5 −10.0)
1.61
= 3.61
and the p-value is
p-value = 2 × P(X ≥3.61) = 2 × 0.0014 = 0.0028
as illustrated in Figure 8.15. Since this p-value is smaller than 0.01, the experimenter now
concludes that the null hypothesis is not a credible statement. In this case, the data set provides

358
CHAPTER 8
INFERENCES ON A POPULATION MEAN
FIGURE 8.16
P-value calculation for
graphite-epoxy composites
0
distribution
p-value = 2  P(X  
t29
|t| = 3.53
3.53) = 2 0.0007= 0.0014
≥
×
×
versus
H0 : μ = 40
HA : μ ̸= 40
0.0007
enough evidence to conclude that the population mean μ cannot be equal to 10.0, because if
the population mean were equal to 10.0, the probability of getting these data or “worse” is
only 0.0028.
Example 47
Graphite-Epoxy
Composites
When the tensile strengths of 30 randomly selected products are measured, a sample mean of
¯x = 38.518 and a sample standard deviation of s = 2.299 are obtained. Since μ0 = 40.0, the
t-statistic is
t =
√
30(38.518 −40.0)
2.299
= −3.53
Since this is a two-sided problem, the p-value is
p-value = 2 × P(X ≥| −3.53|) = 2 × P(X ≥3.53)
where X has a t-distribution with n −1 = 29 degrees of freedom. This can be shown to be
p-value = 2 × 0.0007 = 0.0014
as illustrated in Figure 8.16.
Since the p-value is so small, the null hypothesis can be rejected and there is sufﬁcient
evidence to conclude that the mean tensile strength cannot be equal to the claimed value of
40. In fact, since ¯x = 38.518 < μ0 = 40.0, it is clear that the actual mean tensile strength is
smaller than the claimed value.
Example 14
Metal Cylinder
Production
The data set of metal cylinder diameters has n = 60, ¯x = 49.99856, and s = 0.1334, so that
with μ0 = 50.0 the t-statistic is
t =
√
60(49.99856 −50.0)
0.1334
= −0.0836
Since this is a two-sided problem, the p-value is
p-value = 2 × P(X ≥0.0836)

8.2 HYPOTHESIS TESTING
359
0
p-value = 2×P(X ≥
0.467
0.0836) = 2×0.467 = 0.934
|t| = 0.0836
distribution
t59
versus
H0 : μ = 50.0
HA : μ ̸= 50.0
FIGURE 8.17
P-value calculation for metal cylinder diameters
H : μ ≤μ versus
t =
√n(x−μ )
s
“Worse” data sets
0
0
0
H : μ > μ
A
0
¯
FIGURE 8.18
Worse data sets for one-sided problems
where X has a t-distribution with n −1 = 59 degrees of freedom, which can be shown to be
p-value = 2 × 0.467 = 0.934
as illustrated in Figure 8.17. With such a large p-value the null hypothesis is accepted and the
experimenter can conclude that there is not sufﬁcient evidence to establish that the machine
that produces the metal cylinders is calibrated incorrectly.
One-Sided Problems
The calculation of p-values for one-sided hypothesis testing problems
involves deﬁning “worse” data sets in one direction rather than in two directions. For example,
with the t-statistic
t =
√n(¯x −μ0)
s
and the one-sided hypotheses
H0 : μ ≤μ0
versus
HA : μ > μ0
“worse” data sets are those that have a t-statistic greater than t, as illustrated in Figure 8.18.
This is because for this one-sided problem, the discrepancy between the data set and the null
hypothesis is measured by how much larger the sample mean ¯x is than μ0. Figure 8.19 shows
that in this case the p-value is calculated as
p-value = P(X ≥t)
where again, the random variable X has a t-distribution with n −1 degrees of freedom. This
inference method is known as a one-sided t-test.
Notice that if ¯x ≤μ0, then t ≤0 and the p-value is larger than 0.5. The null hypothesis
is therefore accepted, which is clearly the right decision since the sample mean actually takes
a value that is consistent with the null hypothesis. This situation is illustrated in Figure 8.20.
There really isn’t any point in calculating a p-value in this case because the null hypothesis

360
CHAPTER 8
INFERENCES ON A POPULATION MEAN
FIGURE 8.19
P-value calculation for a
one-sided problem
0
 distribution
p-value =
(X ≥
t
)
tn−1
t
P
versus
H0: μ ≤μ0
HA : μ > μ0
FIGURE 8.20
P-value larger than 0.50 for a
one-sided t-test
distribution
p-value
t
tn−1
p-value
= P X ≥
= P X ≥t) ≥0.50
x ≤μ ⇒t ≤0
H0 is plausible
versus
H0 : μ ≤μ0
HA : μ > μ0
0
0
¯
(
obviously cannot be shown to be an implausible statement. However, if ¯x > μ0, the calculation
of a p-value is useful because it indicates whether ¯x is close enough to μ0 for the null
hypothesis to be considered a plausible statement or whether ¯x is so far away from μ0 that
the null hypothesis is not credible.
For the one-sided hypotheses
H0 : μ ≥μ0
versus
HA : μ < μ0
worse data sets are those that have a t-statistic smaller than t, as illustrated in Figure 8.21. In
this case the p-value is calculated as
p-value = P(X ≤t)
If ¯x ≥μ0, then the null hypothesis is clearly a plausible statement and the p-value calculation
(which will result in a p-value of at least 0.5) is really not necessary.

8.2 HYPOTHESIS TESTING
361
FIGURE 8.21
P-value calculation for a one-sided
problem
 distribution
t
tn−1 
p-value = P(X ≤t)
 versus
: μ
: μ
H :
< μ
μ
HA
≥
0
0
0
“Worse” data sets
0
√
t =
(x −μ )
s
0
n ¯
One-Sided t-Test
Based upon a data set of n observations with a sample mean ¯x and a sample standard
deviation s, the p-value for the one-sided hypothesis testing problem,
H0 : μ ≤μ0
versus
HA : μ > μ0
is
p-value = P(X ≥t)
and the p-value for the one-sided hypothesis testing problem
H0 : μ ≥μ0
versus
HA : μ < μ0
is
p-value = P(X ≤t)
where the random variable X has a t-distribution with n −1 degrees of freedom, and
t =
√n(¯x −μ0)
s
These testing procedures are called one-sided t-tests.
As an illustration of p-value calculations for one-sided problems, consider the one-sided
hypotheses
H0 : μ ≤125.0
versus
HA : μ > 125.0
Suppose that a sample mean of ¯x = 122.3 is observed, as illustrated in Figure 8.22. What
is the p-value? Since the sample mean takes a value that corresponds to a population mean

362
CHAPTER 8
INFERENCES ON A POPULATION MEAN
p-value
Conclusion:  H  is plausible
x = 122.3
μ = 125.0
x < μ0 ⇒
> 0.5
 versus
: μ
: μ
H :
HA
≤125.0
> 125.0
0
0
0
¯
¯
FIGURE 8.22
P-value larger than 0.50 for a one-sided t-test
0
t19 distribution
0.90
H0 : μ ≤ 125.0 versus HA : μ > 125.0
p-value = P(X ≥ 0.90) = 0.190 
Conclusion: H0 is plausible
FIGURE 8.23
P-value calculation for a one-sided t-test
μ contained within the null hypothesis, the p-value is immediately known to be at least 0.5,
and its exact value is immaterial. The data obviously do not indicate that the null hypothesis
is implausible, and the null hypothesis should be accepted.
Suppose instead that a sample mean of ¯x = 128.4 is observed, with n = 20 and s = 16.9.
Since ¯x = 128.4 > μ0 = 125.0, the data suggest that the null hypothesis is false. How
plausible is the null hypothesis? The t-statistic is
t =
√
20(128.4 −125.0)
16.9
= 0.90
so that, as illustrated in Figure 8.23, the p-value is
p-value = P(X ≥0.90)
where the random variable X has a t-distribution with n −1 = 19 degrees of freedom. A
computer package can be used to show that this is
p-value = 0.190
so that the null hypothesis is accepted and the experimenter concludes that the data set does
not provide sufﬁcient evidence to establish that the population mean is larger than μ0 = 125.0.
However, if a sample mean of ¯x = 137.8 is observed instead, then the t-statistic is
t =
√
20(137.8 −125.0)
16.9
= 3.39
and the p-value is
p-value = P(X ≥3.39) = 0.0015
as illustrated in Figure 8.24. In this case the null hypothesis is rejected and the experimenter
has established that the population mean is larger than μ0 = 125.0.

8.2 HYPOTHESIS TESTING
363
0
3.39
t19 distribution
p-value = P(X ≥ 3.39) = 0.0015 
Conclusion: H0 is not plausible
H0 : μ ≤ 125.0 versus HA : μ > 125.0
FIGURE 8.24
P-value calculation for a one-sided t-test
0
 distribution
p-value = P(X ≤ −1.119) = 0.1386 
Conclusion:  H0 is plausible
−1.119
t19
H0 : μ ≥ 35.0 versus HA : μ < 35.0
FIGURE 8.25
P-value calculation for car fuel efﬁciency
Example 48
Car Fuel Efﬁciency
A sample of n = 20 cars driven under varying highway conditions achieved fuel efﬁciencies
with a sample mean of ¯x = 34.271 miles per gallon and a sample standard deviation of
s = 2.915 miles per gallon. With μ0 = 35.0 the t-statistic is therefore
t =
√
20(34.271 −35.0)
2.915
= −1.119
The alternative hypothesis is HA : μ < 35, so that the p-value is
p-value = P(X ≤−1.119)
where X has a t-distribution with n −1 = 19 degrees of freedom. This value can be shown
to be
p-value = 0.1386
as illustrated in Figure 8.25. This p-value is larger than 0.10 and so the null hypothesis
H0 : μ ≥35.0 should be accepted. Even though ¯x = 34.271 < μ0 = 35.0 this data set does
not provide sufﬁcient evidence for the consumer interest group to conclude that the average
miles per gallon achieved in highway driving is any less than 35.
Example 45
Fabric Water
Absorption Properties
The data set of % pickup values has n = 15, ¯x = 59.81%, and s = 4.94%. With μ0 = 55%
the t-statistic is
t =
√
15(59.81 −55.0)
4.94
= 3.77
The alternative hypothesis is HA : μ > 55%, and so the p-value is calculated as
p-value = P(X ≥3.77)
where X has a t-distribution with n −1 = 14 degrees of freedom, which can be shown to be
p-value = 0.0010

364
CHAPTER 8
INFERENCES ON A POPULATION MEAN
as illustrated in Figure 8.26. This small p-value indicates that the null hypothesis can be
rejected and that there is sufﬁcient evidence to conclude that μ > 55%. Therefore the cotton
fabric under consideration has been shown to be suitable for dyeing.
FIGURE 8.26
P-value calculation for fabric
water absorption data set
0
distribution
p-value = P(X
Conclusion:  H  is not plausible
t14
3.77
≥
) =
3.77
0.0010
 versus
:μ
: μ
H :
HA
≤55%
> 55%
0
0
Example 49
Sand Blast Paint
Removal
Sand blasting can be a convenient way for removing paint from items without damaging their
surfaces. The efﬁciency of the procedure depends on various factors such as the particle size
of the sand or medium that is used, the blasting pressure, the distance of the blaster from
the item, and the blasting angle. The data set shown in Figure 8.27 is the times in minutes
FIGURE 8.27
Illustration of the stages of a
hypothesis test for the sand blast
paint removal example
Data and Question
Data set of blast times in minutes: 10.3 9.3 11.2 8.8 9.5 9.0
Question: What evidence is there that the average blast time is less than 10 minutes?
Stage I: Data Summary
Sample average n = 6, sample mean ¯x = 9.683, sample standard deviation s = 0.906.
Stage II: Determination of Suitable Hypotheses
Since the objective is to assess whether there is sufﬁcient evidence to conclude that μ < 10,
this should be the alternative hypothesis.
H0 : μ ≥10
versus
HA : μ < 10.
Stage III: Calculation of the Test Statistic
t =
√n(¯x−μ0)
s
=
√
6(9.683−10.000)
0.906
= −0.857
Stage IV: Expression for the p-value
p-value = P(X ≤−0.857)
where the random variable X has a t-distribution with n −1 = 5 degrees of freedom.
Stage V: Evaluation of the p-value
Table III gives t0.10,5 = 1.476, and consequently it is known that the p-value is larger than 0.10.
Alternatively, exact computer calculation gives the p-value as 0.216.
Stage VI: Decision
Since the p-value is larger than 0.10, the null hypothesis is accepted.
Stage VII: Conclusion
This data set does not provide sufﬁcient evidence to establish that the average blast time
is less than 10 minutes.

8.2 HYPOTHESIS TESTING
365
p-value
0
1
α
Rejects H0
Accepts H0
Size α hypothesis test
FIGURE 8.28
Decision rules for a size α hypothesis test
No error
Type II error
Type I error
No error
H0 rejected
H0 accepted
H0 true
H0 false
FIGURE 8.29
Error classiﬁcation for hypothesis tests
taken to remove paint from a sample of items with equivalent paint thicknesses for a certain
blasting method. It was of interest to assess the evidence that the average blast time was less
than 10 minutes, and each stage in the hypothesis test is identiﬁed to clarify the process.
8.2.4
Signiﬁcance Levels
Hypothesis tests may be deﬁned formally in terms of a signiﬁcance level or size α. As
Figure 8.28 shows, a hypothesis test at size α rejects the null hypothesis H0 if a p-value smaller
than α is obtained and accepts the null hypothesis H0 if a p-value larger than α is obtained.
The signiﬁcance level α is also referred to as the probability of a Type I error. As
Figure 8.29 shows, a Type I error occurs when the null hypothesis is rejected when it is really
true. A Type II error occurs when the null hypothesis is accepted when it is really false. Having
a small probability of a Type I error, that is, having a small signiﬁcance level α, is consistent
with the “protection” of the null hypothesis discussed in Section 8.2.2. This means that the
null hypothesis is rejected only when there is sufﬁcient evidence that it is false.
It is common to use signiﬁcance levels of α = 0.10, α = 0.05, and α = 0.01, which tie
in with the p-value interpretations given in Section 8.2.2. A p-value larger than 0.10 implies
that hypothesis tests with α = 0.10, α = 0.05, and α = 0.01 all accept the null hypothesis.
Similarly, a p-value smaller than 0.01 implies that hypothesis tests with α = 0.10, α = 0.05,
and α = 0.01 all reject the null hypothesis.
A p-value in the range 0.01 to 0.10 is in the intermediate area. A hypothesis test with size
α = 0.10 rejects the null hypothesis, whereas a hypothesis test with size α = 0.01 accepts the
null hypothesis. A hypothesis test with size α = 0.05 may accept or reject the null hypothesis
depending on whether the p-value is larger or smaller than 0.05.
Signiﬁcance Level of a Hypothesis Test
A hypothesis test with a signiﬁcance level or size α rejects the null hypothesis H0 if a
p-value smaller than α is obtained and accepts the null hypothesis H0 if a p-value
larger than α is obtained. In this case, the probability of a Type I error, that is, the
probability of rejecting the null hypothesis when it is true, is no larger than α.
An important point to remember is that
p-values are more informative than knowing whether a size α hypothesis test accepts
or rejects the null hypothesis.

366
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Thisisbecauseifthe p-valueisknown,thentheoutcomeofahypothesistestatanysigniﬁcance
level α can be deduced by comparing α with the p-value. However, the acceptance or rejection
of a size α hypothesis test provides only a lower bound (p-value ≥α) or an upper bound
(p-value < α) on the p-value. Nevertheless, it will be seen that hypothesis tests at a ﬁxed
size level are easy to perform by hand because the test statistics need only be compared
with a tabulated critical point, whereas a p-value calculation requires the determination of
the cumulative distribution function of the appropriate t-distribution. Of course, computer
packages generally indicate the exact p-value.
Two-Sided Problems
For the t-statistic
t =
√n(¯x −μ0)
s
the decision as to whether a size α two-sided hypothesis test rejects or accepts can be made
by determining whether the test statistic |t| falls in the
rejection region |t| > tα/2,n−1
or in the
acceptance region |t| ≤tα/2,n−1
as illustrated in Figure 8.30.
This is because the p-value for a two-sided problem is
2 × P(X ≥|t|)
where the random variable X has a t-distribution with n −1 degrees of freedom. Since the
critical point tα/2,n−1 has the property that
P(X ≥tα/2,n−1) = α
2
it is clear that the p-value is greater than α if |t| ≤tα/2,n−1 and is smaller than α if |t| > tα/2,n−1.
In other words, comparing the test statistic |t| with the critical point tα/2,n−1 indicates whether
the p-value is smaller or greater than α.
FIGURE 8.30
Size α two-sided t-test
versus 
Size α
Rejection region
Acceptance region
Test statistic |t|
|t|
ta/2,
1
n−
≤
ta/2, n−1
|t| >
H : μ
μ0
=
0
HA : μ
μ
0
Accept H
0
Reject H
0
̸=

8.2 HYPOTHESIS TESTING
367
Two-Sided Hypothesis Test for a Population Mean
A size α test for the two-sided hypotheses
H0 : μ = μ0
versus
HA : μ ̸= μ0
rejects the null hypothesis H0 if the test statistic |t| falls in the rejection region
|t| > tα/2,n−1
and accepts the null hypothesis H0 if the test statistic |t| falls in the acceptance region
|t| ≤tα/2,n−1
As an example of a two-sided hypothesis testing problem with ﬁxed signiﬁcance levels,
suppose that a sample of n = 18 observations is obtained. Then Table III provides the critical
points
t0.05,17 = 1.740
for α = 0.10,
t0.025,17 = 2.110
for α = 0.05, and
t0.005,17 = 2.898
for α = 0.01, which are illustrated in Figure 8.31. If the test statistic is |t| = 3.24, then
hypothesis tests with α = 0.10, α = 0.05, and α = 0.01 all reject the null hypothesis, since
the test statistic is larger than the respective critical points. The actual p-value is smaller
than 0.01.
If the test statistic is |t| = 1.625, then hypothesis tests with α = 0.10, α = 0.05,
and α = 0.01 all accept the null hypothesis, because the test statistic is smaller than the
respective critical points. The actual p-value is larger than 0.10. A test statistic in the region
1.740 ≤|t| ≤2.898 falls in the intermediate region with a p-value between 0.01 and 0.10.
For example, if |t| = 2.021, then a hypothesis test with α = 0.10 rejects the null hypothesis,
whereas a hypothesis test with α = 0.05 or α = 0.01 accepts the null hypothesis. In this case
FIGURE 8.31
Hypothesis tests at ﬁxed
signiﬁcance levels
α = 10%
α = 5%
α = 1%
t0.05,17 = 1.740
t0.025,17 = 2.110
t0.005,17 = 2.898
Accept H
Accept H
Accept H
Reject
RejectH
RejectH
|t|
|t|
|t|
0
0
H
0
0
0
0
0

368
CHAPTER 8
INFERENCES ON A POPULATION MEAN
it is sensible to summarize the situation by concluding that there is some evidence that the
null hypothesis is false, but that the evidence is not overwhelming.
Example 47
Graphite-Epoxy
Composites
For this problem the test statistic is
|t| = 3.53
It can be seen from Table III that the critical points are t0.05,29 = 1.699 for a size α = 0.10
hypothesis test, t0.025,29 = 2.045 for a size α = 0.05 hypothesis test, and t0.005,29 = 2.756
for a size α = 0.01 hypothesis test. The test statistic exceeds each of these critical points,
and so the hypothesis tests all reject the null hypothesis. This indicates that the p-value is
smaller than 0.01, which is consistent with the previous analysis, which found the p-value to
be 0.0014.
Example 14
Metal Cylinder
Production
The data set of metal cylinder diameters gives a test statistic of
|t| = 0.0836
The critical points are t0.05,59 = 1.671 for a size α = 0.10 hypothesis test, t0.025,59 = 2.001
for a size α = 0.05 hypothesis test, and t0.005,59 = 2.662 for a size α = 0.01 hypothesis test.
The test statistic is smaller than each of these critical points, and so the hypothesis tests all
accept the null hypothesis. The p-value is therefore known to be larger than 0.10, and in fact
the previous analysis found the p-value to be 0.934.
Example 50
Engine Oil Viscosity
An engine oil is supposed to have a mean viscosity of μ0 = 85.0. A sample of n = 25 viscosity
measurements resulted in a sample mean of ¯x = 88.3 and a sample standard deviation of
s = 7.49. What is the evidence that the mean viscosity is not as stated?
It is appropriate to test the two-sided set of hypotheses
H0 : μ = 85.0
versus
HA : μ ̸= 85.0
With μ0 = 85.0 the t-statistic is
t =
√
25(88.3 −85.0)
7.49
= 2.203
It can be seen from Table III that the critical points are t0.05,24 = 1.711 for a size α = 0.10
hypothesis test, t0.025,24 = 2.064 for a size α = 0.05 hypothesis test, and t0.005,24 = 2.797 for
a size α = 0.01 hypothesis test. The test statistic |t| = 2.203 exceeds the ﬁrst two of these
values but not the third, so that the null hypothesis is rejected with α = 0.10 and α = 0.05
but not with α = 0.01.
This result indicates that the p-value lies somewhere between 0.01 and 0.05, and in fact
it can be calculated to be
p-value = 2 × P(X ≥2.203) = 2 × 0.0187 = 0.0374
where X has t-distribution with 24 degrees of freedom. In summary, there is some evidence
that the mean viscosity is not equal to 85.0, but the evidence is not overwhelming. The
experimenter may wish to obtain a larger sample size to clarify the matter.
There is an important connection between conﬁdence intervals and hypothesis testing
that provides additional insights into the interpretation of a conﬁdence interval. A two-sided
conﬁdence interval for μ with a conﬁdence level of 1 −α actually consists of the values μ0

8.2 HYPOTHESIS TESTING
369
FIGURE 8.32
Relationship between hypothesis
testing and conﬁdence intervals for
two-sided problems
H : μ = μ
H : μ ̸= μ
versus
+
p-value < α
p-value ≥ α
x
1 −α level two-sided
confidence interval
x −
√n
p-value < α
tα/2,n−1
x
√n
tα/2,n−1
0
0
0
0
(
)
s
s
¯
¯
¯
for which the hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
with size α accepts the null hypothesis. In other words, the value μ0 is contained within a
1−α level two-sided conﬁdence interval for μ if the p-value for this two-sided hypothesis test
is larger than α. Thus a conﬁdence interval for μ with a conﬁdence level of 1 −α consists of
“plausible”valuesforμ,whereplausibilityisdeﬁnedintermsofhavinga p-valuelargerthanα.
This relationship between two-sided conﬁdence intervals and hypothesis tests is illustrated in
Figure 8.32.
Relationship between Conﬁdence Intervals and Hypothesis Tests
The value μ0 is contained within a 1 −α level two-sided conﬁdence interval

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

if the p-value for the two-sided hypothesis test
H0 : μ = μ0
versus
HA : μ ̸= μ0
is larger than α. Therefore if μ0 is contained within the 1 −α level conﬁdence
interval, the hypothesis test with size α accepts the null hypothesis, and if μ0 is not
contained within the 1 −α level conﬁdence interval, the hypothesis test with size α
rejects the null hypothesis.
It is useful to remember that
Constructing a conﬁdence interval with a conﬁdence level of 1 −α for μ is more
informative than performing a size α hypothesis test.
This is because the decision made by a size α hypothesis test of H0 : μ = μ0 can be deduced
from a 1 −α level conﬁdence interval for μ by noticing whether or not μ0 is inside the
conﬁdence interval. Thus the conﬁdence interval portrays the decisions made by hypothesis
tests for all possible values of μ0.
It is important to have a clear understanding of the relationships between conﬁdence
intervals, p-values, and hypothesis tests at ﬁxed signiﬁcance levels. Conﬁdence intervals
and p-values for speciﬁc hypotheses of interest generally provide the most useful statistical
inferences.

370
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Example 47
Graphite-Epoxy
Composites
With t0.005,29 = 2.756, a 99% two-sided t-interval for the mean tensile stength is

¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n

=

38.518 −2.756 × 2.299
√
30
, 38.518 + 2.756 × 2.299
√
30

= (37.36, 39.67)
Notice that the value μ0 = 40.0 is not contained within this conﬁdence interval, which is
consistent with the hypothesis testing problem
H0 : μ = 40.0
versus
HA : μ ̸= 40.0
having a p-value of 0.0014, so that the null hypothesis is rejected at size α = 0.01.
In fact, the 99% conﬁdence interval implies that the hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
has a p-value larger than 0.01 for 37.36 ≤μ0 ≤39.67 and a p-value smaller than 0.01
otherwise.
Example 14
Metal Cylinder
Production
In Section 8.1.1 a 90% two-sided t-interval for the mean cylinder diameter was found to be
(49.970, 50.028)
This contains the value μ0 = 50.0 and so is consistent with the hypothesis testing problem
H0 : μ = 50.0
versus
HA : μ ̸= 50.0
having a p-value of 0.934, so that the null hypothesis is accepted at size α = 0.10.
Moreover, the 90% conﬁdence interval implies that the hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
has a p-value larger than 0.10 for 49.970 ≤μ0 ≤50.028 and a p-value smaller than 0.10
otherwise.
Example 50
Engine Oil Viscosity
With t0.025,24 = 2.064, a 95% two-sided t-interval for the mean oil viscosity is

88.3 −2.064 × 7.49
√
25
, 88.3 + 2.064 × 7.49
√
25

= (85.21, 91.39)
and with t0.005,24 = 2.797, a 99% two-sided t-interval for the mean oil viscosity is

88.3 −2.797 × 7.49
√
25
, 88.3 + 2.797 × 7.49
√
25

= (84.11, 92.49)
Notice that the value μ0 = 85.0 is contained within the 99% conﬁdence interval but is not
contained within the 95% conﬁdence interval, which is consistent with the hypothesis testing
problem
H0 : μ = 85.0
versus
HA : μ ̸= 85.0
having a p-value of 0.0374, which lies between 0.01 and 0.05.
One-Sided Problems
The relationships between conﬁdence intervals, p-values, and signif-
icance levels are the same for one-sided problems as they are for two-sided problems. For the

8.2 HYPOTHESIS TESTING
371
one-sided hypotheses
H0 : μ ≤μ0
versus
HA : μ > μ0
a size α hypothesis test rejects the null hypothesis if the test statistic
t =
√n(¯x −μ0)
s
is greater than the critical point tα,n−1 and accepts the null hypothesis if t is smaller than tα,n−1.
In other words, the rejection region is
t > tα,n−1
and the acceptance region is
t ≤tα,n−1
as illustrated in Figure 8.33.
A size α test for the one-sided hypotheses
H0 : μ ≥μ0
versus
HA : μ < μ0
has a rejection region
t < −tα,n−1
and an acceptance region
t ≥−tα,n−1
as illustrated in Figure 8.34. For both of these one-sided problems, the null hypothesis is
rejected when the p-value is smaller than α and is accepted when the p-value is larger than α.
Rejection region
Acceptance region
Test statistic t
tα,n−1
t >
t ≤tα,
1
n−
Accept H0
H
Reject
0
versus 
Size α
H : μ
μ0
≤
0
HA : μ
μ
>
0
FIGURE 8.33
Size α one-sided t-test
Rejection region
Acceptance region
Test statistic t
tα,n−1
−
t <
t ≥−tα,
1
n−
Accept H
0
Reject H
0
versus 
Size α
H : μ
μ0
≥
0
HA : μ
μ
<
0
FIGURE 8.34
Size α one-sided t-test

372
CHAPTER 8
INFERENCES ON A POPULATION MEAN
The relationship between one-sided conﬁdence intervals and one-sided hypothesis testing
problems is as follows. The 1 −α level one-sided conﬁdence interval
μ ∈

−∞, ¯x + tα,n−1s
√n

consists of the values μ0 for which the hypothesis testing problem
H0 : μ ≥μ0
versus
HA : μ < μ0
has a p-value larger than α, as illustrated in Figure 8.35. Similarly, the 1 −α level one-sided
conﬁdence interval
μ ∈

¯x −tα,n−1s
√n
, ∞

consists of the values μ0 for which the hypothesis testing problem
H0 : μ ≤μ0
versus
HA : μ > μ0
has a p-value larger than α, as illustrated in Figure 8.36. Figure 8.37 summarizes the relation-
ships between conﬁdence intervals, p-values, and signiﬁcance levels for two-sided problems
and one-sided problems.
x + tα,n−1s
√n
p-value < α
x
1 − α level one-sided
confidence interval
¯
¯
)
H : μ ≥μ0
HA: μ < μ
versus
0
0
p-value ≥ α
FIGURE 8.35
Relationship between hypothesis testing and conﬁdence
intervals for one-sided problems
p-value ≥ α
p-value < α
x
1 − α level one-sided
confidence interval
H : μ ≤μ0
HA: μ > μ
versus
0
0
(
x −tα,n−1s
√n
¯
¯
FIGURE 8.36
Relationship between hypothesis testing and conﬁdence intervals
for one-sided problems
FIGURE 8.37
Relationship between hypothesis
testing and conﬁdence intervals
for two-sided and one-sided
problems
Hypothesis tests H0: μ = μ0 versus HA: μ ̸= μ0 and conﬁdence intervals 
¯x −
tα/2,n−1s
√n
, ¯x +
tα/2,n−1s
√n

Hypothesis tests H0: μ ≥μ0 versus HA: μ < μ0 and conﬁdence intervals 
−∞, ¯x +
tα,n−1s
√n

Hypothesis tests H0: μ ≤μ0 versus HA: μ > μ0 and conﬁdence intervals 
¯x −
tα,n−1s
√n
, ∞
Signiﬁcance Levels
Conﬁdence Levels
p-Value
α = 0.10
α = 0.05
α = 0.01
1 −α = 0.90
1 −α = 0.95
1 −α = 0.99
≥0.10
accept
accept
accept
contains
contains
contains
H0
H0
H0
μ0
μ0
μ0
0.05–0.10
reject
accept
accept
does not contain
contains
contains
H0
H0
H0
μ0
μ0
μ0
0.01–0.05
reject
reject
accept
does not contain
does not contain
contains
H0
H0
H0
μ0
μ0
μ0
< 0.01
reject
reject
reject
does not contain
does not contain
does not contain
H0
H0
H0
μ0
μ0
μ0

8.2 HYPOTHESIS TESTING
373
One-Sided Inferences on a Population Mean (H0: μ ≤μ0)
A size α test for the one-sided hypotheses
H0 : μ ≤μ0
versus
HA : μ > μ0
rejects the null hypothesis when
t > tα,n−1
and accepts the null hypothesis when
t ≤tα,n−1
The 1 −α level one-sided conﬁdence interval
μ ∈

¯x −tα,n−1s
√n
, ∞

consists of the values μ0 for which this hypothesis testing problem has a p-value
larger than α, that is, the values μ0 for which the size α hypothesis test accepts the null
hypothesis.
One-Sided Inferences on a Population Mean (H0: μ ≥μ0)
A size α test for the one-sided hypotheses
H0 : μ ≥μ0
versus
HA : μ < μ0
rejects the null hypothesis when
t < −tα,n−1
and accepts the null hypothesis when
t ≥−tα,n−1
The 1 −α level one-sided conﬁdence interval
μ ∈

−∞, ¯x + tα,n−1s
√n

consists of the values μ0 for which this hypothesis testing problem has a p-value
larger than α, that is, the values μ0 for which the size α hypothesis test accepts the null
hypothesis.
Example 48
Car Fuel Efﬁciency
The one-sided hypotheses of interest here are
H0 : μ ≥35.0
versus
HA : μ < 35.0
and since the test statistic t = −1.119 is larger than the critical point −t0.10,19 = −1.328,
a size α = 0.10 hypothesis test accepts the null hypothesis. This conclusion is consistent
with the previous analysis where the p-value was found to be 0.1386, which is larger than
α = 0.10.

374
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Furthermore, the one-sided 90% t-interval
μ ∈

−∞, ¯x + tα,n−1s
√n

=

−∞, 34.271 + 1.328 × 2.915
√
20

= (−∞, 35.14)
contains the value μ0 = 35.0, as expected. In fact, this conﬁdence interval indicates that the
hypothesis testing problem
H0 : μ ≥μ0
versus
HA : μ < μ0
has a p-value larger than 0.10 for any value of μ0 ≤35.14.
Example 45
Fabric Water
Absorption Properties
For the hypotheses
H0 : μ ≤55%
versus
HA : μ > 55%
the t-statistic t = 3.77 is larger than the critical point t0.01,14 = 2.624, so the null hypothesis
is rejected at size α = 0.01. This conclusion is consistent with the previous analysis where
the p-value was shown to be 0.0010, which is smaller than α = 0.01.
A one-sided 99% t-interval for the mean water pickup μ is
μ ∈

¯x −tα,n−1s
√n
, ∞

=

59.81 −2.624 × 4.94
√
15
, ∞

= (56.46, ∞)
which, as expected, does not contain the value μ0 = 55.0. Furthermore, this conﬁdence
interval indicates that the hypothesis testing problem
H0 : μ ≤μ0
versus
HA : μ > μ0
has a p-value smaller than 0.01 for any value of μ0 ≤56.46.
Example 46
Hospital Worker
Radiation Exposures
A 99% one-sided t-interval for the mean radiation level μ was found to be
(−∞, 5.496)
This implies that the one-sided hypothesis testing problem
H0 : μ ≥μ0
versus
HA : μ < μ0
has a p-value smaller than 0.01 for μ0 > 5.496 and a p-value larger than 0.01 for μ0 ≤5.496.
Power Levels
The signiﬁcance level α of a hypothesis test designates the probability of a
Type I error, that is, the probability that the null hypothesis is rejected when it is true (see
Figure 8.29). Small signiﬁcance levels are employed in hypothesis tests so that this probability
is small. However, it is also useful to consider the probability of a Type II error, which is the
probability that the null hypothesis is accepted when the alternative hypothesis is true.
Power of a Hypothesis Test
The power of a hypothesis test is deﬁned to be
power = 1 −(probability of Type II error)
which is the probability that the null hypothesis is rejected when it is false.

8.2 HYPOTHESIS TESTING
375
FIGURE 8.38
The speciﬁcation of two quantities
determines the third quantity
size 
Sample
n
Significance level α
Power of hypothesis test
Hypothesis Testing
Confidence level 1 − α
Length of confidence interval
size 
Sample
n
Confidence Intervals
Obviously, large values of power are good. Larger power levels and shorter conﬁdence
intervals are both indications of an increase in the “precision” of an experiment. However, as
shown in Figure 8.38, an experimenter can choose only two quantities out of
■
the sample size n, the signiﬁcance level α, and the power of a hypothesis test
or similarly only two out of
■
the sample size n, the conﬁdence level 1 −α, and the length of a conﬁdence interval
Once two of these quantities have been speciﬁed, the third is automatically determined.
Usually, the sample size n obtained in the experiment and the choice of a signiﬁcance level
α determine the power of the hypothesis test, just as the sample size and a conﬁdence level
determine a conﬁdence interval length. Sometimes, an experimenter may investigate what
sample size n is required to achieve a speciﬁed signiﬁcance level and power level. However, it
is generally more convenient to base sample size determination on conﬁdence interval lengths,
as described in Section 8.1.2. It is important to realize that for a ﬁxed signiﬁcance level α, the
power of a hypothesis test increases as the sample size n increases.
Relationship between Power and Sample Size
For a ﬁxed signiﬁcance level α, the power of a hypothesis test increases as the sample
size n increases.
8.2.5
z-Tests
Testing procedures similar to the t-tests can be employed when an experimenter wishes to
use an assumed “known” value of the population standard deviation σ rather than the sample
standard deviation s. These “variance known” tests are called z-tests and are based upon the
z-statistic
z =
√n(¯x −μ0)
σ
which has a standard normal distribution when μ = μ0.

376
CHAPTER 8
INFERENCES ON A POPULATION MEAN
For a two-sided hypothesis testing problem, the p-value is calculated as
p-value = 2 × P(X ≥|z|)
where the random variable X has a standard normal distribution. With the standard normal
cumulative distribution function (x), this value can be written
p-value = 2 × (1 −(|z|)) = 2 × (−|z|)
For one-sided hypothesis testing problems the p-value is either
p-value = P(X ≥z) = 1 −(z)
or
p-value = P(X ≤z) = (z)
If a ﬁxed signiﬁcance level α is employed, then a critical point zα/2 is appropriate for a
two-sided hypothesis testing problem and a critical point zα is appropriate for a one-sided
hypothesis testing problem.
Two-Sided z-Test
The p-value for the two-sided hypothesis testing problem
H0 : μ = μ0
versus
HA : μ ̸= μ0
based upon a data set of n observations with a sample mean ¯x and an assumed
“known” population standard deviation σ, is
p-value = 2 × (−|z|)
where (x) is the standard normal cumulative distribution function and
z =
√n(¯x −μ0)
σ
which is known as the z-statistic. This testing procedure is called a two-sided z-test.
A size α test rejects the null hypothesis H0 if the test statistic |z| falls in the
rejection region
|z| > zα/2
and accepts the null hypothesis H0 if the test statistic |z| falls in the acceptance region
|z| ≤zα/2
The 1 −α level two-sided conﬁdence interval
μ ∈

¯x −zα/2σ
√n , ¯x + zα/2σ
√n

consists of the values μ0 for which this hypothesis testing problem has a p-value
larger than α, that is, the values μ0 for which the size α hypothesis test accepts the null
hypothesis.

8.2 HYPOTHESIS TESTING
377
One-Sided z-Test (H0: μ ≤μ0)
The p-value for the one-sided hypothesis testing problem
H0 : μ ≤μ0
versus
HA : μ > μ0
based upon a data set of n observations with a sample mean ¯x and an assumed known
population standard deviation σ, is
p-value = 1 −(z)
This testing procedure is called a one-sided z-test.
A size α test rejects the null hypothesis when
z > zα
and accepts the null hypothesis when
z ≤zα
The 1 −α level one-sided conﬁdence interval
μ ∈

¯x −zασ
√n , ∞

consists of the values μ0 for which this hypothesis testing problem has a p-value
larger than α, that is, the values μ0 for which the size α hypothesis test accepts the null
hypothesis.
One-Sided z-Test (H0: μ ≥μ0)
The p-value for the one-sided hypothesis testing problem
H0 : μ ≥μ0
versus
HA : μ < μ0
based upon a data set of n observations with a sample mean ¯x and an assumed known
population standard deviation σ, is
p-value = (z)
This testing procedure is called a one-sided z-test.
A size α test rejects the null hypothesis when
z < −zα
and accepts the null hypothesis when
z ≥−zα
The 1 −α level one-sided conﬁdence interval
μ ∈

−∞, ¯x + zασ
√n

consists of the values μ0 for which this hypothesis testing problem has a p-value
larger than α, that is, the values μ0 for which the size α hypothesis test accepts the null
hypothesis.

378
CHAPTER 8
INFERENCES ON A POPULATION MEAN
COMPUTER NOTE
Find out how to do t-tests (variance unknown) and z-tests (variance known) with your software
package. You will need to specify the value of μ0 and also whether you want an alternative
hypothesis of HA : μ < μ0, HA : μ ̸= μ0, or HA : μ > μ0. If you wish to use a z-test, you
will also need to specify the value of the “known” population standard deviation σ. Usually,
the computer will provide you with an exact p-value and you will not need to specify a
signiﬁcance level α.
8.2.6
Problems
8.2.1 A sample of n = 18 observations has a sample mean of
¯x = 57.74 and a sample standard deviation of s = 11.20.
Consider the hypothesis testing problems:
(a) H0 : μ = 55.0 versus HA : μ ̸= 55.0
(b) H0 : μ ≥65.0 versus HA : μ < 65.0
In each case, write down an expression for the p-value.
What do the critical points in Table III tell you about the
p-values? Use a computer package to evaluate the
p-values exactly.
8.2.2 A sample of n = 39 observations has a sample mean of
¯x = 5532 and a sample standard deviation of s = 287.8.
Consider the hypothesis testing problems:
(a) H0 : μ = 5680 versus HA : μ ̸= 5680
(b) H0 : μ ≤5450 versus HA : μ > 5450
In each case, write down an expression for the p-value.
What do the critical points in Table III tell you about
the p-values? Use a computer package to evaluate the
p-values exactly.
8.2.3 A sample of n = 13 observations has a sample mean of
¯x = 2.879. If an assumed known standard deviation of
σ = 0.325 is used, calculate the p-values for the
hypothesis testing problems:
(a) H0 : μ = 3.0 versus HA : μ ̸= 3.0
(b) H0 : μ ≥3.1 versus HA : μ < 3.1
8.2.4 A sample of n = 44 observations has a sample mean of
¯x = 87.90. If an assumed known standard deviation of
σ = 5.90 is used, calculate the p-values for the
hypothesis testing problems:
(a) H0 : μ = 90.0 versus HA : μ ̸= 90.0
(b) H0 : μ ≤86.0 versus HA : μ > 86.0
8.2.5 An experimenter is interested in the hypothesis testing
problem
H0 : μ = 3.0 mm
versus
HA : μ ̸= 3.0 mm
where μ is the average thickness of a set of glass sheets.
Suppose that a sample of n = 41 glass sheets is obtained
and their thicknesses are measured.
(a) For what values of the t-statistic does the
experimenter accept the null hypothesis with a size
α = 0.10?
(b) For what values of the t-statistic does the
experimenter reject the null hypothesis with a size
α = 0.01?
Suppose that the sample mean is ¯x = 3.04 mm and the
sample standard deviation is s = 0.124 mm.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Write down an expression for the p-value and
evaluate it using a computer package.
8.2.6 An experimenter is interested in the hypothesis testing
problem
H0 : μ = 430.0
versus
HA : μ ̸= 430.0
where μ is the average breaking strength of a bundle of
wool ﬁbers. Suppose that a sample of n = 20 wool ﬁber
bundles is obtained and their breaking strengths are
measured.
(a) For what values of the t-statistic does the experimenter
accept the null hypothesis with a size α = 0.10?
(b) For what values of the t-statistic does the experimenter
reject the null hypothesis with a size α = 0.01?
Suppose that the sample mean is ¯x = 436.5 and the
sample standard deviation is s = 11.90.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Write down an expression for the p-value and
evaluate it using a computer package.
8.2.7 An experimenter is interested in the hypothesis testing
problem
H0 : μ = 1.025 kg
versus
HA : μ ̸= 1.025 kg
where μ is the average weight of a 1-kilogram sugar
packet. Suppose that a sample of n = 16 sugar packets is
obtained and their weights are measured.

8.2 HYPOTHESIS TESTING
379
(a) For what values of the t-statistic does the experimenter
accept the null hypothesis with a size α = 0.10?
(b) For what values of the t-statistic does the experimenter
reject the null hypothesis with a size α = 0.01?
Suppose that the sample mean is ¯x = 1.053 kg and the
sample standard deviation is s = 0.058 kg.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Write down an expression for the p-value and
evaluate it using a computer package.
8.2.8 An experimenter is interested in the hypothesis testing
problem
H0 : μ = 20.0
versus
HA : μ ̸= 20.0
where μ is the average resilient modulus of a clay
mixture. Suppose that a sample of n = 10 resilient
modulus measurements is obtained and that the
experimenter wishes to use a value of σ = 1.0 for the
resilient modulus standard deviation.
(a) For what values of the z-statistic does the experimenter
accept the null hypothesis with a size α = 0.10?
(b) For what values of the z-statistic does the experimenter
reject the null hypothesis with a size α = 0.01?
Suppose that the sample mean is ¯x = 19.50.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Calculate the exact p-value.
8.2.9 An experimenter is interested in the hypothesis testing
problem
H0 : μ ≤0.065
versus
HA : μ > 0.065
where μ is the average density of a chemical solution.
Suppose that a sample of n = 61 bottles of the chemical
solution is obtained and their densities are measured.
(a) For what values of the t-statistic does the experimenter
accept the null hypothesis with a size α = 0.10?
(b) For what values of the t-statistic does the experimenter
reject the null hypothesis with a size α = 0.01?
Suppose that the sample mean is ¯x = 0.0768 and the
sample standard deviation is s = 0.0231.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Write down an expression for the p-value and
evaluate it using a computer package.
8.2.10 An experimenter is interested in the hypothesis testing
problem
H0 : μ ≥420.0
versus
HA : μ < 420.0
where μ is the average radiation level in a research
laboratory. Suppose that a sample of n = 29 radiation
level measurements is obtained and that the experimenter
wishes to use a value of σ = 10.0 for the standard
deviation of the radiation levels.
(a) For what values of the z-statistic does the experimenter
accept the null hypothesis with a size α = 0.10?
(b) For what values of the z-statistic does the experimenter
reject the null hypothesis with a size α = 0.01?
Suppose that the sample mean is ¯x = 415.7.
(a) Is the null hypothesis accepted or rejected with
α = 0.10? With α = 0.01?
(b) Calculate the exact p-value.
8.2.11 A machine is set to cut metal plates to a length of 44.350
mm. The lengths of a random sample of 24 metal plates
have a sample mean of ¯x = 44.364 mm and a sample
standard deviation of s = 0.019 mm. Is there any
evidence that the machine is miscalibrated?
8.2.12 A food manufacturer claims that at the time of purchase
by a consumer the average age of its product is no more
than 120 days. In an experiment to test this claim a
random sample of 36 items are found to have ages at the
time of purchase with a sample mean of ¯x = 122.5 days
and a sample standard deviation of s = 13.4 days.
With this information how do you feel about the
manufacturer’s claim?
8.2.13 A chemical plant is required to maintain ambient sulfur
levels in the working environment atmosphere at an
average level of no more than 12.50. The results of 15
randomly timed measurements of the sulfur level
produced a sample mean of ¯x = 14.82 and a sample
standard deviation of s = 2.91. What is the evidence that
the chemical plant is in violation of the working code?
8.2.14 A company advertises that its electric motors provide an
efﬁciency that is at least 25% higher than the industry
norm. A consumer interest group ran an experiment with
a sample of 23 machines for which the increases in
efﬁciency over the industry norm had a sample mean
of ¯x = 22.8% and a sample standard deviation of
s = 8.72%. What evidence does the consumer interest
group have that the advertised claim is false?
8.2.15 Recall Problem 8.1.17 where a collection of n = 10
samples of chilled cast iron provided corrosion rates with
a sample mean of ¯x = 2.752 and a sample standard
deviation of s = 0.280. Is there sufﬁcient evidence to
conclude that the average corrosion rate of chilled cast
iron of this type is larger than 2.5?

380
CHAPTER 8
INFERENCES ON A POPULATION MEAN
8.2.16 Restaurant Service Times
Consider the data set of service times given in DS 6.1.4.
The manager of the fast-food restaurant claims that at the
time the survey was conducted, the average service time
was less than 65 seconds. What is the evidence that this
claim is false?
8.2.17 Telephone Switchboard Activity
Consider the data set of calls received by a switchboard
given in DS 6.1.6. A manager claims that the switchboard
needs additional stafﬁng because the average number of
calls taken per minute is at least 13. How do you feel
about this claim?
8.2.18 Paving Slab Weights
Consider the data set of paving slab weights given in
DS 6.1.7. The slabs are supposed to have an average
weight of 1.1 kg. Is there any evidence that the
manufacturing process needs adjusting?
8.2.19 Spray Painting Procedure
Consider the data set of paint thicknesses given in DS
6.1.8. The spray painting machine is supposed to spray
paint to a mean thickness of 0.225 mm. What is the
evidence that the spray painting machine is not
performing properly?
8.2.20 Plastic Panel Bending Capabilities
Consider the data set of plastic panel bending capabilities
given in DS 6.1.9. The plastic panels are designed to be
able to bend on average to at least 9.5◦without
deforming. Is there any evidence that this design criterion
has not been met?
8.2.21 An experimenter randomly selects n = 16 batteries from
a production line and measures their voltages. An average
¯x = 239.13 is obtained, with a sample standard deviation
s = 2.80. Does this experiment provide sufﬁcient evidence
for the experimenter to conclude that the average voltage
of the batteries from the production line is at least 238.5?
8.2.22 A two-sided t-procedure is performed. Use Table III to
put bounds on the p-value if:
(a) n = 12, t = 3.21
(b) n = 24, t = 1.96
(c) n = 30, t = 3.88
8.2.23 A company claims that its components have an average
length of 82.50 mm. An experimenter tested this claim
by measuring the lengths of a random sample of
25 components. It was found that ¯x = 82.40 and
s = 0.14. Use a hypothesis test to assess whether the
experimenter has sufﬁcient evidence to conclude that the
average length of the components is different from 82.50.
8.2.24 A random sample of 25 components is obtained, and their
weights are measured. The sample mean is 71.97 g and
the sample standard deviation is 7.44 g. Conduct a
hypothesis test to assess whether there is sufﬁcient
evidence to establish that the components have an average
weight larger than 70 g.
8.2.25 A random sample of 28 plastic items is obtained, and
their breaking strengths are measured. The sample mean
is 7.442 and the sample standard deviation is 0.672.
Conduct a hypothesis test to assess whether there is any
evidence that the average breaking strength is not 7.000.
8.2.26 An experimenter measures the failure times of a random
sample of 25 components. The sample average is 53.43
hours and the sample standard deviation is 3.93 hours.
Use a hypothesis test to determine whether there is
sufﬁcient evidence for the experimenter to conclude that
the average failure time of the components is at least
50 hours.
8.2.27 An experimenter is planning an experiment to assess
whether it can be established that an unknown failure rate
μ is smaller than 25. Write down the null hypothesis and
the alternative hypothesis that the experimenter should
use for the analysis.
8.2.28 Use Table III to indicate whether the p-values for the
following t-tests are less than 1%, between 1% and 10%,
or more than 10%.
(a) H0 : μ = 10, HA : μ ̸= 10, n = 20, ¯x = 12.49,
s = 1.32
(b) H0 : μ ≤3.2, HA : μ > 3.2, n = 43, ¯x = 3.03,
s = 0.11
(c) H0 : μ ≥85, HA : μ < 85, n = 16, ¯x = 73.43,
s = 16.44
8.2.29 Toxicity of Salmon Fillets
An experiment is conducted to investigate the time taken
for salmon ﬁllets to become toxic under certain storage
conditions. Eight samples are prepared, and the times
to toxicity in days are given in DS 8.2.1.
(a) Does this experiment provide sufﬁcient evidence to
conclude that the average time to toxicity of salmon
ﬁllets under these storage conditions is more than
11 days?
(b) Construct a two-sided 99% conﬁdence interval for the
average time to toxicity of salmon ﬁllets under these
storage conditions.

8.3 SUMMARY
381
8.2.30 In testing a hypothesis, if the p-value is less than 1%,
your decision should be to:
A. Reject the null hypothesis.
B. Accept the null hypothesis.
C. Do the analysis again.
D. Give your statistics professor a round of applause.
8.2.31 If your computer reports a p-value of 1.205, then:
A. The null hypothesis should be accepted.
B. The data has almost certainly been faked.
C. The null hypothesis should be rejected.
D. The computer software package has certainly made a
mistake.
8.2.32 In hypothesis testing:
A. The null hypothesis is given the beneﬁt of the doubt
and it can sometimes be proven to be true.
B. The null hypothesis is given the beneﬁt of the doubt
and it cannot be proven to be true.
C. The alternative hypothesis is given the beneﬁt of the
doubt and it can sometimes be proven to be true.
D. The alternative hypothesis is given the beneﬁt of the
doubt and it cannot be proven to be true.
8.2.33 For a one-sample problem, suppose that n = 20,
¯x = 315.9, and s = 22.9. The p-value for the hypotheses
H0 : μ ≥320 versus HA : μ < 320 is:
A. P(t19 ≤−0.80)
B. 2 × P(t19 ≤0.80)
C. 2 × P(t19 ≤−0.80)
D. 2 × P(t19 ≥0.80)
E. P(t19 ≥−0.80)
8.2.34 For a two-sided t-test, which of the following t-statistics
would result in the largest p-value?
A. 2.55
B. −2.55
C. 1.43
D. −1.33
E. 3.22
8.2.35 For a one-sided t-test with HA : μ > 10, which of the
following t-statistics would result in the largest p-value?
A. 2.55
B. −2.55
C. 1.43
D. −1.33
E. 3.22
8.2.36 Mercury Levels in Coal
DS 6.7.19 shows the mercury levels of coal samples that
are taken periodically as the coal is mined further and
further into the seam. Use the statistical methodologies
described in this chapter to analyze this data set.
8.2.37 Natural Gas Consumption
DS 6.7.20 contains data on the total daily natural gas
consumption for a region during the summer. Use the
statistical methodologies described in this chapter to
analyze this data set.
8.3
Summary
Figure 8.39 shows a summary of the process an experimenter goes through in order to make
suitable inferences on a population mean μ. The process consists of two questions followed
by a choice of inference methods.
Question I relates to whether a t-procedure or a z-procedure is appropriate. It is almost
always the case that the experimenter can employ a t-procedure. However, a z-procedure
is appropriate if the experimenter wishes to use an assumed known value for the population
standard deviation σ, presumably obtained from prior experience. With s = σ, the t-procedure
and z-procedure are identical for large sample sizes.
FIGURE 8.39
Decision process for inferences on
a population mean
Question I: t-procedure or z-procedure?
Confidence interval
Hypothesis testing
Question II: two-sided or one-sided?
Inference
methods

382
CHAPTER 8
INFERENCES ON A POPULATION MEAN
Remember that for sample sizes smaller than about 30, the test procedures require that
the distribution of the sample observations should be approximately normally distributed. For
sample sizes larger than about 30, the test procedures are appropriate regardless of the actual
distribution of the sample observations because of the central limit theorem.
If the sample size is small, the t-procedure generally provides fairly sensible results
unless the data observations are clearly not normally distributed. The nonparametric inference
methods discussed in Chapter 15 offer an alternative approach in these situations.
Question II relates to whether a two-sided or a one-sided procedure is appropriate. Gen-
erally, a two-sided inference method is appropriate, and if in doubt it is always appropriate to
employ a two-sided inference method. However, in certain situations where the experimenter
is interested in obtaining only upper or lower bounds on the population mean, a one-sided
approach provides a more efﬁcient analysis.
Figures 8.40 and 8.41 summarize how t-procedures and z-procedures are employed. A
conﬁdence interval for the population mean is usually the best way to summarize the results
of an experiment. It provides a range of plausible values for the population mean, and most
people ﬁnd it easy to interpret. Nevertheless, if there is a value of the population mean μ0 that
is of particular interest to the experimenter, then it can also be useful to calculate a p-value to
assess how plausible that particular value is.
Hypothesis testing at a ﬁxed signiﬁcance level α, so that the result is reported as either
acceptance or rejection of the null hypothesis, can be employed, but it is not as informative
an inference method as conﬁdence interval construction and p-value calculation. Remember
that the result of a size α hypothesis test can be inferred either from a 1 −α level conﬁdence
interval or from the exact p-value of a hypothesis testing problem.
A ﬁnal matter is how an experimenter may determine an appropriate sample size n when
this option is available. Obviously, larger sample sizes allow a more precise statistical analysis
but also incur a greater cost. The most convenient way to assess the precision afforded by a
certain sample size is to estimate the length of the resulting two-sided conﬁdence interval for
the population mean.
FIGURE 8.40
Summary of the t-procedure
t-procedure
(sample size n ≥30 or a small sample size with normally distributed data; variance unknown)
One-sided
Two-sided
One-sided
1 −α level conﬁdence intervals

−∞, ¯x + tα,n−1s
√n


¯x −tα/2,n−1s
√n
, ¯x + tα/2,n−1s
√n


¯x −tα,n−1s
√n
, ∞

Hypothesis testing: test statistic t =
√n ( ¯x−μ0)
s
; X ∼tn−1
H0 : μ ≥μ0, HA : μ < μ0
H0 : μ = μ0, HA : μ ̸= μ0
H0 : μ ≤μ0, HA : μ > μ0
p-value = P(X ≤t)
p-value = 2 × P(X ≥|t|)
p-value = P(X ≥t)
Size α hypothesis tests
accept H0
reject H0
accept H0
reject H0
accept H0
reject H0
t ≥−tα,n−1
t < −tα,n−1
|t| ≤tα/2,n−1
|t| > tα/2,n−1
t ≤tα,n−1
t > tα,n−1

8.4 CASE STUDY: MICROELECTRONIC SOLDER JOINTS
383
FIGURE 8.41
Summary of the z-procedure
z-procedure
(sample size n ≥30 or a small sample size with normally distributed data; variance known)
One-sided
Two-sided
One-sided
1 −α level conﬁdence intervals

−∞, ¯x + zασ
√n


¯x −zα/2σ
√n , ¯x + zα/2σ
√n


¯x −zασ
√n , ∞

Hypothesis testing: test statistic z =
√n( ¯x−μ0)
σ
H0 : μ ≥μ0, HA : μ < μ0
H0 : μ = μ0, HA : μ ̸= μ0
H0 : μ ≤μ0, HA : μ > μ0
p-value = (z)
p-value = 2 × (−|z|)
p-value = 1 −(z)
Size α hypothesis tests
accept H0
reject H0
accept H0
reject H0
accept H0
reject H0
z ≥−zα
z < −zα
|z| ≤zα/2
|z| > zα/2
z ≤zα
z > zα
8.4
Case Study: Microelectronic Solder Joints
The new method that the researcher is investigating is supposed to deposit a nickel layer with
an average thickness of 2.775 microns on the substrate bond pad. The researcher’s data set
provided a sample average of ¯x = 2.7688 microns, which suggests that the method may
not be depositing enough nickel. But is the difference between the sample average and the
target value statistically signiﬁcant? Figure 8.42 shows how a two-sided hypothesis test can
be employed to show that, in fact, the difference is not statistically signiﬁcant.
FIGURE 8.42
Two-sided hypothesis test of
whether the average nickel layer
thickness is 2.775 microns
Data and Question
Data set of nickel layer thicknesses in microns (given in Figure 6.40).
Question: What evidence is there that the average thickness is not 2.775 microns?
Stage I: Data Summary
Sample average n = 16, sample mean ¯x = 2.7688, sample standard deviation s = 0.0260.
Stage II: Determination of Suitable Hypotheses
Since this is a two-sided problem concerning whether μ ̸= 2.775, this should be the alternative hypothesis.
H0 : μ = 2.775
versus
HA : μ ̸= 2.775.
Stage III: Calculation of the Test Statistic
t =
√n(¯x−μ0)
s
=
√
16(2.7688−2.775)
0.0260
= −0.954
Stage IV: Expression for the p-value
p-value = 2 × P(X ≥0.954)
where the random variable X has a t-distribution with n −1 = 15 degrees of freedom.
Stage V: Evaluation of the p-value
Table III gives t0.10,15 = 1.341, and consequently it is known that the p-value is larger than 2 × 0.10 = 0.20.
Alternatively, exact computer calculation gives the p-value as 0.352.
Stage VI: Decision
Since the p-value is larger than 0.10, the null hypothesis is accepted.
Stage VII: Conclusion
This data set does not provide sufﬁcient evidence to establish that the average nickel layer thickness
is not 2.775 microns.

384
CHAPTER 8
INFERENCES ON A POPULATION MEAN
With t0.005,15 = 2.947, a 99% conﬁdence level two-sided conﬁdence interval for the
average thickness is
μ ∈

¯x −t0.005,15s
√n
, ¯x −t0.005,15s
√n

=

2.7688 −2.947 × 0.0260
√
16
, 2.7688 + 2.947 × 0.0260
√
16

= (2.750, 2.788)
This conﬁdence interval contains the target value of 2.775 microns, so just as the hypothesis
test indicated, it is plausible that the average thickness really is 2.775 microns. However, it
is important to remember that the hypothesis test has not proved that the average thickness is
2.775 microns, and the conﬁdence interval indicates that it could be as small as 2.750 microns,
or as large as 2.788 microns.
The researcher decides that it is worthwhile to commit further resources toward investigat-
ing the thicknesses of the nickel layers and decides that it would be useful to be able to have a
99% conﬁdence level two-sided conﬁdence interval for the average thickness that has a length
no longer than 0.02 microns. It can be estimated that this would require a total sample size of
n ≥4 ×
t0.005,15s
L0
2
= 4 ×
2.947 × 0.0260
0.02
2
= 58.7
Consequently, it can be estimated that an additional 59 −16 = 43 nickel layer thicknesses
need to be measured.
8.5
Case Study: Internet Marketing
Using the data in Figure 6.42 on the number of website visits per week over a 10-week period,
a 95% conﬁdence interval for the average number of visits per week can be calculated to be

255,499 , 340,850

whereas a 99% conﬁdence interval is

236,866 , 359,483

which is slightly wider.
8.6
Supplementary Problems
8.6.1 In an experiment to investigate when a radar picks up a
certain kind of target, a total of n = 15 trials are
conducted in which the distance of the target from the
radar is measured when the target is detected. A sample
mean of ¯x = 67.42 miles is obtained, with a sample
standard deviation of s = 4.947 miles.
(a) Is there enough evidence for the scientists to
conclude that the average distance at which the target
is detected is at least 65 miles?
(b) Construct a 99% one-sided t-interval that provides a
lower bound on the average distance at which the
target can be detected.
8.6.2 A company is planning a large telephone survey and is
interested in assessing how long it will take. In a short
pilot study, 40 people are contacted by telephone and are
asked the speciﬁed set of questions. The times of these
40 telephone surveys have a sample mean of ¯x =
9.39 minutes, with a sample standard deviation of s =
1.041 minutes.
(a) Can the company safely conclude that the telephone
surveys will last on average no more than 10 minutes
each?
(b) Construct a 99% one-sided t-interval that provides an
upper bound on the average time of each telephone
call.
8.6.3 A paper company sells paper that is supposed to have a
weight of 75.0 g/m2. In a quality inspection, the weights
of 30 random samples of paper are measured. The sample

8.6 SUPPLEMENTARY PROBLEMS
385
mean of these weights is ¯x = 74.63 g/m2, with a sample
standard deviation of s = 2.095 g/m2.
(a) Is there any evidence that the paper does not have an
average weight of 75.0 g/m2?
(b) Construct a 99% two-sided t-interval for the average
weight of the paper.
(c) If a 99% two-sided t-interval for the average weight
of the paper is required with a length no longer than
1.5 g/m2, how many additional paper samples would
you recommend need to be weighed?
8.6.4 A group of medical researchers is investigating how artery
disease affects the rigidity of the arteries. Deformity
measurements are made on a sample of 14 diseased
arteries, and a sample mean of ¯x = 0.497 is obtained,
with a sample standard deviation of s = 0.0764.
(a) What is the evidence that the average deformity value
of diseased arteries is less than 0.50?
(b) Construct a 99% two-sided t-interval for the average
deformity value of diseased arteries.
(c) If a 99% two-sided t-interval for the average
deformity value of diseased arteries is required with
a length no larger than 0.10, how many additional
arteries would you recommend be analyzed?
8.6.5 Osteoporosis Patient Heights
Consider the data set of osteoporosis patient heights
given in DS 6.7.4. Use a computer package to construct
90%, 95%, and 99% two-sided t-intervals for the mean
height. Is 70 inches a plausible value for the mean height?
8.6.6 Bamboo Cultivation
Consider the data set in DS 6.7.5 of bamboo shoot
heights 40 days after planting. Use a computer package to
construct 90%, 95%, and 99% two-sided t-intervals for the
mean shoot height. A previous study reported that under
similar growing conditions the mean shoot height after 40
days was more than 35 cm. Does the new data set conﬁrm
the previous study? Does it contradict the previous study?
8.6.7 The breaking strengths of a random sample of 26 molded
plastic housings were measured, and a sample mean of
¯x = 479.42 and a sample standard deviation of s = 12.55
were obtained. A conﬁdence interval (472.56, 486.28)
for the average strength of molded plastic housings was
constructed from these results. What is the conﬁdence
level of this conﬁdence interval?
8.6.8 Composites are materials that are made by embedding a
ﬁber, such as glass or carbon, inside a matrix, such as a
metal or a ceramic. Composites are used in civil
engineering structures, and their degradation when
subjected to weather conditions is an important issue. In
an experiment to investigate the effect of moisture on a
certain kind of composite, the weight gains of a collection
of 18 samples of composite subjected to water diffusion
were obtained. The sample mean was ¯x = 0.337%, with a
sample standard deviation of s = 0.025%.
(a) Is it safe to conclude from the results of this
experiment that the average weight gain for
composites of this kind is smaller than 0.36%?
(b) Construct a 99% conﬁdence interval that provides
an upper bound for the average weight gain for
composites of this kind.
8.6.9 Soil Compressibility Tests
Recall the data set of soil compressibility measurements
given in DS 6.7.6. Construct a 99% one-sided conﬁdence
interval that provides an upper bound on the average soil
compressibility. Can the engineers conclude that the
average soil compressibility is no larger than 25.5?
8.6.10 Conﬁdence Interval for a Population Variance
For use with Problems 8.6.11–8.6.14.
Recall that if the data observations are normally
distributed, then the sample variance S2 has the
distribution
S2 ∼σ 2 χ2
n−1
n −1
(a) Show that this result implies that
P

χ2
1−α/2,n−1 ≤(n −1)S2
σ 2
≤χ2
α/2,n−1

= 1 −α
(b) Deduce that
P

(n −1)S2
χ2
α/2,n−1
≤σ 2 ≤(n −1)S2
χ2
1−α/2,n−1

= 1 −α
so that

(n −1)s2
χ2
α/2,n−1
, (n −1)s2
χ2
1−α/2,n−1

is a 1 −α level two-sided conﬁdence interval for the
population variance σ 2.
(c) Explain why
	
(n −1)s2
χ2
α/2,n−1
,
	
(n −1)s2
χ2
1−α/2,n−1

is a 1 −α level two-sided conﬁdence interval for the
population standard deviation σ.
Even though the population mean μ is usually the
parameter of primary interest to an experimenter, in
certain situations it may be helpful to use this method to

386
CHAPTER 8
INFERENCES ON A POPULATION MEAN
construct a conﬁdence interval for the population variance
σ 2 or population standard deviation σ. An unfortunate
aspect of these conﬁdence intervals is that they depend
heavily on the data being normally distributed, and they
should be used only when that is a fair assumption. You
may be able to obtain these conﬁdence intervals on your
computer package.
8.6.11 A sample of n = 18 observations has a sample standard
deviation of s = 6.48. Use the method above to construct
99% and 95% two-sided conﬁdence intervals for the
population variance σ 2.
8.6.12 Consider the data set of 41 glass sheet thicknesses
described in Problem 8.1.2. Construct a 99% two-sided
conﬁdence interval for the standard deviation σ of the
sheet thicknesses.
8.6.13 Consider the data set of breaking strengths of wool ﬁber
bundles described in Problem 8.1.3. Construct a 95%
two-sided conﬁdence interval for the variance σ 2 of the
breaking strengths.
8.6.14 Consider the data set of sugar packet weights described in
Problem 8.1.4. Construct 90%, 95%, and 99% two-sided
conﬁdence intervals for the standard deviation σ of the
packet weights.
8.6.15 A two-sided t-test is performed. Use Table III to put
bounds on the p-value if:
(a) n = 8, t = 1.31
(b) n = 30, t = −2.82
(c) n = 25, t = 1.92
8.6.16 An experimenter measures the compressibility of 16 sam-
ples of clay randomly selected from a particular location,
and they have a sample mean of 76.99 and a sample
standard deviation of 5.37. Does this provide sufﬁcient
evidence for the experimenter to conclude that the average
clay compressibility at the location is less than 81?
8.6.17 A sample of 14 ﬁbers was tested. Their strengths had a
sample average of 266.5 and a sample standard deviation
of 18.6. Use a hypothesis test to assess whether it is safe
to conclude that the average strength of ﬁbers of this type
is at least 260.0.
8.6.18 Consider the data set
34 54 73 38 89 52 75 33 50 39 42 42
40 66 72 85 28 71
which is a random sample from a distribution with an
unknown mean μ. Calculate the following.
(a) The sample size
(b) The sample median
(c) The sample mean
(d) The sample standard deviation
(e) The sample variance
(f) The standard error of the sample mean
(g) A one-sided 99% conﬁdence interval that provides
a lower bound for μ
(h) Consider the hypothesis test H0 : μ = 50 versus
HA : μ ̸= 50. What bounds can you put on the
p-value using Table III?
8.6.19 Are the following statements true or false?
(a) In hypothesis testing the null hypothesis can never be
proved to be correct.
(b) For a given data set a two-sided conﬁdence interval
for a parameter with a conﬁdence level 99% is
shorter than a two-sided conﬁdence interval for the
parameter with a conﬁdence level 95%.
(c) A statistical proof that a statement is true is achieved
when the null hypothesis that the statement is false is
rejected.
(d) A hypothesis test addresses the question of whether
or not there is sufﬁcient evidence to establish that the
null hypothesis is false.
(e) A p-value between 1% and 10% should be
interpreted as implying that there is some evidence
that the null hypothesis is false, but that the evidence
is not overwhelming.
(f) z-intervals are sometimes referred to as large sample
intervals.
(g) If the p-value is 0.39 the null hypothesis is accepted
at size α = 0.05.
8.6.20 A sample of 22 wires was tested. Their resistances had a
sample average of 193.7 and a sample standard deviation
of 11.2. It is claimed that the average resistance of wires
of this type is 200.0. Use an appropriate hypothesis test to
investigate this claim.
8.6.21 An engineer selects 10 components at random and
measures their strengths. It is reported that the average
strength of the components is between 72.3 and 74.5 with
99% conﬁdence.
(a) What is the sample standard deviation of the
10 component strengths?
(b) If a 99% two-sided conﬁdence interval is desired with
a length no longer than 1.0, about how many additional
components would you recommend be tested?
8.6.22 A random sample of 10 items gives ¯x = 614.5 and
s = 42.9.

8.6 SUPPLEMENTARY PROBLEMS
387
(a) Use a hypothesis test to determine whether there is
sufﬁcient evidence for the experimenter to conclude
that the population average is not 600.
(b) Construct a 99% two-sided conﬁdence interval for
the population average.
(c) If a 99% two-sided conﬁdence interval for the
population average is required with a total length no
larger than 30, approximately how many additional
items do you think need to be sampled?
8.6.23 Twelve samples of a metal alloy are tested. The ﬂexibility
measurements had a sample average of 732.9 and a
sample standard deviation of 12.5.
(a) Is there sufﬁcient evidence to conclude that the
ﬂexibility of this kind of metal alloy is smaller than
750? Use an appropriate hypothesis test to investigate
this question.
(b) Construct a 99% conﬁdence interval that provides an
upper bound on the ﬂexibility of this kind of metal
alloy.
8.6.24 Flowrates in Urban Sewer Systems
Flow meters are installed in urban sewer systems to
measure the ﬂows through the pipes. In dry weather
conditions (no rain) the ﬂows are generated by waste
water from households and industries, together with
some possible drainage from water stored in the topsoil
from previous rainfalls. In a study of an urban sewer
system, the values given in DS 8.5.1 were obtained for
ﬂowrates during dry weather conditions.
(a) What is the sample mean?
(b) What is the sample median?
(c) What is the sample standard deviation?
(d) Construct a 99% two-sided conﬁdence interval for
the average ﬂowrate under dry weather conditions.
(e) Construct a 95% one-sided conﬁdence interval that
provides an upper bound for the average ﬂowrate
under dry weather conditions.
(f) If a 99% two-sided conﬁdence interval for the
average ﬂowrate under dry weather conditions is
required with a length no larger than 50, how much
additional sampling would you recommend?
(g) Show how to test H0 : μ = 440 against HA : μ ̸= 440.
(h) Show how to test H0 : μ ≥480 against HA : μ < 480.
8.6.25 Polymer Compound Densities
Eight samples of a polymer compound were obtained and
their densities were measured as given in DS 8.5.2.
(a) Use an appropriate hypothesis test to assess whether
there is sufﬁcient evidence to establish that the
average density of these kind of compounds is larger
than 3.50.
(b) Construct a 99% conﬁdence interval that provides a
lower bound on the average density of these kind of
compounds.
8.6.26 In a sample of size 33 a sample mean of 382.97 and a
sample standard deviation of 3.81 are obtained.
(a) Use an appropriate hypothesis test to assess whether
there is sufﬁcient evidence to establish that the
population mean is different from 385.
(b) Construct a 99% two-sided conﬁdence interval for
the population mean.
8.6.27 Show how Table III can be used to put bounds on the
p-values for these hypothesis tests.
(a) n = 24, ¯x = 2.39, s = 0.21, H0 : μ = 2.5,
HA : μ ̸= 2.5
(b) n = 30, ¯x = 0.538, s = 0.026, H0 : μ ≥0.540,
HA : μ < 0.540
(c) n = 10, ¯x = 143.6, s = 4.8, H0 : μ ≤135.0,
HA : μ > 135.0
You can use the data sets referred to in Problems 8.6.28–8.6.35 to
practice conﬁdence interval construction and hypothesis testing
for an unknown population mean.
8.6.28 Glass Fiber Reinforced Polymer Tensile Strengths
The data set in DS 6.7.7.
8.6.29 Infant Blood Levels of Hydrogen Peroxide
The data set in DS 6.7.8.
8.6.30 Paper Mill Operation of a Lime Kiln
The data set in DS 6.7.9.
8.6.31 River Salinity Levels
The data set in DS 6.7.10.
8.6.32 Dew Point Readings from Coastal Buoys
The data set in DS 6.7.11.
8.6.33 Brain pH levels
The data set in DS 6.7.12.
8.6.34 Silicon Dioxide Percentages in Ocean Floor
Volcanic Glass
The data set in DS 6.7.13.
8.6.35 Network Server Response Times
The data set in DS 6.7.14.
8.6.36 When using a conﬁdence interval for a population mean,
which of these would result in a shorter interval if
everything else remained the same?
A. A smaller sample mean
B. A smaller conﬁdence level

388
CHAPTER 8
INFERENCES ON A POPULATION MEAN
C. A larger sample variance
D. A p-value greater than 10%
8.6.37 If your computer reports a p-value of 0.005, then:
A. The probability that the alternative hypothesis is true
is 0.005.
B. The data has almost certainly been faked.
C. The chance of getting the data set or worse when the
null hypothesis is true is 1 out of 200.
D. The computer software package has certainly made a
mistake.
8.6.38 When deciding what to set as the null hypothesis and
what to set as the alternative hypothesis, the experimenter
should consider what is the objective of the analysis. If
the objective is to see whether there is sufﬁcient evidence
to establish a certain statement, then that statement should
be set as the alternative hypothesis.
A. True
B. False
8.6.39 For a one-sided t-test with HA : μ < 3, which of the
following t-statistics would result in the largest p-value?
A. 2.55
B. −2.55
C. 1.43
D. −1.33
E. 3.22
8.6.40 Hypothesis testing enables you to assess whether things
that your data suggest are actually statistically signiﬁcant.
A. True
B. False
8.6.41 Consider the design of a two-sample experiment to
compare two medical treatments on volunteers. If there is
a strong carry-over effect from one treatment to the other,
then:
A. A paired design is preferable to an independent
samples design.
B. A paired design in which each volunteer takes both
treatments should not be adopted.
C. The test will have little sensitivity.
D. A paired design can be implemented as long as there
is appropriate randomization.
E. None of the above.
8.6.42 Carbon Footprints
Analyze the data in DS 6.7.15, which contains estimates
of the pounds of carbon dioxide released when making
several types of car.
8.6.43 Data Warehouse Design
Power consumption represents a large proportion of a
data center’s costs. Analyze the data in DS 6.7.16 which
shows monthly electricty costs as a percentage of the data
center’s total costs.
8.6.44 Customer Churn
Customer churn is a term used for the attrition of a
company’s customers. DS 6.7.17 contains information
from an Internet service provider on the length of days
that its customers were signed up before switching to
another provider. Use the techniques described in this
chapter to analyze this data.
8.6.45 Mining Mill Operations
DS 6.7.18 contains daily data for the mill operations of a
mining company over a period of a month. Each day, the
company keeps track of the carbon concentration in the
waste material. Use the techniques described in this
chapter to analyze these data.
8.6.46 If your computer reports a p-value of 0.764, then:
A. The null hypothesis should be accepted.
B. The data has almost certainly been faked.
C. The null hypothesis should be rejected.
D. The computer software package has certainly made a
mistake.
8.6.47 In hypothesis testing:
A. Accepting the null hypothesis implies that the null
hypothesis has been proved. Rejecting the null
hypothesis implies that there is sufﬁcient evidence to
establish the alternative hypothesis.
B. Accepting the null hypothesis implies that the null
hypothesis has been proved. Rejecting the null
hypothesis does not imply that there is sufﬁcient
evidence to establish the alternative hypothesis.
C. Accepting the null hypothesis does not imply that the
null hypothesis has been proved. Rejecting the null
hypothesis implies that there is sufﬁcient evidence to
establish the alternative hypothesis.
D. Accepting the null hypothesis does not imply that the
null hypothesis has been proved. Rejecting the null
hypothesis does not imply that there is sufﬁcient
evidence to establish the alternative hypothesis.
8.6.48 If your computer reports a p-value of 0.25, then:
A. The probability that the null hypothesis is true is 0.25.
B. The chance of getting the data set or worse when the
null hypothesis is true is 1 out of 4.
C. Both of the above.
D. Neither of the above.
8.6.49 In hypothesis testing:
A. If the objective is to see whether there is sufﬁcient
evidence to establish a certain statement, then that
statement should be set as the null hypothesis.
B. It may be possible to prove the null hypothesis if
there is enough data.
C. Both of the above.
D. Neither of the above.

C H A P T E R N I N E
Comparing Two Population Means
9.1
Introduction
9.1.1
Two-Sample Problems
One of the most important statistical problems is making comparisons between two probability
distributions, which is considered in this chapter. This issue is often referred to as atwo-sample
problem since an experimenter typically has a set of data observations
x1, . . . , xn
from one population, population A say, and an additional set of data observations
y1, . . . , ym
from another population, population B say. The sample of data observations xi are taken to
be a set of independent observations from the unknown probability distribution governing
population A, with a cumulative distribution function FA(x). Similarly, the sample of data
observations yi are taken to be a set of independent observations from the unknown proba-
bility distribution governing population B, with a cumulative distribution function FB(x). The
sample sizes n and m of the two data sets need not be equal, although experiments are often
designed to have equal sample sizes.
In general, an experimenter is interested in assessing the evidence that there is a difference
between the two probability distributions FA(x) and FB(x). One important aspect of this
assessment is a comparison between the means of the two probability distributions, μA and μB,
as illustrated in Figure 9.1. Thus if μA = μB, the two populations have equal means and this
may be sufﬁcient for the experimenter to conclude that for practical purposes, the populations
are “identical” (although, in addition, a comparison of the variances of the two populations
may be informative, as illustrated in Figure 9.2). If the data analysis provides evidence that
μA ̸= μB, this indicates that the population probability distributions are different.
Example 51
Acrophobia Treatments
The standard treatment of acrophobia, the fear of heights, involves desensitizing patients’ fear
of heights by asking them to imagine being in high places and by taking them to high places,
as illustrated in Figure 9.3. A proposed new treatment using virtual reality provides a patient
with a head-mounted display that simulates the appearance of being in a building and allows
the patient to “travel” around the building. This device allows the patient to “explore” high
buildings while actually remaining in a safe place.
In an experiment to investigate whether the new treatment is effective or not, a group
of 30 patients suffering from acrophobia are randomly assigned to one of the two treatment
methods. Thus, 15 patients undergo the standard treatment, treatment A say, and 15 patients
undergo the proposed new treatment, treatment B. At the conclusion of the treatments the
patients are given a score that measures how much their condition has improved. The scores
389

390
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.1
Comparison of the means of two
probability distributions
μA
B
μ
Probability distribution
of population A
Probability distribution
of population B
Is μA =
B
μ ?
FIGURE 9.2
Comparison of the variances of two
probability distributions
μA
μB
Probability distribution
of population A
Probability distribution
of population B
σ 2
A
σ 2
B
Is 
=
?
of the patients undergoing the standard treatment provide the data observations
x1, . . . , x15
and the scores of the patients undergoing the new treatment provide the data observations
y1, . . . , y15
For this example, a comparison of the population means μA and μB provides an indication of
whether the new treatment is any better or any worse than the standard treatment.
Example 51 provides a good example of the use of a control group, which in this case is
the group of patients who undergo the standard treatment. In general, a control group provides
a standard against which a new procedure or treatment can be measured. Notice that it is good
experimental practice to randomize the allocation of subjects or experimental objects between
the standard treatment and the new treatment, as shown in Figure 9.4. Randomization helps
to eliminate any bias that may otherwise arise if certain kinds of subject are “favored” and
given a particular treatment.
Control groups are particularly important in medical or clinical trials where the efﬁcacy
of a new treatment or drug is under investigation. In these experiments the patients in the
control group may actually be administered a placebo, so that in effect they have no treatment
at all. For example, half the group may be given the new pills that are being tested and the
other half may be given identical looking pills that actually contain no medicine at all. Good

9.1 INTRODUCTION
391
Standard Treatment
Virtual Reality Treatment
Imagining
high places
Going
to high
places
FIGURE 9.3
Treating acrophobia
Standard
treatment
   New 
treatment
Experimental subjects
Randomization
FIGURE 9.4
Randomization of experimental subjects
between two treatments
experimental practice dictates that it is usually appropriate to run blind experiments where
the patients do not know which treatment they are receiving (Figure 9.5). In addition, these
experiments are often run in a double-blind manner whereby the person taking measurements
also does not know which treatment each patient received. These practices help to alleviate any
bias that may arise from patients or experimenters inadvertently allowing their perceptions or
hopes of what should happen to inﬂuence the results.
Of course, some experiments cannot be run blind. In Example 51 concerning acrophobia
treatments the patients obviously know if they are receiving the new virtual reality treatment.
However, it still may be advisable to arrange for the person measuring the progress made by
the patients to be unaware of which patients received which treatment.
Example 52
Kaolin Processing
Kaolin, a white clay material, is processed in a calciner to remove impurities. An important
characteristic of the processed kaolin is its “brightness” since this determines its suitability
for use in such things as paper products, ceramics, paints, medicines, and cosmetics.
A processing company has two calciners and the manager is interested in investigating
whether they are equally effective in processing the kaolin. A batch of kaolin is fed into the
two calciners and 12 randomly selected samples of the processed material are collected from

392
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.5
In a blind experiment the
experimental subjects do not know
which treatment they receive
Treatment A
Treatment B
Experimental
subjects
.
. . .
. . .
. . .
. . .
. . .
. .
.
each of the calciners. If the calciners are labeled A and B, then the brightness measurements
of the 12 samples from calciner A provide the data observations
x1, . . . , x12
and the brightness measurements of the 12 samples from calciner B provide the data obser-
vations
y1, . . . , y12
A comparison of the population means μA and μB provides an indication of whether the two
calciners are equally effective.
Example 53
Kudzu Pulping
Chemical engineers are interested in ﬁnding nonwood ﬁbers that can be used as an alternative
to wood pulp in paper manufacture. In an experiment to investigate the utility of kudzu,
a fast-growing vine that covers much of the southeastern United States, kudzu batches are
pulped with and without the addition of anthraquinone. One question of interest is whether
the addition of anthraquinone increases pulp yield.
A set of 20 experiments performed without anthraquinone (the control group) provide
pulp yield measurements
x1, . . . , x20
and a set of 25 experiments performed with anthraquinone provide pulp yield measurements
y1, . . . , y25

9.1 INTRODUCTION
393
Pulped
without
anthraquinone
Pulped
with
anthraquinone
Data observations
x1
x2
x20
y
y
1
2
25
. . .
y
. . .
Kudzu
FIGURE 9.6
Kudzu pulping experiment
+
−Administer electric
shock
Flexing foot
   muscle
FIGURE 9.7
Nerve conductivity experiment
as illustrated in Figure 9.6. In this experiment, a comparison of the population means μA and
μB indicates whether the addition of anthraquinone increases pulp yield.
Example 54
Nerve Conductivity
Speeds
A neurologist is investigating how diseases of the periphery nerves in humans inﬂuence the
conductivity speed of the nervous system. As Figure 9.7 shows, the conductivity speed of
nerves is determined by administering an electric shock to a patient’s leg and measuring the
time it takes to ﬂex a muscle in the patient’s foot.
Nerve conductivity speed measurements are made on n = 32 healthy patients and on
m = 27 patients who are known to have a periphery nerve disorder. The comparison of the
population means μA and μB provides an indication of whether diseases of the periphery
nerves affect the conductivity speed of the nervous system.
Example 45
Fabric Water
Absorption Properties
WiththeexperimentalapparatusshowninFigure6.37anexperimentercanaltertherevolutions
per minute of the rollers and the pressure between them. If the rollers rotate at 24 revolutions
per minute, how does changing the pressure from 10 pounds per square inch to 20 pounds per
square inch inﬂuence the water pickup of the fabric?
This question can be investigated by collecting some data observations xi of the fabric
water pickup with a pressure of 10 pounds per square inch and some data observations yi of
the fabric water pickup with a pressure of 20 pounds per square inch. A comparison of the
population means μA and μB shows how the average fabric water pickup is inﬂuenced by the
change in pressure.
ThecomparisonbetweentheunknownparametersμA andμB mayinvolvetheconstruction
of a conﬁdence interval for the difference μA −μB. This conﬁdence interval is centered at the
point estimate ¯x −¯y. It is particularly interesting to discover whether or not the conﬁdence

394
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.8
Interpretation of conﬁdence
intervals for μA −μB
Two-sided confidence interval for 
Evidence that
mA > mB
mA = mB
mA < mB
mA −mB
Evidence that
that
Plausible
0
0
0
)
(
)
(
)
(
interval contains 0, because this provides information on the plausibility of the population
means μA and μB being equal, as shown in Figure 9.8.
A more direct approach to assessing the plausibility that the population means μA and μB
are equal is to calculate a p-value for the hypotheses
H0 : μA = μB
versus
HA : μA ̸= μB
Small p-values (less than 0.01) indicate that the null hypothesis is not a plausible statement,
and the experimenter can conclude that there is sufﬁcient evidence to establish that the two
population means are different. A large p-value (greater than 0.10) indicates that there is
not sufﬁcient evidence to establish that the two population means are different. Notice that
the hypothesis testing problem is formulated so that the equality of the population means is
considered to be plausible unless the data present sufﬁcient evidence to prove that this cannot
be the case. One-sided versions of this hypothesis test can also be used.
9.1.2
Paired Samples versus Independent Samples
When collecting and analysing data for the comparison of two populations, it is important
to pay some attention to the experimental design. This term refers to the manner in which
the data are collected. In this chapter a distinction will be made between paired samples and
independent samples, and the appropriate analysis method depends upon which of these two
experimental designs is employed. The advantage of paired samples is that they can alleviate
the effect of variabilities in a factor other than the difference between the two populations.
This concept is illustrated in the following example.
Example 55
Heart Rate Reductions
A new drug for inducing a temporary reduction in a patient’s heart rate is to be compared
with a standard drug. The drugs are to be administered to a patient at rest, and the percentage
reduction in the heart rate is to be measured after ﬁve minutes.
Since the drug efﬁcacy is expected to depend heavily on the particular patient involved, a
paired experiment is run whereby each of 40 patients is administered one drug on one day and
the other drug on the following day. The spacing of the two experiments over two days ensures
that there is no “carryover” effect since the drugs are only temporarily effective. Nevertheless,
as Figure 9.9 illustrates, the order in which the two drugs are administered is decided in a
random manner so that one patient may have the standard drug followed by the new drug
and another patient may have the new drug followed by the standard drug. The comparison
between the two drugs is based upon the differences for each patient in the percentage heart
rate reductions achieved by the two drugs.

9.1 INTRODUCTION
395
FIGURE 9.9
Heart rate reduction experiment
Day 1
Day 2
Difference
patient 1
standard drug x1
new drug y1
z1 = x1 −y1
patient 2
new drug y2
standard drug x2
z2 = x2 −y2
patient 3
standard drug x3
new drug y3
z3 = x3 −y3
patient 4
new drug y4
standard drug x4
z4 = x4 −y4
...
...
...
...
patient 39
standard drug x39
new drug y39
z39 = x39 −y39
patient 40
new drug y40
standard drug x40
z40 = x40 −y40
FIGURE 9.10
The distinction between paired
and independent samples
Population A
Population B
Observations xi
Pairings
Observations yi
Observations xi
No pairings (sample sizes may be unequal)
Observations yi
Population A
Population B
Paired Samples
Independent Samples
As this example illustrates, data from paired samples are of the form
(x1, y1), (x2, y2), . . . , (xn, yn)
which arise from each of n experimental subjects being subjected to both “treatments.” The
data observation xi represents the measurement of treatment A applied to the ith experimental
subject, and the data observation yi represents the measurement of treatment B applied to the
same subject. The comparison between the two treatments is then based upon the pairwise
differences
zi = xi −yi
1 ≤i ≤n
Notice that with paired samples the sample sizes n and m from populations A and B obviously
have to be equal. The distinction between paired samples and independent (unpaired) samples
is illustrated in Figure 9.10.
Conducting an experiment in a paired manner, when it is possible to do so, is a speciﬁc
example of a more general experimental design concept called blocking. The experimenter
attempts to “block out” unwanted sources of variation that otherwise might cloud the com-
parisons of real interest. Additional experimental designs and analyses that use blocking are
presented in Section 11.2.
In Example 55, the medical researchers know that the efﬁcacies of the two drugs vary
considerably from one patient to another. Suppose that the experiment had been run on a
group of 80 patients with 40 patients being assigned randomly to each of the two drugs, as
illustrated in Figure 9.11. If the new treatment then appears to be better than the standard

396
CHAPTER 9
COMPARING TWO POPULATION MEANS
Patient 1
Patient 40
Patient 41
Patient 80
x1
x40
y1
y40
Standard
   drug
New
drug
FIGURE 9.11
Unpaired design for heart rate reduction experiment
Standard
radar
system
New
radar
system
xi
yi
ith target
FIGURE 9.12
Radar detection experiment
treatment, could it be because the new treatment happened to be administered to patients
who are more receptive to drug treatment? In statistical terms, the variability in the patients
creates more “noisy” data, which make it more difﬁcult to detect a difference in the efﬁcacies
of the two drugs. Conducting a paired experiment and looking at the differences in the two
measurements for each patient neutralizes the variability among the patients. Thus, the paired
experiment is more efﬁcient in that it provides more information for the given amount of data
collection.
Example 56
Radar Detection
Systems
A new radar system for detecting airborne objects is being tested against a standard system.
Different types of targets are ﬂown in different atmospheric conditions, and the distance of the
target from the radar system location is measured at the time when the target is ﬁrst detected
by the radar. It is obviously sensible to conduct this experiment with a paired design whereby
both radar systems attempt to detect the same target at the same time, assuming that the two
systems do not interfere with each other while operating simultaneously. The data observations
then consist of pairs (xi, yi), where xi is the distance of the ith target when detected by the
standard radar system and yi is the distance of the ith target when detected by the new radar
system, as shown in Figure 9.12.
An unpaired design for this experiment would consist of a series of targets being tested
against one radar system, with a rerun of the targets (or additional targets) for the other radar
system. The comparison between the two radar systems would then be clouded by possible
variations in the “detectability” of the targets due to possible changes in the experimental
conditions such as atmospheric conditions.
In conclusion, when an extraneous source of variation can be identiﬁed, such as variations
in a patient’s receptiveness to a drug or variations in a target’s detectability, it is best to employ

9.2 ANALYSIS OF PAIRED SAMPLES
397
a paired experimental design where possible. Unfortunately, in many cases it is impossible to
employ a paired experimental design due to the nature of the problem.
For instance, in Example 51 where two treatments for acrophobia are compared, it is to be
expected that a given treatment works better on some patients than on others. In other words,
there is likely to be some patient variability. However, the experiment cannot be run in a paired
fashion since this would require patients to undergo one treatment and then to “revert” to their
previous state before undergoing the second treatment! Undergoing one treatment changes a
subject so that an equivalent assessment of the other treatment on the same subject cannot be
undertaken. Similarly, if in Example 56 the two radar detection systems interfere with each
other if they are operated at the same time from the same place, then the paired design as
described is not feasible.
Section 9.2 discusses the analysis of paired samples based upon their reduction to a one-
sample problem and the employment of the one-sample techniques discussed in Chapter 8. In
Section 9.3 new techniques for analyzing two independent (unpaired) samples are discussed.
9.2
Analysis of Paired Samples
9.2.1
Methodology
The analysis of paired samples with data observations
(x1, y1), (x2, y2), . . . , (xn, yn)
is performed by reducing the problem to a one-sample problem. This is achieved by calculating
the differences
zi = xi −yi
1 ≤i ≤n
The data observations zi can be taken to be independent, identically distributed observations
from some probability distribution with mean μ. The one-sample techniques discussed in
Chapter 8 can be applied to the data set
z1, . . . , zn
in order to make inferences about the unknown mean μ. The parameter μ can be interpreted
as being the average difference between the “treatments” A and B.
Positive values of μ indicate that the random variables Xi tend to be larger than the random
variables Yi, so that the mean of population A, μA, is larger than the mean of population B,
μB. Similarly, negative values of μ indicate that μA < μB. It is usually particularly interesting
to test the hypotheses
H0 : μ = 0
versus
HA : μ ̸= 0
If the null hypothesis is a plausible statement, then it implies that there is not sufﬁcient evi-
dence of a difference between the mean values of the probability distributions of population
A and population B.
It can be instructive to build a simple model for the data observations. The observation xi,
that is, the observation obtained when treatment A is applied to the ith experimental subject,
can be thought of as a treatment A effect μA, together with a subject i effect γi say, and with
some random error ϵ A
i . Thus
xi = μA + γi + ϵ A
i

398
CHAPTER 9
COMPARING TWO POPULATION MEANS
Similarly, the observation yi, that is, the observation obtained when treatment B is applied to
the ith experimental subject, can be thought of as being formed as a treatment B effect μB,
together with the same subject i effect γi, and with a random error ϵ B
i , so that
yi = μB + γi + ϵ B
i
Notice that in these models the pairing of the data implies that the subject effects γi are
the same in the two equations. Also, it is important to notice that μA, μB, and γi are ﬁxed
unknown parameters, while the error terms ϵ A
i and ϵ B
i are observations of random variables
with expectations equal to 0.
With these model representations, it is clear that the differences zi can be represented as
zi = μA −μB + ϵ AB
i
where the error term is
ϵ AB
i
= ϵ A
i −ϵ B
i
Since this error term is an observation from a distribution with a zero expectation, the differ-
ences zi are consequently observations from a distribution with expectation
μ = μA −μB
which does not depend on the subject effect γi.
9.2.2
Examples
Example 55
Heart Rate Reductions
Figure 9.13 contains the percentage reductions in heart rate for the standard drug xi and the
new drug yi, together with the differences zi, for the 40 experimental subjects. First of all,
notice that the patients exhibit a wide variability in their response to the drugs. For some
patients the heart rate reductions are close to 20%, while for others they are over 40%. This
variability conﬁrms the appropriateness of a paired experiment.
An initial investigation of the data observations zi reveals that 30 out of 40 are negative.
This result suggests that μ = μA −μB < 0, so that the new drug has a stronger effect on
average. This suggestion is reinforced by a negative value for the sample average ¯z = −2.655.
The sample standard deviation of the differences zi is s = 3.730, so that with μ = 0 the
t-statistic is
t =
√n(¯z −μ)
s
=
√
40 × (−2.655)
3.730
= −4.50
The p-value for the two-sided hypothesis testing problem
H0 : μ = 0
versus
HA : μ ̸= 0
is therefore
p-value = 2 × P(X > 4.50) ≃0.0001
where the random variable X has a t-distribution with 39 degrees of freedom.
This analysis reveals that it is not plausible that μ = 0, and so the experimenter can
conclude that there is evidence that the new drug has a different effect from the standard
drug. From the critical point t0.005,39 = 2.7079, a 99% two-sided conﬁdence interval for the

9.2 ANALYSIS OF PAIRED SAMPLES
399
FIGURE 9.13
Heart rate reductions data set
(% reduction in heart rate)
1
28.5
34.8
2
26.6
37.3
3
28.6
31.3
4
22.1
24.4
5
32.4
39.5
6
33.2
34.0
7
32.9
33.4
8
27.9
27.4
9
26.8
35.4
10
30.7
35.7
11
39.6
40.4
12
34.9
41.6
13
31.1
30.8
14
21.6
30.5
15
40.2
40.7
16
38.9
39.9
17
31.6
30.2
18
36.0
34.5
19
25.4
31.2
20
35.6
35.5
−6.3
−10.7
−2.7
−2.3
−7.1
−0.8
−0.5
0.5
−8.6
−5.0
−0.8
−6.7
0.3
−8.9
−0.5
−1.0
1.4
1.5
−5.8
0.1
Patient Standard drug
New drug
xi
yi
zi = xi – yi
Patient Standard drug
New drug
xi
yi
zi = xi – yi
21
27.0
25.3
22
33.1
34.5
23
28.7
30.9
24
33.7
31.9
25
33.7
36.9
26
34.3
27.8
27
32.6
35.7
28
34.5
38.4
29
32.9
36.7
30
29.3
36.3
31
35.2
38.1
32
29.8
32.1
33
26.1
29.1
34
25.6
33.5
35
27.6
28.7
36
25.1
31.4
37
23.7
22.4
38
36.3
43.7
39
33.4
30.8
40
40.1
40.8
1.7
−1.4
−2.2
1.8
−3.2
6.5
−3.1
−3.9
−3.8
−7.0
−2.9
−2.3
−3.0
−7.9
−1.1
−6.3
1.3
−7.4
2.6
−0.7
difference between the average effects of the drugs is
μ = μA −μB ∈

¯z −t0.005,39s
√
40
, ¯z + t0.005,39s
√
40

=

−2.655 −2.7079 × 3.730
√
40
, −2.655 + 2.7079 × 3.730
√
40

= (−4.252, −1.058)
Consequently, based upon this data set the experimenter can conclude that the new drug
provides a reduction in a patient’s heart rate of somewhere between 1% and 4.25% more on
average than the standard drug.
Example 56
Radar Detection
Systems
Figure 9.14 shows the radar detection distances in miles for 24 targets. The observations xi
are for the standard system and the observations yi are for the new system. An initial look at
the data indicates that the detectability of the targets varies from about 45 miles in some cases
to over 55 miles in other cases, and this conﬁrms the advisability of a paired experiment.
The differences zi have a sample mean ¯z = −0.261 and a sample standard deviation
s = 1.305. With μ = 0 the t-statistic is therefore
t =
√n(¯z −μ)
s
=
√
24 × (−0.261)
1.305
= −0.980
If the experimenter is interested in ascertaining whether or not the new radar system can detect
targets at a greater distance than the standard system, it is appropriate to consider the one-sided
hypothesis testing problem
H0 : μ ≥0
versus
HA : μ < 0

400
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.14
Radar detection systems data set
(distance of target in miles when
detected)
Target Standard radar system
New radar system
xi
yi
zi = xi −yi
1
48.40
51.14
−2.74
2
47.73
46.48
1.25
3
51.30
50.90
0.40
4
50.49
49.82
0.67
5
47.06
47.99
−0.93
6
53.02
53.20
−0.18
7
48.96
46.76
2.20
8
52.03
54.44
−2.41
9
51.09
49.85
1.24
10
47.35
47.45
−0.10
11
50.15
50.66
−0.51
12
46.59
47.92
−1.33
13
52.03
52.37
−0.34
14
51.96
52.90
−0.94
15
49.15
50.67
−1.52
16
48.12
49.50
−1.38
17
51.97
51.29
0.68
18
53.24
51.60
1.64
19
55.87
54.48
1.39
20
45.60
45.62
−0.02
21
51.80
52.24
−0.44
22
47.64
47.33
0.31
23
49.90
51.13
−1.23
24
55.89
57.86
−1.97
This is because the experimenter is asking whether there is sufﬁcient evidence to establish
that μ < 0. In this case the p-value is
P(X ≤−0.980) = 0.170
where the random variable X has a t-distribution with 23 degrees of freedom. With such a
large p-value, the analysis indicates that this data set does not provide sufﬁcient evidence to
establish that the new radar system is any better than the standard radar system.
9.2.3
Problems
9.2.1 Production Line Assembly Methods
DS 9.2.1 shows the data obtained from a paired
experiment performed to examine which of two assembly
methods is quicker on average. A random sample of 35
workers on an assembly line were selected and all were
timed while they assembled an item in the standard
manner (method A) and while they assembled an item in
the new manner (method B). The times in seconds are
recorded. Analyze the data set and present your
conclusions on how the new assembly method differs
from the standard assembly method. Why are the two
data samples paired? Why did the experimenter decide to
perform a paired experiment rather than an unpaired
experiment?
9.2.2 Red Blood Cell Adherence to Endothelial Cells
Researchers into the genetic disease sickle cell anemia are
interested in how red blood cells adhere to endothelial
cells, which form the innermost lining of blood vessels. A
set of 14 blood samples are obtained, and each sample is
split in half. One half of the blood sample is profused
over an endothelial monolayer of type A, and the other

9.2 ANALYSIS OF PAIRED SAMPLES
401
half of the blood sample is profused over an endothelial
monolayer of type B. The two types of monolayer differ
in respect to the stimulation conditions of the endothelial
cells. The data recorded in DS 9.2.2 are the number of
adherent red blood cells per mm2. Is there any evidence
that the different stimulation conditions affect the
adhesion of red blood cells?
9.2.3 Tire Tread Wear
An experiment is performed to assess whether a new tire
wears more slowly than a standard tire. A set of 20 trucks
is chosen. A new tire is placed on one of the front wheels
of each truck, and a standard tire is placed on the other
front wheel. Right and left positions of the two kinds of
tire are randomized over the 20 trucks. The trucks are
driven over varying road conditions, and then the
reductions in the tread depths of the tires are measured.
DS 9.2.3 contains these data values in mm. Analyze the
data and present your conclusions. Why is this a paired
experiment? Why did the experimenter decide to perform
a paired experiment rather than an unpaired experiment?
9.2.4 Calculus Teaching Methods
A new teaching method for a calculus class is being
evaluated. A set of 80 students is formed into 40 pairs,
where the two-pair members have roughly equal
mathematics test scores. The pairs are then randomly
split, with one member being assigned to section A where
the standard teaching method is used and with one
member being assigned to section B where the new
teaching method is tried. At the end of the course all the
students take the same exam and their scores are shown in
DS 9.2.4. Analyze the data and present your conclusions
regarding how the new teaching method compares with
the standard approach. Why is this a paired experiment?
Why was it decided to perform a paired experiment rather
than an unpaired experiment?
9.2.5 Radioactive Carbon Dating
Two independently operated laboratories provide
historical dating services using radioactive carbon dating
methods. A researcher suspects that one laboratory tends
to provide older datings than the other laboratory. To
investigate this supposition 18 samples of old material are
split in half. One half is sent to laboratory A for dating,
and the other half is sent to laboratory B for dating. The
laboratories are asked to submit their answers to the
nearest decade, and the results obtained are presented in
DS 9.2.5. Is there any evidence that one laboratory tends
to provide older datings than the other laboratory?
9.2.6 Golf Ball Design
A sports manufacturer has developed a new golf ball
with special dimples that it hopes will cause the ball to
travel farther than standard golf balls. This question is
examined by asking 24 golfers to hit ten new balls
and ten standard balls. Each ball is hit off a tee, and
randomization techniques are employed to account for
any fatigue or wind effects. The distances traveled
by the balls are measured, and DS 9.2.6 presents the
average distances in yards for the ten shots for each
of the 24 golfers and the two types of ball. What should
the company conclude from this experiment? Why
was it a good idea to use a paired design for this
experiment?
9.2.7 Stimulus Reaction Times
An experiment was conducted to compare two procedures
(A and B) for measuring a person’s reaction time to a
stimulus. Ten volunteers participated in the experiment,
and each volunteer was given the stimulus twice. For each
person the reaction time was measured once with
procedure A and once with procedure B, as shown in
DS 9.2.7. A reviewer comments that the differences
between the reaction times obtained for procedures A and
B can be explained by the fact that each time a person is
given the stimulus the actual reaction time varies. Do you
agree with the reviewer, or do you think that there is
evidence that procedures A and B do give different
readings on average?
9.2.8 Antibiotic Efﬁcacies
Eight cultures of a bacterium are split in half. One half is
tested using a standard antibiotic and the other half is
tested using a new antibiotic. The data values in DS 9.2.8
are the times taken to kill the bacterium. Use an
appropriate hypothesis test to assess whether there is any
evidence that the new antibiotic is quicker than the
standard antibiotic.
9.2.9 Uranium-Oxide Removal from Water
An experiment is conducted to investigate how the
addition of a surfactant affects the ability of magnetized
steel wool to remove uranium-oxide particles from water.
Six batches of uranium-oxide contaminated water are
obtained that are each split in half, and the surfactant is
added to one of the two halves for each batch. The
uranium-oxide levels are measured for each of the
resulting 12 samples of water both before and after they
are passed through some magnetized steel wool, and the
reductions in the uranium-oxide levels are calculated. The

402
CHAPTER 9
COMPARING TWO POPULATION MEANS
resulting data set is shown in DS 9.2.9. Perform a
hypothesis test to investigate whether this experiment
provides sufﬁcient evidence for the experimenter to
conclude that the addition of the surfactant has an effect
on the ability of magnetized steel wool to remove
uranium-oxide particles from water.
9.2.10 A clinical trial is run to compare two medications. A
blind experiment is one in which a patient does not know
what kind of medication they are getting at any point in
time.
A. If the clinical trial is run as a blind experiment, then it
cannot be designed as a paired experiment.
B. If the clinical trial is run as a blind experiment, then it
may still be possible to design it as a paired
experiment.
9.2.11 For a two-sample problem with n = 11 paired samples,
¯x = 58.42, ¯y = 44.34, sx = 2.80, and sy = 2.96. The test
statistic for the hypotheses H0 : μA = μB versus
HA : μA ̸= μB is:
A. 3.87
B. 3.49
C. 3.09
D. Unknown from the information given
E. None of the above
9.3
Analysis of Independent Samples
The analysis of two independent (unpaired) samples is now considered. The data consist of a
sample of n observations xi from population A with a sample mean ¯x and a sample standard
deviation sx, together with a sample of m observations yi from population B with a sample
mean ¯y and a sample standard deviation sy, as shown in Figure 9.15.
The point estimate of the difference in the population means μA −μB is ¯x −¯y. Since
Var(¯x) = σ 2
A/n and Var(¯y) = σ 2
B/m, where σ 2
A and σ 2
B are the two population variances, this
point estimate has a standard error
s.e.(¯x −¯y) =

σ 2
A
n + σ 2
B
m
Three procedures for making inferences about the difference of the population means μA−μB
are outlined below, and they differ with respect to how the standard error of ¯x −¯y is estimated.
The ﬁrst “general procedure” estimates the standard error as
s.e.(¯x −¯y) =

s2x
n +
s2y
m
A second “pooled variance procedure” is based on the assumption that the population variances
σ 2
A and σ 2
B are equal, and estimates the standard error as
s.e.(¯x −¯y) = sp

1
n + 1
m
where s2
p is a pooled estimate of the common population variance. These two procedures
are referred to as two-sample t-tests. Finally, a two-sample z-test can be used when the
population variances σ 2
A and σ 2
B are assumed to take “known” values. In each case, the choice
of the estimate of the standard error affects the probability distribution used to calculate
FIGURE 9.15
Summary statistics for analysis of
two independent samples
Sample size
Sample mean
Sample standard deviation
Population A
n
¯x
sx
Population B
m
¯y
sy

9.3 ANALYSIS OF INDEPENDENT SAMPLES
403
p-values and critical points. In the ﬁrst two cases t-distributions are appropriate, though with
different degrees of freedom, and in the ﬁnal case the standard normal distribution is used.
Out of these three procedures the general procedure can always be used, although in certain
cases an experimenter may prefer one of the other two procedures.
As with one-sample t-tests and z-tests, these two-sample tests are based on the assumption
that the data are normally distributed. For large sample sizes the central limit theorem implies
that the sample means are normally distributed and this is sufﬁcient to ensure that the tests
are appropriate. For small sample sizes the tests also behave satisfactorily unless the data
observations are clearly not normally distributed, in which case it is wise to employ one of
the nonparametric procedures described in Chapter 15.
9.3.1
General Procedure
A general method for making inferences about the difference of the population means μA−μB
uses a point estimate ¯x −¯y whose standard error is estimated by
s.e.(¯x −¯y) =

s2x
n +
s2y
m
In this case p-values and critical points are calculated from a t-distribution. The degrees of
freedom ν of the t-distribution are usually calculated to be
ν =

s2
x
n +
s2
y
m
2
s4x
n2(n−1) +
s4y
m2(m−1)
rounded down to the nearest integer. The simpler choice ν = min{n, m} −1 can also be used,
although it is a little less powerful.
A two-sided 1 −α level conﬁdence interval for μA −μB is therefore
μA −μB ∈
⎛
⎝¯x −¯y −tα/2,ν

s2x
n +
s2y
m , ¯x −¯y + tα/2,ν

s2x
n +
s2y
m
⎞
⎠
which, as shown in Figure 9.16, is constructed using the standard format
μ ∈( ˆμ −critical point × s.e.( ˆμ), ˆμ + critical point × s.e.( ˆμ))
FIGURE 9.16
A two-sample two-sided t-interval
μA
x
μ
Critical point ×  s.e. (               )
A
μB
−
μA
μA
μB
−   
y
−
tα/2,ν ×
s

2
x
n +
s2
y
m
)
)
¯
¯
ˆ
ˆ
ˆ
ˆ

404
CHAPTER 9
COMPARING TWO POPULATION MEANS
with μ = μA −μB in this case. One-sided conﬁdence intervals are
μA −μB ∈
⎛
⎝−∞, ¯x −¯y + tα,ν

s2x
n +
s2y
m
⎞
⎠
and
μA −μB ∈
⎛
⎝¯x −¯y −tα,ν

s2x
n +
s2y
m , ∞
⎞
⎠
For the two-sided hypothesis testing problem
H0 : μA −μB = δ
versus
HA : μA −μB ̸= δ
for some ﬁxed value δ of interest (usually δ = 0), the appropriate t-statistic is
t = ¯x −¯y −δ

s2x
n +
s2y
m
The two-sided p-value is calculated as
p-value = 2 × P(X > |t|)
where the random variable X has a t-distribution with ν degrees of freedom, and a size α
hypothesis test accepts the null hypothesis if
|t| ≤tα/2,ν
and rejects the null hypothesis when
|t| > tα/2,ν
A one-sided hypothesis testing problem
H0 : μA −μB ≤δ
versus
HA : μA −μB > δ
has a p-value
p-value = P(X > t)
and a size α hypothesis test accepts the null hypothesis if
t ≤tα,ν
and rejects the null hypothesis if
t > tα,ν
Similarly, the one-sided hypothesis testing problem
H0 : μA −μB ≥δ
versus
HA : μA −μB < δ
has a p-value
p-value = P(X < t)
and a size α hypothesis test accepts the null hypothesis if
t ≥−tα,ν
and rejects the null hypothesis if
t < −tα,ν

9.3 ANALYSIS OF INDEPENDENT SAMPLES
405
As an illustration of these inference procedures, suppose that data are obtained with
n = 24, ¯x = 9.005, sx = 3.438 and m = 34, ¯y = 11.864, sy = 3.305. The hypotheses
H0 : μA = μB
versus
HA : μA ̸= μB
are tested with the t-statistic
t =
¯x −¯y

s2x
n + s2y
m
= 9.005 −11.864

3.4382
24
+ 3.3052
34
= −3.169
Two-Sample t-Procedure (Unequal Variances)
Consider a sample of size n from population A with a sample mean ¯x and a sample
standard deviation sx, and a sample of size m from population B with a sample mean ¯y
and a sample standard deviation sy.
A two-sided 1 −α level conﬁdence interval for the difference in population means
μA −μB is
μA −μB ∈
⎛
⎝¯x −¯y −tα/2,ν

s2x
n +
s2y
m , ¯x −¯y + tα/2,ν

s2x
n +
s2y
m
⎞
⎠
where the degrees of freedom of the critical point are
ν =
 s2
x
n +
s2
y
m
2
s4x
n2(n−1) +
s4y
m2(m−1)
One-sided conﬁdence intervals are
μA −μB ∈
⎛
⎝−∞, ¯x −¯y + tα,ν

s2x
n +
s2y
m
⎞
⎠
and
μA −μB ∈
⎛
⎝¯x −¯y −tα,ν

s2x
n +
s2y
m , ∞
⎞
⎠
The appropriate t-statistic for the null hypothesis H0 : μA −μB = δ is
t = ¯x −¯y −δ

s2x
n +
s2y
m
A two-sided p-value is calculated as 2 × P(X > |t|), where the random variable X
has a t-distribution with ν degrees of freedom, and one-sided p-values are P(X > t)
and P(X < t). A size α two-sided hypothesis test accepts the null hypothesis if
|t| ≤tα/2,ν
and rejects the null hypothesis when
|t| > tα/2,ν
and size α one-sided hypothesis tests have rejection regions t > tα,ν or t < −tα,ν.
These procedures are known as two-sample t-tests without a pooled variance
estimate.

406
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.17
Calculation of a two-sided p-value
0
 distribution
t48 
3.169
versus
H0 :
=
:
̸=
HA
μA
μA
μB
μA
μA
μB
p-value = 2 × P(X
) = 2 ×0.00135 = 0.0027
3.169
>
The two-sided p-value is therefore
p-value = 2 × P(X > 3.169)
where the random variable X has a t-distribution with degrees of freedom
ν =

3.4382
24
+ 3.3052
34
2
3.4384
242×23 + 3.3054
342×33
= 48.43
Using the integer value ν = 48 gives
p-value ≃2 × 0.00135 = 0.0027
as illustrated in Figure 9.17, so that there is very strong evidence that the null hypothesis is
not a plausible statement, and the experimenter can conclude that μA ̸= μB.
With a critical point t0.005,48 = 2.6822, a 99% two-sided conﬁdence interval for the
difference in population means can be calculated as
μA −μB ∈

9.005 −11.864 −2.6822

3.4382
24
+ 3.3052
34
,
9.005 −11.864 + 2.6822

3.4382
24
+ 3.3052
34

= (−5.28, −0.44)
The fact that 0 is not contained within this conﬁdence interval implies that the null hypothesis
H0 : μA = μB has a two-sided p-value smaller than 0.01, which is consistent with the result
of the hypothesis test.
Remember that the relationships between conﬁdence intervals, p-values, and size α hy-
pothesis tests for two-sample problems are exactly the same as they are for one-sample
problems. A size α hypothesis test rejects when the p-value is less than α and accepts when
it is larger than α. Furthermore, a 1 −α level conﬁdence interval for μA −μB contains the
values of δ for which the null hypothesis H0 : μA −μB = δ has a p-value larger than α.
These relationships are illustrated in Figure 9.18 for two-sample two-sided problems.

9.3 ANALYSIS OF INDEPENDENT SAMPLES
407
Hypothesis Testing
Conﬁdence Intervals
H0 : μA −μB = δ versus HA : μA −μB ̸= δ
x −y −tα/2,ν

s2x
n +
s2y
m , x −y + tα/2,ν

s2x
n +
s2y
m


Signiﬁcance Levels
Conﬁdence Levels
p-Value
α = 0.10
α = 0.05
α = 0.01
1 −α = 0.90
1 −α = 0.95
1 −α = 0.99
≥0.10
0.05– 0.10
0.01– 0.05
< 0.01
¯
¯
¯
¯
accept H0
accept H0
accept H0
contains δ
contains δ
contains δ
reject H0
accept H0
accept H0
does not contain δ
contains δ
contains δ
reject H0
reject H0
accept H0
does not contain δ
does not contain δ
contains δ
reject H0
reject H0
reject H0
does not contain δ
does not contain δ
does not contain δ
FIGURE 9.18
Relationship between hypothesis testing and conﬁdence intervals for two-sample two-sided problems
9.3.2
Pooled Variance Procedure
The general procedures described above can always be used, except with small sample sizes
when the data are obviously not normally distributed, and in particular they are appropriate
when the population variances σ 2
A and σ 2
B are unequal. However, in certain circumstances an
experimenter may be willing to make the assumption that σ 2
A = σ 2
B, and this allows a slightly
more powerful analysis based on a pooled variance estimate.
If the population variances σ 2
A and σ 2
B are assumed to be equal to a common value σ 2,
then this can be estimated by
ˆσ 2 = s2
p =
(n −1)s2
x + (m −1)s2
y
n + m −2
which is known as the pooled variance estimator. In this case the standard error of ¯x −¯y is
s.e.(¯x −¯y) =

σ 2
A
n + σ 2
B
m = σ

1
n + 1
m
which can be estimated by
s.e.(¯x −¯y) = sp

1
n + 1
m
When a pooled variance estimate is employed, p-values and critical points are calculated from
a t-distribution with n + m −2 degrees of freedom. For example, a two-sided 1 −α level
conﬁdence interval for μA −μB is therefore
μA −μB ∈

¯x −¯y −tα/2,n+m−2 sp

1
n + 1
m , ¯x −¯y + tα/2,n+m−2 sp

1
n + 1
m

which again is constructed using the standard format
μ ∈( ˆμ −critical point × s.e.( ˆμ), ˆμ + critical point × s.e.( ˆμ))
with μ = μA −μB. The box on the two-sample t-procedure (equal variances) shows the other
applications of this methodology.

408
CHAPTER 9
COMPARING TWO POPULATION MEANS
Consider again the data obtained with n = 24, ¯x = 9.005, sx = 3.438 and m = 34,
¯y = 11.864, sy = 3.305. The sample standard deviations are similar and so it may be
reasonable to assume that the population variances are equal. In this case, the estimate of the
common standard deviation is
sp =

(n −1)s2x + (m −1)s2y
n + m −2
=

(23 × 3.4382) + (33 × 3.3052)
24 + 34 −2
= 3.360
The hypotheses
H0 : μA = μB
versus
HA : μA ̸= μB
are now tested with the t-statistic
t =
¯x −¯y
sp

1
n + 1
m
= 9.005 −11.864
3.360

1
24 + 1
34
= −3.192
The two-sided p-value is therefore
p-value = 2 × P(X > 3.192) ≃2 × 0.00115 = 0.0023
where the random variable X has a t-distribution with degrees of freedom n + m −2 = 56,
as illustrated in Figure 9.19.
With a critical point t0.005,56 = 2.6665, a 99% two-sided conﬁdence interval for the
difference in population means can be calculated as
μA −μB ∈

9.005 −11.864 −2.6665 × 3.360 ×

1
24 + 1
34,
9.005 −11.864 + 2.6665 × 3.360 ×

1
24 + 1
34

= (−5.25, −0.47)
FIGURE 9.19
Calculation of a two-sided p-value
 distribution
versus
H0 :
=
:
̸=
t56
3.192
HA
 A
μA
 B
 A
μ
μ
A
= 2 ×
×
P(X
) = 2
0.00115= 0.0023
3.192
>
p-value
0
 μB

9.3 ANALYSIS OF INDEPENDENT SAMPLES
409
Two-Sample t-Procedure (Equal Variances)
Consider a sample of size n from population A with a sample mean ¯x and a sample
standard deviation sx, and a sample of size m from population B with a sample mean ¯y
and a sample standard deviation sy. If an experimenter assumes that the population
variances σ 2
A and σ 2
B are equal, then the common variance can be estimated by
s2
p =
(n −1)s2
x + (m −1)s2
y
n + m −2
which is known as the pooled variance estimate.
A two-sided 1 −α level conﬁdence interval for the difference in population means
μA −μB is
μA −μB ∈

¯x −¯y −tα/2,n+m−2 sp

1
n + 1
m , ¯x −¯y + tα/2,n+m−2 sp

1
n + 1
m

One-sided conﬁdence intervals are
μA −μB ∈

−∞, ¯x −¯y + tα,n+m−2 sp

1
n + 1
m

and
μA −μB ∈

¯x −¯y −tα,n+m−2 sp

1
n + 1
m , ∞

The appropriate t-statistic for the null hypothesis H0 : μA −μB = δ is
t = ¯x −¯y −δ
sp

1
n + 1
m
A two-sided p-value is calculated as 2 × P(X > |t|), where the random variable X
has a t-distribution with n + m −2 degrees of freedom, and one-sided p-values
are P(X > t) and P(X < t). A size α two-sided hypothesis test accepts the null
hypothesis if
|t| ≤tα/2,n+m−2
and rejects the null hypothesis when
|t| > tα/2,n+m−2
and size α one-sided hypothesis tests have rejection regions t > tα,n+m−2 or
t < −tα,n+m−2.
These procedures are known as two-sample t-tests with a pooled variance
estimate.
These results are seen to match well with the corresponding analyses conducted previously
using the general procedure, which does not require the equality of the population variances.
When should an experimenter use this method with a pooled variance estimate, and when
should the general procedure be employed that does not require the equality of the population
variances? The safest answer is to always use the general procedure since it provides a valid
analysisevenwhenthepopulationvariancesareequal.However,ifthepopulationvariancesare

410
CHAPTER 9
COMPARING TWO POPULATION MEANS
equal or quite similar, then the pooled variance procedure generally provides a slightly more
powerful analysis with slightly shorter conﬁdence intervals and slightly smaller p-values.
An experimenter’s assessment of whether or not the population variances can be taken to
be equal may be based on prior experience with the kind of data under consideration, or may
be based on a comparison of the sample standard deviations sx and sy. A formal test for the
equality of the population variances σ 2
A and σ 2
B is described in the Supplementary Problems
section at the end of this chapter. However, this test has some drawbacks and its employment
is not widely recommended.
In general, analyses performed with and without the use of a pooled variance estimate will
often provide similar results. In fact, if the results are quite different, then this will be because
the sample standard deviations sx and sy are quite different, in which case it is proper to use
the general procedure that does not require the equality of the population variances.
9.3.3
z-Procedure
Two-Sample z-Procedure
Consider a sample of size n from population A with a sample mean ¯x, and a sample of
size m from population B with a sample mean ¯y. Suppose that the population
variances are assumed to take known values σ 2
A and σ 2
B.
A two-sided 1 −α level conﬁdence interval for the difference in population means
μA −μB is
μA −μB ∈
⎛
⎝¯x −¯y −zα/2

σ 2
A
n + σ 2
B
m , ¯x −¯y + zα/2

σ 2
A
n + σ 2
B
m
⎞
⎠
One-sided conﬁdence intervals are
μA −μB ∈
⎛
⎝−∞, ¯x −¯y + zα

σ 2
A
n + σ 2
B
m
⎞
⎠
and
μA −μB ∈
⎛
⎝¯x −¯y −zα

σ 2
A
n + σ 2
B
m , ∞
⎞
⎠
The appropriate z-statistic for the null hypothesis H0 : μA −μB = δ is
z = ¯x −¯y −δ

σ 2
A
n + σ 2
B
m
A two-sided p-value is calculated as 2 × (−|z|), and one-sided p-values are
1 −(z) and (z). A size α two-sided hypothesis test accepts the null hypothesis if
|z| ≤zα/2
and rejects the null hypothesis when
|z| > zα/2
and size α one-sided hypothesis tests have rejection regions z > zα or z < −zα.
These procedures are known as two-sample z-tests.

9.3 ANALYSIS OF INDEPENDENT SAMPLES
411
Two-sample z-tests are used when an experimenter wishes to use “known” values of the
population standard deviations σA and σB in place of the sample standard deviations sx and sy.
In this case p-values and critical points are calculated from the standard normal distribution.
As with one-sample procedures, two-sample t-tests with large sample sizes are essentially
equivalent to the two-sample z-test. Consequently, the two-sample z-test can be thought of
as a large-sample procedure and the two-sample t-tests can be thought of as small-sample
procedures.
COMPUTER NOTE
Find out how to perform two-sample procedures for independent samples on your computer
package. You should anticipate being able to make the following choices:
■
t-procedure or z-procedure
■
two-sided procedure or one-sided procedure
■
general procedure or pooled variance procedure
■
conﬁdence interval or hypothesis test
9.3.4
Examples
Example 51
Acrophobia Treatments
Figure 9.20 shows the scores of the 15 patients who underwent the standard treatment and
the scores of the 15 patients who underwent the new treatment. Higher scores correspond
to greater improvements in the patient’s condition. There are two different sets of patients
undergoing the two therapies, so these are independent (unpaired) samples.
Standard
treatment
xi
33
54
62
46
52
42
34
51
26
68
47
40
46
51
60
New
treatment
yi
65
61
37
47
45
53
53
69
49
42
40
67
46
43
51
FIGURE 9.20
Acrophobia treatments data set
(improvement scores)
Figure 9.21 shows descriptive statistics and boxplots of the two samples. The boxplots,
which are drawn with the same scale, clearly indicate that the scores with the new treatment
appear to be slightly higher than the scores with the standard treatment. In fact, the average
of the scores with the new treatment is ¯y = 51.20, whereas the average of the scores with the
standard treatment is ¯x = 47.47. Is this difference statistically signiﬁcant?
Data
New Treatment
Standard Treatment
70
60
50
40
30
20
Standard Treatment
Sample size = 15
Sample mean = 47.47
Sample standard deviation = 11.40
New Treatment
Sample size = 15
Sample mean = 51.20 
Sample standard deviation = 10.09
FIGURE 9.21
Descriptive statistics and boxplots for acrophobia treatments data set

412
CHAPTER 9
COMPARING TWO POPULATION MEANS
The sample standard deviations are sx = 11.40 and sy = 10.09, which are similar. The
boxplots also indicate that the variabilities of the two samples are roughly the same, so that an
experimenter may decide to use a pooled variance analysis, although the general procedure is
also appropriate.
In order to assess the evidence that the new treatment is better than the standard treatment
(that is, μA < μB), the one-sided hypothesis testing problem
H0 : μA ≥μB
versus
HA : μA < μB
is considered. For the general (unpooled) procedure, the appropriate degrees of freedom are
ν =

11.402
15
+ 10.092
15
2
11.404
152×14 + 10.094
152×14
= 27.59
which can be rounded down to ν = 27. The t-statistic is
t = 47.47 −51.20

11.402
15
+ 10.092
15
= −0.949
so that the one-sided p-value is
p-value = P(X < −0.949) = 0.175
where the random variable X has a t-distribution with ν = 27 degrees of freedom.
Table III shows that t0.01,27 = 2.473, so that the 99% one-sided conﬁdence interval is
μA −μB ∈

−∞, 47.47 −51.20 + 2.473

11.402
15
+ 10.092
15

= (−∞, 5.99)
For the pooled variance analysis, the appropriate degrees of freedom are ν = n +m −2 =
28. The pooled variance estimate is
s2
p = (14 × 11.402) + (14 × 10.092)
28
= 115.88
so that the pooled standard deviation is sp =
√
115.88 = 10.76. In this case the t-statistic is
t = 47.47 −51.20
10.76

1
15 + 1
15
= −0.946
which is almost the same as in the unpooled case. The p-value is
p-value = P(X < −0.946) = 0.175
where the random variable X has a t-distribution with v = 28 degrees of freedom. Also,
the pooled variance analysis provides a 99% one-sided conﬁdence interval μA −μB ∈
(−∞, 5, 97).
Both the unpooled and pooled analyses report a p-value of 0.175, so that the experimenter
must conclude that there is insufﬁcient evidence to reject the null hypothesis or, in other words,
that there is insufﬁcient evidence to establish that the new treatment is any better on average
than the standard treatment. The one-sided 99% conﬁdence intervals indicate that the standard

9.3 ANALYSIS OF INDEPENDENT SAMPLES
413
treatment may be up to six points better than the new treatment, which again conﬁrms that it
is impossible to conclude that the new treatment is any better than the standard treatment.
It is worthwhile to reﬂect a moment on what the statistical analysis has achieved. An
experimenter unversed in statistical inference matters may observe that ¯y > ¯x and claim that
the new treatment has been shown to be better than the standard treatment. However, with the
statistical knowledge that we have gained so far we can see that this claim is not justiﬁable.
Our comparison of the difference in the sample averages with the sample variabilities, which
results in a large p-value, leads us to conclude that the difference observed can be attributed
to randomness rather than to a real difference in the treatments, and so we realize that there is
no real evidence of a treatment difference.
Example 52
Kaolin Processing
Figure 9.22 contains the brightness measurements of the processed kaolin from the two cal-
ciners. The two samples are independent, not paired.
Figure 9.23 contains summary statistics of the two samples and boxplots drawn to the same
scale. The boxplots indicate that the brightness measurements appear to be slightly higher on
average for calciner B and that the variability in the measurements may be slightly smaller
for calciner B than for calciner A. The sample averages are ¯x = 91.558 and ¯y = 92.500, and
the sample standard deviations are sx = 2.323 and sy = 1.563.
Whether the difference in population means suggested by the data is really statistically
signiﬁcant is investigated by the following statistical analysis for the two-sided hypotheses
H0 : μA = μB
versus
HA : μA ̸= μB
For this problem it would appear to be safest to use the general procedure with degrees of
freedom
ν =

2.3232
12
+ 1.5632
12
2
2.3234
122×11 + 1.5634
122×11
≃19.3
Calciner A
xi
88.4
93.2
87.4
94.3
93.0
94.3
89.0
90.5
90.8
93.1
92.8
91.9
Calciner B
yi
92.6
93.2
89.2
94.8
93.3
94.0
93.2
91.7
91.5
92.0
90.7
93.8
FIGURE 9.22
Kaolin processing data set (brightness
measurements)
Data
Calciner B
Calciner A
95
94
93
92
91
90
89
88
87
Calciner A
Sample size = 12
Sample mean = 91.558
Sample standard deviation = 2.323
Calciner B
Sample size = 12
Sample mean = 92.500
Sample standard deviation = 1.563
FIGURE 9.23
Descriptive statistics and boxplots for kaolin processing data set

414
CHAPTER 9
COMPARING TWO POPULATION MEANS
which can be rounded down to ν = 19. The t-statistic is
t = 91.558 −92.500

2.3232
12
+ 1.5632
12
= −1.165
so that the two-sided p-value is
p-value = 2 × P(X > 1.165) = 0.258
where the random variable X has a t-distribution with 19 degrees of freedom. Consequently,
the experimenter concludes that there is not sufﬁcient evidence to establish a difference in the
average brightness of the kaolin processed by the two calciners.
Example 53
Kudzu Pulping
Figure 9.24 contains the percentage yield measurements xi for the n = 20 kudzu pulpings
without the addition of anthraquinone and the percentage yield measurements yi for the m =
25 kudzu pulpings with the addition of anthraquinone. The boxplots and summary statistics
shown in Figure 9.25 clearly suggest that the addition of anthraquinone increases average
yield. The sample averages are ¯x = 38.55 and ¯y = 44.17, and the sample standard deviations
are sx = 3.627 and sy = 3.994. The closeness of the two-sample standard deviations suggests
that a pooled variance analysis may be acceptable.
Without
With
anthraquinone
anthraquinone
xi
yi
39.7
43.5
42.4
41.6
34.6
47.9
35.6
39.0
40.6
48.9
41.0
49.2
37.9
46.2
30.2
49.5
44.5
50.3
43.0
37.6
36.0
41.0
35.7
40.4
38.9
47.4
38.2
48.3
39.8
49.4
40.3
44.4
35.7
42.0
41.3
41.0
42.2
38.5
33.5
39.4
42.6
46.9
46.0
42.3
41.2
FIGURE 9.24
Kudzu pulping data set (percentage
yield measurements)
Data
With
anthraquinone
Without
anthraquinone
50
45
40
35
30
Without anthraquinone
Sample size = 20
Sample mean = 38.55
Sample standard deviation = 3.627
With anthraquinone
Sample size = 25
Sample mean = 44.17 
Sample standard deviation = 3.994
FIGURE 9.25
Summary statistics and boxplots for kudzu pulping experiment

9.3 ANALYSIS OF INDEPENDENT SAMPLES
415
In order to assess whether or not there is sufﬁcient evidence to establish that the addition
of anthraquinone increases yield, the one-sided hypothesis testing problem
H0 : μA ≥μB
versus
HA : μA < μB
is considered, where μA is the average yield without anthraquinone and μB is the average
yield with anthraquinone.
With a pooled variance estimate
s2
p = (19 × 3.6272) + (24 × 3.9942)
43
= 14.72
so that sp =
√
14.72 = 3.836, the t-statistic is
t = 38.55 −44.17
3.836

1
20 + 1
25
= −4.884
The p-value is therefore
p-value = P(X < −4.884)
where the random variable X has a t-distribution with ν = n + m −2 = 43 degrees of
freedom, which is less than 0.0001. The experimenter can therefore conclude that there is
sufﬁcient evidence to establish that the addition of anthraquinone does result in an increase
in average yield.
With a critical point t0.01,43 = 2.4163, a one-sided 99% conﬁdence interval for the differ-
ence in the average yields is
μA −μB ∈

−∞, ¯x −¯y + tα,n+m−2sp

1
n + 1
m

=

−∞, 38.55 −44.17 + 2.4163 × 3.836 ×

1
20 + 1
25

= (−∞, −2.84)
Thus, the experimenter can conclude that the addition of anthraquinone increases the average
yield by at least 2.8%.
Nerve disorder
subjects
yi
50.68
47.49
51.47
48.47
52.50
48.55
45.96
50.40
45.07
48.21
50.06
50.63
44.99
47.22
48.71
49.64
47.09
48.73
45.08
45.73
44.86
50.18
52.65
48.50
47.93
47.25
53.98
Healthy
subjects
xi
52.20
53.81
53.68
54.47
54.65
52.43
54.43
54.06
52.85
54.12
54.17
55.09
53.91
52.95
54.41
54.14
55.12
53.35
54.40
53.49
52.52
54.39
55.14
54.64
53.05
54.31
55.90
52.23
54.90
55.64
54.48
52.89
FIGURE 9.26
Nerve conductivity speeds data set
(conductivity speeds in m/s)
Example 54
Nerve Conductivity
Speeds
Figure 9.26 shows the data observations of nerve conductivity speeds, and Figure 9.27 shows
boxplots and summary statistics of the data set. The boxplots suggest that the patients with
a nerve disorder have slower conductivity speeds on average and more variability in their
conductivity speeds. The sample averages are ¯x = 53.994 and ¯y = 48.594, and the sample
standard deviations are sx = 0.974 and sy = 2.490.
A pooled variance analysis is clearly not appropriate here, and so the general procedure
must be employed. Figure 9.28 shows the details of the statistical analysis, and the very
small p-value indicates that the data set provides sufﬁcient evidence to establish that the
nerve conductivity speeds are different for the two groups of subjects. The 99% two-sided
conﬁdence interval for the difference in population means is
μA −μB ∈(4.01, 6.79)
and consequently the experimenter can conclude that a periphery nerve disorder reduces the
average nerve conductivity by somewhere between 4.0 and 6.8 m/s. The experimenter should

416
CHAPTER 9
COMPARING TWO POPULATION MEANS
FIGURE 9.27
Descriptive statistics and boxplots
for the nerve conductivity
speeds data set
Data
Nerve disorder
subjects
Healthy subjects
57.5
55.0
52.5
50.0
47.5
45.0
Healthy subjects
Sample size = 32
Sample mean = 53.994
Sample standard deviation = 0.974
Nerve disorder subjects
Sample size = 27
Sample mean = 48.594 
Sample standard deviation = 2.490
FIGURE 9.28
Nerve conductivity speeds analysis
Stage I: Data Summary
Healthy subjects:
sample average n = 32, sample mean ¯x = 53.994, sample standard deviation sx = 0.974
Nerve disorder subjects:
sample average m = 27, sample mean ¯y = 48.594, sample standard deviation sy = 2.490
Stage II: Determination of Suitable Hypotheses
Question: Is there sufﬁcient evidence to establish that the nerve conductivity speeds
are different for the two groups of subjects?
Use a two-sided procedure: H0 : μA = μB versus HA : μA ̸= μB.
Stage III: Determination of a Suitable Test Procedure
The sample standard deviations sx = 0.974 and sy = 2.490 are very different
so use the general (unequal variances) procedure.
Stage IV: Degrees of freedom

0.9742
32
+ 2.4902
27
2
0.9744
322×31 + 2.4904
272×26
= 32.69
Use ν = 32 degrees of freedom.
Stage IV: Calculation of the Test Statistic
t =
53.994−48.594

0.9742
32
+ 2.4902
27
= 10.61
Stage VI: Expression for the p-value
p-value = 2 × P(X ≥10.61) ≃0
where the random variable X has a t-distribution with 32 degrees of freedom.
Stage VII: Decision
Since the p-value is less than 0.01, the null hypothesis is rejected.
Stage VIII: Conclusion
This data set provides sufﬁcient evidence to establish that the average nerve conductivity
speeds are different for healthy subjects and for nerve disorder subjects.
Stage IX: Conﬁdence Interval
Question: How different are the two groups?
Critical point: t0.005,32 = 2.738
99% conﬁdence interval:
μA −μB ∈53.994 −48.594 ± 2.738

0.9742
32
+ 2.4902
27
= (4.01, 6.79)

9.3 ANALYSIS OF INDEPENDENT SAMPLES
417
also take note of the fact that the variability in conductivity speeds is greater for the nerve
disorder subjects than for the healthy subjects.
Example 45
Fabric Water
Absorption Properties
The % pickup values for the n = 15 cases with a 10 pounds per square inch pressure and the
m = 15 cases with a 20 pounds per square inch pressure are given in Figure 9.29. Figure 9.30
shows boxplots and summary statistics for this data set. The boxplots indicate a similarity in
the variability of the two samples, but with higher water contents at the lower pressure. The
sample averages are ¯x = 59.807 and ¯y = 48.560, and the sample standard deviations are
sx = 4.943 and sy = 4.991.
10 lb/in2
xi
51.8
61.8
57.3
54.5
64.0
59.5
61.2
64.9
54.5
70.2
59.1
55.8
65.4
60.4
56.7
20 lb/in2
yi
55.6
44.6
46.7
45.8
49.9
51.9
44.1
52.3
51.0
39.9
51.6
42.5
45.5
58.0
49.0
FIGURE 9.29
Fabric water absorption properties
data set (% water pickup)
A pooled variance analysis seems reasonable here, and the pooled variance estimate is
s2
p = (14 × 4.9432) + (14 × 4.9912)
28
= 24.67
so that sp =
√
24.67 = 4.967. This gives a t-statistic of
t = 59.807 −48.560
4.967

1
15 + 1
15
= 6.201
A two-sided p-value for the null hypothesis that the population means are equal is therefore
p-value = 2 × P(X > 6.201) ≃0
where the random variable X has a t-distribution with degrees of freedom n + m −2 = 28.
Hence the experiment has established that changing the pressure does affect the average water
absorption.
With a critical point t0.005,28 = 2.763 obtained from Table III, a 99% two-sided conﬁdence
interval for the difference in the population means can be calculated as
μA −μB ∈

59.807 −48.560 −2.763 × 4.967 ×

1
15 + 1
15,
59.807 −48.560 + 2.763 × 4.967 ×

1
15 + 1
15

= (6.24, 16.26)
The experimenter can therefore conclude that increasing the pressure between the rollers from
10 pounds per square inch to 20 pounds per square inch decreases the average water pickup
by somewhere between about 6.2% and 16.3%.
FIGURE 9.30
Boxplots and summary statistics
for the fabric water absorption
properties data set
Data
20-pounds pressure
10-pounds pressure
70
65
60
55
50
45
40
10-pounds pressure
Sample size = 15
Sample mean = 59.807
Sample standard deviation = 4.943
20-pounds pressure
Sample size = 15
Sample mean = 48.560
Sample standard deviation = 4.991

418
CHAPTER 9
COMPARING TWO POPULATION MEANS
9.3.5
Sample Size Calculations
The determination of appropriate sample sizes n and m, or an assessment of the precision
afforded by given sample sizes, is most easily performed by considering the length of a two-
sided conﬁdence interval for the difference in population means μA −μB. For the general
procedure, this conﬁdence interval length is
L = 2 tα/2,ν

s2x
n +
s2y
m
To estimate the sample sizes that are required to obtain a conﬁdence interval of length no
larger than L0, an experimenter needs to use estimated values of the population variances σ 2
A
and σ 2
B (or at least upper bounds on these variances). The experimenter can then estimate that
the sample sizes n and m are adequate as long as
L0 ≥2 tα/2,ν

σ 2
A
n + σ 2
B
m
where a suitable value of the critical point tα/2,ν can be used, depending on the conﬁdence
level 1 −α required. If an experimenter wishes to have equal sample sizes n = m, then this
inequality can be rewritten
n = m ≥
4 t2
α/2,ν

σ 2
A + σ 2
B

L2
0
For example, suppose that an experimenter wishes to construct a 99% conﬁdence interval
with a length no larger than L0 = 5.0 mm for the difference between the mean thicknesses
of two types of plastic sheets. Previous experience suggests that the thicknesses of sheets of
type A have a standard deviation of no more than σA = 4.0 mm and that the thicknesses of
sheets of type B have a standard deviation of no more than σB = 2.0 mm. It can be seen from
Table III that as long as ν ≥30, the critical point t0.005,ν is no larger than 2.75, so that it can
be estimated that equal sample sizes
n = m ≥4 × 2.752 × (4.02 + 2.02)
5.02
= 24.2
will sufﬁce. Therefore, a random sample of 25 sheets of each type would appear to be sufﬁcient
to meet the experimenter’s goal.
Similar calculations can also be employed to ascertain the additional sampling required
to reduce the length of a conﬁdence interval calculated from initial samples from the two
populations. In this case the critical point tα/2,ν used in the initial analysis and the sample
standard deviations sx and sy of the initial samples can be used in the sample size formula.
This method is illustrated in the following example.
Example 45
Fabric Water
Absorption Properties
Recall that a 99% conﬁdence interval for the difference in the average percentage water pickup
at the two pressure levels was calculated to be
μA −μB ∈(6.24, 16.26)
based upon samples with n = m = 15. This interval has a length of over 10%. How much
additional sampling is required if the experimenter wants a 99% conﬁdence interval with a
length no larger than L0 = 4%?
The initial samples have sample standard deviations of sx = 4.943 and sy = 4.991, and
the (pooled) analysis of the initial samples uses a critical point t0.005,28 = 2.763. Using these

9.3 ANALYSIS OF INDEPENDENT SAMPLES
419
values in the formula
n = m ≥
4 t2
α/2,ν

σ 2
A + σ 2
B

L2
0
in place of the critical point and the population variances gives
n = m ≥4 × 2.7632 × (4.9432 + 4.9912)
4.02
= 94.2
Consequently, to meet the speciﬁed goal the experimenter can estimate that total sample sizes
of n = m = 95 will sufﬁce, so that additional sampling of 80 observations from each of the
two pressure levels is required.
9.3.6
Problems
9.3.1 An experimenter wishes to compare two treatments A and
B and obtains some data observations xi using treatment
A and some data observations yi using treatment B. It
turns out that ¯x > ¯y and so the experimenter concludes
that treatment A results in larger data values on average
than treatment B. How do you feel about the
experimenter’s conclusion? What other information
would you like to know?
9.3.2 In an unpaired two-sample problem an experimenter
observes n = 14, ¯x = 32.45, sx = 4.30 from population
A and m = 14, ¯y = 41.45, sy = 5.23 from population B.
(a) Use the pooled variance method to construct a 99%
two-sided conﬁdence interval for μA −μB.
(b) Construct a 99% two-sided conﬁdence interval for
μA−μB without assuming equal population variances.
(c) Consider a two-sided hypothesis test of H0 : μA =
μB without assuming equal population variances.
Does a size α = 0.01 test accept or reject the null
hypothesis? Write down an expression for the exact
p-value.
(This problem is continued in Problem 9.3.14.)
9.3.3 In an unpaired two-sample problem an experimenter
observes n = 8, ¯x = 675.1, sx = 44.76 from population
A and m = 17, ¯y = 702.4, sy = 38.94 from population B.
(a) Use the pooled variance method to construct a 99%
two-sided conﬁdence interval for μA −μB.
(b) Construct a 99% two-sided conﬁdence interval for
μA −μB without assuming equal population
variances.
(c) Consider a two-sided hypothesis test of H0 : μA =
μB using the pooled variance method. Does a size
α = 0.01 test accept or reject the null hypothesis?
Write down an expression for the exact p-value.
9.3.4 In an unpaired two-sample problem an experimenter
observes n = 10, ¯x = 7.76, sx = 1.07 from population A
and m = 9, ¯y = 6.88, sy = 0.62 from population B.
(a) Construct a 99% one-sided conﬁdence interval
μA −μB ∈(c, ∞) without assuming equal
population variances.
(b) Does the value of c increase or decrease if a
conﬁdence level 95% is used?
(c) Consider a one-sided hypothesis test of
H0 : μA ≤μB versus HA : μA > μB without
assuming equal population variances. Does a size
α = 0.01 test accept or reject the null hypothesis?
Write down an expression for the exact p-value.
9.3.5 In an unpaired two-sample problem an experimenter
observes n = 13, ¯x = 0.0548, sx = 0.00128 from
population A and m = 15, ¯y = 0.0569, sy = 0.00096
from population B.
(a) Construct a 95% one-sided conﬁdence interval
μA −μB ∈(−∞, c) using the pooled variance
method.
(b) Consider a one-sided hypothesis test of
H0 : μA ≥μB versus HA : μA < μB using the
pooled variance method. Does a size α = 0.05 test
accept or reject the null hypothesis? What about a
size α = 0.01 test? Write down an expression for the
exact p-value.
9.3.6 The thicknesses of n = 41 glass sheets made using
process A are measured and the statistics ¯x = 3.04 mm
and sx = 0.124 mm are obtained. In addition, the
thicknesses of m = 41 glass sheets made using process B
are measured and the statistics ¯y = 3.12 mm and
sy = 0.137 mm are obtained. Use a pooled variance
procedure to answer the following questions.

420
CHAPTER 9
COMPARING TWO POPULATION MEANS
(a) Does a two-sided hypothesis test with size α = 0.01
accept or reject the null hypothesis that the two
processes produce glass sheets with equal thicknesses
on average?
(b) What is a two-sided 99% conﬁdence interval for the
difference in the average thicknesses of sheets
produced by the two processes?
(c) Is there enough evidence to conclude that the average
thicknesses of sheets produced by the two processes
are different?
(This problem is continued in Problems 9.3.15 and
9.7.11.)
9.3.7 The breaking strengths of n = 20 bundles of wool ﬁbers
have a sample mean ¯x = 436.5 and a sample standard
deviation sx = 11.90. In addition, the breaking strengths
of m = 25 bundles of synthetic ﬁbers have a sample mean
¯y = 452.8 and a sample standard deviation sy = 4.61.
Answer the following questions without assuming that
the two population variances are equal.
(a) Does a one-sided hypothesis test with size α = 0.01
accept or reject the null hypothesis that the synthetic
ﬁber bundles have an average breaking strength no
larger than the wool ﬁber bundles?
(b) What is a one-sided 99% conﬁdence interval that
provides an upper bound on μA −μB, where μA is
the average breaking strength of wool ﬁber bundles
and μB is the average breaking strength of synthetic
ﬁber bundles?
(c) Is there enough evidence to conclude that the average
breaking strength of synthetic ﬁber bundles is larger
than the average breaking strength of wool ﬁber
bundles?
(This problem is continued in Problem 9.7.12.)
9.3.8 A random sample of n = 16 one-kilogram sugar
packets of brand A have weights with a sample mean
¯x = 1.053 kg and a sample standard deviation
sx = 0.058 kg. In addition, a random sample of m = 16
one-kilogram sugar packets of brand B have weights with
a sample mean ¯y = 1.071 kg and a sample standard
deviation sy = 0.062 kg. Is it safe to conclude that brand
B sugar packets weigh slightly more on average than
brand A sugar packets?
9.3.9 In an unpaired two-sample problem, an experimenter
observes n = 47, ¯x = 100.85 from population A and
m = 62, ¯y = 89.32 from population B. Suppose that the
experimenter wishes to use values σA = 25 and σB = 20
for the population standard deviations.
(a) What is the exact p-value for the hypothesis testing
problem H0 : μA = μB + 3.0 versus
HA : μA ̸= μB + 3.0?
(b) Construct a 90% two-sided conﬁdence interval for
μA −μB.
9.3.10 In an unpaired two-sample problem, an experimenter
observes n = 38, ¯x = 5.782 from population A and
m = 40, ¯y = 6.443 from population B. Suppose that the
experimenter wishes to use values σA = σB = 2.0 for the
population standard deviations.
(a) What is the exact p-value for the hypothesis testing
problem H0 : μA ≥μB versus HA : μA < μB?
(b) Construct a 99% one-sided conﬁdence interval that
provides an upper bound for μA −μB.
9.3.11 The resilient moduli of n = 10 samples of a clay mixture
of type A are measured and the sample mean is ¯x = 19.50.
In addition, the resilient moduli of m = 12 samples
of a clay mixture of type B are measured and the sample
mean is ¯y = 18.64. Suppose that the experimenter wishes
to use values σA = σB = 1.0 for the standard deviations
of the resilient modulus of the two types of clay.
(a) What is the exact two-sided p-value for the null
hypothesis that the two types of clay have equal
average values of resilient modulus?
(b) Construct 90%, 95%, and 99% two-sided conﬁdence
intervals for the difference between the average
resilient modulus of the two types of clay.
9.3.12 An experimenter feels that observations from population
A have a standard deviation no larger than 10.0 and that
observations from population B have a standard deviation
no larger than 15.0. If the experimenter wants a two-sided
99% conﬁdence interval for the difference in population
means with a length no larger than L0 = 10.0, what
(equal) sample sizes would you recommend be obtained
from the two populations?
9.3.13 An experimenter would like to construct a two-sided 95%
conﬁdence interval for the difference between the average
resistance of two types of copper cable with a length no
larger than L0 = 1.0 ohms. If the experimenter feels that
the standard deviations of the resistances of either type of
cable are no larger than 1.2 ohms, what (equal) sample
sizes would you recommend be obtained from the two
types of copper cable?
9.3.14 Consider again the data set in Problem 9.3.2 with sample
sizes n = m = 14. If a two-sided 99% conﬁdence interval
for the difference in population means is required with a

9.3 ANALYSIS OF INDEPENDENT SAMPLES
421
length no larger than L0 = 5.0, what additional sample
sizes would you recommend be obtained from the two
populations?
9.3.15 Consider again the data set of glass sheet thicknesses in
Problem 9.3.6 with sample sizes n = m = 41. If a
two-sided 99% conﬁdence interval for the difference in
the average thickness of glass sheets produced by the
two processes is required with a length no larger than
L0 = 0.1 mm, how many additional glass sheets from the
two processes do you think need to be sampled?
9.3.16 An experiment was conducted to investigate how the
corrosion properties of chilled cast iron depend upon the
chromium content of the alloy. A collection of n = 12
samples of chilled cast iron with 0.1% chromium content
provided corrosion rates with a sample mean of ¯x = 2.462
and a sample standard deviation of sx = 0.315, while a
collection of m = 13 samples of chilled cast iron with
0.2% chromium content provided corrosion rates with a
sample mean of ¯y = 2.296 and a sample standard
deviation of sy = 0.297.
(a) Conduct a hypothesis test to investigate whether there
is any evidence that the chromium content has an
effect on the corrosion rate of chilled cast iron.
(b) Construct a 99% two-sided conﬁdence interval for
the difference between the average corrosion rates of
chilled cast iron at the two chromium contents.
9.3.17 Paving Slab Weights
Recall that DS 6.1.7 shows the weights of a sample of
paving slabs from a certain company, manufacturer A say.
In addition, DS 9.3.1 shows the weights of a sample of
paving slabs from another company, manufacturer B.
Is there evidence of any difference in the paving slab
weights between the two manufacturers?
9.3.18 Spray Painting Procedure
An engineer compares the sample of paint thicknesses in
DS 6.1.8 from production line A with a sample of paint
thicknesses in DS 9.3.2 from production line B. What
conclusions should the engineer draw?
9.3.19 Heel-Strike Force on a Treadmill
Physical disorders commonly experienced by
long-distance runners are often related to large vertical
reaction ground forces, and the minimization of such
forces is the goal of much of the current research in sports
biomechanics. DS 9.3.3 contains measurements of the
heel-strike force, in newtons, of a particular runner on a
standard treadmill and on a treadmill with a damped
feature activated. Is the damped feature effective in
reducing the heel-strike force?
9.3.20 Bleaching Agents
In the garment industry, bleaching
is an important component of the manufacturing
process. Chlorine bleach is very effective, but it can be
unsatisfactory for environmental reasons. Consequently,
various alternative bleaches such as hydrogen peroxide
have been investigated. DS 9.3.4 contains the results of an
experiment to compare the bleaching effectiveness of two
levels of hydrogen peroxide, a low level and a high level.
The data values are the whiteness levels, calculated
from color measurements readings, for various samples
of garments bleached with the hydrogen peroxide.
What conclusions would you draw from this data set?
9.3.21 Restaurant Service Times
Recall that DS 6.1.4 shows the service times of customers
at a fast-food restaurant who were served between 2:00
and 3:00 on a Saturday afternoon. In addition, DS 9.3.5
shows the service times of customers at the fast-food
restaurant who were served between 9:00 and 10:00 in the
morning on the same day. What do these data sets tell us
about the difference between the service times at these
two times of day?
9.3.22 The breaking strengths of 14 randomly selected objects
produced from a standard procedure had a mean of 56.43
and a standard deviation of 6.30. In addition, the breaking
strengths of 20 randomly selected objects produced from
a new procedure had a mean of 62.11 and a standard
deviation of 7.15. Perform a hypothesis test to investigate
whether there is sufﬁcient evidence to conclude that the
new procedure has a larger breaking strength on average
than the standard procedure.
9.3.23 Clinical Trial
A simple clinical trial was performed to compare two
medicines. A total of 20 patients were obtained, and they
were randomly split into two groups of 10 patients each.
One group received medicine A, and the other group
received medicine B. The patients’ responses are given
in DS 9.3.6. Does this experiment provide sufﬁcient
evidence to conclude that on average medicine A
provides a higher response than medicine B?
9.3.24 An athlete recorded her practice times running a course.
She had eight times recorded when she ran in the morning
and these had a sample average of 132.52 minutes with a
standard deviation of 1.31 minutes. In addition, she had

422
CHAPTER 9
COMPARING TWO POPULATION MEANS
ten times recorded when she ran in the afternoon and
these had a sample average of 133.87 minutes with a
standard deviation of 1.72 minutes.
(a) Is there sufﬁcient evidence to conclude that the morn-
ing and afternoon are any different on average? Use an
appropriate hypothesis test to investigate this question.
(b) Construct a two-sided 99% conﬁdence interval for the
difference between the average run time in the morning
and the average run time in the afternoon.
9.3.25 A random sample of 10 observations from population A
has a sample mean of 152.30 and a sample standard
deviation of 1.83. A random sample of 8 observations
from population B has a sample standard deviation of
1.94. If the p-value for the one-sided hypothesis test with
an alternative hypothesis HA : μA > μB is less than 1%,
what can you say about the sample mean of the
observations from population B?
9.3.26 For a two-sample problem with independent samples,
n = 6, m = 8, ¯x = 5.42, ¯y = 4.38, sx = 1.84, sy = 2.02,
and ν = 11. A 99% one-sided conﬁdence interval
provides a lower bound for μA −μB of:
A. −2.78
B. −2.28
C. −1.78
D. −1.28
E. −0.78
9.4
Summary
The ﬁrst question in the design and analysis of a two-sample problem is whether it should
be a paired problem or an unpaired problem. If an extraneous source of variability can be
identiﬁed, it is appropriate to design a paired experiment when it is possible to do so. The
analysis of a paired two-sample problem simpliﬁes to a one-sample problem when differences
are taken in the data observations within each pairing.
In many situations either there is no reason to design a paired experiment or a paired
design is not feasible for the speciﬁc problem at hand. In these cases the experimenter has two
independent or unpaired samples that may have unequal sample sizes. It is still very important
to employ good experimental practices in the collection of these data sets, such as the random
allocation of experimental subjects between the two treatments and the employment of blind
or double-blind conditions where possible.
Two independent samples can be analyzed with two-sample t-tests or two-sample
z-tests. The two-sample t-tests can be used without pooling the variances or with a pooled vari-
ance estimate. These two t-procedures are summarized in Figures 9.31 and 9.32. The pooled
variance procedure assumes that the population variances σ 2
A and σ 2
B are equal, whereas the
general procedure makes no assumptions about the population variances. It is usually safest
to use the general procedure, although if the population variances are close, then the pooled
variance approach may allow a slightly more accurate analysis.
The two-sample z-test is summarized in Figure 9.33, and it employs “known” values of
the population variances σ 2
A and σ 2
B. As with one-sample problems, it can also be thought of
as the limiting value of the two-sample t-tests as the sample sizes n and m increase. In this
sense it is often referred to as a large-sample procedure.
The two-sample procedures can be applied to either two-sided or one-sided problems and
can be used to construct conﬁdence intervals, to calculate p-values, or to perform hypothesis
tests at a ﬁxed size. As with one-sample procedures, two-sample procedures are based on the
assumption that the data are normally distributed, although if the sample sizes are large enough,
the central limit theorem ensures that the procedures are applicable. For small sample sizes
and distributions that are evidently not normally distributed, the nonparametric procedures
discussed in Chapter 15 provide a method of analysis.
Sample size calculations are most easily performed by considering the length that the
sample sizes afford for a two-sided conﬁdence interval for the difference in population means
μA −μB. This assessment requires estimates of (or upper bounds on) the population variances
σ 2
A and σ 2
B. If follow-up studies are being planned, then the sample variances from the initial
samples can be used as these estimates.

9.4 SUMMARY
423
Two-sample t-procedure – general procedure
(sample sizes n, m ≥30 or small sample sizes with normally distributed data)
ν =

s2xn +
s2y
m
2
s4x
n2(n−1) +
s4y
m2(m−1)
One-sided
Two-sided
One-sided
1 −α level Conﬁdence Intervals

−∞, ¯x −¯y + tα,ν

s2x
n +
s2y
m


¯x −¯y −tα/2,ν

s2x
n +
s2y
m , ¯x −¯y + tα/2,ν

s2x
n +
s2y
m


¯x −¯y −tα,ν

s2x
n +
s2y
m , ∞

Hypothesis Testing: test statistic t =
¯x−¯y−δ

s2xn +
s2y
m
; X ∼tν
H0: μA −μB ≥δ, HA: μA −μB < δ
H0: μA −μB = δ, HA: μA −μB ̸= δ
H0: μA −μB ≤δ, HA: μA −μB > δ
p-value = P(X < t)
p-value = 2 × P(X > |t|)
p-value = P(X > t)
Size α hypothesis tests
accept H0
reject H0
accept H0
reject H0
accept H0
reject H0
t ≥−tα,ν
t < −tα,ν
|t| ≤tα/2,ν
|t| > tα/2,ν
t ≤tα,ν
t > tα,ν
FIGURE 9.31
Summary of the general two-sample t-procedure
Finally, it should be remembered that the inference procedures discussed above are de-
signed to investigate location differences between the two population probability distributions,
that is, differences between the population means μA and μB. However, it is also important to
notice whether there appears to be a difference between the two population variances σ 2
A and
σ 2
B. A test procedure for comparing two variances is described in the Supplementary Problems
section, but it relies heavily on the normality of the population distributions and its widespread
use is not recommended. In practice, it is sensible and often adequate for the experimenter to
compare boxplots or histograms of the two samples, and to notice whether or not there is any
obvious difference between the sample variabilities.

424
CHAPTER 9
COMPARING TWO POPULATION MEANS
Two-sample t-procedure – pooled variance procedure
(sample sizes n, m ≥30 or small sample sizes with normally distributed data)
Assumption: σ 2
A = σ 2
B
s2
p =
(n−1)s2x +(m−1)s2y
n+m−2
One-sided
Two-sided
One-sided
1 −α level Conﬁdence Intervals

−∞, ¯x −¯y + tα,n+m−2sp

1
n + 1
m


¯x −¯y −tα/2,n+m−2sp

1
n + 1
m , ¯x −¯y + tα/2,n+m−2sp

1
n + 1
m


¯x −¯y −tα,n+m−2sp

1
n + 1
m , ∞

Hypothesis Testing: test statistic t =
¯x−¯y−δ
sp

1
n + 1
m
; X ∼tn+m−2
H0: μA −μB ≥δ, HA: μA −μB < δ
H0: μA −μB = δ, HA: μA −μB ̸= δ
H0: μA −μB ≤δ, HA: μA −μB > δ
p-value = P(X < t)
p-value = 2 × P(X > |t|)
p-value = P(X > t)
Size α hypothesis tests
accept H0
reject H0
accept H0
reject H0
accept H0
reject H0
t ≥−tα,n+m−2
t < −tα,n+m−2
|t| ≤tα/2,n+m−2
|t| > tα/2,n+m−2
t ≤tα,n+m−2
t > tα,n+m−2
FIGURE 9.32
Summary of the pooled variance two-sample t-procedure
9.5
Case Study: Microelectronic Solder Joints
The researcher examines an assembly with 16 solder joints that was made using the original
method for depositing nickel on the bond pads, and measures the nickel layer thicknesses.
This new data set is shown in Figure 9.34, and the sample size is m = 16, the sample average
is ¯y = 2.7981 microns, and the sample standard deviation is sy = 0.0256 microns.
The sample average ¯y = 2.7981 microns of the nickel layer thicknesses for the assembly
prepared using the original method (assembly 2) is larger than the sample average ¯x = 2.7688
microns of the nickel layer thicknesses for the assembly prepared using the new method
(assembly 1) that is given in Figure 6.40, and the boxplots in Figure 9.35 conﬁrm that the

9.5 CASE STUDY: MICROELECTRONIC SOLDER JOINTS
425
Two-sample z-procedure
(sample sizes n, m ≥30 or small sample sizes with normally distributed data; variances known)
One-sided
Two-sided
One-sided
1 −α level Conﬁdence Intervals

−∞, ¯x −¯y + zα

σ2
A
n +
σ2
B
m


¯x −¯y −zα/2

σ2
A
n +
σ2
B
m , ¯x −¯y + zα/2

σ2
A
n +
σ2
B
m


¯x −¯y −zα

σ2
A
n +
σ2
B
m , ∞

Hypothesis Testing: test statistic z =
¯x−¯y−δ

σ2
A
n +
σ2
B
m
H0: μA −μB ≥δ, HA: μA −μB < δ
H0: μA −μB = δ, HA: μA −μB ̸= δ
H0: μA −μB ≤δ, HA: μA −μB > δ
p-value = (z)
p-value = 2 × (−|z|)
p-value = 1 −(z)
Size α hypothesis tests
accept H0
reject H0
accept H0
reject H0
accept H0
reject H0
z ≥−zα
z < −zα
|z| ≤zα/2
|z| > zα/2
z ≤zα
z > zα
FIGURE 9.33
Summary of the two-sample z-procedure
FIGURE 9.34
Data set of nickel layer thicknesses
on substrate bond pads for
assembly 2
2.78
2.77
2.79
2.78
2.81
2.79
2.83
2.75
2.82
2.82
2.85
2.80
2.77
2.80
2.82
2.80
FIGURE 9.35
Boxplots of the nickel layer
thickness for the new method
(assembly 1) and the original
method (assembly 2)
Data
Assembly 2
Assembly 1
2.72
2.74
2.76
2.78
2.80
2.82
2.84
2.86

426
CHAPTER 9
COMPARING TWO POPULATION MEANS
nickel layer thicknesses tend to be larger in assembly 2. The researcher decides to test whether
this difference is statistically signiﬁcant using the two-sided hypothesis test
H0 : μnew = μoriginal
versus
HA : μnew ̸= μoriginal
Using the general procedure, the appropriate degrees of freedom are found to be ν = 29 and
the test statistic is
t =
¯x −¯y

s2x
n + s2y
m
= 2.7688 −2.7981

0.02602
16
+ 0.02562
16
= −3.22
The p-value is therefore 2 × P(X ≥3.22) where the random variable X has a t-distribution
with 29 degrees of freedom, which is 0.003. Since the p-value is less than 1%, the null
hypothesis is rejected and the researcher can conclude that these data sets provide sufﬁcient
evidence to establish that the new method is providing a nickel layer with a smaller average
thickness than the original method.
Furthermore, with t0.005,29 = 2.756 a 99% two-sided conﬁdence interval for the difference
between the two means is
μnew −μoriginal ∈

2.7688 −2.7981 −2.756

0.02602
16
+ 0.02562
16
,
2.7688 −2.7981 + 2.756

0.02602
16
+ 0.02562
16

= (−0.055, −0.004)
and so the researcher can conclude that the difference between the average nickel layer thick-
nesses of the two methods is somewhere between 0.004 microns and 0.055 microns.
9.6
Case Study: Internet Marketing
Over a 2-week period, the organisation monitors how many visits to its website come each day
from two different search engines, as shown in Figure 9.36. These data can be analyzed with
a paired two-sample procedure, and the resulting p-value is very small, which indicates that
FIGURE 9.36
Data set of the number of website
visits from two search engines
Day 1
Day 2
Day 3
Day 4
Day 5
Day 6
Day 7
Day 8
Day 9
Day 10
Day 11
Day 12
Day 13
Day 14
85,851
78,942
75,501
63,412
80,069
73,136
66,731
74,831
78,616
80,672
73,083
75,744
57,580
61,014
Visits from
Search Engine 1
66,356
63,941
62,217
63,127
61,176
42,367
45,448
75,751
61,820
53,597
55,313
58,149
40,645
50,897
Visits from
Search Engine 2
19,495
15,001
13,284
    285
18,893
30,769
21,283
920
16,796
27,075
17,770
17,595
16,935
10,117
Difference
Day

9.7 SUPPLEMENTARY PROBLEMS
427
the difference between the two search engines is statistically signiﬁcant. The 95% conﬁdence
interval for the difference in the visits from the two search engines is
(11,033 , 21,021)
which shows how much more trafﬁc on average per day is being driven by search engine 1
than search engine 2.
9.7
Supplementary Problems
9.7.1 Video Display Designs
A researcher is interested in how a color video display
rather than a black-and-white video display can help a
person assimilate the information provided on a screen. A
set of 22 experimental subjects are used, and each person
undergoes ﬁve trials with a color display and ﬁve trials
with a black-and-white display. In each trial the subject
has to perform a task based upon information provided on
the screen, and the time taken to perform the task is
measured. For each of the 22 subjects, DS 9.7.1 shows the
average time in seconds taken to perform the ﬁve color
trials and the ﬁve black-and-white trials. Do the data
show that color displays are more effective than
black-and-white displays?
9.7.2 Fabric Water Absorption Properties
In assessing how the water absorption properties of cotton
fabric differ with roller pressures of 10 pounds per square
inch and 20 pounds per square inch, an experimenter
suspects that different fabric samples may have different
absorption properties. Therefore a paired experimental
design is adopted whereby 14 samples are split in half,
with one half being examined at one pressure and the
other half being examined at the other pressure. DS 9.7.2
contains the % pickup values obtained. Do the water
absorption properties of cotton fabric depend upon the
roller pressure?
9.7.3 A researcher in the petroleum industry is interested in the
sizes of wax crystals produced when wax dissolved in a
supercritical ﬂuid is sprayed through a capillary nozzle.
In the ﬁrst experiment, a pre-expansion temperature of
80◦C is employed and the diameters of n = 35 crystals
are measured with an electron microscope. A sample
mean of ¯x = 22.73 μm and a sample standard deviation
of sx = 5.20 μm are obtained. In the second experiment,
a pre-expansion temperature of 150◦C is employed and
the diameters of m = 35 crystals are measured, which
have a sample mean of ¯y = 12.66 μm and a sample
standard deviation of sy = 3.06 μm. The researcher
decides that it is not appropriate to assume that the
variances of the crystal diameters are the same under both
sets of experimental conditions.
(a) Write down an expression for the p-value of the
statement that the average crystal size does not
depend upon the pre-expansion temperature. Do you
think that this is a plausible statement?
(b) Construct a 99% two-sided conﬁdence interval for
the difference between the average crystal diameters
at the two pre-expansion temperatures.
(c) If a 99% two-sided conﬁdence interval for the
difference between the average crystal diameters at
the two pre-expansion temperatures is required with a
length no larger than L0 = 4.0 μm, how much
additional sampling would you recommend?
9.7.4 A company is investigating how long it takes its drivers to
deliver goods from its factory to a nearby port for export.
Records reveal that with a standard speciﬁed driving
route, the last n = 48 delivery times have a sample mean
of ¯x = 432.7 minutes and a sample standard deviation of
sx = 20.39 minutes. A new driving route is proposed, and
this has been tried m = 10 times with a sample mean of
¯y = 403.5 minutes and a sample standard deviation of
sy = 15.62 minutes. What is the evidence that the new
route is quicker on average than the standard route?
9.7.5 Bamboo Cultivation
A researcher compares the bamboo shoot heights in DS
6.7.5 obtained under growing conditions A with the
bamboo shoot heights in DS 9.7.3 obtained under
growing conditions B. In each case the bamboo shoot
heights are measured 40 days after planting, but growing
conditions B allow 10% more sunlight than growing
conditions A. Does the extra sunlight tend to increase the
bamboo shoot heights?
9.7.6 Consumer Complaints Division Reorganization
In a quality drive a food manufacturer reorganizes its

428
CHAPTER 9
COMPARING TWO POPULATION MEANS
consumer complaints division. Before the reorganization,
a study was conducted of the time that a consumer calling
the toll-free complaints line had to wait before speaking
to a company employee. After reorganization, a similar
follow-up study was conducted. DS 9.7.4 contains the
two samples of waiting times in seconds that were
recorded. Does the reorganization appear to have been
successful in affecting the time taken to answer calls?
9.7.7 Ocular Motor Measurements
Ocular motor measurements are designed to assess
the amount of contraction in the muscles around the
eyes. High ocular motor measurements may be indicative
of eyestrain, which may lead to spasms and headaches. A
group of ten subjects had their ocular motor measurements
recorded after they had been reading a book for
an hour and also after they had been reading a computer
screen for an hour. The results are listed in DS 9.7.5.
What conclusions can you draw from this data set?
9.7.8 Engine Oil Viscosity
The viscosity of oil after it has been used in an engine
over a period of time may change from its initial value
because the high temperature inside the engine can cause
the oil to break down. An experiment was conducted to
compare the effect on oil viscosity of two different
engines. Various samples of the same type of oil with a
constant viscosity were used, some in engine 1 and some
in engine 2, and the engines were run under identical
operating conditions. The resulting values of the oil
viscosities after having been used in the engines are given
in DS 9.7.6. Is there any evidence that the engines have
different effects on the oil viscosity?
9.7.9 Comparing Two Population Variances
For use with Problems 9.7.10–9.7.12.
Recall that if S2
x is the sample variance of a set of n
observations from a normal distribution with variance σ 2
A,
then
S2
x ∼σ 2
A
χ2
n−1
n −1
and that if S2
y is the sample variance of a set of m
observations from a normal distribution with variance σ 2
B,
then
S2
y ∼σ 2
B
χ2
m−1
m −1
(a) Explain why
σ 2
AS2
y
σ 2
BS2
x
∼Fm−1,n−1
(b) Show that part (a) implies that
P

F1−α/2,m−1,n−1 ≤
σ 2
AS2
y
σ 2
BS2
x
≤Fα/2,m−1,n−1

= 1 −α
or alternatively that
P

1
Fα/2,n−1,m−1
≤
σ 2
AS2
y
σ 2
BS2
x
≤Fα/2,m−1,n−1

= 1 −α
(c) Deduce that
σ 2
A
σ 2
B
∈

s2
x
s2
y Fα/2,n−1,m−1
, s2
x Fα/2,m−1,n−1
s2
y

is a 1 −α level two-sided conﬁdence interval for the
ratio of the population variances.
If such a conﬁdence interval contains the value 1, then
this indicates that it is plausible that the population
variances are equal. An unfortunate aspect of these
conﬁdence intervals, however, is that they depend heavily
on the data being normally distributed, and they should be
used only when that is a fair assumption. You may be able
to obtain these conﬁdence intervals on your computer
package.
9.7.10 A sample of n = 18 observations from population A has a
sample standard deviation of sx = 6.48, and a sample of
m = 21 observations from population B has a sample
standard deviation of sy = 9.62. Obtain a 90% conﬁdence
interval for the ratio of the population variances.
9.7.11 Consider again the data set of glass sheet thicknesses in
Problem 9.3.6 with sample sizes n = m = 41. Construct
a 90% conﬁdence interval for the ratio of the variances of
the thicknesses of glass sheets produced by the two
processes.
9.7.12 Consider again the data set of the breaking strengths of
n = 20 bundles of wool ﬁbers and m = 25 bundles of
synthetic ﬁbers in Problem 9.3.7. Construct 90%, 95%,
and 99% conﬁdence intervals for the ratio of the variances
of the breaking strengths of the two types of ﬁber.
9.7.13 The strengths of two types of canvas were compared in an
experiment. Fourteen samples of type A gave an average
strength of 327,433 with a standard deviation of 9,832.
Twelve samples of type B gave an average strength of
335,537 with a standard deviation of 10,463. Use a
hypothesis test to evaluate whether there is sufﬁcient

9.7 SUPPLEMENTARY PROBLEMS
429
evidence to conclude that there is a difference between
the strengths of the two canvas types.
9.7.14 Reinforced Cement Strengths
The strengths of nine reinforced cement samples were
tested using two procedures. Each sample was split
into two parts, with one part being tested with procedure
1 and the other part being tested with procedure 2. The
resulting data set is given in DS 9.7.7. Use an appropriate
hypothesis test to assess whether there is any evidence
that the two testing procedures provide different results
on average.
9.7.15 Are the following statements true or false?
(a) The advantage of paired experiments is that any
carry-over effects from one treatment to the other
treatment do not affect the analysis.
(b) In an experiment to compare two treatments with an
independent samples design, the random allocation of
the experimental units between the two treatments is
a good tool to help eliminate any bias in the data
collection.
(c) The experimental design refers to the manner in
which the data are collected.
(d) An experiment is performed to compare two medical
treatments. If one treatment is a placebo, then an
unpaired analysis should always be used.
(e) In a double-blind experiment the researcher does not
know the true values of the variable being measured.
(f) A paired experimental design can be conducted as a
blind experiment.
(g) For the analysis of two independent samples, the
unequal variances procedure can still be used even if
the experimenter suspects that the two population
variances may be equal.
(h) The manner in which the data are collected indicates
whether a two-sample data set is paired or
unpaired.
(i) For the analysis of two independent samples using
the unequal variances procedure, the value obtained
from the formula for the degrees of freedom should
be rounded down to the nearest integer.
9.7.16 Comparisons of Experimental Drug Therapies
Eight people participated in an experiment to compare
two experimental drug therapies. Each person was
administered both therapies, but in a random order. The
data values given in DS 9.7.8 were obtained. Perform a
hypothesis test to investigate whether there is any
evidence of a difference between the two therapies.
9.7.17 A sample of 20 items from manufacturer A were
measured and a sample mean of 2376.3 and a sample
standard deviation of 24.1 were obtained. Also, a sample
of 24 items from manufacturer B were measured and a
sample mean of 2402.0 and a sample standard deviation
of 26.4 were obtained.
(a) Use a one-sided hypothesis test to assess whether
there is sufﬁcient evidence to conclude that the items
from manufacturer B provide larger measurements on
average than these items from manufacturer A.
(b) Construct a 95% one-sided conﬁdence interval that
provides an upper bound on how much larger on
average the measurements of the items from
manufacturer B are compared with the items from
manufacturer A.
The data sets given in problems 9.7.18–9.7.22 can be used
to practice the two-sample methodologies presented in this
chapter.
9.7.18 Rubber Seal Curing Methods
An engineer is interested in whether the standard curing
time for the rubber seal on a radial assembly can be
replaced by a new rapid curing method that would reduce
manufacturing costs. However, it is important to
investigate whether the rapid curing method has any
effect on the dimensions of the seal. DS 9.7.9 contains
data on the inside diameter measurements of some seals
prepared with the standard curing method and with the
new rapid curing method.
9.7.19 Light and Dark Regimens for Plant Growth
In an experiment, plants were grown under controlled
conditions whereby the light they received was from a sun
lamp. In one regimen the plants were subjected to
alternate 12 hour periods of light and dark, while in
another regimen the plants were subjected to alternate
6 hour periods of light and dark. DS 9.7.10 contains the
heights of the plants after a certain time period.
9.7.20 Joystick Design for Spinal Cord Injury Patients
Patients with spinal cord injuries can lose mobility in their
arms and hands, and it is important to ﬁnd the optimal
design of a joystick that will enable them to perform tasks
in the most efﬁcient manner. An experiment was designed
to compare two joystick designs. As a target moved
across a computer screen, the patients were asked to use a
joystick to follow the target with a cursor. Nine spinal
cord injury patients participated in the study, and each
patient tried out both joystick designs. DS 9.7.11 contains
data on the mean error measurements that were calculated

430
CHAPTER 9
COMPARING TWO POPULATION MEANS
by aggregating the distances between the target and the
cursor at a series of time points.
9.7.21 Ambient Air Carbon Monoxide Pollution Levels
A researcher hypothesizes that ambient air carbon
monoxide pollution levels at a certain location should be
higher in the winter than they are in the summer. The
reasoning behind this hypothesis is that a major source of
carbon monoxide in the air is from the incomplete
combustion of fuels, and fuels tend to burn less efﬁciently
at low temperatures. Moreover, it is felt that the stagnant
winter air is more likely to trap the pollution. In order to
investigate this hypothesis, the data set in DS 9.7.12 is
collected which shows the ambient air carbon monoxide
pollution levels (parts per million) for ten Sunday
mornings in the middle of winter and ten Sunday
mornings in the middle of summer.
9.7.22 Sphygmomanometer and Finger Monitor Systolic
Blood Pressure Measurements
A sphygmomanometer is a standard instrument for
measuring blood pressure in the arteries consisting of a
pressure gauge and a rubber cuff that wraps around the
upper arm. DS 9.7.13 compares blood pressure readings
for 15 patients using this standard method and a new
method based upon a simple ﬁnger monitor.
9.7.23 A two-sample data analysis is conducted to compare
the sales of agent 1 with the sales of agent 2.
A. If the conﬁdence interval for the difference between
the average sales ability of agent 1 and the average
sales ability of agent 2 extends from a negative value
to a positive value so that it contains zero, then it
should be concluded that there is no evidence of a
difference between the average sales abilities of the
two agents.
B. If the conﬁdence interval for the difference between
the average sales ability of agent 1 and the average
sales ability of agent 2 extends from a negative value
to a positive value so that it contains zero, then it
should be concluded that it has been proved that there
is no difference between the average sales abilities of
the two agents.
C. Both of the above.
D. Neither of the above.
9.7.24 For a two-sample problem with independent samples,
n = 6, m = 8, ¯x = 5.42, ¯y = 4.38, sx = 1.84, sy = 2.02,
and ν = 11. The p-value for the hypotheses
H0 : μA = μB versus HA : μA ̸= μB is:
A. P(t11 ≥1.00)
B. 2 × P(t11 ≥1.84)
C. 2 × P(t11 ≥−1.84)
D. P(t11 ≥1.20)
E. 2 × P(t11 ≥1.00)
9.7.25 Carbon Footprints
Analyze the data in DS 9.7.14, which contains estimates
of the pounds of carbon dioxide released when making
several types of car, together with information on the
whether or not it is an SUV.
9.7.26 Green Management
A company introduces green management techniques to
make its manufacturing processes more environmentally
friendly and to cut waste. DS 9.7.15 shows weekly data
on the percentage of damaged inventory for 10 weeks
before and 10 weeks after the implementation of the new
techniques. How have the green management policies
affected the amount of damaged inventory?
9.7.27 Data Warehouse Design
Power consumption represents a large proportion of a data
center’s costs. A redesign was undertaken by a company
in an attempt to reduce these costs by more efﬁcient uses
of its physical components such as its routers, hubs, and
switches. The data in DS 9.7.16 shows monthly electricty
costs as a percentage of the data center’s total costs. What
doesthisindicateabouttheeffectivenessofthenewdesign?
9.7.28 Customer Churn
Customer churn is a term used for the attrition of a
company’s customers. DS 9.7.17 contains information
from an Internet service provider on the length of days
that its customers were signed up before switching to
another provider, and whether or not they were returning
customers (that is, whether or not they had previously had
Internet service from the company). Use the techniques
described in this chapter to analyze these data.
9.7.29 Natural Gas Consumption
DS 9.7.18 contains data on the total daily natural gas
consumption for a region for both the summer time and
the winter time. Do the natural gas consumption patterns
vary between summer and winter?
9.7.30 Consider an experimental design to compare two
treatments.
A. If there is a carry-over effect between the two
treatments, then a paired design can be used.
If an independent samples design is used, then
randomization cannot be used to allocate
experimental units to the two treatments in an
unbiased manner.

9.7 SUPPLEMENTARY PROBLEMS
431
B. If there is a carry-over effect between the two
treatments, then a paired design can be used. If
an independent samples design is used, then
randomization can be used to allocate experimental
units to the two treatments in an unbiased manner.
C. If there is a carry-over effect between the two
treatments, then a paired design cannot be used.
If an independent samples design is used, then
randomization cannot be used to allocate
experimental units to the two treatments in an
unbiased manner.
D. If there is a carry-over effect between the two
treatments, then a paired design cannot be used.
If an independent samples design is used, then
randomization can be used to allocate experimental
units to the two treatments in an unbiased manner.
9.7.31 When testing the difference between two treatments in a
two-sample problem, if the p-value is less than 1% then:
A. The differences in the data obtained from the two
treatments are not statistically signiﬁcant.
B. The differences in the data obtained from the two
treatments are statistically signiﬁcant.
9.7.32 When using a conﬁdence interval to compare two
treatments, which of these would result in a longer
interval if everything else remained the same?
A. A larger difference between the two sample means
B. A larger conﬁdence level
C. Larger sample sizes
9.7.33 Consider the design of a two-sample experiment to
compare two medical treatments on volunteers. If there is
no carry-over effect from one treatment to the other, then:
A. A paired design is preferable to an independent
samples design.
B. A blind design can be adopted.
C. Both of the above.
D. None of the above.
9.7.34 A two-sample data analysis is conducted to compare the
efﬁciencies of workers who have and who have not taken
a training course.
A. If the conﬁdence interval for the difference in the
efﬁciencies of the two sets of workers extends from a
negative value to a positive value so that it contains
zero, then it should be concluded that it has been
proved that the training method is effective.
B. If the conﬁdence interval for the difference in the
efﬁciencies of the two sets of workers extends from a
positive value to a positive value so that it does not
contain zero, then it should be concluded that the
training course has no effect on the efﬁciencies of the
workers.
C. Both of the above.
D. Neither of the above.

C H A P T E R T E N
Discrete Data Analysis
Subsequent to Chapters 8 and 9, which showed how inferences can be made on the population
means of continuous random variables, in this chapter the problem of making inferences on
the population probabilities of discrete random variables is considered. Recall that discrete
random variables may take only discrete values. For example, a random variable measuring
the number of errors in a software product may take the discrete values
0, 1, 2, 3, 4, . . .
A product’s quality level may be categorized as
high, medium, or low
or a machine breakdown may be characterized as being due to either
mechanical failure, electrical failure, or operator misuse
Sincediscreterandomvariablesoftenarisefromassigninganeventtooneofseveralcategories,
discrete data analysis is also often referred to as categorical data analysis.
A discrete data set consists of the frequencies or counts of the observations found at each
of the possible levels or cells. Discrete data analysis then consists of making inferences on
the cell probabilities. The simplest example of this kind is the problem of making inferences
about the success probability p of a binomial distribution. In this case there are two cells with
probabilities p and 1 −p. The ﬁrst two sections of this chapter discuss inference procedures
on the success probability of a binomial distribution and the comparison of two success
probabilities. The next two sections of the chapter consider more complex data sets in which
there may be threeormoredifferentcategoriesorinwhichobservationsmaybesimultaneously
subjected to more than one categorization process. These data sets can be represented in a
tabular form known as a contingency table, which is often analyzed with a chi-square goodness
of ﬁt test.
10.1
Inferences on a Population Proportion
Suppose that a parameter p represents the unknown proportion of a population that possesses
a particular characteristic. For instance, the population may represent all the items produced
by a particular machine, a proportion p of which are defective. If an observation is taken at
random from the population, it can then be thought of as having a probability p of exhibiting
the characteristic. If a random sample of n observations is obtained from the population,
each of the observations is then a realization of a Bernoulli random variable with “success
probability” p, and so the number of successes x, or in other words the number of observations
that possess the particular characteristic of interest, is a realization of a random variable X
that has a binomial distribution with parameters n and p
X ∼B(n, p)
432

10.1 INFERENCES ON A POPULATION PROPORTION
433
FIGURE 10.1
Generation of binary data
Population
Proportion p with characteristic
With  characteristic
Without characteristic
Random sample
of size n
Cell probability
p
Cell frequency
x
1 − p
Cell probability
n − x
Cell frequency
This framework is illustrated in Figure 10.1. Notice that each observation from the popu-
lation is a discrete random variable with two values, namely, possessing the characteristic or
not possessing the characteristic of interest. The probabilities of these two categories, that is,
the cell probabilities, are
p
and
1 −p
The cell frequencies are the number of observations that are observed to fall in each of the
two categories, and in this case they are
x
and
n −x
respectively.
Recall from Section 7.3.1 that the cell probability or success probability p can be estimated
by the sample proportion
ˆp = x
n
and that
E( ˆp) = p
and
Var( ˆp) = p(1 −p)
n
Furthermore, for large enough values of n the sample proportion can be taken to have approx-
imately the normal distribution
ˆp ∼N

p, p(1 −p)
n

This expression may be rewritten in terms of a standard normal distribution as
ˆp −p

p(1−p)
n
∼N (0, 1)
The normal approximation is appropriate as long as both np and n(1 −p) are larger than 5,
which can be taken to be the case as long as both x and n −x are larger than 5.

434
CHAPTER 10
DISCRETE DATA ANALYSIS
10.1.1
Conﬁdence Intervals for Population Proportions
One of the most useful inferences on a population proportion p is a conﬁdence interval that
summarizes the values that it can plausibly take. Either two-sided or one-sided conﬁdence
intervals can be constructed as shown below.
Two-Sided Conﬁdence Intervals
If Z ∼N(0, 1), then
P(−zα/2 ≤Z ≤zα/2) = 1 −α
Consequently, it is approximately the case that
P
⎛
⎝−zα/2 ≤
ˆp −p

p(1−p)
n
≤zα/2
⎞
⎠= 1 −α
which can be rewritten
P
	
ˆp −zα/2

p(1 −p)
n
≤p ≤ˆp + zα/2

p(1 −p)
n

= 1 −α
This expression implies that
p ∈
	
ˆp −zα/2

p(1 −p)
n
, ˆp + zα/2

p(1 −p)
n

is a two-sided 1 −α conﬁdence level conﬁdence interval for p. Notice that this conﬁdence
interval is of the form
p ∈( ˆp −critical point × s.e.( ˆp), ˆp + critical point × s.e.( ˆp))
where the standard error (s.e.) of the estimate ˆp is (see Section 7.3.1)
s.e.( ˆp) =

p(1 −p)
n
However, as it stands, this conﬁdence interval cannot be constructed because the standard
error s.e.( ˆp) depends on the unknown probability p. It is therefore customary to estimate this
standard error by replacing p with its estimated value ˆp, so that the conﬁdence interval is
constructed using
s.e.( ˆp) =

ˆp(1 −ˆp)
n
= 1
n

x(n −x)
n
where x is the observed number of “successes” in the random sample of size n.

10.1 INFERENCES ON A POPULATION PROPORTION
435
Two-Sided Conﬁdence Intervals for a Population Proportion
If the random variable X has a B(n, p) distribution, then an approximate two-sided
1 −α conﬁdence level conﬁdence interval for the success probability p based on an
observed value of the random variable x is
p ∈
	
ˆp −zα/2

ˆp(1 −ˆp)
n
, ˆp + zα/2

ˆp(1 −ˆp)
n

where the estimated success probability is ˆp = x/n. This conﬁdence interval can also
be written as
p ∈
	
ˆp −zα/2
n

x(n −x)
n
, ˆp + zα/2
n

x(n −x)
n

If a random sample of n observations is taken from a population and x of the
observations are of a certain type, then this expression provides a conﬁdence interval
for the proportion p of the population of that type.
The approximation is reasonable as long as both x and n −x are larger than 5. If it
turns out that the upper end of this conﬁdence interval is larger than 1, then it can of
course be truncated at 1, and if the lower end of the conﬁdence interval is smaller than
0, then it can be truncated at 0.
Example 57
Building Tile Cracks
One construction method employed in many public utility buildings and ofﬁce blocks is to
have the exterior building walls composed of a large number of small tiles. These tiles are
generally cemented into place on the building wall using some type of resin mixture as the
cement. Over time, the resin mixture may contract and expand, resulting in the building tiles
becoming cracked.
A construction engineer is faced with the problem of assessing the tile damage in a certain
group of downtown buildings. The total number of tiles on these buildings is around ﬁve
million and it is far too costly to examine each tile in detail for cracks. Therefore the engineer
constructs a sample of 1250 tiles chosen randomly from the blueprints of the building, as
illustrated in Figure 10.2, and examines each tile in the sample for cracking.
The engineer is interested in the true overall proportion p of all the tiles that are cracked.
Out of the sample of n = 1250 tiles, x = 98 are found to be cracked, and so the overall
proportion p can be estimated as
ˆp = x
n =
98
1250 = 0.0784
With a critical point z0.005 = 2.576, a 99% two-sided conﬁdence interval for the overall
proportion of cracked tiles is
p ∈
	
ˆp −zα/2
n

x(n −x)
n
, ˆp + zα/2
n

x(n −x)
n

=
	
0.0784 −2.576
1250

98 × (1250 −98)
1250
, 0.0784 + 2.576
1250

98 × (1250 −98)
1250

= (0.0784 −0.0196, 0.0784 + 0.0196) = (0.0588, 0.0980)
Consequently, based upon this sample of 1250 tiles, with 99% conﬁdence the true proportion
of cracked tiles is determined to lie somewhere between 5.88% and 9.8%, or somewhere
between about 6% and 10%. If the total number of tiles on the buildings is about ﬁve million,

436
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.2
A random sample of n = 1250 tiles
from a group of buildings
Random selection of tiles from all facades of the buildings
then the construction engineer can infer that the total number of cracked tiles is between about
0.06 × 5,000,000 = 300,000
and
0.10 × 5,000,000 = 500,000
Example 58
Overage Weedkiller
Product
A chemical company produces a weedkiller that is sold in containers for consumers to apply
in their yards and on their lawns. The company knows that after the time of manufacture there
is an optimum time period in which the weedkiller should be used for maximum effect, and
that applications of the weedkiller after this time period are not so effective. Products with an
age older than this optimum time period are considered to be “overage” products.
In order to ensure the effectiveness of its product when it reaches the consumer’s hands,
the chemical company is interested is assessing how much of its product on the shelf waiting
to be sold is in fact overage. A nationwide sampling scheme is developed whereby auditors
visit randomly selected stores and determine whether the shelf product is overage or not from
codings on the weedkiller containers indicating the date of manufacture.
The chemical company is interested in the true overall proportion p of the product on the
shelf that is overage. The auditors examined n = 54,965 weedkiller containers and found
that x = 2779 of them were overage. The overall proportion of overage product can then be
estimated as
ˆp =
2779
54,965 = 0.0506
With a critical point z0.005 = 2.576, a 99% two-sided conﬁdence interval for the overall
proportion of overage product is
p ∈
⎛
⎝0.0506 −2.576
54,965

2779 × (54,965 −2779)
54,965
,
0.0506 + 2.576
54,965

2779 × (54,965 −2779)
54,965
⎞
⎠
= (0.0506 −0.0024, 0.0506 + 0.0024) = (0.0482, 0.0530)

10.1 INFERENCES ON A POPULATION PROPORTION
437
Therefore, the chemical company has discovered that somewhere between about 4.8% and
5.3% of its weedkiller product on the shelf waiting to be sold is overage.
GAMES OF CHANCE
Recall the problem discussed in Section 7.3.1 concerning the investigation of the bias of a
coin. Two scenarios were considered.
■
Scenario I : The coin is tossed 100 times and 40 heads are obtained.
■
Scenario II : The coin is tossed 1000 times and 400 heads are obtained.
In either case, the probability of obtaining a head is estimated to be ˆp = 0.4, but the standard
error of the estimate was shown to be much smaller in scenario II than in scenario I.
The smaller standard error in scenario II results in a smaller conﬁdence interval for the
probability p. For example, in scenario I with z0.025 = 1.96, a 95% two-sided conﬁdence
interval for p is calculated to be
p ∈
	
ˆp −zα/2

ˆp(1 −ˆp)
n
, ˆp + zα/2

ˆp(1 −ˆp)
n

=
	
0.4 −1.96

0.4 × (1 −0.4)
100
, 0.4 + 1.96

0.4 × (1 −0.4)
100

= (0.4 −0.096, 0.4 + 0.096) = (0.304, 0.496)
However, in scenario II the conﬁdence interval is calculated to be
p ∈
	
0.4 −1.96

0.4 × (1 −0.4)
1000
, 0.4 + 1.96

0.4 × (1 −0.4)
1000

= (0.4 −0.030, 0.4 + 0.030) = (0.370, 0.430)
These two conﬁdence intervals are illustrated in Figure 10.3. Notice that the larger value of n
in scenario II results in a shorter conﬁdence interval for p, reﬂecting the increase in precision
of the estimate ˆp. In fact, for a ﬁxed value of ˆp and a ﬁxed conﬁdence level 1 −α, the
conﬁdence interval length is seen to be inversely proportional to the square root of the sample
size n.
One-Sided Conﬁdence Intervals
One-sided conﬁdence intervals for a population propor-
tion p can be used instead of a two-sided conﬁdence interval if an experimenter is interested
FIGURE 10.3
Conﬁdence intervals for the
probability p of obtaining a head in
a toss of a biased coin
0
1
40 heads from 100 coin tosses
p
0
1
p
400 heads from 1000 coin tosses
(
)
(
)
0.304
0.496
0.370
0.430

438
CHAPTER 10
DISCRETE DATA ANALYSIS
in obtaining only an upper bound or a lower bound on the population proportion. Their for-
mat is similar to the two-sided conﬁdence interval except that a (smaller) critical point zα is
employed in place of zα/2.
The one-sided conﬁdence intervals can be constructed as follows. If Z ∼N(0, 1), then
P(Z ≤zα) = 1 −α
so that it is approximately the case that
P
⎛
⎝
ˆp −p

p(1−p)
n
≤zα
⎞
⎠= 1 −α
which can be rewritten
P
	
ˆp −zα

p(1 −p)
n
≤p

= 1 −α
This expression implies that
p ∈
	
ˆp −zα

p(1 −p)
n
, 1

is a one-sided 1 −α conﬁdence level conﬁdence interval that provides a lower bound for p.
Similarly,
P(−zα ≤Z) = 1 −α
so that it is approximately the case that
P
⎛
⎝−zα ≤
ˆp −p

p(1−p)
n
⎞
⎠= 1 −α
which can be rewritten
P
	
p ≤ˆp + zα

p(1 −p)
n

= 1 −α
Therefore
p ∈
	
0, ˆp + zα

p(1 −p)
n

is a one-sided 1 −α conﬁdence level conﬁdence interval that provides an upper bound for p.
As in the two-sided conﬁdence intervals, the unknown value of p is replaced by the
estimate ˆp = x/n in the expression

p(1 −p)
n

10.1 INFERENCES ON A POPULATION PROPORTION
439
One-Sided Conﬁdence Intervals for a Population Proportion
If the random variable X has a B(n, p) distribution, then approximate one-sided 1 −α
conﬁdence level conﬁdence intervals for the success probability p based upon an
observed value x of the random variable are
p ∈
	
ˆp −zα

ˆp(1 −ˆp)
n
, 1

and
p ∈
	
0, ˆp + zα

ˆp(1 −ˆp)
n

where the estimated success probability is ˆp = x/n. These conﬁdence intervals
respectively provide a lower bound and an upper bound on the probability p, and can
also be written as
p ∈
	
ˆp −zα
n

x(n −x)
n
, 1

and
p ∈
	
0, ˆp + zα
n

x(n −x)
n

The approximation is reasonable as long as both x and n −x are larger than 5.
Example 39
Cattle Inoculations
Recall that when a vaccine was administered to n = 500,000 head of cattle, x = 372 were
observed to suffer a serious adverse reaction. The estimate of the probability p of an animal
suffering such a reaction is thus
ˆp =
372
500,000 = 7.44 × 10−4
In order to satisfy government safety regulations, the manufacturers of the vaccine must
provide an upper bound on the probability of an animal suffering such a reaction. With the
critical point z0.01 = 2.326, a one-sided 99% conﬁdence interval for the probability p of an
animal suffering a reaction can be calculated to be
p ∈
	
0, ˆp + zα
n

x(n −x)
n

=
⎛
⎝0, 0.000744 +
2.326
500,000

372 × (500,000 −372)
500,000
⎞
⎠
= (0, 0.000744 + 0.000090) = (0, 0.000834)
as illustrated in Figure 10.4. Thus, with 99% conﬁdence, the manufacturer can claim that the
probability of an adverse reaction to the vaccine is no larger than 8.34 × 10−4.
As a ﬁnal point, notice that an upper bound on a probability p can be used to obtain a lower
bound on the complementary probability 1−p. Thus since 1−8.34× 10−4 = 0.999166, this
result can be rephrased as “the probability that an animal does not suffer an adverse reaction
is at least 0.999166.”
10.1.2
Hypothesis Tests on a Population Proportion
An observation x from a random variable X with a B(n, p) distribution can be used to test
a hypothesis concerning the success probability p. A two-sided hypothesis testing problem
would be
H0 : p = p0
versus
HA : p ̸= p0

440
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.4
Upper conﬁdence bound on the
probability of an adverse reaction
from the cattle vaccine
Vaccine administered to n = 500,000 cattle
              x = 372 suffer an adverse reaction
              p = probability of an adverse reaction
ˆp = x/n = 0.000744
0
ˆp
0.001
p
99% upper confidence bound
p ∈(0, 0.000834)
)
for a particular ﬁxed value p0. This is appropriate if an experimenter wishes to determine
whether there is signiﬁcant evidence that the success probability is different from p0. One-
sided sets of hypotheses
H0 : p ≥p0
versus
HA : p < p0
and
H0 : p ≤p0
versus
HA : p > p0
can also be used.
The p-values for these hypothesis tests can be calculated using the cumulative distri-
bution function of the binomial distribution, which for reasonably large values of n can be
approximated by a normal distribution. If the normal approximation is employed, then
ˆp −p

p(1−p)
n
is taken to have approximately a standard normal distribution, so that if p = p0, the
“z-statistic”
z =
ˆp −p0

p0(1−p0)
n
can be taken to be an observation from a standard normal distribution. Notice that the hypoth-
esized value p0 is used inside the square root term

p0(1 −p0)
n
of this expression, and that when the top and the bottom of the expression are multiplied by
n, the z-statistic can be rewritten as
z =
x −np0
√np0(1 −p0)
The normal approximation can be improved with a continuity correction whereby the numer-
ator of the z-statistic x −np0 is replaced by either x −np0 −0.5 or x −np0 +0.5 as described
in the next section.

10.1 INFERENCES ON A POPULATION PROPORTION
441
Two-Sided Hypothesis Tests
The exact p-value for the two-sided hypothesis testing
problem
H0 : p = p0
versus
HA : p ̸= p0
is usually calculated as
p-value = 2 × P(X ≥x)
if ˆp = x/n > p0, and as
p-value = 2 × P(X ≤x)
if ˆp = x/n < p0, where the random variable X has a B(n, p0) distribution. This can be
deduced from the deﬁnition of the p-value:
The p-value is the probability of obtaining this data set or worse
when the null hypothesis is true.
Notice that under the null hypothesis H0, the expected value of the number of successes is np0.
Consequently, as Figure 10.5 shows, “worse” in the deﬁnition of the p-value means values
of the random variable X farther away from np0 than is observed. This is values larger than
x when x > np0 ( ˆp > p0), and values smaller than x when x < np0 ( ˆp < p0). The tail
probabilities of the binomial distribution
P(X ≥x)
and
P(X ≤x)
are then multiplied by 2 since it is a two-sided problem with the alternative hypothesis HA :
p ̸= p0 allowing values of p both smaller and larger than p0. Of course if ˆp = p0, then the
p-value can be taken to be equal to 1, and there is clearly no evidence that the null hypothesis
is not plausible.
FIGURE 10.5
Constructing two-sided
hypothesis tests
0
1
2
n
. . .
x
 Worse
outcomes
“
”
 Observed
 outcome
Expected
outcome
under
np
x
np
>
x < np0
0
1
2
. . .
x
np0
n
H0
Expected
outcome
under H0
 Observed
 outcome
 Worse
outcomes
“
”
0
0

442
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.6
p-value calculations for two-sided
hypothesis tests
0
z
0
z
p-value = 2×(1−(z))
Standard
   normal
distribution
p-value = 2×(z)
z > 0
z
0
<
H0: p = p0, HA: p ̸= p0
z =
ˆp −p0

p0(1−p0)
n
=
x −np0

np0(1−p0)
When the normal approximation to the distribution of ˆp is appropriate (in this case it can
be considered to be acceptable as long as np0 and n(1−p0) are both larger than 5), the statistic
z =
ˆp −p0

p0(1−p0)
n
=
x −np0

np0(1 −p0)
is calculated and, as Figure 10.6 illustrates, the p-value is
p-value = 2 × P(Z ≥z)
if z > 0, and
p-value = 2 × P(Z ≤z)
if z < 0, where the random variable Z has a standard normal distribution. In either case, the
p-value can be written as
p-value = 2 × (−|z|)
where (·) is the standard normal cumulative distribution function.

10.1 INFERENCES ON A POPULATION PROPORTION
443
The normal approximation can be improved by employing a continuity correction of 0.5
in the numerator of the z-statistic. If x −np0 > 0.5, a z-statistic
z = x −np0 −0.5
√np0(1 −p0)
can be used, and if x −np0 < −0.5, a z-statistic
z = x −np0 + 0.5
√np0(1 −p0)
can be used. Notice that the continuity correction serves to bring the value of the z-statistic
closer to 0. The effect of employing the continuity correction becomes less important as the
sample size n gets larger.
Hypothesis tests at a ﬁxed size α can be performed by comparing the p-value with the
value of α. The null hypothesis is accepted if the p-value is larger than α, so that the acceptance
region is
|z| ≤zα/2
and the null hypothesis is rejected if the p-value is smaller than α, so that the rejection region is
|z| > zα/2
Two-Sided Hypothesis Tests for a Population Proportion
If the random variable X has a B(n, p) distribution, then the exact p-value for the
two-sided hypothesis testing problem
H0 : p = p0
versus
HA : p ̸= p0
based upon an observed value of the random variable x is
p-value = 2 × P(X ≥x)
if ˆp = x/n > p0, and
p-value = 2 × P(X ≤x)
if ˆp = x/n < p0, where the random variable X has a B(n, p0) distribution. When np0
and n(1 −p0) are both larger than 5, a normal approximation can be used to give a
p-value of
p-value = 2 × (−|z|)
where (·) is the standard normal cumulative distribution function and
z =
ˆp −p0

p0(1−p0)
n
=
x −np0
√np0(1 −p0)
In order to improve the normal approximation the value x −np0 −0.5 may be used in
the numerator of the z-statistic when x −np0 > 0.5, and the value x −np0 + 0.5 may
be used in the numerator of the z-statistic when x −np0 < −0.5. A size α hypothesis
test accepts the null hypothesis when
|z| ≤zα/2
and rejects the null hypothesis when
|z| > zα/2

444
CHAPTER 10
DISCRETE DATA ANALYSIS
Example 59
Opossum Progeny
Genders
A biologist is interested in whether opossums give birth to male and female progeny with
equal probabilities. A group of opossums is observed, and out of 23 births, 14 are male and 9
are female.
Suppose that each opossum offspring has a probability p of being male, independent of
any other births. The number of male births out of 23 births is then a random variable with a
B(23, p) distribution, and the hypotheses of interest are
H0 : p = 0.5
versus
HA : p ̸= 0.5
With x = 14 male births out of n = 23 total births, the estimated probability of a male birth
is
ˆp = 14
23 = 0.609
which is larger than the hypothesized value of p0 = 0.5. As Figure 10.7 shows, the exact
p-value is therefore
p-value = 2 × P(X ≥14)
where the random variable X has a B(23, 0.5) distribution. This value can be calculated to be
p-value = 2 × 0.2024 = 0.4048
x = 14 male opossum births
n −x = 9 female opossum births
Data
p = probability of a male birth
ˆp = 14/23 = 0.609
Model
H0 : p = 0.5
HA : p ̸= 0.5
Hypotheses
np0 = 23 × 0.5 = 11.5
x > np0
X ∼B(23, 0.5)
p-value = 2 × P(X ≥14) = 0.4048
p-Value calculation
H0 is plausible
Conclusion
FIGURE 10.7
Exact p-value calculation for opos-
sum progeny genders example
Since np0 = n(1−p0) = 23×0.5 = 11.5 > 5, a normal approximation to the distribution
of X should be reasonable. The value of the z-statistic with continuity correction is
z = x −np0 −0.5

np0(1 −p0)
=
14 −(23 × 0.5) −0.5

23 × 0.5 × (1.0 −0.5)
= 0.83
which gives a p-value (calculated from Table I) of
p-value = 2 × (−0.83) = 2 × 0.2033 = 0.4066
It can be seen that the normal approximation is quite accurate, and that with such large
p-values there is no reason to doubt the validity of the null hypothesis. Based on this data set
the biologist realizes that there is not sufﬁcient evidence to conclude that male and female
births are not equally likely.
However, with only 23 births observed, it should be remembered that there is a wide range
of other plausible values for the probability of a male birth p. In fact, with a critical point
z0.025 = 1.96, a 95% two-sided conﬁdence interval for the probability of a male birth is
p ∈
	
ˆp −zα/2
n

x(n −x)
n
, ˆp + zα/2
n

x(n −x)
n

=
	
0.609 −1.96
23

14 × (23 −14)
23
, 0.609 + 1.96
23

14 × (23 −14)
23

= (0.609 −0.199, 0.609 + 0.199) = (0.410, 0.808)
Thus, the probability of a male birth could in fact be anywhere between about 0.4 and 0.8.

10.1 INFERENCES ON A POPULATION PROPORTION
445
Example 60
Random Variable
Simulations
A mathematician is investigating various algorithms for simulating random variable observa-
tions on a computer. One algorithm is supposed to produce (independent) observations from
a standard normal distribution. The mathematician obtains 10,000 simulations from the algo-
rithm and notices that 6702 of them have an absolute value no larger than 1. Does this cast
any doubt on the validity of the algorithm?
Suppose that the algorithm produces a value between −1 and 1 with a probability p, so
that the number of values out of 10,000 lying in this range has a B(10,000, p) distribution.
If the values really are observations from a standard normal distribution, then the success
probability is
p = (1) −(−1) = 0.8413 −0.1587 = 0.6826
so that the two-sided hypotheses of interest are
H0 : p = 0.6826
versus
HA : p ̸= 0.6826
With n = 10,000 a normal approximation to the p-value is appropriate and the z-statistic is
z =
6702 −(10,000 × 0.6826) + 0.5

10,000 × 0.6826 × (1.0000 −0.6826)
= −2.65
although it can be seen that the continuity correction of 0.5 is not important here. The p-value
(calculated from Table I) is therefore
p-value = 2 × (−2.65) = 2 × 0.0040 = 0.0080
and such a small value leads the mathematician to conclude that the null hypothesis is not
plausible and that the algorithm is not doing a very good job of simulating standard normal
random variables.
One-Sided Hypothesis Tests
The p-values for one-sided hypothesis tests are calculated in
the obvious way as shown in the accompanying box. Exact p-values are calculated from the
tail probabilities of the appropriate binomial distribution, and these can be approximated by
a normal distribution in the usual circumstances. A continuity correction of either +0.5 or
−0.5 should be used, depending on the direction of the one-sided problem.
Example 57
Building Tile Cracks
Legal agreements have been reached whereby if 10% or more of the building tiles are cracked,
then the construction company that originally installed the tiles must help pay for the building
repair costs. Do the survey results of 98 cracked tiles out of 1250 tiles indicate that the
construction company should be required to contribute to the building repair costs?
The construction engineers approach this problem in the following way. If p is the prob-
ability that a tile is cracked, then the one-sided hypotheses
H0 : p ≥0.1
versus
HA : p < 0.1
should be considered. The null hypothesis corresponds to situations in which the construction
company must contribute to the repair costs, and the alternative hypothesis corresponds to
situations where it is not liable for any costs. A rejection of the null hypothesis would therefore
establish that the construction company has no ﬁnancial responsibilities.

446
CHAPTER 10
DISCRETE DATA ANALYSIS
One-Sided Hypothesis Tests for a Population Proportion
If the random variable X has a B(n, p) distribution, then the exact p-value for the
one-sided hypothesis testing problem
H0 : p ≥p0
versus
HA : p < p0
based upon an observed value of the random variable x is
p-value = P(X ≤x)
where the random variable X has a B(n, p0) distribution. The normal approximation
to this is
p-value = (z)
where (·) is the standard normal cumulative distribution function and
z = x −np0 + 0.5

np0(1 −p0)
A size α hypothesis test accepts the null hypothesis when
z ≥−zα
and rejects the null hypothesis when
z < −zα
For the one-sided hypothesis testing problem
H0 : p ≤p0
versus
HA : p > p0
the p-value is
p-value = P(X ≥x)
where the random variable X has a B(n, p0) distribution. The normal approximation
to this is
p-value = 1 −(z)
where
z = x −np0 −0.5

np0(1 −p0)
A size α hypothesis test accepts the null hypothesis when
z ≤zα
and rejects the null hypothesis when
z > zα
A normal approximation is appropriate, and with n = 1250 and x = 98 the z-statistic is
z = x −np0 + 0.5

np0(1 −p0)
=
98 −(1250 × 0.1) + 0.5

1250 × 0.1 × (1.0 −0.1)
= −2.50

10.1 INFERENCES ON A POPULATION PROPORTION
447
FIGURE 10.8
Hypothesis test analysis for
building tile cracks example
0
z
p-value = (−2.50) = 0.0062
= −2.50
 Standard 
   normal
distribution
Conclusion:   The null hypothesis is not plausible and it has been established that
                        the construction company is not liable for the repair costs.
H0 : p ≥0.1
: p
0.1
HA
 <
Construction company
liable for the repair costs
Construction company
not liable for the repair costs
x = 98 cracked tiles in a sample of n = 1250 tiles
             p =  probability that a tile is cracked  
z = x −np0 +0.5

np0(1−p0)
= −2.50
The p-value is therefore
p-value = (−2.50) = 0.0062
which, as Figure 10.8 illustrates, indicates that the null hypothesis is not plausible and can be
rejected. This establishes that the construction company is not required to contribute to the
repair costs.
Example 39
Cattle Inoculations
Suppose that the vaccine can be approved for widespread use if it can be established that
on average no more than one in a thousand cattle will suffer a serious adverse reaction. In
other words, the probability of a serious adverse reaction must be no larger than 0.001. If the
one-sided hypotheses
H0 : p ≥0.001
versus
HA : p < 0.001
are used, then the rejection of the null hypothesis will establish that the vaccine can be approved
for widespread use.
With x = 372 reactions observed in a sample of n = 500,000 cattle, a 99% one-sided
conﬁdence interval for the probability of a reaction was calculated to be
p ∈(0, 0.000834)
Since the upper bound of this conﬁdence interval is smaller than p0 = 0.001, this implies that
the p-value for these one-sided hypotheses will be smaller than 1%. In fact, with a z-statistic
z =
372 −(500,000 × 0.001) + 0.5

500,000 × 0.001 × (1.000 −0.001)
= −5.70

448
CHAPTER 10
DISCRETE DATA ANALYSIS
the p-value is calculated to be
p-value = (−5.70) ≃0
In conclusion, the null hypothesis has been shown not to be plausible, so that the probability
of an adverse reaction is known to be less than one in a thousand and the vaccine can be
approved for widespread use.
GAMES OF CHANCE
If I take a six-sided die, roll it ten times, and score a 6 only once, should I have a reasonable
suspicion that the die is weighted to reduce the chance of scoring a 6? If p is the probability
of scoring a 6 on a roll of the die, the one-sided hypotheses of interest are
H0 : p ≥1
6
versus
HA : p < 1
6
If the null hypothesis is plausible, then there is no reasonable suspicion that the die is weighted
to reduce the chance of scoring a 6. However, a small p-value will indicate that the null
hypothesis is not plausible and will imply that the die is weighted to reduce the chance of
scoring a 6.
With n = 10 and x = 1, the exact p-value for these hypotheses is
p-value = P(X ≤1)
where the random variable X has a B(10, 1/6) distribution, which is
p-value = 0.4845
This large p-value indicates that the null hypothesis is quite plausible and that there is no
reason to suspect that the chance of scoring a 6 is any smaller than 1/6.
What if only one 6 is scored in 20 rolls of the die? In this case the p-value is
p-value = P(X ≤1)
where the random variable X has a B(20, 1/6) distribution, which is
p-value = 0.1304
Again, there is still not sufﬁcient evidence to conclude that the chance of scoring a 6 is any
smaller than 1/6.
Figure 10.9 shows the p-values for additional values of n. If only one 6 is obtained in 30
rolls of the die, the situation is starting to look suspicious because the p-value is about 3%.
With 40 rolls of the die the p-value falls below 1% and the null hypothesis that the probability
of scoring a 6 is at least 1/6 becomes unbelievable.
Figure 10.10 provides a summary chart of conﬁdence interval construction and hypothesis
testing for a population proportion p. As is always the case, the decisions of hypothesis tests
at a ﬁxed size α can be deduced from the corresponding p-value. In general, the inclusion of
p0 in a 1 −α conﬁdence level conﬁdence interval for p implies a p-value greater than α for a
hypothesized value p0 for the population proportion p. Similarly, exclusion generally implies
a p-value smaller than α. In some rare cases, however, there may be a slight discrepancy
in these general rules due to the way in which the quantity √p(1 −p)/n is handled, with
p being replaced either by ˆp in conﬁdence interval construction or by p0 in hypothesis
testing.

10.1 INFERENCES ON A POPULATION PROPORTION
449
FIGURE 10.9
p-values for die example
One 6 is obtained in n rolls of a die. Is the die weighted to reduce the chance of scoring a 6?
p = probability of scoring a 6
H0 : p ≥1/6
HA : p < 1/6
↑
Die weighted to reduce chance
of scoring a 6
X ∼B(n, 1/ 6)
p-value = P(X ≤1)
n
p-value
10
0.4845
20
0.1304
30
0.0295
40
0.0061
50
0.0012
FIGURE 10.10
Summary of inferences on a
population proportion
Inferences on a Population Proportion
Observe x successes from n trials
p = probability of success
ˆp = x/n
Two-sided 1 −α level conﬁdence interval (both x and n −x larger than 5)
p ∈

ˆp −zα/2

ˆp(1−ˆp)
n
, ˆp + zα/2

ˆp(1−ˆp)
n

=

ˆp −
zα/2
n

x(n−x)
n
, ˆp +
zα/2
n

x(n−x)
n

One-sided 1 −α level conﬁdence intervals (both x and n −x larger than 5)
p ∈

ˆp −zα

ˆp(1−ˆp)
n
, 1

=

ˆp −zα
n

x(n−x)
n
, 1

p ∈

0, ˆp + zα

ˆp(1−ˆp)
n

=

0, ˆp + zα
n

x(n−x)
n

Hypothesis testing
X ∼B(n, p0)
H0: p = p0, HA: p ̸= p0
x −np0 > 0.5
z =
x−np0−0.5
√
np0(1−p0)
p-value = 2 × P(X ≥x) ≃2 × (1 −(z))
x −np0 < −0.5
z =
x−np0+0.5
√
np0(1−p0)
p-value = 2 × P(X ≤x) ≃2 × (z)
H0: p ≥p0, HA: p < p0
z =
x−np0+0.5
√
np0(1−p0)
p-value = P(X ≤x) ≃(z)
H0: p ≤p0, HA: p > p0
z =
x−np0−0.5
√
np0(1−p0)
p-value = P(X ≥x) ≃1 −(z)
10.1.3
Sample Size Calculations
The sample size n affects the precision of the inference that can be made about a population
proportion p. It is often useful to gauge the amount of precision afforded by a certain sample
size before any sampling is performed. Furthermore, after the results of an initial sample are

450
CHAPTER 10
DISCRETE DATA ANALYSIS
observed, it may be useful to determine how much additional sampling is required to attain a
speciﬁed precision.
Within a hypothesis testing framework, increased sample sizes result in increased power
for tests at a ﬁxed signiﬁcance level α. This means that when the null hypothesis is false, there
is a greater chance that there will be enough evidence to reject it. However, as in previous
chapters, the most convenient way to assess the amount of precision afforded by a sample size
n is to consider the length L of a two-sided conﬁdence interval for the population proportion
p. If a conﬁdence level 1 −α is used, then the conﬁdence interval length is
L = 2zα/2

ˆp(1 −ˆp)
n
so that the sample size n required to achieve a conﬁdence interval length L is
n =
4z2
α/2 ˆp(1 −ˆp)
L2
Notice that the required sample size n increases either as the conﬁdence interval length L
decreases or as the speciﬁed conﬁdence level 1 −α increases (so that α decreases and zα/2
increases).
A problem in using this formula to ﬁnd the required sample size is that the term ˆp(1 −ˆp)
is unknown. However, Figure 10.11 shows how the value of p(1 −p) varies for p between 0
and 1, and it can be seen that the largest value taken is 1/4 when p = 1/2. Consequently, a
worst case scenario is to take
ˆp(1 −ˆp) = 1
4
in which case the required sample size is
n =
z2
α/2
L2
Nevertheless, if the value of ˆp is far from 0.5, this worst case scenario is wasteful and the
requirement on the conﬁdence interval length L can be met with a smaller sample size. If
prior information or knowledge of the problem allows the experimenter to bound the value
of ˆp away from 0.5, then a smaller required sample size can be determined. Speciﬁcally, if
the experimenter can reasonably expect that ˆp will be less than some value p∗< 0.5, or
alternatively if the experimenter can reasonably expect that ˆp will be greater than a value
FIGURE 10.11
Value of p(1 −p)
1/2
p
0
p(1 − p)
1/4
1

10.1 INFERENCES ON A POPULATION PROPORTION
451
p∗> 0.5, then a sample size
n ≃
4z2
α/2 p∗(1 −p∗)
L2
will sufﬁce.
Example 61
Political Polling
A local newspaper wishes to poll the population of its readership area to determine the propor-
tion p of them who agree with the statement
The city mayor is doing a good job.
The newspaper wishes to present the results as a percentage with a footnote reading “accurate
to within ±3%.” How many people do they need to poll?
The ﬁrst point to notice here is that the newspaper decides to discard anybody who does
not express an opinion. Thus, the sample results will consist of x people who agree with the
statement and n −x people who do not agree with the statement and, as Figure 10.12 shows,
the paper will publish the estimate
ˆp = x
n
The pollsters then decide that if they can construct a 99% conﬁdence interval for p with a
length no larger than L = 6%, they will have achieved the desired accuracy (because this
conﬁdence interval is ˆp ±3%). In addition, the pollsters feel that the population may be fairly
evenly spread on their agreement with the statement. Therefore a worst case scenario with
ˆp = 0.5 is considered, and with z0.005 = 2.576 the required sample size (of people with an
FIGURE 10.12
Political polling example
Population
Random sample size n
“The city mayor is doing a good job.”
Cell frequency
Cell frequency
x
n − x
0.03
0.03
99% confidence interval
Disagree
Agree
ˆp
 Published result:
(accurate to within 3%)
ˆp =
x
n

452
CHAPTER 10
DISCRETE DATA ANALYSIS
opinion on the statement) is calculated to be
n =
z2
α/2
L2 = 2.5762
0.062 = 1843.3
Consequently, the paper decides to obtain the opinion of a representative sample of at least
1844 people.
In fact, you will often see polls reported with a footnote “based on a sample of 1000
respondents, ±3% sampling error.” How should this statement be interpreted? Under the
worst case scenario with ˆp = 0.5, this sample size results in a conﬁdence interval ˆp ± 3%
with a conﬁdence level 1 −α where
zα/2 = √nL =
√
1000 × 0.06 = 1.90
This equation is satisﬁed with α ≃0.057, so that the reader should interpret the sensitivity
of the poll as implying that with about 95% conﬁdence, the true proportion p is within three
percentage points of the reported value. However, if ˆp is far from 0.5, the conﬁdence level
may in fact be much larger than 95%.
Example 57
Building Tile Cracks
Recall that a 99% conﬁdence interval for the overall proportion of cracked tiles was calculated
to be
p ∈(0.0588, 0.0980)
However, the lawyers handling this case decided that they needed to know the overall pro-
portion of cracked tiles to within 1% with 99% conﬁdence. How much additional sampling
is necessary?
The lawyers’ demands can be met with a 99% conﬁdence interval with a length L of 2%. It
is reasonable to expect, based on the conﬁdence interval above, that the estimated proportion
ˆp after the second stage of sampling will be less than p∗= 0.1. Consequently, it can be
estimated that a total sample size of
n ≃
4z2
α/2 p∗(1 −p∗)
L2
= 4 × 2.5762 × 0.1 × (1.0 −0.1)
0.022
= 5972.2
or about 6000 tiles, will sufﬁce. Since the initial sample consisted of 1250 tiles, the engineers
therefore decided to take a secondary representative sample of 4750 tiles.
The results of the secondary sample reveal that 308 of the 4750 tiles examined were
cracked. Together with the initial sample of 98 cracked tiles out of 1250, there are therefore
x = 308 + 98 = 406 cracked tiles out of n = 6000. This gives
ˆp = 406
6000 = 0.0677
and a 99% conﬁdence interval
p ∈
	
ˆp −zα/2
n

x(n −x)
n
, ˆp + zα/2
n

x(n −x)
n

=
	
0.0677 −2.576
6000

406 × (6000 −406)
6000
, 0.0677 + 2.576
6000

406 × (6000 −406)
6000

= (0.0677 −0.0084, 0.0677 + 0.0084) = (0.0593, 0.0761)
Notice that this conﬁdence interval has a length of about 1.7%, which is smaller than the re-
quired length of 2% (since ˆp turned out to be smaller than p∗= 0.1). In conclusion, the overall
proportion of cracked tiles can be reported to be 6.8% ± 1%, with at least 99% conﬁdence.

10.1 INFERENCES ON A POPULATION PROPORTION
453
10.1.4
Problems
10.1.1 Suppose that x = 11 is an observation from a B(32, p)
random variable.
(a) Compute a two-sided 99% conﬁdence interval for p.
(b) Compute a two-sided 95% conﬁdence interval for p.
(c) Compute a one-sided 99% conﬁdence interval that
provides an upper bound for p.
(d) Consider the hypotheses
H0 : p = 0.5
versus
HA : p ̸= 0.5
Calculate an exact p-value using the tail probability
of the binomial distribution and compare it with the
corresponding p-value calculated using a normal
approximation.
10.1.2 Suppose that x = 21 is an observation from a B(27, p)
random variable.
(a) Compute a two-sided 99% conﬁdence interval for p.
(b) Compute a two-sided 95% conﬁdence interval for p.
(c) Compute a one-sided 95% conﬁdence interval that
provides a lower bound for p.
(d) Consider the hypotheses
H0 : p ≤0.6
versus
HA : p > 0.6.
Calculate an exact p-value using the tail probability
of the binomial distribution and compare it with the
corresponding p-value calculated using a normal
approximation.
10.1.3 A random-number generator is supposed to produce a
sequence of 0s and 1s with each value being equally
likely to be a 0 or a 1 and with all values being
independent. In an examination of the random-number
generator, a sequence of 50,000 values is obtained of
which 25,264 are 0s.
(a) Formulate a set of hypotheses to test whether there
is any evidence that the random-number generator is
producing 0s and 1s with unequal probabilities, and
calculate the corresponding p-value.
(b) Compute a two-sided 99% conﬁdence interval for
the probability p that a value produced by the
random-number generator is a 0.
(c) If a two-sided 99% conﬁdence interval for this
probability is required with a total length no larger
than 0.005, how many additional values need to be
investigated?
10.1.4 A new radar system is being developed to detect packages
dropped by airplane. In a series of trials, the radar
detected the packages being dropped 35 times out of
44. Construct a 95% lower conﬁdence bound on the
probability that the radar successfully detects dropped
packages. (This problem is continued in Problem 10.2.3.)
10.1.5 Two experiments are performed. In the ﬁrst experiment
a six-sided die is rolled 50 times and a 6 is scored twice.
In the second experiment the die is rolled 100 times and
a 6 is scored four times. Which of the two experiments
provides the most support for the claim that the die has
been weighted to reduce the chance of scoring a 6?
(Hint: Form a suitable set of hypotheses and compare
the p-values obtained from the two experiments.)
10.1.6 If 21 6s are obtained from 100 rolls of a die, should the
null hypothesis that the probability of scoring a 6 is 1/6
be rejected at the size α = 0.05 level?
10.1.7 A court holds jurisdiction over ﬁve counties, and the
juries are required to be made up in a representative
manner from the eligible populations of these ﬁve
counties. An investigator notices that the county where
she lives has 14% of the total population of the ﬁve
counties eligible for jury duty, yet records reveal that
over the past ﬁve years only 122 out of the 1386 jurors
used by the court reside in her county. Do you feel that
this constitutes reasonable evidence that the jurors are
not being randomly selected from the total population?
10.1.8 In trials of a medical screening test for a particular
illness, 23 cases out of 324 positive results turned out to
be “false-positive” results. The screening test is
acceptable as long as p, the probability of a positive
result being incorrect, is no larger than 10%. Calculate a
p-value for the hypotheses
H0 : p ≥0.1
versus
HA : p < 0.1
Construct a 99% upper conﬁdence bound on p. Do you
think that the screening test is acceptable?
10.1.9 Suppose that you wish to ﬁnd a population proportion p
with accuracy ±1% with 95% conﬁdence. What sample
size n would you recommend if p could be 0.5? What if
the population proportion p can be assumed to be larger
than 0.75?
10.1.10 Suppose that you wish to ﬁnd a population proportion p
with accuracy ±2% with 99% conﬁdence. What sample
size n would you recommend if p could be 0.5? What if
the population proportion p can be assumed to be no
larger than 0.40?

454
CHAPTER 10
DISCRETE DATA ANALYSIS
10.1.11 In experimental bioengineering trials, a successful
outcome was achieved 73 times out of 120 attempts.
Construct a 99% two-sided conﬁdence interval for the
probability of a success under these conditions. If this
probability is required to be known to a precision of
±5%, how many additional trials would you recommend
be run? (This problem is continued in Problem 10.2.8.)
10.1.12 A manufacturer receives a shipment of 100,000
computer chips. A random sample of 200 chips is
examined, and 8 of these are found to be defective.
Construct a 95% conﬁdence level upper bound on the
total number of defective chips in the shipment. (This
problem is continued in Problem 10.2.9.)
10.1.13 A glass tube is designed to withstand a pressure
differential of 1.1 atmospheres. In testing, it was found
that 12 out of 20 tubes could in fact withstand a
pressure differential of 1.5 atmospheres. Calculate a
two-sided 95% conﬁdence interval for the probability
that a tube can withstand a pressure differential of
1.5 atmospheres.
10.1.14 An audit of a federal assistance program implemented
after a major regional disaster discovered that out of 85
randomly selected applications, 17 contained errors due
to either applicant fraud or processing mistakes. If there
were 7607 applications made to the federal assistance
program, calculate a 95% lower bound on the total
number of applications that contained errors. (This
problem is continued in Problem 10.2.10.)
10.1.15 A city councilor asks your advice on how many
householders should be polled in order to gauge the
support for a tax increase to build more schools. The
councilor wants to assess the support to within ±5%
with 95% conﬁdence. What sample size would you
recommend if the councilor advises you that the
householders may be evenly split on the issue? What if
the councilor advises you that fewer than one in three
householders are likely to support the tax increase?
10.1.16 In a particular day, 22 out of 542 visitors to a website
followed a link provided by one of the advertisers.
Calculate a 99% two-sided conﬁdence interval for the
probability that a user of the website will follow a link
provided by an advertiser. (This problem is continued in
Problem 10.2.12.)
10.1.17 Sometimes the following alternative way of constructing
a two-sided conﬁdence interval on a population
proportion p is employed. Recall that there is a
probability of 1 −α that
| ˆp −p|

p(1−p)
n
≤zα/2
By squaring both sides of this inequality and solving the
resulting quadratic expression for p, show that this can
be rewritten
l ≤p ≤u
where
l =
2x + z2
α/2 −zα/2

4x(1 −x/n) + z2
α/2
2(n + z2
α/2)
and
u =
2x + z2
α/2 + zα/2

4x(1 −x/n) + z2
α/2
2(n + z2
α/2)
This result implies that
p ∈(l, u)
is a two-sided 1 −α conﬁdence level conﬁdence interval
for p. Notice that this conﬁdence interval is not
centered at ˆp.
If x = 14 is an observation from a B(39, p)
distribution, compare the 99% two-sided conﬁdence
interval obtained from this method with the standard
99% two-sided conﬁdence interval for p.
10.1.18 The dielectric breakdown strength of an electrical
insulator is deﬁned to be the voltage at which the
insulator starts to leak detectable amounts of electrical
current, and it is an important safety consideration. In
an experiment, 62 insulators of a certain type were
tested at 180◦C, and it was found that 13 had a
dielectric breakdown strength below a speciﬁed
threshold level.
(a) Conduct a hypothesis test to investigate whether this
experiment provides sufﬁcient evidence to conclude
that the probability of an insulator of this type
having a dielectric breakdown strength below the
speciﬁed threshold level is larger than 5%.
(b) Construct a one-sided 95% conﬁdence interval that
provides a lower bound on the probability of an
insulator of this type having a dielectric breakdown
strength below the speciﬁed threshold level.
(This problem is continued in Problem 10.2.13.)
10.1.19 Out of a random sample of 210 parts produced on a
production line, 31 fail a quality inspection. Obtain a
99% two-sided conﬁdence interval for the proportion of

10.2 COMPARING TWO POPULATION PROPORTIONS
455
parts from the production line that will fail the quality
inspection.
10.1.20 A random sample of 38 wheelchair users were asked
whether they preferred cushion type A or B, and 28 of
them preferred type A whereas only 10 of them
preferred type B. Use a hypothesis test to assess whether
it is fair to conclude that cushion type A is at least twice
as popular as cushion type B.
10.1.21 A newspaper reported the results of a political poll about
which candidate likely voters preferred, together with a
note that the margin of error was plus or minus 3.5
percentage points and that the numbers were based on
answers from 793 likely voters. How was this margin of
error calculated? Do you agree with it?
10.1.22 It is claimed that no more than 4 out of 10 small
businesses have adequate accounting records. A survey
was performed to investigate whether there is sufﬁcient
evidence to disprove this claim, and in a sample of
40 small businesses, 22 were found to have adequate
business records. While the survey suggests that the
claim is false, sophisticated statisticians (like us) would
use a statistical inference procedure (such as a
conﬁdence interval or a hypothesis test) to investigate
whether the survey results are statistically signiﬁcant.
A. True
B. False
10.2
Comparing Two Population Proportions
The problem of comparing two population proportions is now considered. Suppose that obser-
vations from population A have a success probability pA and that observations from population
B have a success probability pB. If the random variable X measures the number of successes
observed in a sample of size n from population A, then
X ∼B(n, pA)
and similarly, if the random variable Y measures the number of successes observed in a sample
of size m from population B, then
Y ∼B(m, pB)
The experimenter’s goal is to make inferences on the difference between the two population
proportions
pA −pB
based on observed values x and y of the random variables X and Y.
A good way to do this is to calculate a two-sided conﬁdence interval for pA −pB. Notice
that, as Figure 10.13 illustrates, if the conﬁdence interval contains 0, then it is plausible
that pA = pB and so there is no evidence that the two population proportions are different.
However, if the conﬁdence interval contains only positive values, then this implies that all
the plausible values of the success probabilities satisfy pA > pB, and so it can be concluded
that there is evidence that population A has a larger proportion or success probability than
FIGURE 10.13
Interpretation of conﬁdence
intervals for pA −pB
Two-sided confidence interval for 
Evidence that
pA > pB
pA = pB
pA < pB
pA −pB
Evidence that
that
Plausible
0
0
0

456
CHAPTER 10
DISCRETE DATA ANALYSIS
population B. Similarly, if the conﬁdence interval contains only negative values, then this
implies that all the plausible values of the success probabilities satisfy pA < pB, and so it
can be concluded that there is evidence that population A has a smaller proportion or success
probability than population B.
If the experimenter wants to concentrate on assessing the evidence that the two population
proportions are different, then it is useful to consider the hypotheses
H0 : pA = pB
versus
HA : pA ̸= pB
One-sided hypothesis tests can also be considered, and one-sided conﬁdence intervals for the
difference pA −pB can also be constructed.
The unbiased point estimates of the two population proportions are
ˆpA = x
n
and
ˆpB = y
m
and so an unbiased point estimate of the difference pA −pB can be taken to be the difference
in these two point estimates
ˆpA −ˆpB = x
n −y
m
Conﬁdence Interval Construction
ˆpA = x
n
ˆpB = y
m
Use
z = ( ˆpA −ˆpB) −( pA −pB)
 ˆpA(1−ˆpA)
n
+ ˆpB(1−ˆpB)
m
Hypothesis Testing
H0 : pA = pB
Pooled estimate ˆp = x+y
n+m
Use
z =
ˆpA −ˆpB

ˆp(1 −ˆp)( 1
n + 1
m )
FIGURE 10.14
Comparing two proportions pA and
pB
Furthermore, since
Var( ˆpA) = pA(1 −pA)
n
and
Var( ˆpB) = pB(1 −pB)
m
and since the random variables X and Y are independent, it follows that
Var( ˆpA −ˆpB) = Var( ˆpA) + Var( ˆpB) = pA(1 −pA)
n
+ pB(1 −pB)
m
Both large-sample conﬁdence interval construction and hypothesis testing are based on the
formation of appropriate z-statistics with standard normal distributions, but as Figure 10.14
illustrates, they differ in the manner in which the variance term is estimated. In conﬁdence
interval construction the unknown population proportions pA and pB in the variance term are
replaced by their point estimates, and the z-statistic
z = ( ˆpA −ˆpB) −(pA −pB)

ˆpA(1−ˆpA)
n
+ ˆpB(1−ˆpB)
m
= ( ˆpA −ˆpB) −(pA −pB)

x(n−x)
n3
+ y(m−y)
m3
is employed.
For hypothesis tests with a null hypothesis H0 : pA = pB, a pooled estimate of the
common success probability
ˆp = x + y
n + m
is used, which is employed in place of both of the unknown population proportions pA and
pB in the variance term. This results in a z-statistic
z =
ˆpA −ˆpB

ˆp(1 −ˆp)
 1
n + 1
m

10.2.1
Conﬁdence Intervals for the Difference between Two Population Proportions
For large enough sample sizes n and m, the z-statistic
z = ( ˆpA −ˆpB) −(pA −pB)

ˆpA(1−ˆpA)
n
+ ˆpB(1−ˆpB)
m
= ( ˆpA −ˆpB) −(pA −pB)

x(n−x)
n3
+ y(m−y)
m3

10.2 COMPARING TWO POPULATION PROPORTIONS
457
can be taken to be an observation from a standard normal distribution. Roughly speaking,
sample sizes for which npA, n(1−pA), mpB, and m(1−pB) are all larger than 5 are adequate
forthenormalapproximationtobeappropriate,andtheseconditionscanbetakentobesatisﬁed
as long as x, n −x, y, and m −y are all larger than 5. Two-sided and one-sided conﬁdence
intervals for the difference pA −pB, given in the accompanying box, are obtained in the usual
way by bounding the z-statistic with critical points from the standard normal distribution.
Conﬁdence Intervals for the Difference of Two Population Proportions
If a random variable X has a B(n, pA) distribution and a random variable Y has a
B(m, pB) distribution, then an approximate two-sided 1 −α conﬁdence level
conﬁdence interval for the difference between the success probabilities pA −pB based
upon observed values x and y of the random variables is
pA −pB ∈
	
ˆpA −ˆpB −zα/2

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m
,
ˆpA −ˆpB + zα/2

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m

where the estimated success probabilities are ˆpA = x/n and ˆpB = y/m. This
conﬁdence interval can also be written
pA −pB ∈
	
ˆpA −ˆpB −zα/2

x(n −x)
n3
+ y(m −y)
m3
,
ˆpA −ˆpB + zα/2

x(n −x)
n3
+ y(m −y)
m3

One-sided approximate 1 −α conﬁdence level conﬁdence intervals are
pA −pB ∈
	
ˆpA −ˆpB −zα

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m
, 1

=
	
ˆpA −ˆpB −zα

x(n −x)
n3
+ y(m −y)
m3
, 1

and
pA −pB ∈
	
−1, ˆpA −ˆpB + zα

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m

=
	
−1, ˆpA −ˆpB + zα

x(n −x)
n3
+ y(m −y)
m3

The approximations are reasonable as long as x, n −x, y, and m −y are all larger
than 5.
Example 57
Building Tile Cracks
Recall that a combination of two surveys of the tiles on a group of buildings, buildings A
say, revealed a total of x = 406 cracked tiles out of n = 6000. Suppose that another group
of buildings in another part of town, buildings B, were constructed about the same time as

458
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.15
Analysis of building tile cracks
x = 406 cracked tiles out of n = 6000
y = 83 cracked tiles out of m = 2000
ˆpA = 406
6000 = 0.0677
99% two-sided confidence interval
Conclusion: evidence that  pA > pB
pA −pB ∈(0.0120, 0.0404)
Buildings A
Buildings B
ˆpB =
= 0.0415
83
2000
buildings A and have exterior walls composed of the same type of tiles. However, the tiles
on buildings B were cemented into place with a different resin mixture than that used on
buildings A. The construction engineers are interested in investigating whether the two types
of resin mixtures have different expansion and contraction properties that affect the chances
of the tiles becoming cracked.
As Figure 10.15 shows, a representative sample of m = 2000 tiles on buildings B is
examined, and y = 83 are found to be cracked. If pA represents the probability that a tile on
buildings A becomes cracked, and if pB represents the probability that a tile on buildings B
becomes cracked, then
ˆpA = x
n = 406
6000 = 0.0677
and
ˆpB = y
m =
83
2000 = 0.0415
With z0.005 = 2.576, a two-sided 99% conﬁdence interval for the difference in these proba-
bilities is
pA −pB ∈
	
ˆpA −ˆpB −zα/2

x(n −x)
n3
+ y(m −y)
m3
,
ˆpA −ˆpB + zα/2

x(n −x)
n3
+ y(m −y)
m3

=
	
0.0677 −0.0415 −2.576

406 × (6000 −406)
60003
+ 83 × (2000 −83)
20003
,
0.0677 −0.0415 + 2.576

406 × (6000 −406)
60003
+ 83 × (2000 −83)
20003

= (0.0262 −0.0142, 0.0262 + 0.0142) = (0.0120, 0.0404)

10.2 COMPARING TWO POPULATION PROPORTIONS
459
The fact that this conﬁdence interval contains only positive values indicates that pA > pB,
so that the resin mixture employed on buildings B is better than the resin mixture employed
on buildings A. More speciﬁcally, the conﬁdence interval indicates that the resin mixture
employed on buildings A has a probability of causing a tile to crack somewhere between
about 1.2% and 4.0% larger than the resin mixture employed on buildings B.
Example 58
Overage Weedkiller
Product
The chemical company, company A, not only is interested in the proportion of its own weed-
killer product that is overage, but in addition is interested in the overage proportion of its
main competitor’s weedkiller brand, produced by company B. Therefore, the auditors in the
nationwide sampling scheme are also instructed to investigate the shelf product of company B
to determine whether or not it is overage.
Recall that the auditors examined n = 54,965 weedkiller containers of company A’s
product and found that x = 2779 of them were overage. This ﬁnding results in an estimate of
company A’s overage proportion pA of
ˆpA = x
n =
2779
54,965 = 0.0506
In addition, the auditors examined m = 47,892 weedkiller containers of company B’s product
and found that x = 3298 of them were overage, which provides an estimate of company B’s
overage proportion pB of
ˆpB = y
m =
3298
47,892 = 0.0689
A two-sided 99% conﬁdence interval for the difference in these probabilities is
pA −pB ∈
	
0.0506 −0.0689 −
2.576 ×

2779 × (54,965 −2779)
54,9653
+ 3298 × (47,892 −3298)
47,8923
,
0.0506 −0.0689 +
2.576 ×

2779 × (54,965 −2779)
54,9653
+ 3298 × (47,892 −3298)
47,8923

= (−0.0183 −0.0038, −0.0183 + 0.0038) = (−0.0221, −0.0145)
This conﬁdence interval contains only negative values, which indicates that pA < pB. Thus,
the sampling has provided evidence that company B has proportionally more overage product
on sale than company A. However, the difference in the proportions is quite small, lying
somewhere between about 1.4% and 2.2%.
Example 59
Opossum Progeny
Genders
In the study of evolutionary behavior, the Trivers-Willard hypothesis indicates that healthy
parents should tend to have more male offspring than female, and that weaker parents should
tend to have more female offspring than male. This tendency may maximize the number of
each parent’s grandchildren (and thus help ensure that its genetic code is preserved) since a
healthy male offspring can win many mates, but a relatively unhealthy offspring has the best
chance of mating if it is a female.
In an experiment to examine this hypothesis, a group of 40 opossums were monitored
and 20 of them were given an enhanced diet. Suppose that after a certain period of time, the

460
CHAPTER 10
DISCRETE DATA ANALYSIS
opossums with the enhanced diet had raised 19 male offspring and 14 female offspring, and
the opossums without the enhanced diet had raised 15 male offspring and 15 female offspring.
Does this ﬁnding provide any evidence in support of the Trivers-Willard hypothesis?
Let pA be the probability that opossums on the enhanced diet have a male offspring, and
let pB be the probability that opossums without the enhanced diet have a male offspring.
Then
ˆpA =
19
19 + 14 = 0.576
and
ˆpB =
15
15 + 15 = 0.500
The Trivers-Willard hypothesis suggests that the difference pA −pB should be positive, and
this can be examined by obtaining a one-sided conﬁdence interval providing a lower bound
on pA −pB. With z0.05 = 1.645, a 95% conﬁdence interval of this kind is
pA −pB ∈
	
ˆpA −ˆpB −zα

x(n −x)
n3
+ y(m −y)
m3
, 1

=
	
0.576 −0.500 −1.645

19 × (33 −19)
333
+ 15 × (30 −15)
303
, 1

= (0.076 −0.206, 1) = (−0.130, 1)
This conﬁdence interval contains some negative values and so clearly it is plausible that
pA ≤pB. Consequently, this experiment does not provide any signiﬁcant evidence to substan-
tiate the Trivers-Willard hypothesis. Of course, this experiment does not disprove the Trivers-
Willard hypothesis, and the collection of more data may demonstrate it to be valid in this
situation.
10.2.2
Hypothesis Tests on the Difference between Two Population Proportions
If an experimenter wants to concentrate on assessing the evidence that two population pro-
portions pA and pB are different, then it is useful to consider the two-sided hypotheses
H0 : pA = pB
versus
HA : pA ̸= pB
or the associated one-sided hypotheses. Since the null hypothesis speciﬁes that the two pro-
portions are identical, it is appropriate to employ a pooled estimate of the common success
probability
ˆp = x + y
n + m
in the estimation of the variance of ˆpA −ˆpB. This estimate results in a z-statistic
z =
ˆpA −ˆpB

ˆp(1 −ˆp)
 1
n + 1
m

which with sufﬁciently large sample sizes, can be taken to be an observation from a standard
normal distribution when the null hypothesis is true. The calculation of p-values is performed
intheusualmannerbycomparingthe z-statisticwithastandardnormaldistribution,asoutlined

10.2 COMPARING TWO POPULATION PROPORTIONS
461
in the accompanying box. The determination of whether a ﬁxed size hypothesis test accepts
or rejects the null hypothesis is similarly made in the usual manner.
Hypothesis Tests of the Equality of Two Population Proportions
Suppose that x is an observation from a B(n, pA) distribution and that y is an
observation from a B(m, pB) distribution. Then the two-sided hypothesis testing
problem
H0 : pA = pB
versus
HA : pA ̸= pB
has a p-value
p-value = 2 × (−|z|)
where
z =
ˆpA −ˆpB

ˆp(1 −ˆp)
 1
n + 1
m

and
ˆp = x + y
n + m
A size α hypothesis test accepts the null hypothesis when
|z| ≤zα/2
and rejects the null hypothesis when
|z| > zα/2
The one-sided hypothesis testing problem
H0 : pA −pB ≥0
versus
HA : pA −pB < 0
has a p-value
p-value = (z)
and a size α hypothesis test accepts the null hypothesis when
z ≥−zα
and rejects the null hypothesis when
z < −zα
The one-sided hypothesis testing problem
H0 : pA −pB ≤0
versus
HA : pA −pB > 0
has a p-value
p-value = 1 −(z)
and a size α hypothesis test accepts the null hypothesis when
z ≤zα
and rejects the null hypothesis when
z > zα

462
CHAPTER 10
DISCRETE DATA ANALYSIS
Example 61
Political Polling
When polling the agreement with the statement
The city mayor is doing a good job.
the local newspaper is also interested in how a person’s support for this statement may de-
pend upon his or her age. Therefore the pollsters also gather information on the ages of the
respondents in their random sample.
As Figure 10.16 shows, the polling results consist of n = 952 people aged 18 to 39 of
whom x = 627 agree with the statement, and m = 1043 people aged at least 40 of whom
y = 421 agree with the statement. The estimate of pA, the proportion of the younger group
who agree with the statement, is therefore
ˆpA = 627
952 = 0.659
and the estimate of pB, the proportion of the older group who agree with the statement, is
therefore
ˆpB = 421
1043 = 0.404
Does the strength of support for the statement differ between the two age groups? This
question can be examined with the two-sided hypotheses
H0 : pA = pB
versus
HA : pA ̸= pB
Acceptance of the null hypothesis implies that there is no evidence of a difference in the
proportions of the two age groups who agree with the statement, whereas rejection of the null
hypothesis indicates that there is evidence of a difference between the two age groups.
FIGURE 10.16
Political polling example
Random sample of n = 952
Random sample of m = 1043
age 18--39
age ≥ 40
A
B
The city mayor is doing a good job.,,
,,
H0 : p A = pB, HA : p A ̸= pB
Pooled estimate ˆp =
= 0.525
Conclusion: The null hypothesis is not plausible. There is evidence that the proportion 
    agreeing with the statement differs between the two age groups.
z = 11.39, p-value = 2 × (−11.39)
0
≃
Agree: x = 627
Disagree: n − x = 325
pA
= 0.659
627
952
=
ˆ
Agree: y = 421
Disagree: m − y = 622
B
p
= 0.404
421
1043
=
ˆ
627
421
+
952
1043
+
Population

10.2 COMPARING TWO POPULATION PROPORTIONS
463
The pooled estimate of a common proportion is
ˆp = x + y
n + m = 627 + 421
952 + 1043 = 0.525
and the z-statistic is
z =
ˆpA −ˆpB

ˆp(1 −ˆp)
 1
n + 1
m

=
0.659 −0.404

0.525 × (1.000 −0.525) ×
 1
952 +
1
1043
 = 11.39
The p-value is therefore
p-value = 2 × (−11.39) ≃0
Consequently, the null hypothesis has been shown to be not at all plausible, and the poll has
demonstrated a difference in agreement with the statement between the two age groups.
In fact, a two-sided 99% conﬁdence interval for the difference between the proportions
who agree with the statement is
pA−pB ∈
	
0.659 −0.404 −2.576

627 × (952 −627)
9523
+ 421 × (1043 −421)
10433
,
0.659 −0.404 + 2.576

627 × (952 −627)
9523
+ 421 × (1043 −421)
10433

= (0.255 −0.056, 0.255 + 0.056) = (0.199, 0.311)
Therefore the poll shows that the proportion of the younger group who agree with the statement
is somewhere between about 20% to 30% larger than the proportion of the older group who
agree.
Example 59
Opossum Progeny
Genders
Recall that x = 19 of the n = 33 offspring raised by opossums with the enhanced diet are male,
and that y = 15 of the m = 30 offspring raised by opossums without the enhanced diet are
male. The Trivers-Willard hypothesis suggests that pA > pB, and it can be tested with the
one-sided hypotheses
H0 : pA −pB ≤0
versus
HA : pA −pB > 0
Acceptance of the null hypothesis indicates that there is not sufﬁcient evidence to establish
that the Trivers-Willard hypothesis is true, whereas rejection of the null hypothesis indicates
that there is sufﬁcient evidence to substantiate the Trivers-Willard hypothesis.
The pooled estimate of the probability of a male offspring is
ˆp = x + y
n + m = 19 + 15
33 + 30 = 0.540
and the z-statistic is
z =
0.576 −0.500

0.540 × (1.000 −0.540) ×
 1
33 + 1
30
 = 0.60

464
CHAPTER 10
DISCRETE DATA ANALYSIS
The p-value is therefore
p-value = 1 −(0.60) = 1 −0.7257 ≃0.27
Such a large p-value implies that there is no reason to conclude that the null hypothesis is
not plausible, and so, as was found from the previous construction of a one-sided conﬁdence
interval for pA−pB, this experiment does not provide any substantiation of the Trivers-Willard
hypothesis.
Figure 10.17 provides a summary chart of conﬁdence interval construction and hypothesis
testing procedures for comparing two population proportions.
Population A
Population B
x successes out of n trials
y successes out of m trials
pA = probability of success
pB = probability of success
ˆpA = x
n
ˆpB = y
m
Two-sided 1−α level conﬁdence interval
Two-sided 1−α level conﬁdence interval
One-sided 1−α level conﬁdence interval
pA −pB ∈
 
ˆpA −ˆpB −zα/2

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m
, ˆpA −ˆpB + zα/2

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m


=
 
ˆpA −ˆpB −zα/2

x(n −x)
n3
+ y(m −y)
m3
, ˆpA −ˆpB + zα/2

x(n −x)
n3
+ y(m −y)
m3


pA −pB ∈
 
ˆpA −ˆpB −zα

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m
, 1

=
 
ˆpA −ˆpB −zα

x(n −x)
n3
+ y(m −y)
m3
, 1



pA −pB ∈
 
−1, ˆpA −ˆpB + zα

ˆpA(1 −ˆpA)
n
+ ˆpB(1 −ˆpB)
m


=
 
−1, ˆpA −ˆpB + zα

x(n −x)
n3
+ y(m −y)
m3


Hypothesis testing
ˆp = x + y
n + m
z =
ˆpA −ˆpB

ˆp(1 −ˆp)
)
)
 1
n + 1
m
H0 : pA = pB, HA : pA ̸= pB
H0 : pA −pB ≤0, HA : pA −pB > 0
p-value = 2 × (−|z|)
p-value = 1 −(z)
Size α test
Size α test
accept H0 : |z| ≤zα/2
accept H0 : z ≤zα
H0 : pA −pB ≥0, HA : pA −pB < 0
p-value = (z)
Size α test
accept H0 : z ≥−zα
reject H0 : z < −zα
reject H0 : |z| > zα/2
reject H0 : z > zα
FIGURE 10.17
Summary of inference procedures for comparing two population proportions (valid when x, n −x, m, and m −y are all larger than 5)

10.2 COMPARING TWO POPULATION PROPORTIONS
465
10.2.3
Problems
10.2.1 Suppose that x = 14 is an observation from a B(37, pA)
random variable, and that y = 7 is an observation from a
B(26, pB) random variable.
(a) Compute a two-sided 99% conﬁdence interval for
pA −pB.
(b) Compute a two-sided 95% conﬁdence interval for
pA −pB.
(c) Compute a one-sided 99% conﬁdence interval that
provides a lower bound for pA −pB.
(d) Calculate the p-value for the test of the hypotheses
H0 : pA = pB
versus
HA : pA ̸= pB
10.2.2 Suppose that x = 261 is an observation from a
B(302, pA) random variable, and that y = 401 is an
observation from a B(454, pB) random variable.
(a) Compute a two-sided 99% conﬁdence interval for
pA −pB.
(b) Compute a two-sided 90% conﬁdence interval for
pA −pB.
(c) Compute a one-sided 95% conﬁdence interval that
provides an upper bound for pA −pB.
(d) Calculate the p-value for the test of the hypotheses
H0 : pA = pB
versus
HA : pA ̸= pB
10.2.3 Suppose that the abilities of two new radar systems to
detect packages dropped by airplane are being
compared. In a series of trials, radar system A detected
the packages being dropped 35 times out of 44, while
radar system B detected the packages being dropped 36
times out of 52.
(a) Construct a 99% two-sided conﬁdence interval
for the differences between the probabilities that
the radar systems successfully detect dropped
packages.
(b) Calculate the p-value for the test of the two-sided
null hypothesis that the two radar systems are
equally effective.
Interpret your answers.
10.2.4 Die A is rolled 50 times and a 6 is scored 4 times, while
a 6 is obtained 10 times when die B is rolled 50 times.
(a) Construct a two-sided 99% conﬁdence interval for
the difference in the probabilities of scoring a 6 on
the two dice.
(b) Calculate a p-value for the two-sided null hypo-
thesis that the two dice have equal probabilities of
scoring a 6.
(c) What would your answers be if die A produced a 6
40 times in 500 rolls and die B produced a 6 100
times in 500 rolls?
10.2.5 In an experiment to determine the best conditions to
produce suitable crystals for the recovery and
puriﬁcation of biological molecules such as enzymes
and proteins, crystals had appeared within 24 hours in
27 out of 60 trials of a particular solution without seed
crystals, and had appeared within 24 hours in 36 out of
60 trials of a particular solution with seed crystals.
Construct a one-sided conﬁdence interval and calculate
a one-sided p-value to investigate the evidence that the
presence of seed crystals increases the probability of
crystallization within 24 hours using this method?
(This problem is continued in Problem 10.7.1.)
10.2.6 A new drug is being compared with a standard drug for
treating a particular illness. In the clinical trials, a group
of 200 patients was randomly split into two groups, with
one group being given the standard drug and one group
the new drug. Altogether, 83 out of the 100 patients given
the new drug improved their condition, while only 72 out
of the 100 patients given the standard drug improved their
condition. Construct a one-sided conﬁdence interval and
calculate a one-sided p-value to investigate the evidence
that the new drug is better than the standard drug.
10.2.7 A company has two production lines for constructing
television sets. Over a certain period of time, 23 out of
1128 television sets from production line A are found
not to meet the company’s quality standards, while 24
out of 962 television sets from production line B are
found not to meet the company’s quality standards. Use
a two-sided conﬁdence interval and a two-sided p-value
to assess the evidence of a difference in operating
standards between the two production lines.
10.2.8 In experimental bioengineering trials, a successful
outcome was achieved 73 times out of 120 attempts
using a standard procedure, whereas with a new
procedure a successful outcome was achieved 101 times
out of 120. What is the evidence that the new procedure
is better than the standard procedure?
10.2.9 A manufacturing company has to choose between two
potential suppliers of computer chips. A random sample
of 200 chips from supplier A is examined and 8 are
found to be defective, while 13 chips out of a random

466
CHAPTER 10
DISCRETE DATA ANALYSIS
sample of 250 chips from supplier B are found to be
defective. Use two-sided inference procedures to assess
whether this ﬁnding should inﬂuence the company’s
choice of supplier.
10.2.10 An audit of a federal assistance program implemented
after a major regional disaster discovered that out of
85 randomly selected applications processed during the
ﬁrst two weeks after the disaster, 17 contained errors due
to either applicant fraud or processing mistakes.
However, out of 132 randomly selected applications
processed later than the ﬁrst two weeks after the disaster,
only 16 contained errors. Does this information
substantiate the contention that errors in the assistance
applications are more likely in the initial aftermath of
the disaster?
10.2.11 Two scanning machines are compared. When 185 items
were scanned with machine A, 159 of the the scans were
free of errors. When the same 185 items were scanned
with machine B, only 138 of the scans were free of
errors. Is this sufﬁcient evidence to conclude that in
general the probability of an error-free scan is higher for
machine A than for machine B?
10.2.12 Recall from Problem 10.1.16 that in a particular day, 22
out of 542 visitors to a website followed a link provided
by an advertiser. After the advertisements were
modiﬁed, it was found that 64 out of 601 visitors to the
website on a day followed the link. Is there any evidence
that the modiﬁcations to the advertisements attracted
more customers?
10.2.13 Consider again Problem 10.1.18 where 62 insulators of a
certain type were tested at 180◦C, and it was found that
13 had a dielectric breakdown strength below a speciﬁed
threshold level. In addition, 70 insulators of the same
type were tested at 250◦C, and it was found that 20 had a
dielectric breakdown strength below the speciﬁed
threshold level.
(a) Conduct a one-sided hypothesis test to investigate
whether this experiment provides sufﬁcient
evidence to conclude that the probability of an
insulator of this type having a dielectric breakdown
strength below the speciﬁed threshold level is larger
at 250◦C than it is at 180◦C.
(b) Construct a two-sided 99% conﬁdence interval for
the difference between the probabilities of an
insulator of this type having a dielectric breakdown
strength below the speciﬁed threshold level at
180◦C and at 250◦C.
10.2.14 A group of 250 patients was randomly split into two
groups of 125 patients. The ﬁrst group of 125 patients
was given treatment A, and 72 of them improved their
condition. The second group of 125 patients was given
treatment B, and 60 of them improved their condition.
Perform a hypothesis test to investigate whether there is
evidence of a difference between the two treatments.
10.2.15 A company is performing a failure analysis for two of
its products. It found that for the ﬁrst product 76 out of
243 failures were due to operator misuse, while for the
second product 122 out of 320 failures were due to
operator misuse. Construct a 99% two-sided conﬁdence
interval for the difference between the two products of
the probabilities that a failure is due to operator misuse.
Based on this conﬁdence interval, is there evidence that
these probabilities are different for the two products?
10.2.16 A conﬁdence interval is obtained for the difference
between two probabilities p1 −p2.
A. If the conﬁdence interval contains zero, then it is
plausible that the two probabilities are equal.
B. If the conﬁdence interval contains only positive
values, then this provides evidence (at the given
conﬁdence level) that p1 > p2.
C. Both of the above.
D. None of the above.
10.3
Goodness of Fit Tests for One-Way Contingency Tables
In this section the analysis of classiﬁcations with more than two levels is considered. Thus,
data sets are considered where each unit is assigned to one of three or more different categories.
Whereas the binomial distribution was appropriate to analyze classiﬁcations with two levels,
the multinomial distribution is appropriate when there are three or more classiﬁcation levels.
Hypothesis tests are described for assessing whether the probability vector of the multino-
mial distribution takes a speciﬁed value. In particular, the hypothesis of homogeneity (which
states that every classiﬁcation is equally likely) is commonly of interest. Goodness of ﬁt tests,

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
467
often referred to as chi-square tests, are used to test the hypothesis. These methods can also
be used to test the distributional assumptions of a data set.
10.3.1
One-Way Classiﬁcations
Consider the data set illustrated in Figure 10.18. Each of n observations is classiﬁed into one
(and only one) of k categories or cells. The resulting cell frequencies are
x1, x2, . . . , xk
with
x1 + x2 + · · · + xk = n
For a ﬁxed total sample size n, data sets of this kind can be modeled with a multinomial
distribution that depends upon a set of cell probabilities
p1, p2, . . . , pk
with
p1 + p2 + · · · + pk = 1
The cell frequency xi is an observation from a B(n, pi) distribution, so a particular cell
probability pi can be estimated by
ˆpi = xi
n
Furthermore, the methods described in Section 10.1 can be used to make inferences on a
particular cell probability pi either through conﬁdence interval construction or with hypothesis
testing.
FIGURE 10.18
One-way classiﬁcation
Population
Random sample size n
Cell frequency
Classification
Cell frequency
x1
x2
Cell frequency
k
x
Category 1
Category 2
Category k
x1 + x2 + . . . + xk = n
. . .
. . .

468
CHAPTER 10
DISCRETE DATA ANALYSIS
However, it can be useful to assess the plausibility that the cell probabilities, taken all
together rather than individually, take a set of speciﬁed values
p∗
1, p∗
2, . . . , p∗
k
with
p∗
1 + · · · + p∗
k = 1
In other words, it can be useful to examine the null hypothesis
H0 : pi = p∗
i
1 ≤i ≤k
for speciﬁed values p∗
1, . . . , p∗
k. This can be accomplished with a chi-square goodness of
ﬁt test. The null hypothesis of homogeneity is often of interest and states that the k cell
probabilities are all equal, so that
p∗
i = 1
k
1 ≤i ≤k
Notice that this hypothesis testing problem is intrinsically a two-sided problem. There are
no one-sided versions of it. The implied alternative hypothesis, which is usually not stated, is
that the null hypothesis is false. Thus, it consists of all the sets of probability values p1, . . . , pk
except for the speciﬁc set of values p∗
1, . . . , p∗
k.
Example 1
Machine Breakdowns
Recall that out of n = 46 machine breakdowns, x1 = 9 are attributable to electrical problems,
x2 = 24 are attributable to mechanical problems, and x3 = 13 are attributable to operator
misuse. It is suggested that the probabilities of these three kinds of breakdown are respectively
p∗
1 = 0.2,
p∗
2 = 0.5,
p∗
3 = 0.3
The plausibility of this suggestion can be examined with a chi-square goodness of ﬁt test.
Example 13
Factory Floor
Accidents
A factory is embarking on a new safety drive in an attempt to reduce the number of accidents
occurring on the factory ﬂoor. A manager checks back through the records of factory ﬂoor
accidents and ﬁnds the day of the week on which each of the last n = 270 accidents occurred,
as shown in Figure 10.19. It appears that accidents are more likely on Mondays and Fridays
than on other days. If this is really the case, then it is sensible to be particularly vigilant on
these days or to make some changes to reduce the chances of accidents occurring on these
days.
Does the data set really provide evidence that accidents are more likely on Mondays and
Fridays than on other days? The null hypothesis of homogeneity
H0 : pi = 1
5
1 ≤i ≤5
states that accidents are equally likely to occur on any day of the week. If this hypothesis is
plausible, then there is no evidence that accidents are more likely on any one day than on
another. However, if the hypothesis is rejected, then it can be taken as evidence that accidents
are more likely on Mondays and Fridays than on the other days.
FIGURE 10.19
Day of the week of factory ﬂoor
accidents
Day of week
Monday
Tuesday
Wednesday
Thursday
Friday
Number of accidents
65
43
48
41
73
n = 270

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
469
FIGURE 10.20
Observed and expected cell
frequencies
Sample size n
H0: p1 = p∗
1, p2 = p∗
2, . . . , pk = p∗
k
Expected cell frequencies
e1 = np∗
1, e2 = np∗
2, . . . , ek = np∗
k
Observed cell frequencies
x1, x2, . . . , x
Data values
k
Null hypothesis
The null hypothesis
H0 : pi = p∗
i
1 ≤i ≤k
is tested by comparing a set of observed cell frequencies
x1, x2, . . . , xk
with a set of expected cell frequencies
e1, e2, . . . , ek
As Figure 10.20 illustrates, the observed cell frequencies xi are simply the data values ob-
served in each of the cells (more technically they are the realizations of the random variables
X1, . . . , Xk, which have a multinomial distribution), and the expected cell frequencies ei are
given by
ei = np∗
i
1 ≤i ≤k
Thus the expected cell frequencies ei are the expected values of the multinomial random
variables X1, . . . , Xk when the null hypothesis is true. Notice that in contrast to the observed
cell frequencies xi, the expected cell frequencies ei need not take integer values, but that
e1 + e2 + · · · + ek = n
A goodness of ﬁt test operates by measuring the discrepancy between the observed cell
frequencies xi and the expected cell frequencies ei. The closer these sets of frequencies are to
each other, the more plausible is the null hypothesis. Conversely, the farther apart these sets
of frequencies are, the less plausible is the null hypothesis.
HISTORICAL NOTE
Karl Pearson (1857–1936) was
one of the founders of modern
statistics. He was born in
London, England, and
practiced law for three years
and published two literary
works before he was appointed
professor of applied
mathematics and mechanics at
University College, London, in
1884, where he taught until his
retirement in 1933. His work
on the chi-square statistic
began in 1893 through his
attention to the problem of
applying statistical techniques
to the biological problems of
heredity and evolution. He was
a socialist and a self-described
“free-thinker.”
Statistics that measure the discrepancy between the two sets of cell frequencies are usually
called chi-square statistics, since a p-value is obtained by comparing them with a chi-square
distribution. Two common ones are
X2 =
k

i=1
(xi −ei)2
ei
and
G2 = 2
k

i=1
xi ln
 xi
ei

The former is known as the Pearson chi-square statistic, and the latter is known as the
likelihood ratio chi-square statistic. These statistics both take positive values, and larger
values of the statistics indicate a greater discrepancy between the two sets of cell frequencies.
In the unlikely circumstance that the observed cell frequencies are all exactly equal to the

470
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.21
P-value calculation for chi-square
goodness of ﬁt tests
p-value
χ2
k−1 distribution
chi-square statistic
X2 or G2
P(χ2
≥X2)
= P(χ         ≥ G  )
p
p
-value =
-value
k−1
k−1   
2
2
or
expected cell frequencies so that
xi = ei
1 ≤i ≤k
then
X2 = G2 = 0
indicating a “perfect ﬁt.” The two chi-square statistics arise from two different mathematical
approaches to the testing problem, and there is generally no reason to prefer one statistic to
the other (many statistical software packages provide both statistics).
A p-value is obtained by comparing the chi-square statistic, X2 or G2, with a chi-square
distribution with k −1 degrees of freedom. Speciﬁcally, the p-value is given by
p-value = P
χ2
k−1 ≥X2
or
p-value = P
χ2
k−1 ≥G2
as shown in Figure 10.21. Consequently, a size α hypothesis test accepts the null hypothesis
if the chi-square statistic is no larger than a critical point χ2
α,k−1 and rejects the null hypothesis
if the chi-square statistic is larger than the critical point.
The values of the Pearson chi-square statistic X2 and the likelihood ratio chi-square
statistic G2 are generally very close together, and so it usually makes little difference which
one is employed. The p-value calculations are based upon an asymptotic (large expected cell
frequencies) chi-square distribution for the test statistics, and this can be considered to be
appropriate as long as each cell frequency ei is no smaller than 5. If a cell has an expected
frequency ei less than 5, then standard practice is to group it with another cell, thereby reducing
the number of cells k, so that the new grouped cell has an expected frequency of at least 5.
Sometimes, three or more cells need to be grouped together for this purpose.
Notice that the p-value is obtained by comparing the chi-square statistic with a chi-square
distribution with degrees of freedom k −1, which is one less than the total number of cells. In
situations where the hypothesized cell probabilities p∗
1, . . . , p∗
k depend upon the data values
x1, . . . , xk in some manner, smaller degrees of freedom are appropriate, as illustrated in the
following section on testing distributional assumptions.

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
471
Example 1
Machine Breakdowns
Consider the null hypothesis
H0 : p1 = 0.2, p2 = 0.5, p3 = 0.3
Under this null hypothesis the expected cell frequencies are
e1 = np∗
1 = 46 × 0.2 = 9.2
e2 = np∗
2 = 46 × 0.5 = 23.0
e3 = np∗
3 = 46 × 0.3 = 13.8
As Figure 10.22 shows, the chi-square statistics are
X2 = 0.0942
and
G2 = 0.0945
which, compared with a chi-square distribution with k −1 = 3 −1 = 2 degrees of freedom
(which is an exponential distribution with mean 2), give a p-value of
p-value ≃P
χ2
2 ≥0.094

= 0.95
Goodness of Fit Tests for One-Way Contingency Tables
Consider a multinomial distribution with k cells and a set of unknown cell
probabilities p1, . . . , pk. Based upon a set of observed cell frequencies
x1, . . . , xk with x1 + · · · + xk = n, the null hypothesis
H0 : pi = p∗
i
1 ≤i ≤k
which states that the cell probabilities take the speciﬁc set of values p∗
1, . . . , p∗
k, has a
p-value that can be calculated as either
p-value = P
χ2
k−1 ≥X2
or
p-value = P
χ2
k−1 ≥G2
where the chi-square test statistics are
X2 =
k

i=1
(xi −ei)2
ei
and
G2 = 2
k

i=1
xi ln
 xi
ei

with expected cell frequencies
ei = np∗
i
1 ≤i ≤k
The two p-values calculated in this manner are usually similar, although they may
differ slightly, and they are appropriate as long as the expected cell frequencies ei are
each no smaller than 5.
At size α, the null hypothesis is accepted if
X2 ≤χ2
α,k−1
(or if G2 ≤χ2
α,k−1), and the null hypothesis is rejected if
X2 > χ2
α,k−1
(or if G2 > χ2
α,k−1).

472
CHAPTER 10
DISCRETE DATA ANALYSIS
0.094
2 distribution
2
p-value = P(
2
2 ≥0.094)
0.95
H0 : p1 = 0.2, p2 = 0.5, p3 = 0.3
Electrical
Mechanical
Operator misuse
Observed cell frequencies
x1 = 9
x2 = 24
x3 = 13
n = 46
Expected cell frequencies
e1 = 46 × 0.2 = 9.2
e2 = 46 × 0.5 = 23.0
e3 = 46 × 0.3 = 13.8
n = 46
X2 = (9.0 −9.2)2
9.2
+ (24.0 −23.0)2
23.0
+ (13.0 −13.8)2
13.8
= 0.0942
G2 = 2 ×

9.0 × ln
 9.0
9.2

+ 24.0 × ln
 24.0
23.0

+ 13.0 × ln
 13.0
13.8

= 0.0945
Pearson chi-square statistic:
Likelihood ratio chi-square statistic:
Null hypothesis is plausible
 Conclusion:
χ
χ
∼
FIGURE 10.22
Goodness of ﬁt test for the machine breakdown example
Clearly the null hypothesis is plausible, and such a large p-value indicates that there is a very
close ﬁt between the observed cell frequencies x1, x2, x3 and the expected cell frequencies
e1, e2, e3, as might be observed from a visual comparison of their values.
Of course, the fact that the null hypothesis is plausible does not mean that it has been proven
to be true. There are sets of plausible cell probabilities p1, p2, p3 other than the hypothesized
values 0.2, 0.5, and 0.3. In fact, with z0.025 = 1.96, the method described in Section 10.1 can
be used to obtain a 95% conﬁdence interval for p2, the probability that a machine breakdown
can be attributed to a mechanical failure, as
p2 ∈
	
24
46 −1.96
46

24 × (46 −24)
46
, 24
46 + 1.96
46

24 × (46 −24)
46

= (0.522 −0.144, 0.522 + 0.144) = (0.378, 0.666)
Is the hypothesis of homogeneity plausible here? In other words, is it plausible that the
three kinds of machine breakdown are equally likely? Under this hypothesis, the expected cell
frequencies are
e1 = e2 = e3 = 46
3 = 15.33
and the Pearson chi-square statistic is
X2 = (9.00 −15.33)2
15.33
+ (24.00 −15.33)2
15.33
+ (13.00 −15.33)2
15.33
= 7.87

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
473
This value is much larger than the previous value of 0.0942, which indicates, as expected, that
the hypothesis of homogeneity does not provide as good a ﬁt as the previous hypothesis. In
fact, the p-value for the hypothesis of homogeneity is
p-value = P
χ2
2 ≥7.87

≃0.02
which casts serious doubts on the plausibility of the hypothesis. Notice also that the 95%
conﬁdence interval for p2 does not contain the value p2 = 1/3.
Example 13
Factory Floor
Accidents
Under the null hypothesis
H0 : pi = 1
5
1 ≤i ≤5
the expected cell frequencies are
e1 = e2 = e3 = e4 = e5 = 270
5
= 54
As Figure 10.23 shows, the chi-square statistics are
X2 = 14.95
and
G2 = 14.64
which, compared with a chi-square distribution with k −1 = 5 −1 = 4 degrees of freedom,
give p-values of
p-value ≃P
χ2
4 ≥14.95

= 0.0048
and
p-value ≃P
χ2
4 ≥14.64

= 0.0055
p-value
χ2 distribution
X2 = 14.95
=  0.0048
4
H0 : p1 = p2 = p3 = p4 = p5 =
Monday
Tuesday
Wednesday
Thursday
Friday
Observed cell frequencies
x1 = 65
x2 = 43
x3 = 48
x4 = 41
x5 = 73
n = 270
Expected cell frequencies
e1 = 270 ×
= 54
e2 = 270 ×
= 54
e3 = 270 ×
= 54
e4 = 270 ×
= 54
e5 = 270 ×
= 54
n = 270
Pearson chi-square statistic: X2 = (65 −54)2
54
+ (43 −54)2
54
+ (48 −54)2
54
+ (41 −54)2
54
+ (73 −54)2
54
= 14.95
Likelihood ratio chi-square statistic: G2 = 2 ×

65 × ln
 65
54

+ 43 × ln
 43
54

+ 48 × ln
 48
54

+ 41 × ln
 41
54

+ 73 × ln
 73
54

= 14.64
p-value for Pearson chi-square statistic
Conclusion: Null hypothesis is not plausible.
1
1
5
1
5
5
1
5
1
5
1
5
FIGURE 10.23
Goodness of ﬁt test for the factory ﬂoor accidents example

474
CHAPTER 10
DISCRETE DATA ANALYSIS
Such small p-values lead to the conclusion that the hypothesis of homogeneity is not plausible.
Thus, this data set provides sufﬁcient evidence to conclude that factory ﬂoor accidents are not
equally likely to occur on any day of the week, and one issue that ought to be considered by
the safety drive is why Mondays and Fridays are particularly dangerous days.
10.3.2
Testing Distributional Assumptions
Goodness of ﬁt tests for one-way layouts can be used to test the plausibility that a data
set consists of independent observations from a particular distribution. The observed cell
frequencies xi are the number of data observations falling within the cells, and the expected
cell frequencies ei are the expected frequencies of the cells under the speciﬁc probability
distribution of interest. For discrete distributions the cells can be taken to be the discrete levels
of the distribution, although these may need to be grouped in some manner.
The following example illustrates how to test whether software errors have a Poisson
distribution. The null hypothesis is that the distribution is as speciﬁed, and so rejection of the
null hypothesis indicates that the speciﬁed distribution is not plausible. Acceptance of the null
hypothesis implies that the speciﬁed distribution is plausible, although this does not prove that
the distribution is as speciﬁed. There will be other plausible distributions as well. It should
be remembered that unless the sample size involved is very large, goodness of ﬁt tests of this
kind may not be very powerful, so that a wide range of different distributional assumptions
may be plausible.
Finally, it is important to distinguish between two kinds of null hypotheses, which can be
typiﬁed by the hypotheses
H0 : the software errors have a Poisson distribution with mean λ = 3
and
H0 : the software errors have a Poisson distribution
The ﬁrst null hypothesis speciﬁes precisely the distribution against which the data are to be
tested. The second null hypothesis is more general, requiring only that the distribution be some
Poisson distribution with any parameter value. In the second case, the data can be tested against
a Poisson distribution with parameter λ = ¯x, the average of the data, because this is likely to
give the best ﬁt. The only difference in this latter case is that the degrees of freedom of the chi-
square distribution used to calculate a p-value should be k −2, where k is the number of cells,
rather than the usual k −1, which would be appropriate for testing the ﬁrst null hypothesis.
A general rule for the appropriate degrees of freedom is
number of cells −1 −number of parameters estimated from the data set
Example 3
Software Errors
Figure 10.24 shows a data set of the number of errors found in a total of n = 85 software
products. For example, 3 of the products had no errors, 14 had one error, and so on. Is it
plausible that the number of errors has a Poisson distribution with mean λ = 3?
If the cells are taken to be
no errors, 1 error, . . . , 8 errors, at least 9 errors
then the expected cell frequencies are shown in Figure 10.24. For example, if the random
variable X has a Poisson distribution with mean λ = 3, then
e1 = n × P(X = 0) = 85 × e−3 × 30
0!
= 4.23

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
475
FIGURE 10.24
Distributional goodness of ﬁt test
for the software errors example
H0 : number of errors X has a Poisson distribution with mean λ = 3.0
Number of errors 
found in a soft- 
ware product
0
1
2
3
4
5
6
7
8
Frequency
3
14
20
25
14
6
2
0
1
n = 85
Cell
Expected cell frequency
X = 0
X = 1
X = 2
X = 3
X = 4
X = 5
X = 6
X = 7
X = 8
X ≥ 9
e1 = 85 × P (X = 0) = 85 ×             = 4.23
e3 × 30 
0!
e2 = 85 × P (X = 1) = 85 ×             = 12.70
e3 × 31 
1!
e3 = 85 × P (X = 2) = 85 ×             = 19.04
e3 × 32 
2!
e4 = 85 × P (X = 3) = 85 ×             = 19.04
e3 × 33 
3!
e4 = 85 × P (X = 4) = 85 ×             = 14.28
e3 × 34 
4!
e5 = 85 × P (X = 5) = 85 ×             = 8.57
e3 × 35 
5!
e6 = 85 × P (X = 6) = 85 ×             = 4.28
e3 × 36 
6!
e7 = 85 × P (X = 7) = 85 ×             = 1.84
e3 × 37 
7!
e8 = 85 × P (X = 8) = 85 ×             = 0.69
e3 × 38 
8!
e9 = 85 × P (X ≥ 9)   
       = 0.33
n = 85.0
Group
Group
After grouping
Number of errors
Observed cell frequency 
Expected cell frequency
n = 85
n = 85
0–1 
x1 = 17 
     e1 = 16.93
2 
x2 = 20 
     e2 = 19.04
3 
x3 = 25 
     e3 = 19.04
4 
x4 = 14 
     e4 = 14.28
5 
x5 = 6 
     e5 = 8.57
≥ 6 
x6 = 3 
     e6 = 7.14
and
e2 = n × P(X = 1) = 85 × e−3 × 31
1!
= 12.70
Since some of these expected values are smaller than 5, it is appropriate to group the cells as
shown, so that there are eventually k = 6 cells, each with an expected cell frequency larger
than 5.
The Pearson chi-square statistic is
X2 = (17.00 −16.93)2
16.93
+ (20.00 −19.04)2
19.04
+ (25.00 −19.04)2
19.04
+ (14.00 −14.28)2
14.28
+ (6.00 −8.57)2
8.57
+ (3.00 −7.14)2
7.14
= 5.12

476
CHAPTER 10
DISCRETE DATA ANALYSIS
Comparison with a chi-square distribution with degrees of freedom k −1 = 6 −1 = 5 gives
a p-value of
p-value = P

χ2
5 ≥5.12

= 0.40
which indicates that it is quite plausible that the software errors have a Poisson distribution
with mean λ = 3.
Notice that if the more general null hypothesis
H0 : the software errors have a Poisson distribution
had been considered, then the expected cell frequencies ei would have been calculated using
a Poisson distribution with mean λ = ¯x = 2.76, and a p-value would have been calculated
from a chi-square distribution with k −1 −1 = 4 degrees of freedom.
10.3.3
Problems
10.3.1 DS 10.3.1 gives the results of n = 500 die rolls.
(a) What are the expected cell frequencies if the die is a
fair one?
(b) Calculate the Pearson chi-square statistic X 2 for
testing whether the die is fair.
(c) Calculate the likelihood ratio chi-square statistic G2
for testing whether the die is fair.
(d) What p-values do these chi-square statistics give?
Does a size α = 0.01 test of the null hypothesis
that the die is fair reject or accept the null
hypothesis?
(e) Calculate a 90% two-sided conﬁdence interval for
the probability of obtaining a 6.
10.3.2 DS 10.3.2 presents a data set on the number of rolls of a
die required before a 6 is obtained. If the probability of
scoring a 6 is 1/6, then the distribution of the number
of rolls required until a 6 is scored is a geometric
distribution with parameter p = 1/6. Test whether this
distribution is plausible.
10.3.3 Tire Sales
A garage sells tires of types A, B, and C, and the owner
surmises that a customer is twice as likely to choose
type A as type B, and twice as likely to choose type B as
type C.
(a) Is this supposition plausible based upon the data set
in DS 10.3.3 of this year’s sales?
(b) Calculate a 99% two-sided conﬁdence interval for
the probability that a customer chooses type
A tires.
(This problem is continued in Problem 10.7.10.)
10.3.4 Jury Selection
A court has jurisdiction over ﬁve counties, and of the
people eligible for jury duty, 14% reside in county A,
22% reside in county B, 35% reside in county C, 16%
reside in county D, and 13% reside in county E.
DS 10.3.4 gives the residential locations of the jurors
selected over a ﬁve-year period. Is there any evidence
that the jurors have not been selected at random from the
eligible population?
10.3.5 Infection Recovery
DS 10.3.5 presents a data set compiled from a series of
clinical trials to investigate the effectiveness of a certain
medication in healing an infection.
(a) Is it appropriate to say that there is a 50% chance
that the infection is completely healed and a 10%
chance that there is no change in the infection
(calculate the G2 statistic)?
(b) Calculate a 95% two-sided conﬁdence interval for
the probability that the infection is completely
healed.
10.3.6 Taste Tests for Soft Drink Formulations
A beverage company has three formulations of a soft
drink product. DS 10.3.6 gives the results of some taste
tests where participants are asked to declare which
formulation they like best. Is it plausible that the three
formulations are equally popular?
10.3.7 Hospital Emergency Room Operation
DS 10.3.7 gives a data set of the number of arrivals at a
hospital emergency room during one-hour periods. Is

10.3 GOODNESS OF FIT TESTS FOR ONE-WAY CONTINGENCY TABLES
477
there any evidence that it is not reasonable that the
number of arrivals are modeled with a Poisson
distribution with mean λ = 7?
10.3.8 Radioactive Particle Emissions
DS 10.3.8 gives a data set of the number of radioactive
particles emitted from a substance and passing through a
counter in one-minute periods. Is there any evidence that
it is not reasonable to model these with a Poisson
distribution?
10.3.9 Genetic Variations in Plants
In a biological experiment a large quantity of plants
were grown. For each plant the stem length was
classiﬁed as being either tall or dwarf, and the
position of the ﬂowers was classiﬁed as being either
axial or terminal. All together there were 412 tall axial
plants, 121 tall terminal plants, 148 dwarf axial plants,
and 46 dwarf terminal plants. According to the
proposed genetic theory, the probabilities of the plants
displaying each of these four characteristics should
have relative magnitudes of 9:3:3:1, respectively.
Is the data set consistent with the proposed genetic
theory?
10.3.10 Each of 205 consumers was asked to choose which of
three products they preferred. Product A was chosen by
83 of the consumers, product B was chosen by 75 of
the consumers, and product C was chosen by 47 of the
consumers. Is there sufﬁcient evidence to conclude that
the three products do not have equal probabilities of
being chosen?
10.3.11 Chemical Solution Acidities
The acidity of a chemical solution can be classiﬁed as
“very high,” “high,” “normal,” “low,” and “very low.” A
total of 630 samples of the solution from a production
process were obtained, and the acidities were classiﬁed
as given in DS 10.3.9.
(a) Perform a two-sided hypothesis test of the null
hypothesis that the probability that a solution has
normal acidity is 0.80.
(b) It is claimed that the probability that a solution has
very high acidity is 0.04, the probability that a
solution has high acidity is 0.06, the probability
that a solution has normal acidity is 0.80, the
probability that a solution has low acidity is 0.06
and the probability that a solution has very low
acidity is 0.04. Are the data consistent with this
claim?
10.3.12 An experiment was performed to investigate how long
batteries remain charged under certain storage
conditions. A total of 125 batteries were charged to the
same level and stored in the designated conditions. After
24 hours all 125 batteries were tested and it was found
that 12 of them had charges that had dropped below the
threshold level. After an additional 24 hours the
remaining 113 batteries were tested and it was found
that 53 of them had charges that had dropped below the
threshold level. Finally, after an additional 24 hours the
remaining 60 batteries were tested and it was found that
39 of them had charges that had dropped below the
threshold level. It is claimed that for these batteries
under these storage conditions the time in hours until the
charge drops below the threshold level has a Weibull
distribution with parameters λ = 0.065 and a = 0.45.
Are the results of this experiment consistent with that
claim?
10.3.13 Shark Attacks
The data set in DS 10.3.10 shows the number of shark
attacks along a popular stretch of coastline for each of
the past 76 years. Is it plausible that the number of shark
attacks per year follows a Poisson distribution with
mean 2.5?
10.3.14 A survey is performed to test the claim that three brands
of a product are equally popular. In the survey, 22 people
preferred brand A, 38 people preferred brand B, while
40 people preferred brand C. The Pearson goodness
of ﬁt can be used to test whether the survey provides
sufﬁcient evidence to conclude that the three brands are
not equally popular.
A. True
B. False
10.3.15 In a contingency table analysis using the Pearson
goodness of ﬁt statistic:
A. A p-value of 0.001 implies that the data provide a
good ﬁt to the null hypothesis.
B. A p-value of 0.001 implies that the observed cell
frequencies are all small.
C. A p-value of 0.001 implies that the data were
probably fabricated.
D. None of the above.

478
CHAPTER 10
DISCRETE DATA ANALYSIS
10.4
Testing for Independence in Two-Way Contingency Tables
10.4.1
Two-Way Classiﬁcations
A two-way contingency table is a set of frequencies that summarize how a set of objects is
simultaneously classiﬁed under two different categorizations. If the ﬁrst categorization has r
levels and the second categorization has c levels, then as Figure 10.25 shows, the data can be
presented in tabular form as a set of frequencies
xi j
1 ≤i ≤r
1 ≤j ≤c
Thus, the cell frequency xi j is the number of objects that fall into the ith level of the ﬁrst
categorization and into the jth level of the second categorization. Data sets of this form are
referred to as r × c contingency tables.
The row marginal frequencies x1·, . . . , xr· are deﬁned to be
xi· =
c

j=1
xi j
1 ≤i ≤r
so that they are the sum of the frequencies in each of the r levels of the ﬁrst categorization.
Similarly, the column marginal frequencies x·1, . . . , x·c are deﬁned to be
x· j =
r

i=1
xi j
1 ≤j ≤c
so that they are the sum of the frequencies in each of the c levels of the second categorization.
A subscript “·” is therefore taken to imply that the replaced subscript has been summed over.
The sample size n may be written
n = x·· =
c

j=1
x· j =
r

i=1
xi
FIGURE 10.25
A two-way (r × c) contingency
table
Second Categorization
First Categorization
Level 1
Level 2
Level i
Level r
Level 1
Level 2
Level j
Level c
...
...
...
...
Column marginal
frequencies
Total sample 
size
Row
marginal
frequencies
x11
21
12
22
x
x
x
x
x1c
2c
x
x
xr1
r2
rc
Cell 
frequencies
x 2
x
x c
1
x2
xi
xr
x j
n = x
x·1
·
·
·
··
·
·
·
·
xij

10.4 TESTING FOR INDEPENDENCE IN TWO-WAY CONTINGENCY TABLES
479
In certain data sets some of the marginal frequencies may be ﬁxed due to the manner in
which the data are collected. For example, certain ﬁxed amounts of each of the levels of one
categorization may be investigated to determine which level of the other categorization they
fall into. In other cases all of the marginal frequencies may be random. These differences are
illustrated in the following examples.
Example 29
Drug Allergies
Three drugs are compared with respect to the types of allergic reaction that they cause to
patients. A group of n = 300 patients is randomly split into three groups of 100 patients,
each of which is given one of the three drugs. The patients are then categorized as being
hyperallergic, allergic, mildly allergic, or as having no allergy.
Figure 10.26 shows a 3 × 4 contingency table that presents the results of this experiment.
Notice that the row marginal frequencies xi· are all equal to 100 since each drug is administered
to exactly 100 patients. In contrast, the column marginal frequencies x· j, which represent the
total number of patients with each of the different allergy levels, are not ﬁxed in advance of
the experiment.
Example 58
Overage Weedkiller
Product
Recall that in the nationwide survey the auditors examined n = 54,965 weedkiller containers
and found 2779 of them to be overage. However, the weedkiller containers had three sizes
(small, medium, and large), and the auditors were also required to record the size of the
containers that they examined. Therefore, the full data set, shown in Figure 10.27, takes the
form of a 2 × 3 contingency table. None of the marginal frequencies of this contingency table
is ﬁxed before the survey. The total number of containers examined, n, is also not ﬁxed in
advance, although it could have been estimated from knowledge of the number of stores that
were to be visited in the survey.
Example 57
Building Tile Cracks
One of the most common forms of a two-way contingency table is a 2 × 2 table in which each
of the two categorizations has only two levels. Such a data set has much in common with the
problem of comparing two population proportions discussed in Section 10.2. Figure 10.28
shows how the survey results of cracked tiles on buildings A and on buildings B can be
FIGURE 10.26
Drug allergies data set
Reaction
Hyperallergic
Allergic
Mildly allergic
No allergy
Type of Drug
Drug A
x11 = 11
x12 = 30
x13 = 36
x14 = 23
x1· = 100
Drug B
x21 = 8
x22 = 31
x23 = 25
x24 = 36
x2· = 100
Drug C
x31 = 13
x32 = 28
x33 = 28
x34 = 31
x3· = 100
x·1 = 32
x·2 = 89
x·3 = 89
x·4 = 90
n = x·· = 300
FIGURE 10.27
Overage weedkiller product
data set
Size of Container
Small
Medium
Large
Underage
x11 = 15,595
x12 = 25,869
x13 = 10,722
x1 = 52,186
Age of
Product
Overage
x21 = 612
x22 = 856
x23 = 1311
x2 = 2779
x 1 = 16,207
x 2 = 26,725
x 3 = 12,033
n = x = 54,965

480
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.28
Building tile cracks data set
Location
Buildings A
Buildings B
Undamaged
x11 = 5594
x12 = 1917
x1· = 7511
Tile 
Condition
Cracked
x21 = 406
x22 = 83
x2· = 489
x·1 = 6000
x·2 = 2000
n = x·· = 8000
presented as a 2 × 2 contingency table. Notice that the column marginal frequencies
x·1 = 6000
and
x·2 = 2000
are ﬁxed and represent the sample sizes chosen for the two sets of buildings.
10.4.2
Testing for Independence
One of the most common test procedures applied to a two-way contingency table is a test
of independence between the two categorizations. The exact interpretation of what inde-
pendence means depends upon the speciﬁc contingency table under consideration, but it
essentially can be taken to mean that the two factors that produce the two categorizations
are not associated with each other. More technically it means that, conditional on being in
any level of one of the categorizations, the sets of probabilities of being in the various levels
of the other categorization are all the same. For example, being in level 1 or level 2 of the
ﬁrst categorization does not alter the chances of being in level 1 or level 2 of the second
categorization.
In certain two-way contingency tables with either the row or column marginal frequen-
cies ﬁxed, the test for independence may more appropriately be considered to be a test for
homogeneity between the probability distributions at each of the row or column levels. The
null hypothesis in this case states that these probability distributions are all equal.
Tests of independence operate by taking independence to be the null hypothesis. A p-value
is calculated using a chi-square test.
Example 29
Drug Allergies
In this example, the null hypothesis of independence between the type of drug and the type
of reaction can be interpreted as meaning that the chances of the various kinds of allergic
reaction are the same for each of the three drugs. This means that there is a set of probability
values p1, p2, p3, and p4 that represent the probabilities of getting the four types of reaction
regardless of which drug is used, as illustrated in Figure 10.29. Thus, the three types of drug
can be considered to be equivalent in terms of the reactions that they cause.
If the null hypothesis of independence is rejected, then this implies that there is evidence
to conclude that the three drugs have different sets of probability values for the four types of
reaction. This means that the three types of drug cannot be considered to be equivalent in terms
of the reactions that they cause. (Actually, two of the three drugs could still be equivalent, but
all three cannot be the same.)
Example 58
Overage Weedkiller
Product
Independence in this example can be interpreted as meaning that the overage proportions are
identical for the three different sizes of container. Therefore, if ps is the probability that a
small container is overage, pm is the probability that a medium container is overage, and pl
is the probability that a large container is overage, then the null hypothesis of independence

10.4 TESTING FOR INDEPENDENCE IN TWO-WAY CONTINGENCY TABLES
481
FIGURE 10.29
Independence for drug allergies
example
No Independence
Probability Probability
Probability
Probability
of
of
of
of
hyperallergic
allergic
mildly allergic no allergy
Drug A
pA
1
+
pA
2
+
pA
3
+
pA
4
=
1
Drug B
pB
1
+
pB
2
+
pB
3
+
pB
4
=
1
Drug C
pC
1
+
pC
2
+
pC
3
+
pC
4
=
1
Independence
Probability Probability
Probability
Probability
of
of
of
of
hyperallergic
allergic
mildly allergic no allergy
All drugs
p1
+
p2
+
p3
+
p4
=
1
FIGURE 10.30
Independence for overage
weedkiller product example
No Independence
Container Size
Small
Medium
Large
Probability of underage
1 −ps
1 −pm
1 −pl
Probability of overage
ps
pm
pl
Independence H0 : ps = pm = pl
All containers
Probability of underage
1 −p
Probability of overage
p
can be written
H0 : ps = pm = pl
as illustrated in Figure 10.30. Thus, in this case, the test of independence can be thought of as a
test of the equivalence of a set of binomial parameters. If the null hypothesis of independence
is rejected, then there is enough evidence to conclude that the three overage proportions are
not all equal.
Example 57
Building Tile Cracks
In this example, independence means that the probability that a tile on buildings A be-
comes cracked, pA, is equal to the probability that a tile on buildings B becomes cracked,
pB. Consequently, testing for independence in this example can be viewed as being iden-
tical to the problem of testing the equivalence of two binomial parameters, as discussed in
Section 10.2.2.
The null hypothesis of independence can be tested using either a Pearson or a likelihood
ratio chi-square goodness of ﬁt statistic, as described in the accompanying box. The sum-
mations in these statistics are taken to be over each of the r × c cells, and the expected cell

482
CHAPTER 10
DISCRETE DATA ANALYSIS
frequencies are calculated as
ei j = xi·x· j
n
Thus the expected cell frequency in the i j-cell is the product of the ith row marginal frequency
and the jth column marginal frequency, divided by the total sample size n.
Testing for Independence in a Two-Way Contingency Table
The null hypothesis of independence between two categorizations based upon an r × c
contingency table with cell frequencies xi j can be performed using either the Pearson
chi-square statistic
X2 =
r

i=1
c

j=1
(xi j −ei j)2
ei j
or the likelihood ratio chi-square statistic
G2 = 2
r

i=1
c

j=1
xi j ln
 xi j
ei j

The expected cell frequencies are
ei j = xi·x· j
n
where xi· is the ith row marginal frequency, x· j is the jth column marginal frequency,
and n is the total sample size. A p-value can be calculated as either
p-value = P

χ2
ν ≥X2
or
p-value = P

χ2
ν ≥G2
where the degrees of freedom are
ν = (r −1) × (c −1)
A size α hypothesis test accepts the null hypothesis of independence if the chi-square
statistic is less than the critical point χ2
α,ν, and rejects the null hypothesis of
independence if the chi-square statistic is greater than χ2
α,ν.
One other point to notice is that the degrees of freedom of the chi-square distribution used
to calculate the p-value is (r −1) × (c −1). Also, it is desirable to have the expected cell
frequencies all larger than 5, although if one or two of them are less than 5 it should not be a
big problem. If several are less than 5, then it is best to group some category levels to avoid
this problem.
Example 29
Drug Allergies
Figure 10.31 shows the expected cell frequencies for this example and the calculation of the
Pearson chi-square statistic X2 = 6.391 (the likelihood ratio chi-square statistic is G2 =
6.450). The appropriate degrees of freedom for calculating a p-value are (r −1) × (c −1) =
2 × 3 = 6, so that
p-value = P

χ2
6 ≥6.391

≃0.38

10.4 TESTING FOR INDEPENDENCE IN TWO-WAY CONTINGENCY TABLES
483
Hyperallergic
Allergic
Mildly allergic
No allergy
Drug A
x11 = 11
e11 = 100×32
300
= 10.67
x12 = 30
e12 = 100×89
300
= 29.67
x13 = 36
e13 = 100×89
300
= 29.67
x14 = 23
e14 = 100×90
300
= 30.00
x1· = 100
Drug B
x21 = 8
e21 = 100×32
300
= 10.67
x22 = 31
e22 = 100×89
300
= 29.67
x23 = 25
e23 = 100×89
300
= 29.67
x24 = 36
e24 = 100×90
300
= 30.00
x2· = 100
Drug C
x31 = 13
e31 = 100×32
300
= 10.67
x32 = 28
e32 = 100×89
300
= 29.67
x33 = 28
e33 = 100×89
300
= 29.67
x34 = 31
e34 = 100×90
300
= 30.00
x3· = 100
x·1 = 32
x·2 = 89
x·3 = 89
x·4 = 90
n = x·· = 300
Pearson chi-square statistic: X2 = (11.00 −10.67)2
10.67
+ (30.00 −29.67)2
29.67
+ (36.00 −29.67)2
29.67
+ (23.00 −30.00)2
30.00
+ (8.00 −10.67)2
10.67
+ (31.00 −29.67)2
29.67
+ (25.00 −29.67)2
29.67
+ (36.00 −30.00)2
30.00
+ (13.00 −10.67)2
10.67
+ (28.00 −29.67)2
29.67
+ (28.00 −29.67)2
29.67
+ (31.00 −30.00)2
30.00
= 6.391
FIGURE 10.31
Analysis of drug allergies data set
This large p-value implies that the null hypothesis of independence is plausible, and so there
is no evidence to conclude that the three drugs are any different with respect to the types of
allergic reaction that they cause.
Example 58
Overage Weedkiller
Product
The estimates of the proportions of small, medium, and large containers that are overage are
ˆps =
612
16,207 = 0.038
ˆpm =
856
26,725 = 0.032
ˆpl =
1311
12,033 = 0.109
Is the difference between these estimates statistically signiﬁcant?
Figure 10.32 shows the expected cell frequencies for this example and the calculation of
the likelihood ratio chi-square statistic G2 = 930.8. With (2 −1) × (3 −1) = 2 degrees of
freedom the p-value is
p-value = P

χ2
2 ≥930.8

≃0
and the null hypothesis of independence is clearly not plausible. In fact, with such a large
chi-square statistic the survey provides extremely strong evidence that the overage proportions
ps, pm, and pl are not all equal. Moreover, the data suggest that the large containers have a
considerably greater overage rate than the other types of container.
Suppose that the chemical company checks last year’s records and ﬁnds that 26.3% of
the weedkiller containers sold were the small size, 52.5% were the medium size, and 21.2%

484
CHAPTER 10
DISCRETE DATA ANALYSIS
11,424.62
25,373.80
15,387.58
Small
Medium
Large
Underage
x11 = 15,595
e11 = 52,186×16,207
54,965
= 15,387.58
x12 = 25,869
e12 = 52,186×26,725
54,965
= 25,373.80
x13 = 10,722
e13 = 52,186×12,033
54,965
= 11,424.62
x1· = 52,186
Overage
x21 = 612
e21 = 2779×16207
54965
= 819.42
x22 = 856
e22 = 2779×26,725
54,965
= 1351.20
x23 = 1311
e23 = 2779×12,033
54,965
= 608.38
x2· = 2779
x·1 = 16207
x·2 = 26,725
x·3 = 12,033
n = x·· = 54,965
: G2 = 2 ×
	
15,595 ln

15,595 
+ 25,869ln

25,869 
+10,722 ln

10,722 
+ 612 ln

612
819.42

+ 856 ln

856
1351.20

+ 1311 ln

1311
608.38

= 930.8
Likelihood ratio chi-square statistic
FIGURE 10.32
Analysis of overage weedkiller product data set
were the large size. This suggests that the overall proportion of sales that involve an overage
product should be about equal to
¯p = 0.263ps + 0.525pm + 0.212pl
This value can be estimated as
ˆ¯p = 0.263 ˆps + 0.525 ˆpm + 0.212 ˆpl
= (0.263 × 0.038) + (0.525 × 0.032) + (0.212 × 0.109) = 0.050
with a variance
Var( ˆ¯p) = (0.2632 × Var( ˆps)) + (0.5252 × Var( ˆpm)) + (0.2122 × Var( ˆpl))
If the total number of products examined for each of the three sizes are considered to be
ﬁxed so that ˆps, ˆpm, and ˆpl are considered to be estimates of three binomial parameters, then
Var( ˆps) can be estimated as
ˆps(1 −ˆps)
x·1
= x11x21
x3
·1
= 15,595 × 612
16,2073
and similarly, Var( ˆpm) can be estimated as
25,869 × 856
26,7253
and Var( ˆpl) can be estimated as
10,722 × 1311
12,0333

10.4 TESTING FOR INDEPENDENCE IN TWO-WAY CONTINGENCY TABLES
485
FIGURE 10.33
Analysis of building tile cracks
data set
Buildings A
Buildings B
Undamaged
x11 = 5594
e11 = 7511×6000
8000
= 5633.25
x12 = 1917
e12 = 7511×2000
8000
= 1877.75
x1· = 7511
Cracked
x21 = 406
e21 = 489×6000
8000
= 366.75
x22 = 83
e22 = 489×2000
8000
= 122.25
x2· = 489
x·1 = 6000
x·2 = 2000
n = x·· = 8000
Putting these all together gives
s.e.( ˆ¯p) =

Var( ˆ¯p) = 0.0009
With z0.005 = 2.576, a two-sided 99% conﬁdence interval for ¯p is therefore
¯p ∈(0.050 −(2.576 × 0.0009), 0.050 + (2.576 × 0.0009)) = (0.048, 0.052)
In conclusion, this analysis predicts that somewhere between about 4.8% and 5.2% of the
overall sales involve an overage product.
Example 57
Building Tile Cracks
Figure 10.33 shows the calculation of the expected cell frequencies for this example. The
Pearson chi-square statistic is
X2 = (5594.00 −5633.25)2
5633.25
+ (1917.00 −1877.75)2
1877.75
+ (406.00 −366.75)2
366.75
+ (83.00 −122.25)2
122.25
= 17.896
Compared with a chi-square distribution with (2 −1) × (2 −1) = 1 degree of freedom, this
result gives a p-value of
p-value = P

χ2
1 ≥17.896

≃0
Consequently, it is not plausible that pA and pB, the probabilities of tiles cracking on the two
sets of buildings, are equal. This analysis is consistent with the conﬁdence interval
pA −pB ∈(0.0120, 0.0404)
calculated previously, since the conﬁdence interval does not contain 0.
Finally, it is useful to know that for a 2 × 2 contingency table of this kind, a shortcut
formula for the Pearson chi-square statistic is
X2 = n(x11x22 −x12x21)2
x1·x·1x2·x·2

486
CHAPTER 10
DISCRETE DATA ANALYSIS
FIGURE 10.34
Illustration of Simpson’s paradox
Internet sales
Telephone sales
New customers
199 (11.10%)
63 (6.71%)
Product A Sales
Repeat customers
1594 (88.90%)
876 (93.29%)
1793
939
New customers
243 (11.10%)
138 (9.98%)
Product B Sales
Repeat customers
1946 (88.90%)
1245 (90.02%)
2189
1383
New customers
864 (16.15%)
1107 (15.90%)
Product C Sales Repeat customers
4486 (83.85%)
5855 (84.10%)
5350
6962
New customers
128 (38.32%)
180 (36.59%)
Product D Sales Repeat customers
206 (61.68%)
312 (63.41%)
334
492
New customers
1434 (14.84%)
1488 (15.22%)
Total Sales
Repeat customers
8232 (85.16%)
8288 (84.78%)
9666
9776
For this example, it can be checked that this value gives
X2 = 8000 × ((5594 × 83) −(1917 × 406))2
7511 × 6000 × 489 × 2000
= 17.896
as before.
Simpson’s Paradox
When one analyzes categorical data in the form of contingency tables,
it is important to consider the full extent of categorization that is possible. If the data set is col-
lapsed over one or more categories, then the resulting contingency table may give misleading
indications. This issue is exempliﬁed in an unusual phenomenon known as Simpson’s paradox.
Example 41
Internet Commerce
As an illustration of Simpson’s paradox, consider the data set shown in Figure 10.34. A
companysellsfourproductseitherovertheInternetorbytelephone,andamanagerinvestigates
thecompany’ssalestoseewhethertheyarefromﬁrst-timecustomersorfromrepeatcustomers.
It can be seen that for each of the four products, the proportion of the Internet sales that are
from ﬁrst-time customers is larger than the proportion of telephone sales that are from ﬁrst-
time customers. For example, there were 1793 sales of product A over the Internet, of which
199 or 11.10% were from ﬁrst-time customers. However, out of 939 sales of product A by
telephone, only 63 or 6.71% were from ﬁrst-time costomers. Similarly, for product B the rate
is 11.10% over the Internet but only 9.98% by telephone, for product C the rate is 16.15%
over the Internet but only 15.90% by telephone, and for product D the rate is 38.32% over the
Internet but only 36.59% by telephone. The slightly higher proportions of ﬁrst-time customers
from the Internet sales may be useful information for the manager.
However, if the manager had looked only at total sales, then it would have been seen
that out of 9666 sales over the Internet, 1434 or 14.84% were from ﬁrst-time customers,
while out of 9776 sales by telephone, 1488 or 15.22% were from ﬁrst-time customers. Rather
surprisingly this provides the incorrect indication that the proportion of ﬁrst-time customers
is lower from the Internet than from telephone sales. This strange phenomenon has occurred
as a result of looking at total sales instead of the sales broken down over each of the four
products, and it is known as Simpson’s paradox.

10.4 TESTING FOR INDEPENDENCE IN TWO-WAY CONTINGENCY TABLES
487
10.4.3
Problems
10.4.1 Circuit Board Quality
A computer manufacturer has four suppliers of electrical
circuit boards. Random samples of 200 circuit boards are
taken from each of the suppliers and they are classiﬁed
as being either acceptable or defective, as given in DS
10.4.1. Consider the problem of testing whether the
defective rates are identical for all four suppliers.
(a) Calculate the expected cell frequencies.
(b) Calculate the Pearson chi-square statistic X 2.
(c) Calculate the likelihood ratio chi-square statistic G2.
(d) What p-values do these chi-square statistics give?
(e) Is the null hypothesis that the defective rates are
identical rejected at size α = 0.05?
(f) Calculate a 95% two-sided conﬁdence interval for
the defective rate of supplier A.
(g) Calculate a 95% two-sided conﬁdence interval for
the difference between the defective rates of
suppliers B and C.
10.4.2 Fertilizer Comparisons
Seedlings are grown without fertilizer or with one of two
kinds of fertilizer. After a certain period of time a
seedling’s growth is classiﬁed into one of four
categories, as given in DS 10.4.2. Test whether the
seedlings’ growth can be taken to be the same for all
three sets of growing conditions.
10.4.3 Taste Tests for Soft Drink Formulations
DS 10.4.3 gives the results of a taste test in which a
sample of 200 people in each of three age groups are
asked which of three formulations of a soft drink they
prefer. Test whether the preferences for the different
formulations change with age.
10.4.4 Electric Motor Quality Tests
A factory has ﬁve production lines that assemble electric
motors. A random sample of 180 motors is taken from
each production line and is given a quality examination.
The results are given in DS 10.4.4.
(a) Is there any evidence that the pass rates are any
different for the ﬁve production lines?
(b) Construct a 95% two-sided conﬁdence interval for
the difference between the pass rates of production
lines 1 and 2.
10.4.5 Customer Satisfaction Surveys
An air-conditioner supplier employs four technicians
who visit customers to install and provide maintenance
for their air-conditioner units. In an after-visit
questionnaire the customers are asked to rate their
satisfaction with the technician. DS 10.4.5 gives the
ratings obtained by the four technicians over a period of
time. Is there any evidence that some technicians are
better than others in satisfying their customers?
10.4.6 Show that for a 2 × 2 contingency table the Pearson
chi-square statistic can be written
X2 = n(x11x22 −x12x21)2
x1·x·1x2·x·2
10.4.7 Clinical Trial
DS 10.4.6 presents a 2 × 2 contingency table that com-
pares two drugs with respect to the speed with which a
patient recovers from an ailment. Let ps be the probability
that a patient recovers in less than one week if given the
standard drug, and let pn be the probability that a patient
recovers in less than one week if given the new drug.
(a) Use the Pearson chi-square statistic to test whether
there is any evidence that ps ̸= pn.
(b) Construct a 99% two-sided conﬁdence interval for
ps −pn.
10.4.8 Reactive Ion Etching in Semiconductor
Manufacturing
In the manufacture of semiconductors, reactive ion
etching is a technique whereby the surface of the
semiconductor is bombarded with ions to remove
unwanted material and to leave the desired structure. In
an experiment a set of semiconductors was produced by
this technique, and each one was examined to see whether
the desired structure was complete or incomplete and also
whether the etch depth was satisfactory or unsatisfactory.
It was found that in 1078 cases the structure
was complete and the etch depth was satisfactory,
in 544 cases the structure was complete and the
etch depth was unsatisfactory, in 253 cases the structure
was incomplete and the etch depth was satisfactory, and
in 111 cases the structure was incomplete and the etch
depth was unsatisfactory. Use the Pearson chi-square
statistic to test whether the completeness of the structure
is related to the etch depth, or whether these two factors
can be considered to be independent of each other.
10.4.9 Consumer Warranty Purchases
A company’s sales records show that an extended
warranty was purchased on 38 out of 89 sales of a

488
CHAPTER 10
DISCRETE DATA ANALYSIS
product of type A, an extended warranty was purchased
on 62 out of 150 sales of a product of type B, and an
extended warranty was purchased on 37 out of 111 sales
of a product of type C. Do these data provide sufﬁcient
evidence to indicate that the probability of a customer
purchasing the extended warranty is different for the
three product types?
10.4.10 Asphalt Load Testing
An experiment was conducted to compare three types of
asphalt. Samples of each type of asphalt were subjected
to repeated loads at high temperatures, and the resulting
cracking was analyzed. For type A, 57 samples were
tested, of which 9 had severe cracking, 17 had medium
cracking, and 31 had minor cracking. For type B,
49 samples were tested, of which 4 had severe cracking,
9 had medium cracking, and 36 had minor cracking. For
type C, 90 samples were tested, of which 15 had severe
cracking, 19 had medium cracking, and 56 had minor
cracking. Does this experiment provide any evidence
that the three types of asphalt are different with respect
to cracking?
10.4.11 In a two-way contingency table analysis:
A. The null hypothesis always provides a simpler
explanation for the data than the alternative
hypothesis.
B. The null hypothesis always provides a more
complicated explanation for the data than the
alternative hypothesis.
10.5
Case Study: Microelectronic Solder Joints
The data set in Figure 6.41 reveals that 451 out of 512 solder joints on an assembly were barrel
shaped. With z0.005 = 2.576 and ˆpb = 451/512 = 0.881, a 99% conﬁdence level conﬁdence
interval for pb, the probability that a solder joint will be barrel shaped for this production
method can be calculated as
pb ∈ˆpb ± z0.005

ˆpb(1 −ˆpb)
n
= 0.881 ± 2.576

0.881(1 −0.881)
512
= (0.844, 0.918)
A goodness of ﬁt test can be performed to assess whether the data in Figure 6.41 are
consistent with the supposition that using this production method there is a probability of 0.85
that a solder joint has a barrel shape, there is a probability of 0.03 that a solder joint has a
cylinder shape, and there is a probability of 0.12 that a solder joint has an hourglass shape.
The null hypothesis is therefore
H0 : pb = 0.85, pc = 0.03, ph = 0.12
and the expected values are
eb = 512 × 0.85 = 435.20, ec = 512 × 0.03 = 15.36, eh = 512 × 0.12 = 61.44
The Pearson chi-square statistic is
X2 = (451 −435.20)2
435.20
+ (8 −15.36)2
15.36
+ (53 −61.44)2
61.44
= 0.57 + 3.53 + 1.16 = 5.26
so that the p-value is P(χ2
2 ≥5.26) = 0.072. Since the p-value falls in the range 1% to
10%, the researcher can conclude that there is some evidence that the shape probabilities are
not as stated, but that the evidence is not overwhelming. In particular, the small number of
hourglass-shaped solder joints observed in the data set is somewhat suspicious.
In an experiment to compare two different epoxy formulations for use in the underﬁll, the
researcher prepares 40 assemblies using epoxy of type I and 40 assemblies using epoxy of
type II. Each assembly is then subjected to 2000 temperature cycles before being tested to
see whether it still functions properly. It was found that only 5 of the assemblies with type I
epoxy failed, whereas 15 of the assemblies with type II epoxy failed.

10.6 CASE STUDY: INTERNET MARKETING
489
The probability pI that an assembly produced with underﬁll of type I epoxy fails within
2000 temperature cycles can therefore be estimated as
ˆpI = 5
40 = 0.125
whereas the corresponding probability for assemblies produced with underﬁll of type II epoxy
is
ˆpI I = 15
40 = 0.375
The hypotheses
H0 : pI = pI I
versus
HA : pI ̸= pI I
can be tested with the test statistic
z =
ˆpI −ˆpI I

ˆp(1 −ˆp)
 1
n + 1
m
 =
0.125 −0.375

20
80

1 −20
80
  1
40 + 1
40
 = 2.582
The p-value is 2×(−2.582) = 0.0098, which is just less than 1%. Therefore, the researcher
can conclude that this data set provides sufﬁcient evidence to establish that the failure rates at
2000 temperature cycles are different for the two epoxy formulations, and clearly it is best to
use epoxy type I for the underﬁll. Furthermore, it is interesting to note that if this data set is
analyzed as a 2 × 2 contingency table, then the Pearson chi-square statistic is
X2 = n(x11x22 −x12x21)2
x1.x.1x2.x.2
= 80(5 × 25 −15 × 35)2
20 × 40 × 60 × 40
= 6.667
and the p-value P(χ2
1 ≥6.667) is again just less than 1%.
10.6
Case Study: Internet Marketing
When a particular banner advertisement is employed on a web page, there are 8548 clicks on
the banner over a certain period of time directing the user to the organisation’s own website,
and these lead to 332 purchases. When a different design is used for the banner advertisement,
there are 7562 clicks on the banner over a similar period of time, leading to 259 purchases.
Is there any evidence of a difference in the effectiveness of the two banner designs in terms
of the proportion of purchases to clicks?
For the ﬁrst design the estimated proportion is
ˆp1 = 332
8548 = 3.88%
whereas for the second design the estimated proportion is
ˆp2 = 259
7562 = 3.43%
However, a 95% conﬁdence interval for the difference of the proportions is
(−0.0012, 0.0104)
which contains zero, and so this indicates that there is a no evidence of a difference in the
effectiveness of the two banner designs in terms of the proportion of purchases to clicks.

490
CHAPTER 10
DISCRETE DATA ANALYSIS
10.7
Supplementary Problems
10.7.1 Crystallization is an important step in the recovery and
puriﬁcation of biological molecules such as enzymes
and proteins. The determination of conditions that
produce suitable crystals is of great interest to molecular
biologists. In one experiment, crystals had appeared
within 24 hours in 27 out of 60 trials of a particular
solution. Calculate a 95% two-sided conﬁdence interval
for the probability of crystallization within 24 hours
using this method.
10.7.2 A consumer watchdog organization takes a random
sample of 500 bags of ﬂour made by a company, weighs
them, and discovers that 18 of them are underweight.
Suppose that legal action can be taken if it can be
demonstrated that the proportion of bags sold by the
company that are underweight is more than 1 in 40.
Would you advise the watchdog organization that there
are grounds for legal action?
10.7.3 A bank releases a new credit card that is to be targeted at
a population group of about 1,000,000 customers. In a
trial run the bank mails credit card applications to a
random sample of 5000 customers within this group,
and 384 of them request the credit card. If the bank goes
ahead and mails application forms to all 1,000,000
customers in the target group, construct two-sided 99%
conﬁdence bounds on the total number of these
customers who will request a card.
10.7.4 Upon checking hospital records, a hospital administrator
notices that over a certain period of time, 443 out of 564
surgical operations performed in the morning turned out
to be a “total success,” whereas only 388 out of 545
surgical operations performed in the afternoon turned
out to be a “total success.” Does this substantiate the
hypothesis that surgeons are less effective in the
afternoon because they are more tired? How strong is
the evidence that this data set provides to support this
hypothesis? What other information would you like to
know before making a judgment?
10.7.5 Householders are polled on whether they support a tax
increase to build more schools. The householders are
also asked whether their annual household income is
above or below $60,000. The results of the poll found
106 householders with an annual income above $60,000
of whom 32 support the tax increase and 221
householders with an annual income below $60,000 of
whom 106 support the tax increase. Provide a two-sided
analysis to investigate the evidence that the support for
the tax increase depends upon the householder’s
income.
10.7.6 Archery Contest Scores
DS 10.7.1 contains a data set collected during an amateur
archery contest. Calculate a X 2 statistic to assess
whether it is appropriate to model the probabilities of a
bull’s-eye and a missed target as both being 10%.
10.7.7 Rush-Hour Car Accidents
DS 10.7.2 gives a data set of the number of car accidents
occurring during evening rush-hour trafﬁc in a certain
city. Does it look like it’s reasonable to model these with
a Poisson distribution?
10.7.8 Random-Number Generation
A random-number generator is supposed to provide
numbers that are uniformly distributed between 0 and 1.
A total of n = 10,000 simulations are obtained, and
they are classiﬁed as falling into one of ten intervals of
length 0.1, as given in DS 10.7.3. Is there any evidence
that the random-number generator is not operating
correctly?
10.7.9 Venture Capital
A venture capital organization monitors its investments
in two companies. These companies are involved in
many separate transactions, and for cash-ﬂow purposes
the transaction returns are classiﬁed as late, on time, or
early. What does the data in DS 10.7.4 indicate about the
difference between the two companies?
10.7.10 Tire Sales
A garage sells tires of types A, B, and C, and this year’s
and last year’s sales of the three types are given in DS
10.7.5. Is there evidence of a change in the preferences
for the three types of tire between the two years?
10.7.11 Clinical Trial
DS 10.7.6 gives the results of a set of clinical trials
involving three different medications. Test whether the
three medications are equally effective at treating the
infection.
10.7.12 Student Opinion Polls
DS 10.7.7 presents the results of an opinion poll
conducted on students in the College of Engineering and
students in the College of Arts and Sciences. Is there any

10.7 SUPPLEMENTARY PROBLEMS
491
evidence that opinions differ between students in these
two colleges?
10.7.13 The dimensions of 3877 manufactured parts were
examined and 445 were found to have a length outside a
speciﬁed tolerance range.
(a) Conduct a hypothesis test to investigate whether
there is sufﬁcient evidence to conclude that the
probability of a part having a length outside the
tolerance range is larger than 10%.
(b) Construct a one-sided 99% conﬁdence interval that
provides a lower bound on the probability of a part
having a length outside the tolerance range.
It was also found that out of the 445 parts that had a
length outside the speciﬁed tolerance range, 25 also had
a width outside a speciﬁed tolerance range. Furthermore,
out of the 3432 parts that had a length inside the
speciﬁed tolerance range, it was found that 161 had a
width outside the speciﬁed tolerance range.
(c) Use the Pearson chi-square statistic to test whether
the acceptability of the length and the acceptability
of the width of the parts are related to each other, or
whether these two factors can be considered to be
independent of each other.
10.7.14 Composite Material Properties
In a research report on the effect of moisture on a certain
kind of composite material, it is reported that in 80% of
cases the effect is minimal, in 15% of cases the effect
is strong, and in the remaining 5% of cases the effect is
severe. An experimenter tested these claims by
subjecting 800 samples of the composite material to
moisture, and the results are given in DS 10.7.8.
(a) Perform a chi-square goodness of ﬁt test to examine
whether these experimental results are consistent
with the claims made by the research report.
(b) Use the experimental results to construct a 99%
one-sided conﬁdence interval that provides an
upper bound on the probability of a severe moisture
effect.
10.7.15 Chemical Preparation Methods
An experiment is performed to investigate the best way
to produce a chemical solution. Three different
preparation methods are considered, and various trials
are conducted with each method. The resulting solutions
are classiﬁed as being either too weak, satisfactory, or
too strong, as given in DS 10.7.9. Perform a chi-square
goodness of ﬁt test to examine whether there is any
evidence of a difference between the three preparation
methods in terms of the quality of chemical solutions
that they produce.
10.7.16 Metal Alloy Comparisons
Three types of a metal alloy were investigated to see
how much damage they suffered when subjected to a
high temperature. A total of 220 samples were obtained
for each alloy, and after being subjected to the high
temperature, the damage was classisﬁed as “none,”
“slight,” “medium,” or “severe.” Out of the 220 samples
of type I, 98 had no damage, 32 had slight damage, 48
had medium damage, and 42 had severe damage. Out of
the 220 samples of type II, 52 had no damage, 27 had
slight damage, 67 had medium damage, and 74 had
severe damage. Out of the 220 samples of type III, 112
had no damage, 35 had slight damage, 41 had medium
damage, and 32 had severe damage.
(a) Perform a hypothesis test to assess whether there is
sufﬁcient evidence to conclude that the three alloys
are not all equivalent in terms of the damage that
they suffer.
(b) Perform a hypothesis test to assess whether there is
sufﬁcient evidence to conclude that the probability
of suffering severe damage is different for alloy
type I and alloy type III.
(c) Construct a 99% conﬁdence level two-sided
conﬁdence interval for the probability that a sample
of alloy type II will not suffer any damage.
10.7.17 Company Sales Data
A company’s orders are classiﬁed as coming from
geographical area A, B, C, or D. Over a certain period,
there were 119 orders from area A, 54 orders from area
B, 367 orders from area C, and 115 orders from area D.
(a) It is claimed that the probability that an order is
from area A is 0.25, the probability that an order is
from area B is 0.10, the probability that an order is
from area C is 0.40, and the probability that an order
is from area D is 0.25. Are the data consistent with
this claim?
(b) Construct a two-sided 99% conﬁdence interval for
the probability that an order is received from area C.
10.7.18 Concrete Breaking Loads
An experimenter obtained 84 samples of concrete. When
the samples were each subjected to a load of size 115, a
total of 17 of the samples broke while the other samples
were unharmed. The remaining 67 samples were then
subjected to a load of size 120, and 32 of them broke.

492
CHAPTER 10
DISCRETE DATA ANALYSIS
Finally, the remaining 35 samples were subjected to a
load of size 125, and 21 of them broke. This left 14
samples that survived the highest load. It is claimed that
the breaking load of samples of this type of concrete is
normally distributed with a mean 120 and a standard
deviation 4. Are the results of this experiment consistent
with that claim?
10.7.19 Student Opinion Poll
When a random sample of 64 male students was asked
their opinion on a proposal, 28 of them expressed
support. Also, when a random sample of 85 female
students was asked their opinion on the proposal, 31 of
them expressed support.
(a) Use an appropriate hypothesis test to assess whether
there is sufﬁcient evidence to conclude that the
support for the proposal is different for men and
women.
(b) Construct a 99% two-sided conﬁdence interval that
illustrates the difference in support between men
and women.
10.7.20 Clinical Trial
Patients were diagnosed as being either condition A or
condition B before undergoing a treatment. The treatment
was successful for 56 out of 94 patients classiﬁed
as condition A, and the treatment was successful
for 64 out of 153 patients classiﬁed as condition B.
(a) Perform a hypothesis test to assess whether there is
sufﬁcient evidence to conclude that the chance of
success for patients with condition A is better than
50%.
(b) Construct a two-sided 99% conﬁdence interval for
the difference between the success probabilities for
patients with condition A and with condition B.
(c) Perform a chi-square goodness of ﬁt test to
investigate whether there is sufﬁcient evidence to
conclude that the success probabilities are different
for patients with condition A and with condition B.
What is your conclusion?
10.7.21 Are the following statements true or false?
(a) Contingency tables can be used to summarize count
frequencies for discrete data.
(b) The degrees of freedom used in a chi-square
goodness of ﬁt test are related to the number of cells
being examined.
(c) Independence in a two-way contingency table
implies that for each factor the different levels have
equal probabilities.
(d) In a two-way contingency table analysis, the null
hypothesis of independence is a more complicated
model for the data than the alternative hypothesis.
(e) In a goodness of ﬁt test an extremely high p-value
suggests the possibility that the experimenter
cheated and made up the data.
(f) For comparing two probabilities, either the methods
of Section 10.2 or the methods of Section 10.4 can
be used.
(g) Discrete data analysis can be referred to as
categorical data analysis.
(h) A one-sided conﬁdence interval that provides a lower
bound on p can be used to obtain a one-sided conﬁ-
dence interval that provides an upper bound on 1−p.
(i) The margin of error in a political poll has a
conﬁdence level tacitly associated with it.
(j) The likelihood ratio chi-square statistic always
provides values larger than the Pearson chi-square
statistic, although the values are generally very
close together.
10.7.22 Customer Satisfaction Surveys
In a customer satisfaction survey a random sample of
635 customers were asked their opinion on the service
they received. A total of 485 of these customers replied
that they were very satisﬁed.
(a) Construct a two-sided 95% conﬁdence interval for
the proportion of customers that are very satisﬁed.
(b) Is it safe to conclude that overall at least 75% of the
customers are very satisﬁed? Use an appropriate
hypothesis test.
10.7.23 Hospital Admission Rates
The records at the emergency rooms of ﬁve hospitals
over a certain period of time were examined. Each
patient was classiﬁed as either being “admitted to the
hospital” or as “returned home,” as given in DS 10.7.10.
(a) Is there any evidence to support the claim that the
hospital admission rates differ between the ﬁve
hospitals?
(b) Consider hospitals 3 and 4. Calculate a 95%
two-sided conﬁdence interval for the difference
between the admission rates of these two hospitals.
(c) Consider hospital 1. Is there sufﬁcient evidence to
conclude that the admission rate for this hospital is
larger than 10%?
10.7.24 Scouring Around Bridge Piers
After large ﬂoods and the consequent large river ﬂows,
there can be a reduction in the level of the riverbed

10.7 SUPPLEMENTARY PROBLEMS
493
around the piers of bridges that is known as scouring.
This can be a serious problem if the foundations of the
piers become exposed. The amount of scouring can
depend on the shape of the pier and the consequent ﬂows
and vortexes that are generated. After a large ﬂood, the
data set in DS 10.7.11 was collected concerning the
severity of the scouring around piers of different
designs.
(a) Use a goodness of ﬁt test to examine whether there
is sufﬁcient evidence to conclude that the pier
design has any effect on the amount of scouring.
(b) Consider pier design 1. Are the data consistent with
the contention that for this design the three levels of
scouring are all equally likely?
(c) Show how to perform a two-sided hypothesis test of
whether for pier design 3 the probability of a
minimal scour depth is 25%.
(d) Let p1s be the probability of severe scouring when
pier design 1 is used, and let p2s be the probability
of severe scouring when pier design 2 is used.
Construct a 99% two-sided conﬁdence interval for
p1s −p2s.
10.7.25 In a contingency table analysis using the Pearson
chi-square goodness of ﬁt statistic:
A. A p-value of 0.52 implies that the data are
consistent with the null hypothesis.
B. A p-value of 0.52 implies that the chi-square
analysis is inappropriate.
C. Neither of the above.
D. Both of the above.
10.7.26 In a two-way contingency table analysis:
A. The null hypothesis is that the two variables are
independent.
B. The null hypothesis states that the cell probabilities
are all equal.
C. Neither of the above.
D. Both of the above.
10.7.27 In a marketing study, a group of volunteers is split into
three groups, and each group is shown a different
advertising campaign. Each person is then asked to rate
their enthusiasm for the product as “low,” “average,” or
“high.” Which of the following statements is true?
A. The data can be represented in a two-way
contingency table.
B. The data cannot be represented in a two-way
contingency table.
10.7.28 A chi-square goodness of ﬁt analysis is performed for a
two-way contingency table providing information on a
company’s growth (“low,” “medium,” or “high”) and the
industrial sector of the company.
A. The null hypothesis of independence states that the
growth levels are different for different industrial
sectors.
B. The null hypothesis of independence states that a
company’s growth is equally likely to be “low,”
“medium,” or “high.”
C. Neither of the above.
D. Both of the above.
10.7.29 A conﬁdence interval is obtained for the difference
between two probabilities p1 −p2.
A. If the conﬁdence interval contains zero, then the
difference between the probability estimates is not
statistically signiﬁcant.
B. If the conﬁdence interval contains only negative
values, then this implies that larger samples sizes
are required to establish statistical signiﬁcance.
C. Neither of the above.
D. Both of the above.
10.7.30 In a survey, 46 out of 88 business owners expressed
optimism about the economic situation.
A. The estimate of the proportion of business owners
who are optimistic about the economic situation is
59%.
B. A conﬁdence interval can be obtained for the
proportion of business owners who are optimistic
about the economic situation, which provides more
information than just the estimate of the proportion
alone.
C. Neither of the above.
D. Both of the above.

C H A P T E R E L E V E N
The Analysis of Variance
Extensions to the methods presented in Chapter 9 for comparing two population means are
made in this chapter, where the problem of comparing a set of three or more population means
is considered. The basic ideas behind the statistical analysis are the same. The objective is to as-
certain whether there is any evidence that the population means are unequal, and if there is evi-
dence,tothenascertainwhichpopulationmeanscanbeshowntobedifferentandbyhowmuch.
In Chapter 9 a distinction was made between paired samples and independent samples. A
similar distinction is appropriate in this chapter. A set of independent samples from a set of
several populations is known as a completely randomized design and is analyzed with the
statistical methodology known as the analysis of variance, or ANOVA for short. This topic
is discussed in the ﬁrst part of this chapter.
With three or more populations under consideration, the concept of pairing observations is
known as blocking, which is a very important procedure for improving experimental designs
so that they allow more sensitive statistical analyses. Experimental designs for comparing a
set of several population means that incorporate blocking are known as randomized block
designs and are discussed in the second part of this chapter.
11.1
One-Factor Analysis of Variance
11.1.1
One-Factor Layouts
Suppose that an experimenter is interested in k populations with unknown population means
μ1, μ2, . . . , μk
If only one population is of interest, k = 1, then the one-sample inferences presented in
Chapter 8 are applicable,andifk = 2,thenthetwo-samplecomparisonsdiscussed inChapter9
are appropriate. The one-factor analysis of variance methodology that is discussed in this
section is appropriate for comparing three or more populations, that is, k ≥3.
The experimenter’s objective is to make inferences on the k unknown population means
μi based upon a data set consisting of samples from each of the k populations, as illustrated
in Figure 11.1. In this data set the observation
xi j
represents the jth observation from the ith population. The sample from population i therefore
consists of the ni observations
xi1, . . . , xini
If the sample sizes n1, . . . , nk are all equal, then the data set is referred to as being balanced,
and if the sample sizes are unequal, then the data set is referred to as being unbalanced. The
total sample size of the data set is
nT = n1 + · · · + nk
494

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
495
FIGURE 11.1
One factor layout
Sample from
population 1
· · ·
(factor level 1)
x11
x12
...
x1n1
· · ·
Sample size n1
Sample from
population i
· · ·
(factor level i)
xi1
xi2
...
xini
· · ·
Sample size ni
Sample from
population k
(factor level k)
xk1
xk2
...
xknk
Sample size nk
A data set of this kind is called a one-way or one-factor layout. The single factor is said to
have k levels corresponding to the k populations under consideration. As in all experimental
designs, care should be taken to ensure the integrity of the data set, so that, for example,
there are no unseen biases between the k samples that may compromise the comparisons of
the population means. Usually bias can be avoided by appropriate random sampling. If the
experiment is performed by allocating a total of nT “units” among the k populations, then it is
appropriate to make this allocation in a random manner. With this in mind, one-factor layouts
such as this are often referred to as completely randomized designs.
The analysis of variance is based upon the modeling assumption
xi j = μi + ϵi j
where the error terms ϵi j are independently distributed as
ϵi j ∼N(0, σ 2)
Thus, the observation xi j consists of the ﬁxed unknown population mean μi together with a
random error term ϵi j, which is normally distributed with a mean of 0 and a variance of σ 2.
Equivalently, xi j can be thought of as just an observation from a
N(μi, σ 2)
distribution. Notice that the unknown error variance σ 2 is taken to be the same in each of the k
populations. The analysis of variance is therefore analogous to the pooled variance procedure
for comparing two populations discussed in Section 9.3.2. A discussion of the importance and
possible relaxation of these modeling assumptions is provided in Section 11.1.6.
x1 j ∼N(μ1, σ 2)
1 ≤j ≤n1
ˆμ1 = ¯x1· = x11 + · · · + x1n1
n1
...
xi j ∼N(μi, σ 2)
1 ≤j ≤ni
ˆμi = ¯xi· = xi1 + · · · + xini
ni
...
xkj ∼N(μk, σ 2)
1 ≤j ≤nk
ˆμk = ¯xk· = xk1 + · · · + xknk
nk
FIGURE 11.2
Estimating the population (factor
level) means
Point estimates of the unknown population means μi are obtained in the obvious manner
as the k sample averages, so that as Figure 11.2 shows,
ˆμi = ¯xi· = xi1 + · · · + xini
ni
1 ≤i ≤k
An assessment of the evidence that there is a difference between some of the population means
can be made by testing the null hypothesis
H0 : μ1 = · · · = μk
which states that the population means are all equal. The alternative hypothesis states that at
least two of the population means are not equal, and may be written
HA : μi ̸= μ j
for some i and j

496
CHAPTER 11
THE ANALYSIS OF VARIANCE
Acceptance of the null hypothesis indicates that there is no evidence that any of the population
means are unequal. Rejection of the null hypothesis implies that there is evidence that at least
some of the population means are unequal, and therefore that it is not plausible to assume that
the population means are all equal.
Example 62
Collapse of Blocked
Arteries
Atherosclerosis, or “hardening of the arteries,” is a major contributor to ill health. It progresses
by the accumulation of plaque on the interior wall of arteries that obstructs the blood ﬂow
through the arteries. This obstruction causes pressure differences within the blood ﬂow, and
eventually the artery collapses with possibly fatal consequences.
A mechanical engineer performs an experiment to investigate the collapsability of arteries
due to the presence of stenosis (blocking). A silicone model of an artery is built and part of it
is constricted as shown in Figure 11.3. The amount of stenosis is measured as
stenosis = 1 −d
d0
and the engineer considers three levels
level 1: stenosis = 0.78
level 2: stenosis = 0.71
level 3: stenosis = 0.65
At each stenosis level, the engineer pumps a bloodlike liquid through the model artery and
measures the ﬂowrate at which the tube collapses. Various repetitions of the experiment are
performed, and the resulting data set is shown in Figure 11.4.
d
d0
Blood
Stenosis = 1 −  
d
d0
FIGURE 11.3
Stenosis in silicone model of an artery
Flowrates in ml/s
when artery model collapses
Amount of Stenosis
Level 1
Level 2
Level 3
10.6
11.7
19.6
9.7
12.7
15.1
10.2
12.6
15.6
14.3
16.6
19.2
10.6
16.3
18.7
11.0
14.7
13.0
13.1
19.5
18.9
10.8
16.8
18.0
14.3
15.1
18.6
10.4
15.2
16.6
8.3
13.2
14.8
14.4
17.6
μˆ1 = x¯1· =
= 11.209
10.6 + · · · + 8.3
11
μˆ2 = x¯2· =
= 15.086
11.7 + · · · + 17.6
14
μˆ3 = x¯3· =
= 17.330
19.6 + · · · + 16.6
10
FIGURE 11.4
Data set for collapse of blocked arteries experiment

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
497
Notice that the factor under consideration is the amount of stenosis, which has k = 3
levels. Also, the sample sizes n1 = 11, n2 = 14, and n3 = 10 are unequal, so that the data
set is unbalanced. The unknown population mean μ1 is the average ﬂowrate at collapse for
arteries with stenosis level 1, and similarly μ2 and μ3 are the average ﬂowrates at collapse
for arteries with stenosis levels 2 and 3.
As Figure 11.4 shows, these population means are estimated as
ˆμ1 = ¯x1· = 11.209
ˆμ2 = ¯x2· = 15.086
ˆμ3 = ¯x3· = 17.330
Consequently, the data suggest that the average ﬂowrate at collapse decreases as the amount
of stenosis increases. Is this a statistically signiﬁcant result? This question can be examined
by testing the null hypothesis
H0 : μ1 = μ2 = μ3
Rejection of this null hypothesis will allow the engineer to conclude that the average ﬂowrate
at collapse does depend upon the amount of stenosis.
Example 63
Roadway Base
Aggregates
Immediately below the asphalt surface of a roadway is a layer of base material composed of
a crushed stone or gravel aggregate. The resilient modulus of this aggregate is a measure of
how the aggregate deforms when subjected to stress, and it is an important property affecting
the manner in which the roadway responds to loads.
A construction engineer has four different suppliers of this aggregate material who obtain
their raw materials from four different locations. The engineer would like to assess whether
the aggregates from the four different locations have different values of resilient modulus. An
experiment is performed by randomly selecting ten samples of aggregate from each of the four
locations and measuring their resilient modulus. The resulting data set is given in Figure 11.5.
FIGURE 11.5
Data set for roadway base
aggregates experiment
Location 1
Location 2
Location 3
Location 4
30,060
31,280
29,950
30,430
28,740
30,380
29,190
28,120
29,140
30,620
31,870
30,310
29,090
29,650
30,010
31,650
Resilient modulus 
measurements
30,220
28,080
28,490
29,770
31,120
27,920
31,600
33,100
31,360
27,420
29,450
28,680
28,300
28,860
32,890
31,730
29,750
29,850
31,170
31,480
33,350
30,550
29,470
32,960
Supplier
μˆ1 = x¯1· =
= 30,113
30,060 + · · · + 33,350
10
μˆ2 = x¯2· =
= 29,461
31,280 + · · · + 30,550
10
μˆ3 = x¯3· =
= 30,409
29,950 + · · · + 29,470
10
μˆ4 = x¯4· =
= 30,823
30,430 + · · · + 32,960
10

498
CHAPTER 11
THE ANALYSIS OF VARIANCE
The factor of interest here is the location from which the aggregate material is taken, and
it has k = 4 levels. The data set is balanced with
n1 = n2 = n3 = n4 = 10
The unknown population means μ1, μ2, μ3, and μ4 represent the average resilient modulus
values for aggregate taken from each of the four locations. The sample means are
ˆμ1 = ¯x1· = 30,113
ˆμ2 = ¯x2· = 29,461
ˆμ3 = ¯x3· = 30,409
ˆμ4 = ¯x4· = 30,823
The null hypothesis
H0 : μ1 = μ2 = μ3 = μ4
states that the average resilient modulus values are the same for all four locations. If the null
hypothesis is accepted, then the construction engineer has no reason to worry about which
aggregate supplier is used since there is no evidence of a difference in their products with
respect to average resilient modulus. However, if the null hypothesis is rejected, then the
engineer may want to consider more carefully which supplier to use since the products differ
from one supplier to another.
Example 64
Carpet Fiber Blends
A textile engineer performs an experiment to investigate which of k = 6 different carpet ﬁber
blends produce the best quality carpet. The six different ﬁber blends considered are
type 1: nylon ﬁber type A
type 2: nylon ﬁber type B
type 3: nylon-acrylic ﬁber blend
type 4: polyester ﬁber blend type A
type 5: polyester ﬁber blend type B
type 6: polypropylene ﬁber
Various samples of carpets made with ﬁbers of these six types are subjected to typical amounts
of stress. After a suitable amount of time the worn carpets are taken to be examined by a set of
experts who score the carpets on the amount of wear observed and on the general quality of
the carpets.
The scores on each sample of carpet are averaged out to produce the data set shown in
Figure 11.6. Higher scores indicate carpets that are in a better condition. The factor of interest
here is the type of ﬁber employed in the carpet and it has k = 6 levels. The design is seen to
be unbalanced, with the number of samples of carpet of each type ranging from 13 to 16.
The unknown population mean μi represents the average amount of wear for carpets made
with ﬁber type i. These population means are estimated as
ˆμ1 = ¯x1· = 5.26
ˆμ2 = ¯x2· = 7.66
ˆμ3 = ¯x3· = 5.10
ˆμ4 = ¯x4· = 6.41
ˆμ5 = ¯x5· = 4.80
ˆμ6 = ¯x6· = 6.86

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
499
FIGURE 11.6
Data set for carpet ﬁber blends
experiment
Carpet Fiber Blends
Type 1
Type 2
Type 3
Type 4
Type 5
Type 6
6.5
9.8
5.3
5.1
6.2
6.3
5.4
7.6
4.6
6.7
4.8
7.5
5.0
7.0
4.4
6.5
5.5
5.4
5.4
8.9
5.9
6.5
5.1
7.2
4.3
6.9
5.1
8.3
4.9
5.4
4.6
7.7
5.8
4.9
4.8
7.9
5.5
6.9
5.5
6.0
3.9
5.6
4.0
8.3
5.2
5.5
4.7
6.5
Carpet scores
5.1
6.3
4.5
4.7
4.4
6.1
7.0
9.7
6.5
6.7
5.2
9.2
3.7
7.9
4.7
5.5
4.2
6.7
5.9
8.0
4.9
7.5
4.5
7.8
4.4
6.9
3.9
5.8
5.4
8.3
5.0
5.5
7.6
3.6
6.8
7.0
7.9
8.8
6.2
5.3
7.3
6.5
μˆ1 = x¯1· =
= 5.26
6.5 + · · · + 5.3
16
μˆ2 = x¯2· =
= 7.66
9.8 + · · · + 7.3
16
μˆ3 = x¯3· =
= 5.10
5.3 + · · · + 3.9
13
μˆ4 = x¯4· =
= 6.41
5.1 + · · · + 6.5
16
μˆ5 = x¯5· =
= 4.80
6.2 + · · · + 3.6
14
μˆ6 = x¯6· =
= 6.86
6.3 + · · · + 6.2
15
Is there any evidence that some ﬁber blends wear better than others? The null hypothesis
H0 : μ1 = μ2 = μ3 = μ4 = μ5 = μ6
states that the average amount of wear is the same for all six ﬁber types. The rejection of this
null hypothesis will allow the textile engineer to conclude that some ﬁber blends wear better
than others.
11.1.2
Partitioning the Total Sum of Squares
Treatment Sum of Squares
The assessment of the plausibility of the null hypothesis that
the population means μi are all equal is based upon the variability among the population mean
estimates ˆμi = ¯xi·. If the average of all of the data observations is denoted by
¯x·· = n1 ¯x1· + · · · + nk ¯xk·
nT
= x11 + · · · + xknk
nT
then this variability can be measured by the statistic
SSTr =
k

i=1
ni(¯xi· −¯x··)2

500
CHAPTER 11
THE ANALYSIS OF VARIANCE
which is known as the sum of squares for treatments. The treatments are considered to be
the different levels of the factor under consideration, so that this one factor layout with k levels
can be thought of as a comparison of k treatments.
Notice that the sum of squares for treatments SSTr is formed as a weighted sum of the
squared differences between the population mean estimates ˆμi = ¯xi· and the overall mean
¯x··. Generally speaking, as the variability among the population mean estimates ˆμi = ¯xi·
increases, the sum of squares for treatments SSTr also increases. Thus, the sum of squares for
treatments SSTr should be thought of as a summary measure of the variability between the
factor levels or treatments.
The sum of squares for treatments SSTr is a measure of the variability
between the factor levels.
It is useful to know that expanding the square in the expression for the sum of squares for
treatments SSTr results in the alternative expression
SSTr =
k

i=1
ni(¯xi· −¯x··)2 =
k

i=1
ni ¯x2
i· −2
k

i=1
ni ¯xi· ¯x·· +
k

i=1
ni ¯x2
··
=
k

i=1
ni ¯x2
i· −2nT ¯x2
·· + nT ¯x2
·· =
k

i=1
ni ¯x2
i· −nT ¯x2
··
Error Sum of Squares
Another important consideration in the analysis of a one-factor
layout is the amount of variability within the k factor levels. This variability can be attributed
to the variance σ 2 of the error terms ϵi j and can be measured with the statistic
SSE =
k

i=1
ni

j=1
(xi j −¯xi·)2
which is known as the sum of squares for error. Notice that within this expression the
component
ni

j=1
(xi j −¯xi·)2
measures the variability of the observations xi j at the ith level of the factor about their own
average ¯xi·. The sum of squares for error SSE is then formed by adding up these components
from each of the k levels of the factor.
It is important to notice that the sum of squares for error SSE measures only the variability
within the k levels or treatments, and not between the k levels or treatments.
The sum of squares for error SSE is a measure of the variability
within the factor levels.
If the observations at the ith factor level
xi1, . . . , xini
are replaced by the values
xi1 + c, . . . , xini + c
so that they are all shifted by an amount c, then the sum of squares for error SSE remains
unchanged since the variability within the ith factor level is unchanged. However, the sample

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
501
mean for the ith factor level changes so that the sum of squares for treatments SSTr will
change.
Again, it is useful to know that when the square term is expanded, the sum of squares for
error SSE can also be written as
SSE =
k

i=1
ni

j=1
(xi j −¯xi·)2 =
k

i=1
ni

j=1
x2
i j −2
k

i=1
ni

j=1
xi j ¯xi· +
k

i=1
ni

j=1
¯x2
i·
=
k

i=1
ni

j=1
x2
i j −2
k

i=1
ni ¯x2
i· +
k

i=1
ni ¯x2
i· =
k

i=1
ni

j=1
x2
i j −
k

i=1
ni ¯x2
i·
Total Sum of Squares
If the k factor levels are ignored so that the data set is thought of as
one big sample of size nT , then the total variability within this sample can be measured by
SST =
k

i=1
ni

j=1
(xi j −¯x··)2
which is known as the total sum of squares. SST is composed of the sum of the squared
deviations of each observation xi j about the overall sample average ¯x··.
The total sum of squares SST is a measure of the total variability in the data set.
Notice that when the square term is expanded, the total sum of squares can be written
SST =
k

i=1
ni

j=1
(xi j −¯x··)2 =
k

i=1
ni

j=1
x2
i j −2
k

i=1
ni

j=1
xi j ¯x·· +
k

i=1
ni

j=1
¯x2
··
=
k

i=1
ni

j=1
x2
i j −2nT ¯x2
·· + nT ¯x2
·· =
k

i=1
ni

j=1
x2
i j −nT ¯x2
··
With the alternative expressions for SSTr, SSE, and SST, it can be easily seen that
SST = SSTr + SSE
In summary, it has been shown that the total variability in the data observations in a one-
factor layout can be measured by the total sum of squares SST, which can be partitioned
into two components, as illustrated in Figure 11.7. The ﬁrst component, the sum of squares
for treatments SSTr, measures the variability between the factor levels, while the second
component, the sum of squares for error SSE, measures the variability within the factor levels.
FIGURE 11.7
Partition of total sum of squares for
completely randomized one-factor
layout
Total sum of squares
SST
Total variability in the data set
Treatment sum of squares
SSTr
Variability between the treatments
Error sum of squares
SSE
Variability within the treatments

502
CHAPTER 11
THE ANALYSIS OF VARIANCE
P-Value Considerations
The plausibility of the null hypothesis
H0 : μ1 = · · · = μk
depends upon the relative sizes of the sum of squares for treatments SSTr and the sum of
squares for error SSE.
The plausibility of the null hypothesis that the factor level means are all equal
depends upon the relative size of the sum of squares for treatments SSTr to the sum
of squares for error SSE.
The mechanics of the calculation of a p-value for the null hypothesis are presented in an
analysis of variance table, which is discussed in Section 11.1.3. Nevertheless, it is instructive
to understand at an intuitive level how changes in the sum of squares for treatments SSTr and
the sum of squares for error SSE affect the p-value.
A useful graphical representation of a one-factor layout is a series of boxplots, one for
each factor level, drawn against the same axis. Figure 11.8 shows such a representation for a
data set with k = 3 factor levels. The sample means ¯xi· occur roughly at the center of each
of the boxplots, and so location differences between the boxplots correspond to differences
between the sample means ¯xi·. The sum of squares for treatments SSTr is therefore a measure
of the variability in the locations of the different boxplots. On the other hand, the lengths of
the boxplots provide an indication of the variability within the different factor levels, and so
the sum of squares for error SSE depends upon the lengths of the boxplots, as illustrated in
Figure 11.8.
Figure 11.9 shows how the p-value depends upon changes in the sum of squares for
treatments SSTr and the sum of squares for error SSE. A base scenario is altered in one of four
ways that correspond to increases or decreases in one of the sums of squares. In scenario I,
the boxplots have the same lengths but they are farther apart. This corresponds to an increase
FIGURE 11.8
Interpretation of the sum of squares
for treatments and the sum of
squares for error
¯x2
¯x3
¯x1
Factor level 1
Factor level 2
Factor level 3
SSTr
SSE
.
.
.

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
503
FIGURE 11.9
Dependence of p-value on the sum
of squares for treatments and the
sum of squares for error
Base
Scenario I
Scenario II
Scenario III
Scenario IV
SSTr (I) > SSTr (base)
SSE (I) = SSE (base)
p-value (I) < p-value (base)
SSTr (II) < SSTr (base)
SSE (II) = SSE (base)
p-value (II) > p-value (base)
SSTr (III) = SSTr (base)
SSE (III) > SSE (base)
p-value (III) > p-value (base)
SSTr (IV) = SSTr (base)
SSE (IV) < SSE (base)
p-value (IV) < p-value (base)
in the sum of squares for treatments SSTr, whereas the sum of squares for error SSE remains
constant. The null hypothesis that the factor level means μi are all equal is less plausible in
scenario I than in the base scenario, and so
p-value (I) < p-value (base)
In scenario II, the boxplots have the same lengths, but they are closer together than in
the base scenario. This corresponds to a decrease in the sum of squares for treatments SSTr,
whereas the sum of squares for error SSE remains constant. The null hypothesis that the factor
level means μi are all equal is more plausible in scenario II than it is in the base scenario,
and so
p-value (II) > p-value (base)
In scenario III, the boxplots have roughly the same central locations as in the base scenario,
so that the sample means ¯xi· are roughly the same, but the boxplots are longer than in the base
scenario. This corresponds to an increase in the sum of squares for error SSE, while the sum
of squares for treatments SSTr remains constant. This results in a decrease in the relative size
of the sum of squares for treatments SSTr to the sum of squares for error SSE. Consequently,
the null hypothesis that the factor level means μi are all equal is more plausible in scenario III
than it is in the base scenario, so that
p-value (III) > p-value (base)
In scenario IV, the boxplots have roughly the same central locations as in the base scenario,
but the boxplots are shorter than in the base scenario. This corresponds to a decrease in the
sum of squares for error SSE, while the sum of squares for treatments SSTr remains constant.

504
CHAPTER 11
THE ANALYSIS OF VARIANCE
There is an increase in the relative size of the sum of squares for treatments SSTr to the sum
of squares for error SSE. Consequently, the null hypothesis that the factor level means μi are
all equal is less plausible in scenario IV than it is in the base scenario, so that
p-value (IV) < p-value (base)
Sum of Squares Partition for One-Factor Layout
In a one-factor layout the total variability in the data observations is measured by the
total sum of squares SST, which is deﬁned to be
SST =
k

i=1
ni

j=1
(xi j −¯x··)2 =
k

i=1
ni

j=1
x2
i j −nT ¯x2
··
This value can be partitioned into two components
SST = SSTr + SSE
where the sum of squares for treatments
SSTr =
k

i=1
ni(¯xi· −¯x··)2 =
k

i=1
ni ¯x2
i· −nT ¯x2
··
measures the variability between the factor levels, and the sum of squares for error
SSE =
k

i=1
ni

j=1
(xi j −¯xi·)2 =
k

i=1
ni

j=1
x2
i j −
k

i=1
ni ¯x2
i·
measures the variability within the factor levels. On an intuitive level, the plausibility
of the null hypothesis that the factor level means μi are all equal depends upon the
relative size of the sum of squares for treatments SSTr to the sum of squares for error
SSE.
Example 62
Collapse of Blocked
Arteries
Figure 11.10 shows boxplots of the ﬂowrates at collapse for the three different stenosis levels.
The boxplots provide a clear graphical presentation of the apparent decrease in the ﬂowrates
at collapse for the larger stenosis values.
FIGURE 11.10
Boxplots for collapse of blocked
arteries data set
Level 1
Level 2
Level 3
10.0
20.0
17.5
15.0
Data
12.5

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
505
Hand calculations of the sums of squares are usually most easily performed using the
alternative expressions. With ¯x1· = 11.209, ¯x2· = 15.086, and ¯x3· = 17.330, and with
¯x·· = 10.6 + · · · + 16.6
35
= 14.509
and
k

i=1
ni

j=1
x2
i j = 10.62 + · · · + 16.62 = 7710.39
the total sum of squares is
SST =
k

i=1
ni

j=1
x2
i j −nT ¯x2
·· = 7710.39 −(35 × 14.5092) = 342.5
and the treatment sum of squares is
SSTr =
k

i=1
ni ¯x2
i· −nT ¯x2
··
= (11 × 11.2092) + (14 × 15.0862) + (10 × 17.3302) −(35 × 14.5092)
= 204.0
The sum of squares for error can then be calculated by subtraction to be
SSE = SST −SSTr = 342.5 −204.0 = 138.5
In hand calculations of this kind it is important not to round numbers until the end of
the calculation. This is because the sums of squares are generally obtained as the difference
between two much larger numbers, and rounding in these numbers can lead to substantial
inaccuracies in the resulting sums of squares.
Example 63
Roadway Base
Aggregates
Figure 11.11 shows boxplots of the resilient modulus values of the aggregate samples from
the four locations. A visual inspection of the boxplots reveals that there is not much difference
between the four locations, although it looks as if location 2 may possibly have a smaller
average resilient modulus value than the other three locations.
With ¯x1· = 30,113, ¯x2· = 29,461, ¯x3· = 30,409, and ¯x4· = 30,823, and with
¯x·· = 30,060 + · · · + 32,960
40
= 30,201.5
FIGURE 11.11
Boxplots for roadway base
aggregates data set
Data
34,000
33,000
32,000
31,000
30,000
29,000
28,000
27,000
Location 1
Location 2
Location 3
Location 4

506
CHAPTER 11
THE ANALYSIS OF VARIANCE
and
k

i=1
ni

j=1
x2
i j = 30,0602 + · · · + 32,9602 = 36,573,858,000
the total sum of squares is
SST =
k

i=1
ni

j=1
x2
i j −nT ¯x2
·· = 36,573,858,000 −(40 × 30,201.52)
= 88,633,910
and the treatment sum of squares is
SSTr =
k

i=1
ni ¯x2
i· −nT ¯x2
··
= (10 × 30,1132) + (10 × 29,4612) + (10 × 30,4092)
+ (10 × 30,8232) −(40 × 30,201.52)
= 9,854,910
As before, the sum of squares for error can then be calculated by subtraction to be
SSE = SST −SSTr = 88,633,910 −9,854,910 = 78,779,000
Example 64
Carpet Fiber Blends
Figure 11.12 shows boxplots for the scores of the carpets made with the six types of ﬁber.
There appear to be sizeable differences between the different ﬁber types, with types 2, 4, and
6 appearing to be better than types 1, 3, and 5.
The sums of squares can be calculated to be
SST = 181.889
SSTr = 97.219
SSE = 84.670
FIGURE 11.12
Boxplots for carpet ﬁber blends
data set
Data
Type 6
Type 5
Type 4
Type 3
Type 2
Type 1
10
9
8
7
6
5
4
3

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
507
11.1.3
The Analysis of Variance Table
The sum of squares for treatments SSTr is said to have k −1 degrees of freedom and the mean
squares for treatments is deﬁned to be
MSTr =
SSTr
degrees of freedom = SSTr
k −1
The sum of squares for error SSE has nT −k degrees of freedom (which is actually made up
of ni −1 degrees of freedom from each of the k factor levels), and the mean square error is
deﬁned to be
MSE =
SSE
degrees of freedom =
SSE
nT −k
The mean square error MSE is distributed as
MSE ∼σ 2 χ2
nT −k
nT −k
and since
E

χ2
nT −k

= nT −k
it follows that
E(MSE) = σ 2
so that the mean square error MSE is an unbiased point estimate of the error variance σ 2.
Consequently, the mean square error MSE is sometimes written as ˆσ 2.
Notice that the estimate of the variance at the ith factor level is
s2
i =
ni
j=1(xi j −¯xi·)2
ni −1
so that the sum of squares for error SSE is
SSE =
k

i=1
(ni −1)s2
i
Consequently, the mean square error MSE is seen to be
MSE = ˆσ 2 = (n1 −1)s2
1 + · · · + (nk −1)s2
k
nT −k
so that it is a weighted average of the variance estimates from within each of the k factor
levels.
It can be shown that
E(MSTr) = σ 2 +
k
i=1 ni(μi −¯μ)2
k −1
where
¯μ = n1μ1 + · · · + nkμk
nT
Thus, the expected value of the mean square for treatments MSTr is equal to σ 2 plus a positive
amount that depends upon the variability among the factor level means μi. Furthermore, if
the factor level means μi are all equal (and consequently also all equal to ¯μ), then
E(MSTr) = σ 2

508
CHAPTER 11
THE ANALYSIS OF VARIANCE
In fact, if the factor level means μi are all equal, then it can be shown that the distribution of
the mean square for treatments MSTr is
MSTr ∼σ 2 χ2
k−1
k −1
These results can be used to develop a method for calculating the p-value of the null
hypothesis
H0 : μ1 = · · · = μk
When this null hypothesis is true, then the F-statistic
F = MSTr
MSE
is distributed as a χ2
k−1/(k −1) random variable divided by a χ2
nT −k/(nT −k) random variable,
which is an Fk−1,nT −k random variable. In other words, when the null hypothesis is true, then
F = MSTr
MSE ∼Fk−1,nT −k
The plausibility of the null hypothesis is therefore in doubt whenever the observed value of
the F-statistic does not look like it is an observation from an Fk−1,nT −k distribution.
Moreover, the mean square for treatments MSTr has a larger expected value when the
factor level means μi are unequal than under the null hypothesis, and so in particular, it is
large values of the F-statistic that cast doubt on the plausibility of the null hypothesis. The
p-value is deﬁned to be the probability of obtaining the observed data values or worse when
the null hypothesis is true, and worse data sets are therefore data sets that give larger values
of the F-statistic than the value observed. Consequently, the p-value is calculated as
p-value = P(X ≥F)
where the random variable X has an Fk−1,nT −k distribution, as illustrated in Figure 11.13.
This one factor analysis of variance is presented in an analysis of variance table as shown
in Figure 11.14. The table has three rows corresponding to the treatments (the factor levels),
the errors, and the total of these two rows. The ﬁrst column lists the degrees of freedom, the
second column lists the sums of squares, and the third column lists the mean squares. Notice
that the degrees of freedom in the “Total” row are nT −1, the sum of the degrees of freedom
FIGURE 11.13
P-value calculation for one-factor
analysis of variance table
F = MSTr
p-value
        distribution
Fk − 1,nT−k
MSE

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
509
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Treatments
Error
Total
F = MSTr
MSE
P(Fk−1,nT −k ≥F)
MSTr = SSTr
k−1
MSE =
SSE
nT −k
k −1
nT −k
nT −1
SSTr
SSE
SST
FIGURE 11.14
Analysis of variance table for one-factor layout
for treatments and the degrees of freedom for error, but that a mean square is not given in the
“Total” row. The penultimate column lists the F-statistic, calculated as the ratio of the two
mean squares, and the ﬁnal column contains the p-value.
F-Test for One-Factor Layout
In a one-factor layout with k levels and a total sample size nT , the treatments have
k −1 degrees of freedom and the error has nT −k degrees of freedom. Mean squares
are obtained by dividing a sum of squares by its respective degrees of freedom so that
MSTr = SSTr
k −1
and
MSE =
SSE
nT −k
A p-value for the null hypothesis that the factor level means μi are all equal is
calculated as
p-value = P(X ≥F)
where the F-statistic is
F = MSTr
MSE
and the random variable X has an Fk−1,nT −k distribution.
Example 62
Collapse of Blocked
Arteries
The degrees of freedom for treatments is k −1 = 3 −1 = 2 so that the mean square for
treatments is
MSTr = SSTr
2
= 204.0
2
= 102.0
With nT −k = 35 −3 = 32 degrees of freedom for error, the mean square error is
MSE = SSE
32 = 138.5
32
= 4.33
This results in an F-statistic of
F = MSTr
MSE = 102.0
4.33 = 23.6
and a p-value of
p-value = P(X ≥23.6) ≃0

510
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.15
Analysis of variance table for
collapse of blocked arteries
experiment
23.57
0.000
102.01
4.33
2
32
34
204.02
138.47
342.49
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Stenosis level
Error
Total
FIGURE 11.16
Analysis of variance table for
roadway base aggregates
experiment
1.50
0.2307
3,284,970
2,188,306
3
36
39
9,854,910
78,779,000
88,633,910
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Locations
Error
Total
FIGURE 11.17
Analysis of variance table for
carpet ﬁber blends experiment
19.290
0.000
19.444
1.008
5
84
89
97.219
84.670
181.889
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Carpet type
Error
Total
where the random variable X has an F2,32 distribution. Consequently, the null hypothesis that
the average ﬂowrate at collapse is the same for all three amounts of stenosis is not plausible.
This experiment has established that the ﬂowrate at collapse does depend upon the amount of
stenosis. Figure 11.15 shows the analysis of variance table for this data set.
Example 63
Roadway Base
Aggregates
Figure 11.16 shows the analysis of variance table for this example. Notice that
MSTr = SSTr
k −1 = 9,854,910
3
= 3,284,970
and
MSE =
SSE
nT −k = 78,779,000
36
= 2,188,306
The F-statistic is
F = MSTr
MSE = 3,284,970
2,188,306 = 1.50
so that the p-value is
p-value = P(X ≥1.50) = 0.231
where the random variable X has an F3,36 distribution. A p-value of this size indicates that
the null hypothesis is plausible and so there is no reason for the engineer to conclude that the
aggregate obtained from one location is different from the aggregate obtained from any other
location.
Example 64
Carpet Fiber Blends
Figure 11.17 shows the analysis of variance table for the comparison of the different carpet
ﬁber blends. For this data set
MSTr = SSTr
k −1 = 97.219
5
= 19.444

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
511
and
MSE =
SSE
nT −k = 84.670
84
= 1.008
The F-statistic is therefore
F = MSTr
MSE = 19.444
1.008 = 19.290
so that the p-value is
p-value = P(X ≥19.29) ≃0
where the random variable X has an F5,84 distribution. Consequently, the textile engineer
concludes that this experiment has established that some carpet ﬁbers wear better than others.
Which ones are better and by how much? This question is addressed in the next section.
11.1.4
Pairwise Comparisons of the Factor Level Means
Whenananalysisofvarianceisperformedandthenullhypothesisthatthefactorlevelmeansμi
are all equal is rejected, the experimenter can follow up the analysis with pairwise comparisons
of the factor level means to discover which ones have been shown to be different and by how
much. These conﬁdence intervals are often referred to as the Tukey multiple comparisons
procedure. With k factor levels there are k(k −1)/2 pairwise differences
μi1 −μi2
1 ≤i1 < i2 ≤k
Pairwise Comparisons of the Factor Level Means
When an analysis of variance is performed for a one-factor layout and the null
hypothesis that the factor level means μi are all equal is rejected, it is useful to
calculate the 1 −α conﬁdence level simultaneous conﬁdence intervals
μi1 −μi2 ∈

¯xi1· −¯xi2· −s qα,k,ν
√
2

1
ni1
+ 1
ni2
, ¯xi1· −¯xi2·+s qα,k,ν
√
2

1
ni1
+ 1
ni2

for the k(k −1)/2 pairwise differences of the factor level means. The critical point
qα,k,ν is the upper α point of the Studentized range distribution with parameters k and
degrees of freedom ν = nT −k (given in Table V), and s = ˆσ =
√
MSE. These
simultaneous conﬁdence intervals have an overall conﬁdence level of 1 −α and
indicate which factor level means can be declared to be different, and by how much.
These conﬁdence intervals are similar to the t-intervals based upon a pooled variance
estimate for comparing two population means, which are discussed in Section 9.3.2. The
difference is that the value qα,k,ν/
√
2 is used in place of the critical point tα/2,ν, and the reason
for this is that the t-intervals have an individual conﬁdence level of 1 −α whereas this set
of simultaneous conﬁdence intervals have an overall conﬁdence level of 1 −α. This overall
conﬁdence level has the interpretation that the experimenter has a conﬁdence level of 1−α that
all of the k(k −1)/2 conﬁdence intervals contain their respective parameter value μi1 −μi2,
and this is achieved with a value of qα,k,ν/
√
2 that is larger than the critical point tα/2,ν.
If the conﬁdence interval for the difference μi1 −μi2 contains 0, then there is no evidence
that the means at factor levels i1 and i2 are different. However, if the conﬁdence interval for

512
CHAPTER 11
THE ANALYSIS OF VARIANCE
the difference μi1 −μi2 does not contain 0, then there is evidence that the means at factor
levels i1 and i2 are different. Furthermore, the conﬁdence interval indicates by how much the
factor level means have been shown to be different.
If the analysis of variance results in a rejection of the null hypothesis that the factor level
means μi are all equal, then at least one pair of factor level means will have a conﬁdence
interval that does not contain 0. (Technically, this is generally the case as long as the critical
point qα,k,ν is chosen with a value of α larger than the p-value calculated in the analysis of
variance table.) If the analysis of variance results in an acceptance of the null hypothesis, with
a p-value larger than 10%, then all of the conﬁdence intervals will contain 0, since there is no
evidence that any factor level means are different. In this situation there is usually no reason
to construct the conﬁdence intervals.
One property of these conﬁdence intervals that the experimenter should be aware of is
that as the number of factor levels k increases, the critical point qα,k,ν also increases so that
the conﬁdence intervals become longer and are consequently less sensitive.
Example 62
Collapse of Blocked
Arteries
With
s = ˆσ =
√
MSE =
√
4.33 = 2.080
and the critical point
q0.05,3,32 = 3.48
which can be estimated by interpolation in Table V, or obtained from a computer, the pairwise
conﬁdence intervals for the differences in the treatment means are
μ1 −μ2 ∈

11.209 −15.086 −2.080 × 3.48
√
2
	
1
11 + 1
14,
11.209 −15.086 + 2.080 × 3.48
√
2
	
1
11 + 1
14

= (−3.877 −2.062, −3.877 + 2.062) = (−5.939, −1.814)
and
μ1 −μ3 ∈

11.209 −17.330 −2.080 × 3.48
√
2
	
1
11 + 1
10,
11.209 −17.330 + 2.080 × 3.48
√
2
	
1
11 + 1
10

= (−6.121 −2.236, −6.121 + 2.236) = (−8.357, −3.884)
and
μ2 −μ3 ∈

15.086 −17.330 −2.080 × 3.48
√
2
	
1
14 + 1
10,
15.086 −17.330 + 2.080 × 3.48
√
2
	
1
14 + 1
10

= (−2.244 −2.119, −2.244 + 2.119) = (−4.364, −0.125)
None of these three conﬁdence intervals contains 0, and so the experiment has estab-
lished that each of the three stenosis levels results in a different average ﬂowrate at collapse.

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
513
Moreover, the average ﬂowrate for a stenosis of 0.78 (level 1) is seen to be at least 1.8 ml/s
lower than the average ﬂowrate for a stenosis of 0.71 (level 2) (but no more than 5.9 ml/s
lower) and at least 3.9 ml/s lower than the average ﬂowrate for a stenosis of 0.65 (level 3) (but
no more than 8.4 ml/s lower). Similarly, the average ﬂowrate for a stenosis of 0.71 (level 2)
is seen to be at least 0.1 ml/s lower than the average ﬂowrate for a stenosis of 0.65 (level 3)
(but no more than 4.4 ml/s lower).
Example 63
Roadway Base
Aggregates
No evidence was found for any difference in the average resilient modulus values for the
aggregates from the four locations, and so the experimenter knows that all of the conﬁdence
intervals for the pairwise differences μi1 −μi2 will contain 0. Notice also that for balanced
data sets such as this one with sample sizes all equal to n, the conﬁdence intervals are
μi1 −μi2 ∈

¯xi1· −¯xi2· −s qα,k,ν
√n
, ¯xi1· −¯xi2· + s qα,k,ν
√n

which are all of the same length.
The largest sample mean is ¯x4· = 30,823 and the smallest sample mean is ¯x2· = 29,461.
With
s = ˆσ =
√
MSE =

2,188,305 = 1479
and a critical point
q0.05,4,36 = 3.81
the conﬁdence interval for μ4 −μ2 is
μ4 −μ2 ∈

30,823 −29,461 −1479 × 3.81
√
10
, 30,823 −29,461 + 1479 × 3.81
√
10

= (1362 −1782, 1362 + 1782) = (−420, 3144)
As expected, this conﬁdence interval does contain 0 and so there is no evidence that μ2
and μ4 are unequal. However, notice that the difference in these means could be as large as
3144. Again, remember that acceptance of the null hypothesis does not prove that the average
resilient modulus values are all equal. A more sensitive (powerful) test procedure and shorter
conﬁdence intervals are attained with larger sample sizes, as discussed in Section 11.1.5.
Example 64
Carpet Fiber Blends
Figure 11.18 shows the pairwise conﬁdence intervals for this data set. Altogether there are
k(k −1)/2 = 6 × 5/2 = 15 conﬁdence intervals, of which ﬁve contain 0 and ten do not
contain 0.
It is useful to present these results schematically as in Figure 11.19. The sample means ¯xi·
are written on a linear scale, and in this example they occur in the order 5, 3, 1, 4, 6, and then
2. Any two factor levels i1 and i2 for which the conﬁdence interval for μi1 −μi2 contains 0
are then joined by a line, and this is done in such a way that any two factor levels i1 and i2
for which the conﬁdence interval for μi1 −μi2 does not contain 0 are not joined by a line.
In this case, the conﬁdence intervals for μ1 −μ3, μ1 −μ5, and μ3 −μ5 all contain 0, and
so one continuous line is drawn between ¯x1·, ¯x3·, and ¯x5·. Also, the conﬁdence intervals for
μ2 −μ6 and μ4 −μ6 contain 0, and so lines are drawn between ¯x2· and ¯x6· and between ¯x4·
and ¯x6·. These lines are discontinuous at ¯x6· because the conﬁdence interval for μ2 −μ4 does
not contain 0 and so no line is required between ¯x2· and ¯x4·.

514
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.18
Pairwise conﬁdence intervals for
the carpet ﬁber blends experiment
μ1 − μ2 
(−3.44, −1.37)
μ1 − μ3 
(−0.94, 1.25) 
contains 0
μ2 − μ3 
(1.47, 3.66)
μ1 − μ4 
(−2.19, −0.12)
μ2 − μ4 
(0.22, 2.28)
μ3 − μ4 
(−2.41, −0.22)
μ1 − μ5 
(−0.61, 1.53) 
contains 0
μ2 − μ5 
(1.79, 3.93)
μ3 − μ5 
(−0.83, 1.43) 
contains 0
μ4 − μ5 
(0.54, 2.68)
μ1 − μ6 
(−2.66, −0.55)
μ2 − μ6 
(−0.25, 1.85) 
contains 0
μ3 − μ6 
(−2.87, −0.65)
μ4 − μ6 
(−1.50, 0.60) 
contains 0
μ5 − μ6 
(−3.15, −0.97)
The critical point is q0.05,6,84 = 4.12 and s = σˆ = 
√
MSE = 1.004 so
μi − μj ∈ x¯i. − x¯j. ±
+
1.004  4.12 
√
2
1 
ni
1 
nj
FIGURE 11.19
Schematic presentation of the
results of the carpet ﬁber blends
experiment
¯x .
2
¯x .
3
¯x .
1
¯x .
4
¯x .
5
4.80
5.10 5.26
6.41
6.86
7.66
¯x .
6
The diagram in Figure 11.19 is easily interpreted since factor levels joined by a line are not
distinguishable (there is no evidence that their means are different), whereas factor levels that
are not joined have means that have been shown to be different. One immediate observation
is that as a group, ﬁber types 1, 3, and 5 are all “worse” than ﬁber types 2, 4, and 6. In other
words, the ﬁrst group has average scores that are known to be smaller than the average scores
in the second group.
Also, ﬁber types 1, 3, and 5 are indistinguishable from one another. There is no evidence
that their average scores are any different. In addition, ﬁber types 2 and 6 are indistinguishable,
as are ﬁber types 4 and 6, but ﬁber type 2 is known to have an average score larger than that of
ﬁber type 4. Is this last remark paradoxical? No, because a line joining two factor levels does
not carry the interpretation that the two mean levels are equal. Rather it has the interpretation
that there is no evidence of a difference between the two mean levels. Thus, there is evidence
that μ4 < μ2, but the statements μ2 = μ6 and μ4 = μ6 are both plausible.
In summary, based solely on how well the carpets wear, the best ﬁber type is either type 2,
the nylon ﬁber type B, or type 6, the polypropylene ﬁber. The textile engineer may want to
investigate these two ﬁber types in greater detail to discover whether either one is much better
than the other.
COMPUTER NOTE
Practice performing a one-factor analysis of variance on your computer package. Make sure
that you can understand and interpret the analysis of variance table that the package presents
you with. Find out how to obtain the follow-up pairwise conﬁdence intervals. These are usually
referred to as “Tukey intervals” or “pairwise intervals.” Do not forget that you need to specify
a conﬁdence level for these.

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
515
11.1.5
Sample Size Determination
The sensitivity afforded by a one-factor analysis of variance depends upon the k sample sizes
n1, . . . , nk. Speciﬁcally, the power of the test of the null hypothesis that the factor level means
are all equal increases as the sample sizes increase. This power is the probability that the null
hypothesis is rejected when it is not true. In other words, for a given conﬁguration of unequal
factor level means μi, the probability of obtaining a small p-value in the analysis of variance
table, so that the experimenter has evidence that the factor level means are different, increases
as the sample sizes ni increase.
An increase in the sample sizes also results in a decrease in the lengths of the pairwise
conﬁdence intervals. As before, the consideration of these conﬁdence interval lengths is gen-
erally the most convenient manner in which to assess the sensitivity afforded by certain sample
sizes. The pairwise conﬁdence intervals have lengths
L =
√
2 sqα,k,ν

1
ni1
+ 1
ni2
which are different if the sample sizes ni are unequal. If the experimenter designs a balanced
experiment with sample sizes all equal to n, then these lengths are all equal to
L = 2sqα,k,ν
√n
As in other cases, notice that the conﬁdence interval length L is inversely proportional to the
square root of the sample size n
L ∝
1
√n
so that in order to halve the conﬁdence interval length, the sample size must be increased
fourfold.
If, prior to experimentation, an experimenter decides that a conﬁdence interval length no
larger than L is required, then the necessary sample size is
n ≃4s2q2
α,k,ν
L2
The experimenter needs to estimate the value of s = ˆσ. Also, remember that the critical point
qα,k,ν gets larger as the number of factor levels k increases, which results in a larger sample
size required. This formula can also be used to determine the sample sizes that are required in
a follow-up to an initial experiment in order to achieve increased sensitivity. In this case the
values of s and qα,k,ν from the initial experiment can be used.
Example 63
Roadway Base
Aggregates
For this balanced design, the lengths of the pairwise conﬁdence intervals are all equal to
L = 2sqα,k,ν
√n
= 2 × 1479 × 3.81
√
10
= 3564
Even though the analysis of variance indicates that there is no evidence that any of the
aggregate types have different average values of resilient modulus, the pairwise conﬁdence
intervals indicate that the difference in average resilient modulus values between two locations
could be as large as 3144. Therefore, the construction engineer decides to undertake additional
sampling in order to increase the sensitivity of the comparisons between the different types
of aggregate.

516
CHAPTER 11
THE ANALYSIS OF VARIANCE
The engineer decides to aim for a conﬁdence interval length of 2000. In this case, if all
of the pairwise conﬁdence intervals contain 0 so that there is no evidence of any difference
between the aggregate types, the engineer will know that if there is a difference in the average
values of resilient modulus, then it can be no larger than 2000. With values s = 1479 and
q0.05,4,36 = 3.81 from the ﬁrst experiment, the estimated sample size required is
n ≃4s2q2
α,k,ν
L2
= 4 × 14792 × 3.812
20002
= 31.8
Since the experimenter already has 10 observations of each aggregate type, this result implies
that at least 22 more observations should be taken from each of the four aggregate types.
11.1.6
Model Assumptions
It is important to remember that the analysis of variance for a one-factor layout is based upon
the modeling assumption that the observations are distributed independently with a normal
distribution that has a common variance. The experimenter should be careful to notice whether
any of these assumptions are violated in an obvious way.
The independence of the data observations can be judged from the manner in which a
data set is collected. Where possible, the randomization of experimental units between the k
factor levels helps to ensure independence. If the sample sizes are large enough, histograms
of the data observations within each factor level may be constructed to indicate whether the
distribution is obviously not normal. Often, prior experience with data observations of the kind
under consideration tell the experimenter whether nonnormality is a problem. In any case, the
analysis of variance is fairly robust to the distribution of the observations, so that it provides
fairly accurate results as long as the distribution is not very different from a normal distribution.
Nonparametric procedures for one-way layouts discussed in Section 15.3.1 provide alternative
analysis techniques for data observations that clearly do not have a normal distribution.
Theequalityofthevariancesforeachofthe k factorlevelscanbejudgedfromacomparison
of the sample variances s2
i or from a comparison of the lengths of boxplots of the observations
at each factor level. Again, the analysis of variance provides fairly accurate results unless the
variances are very different. If the variances are different by more than a factor of ﬁve, say,
then it is probably better for an experimenter to make pairwise comparisons between the k
factor levels, one pair at a time, and use a small individual conﬁdence level such as 90%. The
pairwise comparisons can be made using the general procedure discussed in Section 9.3.1,
which does not employ a pooled variance.
11.1.7
Problems
11.1.1 Use Table IV to determine whether these probabilities
are greater than 10%, between 5% and 10%, between
1% and 5%, or less than 1%. Then use a computer
package to evaluate them exactly.
(a) P(X ≥4.2) where X has an F4,15 distribution
(b) P(X ≥2.3) where X has an F7,30 distribution
(c) P(X ≥31.7) where X has an F3,10 distribution
(d) P(X ≥9.3) where X has an F3,12 distribution
(e) P(X ≥0.9) where X has an F6,60 distribution
11.1.2 Find the missing values in the analysis of variance table
shown in Figure 11.20. What is the p-value?
11.1.3 Find the missing values in the analysis of variance table
shown in Figure 11.21. What is the p-value?
11.1.4 A balanced experimental design has a sample size of
n = 12 observations at each of k = 7 factor levels. The
total sum of squares is SST = 133.18, and the sample
averages are ¯x1· = 7.75, ¯x2· = 8.41, ¯x3· = 8.07,
¯x4· = 8.30, ¯x5· = 8.17, ¯x6· = 8.81, and ¯x7· = 8.32.
Compute the analysis of variance table. (Hint: Calculate
SSTr and then subtract it from SST to obtain SSE.)
What is the p-value?

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
517
FIGURE 11.20
An analysis of variance table
?
?
111.4
?
?
23
461.9
?
28
?
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Treatments
Error
Total
FIGURE 11.21
An analysis of variance table
Source
Degreesof freedom
Sumof squares
Mean squares
F-statistic
p-value
Treatments
Error
Total
7
?
?
5.01
?
?
?
3.62
29
?
11.1.5 An experiment to compare k = 4 factor levels has
n1 = 12 and ¯x1· = 16.09, n2 = 8 and ¯x2· = 21.55,
n3 = 13 and ¯x3· = 16.72, and n4 = 11 and ¯x4· = 17.57.
The total sum of squares is SST = 485.53. Compute the
analysis of variance table. What is the p-value?
11.1.6 An experiment to compare k = 3 factor levels has
n1 = 17 and ¯x1· = 32.30, n2 = 20 and ¯x2· = 34.06, and
n3 = 18 and ¯x3· = 32.02. Also,
3

i=1
ni

j=1
x2
i j = 59,843.21
Compute the analysis of variance table. What is the
p-value?
11.1.7 A balanced experimental design has a sample size of
n = 14 observations at each of k = 4 factor levels. The
averages and variance estimates at each of the four
factor levels are ¯x1· = 0.705 and s2
1 = 0.766 × 10−3,
¯x2· = 0.715 and s2
2 = 1.276 × 10−3, ¯x3· = 0.684 and
s2
3 = 2.608 × 10−3, and ¯x4· = 0.692 and
s2
4 = 1.725 × 10−3. Compute the analysis of variance
table. What is the p-value?
11.1.8 A balanced experimental design has a sample size of
n = 11 observations at each of k = 3 factor levels. The
sample averages are ¯x1· = 48.05, ¯x2· = 44.74, and
¯x3· = 49.11, and MSE = 4.96.
(a) Calculate pairwise conﬁdence intervals for the factor
level means with an overall conﬁdence level of 95%.
(b) Make a diagram showing which factor level means
are known to be different and which ones are
indistinguishable.
(c) What additional sampling would you recommend
to reduce the lengths of the pairwise conﬁdence
intervals to no more than 2.0 ?
11.1.9 A balanced experimental design has a sample size of
n = 6 observations at each of k = 6 factor levels. The
sample averages are ¯x1· = 136.3, ¯x2· = 152.1,
¯x3· = 125.7, ¯x4· = 130.2, ¯x5· = 142.3, and ¯x6· = 128.0,
and MSE = 15.95.
(a) Calculate pairwise conﬁdence intervals for the
factor level means with an overall conﬁdence level
of 95%.
(b) Make a diagram showing which factor level means
are known to be different and which ones are
indistinguishable.
(c) What additional sampling would you recommend
to reduce the lengths of the pairwise conﬁdence
intervals to no more than 10.0 ?
11.1.10 If each observation xi j in a one-factor layout is replaced
by the value axi j + b, for some constants a and b, what
happens to the p-value in the analysis of variance
table?
11.1.11 Consider the data set given in DS 11.1.1.
(a) Calculate the sample averages ¯xi·
(b) Calculate the overall average ¯x··.
(c) Calculate the treatment sum of squares SSTr.
(d) Calculate k
i=1
ni
j=1 x2
i j
(e) Calculate the total sum of squares SST.
(f) Calculate the error sum of squares SSE.
(g) Complete the analysis of variance table. What is the
p-value?
(h) Compute conﬁdence intervals for the pairwise
differences of the factor level means with an overall
conﬁdence level of 95%.
(i) Make a diagram showing which factor level means
are known to be different and which ones are
indistinguishable.

518
CHAPTER 11
THE ANALYSIS OF VARIANCE
(j) If the experimenter wants conﬁdence intervals for
the pairwise differences of the factor level means
with an overall conﬁdence level of 95% that are no
longer than 1.0, how much additional sampling
would you recommend?
11.1.12 Complete parts (a)–(i) of Problem 11.1.11 for the data
set in DS 11.1.2.
11.1.13 A factory has three production lines producing glass
sheets that are all supposed to be of the same thickness.
A quality inspector takes a random sample of n = 30
sheets from each production line and measures their
thicknesses. The glass sheets from the ﬁrst production
line have a sample average of ¯x1· = 3.015 mm with a
sample standard deviation of s1 = 0.107 mm. The glass
sheets from the second production line have a sample
average of ¯x2· = 3.018 mm with a sample standard
deviation of s2 = 0.155 mm, while the glass sheets from
the third production line have a sample average of
¯x3· = 2.996 mm with a sample standard deviation of
s3 = 0.132 mm. What conclusions should the quality
inspector draw?
11.1.14 A retail company is interested in examining whether
the time taken for it to obtain approval for a credit
card number by phone varies from one day of the
week to another. The manager collects the following
results. On Mondays, a sample of n1 = 20 cases had a
sample average of ¯x1· = 20.44 seconds with a sample
standard deviation of s1 = 1.338 seconds. On
Wednesdays, a sample of n2 = 15 cases had a sample
average of ¯x2· = 16.33 seconds with a sample standard
deviation of s2 = 1.267 seconds, and on Fridays, a
sample of n3 = 18 cases had a sample average of
¯x3· = 15.34 seconds with a sample standard deviation
of s3 = 1.209 seconds. What should the manager
conclude?
11.1.15 Infrared Radiation Readings
The data set in DS 11.1.3 concerns the infrared radiation
readings from an energy source measured by a particular
meter with various levels of background radiation levels.
The meter’s manufacturers are interested in how the
readings are affected by the background radiation levels.
What would you advise them? Perform the calculations
by hand.
11.1.16 Keyboard Layout Designs
DS 11.1.4 gives the times taken to perform a numerical
task using three different kinds of keyboard layouts for
the numerical keys. Perform hand calculations to
investigate how the different layouts affect the time
taken to perform the task.
11.1.17 Dispersion Polymerization
A chemical engineer is interested in how polymethyl
methacrylate (PMMA) particles can be prepared by
dispersion polymerization in the presence of a stabilizer
polyvinyl pyrrolidone (PVP). DS 11.1.5 gives the
average particle diameters achieved in various
experiments with three different amounts of the
stabilizer. How does the amount of stabilizer affect the
average particle diameter? Use a computer package to
obtain a graphical presentation of the data set, an
analysis of variance table, and pairwise comparisons
between the different stabilizer levels.
11.1.18 Computer Assembly Methods
A computer manufacturer has production lines that
utilize three different assembly methods. These methods
differ in the order in which tasks are performed and the
consequent layout of the production lines. A quality
inspector randomly observes 30 computers assembled
under each of the three methods and records the
assembly times. The resulting data set is given in
DS 11.1.6. Is there any evidence that one assembly
method is any quicker than the others? Use a computer
package to obtain a graphical presentation of the data
set, an analysis of variance table, and pairwise
comparisons between the different assembly methods.
11.1.19 Three catalysts were compared to investigate how
they affect the strength of a compound. When catalyst
A was used, 8 samples of the compound were obtained
and the strengths of these 8 samples had a sample
average of 42.91 and a sample standard deviation of
5.33. When catalyst B was used, 11 samples of the
compound were obtained and the strengths of these
11 samples had a sample average of 44.03 and a sample
standard deviation of 4.01. When catalyst C was used,
10 samples of the compound were obtained and the
strengths of these 10 samples had a sample average
of 43.72 and a sample standard deviation of 5.10.
Construct an analysis of variance table. Is there
sufﬁcient evidence to conclude that there is a difference
between the three catalysts in terms of the strength of
the compound?
11.1.20 The data set in DS 11.1.7 is a one-way layout to
compare the quality scores of a product made from ﬁve
different treatment methods.
(a) Construct the ANOVA table.

11.1 ONE-FACTOR ANALYSIS OF VARIANCE
519
(b) Construct pairwise comparisons of the treatment
effects and make a graphical presentation of what
you ﬁnd. Which treatment mean is largest? Which
treatment mean is smallest?
11.1.21 In a one-way layout to compare ﬁve treatments an
experiment gave n1 = 10, ¯x1· = 46.09, n2 = 9,
¯x2· = 42.21, n3 = 9, ¯x3· = 55.32, n4 = 7, ¯x4· = 54.62,
n5 = 10, ¯x5· = 38.79, with s = ˆσ = 4.33. Construct
pairwise comparisons of the treatment means μi,
1 ≤i ≤5, and draw a diagram to explain what you ﬁnd.
Which treatment mean μi is largest? Which treatment
mean μi is smallest?
11.1.22 Metal Alloy Comparisons
An experiment was performed to compare four metal
alloys. Various samples were made of each alloy, and
their strengths were measured. The following data were
obtained:
Type A : sample size 8, sample mean 10.50, sample
standard deviation 1.02.
Type B : sample size 8, sample mean 9.22, sample
standard deviation 0.86.
Type C : sample size 9, sample mean 6.32, sample
standard deviation 1.13.
Type D : sample size 6, sample mean 11.39, sample
standard deviation 0.98.
(a) Construct the ANOVA table. What null hypothesis
is tested by the ANOVA table? What bounds can
you place on the p-value? What is your conclusion?
(b) Make a graphical representation of the differences
between the alloy types. Which alloy type is the
strongest? Which alloy type is the weakest?
11.1.23 Durations of Investigatory Surgical Procedures
Thirty patients with a similar medical condition were
randomly allocated among six physicians, so that each
physician received exactly ﬁve patients. The times in
minutes taken for the physicians to perform an
investigatory surgical procedure on the patients were
measured and the data set given in DS 11.1.8 was
obtained. Construct an appropriate ANOVA table. What
does the p-value in the ANOVA table tell you?
Construct an appropriate graphical representation of the
differences between the physicians. Which physician is
the quickest? Which physician is the slowest?
11.1.24 E. Coli Colonies in Riverwater
Four samples of water were taken at six different
locations along a river and measurements were obtained
of the amount of E. Coli colonies within the water as
given in DS 11.1.9. Investigate whether there is any
evidence that the E. Coli pollution levels are different at
the six locations. Location 6 is the highest point
upstream, and location 1 is the furthest point
downstream. There is a polluting source between
locations 5 and 6. Make a graphical presentation that
illustrates the differences between the E. Coli levels at
the six locations. Which location has the highest E. Coli
level? Which location has the smallest E. Coli level?
11.1.25 In a one-way layout, seven observations are taken from
treatment 1 and their sum is 327.8, eight observations
are taken from treatment 2 and their sum is 381.3, seven
observations are taken from treatment 3 and their sum is
337.0, and six observations are taken from treatment 4
and their sum is 292.9. The ANOVA table gives a
p-value of exactly 1%.
(a) Write out the complete ANOVA table.
(b) Calculate pairwise conﬁdence intervals for the
treatment means with an overall conﬁdence level of
95%, and make a graphical representation of what
you ﬁnd.
11.1.26 If the p-value in the ANOVA table for a one-way
analysis is larger than 10%, then:
A. The typical follow-up conﬁdence intervals (with
conﬁdence level 95%) for the pairwise comparisons
will all contain zero.
B. Some of the typical follow-up conﬁdence intervals
(with conﬁdence level 95%) for the pairwise
comparisons may not contain zero.
C. None of the typical follow-up conﬁdence intervals
(with conﬁdence level 95%) for the pairwise
comparisons can contain zero.
D. None of the above.
11.1.27 Consider a one-way analysis of variance. (I) A p-value
larger than 10% in the ANOVA table proves that there is
no difference between any of the treatments. (II) A
p-value smaller than 1% in the ANOVA table proves that
each treatment is different from every other treatment.
A. Both (I) and (II) are true.
B. Both (I) and (II) are false.
C. (I) is true but (II) is false.
D. (I) is false but (II) is true.
11.1.28 An analysis of variance can be used to compare:
A. Two treatments with categorical data
B. Three treatments with continuous data
C. Three treatments with categorical data
D. None of the above

520
CHAPTER 11
THE ANALYSIS OF VARIANCE
11.2
Randomized Block Designs
11.2.1
One-Factor Layouts with Blocks
Recall that in the comparison of two population means discussed in Chapter 9, a distinction was
made between independent samples and paired samples. In paired samples, comparisons are
made within each pair of observations. A complete randomized block design is an extension
of paired samples to accommodate the comparison of a set of k population means or factor
levels.
A randomized block design consists of a set of b blocks, which each contain one observa-
tion from each of the k factor levels under consideration. The comparisons of the factor level
means are then based upon comparisons of the data observations within the blocks. In the
same way that paired observations help eliminate the variation due to an extraneous source
so that a more sensitive comparison can be made between two population means, a blocked
design rather than an ordinary completely randomized design (discussed in Section 11.1)
helps to eliminate variation due to the blocking variable, and thereby allows a more sensitive
comparison of the set of k factor level means. The examples discussed in this section illustrate
the use of a blocking variable. Typically a blocking variable may be the day on which an
experiment is performed, the person or operator involved in an experiment, or the batch of
raw material used in an experiment.
Figure 11.22 shows a data set obtained from a randomized block design to compare k
factor levels. There are b blocks and the data set appears similar to a balanced one-factor
layout with b observations in each factor level. However, the difference with a randomized
block design is that a data observation xi j is known to be not from just the ith factor level, but
also from the jth block.
The factor level sample averages are
¯xi· = xi1 + · · · + xib
b
1 ≤i ≤k
FIGURE 11.22
A randomized block design data set
Level 1
Level i
Level k
Block 1
Block j
Block b
Factor levels
Block
sample
averages
Overall
average
Factor level sample averages
x11
x
x
x
x
x
x
x
x
1j
1b
i1
ij
ib
k1
kj
kb
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Blocks
¯x .
1
¯x .i
¯x .k
¯x ..
¯x.b
¯x.j
¯x 1.

11.2 RANDOMIZED BLOCK DESIGNS
521
and the block sample averages are
¯x· j = x1 j + · · · + xkj
k
1 ≤j ≤b
The total number of observations is nT = kb, and the overall average is
¯x·· = ¯x1· + · · · + ¯xk·
k
= ¯x·1 + · · · + ¯x·b
b
= x11 + · · · + xkb
kb
It is usual to model the data observations as
xi j = μi j + ϵi j
where μi j is the mean of the observation in the i j-cell, and the error terms ϵi j are independently
distributed as
ϵi j ∼N(0, σ 2)
Thus the error terms are taken to be independently normally distributed with zero means
and a common variance σ 2. Equivalently, the data observation xi j can be thought of as an
observation from a
N(μi j, σ 2)
distribution.
It is convenient to write the cell means as
μi j = μ + αi + β j
In this expression, μ is referred to as the overall average of the cell means, the parameters
α1, . . . , αk
are referred to as the factor effects, and the parameters
β1, . . . , βb
are referred to as the block effects. The factor effects and the block effects are restricted to
sum to 0, so that
α1 + · · · + αk = 0
and
β1 + · · · + βb = 0
Notice that under this model and with the restrictions on the factor effects and block
effects,
E(¯x··) =
k
i=1
b
j=1 E(xi j)
kb
=
k
i=1
b
j=1 μi j
kb
=
k
i=1
b
j=1(μ + αi + β j)
kb
= μ
Also,
E(¯xi·) =
b
j=1 E(xi j)
b
=
b
j=1 μi j
b
=
b
j=1(μ + αi + β j)
b
= μ + αi
and
E(¯x· j) =
k
i=1 E(xi j)
k
=
k
i=1 μi j
k
=
k
i=1(μ + αi + β j)
k
= μ + β j

522
CHAPTER 11
THE ANALYSIS OF VARIANCE
These results lead to the unbiased parameter estimates
ˆμ = ¯x··
ˆαi = ¯xi· −¯x··
ˆβ j = ¯x· j −¯x··
so that the cell mean estimates are
ˆμi j = ˆμ + ˆαi + ˆβ j = ¯xi· + ¯x· j −¯x··
For this model, the factor level means can be thought of as being
μ + α1, . . . , μ + αk
so that the factor effect αi is the difference between the ith factor level mean and the overall
mean μ. The factor level means are all equal when the factor effects αi are all equal to 0, and
so the null hypothesis
H0 : α1 = · · · = αk = 0
implies that the k factor levels are indistinguishable. Rejection of this null hypothesis indicates
that there is evidence that the k factor level means are not all equal. Finally, the block effect
β j measures the difference between the mean of the jth block level and the overall mean μ.
Example 55
Heart Rate Reductions
In a further evaluation of the effect of a new drug on inducing a temporary reduction in a
patient’s heart rate, three dosage levels of the new drug are compared. When the new drug
was compared with the standard drug, a paired design was adopted in order to account for the
variability in the drug efﬁcacies from one patient to another. Similarly, a randomized block
design is adopted to compare the three dosage levels of the new drug, with the blocks being
the different patients.
The three dosage levels, low, medium, and high, are administered to each of the patients in
a random order, with suitable precautions being made to ensure that there are no “carryover”
effects from one dosage to the next. The data set consisting of the percentage reductions in the
heart rate after 5 minutes is shown in Figure 11.23. There are k = 3 factor levels of interest
(the dosage levels) and b = 8 blocks corresponding to eight patients who participated in the
experiment.
Figure 11.24 provides a graphical presentation of the data set. Notice that the ordering
of the x-axis is arbitrary because it depends upon the manner in which the eight patients are
assigned the numbers 1 to 8. This graph, together with the factor level sample averages
¯x1· = 21.4,
¯x2· = 29.7,
and
¯x3· = 32.2
FIGURE 11.23
Percentage reductions in heart rate
5 minutes after drug is
administered
Drug dosage level
Low
Medium
High
Averages
1
12.2
16.5
25.4
18.033
2
29.3
34.7
29.4
31.133
3
14.1
26.1
34.0
24.733
4
18.5
33.3
21.7
24.500
Patient
5
32.9
36.9
43.0
37.600
6
15.3
32.6
32.1
26.667
7
26.1
28.9
40.9
31.967
8
22.8
28.6
31.1
27.500
Averages
21.400
29.700
32.200
27.767

11.2 RANDOMIZED BLOCK DESIGNS
523
FIGURE 11.24
Graphical presentation of the heart
rate reductions data set
Patient
Percentage reduction
8
7
6
5
4
3
2
1
0
45
40
35
30
25
20
15
10
low
Dose
high
medium
suggest that the low dosage results in a smaller heart rate reduction than the medium and high
dosages. Is this difference statistically signiﬁcant? This question can be answered by testing
the null hypothesis
H0 : α1 = α2 = α3 = 0
Acceptance of the null hypothesis implies that there is no evidence that the different dosage
levels have any effect on the average heart rate reductions, whereas rejection of the null
hypothesis indicates that there is evidence that the different dosage levels have different
average effects on heart rate reduction.
The block sample averages ¯x· j are seen to be varied, and this indicates that the drug efﬁcacy
varies considerably from one patient to another. This ﬁnding underlines why a blocked design
is appropriate. A similar (unblocked) completely randomized design would require a total of
24 patients split into three groups of eight patients, with each group receiving one (and only
one) of the dosage levels. It is clear that the comparison of the dosage levels is then hindered
by the variation between the groups due to the variabilities in the patients.
Example 65
Comparing Types
of Wheat
Agricultural experimentation provides a classic example of the use of blocking. Consider an
experiment to compare four types of wheat with ﬁve ﬁelds available for use. As Figure 11.25
shows, it is sensible to take the ﬁelds as blocks since growing conditions may vary from one
ﬁeld to another due to soil and drainage conditions. Each ﬁeld is partitioned into four units,
and the four types of wheat are randomly allocated to the four units within each ﬁeld. If an
experiment of this kind is run without blocking, so that different wheat types are allocated
to different ﬁelds, then the variation between the different ﬁelds will hinder the comparisons
between the wheat types.
Figure11.26 showsadatasetofcropyieldsfromthisblockedexperiment,andFigure 11.27
provides a graphical presentation of the data set. Remember that in the graph the ordering
of the x-axis (that is the numbering of the ﬁelds) is arbitrary. Wheat type 2 has the largest
average yield, ¯x2· = 156.72. Is the difference between wheat type 2 and the other wheat types
statistically signiﬁcant? This question can be addressed by testing the null hypothesis
H0 : α1 = α2 = α3 = α4 = 0
which states that the four types of wheat produce equal yields on average.

524
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.25
Using ﬁelds as blocks in an
experiment to compare wheat types
Field 1
(block 1)
Field 2
(block 2)
Field 3
(block 3)
Field 4
(block 4)
Field 5
(block 5)
Random
allocation
within
each
field
{
Wheat type 1
Wheat type 2
Wheat type 3
Wheat type 4
FIGURE 11.26
Crop yields for wheat experiment
Wheat types
Type 1
Type 2
Type 3
Type 4
Averages
1
164.4
184.3
161.2
185.8
173.925
2
145.0
142.1
110.8
135.4
133.325
Field
3
152.5
159.6
168.6
154.1
158.700
4
138.5
137.2
134.9
123.2
133.450
5
161.7
160.4
166.1
159.9
162.025
Averages
152.420
156.720
148.320
151.680
152.285
Example 66
Infrared Radiation
Meters
A research establishment has k = 5 infrared radiation meters and an experiment is run
to investigate whether they are providing essentially equivalent readings. A total of b = 15
objects of different materials, temperatures, and sizes are used in the experiment, and radiation
readings from each object are taken with each of the ﬁve meters. The resulting data set is given
in Figure 11.28 and is presented graphically in Figure 11.29.
Notice that the different objects are taken to be the blocks, and the block sample averages
¯x· j indicate a wide variability in the radiation levels obtained from the 15 objects. As expected,
this variability is much larger than the variability among the ﬁve meters, as evidenced by the
meter sample averages ¯xi·. Of course, it makes no sense to run this experiment without blocking
on the objects, since using one meter on one set of objects and another meter on another set
of objects does not allow a useful comparison of the two meters.

11.2 RANDOMIZED BLOCK DESIGNS
525
FIGURE 11.27
Graphical presentation of the crop
yields for the wheat experiment
Crop yield
190
180
170
160
150
140
130
120
110
1
2
3
Field
4
5
4
3
Wheat type
1
2
FIGURE 11.28
Infrared radiation readings
Infrared radiation meters
Meter 1
Meter 2
Meter 3
Meter 4
Meter 5
Averages
1
2.00
2.01
1.89
2.09
2.04
2.006
2
2.27
2.24
2.44
2.40
2.38
2.346
3
1.25
1.03
1.20
1.19
1.33
1.200
4
2.97
2.91
2.99
3.05
3.07
2.998
5
3.97
3.72
3.75
3.90
3.81
3.830
6
1.60
1.67
1.79
1.77
1.83
1.732
7
2.27
2.06
2.16
2.28
2.37
2.228
Object
8
2.55
2.42
2.54
2.40
2.42
2.466
9
3.22
3.09
3.16
3.24
3.15
3.172
10
1.52
1.27
1.50
1.47
1.40
1.432
11
2.10
1.93
2.13
2.07
2.16
2.078
12
1.93
1.72
1.98
1.83
1.97
1.886
13
1.16
1.16
1.13
1.23
1.18
1.172
14
3.57
3.47
3.63
3.58
3.59
3.568
15
2.30
2.20
2.18
2.29
2.44
2.282
Averages
2.312
2.193
2.298
2.319
2.343
2.293
Acceptance of the null hypothesis
H0 : α1 = α2 = α3 = α4 = α5 = 0
implies that there is no evidence of a difference between the average meter readings. However,
rejection of this null hypothesis indicates that at least one of the meters needs recalibrating.
11.2.2
Partitioning the Total Sum of Squares
Total Sum of Squares
As in a completely randomized one-factor layout, the total sum of
squares
SST =
k

i=1
b

j=1
(xi j −¯x··)2 =
k

i=1
b

j=1
x2
i j −kb¯x2
··

526
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.29
Graphical presentation of the
infrared radiation readings data set
Object
Infrared radiation level
16
14
12
10
8
6
4
2
0
4.0
3.5
3.0
2.5
2.0
1.5
1.0
Meter
3
4
5
1
2
FIGURE 11.30
Partition of the total sum of squares
for a randomized block design
Total sum of squares
SST
Treatment sum of squares
SSTr
Block sum of squares
SSBl
Error sum of squares
SSE
measures the total variability in the data observations. However, as Figure 11.30 shows, in
randomized block designs SST is partitioned into three components, the treatment sum of
squares SSTr, the block sum of squares SSBl, and the error sum of squares SSE, so that
SST = SSTr + SSBl + SSE
Treatment Sum of Squares
In a similar manner to the completely randomized one-factor
layout, the treatment sum of squares
SSTr =
k

i=1
b(¯xi· −¯x··)2 =
k

i=1
b¯x2
i· −kb¯x2
··
measures the variability between the k factor levels.
Block Sum of Squares
The block sum of squares SSBl measures the variability among
the b blocks. It is based upon the comparison of the b block sample averages ¯x· j and is deﬁned
to be
SSBl =
b

j=1
k(¯x· j −¯x··)2 =
b

j=1
k ¯x2
· j −kb¯x2
··

11.2 RANDOMIZED BLOCK DESIGNS
527
Notice that the form of the block sum of squares SSBl is similar to that of the treatment sum of
squares SSTr except that it involves block sample averages ¯x· j instead of factor level sample
averages ¯xi·.
Error Sum of Squares
The error sum of squares SSE is deﬁned to be the sum of the squared
differences between the data observations xi j and the estimated cell means ˆμi j, so that
SSE =
k

i=1
b

j=1
(xi j −ˆμi j)2 =
k

i=1
b

j=1
(xi j −¯xi· −¯x· j + ¯x··)2
When the square is expanded, or with subtraction from the total sum of squares as shown here,
SST can be rewritten as
SSE = SST −SSTr −SSBl
=

k

i=1
b

j=1
x2
i j −kb¯x2
··

−

k

i=1
b¯x2
i· −kb¯x2
··

−
 b

j=1
k ¯x2
· j −kb¯x2
··

=
k

i=1
b

j=1
x2
i j −
k

i=1
b¯x2
i· −
b

j=1
k ¯x2
· j + kb¯x2
··
Sum of Squares Partition for a Randomized Block Design
In a randomized block design with k factor levels and b blocks, the total sum of squares
SST =
k

i=1
b

j=1
(xi j −¯x··)2 =
k

i=1
b

j=1
x2
i j −kb¯x2
··
measures the total variability in the data set. It can be partitioned into the treatment
sum of squares
SSTr =
k

i=1
b(¯xi· −¯x··)2 =
k

i=1
b¯x2
i· −kb¯x2
··
which measures the variability between the factor levels, the block sum of squares
SSBl =
b

j=1
k(¯x· j −¯x··)2 =
b

j=1
k ¯x2
· j −kb¯x2
··
which measures the variability between the blocks, and the error sum of squares
SSE =
k

i=1
b

j=1
(xi j −¯xi· −¯x· j + ¯x··)2
=
k

i=1
b

j=1
x2
i j −
k

i=1
b¯x2
i· −
b

j=1
k ¯x2
· j + kb¯x2
··

528
CHAPTER 11
THE ANALYSIS OF VARIANCE
Example 55
Heart Rate Reductions
With
¯x·· = 12.2 + · · · + 31.1
24
= 27.7667
and
3

i=1
8

j=1
x2
i j = 12.22 + · · · + 31.12 = 20,065.76
the total sum of squares is
SST = 20,065.76 −(3 × 8 × 27.76672) = 1562.05
The treatment sum of squares is
SSTr = 8 × (21.402 + 29.702 + 32.202) −(3 × 8 × 27.76672) = 511.41
and the block sum of squares is
SSBl = 3 × (18.0332 + · · · + 27.5002) −(3 × 8 × 27.76672) = 724.68
The error sum of squares can then be calculated by subtraction to be
SSE = SST −SSTr −SSBl = 1562.05 −511.41 −724.68 = 325.96
Example 65
Comparing Types
of Wheat
For this example
¯x·· = 164.4 + · · · + 159.9
20
= 152.285
and
4

i=1
5

j=1
x2
i j = 164.42 + · · · + 159.92 = 470,643.92
The total sum of squares is then
SST = 470,643.92 −(4 × 5 × 152.2852) = 6829.51
The treatment sum of squares is
SSTr = 5 × (152.422 + 156.722 + 148.322 + 151.682) −(4 × 5 × 152.2852)
= 178.87
and the block sum of squares is
SSBl = 4 × (173.9252 + · · · + 162.0252) −(4 × 5 × 152.2852) = 5274.10
As before, the error sum of squares can be calculated by subtraction to be
SSE = SST −SSTr −SSBl = 6829.5 −178.9 −5274.2 = 1376.44
Example 66
Infrared Radiation
Meters
For this example the sums of squares can be shown to be
SST = 45.9536
SSTr = 0.2022
SSBl = 45.4816
SSE = 0.2698

11.2 RANDOMIZED BLOCK DESIGNS
529
11.2.3
The Analysis of Variance Table
Asincompletelyrandomizeddesigns,meansquaresforrandomizedblockdesignsareobtained
by dividing the sums of squares by the appropriate degrees of freedom. Treatments have k −1
degrees of freedom, so that the mean squares for treatments is
MSTr = SSTr
k −1
and blocks have b −1 degrees of freedom so that the mean squares for blocks is
MSBl = SSBl
b −1
For randomized block designs, the degrees of freedom for error are (k −1)(b −1), so that
the mean square error is
MSE =
SSE
(k −1)(b −1)
Notice that the degrees of freedom for treatments, blocks, and error sum to kb −1.
The mean square error MSE is distributed as
MSE ∼σ 2
χ2
(k−1)(b−1)
(k −1)(b −1)
so that, as before, the mean square error MSE is an unbiased point estimate of the error
variance σ 2 and can be written as ˆσ 2. Also, it can be shown that
E(MSTr) = σ 2 + b
k
i=1 α2
i
k −1
and
E(MSBl) = σ 2 + k
b
j=1 β2
j
b −1
Notice that under the null hypothesis
H0 : α1 = · · · = αk = 0
the expected mean square for treatments is
E(MSTr) = σ 2
and in fact when the null hypothesis is true, the F-statistic
FT = MSTr
MSE
is an observation from an Fk−1,(k−1)(b−1) distribution. Consequently the p-value for the null
hypothesis is calculated as
p-value = P(X ≥FT )
where the random variable X has a Fk−1,(k−1)(b−1) distribution.
Also, it should be noted that in a similar fashion, with an F-statistic
FB = MSBl
MSE
the p-value
p-value = P(X ≥FB)
where the random variable X has a Fb−1,(k−1)(b−1) distribution, measures the plausibility of
the null hypothesis that the block effects β j are all zero. In other words, this p-value measures
the plausibility of the blocks being indistinguishable from each other.

530
CHAPTER 11
THE ANALYSIS OF VARIANCE
T
T
Source
reatments
Blocks
Error
otal
F-statistic
FT = MSTr
MSE
FB = MSBl
MSE
p-value
P(Fk−1,(k−1)(b−1) ≥FT )
P(Fb−1,(k−1)(b−1) ≥FB)
Mean squares
MSTr = SSTr
k−1
MSBl = SSBl
b−1
MSE =
SSE
(k−1)(b−1)
Degreesof freedom
k −1
b −1
(k −1)(b −1)
kb −1
Sumof squares
SSTr
SSBl
SSE
SST
FIGURE 11.31
The analysis of variance table for a randomized block design
The analysis of variance table for this randomized block design is shown in Figure 11.31.
Notice that it consists of four rows corresponding to treatments, blocks, error, and their total.
The F-statistic and p-value of primary interest to the experimenter are in the treatments row,
and they indicate the evidence of a difference between the factor level means.
F-Tests for a Randomized Block Design
In a randomized block design with k factor levels and b blocks, the treatments have
k −1 degrees of freedom, the blocks have b −1 degrees of freedom, and the error has
(k −1)(b −1) degrees of freedom. The mean squares are
MSTr = SSTr
k −1
MSBl = SSBl
b −1
MSE =
SSE
(k −1)(b −1)
A p-value for the null hypothesis that the factor level means are all equal is calculated
as
p-value = P(X ≥FT )
where the F-statistic is
FT = MSTr
MSE
and the random variable X has an Fk−1,(k−1)(b−1) distribution. The p-value
p-value = P(X ≥FB)
where the F-statistic is
FB = MSBl
MSE
and the random variable X has an Fb−1,(k−1)(b−1) distribution measures the plausibility
of the blocks being indistinguishable from each other.

11.2 RANDOMIZED BLOCK DESIGNS
531
FIGURE 11.32
The analysis of variance table for
the heart rate reductions experiment
10.98
4.45
0.001
0.009
2
7
23
Source
Degreesof freedom
Sum of squares
Mean squares
F-statistic
p-value
Dose level
Person
255.71
103.53
23.28
14
511.41
724.68
1562.05
325.96
Error
Total
Example 55
Heart Rate Reductions
Figure 11.32 shows the analysis of variance table for this example. Notice that the degrees of
freedom for treatments (dosage) is k −1 = 2 and the mean square for treatments is
MSTr = SSTr
2
= 511.41
2
= 255.71
Similarly, the degrees of freedom for blocks (person) is b −1 = 7 and the mean square for
blocks is
MSBl = SSBl
7
= 724.68
7
= 103.53
Error has (k −1)(b −1) = 14 degrees of freedom and the mean square error is
MSE = ˆσ 2 = SSE
14 = 325.96
14
= 23.28
The F-statistic for treatments is
FT = MSTr
MSE = 255.71
23.28 = 10.98
which gives a p-value of
p-value = P(X ≥10.98) ≃0.001
where the random variable X has an F2,14 distribution. This low p-value indicates that the
null hypothesis
H0 : α1 = α2 = α3 = 0
is not plausible, and so the experiment provides evidence that the different dosage levels have
different average effects on heart rate reduction. Are all three dosage levels signiﬁcantly differ-
ent? This question can be answered with the pairwise comparisons discussed in Section 11.2.4.
Finally, notice that the blocks (persons) have a p-value of 0.009. This value indicates
that there is evidence of a block effect, which in this case is a difference in response from
one patient to another. Thus, while it is clear that the dosage levels have different effects,
the researchers should also notice that the heart rate reductions induced by the drugs vary
considerably from one patient to another.
Example 65
Comparing Types
of Wheat
Figure 11.33 shows the analysis of variance table for this example. Notice that
MSTr = SSTr
k −1 = 178.87
3
= 59.62
MSBl = SSBl
b −1 = 5274.10
4
= 1318.55
MSE = ˆσ 2 =
SSE
(k −1)(b −1) = 1376.44
12
= 114.70

532
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.33
The analysis of variance table for
the wheat comparisons experiment
0.52
11.50
0.6766
0.0005
3
4
19
Source
Degreesof freedom
Sum of squares
Mean squares
F-statistic
p-value
Wheat type
Field
59.62
1318.55
114.70
12
178.87
5274.10
6829.51
1376.44
Error
Total
FIGURE 11.34
The analysis of variance table for
the infrared rediation meters
experiment
10.49
674.29
0.000
0.000
4
14
74
Source
Degreesof freedom
Sum of squares
Mean squares
F-statistic
p-value
Meter
Object
0.0505
3.2487
0.0048
56
0.2022
45.4816
45.9536
0.2698
Error
Total
The F-statistic for treatments is
FT = MSTr
MSE = 59.62
114.70 = 0.52
so that the p-value for the null hypothesis
H0 : α1 = α2 = α3 = α4 = 0
is
p-value = P(X ≥0.52) = 0.6766
where the random variable X has an F3,12 distribution. This large p-value indicates that the
experiment provides no evidence that the four types of wheat are in any way different with
respect to their average yields. However, notice that the p-value of 0.0005 for blocks (ﬁelds)
indicates that the fertility levels of the ﬁve ﬁelds are different. This ﬁnding conﬁrms that it
was sensible to run the experiment with ﬁelds as blocks.
Example 66
Infrared Radiation
Meters
Figure 11.34 shows the analysis of variance table for this experiment. Notice that
MSTr = SSTr
k −1 = 0.2022
4
= 0.0505
MSBl = SSBl
b −1 = 45.4816
14
= 3.2487
MSE = ˆσ 2 =
SSE
(k −1)(b −1) = 0.2698
56
= 0.0048
The F-statistics are
FT = MSTr
MSE = 0.0505
0.0048 = 10.49
and
FB = MSBl
MSE = 3.2487
0.0048 = 674.29
and both p-values are very small. Therefore, as well as the obvious difference in the radiation
levels of the 15 objects, the experiment indicates a difference between the readings of the
ﬁve radiation meters. This result immediately alerts the experimenters that at least one meter
needs recalibrating, and the pairwise comparisons discussed in Section 11.2.4 show which
meter this is.

11.2 RANDOMIZED BLOCK DESIGNS
533
COMPUTER NOTE
Find out how to obtain an analysis of variance table for a randomized block design with
your computer software package. Computationally, it is equivalent to a two-factor analysis of
variance (without interactions), and so this is usually the way in which the computer handles
randomized block designs. Two-factor designs are discussed in Section 14.1.
11.2.4
Pairwise Comparisons of the Factor Level Means
For randomized block designs the interpretation and use of a set of simultaneous conﬁdence
intervals for the pairwise differences between the factor level means are the same as for a
randomized one-way layout discussed in Section 11.1.4.
Pairwise Comparisons of Factor Level Means in a Randomized Block
Design
When an analysis of variance is performed for a randomized block design and the null
hypothesis that the factor level means are all equal is rejected, it is useful to calculate
the 1 −α conﬁdence level simultaneous conﬁdence intervals
μi1 −μi2 ∈

¯xi1· −¯xi2· −s qα,k,ν
√
b
, ¯xi1· −¯xi2· + s qα,k,ν
√
b

for the k(k −1)/2 pairwise differences of the factor level means. The critical point
qα,k,ν is the upper α point of the Studentized range distribution with parameters k and
degrees of freedom ν = (k −1)(b −1) (given in Table V), and s = ˆσ =
√
MSE. These
simultaneous conﬁdence intervals have an overall conﬁdence level of 1 −α and they
indicate which factor level means can be declared to be different, and by how much.
The pairwise conﬁdence intervals are all of the same length
L = 2s qα,k,ν
√
b
which is inversely proportional to the square root of the number of blocks b. Sample size
determinations for randomized block designs simply involve the determination of the number
ofblocksthataretobeused.Thesensitivityofthecomparisonofthefactorlevelmeansafforded
by a given number of blocks b can be assessed by considering the resulting conﬁdence interval
length L.
Example 55
Heart Rate Reductions
To construct conﬁdence intervals with an overall conﬁdence level of 95%, the required critical
point is
q0.05,3,14 = 3.70
so that
s qα,k,ν
√
b
=
√
23.28 × 3.70
√
8
= 6.31
The conﬁdence intervals are therefore
μ1 −μ2 ∈(21.40 −29.70 −6.31, 21.40 −29.70 + 6.31) = (−14.61, −1.99)
μ1 −μ3 ∈(21.40 −32.20 −6.31, 21.40 −32.20 + 6.31) = (−17.11, −4.49)
μ2 −μ3 ∈(29.70 −32.20 −6.31, 29.70 −32.20 + 6.31) = (−8.81, 3.81)

534
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.35
Schematic presentation of the
results of the heart rate reductions
example
.
2
.3
.1
21.40
29.70
32.20
x
x
x
¯
¯
¯
FIGURE 11.36
Schematic presentation of the
results of the infrared radiation
meters example
2.193
2.298
2.312
2.319
2.343
.
2
.3
.1
.
4
.
5
x
x
x
x
x
¯
¯
¯
¯
¯
Thus, the medium dose induces an average heart rate reduction of somewhere between 2.0%
and 14.6% more than the low dose, and the high dose induces an average heart rate reduction
of somewhere between 4.5% and 17.1% more than the low dose. However, there is no evidence
that the average heart rate reductions induced by the medium and high dosages are any different
(see Figure 11.35). If they are different, then the difference is between −8.8% and 3.8%.
Example 66
Infrared Radiation
Meters
With a conﬁdence level of 95%, the critical point for this example is
q0.05,5,56 ≃4.0
so that
s qα,k,ν
√
b
=
√
0.0048 × 4.0
√
15
= 0.072
The conﬁdence intervals are therefore
μi1 −μi2 ∈

¯xi1· −¯xi2· −0.072, ¯xi1· −¯xi2· + 0.072

where
¯x1· = 2.312
¯x2· = 2.193
¯x3· = 2.298
¯x4· = 2.319
¯x5· = 2.343
It can be seen that the conﬁdence intervals for the comparisons of meters 1, 3, 4, and 5 all
contain 0. However, meter 2 has an average reading signiﬁcantly smaller than each of these
four meters, since, for example, the conﬁdence interval
μ1 −μ2 ∈(2.312 −2.193 −0.072, 2.312 −2.193 + 0.072) = (0.047, 0.191)
does not contain 0. This ﬁnding reveals that meter 2 is probably providing inaccurate radiation
readings, and that it should be recalibrated in order to be consistent with the other four meters
(see Figure 11.36).

11.2 RANDOMIZED BLOCK DESIGNS
535
11.2.5
Model Assumptions
Itisimportanttorememberthattheanalysisofvarianceofarandomizedblockdesignpresented
in this section is based upon the modeling assumption
xi j = μ + αi + β j + ϵi j
where the error terms ϵi j are independently distributed
ϵi j ∼N(0, σ 2)
The assumption of independent error terms that are normally distributed with a common
variance is similar to the assumption made in Section 11.1 for the analysis of a completely
randomized design. An additional assumption of the analysis of a randomized block design
is that there is no interaction between the factor levels and the blocks. In other words, it is
assumed that the differences between the factor level effects are the same for each of the
blocks.
The analysis of a randomized block design is similar to the analysis of a two-factor
layout without replications, which is discussed in Section 14.1. Discussions of interactions
and methods of checking the model assumptions through residual analysis are provided in
Chapter 14. Also, it should be noted that a nonparametric method of analyzing a randomized
block design, which does not require the assumption that the data observations are normally
distributed, is discussed in Section 15.3.2.
11.2.6
Problems
11.2.1 Find the missing values in the analysis of variance table
shown in Figure 11.37. What are the p-values?
11.2.2 Find the missing values in the analysis of variance table
shown in Figure 11.38. What are the p-values?
11.2.3 A randomized block design has k = 4 factor levels
and b = 10 blocks. The total sum of squares is
SST = 3736.64 and the block sum of squares is
SSBl = 2839.97. The factor level sample averages are
¯x1· = 45.12, ¯x2· = 42.58, ¯x3· = 43.10, and ¯x4· = 41.86.
Compute the analysis of variance table. What are the
p-values?
11.2.4 A randomized block design has k = 5 factor levels
and b = 15 blocks. The total sum of squares is
SST = 1947.89 and the block sum of squares is
SSBl = 1527.12. The factor level sample averages
are ¯x1· = 21.50, ¯x2· = 22.51, ¯x3· = 26.08, ¯x4· = 21.03,
and ¯x5· = 23.43. Compute the analysis of variance table.
What are the p-values?
FIGURE 11.37
An analysis of variance table
?
9
?
39
Degrees of freedom
?
?
?
64.92
Sum of squares
?
?
1.12
Mean squares
3.02
?
F-statistic
?
?
p-value
Source
Treatments
Blocks
Error
Total
FIGURE 11.38
An analysis of variance table
Source
Degrees of freedom
Sumof squares
Mean squares
F-statistic
p-value
Treatments
Blocks
Error
Total
?
?
3.77
3.56
?
?
?
?
?
?
?
51.92
?
63
122.47

536
CHAPTER 11
THE ANALYSIS OF VARIANCE
11.2.5 A randomized block design has k = 3 factor levels and
b = 8 blocks. The block sum of squares is SSBl =
50.19, and the factor level sample averages are
¯x1· = 5.93, ¯x2· = 4.62, and ¯x3· = 4.78. Also,
3

i=1
8

j=1
x2
i j = 691.44
(a) Compute the analysis of variance table. What is the
p-value?
(b) Calculate pairwise conﬁdence intervals for the
factor level means with an overall conﬁdence level
of 95%.
11.2.6 Suppose that in a randomized block design the
observations in the ﬁrst block xi1 are replaced by the
values xi1 + c for some constant c. Which numbers
change in the analysis of variance table?
11.2.7 Consider the data set given in DS 11.2.1.
(a) Calculate the factor level sample averages ¯xi·
(b) Calculate the block sample averages ¯x· j
(c) Calculate the overall average ¯x··
(d) Calculate the treatment sum of squares SSTr.
(e) Calculate the block sum of squares SSBl.
(f) Calculate k
i=1
b
j=1 x2
i j
(g) Calculate the total sum of squares SST.
(h) Calculate the error sum of squares SSE.
(i) Complete the analysis of variance table. What are
the p-values?
(j) Compute conﬁdence intervals for the pairwise
differences of the factor level means with an overall
conﬁdence level of 95%.
(k) Make a diagram showing which factor level means
are known to be different and which ones are
indistinguishable.
(l) If the experimenter wants conﬁdence intervals for
the pairwise differences of the factor level means
with an overall conﬁdence level of 95% that are no
longer than 2.0, how much additional sampling
would you recommend?
11.2.8 Complete parts (a)–(k) of Problem 11.2.7 for the data set
in DS 11.2.2. If the experimenter wants conﬁdence
intervals for the pairwise differences of the factor level
means with an overall conﬁdence level of 95% that are
no longer than 4.0, how much additional sampling
would you recommend?
11.2.9 Calciner Comparisons
DS 11.2.3 gives a data set of brightness measurements
for b = 7 batches of kaolin processed through k = 3
calciners. The calciner operators are interested in
whether their calciners are operating at different
efﬁciencies. What would you advise them? Perform
hand calculations.
11.2.10 Radar Detection of Airborne Objects
A randomized block design is used to compare k = 3
radar systems for detecting airborne objects. A total of
b = 8 objects are ﬂown toward the radar stations and the
distances at detection by the radar systems are recorded
in DS 11.2.4. Perform hand calculations to investigate
whether there is evidence of any difference between the
radar systems.
11.2.11 Golf Club Comparisons
A total of b = 9 golfers are each asked to hit a golf ball
with each of k = 4 types of driver. DS 11.2.5 gives the
straight line distances traveled by the balls. Are some
drivers better than others? Perform hand calculations.
11.2.12 Production Line Assembly Methods
A manufacturing company wants to investigate which
of three possible assembly methods for an electric motor
is the most efﬁcient. Since there is a fair amount of
variability in the expertise of the workforce, it is decided
to take workers as a blocking variable in the experiment.
A random selection of b = 10 workers are trained in
each of the k = 3 assembly methods, and then after a
suitable amount of practice, they are timed under each
method. The resulting data set is given in DS 11.2.6.
Which assembly method is quickest? Use a computer
package to obtain a graphical presentation of the data set
and to help you with the analysis.
11.2.13 Realtor Commissions
A Realtor ofﬁce has ﬁve agents, and the manager wants
to determine who is the best agent. A data set, given in
DS 11.2.7, is collected that consists of the agents’
commissions in each month of the previous year. Since
house sales vary from one month to the next, the
manager decides that it is sensible to take the months as
blocking variables. Who is the best agent? Who is the
worst agent? Use a computer package to obtain a
graphical presentation of the data set and to help you
with the analysis.
11.2.14 Cleanliness Scores for Detergent Comparisons
A chemical company runs a randomized block design
to compare k = 4 different formulations of a detergent.
A set of b = 20 pieces of cloth are dirtied in various
ways and are then cut into four equal pieces and washed

11.3 CASE STUDY: MICROELECTRONIC SOLDER JOINTS
537
in each of the four detergents. These different cloths are
taken as the blocking variable. After washing, a grid is
place over the cloth and each cell in the grid is evaluated
on its cleanliness by a panel of judges. These evaluations
are averaged to produce the scores given in DS 11.2.8.
Higher scores represent cleaner cloths. Which detergent
formulation is best? Use a computer package to obtain a
graphical presentation of the data set and to help you
with the analysis.
11.2.15 Consider the following ANOVA table for a randomized
block design.
Source
df
SS
MS
F
p-value
Treatments
3
0.151
?
?
?
Blocks
?
?
0.054
?
?
Error
18
?
?
Total
?
0.644
(a) Find the missing values in the ANOVA table.
(b) Suppose that the treatment sample averages are
¯x1. = 0.810, ¯x2. = 0.630, ¯x3. = 0.797, ¯x4. = 0.789.
At a 95% conﬁdence level, is there sufﬁcient
evidence to conclude that treatment 2 has a smaller
mean value than the other treatments?
11.2.16 A randomized block design has k = 3 factor levels
and b = 4 blocks. The total sum of squares is SST =
203.565 and s = 1.445. The factor level sample
averages are ¯x1. = 107.68 ¯x2. = 109.86 and ¯x3. =
111.63. Compute the analysis of variance table.
11.2.17 Consider a randomized block design with b blocks to
compare k treatments. Suppose that each data value xi j
is replaced by axi j + c for some constants a > 0 and c.
For example, if temperature is the response variable, the
units might be changed from Fahrenheit to Centigrade.
Which values change in the ANOVA table, and how do
they change?
11.2.18 A randomized block design has four factor levels with
sample averages ¯x1. = 763.9, ¯x2. = 843.9, ¯x3. = 711.3,
and ¯x4. = 788.2. There are seven blocks and SSBl =
13492.3. Also, SSE = 7052.8. Compute the ANOVA
table. What do the p-values tell you? Construct a
graphical representation of the differences between the
factor means.
11.2.19 Groundwater Pollution Levels
An experiment was conducted to measure the pollution
level in the groundwater at four different locations.
The pollution levels were measured at each of the
locations at ﬁve different time points as given in
DS 11.2.9. Is there any evidence to support the claim
that the pollution levels are different at the four
locations? Make a graphical presentation that illustrates
the differences between the pollution levels at the four
locations.
11.3
Case Study: Microelectronic Solder Joints
The researcher is interested in comparing the gold layer thicknesses on the bond pads of
microelectronic assemblies manufactured by four different companies. A random selection of
ten bond pads is made from assemblies from each of the four companies, and the gold layer
thicknesses are measured. The resulting data set is given in Figure 11.39, and it is represented
by the boxplots in Figure 11.40.
In order to test whether there is a statistically signiﬁcant difference between the four
companies, the researcher conducts a one-way analysis of variance which is shown in Fig-
ure 11.41. The p-value is equal to P(F3,36 ≥16.70) that is much less than 1%, and so the
researcher can conclude that there is a difference between the companies in terms of the gold
layer thicknesses that they have on their substrate bond pads.
The sample averages are ¯xA = 0.00714, ¯xB = 0.00797, ¯xC = 0.00878, and ¯xD =
0.00795, and s =
√
MSE = 0.0005182 so that
s × q0.05,4,36
√n
= 0.0005182 × 3.81
√
10
= 0.000624

538
CHAPTER 11
THE ANALYSIS OF VARIANCE
FIGURE 11.39
Data set of gold layer thicknesses
on substrate bond pads (microns)
Company A
Company B
Company C
Company D
0.0071
0.0084
0.0088
0.0084
0.0078
0.0088
0.0099
0.0089
0.0068
0.0077
0.0092
0.0081
0.0078
0.0077
0.0085
0.0083
0.0067
0.0082
0.0083
0.0070
0.0069
0.0079
0.0083
0.0086
0.0073
0.0083
0.0092
0.0075
0.0070
0.0079
0.0078
0.0078
0.0070
0.0075
0.0089
0.0076
0.0070
0.0073
0.0089
0.0073
FIGURE 11.40
Comparison of gold layer
thicknesses for each company
Data
Company A
Company B
Company C
Company D
0.0100
0.0095
0.0090
0.0085
0.0080
0.0075
0.0070
FIGURE 11.41
Analysis of variance table for
assessing the difference in gold
layer thicknesses between the four
companies
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Companies
3
0.000013450
0.000004483
16.70
0.000
Error
36
0.000009666
0.000000268
Total
39
0.000023116
0.00714
Company
A
Company
D
Company
B
Company
C
0.00795
0.00797
0.00878
Thinnest
Thickest
Indistinguishable
FIGURE 11.42
Schematic presentation of the results of the comparison of the gold layer thicknesses on the substrate bond pads for the four
companies
The differences between the sample averages are all larger than 0.000624 except for the
comparison of companies B and D, and consequently the researcher can conclude that while
there is no evidence of a difference between companies B and D, the data provide sufﬁcient
evidence to conclude that company A has thinner gold layers on average than the other three
companies, while company C has thicker layers on average than the other three companies.
This result is illustrated in Figure 11.42.

11.4 CASE STUDY: INTERNET MARKETING
539
11.4
Case Study: Internet Marketing
One way that the organisation advertises is to have banner advertisements inserted onto a
particular web page. Figure 11.43 shows the results of a 3-week experiment where three
different designs of banner advertisment were tried out on different days. Each design was
tried for one complete day, with their appearances being randomized throughout the 3-week
period (rather than having design A throughout the ﬁrst week, design B throughout the second
week, and design C throughout the third week, in case there was a trend over time due to other
causes). The output variable is the number of clicks on the banner advertisement.
This is a randomized block design, and while the day of the week is signiﬁcant, with a
p-value of 0.004, the design of the banner advertisement is not signiﬁcant, with a p-value
of 0.575. Therefore, there is no evidence that the design of the banner advertisement has any
effect on whether it will be clicked or not. However, Figure 11.44 shows a plot of the mean
number of clicks for the days of the week, and it can be seen that the reason that this is a
signiﬁcant factor is that the number of clicks is highest on Monday and then decreases through
the week, with the lowest values being at the weekend.
FIGURE 11.43
Data set of the number of clicks on
a banner advertisement
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
Sunday
2945
2388
2445
1979
2839
1945
1526
3025
3047
2518
3398
2517
1628
1570
3402
3312
2615
2346
2013
1378
1679
Design of banner advertisement
A
B
C
FIGURE 11.44
Website clicks for different days
Sunday
Saturday
Friday
Thursday
Wednesday
Tuesday
Monday
Clicks
500
0
1000
1500
2000
2500
3000
3500

540
CHAPTER 11
THE ANALYSIS OF VARIANCE
11.5
Supplementary Problems
11.5.1 Biaxial Nanowire Tests
An experimenter measured the Young’s modulus
values of silicon carbide–silica biaxial nanowires.
These synthesized nanowires are potentially useful
for high-strength composites in which mechanical
properties are critical. DS 11.5.1 gives a completely
randomized design consisting of the Young’s modulus
measurements for samples of nanowires of four different
types. Perform hand calculations to investigate whether
there are any signiﬁcant differences in the average
Young’s modulus of the four different types of
nanowire.
11.5.2 Car Gas Efﬁciencies
A courier service buys four new cars of the same
make and model, and they are driven under similar
city and suburban conditions. Each time the cars are
ﬁlled with gasoline, the car’s mileage and amount of
gasoline required to top off the tank are noted. At the
end of a year, the company manager uses these data to
calculate the gas mileages achieved by the cars between
their ﬁllups. This data set is given in DS 11.5.2. Are any
of the cars getting a better gas mileage than the others?
Use a computer package to obtain a graphical
presentation of the data set, an analysis of variance
table, and pairwise comparisons between the different
cars.
11.5.3 Temperature Effect on Cement Curing
An experiment is conducted to assess the effect of
the temperature during curing of a certain kind of
cement. Six batches of the cement are prepared,
and each batch is separated into ﬁve parts that
are then cured at the ﬁve different temperature
levels under consideration. The strengths of
the cured cement samples are presented in
DS 11.5.3. Perform hand calculations to
investigate the effects of the different temperature
levels.
11.5.4 Fertilizer Comparisons
A comparison of k = 5 fertilizers is made in a
randomized block design that incorporates b = 10 ﬁelds
of corn. Each ﬁeld is partitioned into ﬁve equal areas,
and the ﬁve fertilizers are randomly allocated to these
areas. The resulting corn yields are given in DS 11.5.4.
Which fertilizer is best? Use a computer package to
obtain a graphical presentation of the data set and
to help you with the analysis.
11.5.5 Red Blood Cell Adherence to Endothelial Cells
A hospital has k = 4 clinics where it can send blood
samples for adhesion measurements of sickle red blood
cells to endothelium layers. A hospital administrator
decides to perform a randomized block experiment to
investigate the consistency between the four clinics. A
set of b = 12 blood samples is split into four parts,
which are randomly sent to the four clinics. The
numbers of adherent cells per square mm of endothelial
monolayer reported by the clinics are given in DS
11.5.5. Are any of the clinics suspiciously different from
the other clinics? Use a computer package to obtain a
graphical presentation of the data set and to help you
with the analysis.
11.5.6 Insertion Gains of Hearing Aids
The insertion gain of a hearing aid is deﬁned to be the
difference between the sound pressure level at the
eardrum with and without the hearing aid in place.
These sound pressure measurements can be made
by a probe microphone placed in the ear canal of a
subject. An experiment was conducted to investigate
how the insertion gain of a particular hearing aid was
affected by the elevation of the noise stimulus. Data
were collected on the insertion gain for a constant
noise stimulus placed at the horizontal level of the
ear of a subject, placed above the horizontal level, and
placed below the horizontal level. The data are listed in
DS 11.5.6. What conclusions can you draw from this
data set?
11.5.7 Air Resistance Drag for Road Vehicles
The reduction of drag from air resistance for large road
vehicles is an important component of attempts to
improve fuel efﬁciency. Wind tunnel tests were
performed on models of four different vehicle designs,
and the data set given in DS 11.5.7 was obtained. What
conclusions can you draw from this data set about the
drags of the four different designs?
11.5.8 Leather Shrinkage Measurements
When leather is used to produce furniture or clothes, its
shrinkage is an important consideration. An experiment
was conducted to discover the differences beween four

11.5 SUPPLEMENTARY PROBLEMS
541
different preparation methods in terms of the resulting
shrinkages of the leather. Seven batches of leather were
split into four parts each, which were then randomly
assigned to each of the four preparation methods.
The shrinkage measurements are given in DS 11.5.8.
What conclusions can you draw from this data set about
the differences between the four different preparation
methods?
11.5.9 Are the following statements true or false?
(a) A one-way layout with only k = 2 treatments is
equivalent to an independent samples design for
comparing two means.
(b) In an ANOVA table the sum of squares for
treatments can be negative if the only difference
between the treatment levels can be explained by
random variation.
(c) Large variability in the data within the treatments
of a one-way layout produces a large sum of
squares for error that makes it more difﬁcult to
identify a difference between the treatment
means.
(d) In a randomized block design the degrees of
freedom for error cannot be smaller than the degrees
of freedom for blocks.
(e) An analysis of variance table shows how the total
variability in a data set can be attributed to different
sources.
(f) The distinction between randomized block designs
and one-way layouts is analogous to the distinction
between a paired two-sample data set and an
unpaired two-sample data set.
(g) When a graphical representation is made of the
differences between the treatments in a one-way
layout, a line extending between two different
treatments indicates that it has been proved that the
two treatments are identical.
(h) A randomized block design has the advantage that it
is not necessary to assume that the differences
between the factor level effects are the same for
each of the blocks.
11.5.10 Metal Alloy Hardness Tests
The data set in DS 11.5.9 is a one-way layout to
compare the hardness measurements of three different
alloys.
(a) Construct the ANOVA table. What conclusion can
you draw from the ANOVA table about the
differences between the alloys?
(b) Construct pairwise comparisons of the alloys and
make a graphical representation of what you ﬁnd.
Which treatment mean is largest? Which treatment
mean is smallest?
11.5.11 A randomized block design has four factor levels with
sample averages ¯x1. = 11.43, ¯x2. = 12.03, ¯x3. = 14.88,
and ¯x4. = 11.76. There are nine blocks and
SSBl = 53.28. Also, SSE = 14.12. Compute the
ANOVA table. What do the p-values tell you? Construct
a graphical representation of the differences between the
factor means.
11.5.12 Aquatic Radon Levels
An investigation was conducted of the radon levels in
ﬁve different rivers. Six samples of water were obtained
from each river and the radon levels given in DS 11.5.10
were obtained. Construct an appropriate ANOVA table.
What does the p-value in the ANOVA table tell you?
Construct an appropriate graphical representation of the
differences between the rivers. Which river has the
highest radon level?
11.5.13 A completely randomized design to compare k = 4
factor levels has n1 = 11 and ¯x1· = 213.7, n2 = 8 and
¯x2· = 206.3, n3 = 13 and ¯x3· = 205.7, and n4 = 12 and
¯x4· = 215.8. The mean square error is MSE = 11.23.
Calculate pairwise conﬁdence intervals for the factor
level means with an overall conﬁdence level of 95%.
Make a diagram showing which factor level means
are known to be different and which ones are
indistinguishable.
11.5.14 Volatile Organic Carbon Emissions
Volatile organic carbon emissions are measured at
ﬁve locations of an industrial facility on each of
10 days, and the data is given in DS 11.5.11. Is there
any evidence of a difference in the emissions rates
at the ﬁve locations? Which location has the highest
emissions? Which location has the lowest
emissions?
11.5.15 If the p-value in the ANOVA table for a one-way
analysis is less than 1%, then:
A. Some of the typical follow-up conﬁdence intervals
(with conﬁdence level 95%) for the pairwise
comparisons will not contain 0.
B. It must be the case that none of the typical follow-up
conﬁdence intervals (with conﬁdence level 95%) for
the pairwise comparisons will contain 0.

542
CHAPTER 11
THE ANALYSIS OF VARIANCE
C. Neither of the above.
D. Both of the above.
11.5.16 Consider a one-way analysis of variance.
A. A p-value larger than 10% in the ANOVA table
implies that the differences between the sample
means are not statistically signiﬁcant.
B. A p-value smaller than 1% in the ANOVA table
implies that it has been established that the
treatments are not all the same.
C. Neither of the above.
D. Both of the above.

C H A P T E R T W E L V E
Simple Linear Regression and Correlation
Historically, the ideas of linear regression and correlation presented in this chapter have played
a prominent part in statistical data analysis. When dealing with more than one variable, an
experimenter is often interested in how a particular variable depends on one or more of the
other variables. When the variables have a random component there will not be a deterministic
relationship between them, but there may be an underlying structure that the experimenter can
investigate. This modeling is often performed by ﬁnding a functional relationship between
the expected value of a dependent variable and a set of explanatory or independent variables.
Linear regression is a modeling technique in which the expected value of a dependent
variable is modeled as a linear combination of a set of explanatory variables. These linear
models are easy to analyze and are applicable in many situations. Simple linear regression,
discussed in this chapter, refers to a linear regression model with only one explanatory variable,
while multiple linear regression, discussed in Chapter 13, refers to a linear regression model
with two or more explanatory variables. A simple linear regression model is closely related
to the calculation of a correlation coefﬁcient to measure the degree of association between
two variables, which is discussed at the end of this chapter.
12.1
The Simple Linear Regression Model
12.1.1
Model Deﬁnition and Assumptions
Consider a data set consisting of the paired observations
(x1, y1), . . . , (xn, yn)
For example, xi could represent the height and yi could represent the weight of the ith person
in a random sample of n adult males. Statistical modeling techniques can be used to investigate
how the two variables, corresponding to the data values xi and yi, are related. In order to do
this it is convenient to think of the data value yi as being the observed value of a random
variable Yi, whose distribution depends upon a (ﬁxed) value xi.
In particular, with the simple linear regression model
yi = β0 + β1xi + ϵi
the observed value of the dependent variable yi is composed of a linear function β0 + β1xi
of the explanatory variable xi, together with an error term ϵi. The error terms ϵ1, . . . , ϵn are
generally taken to be independent observations from a N(0, σ 2) distribution, for some error
variance σ 2. This implies that the values y1, . . . , yn are observations from the independent
random variables
Yi ∼N(β0 + β1xi, σ 2)
as illustrated in Figure 12.1.
543

544
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.1
Simple linear regression model
β0 + β1xn
β0 + β1x1
β0 + β1x2
x1
x2
xn
x
y
Distribution
of 
y1
Distribution
of 
y2
Distribution
of 
yn
Regression
 line 
y = β0 + β1x
...
...
Notice that
E(Yi) = β0 + β1xi
so that the expected value of the dependent variable is modeled as a linear function of the
values of the explanatory variable. Thus, for the example of peoples’ heights and weights,
people with a height x are modeled as having weights with an expected value of β0 + β1x.
In many cases the choice of which variable should be taken as the dependent variable y and
which variable should be taken as the explanatory variable x is arbitrary. In an investigation
of the relationship between peoples’ heights and weights, either height or weight can sensibly
be taken to be the dependent variable. However, if one variable can reasonably be thought of
as depending upon and resulting from the speciﬁcation of the other variable, then it is clearly
sensible to model the former variable as the dependent variable y and the latter variable as the
explanatory variable x.
The unknown parameters β0 and β1, which determine the relationship between the depen-
dent variable and the explanatory variable, can be estimated from the data set. This procedure
is referred to as “ﬁtting a straight line to the data set.” The parameter β0 is known as the in-
tercept parameter, and the parameter β1 is known as the slope parameter. A third unknown
parameter, the error variance σ 2, can also be estimated from the data set. As illustrated in
Figure 12.2, the data values (xi, yi) lie closer to the line
y = β0 + β1x
as the error variance σ 2 decreases.
The slope parameter β1 is of particular interest since it indicates how the expected value
of the dependent variable depends upon the explanatory variable x, as shown in Figure 12.3.
If β1 = 0 so that the line is ﬂat, then changes in the explanatory variable x do not affect
the distribution of the dependent variable. The two variables can then be interpreted as being
unrelated. If β1 > 0 so that the line slopes upward, then the dependent variable tends to

12.1 THE SIMPLE LINEAR REGRESSION MODEL
545
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Error variance σ 2
1
Regression
 line
y = β0 + β1x
σ 2
1 > σ 2
2
x
y
+
+
+
+
+
+
+
+ + +
++
+
+
+
+ +
Error variance σ 2
2
Regression
 line
y = β0 + β1x
x
y
+
+
+ +
+
+
+
+
+
+
+ +
+
+
+
+
FIGURE 12.2
Interpretation of the error variance σ 2
x
y
β1 = 0
β1 < 0
β1 > 0
x
y
x
y
FIGURE 12.3
Interpretation of slope parameter β1
FIGURE 12.4
For this nonlinear relationship a
simple linear regression model is
not appropriate
x
y
+ + +
+
+ +
++ ++ +
+ +
+ +
+
+++ +
+ +
+
increase in value as the explanatory variable x increases, and if β1 < 0 so that the line slopes
downward, then the dependent variable tends to decrease in value as the explanatory variable
x increases. Clearly for the example of peoples’ heights and weights, a positive value of β1
would be expected.
It should always be remembered that it is ridiculous to ﬁt a straight line to a data set that
clearly does not exhibit a linear relationship. In other words, an experimenter should always
look at a graph of a data set before ﬁtting a line in order to assess whether this is a sensible thing
to do. For example, the data set shown in Figure 12.4 clearly exhibits a quadratic (or at least

546
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
nonlinear) relationship between the two variables, and it would make no sense to ﬁt a straight
line to the data set. Multiple linear regression methods for ﬁtting quadratic and other more
complicated models with more than one explanatory variable are discussed in Chapter 13, and
variable transformations are discussed in Section 12.8.
Simple Linear Regression Model
The simple linear regression model
yi = β0 + β1xi + ϵi
ﬁts a straight line through a set of paired data observations (x1, y1), . . . , (xn, yn). The
error terms ϵ1, . . . , ϵn are taken to be independent observations from a N(0, σ 2)
distribution. The three unknown parameters, the intercept parameter β0, the slope
parameter β1, and the error variance σ 2, are estimated from the data set.
12.1.2
Examples
Example 67
Car Plant Electricity
Usage
The manager of a car plant wishes to investigate how the plant’s electricity usage depends
upon the plant’s production. The data set shown in Figure 12.5 is compiled and provides the
plant’s production and electrical usage for each month of the previous year. The electrical
usage is in units of a million kilowatt-hours, and the production is measured as the value in
million-dollar units of the cars produced in that month.
Figure 12.6 shows a graph of the data set, and the manager concludes that it is sensible to
ﬁt a straight line to the data points. The electricity usage is taken to be the dependent variable
y, and the production is taken to be the explanatory variable x. The linear model
y = β0 + β1x
Production 
($ million)
Electricity usage 
(million kWh)
January
4.51
2.48
February
3.58
2.26
March
4.31
2.47
April
5.06
2.77
May
5.64
2.99
June
4.99
3.05
July
5.29
3.18
August
5.83
3.46
September
4.70
3.03
October
5.61
3.26
November
4.90
2.67
December
4.20
2.53
FIGURE 12.5
Car plant electricity usage data set
Production
Electricity 
usage
6.0
5.5
5.0
4.5
4.0
3.5
3.6
3.4
3.2
3.0
2.8
2.6
2.4
2.2
FIGURE 12.6
Scatter plot of the car plant electricity usage data set

12.1 THE SIMPLE LINEAR REGRESSION MODEL
547
will then allow a month’s electrical usage to be estimated as a function of the month’s
production. The manager can estimate a month’s electricity expenses once the month’s
production targets have been decided. Notice that for this example the slope parameter β1 rep-
resents the expected increase in electricity usage resulting from an increase in production of
$1 million.
Example 68
Nile River Flowrate
Figure 12.7 shows a map of the Nile River ﬂowing from Sudan into Egypt and eventually into
the Mediterranean Sea. The Aswan High Dam, situated at the northern end of Lake Nasser,
controls the ﬂow of water into the fertile regions bordering the Nile River in Egypt and to
important metropolitan areas such as Cairo. The proper control of the water passing through
the dam is critical to the well-being of agriculture, industry, and population centers.
The amount of water that the dam engineers can prudently allow to ﬂow through the dam
depends upon the amount of water reserves in Lake Nasser, which in turn depends upon the
ﬂow of water into the lake at Wadi Halfa. In this respect it is very useful to be able to predict
the future Nile River ﬂowrates at Wadi Halfa. Egyptian engineers have been measuring this
inﬂow for over 100 years, and Figure 12.8 shows the inﬂows at Wadi Halfa for the months of
January and February for the years 1874 to 1988. These inﬂows are deﬁned to be the amount
FIGURE 12.7
The Nile River basin
Mediterranean Sea
Egypt
Sudan
White
Nile
Blue
Nile
Main
Nile
Lake
Nasser
Aswan High
Dam
Wadi Halfa
Cairo
N
Red
Sea

548
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.8
Nile River inﬂows in billions of
cubic meters
Inﬂow
Year
January
February
1874
6.41
4.48
1875
4.18
2.43
1876
5.43
4.18
1877
3.38
1.89
1878
5.47
3.81
1879
5.24
4.19
1880
4.57
2.90
1881
3.46
2.13
1882
7.07
5.36
1883
6.48
5.25
1884
4.45
3.01
1885
4.55
3.06
1886
4.66
3.46
1887
5.16
4.09
1888
4.81
3.33
1889
3.77
2.33
1890
4.64
3.04
1891
5.12
3.18
1892
2.86
1.70
1893
4.09
2.41
1894
5.24
3.19
1895
4.84
3.17
1896
6.14
4.80
1897
5.01
3.29
1898
6.25
4.67
1899
5.93
4.48
1900
6.44
4.54
1901
4.34
3.10
1902
5.58
4.31
1903
1.94
1.23
1904
3.84
2.39
1905
3.38
1.96
1906
3.62
2.24
1907
4.50
3.43
1908
3.85
2.50
1909
3.69
2.68
1910
4.29
2.89
1911
3.60
2.25
1912
4.53
3.43
Inﬂow
Year
January
February
1913
5.00
3.89
1914
4.34
3.20
1915
2.76
2.21
1916
2.89
1.91
1917
1.64
1.23
1918
3.98
2.74
1919
3.49
2.30
1920
4.74
3.79
1921
4.80
3.92
1922
3.28
2.42
1923
2.88
1.95
1924
3.28
2.16
1925
2.91
1.85
1926
3.14
2.01
1927
3.57
2.32
1928
3.25
2.14
1929
2.97
1.92
1930
3.60
2.50
1931
2.41
1.54
1932
3.28
2.18
1933
3.66
2.49
1934
2.46
1.67
1935
3.05
1.97
1936
3.84
3.14
1937
3.89
2.56
1938
3.77
2.62
1939
3.46
2.52
1940
3.11
2.15
1941
3.36
2.15
1942
4.02
3.28
1943
3.13
1.93
1944
2.24
1.53
1945
3.33
2.13
1946
3.15
1.89
1947
2.80
1.81
1948
2.65
1.68
1949
3.82
2.53
1950
4.33
3.67
Inﬂow
Year
January
February
1951
3.71
2.77
1952
3.66
2.89
1953
3.94
3.00
1954
3.63
2.28
1955
3.23
1.91
1956
2.76
1.73
1957
3.12
1.75
1958
4.11
2.90
1959
3.59
2.89
1960
4.02
3.44
1961
2.63
1.80
1962
3.58
2.36
1963
3.68
2.64
1964
3.06
1.84
1965
4.34
3.37
1966
4.03
3.22
1967
5.13
3.96
1968
6.13
5.67
1969
4.58
3.61
1970
4.61
3.76
1971
4.94
4.28
1972
3.91
3.13
1973
4.03
3.19
1974
4.64
3.62
1975
4.45
3.73
1976
2.89
2.45
1977
3.57
2.57
1978
4.18
3.17
1979
4.71
3.93
1980
4.12
2.93
1981
5.76
3.53
1982
4.71
3.55
1983
3.63
2.82
1984
3.74
2.78
1985
3.94
2.85
1986
3.47
2.91
1987
3.88
4.01
1988
3.70
3.12
of water in billions of cubic meters that ﬂows through a cross section of the Nile River at Wadi
Halfa during the month under consideration.
Figure 12.9 shows summary statistics of the monthly inﬂows, and Figure 12.10 shows a
scatter plot of this data set. The January inﬂows are seen to be generally slightly larger than the
February inﬂows, and there does appear to be a fairly linear relationship between the inﬂows
in the two months. It is sensible to ﬁt the line
y = β0 + β1x
where the explanatory variable x is the January inﬂow and the dependent variable y is the
February inﬂow. This model is useful since at the end of January, when the January inﬂow

12.1 THE SIMPLE LINEAR REGRESSION MODEL
549
FIGURE 12.9
Summary statistics for the Nile
River inﬂows
January inﬂow
February inﬂow
n = 115
Sample mean = 4.0252
Sample standard deviation = 1.0228
Maximum = 7.07
Upper quartile = 4.64
Median = 3.85
Lower quartile = 3.33
Minimum = 1.64
n = 115
Sample mean = 2.8960
Sample standard deviation = 0.9163
Maximum = 5.67
Upper quartile = 3.44
Median = 2.85
Lower quartile = 2.16
Minimum = 1.23
FIGURE 12.10
Scatter plot of the Nile River
inﬂows data set
January inflow
February inflow
7
6
5
4
3
2
1
6
5
4
3
2
1
has been measured, the dam engineers can use it to estimate the inﬂow for the month ahead.
The slope parameter β1 indicates the expected increase in the February inﬂow for each unit
increase in the January inﬂow.
Example 44
Army Physical Fitness
Test
Recall that the Army Physical Fitness Test, discussed in Chapter 6, consists of 2 minutes of
pushups followed by 2 minutes of situps and a 2-mile run. A data set of 84 run times for male
Army ofﬁcers was discussed in Chapter 6. Figure 12.11 shows this data set together with the
number of pushups performed by the ofﬁcers before their 2-mile run.
Figure 12.12 shows summary statistics of the number of pushups and the run times, and
Figure 12.13 shows a scatter plot of the run times against the number of pushups. As might be
expected, ofﬁcers performing more pushups tend to have shorter run times, indicating a better
all-around athletic ability. With the run time as the dependent variable y and the number of
pushups as the explanatory variable x, the linear model
y = β0 + β1x
allows run times to be predicted as a function of the number of pushups. This model is useful
for medical researchers to help investigate how well people can perform different kinds of
athletic tasks, and it can be used to identify ofﬁcers whose performance on the test is in some
way different from normal.

FIGURE 12.11
Army Physical Fitness Test data set
Number of
pushups
Two-mile run
time (seconds)
50
880
70
905
69
895
125
720
80
712
99
703
80
741
61
792
50
808
55
761
60
785
60
801
65
810
66
1013
58
882
62
861
60
845
90
865
78
883
86
881
86
921
69
816
74
837
40
1056
40
1034
78
774
70
821
78
850
Number of
pushups
Two-mile run
time (seconds)
70
870
55
931
55
930
80
808
78
828
78
719
68
707
45
934
45
939
47
977
88
896
70
921
80
815
79
838
71
854
50
1063
50
1024
73
780
78
813
78
850
63
902
60
906
78
865
78
886
78
881
50
825
61
821
62
832
Number of
pushups
Two-mile run
time (seconds)
60
847
53
887
60
879
55
919
60
816
78
814
74
814
70
855
46
980
50
954
50
1078
59
1001
62
766
64
916
51
798
66
782
73
836
78
837
80
791
70
838
70
853
79
840
93
740
80
763
78
778
70
855
60
875
78
868
FIGURE 12.12
Summary statistics for the Army
Physical Fitness Test
Pushups
n = 84
Sample mean = 67.79
Sample standard deviation = 14.31
Maximum = 125
Upper quartile = 78
Median = 69.5
Lower quartile = 59.25
Minimum = 40
Run time
n = 84
Sample mean = 857.7
Sample standard deviation = 81.98
Maximum = 1078
Upper quartile = 900.5
Median = 850
Lower quartile = 808.5
Minimum = 703
FIGURE 12.13
Scatter plot of the Army Physical
Fitness Test data set
Number of pushups
2-mile run time
130
120
110
100
90
80
70
60
50
40
1100
1000
900
800
700
550

12.2 FITTING THE REGRESSION LINE
551
12.1.3
Problems
12.1.1 Suppose that a dependent variable y is related to an ex-
planatory variable x through a linear regression model with
β0 = 4.2, β1 = 1.7, and with normally distributed error
termswithazeromeanandastandarddeviationofσ = 3.2.
(a) What is the expected value of the dependent variable
when x = 10.0?
(b) How much does the expected value of the dependent
variable change when the explanatory variable
increases by 3?
(c) What is the probability that the dependent variable is
larger than 12.0 when x = 5?
(d) What is the probability that the dependent variable is
smaller than 17.0 when x = 8?
(e) What is the probability that a dependent variable
obtained at x = 6 is larger than a dependent variable
obtained at x = 7?
12.1.2 Suppose that the purity of a chemical solution y is related
to the amount of a catalyst x through a linear regression
model with β0 = 123.0, β1 = −2.16, and with an error
standard deviation σ = 4.1.
(a) What is the expected value of the purity when the
catalyst level is 20?
(b) How much does the expected value of the purity
change when the catalyst level increases by 10?
(c) What is the probability that the purity is less than 60.0
when the catalyst level is 25?
(d) What is the probability that the purity is between 30
and 40 when the catalyst level is 40?
(e) What is the probability that the purity of a solution
with a catalyst level of 30 is smaller than the purity of
a solution with a catalyst level of 27.5?
12.1.3 Consider the linear regression model y = 5 + 0.9x, with
σ = 1.4, which relates the air content of a concrete
sample x to the porosity of the sample y.
(a) What is the expected value of the porosity for a
concrete sample with an air content of 20?
(b) How much does the expected value of the porosity
decrease as the air content is reduced by 5?
(c) What is the probability that a concrete sample with an
air content of 25 has a porosity no larger than 30?
(d) What is the probability that four independent samples
of concrete each with an air content of 15 have an
average porosity between 17 and 20?
12.1.4 Suppose that you wish to use simple linear regression to
investigate the relationship between two variables. Why
does the model you obtain depend on which variable is
chosen as the y variable? How should you decide which
of your two variables should be the y variable?
12.1.5 Consider the linear regression model
y = 675.30 −5.87x
with σ = 7.32, which relates the viscosity y of a
substance to the temperature x. If a sample of the
substance is prepared at temperature 80, what is the
probability that its viscosity is less than 220?
12.1.6 In simple linear regression, the dependent variable is the:
A. Input variable
B. x variable
C. y variable
D. Coefﬁcient of determination
12.1.7 In simple linear regression:
A. The input variable x and the output variable y may be
either continuous or discrete variables.
B. The input variable x and the output variable y are
both considered as continuous variables.
C. Neither of the above.
D. Both of the above.
12.2
Fitting the Regression Line
12.2.1
Parameter Estimation
The regression line
y = β0 + β1x
is ﬁtted to the data points
(x1, y1), . . . , (xn, yn)

552
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.14
The least squares ﬁt
+
+
+
+
+
+
+
+
+
yi − (β0 + β1xi)
(xi, yi)
β0 + β1xi
x
y
y = β0 + β1x
The fitted line minimizes the sum of squared vertical deviations.
by ﬁnding the line that is “closest” to the data points in some sense. There are many ways in
which closeness can be deﬁned, but the method most generally used is to consider the vertical
deviations between the line and the data points
yi −(β0 + β1xi)
1 ≤i ≤n
As Figure 12.14 illustrates, the ﬁtted line is chosen to be the line that minimizes the sum of
the squares of these vertical deviations
Q =
n

i=1
(yi −(β0 + β1xi))2
and this is referred to as the least squares ﬁt.
Although this deﬁnition of closeness is in a sense arbitrary, with normally distributed error
terms it results in parameter estimates ˆβ0 and ˆβ1 that are maximum likelihood estimates. This
is because an error term ϵi has a probability density function
1
√
2πσ
e−ϵ2
i /2σ 2
so that the joint density of the error terms ϵ1, . . . , ϵn is

1
√
2πσ
n
e−n
i=1 ϵ2
i /2σ 2
This likelihood is maximized by minimizing
n

i=1
ϵ2
i
which is equal to Q because
ϵi = yi −(β0 + β1xi)
The parameter estimates ˆβ0 and ˆβ1 are therefore the values that minimize the quantity Q.
They are easily found by taking partial derivatives of Q with respect to β0 and β1 and setting

12.2 FITTING THE REGRESSION LINE
553
the resulting expressions equal to 0. Since
∂Q
∂β0
= −
n

i=1
2(yi −(β0 + β1xi))
and
∂Q
∂β1
= −
n

i=1
2xi(yi −(β0 + β1xi))
the parameter estimates ˆβ0 and ˆβ1 are thus the solutions to the equations
n

i=1
yi = nβ0 + β1
n

i=1
xi
and
n

i=1
xi yi = β0
n

i=1
xi + β1
n

i=1
x2
i
These equations are known as the normal equations.
The normal equations can be solved to give
ˆβ1 = n n
i=1 xi yi −
 n
i=1 xi
 n
i=1 yi

n n
i=1 x2
i −
 n
i=1 xi
2
and then ˆβ0 can be calculated as
ˆβ0 =
n
i=1 yi
n
−ˆβ1
n
i=1 xi
n
= ¯y −ˆβ1 ¯x
Notice that with the notation
SX X =
n

i=1
(xi −¯x)2 =
n

i=1
x2
i −n ¯x2 =
n

i=1
x2
i −
 n
i=1 xi
2
n
and
SXY =
n

i=1
(xi −¯x)(yi −¯y) =
n

i=1
xi yi −n ¯x ¯y =
n

i=1
xi yi −
 n
i=1 xi
 n
i=1 yi

n
the parameter estimate ˆβ1 can be written as
ˆβ1 = SXY
SX X
The ﬁtted regression line is
y = ˆβ0 + ˆβ1x
For a speciﬁc value of the explanatory variable x∗, this equation provides a ﬁtted value
ˆy|x∗= ˆβ0 + ˆβ1x∗
for the dependent variable y, as illustrated in Figure 12.15. For data points xi, the ﬁtted values
are
ˆyi = ˆβ0 + ˆβ1xi

554
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.15
Fitted values from a regression line
+
+
(xi, yi)
x
y
Fitted
regression
line
+
+
+
+
+
+
+
+
+
ˆy|x∗
ˆyi
xi
x∗
y = ˆβ0 + ˆβ1x
+
The error variance σ 2 can be estimated by considering the deviations between the observed
data values yi and their ﬁtted values ˆyi. Speciﬁcally, the sum of squares for error SSE is deﬁned
to be the sum of the squares of these deviations
SSE =
n

i=1
(yi −ˆyi)2 =
n

i=1
(yi −(ˆβ0 + ˆβ1xi))2
and the estimate of the error variance is
ˆσ 2 = SSE
n −2 =
n
i=1(yi −(ˆβ0 + ˆβ1xi))2
n −2
Expandingthesquareintheexpressionforthesumofsquaresforerrorresultsintheexpressions
SSE =
n

i=1
y2
i −ˆβ0
n

i=1
yi −ˆβ1
n

i=1
xi yi
and consequently
ˆσ 2 =
n
i=1 y2
i −ˆβ0
n
i=1 yi −ˆβ1
n
i=1 xi yi
n −2
which is a more convenient representation for computation purposes.
It is interesting to notice that a regression line can be ﬁtted to the data points (xi, yi) and
the error variance can be estimated using only the six quantities
n
n

i=1
xi
n

i=1
yi
n

i=1
x2
i
n

i=1
y2
i
n

i=1
xi yi

12.2 FITTING THE REGRESSION LINE
555
Parameter Estimates
The point estimates of the three unknown parameters in a simple linear regression
model are
ˆβ1 = n n
i=1 xi yi −
n
i=1 xi
 n
i=1 yi

n n
i=1 x2
i −
n
i=1 xi
2
=
n
i=1(xi −¯x)(yi −¯y)
n
i=1(xi −¯x)2
= SXY
SX X
ˆβ0 = ¯y −ˆβ1 ¯x
ˆσ 2 = SSE
n −2 =
n
i=1(yi −(ˆβ0 + ˆβ1xi))2
n −2
=
n
i=1 y2
i −ˆβ0
n
i=1 yi −ˆβ1
n
i=1 xi yi
n −2
12.2.2
Examples
Example 67
Car Plant Electricity
Usage
For this example n = 12 and
12

i=1
xi = 4.51 + · · · + 4.20 = 58.62
12

i=1
yi = 2.48 + · · · + 2.53 = 34.15
12

i=1
x2
i = 4.512 + · · · + 4.202 = 291.2310
12

i=1
y2
i = 2.482 + · · · + 2.532 = 98.6967
12

i=1
xi yi = (4.51 × 2.48) + · · · + (4.20 × 2.53) = 169.2532
The estimate of the slope parameter is therefore
ˆβ1 = n n
i=1 xi yi −
n
i=1 xi
 n
i=1 yi

n n
i=1 x2
i −
n
i=1 xi
2
= (12 × 169.2532) −(58.62 × 34.15)
(12 × 291.2310) −58.622
= 0.49883
and the estimate of the intercept parameter is
ˆβ0 = ¯y −ˆβ1 ¯x = 34.15
12
−

0.49883 × 58.62
12

= 0.4090
The ﬁtted regression line is thus
y = ˆβ0 + ˆβ1x = 0.409 + 0.499x
which is shown together with the data points in Figure 12.16. If a production level of $5.5 mil-
lion worth of cars is planned for next month, then the plant manager can predict that the

556
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.16
Fitted line plot for the car plant
electricity usage data set
Production
Electricity usage
6.0
5.5
5.0
4.5
4.0
3.5
3.6
3.4
3.2
3.0
2.8
2.6
2.4
2.2
electricity usage will be
ˆy|5.5 = 0.409 + (0.499 × 5.5) = 3.1535
With ˆβ1 = 0.499, this model predicts that the electricity usage increases by about half a
million kilowatt-hours for every additional $1 million of production.
It must be remembered that it is best to use this regression model only for production
values x that are close to the values xi in the data set, that is, for production levels between
about $3.5 million and $6 million. Using the model for production values x outside this
range is known as extrapolation and may give inaccurate results. For example, suppose that
production is going to increase to $8 million next month. Is the estimate of electrical usage
ˆy|8.0 = 0.409 + (0.499 × 8.0) = 4.401
a good estimate? It may be inaccurate. Although it seems reasonable (based upon the data set
available) to model electricity usage as a linear function of production for production values
between x = 3.5 and x = 6.0, this linear relationship may not hold for production values
greater than x = 6.0. Figure 12.17 illustrates the danger of extrapolation.
For this problem the error variance is estimated as
ˆσ 2 =
n
i=1 y2
i −ˆβ0
n
i=1 yi −ˆβ1
n
i=1 xi yi
n −2
= 98.6967 −(0.4090 × 34.15) −(0.49883 × 169.2532)
10
= 0.0299
so that
ˆσ =
√
0.0299 = 0.1729
Example 68
Nile River Flowrate
The statistics for this example are n = 115 and
115

i=1
xi = 6.41 + · · · + 3.70 = 462.90
115

i=1
yi = 4.48 + · · · + 3.12 = 333.04

12.2 FITTING THE REGRESSION LINE
557
FIGURE 12.17
Extrapolation dangers
x
y
Fitted
regression
line
+
+
+
+
+
++
+
+
+
+
+
Range of
data set
Extrapolation
region
Extrapolation
region
+
+ +
+
+
+
+
+
+ +
Model
unreliable
Model
unreliable
y = ˆβ0 + ˆβ1x
+
+ +
+
+
115

i=1
x2
i = 6.412 + · · · + 3.702 = 1982.5264
115

i=1
y2
i = 4.482 + · · · + 3.122 = 1060.2076
115

i=1
xi yi = (6.41 × 4.48) + · · · + (3.70 × 3.12) = 1440.2743
The estimate of the slope parameter is
ˆβ1 = (115 × 1440.2743) −(462.90 × 333.04)
(115 × 1982.5264) −462.902
= 0.8362
and the estimate of the intercept parameter is
ˆβ0 = 333.04
115
−

0.8362 × 462.90
115

= −0.4698
The ﬁtted regression line is therefore
y = ˆβ0 + ˆβ1x = −0.470 + 0.836x

558
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.18
Fitted line plot for the Nile River
inﬂows data set.
January inflow
February inflow
7
6
5
4
3
2
1
6
5
4
3
2
1
which is shown together with the data points in Figure 12.18. If the inﬂow in January is
recorded as being 5 billion cubic meters, then the inﬂow for February can be estimated to be
ˆy|5 = −0.470 + (0.836 × 5) = 3.71
Each additional billion cubic meters of inﬂow in January results in an increase in the predicted
February inﬂow of ˆβ1 = 0.836 billion cubic meters. Finally, the error variance is estimated as
ˆσ 2 = 1060.2076 −(−0.4698 × 333.04) −(0.8362 × 1440.2743)
113
= 0.1092
Example 44
Army Physical
Fitness Test
In this case n = 84 and
84

i=1
xi = 60 + · · · + 62 = 5694
84

i=1
yi = 847 + · · · + 832 = 72,047
84

i=1
x2
i = 602 + · · · + 622 = 402,972
84

i=1
y2
i = 8472 + · · · + 8322 = 62,352,661
84

i=1
xi yi = (60 × 847) + · · · + (62 × 832) = 4,828,432
The estimate of the slope parameter is
ˆβ1 = (84 × 4,828,432) −(5694 × 72,047)
(84 × 402,972) −56942
= −3.2544

12.2 FITTING THE REGRESSION LINE
559
FIGURE 12.19
Fitted line plot for the Army
Physical Fitness Test data set
Number of pushups
2-mile run time
130
120
110
100
90
80
70
60
50
40
1100
1000
900
800
700
and the estimate of the intercept parameter is
ˆβ0 = 72,047
84
−

−3.2544 × 5694
84

= 1078.30
The ﬁtted regression line is therefore
y = ˆβ0 + ˆβ1x = 1078 −3.254x
which is shown together with the data points in Figure 12.19. An ofﬁcer who performs x = 70
pushups has an expected run time of
ˆy|70 = 1078 −(3.254 × 70) = 850.22
seconds, which is 14 minutes and 10 seconds. The slope parameter ˆβ1 = −3.254 indicates
that the expected run time decreases by 3.254 seconds for each additional pushup performed.
The error variance is estimated as
ˆσ 2 = 62,352,661 −(1078.30 × 72,047) −(−3.2544 × 4,828,432)
82
= 4606.42
and
ˆσ =
√
4606.42 = 67.87
In summary, it can be seen that ﬁtting straight lines to data sets is quite easy and has
many uses. However, two prevalent abuses of this simple modeling technique should never
be forgotten. These abuses are ﬁrstly, extrapolation, which was discussed in the example of
electricity usage, and secondly, modeling nonlinear relationships with a straight line. With
regard to the second abuse, although it is always possible to ﬁt a straight line to a data set,
this does not imply that it is always appropriate to ﬁt a straight line to a data set. Figure 12.20
shows a straight line ﬁtted to a data set exhibiting a nonlinear relationship, and the resulting
linear model is clearly misleading and inaccurate.

560
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.20
Inappropriate straight line ﬁtted to
a nonlinear relationship
x
y
+ + +
+
+ +
+ ++ +
+ +
+ +
+
+++
+ +
+ ++ ++ + +
+
+
12.2.3
Problems
12.2.1 Show that the values
ˆβ1 = SXY
SX X
and
ˆβ0 = ¯y −ˆβ1 ¯x
satisfy the normal equations.
12.2.2 A data set has n = 20, 20
i=1 xi = 8.552, 20
i=1 yi =
398.2, 20
i=1 x2
i = 5.196, 20
i=1 y2
i = 9356, and
20
i=1 xi yi = 216.6. Calculate ˆβ0, ˆβ1, and ˆσ 2. What is the
ﬁtted value when x = 0.5?
12.2.3 A data set has n = 30, 30
i=1 xi = −67.11,
30
i=1 yi = 1322.7, 30
i=1 x2
i = 582.0, 30
i=1 y2
i =
60,600, and 30
i=1 xi yi = −3840. Calculate ˆβ0, ˆβ1, and
ˆσ 2. What is the ﬁtted value when x = −2.0?
12.2.4 Oil Well Drilling Costs
Estimating the costs of drilling oil wells is an important
consideration for the oil industry. DS 12.2.1 contains the
total costs and the depths of 16 offshore oil wells located
in the Philippines (taken from “Identifying the major
determinants of exploration drilling costs: A ﬁrst
approximation using the Philippine case” by Gary S.
Makasiar, Energy Exploration and Exploitation, 1985).
(a) Fit a linear regression model with cost as the dependent
variable and depth as the explanatory variable.
(b) What does your model predict as the cost increase for
an additional depth of 1000 feet?
(c) What cost would you predict for an oil well of 10,000
feet depth?
(d) What is the estimate of the error variance?
(e) What could you say about the cost of an oil well of
depth 20,000 feet?
(This problem is continued in Problems 12.3.3, 12.4.4,
12.5.3, 12.6.5, 12.7.1, and 12.9.3.)
12.2.5 Truck Unloading Times
A warehouse manager is interested in the possible
improvements to labor efﬁciency if air-conditioning is
installed in the warehouse. The data set given in DS
12.2.2 is collected, which shows the times taken to unload
a fully laden truck at various temperature levels.
(a) Fit a linear regression model with time as the dependent
variable and temperature as the explanatory variable.
(b) What is the estimate of the error variance?
(c) Does the model suggest that the unloading time
increases with temperature?
(d) What is the predicted unloading time when the
temperature is 72◦F?
(This problem is continued in Problems 12.3.4, 12.6.6,
12.7.2, and 12.9.4.)
12.2.6 VO2-max Aerobic Fitness Measurements
DS 12.2.3 is a data set concerning the aerobic ﬁtness of a
sample of 20 male subjects. An exercising individual
breathes through an apparatus that measures the amount
of oxygen in the inhaled air that is used by the individual.
The maximum value per unit time of the utilized oxygen
is then scaled by the person’s body weight to come up
with a variable VO2-max, which is a general indication of
the aerobic ﬁtness of the individual.
(a) Fit a linear regression model with VO2-max as the
dependent variable and age as the explanatory variable.
(b) Does the sign of ˆβ1 surprise you? What does your
model predict as the change in VO2-max for an
additional ﬁve years of age?
(c) What VO2-max value would you predict for a
50-year-old man?
(d) What does the model tell you about the aerobic ﬁtness
of a 15-year-old boy?
(e) What is the estimate of the error variance?

12.3 INFERENCES ON THE SLOPE PARAMETER β1
561
(This problem is continued in Problems 12.3.5, 12.4.5,
12.5.4, 12.6.7, 12.7.3, and 12.9.5.)
12.2.7 Property Tax Appraisals
A Realtor collects the data set given in DS 12.2.4
concerning the sizes of a random selection of newly
constructed houses in a certain area together with their
appraised values for tax purposes.
(a) Fit a linear regression model with appraised value as
the dependent variable and size as the explanatory
variable.
(b) What is the predicted value of a house with 2600
square feet?
(c) How does the predicted value change as the size
increases by 100 square feet?
(d) What is the estimate of the error variance?
(This problem is continued in Problems 12.3.6, 12.4.6,
12.5.5, 12.6.8, 12.7.4, and 12.9.6.)
12.2.8 Management of Computer Systems
During the installation of a large computer system, it is
useful to know how long speciﬁc tasks will take,
particularly programming changes. A great deal of effort
is spent estimating the amount of time such tasks will
take and learning how to effectively use such estimations.
Having an accurate idea of the time required for these
tasks is crucial for the effective planning and timely
completion of the installation. The data set in DS 12.2.5
relates one expert’s time estimates for programming
changes to the actual time the tasks took.
(a) Fit a linear regression model with actual time as the
dependent variable and estimated time as the
explanatory variable.
(b) What is the predicted increase in the actual time when
the estimated time increases by 1 hour? Is the expert
underestimating or overestimating the times? If the
estimated time is 7 hours, what is the predicted value
of the actual time the task will take?
(c) What does your model tell you about the time it will
take to perform a task that is estimated at 15 hours?
(d) What is the estimate of the error variance?
(This problem is continued in Problems 12.3.7, 12.4.7,
12.5.6, 12.6.9, 12.7.5, and 12.9.7.)
12.2.9 Vacuum Transducer Bobbin Resistances
The data set in DS 12.2.6 concerns the relationship be-
tween the temperature and resistance of vacuum transducer
bobbins, which are used in the automobile industry.
(a) Fit a linear regression model with resistance as the
dependent variable and temperature as the
explanatory variable.
(b) What is the predicted resistance when the temperature
is 69◦F?
(c) How does the predicted resistance change as the
temperature increases by 5 degrees?
(d) What is the estimate of the error variance?
(This problem is continued in Problems 12.3.8, 12.4.8,
12.5.7, 12.6.10, 12.7.6, and 12.9.8.)
12.2.10 In simple linear regression:
A. The standard least squares ﬁt ﬁnds the line with the
smallest sum of vertical deviations from the data
points.
B. The standard least squares ﬁt ﬁnds the line with the
smallest sum of squared vertical deviations from the
data points.
C. Neither of the above.
D. Both of the above.
12.2.11 In simple linear regression:
A. Extrapolation should never be done.
B. Extrapolation is not a problem if the ﬁtted regression
line is based on a very large number of points.
C. Neither of the above.
D. Both of the above.
12.3
Inferences on the Slope Parameter β1
12.3.1
Inference Procedures
The slope parameter β1 is of particular interest to an experimenter since it determines the
nature of the relationship between the explanatory variable x and the dependent variable y.
It is useful to be able to construct conﬁdence intervals for the slope parameter and to test
hypotheses that the slope parameter takes a certain value. Remember that β1 is unknown and
represents the slope of the true unknown regression line, whereas ˆβ1 is the estimate of the
slope obtained by ﬁtting a line to the data set.

562
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
The point estimate of the slope parameter is
ˆβ1 = SXY
SX X
Since
SXY =
n

i=1
(xi −¯x)(yi −¯y) =
n

i=1
(xi −¯x)yi −¯y
n

i=1
(xi −¯x)
=
n

i=1
(xi −¯x)yi −0 =
n

i=1
(xi −¯x)yi
it follows that
ˆβ1 =
n

i=1
ci yi
where
ci = xi −¯x
SX X
Thus the point estimate ˆβ1 is the realization of the random variable
n

i=1
ciYi
where
Yi ∼N(β0 + β1xi, σ 2)
1 ≤i ≤n
The notation ˆβ1 is generally used both for this random variable and for its realization,
because this is unlikely to cause confusion. Notice then that
E( ˆβ1) = E
 n

i=1
ciYi
	
=
n

i=1
ci E(Yi) =
n

i=1
ci(β0 + β1xi)
= β0
n
i=1(xi −¯x)
SX X
+ β1
n
i=1(xi −¯x)xi
SX X
= 0 + β1
n
i=1 x2
i −n ¯x2
SX X
= β1
SX X
SX X
= β1
so that ˆβ1 is an unbiased point estimate of the slope parameter β1. Furthermore,
Var( ˆβ1) = Var
 n

i=1
ciYi
	
=
n

i=1
c2
i Var(Yi) = σ 2
n

i=1
c2
i
= σ 2
n

i=1
(xi −¯x)2
S2
X X
= σ 2
S2
X X
n

i=1
(xi −¯x)2 = σ 2
S2
X X
SX X = σ 2
SX X
so that the standard error of the point estimate ˆβ1 is
s.e.( ˆβ1) =
σ
√SX X
which can be estimated as
ˆσ
√SX X

12.3 INFERENCES ON THE SLOPE PARAMETER β1
563
FIGURE 12.21
The slope parameter β1 is
estimated more accurately in
scenario II than in scenario I
because the data points are more
spread out and Sxx is larger
+
+ + +
+
++
+
+
+
++
+
++ +
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+ +
Scenario II
Scenario I
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
d1
d2
d1 < d2
y
y
x
x
In fact, because ˆβ1 is a linear combination of normally distributed random variables Yi, it also
has a normal distribution, so that
ˆβ1 ∼N

β1, σ 2
SX X

An interesting point to notice is that for a ﬁxed value of the error variance σ 2, the variance
of the slope parameter estimate decreases as the value of SX X increases. This happens as the
values of the explanatory variable xi become more spread out, as illustrated in Figure 12.21.
This result is intuitively reasonable because a greater spread in the values xi provides a greater
“leverage” for ﬁtting the regression line, and therefore the slope parameter estimate ˆβ1 should
be more accurate.
The normal distribution for the slope parameter estimate ˆβ1 can be standardized as
ˆβ1 −β1

σ 2
SX X
∼N(0, 1)
Also, the estimate of the error variance is distributed
ˆσ 2 ∼σ 2 χ2
n−2
n −2

564
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
Inferences on the Slope Parameter β1
A two-sided conﬁdence interval with a conﬁdence level 1 −α for the slope parameter
in a simple linear regression model is
β1 ∈
 ˆβ1 −tα/2,n−2 × s.e.( ˆβ1), ˆβ1 + tα/2,n−2 × s.e.( ˆβ1)

which is
β1 ∈

ˆβ1 −ˆσ tα/2,n−2
√SX X
, ˆβ1 + ˆσ tα/2,n−2
√SX X

One-sided 1 −α conﬁdence level conﬁdence intervals are
β1 ∈

−∞, ˆβ1 + ˆσ tα,n−2
√SX X

and
β1 ∈

ˆβ1 −ˆσ tα,n−2
√SX X
, ∞

The two-sided hypotheses
H0 : β1 = b1
versus
HA : β1 ̸= b1
for a ﬁxed value b1 of interest are tested with the t-statistic
t =
ˆβ1 −b1

ˆσ 2
SX X
The p-value is
p-value = 2 × P(X > |t|)
where the random variable X has a t-distribution with n −2 degrees of freedom.
A size α test rejects the null hypothesis if |t| > tα/2,n−2.
The one-sided hypotheses
H0 : β1 ≥b1
versus
HA : β1 < b1
have a p-value
p-value = P(X < t)
and a size α test rejects the null hypothesis if t < −tα,n−2. The one-sided hypotheses
H0 : β1 ≤b1
versus
HA : β1 > b1
have a p-value
p-value = P(X > t)
and a size α test rejects the null hypothesis if t > tα,n−2.
and if this estimate is used in place of the error variance σ 2, then the resulting expression has
a t-distribution with n −2 degrees of freedom
ˆβ1 −β1

ˆσ 2
SX X
∼tn−2

12.3 INFERENCES ON THE SLOPE PARAMETER β1
565
This distributional result implies that a two-sided conﬁdence interval for the slope param-
eter β1 can be constructed as
β1 ∈
 ˆβ1 −critical point × s.e.( ˆβ1), ˆβ1 + critical point × s.e.( ˆβ1)

where the critical point is taken from a t-distribution with n−2 degrees of freedom. Hypothesis
tests concerning the value of the slope parameter β1 are performed by comparing a t-statistic
with a t-distribution with n −2 degrees of freedom. The conﬁdence intervals and hypothesis
testing procedures are outlined in the accompanying box.
Tests of the null hypothesis H0 : β1 = 0 are particularly interesting because if β1 = 0,
then the regression line is ﬂat and the distribution of the dependent variable is not affected by
the value of the explanatory variable. With b1 = 0, the t-statistic in this case is simply
t =
ˆβ1
s.e.( ˆβ1) =
ˆβ1

ˆσ 2
SX X
=
ˆβ1
√SX X
ˆσ
Computer output generally contains a two-sided p-value for this null hypothesis.
Computer output also usually contains a two-sided p-value for the null hypothesis H0 :
β0 = 0 that the intercept parameter is equal to 0. Inferences on the intercept parameter are
a special case of inferences on the ﬁtted value of the regression line, which are discussed in
Section 12.4. In most cases the null hypothesis H0 : β0 = 0 is not of any particular interest
and its p-value given in the computer output is not important.
12.3.2
Examples
Example 67
Car Plant Electricity
Usage
For this data set
SX X =
12

i=1
x2
i −
 12
i=1 xi
2
12
= 291.2310 −58.622
12
= 4.8723
so that the standard error of the slope parameter estimate ˆβ1 is
s.e.( ˆβ1) =
ˆσ
√SX X
=
0.1729
√
4.8723
= 0.0783
The t-statistic for testing the null hypothesis H0 : β1 = 0 is
t =
ˆβ1
s.e.( ˆβ1) = 0.49883
0.0783 = 6.37
The two-sided p-value is calculated as
p-value = 2 × P(X > 6.37) ≃0
where the random variable X has a t-distribution with 10 degrees of freedom. This low p-value
indicates that the null hypothesis is not plausible and so the slope parameter is known to be
nonzero. In other words, it has been established that the distribution of electricity usage does
depend on the level of production.
With t0.005,10 = 3.169, a 99% two-sided conﬁdence interval for the slope parameter is
β1 ∈
 ˆβ1 −critical point × s.e.( ˆβ1), ˆβ1 + critical point × s.e.( ˆβ1)

= (0.49883 −3.169 × 0.0783, 0.49883 + 3.169 × 0.0783) = (0.251, 0.747)

566
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
Thus the experimenter can be conﬁdent that within the range of the data set, the expected
electricity usage increases by somewhere between a quarter of a million kilowatt-hours and
three quarters of a million kilowatt-hours for every additional $1 million of production.
If you analyze this data set with a statistical software package, you will ﬁnd that a p-value
of 0.314 is given for the intercept parameter β0. In other words, the null hypothesis H0 : β0 = 0
is plausible. However, this is not important since β0 is the expected electricity cost when there
is no production (x = 0), which is extrapolating far below the data set. While it may be
tempting to use a simpliﬁed regression model
y = β1x
with β0 = 0, it is generally best to stay with the full ﬁtted regression model
y = ˆβ0 + ˆβ1x
Example 68
Nile River Flowrate
Here
SX X =
115

i=1
x2
i −
 115
i=1 xi
2
115
= 1982.5264 −462.92
115
= 119.2533
and the standard error of the slope parameter estimate is
s.e.( ˆβ1) =
ˆσ
√SX X
=
√
0.1092
√
119.2533
= 0.03026
The t-statistic for testing the null hypothesis H0 : β1 = 0 is
t =
ˆβ1
s.e.( ˆβ1) = 0.8362
0.03026 = 27.63
and the two-sided p-value is
p-value = 2 × P(X > 27.63) ≃0
where the random variable X has a t-distribution with 113 degrees of freedom. This low
p-value indicates that the slope parameter is nonzero, so that the distribution of the February
inﬂow has been shown to depend on the January inﬂow.
With t0.005,113 = 2.620, a 99% two-sided conﬁdence interval for the slope parameter is
β1 ∈(0.8362 −2.620 × 0.03026, 0.8362 + 2.620 × 0.03026) = (0.757, 0.915)
These values tell the dam engineers that an increase in the January inﬂow of 1 billion cubic
meters results in an increase in the expected February inﬂow of between about 750 and
920 million cubic meters.
Example 44
Army Physical
Fitness Test
With
SX X =
84

i=1
x2
i −
84
i=1 xi
2
84
= 402,972 −56942
84
= 17,000.14
the standard error of the slope parameter estimate is
s.e.( ˆβ1) =
ˆσ
√SX X
=
√
4606.42
√17,000.14 = 0.52054

12.3 INFERENCES ON THE SLOPE PARAMETER β1
567
The t-statistic for testing the null hypothesis H0 : β1 = 0 is
t =
ˆβ1
s.e.( ˆβ1) = −3.2544
0.52054 = −6.25
and the two-sided p-value is
p-value = 2 × P(X > 6.25) ≃0
where the random variable X has a t-distribution with 82 degrees of freedom. Again, the low
p-value indicates that the slope parameter is known to be nonzero, so that there is evidence
that the distribution of the run times depends on the number of pushups performed.
The critical point t0.005,82 = 2.637 gives a 99% two-sided conﬁdence interval
β1 ∈(−3.254 −2.637 × 0.5205, −3.254 + 2.637 × 0.5205) = (−4.63, −1.88)
Thus an additional pushup decreases the expected run time by somewhere between 1.88 and
4.63 seconds.
Example 69
Cranial
Circumferences
An anthropologist is interested in whether a fully grown human’s cranial (skull) circumference
isrelatedtothelengthofthemiddleﬁnger.Figure12.22showsadatasetofthesemeasurements
obtained from n = 20 randomly selected people, together with a ﬁtted regression line obtained
with cranial circumference taken as the dependent variable and ﬁnger length taken as the
explanatory variable.
Finger length
Cranial 
circumference
8.75
8.50
8.25
8.00
7.75
7.50
61
60
59
58
57
56
55
54
53
52
Cranial
circumference
(cm)
Finger
length
(cm)
58.5
7.6
54.2
7.9
57.2
8.4
52.7
7.7
55.1
8.6
60.7
8.6
57.2
7.9
58.8
8.2
56.2
7.7
60.7
8.1
53.5
8.1
60.7
7.9
56.3
8.1
58.1
8.2
56.6
7.8
57.7
7.9
59.2
8.3
57.9
8.0
56.8
8.2
55.4
7.9
FIGURE 12.22
Fitted line plot and data set for the cranial circumferences example

568
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
When a line is ﬁtted to the data, a slope parameter estimate ˆβ1 = 2.086 is obtained,
which suggests that the expected cranial circumference increases by about 2 cm for each ad-
ditional 1 cm of ﬁnger length. However, it can be shown that this point estimate has a standard
error of
s.e.( ˆβ1) = 1.862
so that the t-statistic for testing the null hypothesis H0 : β1 = 0 is
t =
ˆβ1
s.e.( ˆβ1) = 2.086
1.862 = 1.12
The two-sided p-value is therefore
p-value = 2 × P(X > 1.12) = 0.277
where the random variable X has a t-distribution with 18 degrees of freedom.
This large p-value implies that the null hypothesis H0 : β1 = 0 is plausible and so
there is not sufﬁcient evidence to conclude that the distribution of the cranial circumference
depends upon the ﬁnger length. In other words, the ﬁtted regression line is not statistically
signiﬁcant. The anthropologist should conclude that no relationship has been established
between the two variables. Of course, this does not prove that there is no relationship between
the two variables. Perhaps a larger study with a larger sample size might ﬁnd evidence of a
relationship.
12.3.3
Problems
12.3.1 In a simple linear regression analysis with n = 18 data
points, an estimate ˆβ1 = 0.522 is obtained with
s.e.( ˆβ1) = 0.142.
(a) Calculate a two-sided 99% conﬁdence interval for the
slope parameter β1.
(b) Test the null hypothesis H0 : β1 = 0 against a
two-sided alternative hypothesis.
12.3.2 In a simple linear regression analysis with n = 22 data
points, an estimate ˆβ1 = 56.33 is obtained with
s.e.( ˆβ1) = 3.78.
(a) Calculate a two-sided 95% conﬁdence interval for the
slope parameter β1.
(b) Test the null hypothesis H0 : β1 = 50.0 against a
two-sided alternative hypothesis.
12.3.3 Oil Well Drilling Costs
Consider the data set of oil well drilling costs given in
DS 12.2.1.
(a) What is the standard error of ˆβ1?
(b) Construct a two-sided 95% conﬁdence interval for the
slope parameter β1.
(c) Test the null hypothesis H0 : β1 = 0. Interpret your
answers.
12.3.4 Truck Unloading Times
Consider the data set of the times taken to unload a truck
at a warehouse given in DS 12.2.2.
(a) What is the standard error of ˆβ1?
(b) Construct a two-sided 90% conﬁdence interval for the
slope parameter β1.
(c) Test the null hypothesis H0 : β1 = 0.
(d) Does your analysis indicate that there is evidence
that the trucks take longer to unload when the
temperature is higher? Can a case be made that the
installation of air-conditioning will improve worker
efﬁciency?
12.3.5 VO2-max Aerobic Fitness Measurements
Consider the data set of aerobic ﬁtness measurements
given in DS 12.2.3.
(a) What is the standard error of ˆβ1?
(b) Construct and interpret a one-sided 95% conﬁdence
interval for the slope parameter β1 that provides an
upper bound.
(c) Test the null hypothesis H0 : β1 = 0. Is it clear that on
average aerobic ﬁtness decreases with age?

12.4 INFERENCES ON THE REGRESSION LINE
569
12.3.6 Property Tax Appraisals
Consider the data set of appraised house values given in
DS 12.2.4.
(a) What is the standard error of ˆβ1?
(b) Construct a two-sided 99% conﬁdence interval for the
slope parameter β1.
(c) Test the null hypothesis H0 : β1 = 0. Interpret your
answers.
12.3.7 Management of Computer Systems
Consider the data set of the times taken for programming
changes given in DS 12.2.5.
(a) What is the standard error of ˆβ1?
(b) Construct a two-sided 95% conﬁdence interval for the
slope parameter β1.
(c) Why is the null hypothesis H0 : β1 = 1 of particular
interest? Test this null hypothesis. Interpret your
answers.
12.3.8 Vacuum Transducer Bobbin Resistances
Consider the data set of vacuum transducer bobbin
resistances given in DS 12.2.6.
(a) What is the standard error of ˆβ1?
(b) Construct a two-sided 99% conﬁdence interval for the
slope parameter β1.
(c) Test the null hypothesis H0 : β1 = 0. Is it clear that
resistance increases with temperature?
12.3.9 A simple linear regression is performed on 20 data pairs
(xi, yi). It is found that ˆβ1 = 54.87 and s.e. ( ˆβ1) = 21.20.
Is the p-value for the two-sided hypothesis test of
H0 : β1 = 0 (a) greater than 10%, (b) between 1% and
10%, or (c) less than 1%?
12.3.10 Ceramic Baking Procedures
Several samples of ceramic were made with different
baking times. The densities of the samples were measured
and the data set given in DS 12.3.1 was obtained. Use a
simple linear regression model to investigate whether
there is any evidence that the density of the ceramic is
affected by the baking time.
12.3.11 In simple linear regression, the computer output contains
a p-value that can be used to assess whether there is any
evidence that the output (response) variable depends upon
the input variable.
A. True
B. False
12.4
Inferences on the Regression Line
12.4.1
Inference Procedures
For a particular value x∗of the explanatory variable, the true regression line
β0 + β1x∗
speciﬁes the expected value of the dependent variable or, in other words, the expected response.
Thus, if the random variable Y|x∗represents the value of the dependent variable when the
explanatory variable is equal to x∗, then
E(Y|x∗) = β0 + β1x∗
It is useful to be able to construct conﬁdence intervals for this expected value.
The point estimate of the average response at x∗is
ˆy|x∗= ˆβ0 + ˆβ1x∗= (¯y −ˆβ1 ¯x) + ˆβ1x∗= ¯y + ˆβ1(x∗−¯x)
= ¯y +
n

i=1
(xi −¯x)yi
SX X
(x∗−¯x) =
n

i=1
1
n + (xi −¯x)(x∗−¯x)
SX X

yi
This estimate is an observation from the random variable
n

i=1
1
n + (xi −¯x)(x∗−¯x)
SX X

Yi
where
Yi ∼N(β0 + β1xi, σ 2)
1 ≤i ≤n

570
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
Since it is a linear combination of normal random variables, this random variable is also
normally distributed, and it can be shown that it has an expectation and variance of
β0 + β1x∗
and
σ 2
1
n + (x∗−¯x)2
SX X

Thus, ˆβ0 + ˆβ1x∗is an unbiased estimator of the expected response β0 + β1x∗at x∗, and it has
a standard error
s.e.( ˆβ0 + ˆβ1x∗) = σ

1
n + (x∗−¯x)2
SX X
which can be estimated as
s.e.( ˆβ0 + ˆβ1x∗) = ˆσ

1
n + (x∗−¯x)2
SX X
From this estimated standard error it follows that
( ˆβ0 + ˆβ1x∗) −(β0 + β1x∗)
s.e.( ˆβ0 + ˆβ1x∗)
∼tn−2
and this distributional result can be used to make inferences on the expected response as
outlined in the box.
Inferences on the Expected Value of the Dependent Variable
A 1 −α conﬁdence level two-sided conﬁdence interval for β0 + β1x∗, the expected
value of the dependent variable for a particular value x∗of the explanatory variable, is
β0 + β1x∗∈(ˆβ0 + ˆβ1x∗−tα/2,n−2 × s.e.( ˆβ0 + ˆβ1x∗),
ˆβ0 + ˆβ1x∗+ tα/2,n−2 × s.e.( ˆβ0 + ˆβ1x∗))
where
s.e.( ˆβ0 + ˆβ1x∗) = ˆσ

1
n + (x∗−¯x)2
SX X
One-sided conﬁdence intervals are
β0 + β1x∗∈(−∞, ˆβ0 + ˆβ1x∗+ tα,n−2 × s.e.( ˆβ0 + ˆβ1x∗))
and
β0 + β1x∗∈(ˆβ0 + ˆβ1x∗−tα,n−2 × s.e.( ˆβ0 + ˆβ1x∗), ∞)
Hypothesis tests on β0 + β1x∗can be performed by comparing the t-statistic
t = ( ˆβ0 + ˆβ1x∗) −(β0 + β1x∗)
s.e.( ˆβ0 + ˆβ1x∗)
with a t-distribution with n −2 degrees of freedom.

12.4 INFERENCES ON THE REGRESSION LINE
571
Notice that the standard error of the point estimate ˆβ0 + ˆβ1x∗depends on the value of x∗.
When x∗= ¯x it takes its smallest value
s.e.( ˆβ0 + ˆβ1 ¯x) =
ˆσ
√n
and it increases in size as x∗moves farther away from ¯x. On an intuitive level this can be
understood as indicating that the ﬁtted regression line is the most accurate at the center of the
data set and is less accurate toward the edges of the data set.
It is useful to draw conﬁdence bands around the ﬁtted regression line that are composed of
individual conﬁdence intervals for the expected response at different values of the explanatory
variable. These bands are illustrated in the following examples. Since the conﬁdence interval
lengths are proportional to the standard error of ˆβ0 + ˆβ1x∗, the bands are narrowest at x∗= ¯x
and become increasingly wide as x∗moves farther away from ¯x.
Notice that the standard error of ˆβ0 + ˆβ1x∗approaches 0 as the sample size n tends to
inﬁnity (this is the case since SX X will also tend to inﬁnity as n tends to inﬁnity). This implies
that the ﬁtted regression line
y = ˆβ0 + ˆβ1x
becomes an increasingly accurate estimate of the true regression line
y = β0 + β1x
as more and more data observations (yi, xi) are available. Thus, the conﬁdence bands shrink
toward the ﬁtted regression line as the sample size increases.
As a ﬁnal point, notice that taking x∗= 0 provides inferences on the intercept parameter
β0. Hence, a 1 −α conﬁdence level two-sided conﬁdence interval for β0 is
β0 ∈
⎛
⎝ˆβ0 −tα/2,n−2 ˆσ

1
n +
¯x2
SX X
, ˆβ0 + tα/2,n−2 ˆσ

1
n +
¯x2
SX X
⎞
⎠
and a two-sided p-value for the null hypothesis H0 : β0 = 0, contained in most computer
output, is
p-value = 2 × P(X > |t|)
where the random variable X has a t-distribution with n −2 degrees of freedom, and the
t-statistic is
t =
ˆβ0
ˆσ

1
n +
¯x2
SX X
12.4.2
Examples
Example 67
Car Plant Electricity
Usage
For this example
s.e.( ˆβ0 + ˆβ1x∗) = ˆσ

1
n + (x∗−¯x)2
SX X
= 0.1729 ×

1
12 + (x∗−4.885)2
4.8723

572
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.23
Conﬁdence bands for the car plant
electricity usage data set
Production
Electricity usage
6.0
5.5
5.0
4.5
4.0
3.5
3.6
3.4
3.2
3.0
2.8
2.6
2.4
2.2
2.0
With t0.025,10 = 2.228, a 95% conﬁdence interval for β0 + β1x∗is therefore
β0 + β1x∗∈

0.409 + 0.499x∗−2.228 × 0.1729 ×

1
12 + (x∗−4.885)2
4.8723
,
0.409 + 0.499x∗+ 2.228 × 0.1729 ×

1
12 + (x∗−4.885)2
4.8723
	
At x∗= 5 this is
β0 + 5β1 ∈(0.409 + (0.499 × 5) −0.113, 0.409 + (0.499 × 5) + 0.113)
= (2.79, 3.02)
This interval implies that with a monthly production of $5 million, the expected electricity
usage is between about 2.8 and 3.0 kWh.
Figure 12.23 shows the conﬁdence bands on the ﬁtted regression line obtained from the
individual 95% conﬁdence intervals. As expected, they are narrowest at ¯x = 4.885 and they
becomewidertowardtheedgesofthedataset.Itisconvenienttothinkofthetrueregressionline
y = β0 + β1x
as lying somewhere within these conﬁdence bands.
Example 68
Nile River Flowrate
The standard error of the ﬁtted regression line is
s.e.( ˆβ0 + ˆβ1x∗) =
√
0.1092 ×

1
115 + (x∗−4.0252)2
119.25
so that with t0.025,113 = 1.9812, a 95%conﬁdence interval for the ﬁtted regression line is
β0 + β1x∗∈

−0.470 + 0.836x∗−1.9812 ×
√
0.1092 ×

1
115 + (x∗−4.0252)2
119.25
,
−0.470 + 0.836x∗+ 1.9812 ×
√
0.1092 ×

1
115 + (x∗−4.0252)2
119.25
	

12.4 INFERENCES ON THE REGRESSION LINE
573
FIGURE 12.24
Conﬁdence bands for the Nile
River inﬂows data set
January inflow
February inflow
7
6
5
4
3
2
1
6
5
4
3
2
1
FIGURE 12.25
Conﬁdence bands for the Army
Physical Fitness Test data set
Number of pushups
2-mile run time
130
120
110
100
90
80
70
60
50
40
1100
1000
900
800
700
600
These conﬁdence intervals produce the conﬁdence bands shown in Figure 12.24. With n = 115
data observations, these bands appear fairly close together over the range of the data set so that
the dam engineers can deduce that the regression line has been determined fairly accurately.
Example 44
Army Physical Fitness
Test
For this example the standard error of the ﬁtted regression line is
s.e.( ˆβ0 + ˆβ1x∗) = 67.87 ×

1
84 + (x∗−67.79)2
17,000
so that with t0.025,82 = 1.9893, the 95% conﬁdence bands shown in Figure 12.25 are calculated
as
β0 + β1x∗∈
⎛
⎝1078 −3.254x∗−1.9893 × 67.87 ×

1
84 + (x∗−67.79)2
17,000
,
1078 −3.254x∗+ 1.9893 × 67.87 ×

1
84 + (x∗−67.79)2
17,000
⎞
⎠

574
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
For example, at x∗= 70 pushups the conﬁdence interval is
β0 + 70β1 ∈(1078 −(3.254 × 70) −14.91, 1078 −(3.254 × 70) + 14.91)
= (835.3, 865.1)
This interval indicates that based upon the data set available, the expected 2-mile run time
for an ofﬁcer performing 70 pushups is known to lie somewhere between 13 minutes and 55
seconds and 14 minutes and 25 seconds.
12.4.3
Problems
12.4.1 Show that the random variable
n

i=1

1
n + (xi −¯x)(x∗−¯x)
SX X

Yi
where
Yi ∼N(β0 + β1xi, σ 2),
1 ≤i ≤n
has an expectation and variance
β0 + β1x∗
and
σ 2

1
n + (x∗−¯x)2
SX X

12.4.2 In a simple linear regression analysis with n = 17 data
points, the estimates ˆβ0 = 12.08, ˆβ1 = 34.60, and
ˆσ 2 = 17.65 are obtained. If SX X = 1096 and ¯x = 53.2,
construct a two-sided 95% conﬁdence interval for the
expected response at x∗= 40.0. (This problem is
continued in Problem 12.5.1.)
12.4.3 In a simple linear regression analysis with n = 24 data
points, the estimates ˆβ0 = −7.80, ˆβ1 = 2.23, and
ˆσ 2 = 1.76 are obtained. If SX X = 543.5 and ¯x = 10.8,
construct a two-sided 95% conﬁdence interval for the
expected response at x∗= 13.6. (This problem is
continued in Problem 12.5.2.)
12.4.4 Oil Well Drilling Costs
Consider the data set of oil well costs given in DS 12.2.1.
Construct a two-sided 95% conﬁdence interval for the
expected cost of oil wells with a depth of 9500 feet.
12.4.5 VO2-max Aerobic Fitness Measurements
Consider the data set of aerobic ﬁtness measurements
given in DS 12.2.3. Construct and interpret a two-sided
95% conﬁdence interval for the expected VO2-max
measurements of 50-year-old males.
12.4.6 Property Tax Appraisals
Consider the data set of appraised house values given in
DS 12.2.4. Construct a two-sided 99% conﬁdence interval
for the average appraised value of houses with a size of
3200 square feet.
12.4.7 Management of Computer Systems
Consider the data set of the times taken for programming
changes given in DS 12.2.5. Construct a one-sided 95%
conﬁdence interval that provides an upper bound on the
actual time on average for tasks that are estimated to take
7 hours.
12.4.8 Vacuum Transducer Bobbin Resistances
Consider the data set of vacuum transducer bobbin
resistances given in DS 12.2.6. Construct a two-sided
99% conﬁdence interval for the average resistance of a
vacuum transducer bobbin at a temperature of 70◦F.
12.4.9 A simple linear regression analysis is performed to
determine how the strength of a substance depends on its
density. Eight samples of the substance are prepared, with
densities 11.2, 12.5, 13.4, 14.9, 16.0, 16.6. 17.9, and 20.1,
and their strengths are found. The estimates ˆβ0 = 75.32,
ˆβ1 = 0.0674, and ˆσ = 0.0543 are obtained. Calculate a
95% conﬁdence interval for the average strength of
substances with a density 15.
12.4.10 A linear regression model is ﬁtted to the data
x
y
37.0
65.0
36.4
67.2
35.8
70.3
34.3
71.9
33.7
73.8
32.1
75.7
31.5
77.9
with x as the input variable and y as the output variable.
Find ˆβ0, ˆβ1, and ˆσ 2. Construct a 99% conﬁdence interval
for the expected value of the output variable when the
input variable is equal to 35.

12.5 PREDICTION INTERVALS FOR FUTURE RESPONSE VALUES
575
12.5
Prediction Intervals for Future Response Values
12.5.1
Inference Procedures
One of the most important issues for an experimenter to investigate is a future value of
the dependent variable y or, in other words, a response y obtained with a value x∗of the
explanatory variable. It is useful to construct a prediction interval for this future response
value y|x∗, which like a conﬁdence interval for an unknown parameter speciﬁes a region for
the future response value at a given conﬁdence level.
Recall that under the modeling assumptions, a future response y|x∗is composed of a
point on the regression line β0 + β1x∗together with an error term, which is taken to be an
observation from a normal distribution with mean zero and variance σ 2. Consequently, when
one makes inferences about a future response at x∗, there are two sources of uncertainty. The
ﬁrst is the uncertainty in the value of the regression line at x∗and the second is the variability
in the error term. Thus, while the value on the ﬁtted regression line
ˆβ0 + ˆβ1x∗
serves as a point estimate of the expected response at x∗, the total variability is
Var( ˆβ0 + ˆβ1x∗) + σ 2 = σ 2
1
n + (x∗−¯x)2
SX X

+ σ 2 = σ 2
n + 1
n
+ (x∗−¯x)2
SX X

which is the variance of the ﬁtted regression line together with the variance of the error term.
This leads to the prediction intervals shown in the box.
Prediction Intervals for Future Response Values
A 1 −α conﬁdence level two-sided prediction interval for y|x∗, a future value of the
dependent variable for a particular value x∗of the explanatory variable, is
y|x∗∈
⎛
⎝ˆβ0 + ˆβ1x∗−tα/2,n−2 ˆσ

n + 1
n
+ (x∗−¯x)2
SX X
,
ˆβ0 + ˆβ1x∗+ tα/2,n−2 ˆσ

n + 1
n
+ (x∗−¯x)2
SX X
⎞
⎠
One-sided prediction intervals are
y|x∗∈
⎛
⎝−∞, ˆβ0 + ˆβ1x∗+ tα,n−2 ˆσ

n + 1
n
+ (x∗−¯x)2
SX X
⎞
⎠
and
y|x∗∈
⎛
⎝ˆβ0 + ˆβ1x∗−tα,n−2 ˆσ

n + 1
n
+ (x∗−¯x)2
SX X
, ∞
⎞
⎠
It is important to understand the difference between this prediction interval for a future
response at x∗and the conﬁdence interval on the regression line at x∗discussed in the previous

576
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
section. Remember that the regression line at x∗indicates the expected or average response at
x∗, and the length of a conﬁdence interval for the regression line decreases to 0 as the sample
size n becomes increasingly large.
The prediction interval for a future response discussed in this section is a prediction
interval for one particular observation of the dependent variable obtained with the explana-
tory variable equal to x∗. It is necessarily larger than the conﬁdence interval for the re-
gression line. As the sample size n increases, the length of this prediction interval de-
creases, but not all the way to 0. The shortest prediction interval (in the limiting case when
n →∞) is
y|x∗∈
 ˆβ0 + ˆβ1x∗−tα/2,n−2 ˆσ, ˆβ0 + ˆβ1x∗+ tα/2,n−2 ˆσ

since even though the regression line may be estimated with increasing precision, the error
term component of y|x∗always has a ﬁxed variance of σ 2.
Prediction bands for a future response can be drawn around a ﬁtted regression line
in a manner similar to the conﬁdence bands for the regression line itself, as illustrated
in the following examples. The prediction bands for a future response are always wider
than the conﬁdence bands for the regression line, and like the conﬁdence bands for the
regression line they are narrowest at ¯x and become wider toward the edges of the
data set.
12.5.2
Examples
Example 67
Car Plant Electricity
Usage
With t0.025,10 = 2.228, a 95% prediction interval for a future response y|x∗is
y|x∗∈

0.409 + 0.499x∗−2.228 × 0.1729 ×

13
12 + (x∗−4.885)2
4.8723
,
0.409 + 0.499x∗+ 2.228 × 0.1729 ×

13
12 + (x∗−4.885)2
4.8723
	
At x∗= 5 this interval is
y|5 ∈(0.409 + (0.499 × 5) −0.401, 0.409 + (0.499 × 5) + 0.401)
= (2.50, 3.30)
This prediction interval indicates that if next month’s production target is $5 million, then
with 95% conﬁdence next month’s electricity usage will be somewhere between 2.5 and
3.3 kWh.
Recall that a 95% conﬁdence interval for the regression line at x∗= 5 was calculated
as (2.8, 3.0). Thus, while the expected or average electricity usage in a month with $5
million of production is known to lie somewhere between 2.8 and 3.0 kWh, the electric-
ity usage in a particular month with $5 million of production will lie somewhere between
2.5 and 3.3 kWh.
Figure 12.26 shows the prediction bands for a future response obtained from the individual
95% prediction intervals given above, together with the conﬁdence bands on the regression
line. As expected, both sets of bands are narrowest at ¯x = 4.885, and the prediction bands for
a future response are wider than the conﬁdence bands on the regression line.

12.5 PREDICTION INTERVALS FOR FUTURE RESPONSE VALUES
577
FIGURE 12.26
Prediction bands and conﬁdence
bands for the car plant electricity
usage data set
Production
Electricity usage
6.0
5.5
5.0
4.5
4.0
3.5
4.0
3.5
3.0
2.5
2.0
FIGURE 12.27
Prediction bands and conﬁdence
bands for the Nile River inﬂows
data set
January inflow
February inflow
7
6
5
4
3
2
1
6
5
4
3
2
1
0
Example 68
Nile River Flowrate
With t0.025,113 = 1.9812, the 95% prediction bands for a future response, shown in Fig-
ure 12.27, are calculated as
y|x∗∈

−0.470 + 0.836x∗−1.9812 ×
√
0.1092 ×

116
115 + (x∗−4.0252)2
119.25
,
−0.470 + 0.836x∗+ 1.9812 ×
√
0.1092 ×

116
115 + (x∗−4.0252)2
119.25
	
For example, the prediction interval at x∗= 3 is
y|3 ∈(−0.470 + (0.836 × 3) −0.660, −0.470 + (0.836 × 3) + 0.660)
= (1.38, 2.70)

578
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.28
Prediction bands and conﬁdence
bands for the Army Physical
Fitness Test data set
Number of pushups
2-mile run time
130
120
110
100
90
80
70
60
50
40
1100
1000
900
800
700
600
500
Thus, if in a particular year the January inﬂow is measured at 3 billion cubic meters, then
February’s inﬂow can be predicted as being somewhere between 1.38 and 2.70 cubic meters
with 95% conﬁdence.
Example 44
Army Physical Fitness
Test
With t0.025,82 = 1.9893, the 95% prediction bands for a future response shown in Figure 12.28
are
y|x∗∈
⎛
⎝1078 −3.254x∗−1.9893 × 67.87 ×

85
84 + (x∗−67.79)2
17,000
,
1078 −3.254x∗+ 1.9893 × 67.87 ×

85
84 + (x∗−67.79)2
17,000
⎞
⎠
For example, at x∗= 70 pushups the prediction interval for the 2-mile run time is
y|70 ∈(1078 −(3.254 × 70) −135.83, 1078 −(3.254 × 70) + 135.83)
= (714.4, 986.1)
Thus, although it was found in the last section that the expected 2-mile run time for an
ofﬁcer performing 70 pushups lies somewhere between 13 minutes and 55 seconds and 14
minutes and 25 seconds, this prediction interval indicates that a particular ofﬁcer who just
performed 70 pushups will have a run time between 11 minutes and 54 seconds and 16 minutes
and 27 seconds, with 95% conﬁdence.
12.5.3
Problems
12.5.1 In a simple linear regression analysis with n = 17 data
points, the estimates ˆβ0 = 12.08, ˆβ1 = 34.60, and
ˆσ 2 = 17.65 are obtained. If SX X = 1096 and ¯x = 53.2,
construct a two-sided 95% prediction interval for a future
response value at x∗= 40.0.
12.5.2 In a simple linear regression analysis with n = 24 data
points, the estimates ˆβ0 = −7.80, ˆβ1 = 2.23, and
ˆσ 2 = 1.76 are obtained. If SX X = 543.5 and ¯x = 10.8,
construct a two-sided 95% prediction interval for a future
response value at x∗= 13.6.

12.6 THE ANALYSIS OF VARIANCE TABLE
579
12.5.3 Oil Well Drilling Costs
Consider the data set of oil well costs given in DS 12.2.1.
Construct a two-sided 95% prediction interval for the cost
of an oil well that is planned to have a depth of 9500 feet.
12.5.4 VO2-max Aerobic Fitness Measurements
Consider the data set of aerobic ﬁtness measurements
given in DS 12.2.3. Construct and interpret a two-sided
95% prediction interval for the VO2-max measurement of
a new experimental subject who is a 50-year-old male.
12.5.5 Property Tax Appraisals
Consider the data set of appraised house values given in
DS 12.2.4. Construct a two-sided 99% prediction interval
for the appraised value of a newly constructed house with
a size of 3200 square feet.
12.5.6 Management of Computer Systems
Consider the data set of the times taken for programming
changes given in DS 12.2.5. A new programming change
has been decided upon, and the expert estimates that it
will take 7 hours to complete. Construct a one-sided 95%
prediction interval that provides an upper bound on the
actual time that the task will take.
12.5.7 Vacuum Transducer Bobbin Resistances
Consider the data set of vacuum transducer bobbin
resistances given in DS 12.2.6. If a vacuum transducer
bobbin is installed in a new car, construct a two-sided
99% prediction interval for the resistance of that vacuum
transducer bobbin at a temperature of 70◦F.
12.5.8 The amount of catalyst (x) and the yield (y) of a chemical
experiment are analyzed using a simple linear regression
model. There are 30 observations (xi, yi), and it is found
that the ﬁtted model is
y = 51.98 + 3.44x
Suppose that the sum of squared residuals is
30
i=1 e2
i = 329.77, and that 30
i=1 xi = 603.36,
30
i=1 x2
i = 12578.22. If an additional experiment is
planned with a catalyst level 22, give a range of values
with 95% conﬁdence for the yield that you think will be
obtained for that experiment.
12.5.9 A linear regression model is ﬁtted to the data
x
y
17.1
45.9
18.4
48.2
19.8
50.3
20.3
50.9
21.6
52.8
22.1
55.5
23.5
57.9
with x as the input variable and y as the output variable.
Find ˆβ0, ˆβ1 and ˆσ 2. Construct a 99% prediction interval
for an observation of the output variable when the input
variable is equal to 20.
12.5.10 In simple linear regression, if the regression model is
statistically signiﬁcant, then it can be used to predict the
output (response) variable for future values of the input
variable.
A. True
B. False
12.6
The Analysis of Variance Table
12.6.1
Sum of Squares Decomposition
As with a one-factor layout discussed in Chapter 11, an analysis of variance table can also
be constructed for a simple linear regression analysis. This analysis of variance table is based
upon the variability in the dependent variable y and provides a test of the null hypothesis
H0 : β1 = 0
which is equivalent to the similar test described in Section 12.3.
The total sum of squares
SST =
n

i=1
(yi −¯y)2
measures the total variability in the values of the dependent variable (this is also given the
notation SYY). It has n −1 degrees of freedom. As Figure 12.29 shows, SST is partitioned

580
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.29
The sum of squares decomposition
for a regression analysis
Total sum of squares
SST
Sum of squares for error
SSE
Sum of squares for regression
SSR
into a sum of squares for regression SSR with 1 degree of freedom and a sum of squares for
error SSE with n −2 degrees of freedom. The sum of squares for regression
SSR =
n

i=1
(ˆyi −¯y)2
measures the amount of variability in the dependent variable that is accounted for by the ﬁtted
regression line, and the sum of squares for error
SSE =
n

i=1
(yi −ˆyi)2
measures the variability about the ﬁtted regression line. These three sources of variability are
illustrated in Figure 12.30. The mean squares are
MSR = SSR
1
= SSR
and
MSE = SSE
n −2 = ˆσ 2 ∼σ 2 χ2
n−2
n −2
An analysis of variance table can be constructed as shown in Figure 12.31. Under the null
hypothesis
H0 : β1 = 0
the regression sum of squares is distributed
SSR ∼χ2
1
so that the F-statistic has an F-distribution
F = MSR
MSE ∼F1,n−2
The (two-sided) p-value is then
p-value = P(X > F)
where the random variable X has an F1,n−2 distribution.
This p-value is equal to the two-sided p-value obtained in Section 12.3 based upon the
t-statistic
t =
ˆβ1
√SX X
ˆσ
This is because the F-statistic is the square of the t-statistic
t2 = F
and the square of a random variable with a t-distribution with n −2 degrees of freedom has an
F1,n−2 distribution. Therefore, in computer output for simple linear regression problems the

12.6 THE ANALYSIS OF VARIANCE TABLE
581
FIGURE 12.30
The sum of squares for a simple
linear regression
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
y = ˆβ0 + ˆβ1x
SST =
n
i=1(yi −¯y)2
SSR =
n
i=1
( ˆyi −¯y)2
y = ˆβ0 + ˆβ1x
¯y
+
+
¯y
x
y
y
x
y
x
SSE =
n
i=1
(yi −
)2
ˆyi
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression 
Error
F = MSR/MSE
P(F1,n−2 > F)
SSR
SSE
MSR = SSR
ˆσ 2 = MSE = SSE/(n −2)
Total
1
n −2
n −1
SST
FIGURE 12.31
The analysis of variance table for a simple linear regression analysis
p-value given in the analysis of variance table and the p-value given for the slope parameter
are always exactly the same.
The proportion of the total variability in the dependent variable y that is accounted for by
the regression line is
R2 = SSR
SST = 1 −SSE
SST =
1
1 + SSE
SSR
which is known as the coefﬁcient of determination. This coefﬁcient takes a value between 0
and 1, and the closer it is to one the smaller is the sum of squares for error SSE in relation to
the sum of squares for regression SSR. Thus, larger values of R2 tend to indicate that the data

582
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.32
The coefﬁcient of determination R2
is larger in scenario II than in
scenario I
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Scenario I
+
+
+
+ +
++
+
+
+
+
+
Scenario II
+
+
+
+
+
+
+
+
+
+
+
y = ˆβ0 + ˆβ1x
y = ˆβ0 + ˆβ1x
x
x
y
y
Smaller R2
Larger R2
FIGURE 12.33
Analysis of variance table for the
car plant electricity usage
regression
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression
1
1.2124
1.2124
40.53
0.000
Error
10
0.2991
0.0299
Total
11
1.5115
points are closer to the ﬁtted regression line, as shown in Figure 12.32. Nevertheless, a low
value of R2 should not necessarily be interpreted as implying that the ﬁtted regression line is
not appropriate or is not useful. A ﬁtted regression line may be accurate and informative even
though a small value of R2 is obtained because of a large error variance σ 2.
12.6.2
Examples
Example 67
Car Plant Electricity
Usage
The analysis of variance table is given in Figure 12.33. Notice that the F-statistic
F = MSR
MSE = 1.2124
0.0299 = 40.53
is the square of the t-statistic for the slope parameter t = 6.37. The coefﬁcient of determination
R2 is reported as being 80.2%, which is
R2 = SSR
SST = 1.2124
1.5115 = 0.802
Consequently, about 80% of the variability in the electricity usage can be accounted for by
changes in the production levels.

12.6 THE ANALYSIS OF VARIANCE TABLE
583
Coefﬁcient of Determination R2
The total variability in the dependent variable, the total sum of squares
SST =
n

i=1
(yi −¯y)2
can be partitioned into the variability explained by the regression line, the regression
sum of squares
SSR =
n

i=1
(ˆyi −¯y)2
and the variability about the regression line, the error sum of squares
SSE =
n

i=1
(yi −ˆyi)2
The proportion of the total variability accounted for by the regression line is the
coefﬁcient of determination
R2 = SSR
SST = 1 −SSE
SST =
1
1 + SSE
SSR
which takes a value between 0 and 1.
Example 68
Nile River Flowrate
The analysis of variance table is given in Figure 12.34. Again, notice that the F-statistic
F = MSR
MSE = 83.379
0.109 = 763.250
is the square of the t-statistic for the slope parameter t = 27.627. The coefﬁcient of determi-
nation R2 is
R2 = SSR
SST = 83.379
95.723 = 0.871
with R = 0.933. Thus, about 87% of the variability in the February inﬂows can be accounted
for by changes in the January inﬂows.
Example 44
Army Physical Fitness
Test
The analysis of variance table is given in Figure 12.35. The F-statistic is
F = MSR
MSE = 180,051.10
4606.42
= 39.09
which is the square of the t-statistic for the slope parameter t = −6.25. The coefﬁcient of
determination R2 is reported as
R2 = SSR
SST = 180,051.10
557,777.50 = 0.323
FIGURE 12.34
Analysis of variance table for the
Nile River ﬂowrate regression
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression
1
83.379
83.379
763.250
0.000
Error
113
12.344
0.109
Total
114
95.723

584
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.35
Analysis of variance table for the
Army Physical Fitness Test
regression
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression
1
180,051.10
180,051.10
39.09
0.000
Error
82
377,726.40
4606.42
Total
83
557,777.50
Therefore, about 32% of the variability in the 2-mile run time can be accounted for by the
number of pushups performed.
12.6.3
Problems
12.6.1 Calculate the missing values in the analysis of variance
table for a simple linear regression analysis shown in
Figure 12.36. What is the p-value? What is the
coefﬁcient of determination R2?
12.6.2 Repeat Problem 12.6.1 for the analysis of variance table
shown in Figure 12.37.
12.6.3 A data set has n = 10, 10
i=1 yi = 908.8,
10
i=1 y2
i = 83,470, and ˆσ 2 = 0.9781. Compute the
analysis of variance table and calculate the coefﬁcient of
determination R2.
12.6.4 A data set has n = 25, 25
i=1 xi = 1356.25, 25
i=1 yi =
−6225, 25
i=1 x2
i = 97,025, 25
i=1 y2
i = 10,414,600,
and 25
i=1 xi yi = −738,100. Compute the analysis of
variance table and calculate the coefﬁcient of
determination R2.
12.6.5 Oil Well Drilling Costs
Consider the data set of oil well costs given in DS
12.2.1. Compute the analysis of variance table and
calculate the coefﬁcient of determination R2. Check that
the F-statistic is the square of the t-statistic for testing
H0 : β1 = 0, calculated earlier.
12.6.6 Truck Unloading Times
Consider the data set of the times taken to unload a truck
at a warehouse given in DS 12.2.2. Compute the analysis
of variance table and calculate the coefﬁcient of
determination R2. Check that the F-statistic is the
square of the t-statistic for testing H0 : β1 = 0,
calculated earlier. What is the implication of the p-value
in the analysis of variance table?
12.6.7 VO2-max Aerobic Fitness Measurements
Consider the data set of aerobic ﬁtness measurements
given in DS 12.2.3. Compute the analysis of variance
table and calculate the coefﬁcient of determination R2.
Explain the interpretation of the coefﬁcient of
determination.
12.6.8 Property Tax Appraisals
Consider the data set of appraised house values given in
DS 12.2.4. Compute the analysis of variance table and
calculate the coefﬁcient of determination R2. Check that
the F-statistic is the square of the t-statistic for testing
H0 : β1 = 0, calculated earlier. What does the value of
the coefﬁcient of determination tell you about the way
that houses are appraised?
FIGURE 12.36
Analysis of variance table
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression 
Error
2.32
?
1
?
?
576.51
?
?
Total
34
?
FIGURE 12.37
Analysis of variance table
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Regression 
Error
6.47
?
?
?
Total
1
19
?
?
?
474.80

12.7 RESIDUAL ANALYSIS
585
12.6.9 Management of Computer Systems
Consider the data set of the times taken for
programming changes given in DS 12.2.5. Compute the
analysis of variance table and calculate the coefﬁcient of
determination R2. Is the p-value in the analysis of
variance table meaningful?
12.6.10 Vacuum Transducer Bobbin Resistances
Consider the data set of vacuum transducer bobbin
resistances given in DS 12.2.6. Compute the analysis of
variance table and calculate the coefﬁcient of
determination R2. Check that the F-statistic is the
square of the t-statistic for testing H0 : β1 = 0,
calculated earlier.
12.6.11 In simple linear regression, the coefﬁcient of
determination measures:
A. The amount of variability of the y variable that is
explained by the x variable
B. The amount of variability of the x variable that is
explained by the y variable
C. The amount of variability between the treatments
D. The amount of variability within the treatments
12.6.12 In simple linear regression:
A. The larger the coefﬁcient of determination, then the
more precisely the y variable can be predicted from
the x variable.
B. The larger the coefﬁcient of determination, then the
less precisely the y variable can be predicted from
the x variable.
C. Neither of the above.
D. Both of the above.
12.7
Residual Analysis
12.7.1
Residual Analysis Methods
The residuals are deﬁned to be
ei = yi −ˆyi,
1 ≤i ≤n
so that they are the differences between the observed values of the dependent variable yi and
the corresponding ﬁtted values ˆyi. A property of these residuals is that they sum to 0
n

i=1
ei = 0
(as long as the intercept parameter β0 has not been removed from the model). The analysis
of the residuals is an important tool for checking whether the ﬁtted model is a good model
and whether the modeling assumptions appear valid. This residual analysis can be used in
conjunction with a visual examination of the data points and the ﬁtted regression line for
the simple linear regression problem discussed in this chapter. However, residual analysis
becomes a much more important tool for the more complicated models and experimental
designs discussed in Chapters 13 and 14.
Residual analysis can be used to
■
identify data points that are outliers
■
check whether the ﬁtted model is appropriate
■
check whether the error variance is constant
■
check whether the error terms are normally distributed
A basic residual analysis technique for the simple linear regression problem is to plot the
residuals ei against the values of the explanatory variable xi. Ideally, a nice random scatter
plot such as the one in Figure 12.38 should be obtained, and if this is the case then there
are no indications of any problems with the regression analysis. However, any patterns in the
residual plot or any residuals with a large absolute value alert the experimenter to possible
problems with the ﬁtted regression model.

586
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
Residuals 0.0
ei
x
+
++
+ +
+
+++
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
+
+ +
+
++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
FIGURE 12.38
Residual plot showing random scatter and no patterns
0.0
x
1.0
2.0
3.0
−1.0
−2.0
−3.0
Outlier
Outlier
ei
ˆσ
+
+
+
+ +
+
+++
+
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+ +
+
++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
FIGURE 12.39
Residual plot indicating points that may be outliers
A data point (xi, yi) can be considered to be an outlier if it does not appear to be pre-
dicted well by the ﬁtted model. Such data points lie far away from the ﬁtted regression line
relative to the other data points, and consequently their residuals have a large absolute value,
as indicated in Figure 12.39. A convenient rule of thumb is to divide the residual by the
estimated error standard deviation ˆσ, and to consider the data point to be a possible outlier
if this “standardized residual” is larger in absolute value than 3. Computer packages also
usually provide standardized residuals calculated from a more complicated but approximately
equivalent formula, which is discussed in Section 13.3.1.
If a data point is identiﬁed as being a possible outlier, then it should be investigated further.
An explanation may indicate that it should not be modeled together with the other data points
and, if this is the case, it should then be removed from the data set and the regression line
should be ﬁtted again to the reduced data set.
If the residual plot shows positive and negative residuals grouped together as in Fig-
ure 12.40, then a linear model is not appropriate. As Figure 12.40 indicates, a nonlinear model
is needed for such a data set. Also, if the residual plot shows a “funnel shape” as in Fig-
ure 12.41, so that the size of the residuals depends upon the value of the explanatory variable
x, then the assumption of a constant error variance σ 2 is not valid.
The discovery of a nonconstant error variance may be interesting to the experimenter
since it indicates that the variability in the distribution of the dependent variable y changes
at different values of the explanatory variable x. This may be useful information about the
variables under consideration. In practice, the ﬁtted regression line and statistical inferences
discussed in this chapter, which are based upon the assumption of a constant error variance,
still provide fairly sensible results even if there is some variation in the error variance, although
a more accurate analysis may be available using a weighted least squares approach.
Finally, a normal probability plot (also referred to as a normal scores plot) of the
residuals can be used to check whether the error terms ϵi appear to be normally distributed.
In such a plot, the residuals ei are plotted against their “normal scores,” which can be thought
of as being their “expected values” if they have a normal distribution. Speciﬁcally, the normal
score of the ith smallest residual is
	−1

i −3
8
n + 1
4
	

12.7 RESIDUAL ANALYSIS
587
x
+
+ +
+
+ +
+ ++ +
+ +
+ +
+
++
+
++
+
+
x
+ ++
+
+
+
+
+
+
+
+
+
+
++
++
++
+ +
+
+
+
+
+
Positive
Negative
Negative
y = ˆβ0 + ˆβ1x
Residuals
ei
y
+
+
+
+
+
+
+
+
FIGURE 12.40
A grouping of positive and negative residuals
indicates that the linear model is inappropriate
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Residuals
ei
x
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Residuals
ei
x
FIGURE 12.41
A funnel shape in the residual plot indicates a
nonconstant error variance
Normal scores
0.0
−2.5
2.5
FIGURE 12.42
A normal scores plot of a simulated sample from a normal
distribution, which shows the points lying approximately on a
straight line
+
Normal scores
0.0
−2.5
2.5
Normal scores
0.0
−2.5
2.5
Uniform
distribution
Exponential
distribution
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
FIGURE 12.43
Normal scores plots of simulated samples from nonnormal
distributions, which show nonlinear patterns
However, computer packages generally provide normal probability plots and it is not necessary
to construct the normal scores oneself. If the main body of the points in a normal probability
plot lie approximately on a straight line as in Figure 12.42, then it is reasonable that the
corresponding data points are observations from a normal distribution. Any obvious difference
from a straight line in the normal probability plot, such as in Figure 12.43, indicates that
the distribution is not normal.

588
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.44
Plot of the standardized residuals
for the Nile River inﬂows data set
January inflow
Standardized residual
7
6
5
4
3
2
1
4
3
2
1
0
–1
–2
–3
If the normal probability plot of the residuals exhibits a straight line, then the residuals can
be taken to be observations from a normal distribution This in turn indicates that there is no
reason to question the modeling assumption that the error terms ϵi are normally distributed. A
lack of normality in the error terms suggests that the experimenter should be cautious about
the ﬁtted model and perhaps should adopt a different modeling approach. This may involve a
variable transformation, some examples of which are discussed in Section 12.8.
12.7.2
Examples
Example 68
Nile River Flowrate
Figure 12.44 shows a plot of ei/ ˆσ against the January inﬂow xi. This residual plot raises a
couple of points, but overall it indicates that the regression modeling appears to have been
performed satisfactorily.
There appears to be at least one outlier. In 1987 the January inﬂow was x = 3.88 billion
cubic meters while the February inﬂow was y = 4.01 billion cubic meters. For this January
inﬂow the predicted February inﬂow is
ˆy|5 = −0.470 + (0.836 × 3.88) = 2.77
so that the residual is
ei = yi −ˆyi = 4.01 −2.77 = 1.24
This gives
ei
ˆσ =
1.24
√
0.1092
= 3.75
which is large enough to warrant investigation. Also, in 1968 the January inﬂow was x = 6.13
billion cubic meters while the February inﬂow was y = 5.67 billion cubic meters, which gives
a residual of
ei = yi −ˆyi = 5.67 −(−0.470 + (0.836 × 6.13)) = 1.02
and
ei
ˆσ =
1.02
√
0.1092
= 3.07

12.7 RESIDUAL ANALYSIS
589
FIGURE 12.45
Normal probability plot of the
standardized residuals for the Nile
River inﬂows data set
Standardized residual
Percent
4
3
2
1
0
–1
–2
–3
–4
99.9
99
95
90
80
70
60
50
40
30
20
10
5
1
0.1
The dam engineers may wish to remove these two data points from the data set and to ﬁt a
regression line to the remaining 113 data points, although this should not affect the results
very much.
The residual plot also suggests that there is possibly a very slight increase in the error
variance as the value of the January inﬂow increases. This would imply that there is a slightly
greater variability in the distribution of the February inﬂows when the expected February
inﬂow is large, which seems quite a reasonable phenomenon.
Finally, the normal probability plot of the residuals in Figure 12.45 (in which the y-axis
shows the cumulative distribution function of the normal scores) exhibits a fairly straight line
and so there is no problem with the normality assumption for the error terms. Notice that
the two top right points in the normal probability plot correspond to the two possible outliers
identiﬁed above.
12.7.3
Problems
12.7.1 Oil Well Drilling Costs
Consider the data set of oil well costs given in DS 12.2.1.
Plot the residuals against the depth of the oil wells. Are
there any points that are possible outliers? Does the
residual plot have any patterns that suggest that the ﬁtted
regression model is not appropriate?
12.7.2 Truck Unloading Times
Consider the data set of the times taken to unload a truck at a
warehouse given in DS 12.2.2. Plot the residuals against the
temperature. Are there any points that might be considered
to be outliers? Does the residual plot have any patterns that
suggest that the ﬁtted regression model is not appropriate?
Construct and interpret a normal probability plot.
12.7.3 VO2-max Aerobic Fitness Measurements
Consider the data set of aerobic ﬁtness measurements
given in DS 12.2.3. Plot the residuals against the ages of
the participants. Are there any obvious outliers? Does the
residual plot cause you to have any misgivings about the
regression analysis?
12.7.4 Property Tax Appraisals
Consider the data set of appraised house values given in
DS 12.2.4. Plot the residuals against the house sizes. Are
there any points that are obvious outliers? Construct and
interpret a normal scores plot. What action do you think
should be taken?
12.7.5 Management of Computer Systems
Consider the data set of the times taken for programming
changes given in DS 12.2.5. Plot the residuals against the
estimated times. What do you notice? Why do you think
that this is?

590
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
12.7.6 Vacuum Transducer Bobbin Resistances
Consider the data set of vacuum transducer bobbin
resistances given in DS 12.2.6. Plot the residuals against
the temperature. Are there any points that are possible
outliers? Does the residual plot have any patterns that
suggest that the ﬁtted regression model is not
appropriate?
12.7.7 In simple linear regression:
A. Data points that are above the line have positive
residuals while data points below the line have
negative residuals.
B. Data points with a very large positive residual or a
very large negative residual may be considered
outliers.
C. Neither of the above.
D. Both of the above.
12.8
Variable Transformations
12.8.1
Intrinsically Linear Models
At ﬁrst appearance it may seem restrictive to be able to ﬁt only straight lines to data sets.
However, many nonlinear relationships can be transformed into a linear relationship by trans-
formations of one or both of the variables. The simple linear regression methods discussed in
this chapter can then be used to ﬁt a straight line to the transformed data values, and the ﬁtted
model can then be transformed back into the original variables.
Typically, transformations based on logarithms and reciprocals are the most useful, as
illustrated in Figure 12.46. For example, the exponential model
y = γ0eγ1x
for some parameter values γ0 and γ1 can be transformed to a linear format by taking logarithms
of both sides
ln(y) = ln(γ0) + γ1x
A linear regression model can then be ﬁtted to the data points (xi, ln(yi)). In the ﬁtted linear
model the slope parameter β1 corresponds to γ1, and the intercept parameter β0 corresponds
to ln(γ0).
When models are transformed into a linear format, it should be remembered that the usual
error assumptions are required for the linear relationship to which the linear regression model
is ﬁtted. Thus, for the exponential model example it is assumed that
ln(yi) = ln(γ0) + γ1xi + ϵi
where the error terms ϵi are independent observations from a N(0, σ 2) distribution. This
assumption can be checked in the usual manner through residual analysis. However, notice
that in terms of the original exponential model it implies that
y = γ0eγ1xϵ∗
i
so that there are multiplicative error terms ϵ∗
i = eϵi . Also, inferences on the original parameters
γi must be made via corresponding inferences on the linear parameters βi, as illustrated in the
example in Section 12.8.2.
A model that can be transformed into a linear relationship is known as an intrinsically
linear model. The extension of simple linear regression modeling techniques to intrinsically
linear models makes it a general and useful modeling procedure. There are some models,
however, that cannot be transformed into a linear format.
These models, such as
y = γ0 + eγ1x
for example, are not intrinsically linear and need to be handled with nonlinear regression
techniques, which are discussed in Section 13.5.

γ0 > 0, γ1 > 0
γ0 > 0, 0 < γ1 < 1
γ0 > 0, γ1 > 1
γ0
γ0 > 0, γ1 < 0
1γ1
Model: y = γ0 + γ1
x
Linear format: y = γ0 + γ1 1
x
γ0
γ0 > 0, γ1 > 0
γ0 > 0, γ1 < 0
γ0 > 0, γ1 > 0
γ0 > 0, γ1 < 0
Model: y = γ0eγ1x
Linear format: ln y = ln γ0 + γ1 x
Model: y = γ0xγ1
Linear format: ln y = ln γ0 + γ1
x
ln
Linear format: 1y = γ1 + γ0 1x
Model: y =
x
γ1x
γ0 +
x
y
x
y
γ0
x
y
x
y
x
y
x
y
γ0
x
y
x
y
FIGURE 12.46
Some common intrinsically linear models
591

592
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
12.8.2
Example
Example 63
Roadway Base
Aggregates
When an aggregate material is subjected to different stress levels, the resilient modulus of
the aggregate calculated at each stress level exhibits a nonlinear pattern of values. This can
be seen from the plot in Figure 12.47 of n = 16 resilient modulus measurements Mi of an
aggregate made at different values of the bulk stress variable θi. The data set indicates that
there is a nonlinear relationship between the resilient modulus and the bulk stress, and it is
clearly inappropriate to ﬁt a straight line to the data set.
However, the intrinsically linear model
M = γ0θγ1
FIGURE 12.47
Nonlinear transformation for
roadway base aggregates example
log (bulk stress)
log (resilient modulus)
4.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Bulk stress
Resilient modulus
80
70
60
50
40
30
20
10
0
20000
18000
16000
14000
12000
10000
9.9
9.8
9.7
9.6
9.5
9.4
9.3
9.2
9.1

12.8 VARIABLE TRANSFORMATIONS
593
FIGURE 12.47 (continued)
Nonlinear transformation for
roadway base aggregates example
Bulk stress
Resilient modulus
80
70
60
50
40
30
20
10
0
20000
18000
16000
14000
12000
10000
with two unknown parameters γ0 and γ1 is commonly used for this problem. With a logarithmic
transformation, the linear relationship
y = β0 + β1x
is obtained with y = ln(M), x = ln(θ), β0 = ln(γ0), and β1 = γ1. The transformed variables
yi = ln(Mi) and xi = ln(θi) are plotted in Figure 12.47, and they exhibit a linear relationship,
which conﬁrms that this model is appropriate.
A straight line is ﬁtted to the transformed data points (xi, yi) and the ﬁtted regression line
is
y = 8.761 + 0.2494x
This equation can be transformed back into a model in terms of the original variables
M = 6380 θ0.2494
which is shown together with the original data points in Figure 12.47.
The standard error of ˆβ1 is 0.0122, and so with t0.025,14 = 2.145, a 95% conﬁdence interval
for β1 = γ1 is
β1 = γ1 ∈(0.2494 −(2.145 × 0.0122), 0.2494 + (2.145 × 0.0122))
= (0.223, 0.276)
Also, the standard error of ˆβ0 is 0.04175, so that a 95% conﬁdence interval for β0 is
β0 ∈(8.761 −(2.145 × 0.04175), 8.761 + (2.145 × 0.04175))
= (8.67, 8.85)
Therefore with γ0 = eβ0, a 95% conﬁdence interval for γ0 is
γ0 ∈(e8.67, e8.85) = (5830, 6980)

594
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
12.8.3
Problems
12.8.1 Make a plot of the data set given in DS 12.8.1. What
intrinsically linear function should provide a good model
for this data set? What transformation of the variables is
needed? Fit a straight line to the transformed variables
and write the ﬁtted model back in terms of the original
variables. What is the predicted value of the dependent
variable y when x = 2.0?
12.8.2 Repeat Problem 12.8.1 for the data set given in DS 12.8.2.
12.8.3 Cell Growth
A bioengineer measures the growth rate of a substance by
counting the number of cells N present at various times t
as shown in DS 12.8.3. Fit the model
N = γ0eγ1t
and calculate two-sided 95% conﬁdence intervals for the
unknown parameters γ0 and γ1.
12.8.4 Synthetic Human Arteries
In an experiment to investigate the suitability of using a
silicone tube to model the behavior of a human artery, the
data set in DS 12.8.4 is collected, which relates the
pressure differential P across the walls of the tube to the
cross-sectional area A of the tube.
(a) Show that the model
P = γ0 Aγ1
appears to provide a good ﬁt to the data set.
(b) Make a suitable transformation of the variables and
ﬁnd point estimates for γ0 and γ1.
(c) Calculate two-sided 95% conﬁdence intervals for γ0
and γ1.
12.8.5 An experimenter has data on the yield and the temperature
of a chemical process and wishes to ﬁt the model
yield = γ0eγ1temperature
A linear regression model is ﬁtted to the data
y = ln(yield) and x = temperature, with the results
n = 25, ˆβ0 = 2.628, ˆβ1 = 0.341, and s.e. (ˆβ1) = 0.025.
Find ˆγ0, ˆγ1, and calculate a 95% conﬁdence interval for
γ1.
12.8.6 Explain how simple linear regression can be used to ﬁt
the model ey/γ0 = γ1/x2. How would you ﬁnd the
parameter estimates ˆγ0 and ˆγ1?
12.8.7 Strengths of Cracked Plastic Compounds
A crack in a plastic compound affects the strength of the
material. In order to investigate the relationship between
the strength and the length of a crack, an experiment was
conducted where cracked pieces of the plastic compound
were subjected to increasing loads until the point at which
they broke apart. DS 12.8.5 has the data set of the
breakage loads and the lengths of the cracks. Show that a
linear regression model with breakage load as the
dependent variable and crack length as the explanatory
variable is unsatisfactory because the residuals are ﬁrst
positive, then negative, and then positive again. Show that
a better ﬁt can be obtained with the model
breakage load = γ0eγ1×(crack length)
Using this model, what is the expected breaking load
when the crack length is 2.1?
12.9
Correlation Analysis
12.9.1
The Sample Correlation Coefﬁcient
Recall that the correlation
ρ = Corr(X, Y) =
Cov(X, Y)
√Var(X)Var(Y)
measures the strength of linear association between two jointly distributed random variables
X and Y (see Section 2.5.4). Given a set of paired data observations
(xi, yi)
1 ≤i ≤n

12.9 CORRELATION ANALYSIS
595
from this joint distribution, the correlation can be estimated by the sample correlation coef-
ﬁcient (also known as the Pearson product moment correlation coefﬁcient)
r =
SXY
√SX X
√SYY
=
n
i=1(xi −¯x)(yi −¯y)
n
i=1(xi −¯x)2n
i=1(yi −¯y)2
=
n
i=1 xi yi −n ¯x ¯y

n
i=1 x2
i −n ¯x2

n
i=1 y2
i −n ¯y2
This sample correlation coefﬁcient r takes a value between −1 and 1 and, as Figure 12.48
illustrates, it measures the strength of the linear association exhibited by the data points. A
sample correlation coefﬁcient r = 0 indicates that there is no linear association between the
two variables, and their distributions can then be thought of as being independent of each other.
A positive sample correlation coefﬁcient r > 0 indicates that there is a positive association
between the two variables, so that as one variable increases there is a tendency for the other
variable to increase as well. On the other hand, a negative sample correlation coefﬁcient r < 0
indicates that there is a negative association between the two variables, in which case as one
variable increases there is a tendency for the other variable to decrease.
The closer the sample correlation coefﬁcient is to either 1 or −1, the stronger is the linear
association. This happens as the data points lie closer to a straight line. Technically, a sample
correlation coefﬁcient of r = 1 is obtained when the data points lie on a straight line with
an upward slope, and a sample correlation coefﬁcient of r = −1 is obtained when the data
points lie on a straight line with a downward slope.
Correlation
of 0.9
Correlation
of 0.9
Correlation
of 0.3
Correlation
of 0.3
Correlation
of 0.6
Correlation
of 0.6
Correlation
of 0
Correlation
of 0
Correlation
of –0.3
Correlation
of –0.3
Correlation
of –0.6
Correlation
of –0.6
Correlation
of –0.9
Correlation
of –0.9
FIGURE 12.48
Illustrations of the sample
correlation coefﬁcient r
The sample correlation coefﬁcient is clearly related to a regression line ﬁtted to the data
points. In fact, since
ˆβ1 = SXY
SX X
it follows that
r = ˆβ1

SX X
SYY
so that the sample correlation coefﬁcient is simply a scaled version of the estimated slope
parameter ˆβ1. Moreover, it turns out that
r2 = R2
so that the square of the sample correlation coefﬁcient is equal to the coefﬁcient of determi-
nation of the regression model.
The sample correlation coefﬁcient is a convenient way of summarizing the degree of
association between two variables, whereas a more detailed regression analysis actually forms
a linear model relating the two variables. Notice that the sample correlation coefﬁcient is
unchanged if the x and y variables are interchanged and are relabeled the y and x variables.
This is in contrast to a regression analysis, which requires one of the variables to be designated
the dependent variable and one the explanatory variable. In addition, the sample correlation
coefﬁcient is not affected (except for a possible sign change) by any linear transformations of
the variables, so that if
x∗
i = axi + b
and
y∗
i = cyi + d
for some constants a, b, c, and d, then the sample correlation coefﬁcient of the data points
(xi, yi) is equal to the sample correlation coefﬁcient of the data points (x∗
i , y∗
i ) if the constants

596
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
a and c are either both positive or both negative, and the two sample correlation coefﬁcients
are of the same magnitude but of different signs if the constants a and c have different
signs.
A correlation ρ = 0 is particularly interesting since it implies that there is no linear
association between the two variables, and consequently it is useful to be able to test the null
hypothesis
H0 : ρ = 0
against a two-sided alternative. Under the assumption that the X and Y random variables
have a bivariate normal distribution, this hypothesis test can be performed by comparing the
t-statistic
t = r√n −2
√
1 −r2
with a t-distribution with n −2 degrees of freedom. However, this is entirely equivalent to
testing the null hypothesis
H0 : β1 = 0
with the t-statistic
t =
ˆβ1
s.e.( ˆβ1) =
ˆβ1
√SX X
ˆσ
since it can be shown that the two t-statistics are identical. Recall that this test is also equivalent
to the test performed in the analysis of variance table for a regression problem, and so these
are in fact three ways of performing the same test. However, the t-statistic written in terms
of the sample correlation coefﬁcient r is useful when only a correlation analysis is of interest
rather than a full regression analysis.
It is important that an experimenter examines a scatter plot of the data points (xi, yi)
before calculating a sample correlation coefﬁcient since any nonlinear relationship between
the two variables can render the sample correlation coefﬁcient meaningless. This is because
the sample correlation coefﬁcient r is designed to measure the amount of linear association
between two variables. Figure 12.49 shows how two variables with a nonlinear relationship
might have a sample correlation coefﬁcient close to 0, which is misleading in this case because
there is a relationship between the two variables.
Association and Causality
A common pitfall in data interpretation is to mistake association
for causality. If two variables are shown to be correlated, this does not in itself establish that
there is a causal relationship between the two variables. It may, however, suggest that there
FIGURE 12.49
Misleading sample correlation
coefﬁcient for a nonlinear
relationship
+ + +
+
+ +
+ ++ +
+ +
+ +
+
++
+
++
+
+
+
+
y = ˆβ0 + ˆβ1x
The sample correlation
coefficient r may be close to 0
x
y

12.9 CORRELATION ANALYSIS
597
Sample Correlation Coefﬁcient
The sample correlation coefﬁcient r for a set of paired data observations (xi, yi) is
r =
SXY
√SX X
√SYY
=
n
i=1(xi −¯x)(yi −¯y)
n
i=1(xi −¯x)2n
i=1(yi −¯y)2
=
n
i=1 xi yi −n ¯x ¯y

n
i=1 x2
i −n ¯x2

n
i=1 y2
i −n ¯y2
It measures the strength of linear association between two variables and can be
thought of as an estimate of the correlation ρ between the two associated random
variables X and Y.
Under the assumption that the X and Y random variables have a bivariate normal
distribution, a test of the null hypothesis
H0 : ρ = 0
can be performed by comparing the t-statistic
t = r√n −2
√
1 −r2
with a t-distribution with n −2 degrees of freedom. In a regression framework, this
test is equivalent to testing H0 : β1 = 0.
is a causal relationship between the two variables and motivate scientists to try to identify
a mechanism for a causal relationship. Nevertheless, statistical data analysis in itself cannot
establish causality.
For example, suppose that after a party some of the participants fall seriously ill. A doctor
interviews all of the people who attended the party and ﬁnds out how ill they are and how much
wine they consumed. Analysis of these data reveals a positive correlation between the level
of illness and the amount of wine consumed. Does this prove that the wine was responsible
for the illness?
The data analysis certainly suggests that the wine may be responsible for the illness, or
at least may be a contributing factor. However, there are other possibilities. Suppose that the
salted peanuts at the party are the real cause of the illness. Consequently, the more peanuts a
person consumed, the more ill that person is likely to be, so that there is a causal relationship
betweenpeanutsandillnesswithaconsequentpositiveassociation.Also,supposethatthemore
peanuts a person consumed, the thirstier the person is and so the greater is the person’s wine
consumption. Consequently, there is also a positive association between peanut consumption
and wine consumption.
As Figure 12.50 illustrates, this scenario explains the positive association between wine
consumption and illness, even though wine consumption in itself has nothing to do with
the illness. In fact, conditional on the amount of peanuts consumed, wine consumption has
nothing to do with illness. Therefore, even though there is a positive correlation between
wine consumption and illness, it would be incorrect to use this result to infer that the wine
consumption caused the illness. This example illustrates how the positive correlation ex-
hibited between wine consumption and illness can be explained by the presence of a third
variable, in this case peanuts, which is positively correlated with both wine consumption and
illness.

598
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.50
A positive correlation between
wine consumption and illness can
be generated by their relationships
with peanut consumption
Positive association
between wine consumption
and illness
Peanut consumption
          causes
wine consumption
Peanuts
Peanut consumption
causes illness
Wine
 Illness+
12.9.2
Examples
Example 69
Cranial
Circumferences
The data set of ﬁnger lengths and cranial circumferences in Figure 12.22 gives
SYY = 99.4574
SXY = 3.10745
SX X = 1.489505
The sample correlation coefﬁcient is therefore
r =
SXY
√SX X
√SYY
=
3.10745
√
1.489505 ×
√
99.4574
= 0.255
This implies that the coefﬁcient of determination for a regression analysis is R2 = r2 = 0.065.
The t-statistic for testing the null hypothesis H0 : ρ = 0 is
t = r√n −2
√
1 −r2 = 0.255 ×
√
18
√
1 −0.2552 = 1.12
which, as expected, is equal to the t-statistic
t =
ˆβ1
s.e.( ˆβ1)
obtained in Section 12.3, so that, as before, the p-value is 0.277. Consequently, it is plausible
that the ﬁnger length and the cranial circumference are uncorrelated.
Example 68
Nile River Flowrate
For this data set the sample correlation coefﬁcient is
r =
√
R2 =
√
0.871 = 0.933
which conﬁrms the strong positive linear association between the January and February
inﬂows.

12.9 CORRELATION ANALYSIS
599
Example 44
Army Physical Fitness
Test
For this data set the sample correlation coefﬁcient is
r = −
√
R2 = −
√
0.3228 = −0.568
with the number of pushups and the 2-mile run time being negatively correlated.
COMPUTER NOTE
Fortunately, the number crunching involved in a regression analysis can all be performed
by a computer package and you need only enter the data into the package and specify the
model that you want to ﬁt. The computer will furnish you with the ﬁtted regression line,
p-values for the parameters, an analysis of variance table, coefﬁcients of determination and
correlation, and often much more additional information. Upon request, some packages will
also compute conﬁdence intervals for the regression line at speciﬁc values of the explanatory
variable and prediction intervals for future response values. Some computer packages will
draw conﬁdence bands around the ﬁtted regression line. Residuals and standardized residuals
can also be obtained, and possible outliers are often brought to your attention.
The key to being a successful modeler is to make sure that you have a good understanding
of what the computer output means. In addition, remember that one of the worst things that
you can do is to haphazardly ﬁt meaningless straight lines to data sets that exhibit a nonlinear
relationship, so make good use of your package’s graphics facilities to get a good feel for the
data with which you are dealing.
12.9.3
Problems
12.9.1 Show that if the data points lie on a straight line with
yi = a + bxi
1 ≤i ≤n
then the sample correlation coefﬁcient r is equal to 1 if
b > 0 and is equal to −1 if b < 0.
12.9.2 Show that if
x∗
i = axi + b
and
y∗
i = cyi + d
for some constants a, b, c, and d, then the sample
correlation coefﬁcient of the data points (xi, yi) is equal
to the sample correlation coefﬁcient of the data points
(x∗
i , y∗
i ) except for a possible sign change.
For the data sets in Problems 12.9.3 through 12.9.9, what is
the sample correlation coefﬁcient r? Show that the t-statistic
written in terms of the sample correlation coefﬁcient
t =r√n −2/
√
1 −r2 is equal to the t-statistic t = ˆβ1/s.e.( ˆβ1)
calculated earlier.
12.9.3 Oil Well Drilling Costs
The data set of oil well costs given in DS 12.2.1.
12.9.4 Truck Unloading Times
The data set of the times taken to unload a truck at a
warehouse given in DS 12.2.2.
12.9.5 VO2-max Aerobic Fitness Measurements
The data set of aerobic ﬁtness measurements given in
DS 12.2.3.
12.9.6 Property Tax Appraisals
The data set of appraised house values given in
DS 12.2.4.
12.9.7 Management of Computer Systems
The data set of the times taken for programming changes
given in DS 12.2.5.
12.9.8 Vacuum Transducer Bobbin Resistances
The data set of vacuum transducer bobbin resistances
given in DS 12.2.6.
12.9.9 Ceramic Baking Procedures
The data set of ceramic densities and baking times given
in DS 12.3.1
12.9.10 The sample correlation coefﬁcient between y and x is
0.3. If a simple linear regression is performed between y
and x, what can you say about the sign (positive, 0, or
negative) of the estimated slope parameter? What can
you say about the p-value for testing whether the slope
parameter is signiﬁcant?
12.9.11 Suppose that variables A and B have a positive
correlation. Provide a possible explanation for the claim
that changes in B do not cause a change in A.
12.9.12 In simple linear regression, the sample correlation
coefﬁcient between the input variable and the

600
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
output variable, and the estimated slope parameter
ˆβ1:
A. Must have the same sign (negative, zero, or positive)
B. May have opposite signs
C. Must have opposite signs
D. Neither of the above
12.9.13 Suppose that sales staff with more years on the job tend
to have greater sales due to their additional experience.
Also, suppose that greater bonus rewards are awarded to
those sales staff who have the greatest sales. Which of
the following is true?
A. There is an association between years worked and
bonus amount, with those workers with greater years
worked receiving greater bonus amounts.
B. There is a causal relationship between years worked
and bonus amount because the company bases the
bonus decisions on the amount of experience.
C. Both of the above.
D. Neither of the above.
12.9.14 If the correlation between two variables is −0.7, then:
A. As one variable increases, the other variable tends to
increase.
B. As one variable increases, the other variable tends to
decrease.
C. As one variable increases, there is no tendency for
the other variable to change.
D. Neither of the above.
12.10
Case Study: Microelectronic Solder Joints
The researcher is interested in whether the heights of the solder joints have any inﬂuence on
the reliability of the microelectronic assembly. Assemblies are constructed that are identical
except for differences in the solder joint heights. All together, 3 assemblies are made with a
height of 0.540 mm, 3 assemblies are made with a height of 0.555 mm, 3 assemblies are made
with a height of 0.570 mm, and 3 assemblies are made with a height of 0.585 mm. These 12
assemblies are then subjected to temperature cycles, and after every 100 cycles they are tested
to see whether they have failed. The resulting data set of the number of temperature cycles
until failure is shown in Figure 12.51.
A graph of the data is shown in Figure 12.52 together with the ﬁtted regression line that is
cycles = −983 + (9333 × height)
Solder joint
Temperature cycles
height
to failure
0.540
4300
0.540
3800
0.540
4200
0.555
4600
0.555
3700
0.555
4100
0.570
4400
0.570
4100
0.570
4500
0.585
4800
0.585
4000
0.585
4700
FIGURE 12.51
Data set of solder joint heights (mm)
and temperature cycles to failure
Height of solder joint
Temperature 
cycles to failure
0.58
0.57
0.56
0.55
0.54
4800
4600
4400
4200
4000
3600
3800
0.59
FIGURE 12.52
Fitted regression line for temperature cycles to failure and solder joint height data set

12.11 CASE STUDY: INTERNET MARKETING
601
The positive slope of the ﬁtted regression line seems to suggest that the reliability of the
assemblies improves as the solder joint heights increase, but the researcher knows that the
statistical signiﬁcance of the regression line should be examined. The slope parameter has a
standard error of 5532, so the t-statistic for testing whether the slope is 0 is
t =
ˆβ1
s.e.(ˆβ1) = 9333
5532 = 1.69
This gives a p-value of 2 × P(t10 ≥1.69) = 0.122, and since this is larger than 10%, the
researcher concludes that the regression is not signiﬁcant and that this data set does not provide
sufﬁcient evidence to establish that changing the solder joint heights in this range has any
effect on the reliability of the assemblies. However, the researcher is aware that this analysis
has not proved that there is no effect and that collecting further data may provide evidence of
a signiﬁcant relationship between reliability and the solder joint heights.
12.11
Case Study: Internet Marketing
While the organization runs various advertising campaigns it tracks the number of visitors to its
website. Figure 12.53 shows the costs of 20 advertising campaigns and the number of website
visitors during each campaign. A graph of the data reveals curvature in the relationship, and
so a model of the form
y = γ0 + γ1
x
is tried where y is “website visitors” and x is “advertising cost.” Consequently, a linear
regression model is ﬁtted with “website visitors” as the output variable and the reciprocal of
“advertising cost” as the input variable, which gives
y = 814,550 −4.9067 × 1010
x
with a very small p-value for the input variable. The data and model are shown in Figure 12.54,
and it can be seen that there is little practical gain in website visitors for expenditures over
about $400,000.
FIGURE 12.53
Data set of advertising costs and
website visits
Advertising cost
Website visits
113,000
399,559
338,000
710,902
152,000
525,560
222,000
538,010
498,000
711,648
482,000
714,320
222,000
575,451
131,000
435,848
258,000
660,760
148,000
481,615
459,000
741,965
488,000
695,132
431,000
736,889
293,000
660,656
436,000
706,204
38,0000
680,245
22,0000
584,819
311,000
631,668
184,000
533,302
267,000
563,944

602
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
FIGURE 12.54
Relationship between website visits
and advertising costs
Advertising cost ($)
Website visits
500,000
400,000
300,000
200,000
100,000
800,000
700,000
600,000
500,000
400,000
300,000
12.12
Supplementary Problems
12.12.1 SAT Scores and Exam Completion Times
DS 12.12.1 gives the times taken by a class of n = 14
students to ﬁnish a math test together with the math
SAT scores of the students. Perform the following
linear regression analysis with time as the dependent
variable and SAT as the explanatory variable.
(a) Calculate the point estimates ˆβ0, ˆβ1, and ˆσ 2.
(b) Calculate the sample correlation coefﬁcient r.
(c) Calculate a two-sided 95% conﬁdence interval for
the slope parameter β1.
(d) Is there any evidence that the time taken to ﬁnish
the test depends upon the SAT score?
(e) How does the predicted time taken to ﬁnish the test
change as the SAT score increases by 10 points?
(f) What is the predicted time for a student with an
SAT score of 550? Construct a two-sided 95%
conﬁdence interval for the expected time for a
student with an SAT score of 550. Construct a
two-sided 95% prediction interval for the time
taken by a student with an SAT score of 550.
(g) Plot the residuals against the SAT scores. Do there
appear to be any problems with the regression anal-
ysis? Are there any points that are obvious outliers?
Obtain a normal scores plot of the residuals.
12.12.2 Rolling Mill Scrap
The data used in Example 43 are taken from the data
set given in DS 12.12.2, which relates the variable %
scrap to the number of times that the metal plate is
passed through the rollers. It is useful to be able to
model the amount of discarded metal, % scrap, as a
function of the number of passes through the rollers.
Perform a linear regression analysis with % scrap as
the dependent variable and the number of passes
through the rollers as the explanatory variable.
(a) Fit the regression line and estimate the error
variance.
(b) Calculate the sample correlation coefﬁcient r.
(c) What is the evidence that the amount of scrap
material depends upon the number of passes?
(d) Calculate a two-sided 95% conﬁdence interval for
the slope parameter β1.
(e) How does the predicted amount of scrap change
with each extra pass through the rollers?
(f) What is the predicted amount of scrap after seven
passes through the rollers? Construct a two-sided
95% prediction interval for the amount of scrap
after seven passes of a particular sheet through the
rollers.
(g) Plot the residuals against the number of passes.
Which points have values ei/ ˆσ larger than 3 in
absolute value? Do you think that a straight line
regression model is appropriate? Obtain a normal
scores plot of the residuals.
12.12.3 Friction Power Loss from Engine Bearings
DS 12.12.3 contains data from a set of automobile
engines concerning the diameter of the bearing and the

12.12 SUPPLEMENTARY PROBLEMS
603
friction power loss that occurs in the bearing of the
engine. Perform a linear regression analysis with power
loss as the dependent variable and bearing diameter as
the explanatory variable.
Can you conclude that there is a signiﬁcant
association between power loss and bearing diameter?
What is the sample correlation coefﬁcient? What can
you predict for the power loss of a new engine with a
bearing diameter of 25.0? Are there any data points
with values ei/ ˆσ larger than 3 in absolute value?
12.12.4 Double Pipe Heat Exchanger
A double pipe heat exchanger is a device that allows
energy to be exchanged between a hot liquid and a cold
liquid. It consists of two concentric pipes of different
diameter. The hot liquid ﬂows through the inner pipe,
and the cold liquid ﬂows in the gap between the two
pipes, usually in the opposite direction to the hot liquid.
DS 12.12.4 contains some data obtained from these
heat exchangers, listing the energy lost by the hot liquid
and the corresponding energy gained by the cold liquid.
These values are not equal because of measurement
errors, because not all of the energy lost by the hot
liquid is aborbed by the cold liquid, and because the
cold liquid can absorb energy from sources other than
the hot liquid. Perform a linear regression analysis with
energy gain as the dependent variable and energy loss
as the explanatory variable. What is the ﬁtted model
and what is the sample correlation coefﬁcient? What
can you predict for the energy gained by the cold liquid
when the energy lost by the hot liquid is 500?
12.12.5 Capacitance Discharge Pulse Times
Electrical capacitors can be used to deliver
short high-voltage pulses. As the capacitor discharges,
the time span of the resulting pulse is an important
property. DS 12.12.5 contains measurements of the
time spans of the discharges, in milliseconds, obtained
for different capacitance values, in microfarads.
Perform a linear regression analysis with pulse
time as the dependent variable and capacitance as the
explanatory variable. Is there a signiﬁcant association
between the pulse time and the capacitance value? What
is the ﬁtted model and what is the sample correlation
coefﬁcient? What can you predict for the pulse time
of a discharge from a capacitance of 1700 microfarads?
Which point has the largest residual in absolute value?
12.12.6 Deformability of Arteries with Atherosclerosis
DS 12.12.6 contains the circumferential strains S and
lesion areas L of n = 12 specimens of human arteries
with atherosclerosis. This disease is commonly
referred to as “hardening of the arteries” due to
the accumulation of plaque on the artery walls. The
lesion area measures the amount of plaque that has
accumulated on the artery walls and can be thought of
as the disease severity. The circumferential strain is a
measure of the deformability of the artery tissue. A
model that relates the circumferential strain S to the
lesion area L is helpful in assessing the potential
impact of various levels of atherosclerosis.
(a) Show that the model
S = γ0 + γ1
L
appears to provide a good ﬁt to the data set.
(b) After a suitable transformation of the variables,
ﬁnd point estimates for γ0 and γ1.
(c) Calculate two-sided 95% conﬁdence intervals for
γ0 and γ1.
(d) What is the predicted circumferential strain of an
artery with a lesion area of 10.0 mm2?
12.12.7 Catalyst Effect on Chemical Process
DS 12.12.7 contains data on the measurements of the
strength of a chemical solution for different amounts of
a catalyst. It is decided to perform a linear regression
analysis with strength as the dependent variable and
amount of catalyst as the explanatory variable.
(a) Calculate the values of ˆβ0, ˆβ1, and ˆσ.
(b) Perform a two-sided hypothesis test of H0 : β1 = 0
and interpret the result.
(c) If a chemical solution is made with a catalyst
amount of 21.0, obtain a range of values for the
strength with 95% conﬁdence.
(d) What is the residual for the point (28, 17)?
12.12.8 In a simple linear regression analysis with n = 20 data
points, the estimates ˆβ0 = 123.57, ˆβ1 = −3.90, and
ˆσ = 11.52 are obtained, with 20
i=1 xi = 856,
20
i=1 x2
i = 37,636, 20
i=1 yi = −869, and
20
i=1 y2
i = 55,230.
(a) Construct a two-sided 95% prediction interval for a
future response value when the input value is 40.
(b) Compute the analysis of variance table and
calculate the coefﬁcient of determination R2.
12.12.9 Bacteria Cultures
A simple linear regression model is to be ﬁtted to the
data set of temperature and yield values for bacteria
cultivation in DS 12.12.8, with temperature as the
explanatory variable and yield as the dependent
variable.

604
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
(a) Calculate ˆβ0, ˆβ1, and ˆσ 2.
(b) Is there sufﬁcient evidence to conclude that the
yield depends upon the temperature?
(c) What is the residual for the ﬁrst data point?
12.12.10 A simple linear regression analysis has n = 20 and
R2 = 0.853. What can you say about the p-value for
the two-sided hypothesis test of H0 : β1 = 0?
12.12.11 Gold Extraction from Unprocessed Ore
Consider the data set in DS 12.12.9 from the mining
industry that shows the amount of ore processed in
various locations together with the resulting amount of
gold that was obtained. A linear regression model is
used to analyze the data with the amount of ore
processed as the input variable and with the amount
of gold obtained as the output variable.
(a) Calculate the ﬁtted regression line.
(b) Perform a hypothesis test to assess whether it is a
signiﬁcant regression.
(c) What is the correlation between the amount of ore
processed and the amount of gold obtained?
(d) What is the value of the coefﬁcient of
determination R2?
(e) Calculate a 95% prediction interval for the amount
of gold that will be obtained if 15.0 kilotons of ore
are processed.
(f) Calculate the residual value for each data point.
12.12.12 Are the following statements true or false?
(a) In simple linear regression, the choice of which
variable should be designated as the dependent
variable and which variable should be designated
as the explanatory variable is determined by
considering the correlation between the
variables.
(b) In regression modeling the dependent variable may
be called the output variable.
(c) The standard ﬁtted regression line is obtained from
a least squares ﬁt.
(d) In general, extrapolation should be avoided if
possible, but it is not as risky as interpolation.
(e) Prediction bands are narrowest at the sample
average of the input variable.
(f) A small value of the coefﬁcient of determination
indicates that the regression cannot be statistically
signiﬁcant.
(g) If two variables have a causal relationship, then
they have an association.
(h) If two variables have an association, then they must
have a causal relationship.
(i) A simple linear regression model has three
unknown parameters.
(j) The solution of the “normal equations” provides
the ﬁtted intercept parameter and the ﬁtted slope
parameter.
(k) For very large sample sizes the conﬁdence bands
and the predictions bands will be virtually
indistinguishable.
(l) Not every nonlinear model is intrinsically linear.
(m) The correlation between two variables can
sometimes be reduced by changing the units that
they are measured with.
12.12.13 Downloading Computer Files
The data set in DS 12.12.10 shows the size of some
ﬁles and the times that they took to download from a
website. A linear regression model is used to analyze
the data with size as the input variable and time as the
output variable.
(a) Find ˆβ0, ˆβ1, and ˆσ 2.
(b) Perform the hypothesis test to assess whether it is a
signiﬁcant regression.
(c) What is the expected download time for a ﬁle of
size 6.00?
(d) What is the value of R2?
(e) Calculate a 95% prediction interval for the
downloading time of a ﬁle of size 6.00.
(f) What is the residual for the data point (4.56, 103)?
(g) What is the correlation between size and time?
(h) Can you predict the downloading time of a ﬁle of
size 0.40?
12.12.14 Underwater Speed of Sound
Pressure changes in seawater affect the speed at which
sound is transmitted, and the data set in DS 12.12.11
shows how the underwater speed of sound relates to the
depth at which it is measured. Use a linear regression
model to analyze the data with the depth as the input vari-
able and with the speed of sound as the output variable.
(a) Show how to calculate the ﬁtted regression line.
(b) Show how to calculate the estimate of the error
variance.
(c) Perform a hypothesis test to assess whether it is a
signiﬁcant regression.
(d) Calculate a 95% conﬁdence interval for the speed
of sound at a depth of 4 meters.
(e) What is the value of the coefﬁcient of
determination R2?
The following data sets can be used to practice the regression
techniques discussed in this chapter.

12.12 SUPPLEMENTARY PROBLEMS
605
12.12.15 Oxygen Purity in Chemical Distillation Processes
DS 12.12.12 contains the oxygen purities obtained
from a chemical distillation process when the
percentage of hydrocarbons that are present in the main
condenser is varied.
12.12.16 Cotton Bale Productivities
DS 12.12.13 contains the cotton bales per acre
harvested by a cotton farmer over a 30-year period.
Year to year differences in the weather cause a
substantial amount of variability in the productivity, but
the farmer is interested in whether his attempts to
improve soil fertility and irrigation methods have had a
signiﬁcant effect on his output.
12.12.17 Aerodynamics and Wing Design
DS 12.12.14 contains the wing lift generated in various
trials of an airfoil design where the angle of inclination
of the airfoil to the horizontal (the angle of attack) is
varied over a small range. Can a linear model be
effectively used to predict the wing lift generated by an
angle of attack within this range?
12.12.18 Vapor Pressure Modeling for a Gas
DS 12.12.15 contains the vapor pressures of a gas at
different temperatures (Kelvin). How do you feel
about the linear regression model with temperature as
the input variable and vapor pressure as the output
variable? Theory suggests that within this temperature
range the log of the vapor pressure can be modeled
as a linear function of the reciprocal of the temperature.
Do you prefer this model?
12.12.19 Static Breaking Strengths of Ropes
DS 12.12.16 contains the static breaking strengths of
ropes of different diameters. This tensile strength is
the maximum load that a rope can sustain before
breaking. What model would you recommend for the
relationship between breaking strength and rope
diameter?
12.12.20 Protein Concentrations from Cell Lysing
In a biochemical experiment, various cultures of cells
are grown and the number of cells are counted. Then
the cells are lysed so that they release their contents
and the resulting protein concentration is measured as
given in DS 12.12.17.
12.12.21 Heart Tissue Elasticity
A sample of heart tissue was subjected to different
stresses and the resulting strains are given in
DS 12.12.18. Compare the model
strain = β0 + β1 stress
with the model
strain = γ0 stressγ1
12.12.22 Wind Speeds and Air Pressures of Hurricanes
DS 12.12.19 contains the maximum windspeeds and
the minimum air pressures for the hurricanes and
tropical storms occurring off the Atlantic seaboard of
North America in 2004.
12.12.23 Thorax Width and Dry Mass of Wasps
DS 12.12.20 contains the thorax width and the dry
mass of a random sample of male wasps.
12.12.24 In simple linear regression:
A. An association between two variables may exist
because of a causal relationship.
B. Even though there is an association between two
variables, there may not necessarily be a causal
relationship between them.
C. Both of the above.
D. Neither of the above.
12.12.25 A researcher wants to investigate how the revenue of a
company (a continuous variable) depends upon its size.
A. If a company’s size is classiﬁed as being either
“small,” “medium,” or “large,” then an analysis of
variance can be used.
B. If a company’s size is measured as a continuous
variable, then a simple linear regression model can
be used.
C. Neither of the above.
D. Both of the above.
12.12.26 Carbon Footprints
Analyze the data in DS 12.12.21, which contains
estimates of the pounds of carbon dioxide released
when making several types of car, together with
information on the suggested retail price of the car.
12.12.27 Mining Mill Operations
DS 12.12.22 contains daily data for the mill operations
of a mining company over a period of a month. Each
day, the ore throughput is measured, together with the
carbon concentration in the waste material. Is there any
evidence that the carbon concentration varies according
to the ore throughput?
12.12.28 Mercury Levels in Coal
DS 12.12.23 shows the mercury levels of coal
samples that are taken periodically as the coal is mined
further and further into the seam. Is there any evidence
that the mercury levels are changing through the coal
seam?

606
CHAPTER 12
SIMPLE LINEAR REGRESSION AND CORRELATION
12.12.29 Natural Gas Consumption
DS 12.12.24 contains two data sets. The ﬁrst is the
average daily temperature and the total daily natural
gas consumption for a region for the summer time,
while the second data set is the same variables for
winter time. Examine how the natural gas consumption
depends on the temperature for summer and winter
periods.
12.12.30 In simple linear regression, if the regression model is
not statistically signiﬁcant, then this provides evidence
of a relationship between the two variables.
A. True
B. False
12.12.31 In simple linear regression, the computer output
contains a p-value that can be used to assess the
proportion of the variability of the output variable that
is explained by the input variable.
A. True
B. False
12.12.32 In simple linear regression:
A. The sample correlation coefﬁcient between the
input variable and the output variable is positive if
the regression line has a positive slope.
B. The sample correlation coefﬁcient between the
input variable and the output variable cannot be
smaller than zero.
C. Both of the above.
D. Neither of the above.
12.12.33 Suppose that a company’s sales increase when the
economy is doing well. Also, suppose that the
company’s advertising budget is based upon the
amount of sales and that larger sales lead to a larger
advertising budget.
A. There is a causal relationship between how well the
economy is doing and the company’s advertising
budget.
B. There is a causal relationship between the
company’s sales and the company’s advertising
budget.
C. Both of the above.
D. Neither of the above.
12.12.34 Suppose that a company’s sales increase when the
economy is doing well. Also, suppose that the
company’s advertising budget is based upon the
amount of sales and that larger sales lead to a larger
advertising budget.
A. There is an association between how well the
economy is doing and the company’s advertising
budget.
B. There is an association between the company’s
sales and the company’s advertising
budget.
C. Both of the above.
D. Neither of the above.
12.12.35 In simple linear regression:
A. The existence of a surrogate variable can explain
why two variables have an association without
having a causal relationship.
B. A simple linear regression analysis can prove
that there is a causal relationship between two
variables.
C. Both of the above.
D. Neither of the above.
12.12.36 In simple linear regression, the independent variable is
also known as the:
A. Input variable
B. Response variable
C. Both of the above
D. Neither of the above
12.12.37 In simple linear regression:
A. The coefﬁcient of determination R2 can be
negative.
B. The coefﬁcient of determination R2 measures the
amount of variability of the x variable that is
explained by the y variable.
C. Both of the above.
D. Neither of the above.
12.12.38 In simple linear regression, if the p-value is 0.31 and
the regression line slopes downwards, then:
A. It has been established that as one variable
increases then the other variable tends to
increase.
B. It has been established that as one variable
increases, the other variable tends to
decrease.
C. As one variable increases, there is no evidence of a
tendency for the other variable to change.
D. Neither of the above.
12.12.39 In simple linear regression:
A. The model can be used for prediction regardless of
the p-value.
B. The larger the coefﬁcient of determination then the
less precisely the y variable can be predicted from
the x variable.
C. Both of the above.
D. Neither of the above.

12.12 SUPPLEMENTARY PROBLEMS
607
12.12.40 In simple linear regression:
A. The standard model may be referred to as the least
squares ﬁt.
B. The standard model ﬁnds the line with the smallest
sum of squared residuals.
C. Both of the above.
D. Neither of the above.
12.12.41 In simple linear regression:
A. Extrapolation may be necessary but can be risky.
B. Extrapolation can occur both for values of the input
variable larger than the range of the input variable
and also for values of the input variable smaller
than the range of the input variable.
C. Both of the above.
D. Neither of the above.
12.12.42 In simple linear regression:
A. The output variable y must be continuous, but the
input variable x may be either a continuous or a
categorical variable.
B. If a categorical variable is coded numerically, then
it can be used as the output variable y.
C. Both of the above.
D. Neither of the above.
12.12.43 In simple linear regression, a data point with a residual
of 7.3 is further from the regression line than a data
point with a residual of −8.6.
A. True
B. False
12.12.44 In simple linear regression:
A. High leverage data points should be removed and
the analysis repeated.
B. High leverage data points cannot have a large
residual.
C. Both of the above.
D. Neither of the above.
12.12.45 An investor is interested in analyzing the percentage
changes in a company’s stock price over a monthly
period (a continuous variable). To compare company A
with company B, the investor calculates the percentage
monthly stock price changes for both company A and
company B over each month January to December of
the previous year. Which of the following statements is
true?
A. These data should be analyzed as a paired
two-sample problem.
B. These data should be analyzed as an independent
samples two-sample problem.
C. These data should be analyzed as a regression
problem.
D. These data should be analyzed as an analysis of
variance problem.
12.12.46 An investor is interested in analyzing the percentage
changes in a company’s stock price over a monthly
period (a continuous variable). To determine how the
stock price of company A depends upon the price of
oil, the investor calculates the percentage monthly
stock price changes for company A over each month
January to December of the previous year, together
with the average oil price for each of these months.
Which of the following statements is true?
A. These data should be analyzed as a paired
two-sample problem.
B. These data should be analyzed as an independent
samples two-sample problem.
C. These data should be analyzed as a regression
problem.
D. These data should be analyzed as an analysis of
variance problem.
12.12.47 An investor is interested in analyzing the percentage
changes in a company’s stock price over a monthly
period (a continuous variable). To compare different
sectors of the economy, the investor looks only at the
previous month and calculates the percentage stock
price changes in that month for 7 companies in the
retail sector, 12 companies in the hospitality sector, and
8 companies in the transportation sector. Which of the
following statements is true?
A. These data should be analyzed as a paired
two-sample problem.
B. These data should be analyzed as an independent
samples two-sample problem.
C. These data should be analyzed as a regression
problem.
D. These data should be analyzed as an analysis of
variance problem.

C H A P T E R T H I R T E E N
Multiple Linear Regression
and Nonlinear Regression
A multiple linear regression model is an extension of a simple linear regression model that
allowstheresponsevariable y tobemodeledasalinearfunctionofmorethanoneinputvariable
xi. The ideas and concepts behind multiple linear regression are similar to those discussed
in Chapter 12 for simple linear regression, although the computations are considerably more
complex. The best way to deal with multiple linear regression mathematically is with a matrix
algebra approach, which is discussed in Section 13.3. Nevertheless, the discussion of multiple
linear regression and the examples presented in Sections 13.1, 13.2, and 13.4 can be read
without an understanding of Section 13.3.
Nonlinear regression models, discussed in Section 13.5, are models in which the expected
value of the response variable y is not expressed as a linear combination of the parameters.
Even though many of the ideas in nonlinear regression are similar to those in linear regression,
the mathematical processes required to ﬁt the models and to make statistical inferences are
different.
13.1
Introduction to Multiple Linear Regression
13.1.1
The Multiple Linear Regression Model
Consider the problem of modeling a response variable y as a function of k input variables
x1, . . . , xk based upon a data set consisting of the n sets of values
(y1, x11, x21, . . . , xk1)
...
(yn, x1n, x2n, . . . , xkn)
Thus, yi is the value taken by the response variable y for the ith observation, which is obtained
with values x1i, x2i, . . . , xki of the k input variables x1, x2, . . . , xk.
In multiple linear regression, the value of the response variable yi is modeled as
yi = β0 + β1x1i + · · · + βkxki + ϵi
which consists of a linear combination β0 + β1x1i + · · · + βkxki of the corresponding values
of the input variables together with an error term ϵi. The coefﬁcients β0, . . . , βk are unknown
parameters, and the error terms ϵi, 1 ≤i ≤n, are taken to be independent observations from
a N(0, σ 2) distribution. Of course when k = 1 so that there is only one input variable, this
model simpliﬁes to the simple linear regression model discussed in Chapter 12.
The expected value of the response variable at x = (x1, . . . , xk) is
E(Y|x) = β0 + β1x1 + · · · + βkxk
608

13.1 INTRODUCTION TO MULTIPLE LINEAR REGRESSION
609
x2
x1
y
y = β0 + β1x1 + β2x2
FIGURE 13.1
The expected value of the response variable for multiple
linear regression model with two input variables is a plane
y
x1
y = β0 + β1x1 + β2x2
1
FIGURE 13.2
A quadratic regression model
For example, with k = 2 the expected values of the response variable lie on the plane
y = β0 + β1x1 + β2x2
as illustrated in Figure 13.1. The parameter β0 is referred to as the intercept parameter, and the
parameter βi determines how theith input variable xi inﬂuences the response variable when the
other input variables are kept ﬁxed. If βi > 0, then the expected value of the response variable
increases as the ith input variable increases (with the other input variables kept ﬁxed), and if
βi < 0, then the expected value of the response variable decreases as the ith input variable
increases (with the other input variables kept ﬁxed). If βi = 0, then the response variable is
not inﬂuenced by changes in the ith input variable (with the other input variables kept ﬁxed).
The multiple linear regression model can provide a rich variety of functional relationships
by allowing some of the input variables xi to be functions of other input variables. For example,
with k = 2 and x2 = x2
1 the expected value of the response variable is modeled as
y = β0 + β1x1 + β2x2
1
as illustrated in Figure 13.2. This equation is known as a quadratic regression model and
allows the response variable to be related to a quadratic function of an input variable. More
generally, a polynomial regression model of order k has xi = xi
1 so that the model is
y = β0 + β1x1 + β2x2
1 + · · · + βkxk
1
and the response variable is related to a polynomial function of the input variable x1.
Another common model involving quadratic functions of two input variables x1 and x2 is
y = β0 + β1x1 + β2x2
1 + β3x2 + β4x2
2 + β5x1x2
The ﬁnal term x1x2 is referred to as the cross product or interaction term of the two input
variables x1 and x2. This model is known as a response surface model and is illustrated in
Figure 13.3.
The input variables xi may be measurements of a continuous variable or a discrete variable,
as illustrated in the examples in Section 13.2. In particular, a variable may be used to indicate

610
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
FIGURE 13.3
A typical response surface model
x1
x2
y
y = β0 + β1x1 + β2x2
1
+ β4x2
2
+ β3x2
+ β5x1x2
the presence or absence of a certain effect or to designate at which of two levels a data
observation is obtained. Such input variables are known as indicator variables. For example,
if the response variable is a measurement obtained from a person, then the indicator variable
xi taking the values xi = 0 if the person is male and xi = 1 if the person is female can be used
to investigate whether the response variable is related to the gender of the person. In other
cases, indicator variables may be used to indicate whether an observation is taken before or
after a change has occurred or to indicate from which of two machines an observation is taken,
for example.
In summary, the key to deciding whether a particular model can be formulated as a multiple
linear regression model is to determine whether the expected value of the response variable
can be written as a linear combination of a set of (possibly related) input variables that can
represent either continuous or discrete variables. As discussed in Chapter 12 with respect
to the simple linear regression model, transformations of the response variable or the input
variables can be employed to make the linear model appropriate. In practice, the multiple
linear regression model can provide a general and useful way of exploring the relationship
between a response variable and a set of input variables.
13.1.2
Fitting the Linear Regression Model
As in simple linear regression, the parameters β0, . . . , βk are estimated using the method of
least squares. As Figure 13.4 illustrates for k = 2, this implies that the parameters are chosen
so as to minimize the sum of squares of the (vertical) distances between the data observations
yi and their ﬁtted values
ˆβ0 + ˆβ1x1i + · · · + ˆβkxki
With the normal distribution of the error terms ϵi the method of least squares produces the
maximum likelihood estimates of the parameters.
The sum of squares is
Q =
n

i=1
(yi −(β0 + β1x1i + · · · + βkxki))2

13.1 INTRODUCTION TO MULTIPLE LINEAR REGRESSION
611
FIGURE 13.4
Least squares ﬁt for a multiple
linear regression model with two
input variables
x2
y = βˆ
0 + βˆ
1x1 + βˆ
2x2
Data point (yi, x1i, x2i)
Least squares fit 
minimizes the sum 
of squared deviations
x1
y
which can be minimized by taking derivatives with respect to each of the unknown parameters,
setting the resulting expressions equal to 0, and then simultaneously solving the equations to
obtain the parameter estimates ˆβi. The derivative of Q with respect to β0 is
∂Q
∂β0
= −2
n

i=1
(yi −(β0 + β1x1i + · · · + βkxki))
and the derivative of Q with respect to β j, 1 ≤j ≤k, is
∂Q
∂β j
= −2
n

i=1
x ji(yi −(β0 + β1x1i + · · · + βkxki))
Setting these expressions equal to 0 results in the k + 1 equations
n

i=1
yi = nβ0 + β1
n

i=1
x1i + β2
n

i=1
x2i + · · · + βk
n

i=1
xki
n

i=1
yix1i = β0
n

i=1
x1i + β1
n

i=1
x2
1i + β2
n

i=1
x2ix1i + · · · + βk
n

i=1
xkix1i
...
n

i=1
yixki = β0
n

i=1
xki + β1
n

i=1
x1ixki + β2
n

i=1
x2ixki + · · · + βk
n

i=1
x2
ki
The parameter estimates ˆβ0, ˆβ1, . . . , ˆβk are the solutions to these equations, which are known
as the normal equations.
It would be tedious to ﬁnd the solutions to these equations by hand, yet it is a simple
problem for a computer to handle. Mathematically, the best way to deal with this problem
and any subsequent inferences concerning the regression model is to use matrix algebra.

612
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
This procedure is demonstrated in Section 13.3, although an understanding of the matrix
formulation of the multiple linear regression analysis is not essential for the rest of this
chapter.
13.1.3
Analysis of the Fitted Model
The ﬁtted value of the response variable at the data point (x1i, . . . , xki) is
ˆyi = ˆβ0 + ˆβ1x1i + · · · + ˆβkxki
and the ith residual is
ei = yi −ˆyi
As in simple linear regression, the sum of squares for error is deﬁned to be
SSE =
n

i=1
(yi −ˆyi)2 =
n

i=1
e2
i
The estimate of the error variance σ 2 is
MSE = ˆσ 2 =
SSE
n −k −1
(where the degrees of freedom for error n −k −1 are equal to the sample size n minus the
number of parameters estimated k + 1), which is distributed as
ˆσ 2 ∼σ 2 χ2
n−k−1
n −k −1
Also, as with simple linear regression the total sum of squares
SST =
n

i=1
(yi −¯y)2
can be decomposed as
SST = SSR + SSE
where the regression sum of squares is
SSR =
n

i=1
(ˆyi −¯y)2
However, as the analysis of variance table in Figure 13.5 shows, there are now k degrees of
freedom for regression and n −k −1 degrees of freedom for error. The p-value in the analysis
F-statistic
F = MSR/MSE
p-value
P(Fk,n−k−1 > F)
Mean square
MSR = SSR/k
MSE = SSE/(n −k −1)
Source
Regression 
Error
Total
Degrees of freedom
k
n −k −1
n −1
Sum of squares
SSR
SSE
SST
FIGURE 13.5
Analysis of variance table for a multiple linear regression analysis

13.1 INTRODUCTION TO MULTIPLE LINEAR REGRESSION
613
of variance table is for the null hypothesis
H0 : β1 = · · · = βk = 0
(with the alternative hypothesis that at least one of these βi is nonzero), which has the in-
terpretation that the response variable is not related to any of the k input variables. A large
p-value therefore indicates that there is no evidence that any of the input variables affects the
distribution of the response variable y.
The coefﬁcient of (multiple) determination
R2 = SSR
SST
takes a value between 0 and 1 and indicates the amount of the total variability in the values of
the response variable that is accounted for by the ﬁtted regression model. The F-statistic in
the analysis of variance table can be written as
F = (n −k −1)R2
k(1 −R2)
Analysis of Variance Table for Multiple Linear Regression Problem
The analysis of variance table for a multiple linear regression problem provides a test
of the null hypothesis
H0 : β1 = · · · = βk = 0
which implies that the response variable is not related to any of the k input variables.
The p-value is
p-value = P(X > F)
where the random variable X has an F-distribution with degrees of freedom k and
n −k −1, and the F-statistic is
F = SSR
k ˆσ 2 = (n −k −1)R2
k(1 −R2)
A small p-value in the analysis of variance table indicates that the response variable is
related to at least one of the input variables. Computer output provides the user with the
parameter estimates
ˆβ0, ˆβ1, . . . , ˆβk
together with their standard errors
s.e.( ˆβ0), s.e.( ˆβ1), . . . , s.e.( ˆβk)
and these can be used to determine which input variables are needed in the regression model.
If βi = 0, then the distribution of the response variable does not directly depend on the input
variable xi, which can therefore be “dropped” from the model. Consequently, it is useful to
test the hypotheses
H0 : βi = 0
versus
HA : βi ̸= 0

614
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
If the null hypothesis is accepted, then there is no evidence that the response variable is directly
related to the input variable xi, and it can therefore be dropped from the model. If the null
hypothesis is rejected, then there is evidence that the response variable is related to the input
variable, and it should therefore be kept in the model.
These hypotheses are tested with the t-statistics
t =
ˆβi
s.e.( ˆβi)
which are compared to a t-distribution with n −k −1 degrees of freedom. The (two-sided)
p-value is therefore
p-value = 2 × P(X > |t|)
where the random variable X has a t-distribution with n −k −1 degrees of freedom.
Computer output for this analysis generally contains a list of p-values corresponding to
each of the parameters β0, β1, . . . , βk. As with simple linear regression, the p-value for the
intercept parameter β0 is usually not important. For the other parameters, p-values larger than
10% indicate that the corresponding input variable can be dropped from the model, while
p-values smaller than 1% indicate that the corresponding input variable should be kept in the
model. However, p-values between 1% and 10% do not provide a clear indication, and how
the corresponding input variables are dealt with can be left to the experimenter’s judgment or
particular requirements. It should also be noted that when one or more variables are removed
from the model, the p-values of the remaining variables may change when the reduced model
is ﬁtted.
The more general hypotheses
H0 : βi = bi
versus
HA : βi ̸= bi
for a ﬁxed value of bi of interest can be tested with the t-statistic
t =
ˆβi −bi
s.e.( ˆβi)
which is again compared to a t-distribution with n −k −1 degrees of freedom. A conﬁdence
interval for βi can be constructed as shown in the accompanying box.
Model ﬁtting is performed by ﬁnding which subset of the k input variables is required to
model the dependent variable y in the best and most succinct manner. The ﬁnal model that
the experimenter uses for inference problems should consist of input variables that each have
p-values no larger than 10%, because otherwise some variables should be taken out of the
model to simplify it. An important warning when ﬁtting the model is that it is best to remove
only one variable from the model at a time. This is because when one variable is removed
from the model and the subsequent reduced model is ﬁtted, the p-values of the remaining
input variables in the model usually change.
Typically, model ﬁtting may be performed in the following manner. First, an experimenter
starts by ﬁtting all k input variables. If all variables are needed in the model, then no reduction
is necessary. If one or more input variables has a p-value larger than 10%, the variable with the
largest p-value (smallest absolute value of the t-statistic) is removed. The reduced model with
k −1 input variables is then ﬁtted, and the process is repeated. This is known as a backwards
elimination modeling procedure and most computer packages will perform it automatically
on request.
Alternatively, a forward selection modeling procedure can be employed whereby the
model starts off without any variables, and input variables are then added one at a time until

13.1 INTRODUCTION TO MULTIPLE LINEAR REGRESSION
615
Inferences on a Parameter βi
Inferences on the parameter βi in a multiple linear regression model, which indicates
how the response variable is related to the ith input variable, are performed using ˆβi
and s.e.( ˆβi). Speciﬁcally, the hypotheses
H0 : βi = bi
versus
HA : βi ̸= bi
for a ﬁxed value bi of interest have a two-sided p-value
p-value = 2 × P(X > |t|)
where the random variable X has a t-distribution with n −k −1 degrees of freedom
and the t-statistic is
t =
ˆβi −bi
s.e.( ˆβi)
A 1 −α conﬁdence level two-sided conﬁdence interval for βi can be constructed as
βi ∈( ˆβi −tα/2,n−k−1s.e.( ˆβi), ˆβi + tα/2,n−k−1s.e.( ˆβi))
no further additions are needed. Most computer packages can also implement a stepwise
procedure that is a combination of the backward elimination and forward selection procedures,
and that allows either the addition of new variables or the removal of variables already in the
model.
Occasionally, an experimenter may wish to test the hypothesis that a certain subset of the
parameters βi are all equal to 0. Speciﬁcally, suppose that the null hypothesis
H0 : βr+1 = βr+2 = · · · = βk = 0
is of interest. This hypothesis is tested by ﬁrst ﬁtting the full model with parameters β1, . . . , βk
and ﬁnding the error sum of squares SSEk, and then ﬁtting the reduced model with parameters
β1, . . . , βr and ﬁnding the new error sum of squares SSEr. The F-statistic
F = (n −k −1)(SSEr −SSEk)
(k −r)SSEk
is calculated and the null hypothesis has a p-value
p-value = P(X > F)
where the random variable X has an F-distribution with parameters k −r and n −k −1.
13.1.4
Inferences on the Response Variable
Once a ﬁnal model has been decided upon, it can be used to make inferences about the response
variable at certain values of the input variables. Suppose that the ﬁnal model includes the k
input variables x1, . . . , xk which may be only a subset of the initial set of input variables that
were investigated. The ﬁtted value of the response variable at a speciﬁc set of values of the
input variables x∗
i of interest is
ˆy|x∗= ˆβ0 + ˆβ1x∗
1 + · · · + ˆβkx∗
k

616
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
FIGURE 13.6
Extrapolation of the ﬁtted model
can occur even when both variables
x1 and x2 are within their ranges
from the data set
Range of
x2 variable
Set of 
data points
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Extrapolation of fitted model occurs here
Range of x1 variable
+
+
+
+
+
+
x1
x2
which is an estimate of the expected value of the response variable y at these values of the
input variables.
A 1−α conﬁdence level conﬁdence interval for the expected value of the response variable
at x∗
i is
β0 + β1x∗
1 + · · · + βkx∗
k ∈(ˆy|x∗−tα/2,n−k−1s.e.(ˆy|x∗), ˆy|x∗+ tα/2,n−k−1s.e.(ˆy|x∗))
which can be obtained from most computer packages. Prediction intervals for a future obser-
vation obtained at x∗
i can also be obtained.
When the ﬁtted model is used, it is important to be aware of the danger of extrapolation
that occurs when the model is used for values of the input variables outside the range of the
data set. As Figure 13.6 shows, it is generally not sufﬁcient to consider the ranges of each of
the input variables xi and to then consider the ﬁtted model appropriate as long as the values
of each of the input variables lie within its respective range. Extrapolation can occur even
when each individual variable lies within its respective range of data values.
As a ﬁnal point, it should always be remembered that as in simple linear regression, it
is important to check whether the assumptions of the linear model appear to be appropriate.
Residual plots and other techniques related to this issue are discussed in Section 13.4.
13.1.5
Problems
13.1.1 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to a data set of n = 30 observations. The total
sum of squares is SST = 108.9, and the error sum of
squares is SSE = 12.4.
(a) What is the coefﬁcient of determination R2?
(b) Write out the analysis of variance table.
(c) What is the estimate of the error variance ˆσ 2?
(d) What is the p-value for the null hypothesis
H0 : β1 = β2 = β3 = 0
(e) If ˆβ1 = 16.5 and s.e.( ˆβ1) = 2.6, construct a
two-sided 95% conﬁdence interval for β1.
13.1.2 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + β5x5 + β6x6

13.1 INTRODUCTION TO MULTIPLE LINEAR REGRESSION
617
is ﬁtted to a data set of n = 45 observations. The total
sum of squares is SST = 11.62, and the error sum of
squares is SSE = 8.95.
(a) What is the coefﬁcient of determination R2?
(b) Write out the analysis of variance table.
(c) What is the estimate of the error variance ˆσ 2?
(d) What is the p-value for the null hypothesis
H0 : β1 = β2 = β3 = β4 = β5 = β6 = 0
(e) If ˆβ3 = 1.05 and s.e.( ˆβ3) = 0.91, construct a
two-sided 95% conﬁdence interval for β3.
13.1.3 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4
is ﬁtted to a data set of n = 12 observations.
(a) If ˆβ2 = 132.4 and s.e.( ˆβ2) = 27.6, construct a
two-sided 95% conﬁdence interval for β2.
(b) What is the p-value for the null hypothesis
H0 : β2 = 0?
13.1.4 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to a data set of n = 15 observations.
(a) If ˆβ1 = 0.954 and s.e.( ˆβ1) = 0.616, construct a
two-sided 95% conﬁdence interval for β1.
(b) What is the p-value for the null hypothesis
H0 : β1 = 0?
13.1.5 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to a data set of n = 44 observations. The
parameter estimates ˆβ1 = 11.64, ˆβ2 = 132.9, and
ˆβ3 = 0.775 are obtained with standard errors
s.e.( ˆβ1) = 1.03, s.e.( ˆβ2) = 22.8, and s.e.( ˆβ3) = 0.671.
Should any of the variables x1, x2, and x3 be removed
from the model?
13.1.6 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + β5x5 + β6x6
is ﬁtted to a data set of n = 23 observations and an error
sum of squares of 1347.1 is obtained. The model
y = β0 + β1x1 + β2x2
is then ﬁtted to the data set and an error sum of squares
of 1873.4 is obtained. Test the null hypothesis
H0 : β3 = β4 = β5 = β6 = 0
13.1.7 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + β5x5
is ﬁtted to a data set of n = 19 observations and an error
sum of squares of 12.76 is obtained. The model
y = β0 + β4x4 + β5x5
is then ﬁtted to the data set and an error sum of squares
of 28.33 is obtained. Test the null hypothesis
H0 : β1 = β2 = β3 = 0
13.1.8 The multiple linear regression model
y = β0 + β1x1 + β2x2
is ﬁtted to the data set given in DS 13.1.1.
(a) Write out the normal equations and show that they
are satisﬁed by the parameter estimates ˆβ0 = 7.280,
ˆβ1 = −0.313, and ˆβ2 = −0.1861.
(b) What is the ﬁtted value of the expected value of the
response variable when x1 = x2 = 1?
13.1.9 The multiple linear regression model
y = β0 + β1x1 + β2x2
is ﬁtted to a data set with n = 20 observations, and the
parameter estimates ˆβ0 = 104.9, ˆβ1 = 12.76, and
ˆβ2 = 409.6 are obtained.
(a) What is the ﬁtted value of the expected value of the
response variable when x1 = 10 and x2 = 0.3?
(b) If this ﬁtted value has a standard error of 17.6,
construct a two-sided 95% conﬁdence interval for
the expected value of the response variable at this
point.
13.1.10 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to a data set with n = 15 observations, and the
parameter estimates ˆβ0 = 65.98, ˆβ1 = 23.65,
ˆβ2 = 82.04, and ˆβ3 = 17.04 are obtained.
(a) What is the ﬁtted value of the expected value of the
response variable when x1 = 1.5, x2 = 1.5, and
x3 = 2.0?
(b) If the ﬁtted value from part (a) has a standard error
of 2.55, construct a two-sided 95% conﬁdence
interval for the expected value of the response
variable at this point.
13.1.11 The model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to n = 20 data observations. Suppose that
ˆσ = 4.33, 20
i=1 yi = −5.68, and 20
i=1 y2
i = 694.09.
Construct the ANOVA table and put bounds on the

618
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
p-value. What hypothesis is being tested by the
p-value? What proportion of the variability of the y
variable is explained by the model?
13.1.12 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4
is ﬁtted to a data set of n = 22 observations. The
total sum of squares is 45.76; the error sum of squares
is 23.98.
(a) What is the coefﬁcient of determination R2?
(b) Write out the ANOVA table.
(c) What is the estimate of σ 2?
(d) What is the p-value for the null hypothesis
H0 : β1 = β2 = β3 = β4 = 0
(e) If ˆβ2 = 183.2 and s.e.( ˆβ2) = 154.3, construct a
two-sided 95% conﬁdence interval for β2.
13.1.13 In multiple linear regression:
A. The output variable is treated as a continuous
variable and the input variables are all treated as
discrete variables.
B. The output variable is treated as a continuous
variable and some but not all of the input variables
may be treated as discrete variables.
C. The output variable is treated as a discrete variable
and the input variables are all treated as discrete
variables.
D. The output variable is treated as a discrete variable
and some but not all of the input variables may be
treated as discrete variables.
E. Neither of the above.
13.1.14 The purpose of a multiple linear regression model
can be:
A. To investigate which subset of the input variables are
associated with the output variable
B. To ﬁnd a model for predicting future values of the
output variable
C. Both of the above
D. Neither of the above
13.1.15 When interpreting how changes in the ﬁrst input
variable affect the output variable in a multiple linear
regression model, not only must the ﬁtted coefﬁcient ˆβ1
be considered but also the correlations between the ﬁrst
input variable and the other input variables in the model
must be taken into account.
A. True
B. False
13.1.16 In an ANOVA table for multiple linear regression with
k = 3 and n = 10, the F-statistic is 1.40. The p-value is:
A. Less than 0.01
B. Between 0.01 and 0.05
C. Between 0.05 and 0.10
D. Greater than 0.10
13.1.17 A multiple linear regression model with two input
variables is run on a data set with 20 data points. It is
found that ˆβ1 = 75.92 with s.e. (ˆβ1) = 73.92, while
ˆβ2 = −4.65 with s.e. (ˆβ2) = 0.93. The experimenter
should:
A. Remove both variables from the model.
B. Remove variable 1 from the model and do a simple
linear regression with just variable 2 to see whether
it is signiﬁcant.
C. Remove variable 2 from the model and do a simple
linear regression with just variable 1 to see whether
it is signiﬁcant.
D. Keep both variables in the model.
13.2
Examples of Multiple Linear Regression
Skills at modeling with multiple linear regression models are developed primarily through
experience. Every problem has its own unique characteristics due to the types of variables
considered and the relationships between them. Rather than blindly throwing new variables
into the model, a successful modeler always has an understanding and feeling for the variables
under consideration.
13.2.1
Examples
Example 70
Chemical Yields
A chemical engineer measures the yield of a chemical obtained from a reaction performed
at various temperatures, and the data set shown in Figure 13.7 is obtained. The data plot
suggests that there is some curvature in the relationship between yield and temperature, and

13.2 EXAMPLES OF MULTIPLE LINEAR REGRESSION
619
this is conﬁrmed by the straight line ﬁt (simple linear regression model with yield as the
response variable and temperature as the input variable) shown in Figure 13.8, because the
ﬁtted line underestimates the yield at low and high temperatures and overestimates the yield
at the middle temperatures.
Yield
85
76
114
143
164
281
306
358
437
470
649
702
Temperature
°C
90
100
110
120
130
140
150
160
170
180
190
200
FIGURE 13.7
Chemical yields data set
The chemical engineer therefore decides to try the quadratic model
yield = β0 + (β1 × temperature) + (β2 × temperature2)
and a statistical software package is employed to give the ﬁtted curve shown in Figure 13.9,
which appears to provide a satisfactory ﬁt. The equation of the ﬁtted curve is
yield = 293 −(6.14 × temperature) + (0.0411 × temperature2)
An F-statistic of 326.41 is given in the analysis of variance table in the engineer’s computer
output and the p-value for the null hypothesis
H0 : β1 = β2 = 0
FIGURE 13.8
Unsatisfactory linear model for the
chemical yields data set
Temperature
Yield
700
500
600
400
300
200
100
200
100
120
140
160
180
0
FIGURE 13.9
Quadratic model for the chemical
yields data set
Temperature
Yield
700
500
600
400
300
200
100
200
100
120
140
160
180
0

620
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
is very small so that the null hypothesis is not plausible. In other words, there is evidence of a
relationship between the chemical yield and the temperature, which is obvious from the data
plot.
Also, the computer output gives
ˆβ2 = 0.041086
with
s.e.( ˆβ2) = 0.007570
so that the t-statistic for testing H0 : β2 = 0 is
t =
ˆβ2
s.e.( ˆβ2) = 0.041086
0.007570 = 5.43
The p-value is therefore
p-value = 2 × P(X > 5.43) ≃0
where the random variable X has a t-distribution with n −k −1 = 12 −2 −1 = 9 degrees of
freedom. The small p-value indicates that the null hypothesis H0 : β2 = 0 is not plausible and
that the quadratic term in the model signiﬁcantly improves the ﬁt. In fact, with t0.025,9 = 2.262,
a two-sided 95% conﬁdence interval for β2 is
(0.041086 −(2.262 × 0.007570), 0.041086 + (2.262 × 0.007570)) = (0.024, 0.058)
The p-value for the intercept parameter β0 is 0.089, but there is no reason to take it out
of the model. In addition, the p-value for the linear parameter β1 is 0.021. However, because
yield has a signiﬁcant quadratic dependence on temperature there is no reason to remove the
linear term “ ˆβ1 × temperature” from the model. Actually, because there is a quadratic term
in the model, the presence or absence of the linear term and the intercept parameter depends
on the scale used to measure temperature.
Finally, at a temperature of 155◦the predicted value of the yield is
yield = 293 −(6.14 × 155) + (0.0411 × 1552) = 328.3
This ﬁtted value has a standard error of 11.72, and a 95% conﬁdence interval for the expected
value of the chemical yield obtained at this temperature is about (301, 355). A 95% prediction
interval for a future chemical yield obtained at a temperature of 155◦is about (260, 397). These
can be obtained from a computer software package, using the analysis methods described in
Section 13.3.
Example 71
Supermarket Deliveries
A supermarket manager decides to investigate how long it takes to unload deliveries from a
truck. As illustrated in Figure 13.10, an initial consideration of the problem suggests that the
unloading time could depend on both the volume of the delivery and the weight of the delivery.
In addition, the supermarket has a day shift and a night shift, and the manager suspects that
the unloading times may be different for the two shifts.
Ten deliveries are selected at random for both the day and night shifts, and the unloading
times are measured together with the volumes and weights of the deliveries. The data set
obtained is shown in Figure 13.11. Notice that the two shifts are designated by an indicator
variable that takes the value 0 for the day shift and 1 for the night shift.
The multiple linear regression model employed is
y = β0 + β1x1 + β2x2 + β3x3

13.2 EXAMPLES OF MULTIPLE LINEAR REGRESSION
621
FIGURE 13.10
Supermarket deliveries
x3 =
Day shift
Night shift
}
y = Unloading time
x1 = volume
x2 = weight
FIGURE 13.11
Supermarket deliveries data set
Volume of load 
(m3)
Weight of load 
(1000 kg)
Shift 
(0 day, 1 night)
Unloading time 
(minutes)
79
43
0
48
76
46
0
42
84
43
0
44
66
32
0
33
98
56
0
53
98
57
0
58
92
56
0
52
73
38
0
34
80
44
0
40
62
37
0
24
87
45
1
47
67
44
1
32
62
37
1
30
76
36
1
39
93
55
1
56
61
41
1
23
53
32
1
20
63
43
1
35
77
49
1
42
59
40
1
19
where y is the unloading time, x1 is the volume of the delivery, x2 is the weight of the delivery,
and x3 is the indicator variable designating the shift. When a computer is used to analyze this
data set, an F-statistic of 60.99 is obtained in the analysis of variance table and the associated
small p-value for the null hypothesis
H0 : β1 = β2 = β3 = 0
indicates that the unloading time is related to at least some of the input variables. However,
when the coefﬁcients βi are considered individually, volume (β1) has a small p-value, whereas
weight (β2) has a p-value of 0.949 and shift (β3) has a p-value of 0.691. For example,
ˆβ2 = 0.0137
with
s.e.( ˆβ2) = 0.2109

622
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
FIGURE 13.12
Simple linear regression model for
time against volume for the
supermarket deliveries data set
Volume of load
Unloading time
60
40
50
30
100
50
60
70
80
90
20
which gives a t-statistic of
t =
ˆβ2
s.e.( ˆβ2) = 0.0137
0.2109 = 0.07
The p-value of 0.949 for weight is thus obtained as
p-value = 2 × P(X > 0.07)
where the random variable X has a t-distribution with n −k −1 = 20 −3 −1 = 16 degrees
of freedom.
The high p-values for the variables weight and shift suggest that they are not needed in
the model. The variable weight has the largest p-value and so it is removed from the model
ﬁrst. The subsequent model with volume and shift should be ﬁtted again.
The p-value for the shift variable now becomes 0.652 (notice that it has changed slightly
after weight has been removed from the model), which again indicates that it should be
removed from the model. Thus, the model has been reduced to a simple linear regression
model involving just the variable volume.
Figure 13.12 shows the ﬁnal simple linear regression model together with a plot of the
data. The manager has discovered that the best way to predict the unloading time of a delivery
is with the model
time = −24.2 + (0.833 × volume)
and that neither the weight of the delivery nor the shift provides any additional useful infor-
mation. In other words, the time it will take to unload a future load should be estimated from
the volume of the load, and once the volume is known, the weight of the load and the shift are
both irrelevant.
Does this mean that the unloading time and the weight of the delivery are uncorrelated?
In fact it does not. Figure 13.13 shows a plot of the unloading times and the weights together
with a simple linear regression model with unloading time as the response variable and weight
as the input variable. The slope parameter is signiﬁcantly different from 0 (the t-statistic is
5.92) and the unloading time and weight are positively correlated.
Why then was weight dropped from the original model? Figure 13.14, which shows a plot
ofthedeliveryvolumesagainsttheirweights,providesananswertothisquestion.Thevariables

13.2 EXAMPLES OF MULTIPLE LINEAR REGRESSION
623
FIGURE 13.13
Simple linear regression model for
time against weight for the
supermarket deliveries data set
Weight of load
Unloading time
60
40
50
30
60
30
35
40
45
50
55
20
FIGURE 13.14
The input variables volume and
weight are positively correlated for
the supermarket deliveries data set
Weight of load
Volume of load
100
80
90
70
60
30
35
40
45
50
55
50
60
volume and weight are seen to be positively correlated, as would be expected. Therefore, it
turns out that once volume is in the model, weight serves no additional useful purpose. In fact,
the variable volume can be thought of as acting as a surrogate for the variable weight.
When a model with both volume and weight is ﬁtted, the mathematics of the ﬁtting proce-
dure identify which of the two variables is needed to provide the best ﬁt to the unloading times.
The correlation between the unloading times and the delivery volumes is r = 0.96, whereas
the correlation between the unloading times and the delivery weights is r = 0.81, and the
higher correlation for volume helps to explain why volume is preferred to weight in the model.
Example 72
Turf Quality
A turf grower who provides high-quality turf for sporting purposes performs a small exper-
iment to investigate how a particular variety of turf reacts to various amounts of water and
fertilizer. A total of n = 14 samples of turf are grown under different experimental conditions,
and at the end of the allotted time period they are each given a quality score based upon the

624
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
Water level
Fertilizer level
35
30
20
25
15
16
14
2
4
6
8
10
12
71
34
38
41
46
48
46
27
68
37
48
32
62
72
10
Water 
level
9
12
6
15
15
13
13
11
11
7
5
5
3
Quality 
score
72
71
62
32
48
37
68
27
46
48
46
41
38
34
3
Fertilizer 
level
26
20
20
24
16
28
12
32
8
32
28
12
24
16
FIGURE 13.15
Turf quality data set and scatterplot
appearance of the turf, the health of the roots, the health of the grass blades, and the density of
the grass. The resulting data set is shown in Figure 13.15, where higher scores indicate better
quality grass.
The data set indicates that there is some curvature in the relationship between score and
the variables water and fertilizer, with too much water and fertilizer resulting in a drop in the
scores. Consequently, it should be useful to ﬁt the response surface model
y = β0 + β1x1 + β2x2
1 + β3x2 + β4x2
2 + β5x1x2
where the response variable y is the score, x1 is the water level, and x2 is the fertilizer level.
When this model is ﬁtted, all of the parameters β1, . . . , β5 have low p-values so that they
are signiﬁcantly different from 0, and therefore no terms need to be dropped from the model.
The ﬁtted model is
y = −144 + 24.2x1 −1.01x2
1 + 11.5x2 −0.236x2
2 −0.270x1x2
which is shown in Figure 13.16. The ﬁtted model is maximized at a water level of x1 = 9.4
and a fertilizer level of x2 = 19.0, which the turf grower can use as estimates of the optimum
growing conditions.
There is a great deal of additional theory behind the design and analysis of experiments
using response surfaces that the reader may wish to investigate. One question of importance
is how should the design points (in the example, the values of the water and fertilizer levels
used in the experiment) be chosen optimally? Furthermore, some experiments can be run in
a sequential manner where the results of some experimental trials are used to indicate what
additional experiments should be performed.
Example 44
Army Physical Fitness
Test
Recall that the Army Physical Fitness Test consists of 2 minutes of pushups followed by
2 minutes of situps followed by a 2-mile run. Figure 13.17 shows the full data set including
the number of situps performed, and Figure 13.18 gives summary statistics for the number

13.2 EXAMPLES OF MULTIPLE LINEAR REGRESSION
625
FIGURE 13.16
Fitted response surface for turf
quality example
Fertilizer level
70
60
50
40
30
20
Water level
40
35
30
25
20
19.0
15
10
5
2
20
18
16
14
12
10
8
6
4
9.4
x1= 9.4
x1
Water 
Level
Fertilizer
Level
x2
x2= 19.0
Fitted response 
surface for
quality score
Maximum
quality score
of situps. How can a participant’s run time be predicted from the number of pushups and the
number of situps performed by the participant?
The multiple linear regression model
y = β0 + β1x1 + β2x2
can be used where the response variable y is the run time, x1 is the number of pushups,
and x2 is the number of situps. When this model is ﬁtted, pushups are signiﬁcant (p-value =
0.0028), but situps are not signiﬁcant (p-value = 0.3448). This ﬁnding implies that situps
should be dropped from the model and that the run time should be predicted from the num-
ber of pushups using the simple linear regression model discussed in Chapter 12. Once the
number of pushups performed by a participant is known, knowledge of the number of situps

626
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
2-mile
run time
(seconds)
Number 
of 
pushups
Number 
of 
situps
847
60
83
887
53
67
879
60
70
919
55
60
816
60
71
814
78
83
814
74
70
855
70
69
980
46
48
954
50
55
1078
50
48
1001
59
61
766
62
71
916
64
65
798
51
62
782
66
64
836
73
69
837
78
80
791
80
90
838
70
78
853
70
85
2-mile
run time
(seconds)
882
861
845
865
883
881
921
816
837
1056
1034
774
821
850
870
931
930
808
828
719
707
Number 
of 
pushups
58
62
60
90
78
86
86
69
74
40
40
78
70
78
70
55
55
80
78
78
68
Number 
of 
situps
57
67
61
82
72
80
77
56
68
45
50
76
70
70
61
60
73
66
62
78
70
2-mile
run time
(seconds)
Number 
of 
pushups
Number 
of 
situps
840
79
86
740
93
91
763
80
100
778
78
78
855
70
48
875
60
60
868
78
73
880
50
50
905
70
67
895
69
64
720
125
95
712
80
80
703
99
92
741
80
80
792
61
68
808
50
50
761
55
52
785
60
49
801
60
59
810
65
54
1013
66
82
2-mile
run time
(seconds)
Number 
of 
pushups
Number 
of 
situps
934
939
977
896
921
815
838
854
1063
1024
780
813
850
902
906
865
886
881
825
821
832
45
45
47
88
70
80
79
71
50
50
73
78
78
63
60
78
78
78
50
61
62
45
45
51
65
71
84
81
82
55
50
78
71
80
60
60
84
82
82
59
56
67
FIGURE 13.17
Data set for the Army Physical Fitness Test
Situps
n = 84
Sample mean = 68.29
Sample standard deviation = 13.05
Maximum = 100
Upper quartile = 80
Median = 68.5
Lower quartile = 59.25
Minimum = 45
FIGURE 13.18
Summary statistics for the number
of situps
Number of situps
Two-mile run time
1100
900
1000
800
100
40
60
50
70
80
90
700
FIGURE 13.19
A simple linear regression model of run time against situps for the Army Physical Fitness Test data set
performed by the participant does not provide any further assistance in predicting the partici-
pant’s run time.
However, as Figure 13.19 shows, by itself the variable situps is negatively correlated with
the run time (in the simple linear regression of run time against situps, the t-statistic is −5.25
and the p-value is very small). Why then was situps dropped from the original model? This is

13.2 EXAMPLES OF MULTIPLE LINEAR REGRESSION
627
FIGURE 13.20
The input variables pushups and
situps are positively correlated for
the Army Physical Fitness Test
data set
Number of pushups
Number of situps
100
70
60
80
90
50
130
40
70
50
60
80
90
100
110
120
40
because the variable situps is correlated with the variable pushups as shown in Figure 13.20.
The correlation between run time and pushups is r = −0.57, whereas the correlation between
run time and situps is r = −0.50, so that pushups is a marginally more effective predictor
of run time than situps and acts as a surrogate for situps in the model.
An explanation from a physiological point of view may be that the ability to do situps is
a rather specialized skill that depends primarily on the stomach muscles. The abilities to do
pushups and to run fast are more likely to be reﬂections of general athletic ability or of how
physically ﬁt a person is. Consequently, for the purpose of modeling run times, pushups are
the most important variable and situps do not provide any additional useful information.
13.2.2
Problems
13.2.1 Competitive Pricing Policies
The data set given in DS 13.2.1 concerns the sales volume
of a company, the price at which the company sells its
product, and the price of a competing product for each
of n = 10 quarter periods. Use this data set to ﬁt the
multiple linear regression model
y = β0 + β1x1 + β2x2
with the response variable y as the sales volume and
with x1 as the product price and x2 as the competitor’s
price.
(a) Plot the company’s sales against its own price and
against its competitor’s price.
(b) Is the variable competitor’s price needed in the
regression model? What is the sample correlation
coefﬁcient between the competitor’s price and sales?
What is the sample correlation coefﬁcient between
the competitor’s price and the company’s price?
Interpret your answers.
(c) What is the sample correlation coefﬁcient between
the company’s price and its sales? What sales would
you predict if the company priced its product at
$10.0 next quarter?
13.2.2 Polymer Concentrations for Optimal Fiber Strengths
Two polymers are ingredients in the manufacture of a
synthetic ﬁber. The data set given in DS 13.2.2 shows the
results of an experiment conducted to measure the ﬁber
strength resulting from different concentrations of the
two polymers. The polymer concentrations examined
represent their standard levels, coded as 0, together with

628
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
one unit above and below the standard levels. Fit the
response surface model
y = β0 + β1x1 + β2x2
1 + β3x2 + β4x2
2 + β5x1x2
where the response variable y is the ﬁber strength,
x1 is the concentration of polymer 1, and x2 is the
concentration of polymer 2. At what concentrations of
the polymers would you estimate that the ﬁber strength
is maximized?
13.2.3 Oil Well Drilling Costs
Consider again the problem of estimating the costs of
drilling oil wells, which was originally discussed in
Problem 12.2.4. The data set given in DS 13.2.3 contains
the variables geology, downtime, and rig-index in
addition to the variables depth and cost considered before.
The variable geology is a score that measures the
geological properties of the materials that have to be
drilled through. Harder materials have larger scores, and
so larger values of the geology variable indicate that
harder materials have to be drilled through to complete
the oil well. The variable downtime measures the number
of hours that the drilling rig is idle due to factors such as
inclement weather and interruptions for borehole and
geological tests. The variable rig-index compares the
daily rental costs of the drilling rig to the cost in 1980.
Thus, an index of 1 implies that the rental costs are
identical to those in 1980, and an index of 2 implies that
the rental costs are twice what they were in 1980.
(a) Fit the multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4
with the response variable y as the cost and with x1 as
the depth, x2 as geology score, x3 as the downtime,
and x4 as the rig-index, and make plots of cost against
each of the four input variables.
(b) Explain why the variable geology should be removed
from the model. Does this surprise you? What is the
sample correlation coefﬁcient between cost and
geology? What is the sample correlation coefﬁcient
between depth and geology? Why do you think that
geology is not needed in the model?
(c) Should any other variables be removed from the
model? What is the ﬁnal model that you would
recommend for use? (This problem is continued in
Problem 13.4.2.)
13.2.4 VO2-max Aerobic Fitness Measurements
The data set given in DS 13.2.4 extends the data set used
in Problem 12.2.6 to include an individual’s heart rate at
rest, percentage body fat, and weight, together with the
variables age and VO2-max considered before. Fit a
multiple linear regression model to assess whether the
new input variables heart rate at rest, percentage body
fat, and weight, together with age, can be used to provide
an improved model for VO2-max. What model would
you recommend? (This problem is continued in
Problem 13.4.3.)
13.2.5 A categorical input variable has three levels. How can
indicator variables be used to include it in a multiple
linear regression model?
13.2.6 Consider a multiple linear regression of y on two input
variables x1 and x2. The p-value for x1 is less than 1%,
and the p-value for x2 is greater than 10%. Suppose
that a simple linear regression is performed with y and
one input variable x2. What bounds can you put on the
p-value for x2 in the simple linear regression?
13.3
Matrix Algebra Formulation of Multiple Linear Regression
13.3.1
Matrix Representation
The linear model
yi = β0 + β1x1i + · · · + βkxki + ϵi
for 1 ≤i ≤n can be written in matrix form as
Y = Xβ + ϵ

13.3 MATRIX ALGEBRA FORMULATION OF MULTIPLE LINEAR REGRESSION
629
where Y is the n × 1 vector of observed values of the response variable
Y =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
y1
y2
.
.
.
yn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
the design matrix X is the n × (k + 1) matrix containing the values of the input variables
X =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
x11
x21
· · ·
xk1
1
x12
x22
· · ·
xk2
·
·
·
· · ·
·
·
·
·
· · ·
·
·
·
·
· · ·
·
1
x1n
x2n
· · ·
xkn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
the parameter vector β is the (k + 1) × 1 vector
β =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
β0
β1
.
.
.
βk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and ϵ is the n × 1 vector containing the error terms
ϵ =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ϵ1
ϵ2
.
.
.
ϵn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Since the error terms ϵi have independent N(0, σ 2) distributions, the vector of error terms
ϵ has a multivariate normal distribution with a zero mean vector and a covariance matrix σ 2In,
where In is the n × n identity matrix
ϵ ∼Nn(0, σ 2In)
Consequently, the response variable vector Y has the multivariate normal distribution
Y ∼Nn(Xβ, σ 2In)
so that
E(Y) = Xβ
Example 67
Car Plant Electricity
Usage
The car plant is in a southern location with a warm climate and for most of the year air-
conditioning is required to cool the plant to a working temperature of 65◦Fahrenheit. The man-
ager therefore expects that in addition to the plant’s production, the amount of air-conditioner
use required should have a signiﬁcant impact on the plant’s electricity usage. In order to

630
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
FIGURE 13.21
Car plant electricity usage data set
January
February
March
April
May
June
July
August
September
October
November
December
Electricity usage 
(million kWh)
2.48
2.26
2.47
2.77
2.99
3.05
3.18
3.46
3.03
3.26
2.67
2.53
Production 
($ million)
4.51
3.58
4.31
5.06
5.64
4.99
5.29
5.83
4.70
5.61
4.90
4.20
Cooling 
degree 
days
0
0
13
56
117
306
358
330
187
94
23
0
investigate this the variable “cooling degrees days” (CDD) is calculated for each month as
CDD =

days in month
max{average daily temperature −65, 0}
This variable can be interpreted as being the degrees of cooling that are required during the
month in order to reduce the temperature of the plant to 65◦each day.
The full data set is shown in Figure 13.21 and the multiple linear regression model
y = β0 + β1x1 + β2x2
is proposed, where the response variable y is the electricity usage, x1 is the production level,
and x2 is CDD. Notice that n = 12 and k = 2. The response variable vector and the design
matrix are
Y =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2.48
2.26
2.47
2.77
2.99
3.05
3.18
3.46
3.03
3.26
2.67
2.53
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and
X =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
4.51
0
1
3.58
0
1
4.31
13
1
5.06
56
1
5.64
117
1
4.99
306
1
5.29
358
1
5.83
330
1
4.70
187
1
5.61
94
1
4.90
23
1
4.20
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and the parameter vector β is
β =
⎛
⎝
β0
β1
β2
⎞
⎠
The least squares estimates of the parameters βi are found by minimizing the sum of
squares Q, which can be written in matrix form as
Q = (Y −Xβ)′(Y −Xβ)

13.3 MATRIX ALGEBRA FORMULATION OF MULTIPLE LINEAR REGRESSION
631
Notice that the (k + 1) × (k + 1) matrix X′X is
X′X =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
n
n
i=1 x1i
n
i=1 x2i
· · ·
n
i=1 xki
n
i=1 x1i
n
i=1 x2
1i
n
i=1 x1ix2i
· · ·
n
i=1 x1ixki
n
i=1 x2i
n
i=1 x1ix2i
n
i=1 x2
2i
· · ·
n
i=1 x2ixki
.
.
.
· · ·
.
.
.
.
· · ·
.
.
.
.
· · ·
.
n
i=1 xki
n
i=1 x1ixki
n
i=1 x2ixki
· · ·
n
i=1 x2
ki
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and the (k + 1) × 1 vector X′Y is
X′Y =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
n
i=1 yi
n
i=1 yix1i
.
.
.
n
i=1 yixki
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
so that the normal equations derived in Section 13.1.2 can be written as
X′Xβ = X′Y
As long as the input variables are not linearly related, this matrix equation can be solved to
give the vector of parameter estimates ˆβ as
ˆβ = (X′X)−1X′Y
The n × 1 vector of ﬁtted values is
ˆY =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ˆy1
ˆy2
.
.
.
ˆyn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= X ˆβ
and the n × 1 vector of residuals is
e =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
e1
e2
.
.
.
en
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= Y −ˆY =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
y1 −ˆy1
y2 −ˆy2
.
.
.
yn −ˆyn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
The sum of squares for error can then be calculated as
SSE = e′e =
n

i=1
e2
i =
n

i=1
(yi −ˆyi)2

632
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
and
ˆσ 2 = MSE =
SSE
n −k −1
Example 67
Car Plant Electricity
Usage
For this example
12

i=1
x1i = 4.51 + · · · + 4.20 = 58.62
12

i=1
x2
1i = 4.512 + · · · + 4.202 = 291.231
12

i=1
x2i = 0 + 0 + 13 + · · · + 23 + 0 = 1484
12

i=1
x2
2i = 02 + 02 + 132 + · · · + 232 + 0 = 392,028
12

i=1
x1ix2i = (4.51 × 0) + · · · + (4.20 × 0) = 7862.87
so that
X′X =
⎛
⎜
⎝
n
n
i=1 x1i
n
i=1 x2i
n
i=1 x1i
n
i=1 x2
1i
n
i=1 x1ix2i
n
i=1 x2i
n
i=1 x1ix2i
n
i=1 x2
2i
⎞
⎟
⎠=
⎛
⎝
12.0
58.6
1484.0
58.6
291.2
7862.8
1484.0
7862.8
392,028.0
⎞
⎠
This result gives
(X′X)−1 =
⎛
⎝
6.82134
−1.47412
3.74529 × 10−3
−1.47412
0.32605
−9.5962 × 10−4
3.74529 × 10−3
−9.5962 × 10−4
7.6207 × 10−6
⎞
⎠
Also,
12

i=1
yi = 2.48 + · · · + 2.53 = 34.15
12

i=1
yix1i = (2.48 × 4.51) + · · · + (2.53 × 4.20) = 169.2532
12

i=1
yix2i = (2.48 × 0) + · · · + (2.53 × 0) = 4685.06
so that
X′Y =
⎛
⎜
⎝
n
i=1 yi
n
i=1 yix1i
n
i=1 yix2i
⎞
⎟
⎠=
⎛
⎝
34.15
169.2532
4685.06
⎞
⎠

13.3 MATRIX ALGEBRA FORMULATION OF MULTIPLE LINEAR REGRESSION
633
The parameter estimates are therefore
ˆβ =
⎛
⎝
ˆβ0
ˆβ1
ˆβ2
⎞
⎠= (X′X)−1X′Y
=
⎛
⎝
6.82134
−1.47412
3.74529 × 10−3
−1.47412
0.32605
−9.5962 × 10−4
3.74529 × 10−3 −9.5962 × 10−4 7.6207 × 10−6
⎞
⎠
⎛
⎝
34.15
169.2532
4685.06
⎞
⎠
=
⎛
⎝
0.99
0.35
0.0012
⎞
⎠
so that the ﬁtted model is
y = 0.99 + 0.35x1 + 0.0012x2
The vector of ﬁtted values is
ˆY = X ˆβ =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
4.51
0
1
3.58
0
1
4.31
13
1
5.06
56
1
5.64
117
1
4.99
306
1
5.29
358
1
5.83
330
1
4.70
187
1
5.61
94
1
4.90
23
1
4.20
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎝
0.99
0.35
0.0012
⎞
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2.568
2.243
2.514
2.827
3.102
3.099
3.265
3.421
2.856
3.064
2.732
2.460
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
The residuals are then
e = Y −ˆY =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2.48
2.26
2.47
2.77
2.99
3.05
3.18
3.46
3.03
3.26
2.67
2.53
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
−
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2.568
2.243
2.514
2.827
3.102
3.099
3.265
3.421
2.856
3.064
2.732
2.460
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−0.088
0.017
−0.044
−0.057
−0.112
−0.049
−0.085
0.039
0.174
0.196
−0.062
0.070
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and the sum of squares for error is
SSE = e′e = (−0.088)2 + · · · + 0.0702 = 0.1142
The estimate of the error variance is therefore
ˆσ 2 = MSE =
SSE
n −k −1 = 0.1142
9
= 0.0127
with ˆσ =
√
0.0127 = 0.113.

634
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
The parameter estimates are unbiased
E( ˆβ) = β
and they have a multivariate normal distribution with covariance matrix σ 2(X′X)−1
ˆβ ∼Nk+1(β, σ 2(X′X)−1)
The standard error of ˆβi is therefore σ multiplied by the square root of the ith diagonal element
of the matrix (X′X)−1.
At a set of values
x∗= (1, x∗
1, x∗
2, . . . , x∗
k )′
of the input variables, the ﬁtted value of the response variable is
ˆy|x∗= x∗′ ˆβ = ˆβ0 + ˆβ1x∗
1 + · · · + ˆβkx∗
k
which is an estimate of
x∗′β = β0 + β1x∗
1 + · · · + βkx∗
k
A 1 −α conﬁdence level conﬁdence interval for this expected value of the response variable
at x∗is
β0 + β1x∗
1 + · · · + βkx∗
k ∈(ˆy|x∗−tα/2,n−k−1s.e.(ˆy|x∗), ˆy|x∗+ tα/2,n−k−1s.e.(ˆy|x∗))
where
s.e.(ˆy|x∗) = ˆσ

x∗′(X′X)−1x∗
Also, a prediction interval for a future observation obtained at x∗takes into consideration the
extra variability due to an error term ϵ and is given by
(ˆy|x∗−tα/2,n−k−1s.e.(ˆy|x∗+ ϵ), ˆy|x∗+ tα/2,n−k−1s.e.(ˆy|x∗+ ϵ))
where
s.e.(ˆy|x∗+ ϵ) = ˆσ

1 + x∗′(X′X)−1x∗
Finally, notice that
ˆY = X ˆβ = X(X′X)−1X′Y = HY
so that the n × n “hat matrix”
H = X(X′X)−1X′
transforms the vector of response values Y to the vector of ﬁtted values ˆY. The residual vector
is therefore
e = Y −ˆY = Y −HY = (In −H)Y
and it has a covariance matrix σ 2(In −H). The variance of the ith residual ei is thus
Var(ei) = σ 2(1 −hii)
where hii is the ith diagonal element of the matrix H.

13.3 MATRIX ALGEBRA FORMULATION OF MULTIPLE LINEAR REGRESSION
635
A standardized residual can therefore be calculated as
e∗
i =
ei
ˆσ√1 −hii
which is available from most computer packages. Data points that have a standardized residual
with an absolute value larger than 3 do not ﬁt the regression model very closely and the
experimenter may want to treat them as being outliers. Notice that the values of hii are usually
small, so that the standardized residuals can be approximated as
e∗
i ≃ei
ˆσ
as suggested in Chapter 12.
Example 67
Car Plant Electricity
Usage
The diagonal elements of the matrix (X′X)−1 are used to calculate the standard errors of the
parameter estimates as
s.e.( ˆβ0) = ˆσ ×
√
6.82134 =
√
0.0127 ×
√
6.82134 = 0.294
s.e.( ˆβ1) = ˆσ ×
√
0.32605 =
√
0.0127 ×
√
0.32605 = 0.0643
s.e.( ˆβ2) = ˆσ ×

7.6207 × 10−6 =
√
0.0127 ×

7.6207 × 10−6 = 3.11 × 10−4
The t-statistic for testing the null hypothesis H0 : β1 = 0 is therefore
t =
ˆβ1
s.e.( ˆβ1) =
0.35
0.0643 = 5.43
and the p-value is
p-value = 2 × P(X > 5.43) ≃0.0004
where the random variable X has a t-distribution with n −k −1 = 9 degrees of freedom.
Similarly, the t-statistic for testing the null hypothesis H0 : β2 = 0 is
t =
ˆβ2
s.e.( ˆβ2) =
0.0012
3.11 × 10−4 = 3.82
and the p-value is
p-value = 2 × P(X > 3.82) ≃0.0042
These low p-values indicate that both production level and CDD should be kept in the
model and that they are both useful in modeling the monthly electricity costs. The parameter
estimate ˆβ1 = 0.35indicatesthatwhenCDDremainsconstant,anextra1millioninproduction
is estimated to result in an additional 0.35 kWh of electricity usage. It is interesting to compare
this with the coefﬁcient ˆβ1 = 0.499 obtained in Chapter 12 without the variable CDD included
in the model. The higher value obtained in Chapter 12 can be explained by the fact that this data
set has a positive correlation between production levels and CDD. The parameter estimate
ˆβ2 = 0.0012 indicates that for a ﬁxed production level, an increase in CDD of 100◦is
estimated to result in an increase of 0.12 million kWh of electricity usage. Obviously, the
model presented here with both production level and CDD as input variables is a better model
than that calculated in Chapter 12 based only on the production level.
Suppose that next month a production level of $5.5 million is predicted and that weather
records suggest that there will be a CDD value of 150. How does the model allow the plant
manager to forecast next month’s electricity usage? At x∗= (1, 5.5, 150) the estimated

636
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
electricity usage is
ˆy|x∗= ˆβ0 + 5.5 ˆβ1 + 150 ˆβ2 = 0.99 + (5.5 × 0.35) + (150 × 0.0012) = 3.09
Since
x∗′(X′X)−1x∗
= (1, 5.5, 150)
⎛
⎝
6.82134
−1.47412
3.74529 × 10−3
−1.47412
0.32605
−9.5962 × 10−4
3.74529 × 10−3
−9.5962 × 10−4
7.6207 × 10−6
⎞
⎠
⎛
⎝
1
5.5
150
⎞
⎠
= 0.181
this ﬁtted value has a standard error of
s.e.(ˆy|x∗) = ˆσ

x∗′(X′X)−1x∗=
√
0.0127 ×
√
0.181 = 0.0479
With a critical point t0.025,9 = 2.262, a 95% conﬁdence interval for the average electricity
usage in a month with x1 = 5.5 and x2 = 150 is
(3.09 −(2.262 × 0.0479), 3.09 + (2.262 × 0.0479)) = (2.98, 3.20)
Furthermore,
ˆσ

1 + x∗′(X′X)−1x∗=
√
0.0127 ×
√
1 + 0.181 = 0.122
so that a 95% prediction interval for the electricity usage for a future month with x1 = 5.5
and x2 = 150 is
(3.09 −(2.262 × 0.122), 3.09 + (2.262 × 0.122)) = (2.81, 3.37)
Finally, it can be shown that the ﬁrst data observation has a residual
e1 = −0.088029
and the ﬁrst diagonal element of the hat matrix is
h11 = 0.156721
Consequently, the standardized residual for this point is
e∗
1 =
ei
ˆσ√1 −h11
=
−0.088029
√
0.0127√1 −0.156721
= −0.851
13.3.2
Problems
13.3.1 Consider ﬁtting the multiple linear regression model
y = β0 + β1x1 + β2x2
to the data set in DS 13.3.1.
(a) What is the 10 × 1 vector of observed values of the
response variable Y?
(b) What is the 10 × 3 design matrix X?
(c) What is the 3 × 3 matrix X′X?
(d) What is the 3 × 3 matrix (X′X)−1?
(e) What is the 3 × 1 vector X′Y?
(f) Show that the parameter estimates are
ˆβ =
⎛
⎝
0
1
29/30
⎞
⎠
(g) What is the 10 × 1 vector of predicted values of the
response variable ˆY?
(h) What is the 10 × 1 vector of residuals e?
(i) What is the sum of squares for error?
(j) Show that the estimate of the error variance is
ˆσ 2 = 17/15.

13.4 EVALUATING MODEL ADEQUACY
637
(k) What is the standard error of ˆβ1? Of ˆβ2? Should
either of the input variables be dropped from the
model?
(l) What is the ﬁtted value of the response variable when
x1 = 1 and x2 = 2? What is the standard error of this
ﬁtted value? Construct a 95% conﬁdence interval for
the expected value of the response variable when
x1 = 1 and x2 = 2.
(m) Construct a 95% prediction interval for a future value
of the response variable obtained with x1 = 1 and
x2 = 2.
13.3.2 Consider ﬁtting the multiple linear regression model
y = β0 + β1x1 + β2x2
to the data set in DS 13.3.2.
(a) What is the 8 × 1 vector of observed values of the
response variable Y?
(b) What is the 8 × 3 design matrix X?
(c) What is the 3 × 3 matrix X′X?
(d) What is the 3 × 3 matrix (X′X)−1?
(e) What is the 3 × 1 vector X′Y?
(f) Calculate the parameter estimates ˆβ.
(g) What is the 8 × 1 vector of predicted values of the
response variable ˆY?
(h) What is the 8 × 1 vector of residuals e?
(i) Show that SSE = 20.0.
(j) What is the estimate of the error variance ˆσ 2?
(k) What is the standard error of ˆβ1? Of ˆβ2? Should
either of the input variables be dropped from the
model?
(l) What is the ﬁtted value of the response variable when
x1 = x2 = 1? What is the standard error of this ﬁtted
value? Construct a 95% conﬁdence interval for the
expected value of the response variable when
x1 = x2 = 1.
(m) Construct a 95% prediction interval for a future value
of the response variable when x1 = x2 = 1.
13.3.3 The multiple linear regression model
y = β0 + β1x1 + β2x2 + β3x3
is ﬁtted to the data set in DS 13.3.3. Use matrix algebra to
derive the parameter estimates ˆβ0, ˆβ1, ˆβ2, and ˆβ3.
13.4
Evaluating Model Adequacy
In this section various diagnostic tools are discussed that provide an experimenter with in-
formation about the adequacy of the regression model. These tools include residual plots to
investigate whether the assumptions of the regression model appear to be met, and the investi-
gation of individual data points that have large standardized residuals or that have an especially
large inﬂuence on the ﬁtted regression model. The tools also provide insight into how the input
variables are related to one another and how they inﬂuence the model. A good modeler uses
this kind of information to assess the strengths and limitations of a regression model.
13.4.1
Multicolinearity of the Input Variables
The correlation structure among a set of input variables has important consequences for the
model ﬁt and the interpretation of the model, and it is sensible for the modeler to calculate
the sample correlations r between the input variables.
From a mathematical point of view, if two input variables are very highly correlated (that
is, almost colinear), then the matrix X′X is almost singular and there can be computational
difﬁculties in obtaining the inverse matrix X′X−1, which is used to calculate the parameter
estimates ˆβi. In general, computational difﬁculties and roundoff errors in ﬁtting a model can
be minimized by using standardized variables
xi −¯xi
si
in place of xi in the model, where ¯xi and si are the mean and the standard deviation of the
values taken by the input variable xi. However, a high correlation between two input variables

638
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
can still present problems, and many computer packages will alert the user with a warning
message and will automatically remove one of the variables from the model.
From a practical point of view, if two input variables xi and x j are highly correlated, then
it sufﬁces to have just one of them in the model. Little improvement in the model ﬁt can be
obtained by having both variables in the model. In this respect, it is important to remember
that if a variable xi is dropped from the model, then this does not necessarily imply that
the variable xi is not correlated with the response variable y. It may be that there is another
variable x j in the model that is correlated with the variable xi and acts as a surrogate for it.
This phenomenon was observed in Examples 71 and 44 discussed in Section 13.2.
The correlation structure among the input variables also affects the interpretation of the
model parameters βi. Remember that the parameter βi represents the change in the expected
value of the response variable y as the input variable xi is increased by one unit and when all
the other input variables are kept ﬁxed. However, if the input variables are correlated, then a
change in the variable xi is in practice likely to be associated with changes in the other input
variables so that the consequent change in the expected value of the response variable y may
be quite different from βi.
13.4.2
Residual Analysis
The concepts behind residual analysis for a multiple linear regression model are similar to
those for a simple linear regression model discussed in Chapter 12. However, they are much
more important for the multiple linear regression model because of the lack of good graphical
representations of the data set and the ﬁtted model. In simple linear regression a plot of the
response variable against the input variable showing the data points and the ﬁtted regression
line provides a good graphical summary of the regression analysis. With the higher dimensions
of a multiple linear regression model, similar plots cannot be obtained.
Plots of the residuals ei against the ﬁtted values ˆyi and against the input variables xi, as sug-
gested in Figure 13.22, may alert the experimenter to any problems with the regression model.
As discussed in Chapter 12, if all is well with the regression model, then these residual plots
should exhibit a random scatter of points. Any patterns in the residual plots should draw the
experimenter’s attention and should be investigated. For example, a funnel shape to a plot of the
residuals ei against the ﬁtted values ˆyi indicates a lack of homogeneity of the error variance σ 2.
A series of negative, positive, and then negative values in a plot of the residuals ei against an in-
put variable xi suggests that the model can be improved with the addition of a quadratic term x2
i .
In certain cases where the data observations are obtained sequentially over time, it is
also prudent to plot the residuals against a time axis, which designates the order in which
the observations are taken. A residual plot such as that shown in Figure 13.23 indicates that
there is a lack of independence in the error terms, with adjacent observations being positively
correlated. Normal probability plots can always be used to investigate whether there is any
indication that the error terms are not normally distributed.
The standardized residuals can be used to identify individual data points that do not
ﬁt the model well. Typically, computer packages alert the experimenter to points that have a
standardized residual with an absolute value larger than 2. However, remember that even in
an ideal modeling situation, about 5% of the data points are expected to have a standardized
residual with an absolute value larger than 2, and about 1% of the points are expected to have
a standardized residual with an absolute value larger than 2.5. Points with large standardized
residuals should be investigated by the experimenter to determine whether anything is strange
about them. An experimenter may want to consider them as outliers, remove them from the
data set, and reﬁt the model to the remaining data set.

13.4 EVALUATING MODEL ADEQUACY
639
Plots of residuals against
input variables
.  .  .
ei
ˆyi
Plot of residuals against
fitted values
ei
x1i
ei
xki
FIGURE 13.22
Residual plots for multiple linear regression analysis
Time
ei
FIGURE 13.23
Residual plot for data with a positive correlation over time
Example 71
Supermarket Deliveries
When a simple linear regression model is used to ﬁt time against volume, a computer package
may alert the user that the eighteenth observation has a standardized residual of e∗
i = 2.03. For
this observation, an unloading time of 35 minutes was recorded for a load of 63 m3, whereas
the ﬁtted time is
ˆy|63 = −24.16 + (0.8328 × 63) = 28.306
This value gives a residual of
ei = yi −ˆyi = 35.0 −28.306 = 6.694
and with hii = 0.093125 the standardized residual is
e∗
i =
ei
ˆσ√1 −hii
=
6.694
3.462√1 −0.093125 = 2.03
However, this standardized residual is barely larger than 2, and even with an ideal model one
out of n = 20 data points is expected to have a standardized residual larger than 2 anyhow, so
the supermarket manager need not be overly concerned about this data point.
Similarly, when the unloading time is regressed on the weight of the load the twentieth
observation is identiﬁed as having a standardized residual of e∗
i = −2.18. In this case, an

640
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
unloading time of 19 minutes was recorded for a load of 40 tons, whereas the ﬁtted time is
ˆy|40 = −16.08 + (1.250 × 40) = 33.92
This value gives a residual of
ei = yi −ˆyi = 19.0 −33.92 = −14.92
and with hii = 0.062221 the standardized residual is
e∗
i =
ei
ˆσ√1 −hii
=
−14.92
7.065√1 −0.062221 = −2.18
However, as before, the experimenter does not need to be concerned about this data point.
Example 72
Turf Quality
Figure 13.24 shows plots of the residuals ei against the ﬁtted values ˆyi, the water levels x1i,
and the fertilizer levels x2i. Each residual plot exhibits a fairly random scatter of the points,
and they do not indicate that there are any problems with the regression model.
Finally, when an experimenter has a large enough data set, a good modeling procedure
is to split the data set into two parts and to ﬁt the model to just one of the two parts. The
ﬁtted model is then applied to the second part of the data set and residuals are calculated in
the usual manner as the difference between the observed data points and the ﬁtted values.
In this case, however, standardized residuals are calculated as simply the residual ei divided
by the estimated standard error ˆσ. This modeling approach, known as cross-validation, is
particularly reassuring when the model ﬁtted from the ﬁrst part of the data set provides a close
ﬁt to the data points in the second part of the data set.
13.4.3
Inﬂuential Points
Inﬂuential data points are points that have values of the input variables xi that cause them
to have an unusually large inﬂuence on the ﬁtted model. It is prudent for the experimenter
to be aware of these points and to be conﬁdent of their accuracy. In simple linear regression
when there is only one input variable, the most inﬂuential points are those with the largest
and smallest values of the input variable, as shown in Figure 13.25. More generally when
there are two or more input variables, the most inﬂuential data points are those located on
the edges of the collection of input variable values, as shown in Figure 13.26 for two input
variables.
A good way to measure the inﬂuence of a data point is through the corresponding diagonal
element of the hat matrix hii, which is often referred to as the leverage value. Larger leverage
values hii indicate that the variable is more inﬂuential. The leverage values are nonnegative,
and when there is an intercept and k input variables in the regression model, they sum to k +1
n

i=1
hii = k + 1
With n data points the average leverage value is therefore (k + 1)/n. A general rule is that a
data point can be considered to be inﬂuential if hii > 2(k + 1)/n and to be very inﬂuential if
hii > 3(k + 1)/n.

13.4 EVALUATING MODEL ADEQUACY
641
FIGURE 13.24
Residual plots for the turf quality
example
Fitted value
Residual
7.5
0
2.5
2.5
5.0
70
30
40
50
60
5.0
Water level
Residual
7.5
0
2.5
2.5
5.0
16
2
4
6
8
10
12
14
5.0
Fertilizer level
Residual
7.5
0
2.5
2.5
5.0
35
10
15
20
25
30
5.0
Example 44
Army Physical Fitness
Test
Consider the linear regression model with run time as the dependent variable and with both
pushups and situps as the input variables. Both the twenty-fourth and thirty-second observa-
tions may be identiﬁed as being points with a large inﬂuence on the ﬁtted regression model.
The twenty-fourth observation corresponds to an ofﬁcer who performed 80 pushups and 100
situps, and the thirty-second observation corresponds to an ofﬁcer (an age 39, 5′11′′, 149-lb
major) who performed 125 pushups and 95 situps. These two points are easily identiﬁable

642
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
Most influential points
x
FIGURE 13.25
Inﬂuential data points for a simple linear
regression analysis
+
+
+
+
+
++
+ +
+ +
Most influential
points
+
+
x1
x2
FIGURE 13.26
Inﬂuential data points for a multiple linear
regression analysis with two input variables
in the plot of situps against pushups in Figure 13.20 as the topmost point and the rightmost
point, respectively.
The twenty-fourth observation has a leverage h24.24 = 0.119 and the thirty-second obser-
vation has a leverage h32.32 = 0.244. With k = 2 input variables in the model
3(k + 1)
n
= 9
84 = 0.107
and so these two observations can be considered to be very inﬂuential. However, a check of the
test records conﬁrms the accuracy of these two data points and so no further action is required.
Notice that in the simple linear regression model discussed in Chapter 12 with pushups as the
only input variable, the thirty-second observation can be seen on the far right of Figure 12.19
and is clearly very inﬂuential in determining the slope of the ﬁtted regression line.
COMPUTER NOTE
You will need to perform multiple linear regression analyses on a computer. However, this is
easily done and only requires that you enter the data and specify the response variable, the input
variables, and the linear model that you are interested in. As is always the case when using
a statistical software package, the key is to have both a good understanding of what you are
asking the computer to do and a good understanding of what the computer output is telling you.
Make good use of residual plots and other graphical devices where appropriate so that
you have a good feeling for how well your model is doing. Make sure that you are aware of
the correlations among your input variables and which data points have large standardized
residuals or are very inﬂuential.
When you have a large number of input variables to investigate, you will want to start off
using an automatic procedure to ﬁnd good models for you. Stepwise procedures incorporating
backwards elimination and forward selection procedures are useful. In addition, some software
packages can implement a “best subsets” approach whereby regression models based upon
all (or a speciﬁed number of) possible subsets of the collection of all of the input variables
are analyzed. Some criterion such as the value of the coefﬁcient of determination R2 must be
used to determine which models warrant further investigation. Often the statistic
Cr = SSEr
ˆσ2
+ 2(r + 1) −n
is used as well, where ˆσ2 is the estimate of the error variance from a full model with an intercept
and k input variables, and SSEr is the error sum of squares for the model under consideration
with an intercept and r input variables. Values of Cr close to r indicate a reasonable model,
while values of Cr much larger than r indicate a poor model.

13.5 NONLINEAR REGRESSION
643
13.4.4
Problems
13.4.1 Consider Example 70 and the data set in Figure 13.7.
(a) Make a plot of the residuals ei against the ﬁtted
values ˆyi. Make a plot of the residuals ei against
the temperature values xi. Do either of these plots
alert you to any problems with the regression
analysis?
(b) Find the standardized residuals. Are any of them
unusually large?
(c) Find the leverage values hii. Which points have the
largest inﬂuence on the ﬁtted regression line?
13.4.2 Oil Well Drilling Costs
Consider the modeling of oil well drilling costs described
in Problem 13.2.3 and the data set in DS 13.2.3. Suppose
that a model is used with cost as the dependent variable
and with depth and downtime as input variables.
(a) Make plots of the residuals against the ﬁtted values
and against each of the two input variables. What do
you ﬁnd?
(b) Make a plot of the residuals against the variable
geology. Why does this plot conﬁrm that the variable
geology is not needed in the regression model?
(c) Verify that there are no points with an unusually large
inﬂuence on the regression model.
(d) Verify that there is one point with a standardized
residual of 2.01.
13.4.3 VO2-max Aerobic Fitness Measurements
Consider the modeling of aerobic ﬁtness described in
Problem 13.2.4 and the data set in DS 13.2.4. Suppose
that a model is used with VO2-max as the dependent
variable and with heart rate at rest and percentage body
fat as input variables.
(a) Make plots of the residuals against the ﬁtted values
and against each of the two input variables. Are you
alerted to any problems with the regression model?
(b) Make a plot of the residuals against the variable
weight. Why does this plot conﬁrm that the variable
weight is not needed in the regression model?
(c) Verify that there are no points with an unusually large
inﬂuence on the regression model.
(d) Verify that there is one point with a standardized
residual of −2.15.
13.4.4 In a multiple linear regression model, suppose that
observation i has a positive residual. What can you say
about how the leverage value hii of this observation will
change if the response value yi of the observation is
increased?
13.4.5 In regression:
A. High leverage data points are those whose x values lie
close to the middle of all of the x values.
B. High leverage data points are those points that have
the greatest inﬂuence on the ﬁtted regression line.
C. Neither of the above.
D. Both of the above.
13.4.6 In multiple linear regression, residuals can be used to:
A. Detect outliers
B. Detect whether the addition of some quadratic terms
or other transformations of the input variables may
improve the model ﬁt
C. Both of the above
D. Neither of the above
13.4.7 In multiple linear regression, the ratios of the residuals to
the standardized residuals are always the same for all data
points.
A. True
B. False
13.4.8 In multiple linear regression:
A. A high leverage point cannot be an outlier.
B. Changing the scale of one of the input variables
changes the leverage values of all of the data points.
C. Neither of the above.
D. Both of the above.
13.5
Nonlinear Regression
13.5.1
Introduction
While linear regression modeling is very versatile and provides useful models for many
important applications, it is sometimes necessary to ﬁt a nonlinear model to a data set. Recall
that an intrinsically linear model is one that can be transformed into a linear format, and it is

644
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
generally best to perform such a transformation when possible. However, nonlinear regression
techniques must be applied to models that cannot be transformed into a linear format.
In nonlinear regression a function
f (x1, . . . , xk; θ1, . . . , θp)
is speciﬁed that relates the values of k input variables x1, . . . , xk to the expected value of a
response variable y. This function depends upon a set of unknown parameters
θ1, . . . , θp
which are estimated by ﬁtting the model to a data set. The function f (·) may be justiﬁed by
a theoretical argument (as in the following example) or it may simply be chosen to provide a
good ﬁt to the available data.
As with linear regression, the criterion of least squares is generally employed to ﬁt the
model. With a data set
(y1, x11, x21, . . . , xk1)
...
(yn, x1n, x2n, . . . , xkn)
consisting of n sets of values of the response variable and the k input variables, the parameter
estimates
ˆθ1, . . . , ˆθp
are therefore chosen to minimize the sum of squares
n

i=1
(yi −f (x1i, . . . , xki; θ1, . . . , θp))2
However, unlike linear regression, for a nonlinear regression problem there is in general
no simple expression that can be used to calculate the parameter estimates, and in practice they
need to be calculated by an iterative computer search procedure. These procedures generally
require the user to specify “initial guesses” of the parameter values, which need to be suitably
close to the actual values that minimize the sum of squares. The calculation of standard errors
for the parameter estimates is also rather awkward for nonlinear regression problems and they
are usually based on some general asymptotic arguments and should be treated as giving only
a general idea of the sensitivity of the parameter estimates. Residual analysis can be used in
a similar manner to linear regression problems to estimate the error variance and to assess
whether the modeling assumptions appear to be appropriate.
13.5.2
Example
Example 73
Indoor Air Pollution
Levels
This problem is discussed in the paper “Experimental Designs and Emission Rate Modeling
for Chamber Experiments” by Anthony Hayter and Mary Dowling, Atmospheric Environment
27A, 14, 2225–2234 (1993). In recent years heightened attention has been directed toward
the problem of indoor pollution levels. In particular, indoor pollution levels may be the result
of chemical emissions from objects such as carpets, paints, wallpaper, furniture, fabrics, and
other general household or ofﬁce appliances. An essential component of this investigation is
the estimation of pollutant emission rate proﬁles for these polluting substances.
These emission rate proﬁles are estimated by collecting pollutant concentration level
measurements from chamber studies as illustrated in Figure 13.27. A specimen of the sample
under consideration is placed within a chamber, and a constant air ﬂow is maintained through
the chamber with polluted air being blown out in exchange for clean ﬁltered air being drawn in.

13.5 NONLINEAR REGRESSION
645
FIGURE 13.27
Emission chamber experiment
Data
observation
(Ci, ti)
Air exchange rate N
Emission
rate R(t)
Concentration level C(t)
Specimen
Emission chamber
At various sampling times a portion of the exiting air is collected and the amount of pollutant is
measured. This measurement presents an estimate of the pollutant concentration level within
the chamber at that sampling time.
The data obtained from a chamber experiment consequently consist of paired observations
(Ci, ti) representing the chamber pollutant concentration levels Ci at times ti. These values can
be used to model the chamber pollutant concentration level time proﬁle C(t). This proﬁle, in
turn, can be used to estimate the sample emission rate time proﬁle R(t), which is the objective
of the experiment.
The concentration level C(t) and the emission rate R(t) (measured per unit area of the
emitting substance) are related by the following differential equation. If V represents the
volume of the chamber, A represents the area of the emitting substance under investigation,
and N represents the chamber air exchange rate (i.e., the proportion of the chamber’s air that
is replaced in unit time), then the conservation of pollutant mass results in the equation
V dC = ARdt −NVCdt
because during the small time interval dt, V dC is the change of pollutant mass within the
chamber that is accounted for by the difference between the mass emitted from the specimen
ARdt and the mass leaving the chamber NVCdt. This lends to the differential equation
dC(t)
dt
+ NC(t) = aR(t)
where a = A/V .
Suppose that the emission rate R(t) has an exponential decay so that it is given by
R(t; θ0, θ) = θ0e−θt
for some unknown parameters θ0 and θ. With the condition C(0) = 0, the differential equation
above can then be solved to give
C(t; θ0, θ) = aθ0

e−θt −e−Nt
N −θ
For known values of the constants a and N, the unknown parameters θ0 and θ are estimated
by ﬁtting thismodel to the data values (Ci, ti) using nonlinear regression techniques. Notice
that this model is not intrinsically linear because it cannot be transformed into a linear format.
Figure 13.28 shows an illustrative graph of this function C(t; θ0, θ). The concentration
level of the pollutant in the chamber initially rises because the specimen emits pollutant at a
rate greater than the rate at which it is blown out of the chamber. However, after a time
tmax = ln θ −ln N
θ −N
the concentration level in the chamber decreases because the rate of emission is smaller than
the rate at which the pollutant is blown out of the chamber.

646
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
FIGURE 13.28
Typical concentration level curve
for emission chamber experiment
Concentration 
C(t,θ0,θ)
ln
max
t
=
N
θ −ln
N
θ −
Time t
Time 
(hours)
0.5
1.5
2.5
4.5
8.5
24.5
48.5
72.5
144.5
168.5
196.5
216.5
240.5
Concentration 
(μg/m3)
0.219
0.397
0.410
0.549
0.333
0.243
0.163
0.132
0.019
0.031
0.027
0.023
0.018
FIGURE 13.29
Data set of carpet formaldehyde emissions
250
0
50
100
150
200
Concentration
Time
0.6
0.0
0.1
0.2
0.3
0.4
0.5
FIGURE 13.30
Fitted concentration level curve for carpet formaldehyde emissions
Figure13.29containsthedataobtainedfromanexperimentperformedtomeasureformalde-
hyde emissions from a sample of carpet. A chamber of size V = 0.053 m3 was used with a
carpet sample of area A = 0.0210 m2, so that a = 0.40 m−1. A constant air exchange rate was
maintained, which was measured to be N = 1.01 h−1. The data set shows 13 concentration
level observations that were taken within the ﬁrst 250 hours.
A computer package can be used to ﬁt the nonlinear model to the data set and to ob-
tain the parameter estimates ˆθ0 = 1.27 and ˆθ = 0.024. Figure 13.30 shows the ﬁtted curve
C(t; 1.27, 0.024) superimposed on a plot of the data points. It can be seen that the model
has done a fairly good job of ﬁtting the data points, although it has underestimated the peak
concentration level and has also consistently underestimated the concentration levels at times

13.6 CASE STUDY: INTERNET MARKETING
647
larger than 150 hours. Nevertheless, the ﬁtted emission rate
R(t) = ˆθ0 e−ˆθt = 1.27 e−0.024t
should provide some useful information on the formaldehyde emissions from the carpet, at
least within the ﬁrst six days.
13.6
Case Study: Internet Marketing
Figure 13.31 shows a quadratic regression model ﬁtted to the data in Figure 12.53. The
maximum point on the ﬁtted regression line is $426,600, so that the model suggests that
spending any more money than this on an advertising campaign is a waste.
Figure 13.32 shows purchase data for each of 15 weekly periods. The number of website
purchases each week can be related to the number of website visitors that week, and whether
or not there was a promotional campaign in operation that week, through the model
purchases from website = 660 + (1320 × promotional campaign) + (0.0389 × website visits)
where each of the two input variables is statistically signiﬁcant, and the variable “promotional
campaign” is coded 1 when there is a campaign and 0 when there is not a campaign. This
model indicates that about 3.9% of website visits result in a purchase, with an extra 1320
purchases during a week when there is a promotional campaign.
FIGURE 13.31
Relationship between website visits
and advertising costs
Advertising cost ($)
Website visits
500,000
400,000
300,000
200,000
100,000
750,000
600,000
700,000
650,000
550,000
500,000
450,000
400,000
FIGURE 13.32
Data set for the analysis of
online purchases
Promotional
Website
Purchases
Promotional
Website
Purchases
campaign
visits
from website
campaign
visits
from website
0
254,028
8,965
0
212,230
8,836
0
304,394
12,603
0
206,321
9,858
0
354,765
15,122
1
332,288
15,105
0
348,410
14,485
1
367,107
16,081
0
331,162
13,889
1
239,549
11,378
0
263,029
10,441
1
326,935
14,507
0
352,599
15,026
1
230,086
11,086
0
354,808
13,494

648
CHAPTER 13
MULTIPLE LINEAR REGRESSION AND NONLINEAR REGRESSION
13.7
Supplementary Problems
13.7.1 Consider the data set in DS 13.7.1.
(a) Plot the response variable y against the input
variable x and conﬁrm that the quadratic model
y = β0 + β1x + β2x2
appears to be appropriate.
(b) Write out the analysis of variance table using the
fact that SSE = 39.0.
(c) The parameter estimates are ˆβ0 = 18.18, ˆβ1 =
−44.90, and ˆβ2 = 44.08. The standard error of ˆβ2 is
s.e.( ˆβ2) = 7.536. Verify that the quadratic term is
needed in the model.
(d) What is the ﬁtted value of the response variable
when x = 1? If this ﬁtted value has a standard error
of s.e.(ˆy) = 1.005, construct a 95% two-sided
conﬁdence interval for the expected value of the
response variable when x = 1.
13.7.2 Use hand calculations to ﬁt the multiple linear
regression model
y = β0 + β1x1 + β2x2
to the data set in DS 13.7.2.
(a) Write down the vector of observed values of the
response variable Y and the design matrix X.
(b) Calculate X′X.
(c) Verify that
(X′X)−1 =
⎛
⎝
73/680
−1/136
0
−1/136
1/136
0
0
0
1/116
⎞
⎠
(d) Verify that ˆβ0 = 4, ˆβ1 = −3, and ˆβ2 = 1.
(e) Calculate the vector of predicted values of the
response variable ˆY and the vector of residuals e.
(f) What is the sum of squares for error?
(g) Show that the estimate of the error variance is
ˆσ 2 = 54/7.
(h) What is the standard error of ˆβ1? Of ˆβ2? Should
either of the input variables be dropped from the
model?
(i) What is the ﬁtted value of the response variable
when x1 = 2 and x2 = −2? What is the standard
error of this ﬁtted value?
(j) Construct a 95% prediction interval for a future
value of the response variable obtained with x1 = 2
and x2 = −2.
13.7.3 Friction Power Loss from Engine Bearings
DS 13.7.3 contains an extension of the data set presented
in DS 12.12.3 concerning the power loss that occurs in
the bearing of an automobile engine. In addition to the
diameter of the bearing, information is provided on the
clearance and the length of the bearing.
(a) Perform a linear regression analysis with power loss
as the dependent variable and bearing diameter, bear-
ing clearance, and bearing length as the explanatory
variables. Show that the bearing length has a p-value
of 0.331 and can be removed from the model.
(b) What is the ﬁtted model when the explanatory
variables diameter and clearance are used? Calculate
a 95% prediction interval for the power loss of an
engine that has a bearing diameter of 25 and a
bearing clearance of 0.07. Show that the largest
standardized residual in absolute value is 2.46.
13.7.4 Bacteria Cultures
The data set in DS 13.7.4 shows the yields of a bacteria
culture obtained for different amounts of an additive x1
and for different growing temperatures x2.
(a) Investigate the experimental design employed by
looking at the values of the additive levels and the
temperature levels used in the experiment.
(b) Fit the response surface model
y = β0 + β1x1 + β2x2
1 + β3x2 + β4x2
2 + β5x1x2
and show that one data point has a standardized
residual of −3.01.
(c) Remove the data point with the large negative
standardized residual and ﬁt the response surface
model to the remaining data points. Make a plot of
the ﬁtted model. What values of the additive and the
temperature would you recommend to maximize the
yield of the bacteria culture?
13.7.5 The regression model
y = −67.5 + 34.5x1 −0.44x2 + 108.6x3 + 55.8x4
is obtained from n = 44 observations. The ﬁrst
observation is y1 = 288.9, x1 = 12.3, x2 = 143.4, x3 =
−7.2, x4 = 14.4 and it has a standardized residual
−1.98 and a leverage value 0.0887. If SST = 20554,
what is R2?
13.7.6 The model y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + β5x5
is ﬁtted to a data set and ˆβ3 = −5.602 is obtained. What

13.7 SUPPLEMENTARY PROBLEMS
649
can you say about the sign (negative, zero, or positive)
of the sample correlation coefﬁcient between y
and x3?
13.7.7 Are the following statements true or false?
(a) It is necessary to perform diagnostic checks of the
ﬁt when the value of R2 is large.
(b) Multiple linear regression is different from simple
linear regression because more than one model is
provided for the response variable.
13.7.8 The multiple linear regression model
y = β0 + β1x1 + β2x2
is ﬁtted to a data set of n = 30 observations.
(a) Suppose that ˆβ1 = −45.2 and s.e.( ˆβ1) = 39.5, and
that ˆβ2 = 3.55 and s.e.( ˆβ2) = 5.92. Is it clear that
both x1 and x2 should be removed from the model?
(b) Suppose that ˆβ1 = −45.2 and s.e.( ˆβ1) = 8.6, and
that ˆβ2 = 3.55 and s.e.( ˆβ2) = 0.63. Is it clear that
you would not want to remove either x1 or x2 from
the model?
13.7.9 In multiple linear regression, a dummy variable is used
for the input variable “gender.” Experimenter 1 codes
“male = 0” and “female = 1,” while experimenter 2
codes “male = −1” and “female = 1.”
A. The experimenters will have the same estimated
coefﬁcient ˆβi for gender and the same p-value for
gender.
B. The experimenters will have the same estimated
coefﬁcient ˆβi for gender but different p-values for
gender.
C. The experimenters will have different estimated
coefﬁcients ˆβi for gender but the same p-value for
gender.
D. The experimenters will have different estimated
coefﬁcients ˆβi for gender and different p-values for
gender.
13.7.10 In multiple linear regression, if it is known that the ﬁrst
data point has a leverage value of 0.137 and a residual of
−9.54, and if R2 = 0.872, then:
A. It can be deduced that ˆσ = 2.76.
B. It can be deduced that ˆσ = 3.76.
C. It can be deduced that ˆσ = 4.76.
D. The value of ˆσ cannot be determined.
13.7.11 Carbon Footprints
Analyze the data in DS 13.7.5, which contains estimates
of the pounds of carbon dioxide released when making
several types of car, together with information on the
suggested retail price of the car and whether or not it is
an SUV.
13.7.12 A computer package may have difﬁculty ﬁtting a
multiple linear regression model if:
A. The variables are all statistically signiﬁcant.
B. The coefﬁcient of determination R2 is small.
C. Both of the above.
D. Neither of the above.
13.7.13 In multiple linear regression:
A. A binary categorical variable can be coded and used
as an input variable.
B. The output variable is a continuous variable.
C. Both of the above.
D. Neither of the above.
13.7.14 Consider a multiple linear regression model where the
output variable is a company’s revenue for different
months and the purpose is to investigate how the revenue
depends upon the company’s advertising budget. The
input variables can be time-lagged, so that the ﬁrst input
variable is the advertising budget in that month, the
second input variable is the advertising budget in the
previous month, and so on.
A. True
B. False
13.7.15 Consider a regression model y = 8 + 5x1 −3x2.
A. If the ﬁrst input variable increases by 1 and then
second input variable remains constant, then the
output variable is expected to increase by 5.
B. If the ﬁrst input variable increases by 1 and the
second input variable also increases by 1, then the
output variable is expected to increase by 2.
C. Both of the above.
D. Neither of the above.
13.7.16 In multiple linear regression, a backward elimination
procedure builds a model by starting with no input
variables and proceeds by adding and removing
variables sequentially as appropriate.
A. True
B. False
13.7.17 A computer package may have difﬁculty ﬁtting a
multiple linear regression model if:
A. There are a large number of outliers.
B. There is a very high correlation between some of the
input variables.
C. Both of the above.
D. Neither of the above.

C H A P T E R F O U R T E E N
Multifactor Experimental Design and Analysis
Yields from a chemical process, for example, may depend upon a large number of different
factors. The discussion in Chapter 11 on the analysis of variance concerns the relationship
betweenaresponsevariableofinterestandvariouslevelsofasinglefactorofinterest.However,
in many situations like this chemical process, it is useful to simultaneously investigate the
relationship between a response variable and two or more factors of interest. Multifactor
experiments of this kind are considered in this chapter.
Experiments with two factors are considered in Section 14.1. The analysis of variance
methodology is extended to analyze the structure of the relationship between the response
variable and the two factors of interest. Of considerable importance is the assessment of any
interactionbetweenthetwofactorsinthemannerinwhichtheyinﬂuencetheresponsevariable.
Extensions of the methodology to experiments with three or more factors are discussed in
Section 14.2, together with the important research tool of screening experiments, which are
used to determine which of a large number of factors have a signiﬁcant inﬂuence on a response
variable.
14.1
Experiments with Two Factors
The simplest multifactor experiment involves two factors. The analysis of a two factor experi-
ment introduces the important concept of an interaction effect between the two factors, which
is explained in Section 14.1.2.
14.1.1
Two-Factor Experimental Designs
A two-factor experiment can be used to investigate how a response variable of interest depends
on two factors of interest. The following examples illustrate such experiments.
Example 72
Turf Quality
The manager of the turf growing company is interested in how the type of fertilizer and the
temperature at which the grass is grown affect the quality of the turf. A two-factor experiment
can be performed with turf quality as the response variable and with fertilizer type and growing
temperature as the two factors.
Example 9
Car Body Assembly
Line
In an automated car body assembly line, large multispot welding machines clamp two metal
body parts together and weld them at various spots before passing them on to the next stage
of construction. The strength and effectiveness of this welding procedure are clearly of con-
siderable importance.
In one assembly line there are three supposedly similar machines in operation, and the
manager wishes to investigate whether their performances are identical. In addition, the man-
ager wishes to investigate various solder formulations that can be used by the machines. A
two-factor experiment can be performed with the strength of the weld as the response variable
and with machines and solder type as the two factors.
650

14.1 EXPERIMENTS WITH TWO FACTORS
651
FIGURE 14.1
A two-factor experiment with
a × b experimental conﬁgurations
Level 1
Level 1
Level i
Level a
Level b
Level j
Factor B
Factor A
ij
cell
...
...
...
...
...
...
...
...
Example 74
Company
Transportation Costs
A company has to transport some of its goods a considerable distance by road from the factory
where they are produced to a port from where they are exported. The driving time required has
important implications for the logistical planning of the transportation and for the company’s
transportation costs. A company manager realizes that the driving time may depend on the
route taken by the driver and the time of day that the shipment leaves the factory. A two-factor
experiment can be performed to investigate this dependency with driving time as the response
variable and with driving route and time of day as the two factors.
Suppose that the two factors in a two-factor experiment are called factor A with a levels and
factorBwithb levels.AsFigure14.1illustrates,therearethereforea×b differentcombinations
of the two factors, which are known as experimental conﬁgurations or cells. In a complete
balanced experimental design, the experimenter obtains n observations (measurements of the
response variable) from each of the ab experimental conﬁgurations, so that a total of
nT = abn
observations are obtained in the experiment. The n observations obtained at each experimen-
tal conﬁguration are referred to as replication measurements obtained at the experimental
conﬁguration.
The data set obtained from the experiment therefore consists of the values
xi jk
1 ≤i ≤a, 1 ≤j ≤b, 1 ≤k ≤n
as shown in Figure 14.2. Thus, xi jk is the value of the kth measurement obtained at the ith
level of factor A and the jth level of factor B. The cell averages, that is, the averages of the
observations obtained at each of the ab experimental conﬁgurations, are denoted by
¯xi j· = xi j1 + xi j2 + · · · + xi jn
n
= 1
n
n

k=1
xi jk
It is also useful to consider the sample averages at each level of factor A. These are
¯x1··
¯x2··
¯x3··
. . .
¯xa··
where
¯xi·· = 1
bn
b

j=1
n

k=1
xi jk

652
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.2
Data observations and sample
averages for a two-factor
experiment
··· x
Level 1
Level 1
Level i
Level a
Level b
Factor A
¯x.1.
¯x.j.
¯x1..
¯xi..
¯xa
¯···
Factor B
level
sample
averages
Factor A
level
sample
averages
Overall
average
...
...
...
...
...
...
...
...
...
xij1,
,xijn
Average
(
)
¯xij. 
Level j
Factor B
¯x.b.
are the averages of all the bn data observations obtained at each of the levels of factor A.
Similarly, the sample averages at each level of factor B are
¯x·1·
¯x·2·
¯x·3·
. . .
¯x·b·
where
¯x· j· = 1
an
a

i=1
n

k=1
xi jk
The average of all the abn data observations is
¯x··· =
1
abn
a

i=1
b

j=1
n

k=1
xi jk
These sample averages are illustrated in Figure 14.2.
Two-Factor Experiments
Consider an experiment with two factors, A and B. If factor A has a levels and
factor B has b levels, then there are ab experimental conﬁgurations or cells. In a
complete balanced experiment n replicate measurements are taken at each
experimental conﬁguration, resulting in a total sample size of nT = abn data
observations. The data observation xi jk represents the value of the kth measurement
obtained at the ith level of factor A and the jth level of factor B. The cell sample
averages are denoted by ¯xi j., the sample averages at the levels of factor A are denoted
by ¯xi··, the sample averages at the levels of factor B are denoted by ¯x· j·, and the overall
sample average of all abn data observations is denoted by ¯x···.
Example 72
Turf Quality
There are two fertilizers of interest, fertilizer 1 and fertilizer 2, and two growing temperatures,
low and high. Suppose that fertilizer type is considered to be factor A and growing temperature
is considered to be factor B. Since each factor has two levels, a = b = 2 and there are ab = 4
experimental conﬁgurations.

14.1 EXPERIMENTS WITH TWO FACTORS
653
FIGURE 14.3
Data set for the turf quality example
Fertilizer 1
x
= 37
212
x
= 41
214
x
= 41
213
x
= 47
211
¯x
= 41.50
21 .
)
(
Fertilizer 2
Low
Temperature
x
= 76
222
x
= 79
224
x
= 77
223
x
= 84
221
¯x
= 79.00
22 .
)
(
x
= 46
121
x
= 46
124
x
= 48
123
x
= 53
122
¯x
= 48.25
12 .
)
(
High
¯x
= 35.250
1. .
¯x
= 63.625
2. .
¯x
= 60.250
2 ..
¯x
= 49.4375
...
¯x
= 38.625
1 ..
Fertilizer
x
= 40
111
x
= 21
114
x
= 31
113
x
= 24
112
x
= 29.00
11 .
)
(¯
Sixteen turf samples are obtained, with four allocated to each experimental conﬁguration
in a random manner. This arrangement produces n = 4 replicate observations for each experi-
mental conﬁguration. The experiment is conducted by applying the appropriate fertilizer type
to the turf samples and placing them in the appropriate growing temperatures. After a suitable
amount of time, each turf sample is given a score relating to the quality of the grass. The result-
ing data set is shown in Figure 14.3. Recall that higher scores correspond to better quality grass.
Example 9
Car Body Assembly
Line
Suppose that the different machines are taken to be factor A, which consequently has a = 3
levels. If four different solder formulations are to be investigated, then factor B has b = 4
levels. Consequently there are ab = 12 experimental conﬁgurations, and the experiment is
run by conducting n = 3 separate weldings at each of these experimental conﬁgurations. The
data set obtained from measuring the strengths of the weldings is shown in Figure 14.4.
Example 74
Company
Transportation Costs
There are two different driving routes from the factory to the port, route 1 and route 2, and the
time of the day when the truck leaves the factory is classiﬁed as being either in the morning,
the afternoon, or the evening. Driving route can be taken to be factor A with a = 2 levels, and
period of day can be taken to be factor B with b = 3 levels.
Ten trucks leaving the factory in each of the three time periods are randomly chosen, and
ﬁve of them are selected at random to take route 1 and the other ﬁve are selected to take
route 2. The driving times of the trucks are recorded and are shown in Figure 14.5. Notice
that there are ab = 6 experimental conﬁgurations with n = 5 replicate observations at each
of the experimental conﬁgurations.
14.1.2
Models for Two-Factor Experiments
The unknown parameters of interest in a two-factor experiment are the cell means
μi j
1 ≤i ≤a, 1 ≤j ≤b

654
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.4
Data set for the car body assembly
line example
x
= 4.12
111
x
= 6.40
212
x
= 5.27
213
x
= 3.74
211
x
= 6.11
113
x
= 4.21
112
x
= 3.44
121
x
= 4.41
222
x
= 4.98
223
x
= 3.93
221
x
= 3.75
123
x
= 3.32
122
¯x
= 4.81
11
)
(
.
¯x
= 5.14)
21
(
.
x
= 3.79
312
x
= 4.47
313
x
= 6.17
311
¯x
= 4.81
31
)
(
.
¯x
= 3.50)
12
(
.
¯x
= 4.44)
22
(
.
x
= 4.03
322
x
= 3.32
323
x
= 3.05
321
¯x
= 3.47)
32
(
.
x
= 6.09
131
x
= 5.15
232
x
= 6.75
233
x
= 5.00
231
x
= 3.38
133
x
= 4.46
132
¯x
= 4.64)
13
(
.
¯x
= 5.63
23
)
(
.
x
= 4.49
332
x
= 3.49
333
x
= 4.88
331
¯x
= 4.29)
33
(
.
x
= 7.88
141
x
= 5.87
242
x
= 8.50
243
x
= 6.43
241
x
= 7.22
143
x
= 6.75
142
¯x
= 7.28)
14
(
.
¯x
= 6.93
24
)
(
.
x
= 7.10
342
x
= 7.60
343
x
= 8.31
341
¯x
= 7.67)
34
(
.
Solder 2
Solder
Machine
Machine 1
Machine 2
Machine 3
Solder 1
¯x
= 4.92
1.
.
¯x
= 3.80
2.
.
Solder 3
¯x
= 4.85
3.
.
Solder 4
¯x
= 7.30
4.
.
¯x
= 5.06
1 ..
¯x
= 5.54
2
¯x
= 5.06
3
¯x
= 5.22
...
....
..
FIGURE 14.5
Driving times in minutes for the
company transportation costs
example
x111 = 490
x112 = 553
x113 = 489
x114 = 504
x115 = 519
x11· = 511.0
x121 = 511
x122 = 490
x123 = 489
x124 = 492
x125 = 451
x12· = 486.6)
x131 = 435
x132 = 468
x133 = 463
x134 = 450
x135 = 444
x13· = 452.0
x211 = 485
x212 = 489
x213 = 475
x214 = 470
x215 = 459
x21· = 475.6)
x221 = 456
x222 = 460
x223 = 464
x224 = 485
x225 = 473
x22· = 467.6
x231 = 406
x232 = 422
x233 = 459
x234 = 442
x235 = 464
x23· = 438.6
x1·· = 483.2
x2·· = 460.6
x·1· = 493.3
x·2· = 477.1
x·3· = 445.3
x··· = 471.9
Morning
Afternoon
Evening
Period of Day
Route 1
Route 2
Route
¯
¯
¯
¯
¯
¯
¯
¯
¯
¯
¯
¯
(
)
)
)
)
(
(
(
(
(

14.1 EXPERIMENTS WITH TWO FACTORS
655
FIGURE 14.6
Cell means for a two-factor
experiment
Level 1
Level i
Level a
Level 1
Level b
Factor A
µ11
µi1
µa1
µij
µ1j
µ1b
µaj
µib
µab
...
...
...
...
...
...
...
...
Level j
Factor B
As illustrated in Figure 14.6, these are the unknown mean values at each of the ab experi-
mental conﬁgurations, and they represent the expected values of the response variable at these
experimental conﬁgurations.
A measurement obtained at the ith level of factor A and the jth level of factor B is taken
to be an observation from a normally distributed random variable with a mean μi j and with
some variance σ 2. Thus, an observation xi jk is taken to be an observation from a
N(μi j, σ 2)
distribution.
Equivalently, the observation can be written as
xi jk = μi j + ϵi jk
where the error term ϵi jk is taken to be an observation from a
N(0, σ 2)
distribution. An observation taken at the ith level of factor A and the jth level of factor B can
therefore be thought of as being composed of the unknown cell mean μi j together with an
error term that is normally distributed with mean zero and some unknown variance σ 2. The
error terms are assumed to be distributed independently of each other.
The cell means can be estimated by the sample averages within the cells, so that
ˆμi j = ¯xi j.
However, the objective of the analysis of a two-factor experiment is not simply to estimate the
ab cell means μi j, but rather to investigate the structure of these cell means. This structure
provides an indication of how the expected value of the response variable depends on the
two factors under consideration. Speciﬁcally, it is useful to consider three aspects of this
relationship:
■
the interaction effect between factors A and B
■
the main effect of factor A
■
the main effect of factor B

656
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
There is an interaction effect between factors A and B if the effect of one factor on the
response variable is different at the various levels of the other factor. In other words, there is an
interaction effect if the effects that different levels of factor A have on the response variable de-
pendonwhichleveloffactorBisemployed.Similarly,thereisaninteractioneffectiftheeffects
that different levels of factor B have on the response variable depend on which level of factor A
is employed. At an intuitive level, it can be seen that the presence of an interaction between
two factors implies that they do not act independently of each other on the response variable.
Suppose that there is no interaction effect between the two factors A and B. What then
is the effect on the response variable of the different levels of factor A? This is the main
effect of factor A. In other words, the main effect of factor A assesses how changing the
levels of factor A inﬂuences the response variable. Notice that it makes sense to consider
this main effect only when there is no interaction between the two factors, because if there
is an interaction between the two factors, the effect that different levels of factor A have on
the response variable depends on which level of factor B is employed. In a similar manner,
when there is no interaction effect between the two factors A and B, the effect on the response
variable of the different levels of factor B is the main effect of factor B.
A graphical illustration of the cell means μi j in the case where both factors have two levels
(a = b = 2) helps to clarify the interpretation of the main effects and the interaction effect.
In this case there are four experimental conﬁgurations and the four cell means μ11, μ12, μ21,
and μ22 can be plotted. Figure 14.7 shows a situation where there is no interaction effect and
no main effects, so that the cell means are not affected by changes in the levels of the two
factors. In other words, the expected value of the response variable is not inﬂuenced by either
of the two factors considered.
Figure 14.8 shows a situation where there is no interaction effect and no main effect for
factor B, but there is a main effect for factor A. In this case the values of the cell means depend
upon which level of factor A is employed, but they are not affected by which level of factor B is
employed. The interpretation of this model is that the response variable is inﬂuenced by factor
A but not by factor B. Similarly, Figure 14.9 shows a situation where there is no interaction
FIGURE 14.7
Interpretation of models for
two-factor experiments
No interaction effect
No main effects
µ11
µ12
µ21
µ22
µij
1
2
Factor A
Factor B
1
2
µij
µ11
µ12
µ21
µ22
1
2
Factor A

14.1 EXPERIMENTS WITH TWO FACTORS
657
FIGURE 14.8
Interpretation of models for
two-factor experiments
No interaction effect
Factor A main effect
No factor B main effect
µij
µ11
µ12
µ21
µ22
1
2
Factor A
µ12
µij
1
2
Factor A
Factor B
1
2
µ21
µ11
µ22
FIGURE 14.9
Interpretation of models for
two-factor experiments
µij
1
2
Factor A
µ11
µ12
µ21
µ22 Factor B Level 2
Factor B Level 1
No interaction effect
No factor A main effect
Factor B main effect
µ11
µ12
µ21
µ22
µij
1
2
Factor A
Factor B
2
1
effect and no main effect for factor A, but there is a main effect for factor B, so that the
response variable is inﬂuenced by factor B but not by factor A.
Figure 14.10 shows a situation where there is no interaction effect but there are main effects
for factors A and B, and Figure 14.11 shows a situation where there is an interaction effect. The
important distinction between these two cases is that when there is no interaction the plotted
lines in the right-hand diagram are parallel to each other, but when there is an interaction the

658
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
µij
1
2
Factor A
µ11
µ12
µ21
µ22
Factor B Level 2
Factor B Level 1
No interaction effect
Factor A main effect
Factor B main effect
µ11
µ12
µ21
µ22
µij
1
2
Factor A
Factor B
2
1
FIGURE 14.10
Interpretation of models for two-factor experiments
µ11
µ12
µ21
µ22
µij
1
2
Factor A
Factor B
2
1
µij
1
2
Factor A
Factor B Level 2
Factor B Level 1
(lines not parallel)
µ12
µ22
µ11
µ21
Interaction effect
FIGURE 14.11
Interpretation of models for two-factor experiments

14.1 EXPERIMENTS WITH TWO FACTORS
659
lines are not parallel. When the lines are parallel, the change in the cell means due to a change
in the level of factor B is the same for both levels of factor A, because
μ11 −μ12 = μ21 −μ22
Similarly,
μ11 −μ21 = μ12 −μ22
so that the change in the cell means due to a change in the level of factor A is the same for both
levels of factor B. This means that the changes in the expected value of the response variable
resulting from changes in the levels of one factor are the same for all levels of the other factor.
Hence the two factors inﬂuence the response variable in an independent manner. On the other
hand, when there is an interaction effect and the lines are not parallel, it can be seen that
μ11 −μ12 ̸= μ21 −μ22
for example, so that the effect on the response variable due to a change in the level of factor B
is different at the two levels of factor A.
Of course, in practice these plots of the cell means cannot be constructed because the
cell means are unknown. Corresponding plots using the cell sample averages ˆμi j = ¯xi j· in
place of the cell means can be made, but they are often difﬁcult to interpret due to the error
variability in the cell sample averages. Even if there is no interaction, the plots of the cell
sample averages will generally have lines that are not parallel. However, hypothesis tests of
the presence of interaction effects and factor main effects can be conducted, and they are
discussed in Section 14.1.3.
The interpretation of interaction effects and factor main effects are illustrated in the fol-
lowing examples.
Example 72
Turf Quality
The interpretations of the various models including and excluding an interaction effect and
factor main effects are shown in Figure 14.12. Suppose that fertilizer 1 produces on average
higher quality turf than fertilizer 2 when a low growing temperature is used, but that the two
fertilizers have identical effects when a high growing temperature is used. What model does
this correspond to? Clearly this is an example of a model with an interaction effect, because
 Fertilizer main effect             No                        Yes                      No                     Yes
 Temperature main effect      No                        No                       Yes                     Yes
 Interaction effect                    No                        No                        No                      No                        Yes
Quality of Turf
Does not
depend on
either
fertilizer or 
temperature.
Depends
on fertilizer
but not on
temperature.
Depends
on tempera-
ture but not 
on fertilizer.
Depends
on both
fertilizer and
temperature.
Independent
influences.
Interpretation
Depends
on both
fertilizer and
temperature.
Influences not
independent.
FIGURE 14.12
Model interpretation for the turf quality example

660
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
the effect of the level of factor A, fertilizer, depends on what level of factor B, growing
temperature, is employed. If there is no interaction, then there is the same difference between
the two fertilizers at each of the two growing temperatures.
Example 9
Car Body Assembly
Line
In this example an interaction effect has the interpretation that the relative efﬁciencies of
the four solder formulations are different for the three machines. This would appear to be
an unlikely prospect because the three machines are supposed to be identical. If there is no
interaction effect, then the presence of a factor A (machines) main effect indicates that the
machines are actually operating differently, and the presence of a factor B (solder) main effect
indicates that different solder formulations have different effects on the strength of the welding.
Example 74
Company
Transportation Costs
Suppose that route 1 always takes on average 45 minutes longer to drive than route 2 regardless
of the period of day when the truck leaves the factory. What model does this correspond to?
Clearly there is no interaction effect, because the effect of factor A, the route taken, is the same
for all levels of factor B, the period of day. There is also a factor A main effect because the
average driving time is different for the two routes. Is there a factor B main effect? There may
or may not be. If for each particular route driven the average driving times are the same for
each period of day, then there is no factor B main effect. If these average driving times are
different, then there is a factor B main effect.
The presence or absence of an interaction effect and main effects can be investigated
mathematically by introducing parameters μ, αi, β j, and (αβ)i j so that the cell means μi j can
be decomposed as
μi j = μ + αi + β j + (αβ)i j
From a technical point of view, in order for these parameters to be uniquely deﬁned it is also
necessary to impose the conditions
a

i=1
αi = 0
b

j=1
β j = 0
a

i=1
(αβ)i j = 0
b

j=1
(αβ)i j = 0
The motivation behind this representation of the cell means is that it provides a clear
indication of the presence or absence of the interaction and main effects. For example, if some
of the parameters (αβ)i j are nonzero, then this indicates that there is an interaction effect
between the two factors. On the other hand, there is no interaction effect between the two
factors if all of the parameters (αβ)i j are equal to 0.
Consequently, the hypothesis testing problem
H0 : (αβ)i j = 0
1 ≤i ≤a, 1 ≤j ≤b
versus
HA : some (αβ)i j ̸= 0
can be used to assess the evidence that there is an interaction effect between the two factors. The
null hypothesis corresponds to no interaction effect and the alternative hypothesis corresponds
to the presence of an interaction effect. A large p-value leading to acceptance of the null
hypothesis implies that it is plausible that there is no interaction effect, so that the two factors
can be considered to act independently of each other. On the other hand, a small p-value
leading to rejection of the null hypothesis establishes evidence of an interaction effect, so
that it is known that the two factors do not inﬂuence the response variable in an independent
manner.

14.1 EXPERIMENTS WITH TWO FACTORS
661
If there is no interaction effect between the two factors, then the main effect of factor A
can be investigated by considering the parameters αi and the main effect of factor B can be
investigated by considering the parameters β j. If the parameters αi are all equal to 0, then this
implies that the expected value of the response variable does not depend upon which level of
factor A is employed. In other words, the response variable does not depend on factor A. If
some of the parameters αi are nonzero, then the expected value of the response variable does
depend upon which level of factor A is employed.
The hypothesis testing problem
H0 : αi = 0
1 ≤i ≤a
versus
HA : some αi ̸= 0
can be used to assess the evidence that the response variable is inﬂuenced by factor A. The
null hypothesis corresponds to the situation where the expected value of the response variable
does not depend on the levels of factor A. Acceptance of the null hypothesis can therefore
be interpreted as indicating that there is no evidence that the response variable depends on
factor A. Rejection of the null hypothesis establishes evidence that the response variable does
indeed depend on the levels of factor A.
Similarly, the hypothesis testing problem
H0 : β j = 0
1 ≤j ≤b
versus
HA : some β j ̸= 0
can be used to assess the evidence that the response variable is inﬂuenced by factor B. Ac-
ceptance of the null hypothesis indicates that there is no evidence that the response variable
depends on factor B, and rejection of the null hypothesis establishes evidence that the response
variable does indeed depend on the levels of factor B.
In Section 14.1.3 it is shown how an analysis of variance approach can be used to test
these hypotheses. First the null hypothesis of no interaction is tested, and if this hypothesis
is accepted, then hypothesis tests of the factor A and factor B main effects can be performed.
If the null hypothesis of no interaction is rejected, then the presence of an interaction effect
has been established and the hypothesis tests of the factor A and factor B main effects are
redundant and don’t need to be considered.
Modeling Two-Factor Experiments
An observation xi jk in a two-factor experiment is taken to consist of an unknown cell
mean μi j together with an error term ϵi jk that is assumed to be an observation from a
normal distribution with mean zero and unknown variance σ 2. The cell mean μi j
represents the expected value of the response variable at that experimental
conﬁguration, and the structure of the cell means determines how the two factors
affect the response variable. The presence of an interaction effect between the two
factors indicates that the two factors do not operate independently of each other. If
there is no interaction effect, then the presence or absence of main effects for each of
the two factors indicates whether or not they inﬂuence the response variable.
14.1.3
Analysis of Variance Table
The analysis of variance table is based on a decomposition of the total sum of squares into
sums of squares for the factor A and B main effects, the interaction effect, and an error sum
of squares. Point estimates of the parameters μ, αi, β j, and (αβ)i j are ﬁrst obtained in order
to motivate the construction of these sums of squares.

662
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
Notice that
E(¯xi j·) = 1
n
n

k=1
E(xi jk) = μi j = μ + αi + β j + (αβ)i j
Also, the conditions on the summations of the parameters αi, β j, and (αβ)i j being equal to 0
implies that
E(¯xi··) = 1
bn
b

j=1
n

k=1
E(xi jk) = 1
b
b

j=1
μi j
= 1
b
b

j=1
(μ + αi + β j + (αβ)i j) = μ + αi
E(¯x· j·) = 1
an
a

i=1
n

k=1
E(xi jk) = 1
a
a

i=1
μi j
= 1
a
a

i=1
(μ + αi + β j + (αβ)i j) = μ + β j
E(¯x···) =
1
abn
a

i=1
b

j=1
n

k=1
E(xi jk) = 1
ab
a

i=1
b

j=1
μi j
= 1
ab
a

i=1
b

j=1
(μ + αi + β j + (αβ)i j) = μ
These expectations can be used to derive the following unbiased point estimates of the
unknown parameters μ, αi, β j, and (αβ)i j:
ˆμ = ¯x···
ˆαi = ¯xi·· −ˆμ = ¯xi·· −¯x···
ˆβ j = ¯x· j· −ˆμ = ¯x· j· −¯x···
ˆ
(αβ)i j = ¯xi j· −ˆμ −ˆαi −ˆβ j = ¯xi j· −¯xi·· −¯x· j· + ¯x···
The sums of squares for factor main effects and for the interaction effect are based on the
parameter estimates ˆαi, ˆβ j, and
ˆ
(αβ)i j as shown next.
Total Sum of Squares
The total sum of squares for a two-factor experiment is
SST =
a

i=1
b

j=1
n

k=1
(xi jk −¯x···)2 =
a

i=1
b

j=1
n

k=1
x2
i jk −abn ¯x2···
which measures the total variability of all abn data observations about their overall mean
value ¯x···. As shown in Figure 14.13, this total sum of squares can be decomposed into a sum
of squares for the factor A main effect SSA, a sum of squares for the factor B main effect
SSB, a sum of squares for the interaction effect between factors A and B SSAB, and an error
sum of squares SSE, so that
SST = SSA + SSB + SSAB + SSE

14.1 EXPERIMENTS WITH TWO FACTORS
663
FIGURE 14.13
The sum of squares decomposition
for a two-factor experiment
SST
SSA
SSB
SSAB
SSE
Main factor
sum of
squares
Interaction
sum of
squares
Error
sum of
squares
Sum of Squares for Factor A
The sum of squares for the factor A main effect SSA measures
the presence or absence of a factor A main effect based on the estimated parameter values ˆαi.
It is deﬁned to be
SSA = bn
a

i=1
ˆα2
i = bn
a

i=1
(¯xi·· −¯x···)2 = bn
a

i=1
¯x2
i·· −abn ¯x2···
Sum of Squares for Factor B
The sum of squares for the factor B main effect SSB measures
the presence or absence of a factor B main effect based on the estimated parameter values ˆβ j.
It is deﬁned to be
SSB = an
b

j=1
ˆβ2
j = an
b

j=1
(¯x· j· −¯x···)2 = an
b

j=1
¯x2
· j· −abn ¯x2···
Sum of Squares for Interaction
The sum of squares for the interaction effect between
factors A and B SSAB measures the presence or absence of an interaction effect based on the
estimated parameter values
ˆ
(αβ)i j. It is deﬁned to be
SSAB = n
a

i=1
b

j=1
ˆ
(αβ)
2
i j = n
a

i=1
b

j=1
(¯xi j· −¯xi·· −¯x· j· + ¯x···)2
Error Sum of Squares
The sum of squares for error SSE measures the variability within
each of the ab experimental conﬁgurations. It is deﬁned to be
SSE =
a

i=1
b

j=1
n

k=1
(xi jk −¯xi j·)2 =
a

i=1
b

j=1
n

k=1
x2
i jk −n
a

i=1
b

j=1
¯x2
i j·
so that it is based on the variability of the data observations xi jk about the cell sample averages
¯xi j.. It will be seen that the sum of squares for error is used to estimate the error variance σ 2.
These sum of squares are used to construct an analysis of variance table as shown in
Figure 14.14. Notice that there are rows for the factor A main effect, the factor B main effect,
the interaction effect (AB), and error, together with the total of these four components. Mean
sums of squares are calculated from the sums of squares in the usual manner by dividing the

664
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
Degrees of freedom
Sum of squares
Mean squares
F-statistics
p-value
Source
Factor A
Factor B
AB interaction
P(Fa−1,ab(n−1) > FA)
P(Fb−1,ab(n−1) >FB)
P(F(a−1)(b−1), ab(n−1) > FAB)
Error
Total
a −1
b −1
(a −1)(b −1)
ab(n −1)
abn −1
SSA
SSB
SSAB
SSE
SST
MSA = SSA
a−1
MSB = SSB
b−1
MSAB =
SSAB
(a−1)(b−1)
MSE =
SSE
ab(n−1)
FA = MSA
MSE
FB = MSB
MSE
FAB = MSAB
MSE
FIGURE 14.14
Analysis of variance table for a two-factor experiment
sums of squares by their respective degrees of freedom. For this two-factor experiment the
degrees of freedom are
degrees of freedom for factor A main effect = a −1
degrees of freedom for factor B main effect = b −1
degrees of freedom for AB interaction effect = (a −1)(b −1)
degrees of freedom for error = ab(n −1)
The total degrees of freedom are abn −1, one less than the total number of data observations.
As in the analysis of variance tables discussed in previous chapters, the mean square error
MSE is used as the estimate ˆσ 2 of the error variance. It has the scaled chi-square distribution
ˆσ 2 = MSE =
SSE
ab(n −1) ∼σ 2 χ2
ab(n−1)
ab(n −1)
and is an unbiased estimate of the error variance
E(MSE) = σ 2
The other mean sums of squares have expectations
E(MSA) = σ 2 +
bn
a −1
a

i=1
α2
i
E(MSB) = σ 2 +
an
b −1
b

j=1
β2
j
E(MSAB) = σ 2 +
n
(a −1)(b −1)
a

i=1
b

j=1
(αβ)2
i j
so that their sizes relative to the mean square error MSE indicate whether or not the parameters
αi, β j, and (αβ)i j can be taken to be 0. Three different F-statistics can be calculated, which
can be used to test for the presence of an interaction effect and for the two-factor main effects.
Testing for Interaction Effects
The hypothesis testing problem
H0 : (αβ)i j = 0
1 ≤i ≤a, 1 ≤j ≤b
versus
HA : some (αβ)i j ̸= 0

14.1 EXPERIMENTS WITH TWO FACTORS
665
is used to assess the evidence of an interaction effect and can be performed with the F-statistic
FAB = MSAB
MSE
When the null hypothesis is true and there is no interaction effect, the statistic FAB has an
F-distribution with degrees of freedom (a −1)(b −1) and ab(n −1). Large values of the
statistic FAB suggest that the null hypothesis is not true, and a p-value can be calculated as
p-value = P(X > FAB)
where the random variable X has an F-distribution with degrees of freedom (a −1)(b −1)
and ab(n −1).
If a small p-value is obtained in the test for an interaction effect, then the experimenter
has established that the two factors do not operate independently of each other in the manner
in which they inﬂuence the response variable. If a large p-value is obtained, however, so that
there is no evidence of an interaction effect, then it is appropriate to perform hypothesis tests
for the presence of main effects of the two factors.
Testing for Factor A Main Effects
The hypothesis testing problem
H0 : αi = 0
1 ≤i ≤a
versus
HA : some αi ̸= 0
is used to assess the evidence of a factor A main effect and can be performed with the F-statistic
FA = MSA
MSE
When the null hypothesis is true and there is no factor A main effect, the statistic FA has an
F-distribution with degrees of freedom a −1 and ab(n −1). Large values of the statistic FA
suggest that the null hypothesis is not true, and a p-value can be calculated as
p-value = P(X > FA)
wheretherandomvariable X hasan F-distributionwithdegreesoffreedoma−1andab(n−1).
Testing for Factor B Main Effects
The hypothesis testing problem
H0 : β j = 0
1 ≤j ≤b
versus
HA : some β j ̸= 0
is used to assess the evidence of a factor B main effect and can be performed with the F-statistic
FB = MSB
MSE
When the null hypothesis is true and there is no factor B main effect, the statistic FB has an
F-distribution with degrees of freedom b −1 and ab(n −1). Large values of the statistic FB
suggest that the null hypothesis is not true, and a p-value can be calculated as
p-value = P(X > FB)
wheretherandomvariable X hasan F-distributionwithdegreesoffreedomb−1andab(n−1).
COMPUTER NOTE
Computer packages can be used to construct the analysis of variance table for a two-factor
experiment, and the p-values will be reported for each of the three hypothesis tests described
above. However, the experimenter should remember to look ﬁrst at the p-value for the hy-
pothesis test concerning the interaction effect. If there is evidence of an interaction effect,
then the p-values for the hypothesis tests of the factor A and factor B main effects are really
not important.

666
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.15
The analysis of variance table for
the turf quality example
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Fertilizer
1
1870.6
1870.6
66.86
0.000
Temperature
1
3220.6
3220.6
115.11
0.000
Fertilizer*Temperature
1
333.1
333.1
11.90
0.005
Error
12
335.8
28.0
Total
15
5759.9
Example 72
Turf Quality
The analysis of variance table for this two-factor problem is shown in Figure 14.15. The
overall average of the data observations is
¯x··· = x111 + · · · + x224
16
= 40 + · · · + 79
16
= 49.4375
and
2

i=1
2

j=1
4

k=1
x2
i jk = 402 + · · · + 792 = 44,865
so that
SST =
2

i=1
2

j=1
4

k=1
x2
i jk −(2 × 2 × 4 × ¯x2
···)
= 44,865 −(16 × 49.43752) = 5,759.9375
The cell sample averages are
¯x11· = 29.00,
¯x12· = 48.25,
¯x21· = 41.50,
and
¯x22· = 79.00
so that
2

i=1
2

j=1
¯x2
i j· = 29.002 + 48.252 + 41.502 + 79.002 = 11,132.3125
Consequently, the sum of squares for error is
SSE =
2

i=1
2

j=1
4

k=1
x2
i jk −

4 ×
2

i=1
2

j=1
¯x2
i j·

= 44,865 −(4 × 11,132.3125) = 335.75
With fertilizer level sample averages
¯x1·· = 38.625
and
¯x2·· = 60.250
the sum of squares for the fertilizer main effect is
SSA =

2 × 4 ×
2

i=1
¯x2
i··

−(2 × 2 × 4 × ¯x2···)
= (2 × 4 × (38.6252 + 60.2502)) −(16 × 49.43752) = 1870.5625

14.1 EXPERIMENTS WITH TWO FACTORS
667
Similarly, the growing temperature level sample averages are
¯x·1· = 35.250
and
¯x·2· = 63.625
so that the sum of squares for the growing temperature main effect is
SSB =

2 × 4 ×
2

j=1
¯x2
· j·

−(2 × 2 × 4 × ¯x2···)
= (2 × 4 × (35.2502 + 63.6252)) −(16 × 49.43752) = 3220.5625
The sum of squares for interaction SSAB can be calculated from its formula in terms of
the sample averages, but it is now easiest to calculate it by subtracting the sums of squares
already calculated from the total sum of squares, so that
SSAB = SST −SSA −SSB −SSE
= 5759.9375 −1870.5625 −3220.5625 −335.75 = 333.0625
These sums of squares are seen to agree with the (rounded) values given in Figure 14.15,
where the row in the analysis of variance table labeled “Fertizer*Temperature” corresponds
to the interaction.
Notice that there are ab(n −1) = 2 × 2 × (4 −1) = 12 degrees of freedom for error so
that the estimate of the error variance is
ˆσ 2 = MSE = SSE
12 = 335.75
12
= 27.98
The estimate of the error standard deviation is therefore ˆσ =
√
27.98 = 5.29.
Also, there is (a −1)(b −1) = (2 −1) × (2 −1) = 1 degree of freedom for the AB
interaction effect, so that
MSAB = SSAB
1
= SSAB = 333.1
The F-statistic for testing the presence of an interaction effect is therefore
FAB = MSAB
MSE = 333.1
27.98 = 11.90
and the p-value for the null hypothesis that there is no interaction effect is
p-value = P(X > 11.90) = 0.005
where the random variable X has an F-distribution with degrees of freedom 1 and 12. This
small p-value indicates that there is evidence of an interaction effect between fertilizer and
growing temperature. The presence of an interaction effect implies that it is not important to
investigate the main effects of the fertilizer and the growing temperature.
The manager of the turf growing company has discovered from this experiment that the
difference between the two fertilizers depends on which growing temperature is used. The
plot of the cell sample averages shown in Figure 14.16 shows that fertilizer 2 always appears
to produce higher quality turf than fertilizer 1 regardless of the growing temperature, but that
while the increase in average quality is only about 10 units at the low growing temperature,
the increase is about 30 units at the high growing temperature. This is the interaction effect
that has been established. Notice also that of the four experimental conﬁgurations considered,
the optimum conﬁguration employs fertilizer 2 at the high growing temperature, where the
average quality score of the turf is about 80.

668
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.16
Plot of the cell sample averages for
the turf quality example
20
40
60
80
100
Fertilizer 1
Fertilizer 2
Quality score
High growing temperature
Low growing temperature
= 41.50
¯x
= 79.00
¯x
= 48.25
¯x
= 29.00
¯x
12
11
21
22
.
.
.
.
FIGURE 14.17
The analysis of variance table for
the car body assembly line example
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Machine
2
1.81455
0.90728
0.98
0.3882
Solder
3
58.84646
19.61549
21.29
0.0001
Machine*Solder
6
3.95829
0.65972
0.72
0.6404
Error
24
22.11420
0.92142
Total
35
86.73350
Example 9
Car Body Assembly
Line
Figure 14.17 contains the analysis of variance table for the two-factor experiment investigating
welding strength. There are ab(n −1) = 3 × 4 × (3 −1) = 24 degrees of freedom for error
and the mean square error is
ˆσ 2 = MSE = 0.92142
so that ˆσ =
√
0.92142 = 0.960.
There are (a −1)(b −1) = (3 −1) × (4 −1) = 6 degrees of freedom for interaction,
and the mean sum of squares for interaction is
MSAB = 0.65972
The F-statistic for testing the presence of an interaction effect is therefore
FAB = MSAB
MSE = 0.65972
0.92142 = 0.72
and the p-value is
p-value = P(X > 0.72) = 0.6404
where the random variable X has an F-distribution with degrees of freedom 6 and 24.
This large p-value indicates that there is no evidence of an interaction effect between
the machines and the solder formulations. In other words, any difference between the solder
formulations is the same for all three machines. Differences among the machines and among
the solder formulations can now be investigated by considering the machine and solder main
effects.

14.1 EXPERIMENTS WITH TWO FACTORS
669
There are a −1 = 2 degrees of freedom for the machine main effects, and the F-statistic
for testing machine main effects is
FA = MSA
MSE = 0.90728
0.92142 = 0.98
The corresponding p-value is
p-value = P(X > 0.98) = 0.3882
where the random variable X has an F-distribution with degrees of freedom 2 and 24. Again,
this large p-value indicates that there is no evidence of a machine main effect, or in other
words, there is no evidence that there is any difference between the three machines. This
perhaps not unexpected result informs the experimenter that there is no evidence that the three
machines are not operating in an identical fashion with respect to welding strength.
Finally, there are b −1 = 3 degrees of freedom for the solder main effects, and the
F-statistic for testing solder main effects is
FB = MSB
MSE = 19.61549
0.92142 = 21.29
The corresponding p-value is
p-value = P(X > 21.29) = 0.0001
where the random variable X has an F-distribution with degrees of freedom 3 and 24. This
very small p-value indicates that the null hypothesis that there are no solder main effects is not
plausible, and so the experiment has provided evidence that the different solder formulations
do indeed have different effects on the average welding strength.
In conclusion, this experiment has shown that while the three machines appear to be
operating identically, the strength of the welds does depend upon which solder formulation
is employed. An analysis of how the different solder formulations affect the welding strength
should now be undertaken, and this is discussed in Section 14.1.4.
Example 74
Company
Transportation Costs
The analysis of variance table for the two-factor experiment concerning driving times is shown
in Figure 14.18. The p-value for the interaction effect is seen to be 0.430, which implies that
there is no evidence of an interaction effect between the route taken and the period of day.
However, the route main effect is seen to have a p-value of 0.004 and the period of day main
effect is seen to have a p-value of 0.000, so that there is evidence that the driving time depends
on both the route taken and the period of day. Nevertheless, the fact that there is no evidence
of an interaction effect implies that these two factors inﬂuence driving time in an independent
manner. For example, the difference between the average driving times for the two routes is
the same regardless of the period of day.
The plot of the cell sample means in Figure 14.19 shows that route 1 tends to be longer than
route 2 by about 20–25 minutes, and that the later in the day that the truck leaves the factory,
FIGURE 14.18
The analysis of variance table for
the company transportation costs
example
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Route
1
3830.7
3830.7
10.251
0.004
Period
2
11,925.6
5962.8
15.956
0.000
Route*Period
2
653.6
326.8
0.874
0.430
Error
24
8968.8
373.7
Total
29
25,378.7

670
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.19
A plot of the cell sample averages
for the company transportation
costs example
500
475
450
425
¯x
= 511.0
11.
¯x
= 486.6
12.
¯x
= 452.0
13.
Route 1
Route 2
¯x
= 475.6
21.
¯x
= 467.6
22.
¯x
= 438.6
23.
Driving time 
(minutes)
Morning
Afternoon
Evening
Period of day
the shorter the driving time will be. (Notice that because there is no evidence of an interaction
effect, a plot of the cell means μi j could plausibly exhibit parallel lines, even though in the
plot of the cell sample averages ¯xi j. in Figure 14.19 the lines are not quite parallel.) Analyses
should be performed to compare the two different routes with each other and to compare the
three different periods of day with each other, and these are discussed in Section 14.1.4.
14.1.4
Pairwise Comparisons of the Factor Level Means
If the analysis of a two-factor experiment reveals that there is no evidence of an interaction
effect but that there is evidence of one or more factor main effects, then the experimenter can
make pairwise comparisons of the factor level means to determine how the response variable
depends upon the different levels of the factor or factors that have been found to be inﬂuential.
Conﬁdence intervals can be constructed for all of the pairwise comparisons of the factor
level means in a fashion similar to that discussed in Section 11.1.4 for the comparisons of
the treatment means in a one-factor experiment and in Section 11.2.4 for the comparisons
of the treatment means in a randomized block experiment.
If there is evidence of a factor A main effect, then the effect of the a levels of factor A on
the expected value of the response variable should be investigated. The expected value of the
response variable at the a levels of factor A can be thought of as being represented by
μ + α1, . . . , μ + αa
of which there are a(a −1)/2 pairwise comparisons
αi1 −αi2
corresponding to the differences in the expectation of the response variable between levels i1
and i2 of factor A.
Using the critical point qα,a,ν given in Table V, a set of 1−α conﬁdence level simultaneous
conﬁdence intervals for these pairwise differences are constructed as
αi1 −αi2 ∈

¯xi1·· −¯xi2·· −s qα,a,ν
√
bn
, ¯xi1·· −¯xi2·· + s qα,a,ν
√
bn


14.1 EXPERIMENTS WITH TWO FACTORS
671
where s = ˆσ =
√
MSE and the degrees of freedom for the critical point are the error degrees
of freedom ν = ab(n −1).
If a conﬁdence interval contains 0, then there is no evidence that the expected value of
the response variable is different at the two corresponding levels of factor A. However, a
conﬁdence interval that does not contain 0 indicates how the expected value of the response
variable differs between the two corresponding levels of factor A.
Notice that if these conﬁdence intervals are constructed in a situation where there is no
evidence of a factor A main effect, then each of the conﬁdence intervals will contain 0 and
there is no evidence that the expected value of the response variable is any different at any of
the different levels of factor A. Also, if there is an interaction effect, then these conﬁdence
intervals are not interpretable since the different effects of the levels of factor A depend upon
the level of factor B that is employed. Finally, it is worthwhile to point out that if factor A
has only two levels so that a = 2, then the conﬁdence interval constructed for the pairwise
difference α1 −α2 is equivalent to the conﬁdence interval constructed from a two-sample
t-procedure with a pooled variance estimate discussed in Section 9.3.2. If there is evidence of
a factor B main effect, then conﬁdence intervals for the pairwise differences of the expected
values of the response variable at the b different levels of factor B can be constructed in a
similar manner as shown in the accompanying box.
The conﬁdence intervals can be used to provide an indication of the sensitivity afforded by
different sample sizes. Notice that the lengths of the conﬁdence intervals are inversely propor-
tional to the square root of the number of replications n for each experimental conﬁguration.
An experimenter may use the lengths of these conﬁdence intervals to assess how large a sample
size is required to achieve a certain amount of precision for comparing different factor levels.
Pairwise Comparisons of the Factor Level Means
If there is no evidence of an interaction effect in a two-factor experiment, then the
conﬁdence intervals
αi1 −αi2 ∈

¯xi1·· −¯xi2·· −s qα,a,ν
√
bn
, ¯xi1·· −¯xi2·· + s qα,a,ν
√
bn

can be used to investigate the interpretation of a factor A main effect, and the
conﬁdence intervals
β j1 −β j2 ∈

¯x· j1· −¯x· j2· −s qα,b,ν
√an , ¯x· j1· −¯x·bj2· + s qα,b,ν
√an

can be used to investigate the interpretation of a factor B main effect. Each set of
conﬁdence intervals has an overall conﬁdence level of 1 −α and the critical point has
ν = ab(n −1) degrees of freedom. Conﬁdence intervals that contain 0 indicate that
there is no evidence that the expected value of the response variable is different at the
two corresponding levels of the factor, while conﬁdence intervals that do not contain 0
indicate how the expected value of the response variable differs between the two
corresponding levels of the factor.
Example 9
Car Body Assembly
Line
In this experiment no evidence of an interaction effect between machines and solder formu-
lations was found, but the presence of a solder effect was established. Pairwise comparisons
of the b = 4 different solder formulations can be performed to identify how they affect the
welding strength.

672
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.20
Comparisons of the solders for the
car body assembly line example
Solder 2
Solder 4
Solder 3 Solder 1
¯x
= 7.296
4
.
.
¯x
= 3.803
.2.
¯x
= 4.854
.3.
¯x
= 4.920
.1.
The sample averages for the four different solder formulations are
¯x·1· = 4.920,
¯x·2· = 3.803,
¯x·3· = 4.854,
and
¯x·4· = 7.296
Also, q0.05,4,24 = 3.90, so that
s q0.05,4,24
√an
= 0.960 × 3.90
√3 × 3
= 1.248
The conﬁdence intervals for the pairwise differences of the solder effects are therefore
β1 −β2 ∈(4.920 −3.803 −1.248, 4.920 −3.803 + 1.248) = (−0.131, 2.365)
β1 −β3 ∈(4.920 −4.854 −1.248, 4.920 −4.854 + 1.248) = (−1.182, 1.314)
β1 −β4 ∈(4.920 −7.296 −1.248, 4.920 −7.296 + 1.248) = (−3.624, −1.128)
β2 −β3 ∈(3.803 −4.854 −1.248, 3.803 −4.854 + 1.248) = (−2.299, 0.197)
β2 −β4 ∈(3.803 −7.296 −1.248, 3.803 −7.296 + 1.248) = (−4.741, −2.245)
β3 −β4 ∈(4.854 −7.296 −1.248, 4.854 −7.296 + 1.248) = (−3.690, −1.194)
The three conﬁdence intervals comparing solder formulations 1, 2, and 3 all contain 0
so there is no evidence of any difference among these three solders. However, none of the
conﬁdence intervals comparing solder formulation 4 with each of the other three solder for-
mulations contains 0, so solder 4 has been established to have a different effect on welding
strength than any of the other three solder formulations. This situation is shown graphically
in Figure 14.20.
In summary, the experimenter can conclude that solder formulations 1, 2, and 3 appear to
have the same effects on welding strength, but that solder formulation 4 has been shown to
provide stronger welds than each of these other three solder formulations, with an increase in
average strength of at least 1.1 units.
Example 74
Company
Transportation Costs
In this example there is no evidence of an interaction effect, but both route and period of day
have been shown to inﬂuence the driving time, and so they should both be investigated further.
The sample averages for the two different routes are
¯x1·· = 483.2
and
¯x2·· = 460.6
With q0.05,2,24 = 2.92, so that
s q0.05,2,24
√
bn
= 19.33 × 2.92
√3 × 5
= 14.6
the conﬁdence interval for the difference in the expected value of the driving time between
the two routes is therefore
α1 −α2 ∈(483.2 −460.6 −14.6, 483.2 −460.6 + 14.6) = (8.0, 37.2)

14.1 EXPERIMENTS WITH TWO FACTORS
673
FIGURE 14.21
Comparisons of the driving periods
for the company transportation
costs example
Evening
445.3
¯x
= 4
3
. .
Afternoon
477.1
¯x
=
2
. .
Morning
493.3
¯x
=
1
. .
Consequently, this simple experiment has demonstrated that route 1 takes on average at least
8 minutes longer to drive than route 2, and possibly as much as 37 minutes longer on average.
Since there is no evidence of an interaction effect, remember that this result can be taken to
hold regardless of the time of day that the truck leaves the factory.
If the morning is taken to be level 1, the afternoon to be level 2, and the evening to be
level 3, the sample averages for the three different periods of day are
¯x·1· = 493.3,
¯x·2· = 477.1,
and
¯x·3· = 445.3
With q0.05,3,24 = 3.53, so that
s q0.05,3,24
√an
= 19.33 × 3.53
√2 × 5
= 21.6
the conﬁdence intervals for the pairwise differences of the different periods of day are
β1 −β2 ∈(493.3 −477.1 −21.6, 493.3 −477.1 + 21.6) = (−5.4, 37.8)
β1 −β3 ∈(493.3 −445.3 −21.6, 493.3 −445.3 + 21.6) = (26.4, 69.6)
β2 −β3 ∈(477.1 −445.3 −21.6, 477.1 −445.3 + 21.6) = (10.2, 53.4)
This situation is summarized in Figure 14.21, and it is seen that while there is no evidence
that the expected driving time is any different between the morning and afternoon periods, the
experiment has established that trucks leaving in the evening have shorter expected driving
times. The decrease in the expected driving time is seen to be at least 10 minutes between
the evening and afternoon periods (and possibly as much as 53 minutes) and to be at least 26
minutes between the evening and morning periods (and possibly as much as 69 minutes).
14.1.5
Modeling Procedures and Residual Analysis
Residual analysis can be employed in the usual way to check on the model assumptions
and speciﬁcally to investigate whether either of the factors inﬂuences the variability of the
response variable. The construction of the residuals depends on the choice of which model is
appropriate, and some general comments on the modeling procedure are given below.
Fitting Different Models
Recall that for a two-factor experiment the model
μi j = μ + αi + β j + (αβ)i j
has been employed for the cell means μi j. This model allows for the two-factor main effects
together with an interaction effect. If the experimenter ﬁnds evidence of an interaction effect,
then this is the appropriate model for the data set under consideration.
If the experimenter ﬁnds no evidence of an interaction effect but ﬁnds evidence of both
factor main effects, then this implies that the appropriate model for the data set under consid-
eration is
μi j = μ + αi + β j
Notice that this model does not include the interaction parameters (αβ)i j.

674
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
Similarly, if the experimenter ﬁnds no evidence of an interaction effect or of a factor B
main effect but ﬁnds evidence of a factor A main effect, then this implies that the appropriate
model for the data set under consideration is
μi j = μ + αi
If there is a factor B main effect but no factor A main effect or interaction effect, then the
appropriate model for the data set under consideration is
μi j = μ + β j
If there is no evidence of an interaction effect or either of the two-factor main effects, then
the appropriate model for the data set under consideration is
μi j = μ
which just indicates that the response variable has the same expected value in each of the
experimental conﬁgurations.
COMPUTER NOTE
You will ﬁnd that on your computer package you have the option to ﬁt whichever model you
wish to the data set. In fact it is important to make sure that you initially do ﬁt a model with
an interaction term (and not just with the two-factor main effects) so that you can investigate
whether or not there is an interaction effect.
Experiments with One Observation Per Cell
Experiments with only one observation per
cell are a special case of two-factor experiments and need to be handled a little differently. In
this case n = 1 and there are no replicate observations at each experimental conﬁguration.
Notice that in this case the usual analysis would have ab(n −1) = 0 degrees of freedom for
error, and in fact the error sum of squares SSE would also be equal to 0.
With only one observation per cell it is not possible to investigate the presence or absence
of an interaction effect, and an analysis of the factor main effects must be based on the
assumption that there is no interaction effect. If this assumption is made, then the analysis
of the factor main effects can proceed by taking the interaction sum of squares SSAB as the
error sum of squares SSE with the corresponding degrees of freedom (a −1)(b −1).
COMPUTER NOTE
You can analyze a two-factor experiment with only one observation per cell on your computer
package by specifying a model with main effects but without an interaction effect, so that the
model
μi j = μ + αi + β j
is employed.
It should be clear that an experiment with only one observation per cell is a poorly designed
one (although in some cases it may not be possible or practical to obtain replicates) and
that replicate observations at each experimental conﬁguration should be obtained whenever
possible. The replications at the experimental conﬁgurations allow the investigation of the
interaction effect between the two factors, which is usually of considerable interest.
Finally, notice that a two-factor experiment with only one observation per cell is similar
to the randomized block experiments discussed in Section 11.2. In fact the data layout looks
the same with the k factor levels in the randomized block experiment corresponding to the
a levels of factor A in the two-factor experiment, and the b blocks in the randomized block
experiment corresponding to the b levels of factor B in the two-factor experiment. Also, the

14.1 EXPERIMENTS WITH TWO FACTORS
675
analysis of variance tables are similar once the interaction sum of squares SSAB is used as
the error sum of squares SSE in the two-factor experiment.
Nevertheless, the two experimental designs have different interpretations. In the random-
ized block experiment there is one factor of interest and a blocking variable is incorporated
into the experiment to increase the precision with which this factor can be examined. In the
two-factor experiment the objective is to investigate the inﬂuence that the two factors have on
a response variable, although in this case there are unfortunately no replicate observations.
Unbalanced Experimental Designs
The analysis of a two-factor experiment described in
this section considers the situation in which there are exactly n replicate observations for each
of the ab experimental conﬁgurations. This case is known as a balanced design. In many
experiments an unbalanced design occurs where there are not the same number of replications
at each experimental conﬁguration. This design may be intended by the experimenter or it
may be as the result of some missing data observations in an experiment that was originally
intended to be balanced.
The concepts behind the analysis of an unbalanced two-factor experiment are exactly the
same as those for a balanced two-factor experiment. The interaction effect and the factor
main effects are investigated in the same way with the analysis of variance table and their
interpretations are the same.
COMPUTER NOTE
The mathematical analysis of an unbalanced experiment is more complicated than the analysis
of a balanced experiment, and you may have to treat it differently on your computer package.
For example, you may have to analyze it as a “linear model” rather than as a “two-factor
analysis of variance.”
Residual Analysis
The residuals for a particular model are calculated as
ei jk = xi jk −ˆμi j
where the values ˆμi j are the ﬁtted cell values for the model under consideration. Thus, the
residuals are the differences between the data observations xi jk and the ﬁtted values under the
model. You should ﬁnd that the residuals can be calculated by your computer package upon
request.
Residuals with a particularly large absolute value identify data observations that do not ﬁt
the model particularly well. The experimenter may want to investigate these outliers in more
detail to see if there is an explanation for their behavior. The size of the residuals can be judged
by dividing them by the estimated standard deviation ˆσ to produce standardized residuals.
The standardized residuals that are larger than 3 in absolute value are often considered to
correspond to possible outliers.
A normal probability plot of the residuals (as discussed in Section 12.7.1) can be made to
assess whether there is any indication that the residuals are not normally distributed. Evidence
that the residuals are not normally distributed should cause the experimenter to be cautious
about the analysis of the data set. In this case a transformation of the response variable may
result in a model whose assumptions are better satisﬁed.
Finally, the residuals can be plotted against the factors in the experiment. These plots
reveal whether there is any evidence that the variability in the response variable depends upon
the different levels of the factor, as illustrated in Figure 14.22. This may be the case even
though there is no interaction effect and there is no factor main effect so that the expected
value of the response variable is unaffected by the different levels of the factor. The assessment
of the effect of a factor on the variability of the response variable is often as important as the

676
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.22
A greater spread in the residual
values at level 2 indicates that there
is more variability in the response
variable at level 2
+++
++ ++++++ +
+
+ +
+
++++ +++
Factor level 2
Factor level 1
More variability
in the response
variable
Less variability
in the response
variable
Residuals
FIGURE 14.23
Residuals for the turf quality data
set
e111 = x111 −¯x11· = 11.00
211
211
21·
e
= x
−¯x
=
5.50
e112 = x112 −¯x11· = −5.00
e212 = x212 −¯x21· = −4.50
e113 = x113 −¯x11· =
2.00
e213 = x213 −¯x21· = −0.50
e114 = x114 −¯x11· = −8.00
e214 = x214 −¯x21· = −0.50
e121 = x121 −¯x12· = −2.25
e221 = x221 −¯x22· =
5.00
e122 = x122 −¯x12· =
4.75
e222 = x222 −¯x22· = −3.00
e123 = x123 −¯x12· = −0.25
e223 = x223 −¯x22· = −2.00
e124 = x124 −¯x12· = −2.25
e224 = x224 −¯x22· =
0.00
Temperature
Low
High
Fertilizer 1
Fertilizer 2
Fertilizer
assessment of the effect of a factor on the expected value of the response variable, since it
is generally desirable to employ levels of a factor that reduce the variability in the response
variable.
Example 72
Turf Quality
The appropriate model for this data set has been found to contain an interaction effect and the
ﬁtted values are therefore the cell sample averages, so that
ei jk = xi jk −ˆμi j = xi jk −¯xi j·
These residuals are shown in Figure 14.23. Notice that the residual with the largest absolute
value is
e111 = x111 −¯x11· = 40 −29 = 11
but that with ˆσ = 5.29 this is not unusually large. The normal probability plot is shown in
Figure 14.24, and the fairly linear relationship exhibited does not raise any alarms about the
assumption of normality.

14.1 EXPERIMENTS WITH TWO FACTORS
677
FIGURE 14.24
The normal probability plot of the
residuals for the turf quality
example
Residual
Percent
10
5
0
–5
–10
99
95
90
80
70
60
50
40
30
20
10
5
1
Example 9
Car Body Assembly
Line
The analysis of this experiment did not reveal any evidence that the expected value of the
welding strength is any different for the three machines on the factory ﬂoor. Is the variability
of the welding strengths any different for the three machines? This is also an important question
and can be addressed by making residual plots for each of the three machines.
The appropriate model for this data set has been found to be one that involves only main
effects for the solder formulations, and under this model the ﬁtted cell values are the solder
level sample averages ¯x· j·. The residuals are therefore calculated as
ei jk = xi jk −ˆμi j = xi jk −¯x· j·
For example, the residual for the ﬁrst data observation is
e111 = x111 −¯x·1· = 4.120 −4.920 = −0.800
The residual with the largest absolute value turns out to be
e233 = x233 −¯x·3· = 6.750 −4.854 = 1.896
However, with ˆσ = 0.960 this is not unusually large.
Figure 14.25 shows a plot of the residuals for each of the three machines. There is no
obvious difference in the amount of scatter of the three sets of residuals and so there is no evi-
dence that the variability of the welding strengths is any different for the three machines. In
addition, Figure 14.26 shows a plot of the residuals for the four different solder formulations.
Solder formulation 4 (which provides the strongest welds) does not appear to produce a
variability in weld strength that is markedly different from the other three solder formulations.
14.1.6
Problems
14.1.1 A two-factor experiment is conducted to compare
different mixes of gasoline in different types of car. The
response variable measures the driving characteristics of
the car based on acceleration and other properties relating
to the gasoline. Larger values of the response variable
relate to improved driving characteristics. Factor A is the
gasoline type that has a = 2 levels and factor B is the
type of car that also has b = 2 levels. For each
experimental conﬁguration n = 3 replicate observations
are taken. The resulting sums of squares are
SSA = 96.33, SSB = 75.00, SSAB = 341.33, and
SSE = 194.00. The cell sample averages are ¯x11· = 32.33,

678
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.25
Plot of the residuals against the
machines for the car body
assembly line example
Machine
Residual
2.0
1.0
1.5
0.5
0
0.5
1.0
3
1
2
1.5
FIGURE 14.26
Plot of the residuals against the
solder types for the car body
assembly line example
Solder
Residual
2.0
1.0
1.5
0.5
0
0.5
1.0
4
1
2
3
1.5
¯x12· = 38.00, ¯x21· = 37.33, and ¯x22· = 21.67. Construct
the analysis of variance table. What are your conclusions
from this experiment? Which type of gasoline is best?
14.1.2 A two-factor experiment is conducted to investigate
how the tensile strength of a graphite-epoxy composite
depends upon the formulation of the composite and the
temperature of the composite. Four different composite
formulations are considered in the experiment and they
are taken to be factor A. Three different temperature levels
are considered (low = 1, medium = 2, and high = 3)
and they are taken to be factor B. The response variable
is tensile strength and it is measured for two different
samples at each experimental conﬁguration. The sums of
squares SSA = 160.61, SSB = 580.52, SSAB = 58.01,
and SSE = 66.71 are obtained. The cell sample averages
are ¯x11· = 27.05, ¯x12· = 18.80, ¯x13· = 16.40, ¯x21· = 25.50,
¯x22· = 15.60, ¯x23· = 8.25, ¯x31· = 24.15, ¯x32· = 17.90,
¯x33· = 16.95, ¯x41· = 30.00, ¯x42· = 23.40, and ¯x43· = 17.65.
(a) Construct the analysis of variance table.
(b) Show that there is no evidence of an interaction effect
but that there are main effects for both factors.
(c) Construct pairwise conﬁdence intervals to compare
the four different graphite-epoxy composite
formulations. What conclusions can you draw?
(d) Construct pairwise conﬁdence intervals to compare
the three different temperature levels. What
conclusions can you draw?

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
679
14.1.3 Indentation Measurements for Hardness Testing
The data set in DS 14.1.1 shows the results of an
experiment performed to test the machinery used to
measure the hardness of a material. The hardness of a
material is measured by pushing a tip into the material
with a speciﬁed force. The depth h of the resulting
indentation is then measured. A laboratory purchases a
machine that has three different tips that are supposed to
provide identical results. The purpose of the experiment is
to investigate whether the different tips can in fact be
taken to produce identical results. Three types of material
are used and four replicate measurements of indentation
are obtained for each experimental conﬁguration.
(a) Construct the analysis of variance table.
(b) Show that there is no evidence of an interaction effect
and no evidence of a main effect for the tips, but that
there is a main effect for material type. How do you
interpret these results?
(c) Plot the residuals for each of the three tips. Why is it
useful to do this? Does it tell you anything interesting?
14.1.4 Contact Lens Dimensions
The data set in DS 14.1.2 shows the results of an
experiment performed to investigate how the deviation
from speciﬁcation of the width of a contact lens at its
center point may depend on the type of material used to
make the lens and the amount of magniﬁcation provided
by the lens.
(a) Construct the analysis of variance table.
(b) Show that there is no evidence of an interaction effect
and no evidence of a main effect for material type,
but that there is a main effect for magniﬁcation level.
How do you interpret these results?
(c) Plot the residuals for each of the four types of
material. Which material would you recommend be
employed to minimize the variability in the
deviations from speciﬁcation?
14.1.5 Silicon Mass Loss from Glass Leaching
When a specimen of glass is placed in a solution, leaching
occurs whereby some of the constituents of the glass are
absorbed into the solution. DS 14.1.3 contains the results
of a two-factor experiment to investigate how the glass
leaching depends on the glass composition and the acidity
of the solution. The response variable measured is the
mass loss of silicon. Analyze this data set.
14.1.6 Clinical Trial
The amount of improvement of a genetic disease is
measured for 18 patients who are randomly assigned to
the nine experimental conﬁgurations, with two patients
to each conﬁguration, corresponding to three dosage
levels of each of two active ingredients of a drug. The
experimental results are given in DS 14.1.4. Analyze this
data set. What do you notice when you plot the residuals
against the levels of ingredient B?
14.1.7 Optimal Mechanical Component Construction
A mechanical component can be made using three
different designs and from two different material types.
Six components are manufactured according to the six
possible combinations of design and material, and
DS 14.1.5 contains the lifetimes in hours of these
components when they are employed. What does the
data set tell you about the effectiveness of the different
designs and materials?
14.2
Experiments with Three or More Factors
Experiments can be performed to investigate how a response variable depends on three or
more factors of interest. The concepts behind the analysis of such an experiment are similar
to those behind the analysis of a two-factor experiment, except that there are now a larger
number of interaction terms that need to be considered. As with a two-factor experiment, the
analysis is based upon the construction of an analysis of variance table and the consideration
of the residuals.
If there are many factors that each have quite a few levels, then the number of experimental
conﬁgurations can become extremely large so that a large amount of data collection is required
to conduct the experiment. In this case, screening experiments are often performed whereby
each factor is considered at only two levels. This provides a preliminary investigation into how
the response variable depends on the factors of interest. Such an experiment with k factors is

680
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
known as a 2k design. In this section an example is given of a three-factor experiment followed
by an example of a 24 experiment.
14.2.1
Three-Factor Experiments
Consider an experiment to investigate how a response variable depends on three factors of
interest, A, B, and C, that have a levels, b levels, and c levels, respectively. There are therefore
abc experimental conﬁgurations, and suppose that a complete balanced design is employed
with n replicate observations obtained at each of the experimental conﬁgurations. The total
number of data observations obtained is consequently
nT = abcn
The data are of the form
xi jkl
1 ≤i ≤a, 1 ≤j ≤b, 1 ≤k ≤c, 1 ≤l ≤n
so that xi jkl is the lth observation obtained at the ith level of factor A, the jth level of factor B,
and the kth level of factor C. The sample averages are obtained in the obvious manner, so that
the cell sample averages are
¯xi jk· = 1
n
n

l=1
xi jkl
the overall average is
¯x···· =
1
abcn
a

i=1
b

j=1
c

k=1
n

l=1
xi jkl
and, for instance, the averages at the a different levels of factor A are
¯xi··· =
1
bcn
b

j=1
c

k=1
n

l=1
xi jkl
while the averages at the ab different combinations of the levels of factors A and B are
¯xi j·· = 1
cn
c

k=1
n

l=1
xi jkl
The unknown cell means can be denoted as
μi jk
1 ≤i ≤a, 1 ≤j ≤b, 1 ≤k ≤c
and they represent the expected value of the response variable at each of the abc experimental
conﬁgurations. The data observations can then be modeled as
xi jkl = μi jk + ϵi jkl
where the error terms ϵi jkl are taken to be independent observations, which are distributed
ϵi jkl ∼N(0, σ 2)
for some unknown error variance σ 2.
As with two-factor experiments, the objective of the analysis of a three-factor experiment
is to understand the structure of the cell means μi jk, so that the experimenter can interpret the
manner in which the expected value of the response variable is inﬂuenced by the three factors
under consideration. In this respect it is useful to decompose the cell means into a new set of
parameters so that they are written
μi jk = μ + αi + β j + γk + (αβ)i j + (αγ )ik + (βγ ) jk + (αβγ )i jk

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
681
where these new parameters are conditioned to sum to 0 over each of their subscripts. Here the
parameters (αβγ )i jk represent a three-way interaction effect between the three factors. The
presence of this three-way interaction effect indicates that there is a very complex relationship
between the ways in which the three factors affect the expected value of the response variable.
Speciﬁcally, it implies that the relationship between any two factors and the response variable
depends upon the level of the additional factor.
Notice that there are now three different kinds of two-way interaction effects, which are
the interaction effects discussed for two-factor experiments. The parameters (αβ)i j represent
a two-way interaction effect between factors A and B, the parameters (αγ )ik represent a two-
way interaction effect between factors A and C, and the parameters (βγ ) jk represent a
two-way interaction effect between factors B and C. Finally, the parameters αi, β j, and γk
represent the main effects for the factors A, B, and C, respectively.
With this representation of the cell means there are seven different hypotheses that can
be tested corresponding to whether each of the seven different sets of parameters are equal
to 0. The hypothesis that the parameters (αβγ )i jk are all equal to 0 corresponds to there being
no three-way interaction effect. Similarly, the three different hypotheses corresponding to
whether the three different sets of parameters (αβ)i j, (αγ )ik, and (βγ ) jk are all equal to 0 test
for the presence of two-way interaction effects. Also, the hypothesis that the parameters αi are
all equal to 0 tests for the presence of a factor A main effect, the hypothesis that the parameters
β j are all equal to 0 tests for the presence of a factor B main effect, and the hypothesis that
the parameters γk are all equal to 0 tests for the presence of a factor C main effect.
An analysis of variance table is formed by decomposing the total sum of squares SST
into sums of squares for the three factor main effects (SSA, SSB, SSC), the three two-way
interaction effects (SSAB, SSAC, SSBC), the three-way interaction effect SSABC, and the
sum of squares for error SSE, as shown in Figure 14.27, so that
SST = SSA + SSB + SSC + SSAB + SSAC + SSBC + SSABC + SSE
FIGURE 14.27
The sum of squares decomposition
for a three-factor experiment
SSA
SSB
SSC
SSAC
SSAB
SSBC
SSABC
SSE
SST
Sums of 
squares
for factor
main 
effects
Sums of 
squares
for two-way
interaction 
effects
Sum of 
squares
for three-way
interaction 
effect
Sum of 
squares
for error

682
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
These sums of squares are based upon the sums of the squares of the corresponding parameters
estimates and are given by
SST =
a

i=1
b

j=1
c

k=1
n

l=1
(xi jkl −¯x····)2
SSE =
a

i=1
b

j=1
c

k=1
n

l=1
(xi jkl −¯xi jk·)2
SSA = bcn
a

i=1
ˆα2
i = bcn
a

i=1
(¯xi··· −¯x····)2
SSB = acn
b

j=1
ˆβ2
j = acn
b

j=1
(¯x· j·· −¯x····)2
SSC = abn
c

k=1
ˆγ 2
k = abn
c

k=1
(¯x··k· −¯x····)2
SSAB = cn
a

i=1
b

j=1
( ˆαβ)2
i j = cn
a

i=1
b

j=1
(¯xi j·· −¯xi··· −¯x· j·· + ¯x····)2
SSAC = bn
a

i=1
c

k=1
( ˆ
αγ )2
ik = bn
a

i=1
c

k=1
(¯xi·k· −¯xi··· −¯x··k· + ¯x····)2
SSBC = an
b

j=1
c

k=1
( ˆ
βγ )2
jk = an
b

j=1
c

k=1
(¯x· jk· −¯x· j·· −¯x··k· + ¯x····)2
SSABC = n
a

i=1
b

j=1
c

k=1
( ˆ
αβγ )2
i jk
= n
a

i=1
b

j=1
c

k=1
(¯xi jk· −¯xi j·· −¯xi·k· −¯x· jk· + ¯xi··· + ¯x· j·· + ¯x··k· −¯x···)2
The degrees of freedom are a−1, b−1, and c−1 for the factor main effects, (a−1)(b−1),
(a−1)(c−1), and (b−1)(c−1) for the two-way interaction effects, and (a−1)(b−1)(c−1)
for the three-way interaction effect. There are abc(n −1) degrees of freedom for error and as
usual, the mean square error
MSE = ˆσ 2
is the estimate of the error variance. The complete analysis of variance table is shown in
Figure 14.28, and notice that there are seven F-statistics that can be used to test the seven hy-
potheses of interest concerning the three-way interaction effect, the three two-way interaction
effects, and the three main effects.
As with a two-factor experiment, interaction effects should be considered before main
effects, and in this case the three-way interaction effect should be considered before the two-
way interaction effects. If there is evidence of a three-way interaction effect, then there is a
complex relationship between the three factors and the response variable, and the presence or
absence of two-way interaction effects or main effects is redundant. If there is no evidence of
a three-way interaction effect, then the three two-way interaction effects can be considered.
Again, remember that the presence of a two-way interaction effect makes the tests of the main
effects redundant for the two factors involved.

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
683
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistics
p-value
Factor A
a −1
SSA
MSA = SSA
a−1
FA = MSA
MSE
P(Fa−1,abc(n−1) > FA)
Factor B
b −1
SSB
MSB = SSB
b−1
FB = MSB
MSE
P(Fb−1,abc(n−1) > FB)
Factor C
c −1
SSC
MSC = SSC
c−1
FC = MSC
MSE
P(Fc−1,abc(n−1) > FC)
AB interaction
(a −1)(b −1)
SSAB
MSAB =
SSAB
(a−1)(b−1)
FAB = MSAB
MSE
P(F(a−1)(b−1),abc(n−1) > FAB)
AC interaction
(a −1)(c −1)
SSAC
MSAC =
SSAC
(a−1)(c−1)
FAC = MSAC
MSE
P(F(a−1)(c−1),abc(n−1) > FAC)
BC interaction
(b −1)(c −1)
SSBC
MSBC =
SSBC
(b−1)(c−1)
FBC = MSBC
MSE
P(F(b−1)(c−1),abc(n−1) > FBC)
ABC interaction
(a −1)(b −1)(c −1)
SSABC
MSABC =
SSABC
(a−1)(b−1)(c−1)
FA
= MSABC
MSE
P(F(a−1)(b−1)(c−1),abc(n−1) > FABC)
Error
abc(n −1)
SSE
MSE =
SSE
abc(n−1)
Total
abcn −1
SST
BC
FIGURE 14.28
Analysis of variance table for a three-factor experiment
FIGURE 14.29
The automated application of
grease to a shaft
Grease
Shaft
Grease
Cog wheel
Finally, once a ﬁnal model has been decided on, the residuals should be calculated and
examined. Plots of the residuals against the factor levels, even for factors that are not included
in the model, may indicate differences in the variability of the response variable at the different
factor levels, which is important information for the experimenter to be aware of.
Example 75
Robot Assembly
Methods
In an automated assembly line for the production of photocopiers, one of the many tasks is to
apply grease to a metal shaft before a cog wheel is placed onto the shaft and pushed over the
grease, as shown in Figure 14.29. The automatic application of the grease is achieved by a
two pronged fork, which is dipped into a cup of grease and is then pushed over the shaft. It is
important that this automated procedure apply neither too much grease nor too little grease to
the shaft, and a three-factor experiment is conducted to investigate how the amount of grease
deposited upon the shaft depends upon the type of fork used, the type of grease employed,
and the temperature of the grease.

684
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
Suppose that two types of fork are used (one with longer and wider prongs than the
other) and that this is taken to be factor A with a = 2 levels. Four different types of grease
are considered, and this is taken to be factor B, which consequently has b = 4 levels. The
temperature of the cup of grease can be controlled, and this is taken to be factor C with c = 3
levels (level 1 = low temperature, level 2 = medium temperature, level 3 = high temperature).
There are therefore abc = 2 × 4 × 3 = 24 experimental conﬁgurations, and n = 5
replicate observations are obtained at each of these conﬁgurations. In each case the weight of
the grease applied to the shaft is carefully measured, and a total of
nT = abcn = 2 × 4 × 3 × 5 = 120
data observations are obtained which are shown in Figure 14.30.
FIGURE 14.30
Data set for the robot assembly
methods example
Fork Type 1
Grease type 1
Grease type 2
Grease type 3
Low
Temperature
Medium
High
14.98
11.67
12.41
13.39
10.99
11.17
10.89
12.48
14.59
11.43
10.39
11.67
13.11
13.20
10.77
10.45
9.54
11.23
10.68
10.51
12.51
11.88
11.95
12.83
12.82
14.35
13.88
14.70
12.18
14.04
12.36
15.47
14.36
16.35
15.32
13.55
14.36
13.14
14.31
14.38
11.95
11.43
12.67
10.68
13.69
9.36
8.89
9.22
11.29
10.58
10.20
9.23
9.02
10.69
9.30
12.88
10.98
12.10
11.00
9.57
Grease type 1
Grease type 2
Grease type 3
Grease type 4
Low
Temperature
Medium
High
7.55
12.03
14.03
11.63
11.87
10.21
9.06
11.16
14.06
11.71
8.47
15.83
13.60
6.94
10.18
10.59
8.57
10.42
12.76
13.10
12.61
9.57
11.52
10.91
11.24
11.00
14.75
15.29
11.19
12.39
15.47
14.71
12.89
12.05
15.19
13.73
19.36
13.18
13.26
14.55
13.98
14.24
13.34
15.28
10.39
14.41
10.21
9.94
8.67
11.64
9.54
10.85
8.01
12.59
10.96
11.38
9.25
9.75
10.05
13.24
Fork Type 2
Grease type 4

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
685
FIGURE 14.31
The analysis of variance table for
the robot assembly methods
example
Source
Degrees of freedom
Sum of squares
Mean squares
F-statistic
p-value
Fork
1
0.626
0.626
0.22
0.642
Grease
3
173.714
57.905
20.15
0.000
Temperature
2
1.114
0.557
0.19
0.824
Fork*Grease
3
11.829
3.943
1.37
0.256
Fork*Temperature
2
0.145
0.072
0.03
0.975
Grease*Temperature
6
57.838
9.640
3.35
0.005
Fork*Grease*Temperature
6
14.635
2.439
0.85
0.536
Error
96
275.905
2.874
Total
119
535.807
The analysis of variance table for this experiment is shown in Figure 14.31. Notice ﬁrst
that the three-way interaction effect “Fork*Grease*Temperature” has an F-statistic
FABC = MSABC
MSE
= 2.439
2.874 = 0.85
with a p-value of
p-value = P(X > 0.85) = 0.536
where the random variable X has an F-distribution with degrees of freedom 6 and 96. This
large p-value indicates that there is no evidence of a three-way interaction effect.
Since there is no evidence of a three-way interaction effect, the p-values for the three
two-way interaction effects can be considered. Notice that the two-way interaction effect
“Grease*Temperature” has an F-statistic
FBC = MSBC
MSE = 9.640
2.874 = 3.35
The corresponding p-value is
p-value = P(X > 3.35) = 0.005
where the random variable X has an F-distribution with degrees of freedom 6 and 96. This
low p-value indicates that there is evidence of an interaction effect between the type of grease
used and the temperature of the grease. In other words, there is evidence that the manner in
which temperature changes affect the amount of grease deposited on the shaft depends upon
the particular type of grease used.
The “Fork*Grease” two-way interaction has a p-value of 0.256 and the “Fork*Temp-
erature” two-way interaction has a p-value of 0.975, and consequently there is no evidence of
the presence of interaction effects between the type of fork and either of the two other factors.
Which factor main effects need to be considered? Since there is an interaction between
the type of grease and the temperature, the tests of the main effects for these two factors
are redundant. However, the main effect for the type of fork needs to be investigated. An
F-statistic
FA = MSA
MSE = 0.626
2.874 = 0.22
is obtained, and the p-value is
p-value = P(X > 0.22) = 0.642
where the random variable X has an F-distribution with degrees of freedom 1 and 96. There
is thus no evidence of the presence for a main effect for the type of fork.

686
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
Low
Medium
High
¯x
= 12.055
11
.
.
¯x
= 10.785
21
.
.
¯x
= 14.417
31
.
.
¯x
= 10.421
41
.
.
¯x
= 11.676
12
.
.
¯x
= 11.784
22
.
.
¯x
= 14.382
32
.
.
¯x
= 10.039
42
.
.
¯x
= 11.416
13
.
.
¯x
= 13.377
23
.
.
¯x
= 12.765
33
.
.
¯x
= 11.020
43
.
.
Grease type 1
Grease type 3
Grease type 2
Grease type 4
Temperature
Grease
Amount of grease
3-High
1-Low
2-Medium
15
10
11
12
13
14
Temperature
Grease
3
4
1
2
FIGURE 14.32
The sample averages for the grease-temperature combinations for the robot assembly methods example
In summary, the analysis conducted so far has shown the experimenter that the average
amount of grease deposited on the shaft does not depend on which type of fork is used, but it
does depend on the type of grease used and the temperature of the grease. Furthermore, the
inﬂuences of the type of grease used and the temperature of the grease are not independent
of each other. The effects of the different combinations of grease and temperature can be
assessed from the sample averages ¯x· jk·, which are shown in Figure 14.32.
Is it the case that it doesn’t matter which type of fork is used? Let’s look at a residual plot
before jumping to this conclusion. The ﬁnal model that has been decided on is
μi jk = μ + β j + γk + (βγ ) jk
and this can be ﬁtted to the data and residuals can be calculated in the usual manner. Fig-
ure 14.33 shows boxplots of the residuals for each of the two fork types, based on 60 residuals
for each fork type. It is clear from these boxplots that the residuals tend to be closer to 0 for
fork type 1 than for fork type 2, and this implies that the variability in the amount of grease
deposited on the shaft is smaller for fork type 1 than for fork type 2. This ﬁnding indicates
that it is preferable to use fork type 1 rather than fork type 2.
In conclusion, the objective of this experiment is to help design the best automated proce-
dure to deposit the right amount of grease onto the shaft. It has been found that the grease type
and the temperature can both be manipulated in order to make the expected amount of grease
deposited equal to the required amount. Furthermore, it has been found that it is best to then

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
687
FIGURE 14.33
Boxplots of the residuals for the
two fork types in the robot
assembly methods example
Fork type
Residuals
5.0
0
2.5
5.0
2.5
1
2
use fork type 1 so that the variability of the amount of grease deposited about the required
amount is as small as possible.
14.2.2
2k Experiments
In an experiment to compare the effects of k factors on a response variable, where the levels
of the factors are a1, . . . , ak, the total number of different experimental conﬁgurations is
a1 × a2 × · · · × ak
If n replicate observations are obtained at each experimental conﬁguration, the total sample
size required is
nT = a1 × a2 × · · · × ak × n
This sample size becomes very large as the number of factors k increases, and in such circum-
stances it is therefore common to use screening experiments in which each factor is considered
at only two levels so that the ai are all equal to 2.
An experiment conducted with k factors each with two levels is known as a 2k experimental
design. There are 2k experimental conﬁgurations and a total sample size of
nT = 2k × n
is required if there are n replicate observations at each experimental conﬁguration.
The usual type of model is employed in these experiments whereby the data observations
are assumed to be composed of a cell mean together with an error term from a normal
distribution. The cell means are then written as the sum of the k individual factor main effects
together with all the possible two-way interaction effects plus higher order interaction effects.
There are k(k −1)/2 two-way interaction effects, and in general there are

k
r

r way interaction effects, because this is the number of ways that a subset of r factors can be
taken from the k factors.
For2k experimentstheparameterestimatesareeasilyobtainableasthedifferencesbetween
two sets of sample averages. The sums of squares for each set of parameters are obtained
by taking the sums of the squares of the parameter estimates as was shown for three-factor

688
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
experiments. An analysis of variance table is constructed to test for the presence of the various
types of parameters in the usual manner, and all of the rows have one degree of freedom
except for the error degrees of freedom, which are 2k(n −1). The total degrees of freedom are
2kn −1.
The analysis of variance table presents p-values for each of the hypotheses that can be
tested in 2k experiments. As usual, the higher order interactions should be analyzed ﬁrst, and
the presence of any interaction term makes redundant the analysis of any interactions or main
effects formed from its component factors. In practice, there generally will not be any evidence
of four-way interactions or higher order interactions, and three-way interaction effects may
also be rare.
Example 70
Chemical Yields
An experiment is performed to investigate how the yield of a chemical reaction depends on
four factors: temperature, pressure, amount of catalyst 1, and amount of catalyst 2. The four
factors are each considered at two levels, a low level and a high level, which results in a 24
experimental design. There are 16 experimental conﬁgurations and n = 2 observations are
taken at each of these experimental conﬁgurations so that the total sample size is nT = 32.
The data set obtained is shown in Figure 14.34.
The analysis of variance table is shown in Figure 14.35. Notice that there is a row for the
four-way interaction effect
Temp*Pressure*Cat-1*Cat-2
FIGURE 14.34
Data set for the chemical yields
example
Temperature
Pressure
Catalyst 1
Catalyst 2
Yield
548
Low
Low
Low
Low
521
548
Low
Low
Low
High
526
580
Low
Low
High
Low
568
562
Low
Low
High
High
526
507
Low
High
Low
Low
523
507
Low
High
Low
High
544
560
Low
High
High
Low
559
538
Low
High
High
High
533
576
High
Low
Low
Low
556
559
High
Low
Low
High
590
581
High
Low
High
Low
603
608
High
Low
High
High
586
542
High
High
Low
Low
547
549
High
High
Low
High
577
583
High
High
High
Low
588
550
High
High
High
High
535

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
689
FIGURE 14.35
The analysis of variance table for
the chemical yields example
Degrees of
Source
freedom
Sum of squares
Mean squares
F-statistic
p-value
Temp
1
7200.0
7200.0
29.95
0.000
Pressure
1
2738.0
2738.0
11.39
0.004
Cat-1
1
3612.5
3612.5
15.03
0.001
Cat-2
1
338.0
338.0
1.41
0.253
Temp*Pressure
1
200.0
200.0
0.83
0.375
Temp*Cat-1
1
128.0
128.0
0.53
0.476
Temp*Cat-2
1
112.5
112.5
0.47
0.504
Pressure*Cat-1
1
50.0
50.0
0.21
0.654
Pressure*Cat-2
1
72.0
72.0
0.30
0.592
Cat-1*Cat-2
1
2178.0
2178.0
9.06
0.008
Temp*Pressure*Cat-1
1
162.0
162.0
0.67
0.424
Temp*Pressure*Cat-2
1
338.0
338.0
1.41
0.253
Temp*Cat-1*Cat-2
1
0.5
0.5
0.00
0.964
Pressure*Cat-1*Cat-2
1
450.0
450.0
1.87
0.190
Temp*Pressure*Cat-1*Cat-2
1
392.0
392.0
1.63
0.220
Error
16
3846.0
240.4
Total
31
21817.5
and that there are four rows for the three-way interaction effects
Temp*Pressure*Cat-1
Temp*Pressure*Cat-2
Temp*Cat-1*Cat-2
Pressure*Cat-1*Cat-2
There are also rows for the six two-way interaction effects and for the four individual factor
main effects. Each of the interaction effects and main effects has one degree of freedom, while
the degrees of freedom for error are 2k(n −1) = 24 × (2 −1) = 16.
The four-way interaction effect and each of the three-way interaction effects have large p-
values, so there is no evidence of their presence. Notice that one of the six two-way interaction
effects has a small p-value, and this is the interaction between the two catalysts (p-value =
0.008). This indicates that there is evidence of an interaction effect between the two catalysts,
but that there is no evidence of any other interaction effects.
The tests of the main effects of catalyst 1 and catalyst 2 are now redundant because of the
presence of their interaction effect, but the main effects for temperature and pressure should
be examined. The temperature main effect has a very small p-value and the pressure main
effect has a p-value of 0.004, and so there is evidence of the presence of main effects for both
of these factors.
In summary, this experiment has revealed that the expected value of the chemical yield
depends on all four of the factors. In addition, the effects of the changes in temperature and
pressure are independent of each other and of the amounts of the two catalysts (since no
interaction effects have been found involving temperature and pressure with each other or
with the two catalysts). The effects of changes in the two catalysts are not independent of each
other.
More speciﬁc information on these effects can be found by examining the appropriate
sample averages. Figure 14.36 shows the sample averages for the two temperature levels,
the two pressure levels, and the four combinations of the two catalysts. It can be seen that
increasing the temperature increases the yield but that increasing the pressure decreases the

690
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
FIGURE 14.36
The sample averages for the
chemical yields example
Temperature
Low
High
¯x1···· = 540.62
¯x2···· = 570.62
Pressure
Low
High
¯x·1··· = 564.88
¯x·2··· = 546.37
Catalyst 1
Low
High
Low
¯x··11· = 540.00
¯x··21· = 577.75
Catalyst 2
High
¯x··12· = 550.00
¯x··22· = 554.75
yield. The interaction effect between the catalysts is evident since increasing the amount of
catalyst 2 increases the yield at the low level of catalyst 1 but decreases the yield at the high
level of catalyst 2.
This section provides a general introduction to 2k experiments, which are very important
tools for engineering research. A more detailed and in-depth description of the design and
analysis of 2k experiments can be found in more advanced books on the design of experiments.
For example, some 2k experiments are run without replicate observations so that n = 1. In
this case the analysis proceeds on the assumption that some higher order interaction terms
are 0 so that their sums of squares can be used as the error sum of squares (as in a two-factor
experiment without replications). Alternatively, an analysis method can be based upon the
construction of a normal probability plot of the parameters estimates.
Finally, 2k experiments can be performed where data observations are obtained in only a
subset of the 2k experimental conﬁgurations. This serves to reduce the total sample size and
is particularly useful when the number of factors k is large. Such an experimental design is
known as a fractional factorial design. In these cases the parameters are said to be confounded
with each other since they cannot be estimated independently of each other, and the analysis
proceedsontheassumptionthatsomeofthehigherorderinteractioneffectscanbetakentobe0.
14.2.3
Problems
14.2.1 A three-factor experiment is performed to taste test
different blends of a fruit juice mixture. The response
variable scores how the participant liked the fruit juice
based on the answers to a questionnaire. Factor A is the
blend of fruit juice with a = 3 levels. Factor B is the
gender of the participant with b = 2 levels, male and
female. Factor C is the age of the participant with c = 3
age categories. In each experimental conﬁguration n = 5
replicate observations are obtained.
(a) Suppose that an interaction effect is found between
factors A and B. How would you interpret this?
(b) Suppose that an interaction effect is found
between factors A and C. How would you interpret
this?
(c) Suppose that there are no interaction effects
involving factor A, and that there is no factor A
main effect. What does this tell you about the
different blends of fruit juice?
(d) The sums of squares are SSA = 90.65, SSB = 6.45,
SSC = 23.44, SSAB = 17.82, SSAC = 24.09,
SSBC = 24.64, SSABC = 27.87, and SSE = 605.40.
Construct the analysis of variance table. What ﬁnal

14.2 EXPERIMENTS WITH THREE OR MORE FACTORS
691
model is appropriate to model the fruit juice scores?
What is the interpretation of this model?
14.2.2 Optimal Rice Growing Conditions
A three-factor experiment is conducted to investigate the
yields of different brands of rice grown in controlled
greenhouse conditions. The response variable is the rice
yield, and factor A is rice variety with a = 3 levels. Two
levels of fertilizer are considered (factor B) together with
two amounts of sunshine (factor C). There are n = 4
replicate observations per cell and the data set is given in
DS 14.2.1.
(a) Construct the analysis of variance table and
summarize what you learn from it.
(b) Is there any evidence that increasing the amount of
fertilizer increases the expected yield?
(c) Is there any evidence that the effect of increasing the
sunlight depends upon the amount of fertilizer
employed?
(d) Is there any evidence that the effect of increasing
the sunlight depends upon which variety of rice is
used?
14.2.3 Effect of Gasoline Additives on Mileage
DS 14.2.2 contains the data set collected from a
three-factor experiment to investigate how gas mileage
depends on the amounts of two types of additive in the
gasoline and the driving conditions.
(a) Construct the analysis of variance table. How do the
three factors inﬂuence the gas mileage?
(b) Does the amount of additive B have an effect on the
expected value of the gas mileage? Plot the residuals
for each level of factor B. What do you learn?
14.2.4 Detection of Airborne Objects
DS 14.2.3 contains the data from a three-factor
experiment to investigate the distance at detection for
four different radar systems of two different aircraft ﬂying
at day and at night. Construct the analysis of variance
table and summarize what you learn from it.
14.2.5 Digital-Weighing Machine Calibration
DS 14.2.4 contains the results of a 24 experiment
conducted to investigate the accuracies of two digital
weighing machines. The response variable is the
deviation of the weight reading from the true weight of
a particular object that is used throughout the experiment.
The four factors are the machines, the temperature, the
position of the weight on the surface of the weighing
machine, and the angle of the weighing machine to the
horizontal.
(a) What models imply that there is no difference
between the expected values of the readings provided
by the two machines?
(b) How would you interpret an interaction effect
between temperature and machines?
(c) How would you interpret an interaction effect
between position and angle?
(d) Construct the analysis of variance table and
summarize what you learn from it.
(e) Is it fair to say that temperature has no inﬂuence on
the readings provided by the machines? Make a plot
of the residuals at the two temperature levels.
14.2.6 Golf Club Comparisons
DS 14.2.5 contains the results of a 24 experiment
conducted to investigate the driving distances of two
golfers using different clubs and balls and under different
weather conditions. What do you learn from this
experiment? Is there any evidence that the weather
conditions have any effect on the driving distance?
14.2.7 An ANOVA table for an experiment with four factors A,
B, C, and D gave the p-values shown in Figure 14.37.
Classify each term as being “signiﬁcant,”
“not-signiﬁcant,” or “redundant.”
Term
p-value
A
B
C
D
A*B
A*C
A*D
B*C
B*D
C*D
A*B*C
A*B*D
A*C*D
B*C*D
A*B*C*D
0.009
0.185
0.211
0.008
0.226
0.001
0.751
0.003
0.001
0.005
0.620
0.674
0.007
0.111
0.411
FIGURE 14.37
p-values obtained from an ANOVA
analysis of a four-factor experiment

692
CHAPTER 14
MULTIFACTOR EXPERIMENTAL DESIGN AND ANALYSIS
14.2.8 A three-factor experiment (3 × 2 × 2) gave the following
data:
When factor C is low
A low
A middle
A high
B low
50
47
55
B high
40
38
47
When factor C is high
A low
A middle
A high
B low
48
51
72
B high
42
40
64
Suppose that there is no three-way interaction effect, but
that there is an A*C two-way interaction. Construct an in-
teraction plot to demonstrate the A*C two-way interaction.
14.3
Case Study: Internet Marketing
Over a 2-month experimental period, the organization monitors how many visits there are
to its website under different advertising strategies. In some weeks, the organization pays to
have sponsored advertisements on the three leading search engines, and in addition, in some
weeks the organization also has television advertising. The four combinations of advertising
types are run twice, for 1-week periods each in a random order, and the resulting data set is
shown in Figure 14.38.
The analysis of variance table shows that there is no evidence of an interaction effect, since
the p-value is 0.921. However, the p-value for the sponsored advertisements main effect is
0.007 and the p-value for the television advertising is 0.070, and the corresponding factor plots
are shown in Figure 14.39. It is clear that the sponsored advertisements on the search engines
increasethenumberofvisitorstotheorganization’swebsite,andthereissomeevidencethatthe
television advertisements are also effective, independently of the sponsored advertisements.
FIGURE 14.38
Data set of the number of website
visitors
365,236
470,805
558,114
641,051
480,317
523,717
678,522
703,681
None
Yes
None
Yes
Television 
advertisements
Sponsored 
 advertisements
FIGURE 14.39
Analysis of website visits
Website visits
650,000
550,000
600,000
500,000
None
Yes
None
Yes
450,000
Sponsored advertisements
TV advertisements

14.4 SUPPLEMENTARY PROBLEMS
693
14.4
Supplementary Problems
14.4.1 Injection Molding Production
In an injection molding procedure, plastic parts are
produced by injecting the plastic material into a mold.
Some variability is experienced in the resulting weight (or
equivalently density) of the part. DS 14.4.1 contains the
results of a two-factor experiment to investigate how the
weight of the parts depends on the type of injection
material and the pressure of injection.
(a) Construct the analysis of variance table.
(b) Show that there is no evidence of an interaction effect
but that there are main effects for both factors.
(c) Construct pairwise conﬁdence intervals to compare
the three different types of injection material. What
conclusions can you draw?
(d) Construct pairwise conﬁdence intervals to compare
the three different injection pressures. What
conclusions can you draw?
14.4.2 Semiconductor Wafer Yields
Semiconductors (chips) are produced on wafers that
contain 100 chips. The wafer yield is deﬁned to be the
proportion of these chips that are acceptable for use.
One company has two different factory locations for
producing chips and can use one of three different
coatings for the wafers. The data set in DS 14.4.2 is the
result of an experiment run to investigate how wafer yield
depends on these two factors. What conclusions can you
draw from this experiment?
14.4.3 Clinical Trial
DS 14.4.3 contains the recovery times in days for
16 patients allocated at random to a two-factor
experiment to compare four drugs and two levels of
severity of the illness. Analyze the data set and
summarize your conclusions.
14.4.4 Effect of Furnace Operation on Metal Hardness
A three-factor experiment is performed to investigate how
the hardness of a metal bar depends on which furnace is
used to manufacture the bar and the location of the bar in
the furnace. The response variable is the hardness of the
metal, and one of the factors designates which of two
furnaces is used for that metal bar. The other factors are
the layer in the furnace (bottom, middle, or top) and the
position in the furnace (front or back). The data set
obtained is given in DS 14.4.4.
(a) What is the interpretation of a two-way interaction
effect between layer and position?
(b) What is the interpretation of a two-way interaction
effect between furnace and position?
(c) Construct the analysis of variance table and
summarize what you learn from it.
14.4.5 Dispersion Polymerization
DS 14.4.5 contains the results of a 24 experiment that
investigates the preparation of polymers by dispersion
polymerization in an organic medium. The response
variable is the mean diameter of the polymers in microns
(m−6). The four factors are monomer concentration,
stabilizer concentration, catalyst concentration, and water
concentration. Analyze the data set. What do you learn
from the experiment?
14.4.6 Lathe Operation for Gas Cylinder Construction
In the production of low-pressure gas cylinders a lathe
operator has to take the bottom and top parts of a cylinder,
ﬁt them with a footring and a collar, and then weld the
halves together. An experiment was conducted to
investigate the time taken to complete this process. Two
different lathe designs were considered, and three skilled
operators were used in the experiment. Six completion
times were measured for each operator using both of
the lathe designs. The times in seconds are given in
DS 14.3.6. Is there any evidence of a difference in the
efﬁciencies of the two lathe designs? Is the difference in
the efﬁciencies of the two lathe designs the same for each
of the operators?
14.4.7 Electric Motor Noise Levels
A company that manufactures large electric motors has to
pay attention to the noise levels produced by the motors.
Measurements of noise levels were obtained for three
different motor speeds. These measurements were made
at two positions, in front of the machine and at the side of
the machine, and also for four different types of cooling,
which involve different internal fans and ventilation
covers. The data obtained are given in DS 14.4.7. Show
that there is a three-way interaction effect between the
factors speed, cooler, and position. Considering just the
data obtained at position 1, make an interaction plot of the
effects of speed and cooler at that position. Make a
similar interaction plot for the data obtained at position 2.
Compare the two interaction plots to investigate the
three-way interaction.

C H A P T E R F I F T E E N
Nonparametric Statistical Analysis
This chapter provides a discussion and illustration of the implementation of nonparametric or
distribution-free statistical inference methodologies. The general distinction between these
types of inference methods and the alternative parametric inference methods discussed in pre-
vious chapters is that a parametric inference approach is based on a distributional assumption
for data observations, whereas a nonparametric inference approach provides answers that are
not based on any distributional assumptions.
Most of the statistical methodologies discussed in the other chapters of this book would be
considered parametric inference methods. For instance, the analysis of Example 14 concern-
ing metal cylinder diameters in Chapters 7 and 8 is based upon the distributional assumption
that the diameters are normally distributed with some unknown mean parameter μ and some
unknown variance parameter σ 2. The parametric analysis is then based on obtaining esti-
mates ˆμ and ˆσ 2 for these two unknown parameters. For other data sets other distributional
assumptions may be appropriate. For example, measurements of the failure times of certain
machine parts may be modeled with the Weibull or the gamma distribution. However, in
all cases the parametric inference approach operates within the framework of an assumed
distributional model, and it is based on the estimation of the unknown parameters of that
model.
Obviously, the results of a parametric analysis are only as good as the validity of the
assumptions on which they are based. If the assumption of a normal distribution is made,
whereas in fact the unknown true distribution is skewed and asymmetric, then the correspond-
ing inference results may in turn be misleading. Nonparametric inference methods have been
developed with the objective of providing statistical inference methods that are free from any
distributional assumptions. Thus the term “distribution-free method” may be considered to
be synonymous with the term “nonparametric method” and may in fact be a more helpful
term.
While nonparametric methods are thus valid under weaker assumptions than parametric
methods, it should be realized that if the distributional assumption on which a paramet-
ric analysis is based is valid, then it will allow a more precise or more powerful analysis
than the corresponding nonparametric method. Consequently, the comparison between non-
parametric and parametric approaches to a problem can usefully be seen as a compromise
between assumptions and precision. However, the more general validity of the nonparametric
approaches provides the motivation for their widespread adoption in engineering and other
sciences.
In this chapter the analysis of a single population is considered ﬁrst, followed by the com-
parison of two populations and then the comparisons of three or more populations (one-way
layouts). These types of problems have been considered in previous chapters where parametric
methodologies such as t-tests, two-sample t-tests, and analysis of variance techniques have
been considered. In this chapter alternative nonparametric inference methods are proposed
and are compared with the parametric methodologies.
694

15.1 THE ANALYSIS OF A SINGLE POPULATION
695
15.1
The Analysis of a Single Population
Chapter 8 (using the results presented in Chapters 6 and 7) contains a discussion of the analysis
of a sample of data observations from a single population. Much of this analysis concerns the
important case where the data observations can be taken to be normally distributed. In this
section alternative analysis techniques that are not based on any distributional assumptions
are discussed.
15.1.1
The Distribution Function
Consider a data set that consists of observations from some common unknown distribution
function F(x). A sensible starting point in the analysis of the data set is to obtain some
information about the form of the distribution function F(x). In this section the construction
of a nonparametric estimate ˆF(x) for the unknown distribution function is considered, which
is based only on the assumption that the data represent independent, identically distributed
observations from this common unknown distribution function.
For any value x0 of interest, the estimate ˆF(x) provides a point estimate of F(x0), the
probability that an observation is no larger than a speciﬁc value x0. ˆF(x) can also be used to
estimate speciﬁc quantiles F−1(p) of the distribution. The construction of conﬁdence bands
for the distribution function F(x) provides an assessment of the accuracy of the nonparametric
estimate ˆF(x). In addition, these conﬁdence bands can be used to test the plausibility that the
true distribution function F(x) is of a speciﬁc form F0(x).
The Empirical Cumulative Distribution Function
Given a data set x1, . . . , xn of independent observations from an unknown distribution
with a cumulative distribution function F(x), the empirical cumulative distribution
function
ˆF(x) = #xi ≤x
n
provides an estimate of F(x). The empirical cumulative distribution function is an
increasing step function between the values of 0 and 1. A single data observation xi
causes the empirical distribution function to increase by an amount 1/n at xi. If there
are m data observations each taking the value xi, then the function will increase by an
amount m/n at xi.
The Empirical Cumulative Distribution Function
The empirical (or sample) cumulative
distribution function provides a simple estimate of an unknown probability distribution. It can
be considered to be the “best guess” of the true form of the underlying unknown distribution
function F(x). However, it should be pointed out that it is not necessarily expected that F(x)
looks exactly like the estimate ˆF(x). For example, the true distribution function F(x) may be
considered to be smooth (corresponding to a continuous probability density function) rather
than a step function, which corresponds to a discrete probability mass function. Nevertheless,

696
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
the empirical cumulative distribution function ˆF(x) should provide a close ﬁt to the true
distribution function F(x) for reasonable sample sizes n.
Notice that if
S(x) = #xi ≤x
then
S(x) ∼B(n, F(x))
This binomial distribution for S(x) implies that
E( ˆF(x)) = E(S(x))
n
= n F(x)
n
= F(x)
and that
Var( ˆF(x)) = Var(S(x))
n2
= n F(x)(1 −F(x))
n2
= F(x)(1 −F(x))
n
Consequently, for a given value x, the empirical cumulative distribution function ˆF(x) is seen
to be an unbiased estimator of F(x), whose variance is a decreasing function of n.
The empirical cumulative distribution function ˆF(x) becomes more accurate when it is
based upon larger sample sizes. One reason for this is that for an underlying continuous
distribution function F(x), the estimate ˆF(x) becomes smoother at larger sample sizes since
the step sizes 1/n become smaller. In addition, since the variance of the empirical cumulative
distribution function ˆF(x) is a decreasing function of the sample size n, it becomes a more
and more precise estimator of F(x) for larger sample sizes.
Notice that as well as providing estimates of the cumulative distribution function F(x) at
various values of x, the empirical cumulative distribution function ˆF(x) provides estimates
of the quantiles F−1(p) of the distribution for various values of p ∈(0, 1). This is done by
ﬁnding the x value (or values) for which ˆF(x) = p or, equivalently, ﬁnding the values of x
for which a proportion p of the data set is less than x.
It is interesting to compare this nonparametric approach to estimating the distribution
function with a parametric approach. For a given distributional assumption, the parametric
estimate will be the assumed distribution function evaluated at the estimated parameter val-
ues. For example, if the data set x1, . . . , xn is assumed to consist of independent, identically
distributed observations from a normal distribution, and if (x; μ, σ) represents the cumu-
lative distribution function of a normal distribution with mean μ and variance σ 2, then the
distribution function may be estimated by ˆF(x) = (x; ˆμ, ˆσ) for parameter estimates
ˆμ = ¯x =
n
i=1 xi
n
and
ˆσ 2 = s2 =
n
i=1(xi −¯x)2
n −1
Finally, notice that attention is directed toward estimating the cumulative distribution
function F(x) of an unknown distribution rather than toward estimating the probability den-
sity function f (x), although there is a one-to-one correspondence between these two. The
reason for this is that the density function that corresponds to the empirical cumulative distri-
bution function ˆF(x) consists of point masses of amount 1/n at each of the data points. If the
density (mass) function f (x) is actually discrete, then for a large enough sample size (with
consequently many repeat observations) this may be a good estimate of the probability mass
function. However, for a continuous density function f (x), this is not a useful representation.
In this case an idea of the form of the density function f (x) can best be obtained by grouping
the data and drawing a histogram.

15.1 THE ANALYSIS OF A SINGLE POPULATION
697
FIGURE 15.1
Execution times in seconds for the
computer system performance
example
7.5
7.5
7.8
7.9
9.6
10.4
10.6
11.8
11.8
17.0
17.2
22.8
23.1
33.0
8.1
8.1
8.2
8.9
9.3
9.3
5.9
6.0
6.4
6.4
6.5
6.5
6.6
6.7
6.9
7.0
7.1
7.2
12.6
12.9
14.3
15.0
16.2
16.3
40.0
42.8
43.0
44.8
45.0
45.8
FIGURE 15.2
Histogram of execution times for
the computer system performance
example
1
2
3
4
5
6
7
8
9
10
11
12
5
10
15
20
25
30
35
40
45
Execution time (seconds)
Frequency
Example 76
Computer System
Performance
A computer system for recording, updating, and delivering maintenance requests for a large
organization is being evaluated. The evaluation consists of recording the execution times for
typical tasks that the system has to handle. At n = 44 randomly selected times a series of tasks
is submitted to the system, and the average execution time for the tasks is calculated. The data
set shown in Figure 15.1 is obtained, where the data values have been placed in increasing
order. Since the series of tasks submitted to the computer is the same for each of the n = 44
data observations, the data set provides information on the variations in the execution times
due to changes in the load on the computer system.
What can we learn from this data set about the distribution of the execution times? Fig-
ure 15.2 shows a histogram of the data set. The empirical cumulative distribution function
is calculated in Figure 15.3 and it is plotted in Figure 15.4. The sample median execution
time (see Section 6.3.2) can be taken to be the average of the twenty-second and twenty-third
smallest execution times, which is
9.3 + 9.6
2
= 9.45
seconds. The upper and lower sample quartiles (see Section 6.3.6) are 16.825 and 7.125 sec-
onds, respectively. Since 35 out of the 44 observations are smaller than 20 seconds, the

698
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
x
ˆF(x) =
xi ≤x
n
5.9
1/44 = 0.023
6.0
2/44 = 0.045
6.4
4/44 = 0.091
6.5
6/44 = 0.136
6.6
7/44 = 0.159
6.7
8/44 = 0.182
6.9
9/44 = 0.205
7.0
10/44 = 0.227
7.1
11/44 = 0.250
7.2
12/44 = 0.273
7.5
14/44 = 0.318
7.8
15/44 = 0.341
7.9
16/44 = 0.364
8.1
18/44 = 0.409
8.2
19/44 = 0.432
8.9
20/44 = 0.455
9.3
22/44 = 0.500
9.6
23/44 = 0.523
10.4
24/44 = 0.545
10.6
25/44 = 0.568
11.8
27/44 = 0.614
12.6
28/44 = 0.636
12.9
29/44 = 0.659
14.3
30/44 = 0.682
15.0
31/44 = 0.705
16.2
32/44 = 0.727
16.3
33/44 = 0.750
17.0
34/44 = 0.773
17.2
35/44 = 0.795
22.8
36/44 = 0.818
23.1
37/44 = 0.841
33.0
38/44 = 0.864
40.0
39/44 = 0.886
42.8
40/44 = 0.909
43.0
41/44 = 0.932
44.8
42/44 = 0.955
45.0
43/44 = 0.977
45.8
44/44 = 1.000
#
FIGURE 15.3
Calculation of the empirical
cumulative distribution function for the
computer system performance example
10
0
2
4
6
8
12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
ˆF(x)
Execution time (seconds)
FIGURE 15.4
The empirical cumulative distribution function for the computer system performance example
probability of an execution time quicker than 20 seconds can be estimated as
ˆF(20) = #xi ≤20
44
= 35
44 = 0.795
Conﬁdence Bands for the Distribution Function
Although the empirical cumulative dis-
tribution function ˆF(x) may be considered to be the “best guess” of the true unknown distri-
bution function F(x), it is also useful to calculate some conﬁdence bands for the distribution
function F(x). Conﬁdence bands with a conﬁdence level of 1 −α have the property that there

15.1 THE ANALYSIS OF A SINGLE POPULATION
699
is is a conﬁdence level of 1 −α that the true distribution function F(x) lies completely within
the bands for all x values.
The conﬁdence bands are constructed by taking the empirical cumulative distribution
function ˆF(x) and by shifting it up and down by a certain amount dα,n. Bands that exceed 1
are then truncated at 1, and bands below 0 are truncated at 0. For x values at which no truncation
of the bands occurs, the point estimate ˆF(x) is at the center of the conﬁdence bands.
The shift amount dα,n depends on both the conﬁdence level 1 −α and the precision of the
empirical cumulative distribution function ˆF(x) through the sample size n. Intuitively, it is
clear that dα,n should be a decreasing function of α since the width of the conﬁdence bands
should increase as the conﬁdence level increases, and it is also clear that dα,n should be a
decreasing function of n because the width of the conﬁdence bands should decrease as the
precision of the empirical cumulative distribution function ˆF(x) increases due to increases in
the sample size n.
HISTORICAL NOTE
Andrei Nikolaevich
Kolmogorov (1903–1987) was
a Russian mathematician
whose work inﬂuenced several
branches of modern
mathematics. The study of
probability theory was
considered to be his basic
speciality. He graduated from
Moscow State University in
1925 and in 1931 he was
elected a professor there. In
1939 he was elected an
academician of the Academy of
Sciences of the USSR, and in
the latter part of his life he paid
great attention to the problems
of the mathematical education
of schoolchildren.
For sample sizes n larger than 40, the appropriate shift amount dα,n is given by
dα
√n
where the values of dα are
α
0.20
0.10
0.05
0.02
0.01
dα
1.07
1.22
1.36
1.52
1.63
For smaller sample sizes the values of dα,n are given in Table VI. They were originally calcu-
lated by the Russian probabilist Andrei Nikolaevich Kolmogorov in 1933, and the conﬁdence
bands are usually referred to as the Kolmogorov conﬁdence bands.
Kolmogorov Conﬁdence Bands
Conﬁdence bands around an empirical cumulative distribution function ˆF(x) with
conﬁdence level 1 −α are constructed by adding and subtracting an amount dα,n (see
Table VI) to the empirical cumulative distribution function. The bands are truncated
at 0 and 1.
Example 76
Computer System
Performance
Conﬁdence bands with a conﬁdence level of 95% are calculated with
d0.05,44 = d0.05
√
44
= 1.36
√
44
= 0.205
and are shown in Figure 15.5. The true unknown cumulative distribution function F(x) lies
completely within these conﬁdence bands with a conﬁdence level of 95%.
Testing Whether F(x) = F0(x)
Consider the problem of assessing the evidence that the
unknown cumulative distribution function F(x) is equal to a speciﬁed distribution function
F0(x). This problem may be formulated as a hypothesis testing problem with a null hypothesis
H0 : F(x) = F0(x)

700
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
FIGURE 15.5
Conﬁdence bands for the computer
system performance example
10
0
2
4
6
8
12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Execution time (seconds)
Upper confidence band
Lower confidence band
ˆF(x)
0.205
0.205
for all values of x, and with a general alternative hypothesis that F(x) ̸= F0(x) for some value
of x. Notice that this null hypothesis is subtly different from the usual kinds of hypotheses,
which concern the values taken by certain parameters, such as H0 : μ = μ0. This null
hypothesis requires that two functions be identical for all x values.
A size α hypothesis test can be performed by checking whether the speciﬁed distribution
function F0(x) lies completely within the 1−α conﬁdence bands for F(x). The null hypothesis
is accepted if F0(x) lies completely within the conﬁdence bands and is rejected if F0(x) lies
outside the conﬁdence bands for some x values. The test can be done either graphically or by
checking whether the value of the statistic
maxx| ˆF(x) −F0(x)|
is less than dα,n in which case the null hypothesis is accepted, or is greater than dα,n in which
case the null hypothesis is rejected, as shown in Figure 15.6. If this latter approach is taken,
then it is useful to notice that the maximum value of | ˆF(x) −F0(x)| must occur when x is
equal to one of the data points xi, at which point the empirical cumulative distribution function
ˆF(x) has a step. However, it is important to check the value of | ˆF(x) −F0(x)| at both the top
and the bottom of the steps.

15.1 THE ANALYSIS OF A SINGLE POPULATION
701
FIGURE 15.6
Using the conﬁdence bands on the
empirical cumulative distribution
function to perform hypothesis tests
regarding the distribution function
1.0
0.8
0.6
0.4
0.2
0.0
Upper confidence band
Lower confidence band
Upper confidence band
ˆF
F0(x)
H0 : F(x) = F0(x) is accepted
x
1.0
0.8
0.6
0.4
0.2
0.0
ˆF
F0(x)
H0 : F(x) = F0(x) is rejected
x
Lower confidence band
(x)
(x)
If the null hypothesis is accepted, then it is important not to fall into the trap of concluding
that the hypothesis test has “proved” that the distribution function is F0(x). All that has hap-
pened is that F0(x) has been shown to be one of the many plausible distribution functions that
lie within the conﬁdence bands. Finally, recall that the chi-square goodness of ﬁt tests de-
scribed in Section 10.3.2 provide alternative methods of testing the distribution from which a
sample is drawn, and probability plots such as a normal scores plot discussed in Section 12.7.1
can also be used in certain circumstances.

702
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
Testing Whether F(x) = F0(x)
The null hypothesis
H0 : F(x) = F0(x)
that an unknown cumulative distribution function F(x) is equal to a speciﬁed
cumulative distribution function F0(x) is accepted at size α if F0(x) lies completely
within the 1 −α conﬁdence level conﬁdence bands for the empirical cumulative
distribution function ˆF(x) and is rejected if F0(x) lies outside the conﬁdence bands
for some x values. The hypothesis test can be done either graphically or by checking
whether the value of the statistic
maxx| ˆF(x) −F0(x)|
is less than dα,n in which case the null hypothesis is accepted, or is greater than dα,n in
which case the null hypothesis is rejected.
Example 76
Computer System
Performance
The sample average execution time is about 15 seconds, and no observations are observed that
are quicker than 5 seconds. Is it reasonable to model the execution times as being 5 seconds
plus a random variable that has an exponential distribution with mean 10 seconds? This model
corresponds to a cumulative distribution function
F0(x) = 1 −e−0.1(x−5)
for x ≥5 (see Section 4.2), which is plotted together with the empirical cumulative distri-
bution function and the conﬁdence bands in Figure 15.7. Since the cumulative distribution
function F0(x) lies completely within the conﬁdence bands, it is a plausible model for the
execution times.
It should be noted that the conﬁdence bands constructed in this manner can be wide and
imprecise. In the example concerning computer system execution times with a sample size
of n = 44, the total length of the conﬁdence band at a particular x value (where there is no
truncation) is 2 × d0.05,44 = 2 × 0.205 = 0.410. If a sample of size n = 100 is available, the
conﬁdence band length is still 2 × d0.05,100 = 2 × 1.36/
√
100 = 0.272. Conﬁdence bands
at the 0.99 conﬁdence level are even wider. However, one useful point to notice concerning
experimental design is that the experimenter knows in advance the precision afforded by a
certain sample size. This information may be a useful component in the determination of how
large a sample size n to obtain.
The apparent imprecision of the conﬁdence bands is not due to any failings in their
construction. In fact, for a given conﬁdence level they are the best conﬁdence bands available,
and they efﬁciently summarize the information attainable from the given sample size and data
set. In addition, it must be understood that they are simultaneous conﬁdence bands. In other
words, at the given conﬁdence level they provide conﬁdence bounds on F(x) simultaneously
for all values of x.
15.1.2
The Sign Test
The sign test is a basic nonparametric testing procedure that can be used to assess the plausi-
bility of the null hypothesis
H0 : F(x0) = p0

15.1 THE ANALYSIS OF A SINGLE POPULATION
703
FIGURE 15.7
A plausible model for the execution
times in the computer system
performance example
10
0
2
4
6
8
12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Execution time (seconds)
Upper confidence band
Lower confidence band
ˆF
ˆF0
= 1 −e−0.1(x−5)
(x)
(x)
based upon a data set x1, . . . , xn of values that are assumed only to be independent and
identically distributed observations from the unknown distribution F(x). For certain ﬁxed
values of x0 and p0, this null hypothesis implies that the unknown underlying distribution
function has the property that an observation no larger than x0 occurs with a probability of p0.
Consideration of the statement F(x0) = p0 is an example of how the experimenter can
focus on a speciﬁc property of the unknown distribution function F(x) and obtain an analysis
that is more efﬁcient than the more general overall estimation of the distribution function F(x)
for all values of x discussed in Section 15.1.1. A common example of this approach is to take
p0 = 0.5, in which case F(x0) = 0.5 states that the value x0 is the median of the distribution.
The median, which will also be the mean of the distribution if it is a symmetric distribution,
is often a useful summary statistic of the distribution with a meaningful interpretation to the
experimenter.
As well as the median, inferences on the upper quartile or the lower quartile of the distri-
bution may be of interest and are obtained by taking p0 = 0.75 and p0 = 0.25, respectively.
In other cases the tails of the distribution may be of interest.
The ﬁrst step in the implementation of the sign test is to discard all data observations that
are equal to x0. The sample size n is also reduced accordingly. The key aspect of the sign test

704
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
is then that the remaining data observations are reduced to the single statistic
S(x0) = #xi < x0
which counts the number of the data observations that are no larger than x0. This means that
a particular data observation xi inﬂuences the statistic S(x0) (and consequently inﬂuences the
sign test) only through the consideration of whether it is less than x0 or greater than x0. Notice
that the statistic S(x0) must take an integer value between 0 and n.
Consideration of the statistic S(x0) reveals that it has a binomial distribution with param-
eters n and F(x0)
S(x0) ∼B(n, F(x0))
For reasonably large values of the sample size n it is also useful to consider the following
approximate normal distribution for S(x0)/n
S(x0)/n ∼N

F(x0), F(x0)(1 −F(x0))
n

The assessment of the plausibility of the hypothesis F(x0) = p0 is based on whether the
statistic S(x0) looks as if it could have come from a B(n, p0) distribution or, if the normal
approximation is being employed, whether the statistic S(x0)/n looks as if it comes from a
N

p0, p0(1 −p0)
n

distribution.
A p-value can be obtained by comparing S(x0) with the tails of the B(n, p0) distribution or
by comparing S(x0)/n with the tails of the N(p0, p0(1−p0)/n) distribution. Either one-sided
or two-sided tests can be performed depending on whether the experimenter is interested in
alternative hypotheses of the form F(x0) > p0 or F(x0) < p0 in which case one-sided tests
are appropriate, or of the form F(x0) ̸= p0 in which case a two-sided test is appropriate. The
details of the hypothesis tests are similar to the tests on a population proportion discussed in
Section 10.1.2.
The Sign Test
The sign test for the null hypothesis
H0 : F(x0) = p0
is based on the statistic
S(x0) = #xi < x0
which counts the number of the data observations that are no larger than x0 (once any
data observations equal to x0 have been discarded). For the two-sided alternative
hypothesis
HA : F(x0) ̸= p0
the exact p-value is
p-value = 2 × P(X ≥S(x0))

15.1 THE ANALYSIS OF A SINGLE POPULATION
705
The Sign Test, continued
if S(x0) > np0, and
p-value = 2 × P(X ≤S(x0))
if S(x0) < np0, where the random variable X has a B(n, p0) distribution. When np0
and n(1 −p0) are both larger than 5, a normal approximation can be used to give a
p-value of
p-value = 2 × (−|z|)
where (·) is the standard normal cumulative distribution function and
z =
S(x0) −np0
√np0(1 −p0)
In order to improve the normal approximation the value S(x0) −np0 −0.5 may
be used in the numerator of the z-statistic when S(x0) −np0 > 0.5, and the value
S(x0) −np0 + 0.5 may be used in the numerator of the z-statistic when
S(x0) −np0 < 0.5.
For the one-sided alternative hypothesis
HA : F(x0) < p0
the exact p-value is
p-value = P(X ≤S(x0))
which can be approximated by
p-value = (z)
For the one-sided alternative hypothesis
HA : F(x0) > p0
the exact p-value is
p-value = P(X ≥S(x0))
which can be approximated by
p-value = 1 −(z)
The discussion above has dealt with the assessment of the plausibility of the statement
F(x0) = p0 for ﬁxed values of x0 and p0. An extension of this analysis is to consider only
p0 ﬁxed, and then to consider the statement F(x0) = p0 for all values of x0 and to “collect”
those values of x0 for which the statement is “acceptable” at a given error rate. Formally,
if a collection is made of the x0 values for which a size α hypothesis test of the statement
F(x0) = p0 accepts, then the collection forms a conﬁdence interval for the quantile F−1(p0)
with a conﬁdence level equal to 1 −α.
An important special case concerns the construction of a conﬁdence interval for the median
of the distribution F−1(0.5), which can be obtained by taking p0 = 0.5. Such a conﬁdence
interval is generally of more use to the experimenter than the calculation of the p-values for
individual statements that the median takes a speciﬁc value. The conﬁdence interval can be
thought of as summarizing the plausible values for the median.

706
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
If for a data set x1, . . . , xn of size n the ordered values of the data observations are denoted
by x(1) ≤· · · ≤x(n), then the conﬁdence interval for the median generated by the sign test is
of the form
F−1(0.5) ∈(x(m), x(n+1−m))
for some integer value of m no larger than (n + 1)/2. The particular value of m depends on
the conﬁdence level required. Larger conﬁdence levels result in smaller values of m, since for
instance, the conﬁdence interval (X(10), X(n−9)) is shorter and has a smaller conﬁdence level
than the conﬁdence interval (X(9), X(n−8)). However, repeat observations (so that for instance
X(9) = X(10)) may cause some of the conﬁdence interval endpoints to remain unchanged
when the conﬁdence level changes. Strictly speaking, only conﬁdence intervals with certain
discrete conﬁdence levels are available. For two-sided conﬁdence intervals these conﬁdence
levels are 1 −2p, where p is a tail probability of the B(n, 0.5) distribution.
Statistical software packages can be used to perform the sign test and to obtain conﬁdence
intervals for the median of a distribution, as illustrated in the following examples.
Example 45
Fabric Water
Absorption Properties
Figure 15.8 shows the ordered values of the n = 15 data observations of % water pickup ob-
tained with a roller pressure of 10 pounds per square inch. Could the median water pickup be
65%? Consider the hypotheses
H0 : F(65) = 0.5
versus
HA : F(65) ̸= 0.5
There are no data observations equal to 65, 2 data observations are larger than 65, and 13 data
observations are smaller than 65, so that
S(65) = 13
The exact p-value is therefore
p-value = 2 × P(X ≥13) = 0.0074
where the random variable X has a B(15, 0.5) distribution. This small p-value indicates that
65% is not a plausible value for the median value of the % water pickup.
If a conﬁdence interval for the median % water pickup is of interest with a 1 −α = 0.95
conﬁdence level, then a statistical software package can provide the conﬁdence intervals with
conﬁdence levels either side of 95%. These intervals are
(56.70, 61.80)
with a conﬁdence level of 1 −α = 0.8815, and
(55.80, 64.00)
with a conﬁdence level of 1 −α = 0.9648. Notice that if x(1) ≤· · · ≤x(15) are the ordered
values of the data observations, then these conﬁdence intervals are (x(5), x(11)) and (x(4), x(12)).
FIGURE 15.8
Ordered % water pickup values for
the fabric water absorption
properties example
51.8
54.5
54.5
55.8
56.7
57.3
59.1
59.5
60.4
61.2
61.8
64.0
64.9
65.4
70.2

15.1 THE ANALYSIS OF A SINGLE POPULATION
707
Example 76
Computer System
Performance
Consider a test that the median execution time is equal to 10 seconds. With the two-sided
hypotheses.
H0 : F(10) = 0.5
versus
HA : F(10) ̸= 0.5
there are no data observations equal to 10, 21 data observations larger than 65, and 23 data
observations smaller than 65, so that
S(10) = 23
and the exact p-value is
p-value = 2 × P(X ≥23) = 0.8804
where the random variable X has a B(44, 0.5) distribution. Consequently, 10 seconds is clearly
a plausible value for the median execution time. This is not surprising because 10 seconds is
contained within the conﬁdence intervals for the median execution time, which can be found
to be
(x(17), x(28)) = (8.10, 12.60)
with a conﬁdence level of 0.9039 and
(x(16), x(29)) = (7.90, 12.90)
with a conﬁdence level of 0.9512.
Finally, consider the one-sided hypothesis testing problem
H0 : F(7.5) ≥0.5
versus
HA : F(7.5) < 0.5
The null hypothesis states that the probability of an execution time being shorter than 7.5 sec-
onds is larger than 0.5, which in other words implies that the median execution time is no
longer than 7.5 seconds. The alternative hypothesis states that the median execution time is
longer than 7.5 seconds.
The ﬁrst step in this analysis is to discard the two data observations equal to x0 = 7.5.
Of the remaining 42 data observations, 12 are smaller than 7.5 seconds and 30 are larger than
7.5 seconds, and so with
S(7.5) = 12
the p-value is
p-value = P(X ≤12) = 0.004
where the random variable X has a B(42, 0.5) distribution. This analysis therefore establishes
that the median execution time is longer than 7.5 seconds.
The sign test is a particularly appropriate methodology for providing a simple basic anal-
ysis of paired data sets (see Section 9.2). Remember that the analysis of paired data sets
x1, . . . , xn and y1, . . . , yn reduces to a one-sample problem through the consideration of the
differences
z1 = x1 −y1, . . . , zn = xn −yn
If F(x) is deﬁned to be the distribution function of these differences, then z1, . . . , zn can be
considered to be independent, identically distributed observations from this distribution, and
the sign test can be used to make inferences on the distribution function F(x).

708
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
Speciﬁcally, the experimenter is usually interested in estimating the median of the distri-
bution F(x) and assessing whether or not it is 0. If the median is 0, then this implies that an
observation xi is equally likely to be less than or greater than the corresponding observation
yi, and this implies a certain equivalence between the process that generated the observations
xi and the process that generated the observations yi. If the median is greater than 0, then this
implies that the observations from the ﬁrst population tend to be larger than the observations
from the second population, and vice versa if the median is less than 0.
If the sign test is used to investigate whether the median of the distribution function F(x)
is 0, then the analysis depends on how many of the differences zi are positive and how many
are negative. In other words, the sign test is simply based on how many pairs there are with
xi > yi and how many there are with xi < yi.
Example 55
Heart Rate Reductions
Figure 9.13 contains the percentage reductions in heart rate for the standard drug xi and the
new drug yi, together with the differences zi for the n = 40 experimental subjects. The
assessment using the sign test of whether the median reductions are the same for both drugs is
based on the fact that 30 of the differences zi are negative while only 10 are positive. In other
words, 30 of the patients exhibited a larger reduction in heart rate using the new drug rather
than with the standard drug, while only 10 patients exhibited a larger reduction in heart rate
using the standard drug rather than with the new drug.
Formally, the two-sided hypothesis testing problem of interest is
H0 : F(0) = 0.5
versus
HA : F(0) ̸= 0.5
where F(x) is the distribution function of the differences zi. With
S(0) = 30
the exact p-value is
p-value = 2 × P(X ≥30) = 0.0022
where the random variable X has a B(40, 0.5) distribution. This low p-value conﬁrms that it
is not plausible that the two drugs have equivalent effects.
Finally, it is interesting to note that the sign test is one of the oldest and most fundamental
testing procedures. In fact, the sign test can be traced back to 1710 when John Arbuthnot,
physician to Queen Anne, used data on infants in the city of London to investigate whether
male births were more frequent than female births.
In addition, remember that gambling has provided much of the impetus behind the devel-
opment of probability theory and statistical analysis, and in this context consider the following
two questions:
■
If a coin is tossed 50 times and 30 heads and 20 tails are observed, what is the
plausibility of the coin being fair?
■
If a six-sided die is rolled 100 times and a 6 is scored on 10 of the rolls, what is the
plausibility of the die being fair?
These questions can both be easily answered with the use of the sign test. A common
feature of the two questions is that the data are of a binary form taking one of two possible
outcomes. In the ﬁrst question an observation is recorded as “a head or a tail,” and in the second
question an observation is recorded as being “ a 6 or not a 6.” With this in mind, the key to an
understanding of the application of the sign test toward the consideration of statements of the

15.1 THE ANALYSIS OF A SINGLE POPULATION
709
FIGURE 15.9
Some symmetric probability
density functions
The mean and the median are identical for 
symmetric probability density functions.
kind F(x0) = p0 is that the data observations xi are being reduced to a binary form through
the determination of whether xi > x0 or whether xi < x0.
15.1.3
The Signed Rank Test
The signed rank test (often referred to as the Wilcoxon one-sample test procedure) can be
used to make inferences about the median of an unknown distribution function F(x) under the
assumption that the probability density (mass) function is symmetric, as shown in Figure 15.9.
If the distribution is symmetric, then the median of the distribution is equal to the mean of
the distribution, and they can both be denoted by μ = F−1(0.5). Recall that the sign test
discussed in Section 15.1.2 can also be used to provide inferences about the median without
the necessity of making any assumptions about the distribution function F(x). However, the
motivation behind the use of the signed rank test is that it provides a more precise analysis in
situations where the distribution function can reasonably be taken to be symmetric.
The assumption of symmetry may be justiﬁed by an experimenter through observation or
through an understanding of the nature of the data. For example, a simple histogram of the data
may be sufﬁcient to indicate that the assumption of symmetry is not inappropriate. In other
cases, the experimenter may expect that the randomness in the data is composed primarily of
a measurement error that can reasonably be expected to be distributed symmetrically.
The additional precision of the signed rank test over the sign test is achieved through
the consideration of the magnitudes of the deviations of the observed data values xi from the
hypothesized median value μ0. The sign test is performed by looking at whether an observation
xi is less than μ0 or is greater than μ0. The signed rank test not only takes into account this
information but also considers
|xi −μ0|
the magnitude of the distance between xi and μ0. The magnitudes provide additional infor-
mation about the location of the median when the distribution is assumed to be symmetric.
Speciﬁcally, the signed rank test procedure for testing the plausibility of the statement
μ = μ0 concerning the mean of a symmetric distribution, based on a data sample x1, . . . , xn

710
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
of independent observations from this distribution, operates by ﬁrst calculating the deviations
of the data observations xi from the hypothesized mean value μ0
di(μ0) = xi −μ0
As long as the data are not all to one side of μ0, some of these deviations will be positive and
some will be negative. The ranks ri(μ0) are then calculated by ranking the absolute values
of the deviations | di(μ0) |. For example, if n = 5 and
|d3(μ0)| < |d2(μ0)| < |d5(μ0)| < |d1(μ0)| < |d4(μ0)|
then r3(μ0) = 1, r2(μ0) = 2, r5(μ0) = 3, r1(μ0) = 4, and r4(μ0) = 5. The signed rank
test is based on the consideration of the statistic S+(μ0), which is the sum of the ranks of the
observations xi with positive deviations, di(μ0) > 0.
The sum of all of the ranks is
1 + 2 + · · · + n = n(n + 1)
2
and if μ0 really is the mean of the distribution, then S+(μ0) can be expected to be about half of
this value, that is about n(n + 1)/4. However, if S+(μ0) is much larger than n(n + 1)/4, then
there is the suggestion that the mean μ is really larger than μ0, as indicated in Figure 15.10.
On the other hand, if S+(μ0) is much smaller than n(n + 1)/4, then there is the suggestion
that the mean μ is really smaller than μ0.
Two practical points concerning the calculation of the statistic S+(μ0) involve data ob-
servations xi that are equal to μ0 and tied values of the magnitudes of the deviations |di(μ0)|.
If an observation xi is equal to μ0, then it should be deleted from the data set and the sample
FIGURE 15.10
Interpretation of the signed rank
test statistic S+(μ0)
+
++
+ + + + +++
μ0
Data values
di(μ0) = xi −μ0 > 0
di(μ0) = xi −μ0 < 0
(μ0) = sum of ranks
+
++
+ + + + +++
μ0
Data values
di(μ0) = xi −μ0 > 0
di(μ0) = xi −μ0 < 0
(μ0) = sum of ranks
S
(μ0) > n(n+1)
4
suggests μ > μ0
(μ0) < n(n+1)
4
suggests μ < μ0
+
S+
S+
S+

15.1 THE ANALYSIS OF A SINGLE POPULATION
711
size n should be reduced by 1. If some of the absolute values of the deviations |di(μ0)| are
tied, which happens if there are repeat data observations xi, or if some data values are equal
amounts above and below μ0, then it is appropriate to allocate each deviation the average
of the corresponding ranks. For instance, if the ﬁfth and sixth smallest deviation magnitudes
|di(μ0)| are equal, then they should both be ranked 5.5. If the ﬁfth, sixth, and seventh smallest
deviation magnitudes |di(μ0)| are all equal, then they should all be ranked 6. This procedure
ensures that the sum of the ranks always equals n(n + 1)/2.
Either one-sided or two-sided p-values can be constructed for the plausibility of the
statement μ = μ0 by comparing the observed value of the statistic S+(μ0) with its distribution
whenμ = μ0.Thisdistributioncanbecalculatedexactlyforsmallsamplesizesn,andanormal
approximation to the distribution sufﬁces for larger sample sizes.
If there are no ties in the absolute values of the deviations |di(μ0)|, and if μ = μ0, then
the statistic S+(μ0) can be written as
S+(μ0) = (1 × W1) + (2 × W2) + · · · + (n × Wn)
where W1, . . . , Wn are independent, identically distributed random variables that are equally
likely to take the values 0 and 1. This representation of S+(μ0) follows from the fact that
when μ = μ0, so that the distribution is symmetric about μ0, the deviation magnitude |di(μ0)|
corresponding to a particular rank j, say, is equally likely to be based upon an observation
xi that is greater than μ0 or that is less than μ0. Consequently, each of the ranks 1, . . . , n is
equally likely to contribute to S+(μ0) or not to contribute.
The exact distribution of the statistic S+(μ0) when μ = μ0 is straightforward to calculate
using this representation, although the calculations quickly become computationally inten-
sive as the sample size n increases. Essentially, there are 2n possible sets of values of the
random variables W1, . . . , Wn that are each equally likely with probability 1/2n. The exact
discrete distribution of S+(μ0) is calculated by noticing that there is a probability of m/2n
that S+(μ0) = x, where m is the number of possible sets of values of W1, . . . , Wn for which
(1 × W1) + (2 × W2) + · · · + (n × Wn) = x
Usually a computer package will calculate an exact p-value in this manner, although an
approximate normal distribution for the statistic S+(μ0) may be used if the sample size is
large. This normal approximation is determined by the mean and the variance of the statistic.
When μ = μ0, the mean value of S+(μ0) is n(n+1)/4, and since Var(Wi) = 1/4 the variance
of S+(μ0) is
Var(S+(μ0)) = (12 × Var(W1)) + (22 × Var(W2)) + · · · + (n2 × Var(Wn))
= 12 + 22 + · · · + n2
4
= n(n + 1)(2n + 1)
24
Consequently, when μ = μ0 the statistic
Z+(μ0) = S+(μ0) −n(n + 1)/4
√n(n + 1)(2n + 1)/24
can be considered to have approximately a standard normal distribution.
In practice, a continuity correction of 0.5 helps increase the accuracy of the p-value
calculations made by comparing the observed value of Z+(μ0) with the tail probabilities of
the standard normal distribution. The amount 0.5 should be added to the numerator of Z+(μ0)
if it is negative and subtracted from the numerator of Z+(μ0) if it is positive.
A positive value of Z+(μ0) suggests that the mean μ is larger than μ0, and a nega-
tive value of Z+(μ0) suggests that the mean μ is smaller than μ0. One-sided or two-sided
p-values can be obtained by comparing the observed value of Z+(μ0) with the tail probabilities

712
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
of the standard normal distribution. For example, if Z+(μ0) is observed to be 2.00, then a
one-sided p-value (for the alternative that the mean value is greater than μ0) is calculated as
1 −(2.00) = 0.0228, and a two-sided p-value is 2 × 0.0228 = 0.0456.
The Signed Rank Test
The signed rank test procedure for testing that the mean of a symmetric distribution is
equal to a hypothesized value μ0, based upon a data set x1, . . . , xn, is performed by
ranking the absolute values of the deviations di(μ0) = xi −μ0. The test is based upon
the consideration of the statistic S+(μ0), which is the sum of the ranks of the
observations xi with positive deviations di(μ0) > 0. One-sided or two-sided p-values
can be calculated by comparing this statistic with its distribution when the mean is
equal to μ0.
The signed rank test procedure can be used to construct conﬁdence intervals for the mean
of a distribution when it is assumed to be symmetric. A 1 −α conﬁdence level conﬁdence
interval consists of the values μ0 for which the null hypothesis H0 : μ = μ0 is accepted at
size α. As with the sign test, only certain discrete values of the conﬁdence level are attainable
although they can often be found very close to typical values such as 95% and 99%. The use
of the signed rank test is illustrated in the following examples.
Example 45
Fabric Water
Absorption Properties
Consider the two-sided hypothesis testing problem that the median water pickup is 65%
H0 : F(65) = 0.5
versus
HA : F(65) ̸= 0.5
based on the assumption that the distribution of the % water pickup is symmetric. Figure 15.11
shows the deviations of the data values from μ0 = 65 together with the ranks of the absolute
values. The two observations with positive deviations, x = 65.4 and x = 70.2, have ranks 2
and 7 respectively, so that the statistic employed by the signed rank test is
S+(65) = 2 + 7 = 9
FIGURE 15.11
Calculation of the signed rank test
statistic for the fabric water
absorption properties example
Data 
values 
xi
Absolute 
deviations 
|di(65)|
Deviations 
di(65) = xi − 65
Ranks 
ri(65)
51.8
−13.2
13.2
15
54.5
−10.5
10.5
13.5
54.5
−10.5
10.5
13.5
55.8
−9.2
9.2
12
56.7
−8.3
8.3
11
57.3
−7.7
7.7
10
59.1
−5.9
5.9
 9
59.5
−5.5
5.5
 8
60.4
−4.6
4.6
 6
61.2
−3.8
3.8
 5
61.8
−3.2
3.2
 4
64.0
−1.0
1.0
 3
64.9
−0.1
0.1
 1
65.4
0.4
0.4
 2
70.2
5.2
5.2
 7}S   (65) = 2 + 7 = 9
+
}

15.1 THE ANALYSIS OF A SINGLE POPULATION
713
A statistical software package can be used to show that this statistic gives a p-value of 0.004,
so that as with the analysis provided by the sign test, this ﬁnding establishes that 65% is not a
plausible value for the median water pickup. Also, a 95% conﬁdence interval for the median
% water pickup constructed from the signed rank test is (56.80, 62.60), which is similar to the
conﬁdence interval obtained using the t-test in Section 8.1.3.
The signed rank test is particularly appropriate for analyzing paired data sets because the
assumption of distributional symmetry is particularly appropriate in this case. Recall from
Section 9.2.1 that if observations from one population are modeled as
xi = μA + γi + ϵ A
i
and if observations from the other population are modeled as
yi = μB + γi + ϵ B
i
then the differences zi are
zi = μA −μB + ϵ AB
i
where the error term is
ϵ AB
i
= ϵ A
i −ϵ B
i
If the error terms ϵ A
i and ϵ B
i are identically distributed, then the error term ϵ AB
i
necessarily
has a distribution that is symmetric about 0. Consequently, the differences zi are symmetrically
distributed about a mean value of μA −μB.
In other words, if the variations in the two populations under consideration can be assumed
to be identical (although not necessarily symmetric), so that the distribution functions FA(x)
and FB(x) are identical except for a location shift (that is, FA(x) = FB(x + δ) for some
δ = μB −μA), then the differences zi necessarily have a symmetric distribution and it is
appropriate to employ the signed rank test. This assumption that the population distributions
are identical except for a location shift is often reasonable and therefore it is often appropriate
to use the signed rank test to analyze paired data sets.
Example 55
Heart Rate Reductions
Figure 15.12 shows the differences in the heart rate reductions zi together with the ranks of their
absolute values. Notice that patient 20 has the smallest absolute value difference |z20| = 0.1
and that this is given a rank of 1. Patient 13 has a difference |z13| = 0.3, which is given a rank of
2. Patients 7, 8, and 15 then all have differences of either 0.5 or −0.5 and so they are each given
the average of the ranks 3, 4, and 5, which is a ranking of 4 each. Patient 40 has a rank of 6, and
patients 6 and 11 are then tied with |z6| = |z11| = 0.8, and they are each given a rank of 7.5.
The signed rank test that the differences zi have a mean μ0 = 0 is based on the sum of
the ranks of the positive differences zi, which is
S+(0) = 127.5
A computer package can be used to show that this statistic gives a very small p-value, and
that a 95% conﬁdence interval for the median of the distribution of the differences zi based
on the signed rank test is (−3.85, −1.30). This interval implies that the new drug provides a
reduction in a patient’s heart rate of somewhere between 1% and 4% more on average than the
standard drug, and the results of this analysis are seen to be similar to the analysis provided
by the sign test in Section 15.1.2 and the t-test in Section 9.2.2.
The signed rank test provides a useful middle ground between the sign test and a fully
parametric test such as the t-test under a normal distributional assumption. The sign test is a
verygeneralprocedurethatrequiresnodistributionalassumptions.Thesignedranktestutilizes

714
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
FIGURE 15.12
Calculation of the signed rank test
statistic for the heart rate
reductions example
Standard 
drug 
xi
New 
drug 
yi
Absolute 
differences 
|zi|
Patient
Differences 
zi
Rank 
ri(0)
1
28.5
34.8
−6.3
6.3
30.5
2
26.6
37.3
−10.7
10.7
40.0
3
28.6
31.3
−2.7
2.7
21.0
4
22.1
24.4
−2.3
2.3
18.5
5
32.4
39.5
−7.1
7.1
35.0
6
33.2
34.0
−0.8
0.8
7.5
7
32.9
33.4
−0.5
0.5
4.0
8
27.9
27.4
0.5
0.5
4.0
9
26.8
35.4
−8.6
8.6
38.0
10
30.7
35.7
−5.0
5.0
28.0
11
39.6
40.4
−0.8
0.8
7.5
12
34.9
41.6
−6.7
6.7
33.0
13
31.1
30.8
0.3
0.3
2.0
14
21.6
30.5
−8.9
8.9
39.0
15
40.2
40.7
−0.5
0.5
4.0
16
38.9
39.9
−1.0
1.0
9.0
17
31.6
30.2
1.4
1.4
12.5
18
36.0
34.5
1.5
1.5
14.0
19
25.4
31.2
−5.8
5.8
29.0
20
35.6
35.5
0.1
0.1
1.0
21
27.0
25.3
1.7
1.7
15.0
22
33.1
34.5
−1.4
1.4
12.5
23
28.7
30.9
−2.2
2.2
17.0
24
33.7
31.9
1.8
1.8
16.0
25
33.7
36.9
−3.2
3.2
25.0
26
34.3
27.8
6.5
6.5
32.0
27
32.6
35.7
−3.1
3.1
24.0
28
34.5
38.4
−3.9
3.9
27.0
29
32.9
36.7
−3.8
3.8
26.0
30
29.3
36.3
−7.0
7.0
34.0
31
35.2
38.1
−2.9
2.9
22.0
32
29.8
32.1
−2.3
2.3
18.5
33
26.1
29.1
−3.0
3.0
23.0
34
25.6
33.5
−7.9
7.9
37.0
35
27.6
28.7
−1.1
1.1
10.0
36
25.1
31.4
−6.3
6.3
30.5
37
23.7
22.4
1.3
1.3
11.0
38
36.3
43.7
−7.4
7.4
36.0
39
33.4
30.8
2.6
2.6
20.0
40
40.1
40.8
−0.7
0.7
6.0
(0) = 4 + 2 + 12.5 + 14 + 1 + 15 + 16 + 32 + 11 + 20 = 127.5
S+
the additional information provided by an assumption that the distribution is symmetric and
allows more precise inferences to be made about the mean of the distribution in this case.
This assumption of symmetry can often be justiﬁed by an experimenter and provides a middle
ground that is less restrictive than the exact speciﬁcation of a symmetric distribution such as
the normal distribution.
The choice of which inference method to adopt should be made on the basis of which set of
assumptions are reasonable, and histograms or other data plots may be useful to investigate the
form of the distribution. The experimenter should make use of any reasonable assumptions,
so that for example, the signed rank test should be used in preference to the sign test when the
assumption of symmetry can be justiﬁed. Remember that the central limit theorem implies

15.1 THE ANALYSIS OF A SINGLE POPULATION
715
that the t-test (or equivalently the z-test) can be used for large enough sample sizes regardless
of the actual distribution of the data observations, although the sign test and the signed rank
test in practice often provide as precise an inference as the t-test and may still be preferred.
Finally, it is worthwhile pointing out that an additional advantage of nonparametric test
procedures, besides the fact that they require minimal distributional assumptions, is that they
are generally less sensitive than parametric test procedures to “bad” data points. Such data
points may be outliers, in the sense that they are really from a different distribution than the
rest of the data set (due perhaps to some inadvertent change in the experimental conditions),
or they may simply be miscoded or mistyped values.
The sign test is particularly robust to these kinds of errors, because if an observation of
4.00 is incorrectly recorded as 6.00 say, then any inferences about F(x0) for x0 < 4.00 or
x0 > 6.00 are not affected because the statistic of interest, the number of data observations
xi on either side of x0, is unchanged. At worst, a bad data point causes this statistic to be
incorrect by an amount one.
The signed rank test is also quite robust to these kinds of errors. Parametric test procedures
are generally less robust because an incorrect data point usually affects the estimates of each
of the parameters. If a t-test is employed, for example, then an incorrect data point directly
inﬂuences both the estimated mean and the estimated variance.
15.1.4
Problems
15.1.1 Restaurant Service Times
(a) Construct the empirical cumulative distribution
function for the data set of restaurant service times
given in DS 6.1.4.
(b) Draw 95% conﬁdence bands around the empirical
cumulative distribution function.
(c) Is it plausible that the service times are normally
distributed with a mean of 70 seconds and a standard
deviation of 20 seconds?
(d) Is it plausible that the service times are exponentially
distributed with a mean of 70 seconds?
(e) Consider the null hypothesis that the median service
time is no longer than 65 seconds. What statistic is
used by the sign test procedure to test this null
hypothesis? What is the p-value?
(f) Test the null hypothesis in part (e) using the signed
rank test.
(g) Use the sign test and the signed rank test to obtain
95% conﬁdence intervals on the median service time.
15.1.2 Paving Slab Weights
(a) Construct the empirical cumulative distribution
function for the data set of paving slab weights given
in DS 6.1.7.
(b) Draw 95% conﬁdence bands around the empirical
cumulative distribution function.
(c) Is it plausible that the paving slab weights are
normally distributed with a mean of 1.1 kg and a
standard deviation of 0.05 kg? How about with a
mean of 1.0 kg and a standard deviation of 0.05 kg?
(d) Consider the null hypothesis that the median paving
slab weight is 1.1 kg. What statistic is used by the
sign test procedure to test this null hypothesis? What
is the p-value?
(e) Test the null hypothesis in part (d) using the signed
rank test and the t-test. Compare your answers.
(f) Use the sign test, the signed rank test, and the t-test
to obtain 95% conﬁdence intervals for the median
(or mean) paving slab weight. What assumptions are
required by these three test procedures? Do the
assumptions seem appropriate? How would you
summarize your results?
15.1.3 Spray Painting Procedure
Construct the empirical cumulative distribution function
for the data set of paint thicknesses given in DS 6.1.8.
Draw 95% conﬁdence bands around the empirical
cumulative distribution function. Analyze the median
(or mean) paint thickness using the sign test, the signed
rank test, and the t-test. Pay particular attention to
whether the median paint thickness is 0.2 mm. Which
analysis method do you prefer? Summarize your
conclusions.
15.1.4 Plastic Panel Bending Capabilities
Construct the empirical cumulative distribution function
for the data set of deformity angles given in DS 6.1.9.

716
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
Draw 95% conﬁdence bands around the empirical
cumulative distribution function. Analyze the median
(or mean) deformity angle using the sign test, the signed
rank test, and the t-test. Do you think that the
assumptions behind these test procedures are valid?
What evidence is there that the plastic can bend less than
9.5◦on average before deforming?
15.1.5 Suppose that the data set in DS 15.1.1 consists of values
that can be taken to be independent observations from a
particular distribution. Consider testing whether the
median of the distribution is equal to 18.0 against a
two-sided alternative.
(a) What is the value of the test statistic used by the sign
test?
(b) Write down an expression for the exact p-value
using the sign test.
(c) Use the normal approximation to calculate the
p-value using the sign test.
(d) What is the value of the test statistic used by the
signed rank test?
(e) Use the normal approximation to calculate the
p-value using the signed rank test.
15.1.6 Repeat Problem 15.1.5 using the data set in DS 15.1.2
and for the null hypothesis that the median of the
distribution is equal to 40 against a two-sided alternative
hypothesis.
15.1.7 Production Line Assembly Methods
Use the sign test and the signed rank test to analyze the
paired data set of assembly times given in DS 9.2.1.
Why might it be expected that the signed rank test is a
better test procedure than the sign test for this paired
data set? Do you ﬁnd any evidence of a difference
between the two assembly methods?
15.1.8 Red Blood Cell Adherence to Endothelial Cells
Use the sign test and the signed rank test to analyze the
paired data set of adherent red blood cells given in
DS 9.2.2. Do you ﬁnd any evidence of a difference
between the two stimulation conditions?
15.1.9 Calculus Teaching Methods
Use the sign test and the signed rank test to analyze the
paireddatasetofcalculusscoresgiveninDS9.2.4.Doyou
ﬁnd any evidence of a difference between the two teaching
methods? How much better is the new teaching method?
15.1.10 Radioactive Carbon Dating
Use the sign test and the signed rank test to analyze the
paired data set given in DS 9.2.5 concerning the
radioactive carbon dating methods. Do you ﬁnd any
evidence of a difference between the two dating
methods?
15.1.11 Golf Ball Design
Use the sign test and the signed rank test to analyze the
paired data set of golf shots given in DS 9.2.6. Do you ﬁnd
any evidence of a difference between the two ball types?
15.1.12 Carbon Footprints
Analyze the data in DS 6.7.15, which contain estimates
of the pounds of carbon dioxide released when making
several types of car.
15.1.13 Data Warehouse Design
Power consumption represents a large proportion of a data
center’s costs. Use nonparametric methods to analyze
the data in DS 6.7.16, which shows monthly electricty
costs as a percentage of the data center’s total costs.
15.1.14 Customer Churn
Customer churn is a term used for the attrition of a
company’s customers. DS 6.7.17 contains information
from an Internet service provider on the length of days
that its customers were signed up before switching to
another provider. Use the techniques described in this
section to analyze this data.
15.1.15 Mining Mill Operations
DS 6.7.18 contains daily data for the mill operations of a
mining company over a period of a month. Each day, the
company keeps track of the carbon concentration in the
waste material. Use the techniques described in this
section to analyze this data.
15.2
Comparing Two Populations
In this section the problem of comparing two distribution functions FA(x) and FB(x) based
upontheanalysisoftwounpairedindependentsamplesisdiscussed.Speciﬁcally,let x1, . . . , xn
be independent, identically distributed observations from the distribution function FA(x),
and let y1, . . . , ym be independent, identically distributed observations from the distribution
function FB(x). In addition, suppose that the two samples are obtained independently of each

15.2 COMPARING TWO POPULATIONS
717
other, as is the case with unpaired samples. Notice that the two sample sizes n and m need not
be equal, although many experiments are designed to have equal sample sizes.
Two test procedures are considered in this section, the Kolmogorov-Smirnov test proce-
dure and the rank sum test procedure. The Kolmogorov-Smirnov test addresses the general
question of whether the two distribution functions FA(x) and FB(x) can be considered to be
identical. The rank sum test procedure is based upon the assumption that the two distribution
functions are identical except for a location shift, so that FA(x) = FB(x +δ) for some value δ,
and it can be used to construct a conﬁdence interval for and perform hypothesis tests of the
location difference δ.
15.2.1
The Kolmogorov-Smirnov Test
The Kolmogorov-Smirnov test is a general test of the equivalence of two distribution functions
FA(x) and FB(x), and it is based upon the comparison of the two empirical cumulative
distribution functions ˆFA(x) and ˆFB(x). Recall from Section 15.1.1 that these are given by
ˆFA(x) = #xi ≤x
n
and
ˆFB(x) = #yi ≤x
m
AsdiscussedinSection15.1.1, ˆFA(x)and ˆFB(x)providethebestestimatesofthetrueunknown
distribution functions FA(x) and FB(x), respectively. The assessment of the plausibility that
the two distribution functions FA(x) and FB(x) are identical is based on the consideration
of how close together the two empirical cumulative distribution functions ˆFA(x) and ˆFB(x)
appear to be.
The Kolmogorov-Smirnov Test
If x1, . . . , xn are independent, identically distributed observations from a distribution
function FA(x) and y1, . . . , ym are independent, identically distributed observations
from a distribution function FB(x), then the Kolmogorov-Smirnov test of the
equivalence of two distribution functions FA(x) and FB(x) is based on the comparison of
the two empirical cumulative distribution functions ˆFA(x) and ˆFB(x). The test statistic
M = max
x
| ˆFA(x) −ˆFB(x)|
which is the maximum vertical distance between the two empirical cumulative
distribution functions, is used as a measurement of how close together the two
empirical cumulative distribution functions are. For sample sizes n and m larger than
20, a size α hypothesis test of
H0 : FA(x) = FB(x)
for all values of x is performed by comparing the value of the statistic M with the
critical point
dα

1
n + 1
m
where the values of dα are given in Section 15.1.1. Values of the statistic M greater
than the critical point cause the null hypothesis H0 to be rejected at the size α
conﬁdence level, and values of M smaller than the critical point result in the null
hypothesis being accepted.

718
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
Generally speaking, when using the Kolmogorov-Smirnov test, it is advisable to have
sample sizes n and m both larger than 20. Critical points for use with sample sizes smaller
than 20 can be found, although it must be remembered that with such small sample sizes
the test procedure has very little power. In other words, with sample sizes less than 20 the
null hypothesis H0 : FA(x) = FB(x) is unlikely to be rejected unless there is a substantial
difference between the two distributions functions FA(x) and FB(x). Also, as a practical
matter the maximum distance between ˆFA(x) and ˆFB(x) must occur at one of the data values
xi or yi.
It is interesting and instructive to understand the relationship between the problem of com-
paring two empirical cumulative distribution functions, as discussed here, and the problem of
comparing one empirical cumulative distribution function with a known distribution function
F0(x), as discussed in Section 15.1.1. The connection between the two problems is that they
become identical as one of the two sample sizes n or m in the two-sample problem increases
to inﬁnity.
In such a case, having an inﬁnite number of observations from a distribution is conceptually
equivalent to knowing the distribution exactly, so that the limiting value of the empirical
cumulative distribution function can essentially be taken to be a known distribution function
F0(x). Notice that the two-sample critical point dα
√1/n + 1/m reduces to dα/√n as the
sample size m tends to inﬁnity, which is the critical point used in Section 15.1.1 for hypothesis
testing and conﬁdence band construction.
Consequently, it can be seen that the discussion in Section 15.1.1 is simply a special case
of the two-sample problem discussed here, which is attained by letting one of the sample
sizes grow to inﬁnity. The two-sample test was ﬁrst discussed by Smirnov in 1939, although
it is usually referred to as the Kolmogorov-Smirnov test because of its similarity with the
one-sample problem discussed by Kolmogorov in 1933.
Example 77
Petroleum Processing
In petroleum processing changes in the conditions of crude oil and natural gas may cause
dissolved waxes to crystallize and to therefore affect the operation of processing equipment.
If the waxes can be made to crystallize in a structure that is less likely to form a deposit,
or if the waxes can be prevented from crystallizing altogether, then a problem that may be of
serious concern to the petroleum industry can be mitigated. It is therefore important to study
the crystallization properties of the dissolved waxes.
An experiment is performed whereby a solution of waxes is expanded rapidly through
a nozzle, as shown in Figure 15.13, so that the wax particles can be collected on a plate.
Scanning electron microscopy is then used to photograph the samples of wax particles so
that their diameters can be measured. A question of importance is how the pre-expansion
temperature of the solution affects the particle sizes.
FIGURE 15.13
The experimental apparatus for the
petroleum processing example
Solution 
of waxes
Nozzle
Expansion chamber
Collection plate

15.2 COMPARING TWO POPULATIONS
719
FIGURE 15.14
Data set of particle diameters for
the petroleum processing example
Pre-expansion temperature of 80	C (temperature A)
9.00
9.50
9.75
10.50
10.50
11.50
12.00
12.25
12.75
13.25
14.25
14.75
15.25
17.00
18.25
18.25
18.50
18.75
19.25
19.75
20.25
20.75
22.00
22.75
22.75
23.00
23.00
23.25
26.00
26.00
29.00
31.75
32.50
33.00
36.50
38.00
temperature of 170	C (temperature B)
4.00
4.17
4.75
4.83
5.17
5.33
5.50
5.67
5.67
5.67
6.17
6.17
6.67
6.67
7.33
7.60
7.67
7.67
7.75
8.17
8.50
8.50
8.67
8.67
8.67
9.17
9.17
9.33
9.44
9.67
10.00
10.00
10.25
10.67
10.83
12.16
Pre-expansion
FIGURE 15.15
Descriptive statistics and boxplots
for the petroleum processing
example
Data
Pre-expansion temperature 170° C
Pre-expansion temperature 80° C
40
30
20
10
0
Pre-expansion temperature 80° C 
Sample size = 36 
Sample mean = 19.88 
Sample standard deviation = 7.88 
Sample maximum = 38.00 
Sample upper quartile = 23.19 
Sample median = 19.00 
Sample lower quartile = 12.88 
Sample minimum = 9.00 
Pre-expansion temperature 170° C 
Sample size = 36 
Sample mean = 7.68 
Sample standard deviation = 2.09 
Sample maximum = 12.16 
Sample upper quartile = 9.29 
Sample median = 7.71 
Sample lower quartile = 5.67 
Sample minimum = 4.00 
In the ﬁrst experiment a pre-expansion temperature of 80◦C is used, and in a second
experiment a pre-expansion temperature of 170◦C is used. In either case 36 samples of particles
are obtained, and the means of the particle diameters within each sample are calculated. The
resulting data set is shown in Figure 15.14.
How does the pre-expansion temperature affect the particle sizes? This question can
be answered by comparing the distribution function of particle sizes for a pre-expansion
temperature of 80◦C, FA(x), with the distribution function of particle sizes for a pre-expansion
temperature of 170◦C, FB(x). Figure 15.15 shows boxplots and sample statistics for the two
data sets that indicate that the sample mean and the sample median of the particle sizes are
much smaller at the higher pre-expansion temperature. In addition, notice that the sample
standard deviation is also much smaller at the higher pre-expansion temperature.
These observations are conﬁrmed by the plot of the two empirical cumulative distribution
functions ˆFA(x) and ˆFB(x) in Figure 15.16. The empirical cumulative distribution function
ˆFA(x) lies to the right of the empirical cumulative distribution function ˆFB(x), which indicates
that the particle diameters tend to be larger at the lower pre-expansion temperature. In addition,
the empirical cumulative distribution function ˆFB(x) increases quite sharply in comparison
with the empirical cumulative distribution function ˆFA(x), which has a more gradual slope,
and this conﬁrms that there is more variability in the particle sizes at the lower pre-expansion
temperature than at the higher pre-expansion temperature.

720
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
FIGURE 15.16
The empirical cumulative
distribution functions for the
petroleum processing example
0
5
10
15
20
25
30
35
40
45
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Diameter
FˆB(x) (170° C)
FˆA(x) (80° C)
Formally, the value of the Kolmogorov-Smirnov statistic is
M = 30
36 = 0.83
This value is the maximum vertical distance between the two empirical cumulative distribution
functions ˆFA(x) and ˆFB(x), which occurs when 10.25 < x < 10.50, in which case
ˆFA(x) = 3
36
and
ˆFB(x) = 33
36
and also when 10.83 < x < 11.50, in which case
ˆFA(x) = 5
36
and
ˆFB(x) = 35
36
A size α = 0.01 critical point is
d0.01

1
n + 1
m = 1.63 ×

1
36 + 1
36 = 0.38
The value M = 0.83 is much larger than this critical point, which indicates that there is very
strong evidence that the distributions of the particle sizes are different at the two different
pre-expansion temperatures.

15.2 COMPARING TWO POPULATIONS
721
15.2.2
The Rank Sum Test
The rank sum test procedure is another test procedure for comparing two distribution functions
FA(x) and FB(x) based on the analysis of two unpaired samples. Again let x1, . . . , xn be
independent, identically distributed observations from the distribution function FA(x), and let
y1, . . . , ym be independent, identically distributed observations from the distribution function
FB(x). In addition, suppose that the two samples are obtained independently of each other, as
is the case with unpaired samples. As before, the two sample sizes m and n need not be equal,
although many experiments are designed to have equal sample sizes.
The rank sum test procedure is attributed to works of Wilcoxon in 1945 and Mann and
Whitney in 1947. It is referred to as either the Wilcoxon rank sum test (to distinguish it from
the Wilcoxon signed rank test) or the Mann-Whitney test. It is based upon the assumption
that the two distribution functions are identical except for a difference in location, so that
FA(x) = FB(x +δ) for some location difference δ, as illustrated in Figure 15.17. This is often
a reasonable assumption to make, and it is valid if the observations from the two populations
can be modeled as ﬁxed population effects plus identically distributed error terms. The location
difference δ can be interpreted as the difference between the means of the two populations
μB −μA, or alternatively as the difference between the two population medians.
Under the assumption that FA(x) = FB(x + δ) the null hypothesis
H0 : FA(x) = FB(x)
for all values of x is equivalent to testing whether the location difference δ is 0. The rank
sum test procedure is typically applied to test this null hypothesis that the two distribution
functions are identical. More generally, the rank sum test procedure can also be used to
construct a conﬁdence interval for the location difference δ.
The nonparametric nature of the analysis is evident from the fact that the actual shape of
the distribution functions does not need to be speciﬁed. Under the assumption that the data
observations are normally distributed, this problem can be analyzed using the two-sample
t-test discussed in Section 9.3. For large enough sample sizes n and m, the central limit
theorem also validates the test procedures discussed in Section 9.3. Nevertheless, in practice
FIGURE 15.17
The assumption of location shift
for the rank sum test procedure
x
FA(x) = FB(x + δ)
1
0
FA(x)
FB(x)
δ

722
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
the rank sum test procedure often provides as precise an inference as the two-sample t-test
without the distributional assumption.
The ﬁrst step in the implementation of the rank sum test is to combine the two data samples
x1, . . . , xn and y1, . . . , ym into one large sample and to rank the elements of this pooled sample
from 1 to m + n. Tied observations are treated in the usual manner by assigning the averages
of the corresponding rank values. The rank sum test is then based on the consideration of the
statistic SA, which is deﬁned to be the sum of the ranks within the combined sample of the
observations from population A. In other words, the statistic SA is calculated as the sum of
the ranks within the combined sample of the n observations x1, . . . , xn. The results obtained
from the rank sum test do not depend on which of the two populations is labeled population A.
In practice, instead of dealing directly with the statistic SA it is more convenient to consider
the statistic UA deﬁned by
UA = SA −n(n + 1)
2
Notice that the statistic UA can take any integer value between 0 and mn. For example, the
smallest value of UA occurs when each observation xi is less than each observation yi, so
that the ranks of the observations xi are 1, 2, . . . , n in which case SA = n(n + 1)/2. On
the other hand, the largest value of UA occurs when each observation xi is greater than each
observation yi, so that the ranks of the observations xi are m + 1, m + 2, . . . , m + n in which
case SA = mn + n(n + 1)/2.
If the two distribution functions FA(x) and FB(x) are identical, then the statistic UA should
be roughly equal to mn/2. However, if the statistic UA is much larger than mn/2, this suggests
that observations from population A tend to be larger than observations from population B.
Conversely, if the statistic UA is much smaller than mn/2, then this suggests that observations
from population A tend to be smaller than observations from population B.
Computer packages will calculate an exact two-sided p-value for the plausibility of the null
hypothesis H0 : FA(x) = FB(x) by comparing the observed value of the statistic UA with its
distribution under this null hypothesis. This discrete distribution can be found by considering
the value of the statistic UA for each of the different orderings of the n observations from
population A and the m observations from population B. There are

n + m
m

of these orderings, and under the null hypothesis each has an equal probability of
1

n + m
m

Some minor modiﬁcations to the p-value calculation may be made if there are ties in the data
observations.
However, for reasonably large sample sizes it is sufﬁcient to calculate an approximate
p-value by comparing the statistic
z =
UA −mn
2

mn(m+n+1)
12
with the standard normal distribution. If z > 0 a two-sided p-value is 2 × (1 −(z)), and if
z < 0 a two-sided p-value is 2 × (z). As in other cases, a continuity correction of 0.5 can
help increase the accuracy of these p-value calculations. The amount 0.5 should be added to
the numerator of z if it is negative and subtracted from the numerator of z if it is positive.

15.2 COMPARING TWO POPULATIONS
723
In certain situations it may make sense to calculate a one-sided p-value. Remember that
δ = μB −μA, and so if, for example, the experimenter is interested only in detecting whether
or not observations from population B tend to be smaller than observations from population
A, then a null hypothesis of H0 : δ ≥0 is appropriate with an alternative hypothesis of
HA : δ < 0. Positive values of the statistic z suggest that δ < 0, and a one-sided p-value can
be calculated as (1 −(z)).
The Rank Sum Test
If x1, . . . , xn are independent, identically distributed observations from a distribution
function FA(x) and y1, . . . , ym are independent, identically distributed observations
from a distribution function FB(x), then under the assumption FA(x) = FB(x + δ) for
some location difference δ, the rank sum test procedure can be used to test the null
hypothesis
H0 : FA(x) = FB(x)
for all values of x, which is equivalent to testing whether the location difference δ is 0.
More generally, the rank sum test procedure can be used to construct a conﬁdence
interval for the location difference δ, which can be interpreted as the difference
between the means of the two populations μB −μA, or alternatively as the difference
between the two population medians.
The rank sum test is performed by combining the two data samples x1, . . . , xn and
y1, . . . , ym into one large sample and ranking the elements of the pooled sample from
1 to m + n. The rank sum test is then based upon the consideration of the sum of the
ranks within the combined sample of the observations from one of the populations.
If the null hypothesis
H0 : FA(x) = FB(x + δ0)
is of interest for some ﬁxed value of δ0, then it sufﬁces to apply the rank sum test to the
modiﬁed data sets
x1, . . . , xn
and
y1 −δ0, . . . , ym −δ0
This is clear because if FA(x) = FB(x + δ0), then the two distribution functions are identical
except for a location difference of δ0. Therefore, if one of the data sets is shifted by δ0 in an
appropriate manner, the assessment of the plausibility of the null hypothesis
H0 : FA(x) = FB(x + δ0)
simpliﬁes to the assessment of the plausibility that the two resulting samples are generated by
identical distribution functions, which can be performed with the rank sum test.
Finally, under the assumption that FA(x) = FB(x + δ) for some location difference δ, the
rank sum test procedure can be used to construct a conﬁdence interval for δ. Speciﬁcally, a
conﬁdence interval for the location difference δ with a conﬁdence level of 1 −α consists of
all the values δ0 for which a two-sided size α test of the null hypothesis
H0 : FA(x) = FB(x + δ0)
is accepted by the rank sum test. In other words, the conﬁdence interval consists of all the
values δ0 for which the two-sided p-value of this null hypothesis is greater than α. Computer
packages can be used to construct this conﬁdence interval, and as with the sign test and the
signed rank test, only certain discrete conﬁdence levels are available.

724
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
Example 52
Kaolin Processing
Figure 15.18 shows how the n = 12 brightness measurements from calciner A and the m = 12
brightness measurements from calciner B are combined into one sample of 24 observations
and ranked from 1 to 24, with tied values being assigned the average of the corresponding
ranks. The sum of the ranks of the observations from calciner A is
SA = 132
so that
UA = SA −n(n + 1)
2
= 132 −12 × 13
2
= 54
This result is smaller than
mn
2
= 12 × 12
2
= 72
so the data suggest that the brightness measurements from calciner A are smaller than the
brightness measurements from calciner B. Notice that this ﬁnding is consistent with the
discussion in Section 9.3.4 where it is noted that the sample average 91.558 for calciner A is
smaller than the sample average 92.500 for calciner B.
A statistical software package can be used to show that UA = 54 gives a p-value of 0.3123
(or 0.3118 when a modiﬁcation is made because of the ties in the data observations), and a
conﬁdence interval for δ with a conﬁdence level of 95.4% is (−2.7, 1.1) which, as expected,
contains 0. Clearly, this analysis with the rank sum test procedure indicates that there is no
evidence of a difference between the two calciners, which is consistent with the analysis
performed in Section 9.3.4 using the two-sample t-test.
FIGURE 15.18
Calculation of the rank sum test
statistic for the kaolin processing
example
Calciner
A 
A
A
B
A
B
A
B
B
A
B
B
A
A
A
B
B
A
B
B
B
A
A
B
  1
  2
  3
  4
  5
  6
  7
  8
  9
10
11
12
13
14
15
17
17
17
19
20
21
22.5
22.5
24
87.4
88.4
89.0
89.2
90.5
90.7
90.8
91.5
91.7
91.9
92.0
92.6
92.8
93.0
93.1
93.2
93.2
93.2
93.3
93.8
94.0
94.3
94.3
94.8
SA = 1 + 2 + 3 + 5 + 7 + 10 + 13 + 14
+ 15 + 17 + 22.5 + 22.5 = 132
Brightness
measurement
Rank
}

15.2 COMPARING TWO POPULATIONS
725
15.2.3
Problems
15.2.1 Restaurant Service Times
Recall that DS 6.1.4 shows the service times of
customers at a fast-food restaurant who were served
between 2:00 and 3:00 on a Saturday afternoon, and that
DS 9.3.5 shows the service times of customers at the
fast-food restaurant who were served between 9:00 and
10:00 in the morning on the same day.
(a) Make a plot of the empirical cumulative distribution
functions of these two data sets.
(b) What does a visual comparison of the two empirical
cumulative distribution functions suggest about the
difference between the two service time distributions?
(c) Use the Kolmogorov-Smirnov test to assess the
evidence that the two distribution functions are
different.
15.2.2 Paving Slab Weights
Recall that DS 6.1.7 shows the weights of a sample of
paving slabs from manufacturer A and that DS 9.3.1
shows the weights of a sample of paving slabs from
manufacturer B.
(a) Make a plot of the empirical cumulative distribution
functions of these two data sets.
(b) What does a visual comparison of the two empirical
cumulative distribution functions suggest about the
difference between the distributions of the paving
slabs weights for the two manufacturers?
(c) Use the Kolmogorov-Smirnov test to assess the evi-
dence that the two distribution functions are different.
15.2.3 Heel-Strike Force on a Treadmill
DS 9.3.3 contains observations of heel-strike force for a
runner on a treadmill with and without a damped feature
activated. Use plots of the empirical cumulative
distribution functions and the Kolmogorov-Smirnov test
to investigate whether the damped feature is effective in
reducing the heel-strike force.
15.2.4 Use the rank sum test procedure to analyze the two
samples in DS 15.2.1.
(a) Combine the two samples and rank the observations.
(b) What is SA?
(c) What is UA?
(d) Is the value of UA consistent with the observations
from population A being larger or smaller than the
observations from population B?
(e) Use a computer package to ﬁnd a two-sided p-value
for the null hypothesis that the two distribution
functions are identical. Is the difference between the
two distribution functions suggested in part (d)
statistically signiﬁcant?
15.2.5 Repeat Problem 15.2.4 using the data set in DS 15.2.2.
15.2.6 Use the rank sum test to analyze the data set in
Figure 9.20 concerning Example 51 on acrophobia
treatments. Let population A be with the standard
treatment and population B be with the new treatment.
(a) Combine the two samples and rank the observations.
(b) What is SA?
(c) What is UA?
(d) Is the value of UA consistent with the new treatment
being better than the standard treatment?
(e) Use a computer package to ﬁnd a one-sided p-value
for the null hypothesis H0 : μA ≥μB versus the
alternative hypothesis HA : μA < μB.
(f) Are your conclusions from this analysis consistent
with the analysis presented in Section 9.3.4 using the
two-sample t-test?
15.2.7 Spray Painting Procedure
Recall that DS 6.1.8 contains a sample of paint thick-
nesses from production line A and that DS 9.3.2 contains
a sample of paint thicknesses from production line B.
(a) Make a plot of the empirical cumulative
distribution functions of these two data sets.
(b) What does a visual comparison of the
two empirical cumulative distribution functions sug-
gest about the difference between the distributions of
the paint thicknesses from the two production lines?
(c) Use the Kolmogorov-Smirnov test to assess the evi-
dence that the two distribution functions are different.
(d) Use the rank sum test to assess the evidence that the
two distribution functions are different. Obtain a
95% conﬁdence interval for the difference between
the median paint thicknesses for the two production
lines. On what assumption is the rank sum test
based? Do you think that it is a reasonable
assumption in this case?
15.2.8 Bleaching Agents
Recall that DS 9.3.4 contains the results of an experiment
to compare the bleaching effectiveness of two levels of hy-
drogen peroxide, a low level and a high level. Use the rank
sum test to assess whether there is evidence of a difference
between the low and high levels of hydrogen peroxide.

726
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
15.2.9 Clinical Trial
Use the rank sum test to analyze the clinical trial data in
DS 9.3.6.
15.2.10 Carbon Footprints
Use the methods discussed in this chapter to analyze the
data in DS 9.7.14, which contains estimates of the
pounds of carbon dioxide released when making several
types of car, together with information on whether or not
it is an SUV.
15.2.11 Green Management
A company introduces green management techniques to
make its manufacturing processes more environmentally
friendly and to cut waste. DS 9.7.15 shows weekly data
on the percentage of damaged inventory for 10 weeks
before and 10 weeks after the implementation of the new
techniques. Use the methods discussed in this chapter to
assess how the green management policies have affected
the amount of damaged inventory.
15.2.12 Data Warehouse Design
Power consumption represents a large proportion of a
data center’s costs. A redesign was undertaken by a
company in an attempt to reduce these costs by more
efﬁcient uses of its physical components such as its
routers, hubs, and switches. The data in DS 9.7.16
shows monthly electricty costs as a percentage of the
data center’s total costs. Use a nonparametric procedure
to investigate what the data indicate about the
effectiveness of the new design.
15.2.13 Natural Gas Consumption
DS 9.7.18 contains data on the total daily natural gas
consumption for a region for both the summer time and
the winter time. Use nonparametric statistical techniques
to investigate whether the natural gas consumption
patterns vary between summer and winter.
15.3
Comparing Three or More Populations
In Chapter 11 an analysis of variance approach is discussed for comparing three or more popu-
lations which is based upon the assumption that the error terms can be taken to be independent
observations from a normal distribution. In this section nonparametric inference procedures
for comparing three or more populations are discussed that are based only on the assumption
that the error terms are independent observations from some common distribution. In other
words, the assumption of normality is not required. The nonparametric approach can provide
a useful inference, although it is not as good as the analysis of variance approach when the
assumption of normality is reasonable.
In this section both one-way layouts and randomized block designs are considered. The
nonparametric test procedure for a one-way layout is the Kruskal-Wallis test procedure,
and the nonparametric test procedure for a randomized block design is the Friedman test
procedure. Both of these nonparametric test procedures are based on an analysis of appropriate
ranks of the data observations.
15.3.1
One-Way Layouts
Recall the one-way layout (one factor analysis of variance) discussed in Section 11.1 with
data observations
xi j = μi + ϵi j
1 ≤i ≤k, 1 ≤j ≤ni
where the total sample size is
nT = n1 + · · · + nk
The F-test obtained in the analysis of variance table for the null hypothesis
H0 : μ1 = · · · = μk
that the k factor level means are all equal is based upon the assumption that the error terms ϵi j
are independent observations from a N(0, σ 2) distribution. The Kruskal-Wallis test procedure
is a nonparametric procedure for testing this same null hypothesis, which only requires that

15.3 COMPARING THREE OR MORE POPULATIONS
727
the error terms are identically distributed. Therefore, the test is appropriate as long as the
error terms are independent observations from some common distribution, which need not be
a normal distribution.
The ﬁrst step in applying the nonparametric test procedure is to combine the k samples
into one big sample and to rank the observations from 1 to nT . Tied observations are handled
in the usual manner by assigning the average of the corresponding ranks. Notice that the sum
of the ranks is
1 + · · · + nT = nT (nT + 1)
2
and that the average of all the ranks is
nT + 1
2
Let the rank of the observation xi j be denoted by ri j so that the averages of the ranks within
each of the k populations are
¯r1·, . . . , ¯rk·
where
¯ri· = ri1 + · · · + rini
ni
The assessment of the plausibility of the null hypothesis that the k factor level means are
all equal is based upon the consideration of how close the rank averages ¯ri. are to the average
rank value (nT + 1)/2. The more widely spread they are around this average value, the less
plausible is the null hypothesis.
The Kruskal-Wallis Test
The Kruskal-Wallis test procedure is a nonparametric test procedure for assessing
the plausibility that the k factor level means in a one-way layout are all equal. Ranks
ri j of the data observations xi j are obtained by ranking the combined sample of nT
data observations. The average ranks within the k populations are calculated as
¯ri· = ri1 + · · · + rini
ni
1 ≤i ≤k
and the test procedure is based upon the statistic
H =
12
nT (nT + 1)
k

i=1
ni

¯ri· −nT + 1
2
2
=
12
nT (nT + 1)
k

i=1
ni ¯r2
i· −3(nT + 1)
A p-value is calculated as
p-value = P(X > H)
where the random variable X has a chi-square distribution with k −1 degrees of
freedom.
Computer packages can be used to perform this nonparametric test procedure, and the test
statistic H may be modiﬁed slightly if there are ties in the data observations xi j.
Example 62
Collapse of Blocked
Arteries
Recall that Figure 11.4 contains the values of the ﬂowrate at collapse for tubes with three
amounts of stenosis. Figure 15.19 shows how the ranks ri j are calculated for the combined

728
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
FIGURE 15.19
Calculation of the ranks for the
blocked arteries example
Stenosis
level
Data values
xi j
Rank
ri j
  8.3
  9.7
10.2
10.4
10.6
10.6
10.8
11.0
11.7
12.6
12.7
13.0
13.1
13.2
14.3
14.3
14.4
14.7
14.8
15.1
15.1
15.2
15.6
16.3
16.6
16.6
16.8
17.6
18.0
18.6
18.7
18.9
19.2
19.5
19.6
1
1
1
1
1
1
1
1
2
2
2
3
1
2
1
1
2
2
2
2
3
2
3
2
2
3
2
2
3
3
3
3
3
2
3
  1.0
  2.0
  3.0
  4.0
  5.5
  5.5
  7.0
  8.0
  9.0
10.0
11.0
12.0
13.0
14.0
15.5
15.5
17.0
18.0
19.0
20.5
20.5
22.0
23.0
24.0
25.5
25.5
27.0
28.0
29.0
30.0
31.0
32.0
33.0
34.0
35.0
¯r1· = 1 + 2 + 3 + 4 + 5.5
+ 7 + 8 + 13 + 15.5 + 15.5
11
= 80
11 = 7.273
¯r3· = 12 + 20.5 + 23 + 25.5 + 29+ 30 + 31 + 32 + 33 + 35
10
= 271
10 = 27.100
= 9 + 10 + 11 + 14 + 17 + 18 + 19 + 20.5 + 22 + 24 + 25.5 + 27 + 28 + 34
14
= 279
14 = 19.929
¯r2·
}
}
}
}
+ 5.5
sample of nT = 35 observations. The average ranks for the three stenosis levels are
¯r1· = 80
11 = 7.273
¯r2· = 279
14 = 19.929
¯r3· = 271
10 = 27.100

15.3 COMPARING THREE OR MORE POPULATIONS
729
Notice that
¯r1· < ¯r2· < ¯r3·
so the data suggest that the ﬂowrate at collapse increases from stenosis level 1 to level 2
and from stenosis level 2 to level 3, which is consistent with the comparisons of the sample
averages ¯x1· = 11.209, ¯x2· = 15.086, and ¯x3· = 17.330, with
¯x1· < ¯x2· < ¯x3·
The test statistic for the Kruskal-Wallis test procedure is
H =
12
nT (nT + 1)
k

i=1
ni ¯r2
i· −3(nT + 1)
=
12
35 × 36 (11 × 7.2732 + 14 × 19.9292 + 10 × 27.1002) −3 × 36 = 20.44
and the p-value is
p-value = P(X > 20.44) ≃0
where the random variable X has a chi-square distribution with k −1 = 3 −1 = 2 degrees
of freedom. Clearly there is substantial evidence that the amount of stenosis does affect the
ﬂowrate at collapse, which is the same conclusion as that reached in the analysis of this
example in Section 11.1.
15.3.2
Randomized Block Designs
Recall the randomized block design discussed in Section 11.2 with data observations
xi j = μi j + ϵi j
1 ≤i ≤k, 1 ≤j ≤b
where the cell means μi j are written as
μi j = μ + αi + β j
The F-test obtained in the analysis of variance table for the null hypothesis
H0 : α1 = · · · = αk = 0
that the k factor levels are indistinguishable is based on the assumption that the error terms
ϵi j are independent observations from a N(0, σ 2) distribution. The Friedman test procedure
is a nonparametric procedure for testing this same null hypothesis which requires only that
the error terms are identically distributed. In other words, it is appropriate as long as the error
terms are independent observations from some common distribution, which need not be a
normal distribution.
The Friedman test operates by ranking the data observations within each block. Thus, in
the ﬁrst block the data observations
x11, . . . , xk1
are ranked from 1 to k with ties being handled in the usual manner. In the second block the
data observations
x12, . . . , xk2
are also ranked from 1 to k, and similarly for each of the b blocks. The ranks ri j consequently
take values between 1 and k with an average value of
k + 1
2

730
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
The averages of the ranks within each of the k populations are
¯r1·, . . . , ¯rk·
where
¯ri· = ri1 + · · · + rib
b
and the assessment of the plausibility of the null hypothesis that the k factor level means are
indistinguishable is based upon the consideration of how close the rank averages ¯ri· are to the
average rank value (k + 1)/2. The more variability that they have about this average value,
the less plausible is the null hypothesis.
The Friedman Test
The Friedman test procedure is a nonparametric test procedure for assessing the
plausibility that the k factor levels are indistinguishable in a randomized block design.
Ranks ri j of the data observations xi j are obtained by ranking the data observations
from one to k within each of the b blocks. The average ranks for the k factor levels are
¯ri· = ri1 + · · · + rib
b
1 ≤i ≤k
and the test procedure is based upon the statistic
S =
12 b
k(k + 1)
k

i=1

¯ri· −k + 1
2
2
=
12 b
k(k + 1)
k

i=1
¯r2
i· −3b(k + 1)
A p-value is calculated as
p-value = P(X > S)
where the random variable X has a chi-square distribution with k −1 degrees of
freedom.
As with the Kruskal-Wallis test procedure, computer packages can be used to perform this
nonparametric test procedure, and the test statistic S may be modiﬁed slightly if there are ties
in the data observations xi j.
Example 65
Comparing Types
of Wheat
Recall that Figure 11.26 shows the crop yields from the experiment to compare k = 4 wheat
types with b = 5 ﬁelds as blocks. Figure 15.20 shows how the ranks are calculated for the
Friedman test. For example, in the ﬁrst ﬁeld
x31 < x11 < x21 < x41
so that r11 = 2, r21 = 3, r31 = 1, and r41 = 4. The average ranks for the four wheat types are
¯r1· = 14
5 = 2.8
¯r2· = 14
5 = 2.8
¯r3· = 12
5 = 2.4
¯r4. = 10
5 = 2.0

15.3 COMPARING THREE OR MORE POPULATIONS
731
FIGURE 15.20
Calculation of the Friedman test
ranks for the wheat comparisons
data set
Type 1
Type 2
Type 3
Type 4
x11 = 164.4
r11 = 2
x21 = 184.3
r21 = 3
x31 = 161.2
r31 = 1
x41 = 185.8
r41 = 4
x12 = 145.0
r12 = 4
x22 = 142.1
r22 = 3
x32 = 110.8
r32 = 1
x42 = 135.4
r42 = 2
x13 = 152.5
r13 = 1
x23 = 159.6
r23 = 3
x33 = 168.6
r33 = 4
x43 = 154.1
r43 = 2
x14 = 138.5
r14 = 4
x24 = 137.2
r24 = 3
x34 = 134.9
r34 = 2
x44 = 123.2
r44 = 1
x15 = 161.7
r15 = 3
x25 = 160.4
r25 = 2
x35 = 166.1
r35 = 4
x45 = 159.9
r45 = 1
Field 1
Field 2
Field 4
Field 5
Field 3
Wheat types
so that the statistic for the Friedman test procedure is
S =
12 b
k(k + 1)
k

i=1
¯r2
i· −3b(k + 1)
= 12 × 5
4 × 5 (2.82 + 2.82 + 2.42 + 2.02) −3 × 5 × 5 = 1.32
and the p-value is
p-value = P(X > 1.32) = 0.724
where the random variable X has a chi-square distribution with k −1 = 4 −1 = 3 degrees
of freedom. Clearly there is not sufﬁcient evidence to conclude that there is any difference
between the four wheat types with respect to their yields, which is the same result obtained
in the analysis of this example in Section 11.2 using an analysis of variance approach.
15.3.3
Problems
15.3.1 Use the Kruskal-Wallis test procedure to analyze the
data in DS 11.1.1.
(a) Find the ranks ri j.
(b) What are the average ranks ¯r1·, ¯r2·, and ¯r3·?
(c) What is the value of the test statistic H?
(d) Write down an expression for the p-value and use a
computer package to evaluate it.
15.3.2 Use the Kruskal-Wallis test procedure to analyze the
data in DS 11.1.2.
(a) Find the ranks ri j and the average ranks ¯r1·, ¯r2·, ¯r3·,
and ¯r4·.
(b) What is the value of the test statistic H?
(c) Write down an expression for the p-value and use a
computer package to evaluate it.
15.3.3 Infrared Radiation Readings
The data set in DS 11.1.3 concerns the infrared radiation
readings from an energy source measured by a particular
meter with three different background radiation
levels.
(a) Use the Kruskal-Wallis test procedure to investigate
whether the radiation readings are affected by the
background radiation level.
(b) Repeat the analysis using an analysis of variance
table.
(c) What assumptions are required for the
Kruskal-Wallis test procedure? What assumptions
are required for the F-test in the analysis of variance
table? Which analysis method do you prefer?

732
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
15.3.4 Keyboard Layout Designs
DS 11.1.4 contains the times taken to perform a task
using three different keyboard layouts for the numerical
keys. Use the Kruskal-Wallis test procedure to
investigate whether the different layouts affect the time
taken to perform a task.
15.3.5 Computer Assembly Methods
DS 11.1.6 contains the assembly times of computers for
three different assembly methods. Use the
Kruskal-Wallis test procedure to investigate whether
there is any evidence that one assembly method is any
quicker than the other methods.
15.3.6 Use the Friedman test procedure to analyze the data in
DS 11.2.1.
(a) Find the ranks ri j.
(b) What are the average ranks ¯r1·, ¯r2·, and ¯r3·?
(c) What is the value of the test statistic S?
(d) Write down an expression for the p-value and use a
computer package to evaluate it.
15.3.7 Use the Friedman test procedure to analyze the data in
DS 11.2.2.
(a) Find the ranks ri j and the average ranks ¯r1·, ¯r2·, ¯r3·,
and ¯r4·.
(b) What is the value of the test statistic S?
(c) Write down an expression for the p-value and use a
computer package to evaluate it.
15.3.8 Calciner Comparisons
The data set in DS 11.2.3 concerns the brightness
measurements for b = 7 batches of kaolin processed
through k = 3 calciners.
(a) Use the Friedman test procedure to investigate
whether the calciners are operating with different
efﬁciencies.
(b) Repeat the analysis using an analysis of variance
table.
(c) What assumptions are required for the Friedman
test procedure? What assumptions are required for
the F-test in the analysis of variance table? Which
analysis method do you prefer?
15.3.9 Radar Detection of Airborne Objects
DS 11.2.4 contains distances at detection for three radar
systems. Use the Friedman test procedure to investigate
whether there is evidence of any difference between the
radar systems.
15.3.10 Production Line Assembly Methods
The data set in DS 11.2.6 concerns an experiment
to compare three different assembly methods for an
electric motor. Use the Friedman test procedure to
investigate whether there is evidence of any difference
between the three assembly methods.
15.3.11 Realtor Commissions
DS 11.2.7 contains the commissions obtained by ﬁve
agents in a Realtor ofﬁce. Use the Friedman test
procedure to investigate whether there is evidence of any
real difference in the performances of the agents.
15.3.12 Cleanliness Scores for Detergent Comparisons
The data set in DS 11.2.8 concerns an experiment to
compare four different formulations of a detergent. Use
the Friedman test procedure to investigate whether there
is evidence of any difference between the detergent
formulations.
15.3.13 Durations of Investigatory Surgical Procedures
Use the appropriate nonparametric methodology from
this section to analyze the data in DS 11.1.8.
15.3.14 E. Coli Colonies in Riverwater
Use the appropriate nonparametric methodology from
this section to analyze the data in DS 11.1.9.
15.3.15 Groundwater Pollution Levels
Use the appropriate nonparametric methodology from
this section to analyze the data in DS 11.2.9.
15.3.16 Volatile Organic Carbon Emissions
Volatile organic carbon emissions are measured at ﬁve
locations of an industrial facility on each of 10 days, and
the data is given in DS 11.5.11. Use a nonparametric
procedure to investigate whether there is there any
evidence of a difference in the emissions rates at the ﬁve
locations.
15.4
Case Study: Internet Marketing
In Figure 9.36, there are 14 positive differences and only 1 negative difference. The sign test
can be used to test the null hypothesis that the two search engines are equivalent, and the
p-value is given by
p-value = 2 × P(B(15, 0.5) ≤1) = 0.00098

15.5 SUPPLEMENTARY PROBLEMS
733
Such a small p-value indicates that the null hypothesis is not plausible and conﬁrms the conclu-
sion drawn in Chapter 9 that there is evidence of a difference between the two search engines.
The Friedman test can be applied to the data in Figure 11.43. With the three banner designs
as the treatments to be compared, and with the days as the blocks, the sum of the ranks for
design A is 11, the sum of the ranks for design B is 15, and the sum of the ranks for design C
is 16. The resulting p-value is 0.368, which indicates that there is no evidence of a difference
between the three banner designs, as was found in Chapter 11.
15.5
Supplementary Problems
15.5.1 Osteoporosis Patient Heights
(a) Construct the empirical cumulative distribution
function for the data set of adult male heights given
in DS 6.7.4.
(b) Draw 95% conﬁdence bands around the empirical
cumulative distribution function.
(c) Is it plausible that the heights are normally
distributed with a mean of 70 inches and a standard
deviation of 2 inches?
(d) Is it plausible that the heights are normally
distributed with a mean of 71 inches and a standard
deviation of 1 inch?
(e) Consider the null hypothesis that the median height
is 70 inches. What statistic is used by the sign test
procedure to test this null hypothesis? What is the
p-value?
(f) Test the null hypothesis in part (e) using the signed
rank test.
(g) Use the sign test, the signed rank test, and the t-test
to obtain 95% conﬁdence intervals on the median (or
mean) service time. What assumptions are required
by these three test procedures? Do the assumptions
seem appropriate? How would you summarize your
results?
15.5.2 Bamboo Cultivation
Construct the empirical cumulative distribution function
for the data set of bamboo shoot heights given in
DS 6.7.5. Draw 95% conﬁdence bands around the
empirical cumulative distribution function. Analyze the
median (or mean) shoot height using the sign test, the
signed rank test, and the t-test. Is there any evidence that
the median (or mean) shoot height is less than 35 cm?
Which analysis method do you prefer? Summarize your
conclusions.
15.5.3 Tire Tread Wear
Use the sign test and the signed rank test to analyze the
paired data set of tire wear given in DS 9.2.3. Do you
ﬁnd any evidence of a difference between the two types
of tires? Do you think that the analysis with the signed
rank test is better than the analysis with the sign test?
Compare your results with an analysis using the t-test.
What can you say about the difference in average wear
for the two types of tires?
15.5.4 Video Display Designs
Use the sign test and the signed rank test to analyze
the paired data set given in DS 9.7.1 concerning the
assimilation of information from video monitors. Do you
ﬁnd any evidence of a difference between the two color
types? Compare your results with an analysis based
upon the t-test. Summarize your conclusions.
15.5.5 Consumer Complaints Division Reorganization
Recall that DS 9.7.4 contains data observations of
waiting times for a consumer to speak to a company
representative on a telephone complaints line both
before and after a reorganization.
(a) Make a plot of the empirical cumulative distribution
functions of these two data sets.
(b) What does a visual comparison of the two empirical
cumulative distribution functions suggest about the
difference between the distributions of the waiting
times?
(c) Use the Kolmogorov-Smirnov test to assess
the evidence that the two distribution functions are
different. Does the reorganization appear to have been
successful in affecting the times taken to answer calls?
15.5.6 Bamboo Cultivation
A researcher compares the bamboo shoot heights
in DS 6.7.5 obtained under growing conditions A with
the bamboo shoot heights in DS 9.7.3 obtained under
growing conditions B. Recall that the growing conditions
B allowed 10% more sunlight than growing conditions A.
(a) Make a plot of the empirical cumulative distribution
functions of these two data sets.
(b) What does a visual comparison of the two empirical
cumulative distribution functions suggest about

734
CHAPTER 15
NONPARAMETRIC STATISTICAL ANALYSIS
the difference between the distributions of the
bamboo shoot heights under the two growing
conditions?
(c) Use the Kolmogorov-Smirnov test to assess the
evidence that the two distribution functions are
different.
(d) Use the rank sum test to assess the evidence that the
two distribution functions are different. Obtain a
95% conﬁdence interval for the difference between
the median bamboo shoot heights for the two
growing conditions. On what assumption is the rank
sum test based? Do you think that it is a reasonable
assumption in this case?
(e) Compare the results of the rank sum test with an
analysis using a two-sample t-test. Which test
procedure do you prefer in this case?
15.5.7 Use the rank sum test to analyze the data set in
Figure 9.24 concerning Example 53 on kudzu pulping.
Let population A be without the addition of
anthraquinone and population B be with the addition
of anthraquinone.
(a) Combine the two samples and rank the observations.
(b) What is SA?
(c) What is UA?
(d) Is the value of UA consistant with the addition
of anthraquinone increasing or decreasing the
yield?
(e) Use a computer package to ﬁnd a one-sided
p-value for the null hypothesis H0 : μA ≥μB versus
the alternative hypothesis HA : μA < μB.
(f) Are your conclusions from this analysis consistent
with the analysis presented in Section 9.3.4 using the
two-sample t-test?
15.5.8 Biaxial Nanowire Tests
DS 11.5.1 contains Young’s modulus measurements
for four different types of nanowires. Use the
Kruskal-Wallis test procedure to investigate whether
there is any evidence of a difference in the types of
nanowires.
(a) Find the ranks ri j.
(b) What are the average ranks ¯r1·, ¯r2·, ¯r3·, and ¯r4·?
(c) What is the value of the test statistic H?
(d) Write down an expression for the p-value and use a
computer package to evaluate it.
15.5.9 Car Gas Efﬁciencies
The data set in DS 11.5.2 concerns the gas mileages of
four cars.
(a) Use the Kruskal-Wallis test procedure to investigate
whether any of the cars are getting better gas
mileages than the other cars.
(b) Repeat this analysis using an analysis of variance
table.
(c) What assumptions are required for the
Kruskal-Wallis test procedure? What assumptions
are required for the F-test in the analysis of variance
table? Which analysis method do you prefer?
15.5.10 Temperature Effect on Cement Curing
Use the Friedman test procedure to analyze the data set
in DS 11.5.3 concerning cement strengths.
(a) Find the ranks ri j.
(b) What are the average ranks ¯r1·, ¯r2·, ¯r3·, ¯r4·, and ¯r5·?
(c) What is the value of the test statistic S?
(d) Write down an expression for the p-value and use a
computer package to evaluate it.
15.5.11 Fertilizer Comparisons
DS 11.5.4 contains the results of an experiment to
compare ﬁve fertilizers. Use the Friedman test procedure
to investigate whether there is evidence of any difference
between the fertilizers.
15.5.12 Red Blood Cell Adhesion to Endothelial Cells
The data set in DS 11.5.5 concerns the reports of k = 4
clinics for b = 12 samples of blood.
(a) Use the Friedman test procedure to investigate
whether the clinics appear to be reporting similar
results.
(b) Repeat the analysis using an analysis of variance
table.
(c) What assumptions are required for the Friedman test
procedure? What assumptions are required for the
F-test in the analysis of variance table? Which
analysis method do you prefer?
15.5.13 Soil Compressibility Tests
Recall the data set of soil compressibility measurements
given in DS 6.7.6. Construct the empirical cumulative
distribution function for this data set. Use the sign test
and the signed rank test to investigate whether the
engineers can conclude that the average soil
compressibility is no larger than 25.5.
15.5.14 Ocular Motor Measurements
DS 9.7.5 contains the data from an experiment in which
a group of 10 subjects had their ocular motor
measurements recorded after they had been reading a
book for an hour and also after they had been reading a

15.5 SUPPLEMENTARY PROBLEMS
735
computer screen for an hour. Use the sign test and the
signed rank test to analyze the data set.
15.5.15 Engine Oil Viscosity
Oil viscosity values obtained from two engines are given
in DS 9.7.6. Use the rank sum test to assess whether
there is any evidence that the engines have different
effects on the oil viscosity.
15.5.16 Insertion Gains of Hearing Aids
Data collected on the insertion gain of a hearing aid for a
constant noise stimulus placed at the horizontal level of
the ear of a subject, placed above the horizontal level,
and placed below the horizontal level are shown in
DS 11.5.6. Use the Kruskal-Wallis test to analyze this
data set.
15.5.17 Air Resistance Drag for Road Vehicles
Data from wind tunnel tests performed on models of
four different vehicle designs are shown in DS 11.5.7.
What conclusions can you draw from this data set about
the drags of the four different designs using the
Kruskal-Wallis test?
15.5.18 Leather Shrinkage Measurements
The shrinkage measurements of leather for four different
preparation methods are given in DS 11.5.8. What
conclusions can you draw from this data set about the
differences between the four different preparation
methods using the Friedman test?
For problems 15.4.19–15.4.37 use the appropriate nonparametric
methodologies from this chapter to analyze the data sets.
15.5.19 Glass Fiber Reinforced Polymer Tensile Strengths
Data set in DS 6.7.7.
15.5.20 Infant Blood Levels of Hydrogen Peroxide
Data set in DS 6.7.8.
15.5.21 Paper Mill Operation of a Lime Kiln
Data set in DS 6.7.9.
15.5.22 River Salinity Levels
Data set in DS 6.7.10.
15.5.23 Dew Point Readings from Coastal Buoys
Data set in DS 6.7.11.
15.5.24 Brain pH Levels
Data set in DS 6.7.12.
15.5.25 Silicon Dioxide Percentages in Ocean Floor
Volcanic Glass
Data set in DS 6.7.13.
15.5.26 Network Server Response Times
Data set in DS 6.7.14.
15.5.27 Flowrates in Urban Sewer Systems
Data set in DS 8.6.1.
15.5.28 Polymer Compound Densities
Data set in DS 8.6.2.
15.5.29 Reinforced Cement Strengths
Data set in DS 9.7.7.
15.5.30 Comparisons of Experimental Drug Therapies
Data set in DS 9.7.8.
15.5.31 Rubber Seal Curing Methods
Data set in DS 9.7.9.
15.5.32 Light and Dark Regimens for Plant Growth
Data set in DS 9.7.10.
15.5.33 Joystick Design for Spinal Cord Injury Patients
Data set in DS 9.7.11.
15.5.34 Ambient Air Carbon Monoxide Pollution Levels
Data set in DS 9.7.12.
15.5.35 Sphygmomanometer and Finger Monitor Systolic
Blood Pressure Measurements
Data set in DS 9.7.13.
15.5.36 Metal Alloy Hardness Tests
Data set in DS 11.5.9.
15.5.37 Aquatic Radon Levels
Data set in DS 11.5.10.
15.5.38 Customer Churn
Customer churn is a term used for the attrition of a com-
pany’s customers. DS 9.7.17 contains information from
an Internet service provider on the length of days that
its customers were signed up before switching to another
provider, and whether or not they were a returning
customer (that is, whether or not they had previously had
Internet service from the company). Use the techniques
described in this chapter to analyze these data.
15.5.39 Mercury Levels in Coal
DS 6.7.19 shows the mercury levels of coal samples that
are taken periodically as the coal is mined further and
further into the seam. Use the nonparametric
methodologies described in this chapter to analyze
this data set.

C H A P T E R S I X T E E N
Quality Control Methods
16.1
Introduction
Effective quality control methods have become increasingly important as industries strive to
design and produce more reliable products more efﬁciently. Attention has been focused on the
“quality” of a product or service, which is considered to be a general term denoting how well
it meets the particular demands imposed upon it. Many quality control methods incorporate
techniques involving probability and statistics, and they are discussed in this chapter.
The origins of quality control can be traced back to the implementation of control charts by
Walter Shewhart in the 1920s. Interest in the area increased steadily from that beginning and
received considerable attention in Japan due to the pioneering work of W. Edwards Deming.
More recently the original quality control ideas together with related management principles
and guidelines have formed a subject that is often referred to as total quality management.
Various aspects of the quality movement are related to probability and statistics. Foremost
among these is the concentration on the use of proper experimental techniques, such as
the statistical methodologies and experimental designs discussed in Chapters 8, 11, and 14.
Another important quality control tool is statistical process control (SPC), which is discussed
in Section 16.2. Statistical process control utilizes control charts to provide a continuous
monitoring of a process. Control charts for the mean and variability of a variable of interest
are considered in Section 16.3, and control charts for the number of defective items or the
number of defects in an item are considered in Section 16.4. Finally, acceptance sampling
procedures, which can be used to make decisions about the acceptability of batches of items,
are discussed in Section 16.5.
16.2
Statistical Process Control
Consider a manufacturing organization that is involved in the production of a large number of
a certain kind of product, such as a metal part, a computer chip, or a chemical solution. These
products are manufactured using a process that typically involves the input of raw materials,
a series of procedures, and possibly the involvement of one or more operators. Statistical
process control concerns the continuous assessment of the various stages of such a process to
ascertain the “quality” of the product as it passes through the process. A key component of
this assessment is the use of control charts.
16.2.1
Control Charts
A control chart is a simple quality control tool in which certain measurements of products at
a particular point in a manufacturing process are plotted against time. This simple graphical
method allows a supervisor to detect when something unusual happens to the process.
736

16.2 STATISTICAL PROCESS CONTROL
737
The essential aspect of this tool is that measurements are taken successively over time,
usually on a random sample of items, so that the supervisor has “real-time” information on
the process. With this continuous assessment, any problems can be ﬁxed as they occur. This
procedure is in contrast to a less desirable scheme in which products are examined only at the
end of the process, when it is too late to make any changes to the process, and there may be a
large amount of wasted products or products of substandard quality.
Example 23
Piston Head
Construction
Consider the manufacture of a piston head that is designed to have a radius of 30.00 mm. A
control chart can be used to monitor the actual values of the radius of the manufactured piston
heads and to alert a supervisor if any changes in the process occur.
Suppose that every hour a random sample of n piston heads is taken and the radius of each
one is measured. A simple control chart can be constructed by plotting the sample average
of the radius measurements against time, as shown in Figure 16.1. In every manufacturing
process there is some variability and consequently it is observed that there is some variation
in the actual sizes of the manufactured pistons. Nevertheless, the control chart in Figure 16.1
exhibits a fairly random scatter of observations about the desired value of 30.00 mm, and so
the process can be considered to be in control.
Generally speaking, a process can be considered to be in control if it does not exhibit
any unusual changes. This does not mean that the process is perfect, or even good. In this
example, a reduction in the variability of the radius measurements of the piston heads would
be an improvement. However, if a process is observed to be in control, then this indicates that
the process is performing fairly uniformly over time.
The control chart provides an easily interpretable graphical indication of when any changes
in the process occur so that the process moves out of control. For example, the control chart
in Figure 16.2 indicates that there has suddenly been an increase in the average radius of the
piston heads. With the continuous monitoring provided by the control chart the supervisor can
immediately investigate the reasons behind the radius increase and can take the appropriate
corrective measures.
The control chart in Figure 16.3 also indicates that the process has moved out of control,
in this case due to a sudden increase in the variability of the radius values. The control chart
is obviously useful in detecting such a change so that corrective action can be taken before a
large number of substandard piston heads have been produced.
30.00 mm
Average
piston
head
radius
Time
FIGURE 16.1
A control chart exhibiting random scatter
30.00 mm
Average
piston
head
radius
Time
Increase in
mean value
In control
Out of control
FIGURE 16.2
A control chart exhibiting a sudden increase in the mean value

738
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.3
A control chart exhibiting a sudden
increase in the variability
30.00 mm
Average
piston
head
radius
Time
Increase in
variability
In control
Out of control
FIGURE 16.4
Control limits on a control chart
Variable of interest
Upper control limit (UCL)
Center line
Out of control
Time
Out of
control
In
control
Out of
control
Lower control limit (LCL)
In summary, it is useful to interpret a control chart as providing an indication of whether
the variability observed in the measurement of interest is consistent with the usual long-term
variability in the process under consideration (process in control), or whether it is alerting the
supervisor to some real change in the performance of the process (process out of control). Such
a change may be due to such factors as a batch of raw material that is ﬂawed in some manner, a
temperature change at some part of the process, an instrument that needs some adjustment, or
an operator error, and these factors can be corrected once they have been identiﬁed. In practice,
experience with a process often implies that supervisors become very adept at identifying the
causes behind various kinds of changes observed on the control charts.
16.2.2
Control Limits
In order to help judge whether a point on a control chart is indicative of the process having
moved out of control, a control chart is drawn with a center line and two control limits. These
control limits are the upper control limit (UCL) and the lower control limit (LCL) as shown
in Figure 16.4.
The process is considered to be in control as long as all the points lie within the control
limits. However, if a point lies outside the control limits, then this is taken to be evidence that
the process has moved out of control.
It is useful to realize that this procedure is essentially performing a hypothesis test of
whether the process is in control. It is useful to think of the null hypothesis as being
H0 : process in control

16.2 STATISTICAL PROCESS CONTROL
739
with the alternative hypothesis
HA : process out of control
When new observations on the process are taken, the null hypothesis is accepted as long as
the point plotted on the control chart falls within the control limits. However, if the point lies
outside the control limits, then the null hypothesis is rejected and there is evidence that the
process is out of control.
Of course, it is clear that the control limits are consequently associated with a certain
probability for a type I error, which corresponds to the size α of the hypothesis testing problem.
This is the probability of observing a point outside the control limits when the process is
actually still in control. The control limits are chosen so that this probability is very small.
Typically, “3-sigma” control limits are used, which are chosen to be three standard devi-
ations σ above and below the center line, as illustrated in the following example. A random
variable with a normal distribution has a probability of 0.9974 of taking a value within three
standard deviations of its mean value, so that if the control chart measurements are normally
distributed, 3-sigma control limits have a probability of a type I error of
α = 1 −0.9974 = 0.0026
Example 23
Piston Head
Construction
Suppose that experience with the process of manufacturing piston heads leads the supervisors
to conclude that the in-control process produces piston heads with radius values that are
normally distributed with a mean of μ0 = 30.00 mm and a standard deviation of σ = 0.05
mm. How should a control chart be constructed?
Suppose that the observations x1, . . . , xn represent the radius values of the random sample
of n piston heads chosen at a particular time. The point plotted on the control chart is the
observed value of the sample average
¯x = x1 + · · · + xn
n
When the process is in control this sample average is an observation from a distribution with
a mean value of μ0 = 30.00 mm and a standard deviation of
σ ¯X = σ
√n = 0.05
√n
A 3-sigma control chart therefore has a center line at the “control value” μ0 = 30.00 mm
together with control limits
UCL = μ0 + 3 σ
√n
and
LCL = μ0 −3 σ
√n
If the sample size taken every hour is n = 5, then the control limits are
UCL = 30.00 + 3 × 0.05
√
5
= 30.067
and
LCL = 30.00 −3 × 0.05
√
5
= 29.933
as shown in Figure 16.5.
The probability of a type I error for this control chart, that is, the probability that a point
on the control chart lies outside the control limits when the process is really still in control, is
1 −P(29.933 ≤¯X ≤30.067)
where the random variable ¯X is normally distributed with a mean of 30.00 and a standard
deviation of 0.05/
√
5. This probability can be evaluated as
1 −P(−3 ≤Z ≤3)

740
CHAPTER 16
QUALITY CONTROL METHODS
30.000 mm
Average
piston
head
radius
Time
30.067 mm
29.933 mm
Upper control limit (UCL)
Lower control limit (LCL)
Center
line
FIGURE 16.5
Control limits for the piston head construction example
Time
Upper control limit (UCL)
Lower control limit (LCL)
Center
line
Variable of interest
FIGURE 16.6
A series of consecutive points above the center line suggests that there has been a
change in the process
where the random variable Z has a standard normal distribution, which is
1 −0.9974 = 0.0026
as mentioned previously.
It should be mentioned that even if a series of points all lie within the control limits there
may still be reason to believe that the process has moved out of control. Remember that if
the process is in control, then the points plotted on the control chart should exhibit random
scatter about the center line. Any patterns observed in the control chart may be indications
of an out-of-control process. For example, the last set of points on the control chart shown in
Figure 16.6 all lie within the control limits but they are all above the center line. This suggests
that the process may have moved out of control.
COMPUTER NOTE
A series of rules have been developed to help identify patterns in control charts that are
symptomatic of an out-of-control process even though no individual point lies outside the
control limits. These rules are often called the Western Electric rules, which is where they
were ﬁrst suggested. Most computer packages will implement these rules for you upon request.
16.2.3
Properties of Control Charts
In addition to the probability of a type I error α, it is also useful to consider the probability
that the control chart indicates that the process is out of control when it really is out of control.
Within the hypothesis testing framework this power is deﬁned to be
power = P(reject H0 when H0 is false) = 1 −P(Type II error) = 1 −β
The actual value of the power depends upon how far the process has moved out of control.
Another point of interest relates to how long a control chart needs to be run before an
out-of-control process is detected. The expected number of points that need to be plotted on a
control chart before one of them lies outside the control limits and the process is determined
to be out of control is known as the average run length (ARL).
If the process has moved out of control so that each point plotted has a probability of 1−β
of lying outside the control limits independent of the other points on the control chart, then

16.2 STATISTICAL PROCESS CONTROL
741
the number of points that must be plotted before one of them lies outside the control limits has
a geometric distribution with success probability 1 −β (see Section 3.2). The expected value
of a random variable with a geometric distribution is the reciprocal of the success probability,
so that in this case the average run length is
ARL =
1
1 −β
Example 23
Piston Head
Construction
Suppose that the piston head manufacturing process has moved out of control due to a slight
adjustment in some part of the machinery so that the piston heads have radius values that are
now normally distributed with a mean value μ = 30.06 mm, instead of the desired control
value 30.00 mm, and with the same standard deviation σ = 0.05 mm as before. How good is
the control chart at detecting this change?
The plotted points on the control chart are observations of the random variable ¯X, which
is now normally distributed with a mean μ = 30.06 mm and a standard deviation 0.05/
√
5 =
0.0224 mm. The probability that a point lies within the control limits is therefore
P(29.933 ≤¯X ≤30.067)
which can be written as
P
29.933 −30.06
0.0224
≤Z ≤30.067 −30.06
0.0224

where the random variable Z has a standard normal distribution. This is
P(−5.680 ≤Z ≤0.313) = (0.313) −(−5.680) = 0.622 −0 = 0.622
The probability that a point lies outside the control limits is therefore
1 −β = 1 −0.622 = 0.378
In other words, once the process has moved out of control in this manner, there is about a
40% chance that each point plotted on the control chart will alert the supervisor to the problem.
The average run length in this case is
ARL =
1
1 −β =
1
0.378 = 2.65
so the problem should be detected within 2 or 3 hours.
This section has provided a general introduction to the use of control charts and the moti-
vation behind their use. Speciﬁc types of control charts for speciﬁc problems are considered
in more detail in the subsequent sections. In addition to the consideration of the mean value
of a measurement of interest, such as the mean piston head radius in the previous example,
control charts can also be used to monitor changes in the variability of a measurement. In
general, these control charts are referred to as ¯X-charts and R-charts respectively, and they
are discussed in more detail in the next section. Control charts for the proportion of defective
items in a batch, p-charts, and for the number of defects per item, c-charts, are discussed in
Section 16.4.

742
CHAPTER 16
QUALITY CONTROL METHODS
16.2.4
Problems
16.2.1 Suppose that when a process is in control a variable can
be taken to be normally distributed with a mean of
μ0 = 10.0 and a standard deviation of σ = 0.2, and that a
control chart is to be implemented by plotting the average
of n = 4 observations at each time point.
(a) What are the center line and control limits of a
3-sigma control chart that you would construct to
monitor this process?
(b) If a sample average ¯x = 9.5 is obtained, should the
process be declared to be out of control? What if
¯x = 10.25?
(c) If the process moves out of control so that the
variable mean becomes μ = 10.15 with σ = 0.2,
what is the probability that an observation lies
outside the control limits? What is the average run
length for detecting this change?
16.2.2 A production process making chemical solutions is in
control when the solution strengths have a mean of
μ0 = 0.650 and a standard deviation of σ = 0.015.
Suppose that it is a reasonable approximation to take the
solution strengths as being normally distributed. At
regular time intervals the solution strength is measured
and is plotted on a control chart.
(a) What are the center line and control limits of a
3-sigma control chart that you would construct to
monitor the strengths of the chemical solutions?
(b) If a randomly sampled solution had a strength of
x = 0.662, would you take this as evidence that the
production process had moved out of control? What
if x = 0.610?
(c) If the production process moves out of control so that
the chemical solution strengths have a mean
μ = 0.630 with σ = 0.015, what is the probability
that a randomly sampled solution has a strength that
lies outside the control limits? What is the average
run length for detecting this change?
16.2.3 Suppose that a 2-sigma control chart is used to monitor a
variable that can be taken to be normally distributed.
(a) What is the probability that an observation will lie
outside the control limits when the process is still in
control?
(b) If the mean of the variable increases to one standard
deviation above the control value, what is the
probability that an observation will lie outside the
control limits? What is the average run length for
detecting this change?
16.2.4 When a 3-sigma control chart is used, what is the average
run length when the process is in control? In other words,
if the process is in control, what is the expected number
of points plotted on the control chart until one of them
lies outside the control limits and provides the false
indication that the process has moved out of control?
16.2.5 Suppose that a 3-sigma control chart is used. What is the
probability that when the process is in control, the ﬁrst
eight points plotted lie on the same side of the center line
but within the control limits? Do you think that this
should be taken as evidence that the process has moved
out of control?
16.3
Variable Control Charts
When the variable of interest is a continuous measurement, it is common practice to use two
control charts in conjunction with each other. The ﬁrst is an ¯X-chart, which is concerned with
the mean value of the variable of interest, and the second is an R-chart, which is concerned
with the variation among the measurements of the variable of interest. Put simply, the ¯X-chart
looks for changes in the mean value μ and the R-chart looks for changes in the standard
deviation σ of the variable measured.
These control charts can be constructed from a base set of data observations that are
considered to be representative of the process when it is in control. This data set typically
consists of a set of samples of size n taken at k different points in time, as shown in Figure 16.7.
The sample size n is usually small, perhaps only 3, 4, or 5, but it may be as large as 20 in
some cases. It should be a convenient number of observations that can be sampled at frequent

16.3 VARIABLE CONTROL CHARTS
743
FIGURE 16.7
A data set for constructing
¯X-charts and R-charts
x11
x1n
¯x =
xi1+...+xin
n
ri = max{xi1,
, xin} −min{xi1,
, xin}
Sample size
n
Sample 1
Sample 2
x21
x2n
Sample averages
Sample ranges
Sample k
xk1
xkn
i
...
...
...
...
...
...
intervals. Generally speaking, the control chart should be set up using data from at least k = 20
distinct points in time.
16.3.1
¯X-Charts
The ¯X-chart consists of plots of the sample averages ¯x1, . . . , ¯xk against time. The sample
averages
¯xi = xi1 + · · · + xin
n
are the average values of the n data values taken at each point in time.
The center line of the control chart is drawn at the overall average of the k sample averages
¯¯x = ¯x1 + · · · + ¯xk
k
and the control limits are
UCL = ¯¯x + 3 σ
√n
and
LCL = ¯¯x −3 σ
√n
where σ is the standard deviation of the individual data measurements.
There are various ways to estimate σ. Traditionally, a function of the sample ranges has
been used with control charts since this was an easy calculation to perform before computers
were widely employed. The range of the sample at the ith time point is
ri = max{xi1, . . . , xin} −min{xi1, . . . , xin}
so that it is the difference between the largest data observation and the smallest data observation
in the sample of size n.
d2
A2
n = 2
1.128
1.880
n = 3
1.693
1.023
n = 4
2.059
0.729
n = 5
2.326
0.577
n = 6
2.534
0.483
n = 7
2.704
0.419
n = 8
2.847
0.373
n = 9
2.970
0.337
n = 10
3.078
0.308
n = 11
3.173
0.285
n = 12
3.258
0.266
n = 13
3.336
0.249
n = 14
3.407
0.235
n = 15
3.472
0.223
n = 16
3.532
0.212
n = 17
3.588
0.203
n = 18
3.640
0.194
n = 19
3.689
0.187
n = 20
3.735
0.180
n = 21
3.778
0.173
n = 22
3.819
0.167
n = 23
3.858
0.162
n = 24
3.895
0.157
n = 25
3.931
0.153
FIGURE 16.8
Values needed for the construction
of ¯X-charts.
The average sample range is then
¯r = r1 + · · · + rk
k
and the standard deviation is estimated as
ˆσ = ¯r
d2
where the values of d2, which depend only on the sample size n, are given in Figure 16.8. The
control limits are therefore
UCL = ¯¯x + A2¯r
and
LCL = ¯¯x −A2¯r

744
CHAPTER 16
QUALITY CONTROL METHODS
where
A2 =
3
d2
√n
which is also tabulated in Figure 16.8 for convenience.
The ¯X-Chart
An ¯X-chart consists of sample averages plotted against time and monitors changes in
the mean value of a variable. The lines on the control chart can be determined from a
set of k samples of size n, with the center line being taken as ¯¯x, the overall average of
the sample averages, and with control limits
UCL = ¯¯x + A2¯r
and
LCL = ¯¯x −A2¯r
where ¯r is the average of the k sample ranges.
Once the lines on the control chart have been determined, the k sample averages ¯xi should
be plotted. Since these data observations are taken to represent an in-control process, the points
should all lie within the control limits. If some points lie outside the control limits, then this
suggests that the process was not in control at those times. It is then generally best to remove
these data points from the data set and to construct the control chart anew from the reduced
data set.
D3
D4
n = 2
0
3.267
n = 3
0
2.574
n = 4
0
2.282
n = 5
0
2.114
n = 6
0
2.004
n = 7
0.076
1.924
n = 8
0.136
1.864
n = 9
0.184
1.816
n = 10
0.223
1.777
n = 11
0.256
1.744
n = 12
0.283
1.717
n = 13
0.307
1.693
n = 14
0.328
1.672
n = 15
0.347
1.653
n = 16
0.363
1.637
n = 17
0.378
1.622
n = 18
0.391
1.608
n = 19
0.403
1.597
n = 20
0.415
1.585
n = 21
0.425
1.575
n = 22
0.434
1.566
n = 23
0.443
1.557
n = 24
0.451
1.548
n = 25
0.459
1.541
FIGURE 16.9
Values needed for the construction
of R-charts.
The control chart can be used to monitor future observations. The sample average
¯x = x1 + · · · + xn
n
of a future sample of size n can be plotted on the control chart, and if it lies within the control
limits, the process can be considered to be still in control. If it lies outside the control limits,
then this can be taken as evidence that the mean value of the variable under consideration has
moved from its control value. In practice, this ¯X-chart is used in conjunction with an R-chart
discussed next, which monitors changes in the variability of the measurements.
16.3.2
R-Charts
The construction of an R-chart requires the values D3 and D4 that are given in Figure 16.9.
The R-Chart
An R-chart consists of sample ranges plotted against time and monitors changes in the
variability of a measurement of interest. The lines on the control chart can be
determined from a set of k samples of size n, with the center line being taken as ¯r, the
average of the k sample ranges, and with control limits
UCL = D4¯r
and
LCL = D3¯r
Once the control chart lines have been determined, the sample ranges r1, . . . ,rk can be
plotted to check that they fall within the control limits. As with the ¯X-chart, if some points

16.3 VARIABLE CONTROL CHARTS
745
fall outside the control limits, they should be removed from the data set and the control chart
should be reconstructed from the reduced data set. The ranges of future samples can then be
plotted on the control chart to monitor for changes in the standard deviation of the variable of
interest.
16.3.3
Modiﬁcations of ¯X-Charts and R-Charts
You will probably come across various modiﬁcations of the ¯X-chart and the R-chart that have
the same motivation and interpretation but that are constructed differently. Recently the sample
standard deviations are often used to estimate σ in place of the sample rangesri in the construc-
tion of the ¯X-chart. In addition, an “S-chart,” which consists of sample standard deviations
plotted against time, may be used in place of the R-chart for measuring process variability.
Technically speaking, the sample standard deviations are a more efﬁcient way of estimating σ
than the sample ranges for most sample sizes. Most computer packages provide the option of
using either approach, and it is unlikely that they will provide substantially different results.
In some circumstances only one observation is taken at each point in time so that n = 1.
Then the process variability cannot be examined (although the time intervals could be grouped
together to form larger time intervals where the sample sizes are larger than one). A control
chart can still be constructed to monitor changes in the mean value of the process, although
this will differ from the ¯X-chart due to the manner in which σ is estimated.
Finally, an alternative to the ¯X-chart is a CUSUM-chart where, for a control value μ0 of
the variable mean, the cumulative sums
t
i=1
( ¯xi −μ0)
are plotted against time. In other words, at time t the cumulative differences between the
control value and the sample averages at time t and all previous times are plotted, rather
than just the sample average at time t. While the interpretation of a CUSUM-chart is more
complicated than the interpretation of the ¯X-chart, its use is motivated by the fact that it is
more sensitive than the ¯X-chart to small changes in the variable mean from the control value,
so that it can detect these changes with a smaller average run length.
16.3.4
Examples
Example 14
Metal Cylinder
Production
Consider again the data set of 60 metal cylinder diameters given in Figure 6.5 and their
summary statistics given in Figure 6.29. Suppose that these data values represent the mea-
surements of a random sample of n = 3 cylinders taken every 30 minutes over a 10-hour run
of a production process, as shown in Figure 16.10. How can this data set be used to construct
a control chart for future runs of this production process?
As shown in Figure 16.10, the sample averages ¯xi and the sample ranges ri are ﬁrst
constructed. For example, for the ﬁrst sample the sample average is
¯x1 = x11 + x12 + x13
3
= 50.08 + 49.78 + 50.02
3
= 49.960
and the sample range is
r1 = max{x11, x12, x13} −min{x11, x12, x13}
= max{50.08, 49.78, 50.02} −min{50.08, 49.78, 50.02}
= 50.08 −49.78 = 0.30

746
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.10
Metal cylinder production data set
Diameter 
measurements
Sample 
average 
x¯i
Sample 
range 
ri
Sample 
number



1
50.08
49.78
50.02
49.960
0.30
2
50.02
50.13
49.74
49.963
0.39
3
49.84
49.97
49.93
49.913
0.13
4
50.02
50.05
49.94
50.003
0.11
5
50.19
49.86
50.03
50.027
0.33
6
50.04
49.96
49.90
49.967
0.14
7
49.87
50.13
49.81
49.937
0.32
8
50.02
50.26
49.90
50.060
0.36
9
50.01
50.04
50.01
50.020
0.03
10
49.79
50.36
50.21
50.120
0.57
11
50.17
50.12
50.00
50.097
0.17
12
50.01
49.85
49.93
49.930
0.16
13
49.84
50.20
49.94
49.993
0.36
14
49.74
50.00
50.03
49.923
0.29
15
49.92
50.07
49.89
49.960
0.18
16
49.99
50.01
50.09
50.030
0.10
17
49.90
50.05
49.95
49.967
0.15
18
50.20
50.03
49.92
50.050
0.28
19
50.02
49.97
50.27
50.087
0.30
20
49.77
50.07
50.07
49.970
0.30
The center line on the ¯X-chart is drawn at
¯¯x = 50.00
which is the average of all 60 data observations (and similarly is the average of the 20 sample
means), and the average sample range is
¯r = r1 + · · · + r20
20
= 0.30 + 0.39 + · · · + 0.30
20
= 0.2485
With A2 = 1.023 for n = 3, the control limits for the ¯X-chart are
UCL = ¯¯x + A2¯r = 50.00 + (1.023 × 0.2485) = 50.25
and
LCL = ¯¯x −A2¯r = 50.00 −(1.023 × 0.2485) = 49.75
The resulting ¯X-chart with the 20 sample averages plotted is shown in Figure 16.11.
The R-chart has a center line at ¯r = 0.2485, and with D3 = 0 and D4 = 2.574 for n = 3,
the control limits are
UCL = D4¯r = 2.574 × 0.2485 = 0.6397
and
LCL = D3¯r = 0 × 0.2485 = 0
Figure 16.12 shows the resulting R-chart with the 20 sample ranges plotted.
Notice that all of the points lie within the control limits for both the ¯X-chart and the
R-chart, so that it is reasonable to consider the process to be in control while the data set
was collected. However, as mentioned previously, this does not guarantee that the process

16.3 VARIABLE CONTROL CHARTS
747
Sample
Sample mean
19
17
15
13
11
9
7
5
3
1
50.3
50.2
50.1
50.0
49.9
49.8
49.7
x = 49.9988
UCL = 50.2524
LCL = 49.7453
¯¯
FIGURE 16.11
The ¯X-chart for the metal cylinder diameters example
r = 0.2485
UCL = 0.6397
LCL = 0
Sample
Sample range
19
17
15
13
11
9
7
5
3
1
0.7
0.6
0.5
0.4
0.3
0.2
0.0
0.1
¯
FIGURE 16.12
The R-chart for the metal cylinder diameters example
is working well. How well the process is working depends upon the actual speciﬁcations
required for the metal cylinders. Other statistical techniques can be used to assess how
well the production process is working, such as the analysis in Section 8.1.1, which re-
vealed that a 95% two-sided conﬁdence interval for the mean diameter of the cylinders is
(49.964, 50.033).
These control charts can now be used to monitor future runs of the production line. Suppose
that production has started again and after 30 minutes a random sample of n = 3 cylinders

748
CHAPTER 16
QUALITY CONTROL METHODS
have diameters measured as
x1 = 50.06, x2 = 50.01, and x3 = 49.67
Is there any evidence that the process has moved out of control? The sample average is
¯x = 50.06 + 50.01 + 49.67
3
= 49.913
and the sample range is
r = 50.06 −49.67 = 0.39
and when plotted on their respective control charts, both of these points lie within the control
limits. Consequently, the process can be considered to be still in control.
What happens if the data values
x1 = 50.34, x2 = 50.48, and x3 = 50.16
are obtained? In this case the sample average is
¯x = 50.34 + 50.48 + 50.16
3
= 50.327
and the sample range is
r = 50.48 −50.16 = 0.32
While the sample range r lies within the control limits in the R-chart, the sample average
lies above the upper control limit on the ¯X-chart. This ﬁnding alerts the supervisor that the
process has moved out of control due to an increase in the mean value of the cylinder diameters.
Suitable corrective action can be taken straightaway.
If the data values
x1 = 49.66, x2 = 50.47, and x3 = 50.03
are obtained, then the sample average is
¯x = 49.66 + 50.47 + 50.03
3
= 50.053
and the sample range is
r = 50.47 −49.66 = 0.81
In this case the sample average lies within the control limits on the ¯X-chart, but the sample
range lies above the upper control limit on the R-chart. This ﬁnding indicates that the process
has moved out of control due to an increase in the variability of the cylinder diameters.
Example 17
Milk Container
Contents
The milk bottling company has decided to employ a control chart to provide continuous
monitoring of the weight of its milk containers. During an 8-hour shift, a random sample of
n = 5 milk containers is selected every 20 minutes and their weights are measured. The data
set shown in Figure 16.13 is obtained.

16.3 VARIABLE CONTROL CHARTS
749
FIGURE 16.13
Milk container contents data set
Milk container 
weights
Sample 
average 
x¯i
Sample 
range 
ri
Sample 
number
1
2.106
1.965
2.081
2.079
2.129
2.0720
0.164
2
1.950
1.994
2.058
2.039
2.080
2.0242
0.130
3
2.095
2.043
2.158
2.005
2.085
2.0772
0.153
4
2.088
2.053
2.025
2.011
2.128
2.0610
0.117
5
1.980
2.080
2.189
2.024
2.072
2.0690
0.209
6
2.090
2.075
1.970
2.239
2.169
2.1086
0.269
7
2.147
2.124
2.069
2.077
1.999
2.0832
0.148
8
2.037
1.996
2.163
2.044
1.947
2.0374
0.216
9
2.072
2.083
2.033
2.103
2.018
2.0618
0.085
10
2.063
2.073
2.111
2.025
2.093
2.0730
0.086
11
2.045
2.162
2.084
2.032
2.079
2.0804
0.130
12
2.054
2.196
1.964
2.104
2.103
2.0842
0.232
13
1.883
1.833
2.288
2.119
1.957
2.0160
0.455
14
2.212
2.019
2.380
2.139
1.796
2.1092
0.584
15
2.131
1.986
1.785
2.233
2.058
2.0386
0.448
16
2.045
2.218
2.065
2.023
2.089
2.0880
0.195
17
2.048
2.054
2.005
1.969
1.995
2.0142
0.085
18
2.010
2.065
2.249
1.965
2.031
2.0640
0.284
19
2.022
1.996
1.970
1.946
2.146
2.0160
0.200
20
2.094
2.043
2.108
2.046
2.160
2.0902
0.117
21
2.086
2.015
2.097
2.121
2.008
2.0654
0.113
22
1.988
1.970
1.927
2.063
2.028
1.9952
0.136
23
2.031
2.016
2.088
2.096
2.121
2.0704
0.105
24
2.051
2.028
2.041
2.030
2.118
2.0536
0.090



The ¯X-chart and R-chart for this data set are shown in Figure 16.14. The ¯X-chart has a
center line at
¯¯x = 2.0605
and the R-chart has a center line at
¯r = 0.198
With A2 = 0.577 for n = 5, the control limits for the ¯X-chart are
UCL = ¯¯x + A2¯r = 2.0605 + (0.577 × 0.198) = 2.175
and
LCL = ¯¯x −A2¯r = 2.0605 −(0.577 × 0.198) = 1.946
Also, with D3 = 0 and D4 = 2.114 for n = 5, the control limits for the R-chart are
UCL = D4¯r = 2.114 × 0.198 = 0.419
and
LCL = D3¯r = 0 × 0.198 = 0
Notice that while the points on the ¯X-chart all lie within the control limits, there are three
consecutive points on the R-chart that lie above the upper control limit. This indicates that

750
CHAPTER 16
QUALITY CONTROL METHODS
Sample
Sample mean
2.20
2.15
2.10
2.05
2.00
1.95
23
21
19
17
15
13
11
9
7
5
3
1
x = 2.0605
UCL = 2.1747
LCL = 1.9464
Sample
Sample range
0.6
0.5
0.4
0.3
0.2
0.1
0.0
23
21
19
17
15
13
11
9
7
5
3
1
r = 0.1980
UCL = 0.4186
LCL = 0
¯¯
¯
FIGURE 16.14
The ¯X-chart and the R-chart for the milk container weights example
this data set is not representative of an in-control process, and speciﬁcally it indicates that
about 4 hours into the shift the process went out of control due to a sudden increase in the
variability of the milk container weights. It would be instructive for the company to identify
the reasons behind this (temporary) increase in variability so that the problem can be avoided
in the future.
It is best to eliminate the three samples that have sample ranges above the control limit
and to construct the control charts again. The control charts constructed from the reduced data
set are shown in Figure 16.15, and it can be seen that all points now lie within the control
limits. Notice that the control limits are also narrower than those obtained with the full data
set. These control charts can now be used to monitor the weights of future samples of milk
containers.

16.3 VARIABLE CONTROL CHARTS
751
FIGURE 16.15
The ¯X-chart and the R-chart for the
reduced data set of milk container
weights
Sample
Sample mean
2.15
2.10
2.05
2.00
1.95
21
19
17
15
13
11
9
7
5
3
1
Sample
Sample range
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
21
19
17
15
13
11
9
7
5
3
1
x = 2.0614
UCL = 2.1510
LCL = 1.9717
r = 0.1554
UCL = 0.3287
LCL = 0
¯¯
¯
16.3.5
Problems
16.3.1 DS 16.3.1 contains the sample means ¯xi and the sample
ranges ri of samples of size n = 4 of a measurement of
interest collected at k = 20 distinct time points from a
production process.
(a) Construct an ¯X-chart and an R-chart.
(b) Is there any evidence that the process was out of
control while the data values were collected? Should
any of the samples be removed from the data set?
(c) If the control charts are used for monitoring future
production runs, what conclusion would you
draw if a sample x1 = 86.1, x2 = 99.2,
x3 = 94.5, and x4 = 90.7 is obtained?
(d) What if x1 = 91.5, x2 = 78.0, x3 = 86.5, and
x4 = 82.4?
(e) What if x1 = 95.3, x2 = 92.1, x3 = 89.6, and
x4 = 90.1?
(f) What if x1 = 99.2, x2 = 93.8, x3 = 96.1, and
x4 = 94.2?
16.3.2 DS 16.3.2 contains the sample means ¯xi and the sample
ranges ri of a variable measurement based upon samples
of size n = 5, which are collected at k = 25 points in
time.
(a) Use this data set to construct an ¯X-chart and an
R-chart.

752
CHAPTER 16
QUALITY CONTROL METHODS
(b) Is there any evidence that the process was out of
control while the data values were collected? Which
samples should be removed from the data set?
(c) Construct control charts that can be employed to
monitor future measurements.
16.3.3 Glass Sheet Thicknesses
Control charts are to be used to monitor the thicknesses of
glass sheets. DS 16.3.3 contains the thicknesses in mm of
random samples of n = 4 glass sheets collected at k = 24
different times.
(a) What ¯X-chart and R-chart would you recommend be
used?
(b) What conclusion would you draw if in the future a
sample x1 = 3.12, x2 = 2.96, x3 = 2.91, and
x4 = 2.88 is obtained?
16.4
Attribute Control Charts
Whereasthe ¯X-chartandthe R-chartareappropriateformonitoringacontinuousmeasurement
of interest, attribute control charts can be used to monitor certain categorical measurements of
the items of interest. Speciﬁcally a p-chart can be used to monitor the proportion of a sample
that has a particular attribute. Typically, the proportion of defective items is of interest. A
c-chart can be used to monitor the frequency of defects within an item.
16.4.1
p-Charts
A p-chart consists of a set of sample proportions plotted against time. If at a certain point
in time a sample of size n items is obtained of which x possess a certain characteristic, the
sample proportion
p = x
n
is plotted on the control chart. If this point lies within the control limits, then the process can
be considered to be in control, and if it lies outside the control limits, then the process is known
to be out of control due to a change in the proportion of items that possess the characteristic
of interest.
A control chart can be constructed from a base set of data observations that are taken to
be representative of the in-control process. Such a data set consists of a set of proportions
p1, . . . , pk taken at k distinct points in time, where pi is the proportion of the sample of size
n taken at the ith time point that possess the characteristic of interest.
The center line of the control chart is the average proportion
¯p = p1 + · · · + pk
k
which is the proportion of all the items sampled that possess the characteristic. If ¯p really is
the probability that an item has the characteristic, then the number of such items in a future
sample of size n has a binomial distribution with parameters n and ¯p. The standard error of
the sample proportion with the characteristic is therefore
	
¯p(1 −¯p)
n
and consequently a 3-sigma control chart is constructed with control limits
UCL = ¯p + 3
	
¯p(1 −¯p)
n
and
LCL = ¯p −3
	
¯p(1 −¯p)
n

16.4 ATTRIBUTE CONTROL CHARTS
753
Often the lower control limit turns out to be negative, in which case it should be taken to be 0.
If any of the sample proportions p1, . . . , pk lie outside the control limits, then those samples
should be removed from the data set and the control chart should be recalculated.
The p-Chart
A p-chart consists of sample proportions plotted against time, and it monitors changes
in the proportion of items that possess a particular characteristic of interest. The lines
on the control chart can be determined from a set of k sample proportions obtained
from k samples of size n. The center line is taken to be ¯p, the average of the sample
proportions, and the control limits are
UCL = ¯p + 3
	
¯p(1 −¯p)
n
and
LCL = ¯p −3
	
¯p(1 −¯p)
n
Note that p-charts can also be used when the sample sizes are different at each time point,
and in such a case the conﬁdence band widths at each time point also vary. Such a p-chart
is shown in Figure 16.25, with the number of website visitors shown in Figure 16.24 varying
from week to week.
Example 2
Defective Computer
Chips
Consider again the data set in Figure 6.3, which contains the number of defective computer
chips found in 80 boxes of chips, with each box containing a total of n = 500 chips. Suppose
that these 80 boxes are obtained by taking every hundredth box off a production line. How
can this data set be used to set up a control chart to monitor the number of defective chips in
each box?
The sample proportions are
p1 =
1
500 = 0.002, p2 =
3
500 = 0.006, . . . , p80 =
1
500 = 0.002.
These values have an average of
¯p = 0.00615
which is the overall proportion of defective chips found. The control limits are consequently
UCL = ¯p + 3
	
¯p(1 −¯p)
n
= 0.00615 +

3 ×
	
0.00615 × (1 −0.00615)
500

= 0.0166
and
LCL = ¯p −3
	
¯p(1 −¯p)
n
= 0.00615 −

3 ×
	
0.00615 × (1 −0.00615)
500

= −0.0043
Since a negative number is obtained, the lower control limit should be taken to be 0.
Figure 16.16 shows the control chart for this data set. All of the 80 sample proportions
pi lie within the control limits, and so the data set can be taken as being representative of an

754
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.16
The p-chart for the defective
computer chips example
Sample
Proportion
80
70
60
50
40
30
20
10
0.000
0.018
0.016
0.014
0.012
0.010
0.008
0.006
0.004
0.002
p = 0.00615
UCL = 0.01664
LCL = 0
¯
in-control process. The proportion of defective computer chips found in future samples from
the production line can be plotted on the control chart to monitor for changes in the proportion
of defective chips.
In fact, a point will lie outside the upper control limit if the proportion of defective chips
is larger than 0.0166. This value corresponds to at least
0.0166 × 500 = 8.3
defective chips in a box of n = 500 chips or, in other words, nine or more defective chips in
a box. Thus, the control chart indicates that the process can be considered to have moved out
of control if the random sampling (every hundredth box) produces a box containing nine or
more defective chips.
If p is the true probability that a chip is defective, then each time a sample is taken this
procedure is equivalent to testing the hypotheses
H0 : p = ¯p
versus
HA : p ̸= ¯p
The number of defective chips X in a box of n = 500 chips is distributed
X ∼B(500, p)
and so this is a hypothesis testing problem concerning the success probability of a binomial
random variable, which is discussed in Section 10.1.2. Notice that, in general, an observed
proportion ˆp = x/n falls outside the control limits of a p-chart constructed with ¯p = p0 if
the z-statistic
z =
ˆp −p0

p0(1−p0)
n
discussed in Section 10.1.2 has an absolute value larger than 3 (because the p-chart is con-
structed as a 3-sigma chart).

16.4 ATTRIBUTE CONTROL CHARTS
755
Sometimes an np-chart is used in place of a p-chart. An np-chart is similar to a p-chart
except that the number of items possessing the characteristic of interest is plotted on the
control chart instead of the proportion of items. The center line of an np-chart is n ¯p, and the
control limits are
UCL = n ¯p + 3

n ¯p(1 −¯p)
and
LCL = n ¯p −3

n ¯p(1 −¯p)
An np-chart and a p-chart look the same except that the scaling of the y-axis is different.
16.4.2
c-Charts
Whereas a p-chart can be used when an item is classiﬁed as being either defective or satis-
factory, a c-chart allows an item to be judged on the basis of a count of the number of ﬂaws
or defects that it contains. A c-chart is therefore a control chart where the number of defects
found in a sample of items is plotted against time.
A base set of data observations corresponding to the number of defects observed in a set
of k items sampled at k distinct times can be used to construct a c-chart. If these data values
are x1, . . . , xk, then the center line of the control chart is drawn at the sample average
¯x = x1 + · · · + xk
k
Sample 
number
Number of 
fractures
1
3
2
1
3
8
4
1
5
5
6
6
7
5
8
5
9
1
10
6
11
6
12
4
13
5
14
6
15
9
16
4
17
3
18
8
19
4
20
3
21
3
22
5
23
4
24
2
25
5
26
7
27
6
28
10
29
16
30
14
31
15
32
15
FIGURE 16.17
Steel girder fractures data set
If the number of defects X in an item is modeled with a Poisson distribution with parameter
λ, then the expected number of defects per item and the variance of the number of defects per
item are both equal to λ (see Section 3.4). Consequently, λ can be estimated by ˆλ = ¯x, and
the standard deviation of the number of defects X in an item is
√
λ, which can be estimated
by √¯x. Consequently, the 3-sigma control limits are chosen to be
UCL = ¯x + 3
√
¯x
and
LCL = ¯x −3
√
¯x
As before, if the lower conﬁdence bound turns out to be negative, it is taken to be 0.
The c-Chart
A c-chart consists of points corresponding to the number of defects found in sampled
items plotted against time. The lines on the control chart can be determined from a set
of data observations corresponding to the number of defects observed in a set of k
sampled items. The center line is taken to be ¯x, the average of the data values, and the
control limits are
UCL = ¯x + 3
√
¯x
and
LCL = ¯x −3
√
¯x
In practice more than one item can be sampled at each time point, and in fact larger
sample sizes produce a more sensitive control chart. However, the same number of items must
be sampled at each time point with a c-chart. In this case the c-chart can be constructed using
data values corresponding to the total number of defects found at each time point from the
sampled items. A u-chart is a modiﬁcation of a c-chart that allows for varying numbers of
items to be sampled at different points in time.
Example 32
Steel Girder Fractures
A c-chart is to be constructed to monitor the number of hairline fractures occurring along the
edges of steel girders. It is decided that a girder will be randomly selected every 15 minutes and

756
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.18
The c-chart for the steel girder
fractures example
Sample count
0
18
16
14
12
10
8
6
4
2
Sample
32
28
24
20
16
12
8
4
x = 6.09
UCL = 13.50
LCL = 0
¯
that a 1-meter segment of the girder will then be randomly selected. The number of fractures
within this segment will then be recorded. Figure 16.17 contains a data set consisting of 32
observations collected from an 8-hour shift which is to be used to construct the control chart.
The average number of fractures found is
¯x = 3 + 1 + · · · + 15
32
= 6.094
which is the center line of the control chart. The control limits are
UCL = ¯x + 3
√
¯x = 6.094 + (3 ×
√
6.094) = 13.50
and
LCL = ¯x −3
√
¯x = 6.094 −(3 ×
√
6.094) = −1.31
Since a negative value is obtained, the lower control limit can be taken to be 0.
Figure 16.18 shows the resulting control chart together with plots of the data values. It can
be seen that the process appears to have moved out of control toward the end of the 8-hour
shift. In fact the four data observations taken during the last hour are all above the upper
control limit. This increase in the number of fractures should be investigated because it could
provide some useful information on how to keep the number of fractures low in the future.
If the last four data points are removed from the data set, the sample average becomes
¯x = 3 + 1 + · · · + 10
28
= 4.821
and the control limits are
UCL = ¯x + 3
√
¯x = 4.821 + (3 ×
√
4.821) = 11.41
and
LCL = ¯x −3
√
¯x = 4.821 −(3 ×
√
4.821) = −1.77

16.4 ATTRIBUTE CONTROL CHARTS
757
FIGURE 16.19
The c-chart for the reduced data set
of steel girder fractures
Sample
Sample count
28
25
22
19
16
13
10
7
4
1
12
10
8
6
4
2
0
x = 4.82
UCL = 11.41
LCL = 0
¯
which again can be taken to be 0. This control chart is shown in Figure 16.19 and all of the
data points lie between the control limits. The chart can be used to monitor new girders, and
the process will be determined to be out of control if a randomly sampled girder has 12 or
more fractures within the randomly chosen 1-meter segment.
16.4.3
Problems
16.4.1 Construct a p-chart from the data in DS 16.4.1, which are
the number of defective items found in random samples
of n = 100 items taken at k = 30 time points.
(a) Is there any reason to believe that the process was out
of control while this data set was collected?
(b) How many defective items would need to be
discovered in a future sample for the process to be
considered to be out of control?
16.4.2 Spray Painting Procedure
Metal rods are spray painted by a machine and a p-chart
is to be used to monitor the proportion of rods that are not
painted correctly. These defective rods have either an
incomplete coverage or a paint layer that is too thick in
some places. The data set in DS 16.4.2 contains the
number of defective rods found in random samples of
n = 400 rods taken at k = 25 time points.
(a) Why does it appear that the process was out of control
at certain times while this data set was collected?
(b) What p-chart would you recommend be used to
monitor future samples of rods?
(c) How many defective rods would need to be
discovered in a future sample for the process to be
considered to be out of control?
16.4.3 Construct a c-chart from the data in DS 16.4.3, which are
the number of defects found in a randomly sampled
product at k = 24 time points.
(a) Should any observations be deleted from the data set?
(b) How many defects would need to be discovered in a
future randomly sampled product for the process to
be considered to be out of control?
16.4.4 Paper Quality Assessment
A paper mill has decided to use a c-chart to monitor the
number of imperfections in large paper sheets. The data
set in DS 16.4.4 records the number of imperfections
found in k = 22 sheets of paper sampled at different
times.
(a) Construct a c-chart. Why does it appear that the
process was out of control at certain times while this
data set was collected?
(b) What c-chart would you recommend be used to
monitor future samples of paper?
(c) How many imperfections would need to be
discovered in a future randomly sampled paper
sheet for the process to be considered to be out of
control?

758
CHAPTER 16
QUALITY CONTROL METHODS
16.5
Acceptance Sampling
16.5.1
Introduction
Acceptance sampling has traditionally been a partner of statistical process control and control
charts in the area of statistical quality control. Products are shipped around in batches or lots,
and the idea behind acceptance sampling is that a batch can be declared to be satisfactory or
unsatisfactory on the basis of the number of defective items found within a random sample of
items from the batch. Thus acceptance sampling provides a general check on the “quality” of
the items within a batch.
Acceptance sampling can be performed by the producer of the products before they are
shippedout.Inaddition,samplingmaybeperformedbycustomerswhoreceivetheproductsin
order to check that they are receiving high-quality materials. Many companies are accustomed
to performing acceptance sampling on their incoming raw materials before taking delivery of
them.
Notice that there is an important procedural distinction between statistical process control
and acceptance sampling. The control charts used in statistical process control provide a real-
time monitoring of a production process with the objective of avoiding the production of
low-quality materials. Changes in the process can be identiﬁed almost immediately so that
corrective actions can be taken. In contrast, acceptance sampling is performed after production
has been completed. It does not allow any monitoring of the production process, and whole
batches of products can be wasted if they are found to be unsatisfactory.
16.5.2
Acceptance Sampling Procedures
Consider a batch of N items, each one of which can be classiﬁed as being either satisfactory
or defective. When N is very large, a 100% inspection scheme of the batch in which each item
is examined is generally too expensive and time consuming. Therefore, a random sample of
n of the items is chosen, and the number of defective items x in the sample is found.
The acceptance sampling procedure is based upon a rule whereby the batch is declared to
be satisfactory (accept the batch) if the number of defective items x is no larger than a constant
c, and the batch is declared to be unsatisfactory (reject the batch) if the number of defective
items x is larger than c. Thus the batch is accepted if
x = 0, . . . , c
and is rejected if
x = c + 1, . . . , n
as shown in Figure 16.20.
The statistical properties of the acceptance sampling procedure can be determined by
considering how the acceptance probability depends on the true proportion p of defective
items in the batch. It is usual to deﬁne an acceptable quality level (AQL), p0 say, so that a
batch is considered to be acceptable as long as the actual proportion of defective items is no
larger than the AQL, that is as long as
p ≤p0
The batch is not considered to be acceptable if
p > p0

16.5 ACCEPTANCE SAMPLING
759
Batch of N items
Proportion p defective
Random sample
         of n
        items   
n
x = number of defective items in sample
x = 0, 1, 2 , ... , c, c + 1, ... , n
Accept batch Reject batch
FIGURE 16.20
An acceptance sampling procedure
Producer’s risk
Type I error
Type II error
Consumer’s risk
HA : proportion of defective
 
items p > p0
H0 : proportion of defective
 
items p ≤ p0
Accept batch
Reject batch
Acceptance sampling procedure
FIGURE 16.21
The risks associated with an acceptance sampling procedure
There are two kinds of risk associated with the acceptance sampling procedure, a pro-
ducer’s risk and a consumer’s risk. The producer’s risk is the probability that the batch is
rejected when in fact the proportion of defective items p is smaller than the AQL p0. This is the
probability that the producer produces an acceptable batch of items, but that the batch is de-
clared to be unsatisfactory by the acceptance sampling procedure. Obviously, it is desirable
to have the producer’s risk as small as possible.
In contrast, the consumer’s risk is deﬁned to be the probability that a batch is declared to
be satisfactory by the acceptance sampling procedure when in fact the proportion of defective
items p is larger than the AQL p0. This is the probability that a consumer takes delivery of
a batch that is really not acceptable. The amount of consumer’s risk depends on the value
of the actual proportion of defective items p, and again it is desirable to have it as small as
possible.
The concepts of producer’s risk and consumer’s risk are analogous to the probabilities of
a type I error and a type II error for a hypothesis testing problem, as shown in Figure 16.21.
Consider the hypothesis testing problem
H0 : p ≤p0
versus
HA : p > p0
The null hypothesis H0 states that the batch is acceptable and the alternative hypothesis HA
states that the batch is not acceptable.
The size of the hypothesis test is
α = P(Type I error)
= P(reject H0 when H0 is true)
= P(reject batch when batch is acceptable)
= producer’s risk

760
CHAPTER 16
QUALITY CONTROL METHODS
Similarly, if the power of the hypothesis test is 1 −β for a proportion of defective items
p1 > p0, then
β = 1 −power
= P(Type II error)
= P(accept H0 when H0 is false)
= P(accept batch when batch is not acceptable)
= consumer’s risk
Acceptance Sampling Procedures
An acceptance sampling procedure is based on a rule whereby a batch is declared to
be satisfactory (the batch is accepted) if the number of defective items x found in a
random sample from the batch is no larger than a constant c, and the batch is declared
to be unsatisfactory (the batch is rejected) if the number of defective items x is larger
than c.
The producer’s risk is the probability that the batch is rejected when in fact the
proportion of defective items is smaller than an acceptable quality level (AQL) p0.
The consumer’s risk is the probability that a batch is declared to be satisfactory when
in fact the proportion of defective items is larger than the acceptable quality level p0.
The properties of an acceptance sampling procedure can be viewed graphically with an
operating characteristic curve, which plots the probability of accepting a batch
P(accept batch)
against the proportion of defective items p. As Figure 16.22 shows, both the producer’s risk
and the consumer’s risk at a proportion p1 can be found from the operating characteristic
curve.
The operating characteristic curve is constructed from the distribution of the random
variable X, which is the number of defective items found in the random sample of size n.
Sincesamplingisperformedwithoutreplacement,therandomvariable X hasahypergeometric
distribution (see Section 3.3). If the proportion of defective items is p, so that the total number
of defective items in the batch of size N is Np, then the probability mass function of the
number of defective items in the sample is
P(X = x) =

Np
x

×

N(1 −p)
n −x


N
n

The probability that the batch is accepted is
P(accept batch) = P(X = 0) + P(X = 1) + · · · + P(X = c)
If the batch size N is much larger than the sample size n, as is usually the case, then recall
that the hypergeometric distribution can be approximated by a binomial distribution, so that
in this case the approximate distribution
X ∼B(n, p)

16.5 ACCEPTANCE SAMPLING
761
FIGURE 16.22
The operating characteristic curve
for an acceptance sampling plan
Producer’s risk
Operating characteristic
curve
Consumer’s risk
1
00
p0
p1
1
P(accept batch)
AQL
Proportion of 
defective items
p 
can be used. The acceptance probability can then be calculated as
P(accept batch) =
c

i=0

n
i

pi(1 −p)n−i
Example 10
Fiber Coatings
Fibers are packaged in batches of N = 500 ﬁbers and an acceptance sampling scheme is
employed to check the acceptability of the coating on the ﬁbers. An acceptable quality level of
AQL = p0 = 0.05
is decided upon, and a random sample of n = 20 ﬁbers is to be taken from a batch. A value
of c = 2 is used so that the batch is considered to be acceptable if no more than two of
the ﬁbers in the sample are found to have an unsatisfactory coating. What are the operating
characteristics of this acceptance sampling procedure?
With the binomial approximation, the probability of accepting a batch when the true
proportion of defective ﬁbers is p is
P(accept batch) =
2

i=0

20
i

pi(1 −p)20−i
= (1 −p)20 + 20 p(1 −p)19 + 190 p2(1 −p)18
The operating characteristic curve obtained from this formula is shown in Figure 16.23.
When p = p0 = 0.05, the probability that the batch is accepted is
P(accept batch) = 0.9520 + (20 × 0.05 × 0.9519) + (190 × 0.052 × 0.9518)
= 0.92

762
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.23
The operating characteristic curve
for the ﬁber coatings acceptance
sampling plan
0.0
Producer’s risk = 0.08
Consumer’s risk = 0.21
1.0
0.0
p0
p1
P (accept batch)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.05 0.1
0.2
0.3
0.4
0.5
0.6
AQL
p
0.9
0.92
The producer’s risk is the probability that the batch is rejected when p = p0 = 0.05 and is
therefore
producer’s risk = 1 −0.92 = 0.08
What is the consumer’s risk when a proportion p1 = 0.2 of the ﬁbers have an unsatisfactory
coating? This is the probability that the batch is accepted when p = p1 = 0.2, which is
consumer’s risk = P(accept batch)
= 0.820 + (20 × 0.2 × 0.819) + (190 × 0.22 × 0.818) = 0.21
as shown in Figure 16.23.
Many handbooks are available to assist in the implementation of acceptance sampling
procedures. These handbooks indicate what sample sizes n should be taken and what values
of c should be used in order to obtain a desired operating characteristic curve.
It is also worth pointing out that more sophisticated sampling schemes such as double or
sequential sampling plans have been developed. In a double sampling scheme two constants
c1 < c2 are employed. An initial random sample is obtained and the batch is accepted if
the number of defective items is smaller than c1 and the batch is rejected if the number of
defective items is larger than c2. However, if the number of defective items lies between c1
and c2, then a second random sample is taken. In this latter case, a ﬁnal decision is then based
upon the number of defective items found in the combination of the two samples. Sampling
plans such as this have been developed with the objective of minimizing the average amount of
sampling that needs to be performed while maintaining the desired properties of the operating
characteristic curve.

16.6 CASE STUDY: INTERNET MARKETING
763
16.5.3
Problems
16.5.1 A random sample of size n = 5 is taken from a batch of
size N = 50, and the batch is accepted if the number of
defective items found is no larger than c = 2. Use the
hypergeometric distribution to calculate each exact value.
(a) The producer’s risk for an acceptable quality level of
p0 = 0.06
(b) The consumer’s risk if the proportion of defective
items is p1 = 0.20
Recompute these risks using the binomial approximation
to the distribution of the number of defective items found
in the sample.
16.5.2 Electrical fuses are sold in boxes of N = 20 fuses, and an
acceptance sampling procedure is implemented with a
random sample of size n = 3 and a value c = 1. Calculate
and compare each value exactly using the hypergeometric
distribution and approximately using the binomial
distribution.
(a) The producer’s risk for an acceptable quality level of
p0 = 0.10
(b) The consumer’s risk if the proportion of defective
items is p1 = 0.20
16.5.3 An acceptance sampling procedure has n = 40 and c = 8
for a very large batch size N so that the binomial
distribution can be used to calculate the probability of
accepting the batch. Calculate the probability of accepting
the batch for various values of the proportion of defective
items p, and sketch the operating characteristic curve.
(a) What is the producer’s risk for an acceptable quality
level of p0 = 0.01?
(b) What is the consumer’s risk if the proportion of
defective items is p1 = 0.25?
16.5.4 Ceramic tiles are shipped in very large batches and an
acceptance sampling procedure to monitor the number of
cracked tiles in a batch has n = 50 and c = 10. Use the
binomial approximation to calculate the probability of
accepting the batch of tiles for various values of the
proportion of cracked tiles p, and sketch the operating
characteristic curve.
(a) What is the producer’s risk for an acceptable quality
level of p0 = 0.01?
(b) What is the consumer’s risk if the proportion of
defective items is p1 = 0.10?
(c) How do the producer’s and consumer’s risks change
if a value of c = 9 is used?
16.5.5 Suppose that an acceptance plan has n = 30 and that the
binomial approximation can be used to calculate
the probability of accepting a batch. What value of c
should be employed so that the consumer’s risk is
minimized while the producer’s risk is no larger than 5%
for an acceptable quality limit of p0 = 0.10?
16.6
Case Study: Internet Marketing
Over a 6-month period, the company monitors the number of visits to its website and the
number of resulting purchases on a weekly basis, as shown in Figure 16.24. A p-chart can be
used to analyze the proportion of visits that result in a purchase, as shown in Figure 16.25.
Notice that week 10 had an unusually large number of purchases relative to the number of
FIGURE 16.24
Data set of website visits
and purchases
Website visits
Purchases from website
Website visits
Purchases from website
254,028
10,074
264,819
10,977
304,394
12,603
276,647
10,987
354,765
14,669
278,381
11,266
348,410
13,877
314,430
12,874
331,162
13,589
296,117
11,708
263,029
10,441
321,372
13,155
352,599
14,607
261,534
10,060
354,808
14,201
324,995
11,878
212,230
8,836
306,690
12,657
206,321
9,858
329,449
13,290
325,705
12,988
226,729
9,005
227,555
9,433
215,438
8,743
293,472
12,190
222,741
9,080

764
CHAPTER 16
QUALITY CONTROL METHODS
FIGURE 16.25
Analysis of purchases from website
Week
p-chart of purchases from website
Proportion
0.0475
0.0425
0.0450
0.0400
0.0375
0.0350
25
19
22
16
13
10
7
4
1
1
_
P = 0.04058
website visits, whereas the purchase proportions were unusually low in weeks 20 and 21. An
investigation of the speciﬁc circumstances in weeks 10, 20, and 21 should provide some useful
information for the company.
16.7
Supplementary Problems
16.7.1 A company that manufactures soap bars ﬁnds that the
bars tend to be underweight if too much air is blown into
the soap solution so that its density is too low. It is
decided to set up a control chart to monitor the density of
the soap solution, and at regular time intervals the soap
solution density is measured and plotted on a control
chart. Experience indicates that the process is in
control when the solution density has a mean value of
μ0 = 1250 and a standard deviation of σ = 12.
Suppose that it is a reasonable approximation to
take the soap solution density as being normally
distributed.
(a) What are the center line and control limits of a
3-sigma control chart that you would construct to
monitor the soap solution density?
(b) If a randomly sampled solution had a density of 1300
would you take this as evidence that the production
process had moved out of control? What if the
density is 1210?
(c) If the soap production process moves out of control
so that the soap solution density has a mean
μ = 1240 with σ = 12, what is the probability that a
randomly sampled solution has a density that lies
outside the control limits? What is the average run
length for detecting this change?
16.7.2 Paper Quality Assessment
A paper mill has decided to implement a control chart
to monitor the weight of the paper that it is producing.
DS 16.7.1 contains the weights in g/m2 of n = 3 random
paper samples collected at k = 22 different points
in time.
(a) Does it look as though the paper producing
process was in control when this data set was
collected?
(b) What control charts would you suggest are used to
monitor future runs of paper production?
(c) What conclusion would you draw if in the future a
sample x1 = 76.01, x2 = 73.42, and x3 = 72.61
is obtained?
(d) What if x1 = 77.82, x2 = 79.04, and x3 = 75.83?
16.7.3 Date Code Legibility
A factory that packages food products in metal cans has
installed an ink jet to spray a date code onto the bottom of
the cans. Sometimes the process does not work correctly
and the date codes are either missing or illegible. A

16.7 SUPPLEMENTARY PROBLEMS
765
p-chart is to be constructed based on the data set in
DS 16.7.2, which records the number of defective date
codes found in random samples of n = 250 cans taken
at k = 25 time points.
(a) Does it appear that the process was out of
control at any time while this data set was
collected?
(b) What p-chart would you recommend be used to
monitor future samples of date codes?
(c) How many defective date codes would need
to be discovered in a future sample for the process to
be considered to be out of control?
16.7.4 Fabric Flaws
In the textile industry c-charts can be used to monitor
the number of ﬂaws occurring in segments of fabric.
Construct a c-chart from the data set in DS 16.7.3, which
records the number of ﬂaws found in k = 25 fabric
segments sampled at different times.
(a) Should you remove any points from the data set?
What c-chart would you recommend be used to
monitor future samples of fabric?
(b) How many ﬂaws would need to be discovered in a
future randomly selected fabric segment for the
process to be considered to be out of control?
16.7.5 An acceptance sampling procedure is being developed to
check whether batteries have the required voltage. The
batteries are shipped in very large batches, and the
voltages of a random sample of n = 50 batteries are to be
measured. What value of c should be used if it is decided
that the producer’s risk should be no larger than 2.5% for
an acceptable quality limit of p0 = 0.06? In this case,
what is the consumer’s risk if the proportion of defective
batteries is p1 = 0.30?

C H A P T E R S E V E N T E E N
Reliability Analysis and Life Testing
Reliability analysis is an important component of engineering work that draws on the areas
of probability and statistics. As the name suggests, reliability analysis is concerned with the
investigation of the failure rates of components and systems, which are typically represented as
probabilities. Life testing is a general term used to describe the experimentation and statistical
analysis performed to investigate failure rates. Some of the topics in probability and statistics
that have been discussed in this book and that relate directly to the areas of reliability and
life testing are brought together in this chapter and are discussed more explicitly within this
framework.
In Section 17.1 the basic rules for calculating the probabilities of combinations of indepen-
dent events discussed in Chapter 1 are used to calculate the reliabilities of complex systems.
In Section 17.2 the problem of modeling failure rates is considered. Probability distributions
that are used to model failure times are typically the exponential distribution, the gamma
distribution, the Weibull distribution, and the lognormal distribution. Some of the statistical
analysis techniques used in life testing are discussed in Section 17.3. Statistical analysis can
be employed to make inferences on the failure rates under these distributional assumptions
or to make general inferences on the distribution of failure times without any modeling as-
sumptions. Data sets of failure times often contain censored data observations where the exact
failure time is known only to lie within a certain region and is not known exactly. The product
limit estimator discussed in Section 17.3 allows the estimation of the failure time distribution
when some of the data observations are censored in this way.
17.1
System Reliability
The reliability of a component, which can be denoted by r, can in general be thought of as the
probability that it performs a certain task. The complement of this probability is therefore
the probability that the component fails to perform the required task, or the probability that
the component “fails.” In Section 17.2 probability distributions are employed to model
how these reliabilities vary with time. In this section the probability calculations that can
be used to ﬁnd the overall reliability of a system of components are discussed.
A complex system may consist of a large number of components each with its own
reliability value. A system reliability diagram, such as that shown in Figure 17.1, can be used
to show how the failure of the various components affects the overall status of the system.
The overall system operates successfully only if it is possible to progress from one side of the
diagram to the other side by passing only through components that have not failed. If this is
not possible, then the system has failed. If the reliabilities of the individual components are
known, then the question of interest is what is the overall reliability of the system or, in other
words, what is the probability that the system operates successfully?
This question is typically answered by assuming that the failures of the components are
independent of each other and by then applying the basic rules of probability theory. These
766

17.1 SYSTEM RELIABILITY
767
Reliability
r1
Reliability
r2
Reliability
r3
Reliability
r4
Reliability
r5
Reliability
r6
Reliability
r7
Reliability
r8
Reliability
r9
Reliability
r10
Reliability
r11
Reliability
r12
FIGURE 17.1
A system reliability diagram
FIGURE 17.2
A system reliability diagram for n
components placed in series
Overall reliability = r1 × r2
· · · × rn
×
rn
r2
r3
r1
probability rules can be used to calculate the reliabilities of systems consisting of components
in series and in parallel, and the reliabilities of more complex systems can be calculated by
decomposing them into an appropriate collection of modules.
17.1.1
Components in Series
A set of components is considered to be in series if the system works only if each one of the
components works. In other words, the system fails whenever one or more of the components
has failed. The system reliability diagram for a system of n components in series is shown in
Figure 17.2. If the reliabilities of the components are r1, . . . ,rn, then the multiplication law
for independent events discussed in Section 1.5.2 implies that the overall system reliability is
therefore
r = r1 × · · · × rn
Components in Series
If a system consists of n components with independent reliabilities r1, . . . ,rn placed
in series, then the overall system reliability is
r = r1 × · · · × rn
Example 6
Satellite Launching
Suppose that the satellite is launched on a rocket that has four thrusters, each of which
must work in order for the launch to be successful. Suppose furthermore that each thruster is
estimated to have a reliability of 0.995, which implies that they fail on average one time in 200
attempts. Since each thruster must work for the launch to be successful, the system reliability
diagram has the four thrusters in series as shown in Figure 17.3. The overall system relia-
bility is therefore
r = r1 × r2 × r3 × r4 = 0.9954 = 0.980150

768
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
Overall reliability = r1 × r2 × r3 × r4 = 0.9954 = 0.980
Thruster 1
Thruster 2
Thruster 3
Thruster 4
r1 = 0.995
r3 = 0.995
r2 = 0.995
r4 = 0.995
FIGURE 17.3
The system reliability diagram for the thrusters in the satellite launching
example
rn
r2
r3
r1
Overall reliability
= 1 −(1
[
−r1) × · · · × (1
]
−rn)
FIGURE 17.4
A system reliability diagram for n
components placed in parallel
17.1.2
Components in Parallel
A set of components is considered to be in parallel if the system works whenever at least one
of the components works. In other words, the system fails only when all of the components
have failed. The system reliability diagram for a system of n components in parallel is shown
in Figure 17.4. If the reliabilities of the components are r1, . . . ,rn and these are independent,
then the probability that all n components fail is
(1 −r1) × · · · × (1 −rn)
This implies that the overall reliability of the system is
r = 1 −[(1 −r1) × · · · × (1 −rn)]
Notice that adding additional components in parallel increases the reliability of the system.
When components are in parallel, they can be considered as acting as “backups” for each
other.
Components in Parallel
If a system consists of n components with independent reliabilities r1, . . . ,rn placed
in parallel, then the overall system reliability is
r = 1 −[(1 −r1) × · · · × (1 −rn)]
Example 6
Satellite Launching
Recall that the satellite launch requires a computer and that three computers are available,
with computers 2 and 3 acting as backups for computer 1. Consequently, the system reliability

17.1 SYSTEM RELIABILITY
769
FIGURE 17.5
The system reliability diagram for
the computers in the satellite
launching example
Overall reliability 
= 1 − [(1−r1)×(1−r2)×(1−r3)]
= 1 − [(1−0.99)×(1−0.99)×(1−0.99)] 
= 0.999999
Computer 3
r3 = 0.99
Computer 2
r2 = 0.99
Computer 1
r1 = 0.99
Automatic guidance system
Thruster 1
Thruster 2
Thruster 3
Thruster 4
Computer 1
Computer 2
Computer 3
Satellite
receiver
Ground transmitter
Ground transmitter
Satellite
release
r12= 0.95
r10= 0.99
r11= 0.99
Module 2
Module 1
Module 3
Module 4
r1 = 0.995
r2 = 0.995
r3 = 0.995
r4 = 0.995
r5 = 0.99
r6 = 0.99
r7 = 0.99
r8 = 0.9
r9 = 0.925
FIGURE 17.6
The system reliability diagram for the satellite launching example
diagram shown in Figure 17.5 has the three computers placed in parallel. Each computer is
estimated to malfunction with a probability of 0.01, so that the reliabilities are
r1 = r2 = r3 = 0.99
The overall system reliability is therefore
r = 1 −[(1 −r1) × (1 −r2) × (1 −r3)] = 1 −0.013 = 0.999999
and the probability of a system failure is
1 −r = 1 −0.999999 = 10−6
as calculated in Section 1.5.3.
17.1.3
Complex Systems
The overall reliability of a complex system of components can be calculated by decomposing
the system into a series of modules. Each module should consist of a fairly simple system
of components in series or in parallel. The reliabilities of each of the modules can then be
calculated, and the overall system reliability is found by combining the module reliabilities
in the appropriate manner. These ideas are illustrated in the following example.
Example 6
Satellite Launching
The system reliability diagram in Figure 17.1 actually corresponds to the system reliability
diagram for a satellite launch shown in Figure 17.6. Notice that it can be decomposed into
four modules placed in series.

770
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
The ﬁrst module corresponds to the four thrusters operating successfully, and this has been
shown to have an overall reliability of
rm1 = 0.980150
The second module corresponds to at least one of the three computers working, which has
been shown to have an overall reliability of
rm2 = 0.999999
The third module is concerned with whether the rocket can be maneuvered into the correct
orbit for releasing the satellite. This maneuvering can be performed by an automatic guidance
system on the rocket, which has a reliability of r8 = 0.9. If this automatic guidance system
fails, then as a backup the rocket can be maneuvered from the ground. In order for this to
work successfully the receiver on the satellite must function properly and at least one of two
ground transmitters must be operational. The satellite receiver has an estimated reliability
of r9 = 0.925 and the ground transmitters have estimated reliabilities of r10 = r11 = 0.99.
Notice how the third module is therefore composed of the automatic guidance system placed
in parallel with the ground-based system of maneuvering, and that the ground-based system
of maneuvering consists of the two ground transmitters placed in parallel with each other and
in series with the satellite receiver.
The reliability of the third module can be calculated by decomposing it into smaller
modules as shown in Figure 17.7. Module 5 consists of the two ground transmitters placed in
parallel so that it has a reliability of
rm5 = 1 −[(1 −r10) × (1 −r11)] = 1 −0.012 = 0.9999
Module 6 then consists of the satellite receiver and module 5 placed in series so that its
reliability is
rm6 = r9 × rm5 = 0.925 × 0.9999 = 0.924908
FIGURE 17.7
The decomposition of module 3 in
the satellite launching example
Automatic guidance system
Satellite
receiver
Ground transmitter
Ground transmitter
Module 3
Module 5
Module 6
r8 = 0.9
r10 = 0.99
r9 = 0.925
r11 = 0.99

17.1 SYSTEM RELIABILITY
771
Finally, module 3 is composed of the automatic guidance system placed in parallel with
module 6 so that its reliability is
rm3 = 1 −[(1 −r8) × (1 −rm6)] = 1 −[(1 −0.9) × (1 −0.924908)] = 0.992491
Module 4 in Figure 17.6 corresponds to a successful release of the satellite from the rocket,
which has an estimated reliability of r12 = 0.95. The complete system consists of modules
1–4 placed in series, so the overall reliability of the system is therefore
r = rm1 × rm2 × rm3 × rm4
= 0.980150 × 0.999999 × 0.992491 × 0.95 = 0.924
Consequently, this reliability analysis estimates that the chance of a successful satellite de-
ployment is 92.4%.
COMPUTER NOTE
As a ﬁnal comment in this section, it is worthwhile to point out that many systems are
considerably more complicated than the ones that we have considered, and engineers are
consequently often forced to resort to computer simulation methods to evaluate the reliabilities
of complex systems. In these simulations the reliabilities can be allowed to be time dependent
and the components can be randomly assigned as being operational or as having failed. The
system reliability can be estimated as the proportion of the simulations in which the overall
system is operational.
17.1.4
Problems
17.1.1 Calculate the reliability of the system diagram in
Figure 17.8.
r1 = 0.95
r3 = 0.98
r2 = 0.99
FIGURE 17.8
17.1.2 Calculate the reliability of the system diagram in
Figure 17.9.
r2 = 0.90
r4 = 0.95
r5 = 0.92
r3 = 0.99
r1 = 0.99
FIGURE 17.9
17.1.3 (a) If four identical components are placed in series, how
large must their individual reliabilities be in order for
the overall system reliability to be at least 0.95?
(b) If four identical components are placed in parallel,
how large must their individual reliabilities be in
order for the overall system reliability to be at least
0.95?
(c) Provide general answers for parts (a) and (b)
assuming that n identical components are used and
that an overall system reliability r is required.
17.1.4 (a) Three components with reliabilities r1 = 0.92,
r2 = 0.95, and r3 = 0.975 are placed in series. A
fourth component with reliability r4 = 0.96 can be
placed in parallel with any one of these three
components. How should the fourth component be
placed in order to maximize the overall system
reliability?
(b) Does your answer to part (a) change if the value of r4
changes? In general, how does your answer depend
upon the values of r1, r2, and r3?
17.1.5 Calculate the reliability of the system diagram in
Figure 17.10.
17.1.6 Calculate the reliability of the system diagram in
Figure 17.11.

772
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
r1 = 0.90
r2 = 0.90
r3 = 0.90
r4 = 0.90
r5 = 0.90
r6 = 0.98
r7 = 0.95
r14 = 0.90
r12 = 0.98
r8 = 0.99
r9 = 0.99
r10 = 0.90
r11= 0.90
r13 = 0.97
r15 = 0.95
r16 = 0.98
FIGURE 17.10
r2 = 0.98
r3 = 0.97
r4 = 0.96
r5 = 0.95
r6 = 0.98
r14= 0.96
r7 = 0.98
r10 = 0.99
r11= 0.99
r12= 0.95
r13= 0.95
r1 = 0.99
r8 = 0.98
r18= 0.98
r15 = 0.90
r17 = 0.90
r16 = 0.90
r9 = 0.90
FIGURE 17.11
17.2
Modeling Failure Rates
It is often important to be able to model the reliability of a component as a function of time,
r(t). This can be accomplished by assigning a probability density function for the time to
failure of the component. The hazard rate is then a particularly useful tool for assessing how
the risk of failure varies with time.
17.2.1
Time to Failure
Suppose that the random variable T measures the time to failure of a component. A probability
density function f (t) for this random variable provides the probability distribution of the
failure time, and the cumulative distribution function F(t) can be interpreted as the probability

17.2 MODELING FAILURE RATES
773
FIGURE 17.12
Components with exponential
failure times in series
r1(t)
MTTF = μ1
exponential
rn(t)
MTTF = μn
exponential
r2(t)
MTTF = μ2
exponential
Mean time to failure (MTTF) of system =
1
1μ1+ 1μ2 + ··· + 1μn
that the component has failed by time t. Conversely, the reliability function
r(t) = 1 −F(t)
is the probability that the component has not failed by time t. The reliability function is
sometimes written as
¯¯F(t) or may be referred to as the survival function S(t). The mean
time to failure (MTTF) is the expectation of the time to failure T
E(T ) =
 ∞
0
t f (t) dt
Failure times are typically modeled with the exponential distribution, the gamma distri-
bution, the Weibull distribution, and the lognormal distribution.
The Exponential Distribution
The exponential distribution with parameter λ is discussed
in Section 4.2. The probability of a failure by time t is
F(t) = 1 −e−λt
so that the reliability function is
r(t) = e−λt
The mean time to failure is 1/λ.
The exponential distribution is often used to model failure rates, and one main advantage of
doing so is that it allows mathematically tractable solutions to many reliability problems. For
example, suppose that a system consists of n components in series, and that the ith component
has a failure time that can be modeled with an exponential distribution with a mean time to
failureμi,asshowninFigure17.12.Whatisthedistributionofthetimetofailureofthesystem?
The ith component has a failure time that is exponentially distributed with parameter
λi = 1/μi, and so the probability that it is still working at time t is
ri(t) = e−λit
The system is still working at time t only if all of the n components are still working at time t,
and if the failure times of the components are independent, then this implies that the reliability
function of the system is
r(t) = e−λ1t × · · · × e−λnt = e−λt
where
λ = λ1 + · · · + λn

774
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
This implies that the failure time of the system is exponentially distributed with parameter λ,
so that the mean time to failure of the system is
1
λ =
1
λ1 + · · · + λn
=
1
1
μ1 + · · · +
1
μn
Notice that if the mean time to failure for each component is μ∗, then the mean time to failure
of the system is
μ∗
n
The Gamma Distribution
The gamma distribution with parameters k and λ is discussed in
Section 4.3. The exponential distribution is obtained as a special case when k = 1. The mean
time to failure is k/λ, but there is no simple expression for the reliability function r(t).
The Weibull Distribution
The Weibull distribution with parameters a and λ is discussed
in Section 4.4. It is perhaps the most widely used probability distribution for modeling failure
times. The exponential distribution is obtained as a special case when a = 1. The probability
of a failure by time t is
F(t) = 1 −e−(λt)a
so that the reliability function is
r(t) = e−(λt)a
The mean time to failure is
1
λ 

1 + 1
a

where (x) is the gamma function.
The Lognormal Distribution
The lognormal distribution with parameters μ and σ 2 is
discussed in Section 5.4.1. Recall that if the random variable T has a lognormal distribution,
thenthetransformedrandomvariableln(T )hasanormaldistributionwithmeanμandvariance
σ 2. The probability of a failure by time t is
F(t) = 
ln(t) −μ
σ

so that the reliability function is
r(t) = 1 −
ln(t) −μ
σ

The mean time to failure is
eμ+σ 2/2
17.2.2
The Hazard Rate
The hazard rate h(t) of a failure time distribution f (t) measures the instantaneous rate of
failure at time t conditional on the component still being operational at time t. It is sometimes
also referred to as the failure rate and intuitively it represents the chance of a component that
has not failed by time t suddenly failing. Thus if h(t1) is larger than h(t2), then a component

17.2 MODELING FAILURE RATES
775
that is still operating at time t1 is more likely to suddenly fail than a component that is still
operating at time t2.
The hazard rate is calculated as
h(t) = f (t)
r(t) =
f (t)
1 −F(t)
Notice that the hazard rate is uniquely determined by the failure time probability density
function f (t). Similarly, the failure time probability density function f (t) can be calculated
from a hazard rate using the expression
f (t) = h(t) e− t
0 h(x)dx
The Hazard Rate
The hazard rate h(t) of a failure time distribution f (t), which is also known as the
failure rate, is calculated as
h(t) = f (t)
r(t) =
f (t)
1 −F(t)
and can be interpreted as the chance that a component that has not failed by time t
suddenly fails.
A common form of hazard rate for an engineering component is the “bathtub” shape shown
in Figure 17.13. In this case the hazard rate initially decreases, remains fairly constant for a
period of time, and then increases again. The hazard rate has high values initially because some
components fail very early due to errors in their manufacture or construction. These failures
are often referred to as failures in the “burn-in” phase of the components. If a component
survives this initial time period, then it can be expected to last a reasonable period of time
so that the hazard rate remains fairly low over the middle portion. Eventually the age of
the components and their wear and tear lead to an increase in the hazard rate at later times.
Interestingly, human mortality follows this “bathtub” pattern. Child mortality rates are quite
FIGURE 17.13
A typical bathtub shape for a
hazard rate
Burn-in
phase
h(t)
Time
Increasing hazard
rate due
to old age

776
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
high, but having survived infancy a person enjoys a fairly low and constant mortality risk for
a substantial period of time. However, with the onset of old age the mortality rate starts to
increase again.
In practice it is difﬁcult to ﬁnd a probability model that has a hazard rate with the “bathtub”
shape. Consequently, most probability models assume that the burn-in phase of the compo-
nents has been passed so that it is reasonable to use a model with an increasing hazard rate.
Such models are generally referred to as increasing failure rate (IFR) models and have the
intuitively reasonable interpretation that the longer a component has been operating without
failing, the larger is the chance of its sudden failure.
The Exponential Distribution
The hazard rate for an exponential distribution with param-
eter λ is
h(t) = f (t)
r(t) = λ e−λt
e−λt
= λ
This constant hazard rate is related to the memoryless property of the exponential distribution
discussed in Section 4.2.2, and it implies that any two components that are operating suc-
cessfully are equally likely to suddenly fail, regardless of how much older one component is
than the other. This is an unrealistic result for most situations, but nevertheless the simplicity
and mathematical tractability of the exponential distribution cause it to be widely used in
reliability analysis.
The Weibull Distribution
The hazard rate for a Weibull distribution with parameters a
and λ is
h(t) = f (t)
r(t) = a λa ta−1 e−(λt)a
e−(λt)a
= a λa ta−1
This formula provides an increasing failure rate for values of the parameter a larger than one,
and the larger the value of the parameter a, the steeper the increase in the hazard rate.
Example 33
Bacteria Lifetimes
Recall that the lifetime of a bacterium at a certain high temperature is modeled with a Weibull
distribution with parameters a = 2 and λ = 0.1. The hazard rate is therefore
h(t) = a λa ta−1 = 2 × 0.12 × t2−1 = 0.02 t
so that it increases linearly in time. Thus, for example, a bacterium still alive at a time 2t is
twice as likely to suddenly die as a bacterium still alive at a time t.
No simple expressions can be obtained for the hazard rates of the gamma distribution and
the lognormal distribution. The gamma distribution has an increasing failure rate when the
parameter k is larger than 1, although the lognormal distribution typically has a hump-shaped
hazard rate that ﬁrst increases and then decreases.
17.2.3
Problems
17.2.1 A component has an exponential failure time distribution
with a mean time to failure of 225 hours.
(a) What is the probability that the component is still
operating after 250 hours?
(b) What is the probability that the component fails
within 150 hours?
(c) If three of these components are placed in series, what
is the probability that the system is still operating
after 100 hours?
17.2.2 A component has an exponential failure time distribution
with a mean time to failure of 35 days.

17.3 LIFE TESTING
777
(a) What is the probability that the component is still
operating after 35 days?
(b) What is the probability that the component fails
within 40 days?
(c) If six of these components are placed in series, what
is the probability that the system is still operating
after 5 days?
17.2.3 Four components with mean times to failure of 125
minutes, 60 minutes, 150 minutes, and 100 minutes are
placed in series. If the failure times are exponentially
distributed, what is the mean time to failure of the
system?
17.2.4 A component has a constant hazard rate of 0.2.
(a) What is the probability that the component is still
operating at time 4.0?
(b) What is the probability that the component fails
before time 6.0?
17.2.5 A component has a lognormal failure time distribution
with parameters μ = 2.5 and σ = 1.5.
(a) What is the probability that the component is still
operating at time 40?
(b) What is the probability that the component fails
before time 10?
(c) What is the mean time to failure of the component?
(d) What is the median failure time?
17.2.6 A component has a lognormal failure time distribution
with parameters μ = 3.0 and σ = 0.5.
(a) What is the probability that the component is still
operating at time 50?
(b) What is the probability that the component fails
before time 40?
(c) What is the mean time to failure of the component?
(d) What is the median failure time?
17.2.7 A component has a Weibull failure time distribution with
parameters a = 3.0 and λ = 0.25.
(a) What is the probability that the component is still
operating at time 5?
(b) What is the probability that the component fails
before time 3?
(c) What is the median failure time?
(d) What is the hazard rate?
(e) How much more likely to suddenly fail is a
component operating at time 5 compared with
a component operating at time 3?
17.2.8 A component has a Weibull failure time distribution with
parameters a = 4.5 and λ = 0.1.
(a) What is the probability that the component is still
operating at time 12?
(b) What is the probability that the component fails
before time 8?
(c) What is the median failure time?
(d) What is the hazard rate?
(e) How much more likely to suddenly fail is a
component operating at time 12 compared with a
component operating at time 8?
17.3
Life Testing
The failure time distribution f (t) can be investigated by measuring the failure times t1, . . . , tn
of a set of n components. Inferences can be made on the failure time distribution f (t) by
considering these observations to be independent observations from the distribution. Some
model ﬁtting techniques are discussed in Section 17.3.1, and censored data observations,
where the exact failure time is unknown, are discussed in Section 17.3.2.
17.3.1
Model Fitting
Some model ﬁtting methodologies for reliability distributions are illustrated with the expo-
nential distribution and the lognormal distribution.
Exponential Distribution
If the failure times t1, . . . , tn are considered to be observations
from an exponential distribution, then the parameter λ can be estimated as
ˆλ =
n
t1 + · · · + tn
= 1
t

778
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
α/2
α/2
χ2
2n,α/2
χ2
2n,1−α/2
χ2
2n distribution
FIGURE 17.14
Critical points of a χ2
2n distribution
0.6
8.4
12.8
4.3
1.3
8.3
18.1
20.6
21.0
17.8
3.3
6.2
9.7
35.2
13.9
2.6
5.2
5.6
13.7
3.8
FIGURE 17.15
Electrical discharge times in seconds
This is the maximum likelihood estimate of λ and the estimate obtained from the method of
moments (see Section 7.4). The mean failure time μ = 1/λ is then estimated as
ˆμ = 1
ˆλ = t
which is simply the sample average.
A 1 −α conﬁdence level conﬁdence interval for the mean failure time can be constructed
as
μ ∈

2nt
χ2
2n,α/2
,
2nt
χ2
2n,1−α/2

where χ2
2n,α/2 is the upper α/2 critical point and χ2
2n,1−α/2 is the lower α/2 critical point of a
chi-square distribution with 2n degrees of freedom, as shown in Figure 17.14 and tabulated
in Table II. This conﬁdence interval can be rewritten in terms of the parameter λ as
λ =

χ2
2n,1−α/2
2nt
,
χ2
2n,α/2
2nt

Conﬁdence Interval for the Mean Failure Time of an Exponential
Distribution
If t is the sample average of a set of n data observations from an exponential
distribution, then
μ ∈

2nt
χ2
2n,α/2
,
2nt
χ2
2n,1−α/2

is a 1 −α conﬁdence level conﬁdence interval for the mean failure time.
Example 78
Electrical Discharges
A metal plate is administered a certain electrical charge and the time taken for it to suddenly
discharge is measured. The experiment is performed n = 20 times and the resulting data set
is shown in Figure 17.15.

17.3 LIFE TESTING
779
The exponential distribution is often employed to model the time until an electrical dis-
charge occurs since the constant hazard rate (memoryless property) is often appropriate. It is
reasonable to consider the chance of a sudden discharge to be independent of the time that
has elapsed since the electrical charge was administered.
The mean discharge time is estimated as
ˆμ = t = 0.6 + · · · + 3.8
20
= 10.62
and with χ2
40,0.025 = 59.34 and χ2
40,0.975 = 24.43, a 95% conﬁdence interval for the mean
discharge time is
μ ∈
2 × 20 × 10.62
59.34
, 2 × 20 × 10.62
24.43

= (7.16, 17.39)
Lognormal Distribution
A method of moments approach is typically used to ﬁt a lognormal
distribution to a data set t1, . . . , tn of failure times. This can be performed by taking the
logarithms of the failure times
ln(t1), . . . , ln(tn)
which can be considered to be observations from a normal distribution with mean μ and
standard deviation σ. The sample average and sample standard deviation of the logarithms of
the failure times can then be used as estimates of μ and σ.
ti
ln(ti)
0.610
0.718
0.877
0.646
1.012
0.586
0.542
0.596
0.631
0.888
0.872
0.667
0.804
0.735
0.997
0.535
0.853
0.641
0.738
0.838
0.716
0.671
0.933
0.436
0.793
−0.494
−0.331
−0.131
−0.437
0.012
−0.534
−0.612
−0.518
−0.460
−0.119
−0.137
−0.405
−0.218
−0.308
−0.003
−0.625
−0.159
−0.445
−0.304
−0.177
−0.334
−0.399
−0.069
−0.830
−0.232
FIGURE 17.16
Reaction times in seconds
Example 40
Testing Reaction Times
Figure 17.16 contains n = 25 reaction times ti measured in an experiment. If a lognormal
distribution is to be used to model the reaction time, what parameter values μ and σ are
appropriate? The logarithms of the reaction times ln(ti) are also shown in Figure 17.16 and
these have a sample average
ln(t) = ln(t1) + · · · + ln(tn)
n
= −0.331
and a sample standard deviation
	n
i=1(ln(ti) −ln(t))2
n −1
= 0.210
so that ˆμ = −0.331 and ˆσ = 0.210.
The (1 −α) quantile of a lognormal distribution is eμ+σzα, so with z0.10 = 1.282 the 90th
percentile of the distribution of reaction times can be estimated as
e ˆμ+ ˆσ1.282 = e−0.331+(0.210×1.282) = 0.940
Fitting a Weibull distribution or a gamma distribution to a data set is not so straightfor-
ward, and various methods have been proposed based on method of moments approaches or
regression techniques. Details of these model ﬁtting methods can be found in more detailed
reliability textbooks.

780
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
17.3.2
Censored Data
Whereas the previous discussion in this chapter considered data sets comprised of n exact
failure times t1, . . . , tn, attention is now directed to circumstances where a data observation ti
is not known exactly but where a bound on its value is available. Such observations are known
as censored observations.
Censored observations occur very commonly when measurements are taken of the failure
times of a particular experimental item. Whereas an exact observation ti records the exact
failure time of an item, a censored observation arises when an item’s failure time is not
observed exactly. In particular, an observation is said to be right-censored if all that is known
is that failure had not occurred by the given time, and an observation is said to be left-censored
if all that is known is that failure occurred some time before the given time.
There are various “censoring mechanisms” that result in censored observations rather
than exact observations. A common reason for an observation to be right-censored is that the
experimenter loses track of the item before it fails or that the experiment is concluded before
the particular item fails. Left-censored observations can arise if a particular item fails before
it has begun to be monitored.
Example 1
Machine Breakdowns
Suppose that a manager is interested in how often a particular component of a machine needs to
be replaced. This component may be something like a fan-belt, a cartridge, or a battery which
needs to be changed with some regularity. Over a period of time, the manager monitors various
machines and collects the data set of component failure times in days shown in Figure 17.17,
where an asterisk representsa right-censored observation.
15
22*
39
31
12*
8*
24*
26
21*
18
24
28
14*
18*
17
22*
32
21*
17*
11*
20
17
17
13*
24
23*
19*
19
27
23
5*
19
25
9*
16*
24
15
FIGURE 17.17
Data set for machine breakdowns
example
In certain cases the manager is able to observe how long a component lasts before it needs
to be replaced, and this provides an exact observation of a failure time. Thus, for example, the
ﬁrst data observation 15 is for a particular component that is observed to last exactly 15 days
before it needed to be replaced.
However, at regularly scheduled maintenance overhauls of the machines the components
are replaced regardless of whether or not they have failed. This results in some right-censored
observations. For example, the second data observation 22∗is for a particular component
that had been working successfully for 22 days before it was replaced during a maintenance
overhaul. All that the manager knows about the failure time of this component is that it would
have been more than 22 days.
The presence of censored observations necessarily complicates the analysis of a data set.
However, since the censored observations provide information about the underlying probabil-
ity distribution of interest, it is inefﬁcient and misleading to reduce the data set to a collection
of exact observations by ignoring the censored observations. Consequently, it is important to
consider analysis techniques that can incorporate the information provided by the censored
observations.
The following discussion considers the nonparametric analysis of data sets with right-
censored observations. First, the estimation of an unknown reliability function r(t) based
upon a data set of exact and right-censored, independent, identically distributed observations is
addressed using the product limit estimator. Finally, the construction of conﬁdence intervals
for the reliability function r(t) is considered.
The Product Limit Estimator
In Section 15.1.1, the empirical distribution function
ˆF(t) = #ti ≤t
n

17.3 LIFE TESTING
781
was proposed as a nonparametric estimator of an unknown distribution function F(t) based
upon a sample t1, . . . , tn of independent, identically distributed random variables from that
distribution. If some of the data observations are censored, however, then clearly the empirical
distribution function cannot be formed, since the quantity #ti ≤t is in general not known
exactly. For example, in the estimation of F(10), it is not known whether a right-censored
observation at time 9 corresponds to an exact observation less than 10 or greater than 10.
Consequently,amoresophisticatedestimatorisneeded,andthisisprovidedbytheproduct
limit estimator, which was ﬁrst proposed by Edward Kaplan and Paul Meier in 1958. The
product limit estimator is most easily discussed in terms of the reliability function
r(t) = 1 −F(t)
rather than the distribution function F(t) itself.
The product limit estimator ˆr(t) provides an estimate of the reliability function r(t) when
there are some right-censored data observations. Its construction is illustrated in the following
example.
Example 1
Machine Breakdowns
The ﬁrst step in the construction of the product limit estimator ˆr(t) is to order the data
observations. The ordered machine component failure times are shown in Figure 17.18.
5*
8*
9*
11*
12*
13*
14*
15
15
16*
17
17
17
17*
18
18*
19
19
19*
20
21*
21*
22*
22*
23
23*
24
24
24
24*
25
26
27
28
31
32
39
FIGURE 17.18
The ordered data set for the machine
breakdowns example
Next, a list is formed of the distinct times ti at which exact failures are observed. For this
data set these values are
t1 = 15, t2 = 17, t3 = 18, t4 = 19, t5 = 20, t6 = 23, t7 = 24, t8 = 25,
t9 = 26, t10 = 27, t11 = 28, t12 = 31, t13 = 32, t14 = 39
It is also necessary to record the number of exact failures di observed at these times, which
are
d1 = 2, d2 = 3, d3 = 1, d4 = 2, d5 = 1, d6 = 1, d7 = 3, d8 = 1, d9 = 1,
d10 = 1, d11 = 1, d12 = 1, d13 = 1, d14 = 1
Finally, the number of observations ni, censored or exact, that are recorded at or later than
each of the times ti should be calculated. For this example these are
n1 = 30, n2 = 27, n3 = 23, n4 = 21, n5 = 18, n6 = 13, n7 = 11, n8 = 7,
n9 = 6, n10 = 5, n11 = 4, n12 = 3, n13 = 2, n14 = 1
Notice that the smallest seven observations are all censored, so that t1, the ﬁrst time at
which an exact failure is observed, is equal to the eighth observation, which is 15. The next
distinct time at which an exact failure is observed is then t2 = 17. There are two items observed
to fail at exactly t1 = 15, and so d1 = 2. Similarly, d2 = 3 since three items are observed to
fail at exactly t2 = 17. All except the ﬁrst seven censored observations have failure times
(either exact or censored) that are equal to or are larger than t1 = 15, and so n1 = 30. Of these
30 observations, three are smaller than t2 = 17, and so n2 = 30 −3 = 27.
The product limit estimator ˆr(t) is a step function with steps at the times ti, and with the
step heights depending upon the values of the di and the ni. It is given by the formula
ˆr(t) =

j|t j<t
n j −d j
n j


782
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
The product is over all the j values for which t j < t, and if t ≤t1, then ˆr(t) is taken to
be 1.
The product limit estimator ˆr(t) is easily calculated in a sequential manner as shown.
0 < t ≤t1 = 15 ⇒ˆr(t) = 1
t1 = 15 < t ≤t2 = 17 ⇒ˆr(t) = 1 × (n1 −d1)/n1 = 0.933
t2 = 17 < t ≤t3 = 18 ⇒ˆr(t) = 0.933 × (n2 −d2)/n2 = 0.830
t3 = 18 < t ≤t4 = 19 ⇒ˆr(t) = 0.830 × (n3 −d3)/n3 = 0.794
t4 = 19 < t ≤t5 = 20 ⇒ˆr(t) = 0.794 × (n4 −d4)/n4 = 0.718
t5 = 20 < t ≤t6 = 23 ⇒ˆr(t) = 0.718 × (n5 −d5)/n5 = 0.678
t6 = 23 < t ≤t7 = 24 ⇒ˆr(t) = 0.678 × (n6 −d6)/n6 = 0.626
t7 = 24 < t ≤t8 = 25 ⇒ˆr(t) = 0.626 × (n7 −d7)/n7 = 0.455
t8 = 25 < t ≤t9 = 26 ⇒ˆr(t) = 0.455 × (n8 −d8)/n8 = 0.390
t9 = 26 < t ≤t10 = 27 ⇒ˆr(t) = 0.390 × (n9 −d9)/n9 = 0.325
t10 = 27 < t ≤t11 = 28 ⇒ˆr(t) = 0.325 × (n10 −d10)/n10 = 0.260
t11 = 28 < t ≤t12 = 31 ⇒ˆr(t) = 0.260 × (n11 −d11)/n11 = 0.195
t12 = 31 < t ≤t13 = 32 ⇒ˆr(t) = 0.195 × (n12 −d12)/n12 = 0.130
t13 = 32 < t ≤t14 = 39 ⇒ˆr(t) = 0.130 × (n13 −d13)/n13 = 0.065
t14 = 39 < t ⇒ˆr(t) = 0.065 × (n14 −d14)/n14 = 0.000
Figure 17.19 shows a plot of the product limit estimator.
The product limit estimator ˆr(t) provides point estimates of the probability of surviving at
least until a given time t. In this case ˆr(20) = 0.718 so that the manager would estimate that
FIGURE 17.19
The product limit estimator of the
reliability function for the machine
breakdowns example
0
10
20
30
40
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
t
r (t)
ˆ

17.3 LIFE TESTING
783
about 72% of components last for at least 20 days, and ˆr(30) = 0.195 so that the manager
would estimate that about 20% of components last for at least 30 days. The median lifetime can
be estimated as ˆr−1(0.5) = 24, so that the manager expects about half of the components to
last less than 24 days and about half to last more than 24 days. The construction of conﬁdence
intervals around these point estimates is discussed below.
One aspect of the product limit estimator is that if the largest observation is a censored
one, then the estimator ˆr(t) technically never reaches 0. Consequently, it is usual to deﬁne the
product limit estimator to be 0 after the largest observation.
Notice that if the censored observations are discarded and ignored, then the empirical
cumulative distribution function provides an estimate of the distribution function that is a step
function with steps at the values ti. The heights of the steps would be di/n, where n is the
sample size. The product limit estimator is also a step function with steps at the values ti.
However, the step heights depend not only on the number of exact observations di, but also
on the censored data observations through the quantities ni.
An estimate of the survival function r(t) can also be constructed when some of the data
observations are left-censored, or when some observations are doubly censored, so that they
are both right-censored and left-censored. Again, the survival function r(t) is estimated by a
step function with steps at the observed times of exact failures. However, in these cases there
is not a simple form for the step heights and a computer program is needed to construct the
estimator ˆr(t).
Finally, it is interesting to consider how the presence of censored data affects a parametric
approach to the estimation of the distribution function or reliability function. If a parametric
density function f (t; θ) is assumed, with a corresponding distribution function F(t; θ), then
the parameters θ are generally estimated by the method of maximum likelihood.
For a given data set, the likelihood consists of the product of terms of the form f (ti; θ)
for an observation of an exact failure at time ti, and of terms of the form (1 −F(ti; θ)) for a
right-censored observation at time ti. The parameter estimates ˆθ are the values that maximize
the likelihood, although there is usually no simple closed form expressions for these estimates
when there are censored data.
Conﬁdence Intervals for ˆr(t)
For a data set with right-censored observations, a conﬁdence
interval for the survival function r(t) at a particular value of t, based on the product limit
estimator ˆr(t) at t, may be calculated as
r(t) ∈

ˆr(t) −zα/2

Var(ˆr(t)), ˆr(t) + zα/2

Var(ˆr(t))

with an approximate conﬁdence level of 1 −α. The critical point zα/2 is the upper α/2 point
of the standard normal distribution, and Var(ˆr(t)) is an estimate of the variance of ˆr(t), which
is given by
Var(ˆr(t)) = ˆr(t)2 ×

j|t j<t
d j
n j(n j −d j)
This expression for the variance is known as Greenwood’s formula, and again the sum is over
all the j values for which t j < t.
Note that a conﬁdence interval cannot be constructed for r(t) if t ≤t1, in which case
ˆr(t) = 1, nor if t is greater than the largest data observation, in which case ˆr(t) = 0.

784
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
Example 1
Machine Breakdowns
For the data set of machine component failure times, the variances Var(ˆr(t)) can be calculated
to be
t1 = 15 < t ≤t2 = 17 ⇒Var(ˆr(t)) = 0.00207
t2 = 17 < t ≤t3 = 18 ⇒Var(ˆr(t)) = 0.00483
t3 = 18 < t ≤t4 = 19 ⇒Var(ˆr(t)) = 0.00567
t4 = 19 < t ≤t5 = 20 ⇒Var(ˆr(t)) = 0.00722
t5 = 20 < t ≤t6 = 23 ⇒Var(ˆr(t)) = 0.00794
t6 = 23 < t ≤t7 = 24 ⇒Var(ˆr(t)) = 0.00929
t7 = 24 < t ≤t8 = 25 ⇒Var(ˆr(t)) = 0.01196
t8 = 25 < t ≤t9 = 26 ⇒Var(ˆr(t)) = 0.01241
t9 = 26 < t ≤t10 = 27 ⇒Var(ˆr(t)) = 0.01214
t10 = 27 < t ≤t11 = 28 ⇒Var(ˆr(t)) = 0.01115
t11 = 28 < t ≤t12 = 31 ⇒Var(ˆr(t)) = 0.00944
t12 = 31 < t ≤t13 = 32 ⇒Var(ˆr(t)) = 0.00701
t13 = 32 < t ≤t14 = 39 ⇒Var(ˆr(t)) = 0.00387
For example, if t5 = 20 < t ≤t6 = 23, then
Var(ˆr(t)) = ˆr(t)2 ×
5

j=1
d j
n j(n j −d j)
= 0.6782 ×

2
30 × 28 +
3
27 × 24 +
1
23 × 22 +
2
21 × 19 +
1
18 × 17

= 0.00794
A conﬁdence interval for r(22) with a conﬁdence level of 0.95, so that zα/2 = 1.96, is
thus calculated as
r(22) ∈

0.678 −1.96
√
0.00794, 0.678 + 1.96
√
0.00794

= (0.503, 0.853)
Consequently, the manager can be about 95% certain that between 50% and 85% of the
machine components will last at least 22 days. Notice that this statement is based upon a
sample of only 37 observations of which 17 are censored. Obviously, greater precision can be
achieved with a larger sample size.
Individual conﬁdence intervals with conﬁdence levels of 0.95 can be calculated for other
times as
t1 = 15 < t ≤t2 = 17 ⇒r(t) ∈(0.844, 1.000)
t2 = 17 < t ≤t3 = 18 ⇒r(t) ∈(0.694, 0.966)
t3 = 18 < t ≤t4 = 19 ⇒r(t) ∈(0.646, 0.942)
t4 = 19 < t ≤t5 = 20 ⇒r(t) ∈(0.551, 0.885)
t5 = 20 < t ≤t6 = 23 ⇒r(t) ∈(0.503, 0.853)
t6 = 23 < t ≤t7 = 24 ⇒r(t) ∈(0.437, 0.815)
t7 = 24 < t ≤t8 = 25 ⇒r(t) ∈(0.241, 0.669)
t8 = 25 < t ≤t9 = 26 ⇒r(t) ∈(0.172, 0.608)
t9 = 26 < t ≤t10 = 27 ⇒r(t) ∈(0.109, 0.541)
t10 = 27 < t ≤t11 = 28 ⇒r(t) ∈(0.053, 0.467)
t11 = 28 < t ≤t12 = 31 ⇒r(t) ∈(0.005, 0.385)
t12 = 31 < t ≤t13 = 32 ⇒r(t) ∈(0.000, 0.294)
t13 = 32 < t ≤t14 = 39 ⇒r(t) ∈(0.000, 0.187)

17.4 CASE STUDY: INTERNET MARKETING
785
Note that conﬁdence intervals with an upper limit greater than one are truncated at 1, and
conﬁdence intervals with a lower limit less than 0 are truncated at 0.
17.3.3
Problems
17.3.1 A set of n = 30 components are tested and their average
lifetime is t = 132.4 hours.
(a) If the lifetimes are modeled with an exponential
distribution, construct a 99% conﬁdence interval for
the mean time to failure.
(b) The components are advertised as having “an average
lifetime of at least 150 hours.” Do you think that this
is a plausible claim?
17.3.2 Vibration Robustness of Electrical Circuits
The failure times in hours of n = 20 identical electrical
circuits subjected to an intense vibration are given in
DS 17.3.1.
(a) If the failure times are modeled with an exponential
distribution, construct a 95% conﬁdence interval for
the mean time to failure.
(b) Do you think that it is plausible that the mean time to
failure is 14 hours?
17.3.3 Thirty computer chips are tested in a sequential manner.
A chip is placed in a circuit and when it fails it is
immediately replaced by another chip. The ﬁnal chip fails
176.5 hours after the experiment was started.
(a) If the failure times of the computer chips are modeled
with an exponential distribution, construct a 99%
conﬁdence interval for the mean time to failure.
(b) Do you think that it is plausible that the mean time to
failure is 10 hours?
17.3.4 Virus Survival Times
The survival times in hours of a virus under certain
conditions are given in DS 17.3.2.
(a) If the survival times are modeled with a lognormal
distribution, estimate the parameters μ and σ.
(b) Use these point estimates to estimate the probability
that a virus will survive for more than 10 hours under
these conditions.
17.3.5 DS 17.3.3 contains a data set of failure times, where an
asterisk represents a right-censored observation.
(a) Construct and graph the product limit estimator of the
reliability function.
(b) Construct a 95% conﬁdence interval for the
probability that a component operates until at least
time 100.
17.3.6 Customer Churn
Customer churn is a term used for the attrition of a
company’s customers. DS 17.3.4 contains information
from an Internet service provider on the length of days
that its customers were signed up before switching to
another provider. These are terminated services. The data
set also contains the current service lengths of its ongoing
customers (who have not yet terminated their service).
Use this data to estimate the distribution of service times.
17.4
Case Study: Internet Marketing
Recall that when a customer has logged on to the organisation’s website, the length of the
idle periods in minutes is distributed as a gamma distribution with k = 1.1 and λ = 0.9. The
lifetime of interest here is the length that a logged-in individual is idle. In Chapter 4, it was
shown that this lifetime has an expectation of k/λ = 1.1/0.9 = 1.22 minutes with a standard
deviation of
√
k/λ =
√
1.1/0.9 = 1.17 minutes, and it was calculated that if the individual is
automatically logged out when the idle period reaches 5 minutes, then the proportion of the
idle periods resulting in the individual being automatically logged out is 1.4%.
Suppose that for increased security purposes the automatic logout is changed so that
it occurs after 4 minutes. Consequently, the proportion of the idle periods resulting in the
customer being automatically logged out is now
P(Gamma(k = 1.1, λ = 0.9) ≥4) = 1 −P(Gamma(k = 1.1, λ = 0.9) ≤4)
= 1 −0.967 = 0.033
which is 3.3%.

786
CHAPTER 17
RELIABILITY ANALYSIS AND LIFE TESTING
17.5
Supplementary Problems
17.5.1 (a) A set of n identical components with reliabilities
0.90 are placed in parallel. What value of n is needed
to ensure that the overall system reliability is at
least 0.995?
(b) In general, how many identical components with
reliabilities ri need to be placed in parallel to ensure
an overall system reliability of at least r?
17.5.2 Calculate the reliability of the system diagram in
Figure 17.20.
17.5.3 The germination time of a seed in days is modeled as
having a constant hazard rate of 0.31.
(a) What is the probability that a seed has not germinated
after 6 days?
(b) What is the probability that a seed germinates within
2 days?
17.5.4 The failure time of a light bulb in days is modeled as a
Weibull distribution with parameters a = 2.5 and
λ = 0.01.
(a) What is the probability that a light bulb is still
operating after 120 days?
(b) What is the probability that a light bulb fails within
50 days?
(c) What is the median failure time of a light bulb?
(d) What is the hazard rate?
(e) How much more likely to suddenly fail is a light
bulb operating after 120 days compared with a light
bulb operating after 100 days?
17.5.5 Conveyor Belt Malfunctions
The times in hours that a conveyor belt operates before a
mechanical malfunction occurs are given in DS 17.5.1.
(a) If the failure times are modeled with an exponential
distribution, construct a 95% conﬁdence interval for
the mean time to failure.
(b) Do you think that it is plausible that the mean time
to failure is one week?
17.5.6 Concrete Stress Test
The times in minutes taken by n = 25 samples of
concrete to fracture when subjected to a certain stress
are given in DS 17.5.2.
(a) If the failure times are modeled with a lognormal
distribution, estimate the parameters μ and σ.
(b) Use these point estimates to estimate the probability
that a fracture will not occur within 15 minutes.
17.5.7 Electric Motor Reliabilities
DS 17.5.3 contains a data set of failure times in days of
a certain type of electric motor. Exact failure times are
observed when the motor fails to operate correctly and
needs replacing. Right-censored observations, denoted
with an asterisk, occur when a motor is replaced while it
is still operating successfully.
(a) Construct and graph the product limit estimator of
the reliability function for the electric motors.
(b) Construct a 95% conﬁdence interval for the
probability that an electric motor operates
successfully for at least 200 days.
17.5.8 You now know much more about probability and
statistics than you did at the beginning of the book.
A. True
B. False
FIGURE 17.20
r1 = 0.90
r2 = 0.90
r3 = 0.90
r5 = 0.90
r6 = 0.90
r7 = 0.90
r8 = 0.90
r4 = 0.90

TABLES
Table I: Cumulative Distribution Function of the Standard Normal Distribution
x
Φ(x)
0
  α
z
Area α
Critical Points
α
zα
0.10
0.05
0.025
0.01
0.005
1.282
1.645
1.960
2.326
2.576
x
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
−3.4
0.0003
0.0003
0.0003
0.0003
0.0003
0.0003
0.0003
0.0003
0.0003
0.0002
−3.3
0.0005
0.0005
0.0005
0.0004
0.0004
0.0004
0.0004
0.0004
0.0004
0.0003
−3.2
0.0007
0.0007
0.0006
0.0006
0.0006
0.0006
0.0006
0.0005
0.0005
0.0005
−3.1
0.0010
0.0009
0.0009
0.0009
0.0008
0.0008
0.0008
0.0008
0.0007
0.0007
−3.0
0.0013
0.0013
0.0013
0.0012
0.0012
0.0011
0.0011
0.0011
0.0010
0.0010
−2.9
0.0019
0.0018
0.0017
0.0017
0.0016
0.0016
0.0015
0.0015
0.0014
0.0014
−2.8
0.0026
0.0025
0.0024
0.0023
0.0023
0.0022
0.0021
0.0021
0.0020
0.0019
−2.7
0.0035
0.0034
0.0033
0.0032
0.0031
0.0030
0.0029
0.0028
0.0027
0.0026
−2.6
0.0047
0.0045
0.0044
0.0043
0.0041
0.0040
0.0039
0.0038
0.0037
0.0036
−2.5
0.0062
0.0060
0.0059
0.0057
0.0055
0.0054
0.0052
0.0051
0.0049
0.0048
−2.4
0.0082
0.0080
0.0078
0.0075
0.0073
0.0071
0.0069
0.0068
0.0066
0.0061
−2.3
0.0107
0.0104
0.0102
0.0099
0.0096
0.0094
0.0091
0.0089
0.0087
0.0084
−2.2
0.0139
0.0136
0.0132
0.0129
0.0125
0.0122
0.0119
0.0116
0.0113
0.0110
−2.1
0.0179
0.0174
0.0170
0.0166
0.0162
0.0158
0.0154
0.0150
0.0146
0.0143
−2.0
0.0228
0.0222
0.0217
0.0212
0.0207
0.0202
0.0197
0.0192
0.0188
0.0183
−1.9
0.0287
0.0281
0.0274
0.0268
0.0262
0.0256
0.0250
0.0244
0.0239
0.0233
−1.8
0.0359
0.0352
0.0344
0.0336
0.0329
0.0322
0.0314
0.0307
0.0301
0.0294
−1.7
0.0446
0.0436
0.0427
0.0418
0.0409
0.0401
0.0392
0.0384
0.0375
0.0367
−1.6
0.0548
0.0537
0.0526
0.0516
0.0505
0.0495
0.0485
0.0475
0.0465
0.0455
−1.5
0.0668
0.0655
0.0643
0.0630
0.0618
0.0606
0.0594
0.0582
0.0571
0.0559
−1.4
0.0808
0.0793
0.0778
0.0764
0.0749
0.0735
0.0722
0.0708
0.0694
0.0681
−1.3
0.0968
0.0951
0.0934
0.0918
0.0901
0.0885
0.0869
0.0853
0.0838
0.0823
−1.2
0.1151
0.1131
0.1112
0.1093
0.1075
0.1056
0.1038
0.1020
0.1003
0.0985
−1.1
0.1357
0.1335
0.1314
0.1292
0.1271
0.1251
0.1230
0.1210
0.1190
0.1170
−1.0
0.1587
0.1562
0.1539
0.1515
0.1492
0.1469
0.1446
0.1423
0.1401
0.1379
−0.9
0.1841
0.1814
0.1788
0.1762
0.1736
0.1711
0.1685
0.1660
0.1635
0.1611
−0.8
0.2119
0.2090
0.2061
0.2033
0.2005
0.1977
0.1949
0.1922
0.1894
0.1867
−0.7
0.2420
0.2389
0.2358
0.2327
0.2296
0.2266
0.2236
0.2206
0.2177
0.2148
−0.6
0.2743
0.2709
0.2676
0.2643
0.2611
0.2578
0.2546
0.2514
0.2483
0.2451
−0.5
0.3085
0.3050
0.3015
0.2981
0.2946
0.2912
0.2877
0.2843
0.2810
0.2776
−0.4
0.3446
0.3409
0.3372
0.3336
0.3300
0.3264
0.3228
0.3192
0.3156
0.3121
−0.3
0.3821
0.3783
0.3745
0.3707
0.3669
0.3632
0.3594
0.3557
0.3520
0.3483
−0.2
0.4207
0.4168
0.4129
0.4090
0.4052
0.4013
0.3974
0.3936
0.3897
0.3859
−0.1
0.4602
0.4562
0.4522
0.4483
0.4443
0.4404
0.4364
0.4325
0.4286
0.4247
−0.0
0.5000
0.4960
0.4920
0.4880
0.4840
0.4801
0.4761
0.4721
0.4681
0.4641
(Continued on next page)
787

788
TABLES
Table I: (Continued)
x
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.5000
0.5040
0.5080
0.5120
0.5160
0.5199
0.5239
0.5279
0.5319
0.5339
0.5398
0.5438
0.5478
0.5517
0.5557
0.5596
0.5636
0.5675
0.5714
0.5753
0.5793
0.5832
0.5871
0.5910
0.5948
0.5987
0.6026
0.6064
0.6103
0.6141
0.6179
0.6217
0.6255
0.6293
0.6331
0.6368
0.6406
0.6443
0.6480
0.6517
0.6554
0.6591
0.6628
0.6664
0.6700
0.6736
0.6772
0.6808
0.6844
0.6879
0.6915
0.6950
0.6985
0.7019
0.7054
0.7088
0.7123
0.7157
0.7190
0.7224
0.7257
0.7291
0.7324
0.7357
0.7389
0.7422
0.7454
0.7486
0.7517
0.7549
0.7580
0.7611
0.7642
0.7673
0.7704
0.7734
0.7764
0.7794
0.7823
0.7852
0.7881
0.7910
0.7939
0.7967
0.7995
0.8023
0.8051
0.8078
0.8106
0.8133
0.8159
0.8186
0.8212
0.8238
0.8264
0.8289
0.8315
0.8340
0.8365
0.8389
0.8413
0.8438
0.8461
0.8485
0.8508
0.8531
0.8554
0.8577
0.8599
0.8621
0.8643
0.8665
0.8686
0.8708
0.8729
0.8749
0.8770
0.8790
0.8810
0.8830
0.8849
0.8869
0.8888
0.8907
0.8925
0.8944
0.8962
0.8980
0.8997
0.9015
0.9032
0.9019
0.9066
0.9082
0.9099
0.9115
0.9131
0.9147
0.9162
0.9177
0.9192
0.9207
0.9222
0.9236
0.9251
0.9265
0.9278
0.9292
0.9306
0.9319
0.9332
0.9345
0.9357
0.9370
0.9382
0.9394
0.9406
0.9418
0.9429
0.9441
0.9452
0.9463
0.9474
0.9484
0.9495
0.9505
0.9515
0.9525
0.9535
0.9545
0.9554
0.9564
0.9573
0.9582
0.9591
0.9599
0.9608
0.9610
0.9625
0.9633
0.9641
0.9649
0.9656
0.9664
0.9671
0.9678
0.9686
0.9693
0.9699
0.9706
0.9713
0.9719
0.9726
0.9732
0.9738
0.9744
0.9750
0.9756
0.9761
0.9767
0.9772
0.9778
0.9783
0.9788
0.9793
0.9798
0.9803
0.9808
0.9812
0.9817
0.9821
0.9826
0.9830
0.9834
0.9838
0.9842
0.9846
0.9850
0.9854
0.9857
0.9861
0.9864
0.9868
0.9871
0.9875
0.9878
0.9881
0.9884
0.9887
0.9890
0.9893
0.9896
0.9898
0.9901
0.9904
0.9906
0.9909
0.9911
0.9913
0.9916
0.9918
0.9920
0.9922
0.9925
0.9927
0.9929
0.9931
0.9932
0.9934
0.9936
0.9938
0.9940
0.9941
0.9943
0.9945
0.9946
0.9948
0.9949
0.9951
0.9952
0.9953
0.9955
0.9956
0.9957
0.9959
0.9960
0.9961
0.9962
0.9963
0.9964
0.9965
0.9966
0.9967
0.9968
0.9969
0.9970
0.9971
0.9972
0.9973
0.9974
0.9974
0.9975
0.9976
0.9977
0.9977
0.9978
0.9979
0.9979
0.9980
0.9981
0.9981
0.9982
0.9982
0.9983
0.9984
0.9984
0.9985
0.9985
0.9986
0.9986
0.9987
0.9987
0.9987
0.9988
0.9988
0.9989
0.9989
0.9989
0.9990
0.9990
0.9990
0.9991
0.9991
0.9991
0.9992
0.9992
0.9992
0.9992
0.9993
0.9993
0.9993
0.9993
0.9994
0.9994
0.9994
0.9994
0.9994
0.9995
0.9995
0.9995
0.9995
0.9995
0.9995
0.9996
0.9996
0.9996
0.9996
0.9996
0.9996
0.9997
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0
3.1
3.2
3.3
3.4
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9998

TABLES
789
Table II: Critical Points of the Chi-Square Distribution
χ2
α,ν
χ2
ν
Area α
Degrees of 
freedom ν
α
0.995
0.99
0.975
0.95
0.90
0.10
0.05
0.025
0.01
0.005
0.000
0.000
0.001
0.004
0.016
2.706
3.841
5.024
6.635
7.879
0.010
0.020
0.051
0.103
0.211
4.605
5.991
7.378
9.210
10.597
0.072
0.115
0.216
0.352
0.584
6.251
7.815
9.348
11.345
12.838
0.207
0.297
0.484
0.711
1.064
7.779
9.488
11.143
13.277
14.860
0.412
0.554
0.831
1.145
1.610
9.236
11.071
12.833
15.086
16.750
0.676
0.872
1.237
1.635
2.204
10.645
12.592
14.449
16.812
18.548
0.989
1.239
1.690
2.167
2.833
12.017
14.067
16.013
18.475
20.278
1.344
1.646
2.180
2.733
3.490
13.362
15.507
17.535
20.090
21.955
1.735
2.088
2.700
3.325
4.168
14.684
16.919
19.023
21.666
23.589
2.156
2.558
3.247
3.940
4.865
15.987
18.307
20.483
23.209
25.188
2.603
3.053
3.816
4.575
5.578
17.275
19.675
21.920
24.725
26.757
3.074
3.571
4.404
5.226
6.304
18.549
21.026
23.337
26.217
28.299
3.565
4.107
5.009
5.892
7.042
19.812
22.362
24.736
27.688
29.819
4.075
4.660
5.629
6.571
7.790
21.064
23.685
26.119
29.141
31.319
4.601
5.229
6.262
7.261
8.547
22.307
24.996
27.488
30.578
32.801
5.142
5.812
6.908
7.962
9.312
23.542
26.296
28.845
32.000
34.267
5.697
6.408
7.564
8.672
10.085
24.769
27.587
30.191
33.409
35.718
6.265
7.015
8.231
9.390
10.865
25.989
28.869
31.526
34.805
37.156
6.844
7.633
8.907
10.117
11.651
27.204
30.144
32.852
36.191
38.582
7.434
8.260
9.591
10.851
12.443
28.412
31.410
34.170
37.566
39.997
8.034
8.897
10.283
11.591
13.240
29.615
32.671
35.479
38.932
41.401
8.643
9.542
10.982
12.338
14.042
30.813
33.924
36.781
40.289
42.796
9.260
10.196
11.689
13.091
14.848
32.007
35.172
38.076
41.638
44.181
9.886
10.856
12.401
13.848
15.659
33.196
36.415
39.364
42.980
45.559
10.520
11.524
13.120
14.611
16.473
34.382
37.652
40.646
44.314
46.928
11.160
12.198
13.844
15.379
17.292
35.563
38.885
41.923
45.642
48.290
11.808
12.879
14.573
16.151
18.114
36.741
40.113
43.194
46.963
49.645
12.461
13.565
15.308
16.928
18.939
37.916
41.337
44.461
48.278
50.993
13.121
14.257
16.017
17.708
19.768
39.087
42.557
45.722
49.588
52.336
13.787
14.954
16.791
18.493
20.599
40.256
43.773
46.979
50.892
53.672
20.707
22.164
24.433
26.509
29.051
51.805
55.758
59.342
63.691
66.766
27.991
29.707
32.357
34.764
37.689
63.167
67.505
71.420
76.154
79.490
35.534
37.485
40.482
43.188
46.459
74.397
79.082
83.298
88.379
91.952
43.275
45.442
48.758
51.739
55.329
85.527
90.531
95.023
100.425
104.215
51.172
53.540
57.153
60.391
64.278
96.578
101.879
106.629
112.329
116.321
59.196
61.754
65.647
69.126
73.291
107.565
113.145
118.136
124.116
128.299
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
40
50
60
70
80
90
100
67.328
70.065
74.222
77.929
82.358
118.498
124.342
129.561
135.807
140.169

790
TABLES
Table III: Critical Points of the t-Distribution
Area α
tν
tα,ν
α
0.10
0.05
0.025
0.01
0.005
0.001
0.0005
3.078
6.314
12.706
31.821
63.657
318.31
636.62
1.886
2.920
4.303
6.965
9.925
22.326
31.598
1.638
2.353
3.182
4.541
5.841
10.213
12.924
1.533
2.132
2.776
3.747
4.604
7.173
8.610
1.476
2.015
2.571
3.365
4.032
5.893
6.869
1.440
1.943
2.447
3.143
3.707
5.208
5.959
1.415
1.895
2.365
2.998
3.499
4.785
5.408
1.397
1.860
2.306
2.896
3.355
4.501
5.041
1.383
1.833
2.262
2.821
3.250
4.297
4.781
1.372
1.812
2.228
2.764
3.169
4.144
4.587
1.363
1.796
2.201
2.718
3.106
4.025
4.437
1.356
1.782
2.179
2.681
3.055
3.930
4.318
1.350
1.771
2.160
2.650
3.012
3.852
4.221
1.345
1.761
2.145
2.624
2.977
3.787
4.140
1.341
1.753
2.131
2.602
2.947
3.733
4.073
1.337
1.746
2.120
2.583
2.921
3.686
4.015
1.333
1.740
2.110
2.567
2.898
3.646
3.965
1.330
1.734
2.101
2.552
2.878
3.610
3.922
1.328
1.729
2.093
2.539
2.861
3.579
3.883
1.325
1.725
2.086
2.528
2.845
3.552
3.850
1.323
1.721
2.080
2.518
2.831
3.527
3.819
1.321
1.717
2.074
2.508
2.819
3.505
3.792
1.319
1.714
2.069
2.500
2.807
3.485
3.767
1.318
1.711
2.064
2.492
2.797
3.467
3.745
1.316
1.708
2.060
2.485
2.787
3.450
3.725
1.315
1.706
2.056
2.479
2.779
3.435
3.707
1.314
1.703
2.052
2.473
2.771
3.421
3.690
1.313
1.701
2.048
2.467
2.763
3.408
3.674
1.311
1.699
2.045
2.462
2.756
3.396
3.659
1.310
1.697
2.042
2.457
2.750
3.385
3.646
1.303
1.684
2.021
2.423
2.704
3.307
3.551
1.296
1.671
2.000
2.390
2.660
3.232
3.460
1.289
1.658
1.980
2.358
2.617
3.160
3.373
Degrees of 
freedom ν
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
40
60
120
∞
1.282
1.645
1.960
2.326
2.576
3.090
3.291

TABLES
791
Table IV: Critical Points of the F-Distribution
α = 0.10
ν1
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
∞
1
39.86 49.50 53.59 55.84 57.24 58.20 58.90 59.44 59.85 60.20 60.70 61.22 61.74 62.00 62.27 62.53 62.79 63.05 63.33
2
3.53
9.00
9.16
9.24
9.29
9.33
9.35
9.37
9.38
9.39
9.41
9.42
9.44
9.45
9.46
9.47
9.47
9.48
9.49
3
5.54
5.46
5.39
5.34
5.31
5.28
5.27
5.25
5.24
5.23
5.22
5.20
5.18
5.18
5.17
5.16
5.15
5.14
5.13
4
4.54
4.32
4.19
4.11
4.05
4.01
3.98
3.95
3.94
3.92
3.90
3.87
3.84
3.83
3.82
3.80
3.79
3.78
3.76
5
4.06
3.78
3.62
3.52
3.45
3.40
3.37
3.34
3.32
3.30
3.27
3.24
3.21
3.19
3.17
3.16
3.14
3.12
3.10
6
3.78
3.46
3.29
3.18
3.11
3.05
3.01
2.98
2.96
2.94
2.90
2.87
2.84
2.82
2.80
2.78
2.76
2.74
2.72
7
3.59
3.26
3.07
2.96
2.88
2.83
2.78
2.75
2.72
2.70
2.67
2.63
2.59
2.58
2.56
2.54
2.51
2.49
2.47
8
3.46
3.11
2.92
2.81
2.73
2.67
2.62
2.59
2.56
2.54
2.50
2.46
2.42
2.40
2.38
2.36
2.34
2.32
2.29
9
3.36
3.01
2.81
2.69
2.61
2.55
2.51
2.47
2.44
2.42
2.38
2.34
2.30
2.28
2.25
2.23
2.21
2.18
2.16
10
3.28
2.92
2.73
2.61
2.52
2.46
2.41
2.38
2.35
2.32
2.28
2.24
2.20
2.18
2.16
2.13
2.11
2.08
2.06
11
3.23
2.86
2.66
2.54
2.45
2.39
2.34
2.30
2.27
2.25
2.21
2.17
2.12
2.10
2.08
2.05
2.03
2.00
1.97
12
3.18
2.81
2.61
2.48
2.39
2.33
2.28
2.24
2.21
2.19
2.15
2.10
2.06
2.04
2.01
1.99
1.96
1.93
1.90
13
3.14
2.76
2.56
2.43
2.35
2.28
2.23
2.20
2.16
2.14
2.10
2.05
2.01
1.98
1.96
1.93
1.90
1.88
1.85
14
3.10
2.73
2.52
2.39
2.31
2.24
2.19
2.15
2.12
2.10
2.05
2.01
1.96
1.94
1.91
1.89
1.86
1.83
1.80
15
3.07
2.70
2.49
2.36
2.27
2.21
2.16
2.12
2.09
2.06
2.02
1.97
1.92
1.90
1.87
1.85
1.82
1.79
1.76
16
3.05
2.67
2.46
2.33
2.24
2.18
2.13
2.09
2.06
2.03
1.99
1.94
1.89
1.87
1.84
1.81
1.78
1.75
1.72
17
3.03
2.64
2.44
2.31
2.22
2.15
2.10
2.06
2.03
2.00
1.96
1.91
1.86
1.84
1.81
1.78
1.75
1.72
1.69
18
3.01
2.62
2.42
2.29
2.20
2.13
2.08
2.04
2.00
1.98
1.93
1.89
1.84
1.81
1.78
1.75
1.72
1.69
1.66
19
2.99
2.61
2.40
2.27
2.18
2.11
2.06
2.02
1.98
1.96
1.91
1.86
1.81
1.79
1.76
1.73
1.70
1.67
1.63
20
2.97
2.59
2.38
2.25
2.16
2.09
2.04
2.00
1.96
1.94
1.89
1.84
1.79
1.77
1.74
1.71
1.68
1.64
1.61
21
2.96
2.57
2.36
2.23
2.14
2.08
2.02
1.98
1.95
1.92
1.87
1.83
1.78
1.75
1.72
1.69
1.66
1.62
1.59
22
2.95
2.56
2.35
2.22
2.13
2.06
2.01
1.97
1.93
1.90
1.86
1.81
1.76
1.73
1.70
1.67
1.64
1.60
1.57
23
2.94
2.55
2.34
2.21
2.11
2.05
1.99
1.95
1.92
1.89
1.84
1.80
1.74
1.72
1.69
1.66
1.62
1.59
1.55
24
2.93
2.54
2.33
2.19
2.10
2.04
1.98
1.94
1.91
1.88
1.83
1.78
1.73
1.70
1.67
1.64
1.61
1.57
1.51
25
2.92
2.53
2.32
2.18
2.09
2.02
1.97
1.93
1.89
1.87
1.82
1.77
1.72
1.69
1.66
1.63
1.59
1.56
1.52
26
2.91
2.52
2.31
2.17
2.08
2.01
1.96
1.92
1.88
1.86
1.81
1.76
1.71
1.68
1.65
1.61
1.58
1.54
1.50
27
2.90
2.51
2.30
2.17
2.07
2.00
1.95
1.91
1.87
1.85
1.80
1.75
1.70
1.67
1.64
1.60
1.57
1.53
1.49
28
2.89
2.50
2.29
2.16
2.06
2.00
1.94
1.90
1.87
1.84
1.79
1.74
1.69
1.66
1.63
1.59
1.56
1.52
1.48
29
2.89
2.50
2.28
2.15
2.06
1.99
1.93
1.89
1.86
1.83
1.78
1.73
1.68
1.65
1.62
1.58
1.55
1.51
1.47
30
2.88
2.49
2.28
2.14
2.05
1.98
1.93
1.88
1.85
1.82
1.77
1.72
1.67
1.64
1.61
1.57
1.54
1.50
1.46
40
2.84
2.44
2.23
2.09
2.00
1.93
1.87
1.83
1.79
1.76
1.71
1.66
1.61
1.57
1.54
1.51
1.47
1.42
1.35
60
2.79
2.39
2.18
2.04
1.95
1.87
1.82
1.77
1.74
1.71
1.66
1.60
1.54
1.51
1.48
1.44
1.40
1.35
1.29
120
2.75
2.35
2.13
1.99
1.90
1.82
1.77
1.72
1.68
1.65
1.60
1.55
1.48
1.45
1.41
1.37
1.32
1.26
1.18
∞
2.71
2.30
2.08
1.94
1.85
1.77
1.72
1.67
1.63
1.60
1.55
1.49
1.42
1.38
1.34
1.30
1.24
1.17
1.10
ν2
Fν1,ν2 ∼
χ2
ν1/
χ2ν2/
ν1
ν2
Fα,ν 1,ν 2
Area α
Fν1,ν 2
(Continued on next page)

792
TABLES
Table IV: (Continued)
α = 0.05
ν1
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
∞
1
161.44 199.50 215.69 224.57 230.16 233.98 236.78 238.89 240.55 241.89 243.91 245.97 248.02 249.04 250.07 251.13 252.18 253.27 254.31
2
18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.39 19.40 19.41 19.43 19.45 19.45 19.46 19.47 19.48 19.49 19.50
3
10.13
9.55
9.28
9.12
9.01
8.94
8.89
8.85
8.81
8.79
8.74
8.70
8.66
8.64
8.62
8.59
8.57
8.55
8.53
4
7.71
6.94
6.59
6.39
6.20
6.16
6.09
6.04
6.00
5.96
5.91
5.86
5.80
5.77
5.75
5.72
5.69
5.66
5.63
5
6.61
5.79
5.41
5.19
5.05
4.95
4.88
4.82
4.77
4.74
4.68
4.62
4.56
4.53
4.50
4.46
4.43
4.40
4.36
6
5.99
5.14
4.76
4.53
4.39
4.28
4.21
4.15
4.10
4.06
4.00
3.94
3.87
3.84
3.81
3.77
3.74
3.70
3.67
7
5.59
4.74
4.35
4.12
3.97
3.87
3.79
3.73
3.68
3.64
3.57
3.51
3.44
3.41
3.38
3.34
3.30
3.27
3.23
8
5.52
4.46
4.07
3.84
3.69
3.58
3.50
3.44
3.39
3.35
3.28
3.22
3.15
3.12
3.08
3.04
3.01
2.97
2.93
9
5.12
4.26
3.86
3.63
3.48
3.37
3.29
3.23
3.18
3.14
3.07
3.01
2.94
2.90
2.86
2.83
2.79
2.75
2.71
10
4.96
4.10
3.71
3.48
3.33
3.22
3.14
3.07
3.02
2.98
2.91
2.85
2.77
2.74
2.70
2.66
2.62
2.58
2.54
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
2.85
2.79
2.72
2.65
2.61
2.57
2.53
2.49
2.45
2.40
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
2.75
2.69
2.62
2.54
2.51
2.47
2.43
2.38
2.34
2.30
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
2.67
2.60
2.53
2.46
2.42
2.38
2.34
2.30
2.25
2.21
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
2.60
2.53
2.46
2.39
2.35
2.31
2.27
2.22
2.18
2.13
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
2.54
2.48
2.40
2.33
2.29
2.25
2.20
2.16
2.11
2.07
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
2.49
2.42
2.35
2.28
2.24
2.19
2.15
2.11
2.06
2.01
17
4.45
3.59
3.20
2.96
2.81
2.70
2.61
2.55
2.49
2.45
2.38
2.31
2.23
2.19
2.15
2.10
2.06
2.01
1.96
18
4.41
3.55
3.16
2.93
2.77
2.66
2.58
2.51
2.46
2.41
2.34
2.27
2.19
2.15
2.11
2.06
2.02
1.97
1.92
19
4.38
3.52
3.13
2.90
3.74
2.63
2.54
2.48
2.42
2.38
2.31
2.23
2.16
2.11
2.07
2.03
1.98
1.93
1.88
20
4.35
3.49
3.10
2.87
2.71
2.60
2.51
2.45
2.30
2.35
2.28
2.20
2.12
2.08
2.04
1.99
1.95
1.90
1.84
21
4.32
3.47
3.07
2.84
2.68
2.57
2.49
2.42
2.37
2.32
2.25
2.18
2.10
2.05
2.01
1.96
1.92
1.87
1.81
22
4.30
3.44
3.05
2.82
2.66
2.55
2.46
2.40
2.34
2.30
2.23
2.15
2.07
2.03
1.98
1.94
1.89
1.84
1.78
23
4.28
3.42
3.03
2.80
2.64
2.53
2.44
2.37
2.32
2.27
2.20
2.13
2.05
2.01
1.96
1.91
1.86
1.81
1.76
24
4.26
3.40
3.01
2.78
2.62
2.51
2.42
2.36
2.30
2.25
2.18
2.11
2.03
1.98
1.94
1.89
1.84
1.79
1.73
25
4.24
3.39
2.99
2.76
2.60
2.49
2.40
2.34
2.28
2.24
2.16
2.09
2.01
1.96
1.92
1.87
1.82
1.77
1.71
26
4.23
3.37
2.98
2.74
2.59
2.47
2.39
2.32
2.27
2.22
2.15
2.07
1.99
1.95
1.90
1.85
1.80
1.75
1.69
27
4.21
3.35
2.96
2.73
2.57
2.46
2.37
2.31
2.25
2.20
2.13
2.06
1.97
1.93
1.88
1.84
1.79
1.73
1.67
28
4.20
3.34
2.95
2.71
2.56
2.45
2.36
2.29
2.24
2.19
2.12
2.04
1.96
1.91
1.87
1.82
1.77
1.71
1.65
29
4.18
3.33
2.93
3.70
2.55
2.43
2.35
2.28
2.22
2.18
2.10
2.03
1.94
1.90
1.85
1.81
1.75
1.70
1.64
30
4.17
3.32
2.92
2.69
2.53
2.42
2.33
2.27
2.21
2.16
2.09
2.01
1.93
1.89
1.84
1.79
1.74
1.68
1.62
40
4.08
3.23
2.84
2.61
2.45
2.34
2.25
2.18
2.12
2.08
2.00
1.92
1.84
1.79
1.74
1.69
1.64
1.58
1.51
60
4.00
3.15
2.76
2.53
2.37
2.25
2.17
2.10
2.04
1.99
1.92
1.84
1.75
1.70
1.65
1.59
1.53
1.47
1.39
120
3.92
3.09
2.68
2.45
2.29
2.18
2.09
2.02
1.96
1.91
1.83
1.75
1.66
1.61
1.55
1.50
1.43
1.35
1.25
∞
3.84
3.00
2.60
2.37
2.21
2.10
2.01
1.94
1.88
1.83
1.75
1.67
1.57
1.52
1.46
1.39
1.32
1.22
1.00
ν2
(Continued on next page)

TABLES
793
Table IV: (Continued)
α = 0.01
ν1
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
∞
1
4052
4999
5403
5625
5764
5859
5929
5981
6023
6055
6107
6157
6209
6235
6260
6287
6312
6339
6366
2
98.51 99.00 99.17 99.25 99.30 99.33 99.35
99.39 99.40 99.41 99.43 99.44 99.45 99.47 99.47 99.48 99.49 99.50
3
34.12 30.82 29.46 28.71 28.24 27.91 27.67
27.35 27.23 27.05 26.87 26.69 26.60 26.51 26.41 26.32 26.22 26.13
4
21.20 18.00 16.69 15.98 15.52 15.21 14.98
14.66 14.55 14.37 14.20 14.02 13.93 13.84 13.75 13.65 13.56 13.46
5
16.26 13.27 12.06 11.39 10.97 10.67 10.46
10.16 10.05
9.89
9.72
9.55
9.47
9.38
9.29
9.20
9.11
9.02
6
13.75 10.92
9.78
9.15
8.75
8.47
8.26
7.98
7.87
7.72
7.56
7.40
7.31
7.23
7.14
7.06
6.97
6.88
7
12.25
9.55
8.45
7.85
7.46
7.19
6.99
6.72
6.62
6.47
6.31
6.16
6.07
5.99
5.91
5.82
5.74
5.65
8
11.26
8.65
7.59
7.01
6.63
6.37
6.18
5.91
5.81
5.67
5.52
5.36
5.28
5.20
5.12
5.03
4.95
4.86
9
10.56
8.02
6.99
6.42
6.06
5.80
5.61
5.35
5.26
5.11
4.96
4.81
4.73
4.65
4.57
4.48
4.40
4.31
10
10.04
7.56
6.55
5.99
5.64
5.39
5.20
4.94
4.85
4.71
4.56
4.41
4.33
4.25
4.17
4.08
4.00
3.91
11
9.65
7.21
6.22
5.67
5.32
5.07
4.89
4.63
4.54
4.40
4.25
4.10
4.02
3.94
3.86
3.78
3.69
3.60
12
9.33
6.93
5.95
5.41
5.06
4.82
4.64
4.39
4.30
4.16
4.01
3.86
3.78
3.70
3.62
3.54
3.45
3.36
13
9.07
6.70
5.74
5.21
4.86
4.62
4.44
4.19
4.10
3.96
3.82
3.66
3.59
3.51
3.43
3.34
3.25
3.17
14
8.86
6.51
5.56
5.04
4.69
4.46
4.28
4.03
3.94
3.80
3.66
3.51
3.43
3.35
3.27
3.18
3.09
3.00
15
8.68
6.36
5.42
4.89
4.56
4.32
4.14
3.89
3.80
3.67
3.52
3.37
3.29
3.21
3.13
3.05
2.96
2.87
16
8.53
6.23
5.29
4.77
4.44
4.20
4.03
3.78
3.69
3.55
3.41
3.26
3.18
3.10
3.02
2.93
2.84
2.75
17
8.40
6.11
5.18
4.67
4.34
4.10
3.93
3.68
3.59
3.46
3.31
3.16
3.08
3.00
2.92
2.83
2.75
2.65
18
8.29
6.01
5.09
4.58
4.25
4.01
3.84
3.60
3.51
3.37
3.23
3.08
3.00
2.92
2.84
2.75
2.66
2.57
19
8.19
5.93
5.01
4.50
4.17
3.94
3.77
3.52
3.43
3.30
3.15
3.00
2.92
2.84
2.76
2.67
2.58
2.49
20
8.10
5.85
4.94
4.43
4.10
3.87
3.70
3.46
3.37
3.23
3.09
2.94
2.86
2.78
2.69
2.61
2.52
2.42
21
8.02
5.78
4.87
4.37
4.04
3.81
3.64
3.40
3.31
3.17
3.03
2.88
2.80
2.72
2.64
2.55
2.46
2.36
22
7.95
5.72
4.82
4.31
3.99
3.76
3.59
3.35
3.26
3.12
2.98
2.83
2.75
2.67
2.58
2.50
2.40
2.31
23
7.88
5.66
4.76
4.26
3.94
3.71
3.54
3.30
3.21
3.07
2.93
2.78
2.70
2.62
2.54
2.45
2.35
2.26
24
7.82
5.61
4.72
4.22
3.90
3.67
3.50
3.26
3.17
3.03
2.89
2.74
2.66
2.58
2.49
2.40
2.31
2.21
25
7.77
5.57
4.68
4.18
3.85
3.63
3.46
3.22
3.13
2.99
2.85
2.70
2.62
2.54
2.45
2.36
2.27
2.17
26
7.72
5.53
4.64
4.14
3.82
3.59
3.42
3.18
3.09
2.96
2.81
2.66
2.58
2.50
2.42
2.33
2.23
2.13
27
7.68
5.49
4.60
4.11
3.78
3.56
3.39
3.15
3.06
2.93
2.78
2.63
2.55
2.47
2.38
2.29
2.20
2.10
28
7.64
5.45
4.57
4.07
3.75
3.53
3.36
3.12
3.03
2.90
2.75
2.60
2.52
2.44
2.35
2.26
2.17
2.06
29
7.60
5.42
4.54
4.04
3.73
3.50
3.33
3.09
3.00
2.87
2.73
2.57
2.49
2.41
2.33
2.23
2.14
2.03
30
7.56
5.39
4.51
4.02
3.70
3.47
3.30
3.07
2.98
2.84
2.70
2.55
2.47
2.39
3.30
2.31
2.11
2.01
40
7.31
5.18
4.31
3.83
3.51
3.29
3.12
2.89
2.80
2.66
2.52
2.37
2.29
2.20
2.11
2.02
1.92
1.80
60
7.08
4.98
4.13
3.65
3.34
3.12
2.95
2.72
2.63
2.50
2.35
2.20
2.12
2.03
1.94
1.84
1.73
1.60
120
6.85
4.79
3.95
3.48
3.17
2.96
2.79
2.56
2.47
2.34
2.19
2.03
1.95
1.86
1.76
1.66
1.53
1.38
∞
6.63
4.61
3.78
3.32
3.02
2.80
2.64
99.38
27.49
14.80
10.29
8.10
6.84
6.03
5.47
5.06
4.74
4.50
4.30
4.14
4.00
3.89
3.79
3.71
3.63
3.56
3.51
3.45
3.41
3.36
3.32
3.29
3.26
3.23
3.20
3.17
2.99
2.82
2.66
2.51
2.41
2.32
2.18
2.04
1.88
1.79
1.70
1.59
1.47
1.32
1.00
ν2

Table V: Critical Points qα,k,ν of the Studentized Range Distribution
α = 0.05
k
Degrees of 
freedom ν
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
3.64 4.60 5.22 5.67 6.03 6.33 6.58 6.80 6.99 7.17 7.32 7.47 7.60 7.72 7.83 7.93 8.03 8.12 8.21
3.46 4.34 4.90 5.30 5.63 5.90 6.12 6.32 6.49 6.65 6.79 6.92 7.03 7.14 7.24 7.34 7.43 7.51 7.59
3.34 4.16 4.68 5.06 5.36 5.61 5.82 6.00 6.16 6.30 6.43 6.55 6.66 6.76 6.85 6.94 7.02 7.10 7.17
3.26 4.04 4.53 4.89 5.17 5.40 5.60 5.77 5.92 6.05 6.18 6.29 6.39 6.48 6.57 6.65 6.73 6.80 6.87
3.20 3.95 4.41 4.76 5.02 5.24 5.43 5.59 5.74 5.87 5.98 6.09 6.19 6.28 6.36 6.44 6.51 6.58 6.64
3.15 3.88 4.33 4.65 4.91 5.12 5.30 5.46 5.60 5.72 5.83 5.93 6.03 6.11 6.19 6.27 6.34 6.40 6.47
3.11 3.82 4.26 4.57 4.82 5.03 5.20 5.35 5.49 5.61 5.71 5.81 5.90 5.98 6.06 6.13 6.20 6.27 6.33
3.08 3.77 4.20 4.51 4.75 4.95 5.12 5.27 5.39 5.51 5.61 5.71 5.80 5.88 5.95 6.02 6.09 6.15 6.21
3.06 3.73 4.15 4.45 4.69 4.88 5.05 5.19 5.32 5.43 5.53 5.63 5.71 5.79 5.86 5.93 5.99 6.05 6.11
3.03 3.70 4.11 4.41 4.64 4.83 4.99 5.13 5.25 5.36 5.46 5.55 5.64 5.71 5.79 5.85 5.91 5.97 6.03
3.01 3.67 4.08 4.37 4.59 4.78 4.94 5.08 5.20 5.31 5.40 5.49 5.57 5.65 5.72 5.78 5.85 5.90 5.96
3.00 3.65 4.05 4.33 4.56 4.74 4.90 5.03 5.15 5.26 5.35 5.44 5.52 5.59 5.66 5.73 5.79 5.84 5.90
2.98 3.63 4.02 4.30 4.52 4.70 4.86 4.99 5.11 5.21 5.31 5.39 5.47 5.54 5.61 5.67 5.73 5.79 5.84
2.97 3.61 4.00 4.28 4.49 4.67 4.82 4.96 5.07 5.17 5.27 5.35 5.43 5.50 5.57 5.63 5.69 5.74 5.79
2.96 3.59 3.98 4.25 4.47 4.65 4.79 4.92 5.04 5.14 5.23 5.31 5.39 5.46 5.53 5.59 5.65 5.70 5.75
2.95 3.58 3.96 4.23 4.45 4.62 4.77 4.90 5.01 5.11 5.20 5.26 5.36 5.43 4.49 5.55 5.61 5.66 5.71
2.92 3.53 3.90 4.17 4.37 4.54 4.68 4.81 4.92 5.01 5.10 5.16 5.25 5.32 5.38 5.44 5.49 5.55 5.59
2.89 3.49 3.85 4.10 4.30 4.46 4.60 4.72 4.82 4.92 5.00 5.08 5.15 5.21 5.27 5.33 5.38 5.43 5.47
2.86 3.44 3.79 4.04 4.23 4.39 4.52 4.63 4.73 4.82 4.90 4.98 5.04 5.11 5.16 5.22 5.27 5.31 5.36
2.83 3.40 3.74 3.98 4.16 4.31 4.44 4.55 4.65 4.73 4.81 4.88 4.94 5.00 5.06 5.11 5.15 5.20 5.24
2.80 3.36 3.68 3.92 4.10 4.24 4.36 4.47 4.56 4.64 4.71 4.78 4.84 4.90 4.95 5.00 5.04 5.09 5.13
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
24
30
40
60
120
∞
2.77 3.31 3.63 3.86 4.03 4.17 4.29 4.39 4.47 4.55 4.62 4.68 4.74 4.80 4.85 4.89 4.93 4.97 5.01
Degrees of 
freedom ν 
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
24
30
40
60
120
∞
2
5.70
5.24
4.95
4.74
4.60
4.48
4.39
4.32
4.26
4.21
4.17
4.13
4.10
4.07
4.05
4.02
3.96
3.89
3.82
3.76
3.70
3.64
3
6.97
6.33
5.92
5.63
5.43
5.27
5.14
5.04
4.96
4.89
4.83
4.78
4.74
4.70
4.67
4.64
4.54
4.45
4.37
4.28
4.20
4.12
4
7.80
7.03
6.54
6.20
5.96
5.77
5.62
5.50
5.40
5.32
5.25
5.19
5.14
5.09
5.05
5.02
4.91
4.80
4.70
4.60
4.50
4.40
5
8.42
7.56
7.01
6.63
6.35
6.14
5.97
5.84
5.73
5.63
5.56
5.49
5.43
5.38
5.33
5.29
5.17
5.05
4.93
4.82
4.71
4.60
6
8.91
7.97
7.37
6.96
6.66
6.43
6.25
6.10
5.98
5.88
5.80
5.72
5.66
5.60
5.55
5.51
5.37
5.24
5.11
4.99
4.87
4.76
7
9.32
8.32
7.68
7.24
6.91
6.67
6.48
6.32
6.19
6.08
5.99
5.92
5.85
5.79
5.73
5.69
5.54
5.40
5.27
5.13
5.01
4.88
8
9.67
8.61
7.94
7.47
7.13
6.87
6.67
6.51
6.37
6.26
6.16
6.08
6.01
5.94
5.89
5.84
5.69
5.54
5.39
5.25
5.12
4.99
9
9.97
8.87
8.17
7.68
7.32
7.05
6.84
6.67
6.53
6.41
6.31
6.22
6.15
6.08
6.02
5.97
5.81
5.65
5.50
5.36
5.21
5.08
10
10.2
9.10
8.37
7.87
7.49
7.21
6.99
6.81
6.67
6.54
6.44
6.35
6.27
6.20
6.14
6.09
5.92
5.76
5.60
5.45
5.30
5.16
11
10.5
9.30
8.55
8.03
7.65
7.36
7.13
6.94
6.79
6.66
6.55
6.46
6.38
6.31
6.25
6.19
6.02
5.85
5.69
5.53
5.38
5.23
12
10.7
9.49
8.71
8.18
7.78
7.48
7.25
7.06
6.90
6.77
6.66
6.56
6.48
6.41
6.34
6.29
6.11
5.93
5.77
5.60
5.44
5.29
13
10.9
9.65
8.86
8.31
7.91
7.60
7.36
7.17
7.01
6.87
6.76
6.66
6.57
6.50
6.43
6.37
6.19
6.01
5.84
5.67
5.51
5.35
14
11.1
9.81
9.00
8.44
8.03
7.71
7.46
7.26
7.10
6.96
6.84
6.74
6.66
6.58
6.51
6.45
6.26
6.08
5.90
5.73
5.56
5.40
15
11.2
9.95
9.12
8.55
8.13
7.81
7.56
7.36
7.19
7.05
6.93
6.82
6.73
6.65
6.58
6.52
6.33
6.14
5.96
5.79
5.61
5.45
16
11.4
10.1
9.24
8.66
8.23
7.91
7.65
7.44
7.27
7.12
7.00
6.90
6.80
6.72
6.65
6.59
6.39
6.20
6.02
5.84
5.66
5.49
17
11.6
10.2
9.35
8.76
8.32
7.99
7.73
7.52
7.34
7.20
7.07
6.97
6.87
6.79
6.72
6.65
6.45
6.26
6.07
5.89
5.71
5.54
18
11.7
10.3
9.46
8.85
8.41
8.07
7.81
7.59
7.42
7.27
7.14
7.03
6.94
6.85
6.78
6.71
6.51
6.31
6.12
5.93
5.75
5.57
19
11.8
10.4
9.55
8.94
8.49
8.15
7.88
7.66
7.48
7.33
7.20
7.09
7.00
6.91
6.84
6.76
6.56
6.36
6.17
5.98
5.79
5.61
20
11.9
10.5
9.65
9.03
8.57
8.22
7.95
7.73
7.55
7.39
7.26
7.15
7.05
6.96
6.89
6.82
6.61
6.41
6.21
6.02
5.83
5.65
α = 0.01
k
 
794

TABLES
795
Table VI: Critical Points dα,n for the Kolmogorov
and Kolmogorov-Smirnov Procedures
n
α = 0.20
α = 0.10
α = 0.05
α = 0.02
α = 0.01
0.323
0.369
0.409
0.457
0.489
0.308
0.352
0.391
0.437
0.468
0.296
0.338
0.375
0.419
0.449
0.285
0.325
0.361
0.404
0.432
0.275
0.314
0.349
0.390
0.418
0.268
0.304
0.338
0.377
0.404
0.258
0.295
0.327
0.366
0.392
0.250
0.286
0.318
0.355
0.381
0.244
0.279
0.309
0.346
0.371
0.237
0.271
0.301
0.337
0.361
0.232
0.265
0.294
0.329
0.362
0.226
0.259
0.287
0.321
0.344
0.221
0.253
0.281
0.314
0.337
0.216
0.247
0.275
0.307
0.330
0.212
0.242
0.269
0.301
0.323
0.208
0.238
0.264
0.295
0.317
0.204
0.233
0.259
0.290
0.311
0.200
0.229
0.254
0.284
0.305
0.197
0.226
0.250
0.279
0.300
0.193
0.221
0.246
0.275
0.295
0.190
0.218
0.242
0.270
0.290
0.187
0.214
0.238
0.266
0.285
0.184
0.211
0.234
0.262
0.281
0.182
0.208
0.231
0.258
0.277
0.179
0.205
0.227
0.254
0.273
0.177
0.202
0.224
0.251
0.269
0.174
0.199
0.221
0.247
0.265
0.172
0.196
0.218
0.244
0.262
0.170
0.194
0.215
0.241
0.258
0.168
0.191
0.213
0.238
0.255
0.165
0.189
0.210
0.235
0.252
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
Approximation
for n > 40:
1.36
√
n
1.22
√
n
1.63
√
n
1.52
√
n
1.07
√
n

ANSWERS TO ODD-NUMBERED PROBLEMS
Chapter 1
Probability Theory
1.1
Probabilities
1.1.1
S = {(head, head, head), (head, head, tail), (head, tail, head),
(head, tail, tail), (tail, head, head), (tail, head, tail), (tail, tail,
head), (tail, tail, tail)}
1.1.3
S = {0,1,2,3,4}
1.1.5
S = {(on time, satisfactory), (on time, unsatisfactory), (late,
satisfactory), (late, unsatisfactory)}
1.1.7
(a)
p = 0.5
(b)
p = 2
3
(c)
1
3
1.1.9
0 ≤P(V ) ≤0.39
P(I V ) = P(V ) = 0.195
1.1.11
0.17
1.2
Events
1.2.1
(a)
P(b) = 0.15
(b)
P(A) = 0.50
(c)
P(A′) = 0.50
1.2.3
124
1461 and
113
1461
1.2.5
0.35
0.89
1.2.7
P(♠or ♣) = 1
2
1.2.9
(a)
P(Terica is winner) = 1
4
(b)
P(Terica is winner or runner up) = 1
2
1.2.11
(a)
P(both assembly lines are shut down) = 0.02
(b)
P(neither assembly line is shut down) = 0.74
(c)
P(at least one assembly line is at full capacity) = 0.71
(d)
P(exactly one assembly line at full capacity) = 0.52
The complement of “neither assembly line is shut down” consists
of the outcomes
{(S, S), (S, P), (S, F), (P, S), (F, S)}.
The complement of “at least one assembly line is at full
capacity” consists of the outcomes
{(S, S), (S, P), (P, S), (P, P)}.
1.2.13
0.73
1.3
Combinations of Events
1.3.1
The event A contains the outcome 0 while the empty set does not
contain any outcomes.
1.3.5
Yes
No
1.3.7
P(B) = 0.4
1.3.9
Yes
P(A ∪B ∪C) = 3
4
1.3.11
P(O′ ∩S′) = 0.11
1.3.13
0.19
1.3.15
0.72
0.90
1.4
Conditional Probability
1.4.1
(a)
P(A | B) = 0.1739
(b)
P(C | A) = 0.59375
(c)
P(B | A ∩B) = 1
(d)
P(B | A ∪B) = 0.657
(e)
P(A | A ∪B ∪C) = 0.3636
(f)
P(A ∩B | A ∪B) = 0.1143
1.4.3
(a)
P(A♥| red suit) =
1
26
(b)
P(heart | red suit) = 1
2
(c)
P(red suit | heart) = 1
(d)
P(heart | black suit) = 0
(e)
P(King | red suit) =
1
13
(f)
P(King | red picture card) = 1
3
1.4.5
P(shiny | red) = 3
8
P(dull | red) = 5
8
1.4.9
(a) 0.512
(b) 0.619
(c) 0
(d) 0.081
1.4.11
(a) 0.9326
(b) 0.9756
1.4.13
(a) 0.02
(b) 0.775
1.4.15
P(delay) = 0.3568
1.4.17
0.3562
796

ANSWERS TO ODD-NUMBERED PROBLEMS
797
1.5
Probabilities of Event Intersections
1.5.1
(a)
P(both cards are picture cards) =
132
2652
(b)
P(both cards are from red suits) =
650
2652
(c)
P(one card is from a red suit and one is from
black suit) = 26
51
1.5.3
(a) No, they are not independent.
(b) Yes, they are independent.
(c) No, they are not independent.
(d) Yes, they are independent.
(e) No, they are not independent.
1.5.5
P(all 4 cards are hearts) =
1
256
P(all 4 cards are from red suits) =
1
16
P(all 4 cards from different suits) =
3
32
1.5.7
P(message gets through the network) = 0.98096
1.5.9
P(no broken bulbs) = 0.5682
P(no more than one broken bulb in the sample) = 0.9260
1.5.11
P(drawing 2 green balls) = 0.180
P(two balls different colors) = 0.656
1.5.13
p = 0.5 (a fair coin)
1.5.15
(a)
1
32
(b)
5
9
(c)
3
8
(d)
13
34
1.5.19
0.5329
1.6
Posterior Probabilities
1.6.1
(a)
P(positive blood test) = 0.0691
(b)
P(disease | positive blood test) = 0.1404
(c)
P(no disease | negative blood test) = 0.9997
1.6.3
(a)
P(Section I) =
55
100
(b)
P(grade is A) =
21
100
(c)
P(A | Section I) = 10
55
(d)
P(Section I | A) = 10
21
1.6.5
(a) 0.4579
(b)
P(A | did not fail) = 0.7932
1.6.7
(a)
P(H | L) == 0.224
(b)
P(M | L′) = 0.551
1.6.9
0.5711
0.0508
1.7
Counting Techniques
1.7.1
(a) 7! = 5040
(b) 8! = 40320
(c) 4! = 24
(d) 13! = 6, 227, 020, 800
1.7.3
(a) C6
2 == 15
(b) C8
4 = 70
(c) C5
2 = 10
(d) C14
6 = 3003
1.7.5
24
1.7.7
1,860,480
15504
1.7.13
48
72
1.7.15
(a) 27720
1.7.17
168,168,000
1.7.19
3,628,800
252
1.7.21
(a) 0.082
(b) 0.049
1.7.23
125
Chapter 2
Random Variables
2.1
Discrete Random Variables
2.1.1
(a)
P(X = 4) = 0.21
(c)
F(0) = 0.08
F(1) = 0.19
F(2) = 0.46
F(3) = 0.79
F(4) = 1.00
2.1.3
xi
1
2
3
4
5
6
8
9
10
pi
1
36
2
36
2
36
3
36
2
36
4
36
2
36
1
36
2
36
F(xi)
1
36
3
36
5
36
8
36
10
36
14
36
16
36
17
36
19
36

798
ANSWERS TO ODD-NUMBERED PROBLEMS
xi
12
15
16
18
20
24
25
30
36
pi
4
36
2
36
1
36
2
36
2
36
2
36
1
36
2
36
1
36
F(xi)
23
36
25
36
26
36
28
36
30
36
32
36
33
36
35
36
1
2.1.5
xi
−5
−4
−3
−2
−1
0
1
2
3
4
6
8
10
12
pi
1
36
1
36
2
36
2
36
3
36
3
36
2
36
5
36
1
36
4
36
3
36
3
36
3
36
3
36
F(xi)
1
36
2
36
4
36
6
36
9
36
12
36
14
36
19
36
20
36
24
36
27
36
30
36
33
36
1
2.1.7
(a)
xi
0
1
2
3
4
6
8
12
pi
0.061
0.013
0.195
0.067
0.298
0.124
0.102
0.140
(b)
xi
0
1
2
3
4
6
8
12
F(xi)
0.061
0.074
0.269
0.336
0.634
0.758
0.860
1.000
(c) The most likely value is 4.
P(not shipped) = 0.074
2.1.9
xi
1
2
3
4
pi
2
5
3
10
1
5
1
10
F(xi)
2
5
7
10
9
10
1
2.1.11
(a) The state space is {3, 4, 5, 6}.
(b)
P(X = 3) =
1
20
P(X = 4) =
3
20
P(X = 5) =
6
20
P(X = 6) = 1
2
P(X ≤3) =
1
20
P(X ≤4) =
4
20
P(X ≤5) = 10
20
P(X ≤6) = 1
2.2
Continuous Random Variables
2.2.1
(a) Continuous
(b) Discrete
(c) Continuous
(d) Continuous
(e) Discrete
(f) This depends on what level of accuracy to which it is mea-
sured.
It could be considered to be either discrete or continuous.
2.2.3
(a) c = −1
8
(b)
P(−1 ≤X ≤1) =
69
128
(c)
F(x) =
x2
128 + 15x
64 +
7
16
for −2 ≤x ≤0
F(x) = −x2
16 + 3x
8 +
7
16
for 0 ≤x ≤3
2.2.5
(a)
A = 1
B = −1
(b)
P(2 ≤X ≤3) = 0.0855
(c)
f (x) = e−x
for x ≥0
2.2.7
(a)
A = −0.25
B = 0.361
(b)
P(X > 2) = 0.5
(c)
f (x) =
1.08
3x+2
for 0 ≤x ≤10
2.2.9
(a)
A = 1.0007
B = −125.09
(b)
P(X ≤10) = 0.964
(c)
P(X ≥30) = 0.002
(d)
f (r) =
375.3
(r+5)4
for 0 ≤r ≤50
2.2.11
(a)
A =
4
819
(b)
F(x) =
4
819

65x2 −x4
4 −4000
for 10 ≤x ≤11
(c) 0.283

ANSWERS TO ODD-NUMBERED PROBLEMS
799
2.3
The Expectation of a Random Variable
2.3.1
E(X) = 2.48
2.3.3
With replacement:
E(X) = 0.5
Without replacement:
E(X) = 0.5
2.3.5
E(X) = $8.77
2.3.7
E(net winnings) = $1.31
2.3.9
E(winnings) = $0.09
2.3.11
(a)
E(X) = 2.67
(b) 2.83
2.3.13
E(X) = 0.9977
0.6927
2.3.17
(a)
E(X) = 10.418234
(b) 10.385
2.3.19
0.83
2.4
The Variance of a Random Variable
2.4.1
(a)
E(X) = 11
6
(b) Var(X) = 341
36
2.4.3
σ = 1
2.4.5
(a) Var(X) = 0.3325
(b) σ = 0.5766
(c) 4.43
5.42
(d) 0.99
2.4.7
(a) Var(X) = 0.0115
(b) σ = 0.107
(c) 0.217
0.401
(d) 0.184
2.4.9
(a) Var(X) = 18.80 −2.442 = 12.8
(b) σ = 3.58
(c) 0.50
2.93
(d) 2.43
2.4.11
P(109.55 ≤X ≤112.05) ≥0.84
2.4.13
(a) 0.275
(b) 10.69
10.07
2.4.15
E(X) = $1.55
The standard deviation is $1.96.
2.4.17
(a)
E(X) = 3.74
(b) The standard deviation is 0.844.
2.5
Jointly Distributed Random Variables
2.5.3
(a)
A = −1
125
(b)
P(0 ≤X ≤1, 4 ≤Y ≤5) =
9
100
(c)
fX(x) = 2(3−x)
25
for −2 ≤x ≤3
fY (y) =
y
10
for 4 ≤x ≤6
(d) The random variables X and Y are independent.
(e)
fX|Y=5(x) is equal to fX(x)
2.5.5
(a)
A = 0.00896
(b)
P(1.5 ≤X ≤2, 1 ≤Y ≤2) = 0.158
(c)
fX(x) = 0.00896 (ex+3 −e2x−3 −ex + e2x)
for 1 ≤x ≤2
fY (y) = 0.00896 (e2+y + 0.5e4−y −e1+y −0.5e2−y)
for 0 ≤y ≤3
(d) No, the random variables X and Y are not independent.
(e)
fX|Y=0(x) = ex +e2x
28.28
2.5.7
(a)
X\Y
0
1
2
pi+
0
4
16
4
16
1
16
9
16
1
4
16
2
16
0
6
16
2
1
16
0
0
1
16
p+ j
9
16
6
16
1
16
1
(b) See the table above.
(c) No, the random variables X and Y are not independent.
(d)
E(X) = 1
2
Var(X) = 0.375
The random variable Y has the same mean and variance as X.
(e) Cov(X, Y) = −1
8
(f) Corr(X, Y) = −1
3
(g)
P(Y = 0|X = 0) = 4
9
P(Y = 1|X = 0) = 4
9
P(Y = 2|X = 0) = 1
9
P(Y = 0|X = 1) = 2
3
P(Y = 1|X = 1) = 1
3
P(Y = 2|X = 1) = 0
2.5.9
(a)
P(same score) = 0.80
(b)
P(X < Y) = 0.07
(c)
xi
1
2
3
4
pi+
0.12
0.20
0.30
0.38
E(X) = 2.94
Var(X) = 1.0564
(d)
y j
1
2
3
4
p+ j
0.14
0.21
0.30
0.35
E(Y) = 2.86
Var(Y) = 1.1004
(e) The scores are not independent.

800
ANSWERS TO ODD-NUMBERED PROBLEMS
(f)
P(Y = 1|X = 3) =
1
30
P(Y = 2|X = 3) =
3
30
P(Y = 3|X = 3) = 24
30
P(Y = 4|X = 3) =
2
30
(g) Cov(X, Y) = 0.8816
(h) Corr(X, Y) = 0.82
2.6
Combinations and Functions of Random Variables
2.6.1
(a)
E(3X + 7) = 13
Var(3X + 7) = 36
(b)
E(5X −9) = 1
Var(5X −9) = 100
(c)
E(2X + 6Y) = −14
Var(2X + 6Y) = 88
(d)
E(4X −3Y) = 17
Var(4X −3Y) = 82
(e)
E(5X −9Z + 8) = −54
Var(5X −9Z + 8) = 667
(f)
E(−3Y −Z −5) = −4
Var(−3Y −Z −5) = 25
(g)
E(X + 2Y + 3Z) = 20
Var(X + 2Y + 3Z) = 75
(h)
E(6X + 2Y −Z + 16) = 14
Var(6X + 2Y −Z + 16) = 159
2.6.3
E(Y) = 3μ
Var(Y) = 9σ 2
E(Z) = 3μ
Var(Z) = 3σ 2
2.6.5
$18.75
$25.72
2.6.7
10
13
120
169
10
13
2.6.9
(a)
A = 3
4
(b)
fV (v) = 1
2( 3
4π )2/3v−1/3 −
3
16π
for 0 ≤v ≤32π
3 .
(c)
E(V ) = 32π
15
2.6.11
(a) The return has an expectation of $100, a standard deviation
of $20, and a variance of 400.
(b) The return has an expectation of $100, a standard deviation
of $30, and a variance of 900.
(c) The total return has an expectation of $100 and a variance of
325, so that the standard deviation is $18.03.
(d) x = $692
2.6.13
(a) The mean is 66.
The standard deviation is 5.63.
(b) The mean is 64.8.
The standard deviation is 6.61.
2.6.15
(a) The mean is μ = 65.90.
The standard deviation is 0.143.
(b) The mean is 527.2.
The standard deviation is 0.905.
2.6.17
The mean is 280.
The standard deviation is 15.3.
2.6.19
The mean is 43.33.
The standard deviation is 1.22.
2.6.21
n ≥32
2.6.23
(a) $19,799
(b) $9,899
Chapter 3
Discrete Probability Distributions
3.1
The Binomial Distribution
3.1.1
(a)
P(X = 3) = 0.0847
(b)
P(X = 6) = 0.0004
(c)
P(X ≤2) = 0.8913
(d)
P(X ≥7) = 3.085 × 10−5
(e)
E(X) = 1.2
(f) Var(X) = 1.056
3.1.3
X ∼B(6, 0.5)
xi
0
1
2
3
4
5
6
pi
0.0156
0.0937
0.2344
0.3125
0.2344
0.0937
0.0156
E(X) = 3
σ = 1.22
X ∼B(6, 0.7)

ANSWERS TO ODD-NUMBERED PROBLEMS
801
xi
0
1
2
3
4
5
6
pi
0.0007
0.0102
0.0595
0.1852
0.3241
0.3025
0.1176
E(X) = 4.2
σ = 1.12
3.1.5
(a)
P 
B 
8, 1
2

= 5
= 0.2187
(b) 0.3721
(c) 0.2326
3.1.7
0.35
0.0013
p = 0.5
3.1.9
X ∼B(18, 0.6)
(a) 0.3789
(b) 0.0013
3.1.11
0.905
3.2
The Geometric and Negative Binomial Distributions
3.2.1
(a)
P(X = 4) = 0.0189
(b)
P(X = 1) = 0.7
(c)
P(X ≤5) = 0.9976
(d)
P(X ≥8) = 0.0002
3.2.5
(a) 0.0678
(b) 0.0136
(c) 11.11
(d) 33.33
3.2.7
(a) 0.1406
(b) 0.0584
The expected number of cards drawn before the fourth heart
is obtained is 16.
If the ﬁrst two cards are spades, then the probability that the ﬁrst
heart card is obtained on the ﬁfth drawing is the same as the
probability in part (a).
3.2.9
(a) 0.01536
(b) 0.116
3.2.11
0.123
3.2.13
7.69
3.3
The Hypergeometric Distribution
3.3.1
(a)
P(X = 4) =
5
11
(b)
P(X = 5) =
2
11
(c)
P(X ≤3) = 23
66
3.3.3
(a)
90
221
(b)
25
442
(c)
139
442
3.3.5
55
833
The expected value is 3 and the variance is 30
17 .
3.3.7
(a)
7
33
(b)
14
55
3.3.9
(a) 0.431
(b) 0.375
3.3.11
0.333
3.4
The Poisson Distribution
3.4.1
(a)
P(X = 1) = 0.1304
(b)
P(X ≤3) = 0.6025
(c)
P(X ≥6) = 0.1054
(d)
P(X = 0|X ≤3) = 0.0677
3.4.5
0.7788
0.9735
3.4.7
0.7576
3.4.9
B
3.5
The Multinomial Distribution
3.5.1
(a) 0.0416
(b) 0.7967
3.5.3
(a) 0.0502
(b) 0.0670
(c) 0.1577
The expected number of misses is 0.96.
3.5.5
0.0212
Chapter 4
Continuous Probability Distributions
4.1
The Uniform Distribution
4.1.1
(a)
E(X) = 2.5
(b) σ = 3.175
(c) The upper quartile is 5.25.
(d)
P(0 ≤X ≤4) =
4
11
4.1.3
(a) The expectations are:
6
4
5
5
The variances are:
4.2
3.2
3.75
3.75
(b) 0.0087

802
ANSWERS TO ODD-NUMBERED PROBLEMS
4.1.5
(a)
2
3
(b)
P(difference ≤0.0005 | ﬁts in hole) = 1
4
4.2
The Exponential Distribution
4.2.3
(a)
E(X) = 5
(b) σ = 5
(c) The median is 3.47.
(d)
P(X ≥7) = 0.2466
(e) 0.6703
4.2.5
F(x) = 1
2e−λ(θ−x)
for −∞≤x ≤θ, and
F(x) = 1 −1
2e−λ(x−θ)
for θ ≤x ≤∞.
(a)
P(X ≤0) = 0.0012
(b)
P(X ≥1) = 0.9751
4.2.7
(a) λ = 1.8
(b)
E(X) = 0.5556
(c)
P(X ≥1) = 0.1653
(d) A Poisson distribution with parameter 7.2.
(e)
P(X ≥4) = 0.9281
4.2.9
(a)
P(X ≥1.5) = 0.301
(b) 0.217
4.2.11
(a)
P(X ≤6) = 0.699
(b) 0.180
4.2.13
0.355
4.3
The Gamma Distribution
4.3.1
(5.5) = 52.34
4.3.3
(a)
f (3) = 0.2055
F(3) = 0.3823
F−1(0.5) = 3.5919
(b)
f (3) = 0.0227
F(3) = 0.9931
F−1(0.5) = 1.3527
(c)
f (3) = 0.2592
F(3) = 0.6046
F−1(0.5) = 2.6229
4.3.5
(a) A gamma distribution with parameters k = 4 and λ = 2.
(b)
E(X) = 2
(c) σ = 1
(d) The probability can be calculated as
P(X ≥3) = 0.1512
where the random variable X has a gamma distribution with
parameters k = 4 and λ = 2.
The probability can also be calculated as
P(Y ≤3) = 0.1512
where the random variable Y has a Poisson distribution with
parameter
2 × 3 = 6
which counts the number of imperfections in a 3 meter length
of ﬁber.
4.3.7
(a) The expectation is E(X) = 62.86
the variance is Var(X) = 89.80
and the standard deviation is 9.48.
(b) 0.3991
4.4
The Weibull Distribution
4.4.3
(a) 0.5016
(b) 0.6780
0.3422
(c)
P(0.5 ≤X ≤1.5) = 0.5023
4.4.5
(a) 0.8000
(b) 4.5255
0.0888
(c) 31.066
91.022
(d)
P(3 ≤X ≤5) = 0.0722
4.4.7
0.0656
4.5
The Beta Distribution
4.5.1
(a)
A = 60
(b)
E(X) = 4
7
Var(X) =
3
98
(c) This is a beta distribution with a = 4 and b = 3.
4.5.3
(a)
f (0.5) = 1.9418
F(0.5) = 0.6753
F−1(0.75) = 0.5406
(b)
f (0.5) = 0.7398
F(0.5) = 0.7823
F−1(0.75) = 0.4579
(c)
f (0.5) = 0.6563
F(0.5) = 0.9375
F−1(0.75) = 0.3407
4.5.5
(a)
E(X) = 0.7579
Var(X) = 0.0175
(b) From the computer P(X ≥0.9) = 0.1368.

ANSWERS TO ODD-NUMBERED PROBLEMS
803
Chapter 5
The Normal Distribution
5.1
Probability Calculations Using the Normal Distribution
5.1.1
(a) 0.9099
(b) 0.5871
(c) 0.6521
(d) 0.4249
(e) 0.2960
(f) x = 0.1257
(g) x = −0.5828
(h) x = 0.3989
5.1.3
(a)
P(X ≤10.34) = 0.5950
(b)
P(X ≥11.98) = 0.0807
(c)
P(7.67 ≤X ≤9.90) = 0.4221
(d)
P(10.88 ≤X ≤13.22) = 0.2555
(e)
P(|X −10| ≤3) = 0.9662
(f) x = 11.2415
(g) x = 12.4758
(h) x = 0.6812
5.1.5
μ = 1.1569
σ = 4.5663
5.1.9
(a) 0.0478
(b) 0.0062
(c) c = 0.3091.
5.1.11
(a) 4.3809
4.2191
(b) c = 0.1538
5.1.13
(a) 0.2398
(b) 0.4298
(c) 0.0937
(d) 24.56
(e) 25.66
5.1.15
(a) The probability of being outside the range is 0.0956.
(b) σ = 0.304
5.1.17
μ = 93.09
5.2
Linear Combinations of Normal Random Variables
5.2.1
(a) 0.6360
(b) 0.6767
(c) 0.4375
(d) 0.0714
(e) 0.8689
(f) 0.1315
5.2.3
(a) 0.3830
(b) 0.8428
(c) n ≥27.
5.2.5
0.7777
5.2.7
(a) $1050
(b) 0.0002y2 + 0.0003(1000 −y)2
(c) The variance is minimized with y = 600.
0.1807
5.2.9
(a)
N(22.66, 4.312 × 10−3)
(b) x = 22.704
x = 22.616
5.2.11
(a) 0.843
(b) c = 0.624
5.2.13
0.645
5.2.15
n ≥22
5.2.17
(a)
E(X) = 1268000
The standard deviation is 11,180.
(b)
E(X) = μ = 63400
The standard deviation is 456.4.
5.2.19
0.023
5.3
Approximating Distributions with the Normal Distribution
5.3.1
(a) The exact probability is 0.3823.
The normal approximation is 0.3650.
(b) The exact probability is 0.9147.
The normal approximation is 0.9090.
(c) The exact probability is 0.7334.
The normal approximation is 0.7299.
(d) The exact probability is 0.6527.
The normal approximation is 0.6429.
5.3.3
The required probability is equal to
0.2358 for n = 100
0.2764 for n = 200
0.3772 for n = 500
0.4934 for n = 1000
and 0.6408 for n = 2000.
5.3.5
(a) A normal distribution can be used with
μ = 1200
and
σ 2 = 1200
(b) 0.0745
5.3.7
0.9731
5.3.9
0.7210
5.3.11
0.7824
5.3.13
0.9239
5.3.15
(a) 0.1382
(b) Very close to 1.
5.4
Distributions Related to the Normal Distribution
5.4.1
(a)
E(X) = 10.23
(b) Var(X) = 887.69
(c) 9.13
(d) 1.21
(e) 7.92
(f)
P(5 ≤X ≤8) = 0.1136
5.4.5
(a) χ2
0.10,9 = 14.68
(b) χ2
0.05,20 = 31.41
(c) χ2
0.01,26 = 45.64
(d) χ2
0.90,50 = 37.69
(e) χ2
0.95,6 = 1.635

804
ANSWERS TO ODD-NUMBERED PROBLEMS
5.4.7
(a) t0.10,7 = 1.415
(b) t0.05,19 = 1.729
(c) t0.01,12 = 2.681
(d) t0.025,30 = 2.042
(e) t0.005,4 = 4.604
5.4.9
(a)
F0.10,9,10 = 2.347
(b)
F0.05,6,20 = 2.599
(c)
F0.01,15,30 = 2.700
(d)
F0.05,4,8 = 3.838
(e)
F0.01,20,13 = 3.665
5.4.13
P(F5,20 ≥4.00) = 0.011
5.4.15
(a)
P(F10,50 ≥2.5) = 0.016
(b)
P(χ2
17 ≤12) = 0.200
(c)
P(t24 ≥3) = 0.003
(d)
P(t14 ≥−2) = 0.967
5.4.17
(a)
P(t16 ≤1.9) = 0.962
(b)
P(χ2
25 ≥42.1) = 0.018
(c)
P(F9,14 ≤1.8) = 0.844
(d)
P(−1.4 ≤t29 ≤3.4) = 0.913
Chapter 6
Descriptive Statistics
6.3
Sample Statistics
6.3.1
The sample mean is ¯x = 155.95.
The sample median is 159.
The sample trimmed mean is 156.50.
The sample standard deviation is s = 18.43.
The upper sample quartile is 169.5.
The lower sample quartile is 143.25.
6.3.3
The sample mean is ¯x = 37.08.
The sample median is 35.
The sample trimmed mean is 36.35.
The sample standard deviation is s = 8.32.
The upper sample quartile is 40.
The lower sample quartile is 33.5.
6.3.5
The sample mean is ¯x = 69.35.
The sample median is 66.
The sample trimmed mean is 67.88.
The sample standard deviation is s = 17.59.
The upper sample quartile is 76.
The lower sample quartile is 61.
6.3.7
The sample mean is ¯x = 12.211.
The sample median is 12.
The sample trimmed mean is 12.175.
The sample standard deviation is s = 2.629.
The upper sample quartile is 14.
The lower sample quartile is 10.
6.3.9
The sample mean is ¯x = 0.23181.
The sample median is 0.220.
The sample trimmed mean is 0.22875.
The sample standard deviation is s = 0.07016.
The upper sample quartile is 0.280.
The lower sample quartile is 0.185.
6.3.11
x = 13
6.3.13
B
6.3.15
A
Chapter 7
Statistical Estimation and Sampling Distributions
7.2
Properties of Point Estimates
7.2.1
(a) bias( ˆμ1) = 0
The point estimate ˆμ1 is unbiased.
bias( ˆμ2) = 0
The point estimate ˆμ2 is unbiased.
bias( ˆμ3) = 9 −μ
2
(b) Var( ˆμ1) = 6.2500
Var( ˆμ2) = 9.0625
Var( ˆμ3) = 1.9444
The point estimate ˆμ3 has the smallest variance.
(c) MSE( ˆμ1) = 6.2500
MSE( ˆμ2) = 9.0625
MSE( ˆμ3) = 1.9444 + (9 −μ
2 )2
This is equal to 26.9444 when μ = 8.
7.2.3
(a) Var( ˆμ1) = 2.5
(b) The value p = 0.6 produces the smallest variance which is
Var( ˆμ) = 2.4.
(c) The relative efﬁciency is 2.4
2.5 = 0.96.
7.2.5
(a) a1 + . . . + an = 1
(b) a1 = . . . = an = 1
n
7.2.7
MSE( ˆμ) = σ2
4 + (μ0−μ)2
4
MSE(X) = σ 2
7.2.9
The standard deviation is
√
29.49 = 5.43.
7.3
Sampling Distributions
7.3.1
The relative efﬁciency is n1
n2 .
7.3.3
(a) 0.4418
(b) 0.7150
7.3.5
(a) c = 45.46
(b) c = 50.26
7.3.7
(a) c = 0.4552
(b) c = 0.6209
7.3.9
ˆμ = 974.3
s.e.( ˆμ) = 3.594

ANSWERS TO ODD-NUMBERED PROBLEMS
805
7.3.11
ˆp = 0.22
s.e.( ˆp) = 0.0338
7.3.13
ˆμ = 69.35
s.e.( ˆμ) = 1.244
7.3.15
ˆμ = 12.211
s.e.( ˆμ) = 0.277
7.3.17
ˆμ = 0.23181
s.e.( ˆμ) = 0.00810
7.3.19
0.8022
0.9178
7.3.21
0.324
7.3.23
0.017
7.3.25
0.103
7.3.27
0.747
7.3.29
(a) 0.7626
(b) 0.5416
7.3.31
D
7.3.33
B
7.3.35
A
7.4
Constructing Parameter Estimates
7.4.1
ˆλ = 5.63
s.e.(ˆλ) = 0.495
Chapter 8
Inferences on a Population Mean
8.1
Conﬁdence Intervals
8.1.1
(52.30, 54.54)
8.1.3
(431.9, 441.1)
(430.9, 442.1)
(428.9, 444.1)
8.1.5
(0.0272, 0.0384)
8.1.7
A sample size of about n = 64 should be sufﬁcient.
8.1.9
An additional sample of at least 8 observations should be
sufﬁcient.
8.1.11
An additional sample of at least 27 observations should be
sufﬁcient.
8.1.13
c = 0.761
8.1.15
c = 420.0
8.1.17
(2.464, 3.040)
8.1.19
At 95% conﬁdence the conﬁdence interval is (11.66, 12.76).
8.1.21
At 95% conﬁdence the conﬁdence interval is (0.2157, 0.2480).
8.1.23
90%
8.1.25
(a) (5219.6, 5654.8)
(b) It can be estimated that an additional 16 chemical solutions
would need to be measured.
8.1.27
B
8.2
Hypothesis Testing
8.2.1
(a) The p-value is 2 × P(t17 ≥1.04) = 0.313.
(b) The p-value is P(t17 ≤−2.75) = 0.0068.
8.2.3
(a) The p-value is 2 × (−1.34) = 0.180.
(b) The p-value is (−2.45) = 0.007.
8.2.5
(a) The null hypothesis is accepted when |t| ≤1.684.
(b) The null hypothesis is rejected when |t| > 2.704.
(c) The null hypothesis is rejected at size α = 0.10
and accepted at size α = 0.01.
(d) The p-value is 2 × P(t40 ≥2.066) = 0.045.
8.2.7
(a) The null hypothesis is accepted when |t| ≤1.753.
(b) The null hypothesis is rejected when |t| > 2.947.
(c) The null hypothesis is rejected at size α = 0.10
and accepted at size α = 0.01.
(d) The p-value is 2 × P(t15 ≥1.931) = 0.073.
8.2.9
(a) The null hypothesis is accepted when t ≤1.296.
(b) The null hypothesis is rejected when t > 2.390.
(c) The null hypothesis is rejected at size α = 0.01
and consequently also at size α = 0.10.
(d) The p-value is P(t60 ≥3.990) = 0.0001.
8.2.11
There is sufﬁcient evidence to conclude that the machine is mis-
calibrated.
8.2.13
There is sufﬁcient evidence to conclude that the chemical plant
is in violation of the working code.
8.2.15
There is sufﬁcient evidence to conclude that the average corro-
sion rate of chilled cast iron of this type is larger than 2.5.
8.2.17
There is sufﬁcient evidence to conclude that the average number
of calls taken per minute is less than 13 so that the manager’s
claim is false.

806
ANSWERS TO ODD-NUMBERED PROBLEMS
8.2.19
There is not sufﬁcient evidence to conclude that the spray paint-
ing machine is not performing properly.
8.2.21
There is not sufﬁcient evidence to conclude that the average volt-
age of the batteries from the production line is at least 238.5.
8.2.23
There is sufﬁcient evidence to conclude that the average length
of the components is not 82.50.
8.2.25
There is sufﬁcient evidence to conclude that the average breaking
strength is not 7.000.
8.2.27
The hypotheses are H0 : μ ≥25 versus HA : μ < 25.
8.2.29
(a) The experiment does not provide sufﬁcient evidence to con-
clude that the average time to toxicity of salmon ﬁllets under
these storage conditions is more than 11 days.
(b) (9.40, 14.55)
8.2.31
D
8.2.33
A
8.2.35
B
Chapter 9
Comparing Two Population Means
9.2
Analysis of Paired Samples
9.2.1
There is not sufﬁcient evidence to conclude that the new assem-
bly method is any quicker on average than the standard assembly
method.
9.2.3
There is sufﬁcient evidence to conclude that the new tires are
better than the standard tires in terms of the average reduction in
tread depth.
9.2.5
There is not sufﬁcient evidence to conclude that the two labora-
tories are any different in the datings that they provide.
9.2.7
There is not sufﬁcient evidence to conclude that procedures A
and B give different readings on average.
The reviewer’s comments are plausible.
9.2.9
There is not sufﬁcient evidence to conclude that the addition
of the surfactant has an effect on the amount of uranium-oxide
removed from the water.
9.2.11
D
9.3
Analysis of Independent Samples
9.3.3
(a) (−76.4, 21.8)
(b) (−83.6, 29.0)
(c) The null hypothesis is accepted at size α = 0.01.
The p-value is 2 × P(t23 ≥1.56) = 0.132.
9.3.5
(a) (−∞, −0.0014)
(b) The null hypothesis is rejected at size α = 0.01 and conse-
quently is also rejected at size α = 0.05.
The p-value is P(t26 ≤−4.95) = 0.000.
9.3.7
(a) The null hypothesis is rejected at size α = 0.01.
(b) (−∞, −9.3)
(c) There is sufﬁcient evidence to conclude that the synthetic
ﬁber bundles have an average breaking strength larger than
the wool ﬁber bundles.
9.3.9
(a) The p-value is 2 × (−1.92) = 0.055.
(b) (4.22, 18.84)
9.3.11
(a) The p-value is 2 × (−2.009) = 0.045.
(b) A 90% two-sided conﬁdence interval is (0.16, 1.56).
A 95% two-sided conﬁdence interval is (0.02, 1.70).
A 99% two-sided conﬁdence interval is (−0.24, 1.96).
9.3.13
Equal sample sizes of at least 47 can be recommended.
9.3.15
Additional sample sizes of at least 96 −41 = 55 from each
population can be recommended.
9.3.17
There is sufﬁcient evidence to conclude that the paving slabs
from company A weigh more on average than the paving slabs
from company B.
There is also more variability in the weights of the paving slabs
from company A.
9.3.19
There is sufﬁcient evidence to conclude that the damped feature
is effective in reducing the heel-strike force.
9.3.21
There is not sufﬁcient evidence to conclude that the average ser-
vice times are any different at these two times of day.
9.3.23
There is sufﬁcient evidence to conclude that on average medicine
A provides a higher response than medicine B.
9.3.25
¯xB < 149.9
Chapter 10
Discrete Data Analysis
10.1
Inferences on a Population Proportion
10.1.1
(a) (0.127, 0.560)
(b) (0.179, 0.508)
(c) (0, 0.539)
(d) The exact p-value is 0.110.
The approximate p-value is 0.077.
10.1.3
(a) Let p be the probability that a value produced by the random
number generator is a zero, and consider the hypotheses
H0 : p = 0.5 versus HA : p ̸= 0.5
The p-value is 0.018.

ANSWERS TO ODD-NUMBERED PROBLEMS
807
(b) (0.4995, 0.5110)
(c) An additional sample size of about 215,500 would be
required.
10.1.5
There is more support for foul play from the second experiment
than from the ﬁrst.
10.1.7
There is sufﬁcient evidence to conclude that the jurors are not
being randomly selected.
10.1.9
The required sample size for the worst case scenario is n ≥9604.
If p ≥0.75 the required sample size can be calculated as
n ≥7203.
10.1.11 (0.494, 0.723)
An additional sample size of 512 would be required.
10.1.13 (0.385, 0.815)
10.1.15 The required sample size for the worst case scenario is 385 house-
holders.
If p ≤1/3 then the required sample size can be calculated as
342 householders.
10.1.17 The standard conﬁdence interval is (0.161, 0.557).
The alternative conﬁdence interval is (0.195, 0.564).
10.1.19 (0.085, 0.211)
10.1.21 The margin of error was calculated with 95% conﬁdence under
the worst case scenario where the estimated probability could be
close to 0.5.
10.2
Comparing Two Population Proportions
10.2.1
(a) (−0.195, 0.413)
(b) (−0.122, 0.340)
(c) (−0.165, 1)
(d) The p-value is 0.365.
10.2.3
(a) (−0.124, 0.331)
(b) The p-value is 0.251.
There is not sufﬁcient evidence to conclude that one radar
system is any better than the other radar system.
10.2.5
(−1, −0.002)
The p-value is 0.050.
There is some evidence that the presence of seed crystals in-
creases the probability of crystallization within 24 hours but it is
not overwhelming.
10.2.7
(−0.017, 0.008)
The p-value is 0.479.
There is not sufﬁcient evidence to conclude that there is a differ-
ence in the operating standards of the two production lines.
10.2.9
(−0.051, 0.027)
The p-value is 0.549.
There is not sufﬁcient evidence to conclude that there is a
difference in the quality of the computer chips from the two
suppliers.
10.2.11 There is sufﬁcient evidence to conclude that machine A is better
than machine B.
10.2.13 (a) There is not sufﬁcient evidence to conclude that the prob-
ability of an insulator of this type having a dielectric
breakdown strength below the speciﬁed threshold level is
larger at 250 degrees Centigrade than it is at 180 degrees
Centigrade.
(b) (−0.269, 0.117)
10.2.15 (−0.172, 0.036)
The conﬁdence interval contains zero so there is not sufﬁcient
evidence to conclude that the failure rates due to operator misuse
are different for the two products.
10.3
Goodness of Fit Tests for One-way Contingency Tables
10.3.1
(a) The expected cell frequencies are ei = 83.33.
(b) The Pearson chi-square statistic is 4.36.
(c) The likelihood ratio chi-square statistic is 4.44.
(d) The p-values are 0.499 and 0.488.
A size α = 0.01 test of the null hypothesis that the die is fair
is accepted.
(e) (0.159, 0.217)
10.3.3
(a) There is a fairly strong suggestion that the supposition is
not plausible although the evidence is not completely over-
whelming.
(b) (0.425, 0.598)
10.3.5
(a) These probability values are plausible.
(b) (0.358, 0.531)
10.3.7
It is not reasonable to model the number of arrivals with a Poisson
distribution with mean λ = 7.
10.3.9
The data set is consistent with the proposed genetic theory.
10.3.11 (a) There is not sufﬁcient evidence to conclude that the proba-
bility that a solution has normal acidity is not 0.80.
(b) The data is not consistent with the claimed probabilities.
10.3.13 It is plausible that the number of shark attacks per year follows
a Poisson distribution with mean 2.5.
10.3.15 D

808
ANSWERS TO ODD-NUMBERED PROBLEMS
10.4
Testing for Independence in Two-way Contingency Tables
10.4.1
(a) The expected cell frequencies are
Acceptable
Defective
Supplier A
186.25
13.75
Supplier B
186.25
13.75
Supplier C
186.25
13.75
Supplier D
186.25
13.75
(b) The Pearson chi-square statistic is X 2 = 7.087.
(c) The likelihood ratio chi-square statistic is G2 = 6.889.
(d) The p-values are 0.069 and 0.076.
(e) The null hypothesis that the defective rates are identical for
the four suppliers is accepted at size α = 0.05.
(f) (0.020, 0.080)
(g) (−0.086, 0.026)
10.4.3
There is not sufﬁcient evidence to conclude that the preferences
for the different formulations change with age.
10.4.5
There is sufﬁcient evidence to conclude that some technicians
are better than others in satisfying their customers.
10.4.7
(a) There is sufﬁcient evidence to conclude that ps ̸= pn.
(b) (−0.375, −0.085)
10.4.9
There is not sufﬁcient evidence to conclude that the probability
of a customer purchasing the extended warranty is different for
the three product types.
10.4.11 A
Chapter 11
The Analysis of Variance
11.1
One-Factor Analysis of Variance
11.1.1
(a)
P(X ≥4.2) = 0.0177
(b)
P(X ≥2.3) = 0.0530
(c)
P(X ≥31.7) ≤0.0001
(d)
P(X ≥9.3) = 0.0019
(e)
P(X ≥0.9) = 0.5010
11.1.3
Source
df
SS
MS
F
p-value
Treatments
7
126.95
18.136
5.01
0.0016
Error
22
79.64
3.62
Total
29
206.59
11.1.5
Source
df
SS
MS
F
p-value
Treatments
3
162.19
54.06
6.69
0.001
Error
40
323.34
8.08
Total
43
485.53
11.1.7
Source
df
SS
MS
F
p-value
Treatments
3
0.0079
0.0026
1.65
0.189
Error
52
0.0829
0.0016
Total
55
0.0908
11.1.9
(a) μ1 −μ2 ∈(−22.8, −8.8)
μ1 −μ3 ∈(3.6, 17.6)
μ1 −μ4 ∈(−0.9, 13.1)
μ1 −μ5 ∈(−13.0, 1.0)
μ1 −μ6 ∈(1.3, 15.3)
μ2 −μ3 ∈(19.4, 33.4)
μ2 −μ4 ∈(14.9, 28.9)
μ2 −μ5 ∈(2.8, 16.8)
μ2 −μ6 ∈(17.1, 31.1)
μ3 −μ4 ∈(−11.5, 2.5)
μ3 −μ5 ∈(−23.6, −9.6)
μ3 −μ6 ∈(−9.3, 4.7)
μ4 −μ5 ∈(−19.1, −5.1)
μ4 −μ6 ∈(−4.8, 9.2)
μ5 −μ6 ∈(7.3, 21.3)
(c) An additional sample size of 6 observations from each factor
level can be recommended.
11.1.11 (a)
¯x1. = 5.633
¯x2. = 5.567
¯x3. = 4.778
(b)
¯x.. = 5.326
(c)
SST R = 4.076
(d) k
i=1
ni
j=1 x2
i j = 791.30
(e)
SST = 25.432
(f)
SSE = 21.356
(g) Source
df
SS
MS
F
p-value
Treatments
2
4.076
2.038
2.29
0.123
Error
24
21.356
0.890
Total
26
25.432
(h) μ1 −μ2 ∈(−1.04, 1.18)
μ1 −μ3 ∈(−0.25, 1.97)
μ2 −μ3 ∈(−0.32, 1.90)
(j) An additional sample size of 36 observations from each fac-
tor level can be recommended.
11.1.13 Source
df
SS
MS
F
p-value
Treatments
2
0.0085
0.0042
0.24
0.787
Error
87
1.5299
0.0176
Total
89
1.5384
μ1 −μ2 ∈(−0.08, 0.08)
μ1 −μ3 ∈(−0.06, 0.10)
μ2 −μ3 ∈(−0.06, 0.10)
There is not sufﬁcient evidence to conclude that there is a differ-
ence between the three production lines.

ANSWERS TO ODD-NUMBERED PROBLEMS
809
11.1.15 Source
df
SS
MS
F
p-value
Treatments
2
0.0278
0.0139
1.26
0.299
Error
30
0.3318
0.0111
Total
32
0.3596
μ1 −μ2 ∈(−0.15, 0.07)
μ1 −μ3 ∈(−0.08, 0.14)
μ2 −μ3 ∈(−0.04, 0.18)
There is not sufﬁcient evidence to conclude that the radiation
readings are affected by the background radiation levels.
11.1.17 Source
df
SS
MS
F
p-value
Treatments
2
0.4836
0.2418
7.13
0.001
Error
93
3.1536
0.0339
Total
95
3.6372
μ1 −μ2 ∈(−0.01, 0.22)
μ1 −μ3 ∈(0.07, 0.29)
μ2 −μ3 ∈(−0.03, 0.18)
There is sufﬁcient evidence to conclude that the average particle
diameter is larger at the low amount of stabilizer than at the high
amount of stabilizer.
11.1.19 Source
df
SS
MS
F
p-value
Treatments
2
5.981
2.990
0.131
0.878
Error
26
593.753
22.837
Total
28
599.734
There is not sufﬁcient evidence to conclude that there is a dif-
ference between the catalysts in terms of the strength of the
compound.
11.1.21 With a 95% conﬁdence level the pairwise conﬁdence intervals
that contain 0 are:
μ1 −μ2
μ2 −μ5
μ3 −μ4
It can be inferred that the largest mean is either μ3 or μ4 and that
the smallest mean is either μ2 or μ5.
11.1.23 Source
df
SS
MS
F
p-value
Physician
5
1983.8
396.8
15.32
0.000
Error
24
621.6
25.9
Total
29
2605.4
The p-value of 0.000 implies that there is sufﬁcient evidence to
conclude that the times taken by the physicians for the investiga-
tory surgical procedures are different.
The slowest physician is either physician 1, physician 4, or physi-
cian 5.
The quickest physician is either physician 2, physician 3, or
physician 6.
11.1.25 (a) Source
df
SS
MS
F
p-value
Treatments
3
13.77
4.59
4.72
0.01
Error
24
23.34
0.97
Total
27
37.11
(b) μ1 −μ2 ∈(−2.11, 0.44)
μ1 −μ3 ∈(−2.63, 0.01)
μ1 −μ4 ∈(−3.36, −0.62)
μ2 −μ3 ∈(−1.75, 0.79)
μ2 −μ4 ∈(−2.48, 0.18)
μ3 −μ4 ∈(−2.04, 0.70)
There is sufﬁcient evidence to establish that μ4 is larger
than μ1.
11.1.27 B
11.2
Randomized Block Designs
11.2.1
Source
df
SS
MS
F
p-value
Treatments
3
10.15
3.38
3.02
0.047
Blocks
9
24.53
2.73
2.43
0.036
Error
27
30.24
1.12
Total
39
64.92
11.2.3
Source
df
SS
MS
F
p-value
Treatments
3
58.72
19.57
0.63
0.602
Blocks
9
2839.97
315.55
10.17
0.0000
Error
27
837.96
31.04
Total
39
3736.64
11.2.5
(a) Source
df
SS
MS
F
p-value
Treatments
2
8.17
4.085
8.96
0.0031
Blocks
7
50.19
7.17
15.72
0.0000
Error
14
6.39
0.456
Total
23
64.75
(b) μ1 −μ2 ∈(0.43, 2.19)
μ1 −μ3 ∈(0.27, 2.03)
μ2 −μ3 ∈(−1.04, 0.72)
11.2.7
(a)
¯x1. = 6.0617
¯x2. = 7.1967
¯x3. = 5.7767
(b)
¯x.1 = 7.4667
¯x.2 = 5.2667
¯x.3 = 5.1133
¯x.4 = 7.3300
¯x.5 = 6.2267
¯x.6 = 6.6667
(c)
¯x.. = 6.345
(d)
SSTr = 6.7717
(e)
SSBl = 15.0769
(f) 3
i=1
6
j=1 x2
i j = 752.1929
(g)
SST = 27.5304
(h)
SSE = 5.6818

810
ANSWERS TO ODD-NUMBERED PROBLEMS
(i) Source
df
SS
MS
F
p-value
Treatments
2
6.7717
3.3859
5.96
0.020
Blocks
5
15.0769
3.0154
5.31
0.012
Error
10
5.6818
0.5682
Total
17
27.5304
(j) μ1 −μ2 ∈(−2.33, 0.05)
μ1 −μ3 ∈(−0.91, 1.47)
μ2 −μ3 ∈(0.22, 2.61)
(l) An additional 3 blocks can be recommended.
11.2.9
Source
df
SS
MS
F
p-value
Treatments
2
17.607
8.803
2.56
0.119
Blocks
6
96.598
16.100
4.68
0.011
Error
12
41.273
3.439
Total
20
155.478
μ1 −μ2 ∈(−1.11, 4.17)
μ1 −μ3 ∈(−0.46, 4.83)
μ2 −μ3 ∈(−1.99, 3.30)
There is not sufﬁcient evidence to conclude that the calciners are
operating at different efﬁciencies.
11.2.11 Source
df
SS
MS
F
p-value
Treatments
3
3231.2
1,077.1
4.66
0.011
Blocks
8
29256.1
3,657.0
15.83
0.000
Error
24
5545.1
231.0
Total
35
38032.3
μ1 −μ2 ∈(−8.20, 31.32)
μ1 −μ3 ∈(−16.53, 22.99)
μ1 −μ4 ∈(−34.42, 5.10)
μ2 −μ3 ∈(−28.09, 11.43)
μ2 −μ4 ∈(−45.98, −6.46)
μ3 −μ4 ∈(−37.65, 1.87)
There is sufﬁcient evidence to conclude that driver 4 is better
than driver 2.
11.2.13
Source
df
SS
MS
F
p-value
Treatments
4
8.462 × 108
2.116 × 108
66.55
0.000
Blocks
11
19.889 × 108
1.808 × 108
56.88
0.000
Error
44
1.399 × 108
3.179 × 106
Total
59
29.750 × 108
μ1 −μ2 ∈(4372, 8510)
μ1 −μ3 ∈(4781, 8919)
μ1 −μ4 ∈(5438, 9577)
μ1 −μ5 ∈(−3378, 760)
μ2 −μ3 ∈(−1660, 2478)
μ2 −μ4 ∈(−1002, 3136)
μ2 −μ5 ∈(−9819, −5681)
μ3 −μ4 ∈(−1411, 2727)
μ3 −μ5 ∈(−10228, −6090)
μ4 −μ5 ∈(−10886, −6748)
There is sufﬁcient evidence to conclude that either agent 1 or
agent 5 is the best agent.
The worst agent is either agent 2, 3 or 4.
11.2.15 (a) Source
df
SS
MS
F
p-value
Treatments
3
0.151
0.0503
5.36
0.008
Blocks
6
0.324
0.054
5.75
0.002
Error
18
0.169
0.00939
Total
27
0.644
(b) μ2 −μ1 ∈(−0.326, −0.034)
μ2 −μ3 ∈(−0.313, −0.021)
μ2 −μ4 ∈(−0.305, −0.013)
None of these conﬁdence intervals contains 0 so there is suf-
ﬁcient evidence to conclude that treatment 2 has a smaller
mean value than each of the other treatments.
11.2.17 The new analysis of variance table is
Source
df
SS
MS
F
p-value
Treatments
same
a2 SSTr
a2 MSTr
same
same
Blocks
same
a2 SSBl
a2 MSBl
same
same
Error
same
a2 SSE
a2 MSE
Total
same
a2 SST
11.2.19 Source
df
SS
MS
F
p-value
Locations
3
3.893
1.298
0.49
0.695
Time
4
472.647
118.162
44.69
0.000
Error
12
31.729
2.644
Total
19
508.270
The p-value of 0.695 implies that there is not sufﬁcient evi-
dence to conclude that the pollution levels are different at the four
locations.
The conﬁdence intervals for all of the pairwise comparisons con-
tain 0, so the graphical representation has one line joining all four
sample means.
Chapter 12
Simple Linear Regression and Correlation
12.1
The Simple Linear Regression Model
12.1.1
(a) 21.2
(b) 5.1
(c) 0.587
(d) 0.401
(e) 0.354
12.1.3
(a) 23.0
(b) The expected value of the porosity decreases by 4.5.
(c) 0.963
(d) 0.968
12.1.5
0.975
12.1.7
B

ANSWERS TO ODD-NUMBERED PROBLEMS
811
12.2
Fitting the Regression Line
12.2.3
ˆβ0 = 39.5
ˆβ1 = −2.04
ˆσ 2 = 17.3
43.6
12.2.5
(a)
ˆβ0 = 36.19
ˆβ1 = 0.2659
(b)
ˆσ 2 = 70.33
(c) Yes.
(d) 55.33
12.2.7
(a)
ˆβ0 = −29.59
ˆβ1 = 0.07794
(b) 173.1
(c) 7.794
(d)
ˆσ 2 = 286
12.2.9
(a)
ˆβ0 = 12.864
ˆβ1 = 0.8051
(b) 68.42
(c) 4.03
(d)
ˆσ 2 = 3.98
12.2.11 C
12.3
Inferences on the Slope Parameter ˆβ1
12.3.1
(a) (0.107, 0.937)
(b) The p-value is 0.002.
12.3.3
(a) s.e.( ˆβ1) = 0.08532
(b) (0.820, 1.186)
(c) The p-value is 0.000.
12.3.5
(a) s.e.( ˆβ1) = 0.1282
(b) (−∞, −0.115)
(c) The (two-sided) p-value is 0.017.
12.3.7
(a) s.e.( ˆβ1) = 0.2829
(b) (1.041, 2.197)
(c) If β1 = 1 then the actual times are equal to the estimated
times except for a constant difference of β0.
The p-value is 0.036.
12.3.9
The p-value is 0.019.
12.3.11 A
12.4
Inferences on the Regression Line
12.4.3
(21.9, 23.2)
12.4.5
(33.65, 41.02)
12.4.7
(−∞, 10.63)
12.4.9
(76.284, 76.378)
12.5
Prediction Intervals for Future Response Values
12.5.1
(1386, 1406)
12.5.3
(5302, 9207)
12.5.5
(165.7, 274.0)
12.5.7
(63.48, 74.96)
12.5.9
ˆβ1 = 1.851
ˆβ0 = 13.875
ˆσ 2 = 0.494
(47.864, 53.941)
12.6
The Analysis of Variance Table
12.6.1
Source
df
SS
MS
F
p-value
Regression
1
40.53
40.53
2.32
0.137
Error
33
576.51
17.47
Total
34
617.04
R2 = 0.066
12.6.3
Source
df
SS
MS
F
p-value
Regression
1
870.43
870.43
889.92
0.000
Error
8
7.82
0.9781
Total
9
878.26
R2 = 0.991
12.6.5
Source
df
SS
MS
F
p-value
Regression
1
10.71 × 107
10.71 × 107
138.29
0.000
Error
14
1.08 × 107
774,211
Total
15
11.79 × 107
R2 = 0.908
12.6.7
Source
df
SS
MS
F
p-value
Regression
1
397.58
397.58
6.94
0.017
Error
18
1031.37
57.30
Total
19
1428.95
R2 = 0.278
12.6.9
Source
df
SS
MS
F
p-value
Regression
1
411.26
411.26
32.75
0.000
Error
30
376.74
12.56
Total
31
788.00
R2 = 0.522
The p-value is not very meaningful because it tests the null
hypothesis that the actual times are unrelated to the estimated
times.
12.6.11 A

812
ANSWERS TO ODD-NUMBERED PROBLEMS
12.7
Residual Analysis
12.7.1
There is no suggestion that the ﬁtted regression model is not
appropriate.
12.7.3
There is a possible suggestion of a slight reduction in the vari-
ability of the VO2-max values as age increases.
12.7.5
The variability of the actual times increases as the estimated time
increases.
12.7.7
D
12.8
Variable Transformations
12.8.1
The model
y = γ0 eγ1x
is appropriate.
A linear regression can be performed with ln(y) as the dependent
variable and with x as the input variable.
ˆγ0 = 9.12
ˆγ1 = 0.28
ˆγ0 e ˆγ1×2.0 = 16.0
12.8.3
ˆγ0 = 8.81
ˆγ1 = 0.523
γ0 ∈(6.84, 11.35)
γ1 ∈(0.473, 0.573)
12.8.5
ˆγ0 = 13.85
ˆγ1 = 0.341
(0.289, 0.393)
12.8.7
ˆγ0 = 12.775
ˆγ1 = −0.5279
When the crack length is 2.1 the expected breaking load is 4.22.
12.9
Correlation Analysis
12.9.3
The sample correlation coefﬁcient is r = 0.95.
12.9.5
The sample correlation coefﬁcient is r = −0.53.
12.9.7
The sample correlation coefﬁcient is r = 0.72.
12.9.9
The sample correlation coefﬁcient is r = 0.431.
12.9.11 The variables A and B may both be related to a third surrogate
variable C.
12.9.13 A
Chapter 13
Multiple Linear Regression and Nonlinear Regression
13.1
Introduction to Multiple Linear Regression
13.1.1
(a)
R2 = 0.89
(b) Source
df
SS
MS
F
p-value
Regression
3
96.5
32.17
67.4
0.000
Error
26
12.4
0.477
Total
29
108.9
(c)
ˆσ 2 = 0.477
(d) The p-value is 0.000.
(e) (11.2, 21.8)
13.1.3
(a) (67.1, 197.7)
(b) The p-value is 0.002.
13.1.5
Variable x3 should be removed from the model.
13.1.7
The p-value is 0.013.
13.1.9
(a)
ˆy = 355.38
(b) (318.24, 392.52)
13.1.11 Source
df
SS
MS
F
p-value
Regression
3
392.495
130.832
6.978
0.003
Error
16
299.982
18.749
Total
19
692.477
The p-value in the analysis of variance table is for the null hy-
pothesis
H0 : β1 = β2 = β3 = 0.
The proportion of the variability of the y variable that is explained
by the model is
R2 = 56.7%.
13.1.13 B
13.1.15 A
13.1.17 B
13.2
Examples of Multiple Linear Regression
13.2.1
(b) The variable competitor’s price has a p-value of 0.216 and
is not needed in the model.
The sample correlation coefﬁcient between the competitor’s
price and the sales is r = −0.91.
The sample correlation coefﬁcient between the competitor’s
price and the company’s price is r = 0.88.
(c) The sample correlation coefﬁcient between the company’s
price and the sales is r = −0.96.

ANSWERS TO ODD-NUMBERED PROBLEMS
813
Using the model
sales = 107.4 −(3.67 × company’s price)
the predicted sales are 107.4 −(3.67 × 10.0) = 70.7.
13.2.3
(a)
ˆβ0 = −3, 238.6
ˆβ1 = 0.9615
ˆβ2 = 0.732
ˆβ3 = 2.889
ˆβ4 = 389.9
(b) The variable geology has a p-value of 0.737 and is not needed
in the model.
The sample correlation coefﬁcient between the cost and
geology is r = 0.89.
The sample correlation coefﬁcient between the depth and
geology is r = 0.92.
The variable geology is not needed in the model because it
is highly correlated with the variable depth which is in the
model.
(c) The variable rig-index can also be removed from the
model.
A ﬁnal model
cost = −3011 + (1.04 × depth) + (2.67 × downtime)
can be recommended.
13.2.5
Two indicator variables x1 and x2 are needed.
13.3
Matrix Algebra Formulation of Multiple Linear Regression
13.3.1
(a)
Y =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2
−2
4
−2
2
−4
1
3
1
−5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(b)
X =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
1
1
0
−1
1
1
4
1
1
−4
1
−1
2
1
−1
−2
1
2
0
1
2
0
1
−2
3
1
−2
−3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(c)
X′X =

10
0
0
0
20
0
0
0
60

(d)
(X′X)−1 =

0.1000
0
0
0
0.0500
0
0
0
0.0167

(e)
X′Y =

0
20
58

(g)
ˆY =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.967
−0.967
4.867
−2.867
0.933
−2.933
2.000
2.000
0.900
−4.900
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(h)
e =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1.033
−1.033
−0.867
0.867
1.067
−1.067
−1.000
1.000
0.100
−0.100
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(i)
SSE = 7.933
(k) s.e.( ˆβ1) = 0.238
s.e.( ˆβ2) = 0.137
Both input variables should be kept in the model.
(l) The ﬁtted value is 2.933.
The standard error is 0.496.
The conﬁdence interval is (1.76, 4.11).
(m) The prediction interval is (0.16, 5.71).
13.3.3
Y =
⎛
⎜
⎜
⎜
⎜
⎝
10
0
−5
2
3
−6
⎞
⎟
⎟
⎟
⎟
⎠
X =
⎛
⎜
⎜
⎜
⎜
⎝
1
−3
1
3
1
−2
1
0
1
−1
1
−5
1
1
−6
1
1
2
−3
0
1
3
6
1
⎞
⎟
⎟
⎟
⎟
⎠
X′X =
⎛
⎜
⎝
6
0
0
0
0
28
0
0
0
0
84
−2
0
0
−2
36
⎞
⎟
⎠
(X′X)−1 =
⎛
⎜
⎝
0.16667
0
0
0
0 0.03571
0
0
0
0 0.01192 0.00066
0
0 0.00066 0.02781
⎞
⎟
⎠
X′Y =
⎛
⎜
⎝
4
−35
−52
51
⎞
⎟
⎠
ˆβ =
⎛
⎜
⎝
0.6667
−1.2500
−0.5861
1.3841
⎞
⎟
⎠

814
ANSWERS TO ODD-NUMBERED PROBLEMS
13.4
Evaluating Model Accuracy
13.4.1
(a) There is a slight suggestion of a greater variability in the
yields at higher temperatures.
(b) There are no unusually large standardized residuals.
(c) The points (90, 85) and (200, 702) have leverage values
hii = 0.547.
13.4.3
(a) The residual plots do not indicate any substantial problems.
(b) If it were beneﬁcial to add the variable weight to the model,
then there would be some pattern in this residual plot.
(d) The observation with VO2-max = 23 has a standardized
residual of −2.15.
13.4.5
B
13.4.7
B
Chapter 14
Multifactor Experimental Design and Analysis
14.1
Experiments with Two Factors
14.1.1
Source
df
SS
MS
F
p-value
Fuel
1
96.33
96.33
3.97
0.081
Car
1
75.00
75.00
3.09
0.117
Fuel*Car
1
341.33
341.33
14.08
0.006
Error
8
194.00
24.25
Total
11
706.66
14.1.3
Source
df
SS
MS
F
p-value
Tip
2
0.1242
0.0621
1.86
0.175
Material
2
14.1975
7.0988
212.31
0.000
Tip*Material
4
0.0478
0.0120
0.36
0.837
Error
27
0.9028
0.0334
Total
35
15.2723
14.1.5
Source
df
SS
MS
F
p-value
Glass
2
7.568
3.784
0.70
0.514
Acidity
1
12.667
12.667
2.36
0.151
Glass*Acidity
2
93.654
46.827
8.71
0.005
Error
12
64.540
5.378
Total
17
178.429
14.1.7
Source
df
SS
MS
F
p-value
Design
2
3.896 × 103
1.948 × 103
0.46
0.685
Material
1
0.120 × 103
0.120 × 103
0.03
0.882
Error
2
8.470 × 103
4.235 × 103
Total
5
12.487 × 103
14.2
Experiments with Three or More Factors
14.2.1
Source
df
SS
MS
F
p-value
Drink
2
90.65
45.32
5.39
0.007
Gender
1
6.45
6.45
0.77
0.384
Age
2
23.44
11.72
1.39
0.255
Drink*Gender
2
17.82
8.91
1.06
0.352
Drink*Age
4
24.09
6.02
0.72
0.583
Gender*Age
2
24.64
12.32
1.47
0.238
Drink*Gender*Age
4
27.87
6.97
0.83
0.511
Error
72
605.40
8.41
Total
89
820.36
14.2.3
Source
df
SS
MS
F
p-value
Add-A
2
324.11
162.06
8.29
0.003
Add-B
2
5.18
2.59
0.13
0.877
Conditions
1
199.28
199.28
10.19
0.005
Add-A*Add-B
4
87.36
21.84
1.12
0.379
Add-A*Conditions
2
31.33
15.67
0.80
0.464
Add-B*Conditions
2
2.87
1.44
0.07
0.930
Add-A*Add-B*Conditions
4
21.03
5.26
0.27
0.894
Error
18
352.05
19.56
Total
35
1023.21

ANSWERS TO ODD-NUMBERED PROBLEMS
815
14.2.5
Source
df
SS
MS
F
p-value
Machine
1
387.1
387.1
3.15
0.095
Temp
1
29.5
29.5
0.24
0.631
Position
1
1271.3
1271.3
10.35
0.005
Angle
1
6865.0
6685.0
55.91
0.000
Machine*Temp
1
43.0
43.0
0.35
0.562
Machine*Position
1
54.9
54.9
0.45
0.513
Machine*Angle
1
1013.6
1013.6
8.25
0.011
Temp*Position
1
67.6
67.6
0.55
0.469
Temp*Angle
1
8.3
8.3
0.07
0.798
Position*Angle
1
61.3
61.3
0.50
0.490
Machine*Temp*Position
1
21.0
21.0
0.17
0.685
Machine*Temp*Angle
1
31.4
31.4
0.26
0.620
Machine*Position*Angle
1
13.7
13.7
0.11
0.743
Temp*Position*Angle
1
17.6
17.6
0.14
0.710
Machine*Temp*Position*Angle
1
87.5
87.5
0.71
0.411
Error
16
1964.7
122.8
Total
31
11937.3
14.2.7
A redundant
B redundant
C redundant
D redundant
A*B not signiﬁcant
A*C redundant
A*D redundant
B*C signiﬁcant
B*D signiﬁcant
C*D redundant
A*B*C not signiﬁcant
A*B*D not signiﬁcant
A*C*D signiﬁcant
B*C*D not signiﬁcant
A*B*C*D not signiﬁcant
Chapter 15
Nonparametric Statistical Analysis
15.1
The Analysis of a Single Population
15.1.1
(c) It is not plausible.
(d) It is not plausible.
(e) The p-value is 0.064.
(f) The p-value is 0.001.
(g) The conﬁdence interval from the sign test is (65.0, 69.0).
The conﬁdence interval from the signed rank test is
(66.0, 69.5).
15.1.3
The p-values for the hypotheses
H0 : μ = 0.2
versus
HA : μ ̸= 0.2
are 0.004 for the sign test,
0.000 for the signed rank test,
and 0.000 for the t-test.
Conﬁdence intervals for μ with a conﬁdence level of at least 95%
are
(0.207, 0.244) for the sign test,
(0.214, 0.244) for the signed rank test,
and (0.216, 0.248) for the t-test.
There is sufﬁcient evidence to conclude that the median paint
thickness is larger than 0.2 mm.
15.1.5
(a)
S(18.0) = 14
(b) The exact p-value is 0.115.
(c) 0.116
(d)
S+(18.0) = 37
(e) 0.012
15.1.7
It is reasonable to assume that the differences of the data have a
symmetric distribution in which case the signed rank test can be
used.
The p-values for the hypotheses
H0 : μA −μB = 0
versus
HA : μA −μB ̸= 0
are 0.296 for the sign test and 0.300 for the signed rank test.
Conﬁdence intervals for μA −μB with a conﬁdence level of at
least 95% are
(−1.0, 16.0) for the sign test and
(−6.0, 17.0) for the signed rank test.

816
ANSWERS TO ODD-NUMBERED PROBLEMS
There is not sufﬁcient evidence to conclude that there is a differ-
ence between the two assembly methods.
15.1.9
The p-values for the hypotheses
H0 : μA −μB = 0
versus
HA : μA −μB ̸= 0
are 0.003 for the sign test and 0.002 for the signed rank test.
Conﬁdence intervals for μA −μB with a conﬁdence level of at
least 95% are
(−13.0, −1.0) for the sign test and
(−12.0, −3.5) for the signed rank test.
15.1.11 The p-values for the hypotheses
H0 : μA −μB = 0
versus
HA : μA −μB ̸= 0
are 0.541 for the sign test and 0.721 for the signed rank test.
Conﬁdence intervals for μA −μB with a conﬁdence level of at
least 95% are
(−13.6, 7.3) for the sign test and
(−6.6, 6.3) for the signed rank test.
There is not sufﬁcient evidence to conclude that there is a differ-
ence between the two ball types.
15.2
Comparing Two Populations
15.2.1
(c) The Kolmogorov-Smirnov statistic is M = 0.2006.
There is sufﬁcient evidence to conclude that the two distri-
bution functions are different.
15.2.3
The Kolmogorov-Smirnov statistic is M = 0.40.
There is sufﬁcient evidence to conclude that the two distribution
functions are different.
15.2.5
(b)
SA = 245
(c) UA = 140
(d) The value of UA is consistent with the observations from
population A being larger than the observations from popu-
lation B.
(e) The p-value is 0.004.
There is sufﬁcient evidence to conclude that the observations
from population A tend to be larger than the observations
from population B.
15.2.7
(c) The Kolmogorov-Smirnov statistic is M = 0.218.
There is some evidence that the two distribution functions
are different, although the evidence is not overwhelming.
(d)
SA = 6555.5
UA = 3705.5
The value of UA is consistent with the observations from pro-
duction line A being larger than the observations from
production line B.
The two-sided p-value is 0.027.
A 95% conﬁdence interval for the difference in the popula-
tion medians is (0.003, 0.052).
15.3
Comparing Three or More Populations
15.3.1
(b)
¯r1. = 16.6
¯r2. = 15.5
¯r3. = 9.9
(c)
H = 3.60
(d) The p-value is P(χ2
2 > 3.60) = 0.165.
15.3.3
(a)
H = 1.84
The p-value is 0.399.
There is not sufﬁcient evidence to conclude that the radiation
readings are affected by the background radiation level.
15.3.5
H = 25.86
The p-value is about 0.000.
There is sufﬁcient evidence to conclude that the computer assem-
bly times are affected by the different assembly methods.
15.3.7
(a)
¯r1. = 2.250
¯r2. = 1.625
¯r3. = 3.500
¯r4. = 2.625
(b)
S = 8.85
(c) The p-value is 0.032.
15.3.9
S = 12.25
The p-value is 0.002.
There is sufﬁcient evidence to conclude that there is a difference
between the radar systems.
15.3.11 S = 37.88
The p-value is about 0.000.
There is sufﬁcient evidence to conclude that there is a difference
in the performances of the agents.
Chapter 16
Quality Control Methods
16.2
Statistical Process Control
16.2.1
(a) The center line is 10.0 and the control limits are 9.7 and 10.3.
(b) The process is declared to be out of control at ¯x = 9.5 but
not at ¯x = 10.25.
(c) The probability that an observation lies outside the control
limits is 0.0668.
The average run length for detecting the change is 15.0.
16.2.3
(a) The probability that an observation lies outside the control
limits is 0.0456.
(b) The probability that an observation lies outside the control
limits is 0.1600.
The average run length for detecting the change is 6.25.
16.2.5
The probability that all eight points lie on the same side of the
center line and within the control limits is 0.0076.

ANSWERS TO ODD-NUMBERED PROBLEMS
817
16.3
Variable Control Charts
16.3.1
(a) The ¯X-chart has a center line at 91.33 and control limits at
87.42 and 95.24.
The R-chart has a center line at 5.365 and control limits at 0
and 12.24.
(b) No
(c) The process can be declared to be out of control due to an
increase in the variability.
(d) The process can be declared to be out of control due to an
increase in the variability and a decrease in the mean value.
(e) There is no evidence that the process is out of control.
(f) The process can be declared to be out of control due to an
increase in the mean value.
16.3.3
(a) The ¯X-chart has a center line at 2.993 and control limits at
2.801 and 3.186.
The R-chart has a center line at 0.2642 and control limits at
0 and 0.6029.
(b) There is no evidence that the process is out of control.
16.4
Attribute Control Charts
16.4.1
The p-charthasacenterlineat0.0500andcontrollimitsat0.0000
and 0.1154.
(a) No
(b) It is necessary that x ≥12.
16.4.3
The c-chart has a center line at 12.42 and control limits at 1.85
and 22.99.
(a) No
(b) At least 23.
16.5
Acceptance Sampling
16.5.1
(a) With p0 = 0.06 there would be 3 defective items in the batch
of N = 50 items.
The producer’s risk is 0.0005.
(b) With p1 = 0.20 there would be 10 defective items in the
batch of N = 50 items.
The consumer’s risk is 0.952.
Using a binomial approximation these probabilities are estimated
to be 0.002 and 0.942.
16.5.3
(a) The producer’s risk is 0.000.
(b) The consumer’s risk is 0.300.
16.5.5
c = 6
Chapter 17
Reliability Analysis and Life Testing
17.1
System Reliability
17.1.1
r = 0.9985
17.1.3
(a) If r1 is the individual reliability then it is necessary that
r1 ≥0.9873.
(b) If r1 is the individual reliability then it is necessary that
r1 ≥0.5271.
(c) r1 ≥r 1/n
r1 ≥1 −(1 −r)1/n
17.1.5
r = 0.820
17.2
Modeling Failure Rates
17.2.1
(a) 0.329
(b) 0.487
(c) 0.264
17.2.3
24.2 minutes
17.2.5
(a) 0.214
(b) 0.448
(c) 37.5
(d) 12.2.
17.2.7
(a) 0.142
(b) 0.344
(c) 3.54
(d) h(t) = 0.0469 × t2
(e) 2.78
17.3
Life Testing
17.3.1
(a) (86.4, 223.6)
(b) The value 150 is within the conﬁdence interval, so the claim
is plausible.
17.3.3
(a) (3.84, 9.93)
(b) The value 10 is not included within the conﬁdence interval,
and so it is not plausible that the mean time to failure is
10 hours.
17.3.5
(b) (0.457, 0.833)

INDEX
Boldface indicates references to tables.
acceptable quality level (AQL), 758–759
acceptance sampling, 736, 758–763
alternative hypotheses, 349
analysis of variance (ANOVA), 494
one-factor, 494–516
analysis of variance table for, 507–511
model assumptions in, 516
one-factor layouts for, 494–499
pairwise comparisons of factor level
means in, 511–514
partitioning total sum of squares in,
499–506
sample size determination in, 515–516
randomized block designs in, 494, 520–535
analysis of variance tables for, 529–533
model assumptions in, 535
one-factor layouts with blocks, 520–525
pairwise comparisons of factor level
means in, 533–534
partitioning total sum of squares in,
525–528
analysis of variance tables
for multiple linear regression, 613
in one-factor analysis of variance, 507–511
in randomized block designs, 529–533
simple linear regression and, 579–584
in three-factor experiments, 681
in two-factor experiments, 661–670
association between variables, 596–597
attribute control charts, 752–757
c-charts, 755–757
p-charts, 752–755
average run length (ARL), 740
backwards elimination, 614
balanced data sets, 494
balanced design, 675
bar charts, 272–274
histograms distinguished from, 275
batches, of products, 758
bell-shaped curves. See normal distributions
Bernoulli, James (Jakob, Jacques), 148
Bernoulli random variables, 147–148, 432
deﬁnition of, 148
Bernoulli trials, 158
geometric distributions of, 160–162
beta distributions, 209–212
bias, deﬁnition of, 301
biased point estimates, 302
bimodal histograms, 278
binary variables, 269
binomial distributions, 147–159
approximating, using normal distribution,
240–243
Bernoulli random variables, 147–148
binomial distribution, 148–153
deﬁnition of, 148
examples of, 153–159
inferences involving, 432
negative, 162–164
examples of, 164–167
binomial random variables, 148
bivariate normal distributions, 258–260
blind experiments, 391
block effects, 521
blocking (in experimental design), 395, 494
blocking variables, 520
block sum of squares (SSBl), 526–527
Bortkiewicz, L., 174
boxplots, 284–285
card draws, 3–4
categorical data, 269
analysis of, 432
bar charts for, 272
pie charts for, 274
causality, 596–597
c-charts, 755–757
cell frequencies, 467, 478
cell means, 653–655
in three-factor experiments, 680–681
cell probabilities, 467
cells, 432, 651
experiments with one observation per
cell, 674
censored observations, 780–785
central limit theorem, 216, 240, 243–244
experiment using, 245–247
chance, 1
Chebyshev, Pafnutii Lvovich, 108
Chebyshev’s inequality, 107–109
deﬁnition of, 107
chi-square distribution, 253–255
critical points for, 789
for Friedman test, 731
chi-square goodness of ﬁt tests, 432
chi-square statistics, 469–470
coefﬁcient of determination (R2), 581–583
coefﬁcient of variation, 285–286
coin tosses, 2–3
column marginal frequencies, 478
combinations, 59–63
deﬁnition of, 61
of events, 15–31
examples of, 21–28
intersection of, 15–18
of three or more events, 28–31
unions of, 18–21
examples of, 62–63
linear, of random variables, 132–137
complements of events, 8–13
deﬁnition of, 9
completely randomized design, 494, 495
complex systems, system reliability testing of
components in, 769–771
components, system reliability testing of
for complex systems, 769–771
failure rates of, 772–776
in parallel, 768–769
in series, 767–768
computer simulations, 771
conditional probabilities, 33–38
deﬁnition of, 33–34
examples of, 34–38
conditional probability distributions, 120–122
deﬁnition of, 121
conﬁdence bands, 571
for distribution functions, 695, 698–699
Kolmogorov conﬁdence bands, 699
conﬁdence intervals
deﬁnition of, 333
hypothesis testing and, 369
for mean failure time, 778
for population means, 333–346
effect of sample size on, 338–340
examples of, 340–341
one-sided conﬁdence intervals for,
342–345
z-intervals for, 345–346
for population proportions, 434–439
difference between two, 456–460
for product limit estimators, 783
for response variables, 616
for sign test, 705
818

INDEX
819
conﬁdence levels, 333
effect of, on conﬁdence interval length, 336
consumer’s risks, 759
contingency tables, 432
one-way, goodness of ﬁt tests for, 466–476
two-way, testing for independence in,
478–486
continuous probability distributions, 186
beta distributions, 209–212
exponential distributions, 190–198
deﬁnition of, 190–192
examples of, 194–198
memoryless property of, 192–193
Poisson process for, 193–194
gamma distributions, 199–203
deﬁnition of, 199–202
examples of, 202–203
uniform distributions, 186–189
deﬁnition of, 186–188
examples of, 188–189
Weibull distributions, 204–208
deﬁnition of, 204–206
examples of, 206–208
continuous random variables
cumulative distribution function for, 87–90
examples of, 81–83
expected values for, 96–99
probability density function for, 83–87
continuous variables, 269
histograms for, 275
control charts, 738–740
attribute, 752–757
c-charts, 755–757
p-charts, 752–755
properties of, 740–742
R-charts, 744–745
variable, 742–752
¯X-charts, 743–745
control groups, 390–391
control limits, 738–740
correlation analysis, 594–599
correlation coefﬁcients, 543
correlations
among input variables in multiple linear
regression, 637–638
between random variables, 125
counting techniques, 56–63
examples of, 60–63
multiplication rule in, 56–59
permutations and combinations for, 59–62
covariance
among random variables, 123–127
deﬁnition of, 125
critical points
for chi-square distribution, 789
for F-distribution, 791–793
for Kolmogorov and Kolmogorov-Smirnov
procedures, 795
for studentized range distribution, 794
for t-distribution, 256, 790
cross product (interaction) terms, 609
cumulative distribution functions
deﬁnition of, 77
empirical cumulative distribution function,
695–702
for random variables
continuous, 87–90
discrete, 77–80
of standard normal distribution, 787–788
data
censored, 780–785
outliers, 278–279
types of, 269
data presentation, 272–279
bar charts and Pareto charts, 272–274
boxplots, 284–285
control charts for, 738–740
attribute, 752–757
c-charts, 755–757
p-charts, 752–755
properties of, 740–742
R-charts, 744–745
variable, 742–752
¯X-charts, 743–745
histograms, 245, 275–278
outliers in, 278–279
pie charts, 274–275
data sets, 267
balanced and unbalanced, 494
outliers in, 278–279
degrees of freedom
for chi-square distribution, 253, 254
for F-distribution, 257–258
in one-factor analysis of variance, 507
for t-distributions, 255
for two-sided t-tests, 356
Deming, W. Edwards, 736
dependent variables
residuals of, 585
in simple linear regression, 543
descriptive statistics, 267
data presentation of, 272–279
bar charts and Pareto charts,
272–274
histograms, 275–278
outliers in, 278–279
pie charts, 274–275
examples of, 288–292
for experimentation, 267–271
examples of, 269–271
samples used for, 267–269
means
of random variables (expected values),
93–101
in standard normal distributions, 217
medians
deﬁnition of, 99
of random variables, 99–101
quantiles
deﬁnition of, 110
of random variables, 109–112
quartiles, 110
range and interquartile range, 110, 111
sample statistics, 280–287
boxplots, 284–285
coefﬁcient of variation, 285–286
examples of, 286–287
means, 281
medians, 281
modes, 282
quantiles, 284
trimmed means, 282
variance, 282–284
standard deviations
deﬁnition of, 104
of random variables, 104
in standard normal distributions, 217
variance
covariance and, 123–127
deﬁnition of, 103
of random variables, 102–112
dice, 3
discrete data analysis, 432
comparing two population proportions,
455–464
conﬁdence intervals for, 456–460
hypothesis testing of, 460–464
goodness of ﬁt tests for one-way
contingency tables, 466–476
independence in two-way contingency
tables, testing for, 478–486
inferences involving population proportions,
432–452
conﬁdence intervals for, 434–439
hypothesis testing on, 439–449
sample size calculations for, 449–452
discrete probability distributions
binomial, 147–159
Bernoulli random variables, 147–148
binomial distribution, 148–153
examples of, 153–159
geometric and negative binomial
distributions, 160–167
examples of, 164–167
geometric distributions, 160–162
negative binomial distributions, 162–164
hypergeometric distributions, 168–172
multinomial distributions, 179–182
Poisson distributions, 173–178
discrete random variables, 71–80
conditional probability distributions for, 120
cumulative distribution function for, 77–80
examples of, 71–74

820
INDEX
discrete random variables (continued)
expected values for, 93–96
inferences involving, 432
probability mass function for, 74–77
examples of, 75–77
See also discrete data analysis
distribution-free methods. See nonparametric
statistical analyses
distribution functions, 695–702
Kolmogorov-Smirnov test of, 717–720
double-blind experiments, 391
efﬁciency, of unbiased point estimates,
306–307
elementary (simple) events, 9–10
empirical cumulative distribution function,
284, 695–702
for censored data, 780–781
Kolmogorov-Smirnov test and, 717
errors, Type I and Type II, 365
error sum of squares (SSE)
in one-factor analysis of variance, 500–501
in randomized block designs, 527
in two-factor experiments, 663
error terms, 495
in simple linear regression, 543
in two-factor experiments, 655
error variance, in simple linear regression,
543, 544
estimation, 296
constructing parameter estimates, 320–326
examples of, 324–326
maximum likelihood estimates, 322–324
method of moments, 320–322
deﬁnition of, 298
empirical cumulative distribution function
for, 695–699
point estimates, 296–301
estimation of, 298–301
minimum variance estimates, 305–309
of parameters, 296–297
properties of, 301–305
statistics distinguished from
parameters, 297–298
of population parameters, 320–326
maximum likelihood estimation,
322–326
method of moments, 320–322
product limit estimators, 780–783
for sampling distributions, 311–317
sample means, 312–314
sample proportions, 311–312
sample variance, 314–315
in simple linear regression, 551–559
events
combinations of, 15–31
examples of, 21–28
intersection of, 15–18
of three or more events, 28–31
unions of, 18–21
complements of, 8–13
deﬁnition of, 9
examples of, 10–13
intersection of, 15–18
probabilities of, 40–48
outliers as, 279
expected values, of random variables
continuous, 96–99
discrete, 93–96
medians, 99–101
experimental conﬁgurations (cells), 651
experimental design
completely randomized design, 494
control groups in, 390–391
for multifactor experiments
with three or more factors, 679–687
with two factors, 650–653
for 2k experiments, 687–690
paired samples versus independent samples
in, 394–397
randomized block designs in, 520–535
unbalanced, 675
experiments and experimentation, 267–271
censored data from, 780–785
control groups in, 390–391
deﬁnition of, 1
descriptive statistics for, 267–271
examples of, 269–271
samples for, 267–269
hypothesis testing for, 349–378
calculation of p-values for, 353–365
hypotheses in, 349–350
interpretation of p-values for,
350–353
signiﬁcance levels for, 365–375
z-tests for, 375–378
multifactor experiments
with three or more factors, 679–690
with two factors, 650–677
probability values for outcomes of, 4–6
exponential distributions, 190–198
deﬁnition of, 190–192
examples of, 194–198
for hazard rates, 776
for life testing, 777–779
memoryless property of, 192–193
for modeling failure rates, 773–774
Poisson process for, 193–194
extrapolation, from simple linear
regression, 556
factor effects, 521
factorials, 60
factor level means, pairwise comparisons of
in one-factor analysis of variance, 511–514
in randomized block designs, 533–534
in two-factor experiments, 670–673
failure rates, modeling, 772–776
failure time distributions, 777
F-distribution, 257–258
critical points for, 791–793
ﬁtted models, 612–615
forward selection, 614–615
fractional factorial design, 690
Fr´echet, Maurice, 205
Friedman test, 726, 729–731
F-statistic (F-tests)
for analysis of variance, 508
for one-factor layout, 509
for randomized block designs, 530
for sum of squares in analysis of variance
tables, 580
games of chance
binomial distributions in, 156–158
conditional probabilities in, 36–38
cumulative distribution functions in, 80,
90–91
events in, 12
expected values in, 95–96
expected values and variance in, 137–139
geometric distributions in, 167
hypergeometric distributions in, 172
independence in, 124–125
inferences on population proportions
for, 437
intersections and unions of events
in, 24–28
multinomial distributions in, 182
normal distribution used to approximate
binomial distribution for, 242–243
one-sided hypothesis testing in, 448–449
permutations and combinations in, 62–63
probabilities of events intersections in,
47–48
probability density functions in, 86–87,
98–99
probability values for outcomes of, 6–8
random variables in, 73–74
sample proportions in, 312
standard deviations in, 106–107
uniform distributions in, 189
gamma distributions, 199–203
chi-square distribution as, 254
deﬁnition of, 199–202
examples of, 202–203
for modeling failure rates, 774
gamma function, 202–203
Gauss, Carl Friedrich, 216
Gaussian distributions. See normal distributions
general multiplication law, 40–41
geometric distributions, 160–162
examples of, 164–167
hypergeometric distributions, 168–172

INDEX
821
goodness of ﬁt tests, for one-way contingency
tables, 466–476
one-way classiﬁcations for, 467–474
testing distributional assumptions,
474–476
Gosset, William Sealey, 255
graphical presentations of data. See data
presentation
hazard rates, 772, 774–776
histograms, 245, 275–278
bar charts distinguished from, 274
homogeneity, hypothesis of, 466–467
hypergeometric distributions, 168–172
deﬁnition of, 168–170
examples of, 170–172
hypotheses, 349–353
hypothesis testing, 349–378
calculation of p-values for, 353–365
hypotheses in, 349–350
interpretation of p-values for, 350–353
on population proportions, 439–449
on difference between two, 460–464
signiﬁcance levels for, 365–375
z-tests for, 375–378
increasing failure rate (IFR) models, 776
independence
among random variables, 123–127
in two-way contingency tables, testing for,
478–486
independent events, 38
deﬁnition of, 42
probability of intersections of, 41–43
independent random variables, 123
central limit theorem for, 243–244
independent normal random variables
averaging, 232
linear combinations of, 231
sum of two, 230
independent samples
analysis of, 402–403
general procedure for, 403–407
pooled variance procedure for,
407–410
sample size calculations for, 418–419
z-procedure for, 410–417
paired samples versus, 394–397
independent variables, in simple linear
regression, 543
indicator variables, 610
inferences
guide to methodologies for, 331–332
in parametric and nonparametric statistical
analyses, 694
on population means, 334
statistical, 267, 268
inﬂuential points, 640–642
interaction effects, in two-factor experiments,
655–656
sum of squares of, 663
testing for, 664–665
interaction (cross product) terms, 609
intercept parameter (β0), 544
interquartile range, 110
deﬁnition of, 111
sample interquartile range, 284
intersections of events, 15–18
deﬁnition of, 15
examples of, 21–28
probabilities of, 40–48
examples of and probability trees, 43–48
general multiplication law in, 40–41
for independent events, 41–43
intrinsically linear models, 590–593
joint cumulative distribution function,
114–115
jointly distributed random variables, 114–127
conditional probability distributions,
120–122
correlation analysis for, 594–599
examples of, 115–116
independence and covariance of, 123–127
joint probability distributions, 114–116
marginal probability distributions, 117–120
joint probability distributions, 114
deﬁnition of, 115
joint probability mass function, 114
Kaplan, Edward, 781
Kolmogorov, Andrei Nikolaevich, 699, 718
Kolmogorov conﬁdence bands, 699
Kolmogorov procedure, critical points
for, 795
Kolmogorov-Smirnov test, 717–720
critical points for, 795
Kruskal-Wallis test procedure, 726–729
least squares method, 610–612
left-censored observations, 780
left-skewed histograms, 278
leverage values, 640
life testing, 777–785
likelihood ratio chi-square statistic, 469–470
linear combinations of random variables,
132–137
normal, 229–237
linear functions of random variables, 129–131
linear models, transformations of variables in,
590–593
linear regression. See multiple linear
regression; simple linear regression
lognormal distributions, 251–253
for life testing, 779
for modeling failure rates, 774
lots, of products, 758
lower control limit (LCL), 738
lower quartile, 110
main effects, in two-factor experiments,
655–656
sum of squares for, 663
testing for, 665
Mann, 721
marginal distributions, 120, 121
marginal probability distributions, 117–120
deﬁnition of, 117
matrix algebra formulation, of multiple linear
regression, 628–636
maximum likelihood estimates, for parameter
estimates, 322–324
examples of, 324–326
for one parameter, 322
for two parameters, 323
mean failure time, 778
means
cell means, 653–655
population means (μ)
comparison of two, 389–423
conﬁdence level construction for,
333–346
hypothesis testing of, 349–378
inferences regarding, 333
point estimate of, 303
of random variables (expected values)
continuous, 96–99
discrete, 93–96
medians, 99–101
sample means, 281, 312–314
trimmed, 282
in sign test, 703
in standard normal distributions, 217
See also population means
mean square error (MSE), 308–309
in one-factor analysis of variance, 507–511
mean squares for treatments (MST), 507–511
mean time to failure (MTTF), 773
medians
deﬁnition of, 99
of random variables, 99–101
sample medians, 281
signed rank test for, 709–715
in sign test, 703
memoryless property, 192–193
Meter, Paul, 781
method of least squares, 610–612
method of moments for parameter estimates,
320–322
for one parameter, 320–321
for two parameters, 321–322
minimum variance estimates, 305–309
minimum variance unbiased estimate
(MVUE), 306

822
INDEX
model assumptions
in one-factor analysis of variance, 516
in randomized block designs, 535
in simple linear regression, 543–550
modeling
failure rates, 772–776
in simple linear regression, 543
two-factor experiments, 661
procedures for, 673–677
models
for life testing, 777–779
in multiple linear regression, 608–616
evaluating model adequacy in,
637–642
in simple linear regression, 543–550
intrinsically linear models, 590–593
two-factor experiments, 653–661, 673–677
modes
in histograms, 278
sample modes, 282
multicolinearity of input variables, in multiple
linear regression, 637–638
multifactor experiments, 650
with three or more factors, 679–690
three-factor experiments, 680–687
2k experiments, 687–690
with two factors, 650–677
analysis of variance tables for,
661–670
experimental design for, 650–653
modeling procedures and residual
analysis for, 673–677
models for, 653–661
pairwise comparisons of factor level
means, 670–673
multinomial distributions, 179–182, 466
multiple linear regression, 608
evaluating model adequacy in, 637–642
inﬂuential points in, 640–642
multicolinearity of input variables in,
637–638
residual analysis in, 638–640
examples of, 618–627
matrix algebra formulation of, 628–636
model for, 608–616
analysis of ﬁtted model in, 612–615
inferences on response variables in,
615–616
simple linear regression distinguished
from, 543
multiplication law, general, 40–41
multiplication rule, 56–59
deﬁnition of, 57
examples of, 57–59
multivariate normal distributions,
258–260
mutually exclusive events, 17
unions of, 30
negative binomial distributions, 162–164
deﬁnition of, 163
examples of, 164–167
negatively skewed histograms, 278
nominal data, 269
nonlinear functions of random variables,
137–139
nonlinear regression, 608, 643–647
nonparametric statistical analyses, 694
comparing three or more populations,
726–731
for one-way layouts, 726–729
for randomized block designs, 729–731
comparing two populations, 716–724
Kolmogorov-Smirnov test, 717–720
rank sum test, 721–724
for single population studies, 695–715
distribution functions for, 695–702
signed rank test, 709–715
sign test, 702–709
normal distributions, 216
approximating distributions with, 240–250
of binomial distributions, 240–243
central limit theorem for, 243–244
examples of, 247–250
cumulative distribution function
of, 787–788
deﬁnition of, 216–217
distributions related to, 251–260
chi-square distribution, 253–255
F-distribution, 257–258
lognormal distributions, 251–253
multivariate normal distributions,
258–260
t-distributions, 255–257
examples of, 225–227
linear combinations of normal random
variables in, 229–237
probability calculations using, 216–225
standard normal distributions, 217–221
normal equations, 553, 611
normal probability plots (normal scores
plots), 586
normal random variables, 224
linear combinations of, 229–237
examples of, 232–237
null hypothesis, 349
in analysis of variance, 496
for control limits, 738–739
for empirical cumulative distribution
function, 699–702
p-values to measure plausibility of,
350–352
calculation of, 353–365
for rank sum test, 723
for sign test, 702–705
numerical data, 269
histograms for, 275
observations
censored, 780–785
distributional assumption for, 694
in three-factor experiments, 680
in two-factor experiments, 674
observed level of signiﬁcance (p-value), 350
one-factor analysis of variance, 494–516
analysis of variance table for, 507–511
model assumptions in, 516
one-factor layouts for, 494–499
pairwise comparisons of factor level means
in, 511–514
partitioning total sum of squares in, 499–506
sample size determination in, 515–516
one-factor layouts, 494–495
with blocks, 520–525
sum of squares partition for, 504
one-sided conﬁdence intervals
for population means, 342–345
for population proportions, 437–439
one-sided hypothesis testing, 359–365
for population proportions, 445–449
signiﬁcance levels for, 370–374
z-tests for, 377
one-sided t-intervals, 344, 345
one-sided t-tests, 359–365
one-sided z-intervals, 346
one-sided z-tests, 377
one-way classiﬁcations, 467–474
one-way contingency tables, goodness of ﬁt
tests for, 466–476
one-way classiﬁcations for, 467–474
testing distributional assumptions, 474–476
one-way layouts
nonparametric statistical analyses for,
726–729
one-factor analysis of variance for,
494–495
operating characteristic curves, 760
outliers, 278–279
eliminated in trimmed means, 282
inﬂuential points as, 640–642
paired experiments, 394
paired samples
analysis of, 397–400
independent samples versus, 394–397
pairwise comparisons of factor level means
in one-factor analysis of variance, 511–514
in randomized block designs, 533–534
in two-factor experiments, 670–673
pairwise intervals, 514
parallel, system reliability testing of
components in, 768–769
parameterization
for beta distributions, 209
for gamma distribution, 200
for Weilbull distributions, 206

INDEX
823
parameters
deﬁnition of, 296
estimating, 320–326
examples of, 324–326
maximum likelihood estimates, 322–324
method of moments, 320–322
in simple linear regression, 551–559
in parametric and nonparametric statistical
analyses, 694
point estimates of, 296–298
statistics distinguished from, 296
parametric inference methods, 694
Pareto, Vilfredo, 273
Pareto charts, 272–274
partitioning total sum of squares
in one-factor analysis of variance, 499–506
in randomized block designs, 525–528
partitions, of sample space, 30
p-charts, 752–755
Pearson, Karl, 469
Pearson chi-square statistic, 469–470
Pearson product moment correlation coefﬁcient
(sample correlation coefﬁcient),
595, 597
percentiles, 109
sample percentiles, 284
permutations, 59–62
deﬁnition of, 60
pie charts, 274
placebos, 390
point estimates
deﬁnition of, 298
of parameters, 296–297
properties of, 301–309
minimum variance estimates, 305–309
unbiased estimates, 301–305
of statistics, 297–298
Poisson, Simeon Denis, 174
Poisson distributions, 173–178
deﬁnition of, 173–175
examples of, 175–178
testing for, 474
Poisson process, 193–194
Poisson random variables, 174
polynomial regression models, 609
pooled variance procedure, 402, 407–410, 424
population means (μ)
comparison of two, 389–423
independent samples, 402–419
paired samples, 397–400
two-sample problems, 389–397
conﬁdence level construction for,
333–346
effect of sample size on, 338–340
examples of, 340–341
one-sided conﬁdence intervals for,
342–345
z-intervals for, 345–346
hypothesis testing of, 349–378
calculation of p-values for, 353–365
hypotheses in, 349–350
interpretation of p-values for,
350–353
signiﬁcance levels for, 365–375
z-tests for, 375–378
inferences regarding, 333
point estimate of, 303
process for making inferences on,
381–383
population parameters. See parameters
population proportions
comparing two, 455–464
conﬁdence intervals for, 456–460
hypothesis testing of, 460–464
in discrete data analysis, 432–452
conﬁdence intervals for, 434–439
hypothesis testing on, 439–449
sample size calculations for, 449–452
populations
deﬁnition of, 268
parameters of, 296
population variance
point estimate of, 304
in pooled variance procedure,
407–410
positively skewed histograms, 278
posterior probabilities, 50–55
calculation of, 51–52
examples of, 52–55
law of total probability in, 50–51
power levels, 374–375
prediction bands, 576
prediction intervals, 575–578
probability density functions
for continuous random variables,
83–87
deﬁnition of, 83
for signed rank test, 709
probability mass functions
deﬁnition of, 75
for discrete random variables, 74–77
for samples, 267
probability theory
conditional probabilities, 33–38
counting techniques, 56–63
events, 8–15
combinations of, 15–31
intersections of, 40–48
probabilities, 1–8
posterior, 50–55
probability trees, 43–47
probability values, 4–6
examples of, 5–8
process control. See statistical process control
producer’s risks, 759
product limit estimators, 780–783
proportions
population proportions
comparing two, 455–464
in discrete data analysis, 432–452
sample proportions, 311–312
p-values
calculation of, 353–365
deﬁnition of, 354
interpretation of, 350–353
in one-factor analysis of variance, 502–504
for one-sided hypothesis testing, 359–365
quadratic regression models, 609
quality control, 736
acceptance sampling in, 758–763
attribute control charts for, 752–757
c-charts, 755–757
p-charts, 752–755
failure rates modeled for, 772–776
Pareto charts used in, 272
statistical process control for, 736–741
control charts, 736–738, 740–741
control limits, 738–740
system reliability and, 766–771
for complex systems, 769–771
for components in parallel, 768–769
for components in series, 767–768
variable control charts for, 742–751
R-charts, 744–745
¯X-charts, 743–745
quantiles
deﬁnition of, 110
empirical cumulative distribution function
for, 696
of random variables, 109–112
sample quantiles, 284
sample quartiles, 284
in boxplots, 284–285
quartiles, 110
randomization of subjects, 390
randomized block designs, 494, 520–535
analysis of variance tables for, 529–533
Friedman test for, 729–731
model assumptions in, 535
one-factor layouts with blocks, 520–525
pairwise comparisons of factor level means
in, 533–534
partitioning total sum of squares in,
525–528
random numbers, 245
random samples, 268
random variables
Bernoulli, 147–148
combinations and functions of, 129–139
linear combinations of, 132–137
linear functions of, 129–131
nonlinear functions of, 137–139

824
INDEX
random variables (continued)
continuous
cumulative distribution function for,
87–90
examples of, 81–83
expected values for, 96–99
probability density function for, 83–87
deﬁnition of, 71
discrete, 71–80
cumulative distribution function for,
77–80
examples of, 71–74
expected values for, 93–96
probability mass function for, 74–77
examples of, 71–74
expected values of, 93–101
for continuous random variables, 96–99
for discrete random variables, 93–96
medians, 99–101
independent, 123
jointly distributed, 114–127
conditional probability distributions,
120–122
independence and covariance of, 123–127
joint probability distributions, 114–116
marginal probability distributions,
117–120
normal, 224
linear combinations of, 229–237
Poisson, 174
standardizing, 222, 223
symmetric, 97–98
variance of, 102–112
calculation of, 104–107
Chebyshev’s inequality, 107–109
deﬁnition and interpretation of, 102–104
quantiles of, 109–112
range
interquartile range, 110
deﬁnition of, 111
sample interquartile range, 284
ranks, 710
rank sum test, 721–724
R-charts, 744–745
regression
multiple linear regression
evaluating model adequacy in, 637–642
examples of, 618–627
matrix algebra formulation of, 628–636
model for, 608–616
simple linear regression distinguished
from, 543
nonlinear regression, 643–647
simple linear regression, 543
analysis of variance tables and, 579–584
ﬁtting regression lines in, 551–560
model deﬁnition and assumptions in,
543–550
prediction intervals for future response
values, 575–578
regression line inferences in, 569–574
residual analysis and, 585–589
slope parameter inferences in, 561–568
variable transformations for, 590–593
regression lines, in simple linear regression,
551–559
inferences on, 569–574
relative efﬁciency of unbiased point estimates,
306–307
reliability analysis
life testing and, 777–785
modeling failure rates in, 772–776
for system reliability, 766–771
replications measurements, 651
residual analysis
in multiple linear regression, 638–640
in simple linear regression, 585–589
in two-factor experiments, 673–677
residuals, 585
response surface model, 609–610
response values, 575–578
response variables
in multifactor experiments, 650
in multiple linear regression, 608, 615–616
right-censored observations, 780
right-skewed histograms, 278
robust statistics, 282
row marginal frequencies, 478
sample correlation coefﬁcient (Pearson product
moment correlation coefﬁcient),
595, 597
sample interquartile range, 284
sample means, 281, 312–314
in boxplots, 284–285
trimmed, 282
as unbiased estimate, 282
sample medians, 281
in boxplots, 284–285
sample modes, 282
sample percentiles, 284
sample proportions, 311–312
sample quartiles, 284
in boxplots, 284–285
samples, 267–269
deﬁnition of, 268
statistics describing, 297–298
sample size
in analysis of independent samples,
418–419
in deciding between t-tests and z-tests, 382
effect on conﬁdence intervals of, 338–339
for empirical cumulative distribution
function, 696
for one-factor analysis of variance, 515
point estimates dependent upon, 309
for population proportions, 449–452
power levels and, 375
sample spaces, 1–4
counting techniques for, 56–63
deﬁnition of, 1
examples of, 1–4
partitions of, 30
samples quartiles, 284
sample statistics, 280–287
boxplots, 284–285
coefﬁcient of variation, 285–286
examples of, 286–287
means, 281
medians, 281
modes, 282
quantiles, 284
trimmed means, 282
variance, 282–284
sample trimmed means, 282
sample variance, 282–284, 314–315
sampling
acceptance sampling, 758–763
in hypergeometric distributions, with
replacement, 168–170
sampling distributions, 311–317
sample means, 312–314
sample proportions, 311–312
sample variance, 314–315
scale parameter, 205
screening experiments, 650
series, system reliability testing of components
in, 767–768
shape parameter, 205
Shewhart, Walter, 736
signed rank test (Wilcoxon one-sample test
procedure), 709–715
signiﬁcance levels, 365–375
sign test, 702–709
simple (elementary) events, 9–10
simple linear regression, 543
analysis of variance tables and, 579–584
ﬁtting regression lines in, 551–560
model deﬁnition and assumptions in,
543–550
prediction intervals for future response
values, 575–578
regression line inferences in, 569–574
residual analysis and, 585–589
slope parameter inferences in, 561–568
variable transformations for, 590–593
simple linear regression model, 543–549
simulations, 771
skewness, 278
slope parameter (β1), 544–545
inferences on
in multiple linear regression, 615
in simple linear regression, 561–568
Smirnov, 718

INDEX
825
standard deviations, 282
deﬁnition of, 104
of random variables, 104
of sample proportions (standard errors), 311
in standard normal distributions, 217
in z-tests, 375–377
standard errors
of sample means, 313
of sample proportions, 311
in simple linear regression, 571
standardized random variables, 222, 223
standardized residuals, 638
standard normal distributions, 217–222
cumulative distribution function
of, 787–788
state spaces (sample spaces), 1
statistical estimation. See estimation
statistical inference, 267, 268
guide to methodologies for, 331–332
statistical process control (SPC), 736–742
acceptance sampling distinguished
from, 758
control charts for, 736–738, 740–741
control limits for, 738–740
statistics
deﬁnition of, 298
parameters distinguished from, 296
point estimates of, 297–298
stochastic processes, 193
studentized range distribution, critical points
for, 794
success probability, point estimate of, 302
sum of squares
partitioning
in randomized block designs, 527
total, 499–506
simple linear regression and, 579–584
in three-factor experiments, 681–683
in two-factor experiments, 661–664
in variance, 282
sum of squares for error (SSE)
in multiple linear regression, 612
in simple linear regression, 500–501
sum of squares for factor A and factor B, 663
sum of squares for interactions, 663
sum of squares for regression (SSR), 580
sum of squares for treatments (SST), 500
in analysis of variance tables, 507
symmetric binomial distributions, 151
symmetric distributions
signed rank test of, 712
sign test of, 703
symmetric random variables, 97–98
symmetry, in histograms, 278
system reliability, 766–772
for complex systems, 769–771
for components in parallel, 768–769
for components in series, 767–768
t-distribution, 255–257
critical points for, 790
three-factor experiments, 679–687
time to failure, 772–774
t-intervals, 333–334
one-sided, 344, 345
two-sided, 335, 337–338
total probability, law of, 50–51
total quality management, 736
total sum of squares
partitioning
in one-factor analysis of variance,
499–506
in randomized block designs, 525–528
in simple linear regression, 501–506,
579–584
in two-factor experiments, 662
transformations of variables, 590–593
treatment sum of squares (SST)
in analysis of variance tables, 507
in one-factor analysis of variance, 499–500
in randomized block designs, 526
trimmed means, 282
as unbiased estimate, 303
t-statistic, 315
for one-sided hypothesis testing, 359–365
for two-sided hypothesis testing, 354–358
t-tests
deciding between z-tests and, 381–383
one-sided, 359–365
summary of, 382
two-sided, 356–358
Tukey intervals, 514
two-factor experiments, 650–677
analysis of variance tables for, 661–670
experimental design for, 650–653
modeling procedures and residual analysis
for, 673–677
models for, 653–661
pairwise comparisons of factor level means,
670–673
2k experiments, 687–690
two-sample problems
comparing two population means, 389–394
paired samples versus independent samples
in, 394–397
two-sample t-tests, 402
general procedure, 423
with equal variances, 409
with unequal variances, 405–406
pooled variance procedure, 424
two-sample z-tests, 402, 410–411
two-sided conﬁdence intervals, for population
proportions, 435
two-sided hypothesis testing, 354–358
for population proportions, 441–445
signiﬁcance levels for, 366–368
z-tests for, 376
two-sided t-intervals, 334, 335, 337–338
two-sided t-tests, 356–358
two-sided z-intervals, 346
two-sided z-tests, 376
two-way classiﬁcations, 478–480
two-way contingency tables, 478–486
Type I errors, 365
Type II errors, 365
power levels and, 374–375
unbalanced data sets, 494
unbalanced experimental designs, 675
unbiased estimates, 301–305
uncertainty, 1
uniform distributions, 186–189
deﬁnition of, 186–188
examples of, 188–189
unimodal histograms, 278
unions of events, 18–21
deﬁnition of, 18
examples of, 21–28
of mutually exclusive events, 30
of three events, 29
upper control limit (UCL), 738
upper quartile, 110
variable control charts, 742–752
R-charts, 744–745
¯X-charts, 743–745
variables
association and causality between, 596–597
in multiple linear regression,
multicolinearity of, 637–638
simple linear regression of relationships
among, 543
transformations of, in simple linear
regression, 590–593
See also random variables
variance, 282–284
covariance and, 123–127
deﬁnition of, 103
minimum variance estimates of, 305–309
population variance, point estimate of, 304
of random variables, 102–112
calculation of, 104–107
Chebyshev’s inequality, 107–109
deﬁnition and interpretation of, 102–104
quantiles of, 109–112
sample variance, 314–315
See also analysis of variance
Venn diagrams, 2
Weibull, Ernst Hjalmar Waloddi, 205
Weibull distributions, 204–208
deﬁnition of, 204–206
examples of, 206–208
for hazard rates, 776
for modeling failure rates, 774

826
INDEX
Whitney, Hassler, 721
Wilcoxon, 721
Wilcoxon one-sample test procedure (signed
rank test), 709–715
Wilcoxon rank sum test, 721–724
¯X-charts, 743–744
modiﬁcations of, 745
z-intervals, 346
z-tests, 375–378
deciding between t-tests and, 381–383
summary of, 383
two-sample, 410–411


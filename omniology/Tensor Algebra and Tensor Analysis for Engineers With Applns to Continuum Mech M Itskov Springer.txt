
Mikhail Itskov
Tensor Algebra and Tensor Analysis for Engineers

Mikhail Itskov
Tensor Algebra
and Tensor Analysis
for Engineers
ith Applications to Continuum Mechanics
With 13 Figures and 3 Tables
123
W

Professor Dr.-Ing. Mikhail Itskov
Eilfschornsteinstr. 18
52062 Aachen
Germany
itskov@km.rwth-aachen.de
ISBN 978-3-540-36046-9 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, speciÔ¨Åcally the rights of translation, reprinting, reuse of illustrations, recitation, broad-
casting, reproduction on microÔ¨Ålm or in any other way, and storage in data banks. Duplication of
this publication or parts thereof is permitted only under the provisions of the German Copyright Law
of September 9, 1965, in its current version, and permission for use must always be obtained from
Springer. Violations are liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
¬© Springer-Verlag Berlin Heidelberg 2007
The use of general descriptive names, registered names, trademarks, etc. in this publication does not
imply, even in the absence of a speciÔ¨Åc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Typesetting: camera-ready by the Author
Production: LE-TEX Jelonek, Schmidt & V√∂ckler GbR, Leipzig
Cover: eStudio Calamar, Spain
Printed on acid-free paper
/3100YL - 5 4 3 2 1 0
Library of Congress Control Number: 2007920572
Department of Continuum Mechanics
7
RWTH Aachen University 

Moim roditelm

Preface
Like many other textbooks the present one is based on a lecture course given
by the author for master students of the RWTH Aachen University. In spite
of a somewhat diÔ¨Écult matter those students were able to endure and, as far
as I know, are still Ô¨Åne. I wish the same for the reader of the book.
Although the present book can be referred to as a textbook one Ô¨Ånds only
little plain text inside. I tried to explain the matter in a brief way, neverthe-
less going into detail where necessary. I also avoided tedious introductions and
lengthy remarks about the signiÔ¨Åcance of one topic or another. A reader in-
terested in tensor algebra and tensor analysis but preferring, however, words
instead of equations can close this book immediately after having read the
preface.
The reader is assumed to be familiar with the basics of matrix algebra
and continuum mechanics and is encouraged to solve at least some of numer-
ous exercises accompanying every chapter. Having read many other texts on
mathematics and mechanics I was always upset vainly looking for solutions to
the exercises which seemed to be most interesting for me. For this reason, all
the exercises here are supplied with solutions amounting a substantial part of
the book. Without doubt, this part facilitates a deeper understanding of the
subject.
As a research work this book is open for discussion which will certainly
contribute to improving the text for further editions. In this sense, I am very
grateful for comments, suggestions and constructive criticism from the reader.
I already expect such criticism for example with respect to the list of references
which might be far from being complete. Indeed, throughout the book I only
quote the sources indispensable to follow the exposition and notation. For this
reason, I apologize to colleagues whose valuable contributions to the matter
are not cited.
Finally, a word of acknowledgment is appropriate. I would like to thank
Uwe Navrath for having prepared most of the Ô¨Ågures for the book. Fur-
ther, I am grateful to Alexander Ehret who taught me Ô¨Årst steps as well
as some ‚Äúdirty‚Äù tricks in LATEX, which were absolutely necessary to bring the

VIII
Preface
manuscript to a printable form. He and Tran Dinh Tuyen are also acknowl-
edged for careful proof reading and critical comments to an earlier version
of the book. My special thanks go to the Springer-Verlag and in particular
to Eva Hestermann-Beyerle and Monika Lempe for their friendly support in
getting this book published.
Aachen, November 2006
Mikhail Itskov

Contents
1
Vectors and Tensors in a Finite-Dimensional Space . . . . . . . .
1
1.1
Notion of the Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Basis and Dimension of the Vector Space . . . . . . . . . . . . . . . . . . .
3
1.3
Components of a Vector, Summation Convention . . . . . . . . . . . .
5
1.4
Scalar Product, Euclidean Space, Orthonormal Basis . . . . . . . . .
6
1.5
Dual Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.6
Second-Order Tensor as a Linear Mapping . . . . . . . . . . . . . . . . . . 12
1.7
Tensor Product, Representation of a Tensor with Respect to
a Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.8
Change of the Basis, Transformation Rules . . . . . . . . . . . . . . . . . 18
1.9
Special Operations with Second-Order Tensors . . . . . . . . . . . . . . 19
1.10 Scalar Product of Second-Order Tensors . . . . . . . . . . . . . . . . . . . . 25
1.11 Decompositions of Second-Order Tensors . . . . . . . . . . . . . . . . . . . 27
1.12 Tensors of Higher Orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2
Vector and Tensor Analysis in Euclidean Space . . . . . . . . . . . . 33
2.1
Vector- and Tensor-Valued Functions, DiÔ¨Äerential Calculus . . . 33
2.2
Coordinates in Euclidean Space, Tangent Vectors . . . . . . . . . . . . 35
2.3
Coordinate Transformation. Co-, Contra- and Mixed Variant
Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.4
Gradient, Covariant and Contravariant Derivatives . . . . . . . . . . 40
2.5
ChristoÔ¨Äel Symbols, Representation of the Covariant Derivative 44
2.6
Applications in Three-Dimensional Space: Divergence and Curl 47
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3
Curves and Surfaces in Three-Dimensional Euclidean Space 57
3.1
Curves in Three-Dimensional Euclidean Space. . . . . . . . . . . . . . . 57
3.2
Surfaces in Three-Dimensional Euclidean Space . . . . . . . . . . . . . 64
3.3
Application to Shell Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

X
Contents
4
Eigenvalue Problem and Spectral Decomposition of
Second-Order Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.1
ComplexiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.2
Eigenvalue Problem, Eigenvalues and Eigenvectors . . . . . . . . . . . 80
4.3
Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.4
Spectral Decomposition and Eigenprojections . . . . . . . . . . . . . . . 85
4.5
Spectral Decomposition of Symmetric Second-Order Tensors . . 90
4.6
Spectral Decomposition of Orthogonal
and Skew-Symmetric Second-Order Tensors . . . . . . . . . . . . . . . . . 92
4.7
Cayley-Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5
Fourth-Order Tensors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.1
Fourth-Order Tensors as a Linear Mapping . . . . . . . . . . . . . . . . . 99
5.2
Tensor Products, Representation of Fourth-Order Tensors
with Respect to a Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.3
Special Operations with Fourth-Order Tensors . . . . . . . . . . . . . . 102
5.4
Super-Symmetric Fourth-Order Tensors . . . . . . . . . . . . . . . . . . . . 105
5.5
Special Fourth-Order Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6
Analysis of Tensor Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.1
Scalar-Valued Isotropic Tensor Functions . . . . . . . . . . . . . . . . . . . 111
6.2
Scalar-Valued Anisotropic Tensor Functions . . . . . . . . . . . . . . . . . 115
6.3
Derivatives of Scalar-Valued Tensor Functions . . . . . . . . . . . . . . . 118
6.4
Tensor-Valued Isotropic and Anisotropic Tensor Functions . . . . 124
6.5
Derivatives of Tensor-Valued Tensor Functions . . . . . . . . . . . . . . 131
6.6
Generalized Rivlin‚Äôs Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7
Analytic Tensor Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.2
Closed-Form Representation for Analytic Tensor Functions
and Their Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.3
Special Case: Diagonalizable Tensor Functions . . . . . . . . . . . . . . 148
7.4
Special case: Three-Dimensional Space . . . . . . . . . . . . . . . . . . . . . 150
7.5
Recurrent Calculation of Tensor Power Series and Their
Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
8
Applications to Continuum Mechanics . . . . . . . . . . . . . . . . . . . . . 161
8.1
Polar Decomposition of the Deformation Gradient . . . . . . . . . . . 161
8.2
Basis-Free Representations for the Stretch and Rotation Tensor162
8.3
The Derivative of the Stretch and Rotation Tensor
with Respect to the Deformation Gradient . . . . . . . . . . . . . . . . . . 165

Contents
XI
8.4
Time Rate of Generalized Strains . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.5
Stress Conjugate to a Generalized Strain . . . . . . . . . . . . . . . . . . . 171
8.6
Finite Plasticity Based on the Additive
Decomposition of Generalized Strains . . . . . . . . . . . . . . . . . . . . . . 173
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235

1
Vectors and Tensors in a Finite-Dimensional
Space
1.1 Notion of the Vector Space
We start with the deÔ¨Ånition of the vector space over the Ô¨Åeld of real numbers
R.
DeÔ¨Ånition 1.1. A vector space is a set V of elements called vectors satisfying
the following axioms.
A. To every pair, x and y of vectors in V there corresponds a vector x + y,
called the sum of x and y, such that
(A.1) x + y = y + x (addition is commutative),
(A.2) (x + y) + z = x + (y + z) (addition is associative),
(A.3) there exists in V a unique vector zero 0, such that 0 +x = x, ‚àÄx ‚ààV,
(A.4) to every vector x in V there corresponds a unique vector ‚àíx such that
x + (‚àíx) = 0.
B. To every pair Œ± and x, where Œ± is a scalar real number and x is a vector in
V, there corresponds a vector Œ±x, called the product of Œ± and x, such that
(B.1) Œ± (Œ≤x) = (Œ±Œ≤) x (multiplication by scalars is associative),
(B.2) 1x = x,
(B.3) Œ± (x + y) = Œ±x + Œ±y (multiplication by scalars is distributive with
respect to vector addition),
(B.4) (Œ± + Œ≤) x = Œ±x + Œ≤x (multiplication by scalars is distributive with
respect to scalar addition),
‚àÄŒ±, Œ≤ ‚ààR, ‚àÄx, y ‚ààV.
Examples of vector spaces.
1) The set of all real numbers R.

2
1 Vectors and Tensors in a Finite-Dimensional Space
zero vector
vector addition
x
y
2.5x
2x
x
multiplication by a real scalar
‚àíx
x
negative vector
x + y = y + x
Fig. 1.1. Geometric illustration of vector axioms in two dimensions
2) The set of all directional arrows in two or three dimensions. Applying the
usual deÔ¨Ånitions for summation, multiplication by a scalar, the negative
and zero vector (Fig. 1.1) one can easily see that the above axioms hold
for directional arrows.
3) The set of all n-tuples of real numbers R:
a =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
a1
a2
.
.
an
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
.
Indeed, the axioms (A) and (B) apply to the n-tuples if one deÔ¨Ånes addi-
tion, multiplication by a scalar and Ô¨Ånally the zero tuple by
Œ±a =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
Œ±a1
Œ±a2
.
.
Œ±an
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
,
a + b =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
a1 + b1
a2 + b2
.
.
an + bn
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
,
0 =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
0
0
.
.
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
.
4) The set of all real-valued functions deÔ¨Åned on a real line.

1.2 Basis and Dimension of the Vector Space
3
1.2 Basis and Dimension of the Vector Space
DeÔ¨Ånition 1.2. A set of vectors x1, x2, . . . , xn is called linearly dependent if
there exists a set of corresponding scalars Œ±1, Œ±2, . . . , Œ±n ‚ààR, not all zero,
such that
n
	
i=1
Œ±ixi = 0.
(1.1)
Otherwise, the vectors x1, x2, . . . , xn are called linearly independent. In this
case, none of the vectors xi is the zero vector (Exercise 1.2).
DeÔ¨Ånition 1.3. The vector
x =
n
	
i=1
Œ±ixi
(1.2)
is called linear combination of the vectors x1, x2, . . . , xn, where Œ±i ‚ààR (i
= 1, 2, . . ., n).
Theorem 1.1. The set of n non-zero vectors x1, x2, . . . , xn is linearly depen-
dent if and only if some vector xk (2 ‚â§k ‚â§n) is a linear combination of the
preceding ones xi (i = 1, . . . , k ‚àí1).
Proof. If the vectors x1, x2, . . . , xn are linearly dependent, then
n
	
i=1
Œ±ixi = 0,
where not all Œ±i are zero. Let Œ±k (2 ‚â§k ‚â§n) be the last non-zero number, so
that Œ±i = 0 (i = k + 1, . . . , n). Then,
k
	
i=1
Œ±ixi = 0 ‚áíxk =
k‚àí1
	
i=1
‚àíŒ±i
Œ±k
xi.
Thereby, the case k = 1 is avoided because Œ±1x1 = 0 implies that x1 = 0
(Exercise 1.1). Thus, the suÔ¨Éciency is proved. The necessity is evident.
DeÔ¨Ånition 1.4. A basis of a vector space V is a set G of linearly independent
vectors such that every vector in V is a linear combination of elements of G.
A vector space V is Ô¨Ånite-dimensional if it has a Ô¨Ånite basis.
Within this book, we restrict our attention to Ô¨Ånite-dimensional vector spaces.
Although one can Ô¨Ånd for a Ô¨Ånite-dimensional vector space an inÔ¨Ånite number
of bases, they all have the same number of vectors.

4
1 Vectors and Tensors in a Finite-Dimensional Space
Theorem 1.2. All the bases of a Ô¨Ånite-dimensional vector space V contain
the same number of vectors.
Proof. Let G = {g1, g2, . . . , gn} and F = {f 1, f 2, . . . , f m} be two arbitrary
bases of V with diÔ¨Äerent numbers of elements, say m > n. Then, every vector
in V is a linear combination of the following vectors:
f 1, g1, g2, . . . , gn.
(1.3)
These vectors are non-zero and linearly dependent. Thus, according to The-
orem 1.1 we can Ô¨Ånd such a vector gk, which is a linear combination of the
preceding ones. Excluding this vector we obtain the set G‚Ä≤ by
f 1, g1, g2, . . . , gk‚àí1, gk+1, . . . , gn
again with the property that every vector in V is a linear combination of the
elements of G‚Ä≤. Now, we consider the following vectors
f 1, f 2, g1, g2, . . . , gk‚àí1, gk+1, . . . , gn
and repeat the excluding procedure just as before. We see that none of the
vectors f i can be eliminated in this way because they are linearly independent.
As soon as all gi (i = 1, 2, . . ., n) are exhausted we conclude that the vectors
f 1, f 2, . . . , f n+1
are linearly dependent. This contradicts, however, the previous assumption
that they belong to the basis F.
DeÔ¨Ånition 1.5. The dimension of a Ô¨Ånite-dimensional vector space V is the
number of elements in a basis of V.
Theorem 1.3. Every set F = {f 1, f 2, . . . , f n} of linearly independent vec-
tors in an n-dimensional vectors space V forms a basis of V. Every set of
more than n vectors is linearly dependent.
Proof. The proof of this theorem is similar to the preceding one. Let G =
{g1, g2, . . . , gn} be a basis of V. Then, the vectors (1.3) are linearly dependent
and non-zero. Excluding a vector gk we obtain a set of vectors, say G‚Ä≤, with
the property that every vector in V is a linear combination of the elements
of G‚Ä≤. Repeating this procedure we Ô¨Ånally end up with the set F with the
same property. Since the vectors f i (i = 1, 2, . . . , n) are linearly independent
they form a basis of V. Any further vectors in V, say f n+1, f n+2, . . . are thus
linear combinations of F. Hence, any set of more than n vectors is linearly
dependent.
Theorem 1.4. Every set F = {f 1, f 2, . . . , f m} of linearly independent vec-
tors in an n-dimensional vector space V can be extended to a basis.

1.3 Components of a Vector, Summation Convention
5
Proof. If m = n, then F is already a basis according to Theorem 1.3. If
m < n, then we try to Ô¨Ånd n ‚àím vectors f m+1, f m+2, . . . , f n, such that all
the vectors f i, that is, f 1, f 2, . . . , f m, f m+1, . . . , f n are linearly independent
and consequently form a basis. Let us assume, on the contrary, that only
k < n ‚àím such vectors can be found. In this case, for all x ‚ààV there exist
scalars Œ±, Œ±1, Œ±2, . . . , Œ±m+k, not all zero, such that
Œ±x + Œ±1f 1 + Œ±2f 2 + . . . + Œ±m+kf m+k = 0,
where Œ± Ã∏= 0 since otherwise the vectors f i (i = 1, 2, . . ., m + k) would be
linearly dependent. Thus, all the vectors x of V are linear combinations of
f i (i = 1, 2, . . . , m + k). Then, the dimension of V is m + k < n, which con-
tradicts the assumption of this theorem.
1.3 Components of a Vector, Summation Convention
Let G = {g1, g2, . . . , gn} be a basis of an n-dimensional vector space V. Then,
x =
n
	
i=1
xigi,
‚àÄx ‚ààV.
(1.4)
Theorem 1.5. The representation (1.4) with respect to a given basis G is
unique.
Proof. Let
x =
n
	
i=1
xigi
and
x =
n
	
i=1
yigi
be two diÔ¨Äerent representations of a vector x, where not all scalar coeÔ¨Écients
xi and yi (i = 1, 2, . . ., n) are pairwise identical. Then,
0 = x + (‚àíx) = x + (‚àí1) x =
n
	
i=1
xigi +
n
	
i=1

‚àíyi
gi =
n
	
i=1

xi ‚àíyi
gi,
where we use the identity ‚àíx = (‚àí1) x (Exercise 1.1). Thus, either the num-
bers xi and yi are pairwise equal xi = yi (i = 1, 2, . . . , n) or the vectors gi are
linearly dependent. The latter one is likewise impossible because these vectors
form a basis of V.
The scalar numbers xi (i = 1, 2, . . ., n) in the representation (1.4) are called
components of the vector x with respect to the basis G = {g1, g2, . . . , gn}.
The summation of the form (1.4) is often used in tensor analysis. For this
reason it is usually represented without the summation symbol in a short form
by

6
1 Vectors and Tensors in a Finite-Dimensional Space
x =
n
	
i=1
xigi = xigi
(1.5)
referred to as Einstein‚Äôs summation convention. Accordingly, the summation is
implied if an index appears twice in a multiplicative term, once as a superscript
and once as a subscript. Such a repeated index (called dummy index) takes
the values from 1 to n (the dimension of the vector space in consideration).
The sense of the index changes (from superscript to subscript or vice versa)
if it appears under the fraction bar.
1.4 Scalar Product, Euclidean Space, Orthonormal Basis
The scalar product plays an important role in vector and tensor algebra. The
properties of the vector space essentially depend on whether and how the
scalar product is deÔ¨Åned in this space.
DeÔ¨Ånition 1.6. The scalar (inner) product is a real-valued function x ¬∑ y of
two vectors x and y in a vector space V, satisfying the following conditions.
C. (C.1) x ¬∑ y = y ¬∑ x
(commutative rule),
(C.2) x ¬∑ (y + z) = x ¬∑ y + x ¬∑ z (distributive rule),
(C.3) Œ± (x ¬∑ y) = (Œ±x) ¬∑ y = x ¬∑ (Œ±y)
(associative rule for the multiplica-
tion by a scalar), ‚àÄŒ± ‚ààR, ‚àÄx, y, z ‚ààV,
(C.4) x ¬∑ x ‚â•0 ‚àÄx ‚ààV,
x ¬∑ x = 0 if and only if x = 0.
An n-dimensional vector space furnished by the scalar product with properties
(C.1-C.4) is called Euclidean space En. On the basis of this scalar product
one deÔ¨Ånes the Euclidean length (also called norm) of a vector x by
‚à•x‚à•= ‚àöx ¬∑ x.
(1.6)
A vector whose length is equal to 1 is referred to as unit vector.
DeÔ¨Ånition 1.7. Two vectors x and y are called orthogonal (perpendicular),
denoted by x‚ä•y, if
x ¬∑ y = 0.
(1.7)
Of special interest is the so-called orthonormal basis of the Euclidean space.
DeÔ¨Ånition 1.8. A basis E = {e1, e2, . . . , en} of an n-dimensional Euclidean
space En is called orthonormal if
ei ¬∑ ej = Œ¥ij,
i, j = 1, 2, . . . , n,
(1.8)
where

1.4 Scalar Product, Euclidean Space, Orthonormal Basis
7
Œ¥ij = Œ¥ij = Œ¥i
j =

1 for i = j,
0 for i Ã∏= j
(1.9)
denotes the Kronecker delta.
Thus, the elements of an orthonormal basis represent pairwise orthog-
onal unit vectors. Of particular interest is the question of the existence of
an orthonormal basis. Now, we are going to demonstrate that every set of
m ‚â§n linearly independent vectors in En can be orthogonalized and nor-
malized by means of a linear transformation (Gram-Schmidt procedure).
In other words, starting from linearly independent vectors x1, x2, . . . , xm
one can always construct their linear combinations e1, e2, . . . , em such that
ei ¬∑ ej = Œ¥ij (i, j = 1, 2, . . ., m). Indeed, since the vectors xi (i = 1, 2, . . ., m)
are linearly independent they are all non-zero (see Exercise 1.2). Thus, we can
deÔ¨Åne the Ô¨Årst unit vector by
e1 =
x1
‚à•x1‚à•.
(1.10)
Next, we consider the vector
e‚Ä≤
2 = x2 ‚àí(x2 ¬∑ e1) e1
(1.11)
orthogonal to e1. This holds for the unit vector e2 = e‚Ä≤
2/‚à•e‚Ä≤
2‚à•as well. It
is also seen that ‚à•e‚Ä≤
2‚à•=

e‚Ä≤
2 ¬∑ e‚Ä≤
2 Ã∏= 0 because otherwise e‚Ä≤
2 = 0 and thus
x2 = (x2 ¬∑ e1) e1 = (x2 ¬∑ e1) ‚à•x1‚à•‚àí1 x1. However, the latter result contradicts
the fact that the vectors x1 and x2 are linearly independent.
Further, we proceed to construct the vectors
e‚Ä≤
3 = x3 ‚àí(x3 ¬∑ e2) e2 ‚àí(x3 ¬∑ e1) e1,
e3 =
e‚Ä≤
3
‚à•e‚Ä≤
3‚à•
(1.12)
orthogonal to e1 and e2. Repeating this procedure we Ô¨Ånally obtain the set
of orthonormal vectors e1, e2, . . . , em. Since these vectors are non-zero and
mutually orthogonal, they are linearly independent (see Exercise 1.6). In the
case m = n, this set represents, according to Theorem 1.3, the orthonormal
basis (1.8) in En.
With respect to an orthonormal basis the scalar product of two vectors
x = xiei and y = yiei in En takes the form
x ¬∑ y = x1y1 + x2y2 + . . . + xnyn.
(1.13)
For the length of the vector x (1.6) we thus obtain the Pythagoras formula
‚à•x‚à•=

x1x1 + x2x2 + . . . + xnxn,
x ‚ààEn.
(1.14)

8
1 Vectors and Tensors in a Finite-Dimensional Space
1.5 Dual Bases
DeÔ¨Ånition 1.9. Let G = {g1, g2, . . . , gn} be a basis in the n-dimensional Eu-
clidean space En. Then, a basis G‚Ä≤ =

g1, g2, . . . , gn
of En is called dual to
G, if
gi ¬∑ gj = Œ¥j
i ,
i, j = 1, 2, . . . , n.
(1.15)
In the following we show that a set of vectors G‚Ä≤ =

g1, g2, . . . , gn
satisfying
the conditions (1.15) always exists, is unique and forms a basis in En.
Let E = {e1, e2, . . . , en} be an orthonormal basis in En. Since G also
represents a basis, we can write
ei = Œ±j
igj,
gi = Œ≤j
i ej,
i = 1, 2, . . ., n,
(1.16)
where Œ±j
i and Œ≤j
i (i = 1, 2, . . ., n) denote the components of ei and gi, respec-
tively. Inserting the Ô¨Årst relation (1.16) into the second one yields
gi = Œ≤j
i Œ±k
j gk,
‚áí
0 =

Œ≤j
i Œ±k
j ‚àíŒ¥k
i

gk,
i = 1, 2, . . . , n.
(1.17)
Since the vectors gi are linearly independent we obtain
Œ≤j
i Œ±k
j = Œ¥k
i ,
i, k = 1, 2, . . ., n.
(1.18)
Let further
gi = Œ±i
jej,
i = 1, 2, . . . , n,
(1.19)
where and henceforth we set ej = ej (j = 1, 2, . . . , n) in order to take the
advantage of Einstein‚Äôs summation convention. By virtue of (1.8), (1.16) and
(1.18) one Ô¨Ånally Ô¨Ånds
gi¬∑gj =

Œ≤k
i ek

¬∑

Œ±j
l el
= Œ≤k
i Œ±j
l Œ¥l
k = Œ≤k
i Œ±j
k = Œ¥j
i ,
i, j = 1, 2, . . ., n. (1.20)
Next, we show that the vectors gi (i = 1, 2, . . ., n) (1.19) are linearly indepen-
dent and for this reason form a basis of En. Assume on the contrary that
aigi = 0,
where not all scalars ai (i = 1, 2, . . ., n) are zero. The scalar product of this
relation with the vectors gj (j = 1, 2, . . . , n) leads to a contradiction. Indeed,
we obtain in this case
0 = aigi ¬∑ gj = aiŒ¥i
j = aj,
j = 1, 2, . . ., n.
The next important question is whether the dual basis is unique. Let G‚Ä≤ =

g1, g2, . . . , gn
and H‚Ä≤ =

h1, h2, . . . , hn
be two arbitrary non-coinciding
bases in En, both dual to G = {g1, g2, . . . , gn}. Then,

1.5 Dual Bases
9
hi = hi
jgj,
i = 1, 2, . . ., n.
Forming the scalar product with the vectors gj (j = 1, 2, . . ., n) we can con-
clude that the bases G‚Ä≤ and H‚Ä≤ coincide:
Œ¥i
j = hi ¬∑ gj =

hi
kgk
¬∑ gj = hi
kŒ¥k
j = hi
j
‚áí
hi = gi,
i = 1, 2, . . . , n.
Thus, we have proved the following theorem.
Theorem 1.6. To every basis in an Euclidean space En there exists a unique
dual basis.
Relation (1.19) enables to determine the dual basis. However, it can also be
obtained without any orthonormal basis. Indeed, let gi be a basis dual to
gi (i = 1, 2, . . . , n). Then
gi = gijgj,
gi = gijgj,
i = 1, 2, . . ., n.
(1.21)
Inserting the second relation (1.21) into the Ô¨Årst one yields
gi = gijgjkgk,
i = 1, 2, . . ., n.
(1.22)
Multiplying scalarly with the vectors gl we have by virtue of (1.15)
Œ¥i
l = gijgjkŒ¥k
l = gijgjl,
i, l = 1, 2, . . . , n.
(1.23)
Thus, we see that the matrices [gkj] and

gkj
are inverse to each other such
that

gkj
= [gkj]‚àí1 .
(1.24)
Now, multiplying scalarly the Ô¨Årst and second relation (1.21) by the vectors
gj and gj (j = 1, 2, . . . , n), respectively, we obtain with the aid of (1.15) the
following important identities:
gij = gji = gi ¬∑ gj,
gij = gji = gi ¬∑ gj,
i, j = 1, 2, . . . , n.
(1.25)
By deÔ¨Ånition (1.8) the orthonormal basis in En is self-dual, so that
ei = ei,
ei ¬∑ ej = Œ¥j
i ,
i, j = 1, 2, . . ., n.
(1.26)
With the aid of the dual bases one can represent an arbitrary vector in En by
x = xigi = xigi,
‚àÄx ‚ààEn,
(1.27)
where
xi = x ¬∑ gi,
xi = x ¬∑ gi,
i = 1, 2, . . ., n.
(1.28)
Indeed, using (1.15) we can write

10
1 Vectors and Tensors in a Finite-Dimensional Space
x ¬∑ gi =

xjgj

¬∑ gi = xjŒ¥i
j = xi,
x ¬∑ gi =

xjgj
¬∑ gi = xjŒ¥j
i = xi,
i = 1, 2, . . ., n.
The components of a vector with respect to the dual bases are suitable for
calculating the scalar product. For example, for two arbitrary vectors x =
xigi = xigi and y = yigi = yigi we obtain
x ¬∑ y = xiyjgij = xiyjgij = xiyi = xiyi.
(1.29)
The length of the vector x can thus be written by
‚à•x‚à•=

xixjgij =

xixjgij =

xixi.
(1.30)
Example. Dual basis in E3. Let G = {g1, g2, g3} be a basis of the
three-dimensional Euclidean space and
g = [g1g2g3] ,
(1.31)
where [‚Ä¢ ‚Ä¢ ‚Ä¢] denotes the mixed product of vectors. It is deÔ¨Åned by
[abc] = (a √ó b) ¬∑ c = (b √ó c) ¬∑ a = (c √ó a) ¬∑ b,
(1.32)
where ‚Äú√ó‚Äù denotes the vector (also called cross or outer) product of vectors.
Consider the following set of vectors:
g1 = g‚àí1g2 √ó g3,
g2 = g‚àí1g3 √ó g1,
g3 = g‚àí1g1 √ó g2.
(1.33)
It seen that the vectors (1.33) satisfy conditions (1.15), are linearly indepen-
dent (Exercise 1.11) and consequently form the basis dual to gi (i = 1, 2, 3).
Further, it can be shown that
g2 = |gij| ,
(1.34)
where |‚Ä¢| denotes the determinant of the matrix [‚Ä¢]. Indeed, with the aid of
(1.16)2 we obtain
g = [g1g2g3] =

Œ≤i
1eiŒ≤j
2ejŒ≤k
3ek

= Œ≤i
1Œ≤j
2Œ≤k
3 [eiejek] = Œ≤i
1Œ≤j
2Œ≤k
3eijk =
Œ≤i
j
 ,
(1.35)
where eijk denotes the permutation symbol (also called Levi-Civita symbol).
It is deÔ¨Åned by
eijk = eijk = [eiejek]
=
‚éß
‚é™
‚é®
‚é™
‚é©
1
if ijk is an even permutation of 123,
‚àí1
if ijk is an odd permutation of 123,
0
otherwise,
(1.36)

1.5 Dual Bases
11
where the orthonormal vectors e1, e2 and e3 are numerated in such a way
that they form a right-handed system. In this case, [e1e2e3] = 1.
On the other hand, we can write again using (1.16)2
gij = gi ¬∑ gj =
3
	
k=1
Œ≤k
i Œ≤k
j .
The latter sum can be represented as a product of two matrices so that
[gij] =

Œ≤j
i
 
Œ≤j
i
T
.
(1.37)
Since the determinant of the matrix product is equal to the product of the
matrix determinants we Ô¨Ånally have
|gij| =
Œ≤j
i

2
= g2.
(1.38)
With the aid of the permutation symbol (1.36) one can represent the identities
(1.33) by
gi √ó gj = eijk g gk,
i, j = 1, 2, 3,
(1.39)
which also delivers
[gigjgk] = eijk g,
i, j, k = 1, 2, 3.
(1.40)
Similarly to (1.34) one can also show that (see Exercise 1.12)

g1g2g32 =
gij = g‚àí2.
(1.41)
Thus,
gi √ó gj = eijk
g gk,
i, j = 1, 2, 3,
(1.42)
which yields by analogy with (1.40)

gigjgk
= eijk
g ,
i, j, k = 1, 2, 3.
(1.43)
Relations (1.39) and (1.42) permit a useful representation of the vector prod-
uct. Indeed, let a = aigi = aigi and b = bjgj = bjgj be two arbitrary vectors
in E3. Then,
a √ó b =

aigi

√ó

bjgj

= aibjeijkggk = g

a1 a2 a3
b1 b2 b3
g1 g2 g3

,

12
1 Vectors and Tensors in a Finite-Dimensional Space
a √ó b =

aigi
√ó

bjgj
= aibjeijkg‚àí1gk = 1
g

a1 a2 a3
b1 b2 b3
g1 g2 g3

.
(1.44)
For the orthonormal basis in E3 relations (1.39) and (1.42) reduce to
ei √ó ej = eijkek = eijkek,
i, j = 1, 2, 3,
(1.45)
so that the vector product (1.44) can be written by
a √ó b =

a1 a2 a3
b1 b2 b3
e1 e2 e3

,
(1.46)
where a = aiei and b = bjej.
1.6 Second-Order Tensor as a Linear Mapping
Let us consider a set Linn of all linear mappings of one vector into another
one within En. Such a mapping can be written as
y = Ax,
y ‚ààEn,
‚àÄx ‚ààEn,
‚àÄA ‚ààLinn.
(1.47)
Elements of the set Linn are called second-order tensors or simply tensors.
Linearity of the mapping (1.47) is expressed by the following relations:
A (x + y) = Ax + Ay,
‚àÄx, y ‚ààEn,
‚àÄA ‚ààLinn,
(1.48)
A (Œ±x) = Œ± (Ax) ,
‚àÄx ‚ààEn,
‚àÄŒ± ‚ààR,
‚àÄA ‚ààLinn.
(1.49)
Further, we deÔ¨Åne the product of a tensor by a scalar number Œ± ‚ààR as
(Œ±A) x = Œ± (Ax) = A (Œ±x) ,
‚àÄx ‚ààEn
(1.50)
and the sum of two tensors A and B as
(A + B) x = Ax + Bx,
‚àÄx ‚ààEn.
(1.51)
Thus, properties (A.1), (A.2) and (B.1-B.4) apply to the set Linn. Setting in
(1.50) Œ± = ‚àí1 we obtain the negative tensor by
‚àíA = (‚àí1) A.
(1.52)
Further, we deÔ¨Åne a zero tensor 0 in the following manner
0x = 0,
‚àÄx ‚ààEn,
(1.53)
so that the elements of the set Linn also fulÔ¨Åll conditions (A.3) and (A.4) and
accordingly form a vector space.

1.6 Second-Order Tensor as a Linear Mapping
13
The properties of second-order tensors can thus be summarized by
A + B = B + A,
(addition is commutative),
(1.54)
A + (B + C) = (A + B) + C,
(addition is associative),
(1.55)
0 + A = A,
(1.56)
A + (‚àíA) = 0,
(1.57)
Œ± (Œ≤A) = (Œ±Œ≤) A,
(multiplication by scalars is associative),
(1.58)
1A = A,
(1.59)
Œ± (A + B) = Œ±A + Œ±B,
(multiplication by scalars is distributive
with respect to tensor addition),
(1.60)
(Œ± + Œ≤) A = Œ±A + Œ≤A,
(multiplication by scalars is distributive
with respect to scalar addition),
‚àÄA, B, C ‚ààLinn, ‚àÄŒ±, Œ≤ ‚ààR.
(1.61)
Example. Vector product in E3. The vector product of two vectors in
E3 represents again a vector in E3
z = w √ó x,
z ‚ààE3,
‚àÄw, x ‚ààE3.
(1.62)
According to (1.44) the mapping x ‚Üíz is linear so that
w √ó (Œ±x) = Œ± (w √ó x) ,
w √ó (x + y) = w √ó x + w √ó y,
‚àÄw, x, y ‚ààE3,
‚àÄŒ± ‚ààR.
(1.63)
Thus, it can be described by means of a tensor of the second order by
w √ó x = Wx,
W ‚ààLin3,
‚àÄx ‚ààE3.
(1.64)
The tensor which forms the vector product by a vector w according to (1.64)
will be denoted in the following by ÀÜw. Thus, we write
w √ó x = ÀÜwx.
(1.65)
Example. Representation of a rotation by a second-order tensor.
A rotation of a vector a in E3 about an axis yields another vector r in E3. It
can be shown that the mapping a ‚Üír (a) is linear such that
r (Œ±a) = Œ±r (a) , r (a + b) = r (a) + r (b) , ‚àÄŒ± ‚ààR, ‚àÄa, b ‚ààE3.
(1.66)
Thus, it can again be described by a second-order tensor as
r (a) = Ra,
‚àÄa ‚ààE3,
R ‚ààLin3.
(1.67)
This tensor R is referred to as rotation tensor.

14
1 Vectors and Tensors in a Finite-Dimensional Space
a
Àúa
y
a‚àó
œâ
x
e
Fig. 1.2. Finite rotation of a vector in E3
Let us construct the rotation tensor which rotates an arbitrary vector a ‚àà
E3 about an axis speciÔ¨Åed by a unit vector e ‚ààE3 (see Fig. 1.2). Decomposing
the vector a by a = a‚àó+ x in two vectors along and perpendicular to the
rotation axis we can write
Àúa = a‚àó+ x cos œâ + y sin œâ = a‚àó+ (a ‚àía‚àó) cos œâ + y sin œâ,
(1.68)
where œâ denotes the rotation angle. By virtue of the geometric identities
a‚àó= (a ¬∑ e) e = (e ‚äóe) a,
y = e√óx = e√ó(a ‚àía‚àó) = e√óa = ÀÜea, (1.69)
where ‚Äú‚äó‚Äù denotes the so-called tensor product (1.75) (see Sect. 1.7), we
obtain
Àúa = cos œâa + sin œâÀÜea + (1 ‚àícos œâ) (e ‚äóe) a.
(1.70)
Thus the rotation tensor can be given by
R = cos œâI + sin œâÀÜe + (1 ‚àícos œâ) e ‚äóe,
(1.71)
where I denotes the so-called identity tensor (1.84) (see Sect. 1.7).
Example. The Cauchy stress tensor as a linear mapping of the
unit surface normal into the Cauchy stress vector. Let us consider a
body B in the current conÔ¨Åguration at a time t. In order to deÔ¨Åne the stress
in some point P let us further imagine a smooth surface going through P and
separating B into two parts (Fig. 1.3). Then, one can deÔ¨Åne a force Œîp and
a couple Œîm resulting from the forces exerted by the (hidden) material on

1.6 Second-Order Tensor as a Linear Mapping
15










Fig. 1.3. Cauchy stress vector
one side of the surface ŒîA and acting on the material on the other side of
this surface. Let the area ŒîA tend to zero keeping P as inner point. A basic
postulate of continuum mechanics is that the limit
t =
lim
ŒîA‚Üí0
Œîp
ŒîA
exists and is Ô¨Ånal. The so-deÔ¨Åned vector t is called Cauchy stress vector.
Cauchy‚Äôs fundamental postulate states that the vector t depends on the sur-
face only through the outward unit normal n. In other words, the Cauchy
stress vector is the same for all surfaces through P which have n as the nor-
mal in P. Further, according to Cauchy‚Äôs theorem the mapping n ‚Üít is linear
provided t is a continuous function of the position vector x at P. Hence, this
mapping can be described by a second-order tensor œÉ called the Cauchy stress
tensor so that
t = œÉn.
(1.72)
On the basis of the ‚Äúright‚Äù mapping (1.47) we can also deÔ¨Åne the ‚Äúleft‚Äù
one by the following condition
(yA) ¬∑ x = y ¬∑ (Ax) ,
‚àÄx ‚ààEn,
A ‚ààLinn.
(1.73)
First, it should be shown that for all y ‚ààEn there exists a unique vector yA ‚àà
En satisfying the condition (1.73) for all x ‚ààEn. Let G = {g1, g2, . . . , gn}
and G‚Ä≤ =

g1, g2, . . . , gn
be dual bases in En. Then, we can represent two
arbitrary vectors x, y ‚ààEn, by x = xigi and y = yigi. Now, consider the
vector
yA = yi

gi ¬∑

Agj
gj.
It holds: (yA) ¬∑ x = yixj

gi ¬∑

Agj
. On the other hand, we obtain the same
result also by

16
1 Vectors and Tensors in a Finite-Dimensional Space
y ¬∑ (Ax) = y ¬∑

xjAgj
= yixj

gi ¬∑

Agj
.
Further, we show that the vector yA, satisfying condition (1.73) for all x ‚ààEn,
is unique. Conversely, let a, b ‚ààEn be two such vectors. Then, we have
a ¬∑ x = b ¬∑ x ‚áí(a ‚àíb) ¬∑ x = 0, ‚àÄx ‚ààEn ‚áí(a ‚àíb) ¬∑ (a ‚àíb) = 0,
which by axiom (C.4) implies that a = b.
Since the order of mappings in (1.73) is irrelevant we can write them
without brackets and dots as follows
y ¬∑ (Ax) = (yA) ¬∑ x = yAx.
(1.74)
1.7 Tensor Product, Representation of a Tensor with
Respect to a Basis
The tensor product plays an important role since it enables to construct a
second-order tensor from two vectors. In order to deÔ¨Åne the tensor product
we consider two vectors a, b ‚ààEn. An arbitrary vector x ‚ààEn can be mapped
into another vector a (b ¬∑ x) ‚ààEn. This mapping is denoted by symbol ‚Äú‚äó‚Äù
as a ‚äób. Thus,
(a ‚äób) x = a (b ¬∑ x) ,
a, b ‚ààEn, ‚àÄx ‚ààEn.
(1.75)
It can be shown that the mapping (1.75) fulÔ¨Ålls the conditions (1.48-1.50) and
for this reason is linear. Indeed, by virtue of (B.1), (B.4), (C.2) and (C.3) we
can write
(a ‚äób) (x + y) = a [b ¬∑ (x + y)] = a (b ¬∑ x + b ¬∑ y)
= (a ‚äób) x + (a ‚äób) y,
(1.76)
(a ‚äób) (Œ±x) = a [b ¬∑ (Œ±x)] = Œ± (b ¬∑ x) a
= Œ± (a ‚äób) x,
a, b ‚ààEn, ‚àÄx, y ‚ààEn, ‚àÄŒ± ‚ààR.
(1.77)
Thus, the tensor product of two vectors represents a second-order tensor.
Further, it holds
c ‚äó(a + b) = c ‚äóa + c ‚äób,
(a + b) ‚äóc = a ‚äóc + b ‚äóc,
(1.78)
(Œ±a) ‚äó(Œ≤b) = Œ±Œ≤ (a ‚äób) ,
a, b, c ‚ààEn, ‚àÄŒ±, Œ≤ ‚ààR.
(1.79)
Indeed, mapping an arbitrary vector x ‚ààEn by both sides of these relations
and using (1.51) and (1.75) we obtain
c ‚äó(a + b) x = c (a ¬∑ x + b ¬∑ x) = c (a ¬∑ x) + c (b ¬∑ x)
= (c ‚äóa) x + (c ‚äób) x = (c ‚äóa + c ‚äób) x,

1.7 Tensor Product, Representation of a Tensor with Respect to a Basis
17
[(a + b) ‚äóc] x = (a + b) (c ¬∑ x) = a (c ¬∑ x) + b (c ¬∑ x)
= (a ‚äóc) x + (b ‚äóc) x = (a ‚äóc + b ‚äóc) x,
(Œ±a) ‚äó(Œ≤b) x = (Œ±a) (Œ≤b ¬∑ x)
= Œ±Œ≤a (b ¬∑ x) = Œ±Œ≤ (a ‚äób) x,
‚àÄx ‚ààEn.
For the ‚Äúleft‚Äù mapping by the tensor a‚äób we obtain from (1.73) (see Exercise
1.19)
y (a ‚äób) = (y ¬∑ a) b,
‚àÄy ‚ààEn.
(1.80)
We have already seen that the set of all second-order tensors Linn repre-
sents a vector space. In the following, we show that a basis of Linn can be
constructed with the aid of the tensor product (1.75).
Theorem 1.7. Let F = {f 1, f 2, . . . , f n} and G = {g1, g2, . . . , gn} be two
arbitrary bases of En. Then, the tensors f i ‚äógj (i, j = 1, 2, . . . , n) represent
a basis of Linn. The dimension of the vector space Linn is thus n2.
Proof. First, we prove that every tensor in Linn represents a linear combi-
nation of the tensors f i ‚äógj (i, j = 1, 2, . . . , n). Indeed, let A ‚ààLinn be an
arbitrary second-order tensor. Consider the following linear combination
A‚Ä≤ =

f iAgj
f i ‚äógj,
where the vectors f i and gi (i = 1, 2, . . ., n) form the bases dual to F and G,
respectively. The tensors A and A‚Ä≤ coincide if and only if
A‚Ä≤x = Ax,
‚àÄx ‚ààEn.
(1.81)
Let x = xjgj. Then
A‚Ä≤x =

f iAgj
f i ‚äógj

xkgk
=

f iAgj
f ixkŒ¥k
j = xj

f iAgj
f i.
On the other hand, Ax = xjAgj. By virtue of (1.27-1.28) we can repre-
sent the vectors Agj (j = 1, 2, . . . , n) with respect to the basis F by Agj =

f i ¬∑

Agj
f i =

f iAgj
f i (j = 1, 2, . . . , n). Hence,
Ax = xj

f iAgj
f i.
Thus, it is seen that condition (1.81) is satisÔ¨Åed for all x ‚ààEn. Finally,
we show that the tensors f i ‚äógj (i, j = 1, 2, . . . , n) are linearly independent.
Otherwise, there would exist scalars Œ±ij (i, j = 1, 2, . . . , n), not all zero, such
that
Œ±ijf i ‚äógj = 0.
The right mapping of gk (k = 1, 2, . . ., n) by this tensor equality yields then:
Œ±ikf i = 0 (k = 1, 2, . . ., n). This contradicts, however, the fact that the vec-
tors f k (k = 1, 2, . . . , n) form a basis and are therefore linearly independent.

18
1 Vectors and Tensors in a Finite-Dimensional Space
For the representation of second-order tensors we will in the following use
primarily the bases gi ‚äógj, gi ‚äógj, gi ‚äógj or gi ‚äógj (i, j = 1, 2, . . ., n). With
respect to these bases a tensor A ‚ààLinn is written as
A = Aijgi ‚äógj = Aijgi ‚äógj = Ai
¬∑jgi ‚äógj = A j
i¬∑gi ‚äógj
(1.82)
with the components (see Exercise 1.20)
Aij = giAgj,
Aij = giAgj,
Ai
¬∑j = giAgj,
A j
i¬∑ = giAgj,
i, j = 1, 2, . . ., n.
(1.83)
Note, that the subscript dot indicates the position of the above index. For
example, for the components Ai
¬∑j, i is the Ô¨Årst index while for the components
A i
j¬∑, i is the second index.
Of special importance is the so-called identity tensor I. It is deÔ¨Åned by
Ix = x,
‚àÄx ‚ààEn.
(1.84)
With the aid of (1.25), (1.82) and (1.83) the components of the identity tensor
can be expressed by
Iij = giIgj = gi ¬∑ gj = gij,
Iij = giIgj = gi ¬∑ gj = gij,
Ii
¬∑j = I j
i¬∑ = Ii
j = giIgj = giIgj = gi ¬∑ gj = gi ¬∑ gj = Œ¥i
j,
(1.85)
where i, j = 1, 2, . . . , n. Thus,
I = gijgi ‚äógj = gijgi ‚äógj = gi ‚äógi = gi ‚äógi.
(1.86)
It is seen that the components (1.85)1,2 of the identity tensor are given by
relation (1.25). In view of (1.30) they characterize metric properties of the
Euclidean space and are referred to as metric coeÔ¨Écients. For this reason, the
identity tensor is frequently called metric tensor. With respect to an orthonor-
mal basis relation (1.86) reduces to
I =
n
	
i=1
ei ‚äóei.
(1.87)
1.8 Change of the Basis, Transformation Rules
Now, we are going to clarify how the vector and tensor components transform
with the change of the basis. Let x be a vector and A a second-order tensor.
According to (1.27) and (1.82)
x = xigi = xigi,
(1.88)

1.9 Special Operations with Second-Order Tensors
19
A = Aijgi ‚äógj = Aijgi ‚äógj = Ai
¬∑jgi ‚äógj = A j
i¬∑gi ‚äógj.
(1.89)
With the aid of (1.21) and (1.28) we can write
xi = x ¬∑ gi = x ¬∑

gijgj

= xjgji,
xi = x ¬∑ gi = x ¬∑

gijgj
= xjgji, (1.90)
where i = 1, 2, . . . , n. Similarly we obtain by virtue of (1.83)
Aij = giAgj = giA

gjkgk

=

gilgl

A

gjkgk

= Ai
¬∑kgkj = gilAlkgkj,
(1.91)
Aij = giAgj = giA

gjkgk
=

gilgl
A

gjkgk
= A k
i¬∑ gkj = gilAlkgkj,
(1.92)
where i, j = 1, 2, . . . , n. The transformation rules (1.90-1.92) hold not only for
dual bases. Indeed, let gi and ¬Øgi (i = 1, 2, . . ., n) be two arbitrary bases in
En, so that
x = xigi = ¬Øxi¬Øgi,
(1.93)
A = Aijgi ‚äógj = ¬ØA
ij¬Øgi ‚äó¬Øgj.
(1.94)
By means of the relations
gi = aj
i ¬Øgj,
i = 1, 2, . . ., n
(1.95)
one thus obtains
x = xigi = xiaj
i ¬Øgj
‚áí
¬Øxj = xiaj
i,
j = 1, 2, . . ., n,
(1.96)
A = Aijgi ‚äógj = Aij 
ak
i ¬Øgk

‚äó

al
j¬Øgl

= Aijak
i al
j¬Øgk ‚äó¬Øgl
‚áí¬ØA
kl = Aijak
i al
j,
k, l = 1, 2, . . ., n.
(1.97)
1.9 Special Operations with Second-Order Tensors
In Sect. 1.6 we have seen that the set Linn represents a Ô¨Ånite-dimensional
vector space. Its elements are second-order tensors that can be treated as
vectors in En2 with all the operations speciÔ¨Åc for vectors such as summation,
multiplication by a scalar or a scalar product (the latter one will be deÔ¨Åned
for second-order tensors in Sect. 1.10). However, in contrast to conventional
vectors in the Euclidean space, for second-order tensors one can additionally
deÔ¨Åne some special operations as for example composition, transposition or
inversion.

20
1 Vectors and Tensors in a Finite-Dimensional Space
Composition (simple contraction). Let A, B ‚ààLinn be two second-
order tensors. The tensor C = AB is called composition of A and B if
Cx = A (Bx) ,
‚àÄx ‚ààEn.
(1.98)
For the left mapping (1.73) one can write
y (AB) = (yA) B,
‚àÄy ‚ààEn.
(1.99)
In order to prove the last relation we use again (1.73) and (1.98):
y (AB) x = y ¬∑ [(AB) x] = y ¬∑ [A (Bx)]
= (yA) ¬∑ (Bx) = [(yA) B] ¬∑ x,
‚àÄx ‚ààEn.
The composition of tensors (1.98) is generally not commutative so that AB Ã∏=
BA. Two tensors A and B are called commutative if on the contrary AB =
BA. Besides, the composition of tensors is characterized by the following
properties (see Exercise 1.24):
A0 = 0A = 0,
AI = IA = A,
(1.100)
A (B + C) = AB + AC,
(B + C) A = BA + CA,
(1.101)
A (BC) = (AB) C.
(1.102)
For example, the distributive rule (1.101)1 can be proved as follows
[A (B + C)] x = A [(B + C) x] = A (Bx + Cx) = A (Bx) + A (Cx)
= (AB) x + (AC) x = (AB + AC) x,
‚àÄx ‚ààEn.
For the tensor product (1.75) the composition (1.98) yields
(a ‚äób) (c ‚äód) = (b ¬∑ c) a ‚äód,
a, b, c, d ‚ààEn.
(1.103)
Indeed, by virtue of (1.75), (1.77) and (1.98)
(a ‚äób) (c ‚äód) x = (a ‚äób) [(c ‚äód) x] = (d ¬∑ x) (a ‚äób) c
= (d ¬∑ x) (b ¬∑ c) a = (b ¬∑ c) (a ‚äód) x
= [(b ¬∑ c) a ‚äód] x,
‚àÄx ‚ààEn.
Thus, we can write
AB = AikB j
k¬∑gi ‚äógj = AikBkjgi ‚äógj
= Ai
¬∑kBk
¬∑jgi ‚äógj = A k
i¬∑ Bkjgi ‚äógj,
(1.104)
where A and B are given in the form (1.82).
Powers, polynomials and functions of second-order tensors. On
the basis of the composition (1.98) one deÔ¨Ånes by

1.9 Special Operations with Second-Order Tensors
21
Am = AA . . . A



m times
,
m = 1, 2, 3 . . .,
A0 = I
(1.105)
powers (monomials) of second-order tensors characterized by the following
evident properties
AkAl = Ak+l,

Akl = Akl,
(1.106)
(Œ±A)k = Œ±kAk,
k, l = 0, 1, 2 . . .
(1.107)
With the aid of the tensor powers a polynomial of A can be deÔ¨Åned by
g (A) = a0I + a1A + a2A2 + . . . + amAm =
m
	
k=0
akAk.
(1.108)
g (A): Linn ‚ÜíLinn represents a tensor function mapping one second-order
tensor into another one within Linn. By this means one can deÔ¨Åne various
tensor functions. Of special interest is the exponential one
exp (A) =
‚àû
	
k=0
Ak
k!
(1.109)
given by the inÔ¨Ånite power series.
Transposition. The transposed tensor AT is deÔ¨Åned by:
ATx = xA,
‚àÄx ‚ààEn,
(1.110)
so that one can also write
Ay = yAT,
xAy = yATx,
‚àÄx, y ‚ààEn.
(1.111)
Indeed,
x ¬∑ (Ay) = (xA) ¬∑ y = y ¬∑

ATx

= yATx = x ¬∑

yAT
, ‚àÄx, y ‚ààEn.
Consequently,

ATT = A.
(1.112)
Transposition represents a linear operation over a second-order tensor since
(A + B)T = AT + BT
(1.113)
and
(Œ±A)T = Œ±AT,
‚àÄŒ± ‚ààR.
(1.114)
The composition of second-order tensors is transposed by

22
1 Vectors and Tensors in a Finite-Dimensional Space
(AB)T = BTAT.
(1.115)
Indeed, in view of (1.99) and (1.110)
(AB)T x = x (AB) = (xA) B = BT (xA) = BTATx,
‚àÄx ‚ààEn.
For the tensor product of two vectors a, b ‚ààEn we further obtain by use of
(1.75) and (1.80)
(a ‚äób)T = b ‚äóa.
(1.116)
This ensures the existence and uniqueness of the transposed tensor. Indeed,
every tensor A in Linn can be represented with respect to the tensor product
of the basis vectors in En in the form (1.82). Hence, considering (1.116) we
have
AT = Aijgj ‚äógi = Aijgj ‚äógi = Ai
¬∑jgj ‚äógi = A j
i¬∑gj ‚äógi,
(1.117)
or
AT = Ajigi ‚äógj = Ajigi ‚äógj = Aj
¬∑igi ‚äógj = A i
j¬∑gi ‚äógj.
(1.118)
Comparing the latter result with the original representation (1.82) one ob-
serves that the components of the transposed tensor can be expressed by

AT
ij = Aji,

ATij = Aji,
(1.119)

AT j
i¬∑ = Aj
¬∑i = gjkA l
k¬∑gli,

ATi
¬∑j = A i
j¬∑ = gjkAk
¬∑lgli.
(1.120)
For example, the last relation results from (1.83) and (1.111) within the fol-
lowing steps

ATi
¬∑j = giATgj = gjAgi = gj

Ak
¬∑lgk ‚äógl
gi = gjkAk
¬∑lgli.
According to (1.119) the homogeneous (covariant or contravariant) compo-
nents of the transposed tensor can simply be obtained by reÔ¨Çecting the matrix
of the original components from the main diagonal. It does not, however, hold
for the mixed components (1.120).
The transposition operation (1.110) gives rise to the deÔ¨Ånition of symmet-
ric MT = M and skew-symmetric second-order tensors WT = ‚àíW.
Obviously, the identity tensor is symmetric
IT = I.
(1.121)
Indeed,
xIy = x ¬∑ y = y ¬∑ x = yIx = xITy,
‚àÄx, y ‚ààEn.

1.9 Special Operations with Second-Order Tensors
23
Inversion. Let
y = Ax.
(1.122)
A tensor A ‚ààLinn is referred to as invertible if there exists a tensor A‚àí1 ‚àà
Linn satisfying the condition
x = A‚àí1y,
‚àÄx ‚ààEn.
(1.123)
The tensor A‚àí1 is called inverse of A. The set of all invertible tensors Invn =

A ‚ààLinn : ‚àÉA‚àí1
forms a subset of all second-order tensors Linn.
Inserting (1.122) into (1.123) yields
x = A‚àí1y = A‚àí1 (Ax) =

A‚àí1A

x,
‚àÄx ‚ààEn
and consequently
A‚àí1A = I.
(1.124)
Theorem 1.8. A tensor A is invertible if and only if Ax = 0 implies that
x = 0.
Proof. First we prove the suÔ¨Éciency. To this end, we map the vector equation
Ax = 0 by A‚àí1. According to (1.124) it yields: 0 = A‚àí1Ax = Ix = x. To
prove the necessity we consider a basis G = {g1, g2, . . . , gn} in En. It can be
shown that the vectors hi = Agi (i = 1, 2, . . ., n) form likewise a basis of En.
Conversely, let these vectors be linearly dependent so that aihi = 0, where not
all scalars ai (i = 1, 2, . . . , n) are zero. Then, 0 = aihi = aiAgi = Aa, where
a = aigi Ã∏= 0, which contradicts the assumption of the theorem. Now, consider
the tensor A‚Ä≤ = gi ‚äóhi, where the vectors hi are dual to hi (i = 1, 2, . . ., n).
One can show that this tensor is inverse to A, such that A‚Ä≤ = A‚àí1. Indeed,
let x = xigi be an arbitrary vector in En. Then, y = Ax = xiAgi = xihi
and therefore A‚Ä≤y = gi ‚äóhi 
xjhj

= gixjŒ¥i
j = xigi = x.
Conversely, it can be shown that an invertible tensor A is inverse to A‚àí1 and
consequently
AA‚àí1 = I.
(1.125)
For the proof we again consider the bases gi and Agi (i = 1, 2, . . ., n). Let
y = yiAgi be an arbitrary vector in En. Let further x = A‚àí1y = yigi in
view of (1.124). Then, Ax = yiAgi = y which implies that the tensor A is
inverse to A‚àí1.
Relation (1.125) implies the uniqueness of the inverse. Indeed, if A‚àí1 and

A‚àí1 are two distinct tensors both inverse to A then there exists at least one
vector y ‚ààEn such that A‚àí1y Ã∏= 
A‚àí1y. Mapping both sides of this vector
inequality by A and taking (1.125) into account we immediately come to the
contradiction.

24
1 Vectors and Tensors in a Finite-Dimensional Space
By means of (1.115), (1.121) and (1.125) we can write (see Exercise 1.37)

A‚àí1T =

AT‚àí1 = A‚àíT.
(1.126)
The composition of two arbitrary invertible tensors A and B is inverted by
(AB)‚àí1 = B‚àí1A‚àí1.
(1.127)
Indeed, let
y = ABx.
Mapping both sides of this vector identity by A‚àí1 and then by B‚àí1, we obtain
with the aid of (1.124)
x = B‚àí1A‚àí1y,
‚àÄx ‚ààEn.
On the basis of transposition and inversion one deÔ¨Ånes the so-called orthogonal
tensors. They do not change after consecutive transposition and inversion and
form the following subset of Linn:
Orthn =

Q ‚ààLinn : Q = Q‚àíT
.
(1.128)
For orthogonal tensors we can write in view of (1.124) and (1.125)
QQT = QTQ = I,
‚àÄQ ‚ààOrthn.
(1.129)
For example, one can show that the rotation tensor (1.71) is orthogonal. To
this end, we complete the vector e deÔ¨Åning the rotation axis (Fig. 1.2) to
an orthonormal basis {e, q, p} such that e = q √ó p. Then, using the vector
identity (see Exercise 1.15)
p (q ¬∑ x) ‚àíq (p ¬∑ x) = (q √ó p) √ó x,
‚àÄx ‚ààE3
(1.130)
we can write
ÀÜe = p ‚äóq ‚àíq ‚äóp.
(1.131)
The rotation tensor (1.71) takes thus the form
R = cos œâI + sin œâ (p ‚äóq ‚àíq ‚äóp) + (1 ‚àícos œâ) (e ‚äóe) .
(1.132)
Hence,
RRT = [cos œâI + sin œâ (p ‚äóq ‚àíq ‚äóp) + (1 ‚àícos œâ) (e ‚äóe)]
[cos œâI ‚àísin œâ (p ‚äóq ‚àíq ‚äóp) + (1 ‚àícos œâ) (e ‚äóe)]
= cos2 œâI + sin2 œâ (e ‚äóe) + sin2 œâ (p ‚äóp + q ‚äóq) = I.

1.10 Scalar Product of Second-Order Tensors
25
It is interesting that the exponential function (1.109) of a skew-symmetric
tensors represents an orthogonal tensor. Indeed, keeping in mind that a skew-
symmetric tensor W commutes with its transposed counterpart WT = ‚àíW
and using the identities exp (A + B) = exp (A) exp (B) for commutative ten-
sors (Exercise 1.27) and

AkT =

ATk for integer k (Exercise 1.35) we can
write
I = exp (0) = exp (W ‚àíW) = exp

W + WT
= exp (W) exp

WT
= exp (W) [exp (W)]T ,
‚àÄW ‚ààSkewn.
(1.133)
1.10 Scalar Product of Second-Order Tensors
Consider two second-order tensors a‚äób and c‚äód given in terms of the tensor
product (1.75). Their scalar product can be deÔ¨Åned in the following manner:
(a ‚äób) : (c ‚äód) = (a ¬∑ c) (b ¬∑ d) ,
a, b, c, d ‚ààEn.
(1.134)
It leads to the following identity (Exercise 1.39):
c ‚äód : A = cAd = dATc.
(1.135)
For two arbitrary tensors A and B given in the form (1.82) we thus obtain
A : B = AijBij = AijBij = Ai
¬∑jB j
i¬∑ = A j
i¬∑Bi
¬∑j.
(1.136)
Similar to vectors the scalar product of tensors is a real function characterized
by the following properties (see Exercise 1.40)
D. (D.1) A : B = B : A (commutative rule),
(D.2) A : (B + C) = A : B + A : C (distributive rule),
(D.3) Œ± (A : B) = (Œ±A) : B = A : (Œ±B) (associative rule for multiplica-
tion by a scalar),
‚àÄA, B ‚ààLinn, ‚àÄŒ± ‚ààR,
(D.4) A : A ‚â•0 ‚àÄA ‚ààLinn,
A : A = 0 if and only if A = 0.
We prove for example the property (D.4). To this end, we represent the tensor
A with respect to an orthonormal basis (1.8) in En as: A = Aijei ‚äóej =
Aijei ‚äóej, where Aij = Aij, (i, j = 1, 2, . . . , n), since ei = ei (i = 1, 2, . . ., n).
Keeping (1.136) in mind we then obtain:
A : A = AijAij =
n
	
i,j=1
AijAij =
n
	
i,j=1

Aij2 ‚â•0.
Using this important property one can deÔ¨Åne the norm of a second-order
tensor by:

26
1 Vectors and Tensors in a Finite-Dimensional Space
‚à•A‚à•= (A : A)1/2 ,
A ‚ààLinn.
(1.137)
For the scalar product of tensors one of which is given by a composition we
can write
A : (BC) =

BTA

: C =

ACT
: B.
(1.138)
We prove this identity Ô¨Årst for the tensor products:
(a ‚äób) : [(c ‚äód) (e ‚äóf)] = (d ¬∑ e) [(a ‚äób) : (c ‚äóf)]
= (d ¬∑ e) (a ¬∑ c) (b ¬∑ f) ,

(c ‚äód)T (a ‚äób)

: (e ‚äóf) = [(d ‚äóc) (a ‚äób)] : (e ‚äóf)
= (a ¬∑ c) [(d ‚äób) : (e ‚äóf)]
= (d ¬∑ e) (a ¬∑ c) (b ¬∑ f) ,

(a ‚äób) (e ‚äóf)T
: (c ‚äód) = [(a ‚äób) (f ‚äóe)] : (c ‚äód)
= (b ¬∑ f) [(a ‚äóe) : (c ‚äód)]
= (d ¬∑ e) (a ¬∑ c) (b ¬∑ f) .
For three arbitrary tensors A, B and C given in the form (1.82) we can write
in view of (1.120) and (1.136)
Ai
¬∑j

B k
i¬∑ C j
k¬∑

=

B k
i¬∑ Ai
¬∑j

C j
k¬∑ =

BTk
¬∑i Ai
¬∑j

C j
k¬∑,
Ai
¬∑j

B k
i¬∑ C j
k¬∑

=

Ai
¬∑jC j
k¬∑

B k
i¬∑ =

Ai
¬∑j

CTj
¬∑k

B k
i¬∑ .
(1.139)
Similarly we can prove that
A : B = AT : BT.
(1.140)
On the basis of the scalar product one deÔ¨Ånes the trace of second-order tensors
by:
trA = A : I.
(1.141)
For the tensor product (1.75) the trace (1.141) yields in view of (1.135)
tr (a ‚äób) = a ¬∑ b.
(1.142)
With the aid of the relation (1.138) we further write
tr (AB) = A : BT = AT : B.
(1.143)
In view of (D.1) this also implies that
tr (AB) = tr (BA) .
(1.144)

1.11 Decompositions of Second-Order Tensors
27
1.11 Decompositions of Second-Order Tensors
Additive decomposition into a symmetric and a skew-symmetric
part. Every second-order tensor can be decomposed additively into a sym-
metric and a skew-symmetric part by
A = symA + skewA,
(1.145)
where
symA = 1
2

A + AT
,
skewA = 1
2

A ‚àíAT
.
(1.146)
Symmetric and skew-symmetric tensors form subsets of Linn deÔ¨Åned by
Symn =

M ‚ààLinn : M = MT
,
(1.147)
Skewn =

W ‚ààLinn : W = ‚àíWT
.
(1.148)
One can easily show that these subsets represent vector spaces and can be
referred to as subspaces of Linn. Indeed, the axioms (A.1-A.4) and (B.1-B.4)
including operations with the zero tensor are valid both for symmetric and
skew-symmetric tensors. The zero tensor is the only linear mapping that is
both symmetric and skew-symmetric such that Symn‚à©Skewn = 0.
For every symmetric tensor M = Mijgi ‚äógj it follows from (1.119) that
Mij = Mji (i Ã∏= j, i, j = 1, 2, . . . , n). Thus, we can write
M =
n
	
i=1
Miigi ‚äógi +
n
	
i,j=1
i>j
Mij (gi ‚äógj + gj ‚äógi) ,
M ‚ààSymn. (1.149)
Similarly we can write for a skew-symmetric tensor
W =
n
	
i,j=1
i>j
Wij (gi ‚äógj ‚àígj ‚äógi) ,
W ‚ààSkewn
(1.150)
taking into account that Wii = 0 and Wij = ‚àíWji (i Ã∏= j, i, j = 1, 2, . . ., n).
Therefore, the basis of Symn is formed by n tensors gi ‚äógi and 1
2n (n ‚àí1)
tensors gi‚äógj +gj ‚äógi, while the basis of Skewn consists of 1
2n (n ‚àí1) tensors
gi ‚äógj ‚àígj ‚äógi, where i > j = 1, 2, . . . , n. Thus, the dimensions of Symn
and Skewn are 1
2n (n + 1) and 1
2n (n ‚àí1), respectively. It follows from (1.145)
that any basis of Skewn complements any basis of Symn to a basis of Linn.
Obviously, symmetric and skew-symmetric tensors are mutually orthogo-
nal such that (see Exercise 1.43)
M : W = 0,
‚àÄM ‚ààSymn, ‚àÄW ‚ààSkewn.
(1.151)
Spaces characterized by this property are called orthogonal.

28
1 Vectors and Tensors in a Finite-Dimensional Space
Additive decomposition into a spherical and a deviatoric part.
For every second-order tensor A we can write
A = sphA + devA,
(1.152)
where
sphA = 1
ntr (A) I,
devA = A ‚àí1
ntr (A) I
(1.153)
denote its spherical and deviatoric part, respectively. Thus, every spherical
tensor S can be represented by S = Œ±I, where Œ± is a scalar number. In turn,
every deviatoric tensor D is characterized by the condition trD = 0. Just like
symmetric and skew-symmetric tensors, spherical and deviatoric tensors form
orthogonal subspaces of Linn.
1.12 Tensors of Higher Orders
Similarly to second-order tensors we can deÔ¨Åne tensors of higher orders. For
example, a third-order tensor can be deÔ¨Åned as a linear mapping from En to
Linn. Thus, we can write
Y = Ax,
Y ‚ààLinn,
‚àÄx ‚ààEn,
‚àÄA ‚ààLinn,
(1.154)
where Linn denotes the set of all linear mappings of vectors in En into second-
order tensors in Linn. The tensors of the third order can likewise be repre-
sented with respect to a basis in Linn e.g. by
A = Aijkgi ‚äógj ‚äógk = Aijkgi ‚äógj ‚äógk
= Ai
¬∑jkgi ‚äógj ‚äógk = A j
i¬∑kgi ‚äógj ‚äógk.
(1.155)
For the components of the tensor A (1.155) we can thus write by analogy with
(1.139)
Aijk = Aij
¬∑¬∑sgsk = Ai
¬∑stgsjgtk = Arstgrigsjgtk,
Aijk = Ar
¬∑jkgri = Ars
¬∑¬∑kgrigsj = Arstgrigsjgtk.
(1.156)
Exercises
1.1. Prove that if x ‚ààV is a vector and Œ± ‚ààR is a scalar, then the following
identities hold.
(a) ‚àí0 = 0,
(b) Œ±0 = 0,
(c) 0x = 0,
(d) ‚àíx = (‚àí1) x,
(e) if Œ±x = 0,
then either Œ± = 0 or x = 0 or both.

1.12 Tensors of Higher Orders
29
1.2. Prove that xi Ã∏= 0 (i = 1, 2, . . ., n) for linearly independent vectors
x1, x2, . . . , xn. In other words, linearly independent vectors are all non-zero.
1.3. Prove that any non-empty subset of linearly independent vectors x1, x2,
. . . , xn is also linearly independent.
1.4. Write out in full the following expressions for n = 3: (a) Œ¥i
jaj, (b) Œ¥ijxixj,
(c) Œ¥i
i,
(d) ‚àÇfi
‚àÇxj dxj.
1.5. Prove that 0 ¬∑ x = 0, ‚àÄx ‚ààEn.
1.6. Prove that a set of mutually orthogonal non-zero vectors is always linearly
independent.
1.7. Prove the so-called parallelogram law: ‚à•x + y‚à•2 = ‚à•x‚à•2 + 2x ¬∑ y + ‚à•y‚à•2.
1.8. Let G = {g1, g2, . . . , gn} be a basis in En and a ‚ààEn be a vector. Prove
that a ¬∑ gi = 0 (i = 1, 2, . . ., n) if and only if a = 0.
1.9. Prove that a = b if and only if a ¬∑ x = b ¬∑ x, ‚àÄx ‚ààEn.
1.10. (a) Construct an orthonormal set of vectors orthogonalizing and nor-
malizing (with the aid of the procedure described in Sect. 1.4) the following
linearly independent vectors:
g1 =
‚éß
‚é®
‚é©
1
1
0
‚é´
‚é¨
‚é≠,
g2 =
‚éß
‚é®
‚é©
2
1
‚àí2
‚é´
‚é¨
‚é≠,
g3 =
‚éß
‚é®
‚é©
4
2
1
‚é´
‚é¨
‚é≠,
where the components are given with respect to an orthonormal basis.
(b) Construct a basis in E3 dual to the given above by means of (1.21)1, (1.24)
and (1.25)2.
(c) Calculate again the vectors gi dual to gi (i = 1, 2, 3) by using relations
(1.33) and (1.35). Compare the result with the solution of problem (b).
1.11. Verify that the vectors (1.33) are linearly independent.
1.12. Prove identity (1.41) by means of (1.18), (1.19) and (1.36).
1.13. Prove relations (1.39) and (1.42) using (1.33), (1.36) and (1.41).
1.14. Verify the following identities involving the permutation symbol (1.36)
for n = 3: (a) Œ¥ijeijk = 0, (b) eikmejkm = 2Œ¥i
j, (c) eijkeijk = 6, (d) eijmeklm =
Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k.
1.15. Prove the identity
(a √ó b) √ó c = (a ¬∑ c) b ‚àí(b ¬∑ c) a,
‚àÄa, b, c ‚ààE3.
(1.157)

30
1 Vectors and Tensors in a Finite-Dimensional Space
1.16. Prove that A0 = 0A = 0, ‚àÄA ‚ààLinn.
1.17. Prove that 0A = 0, ‚àÄA ‚ààLinn.
1.18. Prove formula (1.57), where the negative tensor ‚àíA is deÔ¨Åned by (1.52).
1.19. Prove relation (1.80).
1.20. Prove (1.83) using (1.82) and (1.15).
1.21. Evaluate the tensor W = ÀÜw = w√ó, where w = wigi.
1.22. Evaluate components of the tensor describing a rotation about the axis
e3 by the angle Œ±.
1.23. Let A = Aijgi ‚äógj , where

Aij
=
‚é°
‚é£
0 ‚àí1 0
0 0 0
1 0 0
‚é§
‚é¶
and the vectors gi are given in Exercise 1.10. Evaluate the components Aij,
Ai
¬∑j and A j
i¬∑.
1.24. Prove identities (1.100) and (1.102).
1.25. Let A = Ai
¬∑jgi‚äógj, B = Bi
¬∑jgi‚äógj, C = Ci
¬∑jgi‚äógj and D = Di
¬∑jgi‚äógj,
where

Ai
¬∑j

=
‚é°
‚é£
0 2 0
0 0 0
0 0 0
‚é§
‚é¶,

Bi
¬∑j

=
‚é°
‚é£
0 0 0
0 0 0
0 0 1
‚é§
‚é¶,

Ci
¬∑j

=
‚é°
‚é£
1 2 3
0 0 0
0 1 0
‚é§
‚é¶,

Di
¬∑j

=
‚é°
‚é£
1
0
0
0 1/2 0
0
0
10
‚é§
‚é¶.
Find commutative pairs of tensors.
1.26. Let A and B be two commutative tensors. Write out in full (A + B)k,
where k = 2, 3, . . .
1.27. Prove that
exp (A + B) = exp (A) exp (B) ,
(1.158)
where A and B commute.
1.28. Prove that exp (kA) = [exp (A)]k, where k = 2, 3, . . .
1.29. Evaluate exp (0) and exp (I).

1.12 Tensors of Higher Orders
31
1.30. Prove that exp (‚àíA) exp (A) = exp (A) exp (‚àíA) = I.
1.31. Prove that exp (A + B) = exp (A) + exp (B) if AB = BA = 0.
1.32. Prove that exp

QAQT
= Q exp (A)QT, ‚àÄQ ‚ààOrthn.
1.33. Compute the exponential of the tensors D = Di
¬∑jgi ‚äógj, E = Ei
¬∑jgi ‚äógj
and F = Fi
¬∑jgi ‚äógj, where

Di
¬∑j

=
‚é°
‚é£
2 0 0
0 3 0
0 0 1
‚é§
‚é¶,

Ei
¬∑j

=
‚é°
‚é£
0 1 0
0 0 0
0 0 0
‚é§
‚é¶,

Fi
¬∑j

=
‚é°
‚é£
0 2 0
0 0 0
0 0 1
‚é§
‚é¶.
1.34. Prove that (ABCD)T = DTCTBTAT.
1.35. Verify that

AkT =

ATk, where k = 1, 2, 3, . . .
1.36. Evaluate the components Bij, Bij, Bi
¬∑j and B j
i¬∑ of the tensor B = AT,
where A is deÔ¨Åned in Exercise 1.23.
1.37. Prove relation (1.126).
1.38. Verify that

A‚àí1k =

Ak‚àí1 = A‚àík, where k = 1, 2, 3, . . .
1.39. Prove identity (1.135) using (1.82) and (1.134).
1.40. Prove by means of (1.134-1.136) the properties of the scalar product
(D.1-D.3).
1.41. Verify that [(a ‚äób) (c ‚äód)] : I = (a ¬∑ d) (b ¬∑ c).
1.42. Express trA in terms of the components Ai
¬∑j, Aij, Aij.
1.43. Prove that M : W = 0, where M is a symmetric tensor and W a skew-
symmetric tensor.
1.44. Evaluate trWk, where W is a skew-symmetric tensor and k = 1, 3, 5, . . .
1.45. Verify that sym (skewA) = skew (symA) = 0, ‚àÄA ‚ààLinn.
1.46. Prove that sph (devA) = dev (sphA) = 0, ‚àÄA ‚ààLinn.

2
Vector and Tensor Analysis in Euclidean Space
2.1 Vector- and Tensor-Valued Functions, DiÔ¨Äerential
Calculus
In the following we consider a vector-valued function x (t) and a tensor-valued
function A (t) of a real variable t. Henceforth, we assume that these functions
are continuous such that
lim
t‚Üít0 [x (t) ‚àíx (t0)] = 0,
lim
t‚Üít0 [A (t) ‚àíA (t0)] = 0
(2.1)
for all t0 within the deÔ¨Ånition domain. The functions x (t) and A (t) are called
diÔ¨Äerentiable if the following limits
dx
dt = lim
s‚Üí0
x (t + s) ‚àíx (t)
s
,
dA
dt = lim
s‚Üí0
A (t + s) ‚àíA (t)
s
(2.2)
exist and are Ô¨Ånite. They are referred to as the derivatives of the vector- and
tensor-valued functions x (t) and A (t), respectively.
For diÔ¨Äerentiable vector- and tensor-valued functions the usual rules of
diÔ¨Äerentiation hold.
1) Product of a scalar function with a vector- or tensor-valued function:
d
dt [u (t) x (t)] = du
dt x (t) + u (t) dx
dt ,
(2.3)
d
dt [u (t) A (t)] = du
dt A (t) + u (t) dA
dt .
(2.4)
2) Mapping of a vector-valued function by a tensor-valued function:
d
dt [A (t) x (t)] = dA
dt x (t) + A (t) dx
dt .
(2.5)

34
2 Vector and Tensor Analysis in Euclidean Space
3) Scalar product of two vector- or tensor-valued functions:
d
dt [x (t) ¬∑ y (t)] = dx
dt ¬∑ y (t) + x (t) ¬∑ dy
dt ,
(2.6)
d
dt [A (t) : B (t)] = dA
dt : B (t) + A (t) : dB
dt .
(2.7)
4) Tensor product of two vector-valued functions:
d
dt [x (t) ‚äóy (t)] = dx
dt ‚äóy (t) + x (t) ‚äódy
dt .
(2.8)
5) Composition of two tensor-valued functions:
d
dt [A (t) B (t)] = dA
dt B (t) + A (t) dB
dt .
(2.9)
6) Chain rule:
d
dtx [u (t)] = dx
du
du
dt ,
d
dtA [u (t)] = dA
du
du
dt .
(2.10)
7) Chain rule for functions of several arguments:
d
dtx [u (t) ,v (t)] = ‚àÇx
‚àÇu
du
dt + ‚àÇx
‚àÇv
dv
dt ,
(2.11)
d
dtA [u (t) ,v (t)] = ‚àÇA
‚àÇu
du
dt + ‚àÇA
‚àÇv
dv
dt ,
(2.12)
where ‚àÇ/‚àÇu denotes the partial derivative. It is deÔ¨Åned for vector and
tensor valued functions in the standard manner by
‚àÇx (u,v)
‚àÇu
= lim
s‚Üí0
x (u + s,v) ‚àíx (u,v)
s
,
(2.13)
‚àÇA (u,v)
‚àÇu
= lim
s‚Üí0
A (u + s,v) ‚àíA (u,v)
s
.
(2.14)
The above diÔ¨Äerentiation rules can be veriÔ¨Åed with the aid of elementary
diÔ¨Äerential calculus. For example, for the derivative of the composition of two
second-order tensors (2.9) we proceed as follows. Let us deÔ¨Åne two tensor-
valued functions by
O1 (s) = A (t + s) ‚àíA (t)
s
‚àídA
dt ,
O2 (s) = B (t + s) ‚àíB (t)
s
‚àídB
dt .
Bearing the deÔ¨Ånition of the derivative (2.2) in mind we have
lim
s‚Üí0 O1 (s) = 0,
lim
s‚Üí0 O2 (s) = 0.

2.2 Coordinates in Euclidean Space, Tangent Vectors
35
Then,
d
dt [A (t) B (t)] = lim
s‚Üí0
A (t + s) B (t + s) ‚àíA (t) B (t)
s
= lim
s‚Üí0
1
s
!"
A (t) + sdA
dt + sO1 (s)
# "
B (t) + sdB
dt + sO2 (s)
#
‚àíA (t) B (t)
$
= lim
s‚Üí0
!"dA
dt + O1 (s)
#
B (t) + A (t)
"dB
dt + O2 (s)
#$
+ lim
s‚Üí0 s
"dA
dt + O1 (s)
# "dB
dt + O2 (s)
#
= dA
dt B (t) + A (t) dB
dt .
2.2 Coordinates in Euclidean Space, Tangent Vectors
DeÔ¨Ånition 2.1. A coordinate system is a one to one correspondence between
vectors in the n-dimensional Euclidean space En and a set of n real numbers
(x1, x2, . . . , xn). These numbers are called coordinates of the corresponding
vectors.
Thus, we can write
xi = xi (r)
‚áî
r = r

x1, x2, . . . , xn
,
(2.15)
where r ‚ààEn and xi ‚ààR (i = 1, 2, . . ., n). Henceforth, we assume that the
functions xi = xi (r) and r = r

x1, x2, . . . , xn
are suÔ¨Éciently diÔ¨Äerentiable.
Example. Cylindrical coordinates in E3. The cylindrical coordinates
(Fig. 2.1) are deÔ¨Åned by
r = r (œï, z, r) = r cos œïe1 + r sin œïe2 + ze3
(2.16)
and
r =

(r ¬∑ e1)2 + (r ¬∑ e2)2,
z = r ¬∑ e3,
œï =
‚éß
‚é®
‚é©
arccos r ¬∑ e1
r
if r ¬∑ e2 ‚â•0,
2œÄ ‚àíarccos r ¬∑ e1
r
if r ¬∑ e2 < 0,
(2.17)
where ei (i = 1, 2, 3) form an orthonormal basis in E3.

36
2 Vector and Tensor Analysis in Euclidean Space
œï
e1
r
x1
e2
x2
x3 = z
g3
g1
g2
e3
r
Fig. 2.1. Cylindrical coordinates in three-dimensional space
The vector components with respect to a Ô¨Åxed basis, say H = {h1, h2, . . . ,
hn}, obviously represent its coordinates. Indeed, according to Theorem 1.5 of
the previous chapter the following correspondence is one to one
r = xihi
‚áî
xi = r ¬∑ hi,
i = 1, 2, . . ., n,
(2.18)
where r ‚ààEn and H‚Ä≤ =

h1, h2, . . . , hn
is the basis dual to H. The compo-
nents xi (2.18)2 are referred to as the linear coordinates of the vector r.
Let xi = xi (r) and yi = yi (r) (i = 1, 2, . . . , n) be two arbitrary coordinate
systems in En. Since their correspondences are one to one, the functions
xi = ÀÜxi 
y1, y2, . . . , yn
‚áîyi = ÀÜyi 
x1, x2, . . . , xn
, i = 1, 2, . . ., n (2.19)
are invertible. These functions describe the transformation of the coordinate
systems. Inserting one relation (2.19) into another one yields
yi = ÀÜyi 
ÀÜx1 
y1, y2, . . . , yn
,
ÀÜx2 
y1, y2, . . . , yn
, . . . , ÀÜxn 
y1, y2, . . . , yn
.
(2.20)

2.2 Coordinates in Euclidean Space, Tangent Vectors
37
The further diÔ¨Äerentiation with respect to yj delivers with the aid of the chain
rule
‚àÇyi
‚àÇyj = Œ¥ij = ‚àÇyi
‚àÇxk
‚àÇxk
‚àÇyj ,
i, j = 1, 2, . . . , n.
(2.21)
The determinant of the matrix (2.21) takes the form
|Œ¥ij| = 1 =

‚àÇyi
‚àÇxk
‚àÇxk
‚àÇyj
 =

‚àÇyi
‚àÇxk


‚àÇxk
‚àÇyj
 .
(2.22)
The determinant
‚àÇyi/‚àÇxk on the right hand side of (2.22) is referred to as Ja-
cobian determinant of the coordinate transformation yi = ÀÜyi 
x1, x2, . . . , xn
(i = 1, 2, . . . , n). Thus, we have proved the following theorem.
Theorem 2.1. If the transformation of the coordinates yi = ÀÜyi 
x1, x2, . . . , xn
admits an inverse form xi = ÀÜxi 
y1, y2, . . . , yn
(i = 1, 2, . . . , n) and if J and
K are the Jacobians of these transformations then JK = 1.
One of the important consequences of this theorem is that
J =

‚àÇyi
‚àÇxk
 Ã∏= 0.
(2.23)
Now, we consider an arbitrary curvilinear coordinate system
Œ∏i = Œ∏i (r) ‚áîr = r

Œ∏1, Œ∏2, . . . , Œ∏n
,
(2.24)
where r ‚ààEn and Œ∏i ‚ààR (i = 1, 2, . . . , n). The equations
Œ∏i = const, i = 1, 2, . . . , k ‚àí1, k + 1, . . . , n
(2.25)
deÔ¨Åne a curve in En called Œ∏k-coordinate line. The vectors
gk = ‚àÇr
‚àÇŒ∏k ,
k = 1, 2, . . ., n
(2.26)
are called the tangent vectors to the corresponding Œ∏k-coordinate lines (2.25).
One can verify that the tangent vectors are linearly independent and form thus
a basis of En. Conversely, let the vectors (2.26) be linearly dependent. Then,
there are scalars Œ±i ‚ààR (i = 1, 2, . . . , n), not all zero, such that Œ±igi = 0. Let
further xi = xi (r) (i = 1, 2, . . ., n) be linear coordinates in En with respect
to a basis H = {h1, h2, . . . , hn}. Then,
0 = Œ±igi = Œ±i ‚àÇr
‚àÇŒ∏i = Œ±i ‚àÇr
‚àÇxj
‚àÇxj
‚àÇŒ∏i = Œ±i ‚àÇxj
‚àÇŒ∏i hj.
Since the basis vectors hj (j = 1, 2, . . ., n) are linearly independent
Œ±i ‚àÇxj
‚àÇŒ∏i = 0,
j = 1, 2, . . . , n.

38
2 Vector and Tensor Analysis in Euclidean Space
This is a homogeneous linear equation system with a non-trivial solution
Œ±i (i = 1, 2, . . . , n). Hence,
‚àÇxj/‚àÇŒ∏i = 0, which obviously contradicts re-
lation (2.23).
Example. Tangent vectors and metric coeÔ¨Écients of cylindrical
coordinates in E3. By means of (2.16) and (2.26) we obtain
g1 = ‚àÇr
‚àÇœï = ‚àír sin œïe1 + r cos œïe2,
g2 = ‚àÇr
‚àÇz = e3,
g3 = ‚àÇr
‚àÇr = cos œïe1 + sin œïe2.
(2.27)
The metric coeÔ¨Écients take by virtue of (1.24) and (1.25)2 the form
[gij] = [gi ¬∑ gj] =
‚é°
‚é£
r2 0 0
0 1 0
0 0 1
‚é§
‚é¶,

gij
= [gij]‚àí1 =
‚é°
‚é£
r‚àí2 0 0
0
1 0
0
0 1
‚é§
‚é¶.
(2.28)
The dual basis results from (1.21)1 by
g1 = 1
r2 g1 = ‚àí1
r sin œïe1 + 1
r cos œïe2,
g2 = g2 = e3,
g3 = g3 = cos œïe1 + sin œïe2.
(2.29)
2.3 Coordinate Transformation. Co-, Contra- and Mixed
Variant Components
Let Œ∏i = Œ∏i (r) and ¬ØŒ∏i = ¬ØŒ∏i (r) (i = 1, 2, . . ., n) be two arbitrary coordinate
systems in En. It holds
¬Øgi = ‚àÇr
‚àÇ¬ØŒ∏i = ‚àÇr
‚àÇŒ∏j
‚àÇŒ∏j
‚àÇ¬ØŒ∏i = gj
‚àÇŒ∏j
‚àÇ¬ØŒ∏i ,
i = 1, 2, . . . , n.
(2.30)
If gi is the dual basis to gi (i = 1, 2, . . . , n), then we can write
¬Øgi = gj ‚àÇ¬ØŒ∏i
‚àÇŒ∏j ,
i = 1, 2, . . ., n.
(2.31)
Indeed,
¬Øgi ¬∑ ¬Øgj =
%
gk ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
&
¬∑
%
gl
‚àÇŒ∏l
‚àÇ¬ØŒ∏j
&
= gk ¬∑ gl
% ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
‚àÇŒ∏l
‚àÇ¬ØŒ∏j
&
= Œ¥k
l
% ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
‚àÇŒ∏l
‚àÇ¬ØŒ∏j
&
= ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
‚àÇŒ∏k
‚àÇ¬ØŒ∏j = ‚àÇ¬ØŒ∏i
‚àÇ¬ØŒ∏j = Œ¥i
j, i, j = 1, 2, . . ., n.
(2.32)

2.3 Co-, Contra- and Mixed Variant Components
39
One can observe the diÔ¨Äerence in the transformation of the dual vectors (2.30)
and (2.31) which results from the change of the coordinate system. The trans-
formation rules of the form (2.30) and (2.31) and the corresponding variables
are referred to as covariant and contravariant, respectively. Covariant and
contravariant variables are denoted by lower and upper indices, respectively.
The co- and contravariant rules can also be recognized in the transforma-
tion of the components of vectors and tensors if they are related to tangent
vectors. Indeed, let
x = xigi = xigi = ¬Øxi¬Øgi = ¬Øxi¬Øgi,
(2.33)
A = Aijgi ‚äógj = Aijgi ‚äógj = Ai
¬∑jgi ‚äógj
= ¬ØAij¬Øgi ‚äó¬Øgj = ¬ØA
ij¬Øgi ‚äó¬Øgj = ¬ØA
i
¬∑j¬Øgi ‚äó¬Øgj.
(2.34)
Then, by means of (1.28), (1.83), (2.30) and (2.31) we obtain
¬Øxi = x ¬∑ ¬Øgi = x ¬∑
%
gj
‚àÇŒ∏j
‚àÇ¬ØŒ∏i
&
= xj
‚àÇŒ∏j
‚àÇ¬ØŒ∏i ,
(2.35)
¬Øxi = x ¬∑ ¬Øgi = x ¬∑
%
gj ‚àÇ¬ØŒ∏i
‚àÇŒ∏j
&
= xj ‚àÇ¬ØŒ∏i
‚àÇŒ∏j ,
(2.36)
¬ØAij = ¬ØgiA¬Øgj =
%
gk
‚àÇŒ∏k
‚àÇ¬ØŒ∏i
&
A
%
gl
‚àÇŒ∏l
‚àÇ¬ØŒ∏j
&
= ‚àÇŒ∏k
‚àÇ¬ØŒ∏i
‚àÇŒ∏l
‚àÇ¬ØŒ∏j Akl,
(2.37)
¬ØA
ij = ¬ØgiA¬Øgj =
%
gk ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
&
A
%
gl ‚àÇ¬ØŒ∏j
‚àÇŒ∏l
&
= ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
‚àÇ¬ØŒ∏j
‚àÇŒ∏l Akl,
(2.38)
¬ØA
i
¬∑j = ¬ØgiA¬Øgj =
%
gk ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
&
A
%
gl
‚àÇŒ∏l
‚àÇ¬ØŒ∏j
&
= ‚àÇ¬ØŒ∏i
‚àÇŒ∏k
‚àÇŒ∏l
‚àÇ¬ØŒ∏j Ak
¬∑l.
(2.39)
Accordingly, the vector and tensor components xi, Aij and xi, Aij are called
covariant and contravariant, respectively. The tensor components Ai
¬∑j are re-
ferred to as mixed variant. The transformation rules (2.35-2.39) can similarly
be written for tensors of higher orders as well. For example, one obtains for
third-order tensors
¬ØAijk = ‚àÇŒ∏r
‚àÇ¬ØŒ∏i
‚àÇŒ∏s
‚àÇ¬ØŒ∏j
‚àÇŒ∏t
‚àÇ¬ØŒ∏k Arst,
¬ØAijk = ‚àÇ¬ØŒ∏i
‚àÇŒ∏r
‚àÇ¬ØŒ∏j
‚àÇŒ∏s
‚àÇ¬ØŒ∏k
‚àÇŒ∏t Arst, . . .
(2.40)
From the very beginning we have supplied coordinates with upper indices
which imply the contravariant transformation rule. Indeed, let us consider
the transformation of a coordinate system ¬ØŒ∏i = ¬ØŒ∏i 
Œ∏1, Œ∏2, . . . , Œ∏n
(i = 1, 2,
. . . , n). It holds:
d¬ØŒ∏i = ‚àÇ¬ØŒ∏i
‚àÇŒ∏k dŒ∏k,
i = 1, 2, . . ., n.
(2.41)

40
2 Vector and Tensor Analysis in Euclidean Space
Thus, the diÔ¨Äerentials of the coordinates really transform according to the
contravariant law (2.31).
Example. Transformation of linear coordinates into cylindrical
ones (2.16). Let xi = xi (r) be linear coordinates with respect to an or-
thonormal basis ei (i = 1, 2, 3) in E3:
xi = r ¬∑ ei
‚áî
r = xiei.
(2.42)
By means of (2.16) one can write
x1 = r cos œï,
x2 = r sin œï,
x3 = z
(2.43)
and consequently
‚àÇx1
‚àÇœï = ‚àír sin œï = ‚àíx2,
‚àÇx1
‚àÇz = 0,
‚àÇx1
‚àÇr = cos œï = x1
r ,
‚àÇx2
‚àÇœï = r cos œï = x1,
‚àÇx2
‚àÇz = 0,
‚àÇx2
‚àÇr = sin œï = x2
r ,
‚àÇx3
‚àÇœï = 0,
‚àÇx3
‚àÇz = 1,
‚àÇx3
‚àÇr = 0.
(2.44)
The reciprocal derivatives can easily be obtained from (2.22) by inverting the
matrix

‚àÇxi
‚àÇœï
‚àÇxi
‚àÇz
‚àÇxi
‚àÇr

according to Theorem 2.1. This yields:
‚àÇœï
‚àÇx1 = ‚àí1
r sin œï = ‚àíx2
r2 ,
‚àÇœï
‚àÇx2 = 1
r cos œï = x1
r2 ,
‚àÇœï
‚àÇx3 = 0,
‚àÇz
‚àÇx1 = 0,
‚àÇz
‚àÇx2 = 0,
‚àÇz
‚àÇx3 = 1,
‚àÇr
‚àÇx1 = cos œï = x1
r ,
‚àÇr
‚àÇx2 = sin œï = x2
r ,
‚àÇr
‚àÇx3 = 0.
(2.45)
2.4 Gradient, Covariant and Contravariant Derivatives
Let Œ¶ = Œ¶

Œ∏1, Œ∏2, . . . , Œ∏n
, x = x

Œ∏1, Œ∏2, . . . , Œ∏n
and A = A

Œ∏1, Œ∏2, . . . , Œ∏n
be, respectively, a scalar-, a vector- and a tensor-valued diÔ¨Äerentiable function
of the coordinates Œ∏i ‚ààR (i = 1, 2, . . . , n). Such functions of coordinates are
generally referred to as Ô¨Åelds, as for example, the scalar Ô¨Åeld, the vector Ô¨Åeld
or the tensor Ô¨Åeld. Due to the one to one correspondence (2.24) these Ô¨Åelds
can alternatively be represented by
Œ¶ = Œ¶ (r) ,
x = x (r) ,
A = A (r) .
(2.46)

2.4 Gradient, Covariant and Contravariant Derivatives
41
In the following we assume that the so-called directional derivatives of the
functions (2.46)
d
dsŒ¶ (r + sa)

s=0
= lim
s‚Üí0
Œ¶ (r + sa) ‚àíŒ¶ (r)
s
,
d
dsx (r + sa)

s=0
= lim
s‚Üí0
x (r + sa) ‚àíx (r)
s
,
d
dsA (r + sa)

s=0
= lim
s‚Üí0
A (r + sa) ‚àíA (r)
s
(2.47)
exist for all a ‚ààEn. Further, one can show that the mappings a ‚Üí
d
dsŒ¶ (r + sa)

s=0, a ‚Üí
d
dsx (r + sa)

s=0 and a ‚Üí
d
dsA (r + sa)

s=0 are lin-
ear with respect to the vector a. For example, we can write for the directional
derivative of the scalar function Œ¶ = Œ¶ (r)
d
dsŒ¶ [r + s (a + b)]

s=0
= d
dsŒ¶ [r + s1a + s2b]

s=0
,
(2.48)
where s1 and s2 are assumed to be functions of s such that s1 = s and s2 = s.
With the aid of the chain rule this delivers
d
dsŒ¶ [r + s1a + s2b]

s=0
=
! ‚àÇ
‚àÇs1
Œ¶ [r + s1a + s2b] ds1
ds +
‚àÇ
‚àÇs2
Œ¶ [r + s1a + s2b] ds2
ds
$
s=0
=
‚àÇ
‚àÇs1
Œ¶ (r + s1a + s2b)

s1=0,s2=0
+
‚àÇ
‚àÇs2
Œ¶ (r + s1a + s2b)

s1=0,s2=0
= d
dsŒ¶ (r + sa)

s=0
+ d
dsŒ¶ (r + sb)

s=0
and Ô¨Ånally
d
dsŒ¶ [r + s (a + b)]

s=0
= d
dsŒ¶ (r + sa)

s=0
+ d
dsŒ¶ (r + sb)

s=0
(2.49)
for all a, b ‚ààEn. In a similar fashion we can write
d
dsŒ¶ (r + sŒ±a)

s=0
=
d
d (Œ±s)Œ¶ (r + sŒ±a) d (Œ±s)
ds

s=0
= Œ± d
dsŒ¶ (r + sa)

s=0
, ‚àÄa ‚ààEn, ‚àÄŒ± ‚ààR.
(2.50)
Thus, comparing (2.49) and (2.50) with the properties of the scalar product
(C.2) and (C.3) we can represent the directional derivative by

42
2 Vector and Tensor Analysis in Euclidean Space
d
dsŒ¶ (r + sa)

s=0
= gradŒ¶ ¬∑ a,
‚àÄa ‚ààEn,
(2.51)
where the vector gradŒ¶ ‚ààEn is referred to as gradient of the function Œ¶ =
Œ¶ (r).
Example. Gradient of the scalar function ‚à•r‚à•. Using the deÔ¨Ånition
of the directional derivative (2.47) we can write
d
ds ‚à•r + sa‚à•

s=0
= d
ds

(r + sa) ¬∑ (r + sa)

s=0
= d
ds

r ¬∑ r + 2s (r ¬∑ a) + s2 (a ¬∑ a)

s=0
= 1
2
2 (r ¬∑ a) + 2s (a ¬∑ a)

r ¬∑ r + 2s (r ¬∑ a) + s2 (a ¬∑ a)

s=0
= r ¬∑ a
‚à•r‚à•.
Comparing this result with (2.51) delivers
grad‚à•r‚à•=
r
‚à•r‚à•.
(2.52)
Similarly to (2.51) one deÔ¨Ånes the gradient of the vector function x = x (r)
and the gradient of the tensor function A = A (r):
d
dsx (r + sa)

s=0
= (gradx) a,
‚àÄa ‚ààEn,
(2.53)
d
dsA (r + sa)

s=0
= (gradA) a,
‚àÄa ‚ààEn.
(2.54)
Herein, gradx and gradA represent tensors of second and third order, respec-
tively.
In order to evaluate the above gradients (2.51), (2.53) and (2.54) we rep-
resent the vectors r and a with respect to the linear coordinates (2.18) as
r = xihi,
a = aihi.
(2.55)
With the aid of the chain rule we can further write for the directional deriva-
tive of the function Œ¶ = Œ¶ (r):
d
dsŒ¶ (r + sa)

s=0
= d
dsŒ¶

xi + sai
hi

s=0
=
‚àÇŒ¶
‚àÇ(xi + sai)
d

xi + sai
ds

s=0
= ‚àÇŒ¶
‚àÇxi ai
=
% ‚àÇŒ¶
‚àÇxi hi
&
¬∑

ajhj

=
% ‚àÇŒ¶
‚àÇxi hi
&
¬∑ a,
‚àÄa ‚ààEn.

2.4 Gradient, Covariant and Contravariant Derivatives
43
Comparing this result with (2.51) and bearing in mind that it holds for all
vectors a we obtain
gradŒ¶ = ‚àÇŒ¶
‚àÇxi hi.
(2.56)
The representation (2.56) can be rewritten in terms of arbitrary curvilinear co-
ordinates r = r

Œ∏1, Œ∏2, . . . , Œ∏n
and the corresponding tangent vectors (2.26).
Indeed, in view of (2.31) and (2.56)
gradŒ¶ = ‚àÇŒ¶
‚àÇxi hi = ‚àÇŒ¶
‚àÇŒ∏k
‚àÇŒ∏k
‚àÇxi hi = ‚àÇŒ¶
‚àÇŒ∏i gi.
(2.57)
According to the deÔ¨Ånition (2.51) the gradient is independent of the choice
of the coordinate system. This can also be seen from relation (2.57). Indeed,
taking (2.31) into account we can write for an arbitrary coordinate system
¬ØŒ∏i = ¬ØŒ∏i 
Œ∏1, Œ∏2, . . . , Œ∏n
(i = 1, 2, . . ., n):
gradŒ¶ = ‚àÇŒ¶
‚àÇŒ∏i gi = ‚àÇŒ¶
‚àÇ¬ØŒ∏j
‚àÇ¬ØŒ∏j
‚àÇŒ∏i gi = ‚àÇŒ¶
‚àÇ¬ØŒ∏j ¬Øgj.
(2.58)
Similarly to relation (2.57) one can express the gradients of the vector-valued
function x = x (r) and the tensor-valued function A = A (r) by
gradx = ‚àÇx
‚àÇŒ∏i ‚äógi,
gradA = ‚àÇA
‚àÇŒ∏i ‚äógi.
(2.59)
Henceforth, the derivatives of the functions Œ¶ = Œ¶

Œ∏1, Œ∏2, . . . , Œ∏n
, x =
x

Œ∏1, Œ∏2, . . . , Œ∏n
and A = A

Œ∏1, Œ∏2, . . . , Œ∏n
with respect to curvilinear coor-
dinates Œ∏i will be denoted shortly by
Œ¶,i = ‚àÇŒ¶
‚àÇŒ∏i ,
x,i = ‚àÇx
‚àÇŒ∏i ,
A,i = ‚àÇA
‚àÇŒ∏i .
(2.60)
They obey the covariant transformation rule (2.30) with respect to the index
i since
‚àÇŒ¶
‚àÇŒ∏i = ‚àÇŒ¶
‚àÇ¬ØŒ∏k
‚àÇ¬ØŒ∏k
‚àÇŒ∏i ,
‚àÇx
‚àÇŒ∏i = ‚àÇx
‚àÇ¬ØŒ∏k
‚àÇ¬ØŒ∏k
‚àÇŒ∏i ,
‚àÇA
‚àÇŒ∏i = ‚àÇA
‚àÇ¬ØŒ∏k
‚àÇ¬ØŒ∏k
‚àÇŒ∏i
(2.61)
and represent again a scalar, a vector and a second-order tensor, respectively.
The latter ones can be represented with respect to a basis as
x,i = xj|i gj = xj|i gj,
A,i = Akl|i gk ‚äógl = Akl|i gk ‚äógl = Ak
¬∑ l|i gk ‚äógl,
(2.62)
where (‚Ä¢)|i denotes some diÔ¨Äerential operator on the components of the vector
x or the tensor A. In view of (2.61) and (2.62) this operator transforms with
respect to the index i according to the covariant rule and is called covariant

44
2 Vector and Tensor Analysis in Euclidean Space
derivative. The covariant type of the derivative is accentuated by the lower
position of the coordinate index.
On the basis of the covariant derivative we can also deÔ¨Åne the contravariant
one. To this end, we formally apply the rule of component transformation
(1.90)1 as (‚Ä¢)|i= gij (‚Ä¢)|j. Accordingly,
xj|i= gikxj|k,
xj|i= gikxj|k,
Akl|i= gimAkl|m,
Akl|i= gimAkl|m,
Ak
¬∑ l|i= gimAk
¬∑ l|m .
(2.63)
For scalar functions the covariant and the contravariant derivatives identically
coincide with the partial one:
Œ¶,i = Œ¶|i= Œ¶|i .
(2.64)
In view of (2.58-2.60), (2.62) and (2.64) the gradients of the functions Œ¶ =
Œ¶

Œ∏1, Œ∏2, . . . , Œ∏n
, x = x

Œ∏1, Œ∏2, . . . , Œ∏n
and A = A

Œ∏1, Œ∏2, . . . , Œ∏n
take the
form
gradŒ¶ = Œ¶|i gi = Œ¶|i gi,
gradx = xj|i gj ‚äógi = xj|i gj ‚äógi = xj|i gj ‚äógi = xj|i gj ‚äógi,
gradA = Akl|i gk ‚äógl ‚äógi = Akl|i gk ‚äógl ‚äógi = Ak
¬∑ l|i gk ‚äógl ‚äógi
= Akl|i gk ‚äógl ‚äógi = Akl|i gk ‚äógl ‚äógi = Ak
¬∑ l|i gk ‚äógl ‚äógi.
(2.65)
2.5 ChristoÔ¨Äel Symbols, Representation of the Covariant
Derivative
In the previous section we have introduced the notion of the covariant deriva-
tive but have not so far discussed how it can be taken. Now, we are going to
formulate a procedure constructing the diÔ¨Äerential operator of the covariant
derivative. In other words, we would like to express the covariant derivative in
terms of the vector or tensor components. To this end, the partial derivatives
of the tangent vectors (2.26) with respect to the coordinates are Ô¨Årst needed.
Since these derivatives again represent vectors in En, they can be expressed
in terms of the tangent vectors gi or dual vectors gi (i = 1, 2, . . . , n) both
forming bases of En. Thus, one can write
gi,j = Œìijkgk = Œìk
ijgk,
i, j = 1, 2, . . ., n,
(2.66)
where the components Œìijk and Œìk
ij (i, j, k = 1, 2, . . . , n) are referred to as the
ChristoÔ¨Äel symbols of the Ô¨Årst and second kind, respectively. In view of the

2.5 ChristoÔ¨Äel Symbols, Representation of the Covariant Derivative
45
relation gk = gklgl (k = 1, 2, . . ., n) (1.21) these symbols are connected with
each other by
Œìk
ij = gklŒìijl,
i, j, k = 1, 2, . . . , n.
(2.67)
Keeping the deÔ¨Ånition of tangent vectors (2.26) in mind we further obtain
gi,j = r,ij = r,ji = gj,i ,
i, j = 1, 2, . . . , n.
(2.68)
With the aid of (1.28) the ChristoÔ¨Äel symbols can thus be expressed by
Œìijk = Œìjik = gi,j ¬∑gk = gj,i ¬∑gk,
(2.69)
Œìk
ij = Œìk
ji = gi,j ¬∑gk = gj,i ¬∑gk,
i, j, k = 1, 2, . . ., n.
(2.70)
For the dual basis gi (i = 1, 2, . . . , n) one further gets by diÔ¨Äerentiating the
identities gi ¬∑ gj = Œ¥i
j (1.15):
0 =

Œ¥i
j

,k =

gi ¬∑ gj

,k = gi,k ¬∑gj + gi ¬∑ gj,k
= gi,k ¬∑gj + gi ¬∑

Œìl
jkgl

= gi,k ¬∑gj + Œìi
jk,
i, j, k = 1, 2, . . . , n.
Hence,
Œìi
jk = Œìi
kj = ‚àígi,k ¬∑gj = ‚àígi,j ¬∑gk,
i, j, k = 1, 2, . . . , n
(2.71)
and consequently
gi,k = ‚àíŒìi
jkgj = ‚àíŒìi
kjgj,
i, k = 1, 2, . . . , n.
(2.72)
By means of the identity
gij,k = (gi ¬∑ gj) ,k = gi,k ¬∑gj + gi ¬∑ gj,k ,
i, j, k = 1, 2, . . ., n
(2.73)
and in view of (2.67-2.69) we Ô¨Ånally obtain
Œìijk = 1
2 (gki,j +gkj,i ‚àígij,k ) ,
(2.74)
Œìk
ij = 1
2gkl (gli,j +glj,i ‚àígij,l ) ,
i, j, k = 1, 2, . . ., n.
(2.75)
It is seen from (2.74) and (2.75) that all ChristoÔ¨Äel symbols identically vanish
in the Cartesian coordinates deÔ¨Åned with respect to an orthonormal basis as
r = xiei.
(2.76)
Indeed, in this case
gij = ei ¬∑ ej = Œ¥ij,
i, j = 1, 2, . . ., n
(2.77)
and hence

46
2 Vector and Tensor Analysis in Euclidean Space
Œìijk = Œìk
ij = 0,
i, j, k = 1, 2, . . . , n.
(2.78)
Example. ChristoÔ¨Äel symbols for cylindrical coordinates in E3
(2.16). By virtue of relation (2.28)1 we realize that g11,3 = 2r, while all other
derivatives gik,j (i, j, k = 1, 2, 3) (2.73) are zero. Thus, eq. (2.74) delivers
Œì131 = Œì311 = r,
Œì113 = ‚àír,
(2.79)
while all other ChristoÔ¨Äel symbols of the Ô¨Årst kind Œìijk (i, j, k = 1, 2, 3) are
likewise zero. With the aid of (2.67) and (2.28)2 we further obtain
Œì1
ij = g11Œìij1 = r‚àí2Œìij1,
Œì2
ij = g22Œìij2 = Œìij2,
Œì3
ij = g33Œìij3 = Œìij3,
i, j = 1, 2, 3.
(2.80)
By virtue of (2.79) we can further write
Œì1
13 = Œì1
31 = 1
r ,
Œì3
11 = ‚àír,
(2.81)
while all remaining ChristoÔ¨Äel symbols of the second kind Œìk
ij (i, j, k = 1, 2, 3)
(2.75) vanish.
Now, we are in a position to express the covariant derivative in terms of
the vector or tensor components by means of the ChristoÔ¨Äel symbols. For the
vector-valued function x = x

Œ∏1, Œ∏2, . . . , Œ∏n
we can write using (2.66)
x,j =

xigi

,j = xi,j gi + xigi,j
= xi,j gi + xiŒìk
ijgk =

xi,j +xkŒìi
kj

gi,
(2.82)
or alternatively using (2.72)
x,j =

xigi
,j = xi,j gi + xigi,j
= xi,j gi ‚àíxiŒìi
kjgk =

xi,j ‚àíxkŒìk
ij

gi.
(2.83)
Comparing these results with (2.62) yields
xi|j= xi,j +xkŒìi
kj,
xi|j= xi,j ‚àíxkŒìk
ij,
i, j = 1, 2, . . ., n.
(2.84)
Similarly, we treat the tensor-valued function A = A

Œ∏1, Œ∏2, . . . , Œ∏n
:
A,k =

Aijgi ‚äógj

,k
= Aij,k gi ‚äógj + Aijgi,k ‚äógj + Aijgi ‚äógj,k
= Aij,k gi ‚äógj + Aij 
Œìl
ikgl

‚äógj + Aijgi ‚äó

Œìl
jkgl

=

Aij,k +AljŒìi
lk + AilŒìj
lk

gi ‚äógj.
(2.85)

2.6 Applications in Three-Dimensional Space: Divergence and Curl
47
Thus,
Aij|k= Aij,k +AljŒìi
lk + AilŒìj
lk,
i, j, k = 1, 2, . . ., n.
(2.86)
By analogy, we further obtain
Aij|k= Aij,k ‚àíAljŒìl
ik ‚àíAilŒìl
jk,
Ai
¬∑j|k= Ai
¬∑j,k +Al
¬∑jŒìi
lk ‚àíAi
¬∑lŒìl
jk,
i, j, k = 1, 2, . . ., n.
(2.87)
Similar expressions for the covariant derivative can also be formulated for
tensors of higher orders.
From (2.78), (2.84), (2.86) and (2.87) it is seen that the covariant derivative
taken in Cartesian coordinates (2.76) coincides with the partial derivative:
xi|j= xi,j ,
xi|j= xi,j ,
Aij|k= Aij,k , Aij|k= Aij,k , Ai
¬∑j|k= Ai
¬∑j,k ,
i, j, k = 1, 2, . . ., n.
(2.88)
Formal application of the covariant derivative (2.84) and (2.86-2.87) to the
tangent vectors (2.26) and metric coeÔ¨Écients (1.85)1,2 yields by virtue of
(2.66), (2.67), (2.72) and (2.74) the following identities referred to as Ricci‚Äôs
Theorem:
gi|j= gi,j ‚àíglŒìl
ij = 0,
gi|j= gi,j +glŒìi
lj = 0,
(2.89)
gij|k= gij,k ‚àígljŒìl
ik ‚àígilŒìl
jk = gij,k ‚àíŒìikj ‚àíŒìjki = 0,
(2.90)
gij|k= gij,k +gljŒìi
lk + gilŒìj
lk = gilgjm (‚àíglm,k +Œìmkl + Œìlkm) = 0, (2.91)
where i, j, k = 1, 2, . . ., n. The latter two identities can alternatively be proved
by taking (1.25) into account and using the product rules of diÔ¨Äerentiation
for the covariant derivative which can be written as (Exercise 2.6)
Aij|k= ai|k bj + aibj|k
for
Aij = aibj,
(2.92)
Aij|k= ai|k bj + aibj|k
for
Aij = aibj,
(2.93)
Ai
j|k = ai|k bj + aibj|k
for
Ai
j = aibj,
i, j, k = 1, 2, . . ., n.
(2.94)
2.6 Applications in Three-Dimensional Space:
Divergence and Curl
Divergence of a tensor Ô¨Åeld. One deÔ¨Ånes the divergence of a tensor Ô¨Åeld
S (r) by
divS = lim
V ‚Üí0
1
V
'
A
SndA,
(2.95)

48
2 Vector and Tensor Analysis in Euclidean Space
P
A(1)
ŒîŒ∏3
s1(Œ∏1)
s1(Œ∏1 + ŒîŒ∏1)
A(2)
dA(1)(Œ∏1)
A(3)
Œ∏3
g 3
g 2
g 1
ŒîŒ∏1
ŒîŒ∏2
dA(1)(Œ∏1 + ŒîŒ∏1)
Œ∏1
Œ∏2
Fig. 2.2. Derivation of the divergence in three-dimensional space
where the integration is carried out over a closed surface area A with the
volume V and the outer unit normal vector n.
For the integration we consider a curvilinear parallelepiped with the edges
formed by the coordinate lines Œ∏1, Œ∏2, Œ∏3 and Œ∏1 +ŒîŒ∏1, Œ∏2 +ŒîŒ∏2, Œ∏3 +ŒîŒ∏3 (Fig.
2.2). The inÔ¨Ånitesimal surface elements of the parallelepiped can be deÔ¨Åned
in a vector form by
dA(i) = ¬±

dŒ∏jgj

√ó

dŒ∏kgk

= ¬±ggidŒ∏jŒ∏k,
i = 1, 2, 3,
(2.96)
where g = [g1g2g3] (1.31) and i, j, k is an even permutation of 1,2,3. The cor-
responding inÔ¨Ånitesimal volume element can thus be given by (no summation
over i)
dV = dA(i) ¬∑

dŒ∏igi

=

dŒ∏1g1 dŒ∏2g2 dŒ∏3g3

= [g1g2g3] dŒ∏1dŒ∏2dŒ∏3 = gdŒ∏1dŒ∏2dŒ∏3.
(2.97)
We also need the following identities
g,k = [g1g2g3] ,k = Œìl
1k [glg2g3] + Œìl
2k [g1glg3] + Œìl
3k [g1g2gl]
= Œìl
lk [g1g2g3] = Œìl
lkg,
(2.98)

ggi
,i = g,i gi + ggi,i = Œìl
liggi ‚àíŒìi
liggl = 0.
(2.99)
With these results in hand, one can express the divergence (2.95) as follows

2.6 Applications in Three-Dimensional Space: Divergence and Curl
49
divS = lim
V ‚Üí0
1
V
'
A
SndA
= lim
V ‚Üí0
1
V
3
	
i=1
'
A(i)

S

Œ∏i + ŒîŒ∏i
dA(i) 
Œ∏i + ŒîŒ∏i
+ S

Œ∏i
dA(i) 
Œ∏i
.
Using the abbreviation
si 
Œ∏i
= S

Œ∏i
g

Œ∏i
gi 
Œ∏i
,
i = 1, 2, 3
(2.100)
we can thus write
divS = lim
V ‚Üí0
1
V
3
	
i=1
Œ∏k+ŒîŒ∏k
'
Œ∏k
Œ∏j+ŒîŒ∏j
'
Œ∏j

si 
Œ∏i + ŒîŒ∏i
‚àísi 
Œ∏i
dŒ∏jdŒ∏k
= lim
V ‚Üí0
1
V
3
	
i=1
Œ∏k+ŒîŒ∏k
'
Œ∏k
Œ∏j+ŒîŒ∏j
'
Œ∏j
Œ∏i+ŒîŒ∏i
'
Œ∏i
‚àÇsi
‚àÇŒ∏i dŒ∏idŒ∏jdŒ∏k
= lim
V ‚Üí0
1
V
3
	
i=1
'
V
si,i
g dV,
(2.101)
where i, j, k is again an even permutation of 1,2,3. Assuming continuity of the
integrand in (2.101) and applying (2.99) and (2.100) we obtain
divS = 1
g si,i = 1
g

Sggi
,i = 1
g

S,i ggi + S

ggi
,i

= S,i gi,
(2.102)
which Ô¨Ånally yields by virtue of (2.62)2
divS = S,i gi = S i
j¬∑|i gj = Sji|i gj.
(2.103)
Example. The momentum balance in Cartesian and cylindrical
coordinates. Let us consider a material body or a part of it with a mass
m, volume V and outer surface A. According to the Euler law of motion the
vector sum of external volume forces fdV and surface tractions tdA results in
the vector sum of inertia forces ¬®xdm, where x stands for the position vector
of a material element dm and the superposed dot denotes the material time
derivative. Hence,
'
m
¬®xdm =
'
A
tdA +
'
V
fdV.
(2.104)
Applying the Cauchy theorem (1.72) to the Ô¨Årst integral on the right hand
side it further delivers

50
2 Vector and Tensor Analysis in Euclidean Space
'
m
¬®xdm =
'
A
œÉndA +
'
V
fdV.
(2.105)
Dividing this equation by V and considering the limit case V ‚Üí0 we obtain
by virtue of (2.95)
œÅ¬®x = divœÉ + f,
(2.106)
where œÅ denotes the density of the material. This vector equation is referred
to as the momentum balance.
Representing vector and tensor variables with respect to the tangent vec-
tors gi (i = 1, 2, 3) of an arbitrary curvilinear coordinate system as
¬®x = ¬®xigi,
œÉ = œÉijgi ‚äógj,
f = f igi
and expressing the divergence of the Cauchy stress tensor by (2.103) we obtain
the component form of the momentum balance (2.106) by
œÅ¬®xi = œÉij|j +f i,
i = 1, 2, 3.
(2.107)
With the aid of (2.86) the covariant derivative of the Cauchy stress tensor can
further be written by
œÉij|k= œÉij,k +œÉljŒìi
lk + œÉilŒìj
lk,
i, j, k = 1, 2, 3
(2.108)
and thus,
œÉij|j= œÉij,j +œÉljŒìi
lj + œÉilŒìj
lj,
i = 1, 2, 3.
(2.109)
By virtue of the expressions for the ChristoÔ¨Äel symbols (2.81) and keeping in
mind the symmetry of the Cauchy stress tensors œÉij = œÉji (i Ã∏= j = 1, 2, 3) we
thus obtain for cylindrical coordinates:
œÉ1j|j = œÉ11,œï +œÉ12,z +œÉ13,r +3œÉ31
r
,
œÉ2j|j = œÉ21,œï +œÉ22,z +œÉ23,r +œÉ32
r ,
œÉ3j|j = œÉ31,œï +œÉ32,z +œÉ33,r ‚àírœÉ11 + œÉ33
r .
(2.110)
The balance equations Ô¨Ånally take the form
œÅ¬®x1 = œÉ11,œï +œÉ12,z +œÉ13,r +3œÉ31
r
+ f 1,
œÅ¬®x2 = œÉ21,œï +œÉ22,z +œÉ23,r +œÉ32
r
+ f 2,
œÅ¬®x3 = œÉ31,œï +œÉ32,z +œÉ33,r ‚àírœÉ11 + œÉ33
r
+ f 3.
(2.111)

2.6 Applications in Three-Dimensional Space: Divergence and Curl
51
In Cartesian coordinates, where gi = ei (i = 1, 2, 3), the covariant derivative
coincides with the partial one, so that
œÉij|j= œÉij,j = œÉij,j .
(2.112)
Thus, the balance equations reduce to
œÅ¬®x1 = œÉ11,1 +œÉ12,2 +œÉ13,3 +f1,
œÅ¬®x2 = œÉ21,1 +œÉ22,2 +œÉ23,3 +f2,
œÅ¬®x3 = œÉ31,1 +œÉ32,2 +œÉ33,3 +f3.
(2.113)
Divergence and curl of a vector Ô¨Åeld. Now, we consider a diÔ¨Äer-
entiable vector Ô¨Åeld t

Œ∏1, Œ∏2, Œ∏3
. One deÔ¨Ånes the divergence and curl of
t

Œ∏1, Œ∏2, Œ∏3
respectively by
divt = lim
V ‚Üí0
1
V
'
A
(t ¬∑ n) dA,
(2.114)
curlt = lim
V ‚Üí0
1
V
'
A
(n √ó t) dA = ‚àílim
V ‚Üí0
1
V
'
A
(t √ó n) dA,
(2.115)
where the integration is again carried out over a closed surface area A with
the volume V and the outer unit normal vector n. Considering (1.65) and
(2.95), the curl can also be represented by
curlt = ‚àílim
V ‚Üí0
1
V
'
A
ÀÜtndA = ‚àídivÀÜt.
(2.116)
Treating the vector Ô¨Åeld in the same manner as the tensor Ô¨Åeld we can write
divt = t,i ¬∑gi = ti|i
(2.117)
and in view of (2.65)2
divt = tr (gradt) .
(2.118)
The same procedure applied to the curl (2.115) leads to
curlt = gi √ó t,i .
(2.119)
By virtue of (2.62) and (1.42) we further obtain (see also Exercise 2.7)
curlt = ti|j gj √ó gi = ejik 1
gti|j gk.
(2.120)
With respect to Cartesian coordinates with gi = ei (i = 1, 2, 3) the divergence
(2.117) and curl (2.120) simplify to

52
2 Vector and Tensor Analysis in Euclidean Space
divt = ti,i = t1,1 +t2,2 +t3,3 = t1,1 +t2,2 +t3,3 ,
(2.121)
curlt = ejikti,j ek
= (t3,2 ‚àít2,3 ) e1 + (t1,3 ‚àít3,1 ) e2 + (t2,1 ‚àít1,2 ) e3.
(2.122)
Now, we are going to discuss some combined operations with a gradient, diver-
gence, curl, tensor mapping and products of various types (see also Exercise
2.11).
1) Curl of a gradient:
curl gradŒ¶ = 0.
(2.123)
2) Divergence of a curl:
div curlt = 0.
(2.124)
3) Divergence of a vector product:
div (u √ó v) = v ¬∑ curlu ‚àíu ¬∑ curlv.
(2.125)
4) Gradient of a divergence:
graddivt = curl curlt + div gradt,
(2.126)
graddivt = div (gradt)T .
(2.127)
The combined operator div gradt appearing in (2.126) is known as the
Laplacian and is also denoted by Œît.
5) Divergence of a (left) mapping
div (tA) = A : gradt + t ¬∑ divA.
(2.128)
6) Divergence of a product of a scalar-valued function and a vector-valued
function
div (Œ¶t) = t ¬∑ gradŒ¶ + Œ¶divt.
(2.129)
7) Divergence of a product of a scalar-valued function and a tensor-valued
function
div (Œ¶A) = AgradŒ¶ + Œ¶divA.
(2.130)
We prove, for example, identity (2.123). To this end, we apply (2.65)1, (2.72)
and (2.119). Thus, we write
curl gradŒ¶ = gj √ó

Œ¶|i gi
,j = Œ¶,ij gj √ó gi + Œ¶,i gj √ó gi,j
= Œ¶,ij gj √ó gi ‚àíŒ¶,i Œìi
kjgj √ó gk = 0
(2.131)

2.6 Applications in Three-Dimensional Space: Divergence and Curl
53
taking into account that Œ¶,ij = Œ¶,ji, Œìl
ij = Œìl
ji and gi √ó gj = ‚àígj √ó gi
(i Ã∏= j, i, j = 1, 2, 3).
Example. Balance of mechanical energy as an integral form of
the momentum balance. Using the above identities we are now able to
formulate the balance of mechanical energy on the basis of the momentum
balance (2.106). To this end, we multiply this vector relation scalarly by the
velocity vector v = Àôx
v ¬∑ (œÅ¬®x) = v ¬∑ divœÉ + v ¬∑ f.
Using (2.128) we can further write
v ¬∑ (œÅ¬®x) + œÉ : gradv = div (vœÉ) + v ¬∑ f.
Integrating this relation over the volume of the body V yields
d
dt
'
m
%1
2v ¬∑ v
&
dm +
'
V
œÉ : gradvdV =
'
V
div (vœÉ) dV +
'
V
v ¬∑ fdV,
where dm = œÅdV and m denotes the mass of the body. Keeping in mind the
deÔ¨Ånition of the divergence (2.95) and applying the Cauchy theorem (1.72)
according to which the Cauchy stress vector is given by t = œÉn, we Ô¨Ånally
obtain the relation
d
dt
'
m
%1
2v ¬∑ v
&
dm +
'
V
œÉ : gradvdV =
'
A
v ¬∑ tdA +
'
V
v ¬∑ fdV
(2.132)
expressing the balance of mechanical energy. Indeed, the Ô¨Årst and the second
integrals on the left hand side of (2.132) represent the time rate of the kinetic
energy and the stress power, respectively. The right hand side of (2.132) ex-
presses the power of external forces i.e. external tractions t on the boundary
of the body A and external volume forces f inside of it.
Example. Navier-Stokes equations for a linear-viscous Ô¨Çuid in
Cartesian and cylindrical coordinates. A linear-viscous Ô¨Çuid (also called
Newton Ô¨Çuid or Navier-Poisson Ô¨Çuid) is deÔ¨Åned by a constitutive equation
œÉ = ‚àípI + 2Œ∑D + Œª (trD) I,
(2.133)
where
D = 1
2

gradv + (gradv)T
(2.134)
denotes the spatial strain rate tensor, p is the hydrostatic pressure while Œ∑
and Œª represent material constants referred to as shear viscosity and second
viscosity coeÔ¨Écient, respectively. Inserting (2.134) into (2.133) and taking
(2.118) into account delivers

54
2 Vector and Tensor Analysis in Euclidean Space
œÉ = ‚àípI + Œ∑

gradv + (gradv)T
+ Œª (divv) I.
(2.135)
Substituting this expression into the momentum balance (2.106) and using
(2.127) and (2.130) we obtain the relation
œÅ Àôv = ‚àígradp + Œ∑div gradv + (Œ∑ + Œª) graddivv + f
(2.136)
referred to as the Navier-Stokes equation. By means of (2.126) it can be rewrit-
ten as
œÅ Àôv = ‚àígradp + (2Œ∑ + Œª) graddivv ‚àíŒ∑curl curlv + f.
(2.137)
For an incompressible Ô¨Çuid characterized by the kinematic condition trD =
divv = 0, the latter two equations simplify to
œÅ Àôv = ‚àígradp + Œ∑Œîv + f,
(2.138)
œÅ Àôv = ‚àígradp ‚àíŒ∑curl curlv + f.
(2.139)
With the aid of the identity Œîv = v,i|i (see Exercise 2.13) we thus can write
œÅ Àôv = ‚àígradp + Œ∑v,i|i +f.
(2.140)
In Cartesian coordinates this relation is thus written out as
œÅÀôvi = ‚àíp,i +Œ∑ (vi,11 +vi,22 +vi,33 ) + fi,
i = 1, 2, 3.
(2.141)
For arbitrary curvilinear coordinates we use the following representation for
the vector Laplacian (see Exercise 2.15)
Œîv = gij 
vk,ij +2Œìk
livl,j ‚àíŒìm
ij vk,m +Œìk
li,j vl + Œìk
mjŒìm
li vl ‚àíŒìm
ij Œìk
lmvl
gk.
(2.142)
For the cylindrical coordinates it takes by virtue of (2.28) and (2.81) the
following form
Œîv =

r‚àí2v1,11 +v1,22 +v1,33 +3r‚àí1v1,3 +2r‚àí3v3,1

g1
+

r‚àí2v2,11 +v2,22 +v2,33 +r‚àí1v2,3

g2
+

r‚àí2v3,11 +v3,22 +v3,33 ‚àí2r‚àí1v1,1 +r‚àí1v3,3 ‚àír‚àí2v3
g3.
Inserting this result into (2.138) and using the representations Àôv = Àôvigi and
f = f igi we Ô¨Ånally obtain
œÅÀôv1 = f 1 ‚àí‚àÇp
‚àÇœï + Œ∑
% 1
r2
‚àÇ2v1
‚àÇœï2 + ‚àÇ2v1
‚àÇz2 + ‚àÇ2v1
‚àÇr2 + 3
r
‚àÇv1
‚àÇr + 2
r3
‚àÇv3
‚àÇœï
&
,
œÅÀôv2 = f 2 ‚àí‚àÇp
‚àÇz + Œ∑
% 1
r2
‚àÇ2v2
‚àÇœï2 + ‚àÇ2v2
‚àÇz2 + ‚àÇ2v2
‚àÇr2 + 1
r
‚àÇv2
‚àÇr
&
,
œÅÀôv3 = f 3 ‚àí‚àÇp
‚àÇr + Œ∑
% 1
r2
‚àÇ2v3
‚àÇœï2 + ‚àÇ2v3
‚àÇz2 + ‚àÇ2v3
‚àÇr2 ‚àí2
r
‚àÇv1
‚àÇœï + 1
r
‚àÇv3
‚àÇr ‚àív3
r2
&
.
(2.143)

2.6 Applications in Three-Dimensional Space: Divergence and Curl
55
Exercises
2.1. Evaluate tangent vectors and metric coeÔ¨Écients of spherical coordinates
in E3 deÔ¨Åned by
r (œï, œÜ, r) = r sin œï sin œÜe1 + r cos œÜe2 + r cos œï sin œÜe3.
(2.144)
2.2. Evaluate the coeÔ¨Écients ‚àÇ¬ØŒ∏i
‚àÇŒ∏k (2.41) for the transformation of linear co-
ordinates in the spherical ones and vice versa.
2.3. Evaluate gradients of the following functions of r:
(a)
1
‚à•r‚à•,
(b) r ¬∑ w,
(c) rAr,
(d) Ar,
(e) w √ó r,
where w and A are some vector and tensor, respectively.
2.4. Evaluate the ChristoÔ¨Äel symbols of the Ô¨Årst and second kind for spherical
coordinates (2.144).
2.5. Verify relations (2.87).
2.6. Prove the product rules of diÔ¨Äerentiation for the covariant derivative
(2.92-2.94).
2.7. Verify relation (2.120) applying (2.103), (2.116) and using the results of
Exercise 1.21.
2.8. Write out the balance equations (2.107) in spherical coordinates (2.144).
2.9. Evaluate tangent vectors, metric coeÔ¨Écients and ChristoÔ¨Äel symbols for
cylindrical surface coordinates deÔ¨Åned by
r (r, s, z) = r cos s
r e1 + r sin s
re2 + ze3.
(2.145)
2.10. Write out the balance equations (2.107) in cylindrical surface coordi-
nates (2.145).
2.11. Prove identities (2.124-2.130).
2.12. Write out the gradient, divergence and curl of a vector Ô¨Åeld t (r) in
cylindrical and spherical coordinates (2.16) and (2.144), respectively.
2.13. Prove that the Laplacian of a vector-valued function t (r) can be given
by Œît = t,i|i. Specify this identity for Cartesian coordinates.
2.14. Write out the Laplacian ŒîŒ¶ of a scalar Ô¨Åeld Œ¶ (r) in cylindrical and
spherical coordinates (2.16) and (2.144), respectively.
2.15. Write out the Laplacian of a vector Ô¨Åeld t (r) in component form in
an arbitrary curvilinear coordinate system. Specify the result for spherical
coordinates (2.144).

3
Curves and Surfaces in Three-Dimensional
Euclidean Space
3.1 Curves in Three-Dimensional Euclidean Space
A curve in three-dimensional space is deÔ¨Åned by a vector function
r = r (t) ,
r ‚ààE3,
(3.1)
where the real variable t belongs to some interval: t1 ‚â§t ‚â§t2. Henceforth, we
assume that the function r (t) is suÔ¨Éciently diÔ¨Äerentiable and
dr
dt Ã∏= 0
(3.2)
over the whole deÔ¨Ånition domain. Specifying an arbitrary coordinate system
(2.15) as
Œ∏i = Œ∏i (r) ,
i = 1, 2, 3,
(3.3)
the curve (3.1) can alternatively be deÔ¨Åned by
Œ∏i = Œ∏i (t) ,
i = 1, 2, 3.
(3.4)
Example. Straight line. A straight line can be deÔ¨Åned by
r (t) = a + bt,
a, b ‚ààE3.
(3.5)
With respect to linear coordinates related to a basis H = {h1, h2, h3} it is
equivalent to
ri (t) = ai + bit,
i = 1, 2, 3,
(3.6)
where r = rihi, a = aihi and b = bihi.
Example. Circular helix. The circular helix (Fig. 3.1) is deÔ¨Åned by

58
3 Curves and Surfaces in Three-Dimensional Euclidean Space
x3
2œÄc
e1
e2
x2
x1
r
t
e3
R
Fig. 3.1. Circular helix
r (t) = R cos (t) e1 + R sin (t) e2 + cte3,
c Ã∏= 0,
(3.7)
where ei (i = 1, 2, 3) form an orthonormal basis in E3. For the deÔ¨Ånition of
the circular helix the cylindrical coordinates (2.16) appear to be very suitable.
Indeed, alternatively to (3.7) we can write
r = R,
œï = t,
z = ct.
(3.8)
In the previous chapter we deÔ¨Åned tangent vectors to the coordinate lines. By
analogy one can also deÔ¨Åne a vector tangent to the curve (3.1) as
gt = dr
dt .
(3.9)
It is advantageous to parametrize the curve (3.1) in terms of the so-called arc
length. To this end, we Ô¨Årst calculate the length of a curve segment between
the points corresponding to parameters t1 and t as
s (t) =
r(t)
'
r(t1)
‚àö
dr ¬∑ dr.
(3.10)

3.1 Curves in Three-Dimensional Euclidean Space
59
With the aid of (3.9) we can write dr = gtdt and consequently
s (t) =
t
'
t1
‚àögt ¬∑ gtdt =
t
'
t1
‚à•gt‚à•dt =
t
'
t1

gtt (t)dt.
(3.11)
Using this equation and keeping in mind assumption (3.2) we have
ds
dt =

gtt (t) Ã∏= 0.
(3.12)
This implies that the function s = s (t) is invertible and
t (s) =
s
'
s(t1)
‚à•gt‚à•‚àí1 ds =
s
'
s(t1)
ds

gtt (t)
.
(3.13)
Thus, the curve (3.1) can be redeÔ¨Åned in terms of the arc length s as
r = r (t (s)) =
‚å¢r (s) .
(3.14)
In analogy with (3.9) one deÔ¨Ånes the vector tangent to the curve
‚å¢r (s) (3.14)
as
a1 = d
‚å¢r
ds = dr
dt
dt
ds =
gt
‚à•gt‚à•
(3.15)
being a unit vector: ‚à•a1‚à•= 1. DiÔ¨Äerentiation of this vector with respect to s
further yields
a1,s = da1
ds = d2 ‚å¢r
ds2 .
(3.16)
It can be shown that the tangent vector a1 is orthogonal to a1,s provided
the latter one is not zero. Indeed, diÔ¨Äerentiating the identity a1 ¬∑ a1 = 1 with
respect to s we have
a1 ¬∑ a1,s = 0.
(3.17)
The length of the vector a1,s
Œ∫ (s) = ‚à•a1,s ‚à•
(3.18)
plays an important role in the theory of curves and is called curvature. The
inverse value
œÅ (s) =
1
Œ∫ (s)
(3.19)

60
3 Curves and Surfaces in Three-Dimensional Euclidean Space
is referred to as the radius of curvature of the curve at the point
‚å¢r (s). Hence-
forth, we focus on curves with non-zero curvature. The case of zero curvature
corresponds to a straight line (see Exercise 3.1) and is trivial.
Next, we deÔ¨Åne the unit vector in the direction of a1,s
a2 =
a1,s
‚à•a1,s ‚à•= a1,s
Œ∫ (s)
(3.20)
called the principal normal vector to the curve. The orthogonal vectors a1
and a2 can further be completed to an orthonormal basis in E3 by the vector
a3 = a1 √ó a2
(3.21)
called the unit binormal vector. The triplet of vectors a1, a2 and a3 is referred
to as the moving trihedron of the curve.
In order to study the rotation of the trihedron along the curve we again
consider the arc length s as a coordinate. In this case, we can write similarly
to (2.66)
ai,s = Œìk
isak,
i = 1, 2, 3,
(3.22)
where Œìk
is = ai,s ¬∑ak (i, k = 1, 2, 3). From (3.17), (3.20) and (3.21) we imme-
diately observe that Œì2
1s = Œ∫ and Œì1
1s = Œì3
1s = 0. Further, diÔ¨Äerentiating the
identities
a3 ¬∑ a3 = 1,
a1 ¬∑ a3 = 0
(3.23)
with respect to s yields
a3 ¬∑ a3,s = 0,
a1,s ¬∑a3 + a1 ¬∑ a3,s = 0.
(3.24)
Taking into account (3.20) this results in the following identity
a1 ¬∑ a3,s = ‚àía1,s ¬∑a3 = ‚àíŒ∫a2 ¬∑ a3 = 0.
(3.25)
Relations (3.24) and (3.25) suggest that
a3,s = ‚àíœÑ (s) a2,
(3.26)
where the function
œÑ (s) = ‚àía3,s ¬∑a2
(3.27)
is called torsion of the curve at the point
‚å¢r(s). Thus, Œì2
3s = ‚àíœÑ and
Œì1
3s = Œì3
3s = 0. The sign of the torsion (3.27) has a geometric meaning and
remains unaÔ¨Äected by the change of the positive sense of the curve, i.e. by
the transformation s = ‚àís‚Ä≤ (see Exercise 3.2). Accordingly, one distinguishes
right-handed curves with a positive torsion and left-handed curves with a neg-
ative torsion. In the case of zero torsion the curve is referred to as a plane
curve.

3.1 Curves in Three-Dimensional Euclidean Space
61
Finally, diÔ¨Äerentiating the identities
a2 ¬∑ a1 = 0,
a2 ¬∑ a2 = 1,
a2 ¬∑ a3 = 0
with respect to s and using (3.20) and (3.27) we get
a2,s ¬∑a1 = ‚àía2 ¬∑ a1,s = ‚àíŒ∫a2 ¬∑ a2 = ‚àíŒ∫,
(3.28)
a2 ¬∑ a2,s = 0,
a2,s ¬∑a3 = ‚àía2 ¬∑ a3,s = œÑ,
(3.29)
so that Œì1
2s = ‚àíŒ∫, Œì2
2s = 0 and Œì3
2s = œÑ. Summarizing the above results we
can write

Œìj
is

=
‚é°
‚é£
0
Œ∫ 0
‚àíŒ∫
0 œÑ
0 ‚àíœÑ 0
‚é§
‚é¶
(3.30)
and
a1,s =
a2,s =
a3,s =
Œ∫a2,
‚àíŒ∫a1
+œÑa3,
‚àíœÑa2.
(3.31)
Relations (3.31) are known as the Frenet formulas.
‚å¢r(s)
a1(s)
a1(s0)
a 2(s)
a 3(s)
Q
a2(s0)
‚å¢r(s0)
a 3(s0)
Fig. 3.2. Rotation of the moving trihedron
A useful illustration of the Frenet formulas can be gained with the aid of a
skew-symmetric tensor. To this end, we consider the rotation of the trihedron
from some initial position at s0 to the actual state at s. This rotation can be
described by an orthogonal tensor Q (s) as (Fig. 3.2)

62
3 Curves and Surfaces in Three-Dimensional Euclidean Space
ai (s) = Q (s) ai (s0) ,
i = 1, 2, 3.
(3.32)
DiÔ¨Äerentiating this relation with respect to s yields
ai,s (s) = Q,s (s) ai (s0) ,
i = 1, 2, 3.
(3.33)
Mapping both sides of (3.32) by QT (s) and inserting the result into (3.33)
we further obtain
ai,s (s) = Q,s (s) QT (s) ai (s) ,
i = 1, 2, 3.
(3.34)
DiÔ¨Äerentiating the identity (1.129) Q (s) QT (s) = I with respect to s we have
Q,s (s) QT (s) + Q (s) QT,s (s) = 0, which implies that the tensor W (s) =
Q,s (s) QT (s) is skew-symmetric. Hence, eq. (3.34) can be rewritten as (see
also [3])
ai,s (s) = W (s) ai (s) ,
W ‚ààSkew3,
i = 1, 2, 3,
(3.35)
where according to (3.31)
W (s) = œÑ (s) (a3 ‚äóa2 ‚àía2 ‚äóa3) + Œ∫ (s) (a2 ‚äóa1 ‚àía1 ‚äóa2) .
(3.36)
By virtue of (1.130) and (1.131) we further obtain
W = œÑ ÀÜa1 + Œ∫ÀÜa3
(3.37)
and consequently
ai,s = d √ó ai = ÀÜdai,
i = 1, 2, 3,
(3.38)
where
d = œÑa1 + Œ∫a3
(3.39)
is referred to as the Darboux vector.
Example. Curvature, torsion, moving trihedron and Darboux
vector for a circular helix. Inserting (3.7) into (3.9) delivers
gt = dr
dt = ‚àíR sin (t) e1 + R cos (t) e2 + ce3,
(3.40)
so that
gtt = gt ¬∑ gt = R2 + c2 = const.
(3.41)
Thus, using (3.13) we may set
t (s) =
s
‚àö
R2 + c2 .
(3.42)

3.1 Curves in Three-Dimensional Euclidean Space
63
Using this result, the circular helix (3.7) can be parametrized in terms of the
arc length s by
‚å¢r(s) = R cos
%
s
‚àö
R2 + c2
&
e1 + R sin
%
s
‚àö
R2 + c2
&
e2 +
cs
‚àö
R2 + c2 e3.
(3.43)
With the aid of (3.15) we further write
a1 = d
‚å¢r
ds =
1
‚àö
R2 + c2
"
‚àíR sin
%
s
‚àö
R2 + c2
&
e1
+ R cos
%
s
‚àö
R2 + c2
&
e2 + ce3
#
,
(3.44)
a1,s = ‚àí
R
R2 + c2
"
cos
%
s
‚àö
R2 + c2
&
e1 + sin
%
s
‚àö
R2 + c2
&
e2
#
.
(3.45)
According to (3.18) the curvature of the helix is thus
Œ∫ =
R
R2 + c2 .
(3.46)
By virtue of (3.20), (3.21) and (3.27) we have
a2 = a1,s
Œ∫
= ‚àícos
%
s
‚àö
R2 + c2
&
e1 ‚àísin
%
s
‚àö
R2 + c2
&
e2,
(3.47)
a3 = a1 √ó a2 =
1
‚àö
R2 + c2
"
c sin
%
s
‚àö
R2 + c2
&
e1
‚àíc cos
%
s
‚àö
R2 + c2
&
e2 + Re3
#
.
(3.48)
a3,s =
c
R2 + c2
"
cos
%
s
‚àö
R2 + c2
&
e1 + sin
%
s
‚àö
R2 + c2
&
e2
#
,
(3.49)
œÑ =
c
R2 + c2 .
(3.50)
It is seen that the circular helix is right-handed for c > 0, left-handed for
c < 0 and becomes a circle for c = 0. For the Darboux vector (3.39) we Ô¨Ånally
obtain
d = œÑa1 + Œ∫a3 =
1
‚àö
R2 + c2 e3.
(3.51)

64
3 Curves and Surfaces in Three-Dimensional Euclidean Space
3.2 Surfaces in Three-Dimensional Euclidean Space
A surface in three-dimensional Euclidean space is deÔ¨Åned by a vector function
r = r

t1, t2
,
r ‚ààE3,
(3.52)
of two real variables t1 and t2 referred to as Gauss coordinates. With the aid
of the coordinate system (3.3) one can alternatively write
Œ∏i = Œ∏i 
t1, t2
,
i = 1, 2, 3.
(3.53)
In the following, we assume that the function r

t1, t2
is suÔ¨Éciently diÔ¨Äeren-
tiable with respect to both arguments and
dr
dtŒ± Ã∏= 0,
Œ± = 1, 2
(3.54)
over the whole deÔ¨Ånition domain.
Example 1. Plane. Let us consider three linearly independent vectors
xi (i = 0, 1, 2) specifying three points in three-dimensional space. The plane
going through these points can be deÔ¨Åned by
r

t1, t2
= x0 + t1 (x1 ‚àíx0) + t2 (x2 ‚àíx0) .
(3.55)
Example 2. Cylinder. A cylinder of radius R with the axis parallel to
e3 is deÔ¨Åned by
r

t1, t2
= R cos t1e1 + R sin t1e2 + t2e3,
(3.56)
where ei (i = 1, 2, 3) again form an orthonormal basis in E3. With the aid of
the cylindrical coordinates (2.16) we can alternatively write
œï = t1,
z = t2,
r = R.
(3.57)
Example 3. Sphere. A sphere of radius R with the center at r = 0 is
deÔ¨Åned by
r

t1, t2
= R sin t1 sin t2e1 + R cos t2e2 + R cos t1 sin t2e3,
(3.58)
or by means of spherical coordinates (2.144) as
œï = t1,
œÜ = t2,
r = R.
(3.59)
Using a parametric representation (see, e.g., [25])
t1 = t1 (t) ,
t2 = t2 (t)
(3.60)

3.2 Surfaces in Three-Dimensional Euclidean Space
65
r
g 1
g 3
g 2
g t
t2-line
t1-line
normal plane
normal
section
Fig. 3.3. Coordinate lines on the surface, normal section and tangent vectors
one deÔ¨Ånes a curve on the surface (3.52). In particular, the curves t1 = const
and t2 = const are called t2 and t1 coordinate lines, respectively (Fig. 3.3).
The vector tangent to the curve (3.60) can be expressed by
gt = dr
dt = ‚àÇr
‚àÇt1
dt1
dt + ‚àÇr
‚àÇt2
dt2
dt = g1
dt1
dt + g2
dt2
dt ,
(3.61)
where
gŒ± = ‚àÇr
‚àÇtŒ± = r,Œ± ,
Œ± = 1, 2
(3.62)
represent tangent vectors to the coordinate lines. For the length of an in-
Ô¨Ånitesimal element of the curve (3.60) we thus write
(ds)2 = dr¬∑dr = (gtdt)¬∑(gtdt) =

g1dt1 + g2dt2
¬∑

g1dt1 + g2dt2
. (3.63)
With the aid of the abbreviation
gŒ±Œ≤ = gŒ≤Œ± = gŒ± ¬∑ gŒ≤,
Œ±, Œ≤ = 1, 2,
(3.64)
it delivers the quadratic form
(ds)2 = g11

dt12 + 2g12dt1dt2 + g22

dt22
(3.65)
referred to as the Ô¨Årst fundamental form of the surface. The latter result can
brieÔ¨Çy be written as
(ds)2 = gŒ±Œ≤dtŒ±dtŒ≤,
(3.66)
where and henceforth within this chapter the summation convention is implied
for repeated Greek indices taking the values from 1 to 2. Similar to the metric

66
3 Curves and Surfaces in Three-Dimensional Euclidean Space
coeÔ¨Écients (1.85)1,2 in n-dimensional Euclidean space gŒ±Œ≤ (3.64) describe the
metric on a surface. Generally, the metric described by a diÔ¨Äerential quadratic
form like (3.66) is referred to as Riemannian metric.
The tangent vectors (3.62) can be completed to a basis in E3 by the unit
vector
g3 =
g1 √ó g2
‚à•g1 √ó g2‚à•
(3.67)
called principal normal vector to the surface.
In the following, we focus on a special class of surface curves called normal
sections. These are curves passing through a point of the surface r

t1, t2
and
obtained by intersection of this surface with a plane involving the principal
normal vector. Such a plane is referred to as the normal plane.
In order to study curvature properties of normal sections we Ô¨Årst express
the derivatives of the basis vectors gi (i = 1, 2, 3) with respect to the surface
coordinates. Using the formalism of ChristoÔ¨Äel symbols we can write
gi,Œ± = ‚àÇgi
‚àÇtŒ± = ŒìiŒ±kgk = Œìk
iŒ±gk,
i = 1, 2, 3,
(3.68)
where
ŒìiŒ±k = gi,Œ± ¬∑gk,
Œìk
iŒ± = gi,Œ± ¬∑gk,
i = 1, 2, 3,
Œ± = 1, 2.
(3.69)
Taking into account the identity g3 = g3 resulting from (3.67) we immediately
observe that
ŒìiŒ±3 = Œì3
iŒ±,
i = 1, 2, 3,
Œ± = 1, 2.
(3.70)
DiÔ¨Äerentiating the relations
gŒ± ¬∑ g3 = 0,
g3 ¬∑ g3 = 1
(3.71)
with respect to the Gauss coordinates we further obtain
gŒ±,Œ≤ ¬∑g3 = ‚àígŒ± ¬∑ g3,Œ≤ ,
g3,Œ± ¬∑g3 = 0,
Œ±, Œ≤ = 1, 2
(3.72)
and consequently
Œì3
Œ±Œ≤ = ‚àíŒì3Œ≤Œ±,
Œì3
3Œ± = 0,
Œ±, Œ≤ = 1, 2.
(3.73)
Using in (3.68) the abbreviation
bŒ±Œ≤ = bŒ≤Œ± = Œì3
Œ±Œ≤ = ‚àíŒì3Œ±Œ≤ = gŒ±,Œ≤ ¬∑g3,
Œ±, Œ≤ = 1, 2,
(3.74)
we arrive at the relations
gŒ±,Œ≤ = ŒìœÅ
Œ±Œ≤gœÅ + bŒ±Œ≤g3,
Œ±, Œ≤ = 1, 2
(3.75)

3.2 Surfaces in Three-Dimensional Euclidean Space
67
called the Gauss formulas.
Similarly to a coordinate system one can notionally deÔ¨Åne the covariant
derivative also on the surface. To this end, relations (2.84), (2.86) and (2.87)
are speciÔ¨Åed to the two-dimensional space in a straight forward manner as
f Œ±|Œ≤= f Œ±,Œ≤ +f œÅŒìŒ±
œÅŒ≤,
fŒ±|Œ≤= fŒ±,Œ≤ ‚àífœÅŒìœÅ
Œ±Œ≤,
(3.76)
FŒ±Œ≤|Œ≥= FŒ±Œ≤,Œ≥ +FœÅŒ≤ŒìŒ±
œÅŒ≥ + FŒ±œÅŒìŒ≤
œÅŒ≥,
FŒ±Œ≤|Œ≥= FŒ±Œ≤,Œ≥ ‚àíFœÅŒ≤ŒìœÅ
Œ±Œ≥ ‚àíFŒ±œÅŒìœÅ
Œ≤Œ≥,
FŒ±
¬∑Œ≤|Œ≥= FŒ±
¬∑Œ≤,Œ≥ +FœÅ
¬∑Œ≤ŒìŒ±
œÅŒ≥ ‚àíFŒ±
¬∑œÅŒìœÅ
Œ≤Œ≥,
Œ±, Œ≤, Œ≥ = 1, 2.
(3.77)
Thereby, with the aid of (3.76)2 the Gauss formulas (3.75) can alternatively
be given by (cf. (2.89))
gŒ±|Œ≤= bŒ±Œ≤g3,
Œ±, Œ≤ = 1, 2.
(3.78)
Further, we can write
bŒ≤
Œ± = bŒ±œÅgœÅŒ≤ = ‚àíŒì3Œ±œÅgœÅŒ≤ = ‚àíŒìŒ≤
3Œ±,
Œ±, Œ≤ = 1, 2.
(3.79)
Inserting the latter relation into (3.68) and considering (3.73)2, this yields the
identities
g3,Œ± = g3|Œ±= ‚àíbœÅ
Œ±gœÅ,
Œ± = 1, 2
(3.80)
referred to as the Weingarten formulas.
Now, we are in a position to express the curvature of a normal section. It
is called normal curvature and denoted in the following by Œ∫n. At Ô¨Årst, we
observe that the principal normals of the surface and of the normal section
coincide in the sense that a2 = ¬±g3. Using (3.13), (3.28), (3.61), (3.72)1 and
(3.74) and assuming for the moment that a2 = g3 we get
Œ∫n = ‚àía2,s ¬∑a1 = ‚àíg3,s ¬∑ gt
‚à•gt‚à•= ‚àí
%
g3,t
dt
ds
&
¬∑
gt
‚à•gt‚à•= ‚àíg3,t ¬∑
gt
‚à•gt‚à•2
= ‚àí
%
g3,Œ±
dtŒ±
dt
&
¬∑
%
gŒ≤
dtŒ≤
dt
&
‚à•gt‚à•‚àí2 = bŒ±Œ≤
dtŒ±
dt
dtŒ≤
dt ‚à•gt‚à•‚àí2 .
By virtue of (3.63) and (3.66) this leads to the following result
Œ∫n = bŒ±Œ≤dtŒ±dtŒ≤
gŒ±Œ≤dtŒ±dtŒ≤ ,
(3.81)
where the quadratic form
bŒ±Œ≤dtŒ±dtŒ≤ = ‚àídr ¬∑ dg3
(3.82)
is referred to as the second fundamental form of the surface. In the case
a2 = ‚àíg3 the sign of the expression for Œ∫n (3.81) must be changed. Instead of

68
3 Curves and Surfaces in Three-Dimensional Euclidean Space
that, we assume that the normal curvature can, in contrast to the curvature
of space curves (3.18), be negative. However, the sign of Œ∫n (3.81) has no
geometrical meaning. Indeed, it depends on the orientation of g3 with respect
to a2 which is immaterial. For example, g3 changes the sign in coordinate
transformations like ¬Øt1 = t2, ¬Øt2 = t1.
Of special interest is the dependence of the normal curvature Œ∫n on the
direction of the normal section. For example, for the normal sections passing
through the coordinate lines we have
Œ∫n|t2=const = b11
g11
,
Œ∫n|t1=const = b22
g22
.
(3.83)
In the following, we are going to Ô¨Ånd the directions of the maximal and mini-
mal curvature. Necessary conditions for the extremum of the normal curvature
(3.81) are given by
‚àÇŒ∫n
‚àÇtŒ± = 0,
Œ± = 1, 2.
(3.84)
Rewriting (3.81) as
(bŒ±Œ≤ ‚àíŒ∫ngŒ±Œ≤) dtŒ±dtŒ≤ = 0
(3.85)
and diÔ¨Äerentiating with respect to tŒ± we obtain
(bŒ±Œ≤ ‚àíŒ∫ngŒ±Œ≤) dtŒ≤ = 0,
Œ± = 1, 2.
(3.86)
Multiplying both sides of this equation system by gŒ±œÅ and summing up over
Œ± we have with the aid of (3.79)

bœÅ
Œ≤ ‚àíŒ∫nŒ¥œÅ
Œ≤

dtŒ≤ = 0,
œÅ = 1, 2.
(3.87)
A nontrivial solution of this homogeneous equation system exists if and only
if

b1
1 ‚àíŒ∫n b1
2
b2
1
b2
2 ‚àíŒ∫n
 = 0.
(3.88)
Writing out the above determinant we can also write
Œ∫2
n ‚àíbŒ±
Œ±Œ∫n +
bŒ±
Œ≤
 = 0.
(3.89)
The maximal and minimal curvatures Œ∫1 and Œ∫2 resulting from this quadratic
equation are called the principal curvatures. One can show that directions of
principal curvatures are mutually orthogonal (see Theorem 4.5, Sect. 4). These
directions are called principal directions of normal curvature or curvature
directions (see also [25]).
According to the Vieta theorem the product of principal curvatures can
be expressed by

3.2 Surfaces in Three-Dimensional Euclidean Space
69
K = Œ∫1Œ∫2 =
bŒ±
Œ≤
 = b
g2 ,
(3.90)
where
b = |bŒ±Œ≤| =

b11 b12
b21 b22
 = b11b22 ‚àí(b12)2 ,
(3.91)
g2 = [g1g2g3]2 =

g11 g12 0
g21 g22 0
0
0 1

= g11g22 ‚àí(g12)2 .
(3.92)
For the arithmetic mean of the principal curvatures we further obtain
H = 1
2 (Œ∫1 + Œ∫2) = 1
2bŒ±
Œ±.
(3.93)
The values K (3.90) and H (3.93) do not depend on the direction of the
normal section and are called the Gaussian and mean curvatures, respectively.
In terms of K and H the solutions of the quadratic equation (3.89) can simply
be given by
Œ∫1,2 = H ¬±

H2 ‚àíK.
(3.94)
One recognizes that the sign of the Gaussian curvature K (3.90) is deÔ¨Åned
by the sign of b (3.91). For positive b both Œ∫1 and Œ∫2 are positive or negative so
that Œ∫n has the same sign for all directions of the normal sections at r

t1, t2
.
In other words, the orientation of a2 with respect to g3 remains constant. Such
a point of the surface is called elliptic.
For b < 0, principal curvatures are of diÔ¨Äerent signs so that diÔ¨Äerent normal
sections are characterized by diÔ¨Äerent orientations of a2 with respect to g3.
There are two directions of the normal sections with zero curvature. Such
normal sections are referred to as asymptotic directions. The corresponding
point of the surface is called hyperbolic or saddle point.
In the intermediate case b = 0, Œ∫n does not change sign. There is only one
asymptotic direction which coincides with one of the principal directions (of
Œ∫1 or Œ∫2). The corresponding point of the surface is called parabolic point.
Example. Torus. A torus is a surface obtained by rotating a circle about
a coplanar axis (see Fig. 3.4). Additionally we assume that the rotation axis
lies outside of the circle. Accordingly, the torus can be deÔ¨Åned by
r

t1, t2
=

R0 + R cos t2
cos t1e1
+

R0 + R cos t2
sin t1e2 + R sin t2e3,
(3.95)
where R0 is the radius of the circle and R is the distance between its center
and the rotation axis. By means of (3.62) and (3.67) we obtain
g1 = ‚àí

R0 + R cos t2
sin t1e1 +

R0 + R cos t2
cos t1e2,

70
3 Curves and Surfaces in Three-Dimensional Euclidean Space
x3
x1
e2
e3
e1
R0
x2
t1
R
t2
Fig. 3.4. Torus
g2 = ‚àíR cos t1 sin t2e1 ‚àíR sin t1 sin t2e2 + R cos t2e3,
g3 = cos t1 cos t2e1 + sin t1 cos t2e2 + sin t2e3.
(3.96)
Thus, the coeÔ¨Écients (3.64) of the Ô¨Årst fundamental form (3.65) are given by
g11 =

R0 + R cos t22 ,
g12 = 0,
g22 = R2.
(3.97)
In order to express coeÔ¨Écients (3.74) of the second fundamental form (3.82)
we Ô¨Årst calculate derivatives of the tangent vectors (3.96)1,2
g1,1 = ‚àí

R0 + R cos t2
cos t1e1 ‚àí

R0 + R cos t2
sin t1e2,
g1,2 = g2,1 = R sin t1 sin t2e1 ‚àíR cos t1 sin t2e2,
g2,2 = ‚àíR cos t1 cos t2e1 ‚àíR sin t1 cos t2e2 ‚àíR sin t2e3.
(3.98)
Inserting these expressions as well as (3.96)3 into (3.74) we obtain
b11 = ‚àí

R0 + R cos t2
cos t2,
b12 = b21 = 0,
b22 = ‚àíR.
(3.99)
In view of (3.79) and (3.97) b2
1 = b1
2 = 0. Thus, the solution of the equation
system (3.88) delivers
Œ∫1 = b1
1 = b11
g11
= ‚àí
cos t2
R0 + R cos t2 ,
Œ∫2 = b2
2 = b22
g22
= ‚àíR‚àí1.
(3.100)
Comparing this result with (3.83) we see that the coordinate lines of the torus
(3.95) coincide with the principal directions of the normal curvature. Hence,
by (3.90)

3.3 Application to Shell Theory
71
K = Œ∫1Œ∫2 =
cos t2
R (R0 + R cos t2).
(3.101)
Thus, points of the torus for which ‚àíœÄ/2 < t2 < œÄ/2 are elliptic while points
for which œÄ/2 < t2 < 3œÄ/2 are hyperbolic. Points of the coordinates lines
t2 = ‚àíœÄ/2 and t2 = œÄ/2 are parabolic.
3.3 Application to Shell Theory
Geometry of the shell continuum. Let us consider a surface in the three-
dimensional Euclidean space deÔ¨Åned by (3.52) as
r = r

t1, t2
,
r ‚ààE3
(3.102)
and bounded by a closed curve C (Fig. 3.5). The shell continuum can then be
described by a vector function
r‚àó= r‚àó
t1, t2, t3
= r

t1, t2
+ g3t3,
(3.103)
where the unit vector g3 is deÔ¨Åned by (3.62) and (3.67) while ‚àíh/2 ‚â§t3 ‚â§
h/2. The surface (3.102) is referred to as the middle surface of the shell.
The thickness of the shell h is assumed to be small in comparison to its other
dimensions as for example the minimal curvature radius of the middle surface.
Every Ô¨Åxed value of the thickness coordinate t3 deÔ¨Ånes a surface r‚àó
t1, t2
whose geometrical variables are obtained according to (1.40), (3.62), (3.64),
(3.79), (3.80), (3.90), (3.93) and (3.103) as follows.
g‚àó
Œ± = r‚àó,Œ± = gŒ± + t3g3,Œ± =

Œ¥œÅ
Œ± ‚àít3bœÅ
Œ±

gœÅ,
Œ± = 1, 2,
(3.104)
g‚àó
3 =
g‚àó
1 √ó g‚àó
2
‚à•g‚àó
1 √ó g‚àó
2‚à•= r‚àó,3 = g3,
(3.105)
g‚àó
Œ±Œ≤ = g‚àó
Œ± ¬∑ g‚àó
Œ≤ = gŒ±Œ≤ ‚àí2t3bŒ±Œ≤ +

t32 bŒ±œÅbœÅ
Œ≤,
Œ±, Œ≤ = 1, 2,
(3.106)
g‚àó= [g‚àó
1g‚àó
2g‚àó
3] =

Œ¥œÅ
1 ‚àít3bœÅ
1

gœÅ

Œ¥Œ≥
2 ‚àít3bŒ≥
2

gŒ≥g3

=

Œ¥œÅ
1 ‚àít3bœÅ
1
 
Œ¥Œ≥
2 ‚àít3bŒ≥
2

geœÅŒ≥3 = g
Œ¥Œ±
Œ≤ ‚àít3bŒ±
Œ≤

= g

1 ‚àí2t3H +

t32 K

.
(3.107)
The factor in brackets in the latter expression
Œº = g‚àó
g = 1 ‚àí2t3H +

t32 K
(3.108)
is called the shell shifter.

72
3 Curves and Surfaces in Three-Dimensional Euclidean Space
g ‚àó
2
g ‚àó
3 = g 3
g ‚àó
1
h/2
h/2
A(2)
A(1)
r ‚àó(t1, t2, t3)
g 3
t2-line
t3-line
t1-line
g 1
middle surface
boundary
curve C
r(t1, t2)
g 2
Fig. 3.5. Geometry of the shell continuum
Internal force variables. Let us consider an element of the shell contin-
uum (see Fig. 3.6) bounded by the coordinate lines tŒ± and tŒ±+ŒîtŒ± (Œ± = 1, 2).
One deÔ¨Ånes the force vector f Œ± and the couple vector mŒ± relative to the mid-
dle surface of the shell, respectively, by
f Œ± =
h/2
'
‚àíh/2
ŒºœÉg‚àóŒ±dt3,
mŒ± =
h/2
'
‚àíh/2
Œºr‚àó√ó (œÉg‚àóŒ±) dt3,
Œ± = 1, 2,
(3.109)

3.3 Application to Shell Theory
73
m1(t1 + Œît1)
p
c
‚àím 2(t2)
‚àíf 2(t2)
f 1(t1 + Œît1)
f 2(t2 + Œît2)
‚àím1(t1)
‚àíf 1(t1)
t1
Œît1
Œît2
r
m2(t2 + Œît2)
t3
t2
Fig. 3.6. Force variables related to the middle surface of the shell
where œÉ denotes the Cauchy stress tensor on the boundary surface A(Œ±)
spanned on the coordinate lines t3 and tŒ≤ (Œ≤ Ã∏= Œ±). The unit normal to this
boundary surface is given by
n(Œ±) =
g‚àóŒ±
‚à•g‚àóŒ±‚à•=
g‚àóŒ±
‚àög‚àóŒ±Œ± =
g‚àó
g‚àó
Œ≤Œ≤
g‚àóŒ±,
Œ≤ Ã∏= Œ± = 1, 2,
(3.110)
where we keep in mind that g‚àóŒ± ¬∑ g‚àó
Œ≤ = g‚àóŒ± ¬∑ g3 = 0 and (see Exercise 3.7)
g‚àóŒ±Œ± =
g‚àó
Œ≤Œ≤
g‚àó2 ,
Œ≤ Ã∏= Œ± = 1, 2.
(3.111)
Applying the Cauchy theorem (1.72) and bearing (3.108) in mind we obtain
f Œ± = 1
g
h/2
'
‚àíh/2

g‚àó
Œ≤Œ≤tdt3,
mŒ± = 1
g
h/2
'
‚àíh/2

g‚àó
Œ≤Œ≤ (r‚àó√ó t) dt3,
(3.112)
where again Œ≤ Ã∏= Œ± = 1, 2 and t denotes the Cauchy stress vector. The force
and couple resulting on the whole boundary surface can thus be expressed
respectively by
'
A(Œ±)
tdA(Œ±) =
tŒ≤+ŒîtŒ≤
'
tŒ≤
h/2
'
‚àíh/2
t

g‚àó
Œ≤Œ≤dt3dtŒ≤ =
tŒ≤+ŒîtŒ≤
'
tŒ≤
gf Œ±dtŒ≤,
(3.113)

74
3 Curves and Surfaces in Three-Dimensional Euclidean Space
'
A(Œ±)
(r‚àó√ó t) dA(Œ±) =
tŒ≤+ŒîtŒ≤
'
tŒ≤
h/2
'
‚àíh/2
(r‚àó√ó t)

g‚àó
Œ≤Œ≤dt3dtŒ≤
=
tŒ≤+ŒîtŒ≤
'
tŒ≤
gmŒ±dtŒ≤,
Œ≤ Ã∏= Œ± = 1, 2,
(3.114)
where we make use of the relation
dA(Œ±) = g‚àó‚àög‚àóŒ±Œ±dtŒ≤dt3 =

g‚àó
Œ≤Œ≤dtŒ≤dt3,
Œ≤ Ã∏= Œ± = 1, 2
(3.115)
following immediately from (2.96) and (3.111).
The force and couple vectors (3.109) are usually represented with respect
to the basis related to the middle surface as (see also [1])
f Œ± = f Œ±Œ≤gŒ≤ + qŒ±g3,
mŒ± = mŒ±Œ≤g3 √ó gŒ≤ = g e3Œ≤œÅmŒ±Œ≤gœÅ.
(3.116)
In shell theory, their components are denoted as follows.
f Œ±Œ≤ - components of the stress resultant tensor,
qŒ±
- components of the transverse shear stress vector,
mŒ±Œ≤ - components of the moment tensor.
External force variables. One deÔ¨Ånes the load force vector and the load
moment vector related to a unit area of the middle surface, respectively by
p = pigi,
c = cœÅg3 √ó gœÅ.
(3.117)
The load moment vector c is thus assumed to be tangential to the middle
surface. The resulting force and couple can be expressed respectively by
t2+Œît2
'
t2
t1+Œît1
'
t1
pgdt1dt2,
t2+Œît2
'
t2
t1+Œît1
'
t1
cgdt1dt2.
(3.118)
Equilibrium conditions. Taking (3.113) and (3.118)1 into account the
force equilibrium condition of the shell element can be expressed as
2
	
Œ±,Œ≤=1
Œ±Ã∏=Œ≤
tŒ≤+ŒîtŒ≤
'
tŒ≤
[g (tŒ± + ŒîtŒ±) f Œ± (tŒ± + ŒîtŒ±) ‚àíg (tŒ±) f Œ± (tŒ±)] dtŒ≤

3.3 Application to Shell Theory
75
+
t2+Œît2
'
t2
t1+Œît1
'
t1
pgdt1dt2 = 0.
(3.119)
Rewriting the Ô¨Årst integral in (3.119) we further obtain
t2+Œît2
'
t2
t1+Œît1
'
t1
[(gfŒ±) ,Œ± +gp] dt1dt2 = 0.
(3.120)
Since the latter condition holds for all shell elements we infer that
(gf Œ±) ,Œ± +gp = 0,
(3.121)
which leads by virtue of (2.98) and (3.73)2 to
f Œ±|Œ± +p = 0,
(3.122)
where the covariant derivative is formally applied to the vectors f Œ± according
to (3.76)1.
In a similar fashion we can treat the moment equilibrium. In this case, we
obtain instead of (3.121) the following condition
[g (mŒ± + r √ó f Œ±)] ,Œ± +gr √ó p + gc = 0.
(3.123)
With the aid of (3.62) and keeping (3.122) in mind, it Ô¨Ånally delivers
mŒ±|Œ± +gŒ± √ó f Œ± + c = 0.
(3.124)
In order to rewrite the equilibrium conditions (3.122) and (3.124) in compo-
nent form we further utilize representations (3.116), (3.117) and apply the
product rule of diÔ¨Äerentiation for the covariant derivative (see, e.g., (2.92-
2.94)). By virtue of (3.78) and (3.80) it delivers
(f Œ±œÅ|Œ± ‚àíbœÅ
Œ±qŒ± + pœÅ) gœÅ +

f Œ±Œ≤bŒ±Œ≤ + qŒ±|Œ± +p3
g3 = 0,
(3.125)
(mŒ±œÅ|Œ± ‚àíqœÅ + cœÅ) g3 √ó gœÅ + g eŒ±Œ≤3 Àúf Œ±Œ≤g3 = 0
(3.126)
with a new variable
Àúf Œ±Œ≤ = f Œ±Œ≤ + bŒ≤
Œ≥mŒ≥Œ±,
Œ±, Œ≤ = 1, 2
(3.127)
called pseudo-stress resultant. Keeping in mind that the vectors gi (i = 1, 2, 3)
are linearly independent we thus obtain the following scalar force equilibrium
conditions
f Œ±œÅ|Œ± ‚àíbœÅ
Œ±qŒ± + pœÅ = 0,
œÅ = 1, 2,
(3.128)
bŒ±Œ≤f Œ±Œ≤ + qŒ±|Œ± +p3 = 0
(3.129)

76
3 Curves and Surfaces in Three-Dimensional Euclidean Space
and moment equilibrium conditions
mŒ±œÅ|Œ± ‚àíqœÅ + cœÅ = 0,
œÅ = 1, 2,
(3.130)
Àúf Œ±Œ≤ = Àúf Œ≤Œ±,
Œ±, Œ≤ = 1, 2,
Œ± Ã∏= Œ≤.
(3.131)
With the aid of (3.127) one can Ô¨Ånally eliminate the components of the stress
resultant tensor f Œ±Œ≤ from (3.128) and (3.129). This leads to the following
equation system
Àúf Œ±œÅ|Œ± ‚àí

bœÅ
Œ≥mŒ≥Œ±
|Œ± ‚àíbœÅ
Œ±qŒ± + pœÅ = 0,
œÅ = 1, 2,
(3.132)
bŒ±Œ≤ Àúf Œ±Œ≤ ‚àíbŒ±Œ≤bŒ≤
Œ≥mŒ≥Œ± + qŒ±|Œ± +p3 = 0,
(3.133)
mŒ±œÅ|Œ± ‚àíqœÅ + cœÅ = 0,
œÅ = 1, 2,
(3.134)
where the latter relation is repeated from (3.130) for completeness.
Example. Equilibrium equations of plate theory. In this case, the
middle surface of the shell is a plane (3.55) for which
bŒ±Œ≤ = bŒ±
Œ≤ = 0,
Œ±, Œ≤ = 1, 2.
(3.135)
Thus, the equilibrium equations (3.132-3.134) simplify to
f Œ±œÅ,Œ± +pœÅ = 0,
œÅ = 1, 2,
(3.136)
qŒ±,Œ± +p3 = 0,
(3.137)
mŒ±œÅ,Œ± ‚àíqœÅ + cœÅ = 0,
œÅ = 1, 2,
(3.138)
where in view of (3.127) and (3.131) f Œ±Œ≤ = f Œ≤Œ± (Œ± Ã∏= Œ≤ = 1, 2).
Example. Equilibrium equations of membrane theory. The mem-
brane theory assumes that the shell is moment free so that
mŒ±Œ≤ = 0,
cŒ≤ = 0,
Œ±, Œ≤ = 1, 2.
(3.139)
In this case, the equilibrium equations (3.132-3.134) reduce to
f Œ±œÅ|Œ± +pœÅ = 0,
œÅ = 1, 2,
(3.140)
bŒ±Œ≤f Œ±Œ≤ + p3 = 0,
(3.141)
qœÅ = 0,
œÅ = 1, 2,
(3.142)
where again f Œ±Œ≤ = f Œ≤Œ± (Œ± Ã∏= Œ≤ = 1, 2).

3.3 Application to Shell Theory
77
Exercises
3.1. Show that a curve r (s) is a straight line if Œ∫ (s) ‚â°0 for any s .
3.2. Show that the curves r (s) and r‚Ä≤ (s) = r (‚àís) have the same curvature
and torsion.
3.3. Show that a curve r (s) characterized by zero torsion œÑ (s) ‚â°0 for any s
lies in a plane.
3.4. Evaluate the ChristoÔ¨Äel symbols of the second kind, the coeÔ¨Écients of
the Ô¨Årst and second fundamental forms, the Gaussian and mean curvatures
for the cylinder (3.56).
3.5. Evaluate the ChristoÔ¨Äel symbols of the second kind, the coeÔ¨Écients of
the Ô¨Årst and second fundamental forms, the Gaussian and mean curvatures
for the sphere (3.58).
3.6. For the so-called hyperbolic paraboloidal surface deÔ¨Åned by
r

t1, t2
= t1e1 + t2e2 + t1t2
c e3,
c > 0,
(3.143)
evaluate the tangent vectors to the coordinate lines, the coeÔ¨Écients of the
Ô¨Årst and second fundamental forms, the Gaussian and mean curvatures.
3.7. Verify relation (3.111).
3.8. Write out equilibrium equations (3.140-3.141) of the membrane theory
for a cylindrical shell and a spherical shell.

4
Eigenvalue Problem and Spectral
Decomposition of Second-Order Tensors
4.1 ComplexiÔ¨Åcation
So far we have considered solely real vectors and real vector spaces. For the
purposes of this chapter an introduction of complex vectors is, however, nec-
essary. Indeed, in the following we will see that the existence of a solution of
an eigenvalue problem even for real second-order tensors can be guaranteed
only within a complex vector space. In order to deÔ¨Åne the complex vector
space let us consider ordered pairs ‚ü®x, y‚ü©of real vectors x and y ‚ààEn. The
sum of two such pairs is deÔ¨Åned by [14]
‚ü®x1, y1‚ü©+ ‚ü®x2, y2‚ü©= ‚ü®x1 + x2, y1 + y2‚ü©.
(4.1)
Further, we deÔ¨Åne the product of a pair ‚ü®x, y‚ü©by a complex number Œ± + iŒ≤
by
(Œ± + iŒ≤) ‚ü®x, y‚ü©= ‚ü®Œ±x ‚àíŒ≤y, Œ≤x + Œ±y‚ü©,
(4.2)
where Œ±, Œ≤ ‚ààR and i = ‚àö‚àí1. These formulas can easily be recovered assuming
that
‚ü®x, y‚ü©= x + iy.
(4.3)
The deÔ¨Ånitions (4.1) and (4.2) enriched by the zero pair ‚ü®0, 0‚ü©are suÔ¨Écient to
ensure that the axioms (A.1-A.4) and (B.1-B.4) of Chap. 1 are valid. Thus, the
set of all pairs z = ‚ü®x, y‚ü©characterized by the above properties forms a vector
space referred to as complex vector space. Every basis G = {g1, g2, . . . , gn}
of the underlying Euclidean space En represents simultaneously a basis of the
corresponding complexiÔ¨Åed space. Indeed, for every complex vector within
this space
z = x + iy,
(4.4)
where x, y ‚ààEn and consequently

80
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
x = xigi,
y = yigi,
(4.5)
we can write
z =

xi + iyi
gi.
(4.6)
Thus, the dimension of the complexiÔ¨Åed space coincides with the dimension
of the original real vector space. Using this fact we will denote the complex
vector space based on En by Cn. Clearly, En represents a subset of Cn.
For every vector z ‚ààCn given by (4.4) one deÔ¨Ånes a complex conjugate
counterpart by
z = x ‚àíiy.
(4.7)
Of special interest is the scalar product of two complex vectors, say z1 =
x1 + iy1 and z2 = x2 + iy2, which we deÔ¨Åne by (see also [4])
(x1 + iy1) ¬∑ (x2 + iy2) = x1 ¬∑ x2 ‚àíy1 ¬∑ y2 + i (x1 ¬∑ y2 + y1 ¬∑ x2) .
(4.8)
This scalar product is commutative (C.1), distributive (C.2) and linear in
each factor (C.3). Thus, it diÔ¨Äers from the classical scalar product of complex
vectors given in terms of the complex conjugate (see, e.g., [14]). As a result,
the axiom (C.4) does not generally hold. For instance, one can easily imagine a
non-zero complex vector whose scalar product with itself is zero. For complex
vectors with the scalar product (4.8) the notions of length, orthogonality or
parallelity can hardly be interpreted geometrically.
However, for complex vectors the axiom (C.4) can be reformulated by
z ¬∑ z ‚â•0,
z ¬∑ z = 0
if and only if
z = 0.
(4.9)
Indeed, using (4.4), (4.7) and (4.8) we obtain z ¬∑ z = x ¬∑ x + y ¬∑ y. Bearing in
mind that the vectors x and y belong to the Euclidean space this immediately
implies (4.9).
As we learned in Chap. 1, the Euclidean space En is characterized by
the existence of an orthonormal basis (1.8). This can now be postulated for
the complex vector space Cn as well, because Cn includes En by the very
deÔ¨Ånition. Also Theorem 1.6 remains valid since it has been proved without
making use of the property (C.4). Thus, we may state that for every basis in
Cn there exists a unique dual basis.
The last step of the complexiÔ¨Åcation is a generalization of a linear mapping
on complex vectors. This can be achieved by setting for every tensor A ‚ààLinn
A (x + iy) = Ax + i (Ay) .
(4.10)
4.2 Eigenvalue Problem, Eigenvalues and Eigenvectors
Let A ‚ààLinn be a second-order tensor. The equation

4.2 Eigenvalue Problem, Eigenvalues and Eigenvectors
81
Aa = Œªa,
a Ã∏= 0
(4.11)
is referred to as the eigenvalue problem of the tensor A. The non-zero vector
a ‚ààCn satisfying this equation is called an eigenvector of A; Œª is called an
eigenvalue of A. It is clear that any product of an eigenvector with any (real
or complex) scalar is again an eigenvector.
The eigenvalue problem (4.11) and the corresponding eigenvector a can
be regarded as the right eigenvalue problem and the right eigenvector, respec-
tively. In contrast, one can deÔ¨Åne the left eigenvalue problem by
bA = Œªb,
b Ã∏= 0,
(4.12)
where b ‚ààCn is the left eigenvector. In view of (1.110), every right eigenvector
of A represents the left eigenvector of AT and vice versa. In the following,
unless indicated otherwise, we will mean the right eigenvalue problem and the
right eigenvector.
Mapping (4.11) by A several times we obtain
Aka = Œªka,
k = 1, 2, . . .
(4.13)
This leads to the following (spectral mapping) theorem.
Theorem 4.1. Let Œª be an eigenvalue of the tensor A and let g (A) =
(m
k=0 akAk be a polynomial of A. Then g (Œª) = (m
k=0 akŒªk is the eigenvalue
of g (A).
Proof. Let a be an eigenvector of A associated with Œª. Then, in view of (4.13)
g (A) a =
m
	
k=0
akAka =
m
	
k=0
akŒªka =
) m
	
k=0
akŒªk
*
a = g (Œª) a.
In order to Ô¨Ånd the eigenvalues of the tensor A we consider the following
representations:
A = Ai
¬∑jgi ‚äógj,
a = aigi,
b = bigi,
(4.14)
where G = {g1, g2, . . . , gn} and G‚Ä≤ =

g1, g2, . . . , gn
are two arbitrary mu-
tually dual bases in En and consequently also in Cn. Note that we prefer here
the mixed variant representation of the tensor A. Inserting (4.14) into (4.11)
and (4.12) further yields
Ai
¬∑jajgi = Œªaigi,
Ai
¬∑jbigj = Œªbjgj,
and therefore

Ai
¬∑jaj ‚àíŒªai
gi = 0,

Ai
¬∑jbi ‚àíŒªbj

gj = 0.
(4.15)

82
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
Since the vectors gi on the one side and gi (i = 1, 2, . . ., n) on the other side
are linearly independent the associated scalar coeÔ¨Écients in (4.15) must be
zero. This results in the following two linear homogeneous equation systems

Ai
¬∑j ‚àíŒªŒ¥i
j

aj = 0,

Aj
¬∑i ‚àíŒªŒ¥j
i

bj = 0,
i = 1, 2, . . ., n
(4.16)
with respect to the components of the right eigenvector a and the left eigen-
vector b, respectively. A non-trivial solution of these equation systems exists
if and only if
Ai
¬∑j ‚àíŒªŒ¥i
j
 = 0,
(4.17)
where |‚Ä¢| denotes the determinant of a matrix. Eq. (4.17) is called the char-
acteristic equation of the tensor A. Writing out the determinant on the left
hand side of this equation one obtains a polynomial of degree n with respect
to the powers of Œª
pA (Œª) = Œªn ‚àíŒªn‚àí1I(1)
A + . . . + (‚àí1)k Œªn‚àíkI(k)
A + . . . + (‚àí1)n I(n)
A ,
(4.18)
referred to as the characteristic polynomial of the tensor A. Thereby, it can
easily be seen that
I(1)
A = Ai
¬∑i = trA,
I(n)
A =
Ai
¬∑j
 .
(4.19)
The characteristic equation (4.17) can brieÔ¨Çy be written as
pA (Œªi) = 0.
(4.20)
According to the fundamental theorem of algebra, a polynomial of degree n
has n complex roots which may be multiple. These roots are the eigenvalues
Œªi (i = 1, 2, . . . , n) of the tensor A.
Factorizing the characteristic polynomial (4.18) yields
pA (Œª) =
n
+
i=1
(Œª ‚àíŒªi) .
(4.21)
Collecting multiple eigenvalues the polynomial (4.21) can further be rewritten
as
pA (Œª) =
s
+
i=1
(Œª ‚àíŒªi)ri ,
(4.22)
where s (1 ‚â§s ‚â§n) denotes the number of distinct eigenvalues, while ri is
referred to as an algebraic multiplicity of the eigenvalue Œªi (i = 1, 2, . . . , s). It
should formally be distinguished from the so-called geometric multiplicity ti,
which represents the number of linearly independent eigenvectors associated
with this eigenvalue.

4.3 Characteristic Polynomial
83
Example. Eigenvalues and eigenvectors of the deformation gradi-
ent in the case of simple shear. In simple shear, the deformation gradient
can be given by F = Fi
¬∑jei ‚äóej, where

Fi
¬∑j

=
‚é°
‚é£
1 Œ≥ 0
0 1 0
0 0 1
‚é§
‚é¶
(4.23)
and Œ≥ denotes the amount of shear. The characteristic equation (4.17) for the
tensor F takes thus the form

1 ‚àíŒª
Œ≥
0
0
1 ‚àíŒª
0
0
0
1 ‚àíŒª

= 0.
Writing out this determinant we obtain
(1 ‚àíŒª)3 = 0,
which yields one triple eigenvalue
Œª1 = Œª2 = Œª3 = 1.
The associated (right) eigenvectors a = aiei can be obtained from the equa-
tion system (4.16)1 i.e.

Fi
¬∑j ‚àíŒªŒ¥i
j

aj = 0,
i = 1, 2, 3.
In view of (4.23) it reduces to the only non-trivial equation
a2Œ≥ = 0.
Hence, all eigenvectors of F can be given by a = a1e1 + a3e3. They are linear
combinations of the only two linearly independent eigenvectors e1 and e3.
Accordingly, the geometric and algebraic multiplicities of the eigenvalue 1 are
t1 = 2 and r1 = 3, respectively.
4.3 Characteristic Polynomial
By the very deÔ¨Ånition of the eigenvalue problem (4.11) the eigenvalues are
independent of the choice of the basis. This is also the case for the coeÔ¨É-
cients I(i)
A (i = 1, 2, . . ., n) of the characteristic polynomial (4.18) because they
uniquely deÔ¨Åne the eigenvalues and vice versa. These coeÔ¨Écients are called
principal invariants of A. Comparing (4.18) with (4.21) and applying the Vi-
eta theorem we obtain the following relations between the principal invariants
and eigenvalues:

84
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
I(1)
A = Œª1 + Œª2 + . . . + Œªn,
I(2)
A = Œª1Œª2 + Œª1Œª3 + . . . + Œªn‚àí1Œªn,
...
I(k)
A =
n
	
o1<o2<...<ok
Œªo1Œªo2 . . . Œªok,
...
I(n)
A = Œª1Œª2 . . . Œªn.
(4.24)
The principal invariants can also be expressed in terms of the so-called princi-
pal traces trAk (k = 1, 2, . . ., n). Indeed, by use of (4.13), (4.19)1 and (4.24)1
we Ô¨Årst write
trAk = Œªk
1 + Œªk
2 + . . . + Œªk
n,
k = 1, 2, . . . , n.
(4.25)
Then, we apply Newton‚Äôs formula (see e.g. [9]) relating coeÔ¨Écients of a poly-
nomial to its roots represented by the sum of the powers in the form of the
right hand side of (4.25). Taking (4.25) into account, Newton‚Äôs formula can
thus be written as
I(1)
A = trA,
I(2)
A = 1
2

I(1)
A trA ‚àítrA2
,
I(3)
A = 1
3

I(2)
A trA ‚àíI(1)
A trA2 + trA3
,
...
I(k)
A = 1
k

I(k‚àí1)
A
trA ‚àíI(k‚àí2)
A
trA2 + . . . + (‚àí1)k‚àí1 trAk
= 1
k
k
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A
trAi,
...
I(n)
A = detA,
(4.26)
where we set I(0)
A = 1 and
detA =
Ai
¬∑j
 =
A i
j¬∑

(4.27)
is called the determinant of the tensor A.
Example. Three-dimensional space. For illustration, we consider a
second-order tensor A in three-dimensional space. In this case, the character-
istic polynomial (4.18) takes the form
pA (Œª) = Œª3 ‚àíIAŒª2 + IIAŒª ‚àíIIIA,
(4.28)

4.4 Spectral Decomposition and Eigenprojections
85
where
IA = I(1)
A = trA,
IIA = I(2)
A = 1
2

(trA)2 ‚àítrA2
,
IIIA = I(3)
A = 1
3
"
trA3 ‚àí3
2trA2trA + 1
2 (trA)3
#
= detA
(4.29)
are the principal invariants (4.24) of the tensor A. They can alternatively be
expressed in terms of the eigenvalues as follows
IA = Œª1 + Œª2 + Œª3,
IIA = Œª1Œª2 + Œª2Œª3 + Œª3Œª1,
IIIA = Œª1Œª2Œª3. (4.30)
The roots of the cubic polynomial (4.28) can be obtained in a closed form by
means of the Cardano formula (see, e.g. [5]) as
Œªk = 1
3
!
IA + 2

I2
A ‚àí3IIA cos 1
3 [œë + 2œÄ (k ‚àí1)]
$
,
k = 1, 2, 3,
(4.31)
where
œë = arccos
,
2I3
A ‚àí9IAIIA + 27IIIA
2

I2
A ‚àí3IIA
3/2
-
,
I2
A ‚àí3IIA Ã∏= 0.
(4.32)
In the case I2
A ‚àí3IIA = 0, the eigenvalues of A take another form
Œªk = 1
3IA + 1
3

27IIIA ‚àíI3
A
1/3 
cos

 2
3œÄk

+ i sin

 2
3œÄk

,
(4.33)
where k = 1, 2, 3.
4.4 Spectral Decomposition and Eigenprojections
The spectral decomposition is a powerful tool for the tensor analysis and
tensor algebra. It enables to gain a deeper insight into the properties of second-
order tensors and to represent various useful tensor operations in a relatively
simple form. In the spectral decomposition, eigenvectors represent one of the
most important ingredients.
Theorem 4.2. The eigenvectors of a second-order tensor corresponding to
pairwise distinct eigenvalues are linearly independent.
Proof. Suppose that these eigenvectors are linearly dependent. Among all pos-
sible nontrivial linear relations connecting them we can choose one involving
the minimal number, say r, of eigenvectors ai Ã∏= 0 (i = 1, 2, . . . , r). Obviously,
1 < r ‚â§n. Thus,

86
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
r
	
i=1
Œ±iai = 0,
(4.34)
where all Œ±i (i = 1, 2, . . . , r) are non-zero. We can also write
Aai = Œªiai,
i = 1, 2, . . . , r,
(4.35)
where Œªi Ã∏= Œªj, (i Ã∏= j = 1, 2, . . ., r). Mapping both sides of (4.34) by A and
taking (4.35) into account we obtain
r
	
i=1
Œ±iAai =
r
	
i=1
Œ±iŒªiai = 0.
(4.36)
Multiplying (4.34) by Œªr and subtracting from (4.36) yield
0 =
r
	
i=1
Œ±i (Œªi ‚àíŒªr) ai =
r‚àí1
	
i=1
Œ±i (Œªi ‚àíŒªr) ai.
In the latter linear combination none of the coeÔ¨Écients is zero. Thus, we have
a linear relation involving only r ‚àí1 eigenvectors. This contradicts, however,
the earlier assumption that r is the smallest number of eigenvectors satisfying
such a relation.
Theorem 4.3. Let bi be a left and aj a right eigenvector associated with
distinct eigenvalues Œªi Ã∏= Œªj of a tensor A. Then,
bi ¬∑ aj = 0.
(4.37)
Proof. With the aid of (1.73) and taking (4.11) into account we can write
biAaj = bi ¬∑ (Aaj) = bi ¬∑ (Œªjaj) = Œªjbi ¬∑ aj.
On the other hand, in view of (4.12)
biAaj = (biA) ¬∑ aj = (biŒªi) ¬∑ aj = Œªibi ¬∑ aj.
Subtracting one equation from another one we obtain
(Œªi ‚àíŒªj) bi ¬∑ aj = 0.
Since Œªi Ã∏= Œªj this immediately implies (4.37).
Now, we proceed with the spectral decomposition of a second-order tensor
A. First, we consider the case of n simple eigenvalues. Solving the equa-
tion systems (4.16) one obtains for every simple eigenvalue Œªi the compo-
nents of the right eigenvector ai and the components of the left eigenvector
bi (i = 1, 2, . . . , n). n right eigenvectors on the one hand and n left eigen-
vectors on the other hand are linearly independent and form bases of Cn.

4.4 Spectral Decomposition and Eigenprojections
87
Obviously, bi ¬∑ ai Ã∏= 0 (i = 1, 2, . . . , n) because otherwise it would contradict
(4.37) (see Exercise 1.8). Normalizing the eigenvectors we can thus write
bi ¬∑ aj = Œ¥ij,
i, j = 1, 2, . . . , n.
(4.38)
Accordingly, the bases ai and bi are dual to each other such that ai = bi
and bi = ai (i = 1, 2, . . ., n). Now, representing A with respect to the basis
ai ‚äóbj (i, j = 1, 2, . . . , n) as A = Aijai ‚äóbj we obtain with the aid of (1.83),
(4.11) and (4.38)
Aij = aiAbj = biAaj = bi ¬∑ (Aaj) = bi ¬∑ (Œªjaj) = ŒªjŒ¥ij,
where i, j = 1, 2, . . . , n. Thus,
A =
n
	
i=1
Œªiai ‚äóbi.
(4.39)
Next, we consider second-order tensors with multiple eigenvalues. We assume,
however, that the algebraic multiplicity ri of every eigenvalue Œªi coincides with
its geometric multiplicity ti. In this case we again have n linearly independent
right eigenvectors forming a basis of Cn (Exercise 4.3). We will denote these
eigenvectors by a(k)
i
(i = 1, 2, . . . , s; k = 1, 2, . . . , ri) where s is the number of
pairwise distinct eigenvalues. Constructing the basis b(l)
j
dual to a(k)
i
such
that
a(k)
i
¬∑ b(l)
j
= Œ¥ijŒ¥kl, i, j = 1, 2, . . ., s; k = 1, 2, . . . , ri; l = 1, 2, . . . , rj (4.40)
we can write similarly to (4.39)
A =
s
	
i=1
Œªi
ri
	
k=1
a(k)
i
‚äób(k)
i
.
(4.41)
The representations of the form (4.39) or (4.41) are called spectral decompo-
sition in diagonal form or, brieÔ¨Çy, spectral decomposition. Note that not every
second-order tensor A ‚ààLinn permits the spectral decomposition. The tensors
which can be represented by (4.39) or (4.41) are referred to as diagonalizable
tensors. For instance, we will show in the next sections that symmetric, skew-
symmetric and orthogonal tensors are always diagonalizable. If, however, the
algebraic multiplicity of at least one eigenvalue exceeds its geometric multi-
plicity, the spectral representation is not possible. Such eigenvalues (for which
ri > ti) are called defective eigenvalues. A tensor that has one or more defec-
tive eigenvalues is called defective tensor. In Sect. 4.2 we have seen, for ex-
ample, that the deformation gradient F represents in the case of simple shear
a defective tensor since its triple eigenvalue 1 is defective. Clearly, a simple
eigenvalue (ri = 1) cannot be defective. For this reason, a tensor whose all
eigenvalues are simple is diagonalizable.

88
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
Now, we look again at the spectral decompositions (4.39) and (4.41). With
the aid of the abbreviation
Pi =
ri
	
k=1
a(k)
i
‚äób(k)
i
,
i = 1, 2, . . ., s
(4.42)
they can be given in a uniÔ¨Åed form by
A =
s
	
i=1
ŒªiPi.
(4.43)
The generally complex tensors Pi (i = 1, 2, . . ., s) deÔ¨Åned by (4.42) are called
eigenprojections. It follows from (4.40) and (4.42) that (Exercise 4.4)
PiPj = Œ¥ijPi,
i, j = 1, 2, . . ., s
(4.44)
and consequently
PiA = APi = ŒªiPi,
i = 1, 2, . . . , s.
(4.45)
Bearing in mind that the eigenvectors a(k)
i
(i = 1, 2, . . . , s; k = 1, 2, . . ., ri)
form a basis of Cn and taking (4.40) into account we also obtain (Exercise
4.5)
s
	
i=1
Pi = I.
(4.46)
Due to these properties of eigenprojections (4.42) the spectral representation
(4.43) is very suitable for calculating tensor powers, polynomials and other
tensor functions deÔ¨Åned in terms of power series. Indeed, in view of (4.44)
powers of A can be expressed by
Ak =
s
	
i=1
Œªk
i Pi,
k = 0, 1, 2, . . .
(4.47)
For a tensor polynomial it further yields
g (A) =
s
	
i=1
g (Œªi) Pi.
(4.48)
For example, the exponential tensor function (1.109) can thus be represented
by
exp (A) =
s
	
i=1
exp (Œªi) Pi.
(4.49)

4.4 Spectral Decomposition and Eigenprojections
89
With the aid of (4.44) and (4.46) the eigenprojections can be obtained without
solving the eigenvalue problem in the general form (4.11). To this end, we Ô¨Årst
consider s polynomial functions pi (Œª) (i = 1, 2, . . ., s) satisfying the following
conditions
pi (Œªj) = Œ¥ij,
i, j = 1, 2, . . ., s.
(4.50)
Thus, by use of (4.48) we obtain
pi (A) =
s
	
j=1
pi (Œªj) Pj =
s
	
j=1
Œ¥ijPj = Pi,
i = 1, 2, . . . , s.
(4.51)
Using Lagrange‚Äôs interpolation formula (see, e.g., [5]) and assuming that s Ã∏= 1
one can represent the functions pi (Œª) (4.50) by the following polynomials of
degree s ‚àí1:
pi (Œª) =
s
+
j=1
jÃ∏=i
Œª ‚àíŒªj
Œªi ‚àíŒªj
,
i = 1, 2, . . . , s > 1.
(4.52)
Considering these expressions in (4.51) we obtain the so-called Sylvester for-
mula as
Pi =
s
+
j=1
jÃ∏=i
A ‚àíŒªjI
Œªi ‚àíŒªj
,
i = 1, 2, . . . , s > 1.
(4.53)
Note that according to (4.46), P1 = I in the the case of s = 1. With this
result in hand the above representation can be generalized by
Pi = Œ¥1sI +
s
+
j=1
jÃ∏=i
A ‚àíŒªjI
Œªi ‚àíŒªj
,
i = 1, 2, . . . , s.
(4.54)
Writing out the product on the right hand side of (4.54) also delivers (see,
e.g., [47])
Pi = 1
Di
s‚àí1
	
p=0
Œπi s‚àíp‚àí1Ap,
i = 1, 2, . . . , s,
(4.55)
where Œπi0 = 1,
Œπip = (‚àí1)p
	
1‚â§o1‚â§¬∑¬∑¬∑‚â§op‚â§s
Œªo1 ¬∑ ¬∑ ¬∑ Œªop (1 ‚àíŒ¥io1) ¬∑ ¬∑ ¬∑

1 ‚àíŒ¥iop

,
Di = Œ¥1s +
s
+
j=1
jÃ∏=i
(Œªi ‚àíŒªj) ,
p = 1, 2, . . . , s ‚àí1,
i = 1, 2, . . ., s.
(4.56)

90
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
4.5 Spectral Decomposition of Symmetric Second-Order
Tensors
We begin with some useful theorems concerning eigenvalues and eigenvectors
of symmetric tensors.
Theorem 4.4. The eigenvalues of a symmetric second-order tensor M ‚àà
Symn are real, the eigenvectors belong to En.
Proof. Let Œª be an eigenvalue of M and a a corresponding eigenvector such
that according to (4.11)
Ma = Œªa.
The complex conjugate counterpart of this equation is
M a = Œª a.
Taking into account that M is real and symmetric such that M = M and
MT = M we obtain in view of (1.110)
a M = Œª a.
Hence, one can write
0 = aMa ‚àíaMa = a ¬∑ (Ma) ‚àí(aM) ¬∑ a
= Œª (a ¬∑ a) ‚àíŒª (a ¬∑ a) =

Œª ‚àíŒª

(a ¬∑ a) .
Bearing in mind that a Ã∏= 0 and taking (4.9) into account we conclude that
a ¬∑ a > 0. Hence, Œª = Œª. The components of a with respect to a basis G =
{g1, g2, . . . , gn} in En are real since they represent a solution of the linear
equation system (4.16)1 with real coeÔ¨Écients. Therefore, a ‚ààEn.
Theorem 4.5. Eigenvectors of a symmetric second-order tensor correspond-
ing to distinct eigenvalues are mutually orthogonal.
Proof. According to Theorem 4.3 right and left eigenvectors associated with
distinct eigenvalues are mutually orthogonal. However, for a symmetric tensor
every right eigenvector represents the left eigenvector associated with the same
eigenvalue and vice versa. For this reason, right (left) eigenvectors associated
with distinct eigenvalues are mutually orthogonal.
Theorem 4.6. Let Œªi be an eigenvalue of a symmetric second order tensor
M. Then, the algebraic and geometric multiplicity of Œªi coincide.
Proof. Let ak ‚ààEn (k = 1, 2, . . ., ti) be all linearly independent eigenvectors
associated with Œªi, while ti and ri denote its geometric and algebraic multi-
plicity, respectively. Every linear combination of ak is again an eigenvector
associated with Œªi. Indeed,

4.5 Spectral Decomposition of Symmetric Second-Order Tensors
91
M
ti
	
k=1
Œ±kak =
ti
	
k=1
Œ±k (Mak) =
ti
	
k=1
Œ±kŒªiak = Œªi
ti
	
k=1
Œ±kak.
(4.57)
According to Theorem 1.4 the set of vectors ak (k = 1, 2, . . ., ti) can be
completed to a basis of En. With the aid of the Gram-Schmidt procedure
described in Chap. 1 (Sect. 1.4) this basis can be transformed to an or-
thonormal basis el (l = 1, 2, . . ., n). Since the vectors ej (j = 1, 2, . . ., ti) are
linear combinations of ak (k = 1, 2, . . ., ti) they likewise represent eigenvec-
tors of M associated with Œªi. Further, we represent the tensor M with
respect to the basis el ‚äóem (l, m = 1, 2, . . . , n). In view of the identities
Mek = Œªiek (k = 1, 2, . . . , ti) and keeping in mind the symmetry of M we
can write:
M = Œªi
ti
	
k=1
ek ‚äóek +
n
	
l,m=ti+1
M‚Ä≤
lmel ‚äóem.
(4.58)
Thus, the characteristic polynomial of M can be given as
pM (Œª) =
M‚Ä≤
lm ‚àíŒªŒ¥lm
 (Œªi ‚àíŒª)ti ,
(4.59)
which implies that ri ‚â•ti.
Now, we consider the vector space En‚àíti of all linear combinations of the
vectors el (l = ti + 1, . . . , n). The tensor
M‚Ä≤ =
n
	
l,m=ti+1
M‚Ä≤
lmel ‚äóem
represents a linear mapping of this space into itself. The eigenvectors of M‚Ä≤
are linear combinations of el (l = ti + 1, . . . , n) and therefore are linearly in-
dependent of ek (k = 1, 2, . . ., ti). Consequently, Œªi is not an eigenvalue of
M‚Ä≤. Otherwise, the eigenvector corresponding to this eigenvalue Œªi would be
linearly independent of ek (k = 1, 2, . . ., ti) which contradicts the previous as-
sumption. Thus, all the roots of the characteristic polynomial of this tensor
pM‚Ä≤ (Œª) =
M‚Ä≤
lm ‚àíŒªŒ¥lm

diÔ¨Äer from Œªi. In view of (4.59) this implies that ri = ti .
As a result of this theorem, the spectral decomposition of a symmetric second-
order tensor can be given by
M =
s
	
i=1
Œªi
ri
	
k=1
a(k)
i
‚äóa(k)
i
=
s
	
i=1
ŒªiPi,
M ‚ààSymn,
(4.60)
in terms of the real symmetric eigenprojections

92
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
Pi =
ri
	
k=1
a(k)
i
‚äóa(k)
i
,
(4.61)
where the eigenvectors a(k)
i
form an orthonormal basis in En so that
a(k)
i
¬∑ a(l)
j
= Œ¥ijŒ¥kl,
(4.62)
where i, j = 1, 2, . . . , s; k = 1, 2, . . ., ri; l = 1, 2, . . ., rj.
4.6 Spectral Decomposition of Orthogonal
and Skew-Symmetric Second-Order Tensors
We begin with the orthogonal tensors Q ‚ààOrthn deÔ¨Åned by the condition
(1.129). For every eigenvector a and the corresponding eigenvalue Œª we can
write
Qa = Œªa,
Qa = Œª a,
(4.63)
because Q is by deÔ¨Ånition a real tensor such that Q = Q. Mapping both sides
of these vector equations by QT and taking (1.110) into account we have
aQ = Œª‚àí1a,
aQ = Œª
‚àí1a.
(4.64)
Thus, every right eigenvector of an orthogonal tensor represents its left eigen-
vector associated with the inverse eigenvalue. Hence, if Œª Ã∏= Œª‚àí1 or, in other
words, Œª is neither +1 nor ‚àí1, Theorem 4.3 immediately implies the relations
a ¬∑ a = 0,
a ¬∑ a = 0,
Œª Ã∏= Œª‚àí1
(4.65)
indicating that a and consequently a are complex (deÔ¨Ånitely not real) vectors.
Using the representation
a =
1
‚àö
2 (p + iq) ,
p, q ‚ààEn
(4.66)
and applying (4.8) one can write
‚à•p‚à•= ‚à•q‚à•= 1,
p ¬∑ q = 0.
(4.67)
Now, we consider the product aQa. With the aid of (4.63)1 and (4.64)2 we
obtain
aQa = Œª (a ¬∑ a) = Œª
‚àí1 (a ¬∑ a) .
(4.68)
Since, however, a ¬∑ a = 1/2 (p ¬∑ p + q ¬∑ q) = 1 we infer that

4.6 Spectral Decomposition of Orthogonal and Skew-Symmetric Tensors
93
ŒªŒª = 1.
(4.69)
Thus, all eigenvalues of an orthogonal tensor have absolute value 1 so that we
can write
Œª = eœâi = cos œâ + i sin œâ.
(4.70)
By virtue of (4.69) one can further rewrite (4.64) as
aQ = Œªa,
aQ = Œªa.
(4.71)
Summarizing these results we conclude that every complex (deÔ¨Ånitely not real)
eigenvalue Œª of an orthogonal tensor comes in pair with its complex conjugate
counterpart Œª = Œª‚àí1. If a is a right eigenvector associated with Œª, then a is
its left eigenvector. For Œª, a is, vice versa, the left eigenvector and a the right
one.
Next, we show that the algebraic and geometric multiplicities of ev-
ery eigenvalue of an orthogonal tensor Q coincide. Let Àúak (k = 1, 2, . . . , ti)
be all linearly independent right eigenvectors associated with an eigenvalue
Œªi. According to Theorem 1.4 these vectors can be completed to a basis
of Cn. With the aid of the Gram-Schmidt procedure (see Exercise 4.11)
a linear combination of this basis can be constructed in such a way that
ak ¬∑ al = Œ¥kl (k, l = 1, 2, . . ., n). Since the vectors ak (k = 1, 2, . . . , ti) are
linear combinations of Àúak (k = 1, 2, . . ., ti) they likewise represent eigenvec-
tors of Q associated with Œªi. Thus, representing Q with respect to the basis
ak ‚äóal (k, l = 1, 2, . . ., n) we can write
Q = Œªi
ti
	
k=1
ak ‚äóak +
n
	
l,m=ti+1
Q‚Ä≤
lmal ‚äóam.
Comparing this representation with (4.58) and using the same reasoning as
applied for the proof of Theorem 4.6 we infer that Œªi cannot be an eigenvalue of
Q‚Ä≤ = (n
l,m=ti+1 Q‚Ä≤
lmal ‚äóam. This means that the algebraic multiplicity ri of
Œªi coincides with its geometric multiplicity ti. Thus, every orthogonal tensor
Q ‚ààOrthn is characterized by exactly n linearly independent eigenvectors
forming a basis of Cn. Using this fact the spectral decomposition of Q can be
given by
Q =
r+1
	
k=1
a(k)
+1 ‚äóa(k)
+1 ‚àí
r‚àí1
	
l=1
a(l)
‚àí1 ‚äóa(l)
‚àí1
+
s
	
i=1

Œªi
ri
	
k=1
a(k)
i
‚äóa(k)
i
+ Œªi
ri
	
k=1
a(k)
i
‚äóa(k)
i
.
,
(4.72)
where r+1 and r‚àí1 denote the algebraic multiplicities of real eigenvalues +1
and ‚àí1, respectively, while a(k)
+1 (k = 1, 2, . . ., r+1) and a(l)
‚àí1 (l = 1, 2, . . . , r‚àí1)

94
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
are the corresponding orthonormal real eigenvectors. s is the number of com-
plex conjugate pairs of eigenvalues Œªi = cos œâi¬±i sin œâi with distinct arguments
œâi and the multiplicities ri. The associated eigenvectors a(k)
i
and a(k)
i
obey
the following relations (see also Exercise 4.12)
a(k)
i
¬∑a(o)
+1 = 0,
a(k)
i
¬∑a(p)
‚àí1 = 0,
a(k)
i
¬∑a(l)
j
= Œ¥ijŒ¥kl,
a(k)
i
¬∑a(m)
i
= 0, (4.73)
where i, j = 1, 2, . . . , s; k, m = 1, 2, . . . , ri; l = 1, 2, . . . , rj; o = 1, 2, . . . , r+1;
p = 1, 2, . . . , r‚àí1. Using the representations (4.66) and (4.70) the spectral
decomposition (4.72) can alternatively be written as
Q =
r+1
	
k=1
a(k)
+1 ‚äóa(k)
+1 +
s
	
i=1
cos œâi
ri
	
k=1

p(k)
i
‚äóp(k)
i
+ q(k)
i
‚äóq(k)
i

‚àí
r‚àí1
	
l=1
a(l)
‚àí1 ‚äóa(l)
‚àí1 +
s
	
i=1
sin œâi
ri
	
k=1

p(k)
i
‚äóq(k)
i
‚àíq(k)
i
‚äóp(k)
i

.
(4.74)
Now, we turn our attention to skew-symmetric tensors W ‚ààSkewn as deÔ¨Åned
in (1.148). Instead of (4.64) and (4.68) we have in this case
aW = ‚àíŒªa,
a W = ‚àíŒª a,
(4.75)
aWa = Œª (a ¬∑ a) = ‚àíŒª (a ¬∑ a)
(4.76)
and consequently
Œª = ‚àíŒª.
(4.77)
Thus, the eigenvalues of W are either zero or imaginary. The latter ones come
in pairs with the complex conjugate like in the case of orthogonal tensors.
Similarly to (4.72) and (4.74) we thus obtain
W =
s
	
i=1
œâii
ri
	
k=1

a(k)
i
‚äóa(k)
i
‚àía(k)
i
‚äóa(k)
i

=
s
	
i=1
œâi
ri
	
k=1

p(k)
i
‚äóq(k)
i
‚àíq(k)
i
‚äóp(k)
i

,
(4.78)
where s denotes the number of pairwise distinct imaginary eigenvalues œâii
while the associated eigenvectors a(k)
i
and a(k)
i
are subject to the restrictions
(4.73)3,4.
Orthogonal tensors in three-dimensional space. In the three-dimen-
sional case Q ‚ààOrth3, at least one of the eigenvalues is real, since complex
eigenvalues of orthogonal tensors appear in pairs with the complex conjugate.
Hence, we can write
Œª1 = ¬±1,
Œª2 = eiœâ = cos œâ + i sin œâ,
Œª3 = e‚àíiœâ = cos œâ ‚àíi sin œâ. (4.79)

4.6 Spectral Decomposition of Orthogonal and Skew-Symmetric Tensors
95
In the case sin œâ = 0 all three eigenvalues become real. The principal invariants
(4.30) take thus the form
IQ = Œª1 + 2 cosœâ = ¬±1 + 2 cosœâ,
IIQ = 2Œª1 cos œâ + 1 = Œª1IQ = ¬±IQ,
IIIQ = Œª1 = ¬±1.
(4.80)
The spectral representation (4.72) takes the form
Q = ¬±a1 ‚äóa1 + (cos œâ + i sin œâ) a ‚äóa + (cos œâ ‚àíi sin œâ) a ‚äóa,
(4.81)
where a1 ‚ààE3 and a ‚ààC3 is given by (4.66) and (4.67). Taking into account
that by (4.73)
a1 ¬∑ a = a1 ¬∑ p = a1 ¬∑ q = 0
(4.82)
we can set
a1 = q √ó p.
(4.83)
Substituting (4.66) into (4.81) we also obtain
Q = ¬±a1 ‚äóa1 + cos œâ (p ‚äóp + q ‚äóq) + sin œâ (p ‚äóq ‚àíq ‚äóp) .
(4.84)
By means of the vector identity (1.130) and considering (1.65) and (4.83) it
Ô¨Ånally leads to
Q = cos œâI + sin œâ ÀÜa1 + (¬±1 ‚àícos œâ) a1 ‚äóa1.
(4.85)
Comparing this representation with (1.71) we observe that any orthogonal
tensor Q ‚ààOrth3 describes a rotation in three-dimensional space if IIIQ =
Œª1 = 1. The eigenvector a1 corresponding to the eigenvalue 1 speciÔ¨Åes the
rotation axis. In this case, Q is referred to as a proper orthogonal tensor.
Skew-symmetric tensors in three-dimensional space. For a skew-
symmetric tensor W ‚ààSkew3 we can write in view of (4.77)
Œª1 = 0,
Œª2 = œâi,
Œª3 = ‚àíœâi.
(4.86)
Similarly to (4.80) we further obtain (see Exercise 4.13)
IW = 0,
IIW = 1
2 ‚à•W‚à•2 = œâ2,
IIIW = 0.
(4.87)
The spectral representation (4.78) takes the form
W = œâi (a ‚äóa ‚àía ‚äóa) = œâ (p ‚äóq ‚àíq ‚äóp) ,
(4.88)
where a, p and q are again related by (4.66) and (4.67). With the aid of the
abbreviation

96
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
w = œâa1 = œâq √ó p
(4.89)
and bearing (1.130) in mind we Ô¨Ånally arrive at the representation (1.64)
W = ÀÜw.
(4.90)
Thus, every skew-symmetric tensor in three-dimensional space describes a
cross product by its eigenvector w (4.89) corresponding to the zero eigenvalue.
The vector w is called in this case the axial vector of the skew-symmetric
tensor W.
4.7 Cayley-Hamilton Theorem
Theorem 4.7. Let pA (Œª) be the characteristic polynomial of a second-order
tensor A ‚ààLinn. Then,
pA (A) =
n
	
k=0
(‚àí1)k I(k)
A An‚àík = 0.
(4.91)
Proof. As a proof (see, e.g., [11]) we show that
pA (A) x = 0,
‚àÄx ‚ààEn.
(4.92)
For x = 0 it is trivial, so we suppose that x Ã∏= 0. Consider the vectors
yi = Ai‚àí1x,
i = 1, 2, . . ..
(4.93)
Obviously, there is an integer number k such that the vectors y1, y2, . . . , yk
are linearly independent, but
a1y1 + a2y2 + . . . + akyk + Akx = 0.
(4.94)
Note that 1 ‚â§k ‚â§n. If k Ã∏= n we can complete the vectors yi (i = 1, 2, . . . , k)
to a basis yi (i = 1, 2, . . . , n) of En. Let A = Ai
¬∑jyi ‚äóyj, where the vectors yi
form the basis dual to yi (i = 1, 2, . . . , n). By virtue of (4.93) and (4.94) we
can write
Ayi =
‚éß
‚é™
‚é®
‚é™
‚é©
yi+1
if i < k,
‚àí
k(
j=1
ajyj
if i = k.
(4.95)
The components of A can thus be given by

Ai
¬∑j

=

yiAyj

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
0 0 . . . 0 ‚àía1
1 0 . . . 0 ‚àía2
... ...
...
...
A‚Ä≤
0 0 . . . 1 ‚àíak
0
A‚Ä≤‚Ä≤
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,
(4.96)

4.7 Cayley-Hamilton Theorem
97
where A‚Ä≤ and A‚Ä≤‚Ä≤ denote some submatrices. Therefore, the characteristic poly-
nomial of A takes the form
pA (Œª) = pA‚Ä≤‚Ä≤ (Œª)

‚àíŒª
0 . . . 0
‚àía1
1 ‚àíŒª . . . 0
‚àía2
...
...
...
...
0
0 . . . 1 ‚àíak ‚àíŒª

,
(4.97)
where pA‚Ä≤‚Ä≤ (Œª) = det (A‚Ä≤‚Ä≤ ‚àíŒªI). By means of the Laplace expansion rule (see,
e.g., [5]) we expand the determinant in (4.97) along the last column, which
yields
pA (Œª) = pA‚Ä≤‚Ä≤ (Œª) (‚àí1)k 
a1 + a2Œª + . . . + akŒªk‚àí1 + Œªk
.
(4.98)
Bearing (4.93) and (4.94) in mind we Ô¨Ånally prove (4.92) by
pA (A) x = (‚àí1)k pA‚Ä≤‚Ä≤ (A)

a1I + a2A + . . . + akAk‚àí1 + Ak
x
= (‚àí1)k pA‚Ä≤‚Ä≤ (A)

a1x + a2Ax + . . . + akAk‚àí1x + Akx

= (‚àí1)k pA‚Ä≤‚Ä≤ (A) (a1y1 + a2y2 + . . . + akyk + yk+1) = 0.
Exercises
4.1. Evaluate eigenvalues and eigenvectors of the right Cauchy-Green tensor
C = FTF in the case of simple shear, where F is deÔ¨Åned by (4.23).
4.2. Prove identity (4.29)3 using Newton‚Äôs formula (4.26).
4.3. Prove that eigenvectors a(k)
i
(i = 1, 2, . . ., s; k = 1, 2, . . . , ti) of a second
order tensor A ‚ààLinn are linearly independent and form a basis of Cn if for
every eigenvalue the algebraic and geometric multiplicities coincide so that
ri = ti (i = 1, 2, . . ., s).
4.4. Prove identity (4.44) using (4.40) and (4.42).
4.5. Prove identity (4.46) taking (4.40) and (4.42) into account and using the
results of Exercise 4.3.
4.6. Prove the identity det [exp (A)] = exp (trA).
4.7. Verify the Sylvester formula for s = 3 by inserting (4.43) and (4.46) into
(4.54).

98
4 Eigenvalue Problem and Spectral Decomposition of Second-Order Tensors
4.8. Calculate eigenvalues and eigenprojections of the tensor A = Ai
jei ‚äóej,
where

Ai
j

=
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶.
Apply the Cardano formula (4.31) and Sylvester formula (4.54).
4.9. Calculate the exponential of the tensor A given in Exercise 4.8 using the
spectral representation in terms of eigenprojections (4.43).
4.10. Calculate eigenvectors of the tensor A deÔ¨Åned in Exercise 4.8. Express
eigenprojections by (4.42) and compare the results with those obtained by the
Sylvester formula (Exercise 4.8).
4.11. Let ci (i = 1, 2, . . . , m) ‚ààCn be a set of linearly independent complex
vectors. Using the (Gram-Schmidt) procedure described in Chap. 1 (Sect. 1.4),
construct linear combinations of these vectors, say ai (i = 1, 2, . . ., m), again
linearly independent, in such a way that ai ¬∑ aj = Œ¥ij (i, j = 1, 2, . . ., m).
4.12. Let a(k)
i
(k = 1, 2, . . . , ti) be all linearly independent right eigenvectors
of an orthogonal tensor associated with a complex (deÔ¨Ånitely not real) eigen-
value Œªi. Show that a(k)
i
¬∑ a(l)
i
= 0 (k, l = 1, 2, . . . , ti).
4.13. Evaluate principal invariants of a skew-symmetric tensor in three-
dimensional space using (4.29).
4.14. Evaluate eigenvalues, eigenvectors and eigenprojections of the tensor
describing the rotation by the angle Œ± about the axis e3 (see Exercise 1.22).
4.15. Verify the Cayley-Hamilton theorem for the tensor A deÔ¨Åned in Exercise
4.8.
4.16. Verify the Cayley-Hamilton theorem for the deformation gradient in the
case of simple shear (4.23).

5
Fourth-Order Tensors
5.1 Fourth-Order Tensors as a Linear Mapping
Fourth-order tensors play an important role in continuum mechanics where
they appear as elasticity and compliance tensors. In this section we deÔ¨Åne
fourth-order tensors and learn some basic operations with them. To this end,
we consider a set Linn of all linear mappings of one second-order tensor into
another one within Linn. Such mappings are denoted by a colon as
Y = A : X,
A ‚ààLinn, Y ‚ààLinn, ‚àÄX ‚ààLinn.
(5.1)
The elements of Linn are called fourth-order tensors.
Example. Elasticity and compliance tensors. A constitutive law of a
linearly elastic material establishes a linear relationship between the Cauchy
stress tensor œÉ and Cauchy strain tensor œµ. Since these tensors are of the
second-order a linear relation between them can be expressed by fourth-order
tensors like
œÉ = C : œµ
or
œµ = H : œÉ.
(5.2)
The fourth-order tensors C and H describe properties of the elastic material
and are called the elasticity and compliance tensor, respectively.
Linearity of the mapping (5.1) implies that
A : (X + Y) = A : X + A : Y,
(5.3)
A : (Œ±X) = Œ± (A : X) ,
‚àÄX, Y ‚ààLinn, ‚àÄŒ± ‚ààR, A ‚ààLinn.
(5.4)
Similarly to second-order tensors one deÔ¨Ånes the product of a fourth-order
tensor with a scalar
(Œ±A) : X = Œ± (A : X) = A : (Œ±X)
(5.5)

100
5 Fourth-Order Tensors
and the sum of two fourth-order tensors by
(A + B) : X = A : X + B : X,
‚àÄX ‚ààLinn.
(5.6)
Further, we deÔ¨Åne the zero-tensor O of the fourth-order by
O : X = 0,
‚àÄX ‚ààLinn.
(5.7)
Thus, summarizing the properties of fourth-order tensors one can write simi-
larly to second-order tensors
A + B = B + A,
(addition is commutative),
(5.8)
A + (B + C) = (A + B) + C,
(addition is associative),
(5.9)
O + A = A,
(5.10)
A + (‚àíA) = O,
(5.11)
Œ± (Œ≤A) = (Œ±Œ≤) A,
(multiplication by scalars is associative),
(5.12)
1A = A,
(5.13)
Œ± (A + B) = Œ±A + Œ±B,
(multiplication by scalars is distributive
with respect to tensor addition),
(5.14)
(Œ± + Œ≤) A = Œ±A + Œ≤A,
(multiplication by scalars is distributive
with respect to scalar addition),
‚àÄA, B, C ‚ààLinn, ‚àÄŒ±, Œ≤ ‚ààR.
(5.15)
Thus, the set of fourth-order tensors Linn forms a vector space.
On the basis of the ‚Äúright‚Äù mapping (5.1) and the scalar product of two
second-order tensors (1.136) we can also deÔ¨Åne the ‚Äúleft‚Äù mapping by
(Y : A) : X = Y : (A : X) ,
‚àÄX, Y ‚ààLinn.
(5.16)
5.2 Tensor Products, Representation of Fourth-Order
Tensors with Respect to a Basis
For the construction of fourth-order tensors from second-order ones we intro-
duce two tensor products as follows
A‚äóB : X = AXB,
A‚äôB : X = A (B : X) ,
‚àÄA, B, X ‚ààLinn. (5.17)
Note, that the tensor product ‚Äú‚äó‚Äù (5.17)1 applied to second-order tensors
diÔ¨Äers from the tensor product of vectors (1.75). One can easily show that the
mappings described by (5.17) are linear and therefore represent fourth-order
tensors. Indeed, we have, for example, for the tensor product ‚Äú‚äó‚Äù (5.17)1

5.2 Tensor Products, Representation of Fourth-Order Tensors
101
A ‚äóB : (X + Y) = A (X + Y) B
= AXB + AYB = A ‚äóB : X + A ‚äóB : Y,
(5.18)
A ‚äóB : (Œ±X) = A (Œ±X) B = Œ± (AXB)
= Œ± (A ‚äóB : X) ,
‚àÄX, Y ‚ààLinn, ‚àÄŒ± ‚ààR.
(5.19)
With deÔ¨Ånitions (5.17) in hand one can easily prove the following identities
A ‚äó(B + C) = A ‚äóB + A ‚äóC,
(B + C) ‚äóA = B ‚äóA + C ‚äóA, (5.20)
A ‚äô(B + C) = A ‚äôB + A ‚äôC,
(B + C) ‚äôA = B ‚äôA + C ‚äôA. (5.21)
For the left mapping (5.16) the tensor products (5.17) yield
Y : A ‚äóB = ATYBT,
Y : A ‚äôB = (Y : A) B.
(5.22)
As fourth-order tensors represent vectors they can be given with respect to a
basis in Linn.
Theorem 5.1. Let F = {F1, F2, . . . , Fn2} and G = {G1, G2, . . . , Gn2} be two
arbitrary (not necessarily distinct) bases of Linn. Then, fourth-order tensors
Fi ‚äôGj

i, j = 1, 2, . . . , n2
form a basis of Linn. The dimension of Linn is
thus n4.
Proof. See the proof of Theorem 1.6.
A basis in Linn can be represented in another way as by the tensors
Fi ‚äôGj

i, j = 1, 2, . . . , n2
. To this end, we prove the following identity
(a ‚äód) ‚äô(b ‚äóc) = a ‚äób ‚äóc ‚äód,
(5.23)
where we set
(a ‚äób) ‚äó(c ‚äód) = a ‚äób ‚äóc ‚äód.
(5.24)
Indeed, let X ‚ààLinn be an arbitrary second-order tensor. Then, in view of
(1.135) and (5.17)2
(a ‚äód) ‚äô(b ‚äóc) : X = (bXc) (a ‚äód) .
(5.25)
For the right hand side of (5.23) we obtain the same result using (5.17)1 and
(5.24)
a ‚äób ‚äóc ‚äód : X = (a ‚äób) ‚äó(c ‚äód) : X = (bXc) (a ‚äód) .
(5.26)
For the left mapping (5.16) it thus holds
Y : a ‚äób ‚äóc ‚äód = (aYd) (b ‚äóc) .
(5.27)
Now, we are in a position to prove the following theorem.

102
5 Fourth-Order Tensors
Theorem 5.2. Let E = {e1, e2, . . . , en}, F
= {f 1, f 2, . . . , f n}, G =
{g1, g2, . . . , gn} and Ô¨Ånally H = {h1, h2, . . . , hn} be four arbitrary (not nec-
essarily distinct) bases of En. Then, fourth-order tensors ei ‚äóf j ‚äógk ‚äóhl
(i, j, k, l = 1, 2, . . ., n) represent a basis of Linn.
Proof. In view of (5.23)
ei ‚äóf j ‚äógk ‚äóhl = (ei ‚äóhl) ‚äô(f j ‚äógk) .
According to Theorem 1.6 the second-order tensors ei ‚äóhl (i, l = 1, 2, . . . , n)
on the one hand and f j‚äógk (j, k = 1, 2, . . ., n) on the other hand form bases of
Linn. According to Theorem 5.1 the fourth-order tensors (ei ‚äóhl)‚äô(f j ‚äógk)
and consequently ei‚äóf j ‚äógk‚äóhl (i, j, k, l = 1, 2, . . ., n) represent thus a basis
of Linn.
As a result of this Theorem any fourth-order tensor can be represented by
A = Aijklgi ‚äógj ‚äógk ‚äógl = Aijklgi ‚äógj ‚äógk ‚äógl
= Aij
¬∑ ¬∑klgi ‚äógj ‚äógk ‚äógl = . . .
(5.28)
The components of A appearing in (5.28) can be expressed by
Aijkl = gi ‚äógl : A : gj ‚äógk,
Aijkl = gi ‚äógl : A : gj ‚äógk,
Aij
¬∑ ¬∑kl = gi ‚äógl : A : gj ‚äógk,
i, j, k, l = 1, 2, . . ., n.
(5.29)
By virtue of (1.104), (5.17)1 and (5.22)1 the right and left mappings with a
second-order tensor (5.1) and (5.16) can thus be represented by
A : X =

Aijklgi ‚äógj ‚äógk ‚äógl

: (Xqpgq ‚äógp) = AijklXjkgi ‚äógl,
X : A = (Xqpgq ‚äógp) :

Aijklgi ‚äógj ‚äógk ‚äógl

= AijklXilgj ‚äógk.
(5.30)
We observe that the basis vectors of the second-order tensor are scalarly mul-
tiplied either by the ‚Äúinner‚Äù (right mapping) or ‚Äúouter‚Äù (left mapping) basis
vectors of the fourth-order tensor.
5.3 Special Operations with Fourth-Order Tensors
Similarly to second-order tensors one deÔ¨Ånes also for fourth-order tensors some
speciÔ¨Åc operations which are not generally applicable to conventional vectors
in the Euclidean space.
Composition. In analogy with second-order tensors we deÔ¨Åne the com-
position of two fourth-order tensors A and B denoted by A : B as

5.3 Special Operations with Fourth-Order Tensors
103
(A : B) : X = A : (B : X) ,
‚àÄX ‚ààLinn.
(5.31)
For the left mapping (5.16) one can thus write
Y : (A : B) = (Y : A) : B,
‚àÄY ‚ààLinn.
(5.32)
For the tensor products (5.17) the composition (5.31) further yields
(A ‚äóB) : (C ‚äóD) = (AC) ‚äó(DB) ,
(5.33)
(A ‚äóB) : (C ‚äôD) = (ACB) ‚äôD,
(5.34)
(A ‚äôB) : (C ‚äóD) = A ‚äô

CTBDT
,
(5.35)
(A ‚äôB) : (C ‚äôD) = (B : C) A ‚äôD,
A, B, C, D ‚ààLinn.
(5.36)
For example, the identity (5.33) can be proved within the following steps
(A ‚äóB) : (C ‚äóD) : X = (A ‚äóB) : (CXD)
= ACXDB = (AC) ‚äó(DB) : X,
‚àÄX ‚ààLinn,
where we again take into account the deÔ¨Ånition of the tensor product (5.17).
For the component representation (5.28) we further obtain
A : B =

Aijklgi ‚äógj ‚äógk ‚äógl

:

Bpqrtgp ‚äógq ‚äógr ‚äógt
= AijklBjqrkgi ‚äógq ‚äógr ‚äógl.
(5.37)
Note that the ‚Äúinner‚Äù basis vectors of the left tensor A are scalarly multiplied
with the ‚Äúouter‚Äù basis vectors of the right tensor B.
The composition of fourth-order tensors also gives rise to the deÔ¨Ånition of
powers as
Ak = A : A : . . . : A



k times
,
k = 1, 2, . . . ,
A0 = I,
(5.38)
where I stands for the fourth-order identity tensor to be deÔ¨Åned in the next
section. By means of (5.33) and (5.36) powers of tensor products (5.17) take
the following form
(A ‚äóB)k = Ak‚äóBk, (A ‚äôB)k = (A : B)k‚àí1 A‚äôB, k = 1, 2, . . . (5.39)
Simple composition with second-order tensors. Let D be a fourth-
order tensor and A, B two second-order tensors. One deÔ¨Ånes a fourth-order
tensor ADB by
(ADB) : X = A (D : X) B,
‚àÄX ‚ààLinn.
(5.40)
Thus, we can also write
ADB = (A ‚äóB) : D.
(5.41)

104
5 Fourth-Order Tensors
This operation is very useful for the formulation of tensor diÔ¨Äerentiation rules
to be discussed in the next chapter.
For the tensor products (5.17) we further obtain
A (B ‚äóC) D = (AB) ‚äó(CD) = (A ‚äóD) : (B ‚äóC) ,
(5.42)
A (B ‚äôC) D = (ABD) ‚äôC = (A ‚äóD) : (B ‚äôC) .
(5.43)
With respect to a basis the simple composition can be given by
ADB = (Apqgp ‚äógq)

Dijklgi ‚äógj ‚äógk ‚äógl

(Brsgr ‚äógs)
= ApiDijklBlsgp ‚äógj ‚äógk ‚äógs.
(5.44)
It is seen that expressed in component form the simple composition of second-
order tensors with a fourth-order tensor represents the so-called simple con-
traction of the classical tensor algebra (see, e.g., [41]).
Transposition. In contrast to second-order tensors allowing for the
unique transposition operation one can deÔ¨Åne for fourth-order tensors various
transpositions. We conÔ¨Åne our attention here to the following two operations
(‚Ä¢)T and (‚Ä¢)t deÔ¨Åned by
AT : X = X : A,
At : X = A : XT,
‚àÄX ‚ààLinn.
(5.45)
Thus we can also write
Y : At = (Y : A)T .
(5.46)
Indeed, a scalar product with an arbitrary second order tensor X yields in
view of (1.140) and (5.16)

Y : At
: X = Y :

At : X

= Y :

A : XT
= (Y : A) : XT = (Y : A)T : X,
‚àÄX ‚ààLinn.
Of special importance is also the following symmetrization operation resulting
from the transposition (‚Ä¢)t:
Fs = 1
2

F + Ft
.
(5.47)
In view of (1.146)1, (5.45)2 and (5.46) we thus write
Fs : X = F : symX,
Y : Fs = sym (Y : F) .
(5.48)
Applying the transposition operations to the tensor products (5.17) we have
(A ‚äóB)T = AT ‚äóBT,
(A ‚äôB)T = B ‚äôA,
(5.49)
(A ‚äôB)t = A ‚äôBT,
A, B ‚ààLinn.
(5.50)

5.4 Super-Symmetric Fourth-Order Tensors
105
With the aid of (5.26) and (5.27) we further obtain
(a ‚äób ‚äóc ‚äód)T = b ‚äóa ‚äód ‚äóc,
(5.51)
(a ‚äób ‚äóc ‚äód)t = a ‚äóc ‚äób ‚äód.
(5.52)
It can also easily be proved that
ATT = A,
Att = A,
‚àÄA ‚ààLinn.
(5.53)
Note, however, that the transposition operations (5.45) are not commutative
with each other so that generally DTt Ã∏= DtT.
Applied to the composition of fourth-order tensors these transposition op-
erations yield (Exercise 5.6):
(A : B)T = BT : AT,
(A : B)t = A : Bt.
(5.54)
For the tensor products (5.17) we also obtain the following relations (see
Exercise 5.7)
(A ‚äóB)t : (C ‚äóD) =

ADT
‚äó

CTB
t
,
(5.55)
(A ‚äóB)t : (C ‚äôD) =

ACTB

‚äôD.
(5.56)
Scalar product. Similarly to second-order tensors the scalar product of
fourth-order tensors can be deÔ¨Åned in terms of the basis vectors or tensors.
To this end, let us consider two fourth-order tensors A‚äôB and C‚äôD, where
A, B, C, D ‚ààLinn. Then, we set
(A ‚äôB) :: (C ‚äôD) = (A : C) (B : D) .
(5.57)
As a result of this deÔ¨Ånition we also obtain in view of (1.134) and (5.23)
(a ‚äób ‚äóc ‚äód) :: (e ‚äóf ‚äóg ‚äóh) = (a ¬∑ e) (b ¬∑ f) (c ¬∑ g) (d ¬∑ h) .
(5.58)
For the component representation of fourth-order tensors it Ô¨Ånally yields
A :: B =

Aijklgi ‚äógj ‚äógk ‚äógl

::

Bpqrtgp ‚äógq ‚äógr ‚äógt
= AijklBijkl.
(5.59)
Using the latter relation one can easily prove that the properties of the scalar
product (D.1-D.4) hold for fourth-order tensors as well.
5.4 Super-Symmetric Fourth-Order Tensors
On the basis of the transposition operations one deÔ¨Ånes symmetric and super-
symmetric fourth-order tensors. Accordingly, a fourth-order tensor C is said
to be symmetric if (major symmetry)

106
5 Fourth-Order Tensors
CT = C
(5.60)
and super-symmetric if additionally (minor symmetry)
Ct = C.
(5.61)
In this section we focus on the properties of super-symmetric fourth-order ten-
sors. They constitute a subspace of Linn denoted in the following by Ssymn.
First, we prove that every super-symmetric fourth-order tensor maps an ar-
bitrary (not necessarily symmetric) second-order tensor into a symmetric one
so that
(C : X)T = C : X,
‚àÄC ‚ààSsymn,
‚àÄX ‚ààLinn.
(5.62)
Indeed, in view of (5.45), (5.46), (5.60) and (5.61) we have
(C : X)T =

X : CTT
= (X : C)T = X : Ct = X : C = X : CT = C : X.
Next, we deal with representations of super-symmetric fourth-order tensors
and study the properties of their components. Let F = {F1, F2, . . . , Fn2} be
an arbitrary basis of Linn and F‚Ä≤ =
1
F1, F2, . . . , Fn22
the corresponding
dual basis such that
Fp : Fq = Œ¥q
p,
p, q = 1, 2, . . ., n2.
(5.63)
According to Theorem 5.1 we Ô¨Årst write
C = CpqFp ‚äôFq.
(5.64)
Taking (5.60) into account and in view of (5.49)2 we infer that
Cpq = Cqp,
p Ã∏= q; p, q = 1, 2, . . . , n2.
(5.65)
Mapping (5.64) with the dual tensors Fr further yields
C : Fr = (CpqFp ‚äôFq) : Fr = CprFp,
r = 1, 2, . . ., n2.
(5.66)
Let now Fp = Mp (p = 1, 2, . . ., m) and Fq = Wq‚àím

q = m + 1, . . . , n2
be
bases of Symn and Skewn (Sect. 1.9), respectively, where m = 1
2n (n + 1). In
view of (5.45)2, (5.61), (5.62) and (5.66) we conclude that
Cpr = Crp = 0,
p = 1, 2, . . ., n2; r = m + 1, . . . , n2
(5.67)
and consequently
C =
m
	
p,q=1
CpqMp ‚äôMq,
m = 1
2n (n + 1) .
(5.68)

5.5 Special Fourth-Order Tensors
107
Keeping (5.65) in mind we can also write by analogy with (1.149)
C =
m
	
p=1
CppMp ‚äôMp +
m
	
p,q=1
p>q
Cpq (Mp ‚äôMq + Mq ‚äôMp) .
(5.69)
Therefore, every super-symmetric fourth-order tensor can be represented
with respect to the basis 1
2 (Mp ‚äôMq + Mq ‚äôMp), where Mq ‚ààSymn and
p ‚â•q = 1, 2, . . . , 1
2n (n + 1). Thus, we infer that the dimension of Ssymn is
1
2m (m + 1) = 1
8n2 (n + 1)2 + 1
4n (n + 1). We also observe that Ssymn can be
considered as the set of all linear mappings within Symn.
Applying Theorem 5.2 we can also represent a super-symmetric tensor by
C = Cijklgi‚äógj ‚äógk ‚äógl. In this case, (5.51) and (5.52) require that (Exercise
5.8)
Cijkl = Cjilk = Cikjl = Cljki = Cklij.
(5.70)
Thus, we can also write
C = Cijkl (gi ‚äógl) ‚äô(gj ‚äógk)
= 1
4Cijkl (gi ‚äógl + gl ‚äógi) ‚äô(gj ‚äógk + gk ‚äógj)
= 1
4Cijkl (gj ‚äógk + gk ‚äógj) ‚äô(gi ‚äógl + gl ‚äógi) .
(5.71)
Finally, we brieÔ¨Çy consider the eigenvalue problem for super-symmetric fourth-
order tensors. It is deÔ¨Åned as
C : M = ŒõM,
C ‚ààSsymn,
(5.72)
where Œõ and M ‚ààSymn denote the eigenvalue and the corresponding eigen-
tensor, respectively. The spectral decomposition of C can be given similarly
to symmetric second-order tensors (4.60) by
C =
m
	
p=1
ŒõpMp ‚äôMp,
(5.73)
where again m = 1
2n (n + 1) and
Mp : Mq = Œ¥pq,
p, q = 1, 2, . . ., m.
(5.74)
5.5 Special Fourth-Order Tensors
Identity tensor. The fourth-order identity tensor I is deÔ¨Åned by
I : X = X,
‚àÄX ‚ààLinn.
(5.75)

108
5 Fourth-Order Tensors
It is seen that I is a symmetric (but not super-symmetric) fourth-order tensor
such that IT = I. Indeed,
X : I = X,
‚àÄX ‚ààLinn.
(5.76)
With the aid of (1.86), (5.17)1 or alternatively by using (5.29) the fourth-order
identity tensor can be represented by
I = I ‚äóI = gi ‚äógi ‚äógj ‚äógj.
(5.77)
For the composition with other fourth-order tensors we can also write
I : A = A : I = A,
‚àÄA ‚ààLinn.
(5.78)
Transposition tensor. The transposition of second-order tensors repre-
sents a linear mapping and can therefore be expressed in terms of a fourth-
order tensor. This tensor denoted by T is referred to as the transposition
tensor. Thus,
T : X = XT,
‚àÄX ‚ààLinn.
(5.79)
One can easily show that (Exercise 5.9)
Y : T = YT,
‚àÄY ‚ààLinn.
(5.80)
Hence, the transposition tensor is symmetric such that T = TT. By virtue of
(5.45)2 and (5.75), T can further be expressed in terms of the identity tensor
by
T = It.
(5.81)
Indeed,
It : X = I : XT = XT = T : X,
‚àÄX ‚ààLinn.
Considering (5.52) and (5.77) in (5.81) we thus obtain
T = (I ‚äóI)t = gi ‚äógj ‚äógi ‚äógj.
(5.82)
The transposition tensor can further be characterized by the following iden-
tities (see Exercise 5.10)
A : T = At,
T : A = ATtT,
T : T = I,
‚àÄA ‚ààLinn.
(5.83)
Super-symmetric identity tensor. The identity tensor (5.77) is sym-
metric but not super-symmetric. For this reason, it is useful to deÔ¨Åne a special
identity tensor within Ssymn. This super-symmetric tensor maps every sym-
metric second-order tensor into itself like the identity tensor (5.77). It can be
expressed by

5.5 Special Fourth-Order Tensors
109
Is = 1
2 (I + T) = (I ‚äóI)s .
(5.84)
However, in contrast to the identity tensor I (5.77), the super-symmetric iden-
tity tensor Is (5.84) maps any arbitrary (not necessarily symmetric) second-
order tensor into its symmetric part so that in view of (5.48)
Is : X = X : Is = symX,
‚àÄX ‚ààLinn.
(5.85)
Spherical, deviatoric and trace projection tensors. The spherical
and deviatoric part of a second-order tensor are deÔ¨Åned as a linear mapping
(1.153) and can thus be expressed by
sphA = Psph : A,
devA = Pdev : A,
(5.86)
where the fourth-order tensors Psph and Pdev are called the spherical and
deviatoric projection tensors, respectively. In view of (1.153) they are given
by
Psph = 1
nI ‚äôI,
Pdev = I ‚àí1
nI ‚äôI,
(5.87)
where I ‚äôI represents the so-called trace projection tensor. Indeed,
I ‚äôI : X = ItrX,
‚àÄX ‚ààLinn.
(5.88)
According to (5.49)2 and (5.50), the spherical and trace projection tensors are
super-symmetric. The spherical and deviatoric projection tensors are further-
more characterized by the properties:
Pdev : Pdev = Pdev,
Psph : Psph = Psph,
Pdev : Psph = Psph : Pdev = O.
(5.89)
Example. Elasticity tensor for the generalized Hooke‚Äôs law. The
generalized Hooke‚Äôs law is written as
œÉ = 2Gœµ + Œªtr (œµ) I = 2Gdevœµ +
%
Œª + 2
3G
&
tr (œµ) I,
(5.90)
where G and Œª denote the so-called Lam¬¥e constants. The corresponding super-
symmetric elasticity tensor takes the form
C = 2GIs + ŒªI ‚äôI = 2GPs
dev + (3Œª + 2G) Psph.
(5.91)
Exercises
5.1. Prove relations (5.20) and (5.21).

110
5 Fourth-Order Tensors
5.2. Prove relations (5.22).
5.3. Prove relations (5.42) and (5.43).
5.4. Prove relations (5.49-5.52).
5.5. Prove that ATt Ã∏= AtT for A = a ‚äób ‚äóc ‚äód.
5.6. Prove identities (5.54).
5.7. Verify relations (5.55) and (5.56).
5.8. Prove relations (5.70) for the components of a super-symmetric fourth-
order tensor using (5.51) and (5.52).
5.9. Prove relation (5.80) using (5.16) and (5.79).
5.10. Verify the properties of the transposition tensor (5.83).
5.11. Prove that the fourth-order tensor of the form
C = (M1 ‚äóM2 + M2 ‚äóM1)s
is super-symmetric if M1, M2 ‚ààSymn.
5.12. Calculate eigenvalues and eigentensors of the following super-symmetric
fourth-order tensors for n = 3: (a) Is(5.84), (b) Psph (5.87)1, (c) Ps
dev (5.87)2,
(d) C (5.91).

6
Analysis of Tensor Functions
6.1 Scalar-Valued Isotropic Tensor Functions
Let us consider a real scalar-valued function f (A1, A2, . . . , Al) of second-
order tensors Ak ‚ààLinn (k = 1, 2, . . ., l). The function f is said to be isotropic
if
f

QA1QT, QA2QT, . . . , QAlQT
= f (A1, A2, . . . , Al) ,
‚àÄQ ‚ààOrthn.
(6.1)
Example. Consider the function f (A, B) = tr (AB). Since in view of
(1.129) and (1.144)
f

QAQT, QBQT
= tr

QAQTQBQT
= tr

QABQT
= tr

ABQTQ

= tr (AB) = f (A, B) ,
‚àÄQ ‚ààOrthn,
this function is isotropic according to the deÔ¨Ånition (6.1). In contrast, the func-
tion f (A) = tr (AL), where L denotes a second-order tensor, is not isotropic.
Indeed,
f

QAQT
= tr

QAQTL

Ã∏= tr (AL) .
Scalar-valued isotropic tensor functions are also called isotropic invariants
of the tensors Ak (k = 1, 2, . . ., l). For such a tensor system one can construct,
in principle, an unlimited number of isotropic invariants. However, for every
Ô¨Ånite system of tensors one can Ô¨Ånd a Ô¨Ånite number of isotropic invariants
in terms of which all other isotropic invariants can be expressed (Hilbert‚Äôs
theorem). This system of invariants is called functional basis of the tensors
Ak (k = 1, 2, . . ., l). For one and the same system of tensors there exist many

112
6 Analysis of Tensor Functions
functional bases. A functional basis is called irreducible if none of its elements
can be expressed in a unique form in terms of the remaining invariants.
First, we focus on isotropic functions of one second-order tensor
f

QAQT
= f (A) ,
‚àÄQ ‚ààOrthn,
A ‚ààLinn.
(6.2)
One can show that the principal traces trAk, principal invariants I(k)
A
and
eigenvalues Œªk, (k = 1, 2, . . ., n) of the tensor A represent its isotropic tensor
functions. Indeed, for the principal traces we can write by virtue of (1.144)
tr

QAQTk
= tr
‚éõ
‚éùQAQTQAQT . . . QAQT



k times
‚éû
‚é†= tr

QAkQT
= tr

AkQTQ

= trAk,
‚àÄQ ‚ààOrthn.
(6.3)
The principal invariants are uniquely expressed in terms of the principal traces
by means of Newton‚Äôs formula (4.26), while the eigenvalues are, in turn, de-
Ô¨Åned by the principal invariants as solutions of the characteristic equation
(4.20) with the characteristic polynomial given by (4.18).
Further, we prove that both the eigenvalues Œªk, principal invariants I(k)
M
and principal traces trMk (k = 1, 2, . . ., n) of one symmetric tensor M ‚àà
Symn form its functional bases (see also [45]). To this end, we consider two
arbitrary symmetric second-order tensors M1, M2 ‚ààSymn with the same
eigenvalues. Then, the spectral representation (4.60) takes the form
M1 =
n
	
i=1
Œªini ‚äóni,
M2 =
n
	
i=1
Œªimi ‚äómi,
(6.4)
where according to (4.62) both the eigenvectors ni and mi form orthonormal
bases such that ni ¬∑ nj = Œ¥ij and mi ¬∑ mj = Œ¥ij (i, j = 1, 2, . . ., n). Now, we
consider the orthogonal tensor
Q =
n
	
i=1
mi ‚äóni.
(6.5)
Indeed,
QQT =
) n
	
i=1
mi ‚äóni
* ‚éõ
‚éù
n
	
j=1
nj ‚äómj
‚éû
‚é†
=
n
	
i,j=1
Œ¥ijmi ‚äómj =
n
	
i=1
mi ‚äómi = I.
By use of (1.116), (6.4) and (6.5) we further obtain

6.1 Scalar-Valued Isotropic Tensor Functions
113
QM1QT =
) n
	
i=1
mi ‚äóni
* ‚éõ
‚éù
n
	
j=1
Œªjnj ‚äónj
‚éû
‚é†
) n
	
k=1
nk ‚äómk
*
=
n
	
i,j,k=1
Œ¥ijŒ¥jkŒªjmi ‚äómk =
n
	
i=1
Œªimi ‚äómi = M2.
(6.6)
Hence,
f (M1) = f

QM1QT
= f (M2) .
(6.7)
Thus, f takes the same value for all symmetric tensors with pairwise equal
eigenvalues. This means that an isotropic tensor function of a symmetric
tensor is uniquely deÔ¨Åned in terms of its eigenvalues, principal invariants or
principal traces because the latter ones are, in turn, uniquely deÔ¨Åned by the
eigenvalues according to (4.24) and (4.25). This implies the following repre-
sentations
f (M) =
‚å¢
f

I(1)
M , I(2)
M , . . . , I(n)
M

= ÀÜf (Œª1, Œª2, . . . , Œªn)
= Àúf

trM, trM2, . . . , trMn
,
M ‚ààSymn.
(6.8)
Example. Strain energy function of an isotropic hyperelastic ma-
terial. A material is said to be hyperelastic if it is characterized by the ex-
istence of a strain energy œà deÔ¨Åned as a function, for example, of the right
Cauchy-Green tensor C. For isotropic materials this strain energy function
obeys the condition
œà

QCQT
= œà (C) ,
‚àÄQ ‚ààOrth3.
(6.9)
By means of (6.8) this function can be expressed by
œà (C) =
‚å¢
œà (IC, IIC, IIIC) = ÀÜœà (Œª1, Œª2, Œª3) = Àúœà

trC, trC2, trC3
,
(6.10)
where Œªi denote the so-called principal stretches. They are expressed in
terms of the eigenvalues Œõi (i = 1, 2, 3) of the right Cauchy-Green tensor
C = (3
i=1 ŒõiPi as Œªi = ‚àöŒõi. For example, the strain energy function of
the so-called Mooney-Rivlin material is given in terms of the Ô¨Årst and second
principal invariants by
œà (C) = c1 (IC ‚àí3) + c2 (IIC ‚àí3) ,
(6.11)
where c1 and c2 represent material constants. In contrast, the strain energy
function of the Ogden material [29] is deÔ¨Åned in terms of the principal stretches
by
œà (C) =
m
	
r=1
Œºr
Œ±r
(ŒªŒ±r
1
+ ŒªŒ±r
2
+ ŒªŒ±r
3 ‚àí3) ,
(6.12)

114
6 Analysis of Tensor Functions
where Œºr, Œ±r (r = 1, 2, . . . , m) denote material constants.
For isotropic functions (6.1) of a Ô¨Ånite number l of arbitrary second-order
tensors the functional basis is obtained only for three-dimensional space. In or-
der to represent this basis, the tensor arguments are split according to (1.145)
into a symmetric and a skew-symmetric part respectively as follows:
Mi = symAi = 1
2

Ai + AT
i

,
Wi = skewAi = 1
2

Ai ‚àíAT
i

.
(6.13)
In this manner, every isotropic tensor function can be given in terms of a
Ô¨Ånite number of symmetric tensors Mi ‚ààSym3 (i = 1, 2, . . ., m) and skew-
symmetric tensors Wi ‚ààSkew3 (i = 1, 2, . . ., w) as
f = ÀÜf (M1, M2, . . . , Mm, W1, W2, . . . , Ww) .
(6.14)
An irreducible functional basis of such a system of tensors is proved to be
given by (see [2], [32], [40])
trMi, trM2
i , trM3
i ,
tr (MiMj) , tr

M2
i Mj

, tr

MiM2
j

, tr

M2
i M2
j

, tr (MiMjMk) ,
trW2
p, tr (WpWq) , tr (WpWqWr) ,
tr

MiW2
p

, tr

M2
i W2
p

, tr

M2
i W2
pMiWp

, tr (MiWpWq) ,
tr

MiW2
pWq

, tr

MiWpW2
q

, tr (MiMjWp) ,
tr

MiW2
pMjWp

, tr

M2
i MjWp

, tr

MiM2
jWp

,
i < j < k = 1, 2, . . . , m,
p < q < r = 1, 2, . . ., w.
(6.15)
For illustration of this result we consider some examples.
Example 1. Functional basis of one skew-symmetric second-order tensor
W ‚ààSkew3. With the aid of (6.15) and (4.87) we obtain the basis consisting
of only one invariant
trW2 = ‚àí2IIW = ‚àí‚à•W‚à•2 .
(6.16)
Example 2. Functional basis of an arbitrary second-order tensor A ‚àà
Lin3. By means of (6.15) one can write the following functional basis of A
trM, trM2, trM3,
trW2, tr

MW2
, tr

M2W2
, tr

M2W2MW

,
(6.17)
where M = symA and W = skewA. Inserting representations (6.13) into
(6.17) the functional basis of A can be rewritten as (see Exercise 6.2)

6.2 Scalar-Valued Anisotropic Tensor Functions
115
trA, trA2, trA3, tr

AAT
, tr

AAT2 , tr

A2AT
,
tr

AT2 A2ATA ‚àíA2 
AT2 AAT
.
(6.18)
Example 3. Functional basis of two symmetric second-order tensors
M1, M2 ‚ààSym3. According to (6.15) the functional basis includes in this
case the following ten invariants
trM1, trM2
1, trM3
1, trM2, trM2
2, trM3
2,
tr (M1M2) , tr

M2
1M2

, tr

M1M2
2

, tr

M2
1M2
2

.
(6.19)
6.2 Scalar-Valued Anisotropic Tensor Functions
A real scalar-valued function f (A1, A2, . . . , Al) of second-order tensors Ak ‚àà
Linn (k = 1, 2, . . . , l) is said to be anisotropic if it is invariant only with respect
to a subset of all orthogonal transformations:
f

QA1QT, QA2QT, . . . , QAlQT
= f (A1, A2, . . . , Al) ,
‚àÄQ ‚ààSorthn ‚äÇOrthn.
(6.20)
The subset Sorthn represents a group called symmetry group. In continuum
mechanics, anisotropic properties of materials are characterized by their sym-
metry group. The largest symmetry group Orth3 (in three-dimensional space)
includes all orthogonal transformations and is referred to as isotropic. In con-
trast, the smallest symmetry group consists of only two elements I and ‚àíI
and is called triclinic.
Example. Transversely isotropic material symmetry. In this case
the material is characterized by symmetry with respect to one selected direc-
tion referred to as principal material direction. Properties of a transversely
isotropic material remain unchanged by rotations about, and reÔ¨Çections from
the planes orthogonal or parallel to, this direction. Introducing a unit vector
l in the principal direction we can write
Ql = ¬±l,
‚àÄQ ‚ààgt,
(6.21)
where gt ‚äÇOrth3 denotes the transversely isotropic symmetry group. With
the aid of a special tensor
L = l ‚äól,
(6.22)
called structural tensor, condition (6.21) can be represented as
QLQT = L,
‚àÄQ ‚ààgt.
(6.23)

116
6 Analysis of Tensor Functions
Hence, the transversely isotropic symmetry group can be deÔ¨Åned by
gt =

Q ‚ààOrth3 : QLQT = L

.
(6.24)
A strain energy function œà of a transversely isotropic material is invariant with
respect to all orthogonal transformations within gt. Using a representation in
terms of the right Cauchy-Green tensor C this leads to the following condition:
œà

QCQT
= œà (C) ,
‚àÄQ ‚ààgt.
(6.25)
It can be shown that this condition is a priori satisÔ¨Åed if the strain energy
function can be represented as an isotropic function of both C and L so that
ÀÜœà

QCQT, QLQT
= ÀÜœà (C, L) ,
‚àÄQ ‚ààOrth3.
(6.26)
Indeed,
ÀÜœà (C, L) = ÀÜœà

QCQT, QLQT
= ÀÜœà

QCQT, L

,
‚àÄQ ‚ààgt.
(6.27)
With the aid of the functional basis (6.19) and taking into account the iden-
tities
Lk = L,
trLk = 1,
k = 1, 2, . . .
(6.28)
resulting from (6.22) we can thus represent the transversely isotropic function
in terms of the Ô¨Åve invariants by (see also [42])
œà = ÀÜœà (C, L) = Àúœà

trC, trC2, trC3, tr (CL) , tr

C2L

.
(6.29)
The above procedure can be generalized for an arbitrary anisotropic sym-
metry group g. Let Li (i = 1, 2, . . ., m) be a set of second-order tensors which
uniquely deÔ¨Åne g by
g =

Q ‚ààOrthn : QLiQT = Li, i = 1, 2, . . . , m

.
(6.30)
In continuum mechanics the tensors Li are called structural tensors since they
lay down the material or structural symmetry.
It is seen that the isotropic tensor function
f

QAiQT, QLjQT
= f (Ai, Lj) ,
‚àÄQ ‚ààOrthn,
(6.31)
where we use the abbreviated notation
f (Ai, Lj) = f (A1, A2, . . . , Al, L1, L2, . . . , Lm) ,
(6.32)
is anisotropic with respect to the arguments Ai (i = 1, 2, . . ., l) so that

6.2 Scalar-Valued Anisotropic Tensor Functions
117
f

QAiQT
= f (Ai) ,
‚àÄQ ‚ààg.
(6.33)
Indeed, by virtue of (6.30) and (6.31) we have
f (Ai, Lj) = f

QAiQT, QLjQT
= f

QAiQT, Lj

,
‚àÄQ ‚ààg.
(6.34)
Thus, every isotropic invariant of the tensor system Ai (i = 1, 2, . . . , l), Lj
(j = 1, 2, . . ., m) represents an anisotropic invariant of the tensors Ai (i =
1, 2, . . . , l) in the sense of deÔ¨Ånition (6.20). Conversely, one can show that
for every anisotropic function (6.33) there exists an equivalent isotropic func-
tion of the tensor system Ai (i = 1, 2, . . . , l) , Lj (j = 1, 2, . . ., m). In order to
prove this statement we consider a new tensor function deÔ¨Åned by
ÀÜf (Ai, Xj) = f

Q‚Ä≤AiQ‚Ä≤T
,
(6.35)
where the tensor Q‚Ä≤ ‚ààOrthn results from the condition:
Q‚Ä≤XjQ‚Ä≤T = Lj,
j = 1, 2, . . ., m.
(6.36)
Thus, the function ÀÜf is deÔ¨Åned only over such tensors Xj that can be obtained
from the structural tensors Lj (j = 1, 2, . . ., m) by the transformation
Xj = Q‚Ä≤TLjQ‚Ä≤,
j = 1, 2, . . ., m,
(6.37)
where Q‚Ä≤ is an arbitrary orthogonal tensor.
Further, one can show that the so-deÔ¨Åned function (6.35) is isotropic.
Indeed,
ÀÜf

QAiQT, QXjQT
= f

Q‚Ä≤‚Ä≤QAiQTQ‚Ä≤‚Ä≤T
,
‚àÄQ ‚ààOrthn,
(6.38)
where according to (6.36)
Q‚Ä≤‚Ä≤QXjQTQ‚Ä≤‚Ä≤T = Lj,
Q‚Ä≤‚Ä≤ ‚ààOrthn.
(6.39)
Inserting (6.37) into (6.39) yields
Q‚Ä≤‚Ä≤QQ‚Ä≤TLjQ‚Ä≤QTQ‚Ä≤‚Ä≤T = Lj,
(6.40)
so that
Q‚àó= Q‚Ä≤‚Ä≤QQ‚Ä≤T ‚ààg.
(6.41)
Hence, we can write
f

Q‚Ä≤‚Ä≤QAiQTQ‚Ä≤‚Ä≤T
= f

Q‚àóQ‚Ä≤AiQ‚Ä≤TQ‚àóT
= f

Q‚Ä≤AiQ‚Ä≤T
= ÀÜf (Ai, Xj)
and consequently in view of (6.38)
ÀÜf

QAiQT, QXjQT
= ÀÜf (Ai, Xj) ,
‚àÄQ ‚ààOrthn.
(6.42)
Thus, we have proved the following theorem [49].

118
6 Analysis of Tensor Functions
Theorem 6.1. A scalar-valued function f (Ai) is invariant within the sym-
metry group g deÔ¨Åned by (6.30) if and only if there exists an isotropic function
ÀÜf (Ai, Lj) such that
f (Ai) = ÀÜf (Ai, Lj) .
(6.43)
6.3 Derivatives of Scalar-Valued Tensor Functions
Let us again consider a scalar-valued tensor function f (A) : Linn ‚ÜíR. This
function is said to be diÔ¨Äerentiable in a neighborhood of A if there exists a
tensor f (A) ,A ‚ààLinn, such that
d
dtf (A + tX)

t=0
= f (A) ,A : X,
‚àÄX ‚ààLinn.
(6.44)
This deÔ¨Ånition implies that the directional derivative (also called Gateaux
derivative)
d
dtf (A + tX)

t=0
exists and is continuous at A. The tensor
f (A) ,A is referred to as the derivative or the gradient of the tensor func-
tion f (A).
In order to obtain a direct expression for f (A) ,A we represent the
tensors A and X in (6.44) with respect to an arbitrary basis, say gi ‚äó
gj (i, j = 1, 2, . . . , n). Then, using the chain rule one can write
d
dtf (A + tX)

t=0
= d
dtf

Ai
¬∑j + tXi
¬∑j

gi ‚äógj
t=0
= ‚àÇf
‚àÇAi
¬∑j
Xi
¬∑j.
Comparing this result with (6.44) yields
f (A) ,A = ‚àÇf
‚àÇAi
¬∑j
gi‚äógj =
‚àÇf
‚àÇAij
gi‚äógj =
‚àÇf
‚àÇAij gi‚äógj =
‚àÇf
‚àÇA j
i¬∑
gi‚äógj. (6.45)
If the function f (A) is deÔ¨Åned not on all linear transformations but only
on a subset Slinn ‚äÇLinn, the directional derivative (6.44) does not, however,
yield a unique result for f (A) ,A. In this context, let us consider for example
scalar-valued functions of symmetric tensors: f (M) : Symn ‚ÜíR. In this case,
the directional derivative (6.44) deÔ¨Ånes f (M) ,M only up to an arbitrary skew-
symmetric component W. Indeed,
f (M) ,M : X = [f (M) ,M +W] : X,
‚àÄW ‚ààSkewn, ‚àÄX ‚ààSymn. (6.46)
In this relation, X is restricted to symmetric tensors because the tensor M+tX
appearing in the directional derivative (6.44) must belong to the deÔ¨Ånition
domain of the function f for all real values of t.

6.3 Derivatives of Scalar-Valued Tensor Functions
119
To avoid this non-uniqueness we will assume that the derivative f (A) ,A
belongs to the same subset Slinn ‚äÇLinn as its argument A ‚ààSlinn. In
particular, for symmetric tensor functions it implies that
f (M) ,M ‚ààSymn
for
M ‚ààSymn.
(6.47)
In order to calculate the derivative of a symmetric tensor function satisfy-
ing the condition (6.47) one can apply the following procedure. First, the
deÔ¨Ånition domain of the function f is notionally extended to all linear trans-
formations Linn. Applying then the directional derivative (6.44) one obtains a
unique result for the tensor f,M which is Ô¨Ånally symmetrized. For the deriva-
tive with respect to a symmetric part (1.146) of a tensor argument this pro-
cedure can be written by
f (A) ,symA = sym [f (A) ,A ] ,
A ‚ààLinn.
(6.48)
The problem with the non-uniqueness appears likewise by using the com-
ponent representation (6.45) for the gradient of symmetric tensor functions.
Indeed, in this case Mij = Mji (i Ã∏= j = 1, 2, . . . , n), so that only n (n + 1) /2
among all n2 components of the tensor argument M ‚ààSymn are independent.
Thus, according to (1.149)
M =
n
	
i=1
Miigi ‚äógi +
n
	
i,j=1
j<i
Mij (gi ‚äógj + gj ‚äógi) ,
M ‚ààSymn.
(6.49)
Hence, instead of (6.45) we obtain
f (M) ,M = 1
2
n
	
i,j=1
j‚â§i
‚àÇf
‚àÇMij

gi ‚äógj + gj ‚äógi
= 1
2
n
	
i,j=1
j‚â§i
‚àÇf
‚àÇMij
(gi ‚äógj + gj ‚äógi) ,
M ‚ààSymn.
(6.50)
It is seen that the derivative is taken here only with respect to the independent
components of the symmetric tensor argument; the resulting tensor is then
symmetrized.
Example 1. Derivative of the quadratic norm ‚à•A‚à•=
‚àö
A : A:
d
dt [(A + tX) : (A + tX)]1/2

t=0
= d
dt

A : A + 2tA : X + t2X : X
1/2

t=0
=
2A : X + 2tX : X
2 [A : A + 2tA : X + t2X : X]1/2

t=0
=
A
‚à•A‚à•: X.

120
6 Analysis of Tensor Functions
Thus,
‚à•A‚à•,A =
A
‚à•A‚à•.
(6.51)
The same result can also be obtained using (6.45). Indeed, let A = Aijgi ‚äógj.
Then,
‚à•A‚à•=
‚àö
A : A =

(Aijgi ‚äógj) : (Aklgk ‚äógl) =

AijAklgikgjl.
Utilizing the identity
‚àÇAij
‚àÇApq
= Œ¥p
i Œ¥q
j ,
i, j, p, q = 1, 2, . . . , n
we further write
‚à•A‚à•,A = ‚àÇ

AijAklgikgjl
‚àÇApq
gp ‚äógq
=
1
2 ‚à•A‚à•

Aklgikgjlgi ‚äógj + Aijgikgjlgk ‚äógl

=
1
2 ‚à•A‚à•2Aklgikgjlgi ‚äógj =
1
‚à•A‚à•Aklgk ‚äógl =
A
‚à•A‚à•.
Example 2. Derivatives of the principal traces trAk (k = 1, 2, . . .):
d
dt

tr (A + tX)k
t=0
= d
dt

(A + tX)k : I

t=0
= d
dt

(A + tX)k
t=0
: I
= d
dt
‚é°
‚é£(A + tX) (A + tX) . . . (A + tX)



k times
‚é§
‚é¶

t=0
: I
= d
dt
,
Ak + t
k‚àí1
	
i=0
AiXAk‚àí1‚àíi + t2 . . .
-
t=0
: I
=
k‚àí1
	
i=0
AiXAk‚àí1‚àíi : I = k

Ak‚àí1T : X.
Thus,

trAk
,A = k

Ak‚àí1T .
(6.52)
In the special case k = 1 we obtain
(trA) ,A = I.
(6.53)

6.3 Derivatives of Scalar-Valued Tensor Functions
121
Example 3. Derivatives of tr

AkL

(k = 1, 2, . . .) with respect to A:
d
dt

(A + tX)k : LT
t=0
= d
dt

(A + tX)k
t=0
: LT
=
k‚àí1
	
i=0
AiXAk‚àí1‚àíi : LT =
k‚àí1
	
i=0

ATi LT 
ATk‚àí1‚àíi : X.
Hence,
tr

AkL

,A =
k‚àí1
	
i=0

AiLAk‚àí1‚àíiT
.
(6.54)
In the special case k = 1 we have
tr (AL) ,A = LT.
(6.55)
It is seen that the derivative of tr

AkL

is not in general symmetric even if
the tensor argument A is. Applying (6.48) we can write in this case
tr

MkL

,M = sym
,k‚àí1
	
i=0

MiLMk‚àí1‚àíiT
-
=
k‚àí1
	
i=0
Mi (symL) Mk‚àí1‚àíi,
(6.56)
where M ‚ààSymn.
Example 4. Derivatives of the principal invariants I(k)
A (k = 1, 2,
. . . , n) of a second-order tensor A ‚ààLinn. By virtue of the representations
(4.26) and using (6.52) we obtain
I(1)
A ,A = (trA) ,A = I,
I(2)
A ,A = 1
2

I(1)
A trA ‚àítrA2
,A = I(1)
A I ‚àíAT,
I(3)
A ,A = 1
3

I(2)
A trA ‚àíI(1)
A trA2 + trA3
,A
= 1
3

trA

I(1)
A I ‚àíAT
+ I(2)
A I ‚àí

trA2
I ‚àí2I(1)
A AT + 3

AT2
=

A2 ‚àíI(1)
A A + I(2)
A I
T
,
. . .
(6.57)
Herein, one can observe the following regularity
I(k)
A ,A =
k‚àí1
	
i=0
(‚àí1)i I(k‚àí1‚àíi)
A

ATi = ‚àíI(k‚àí1)
A
,A AT + I(k‚àí1)
A
I,
(6.58)

122
6 Analysis of Tensor Functions
where we again set I(0)
A = 1. The above identity can be proved by mathematical
induction (see also [7]). Indeed, according to (4.26) and (6.52)
I(k)
A ,A = 1
k
, k
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A
trAi
-
,A
= 1
k
k
	
i=1
(‚àí1)i‚àí1 iI(k‚àíi)
A

ATi‚àí1+ 1
k
k‚àí1
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A
,A trAi. (6.59)
Now, let
Yk+1 =
k
	
i=0
(‚àí1)i I(k‚àíi)
A

ATi = ‚àíI(k)
A ,A AT + I(k)
A I.
(6.60)
Inserting (4.26) and (6.59) into the latter expression (6.60) delivers
Yk+1 = ‚àí1
k
k
	
i=1
(‚àí1)i‚àí1 iI(k‚àíi)
A

ATi ‚àí
,k‚àí1
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A
,A trAi
-
AT
k
+ I
k
, k
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A
trAi
-
.
Adding Yk+1/k to both sides of this equality and using for Yk+1 the Ô¨Årst
expression in (6.60) we further obtain
k + 1
k
Yk+1 = 1
k
k
	
i=1
(‚àí1)i iI(k‚àíi)
A

ATi + 1
k
k
	
i=0
(‚àí1)i I(k‚àíi)
A

ATi
+ 1
k
, k
	
i=1
(‚àí1)i‚àí1 
‚àíI(k‚àíi)
A
,A AT + I(k‚àíi)
A
I

trAi
-
.
Now, let us assume that representation (6.58) holds at least until the number
k. Then, taking (6.59) again into account we can write
k + 1
k
Yk+1 = 1
k
k
	
i=0
(‚àí1)i (i + 1) I(k‚àíi)
A

ATi
+ 1
k
, k
	
i=1
(‚àí1)i‚àí1 I(k+1‚àíi)
A
,A trAi
-
= k + 1
k
I(k+1)
A
,A .
Hence,
Yk+1 = I(k+1)
A
,A ,
which immediately implies that (6.58) holds for k + 1 as well. Thereby, repre-
sentation (6.58) is proved.

6.3 Derivatives of Scalar-Valued Tensor Functions
123
For invertible tensors one can get a simpler representation for the deriva-
tive of the last invariant I(n)
A . This representation results from the Cayley-
Hamilton theorem (4.91) as follows
I(n)
A ,A AT =
,n‚àí1
	
i=0
(‚àí1)i I(n‚àí1‚àíi)
A

ATi
-
AT =
n
	
i=1
(‚àí1)i‚àí1 I(n‚àíi)
A

ATi
=
n
	
i=0
(‚àí1)i‚àí1 I(n‚àíi)
A

ATi + I(n)
A I = I(n)
A I.
Thus,
I(n)
A ,A = I(n)
A A‚àíT,
A ‚ààInvn.
(6.61)
Example 5. Derivatives of the eigenvalues Œªi. First, we show that
simple eigenvalues of a second-order tensor A are diÔ¨Äerentiable. To this end,
we consider the directional derivative (6.44) of an eigenvalue Œª:
d
dtŒª (A + tX)

t=0
.
(6.62)
Herein, Œª (t) represents an implicit function deÔ¨Åned through the characteristic
equation
det (A + tX ‚àíŒªI) = p (Œª, t) = 0.
(6.63)
This equation can be written out in the polynomial form (4.18) with respect
to powers of Œª. The coeÔ¨Écients of this polynomial are principal invariants of
the tensor A + tX. According to the results of the previous example these
invariants are diÔ¨Äerentiable with respect to A + tX and therefore also with
respect to t. For this reason, the function p (Œª, t) is diÔ¨Äerentiable both with
respect to Œª and t. For a simple eigenvalue Œª0 = Œª (0) we can further write
(see also [26])
p (Œª0, 0) = 0,
‚àÇp (Œª, 0)
‚àÇŒª

Œª=Œª0
Ã∏= 0.
(6.64)
According to the implicit function theorem (see, e.g., [5]), the above condition
ensures the diÔ¨Äerentiability of the function Œª (t) at t = 0. Thus, the directional
derivative (6.62) exists and is continuous at A.
In order to represent the derivative Œªi,A we Ô¨Årst consider the spectral
representation (4.43) of the tensor A with pairwise distinct eigenvalues
A =
n
	
i=1
ŒªiPi,
(6.65)

124
6 Analysis of Tensor Functions
where Pi (i = 1, 2, . . . , n) denote the eigenprojections. They can uniquely be
determined from the equation system
Ak =
n
	
i=1
Œªk
i Pi,
k = 0, 1, . . ., n ‚àí1
(6.66)
resulting from (4.47). Applying the Vieta theorem to the tensor Al (l =
1, 2, . . . , n) we further obtain relation (4.25) written as
trAl =
n
	
i=1
Œªl
i,
l = 1, 2, . . ., n.
(6.67)
The derivative of (6.67) with respect to A further yields by virtue of (6.52)
l

ATl‚àí1 = l
n
	
i=1
Œªl‚àí1
i
Œªi,A ,
l = 1, 2, . . . , n
and consequently
Ak =
n
	
i=1
Œªk
i (Œªi,A )T ,
k = 0, 1, . . ., n ‚àí1.
(6.68)
Comparing the linear equation systems (6.66) and (6.68) we notice that
Œªi,A = PT
i .
(6.69)
Finally, the Sylvester formula (4.54) results in the expression
Œªi,A = Œ¥1nI +
n
+
j=1
jÃ∏=i
AT ‚àíŒªjI
Œªi ‚àíŒªj
.
(6.70)
It is seen that the solution (6.70) holds even if the remainder eigenvalues
Œªj (j = 1, 2, . . . , i ‚àí1, i + 1, . . . , n) of the tensor A are not simple. In this case
(6.70) transforms to
Œªi,A = Œ¥1nI +
s
+
j=1
jÃ∏=i
AT ‚àíŒªjI
Œªi ‚àíŒªj
,
(6.71)
where s denotes the number of pairwise distinct eigenvalues Œªi (i = 1, 2, . . . , s).
6.4 Tensor-Valued Isotropic and Anisotropic Tensor
Functions
A tensor-valued function g (A1, A2, . . . , Al) ‚ààLinn of a tensor system Ak ‚àà
Linn (k = 1, 2, . . . , l) is called anisotropic if

6.4 Tensor-Valued Isotropic and Anisotropic Tensor Functions
125
g

QA1QT, QA2QT, . . . , QAlQT
= Qg (A1, A2, . . . , Al) QT,
‚àÄQ ‚ààSorthn ‚äÇOrthn.
(6.72)
For isotropic tensor-valued tensor functions the above identity holds for all
orthogonal transformations so that Sorthn = Orthn.
As a starting point for the discussion of tensor-valued tensor functions we
again consider isotropic functions of one argument. In this case,
g

QAQT
= Qg (A) QT,
‚àÄQ ‚ààOrthn.
(6.73)
For example, one can easily show that the polynomial function (1.108) and
the exponential function (1.109) introduced in Chap. 1 are isotropic. Indeed,
for a tensor polynomial g (A) = (m
k=0 akAk we have (see also Exercise 1.32)
g

QAQT
=
m
	
k=0
ak

QAQTk
=
m
	
k=0
ak
‚éõ
‚éùQAQTQAQT . . . QAQT



k times
‚éû
‚é†
=
m
	
k=0
ak

QAkQT
= Q
) m
	
k=0
akAk
*
QT
= Qg (A) QT,
‚àÄQ ‚ààOrthn.
(6.74)
Of special interest are isotropic functions of a symmetric tensor. First, we
prove that the tensors g (M) and M ‚ààSymn are coaxial i.e. have the eigen-
vectors in common. To this end, we represent M in the spectral form (4.60)
by
M =
n
	
i=1
Œªibi ‚äóbi,
(6.75)
where bi ¬∑ bj = Œ¥ij (i, j = 1, 2, . . . , n). Further, we choose an arbitrary eigen-
vector, say bk, and show that it simultaneously represents an eigenvector of
g (M). Indeed, let
Q = 2bk ‚äóbk ‚àíI = bk ‚äóbk +
n
	
i=1
iÃ∏=k
(‚àí1) bi ‚äóbi
(6.76)
bearing in mind that I = (n
i=1 bi ‚äóbi in accordance with (1.87). The tensor
Q (6.76) is orthogonal since
QQT = (2bk ‚äóbk ‚àíI) (2bk ‚äóbk ‚àíI) = 4bk‚äóbk‚àí2bk‚äóbk‚àí2bk‚äóbk+I = I
and symmetric as well. One of its eigenvalues is equal to 1 while all the other
ones are ‚àí1. Thus, we can write

126
6 Analysis of Tensor Functions
QM = (2bk ‚äóbk ‚àíI) M = 2Œªkbk ‚äóbk ‚àíM = M (2bk ‚äóbk ‚àíI) = MQ
and consequently
QMQT = M.
(6.77)
Since the function g (M) is isotropic
g (M) = g

QMQT
= Qg (M) QT
and therefore
Qg (M) = g (M) Q.
(6.78)
Mapping the vector bk by both sides of this identity yields in view of (6.76)
Qg (M) bk = g (M) bk.
(6.79)
It is seen that the vector g (M) bk is an eigenvector of Q (6.76) associated
with the eigenvalue 1. Since it is the simple eigenvalue
g (M) bk = Œ≥kbk,
(6.80)
where Œ≥k is some real number. Hence, bk represents the right eigenvector of
g (M). Forming the left mapping of bk by (6.78) one can similarly show that
bk is also the left eigenvector of g (M), which implies the symmetry of the
tensor g (M).
Now, we are in a position to prove the following representation theorem
[35], [45].
Theorem 6.2. A tensor-valued tensor function g (M), M ‚ààSymn is isotropic
if and only if it allows the following representation
g (M) = œï0I + œï1M + œï2M2 + . . . + œïn‚àí1Mn‚àí1 =
n‚àí1
	
i=0
œïiMi,
(6.81)
where œïi are isotropic invariants (isotropic scalar functions) of M and can
therefore be expressed as functions of its principal invariants by
œïi =
‚å¢œïi

I(1)
M , I(2)
M , . . . , I(n)
M

,
i = 0, 1, . . ., n ‚àí1.
(6.82)
Proof. We have already proved that the tensors g (M) and M have eigenvec-
tors in common. Thus, according to (6.75)
g (M) =
n
	
i=1
Œ≥ibi ‚äóbi,
(6.83)

6.4 Tensor-Valued Isotropic and Anisotropic Tensor Functions
127
where Œ≥i = Œ≥i (M). Hence (see Exercise 6.1(e)),
g

QMQT
=
n
	
i=1
Œ≥i

QMQT
Q (bi ‚äóbi) QT.
(6.84)
Since the function g (M) is isotropic we have
g

QMQT
= Qg (M) QT
=
n
	
i=1
Œ≥i (M) Q (bi ‚äóbi) QT,
‚àÄQ ‚ààOrthn.
(6.85)
Comparing (6.84) with (6.85) we conclude that
Œ≥i

QMQT
= Œ≥i (M) ,
i = 1, . . . , n,
‚àÄQ ‚ààOrthn.
(6.86)
Thus, the eigenvalues of the tensor g (M) represent isotropic (scalar-valued)
functions of M. Collecting repeated eigenvalues of g (M) we can further
rewrite (6.83) in terms of the eigenprojections Pi (i = 1, 2, . . . , s) by
g (M) =
s
	
i=1
Œ≥iPi,
(6.87)
where s (1 ‚â§s ‚â§n) denotes the number of pairwise distinct eigenvalues of
g (M). Using the representation of the eigenprojections (4.55) based on the
Sylvester formula (4.54) we can write
Pi =
s‚àí1
	
r=0
Œ±(r)
i
(Œª1, Œª2, . . . , Œªs) Mr,
i = 1, 2, . . ., s.
(6.88)
Inserting this result into (6.87) yields the representation (suÔ¨Éciency):
g (M) =
s‚àí1
	
i=0
œïiMi,
(6.89)
where the functions œïi (i = 0, 1, 2, . . ., s ‚àí1) are given according to (6.8)
and (6.86) by (6.82). The necessity is evident. Indeed, the function (6.81)
is isotropic since in view of (6.74)
g

QMQT
=
n‚àí1
	
i=0
œïi

QMQT
QMiQT
= Q
,n‚àí1
	
i=0
œïi (M) Mi
-
QT = Qg (M) QT, ‚àÄQ ‚ààOrthn.(6.90)

128
6 Analysis of Tensor Functions
Example. Constitutive relations for isotropic materials. For iso-
tropic materials the second Piola-KirchhoÔ¨Ästress tensor S represents an
isotropic function of the right Cauchy-Green tensor C so that
S

QCQT
= QS (C) QT,
‚àÄQ ‚ààOrth3.
(6.91)
Thus, according to the representation theorem
S (C) = Œ±0I + Œ±1C + Œ±2C2,
(6.92)
where Œ±i = Œ±i (C) (i = 0, 1, 2) are some scalar-valued isotropic functions of C.
The same result can be obtained for isotropic hyperelastic materials by con-
sidering the representation of the strain energy function (6.10) in the relation
(see, e.g., [29])
S = 2 ‚àÇœà
‚àÇC.
(6.93)
Indeed, using the chain rule of diÔ¨Äerentiation and keeping in mind that the
tensor C is symmetric we obtain by means of (6.52)
S = 2
3
	
k=1
‚àÇÀúœà
‚àÇtrCk
‚àÇtrCk
‚àÇC
= 2
3
	
k=1
k
‚àÇÀúœà
‚àÇtrCk Ck‚àí1,
(6.94)
so that Œ±i (C) = 2 (i + 1) ‚àÇÀúœà/‚àÇtrCi+1 (i = 0, 1, 2).
Let us further consider a linearly elastic material characterized by a linear
stress-strain response. In this case, the relation (6.92) reduces to
S (C) = œï (C) I + cC,
(6.95)
where c is a material constant and œï (C) represents an isotropic scalar-valued
function linear in C. In view of (6.15) this function can be expressed by
œï (C) = a + btrC,
(6.96)
where a and b are again material constants. Assuming that the reference con-
Ô¨Åguration, in which C = I, is stress free, yields a+3b+c = 0 and consequently
S (C) = (‚àíc ‚àí3b + btrC) I + cC = b (trC ‚àí3) I + c (C ‚àíI) .
Introducing further the so-called Green-Lagrange strain tensor deÔ¨Åned by
ÀúE = 1
2 (C ‚àíI)
(6.97)
we Ô¨Ånally obtain
S

ÀúE

= 2b

trÀúE

I + 2cÀúE.
(6.98)

6.4 Tensor-Valued Isotropic and Anisotropic Tensor Functions
129
The material described by the linear constitutive relation (6.98) is referred
to as St.Venant-KirchhoÔ¨Ämaterial. The corresponding material constants 2b
and 2c are called Lam¬¥e constants. The strain energy function resulting in the
constitutive law (6.98) by (6.93) or equivalently by S = ‚àÇœà/‚àÇÀúE is of the form
œà

ÀúE

= btr2 ÀúE + ctrÀúE2.
(6.99)
For isotropic functions of an arbitrary tensor system Ak ‚ààLinn (k =
1, 2, . . . , l) the representations are obtained only for the three-dimensional
space. One again splits tensor arguments into symmetric Mi ‚ààSym3 (i =
1, 2, . . . , m) and skew-symmetric tensors Wj ‚ààSkew3 (j = 1, 2, . . ., w) ac-
cording to (6.13). Then, all isotropic tensor-valued functions of these tensors
can be represented as linear combinations of the following terms (see [32],
[40]), where the coeÔ¨Écients represent scalar-valued isotropic functions of the
same tensor arguments.
Symmetric generators:
I,
Mi,
M2
i ,
MiMj + MjMi,
M2
i Mj + MjM2
i ,
MiM2
j + M2
jMi,
W2
p,
WpWq + WqWp,
W2
pWq ‚àíWqW2
p,
WpW2
q ‚àíW2
qWp,
MiWp ‚àíWpMi,
WpMiWp,
M2
i Wp ‚àíWpM2
i ,
WpMiW2
p ‚àíW2
pMiWp.
(6.100)
Skew-symmetric generators:
Wp,
WpWq ‚àíWqWp,
MiMj ‚àíMjMi,
M2
i Mj ‚àíMjM2
i ,
MiM2
j ‚àíM2
jMi,
MiMjM2
i ‚àíM2
i MjMi,
MjMiM2
j ‚àíM2
jMiMj,
MiMjMk +MjMkMi+MkMiMj ‚àíMjMiMk ‚àíMkMjMi‚àíMiMkMj,
MiWp + WpMi,
MiW2
p ‚àíW2
pMi,
i < j = 1, 2, . . ., m,
p < q = 1, 2, . . ., w.
(6.101)
For anisotropic tensor-valued tensor functions one utilizes the procedure
applied for scalar-valued functions. It is based on the following theorem [49]
(cf. Theorem 6.1).
Theorem 6.3. (Rychlewski‚Äôs theorem) A tensor-valued function g (Ai) is
anisotropic with the symmetry group Sorthn = g deÔ¨Åned by (6.30) if and
only if there exists an isotropic tensor-valued function ÀÜg (Ai, Lj) such that
g (Ai) = ÀÜg (Ai, Lj) .
(6.102)

130
6 Analysis of Tensor Functions
Proof. Let us deÔ¨Åne a new tensor-valued function by
ÀÜg (Ai, Xj) = Q‚Ä≤Tg

Q‚Ä≤AiQ‚Ä≤T
Q‚Ä≤,
(6.103)
where the tensor Q‚Ä≤ ‚ààOrthn results from the condition (6.36). The further
proof is similar to Theorem 6.1 (Exercise 6.12).
Example. Constitutive relations for a transversely isotropic elas-
tic material. For illustration of the above results we construct a general
constitutive equation for an elastic transversely isotropic material. The trans-
versely isotropic material symmetry is deÔ¨Åned by one structural tensor L
(6.22) according to (6.24). The second Piola-KirchhoÔ¨Ästress tensor S is a
transversely isotropic function of the right Cauchy-Green tensor C. According
to Rychlewski‚Äôs theorem S can be represented as an isotropic tensor function
of C and L by
S = S (C, L) ,
(6.104)
such that
S

QCQT, QLQT
= QS (C, L) QT,
‚àÄQ ‚ààOrth3.
(6.105)
This ensures that the condition of the material symmetry is fulÔ¨Ålled a priori
since
S

QCQT, L

= S

QCQT, QLQT
= QS (C, L) QT, ‚àÄQ ‚ààgt. (6.106)
Keeping in mind that S, C and L are symmetric tensors we can write by
virtue of (6.28)1 and (6.100)
S (C, L) = Œ±0I + Œ±1L + Œ±2C
+ Œ±3C2 + Œ±4 (CL + LC) + Œ±5

C2L + LC2
.
(6.107)
The coeÔ¨Écients Œ±i (i = 0, 1, . . ., 5) represent scalar-valued isotropic tensor
functions of C and L so that similar to (6.29)
Œ±i (C, L) = ÀÜŒ±i

trC, trC2, trC3, tr (CL) , tr

C2L

.
(6.108)
For comparison we derive the constitutive equations for a hyperelastic trans-
versely isotropic material. To this end, we utilize the general representation
for the transversely isotropic strain energy function (6.29). By the chain rule
of diÔ¨Äerentiation and with the aid of (6.52) and (6.54) we obtain
S = 2 ‚àÇÀúœà
‚àÇtrCI + 4
‚àÇÀúœà
‚àÇtrC2 C + 6
‚àÇÀúœà
‚àÇtrC3 C2
+2
‚àÇÀúœà
‚àÇtr (CL)L + 2
‚àÇÀúœà
‚àÇtr (C2L) (CL + LC)
(6.109)

6.5 Derivatives of Tensor-Valued Tensor Functions
131
and Ô¨Ånally
S = Œ±0I + Œ±1L + Œ±2C + Œ±3C2 + Œ±4 (CL + LC) .
(6.110)
Comparing (6.107) and (6.110) we observe that the representation for the
hyperelastic transversely isotropic material does not include the last term in
(6.107) with C2L+LC2. Thus, the constitutive equations containing this term
correspond to an elastic but not hyperelastic transversely isotropic material.
The latter material cannot be described by a strain energy function.
6.5 Derivatives of Tensor-Valued Tensor Functions
The derivative of a tensor-valued tensor function can be deÔ¨Åned in a similar
fashion to (6.44). A function g (A) : Linn ‚ÜíLinn is said to be diÔ¨Äerentiable
in a neighborhood of A if there exists a fourth-order tensor g (A) ,A ‚ààLinn
(called the derivative), such that
d
dtg (A + tX)

t=0
= g (A) ,A : X,
‚àÄX ‚ààLinn.
(6.111)
The above deÔ¨Ånition implies that the directional derivative d
dtg (A + tX)

t=0
exists and is continuous at A.
Similarly to (6.45) we can obtain a direct relation for the fourth-order
tensor g (A) ,A. To this end, we represent the tensors A, X and G = g (A)
with respect to an arbitrary basis in Linn, say gi ‚äógj (i, j = 1, 2, . . ., n).
Applying the chain rule of diÔ¨Äerentiation we can write
d
dtg (A + tX)

t=0
= d
dt
1
Gi
¬∑j

Ak
¬∑l + tXk
¬∑l

gk ‚äógl
gi ‚äógj2
t=0
= ‚àÇGi
¬∑j
‚àÇAk
¬∑l
Xk
¬∑lgi ‚äógj.
(6.112)
In view of (5.30)1 and (6.111) this results in the following representations
g,A = ‚àÇGi
¬∑j
‚àÇAk
¬∑l
gi ‚äógk ‚äógl ‚äógj = ‚àÇGi
¬∑j
‚àÇA l
k¬∑
gi ‚äógk ‚äógl ‚äógj
= ‚àÇGi
¬∑j
‚àÇAkl gi ‚äógk ‚äógl ‚äógj = ‚àÇGi
¬∑j
‚àÇAkl
gi ‚äógk ‚äógl ‚äógj.
(6.113)
For functions deÔ¨Åned only on a subset Slinn ‚äÇLinn the directional deriva-
tive (6.111) again does not deliver a unique result. Similarly to scalar-valued
functions this problem can be avoided deÔ¨Åning the fourth-order tensor g (A) ,A

132
6 Analysis of Tensor Functions
as a linear mapping on Slinn. Of special interest in this context are symmet-
ric tensor functions. In this case, using (5.47) and applying the procedure
described in Sect. 6.3 we can write
g (A) ,symA = [g (A) ,A ]s ,
A ‚ààLinn.
(6.114)
The component representation (6.113) can be given for symmetric tensor func-
tions by
g (M) ,M = 1
2
n
	
k,l=1
l‚â§k
‚àÇGi
¬∑j
‚àÇMkl gi ‚äó

gk ‚äógl + gl ‚äógk
‚äógj
= 1
2
n
	
k,l=1
l‚â§k
‚àÇGi
¬∑j
‚àÇMkl
gi ‚äó(gk ‚äógl + gl ‚äógk) ‚äógj,
(6.115)
where M ‚ààSymn.
Example 1. Derivative of the power function Ak (k = 1, 2, . . .). The di-
rectional derivative (6.111) of the power function yields
d
dt (A + tX)k

t=0
= d
dt
)
Ak + t
k‚àí1
	
i=0
AiXAk‚àí1‚àíi + t2 . . .
*
t=0
=
k‚àí1
	
i=0
AiXAk‚àí1‚àíi.
(6.116)
Bearing (5.17)1 and (6.111) in mind we Ô¨Ånally obtain
Ak,A =
k‚àí1
	
i=0
Ai ‚äóAk‚àí1‚àíi,
A ‚ààLinn.
(6.117)
In the special case k = 1 it leads to the identity
A,A = I,
A ‚ààLinn.
(6.118)
For power functions of symmetric tensors application of (6.114) yields
Mk,M =
k‚àí1
	
i=0

Mi ‚äóMk‚àí1‚àíis ,
M ‚ààSymn
(6.119)
and consequently
M,M = Is,
M ‚ààSymn.
(6.120)
Example 2. Derivative of the transposed tensor AT. In this case, we can

6.5 Derivatives of Tensor-Valued Tensor Functions
133
write
d
dt (A + tX)T

t=0
= d
dt

AT + tXT
t=0
= XT.
On use of (5.79) this yields
AT,A = T.
(6.121)
Example 3. Derivative of the inverse tensor A‚àí1, where A ‚ààInvn. Con-
sider the directional derivative of the identity A‚àí1A = I. It delivers:
d
dt (A + tX)‚àí1 (A + tX)

t=0
= 0.
Applying the product rule of diÔ¨Äerentiation (2.9) and using (6.116) we further
write
d
dt (A + tX)‚àí1

t=0
A + A‚àí1X = 0
and Ô¨Ånally
d
dt (A + tX)‚àí1

t=0
= ‚àíA‚àí1XA‚àí1.
Hence, in view of (5.17)1
A‚àí1,A = ‚àíA‚àí1 ‚äóA‚àí1.
(6.122)
The calculation of the derivative of tensor functions can be simpliÔ¨Åed by
means of diÔ¨Äerentiation rules. One of them is the following composition rule.
Let G = g (A) and H = h (A) be two arbitrary diÔ¨Äerentiable tensor-valued
functions of A. Then,
(GH) ,A = G,A H + GH,A .
(6.123)
For the proof we again apply the directional derivative (6.111) taking (2.9)
and (5.40) into account
(GH) ,A : X = d
dt [g (A + tX) h (A + tX)]

t=0
= d
dtg (A + tX)

t=0
H + G d
dth (A + tX)

t=0
= (G,A : X) H + G (H,A : X)
= (G,A H + GH,A ) : X,
‚àÄX ‚ààLinn.

134
6 Analysis of Tensor Functions
Example 4. The right and left Cauchy-Green tensors are given in terms
of the deformation gradient F respectively by
C = FTF,
b = FFT.
(6.124)
Of special interest in continuum mechanics is the derivative of these tensors
with respect to F. With the aid of the product rule (6.123) and using (5.42),
(5.77), (5.82), (5.83)1, (6.118) and (6.121) we obtain
C,F = FT,F F + FTF,F = TF + FTI = (I ‚äóF)t + FT ‚äóI,
(6.125)
b,F = F,F FT + FFT,F = IFT + FT = I ‚äóFT + (F ‚äóI)t .
(6.126)
Further product rules of diÔ¨Äerentiation of tensor functions can be written
as
(fG) ,A = G ‚äôf,A +fG,A ,
(6.127)
(G : H) ,A = H : G,A +G : H,A ,
(6.128)
where f = ÀÜf (A), G = g (A) and H = h (A) are again a scalar-valued and two
tensor-valued diÔ¨Äerentiable tensor functions, respectively. The proof is similar
to (6.123) (see Exercise 6.14).
Example 5. With the aid of the above diÔ¨Äerentiation rules we can eas-
ily express the derivatives of the spherical and deviatoric parts (1.153) of a
second-order tensor by
sphA,A =
" 1
ntr (A) I
#
,A = 1
nI ‚äôI = Psph,
(6.129)
devA,A =
"
A ‚àí1
ntr (A) I
#
,A = I ‚àí1
nI ‚äôI = Pdev.
(6.130)
Example 6. Tangent moduli of hyperelastic isotropic and trans-
versely isotropic materials. The tangent moduli are deÔ¨Åned by (see, e.g.,
[29])
C = ‚àÇS
‚àÇÀúE
= 2 ‚àÇS
‚àÇC,
(6.131)
where ÀúE denotes the Green-Lagrange strain tensor deÔ¨Åned in (6.97). For hy-
perelastic materials this deÔ¨Ånition implies in view of (6.93) the representation
C =
‚àÇ2œà
‚àÇÀúE‚àÇÀúE
= 4 ‚àÇ2œà
‚àÇC‚àÇC.
(6.132)
For a hyperelastic isotropic material we thus obtain by virtue of (6.119),
(6.127), (6.10) or (6.94)

6.6 Generalized Rivlin‚Äôs Identities
135
C = 4
3
	
k,l=1
kl
‚àÇ2 Àúœà
‚àÇtrCk‚àÇtrCl Ck‚àí1 ‚äôCl‚àí1
+8
‚àÇÀúœà
‚àÇtrC2 Is + 12
‚àÇÀúœà
‚àÇtrC3 (C ‚äóI + I ‚äóC)s .
(6.133)
For a hyperelastic transversely isotropic material the above procedure yields
with the aid of (6.109)
C = 4
3
	
k,l=1
kl
‚àÇ2 Àúœà
‚àÇtrCk‚àÇtrCl Ck‚àí1 ‚äôCl‚àí1 + 4
‚àÇ2 Àúœà
‚àÇtr (CL) ‚àÇtr (CL)L ‚äôL
+ 4
‚àÇ2 Àúœà
‚àÇtr (C2L) ‚àÇtr (C2L) (CL + LC) ‚äô(CL + LC)
+ 4
3
	
k
k
‚àÇ2 Àúœà
‚àÇtrCk‚àÇtr (CL)

Ck‚àí1 ‚äôL + L ‚äôCk‚àí1
+ 4
3
	
k
k
‚àÇ2 Àúœà
‚àÇtrCk‚àÇtr (C2L)

Ck‚àí1 ‚äô(CL + LC) + (CL + LC) ‚äôCk‚àí1
+ 4
‚àÇ2 Àúœà
‚àÇtr (CL) ‚àÇtr (C2L) [L ‚äô(CL + LC) + (CL + LC) ‚äôL] + 8 ‚àÇÀúœà
‚àÇtrC2 Is
+ 12 ‚àÇÀúœà
‚àÇtrC3 (C ‚äóI + I ‚äóC)s + 4
‚àÇÀúœà
‚àÇtr (C2L) (L ‚äóI + I ‚äóL)s .
(6.134)
6.6 Generalized Rivlin‚Äôs Identities
The Cayley-Hamilton equation (4.91)
An ‚àíI(1)
A An‚àí1 + I(2)
A An‚àí2 + . . . + (‚àí1)n I(n)
A I = 0
(6.135)
represents a universal relation connecting powers of a second-order tensor
A with its principal invariants. Similar universal relations connecting several
second-order tensors might also be useful for example for the representation of
isotropic tensor functions or for the solution of tensor equations. Such relations
are generally called Rivlin‚Äôs identities.
In order to formulate the Rivlin identities we Ô¨Årst diÔ¨Äerentiate the Cayley-
Hamilton equation (6.135) with respect to A. With the aid of (6.58), (6.117)
and (6.127) we can write

136
6 Analysis of Tensor Functions
O =
, n
	
k=0
(‚àí1)k I(k)
A An‚àík
-
,A
=
n
	
k=1
(‚àí1)k An‚àík ‚äô
, k
	
i=1
(‚àí1)i‚àí1 I(k‚àíi)
A

ATi‚àí1
-
+
n‚àí1
	
k=0
(‚àí1)k I(k)
A
,n‚àík
	
i=1
An‚àík‚àíi ‚äóAi‚àí1
-
.
Substituting in the last row the summation index k + i by k and using (5.42)
and (5.43) we further obtain
n
	
k=1
An‚àík
k
	
i=1
(‚àí1)k‚àíi I(k‚àíi)
A

I ‚äô

ATi‚àí1 ‚àíI ‚äóAi‚àí1
= O.
(6.136)
Mapping an arbitrary second-order tensor B by both sides of this equation
yields an identity written in terms of second-order tensors [10]
n
	
k=1
An‚àík
k
	
i=1
(‚àí1)k‚àíi I(k‚àíi)
A

tr

Ai‚àí1B

I ‚àíBAi‚àí1
= 0.
(6.137)
This relation is referred to as the generalized Rivlin‚Äôs identity. Indeed, in the
special case of three-dimensional space (n = 3) it takes the form
ABA + A2B + BA2 ‚àítr (A) (AB + BA) ‚àítr (B) A2
‚àí[tr (AB) ‚àítrAtrB] A + 1
2

tr2A ‚àítrA2
B
‚àí
!
tr

A2B

‚àítrAtr (AB) + 1
2trB

tr2A ‚àítrA2$
I = 0, (6.138)
originally obtained by Rivlin [34] by means of matrix calculations.
DiÔ¨Äerentiating (6.137) again with respect to A delivers
O =
n‚àí1
	
k=1
n‚àík
	
j=1

An‚àík‚àíj ‚äóAj‚àí1
k
	
i=1
(‚àí1)k‚àíi I(k‚àíi)
A

tr

Ai‚àí1B

I ‚àíBAi‚àí1
+
n
	
k=2
k‚àí1
	
i=1
(‚àí1)k‚àíi An‚àík 
tr

Ai‚àí1B

I ‚àíBAi‚àí1
‚äô
‚é°
‚é£
k‚àíi
	
j=1
(‚àí1)j‚àí1 I(k‚àíi‚àíj)
A

ATj‚àí1
‚é§
‚é¶

6.6 Generalized Rivlin‚Äôs Identities
137
+
n
	
k=2
k
	
i=2
(‚àí1)k‚àíi I(k‚àíi)
A
An‚àík ‚äô
‚é°
‚é£
i‚àí1
	
j=1

Aj‚àí1BAi‚àí1‚àíjT
‚é§
‚é¶
‚àí
n
	
k=2
k
	
i=2
(‚àí1)k‚àíi I(k‚àíi)
A
An‚àíkB
‚é°
‚é£
i‚àí1
	
j=1

Ai‚àíj‚àí1 ‚äóAj‚àí1
‚é§
‚é¶.
Changing the summation indices and summation order we obtain
n‚àí1
	
i=1
n
	
k=i+1
k‚àíi
	
j=1
(‚àí1)k‚àíi‚àíj I(k‚àíi‚àíj)
A
An‚àík 1
I ‚äó

tr

Aj‚àí1B

Ai‚àí1
‚àíAi‚àí1BAj‚àí1
‚àí

tr

Ai‚àí1B

I ‚àíBAi‚àí1
‚äô

ATj‚àí1
+ I ‚äô

Ai‚àí1BAj‚àí1T ‚àíBAj‚àí1 ‚äóAi‚àí12
= O.
(6.139)
The second-order counterpart of this relation can be obtained by mapping
another arbitrary second-order tensor C ‚ààLinn as [10]
n‚àí1
	
i=1
n
	
k=i+1
k‚àíi
	
j=1
(‚àí1)k‚àíi‚àíj I(k‚àíi‚àíj)
A
An‚àík 
tr

Aj‚àí1B

CAi‚àí1
‚àíCAi‚àí1BAj‚àí1 ‚àí

tr

Ai‚àí1B

I ‚àíBAi‚àí1
tr

Aj‚àí1C

+tr

Ai‚àí1BAj‚àí1C

I ‚àíBAj‚àí1CAi‚àí1
= 0.
(6.140)
In the special case of three-dimensional space (n = 3) equation (6.140) leads
to the well-known identity (see [27], [34], [36])
ABC + ACB + BCA + BAC + CAB + CBA ‚àítr (A) (BC + CB)
‚àítr (B) (CA + AC) ‚àítr (C) (AB + BA) + [tr (B) tr (C) ‚àítr (BC)] A
+ [tr (C) tr (A) ‚àítr (CA)] B + [tr (A) tr (B) ‚àítr (AB)] C
‚àí[tr (A) tr (B) tr (C) ‚àítr (A) tr (BC) ‚àítr (B) tr (CA)
‚àítr (C) tr (AB) + tr (ABC) + tr (ACB)] I = 0.
(6.141)
Exercises
6.1. Check isotropy of the following tensor functions:
(a) f (A) = aAb, where a, b ‚ààEn,
(b) f (A) = A11 + A22 + A33,
(c) f (A) = A11 +A12 +A13, where Aij represent the components of A ‚ààLin3

138
6 Analysis of Tensor Functions
with respect to an orthonormal basis ei (i = 1, 2, 3), so that A = Aijei ‚äóej,
(d) f (A) = detA,
(e) f (A) = Œªmax, where Œªmax denotes the maximal (in the sense of the norm
‚àö
ŒªŒª) eigenvalue of A ‚ààLinn.
6.2. Prove the alternative representation (6.18) for the functional basis of an
arbitrary second-order tensor A.
6.3. An orthotropic symmetry group go is described in terms of three struc-
tural tensors deÔ¨Åned by Li = li ‚äóli, where li ¬∑ lj = Œ¥ij (i, j = 1, 2, 3) are unit
vectors along mutually orthogonal principal material directions. Represent the
general orthotropic strain energy function
œà

QCQT
= œà (C) ,
‚àÄQ ‚ààgo
(6.142)
in terms of the orthotropic invariants.
6.4. Using the results of Exercise 6.3, derive the constitutive relation for the
second Piola-KirchhoÔ¨Ästress tensor S (6.93) and the tangent moduli C (6.131)
for the general hyperelastic orthotropic material.
6.5. Represent the general constitutive relation for an orthotropic elastic ma-
terial as a function S (C).
6.6. A symmetry group gf of a Ô¨Åber reinforced material with an isotropic
matrix is described in terms of structural tensors deÔ¨Åned by Li = li ‚äóli,
where the unit vectors li (i = 1, 2, . . ., k) deÔ¨Åne the directions of Ô¨Åber families
and are not necessarily orthogonal to each other. Represent the strain energy
function
œà

QCQT
= œà (C) ,
‚àÄQ ‚ààgf
(6.143)
of a Ô¨Åber reinforced material with two families of Ô¨Åbers (k = 2).
6.7. Derive the constitutive relation S = 2‚àÇœà/‚àÇC + pC‚àí1 and the tangent
moduli C = 2‚àÇS/‚àÇC for the Mooney-Rivlin material represented by the strain
energy function (6.11).
6.8. Derive the constitutive relation for the Ogden model (6.12) in terms of
the second Piola-KirchhoÔ¨Ästress tensor using expression (6.93).
6.9. Show that tr (CLiCLj), where Li (i = 1, 2, 3) are structural tensors de-
Ô¨Åned in Exercise 6.3, represents an orthotropic tensor function (orthotropic
invariant) of C. Express this function in terms of the orthotropic functional
basis obtained in Exercise 6.3.

6.6 Generalized Rivlin‚Äôs Identities
139
6.10. The strain energy function of the orthotropic St.Venant-KirchhoÔ¨Äma-
terial is given by
œà

ÀúE

= 1
2
3
	
i,j=1
aijtr

ÀúELi

tr

ÀúELj

+
3
	
i,j=1
iÃ∏=j
Gijtr

ÀúELi ÀúELj

,
(6.144)
where ÀúE denotes the Green-Lagrange strain tensor (6.97) and Li (i = 1, 2, 3)
are the structural tensors deÔ¨Åned in Exercise 6.3. aij = aji (i, j = 1, 2, 3) and
Gij = Gji (i Ã∏= j = 1, 2, 3) represent material constants. Derive the consti-
tutive relation for the second Piola-KirchhoÔ¨Ästress tensor S (6.93) and the
tangent moduli C (6.131).
6.11. Show that the function œà(ÀúE) (6.144) becomes transversely isotropic if
a22 = a33,
a12 = a13,
G12 = G13,
G23 = 1
2 (a22 ‚àía23)
(6.145)
and isotropic of the form (6.99) if
a12 = a13 = a23 = Œª,
G12 = G13 = G23 = G,
a11 = a22 = a33 = Œª + 2G.
(6.146)
6.12. Complete the proof of Theorem 6.3.
6.13. Express A‚àík,A, where k = 1, 2, . . ..
6.14. Prove the product rules of diÔ¨Äerentiation (6.127) and (6.128).
6.15. Write out Rivlin‚Äôs identity (6.137) for n = 2.

7
Analytic Tensor Functions
7.1 Introduction
In the previous chapter we discussed isotropic and anisotropic tensor func-
tions and their general representations. Of particular interest in continuum
mechanics are isotropic tensor-valued functions of one arbitrary (not neces-
sarily symmetric) tensor. For example, the exponential function of the velocity
gradient or other non-symmetric strain rates is very suitable for the formula-
tion of evolution equations in large strain anisotropic plasticity. In this section
we focus on a special class of isotropic tensor-valued functions referred here
to as analytic tensor functions. In order to specify this class of functions we
Ô¨Årst deal with the general question how an isotropic tensor-valued function
can be deÔ¨Åned.
For isotropic functions of diagonalizable tensors the most natural way is
the spectral decomposition (4.43)
A =
s
	
i=1
ŒªiPi,
(7.1)
so that we may write similarly to (4.48)
g (A) =
s
	
i=1
g (Œªi) Pi,
(7.2)
where g (Œªi) is an arbitrary (not necessarily polynomial) scalar function de-
Ô¨Åned on the spectrum Œªi (i = 1, 2, . . ., s) of the tensor A. Obviously, the so-
deÔ¨Åned function g (A) is isotropic in the sense of the condition (6.73). Indeed,
g

QAQT
=
s
	
i=1
g (Œªi) QPiQT = Qg (A) QT,
‚àÄQ ‚ààOrthn,
(7.3)
where we take into account that the spectral decomposition of the tensor
QAQT is given by

142
7 Analytic Tensor Functions
QAQT =
s
	
i=1
ŒªiQPiQT.
(7.4)
Example. Generalized strain measures. The so-called generalized
strain measures E and e (also known as Hill‚Äôs strains, [15], [16]) play an im-
portant role in kinematics of continuum. They are deÔ¨Åned by (7.2) as isotropic
tensor-valued functions of the symmetric right and left stretch tensor U and
v and are referred to as Lagrangian (material) and Eulerian (spatial) strains,
respectively. The deÔ¨Ånition of the generalized strains is based on the spectral
representations by
U =
s
	
i=1
ŒªiPi,
v =
s
	
i=1
Œªipi,
(7.5)
where Œªi > 0 are the eigenvalues (referred to as principal stretches) while Pi
and pi (i = 1, 2, . . . , s) denote the corresponding eigenprojections. Accord-
ingly,
E =
s
	
i=1
f (Œªi) Pi,
e =
s
	
i=1
f (Œªi) pi,
(7.6)
where f is a strictly-increasing scalar function satisfying the conditions f (1) =
0 and f ‚Ä≤ (1) = 1. A special class of generalized strain measures speciÔ¨Åed by
E(a) =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
s(
i=1
1
a (Œªa
i ‚àí1) Pi
for a Ã∏= 0,
s(
i=1
ln (Œªi) Pi
for a = 0,
(7.7)
e(a) =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
s(
i=1
1
a (Œªa
i ‚àí1) pi
for a Ã∏= 0,
s(
i=1
ln (Œªi) pi
for a = 0
(7.8)
are referred to as Seth‚Äôs strains [39], where a is a real number. For example, the
Green-Lagrange strain tensor (6.97) introduced in Chap. 6 belongs to Seth‚Äôs
strains as E(2).
Since non-symmetric tensors do not generally admit the spectral decompo-
sition in the diagonal form (7.1), it is necessary to search for other approaches
for the deÔ¨Ånition of the isotropic tensor function g (A) : Linn ‚ÜíLinn. One of
these approaches is the tensor power series of the form
g (A) = a0I + a1A + a2A2 + . . . =
‚àû
	
r=0
arAr.
(7.9)

7.1 Introduction
143
Indeed, in view of (6.74)
g

QAQT
=
‚àû
	
r=0
ar

QAQTr
=
‚àû
	
r=0
arQArQT = Qg (A) QT,
‚àÄQ ‚ààOrthn.
(7.10)
For example, the exponential tensor function can be deÔ¨Åned in terms of the
inÔ¨Ånite power series (7.9) by (1.109).
One can show that the power series (7.9), provided it converges, repre-
sents a generalization of (7.2) to arbitrary (and not necessarily diagonalizable)
second-order tensors. Conversely, the isotropic tensor function (7.2) with g (Œª)
analytic on the spectrum of A can be considered as an extension of inÔ¨Ånite
power series (7.9) to its non-convergent domain if the latter exists. Indeed, for
diagonalizable tensor arguments within the convergence domain of the tensor
power series (7.9) both deÔ¨Ånitions coincide. For example, inserting (7.1) into
(7.9) and taking (4.47) into account we have
g (A) =
‚àû
	
r=0
ar
) s
	
i=1
ŒªiPi
*r
=
‚àû
	
r=0
ar
s
	
i=1
Œªr
i Pi =
s
	
i=1
g (Œªi) Pi
(7.11)
with the abbreviation
g (Œª) =
‚àû
	
r=0
arŒªr,
(7.12)
so that
ar = 1
r!
‚àÇrg (Œª)
‚àÇŒªr

Œª=0
.
(7.13)
The above mentioned convergence requirement vastly restricts the deÔ¨Åni-
tion domain of many isotropic tensor functions deÔ¨Åned in terms of inÔ¨Ånite
series (7.9). For example, one can show that the power series for the logarith-
mic tensor function
ln (A + I) =
‚àû
	
r=1
(‚àí1)r+1 Ar
r
(7.14)
converges for |Œªi| < 1 (i = 1, 2, . . . , s) and diverges if |Œªk| > 1 at least for some
k (1 ‚â§k ‚â§s) (see, e.g., [12]).
In order to avoid this convergence problem we consider a tensor function
deÔ¨Åned by the so-called Dunford-Taylor integral as (see, for example, [24])
g (A) =
1
2œÄi
7
Œì
g (Œ∂) (Œ∂I ‚àíA)‚àí1 dŒ∂
(7.15)

144
7 Analytic Tensor Functions
taken on the complex plane over Œì, where Œì represents a closed curve or
consists of simple closed curves, the union interior of which includes all the
eigenvalues Œªi ‚ààC (i = 1, 2, . . ., s) of the tensor argument A. g (Œ∂) : C ‚ÜíC
is an arbitrary scalar function analytic within and on Œì.
One can easily prove that the tensor function (7.15) is isotropic in the
sense of the deÔ¨Ånition (6.73). Indeed, with the aid of (1.127) and (1.128) we
obtain (cf. [33])
g

QAQT
=
1
2œÄi
7
Œì
g (Œ∂)

Œ∂I ‚àíQAQT‚àí1 dŒ∂
=
1
2œÄi
7
Œì
g (Œ∂)

Q (Œ∂I ‚àíA) QT‚àí1 dŒ∂
=
1
2œÄi
7
Œì
g (Œ∂) Q (Œ∂I ‚àíA)‚àí1 QTdŒ∂
= Qg (A) QT,
‚àÄQ ‚ààOrthn.
(7.16)
It can be veriÔ¨Åed that for diagonalizable tensors the Dunford-Taylor integral
(7.15) reduces to the spectral decomposition (7.2) and represents therefore its
generalization. Indeed, inserting (7.1) into (7.15) delivers
g (A) =
1
2œÄi
7
Œì
g (Œ∂)
)
Œ∂I ‚àí
s
	
i=1
ŒªiPi
*‚àí1
dŒ∂
=
1
2œÄi
7
Œì
g (Œ∂)
, s
	
i=1
(Œ∂ ‚àíŒªi) Pi
-‚àí1
dŒ∂
=
1
2œÄi
7
Œì
g (Œ∂)
s
	
i=1
(Œ∂ ‚àíŒªi)‚àí1 PidŒ∂
=
s
	
i=1
" 1
2œÄi
7
Œì
g (Œ∂) (Œ∂ ‚àíŒªi)‚àí1 dŒ∂
#
Pi =
s
	
i=1
g (Œªi) Pi,
(7.17)
where we keep (4.46) in mind and apply the the Cauchy integral formula (see,
e.g. [5]). Using this result we can represent, for example, the generalized strain
measures (7.6) by
E = f (U) ,
e = f (v) ,
(7.18)
where the tensor functions f (U) and f (v) are deÔ¨Åned by (7.15).
Further, one can show that the Dunford-Taylor integral (7.15) also repre-
sents a generalization of tensor power series (7.9). For this purpose, it suÔ¨Éces
to verify that (7.15) based on a scalar function g (Œ∂) = Œ∂k (k = 0, 1, 2, . . .)
results into the monomial g (A) = Ak. To this end, we consider in (7.15) the
following identity [24]

7.2 Closed-Form Representation for Analytic Tensor Functions
145
g (Œ∂) I = (Œ∂I)k = (Œ∂I ‚àíA + A)k = (Œ∂I ‚àíA)k + . . . + Ak.
(7.19)
Thereby, all terms except of the last one have no pole within Œì and vanish
according to the Cauchy theorem (see, e.g., [5]), so that
g (A) =
1
2œÄi
7
Œì

(Œ∂I ‚àíA)k‚àí1 + . . . + Ak (Œ∂I ‚àíA)‚àí1
dŒ∂ = Ak.
(7.20)
Isotropic tensor functions deÔ¨Åned by (7.15) will henceforth be referred to
as analytic tensor functions. The above discussed properties of analytic tensor
functions can be completed by the following relations (Exercise 7.3)
g (A) = Œ±f (A) + Œ≤h (A) ,
if
g (Œª) = Œ±f (Œª) + Œ≤h (Œª) ,
g (A) = f (A) h (A) ,
if
g (Œª) = f (Œª) h (Œª) ,
g (A) = f (h (A)) ,
if
g (Œª) = f (h (Œª)) .
(7.21)
In the following we will deal with representations for analytic tensor functions
and their derivatives.
7.2 Closed-Form Representation for Analytic Tensor
Functions and Their Derivatives
Our aim is to obtain the so-called closed form representation for analytic
tensor functions and their derivatives. This representation should be given
only in terms of Ô¨Ånite powers of the tensor argument and its eigenvalues and
avoid any reference to the integral over the complex plane or to power series.
We start with the Cayley-Hamilton theorem (4.91) for the tensor Œ∂I ‚àíA
n
	
k=0
(‚àí1)k I(k)
Œ∂I‚àíA (Œ∂I ‚àíA)n‚àík = 0.
(7.22)
With the aid of the Vieta theorem (4.24) we can write
I(0)
Œ∂I‚àíA = 1,
I(k)
Œ∂I‚àíA =
n
	
i1<i2<...<ik
(Œ∂ ‚àíŒªi1) (Œ∂ ‚àíŒªi2) . . . (Œ∂ ‚àíŒªik) ,
(7.23)
where k = 1, 2, . . ., n and the eigenvalues Œªi (i = 1, 2, . . ., n) of the tensor A
are counted with their multiplicity.
Composing (7.22) with the so-called resolvent of A
R (Œ∂) = (Œ∂I ‚àíA)‚àí1
(7.24)
yields

146
7 Analytic Tensor Functions
R (Œ∂) =
1
I(n)
Œ∂I‚àíA
n‚àí1
	
k=0
(‚àí1)n‚àík‚àí1 I(k)
Œ∂I‚àíA (Œ∂I ‚àíA)n‚àík‚àí1
=
1
I(n)
Œ∂I‚àíA
n‚àí1
	
k=0
I(k)
Œ∂I‚àíA (A ‚àíŒ∂I)n‚àík‚àí1 .
(7.25)
Applying the binomial theorem (see, e.g., [5])
(A ‚àíŒ∂I)l =
l
	
p=0
(‚àí1)l‚àíp
%l
p
&
Œ∂l‚àípAp,
l = 1, 2, . . . ,
(7.26)
where
%l
p
&
=
l!
p! (l ‚àíp)!,
(7.27)
we obtain
R (Œ∂) =
1
I(n)
Œ∂I‚àíA
n‚àí1
	
k=0
I(k)
Œ∂I‚àíA
n‚àík‚àí1
	
p=0
(‚àí1)n‚àík‚àí1‚àíp
%n ‚àík ‚àí1
p
&
Œ∂n‚àík‚àí1‚àípAp.
(7.28)
Rearranging this expression with respect to the powers of the tensor A delivers
R (Œ∂) =
n‚àí1
	
p=0
Œ±pAp
(7.29)
with
Œ±p =
1
I(n)
Œ∂I‚àíA
n‚àíp‚àí1
	
k=0
(‚àí1)n‚àík‚àíp‚àí1
%n ‚àík ‚àí1
p
&
I(k)
Œ∂I‚àíAŒ∂n‚àík‚àíp‚àí1,
(7.30)
where p = 0, 1, . . ., n ‚àí1. Inserting this result into (7.15) we obtain the fol-
lowing closed-form representation for the tensor function g (A) [20]
g (A) =
n‚àí1
	
p=0
œïpAp,
(7.31)
where
œïp =
1
2œÄi
7
Œì
g (Œ∂) Œ±pdŒ∂,
p = 0, 1, . . ., n ‚àí1.
(7.32)
The Cauchy integrals in (7.32) can be calculated with the aid of the residue
theorem (see, e.g.,[5]). To this end, we Ô¨Årst represent the determinant of the
tensor Œ∂I ‚àíA in the form

7.2 Closed-Form Representation for Analytic Tensor Functions
147
I(n)
Œ∂I‚àíA = det (Œ∂I ‚àíA) =
s
+
i=1
(Œ∂ ‚àíŒªi)ri ,
(7.33)
where Œªi denote pairwise distinct eigenvalues with the algebraic multiplicities
ri (i = 1, 2, . . . , s) such that
s
	
i=1
ri = n.
(7.34)
Thus, inserting (7.30) and (7.33) into (7.32) we obtain
œïp =
s
	
i=1
1
(ri ‚àí1)! lim
Œ∂‚ÜíŒªi
! dri‚àí1
dŒ∂ri‚àí1 [g (Œ∂) Œ±p (Œ∂) (Œ∂ ‚àíŒªi)ri]
$
,
(7.35)
where p = 1, 2, . . ., n ‚àí1.
The derivative of the tensor function g (A) can be obtained by direct
diÔ¨Äerentiation of the Dunfod-Taylor integral (7.15). Thus, by use of (6.122)
we can write
g (A) ,A =
1
2œÄi
7
Œì
g (Œ∂) (Œ∂I ‚àíA)‚àí1 ‚äó(Œ∂I ‚àíA)‚àí1 dŒ∂
(7.36)
and consequently
g (A) ,A =
1
2œÄi
7
Œì
g (Œ∂) R (Œ∂) ‚äóR (Œ∂) dŒ∂.
(7.37)
Taking (7.29) into account further yields
g (A) ,A =
n‚àí1
	
p,q=0
Œ∑pqAp ‚äóAq,
(7.38)
where
Œ∑pq = Œ∑qp =
1
2œÄi
7
Œì
g (Œ∂) Œ±p (Œ∂) Œ±q (Œ∂) dŒ∂,
p, q = 0, 1, . . . , n ‚àí1.
(7.39)
The residue theorem Ô¨Ånally delivers
Œ∑pq =
s
	
i=1
1
(2ri ‚àí1)! lim
Œ∂‚ÜíŒªi
! d2ri‚àí1
dŒ∂2ri‚àí1

g (Œ∂) Œ±p (Œ∂) Œ±q (Œ∂) (Œ∂ ‚àíŒªi)2ri$
,
(7.40)
where p, q = 0, 1, . . ., n ‚àí1.

148
7 Analytic Tensor Functions
7.3 Special Case: Diagonalizable Tensor Functions
For analytic functions of diagonalizable tensors the deÔ¨Ånitions in terms of
the Dunford-Taylor integral (7.15) on the one side and eigenprojections (7.2)
on the other side become equivalent. In this special case, one can obtain
alternative closed-form representations for analytic tensor functions and their
derivatives. To this end, we Ô¨Årst derive an alternative representation of the
Sylvester formula (4.54). In Sect. 4.4 we have shown that the eigenprojections
can be given by (4.51)
Pi = pi (A) ,
i = 1, 2, . . . , s,
(7.41)
where pi (i = 1, 2, . . . , s) are polynomials satisfying the requirements (4.50).
Thus, the eigenprojections of a second-order tensor can be considered as its
analytic (isotropic) tensor functions. Applying the Dunford-Taylor integral
(7.15) we can thus write
Pi =
1
2œÄi
7
Œì
pi (Œ∂) (Œ∂I ‚àíA)‚àí1 dŒ∂,
i = 1, 2, . . ., s.
(7.42)
Similarly to (7.31) and (7.35) we further obtain
Pi =
n‚àí1
	
p=0
œÅipAp,
i = 1, 2, . . ., s,
(7.43)
where
œÅip =
s
	
k=1
1
(rk ‚àí1)! lim
Œ∂‚ÜíŒªk
! drk‚àí1
dŒ∂rk‚àí1 [pi (Œ∂) Œ±p (Œ∂) (Œ∂ ‚àíŒªk)rk]
$
(7.44)
and Œ±p (p = 0, 1, . . ., n ‚àí1) are given by (7.30). With the aid of polynomial
functions pi (Œª) satisfying in addition to (4.50) the following conditions
dr
dŒªr pi (Œª)

Œª=Œªj
= 0
i, j = 1, 2, . . ., s; r = 1, 2, . . . , ri ‚àí1
(7.45)
we can simplify (7.44) by
œÅip =
1
(ri ‚àí1)! lim
Œ∂‚ÜíŒªi
! dri‚àí1
dŒ∂ri‚àí1 [Œ±p (Œ∂) (Œ∂ ‚àíŒªi)ri]
$
.
(7.46)
Now, inserting (7.43) into (7.2) delivers
g (A) =
s
	
i=1
g (Œªi)
n‚àí1
	
p=0
œÅipAp.
(7.47)

7.3 Special Case: Diagonalizable Tensor Functions
149
In order to obtain an alternative representation for g (A) ,A we again consider
the tensor power series (7.9). Direct diÔ¨Äerentiation of (7.9) with respect to A
delivers with the aid of (6.117)
g (A) ,A =
‚àû
	
r=1
ar
r‚àí1
	
k=0
Ar‚àí1‚àík ‚äóAk.
(7.48)
Applying the spectral representation (7.1) and taking (4.47) and (7.12) into
account we further obtain (see also [18], [48])
g (A) ,A =
‚àû
	
r=1
ar
r‚àí1
	
k=0
s
	
i,j=1
Œªr‚àí1‚àík
i
Œªk
j Pi ‚äóPj
=
s
	
i=1
‚àû
	
r=1
rarŒªr‚àí1
i
Pi ‚äóPi +
s
	
i,j=1
jÃ∏=i
‚àû
	
r=1
ar
Œªr
i ‚àíŒªr
j
Œªi ‚àíŒªj
Pi ‚äóPj
=
s
	
i=1
g‚Ä≤ (Œªi) Pi ‚äóPi +
s
	
i,j=1
jÃ∏=i
g (Œªi) ‚àíg (Œªj)
Œªi ‚àíŒªj
Pi ‚äóPj
=
s
	
i,j=1
GijPi ‚äóPj,
(7.49)
where
Gij =
‚éß
‚é™
‚é®
‚é™
‚é©
g‚Ä≤ (Œªi)
if i = j,
g (Œªi) ‚àíg (Œªj)
Œªi ‚àíŒªj
if i Ã∏= j.
(7.50)
Inserting into (7.49) the alternative representation for the eigenprojections
(7.43) yields
g (A) ,A =
s
	
i,j=1
Gij
n‚àí1
	
p,q=0
œÅipœÅjqAp ‚äóAq.
(7.51)
Thus, we again end up with the representation (7.38)
g (A) ,A =
n‚àí1
	
p,q=0
Œ∑pqAp ‚äóAq,
(7.52)
where
Œ∑pq = Œ∑qp =
s
	
i,j=1
GijœÅipœÅjq,
p, q = 0, 1, . . . , n ‚àí1.
(7.53)

150
7 Analytic Tensor Functions
Finally, let us focus on the diÔ¨Äerentiability of eigenprojections. To this end,
we represent them by [24] (Exercise 7.4)
Pi =
1
2œÄi
7
Œìi
(Œ∂I ‚àíA)‚àí1 dŒ∂,
i = 1, 2, . . . s,
(7.54)
where the integral is taken on the complex plane over a closed curve Œìi the
interior of which includes only the eigenvalue Œªi. All other eigenvalues of A lie
outside Œìi. Œìi does not depend on Œªi as far as this eigenvalue is simple and does
not lie directly on Œìi. Indeed, if Œªi is multiple, a perturbation of A by A+tX
can lead to a split of eigenvalues within Œìi. In this case, (7.54) yields a sum of
eigenprojections corresponding to these split eigenvalues which coalesce in Œªi
for t = 0. Thus, the eigenprojection Pi corresponding to a simple eigenvalue
Œªi is diÔ¨Äerentiable according to (7.54). Direct diÔ¨Äerentiation of (7.54) delivers
in this case
Pi,A =
1
2œÄi
7
Œìi
(Œ∂I ‚àíA)‚àí1 ‚äó(Œ∂I ‚àíA)‚àí1 dŒ∂,
ri = 1.
(7.55)
By analogy with (7.38) we thus obtain
Pi,A =
n‚àí1
	
p,q=0
œÖipqAp ‚äóAq,
(7.56)
where
œÖipq = œÖiqp =
1
2œÄi
7
Œìi
Œ±p (Œ∂) Œ±q (Œ∂) dŒ∂,
p, q = 0, 1, . . . , n ‚àí1.
(7.57)
By the residue theorem we further write
œÖipq = lim
Œ∂‚ÜíŒªi
! d
dŒ∂

Œ±p (Œ∂) Œ±q (Œ∂) (Œ∂ ‚àíŒªi)2$
,
p, q = 0, 1, . . . , n ‚àí1. (7.58)
With the aid of (7.49) one can obtain an alternative representation for the
derivative of the eigenprojections in terms of the eigenprojections themselves.
Indeed, substituting the function g in (7.49) by pi and taking the properties
of the latter function (4.50) and (7.45) into account we have
Pi,A =
s
	
j=1
jÃ∏=i
Pi ‚äóPj + Pj ‚äóPi
Œªi ‚àíŒªj
.
(7.59)
7.4 Special case: Three-Dimensional Space
First, we specify the closed-form solutions (7.31) and (7.38) for three-dimen-
sional space (n = 3). In this case, the functions Œ±k (Œ∂) (k = 0, 1, 2) (7.30) take
the form

7.4 Special case: Three-Dimensional Space
151
Œ±0 (Œ∂) = Œ∂2 ‚àíŒ∂IŒ∂I‚àíA + IIŒ∂I‚àíA
IIIŒ∂I‚àíA
= Œ∂2 ‚àíŒ∂ (Œª1 + Œª2 + Œª3) + Œª1Œª2 + Œª2Œª3 + Œª3Œª1
(Œ∂ ‚àíŒª1) (Œ∂ ‚àíŒª2) (Œ∂ ‚àíŒª3)
,
Œ±1 (Œ∂) = IŒ∂I‚àíA ‚àí2Œ∂
IIIŒ∂I‚àíA
=
Œ∂ ‚àíŒª1 ‚àíŒª2 ‚àíŒª3
(Œ∂ ‚àíŒª1) (Œ∂ ‚àíŒª2) (Œ∂ ‚àíŒª3),
Œ±2 (Œ∂) =
1
IIIŒ∂I‚àíA
=
1
(Œ∂ ‚àíŒª1) (Œ∂ ‚àíŒª2) (Œ∂ ‚àíŒª3).
(7.60)
Inserting these expressions into (7.35) and (7.40) and considering separately
cases of distinct and repeated eigenvalues, we obtain the following result [22].
Distinct eigenvalues: Œª1 Ã∏= Œª2 Ã∏= Œª3 Ã∏= Œª1,
œï0 =
3
	
i=1
g (Œªi) ŒªjŒªk
Di
,
œï1 = ‚àí
3
	
i=1
g (Œªi) (Œªj + Œªk)
Di
,
œï2 =
3
	
i=1
g (Œªi)
Di
,
(7.61)
Œ∑00 =
3
	
i=1
Œª2
jŒª2
kg‚Ä≤ (Œªi)
D2
i
‚àí
3
	
i,j=1
iÃ∏=j
ŒªiŒªjŒª2
k [g (Œªi) ‚àíg (Œªj)]
(Œªi ‚àíŒªj)3 Dk
,
Œ∑01 = Œ∑10 = ‚àí
3
	
i=1
(Œªj + Œªk) ŒªjŒªkg‚Ä≤ (Œªi)
D2
i
+
3
	
i,j=1
iÃ∏=j
(Œªj + Œªk) ŒªiŒªk [g (Œªi) ‚àíg (Œªj)]
(Œªi ‚àíŒªj)3 Dk
,
Œ∑02 = Œ∑20 =
3
	
i=1
ŒªjŒªkg‚Ä≤ (Œªi)
D2
i
‚àí
3
	
i,j=1
iÃ∏=j
ŒªiŒªk [g (Œªi) ‚àíg (Œªj)]
(Œªi ‚àíŒªj)3 Dk
,
Œ∑11 =
3
	
i=1
(Œªj + Œªk)2 g‚Ä≤ (Œªi)
D2
i
‚àí
3
	
i,j=1
iÃ∏=j
(Œªj + Œªk) (Œªi + Œªk) [g (Œªi) ‚àíg (Œªj)]
(Œªi ‚àíŒªj)3 Dk
,

152
7 Analytic Tensor Functions
Œ∑12 = Œ∑21 = ‚àí
3
	
i=1
(Œªj + Œªk) g‚Ä≤ (Œªi)
D2
i
+
3
	
i,j=1
iÃ∏=j
(Œªi + Œªk) [g (Œªi) ‚àíg (Œªj)]
(Œªi ‚àíŒªj)3 Dk
,
Œ∑22 =
3
	
i=1
g‚Ä≤ (Œªi)
D2
i
‚àí
3
	
i,j=1
iÃ∏=j
g (Œªi) ‚àíg (Œªj)
(Œªi ‚àíŒªj)3 Dk
,
i Ã∏= j Ã∏= k Ã∏= i,
(7.62)
where
Di = (Œªi ‚àíŒªj) (Œªi ‚àíŒªk) ,
i Ã∏= j Ã∏= k Ã∏= i = 1, 2, 3.
(7.63)
Double coalescence of eigenvalues: Œªi Ã∏= Œªj = Œªk = Œª, j Ã∏= k,
œï0 = ŒªŒªg (Œªi) ‚àíŒªig (Œª)
(Œªi ‚àíŒª)2
+ Œªig (Œª)
(Œªi ‚àíŒª) ‚àíŒªŒªig‚Ä≤ (Œª)
(Œªi ‚àíŒª) ,
œï1 = ‚àí2Œªg (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)2
+ g‚Ä≤ (Œª) (Œªi + Œª)
(Œªi ‚àíŒª)
,
œï2 = g (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)2
‚àí
g‚Ä≤ (Œª)
(Œªi ‚àíŒª),
(7.64)
Œ∑00 =

2Œª2Œª2
i ‚àí6Œª3Œªi

[g (Œªi) ‚àíg (Œª)]
(Œªi ‚àíŒª)5
+ Œª4g‚Ä≤ (Œªi) +

2Œª3Œªi + 4Œª2Œª2
i ‚àí4ŒªŒª3
i + Œª4
i

g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
+

2Œª2Œª2
i ‚àíŒª3
i Œª

g‚Ä≤‚Ä≤ (Œª)
(Œªi ‚àíŒª)3
+ Œª2Œª2
i g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2 ,
Œ∑01 = Œ∑10 =

3Œª3 + 7ŒªiŒª2 ‚àí2Œª2
i Œª

[g (Œªi) ‚àíg (Œª)]
(Œªi ‚àíŒª)5
‚àí2Œª3g‚Ä≤ (Œªi) +

Œª3 + 7ŒªiŒª2 ‚àí2Œª2
i Œª

g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
‚àí

4Œª2Œªi + Œª2
i Œª ‚àíŒª3
i

g‚Ä≤‚Ä≤ (Œª)
2 (Œªi ‚àíŒª)3
‚àíŒªiŒª (Œªi + Œª) g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2
,

7.4 Special case: Three-Dimensional Space
153
Œ∑02 = Œ∑20 =

Œª2
i ‚àí3ŒªiŒª ‚àí2Œª2
[g (Œªi) ‚àíg (Œª)]
(Œªi ‚àíŒª)5
+ Œª2g‚Ä≤ (Œªi) +

Œª2 + 3ŒªiŒª ‚àíŒª2
i

g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
+

3ŒªŒªi ‚àíŒª2
i

g‚Ä≤‚Ä≤ (Œª)
2 (Œªi ‚àíŒª)3
+ ŒªiŒªg‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2 ,
Œ∑11 = ‚àí4Œª (Œªi + 3Œª) [g (Œªi) ‚àíg (Œª)]
(Œªi ‚àíŒª)5
+ 4Œª2g‚Ä≤ (Œªi) + Œª (Œªi + 2Œª) g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
+ 2Œª (Œªi + Œª) g‚Ä≤‚Ä≤ (Œª)
(Œªi ‚àíŒª)3
+ (Œªi + Œª)2 g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2
,
Œ∑12 = Œ∑21 = (Œªi + 7Œª) [g (Œªi) ‚àíg (Œª)]
(Œªi ‚àíŒª)5
‚àí2Œªg‚Ä≤ (Œªi) + (Œªi + 5Œª) g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
‚àí(Œªi + 3Œª) g‚Ä≤‚Ä≤ (Œª)
2 (Œªi ‚àíŒª)3
‚àí(Œªi + Œª) g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2
,
Œ∑22 = ‚àí4g (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)5
+ g‚Ä≤ (Œªi) + 3g‚Ä≤ (Œª)
(Œªi ‚àíŒª)4
+
g‚Ä≤‚Ä≤ (Œª)
(Œªi ‚àíŒª)3 +
g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6 (Œªi ‚àíŒª)2 . (7.65)
Triple coalescence of eigenvalues: Œª1 = Œª2 = Œª3 = Œª,
œï0 = g (Œª) ‚àíŒªg‚Ä≤ (Œª) + 1
2Œª2g‚Ä≤‚Ä≤ (Œª) ,
œï1 = g‚Ä≤ (Œª) ‚àíŒªg‚Ä≤‚Ä≤ (Œª) ,
œï2 = 1
2g‚Ä≤‚Ä≤ (Œª) ,
(7.66)
Œ∑00 = g‚Ä≤ (Œª) ‚àíŒªg‚Ä≤‚Ä≤ (Œª) + Œª2g‚Ä≤‚Ä≤‚Ä≤ (Œª)
2
‚àíŒª3gIV (Œª)
12
+ Œª4gV (Œª)
120
,
Œ∑01 = Œ∑10 = g‚Ä≤‚Ä≤ (Œª)
2
‚àíŒªg‚Ä≤‚Ä≤‚Ä≤ (Œª)
2
+ Œª2gIV (Œª)
8
‚àíŒª3gV (Œª)
60
,
Œ∑02 = Œ∑20 = g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6
‚àíŒªgIV (Œª)
24
+ Œª2gV (Œª)
120
,
Œ∑11 = g‚Ä≤‚Ä≤‚Ä≤ (Œª)
6
‚àíŒªgIV (Œª)
6
+ Œª2gV (Œª)
30
,
Œ∑12 = Œ∑21 = gIV (Œª)
24
‚àíŒªgV (Œª)
60
,

154
7 Analytic Tensor Functions
Œ∑22 = gV (Œª)
120 ,
(7.67)
where superposed Roman numerals denote the order of the derivative.
Example. To illustrate the application of the above closed-form solution
we consider the exponential function of the velocity gradient under simple
shear. The velocity gradient is deÔ¨Åned as the material time derivative of the
deformation gradient by L = ÀôF. Using the representation of F in the case of
simple shear (4.23) we can write
L = Li
¬∑jei ‚äóej,
where

Li
¬∑j

=
‚é°
‚é£
0 ÀôŒ≥ 0
0 0 0
0 0 0
‚é§
‚é¶.
(7.68)
We observe that L has a triple (r1 = 3) zero eigenvalue
Œª1 = Œª2 = Œª3 = Œª = 0.
(7.69)
This eigenvalue is, however, defect since it is associated with only two (t1 = 2)
linearly independent (right) eigenvectors
a1 = e1,
a2 = e3.
(7.70)
Therefore, L (7.68) is not diagonalizable and admits no spectral decomposition
in the form (7.1). For this reason, isotropic functions of L as well as their
derivative cannot be obtained on the basis of eigenprojections. Instead, we
exploit the closed-form solution (7.31), (7.38) with the coeÔ¨Écients calculated
for the case of triple coalescence of eigenvalues by (7.66) and (7.67). Thus, we
can write
exp (L) = exp (Œª)
"%1
2Œª2 ‚àíŒª + 1
&
I + (1 ‚àíŒª) L + 1
2L2
#
,
(7.71)
exp (L) ,L = exp (Œª)
"%
1 ‚àíŒª + Œª2
2 ‚àíŒª3
12 + Œª4
120
&
I
+
%1
2 ‚àíŒª
2 + Œª2
8 ‚àíŒª3
60
&
(L ‚äóI + I ‚äóL)
+
%1
6 ‚àíŒª
6 + Œª2
30
&
L ‚äóL
+
%1
6 ‚àíŒª
24 + Œª2
120
& 
L2 ‚äóI + I ‚äóL2
+
% 1
24 ‚àíŒª
60
& 
L2 ‚äóL + L ‚äóL2
+
1
120L2 ‚äóL2
#
.
(7.72)

7.4 Special case: Three-Dimensional Space
155
On use of (7.69) this Ô¨Ånally leads to the following expressions
exp (L) = I + L + 1
2L2,
(7.73)
exp (L) ,L = I + 1
2 (L ‚äóI + I ‚äóL) + 1
6L ‚äóL + 1
6

L2 ‚äóI + I ‚äóL2
+ 1
24

L2 ‚äóL + L ‚äóL2
+
1
120L2 ‚äóL2.
(7.74)
Taking into account a special property of L (7.68):
Lk = 0,
k = 2, 3, . . .
(7.75)
the same results can also be obtained directly from the power series (1.109)
and its derivative. By virtue of (6.117) the latter one can be given by
exp (L) ,L =
‚àû
	
r=1
1
r!
r‚àí1
	
k=0
Lr‚àí1‚àík ‚äóLk.
(7.76)
For diagonalizable tensor functions the representations (7.31) and (7.38)
can be simpliÔ¨Åed in the cases of repeated eigenvalues where the coeÔ¨Écients
œïp and Œ∑pq are given by (7.64-7.67). To this end, we use the identities
A2 = (Œªi + Œª) A ‚àíŒªiŒªI for the case of double coalescence of eigenvalues
(Œªi Ã∏= Œªj = Œªk = Œª) and A = ŒªI, A2 = Œª2I for the case of triple coalescence
of eigenvalues (Œª1 = Œª2 = Œª3 = Œª). Thus, we obtain the following result well-
known for symmetric isotropic tensor functions [7].
Double coalescence of eigenvalues: Œªi Ã∏= Œªj = Œªk = Œª, A2 = (Œªi + Œª) A‚àíŒªiŒªI,
œï0 = Œªig (Œª) ‚àíŒªg (Œªi)
Œªi ‚àíŒª
,
œï1 = g (Œªi) ‚àíg (Œª)
Œªi ‚àíŒª
,
œï2 = 0,
(7.77)
Œ∑00 = ‚àí2ŒªiŒªg (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)3
+ Œª2g‚Ä≤ (Œªi) + Œª2
i g‚Ä≤ (Œª)
(Œªi ‚àíŒª)2
,
Œ∑01 = Œ∑10 = (Œªi + Œª) g (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)3
‚àíŒªg‚Ä≤ (Œªi) + Œªig‚Ä≤ (Œª)
(Œªi ‚àíŒª)2
,
Œ∑11 = ‚àí2g (Œªi) ‚àíg (Œª)
(Œªi ‚àíŒª)3
+ g‚Ä≤ (Œªi) + g‚Ä≤ (Œª)
(Œªi ‚àíŒª)2
,
Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0.
(7.78)
Triple coalescence of eigenvalues: Œª1 = Œª2 = Œª3 = Œª, A = ŒªI, A2 = Œª2I,
œï0 = g (Œª) ,
œï1 = œï2 = 0,
(7.79)

156
7 Analytic Tensor Functions
Œ∑00 = g‚Ä≤ (Œª) ,
Œ∑01 = Œ∑10 = Œ∑11 = Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0. (7.80)
Finally, we specify the representations for eigenprojections (7.43) and their
derivative (7.56) for three-dimensional space. The expressions for the functions
œÅip (7.46) and œÖipq (7.58) can be obtained from the representations for œïp
(7.61), (7.77), (7.79) and Œ∑pq (7.62), (7.78), respectively. To this end, we set
there g (Œªi) = 1, g (Œªj) = g (Œªk) = g‚Ä≤ (Œªi) = g‚Ä≤ (Œªj) = g‚Ä≤ (Œªk) = 0. Accordingly,
we obtain the following representations.
Distinct eigenvalues: Œª1 Ã∏= Œª2 Ã∏= Œª3 Ã∏= Œª1,
œÅi0 = ŒªjŒªk
Di
,
œÅi1 = ‚àíŒªj + Œªk
Di
,
œÅi2 = 1
Di
,
(7.81)
œÖi00 = ‚àí2ŒªiŒªjŒªk
,
Œªk
(Œªi ‚àíŒªj)3 Dk
+
Œªj
(Œªi ‚àíŒªk)3 Dj
-
,
œÖi01 = œÖi10 = Œªk
Œªi (Œªj + Œªk) + Œªj (Œªi + Œªk)
(Œªi ‚àíŒªj)3 Dk
+Œªj
Œªi (Œªj + Œªk) + Œªk (Œªi + Œªj)
(Œªi ‚àíŒªk)3 Dj
,
œÖi02 = œÖi20 = ‚àíŒªk
Œªi + Œªj
(Œªi ‚àíŒªj)3 Dk
‚àíŒªj
Œªi + Œªk
(Œªi ‚àíŒªk)3 Dj
,
œÖi11 = ‚àí2 (Œªj + Œªk)
,
Œªi + Œªk
(Œªi ‚àíŒªj)3 Dk
+
Œªi + Œªj
(Œªi ‚àíŒªk)3 Dj
-
,
œÖi12 = œÖi21 = Œªi + Œªj + 2Œªk
(Œªi ‚àíŒªj)3 Dk
+ Œªi + 2Œªj + Œªk
(Œªi ‚àíŒªk)3 Dj
,
œÖi22 = ‚àí
2
(Œªi ‚àíŒªj)3 Dk
‚àí
2
(Œªi ‚àíŒªk)3 Dj
,
i Ã∏= j Ã∏= k Ã∏= i = 1, 2, 3.
(7.82)
Double coalescence of eigenvalues: Œªi Ã∏= Œªj = Œªk = Œª, j Ã∏= k,
œÅi0 = ‚àí
Œª
Œªi ‚àíŒª,
œÅi1 =
1
Œªi ‚àíŒª,
œÅi2 = 0,
(7.83)
œÖi00 = ‚àí
2ŒªŒªi
(Œªi ‚àíŒª)3 ,
œÖi01 = œÖi10 =
Œªi + Œª
(Œªi ‚àíŒª)3 ,
œÖi11 = ‚àí
2
(Œªi ‚àíŒª)3 ,
œÖi02 = œÖi20 = œÖi12 = œÖi21 = œÖi22 = 0.
(7.84)
Triple coalescence of eigenvalues: Œª1 = Œª2 = Œª3 = Œª,
œÅ10 = 1,
œÅ11 = œÅ12 = 0.
(7.85)
The functions œÖ1pq (p, q = 0, 1, 2) are in this case undeÔ¨Åned since the only
eigenprojection P1 is not diÔ¨Äerentiable.

7.5 Recurrent Calculation of Tensor Power Series and Their Derivatives
157
7.5 Recurrent Calculation of Tensor Power Series and
Their Derivatives
In numerical calculations with a limited number of digits the above presented
closed-form solutions especially those ones for the derivative of analytic tensor
functions can lead to inexact results if at least two eigenvalues of the tensor
argument are close to each other but do not coincide (see [19]). In this case,
a numerical calculation of the derivative of an analytic tensor function on
the basis of the corresponding power series expansion might be advantageous
provided this series converges very fast so that only a relatively small number
of terms are suÔ¨Écient in order to ensure a desired precision. This numerical
calculation can be carried out by means of a recurrent procedure presented
below.
The recurrent procedure is based on the sequential application of the
Cayley-Hamilton equation (4.91). Accordingly, we can write for an arbitrary
second-order tensor A ‚ààLinn
An =
n‚àí1
	
k=0
(‚àí1)n‚àík+1 I(n‚àík)
A
Ak.
(7.86)
With the aid of this relation any non-negative integer power of A can be
represented by
Ar =
n‚àí1
	
k=0
œâ(r)
k Ak,
r = 0, 1, 2, . . .
(7.87)
Indeed, for r ‚â§n one obtains directly from (7.86)
œâ(r)
k
= Œ¥rk,
œâ(n)
k
= (‚àí1)n‚àík+1 I(n‚àík)
A
,
r, k = 0, 1, . . ., n ‚àí1.
(7.88)
Further powers of A can be expressed by composing (7.87) with A and rep-
resenting An by (7.86) as
Ar+1 =
n‚àí1
	
k=0
œâ(r)
k Ak+1 =
n‚àí1
	
k=1
œâ(r)
k‚àí1Ak + œâ(r)
n‚àí1An
=
n‚àí1
	
k=1
œâ(r)
k‚àí1Ak + œâ(r)
n‚àí1
n‚àí1
	
k=0
(‚àí1)n‚àík‚àí1 I(n‚àík)
A
Ak.
Comparing with (7.87) we obtain the following recurrent relations (see also
[38])
œâ(r+1)
0
= œâ(r)
n‚àí1 (‚àí1)n‚àí1 I(n)
A ,
œâ(r+1)
k
= œâ(r)
k‚àí1 + œâ(r)
n‚àí1 (‚àí1)n‚àík‚àí1 I(n‚àík)
A
,
k = 1, 2, . . ., n ‚àí1.
(7.89)

158
7 Analytic Tensor Functions
With the aid of representation (7.87) the inÔ¨Ånite power series (7.9) can thus
be expressed by (7.31)
g (A) =
n‚àí1
	
p=0
œïpAp,
(7.90)
where
œïp =
‚àû
	
r=0
arœâ(r)
p .
(7.91)
Thus, the inÔ¨Ånite power series (7.9) with the coeÔ¨Écients (7.13) results in
the same representation as the corresponding analytic tensor function (7.15)
provided the inÔ¨Ånite series (7.91) converges.
Further, inserting (7.87) into (7.48) we obtain again the representation
(7.38)
g (A) ,A =
n‚àí1
	
p,q=0
Œ∑pqAp ‚äóAq,
(7.92)
where
Œ∑pq = Œ∑qp =
‚àû
	
r=1
ar
r‚àí1
	
k=0
œâ(r‚àí1‚àík)
p
œâ(k)
q ,
p, q = 0, 1, . . ., n ‚àí1.
(7.93)
The procedure computing the coeÔ¨Écients Œ∑pq (7.93) can be simpliÔ¨Åed by means
of the following recurrent identity (see also [30])
r
	
k=0
Ar‚àík ‚äóAk = Ar ‚äóI +
,r‚àí1
	
k=0
Ar‚àí1‚àík ‚äóAk
-
A
= A
,r‚àí1
	
k=0
Ar‚àí1‚àík ‚äóAk
-
+ I ‚äóAr,
r = 1, 2 . . .,
(7.94)
where
r‚àí1
	
k=0
Ar‚àí1‚àík ‚äóAk =
n‚àí1
	
p,q=0
Œæ(r)
pq Ap ‚äóAq,
r = 1, 2 . . .
(7.95)
Thus, we obtain
Œ∑pq =
‚àû
	
r=1
arŒæ(r)
pq ,
(7.96)
where [19]

7.5 Recurrent Calculation of Tensor Power Series and Their Derivatives
159
Œæ(1)
pq = Œæ(1)
qp = œâ(0)
p œâ(0)
q
= Œ¥0pŒ¥0q,
p ‚â§q; p, q = 0, 1, . . . , n ‚àí1,
Œæ(r)
00 = Œæ(r‚àí1)
0 n‚àí1œâ(n)
0
+ œâ(r‚àí1)
0
,
Œæ(r)
0q = Œæ(r)
q0 = Œæ(r‚àí1)
0 q‚àí1 + Œæ(r‚àí1)
0 n‚àí1œâ(n)
q
= Œæ(r‚àí1)
n‚àí1 qœâ(n)
0
+ œâ(r‚àí1)
q
,
Œæ(r)
pq = Œæ(r)
qp = Œæ(r‚àí1)
p q‚àí1 + Œæ(r‚àí1)
p n‚àí1œâ(n)
q
= Œæ(r‚àí1)
p‚àí1 q + Œæ(r‚àí1)
n‚àí1 qœâ(n)
p ,
p ‚â§q; p, q = 1, 2, . . . , n ‚àí1,
r = 2, 3, . . .
(7.97)
The calculation of coeÔ¨Écient series (7.89) and (7.97) can be Ô¨Ånished as soon
as for some r
arœâ(r)
p
 ‚â§Œµ

r
	
t=0
atœâ(t)
p
 ,
arŒæ(r)
pq
 ‚â§Œµ

r
	
t=1
atŒæ(t)
pq
 ,
p, q = 0, 1, . . . , n ‚àí1,
(7.98)
where Œµ > 0 denotes a precision parameter.
Example. To illustrate the application of the above recurrent procedure
we consider again the exponential function of the velocity gradient under
simple shear (7.68). In view of (7.69) we can write
I(1)
L
= I(2)
L
= I(3)
L
= 0.
(7.99)
With this result in hand the coeÔ¨Écients œâ(r)
p
and Œæ(r)
pq (p, q = 0, 1, 2) appear-
ing in the representation of the analytic tensor function (7.90), (7.91) and
its derivative (7.92), (7.96) can easily be calculated by means of the above
recurrent formulas (7.88), (7.89) and (7.97). The results of the calculation are
summarized in Table 7.1.
Considering these results in (7.90), (7.91), (7.92) and (7.96) we obtain the
representations (7.73) and (7.74). Note that the recurrent procedure delivers
an exact result only in some special cases like this where the argument tensor
is characterized by the property (7.75).
Exercises
7.1. Let R (œâ) be a proper orthogonal tensor describing a rotation about some
axis e ‚ààE3 by the angle œâ. Prove that Ra (œâ) = R (aœâ) for any real number
a.
7.2. Specify the right stretch tensor U (7.5)1 for simple shear utilizing the
results of Exercise 4.1.
7.3. Prove the properties of analytic tensor functions (7.21).

160
7 Analytic Tensor Functions
Table 7.1. Recurrent calculation of the coeÔ¨Écients œâ(r)
p
and Œæ(r)
pq
r
œâ(r)
0
œâ(r)
1
œâ(r)
2
Œæ(r)
00 Œæ(r)
01 Œæ(r)
02 Œæ(r)
11 Œæ(r)
12 Œæ(r)
22
ar
0
1
0
0
1
1
0
1
0
1
0
0
0
0
0
1
2
0
0
1
0
1
0
0
0
0
1/2
3
0
0
0
0
0
1
1
0
0
1/6
4
0
0
0
0
0
0
0
1
0
1/24
5
0
0
0
0
0
0
0
0
1
1/120
6
0
0
0
0
0
0
0
0
0
1/720
r=0
arœâ(r)
p
1
1
1
2
r=1
arŒæ(r)
pq
1
1
2
1
6
1
6
1
24
1
120
7.4. Prove representation (7.54) for eigenprojections of diagonalizable second-
order tensors.
7.5. Calculate eigenprojections and their derivatives for the tensor A (Exer-
cise 4.8) using representations (7.81-7.85).
7.6. Calculate by means of the closed-form solution exp (A) and exp (A) ,A,
where the tensor A is deÔ¨Åned in Exercise 4.8. Compare the results for exp (A)
with those of Exercise 4.9.
7.7. Compute exp (A) and exp (A) ,A by means of the recurrent procedure
with the precision parameter Œµ = 1 ¬∑ 10‚àí6, where the tensor A is deÔ¨Åned in
Exercise 4.8. Compare the results with those of Exercise 7.6.

8
Applications to Continuum Mechanics
8.1 Polar Decomposition of the Deformation Gradient
The deformation gradient F represents an invertible second-order tensor gen-
erally permitting a unique polar decomposition by
F = RU = vR,
(8.1)
where R is an orthogonal tensor while U and v are symmetric tensors. In
continuum mechanics, R is called rotation tensor while U and v are referred to
as the right and left stretch tensor, respectively. The latter ones have already
been introduced in Sect. 7.1 in the context of generalized strain measures.
In order to show that the polar decomposition (8.1) always exists and
is unique we Ô¨Årst consider the so-called right and left Cauchy-Green tensors
respectively by
C = FTF,
b = FFT.
(8.2)
These tensors are symmetric and have principal traces in common. Indeed, in
view of (1.144)
tr

Ck
= tr

FTF . . . FTF




k times
= tr

FFT . . . FFT



k times
= tr

bk
.
(8.3)
For this reason, all scalar-valued isotropic functions of C and b such as prin-
cipal invariants or eigenvalues coincide. Thus, we can write
C =
s
	
i=1
ŒõiPi,
b =
s
	
i=1
Œõipi,
(8.4)
where eigenvalues Œõi are positive. Indeed, let ai be a unit eigenvector asso-
ciated with the eigenvalue Œõi. Then, in view of (1.73), (1.99), (1.110) and by
Theorem 1.8 one can write

162
8 Applications to Continuum Mechanics
Œõi = ai ¬∑ (Œõiai) = ai ¬∑ (Cai) = ai ¬∑

FTFai

=

aiFT
¬∑ (Fai) = (Fai) ¬∑ (Fai) > 0.
Thus, square roots of C and b are unique tensors deÔ¨Åned by
U =
‚àö
C =
s
	
i=1

ŒõiPi,
v =
‚àö
b =
s
	
i=1

Œõipi.
(8.5)
Further, one can show that
R = FU‚àí1
(8.6)
represents an orthogonal tensor. Indeed,
RRT = FU‚àí1U‚àí1FT = FU‚àí2FT = FC‚àí1FT
= F

FTF
‚àí1 FT = FF‚àí1F‚àíTFT = I.
Thus, we can write taking (8.6) into account
F = RU =

RURT
R.
(8.7)
The tensor
RURT = FRT
(8.8)
in (8.7) is symmetric due to symmetry of U (8.5)1. Thus, one can write

RURT2
=

RURT 
RURTT
=

FRT 
FRTT
= FRTRFT = FFT = b.
(8.9)
In view of (8.5)2 there exists only one real symmetric tensor whose square is
b. Hence,
RURT = v,
(8.10)
which by virtue of (8.7) results in the polar decomposition (8.1).
8.2 Basis-Free Representations for the Stretch and
Rotation Tensor
With the aid of the closed-form representations for analytic tensor functions
discussed in Chap. 7 the stretch and rotation tensors can be expressed directly
in terms of the deformation gradient and Cauchy-Green tensors without any
reference to their eigenprojections. First, we deal with the stretch tensors

8.2 Basis-Free Representations for the Stretch and Rotation Tensor
163
(8.5). Inserting in (7.61) g (Œõi) = ‚àöŒõi = Œªi and keeping in mind (7.31) we
write
U = œï0I + œï1C + œï2C2,
v = œï0I + œï1b + œï2b2,
(8.11)
where [44]
œï0 =
Œª1Œª2Œª3 (Œª1 + Œª2 + Œª3)
(Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1),
œï1 = Œª2
1 + Œª2
2 + Œª2
3 + Œª1Œª2 + Œª2Œª3 + Œª3Œª1
(Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1)
,
œï2 = ‚àí
1
(Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1).
(8.12)
These representations for œïi are free of singularities and are therefore generally
valid for the case of simple as well as repeated eigenvalues of C and b.
The rotation tensor results from (8.6) where we can again write
U‚àí1 = œÇ0I + œÇ1C + œÇ2C2.
(8.13)
The representations for œÇp (p = 0, 1, 2) can be obtained either again by (7.61)
where g (Œõi) = Œõ‚àí1/2
i
= Œª‚àí1
i
or by applying the Cayley-Hamilton equation
(4.91) leading to
U‚àí1 = III‚àí1
U

U2 ‚àíIUU + IIUI

= III‚àí1
U

(IIU ‚àíœï0IU) I + (1 ‚àíœï1IU) C ‚àíœï2IUC2
,
(8.14)
where
IU = Œª1 + Œª2 + Œª3,
IIU = Œª1Œª2 + Œª2Œª3 + Œª3Œª1,
IIIU = Œª1Œª2Œª3. (8.15)
Both procedures yield the same representation (8.13) where
œÇ0 = Œª1Œª2 + Œª2Œª3 + Œª3Œª1
Œª1Œª2Œª3
‚àí
(Œª1 + Œª2 + Œª3)2
(Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1),
œÇ1 =
1
Œª1Œª2Œª3
‚àí

Œª2
1 + Œª2
2 + Œª2
3 + Œª1Œª2 + Œª2Œª3 + Œª3Œª1

(Œª1 + Œª2 + Œª3)
Œª1Œª2Œª3 (Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1)
,
œÇ2 =
Œª1 + Œª2 + Œª3
Œª1Œª2Œª3 (Œª1 + Œª2) (Œª2 + Œª3) (Œª3 + Œª1).
(8.16)
Thus, the rotation tensor (8.6) can be given by
R = F

œÇ0I + œÇ1C + œÇ2C2
,
(8.17)
where the functions œÇi (i = 0, 1, 2) are given by (8.16) in terms of the principal
stretches Œªi =
‚àö
Œõi, while Œõi (i = 1, 2, 3) denote the eigenvalues of the right
Cauchy-Green tensor C (8.2).

164
8 Applications to Continuum Mechanics
Example. Stretch and rotation tensor in the case of simple shear.
In this loading case the right and left Cauchy-Green tensors take the form (see
Exercise 4.1)
C = Ci
jei ‚äóej,

Ci
j

=
‚é°
‚é£
1
Œ≥
0
Œ≥ 1 + Œ≥2 0
0
0
1
‚é§
‚é¶,
(8.18)
b = bi
jei ‚äóej,

bi
j

=
‚é°
‚é£
1 + Œ≥2 Œ≥ 0
Œ≥
1 0
0
0 1
‚é§
‚é¶
(8.19)
with the eigenvalues
Œõ1/2 = 1 + Œ≥2 ¬±

4Œ≥2 + Œ≥4
2
=
)
Œ≥ ¬±

4 + Œ≥2
2
*2
,
Œõ3 = 1.
(8.20)
For the principal stretches we thus obtain
Œª1/2 =

Œõ1/2 =

4 + Œ≥2 ¬± Œ≥
2
,
Œª3 =

Œõ3 = 1.
(8.21)
The stretch tensors result from (8.11) where
œï0 =
1 +

Œ≥2 + 4
2

Œ≥2 + 4 + Œ≥2 + 4
,
œï1 = 1 +

Œ≥2 + 4
2 +

Œ≥2 + 4
,
œï2 = ‚àí
1
2

Œ≥2 + 4 + Œ≥2 + 4
.
(8.22)
This yields the following result (cf. Exercise 7.2)
U = Ui
jei ‚äóej,

Ui
j

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2

Œ≥2 + 4
Œ≥

Œ≥2 + 4
0
Œ≥

Œ≥2 + 4
Œ≥2 + 2

Œ≥2 + 4
0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(8.23)
v = vi
jei ‚äóej,

vi
j

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ≥2 + 2

Œ≥2 + 4
Œ≥

Œ≥2 + 4
0
Œ≥

Œ≥2 + 4
2

Œ≥2 + 4
0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(8.24)

8.3 The Derivative of the Stretch and Rotation Tensor
165
The rotation tensor can be calculated by (8.17) where
œÇ0 =

Œ≥2 + 4 ‚àí
1
2

Œ≥2 + 4 + Œ≥2 + 4
,
œÇ1 = ‚àí3 +

Œ≥2 + 4 + Œ≥2
2 +

Œ≥2 + 4
,
œÇ2 =
1 +

Œ≥2 + 4
2

Œ≥2 + 4 + Œ≥2 + 4
.
(8.25)
By this means we obtain
R = Ri
¬∑jei ‚äóej,

Ri
¬∑j

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2

Œ≥2 + 4
Œ≥

Œ≥2 + 4
0
‚àí
Œ≥

Œ≥2 + 4
2

Œ≥2 + 4
0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(8.26)
8.3 The Derivative of the Stretch and Rotation Tensor
with Respect to the Deformation Gradient
In continuum mechanics these derivatives are used for the evaluation of the
rate of the stretch and rotation tensor. We begin with a very simple represen-
tation in terms of eigenprojections of the right and left Cauchy-Green tensors
(8.2). Applying the chain rule of diÔ¨Äerentiation and using (6.125) we Ô¨Årst write
U,F = C1/2,C : C,F = C1/2,C :

(I ‚äóF)t + FT ‚äóI

.
(8.27)
Further, taking into account the spectral representation of C (8.4)1 and keep-
ing its symmetry in mind we obtain by virtue of (7.49-7.50)
C1/2,C =
s
	
i,j=1
(Œªi + Œªj)‚àí1 (Pi ‚äóPj)s .
(8.28)
Inserting this result into (8.27) delivers by means of (5.33), (5.47), (5.54)2 and
(5.55)
U,F =
s
	
i,j=1
(Œªi + Œªj)‚àí1 
(Pi ‚äóFPj)t + PiFT ‚äóPj

.
(8.29)
The same procedure applied to the left stretch tensor yields by virtue of
(6.126)

166
8 Applications to Continuum Mechanics
v,F =
s
	
i,j=1
(Œªi + Œªj)‚àí1 
pi ‚äóFTpj + (piF ‚äópj)t
.
(8.30)
Now, applying the product rule of diÔ¨Äerentiation (6.123) to (8.6) and taking
(6.122) into account we write
R,F =

FU‚àí1
,F = I ‚äóU‚àí1 + FU‚àí1,U : U,F
= I ‚äóU‚àí1 ‚àíF

U‚àí1 ‚äóU‚àí1s : U,F .
(8.31)
With the aid of (7.2) and (8.29) this Ô¨Ånally leads to
R,F = I ‚äó
) s
	
i=1
Œª‚àí1
i
Pi
*
‚àíF
s
	
i,j=1
[(Œªi + Œªj) ŒªiŒªj]‚àí1 
(Pi ‚äóFPj)t + PiFT ‚äóPj

.
(8.32)
Note that the eigenprojections Pi and pi (i = 1, 2, . . ., s) are uniquely deÔ¨Åned
by the Sylvester formula (4.54) or its alternative form (7.43) in terms of C and
b, respectively. The functions œÅip appearing in (7.43) are, in turn, expressed
in the unique form by (7.81), (7.83) and (7.85) in terms of the eigenvalues
Œõi = Œª2
i (i = 1, 2, . . ., s).
In order to avoid the direct reference to the eigenprojections one can obtain
the so-called basis-free solutions for U,F, v,F and R,F (see, e.g., [8], [13],
[17], [37], [46], [48]). As a rule, they are given in terms of the stretch and
rotation tensors themselves and require therefore either the explicit polar
decomposition of the deformation gradient or a closed-form representation for
U, v and R like (8.11) and (8.17). In the following we present the basis-free
solutions for U,F, v,F and R,F in terms of the Cauchy-Green tensors C and
b (8.2) and the principal stretches Œªi =
‚àö
Œõi (i = 1, 2, . . ., s). To this end, we
apply the representation (7.38) for the derivative of the square root. Thus, we
obtain instead of (8.28)
C1/2,C =
2
	
p,q=0
Œ∑pq (Cp ‚äóCq)s ,
b1/2,b =
2
	
p,q=0
Œ∑pq (bp ‚äóbq)s ,
(8.33)
where the functions Œ∑pq result from (7.62) by setting again g (Œõi) = ‚àöŒõi = Œªi.
This leads to the following expressions (cf. [17])
Œ∑00 = Œî‚àí1
I5
UIII2
U ‚àíI4
UII2
UIIIU + I3
UII4
U
‚àíI2
UIIIU

3II3
U ‚àí2III2
U

+ 3IUII2
UIII2
U ‚àíIIUIII3
U

,
Œ∑01 = Œ∑10 = Œî‚àí1
I6
UIIIU ‚àíI5
UII2
U ‚àíI4
UIIUIIIU
+2I3
U

II3
U + III2
U

‚àí4I2
UII2
UIIIU + 2IUIIUIII2
U ‚àíIII3
U

,

8.3 The Derivative of the Stretch and Rotation Tensor
167
Œ∑02 = Œ∑20 = Œî‚àí1 
‚àíI4
UIIIU + I3
UII2
U ‚àíI2
UIIUIIIU ‚àíIUIII2
U

,
Œ∑11 = Œî‚àí1
I7
U ‚àí4I5
UIIU + 3I4
UIIIU
+4I3
UII2
U ‚àí6I2
UIIUIIIU + IUIII2
U + II2
UIIIU

,
Œ∑12 = Œ∑21 = Œî‚àí1 
‚àíI5
U + 2I3
UIIU ‚àí2I2
UIIIU + IIUIIIU

,
Œ∑22 = Œî‚àí1 
I3
U + IIIU

,
(8.34)
where
Œî = 2 (IUIIU ‚àíIIIU)3 IIIU
(8.35)
and the principal invariants IU, IIU and IIIU are given by (8.15).
Finally, substitution of (8.33) into (8.27) yields
U,F =
2
	
p,q=0
Œ∑pq

(Cp ‚äóFCq)t + CpFT ‚äóCq
.
(8.36)
Similar we can also write
v,F =
2
	
p,q=0
Œ∑pq

bp ‚äóFTbq + (bpF ‚äóbq)t
.
(8.37)
Inserting further (8.13) and (8.36) into (8.31) we get
R,F = I ‚äó
2
	
p=0
œÇpCp
‚àíF
2
	
p,q,r,t=0
œÇrœÇtŒ∑pq

Cp+r ‚äóFCq+tt + Cp+rFT ‚äóCq+t
,
(8.38)
where œÇp and Œ∑pq (p, q = 0, 1, 2) are given by (8.16) and (8.34), respectively.
The third and fourth powers of C in (8.38) can be expressed by means of the
Cayley-Hamilton equation (4.91):
C3 ‚àíICC2 + IICC ‚àíIIICI = 0.
(8.39)
Composing both sides with C we can also write
C4 ‚àíICC3 + IICC2 ‚àíIIICC = 0.
(8.40)
Thus,
C3 = ICC2 ‚àíIICC + IIICI,

168
8 Applications to Continuum Mechanics
C4 =

I2
C ‚àíIIC

C2 + (IIIC ‚àíICIIC) C + ICIIICI.
(8.41)
Considering these expressions in (8.38) and taking into account that (see, e.g.,
[43])
IC = I2
U ‚àí2IIU,
IIC = II2
U ‚àí2IUIIIU,
IIIC = III2
U
(8.42)
we Ô¨Ånally obtain
R,F = I ‚äó
2
	
p=0
œÇpCp + F
2
	
p,q=0
Œºpq

(Cp ‚äóFCq)t + CpFT ‚äóCq
,
(8.43)
where
Œº00 = Œ• ‚àí1
I6
UIII3
U + 2I5
UII2
UIII2
U ‚àí3I4
UII4
UIIIU ‚àí7I4
UIIUIII3
U
+I3
UII6
U + 8I3
UII3
UIII2
U + 6I3
UIII4
U ‚àí3I2
UII5
UIIIU
‚àí6I2
UII2
UIII3
U + 3IUII4
UIII2
U ‚àíII3
UIII3
U + III5
U

,
Œº01 = Œº10 = Œ• ‚àí1
I7
UIII2
U + I6
UII2
UIIIU ‚àíI5
UII4
U ‚àí6I5
UIIUIII2
U + I4
UII3
UIIIU
+5I4
UIII3
U + 2I3
UII5
U + 4I3
UII2
UIII2
U ‚àí6I2
UII4
UIIIU
‚àí6I2
UIIUIII3
U + 6IUII3
UIII2
U + IUIII4
U ‚àí2II2
UIII3
U

,
Œº02 = Œº20 = ‚àíŒ• ‚àí1
I5
UIII2
U + I4
UII2
UIIIU ‚àíI3
UII4
U ‚àí4I3
UIIUIII2
U
+3I2
UII3
UIIIU + 4I2
UIII3
U ‚àí3IUII2
UIII2
U + IIUIII3
U

,
Œº11 = Œ• ‚àí1
I8
UIIIU + I7
UII2
U ‚àí7I6
UIIUIIIU ‚àí4I5
UII3
U
+5I5
UIII2
U + 16I4
UII2
UIIIU + 4I3
UII4
U ‚àí16I3
UIIUIII2
U
‚àí12I2
UII3
UIIIU + 3I2
UIII3
U + 12IUII2
UIII2
U ‚àí3IIUIII3
U

,
Œº12 = Œº21 = ‚àíŒ• ‚àí1
I6
UIIIU + I5
UII2
U ‚àí5I4
UIIUIIIU ‚àí2I3
UII3
U
+4I3
UIII2
U + 6I2
UII2
UIIIU ‚àí6IUIIUIII2
U + III3
U

,
Œº22 = Œ• ‚àí1IU

I3
UIIIU + I2
UII2
U ‚àí3IUIIUIIIU + 3III2
U

(8.44)
and
Œ• = ‚àí2 (IUIIU ‚àíIIIU)3 III3
U,
(8.45)
while the principal invariants IU, IIU and IIIU are given by (8.15).
The same result for R,F also follows from
R,F =

FU‚àí1
,F = I ‚äóU‚àí1 + FU‚àí1,C : C,F
(8.46)

8.4 Time Rate of Generalized Strains
169
by applying for U‚àí1,C (7.38) and (7.62) where we set g (Œõi) = (Œõi)‚àí1/2 = Œª‚àí1
i
.
Indeed, this yields
C‚àí1/2,C = U‚àí1,C =
2
	
p,q=0
Œºpq (Cp ‚äóCq)s ,
(8.47)
where Œºpq (p, q = 0, 1, 2) are given by (8.44).
8.4 Time Rate of Generalized Strains
Applying the chain rule of diÔ¨Äerentiation we Ô¨Årst write
ÀôE = E,C : ÀôC,
(8.48)
where the superposed dot denotes the material time derivative. The derivative
E,C can be expressed in a simple form in terms of the eigenprojections of E
and C. To this end, we apply (7.49-7.50) taking (7.18) and (8.5) into account
which yields
E,C =
s
	
i,j=1
fij (Pi ‚äóPj)s ,
(8.49)
where
fij =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
f ‚Ä≤ (Œªi)
2Œªi
if i = j,
f (Œªi) ‚àíf (Œªj)
Œª2
i ‚àíŒª2
j
if i Ã∏= j.
(8.50)
A basis-free representation for E,C can be obtained either from (8.49) by
expressing the eigenprojections by (7.43) with (7.81), (7.83) and (7.85) or
directly by using the closed-form solution (7.38) with (7.62), (7.78) and (7.80).
Both procedures lead to the same result as follows (cf. [21], [47]).
E,C =
2
	
p,q=0
Œ∑pq (Cp ‚äóCq)s .
(8.51)
Distinct eigenvalues: Œª1 Ã∏= Œª2 Ã∏= Œª3 Ã∏= Œª1,
Œ∑00 =
3
	
i=1
Œª4
jŒª4
kf ‚Ä≤ (Œªi)
2ŒªiŒî2
i
‚àí
3
	
i,j=1
iÃ∏=j
Œª2
i Œª2
jŒª4
k [f (Œªi) ‚àíf (Œªj)]

Œª2
i ‚àíŒª2
j
3 Œîk
,

170
8 Applications to Continuum Mechanics
Œ∑01 = Œ∑10 = ‚àí
3
	
i=1

Œª2
j + Œª2
k

Œª2
jŒª2
kf ‚Ä≤ (Œªi)
2ŒªiŒî2
i
+
3
	
i,j=1
iÃ∏=j

Œª2
j + Œª2
k

Œª2
i Œª2
k [f (Œªi) ‚àíf (Œªj)]

Œª2
i ‚àíŒª2
j
3 Œîk
,
Œ∑02 = Œ∑20 =
3
	
i=1
Œª2
jŒª2
kf ‚Ä≤ (Œªi)
2ŒªiŒî2
i
‚àí
3
	
i,j=1
iÃ∏=j
Œª2
i Œª2
k [f (Œªi) ‚àíf (Œªj)]

Œª2
i ‚àíŒª2
j
3 Œîk
,
Œ∑11 =
3
	
i=1

Œª2
j + Œª2
k
2 f ‚Ä≤ (Œªi)
2ŒªiŒî2
i
‚àí
3
	
i,j=1
iÃ∏=j

Œª2
j + Œª2
k
 
Œª2
i + Œª2
k

[f (Œªi) ‚àíf (Œªj)]

Œª2
i ‚àíŒª2
j
3 Œîk
,
Œ∑12 = Œ∑21 = ‚àí
3
	
i=1

Œª2
j + Œª2
k

f ‚Ä≤ (Œªi)
2ŒªiŒî2
i
+
3
	
i,j=1
iÃ∏=j

Œª2
i + Œª2
k

[f (Œªi) ‚àíf (Œªj)]

Œª2
i ‚àíŒª2
j
3 Œîk
,
Œ∑22 =
3
	
i=1
f ‚Ä≤ (Œªi)
2ŒªiŒî2
i
‚àí
3
	
i,j=1
iÃ∏=j
f (Œªi) ‚àíf (Œªj)

Œª2
i ‚àíŒª2
j
3 Œîk
,
i Ã∏= j Ã∏= k Ã∏= i,
(8.52)
with
Œîi =

Œª2
i ‚àíŒª2
j
 
Œª2
i ‚àíŒª2
k

,
i Ã∏= j Ã∏= k Ã∏= i = 1, 2, 3.
(8.53)
Double coalescence of eigenvalues: Œªi Ã∏= Œªj = Œªk = Œª,
Œ∑00 = ‚àí2Œª2
i Œª2 f (Œªi) ‚àíf (Œª)
(Œª2
i ‚àíŒª2)3
+ Œª5f ‚Ä≤ (Œªi) + Œª5
i f ‚Ä≤ (Œª)
2ŒªiŒª (Œª2
i ‚àíŒª2)2
,
Œ∑01 = Œ∑10 =

Œª2
i + Œª2 f (Œªi) ‚àíf (Œª)
(Œª2
i ‚àíŒª2)3
‚àíŒª3f ‚Ä≤ (Œªi) + Œª3
i f ‚Ä≤ (Œª)
2ŒªiŒª (Œª2
i ‚àíŒª2)2
,
Œ∑11 = ‚àí2f (Œªi) ‚àíf (Œª)
(Œª2
i ‚àíŒª2)3
+ Œªf ‚Ä≤ (Œªi) + Œªif ‚Ä≤ (Œª)
2ŒªiŒª (Œª2
i ‚àíŒª2)2 ,
Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0.
(8.54)
Triple coalescence of eigenvalues: Œª1 = Œª2 = Œª3 = Œª,
Œ∑00 = f ‚Ä≤ (Œª)
2Œª ,
Œ∑01 = Œ∑10 = Œ∑11 = Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0. (8.55)

8.5 Stress Conjugate to a Generalized Strain
171
8.5 Stress Conjugate to a Generalized Strain
Let E be an arbitrary Lagrangian strain (7.6)1. Assume existence of the so-
called strain energy function œà (E) diÔ¨Äerentiable with respect to E. The sym-
metric tensor
T = œà (E) ,E
(8.56)
is referred to as stress conjugate to E. With the aid of the chain rule it can
be represented by
T = œà (E) ,C : C,E = 1
2S : C,E ,
(8.57)
where S = 2œà (E) ,C denotes the second Piola-KirchhoÔ¨Ästress tensor. The
latter one is deÔ¨Åned in terms of the Cauchy stress œÉ by (see, e.g., [45])
S = det (F) F‚àí1œÉF‚àíT.
(8.58)
Using (8.56) and (7.7) one can also write
Àôœà = T : ÀôE = S : 1
2
ÀôC = S : ÀôE(2).
(8.59)
The fourth-order tensor C,E appearing in (8.57) can be expressed in terms of
the right Cauchy-Green tensor C by means of the relation
Is = E,E = E,C : C,E ,
(8.60)
where the derivative E,C is given by (8.49-8.50). The basis tensors of the latter
representation are
Pij =

(Pi ‚äóPi)s
if i = j,
(Pi ‚äóPj + Pj ‚äóPi)s
if i Ã∏= j.
(8.61)
In view of (4.44), (5.33) and (5.55) they are pairwise orthogonal (see Exercise
8.2) such that (cf. [47])
Pij : Pkl =

Pij
if i = k and j = l or i = l and j = k,
O
otherwise.
(8.62)
By means of (4.46) and (5.84) we can also write
s
	
i,j=1
j‚â•i
Pij =
‚é°
‚é£
) s
	
i=1
Pi
*
‚äó
‚éõ
‚éù
s
	
j=1
Pj
‚éû
‚é†
‚é§
‚é¶
s
= (I ‚äóI)s = Is.
(8.63)
Using these properties we thus obtain

172
8 Applications to Continuum Mechanics
C,E =
s
	
i,j=1
f ‚àí1
ij (Pi ‚äóPj)s ,
(8.64)
where fij (i, j = 1, 2, . . ., s) are given by (8.50). Substituting this result into
(8.57) and taking (5.22)1, (5.46) and (5.47) into account yields [18]
T = 1
2
s
	
i,j=1
f ‚àí1
ij PiSPj.
(8.65)
In order to avoid any reference to eigenprojections we can again express them
by (7.43) with (7.81), (7.83) and (7.85) or alternatively use the closed-form
solution (7.38) with (7.62), (7.78) and (7.80). Both procedures lead to the
following result (cf. [47]).
T =
2
	
p,q=0
Œ∑pqCpSCq.
(8.66)
Distinct eigenvalues: Œª1 Ã∏= Œª2 Ã∏= Œª3 Ã∏= Œª1,
Œ∑00 =
3
	
i=1
Œª4
jŒª4
kŒªi
f ‚Ä≤ (Œªi) Œî2
i
‚àí
3
	
i,j=1
iÃ∏=j
Œª2
i Œª2
jŒª4
k
2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
Œ∑01 = Œ∑10 = ‚àí
3
	
i=1

Œª2
j + Œª2
k

Œª2
jŒª2
kŒªi
f ‚Ä≤ (Œªi) Œî2
i
+
3
	
i,j=1
iÃ∏=j

Œª2
j + Œª2
k

Œª2
i Œª2
k
2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
Œ∑02 = Œ∑20 =
3
	
i=1
Œª2
jŒª2
kŒªi
f ‚Ä≤ (Œªi) Œî2
i
‚àí
3
	
i,j=1
iÃ∏=j
Œª2
i Œª2
k
2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
Œ∑11 =
3
	
i=1

Œª2
j + Œª2
k
2 Œªi
f ‚Ä≤ (Œªi) Œî2
i
‚àí
3
	
i,j=1
iÃ∏=j

Œª2
j + Œª2
k
 
Œª2
i + Œª2
k

2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
Œ∑12 = Œ∑21 = ‚àí
3
	
i=1

Œª2
j + Œª2
k

Œªi
f ‚Ä≤ (Œªi) Œî2
i
+
3
	
i,j=1
iÃ∏=j
Œª2
i + Œª2
k
2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
Œ∑22 =
3
	
i=1
Œªi
f ‚Ä≤ (Œªi) Œî2
i
‚àí
3
	
i,j=1
iÃ∏=j
1
2

Œª2
i ‚àíŒª2
j

[f (Œªi) ‚àíf (Œªj)] Œîk
,
(8.67)

8.6 Finite Plasticity Based on Generalized Strains
173
where i Ã∏= j Ã∏= k Ã∏= i and Œîi are given by (8.53).
Double coalescence of eigenvalues: Œªi Ã∏= Œªj = Œªk = Œª,
Œ∑00 = ‚àí
Œª2
i Œª2
(Œª2
i ‚àíŒª2) [f (Œªi) ‚àíf (Œª)] +
ŒªiŒª
(Œª2
i ‚àíŒª2)2
"
Œª3
f ‚Ä≤ (Œªi) +
Œª3
i
f ‚Ä≤ (Œª)
#
,
Œ∑01 = Œ∑10 =
Œª2
i + Œª2
2 (Œª2
i ‚àíŒª2) [f (Œªi) ‚àíf (Œª)] ‚àí
ŒªiŒª
(Œª2
i ‚àíŒª2)2
"
Œª
f ‚Ä≤ (Œªi) +
Œªi
f ‚Ä≤ (Œª)
#
,
Œ∑11 = ‚àí
1
(Œª2
i ‚àíŒª2) [f (Œªi) ‚àíf (Œª)] +
1
(Œª2
i ‚àíŒª2)2
"
Œªi
f ‚Ä≤ (Œªi) +
Œª
f ‚Ä≤ (Œª)
#
,
Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0.
(8.68)
Triple coalescence of eigenvalues: Œª1 = Œª2 = Œª3 = Œª,
Œ∑00 =
Œª
f ‚Ä≤ (Œª),
Œ∑01 = Œ∑10 = Œ∑11 = Œ∑02 = Œ∑20 = Œ∑12 = Œ∑21 = Œ∑22 = 0. (8.69)
8.6 Finite Plasticity Based on the Additive
Decomposition of Generalized Strains
Keeping in mind the above results regarding generalized strains we are con-
cerned in this section with a thermodynamically based plasticity theory. The
basic kinematic assumption of this theory is the additive decomposition of
generalized strains (7.6) into an elastic part Ee and a plastic part Ep as
E = Ee + Ep.
(8.70)
The derivation of evolution equations for the plastic strain is based on the
second law of thermodynamics and the principle of maximum plastic dissipa-
tion. The second law of thermodynamics can be written in the Clausius-Planck
form as (see, e.g. [45])
D = T : ÀôE ‚àíÀôœà ‚â•0,
(8.71)
where D denotes the dissipation and T is again the stress tensor work conju-
gate to E. Inserting (8.70) into (8.71) we further write
D =
%
T ‚àí‚àÇœà
‚àÇEe
&
: ÀôEe + T : ÀôEp ‚â•0,
(8.72)
where the strain energy is assumed to be a function of the elastic strain as
œà = ÀÜœà (Ee). The Ô¨Årst term in the expression of the dissipation (8.72) depends
solely on the elastic strain rate ÀôEe, while the second one on the plastic strain

174
8 Applications to Continuum Mechanics
rate ÀôEp. Since the elastic and plastic strain rates are independent of each other
the dissipation inequality (8.72) requires that
T = ‚àÇœà
‚àÇEe
.
(8.73)
This leads to the so-called reduced dissipation inequality
D = T : ÀôEp ‚â•0.
(8.74)
Among all admissible processes the real one maximizes the dissipation (8.74).
This statement is based on the postulate of maximum plastic dissipation (see,
e.g., [28]). According to the converse Kuhn-Tucker theorem (see, e.g., [6]) the
suÔ¨Écient conditions of this maximum are written as
ÀôEp = ÀôŒ∂ ‚àÇŒ¶
‚àÇT,
ÀôŒ∂ ‚â•0,
ÀôŒ∂Œ¶ = 0,
Œ¶ ‚â§0,
(8.75)
where Œ¶ represents a convex yield function and ÀôŒ∂ denotes a consistency pa-
rameter. In the following, we will deal with an ideal-plastic isotropic material
described by a von Mises-type yield criterion. Written in terms of the stress
tensor T the von Mises yield function takes the form [31]
Œ¶ = ‚à•devT‚à•‚àí
8
2
3œÉY ,
(8.76)
where œÉY denotes the normal yield stress. With the aid of (6.51) and (6.130)
the evolution equation (8.75)1 can thus be given by
ÀôEp = ÀôŒ∂ ‚à•devT‚à•,T
= ÀôŒ∂ ‚à•devT‚à•,devT : devT,T = ÀôŒ∂ devT
‚à•devT‚à•: Pdev = ÀôŒ∂ devT
‚à•devT‚à•.
(8.77)
Taking the quadratic norm on both the right and left hand side of this identity
delivers the consistency parameter as ÀôŒ∂ = || ÀôEp||. In view of the yield condition
Œ¶ = 0 we thus obtain
devT =
8
2
3œÉY
ÀôEp
999 ÀôEp
999
,
(8.78)
which immediately requires that (see Exercise 1.46)
tr ÀôEp = 0.
(8.79)
In the following, we assume small elastic but large plastic strains and spec-
ify the above plasticity model for Ô¨Ånite simple shear. In this case all three
principal stretches (8.21) are distinct so that we can write by virtue of (7.6)

8.6 Finite Plasticity Based on Generalized Strains
175
ÀôEp = ÀôE =
3
	
i=1
f ‚Ä≤ (Œªi) ÀôŒªiPi +
3
	
i=1
f (Œªi) ÀôPi.
(8.80)
By means of the identities trPi = 1 and tr ÀôPi = 0 following from (4.61) and
(4.62) where ri = 1 (i = 1, 2, 3) the condition (8.79) requires that
3
	
i=1
f ‚Ä≤ (Œªi) ÀôŒªi = 0.
(8.81)
In view of (8.21) it leads to the equation
f ‚Ä≤ (Œª) ‚àíf ‚Ä≤ 
Œª‚àí1
Œª‚àí2 = 0,
‚àÄŒª > 0,
(8.82)
where we set Œª1 = Œª and consequently Œª2 = Œª‚àí1. Solutions of this equations
can be given by [21]
fa (Œª) =
‚éß
‚é™
‚é®
‚é™
‚é©
1
2a (Œªa ‚àíŒª‚àía)
for a Ã∏= 0,
ln Œª
for a = 0.
(8.83)
By means of (7.6)1 or (7.18)1 the functions fa (8.83) yield a set of new gen-
eralized strain measures
E‚ü®a‚ü©=
‚éß
‚é™
‚é®
‚é™
‚é©
1
2a (Ua ‚àíU‚àía) = 1
2a

Ca/2 ‚àíC‚àía/2
for a Ã∏= 0,
ln U = 1
2 ln C
for a = 0,
(8.84)
among which only the logarithmic one (a = 0) belongs to Seth‚Äôs family (7.7).
Henceforth, we will deal only with the generalized strains (8.84) as able to
provide the traceless deformation rate (8.79). For these strains eq. (8.78) takes
the form
devT‚ü®a‚ü©=
8
2
3œÉY
ÀôE‚ü®a‚ü©
999 ÀôE‚ü®a‚ü©
999
,
(8.85)
where T‚ü®a‚ü©denotes the stress tensor work conjugate to E‚ü®a‚ü©. T‚ü®a‚ü©itself has
no physical meaning and should be transformed to the Cauchy stresses. With
the aid of (8.57), (8.58) and (8.60) we can write
œÉ =
1
detFFSFT =
1
detFF

T‚ü®a‚ü©: Pa

FT,
(8.86)
where
Pa = 2E‚ü®a‚ü©,C
(8.87)

176
8 Applications to Continuum Mechanics
can be expressed either by (8.49-8.50) or by (8.51-8.55). It is seen that this
fourth-order tensor is super-symmetric (see Exercise 5.11), so that T‚ü®a‚ü©: Pa =
Pa : T‚ü®a‚ü©. Thus, by virtue of (1.152) and (1.153) representation (8.86) can be
rewritten as
œÉ =
1
detFF

Pa : T‚ü®a‚ü©
FT
=
1
detFF
"
Pa : devT‚ü®a‚ü©+ 1
3trT‚ü®a‚ü©(Pa : I)
#
FT.
(8.88)
With the aid of the relation
Pa : I = 2 d
dtE‚ü®a‚ü©(C + tI)

t=0
= 2 d
dt
3
	
i=1
fa
%
Œª2
i + t
&
Pi

t=0
=
3
	
i=1
f ‚Ä≤
a (Œªi) Œª‚àí1
i Pi
(8.89)
following from (6.111) and taking (8.83) into account one obtains
F (Pa : I) FT = 1
2F

Ca/2‚àí1 + C‚àía/2‚àí1
FT = 1
2

ba/2 + b‚àía/2
.
Inserting this result into (8.88) yields
œÉ =
1
detFF

Pa : devT‚ü®a‚ü©
FT + ÀÜœÉ
(8.90)
with the abbreviation
ÀÜœÉ = trT‚ü®a‚ü©
6 detF

ba/2 + b‚àía/2
.
(8.91)
Using the spectral decomposition of b by (8.4) and taking into account that
in the case of simple shear detF = 1 we can further write
ÀÜœÉ = 1
6trT‚ü®a‚ü©
Œªa + Œª‚àía
(p1 + p2) + 2p3

,
(8.92)
where Œª is given by (8.21). Thus, in the 1-2 shear plane the stress tensor ÀÜœÉ
has the double eigenvalue 1
6trT‚ü®a‚ü©(Œªa + Œª‚àía) and causes equibiaxial tension
or compression. Hence, in this plane the component ÀÜœÉ (8.91) is shear free and
does not inÔ¨Çuence the shear stress response. Inserting (8.85) into (8.90) and
taking (8.18) and (8.48) into account we Ô¨Ånally obtain
œÉ =
8
2
3œÉY F
"Pa : Pa : A
‚à•Pa : A‚à•
#
FT + ÀÜœÉ,
(8.93)
where

8.6 Finite Plasticity Based on Generalized Strains
177
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
1
2
3
4
5
6
7
8
9
10
Normalized shear stress œÉ12/œÑY
Amount of shear Œ≥
expected (multiplicative plasticity)
additive logarithmic plasticity (a = 0)
additive generalized plasticity, a = 0.5
additive generalized plasticity, a = 0.8
additive generalized plasticity, a = 1
additive generalized plasticity, a = 2
Fig. 8.1. Simple shear of an ideal-plastic material: shear stress responses based on
the additive decomposition of generalized strains
A = 1
2 ÀôŒ≥
ÀôC =
‚é°
‚é£
0
1/2 0
1/2 Œ≥
0
0
0
0
‚é§
‚é¶ei ‚äóej.
(8.94)
Of particular interest is the shear stress œÉ12 as a function of the amount
of shear Œ≥. Inserting (8.51-8.52) and (8.87) into (8.93) we obtain after some
algebraic manipulations
œÉ12
œÑY
=
2

(4 + Œ≥2) Œì 2f ‚Ä≤a
2 (Œì) + 4f 2a (Œì)
4 + Œ≥2
,
(8.95)
where
Œì = Œ≥
2 +

4 + Œ≥2
2
(8.96)
and œÑY = œÉY /
‚àö
3 denotes the shear yield stress. Equation (8.95) is illustrated
graphically in Fig. 8.1 for several values of the parameter a. Since the presented

178
8 Applications to Continuum Mechanics
plasticity model considers neither softening nor hardening and is restricted to
small elastic strains a constant shear stress response even at large plastic
deformations is expected. It is also predicted by a plasticity model based on
the multiplicative decomposition of the deformation gradient (see, e.g., [21]
for more details). The plasticity model based on the additive decomposition
of generalized strains exhibits, however, a non-constant shear stress for all
examined values of a. This restricts the applicability of this model to moderate
plastic shears. Indeed, in the vicinity of the point Œ≥ = 0 the power series
expansion of (8.95) takes the form
œÉ12
œÑY
= 1 + 1
4a2Œ≥2 +
% 1
16a4 ‚àí3
4a2 ‚àí1
&
Œ≥4 + O

Œ≥6
.
(8.97)
Thus, in the case of simple shear the amount of shear is limited for the log-
arithmic strain (a = 0) by Œ≥4 ‚â™1 and for other generalized strain measures
by Œ≥2 ‚â™1.
Exercises
8.1. The deformation gradient is given by F = Fi
¬∑jei ‚äóej, where

Fi
¬∑j

=
‚é°
‚é£
1 2 0
‚àí2 2 0
0 0 1
‚é§
‚é¶.
Evaluate the stretch tensors U and v and the rotation tensor R using (8.11-
8.12) and (8.16-8.17).
8.2. Prove the orthogonality (8.62) of the basis tensors (8.61) using (4.44),
(5.33) and (5.55).

Solutions
Exercises of Chapter 1
1.1 (a) (A.4), (A.3):
0 = 0 + (‚àí0) = ‚àí0.
(b) (A.2-A.4), (B.3):
Œ±0 = 0 + Œ±0 = Œ±x + (‚àíŒ±x) + Œ±0
= Œ± (0 + x) + (‚àíŒ±x) = Œ±x + (‚àíŒ±x) = 0.
(c) (A.2-A.4), (B.4):
0x = 0x + 0 = 0x + 0x + (‚àí0x) = 0x + (‚àí0x) = 0,
‚àÄx ‚ààV.
(d) (A.2-A.4), (B.2), (B.4), (c):
(‚àí1) x = (‚àí1) x + 0 = (‚àí1) x + x + (‚àíx)
= (‚àí1 + 1) x + (‚àíx) = 0x + (‚àíx) = 0 + (‚àíx) = ‚àíx,
‚àÄx ‚ààV.
(e) If, on the contrary, Œ± Ã∏= 0 and x Ã∏= 0, then according to (b), (B.1), (B.2):
0 = Œ±‚àí10 = Œ±‚àí1 (Œ±x) = x.
1.2 Let, on the contrary, xk = 0 for some k. Then, (n
i=1 Œ±ixi = 0, where
Œ±k = 1, Œ±i = 0, i = 1, . . . , k ‚àí1, k + 1, . . . , n.
1.3
If, on the contrary, for some k < n: (k
i=1 Œ±ixi = 0, where not all
Œ±i, (i = 1, 2, . . . , k) are zero, then we can also write: (n
i=1 Œ±ixi = 0, where
Œ±i = 0, for i = k + 1, . . . , n.
1.4 (a) Œ¥i
jaj = Œ¥i
1a1 + Œ¥i
2a2 + Œ¥i
3a3 = ai,
(b) Œ¥ijxixj = Œ¥11x1x1 + Œ¥12x1x2 + . . . + Œ¥33x3x3 = x1x1 + x2x2 + x3x3,
(c) Œ¥i
i = Œ¥1
1 + Œ¥2
2 + Œ¥3
3 = 3,

180
Solutions
(d) ‚àÇfi
‚àÇxj dxj = ‚àÇfi
‚àÇx1 dx1 + ‚àÇfi
‚àÇx2 dx2 + ‚àÇfi
‚àÇx3 dx3.
1.5 (A.4), (C.2), (C.3), Ex. 1.1 (d):
0 ¬∑ x = [x + (‚àíx)] ¬∑ x = [x + (‚àí1) x] ¬∑ x = x ¬∑ x ‚àíx ¬∑ x = 0.
1.6
Let on the contrary (m
i=1 Œ±igi = 0, where not all Œ±i (i = 1, 2, . . ., m)
are zero. Multiplying scalarly by gj we obtain: 0 = gj ¬∑ ((m
i=1 Œ±igi). Since
gi ¬∑ gj = 0 for i Ã∏= j, we can write: Œ±jgj ¬∑ gj = 0 (j = 1, 2, . . ., m). The fact
that the vectors gj are non-zero leads in view of (C.4) to the conclusion that
Œ±j = 0 (j = 1, 2, . . . , m) which contradicts the earlier assumption.
1.7 (1.6), (C.1), (C.2):
‚à•x + y‚à•2 = (x + y) ¬∑ (x + y)
= x ¬∑ x + x ¬∑ y + y ¬∑ x + y ¬∑ y = ‚à•x‚à•2 + 2x ¬∑ y + ‚à•y‚à•2 .
1.8
Since G = {g1, g2, . . . , gn} is a basis we can write a = aigi. Then,
a ¬∑ a = ai (gi ¬∑ a). Thus, if a ¬∑ gi = 0 (i = 1, 2, . . ., n), then a ¬∑ a = 0 and
according to (C.4) a = 0 (suÔ¨Éciency). Conversely, if a = 0, then (see Exercise
1.5) a ¬∑ gi = 0 (i = 1, 2, . . . , n) (necessity).
1.9 Necessity. (C.2): a ¬∑ x = b ¬∑ x ‚áía ¬∑ x ‚àíb ¬∑ x = (a ‚àíb) ¬∑ x = 0, ‚àÄx ‚ààEn.
Let x = a ‚àíb, then (a ‚àíb) ¬∑ (a ‚àíb) = 0 and according to (C.4) a ‚àíb = 0.
This implies that a = b. The suÔ¨Éciency is evident.
1.10 (a) Orthonormal vectors e1, e2 and e3 can be calculated by means of
the Gram-Schmidt procedure (1.10-1.12) as follows
e1 =
g1
‚à•g1‚à•=
‚éß
‚é®
‚é©
‚àö
2/2
‚àö
2/2
0
‚é´
‚é¨
‚é≠,
e‚Ä≤
2 = g2 ‚àí(g2 ¬∑ e1) e1 =
‚éß
‚é®
‚é©
1/2
‚àí1/2
‚àí2
‚é´
‚é¨
‚é≠,
e2 =
e‚Ä≤
2
‚à•e‚Ä≤
2‚à•=
‚éß
‚é®
‚é©
‚àö
2/6
‚àí
‚àö
2/6
‚àí2
‚àö
2/3
‚é´
‚é¨
‚é≠,
e‚Ä≤
3 = g3‚àí(g3 ¬∑ e2) e2‚àí(g3 ¬∑ e1) e1 =
‚éß
‚é®
‚é©
10/9
‚àí10/9
5/9
‚é´
‚é¨
‚é≠, e3 =
e‚Ä≤
3
‚à•e‚Ä≤
3‚à•=
‚éß
‚é®
‚é©
2/3
‚àí2/3
1/3
‚é´
‚é¨
‚é≠.
(b) First, we calculate the matrices [gij] and

gij
by (1.25)2 and (1.24)
[gij] = [gi ¬∑ gj] =
‚é°
‚é£
2 3 6
3 9 8
6 8 21
‚é§
‚é¶,

gij
= [gij]‚àí1 =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
5 ‚àí3
5 ‚àí6
5
‚àí3
5
6
25
2
25
‚àí6
5
2
25
9
25
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,

Solutions
181
With the aid of (1.21) we thus obtain
g1 = g11g1 + g12g2 + g13g3 =
‚éß
‚é®
‚é©
‚àí1
2
0
‚é´
‚é¨
‚é≠,
g2 = g21g1 + g22g2 + g23g3 =
‚éß
‚é®
‚é©
1/5
‚àí1/5
‚àí2/5
‚é´
‚é¨
‚é≠,
g3 = g31g1 + g32g2 + g33g3 =
‚éß
‚é®
‚é©
2/5
‚àí2/5
1/5
‚é´
‚é¨
‚é≠.
(c) By virtue of (1.35) we write
g =
Œ≤i
j
 =

1 1
0
2 1 ‚àí2
4 2
1

= ‚àí5.
Applying (1.33) we further obtain with the aid of (1.46)
g1 = g‚àí1g2 √ó g3 = ‚àí1
5

2
1 ‚àí2
4
2
1
a1 a2 a3

= ‚àía1 + 2a2,
g2 = g‚àí1g3 √ó g1 = ‚àí1
5

4
2
1
1
1
0
a1 a2 a3

= 1
5 (a1 ‚àía2 ‚àí2a3) ,
g3 = g‚àí1g1 √ó g2 = ‚àí1
5

1
1
0
2
1 ‚àí2
a1 a2 a3

= 1
5 (2a1 ‚àí2a2 + a3) ,
where ai denote the orthonormal basis the components of the original vectors
gi (i = 1, 2, 3) are related to.
1.11 Let on the contrary (3
i=1 Œ±igi = 0, where not all Œ±i are zero. Multi-
plying scalarly by gj we obtain by virtue of (1.15): 0 = gj ¬∑
(3
i=1 Œ±igi
=
(3
i=1 Œ±iŒ¥i
j = Œ±j (j = 1, 2, 3).
1.12 Similarly to (1.35) we write using also (1.18), (1.19) and (1.36)

g1g2g3
=

Œ±1
i eiŒ±2
jejŒ±3
kek
= Œ±1
i Œ±2
jŒ±3
k

eiejek
= Œ±1
i Œ±2
jŒ±3
keijk =
Œ±i
j
 =
Œ≤i
j
‚àí1 = g‚àí1.
1.13 In view of (1.33), (1.36) and (1.38) the left and right hand side of (1.39)
yield for the case

182
Solutions
i = j:
gi √ó gi = 0,
eiikggk = 0,
‚àÉk such that ijk is an even permutation of 123:
gi √ó gj = [g1g2g3] gk,
eijlggl = [g1g2g3] gk,
‚àÉk such that ijk is an odd permutation of 123:
gi √ó gj = ‚àígj √ó gi = ‚àí[g1g2g3] gk,
eijlggl = ‚àí[g1g2g3] gk.
Using the result of the previous exercise one proves in the same manner (1.42).
1.14 (a) Œ¥ijeijk = Œ¥11e11k + Œ¥12e12k + . . . + Œ¥33e33k = 0.
(b) Writing out the term eikmejkm we Ô¨Årst obtain
eikmejkm = ei11ej11 + ei12ej12 + . . . + ei33ej33
= ei12ej12 + ei21ej21 + ei13ej13 + ei31ej31 + ei32ej32 + ei23ej23.
For i Ã∏= j each term in this sum is equal to zero. Let further i = j = 1.
Then we obtain ei12ej12 + ei21ej21 + ei13ej13 + ei31ej31 + ei32ej32 + ei23ej23 =
e132e132 + e123e123 = (‚àí1) (‚àí1) + 1 ¬∑ 1 = 2. The same result also holds for the
cases i = j = 2 and i = j = 3. Thus, we can write eikmejkm = 2Œ¥i
j.
(c) By means of the previous result (b) we can write: eijkeijk = 2Œ¥i
i = 2(Œ¥1
1 +
Œ¥2
2 + Œ¥3
3) = 6. This can also be shown directly by
eijkeijk = e123e123 + e132e132 + e213e213 + e231e231 + e312e312 + e321e321
= 1 ¬∑ 1 + (‚àí1) ¬∑ (‚àí1) + (‚àí1) ¬∑ (‚àí1) + 1 ¬∑ 1 + 1 ¬∑ 1 + (‚àí1) ¬∑ (‚àí1) = 6.
(d) eijmeklm = eij1ekl1 + eij2ekl2 + eij3ekl3. It is seen that in the case i = j
or k = l this sum as well as the right hand side Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k are both zero. Let
further i Ã∏= j. Then, only one term in the above sum is non-zero if k = i and
l = j or vice versa l = i and k = j. In the Ô¨Årst case the left hand side is 1 and
in the last case ‚àí1. The same holds also for the right side Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k. Indeed,
we obtain for k = i Ã∏= l = j: Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k = 1 ¬∑ 1 ‚àí0 = 1 and for l = i Ã∏= k = j:
Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k = 0 ‚àí1 ¬∑ 1 = ‚àí1.
1.15 Using the representations a = aigi, b = bjgj and c = clgl we can write
by virtue of (1.39) and (1.42)
(a √ó b) √ó c =

aigi

√ó

bjgj

√ó c =

aibjeijkggk
√ó

clgl
= aibjcleijkeklmgm = aibjcleijkelmkgm.
With the aid of the identity eijmeklm = Œ¥i
kŒ¥j
l ‚àíŒ¥i
lŒ¥j
k (Exercise 1.14) we Ô¨Ånally
obtain

Solutions
183
(a √ó b) √ó c = aibjcl

Œ¥l
iŒ¥m
j ‚àíŒ¥m
i Œ¥l
j

gm = aibjclŒ¥l
iŒ¥m
j gm ‚àíaibjclŒ¥m
i Œ¥l
jgm
= aibjcigj ‚àíaibjcjgi = (a ¬∑ c) b ‚àí(b ¬∑ c) a.
1.16 (A.2-A.4), (1.48):
0 = Ax + (‚àíAx) = A (x + 0) + (‚àíAx) = Ax + A0 + (‚àíAx) = A0.
1.17 (1.49), Exercises 1.1(c), 1.16: (0A) x = A (0x) = A0 = 0, ‚àÄx ‚ààEn.
1.18 (1.61), Ex. 1.17: A + (‚àíA) = A + (‚àí1) A = (1 ‚àí1) A = 0A = 0.
1.19 Indeed, a scalar product of the right-hand side of (1.80) with an arbitrary
vector x yields [(y ¬∑ a) b]¬∑x = (y ¬∑ a) (b ¬∑ x). The same result follows also from
y ¬∑ [(a ‚äób) x] = (y ¬∑ a) (b ¬∑ x) , ‚àÄx, y ‚ààEn for the left-hand side. This implies
that the identity (1.80) is true (see Exercise 1.9).
1.20 For (1.83)1 we have for example
giAgj = gi 
Aklgk ‚äógl

gj = Akl 
gi ¬∑ gk
 
gl ¬∑ gj
= AklŒ¥i
kŒ¥j
l = Aij.
1.21 For an arbitrary vector x = xigi ‚ààE3 we can write using (1.28), (1.39)
and (1.75)
Wx = w √ó x =

wigi

√ó

xjgj

= eijkgwixjgk = eijkgwi 
x ¬∑ gj
gk = eijkgwi 
gk ‚äógj
x.
Comparing the left and right hand side of this equality we obtain
W = eijkgwigk ‚äógj,
(S.1)
so that the components of W = Wkjgk ‚äógj can be given by Wkj = eijkgwi
or in the matrix form as
[Wkj] = g

eijkwi
= g
‚é°
‚é£
0
‚àíw3 w2
w3
0
‚àíw1
‚àíw2 w1
0
‚é§
‚é¶.
This yields also an alternative representation for Wx as follows
Wx = g

w2x3 ‚àíw3x2
g1 +

w3x1 ‚àíw1x3
g2 +

w1x2 ‚àíw2x1
g3
.
It is seen that the tensor W is skew-symmetric because WT = ‚àíW.
1.22 According to (1.71) we can write
R = cos Œ±I + sin Œ±ÀÜe3 + (1 ‚àícos Œ±) (e3 ‚äóe3) .
Thus, an arbitrary vector a = aiei in E3 is rotated to Ra = cos Œ±

aiei

+
sin Œ±e3 √ó

aiei

+ (1 ‚àícos Œ±) a3e3. By virtue of (1.45) we can further write

184
Solutions
Ra = cos Œ±

aiei

+ sin Œ±

a1e2 ‚àía2e1

+ (1 ‚àícos Œ±) a3e3
=

a1 cos Œ± ‚àía2 sin Œ±

e1 +

a1 sin Œ± + a2 cos Œ±

e2 + a3e3.
Thus, the rotation tensor can be given by R = Rijei ‚äóej, where

Rij
=
‚é°
‚é£
cos Œ± ‚àísin Œ± 0
sin Œ± cos Œ± 0
0
0
1
‚é§
‚é¶.
1.23 With the aid of (1.83) and (1.92) we obtain

Ai
¬∑j

=

Aikgkj

=

Aik
[gkj] =
‚é°
‚é£
0 ‚àí1 0
0 0 0
1 0 0
‚é§
‚é¶
‚é°
‚é£
2 3 6
3 9 8
6 8 21
‚é§
‚é¶=
‚é°
‚é£
‚àí3 ‚àí9 ‚àí8
0
0
0
2
3
6
‚é§
‚é¶,

A j
i¬∑

=

gikAkj
= [gik]

Akj
=
‚é°
‚é£
2 3 6
3 9 8
6 8 21
‚é§
‚é¶
‚é°
‚é£
0 ‚àí1 0
0 0 0
1 0 0
‚é§
‚é¶=
‚é°
‚é£
6 ‚àí2
0
8 ‚àí3
0
21 ‚àí6
0
‚é§
‚é¶,
[Aij] =

gikAk
¬∑j

= [gik]

Ak
¬∑j

=

A k
i¬∑gkj

=

A k
i¬∑

[gkj]
=
‚é°
‚é£
2 3 6
3 9 8
6 8 21
‚é§
‚é¶
‚é°
‚é£
‚àí3 ‚àí9 ‚àí8
0
0
0
2
3
6
‚é§
‚é¶=
‚é°
‚é£
6 ‚àí2
0
8 ‚àí3
0
21 ‚àí6
0
‚é§
‚é¶
‚é°
‚é£
2 3 6
3 9 8
6 8 21
‚é§
‚é¶=
‚é°
‚é£
6
0 20
7 ‚àí3 24
24
9 78
‚é§
‚é¶.
1.24 By means of (1.53), (1.84), (1.98), Exercise 1.16 we can write
(A0) x = A (0x) = A0 = 0,
(0A) x = 0 (Ax) = 0,
(AI) x = A (Ix) = Ax,
(IA) x = I (Ax) = Ax,
A (BC) x = A [B (Cx)] = (AB) (Cx) = [(AB) C] x,
‚àÄx ‚ààEn.
1.25 To check the commutativeness of the tensors A and B we compute the
components of the tensor AB ‚àíBA:

(AB ‚àíBA)i
¬∑j

=

Ai
¬∑kBk
¬∑j ‚àíBi
¬∑kAk
¬∑j

=

Ai
¬∑k
 
Bk
¬∑j

‚àí

Bi
¬∑k
 
Ak
¬∑j

=
‚é°
‚é£
0 2 0
0 0 0
0 0 0
‚é§
‚é¶
‚é°
‚é£
0 0 0
0 0 0
0 0 1
‚é§
‚é¶‚àí
‚é°
‚é£
0 0 0
0 0 0
0 0 1
‚é§
‚é¶
‚é°
‚é£
0 2 0
0 0 0
0 0 0
‚é§
‚é¶=
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶.
Similar we also obtain

(AC ‚àíCA)i
¬∑j

=
‚é°
‚é£
0 ‚àí2 0
0
0 0
0
0 0
‚é§
‚é¶,

(AD ‚àíDA)i
¬∑j

=
‚é°
‚é£
0 ‚àí1 0
0
0 0
0
0 0
‚é§
‚é¶,

Solutions
185

(BC ‚àíCB)i
¬∑j

=
‚é°
‚é£
0 0 ‚àí3
0 0
0
0 1
0
‚é§
‚é¶,

(BD ‚àíDB)i
¬∑j

=
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶,

(CD ‚àíDC)i
¬∑j

=
‚é°
‚é£
0
1 ‚àí27
0
0
0
0 19/2
0
‚é§
‚é¶.
Thus, A commutes with B while B also commutes with D.
1.26 Taking into account commutativeness of A and B we obtain for example
for k = 2
(A + B)2 = (A + B) (A + B) = A2 + AB+ BA+ B2 = A2 + 2AB+ B2.
Generalizing this result for k = 2, 3, . . . we obtain using the Newton formula
(A + B)k =
k
	
i=0
%k
i
&
Ak‚àíiBi,
where
%k
i
&
=
k!
i! (k ‚àíi)!.
(S.2)
1.27 Using the result of the previous Exercise we Ô¨Årst write out the left hand
side of (1.158) by
exp (A + B) =
‚àû
	
k=0
(A + B)k
k!
=
‚àû
	
k=0
1
k!
k
	
i=0
%k
i
&
Ak‚àíiBi
=
‚àû
	
k=0
1
k!
k
	
i=0
k!
i! (k ‚àíi) !Ak‚àíiBi =
‚àû
	
k=0
k
	
i=0
Ak‚àíiBi
i! (k ‚àíi) !.
Changing the summation order as shown in Fig. S.1 and using the abbre-
viation l = k ‚àíi it yields
exp (A + B) =
‚àû
	
l=0
‚àû
	
i=0
AlBi
i!l! .
The same expression can alternatively be obtained by applying formally the
Cauchy product of inÔ¨Ånite series (see e.g. [23]). For the right hand side of
(1.158) we Ô¨Ånally get the same result as above:
exp (A) exp (B) =
) ‚àû
	
l=0
Al
l!
* ) ‚àû
	
i=0
Bi
i!
*
=
‚àû
	
l=0
‚àû
	
i=0
AlBi
l!i! .
1.28 Using the result of the previous Exercise we can write
exp (kA) = exp [(k ‚àí1) A + A] = exp [(k ‚àí1) A] exp (A)
= exp [(k ‚àí2) A] [exp (A)]2 = . . .
= exp (A) [exp (A)]k‚àí1 = [exp (A)]k .

186
Solutions
0
1
2
3
0
1
2
3
¬∑ ¬∑ ¬∑
‚àû
i = k
k
2
1
3
0
3
2
1
0
i = k
¬∑ ¬∑ ¬∑
‚àû
‚àû
k
i
i
¬∑ ¬∑ ¬∑
‚àû
‚àû
...
...
summation
summation
area
area
Fig. S.1. Geometric illustration of the summation order
1.29 Using the deÔ¨Ånition of the exponential tensor function (1.109) we get
exp (0) =
‚àû
	
k=0
0k
k! = I + 0 + 0 + . . . = I,
exp (I) =
‚àû
	
k=0
Ik
k! =
‚àû
	
k=0
I
k! = I
‚àû
	
k=0
1
k! = exp (1) I = eI.
1.30 Since the tensors A and ‚àíA commute we can write
exp (A) exp (‚àíA) = exp (‚àíA) exp (A) = exp [A + (‚àíA)] = exp (0) = I.
1.31 (1.109), (S.2):
exp (A + B) =
‚àû
	
k=0
(A + B)k
k!
=
‚àû
	
k=0
Ak + Bk
k!
= exp (A) + exp (B).
1.32 (1.109), (1.129):
exp

QAQT
=
‚àû
	
k=0
1
k!

QAQTk
=
‚àû
	
k=0
1
k! QAQTQAQT . . . QAQT



k times
=
‚àû
	
k=0
1
k!QAkQT = Q
) ‚àû
	
k=0
1
k!Ak
*
QT = Q exp (A)QT.
1.33 We begin with the power of the tensor D.

Solutions
187
D2 = DD =

Di
¬∑jgi ‚äógj 
Dk
¬∑lgk ‚äógl
= Di
¬∑jDk
¬∑lŒ¥j
kgi ‚äógl = Di
¬∑jDj
¬∑lgi ‚äógl =

D2i
¬∑j gi ‚äógj,
where

D2i
¬∑j

=

Di
¬∑j
 
Di
¬∑j

. Generalizing this results for an arbitrary integer
exponent yields

(Dm)i
¬∑j

=

Di
¬∑j

. . .

Di
¬∑j




m times
=
‚é°
‚é£
2m 0
0
0 3m 0
0
0 1m
‚é§
‚é¶.
We observe that the composition of tensors represented by mixed components
related to the same mixed basis can be expressed in terms of the product of
the component matrices. With this result in hand we thus obtain
exp (D) =
‚àû
	
m=0
Dm
m! = exp (D)i
¬∑j gi ‚äógj,
where

exp (D)i
¬∑j

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àû
(
m=0
2m
m!
0
0
0
‚àû
(
m=0
3m
m!
0
0
0
‚àû
(
m=0
1m
m!
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
=
‚é°
‚é£
e2 0 0
0 e3 0
0 0 e
‚é§
‚é¶.
For the powers of the tensor E we further obtain
Ek = 0, k = 2, 3 . . .
Hence,
exp (E) =
‚àû
	
m=0
Em
m! = I + E + 0 + 0 + . . . = I + E,
so that

exp (E)i
¬∑j

=
‚é°
‚é£
1 1 0
0 1 0
0 0 1
‚é§
‚é¶.
To express the exponential of the tensor F we Ô¨Årst decompose it by F = X+Y,
where

Xi
¬∑j

=
‚é°
‚é£
0 0 0
0 0 0
0 0 1
‚é§
‚é¶,

Yi
¬∑j

=
‚é°
‚é£
0 2 0
0 0 0
0 0 0
‚é§
‚é¶.

188
Solutions
X and Y are commutative since XY = YX = 0. Hence,
exp (F) = exp (X + Y) = exp (X) exp (Y) .
Noticing that X has the form of D and Y that of E we can write

exp (X)i
¬∑j

=
‚é°
‚é£
1 0 0
0 1 0
0 0 e
‚é§
‚é¶,

exp (Y)i
¬∑j

=
‚é°
‚é£
1 2 0
0 1 0
0 0 1
‚é§
‚é¶.
Finally, we obtain

exp (F)i
¬∑j

=

exp (X)i
¬∑j
 
exp (Y)i
¬∑j

=
‚é°
‚é£
1 0 0
0 1 0
0 0 e
‚é§
‚é¶
‚é°
‚é£
1 2 0
0 1 0
0 0 1
‚é§
‚é¶=
‚é°
‚é£
1 2 0
0 1 0
0 0 e
‚é§
‚é¶.
1.34 (1.115): (ABCD)T = (CD)T (AB)T = DTCTBTAT.
1.35 Using the result of the previous Exercise we can write
(AA . . . A



k times
)T = ATAT . . . AT



k times
=

ATk .
1.36 According to (1.119) and (1.120) Bij = Aji, Bij = Aji, B j
i¬∑ = Aj
¬∑i and
Bi
¬∑j = A i
j¬∑ so that (see Exercise 1.23)

Bij
=

AijT =
‚é°
‚é£
0 0 1
‚àí1 0 0
0 0 0
‚é§
‚é¶,
[Bij] = [Aij]T =
‚é°
‚é£
6
7 24
0 ‚àí3
9
20 24 78
‚é§
‚é¶,

B j
i¬∑

=

Ai
¬∑j
T =
‚é°
‚é£
‚àí3 0 2
‚àí9 0 3
‚àí8 0 6
‚é§
‚é¶,

Bi
¬∑j

=

A j
i¬∑
T
=
‚é°
‚é£
6
8
21
‚àí2 ‚àí3 ‚àí6
0
0
0
‚é§
‚é¶.
1.37 (1.115), (1.121), (1.125):
I = IT =

AA‚àí1T =

A‚àí1T AT.
1.38

Ak‚àí1 is the tensor satisfying the identity

Ak‚àí1 Ak = I. On the
other hand,

A‚àí1k Ak = A‚àí1A‚àí1 . . . A‚àí1



k times
AA . . . A



k times
= I. Thus,

A‚àí1k =

Ak‚àí1.
1.39
An arbitrary tensor A ‚ààLinn can be represented with respect to a
basis for example by A = Aijgi ‚äógj. Thus, by virtue of (1.134) we obtain:
c ‚äód : A = c ‚äód :

Aijgi ‚äógj

= Aij (c ¬∑ gi) (gj ¬∑ d)
= c

Aijgi ‚äógj

d = cAd = dATc.

Solutions
189
1.40 The properties (D.1) and (D.3) directly follow from (1.134) and (1.136).
Further, for three arbitrary tensors A, B = Bijgi ‚äógj and C = Cijgi ‚äógj
we have with the aid of (1.135)
A : (B + C) = A :

Bij + Cij
(gi ‚äógj)

=

Bij + Cij
(giAgj)
= Bij (giAgj) + Cij (giAgj)
= A :

Bijgi ‚äógj

+ A :

Cijgi ‚äógj

= A : B + A : C,
which implies (D.2).
1.41 By virtue of (1.103), (1.84) and (1.135) we obtain
[(a ‚äób) (c ‚äód)] : I = [(b ¬∑ c) (a ‚äód)] : I
= (b ¬∑ c) (aId) = (a ¬∑ d) (b ¬∑ c) .
1.42 By virtue of (1.23), (1.82), (1.86), (1.91) and (1.134) we can write
trA = A : I =

Aijgi ‚äógj

:

gk ‚äógk
= Aij (gi ¬∑ gk)

gj ¬∑ gk
= AijgikŒ¥k
j = Aijgij = Ai
¬∑lgljgij = Ai
¬∑lŒ¥l
i = Ai
¬∑i = Ajigji.
1.43 (1.140): M : W = MT : WT = M : (‚àíW) = ‚àí(M : W) = 0.
1.44
Wk is skew-symmetric for odd k. Indeed,

WkT =

WTk =
(‚àíW)k = (‚àí1)k Wk = ‚àíWk. Thus, using the result of the previous Exercise
we can write: trWk = Wk : I = 0.
1.45 By means of the deÔ¨Ånition (1.146) we obtain
sym (skewA) = 1
2

skewA + (skewA)T
= 1
2
"1
2

A ‚àíAT
+ 1
2

A ‚àíATT#
= 1
2
"1
2A ‚àí1
2AT + 1
2AT ‚àí1
2A
#
= 0.
The same procedure leads to the identity skew (symA) = 0.
1.46 On use of (1.153) we can write
sph (devA) = sph
"
A ‚àí1
ntr (A) I
#
= 1
ntr
"
A ‚àí1
ntr (A) I
#
I = 0,
where we take into account that trI = n. In the same way, one proves that
dev (sphA) = 0.
Exercises of Chapter 2
2.1 The tangent vectors take the form:

190
Solutions
g1 = ‚àÇr
‚àÇœï = r cos œï sin œÜe1 ‚àír sin œï sin œÜe3,
g2 = ‚àÇr
‚àÇœÜ = r sin œï cos œÜe1 ‚àír sin œÜe2 + r cos œï cos œÜe3,
g3 = ‚àÇr
‚àÇr = sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3.
(S.3)
For the metrics coeÔ¨Écients we can further write:
g1 ¬∑ g1 =
(r cos œï sin œÜe1 ‚àír sin œï sin œÜe3)
¬∑ (r cos œï sin œÜe1 ‚àír sin œï sin œÜe3) = r2 sin2 œÜ,
g1 ¬∑ g2 =
(r cos œï sin œÜe1 ‚àír sin œï sin œÜe3)
¬∑ (r sin œï cos œÜe1 ‚àír sin œÜe2 + r cos œï cos œÜe3)
= r2 (sin œï cos œï sin œÜ cos œÜ ‚àísin œï cos œï sin œÜ cos œÜ) = 0,
g1 ¬∑ g3 =
(r cos œï sin œÜe1 ‚àír sin œï sin œÜe3)
¬∑ (sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3)
= r

sin œï cos œï sin2 œÜ ‚àísin œï cos œï sin2 œÜ

= 0,
g2 ¬∑ g2 =
(r sin œï cos œÜe1 ‚àír sin œÜe2 + r cos œï cos œÜe3)
¬∑ (r sin œï cos œÜe1 ‚àír sin œÜe2 + r cos œï cos œÜe3)
= r2 
sin2 œï cos2 œÜ + sin2 œÜ + cos2 œï cos2 œÜ

= r2,
g2 ¬∑ g3 =
(r sin œï cos œÜe1 ‚àír sin œÜe2 + r cos œï cos œÜe3)
¬∑ (sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3)
= r

sin2 œï sin œÜ cos œÜ ‚àísin œÜ cos œÜ + cos2 œï sin œÜ cos œÜ

= 0,
g3 ¬∑ g3 =
(sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3)
¬∑ (sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3)
= sin2 œï sin2 œÜ + cos2 œÜ + cos2 œï sin2 œÜ = 1.
Thus,
[gij] = [gi ¬∑ gj] =
‚é°
‚é£
r2 sin2 œÜ 0 0
0
r2 0
0
0 1
‚é§
‚é¶
and consequently

gij
= [gij]‚àí1 =
‚é°
‚é¢‚é¢‚é¢‚é£
1
r2 sin2 œÜ 0 0
0
1
r2 0
0
0 1
‚é§
‚é•‚é•‚é•‚é¶.
(S.4)

Solutions
191
Finally, we calculate the dual basis by (1.21)1:
g1 =
1
r2 sin2 œÜg1 = r‚àí1 cos œï
sin œÜ e1 ‚àír‚àí1 sin œï
sin œÜ e3,
g2 = 1
r2 g2 = r‚àí1 sin œï cos œÜe1 ‚àír‚àí1 sin œÜe2 + r‚àí1 cos œï cos œÜe3,
g3 = g3 = sin œï sin œÜe1 + cos œÜe2 + cos œï sin œÜe3.
2.2 The connection between the linear and spherical coordinates (2.144) can
be expressed by
x1 = r sin œï sin œÜ,
x2 = r cos œÜ,
x3 = r cos œï sin œÜ.
Thus, we obtain
‚àÇx1
‚àÇœï = r cos œï sin œÜ,
‚àÇx1
‚àÇœÜ = r sin œï cos œÜ,
‚àÇx1
‚àÇr = sin œï sin œÜ,
‚àÇx2
‚àÇœï = 0,
‚àÇx2
‚àÇœÜ = ‚àír sin œÜ,
‚àÇx2
‚àÇr = cos œÜ,
‚àÇx3
‚àÇœï = ‚àír sin œï sin œÜ,
‚àÇx3
‚àÇœÜ = r cos œï cos œÜ,
‚àÇx3
‚àÇr = cos œï sin œÜ.
Inverting the so-constructed matrix
" ‚àÇxi
‚àÇœï
‚àÇxi
‚àÇœÜ
‚àÇxi
‚àÇr
#
further yields
‚àÇœï
‚àÇx1 = cos œï
r sin œÜ,
‚àÇœï
‚àÇx2 = 0,
‚àÇœï
‚àÇx3 = ‚àísin œï
r sin œÜ,
‚àÇœÜ
‚àÇx1 = sin œï cos œÜ
r
,
‚àÇœÜ
‚àÇx2 = ‚àísin œÜ
r
,
‚àÇœÜ
‚àÇx3 = cos œï cos œÜ
r
,
‚àÇr
‚àÇx1 = sin œï sin œÜ,
‚àÇr
‚àÇx2 = cos œÜ,
‚àÇr
‚àÇx3 = cos œï sin œÜ.
2.3 Applying the directional derivative we have
(a) :
d
ds ‚à•r + sa‚à•‚àí1

s=0
= d
ds [(r + sa) ¬∑ (r + sa)]‚àí1/2

s=0
= d
ds

r ¬∑ r + 2sr ¬∑ a + s2a ¬∑ a
‚àí1/2

s=0
= ‚àí1
2
2r ¬∑ a + 2sa ¬∑ a
[(r + sa) ¬∑ (r + sa)]3/2

s=0
= ‚àír ¬∑ a
‚à•r‚à•3 .
Comparing with (2.51) Ô¨Ånally yields

192
Solutions
grad‚à•r‚à•‚àí1 = ‚àí
r
‚à•r‚à•3 .
(b) :
d
ds (r + sa) ¬∑ w

s=0
= d
ds (r ¬∑ w + sa ¬∑ w)

s=0
= a ¬∑ w.
Hence, grad(r ¬∑ w) = w.
(c) :
d
ds (r + sa) A (r + sa)

s=0
= d
ds

rAr + saAr + srAa + s2aAa

s=0
= aAr + rAa = (Ar) ¬∑ a + (rA) ¬∑ a = (Ar + rA) ¬∑ a,
Thus, applying (1.110) and (1.146)1 we can write
grad(rAr) = Ar + rA =

A + AT
r = 2 (symA) r.
(d) :
d
dsA (r + sa)

s=0
= d
ds (Ar + sAa)

s=0
= Aa.
Comparing with (2.53) we then have
grad(Ar) = A.
(e) : In view of (1.64) and using the results of (d) we obtain
grad(w √ó r) = grad(Wr) = W.
With the aid of the representation w = wigi we can further write (see Exercise
1.21)
W = Wijgi ‚äógj,
[Wij] = g
‚é°
‚é£
0
‚àíw3 w2
w3
0
‚àíw1
‚àíw2 w1
0
‚é§
‚é¶.
2.4 We begin with the derivative of the metrics coeÔ¨Écients obtained in Ex-
ercise 2.1:
[gij,1 ] =
"‚àÇgij
‚àÇœï
#
=
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶, [gij,2 ] =
"‚àÇgij
‚àÇœÜ
#
=
‚é°
‚é£
2r2 sin œÜ cos œÜ 0 0
0
0 0
0
0 0
‚é§
‚é¶,
[gij,3 ] =
"‚àÇgij
‚àÇr
#
=
‚é°
‚é£
2r sin2 œÜ
0
0
0
2r 0
0
0
0
‚é§
‚é¶.
Thus, according to (2.74)

Solutions
193
[Œìij1] =
"1
2 (g1i,j +g1j,i ‚àígij,1 )
#
=
‚é°
‚é£
0
r2 sin œÜ cos œÜ r sin2 œÜ
r2 sin œÜ cos œÜ
0
0
r sin2 œÜ
0
0
‚é§
‚é¶,
[Œìij2] =
"1
2 (g2i,j +g2j,i ‚àígij,2 )
#
=
‚é°
‚é£
‚àír2 sin œÜ cos œÜ 0 0
0
0 r
0
r 0
‚é§
‚é¶,
[Œìij3] =
"1
2 (g3i,j +g3j,i ‚àígij,3 )
#
=
‚é°
‚é£
‚àír sin2 œÜ
0
0
0
‚àír 0
0
0
0
‚é§
‚é¶.
With the aid of (2.67) we further obtain
Œì1
ij = g1lŒìijl = g11Œìij1 + g12Œìij2 + g13Œìij3 =
Œìij1
r2 sin2 œÜ,
i, j = 1, 2, 3,

Œì1
ij

=
‚é°
‚é£
0
cot œÜ r‚àí1
cot œÜ
0
0
r‚àí1
0
0
‚é§
‚é¶,
(S.5)
Œì2
ij = g2lŒìijl = g21Œìij1 + g22Œìij2 + g23Œìij3 = Œìij2
r2 ,
i, j = 1, 2, 3,

Œì2
ij

=
‚é°
‚é£
‚àísin œÜ cos œÜ
0
0
0
0
r‚àí1
0
r‚àí1
0
‚é§
‚é¶,
(S.6)
Œì3
ij = g3lŒìijl = g31Œìij1 + g32Œìij2 + g33Œìij3 = Œìij3,
i, j = 1, 2, 3,

Œì3
ij

=
‚é°
‚é£
‚àír sin2 œÜ
0
0
0
‚àír 0
0
0
0
‚é§
‚é¶.
(S.7)
2.5 Relations (2.87) can be obtained in the same manner as (2.85). Indeed,
using the representation A = Aijgi ‚äógj and by virtue of (2.72) we have for
example for (2.87)1:
A,k =

Aijgi ‚äógj
,k
= Aij,k gi ‚äógj + Aijgi,k ‚äógj + Aijgi ‚äógj,k
= Aij,k gi ‚äógj + Aij

‚àíŒìi
lkgl
‚äógj + Aijgi ‚äó

‚àíŒìj
lkgl
=

Aij,k ‚àíAljŒìl
ik ‚àíAilŒìl
jk

gi ‚äógj.

194
Solutions
2.6 Using (2.87)1 we write for example for the left hand side of (2.92)
Aij|k= Aij,k ‚àíAljŒìl
ik ‚àíAilŒìl
jk.
In view of (2.84)2 the same result holds for the right hand side of (2.92) as
well. Indeed,
ai|k bj + aibj|k =

ai,k ‚àíalŒìl
ik

bj + ai

bj,k ‚àíblŒìl
jk

= ai,k bj + aibj,k ‚àíalbjŒìl
ik ‚àíaiblŒìl
jk
= Aij,k ‚àíAljŒìl
ik ‚àíAilŒìl
jk.
2.7 By analogy with (S.1)
ÀÜt = eijkg‚àí1tigk ‚äógj.
Inserting this expression into (2.116) and taking (2.103) into account we fur-
ther write
curlt = ‚àídivÀÜt = ‚àí

eijkg‚àí1tigk ‚äógj

,l gl.
With the aid of the identities

g‚àí1gj

,l ¬∑gl = 0 (j = 1, 2, 3) following from
(2.66) and (2.98) and applying the product rule of diÔ¨Äerentiation we Ô¨Ånally
obtain
curlt = ‚àíeijkg‚àí1ti,j gk ‚àíeijkg‚àí1tigk,j
= ‚àíeijkg‚àí1ti,j gk = ‚àíeijkg‚àí1ti|j gk = ejikg‚àí1ti|j gk
keeping (1.36), (2.68) and (2.84)2 in mind.
2.8 We begin with the covariant derivative of the Cauchy stress components
(2.109). Using the results of Exercise 2.4 concerning the ChristoÔ¨Äel symbols
for the spherical coordinates we get
œÉ1j|j= œÉ1j,j +œÉljŒì1
lj + œÉ1lŒìj
lj = œÉ11,1 +œÉ12,2 +œÉ13,3 +3œÉ12 cot œÜ + 4œÉ13
r ,
œÉ2j|j = œÉ2j,j +œÉljŒì2
lj + œÉ2lŒìj
lj
= œÉ21,1 +œÉ22,2 +œÉ23,3 ‚àíœÉ11 sin œÜ cos œÜ + œÉ22 cot œÜ + 4œÉ23
r ,
œÉ3j|j = œÉ3j,j +œÉljŒì3
lj + œÉ3lŒìj
lj
= œÉ31,1 +œÉ32,2 +œÉ33,3 ‚àíœÉ11r sin2 œÜ ‚àíœÉ22r + œÉ32 cot œÜ + 2œÉ33
r .
The balance equations (2.107) take thus the form

Solutions
195
œÅ¬®x1 = œÉ11,1 +œÉ12,2 +œÉ13,3 +3œÉ12 cot œÜ + 4œÉ13
r
+ f 1,
œÅ¬®x2 = œÉ21,1 +œÉ22,2 +œÉ23,3 ‚àíœÉ11 sin œÜ cos œÜ + œÉ22 cot œÜ + 4œÉ23
r
+ f 2,
œÅ¬®x3 = œÉ31,1 +œÉ32,2 +œÉ33,3 ‚àíœÉ11r sin2 œÜ ‚àíœÉ22r + œÉ32 cot œÜ + 2œÉ33
r
+ f 3.
2.9 The tangent vectors take the form:
g1 = ‚àÇr
‚àÇr =

cos s
r + s
r sin s
r

e1 +

sin s
r ‚àís
r cos s
r

e2,
g2 = ‚àÇr
‚àÇs = ‚àísin s
r e1 + cos s
r e2,
g3 = ‚àÇr
‚àÇz = e3.
The metrics coeÔ¨Écients can further be written by
[gij] = [gi ¬∑ gj] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
1 + s2
r2
‚àís
r 0
‚àís
r
1
0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é¶
,

gij
= [gij]‚àí1 =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
1
s
r
0
s
r
1 + s2
r2 0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
For the dual basis we use (1.21)1:
g1 = g1 + s
rg2 = cos s
r e1 + sin s
re2,
g2 = s
r g1 +
%
1 + s2
r2
&
g2
=

‚àísin s
r + s
r cos s
r

e1 +

cos s
r + s
r sin s
r

e2,
g3 = g3 = e3.
The derivatives of the metrics coeÔ¨Écients become
[gij,1 ] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
‚àí2s2
r3
s
r2 0
s
r2
0
0
0
0
0
‚é§
‚é•‚é•‚é•‚é•‚é¶
, [gij,2 ] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
2s
r2
‚àí1
r 0
‚àí1
r
0
0
0
0
0
‚é§
‚é•‚é•‚é•‚é•‚é¶
, [gij,3 ] =
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶.
For the ChristoÔ¨Äel symbols we thus obtain by means of (2.74) and (2.67):
[Œìij1] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
‚àís2
r3
s
r2 0
s
r2
‚àí1
r 0
0
0
0
‚é§
‚é•‚é•‚é•‚é•‚é¶
, [Œìij2] =
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶, [Œìij3] =
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶,

196
Solutions

Œì1
ij

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
‚àís2
r3
s
r2
0
s
r2
‚àí1
r 0
0
0
0
‚é§
‚é•‚é•‚é•‚é•‚é¶
,

Œì2
ij

=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àís3
r4
s2
r3
0
s2
r3
‚àís
r2 0
0
0
0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,

Œì3
ij

=
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶.
2.10 First, we express the covariant derivative of the Cauchy stress compo-
nents by (2.109) using the results of the previous exercise:
œÉ1j|j= œÉ11,r +œÉ12,s +œÉ13,z ‚àíœÉ11 s2
r3 ‚àíœÉ22
r
+ 2œÉ12 s
r2 ,
œÉ2j|j= œÉ21,r +œÉ22,s +œÉ23,z ‚àíœÉ11 s3
r4 ‚àíœÉ22 s
r2 + 2œÉ12 s2
r3 ,
œÉ3j|j= œÉ31,r +œÉ32,s +œÉ33,z .
The balance equations (2.107) become
œÅ¬®x1 = œÉ11,r +œÉ12,s +œÉ13,z ‚àíœÉ11 s2
r3 ‚àíœÉ22
r
+ 2œÉ12 s
r2 + f 1,
œÅ¬®x2 = œÉ21,r +œÉ22,s +œÉ23,z ‚àíœÉ11 s3
r4 ‚àíœÉ22 s
r2 + 2œÉ12 s2
r3 + f 2,
œÅ¬®x3 = œÉ31,r +œÉ32,s +œÉ33,z +f 3.
2.11 (2.117), (2.119), (1.32), (2.72):
div curlt =

gi √ó t,i

,j ¬∑gj =

‚àíŒìi
kjgk √ó t,i +gi √ó t,ij

¬∑ gj
= ‚àí

Œìi
kjgj √ó gk
¬∑ t,i +

gj √ó gi
¬∑ t,ij = 0,
where we take into consideration that t,ij = t,ji, Œìl
ij = Œìl
ji and gi √ó gj =
‚àígj √ó gi (i Ã∏= j, i, j = 1, 2, 3).
(2.117), (2.119), (1.32):
div (u √ó v) = (u √ó v) ,i ¬∑gi = (u,i √óv + u √ó v,i ) ¬∑ gi
=

gi √ó u,i

¬∑ v +

v,i √ógi
¬∑ u = v ¬∑ curlu ‚àíu ¬∑ curlv.
(2.6), (2.65)1, (2.117):
graddivt =

t,i ¬∑gi
,j gj =

t,ij ¬∑gi
gj +

t,i ¬∑gi,j

gj.
Using the relation

t,i ¬∑gi,j

gj =

t,i ¬∑

‚àíŒìi
jkgk
gj
=

t,i ¬∑gk 
‚àíŒìi
jkgj
=

t,i ¬∑gk
gi,k =

t,i ¬∑gj
gi,j
(S.8)
following from (2.72) we thus write

Solutions
197
graddivt =

t,ij ¬∑gi
gj +

t,i ¬∑gj
gi,j .
(2.119), (1.157):
curl curlt = gj √ó

gi √ó t,i

,j = gj √ó

gi,j √ót,i

+ gj √ó

gi √ó t,ij

=

gj ¬∑ t,i

gi,j ‚àí

gj ¬∑ gi,j

t,i +

gj ¬∑ t,ij

gi ‚àígijt,ij .
(2.8), (2.59)1, (2.117), (1.116), (S.8):
div gradt =

t,i ‚äógi
,j ¬∑gj =

t,ij ‚äógi
¬∑ gj +

t,i ‚äógi,j

¬∑ gj
= gijt,ij +

gi,j ¬∑gj
t,i .
(S.9)
div (gradt)T =

t,i ‚äógiT ,j ¬∑gj =

gi ‚äót,ij

¬∑ gj +

gi,j ‚äót,i

¬∑ gj
=

t,ij ¬∑gj
gi +

t,i ¬∑gj
gi,j .
The latter four relations immediately imply (2.126) and (2.127).
(2.5), (1.135), (2.103), (2.117), (2.59)1:
div (tA) = (tA) ,i ¬∑gi = (t,i A) ¬∑ gi + (tA,i ) ¬∑ gi
= A : t,i ‚äógi + t ¬∑

A,i gi
= A : gradt + t ¬∑ divA.
(2.3), (2.58), (2.117):
div (Œ¶t) = (Œ¶t) ,i ¬∑gi = (Œ¶,i t) ¬∑ gi + (Œ¶t,i ) ¬∑ gi
= t ¬∑

Œ¶,i gi
+ Œ¶

t,i ¬∑gi
= t ¬∑ gradŒ¶ + Œ¶divt.
(2.4), (2.58), (2.103):
div (Œ¶A) = (Œ¶A) ,i gi = (Œ¶,i A) gi + (Œ¶A,i ) gi
= A

Œ¶,i gi
+ Œ¶

A,i gi
= AgradŒ¶ + Œ¶divA.
2.12 Cylindrical coordinates, (2.65)2, (2.84), (2.81):
gradt = ti|j gi ‚äógj =

ti,j ‚àítkŒìk
ij

gi ‚äógj
= ti,j gi ‚äógj + rt3g1 ‚äóg1 ‚àír‚àí1t1

g1 ‚äóg3 + g3 ‚äóg1
,
or alternatively
gradt = ti|j gi ‚äógj =

ti,j +tkŒìi
kj

gi ‚äógj
= ti,j gi ‚äógj + r‚àí1t3g1 ‚äóg1 + t1 
r‚àí1g1 ‚äóg3 ‚àírg3 ‚äóg1
.
(2.28), (2.118):
divt = tr gradt = ti,j gij + rt3g11 ‚àí2r‚àí1t1g13
= r‚àí2t1,1 +t2,2 +t3,3 +r‚àí1t3,

198
Solutions
or alternatively
divt = tr gradt = ti,i +r‚àí1t3 = ti,i +r‚àí1t3 = t1,1 +t2,2 +t3,3 +r‚àí1t3.
(2.84), (2.120):
curlt = ejik 1
gti|j gk
= g‚àí1 [(t3|2 ‚àít2|3) g1 + (t1|3 ‚àít3|1) g2 + (t2|1 ‚àít1|2) g3]
= r‚àí1 [(t3,2 ‚àít2,3 ) g1 + (t1,3 ‚àít3,1 ) g2 + (t2,1 ‚àít1,2 ) g3] .
Spherical coordinates, (S.5-S.7):
gradt =

ti,j ‚àítkŒìk
ij

gi ‚äógj
=

t1,1 +t2 sin œÜ cos œÜ + t3r sin2 œÜ

g1 ‚äóg1 + (t2,2 +t3r) g2 ‚äóg2
+ t3,3 g3 ‚äóg3 + (t1,2 ‚àít1 cot œÜ) g1 ‚äóg2 + (t2,1 ‚àít1 cot œÜ) g2 ‚äóg1
+

t1,3 ‚àít1r‚àí1
g1 ‚äóg3 +

t3,1 ‚àít1r‚àí1
g3 ‚äóg1
+

t2,3 ‚àít2r‚àí1
g2 ‚äóg3 +

t3,2 ‚àít2r‚àí1
g3 ‚äóg2,
or alternatively
gradt =

ti,j +tkŒìi
kj

gi ‚äógj =

t1,1 +t2 cot œÜ + t3r‚àí1
g1 ‚äóg1
+

t2,2 +t3r‚àí1
g2 ‚äóg2 + t3,3 g3 ‚äóg3
+

t1,2 +t1 cot œÜ

g1 ‚äóg2 +

t2,1 ‚àít1 sin œÜ cos œÜ

g2 ‚äóg1
+

t1,3 +t1r‚àí1
g1 ‚äóg3 +

t3,1 ‚àít1r sin2 œÜ

g3 ‚äóg1
+

t2,3 +t2r‚àí1
g2 ‚äóg3 +

t3,2 ‚àít2r

g3 ‚äóg2,
(2.84), (2.118), (2.120), (S.3-S.7):
divt =

ti,j ‚àítkŒìk
ij

gij =
t1,1
r2 sin2 œÜ + r‚àí2t2,2 +t3,3 +r‚àí2 cot œÜt2 + 2r‚àí1t3
= ti,i +tkŒìi
ki = t1,1 +t2,2 +t3,3 + cot œÜt2 + 2r‚àí1t3,
curlt = g‚àí1 [(t3|2 ‚àít2|3) g1 + (t1|3 ‚àít3|1) g2 + (t2|1 ‚àít1|2) g3]
= ‚àí
1
r2 sin œÜ [(t3,2 ‚àít2,3 ) g1 + (t1,3 ‚àít3,1 ) g2 + (t2,1 ‚àít1,2 ) g3] .
2.13 According to the result (S.9) of Exercise 2.11
Œît = div gradt = gijt,ij +

gi,j ¬∑gj
t,i .

Solutions
199
By virtue of (2.63), (2.72) and (2.84)2 we further obtain
Œît = gijt,ij ‚àíŒìk
ijgijt,k = gij 
t,ij ‚àíŒìk
ijt,k

= gijt,i|j= t,i|i .
In Cartesian coordinates it leads to the well-known relation
div gradt = t,11 +t,22 +t,33 .
2.14 Specifying the result of Exercise 2.13 to scalar functions we can write
ŒîŒ¶ = gij 
Œ¶,ij ‚àíŒìk
ijŒ¶,k

= Œ¶,i|i .
For the cylindrical coordinates it takes in view of (2.28) and (2.81) the fol-
lowing form
ŒîŒ¶ = r‚àí2Œ¶,11 +Œ¶,22 +Œ¶,33 +r‚àí1Œ¶,3 = 1
r2
‚àÇ2Œ¶
‚àÇœï2 + ‚àÇ2Œ¶
‚àÇz2 + ‚àÇ2Œ¶
‚àÇr2 + 1
r
‚àÇŒ¶
‚àÇr .
For the spherical coordinates we use (S.4-S.7). Thus,
ŒîŒ¶ =
1
r2 sin2 œïŒ¶,11 +r‚àí2Œ¶,22 +Œ¶,33 + cos œÜ
r2 sin œÜŒ¶,2 +2r‚àí1Œ¶,3
=
1
r2 sin2 œï
‚àÇ2Œ¶
‚àÇœï2 + r‚àí2 ‚àÇ2Œ¶
‚àÇœÜ2 + ‚àÇ2Œ¶
‚àÇr2 + r‚àí2 cot œÜ‚àÇŒ¶
‚àÇœÜ + 2r‚àí1 ‚àÇŒ¶
‚àÇr .
2.15 According to the solution of Exercise 2.13
Œît = gij 
t,ij ‚àíŒìm
ijt,m

,
(S.10)
where in view of (2.62)1
t,i = tk|i gk,
t,ij = tk|ij gk.
By virtue of (2.84)1 we further write tk|i= tk,i +Œìk
litl and consequently
tk|ij= tk|i,j +Œìk
mjtm|i= tk,ij +Œìk
li,j tl + Œìk
litl,j +Œìk
mjtm,i +Œìk
mjŒìm
li tl.
Substituting these results into the expression of the Laplacian (S.10) Ô¨Ånally
yields
Œît = gij 
tk,ij +2Œìk
litl,j ‚àíŒìm
ij tk,m +Œìk
li,j tl + Œìk
mjŒìm
li tl ‚àíŒìm
ij Œìk
lmtl
gk.
Taking (S.4-S.7) into account we thus obtain for the spherical coordinates
(2.144)

200
Solutions
Œît =
% t1,œïœï
r2 sin2 œÜ + t1,œÜœÜ
r2
+ t1,rr
+3 cotœÜ
r2
t1,œÜ + 2 cos œÜ
r2 sin3 œÜt2,œï +4t1,r
r
+
2t3,œï
r3 sin2 œÜ
&
g1
+
% t2,œïœï
r2 sin2 œÜ + t2,œÜœÜ
r2
+ t2,rr
‚àí2 cotœÜ
r2
t1,œï +cot œÜ
r2 t2,œÜ +4t2,r
r
+ 2t3,œÜ
r3
+ 1 ‚àícot2 œÜ
r2
t2
&
g2
+
% t3,œïœï
r2 sin2 œÜ + t3,œÜœÜ
r2
+ t3,rr
‚àí2t1,œï
r
‚àí2t2,œÜ
r
+ cot œÜ
r2 t3,œÜ +2t3,r
r
‚àí2 cot œÜ
r
t2 ‚àí2t3
r2
&
g3.
Exercises of Chapter 3
3.1 (C.4), (3.18): a1 = dr/ds = const. Hence, r (s) = b + sa1.
3.2 Using the fact that d/d (‚àís) = ‚àíd/ds we can write by means of (3.15),
(3.18), (3.20), (3.21) and (3.27): a‚Ä≤
1 (s) = ‚àía1 (s), a‚Ä≤
2 (s) = a2 (s), a‚Ä≤
3 (s) =
‚àía3 (s), Œ∫‚Ä≤ (s) = Œ∫ (s) and œÑ‚Ä≤ (s) = œÑ (s).
3.3 Let us show that the curve r (s) with the zero torsion œÑ (s) ‚â°0 belongs to
the plane p

t1, t2
= r (s0) + t1a1 (s0) + t2a2 (s0), where a1 (s0) and a2 (s0)
are, respectively, the unit tangent vector and the principal normal vector at
a point s0. For any arbitrary point we can write using (3.15)
r (s) = r (s0) +
r(s)
'
r(s0)
dr = r (s0) +
s
'
s0
a1 (s) ds.
(S.11)
The vector a1 (s) can further be represented with respect to the trihedron at s0
as a1 (s) = Œ±i (s) ai (s0). Taking (3.26) into account we observe that a3,s = 0
and consequently a3 (s) ‚â°a3 (s0). In view of (3.23)2 it yields a1 (s)¬∑a3 (s0) =
0, so that a1 (s) = Œ±1 (s) a1 (s0) + Œ±2 (s) a2 (s0). Inserting this result into
(S.11) we have
r (s) = r (s0) + a1 (s0)
s
'
s0
Œ±1 (s) ds + a2 (s0)
s
'
s0
Œ±2 (s) ds
= r (s0) + t1a1 (s0) + t2a2 (s0) ,

Solutions
201
where we set ti =
: s
s0 Œ±i (s) ds (i = 1, 2).
3.4 Setting in (2.28) r = R yields
[gŒ±Œ≤] =
"
R2 0
0 1
#
.
By means of (2.81), (3.74), (3.79), (3.90) and (3.93) we further obtain
[bŒ±Œ≤] =
"‚àíR 0
0
0
#
,

bŒ±
Œ≤

=
"‚àíR‚àí1 0
0
0
#
,
Œì1
Œ±Œ≤ = Œì2
Œ±Œ≤ = 0,
Œ±, Œ≤ = 1, 2,
K =
bŒ±
Œ≤
 = 0,
H = 1
2bŒ±
Œ± = ‚àí1
2R‚àí1.
(S.12)
3.5 Keeping in mind the results of Exercise 2.1 and using (S.5-S.7), (3.58),
(3.62), (3.67), (3.74), (3.79), (3.90) and (3.93) we write
g1 = R cos t1 sin t2e1 ‚àíR sin t1 sin t2e3,
g2 = R sin t1 cos t2e1 ‚àíR sin t2e2 + R cos t1 cos t2e3,
g3 = ‚àísin t1 sin t2e1 ‚àícos t2e2 ‚àícos t1 sin t2e3,
[gŒ±Œ≤] =
"
R2 sin2 t2 0
0
R2
#
,
[bŒ±Œ≤] =
"
R sin2 t2 0
0
R
#
,

bŒ±
Œ≤

=
"
R‚àí1
0
0
R‚àí1
#
,

Œì1
Œ±Œ≤

=
"
0
cot t2
cot t2
0
#
,

Œì2
Œ±Œ≤

=
" ‚àísin t2 cos t2 0
0
0
#
,
K =
bŒ±
Œ≤
 = R‚àí2,
H = 1
2bŒ±
Œ± = R‚àí1.
(S.13)
3.6 (3.62), (3.67), (3.143):
g1 = ‚àÇr
‚àÇt1 = e1 + ¬Øt2e3,
g2 = ‚àÇr
‚àÇt2 = e2 + ¬Øt1e3,
g3 =
g1 √ó g2
‚à•g1 √ó g2‚à•=
1

1 + (¬Øt1)2 + (¬Øt2)2

‚àí¬Øt2e1 ‚àí¬Øt1e2 + e3

,
where ¬Øti = ti
c (i = 1, 2). Thus, the coeÔ¨Écients of the Ô¨Årst fundamental form
are
g11 = g1¬∑g1 = 1+

¬Øt22 , g12 = g21 = g1¬∑g2 = ¬Øt1¬Øt2, g22 = g2¬∑g2 = 1+

¬Øt12 .
For the coeÔ¨Écients of the inversed matrix

gŒ±Œ≤
we have

gŒ±Œ≤
=
1
1 + (¬Øt1)2 + (¬Øt2)2
,
1 +

¬Øt12
‚àí¬Øt1¬Øt2
‚àí¬Øt1¬Øt2
1 +

¬Øt22
-
.
The derivatives of the tangent vectors result in

202
Solutions
g1,1 = 0,
g1,2 = g2,1 = 1
c e3,
g2,2 = 0.
By (3.74), (3.79), (3.90) and (3.93) we further obtain
b11 = 0,
b12 = b21 =
1
c

1 + (¬Øt1)2 + (¬Øt2)2 ,
b22 = 0,

b Œ≤
Œ±¬∑

=
1
c

1 + (¬Øt1)2 + (¬Øt2)23/2
,
‚àí¬Øt1¬Øt2
1 +

¬Øt22
1 +

¬Øt12
‚àí¬Øt1¬Øt2
-
,
K =
bŒ±
Œ≤
 = ‚àí
1
c2

1 + (¬Øt1)2 + (¬Øt2)22 = ‚àí

c2 +

t12 +

t22‚àí2
,
H = 1
2bŒ±
Œ± = ‚àí
¬Øt1¬Øt2
c

1 + (¬Øt1)2 + (¬Øt2)23/2 .
3.7 Taking (3.105) into account we can write

g‚àó
ij

=
‚é°
‚é£
g‚àó
11 g‚àó
12 0
g‚àó
21 g‚àó
22 0
0
0 1
‚é§
‚é¶,

g‚àóij
=

g‚àó
ij
‚àí1 =
1
g‚àó
ij

‚é°
‚é£
g‚àó
22 ‚àíg‚àó
21 0
‚àíg‚àó
12 g‚àó
11 0
0
0
1
‚é§
‚é¶,
which immediately implies (3.111).
3.8 For a cylindrical shell equilibrium equations (3.140-3.141) take by means
of (3.77)1 and (S.12) the form
f 11,1 +f 12,2 +p1 = 0,
f 12,1 +f 22,2 +p2 = 0,
‚àíRf 11 + p3 = 0.
For a spherical shell we further obtain by virtue of (S.13)
f 11,1 +f 12,2 +3 cot t2f 12 + p1 = 0,
f 12,1 +f 22,2 ‚àísin t2 cos t2f 11 + cot t2f 22 + p2 = 0,
R sin2 t2f 11 + Rf 22 + p3 = 0.
Exercises of Chapter 4
4.1
In the case of simple shear the right Cauchy-Green tensor C has the
form:
C = Ci
jei ‚äóej,

Ci
j

= [Cij] =
‚é°
‚é£
1
Œ≥
0
Œ≥ 1 + Œ≥2 0
0
0
1
‚é§
‚é¶.

Solutions
203
The characteristic equation can then be written by

1 ‚àíŒõ
Œ≥
0
Œ≥
1 + Œ≥2 ‚àíŒõ
0
0
0
1 ‚àíŒõ

= 0
‚áí
(1 ‚àíŒõ)

Œõ2 ‚àíŒõ

2 + Œ≥2
+ 1

= 0.
Solving the latter equation with respect to Œõ we obtain the eigenvalues of C
as
Œõ1/2 = 1 + Œ≥2 ¬±

4Œ≥2 + Œ≥4
2
=
)
Œ≥ ¬±

4 + Œ≥2
2
*2
,
Œõ3 = 1.
(S.14)
The eigenvectors a = aiei corresponding to the Ô¨Årst two eigenvalues result
from the equation system (4.16)1
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
‚àíŒ≥2 ‚àì

4Œ≥2 + Œ≥4
2
a1
+Œ≥a2
= 0,
Œ≥a1 +Œ≥2 ‚àì

4Œ≥2 + Œ≥4
2
a2
= 0,
‚àíŒ≥2 ‚àì

4Œ≥2 + Œ≥4
2
a3 = 0.
Since the Ô¨Årst and second equation are equivalent we only obtain
a2 = Œ≥ ¬±

4 + Œ≥2
2
a1,
a3 = 0,
so that a2 = ‚àöŒõ1a1 or a2 = ‚àí‚àöŒõ2a1. In order to ensure the unit length of
the eigenvectors we also require that

a12 +

a22 +

a32 = 1.
This yields
a1 =
1
‚àö1 + Œõ1
e1 +
8
Œõ1
1 + Œõ1
e2,
a2 =
1
‚àö1 + Œõ2
e1 ‚àí
8
Œõ2
1 + Œõ2
e2. (S.15)
Applying the above procedure for the third eigenvector corresponding to the
eigenvalue Œõ3 = 1 we easily obtain: a3 = e3.
4.2 Using (4.26)1‚àí3 we write
IA = trA,
IIA = 1
2

(trA)2 ‚àítrA2
,
IIIA = 1
3

IIAtrA ‚àíIAtrA2 + trA3
.

204
Solutions
Inserting the Ô¨Årst and second expression into the third one we obtain
IIIA = 1
3
!1
2

(trA)2 ‚àítrA2
trA ‚àítrAtrA2 + trA3
$
= 1
3
"
trA3 ‚àí3
2trA2trA + 1
2 (trA)3
#
.
4.3 Since ri = ti for every eigenvalue Œªi we have exactly n = (s
i=1 ri eigenvec-
tors, say a(k)
i
(i = 1, 2, . . . , s; k = 1, 2, . . . , ri). Let us assume, on the contrary,
that they are linearly dependent so that
s
	
i=1
ri
	
k=1
Œ±(k)
i
a(k)
i
= 0,
where not all Œ±(k)
i
are zero. Since linear combinations of the eigenvectors ai =
(ri
k=1 Œ±(k)
i
a(k)
i
associated with the same eigenvalue Œªi are again eigenvectors
we arrive at
s
	
i=1
Œµiai = 0,
where Œµi are either one or zero (but not all). This relation establishes the
linear dependence between eigenvectors corresponding to distinct eigenvalues,
which contradicts the statement of Theorem 4.2. Applying then Theorem 1.3
for the space Cn instead of V we infer that the eigenvectors a(k)
i
form a basis
of Cn.
4.4 (4.40), (4.42):
PiPj =
) ri
	
k=1
a(k)
i
‚äób(k)
i
* ) rj
	
l=1
a(l)
j
‚äób(l)
j
*
=
ri
	
k=1
rj
	
l=1
Œ¥ijŒ¥kla(k)
i
‚äób(l)
j
= Œ¥ij
ri
	
k=1
a(k)
i
‚äób(k)
j
=

Pi
if i = j,
0
if i Ã∏= j.
4.5
By means of (4.40) and (4.42) we infer that Pia(l)
j
= Œ¥ija(l)
j . Ev-
ery vector x in Cn can be represented with respect the basis of this space
a(k)
i
(i = 1, 2, . . ., s; k = 1, 2, . . . , ri) by x = (s
j=1
(rj
k=1 x(k)
j
a(k)
j . Hence,
) s
	
i=1
Pi
*
x =
s
	
i=1
s
	
j=1
rj
	
k=1
x(k)
j Pia(k)
j
=
s
	
i=1
s
	
j=1
rj
	
k=1
x(k)
j Œ¥ija(k)
j
=
s
	
j=1
rj
	
k=1
x(k)
j a(k)
j
= x,
‚àÄx ‚ààCn,

Solutions
205
which immediately implies (4.46).
4.6 Let Œª1, Œª2, . . . , Œªn be eigenvalues of A ‚ààLinn. By the spectral mapping
theorem (Theorem 4.1) we infer that exp (Œªi) (i = 1, 2, . . ., n) are eigenvalues
of exp A. On use of (4.24) and (4.26) we can thus write: det [exp (A)] =
n;
i=1
exp Œªi = exp
% n(
i=1
Œªi
&
= exp (trA).
4.7 Let us consider the right hand side of (4.54) for example for i = 1. In
this case we have
3
+
j=1
jÃ∏=1
A ‚àíŒªjI
Œª1 ‚àíŒªj
= A ‚àíŒª2I
Œª1 ‚àíŒª2
A ‚àíŒª3I
Œª1 ‚àíŒª3
.
On use of (4.43), (4.44) and (4.46) we further obtain
A ‚àíŒª2I
Œª1 ‚àíŒª2
A ‚àíŒª3I
Œª1 ‚àíŒª3
=
3(
i=1
(Œªi ‚àíŒª2) Pi
Œª1 ‚àíŒª2
3(
j=1
(Œªj ‚àíŒª3) Pj
Œª1 ‚àíŒª3
=
3(
i,j=1
(Œªi ‚àíŒª2) (Œªj ‚àíŒª3) Œ¥ijPi
(Œª1 ‚àíŒª2) (Œª1 ‚àíŒª3)
=
3(
i=1
(Œªi ‚àíŒª2) (Œªi ‚àíŒª3) Pi
(Œª1 ‚àíŒª2) (Œª1 ‚àíŒª3)
= (Œª1 ‚àíŒª2) (Œª1 ‚àíŒª3) P1
(Œª1 ‚àíŒª2) (Œª1 ‚àíŒª3)
= P1.
In a similar way, one veriÔ¨Åes the Sylvester formula also for i = 2 and i = 3.
4.8 The characteristic equation of the tensor A takes the form:

‚àí2 ‚àíŒª
2
2
2
1 ‚àíŒª
4
2
4
1 ‚àíŒª

= 0.
Writing out this determinant we get after some algebraic manipulations
Œª3 ‚àí27Œª ‚àí54 = 0
Comparing this equation with (4.28) we see that
IA = 0,
IIA = ‚àí27,
IIIA = 54.
(S.16)
Inserting this result into the Cardano formula (4.31) and (4.32) we obtain
œë = arccos
,
2I3
A ‚àí9IAIIA + 27IIIA
2

I2
A ‚àí3IIA
3/2
-
= arccos
,
27 ¬∑ 54
2 (3 ¬∑ 27)3/2
-
= arccos(1) = 0,

206
Solutions
Œªk = 1
3
!
IA + 2

I2
A ‚àí3IIA cos 1
3 [œë + 2œÄ (k ‚àí1)]
$
= 2
3
‚àö
3 ¬∑ 27 cos
%2
3œÄ (k ‚àí1)
&
= 6 cos
%2
3œÄ (k ‚àí1)
&
,
k = 1, 2, 3.
Thus, we obtain two pairwise distinct eigenvalues (s = 2):
Œª1 = 6,
Œª2 = Œª3 = ‚àí3.
(S.17)
The Sylvester formula (4.54) further yields
P1 =
2
+
j=1
jÃ∏=1
A ‚àíŒªjI
Œªi ‚àíŒªj
= A ‚àíŒª2I
Œª1 ‚àíŒª2
= A + 3I
9
= 1
9
‚é°
‚é£
1 2 2
2 4 4
2 4 4
‚é§
‚é¶ei ‚äóej,
P2 =
2
+
j=1
jÃ∏=2
A ‚àíŒªjI
Œªi ‚àíŒªj
= A ‚àíŒª1I
Œª2 ‚àíŒª1
= A ‚àí6I
‚àí9
= 1
9
‚é°
‚é£
8 ‚àí2 ‚àí2
‚àí2
5 ‚àí4
‚àí2 ‚àí4
5
‚é§
‚é¶ei ‚äóej.
4.9 The spectral representation of A takes the form
A =
s
	
i=1
ŒªiPi = Œª1P1 + Œª2P2 = 6P1 ‚àí3P2.
Thus,
A =
s
	
i=1
exp (Œªi) Pi
= exp (Œª1) P1 + exp (Œª2) P2 = exp (6) P1 + exp (‚àí3) P2
= e6
9
‚é°
‚é£
1 2 2
2 4 4
2 4 4
‚é§
‚é¶ei ‚äóej + e‚àí3
9
‚é°
‚é£
8 ‚àí2 ‚àí2
‚àí2
5 ‚àí4
‚àí2 ‚àí4
5
‚é§
‚é¶ei ‚äóej
= 1
9
‚é°
‚é£
e6 + 8e‚àí3
2e6 ‚àí2e‚àí3
2e6 ‚àí2e‚àí3
2e6 ‚àí2e‚àí3 4e6 + 5e‚àí3 4e6 ‚àí4e‚àí3
2e6 ‚àí2e‚àí3 4e6 ‚àí4e‚àí3 4e6 + 5e‚àí3
‚é§
‚é¶ei ‚äóej.
4.10
Components of the eigenvectors a = aiei result from the equation
system (4.16)

Ai
j ‚àíŒ¥i
jŒª

aj = 0,
i = 1, 2, 3.
(S.18)
Setting Œª = 6 we obtain only two independent equations

‚àí8a1 +2a2 +2a3 = 0,
2a1 ‚àí5a2 +4a3 = 0.

Solutions
207
Multiplying the Ô¨Årst equation by two and subtracting from the second one
we get a2 = 2a1 and consequently a3 = 2a1. Additionally we require that the
eigenvectors have unit length so that

a12 +

a22 +

a32 = 1,
(S.19)
which leads to
a1 = 1
3e1 + 2
3e2 + 2
3e3.
Further, setting in the equation system (S.18) Œª = ‚àí3 we obtain only one
independent linear equation
a1 + 2a2 + 2a3 = 0
(S.20)
with respect to the components of the eigenvectors corresponding to this dou-
ble eigenvalue. One of these eigenvectors can be obtained by setting for ex-
ample a1 = 0. In this case, a2 = ‚àía3 and in view of (S.19)
a(1)
2
=
1
‚àö
2e2 ‚àí1
‚àö
2e3.
Requiring that the eigenvectors a(1)
2
and a(2)
2
corresponding to the double
eigenvalue Œª = ‚àí3 are orthogonal we get an additional condition a2 = a3 for
the components of a(2)
2 . Taking into account (S.19) and (S.20) this yields
a(2)
2
= ‚àí4
3
‚àö
2e1 +
1
3
‚àö
2e2 +
1
3
‚àö
2e3.
With the aid of the eigenvectors we can construct eigenprojections without
the Sylvester formula by (4.42):
P1 = a1 ‚äóa1
=
%1
3e1 + 2
3e2 + 2
3e3
&
‚äó
%1
3e1 + 2
3e2 + 2
3e3
&
= 1
9
‚é°
‚é£
1 2 2
2 4 4
2 4 4
‚é§
‚é¶ei ‚äóej,
P2 = a(1)
2
‚äóa(1)
2
+ a(2)
2
‚äóa(2)
2
=
% 1
‚àö
2e2 ‚àí1
‚àö
2e3
&
‚äó
% 1
‚àö
2e2 ‚àí1
‚àö
2e3
&
+
%
‚àí4
3
‚àö
2
e1 +
1
3
‚àö
2
e2 +
1
3
‚àö
2
e3
&
‚äó
%
‚àí4
3
‚àö
2
e1 +
1
3
‚àö
2
e2 +
1
3
‚àö
2
e3
&
= 1
9
‚é°
‚é£
8 ‚àí2 ‚àí2
‚àí2
5 ‚àí4
‚àí2 ‚àí4
5
‚é§
‚é¶ei ‚äóej.

208
Solutions
4.11
Since linearly independent vectors are non-zero it follows from (4.9)
that ci ¬∑ ci Ã∏= 0 (i = 1, 2, . . ., m). Thus, the Ô¨Årst vector can be given by
a1 =
c1
‚àöc1 ¬∑ c1
,
such that a1 ¬∑ a1 = 1. Next, we set
a‚Ä≤
2 = c2 ‚àí(c2 ¬∑ a1) a1,
so that a‚Ä≤
2 ¬∑ a1 = 0. Further, a‚Ä≤
2 Ã∏= 0 because otherwise c2 = (c2 ¬∑ a1) a1 =
(c2 ¬∑ a1) (c1 ¬∑ c1)‚àí1/2 c1 which implies a linear dependence between c1 and c2.
Thus, we can set a2 = a‚Ä≤
2/

a‚Ä≤
2 ¬∑ a‚Ä≤
2. The third vector can be given by
a3 =
a‚Ä≤
3

a‚Ä≤
3 ¬∑ a‚Ä≤
3
,
where
a‚Ä≤
3 = c3 ‚àí(c3 ¬∑ a2) a2 ‚àí(c3 ¬∑ a1) a1,
so that a3 ¬∑ a1 = a3 ¬∑ a2 = 0. Repeating this procedure we Ô¨Ånally obtain the
set of vectors ai satisfying the condition ai ¬∑ aj = Œ¥ij, (i, j = 1, 2, . . ., m). One
can easily show that these vectors are linearly independent. Indeed, otherwise
(m
i=1 Œ±iai = 0, where not all Œ±i are zero. Multiplying this vector equation
scalarly by aj (j = 1, 2, . . . , m) yields, however, Œ±j = 0 (j = 1, 2, . . . , m).
4.12
Comparing (4.63)1 with (4.71)1 we infer that the right eigenvectors
a(k)
i
(k = 1, 2, . . ., ti) associated with a complex eigenvalue Œªi are simulta-
neously the left eigenvalues associated with Œªi. Since Œªi Ã∏= Œªi Theorem 4.3
implies that a(k)
i
¬∑ a(l)
i
= 0 (k, l = 1, 2, . . ., ti).
4.13 Taking into account the identities trWk = 0, where k = 1, 3, 5, . . . (see
Exercise 1.46) we obtain from (4.29)
IW = trW = 0,
IIW = 1
2

(trW)2 ‚àítrW2
= ‚àí1
2trW2 = ‚àí1
2

W : WT
= 1
2 (W : W) = 1
2 ‚à•W‚à•2 ,
IIIW = 1
3
"
trW3 ‚àí3
2trW2trW + 1
2 (trW)3
#
= 0,
or in another way
IIIW = detW = detWT = ‚àídetWT = ‚àíIIIW = 0.
4.14 Eigenvalues of the rotation tensor (Exercise 1.22)
R = Ri
¬∑jei ‚äóej,
where

Ri
¬∑j

=

Rij
=
‚é°
‚é£
cos Œ± ‚àísin Œ± 0
sin Œ± cos Œ± 0
0
0
1
‚é§
‚é¶

Solutions
209
result from the characteristic equation

cos Œ± ‚àíŒª
‚àísin Œ±
0
sin Œ±
cos Œ± ‚àíŒª
0
0
0
1 ‚àíŒª

= 0.
Writing out this determinant we have
(1 ‚àíŒª)

Œª2 ‚àí2Œª cosŒ± + 1

= 0
and consequently
Œª1 = 1,
Œª2/3 = cos Œ± ¬± i sin Œ±.
Components of the right eigenvectors a = aiei result from the equation system
(4.16)

Ri
¬∑j ‚àíŒ¥i
jŒª

aj = 0,
i = 1, 2, 3.
(S.21)
Setting Ô¨Årst Œª = 1 we obtain a homogeneous equation system
a1 (cos Œ± ‚àí1) ‚àía2 sin Œ± = 0,
a1 sin Œ± + a2 (cos Œ± ‚àí1) = 0,
leading to a1 = a2 = 0. Thus, a1 = a3e3, where a3 can be an arbitrary real
number. The unit length condition requires further that
a1 = e3.
Next, inserting Œª = cos Œ± ¬± i sin Œ± into (S.21) yields
a2 = ‚àìia1,
a3 = 0.
Thus, the right eigenvectors associated with the complex conjugate eigenvalues
Œª2/3 are of the form a2/3 = a1 (e1 ‚àìie2). Bearing in mind that any rotation
tensor is orthogonal we infer that a2/3 = a3/2 = a1 (e1 ¬± ie2) are the left
eigenvectors associated with Œª2/3. Imposing the additional condition a2 ¬∑a2 =
a2 ¬∑ a3 = 1 (4.38) we Ô¨Ånally obtain
a2 =
‚àö
2
2 (e1 ‚àíie2) ,
a3 =
‚àö
2
2 (e1 + ie2) .
The eigenprojections can further be expressed by (4.42) as
P1 = a1 ‚äóa1 = e3 ‚äóe3,
P2 = a2 ‚äóa2 =
‚àö
2
2 (e1 ‚àíie2) ‚äó
‚àö
2
2 (e1 + ie2)
= 1
2 (e1 ‚äóe1 + e2 ‚äóe2) + 1
2i (e1 ‚äóe2 ‚àíe2 ‚äóe1) ,

210
Solutions
P3 = a3 ‚äóa3 =
‚àö
2
2 (e1 + ie2) ‚äó
‚àö
2
2 (e1 ‚àíie2)
= 1
2 (e1 ‚äóe1 + e2 ‚äóe2) ‚àí1
2i (e1 ‚äóe2 ‚àíe2 ‚äóe1) .
4.15 First, we write

A2i
j

=
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶=
‚é°
‚é£
12
6
6
6 21 12
6 12 21
‚é§
‚é¶,

A3i
j

=
‚é°
‚é£
12
6
6
6 21 12
6 12 21
‚é§
‚é¶
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶=
‚é°
‚é£
0
54
54
54
81 108
54 108
81
‚é§
‚é¶.
Then,
pA (A) = A3 ‚àí27A ‚àí54I =
‚é°
‚é£
0
54
54
54
81 108
54 108
81
‚é§
‚é¶ei ‚äóej
‚àí27
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶ei ‚äóej ‚àí54
‚é°
‚é£
1 0 0
0 1 0
0 0 1
‚é§
‚é¶ei ‚äóej =
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶ei ‚äóej.
4.16
The characteristic polynomial of F (4.23) can be represented by
pA (Œª) = (1 ‚àíŒª)3. Hence,
pF (F) = (I ‚àíF)3 =
‚é°
‚é£
0 ‚àíŒ≥ 0
0
0 0
0
0 0
‚é§
‚é¶
3
ei ‚äóej =
‚é°
‚é£
0 0 0
0 0 0
0 0 0
‚é§
‚é¶ei ‚äóej = 0.
Exercises of Chapter 5
5.1 By using (1.101)1, (D.2) and (5.17) one can verify for example (5.20)1
and (5.21)1 within the following steps
A‚äó(B + C) : X = AX (B + C) = AXB+AXC = (A ‚äóB + A ‚äóC) : X,
A ‚äô(B + C) : X = A [(B + C) : X] = A (B : X + C : X)
= A (B : X) + A (C : X)
= (A ‚äôB + A ‚äôC) : X,
‚àÄX ‚ààLinn.
The proof of (5.20)2 and (5.21)2 is similar.
5.2 With the aid of (5.16), (5.17) and (1.138) we can write
(Y : A ‚äóB) : X = Y : (A ‚äóB : X) = Y : AXB = ATYBT : X,

Solutions
211
(Y : A ‚äôB) : X = Y : (A ‚äôB : X) = Y : [A (B : X)]
= (Y : A) (B : X) = [(Y : A) B] : X,
‚àÄX, Y ‚ààLinn.
5.3 Using the deÔ¨Ånition of the simple composition (5.40) and taking (5.17)
into account we obtain
A (B ‚äóC) D : X = A (B ‚äóC : X) D = A (BXC) D
= (AB) X (CD) = (AB) ‚äó(CD) : X,
A (B ‚äôC) D : X = A (B ‚äôC : X) D = A [B (C : X)] D
= ABD (C : X) = (ABD) ‚äôC : X,
‚àÄX ‚ààLinn.
5.4 By means of (1.140), (5.17), (5.22) and (5.45) we can write
(A ‚äóB)T : X = X : (A ‚äóB) = ATXBT =

AT ‚äóBT
: X,
(A ‚äôB)T : X = X : (A ‚äôB) = (X : A) B = (B ‚äôA) : X,
(A ‚äôB)t : X = (A ‚äôB) : XT = A

B : XT
= A

BT : X

=

A ‚äôBT
: X,
‚àÄX ‚ààLinn.
Identities (5.51) and (5.52) follow immediately from (1.116) (5.23), (5.24)
(5.49)1 and (5.50) by setting A = a ‚äób, B = c ‚äód or A = a ‚äód, B = b ‚äóc,
respectively.
5.5 Using (5.51) and (5.52) we obtain for the left and right hand sides diÔ¨Äerent
results:
(a ‚äób ‚äóc ‚äód)tT = (a ‚äóc ‚äób ‚äód)T = c ‚äóa ‚äód ‚äób,
(a ‚äób ‚äóc ‚äód)Tt = (b ‚äóa ‚äód ‚äóc)t = b ‚äód ‚äóa ‚äóc.
5.6 (5.31), (5.32), (5.45):
(A : B)T : X = X : (A : B) = (X : A) : B
= BT : (X : A) = BT :

AT : X

=

BT : AT
: X,
(A : B)t : X = (A : B) : XT = A :

B : XT
= A :

Bt : X

=

A : Bt
: X,
‚àÄX ‚ààLinn.
5.7
In view of (1.115), (5.17) and (5.45) we write for an arbitrary tensor
X ‚ààLinn
(A ‚äóB)t : (C ‚äóD) : X = (A ‚äóB)t : (CXD) = (A ‚äóB) : (CXD)T
= (A ‚äóB) :

DTXTCT
= ADTXTCTB
=

ADT
‚äó

CTB

: XT =

ADT
‚äó

CTB
t : X,

212
Solutions
(A ‚äóB)t : (C ‚äôD) : X = (A ‚äóB)t : [(D : X) C]
= (A ‚äóB) :

(D : X) CT
= (D : X) ACTB =

ACTB

‚äôD : X.
5.8 By virtue of (5.51) and (5.52) we can write
CT =

Cijklgi ‚äógj ‚äógk ‚äógl
T = Cijklgj ‚äógi ‚äógl ‚äógk
= Cjilkgi ‚äógj ‚äógk ‚äógl,
Ct =

Cijklgi ‚äógj ‚äógk ‚äógl
t = Cijklgi ‚äógk ‚äógj ‚äógl
= Cikjlgi ‚äógj ‚äógk ‚äógl.
According to (5.60) and (5.61) CT = Ct = C. Taking also into account that
the tensors gi ‚äógj ‚äógk ‚äógl (i, j, k, l = 1, 2, . . . , n) are linearly independent
we thus write
Cijkl = Cjilk = Cikjl.
The remaining relations (5.70) are obtained in the same manner by applying
the identities C = CTtT and C = CtTt.
5.9 With the aid of (1.140), (5.16) and (5.79) we get
(Y : T) : X = Y : (T : X) = Y : XT = YT : X,
‚àÄX, Y ‚ààLinn.
5.10 On use of (5.31), (5.45)2 and (5.79) we obtain
(A : T) : X = A : (T : X) = A : XT = At : X,
‚àÄX ‚ààLinn.
The second identity (5.83) can be derived by means of (5.54), (5.78) and (5.81)
as follows
ATtT = (I : A)TtT =

AT : I
tT
=

AT : ItT
=

AT : T
T
= T : A.
The last identity (5.83) can Ô¨Ånally be proved by
(T : T) : X = T : (T : X) = T : XT = X = T : X,
‚àÄX ‚ààLinn.
5.11 C possesses the minor symmetry (5.61) by the very deÔ¨Ånition. In order
to prove the major symmetry (5.60) we show that C : X = X : C, ‚àÄX ‚ààLinn.
Indeed, in view of (5.17)1, (5.22)1 and (5.48)
C : X = (M1 ‚äóM2 + M2 ‚äóM1)s : X = (M1 ‚äóM2 + M2 ‚äóM1) : symX
= M1 (symX) M2 + M2 (symX) M1,
X : C = X : (M1 ‚äóM2 + M2 ‚äóM1)s
= sym [X : (M1 ‚äóM2 + M2 ‚äóM1)]
= sym (M1XM2 + M2XM1) = M1 (symX) M2 + M2 (symX) M1.

Solutions
213
5.12 (a) Let ei (i = 1, 2, 3) be an orthonormal basis in E3. By virtue of (5.77),
(5.82) and (5.84) we can write
Is =
3
	
i,j=1
1
2ei ‚äó(ei ‚äóej + ej ‚äóei) ‚äóej.
Using the notation
Mi = ei ‚äóei,
i = 1, 2, 3,
M4 = e1 ‚äóe2 + e2 ‚äóe1
‚àö
2
,
M5 = e1 ‚äóe3 + e3 ‚äóe1
‚àö
2
,
M6 = e3 ‚äóe2 + e2 ‚äóe3
‚àö
2
(S.22)
and taking (5.23) into account one thus obtains the spectral decomposition of
Is as
Is =
6
	
p=1
Mp ‚äôMp.
The only eigenvalue 1 is of multiplicity 6. Note that the corresponding eigen-
tensors Mp (p = 1, 2, . . ., 6) form an orthonormal basis of Lin3.
(b) Using the orthonormal basis (S.22) we can write
Psph = 1
3 (M1 + M2 + M3) ‚äô(M1 + M2 + M3)
=
6
	
p,q=1
Ppq
sphMp ‚äôMq,
where
[Ppq
sph] = 1
3
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1 1 1 0 0 0
1 1 1 0 0 0
1 1 1 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Eigenvalues and eigenvectors of this matrix can be represented as
Œõ1 = 1,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
1
1
1
0
0
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
,
Œõ2 = Œõ3 = Œõ4 = Œõ5 = Œõ6 = 0,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
‚àí1
1
0
0
0
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
‚àí1
0
1
0
0
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
0
0
0
1
0
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
0
0
0
0
1
0
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
,
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
0
0
0
0
0
1
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
.

214
Solutions
Thus, the orthonormal eigentensors can be given by
<
M1 =
1
‚àö
3 (M1 + M2 + M3) =
1
‚àö
3I,
<
M2 = ‚àí
‚àö
2
2 M1 +
‚àö
2
2 M2,
<
M3 = ‚àí
‚àö
6
6 M1 ‚àí
‚àö
6
6 M2 +
‚àö
6
3 M3,
<
Mp = Mp,
p = 4, 5, 6,
(S.23)
where the tensors Mq, (q = 1, 2, . . ., 6) are deÔ¨Åned by (S.22).
(c) For the super-symmetric counterpart of the deviatoric projection tensor
(5.87)2 (n = 3) we can write
Ps
dev =
6
	
p,q=1
Ppq
devMp ‚äôMq,
where
[Ppq
dev] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2
3 ‚àí1
3 ‚àí1
3 0 0 0
‚àí1
3
2
3 ‚àí1
3 0 0 0
‚àí1
3 ‚àí1
3
2
3 0 0 0
0
0
0 1 0 0
0
0
0 0 1 0
0
0
0 0 0 1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Thus, the eigenvalues of Ps
dev are Œõ1 = 0, Œõq = 1 (q = 2, 3, . . . , 6). The cor-
responding eigentensors are again given by (S.23).
(d) With respect to the orthonormal basis (S.22) the elasticity tensor (5.91)
can be represented by
C =
6
	
p,q=1
CpqMp ‚äôMq,
where
[Cpq] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2G + Œª
Œª
Œª
0
0
0
Œª
2G + Œª
Œª
0
0
0
Œª
Œª
2G + Œª 0
0
0
0
0
0
2G 0
0
0
0
0
0 2G 0
0
0
0
0
0 2G
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
The eigentensors of C are the same as of Psph and Ps
dev and are given by (S.23).
The eigenvalues are as follows: Œõ1 = 2G+3Œª, Œõq = 2G (q = 2, 3, . . . , 6). They
can be obtained as linear combinations of those ones of Psph and Ps
dev.
Exercises of Chapter 6
6.1 (a) f

QAQT
= aQAQTb Ã∏= aAb.

Solutions
215
(b) Since the components of A are related to an orthonormal basis we can
write
f (A) = A11 + A22 + A33 = A1
¬∑1 + A2
¬∑2 + A3
¬∑3 = Ai
¬∑i = trA.
Trace of a tensor represents its isotropic function.
(c) For an isotropic tensor function the condition (6.1) f(QAQT) = f (A)
must hold on the whole deÔ¨Ånition domain of the arguments A and ‚àÄQ ‚àà
Orth3. Let us consider a special case where
A =
‚é°
‚é£
1 0 0
0 0 0
0 0 0
‚é§
‚é¶ei ‚äóej,
Q =
‚é°
‚é£
0 1 0
‚àí1 0 0
0 0 1
‚é§
‚é¶ei ‚äóej.
Thus,
A‚Ä≤ = QAQT =
‚é°
‚é£
0 0 0
0 1 0
0 0 0
‚é§
‚é¶ei ‚äóej
and consequently
f (A) = A11 + A12 + A13 = 1 Ã∏= 0 = A‚Ä≤11 + A‚Ä≤12 + A‚Ä≤13 = f

QAQT
,
which means that the function f (A) is not isotropic.
(d) detA is the last principal invariant of A and represents thus its isotropic
tensor function. Isotropy of the determinant can, however, be shown directly
using the relation det (BC) = detBdetC. Indeed,
det

QAQT
= detQ detA detQT = detQ detQTdetA
= det

QQT
detA = detI detA = detA,
‚àÄQ ‚ààOrthn.
(e) Eigenvalues of a second-order tensor are uniquely deÔ¨Åned by its principal
invariants and represent thus its isotropic functions. This can also be shown
in a direct way considering the eigenvalue problem for the tensor QAQT as

QAQT
a = Œªa.
Mapping both sides of this vector equation by QT yields

QTQAQT
a = ŒªQTa.
Using the abbreviation a‚Ä≤ = QTa we Ô¨Ånally obtain
Aa‚Ä≤ = Œªa‚Ä≤.

216
Solutions
Thus, every eigenvalue of QAQT is the eigenvalue of A and vice versa. In
other words, the eigenvalues of these tensors are pairwise equal which imme-
diately implies that they are characterized by the same value of Œªmax. The
tensors obtained by the operation QAQT from the original one A are called
similar tensors.
6.2 Inserting
M = 1
2

A + AT
,
W = 1
2

A ‚àíAT
into (6.17) we obtain
trM = 1
2

trA + trAT
= trA,
trM2 = 1
4tr

A + AT2
= 1
4

trA2 + tr

AAT
+ tr

ATA

+ tr

AT2
= 1
2

trA2 + tr

AAT
,
trM3 = 1
8tr

A + AT3
= 1
8
1
trA3 + tr

A2AT
+ tr

AATA

+ tr

A

AT2
+ tr

ATA2
+ tr

ATAAT
+ tr

AT2 A

+ tr

AT32
= 1
4

trA3 + 3tr

A2AT
,
trW2 = 1
4tr

A ‚àíAT2
= 1
4

trA2 ‚àítr

AAT
‚àítr

ATA

+ tr

AT2
= 1
2

trA2 ‚àítr

AAT
,
tr

MW2
= 1
8tr

A + AT 
A ‚àíAT2
= 1
8
1
trA3 ‚àítr

A2AT
‚àítr

AATA

+ tr

A

AT2
+ tr

ATA2
‚àítr

ATAAT
‚àítr

AT2 A

+ tr

AT32

Solutions
217
= 1
4

trA3 ‚àítr

A2AT
,
tr

M2W2
= 1
16tr

A + AT2 
A ‚àíAT2
= 1
16tr
1
A2 + AAT + ATA +

AT2 
A2 ‚àíAAT ‚àíATA +

AT22
= 1
16tr

A4 ‚àíA3AT ‚àíA2ATA + A2 
AT2 + AATA2 ‚àíAATAAT
‚àíA

AT2 A + A

AT3 + ATA3 ‚àíATA2AT ‚àíATAATA
+ ATA

AT2 +

AT2 A2 ‚àí

AT2 AAT ‚àí

AT3 A +

AT4
= 1
8

trA4 ‚àítr

AAT2
,
tr

M2W2MW

= 1
64tr

A4 ‚àíA3AT ‚àíA2ATA + A2 
AT2 + AATA2 ‚àíAATAAT
‚àíA

AT2 A + A

AT3 + ATA3 ‚àíATA2AT ‚àíATAATA
+ ATA

AT2 +

AT2 A2 ‚àí

AT2 AAT ‚àí

AT3 A +

AT4

A2 ‚àíAAT + ATA ‚àí

AT22
= 1
16tr

AT2 A2ATA ‚àíA2 
AT2 AAT
.
Finally, trA4 should be expressed in terms of the principal traces trAi (i
= 1, 2, 3) presented in the functional basis (6.18). To this end, we apply the
Cayley-Hamilton equation (4.91). Its composition with A yields
A4 ‚àíIAA3 + IIAA2 ‚àíIIIAA = 0,
so that
trA4 = IAtrA3 ‚àíIIAtrA2 + IIIAtrA,
where IA, IIA and IIIA are given by (4.29). Thus, all the invariants of the
functional basis (6.17) are expressed in a unique form in terms of (6.18).
6.3
By Theorem 6.1 œà is an isotropic function of C and Li (i = 1, 2, 3).
Applying further (6.15) and taking into account the identities
LiLj = 0,
Lk
i = Li,
trLk
i = 1, i Ã∏= j, i, j = 1, 2, 3; k = 1, 2, . . .
(S.24)

218
Solutions
we obtain the following orthotropic invariants
trC,
trC2,
trC3,
tr (CL1) = tr

CL2
1

,
tr (CL2) = tr

CL2
2

,
tr (CL3) = tr

CL2
3

,
tr

C2L1

= tr

C2L2
1

, tr

C2L2

= tr

C2L2
2

, tr

C2L3

= tr

C2L2
3

,
tr (LiCLj) = tr (CLjLi) = tr (LjLiC) = 0,
i Ã∏= j = 1, 2, 3.
(S.25)
Using the relation
3
	
i=1
Li = I
(S.26)
one can further write
tr (C) = C : I = C : (L1 + L2 + L3)
= C : L1 + C : L2 + C : L3 = tr (CL1) + tr (CL2) + tr (CL3) .
In the same manner we also obtain
tr

C2
= tr

C2L1

+ tr

C2L2

+ tr

C2L3

.
Thus, the invariants trC and trC2 are redundant and can be excluded from
the functional basis (S.25). Finally, the orthotropic strain energy function can
be represented by
œà = Àúœà [tr (CL1) , tr (CL2) , tr (CL3) ,
tr

C2L1

, tr

C2L2

, tr

C2L3

, trC3
.
(S.27)
Alternatively, a functional basis for the orthotropic material symmetry can be
obtained in the component form. To this end, we represent the right Cauchy-
Green tensor by C = Cijli ‚äólj. Then,
tr (CLi) =

Ckllk ‚äóll

: li ‚äóli = Cii,
i = 1, 2, 3,
tr

C2Li

=

Ci12 +

Ci22 +

Ci32 ,
i = 1, 2, 3,
tr

C3
=

C113 +

C223 +

C333 + 3

C122 
C11 + C22
+ 3

C132 
C11 + C33
+ 3

C232 
C22 + C33
+ 6C12C13C23.
Thus, the orthotropic strain energy function (S.27) can be given in another
form as
œà = ÀÜœà

C11, C22, C33,

C122 ,

C132 ,

C232 , C12C13C23
.
6.4 (6.52), (6.54), (6.93), (6.119), (6.123), (6.127) (6.131), (S.27):

Solutions
219
S = 6
‚àÇÀúœà
‚àÇtrC3 C2 + 2
3
	
i=1
‚àÇÀúœà
‚àÇtr (CLi)Li + 2
3
	
i=1
‚àÇÀúœà
‚àÇtr (C2Li) (CLi + LiC) ,
C = 36
‚àÇ2 Àúœà
‚àÇtrC3‚àÇtrC3 C2 ‚äôC2 + 4
3
	
i,j=1
‚àÇ2 Àúœà
‚àÇtr (CLi) ‚àÇtr (CLj)Li ‚äôLj
+ 4
3
	
i,j=1
‚àÇ2 Àúœà
‚àÇtr (C2Li) ‚àÇtr (C2Lj) (CLi + LiC) ‚äô(CLj + LjC)
+ 12
3
	
i=1
‚àÇ2 Àúœà
‚àÇtr (CLi) ‚àÇtrC3

Li ‚äôC2 + C2 ‚äôLi

+ 12
3
	
i=1
‚àÇ2 Àúœà
‚àÇtr (C2Li) ‚àÇtrC3

C2 ‚äô(CLi + LiC) + (CLi + LiC) ‚äôC2
+ 4
3
	
i,j=1
‚àÇ2 Àúœà
‚àÇtr (CLi) ‚àÇtr (C2Lj) [Li ‚äô(CLj + LjC) + (CLj + LjC) ‚äôLi]
+ 12
‚àÇÀúœà
‚àÇtrC3 (C ‚äóI + I ‚äóC)s + 4
3
	
i=1
‚àÇÀúœà
‚àÇtr (C2Li) (Li ‚äóI + I ‚äóLi)s .
6.5
Any orthotropic function S (C) is an isotropic function of C and
Li (i = 1, 2, 3). The latter function can be represented by (6.100). Taking
(S.24) and (S.26) into account we thus obtain
S =
3
	
i=1

Œ±iLi + Œ≤i (CLi + CLi) + Œ≥i

C2Li + LiC2
,
where Œ±i, Œ≤i and Œ≥i (i = 1, 2, 3) are some scalar-valued orthotropic functions
of C (isotropic functions of C and Li (i = 1, 2, 3)).
6.6 Applying (6.15) and taking into account the identities Lm
i = Li, trLm
i =
1 (i = 1, 2; m = 1, 2, . . .) we obtain similarly to (S.25)
trC,
trC2,
trC3,
tr (CL1) = tr

CL2
1

,
tr (CL2) = tr

CL2
2

,
tr (L1L2) = tr

L1L2
2

= tr

L2
1L2

= (l1 ‚äól1) : (l2 ‚äól2) = (l1 ¬∑ l2)2 = cos2 œÜ,
tr

C2L1

= tr

C2L2
1

, tr

C2L2

= tr

C2L2
2

, tr (L1CL2) ,

220
Solutions
where œÜ denotes the angle between the Ô¨Åber directions l1 and l2. Thus, we
can write
œà = Àúœà

trC, trC2, trC3, tr (CL1) , tr (CL2) ,
tr

C2L1

, tr

C2L2

, tr (L1L2) , tr (L1CL2)

.
6.7 Using (6.57), (6.120), (6.122) and (6.127) we obtain
S = 2 ‚àÇœà
‚àÇC + pC‚àí1 = 2c1IC,C +2c2IIC,C +pC‚àí1
= 2c1I + 2c2 (ICI ‚àíC) + pC‚àí1 = 2 (c1 + c2IC) I ‚àí2c2C + pC‚àí1,
C = 2 ‚àÇS
‚àÇC = 4c2 (I ‚äôI ‚àíIs) ‚àí2p

C‚àí1 ‚äóC‚àí1s .
6.8 Using the abbreviation Œõi = Œª2
i (i = 1, 2, 3) for the eigenvalues of C we
can write
œà (C) =
m
	
r=1
Œºr
Œ±r

ŒõŒ±r/2
1
+ ŒõŒ±r/2
2
+ ŒõŒ±r/2
3
‚àí3

.
Assuming further that Œõ1 Ã∏= Œõ2 Ã∏= Œõ3 Ã∏= Œõ1 and applying (6.69) we obtain
S = 2 ‚àÇœà
‚àÇC =
m
	
r=1
Œºr

ŒõŒ±r/2‚àí1
1
Œõ1,C +ŒõŒ±r/2‚àí1
2
Œõ2,C +ŒõŒ±r/2‚àí1
3
Œõ3,C

=
m
	
r=1
Œºr

ŒõŒ±r/2‚àí1
1
P1 + ŒõŒ±r/2‚àí1
2
P2 + ŒõŒ±r/2‚àí1
3
P3

=
m
	
r=1
ŒºrCŒ±r/2‚àí1.
Note that the latter expression is obtained by means of (7.2).
6.9 Using the identities
QTLiQ = QLiQT = Li,
‚àÄQ ‚ààgo
and taking (1.144) into account we can write
tr

QCQTLiQCQTLj

= tr

CQTLiQCQTLjQ

= tr (CLiCLj) ,
‚àÄQ ‚ààgo.
Further, one can show that
tr (CLiCLi) = tr2 (CLi) ,
i = 1, 2, 3,
(S.28)
where we use the abbreviation tr2 (‚Ä¢) = [tr (‚Ä¢)]2. Indeed, in view of the relation
tr (CLi) = C : (li ‚äóli) = liCli we have

Solutions
221
tr (CLiCLi) = tr (Cli ‚äóliCli ‚äóli) = liClitr (Cli ‚äóli)
= liClitr (CLi) = tr2 (CLi) ,
i = 1, 2, 3.
Next, we obtain
tr

C2Li

= tr (CICLi) = tr [C (L1 + L2 + L3) CLi] =
3
	
j=1
tr (CLjCLi)
and consequently
tr (CL2CL1) + tr (CL3CL1) = tr

C2L1

‚àítr2 (CL1) ,
tr (CL3CL2) + tr (CL1CL2) = tr

C2L2

‚àítr2 (CL2) ,
tr (CL1CL3) + tr (CL2CL3) = tr

C2L3

‚àítr2 (CL3) .
The latter relations can be given brieÔ¨Çy by
tr (CLjCLi) + tr (CLkCLi)
= tr

C2Li

‚àítr2 (CLi) ,
i Ã∏= j Ã∏= k Ã∏= i; i, j, k = 1, 2, 3.
Their linear combinations Ô¨Ånally yield:
tr (CLiCLj) = 1
2

tr

C2Li

+ tr

C2Lj

‚àítr

C2Lk

‚àí1
2

tr2 (CLi) + tr2 (CLj) ‚àítr2 (CLk)

,
where i Ã∏= j Ã∏= k Ã∏= i; i, j, k = 1, 2, 3.
6.10 We begin with the directional derivative of tr

ÀúELi ÀúELj

:
d
dttr

ÀúE + tX

Li

ÀúE + tX

Lj

t=0
= d
dt

ÀúELi ÀúELj + t

XLi ÀúELj + ÀúELiXLj

+ t2XLiXLj

t=0
: I
=

XLi ÀúELj + ÀúELiXLj

: I =

XLi ÀúELj + Lj ÀúELiX

: I
=

Li ÀúELj + Lj ÀúELi

: XT =

Li ÀúELj + Lj ÀúELi
T
: X.
Hence,
tr

ÀúELi ÀúELj

,ÀúE = Li ÀúELj + Lj ÀúELi.
For the second Piola-KirchhoÔ¨Ästress tensor S we thus obtain

222
Solutions
S = ‚àÇœà
‚àÇÀúE
= 1
2
3
	
i,j=1
aijLitr

ÀúELj

+ 1
2
3
	
i,j=1
aijtr

ÀúELi

Lj
+
3
	
i,j=1
jÃ∏=i
Gij

Li ÀúELj + Lj ÀúELi

=
3
	
i,j=1
aijLitr

ÀúELj

+ 2
3
	
i,j=1
jÃ∏=i
GijLi ÀúELj.
By virtue of (5.42), (6.120), (6.123) and (6.127) the tangent moduli Ô¨Ånally
take the form
C = ‚àÇS
‚àÇÀúE
=
3
	
i,j=1
aijLi ‚äôLj + 2
3
	
i,j=1
jÃ∏=i
Gij (Li ‚äóLj)s .
6.11 Setting (6.145) in (6.144) yields
œà

ÀúE

= 1
2a11tr2 
ÀúEL1

+ 1
2a22

tr2 
ÀúEL2

+ tr2 
ÀúEL3

+ a12

tr

ÀúEL1

tr

ÀúEL2

+ tr

ÀúEL1

tr

ÀúEL3

+ a23tr

ÀúEL2

tr

ÀúEL3

+ (a22 ‚àía23) tr

ÀúEL2 ÀúEL3

+ 2G12

tr

ÀúEL1 ÀúEL2

+ tr

ÀúEL1 ÀúEL3

.
Thus, we can write keeping in mind (S.28)
œà

ÀúE

= 1
2a11tr2 
ÀúEL1

+ 1
2a23

tr

ÀúEL2

+ tr

ÀúEL3
2
+ a12tr

ÀúEL1
 
tr

ÀúEL2

+ tr

ÀúEL3

+ 1
2 (a22 ‚àía23) tr

ÀúEL2 + ÀúEL3
2
+ 2G12tr

ÀúEL1 ÀúE (L2 + L3)

.
Using the abbreviation L = L1 and taking (S.26) into account one thus obtains
œà

ÀúE

= 1
2a11tr2 
ÀúEL

+ 1
2a23

trÀúE ‚àítr

ÀúEL
2
+ a12tr

ÀúEL
 
trÀúE ‚àítr

ÀúEL

+ 2G12

tr

ÀúE2L

‚àítr2 
ÀúEL

+ 1
2 (a22 ‚àía23)

trÀúE2 ‚àí2tr

ÀúE2L

+ tr2 
ÀúEL

.

Solutions
223
Collecting the terms with the transversely isotropic invariants delivers
œà

ÀúE

= 1
2a23tr2 ÀúE + 1
2 (a22 ‚àía23) trÀúE2 + (a23 ‚àía22 + 2G12) tr

ÀúE2L

+
%1
2a11 + 1
2a22 ‚àía12 ‚àí2G12
&
tr2 
ÀúEL

+(a12 ‚àía23) trÀúEtr

ÀúEL

.
It is seen that the function œà(ÀúE) is transversely isotropic in the sense of the
representation (6.29). Finally, considering (6.146) in the latter relation we
obtain the isotropic strain energy function of the form (6.99) as
œà

ÀúE

= 1
2Œªtr2 ÀúE + GtrÀúE2.
6.12 The tensor-valued function (6.103) can be shown to be isotropic. Indeed,
ÀÜg

QAiQT, QXjQT
= Q‚Ä≤‚Ä≤Tg

Q‚Ä≤‚Ä≤QAiQTQ‚Ä≤‚Ä≤T
Q‚Ä≤‚Ä≤,
‚àÄQ ‚ààOrthn,
where Q‚Ä≤‚Ä≤ is deÔ¨Åned by (6.39). Further, we can write taking (6.41) into account
Q‚Ä≤‚Ä≤Tg

Q‚Ä≤‚Ä≤QAiQTQ‚Ä≤‚Ä≤T
Q‚Ä≤‚Ä≤ = Q‚Ä≤‚Ä≤Tg

Q‚àóQ‚Ä≤AiQ‚Ä≤TQ‚àóT
Q‚Ä≤‚Ä≤
= Q‚Ä≤‚Ä≤TQ‚àóg

Q‚Ä≤AiQ‚Ä≤T
Q‚àóTQ‚Ä≤‚Ä≤ = QQ‚Ä≤Tg

Q‚Ä≤AiQ‚Ä≤T
Q‚Ä≤QT
= QÀÜg (Ai, Xj) QT,
which Ô¨Ånally yields
ÀÜg

QAiQT, QXjQT
= QÀÜg (Ai, Xj) QT,
‚àÄQ ‚ààOrthn.
Thus, the suÔ¨Éciency is proved. The necessity is evident.
6.13 Consider the directional derivative of the identity A‚àíkAk = I. Taking
into account (2.9) and using (6.116) we can write
d
dt (A + tX)‚àík

t=0
Ak + A‚àík
)k‚àí1
	
i=0
AiXAk‚àí1‚àíi
*
= 0
and consequently
d
dt (A + tX)‚àík

t=0
= ‚àíA‚àík
)k‚àí1
	
i=0
AiXAk‚àí1‚àíi
*
A‚àík
= ‚àí
k‚àí1
	
i=0
Ai‚àíkXA‚àí1‚àíi.
Hence, in view of (5.17)1

224
Solutions
A‚àík,A = ‚àí
k
	
j=1
Aj‚àík‚àí1 ‚äóA‚àíj.
(S.29)
6.14 (2.4), (2.7), (5.16), (5.17)2, (6.111):
(fG) ,A : X = d
dt

ÀÜf (A + tX) g (A + tX)

t=0
= d
dt
ÀÜf (A + tX)

t=0
G + f d
dtg (A + tX)

t=0
= (f,A : X) G + f (G,A : X)
= (G ‚äôf,A +fG,A ) : X,
(G : H) ,A : X = d
dt [g (A + tX) : h (A + tX)]

t=0
= d
dtg (A + tX)

t=0
: H + G : d
dth (A + tX)

t=0
= (G,A : X) : H + G : (H,A : X)
= (H : G,A +G : H,A ) : X,
‚àÄX ‚ààLinn,
where f = ÀÜf (A), G = g (A) and H = h (A).
6.15 In the case n = 2 (6.137) takes the form
0 =
2
	
k=1
A2‚àík
k
	
i=1
(‚àí1)k‚àíi I(k‚àíi)
A

tr

Ai‚àí1B

I ‚àíBAi‚àí1
= A [tr (B) I ‚àíB] ‚àíI(1)
A [tr (B) I ‚àíB] + tr (AB) I ‚àíBA
and Ô¨Ånally
AB + BA ‚àítr (B) A ‚àítr (A) B + [tr (A) tr (B) ‚àítr (AB)] I = 0. (S.30)
Exercises of Chapter 7
7.1 By using (4.79) and (4.81) we can write
R (œâ) = P1 + eiœâP2 + e‚àíiœâP3.
Applying further (7.2) we get
Ra (œâ) = 1aP1 +

eiœâa P2 +

e‚àíiœâa P3
= P1 + eiaœâP2 + e‚àíiaœâP3 = R (aœâ) .

Solutions
225
7.2 (7.5)1, (S.14), (S.15):
U =
s
	
i=1
ŒªiPi =
s
	
i=1

Œõiai ‚äóai = e3 ‚äóe3
+

Œõ1
)
1
‚àö1 + Œõ1
e1 +
8
Œõ1
1 + Œõ1
e2
*
‚äó
)
1
‚àö1 + Œõ1
e1 +
8
Œõ1
1 + Œõ1
e2
*
+

Œõ2
)
1
‚àö1 + Œõ2
e1 ‚àí
8
Œõ2
1 + Œõ2
e2
*
‚äó
)
1
‚àö1 + Œõ2
e1 ‚àí
8
Œõ2
1 + Œõ2
e2
*
=
2

Œ≥2 + 4
e1 ‚äóe1 +
Œ≥

Œ≥2 + 4
(e1 ‚äóe2 + e2 ‚äóe1) +
Œ≥2 + 2

Œ≥2 + 4
e2 ‚äóe2
+ e3 ‚äóe3.
7.3 The proof of the Ô¨Årst relation (7.21) directly results from the deÔ¨Ånition
of the analytic tensor function (7.15) and is obvious. In order to prove (7.21)2
we Ô¨Årst write
f (A) =
1
2œÄi
7
Œì
f (Œ∂) (Œ∂I ‚àíA)‚àí1 dŒ∂, h (A) =
1
2œÄi
7
Œì ‚Ä≤ h (Œ∂‚Ä≤) (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤,
where the closed curve Œì ‚Ä≤ of the second integral lies outside Œì which, in turn,
includes all eigenvalues of A. Using the identity
(Œ∂I ‚àíA)‚àí1 (Œ∂‚Ä≤I ‚àíA)‚àí1 = (Œ∂‚Ä≤ ‚àíŒ∂)‚àí1 
(Œ∂I ‚àíA)‚àí1 ‚àí(Œ∂‚Ä≤I ‚àíA)‚àí1
valid both on Œì and Œì ‚Ä≤ we thus obtain
f (A) h (A) =
1
(2œÄi)2
7
Œì ‚Ä≤
7
Œì
f (Œ∂) h (Œ∂‚Ä≤) (Œ∂I ‚àíA)‚àí1 (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂dŒ∂‚Ä≤
=
1
2œÄi
7
Œì
f (Œ∂) 1
2œÄi
7
Œì ‚Ä≤
h (Œ∂‚Ä≤)
Œ∂‚Ä≤ ‚àíŒ∂ dŒ∂‚Ä≤ (Œ∂I ‚àíA)‚àí1 dŒ∂
+
1
2œÄi
7
Œì ‚Ä≤ h (Œ∂‚Ä≤) 1
2œÄi
7
Œì
f (Œ∂)
Œ∂ ‚àíŒ∂‚Ä≤ dŒ∂ (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤.
Since the function f (Œ∂) (Œ∂ ‚àíŒ∂‚Ä≤)‚àí1 is analytic in Œ∂ inside Œì the Cauchy theorem
(see, e.g. [5]) implies that
1
2œÄi
7
Œì
f (Œ∂)
Œ∂ ‚àíŒ∂‚Ä≤ dŒ∂ = 0.
Noticing further that
1
2œÄi
7
Œì ‚Ä≤
h (Œ∂‚Ä≤)
Œ∂‚Ä≤ ‚àíŒ∂ dŒ∂‚Ä≤ = h (Œ∂)
we obtain

226
Solutions
f (A) h (A) =
1
2œÄi
7
Œì
f (Œ∂) 1
2œÄi
7
Œì ‚Ä≤
h (Œ∂‚Ä≤)
Œ∂‚Ä≤ ‚àíŒ∂ dŒ∂‚Ä≤ (Œ∂I ‚àíA)‚àí1 dŒ∂
=
1
2œÄi
7
Œì
f (Œ∂) h (Œ∂) (Œ∂I ‚àíA)‚àí1 dŒ∂
=
1
2œÄi
7
Œì
g (Œ∂) (Œ∂I ‚àíA)‚àí1 dŒ∂ = g (A) .
Finally, we focus on the third relation (7.21). It implies that the functions
h and f are analytic on domains containing all the eigenvalues Œªi of A and
h (Œªi) (i = 1, 2, . . ., n) of B = h (A), respectively. Hence (cf. [24]),
f (h (A)) = f (B) =
1
2œÄi
7
Œì
f (Œ∂) (Œ∂I ‚àíB)‚àí1 dŒ∂,
(S.31)
where Œì encloses all the eigenvalues of B. Further, we write
(Œ∂I ‚àíB)‚àí1 = (Œ∂I ‚àíh (A))‚àí1 =
1
2œÄi
7
Œì ‚Ä≤ (Œ∂ ‚àíh (Œ∂‚Ä≤))‚àí1 (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤,
(S.32)
where Œì ‚Ä≤ includes all the eigenvalues Œªi of A so that the image of Œì ‚Ä≤ under h
lies within Œì. Thus, inserting (S.32) into (S.31) delivers
f (h (A)) =
1
(2œÄi)2
7
Œì
7
Œì ‚Ä≤ f (Œ∂) (Œ∂ ‚àíh (Œ∂‚Ä≤))‚àí1 (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤dŒ∂
=
1
(2œÄi)2
7
Œì ‚Ä≤
7
Œì
f (Œ∂) (Œ∂ ‚àíh (Œ∂‚Ä≤))‚àí1 dŒ∂ (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤
=
1
2œÄi
7
Œì ‚Ä≤ f (h (Œ∂‚Ä≤)) (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤
=
1
2œÄi
7
Œì ‚Ä≤ g (Œ∂‚Ä≤) (Œ∂‚Ä≤I ‚àíA)‚àí1 dŒ∂‚Ä≤ = g (A) .
7.4 Inserting into the right hand side of (7.54) the spectral decomposition in
terms of eigenprojections (7.1) and taking (4.46) into account we can write
similarly to (7.17)
1
2œÄi
7
Œìi
(Œ∂I ‚àíA)‚àí1 dŒ∂ =
1
2œÄi
7
Œìi
‚éõ
‚éùŒ∂I ‚àí
s
	
j=1
ŒªjPj
‚éû
‚é†
‚àí1
dŒ∂
=
1
2œÄi
7
Œìi
‚é°
‚é£
s
	
j=1
(Œ∂ ‚àíŒªj) Pj
‚é§
‚é¶
‚àí1
dŒ∂ =
1
2œÄi
7
Œìi
s
	
j=1
(Œ∂ ‚àíŒªj)‚àí1 PjdŒ∂
=
s
	
j=1
" 1
2œÄi
7
Œìi
(Œ∂ ‚àíŒªj)‚àí1 dŒ∂
#
Pj.

Solutions
227
In the case i Ã∏= j the closed curve Œìi does not include any pole so that
1
2œÄi
7
Œìi
(Œ∂ ‚àíŒªj)‚àí1 dŒ∂ = Œ¥ij,
i, j = 1, 2, . . .s.
This immediately leads to (7.54).
7.5 By means of (7.43) and (7.83) and using the result for the eigenvalues of
A by (S.17), Œªi = 6, Œª = ‚àí3 we write
P1 =
2
	
p=0
œÅ1pAp = ‚àí
Œª
(Œªi ‚àíŒª)I +
1
(Œªi ‚àíŒª)A = 1
3I + 1
9A,
P2 = I ‚àíP1 = 2
3I ‚àí1
9A.
Taking symmetry of A into account we further obtain by virtue of (7.56) and
(7.84)
P1,A =
2
	
p,q=0
œÖ1pq (Ap ‚äóAq)s
= ‚àí
2ŒªŒªi
(Œªi ‚àíŒª)3 Is + Œªi + Œª
(Œªi ‚àíŒª)3 (I ‚äóA + A ‚äóI)s ‚àí
2
(Œªi ‚àíŒª)3 (A ‚äóA)s
= 4
81Is +
1
243 (I ‚äóA + A ‚äóI)s ‚àí
2
729 (A ‚äóA)s .
The eigenprojection P2 corresponds to the double eigenvalue Œª = ‚àí3 and for
this reason is not diÔ¨Äerentiable.
7.6
Since A is a symmetric tensor and it is diagonalizable. Thus, taking
double coalescence of eigenvalues (S.17) into account we can apply the repre-
sentations (7.77) and (7.78). Setting there Œªa = 6, Œª = ‚àí3 delivers
exp (A) = e6 + 2e‚àí3
3
I + e6 ‚àíe‚àí3
9
A,
exp (A) ,A = 13e6 + 32e‚àí3
81
Is + 10e6 ‚àí19e‚àí3
243
(A ‚äóI + I ‚äóA)s
+ 7e6 + 11e‚àí3
729
(A ‚äóA)s .
Inserting
A =
‚é°
‚é£
‚àí2 2 2
2 1 4
2 4 1
‚é§
‚é¶ei ‚äóej

228
Solutions
into the expression for exp (A) we obtain
exp (A) = 1
9
‚é°
‚é£
e6 + 8e‚àí3
2e6 ‚àí2e‚àí3
2e6 ‚àí2e‚àí3
2e6 ‚àí2e‚àí3 4e6 + 5e‚àí3 4e6 ‚àí4e‚àí3
2e6 ‚àí2e‚àí3 4e6 ‚àí4e‚àí3 4e6 + 5e‚àí3
‚é§
‚é¶ei ‚äóej,
which coincides with the result obtained in Exercise 4.9.
7.7 The computation of the coeÔ¨Écients series (7.89), (7.91) and (7.96), (7.97)
with the precision parameter Œµ = 1 ¬∑ 10‚àí6 has required 23 iteration steps and
has been carried out by using MAPLE-program. The results of the compu-
tation are summarized in Tables S.1 and S.2. On use of (7.90) and (7.92) we
thus obtain
exp (A) = 44.96925I + 29.89652A + 4.974456A2,
exp (A) ,A = 16.20582Is + 6.829754 (I ‚äóA + A ‚äóI)s + 1.967368 (A ‚äóA)s
+1.039719

I ‚äóA2 + A2 ‚äóI
s + 0.266328

A ‚äóA2 + A2 ‚äóA
s
+0.034357

A2 ‚äóA2s .
Taking into account double coalescence of eigenvalues of A we can further
write
A2 = (Œªa + Œª) A ‚àíŒªaŒªI = 3A + 18I.
Inserting this relation into the above representations for exp (A) and exp (A) ,A
Ô¨Ånally yields
exp (A) = 134.50946I + 44.81989A,
exp (A) ,A = 64.76737Is + 16.59809 (I ‚äóA + A ‚äóI)s + 3.87638 (A ‚äóA)s .
Note that the relative error of this result in comparison to the closed-form
solution used in Exercise 7.6 lies within 0.044%.
Exercises of Chapter 8
8.1 By (8.2) we Ô¨Årst calculate the right and left Cauchy-Green tensors as
C = FTF =
‚é°
‚é£
5 ‚àí2 0
‚àí2
8 0
0
0 1
‚é§
‚é¶ei ‚äóej,
b = FFT =
‚é°
‚é£
5 2 0
2 8 0
0 0 1
‚é§
‚é¶ei ‚äóej,
with the following eigenvalues Œõ1 = 1, Œõ2 = 4, Œõ3 = 9. Thus, Œª1 =
‚àö
Œõ1 = 1,
Œª2 =
‚àö
Œõ2 = 2, Œª3 =
‚àö
Œõ3 = 3. By means of (8.11-8.12) we further obtain
œï0 = 3
5, œï1 =
5
12, œï2 = ‚àí1
60 and

Solutions
229
Table S.1. Recurrent calculation of the coeÔ¨Écients œâ(r)
p
r
arœâ(r)
0
arœâ(r)
1
arœâ(r)
2
0
1
0
0
1
0
1
0
2
0
0
0.5
3
9.0
4.5
0
4
0
2.25
1.125
5
12.15
6.075
0.45
6
4.05
4.05
1.0125
. . .
. . .
. . .
. . .
23 ¬∑10‚àí6
3.394287
2.262832
0.377134
œïp
44.96925
29.89652
4.974456
Table S.2. Recurrent calculation of the coeÔ¨Écients Œæ(r)
pq
r
arŒæ(r)
00
arŒæ(r)
01
arŒæ(r)
02
arŒæ(r)
11
arŒæ(r)
12
arŒæ(r)
22
1
1
0
0
0
0
0
2
0
0.5
0
0
0
0
3
0
0
0.166666 0.166666 0
0
4
4.5
1.125
0
0
0.041666 0
5
0
0.9
0.225
0.45
0
0.008333
6
4.05
1.0125
0.15
0.15
0.075
0
. . .
. . .
. . .
. . .
23 ¬∑10‚àí6
2.284387 1.229329 0.197840 0.623937 0.099319 0.015781
Œ∑pq
16.20582 6.829754 1.039719 1.967368 0.266328 0.034357
U = 3
5I + 5
12C ‚àí1
60C2 = 1
5
‚é°
‚é£
11 ‚àí2 0
‚àí2 14 0
0
0 5
‚é§
‚é¶ei ‚äóej,
v = 3
5I + 5
12b ‚àí1
60b2 = 1
5
‚é°
‚é£
11
2 0
2 14 0
0
0 5
‚é§
‚é¶ei ‚äóej.
Eqs. (8.16-8.17) further yield œÇ0 = 37
30, œÇ1 = ‚àí1
4, œÇ2 =
1
60 and
R = F
%37
30I ‚àí1
4C + 1
60C2
&
= 1
5
‚é°
‚é£
3 4 0
‚àí4 3 0
0 0 5
‚é§
‚é¶ei ‚äóej.
8.2 (4.44), (5.33), (5.47), (5.55), (5.83)1:

230
Solutions
Pij : Pkl = (Pi ‚äóPj + Pj ‚äóPi)s : (Pk ‚äóPl + Pl ‚äóPk)s
= [(Pi ‚äóPj + Pj ‚äóPi)s : (Pk ‚äóPl + Pl ‚äóPk)]s
= 1
2
1
Pi ‚äóPj + Pj ‚äóPi + (Pi ‚äóPj)t + (Pj ‚äóPi)t
: (Pk ‚äóPl + Pl ‚äóPk)}s
= (Œ¥ikŒ¥jl + Œ¥ilŒ¥jk) (Pi ‚äóPj + Pj ‚äóPi)s ,
i Ã∏= j, k Ã∏= l.
In the case i = j or k = l the previous result should be divided by 2, whereas
for i = j and k = l by 4, which immediately leads to (8.62).

References
1. Ba¬∏sar Y, Kr¬®atzig WB (1985) Mechanik der Fl¬®achentragwerke. Vieweg Verlag,
Braunschweig
2. Boehler JP (1977) Z Angew Math Mech 57: 323‚Äì327
3. de Boer R (1982) Vektor- und Tensorrechnung f¬®ur Ingenieure. Springer, Berlin
Heidelberg New York
4. Boulanger Ph, Hayes M (1993) Bivectors and Waves in Mechanics and Optics.
Chapman & Hall, London
5. Bronstein IN, Semendyayev KA, Musiol G, Muehlig H (2004) Handbook of
Mathematics. Springer, Berlin Heidelberg New York
6. Brousse P (1988) Optimization in Mechanics: Problems and Methods. Elsevier
Science, Amsterdam
7. Carlson DE, Hoger A (1986) J Elasticity 16:221‚Äì224
8. Chen Y, Wheeler L (1993) J Elasticity 32:175‚Äì182
9. Chrystal G (1980) Algebra. An elementary text-book. Part I. Chelsea Publish-
ing Company, New York
10. Dui G, Chen Y-C (2004) J Elasticity 76:107‚Äì112
11. Friedberg SH, Insel AJ, Spence LE (2003) Linear Algebra. Pearson Education,
Upper Saddle River, New Jersey
12. Gantmacher FR (1959) The theory of matrices. Chelsea Publishing Company,
New York
13. Guo ZH (1984) J Elasticity 14:263‚Äì267
14. Halmos PR (1958) Finite-Dimensional Vector Spaces. Van Nostrand, New York
15. Hill R (1968) J Mech Phys Solids 16:229‚Äì242
16. Hill R (1978) Adv Appl Mech 18:1‚Äì75
17. Hoger A, Carlson DE (1984) J Elasticity 14:329‚Äì336
18. Itskov M (2002) Z Angew Math Mech 82:535‚Äì544
19. Itskov M (2003) Comput Meth Appl Mech Engrg 192:3985‚Äì3999
20. Itskov M (2003) Proc R Soc Lond A 459:1449‚Äì1457
21. Itskov M (2004) Mech Res Commun 31:507‚Äì517
22. Itskov M, Aksel N (2002) Int J Solids Struct 39:5963‚Äì5978
23. Kaplan W (2003) Advanced calculus. Addison Wesley, Boston
24. Kato T (1966) Perturbation theory for linear operators. Springer, New York
25. Kreyszig E (1991) DiÔ¨Äerential geometry. Dover Publication, New York
26. Lax PD (1997) Linear algebra. John Wiley & Sons, New York

232
References
27. Lew JS (1966) Z Angew Math Phys 17:650‚Äì653
28. Lubliner J (1990) Plasticity theory. Macmillan Publishing Company, New York
29. Ogden RW (1984) Non-Linear Elastic Deformations. Ellis Horwood, Chichester
30. Ortiz M, Radovitzky RA, Repetto EA (2001) Int J Numer Meth Engrg 52:1431-
1441
31. Papadopoulos P, Lu J (2001) Comput Methods Appl Mech Engrg 190:4889‚Äì
4910
32. Pennisi S, Trovato M (1987) Int J Engng Sci 25:1059‚Äì1065
33. Rinehart RF (1955) Am Math Mon 62:395‚Äì414
34. Rivlin RS (1955) J Rat Mech Anal 4:681‚Äì702
35. Rivlin RS, Ericksen JL (1955) J Rat Mech Anal 4:323‚Äì425
36. Rivlin RS, Smith GF (1975) Rendiconti di Matematica Serie VI 8:345‚Äì353
37. Rosati L (1999) J Elasticity 56:213‚Äì230
38. Sansour C, Kollmann FG (1998) Comput Mech 21:512‚Äì525
39. Seth BR (1964) Generalized strain measures with applications to physical prob-
lems. In: Reiner M, Abir D (eds) Second-order eÔ¨Äects in elasticity, plasticity
and Ô¨Çuid dynamics. Academic Press, Jerusalem
40. Smith GF (1971) Int J Engng Sci 9:899‚Äì916
41. SokolnikoÔ¨ÄIS (1964) Tensor analysis. Theory and applications to geometry and
mechanics of continua. John Wiley & Sons, New York
42. Spencer AJM (1984) Constitutive theory for strongly anisotropic solids. In:
Spencer AJM(ed) Continuum theory of the mechanics of Ô¨Åbre-reinforced com-
posites. Springer, Wien, New York
43. Steigmann DJ (2002) Math Mech Solids 7:393‚Äì404
44. Ting TCT (1985) J Elasticity 15:319‚Äì323
45. Truesdell C, Noll W (1965) The nonlinear Ô¨Åeld theories of mechanics. In: Fl¬®ugge
S (ed) Handbuch der Physik, Vol. III/3. Springer, Berlin.
46. Wheeler L (1990) J Elasticity 24:129‚Äì133
47. Xiao H (1995) Int J Solids Struct 32:3327‚Äì3340
48. Xiao H, Bruhns OT, Meyers ATM (1998) J Elasticity 52:1‚Äì41
49. Zhang JM, Rychlewski J (1990) Arch Mech 42:267‚Äì277
Further Reading
50. Abraham R, Marsden JE, Ratiu T (1988) Manifolds, Tensor Analysis and Ap-
plications. Springer, Berlin Heidelberg New York
51. Akivis MA, Goldberg VV (2003) Tensor Calculus with Applications. World
ScientiÔ¨Åc Publishing, Singapore
52. Anton H, Rorres C (2000) Elementary linear algebra: application version. John
Wiley & Sons, New York
53. Ba¬∏sar Y, Weichert D (2000) Nonlinear Continuum Mechanics of Solids. Funda-
mental Mathematical and Physical Concepts. Springer, Berlin Heidelberg New
York
54. Bertram A (2005) Elasticity and Plasticity of Large Deformations. An Intro-
duction. Springer, Berlin Heidelberg New York
55. Betten J (1987) Tensorrechnung f¬®ur Ingenieure. Teubner-Verlag, Stuttgart
56. Bishop RL, Goldberg SI (1968) Tensor Analysis on Manifolds. The Macmillan
Company, New York

References
233
57. Borisenko AI, Tarapov IE (1968) Vector and Tensor Analysis with Applications.
Prentice-Hall, Englewood CliÔ¨Äs
58. Bowen RM, Wang C-C (1976) Introduction to vectors and tensors. Plenum
Press, New York
59. Brillouin L (1964) Tensors in Mechanics and Elasticity. Academic Press, New
York
60. Chadwick P (1976) Continuum Mechanics. Concise Theory and Problems.
George Allen & Unwin, London
61. Dimitrienko Yu I (2002) Tensor Analysis and Nonlinear Tensor Functions.
Kluwer Academic Publishers, Dordrecht
62. Fl¬®ugge W (1972) Tensor Analysis and Continuum Mechanics. Springer, Berlin
Heidelberg New York
63. Golub GH, van Loan CF (1996) Matrix computations. The Johns Hopkins
University Press, Baltimore
64. Gurtin ME (1981) An Introduction to Continuum Mechanics. Academic Press,
New York
65. Lebedev LP, Cloud MJ (2003) Tensor Analysis. World ScientiÔ¨Åc Publishing,
Singapore
66. L¬®utkepohl H (1996) Handbook of matrices. John Wiley & Sons, Chichester
67. Narasimhan MNL (1993) Principles of Continuum Mechanics. John Wiley &
Sons, New York
68. Noll W (1987) Finite-Dimensional Spaces. Martinus NijhoÔ¨ÄPublishers, Dor-
drecht
69. Renton JD (2002) Applied Elasticity: Matrix and Tensor Analysis of Elastic
Continua. Horwood Publishing, Chichester
70. Ru¬¥ƒ±z-Tolosa JR, Castillo (2005) From Vectors to Tensors. Springer, Berlin Hei-
delberg New York
71. Schade H (1997) Tensoranalysis. Walter der Gruyter, Berlin, New York
72. Schey HM (2005) Div, grad, curl and all that: an informal text on vector cal-
culus. W.W.Norton & Company, New York
73. Schouten JA (1990) Tensor analysis for physicists. Dover Publications, New
York
74. ÀáSilhav¬¥y M (1997) The Mechanics and Thermodynamics of Continuous Media.
Springer, Berlin Heidelberg New York
75. Simmonds JG (1997) A Brief on Tensor Analysis. Springer, Berlin Heidelberg
New York
76. Talpaert YR (2002) Tensor Analysis and Continuum Mechanics. Kluwer Aca-
demic Publishers, Dordrecht

Index
algebraic multiplicity of an eigenvalue,
82, 87, 90, 93
analytic tensor function, 145
anisotropic tensor function, 115
arc length, 58
asymptotic direction, 69
axial vector, 96
basis of a vector space, 3
binomial theorem, 146
binormal vector, 60
Cardano formula, 85
Cartesian coordinates, 45, 47, 51, 54
Cauchy
integral, 146
integral formula, 144
strain tensor, 99
stress tensor, 14, 73, 99, 175
stress vector, 15, 53, 73
theorem, 15, 53
Cayley-Hamilton equation, 157, 163,
167
Cayley-Hamilton theorem, 96, 145
characteristic
equation, 82
polynomial, 82
ChristoÔ¨Äel symbols, 44‚Äì46, 50, 55, 66,
77
coaxial tensors, 125
commutative tensors, 20
complex
conjugate vector, 80
number, 79
vector space, 79
complexiÔ¨Åcation, 79
compliance tensor, 99
components
contravariant, 39
covariant, 39
mixed variant, 39
of a vector, 5
composition of tensors, 20
contravariant
components, 39
derivative, 44
coordinate
line, 37, 65
system, 35
transformation, 37
coordinates
Cartesian, 45, 47, 51, 54
cylindrical, 35, 38, 40, 46, 50, 54
linear, 36, 40, 42, 55
spherical, 55
covariant
components, 39
derivative, 44
on a surface, 67
curl of a vector Ô¨Åeld, 51
curvature
directions, 68
Gaussian, 69
mean, 69
normal, 67
of the curve, 59
radius of, 60

236
Index
curve, 57
left-handed, 60
on a surface, 64
plane, 60
right-handed, 60
torsion of, 60
cylinder, 64
cylindrical coordinates, 35, 38, 40, 46,
50, 54
Darboux vector, 62
defective
eigenvalue, 87
tensor, 87
deformation gradient, 134, 154, 161
derivative
contravariant, 44
covariant, 44
directional, 118, 131
Gateaux, 118, 131
determinant of a tensor, 84
deviatoric
projection tensor, 109
tensor, 28
diagonalizable tensor, 87, 144, 148
dimension of a vector space, 3, 4
directional derivative, 118, 131
divergence, 47
dual basis, 8
dummy index, 6
Dunford-Taylor integral, 143, 148
eigenprojection, 88
eigentensor, 107
eigenvalue, 81
defective, 87
problem, 81, 107
left, 81
right, 81
eigenvector, 81
left, 81
right, 81
Einstein‚Äôs summation convention, 6
elasticity tensor, 99
elliptic point, 69
Euclidean space, 6, 79, 80
Eulerian strains, 142
exponential tensor function, 21, 88, 125,
154, 159
fourth-order tensor, 99
deviatoric projection, 109
spherical projection, 109
super-symmetric, 105
trace projection, 109
transposition, 108
Frenet formulas, 61
functional basis, 111
fundamental form of the surface
Ô¨Årst, 65
second, 67
Gateaux derivative, 118, 131
Gauss
coordinates, 64, 66
formulas, 67
Gaussian curvature, 69
generalized
Hooke‚Äôs law, 109
Rivlin‚Äôs identity, 136
strain measures, 142
geometric multiplicity of an eigenvalue,
82, 87, 90, 93
gradient, 42
Gram-Schmidt procedure, 7, 91, 93, 98
Green-Lagrange strain tensor, 128, 134,
142
Hill‚Äôs strains, 142
Hooke‚Äôs law, 109
hyperbolic
paraboloidal surface, 77
point, 69
hyperelastic material, 113, 128, 134
identity tensor, 18
invariant
isotropic, 111
principal, 83
inverse of the tensor, 23
inversion, 23
invertible tensor, 23
irreducible functional basis, 112
isotropic
invariant, 111
material, 113, 128, 134
symmetry, 115
tensor function, 111
Jacobian determinant, 37

Index
237
Kronecker delta, 7
Lagrangian strains, 142
Lam¬¥e constants, 109, 129
Laplace expansion rule, 97
Laplacian, 52
left
Cauchy-Green tensor, 134, 161
eigenvalue problem, 81
eigenvector, 81
mapping, 15, 17, 20, 100‚Äì103
stretch tensor, 142, 161
left-handed curve, 60
length of a vector, 6
Levi-Civita symbol, 10
linear
combination, 3
coordinates, 36, 40, 42, 55
mapping, 12, 27, 28, 99, 108, 109
linear-viscous Ô¨Çuid, 53
linearly elastic material, 99, 128
logarithmic tensor function, 143
major symmetry, 105
mapping
left, 15, 17, 20, 100‚Äì103
right, 15, 100, 102
material
hyperelastic, 113, 128, 134
isotropic, 113, 128, 134
linearly elastic, 99, 128
Mooney-Rivlin, 113
Ogden, 113
orthotropic, 138
St.Venant-KirchhoÔ¨Ä, 129
transversely isotropic, 115, 130, 134
mean curvature, 69
mechanical energy, 53
membrane theory, 76
metric coeÔ¨Écients, 18, 66
middle surface of the shell, 71
minor symmetry, 106
mixed product of vectors, 10
mixed variant components, 39
moment tensor, 74
momentum balance, 50
Mooney-Rivlin material, 113
moving trihedron of the curve, 60
multiplicity of an eigenvalue
algebraic, 82, 87, 90, 93
geometric, 82, 87, 90, 93
Navier-Stokes equation, 54
Newton‚Äôs formula, 84
normal
curvature, 67
plane, 66
section of the surface, 66
yield stress, 174
Ogden material, 113
orthogonal
spaces, 27
tensor, 24, 92, 94
vectors, 6
orthonormal basis, 6
orthotropic material, 138
parabolic point, 69
permutation symbol, 10
plane, 64
plane curve, 60
plate theory, 76
point
elliptic, 69
hyperbolic, 69
parabolic, 69
saddle, 69
polar decomposition, 161
principal
curvature, 68
invariants, 83
material direction, 115, 138
normal vector, 60, 66
stretches, 142, 163, 164, 166
traces, 84
proper orthogonal tensor, 95
Pythagoras formula, 7
radius of curvature, 60
representation theorem, 126, 128
residue theorem, 146, 147
Ricci‚Äôs Theorem, 47
Riemannian metric, 66
right
Cauchy-Green tensor, 113, 116, 128,
134, 161
eigenvalue problem, 81

238
Index
eigenvector, 81
mapping, 15, 100, 102
stretch tensor, 142, 159, 161
right-handed curve, 60
Rivlin‚Äôs identities, 135
rotation, 13
tensor, 13, 161
Rychlewski‚Äôs theorem, 129
saddle point, 69
scalar
Ô¨Åeld, 40
product, 6
of tensors, 25
second Piola-KirchhoÔ¨Ästress tensor,
128, 138, 171
second-order tensor, 12
Seth‚Äôs strains, 142
shear yield stress, 177
shell
continuum, 71
shifter, 71
similar tensors, 216
simple shear, 83, 154, 159, 164
skew-symmetric
generator, 129
tensor, 22, 94, 95
spectral
decomposition, 87, 107
mapping theorem, 81
sphere, 64
spherical
coordinates, 55
projection tensor, 109
tensor, 28
St.Venant-KirchhoÔ¨Ämaterial, 129
straight line, 57
strain energy function, 113
strain tensor
Cauchy, 99
Green-Lagrange, 128, 134, 142
strains
Eulerian, 142
Hill‚Äôs, 142
Lagrangian, 142
Seth‚Äôs, 142
stress resultant tensor, 74
stress tensor
Cauchy, 14, 73, 99
second Piola-KirchhoÔ¨Ä, 128
stretch tensors, 142, 161
structural tensor, 115
summation convention, 6
super-symmetric fourth-order tensor,
105
surface, 64
hyperbolic paraboloidal, 77
Sylvester formula, 89, 148
symmetric
generator, 129
tensor, 22, 90, 91
symmetry
major, 105
minor, 106
symmetry group, 115
anisotropic, 116
isotropic, 115
of Ô¨Åber reinforced material, 138
orthotropic, 138
transversely isotropic, 115, 130
triclinic, 115
tangent
moduli, 134
vectors, 37
tensor
defective, 87
deviatoric, 28
diagonalizable, 87, 144, 148
Ô¨Åeld, 40
function, 33
analytic, 145
anisotropic, 115
exponential, 21, 88, 125, 154, 159
isotropic, 111
logarithmic, 143
identity, 18
invertible, 23
left Cauchy-Green, 134, 161
left stretch, 142, 161
monomial, 21, 144
of the fourth order, 99
of the second order, 12
of the third order, 28
orthogonal, 24, 92, 94
polynomial, 21, 88, 125
power series, 21, 142
product, 16

Index
239
proper orthogonal, 95
right Cauchy-Green, 113, 116, 128,
134, 161
right stretch, 142, 161
rotation, 13, 161
skew-symmetric, 22, 94, 95
spherical, 28
structural, 115
symmetric, 22, 90, 91
tensors
coaxial, 125
commutative, 20
composition of, 20
scalar product of, 25
third-order tensor, 28
torsion of the curve, 60
torus, 69
trace, 26
trace projection tensor, 109
transposition, 21
transposition tensor, 108
transverse shear stress vector, 74
transversely isotropic material, 115,
130, 134
triclinic symmetry, 115
unit vector, 6
vector
axial, 96
binormal, 60
complex conjugate, 80
components, 5
Darboux, 62
Ô¨Åeld, 40
function, 33
length, 6
product of vectors, 10, 13
space, 1
basis of, 3
complex, 79
dimension of, 3, 4
Euclidean, 6
zero, 1
vectors
mixed product of, 10
orthogonal, 6
tangent, 37
velocity gradient, 141, 154, 159
Vieta theorem, 68, 83, 145
von Mises yield function, 174
Weingarten formulas, 67
yield stress
normal, 174
shear, 177
zero tensor, 12
zero vector, 1


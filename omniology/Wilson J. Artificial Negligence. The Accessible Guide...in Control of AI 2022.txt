
 
 
 
 
 
 
 
 
 
 
 
“AI is likely to be either the best or
the worst thing to happen to
humanity”
-          Stephen Hawkings

 
 
 
 
 
 
 
For Sam, Liv, Fred & Ruby
 
With Love
 
 
 
 
 
 
 
 
 
 
 
 
 
© 2022 James L Wilson
All Rights Reserved
 
 
All images (including cover) are courtesy of Shutterstock except
where individual accreditation is provided.
Use of individually accredited images is covered under the Creative
Commons licensing agreements (CC BY 2.0, CC BY 3.0 and CC BY
4.0) as appropriate to the individual image
All Figures are Copyright James Wilson 2022
 
 
First Edition

CONTENTS
CONTENTS
Get Your Phone Out!
READ.ME.FIRST
Why Write This Now?
What is Artificial Intelligence?
Algorithms
Narrow vs. General AI
Machine Learning
Supervised Learning
Unsupervised Learning
Reinforced Learning
Neural Networks and Deep Learning
Natural Language Processing
Computer Vision
Digital Twins
Generative AI
Generative Adversarial Networks (GANs)
Swarm Intelligence
Human Augmentation
Chaining
Quantum Computing
Blockchain/Cryptocurrency
Virtual Reality and Augmented Reality
More to Come…
What Isn’t Artificial Intelligence?
It Isn’t T-800
Robotics vs AI
It Is Also Not HAL 9000 (yet)
It Is Not Absolution From Blame

It’s Not A Replacement For Humanity
Instinct
They Are Not Genuinely Creative
The Wonders of Artificial Intelligence
Medical/Health
Industrial Applications
Agricultural and Food
Sustainability
Smart Cities/Buildings
Solving Problems Others Can’t
Transport
What are Autonomous Vehicles?
Two Main Types of Autonomous Systems
Applications of Autonomy
What’s the Hold-up?
Big Benefits
Even Bigger Challenges
The Moral Dilemma
Who Is To Blame?
The Elephant in the Room
The Gorilla behind the Elephant in the Room
Work
Introducing Maslow
AI and Automation
The Potential of AI and Automation
The impact on the Workplace
The Impact on the Workforce
The Dynamics of Work
Mortality
Transhumanism
Biogerontology

The Matrix Theory
Life, But Not As We Know It
Social Media
Where does Artificial Intelligence fit in?
The Metaverse
Social Media and the Education System
A few words on Google
Privacy
Privacy Challenges We Cannot Easily Control
The Only Way Is Ethics
Moral Principles
Basic Behavioural Scenarios
So What?
Bias in AI Creation
How We Must Apply Ethical Principles
How AI Must Apply Ethical Principles
How Could AI Creators Be Held Accountable?
The Other Side Of The Coin
Explain Yourself
The Challenges With Explainability
It’s All About The Data
Garbage In…Garbage Out
Bias or Skew
Nefarious Bad Data
Do You Speak Data?
Sustainability and Data
Anthropomorphism
The Human Disease
Oh Good Boy!
How Far Do We Go?

AI as a Weapon
Conventional Weapons
Weaponising Cyberspace
Staying in Control
The Singularity etc.
We Are Not There Yet
Artificial General Intelligence
Beyond Human
The Turing Test
The Singularity
Doomsday
Harmonious Living
Our Actions Matter
How Do We Stay In Control?
Stop Everything
The Oracle Or Walled Garden Approach
Regulation, Control And Visibility
Control Is Not Optional
READ.ME.LAST
Sauces
Films:
TV Series/Shows:
Books:
Podcasts:
Video Game
About the Author

Get Your Phone Out!
Hello Reader/Person in a Bookshop/Person in a Charity Shop
 
Yes, I am talking directly to you. Listen carefully or you might miss
out!
 
You will come across various QR Codes in amongst the text of the
chapters ahead of you. Feel free to scan these with the photo app
on your phone/AR glasses/Virtual Holographic Assistant… whatever
is the in-thing for scanning QR codes by the time you read this.
 
And on some devices you will be able to just click/press on the QR
code to open the content, but I can’t guarantee it will work on all
your gadgets. Give it a go.
 
Actually… Come to think of it… QR codes might not even exist
anymore and you might be uploading this text directly via your
neural link. If so, disregard this completely. Otherwise you can
follow the links that are offered up in the codes to directly access
some of the external content I recommend along the way, like cat-
free Youtube videos and informative websites.
 
Have fun 
 

READ.ME.FIRST
Red Flags Everywhere!
 
A call from my son: “Hi Dad, my manager has offered for me to go
on some training and get a certification of some sort. He suggested
I do my CIMA and become a chartered accountant. What do you
think?”
 
Red Flag 
 
A conversation I had with a colleague while at a conference
recently: “Have you ever thought about exploring your family tree? I
signed up to one of those genealogy services, did a DNA test and
now I have been connected with people all over the world who I
never knew I was related to. You should do it too!”
 
Red Flag 
 
A posting from a contact on LinkedIn: “...using innovations like
DALL-E will mean that AI can be truly creative. The old ways of
creating art are dead.”
 
Very infuriating Red Flag 
 
The main point here is that these weren’t isolated incidents. Almost
every day something gets my spider-sense tingling. I encounter
people that are misrepresenting, misunderstanding, or just plain
ignoring what Artificial Intelligence means for us, now and in the
future. And, as hard as I have tried, I simply couldn’t locate a single
book that pitches the points and arguments people need to consider
at the right level. There just doesn’t seem to be one volume that I
could recommend for the vast majority of people who don’t have

the natural inclination, or time, to read a fully-fledged scientific
book. It’s not a judgement of their intellect, simply a reflection on
their interest in the subject; just in the same way that I wouldn’t
naturally reach for a heavyweight book about Spirituality, or maybe
Gardening. Of course, there are some fantastic books out there by
the likes of Nick Bostrom, Stuart Russell, Max Tegmark and Kai-fu
Lee, but almost without exception they take their explanations to a
level that is too deep and analytical for my target audience; their
books just wouldn’t pass the quick flick-through test in the book
shop.
 
This is AI for the generalist, not the scientist, but my greatest hope
is that something, or somethings, in this book will provoke you into
doing your own research. It’s not hard; simply pick up your device
and search the internet, or read one of the great books by authors
like the ones I referenced above.
 
Have fun and don’t be too scared; our future is mostly still in our
own hands. 
 
Enjoy and share!

Why Write This Now?
I am probably already a bit late.
 
This section is one of the most theoretical and technical, but I will
try to keep it as short as possible, because the themes in it are
fundamental if we are going to be able to understand and react to
the changes that are already underway.
 
To explain all of this I need to start by ensuring everyone is
comfortable with one term - Exponential - and how it differs from
another - Linear.
 
Confession time; I am a bit of a space geek. Not in a Star Wars/Trek
Sci-Fi way. No, I am fascinated by real space exploration,
particularly the amazing endeavours of the 50’s and 60’s when
humans risked, and often sacrificed, their lives by strapping
themselves to what equated to over 500 tonnes of TNT (the total
explosive power produced by the  3 stages during the launch of a 
Saturn V rocket) and literally exploding into orbit. I am enthralled
by the bravery of those amazing cosmonauts and astronauts that
pushed the envelope of human ability, even if some of the
underlying drive for that push was founded in the competitive
tensions of the Cold War. But I choose to believe there is something
more innate about our desire for progression and that, despite the
political backdrop to the Space Race, men like President John F
Kennedy also believed this. Take a look at an excerpt from his
famous “We choose to go to the moon” speech at Rice University in
1962:
 
“No man can fully grasp how far and how fast we have come, but
condense, if you will, the 50,000 years of man’s recorded history in
a time span of but a half-century. Stated in these terms, we know
very little about the first 40 years, except at the end of them
advanced man had learned to use the skins of animals to cover

them. Then about 10 years ago, under this standard, man emerged
from his caves to construct other kinds of shelter. Only five years
ago man learned to write and use a cart with wheels. Christianity
began less than two years ago. The printing press came this year,
and then less than two months ago, during this whole 50-year span
of human history, the steam engine provided a new source of
power.
Newton explored the meaning of gravity. Last month electric lights
and telephones and automobiles and airplanes became available.
Only last week did we develop penicillin and television and nuclear
power, and now if America's new spacecraft succeeds in reaching
Venus, we will have literally reached the stars before midnight
tonight.”
JFK was latching on to the fact that the pace of change has always
been accelerating and if you were to plot the innovations he
namechecks in these lines onto a chart and drew a line through
them you would produce an Exponential curve of change.
 
The underlying concept of Exponential is fairly straightforward and
basically means that, for every repeated interval (most often this is
a time interval - day, month, year etc.) something else at least
doubles. Let me give you a fun(ish) and appropriate example that
shows the difference between Exponential and Linear progression
more clearly:
 
If your stride length was approximately one metre and I asked you
how far you would get in 30 normal, linear strides, one foot after
the other, in a straight line, you would surely come back with the
obvious answer; 30 metres, right?

Next I would tell you that you could take 30 Exponential steps,
where the first step was 1 metre, the second 2 metres, third 4
metres, fourth 8 metres, and so on. Then I would ask you how far
30 strides would take you this time. That’s a bit trickier to work out
in your head and the answer would probably surprise many of you.
In fact your 29th step would take you to the surface of the moon
and your 30th step would be long enough to get you back home
again.
 
 
So if we plot those two very different journeys on a simple chart we
get this:
 
And you can see that the 30 metres in 30 linear steps is basically a
flat line and barely registers on the same chart as our exponential
journey of nearly 1,100,000,000 metres!
 

Impressive huh? Well that’s what is happening with technology and
it is largely being driven by something called Moore’s “Law”.
 
In 1965 Computer Engineer Gordon Moore stated that “The number
of transistors per silicon chip doubles every year”. That time interval
was subsequently revised to two years, but in actual fact the reality
has been somewhere in-between, at around 18 months. Since
transistors on computer chips are the medium by which the
instructions in our software is actually executed, this doubling is
synonymous with a doubling in computing power. Some of you may
have noticed the quotes I placed around the word “Law” when I
introduced it. There are three reasons for this:
1. It is not a law in the same sense as, for instance, the Law of
Gravity or Bernoulli's Law of Fluid Dynamics. In fact it is more
of a credo (meaning “a statement of the beliefs or aims which
guide someone's actions” - Oxford English Dictionary). In
reality chip manufacturers have had to work themselves to the
bone in order to keep pace with Moore’s prediction. So, in
some respects it has been more of a “gauntlet throw down”
than a “law”.
2. Unlike a real law, it is very unlikely to continue ad infinitum, at
least in terms of transistors on chips (but it may be possible if
you think of it in terms of computing power once Quantum
Computing becomes viable), because there is a physical limit
to how small a transistor can be (e.g. one atom?). That limit is
approaching, although in 2022, Chinese Scientists have been
able to create a transistor which is only 0.3 nanometers in
width.
3. The costs of developing and maintaining the laboratories and
manufacturing facilities capable of producing these rapidly
miniaturising components is eye-watering and global economic
conditions may start to hamper future investments required to
continue the trend.
 
But that slowdown doesn’t seem to be upon us just yet, and the
chart below represents the rapid, Moore’s “law”-abiding, growth of

transistors on chips over the past 50 years and how it overlays on
our Exponential curve:
 
So, you can see how we are now on an increasingly steep trajectory
of growth in computing power and consequently entering a period
of almost terrifying technological change. Are we ready for it?
 
Short answer is; No.
 
And the reason for this is that we are largely conditioned to live in a
linear society. We always have been. From the dawn of man we
have been self-conditioned to live in a world where we get out of
bed at a pre-defined time, have breakfast, commute, work,
commute again, dine, partake in some form of recreation, then
sleep. Rinse and repeat. We are conditioned to yearn for
predictability and stability. In the developed world, our upbringing
generally coaches us towards a life like this:

 
Even when we do embrace change, we generally do so at a glacial
pace, often as a result of necessity rather than desire. But if you
think we, as individuals, are poor adopters of change, consider how
poor the governing bodies of the world are at doing it; as of 2021
there are still only 29 countries where same-sex marriage is legal,
and it wasn’t until 1992 that the Catholic church pardoned Galileo
for daring to suggest that the Earth wasn’t the centre of the
universe (only 300 years late)!
 
But up until this point we have somehow coped with technological
advance, possibly because of the lack of mandate on these
changes; you can still ignore the online banking apps and walk into
a branch to pay a bill if you want to. But it is also probably because
the pace is only just beginning to ride the uptick of the exponential
curve. This has given us the luxury of time to adapt and adopt. A lot
of the technological changes thus far have been released in
relatively small chunks. Let’s call these “micro-changes’. For
example, the release of the “like” button and later the ability to
“tag” people we know in photographs on social media were micro-
changes with nearly two years between their introduction. This
relatively gradual introduction allowed people time to adjust to the
associated behavioural change (note that I used the word “adjust”,
not “benefit”).
 
However we have reached a point now where the rate of
technological change means that the pace of micro-change release

is increasing dramatically while, societally, we still live largely the
way we did thirty years ago; same routine, same expectations.
Underpinned by Moore’s law (and enormous reductions in the cost
of production**), this rapid acceleration is being facilitated by the
rapid uptake of Artificial Intelligence, which we are nurturing at
what feels like the speed of light. Every single day you can skim
through the industry press and hit headlines about some new
service enabled by the advances in AI. In fact, I was at risk of never
actually finishing this book because every day there was some new
innovation in the press or research I was reviewing that I wanted to
reference. Today, it was the vacuum cleaner that uses Computer
Vision and Lidar to detect the amount of dust ahead and then
moderate its suction power to save energy. So cool! I am genuinely
such a geek! 
Eventually I had to draw a line.
 
Up until now we have still been coping, still in control, largely
because the majority of the changes are still micro-changes that we
can easily consume, or at least understand. These changes have
been based upon relatively Narrow AI. This has meant that they are
generally automating or introducing something new in a relatively
specific and small way. We can handle that. We can feel like we
remain in control because it doesn’t deviate too far from our Linear
expectations and it still feels like “something we made”. But as the
breadth of AI increases and we head towards Artificial General
Intelligence (some analysts predict this could happen within the
next 20 years), that grasp and control will loosen. Micro-changes
will be replaced by more difficult to comprehend macro-changes. If
we do not take great care and prepare society for the road ahead
we will no longer be able to fully understand, control or adapt to
them. There are even bleaker scenarios than this, but I don’t want
to descend into Skynet (the “evil” computer in the Terminator
franchise) territory just yet.
 

Coming back to JFK’s speech from ‘62; he topped and tailed the
section I reproduced above with the following sentient warnings:
 
“...in an age of both knowledge and ignorance. The greater our
knowledge increases, the greater our ignorance unfolds.”
and
“This is a breathtaking pace, and such a pace cannot help but
create new ills as it dispels old, new ignorance, new problems, new
dangers. Surely the opening vistas of space promise high costs and
hardships, as well as high reward.”
So that is why I am writing this book. Just as JFK predicted the
challenges and potential of the space race, we must now be
prepared for the struggles and, hopefully, conquests of the AI age.I
want to raise awareness of the Exponential and almost inevitable
changes ahead of us, because there are a large number of people
who are either completely oblivious to what lies ahead, or at best
have a slightly rosy, or alternatively very dark, view of the future.
 
I am not setting out to write something that condemns the future.
That would be a very strange thing for me to do since I have
worked in this field for over 30 years. It would be a damning
indictment on my own career (unless it has all been one long
session of aversion therapy), but also because I am, at heart, an
optimist, and I still believe there is time for an aware and mindful
human race to harness all of the wonderful opportunities that
Artificial Intelligence offers us while avoiding most, if not all, of the
bad stuff. In order to reach that happy place I need each and every
one of you to consider and share the arguments in this book, and if
you are interested, dive into some of the far more coherent and
deep resources I recommend along the way.
 
If, however, it all goes wrong because of Artificial Intelligence, be
under no illusion that we will only have ourselves (and perhaps

cats) to blame. We are the creators and we need to remain the
masters, or at least equal partners with our creation.
 
One thing is for certain - Negligence is not an option…or an excuse.
 
**In 1985 the level of technology in your smartphone would have
been valued at over $1 Million. In that same year, the GPS
functionality alone would have cost almost $250K to rent for one
hour!

What is Artificial Intelligence?
 
Let’s start with a definition from the Oxford English Dictionary:
 
 
Pretty straightforward really and probably not a million miles away
from the general school of thought. Artificial Intelligence is a way of
doing stuff usually done by humans in a computerised, systematic
way. But notice that the definition is adverb and qualification free. It
doesn’t say “Artificial Intelligence is a way of getting computers to
do stuff we humans usually do only faster and better.” This
definition doesn’t really capture the motivation or aspirations that
drove the world’s biggest organisations to invest approximately $19
Billion in it during 2020 alone. It is true that Artificial Intelligence is
largely about automation and simulation of human behaviour, but
the definition is completely missing the why. OK, so some of the
theoretical research and development in the field might appear at
times to be an exercise in simulation for simulation’s sake, and
often doesn’t produce results that are faster or better than human
endeavour, but some of the main reasons that companies and
governments are investing so much time and money in the field
are:
●      Saving Time
●      Saving Effort
●      Saving Money
●      Negating Risk
●      Improving Costly, Dangerous and Time Consuming Processes
●      Finding Solutions to Problems We Can’t Solve Humanly

●      Finding Solutions to Problems We Don’t Know We Have Yet
Sounds like a sensible-ish list, right? But what actually is this
technology that is going to help us achieve these goals? Well let’s
look at some of the basics around the fields of research we are
ploughing our time and money into:
Algorithms
At its simplest level, Artificial Intelligence is computer code that
performs a task, or tasks, with a series of input parameters, some
conditional logic, and ultimately some output. This is, as computer
programming has always been, a reflection of how human beings
operate.
 
This morning I woke up before my alarm, turned over and looked at
my alarm clock, rolled my eyes because it was about 6:15am and
there really was no point in going back to sleep. So I got up.
 
An algorithmic way of thinking about this would be:
IF JAMES WAKES UP AND ALARM_TIME MINUS ACTUAL_TIME IS
LESS THAN 30 MINUTES THEN JAMES GETS UP ELSE JAMES GOES
BACK TO SLEEP
 
The Inputs are James’ waking up, the time his alarm is set for and
the actual time. The process logic is to work out if it’s too close to
his planned time to get up for it to be worth his while trying to get
back to sleep. The output is either for James to get up or James to
go back to sleep.
 
Rules-based algorithms are everywhere in life. Consider something
as simple as a food recipe providing a set of ingredients (inputs),
cooking instructions (logic) and finally a meal that never looks quite
like the picture (output).
 
Fundamentally, that is what Artificial Intelligence is, completely
aligned to our Dictionary definition above; it’s a way of setting a

computer to work to do something we want it to do (well at least
that’s generally the aspiration) in an algorithmic way.
Narrow vs. General AI
This is more of a clarification than a technique or technology, but it
is worth making it clear that most, if not all, of the Artificial
Intelligence solutions we currently create are Narrow AI, meaning
that the scope is very much limited to solving one particular
problem. However we are on the cusp (because of Machine
Learning) of generating AI solutions that are more versatile and can
tackle a broader scope of tasks. Take for example AlphaGo,
produced by DeepMind (a subsidiary of Google). This was initially
developed entirely for the purposes of winning at the ancient game
of Go, however since proving universally successful at this task, the
scope has been broadened such that it can now train itself to play
almost any game and win.
AlphaGo beat the human world champion Lee Seedol 4-1 in a Go challenge. The
same technology has since beaten the world’s most advanced Chess simulator
after taking just 4 hours to train itself on the rules and strategies of the game.
(Left Image courtesy of Cyberoro ORO. Right image courtesy of Xchen27)
 
Now let’s look a little more deeply at some of the key AI techniques
that have been developed to allow the computer to do this kind of
thing.

Machine Learning
Machine Learning is the innovation in Artificial Intelligence at the
heart of almost all of the advances we are seeing in the field. It will
be critical for the movement away from Narrow/Weak AI solutions
towards AI with more broad capabilities, potentially even Artificial
General Intelligence (one day…possibly…we aren’t even close at this
stage…don’t panic).
 
Instead of relying on a data scientist to write a structured
algorithm, full of IF-THEN-ELSE logic (often called GOFAI - Good Old
Fashioned AI), Machine Learning directly applies statistical
techniques against the oceans of data we create and hoard in order
to generate its own algorithms (in machine learning parlance these
are often called models). Machine Learning looks for patterns and
repeated features in the data it scans and makes its own inferences
and associations based upon them. Then it builds its own rule set to
allow it to identify the same patterns in fresh data. I will get on to a
little more (but not too much) on the structure of these a bit further
down.
 
To help set the scene, let’s look at one of the earliest documented
examples of Machine Learning in action. Back in 2012, Google set
their newly developed Machine Learning platform loose on
thousands of Youtube Videos and asked it to identify objects in
them. The Google engineers didn’t provide the model with any
definition of what objects it might find, simply allowing it to make
whatever sense it could of the video data on its own. It analysed
them frame-by-frame, building its own inferences about what
common objects it could identify. And there was one particular
object that it became increasingly good at identifying as more and
more videos were processed; one thing that seemed to be in almost
every video it processed:
 
Cats. 
 

Youtube is simply awash with videos of our self-entitled household
pets doing supposedly humorous and cute things. The machine
learning model taught itself to identify them above all other things
it identified. Ok, it didn’t actually know that they were called “cats”,
but it was one of the first really successful experiments in applying
the statistical techniques of Machine Learning using vast quantities
of training data and was the springboard for a lot of the advances in
the field of AI that have followed.
 
So if it all goes wrong from here on in we can squarely blame it on
the cats. I never trusted them and I won’t be surprised at all.
Supervised Learning
As I have already said, Machine Learning doesn’t write structured
IF-THEN-ELSE code as it analyses the training data, instead it builds
a kind of decision tree based upon the features in the data filtering
through it, drawing connecting lines between these features based
upon the likelihood of more than one of them being found together.
If a big enough chain is connected together, the Machine Learning
makes the inference, or decision, that it has identified a specific
object in the data. For instance, let’s assume that, using Computer
Vision (more on this later), the Machine Learning Model has learnt
to identify the following features:

1.       Pointy ears
2.       Floppy ears
3.       No ears
4.       A Tail
5.       Four legs
6.       Two legs
7.       No legs
8.       Two arms
9.       Fur
10.   No fur
11.   Short nose
12.   Long Nose
 
Now, let’s pump a bunch of data (e.g. YouTube videos) through the
model, which has been pre-trained to try and pick out cats and dogs
in video footage. This type of training is called Supervised Learning,
where the expected outcomes are identified in the training data so
that the model focuses its efforts on getting as good as possible at
reaching the goal, or goals, we have set (i.e. identifying cats and
dogs). For this first pass of data (imaginatively called Pass 1) the
model would likely decide that, if the images contained features 1,
4 and 9, the subject was a cat, while if it contained 2, 4 and 9, the
subject was a dog. So out the other end of the model will come
three categories of results; one grouping of all the videos it found
with what it believes are cats in them, one with all those likely to
contain dogs, and then a third group for all the footage it thought
contained neither.
 

But the thing is that some of the results would be wrong the first
time around. Amongst other issues, there would be stuff in the cat
grouping that would in fact be videos of dogs, because quite a few
breeds of dogs have pointy ears not floppy ears. So the data
scientists would move the incorrectly categorised data between the
groupings, do a little bit more training on this basis, and then pass
it all back through the model. This is Pass 2.

This time, the model would be forced to use other characteristic
features to make its inferences, because it now knows it can’t rely
on the pointiness of the ears alone to make the decision. It notices
that there is also a difference in nose size and position in the
recategorised images. So it adds feature 11 to the cat
categorisation and feature 12 to dogs. And this time the results are
almost perfect. Almost. There are still a few outliers finding their
way into the dogs category. The data scientists take a look and
realise the problem straight away. There are videos of people in
fancy dress costumes being categorised as dogs! The data scientists
move the videos of Pluto at Disneyland etc into the reject grouping
and feed the data back through the model again. This is Pass 3.
 
This time the Machine Learning works out that feature 5 (four legs)
is important as well and, now the algorithm identifies 99% of the
videos with cats and dogs accurately. The model is deemed a
success.
 
That is a perfect example of Supervised Learning in action, with
specific goals achieved, well at least 99% achieved. The thing with
Machine Learning is that, in practice, it is very rarely 100%

accurate, but in practice, and with enough training, it can easily be
far more accurate than any human, and will always be infinitely
faster. If I gave you a million pictures of animals and asked you to
categorise them as per the example above it would probably take
you days. You would certainly make errors along the way and
definitely get very bored. A Machine Learning model running on a
decent computer will do it in a matter of seconds, without any
complaint.
Unsupervised Learning
So, what about Unsupervised Learning? Well, imagine the same
scenario as above, only this time the data scientists haven’t
instructed the model to specifically find household pets. Instead it is
given free reign to identify whatever features, and correlations
between them it can, producing a set of categorisations of its own.
No training data is required this time because we aren’t looking for
specific goals, allowing the model to make up its own mind about
what groupings of features it puts in each category. This is exactly
what Google did in the cats example at the start of this section. But
assuming the model can find the same features as before it would
probably make a stab at categorisations like this:

So without any interference from the data scientists the model has
been able to to work out categories for four-legged animals, two-
legged animals and other stuff. Not a bad first run, and with more
training data the model would likely be able to refine the existing
categorisations and probably define even more. The big value add
of this Unsupervised Learnng approach is that the Machine Learning
is free to apply whatever techniques it likes and make unguided
inferences as a result. Just in the example above, the unsupervised
approach has already been able to find at least one whole category
of objects (two-legged animals) that our supervised one totally
ignored because it was outside of the set goals. It may also have
started to make an inference on a category for furniture, but it’s a
bit early to tell without more data. The key point is that
Unsupervised models are perfect for discovery because they are not
constrained by very specific goals.They are free to explore beyond
these constraints. This is wonderful where the AI is exploring large
datasets to look for, for instance, a cure for a disease that has
evaded human scientists, but not advisable where the AI has to
make critical decisions based upon the output of the model. As
discussed later in the Autonomous Vehicles section, you would
never let your self-driving car loose on the streets if it ran on a
model where it was allowed to learn by trial and error.
Reinforced Learning
Happily there is a halfway house between Supervised and
Unsupervised Learning that leverages the best of both models (i.e.
the specificity and accuracy of Supervised versus the opportunism
and exploration of Unsupervised). It is called Reinforced Learning.
In simple terms, this involves swapping between phases of both
techniques; after starting with a period of Unsupervised learning,
the data scientists might provide a little supervision to correct any
glaring issues, or perhaps to validate good categorisations that the
model is making before moving back to Unsupervised learning to
continue; rinse and repeat until the model has either proved to be
as successful as possible and is applied to the real world, or is

discarded if it is not fit for purpose or providing useful/valuable
results.
Neural Networks and Deep Learning
A few words on two terms you will hear often when people start
talking about Artificial Intelligence and Machine Learning. Basically
these refer to the structures that the models are stored in. I
mentioned before that models store the identified features in the
data and the connections between them. Well the way they do this
is fairly synonymous with the way our brains work, with Neurons
storing data points and Synapses connecting them. The strength of
the synaptic connection between two neurons is representative of
how likely the two features are to be found together i.e. a tail and
four legs. This is exactly the same way that Machine Learning works
and as such the model is stored in a structure that closely mimics
the workings of the brain. Hence the name Neural Network.
 
“Deep Learning”, a term made popular in the mid-2000’s by
Geoffrey Hinton, a British-Canadian cognitive psychologist, is really
nothing more than a Neural Network with a lot of layers of Neurons,
connected by a lot of Synapses. The more layers and connections,
the more elaborate the collection of data features the model can
use to make its inferences. The “deeper” these get, the more brain-
like the capabilities of the models they house. But we are some way
off being able to mimic something of the scale of a real human
brain, which has about a hundred billion (1011) neurons and a
quintillion (1015) synapses!
 
 
A couple of times during the overview above I have mentioned
Computer Vision, which is
just one of a number of advanced modelling techniques that AI
models can apply to identify features in the data. Let’s look at the
range of common techniques in use in models today, as well as
some of the complementary technologies that are associated with
them:

Natural Language Processing
 
Let’s start by breaking this down into three key components, but it
is often referred to as just NLP:
 
●      Natural Language Understanding (NLU) - The ability to
identify the words and phrases in human speech
●      Natural Language Processing (NLP) - The ability to
interpret the words and phrases identified in the NLU step
●      Natural Language Generation (NLG) - the ability to
formulate an appropriate response to the two steps above
 
There are a number of heavyweight Deep Learning based NLP
platforms that have already been developed, crunching huge
volumes of training data (for example the whole of Wikipedia) to
enable startling capabilities in this field. One of the most popular is
OpenAI’s GPT-3. As an example of its capabilities, see the brief
conversation I had with it while writing this section:

OK, so some of the responses (“organic sack of bones” 
) are a
little bit creepy and not quite what you would expect, but you can
see how smart it is becoming already and this will only continue to
progress until applications using this technology can regularly and
easily pass the Turing Test; put simply - If a machine can engage in
a conversation with a human without being detected as a machine,
it has demonstrated human intelligence. The biggest challenge in
trying to make NLP systems mimic a human is in making them
comprehend the words in the same way, and with the same
context, a human would. For instance, if I said to you “I saw a lady
with a cat”, you would probably envisage a woman with a cat next
to her, or possibly on her lap. What you are unlikely to imagine
would be the lady lying on the floor while I try to saw her in half
with a particularly sharp cat, or possibly me using the cat as a
bizarre organic telescope to spy on the woman. They sound like
ridiculous ideas to you and me, but the AI system does not have our
experiences or understanding of reality to help ground its
interpretation of our words. A lot of the research and development
in NLP revolves around trying to embed this contextual
understanding of human behaviour and speech patterns into the
NLP engines.
 
Natural extensions to this technique include both Speech
Recognition and Speech Generation (also important for Generative
AI below), which are in effect audio inputs and outputs to the NLP
process. There’s also Sentiment Analysis, which can identify the
emotional nature of human responses to situations. These
technologies have been in regular use for some time now,
particularly in analysis of posts on chatrooms, forums and social
media.
 
One of the most successful demonstrations of NLP was the
resounding victory of IBM’s Watson in the popular US game show
Jeopardy. Trained on a huge volume of data, the computer

absolutely crushed reigning champions Ken Jennings and Brad
Rutter in a three day competition.
 
IBM Watson (represented by the white monitor) crushing the competition on the
game show Jeopardy
(Image courtesy of Rosemaryetoufee)
 
To date, one of the biggest non-gameshow, business applications of
this technology has been seen with Chatbots, used in customer
service departments around the world, where hand off to a human
representative is only required when the conversation becomes too
specialist for the knowledge base in the NLP. However NLP
technology is almost everywhere nowadays, particularly with the
proliferation of voice assistants like Amazon Alexa and Apple’s Siri in
our homes and devices.
 
Computer Vision
As well as the ability to recognise and process text input, one the
other major techniques for capturing and interpreting data is
Computer Vision; the ability to interpret images, whether they are
still or animated (e.g. video). This was obviously the technique
being used in the ground-breaking cat-wrangling work Google did
back in 2012, and also the one I used to explain how Machine
Learning works. Over the past decade there have been incredible

advances in this field, driven by machine learning, enabling accurate
object identification; a critical component of self-driving vehicle
technology, facial recognition, medical diagnosis and interpretation
of satellite images for examining things like climate change impact.
Basically, if you can see it, Computer Vision can interpret and
categorise it.
 
 
Other emerging capabilities of Computer Vision include Affective
Computing, where the input data can be used to identify emotional
responses. One initial experiment with this was introduced using
cameras in the eyes of store mannequins to read customers’
reactions to the outfits on display, and several US State Police
Forces are evaluating this technology, combined with biometric
data, to identify if an interviewed suspect is lying. Initial trial results

suggest that it is far more accurate than traditional polygraph
technology.
Digital Twins
Up front, this is not the ability to clone yourself physically…at least
not yet, but it is the ability to simulate a complex physical and/or
digital ecosystem in software that mirrors the properties and
behaviours of the objects in the real system. This enables the
creators to do clever things like modelling the impacts of change in
the real system and even run what-if scenarios that allow them to
plan more effectively for things that could happen in the future. This
technology is seeing widespread adoption in pretty much every
industry, from modelling financial markets to simulate the impact of
global events that could affect trading (such as a global pandemic),
to creating digital twins of smart cities to simulate the impacts of
adverse events such as power outages or traffic disruption.
Generative AI
Grabbing a lot of (often fake) headlines, Generative AI is a rapidly
growing field of AI where the Algorithms generate new audio, text
or video content based upon their training data. While this has
many useful and practical applications, many of which are in the
creative arena where teams are working on training algorithms to
paint, compose or write new works of fiction, it is also heavily
utilised in the growing phenomenon called Deep Fakes. If you want
to see some good examples of the power of Generative AI in
harmless deep fakes, try looking up the “Alternative Queen’s Speech
2020”  or “Obama Deep Fake” on Youtube. In the latter example,
just 14 hours of training video of the real, former US President was
enough for the deep learning model to generate a video that fooled
thousands of people.

These QR Codes take you to the harmless deep fakes of speeches by the Queen
and Obama
(Left image courtesy of Pete Souza, right image courtesy of the Peabody
Awards)
 
In that case, the movements and speech of the video were
synchronised to those of Jordan Peele, an American comedian,
however this technique is already fully capable of generating
completely new content without the need of a human stooge.
Imagine the scenario where an actor is required to perform a
dangerous stunt; rather than risking the life of the movie’s main
asset, or a real stunt double, why not use Generative AI as a virtual
double for the scene? If nothing else, think of the insurance money
the producers would save!
 
To counteract the rise of deep fakes, one critical area of focus has
been on developing AI systems that can analyse content and
identify whether it is real or fake. This involves training the
algorithms to detect imperfections in the audio and video, as well
as fact checking content against reliable sources. However, this is a
continual struggle as the fakes become increasingly elaborate and
difficult to detect, leading to the need to apply the next technique…

Generative Adversarial Networks (GANs)
A fascinating and innovative area of development where two (or
more) Generative AI systems are pitted against each other,
theoretically improving the quality of their own training with the
outputs from the other system. There is a brilliant example of this
(and Generative AI) in the first story in Kai-fu Lee and Chen Quifan’s
book AI 2041, where a militant organisation are using Generative AI
to create fake speeches to incite a political uprising (why do the
phrases “Fake News” and “Donald Trump” come to mind at this
point…
 ). In this story, the militants use a GAN to try to make the
fake nature of the video almost impossible to detect. The
Generative AI system creates V1 of the video. This is then viewed
by the Detective system to see if it can identify the fake. When it
does, the outputs from the Detective system are fed back into the
Generative AI system. This process is repeated until a point is
reached where the Detective system can no longer detect any flaws
and in the process both systems have been trained to a far higher
level, one to generate content, the other to detect fakes.
QR link to the home of Kai-fu Lee’s book AI 2041
Swarm Intelligence
Based upon the behaviour of many different species in nature (e.g.
schools of fish, flocks of birds, or ant colonies), and built using
Machine learning, this is the concept of setting a series of
algorithms loose to cooperate on solving a problem or defining next
best action. No one algorithm may have all of the right answers, but
by combining the results of all of them a consensus answer can be
determined. While this is more in its infancy than the concepts
above, the theory is sound and will become increasingly pervasive
in research related functions. Technology innovator Louis Rosenberg
demonstrated it very effectively with an experiment (albeit using

humans as the individual algorithms) to predict the first four places
in the 2016 Kentucky Derby. While none of the twenty pundits
correctly identified all four of the winning places, their collective
intelligence accurately identified all of them. A $20 accumulator bet
placed on their selections would have returned $11,000! If you want
to find out more about this, take a look at Louis Rosenberg’s
Unanimous.ai site.
The animal swarm theory embraced by Louis Rosenberg. QR code links to his
website.
Human Augmentation
Combining AI with the advances in robotics has given us the ability
to produce incredibly advanced prosthetics, and even exoskeletons,
to provide mobility for people with all sorts of disabilities. But now
this combination of technologies has started to be utilised to
augment humans with powers (e.g. enhanced strength) previously
reserved for the movies.

Robot exoskeleton allowing this worker to shift heavy loads…Ripley’s Power
Loader Suit in Aliens anyone?
 
There is a lot more on this concept coming up later in the section on
Mortality.
Chaining
While all of the techniques and technologies in this section are
powerful in their own right, they are generally combined into
chained solutions. This is where the real power of AI becomes most
obvious. Consider a situation where a human can have a discussion
with a customer service chatbot on their computer to raise a
product complaint. In this scenario the solution would utilise
Computer Vision to read the expressions of the disgruntled
customer, use the full spectrum of Natural Language Processing
capabilities to identify, interpret and formulate a response to the
points raised, and then use Generative AI to respond with a
solution.
 
Quantum Computing
OK, I’m going to lay my cards on the table here. I kind of
understand Quantum Computing…kind of…But do I feel qualified to
try and explain the theory of Quantum Mechanics to you? Well, no
would be the answer there. However I don’t really need to, or at
least that’s the excuse I am giving myself. Suffice to say, it’s a way

of storing and processing data based on a unit called a qubit, which
can have more than one value at the same time…See! Even that
sentence makes no real sense to me, so I am clearly not qualified
to write anymore about it. If you do feel compelled to know more
try reading Quantum Computing Made Easy by David Bradshaw,
otherwise just file it somewhere in a dusty corner of your mind and
move on. Just be aware that Quantum Computing will be a game
changer when it reaches the mainstream because it offers
processing speeds that are up to 100 million times faster than
conventional computers. As an example of what that means, the
encryption used to secure blockchain records would take even the
most powerful conventional, bit-based, computer about 100,000
years to break, while theoretically a quantum-based machine could
crack the security in less than a day. While this encryption cracking
example demonstrates the spectacular potential of quantum
computing, it also tells you a little bit about the threat this
innovation brings with it.
At the moment, the quantum computers that have been constructed
are extremely fragile and can only work on small datasets, however
if the problems of scale and stability can be solved, quantum
computing will totally revolutionise computing platforms and keep
Moore’s “law” ticking over for some time to come.
Blockchain/Cryptocurrency
While I don’t actually class these phenomena as Artificial
Intelligence per se, they are very much products of the AI age, and
I know someone would say “he wrote this whole book without
mentioning Blockchain. What an idiot.”, so I have included them just
to cover my bases 
 
Blockchain and its application to enable cryptocurrencies, of which
there are now literally hundreds, is based upon the concept of
something called a Public Ledger. This is effectively a massively
distributed and copied list of all of the data related to an electronic
record of something. In the case of a cryptocurrency e.g. Bitcoin,

you could imagine it like this; If you buy some cryptocurrency, every
record related to that virtual currency, including your purchase and
every use of that currency thereafter is recorded in a series of
transaction records that are completely tamper proof (at least in
theory, because of the incredibly strong encryption used to secure
them) and copied all over the internet. It’s a bit like a copy of your
bank statement being openly visible and copied all over the place.
This cryptocurrency can then be used to purchase goods and
services just as you would use your credit card, however since the
data lives outside of the traditional banking service, your funds and
what you use them for are no longer moderated or regulated by the
classic financial behemoths that look after (allegedly) your regular
funds. Nor is your cryptocurrency at the whim of classic financial
mechanisms like interest or exchange rates. This is obviously
something the banks have been none too happy about, however
some of the more savvy financial institutions are now trying to
embrace the concept. The question you need to ask yourself is why
would you need a third party like a bank to look after your
cryptocurrency. What value will they add, other than a questionable
sense of security?
 
Beyond cryptocurrency, the concept of blockchain is now being
employed to track the transactions related to an ever growing
number of things. One of the coolest I have seen is the use of
blockchain in the supply of highstreet products. Using a specially
built application, the purchaser can scan a product and see all the
transactional history related to it; where it was originally
manufactured, where its ingredients were sourced from, whether it
was tested on animals etc. Blockchain promises complete
transparency in this sort of thing and will become increasingly
adopted by suppliers and vendors as a way of proving the ethical
“purity” of their produce, and also to avoid the risks such as
tampering and counterfeit goods.
 
As I said up front, it’s not really an AI technology, however AI
systems will increasingly leverage blockchain data as a source, so it

is worth being aware of it.
 
Virtual Reality and Augmented Reality
Commonly shortened to VR/AR, this is again not specifically Artificial
Intelligence, however the application of these incredible
technologies is largely powered by it.
 
Virtual Reality relies on completely computer generated
environments which the headset wearing user navigates, typically
from a stationary or seated position (although a recent innovation
has been the introduction of multidirectional treadmills that allow
the user to simulate movement, per the rigs used in the film and
book Ready Player One). Undoubtedly the biggest market for this
technology is video games, however it is also being used for
training users before letting them loose on expensive or dangerous
equipment (e.g. pilot training). It is also being applied for virtual
experiences that would otherwise be impossible or prohibitively
expensive for the user. I recently took Buzz Aldrin’s seat in Apollo 11
and experienced a quite breathtaking simulation of the entire moon
landing, from launch through to stepping foot on the moon.
 
Augmented Reality and its progression Extended Reality (XR) are
where we will really see extensive  application of AI. Both
technologies rely on overlaying computer generated objects onto
real environments. These environments are either visible to the
user by displaying the output of cameras on the outside of the
enclosed headset, or in cases where the headset has clear glass
lenses e.g Google Glass smart glasses, projecting the information
onto the glass like a heads-up display. AR is also commonly
available on a lot of smartphones, using the forward facing camera
to capture an image and then overlay some computer generated
objects. Probably the stand out application of this technology on a
smartphone so far is the Pokemon Go application that became a
craze around the world in 2016, propelling millions of users on

frantic hunts around their cities for rare
Pokemon/Pokemons/Pokemen…whatever the plural is.
(Image courtesy of Cyclonebiskit)
 
XR is really just a natural extension of this technology that uses
other AI techniques to properly map the environment captured on
the camera in 3D, and then model the computer generated content
to more accurately integrate with it. Imagine a computer generated
zombie that could actually trip over the coffee table in front of you
as it tries to attack.  
 
Virtual and Augmented Reality are going to play an integral part of
the implementation of something called the Metaverse, which is
covered later in the Social Media section.
 
More to Come…
Everything I have described in this section has really started to
become a practical reality in the last decade or so and we can be
sure that the list of amazing capabilities of Artificial Intelligence will
continue to grow and grow. As my new friend GPT-3 said, I am
going to have to revise this book over and over in an effort to keep
up. 

What Isn’t Artificial Intelligence?
 
It Isn’t T-800
 
 
AI is not evil killer robots. At least not quite, although some of the
advances being made by companies like Boston Dynamics, Toyota
and Hyundai make this type of autonomous humanoid a feasible
reality. If you want to see the art of the possible (or at least what it
was in 2021) take a look at the Boston Dynamics website and
explore the commercially available Spot the Dog, as well as the
amazingly agile Alpha (not commercially available). But these are
predominantly examples of using advanced robotics to mimic the
physical capability of a flesh and blood creature. Yes, the actuators
that control the movements of these robots are controlled by AI, but
in reality, the robot shell is the clever bit here.
Breathtaking dance video featuring Boston Dynamics robots
 
It is important to be able to disassociate Artificial
Intelligence from Robotics. They can exist independently of
each other. Robots can be human-controlled or AI
controlled. AI can be in robotic form, or exist entirely inside

an individual device, on a network or cloud, with no
physical form. Robots are just an example of where AI often
has a physical manifestation, as are other connected
devices such as your Alexa, Google Home or even an AI-
enabled fridge.
Robotics vs AI
There is one fundamental quandry we need to consider before we
envisage a world full of Terminators or, more optimistically, C-3PO’s:
 
Why?
 
Straightforward question, slightly more complex answer. Science
fiction has always been an amazing window into what will become
science fact in the future (how many of the gadgets used on the
Starship Enterprise have already become real?) and since the
introduction of the concept in works like Karel Čapek’s book R.U.R,
or Fritz Lang’s film Metropolis, we have fairly consistently introduced
Artificial Intelligence as being in humanoid form. There is no
question that, to an extent, some element of this is simply down to
our arrogance as a species. In our eyes, what creature would not
want to be like us? However, many researchers have gone as far as
to say that continuing research in development of humanoid
robotics is a waste of time and has little practical use. However
there are a number of counter-arguments to this:
1. We have developed our world around our form. In order for a
robotic device to operate in our environment and provide us
with support or automation we will most likely need them to
be able to mimic some or all of our range of movements.
Examples of this type of application of robotics are manifold
today, although at the time of writing this, many of these are
still controlled remotely by VR-wearing human beings, so the
value of automation is not really being fully realised…yet.

  
 
2. There are situations that put human life in danger but still
require interaction with typically human environments. These
clearly are a great use case for a humanoid robot; imagine AI-
driven robot firemen for instance.
 
3. There are times when our human psyche reacts best to
interaction with something familiar in form. A particularly good
example of this is the successful introduction of Pepper, an AI-
driven robot, which is somewhat humanoid in appearance
(although on wheels not legs) and has been successfully
introduced as a companion for residents in care homes in
several countries.
(Image courtesy of Richard Croft)
 
Setting aside the difference between Robotics and AI for a moment,
it is important that we make good choices in the design of our

Robots. They should be implemented in the most practical and
efficient form for the task or tasks they are designed to accomplish.
 
It Is Also Not HAL 9000 (yet)
Just to balance the equation, it is also not yet a cloud-dwelling
super-mind like HAL 9000 from Arthur C. Clarke’s 2001: Space
Odyssey. For anyone who has missed this classic book and film, HAL
was the Alexa-on-steroids that went off the rails and chose the
preservation of the mission goals over the continued breathing of
the spaceship’s crew members, thus breaking all 3 of Assimov’s
Laws of Robotics (More on these in the Ethics section).
 
In order for AI to behave in such a generally despicable way we
would have to first develop General rather than Narrow applications
of AI, and entrust these implementations with control over a broad
range of interfaced systems (in the case of 2001, the controls of the
spaceship). We are quite some way away from this kind of

capability, however as an eventual outcome it is far more realistic
than armies of T-800’s. (HAL 9000 Image courtesy of Tom Cowap) 
It Is Not Absolution From Blame
Already we have seen real examples of where AI has malfunctioned
or been trained incorrectly and has produced results that are to the
detriment of humanity. This scenario is probably occurring a lot
more frequently than we even know, but some widely publicised
examples include:
●      Racial and Gender bias in Recruitment Systems
●      Gender bias in determining Credit Card Spend Limits
●      Lost or incorrect Medical Test results directly risking people’s
health
●      The 2010 Flash Crash that wrote billions of dollars off the
Financial Market
●      The Utility company whose compensation algorithm printed
and sent hundreds of cheques, with values of over a trillion
pounds each, to people who lost power for several days during
a particularly bad UK storm. Sadly the cheques were not
honoured.
Whatever the outcome of the erroneous algorithm, it is not the
computer that is to blame. As with almost anything constructed
by humankind, it is important that ultimately the designers and
engineers of any AI solution are responsible for ensuring it works
fairly and correctly. For some further thoughts on this read the
section on How Can We Stay In Control?
It’s Not A Replacement For Humanity
Apart from within the mind of sci-fi comedy genius Douglas Adams,
AI is not an excuse to become less human.

Alarmingly I did come across a video of a Robotic Priest in Germany
(Image courtesy of Michael Hughes)
 
So, it is not an opportunity for us to be less human by thinking we
don’t need to apply our values to any situation. Already we have
seen examples where actions have been taken as a result of
Algorithms with questionable results and an inevitable backlash. In
these scenarios it is not OK to say “It’s the AI’s fault.”
 
Creators must take responsibility and accountability for
their creations.
 
Additionally, AI cannot be expected to demonstrate genuine human
emotions such as Empathy or Sympathy. Machines do not have a
conscience or self awareness. Some of the more advanced
Natural Language Processing platforms can seem human in their
responses, but the reality is that their responses are merely
simulations of human emotion. A worrying parallel to this is the
ability of a human psychopath to fit in with society, and appear
genuinely charming, right up until the point when they don’t.
Instinct
There is something proprietary to living creatures - Instinct; that
ability to know when things are not quite what they seem and react
accordingly. This sixth sense is far beyond the capabilities of any
computer system. Sure they can be trained or written to respond to
many different scenarios, but they will always be lagging behind

this innate capability demonstrated throughout the animal kingdom,
whether it is a prey animal sensing the approach of a predator, or a
driver sensing that another road-user is going to behave
unexpectedly moments before they do.
 
Take for instance Stanislav Petrov. If this man isn’t on your
Christmas card list, he certainly should be:
 
On the 26th September 1983, Petrov, an officer within the Soviet Air
Defence Forces on duty at the command center of the early-warning
system, was presented with a radar screen showing six nuclear
warheads, launched from the United States and heading towards
Russia. His training and protocol dictated that he should
immediately escalate this up the chain-of-command, who would
have been compelled to launch an immediate counter strike. The
outcome would have been a globally catastrophic nuclear war.
However Petrov decided to wait for more evidence, not believing in
his gut that the NATO forces would launch a strike of just six
missiles as was shown on his screen. Shortly thereafter the traces
disappeared and Armegeddon was avoided because of Petrov’s cool
head and instincts.
Stanislav Petrov: All round great guy
(Image courtesy of Queery-54)
 
It was later discovered that the alert he received was actually the
result of a system malfunction, possibly caused by strong sunlight
shining on the top of fast moving cloud formations.

 
Imagine if this decision was in the hands of an Artificial Intelligence
platform. Based upon the evidence presented to it and its lack of
human instinct, it would almost certainly have started a civilisation-
ending World War 3.
 
They Are Not Genuinely Creative
 
A final word in this section about creativity. Whatever examples you
have seen of art, music or literature being created by AI, it is not
genuine creativity in the same way that a human being is.
Ultimately, AI creativity is an analysis of previous creative works to
find a way of amalgamating aspects of it to produce new work. The
computer has no facility to appreciate the beauty, or not, of what it
is producing, unlike a human. Instead it relies on us to vote for what
we like or dislike about its creations. Humans have an innate spark
of ingenuity and innovation that allows us to be genuinely creative
at times, although it is also true that a lot of what we call creativity
is really also just regurgitation of existing work. There are examples
out there already, often award winning, of AI generated art and
music, which may be appealing to the eye and/or ear, but part of
what we, as humans recognise as art is the struggle, the part of our
soul that we leave behind on the canvas or the mixtape. For AI, this
is the job we have programmed it to do, there is no real “essence”
to its creation. In a great example of this, I recently watched a clip
on YouTube where a group of data scientists demonstrated an AI
generated piece of art:

An image created using generative AI, based upon the keywords “Haunted
Mansion” 
 
 
It looks…fine, but when you watch the actual generation process
and examine it up close, you can’t escape the fact that it is
computer generated. Where are the layered brush strokes of  a
Michaelangelo or a Da Vinci? The ones you can see catching the
light when you get up close to an original oil painting? The ones
that perfectly tell the story of the struggle and passion of the artist?
Where are the perfect imperfections that only a human could
introduce onto a canvas? Missing, that’s where. And they will never
be there in my opinion. The problem is that AI is wired for
perfection and that is simply not what art is. Whether it is one of
the world’s great painters (namechecked a couple already), writers
(e.g. Ernest Hemingway) or musicians (e.g. Joe Cocker), AI is
simply never going to be able to replace the critical ingredients
imbued in their work; soul, struggle and human emotion.
 

The Wonders of Artificial Intelligence
 
I was self isolating over New Years Eve 2021/22. Feeling somewhat
humbug I chose to spend the evening watching a James Bond Movie
rather than getting in the spirit of things. However, at 11:55 I
relented and turned on BBC One to watch the fireworks. Within
seconds I was wishing that the fireworks would go away, because
they were obscuring my view of the incredible drone display,
including a breathtaking lion that roared over the centre of London.
 
I was mesmerised, and like many others I was not just marvelling
at the imagery. I was marvelling at the technological mastery that
coordinated hundreds of autonomous drones to produce it.
Something that a few years ago would have seemed like science
fiction was now not only reality, but seemed almost effortless.
 
The wonder of AI had struck again.
 
And it was a perfect signal of just how far the field has come in such
a short time, however we don’t need to wait until 31st December to
witness it again. The internet and your app stores are flooded with
examples of new applications of the technology. More and more
every day. I recently came across a fantastic site where, for free,
you can upload any image, could be a recent photo, a reasonably
clear portrait painting, or even a historical photograph, and almost
instantly it renders a 15 second video, animating the image,
bringing the subject to life before your eyes. The results are
startling; There was an animation of an old photograph of Abraham
Lincoln that took my breath away. And behind the magic, AI
techniques such as Computer Vision are being used to identify the
facial features in the picture and Generative AI is being used to
create the animations. I love it, even if some of the results can look
a little creepy.
 

But I am only scratching the surface with these examples and I
think AI deserves a little bit more of a showcasing than that. Below
are just a few of the major areas that Artificial Intelligence is
already (or promises to soon be) paying huge dividends for
humanity. A quick search of the Internet for Benefits of AI will
provide you with a lot more:
 
Medical/Health
One of the areas where AI has seen the most significant uptake,
with remarkable results already. Investment in AI within the medical
sector is expected to reach as much as $2 Billion by 2025 and
success stories to date include:
●      Medical diagnostic solutions, where AI systems using Computer
Vision techniques and/or mining huge volumes of data in a
fraction of the time than any human could ever do, have been
able to provide diagnosis of medical conditions (e.g.
identification of tumors on scan images) with an equal or
higher accuracy than medical specialists.
●      Drug regime identification, where the algorithms crunch huge
volumes of data about clinical trials and patients to identify
new precision medicines to treat individual patients with
specific conditions, improving efficacy and reducing risk from
adverse reactions and side effects.
●      Robotic surgeons or theatre assistants are becoming
increasingly popular in hospitals because of their accuracy,
efficiency and avoidance of human error. For example, in the
Netherlands, a robotic surgeon was able to suture, 100%
accurately, blood vessels as narrow as .03mm! Personally, I
am still a bit unsure about looking up at something like the
example below from the operating table. I think I would want
to be anaesthetised fully beforehand 

 
●      Chatbots being used for initial symptom diagnosis and
treatment routing to alleviate the load and waiting times for
first consultations. However there will be a limit to how deep
into the treatment regime this technology can ever permeate
because there are times when human empathy, and even
sympathy, are required to deliver the right level of emotional
support. A metallic pat on the shoulder from Dr C3PO simply
wouldn’t cut it.
 
Industrial Applications
AI is already used broadly throughout the business world and it is
becoming more visible on a daily basis. Whether it be in the form of
autonomous vehicles and machinery operating in manufacturing
plants and warehouses (Amazon alone has several hundred
thousand robots operating in their warehouses, picking, packing and
distributing your orders), or drones scanning pipelines for defects,
these are all real world examples of industrial applications of AI.
Additionally, as discussed in the section on Work, automation is
directly addressing jobs and tasks that fit within the 3D category
(Dull/Dirty/Dangerous), ensuring our safety and generally improving
the quality of our working lives.
Even more common in business settings is the application of
Artificial Intelligence in robotic process automation (RPA)  and
improvement. For instance, the application of AI to improve forecast

accuracy or to automate office processes that otherwise involve
lengthy or error-strewn manual work.
 
Agricultural and Food
An area where the impact of AI is more immature, however there
are already significant success areas including autonomous farm
machinery to work in the fields and robots using Computer Vision to
pick produce and apply quality control on crops. Additionally,
algorithms are becoming increasingly effective in helping farmers to
manage the impacts of perennial problems like the unpredictability
of our weather and calculating the correct levels of irrigation and
fertilisation.
 
It is also probably worth briefly talking about food synthesis; as
popularised by shows like Star Trek, where Kirk and the crew could
walk up to a console, select the food or drink of their choice, and
then it would be automatically laser printed for them, there and
then. While 3D printing technology is very much at the level of
maturity where people can buy a printer for their home and print
things for themselves using a number of materials, we are not quite
there with food. That journey has started however, and a number of
companies are working on the technology to produce 3D printers
that can synthesise organic matter, much in the way that things like
meat can be synthesised today in laboratories. While this is some

way off, it will come, and along with the ability to order whatever
crazy sandwich filling you want, when you want it, this technology
will mean a huge shift towards localisation of food supply, which will
also clearly have a positive impact on… 
Sustainability
Very much an area of focus given the fact that we have finally
reached broad awareness that our excesses and abuses against the
planet really are leading us towards disaster (how are you feeling
now climate change deniers?). The use of AI promises to help us
with this situation in a number of ways:
●      Improving efficiency of industrial/agricultural/city infrastructure
and planning through the application of AI will optimise
processes and reduce energy usage, leading to obvious
reductions in the emissions of greenhouse gases.
●      Algorithms are already being created to look at innovative
ways to tackle the very real problem of climate change by
identifying opportunities to use alternative, renewable energy
sources in ways that human researchers cannot otherwise
identify, or at least would do so at a much slower pace.
●      Aligned to the point above, AI can help to make more efficient
use of the energy captured in our national grids, by modelling
usage patterns and directing spare capacity appropriately. This
increase in usage efficiency will translate to less need for
energy generation and thus reductions in emissions.
●      AI, coupled with 3D Printing technology, promises to have a
radical impact on the lifetimes of devices, which is a huge
positive from a sustainability perspective, reducing the need to
replace and recycle appliances etc on such a regular basis.
Consider this scenario, which we are probably less than a
decade away from; A sensor in your connected washing
machine identifies that one of the seals on a water pipe is
wearing out and needs replacing. The connected device is able
to inform you directly of this issue, order the part to be
automatically printed at a local 3D printing site, delivered by

an autonomous postal drone, and then a local repair engineer
organised to come in and fit the part the following day.
●      Localisation of supply chains is also being more generally
driven by AI and 3D printing. Why ship goods and parts across
the world when you can print them locally? Obviously we
already need to reduce the carbon footprint related to freight
transport, and this solution has the added benefits of
convenience and speed of availability. Some cities have
already set specific target dates by which they want goods to
be locally sourced using localisation techniques like this. For
instance, Barcelona has set a target of 2050 for both this and
self sufficiency from an energy consumption perspective.      
As an interesting reflection on this, it might surprise you to know
that one of the main reasons for the rapid growth in the use of coal
power in the 19th century was the concerns raised by
environmentalists (yes, they did exist back then as well) about the
damage we were inflicting on the world through deforestation. The
Industrial Revolution required great surges in fuel usage and the
obvious choice seemed to be to mine it from under the ground.
What harm could that possibly do? 
 If only they had some
algorithms back then that would have helped them forecast the
eventual impact of this choice.
Smart Cities/Buildings
One of my personal favourites is the implementation of AI in Smart
City projects, enabling far more efficient services (and thus also
supporting the Sustainability points above) and improving life
quality for the inhabitants. Examples of the connected services
being introduced in Smart Cities include improved traffic
management systems that ensure traffic flow is effectively managed
during peak times, public services that are available on demand in a
timely and efficient way, and energy usage in buildings being
optimised.
One of the most interesting examples of this is the ongoing
development of The Line by a Saudi Arabian organisation called

NEOM. No matter your opinion of the creators’ politics and ethics,
it’s worth taking a look at some of the videos on their website to
marvel at their vision of how a fully sustainable AI/data driven city
will work.
 
QR Link is to the website for the NEOM Smart City Project
 
Solving Problems Others Can’t
I wanted to include a few words as a catch-all for some of the
fundamental value we are already getting from AI that can be
attributed to all of the amazing applications above, but also to so
much more. AI succeeds at a number of things far better than any
human could ever achieve:

●      Speed - AI, powered by the ever faster and optimised
processors we produce (Moore’s “law”), is able to process
tasks at a far greater pace than any human being could ever
do.
●      Accuracy - Properly coded and trained, AI has already been
proved to be at least as accurate, but generally more
accurate, than humans, even those with significant experience
in their professional field.
●      Longevity - AI will work all day, all night, all weekend. No
arguments or demands for “time and a half” in their pay
packet.
●      The Bigger Picture - With the speed of processing and the
ability to process volumes of data that it would be
inconceivable for a human being to tackle, AI can find answers
to questions and problems (regularly ones that us humans
couldn’t even detect) that would simply be beyond the
capability of even the sharpest human mind.
 
In short, Artificial Intelligence, properly implemented, effectively
managed, could and should produce incredible results for
humankind, which will far outstrip the capabilities of humans alone.
 

Transport
What are Autonomous Vehicles?
Obviously a lot of this chapter will be about self-driving cars, which
are one of the most publicly visible and discussed applications of AI
today, but I also want to expand the subject to cover a little about
other applications of the technology, including drones and industrial
vehicles. But, let’s start with an understanding of the six levels of
autonomy used to classify vehicles:
As you can see, they span from Level 0, where the human is in
complete, uninterrupted control, to Level 5, where the vehicle no
longer even needs an interface for human control (pedals and
steering wheel for instance). As of 2021, many new vehicles on sale
to the public have been pre-fitted with Level 2 or Level 3 which
would technically mean the vehicle could perform a large portion of
the driving unassisted, however full use of many of these features is
not yet legal in the large majority of countries (although
agreements are almost in place for this to be the case in Japan and
the EU). Despite this caution in Europe and Asia, in 2018 Google
was granted license to launch pilot schemes of its Autonomous

Vehicle system, rebranded as Waymo, in both California and
Arizona, US.
 
All of these features, whatever the level, are powered by inputs
from a vast array of sensors, including all the telemetry from the
vehicle itself, laser-based radar called LiDAR, and image recognition
cameras, which are trained to identify obstacles, vehicles, road
signals, and probably most importantly, us humans, using
techniques like Computer Vision. Many of the self-driving systems
also use a technology called V2X which communicates key
information about the vehicle’s position, speed and intentions to the
other vehicles nearby in real time.
All the data from these inputs (it is estimated that the average daily
data usage by a self-driving car in 2020 was already 1 Terabyte) is
used by the central computer, the brain, of the car to select the
right next action, or in the case of Driver Assistance, provide a
warning or perform an evasive manoeuvre. And this brain is…you’ve
guessed it…powered by Artificial Intelligence, based upon Machine
Learning models that have been trained using huge training sets of
data generated artificially, or as a result of on-road trials. As of mid-
2020, the Waymo test fleet had covered over 10 million miles of
public road and they had also generated about 7 billion miles of

artificial training data using simulations. This artificially generated
data is vital because we cannot expect the real cars collecting
training data to encounter every possible scenario that could be
encountered on our roads and streets. Human beings are just not
that predictable. However, they have been presented with some
fairly wacky scenarios. For example, in one real world scenario, a
Waymo vehicle was confronted with a lady in an electric wheelchair
chasing a duck up the middle of a suburban street! And there are a
number of scenarios we cannot anticipate or easily simulate in the
real world, such as how the vehicle should react during natural
disasters like earthquakes. The only way to provide data to train
the fleet of vehicles for this kind of thing is to create it artificially
using Digital Twins of roadways and environments.
On the flip side of that, consider that a human driver only ever has
their own individual training to rely on for any situation, while
autonomous vehicles have the benefit of all of the training
experiences of ALL of the autonomous vehicles on the road i.e.
millions of times as much data and experiences as individual human
drivers are expected to rely on to make safe decisions.
 
Two Main Types of Autonomous Systems
In essence, there are two different approaches that are being
employed to implement AI driven autonomy in vehicles. While both
may seem to have the same end goal in mind, the approach to their
development and deployment is very different:
 
●      Driver Assistance - In this approach, which is popular
amongst most of the traditional vehicle manufacturers, the
goal is to develop systems that augment the capabilities of the
vehicle to lessen the burden of driving and improve the safety
of the driver, the vehicle’s passengers, and everyone in their
vicinity. While this will ultimately result in the development of
full autonomy, the approach to development and delivery is
incrementing gradually through the levels of autonomy above.

●      Full Autonomy - Championed by the technology companies
that are tackling this e.g. Tesla and Google/Waymo, this
approach is to develop systems embedded in the vehicles that
go straight to Level 4 or 5 (Tesla for instance claims that every
vehicle it has ever sold is already capable of Level 4
autonomy). The public release of these capabilities will then
be based upon legislative agreements with individual
governments, upon acceptance that the Artificial Intelligence
is performing at a suitable level of safety. 
 
It should be noted that there is some crossover between the
approaches of the vehicle manufacturers and the tech giants playing
in this space. For instance, in 2018, Waymo entered into a
collaboration with Jaguar Land Rover to embed Waymo’s full
autonomy capabilities in a fleet of 20,000 of Jaguar’s i-Pace SUV’s.
 
Applications of Autonomy
 
There are clearly a number of different ways this technology can be
applied. Below are some of the most common applications that are
either already being deployed, or at least developed:
 
●      Self-Driving Cars - The obvious one. Apparently we spend
an average of 4.3 years of our lives in cars, and for many a
large percentage of this time is consumed by commuting to
and from work. In most cases this is wasted, frustrating and
tiring time spent concentrating on the bumper of the car in
front of you. This frustration is unsurprising when you consider
that, compared to 30 years ago, we now spend 40% more
time in cars, yet in that same time period the road network
has only expanded by a meagre 6%. So roads are busier and
congestion is increasing year on year. Imagine a world where
instead we could be using this time for social, work or
recreational activities while the vehicle transported us

seamlessly to our destination in total safety. In a world where
all vehicles on the road were fully autonomous and working in
safe synchronisation, we would be able to do away with road
signals and signs and reduce journey time, since the vehicles
would be able to travel safely at higher speeds. Our ride would
drop us off at the door of our office or home and then go off
and park itself unassisted, or even head off to pick up its next
passenger. That’s the vision, but there is still some way to go
before it can become a reality.
●      Driverless Public Transport - Already in small scale pilots in
various cities around the world, we are going to see a growth
in the number of driverless buses and taxis on our streets, one
of the big advantages being that you no-longer have to suffer
the bus driver rolling their eyes when you press the bell for a
request stop.
●      Commercial Vehicles - Once the same hurdles that exist for
Self-Driving Cars are overcome, we will see fleets of long-
distance road haulage, where autonomous vehicles would be
able to safely transport full loads across long distances without
any need to stop for a rest. In the interim, the technology is
already being deployed in environments such as enclosed
mining and industrial sites to move materials safely and
efficiently.
Concept Autonomous Bulldozer
 
●      Drones - Predominantly being deployed for security,
inspection and cinematography purposes, although the last

one is most often under some level of human control.
Additionally, Amazon and other retailers are exploring the use
of drones as delivery vehicles to replace the fleet of vans
clogging up our roads; however, rollout of this has been
stuttering for reasons including threats to civil airspace and
concerns raised about the privacy rights of people whose
houses the parcel and camera toting drones would fly over on
their flightpath to delivery. Setting aside the surveillance and
security usage (often for military purposes), one of the main
industrial applications of drones already in use is for
inspection, for instance fully autonomous fleets of drones
flying over solar farms or along dams and pipelines to look for
cracks or faults. This is a very efficient, time-saving use of the
technology to inspect structures where human inspection
would be very difficult, dangerous or time-consuming.
●      Air Transport - Already nearly 90% of a typical chartered or
scheduled flight is performed under the control of computer-
based systems, with only the take off and landing fully
controlled by the pilots. As the Artificial Intelligence in flight
computers improves, the necessity for human intervention will
continue to reduce until we reach the point where we could
feasibly have pilotless planes; however, may be a difficult
concept for some air travellers to swallow.
 
What’s the Hold-up?
 
Well, mostly it’s us, the human race. We are the problem.
 
Big Benefits
If it wasn’t for all of the non-autonomous vehicles on our roads, and
if we didn’t mill about our city streets on foot or bicycle, the current
level of training implemented in self-driving cars would be more
than sufficient for us to move fully to Level 4 and 5 all across the
world. As I mentioned earlier in the chapter, a world where all

vehicles were at Level 4 or 5 would mean a radical change to the
road network. There would be no need for traffic signals or road
signs and no requirement for stringent speed limits. The central
computer AI would be able to manage seamless navigation from A
to B while avoiding any risk of collision because of its ability to
comfortably process the wealth of data being shared with it from its
own sensors and the other vehicles in its vicinity far more quickly
than any human could ever do. All of this would allow us to be more
relaxed and productive in transit, as well as vastly improving road
safety. Today 1.2 million people perish every year, worldwide, in
road traffic accidents. Autonomous vehicles offer a future where this
number could become closer to zero.
Even Bigger Challenges
Unless we banned all non-autonomous vehicles from our roads and
did something equally as radical about the number of pedestrians
on our streets (there are actually proposals for some pedestrian
free zones or roadways floating about) it seems unlikely that Level
4 or 5 will gain general acceptance unless the AI systems can mimic
or surpass the innate, instinctive capabilities of a human.
 
Imagine the scenario where you are driving down a suburban street
and you see a person walking along the pavement in the opposite
direction of travel to you, talking on their phone, seemingly
oblivious to what is going on around them. Something in their body
language causes you to just ease off the accelerator a little and, as
you somehow knew would happen, they turn and step out into the
road in front of you. Luckily your instinct means you have plenty of
time to brake safely. At this stage no autonomous driving system
can emulate the intuition, perceptiveness or unpredictability of a
human being. Until this is the case it is going to be a hard sell for
full autonomy. Unfortunately human nature compounds the difficulty
with bolstering public opinion. The press seem to thrive on reporting
even the most innocuous accident involving a self-driving vehicle,
and this is never balanced against the number of far worse
accidents that occur every day because of human error.

 
However, the biggest sticking point to self-driving vehicle adoption
is the moral issue surrounding the behaviour of the vehicle in
situations where a collision is unavoidable. This fascinating, and
slightly morbid, dilemma is the subject of a number of studies and
debates and deserves its own section.
 
So here it is…
The Moral Dilemma
Your Level 5 self-driving car is speeding along the high street,
approaching a pedestrian crossing. As it gets nearer it detects
pedestrians getting ready to cross. From the left comes an old lady.
From the right comes a young man in a suit, talking on the phone.
As both parties step out onto the road your car correctly calculates
the stopping strategy and puts its virtual foot on the brake pedal.
 
Nothing happens. The brakes have failed.
 
With no escape roads to the side you have no choice but to hit one
or other of the pedestrians, likely causing their death. Which option
should your car choose.
 
Another one?… This is fun right?
 
You, your partner and two young children are being self-driven up a
beautiful mountain pass, with a wall of rock to your left and a steep
cliff to your right. As you turn a corner you are confronted by six
people hiking towards you in the middle of the road. Your
autonomous car immediately starts braking, but again your brakes
fail (you really must get those looked at). Your vehicle is left with
the choice of hitting the hikers or plummeting over the cliff edge,
sending you and your family to your deaths. What should it do?
 

So you see the issue? And this has been probably the main point of
discussion and contention around self-driving vehicles. This dilemma
is not new, first being introduced by an English philosopher, Philippa
Foot, in the 1960’s. Then it wasn’t about AI, rather about an out of
control trolley car approaching a set of points at which you are
standing with the control lever in hand. On both forks of the track
are different combinations of human beings. Who lives or dies is
entirely in your shaking hands. All that has happened now is that
the same dilemma has been transported to the world of self-driving
cars, with the human in charge of the points replaced by the
Artificial Intelligence in the Central Computer.
MIT has set up a fascinating website called the Moral Machine
(http://moralmachine.net) on which members of the public are
(anonymously) faced with a series of similar dilemmas based upon
different combinations of occupants in the car and different
pedestrians being in jeopardy. You are asked to judge a series of 13
out-of-control self-driving car scenarios (randomly selected from a
larger library) where you must choose which young, old, homeless,
even criminal, pedestrians and vehicle occupants must be sacrificed
in order to save others.
 

   
Screenshots from Moralmachine.net NOT some weird low-res version of Grand
Theft Auto
 
Over 2 million people have now completed the test and the results
are startling in their diversity. One of the most interesting
discoveries has been the cultural nuance in the responses;
respondents in different geographic regions have demonstrated
locally and culturally aligned preferences over whether to preserve
the life of others over theirs, or their family’s. For instance, people
in Asia showed a distinct preference for saving the elderly, while
across Europe it was children who fared best. 
 
So there doesn’t seem to be one universal answer as to how
autonomous vehicles should behave and the only real solution
seems to be to reach a point of acceptable, and localised,
compromise that matches the cultural norms of whichever society
the vehicle is in. If you want to read more about this subject I
would thoroughly recommend The Car That Knew Too Much by
Jean-Francois Bonnefon, one of the originators of the Moral
Machine.
 

Who Is To Blame?
Adding to the dilemma above is the question of apportioning blame,
since we live in an increasingly litigious world. If any of the horrific
scenarios above were to come true, who ultimately should take the
blame? In a human operated vehicle it is almost always clear cut.
Unless it is clearly the result of unavoidable mechanical failure or
obvious external factors, the blame will ultimately lie with the driver
of the vehicle or the negligence of a pedestrian. Whichever is the
case, if there are serious injuries, the survivors will be permanently
scarred by the guilt of their involvement.
 
Now consider where the decision and actions are taken out of
human control by self-driving technology. Who now takes the
ultimate blame for the incident? If it is a Level 3 or 4 vehicle, is it
the occupant/driver, because they did not react to the warnings
appropriately, or failed to remain vigilant? If the accident occurs in a
Level 5 vehicle, who is ultimately at fault? There is no facility for a
driver to take control, so how can the occupants be blamed? Surely
the fault needs to lie with the manufacturer? What happens if the AI
has chosen to sacrifice the occupants (the cliffside scenario)? Can
their family sue the manufacturer?
 
All of these questions are still the subject of discussion between the
vehicle and technology manufacturers, and state governments. Until
a realistic compromise is found, we can expect a rocky road ahead
for self-driving vehicles (pun fully intended 
).
 
In a report published in January 2021 by the Law Commissions for
the UK nations, the following recommendations were put forward,
although as of the date of publication of this book they had not yet
been put into law:
●      The user-in-charge of a vehicle cannot be prosecuted for
offences arising directly from the autonomous driving of a car,
whether they are speeding, dangerous driving etc, however

the user-in-charge is responsible for things like ensuring
insurance is in place and all passengers are wearing seatbelts.
●      The company behind the self-driving system would be
responsible for offences resulting from the autonomous driving
of the car.
●      All data from the vehicles involved in the incident must be
made available to fully assess the fault.
So, what this is suggesting is that liability resides with the
manufacturers, not the vehicle occupants. If this is enforced in law I
can imagine that it will significantly slow down the rollout of fully
autonomous vehicles on our roads since the manufacturers are
unlikely to want to accept this burden.
 
Taking it a step further, in a world where Level 5 Autonomous
vehicles have become the norm, we need to consider the impacts
on the Insurance industry. According to the Government’s own
statistics, there were nearly 120,000 injuries (incl. fatalities) in the
year to June 2021 on the UK’s roads. Over 90% of those were
related to human error, so if we did away with those pesky human
drivers we would make our roads so safe that insurance premiums
would have to be reduced radically, with obvious impacts on the
revenue for the Insurance industry. An even bigger, existential
question will eventually surface about who should actually pay for
insurance. If I am no longer in charge of the vehicle that ferries me
from A to B, why should I be liable for insuring it for any accidents it
causes? How this all pans out will depend on how legislation
develops over time, with one theoretical outcome being that the
insurance liability shifts to the vehicle manufacturer (or AI
developer if it is different).
The Elephant in the Room
If you have seen The Fast and the Furious 8, you can’t have missed
the scene where Cipher and her band of villains hack into the self-
driving cars in New York and wreak havoc on Times Square.
Basically, the baddies manage to hack all of the self-driving cars and

use them as a weapon to foil our heroes Mike and Dom, with the
usual hair-raising stunt extravaganza to follow. It’s totally over the
top and absolute Hollywood gold, but it does raise the question
about just how secure autonomous vehicles are from hacking. OK, it
is unlikely that Dr Evil is going to hack all the cars in a city en
masse but, as the technology becomes more prevalent, there is
definitely a question there to be answered…
 
QR Link to the trailer for The Fast and the Furious 8
The Gorilla behind the Elephant in the Room
Daniel Kahneman, the brilliant author of Thinking, Fast and Slow,
introduced System 1 and System 2 thinking. The concept is pretty
straightforward. System 1 thinking is where you do something
almost unconsciously, semi-automatically, your brain processing
your actions and reactions to events without you having to really
concentrate on what you are doing. It is similar to muscle memory.
System 2, is (unsurprisingly) where you have to consciously think
about your actions before you do something. If you apply this to
driving, consider that when you were learning to drive your actions
were all System 2, while you were paying 110% attention to your
driving instructor or semi-terrified relative and gripping the steering
wheel like a maniac. However over time the art of driving becomes
second nature, by which time your actions are being controlled
squarely by System 1 thinking. We’ve probably all had those days
when we have got out of the car and can’t actually recall all of the
details of the journey we have just taken; that’s System 1 thinking
in action. With the introduction of level 3 autonomy, the driver is
still expected to stay alert and ready to intercede as soon as the
vehicle detects a situation outside of its comfort zone. So, as long
as you are being a good and diligent passenger/driver, you should

be ready to react, fully aware of the events unfolding in front of
your windscreen. However, as we move towards level 4 autonomy,
two complimentary things will happen that change the dynamic
considerably. Firstly, the driver will not be expected to stay fully
engaged, because that’s kind of the point of level 4 (and 5).
Secondly, the gap between the times you are expected to intervene
will get longer and longer. It could be literally thousands of hours of
driving before you are suddenly alerted that a bus has just gone out
of control and is veering into your lane. Even if you are actually
awake and not watching Netflix when the klaxon sounds, do you
really think that the necessary reactions to save yourself and your
family will still be ingrained in your System 1 thinking capabilities
when you haven’t driven a car for so long? Without wanting to get
too bleak, there are parallels with this situation that have already
occurred in the aviation industry, where tragedies have occurred
because the autopilot has malfunctioned or unexpectedly
disengaged and the pilot and crew have been unable to react
quickly enough or with the correct remedial action in order to avoid
disaster, because either they couldn’t engage the right System 1
thinking, or in some cases, their training was so out of date that
they couldn’t even remember what the right corrective action was.
 
This whole scenario around Level 4 needs a lot more thought before
we fully embrace it. In fact it may even be necessary to skip
straight to level 5.

Work
Introducing Maslow
Let’s start by looking at the Hierarchy of Needs, introduced by
American Psychologist Abraham Maslow in his 1943 paper ‘The
Theory of Human Motivation’:
Maslow’s Hierarchy of Needs
(Image courtesy of Androidmarsexpress)
 
This popular model, sometimes elaborated further into 8 levels,
resonates with many people, representing why they are motivated
to live the way they do, and what they need (i.e. what pre-
requisites they have) in order to achieve self-fulfillment. In order to
achieve physical survival we need to address our Basic needs;
Sustenance (food, water etc) and Safety from danger. Only then can
we fully realise our Psychological needs; Friendships and
Relationships, along with some sense of Worth. And it is only once
all of these are in place that we can reach real self-fulfillment and
actually feel like we are achieving something approaching our full
potential. Not everyone will be able to reach these heights during
their lives, largely because of factors such as the political and
economic environment they are born into. I do not intend to use

this book for overt political campaigning (as tempting as it is), so
for now I will use this model to represent people in a society where
oppression, subjugation or shortage of resources are not an
obstacle to at least the opportunity for fulfillment.
 
For most people in a healthy state, work is at least fundamental to
the Basic needs in the hierarchy. We seek employment to get paid
in order to buy food for ourselves and our family, and to afford
comfortable accommodation and transport. If we are lucky we also
work somewhere that affords us enjoyable social interactions and
helps us to build friendships (a factor that has been all too apparent
during the COVID-19 pandemic when the psychological impact of
extended periods of home working became a major concern for
many employers and employees). If we are really really fortunate
we might feel good about what we achieve at work and bathe in
the glow from the recognition we receive for our contribution. And if
we are even more truly blessed, and I recognise that this might be
limited to only a small percentage of very lucky people, our work
actually allows us to reach full self-actualisation. In short, our work
doesn’t feel like…well…work. Per the suggestion in the hierarchy,
this self-actualisation is often represented by the ability to express
ourselves creatively. Stephen King, the globally recognised best-
selling author, once said “I am so lucky that my job is also my
hobby.”
 
Even if this is not the case and our work does not provide us with
complete self-fulfillment, it almost certainly is the source of our
Basic needs which provides a springboard from which we can strive
for, outside of work, our Psychological and hopefully, our Self-
actualisation needs.
 
Interestingly, and possibly a little disturbingly, some people in less
fulfilling jobs will develop personal narratives that allow them to
feel a false sense of purpose and worth rather than addressing the
problem (and perhaps seeking alternative employment). It is sad to
say that many people in what could be classed as “middle

management” afflict this upon themselves rather than dealing with
the fact that their roles are largely superfluous. This behaviour has
a clear link to mental health problems.
AI and Automation
So, that’s why (typically) we work. Now we need to examine what is
happening to the opportunities we have to do it, and what will
happen as these opportunities diminish.
 
Firstly, automation is nothing new. Think of the Spinning Jenny loom
that was a linchpin of the Industrial Revolution, or the Ford Model T
production line. We have been constantly looking for ways to
automate work processes. And every piece of automation has had
its dissenters. For instance, the Luddite revolt, catalysed by the
introduction of the Spinning Jenny and the impact it had on
employment in the textiles industry of nineteenth century England;
the loss of employment was devastating and pre-dated any form of
welfare state or practical unionisation to protect people’s basic
needs. So rebellion was inevitable.
 
Generally, in the 21st century, such Basic needs are protected
(although I recognise this is not always the case), but loss of
income through unemployment makes it far harder to achieve Self-
actualisation and can have catastrophic impacts on people’s self
esteem and other Psychological needs.
 
So let’s look objectively at what AI has done so far to the job
market, focussing on the impact in developed countries, and how
that trend is likely to continue and accelerate. We will then look at
what that means for the workplace and the workforce of the future,
exploring the impact to education and society in general in the
process.
 

The Potential of AI and Automation
Often called Robotic Process Automation (RPA), the process of using
technologies often (but not always) infused with Artificial
Intelligence to replace or augment tasks otherwise performed
manually, can be a very positive thing for a number of reasons:
 
●      Speed - The human brain is still the most amazing computer
in the world and can process incredibly complex problems, but
clearly microchip-based computers can do it faster and Moore’s
“law” means this processing speed is just getting quicker and
quicker.
●      Accuracy - In addition to speed, the advances in techniques
like image recognition (using Computer Vision) mean that a
computer can often perform tasks more accurately than even
the most experienced and skillful human. Take for example the
advances in medical diagnosis facilitated by AI systems, which
have been proven to be at least, if not more, accurate when
identifying tumors and other conditions from medical scans
than their specialist human counterparts.
●      Volume and Working Hours - Partly as a result of the first 2
points, but also partly because they don’t care if you turn off
the office lights and go home, computer systems can continue
processing work 24/7, often without any direct human
supervision. This means that far more can be done without the
need for lengthy hours of work or the need to employ so many
people on shifts.
●      Cost - After the initial investment in automation, the
manpower costs of any company can be reduced by
automation, although there may be some costs still associated
with maintenance of the robots, along with a level of training
and supervision.
●      Convenience - A lot of automated processes are available as
and when you need them, rather than waiting for a human
operator to be available. Need another customer service

operative? No problem, as long as you have enough
processing power on your servers.
●      The Three D’s - Automation can take on the jobs no one
wants - those that are Dull, Dirty and/or Dangerous.
●      Increased Time to Focus on Higher Value Activities -
Whether it is more fulfilling, useful or innovative activities at
work, or increased leisure time, by taking away the drudgery
automation is going to give employees the opportunity to
explore other opportunities.
 
And we can see many of these automation benefits already, from
things like the banking apps on our phones, through the automation
of catering and cleaning jobs, to the rise of self-driving vehicles.
 
Sure, there are risks to automation, particularly if you don’t have
some kind of “human-in-the-loop” supervision to make sure the
machine doesn’t go rogue on you (generally because of biased data
or bad software engineering i.e. man-made issues), but if it is done
right, the only real concern, and it is a major one, is the impact on
our employment opportunities and with it our ability to reach self-
fulfillment.
 
The impact on the Workplace
On the positive side of things, automation will remove drudgery etc.
- The Three D’s - giving us the opportunity to focus on more
fulfilling activities, allowing more time for creativity, which is a
major facet of self-actualisation. Things we used to do in a manual
and error-prone way will be achieved more quickly and accurately,
accelerating business processes while reducing cost, and leaving us
to focus on the really high value activities and opportunities.
From a possibly less positive perspective, a lot of what are currently
the typical entry-level, semi-repetitive, jobs will become automated
first, meaning that the profile of junior employees needs to change
radically, and rapidly. We won’t need so many people flipping
burgers or working the tills, so if we want our young people to have

an opportunity to gain valuable employment they will need more
and different skills than the limited requirements we have for entry-
level jobs today.   
The Impact on the Workforce
While there have been many bold statements around the fact that
AI and Automation will generate jobs to replace, or even exceed,
those lost as a result of its implementation, I think that this will be
a blip in the grand scheme of things. Let’s be candid about this,
automation will cost jobs. Yes, we need more data engineers and
scientists to implement the automation. Yes, we will need some
resources to maintain/supervise the processes and robots that are
implemented. However a few specialist data engineers and a few
trained supervisors could potentially oversee a fleet of robots that
replace the gainful employment of tens or hundreds of employees.
As an example, it is estimated that the number of people employed
in the US automotive industry has dropped by 50% in the last 30
years as a result of automation.
 
In short, if the way we approach work does not change, we will be
making it far harder to meet the Basic needs of Maslow’s Hierarchy
through employment.
 
So, what should we be doing to ensure that the workforce, and with
it our established social fabric, doesn’t crumble in the face of the
accelerating wave of automation spreading through our workplaces?
Well, there are a number of changes we probably need to make in
parallel if we want to ensure people still have a path to self-
actualisation:
 
1. Provide Career Guidance to Students - While more and
more jobs will eventually be fully or partially automated, the
fact that it will be some time (if ever) before AI can fully
simulate or realise genuine compassion and empathy, or
creative spark, means students should be steered towards
career choices in more creative roles, or roles which require a

degree of human compassion. Also, for some period at least
there will be a rise in the need for creative data scientists who
will be able to build and train the ever more complex
algorithms, robots and autonomous vehicles that we will rely
on to perform tasks that previously provided employment
opportunities for their now redundant human operators.
 
Figure 1: Some idea of how different jobs might fare as the pace of AI adoption
increases
 
As you can see from this diagram we need to be encouraging
the workforce of tomorrow to make career path decisions
today that ensure they are steering away from jobs that would
naturally fall in the bottom left quadrant. Future Cabbies and
Lorry Drivers beware. Instead we should be nudging them
towards careers that require creativity and/or compassion for
others. We could be breeding a world of singing nurses before
we know it.
2. Change the Educational Focus - Tied to point 1, critical
skills for employees of tomorrow will be problem solving,
creativity and social skills. We need to take a long hard look at
the current educational syllabus, and the profile of our

educators. Learning the six wives of Henry the Eigth and how
many of them managed to keep their heads on is going to be
completely superfluous, but being able to manage complex
social scenarios and solve problems where automation of
processes cannot apply are going to be key skills. Teaching
techniques such as verbal and non-verbal reasoning should
become a primary focus on the syllabus at all ages. Creativity
will also be highly valued and, even if working weeks are
reduced or replaced by the Minimum Living Wage or Universal
Basic Income (discussed in point 4 below), the conditions for
self-actualisation could still be created by providing
entertainment or creative products to others. There will
certainly always be a market for these skills, as long as the
cost is not exorbitant. Consider the scenario where you want
to buy a new bedside lamp and you see two almost identical
models in the showroom, one printed on a 3D printer from a
resin with a wood finish, the other hand-carved lovingly in oak
by a master craftsman. If the price was not too high you would
choose the hand-carved one, right?
3. Re-evaluate the Working Week -  In most developed
nation states, employment contracts are based around
working weeks of 40+ hours. If the amount of available work
is no longer enough to support this quota of hours then we
need to consider shortening the working week. This has
recently been very successfully trialed in Iceland, leading to
nearly 90% of the country’s workforce being on reduced hour
contracts without any impact on salary and absolutely no drop
in output. Additional benefits have included a radical drop in
the number of employees reporting health issues such as
burnout. This scheme is now being trialed in other European
states (although sadly not so much in the UK yet 
 ).
Obviously this will get some pushback from short-sighted
employers who see this as a lost opportunity for reduced
salary costs as opposed to happier, more productive
employees, but that is a different, more political, discussion.

But, in order to provide more people with the opportunity to
fulfill the Basic and Psychological needs serviced through the
opportunity of employment, employers will almost certainly
need to look at reducing expected working hours, enabling
opportunities for more people to achieve gainful employment.
4. Minimum Living Wage or Universal Basic Income -
Further away, but something that is already gaining political
momentum, is the concept of the Universal Basic Income. As
employment becomes more and more scarce and
(theoretically) more and more of our work is done by AI
enabled systems, there will come a point when there is
literally not enough work to support the population. However,
since much of what we consume and produce right now is
generated through costly work by employees and will, in the
future, theoretically be performed in a much more cost
effective way by automated processes, this saving could be
redistributed to the redundant workforce as a basic living
wage that is enough for everyone to live comfortably, if not
extravagantly. Additional salary could then be earned for
contributions to society, whether that is in caring for the sick
and elderly or providing creative entertainment for the now
very leisurely populace (which would also be a path to self-
actualisation for many). Discussions are still fairly immature on
this, although it has been raised in high offices, including the
US Senate on several occasions. Personally I think that it will
be a radical adjustment for some people who will not be
comfortable with how “socialist” it appears; their level of
perceived personal entitlement will make it a struggle for
them to support it. I also think it needs very careful handling
to avoid parallels with the social scoring phenomena sweeping
across China, where who you meet for a drink can directly
impact your capability to get cheap car insurance (Google it,
I’m not going any further with that one here).
 
We are on the verge of significant changes to the way we, and
future generations, work as a result of our adoption of AI, but it is

not too late to adjust and educate so that we can continue to
provide people with the capability to meet their basic and
psychological needs, along with the motivation to attain self-
actualisation. While we are talking about this, we can’t avoid the
parallel with Disney’s Wall-E, and in particular the overly leisurely
and thus grossly overweight human population. This is an
interesting point to consider because, if we suddenly have a great
deal more leisure time, will we still have the personal motivation
(back to Maslow’s Hierarchy for one last time) to actually do things
that maintain our self esteem and provide self-fulfillment? Or will
we just laze about on our increasingly padded backsides watching
Netflix?
 
The Dynamics of Work
Despite my slightly disparaging words on Mr Zuckerberg’s latest
brainchild in the Social Media section (spoiler alert for those of you
reading this in the right order), there is one place where I do see
some potential for the Metaverse to add real value for humanity.
 
In 2020 we all learnt a hard lesson about how fragile our existence
is when the pandemic swept across the globe. One of the few
positive aspects of this disaster was that people finally accepted
just how pointless the 9-to-5 office-based existence we had mostly
been enduring really was. After an initial stutter, most of the world
of business normalised around the fact that people could do just as
good a job, if not an even better one, when they had the ability to
work from home. By-products of this included the reduction in
wasted time commuting (with all the added environmental and
sustainability benefits) and the ability to better balance work
commitments with social and family commitments. Obviously there
were downsides too; some people still find it difficult to “switch off”
from their work commitments, the line between office life and home
life becoming blurred, and some of the collaborative and social
aspects of the workplace became a challenge. Technology providers
rushed to try and address this gap with collaboration tools such as

Microsoft Teams and the eponymous Zoom. These platforms
provided the capabilities to bring people together for meetings and
social interactions, however there are limitations to the social
dynamics possible through these interfaces. For instance, I suspect
that many of you will have experienced a group video meeting on
one of these platforms and you will know that, unlike when you are
in a physical meeting space, it is difficult to have side conversations
during a group call; a natural activity when you are sitting next to
someone in a room. And, while they do facilitate socialisation and
collaboration, it is a bit clunky. There are none of the elevator or
water-cooler conversations offices are famous for; everything has to
be a little more deliberate and orchestrated. The Metaverse
promises to address this, making interactions more natural and
fluid. Companies can set up virtual offices and meeting rooms and
employees can wander around them virtually, having fairly natural
conversations with their customers and colleagues. Even if their
boss has chosen an avatar that looks like a weird Super Mario.   

Mortality
 
Fun topic…I know 
 
So, with all this seemingly inevitable progress and change, we
should take a little time to think about what this means for our
longevity. Already, over the last century, our average lifespan has
more than doubled as a result of improvements to society that have
their roots in technology - particularly advances in health and social
care. This trend will continue, but we have to make some hard
choices along the way and understand not just how we are
increasing our life expectancy, but also why.
 
There are several contributing factors to this I have touched on in
other sections of this book, but in summary:
 
●      AI is already helping us to find new and improved treatments
for life threatening or limiting conditions. This will only
continue to accelerate as life sciences organisations harness
the data and analysis techniques, becoming increasingly
available to them, to accelerate drug discovery and precision
medicines. In fact there are some scientists who theorise that,
through medical advances, we could reach a point somewhere
in the 2030’s where life expectancy could be extended by
more than a year for every year we live. This is sometimes
called Methuselarity.
●      If we get things right and people’s quality of life really benefits
from the potential value of the AI, life should become
increasingly fulfilling and people will likely want to stick around
longer.
●      No one likes getting old. Things stop working properly.
●      Find me one person who wouldn’t like to be able to control the
date of their demise. Think about that one a bit before you

react.
 
There are two main schools of thought about how to address
mortality and both have their advocates and critics. Both also have
supporting communities who are actively pursuing their goals. And
both are already making solid progress.
 
Transhumanism
 
The simplest way to think of Transhumanism is that it is the use of
technology to overcome illness and disability, augment our
capabilities and potentially extend our life spans. We already see
this day-to-day in the medical devices we take for granted, such as
heart pacemakers, and new and innovative medical devices are
appearing at an incredible rate; robotic exoskeletons for people
with spinal issues, prosthetics that respond to neural signals to
function more and more like the missing body part, and even a
mobile phone application that can use image recognition to describe
their surroundings to people with partial or no sight. And I haven’t
even touched on the concept of nanotechnology and what this could
do for medical procedures - “That’s a nasty tumor sir, just take this
pill and the nanobots will remove it for you”.
 
But this is just the tip of the iceberg.
 
Futurist, inventor of the desktop scanner amongst other things, and
Google big cheese, Ray Kurzweil, has written several books that
explore the impact of AI, and specifically look at health and
longevity. In particular his book The Singularity is Near explores
how we must embrace merging with technology as Artificial
Intelligence progresses towards the point of singularity, where the
capabilities of technology will far outstrip those of us humble
humans. In essence we will have to embrace this merging process
to remain relevant. In the section on What Artificial Intelligence

isn’t, I explored the challenges around making a computer that
demonstrated real emotions and genuine original thought in order
to really replicate the capabilities of the human mind, however the
transhuman movement believes that, in the not so distant future,
we will be able to merge our minds with a computer. There is some
good sense here because, if you think about it, every single thing
we think of or do, even whether we decide to find a cat cute or
detestable and arrogant (sorry, there’s the cat thing again - I will try
to curb it), can be broken down into a series of If-Then-Else
statements, merged with some historical training data (i.e.
memories) that we process. Even some things we do without really
understanding our motives, such as falling in love, can most likely
be explained by the same logic process, but we just can’t quite
understand or articulate it yet. Simon Sinek covered this brilliantly
when he talked about the fact we say things like “I love you
because you complete me”. Utterly meaningless. How do you
complete me? Is it because you provide the leg I was missing? No.
It’s simply because the limbic area of the brain where love and
desire are formulated is a bit of a black box and these cheesy
statements are all we can manage because we really don’t have
ready access to that particular bit of the brain’s algorithm/logic…
yet.
But, in essence, everything we do in what we call our mind and
consciousness, can be boiled down to a set of decisions based upon
parameters and some input of memories (relevant past
experiences). And the output of these decisions includes the
generation of new memories as well as (possibly) some
recommendations to ourselves to modify that particular bit of code
for next time to produce a better result. As unpleasant a
comparison as it may seem, we are in effect algorithms trapped in
meat machines.

 
So, if we can develop complex enough human-brain interfaces, why
couldn’t we upload a human to a computer?
 
Some technologists and futurists are predicting this capability could
be with us by as early as 2050. Why not? We are already using the
human brain as a model for some AI techniques (e.g. Neural
Networks), so as we get better at it, we will surely end up being
able to develop a computer that has enough complexity to house a
human mind. Scientists are already making progress on the
necessary human-brain interface research, funded by innovators like
Elon Musk.
 
All sounds really great, right? We can live on forever, even when our
body withers and dies. All we have to do is upload ourselves to our
personal server and maybe even download ourselves again into the
latest human-like android, developed to look exactly like our old,
healthy, self (or Brad Pitt if you prefer).
 
There is every possibility that Ray Kurzweil and others are right in
thinking that Transhumanism is an absolute necessity for the human
race to remain relevant when AI becomes the most intelligent being
on earth. Irrespective of your opinion on this, there are some great
potential benefits we could leverage from this merging-with-the-
machine scenario. Consider for instance the ability to send a human

consciousness somewhere that is currently too physically
challenging or expensive to do so, for instance a trip to the far flung
regions of space. Currently the limitations of human form, and the
cost, mean that even a mission to Mars seems on the edge of our
abilities. If we could package up our consciousness (or at least a
copy of it) in a robot, we would be able to send it wherever we like
with almost no risk (assuming we had a backup), and at
significantly reduced cost.
 
Maybe it is great and perhaps it is a necessity, but for balance I
think we should take a little longer to consider a few of the risks
and cautions related to the potential of cyber-immortality:
 
1. Do we really want to live forever? Think about what that
means. Will our lives continue to have a purpose and
fulfillment ad infinitum? Or will we eventually get bored, or
even worse lack drive and purpose - why do something today
when we can put it off until tomorrow? Next week? In a
decade or two?
2. For many people a major driving factor in their lives is the
desire to leave a legacy. If we never really die, we can never
truly achieve that.
3. Fear of our mortality is a major contributor to human
behaviour. Sure, no-one wants to get run over by a bus on the
way to work, but (in general) our humanity revolves around
wanting to protect and care for others, particularly those we
love. Imagine a world where this scenario exists: “Oh no
darling, little Tommy’s died again. How embarrassing. Shall we
start a new one or download him from the last back-up?” I am
genuinely worried about what society would become if that
was a reality.
4. If we decide that human form is no longer a necessity, and we
decide to live “in the machine”, are we really alive? Or are we
just an algorithm? What actually is “living”? Without physical
presence we would not be able to experience the physical
pleasures which are a major driving force in most people’s

lives. Unless of course the sensations we experience physically
could in some way be simulated in a sensory way in our new
virtual world.
5. Equally, if we decide to download our minds into another
android or robot, is it really still us? I know this is getting
somewhat philosophical, but it’s a valid question we all need
to consider.
6. Part of being us is that we have a public and private side; our
secrets, our desires. It’s part of our essence as individuals.
Once we are uploaded into the cloud you can be damn sure
that individuality and privacy will be breached or even made
public domain at the earliest opportunity. Do you really want
the computer to know, and possibly share, every thought you
have? I would never be able to watch a 90’s Kylie Minogue
video again.
 
Biogerontology
 
The other school of thought is that we explore ways to extend our
existing life expectancy and health far beyond our current norms.
Here AI will continue to be the major driver for research into ways
to keep us healthy for longer. Non-profit Institutions like Jeff Bezos’s
Altos Labs and the SENS Research Foundation are already making
great strides and finding promising areas of research. SENS have
broken cell degeneration and damage down into 7 types, and for
each of these there are active programs that are all making
progress; baby-steps, but promising.
 
This seems like a great cause doesn’t it? I mean, who doesn’t want
to not get old, creaky, incontinent and wrinkly? Who wouldn’t want
to be able to play tennis like they could when they were twenty-
five, for their whole life (or is that maybe just me)?

 
While points 1 and 2 from the cautionary list above in the
Transhumanism section still apply to some degree to
Biogerontology, particularly if this extension to healthy life is
lengthy, there are a few additional points I think we need to
consider with regenerative technologies and techniques:
 
1. You would need to have been living in a cave in the desert for
the past couple of decades to have not witnessed the almost
certainly irreparable damage we have been doing to our
planet. Much of this damage is being caused as we mine and
manufacture goods and services to support our mortal coils. If
we stick around for longer in human form, and keep producing
more baby humans, we are going to put even more pressure
on this supply chain. I simply don’t know if the planet can take
it.
2. Biogerontology doesn’t really address the concerns that
Transhumanism meets head on; how do we remain relevant in
a world where we are no longer the smartest kid in the room?
Without becoming “part of the machine” what is to stop the
machine discarding or neglecting us?
3. Slightly weird this one, but if we all remain healthy for longer,
there could be impacts on employment (whatever that looks
like, per the Work section). Imagine two candidates for a role -

one is 22 and just finished his degree, the other is 75 and has
50 years more experience, yet is still healthy and vital. Which
would you hire? This could really cause a challenge for youth
employment.
4. One of the areas of research that is currently underfunded is
the cure for Alzheimer’s Disease. No matter how healthy the
body is, most people will start to develop this debilitating
condition by the age of 85 because of the buildup of plaque in
the brain. So unless this research subject becomes a
significant area of focus it is really not going to help if we have
healthy bodies. 
 
 
In reality both Transhumanism and Biogerontology have merits and
both are likely to come to fruition to some extent, with other factors
dictating just how much this is true.There are a couple of general
concerns I have around the impacts they will have on society,
particularly during the inevitable period when the application of
these technologies will only be available to the privileged and
wealthy early adopters. This period could easily lead to a very
damaging class-division in society; the haves and have-nots; the
augmented and the decrepit. Once this happens, it’s not a huge
leap to a full-on caste system where the rich flourish and the poor
wither and suffer. Avoiding this would require significant and robust
action from the governments of the world in order to make these
treatments and technologies available to everyone, despite the
huge pressures that would be applied by their profit-hungry
manufacturers etc. We already see this in the differences in health
and social care provision between developed and developing states.
And given how ineffective most regimes are in adapting to change I
do not have high hopes that they will address this effectively.
 

The Matrix Theory
Worth adding in, even if the concept is a little mind-bending and, in
my opinion, highly unlikely. The Wachowskis’ film postulated that
we, humankind, are already slaves to the Singularity and that our
perceived reality is purely virtual. They proposed that we are
physically entrapped in cocoons that sustain our physical needs
while our minds power this Transhumanistic virtual reality
environment called The Matrix. Influenced by the writings of French
philosopher Jean Baudillard, the Matrix and its three sequels
document the adventures of Thomas Anderson, hacker name Neo,
as he discovers the truth and leads the rebellion to try and free
humanity from its AI overlord(s). While it makes for some fantastic
and iconic movie set pieces, the likelihood that the AI would even
see the need to keep us around if it became that advanced does
stretch credulity in my opinion, although Nick Bostrom, the
esteemed mind behind the incredible book Superintelligence has, in
the past, suggested this might indeed be the case.
(Image courtesy of  Church of emacs)
Life, But Not As We Know It
Whichever of these scenarios turns out to become reality, and I
expect that we will see some elements of Transhumanism and
Biogerontology becoming commonplace over the coming years, we
are likely to see continuing extension to our natural life expectancy,
and the science that drives these changes will largely be powered
by Artificial Intelligence. However this develops, I think we all need
to be prepared for the fact that our current expectations around

what the meaning of being a living human being will be is going to
change radically as we continue to develop AI towards the point of
Singularity.

Social Media
 
It would be tempting to just say “go and watch The Social Dilemma
and also… perhaps… Coded Bias on Netflix” here, but that would be
a bit of a cheat, so I better expand a litte (although…do go and
watch those two excellent documentaries as well…please…). 
 
The world embraced the internet as a medium for ecommerce, data
sharing and communication, so why not use it to make social
interactions easier? In the early 2000s the vision for what Social
Media could be was set by platforms like MySpace, but the mantle
was firmly grasped by Facebook, the platform launched by Harvard
frat-boys Mark Zuckerberg (who is very publicly still at the helm),
Eduardo Severin (who is very publicly not…watch The Social
Network movie for more on this) and others in 2004. By 2020 it had
nearly 3 billion users. While Facebook is unquestionably the ten
tonne gorilla in the room, other platforms like Instagram (which is
owned by Facebook), Twitter, Snapchat, Pinterest, Linkedin and
TikTok have garnered huge popularity in this space as well. Other
contenders have come and gone (think of Friends Reunited or Vine
for example), often subsumed into one of the other more popular
options.
 
(Image courtesy of Ibrahim.ID)

 
And the premise is actually pretty sound; provide people with a way
to connect and communicate with their friends and contacts and
share important stories and events. In fact Facebook’s “Mission
Statement” is exactly that “to give people the power to build
community and bring the world closer together”. This, at least on
the surface, is a great idea and there is no question that a lot of
good comes from this ability to connect with people more easily.
The reality is however that, with the advances in technology, and
thus the advances in the availability and flexibility of these Social
Media platforms, several other behaviours and impacts have
become commonplace:
 
●      Over-posting - The tendency to post pictures of what you
had for breakfast, lunch and dinner; information that none of
your contacts really need to, or generally want to, know.
●      Physical distancing - The use of Social Media as a substitute
for physical socialisation. In a recent study published by
Statista, it was claimed that the average daily use of Social
Media in 2020 was approximately 145 minutes, up (but only
slightly) over previous years. The increase can be somewhat
explained by the restrictions on physical socialisation imposed
by the COVID-19 pandemic, however, just take a moment to
consider what that 145 minutes actually means. If you take
your average day and deduct the time you spend sleeping,
working, exercising, eating etc, 145 minutes is the absolute
lion’s share of what is left. In short, socially distanced
socialisation is becoming the norm. This, for me, is a slightly
depressing and worrying trend.
●      Mental Health - There are a couple of angles to this:
○      The first, you could argue, is a result of a common
human trait - laziness - the fact that it is far easier to
click, tap and swipe on our devices than to actually make
the effort to go out and meet up with people means we
are living an increasing amount of our social lives online.
The quality of these interactions are unlikely to be as

good or genuinely enriching as physical interactions. Try
it for yourselves; compare the energy and genuine
pleasure you get from meeting a friend for a coffee and a
chat to the feeling you get after an online conversation.
There is an extra energy you feed off from physical social
interactions which is absent from our online experiences
(as demonstrated by the morale issues people have
experienced from extended periods of working from
home). This has been proven to lead to a genuine
reduction in happiness and our ability to interact in a
positive way in general. In short, our current reliance on
social media has made us less sociable.
○      The other impact on mental health is even more
disturbing. The ability to physically disassociate
ourselves from the words we use, and the impact of
them, in our social interactions on these platforms has
led to an enormous increase in cyber-bullying and
trolling. The knock on effect of this is a sometimes
unmanageable increase in the mental and emotional
pressure applied on young people who have not yet
sufficiently matured (at least mentally) to cope with the
pressures of the explosion of social interactions that have
become almost mandatory for them. They are pressured
by the need to always “make the perfect post” or fear
the repercussions. I would strongly recommend watching
The Social Dilemma and also looking for some of the
online interviews with Es Devlin on the topic for a
broader perspective. Below is a startling reflection of the
possible impact of social media on the mental health of
teenage girls since the widespread introduction of
platforms like Facebook:
 

      
Figure 1: The rise in female youth suicide rates since the introduction of Social
Media
 
●      Fake News and Political Influence - The popularity of
these platforms has meant that they are a primary target for
posts that are either completely false or at best unverified.
Given that there are in the range of 5 billion posts and 250
million images uploaded to Facebook alone every day, it is no
surprise that there is no practical way (yet) for these Social
Media companies to effectively police this. In most cases, the
volume of posts makes it impossible to mediate the quality or
validity of the posts manually, and the Artificial Intelligence
algorithms they are deploying are not 100% effective, so there
is often a reliance on the reader to report dubious content.
This means that a great deal of misinformation is still being
shared with users of these platforms (although not as much as
Donald Trump would have us believe). Sometimes this
misinformation is harmless, but in many cases these posts
have been introduced and circulated for nefarious purposes,

including in well documented claims that fake news was
critical in its influence on both the US election of Mr Trump and
the Brexit vote in the UK. Until a sensible and practical
solution is found for vetting posts on Social Media this will
continue to be a major concern.
 
Where does Artificial Intelligence fit in?
So far I have described what Social Media is from a user’s
perspective (good and bad) and what the stated intentions are for
introducing it, however now I want to take a look a little more
under the covers, and this is where Data and Artificial Intelligence
become a major factor. Firstly, for all the profound words in
Facebook’s Mission Statement, it is time for a large reality check. If
that, truly altruistic vision, was its only goal, Facebook would have
been registered as a non-profit or charitable organisation. As much
as all of the leaders of these companies try to make you believe
that they exist to make your world smaller, and better, this is in
effect no different than the distractive techniques employed by
magicians. No, like almost every other social media platform,
Facebook exists to monetise the relationships it has with its users
and use the data people willingly provide to it in whatever way it
can to make money. It exists to make a profit, which is something
you shouldn’t be surprised about or blame Mark Zuckerberg for.
After all, it's the primary goal of pretty much every corporation.
Whether that monetisation is in the form of directly taking money
from the user or, as is largely the case with Facebook, generating
revenue from targeted marketing and advertising to the users, be
under no illusion that making a healthy profit from its users is the
real mission of these platforms. Worryingly, a third possible revenue
stream is the selling of user, or user behaviour, data to other
companies. A particularly sinister example of this is well
documented in the scandal surrounding the company Cambridge
Analytica and its use of Facebook data to influence several different
political campaigns. If you don’t want your data to be part of these

types of datasets, and thus become part of the next political coup,
you need to start reading the T’s & C’s and usage agreements very
carefully before using any of these platforms. There is much more
on this in the Privacy section.
 
So, in order to turn a profit, the platforms need to make really good
use of the data captured from users to make the advertising
campaigns and click-through rates as high as possible. Facebook
alone holds over 300 Petabytes of data that it has captured from its
users. To illustrate how much that is, consider that, if it was all
music files, a 300 Petabytes playlist of tracks would take 600,000
years to play from start to finish. So, that much data, which is
obviously growing rapidly day by day, means that they need some
pretty advanced AI algorithms in order to target their advertising
etc. Those algorithms interpret the wealth of data they capture
about you; what links you click on, how often you log in, whose
pictures you appear in, what you like (it is interesting to know that
the engineer behind the introduction of the Facebook Like button
has now completely removed himself from social media), and an
ever increasing number of other data points they track about what
you do on their, or affiliated, websites (again, check those User
Agreements). This allows them to target you for different
advertising campaigns and influence your behaviour on their
platforms.
 
In order to mine the petabytes of data they are storing about us,
and maximise the potential benefits for themselves (and
possibly/hopefully for us) they need to use Machine Learning.
Simple rules-based algorithms would not be able to cope. As the
volume and depth of data collected increases there is an ever-
growing risk that these ML algorithms will become, consciously or
unconsciously, biased, unpredictable, and beyond human control.   
 
So, as long as you are happy about how and why they are using
your data, and trust in the ideal that they are simply trying to make
your experience as tailored to your needs and desires as possible,

and that they are fully in control of the machine learning models
running in the background, carry on using these platforms, while
remaining clear that you are giving away a large amount of data
about yourself, and that you will need to ensure you stay completely
up to date with the T’s & C’s in their user agreements if you want to
retain any semblance of control over how and why they are using
this, very personal, data.
 
On a personal level, I have decided that the effort of doing this and
the risks to my privacy far outweigh their potential value to me. As
a result, I no longer use Facebook or Instagram, however I do
continue to use Linkedin for professional purposes. In all other
cases I have decided that a trip to a local cafe to meet a friend for a
cup of tea and an old-fashioned chat is far more fun, and far less
risky. 
 
The Metaverse
Mr Zuckerberg’s latest brainchild is the generation of a virtual world
in which we can meet each other in avatar form and interact
socially and eventually for work; think Fortnite without the rocket
launchers and the cool gliders, but probably still with the irritating
dance routines. This will be accessed using VR headsets and
promises to be a very immersive experience. While there are
definitely some situations where the Metaverse could be both
practical and useful (See the Work section), it threatens to further
exacerbate many of the challenges we are already seeing that are
caused by over-reliance on Social Media as a medium for our social
interactions. It’s still early days for this new medium, but there is
good reason to tread cautiously.
 

 
@Zuckerberg, Mark if you want to discuss my stance at any time,
my Twitter handle is…oh no…wait… I don’t use Twitter anymore
either.  
Social Media and the Education System
It is already commonplace for schools and colleges to offer Social
Media Impact Awareness sessions for both children and their
parents. This is a noble and worthwhile exercise since it raises
awareness of the pressures and dangers represented on our
younger generations by their increasing reliance on Social Media as
their primary form of social interaction. However, these sessions are
generally voluntary and outside of the core curriculum, which
predictably means they are not as well attended as they should be.
What is really needed is more focus on this as part of the
mandatory curriculum in our education establishments, along with a
focus on promoting the very real benefits of genuine social
interaction and techniques for handling the immense pressure these
platforms place on our only partially emotionally developed
children.

A few words on Google
OK, I know that Google isn’t really a social media platform but, until
2021, when TikTok replaced it as the most popular domain on the
internet, it was the number 1 place for people to spend their time in
cyberspace. In fact Google is still the most popular homepage for
people’s browsers. But, why have I included them here? In the
social media section? Well basically because I have already talked
about the way that platforms like Facebook (which coincidentally is
still the most popular search term entered into Google) exist largely
to monetise their relationship with us through the use of our data,
and I didn’t want to write a whole chapter on search engines that
basically covers the same theme.
The bulk of Google’s revenue still comes from advertising ($209bn
out of a total revenue of $256bn in 2021), and as such they focus
very hard on understanding your behaviour on their site and then
targeting you with the right adverts and content. There has been a
very large paradigm shift since 2006, when co-founder Sergey Brin
went on record saying that Google’s intention was to get you to the
site you need as quickly as possible. Instead, they now focus on
trying to keep you looking at their own content for as long as
possible. Whether it is by toploading your search results with
sponsored adverts and paid links rather than the most relevant
ones, or through sections like “People also ask”, or the “Knowledge
Panels” that give you a preçis of the key facts about your search
subject, Google’s results are now so good at keeping you locked
into their content that more than 50% of the first screen that is
returned in search results is generally Google’s own content and
over 60% of searches never actually leave their site.  

Keeping you on their own site for as long as possible obviously
means they get to learn even more about your behaviour,
increasing their opportunity to find ever more elaborate ways to
monetise the relationship. This opportunity has been bolstered even
further by the introduction of their own browser, Google Chrome,
which as of 2022 was used for 66% of all web activity worldwide.
Using Chrome means that, even when you do escape their grasp
and venture elsewhere on the web, they still have an opportunity to
follow you, unless you spend considerable time messing with their
settings and turning off trackers and cookies etc. Obviously they
don’t make it a simple exercise to do that. Public awareness of this
type of under-handed privacy invasion has led to the rise in
popularity of privacy-enabling browsers like DuckDuckGo who
commit to never tracking your activity and blocking unnecessary
cookies from sites you visit.
More on all that in the next chapter… 

Privacy
Certainly one of the most publically visible impacts of the
Information Age we live in is the enormous shift in what we expect,
and are willing to accept, as privacy. And while technology has
undoubtedly been a facilitator for this change we, the human race,
must admit to having fully embraced the opportunities it offers to
us. We’ve grasped them with both hands. Now we tweet every
random thought we have during the day and post mindless pictures
of our breakfast (you know who you are and you know, deep down,
that’s optional - right?). On top of which every time we use a
device, whether it is our smartphone, a laptop, or even our
connected car, the operating system of the device itself and the
applications and sites we visit while using it, are all capturing
information about our behaviour, and leaving a trail of the
innocuous sounding “cookies”, which in fact allow them to track our
every move. They hide the fact that they are capturing this
information in plain sight using their “Privacy Policy”, a document
you are presented with when you first visit the site, and which you
must consciously accept before you can proceed. The problem is
that this takes advantage of the cruel fact that hardly anyone takes
the time to actually read the policy, or understand its implications,
before accepting the terms. The Accept button might as well say
“Sign Away Your Privacy Here”.
We are all too anxious to get on with using the site or application to
take the time to actually scroll through the details and absorb the
ramifications of clicking on OK or Accept. It’s human nature;
whether it’s because we are eager to actually buy those new
trainers that await us beyond the pseudo-legalise of the policy, or
we are simply filled with false confidence that the terms inside the
pop-up that is blocking our view of the latest funny cat videos are
harmless, we just don’t take the time to read it.

In short, we are giving away our data willingly and negligently.
Sure, some of the uses made of this data are mutually beneficial,
such as personalisation and recommendation on ecommerce sites
(“People who read or looked at this book also bought works by
these authors…” etc) or alerts about new releases by our favourite
singer, but ultimately this capture of data is for one reason only; to
ensure the application or site can monetise the relationship they
have with us as much as possible. Even those alerts about the new
album by Harry Styles are there to make sure you listen on their
streaming service, paying their subscription or viewing their adverts
as a price (adverts that are obviously tailored to you based upon
your browsing history). And even worse, many of these data
collection services inform us, in the policies that we never read, that
they will be sharing that data with their “partner” organisations. In
other words, it won’t be long before you are getting spammed with
adverts for services you never knew you needed. Sure, It’s all in
there; what they are doing and how they are complying to the
appropriate legislation, such as the General Data Protection
Regulations (GDPR), which does place sensible limitations on how
long, and for what purpose, companies collect and keep our
personal details, but very few of us can swear on oath to be in full
control of what data is being collected and used by these
organisations. I recently revisited the terms of use and privacy
policies for a Smart TV I owned and was shocked to discover that
data about my usage was being shared with over 300 partner
organisations! While there was a simple one click option in the
terms to accept sharing my data with all of them, I had to actually

scroll through and de-select all 300+ of them individually in order to
reject them. It took almost 30 minutes to do this. Imagine having to
repeat that for every device and app you use!?!?!
Worse still, even if we take the time to fully process the contents of
these privacy policies, companies are regularly updating them,
which means we receive a mass of “Updates to our Terms and
Conditions” emails that we are even more unlikely to read, and
which may well include new abuses of what we used to call privacy.
Using an often used analogy, trying to get on top of these privacy
policies is akin to re-arranging the deckchairs on the Titanic.
 
This is not just a rant on my part, it is very much a reality for most
people today. Ask yourself, how many websites and applications do
you use every day, and for how many of them do you really have a
handle on what they are doing with your data? And If you want
something else to make you think twice before you share too much
more data, take the time to watch The Great Hack on Netflix, or
read Privacy is Power by Carissa Vélez, and get more familiar with
how that very same data can be abused (legally or illegally) by the
same companies we are blindly trusting it to.
 
And it goes beyond just the personal data that is actively collected
about us when we are browsing websites. The granularity of data
that is captured now about our behaviour when scrolling through
the pages of a site is eye-watering. Companies can, and will, track
data such as how long we dwell on specific images in order to
identify the content they should serve up for us to keep us hooked.
This is called “Clickbait” and a nefarious example of how this is
employed is perfectly portrayed in The Social Dilemma on Netflix.

The way that these companies calculate the content to serve us up
to keep us hooked and to maximise their monetisation opportunities
is through the deployment of increasingly elaborate
implementations of Artificial Intelligence. Whether it is simply
algorithms which analyse the mountains of data they have received
from our behaviour on their sites, or through applying Computer
Vision to the holiday pictures we uploaded on their site (or one of
their “partner” sites) to identify opportunities to sell us another
similar holiday, be confident that they will be doing everything they
can within the bounds of their unread Privacy Policy, and the
applicable legal regulations, to monetise their relationship with you
using their AI.
Privacy Challenges We Cannot Easily Control
On top of this, there are times when data is collected about us
which we have absolutely no opportunity to control (depending on
where we live). I see three main areas where this happens:
●      CCTV - The first is one where we have no means to avoid our
data being captured, well short of wearing a false moustache
and sunglasses, or a balaclava. I am talking about the use of
closed circuit TV cameras (CCTV), commonplace nowadays in
our high streets and in most office facilities. These CCTV
systems are primarily deployed for our own safety and have
undoubtedly been instrumental in reducing criminal activity in
some areas, however they are increasingly using AI
technology, particularly Computer Vision, to identify the
individuals passing by the cameras. While this is in itself

causing no harm, there are a number of concerns that are
being voiced about their widespread deployment, such as:
○      The fact that they inhibit the behaviour of people,
making them act less naturally and freely when under
scrutiny. The impact of this on societal norms is yet to be
truly understood. In addition, in some areas, the
excessive deployment of CCTV is already being
considered a form of discrimination by the local
populace. For a perfect example of how people felt, en
masse, that CCTV was invading their privacy, examine
the widespread discontent in Hong Kong since 2018,
when the Chinese Government radically increased their
use of CCTV to monitor the population of the Island.
○      The data they collect could be abused or stolen, or used
for criminal activities such as blackmail.
○      As mentioned in other sections, the reliability of the facial
recognition performed by the current generation of
Computer Vision systems is questionable at best,
meaning that uses of these systems could lead to invalid
identifications and false arrests.
○      A 2019 study in the UK identified that the operatives
running these surveillance systems were predominantly
male and 1 in 10 use cases boiled down to little more
than voyeurism.
●      State Sponsored Surveillance - Often under the banner of
State Security or Public Safety, government agencies can
easily gain access to all our online information. While there is
incredible value in having well informed security services to
help apprehend criminals and would-be terrorists, you need to
understand that everything you produce, whether it is emails,
SMS, social posts or voicemails, is fair game if for any reason
your name becomes associated with any of their activities in
the pursuit of security and safety. A great way to learn more
about this is to watch the documentary CitizenFour (with
Edward Snowden) about the NSA’s PRISM surveillance
program.

●      Social Scoring Systems - Being pioneered in the city of
Hangzhou in China right now, this is the use of every ounce of
available data about citizens (and China’s lack of individual
privacy rights means that’s pretty much everything, including
people's movements being tracked by GPS and CCTV) in a
radical dilution of privacy, to develop a fully conformed society.
Crossing the road where you possibly shouldn’t could lead to a
reduction in your personal social score, thus leading to
increased cost for insurance premiums, reduced promotion
opportunities at work, or even polarisation from the social
group you usually hang out with. Now, I know that I am very
culturally removed from the Chinese people, but this creeps
me out. Completely. If there was ever a reason for this next
image, this is the one:
Big Brother from the iconic 1984 by George Orwell (come on, you knew I was
going to mention it somewhere in this section…). I resisted the version of this
picture where someone had photoshopped in Mr Zuckerberg’s eyes 
 
In some respects, I really hope you have found this section a bit
disturbing because, if you have, then maybe you might start being a
little more aware of where you are giving your data away. As I
stated earlier, there are good and bad aspects to how our data is
used and I am not in any way advocating that you all become

digital hermits, but you need to consider that the organisations you
share your data with are not benevolent (for the most part at
least), and therefore it is imperative that you get a handle for
yourself on where, when, and why, you are giving it away. 

The Only Way Is Ethics
Moral Principles
 
We all live by a personal code of ethics, it steers just about every
decision we make in life.
Let’s take a moment to just establish what Ethics actually are. From
the Oxford English Dictionary:
 
 
It is important to consider the fact that ethics are, for the most part,
individual, but there are some broad themes that appear in most
people’s principles, such as it being bad to do something that will
result in harm to another person. Our moral principles are reinforced
by the traditions and legislation of the society we live in, or by our
upbringing; the influence of our parents, relatives, friends, what we
read and see on the TV, or our social feeds. Consider these
influences to be our Ethical Training. These influences change our
ethical principles by introducing biases based upon our experiences
and what we are told is right by those trusted sources.
Unfortunately, this is the source of many of the worst characteristics
we see in society, such as personal biases towards race, age and
gender. And it is important to consider that these are principles that
drive our behaviour and not laws, although some of them do make
their way into “Ethical Codes Of Conduct”, such as those in place for
the medical profession, where they have gone beyond being just

ethical and moral principles and become conditions of continued
employment and guardrails against prosecution. In other words
they become rules or even pseudo-laws.
 
Basic Behavioural Scenarios
Setting aside this distinction between laws and principles for the
moment, I think we can all agree that there are some things we
think are the right thing to do (our ethical principles) and some
things we know are wrong to do, but we still do for whatever reason
(when we act unethically). However, there are also things that we
do that we think are the right thing to do, but in the eyes of others
are not so ethical.
 
If we think we have a reasoned argument for our principles based
on our experiences and influences we can call this Conscious Bias.
However, there are also times when our ethical principles are so
embedded in our upbringing and the social influences upon us that
we do not know, or accept, that there is any other alternative
perspective. This is called Unconscious Bias. A great example would
be the digital assistants on our phones and our satnavs; why do
they always seem to default to a female voice? There is no good
reason, but somehow the developers always seem to make it so.
Perhaps the influences behind this include the fact that all the other
manufacturers of assistants use female voices. Perhaps there is a
deep-seeded memory of how evil the most famous male digital
assistant turned out to be (HAL 9000). Whatever the reason, the
decision is clearly biased, unconsciously so.
 
Whether the bias is conscious or unconscious, we often look for and
select only data points that back up our decisions. This cherry-
picking of data that backs up our opinions is called Confirmation
Bias.
 

So What?
All of the scenarios above also apply directly to our Artificial
Intelligence solutions, but with one important caveat:
 
Artificial Intelligence solutions are only ever going to reflect the
ethical principles of their creators and users. When we implement
Machine Learning Models and Neural Networks, they will learn their
behaviours from how we train them (as per the training we received
ourselves from our upbringing and social influences). Whether the
bias in this training data is conscious (i.e. the creator or user feeds
the algorithm data they want to in order to influence the
behaviour), or unconscious (i.e. the data fed to the algorithm
unintentionally contains bias), there are great examples we can
look at from recent history:
 
Conscious Bias - Microsoft released a twitterbot called Tay based
upon their NLP platform, in March 2016. Within a matter of hours,
trained by the interactions it had been having with other twitter
users, Tay had become a racist sociopath.

Obviously, Microsoft shut the service down almost immediately, but
this is a perfect example of a platform that inherited the moral
principles of its users.
 
Unconscious Bias - In 2014 Amazon’s recruitment team started
work on a machine learning algorithm to filter CV’s for their various
software engineering roles. Within a short time of it going live, it
became clear that the algorithm was not treating the applicants in a
gender neutral way. The reason? The engineers who created the
algorithm had fed it with training data from the previous 10 years
which was in itself highly biased towards male applicants. The
algorithm has unsurprisingly been retired and manual vetting of
CV’s reinstated.
Bias in AI Creation
As well as the bias that can be introduced through training and use,
we also have to come back to the fact that all AI is created by a
human or humans.  I read recently that “If you want to get good
into the machine you need to first get good into the machines that
create the machines”. Whether Artificial Intelligence is created by
an individual, or a team, there are always opportunities for the
actual code to contain bias in itself. Irrespective of whether it is
explicit and conscious bias based upon the creator’s beliefs and
assumptions, they may code the algorithm in such a way as to
favour one result over another just because they believe it is the
right thing to do. This is why it is very important that the logic of
algorithms is fully documented and preferably reviewed by someone
external to the creator’s direct peer group (who may well have
similar ethical principles and biases as the creator).
 
How We Must Apply Ethical Principles
As with anything, and this is a message I have been re-iterating
throughout this book, everything that Artificial Intelligence does to

us will stem directly from how we create it. We need to take full
responsibility and have clarity of purpose in whatever we build.
 
“Be the change you want to see in the world” - Gandhi.
 
When it comes to some of the giants of the tech industry, there
have been instances where their ethics, whether it is their personal
principles, or those stated within their company policies, have been
somewhat questionable. This alone should be food for thought. For
instance, Google’s code of conduct actually includes the principle
“Don’t be evil,” which sounds great, right? However, in 2010, Eric
Schmidt, who was CEO of Google at the time, and would later
become Executive Chairman of the data giant, stood in front of a
crowded room of journalists and stated: “The Google policy on a lot
of things is to get right up to the creepy line and not cross it.”
Suddenly not sounding so great. 
 
And then there is the wonderful quote from Mark Zuckerberg
(admittedly in 2004, and I still live in hope that he changed his
ways, despite all of the evidence to the contrary) - “You can be
unethical and still be legal; that's the way I live my life.” Not what
you want to hear from the man at the helm of the most powerful
social media platform in the world…
 
So, in whatever we do, we need to apply ethical principles that we
would be happy to have applied to ourselves. I have been lucky
enough to discuss and coach many senior business leaders on the
ethical use of customer data over the past few years, and I almost
always start the conversation with the same tagline:
 
“It’s not what you could do with data, it’s what you should do with
data.”
 
And this is a good place to start, by asking yourself how you would
like AI to treat yourself and the rest of humanity, and then build and

train it to act accordingly. Although even this approach won’t stop
you from introducing your own biases into whatever you create. And
as AI models become more and more complex and self-training, it is
increasingly complex for us to keep a grasp on what our creations
are doing. Perhaps that should make us pause for thought about
how far we want to take the autonomy of what we create. Which
brings me to the second “angle” we need to use when approaching
ethical AI…
 
How AI Must Apply Ethical Principles
“The road to hell is paved with good intentions” - Saint Bernard of
Clairvaux.
 
If we continue to develop Artificial Intelligence at the exponential
rate it is being developed today, it will only be a matter of time
before we reach the point of Singularity when we simply do not
have sufficient control over what we have created anymore to
continually police its ethical stance. As such, we have to implement
some clear ethical rules in the code of the AI algorithms (not just in
the data that is used to train it, for all the reasons discussed in the
sections above). This is not a new idea. For instance, Isaac Asimov
introduced the 3 laws of Robotics in 1942**:
 
First Law - A robot may not injure a human being or, through
inaction, allow a human being to come to harm.
Second Law - A robot must obey the orders given to it by human
beings except where such orders would conflict with the First Law.
Third Law - A robot must protect its own existence as long as such
protection does not conflict with the First or Second Law.
 
Curiously, or possibly portentously, almost every one of his 80 or so
works of fiction which related to robots demonstrated that these
laws were almost impossible to obey. There was always some
dilemma in the storyline which meant that the robot had to break

one, two or all three of their own laws. In general, these often
repeated “laws” are adapted to the general premise that a robot
must not cause the death of a human, however this in itself raises a
really challenging dilemma; how do you train a robot to
differentiate between harming and killing a human? Harm is a
hugely complex and subjective concept, which will be extremely
difficult to teach to Artificial Intelligence. A fantastic example of this
can be found in visionary director Neill Blomkamp’s Chappie, where
the eponymous robot is trained not to kill a human, but the
nefarious gang at the centre of the plot easily convince it that
harming a human is nothing like killing one, with unsurprising
consequences.
 
Irrespective of this, Asimov’s laws are really a bit superficial and
almost impossible to govern. In 2021, the European Commission
released a far more comprehensive (although still open to
interpretation) set of guidelines that they wish to enforce across all
organisations implementing AI within Europe. In short form they
are:
 
1. AI systems must be continuously assessed for risk
2. Only high quality data should be used to train AI systems
3. All AI algorithms must have their logic clearly documented
4. This logic must be clearly and unambiguously provided to
everyone using the system
5. Quality and Compliance must be prioritised and clearly
demonstrated at all times
6. There must be human oversight in place at all times when AI
is operating, with the capability to stop or override the AI at
any time (i.e. A human in the loop)
7. AI must be coded to the highest level of robustness (i.e. Don’t
make it buggy)
8. All systems that are assessed to be of high risk must be held
on a publicly accessible register. Areas where AI has been
identified as being high risk include:

a. Systems related to employment, recruitment or employee
performance
b. Anything related to biometrics
c. Any system involved in the management or operation of
critical infrastructure
d. Any system that relates to public or private service or
benefits
e. Anything that relates to education, training or
assessment of students
9. The provider of the AI system must be able to continuously
demonstrate that points 1-8 are being adhered to
 
These guidelines seem pretty sensible at first glance, however they
stop short of explicitly assigning clear responsibility and
accountability to the creators of the AI; it is implicit as opposed to
explicit. While many organisations that are responsible for
generating AI solutions are already actively forming Ethics
committees, or employing an external Ombudsman to govern the AI
they produce and employ, this is not as yet a legal requirement
(and is not mandated explicitly in the guidelines above). There are
efforts underway to introduce certification processes for corporate
AI ethics, however certification is a long way from full regulation
and oversight, which would be immensely complex to implement
and police. The court of public opinion is forcing companies to have
a conscious and explicit focus on Ethics, however I think it may be
time to consider a far more clear level of accountability.
How Could AI Creators Be Held Accountable?
Answer: In exactly the same way that Doctors are held accountable
for our health. In the medical world it is called the Hypocratic Oath.
Every medical professional has to adhere to the oath in their
practice. Currently, it looks something like this (UK Version from
2021):
 

“As a new doctor, and a member of the
medical profession:
●      I solemnly pledge that I will do my best to serve humanity –
caring for the sick, promoting good health and alleviating pain
and suffering.
●      I will care for all patients equally and not allow prejudice to
influence my practice.
●      I will respect the autonomy and dignity of my patients, and
will uphold their confidentiality.
●      I will acknowledge my patients’ physical, psychological, and
social needs and assist them to make informed decisions that
reflect their own values and beliefs.
●      I will respect, support and give gratitude to my teachers,
colleagues and all those who sustain the NHS.
●      I will reflect on my practice and recognise my limits.
●      I will seek to increase my understanding and skills, and
promote the advancement of medicine as both a teacher and
a learner.
●      I will work towards a fairer distribution of health resources
and oppose policies in breach of human rights.
●      I will look after my own physical, mental and emotional well-
being in my personal and professional life.
●      I shall never intentionally cause harm to my patients, and
will have the utmost respect for human life.
●      I will practice medicine with integrity, humility, honesty and
compassion.
●      I recognise that the practice of medicine is a privilege with
which comes considerable responsibility and I will not abuse
my position.
I make this declaration sincerely, freely and upon my
honour.”
 
These tenets are what guide the behaviours of all medical
professionals and why, on the whole, we as patients feel

comfortable literally putting our lives in their hands.
 
So, why don’t we have something similar for other professionals
who are responsible for creating things that could potentially do just
as much harm to our wellbeing as medical malpractice does?
 
My suggestion is that, as part of the code of conduct, for any
employee with responsibility for the creation or maintenance of AI
systems, we should introduce a clear and universal framework of
moral principles that they take into their actions. And as with any
doctor who transgresses the Hypocratic Oath, employees who do
not adhere to these guidelines will be “struck off” and no longer be
able to gain employment as an AI Engineer etc. I appreciate that
this might seem complex and possibly a little draconian, but it
would certainly be a way of focussing the minds of the people
implementing Artificial Intelligence and directly shaping our future. 
 
The Other Side Of The Coin
So, supposing (or assuming) we reach the point where AI is
pervasive in society, physically manifested in robots and gizmos that
are as much a part of day to day life as are our friends and
neighbours, walking down our streets, serving us in shops, doing
our work, ask yourself the question; when do they have rights?
When do we have to start treating them with the same respect we
afford humans? I am already a bit guilty of this; I anthropomorphise
many of the gadgets in my home. I have a name for my car - Bruce
(not going to explain why
) and my Robot Vacuum Cleaner is
called Robbie. We are (or at least I am) often guilty of humanising
inanimate objects. We have been doing it for centuries. And, as a
result, we unconsciously associate human-like qualities with them.
It is no coincidence we tend to apply gender to these objects, ships
for example typically referred to as “she”.

(Image courtesy of NBC Television)
 
So, how long before this Anthropomorphism, which has its own
section later, extends to endowing these AI powered mechanoids
with rights, whether it be on the level of the rights we bestow on
animals, like our pets, to be free from cruelty and mistreatment, or
more akin to the rights we associate with fellow human beings?
 
Although it was arguably an act of propaganda and promotion for its
apparently improved stance on women’s rights, in 2017 Saudi
Arabia granted citizenship to a female AI robot called Sophia. While
this is a one off and has divided the scientific community, once AI
has reached Artificial General Intelligence and can genuinely think
and act for itself, there is a compelling argument for a set of rules
related to how we should treat them, complementing our
expectations for how they should treat us.
 
I wonder what suit I should wear to my first robot wedding….
 
**In a 1985 interview Isaac Asimov admitted that he didn’t
actually create the 3 Laws of Robotics that he is famous for; it was
actually his editor John W Campbell.

Explain Yourself
A current area of focus with Artificial Intelligence is the demand for
Explainability and Transparency in the operation of algorithms. In
fact at the end of 2021 the UK Government announced an
Algorithmic Transparency Standard. Ok, it may have been
somewhat “encouraged” (or should that be “shamed”) into doing so
by recent AI-related debacles such as the COVID-19 Track and Trace
program and the algorithm that decided A Level and GCSE student
grades in 2020, but it does show that sometimes good things can
come out of bad. 
 
While the initial focus of the UK Standard is on the use of AI in
public office, there is an increasing demand on private organisations
to also provide transparency into the workings of their AI
algorithms. And this drive comes from a couple of different angles:
●      Public Pressure - As awareness of the prevalence of AI in the
systems used within the businesses and services we transact
with rises, there is an increasing demand for transparency on
how these algorithms actually work. Whether it is a question
of how the results get ranked in a search engine, or why a
loan application was rejected, people are increasingly, and
rightly, demanding explanations of the logic behind these
decisions that directly impact their lives. In recent times there
have been a number of legal actions and investigations
against corporations, ranging from accusations of
discrimination in recruitment processes, to tampering with
search results to influence voting in elections, all of which
have resulted in legal mandates to provide full transparency
into the workings of algorithms.
●      Corporate Responsibility - The pressure on organisations to
provide explanations of their algorithms will increase as
external audit firms demand it in order for them to be able to
fulfil their legal requirement of corporate oversight. How can

the auditors be expected to provide positive audit approvals if
they cannot fully understand the decision making processes
behind the operations of the organisation they are auditing? In
addition, the implementation of AI Ethics legislation such as
that proposed in the EU would force organisations to provide
clear explanations of the logic in their algorithms to everyone
who uses the system.
The Challenges With Explainability
There are a number of reasons why organisations will try to resist
the call for them to fully and publically explain their algorithms:   
●      They Don’t Know Themselves - Sounds a bit crazy on the
surface doesn’t it, but it is often the case with Machine
Learning based algorithms. As described in the What is AI?
section, Machine Learning algorithms are trained by feeding in
sets of data and setting a goal or goals. How the algorithm
builds itself to successfully achieve those goals is in effect a
black box. Think of it this way; if I was to write an old school
rules-based algorithm I could draw out a flowchart of the logic
beforehand and anyone could follow it, but with a Machine
learning algorithm we don’t write the code, we just provide
some examples of good and bad outcomes/goals and a set of
training data. If the algorithm doesn’t reach the goals we set,
we simply feed it more data and inform it of the mistakes it
made last time, until we find the results acceptable.
 
So, what happens in the ML algorithm stays in the ML
algorithm.
 
One of the biggest concerns with this, and one of the primary
reasons why explainability is so important, is that we can’t
have full confidence that the algorithm is applying sensible
logic, or whether it is even using valid data points to make its
inferences. As a great example of this (there are a number of
variants of this story but this is the way I heard it) there was

an algorithm created using computer vision that was tasked
with identifying dog breeds. Through training, the algorithm
reached astronomical levels of accuracy, however every now
and then it would misidentify a dog breed (it had correctly
picked out elsewhere) as a husky. Flummoxed, the engineers
behind the algorithm tweaked the code to force it to highlight
which features of the dog in the picture were being used to
make the erroneous identification. The answer was none of
them. What it was actually looking at was the snow and trees
in the background of the picture. The features of the dog were
being completely ignored. In the training data that had been
provided, all the pictures of huskies had snow and trees in
them and the algorithm had picked up on this consistent
feature as being the primary information it would use to make
its inference for that breed. So, imagine if the ML algorithm
that has been trained to decide whether you should be
approved for a loan, or to set your insurance premium, ignored
all of the details around your health, employment and financial
status and instead focussed in on your declared ethnicity or
gender (effectively the snow and trees in your application
form)... A very good reason for transparent, explainable
logic.  
●      They Don’t Want To Tell You - A reasonable challenge if the
business feels that divulging the logic of their algorithm would
erode their competitive advantage or differentiation. What will
stop their competitors from copying the logic? This argument
has been raised a number of times as a legal defence,
including the investigation into Facebook after the 2016 US
Election scandal. Despite initial resistance, Mark Zuckerberg
was eventually forced to provide the logic used within
Facebook to select and rank the content the site displayed to
users. An additional (and quite common) layer of complexity
results from the fact that many organisations use algorithms
that they purchase from other 3rd party software vendors. In
this case, the software vendor would not be readily willing to
provide an explanation of the logic within their product as it is

effectively the intellectual property on which their business is
based.
●      It’s Just Difficult To Explain - Even if the logic is
understood, it may be difficult to explain in plain english,
particularly if it is complex, or involves terminology or industry
specific knowledge to fully understand it. On the flip side of
this, there is also a possibility that organisations might try to
conceal the logic of their algorithms behind a shield of
complex language and terms. This is the reason for the “plain
english” requirement within the proposed EU legislation.
 
Expect to see increased pressure on organisations to make the logic
of their algorithms publicly available. Next time you are presented
with the outcome of an application or request you make online, ask
yourself - do I really understand why it provided that response?

It’s All About The Data
It really is.
 
It has been estimated that in 2021 there were already 46 billion
devices connected to the internet, with 35 billion of these being IOT
(Internet of Things) devices such as sensors and connected home
appliances. The worldwide average for the number of connected
devices in a household was already 10. These numbers are
increasing rapidly, with the number of connected IOT devices
expected to nearly quadruple in the next decade. And the common
theme throughout all of these devices is the fact that they all create
data. Lots of it.
Garbage In…Garbage Out
Throughout this book I have made numerous references to the
criticality of training machine learning models using reliable data
sources. I have also talked about the ethical issues caused by
biased data, and I have hopefully scared you a little about just how
not-in-control you actually are over the use of your data in the
social media and fun cat video subscriptions you have signed up to
over the years (not judging). With all that in mind I thought it was
probably worth dedicating a couple of pages specifically to the
subject of data.
 
Data is everywhere, and everything we do (or don’t do) creates
more. It has been estimated that, as of 2022, we are generating 2.5
quintillion bytes of data every day; in text, audio and video formats.
And the amount we are creating is on the same sort of trajectory as
the technology we are developing to exploit it i.e. it’s growing
Exponentially, fuelled by the increasing number of newly connected
devices and services we sign up to every day.
 
In fact 90% of the data that exists right now was created in just the
past 2 years.

 
A lot of that data is consciously created by us on our devices
(smartphones, laptops, tablets etc). Every minute we…
●      Perform 3.8 Million Google searches
●      Send 16 Million Text messages
●      Send 156 Million Emails
●      Post 500 hours of video to YouTube (including approx 80 new
cat videos 
)
●      Swipe 990,000 times on Tinder
 
However, some data is also being created unconsciously. In fact
while I was sitting here writing this page I paused for a moment to
think about the data I was inadvertently creating right then and
there, beyond the words on this page obviously. Just off the top of
my head I could think of the biometric data my smart watch was
continuing to collect about my heart rate, movement, location, lack
of exercise, as well as the behavioural data being tracked by
Youtube because of the playlist I had chosen to play as background
noise (I can assure you it was not cat related). There was also
some search activity while I fact-checked a few points on this page;
I am sure that Google wouldn’t pass up the opportunity to capture
that. So, even when we are not trying to, we are still always adding
to that global data mountain.
 
If you have read other sections including What is Artificial
Intelligence? You will be familiar now with the concept that we are
generally moving away from rules-based algorithms to a world
where we rely more and more on machine learning, neural
networks and deep learning, allowing the algorithms to effectively
write and tune themselves based upon the data we feed to them
and the goals we set for them, with varying degrees of human
oversight. And as the volume of data explodes, so do the
opportunities for this Artificial Intelligence to mine this treasure
trove and identify amazing insights that far outstrip the capabilities
of even the smartest human minds. But equally, there are as many

opportunities for the algorithms to make connections, discoveries,
and decisions that are wrong or harmful if that’s the inference it
gains from the data.
 
So, it’s critical that we get data quality right.
 
Sounds straightforward right? But it really isn’t. In fact Data Quality
is one of the biggest challenges faced by the engineers working in
Artificial Intelligence today. Sometimes the issues are simply caused
by data sources recording the data incorrectly, incompletely, too
slowly, or in duplicate. These types of quality issues are estimated
to cost businesses in the US alone about $3 trillion a year (source;
Gartner inc). Companies invest millions of dollars in staff and
processes (generally called Data Governance) to battle the data
quality demons in an effort to ensure that they aren’t going to make
the sort of costly mistakes in these real examples:
 
●      The O Rings that failed during the launch of the Space Shuttle
Challenger were correctly categorised in the data of some
systems as “Critical”, but erroneously in others as “Redundant”,
which lead to them not being investigated thoroughly when
the problem was initially identified, 9 months before the tragic
events of January 1986.
●      While they were setting up their home delivery service, a
market-leading UK grocery retailer initially had the decimal
point in the wrong place on some of the average dimensions
for their fresh produce. This resulted in three vans turning up
to pick up the first customer order containing a dozen 4 metre
cubed tomatoes!
●      Columbus’s voyage of discovery to the Americas in 1492 was
actually aiming for Asia, but the data provided by the Persian
geographer Alfranagus might just have been a little bit awry…
●      Another space one to finish with, this one thankfully slightly
less tragic in human terms. In 1999 the Mars Orbiter crashed

into the surface of the red planet because Lockheed Martin,
who had built the thrusters, had calculated the engine power
required to keep it in orbit in Imperial Standard Pounds of
force, while NASA, who took control of launching it, were
working in Newtons, the metric units. Ooops. That alone was a
$300 Million mistake.
 
These are all examples of where the quality of data was
unintentionally wrong, but we also need to consider a couple of
other scenarios:
Bias or Skew
Bias or skew in data could occur for a number of reasons, some of
which are discussed in more detail in the Ethics section, but in
short, our own biases (whether conscious or unconscious) can
influence the data we choose to feed to the machine. We may even
cherry-pick the data that helps to confirm our personal biases or
assumptions. However, quite often the skew can be unintentionally
caused by factors such as a reliance on historical data sets that are
in themselves skewed or biased, as was the case with the engineer
profiles data used to train Amazon’s recruitment algorithm. The fact
that, historically, most employees in the computer industry were
white and male should not have been used as conditions for
selecting the next generation of software engineers.
 
Theodore Roosevelt once said “I believe the more you know about
the past, the better you are prepared for the future.” However,
sometimes sufficient historic data is simply unavailable and
synthetic training data needs to be generated to provide a dataset
sufficient in size to train the algorithm. This creation exercise in
itself is an opportunity for error and bias, exactly as is the case with
real data.
 
Basically there is no guarantee that good quality training data will
make its way into our AI implementations, and as such close

scrutiny of the data AND the impact it has on the training and
subsequent behaviour of algorithms are imperative.
Nefarious Bad Data
Fake news is a scourge on society and is something we encounter
more every day. There are two different forms of fake news
propagation; Disinformation is the intentional (and in my opinion,
generally criminal) activity of creating fake news stories and
proliferating them in cyberspace. Misinformation is the further
distribution of these fake news stories by parties other than the
original creators. This proliferation of misinformation is often
unintentional since the person spreading it around the internet is
generally unaware that the original story was fake. Recent
examples of fake news and it’s impact include:
●      Campaigns of coordinated disinformation on platforms such as
Facebook during several major elections, including the 2016
US election and the UK Brexit vote. While some of this activity
originated from the now defunct British Consulting Firm
Cambridge Analytica, origins of much of the disinformation
was never fully established. Most of the evidence points to
certain foreign states working to destabilise the subject
governments and economies. However this type of fake news
is not limited to just disrupting political campaigns; it has been
used for even more sinister purposes including providing false
justification for military invasions.

●      The claims that the life-saving COVID-19 vaccines contained
minute 5G chips, designed by Bill Gates of Microsoft, to
facilitate some form of evil mass-tracking initiative. Doesn’t it
sound ridiculous when you read it? Apparently not to any
number of anti-vaxxers who lapped it up and used it as
another excuse to increase the already unreasonable pressure
being put on health services around the world. Amusingly, the
circuit diagram “leaked” with this story turned out to actually
be that of a pre-amplifier for a guitar. 
 
But what does this section really have to do with AI? I can almost
hear you say…
Well, whatever the purpose, or the immediate effect, of fake news,
it is data and produces further data in terms of biased or skewed
response activity. And this data is largely indiscernable from all the

trustworthy stuff it shares cyberspace with. Although mainly
unintentionally, it is increasingly finding its way into the training
data sets that we feed into our algorithms.
 
Unless we get a handle on the spread of fake news and the skewed
data it produces we are seriously impacting the trust we can put in
the AI we use everyday.
 
Do You Speak Data?
There is a possibility that you may have heard the term Data
Literacy, perhaps at your place of work. You may even have been
on some Literacy training, which hopefully focussed on building up
your knowledge and confidence in using your business data and
systems, and perhaps increasing your level of curiousity as to what
else it could be possible to achieve with the growing datasets
surrounding you at work. The training should hopefully also have
made you more conscious of the fact that everyone in the company
has a duty of care for the data that passes across their screens,
ensuring that it is complete and accurate, as well as ensuring its
use remains within the bounds of the laws and regulations related
to it e.g. GDPR. This particular responsibility is called Data
Stewardship and is a major component of the Data Governance
framework most organisations employ to try and ensure that their
data quality remains as high as possible. Your company may even
have people whose role includes formal Data Stewardship
responsibilities, however everyone in the company should be an
aspiring steward.
 
Data Literacy is now a major focus for many organisations who
realise that, without it, their processes are often inefficient, error
strewn, and the opportunities for innovation and improvement that
exist in almost everything we do at work are being missed. And we
are not really talking about a new set of skills here. Most of us are
already Data Literate to some extent; we use banking apps, keep

spreadsheet lists of our vinyl records, and we certainly know how to
use the satnav in our car or on our phone. The Data Literacy
curriculum at work is just about making people apply the same skills
to the data and processes in their workspace. It may sound
surprising that this is not natural, but it clearly isn’t and tailored
Data Literacy training for the workforce is now a major agenda item
for the majority of business leaders.
 
The term Data Literacy covers a broad spectrum of capabilities and
everybody at work needs some degree of familiarity with each and
every one of these. Gartner for instance identifies about 10
categories, ranging from the ability to tell stories with data, to
understanding how AI is used. Of course not every employee needs
to know the details of how to implement AI, or we would all be
going on coding courses in programming languages like Python.
However every single employee should at least have a basic
understanding of what Artificial Intelligence is, how it is used in
their company, and how the data they work with is used by the
algorithms.
 
This is all well and good in the spirit of making the workforce more
data literate but, now that Data and AI are playing an increasingly
large part in our everyday life, it is becoming increasingly important
to re-visit aspects of Data Literacy outside of work as well,
particularly ensuring everyone has a basic understanding of AI and
how it plays an integral part in their lives. Some countries are more
mature than others in their thinking around this. Take for instance
Finland; I have been lucky enough in the past to have worked with
some of their central government team who are responsible for a
program called Aurora AI (lots about this on the internet) which is
at the heart of their AI Strategy. Aurora AI is going to
comprehensively connect the services provided by the State to the
life journey and needs of their citizens. It is visionary and will put
them close to the top of the pile in terms of AI-enabled countries,
promising huge benefits for the whole population. But what I think
is really important and truly visionary is that the team has identified

that if they are going to become a leader in AI they are going to
need to bring the population along for the ride. Their approach is to
offer free courses (through the University of Helsinki) on the
“Elements of AI”, initially to a small percentage of the population,
but eventually to as many people as they possibly can. I am hoping
(although not optimistic) that this approach will be mimicked by
other nations, because Artificial Intelligence is equally as pervasive
in the lives of people in most developed countries, yet they are
offered little education on what its real impact on them will be.
 
To some extent, you could consider this book to be part of
your AI Literacy Training.
Sustainability and Data
Back in the Wonders of Artificial Intelligence section I talked a bit
about how AI can and is helping solve the problems we have made
for ourselves with the wilful destruction of our planet. Unfortunately
the advances we are making with AI and data are actually
contributing to the same problems that they may help us eventually
solve, mainly because of the excessive power usage needed to run
them. A couple of facts for you to mull over:
●      In 2020 it was estimated that the power consumption of the
servers running the Ethereum Bitcoin exchange (which is the
2nd biggest cryptocurrency in the world) was about 33
GigaHertz per hour, which is equivalent to the national power
consumption of Serbia.
●      It is estimated that by 2025, one fifth of the world’s power
consumption will be used for data storage.
So, it is clearly important that we build and use our computer
systems in such a way that they don’t contribute further to our
demise. And there are signs that the message is getting through.
Many major corporations now have deliberate sustainability plans
based around how their IT systems contribute towards their own
carbon neutrality, recognising the reputational boost it gives them in
the eyes of their customers. And these plans factor in the amount of

data they store, how long they keep it, how much processing power
their algorithms use, and how efficient they are. They also evaluate
how much power they are using to keep their server rooms at the
right temperature and where they source that power from (e.g.
renewable sources vs. fossil fuels). Companies who provide a lot of
the cloud storage used by these organisations to host their data are
trying to make changes too; Microsoft, for instance, are building
underwater data centres off the coast of the UK, using the sea
water to naturally cool the servers inside instead of burning
unnecessary electricity. And the Bitcoin exchanges are committed to
changing the way they process transactions on their ledgers. Their
planned approach, if implemented, would mean a reduction of
nearly 80% in their power usage.
There is a long way to go, but at least there is momentum building
around sustainability for data. We have reason to be optimistic, but
we mustn’t take our foot off the gas or reduce the public pressure
on organisations to do the right thing. If we do, there is every
chance that they will just revert back to their old, power-hungry,
planet-killing ways.
 

Anthropomorphism
The Human Disease
As a result of the arrogance of our species, we tend to imagine that
all the best things come bundled in the classic humanoid structure.
Two arms, two legs, one head; it’s just the way our limited
imaginations work. It’s why nearly every alien in a movie has them
and why we automatically navigate to that shape when we think
about the physical manifestation of Artificial Intelligence. It’s pretty
ludicrous really. Take for instance the first image that the media
always use when they report on AI (here it comes again):
Even setting aside the question of why we would even design
androids with bad dental work, the whole concept of killer robots in
humanoid shape seems more than a little short-sighted. As a form,
two arms and two legs is enormously limiting in terms of mobility
when you compare this to drones for instance. Yet still we do it
because, as a species, we collectively suffer from anthopomorphitis,
my new word. These images are so deeply embedded in our psyche
that I witnessed an interview with some school children where they
were asked how they would tell the difference between good AI and
bad AI and their answer: Good AI robots would have blue eyes, bad
AI would have red.
But this anthropomorphism does have some uses, just not the killer
robot use case we seem to always gravitate towards. Care

assistants for instance; robots such as Pepper (pictured below) are
being actively deployed to help with care for the elderly, the
anthropomorphism successfully providing a sense of companionship
and connection for the occupants of the care homes where they are
active.
(Image courtesy of Softbank Robotics Europe)
The trick to making this work seems to be two-fold; don’t make the
android too human-like (which makes them creepy), and make
them smaller than your average human adult in stature.
There are some exceptions to these rules however, for instance the
sex robot trade. Whatever your opinion of this, there are companies
developing extremely life-like replicas of sexual partners for their
clientele. As of 2021 these generally don’t have much actual
mobility (thankfully), however they are extremely visually and
texturally realistic (I am getting slightly uncomfortable just typing
this). Some of them also include fairly cutting edge Computer Vision
and Natural Language Processing capabilities (allegedly), allowing
them to recognise their owner/mate and hold…errr…intimate
conversations.
 
Hollywood has given us many visions of this anthropomorphism that
are, in their own ways, far more chilling than the in-you-face
Terminator franchise. Consider for instance the classic Blade
Runner, where the replicants are so indistinguishable from the
humans that there are specialist detectives like Deckard tasked with
weeding them out. And in an ingenious twist the reader or audience
are left somewhat unsure whether he is himself a replicant.

However, film directors also have a tendency to use this
anthropomorphism in a way that reinforces our fear or distrust in
Artificial Intelligence. In the otherwise very enlightening
documentary The Social Dilemma on Netflix, the AI that is
manipulating and controlling our behaviours through social media is
portrayed in human form by 3 versions of the excellent actor Victor
Katheiser. By using this tactic, the filmmakers added a sinister edge
to the AI that is almost certainly absent in reality.
 
Oh Good Boy!
Whether our companions and co inhabitants have human form, and
irrespective of the fact that they are not living, sentient beings, we
sometimes exhibit another symptom of anthropomorphitis towards
them; we give them personalities and attribute feelings to them.
Just as we talk to our dogs and cats (despite the fact that the latter
just ignore what we are saying), we also chastise Robbie the Robot
Vacuum when he rolls over the same piece of fluff 3 times without
picking it up (or… again… is that just me?). And if they break, we
can feel some level of grief that you wouldn’t normally associate
with an inanimate household appliance. An amazing example of our
level of this type of anthropomorphism can be seen in the response
when Boston Dynamics released a series of videos showing humans
kicking and pushing over their robots to demonstrate how quickly
and ably they recover or right themselves when they trip over.
There was an avalanche of complaints on their Youtube channel
about the cruelty being inflicted on these inanimate, unfeeling
mechanoids! That’s pure anthropomorphism at its best.
How Far Do We Go?
But how far do we take this? The term Anthropomorphism itself
does not only refer to applying physical form, but also to other
characteristics we associate with humanity. Take for instance
physical sensations like pain: there is a really interesting debate
about whether we should give our AI powered androids this type of

human attribute in order to train them to behave in the right way.
Early humans very quickly learnt not to put their hands over a
naked flame, and then used this new lesson to warn future
generations that it would also cause them harm. How would AI
learn this type of lesson if we do not give them the ability to sense
pain? Without it, would we just see generation after generation of
robots with burnt off fingers?
And on a more deep level, our ability to feel pain, whether it is
physical or emotional, is a major building block of our consciousness
and self-awareness; it is one of the main ways we understand good
from bad, and how we develop our sense of empathy and sympathy.
If, and again this is almost certainly a symptom of
anthropomorphitis, we truly are intent on developing AI that is at
least as intelligent as we are (see the section on Singularity for why
this will probably only be a fleeting parity), we need to consider if
this is possible without AI being able to learn good from bad, or
pain from ecstasy.
 
If we do not find a way to provide these lessons, how can we
expect our creation to understand what is good or bad for their
human cohabitors?
 
I would strongly recommend looking up the series of short films on
the subject created by Beth Singler, Anthropologist and Researcher,
and her team at Cambridge University. They bring these dilemmas
to life very vividly and entertainingly; Pain in the Machine, Friend in
the Machine, Good in the Machine and Ghost in the Machine.
 

QR Links to all 4 of Beth Singler’s fantastic videos exploring our coexistence with
AI

AI as a Weapon
 
If there is one section of this book that keeps me awake at night
more than any other it's this one.
 
Let me start again by stating that I really don’t think the future
looks like this:
 
 
It just seems unlikely that, however much of the superpowers of the
world’s tax dollars are spent on robotics research, the outcome
would be to invent armies of super-intelligent, super-strength
soldiers that have all the inefficiencies and failings of the human
body. This is a hypothesis that is well supported by many
researchers and members of the military. That being said, you can’t
get through the episode of the extremely thought-provoking series
Black Mirror called Metalhead without making comparisons to
Boston Dynamics’s Spot robot:

(Image courtesy of Saggittarius A)
Conventional Weapons
No, it is more likely that conventional weapons will continue to take
the form of drones and autonomous vehicles, whether it is for
surveillance or offensive purposes. And there are some very valid
reasons for using AI-based systems to replace humans in the
theatre of war:
●      Replacing a human with a robot means potentially one less
human soul at risk of harm on the battlefield.
●      While the systems, weapons and vehicles deployed by the
military are all extremely expensive, one of the largest costs is
related to training, maintaining, and protecting the soldiers,
sailors and pilots that operate the conventional equipment.
●      Humans are fallible in more than one way. Firstly, they can
make bad or poor decisions, particularly when put under
pressure, and I can think of no more pressurised situation than
being shot at. Secondly, their bodies also have limits. There is
a finite limit to the number of G they can cope with in an
evasive flying manoeuvre (according to the latest Top Gun film
that limit is between 6G and 10G, depending on where you are
in the star billing for the film - sorry about the spoiler). A
drone does not have this degree of limitation.
●      Building and maintaining human-operated machinery is a
costly business. The price of the European Typhoon fighter
plane is around $120M per plane and there is no getting away
from the fact that a lot of that cost is down to the fact that the
design has to cater for the frailties and safety of the pilot.
Setting aside the potential for tragic loss of life when one of
those vehicles is shot down or crashes in training, $120M is a
lot of money, and in the end, it is your tax bill that is paying
for it. AI powered drones with equal, or even more
devastating, firepower have the potential to be far less costly
to build and operate.
 
 

 
The image on the left is a miniature surveillance drone being used by the US
Military (courtesy of Cpl. Christian J. Lopez). The image on the right (courtesy of
Armyinform.com.ua) is the STM Kargu attack drone that is fully autonomous
and has already been actively deployed by Turkey in a military campaign in Libya.
It is interesting to note that the United Nations proposed a ban on drones as
offensive weapons in 2018, but both Russia and the United States blocked the
motion.
 
But, the truly frightening dimension of this is not so much the lethal
killing power and versatility of the machinery itself; it is the rise of
fully autonomous weapons with no human control, such as the STM
Kargu drone pictured above. The advances in AI capabilities and the
extensive research and development budgets of the world’s military
powers (e.g. DARPA in the United States) means that we are rapidly
moving away from developing weaponry that has a Human-in-the-
loop (where the kill decision is made by a human operator) towards
weapons with a Human-on-the-loop (where the kill decision is made
by the AI, but the human has the opportunity to review and
override the decision), and then even further towards weapons with

the Human-out-of-the-loop (where the human has no control over
the kill decision at all).
 
The fully autonomous hunting, identification and elimination of a
named target in Libya in 2019, by a Kargu drone, is thought to be
the first active use of a fully AI driven Human-out-of-the-loop
weapon. It is designed to be deployed in huge swarms that would
be almost unstoppable by even the most well-armed foot soldier.
Let’s just pause to reflect on this for a moment:
●      This is a real weapon, fully driven by Artificial Intelligence - a
robot killing machine.
●      It relies on facial recognition software. The same technology
that was shamed so effectively in the 2020 Netflix
documentary Coded Bias (watch it as soon as you can). The
same technology that, again in 2020, the Detroit Chief-of-
Police stated was inaccurate 96% of the time. The same
technology that, in 2018, incorrectly matched 28 members of
the US Congress with known felons.
●      Given the fact that this is a relatively cheap and transportable
weapon, how long before this becomes the terrorist’s
assassination weapon of choice?
●      It’s a robot killing machine! I may have already mentioned
that.
 
You could argue that autonomous robot killing machines are not
really such a new concept. After all, isn’t that what a landmine is?
However, one of the major differences is that most (if not all) of the
world’s superpowers are actively signing up to the treaty to ban the
use of landmines, while the same cannot be said about their
intentions with drones etc. Anyone who wants to truly terrify
themselves over this should take a few minutes to watch
Slaughterbots and its equally disturbing sequel Slaughterbots - If
human: kill() :

       
QR Links to both Slaughterbots videos
There have been several attempts to introduce bans on the
development of these weapons, including a proposal put forward as
part of the nattily titled Convention on Certain Conventional
Weapons (C.C.W), which 125 countries have signed up to. Notable
abstentions from this agreement included the USA and Russia, but it
is not just these two super-powers that seem to have other ideas
about the deployment of these terrifying weapons. For example,
take a look at this section from the current UK Data Strategy for the
Ministry of Defence, available openly on the gov.uk website:
Extracted from the UK MOD Data Strategy
 
Despite all this, there is a use of AI that is more immediately
threatening to our welfare…
Weaponising Cyberspace
In the short to medium term, one of the primary forms of territorial
agression between political powers will be cyberattacks against
critical infrastructure and natural resources necessary for the
functioning of society. Nations will be deprived of these fundamental

resources (such as water, power, oil) as a result of state sponsored
attacks enabled by ever increasingly powerful Artificial Intelligence
algorithms. These attacks may well be a pre-cursor to conventional
military operations e.g. an invasion. And they are already
happening on a frighteningly regular basis:
●      Between late April and mid-May 2007 a massive sequence of
Distibuted Denial-of-Service (DDOS) attacks against key
internet-based infrastructure services, including key banking
services, public services websites, telecommunications
providers and media agencies, caused complete chaos across
Estonia’s capital Talinn. Although the Estonian parliament have
fallen short of directly accusing Russia of coordinating the
attacks that brought the country to it’s knees, it is widely
recognised that the attacks were a political reprisal for the
removal of a statue commemorating Russia’s role in liberating
Estonia from the Nazi’s in WWII. A list of originating IP
addresses, all in Russia, has been published, and it seems
likely this was no coincidence.
●      For a period of 3 years, starting in 2003, persistent and
prolonged cyberattacks against Government and Security
Services in both the US and UK caused panic and widespread
disruption. It is widely believed that this activity was
sponsored by the Chinese government.
●      In March 2007, in an exercise by US Homeland Security
employees, they simulated a cyberattack on a US Power
Station that actually managed to make the generator
physically explode. Search in Google for ‘Power Grid Hack’ and
you will find an alarming number of confirmed real world hacks
against grids all over the world, whether it is electrical, nuclear
or hydro power, causing disruption and misery to thousands.
●      In what is frustratingly a genius 2014 hack by the Russian,
state sponsored, hacker Fancybear, Ukrainian military units
received a free, and apparently legitimate, upgrade to the
Android software that they used to target their mobile field
guns. The new software promised to reduce the time taken to
calculate the targeting solution from minutes to seconds. It

lived up to this promise, but at the same time it started
sending accurate location information back to the Russian
military, allowing them to precisely target and destroy the
majority of the otherwise elusive mobile gun units. 
 
The point of these examples is to show you how pervasive and
regularly these allegedly state-sponsored attacks occur. In fact,
Cyber is now formally recognised as the fifth theatre of war by the
UK Ministry of Defence, the others being land, sea, air, and space.
However, hacking and cyberattacks are by no means the sole
property of state sponsored agencies; take for instance the
activities of any number of bedroom hackers out to make a name or
a quick buck, such as Navinder Singh Sarao, the individual whose
stocktrading algorithm inadvertently caused the Flash Crash that
decimated the financial markets in May 2010, as well as the
activities of hacktivist organisations such as the well-publicised
Anonymous. The activity of these organisations is often publicly
driven by a political agenda and there is no better example of this
than in the activities of these groups during the recent conflict in
the Ukraine, where hacktivists have been responsible for a number
of attacks against the infrastructure of the Russian invaders,
effectively democratising warfare. You no longer need a uniform and
a gun, just a belief in a cause and decent Wifi. 
 
These cyberattacks are increasingly reliant on AI. Whether it is the
use of swarms of AI bots to inflict denial-of-service attacks against
websites by flooding them with access requests and overpowering

the servers, or the use of ever-increasingly elaborate algorithms to
crack security measures to gain access (there is a wonderful
fictional example of this in Kai-fu Lee and Chen Qiufan’s AI 2041
using Quantum technology), the point is that those that wish to act
nefariously on the internet are increasingly leveraging Artificial
Intelligence more and more. In fact in 2017, Russia’s President
Vladimir Putin stated: “Artificial intelligence is the future, not only
for Russia, but for all humankind. It comes with colossal
opportunities, but also threats that are difficult to predict. Whoever
becomes the leader in this sphere will become the ruler of the
world.” Prescient stuff, although, in the interest of fairness, he did
say this to an audience of school children, and his next line was “If
Russia achieves this, it would have a responsibility to share the
benefits with the whole world.” I trust him…
(Image courtesy of kremlin.ru)
 
Likewise, China has openly stated that it is focused on becoming
the world’s AI Superpower and as of 2021 seems intent on making
good on its word. The percentage of total research papers on AI in
the past year produced in China (28%) has leapfrogged those of
their nearest rival - the US (18%). Given the number of
cyberattacks attributed to Chinese sources this is extremely
ominous.
 

One concern is that the state or superpower that achieves this
dominance and becomes the first to achieve Artificial General
Intelligence may be seen as an unacceptable threat to other nations
who are lagging behind. Achieving Artificial General Intelligence
would probably mean a state would be able to make such rapid and
significant technological advances that other countries would fear
their dominant position. Bad things tend to happen when one nation
feels threatened by another… 
Staying in Control
All the way through this book I have continued to stress the
importance of staying in control of our implementations of Artificial
Intelligence, and the need for human-in-the-loop control wherever it
is used, but as we implement increasingly autonomous AI solutions
and get closer and closer to the concept of Artificial General
Intelligence and The Singularity, we run the risk of AI making life
threatening or eliminating decisions. Imagine if the 1983 Russian
Missile Defence System did not have a Stanislav Petrov (remember
him?) to instil some humanity into the process. If we allow AI to
fully control life threatening scenarios, either intentionally or
unintentionally, it won’t be long before the remake of the 1983 film
classic War Games will be a historical documentary as opposed to
fiction. If however we do manage to keep a human-in-the-loop of
our potentially life-endangering AI implementations (something that
even the Chinese have proclaimed will be the case), we need to
make sure that the human is not placed under duress or pressure so
as to ensure that they are making the right decisions for humanity.
Imagine if his commander had been standing over Stanislav Petrov
with a cocked pistol commanding him to follow orders…
       

The Singularity etc.
We Are Not There Yet
I have talked about applications of AI that perform a task or set of
tasks with varying degrees of success, whether it be driving a car or
winning at a quiz game. These are all examples of Narrow AI
(sometimes also referred to as Weak AI), where the algorithms are
specifically designed and tuned to solve for a specific challenge. The
reality is that this is all we are capable of creating at the moment.
Sure, some of the systems we have built can be adapted quickly to
solve alternative problems. Take for instance Deepmind’s Alphazero
chess algorithm, which was successfully adapted into Alphago which
in turn was equally as ruthless towards its human opponents as its
chess counterpart and was then further adapted to become
Alphafold, solving a 50 year old challenge to map out protein
structures. These are all specific narrow solutions and each time the
gauntlet was laid down, the engineers and boffins at Deepmind had
to get back under the hood to retrain and tweak the algorithm for
success. At no stage was the AI able to adapt itself to take on these
new tasks. Even though Deepmind have also created a variant that
can play and win at all 57 cartridge games released on the old Atari
2600 console (those were the days), it is still really an example of
Narrow AI since the scope is actually just one challenge - win at
video games.
 
There are effectively 2 further stages to AI development that we
have not yet been able to achieve; Artificial General Intelligence
and the phenomenon first coined by the brilliant mathematician
John Von Neumann back in the 1950’s, The Singularity. These terms
are often used somewhat interchangeably, which is a mistake
because they are actually 2 slightly different things which merit
being separately defined and handled.
 

Artificial General Intelligence
AGI is the obvious successor to the Narrow AI we are currently
developing; an AI that can turn its hand to any challenge thrown at
it. So, imagine a scenario where Deepmind’s Alphazero could have
learnt how to win at Chess, and then turned its hand to successfully
driving a car and maybe solved world hunger for dessert, all without
the need for retraining or any other intervention by the human
engineers that created it. This is the next logical goal for AI
innovators and progress is being made, with at least 50 projects
underway by 2020 to try and achieve it. But the development of an
AGI will come in phases. Don’t make the mistake of imagining we
will go from Narrow AI to human level AGI in one shot. The reality is
that it will come in stages that will initially mimic the intellectual
capabilities of creatures lower down the evolutionary scale than
humans (no arrogance implied here). It will start with recreating the
intellectual capabilities of, say a single-celled organism, followed by
perhaps an insect, then a rat etc. etc. until eventually the AGI can
be proven to reach the level of a human. Even amongst experts in
the field there are a wide range of estimates as to when AGI could
reach human levels, however the range 2040-50 seems to come up
repeatedly in studies.
 
It is important to understand that achieving human level Artificial
General Intelligence does not in itself mean that the AI will have a
genuine consciousness or real self awareness, although these
qualities could well be sufficiently simulated in the system so as to
allow the AGI to pass tests such as the Turing Test (see below).
 
Beyond Human
If the AGI continues to learn and adapt beyond human level, which
is highly likely, it will be the creation of Artificial Super Intelligence
(ASI). At this stage it will be able to solve problems and innovate
beyond the capabilities of even the smartest of humans e.g.
someone like Albert Einstein or John Von Neumann, and per the

image below there would probably only be a fleeting moment of
parity with human intelligence before the AI left us far behind.
 
 
I strongly recommend watching Sam Harris’s TED talk on the
subject; Can we build AI without losing control over it? which
explores this phenomenon and delves into some of our fears about
what could happen if we stumble blindly towards AGI and the
Singularity.
QR Link to Sam Harris’s TED Talk on AGI

The Turing Test    
Created in 1950 by brilliant mathematician and linchpin in the
cracking of the Enigma code in WWII, Alan Turing, the Turing Test
was a proposed test that could be set for any computer system to
determine if it could demonstrate intelligent behaviour that was
indistinguishable from a human. It continues to be the most often
referenced benchmark we use to judge the progress being made
towards AGI. The test involves an evaluator holding a conversation
via text inputs (well this was the 1950’s after all) with two interview
subjects simultaneously. One subject would be a computer, the
other a human. If, at the end of the interview, the evaluator could
not confidently identify the human interviewee from the computer,
the computer would be deemed to have passed the Turing Test.
This regularly referenced test has its critics since the style and
scope of questioning has been deemed insufficient to identify the
presence of a consciousness or self awareness, and as such is
largely a test for the presence of Artificial General Intelligence as
opposed to the Singularity.
The Singularity
Although the term actually originated in the field of cosmology,
relating to black holes, Singularity has since been adopted and re-
purposed by researchers and futurists to refer to the ultimate end
game in AI evolution. Written about heavily by futurists such as Ray
Kurzweil, it is the logical progression from Artificial General
Intelligence where the AI is genuinely self-aware, no longer directly
under control of its human creators, and evolving its capabilities at
a rate which we cannot comprehend or hope to control. In essence
it will have become entirely independent of human influence, well
other than possibly being reliant on us to maintain its hardware and

provide natural resources to power it (although this is in itself
debatable since the AI may be able to determine ways to do this
without the need for us humans). One major thing that delineates
this exponential progression from human adaptation and change is
that it will be the first instance of truly conscious evolution, by
which I mean that the AI will be able to decide exactly how it
adapts, and then reconfigure itself instantly to take on new
challenges, over and over again. Whereas us humans, and every
other living creature, have to wait for the slow pace of natural
evolution to take effect. It would be like humans deciding they
needed opposable thumbs to get by and just clicking their fingers
and the new digits appearing. OK, I get that they couldn’t actually
click their fingers if they didn’t have opposable thumbs, but you get
my drift. 
   
 
Singularity is the concept that divides researchers and experts the
most. Not only is the timing of when we could reach this point a
huge area of debate, with estimates ranging from 2070 to, in the
opinion of 21% of a pool of nearly 1000 interviewees, never, but
also the impacts on humanity range from total obliteration to the
development of a perfect enlightened society living in harmony with
our new robot friend. The reality is that the outcome is likely to be
somewhere in between these two scenarios, but the exact
positioning of the needle on the heaven to hell scale will be entirely
our responsibility. Let’s take a look at the two extremes.
 
Doomsday
This scenario is so popular in fiction because of the obvious appeal
of killer robots. Here, the AI will either intentionally (per Skynet in
Terminator), or unintentionally, decide that we are surplus to
requirements and either enslave or totally destroy humankind.
There are a number of arguments for this scenario; if AI becomes
truly enlightened it may look at humans as being the cause of most
of the world’s problems and decide the planet is better off without

us. This does have a huge amount of credence when you consider
the facts of our history; we invade, we plunder, we drive other
species to the point of extinction and beyond. Our resumé doesn’t
look great. Why would the Singularity want to keep us around if it
perceives that we do more harm than good? An alternative
doomsday scenario would be where the goals of the AI lead to the
unintentional destruction of humankind because these goals do not
rely on our existence or cooperation. This is akin to the species we
have driven to extinction on our planet through actions such as
deforestation. We did not set out to wipe out thousands of species
of flora and fauna, but our goals did not include preserving them.
Nick Bostrom, one of the most eminent personalities researching
this space, has suggested that the most likely doomsday scenario
perpetrated by the rise of AI will be the complete failure of our
global economy, with the ensuing political and civil unrest leading to
the disintegration of society.
 
Harmonious Living
In this more optimistic scenario we find a way to adapt our
relationship with the machine such that we can exist in a mutually
beneficial way (less Terminator, more Wall-E, only hopefully with a
better human fitness regime). In this world, the AI would value our
existence as much as we value it. AI would help us to solve some, if
not all, of the major problems we face (most of which we have
inflicted on ourselves) such as global warming and life threatening
diseases like cancer. In the extreme Transhumanism version of this
scenario we have become “at one” with the AI, where the
boundaries between human and AI are blurred, whether it is
through implants or neural linkage or, in the most extreme variant,
full upload of our consciousnesses such that we become in effect
one gestalt digital entity. Don’t laugh, it could happen.
 

Our Actions Matter
Clearly we want to steer things towards the second scenario and
away from the first as we develop AGI and edge towards the
Singularity, and there are a number of steps we can take as we
develop AI to help tip the balance in our favour. Mainly we need to
ensure that we are training it to value humanity and ensuring that
we instil in it the right ethical code; ensuring that we approach the
path to Singularity with the right level of caution and control to
allow us to correctly judge its intentions before we grant it full
citizenship will be crucial. See also the How Do We Stay In Control
section (Oh! It’s up next anyway…how exciting! 
).

How Do We Stay In Control?
Whether you have jumped straight to this section, or read this book
from start to finish, I am really hoping that the question you are
asking yourself is; well, given all of the wonderful opportunities AI
presents for the human race, but also all of the threats to our
wellbeing that could result from our negligence, or abuse, of its
power, just how do we stay in control of it?
 
Or something like that…
 
There are a number of schools of thought about just how we might
achieve this, some of which I have already covered in other sections
of the book, but let’s explore the possibilities:
Stop Everything
Back in the early 19th century, 1811-1816 to be exact, during the
Industrial Revolution, a group of disgruntled workers, led (or at
least inspired) by the eponymous Ned Ludd, revolted against the
automation being introduced as a result of the Industrial Revolution
and the impact it was having on the livelihoods of the workers being
replaced by the machines. Their revolt consisted of sabotaging the
new mechanical looms etc. being installed in their workplaces, and
generally making a nuisance of themselves to their erstwhile
employers. While their revolt was ultimately unsuccessful, we have
been forever furnished with the word Luddite, nowadays referring to
someone who does not accept or adopt technological change.
Unsurprisingly, the Luddites were mentioned many times in Ted
Kaczynski’s Unabomber manifesto.
So, taking Ned and the gang’s example, and I am not advocating
that people rush out and start smashing up their work laptops, one
option is that we simply stop adopting or progressing AI now,
before there is the opportunity for it to all turn bad on us. However,
the reality is that it is probably already too late for that. We are
already reliant on AI for so many things in our day to day life that

we would really struggle to roll it all back, and even if we just
stopped advancing our adoption and implementation of new
systems, there is every possibility that the systems already in place
could in themselves “learn” to go bad if we stopped maintaining
them and feeding them quality-controlled data. Additionally, there is
a fundamental question here about why we would want to stop
advancing technology that, if managed correctly, could and most
likely will, hugely benefit humankind.
The Luddite Revolt: Some (rightly) very angry people
(Image courtesy of Chris Sunde)
 
And a final thought on this option, well more a reflection on
humanity and human nature; in general, if you tell people to stop
doing something, someone, somewhere will find a way and an
excuse to do it anyway. Then you will run the even greater risk that
the development of AI will be performed outside of any form of
regulation and the potential damage could, and probably would, be
far worse than if we pool our collective resources on keeping it
under control.
The Oracle Or Walled Garden Approach
Again, we are somewhat too late for this because Artificial
Intelligence is already so prevalent in our world, but one approach
that has been proposed is that, as we head closer and closer to
Artificial General Intelligence, we could consider ensuring the
Artificial Intelligence remains powerless to impact humanity by
ensuring it has no way to directly control the environment around it.

Sometimes called the Oracle approach after the Oracle of Delphi, a
priestess who resided in a shrine in Greece in the 8th century BC
and was famed for knowing the future, the approach here is to
allow the AI to have access to all of the information it needs to
make decisions, but none of the power to actually do anything
about it. So, in this approach, all connections to systems that the AI
could otherwise control in response to the decisions it makes would
be turned off, or at least manually controlled (the human-in-the-
loop concept). This would remain the case permanently, or at least
until we were 100% confident that the AI would only ever work in
our favour.
One risk to this approach should be immediately obvious if you think
about the state of technology and internet security today: if hackers
have the capability to break in to our supposedly impenetrable
systems today, how likely is it that the super intelligence we create
in the future wouldn’t find a way to break out?
 
Regulation, Control And Visibility
This is the most realistic option, but still requires a great deal of
work to educate, and most importantly enforce. We should be
looking to apply rigour around how we deploy AI algorithms. We
need to be able to enforce the kinds of guidelines proposed by the
EU Commission (see the Ethics section for details), but we will need
more focus to make these effective. In order to make AI regulation
effective we would need to ensure that, per the proposal from the
UK Law Commision related to autonomous vehicles, the creators,
not the users are liable for any failures or accidents when using
Artificial Intelligence systems, and also that there is always a fully
liable human-in-the-loop for any critical decision being made by the
AI system. Of course this opens a whole can of worms about what is
actually a “critical” decision. And even then we need to make sure
that the creators of these algorithms are fully qualified to build
systems that put humans in the position to make these types of
decisions, per my suggestion on the Hypocratic Oath.

Control Is Not Optional
The three options above, and flavours in between, all have benefits
and drawbacks, but the key is that however we approach controlling
our AI future, we need to do just that; Control it. This will involve
concerted effort that spans across state boundaries and bypasses
political grudges in order to be effective. It is amazing to think that
we have invested literally billions of dollars in defence systems to
protect our planet from the unquantifiable risk of a rogue asteroid
hitting us in the future, but we have yet to invest anything nearly as
significant or coherent in the much more likely risk to our safety
that would be caused by the loss of control over Artificial
Intelligence.

READ.ME.LAST
 
Wow! You made it. Or you cheated and skipped straight to this
section with some form of weird desire to find out the ending before
you experience the thrill of the ride. Whatever. I am not going to
judge. Whichever it is, I hope that you found some or all of this
enlightening, or at least interesting. The point of this was not to
scare you unnecessarily about our AI-laden present and future, but
simply to provide you with some pointers on how to deal with it.
Plus I am tired of being the only AI bore at dinner parties (sharing
the love). All that I have written in this book is based upon factual
and respected research, but I will admit that, in parts, I have also
infused my own perspectives (and conscious bias). Having over 30
years experience in the field I feel reasonably qualified to do that.
 
Whatever else you take away from this book, I would ask you all to
consider the following six point action plan for yours, and everyone
else’s, future:
 
●      Whatever your role in the proliferation of AI systems, whether
it is as a creator or user, you need to ensure that you are
comfortable that the AI puts the safety, equality and comfort
of living creatures first, over all other goals. If you are a user,
make sure you are comfortable that the creators and
administrators have these goals clearly addressed in the way
the system is written and trained. If you are unsure, ask them.
If you are still unsure, or they refuse to answer, consider not
using their AI.
●      Make sure you understand and are comfortable with how your
data is being used by the companies you share it with. Read
those T’s & C’s agreements (I know they are long and boring)
and consider carefully what you post on social media. And if
you feel uncomfortable about what data they may hold, ask
them to tell you. It is your legal right. If you are still not

happy, consider stopping using them and deleting your
account. Legislation such as GDPR means that you have every
right to demand that you are completely forgotten by them
and have all records related to you removed from their data.
●      If you, or your children, are considering your future career
path, you need to think about what types of roles will still
make sense in a world that is filled with AI and automation.
Try to avoid career or education choices that will steer you
towards jobs that could be easily automated and towards roles
that, at least in part, require skills that it is difficult for AI to
emulate; things that require a degree of human empathy
(such as the medical profession), or things that need genuine
creativity (such as journalism or music). While there is no
guarantee that these roles won’t also be the victims of AI
automation to some extent, they are likely to be the last to
go.
●      On the subject of education, if you have some responsibility for
the development opportunities of others, whether it be as a
parent or a teacher, look for opportunities for your students to
develop skills that will be relevant to the types of roles that
will still make sense. Focus their development on problem-
solving, social/communication and creative skills, and away
from the traditional list-based memory exercises that have
infested our education system since the times of the
schoolhouses in the novels of Charles Dickens.
●      Talk about the future. I don’t mean to bore the pants off
everyone you meet with knowledge nuggets you picked up
from this book or the web, but when it makes sense, share
what you know now about AI and its impact on our world. You
will be helping others to arm themselves with the knowledge
they need to make the same sort of informed decisions that
you are now able to make. You will be doing your bit to
protect our future, only without the superhero suit (well, you
can put one on if you want).
●      Use your influence, wherever you can, whatever you feel that
level is, to encourage others to ensure that we are

implementing AI that is responsible AND beneficial to
humanity. As cool as it is to see a picture-animating
application, or an AI “artist”, think about where the same
technology could actually be used for the betterment of
mankind. The World Health Organisation estimates that at
least 10% of the world’s population have some form of
disability, or physical impairment; wouldn’t it be even more
fulfilling knowing that data scientists are developing
technology that assists them in leading a full and productive
life, rather than creating a soulless reproduction of a Monet?
While some level of experimentation is undoubtedly required
to fully realise the potential capabilities of Artificial
Intelligence, once that experimentation becomes frivolous we
need to start asking the question; why are we wasting
valuable resources doing this? In other words, let’s use the
opportunity that this wonderful technology gives us to solve
the right problems! 
 
When you aren’t vigorously pursuing the action plan above, use the
insight you have gained to analyse the world around you. Look at
how AI is already invasive in your daily life, whether it be the apps
and tools you use on your smartphones, or why Netflix keeps
recommending documentaries about cats for you…
  Before long
you will start being able to identify where Computer Vision, Natural
Language Processing, all of those other amazing AI techniques, are
being employed. This new found confidence with these technologies
will help you greatly with the exponentially accelerating changes
that are undoubtedly in our near future.
 
And whatever you do, just remember that we are the creators and
as such we need to take responsibility for our creations. If it goes
wrong we really only have ourselves (and cats 
) to blame.
 
 

It’s not going to be the fault of AI if it all goes wrong. It’s
going to be the fault of HI (Human Ignorance).
 

Sauces
Below are a collection of books, films, shows and other resources
that I recommend and cover the topics in this book. My hope is that
reading through this book has piqued your interest enough to make
you reach for some of them as a follow-up. I have tried to avoid too
many spoilers, but I have added tagging (e.g. #AIEthics) to most of
them to connect their contents to the major themes I have covered
and added a few, very brief, thoughts on why I think they are so
relevant. This list is far from comprehensive and is really just
intended to be a taster to get you started.
Films:
●      Star Wars Episodes 1-9 - They probably don’t need much of
an introduction unless you have been living in an Amish
community for the past 45 years, and for many of my age
group they were the first introduction to what a fully formed
android could be. However, what is really interesting is to
consider the question of why, when they can produce an
android that is fluent in over 6 million forms of communication
(C3PO) and they have mastered insanely complex things like
hyperspace drives, everything else in their universe is still so
incredibly analogue. They still have to manually pilot their X-
wings and Tie-fighters, they still have to manually target
weapons… Best not to ponder for too long if you don’t want to
ruin what is otherwise one of the most iconic and inspirational
series of films ever made. #AGI
●      Blade Runner - Probably a more feasible vision of our future
than the Star Wars films, Blade Runner, and its sequel Blade
Runner 2049, paint a picture of a world where AGI is fully
integrated into a dystopian and slightly steampunk society.
The storyline in both films is excellent and there are a number
of great observations on how one AI-infused future could look.
#AGI, #Transhumanism, #AIEthics  

●      Archive - A less well known film that covers a number of the
themes explored in this book. Great storyline that explores
some very interesting moral dilemmas. #AGI,
#Transhumanism, #AIEthics
●      Her - An interesting film that explores concepts of emotional
attachment to AI. As well as exploring some interesting moral
and philosophical questions around our relationship with
technology, the film also sets the benchmark for what a digital
assistant could be…sorry Alexa - you need to step up. #AGI,
#Anthropomorphism, #AIEthics  
●      A.I. - Quite a long, slow-paced film, but a really interesting
exploration of anthropomorphic, android AI and the challenges
facing their integration into a family unit.  #AGI,
#Anthropomorphism, #AIEthics, #Singularity, #Sustainability
●      The Bicentennial Man - Very similar in vein to A.I., only with
a slightly less cutesy android. #AGI, #Anthropomorphism,
#AIEthics, #Singularity  
●      Chappie - A major over-haul of the topics covered in the 80’s
film Short Circuit, with a much darker tone. #AGI,
#Anthropomorphism, #AIEthics, #AI_as_a_Weapon
●      War Games - Certainly shows its age now, but still an
interesting watch. No Stanislav Petrov in sight. #AGI,
#Singularity, #AI_as_a_Weapon
●      Blackhat - Hollywood shlock for the main part, but cleverly
covers some aspects of cyber warfare amongst the usual over-
the-top explosions and stunts. #AI_as_a_Weapon, #Privacy
●      Ready Player One - Set in a dystopian near-future and
focuses heavily on the social and gaming aspects of AI. #VR,
#SocialMedia, #AGI
●      Wall-E - Despite the Disney treatment of the subject, Wall-E
really asks some great questions about the future of society. I
actually came away from watching it with a slightly bleaker
outlook than you might expect. #AGI, #Work, #Mortality,
#sustainability
●      2001: Space Odyssey - Responsible for a lot of the bad
press related to AGI, and with some plot points that are more

complex than any Christopher Nolan film. Definitely a must-
see film on the subject, despite its age. HAL-9000 is still one of
the most iconic digital “assistants” ever, even if you might cast
a wary eye at your Alexa when the credits roll. #Singularity,
#AGI
●      Terminator - Along with 2001 (above), this film does its best
to paint a bad picture about AI. Despite the fact that I
disagree with some of the film’s image of what the doomsday
scenario might look like, there is still a lot in here that
provides a solid warning about what could happen if we take
our eye off the ball. #Mortality, #AGI, #Singularity,
#AI_as_a_Weapon
●      Transcendence - Probably the film that explores most closely
the impact of the Singularity. Some of the plot mechanisms
seemed a bit clunky to me, but overall it is a solid exploration
of the theories about the future explored by many of the
authors mentioned below. #Singularity, #Transhumanism,
#Privacy, #AIEthics
●      Ex Machina - Really good exploration of a number of the
philosophical aspects of developing Artificial General
Intelligence. including the Turing test and Anthropomorphism.
#AGI, #Anthropomorphism, #AIEthics, #Singularity
TV Series/Shows:
●      The Social Dilemma - This is really mandatory viewing,
especially if you, or your dependents, are heavy users of social
media platforms. #SocialMedia, #Privacy, #AIEthics
●      The Great Hack - An exposé of the Cambridge Analytica
scandal. Some aspects of it remain a little inconclusive, but
still a worthy watch. #SocialMedia, #Privacy, #AIEthics
●      Coded Bias - Brilliant exploration of Ethics and particularly the
impacts of poorly and biased design and implementation of
facial recognition platforms. #Privacy, #AIEthics,
#ComputerVision

●      The Undeclared War - Rivetting drama that follows the
escalation of Cyber Warfare from the perspectives of an intern
at GCHQ and a young Russian. #Privacy, #AI_as_a_Weapon
●      Humans - Rather like A.I., Humans explores integration of an
android, called a Synth in the series, into a family unit. #AGI,
#Anthropomorphism, #AIEthics
●      Westworld - Explores a future where AI and Androids are
primarily used in a theme park as adult entertainment. No
surprises when that goes wrong. #AGI, #Anthropomorphism,
#AIEthics
●      Star Trek - Along with Star Wars, Star Trek is the series that
has been used as the barometer for what the future might
look like. How many of the devices and technologies used in
the show are either already with us, or now seem to be visible
on the horizon? #AGI
●      Black Mirror - Not every episode of this series is relevant, but
in many episodes Charlie Brooker and co manage to cover
some stark and vivid visions of our future with AI. I strongly
recommend at least the following episodes: Metalhead, USS
Callister, Arkangel, Nosedive, Hated in the Nation, Be Right
back, White Christmas, and Striking Vipers. #SocialMedia,
#Privacy, #AIEthics, #AGI
●      Upload - Although this might initially seem to be a bit glossy,
Upload actually does explore some really deep philosophical
aspects related to Transhumanism. #SocialMedia, #Privacy,
#AIEthics, #AGI, #Transhumanism, #Work
●      Becoming Human - Despite the sometimes slightly OTT
presentation, Becoming Human is a good, short series that
covers pretty much everything in this book to some extent in a
fairly engaging style. #SocialMedia, #Privacy, #AIEthics,
#AGI, #Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon
●      The Future of Warfare - No prizes for guessing what this is
all about, but the series is really well produced and covers all
aspects of AI in Warfare, from Autonomous killing machines

through to Cyber Warfare. #AI_as_Weapon, #AIEthics,
#Transhumanism
Books:
●      Life 3.0 by Max Tegmark - One of the more accessible
books on the list because Max starts with a fictional portrayal
of the rise of AGI and the evolution of the Singularity through
the story of the Omega Corporation. Covers pretty much every
aspect of AI, but a couple of the concepts in later chapters are
quite complex. #SocialMedia, #Privacy, #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon, #VR,
#ComputerVision, #GenerativeAI
●      AI 2041 by Kai-fu Lee and Chen Quifan - A really
engaging read with a series of short stories that cover pretty
much everything in this book. Each story is accompanied by a
chapter where Kai-fu Lee explains the AI concepts within the
story. #SocialMedia, #Privacy, #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon, #VR,
#ComputerVision, #GenerativeAI
●      Human Compatible by Stuart Russell - Probably a bit of a
heavier read than Life 3.0 or AI 2041, but there is no question
that Stuart is one of the leading thinkers on how we must
approach our AI-soaked future.  #SocialMedia, #Privacy,
#AIEthics, #AGI, #Anthropomorphism, #Transhumanism,
#Singularity, #Sustainability, #Work, #AI_as_a_Weapon
●      Superintelligence by Nick Bostrom - Up there with Stuart
Russell as one of the deepest thinkers on the topic.
#SocialMedia, #Privacy, #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon
●      Privacy is Power by Carissa Véliz - This is a very accessible
and extremely frightening exploration on how fragile our
privacy is in the digital age. #SocialMedia, #Privacy, #AIEthics

●      Genius Makers by Cade Metz - Less of a deep exploration
of the technology itself, but a really interesting read about the
genesis of companies like Google and the personalities behind
the rise of AI and Machine Learning. #AGI
●      Hello World by Hannah Fry - Accessible and interesting
read about the evolution of AI and many of the key topics
discussed in this book. #SocialMedia, #Privacy, #AIEthics,
#AGI, #Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work
●      A World Without Work by Daniel Susskind - Fairly
detailed exploration of the impact on society as a result of AI
and automation. #Work
●      Anything by Yuval Noah Harari - But particularly Homo
Deus and 21 Lessons for the 21st Century. These books
are heavyweight and very detailed, but provide really great
insight on some of the more philosophical aspects of
humankind’s relationship with technology. #Privacy, #AIEthics,
#AGI, #Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work
●      AI Ethics by Mark Coeckelbergh - Covers pretty much what
it says on the cover, but does so in an accessible and fairly
easy to read manner, even though the subject matter is
necessarily complex and thought provoking. #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #Privacy 
●      Weapons of Math Destruction by Cathy O’Neill - Written
by one of the collaborators on the documentary Coded Bias,
this book looks at the impact of bias, bad data and downright
stupidity on the algorithms being implemented to supposedly
improve services like policing and college admissions.
#AIEthics, #Privacy
●      Artificial Intelligence and You by Peter J Scott -
Companion to the wonderful podcast referenced below. This
book covers almost every aspect of AI and how it might, or
already does, impact our lives. #SocialMedia, #Privacy,
#AIEthics, #AGI, #Anthropomorphism, #Transhumanism,

#Singularity, #Sustainability, #Work, #AI_as_a_Weapon,
#VR, #ComputerVision, #GenerativeAI
Podcasts:
●      AI and You - Peter Scott’s long running one-on-one
discussions with experts from all areas of AI development and
application. #SocialMedia, #Privacy, #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon, #VR,
#ComputerVision, #GenerativeAI
●      Making Sense Podcast - Not exclusively about AI, but host
(and Ted Talk Royalty) Sam Harris is extremely engaging, and
the discussions he has with technology experts are incredibly
insightful. #SocialMedia, #Privacy, #AIEthics, #AGI,
#Anthropomorphism, #Transhumanism, #Singularity,
#Sustainability, #Work, #AI_as_a_Weapon, #VR,
#ComputerVision, #GenerativeAI
Video Game
●      Detroit: Becoming Human - I know this is a bit of a stretch,
but if you have a platform on which you can get a copy of this
game, and you are prepared to get lost (for a total of about 20
hours) in an incredibly immersive and interactive story about
the plight of two AGI Androids in a society where AGI is
considered somewhat subservient to human beings, I can’t
recommend this highly enough.  #AIEthics, #AGI,
#Anthropomorphism

 
About the Author
If, like me, you are the sort of person who
skips these bits in books, read the bit in the
grey box. The rest of you can fill your boots
with the details. 
 
Boy discovers computers.Older boy starts a career in Information
Technology, particularly data and analytics. Young man becomes a
data geek. Older man decides that, while there are plenty of
exceptionally well written and engaging books that tackle the risks
and benefits of Artificial Intelligence, there doesn't seem to be one
single volume that covers the major talking points in an accessible
form to raise awareness and get more conversations started on
what could be both the best and worst thing ever to happen to the
human race. Old man decides to write just such a volume.
 
Right, for the rest of you, here’s the longer version:
 
In 1981, when I was just 11 years old, I was lucky enough to get a
Sinclair ZX81 for my birthday. Armed with its 1KB of memory I was
soon writing rudimentary programs that printed blocky messages on
the screen. The unmetered joy of seeing “HELLO JAMES” on the
screen (I know - I was easily pleased) cemented my addiction; I
was a certified computer addict. My teen years revolved around
getting through school as quickly as possible so that I could get
back in front of that lo-res screen. The encouragement and support
of my parents, who were willing to invest in my addiction - first with
a 16KB RAM Pack for my ZX81, but then with the magical
Commodore 64 and it’s successor the Amiga, meant that by the
time I stumbled across the line of my A-Levels there was absolutely
no question about where my future career lay. I took a year out pre-

college to work for IBM and then immersed myself in a Business
Information Technology course at Bournemouth University. When I
was finished, the UK was in the middle of a recession and getting a
job was a bit of a struggle, so I lurched from one temping job to
another for 9 months before I secured a role with a subsidiary of RS
Components based in Hampshire. I was to be a Junior Programmer
writing in a language called MUMPS (Google it - it was a thing).
 
I loved it.
 
I spent my days working on order management workflows and my
evenings hunched over my Amiga trying vainly to write the next
blockbuster game. Then, after about 2 years, while I was working
on something for the product marketing team, I was presented with
a problem that involved generating reports from the data in the
order management system to support various purchasing and
product decisions. I was shown how the managers scraped data
from the screens in the MUMPS platform and pasted it into this
brand new spreadsheet-thing called Microsoft Excel. They then had
to religiously follow a typed out procedure list (effectively a rules-
based algorithm, explained a bit later) that documented a laborious
and lengthy series of keystrokes, which manipulated the data and, if
they didn’t make any mistakes, resulted in a simple ordered list of
buying priorities and a supporting pie chart. But the problem was
that it only worked about 50% of the time. Whether it was user
error, or unexpected fluctuations in the data volume or structure, it
just failed almost as often as it worked, and more worryingly
sometimes it appeared to have functioned correctly but the results
were misleading. My boss gave me a small budget and 3 months to
“see what I could do to make it more reliable”. What started out as
a 3 month project gained so much support with the business, as I
made incremental improvements to the reliability and performance
of the process, that it ended up consuming 2 years of my life, at the
end of which I had developed a fully automated, macro-driven
reporting suite that was rolled out across all of the company’s
international sites. I had officially become the company’s data guy,

delivering the first fully functional Business Intelligence Reporting
system they had ever seen. Plaudits and kudos came my way, and
from there my career path was irreversibly set in stone.
 
From those humble beginnings began a 30 year (so far) career that
has taken me across the globe, from role to role, industry to
industry, but always in a data related role. In the early years these
were development and project roles, but over time I followed the
well-trodden path into management and leadership. Along the way
I have witnessed the explosion of data available to decision
makers; millions of rows of call data records that could be mined for
customer behaviour and fraud analysis at a mobile phone network
provider; hundreds of millions of rows of patient trial data that could
be used to evaluate new and potentially life saving drug regimens
at a global pharmaceutical company. By 2017 I had landed at
Gartner, a globally renowned research and advisory company who
prided themselves on providing thought leadership to their
customers, steering their businesses towards best practice and, in
my case, getting the most business value possible out of the wealth
of data available to them. At any one time, my role involved
working with a portfolio of approximately 30 business leaders who
led the use of data within their organisation and I was covering
pretty much every sector you could imagine. To help them on this
journey I used the wealth of amazing research available within their
subscription, along with direct support from the Gartner Analysts
who authored it. Between mid-2017 and February 2022, when I left
to embark on a new adventure, I watched these customers
developing their data strategies and embracing new technologies
and practices to further the success of their companies. Front and
centre in most of this success was the implementation of Data
Science and Artificial Intelligence.
 
And, while all this was happening, Moore’s “law” was continuing to
apply. Computers were getting more powerful, faster, cheaper, more
accessible. This catalysed the acceleration in technological
innovation that has become pervasive throughout society (although,

as you may have read by now, not always for the better). Things
that once seemed like expensive and wondrous marvels, such as
satellite navigation or mobile internet, became inexpensive
commodities.
 
Then, in the world’s own Annus Horriblis; 2021, everything was
turned on its head. COVID-19 reared its oh-so-ugly head and
suddenly my regular business trips became video calls, and with this
adjustment I found myself with a great deal more spare time.
Although, like most others, I was initially seduced by the lure of
Netflix, it wasn’t long before I rediscovered books, both reading and
writing. Back before my life became so consumed with family,
children and all the welcome responsibilities that came with them, I
had flirted with writing a few novels, but it was always a hobby as
opposed to a career aspiration. But reading was always a favourite
passtime and, now I had a lot more travel and client-entertainment-
free evenings, I started to devour books again. While I do enjoy a
bit of fiction, I was drawn to the list of management, psychology
and personal development titles that had been building up on my
to-do list. What I discovered was a wealth of fascinating and
excellent titles by the likes of Simon Sinek, Yuval Noah Harari,
Stuart Russell, Max Tegmark and Kai-fu Lee, along with many other
excellent authors, many of whom I will refer to later in this book.
With few exceptions I found these books rewarding and
enlightening. Many of them covered the evolution and potential
impact of Artificial Intelligence, and I found myself regularly boring
my family and friends with the latest facts and concepts I had been
reading about - things I thought it was important for them to
consider, such as the impact of AI on the education and future
employment prospects of our children.
 
While I was doing this I realised something; almost without
exception I was having to paraphrase or simplify the messages from
the wonderful books I had been reading. This was, 100%
categorically, not because my captive audiences were in any way
intellectually inferior to me. No, the challenge was that these books

were written to be read by people with some business or industry
experience, or at least an inclination towards technology, beyond
the themes they saw in films like Terminator or Ex Machina. I
realised what was needed was something to bridge the gap
between the two; an accessible book that explained the major
themes related to the rise of Artificial Intelligence, its value (already
realised and potential for the future) and the related risks it could
introduce for humanity. An easy-ish read that wouldn’t require any
understanding of the concepts, while not being too patronising for
anyone to pick up and enjoy.
 
 
James



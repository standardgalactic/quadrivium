Machine Learning Specialization 
Scaling to Huge Datasets &  
Online Learning 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
2 
Data 
Why gradient ascent is slow… 
©2015-2016 Emily Fox & Carlos Guestrin 
w(t) 
Compute  
gradient 
Pass 
over data 
w(t+1) 
Update 
coeﬃcients 
w(t+2) 
Update 
coeﬃcients 
  Δ 
ℓ(w(t)) 
  Δ 
ℓ(w(t+1)) 
Every update requires  
a full pass over data 

Machine Learning Specialization 
3 
Data sets are getting huge, and we need them! 
©2015-2016 Emily Fox & Carlos Guestrin 
5B views/day 
500M Tweets/day 
WWW 
4.8B webpages 
300 hours  
uploaded/min 
1 B users, 
ad revenue 
Need ML algorithm to learn from  
billions of video views every day, &  
to recommend ads within milliseconds 
Internet of  
Things 
Sensors  
everywhere 

Machine Learning Specialization 
5 
ML improves (signiﬁcantly)  
with bigger datasets 
©2015-2016 Emily Fox & Carlos Guestrin 
1996 
2006 
2016 
Big data 
Simple 
•  Needed for  
speed 
Logistic regression 
Matrix factorization 
Bigger data 
Complex 
•  Needed for accuracy 
•  Parallelism, GPUs, 
computer clusters 
Boosted trees 
Tensor factorization 
Deep learning 
Massive graphical models 
“The Unreasonable Eﬀectiveness of Data”  
[Halevy, Norvig, Pereira ’09] 
Small data 
Data  
size 
Complex 
•  Needed for  
accuracy 
Model 
complexity 
Kernels 
Graphical models 
Hot 
topics 

Machine Learning Specialization 
7 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
This module: small 
change to algorithm, 
which often improves 
running time a lot 

Machine Learning Specialization 
8 
Data 
Stochastic gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
w(t) 
Compute  
gradient 
Use only 
 small subsets  
of data 
w(t+1) 
Update 
coeﬃcients 
w(t+2) 
Update 
coeﬃcients 
w(t+3) 
Update 
coeﬃcients 
w(t+4) 
Update 
coeﬃcients 
Many updates for  
each pass over data 

Machine Learning Specialization 
Learning, one data point at a time 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
11 
Gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) + η   ℓ(w(t))  
  Δ 

Machine Learning Specialization 
12 
How expensive is gradient ascent? 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
Contribution of  
data point xi,yi to gradient 
N
X
i=1
Sum over  
data points 
⇣
1[yi = +1] −P(y = +1 | xi, w)
⌘
hj(xi)
@`i(w)
@wj

Machine Learning Specialization 
13 
Every step requires touching  
every data point!!! 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
N
X
i=1
Sum over  
data points 
@`i(w)
@wj
Time to compute 
contribution of xi,yi 
# of data points (N) 
Total time to compute  
1 step of gradient ascent 
1 millisecond  
1000 
1 second 
1000 
1 millisecond  
10 million 
1 millisecond  
10 billion 
Time to compute 
contribution of xi,yi 
# of data points (N) 
Total time to compute  
1 step of gradient ascent 
1 millisecond  
1000 
1 second 
1000 
1 millisecond  
10 million 
Time to compute 
contribution of xi,yi 
# of data points (N) 
Total time to compute  
1 step of gradient ascent 
1 millisecond  
1000 
1 second 
1000 
Time to compute 
contribution of xi,yi 
# of data points (N) 
Total time to compute  
1 step of gradient ascent 
1 millisecond  
1000 

Machine Learning Specialization 
15 
Instead of all data points for gradient,  
use 1 data point only??? 
©2015-2016 Emily Fox & Carlos Guestrin 
Gradient  
ascent 
@`(w)
@wj
=
N
X
i=1
Sum over  
data points 
@`i(w)
@wj
Stochastic 
gradient  
ascent 
@`(w)
@wj
=
@`i(w)
@wj
≈ 
Each time, pick  
diﬀerent data point i 

Machine Learning Specialization 
Stochastic gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
18 
Stochastic gradient ascent 
for logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0, t=1 
until converged 
   
X
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘
for j=0,…,D 
 partial[j] = 
 
 wj
(t+1) ß wj
(t) + η partial[j] 
t ß t + 1 
 
for i=1,…,N 
N
X
i=1
Sum over  
data points 
Each time, pick  
diﬀerent data point i 

Machine Learning Specialization 
19 
Comparing computational time per step 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
N
X
i=1
@`i(w)
@wj
Time to compute 
contribution of xi,yi # of data points (N) 
Total time for  
1 step of  
gradient 
Total time for  
1 step of  
stochastic gradient 
1 millisecond  
1000 
1 second 
1 second 
1000 
16.7 minutes 
1 millisecond  
10 million 
2.8 hours 
1 millisecond  
10 billion 
115.7 days 
Gradient ascent 
@`(w)
@wj
=
@`i(w)
@wj
≈ 
Stochastic gradient ascent 

Machine Learning Specialization 
Comparing gradient to  
stochastic gradient 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
23 
Which one is better??? Depends… 
Algorithm 
Time per 
iteration 
Gradient 
Slow for 
large data 
Stochastic 
gradient 
Always  
fast 
©2015-2016 Emily Fox & Carlos Guestrin 
Algorithm 
Time per 
iteration 
In theory 
Gradient 
Slow for 
large data 
Slower 
Stochastic 
gradient 
Always  
fast 
Faster 
Algorithm 
Time per 
iteration 
In theory 
In practice 
Gradient 
Slow for 
large data 
Slower 
Often  
slower 
Stochastic 
gradient 
Always  
fast 
Faster 
Often  
faster 
Algorithm 
Time per 
iteration 
In theory 
In practice 
Sensitivity to 
parameters 
Gradient 
Slow for 
large data 
Slower 
Often  
slower 
Moderate 
Stochastic 
gradient 
Always  
fast 
Faster 
Often  
faster 
Very high 
Total time to  
convergence for large data 

Machine Learning Specialization 
25 
Avg. log likelihood 
Comparing gradient to stochastic gradient 
©2015-2016 Emily Fox & Carlos Guestrin 
Better 
Gradient 
Stochastic 
gradient 
Total time proportional to  
number of passes over data 
Stochastic gradient achieves higher  
likelihood sooner, but it’s noisier 

Machine Learning Specialization 
26 
Eventually, gradient catches up 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 
Gradient 
Stochastic 
gradient 
Note: should only trust “average” quality  
of stochastic gradient (more discussion later) 

Machine Learning Specialization 
27 
Summary of stochastic gradient 
Tiny change to gradient ascent 
Much better scalability 
Huge impact in real-world 
Very tricky to get right in practice 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Why would stochastic  
gradient ever work??? 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
30 
Gradient is direction of steepest ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
Gradient is “best” direction, but 
any direction that goes “up” would be useful 

Machine Learning Specialization 
31 
In ML, steepest direction is  
sum of “little directions”  
from each data point 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
N
X
i=1
Sum over  
data points 
@`i(w)
@wj
For most data points, 
contribution points “up”  

Machine Learning Specialization 
32 
Stochastic gradient: pick a  
data point and move in direction 
©2015-2016 Emily Fox & Carlos Guestrin 
Most of the time,  
total likelihood will increase 
@`(w)
@wj
=
@`i(w)
@wj
≈ 

Machine Learning Specialization 
33 
Stochastic gradient ascent: 
Most iterations increase likelihood,  
but sometimes decrease it è#
On average, make progress 
©2015-2016 Emily Fox & Carlos Guestrin 
until converged 
 for i=1,…,N 
 
 for j=0,…,D 
 
 wj
(t+1) ß wj
(t) + η 
 
 t ß t + 1   
@`i(w)
@wj

Machine Learning Specialization 
Convergence path 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
37 
Convergence paths 
©2015-2016 Emily Fox & Carlos Guestrin 
Gradient 
Stochastic 
gradient 

Machine Learning Specialization 
38 
Stochastic gradient convergence is “noisy” 
©2015-2016 Emily Fox & Carlos Guestrin 
Better 
Avg. log likelihood 
Gradient usually increases  
likelihood smoothly 
Stochastic gradient  
makes “noisy” progress 

Machine Learning Specialization 
40 
Summary of why stochastic gradient works 
Gradient ﬁnds direction of 
steeps ascent 
Gradient is sum of contributions 
from each data point 
Stochastic gradient uses 
direction from 1 data point 
On average increases likelihood, 
sometimes decreases 
Stochastic gradient has “noisy” 
convergence 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Stochastic gradient:  
practical tricks 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
43 
Stochastic gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0, t=1 
until converged 
 for i=1,…,N 
  
 for j=0,…,D 
  
 
 wj
(t+1) ß wj
(t) + η 
  
 t ß t + 1   
@`i(w)
@wj

Machine Learning Specialization 
44 
Order of data can introduce bias 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
Stochastic gradient  
updates parameters  
1 data point at a time 
Systematic order in data  can introduce signiﬁcant bias,  
e.g., all negative points ﬁrst, or temporal order, younger ﬁrst, or … 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 

Machine Learning Specialization 
45 
Shuﬄe data before running  
stochastic gradient! 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = 
#awesome 
x[2] = 
#awful 
y = 
sentiment 
1 
1 
+1 
3 
3 
-1 
0 
2 
-1 
4 
1 
+1 
2 
1 
+1 
2 
4 
-1 
0 
1 
-1 
0 
3 
-1 
2 
1 
+1 
x[1] = 
#awesome 
x[2] = 
#awful 
y = 
sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
Shuﬄe 
rows 

Machine Learning Specialization 
47 
Stochastic gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0, t=1 
until converged 
 for i=1,…,N 
  
 for j=0,…,D 
  
 
 wj
(t+1) ß wj
(t) + η 
  
 t ß t + 1   
@`i(w)
@wj
Shuﬄe data 
Before running  
stochastic gradient,  
make sure data is shuﬄed 

Machine Learning Specialization 
Choosing the step size η 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
49 
Picking step size  
for stochastic gradient  
is very similar to  
picking step size  
for gradient 
©2015-2016 Emily Fox & Carlos Guestrin 
But stochastic gradient  
is a lot more unstable… L 

Machine Learning Specialization 
51 
If step size is too small,  
stochastic gradient slow to converge 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 

Machine Learning Specialization 
52 
If step size is too large,  
stochastic gradient oscillates 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 

Machine Learning Specialization 
53 
If step size is very large,  
stochastic gradient goes crazy L 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 

Machine Learning Specialization 
54 
Simple rule of thumb for picking step size η  
similar to gradient 
•  Unfortunately, picking step size requires a lot a lot 
of trial and error, much worst than gradient L 
•  Try a several values, exponentially spaced 
- Goal: plot learning curves to 
•  ﬁnd one η that is too small 
•  ﬁnd one η that is too large 
•  Advanced tip: step size that decreases with iterations is 
very important for stochastic gradient, e.g., 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Don’t trust the last coeﬃcients… L 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
57 
Stochastic gradient never fully “converges” 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 
Gradient will  
eventually 
stabilize on  
a solution 
Stochastic gradient will eventually  
oscillate around a solution 

Machine Learning Specialization 
58 
The last coeﬃcients may be really good or really bad!! L 
©2015-2016 Emily Fox & Carlos Guestrin 
w(1000) was bad 
w(1005) was good 
How do we 
minimize risk of 
picking bad 
coeﬃcients 
Stochastic gradient will eventually  
oscillate around a solution 

Machine Learning Specialization 
59 
Stochastic gradient returns average coeﬃcients 
•  Minimize noise:  
don’t return last learned coeﬃcients 
•  Instead, output average: 
©2015-2016 Emily Fox & Carlos Guestrin 
ŵ =  1         w(t) 
         T 
T
X
t=1

Machine Learning Specialization 
Learning from batches of data 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
61 
Gradient/stochastic gradient:  
two extremes 
©2015-2016 Emily Fox & Carlos Guestrin 
Data 
Data 
Stochastic gradient: 
1 point/iteration 
Gradient: 
N points/iteration 
Mini-batches: 
B points/iteration  
Reduces noise,  
increases stability  

Machine Learning Specialization 
63 
Convergence paths 
©2015-2016 Emily Fox & Carlos Guestrin 
Stochastic gradient 
Batch size = 1 
Gradient 
Stochastic gradient 
Batch size = 25 

Machine Learning Specialization 
64 
Batch size eﬀect 
Avg. log likelihood 
Better 
Too large è Slow convergence 
Too small è Noisy,  
 
 
 may not converge 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
66 
Stochastic gradient ascent  
with mini-batches 
©2015-2016 Emily Fox & Carlos Guestrin 
Shuﬄe data 
init w(1)=0, t=1 
until converged 
  
  
 for j=0,…,D 
  
     wj
(t+1) ß wj
(t) + η 
  
 t ß t + 1   
@`i(w)
@wj
Sum over  
data points in  
mini-batch k 
(k+1)⇤B
X
i=1+k⇤B
For each  
mini-batch 
for k=0,…,N/B-1 

Machine Learning Specialization 
Measuring convergence 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
68 
Avg. log likelihood 
Better 
How did we make these plots??? 
Need to compute log likelihood of  
data at every iteration??? 
©2015-2016 Emily Fox & Carlos Guestrin 
 è Really really slow,  
product over all data points! 
 
 
 
`(w) = ln
N
Y
i=1
P(yi | xi, w)
Actually, plotting average  
log likelihood over  
last 30 mini-batches… 

Machine Learning Specialization 
70 
Computing log-likelihood during  
run of stochastic gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0, t=1 
until converged 
   
X
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘
for j=0,…,D 
 partial[j] = 
 
 wj
(t+1) ß wj
(t) + η partial[j] 
t ß t + 1 
for i=1,…,N 
Log-likelihood of data point i is simply: 
 
 
 
`i(w(t)) =
8
<
:
ln P(y = +1 | xi, w(t)),
if yi = +1
ln
$
1 −P(y = +1 | xi, w(t))
%
,
if yi = −1

Machine Learning Specialization 
72 
`i(w(t))
Estimate log-likelihood with sliding window 
©2015-2016 Emily Fox & Carlos Guestrin 
Iteration t 
Log-likelihood   
of single data point 
t=75 
Report average  
of last k values 
Inexpensive estimate of  
log-likelihood;  
useful to evaluate convergence 

Machine Learning Specialization 
73 
That’s what average log-likelihood meant… J 
(In this case, over last k=30 mini-batches, with batch-size B = 100) 
©2015-2016 Emily Fox & Carlos Guestrin 
Avg. log likelihood 
Better 

Machine Learning Specialization 
Adding regularization 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
75 
Consider speciﬁc total cost 
 
 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
  
 
 
            of coeﬃcients 
 
©2015 Emily Fox & Carlos Guestrin 
ℓ(w) 
(w) 
||w||2  
 
2 

Machine Learning Specialization 
76 
Gradient of L2 regularized log-likelihood 
 
 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
  
 
 
            of coeﬃcients 
 
©2015 Emily Fox & Carlos Guestrin 
Total  
derivative = 
λ ||w||2  
 
2 
- 2 λ wj 
ℓ(w) 
(w) 
N
X
i=1
@`i(w)
@wj

Machine Learning Specialization 
77 
Stochastic gradient for regularized objective 
•  What about regularization term? 
©2015-2016 Emily Fox & Carlos Guestrin 
Total  
derivative = 
- 2 λ wj 
N
X
i=1
@`i(w)
@wj
Stochastic 
gradient  
ascent 
@`i(w)
@wj
Each time, pick  
diﬀerent data point i 
≈ 
Total  
derivative 
- 2 λ wj
N 
Each data point contributes 
1/N to regularization 

Machine Learning Specialization 
78 
Stochastic gradient ascent with regularization 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0, t=1 
until converged 
 for i=1,…,N 
  for j=0,…,D 
  
 wj
(t+1) ß wj
(t) + η 
  t ß t + 1   
@`i(w)
@wj
Shuﬄe data 
- 2 λ wj
N 

Machine Learning Specialization 
Online learning:  
Fitting models from  
streaming data 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Batch vs online learning 
Batch learning 
•  All data is available at 
start of training time 
Online learning 
•  Data arrives (streams in) over time 
-  Must train model as data arrives! 
©2015-2016 Emily Fox & Carlos Guestrin 
Data 
ML  
algorithm 
time 
ŵ 
Data 
t=1 
Data 
t=2 
ML  
algorithm 
ŵ(1) 
Data 
t=3 
Data 
t=4 
ŵ(2) 
ŵ(3) 
ŵ(4) 

Machine Learning Specialization 
83 
Online learning example:  
Ad targeting 
©2015-2016 Emily Fox & Carlos Guestrin 
Website 
….......…....... 
….......…....... 
….......…....... 
….......….......  
….......…....... 
….......…....... 
….......…....... 
….......…....... 
Input:  xt 
User info, 
page text 
ML  
algorithm 
ŵ(t) 
Ad1 
Ad2 
Ad3 
ŷ = Suggested ads 
User clicked  
on Ad2 
ê 
yt=Ad2 
ŵ(t+1) 

Machine Learning Specialization 
84 
Online learning problem 
•  Data arrives over each time step t: 
- Observe input xt 
•  Info of user, text of webpage 
- Make a prediction ŷt 
•  Which ad to show 
- Observe true output yt 
•  Which ad user clicked on 
©2015-2016 Emily Fox & Carlos Guestrin 
Need ML algorithm to  
update coeﬃcients each time step! 

Machine Learning Specialization 
85 
Stochastic gradient ascent  
can be used for online learning!!! 
©2015-2016 Emily Fox & Carlos Guestrin 
•  init w(1)=0, t=1 
•  Each time step t: 
- Observe input xt 
- Make a prediction ŷt 
- Observe true output yt 
- Update coeﬃcients: 
for j=0,…,D 
     wj
(t+1) ß wj
(t) + η @`t(w)
@wj

Machine Learning Specialization 
86 
Summary of online learning 
Data arrives over time 
Must make a prediction every 
time new data point arrives 
Observe true class after 
prediction made 
Want to update parameters 
immediately 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
87 
Updating coeﬃcients immediately:  
Pros and Cons 
©2015-2016 Emily Fox & Carlos Guestrin 
Pros 
•  Model always up to date è  
  
  
 
 
 Often more accurate 
•  Lower computational cost 
•  Don’t need to store all data, but often do anyway  
Cons 
•  Overall system is *much* more complex 
-  Bad real-world cost in terms of $$$ to build & maintain  
Most companies opt for systems that save data  
and update coeﬃcients every night, or hour, week,… 

Machine Learning Specialization 
Summary of scaling to  
huge datasets & online learning 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
89 
Scaling through parallelism 
©2015-2016 Emily Fox & Carlos Guestrin 
Multicore 
processors 
Computer 
clusters 
Huge opportunity for scaling 
Require parallel & distributed  
ML algorithms 

Machine Learning Specialization 
90 
What you can do now… 
•  Signiﬁcantly speedup learning 
algorithm using stochastic gradient 
•  Describe intuition behind why 
stochastic gradient works 
•  Apply stochastic gradient in practice 
•  Describe online learning problems 
•  Relate stochastic gradient to online 
learning 
©2015-2016 Emily Fox & Carlos Guestrin 


Analysis and Simulation
of Chaotic Systems,
Second Edition
Frank C. Hoppensteadt
Springer





Acknowledgments
I thank all of the teachers and students whom I have encountered, and I
thank my parents, children, and pets for the many insights into life and
mathematics that they have given me‚Äîoften unsolicited. I have published
parts of this book in the context of research or expository papers done
with co-authors, and I thank them for the opportunity to have worked
with them. The work presented here was mostly derived by others, al-
though parts of it I was fortunate enough to uncover for the Ô¨Årst time. My
work has been supported by various agencies and institutions including the
University of Wisconsin, New York University and the Courant Institute of
Mathematical Sciences, the University of Utah, Michigan State University,
Arizona State University, the National Science Foundation, ARO, ONR,
and the AFOSR. This investment in me has been greatly appreciated, and
the work in this book describes some outcomes of that investment. I thank
these institutions for their support.
The preparation of this second edition was made possible through the
help of Linda Arneson and Tatyana Izhikevich. My thanks to them for their
help.

Contents
Acknowledgments
v
Introduction
xiii
1
Linear Systems
1
1.1
Examples of Linear Oscillators . . . . . . . . . . . . . . .
1
1.1.1
Voltage-Controlled Oscillators . . . . . . . . . . .
2
1.1.2
Filters
. . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.3
Pendulum with Variable Support Point . . . . . .
4
1.2
Time-Invariant Linear Systems . . . . . . . . . . . . . . .
5
1.2.1
Functions of Matrices . . . . . . . . . . . . . . . .
6
1.2.2
exp(At)
. . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3
Laplace Transforms of Linear Systems
. . . . . .
9
1.3
Forced Linear Systems with Constant CoeÔ¨Écients . . . .
10
1.4
Linear Systems with Periodic CoeÔ¨Écients . . . . . . . . .
12
1.4.1
Hill‚Äôs Equation
. . . . . . . . . . . . . . . . . . .
14
1.4.2
Mathieu‚Äôs Equation . . . . . . . . . . . . . . . . .
15
1.5
Fourier Methods . . . . . . . . . . . . . . . . . . . . . . .
18
1.5.1
Almost-Periodic Functions
. . . . . . . . . . . .
18
1.5.2
Linear Systems with Periodic Forcing . . . . . . .
21
1.5.3
Linear Systems with Quasiperiodic Forcing . . . .
22
1.6
Linear Systems with Variable CoeÔ¨Écients: Variation of
Constants Formula
. . . . . . . . . . . . . . . . . . . . .
23
1.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
24

viii
Contents
2
Dynamical Systems
27
2.1
Systems of Two Equations . . . . . . . . . . . . . . . . .
28
2.1.1
Linear Systems
. . . . . . . . . . . . . . . . . . .
28
2.1.2
Poincar¬¥e and Bendixson‚Äôs Theory . . . . . . . . .
29
2.1.3
x‚Ä≤‚Ä≤ + f(x)x‚Ä≤ + g(x) = 0 . . . . . . . . . . . . . . .
32
2.2
Angular Phase Equations . . . . . . . . . . . . . . . . . .
35
2.2.1
A Simple Clock: A Phase Equation on T 1
. . . .
37
2.2.2
A Toroidal Clock: Denjoy‚Äôs Theory . . . . . . . .
38
2.2.3
Systems of N (Angular) Phase Equations
. . . .
40
2.2.4
Equations on a Cylinder: PLL . . . . . . . . . . .
40
2.3
Conservative Systems . . . . . . . . . . . . . . . . . . . .
42
2.3.1
Lagrangian Mechanics
. . . . . . . . . . . . . . .
42
2.3.2
Plotting Phase Portraits Using Potential Energy .
43
2.3.3
Oscillation Period of x‚Ä≤‚Ä≤ + Ux(x) = 0
. . . . . . .
46
2.3.4
Active Transmission Line . . . . . . . . . . . . . .
47
2.3.5
Phase-Amplitude (Angle-Action) Coordinates . .
49
2.3.6
Conservative Systems with N Degrees of Freedom
52
2.3.7
Hamilton‚ÄìJacobi Theory . . . . . . . . . . . . . .
53
2.3.8
Liouville‚Äôs Theorem . . . . . . . . . . . . . . . . .
56
2.4
Dissipative Systems . . . . . . . . . . . . . . . . . . . . .
57
2.4.1
van der Pol‚Äôs Equation . . . . . . . . . . . . . . .
57
2.4.2
Phase Locked Loop . . . . . . . . . . . . . . . . .
57
2.4.3
Gradient Systems and the Cusp Catastrophe . . .
62
2.5
Stroboscopic Methods . . . . . . . . . . . . . . . . . . . .
65
2.5.1
Chaotic Interval Mappings . . . . . . . . . . . . .
66
2.5.2
Circle Mappings . . . . . . . . . . . . . . . . . . .
71
2.5.3
Annulus Mappings
. . . . . . . . . . . . . . . . .
74
2.5.4
Hadamard‚Äôs Mappings of the Plane . . . . . . . .
75
2.6
Oscillations of Equations with a Time Delay . . . . . . .
78
2.6.1
Linear Spline Approximations . . . . . . . . . . .
80
2.6.2
Special Periodic Solutions
. . . . . . . . . . . . .
81
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3
Stability Methods for Nonlinear Systems
91
3.1
Desirable Stability Properties of Nonlinear Systems . . .
92
3.2
Linear Stability Theorem . . . . . . . . . . . . . . . . . .
94
3.2.1
Gronwall‚Äôs Inequality . . . . . . . . . . . . . . . .
95
3.2.2
Proof of the Linear Stability Theorem
. . . . . .
96
3.2.3
Stable and Unstable Manifolds . . . . . . . . . . .
97
3.3
Liapunov‚Äôs Stability Theory . . . . . . . . . . . . . . . .
99
3.3.1
Liapunov‚Äôs Functions . . . . . . . . . . . . . . . .
99
3.3.2
UAS of Time-Invariant Systems . . . . . . . . . .
100
3.3.3
Gradient Systems . . . . . . . . . . . . . . . . . .
101
3.3.4
Linear Time-Varying Systems . . . . . . . . . . .
102
3.3.5
Stable Invariant Sets . . . . . . . . . . . . . . . .
103

Contents
ix
3.4
Stability Under Persistent Disturbances . . . . . . . . . .
106
3.5
Orbital Stability of Free Oscillations
. . . . . . . . . . .
108
3.5.1
DeÔ¨Ånitions of Orbital Stability . . . . . . . . . . .
109
3.5.2
Examples of Orbital Stability
. . . . . . . . . . .
110
3.5.3
Orbital Stability Under Persistent Disturbances .
111
3.5.4
Poincar¬¥e‚Äôs Return Mapping
. . . . . . . . . . . .
111
3.6
Angular Phase Stability
. . . . . . . . . . . . . . . . . .
114
3.6.1
Rotation Vector Method . . . . . . . . . . . . . .
114
3.6.2
Huygen‚Äôs Problem . . . . . . . . . . . . . . . . . .
116
3.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
4
Bifurcation and Topological Methods
121
4.1
Implicit Function Theorems
. . . . . . . . . . . . . . . .
121
4.1.1
Fredholm‚Äôs Alternative for Linear Problems
. . .
122
4.1.2
Nonlinear Problems: The Invertible Case . . . . .
126
4.1.3
Nonlinear Problems: The Noninvertible Case . . .
128
4.2
Solving Some Bifurcation Equations . . . . . . . . . . . .
129
4.2.1
q = 1: Newton‚Äôs Polygons
. . . . . . . . . . . . .
130
4.3
Examples of Bifurcations . . . . . . . . . . . . . . . . . .
132
4.3.1
Exchange of Stabilities . . . . . . . . . . . . . . .
132
4.3.2
Andronov‚ÄìHopf Bifurcation
. . . . . . . . . . . .
133
4.3.3
Saddle-Node on Limit Cycle Bifurcation
. . . . .
134
4.3.4
Cusp Bifurcation Revisited . . . . . . . . . . . . .
134
4.3.5
Canonical Models and Bifurcations . . . . . . . .
135
4.4
Fixed-Point Theorems
. . . . . . . . . . . . . . . . . . .
136
4.4.1
Contraction Mapping Principle
. . . . . . . . . .
136
4.4.2
Wazewski‚Äôs Method . . . . . . . . . . . . . . . . .
138
4.4.3
Sperner‚Äôs Method . . . . . . . . . . . . . . . . . .
141
4.4.4
Measure-Preserving Mappings . . . . . . . . . . .
142
4.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5
Regular Perturbation Methods
145
5.1
Perturbation Expansions . . . . . . . . . . . . . . . . . .
147
5.1.1
Gauge Functions: The Story of o, O . . . . . . . .
147
5.1.2
Taylor‚Äôs Formula
. . . . . . . . . . . . . . . . . .
148
5.1.3
Pad¬¥e‚Äôs Approximations
. . . . . . . . . . . . . .
148
5.1.4
Laplace‚Äôs Methods
. . . . . . . . . . . . . . . . .
150
5.2
Regular Perturbations of Initial Value Problems . . . . .
152
5.2.1
Regular Perturbation Theorem
. . . . . . . . . .
152
5.2.2
Proof of the Regular Perturbation Theorem
. . .
153
5.2.3
Example of the Regular Perturbation Theorem
.
155
5.2.4
Regular Perturbations for 0 ‚â§t < ‚àû. . . . . . .
155
5.3
ModiÔ¨Åed Perturbation Methods for Static States . . . . .
157
5.3.1
Nondegenerate Static-State Problems Revisited .
158
5.3.2
ModiÔ¨Åed Perturbation Theorem . . . . . . . . . .
158

x
Contents
5.3.3
Example: q = 1 . . . . . . . . . . . . . . . . . . .
160
5.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6
Iterations and Perturbations
163
6.1
Resonance . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.1.1
Formal Perturbation Expansion of Forced Oscilla-
tions . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.1.2
Nonresonant Forcing . . . . . . . . . . . . . . . .
167
6.1.3
Resonant Forcing . . . . . . . . . . . . . . . . . .
170
6.1.4
ModiÔ¨Åed Perturbation Method for Forced Oscilla-
tions . . . . . . . . . . . . . . . . . . . . . . . . .
172
6.1.5
JustiÔ¨Åcation of the ModiÔ¨Åed Perturbation Method
173
6.2
DuÔ¨Éng‚Äôs Equation
. . . . . . . . . . . . . . . . . . . . .
174
6.2.1
ModiÔ¨Åed Perturbation Method . . . . . . . . . . .
175
6.2.2
DuÔ¨Éng‚Äôs Iterative Method . . . . . . . . . . . . .
176
6.2.3
Poincar¬¥e‚ÄìLinstedt Method . . . . . . . . . . . . .
177
6.2.4
Frequency-Response Surface . . . . . . . . . . . .
178
6.2.5
Subharmonic Responses of DuÔ¨Éng‚Äôs Equation . .
179
6.2.6
Damped DuÔ¨Éng‚Äôs Equation . . . . . . . . . . . .
181
6.2.7
DuÔ¨Éng‚Äôs Equation with Subresonant Forcing
. .
182
6.2.8
Computer Simulation of DuÔ¨Éng‚Äôs Equation
. . .
184
6.3
Boundaries of Basins of Attraction
. . . . . . . . . . . .
186
6.3.1
Newton‚Äôs Method and Chaos . . . . . . . . . . . .
187
6.3.2
Computer Examples
. . . . . . . . . . . . . . . .
188
6.3.3
Fractal Measures
. . . . . . . . . . . . . . . . . .
190
6.3.4
Simulation of Fractal Curves . . . . . . . . . . . .
191
6.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
7
Methods of Averaging
195
7.1
Averaging Nonlinear Systems
. . . . . . . . . . . . . . .
199
7.1.1
The Nonlinear Averaging Theorem . . . . . . . .
200
7.1.2
Averaging Theorem for Mean-Stable Systems
. .
202
7.1.3
A Two-Time Scale Method for the Full Problem .
203
7.2
Highly Oscillatory Linear Systems . . . . . . . . . . . . .
204
7.2.1
dx/dt = ŒµB(t)x . . . . . . . . . . . . . . . . . . .
205
7.2.2
Linear Feedback System . . . . . . . . . . . . . .
206
7.2.3
Averaging and Laplace‚Äôs Method
. . . . . . . . .
207
7.3
Averaging Rapidly Oscillating DiÔ¨Äerence Equations . . .
207
7.3.1
Linear DiÔ¨Äerence Schemes . . . . . . . . . . . . .
210
7.4
Almost Harmonic Systems . . . . . . . . . . . . . . . . .
214
7.4.1
Phase-Amplitude Coordinates . . . . . . . . . . .
215
7.4.2
Free Oscillations . . . . . . . . . . . . . . . . . . .
216
7.4.3
Conservative Systems . . . . . . . . . . . . . . . .
219
7.5
Angular Phase Equations . . . . . . . . . . . . . . . . . .
223
7.5.1
Rotation Vector Method . . . . . . . . . . . . . .
224

Contents
xi
7.5.2
Rotation Numbers and Period Doubling Bifurca-
tions . . . . . . . . . . . . . . . . . . . . . . . . .
227
7.5.3
Euler‚Äôs Forward Method for Numerical Simulation
227
7.5.4
Computer Simulation of Rotation Vectors
. . . .
229
7.5.5
Near Identity Flows on S1 √ó S1 . . . . . . . . . .
231
7.5.6
KAM Theory
. . . . . . . . . . . . . . . . . . . .
233
7.6
Homogenization . . . . . . . . . . . . . . . . . . . . . . .
234
7.7
Computational Aspects of Averaging . . . . . . . . . . .
235
7.7.1
Direct Calculation of Averages . . . . . . . . . . .
236
7.7.2
Extrapolation . . . . . . . . . . . . . . . . . . . .
237
7.8
Averaging Systems with Random Noise . . . . . . . . . .
238
7.8.1
Axioms of Probability Theory . . . . . . . . . . .
238
7.8.2
Random Perturbations . . . . . . . . . . . . . . .
241
7.8.3
Example of a Randomly Perturbed System . . . .
242
7.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
8
Quasistatic-State Approximations
249
8.1
Some Geometrical Examples of Singular Perturbation
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
8.2
Quasistatic-State Analysis of a Linear Problem
. . . . .
257
8.2.1
Quasistatic Problem
. . . . . . . . . . . . . . . .
258
8.2.2
Initial Transient Problem . . . . . . . . . . . . . .
261
8.2.3
Composite Solution . . . . . . . . . . . . . . . . .
263
8.2.4
Volterra Integral Operators with Kernels Near Œ¥ .
264
8.3
Quasistatic-State Approximation for Nonlinear Initial Value
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
8.3.1
Quasistatic Manifolds . . . . . . . . . . . . . . . .
265
8.3.2
Matched Asymptotic Expansions
. . . . . . . . .
268
8.3.3
Construction of QSSA
. . . . . . . . . . . . . . .
270
8.3.4
The Case T = ‚àû. . . . . . . . . . . . . . . . . .
271
8.4
Singular Perturbations of Oscillations . . . . . . . . . . .
273
8.4.1
Quasistatic Oscillations . . . . . . . . . . . . . . .
274
8.4.2
Nearly Discontinuous Oscillations . . . . . . . . .
279
8.5
Boundary Value Problems . . . . . . . . . . . . . . . . .
281
8.6
Nonlinear Stability Analysis near Bifurcations . . . . . .
284
8.6.1
Bifurcating Static States . . . . . . . . . . . . . .
284
8.6.2
Nonlinear Stability Analysis of Nonlinear Oscilla-
tions . . . . . . . . . . . . . . . . . . . . . . . . .
287
8.7
Explosion Mode Analysis of Rapid Chemical Reactions .
289
8.8
Computational Schemes Based on QSSA . . . . . . . . .
292
8.8.1
Direct Calculation of x0(h), y0(h) . . . . . . . . .
293
8.8.2
Extrapolation Method
. . . . . . . . . . . . . . .
294
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Supplementary Exercises
301

xii
Contents
References
303
Index
311

Introduction
This book describes aspects of mathematical modeling, analysis, computer
simulation, and visualization that are widely used in the mathematical
sciences and engineering.
Scientists often use ordinary language models to describe observations
of physical and biological phenomena. These are precise where data are
known and appropriately imprecise otherwise. Ordinary language modelers
carve away chunks of the unknown as they collect more data. On the other
hand, mathematical modelers formulate minimal models that produce re-
sults similar to what is observed. This is the Ockham‚Äôs razor approach,
where simpler is better, with the caution from Einstein that ‚ÄúEverything
should be made as simple as possible, but not simpler.‚Äù
The success of mathematical models is diÔ¨Écult to explain. The same
tractable mathematical model describes such diverse phenomena as when
an epidemic will occur in a population or when chemical reactants will
begin an explosive chain-branched reaction, and another model describes
the motion of pendulums, the dynamics of cryogenic electronic devices, and
the dynamics of muscle contractions during childbirth.
Ordinary language models are necessary for the accumulation of experi-
mental knowledge, and mathematical models organize this information, test
logical consistency, predict numerical outcomes, and identify mechanisms
and parameters that characterize them.
Often mathematical models are quite complicated, but simple approx-
imations can be used to extract important information from them. For
example, the mechanisms of enzyme reactions are complex, but they can
be described by a single diÔ¨Äerential equation (the Michaelis‚ÄìMenten equa-

xiv
Introduction
tion [14]) that identiÔ¨Åes two useful parameters (the saturation constant and
uptake velocity) that are used to characterize reactions. So this modeling
and analysis identiÔ¨Åes what are the critical data to collect. Another example
is Semenov‚Äôs theory of explosion limits [128], in which a single diÔ¨Äerential
equation can be extracted from over twenty chemical rate equations model-
ing chain-branched reactions to describe threshold combinations of pressure
and temperature that will result in an explosion.
Mathematical analysis includes geometrical forms, such as hyperbolic
structures, phase planes, and isoclines, and analytical methods that derive
from calculus and involve iterations, perturbations, and integral transforms.
Geometrical methods are elegant and help us visualize dynamical processes,
but analytical methods can deal with a broader range of problems, for ex-
ample, those including random perturbations and forcing over unbounded
time horizons. Analytical methods enable us to calculate precisely how
solutions depend on data in the model.
As humans, we occupy regions in space and time that are between very
small and very large and very slow and very fast. These intermediate
space and time scales are perceptible to us, but mathematical analysis
has helped us to perceive scales that are beyond our senses. For example,
it is very diÔ¨Écult to ‚Äúunderstand‚Äù electric and magnetic Ô¨Åelds. Instead,
our intuition is based on solutions to Maxwell‚Äôs equations. Fluid Ô¨Çows are
quite complicated and usually not accessible to experimental observations,
but our knowledge is shaped by the solutions of the Navier‚ÄìStokes equa-
tions. We can combine these multiple time and space scales together with
mathematical methods to unravel such complex dynamics. While realistic
mathematical models of physical or biological phenomena can be highly
complicated, there are mathematical methods that extract simpliÔ¨Åcations
to highlight and elucidate the underlying process. In some cases, engineers
use these representations to design novel and useful things.
We also live with varying levels of logical rigor in the mathematical
sciences that range from complete detailed proofs in sharply deÔ¨Åned math-
ematical structures to using mathematics to probe other structures where
its validity is not known.
The mathematical methods presented and used here grew from several
diÔ¨Äerent scientiÔ¨Åc sources. Work of Newton and Leibniz was partly rigor-
ous and partly speculative. The G¬®ottingen school of Gauss, Klein, Hilbert,
and Courant was carried forward in the U.S. by Fritz John, James Stoker,
and Kurt Friedrichs, and they and their students developed many impor-
tant ideas that reached beyond rigorous diÔ¨Äerential equation models and
studied important problems in continuum mechanics and wave propaga-
tion. Russian and Ukrainian workers led by Liapunov, Bogoliubov, Krylov,
and Kolmogorov developed novel approaches to problems of bifurcation
and stability theory, statistical physics, random processes, and celestial
mechanics. Fourier‚Äôs and Poincar¬¥e‚Äôs work on mathematical physics and dy-
namical systems continues to provide new directions for us, and the U.S.

Introduction
xv
mathematicians G. D. BirkhoÔ¨Äand N. Wiener and their students have con-
tributed to these topics as well. Analytical and geometrical perturbation
and iteration methods were important to all of this work, and all involved
diÔ¨Äerent levels of rigor.
Computer simulations have enabled us to study models beyond the reach
of mathematical analysis. For example, mathematical methods can provide
a language for modeling and some information, such as existence, unique-
ness, and stability, about their solutions. And then well executed computer
algorithms and visualizations provide further qualitative and quantitative
information about solutions. The computer simulations presented here de-
scribe and illustrate several critical computer experiments that produced
important and interesting results.
Analysis and computer simulations of mathematical models are im-
portant parts of understanding physical and biological phenomena. The
knowledge created in modeling, analysis, simulation, and visualization
contributes to revealing the secrets they embody.
The Ô¨Årst two chapters present background material for later topics in
the book, and they are not intended to be complete presentations of Linear
Systems (Chapter 1) and Dynamical Systems (Chapter 2). There are many
excellent texts and research monographs dealing with these topics in great
detail, and the reader is referred to them for rigorous developments and
interesting applications. In fact, to keep this book to a reasonable size
while still covering the wide variety of topics presented here, detailed proofs
are not usually given, except in cases where there are minimal notational
investments and the proofs give readily accessible insight into the meaning
of the theorem. For example, I see no reason to present the details of proofs
for the Implicit Function Theorem or for the main results of Liapunov‚Äôs
stability theory. Still, these results are central to this book. On the other
hand, the complete proofs of some results, like the Averaging Theorem for
DiÔ¨Äerence Equations, are presented in detail.
The remaining chapters of this book present a variety of mathematical
methods for solving problems that are sorted by behavior (e.g., bifurca-
tion, stability, resonance, rapid oscillations, and fast transients). However,
interwoven throughout the book are topics that reappear in many diÔ¨Äer-
ent, often surprising, incarnations. For example, the cusp singularity and
the property of stability under persistent disturbances arise often. The
following list describes cross-cutting mathematical topics in this book.
1. Perturbations. Even the words used here cause some problems. For
example, perturb means to throw into confusion, but its purpose here is
to relate to a simpler situation. While the perturbed problem is confused,
the unperturbed problem should be understandable. Perturbations usually
involve the identiÔ¨Åcation of parameters, which unfortunately is often mis-
understood by students to be perimeters from their studies of geometry.
Done right, parameters should be dimensionless numbers that result from
the model, such as ratios of eigenvalues of linear problems. Parameter iden-

xvi
Introduction
tiÔ¨Åcation in problems might involve diÔ¨Écult mathematical preprocessing in
applications. However, once this is done, basic perturbation methods can
be used to understand the perturbed problem in terms of solutions to the
unperturbed problem. Basic perturbation methods used here are Taylor‚Äôs
method for approximating a smooth function by a polynomial and Laplace‚Äôs
method for the approximation of integral formulas. These lead to the im-
plicit function theorem and variants of it, and to matching, averaging, and
central-limit theorems. Adaptations of these methods to various other prob-
lems are described here. Two particularly useful perturbation methods are
the method of averaging and the quasistatic-state approximation. These
are dealt with in detail in Chapters 7 and 8, respectively.
2. Iterations. Iterations are mathematical procedures that begin with a
state vector and change it according to some rule. The same rule is applied
to the new state, and so on, and a sequence of iterates of the rule results.
Fra Fibonacci in 1202 introduced a famous iteration that describes the
dynamics of an age-structured population. In Fibonacci‚Äôs case, a population
was studied, geometric growth was deduced, and the results were used to
describe the compounding of interest on investments.
Several iterations are studied here. First, Newton‚Äôs method, which con-
tinues to be the paradigm for iteration methods, is studied. Next, we study
DuÔ¨Éng‚Äôs iterative method and compare the results with similar ones de-
rived using perturbation methods. Finally, we study chaotic behavior that
often occurs when quite simple functions are iterated. There has been a
controversy of sorts between iterationists and perturbationists; each has its
advocates and each approach is useful.
3. Chaos. The term was introduced in its present connotation by Yorke
and Li in 1976 [101, 48]. It is not a precisely deÔ¨Åned concept, but it occurs in
various physical and religious settings. For example, Boltzmann used it in a
sense that eventually resulted in ergodic theories for dynamical systems and
random processes, and Poincar¬¥e had a clear image of the chaotic behavior
of dynamical systems that occurs when stable and unstable manifolds cross.
The book of Genesis begins with chaos, and philosophical discussions about
it and randomness continue to this day. For the most part, the word chaos
is used here to indicate behavior of solutions to mathematical models that
is highly irregular and usually unexpected. We study several problems that
are known to exhibit chaotic behavior and present methods for uncovering
and describing this behavior. Related to chaotic systems are the following:
a. Almost periodic functions and generalized Fourier analysis [11, 140].
b. Poincar¬¥e‚Äôs stroboscopic mappings, which are based on snapshots of
a solution at Ô¨Åxed time intervals‚Äî‚ÄúChaos, illumined by Ô¨Çashes of
lightning‚Äù [from Oscar Wilde in another context] [111].

Introduction
xvii
c. Fractals, which are space Ô¨Ålling curves that have been studied since
Weierstrass, HausdorÔ¨Ä, Richardson, and Peano a century ago and
more recently by Mandelbrodt [107].
d. Catastrophes, which were introduced by Ren¬¥e Thom [133] in the
1960s.
e. Fluid turbulence that occurs in convective instabilities described by
Lorenz and Keller [104].
f. Irregular ecological dynamics studied by Ricker and May [48].
g. Random processes, including the Law of Large Numbers and ergodic
and other limit theorems [82].
These and many other useful and interesting aspects of chaos are described
here.
4. Oscillations. Oscillators play fundamental roles in our lives‚Äî
‚Äúdiscontented pendulums that we are‚Äù [R.W. Emerson]. For example, most
of the cells in our bodies live an oscillatory life in an oscillating chemical
environment. The study of pendulums gives great insight into oscillators,
and we focus a signiÔ¨Åcant eÔ¨Äort here in studying pendulums and similar
physical and electronic devices.
One of the most interesting aspects of oscillators is their tendency to syn-
chronize with other nearby oscillators. This had been observed by musicians
dating back at least to the time of Aristotle, and eventually it was addressed
as a mathematical problem by Huygens in the 17th century and Korteweg
around 1900 [142]. This phenomenon is referred to as phase locking, and
it now serves as a fundamental ingredient in the design of communications
and computer-timing circuits. Phase locking is studied here for a variety of
diÔ¨Äerent oscillator populations using the rotation vector method. For ex-
ample, using the VCON model of a nerve cell, we model neural networks as
being Ô¨Çows on high-dimensional tori. Phase locking occurs when the Ô¨Çow
reduces to a knot on the torus for the original and all nearby systems.
5. Stability. The stability of physical systems is often described using
energy methods. These methods have been adapted to more general dy-
namical systems by Liapunov and others. Although we do study linear and
Liapunov stability properties of systems here, the most important stability
concept used here is that of stability under persistent disturbances. This
idea explains why mathematical results obtained for minimal models can
often describe behavior of systems that are operating in noisy environ-
ments. For example, think of a metal bowl having a lowest point in it. A
marble placed in the bowl will eventually move to the minimum point. If
the bowl is now dented with many small craters or if small holes are put
in it, the marble will still move to near where the minimum of the original
bowl had been, and the degree of closeness can be determined from the
size of the dents and holes. The dents and the holes introduce irregular

xviii
Introduction
disturbances to the system, but the dynamics of the marble are similar in
both the simple (ideal) bowl and the imperfect (realized) bowl.
Stability under persistent disturbances is sometimes confused with struc-
tural stability. The two are quite diÔ¨Äerent. Structural stability is a concept
introduced to describe systems whose behavior does not change when the
system is slightly perturbed. Hyperbolic structures are particularly im-
portant examples of this. However, it is the changes in behavior when a
system is slightly perturbed that are often the only things observable in ex-
periments: Did something change? Stability under persistent disturbances
carries through such changes. For example, the diÔ¨Äerential equation
Àôx = ax ‚àíx3 + Œµf(t),
where f is bounded and integrable, Œµ is small, and a is another parameter,
occurs in many models. When Œµ = 0 and a increases through the value
a = 0, the structure of static state solutions changes dramatically: For
a < 0, there is only one (real) static state, x = 0; but for a > 0 there
are three: x = ¬±‚àöa are stable static states, and x = 0 is an unstable
one. This problem is important in applications, but it is not structurally
stable at a = 0. Still, there is a Liapunov function for a neighborhood of
x = 0, a = 0, Œµ = 0, namely, V (x) = x2. So, the system is stable under
persistent disturbances. Stability under persistent disturbances is based on
results of Liapunov, Malkin, and Massera that we study here.
6. Computer simulation. The two major topics studied in this book are
mathematical analysis and computer simulation of mathematical models.
Each has its uses, its strengths, and its deÔ¨Åciencies. Our mathematical anal-
ysis builds mostly on perturbation and iteration methods: They are often
diÔ¨Écult to use, but once they are understood, they can provide information
about systems that is not otherwise available. Understanding them for the
examples presented here also lays a basis for one to use computer packages
such as Mathematica, Matlab or Maple to construct perturbation expan-
sions. Analytical methods can explain regular behavior of noisy systems,
they can simplify complicated systems with Ô¨Ådelity to real behavior, and
they can go beyond the edges of practical computability in dealing with
fast processes (e.g., rapid chemical reactions) and small quantities (e.g.,
trace-element calculations).
Computer simulation replaces much of the work formerly done by mathe-
maticians (often as graduate students), and sophisticated software packages
are increasing simulation power. Simulations illustrate solutions of a math-
ematical model by describing a sample trajectory, or sample path, of the
process. Sample paths can be processed in a variety of ways‚Äîplotting, cal-
culating ensemble statistics, and so on. Simulations do not describe the
dependence of solutions on model parameters, nor are their stability, ac-
curacy, or reliability always assured. They do not deal well with chaotics
or unexpected catastrophes‚Äîirregular or unexpected rapid changes in a
solution‚Äîand it is usually diÔ¨Écult to determine when chaos lurks nearby.

Introduction
xix
Mathematical analysis makes possible computer simulations; conversely,
computer simulations can help with mathematical analysis. New computer-
based methods are being derived with parallelization of computations,
simpliÔ¨Åcation of models through automatic preprocessing, and so on, and
the future holds great promise for combined work of mathematical and
computer-based analysis. There have been many successes to date, for
example the discovery and analysis of solitons.
The material in this book is not presented in order of increasing diÔ¨Éculty.
The Ô¨Årst two chapters provide background information for the last six chap-
ters, where oscillation, iteration, and perturbation techniques and examples
are developed. We begin with three examples that are useful throughout
the rest of the book. These are electrical circuits and pendulums. Next,
we describe linear systems and spectral decomposition methods for solv-
ing them. These involve Ô¨Ånding eigenvalues of matrices and deducing how
they are involved in the solution of a problem. In the second chapter we
study dynamical systems, beginning with descriptions of how periodic or
almost periodic solutions can be found in nonlinear dynamical systems
using methods ranging from Poincar¬¥e and Bendixson‚Äôs method for two dif-
ferential equations to entropy methods for nonlinear iterations. The third
chapter presents stability methods for studying nonlinear systems. Partic-
ularly important for later work is the method of stability under persistent
disturbances.
The remainder of the book deals with methods of approximation and
simulation. First, some useful algebraic and topological methods are de-
scribed, followed by a study of implicit function theorems and modiÔ¨Åcations
and generalizations of them. These are applied to several bifurcation prob-
lems. Then, regular perturbation problems are studied, in which a small
parameter is identiÔ¨Åed and the solutions are constructed directly using the
parameter. This is illustrated by several important problems in nonlinear
oscillations, including DuÔ¨Éng‚Äôs equation and nonlinear resonance.
In Chapter 7 the method of averaging is presented. This is one of the most
interesting techniques in all of mathematics. It is closely related to Fourier
analysis, to the Law of Large Numbers in probability theory, and to the
dynamics of physical and biological systems in oscillatory environments.
We describe here multitime methods, Bogoliubov‚Äôs transformation, and
integrable systems methods.
Finally, the method of quasistatic-state approximations is presented.
This method has been around in various useful forms since 1900, and it
has been called by a variety of names‚Äîthe method of matched asymptotic
expansions being among the most civil. It has been derived in some quite
complicated ways and in some quite simple ones. The approach taken here
is of quasistatic manifolds, which has a clear geometric Ô¨Çavor that can aid
intuition. It combines the geometric approach of Hadamard with the an-

xx
Introduction
alytical methods of Perron to construct stable and unstable manifolds for
systems that might involve irregular external forcing.
In rough terms, averaging applies when a system involves rapid oscilla-
tions that are slowly modulated, and quasistatic-state approximations are
used when solutions decay rapidly to a manifold on which motions are
slower. When problems arise where both kinds of behavior occur, they can
often be unraveled. But there are many important problems where neither
of these methods apply, including diÔ¨Äraction by crossed wires in electro-
magnetic theory, stagnation points in Ô¨Çuid Ô¨Çows, Ô¨Çows in domains with
sharp corners, and problems with intermittent rapid time scales.
I have taught courses based on this book in a variety of ways depend-
ing on the time available and the background of the students. When the
material is taught as a full year course for graduate students in mathe-
matics and engineering, I cover the whole book. Other times I have taken
more advanced students who have had a good course in ordinary diÔ¨Äerential
equations directly to Chapters 4, 5, 6, 7, and 8. A one quarter course is pos-
sible using, for example, Chapters 1, 7, and 8. For the most part Chapters 1
and 2 are intended as background material for the later chapters, although
they contain some important computer simulations that I like to cover in
all of my presentations of this material. A course in computer simulations
could deal with sections from Chapters 2, 4, 7, and 8. The exercises also
contain several simulations that have been interesting and useful.
The exercises are graded roughly in increasing diÔ¨Éculty in each chapter.
Some are quite straightforward illustrations of material in the text, and
others are quite lengthy projects requiring extensive mathematical analysis
or computer simulation. I have tried to warn readers about more diÔ¨Écult
problems with an asterisk where appropriate.
Students must have some degree of familiarity with methods of ordinary
diÔ¨Äerential equations, for example, from a course based on Coddington
and Levinson [24], Hale [58], or Hirsch and Smale [68]. They should also be
competent with matrix methods and be able to use a reference text such as
Gantmacher [46]. Some familiarity with Interpretation of Dreams [45] has
also been found to be useful by some students.
Frank C. Hoppensteadt
Paradise Valley, Arizona
June 1999

1
Linear Systems
A linear system of ordinary diÔ¨Äerential equations has the form
dx
dt = A(t)x + f(t).
Given an N-dimensional vector f and an N √ó N matrix A(t) of functions
of t, we seek a solution vector x(t). We write x, f ‚ààEN and A ‚ààEN√óN
and sometimes x‚Ä≤ = dx/dt or Àôx = dx/dt.
Many design methods in engineering are based on linear systems. Also,
most of the methods used to study nonlinear problems grew out of methods
for linear problems, so mastery of linear problems is essential for under-
standing nonlinear ones. Section 1.1 presents several examples of physical
systems that are analyzed in this book. In Sections 1.2 and 1.3 we study
linear systems where A is a matrix of constants. In Sections 1.4 and 1.5
we study systems where A is a periodic or almost-periodic matrix, and in
Section 1.6 we consider general linear systems.
1.1
Examples of Linear Oscillators
The following examples illustrate typical problems in oscillations and per-
turbations, and they are referred to throughout this book. The Ô¨Årst two
examples describe electrical circuits and the third a mechanical system.

2
1.
Linear Systems
 V(x)
 V in
 VCO
Figure 1.1. A voltage-controlled oscillator. The controlling voltage Vin is applied
to the circuit, and the output has a Ô¨Åxed periodic waveform (V ) whose phase x
is modulated by Vin.
1.1.1
Voltage-Controlled Oscillators
Modern integrated circuit technology has had a surprising impact on math-
ematical models. Rather than the models becoming more complicated
as the number of transistors on a chip increases, the mathematics in
many cases has become dramatically simpler, usually by design. Voltage-
controlled oscillators (VCOs) illustrate this nicely. A VCO is an electronic
device that puts out a voltage in a Ô¨Åxed waveform, say V , but with a
variable phase x that is controlled by an input voltage Vin. The device is
described by the circuit diagram in Figure 1.1. The voltages in this and
other Ô¨Ågures are measured relative to a common ground that is not shown.
The output waveform V might be a Ô¨Åxed period square wave, a triangular
wave, or a sinusoid, but its phase x is the unknown. VCOs are made up of
many transistors, and a detailed model of the circuit is quite complicated
[83, 65]. However, there is a simple input‚Äìoutput relation for this device:
The input voltage Vin directly modulates the output phase as described by
the equation
dx
dt = œâ + Vin,
where the constant œâ is called the center frequency. The center frequency
is sustained by a separate (Ô¨Åxed supply) voltage in the device, and it can
be changed by tuning resistances in the VCO. Thus, a simple diÔ¨Äerential
equation models this device. The solution for x is found by integrating this
equation:
x(t) = x(0) + œât +
 t
0
Vin(s)ds.
The voltage V (x) is observable in this circuit, and the higher the input
voltage or the center frequency is, the faster V will oscillate.
Equations like this one for x play a central role in the theory of nonlinear
oscillations. In fact, a primary goal is often to transform a given system
into phase-and-amplitude coordinates, which is usually diÔ¨Écult to carry
out. This model is given in terms of phase and serves as an example of how
systems are studied once they are in phase and amplitude variables.

1.1. Examples of Linear Oscillators
3
 V in
 V
 L
 R
 C
 I
 Ground
Figure 1.2. An RLC circuit.
1.1.2
Filters
Filters are electrical circuits composed of resistors, inductors, and capaci-
tors. Figure 1.2 shows an RLC circuit, in which Vin, R, L, and C are given,
and the unknowns are the output voltage (V ) and the current (I) through
the circuit. The circuit is described by the mathematical equation
C dV
dt
=
I
LdI
dt + RI
=
Vin ‚àíV.
The Ô¨Årst equation describes the accumulation of charge in the capacitor;
the second relates the voltage across the inductor and the voltage across the
resistor (Ohm‚Äôs Law) to the total voltage Vin ‚àíV . Using the Ô¨Årst equation
to eliminate I from the model results in a single second-order equation:
LC d2V
dt2 + RC dV
dt + V = Vin.
Thus, the Ô¨Årst-order system for V and I can be rewritten as a second-order
equation for the scalar V .
The RLC circuit is an example of a Ô¨Ålter. In general, Ô¨Ålters are circuits
whose models have the form
an
dnV
dtn + an‚àí1
dn‚àí1V
dtn‚àí1 + ¬∑ ¬∑ ¬∑ + a0V = bm
dmW
dtm + ¬∑ ¬∑ ¬∑ + b0W,
where W is the input voltage, V is the output voltage, and the constants
{ai} and {bi} characterize various circuit elements. Once W is given, this
equation must be solved for V .
Filters can be described in a concise form: Using the notation p = d/dt,
sometimes referred to as Heaviside‚Äôs operator, we can write the Ô¨Ålter
equation as
V = H(p)W,
where the function H is a rational function of p:
H = (bmpm + ¬∑ ¬∑ ¬∑ + b0)
(anpn + ¬∑ ¬∑ ¬∑ + a0) .

4
1.
Linear Systems
This notation is made precise later using Laplace transforms, but for now
it is taken to be a shorthand notation for the input‚Äìoutput relation of the
Ô¨Ålter. The function H is called the Ô¨Ålter‚Äôs transfer function.
In summary, Ô¨Ålters are circuits whose models are linear nth-order ordi-
nary diÔ¨Äerential equations. They can be written concisely using the transfer
function notation, and they provide many examples later in this book.
1.1.3
Pendulum with Variable Support Point
Simple pendulums are described by equations that appear in a surprising
number of diÔ¨Äerent applications in physics and biology. Consider a pen-
dulum of length L supporting a mass m that is suspended from a point
with vertical coordinate V (t) and horizontal coordinate H(t) as shown in
Figure 1.3. The action integral for this mechanical system is deÔ¨Åned by
 b
a
mL2
2
dx
dt
2
‚àí
d2V
dt2 + g

mL(1 ‚àícos x) ‚àíd2H
dt2 mL sin x

dt,
where g is the acceleration of gravity. Hamilton‚Äôs principle [28] shows that
an extremum of this integral is attained by the solution x(t) of the equation
Ld2x
dt2 +
d2V
dt2 + g

sin x + d2H
dt2 cos x = 0,
which is the Euler‚ÄìLagrange equation for functions x(t) that make the
action integral stationary.
Furthermore, a pendulum in a resistive medium to which a torque is
applied at the support point is described by
Ld2x
dt2 + f dx
dt +
d2V
dt2 + g

sin x + d2H
dt2 cos x = I,
where f is the coeÔ¨Écient of friction and I is the applied torque.
For x near zero, sin x ‚âàx and cos x ‚âà1, so the equation is approximately
linear in x:
Ld2x
dt2 + f dx
dt +
d2V
dt2 + g

x = I ‚àíd2H
dt2 .
This linear equation for x(t), whose coeÔ¨Écients vary with t, involves many
diÔ¨Écult problems that must be solved to understand the motion of a pen-
dulum. Many of the methods used in the theory of nonlinear oscillations
grew out of studies of such pendulum problems; they are applicable now to
a wide variety of new problems in physics and biology.

1.2. Time-Invariant Linear Systems
5
 H(t)
 V(t)
 L
 m
 x
Figure 1.3. A pendulum with a moving support point. (H(t), V (t)) gives the
location of the support point at time t. The pendulum is a massless rod of length
L suspending a mass m, and x measures the angular deÔ¨Çection of the pendulum
from rest (down).
1.2
Time-Invariant Linear Systems
Systems of linear, time-invariant diÔ¨Äerential equations can be studied in
detail. Suppose that the vector of functions x(t) ‚ààEN satisÔ¨Åes the system
of diÔ¨Äerential equations
dx
dt = Ax
for a ‚â§t ‚â§b, where A ‚ààEN√óN is a matrix of constants.
Systems of this kind occur in many ways. For example, time-invariant
linear nth-order diÔ¨Äerential equations can be rewritten in the form of Ô¨Årst-
order systems of equations: Suppose that y(t) is a scalar function that
satisÔ¨Åes the linear equation
any(n) + ¬∑ ¬∑ ¬∑ + a0y = 0.
Let us set x1 = y, x2 = y(1), . . . , xn = y(n‚àí1), and
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
1
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
1
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
...
...
0
0
0
0
¬∑ ¬∑ ¬∑
0
1
b0
b1
b2
b3
¬∑ ¬∑ ¬∑
bn‚àí2
bn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª

6
1.
Linear Systems
where b0 = ‚àía0/an, . . . , bn‚àí1 = ‚àían‚àí1/an. A matrix in this form is called
a companion matrix [40]. With this, the vector x satisÔ¨Åes the diÔ¨Äerential
equation
dx
dt = Ax.
If this were a scalar equation (i.e., n = 1), the solution would be x(t) =
exp(At)x(0), where x(0) is given. We show next that this formula also
deÔ¨Ånes a solution when A is any constant matrix.
1.2.1
Functions of Matrices
Let g(z) be an analytic function of z in some set S of the complex plane.
Suppose that g has a convergent power series expansion
g(z) =
‚àû

n=0
cnzn
for z in S. If A is an N √óN matrix, then we can make sense of the function
g(A) by deÔ¨Åning it to be the power series
g(A) =
‚àû

n=0
cnAn.
This converges absolutely if the series of scalars  |cn||A|n converges,
where |c| denotes the modulus of a complex number c and |A| denotes
the Euclidean norm of the matrix A [34]:
|A| =

i

j
|ai,j|2.
Here ai,j is the component in the ith row and jth column of A.
If A is a diagonalizable matrix, then it can be written in terms of its
spectral decomposition:
A =
N

j=1
ŒªjPj,
where Œª1, . . . , Œªn are the eigenvalues of A and P1, . . . , Pn are projection
matrices, which satisfy the conditions PiPj = Pi if i = j and PiPj = 0
otherwise. Because of this, we see that for any integer m,
Am =
N

j=1
Œªm
j Pj.

1.2. Time-Invariant Linear Systems
7
In addition, we have
g(A) =
‚àû

n=0
cnAn =
N

j=1
‚àû

n=0
cnŒªn
j Pj =
N

j=1
g(Œªj)Pj,
provided that each eigenvalue Œªj lies in the domain where g is analytic.
Note that the spectral decomposition enables us to calculate functions
of A in terms of powers of the scalars {Œªj}, rather than powers of A. The
result is that once the eigenvalues and their projection matrices are found,
eÔ¨Äort in calculating functions of A is greatly reduced.
However, not every matrix can be diagonalized. The most that can be
said is that any matrix A can be put into Jordan canonical form. That is,
there is a block diagonal matrix J and a transforming matrix T such that
T ‚àí1AT = J.
The blocks on the main diagonal of J have the form
ŒªkIk + Zk =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œªk
1
0
0
0
0
0
Œªk
1
¬∑ ¬∑ ¬∑
0
0
0
0
0
Œªk
0
0
0
...
...
...
0
0
0
Œªk
1
0
0
0
0
¬∑ ¬∑ ¬∑
0
Œªk
1
0
0
0
0
0
Œªk
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
where Ik is an identity matrix of dimension k for k = 1, . . . , K, where K
is the number of blocks and Z is a matrix of zeros except for some ones on
the superdiagonal (where Zi,j = Œ¥i+1,j). Since ZN = 0, Z is referred to as
being a nilpotent matrix. There may also be a diagonalizable term on the
main diagonal of J (see [46]).
1.2.2
exp(At)
Since exp(t) is an entire function, exp(At) can be deÔ¨Åned by the series
exp(At) =
‚àû

n=0
(At)n
n!
,
which converges for all (real and complex) numbers t and for any matrix
A. We see directly from this series that
d exp(At)
dt
= A exp(At).
Therefore, this series deÔ¨Ånes a solution of the diÔ¨Äerential equation
dx
dt = Ax.

8
1.
Linear Systems
What does exp(At) look like? If A is diagonalizable, then we can interpret
exp(At) easily; namely,
exp(At) =
N

j=1
exp(Œªjt)Pj,
so that exp(At) is simply a linear combination of matrices whose coeÔ¨Écients
are exponentials of eigenvalues of A. In general, the series expansion of
exp(At) shows that
exp(At) = exp(TJT ‚àí1t) = T exp(Jt)T ‚àí1.
Moreover, the exponential of a block diagonal matrix is again a block di-
agonal matrix having blocks of the same dimensions. Thus, it is suÔ¨Écient
to consider a typical irreducible block matrix:
exp[(ŒªI + Z)t].
This has the form
exp(Œªt)

I + tZ + t2Z2
2
+ ¬∑ ¬∑ ¬∑ + tk‚àí1Zk‚àí1
(k ‚àí1)!

,
where k is the dimension of this block. Therefore, for any matrix A, the
function exp(At) is a linear combination of matrices whose coeÔ¨Écients are
polynomials in t multiplied by exponentials of eigenvalues of A.
Finally, we note that the spectrum of a matrix A can be split into three
parts: those eigenvalues having negative real parts (S), those having pos-
itive real parts (U), and those that are purely imaginary (O). If A is
diagonalizable, then it can be transformed into a block diagonal matrix
having three blocks: The Ô¨Årst consists of those eigenvalues in S, the second
of those in O, and the third of those in U. Therefore, we can write
exp(At) =

j in S
exp(Œªjt)Pj +

j in O
exp(Œªjt)Pj +

j in U
exp(Œªjt)Pj.
The Ô¨Årst sum approaches zero as t ‚Üí‚àû, the second one oscillates, and the
third one grows as t ‚Üí‚àû. Any solution of the system
dx
dt = Ax
can therefore be written in the form
x(t) =

j in S
exp(Œªjt)Pjx(0) +

j in O
exp(Œªjt)Pjx(0) +

j in U
exp(Œªjt)Pjx(0).
In the Ô¨Årst sum, x(0) is said to excite stable modes; in the second, oscilla-
tory modes; and in the third, unstable modes. Thus, the matrix A deÔ¨Ånes
a partition of the entire space EN into three parts: a stable manifold that
is deÔ¨Åned by the span of the projection matrices Pj for j ‚ààS, an unstable

1.2. Time-Invariant Linear Systems
9
manifold deÔ¨Åned by the span of the matrices Pj for j ‚ààU, and an oscil-
latory, or center manifold, that is spanned by the matrices Pj for j ‚ààO.
We shall see later that this decomposition carries over to certain nonlinear
systems.
1.2.3
Laplace Transforms of Linear Systems
Suppose that g(t) is a vector of smooth functions that grow no faster than
an exponential as t ‚Üí‚àû. We deÔ¨Åne the Laplace transform of g to be
g‚àó(p) =
 ‚àû
0
exp(‚àípt)g(t) dt,
where p is a complex number called the transform variable. Because of our
assumption on g, the integrand is dominated by a decreasing exponential
if Re(p) is suÔ¨Éciently large, where Re(p) denotes the real part of p. Note
that
dg
dt
‚àó
= pg‚àó‚àíg(0).
Therefore, we see that Laplace‚Äôs transform converts diÔ¨Äerentiation into
multiplication, and it justiÔ¨Åes using p as the Heaviside operator described
in Section 1.1.2.
How does one recover g from its transform? If G(p) is a function that is
analytic in a region except for pole singularities, then we deÔ¨Åne
g(t) =
1
2œÄi

C
exp(pt)G(p) dp,
where C is a curve lying in the region and enclosing all of the singularities
of G. With g deÔ¨Åned in this way, g‚àó(p) = G(p). This formula for g is the
Laplace inversion formula, and it shows how to recover the original function
from its transform.
Calculation of the inverse formula uses the method of residues, which is
based on Cauchy‚Äôs formula: If F(z) is a function that is analytic in some
region containing a point z0 and if C is a curve lying in this region and
enclosing z0, then we have the formula
F(z0) =
1
2œÄi

C
F(z)
(z ‚àíz0) dz.
This is referred to as the Cauchy integral formula, and the method of
residues is based on it. For example, if G(p) = 1/(p ‚àía), then
g(t) =
1
2œÄi

C
exp(pt)
(p ‚àía) dp = exp(at)
if C encloses the point z = a.

10
1.
Linear Systems
A low-pass Ô¨Ålter is described in Section 1.1.2 (with inductance L = 0).
Using the notation of that section, we have
V = H(p)W,
where H(p) = (RCp + 1)‚àí1. This formula should be interpreted as one for
the transforms of V and W:
V ‚àó= H(p)W ‚àó,
and so V (t) is the inverse Laplace transform of the function H(p)W ‚àó(p).
It follows that
V (t) = exp

‚àít
RC

V (0) +
 t
0
exp

‚àí(t ‚àís)
RC

W(s) ds
RC .
Finally, we note that another useful representation of the matrix exp(At)
can be found using Laplace transforms. Namely, we can deÔ¨Åne
exp(At) =
1
2œÄi

C
(pI ‚àíA)‚àí1 exp(pt)dp.
This formula is proved by reducing A to its Jordan canonical form and
applying Cauchy‚Äôs formula to each term in the matrix, as shown in the
next section.
1.3
Forced Linear Systems with Constant
CoeÔ¨Écients
Now let us consider a system of the form
dx
dt = Ax + f(t).
This describes a linear system to which a forcing function f is applied. We
suppose that f grows no faster than an exponential. Taking the Laplace
transform of this equation gives
(pI ‚àíA)x‚àó(p) = f ‚àó(p),
or
x‚àó(p) = (pI ‚àíA)‚àí1f ‚àó(p),
where this inverse matrix is deÔ¨Åned except when p is an eigenvalue of A.
Since x‚àóis the product of two transforms, x(t) is a convolution product;
that is, there is a matrix h(t) such that
x(t) =
 t
0
h(t ‚àís)f(s)ds,
where h‚àó(p) = (pI ‚àíA)‚àí1.

1.3. Forced Linear Systems with Constant CoeÔ¨Écients
11
What is h? If A is diagonalizable, then we can use its spectral
decomposition to evaluate h. Since
A =
N

j=1
ŒªjPj,
we have that
(pI ‚àíA)‚àí1 =
N

j=1
1
(p ‚àíŒªj)Pj.
Therefore,
h(t) =
N

j=1
1
2œÄi

C
exp(pt)
(p ‚àíŒªj)dpPj =
N

j=1
eŒªjtPj = exp(At).
This motivates the following observation: For any matrix A, we have the
function
y(t) =
 t
0
exp[A(t ‚àís)]f(s)ds,
which satisÔ¨Åes
dy
dt ‚àíAy = f(t).
This formula for y gives a particular solution of the equation, and the
general solution has the form
x(t) = y(t) + z(t),
where z solves the homogeneous problem
dz
dt ‚àíAz = 0
and
z(0) = x(0),
and so
z(t) = exp(At)x(0).
In summary, if x(t) solves the equation
dx
dt = Ax + f(t),
then
x(t) = exp(At)x(0) +
 t
0
exp[A(t ‚àís)]f(s) ds.
This is the variation of constants formula for x(t), and it might or might
not be useful depending on how easily the matrix exp(At) can be evaluated.
The transfer function notation for this is
x(t) = (pI ‚àíA)‚àí1f(t),

12
1.
Linear Systems
so we see that the transfer function notation summarizes a great deal of
work. The general variation of constants formula is described in Section 1.6.
1.4
Linear Systems with Periodic CoeÔ¨Écients
Consider the linear system
dx
dt = A(t)x,
where A is now an N √ó N matrix of continuous periodic functions, say
A(t + T) = A(t) for all t, ‚àí‚àû< t < ‚àû, where T is a period of A.
We say that Œ¶(t) is a fundamental matrix for this system if
dŒ¶
dt = A(t)Œ¶
and
Œ¶(0) is nonsingular.
Note that if d(t) denotes the determinant of Œ¶(t), then
dd
dt = tr[A(t)]d
and
d(0) Ã∏= 0
[24]. Therefore,
d(t) = exp
  t
0
tr[A(s)]ds

d(0),
so d(t) Ã∏= 0 for all t where tr(A) = N
k=1 Ak,k is the trace of A. It follows
that Œ¶(t) is nonsingular for all t, and therefore the columns of Œ¶(t) deÔ¨Åne
a set of N linearly independent solutions of the system.
The following theorem is very useful for studying periodic systems.
Floquet‚Äôs Theorem.Let Œ¶ be as described above. Then there is a
periodic matrix P(t), having period T, and a constant matrix R such that
Œ¶(t) = P(t) exp(Rt).
Proof of Floquet‚Äôs Theorem. If Œ¶(t) is a fundamental matrix of the problem,
then so is Œ¶(t + T). Moreover, calculation shows that
d
dtŒ¶‚àí1 = ‚àíŒ¶‚àí1(t)Œ¶‚Ä≤(t)Œ¶‚àí1(t).
Therefore,
d
dt[Œ¶‚àí1(t)Œ¶(t + T)] = 0.
It follows that there is a constant nonsingular matrix, namely,
C = Œ¶‚àí1(0)Œ¶(t),

1.4. Linear Systems with Periodic CoeÔ¨Écients
13
such that
Œ¶(t + T) = Œ¶(t)C
for all t.
This is the key observation on which further developments are based. In
particular, it follows from this formula that for any integer n,
Œ¶(nT) = Œ¶(0)Cn.
Therefore, the long-term behavior of solutions can be determined from the
eigenvalues of the matrix C. If we can deÔ¨Åne a matrix R by the formula
R = log(C)
T
,
so that exp(RT) = C, then the matrix
P(t) = Œ¶(t) exp(‚àíRt)
satisÔ¨Åes the identity
P(t + T) = P(t)
for all t. This follows because for all t,
P(t + T) = Œ¶(t + T) exp(‚àíRT) exp(‚àíRt) = Œ¶(t) exp(‚àíRt) = P(t).
The logarithm of C is well-deÔ¨Åned (as shown in [24]), although it might be
a matrix of complex numbers.
This result is especially helpful in determining the behavior of x(t) as
t ‚Üí‚àû. For example, if all of the eigenvalues of R have negative real parts,
then x(t) ‚Üí0 as t ‚Üí‚àû. However, this theorem is diÔ¨Écult to apply, since
it is usually diÔ¨Écult to Ô¨Ånd the matrix R.
An interesting consequence of Floquet‚Äôs Theorem is that any periodic
system can be transformed into one having constant coeÔ¨Écients. In fact,
the change of variables x = P(t)y takes the problem
dx
dt = A(t)x
into the linear system
dy
dt = Ry.
A very useful example is the system
d
dt

x1
x2

=
 cos œât
sin œât
‚àísin œât
cos œât

B
 cos œât
‚àísin œât
sin œât
cos œât
  x1
x2

,
where B is any 2 √ó 2 matrix of constants. Using Floquet‚Äôs transformation

y1
y2

=

cos œât
‚àísin œât
sin œât
cos œât
 
x1
x2


14
1.
Linear Systems
we convert this system into
d
dt

y1
y2

=

B + œâ

0
‚àí1
1
0
 
y1
y2

.
Thus, in this case, Floquet‚Äôs transformation is easy to carry out, and R =
B + œâJ, where J is Jacobi‚Äôs matrix:
J =

0
‚àí1
1
0

.
1.4.1
Hill‚Äôs Equation
The equation
d2x
dt2 + p(t)x = 0,
where p is a continuous periodic function, is known as Hill‚Äôs equation.
This equation arises frequently in mathematical physics, for example,
Schr¬®odinger‚Äôs equation in quantum mechanics and studies of the stability
of periodic solutions by linearization often have this form (see [28, 105]).
Let y1 denote the solution of this equation that satisÔ¨Åes the initial con-
ditions y1(0) = 1 and y‚Ä≤
1(0) = 0, and let y2 denote the solution of this
equation that satisÔ¨Åes y2(0) = 0, y‚Ä≤
2(0) = 1. Then, if we set x‚Ä≤ = y, we can
rewrite Hill‚Äôs equation as a Ô¨Årst-order system:
dx
dt
=
y
dy
dt
=
‚àíp(t)x,
and the matrix
Œ¶(t) =
 y1(t)
y2(t)
y‚Ä≤
1(t)
y‚Ä≤
2(t)

deÔ¨Ånes a fundamental solution. The matrix Œ¶ is called the Wronskian ma-
trix for this system. In fact, each column of this matrix solves the system,
and Œ¶(0) = I.
Suppose that the period of p is T, so p(t + T) = p(t) for all t. Floquet‚Äôs
Theorem shows that Œ¶(T) = exp(RT). The eigenvalues of this matrix
are called characteristic multipliers, and if they have modulus equal to
one, then all solutions of Hill‚Äôs equation are bounded as t ‚Üí¬±‚àû. The
eigenvalues of RT are called the characteristic exponents of the problem.
On the other hand, a great deal has been determined about the eigen-
values of R for Hill‚Äôs equation. For example, the eigenvalues of Œ¶(T) are
determined from the characteristic equation
Œª2 ‚àí[y1(T) + y‚Ä≤
2(T)]Œª + det Œ¶(T) = 0,

1.4. Linear Systems with Periodic CoeÔ¨Écients
15
and they have the form
Œª = ‚àÜ¬±

‚àÜ2 ‚àídet[Œ¶(T)],
where 2‚àÜ
=
tr[Œ¶(T)], the trace of Œ¶(T), and det[Œ¶(T)] denotes
its determinant. Note that 2‚àÜ
=
y1(T) + y‚Ä≤
2(T), and det[Œ¶(T)]
=
exp
 T
0 tr[A(s)]ds

= 1.
Therefore, if ‚àÜcan be evaluated, then the nature of the eigenvalues of
Œ¶(T) can be determined. In particular, if |‚àÜ| < 1, then the eigenvalues of
Œ¶(T) are complex conjugates and have modulus 1. In this case, all solutions
of Hill‚Äôs equation are bounded on the entire interval ‚àí‚àû< t < ‚àû. On the
other hand, if ‚àÜ> 1, both roots have positive real parts, and if ‚àÜ< ‚àí1,
then both eigenvalues have negative real parts. In either of these two cases,
no solution of Hill‚Äôs equation remains bounded on the whole interval ‚àí‚àû<
t < ‚àû(see [105]). Finally, if ‚àÜ= 1, then there is (at least) one solution
having period T, and if ‚àÜ= ‚àí1, then there is (at least) one solution having
period 2T. In either case, the other solution can grow no faster than O(t)
as t ‚Üí‚àû.
Hill‚Äôs equation with damping has the form
d2x
dt2 + rdx
dt + p(t)x = 0,
where r > 0. Setting x = exp(‚àírt/2)y gives
d2y
dt2 +

p(t) ‚àí
r
2
2
y = 0.
This is of the form just considered, and we see that if ‚àÜ< 1 for this
equation, then y remains bounded for 0 ‚â§t < +‚àû, and consequently
x ‚Üí0 as t ‚Üí+‚àû.
Pr¬®ufer introduced polar coordinates to Hill‚Äôs equation setting y = dx/dt
and dy/dt = ‚àíp(t)x. Then x = r cos Œ∏ and y = r sin Œ∏. The result is
dr
dt
=
r[1 ‚àíp(t)] cos Œ∏ sin Œ∏
dŒ∏
dt
=
‚àíp(t) cos2 Œ∏ ‚àísin2 Œ∏.
Note that the angular variable Œ∏ is separated from the amplitude variable in
this case! Thus, Hill‚Äôs equation is easily put into phase‚Äìamplitude variables,
which we study further in Section 2.3.5, and the problem reduces to study
of the Ô¨Årst-order diÔ¨Äerential equation for Œ∏.
1.4.2
Mathieu‚Äôs Equation
More can be said about the special case of Hill‚Äôs equation when p(t) =
Œ¥ + Œµ cos t, where Œ¥ and Œµ are constants. The result is known as Mathieu‚Äôs
equation, and its solutions are either bounded or unbounded, as for Hill‚Äôs

16
1.
Linear Systems
Œµ
Œ¥
|   |>1
|   |<1
‚àÜ
‚àÜ
Figure 1.4. Stability diagram for Mathieu‚Äôs equation. If (Œ¥, Œµ) lies in one of the
labeled regions, then |‚àÜ| < 1 and all solutions of Mathieu‚Äôs equation are bounded.
Note that if Œ¥ = 0 and Œµ = 0, then x(t) = at + b for some constants a and b.
equation. Figure 1.4 shows the values of Œ¥ and Œµ for which solutions are
bounded.
Meissner introduced a practice problem, where the term cos t is replaced
by a 2œÄ-periodic square wave q(t), where
q(t) =

1
for 0 ‚â§t < œÄ,
‚àí1
for œÄ ‚â§t < 2œÄ.
There is an interesting application of this to the pendulum model de-
scribed in Section 1.1.3. Let H = 0 and V = A cos œât. Then linearizing
the pendulum equation about x = 0 and replacing t by œât gives
d2x
dt2 +
 g
Lœâ2

‚àíA
L cos t

x = 0,
and linearizing about x = œÄ gives
d2x
dt2 ‚àí
 g
Lœâ2

‚àíA
L cos t

x = 0.
We see that if Œ¥ = ¬±(g/Lœâ2) and Œµ = ¬±A/L lie in the overlap region
in Figure 1.5, then both equilibria are stable. Thus, if the support point

1.4. Linear Systems with Periodic CoeÔ¨Écients
17
Œµ
Œ¥
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xx
xx
xx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
Overlap
Regions
Figure 1.5. Overlap regions. If the data are in the doubly shaded regions, then
both the up and down positions of the pendulum are stable. In this case, a small
perturbation of the pendulum from the straight up position persists, but the
pendulum remains nearly straight up for all future times.
is vibrated vertically with a range of frequencies and amplitudes so that
(g/Lœâ2) and A/L lie in the doubly shaded region in Figure 1.5, then both
the up and the down positions of the pendulum are stable.
If the pendulum is damped, then the linear equations are
d2x
dt2 + rdx
dt ¬±
 g
Lœâ2

‚àíA
L cos t

x = 0,
respectively. Setting x = exp(‚àírt/2)y in each of these equations gives
d2y
dt2 ¬±
 g
Lœâ2

‚àíA
L cos t ‚àì
r
2
2
y = 0,
respectively. In these cases, we have that
Œ¥ = (g/Lœâ2) ‚àí(r/2)2
and
Œ¥ = ‚àí(g/Lœâ2) ‚àí(r/2)2,
respectively.
Figure 1.5 shows the stability diagram for these two oscillators drawn
together on the same coordinate axes when r = 0. Figure 1.6 shows the
same result for small r. In this case small perturbations of the pendulum
from the straight up position die out, approximately like exp(‚àírt/2). This

18
1.
Linear Systems
Œµ
Œ¥
xx
xx
xx
Overlap
Regions
r
-r
Figure 1.6. The stability diagram of the up and down positions when damping
is accounted for (r > 0). In this case, small perturbations of the pendulum from
the straight up position die out, approximately like exp(‚àírt/2).
shows that an oscillating environment can stabilize a static state that is
unstable without oscillation. This is an important phenomenon found in
many physical and biological systems [1, 87, 127].
1.5
Fourier Methods
No result comparable to Floquet‚Äôs Theorem is available for systems having
almost periodic coeÔ¨Écients. However, when these systems do arise in ap-
plications, they can be studied using generalized Fourier methods. Some of
these methods are described in this section.
1.5.1
Almost-Periodic Functions
Almost-periodic functions play important roles in studies of nonlinear os-
cillators. An almost-periodic function, say f(t), is one that comes close to
being periodic in the following sense: For a given tolerance Œµ there is a
number TŒµ, called a translation number, such that in any interval of length

1.5. Fourier Methods
19
TŒµ there is a number T ‚Ä≤ for which
|f(t + T ‚Ä≤) ‚àíf(t)| < Œµ
for all t. The translation number of a periodic function is simply its period.
References [11] and [140] present introductions to almost-periodic functions
and their properties.
If f is almost-periodic, then it has a generalized Fourier expansion:
f(t) ‚àº
‚àû

n=1
Cn exp(itŒªn),
where the amplitudes {Cn} (complex) and the frequencies {Œªn} (real) char-
acterize the function. The frequencies of f are deÔ¨Åned to be the values of
Œª for which the average
A(Œª) = lim
T ‚Üí‚àû
1
T
 T
0
exp(‚àíiŒªt)f(t)dt
is not zero. It is known that this happens for at most countably many
values of Œª, say Œª1, Œª2, . . . . The amplitudes of the modes are given by the
formulas
Cn = A(Œªn)
for n = 1, 2, . . . .
An important example of an almost-periodic function is deÔ¨Åned by the
series
f(t) =
‚àû

n=1
exp(it/2n)
n2
.
The nth term in the series has period 2n+1œÄ, so the function is not periodic.
But given a tolerance Œµ, we can choose N so large that
‚àû

n=N
1
n2 < Œµ
2.
Then, setting TŒµ = 2N+1œÄ, we have
|f(t + TŒµ) ‚àíf(t)| < Œµ
for all t. This shows that f is almost periodic, but note that the integral
of this series does not converge, and so does not deÔ¨Åne an almost-periodic
function.
The class of all almost-periodic functions is larger than needed to study
many nonlinear oscillators, and the smaller class of quasiperiodic functions
is useful. These functions are generated by Ô¨Ånitely many frequencies as
follows. Let œâ be a vector of M real numbers,
œâ = (œâ1, . . . , œâM),

20
1.
Linear Systems
and let ‚Éón be a multi-index, that is, ‚Éón is a vector of integers such that
‚Éón = (n1, . . . , nM).
If the components of œâ are rationally related, then the sequence {‚Éón ¬∑ œâ} is
equivalent to a sequence of integer multiples of a real number. However,
if the components of œâ are not rationally related, then the sequence is
dense in the real numbers. We deÔ¨Åne |‚Éón| =  ni, and we consider a set
of amplitudes {C‚Éón} that satisÔ¨Åes some convergence condition, say |C‚Éón| ‚â§
1/|‚Éón|2 as |‚Éón| ‚Üí‚àû. Then the series
‚àû

|‚Éón|=‚àí‚àû
C‚Éón exp(it‚Éón ¬∑ œâ)
deÔ¨Ånes an almost-periodic function whose frequencies are generated by
a Ô¨Ånite set, namely, the components of œâ. Such a function is called a
quasiperiodic function.
Quasiperiodic functions are closely related to periodic functions. For
example, if f is a quasiperiodic function, then there is a function F(s),
s ‚ààEM, that is periodic in each component of s, such that f(t) = F(œât).
In particular, if f has the series representation
f(t) =
‚àû

|‚Éón|=‚àí‚àû
C‚Éón exp(it‚Éón ¬∑ œâ),
then F is deÔ¨Åned by the formula
F(s) =
‚àû

|‚Éón|=‚àí‚àû
C‚Éón exp(i‚Éón ¬∑ s).
On the other hand, let F(s1, . . . , sM) be a diÔ¨Äerentiable function that is
2œÄ-periodic in each variable s1, . . . , sM. Such functions have Fourier series,
say
F(s1, . . . , sM) =
‚àû

|‚Éón|=‚àí‚àû
F‚Éón exp(i‚Éón ¬∑ s),
where ‚Éón = (n1, . . . , nM) and s = (s1, . . . , sM). With a vector of frequencies
œâ = (œâ1, . . . , œâM), we can deÔ¨Åne a function f by the formula
f(t) = F(œâ1t, . . . , œâMt).
This function has a generalized Fourier series, namely,
f(t) =
‚àû

|‚Éón|=‚àí‚àû
F‚Éón exp(it‚Éón ¬∑ œâ),
so it is a quasiperiodic function.

1.5. Fourier Methods
21
These remarks show that quasiperiodic functions are essentially multiply
periodic functions that involve more than one frequency.
The following theorem will be used later:
Weak Ergodic Theorem. Let f be a quasiperiodic function with F de-
Ô¨Åned as above. Suppose that the frequencies deÔ¨Åning f, say œâ, are rationally
independent; that is, n ¬∑ œâ Ã∏= 0 if n Ã∏= 0. Then
lim
T ‚Üí‚àû
1
T
 T
0
f(t)dt =
 1
2œÄ
M  2œÄ
0
¬∑ ¬∑ ¬∑
 2œÄ
0
F(s1, . . . , sM)ds1 ¬∑ ¬∑ ¬∑ dsM.
In fact, both sides are equal to C0, which proves the theorem. This form
of the ergodic theorem from statistical mechanics is useful in a variety of
numerical computations described in Chapter 7.
1.5.2
Linear Systems with Periodic Forcing
Consider the equation
dx
dt = Ax + g(t),
where A is a constant diagonalizable matrix and g is a continuous, periodic
function, say
g(t) =
‚àû

m=‚àí‚àû
Cm exp(imœât),
which has period 2œÄ/œâ. The solution x(t) is given by the formula
x(t) = exp(At)x(0) +
 t
0
exp[A(t ‚àís)]g(s)ds.
If the spectral decomposition of A is given by
A =
N

j=1
ŒªjPj,
then the integral in the formula for x(t) becomes
 t
0
N

j=1
‚àû

m=‚àí‚àû
Cm exp[Œªj(t ‚àís) + i(mœâ)s]dsPj
=
N

j=1
‚àû

m=‚àí‚àû
Cm
(eimœât)
(Œªj ‚àíimœâ) ‚àíeŒªjt Pj
as long as the ratio Œªj/iœâ is not an integer m for which Cm Ã∏= 0. This
condition ensures that the forcing is not resonant.

22
1.
Linear Systems
If the forcing is resonant, that is, if Œªj/iœâ = k for some integers k and
j for which Ck Ã∏= 0, then the corresponding term in the solution has the
form
 t
0
Ck exp[Œªj(t ‚àís) + ikœâs]dsPj = Ckt exp(iœâkt)Pj.
The resonance case occurs when the forcing function has a frequency that
matches one of the free frequencies of the problem, that is, one of the
eigenvalues of A. Subresonant forcing occurs when one of the frequencies
of g matches an integer multiple of one of the free frequencies.
1.5.3
Linear Systems with Quasiperiodic Forcing
Now consider the equation
dx
dt = Ax + g(t),
where A is a constant diagonalizable matrix and g is a continuous,
quasiperiodic function generated by two frequencies, say œâ and ¬µ:
g(t) =
‚àû

m,n=‚àí‚àû
Cm,n exp[i(mœâ + n¬µ)t].
We suppose that |Cm,n| ‚â§K/(m2 + n2) for some constant K.
The solution x(t) is given by the formula
x(t) = exp(At)x(0) +
 t
0
exp[(A(t ‚àís)]g(s)ds.
If the spectral decomposition of A is
A =
N

j=1
ŒªjPj,
then the integral in the formula for x(t) becomes
 t
0
N

j=1
‚àû

m,n=‚àí‚àû
Cm,n exp[Œªj(t ‚àís) + i(mœâ + n¬µ)s]Pjds
=
N

j=1
‚àû

m,n=‚àí‚àû
Cm,n
exp[i(mœâ + n¬µ)t]
i(mœâ + n¬µ) ‚àíŒªj
Pj
‚àí
N

j=1
‚àû

m,n=‚àí‚àû
Cm,n exp(Œªjt)
i(mœâ + n¬µ) ‚àíŒªj
Pj

1.6. Linear Systems with Variable CoeÔ¨Écients: Variation of Constants Formula
23
if these series converge. The series do converge if the eigenvalues of A
are not purely imaginary. However, if some eigenvalues of A lie on the
imaginary axis and the frequencies ¬µ and œâ are rationally independent,
then the series representing x(t) becomes more diÔ¨Écult to evaluate. In
particular, the numbers i{mœâ + n¬µ} are dense in the imaginary axis, and
so the denominators i(mœâ + n¬µ) ‚àíŒªj become arbitrarily small for large m
and n. This is a small divisor problem, and we will deal with aspects of it
in Section 7.4.3.
1.6
Linear Systems with Variable CoeÔ¨Écients:
Variation of Constants Formula
A general linear system has the form
dx
dt = A(t)x + f(t),
where x, f ‚ààEN and A is an N √ó N matrix of continuous functions. Let
Œ¶(t) denote the solution of the matrix equation
dŒ¶
dt = A(t)Œ¶,
Œ¶(0) = identity.
The matrix Œ¶ is called the fundamental solution for the problem, and
it is determined by solving these N 2 scalar diÔ¨Äerential equations for the
components of Œ¶.
The solution x(t) of the linear problem is given by the variation of
constants formula
x(t) = Œ¶(t)x(0) +
 t
0
Œ¶(t)Œ¶‚àí1(s)f(s) ds.
This formula can be easily derived by setting x = Œ¶(t)y, since the equation
for y is
dy
dt = Œ¶‚àí1(t)f(t).
The diÔ¨Éculty in using this formula is that Œ¶ is usually not available. If
A is constant, then we have seen that
Œ¶(t) = exp(At).
If A is periodic, then
Œ¶(t) = P(t) exp(Rt),
which at least shows the form of Œ¶ even though P and R may be diÔ¨Écult
to Ô¨Ånd. No comparable result is available in cases where A is quasiperiodic

24
1.
Linear Systems
or almost periodic. In the general case, usually
Œ¶(t) Ã∏= exp
  t
0
A(s)ds

,
since in general
A(t)
 t
0
A(s)ds Ã∏=
 t
0
A(s)dsA(t).
1.7
Exercises
1.1. a. Show that the Laplace transform of a convolution
 t
0
h(t ‚àís)f(s)ds
is the product of the transforms of h and f if f and h do not grow
faster than exponential functions as t ‚Üí‚àû. Evaluate the Laplace
transform of the derivative dg/dt of a diÔ¨Äerentiable function g(t).
b. Use the Laplace transform to solve the diÔ¨Äerential equation
d2x
dt2 + dx
dt + bx = f(t),
where a and b are known constants and f(t) is a given function.
c. Show that if
y(t) =
 t
0
exp[A(t ‚àís)]f(s)ds,
where y, f ‚ààEN and A in EN√óN is a constant matrix, then
dy
dt ‚àíAy = f.
Suppose that A is diagonalizable. Use the spectral decomposition of
A and the Laplace transform to relate the solution of this diÔ¨Äerential
equation to the eigenvalues of A.
d. Show that any solution of the diÔ¨Äerential equation
dy
dt ‚àíAy = f
can be written in the form
y(t) = exp(At)y(0) +
 t
0
exp[A(t ‚àís)]f(s)ds
using the method of Laplace transforms.
1.2. a. Let P(t) = Œ¶(t) exp(‚àíRt) as in the proof of Floquet‚Äôs Theorem in
Section 1.4. Show that the change of variables x = P(t)y takes the
periodic system
dx
dt = A(t)x

1.7. Exercises
25
into the system
dy
dt = Ry,
where R is a constant matrix. Thus, show that any periodic system
can be transformed in this way into a linear system having constant
coeÔ¨Écients.
b. Find a 2 √ó 2 matrix of functions, say A(t), such that
A(t)
  t
0
A(s)ds

Ã∏=
  t
0
A(s)ds

A(t).
Show that in this case the matrix Œ¶(t) = exp[
 t
0 A(s)ds] does not
solve the equation
dŒ¶/dt = A(t)Œ¶.
Find such a matrix A(t) that is periodic. Relate this result to
Floquet‚Äôs theorem.
c. Show that if Œ¶(t) is an N √ó N matrix of functions satisfying the
equation
dŒ¶
dt = A(t)Œ¶,
then the function D(t) = det[Œ¶(t)] satisÔ¨Åes the equation
dD
dt = tr[A(t)]D(t),
where tr[A] is the trace of A (i.e., the sum of the elements of A lying
on the main diagonal).
1.3. Let A(t) = U ‚àí1(t)CU(t), where C is a constant 2 √ó 2 matrix and the
components of the 2 √ó 2 matrix U are
U11 = cos t,
U12 = sin t,
U21 = ‚àísin t,
U22 = cos t.
The eigenvalues of A and C are identical. Apply Floquet‚Äôs transforma-
tion to this system. Show by example that the resulting system having
constant coeÔ¨Écients can have a positive eigenvalue even though both
the eigenvalues of C have negative real parts. Conclude that eigen-
values of a periodic coeÔ¨Écient matrix do not determine stability
properties of the linear system.
1.4. Show that if |‚àÜ| < 1, then all solutions of Hill‚Äôs equation (Sec-
tion 1.4.1) are bounded for ‚àí‚àû< t < ‚àû. Also, show that if |‚àÜ| > 1,
then all solutions (excluding the one that is zero everywhere) are
unbounded on ‚àí‚àû< t < ‚àû.
1.5. Construct the complete stability diagram for Meissner‚Äôs equation in
Section 1.4.2 using numerical simulation.
1.6. Verify the weak ergodic theorem for the function F(s1, s2) = 1 +
cos s1 + cos s2 and œâ1 = 1 and œâ2 =
‚àö
2 by direct substitution. That
is, show that
(2œÄ)‚àí2
 2œÄ
0
F(s1, s2)ds1ds2 = lim
T ‚Üí‚àû
1
T
 T
0
F

t,
‚àö
2t

dt.

2
Dynamical Systems
This chapter contains material intended to provide the reader with some
background in topics of dynamical systems that are used later. There is
a large literature on each topic in this chapter, and the presentation here
provides only an overview of topics relevant to the remainder of the book.
A distinction is usually made between systems that are isolated, known
as free systems, and those that interact with the outside world, known as
forced systems. But this classiÔ¨Åcation is not reliable. Often we reduce forced
systems to (apparently) free ones by looking at the system stroboscopically
or by introducing extra variables to describe external inÔ¨Çuences. Often
we reduce free problems to ones that appear to be forced because certain
combinations of their variables are time-like. For example, systems in which
energy is conserved can have dissipative components within them such as
hyperbolic equilibria, which can be uncovered by Ô¨Ånding a time-like variable
among the variables of the system and using it to reduce the problem to a
dissipative one of lower order.
The term oscillation usually refers to a periodic or almost-periodic solu-
tion of a system of diÔ¨Äerential equations. Free oscillations are oscillatory
solutions to models that are time-invariant. The LC circuit is a typical
free oscillator. Nonlinear oscillations are oscillatory solutions to nonlinear
diÔ¨Äerential equations. This chapter begins with a study of free nonlinear
problems in two variables, about which a great deal is known.
Most oscillations studied here can be described by functions of the
form F(x1, . . . , xN) that are 2œÄ-periodic in each of N phase variables
x1, . . . , xN, which themselves vary with time. A convenient mathematical
description of an oscillator is one given in terms of such phase variables,

28
2. Dynamical Systems
and phase equations are studied in Section 2.2. However, most physical
problems are described in terms of physical observables like voltages, cur-
rents, displacements, and so on, and converting such problems to convenient
phase variables is usually diÔ¨Écult. The third and fourth sections investi-
gate general conservative and dissipative systems, respectively. This leads
to a discussion of discrete dynamics and function iterations in Section 2.5.
Finally, some oscillations caused by time delays are described in Section 2.6.
2.1
Systems of Two Equations
It is possible to describe in detail solutions of two linear equations. Much of
this work carries over to nonlinear systems as well, at least near equilibria.
Poincar¬¥e and Bendixson‚Äôs theory deals with general nonlinear systems for
two variables. It is derived in Section 2.1.2 and then later applied to study
Lienard‚Äôs equation.
2.1.1
Linear Systems
Consider the linear system
dx
dt
=
ax + by
dy
dt
=
cx + dy
where a, b, c, and d are real numbers. The point x = 0, y = 0 is a equilibrium
for this system, and the following analysis shows how all solutions of the
system behave.
We saw in Chapter 1 that the solutions of this system are based on
exponentials of eigenvalues of the coeÔ¨Écient matrix, namely, solutions of
the characteristic equation
Œª2 ‚àí(a + d)Œª + (ad ‚àíbc) = 0,
where the coeÔ¨Écient of ‚àíŒª is tr(A), the trace of the matrix
A =

a
b
c
d

,
and the last term is its determinant, det A. Therefore, this equation has
the form
Œª2 ‚àítr(A)Œª + det(A) = 0,
and the eigenvalues are
Œª = tr(A)
2
¬±
tr(A)
2
2
‚àídet(A).

2.1. Systems of Two Equations
29
 y
 x
Figure 2.1. A = diagonal (‚àí2, ‚àí1). A stable node.
If we suppose that det A Ã∏= 0, then there are three cases:
1. The roots are real and have the same sign. If the roots are negative,
then all solutions tend to the critical point as t ‚Üí‚àû. This is called a
stable node (see Figure 2.1). In the other case, it is an unstable node.
2. The roots are real and have opposite signs. In this case, there is a one-
dimensional stable manifold and a one-dimensional unstable manifold
(see Section 1.2.2). The critical point x = 0, y = 0, is called a saddle
point or hyperbolic point in this case (see Figure 2.2).
3. The roots are complex conjugates. If the real part of the eigenvalues
is negative, then all solutions spiral into the critical point. This is a
stable spiral or a spiral sink (Figure 2.3). If the real part is positive,
solutions spiral away from the critical point. This is an unstable spiral
or a spiral source. If the real parts are zero, then this is a center, and
solutions circle around it (see Figure 2.4).
Much of the behavior depicted in Figures 2.1‚Äì 2.4 is also seen in nonlinear
systems. However, nonlinear systems can have more complicated behavior.
2.1.2
Poincar¬¥e and Bendixson‚Äôs Theory
Consider a system of two ordinary diÔ¨Äerential equations of the form
dx
dt
=
f(x, y)
dy
dt
=
g(x, y),

30
2. Dynamical Systems
 y
 x
Figure 2.2. A = diagonal (‚àí1, 1). A saddle point.
Figure 2.3. A stable spiral.
Figure 2.4. A center.

2.1. Systems of Two Equations
31
where f and g are continuously diÔ¨Äerentiable functions of (x, y) over some
compact (i.e., closed and bounded) set D in E2. Given a point (Œæ, Œ∑) in D
there is a unique solution of this system passing through it. We say that
(Œæ, Œ∑) is an equilibrium (equivalently, rest point, critical point, or static
state) for the system if f(Œæ, Œ∑) = g(Œæ, Œ∑) = 0. Otherwise, we write the
unique solution beginning (t = 0) at (Œæ, Œ∑) as x = x(t, Œæ, Œ∑), y = y(t, Œæ, Œ∑).
Poincar¬¥e and Bendixson‚Äôs theory describes what can happen to this solution
if it eventually remains in D.
Poincar¬¥e‚ÄìBendixson Theorem. Suppose that D is a closed and
bounded subset of E2 containing at most a Ô¨Ånite number of rest points.
Moreover, suppose that (x(t), y(t)) is a solution of the system
dx
dt
=
f(x, y)
dy
dt
=
g(x, y),
and that for some time t0 it remains in D for all t ‚â•t0. Then there are
three possibilities:
1. [x(t), y(t)] tends to a critical point as t ‚Üí‚àû.
2. [x(t), y(t)] tends to a periodic orbit as t ‚Üí‚àû. This means that there
is a periodic solution of the system, say x = p(t) and y = q(t), such
that the solution [x(t), y(t)] approaches the set {[p(œÑ), q(œÑ)] : ‚àí‚àû<
œÑ < ‚àû} as t ‚Üí‚àû.
3. [x(t), y(t)] approaches a set that is homeomorphic to a circle and that
contains critical points and orbits joining them.
Thus, solutions either approach a critical point of the system, an os-
cillation, or they approach a necklace of critical points on a strand of
orbits.
The proof of this result can be found in [24, 58, 68] and it is not presented
here.
This remarkable result rules out a great number of pathologies for two-
dimensional systems that must be dealt with in higher dimensional systems.
Much of the work on diÔ¨Äerential equations is restricted to problems that
can be reduced to systems to which this theorem can be applied. A typical
application of the Poincar¬¥e‚ÄìBendixson theory is given in Section 2.1.3.
This theorem is not valid in dimensions higher than two. It relies es-
sentially on reducing the system to a single scalar ordinary diÔ¨Äerential
equation
dx
dy = f(x, y)
g(x, y)

32
2. Dynamical Systems
and then using special properties of solutions to such equations. Lorenz‚Äôs
system of equations provides an important example of three equations
where most solutions do not converge to any of the three options in the
Poincar¬¥e‚ÄìBendixson Theorem. Instead, they wander in a structured but
chaotic way for all time. Lorenz‚Äôs system is
dx
dt
=
œÉ(y ‚àíx)
dy
dt
=
‚àíxz + Œ±x ‚àíy
dz
dt
=
xy ‚àíŒ≤z,
where Œ±, Œ≤, and œÉ are constants. Chaotic solutions of this system are present
when œÉ = 10, Œ± = 28, and Œ≤ = 8/3. Solutions are depicted in Figure 2.5.
2.1.3
x‚Ä≤‚Ä≤ + f(x)x‚Ä≤ + g(x) = 0
Lienard‚Äôs equation includes a variety of nonlinear second-order equations
that are used in many applications. It is
d2x
dt2 + f(x)dx
dt + g(x) = 0,
where f and g are continuous functions of x. Two cases of special
importance are van der Pol‚Äôs equation
f(x) = ax2 ‚àíb
and
g(x) = œâ2x
and DuÔ¨Éng‚Äôs equation
f(x) = 0
and
g(x) = x3 + ax + b
for some constants œâ, a, and b. These two cases are studied at several points
throughout this book. However, in this section we study a form of Lienard‚Äôs
equation that includes van der Pol‚Äôs equation, namely,
d2x
dt2 + f(x)dx
dt + x = 0,
where f is a smooth function. We now derive conditions that ensure that
this equation supports a nonlinear oscillation.
In analogy with mechanical systems we deÔ¨Åne the energy of the system
to be the sum of the kinetic and potential energies:
Àôx2
2 + x2
2 .
In this case we have
Àôx2
2 + x2
2 = C ‚àí
 t
0
f(x(s)) Àôx2(s)ds,

2.1. Systems of Two Equations
33
‚àí20
‚àí10
0
10
20
‚àí50
0
50
0
5
10
15
20
25
30
35
40
45
50
x
y
z
‚àí20
‚àí10
0
10
20
‚àí50
0
50
0
5
10
15
20
25
30
35
40
45
50
x
y
z
‚àí20
‚àí15
‚àí10
‚àí5
0
5
10
15
20
‚àí30
‚àí20
‚àí10
0
10
20
30
x
y
Figure 2.5. Top: Cross-eyed stereo depiction of a solution of Lorenz‚Äôs system.
Bottom: Projection of the solution into the xy-plane.

34
2. Dynamical Systems
where C is a constant. Thus, if f = 0, energy is conserved; if f(x) > 0,
then energy is taken out of the system; and, if f(x) < 0, then energy is put
into the system.
Integrating the diÔ¨Äerential equation once results in an equivalent Ô¨Årst-
order system:
dx
dt
=
y ‚àíF(x)
dy
dt
=
‚àíx,
where
F(x) =
 x
0
f(z)dz.
Hypothesis H.
1. F(‚àíx) = ‚àíF(x) for all x.
2. There is a number Œ± > 0 such that F(x) is negative for 0 < x < Œ±.
3. There is a number Œ≤ ‚â•Œ± such that F(x) is positive and strictly
increasing for x > Œ≤.
4. F(‚àû) = ‚àû.
In analogy with the RLC Ô¨Ålter (see Section 1.1.2), the coeÔ¨Écient f(x)
corresponds to a resistance: If f > 0, then there is resistance, and energy is
being taken out of the system. If f < 0, then there is negative resistance,
and energy is being put into it. The fact that F < 0 for x near 0 implies
that x = 0 lies in a negative resistance region. The fact that F > 0 for
large x indicates that the system is resistive for large x. We will see exam-
ples of electronic devices, such as tunnel diodes, that exhibit both positive
and negative resistances later. With Hypothesis H, we have the following
Theorem.
Lienard‚Äôs Theorem. Let Hypothesis H be satisÔ¨Åed. Then Lienard‚Äôs
equation has a (nontrivial) periodic solution.
Proof. Consider a solution of the system starting with x(0) = 0 and
y(0) = y0 > 0. If y0 is large, then there is a solution of the equation that
proceeds clockwise completely around the origin and hits the positive y
axis at a point y1(y0).
We use the following lemma, which we state here without proof (see [58],
pp. 57‚Äì60).
Lemma. The mapping y1 : y0 ‚Üíy1(y0) has the following properties:
1. If Œ± < Œ≤, then y1 is deÔ¨Åned for large values of y0 and for small values
of y0. In addition, for large values of y0, y1(y0) < y0, and for small
values of y0, y1(y0) > y0.

2.2. Angular Phase Equations
35
2. If Œ± = Œ≤, then y1 is deÔ¨Åned for all values of y0 on the positive y-axis.
Moreover, y1 is a monotone decreasing function of y0.
The results in the above lemma (1) are illustrated in Figure 2.6. Since
there are no critical points in the annular region described in Figure 2.6, it
follows from the Poincar¬¥e‚ÄìBendixson Theorem that any solution starting
on the (upper) line segment [y1(y0), y0] must approach a periodic solution
of the system. This completes the proof of the theorem.
An interesting corollary of this theorem uses the additional condition:
Hypothesis H‚Ä≤. Œ± = Œ≤.
With this we have the following theorem.
Theorem. Let conditions H and H‚Ä≤ be satisÔ¨Åed. Then Lienard‚Äôs equation
has a unique periodic solution. All solutions in the plane approach this orbit
except for the unstable equilibrium x = 0, y = 0.
Proof. The proof follows directly from the above lemma (2). With con-
dition H‚Ä≤ the mapping y0 ‚Üíy1(y0) is monotone. Since the previous
theorem shows that this mapping has a Ô¨Åxed point, it must be unique.
This completes the proof (see also [24]).
Example of van der Pol‚Äôs Equation.
We can use these results to
study van der Pol‚Äôs equation
d2x
dt2 + A(x2 ‚àí1)dx
dt + x = 0.
In this case, F(x) = A(x3/3‚àíx) and Œ± = Œ≤ =
‚àö
3. The last theorem shows
that there is a unique periodic solution of van der Pol‚Äôs equation for any
choice of the parameter A (A > 0). Therefore, van der Pol‚Äôs equation has
a globally stable periodic solution for any A > 0. We study this nonlinear
oscillation when A ‚â™1 and when A ‚â´1 in Chapters 7 and 8, respectively.
2.2
Angular Phase Equations
Unfortunately, the term phase equation is used with at least two diÔ¨Äerent
meanings. First, phase refers to an angular variable, as in the phase of a
sinusoidal function; second, phase variables are position and momentum
variables in mechanical systems. In this section we study angular phase
variables.
Phase equations give a convenient description of nonlinear oscillators.
In the next section we show how some other models can be transformed
into phase-amplitude coordinates, but in this section we consider systems
of (angular) phase equations. In general, these models are measured by an

36
2. Dynamical Systems
 y
 x
y  (y  )
1     0
y  (y  )
1     0
 large
0
 y
 small
 y0
Figure 2.6. An annular region that is invariant to solutions of Lienard‚Äôs equation.
observable, say F(x1, . . . , xN), which is 2œÄ-periodic in each of the phases
x1, . . . , xN. Because of this, we need only consider these variables in a set
T N = {(x1, . . . , xN) : 0 ‚â§xj < 2œÄ for j = 1, . . . , N}.
The set T N is referred to as the N torus. For N = 1, it can be identiÔ¨Åed with
a circle where x is proportional to arc length along the circle (Figure 2.7);
for N = 2, it can be identiÔ¨Åed with a torus that is constructed by cutting
a patch out of the x1x2-plane that is length 2œÄ on a side and identifying
the edge x1 = 0 with the edge x1 = 2œÄ by sewing them together to form
a cylinder, and then identifying the edge x2 = 0 with the edge x2 = 2œÄ
by sewing them together (Figure 2.8). The general case is more diÔ¨Écult to
visualize, although one can usefully think of T N being a cube out of EN
with opposite sides identiÔ¨Åed.
Where do phase equations come from? First, we will see in Section 2.3.5
that phase‚Äìamplitude coordinates can be used eÔ¨Äectively to study some
nonlinear oscillators. Two important mechanical cases are perturbed har-
monic oscillators and weakly coupled pendulums where phase variables
describe angular momentum. Second, modern electronic circuits are de-
signed to be modeled directly using phase variables to facilitate their use
in more complicated circuits. This is illustrated by the VCO in Chapter 1.

2.2. Angular Phase Equations
37
 x
Figure 2.7. An angular variable.
2.2.1
A Simple Clock: A Phase Equation on T 1
A single phase is an angular variable describing a circle as shown in
Figure 2.7.
Let us consider a clock having one hand and with its circumference num-
bered from 0 to 2œÄ (counterclockwise). The hand moves at a constant rate
of œâ radians per hour, so if x denotes the angle between the hand and 0,
then it satisÔ¨Åes the diÔ¨Äerential equation
dx
dt = œâ
where t is measured in hours. The solution of this equation is x(t) = œât+x0.
An important example of a single phase equation is the harmonic
oscillator. Consider the equation
u‚Ä≤‚Ä≤ + œâ2u = 0.
If we introduce polar coordinates to this equation by setting
u‚Ä≤ + iœâu = yeix,
then
y‚Ä≤ = 0
and
x‚Ä≤ = œâ,
which is the phase-amplitude version of a harmonic oscillator. Since the
angle equation is uncoupled from the amplitude, we see that the harmonic
oscillator deÔ¨Ånes a simple clock. This is a physically important model whose
isochrons, or lines of constant phase, are rays from the origin.
Scalar equations of the form
dx
dt = f(x)
cannot have periodic solutions. In fact, if x(t) is an oscillatory solution of
this equation, then there must be a time when dx/dt = 0, so the solution
must have hit an equilibrium. But since solutions are unique, x(t) must be
an equilibrium. Also, solutions x(t) are monotone between equilibria, since
the sign of f can change only at equilibria.

38
2. Dynamical Systems
Another important example is the Ô¨Årst-order phase locked loop. This is
an electronic feedback circuit that is based on the VCO in Chapter 1. The
output voltage of the VCO can be taken to be cos x(t). This is fed back
into the VCO as part of its controlling voltage. The mathematical model
for this is
dx
dt = œâ + cos x.
If the center frequency œâ is greater than 1, then x(t) ‚Üí‚àûas t ‚Üí‚àû, and
we refer to this as an oscillation, since the voltage cos x(t) oscillates in this
case. We say that there is a periodic solution for x on the circle (i.e., x
modulo 2œÄ).
If the center frequency œâ is less than 1, then there is a static state, and
since the solutions of this equation are monotone between static states,
each tends toward a static state as t ‚Üí‚àû. In particular, cos x(t) ‚Üí‚àíœâ.
When œâ > 1, this equation can be solved by quadrature using the
expression
dt =
dx
œâ + cos x.
We have that when x increases by 2œÄ units, t increases by an amount T,
which is the period of oscillation. In particular,
T =
 2œÄ
0
dx
œâ + cos x.
2.2.2
A Toroidal Clock: Denjoy‚Äôs Theory
Two simple clocks, say with phases x and y, respectively, are modeled by
the equations
dx
dt
=
œâ
dy
dt
=
¬µ.
Telling time on this pair of clocks is like locating a point on T 2, so we refer
to this as a toroidal clock. For example, let t be solar time in days, let x
be the phase of a solar day, and let y be the phase of the moon. Then in
solar time units œâ = 2œÄ and ¬µ = 2œÄ/T, since the x variable must complete
a cycle of length 2œÄ each day and the y variable must complete a cycle of
2œÄ every T days (approximately 29 days).
A pair of phase equations has the form
dx
dt
=
f(x, y)
dy
dt
=
g(x, y),

2.2. Angular Phase Equations
39
where f and g are smooth functions that are 2œÄ-periodic in each of x and
y.
Since only values of x and y modulo 2œÄ play a role, we might as well
consider this system to be on the xy-plane reduced modulo 2œÄ. As noted
earlier, this is equivalent to a torus T 2. Therefore, this doubly periodic sys-
tem is often referred to as being a system of ordinary diÔ¨Äerential equations
on a torus, which is generated by the two circles labeled x = 0 and y = 0
in Figure 2.8.
Poincar¬¥e and Denjoy studied this problem in some detail [24]. They
deÔ¨Åned the rotation number to be the limit
œÅ = lim
t‚Üí‚àû
x(t)
y(t).
The limit œÅ is a dimensionless number that gives the relative frequency
between x and y, and it has some very interesting and useful features.
They derived the following results [30].
Denjoy‚Äôs Theorem. Let f, g be as above. Then the number œÅ has the
following properties:
1. œÅ exists, and its value is independent of the initial values, x(0) and
y(0), used to compute it.
2. œÅ depends continuously on f and g.
3. If œÅ is rational, then every solution of this system approaches a peri-
odic solution on the torus, called a torus knot. Moreover, if œÅ = p/q
where p and q are integers (lowest common terms), then the solu-
tion rotates in the x direction p times around the torus and in the y
direction q times around the torus each period.
4. If œÅ is irrational, then all solutions of the system are dense in the
torus. This is referred to as the ergodic case.
The rotation number is usually diÔ¨Écult to evaluate, but it can be sim-
ulated using a computer, and in that way periodic solutions on the torus
can be detected by simulation.
Figure 2.9 shows a simulation of the rotation number for the system
dx
dt
=
œâ + Œª sin(x ‚àíy) + (1 ‚àíŒª)[sin(2x ‚àíy) ‚àísin(2y ‚àíx)]
dy
dt
=
¬µ.
Denjoy and Poincar¬¥e‚Äôs theory leads to a useful numerical method for de-
scribing the behavior of nonlinear oscillators. We will use the theory in
Chapter 7.

40
2. Dynamical Systems
y
x
x
y
Figure 2.8. Toroidal coordinates (x1, x2). The lines x1 ‚â°x = 0 and x2 ‚â°y = 0
correspond to the circles shown.
2.2.3
Systems of N (Angular) Phase Equations
Systems of N angular phase equations have the form
dx
dt = œâ + f(x),
where x, f, œâ ‚ààEN, and f is a vector of functions that are 2œÄ-periodic in
each component of x. Without loss of generality we assume that
1
2œÄ
 2œÄ
0
f(x)dx = 0,
since we can move a nonzero average of the feedback into œâ. These equa-
tions arise in celestial mechanics, statistical mechanics, modern electronic
circuit theory, brain science, and many other applications. Not much can be
said about them in general, although if œâj ‚â´fj, then averaging methods
can be used, as shown in Chapter 7, to derive the rotation vector method
that partially extends Denjoy and Poincar¬¥e‚Äôs theory of rotation numbers
to higher dimensions.
2.2.4
Equations on a Cylinder: PLL
The circuit in Figure 2.10 is a second order phase locked loop [75, 83].
A voltage W comes into a phase detector whose output is V (x)W. This
signal passes through a low-pass Ô¨Ålter, and the result controls the VCO.
The mathematical formulation of a phase locked loop is as follows: The
unknowns in this model are the Ô¨Ålter output voltage z(t) and the VCO
phase x(t) as described in Chapter 1, V (x) is a 2œÄ-periodic function of the
phase x. The model for this circuit is
dx
dt
=
œâ + z
œÑ dz
dt
=
‚àíz + V (x)W.

2.2. Angular Phase Equations
41
0
1
1
0
œÅ
0
1
1
0
Frequency
P‚àímap
Figure 2.9. Rotation number simulation for x‚Ä≤ = Œª sin(x ‚àít) + (1 ‚àíŒª) sin(2x ‚àít).
While the transitions between 1/2 and 1, etc., seem to be discontinuous, Denjoy‚Äôs
theory ensures that this function is a continuous function of Œª. Further simulation
of the rotation number at these ‚Äújumps‚Äù shows many steps. The function is similar
to Cantor‚Äôs function.

42
2. Dynamical Systems
   Voltage
 Controlled 
  Oscillator
     Low
     Pass
    Filter
 z
Diode
 W
 V(x) 
Figure 2.10. A voltage-controlled oscillator neuron (VCON) model.
The Ô¨Årst equation describes the VCO: œâ is the VCO center frequency
and z is its controlling voltage. The second equation describes the output
z of the low-pass Ô¨Ålter where œÑ is the Ô¨Ålter time constant. This model is
quite similar to the pendulum with an oscillating support point and an
applied torque in Section 1.2 [22] and to the point Josephson junction
in Section 8.5 [98]. The PLL is also the basis for the VCON model in
mathematical neuroscience [75].
If W is constant, this model has one phase variable x and one voltage
variable z. It is convenient to visualize this model geometrically on a cylin-
der as shown in Figure 2.11. A nonlinear oscillation can be a closed curve
that goes around the cylinder, in which case x ‚Üí‚àûwhile the orbit is
closed in this geometry. This model is studied further in Section 2.4.2.
2.3
Conservative Systems
Second-order oscillators are complicated to study. The easiest ones are those
derived from conservation laws, and these are discussed in this section. But
Ô¨Årst we recall some notations and ideas from mechanics [28].
2.3.1
Lagrangian Mechanics
Consider a mechanical system whose position or state at time t is described
by a scalar function x(t). Let the potential energy of this system be denoted
by U(x), and denote the kinetic energy by m Àôx2/2, where m is mass. The
Lagrangian of this system is deÔ¨Åned to be
L = m Àôx2
2 ‚àíU(x),
and Hamilton‚Äôs principle states that the system moves between any two
times a and b in such a way as to make the action integral
I(x) =
 b
a
L(x, Àôx)dt
stationary. That is, if we perturb x by a continuously diÔ¨Äerentiable function
ŒµŒ∑(t), where Œ∑(a) = Œ∑(b) = 0 and where Œµ is a small positive real number,

2.3. Conservative Systems
43
y
x
Figure 2.11. Cylindrical coordinates.
then we consider the integral
I(x + ŒµŒ∑) = I(x) + ŒµI‚Ä≤(x)Œ∑ + O(Œµ2),
where
I‚Ä≤(x)Œ∑ =
 b
a
‚àÇL
‚àÇx Œ∑ + ‚àÇL
‚àÇÀôx ÀôŒ∑

dt.
The integral is stationary if I‚Ä≤(x)Œ∑ = 0 for any function Œ∑ as described
above.
A necessary condition for this is obtained by integrating this formula by
parts and setting the resulting integrand equal to zero:
‚àÇL
‚àÇx ‚àí‚àÇ
‚àÇt
‚àÇL
‚àÇÀôx

= 0.
This is the Euler‚ÄìLagrange equation for I(x) [28]. The result in this case is
that
md2x
dt2 + ‚àÇU
‚àÇx = 0.
Multiplying both sides of this equation by Àôx and integrating it gives
m
2
dx
dt
2
+ U(x) = constant.
Thus, the total energy of the system is conserved.
2.3.2
Plotting Phase Portraits Using Potential Energy
Consider the conservation equation
d2x
dt2 + ‚àÇU
‚àÇx = 0,
where the potential function U is a smooth function of x. Maxima of U
correspond to saddle points, and minima correspond to centers in the phase
portrait of solutions to this equation. To see this, suppose that x‚àóis an

44
2. Dynamical Systems
extremum of U; that is, (‚àÇU/‚àÇx)(x‚àó) = 0. Linearizing the equation about
this state gives (x = x‚àó+ u)
¬®u + au = 0,
where
a = d2U
dx2 (x‚àó).
If x‚àóis a maximum (with a < 0), then the characteristic equation
Œª2 + a = 0
has two real roots, one positive and one negative. If x‚àóis a minimum for
U (with a > 0), then both roots are imaginary.
Solutions can be visualized by thinking of U(x) as describing the proÔ¨Åle
of a surface on which a marble is rolling under the force of gravity. The
marble‚Äôs potential energy is proportional to its height U(x), and its motion
is described by Newton‚Äôs law (F = ma). Obviously, if there is dissipation
in the system (e.g., friction), then the marble will eventually roll to a valley
Ô¨Çoor, which is a (local) minimum of U. A marble starting exactly on a peak
of U will remain there, but starting near a peak, it will roll away. However,
the system studied here does not dissipate energy, and so a marble will not
settle to a valley Ô¨Çoor unless it starts there. It will either oscillate endlessly
back and forth across a valley, be Ô¨Åxed at an extremum for U, or it will
move to ‚àû.
Solutions can be plotted in the phase plane when we use x and Àôx as
variables. Since energy is conserved, we have
Àôx2
2 + U(x) = E,
where E is the initial energy of the system. Figure 2.12 depicts two local
minima of U and the corresponding phase portrait.
Let y = Àôx; then the trajectory with energy E2 lies on the graph of the
function y2 = 2[E2‚àíU(x)]. This curve is deÔ¨Åned for an interval A ‚â§x ‚â§B.
The quantity y2 measures the distance between the constant E2 and the
potential energy U = U(x). Thus, the orbit is closed and so represents a
periodic solution of the system.
Closed horizontal lines in the graph of U correspond to periodic solutions
in the phase plane, and horizontal lines tangent at a local maximum of
U correspond to separatrices. If such a line is bounded, the separatrix is
homoclinic, that is, it goes from the saddle point and returns to it. If it
is tangent to two maxima, the separatrix is heteroclinic, going from one
saddle to the other.
Example of a Quartic Potential. Consider the quartic potential
U(x) = x4
4 ‚àíax2
2 + bx + c

2.3. Conservative Systems
45
 U(x)
 x
 x
 dx/dt
 E
 E 1
 E 2
 E 1
 E
 E 2
Figure 2.12. Phase portrait derived from the potential function U. Various energy
levels are shown to correspond to various orbits in the phase plane.
for some constants a, b, and c. Figure 2.13 shows U and the correspond-
ing phase portraits in two typical cases. Note that the extrema of U are
determined by a cusp catastrophe (Section 2.4.3).
One special case is DuÔ¨Éng‚Äôs equation,
¬®x + x ‚àíx3 = 0,
which arises in many applications. For example, it is an approximation to
a pendulum equation using
sin y ‚âày ‚àíy3
3
and x = y/
‚àö
3. In this case, U(x) = x2/2 ‚àíx4/4, so there are always three
rest points, x = 0 and x = ¬±1. The Ô¨Årst is a center and the second two are

46
2. Dynamical Systems
 x
 x
 U
 U
Figure 2.13. Phase portraits for a fourth-order potential. The two other typical
cases are found by the change of variables x ‚Üí‚àíx, which Ô¨Çips these plots through
the y-axis.
saddle points. It is interesting that the saddle‚Äìsaddle connections can be
found explicitly in this case. In particular, the function x(t) = tanh(t/
‚àö
2) is
a solution of the equation that satisÔ¨Åes the boundary conditions x(¬±‚àû) =
¬±1.
2.3.3
Oscillation Period of x‚Ä≤‚Ä≤ + Ux(x) = 0
Consider the conservation equation
d2x
dt2 + ‚àÇU
‚àÇx (x) = 0.
Suppose that we set an energy level E and that corresponding to it is
an oscillation as shown in Figure 2.14. The energy level E determines the
values of x1 and x2, and solving
1
2
dx
dt
2
+ U(x) = E

2.3. Conservative Systems
47
 dx/dt
 x
 x
 x 2
1
Figure 2.14. Closed orbit for U(x) = E. The upper branch from x2 to x1 is
traversed in the same time as the lower branch from x1 to x2.
on the top half of the orbit gives
dx
dt =

2[E ‚àíU(x)].
Integrating this over the top half of an orbit, as shown next, gives
1
‚àö
2
 x2
x1
dx

E ‚àíU(x)
= T
2 ,
which is half the period. Thus, the period of the oscillation is
T =
‚àö
2
 x2
x1
dx

E ‚àíU(x)
.
2.3.4
Active Transmission Line
Figure 2.15 shows a series of circuits that are coupled by resistors (R). In
each circuit, there is a battery (voltage E) in series with a resistor (R‚àó).
Each is in parallel with a capacitor (C) and a tunnel diode the current
through which is given by f(V ).
The current coming into the node labeled Vi is Ii‚àí1, and according to
KirchhoÔ¨Ä‚Äôs law [83], it is balanced by the total current going out of the
node: Ii + I. In turn, I splits into three components, one for each branch
in the circuit:
I = f(Vi) + (Vi ‚àíE)
R‚àó
+ C dVi
dt .

48
2. Dynamical Systems
 V i
 R *
 I
 C
 R
 E
 f(V)
 I i-1
 V i-1
 R *
 C
 E
 f(V)
 V i+1
 R *
 C
 R
 E
 f(V)
 f(V)
 Ground
Figure 2.15. An active transmission line.
On the other hand,
Ii‚àí1 = Vi‚àí1 ‚àíVi
R
and
Ii = Vi ‚àíVi+1
R
.
Combining these formulas gives
C dVi
dt = Vi‚àí1 ‚àí2Vi + Vi+1
R
‚àíf(Vi) ‚àíVi ‚àíE
R‚àó
.
A useful approximation views this as being nearly a continuum of circuits.
We write V (s, t) for the voltage at position s. Thus, Vi(t) = V (iŒ¥s, t), where
Œ¥s is the distance between circuit nodes. As the distance between nodes is
decreased, the resistance R decreases. We suppose that (Œ¥s)2/R ‚âàD as
Œ¥s ‚Üí0. If Œ¥s ‚â™1, then the model becomes (approximately)
C ‚àÇV
‚àÇt = D‚àÇ2V
‚àÇs2 ‚àíf(V ) ‚àíV ‚àíE
R
.
If we look for time-invariant voltage proÔ¨Åles, we study the equation
0 = DV ‚Ä≤‚Ä≤ ‚àíf(V ) ‚àíV ‚àíE
R
.
In the case of tunnel diodes, f(V ) is nearly a cubic curve, so this equation
has the form of a conservation law with quartic potential.
For example, consider the equation
V ‚Ä≤‚Ä≤ + A
V 3
3 ‚àíV

‚àíV ‚àíE
R
= 0,
where here ‚Ä≤ = d/ds. This equation is conservative with potential energy
U(V ) = AV 4
12 ‚àí(1/R + A)V 2
2
+ EV
R .
Our earlier work on quartic potential functions enables us to describe the
solutions of this problem.

2.3. Conservative Systems
49
Given boundary conditions, such as V (¬±‚àû, t) = 0 for the original trans-
mission line, we determine that a static solution of this is possible only
if V = 0 is a homoclinic saddle point, that is, a saddle point where one
unstable separatrix eventually returns to it along a stable one. There are
many static solutions that are periodic functions of s but do not satisfy
the boundary condition. The structure of solutions of the transmission line
problem and their coordinated behavior as functions of t and s is rich, and
it deserves further study [130].
The dynamics of solutions for V (s, t) of the original model can also be
studied using the phase plane. For example, if for each of a sequence of
times the solution curve V (s, tn) is plotted on the V, V ‚Ä≤ coordinates, then
interesting aspects of the solution can be presented [74].
2.3.5
Phase-Amplitude (Angle-Action) Coordinates
It is often convenient to describe conservation models using the momentum
p = m Àôx rather than the velocity Àôx. At the same time, we change notation
by replacing x by q. We deÔ¨Åne the Hamiltonian (total energy) as
H(p, q) = p2
2m + U(q).
The Euler‚ÄìLagrange equation can now be written as (see Section 2.3.1)
dq
dt
=
‚àÇH
‚àÇp
dp
dt
=
‚àí‚àÇH
‚àÇq .
This is a Hamiltonian system of equations corresponding to the physical
problem whose energy is H [28].
Note that if G(p, q) is a function that is invariant when evaluated along
solutions of this system, then
0
=
dG
dt = ‚àÇG
‚àÇp Àôp + ‚àÇG
‚àÇq Àôq
=
‚àí‚àÇG
‚àÇp
‚àÇH
‚àÇq + ‚àÇG
‚àÇq
‚àÇH
‚àÇp
‚â°
[H, G].
The expression [H, G] is called the Poisson bracket, and so we see that any
function that is invariant under the Ô¨Çow of solutions must commute with
H in the sense that the Poisson bracket vanishes, and conversely.
This observation is a starting point for the development of Lie‚Äôs algebraic
theory for nonlinear oscillators. Lie‚Äôs theory uses the product deÔ¨Åned by
the Poisson bracket to deÔ¨Åne an algebra. By viewing solutions as being
actions in this algebra, we can accomplish replacing our original nonlinear
problem by a linear one for representations of the Lie algebra [39].

50
2. Dynamical Systems
Hamiltonian systems are integrable, that is, the structure of these equa-
tions often allows us to change variables in a way that produces solvable
equations. This change of variables does not always exist, and even when it
does, it is not usually possible to carry it out. Still, there are two important
cases where the work is straightforward, namely, the harmonic oscillator
and the pendulum.
We Ô¨Årst introduce a new amplitude variable a by the formula
a = H(p, q),
and we deÔ¨Åne a new phase variable Œæ and a new frequency œâ by solving the
equations
œâ
2
dq
dŒæ
=
‚àÇH
‚àÇp (p, q)
œâ
2
dp
dŒæ
=
‚àí‚àÇH
‚àÇq (p, q),
for p = p(Œæ, a), q = q(Œæ, a) subject to the constraints that H(p, q) = a. The
frequency œâ is chosen so that these solutions p and q have period 2œÄ in Œæ.
This can be done near nondegenerate energy levels of the system, that is,
near orbits that represent nontrivial periodic solutions, but this step is the
Ô¨Årst major hurdle in using the method.
The change of variables
p
=
p(Œæ, a)
q
=
q(Œæ, a)
is invertible, since the Jacobian is
det
Ô£Æ
Ô£ØÔ£∞
‚àÇp
‚àÇŒæ
‚àÇp
‚àÇa
‚àÇq
‚àÇŒæ
‚àÇq
‚àÇa
Ô£π
Ô£∫Ô£ª= ‚àí2
œâ Ã∏= 0.
Therefore, we can deÔ¨Åne inverse variables by
Œæ
=
Œæ(p, q)
a
=
H(p, q)
[22]. The second problem is Ô¨Ånding these inverse variables in a useful form.
When this can be done, it follows that a and Œæ satisfy the equations
da
dt
=
0
dŒæ
dt
=
œâ(a),
which can be integrated directly. Because of this fact, such Hamiltonian
systems are referred to as being integrable systems. Since a is an amplitude

2.3. Conservative Systems
51
and Œæ is a phase variable, these are referred to as being phase-amplitude,
or sometimes angle-action, variables.
Example of a Pendulum. Chester‚Äôs analysis of the pendulum [22]
begins with the equation
d2x
dt2 + sin x = 0.
This can be converted to a Hamiltonian system by setting q for x and
H(p, q) = p2
2 + 2 sin2 q
2.
Then
dq
dt
=
p
dp
dt
=
‚àísin q.
We set H(p, q) = 2a2, and we seek functions p = p(Œæ, a) and q = q(Œæ, a)
and a constant œâ(a) such that
œâ dq
dŒæ
=
p
œâ dp
dŒæ
=
‚àísin q,
where these functions are to have period 2œÄ in Œæ. We have

œâ ‚àÇq
‚àÇŒæ
2
= p2 = 4

a2 ‚àísin2 q
2

,
so we study the two cases a < 1 and a > 1 separately. In the Ô¨Årst case, the
pendulum oscillates regularly, and in the second, it executes full cycles.
If a < 1, we set sin(q/2) = as, and we get an equivalent integral equation
for s,
œâ
 s
0
(1 ‚àíœÉ2)‚àí1/2(1 ‚àía2œÉ2)‚àí1/2dœÉ = Œæ,
if q = 0 corresponds to Œæ = 0. The inverse relation is
s = sin(q/2)
a
= sn
 Œæ
œâ , a2

,
where sn denotes the elliptic sine function [1].
This function is known to have period 4K(a2) in Œæ/œâ, where
K(a2) =
 1
0
ds

(1 ‚àís2)(1 ‚àía2s2)
=
 œÄ/2
0
dœÜ

1 ‚àía2 sin2 œÜ
.

52
2. Dynamical Systems
In order that the function sn(Œæ/œâ, a2) have period 2œÄ in Œæ, we make the
choice
œâ =
œÄ
2K(a2).
Note that as a ‚Üí0, K(a2) ‚ÜíœÄ/2, so the period of small amplitude
oscillations is 2œÄ.
For a > 1, we deÔ¨Åne sin(q/2) = s, so
œâ
 s
0
dœÉ

(1 ‚àíœÉ2)(1 ‚àí(œÉ/a)2)
= aŒæ
and
s = sin q
2 = sn
aŒæ
œâ , a‚àí2

.
This function has period 4K(a‚àí2) in aŒæ/œâ, or 2œÄ in Œæ if
œâ =
œÄa
2K(a‚àí2).
In this case the pendulum is executing full clocking oscillations. In ei-
ther case we have derived œâ(a) for the integrable form of the pendulum
equations, as well as the change of variables that takes the pendulum to in-
tegrable form. Explicit formulas for the oscillations can be found by solving
for q in each case.
2.3.6
Conservative Systems with N Degrees of Freedom
A system of N particles will have N position and N momentum co-
ordinates. Say particle i has position qi(t) at time t and (generalized)
momentum pi(t). If the potential energy is
U(q1, . . . , qN),
then the Hamiltonian is given by the formula
H(p, q) =
N

i=1
p2
i
2mi
+ U(q),
and the equations of motion are
dqi
dt
=
‚àÇH
‚àÇpi
dpi
dt
=
‚àí‚àÇH
‚àÇqi
for i = 1, . . . , N. Thus, 2N equations result, which are equivalent to the
N second-order equations
mj ¬®qj + ‚àÇU
‚àÇqj
(q) = 0

2.3. Conservative Systems
53
for j = 1, . . . , N.
This is a starting point for many studies in statistical and celestial
mechanics [92, 129]. In addition, these equations arise as characteristic
equations in solving Ô¨Årst-order nonlinear partial diÔ¨Äerential equations.
They also arise in the calculus of variations, where they appear as Euler‚Äì
Lagrange equations. Finally, an extensive geometrical theory, a symplectic
geometry that is based on diÔ¨Äerential invariants of this system, has evolved.
Because of the central role played by such systems in a wide variety of math-
ematical disciplines and physics, these equations have been and continue to
be extensively studied. We study examples of this general system further
in later chapters. Here we note the following useful connection between
Hamiltonian systems and certain partial diÔ¨Äerential equations [28].
2.3.7
Hamilton‚ÄìJacobi Theory
Hamiltonian systems are closely related to certain Ô¨Årst-order partial diÔ¨Äer-
ential equations. On one hand, let œÜ(t, x, a), where x ‚ààEN, be a smooth
solution of the Hamilton‚ÄìJacobi equation
‚àÇœÜ
‚àÇt + H(x, ‚àáœÜ) = 0,
for t ‚â•0. We suppose that œÜ contains N parameters a1, . . . , aN such that
det
‚àÇ2œÜ
‚àÇxi‚àÇaj
Ã∏= 0.
Then œÜ is called a complete integral of the equation. We suppose here
that H(x, p) is a continuously diÔ¨Äerentiable function of the 2N variables
x1, . . . , xN, p1, . . . , pN. Then the equations
‚àÇœÜ
‚àÇaj
= bj,
where b1, . . . , bn are free constants, can be used to determine x as a unique
function of t, a, and b.
Let us deÔ¨Åne
pj = ‚àÇœÜ/‚àÇxj;
then these functions satisfy the ordinary diÔ¨Äerential equations
dxj
dt
=
‚àÇH
‚àÇpj
dpj
dt
=
‚àí‚àÇH
‚àÇxj
for j = 1, . . . , N [47]. This shows how solutions of a Hamilton‚ÄìJacobi par-
tial diÔ¨Äerential equation can deÔ¨Åne solutions of an associated Hamiltonian
system.

54
2. Dynamical Systems
On the other hand, a Hamilton‚ÄìJacobi equation
‚àÇœÜ/‚àÇt + H(x, ‚àáœÜ) = 0
with initial conditions
œÜ(0, x) = g(x)
and
‚àáœÜ(0, x) = ‚àág(x)
for a given smooth function g(x) can be solved in the following way using
the Hamiltonian system. DeÔ¨Åne variables
pj = ‚àÇœÜ
‚àÇxj
for
j = 1, . . . , N,
and for each Œæ ‚ààEN, let x(t, Œæ), p(t, Œæ) denote solutions of the Hamiltonian
system that satisfy the initial conditions
xj(0) = Œæj,
pj(0) = ‚àÇg
‚àÇxj
(Œæj).
Finally, let Œ¶(t, Œæ) denote the solution of the equation
dŒ¶
dt = p(t, Œæ) ¬∑ ‚àÇH
‚àÇp [x(t, Œæ), p(t, Œæ)] ‚àíH
with initial condition Œ¶(0, Œæ) = g(Œæ). The change of variables x = x(t, Œæ)
can be inverted: say, Œæ = ÀÜŒæ(t, x). Then we deÔ¨Åne
œÜ(t, x) = Œ¶[t, ÀÜŒæ(t, x)].
This function satisÔ¨Åes the Hamilton‚ÄìJacobi equation
‚àÇœÜ
‚àÇt + H(x, ‚àáœÜ) = 0
and initial conditions
œÜ(0, x) = g(x),
‚àáœÜ(0, x) = ‚àág(x).
We say that the Hamiltonian system forms the set of characteristic
equations for this Hamilton‚ÄìJacobi equation.
These two calculations show the relation between a Hamiltonian system
and its associated Hamilton‚ÄìJacobi equation. There are other interesting
connections between these systems with interpretations in the calculus of
variations [28]. The following example illustrates Jacobi‚Äôs method.
Example of a Two-Body Problem. If bodies of masses m1 and m2
are placed at points x and y ‚ààE3, respectively, then Newton‚Äôs law of
gravitation states that the potential energy of the pair is
U(x, y) = am1m2
r
,

2.3. Conservative Systems
55
where r = |x ‚àíy| and a is a constant. The equations of motion of this
system are
m1¬®x
=
‚àí‚àáxU
m2¬®y
=
‚àí‚àáyU.
The motion can be shown to lie in a plane, and for convenience, we place
the second mass at the origin of our coordinate system. The result is a
system of two equations for two of the components of x, say x1 and x2:
m1¬®x1 + ‚àÇU
‚àÇx1
=
0
m2¬®x2 + ‚àÇU
‚àÇx2
=
0,
where now
U(x1, x2) =
am1m2

x2
1 + x2
2
.
This is equivalent to a Hamiltonian system. Let m1 = m2 = 1. Then
H(x1x2, p1, p2) = p2
1 + p2
2
2
+ U(x1, x2).
The corresponding Hamilton‚ÄìJacobi equation is
‚àÇœÜ
‚àÇt + 1
2
 ‚àÇœÜ
‚àÇx1
2
+ 1
2
 ‚àÇœÜ
‚àÇx2
2
=
b

x2
1 + x2
2
,
where b = ‚àía. In polar coordinates, this equation becomes
‚àÇœà
‚àÇt + 1
2
‚àÇœà
‚àÇr
2
+
1
2r2
‚àÇœà
‚àÇŒ∏
2
= b
r,
where œà(t, r, Œ∏) = œÜ(t, r cos Œ∏, r sin Œ∏). This equation can be solved as a sum
of functions, one in each variable. The result is that
œà = a1t + a2Œ∏ + g(r, a1, a2),
where
g =
 r
r0

2bÀÜr‚àí1 ‚àí2a1 ‚àía2
2ÀÜr‚àí2dÀÜr.
Solving the equations
b1 = ‚àÇœà/‚àÇa1,
b2 = ‚àÇœà/‚àÇa2
leads to the formula
r = c[1 ‚àíe2 sin(Œ∏ ‚àíb2)]‚àí1,
which deÔ¨Ånes a solution of the two-body problem.

56
2. Dynamical Systems
2.3.8
Liouville‚Äôs Theorem
Consider the system of diÔ¨Äerential equations
dx
dt = f(x),
x(0) = Œæ,
where x, Œæ, and f are in EN and f is a smooth function of the components
of x. Let us consider the ensemble of solutions beginning in a volume V (0).
The size of this set after t time units will be given by the integral

V (t)
dx,
where V (t) is a collection of points
{x = x(t, Œæ) : Œæ ‚ààV (0)}.
This is the image of V (0) under the Ô¨Çow deÔ¨Åned by the solutions of the
diÔ¨Äerential equation. If we make the change of variables Œæ ‚Üíx by setting
x = x(t, Œæ), then we have from calculus that

V (t)
dx =

V (0)
det
‚àÇx
‚àÇŒæ

dŒæ.
The derivative ‚àÇx/‚àÇŒæ can be determined from the original equation by
solving the problem
d
dt
‚àÇx
‚àÇŒæ = ‚àÇf
‚àÇx(x)‚àÇx
‚àÇŒæ ,
‚àÇx
‚àÇŒæ (0) = identity.
Thus, the function y(t) = det(‚àÇx/‚àÇŒæ)(t) satisÔ¨Åes
Àôy = tr[fx(x)]y,
y(0) = 1.
The coeÔ¨Écient of the right-hand side of this equation is the divergence of
f, divf = ‚àá¬∑ f. It follows that

V (t)
dx =

V (0)
exp
  t
0
div f(x(t‚Ä≤, Œæ))dt‚Ä≤

dŒæ.
If div f = 0, then volume V (0) = volume V (t) for any t > 0, since these
measures are deÔ¨Åned by the same integral. With these calculations, we have
proved the following useful theorem.
Liouville‚Äôs Theorem. Let the conditions listed above on f be satisÔ¨Åed.
Then the Ô¨Çow deÔ¨Åned by the diÔ¨Äerential equation dx/dt = f(x) is volume-
preserving if div f(x) = 0 for all x.
This result shows that Hamiltonian systems deÔ¨Åne volume-preserving
Ô¨Çows. In fact, the divergence of the right-hand side of a Hamiltonian system

2.4. Dissipative Systems
57
is zero, since
N

i=1
‚àÇÀôq1
‚àÇqi
+
N

i=1
‚àÇÀôp1
‚àÇpi
=
N

i=1
‚àÇ2H
‚àÇpi‚àÇqi
‚àí
N

i=1
‚àÇ2H
‚àÇpi‚àÇqi
= 0.
Therefore, for each t the solutions of a Hamiltonian system deÔ¨Åne a mapping
of EN into itself for which the Lebesgue measure is invariant.
2.4
Dissipative Systems
We have seen that conservative systems typically have a rich structure of
oscillatory solutions. Systems that do not conserve energy are diÔ¨Äerent. For
example, oscillations of such systems are usually isolated, as for Lienard‚Äôs
equation in Section 2.1.3. Two important examples are described in detail
in this section: van der Pol‚Äôs equation and the second order phase locked
loop (PLL) model.
2.4.1
van der Pol‚Äôs Equation
Recall that van der Pol‚Äôs equation is
¬®x + A(x2 ‚àí1) Àôx + x = 0.
We know from Section 2.1.3 that for any choice of A > 0 there is a unique
periodic solution of this equation, and all other solutions approach it as
t ‚Üí‚àûexcept for the unstable equilibrium at x = 0, Àôx = 0.
It is diÔ¨Écult to estimate the period of this oscillation. After some work
on perturbation methods in Chapter 8 we will see that the period for large
values of A is comparable to A, and for small values of A the period is
comparable to 2œÄ. Figure 2.16 shows the oscillation for two values of A.
2.4.2
Phase Locked Loop
The phase locked loop (Section 2.2.4) is one of the basic circuits of modern
electronics. It is the basis of many important synchronous control systems,
including FM radio, Doppler radar, and computer timing devices [83]. The
voltage-controlled oscillator neuron model (VCON) is a model in mathe-
matical neuroscience that is similar to a phase locked loop [75]. A PLL
comprises a VCO whose output is V (x) = cos x, a phase detector that
multiples V (x) by the input, say W, and a low-pass Ô¨Ålter whose output
controls the VCO. The mathematical model is
dx
dt
=
œâ + z
œÑ dz
dt
=
‚àíz + W cos x.

58
2. Dynamical Systems
‚àí2
‚àí1
0
1
2
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
a
‚àí2
‚àí1
0
1
2
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
b
Figure 2.16. (a) A = 10.0, x = x‚Ä≤ = 0 is an unstable node. The underlying
cubic isocline is indicated with dashed lines. The portions of the solution near
this isocline are traversed slowly compared to the rapid jumps away from it. (b)
A = 0.5. The limit cycle is approximately a circle, and its period is nearly 2œÄ. In
this case the solution shown starts at x = 1, x‚Ä≤ = 0, and it slowly winds out to
the limit cycle.
In this section we study the free case of this system when W = A is a
positive constant.
With y = œâ + z, the system takes the form
dx
dt
=
y
dy
dt
=
‚àíœÉy + I + f(x),
where œÉ = 1/œÑ and I = œâ/œÑ are constants and f(x) = (A/œÑ) cos x.
This problem can be analyzed completely. First, all solutions approach
the strip
S = {(x, y) : I ‚àíf ‚àó‚â§œÉy ‚â§I + f ‚àó},
where f ‚àó= maxx |f(x)| = A/œÑ. In fact, we see directly from the second
equation that solutions approach this strip at an exponential rate with time
constant œÉ. It is appropriate to view this system as being on the cylinder

2.4. Dissipative Systems
59
|y| < ‚àû, 0 ‚â§x ‚â§2œÄ, since the equations are invariant under the translation
x ‚Üíx + 2œÄ. We consider several cases.
Case |I| > f ‚àó. When I > f ‚àó, there are no static states, since the isoclines
dx/dt = 0 and dy/dt = 0 never cross. Using the fact that the strip S is
invariant, we will show that the mapping deÔ¨Åned from the cross-section of
this strip at x = 0 to the one at x = 2œÄ has a Ô¨Åxed point in S, and so there
is a periodic solution, say y = Y (x). This periodic solution is a limit cycle
on the cylinder. We prove this in the following theorem [98].
Theorem. If I > f ‚àó, there is a unique limit cycle on the cylinder, and
all solutions approach it as t ‚Üí‚àû.
Proof. First, we need only consider the strip S. Since S is invariant, the
solutions deÔ¨Åne a mapping of the cross-section x = 0 of S into the one for
x = 2œÄ. This mapping is also a contraction mapping, which follows from
the following short calculation showing that the mapping has a positive
derivative that is less than one.
Let M(y0) denote the value of y when x = 2œÄ given that y = y0 when
x = 0. We can write y as a function of x, since there are no critical points
in S, and we have
dy
dx = ‚àíœÉy + I + f(x)
y
,
y(0) = y0. The derivative ‚àÇy/‚àÇy0 satisÔ¨Åes the linear problem
d
dx
‚àÇy
‚àÇy0
=
‚àí
I + f(x)
y2
 ‚àÇy
‚àÇy0
‚àÇy
‚àÇy0
(0)
=
1.
Therefore,
M ‚Ä≤(y0) = exp

‚àí
 2œÄ
0
I + f(x)
y2
dx

< 1.
It follows that this transformation has a unique Ô¨Åxed point, and it lies on a
limit cycle, say L. The stability of this limit cycle follows from the stability
of S and the fact that L is the largest invariant set in S.
If we denote the limit cycle L by y = Y (x), then the relation between x
and t on this limit cycle can be determined by integrating
dx
Y (x) = dt.
In fact, the period is
T =
 2œÄ
0
dx
Y (x).

60
2. Dynamical Systems
0
2
4
6
8
10
12
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Figure 2.17. In this case, I = 0.5, œÑ = œÉ = 1.0, f(x) = cos x, and we see that
all solutions approach stable nodes except for the stable manifolds of the saddle
points. Thus, the VCON output equilibrates to a stable voltage, V (xL). Here
0 ‚â§x ‚â§4œÄ shows two sheets of the cylinder. On each the left equilibrium is a
stable node and the right one is a saddle point.
Case |I| < f ‚àó. In this case there are two equilibria on the cylinder,
namely, when f(x) = ‚àíI. The left one is denoted by xL, and for it f ‚Ä≤(xL) <
0. The other is denoted by xR, and f ‚Ä≤(xR) > 0. The linearization of the
system about these states has the form
d2x
dt2 + œÉ dx
dt ‚àíf ‚Ä≤(x‚àó)x = 0.
The characteristic polynomial is Œª2 + œÉŒª ‚àíf ‚Ä≤(x‚àó) = 0, and its roots are
Œª = ‚àíœÉ
2 ¬±

œÉ2
4 + f ‚Ä≤(x‚àó).
Thus, we see that x‚àó= xL is a stable sink. It is a stable spiral point if I
is small, and it is a stable node if I is near f ‚àó, since f ‚Ä≤(xL) ‚âà0. Similarly,
we see that xR is a saddle point. The following theorem describes stability
properties of the system when |I| < f ‚àó.
Theorem. For each value of œÑ and A, there is a number I‚àó(A, t) < f ‚àó
such that

2.4. Dissipative Systems
61
1. If I < I‚àó, then the sink is globally stable on the cylinder, with the
exception of separatrices.
2. If I‚àó< I < f ‚àó, then a stable sink and a stable limit cycle both exist.
The proof of this theorem is given in [98]. It is accomplished by showing
that if I is small, then the stable and unstable manifolds are as in Fig-
ure 2.17, and for I near I = 1 as in Figure 2.18. The number I‚àóis the value
at which there is a saddle-saddle connection.
0
2
4
6
8
10
12
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Figure 2.18. In this case, I = 0.99, and the other data are as in Figure 2.17.
We see that there is a stable node and a stable periodic solution, y = Y (x), for
this choice of parameters. Y is referred to as being a running periodic solution
or a limit cycle on the cylinder. The running periodic solution corresponds to
repetitive Ô¨Åring by the cell, and the VCON has two stable modes of behavior in
this case.
This system exhibits hysteresis as œâ increases from zero to A and back to
zero. For small values of œâ, the system equilibrates to a stable node, where
it remains until the saddles and nodes coalesce and disappear (at œâ = A).
After this, the limit cycle is reached as shown in Figure 2.19. When œâ is
decreased, the system remains on the limit cycle until the saddle-saddle
connection is reached. Below this, the solution equilibrates to a sink.

62
2. Dynamical Systems
0
2
4
6
8
10
12
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Figure 2.19. In this case, I = 1.1, and the other data are as in Figure 2.17. There
are no equilibria, but there is a unique limit cycle, and it is globally stable. The
dots show where the saddle and node collided.
2.4.3
Gradient Systems and the Cusp Catastrophe
Let F(x) denote a real valued mapping from x ‚ààEN to E1. The gradient
of F, denoted by ‚àáF(x), is a vector that is normal to the level surface
F(x) = c at the point x. Thus, solutions of the equation
dx
dt = ‚àí‚àáF(x)
are curves in EN that are parallel to these normal vectors. Because of this,
F plays the role of an energylike function. However, the values of F are
not conserved as is energy in a conservative system. Instead, F decreases
along solutions. In fact, if x(t) is a solution of this gradient system, then
dF
dt (x(t)) = ‚àáF(x) ¬∑ Àôx = ‚àí|‚àáF(x(t))|2,
which is negative unless x is an equilibrium for the system. We call such a
system dissipative.

2.4. Dissipative Systems
63
 x
 b
 a
Figure 2.20. Bifurcation diagram of the cusp catastrophe.
There can be no periodic solution to a gradient system, since a solution
having a least period T > 0 will satisfy
0 = F[x(T)] ‚àíF[x(0)] =
 T
0
dx
dt ‚àáF(x(t))dt =
 T
0
|‚àáF(x(t))|2dt,
which implies that ‚àáF(x) = 0 for all t. Thus, x(t) must be an equilibrium
of the system.
Gradient systems arise in several settings. One interesting source is from
optimization problems: Suppose that we want to Ô¨Ånd proper extrema of the
function F(x). We Ô¨Ånd these among the values of x for which ‚àáF(x) = 0.
Thus, we solve either of the associated gradient problems
Àôx = ¬±‚àáF(x)
using an initial guess x(0). If this solution approaches a static state, the
limit will be a candidate for an extremum. We do this for many initial points
and build up a library of possible extrema that must be tested separately
[124].
Example of the Cusp Catastrophe
The cusp catastrophe surface provides a number of interesting examples
of nonlinear oscillations. We saw this with the quartic potential in a conser-
vative system. It also provides interesting examples of dissipative systems.
Consider the quartic polynomial
F(x) = x4
4 ‚àíax2
2 + bx + c,
where a, b, and c are constants. Note that this form includes all quartics,
since the coeÔ¨Écient of x3 is the sum of the roots and by a translation this

64
2. Dynamical Systems
 x
 b
 a
Figure 2.21. van der Pol oscillation on a cusp surface.
can always be made zero. We study the solutions of the gradient system
Àôx = ‚àí‚àÇF
‚àÇx .
First we Ô¨Ånd the critical values, that is, the values of x for which ‚àÇF/‚àÇx = 0.
To do this, we must solve the equation
x3 ‚àíax + b = 0
in a way that describes all solutions for any choices of a and b.
There are either one or three real roots, and they can be found by solving
for b and plotting the resulting surface:
b = ax ‚àíx3.
Extreme values of this function of a and x occur where ‚àÇb/‚àÇx = 0, that is,
a ‚àí3x2 = 0. Thus,
b = ¬±2a3/2
3
‚àö
3 .
The result is depicted in Figure 2.20.
On the other hand, the ax trace of the solution when b = 0 is a parabola
a = x2 and the line x = 0. Combining these facts leads to the surface
plotted in Figure 2.20. This is called the cusp catastrophe surface [133].
The upper and lower branches of this surface are stable equilibria for the
system
Àôx = ‚àí‚àÇF
‚àÇx .
Interesting oscillators can be generated by allowing a and b to change with
time, and the following examples illustrate two of them.
van der Pol‚Äôs Equation.

2.5. Stroboscopic Methods
65
 x
 b
 a
Figure 2.22. Lock-washer oscillation on a cusp surface.
Consider the system
Àôx = ‚àí‚àÇF
‚àÇx
Àôa = 0,
a(0) > 0
Àôb = Œµx,
where Œµ is a small parameter, Œµ ‚â™1. Figure 2.21 shows the solutions of
this system. Since Œµ ‚â™1, the solutions equilibrate to the cusp catastrophe
surface faster than b changes. Therefore, the solutions appear to be glued
to this surface except when they cross an edge.
Lock-Washer Oscillator.
Consider the system
Àôx
=
‚àí‚àÇF
‚àÇx
Àôa
=
‚àíŒµb
Àôb
=
Œµa.
Now, the point [a(t), b(t)] executes a slow circle in the ab plane, and the x
variable is attracted to the stable branch of the cusp catastrophe surface
lying above or below the circle. Figure 2.22 shows a typical solution of this
problem.
Many other oscillations can be visualized using this surface by adjusting
the dynamics of a and b. This surface occurs in our analysis of various
aspects of nonlinear oscillations.
2.5
Stroboscopic Methods
If we trace an oscillation using a Ô¨Çashing light, then the sequence of points
observed at the Ô¨Çashes deÔ¨Ånes a table of values for an iteration of EN into
itself. Little is known about such iterations when N ‚â•3.

66
2. Dynamical Systems
So, we restrict attention here to examples of interval mappings, circle
mappings, and mappings of the plane. Most of these topics are of inde-
pendent interest, but we restrict our attention in most cases to problems
related directly to oscillations and chaotic dynamics.
2.5.1
Chaotic Interval Mappings
Iterations of a real-valued function can be surprisingly complicated. Even
simple functions can have highly irregular iterates, and similar behavior is
found in solutions of certain nonlinear oscillator problems. We begin with
deÔ¨Ånitions and a summary of some relevant work on function iteration.
This is followed by some computer experiments for iterations.
Consider a continuous function g that maps a Ô¨Ånite interval I = (a, b)
into itself. We deÔ¨Åne iterates of g by the notation g0(x) = x, g1(x) = g(x),
and gm(x) = g(gm‚àí1(x)) for m = 2, 3, 4 . . . . We say that x‚àóis a Ô¨Åxed point
of period m if gm(x‚àó) = x‚àó, but gj(x‚àó) is not equal to x‚àófor j < m. The
orbit of a point y is the set of points
‚Ñ¶(y) = {gj(y) : j = 0, 1, 2, . . . }.
Finally, we say that x‚àóis a stable Ô¨Åxed point of g if (1) g(x‚àó) = x‚àóand
(2) for every point y near x‚àówe have that
lim
n‚Üí‚àûgn(y) = x‚àó.
Sarkovski‚Äôs Sequence.
A striking result found by Sarkovski describes interesting things about
Ô¨Åxed points of g [126]. Sarkovski‚Äôs sequence is deÔ¨Åned by rearranging the
natural numbers as follows:
1, 2, 22, . . . , 2n, . . . # . . . , 9 ¬∑ 2n, 7 ¬∑ 2n, 5 ¬∑ 2n, 3 ¬∑ 2n, . . . , 9 ¬∑ 2, 7 ¬∑ 2, 5 ¬∑ 2,
3 ¬∑ 2, . . . , 9, 7, 5, 3.
The symbol # separates powers of 2 (pure harmonics) coming from the left
from partials (odd numbers and their harmonics) coming from the right.
Sarkovski showed that if g has a Ô¨Åxed point of order p, then it has a
Ô¨Åxed point of each order q lying to the left of p in this list. Thus, if there
is a Ô¨Åxed point of period two, then there is also a Ô¨Åxed point; at the other
extreme, the presence of a Ô¨Åxed point of period 3 implies that there are
Ô¨Åxed points of all periods (see also [101]).
In one sense, Sarkovski‚Äôs sequence describes the complexity of invariant
sets for g. The farther along g is in the sequence, the richer the collection
of sets that are invariant under it.
An interesting approach to iteration of smooth functions is described by
Ulam [61, 134]. Under certain circumstances there is a function (called a
density function) that is invariant under g. A simple example of this occurs

2.5. Stroboscopic Methods
67
when
g = min(2x, 2 ‚àí2x)
and I = [0, 1]. In this case, the inverse image of an interval of length
L is the union of two intervals of total length L. Therefore, the density
function d = 1 is invariant, and we say in this case that Lebesgue measure
is invariant under g. Invariant measures describe for interval mappings the
idea of volume for Hamiltonian Ô¨Çows [see Section 2.3.8].
The ergodic theorem [117] has a useful implication for this simple ex-
ample. It states (roughly) that if Lebesgue measure is invariant, iterates
under g of most initial points move all over the interval. In particular, the
probability of an iterate hitting a given subinterval is proportional to the
length of that subinterval. This is the basis of the work in [76], which is
described next.
Iteration Histogram and Entropy.
The ergodic theorem suggests a method for using a computer to exper-
iment with function iterates. In particular, let us partition the invariant
interval I into M equal parts (cells) and select a point from I at random.
This point is iterated N ‚àótimes under g and the number of the iterates
that hit each cell is recorded. If N ‚àóand M are suÔ¨Éciently large, then in
cases where the ergodic theorem applies (in particular, when there is an
invariant measure that is absolutely continuous with respect to Lebesgue
measure) the histogram that results from the iteration experiment reÔ¨Çects
the graph of an invariant density function.
Suppose that the iteration histogram has cell contents c1, . . . , cM. If we
normalize these by setting
pj = cj
 M

k=1
ck,
then (p1, . . . , pM) deÔ¨Ånes a probability distribution. The number
H(p) = ‚àí
M

j=1
pj log pj
is called the entropy of the distribution described by p.
In the above equation H indicates how spread out are the components
of p. For example, if the components of p were approximately normally
distributed, we would have pj ‚âàexp[‚àí(j ‚àím)2], and for this H would be a
second moment of the distribution. Therefore, it is related to p‚Äôs variance.
If all cells are equally Ô¨Ålled (pj = 1/M), then H(p) = log M, which is the
maximum value of H. If only one cell is Ô¨Ålled (pJ = 1), then H(p) = 0,
which is the minimum value of H.
Rather than plotting a density function‚Äôs histogram as in [76], we will
simply indicate which cells are eventually hit by the iteration. Figure 2.23

68
2. Dynamical Systems
2.5
2.8
3.1
3.4
3.7
4.0
1
0.0
400
300
200
100
0
r
Period 22
Period 21
Period 20
Entropy
Iterate Density
Figure 2.23. Iterate density and entropy as computed for g(x) = rx(1 ‚àíx).
shows the result of the iteration experiment performed for the function
g(x) = rx(1 ‚àíx).
In Figure 2.23, the value g(1/2) is iterated 2,000 times, and the Ô¨Årst 100
iterates are discarded to suppress transients. The value x = 1/2 is chosen
for the initial point of the iteration because it is known that for any choice
of r the orbit of this point evolves into the invariant set of g that attracts
most initial points. We plot also the entropy of the iteration‚Äôs distribution
to quantify its complexity.
In Figure 2.23, the unit interval 0 ‚â§x ‚â§1 is partitioned into 300 subin-
tervals. The dark pixels are those subintervals in which some iterates of the
mapping lie. Figure 2.23 illustrates high-resolution computation followed
by pixel truncation imposed by the plotting device. In this, we choose 300
values of r between r = 1 and r = 4, and for each we plot horizontally the
pixels containing at least one iterate of g. We could successively rescale the
viewing window and get successively more reÔ¨Åned views of the iteration,
since the computations are of greater accuracy than the plotting device
used here. In Figure 2.23, all cells having at least one iterate are plotted

2.5. Stroboscopic Methods
69
for each of 300 values of r between 1 and 4. We see Ô¨Årst (1 ‚â§r ‚â§3) the
stable Ô¨Åxed point x = (r ‚àí1)/r. Beyond r = 3, there is a cascade of new
periodic orbits appearing that correspond to Sarkovski‚Äôs sequence lying to
the left of #. The # occurs approximately at r ‚âà3.57 for this function.
The left and right branches of this diagram come together at r = r‚àó‚âà3.68.
The behavior beyond # (r > 3.57) is often referred to as being chaotic,
although r intervals where there are stable 6-, 5-, and 3-period orbits are
clearly visible even at the level of resolution of this experiment. The term
chaotic is not clearly deÔ¨Åned, and it is used with diÔ¨Äerent meanings in
various contexts. We take it to mean highly irregular behavior.
The experiment described in Figure 2.23 shows that the simulation is
useful when there is a single Ô¨Åxed point of g, a case to which the ergodic
theorem does not apply directly. In such a case there is an invariant measure
whose density function is a Dirac delta function. Still, the iteration experi-
ment produces a quantized approximation to the Dirac function, since the
iterates all accumulate in the cell containing the Ô¨Åxed point. This observa-
tion enables us to uncover Sarkovski‚Äôs sequence from function iteration, up
to the resolution of the output device, as shown in Figure 2.23.
Finite Semigroup Approximation.
Some insight into the complicated dynamics described above can be
found by approximating the function g by a pixel approximation. We par-
tition the interval [0, 1] into N equal subintervals, J1 = [x0, x1], J2 =
(x1x2], . . . , JN = (xN‚àí1, xN]. We deÔ¨Åne an integer-valued function f by
the table of values
f(i) = j
if g(xi‚àí1) ‚ààJj
for i, j = 1, . . . , N.
Thus, f(i) = j if g maps the left endpoint of Ji into Jj, and f deÔ¨Ånes a
mapping of the set S = {1, . . . , N} into itself. It also describes the graph
of g when N pixels are used to plot it. The iterates of f deÔ¨Åne a Ô¨Ånite
semigroup [95] Figure 2.24 shows the result of performing the iteration
experiment just described for f rather than g.
Markov Chain Approximation to g.
Are iterates of g random? Using f, we can associate with g a Markov
chain. Let us deÔ¨Åne the graph matrix G[f] by the formulas
Gj,k =

1
if f(j) = k,
0
otherwise,
for j, k = 1, . . . , N. The matrix G is a stochastic matrix, since each of its
row sums is equal to one, and each of its components is nonnegative. The
graph of f appears in G[f], where it is traced by the ones appearing in that
matrix as shown in Figure 2.24.
The spectrum of G consists of ones, zeros, and some roots of unity.
These eigenvalues are closely related to the dynamics of f. In fact, each
nontrivial root-of-unity eigenvalue corresponds to a periodic orbit of f, and
the corresponding left eigenvector consists of ones lying over a periodic

70
2. Dynamical Systems
1
10
20
30
40
50
60
70
80
90
100
100
90
80
70
60
50
40
30
20
10
1
Figure 2.24. Graph transition matrix for g(x) = 3.98x(1 ‚àíx) using 100 pixels.
orbit of f. Similarly, each unit eigenvalue (those not among roots-of-unity
eigenvalues) corresponds to a Ô¨Åxed point of f and each zero eigenvalue
corresponds to a transient state of f. Finally, we can associate with g and
N another Markov chain. We deÔ¨Åne Mjk to be the proportion of Jj that
maps into Jk under g. This Markov chain presumably better approximates
g than does f or G[f]. Figure 2.26 shows the results of a Monte Carlo
simulation based on this Markov chain.
The complexity of these iterations can be described using the histogram
entropy. For example, let the eigenpairs for G[f] be denoted by Œªk, œÜk for
k = 1, . . . , N. Add up all of the eigenvectors œÜk for which Œªk Ã∏= 0:
œÜ =

ŒªkÃ∏=0
œÜk.
Normalize this vector by its length, say q = œÜ/|œÜ|, so q = (q1, . . . , qN)
is a probability vector. The entropy H(q) describes the complexity of the

2.5. Stroboscopic Methods
71
2.5
2.75
3
3.25
3.5
3.75
4
1
.5
0
200
150
100
50
0
Entropy
Iterate Density
Figure 2.25. Iteration and entropy of the pixel approximation to g using 100
pixels.
iterates of f and so gives an indication of the dynamics of g. A similar
statistic can be computed for the Markov chain M (see [37]).
2.5.2
Circle Mappings
Consider now a system of diÔ¨Äerential equations on a torus as described in
Section 2.2.2:
Àôx
=
F(x, y)
Àôy
=
G(x, y),
where F and G are diÔ¨Äerentiable functions that are doubly periodic, say
F(x + 2œÄ, y) = F(x, y) = F(x, y + 2œÄ)
for all x and y. If G Ã∏= 0, then we can divide the Ô¨Årst equation by the second
and replace y by t:
Àôx = f(t, x),

72
2. Dynamical Systems
Figure 2.26. Markov chain transition matrix for g(x) using 100 pixels (r = 3.98).
Row sums add to 1.0, and shading indicates the relative density of each compo-
nent. In this example, the values are 0.8 on the original graph and 0.1 in each
cell adjacent to it.
where now f(t, x) = F(t, x)/G(t, x) is a 2œÄ-periodic function of t and x.
Thus, certain systems on the torus are equivalent to periodically forced
scalar problems.
Poincar¬¥e‚Äôs mapping of solutions is deÔ¨Åned by
œÜ : x(0) ‚Üíx(2œÄ) ‚â°œÜ[x(0)],
and it corresponds to a mapping of the circle y = 0 on the torus into itself.
Consider a function Y (t) for which
dY
dt = f(t, Y )
and
Y (0) = x(0) + 2œÄ.
Let z = Y (t) ‚àíx(t). Then
dz
dt = f(t, z + x) ‚àíf(t, x)
and
z(0) = 2œÄ.
But z(t) = 2œÄ is the unique solution of this problem! Therefore, Y (t) =
x(t) + 2œÄ, and so
œÜ(x + 2œÄ) = œÜ(x) + 2œÄ
for all x. This is referred to as the circle mapping property.

2.5. Stroboscopic Methods
73
Figure 2.27. Iteration histogram of Markov graph transition matrix.
Iteration of Circle Mappings.
Let xn+1 denote the image of xn under Poincar¬¥e‚Äôs mapping:
xn+1 = œÜ(xn).
In particular, given a point x0, the dynamics of the solution x(t) are de-
scribed by the sequence {xn}, which gives the values of the solution x(t)
each time it crosses the circle t = 0. It is similar to Ô¨Çashing a light on the
solution x(t) mod 2œÄ each time t hits an integer multiple of 2œÄ, and so this
is sometimes referred to as the stroboscopic method [111].
Because of the circle mapping property, it is necessary to consider only
the iterates of œÜ modulo 2œÄ. In fact, if we deÔ¨Åne a sequence yn to be the
residues of the sequence xn modulo 2œÄ (i.e., xn = 2œÄMn + yn for some
integer Mn, where 0 ‚â§yn < 2œÄ), then
xn+1 = yn+1 + 2œÄMn+1 = œÜ(xn) = œÜ(yn + 2œÄMn) = œÜ(yn) + 2œÄMn,
and so
yn+1 = œÜ(yn) mod 2œÄ.
For example, consider the diÔ¨Äerential equation
Àôx = Œ±,

74
2. Dynamical Systems
where Œ± is a constant. The solutions are
x(t) = Œ±t + x(0).
Therefore,
œÜ(x) = x + 2œÄŒ±,
so œÜ amounts to rotation through 2œÄŒ± radians. This case is referred to as
the Kronecker Ô¨Çow on the torus [115]. The iterations modulo 2œÄ become
yn+1 = yn + 2œÄŒ±
(mod 2œÄ).
These are plotted in Figure 2.28.
We see that if Œ± is rational, then every orbit closes, since in that case
yn = y0+2œÄŒ±n = y0 when n is a multiple of the denominator in Œ±. But if Œ±
is irrational, every orbit is dense in the torus, and so all orbits are ergodic.
The rotation number is deÔ¨Åned for Poincar¬¥e‚Äôs mapping by the formula
œÅ = lim
t‚Üí‚àû
x(t)
t
= lim
n‚Üí‚àû
œÜn(Œæ)
2œÄn ,
where Œæ is the initial condition of x(t). Recall that œÅ does not depend on
the choice of Œæ (see Section 2.2.2).
For dx/dt = Œ±, we have
lim
n‚Üí‚àû
œÜn(Œæ)
2œÄn = lim
n‚Üí‚àû
2œÄnŒ±
2œÄn = Œ±.
Therefore, we have a simple illustration of Denjoy‚Äôs theorem.
The next example illustrates a use of the rotation number in computer
simulations of a Ô¨Årst-order VCON network. Consider the system as
Àôx
=
œâ + cos x + (1 ‚àícos x)(1 + cos y)
Àôy
=
1.
Figure 2.29 shows the rotation number of this equation. We clearly see
intervals where the graph of œÅ plateaus, and it is over these that x is
phase-locked to y.
2.5.3
Annulus Mappings
A variety of interesting results were derived by H. Poincar¬¥e and G. D.
BirkhoÔ¨Äfor mappings of an annulus into itself [12]. An annular region in the
plane is one that is bounded by two concentric, nonintersecting smooth arcs.
Mappings of an annular region into itself occur naturally when we study the
response of a nonlinear oscillation to periodic forcing. The forced oscillation
often lies near the periodic solution, and so an invariant annular region is
formed. Poincar¬¥e‚Äôs Twist Theorem can be applied to prove the existence of
periodic and more complicated oscillations for annulus mappings.

2.5. Stroboscopic Methods
75
0
2
2
n
n+1
œÄ
œÄ
œà
œà
Figure 2.28. œàn+1 = œàn + 2œÄŒ± modulo 2œÄ for Œ± = 3/8.
Poincar¬¥e‚Äôs Twist Theorem. Suppose that Œ¶ maps an annular region
A in E2 into itself such that Œ¶ preserves area and leaves the boundaries of
A invariant. Suppose that the outside boundary is rotated one way and the
inside boundary the other way. Then there must be at least one Ô¨Åxed point
for Œ¶ in A.
The proof is not presented here. If the conditions of this theorem are
satisÔ¨Åed and if Œ¶ is Poincar¬¥e‚Äôs mapping for some dynamical system, then
that system must have a periodic solution whose stroboscopic iterates lie
in A. In fact, the behavior of iterates of Œ¶ are usually much richer, as in
the example of this theorem that is presented in Section 2.6 [86].
2.5.4
Hadamard‚Äôs Mappings of the Plane
Let us now consider more general mappings of the plane into itself. We
begin with a linear iteration
xn+1
=
(1 + œÉ)xn
yn+1
=
(1 ‚àíœÑ)yn,
where œÉ and œÑ are real numbers with 0 < 1 ‚àíœÑ < 1 < 1 + œÉ. Following the
work of Hadamard [56], we begin with a circle of initial data
x2
0 + y2
0 = c2,

76
2. Dynamical Systems
0
1.01
1.18
1.34
1.5
1.1
.82
.55
.28
0
Frequency
œÅ
0
1.01
1.18
1.34
1.5
1
.75
.5
.25
0
Frequency
P‚àímap
Figure
2.29.
Rotation
number
and
return
map
simulations
for
Àôx
=
œâ + cos 8œÄx + 0.1(1 ‚àícos 8œÄx) cos 2œÄt. Note that there is a unique
Ô¨Åxed point for the return mapping when œÅ = 1, two points when œÅ = 1/2, etc.

2.5. Stroboscopic Methods
77
where c is a constant. Then the nth iterate satisÔ¨Åes

xn
(1 + œÉ)n
2
+

yn
(1 ‚àíœÑ)n
2
= c2,
which is the formula for an ellipse having radius c(1 + œÉ)n along the x-axis
(its major axis) and radius c(1‚àíœÑ)n along the y-axis (its minor axis). Thus,
the circle generates a sequence of ellipses that converge to the x-axis.
The point x = 0, y = 0, is an equilibrium for this system. It is a hy-
perbolic (saddle) point, since its characteristic multipliers are 1 + œÉ > 1 >
1 ‚àíœÑ.
Next, consider a nonlinear mapping
xn+1
=
(1 + œÉ)xn + f(xn, yn)
yn+1
=
(1 ‚àíœÑ)yn + g(xn, yn),
where f and g are smooth functions having no linear terms in their Taylor
expansions near x = 0, y = 0. We write
(xn+1, yn+1) = Œ¶(xn, yn)
for this iteration. In analogy with Perron‚Äôs Theorem for diÔ¨Äerential equa-
tions (Chapter 3), we expect there to be a smooth stable manifold and a
smooth unstable one. This fact will not be proved here. However, we can
see that this is the case near x = 0, y = 0.
First, Ô¨Ånd a function y = Y (x) for x near 0 such that
Y = (1 ‚àíœÑ)Y + g(x, Y )
and Y (0) = 0. This can be done, since the following conditions are satisÔ¨Åed:
1. x = 0, Y = 0 is a solution.
2. The function g is smooth near this point.
3. The Jacobian {‚àÇ[‚àíœÑY + g(x, Y )]/‚àÇY }(0, 0) = ‚àíœÑ Ã∏= 0.
Therefore, the implicit function theorem ensures that there is a unique
solution of this equation that satisÔ¨Åes Y (0) = 0. The solution is smooth,
and it can be expanded in a Taylor polynomial about x = 0.
In fact, the set
U = {(x, y) : y = Y (x)}
is the unstable manifold for this system, and any point beginning on it [say,
y0 = Y (x0)] generates a sequence (xn, yn) that diverges from (0, 0) as n
increases.
Similarly, we can Ô¨Ånd a function x = X(y) that solves the problem
X = (1 + œÉ)X + f(X, y)
for y near 0. The set S = {(x, y) : x = X(y)} is the stable manifold for this
problem.

78
2. Dynamical Systems
Homoclinic Points.
The stable and unstable manifolds can eventually cross, and when this
happens, the consequences are dramatic. Suppose that there is a point
R = (Œæ, Œ∑) ‚ààS ‚à©U. Then, Œ¶n(R) ‚Üí(0, 0) as n ‚Üí¬±‚àû. It can also happen
that the manifolds S and U cross transversally at R; that is, the tangents
to these two manifolds at R are not collinear, and we say that R is a
transversal homoclinic point. In this case, the loop in Figure 2.30 on the
manifold U between R and Œ¶(R) must approach S as n ‚Üí‚àû. It becomes
terribly wrapped under iterations of Œ¶.
In particular, consider the rectangular region R in Figure 2.30. It is
known that R contains an invariant set that has inÔ¨Ånitely many periodic
solutions in it, as well as solutions that remain in R but never repeat. These
are aperiodic oscillations. In fact, there are two subsets of R, say labeled 0
and 1, such that given any sequence of ones and zeros, there is a solution
that hits the set 0 and the set 1 in the order speciÔ¨Åed by this sequence.
This also occurs in forced oscillators, as is shown later [5].
2.6
Oscillations of Equations with a Time Delay
Equations with time delays can often be analyzed using iteration methods.
Consider a system of diÔ¨Äerential-diÔ¨Äerence equations
Àôx = f[x(t), x(t ‚àíœÉ)]
for x, f ‚ààEN, and where œÉ > 0 is a Ô¨Åxed time delay. It is shown in [10]
that if f is a smooth function of its arguments and if x is speciÔ¨Åed on some
interval of length œÉ, say
x(t) = œà(t)
for ‚àíœÉ ‚â§t ‚â§0,
then there is a unique solution of this system on some future interval. In
fact, on the interval 0 ‚â§t ‚â§œÉ, this is just the system of ordinary diÔ¨Äerential
equations
dx
dt = f[x(t), œà(t ‚àíœÉ)],
x(0) = œà(0),
and all the usual existence and uniqueness theory for ordinary diÔ¨Äerential
equations carries over. If a solution exists, the process can be continued by
marching forward in steps of size œÉ.
The linear delay equation
Àôx = ‚àíŒ±x(t ‚àí1),
x(t) = œà(t)
for ‚àí1 ‚â§t ‚â§0
where Œ± is a scalar illustrates several interesting things. First, we can solve
this problem using Laplace transforms. Applying the Laplace transform to

2.6. Oscillations of Equations with a Time Delay
79
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxx
S
U
R
Figure 2.30. S and U for a transverse homoclinic point. InÔ¨Ånitely many periodic
orbits hit the rectangle R. In addition, given any sequence of 0‚Äôs and 1‚Äôs there
are two sets O and I in R and an orbit that hits these two sets in the order listed
in the given sequence [5].
both sides of this equation gives
px‚àó(p) ‚àíx(0) = ‚àíŒ± exp(‚àíp)x‚àó(p) ‚àíŒ±
 0
‚àí1
exp[‚àíp(s + 1)]œà(s)ds.
Therefore, the characteristic equation is
p + Œ± exp(‚àíp) = 0,
since this equation describes the poles of the problem‚Äôs transfer function
(see Chapter 1). [10] further shows that there are countably many complex
number solutions to this equation, say {pm}, and that the solution x(t) can
be written as the series
x(t) =
‚àû

m=0
Bm exp(pmt).

80
2. Dynamical Systems
The complex values for p that solve the characteristic equation have the
form p = œÅ + iœÑ, and so separating real and imaginary parts in the
characteristic equation gives an equivalent system
œÅ
=
‚àíŒ±e‚àíœÅ cos œÑ
œÑ
=
Œ±e‚àíœÅ sin œÑ.
When Œ± = œÄ/2, there are two imaginary characteristic values, namely,
œÅ = 0, œÑ = ¬±œÄ/2. For that value of Œ±, the equation has periodic solutions.
When Œ± < œÄ/2, then œÅ < 0, and so x(t) ‚Üí0 as t ‚Üí‚àû. Finally, when
Œ± > œÄ/2, there is a characteristic root having œÅ > 0, so solutions grow as
t ‚Üí‚àû.
It is appropriate to think of a diÔ¨Äerential-diÔ¨Äerence equation as being one
for functions that are in an inÔ¨Ånite dimensional space that is spanned by the
functions {exp(pnt)}. This observation leads to mathematical results that
have been eÔ¨Äectively used to study nonlinear parabolic partial diÔ¨Äerential
equations [131].
Note that setting œÑ = Œ±t and y(œÑ) = x(œÑ/Œ±) changes the example into
the equation
dy
dœÑ = ‚àíy(œÑ ‚àíŒ±).
Therefore, increasing the value of Œ± has the eÔ¨Äect of increasing the delay.
The Laplace transform calculations show that oscillations can appear as
the delay increases through Œ± = œÄ/2.
2.6.1
Linear Spline Approximations
A useful technique for approximating diÔ¨Äerential-diÔ¨Äerence equations of the
form
dx
dt = f[x(t), x(t ‚àíœÉ)]
with the goal of solving them numerically was derived in [7]. This approach
also gives useful insight to solutions of these equations. The idea is to take
the delay interval of length œÉ and divide it into N equal parts. At each point
of the partition, say tj, we deÔ¨Åne x(tj) to be a node of a linear spline. We
will approximate the function x(t) over this interval by taking a piecewise
linear function that agrees with x at the nodes tj. The approximating
function is described by a vector (y0, . . . , yN): Let
yj(t) = x(t ‚àíjœÉ/N)
for the N + 1 nodes j = 0, . . . , N. In particular, yN(t) = x(t ‚àíœÉ). Be-
tween nodes, yj is a linear function as shown in Figure 2.31. We interpret
diÔ¨Äerentiation in the equation to be from the right.

2.6. Oscillations of Equations with a Time Delay
81
With this notation, we see that the derivative of yj(t) is approximately
Àôyj ‚âàyj‚àí1 ‚àíyj
œÉ/N
.
Using the linear spline approximation, we obtain the system of ordinary
diÔ¨Äerential equations (containing no time delay)
Àôy0
=
f(y0, yN)
Àôyj
=
N
œÉ (yj‚àí1 ‚àíyj),
for j = 1, . . . , N, for the components of the vector y(t). With this notation,
we have the following theorem.
Theorem on Linear Spline Approximation. Under the conditions
listed above,
x(t) = y0(t) + O(1/N)
as N ‚Üí‚àû, uniformly for t on any Ô¨Ånite set over which x(t) exists.
The proof of this result is given in [7].
Example of a Linear Spline Solution. Consider the equation
Àôx = ‚àíŒ±x(t ‚àí1)[1 ‚àíx2(t)].
The linear spline approximation method with N = 2 gives
Àôy0
=
‚àíŒ±y2(t)[1 ‚àíy2
0(t)]
Àôy1
=
‚àí2(y1 ‚àíy0)
Àôy2
=
‚àí2(y2 ‚àíy1).
This system has equilibria when y2 = y1 = y0 for y0 = 0 and y0 = ¬±1. The
last two are hyperbolic. Linearizing about x = 0 gives the same equation
as in the preceding example. For Œ± < œÄ/2 it is stable. For Œ± > œÄ/2 there
appears a nonlinear oscillation.
2.6.2
Special Periodic Solutions
Consider the equation
Àôx = h[x(t)]f[x(t ‚àí1)],
where f and h are smooth functions. We can transform this equation by a
change of variables x ‚Üíg(z) into the form
Àôz = F(z(t ‚àí1)).
In fact,
g‚Ä≤(z) Àôz = f{g[z(t ‚àí1)]}h[g(z)],

82
2. Dynamical Systems
 x(t)
 t
 y (t)
j
 t
 y 0
 y N
 y 1
Figure 2.31. (Top) x(t). (Middle) An equal partition of the t-axis on which the
linear spline approximation is based. (Bottom) The linear spline approximation
to x(t).
so if g‚Ä≤ = h(g), we have that F(z) = f[g(z)]. We see that if h(x) = 1 ‚àíx2
as in Section 2.6.1, then g = tanh z.
Thus, we consider the equation
Àôz = F(z(t ‚àí1)),
where F is a smooth function. We look for special periodic solutions of this
equation that satisfy the conditions
z(t + 2) = ‚àíz(t),
z(‚àít) = ‚àíz(t).
Setting y(t) = z(t ‚àí1) and x = ‚àíz gives
Àôx
=
‚àíF(y)
Àôy
=
F(x).
This Hamiltonian system was derived in [119].
An interesting feature of this system has been pointed out [119]. Suppose
that we try to solve for special periodic solutions using a semi-implicit Euler

2.7. Exercises
83
 F
 z
Figure 2.32. F(z) for the sample iteration.
numerical algorithm, say
yn+1
=
yn + ahF(xn)
xn=1
=
xn ‚àíahF(yn+1).
This deÔ¨Ånes a mapping of the xy-plane into itself, say T(xn, yn). The
Jacobian of this system is given by
det

1
ahF ‚Ä≤(xn)
‚àíahF ‚Ä≤(yn+1)
1 ‚àía2h2F ‚Ä≤(yn+1)F ‚Ä≤(xn)

= 1.
Therefore, T is an area preserving transformation of the plane.
Suppose that F(z) has the form shown in Figure 2.32. Then the equilibria
are determined from the equations
F(x‚àó) = F(y‚àó) = 0,
and their stability is determined using the eigenvalues of the matrix

0
‚àíF ‚Ä≤(y‚àó)
F ‚Ä≤(x‚àó)
0

.
Moreover, for F as shown in Figure 2.32 this iteration has an invariant
annulus. Figure 2.33 shows a numerical simulation of this iteration.
Poincar¬¥e‚Äôs Twist Theorem (Section 2.5.3) shows that there is at least
one Ô¨Åxed point within this annulus. However, the actual behavior of the
iteration is much more complicated.
2.7
Exercises
2.1.
Reproduce the phase plane portraits for the four cases of a two-
dimensional linear system with constant coeÔ¨Écients, as depicted in
Figures 2.1‚Äì 2.4.

84
2. Dynamical Systems
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 2.33. An invariant annulus is shown in iterations of the area preserving
mapping described in the text. Note that there are Ô¨Åve elliptic points and four
hyperbolic (heteroclinic) points.
2.2.
The Poincar¬¥e‚ÄìBendixson Theorem indicates that there are three
possible evolutions for a bounded trajectory of a two-dimensional
autonomous system: It approaches a stable equilibrium, a stable pe-
riodic orbit, or a stable necklace. Construct an example of each of
these cases.
2.3. a. The FitzHugh‚ÄìNagumo model arises in studies of tunnel diodes and
neurons:
dv
dt
=
w ‚àíf(v)
dw
dt
=
v ‚àíbw ‚àíE,
where f(v) = v(v ‚àí1)(v ‚àía) and a, b and E are Ô¨Åxed parameters,
0 ‚â§a ‚â§1 and b ‚â•0. Show that for some values of E, b, and a this
system has a unique periodic solution using the results for Lienard‚Äôs
equation.
b‚àó. Consider the general Lienard equation
d2x
dt2 + g(x)dx
dt + Ux(x) = 0.
Suppose that U and g are smooth functions (at least continuously
diÔ¨Äerentiable), that g is an even function of x (that is, g(‚àíx) = g(x)

2.7. Exercises
85
for all x), U is an odd function (that is, U(‚àíx) = ‚àíU(x) for all x),
and that U(¬±‚àû) = ‚àû. Also, suppose that there is a number Œ± such
that
G(x) =
 x
0
g(s)ds
is negative for x < Œ± and positive monotone for x > Œ± with G(‚àû) =
‚àû. Show that there is a unique nontrivial periodic solution. Moreover,
show that any nontrivial solution approaches this solution as t ‚Üí‚àû.
(Hint: Consider the function E(x, dx/dt) = (dx/dt)2/2 + U(x) in the
proof in Section 2.1.3.)
2.4.
Consider the diÔ¨Äerential equations
dx
dt = 1,
dy
dt = Œ±
for (x, y) on the torus T = {(x, y) : 0 ‚â§x, y < 2œÄ}
a. Show that the rotation number of this system is œÅ = 1/Œ±. Describe so-
lutions when Œ± is a rational number. What happens to solutions when
Œ± is irrational? Why? Discuss the computability in these two cases
by dividing the torus into small squares for which each is darkened
and remains so when a trajectory hits it. Corroborate your discussion
with a computer simulation illustrating each case.
b. If Œ± = 1/29.6, plot the nonnegative function (cos x(t) cos y(t))+ over
a toroidal patch 0 ‚â§x, y ‚â§2œÄ. If t denotes solar time in days, then
the pair (x, y) simultaneously records the solar and lunar times.
c. Use a computer simulation to calculate the rotation number of the
system
dx
dt
=
1 + Œª + (1 ‚àíŒª) sin(2x ‚àíy) + Œª sin(x ‚àí2y)
dy
dt
=
2 ‚àíŒª,
for 0 ‚â§Œª ‚â§2. Plot œÅ versus Œª for each choice you make of Œª (see
Figure 2.9).
d. Consider the equations
dx
dt = p,
dy
dt = q,
where p and q are integers and x(0) and y(0) are given, say x(0) = 1.0,
y(0) = 1.0. Show that œÅ = p/q. The solution of this system is a
toroidal knot having winding numbers p and q. Plot the solution of
this equation in the four cases
i. p = 1
q = 2
ii. p = 1
q = 3
iii. p = 2
q = 3
iv. p = 5
q = 6.
Deduce that for a general system on the torus as considered by Denjoy
(Section 2.2.2) if the rotation number is rational (so all solutions

86
2. Dynamical Systems
approach a periodic solution whose rotation number is p/q), then
there is a torus knot that is a stable oscillation of the system.
2.5. a. Consider the three diÔ¨Äerential equations
dx
dt
=
a + sin(x ‚àíy)
dy
dt
=
b + sin(y ‚àíz)
dz
dt
=
c + sin(z ‚àíx),
where a, b, c > 0 and a + b + c > 3.
Reduce this system to one on a torus using the projection method in
Section 2.2.3. (Hint: x + y + z is a time-like variable.)
What is limt‚Üí‚àûx(t) : y(t) : z(t) if a = 2, b = 2, and c = 2?
b‚àó. The common ratios in part a can be plotted using areal coordinates:
Let p = x/(x + y + z), q = y/(x + y + z), r = z/(x + y + z). If a > 1,
b > 1, and c > 1, then these three numbers are nonnegative and add
to 1.0. Therefore, the point (p, q, r) can be identiÔ¨Åed with one lying
on an equilateral triangle in the positive orthant of E3. Plot these
numbers for t = 100œÄ and for various choices of a, b, and c using
computer simulations (see Figure 7.4).
2.6. a. Plot the phase portrait of solutions to the conservative equation
d2x
dt2 + Ux(x) = 0
corresponding to each of the potential functions
i. U(x)
=
x2/2;
ii. U(x)
=
a(sin x/2)2.
b. Show that in case i, all solutions have period 2œÄ.
c‚àó. Calculate the period of oscillations in case ii near stable equilibria.
Describe the period of oscillations of bound states as a function of
amplitude by plotting the energy E versus the period of the oscillation
having this energy. Use this calculation to discuss the solution of the
two-point boundary value problem
d2x
dt2 + Ux(x) = 0,
x(0) = 0,
x(œÑ) = 0.
For what values of œÑ is this solvable? How many solutions are there?
2.7. a. In Section 2.3.5 we derived the transformation from cartesian co-
ordinates to phase‚Äìamplitude coordinates for Hamiltonian systems:
p = p(a, Œæ), q = q(a, Œæ). Calculate the Jacobian of this transformation.
Carry out the full transformation in the case U(x) = x2/2. Derive the
change of variables and use it to solve the equation
d2x
dt2 + Ux(x) = 0.
b‚àó. Carry out the same program when U(x) = (sin x/2)2. (Hint: See [22].)
2.8. a. Consider the Hamiltonian H(p, q) = 
k=1,... ,N(p2
k+q2
k)/2+ŒµH1(p, q),
where Œµ is a small number and H1(p, q) is a smooth (diÔ¨Äerentiable)

2.7. Exercises
87
function. Write the resulting Hamiltonian system in phase-amplitude
coordinates. (Hint: Set dqj/dt + iœâjqj = yj exp ixj.)
b. A collection of N bodies, say located at points x1, . . . , xN in E3 and
having masses m1, . . . , mN satisfy the equations
mj d2xj
dt2 = ‚àí‚àÇU
‚àÇxj
for j = 1, . . . , N, where
U = ‚àí

kÃ∏=j
mkmj
|xk ‚àíxj|
and the sum is taken over all indices j and k for which j Ã∏= k. Carefully
write out the resulting Hamiltonian system.
c. The two-body problem results in part b when N = 2. Use a computer
simulation to plot solutions to the two-body problem.
2.9.
Use the Hamilton‚ÄìJacobi method to solve the initial value problems
‚àÇœÜ
‚àÇt + x‚àÇœÜ
‚àÇx = 0,
œÜ(0, x) = x,
where ‚àí‚àû< x < ‚àû
and
‚àÇœÜ
‚àÇt + x ¬∑ ‚àáœÜ = 0,
œÜ(0, x) = g(x),
where x is in EN.
2.10.
Let U(x) = x4/4 ‚àíax2/2 + bx + c.
a. Describe the solutions of the diÔ¨Äerential equation
d2x
dt2 + Ux(x) = 0
for all choices of a, b, and c, using the methods of Sections 2.3.2 and
2.4.3.
b. Compute and plot the solutions of van der Pol‚Äôs equation and the
lock-washer oscillators in Section 2.4.3 for Œµ = 0.1. Also, identify the
underlying cusp surface.
2.11.
Let f be a single-valued function mapping the set S = {1, . . . , N}
into itself.
a. Show that the iterates of f, say f j, deÔ¨Åne a semigroup using the
operation of function composition: That is, show that f if j = f i+j.
b. Show that the spectrum of the graph transition matrix for f consists
of zeros and roots of unity. Relate the corresponding eigenvectors to
the invariant sets of f.
c. Perform a Monte Carlo simulation of the matrix M associated with
the function
g(x) = 3.54x(1.0 ‚àíx)
as in Section 2.5.1.

88
2. Dynamical Systems
2.12. a. Show that the function œÜ deÔ¨Åned in Section 2.5.2 deÔ¨Ånes a circle
mapping. In particular, show that the solutions x(t) and Œæ(t) of the
diÔ¨Äerential equation
dx
dt = f(t, x),
where f is doubly periodic and where the initial data satisfy
Œæ(0) is arbitrary and x(0) = Œæ(0) + 2œÄ
satisfy
x(t) = Œæ(t) + 2œÄ
for all t.
b. Show that the residues of x(2œÄn) modulo 2œÄ satisfy the iteration
yn+1 = œÜ(yn)
(mod 2œÄ).
Also, show how to use the sequence yn to reconstruct the sequence
xn. (Hint: x0 ‚àíy0 = 2œÄM for some integer multiple M.)
c. Show that if Œ± is rational, then all iterates deÔ¨Åned by
yn+1 = yn + 2œÄŒ±
are periodic. Show that if Œ± is irrational, then all iterates are dense
in the circle 0 ‚â§y < 2œÄ.
d. Consider the mapping r exp(iŒ∏) ‚Üí2r exp(2iŒ∏)/(1+r). Show that the
circle r = 1 is invariant for this mapping; in fact, show that all points
in the plane (except r = 0) approach this circle under iterations of
this mapping. Show that there are inÔ¨Ånitely many distinct periodic
solutions on the circle. [Hint: The mapping restricted to the circle
r = 1 deÔ¨Ånes the circle mapping Œ∏n+1 = 2Œ∏n (mod 2œÄ).] The set
r = 1 is a strange attractor for this system.
2.13.
Reproduce Figure 2.27.
2.14. a. (Routh‚ÄìHurwitz Criterion for Stability.) Show that if all of the coef-
Ô¨Åcients of a polynomial are positive, then all of its roots must have
negative real parts [10].
b‚àó. The diÔ¨Äerential‚ÄìdiÔ¨Äerence equation dx/dt = ‚àíax(t‚àí1) can be solved
using Laplace‚Äôs transform. Show that the characteristic equation in
this case is p = ‚àíae‚àíp. Consider the roots for p of the function
p + ae‚àíp. Show that sin(œÄt/2) solves the problem when a = œÄ/2.
Show that for a < œÄ/2 all roots have negative real part. Describe how
the set of roots changes as a increases beyond œÄ/2. (Hint: pep +a = 0.
Let p = œÉ + iœÑ, and derive equations relating a, œÉ, and œÑ.)
2.15. a. Describe all solutions of the Hamiltonian system
dz
dt
=
‚àíF(y)
dy
dt
=
F(z),
where F(y) = y(1 ‚àíy)(y ‚àía) and a is a Ô¨Åxed number 0 < a <
1. (Hint: Show that G(z) + G(y) is constant along solutions, where
G‚Ä≤(z) = F(z).)

2.7. Exercises
89
b. Discretize this model using the semi-implicit method described in Sec-
tion 2.6.2. Show that the transformation T deÔ¨Åned by this iteration
satisÔ¨Åes det(T) = 1. Show that this implies that the transformation
is area preserving.

3
Stability Methods for Nonlinear
Systems
A mechanical or an electrical device can be fabricated to a level of accuracy
that is restricted by technical or economic constraints. What happens to the
output if the construction is a little oÔ¨ÄspeciÔ¨Åcations? Does output remain
near design values? How sensitive is the design to variations in fabrication
parameters?
Stability theory gives some answers to these and similar questions. The
ultimate concept of stability developed here is stability under persistent
disturbances. Much of our later work in perturbation theory is based on
this idea. However, linear stability theory is the most convenient concept
to apply. It is based on a study of small deviations from a design state,
and so it reduces the problem to a linear problem for small deviations. The
methods of Chapter 1 can be used once this is done.
Sometimes the linear analysis carries little or no useful information. For
example, if large deviations must be considered, then the linear analysis
is unreliable, or the small deviations problem might have neutral stability
properties that do not carry over directly to the full model. In some cases,
the linear problem is unstable but nonlinearities capture growing modes
with the result being that the design output is still stable. In these cases,
other methods based on energy considerations are useful, and Liapunov‚Äôs
theory is the one developed and used here.

92
3. Stability Methods for Nonlinear Systems
3.1
Desirable Stability Properties of Nonlinear
Systems
Consider a system of diÔ¨Äerential equations of the form
dx
dt = f(t, x),
where x, f ‚ààEN, the components of f are smooth functions of their argu-
ments, and as before, t is a real variable. Assume throughout this section
that the components of f are at least continuous in t and twice continuously
diÔ¨Äerentiable with respect to the components of x.
Suppose that there is a solution of this equation, say x = œÜ(t), that exists
for all future times, say t0 ‚â§t < ‚àû. Deviations from œÜ are described by
the variable y = x ‚àíœÜ. With this the equation becomes
dy
dt = F(t, y),
where F(t, y) = f[t, y + œÜ(t)] ‚àíf[t, œÜ(t)]. This new equation has y = 0 as a
solution. In this way, the study of a solution x = œÜ(t) is reduced to the study
of the static state y = 0 of an associated problem. For the convenience of
notation, we return to the original notation and suppose that f(t, 0) = 0
for all t, t0 ‚â§t < ‚àû.
The question asked in this section is: How do solutions beginning near
x = 0 behave? We hope that they stay near x = 0 for all time, or even
approach it as t ‚Üí‚àû, and the methods developed in this section provide
some useful ways to determine this.
We begin with a series of deÔ¨Ånitions. Let x(t, t0, x0) denote the solution
of the system
dx
dt = f(t, x)
that satisÔ¨Åes the initial conditions
x(t0, t0, x0) = x0.
Since f(t, 0) = 0, we see that x(t, t0, 0) ‚â°0.
1. The solution x ‚â°0 is said to be stable if given any tolerance Œµ > 0
and any initial time t0, there is a constraint Œ¥(Œµ, t0) > 0 such that
|x0| < Œ¥ implies that x(t, t0, x0) exists for t0 ‚â§t < ‚àûand satisÔ¨Åes
|x(t, t0, x0)| < Œµ
for all t ‚â•t0. Thus, a solution will stay near x = 0 if it starts nearby.
2. The solution x ‚â°0 is said to be asymptotically stable if it is stable
and if there is a constraint Œ¥1(Œµ, t0) > 0 such that |x0| < Œ¥1 implies
that
|x(t, t0, x0)| ‚Üí0

3.1. Desirable Stability Properties of Nonlinear Systems
93
as t ‚Üí‚àû. Thus, x = 0 is stable, and solutions starting near it will
approach it.
3. The solution x ‚â°0 is said to be uniformly asymptotically stable (UAS)
if it is stable with its constraint Œ¥ being independent of t0 and if given
a tolerance Œµ > 0 there is a number T(Œµ) such that t ‚àít0 > T(Œµ)
implies that
|x(t, t0, x0)| < Œµ.
Thus, x approaches zero as t ‚àít0 ‚Üí‚àû, uniformly in t0 and in x0.
4. The solution x ‚â°0 is said to be exponentially asymptotically stable
(EAS) if there are constants Œ¥, K, and Œ±, all positive, such that
|x0| < Œ¥ implies that
|x(t, t0, x0)| ‚â§Ke‚àíŒ±(t‚àít0)|x0|
for all t ‚â•t0.
5. The solution x ‚â°0 is said to be stable under persistent disturbances
(SPD) if given a tolerance Œµ there are constraints Œ¥1 and Œ¥2 such that
(a) for any initial state Œ∑ satisfying |Œ∑| < Œ¥1 and (b) for any admissible
perturbing function g(t, x), satisfying |g(t, x)| < Œ¥2 for t0 ‚â§t < ‚àû
and for x near x = 0, the solution of the perturbed equation
dy
dt = f(t, y) + g(t, y)
that satisÔ¨Åes the initial condition
y(t0) = Œ∑
meets the tolerance
|y(t)| < Œµ
for t0 ‚â§t < ‚àû. A perturbation g(t, x) is admissible if it is continu-
ously diÔ¨Äerentiable with respect to the components of x and Lebesgue
integrable in t.
Stability under persistent disturbances, sometimes called total stability
[57], plays a central role in perturbation theory, as shown in later chapters.
It states that adding a small, but possibly very erratic, perturbation to the
original system might change the solutions, but it does not change them
much: They still remain near x = 0. The condition that g be measurable
in t is quite a mild one, but it can be relaxed still further [24].
The domain of attraction of an asymptotically stable state is the set of
all initial values x0 such that x(t, t0, x0) approaches the state as t ‚Üí‚àû.
These deÔ¨Ånitions are used in many contexts. Stability and asymptotic
stability are not strong properties, in the sense that they are not preserved
under small changes in the system. However, EAS, UAS, and SPD are quite

94
3. Stability Methods for Nonlinear Systems
strong ideas and will be developed further in this chapter. Note, though,
that asymptotic stability in a system that is time-invariant or periodic in
t implies UAS. Of course, a solution that is EAS is also UAS [6]. First, we
study systems that are exponentially asymptotically stable.
3.2
Linear Stability Theorem
Taylor‚Äôs expansion of f(t, x) about x = 0 can be used to derive a linear
problem for small deviations from x = 0. Since f(t, 0) = 0, expanding f
about x = 0 gives the formula
f(t, x) = A(t)x + G(t, x),
where A is the Jacobian matrix having components
Ai,j(t) = ‚àÇfi
‚àÇxj
(t, 0)
for i, j = 1, . . . , N and G is the remainder in Taylor‚Äôs formula, so for each
t there is a constant K1 such that
|G(t, x)| ‚â§K1|x|2.
We denote this property by writing G(t, x) = O(|x|2) as x ‚Üí0 for each
t; the notation O(|x|2) is explained fully in Chapter 5. Now, suppose that
this estimate holds uniformly for t0 ‚â§t < ‚àû(i.e., K1 does not depend on
t). Of course, if f is a linear function of x, then we have G = 0.
Solutions of the full system near x = 0 can be studied by deriving the
linear problem and ignoring G. We show here that this is valid when the
linear problem is EAS. Let Œ¶(t) be a fundamental solution of this system:
dŒ¶
dt = A(t)Œ¶,
Œ¶(t0) = identity.
The connection between the linear problem and the full nonlinear problem
can be established using the variation of constants formula and treating G
as though it were known. As in Section 1.6, the result is that the diÔ¨Äerential
equation
dx
dt = A(t)x + G(t, x)
is equivalent to the integral equation
x(t) = Œ¶(t)x(t0) +
 t
t0
Œ¶(t)Œ¶‚àí1(s)G[s, x(s)]ds.
This is an important step in many studies of nonlinear problems. The
matrix Œ¶ carries information about the behavior of solutions to the linear
part of the problem, and the function G carries information about the
problem‚Äôs nonlinearity.

3.2. Linear Stability Theorem
95
We show next that when the linear problem is exponentially asymptot-
ically stable, then so is the nonlinear problem. Let us suppose that the
linear part of the problem is EAS:
Hypothesis H. There are constants K and Œ±, both positive, such that
|Œ¶(t)Œ¶‚àí1(s)| ‚â§K exp[‚àíŒ±(t ‚àís)]
for all t0 ‚â§s ‚â§t < ‚àû.
For example, this condition is satisÔ¨Åed if A is a constant matrix and all
its eigenvalues have negative real parts. There are other linear systems that
satisfy condition H as well (see Sections 1.4 and 3.3.4).
With this condition, we can state and prove the following useful theorem:
Linear Stability Theorem (See [24, 58, 27, 63]). Let condition H hold,
and suppose that G(t, x) = O(|x|2) as |x| ‚Üí0, uniformly for t0 ‚â§t < ‚àû.
Then there is a number Œ¥0 > 0 such that if |x(t0)| < Œ¥0, then the solution
emerging from this initial state exists and is unique, and it approaches
x = 0 as t ‚Üí‚àû. In fact, there is a positive constant Œ±0 such that
|x(t)| ‚â§K|x(t0)| exp[‚àíŒ±0(t ‚àít0)]
for all t ‚â•t0.
We conclude from this estimate that the solution x = 0 of the full non-
linear problem is exponentially asymptotically stable. The proof of this
theorem rests on Gronwall‚Äôs inequality, described below, which is closely
related to mathematical induction.
3.2.1
Gronwall‚Äôs Inequality
Let u(t), v(t), and w(t) be continuous (scalar) functions deÔ¨Åned for a ‚â§
t ‚â§b. Suppose that w(t) > 0 and that u(t) satisÔ¨Åes the inequality
u(t) ‚â§v(t) +
 t
a
w(s)u(s)ds
for all a ‚â§t ‚â§b. Then we can estimate u(t) in terms of v and w; namely,
u(t) ‚â§v(t) +
 t
a
w(s)v(s) exp
  t
s
w(s‚Ä≤)ds‚Ä≤

ds
for all a ‚â§t ‚â§b.
Proof. The proof of this result follows from setting
R(t) =
 t
a
w(s)u(s)ds.

96
3. Stability Methods for Nonlinear Systems
Then R‚Ä≤ = wu ‚â§wv + wR, or
d
dt

R(t) exp

‚àí
 t
a
w(s)ds

‚â§exp

‚àí
 t
a
w(s)ds

w(t)v(t).
Integrating this gives the result.
3.2.2
Proof of the Linear Stability Theorem
The integral equation
x(t) = Œ¶(t)Œ¶‚àí1(t0)x(t0) +
 t
t0
Œ¶(t)Œ¶‚àí1(s)G[s, x(s)]ds
has a unique solution on some interval, say t0 ‚â§t ‚â§t0 + Œ¥‚àó, and it can be
continued as long as the solution remains bounded [24]. Taking norms of
both sides of this equation and using condition H gives
|x(t)| ‚â§K exp[‚àíŒ±(t ‚àít0)]|x(t0)| +
 t
t0
K exp[‚àíŒ±(t ‚àís)]|G[s, x(s)]|ds.
If |x| remains less than Œ¥/K1 over an interval t0 ‚â§t ‚â§T and Œ¥ is suÔ¨Éciently
small, then we have |G(t, x)| ‚â§Œ¥|x|. Setting u(t) = |x(t)|eŒ±t, we have
u(t) ‚â§K exp(Œ±t0)|x(t0)| + Œ¥K
 t
t0
u(s)ds
for t0 ‚â§t ‚â§T. Gronwall‚Äôs inequality with v = K exp(Œ±t0)|x(t0)| and
w = Œ¥K shows that
u(t) ‚â§K exp(Œ±t0)|x(t0)| exp[Œ¥K(t ‚àít0)].
Therefore,
|x(t)| ‚â§K|x(t0)| exp[‚àí(Œ± ‚àíŒ¥K)(t ‚àít0)].
If we choose Œ¥ so small that Œ¥ < Œ±/K and restrict |x(t0)| so that K|x(t0)| <
Œ¥/K1, then x remains so small on any interval [t0, T] that this estimate is
justiÔ¨Åed. We conclude that the theorem is valid for t0 ‚â§t < ‚àûwith
Œ±0 = Œ± ‚àíŒ¥K.
This powerful result shows that if the small deviation problem is expo-
nentially asymptotically stable, then the nonlinear problem is also. Thus,
small deviations die out for the full model.
Example. The Linear Stability Theorem shows that the solution x = 0
of the scalar equation
dx
dt = ax ‚àíx3

3.2. Linear Stability Theorem
97
 x
 a
x =    a
              EAS
               EAS
x = -   a
x = 0
            unstable
           EAS
x = 0
x = 0
           UAS
Figure 3.1. Pitchfork bifurcation. The static state x = 0 is exponentially asymp-
totically stable for a < 0, uniformly asymptotically stable for a = 0, and unstable
for a > 0. As a increases through a = 0, two new static states bifurcate from
x = 0. Each is exponentially asymptotically stable.
is exponentially asymptotically stable if a < 0. Note that not all solutions
of this equation tend to zero. The static states and their stability are shown
in Figure 3.1.
3.2.3
Stable and Unstable Manifolds
Consider the time-invariant system
dx
dt = Ax + f(x),
where the constant matrix A has all eigenvalues lying oÔ¨Äthe imaginary axis.
The eigenvalues then are either stable ones (Re Œª < 0) or unstable ones
(Re Œª > 0). In this case, we say that A deÔ¨Ånes an exponential dichotomy
[109] on the space EN. This was discussed for linear problems in Chapter 1,
and we assume without loss of generality that A has a block diagonal form
A = diag(S, U),
where the matrix S has dimensions m √ó m and all of its eigenvalues are
stable, and the matrix U has dimensions n √ó n and all of its eigenvalues
are unstable. Since these account for all eigenvalues of A, m + n = N. We
break the vectors x and f into two vectors, say X and g, and Y and h,

98
3. Stability Methods for Nonlinear Systems
respectively, where X, g ‚ààEm and Y, h ‚ààEn, and we rewrite the system
as two equations
dX
dt
=
SX + g(X, Y )
dY
dt
=
UY + h(X, Y ).
Perron rewrote this system [121] as a system of integral equations in a
special way using the variation of constants formula. Consider the integral
equation
X(t)
=
exp(St)Œæ +
 t
0
exp[S(t ‚àís)]g[X(s), Y (s)]ds
Y (t)
=
‚àí
 ‚àû
t
exp[U(t ‚àís)]h[X(s), Y (s)]ds.
If Œæ is suÔ¨Éciently small, then this system of equations has a unique solution,
say X = X(t, Œæ), Y = Y (t, Œæ), which exists for 0 ‚â§t < ‚àû. The proof is
based on successive approximations, quite like those used in proving the
implicit function theorem in Section 4.1.2. The proof is presented in [24].
Gronwall‚Äôs inequality applies to this system, and it shows that
[X(t, Œæ), Y (t, Œæ)] ‚Üí(0, 0)
as t ‚Üí‚àû. Let us deÔ¨Åne a set S by
S = {[Œæ, Y (0, Œæ)] : Œæ is small}.
This set deÔ¨Ånes an m-dimensional manifold for Œæ near zero, and any solution
starting on it solves Perron‚Äôs integral equations. Therefore, it approaches
zero at an exponential rate, and S is referred to as being a stable manifold.
It can be shown that any solution starting away from S is repelled from
it at an exponential rate [24]. We summarize these results in the following
theorem:
Perron‚Äôs Theorem. Suppose that the eigenvalues of the matrix A sat-
isfy Re(Œª) Ã∏= 0. In particular, suppose that there are m eigenvalues with
Re(Œª) < 0 and n with Re(Œª) > 0, where m + n = N. Finally, suppose that
f(x) is a smooth function for x near zero and f(0) = 0. Then there is an
m-dimensional stable manifold for the system
dx
dt = Ax + f(x).
Replacing t by ‚àít in this system shows that there is an n-dimensional
stable manifold as t ‚Üí‚àí‚àû. We denote this by U, and it is referred to as
an unstable manifold for the full system.
This result shows that the stable and unstable manifolds derived for
linear problems in Chapter 1 can carry over to nonlinear systems. An inter-

3.3. Liapunov‚Äôs Stability Theory
99
esting geometrical approach to stable and unstable manifolds was proposed
by Hadamard as described in Section 2.5.4. These results can be extended
to cases where A and f depend explicitly on t, but that is not carried out
here. Finally, if some of the eigenvalues have Re(Œª) = 0, then the nonlinear
system can have a manifold of oscillatory or static solutions that is called
a center manifold [68]. This is discussed in Section 1.2.2 and in Chapter 7.
3.3
Liapunov‚Äôs Stability Theory
Linear stability theory enables us to determine the stability of critical points
by linearizing the problem near them. However, linear theory is not useful
in many problems. For example, the equation described in Figure 3.1 is
dx
dt = ax ‚àíx3.
It has x = 0 as a critical point, and if a < 0, it is asymptotically stable.
However, this also happens when a = 0, that is, when the linear part of
this equation is not exponentially asymptotically stable. Liapunov‚Äôs theory
enables us to resolve such problems.
3.3.1
Liapunov‚Äôs Functions
Liapunov generalized the idea of a gradient system by observing certain
features that can be mimicked. He derived convenient functions without
reference to the physics of the problem. These are now called Liapunov‚Äôs
functions, although sometimes they are still referred to as energy functions
even when they are not related to a physical energy. These are used to
study stability properties of nonlinear systems of diÔ¨Äerential equations.
The following deÔ¨Ånitions are facilitated by using comparison functions that
are continuous (scalar) functions, say a(u), deÔ¨Åned for u ‚â•0, monotone
increasing, and satisfying a(0) = 0.
A Liapunov function for the static state x = 0 of the system
dx
dt = f(t, x),
f(t, 0) ‚â°0,
is any smooth scalar-valued function
V (t, x) : [0, ‚àû) √ó EN ‚ÜíE1
for which there are comparison functions a, b, and c such that
1. V (t, 0) = 0 for all t ‚â•0.
2. a(|x|) ‚â§V (t, x) ‚â§b(|x|) for t ‚â•0 and x near 0.
3. ‚àÇV/‚àÇt + ‚àáV ¬∑ f(t, x) ‚â§‚àíc(|x|).

100
3. Stability Methods for Nonlinear Systems
We deÔ¨Åne another comparison function d by the formula
d(u) = c[b‚àí1(u)]
for later use.
The Ô¨Årst condition states that V vanishes at the system‚Äôs static state
x = 0. The second comparisons show that if V ‚Üí0, then |x| ‚Üí0, and
conversely. The third condition states that V is decreasing along solutions,
since the left-hand side of this inequality is simply the derivative of V along
solutions:
dV [t, x(t)]
dt
= ‚àÇV
‚àÇt + ‚àáV ¬∑ f(t, x),
where x(t) is a solution of the system.
The deÔ¨Ånition of Liapunov‚Äôs function used here is more restrictive than
necessary for many results, but it is convenient for our limited goals. Further
details of these deÔ¨Ånitions and applications of them can be found in [57],
but the following examples illustrate the basic arguments of the theory.
3.3.2
UAS of Time-Invariant Systems
Consider the time-invariant system
dx
dt = f(x),
where f(0) = 0 and the components of f are smooth functions of the
components of x for x near zero. We have the following result.
Theorem. If there is a Liapunov function for this system, then the
solution x = 0 is uniformly asymptotically stable.
Proof. This result follows from two simple arguments. First, the
fundamental theorem of calculus states that
V [x(t)] ‚àíV [x(s)] =
 t
s
dV [x(t‚Ä≤)]
dt
dt‚Ä≤,
and so
V [x(t)] ‚àíV [x(s)] =
 t
s
‚àáV [x(t‚Ä≤)] ¬∑ f[x(t‚Ä≤)]dt‚Ä≤ ‚â§‚àí
 t
s
d(V )dt‚Ä≤ < 0
for 0 ‚â§s < t < ‚àûand for any solution with x(0) Ã∏= 0. Therefore,
a(|x(t)|) ‚â§V [x(t)] ‚â§V [x(0)] ‚â§b[|x(0)|],
and so the solution x = 0 is stable.
If x(t) starts near zero but does not approach zero as t ‚Üí‚àû, then the
fact that
dV [x(t)]
dt
‚â§‚àíd (V [x(t)])

3.3. Liapunov‚Äôs Stability Theory
101
leads to the estimate
 V [x(t0)]
V [x(t)]
dV
d(V ) ‚â•t ‚àít0.
If x(t) does not approach zero, there is a number Œ¥ > 0 such that V [x(t)] ‚â•Œ¥
for all large t. Therefore,
Constant ‚â•
 V [x(t0)]
V [x(t)]
dV
d(V ) ‚â•t ‚àít0,
which eventually contradicts itself. This shows that x(t) ‚Üí0, and it follows
that x = 0 is uniformly asymptotically stable.
Massera‚Äôs Theorem ( Section 3.4) shows that the converse of this theorem
is also valid. Because of this, the idea of uniform asymptotic stability is
quite natural for nonlinear systems. The remainder of this section presents
examples of Liapunov‚Äôs functions and some of their uses.
3.3.3
Gradient Systems
Gradient systems are important in many applications [124]. These are sys-
tems whose right-hand side is the gradient of a scalar function. For example,
consider the system
dx
dt = ‚àí‚àáF(x),
where x ‚ààEN and ‚àáF is the gradient of a smooth scalar valued function
F(x). This is concise notation for the system of equations
dxi
dt = ‚àí‚àÇF
‚àÇxi
(x)
for i = 1, . . . , N. One interesting thing about such systems is that an
isolated minimum of F corresponds to an asymptotically stable rest point
for this system. In fact, if x‚àóis an isolated minimum for F, then the function
V (x) = F(x) ‚àíF(x‚àó)
is a Liapunov function.
This observation is the basis of a numerical algorithm, called the gra-
dient method, used for Ô¨Ånding extrema of functions of several variables.
Candidates for minima are among the values of x for which the gradient
of F is zero: ‚àáF(x) = 0. We have just seen that these values of x are
asymptotically stable equilibria for the dynamical system
dx
dt = ‚àí‚àáF(x).
Because of this, we can begin with an initial guess, say x0, then solve
this associated gradient system using a convenient computer package and

102
3. Stability Methods for Nonlinear Systems
trace the evolution of the solution. If the solution converges to a point, say
x‚àó, then we have located a candidate for a minimum of F. The computer
can also be used to test x‚àófor being a real minimum of F directly or
by calculating a Hessian matrix. Maxima of F are minima of ‚àíF, so the
resulting system can be solved for t ‚Üí‚àí‚àûto Ô¨Ånd maxima.
Example. Let us consider the entropy function
F(x) = ‚àí
N

k=1
xk log xk
for vectors x in the probability simplex
Œ£p = {x such that x1 + ¬∑ ¬∑ ¬∑ + xN = 1 and x1 > 0, . . . , xN > 0}.
Then the maxima of F can be found by solving the dynamical system
dxj
dt = ‚àílog xj ‚àí1.
If we use logarithms to base N, then 1 = log N, so this equation is
dxj
dt = ‚àílog Nxj.
It follows that Nxj ‚Üí1 as t ‚Üí‚àû, and so x(t) approaches a uniform
distribution, the one expected for maximum entropy.
The function F is called the entropy of the distribution x in Œ£p, and
this calculation shows that the gradient method moves the entropy toward
its maximum value, which occurs at the barycenter (all xj = 1/N) of the
simplex Œ£p.
3.3.4
Linear Time-Varying Systems
Liapunov functions can be useful in studying general linear systems. For
example, consider the linear system
dx
dt = A(t)x,
where A is a matrix of smooth and bounded functions. We ask, under what
conditions on A is the function
V (x) = |x|2
a Liapunov function for this system?
The comparison functions are a(u) = b(u) = u2, and it only remains
to evaluate the derivative of V along solutions: Since |x|2 = x‚ä§x, we have
that
d(x‚ä§x)
dt
= x‚ä§(A‚ä§+ A)x,

3.3. Liapunov‚Äôs Stability Theory
103
where A‚ä§denotes the transpose of A and x‚ä§denotes the transpose of the
column vector x (so it is a row vector). Since A‚ä§+A is a symmetric matrix,
it is diagonalizable [46]. Its spectral decomposition shows that if all of its
eigenvalues have negative real parts, then there is a positive number ¬µ such
that
x‚ä§Ax = x‚ä§(A + A‚ä§)x/2 ‚â§‚àí¬µx‚ä§x
for all vectors x. Thus, this function is negative deÔ¨Ånite if the eigenvalues
of the symmetric part of A have negative real parts. In this case we would
take c(u) = ¬µu2. The following example illustrates this result.
Example. As in Section 1.4, let
U(t) =

cos œât
‚àísin œât
sin œât
cos œât

.
Note that U ‚ä§is the inverse of U, since U ‚ä§U = I. Consider the system of
equations
dx
dt = C(t)x,
where x ‚ààE2, C(t) = U ‚ä§(t)BU(t), and
B =

a
b
c
d

.
First, C(t) and B have the same eigenvalues. Therefore, the eigenvalues of
C(t) are constants even though C itself varies with t. Second, the derivative
of |x|2 along solutions of the system is
dx‚ä§x
dt
= (Ux)‚ä§(B‚ä§+ B)(Ux) ‚â§œÅ|x|2,
where œÅ is any real number that majorizes 2Re Œª for all eigenvalues Œª
of B‚ä§+ B. Therefore, if the eigenvalues of the symmetric part of B are
stable, then the solution x = 0 is exponentially asymptotically stable (see
Exercise 1.3).
This important example shows that stability of a linear system with peri-
odic coeÔ¨Écients cannot be determined from the eigenvalues of its coeÔ¨Écient
matrix alone. Chapter 7 shows that if œâ is suÔ¨Éciently small and B is stable,
then the full system is stable.
3.3.5
Stable Invariant Sets
Liapunov‚Äôs functions can also describe the stability of invariant sets. We
say that a set S is invariant with respect to solutions of the system
dx
dt = f(x)

104
3. Stability Methods for Nonlinear Systems
if any solution x(t) of this system with y(t0) ‚ààS satisÔ¨Åes x(t) ‚ààS for all
t ‚â•t0.
For example, if x0 is a point for which the solution x = x(t, x0) that
satisÔ¨Åes the initial conditions x(0, x0) = x0 remains bounded for t ‚â•0,
then we deÔ¨Åne the œâ-limit set of this solution to be the set
œâ(x0)
=
{Œæ in EN such that there is a sequence {tn}
with tn ‚Üí‚àûand x(tn, x0) ‚ÜíŒæ}.
Thus, the œâ-limit set consists of all points that are approached by some
subsequence of values lying on the orbit starting at x = x0. It is known
that the œâ-limit set of a solution is an invariant set; that is, if Œæ ‚ààœâ(x0),
then x(t, Œæ) ‚ààœâ(x0) for all t ‚â•0 [24].
Suppose that there is a smooth function V (x) : EN ‚ÜíE1 that has the
following properties:
1. V (x) ‚â•0.
2. ‚àáV (x) ¬∑ f(x) ‚â§0.
Let S = {x ‚ààEN such that ‚àáV (x) ¬∑ f(x) = 0}. With this notation we
have the following result for this system:
Theorem. If x0 is such that x(t, x0) remains bounded for t ‚â•0, then
œâ(x0) ‚äÇS. Thus, x(t, x0) ‚ÜíS as t ‚Üí‚àû.
Proof. First, since V is bounded below and since it is monotone and
nonincreasing along solutions (V ‚Ä≤ ‚â§0), limt‚Üí‚àûV [x(t, x0)] exists, say with
value c. Second, V (Œæ) = c for all Œæ ‚ààœâ(x0). This follows from the fact that
for any such value Œæ there is a sequence tn such that x(tn, x0) ‚ÜíŒæ and
so V [x(tn, x0)] ‚ÜíV (Œæ) = c, since V is a continuous function. Finally, for
Œæ ‚ààœâ(x0), the solution x(t, Œæ) remains in œâ(x0), and since V is constant on
this set, we have that
d
dtV [x(t, Œæ)] = dc
dt = 0.
As a result, ‚àáV (Œæ) ¬∑ f(Œæ) = 0. Thus, Œæ ‚ààS. These calculations show that
œâ(x0) ‚äÇS.
As a result of this theorem, we see that the set S is a global attractor
for all solutions.
Example. Lienard‚Äôs Equation. Lienard‚Äôs equation has the form
x‚Ä≤‚Ä≤ + f(x)x‚Ä≤ + h(x) = 0,
where f and h are smooth functions. We suppose, as in Section 2.1.3, that
h behaves like a restoring force,
xh(x) > 0
for
x Ã∏= 0,

3.3. Liapunov‚Äôs Stability Theory
105
and that
H(x) =
 x
0
h(s)ds
approaches ‚àûas x ‚Üí‚àû. Here we suppose that f(x) > 0 for all values of
|x| Ã∏= 0.
Theorem. With these conditions on Lienard‚Äôs equation, the solution
x = 0 is globally asymptotically stable.
Proof. To prove this, we rewrite the equation as a Ô¨Årst-order system:
dx
dt
=
y
dy
dt
=
‚àíh(x) ‚àíf(x)y,
and we deÔ¨Åne a function V by the formula
V (x, y) = y2
2 + H(x).
V has the following properties:
1. It is a smooth function of x and y.
2. V > 0 for (x, y) Ã∏= 0.
3. dV/dt = ‚àíf(x)y2, which is negative for y Ã∏= 0.
This is not a Liapunov function for Lienard‚Äôs equation, since the derivative
of V along solutions of Lienard‚Äôs system is not strictly negative (it vanishes
whenever y = 0 for any x.)
Lemma. Every solution of Lienard‚Äôs system is bounded and approaches
the largest invariant set M in S = {(x, y) : f(x)y2 = 0}.
Proof. Since V ‚Üí‚àûas |x| ‚Üí‚àû, observation 3 of the above theorem en-
sures that all solutions are bounded. Moreover, the previous theorem shows
that all solutions approach S. It follows that the solution must approach
the largest invariant set in S. This is because the œâ-limit set of any orbit
is an invariant subset of S, and the largest invariant set is the union of all
of the œâ-limit sets.
Proof of Theorem (continued). The only invariant set for Lienard‚Äôs equa-
tion in S is x = 0, y = 0. Therefore, it is globally asymptotically stable.
This completes the proof of the theorem.
Example. As a Ô¨Ånal illustration of this result, we consider a slight
variant of van der Pol‚Äôs equation:
x‚Ä≤‚Ä≤ + x2x‚Ä≤ + x = 0.

106
3. Stability Methods for Nonlinear Systems
We have
dx
dt
=
y
dy
dt
=
‚àíx ‚àíx2y,
and
V (x, y) = x2 + y2
2
.
The derivative of this function along solutions is
dV
dt = ‚àíx2y2.
The theorem applies to this equation where S is the union of the x- and
y- axes. The only invariant subset of S is (0, 0). Therefore, all solutions
approach x = 0, y = 0 as t ‚Üí‚àû.
3.4
Stability Under Persistent Disturbances
Uniform asymptotic stability might seem to be a complicated idea, but the
next theorem shows that it ensures the existence of a Liapunov function.
Consider the system
dx
dt = f(t, x),
where x, f ‚ààEN, f is a diÔ¨Äerentiable function, and f(t, 0) = 0 for all t ‚â•t0.
Massera [108] proved the following result.
Massera‚Äôs Inverse Theorem. Suppose that x = 0 is a uniformly
asymptotically stable solution of this system and that f is a continu-
ously diÔ¨Äerentiable function of its arguments for x near x = 0 and for
t0 ‚â§t < ‚àû. Then there is a Liapunov function for this system having the
three properties listed in Section 3.3.1.
The proof of this theorem is not presented here. However, the construc-
tion of Liapunov‚Äôs function in Massera‚Äôs proof involves integration along
trajectories of the equation, and since these are not known, the proof gives
few useful hints about constructing a Liapunov function. This is one of the
major problems in the theory.
Still, this result plays an important role in stability theory: It is about
the only inverse theorem known, and it gives conditions under which we can
detect stability under persistent disturbances. This is a much overlooked,
but very important, result due to Malkin [106].

3.4. Stability Under Persistent Disturbances
107
Consider the system
dx
dt = f(t, x),
(3.1)
where f(t, 0) = 0 for all t and f is continuously diÔ¨Äerentiable in t and in the
components of x for x near 0. We refer to this as the unperturbed equation.
A Carath¬¥eodory perturbation of this equation has the form
dy
dt = f(t, y) + g(t, y),
(3.2)
where g is bounded and Lebesgue measurable with respect to t and a con-
tinuously diÔ¨Äerentiable function with respect to the components of y. Such
functions g are referred to here as Carath¬¥eodory functions. The general
question studied is: What stability conditions on System (3.1) ensure that
solutions of the perturbed Equation (3.2) lie near y = 0 for any function g
that is not too large?
There are many answers to this question, but Malkin found conditions
on Equation (3.1) that ensure that its static state x = 0 is stable under
persistent disturbances. Recall that the solution x = 0 of Equation (3.1)
is stable under persistent disturbances if given a tolerance Œµ > 0 there are
numbers Œ¥1 and Œ¥2 such that |g(t, y)| < Œ¥2 for all t and y together with
|y(0)| < Œ¥1 imply that |y(t)| < Œµ for all t, 0 ‚â§t < ‚àû, where x(t) solves
Equation (3.1) and y(t) solves Equation (3.2). Thus, if |g| is small and y
starts near x, then y remains close to x for all t. The following theorem
gives an answer to our question.
Malkin‚Äôs Theorem. Suppose that the solution x = 0 is a uniformly
asymptotically stable solution of Equation (3.1). Then it is stable under
persistent disturbances.
The proof of this result involves several steps that we do not present here
in detail. However, the following arguments present the essential ideas of
the proof.
Massera‚Äôs theorem shows that there is a Liapunov function for the sys-
tem (3.1), say V (t, x). Let us calculate its derivative along solutions of
Equation (3.2):
dV [t, y(t)]
dt
= ‚àÇV
‚àÇt + ‚àáV ¬∑ f(t, y) + ‚àáV ¬∑ g(t, y) ‚â§‚àíc(|y|) + O(Œ¥2).
This is strictly negative for y outside some neighborhood of y = 0. Hence,
by the arguments used in Section 3.3.2, we see that the solutions of Equa-
tion (3.2) approach some neighborhood of y = 0 as t ‚Üí‚àû. Specifying the
radius of this neighborhood to be Œµ, we adjust Œ¥1 and Œ¥2 to make the stable
neighborhood suÔ¨Éciently small to meet the tolerance Œµ. This is shown in
Figure 3.2.
Note that this result does not ensure that Equation (3.2) has a uniformly
asymptotically stable solution, but only that its solutions starting near zero

108
3. Stability Methods for Nonlinear Systems
|x|
a(|x|)
- c(|x|)
(a)
|y|
a(|y|)
- c(|y|)+
(b)
  Œ¥
Figure 3.2. (a) Shown here are lower bounds for V (above) and upper bounds for
dV/dt (below). The result is that when g = 0, x(t) ‚Üí0 as t ‚Üí‚àû. (b) Shown
here are lower bounds for V (above) and upper bounds for dV/dt (below) when
g Ã∏= 0. Bounds for the values of dV/dt near the origin are not known, but the
result is that y(t) approaches a neighborhood of 0 as t ‚Üí‚àû.
stay near zero. We make use of this result in some surprising ways when we
study perturbation methods. Note that the conditions of the linear stability
theorem imply that the solution x = 0 in that case is stable under persistent
disturbances.
We see in Chapter 6 that this form of stability can carry us through
bifurcation points where systems are usually not structurally stable ([55],
p. 259). Bifurcation points are extremely important in applications, since
it is at these places that there is a qualitative change in solution behavior
that is often observable in experiments. Systems are not structurally stable
near bifurcation points.
3.5
Orbital Stability of Free Oscillations
We have studied periodic solutions of conservative systems and of Lienard‚Äôs
equation in Chapter 2. In what senses are these solutions stable?
A periodic solution of a time-invariant system cannot be asymptotically
stable. To see this, consider the system
dx
dt = f(x),
where x, f ‚ààEN, and suppose that it has a (nonconstant) periodic solution,
say x = p(t), having least period T > 0. For any number a, the function
p(t + a) is also a periodic solution of this system. Moreover, if a is small, it
starts very near p(t), but the diÔ¨Äerence
p(t) ‚àíp(t + a)

3.5. Orbital Stability of Free Oscillations
109
cannot approach zero as t ‚Üí‚àû.
Because of this, a new kind of stability must be introduced, namely,
orbital stability. Although a periodic solution of a time-invariant system
cannot be asymptotically stable in the sense of Liapunov (it can be stable),
the orbit deÔ¨Åned by the periodic solution can be. In a sense, the periodic
solution can be asymptotically stable in amplitudes, and we can interpret
arc length along the orbit as being analogous to time in nonautonomous
systems.
3.5.1
DeÔ¨Ånitions of Orbital Stability
Let p(t) be a solution of least period T(> 0) of the system
dx
dt = f(x).
We deÔ¨Åne the orbit of p to be the set
‚Ñ¶(p) = {x = p(t) : 0 ‚â§t ‚â§T}.
Recall that the distance between a point Œ∑ and a set S is deÔ¨Åned to be
d(Œ∑, S) = inf{|Œ∑ ‚àíz| : z ‚ààS}.
We say that p is orbitally stable if given a tolerance Œµ > 0 there is a
number Œ¥ > 0 such that d[x0, ‚Ñ¶(p)] < Œ¥ implies that
d[x(t, x0), ‚Ñ¶(p)] < Œµ
for all t ‚â•0. Also, p is orbitally asymptotically stable if it is orbitally stable
and solutions that start near the orbit approach it as t ‚Üí‚àû. Finally, p
is orbitally stable under persistent disturbances if given a tolerance Œµ > 0,
there are numbers Œ¥1 and Œ¥2 such that for any Carath¬¥eodory function g(t, y)
with |g(t, y)| < Œ¥2 and any initial point y0 that is near the orbit, say
d[y0, ‚Ñ¶(p)] < Œ¥1, the solution y(t, y0) of
dy
dt = f(y) + g(t, y),
y(0) = y0
satisÔ¨Åes
d[y(t), ‚Ñ¶(p)] < Œµ
for all t ‚â•0.

110
3. Stability Methods for Nonlinear Systems
3.5.2
Examples of Orbital Stability
A collection of excellent practice problems for studying orbital stability is
presented by Œªœâ-systems. Consider the system
dx
dt
=
Œª(x, y)x ‚àíœâ(x, y)y
dy
dt
=
œâ(x, y)x + Œª(x, y)y,
where Œª and œâ are arbitrary functions of x and y. Polar coordinates are
introduced by setting r2 = x2 + y2 and Œ∏ = tan‚àí1(y/x). The result is that
rr‚Ä≤ = xx‚Ä≤ + yy‚Ä≤ = Œª(x, y)r2
and
r2Œ∏‚Ä≤ = xy‚Ä≤ ‚àíyx‚Ä≤ = œâ(x, y)r2.
Thus, the system becomes
dr
dt
=
Œª(r cos Œ∏, r sin Œ∏)r
dŒ∏
dt
=
œâ(r cos Œ∏, r sin Œ∏).
For example, if œâ = constant and Œª = (1‚àír), then this system has a unique
periodic orbit (r = 1), and it is orbitally asymptotically stable.
Recall the conservation equation
x‚Ä≤‚Ä≤ + Ux(x) = 0,
where the potential function U is a quartic, as shown in Section 2.3.2.
There are many periodic orbits deÔ¨Åned by this equation, and all of them
are orbitally stable. However, the separatrices that begin and end at the
saddle point x‚àóare not. These observations can be proved using the total
energy
x‚Ä≤2
2 + U(x)
as a measure of deviation.
Finally, recall that van der Pol‚Äôs equation
x‚Ä≤‚Ä≤ + (x2 ‚àí1)x‚Ä≤ + x = 0
satisÔ¨Åes the conditions of Section 2.1.3 for Lienard‚Äôs equation, so it has
a unique periodic solution (other than x = 0). It follows from the
Poincar¬¥e‚ÄìBendixson Theorem [24] that this periodic solution is orbitally
asymptotically stable.

3.5. Orbital Stability of Free Oscillations
111
3.5.3
Orbital Stability Under Persistent Disturbances
The next theorem shows how stability under persistent disturbances can
result for orbits.
Theorem. Let p(t) be an orbitally asymptotically stable solution of the
time-invariant system
dx
dt = f(x),
where f is a smooth function. Then p(t) is orbitally stable under persistent
disturbances.
The proof is not presented here [143]. However, the conditions of the
theorem ensure that the orbit ‚Ñ¶(p) has a Liapunov function. In fact, there
is a function V (t, x) that is periodic in t, V (t, x) > 0, and dV/dt < 0 for x
not in ‚Ñ¶(p). Thus, ‚Ñ¶(p) plays the role of S in the lemma in Section 3.3.5.
The result follows from a straightforward extension of Malkin‚Äôs theorem
presented in the last section.
This theorem shows that the stable oscillation of Lienard‚Äôs equation in
Section 3.3.5 is orbitally stable under persistent disturbances.
3.5.4
Poincar¬¥e‚Äôs Return Mapping
Let x = p(t) ‚ààEN be a periodic solution having least period T > 0 of
dx
dt = f(x),
where f is a smooth function of the components of the vector x ‚ààEN. We
study the deviation of solutions from p by setting u = x ‚àíp(t). Then u
satisÔ¨Åes the equation
du/dt = A(t)u + g[p(t), u],
where A is the Jacobian matrix A(t) = fx[p(t)] and g[p(t), u] = O(|u|2) as
|u| ‚Üí0 uniformly for 0 ‚â§t ‚â§T.
Let vectors q2, . . . , qN be pairwise orthogonal and orthogonal to p‚Ä≤(0).
Thus, the column vectors
p‚Ä≤(0), q2, . . . , qN
span EN. Let Y (t) be the fundamental solution of the linear equation
du
dt = A(t)u
that satisÔ¨Åes the initial conditions
Y (0) = [p‚Ä≤(0) q2 ¬∑ ¬∑ ¬∑ qN]

112
3. Stability Methods for Nonlinear Systems
given here in terms of its columns. This equation has a periodic solution,
namely, u = p‚Ä≤(t), and our choice of initial conditions ensures that p‚Ä≤(t) is
the Ô¨Årst column of Y (t).
Floquet‚Äôs Theorem shows that
Y (t) = P(t) exp(Lt),
where P(t) has period T, P(0) = Y (0), and
L = diag(0, Œõ),
where Œõ is an (N ‚àí1) √ó (N ‚àí1) matrix. The eigenvalues of L are called
the characteristic exponents of the system, and the eigenvalues of exp(LT)
are called the characteristic multipliers. Note that at least one of the char-
acteristic multipliers must have a modulus equal to 1, since the function
u = p‚Ä≤(t) is a periodic solution of the linear system. By our choice of initial
conditions for Y , we see that p‚Ä≤(t) is the Ô¨Årst column of P(t) and L has the
form shown.
The variation of constants formula shows that u satisÔ¨Åes the integral
equation
u(t) = Y (t)u(0) +
 t
0
Y (t)Y ‚àí1(s)g[p(s), u(s)]ds.
We introduce one more change of variables: Let
w = P ‚àí1(0)u.
It follows that w satisÔ¨Åes the integral equation
w(t) = P ‚àí1(0)Y (t)P(0)w(0) +
 t
0
P ‚àí1(0)Y (t)Y ‚àí1(s)g[p(s), P(0)w(s)]ds.
We consider solutions of this equation that satisfy initial conditions of the
form
w(0) = col(0, b2, . . . , bN),
and let b be the vector (b ‚ààEN‚àí1) given by the last N ‚àí1 components
of w(0). The notation ‚Äúcol‚Äù indicates that w(0) is a column vector having
the given components.
With this, we have from [24] the following lemma.
Lemma. [24] With the above notation and assumptions, for every vector
b near zero there is a unique number T(b) such that the Ô¨Årst component
of w, say w1(t), satisÔ¨Åes
w1[T(b)] = 0
and T(b) is a continuous function of b for b near zero with T(0) = T,
where T is the period of p(t).

3.5. Orbital Stability of Free Oscillations
113
 B k
 x = x(t)
 w  = 0
 B k+1
1
Figure 3.3. The return mapping: w1 = 0 deÔ¨Ånes a plane that is transverse to
solutions.
Proof. The solutions of the system near p(t) are depicted in Figure 3.3:
The solutions beginning with w1(0) = 0 lie on a plane that is orthogonal to
p(t) at the point p(0). The fact that solutions depend continuously on initial
data ensures that solutions starting with b near zero (i.e., starting near the
orbit of p) remain near the orbit for 0 ‚â§t ‚â§2T. In particular, there are
values of t near T, say t1 and t2 for which w1(t1) < 0 and w1(t2) > 0.
The intermediate value theorem shows that there is a Ô¨Årst value t, say
t‚àó, such that t‚àóis near T and w1(t‚àó) = 0. We deÔ¨Åne T(b) = t‚àó. The
continuity of T(b) follows from the fact that the solutions of the system
depend continuously on b.
The Return Mapping. T(b) is called the return time to the transversal
plane deÔ¨Åned by w1 = 0. Let col(0, Bk) denote a point on this plane. Then
w[T(Bk)] deÔ¨Ånes a new point on the plane, say col[0, Bk+1]. The mapping
Bk ‚ÜíBk+1
is called the return mapping. The fact that the return mapping is well-
deÔ¨Åned relies on the plane section being transversal to the Ô¨Çow deÔ¨Åned by
the solutions.
If the eigenvalues of the matrix Œõ have negative real parts, then it follows
that Bk ‚Üí0 as k ‚Üí‚àû. We summarize these results in the following
theorem:
Theorem. Let p(t) be a periodic solution of the equation
dx
dt = f(x)
having least period T > 0 (in particular, p is not a rest point for the system).
Suppose that f is a smooth function of the components of x. Moreover,

114
3. Stability Methods for Nonlinear Systems
suppose that N ‚àí1 characteristic multipliers of the linear system
dx
dt = fx[p(t)]x
have a modulus less than 1. Then p(t) is orbitally asymptotically stable.
The proof of this result follows from the calculations presented above
and the fact that if the vectors Bj approach zero, then the corresponding
solutions of the system, x(t, Bj), approach p(t) uniformly on the interval
0 ‚â§t ‚â§T. Important examples of how this result can be used are described
later when we discuss the bifurcation of periodic solutions in Chapter 6.
Unfortunately, this result does not apply in some important cases. For
example, consider the oscillator
x‚Ä≤‚Ä≤ + f(x) = 0,
where x is a scale and f is a smooth function. Suppose that there is a 2œÄ-
periodic solution to this equation, say x = p(t). Linearizing the equation
about this oscillation leads to the equation
u‚Ä≤‚Ä≤ + f ‚Ä≤[p(t)]u = 0,
which is a form of Hill‚Äôs equation. In this case, we know that one solution
of this equation is u = p‚Ä≤(t), and by shifting the time variable we can
take p‚Ä≤(2œÄ) = 1. It follows that one of the characteristic multipliers of this
equation has modulus 1, so the other one must also have modulus 1 (see
Section 1.4.1). Thus, the conditions of the theorem are not satisÔ¨Åed.
3.6
Angular Phase Stability
The preceding discussion of amplitude stability should not hide the fact
that phase stability is very important, too. In fact, it is surprising that
asymptotic stability of phases can occur in systems in which energy is
conserved. This result is perhaps against intuition, but we illustrate it here
by studying the clock problem formulated by Huygens in the 1600s. But
we Ô¨Årst describe a useful method for analyzing phase equations.
3.6.1
Rotation Vector Method
Consider a system of diÔ¨Äerential equations on an N-dimensional torus,
dx
dt = œâ + f(x),
where f is a continuously diÔ¨Äerentiable function that is periodic in each
component of x, say with period 2œÄ. Let œâ be nearly proportional to a
vector of integers, say
œâ = Œ±n + Àú‚Ñ¶,

3.6. Angular Phase Stability
115
where Œ± is a constant of proportionality, all the components of n are positive
integers (n1, . . . , nN), and Àú‚Ñ¶is some Ô¨Åxed vector in EN.
We deÔ¨Åne new variables
yj
=
xj
Œ±nj
Œµgj(y)
=
1
Œ±nj
(fj(Œ±n1y1, . . . , Œ±nNyN) + Àú‚Ñ¶j).
Then
dy/dt = 1 + Œµg(y),
where 1 is the N-vector of all ones and Œµ = maxj{1/(Œ±nj)} ‚â™1.
We are interested in determining conditions that will ensure that
lim
t‚Üí‚àûy1(t) : ¬∑ ¬∑ ¬∑ : yN(t) = 1 : ¬∑ ¬∑ ¬∑ : 1
To do this, we consider the problem in the probability simplex Œ£p. We
deÔ¨Åne the components of the phase distribution vector to be
pj(t) =
xj(t)/nj
N
k=1 xk(t)/nk
.
These ratios all approach 1/N as t ‚Üí‚àûif and only if the entropy function
H(p) = ‚àí
1
log N
N

j=1
pj log pj
approaches a maximum value (H = 1.0). Equivalently,
x1 : x2 : ¬∑ ¬∑ ¬∑ : xN ‚Üín1 : n2 : ¬∑ ¬∑ ¬∑ : nN
as t ‚Üí‚àû. Thus, H, which is the entropy of the distribution p, provides a
useful measure of synchronization of the oscillators.
Let v = N
k=1 yk. Then pj = yj/v, and
dv
dt = N(1 + Œµ¬Øg(vp)),
where ¬Øg(y) = (1/N) N
k=1 gk(y). Moreover,
v dpj
dt = 1 ‚àípjN + Œµ(gj ‚àíNpj¬Øg(vy)).
Next, we set
q = v(Np ‚àí1).
With this,
dqj
dt = Àôv(Np ‚àí1) + vn Àôp = ŒµN(gj ‚àí¬Øg).

116
3. Stability Methods for Nonlinear Systems
Therefore,
dqj
dv = Œµ(gj ‚àí¬Øg)
1 + Œµ¬Øg
= Œµ

gj ‚àí1
N
N

k=1
gk
  1
N + q
N

+ O(Œµ2).
We return to this calculation in Chapter 7, where the method of averaging
will be used to Ô¨Ånish the calculation.
The rotation vector method [75] comprises a collection of stability results
about this system. The central idea is that if the functions qj deÔ¨Åned by
these equations remain bounded as v ‚Üí‚àûand if v ‚Üí‚àûas t ‚Üí‚àû, then
the formula
qj(t) = v(Npj(t) ‚àí1),
in the limit t = ‚àû, yields that the relative ratios of the components of x
approach those of œâ:
x1 : ¬∑ ¬∑ ¬∑ : xN ‚Üín1 : ¬∑ ¬∑ ¬∑ : nN
as t ‚Üí‚àû. The right-hand side of this limit is referred to as a rotation
vector, in analogy with the rotation number of Denjoy [75, 30].
In later chapters we derive useful conditions under which the system for
q is stable under persistent disturbances. The result that
x1 : ¬∑ ¬∑ ¬∑ : xN ‚Üíœâ1 : ¬∑ ¬∑ ¬∑ : œâN
as t ‚Üí‚àûimplies that the system is synchronized.
If the system for q is stable under persistent disturbances, then the same
result holds for all nearby systems. This is the phenomenon of phase-locking;
namely, the phases synchronize at the same relative frequencies even though
the system is slightly changed. This is described in speciÔ¨Åc examples after
we have discussed the method of averaging in Chapter 7.
3.6.2
Huygen‚Äôs Problem
Let us consider a system of N pendulums, where the angular deviation
from rest of the jth pendulum is Œ∏j, which we suppose is not large. Then
the system is described by the equations
¬®Œ∏j + œâ2
j sin Œ∏j = ‚àíŒµFj(Œ∏, ÀôŒ∏),
where Fj describes the force exerted on the jth pendulum by the system
when it is in state Œ∏, ÀôŒ∏. This problem has been studied in detail by Korteweg
[142], and we consider here only an illustrative example. When Œµ = 0, there
is no coupling, and the jth pendulum oscillates with frequency œâj. This
corresponds to each pendulum swinging in isolation from the others. When
the pendulums are coupled, they tend to synchronize. As an example, let
us consider a lattice of oscillators having nearest neighbor interactions. To

3.6. Angular Phase Stability
117
‚àí1
‚àí0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
œâ1
œâ2
œâ3
p1
p2
p3
Figure 3.4. Synchronization for Huygens problem. For each of 25 choices for
(œâ1, œâ2, œâ3) shown on the parabola in the top Ô¨Ågure, the problem for (Œ∏1, Œ∏2, Œ∏3)
was solved using Œµ
=
0.5 and Ij
=
0.1 up to t
=
100œÄ. The ratios
pj = Œ∏j/(Œ∏1 + Œ∏2 + Œ∏3) for j = 1, 2, 3, are plotted in a probability simplex in
the lower Ô¨Ågure. Most of the values are near where p1(t) : p2(t) : p3(t) ‚âà1 : 1 : 1,
which shows that the oscillators are near synchrony.
Ô¨Åx ideas, we take
Fj(Œ∏, ÀôŒ∏) =
‚àÇ
‚àÇŒ∏j
N

k=1
cos(Œ∏k ‚àíŒ∏k+1) + Ij,
where Ij is the torque applied to the support point, and we take Œ∏N+1 = Œ∏1.
In the case of N = 3, we have
¬®Œ∏1 + œâ2
1 sin Œ∏1
=
ŒµI1 + Œµ[sin(Œ∏1 ‚àíŒ∏2) + sin(Œ∏1 ‚àíŒ∏3)]
¬®Œ∏2 + œâ2
2 sin Œ∏2
=
ŒµI2 + Œµ[sin(Œ∏2 ‚àíŒ∏3) + sin(Œ∏2 ‚àíŒ∏1)]
¬®Œ∏3 + œâ2
3 sin Œ∏3
=
ŒµI3 + Œµ[sin(Œ∏3 ‚àíŒ∏1) + sin(Œ∏3 ‚àíŒ∏2)].
We simulate this system using an interesting form of output: Select from
a uniform distribution of values of the vector œâ = (œâ1, œâ2, œâ3) in the prob-
ability simplex, and for each calculate the corresponding solution. Then,
normalize these values by setting pj = Œ∏j/(Œ∏1 +Œ∏2 +Œ∏3) and plot the results
using a real coordinates.

118
3. Stability Methods for Nonlinear Systems
Figure 3.4 shows the results of simulations of this system. In this case,
we wish to consider the ratio Œ∏1 : Œ∏2 : Œ∏3 as t ‚Üí‚àû. These can be plotted
simultaneously using triangular coordinates.
3.7
Exercises
3.1. a. Gronwall‚Äôs inequality shows that if u, w, and v are continuous
functions that are nonnegative and if
u(t) ‚â§v(t) +
 t
0
w(s)u(s)ds,
then
u(t) ‚â§v(t) +
 t
0
w(s)v(s) exp
  t
s
w(s‚Ä≤)ds‚Ä≤

ds.
Prove this by deÔ¨Åning z(t) =
 t
0 w(s)u(s)ds, showing that
dz/dt ‚â§wv + wz
and integrating this inequality. Simplify the result in the case where
v = constant.
b. The following comparison theorem is quite useful: Let x(t) solve the
integral equation
x(t) = f(t) +
 t
0
k(t ‚àís)x(s)ds,
where f and k are smooth functions, and suppose that z(t) satisÔ¨Åes
the inequality
z(t) ‚â§|f(t)| +
 t
0
|k(t ‚àís)|z(s)ds
and z(0) = |x(0)|. Show that z(t) ‚â•|x(t)| for all t.
3.2. a. Show that a solution of Perron‚Äôs integral equations in Section 3.2.3
deÔ¨Ånes a solution of diÔ¨Äerential equations in that section.
b‚àó. Show that Perron‚Äôs integral equations have a unique solution, and
that it exists for all 0 ‚â§t < ‚àû. Show that the solution of this system
approaches zero as t ‚Üí‚àû. (Hint: Use successive approximations and
Gronwall‚Äôs inequality.)
3.3.
Suppose that f(t, x) is a vector of N functions having period T in t
and being continuously diÔ¨Äerentiable. Suppose that x = 0 is a solution
of the diÔ¨Äerential equation
dx
dt = f(t, x).
That is, f(t, 0) = 0 for all t.
Show that if x = 0 is asymptotically stable, then it is uniformly
asymptotically stable.

3.7. Exercises
119
3.4.
Consider the gradient system
dx
dt = ‚àí‚àáF(x).
Suppose that x‚àóis a local minimum for F. Show that V (x) =
F(x)‚àíF(x‚àó) deÔ¨Ånes a Liapunov function for this system near x = x‚àó.
Show that this gradient system cannot have a periodic solution. [Hint:
Integrate (dx/dt, dx/dt)(t) around an orbit.]
b. Apply the gradient method to Ô¨Ånd the extrema of the function
F(x, y) = ax2 + bxy + cy2
in each of the three cases
i.
b = 0
ac > 0,
a > 0.
ii.
a = c = 0
b > 0.
iii.
b = 0
ac < 0.
Discuss the general case.
c. Show that the gradient system with potential function
F(x) = ‚àí
N

k=1
xk log xk
can be solved using the exponential integral function [1]
Ei(u) =
 u
‚àí‚àû
ev
v dv.
Show that xj = (1/N) exp[Ei‚àí1(t)], and so show that xj(t) ‚Üí1/N
as t ‚Üí‚àûfor each j = 1, . . . , N.
3.5.
Find a matrix A having a positive eigenvalue while the eigenvalues of
the symmetric part of A are negative. Use this example to show that
x‚ä§x might not deÔ¨Åne a Liapunov function for the system dx/dt = Ax.
3.6.
Show that the return mapping in Section 3.5.4 has the form
Bk+1 = exp(ŒõT)Bk + o(|Bk|).
Show that if all of the eigenvalues of Œõ have negative real parts, then
|Bk| ‚Üí0 as k ‚Üí‚àû. Show how the eigenvalues of Œõ are related to
the original system in Section 3.5.4.
3.7.
Show that if p(t) is a periodic solution of the system
dx
dt = f(x),
then the function u =
dp
dt deÔ¨Ånes a periodic solution of the linear
problem
du
dt = ‚àÇf
‚àÇx[p(t)]u.
Conclude that at least one characteristic exponent of this system
has real part zero, and so at least one characteristic multiplier has
modulus equal to one.

120
3. Stability Methods for Nonlinear Systems
3.8.
Determine the (angular) phase stability of the system
dx
dt
=
œâ + sin(x ‚àíy)
dy
dt
=
¬µ + sin(y ‚àíx).
In particular, determine the values of œâ and ¬µ for which x/y ‚Üí1.
3.9.
Consider the linear problem
Àôx = Ax,
where A is a stable matrix of constants. Show that
V (x) =
 ‚àû
0
|eAtx|2dt
deÔ¨Ånes a Liapunov function for this system. (This is the essential
argument in proving Massera‚Äôs Theorem.)

4
Bifurcation and Topological Methods
It is often possible to reduce stability and oscillation problems for dif-
ferential equations to problems for systems of algebraic or operator
equations, which can be studied using implicit function theorems and Ô¨Åxed
point-methods.
Consider a system of (algebraic) equations
F(u, Œª) = 0,
where u ‚ààEN is a vector of unknowns, Œª ‚ààEK is a vector of parameters,
and F is a vector of smooth functions, taking its values in EN. Suppose
that F has at least M + 1 continuous derivatives with respect to each of
its arguments.
The problem studied in this chapter is to solve this system for u in terms
of the parameters in Œª. We Ô¨Årst derive some implicit function theorems, and
we then describe techniques for solving (nonlinear) bifurcation equations.
These results are illustrated with examples of bifurcations of static and
periodic solutions in systems of diÔ¨Äerential equations.
Finally, we consider some geometric methods that are useful for Ô¨Ånding
solutions. Among these are Ô¨Åxed-point theorems from topology.
4.1
Implicit Function Theorems
Linear systems of equations are the easiest to solve, but even they are
usually complicated. Solutions of them are described Ô¨Årst, followed by non-

122
4. Bifurcation and Topological Methods
linear problems using implicit function methods that are based on solving
linear problems.
4.1.1
Fredholm‚Äôs Alternative for Linear Problems
Consider the equation for x ‚ààEN
Ax = f,
where A ‚ààEN√óN and f ‚ààEN. The matrix A and the vector f are given,
and we are to determine a vector x that solves this equation.
If the matrix A is invertible, then the solution is given by the formula
x = A‚àí1f.
This formula can be evaluated using the spectral decomposition of A or by
direct numerical evaluation, which may or may not be diÔ¨Écult. However,
if the matrix A is not invertible, then other problems arise. For example,
there may not be a solution.
Suppose for the remainder of this section that A has rank N ‚àíq and
that there are q linearly independent vectors, say œÜ1, . . . , œÜq, that span the
null space, or the kernel of A, which we denote by ker(A); that is, if v is
a vector for which Av = 0, then there are unique constants b1, . . . , bq such
that
v = b1œÜ1 + ¬∑ ¬∑ ¬∑ + bqœÜq.
We denote by ker(A)‚ä•the collection of all vectors that are orthogonal to
ker(A); that is,
ker(A)‚ä•= {v ‚ààEN : v ¬∑ œÜj = 0 for j = 1, . . . , q}.
The set ker(A)‚ä•is called the orthogonal complement of ker(A), or the
cokernel of A. From these deÔ¨Ånitions we have that
EN = ker(A) ‚äïker(A)‚ä•.
This means that any vector v in EN can be written in a unique way as
v = c1œÜ1 + ¬∑ ¬∑ ¬∑ + cqœÜq + w,
where w ‚ààker(A)‚ä•and c1, . . . , cq are uniquely deÔ¨Åned constants.
We denote by A‚ä§the transpose of A, and by ker(A‚ä§) and ker(A‚ä§)‚ä•the
kernel of A‚ä§and its orthogonal complement, respectively.
Fredholm‚Äôs Alternative is usually stated in the following way [28].
Fredholm‚Äôs Alternative. Either the equation
Ax = f

4.1. Implicit Function Theorems
123
has a unique solution for any choice of the forcing vector f, or the
homogeneous equation
Ax = 0
has a nonzero solution.
However, the alternative has come to mean more in the current literature.
Figure 4.1 describes decompositions of EN that A and A‚ä§deÔ¨Åne, and so
it tells a great deal about solving for x.
Fredholm‚Äôs Alternative comprises the following results, summarized in
Figure 4.1.
1. The matrix A maps the set ker(A) into 0, since all of the vectors in
ker(A) are null vectors.
2. A maps the set S = ker(A)‚ä•one-to-one onto the set T = ker(A‚ä§)‚ä•.
That is, if f ‚ä•ker(A‚ä§), then there is a unique vector in ker(A)‚ä•, say
w‚àó, such that Aw‚àó= f.
3. In order for there to be a solution to the equation Ax = f, f must
lie in ker(A‚ä§)‚ä•. In that case, the general solution has the form
x = c1œÜ1 + ¬∑ ¬∑ ¬∑ + cqœÜq + w‚àó,
where c1, . . . , cq are arbitrary constants, but w‚àóis uniquely deter-
mined by f.
Therefore, Fredholm‚Äôs Alternative establishes a useful decomposition of
EN into the null space of A‚ä§and its complement. Since A does not map
into any part of ker(A‚ä§), the equation cannot be solved if part of f lies in
this set. This can be seen simply by Ô¨Ånding vectors œà1, . . . , œàq that span
ker(A‚ä§) and observing that if there is a solution (say, Ax = f), then
œàj ¬∑ f = œàj ¬∑ Ax = A‚ä§œàj ¬∑ x = 0
for j = 1, . . . , q. The equations
œàj ¬∑ f = 0
for j = 1, . . . , q are called the solvability conditions for this equation. It
is necessary that f satisfy these conditions if there is to be a solution. In
other words, they ensure that f is in ker(A‚ä§)‚ä•.
Proof of Fredholms Alternative. Result 1 is obvious, and the Ô¨Årst part of
result 3 was just proved. It remains to show that A deÔ¨Ånes a one-to-one
mapping of S = ker(A)‚ä•onto T = ker(A‚ä§)‚ä•. First, if w and w1 are in S
and if Aw = Aw1, then A(w ‚àíw1) = 0, so w ‚àíw1 ‚ààker(A). Since S is
a linear subspace of EN and since 0 is the only element common to both
ker(A) and S, it must be that w‚àíw1 = 0. Therefore, A deÔ¨Ånes a one-to-one
mapping of S into T.

124
4. Bifurcation and Topological Methods
Now suppose that f ‚ààker(A‚ä§)‚ä•. We must show that there is a vector
w‚àó‚ààS such that Aw‚àó= f. This can be done by actually constructing an
approximate solution for general f.
Least Squares Solution of Ax = f.
We Ô¨Årst ask for a vector x that minimizes the norm of Ax ‚àíf:
Œ∑ = (Ax ‚àíf) ¬∑ (Ax ‚àíf) = |Ax ‚àíf|2 = min .
If there is one, it is called the least-squares solution, and we denote it by
xLSQ. The value for xLSQ is to be found among the values for which the
gradient of this norm vanishes. Since Œ∑ = (A‚ä§Ax ‚àí2A‚ä§f) ¬∑ x + f ¬∑ f, we
see that ‚àÇŒ∑/‚àÇxj = 0 for j = 1, . . . , N when
A‚ä§Ax = A‚ä§f.
This system has certain advantages over the original one: The matrix on
the left-hand side is now symmetric, so a spectral decomposition of A‚ä§A
is available and it is useful. Also, the forcing vector A‚ä§f satisÔ¨Åes the
solvability conditions.
The matrix A‚ä§A is a symmetric matrix of rank N ‚àíq and nullity q.
Therefore, there is an orthogonal matrix P(P ‚ä§P = PP ‚ä§= I) having the
form
P = (œÜ1, . . . , œÜq, Œæq+1, . . . , ŒæN),
where the column vectors œÜ1, . . . , œÜq span ker(A) such that P diagonalizes
A‚ä§A; that is,
P ‚ä§A‚ä§AP =
 0
0
0
D

,
(4.1)
where D is a diagonal matrix, say D = diag(dq+1, . . . , dn) [46]. We have
det D Ã∏= 0, since the rank of D is N ‚àíq. Note that the columns of AP are
(0, . . . , 0, AŒæq+1, . . . , AŒæN).
In fact, if we denote the ith column of the matrix AP by (AP)i, then
(AP)i = 0 if i = 1, . . . , q, but (AP)i = AŒæi for i = q + 1, . . . , N. Moreover,
from Equation (4.1) we see that
(AP)i ¬∑ (AP)j = (AP)‚ä§
i (AP)j = 0
if i Ã∏= j or if i = j ‚â§q
and
(AP)j ¬∑ (AP)j = dj
if j > q.
The vectors (AP)q+1, . . . , (AP)N span ker(A‚ä§)‚ä•, since they are N ‚àíq
linearly independent elements of that subspace. Note that (Œæq+1, . . . , ŒæN)
are eigenvectors of A corresponding to nonzero eigenvalues.
Next, we deÔ¨Åne z = P ‚ä§x. With this, the equation becomes
P ‚ä§A‚ä§APz = P ‚ä§A‚ä§f.

4.1. Implicit Function Theorems
125
Hence,
 0
0
0
D

z = (AP)‚ä§f = (0, . . . , 0, AŒæq+1, . . . , AŒæN)‚ä§f.
Thus, the Ô¨Årst q components of the right-hand side are zero, and the last
are given by (AŒæj)‚ä§f for j = q + 1, . . . , N.
The Ô¨Årst q equations show that z1, . . . , zq are free. The last N ‚àíq
equations show that zj = (AŒæj) ¬∑ f/dj. Therefore,
xLSQ =
q

i=1
ziœÜi +
N

i=q+1
(f ¬∑ AŒæi) Œæi
di
.
This vector solves the equation
A‚ä§AxLSQ = A‚ä§f.
Next, let
fR = f ‚àí
q

j=1
Cjœàj,
where Cj = f ¬∑ œàj. The vector fR is the part of f that lies in ker(A‚ä§)‚ä•.
We have the following lemma.
Lemma. AxLSQ = fR.
Proof. The vectors AŒæq+1, . . . , AŒæN, are orthogonal, and they span
ker(A‚ä§)‚ä•. Therefore, fR can be expanded in terms of them:
fR =
N

i=q+1
fiAŒæi,
where fj = fR ¬∑ AŒæj/dj. Since
xLSQ =
q

i=q
ziœÜi +
N

i=q+1
(f ¬∑ AŒæi) Œæi
di
,
we have that
AxLSQ = A
N

i=q+1
(f ¬∑ AŒæi) Œæi
di
=
N

i=q+1
(f ¬∑ AŒæi)AŒæi
di
= fR.
In summary, we see that the equation
Ax = f

126
4. Bifurcation and Topological Methods
has a solution if and only if f ‚ààker(A‚ä§)‚ä•. In that case, the solution is
given by the formula
xLSQ =
q

i=1
ziœÜi +
N

i=q+1
(f ¬∑ AŒæi) Œæi
di
,
where z1, . . . , zq are free constants. In particular, if f = fR, then there is
a unique w‚àóin ker(A)‚ä•such that Aw‚àó= f. This completes the proof of
result 2 of Fredholm‚Äôs Alternative, and the second part of 3.
It is tempting to use the least-squares method as the basis for a nu-
merical scheme for Ô¨Ånding xLSQ. However, there are signiÔ¨Åcant accuracy
problems in this approach, and more sophisticated decomposition methods
are usually used [21].
4.1.2
Nonlinear Problems: The Invertible Case
Suppose that u = u0 (in EN) solves the equation
F(u0, Œª0) = 0
for some choice of the parameters Œª0 in EK. Without loss of generality, we
can assume that u0 = 0 and Œª0 = 0, since a simple translation of variables
(u ‚Üíu ‚àíu0, Œª ‚ÜíŒª ‚àíŒª0) gives a new system for which this is true. Thus,
we suppose that
F(0, 0) = 0.
Recall that the components of F are assumed to be at least M + 1 times
continuously diÔ¨Äerentiable with respect to the components of u and of Œª.
We look only for small solutions of this system for small values of Œª; that
is, we seek solutions that are near u = 0 for parameters near Œª = 0. We
begin by expanding the equation about this known solution:
F(u, Œª) = C(Œª) + A(Œª)u + G(u, Œª),
where C(Œª) = F(0, Œª), A(Œª) = Fu(0, Œª) is the Jacobian matrix, and G(u, Œª)
is the remainder. There is a constant K such that
|G(u, Œª)| ‚â§K|u|2
for all small values of |u| and |Œª|. We consider here the case where A(Œª) is
invertible, and, in the next section, the case where it is not.
Hypothesis HI. A(0) is an invertible matrix.
With this we have the following result.
Implicit Function Theorem. Suppose that condition HI and the above
conditions are satisÔ¨Åed. Then there is a unique small solution, say u = u(Œª),

4.1. Implicit Function Theorems
127
of the equation
F(u, Œª) = 0
for Œª near zero. In fact, there are functions Uj(Œª), homogeneous poly-
nomials in the components of Œª of degree j, for j = 1, . . . , M, such
that
u(Œª) =
M

i=0
Ui(Œª) + O(|Œª|M+1).
The functions Uj have the form
Uj(Œª) =

k1+¬∑¬∑¬∑+kK=j
U(j, k1, . . . , kK)Œªk1
1 ¬∑ ¬∑ ¬∑ ŒªkK
K ,
where the sum is taken over nonnegative indices k1, . . . , kK that satisfy the
constraint.
Proof of the Implicit Function Theorem. The proof is accomplished in
three steps.
Step 1: Determine the functions Uj recursively from the equation. Ob-
viously, U0 = 0, since we are looking for small solutions. Next, U1(Œª) =
‚àíA‚àí1GŒª(0, 0)Œª, etc. These coeÔ¨Écients give Taylor‚Äôs expansion of the
expected solution.
Step 2: Let v = u‚àíM
j=0 Uj(Œª), and derive an equation for the remainder
v. The result has the form
v = u ‚àí
M

i=0
Ui(Œª) =
‚àí
A‚àí1

G

v +
M

i=0
Ui(Œª), Œª

‚àíG
 M

i=0
Ui(Œª), Œª

+
O(|Œª|M+1).
Thus, the remainder v solves the equation
v = H(v, Œª),
where H is a smooth function with
H = O(|v|2) + O

|Œª||v|) + O(|Œª|M+1
as |v| ‚Üí0 and |Œª| ‚Üí0.
Step 3: Finally, show that there is a unique solution for v and that v =
O(|Œª|M+1) as |Œª| ‚Üí0. This is done by deÔ¨Åning a sequence of successive
approximations to v by v0 = 0, and for j = 1, 2, . . . , vj = H(vj‚àí1, Œª).
This deÔ¨Ånes a Cauchy sequence (see Section 4.4.1), and so it converges
to a solution of the equation. To see that this solution satisÔ¨Åes the error
estimate, we note that v1 does, and that each further approximation does.
In fact, we have that each vj satisÔ¨Åes
|vj| ‚â§KŒ¥|vj‚àí1| + K|Œª|M+1

128
4. Bifurcation and Topological Methods
for some constant K, all small Œª, and vj‚àí1 satisfying |Œª| + |vj‚àí1| ‚â§Œ¥.
Therefore,
|v1|
‚â§
K|Œª|M+1
|v2|
‚â§
KŒ¥|v1| + K|Œª|M+1,
and, by a straightforward induction,
|vj| ‚â§K(1 + r + r2 + ¬∑ ¬∑ ¬∑ + rj‚àí1)|Œª|M+1,
where r = KŒ¥. It follows that v satisÔ¨Åes the estimate
|v| ‚â§K|Œª|M+1
1 ‚àír
= O

|Œª|M+1
.
4.1.3
Nonlinear Problems: The Noninvertible Case
Now suppose that the linear part of the problem is not invertible.
SpeciÔ¨Åcally, suppose the following condition:
Hypothesis HNI: A(0)x = 0 has q linearly independent solutions for
x.
With this, ker[A(0)] is spanned by some vectors, œÜ1, . . . , œÜq, and
ker[A(0)‚ä§] is spanned by œà1, . . . , œàq. We suppose that these are biorthog-
onal sets with
œàj ¬∑ œÜk = Œ¥j,k
for j, k = 1, . . . , q, where Œ¥j,k is Kronecker‚Äôs delta function (Œ¥j,k = 1 if
k = j, = 0 otherwise).
As in Section 4.1, we let
u = c1œÜ1 + ¬∑ ¬∑ ¬∑ + cqœÜq + w,
where c1, . . . , cq are arbitrary constants and w is a vector that is orthogonal
to the null space of A(0), that is, w ¬∑ œÜj = 0 for j = 1, . . . , q.
The equations to be solved become
A(0)w
=
H(c1, . . . , cq, w, Œª)
‚â°
‚àíC(Œª) ‚àí[A(Œª) ‚àíA(0)]w ‚àíG(c1œÜ1 + ¬∑ ¬∑ ¬∑ + cqœÜq + w, Œª).
Here and below we write A for A(0), and we use the matrix notation Œ¶c =
c1œÜ1+¬∑ ¬∑ ¬∑+cqœÜq, where Œ¶ is an N √óq matrix whose columns are the vectors
œÜ1, . . . , œÜq and c is the column vector whose components are c1, . . . , cq.
The invertible form of the implicit function theorem can be used to prove
the existence of a unique function w‚àó(c1, . . . , cq, Œª) that solves the equation
A‚ä§Aw = ‚àíA‚ä§H(c1, . . . , cq, w, Œª)

4.2. Solving Some Bifurcation Equations
129
ker(  )
      A
      A
ker(     )
tr
    = ker(  )
 S          A  
T           A  
   = ker(     )
tr
A
Figure 4.1. Diagram of the Fredholm Alternative.
and is orthogonal to the null space of A. This is the least-squares version of
the original equations. The solution w‚àóis a solution if the original system
of the solvability conditions
œàj ¬∑ H[c1, . . . , cq, w‚àó(c, Œª), Œª] = 0
are satisÔ¨Åed for j = 1, . . . , q. This fact is a restatement of Fredholm‚Äôs
Alternative. These q equations for the q unknowns c1, . . . , cq, are called
the bifurcation equations. Each small solution of them for small Œª gives a
small solution of the original problem, namely,
u = Œ¶c + w‚àó(c, Œª).
This approach has been extended to study quite sophisticated problems
in functional analysis, and it is now referred to as the Liapunov‚ÄìSchmidt
Method [135].
Figure 4.1 is important to keep in mind, since it describes where w‚àó
and the bifurcation equations come from. The combinations of c1, . . . , cq
are selected by the solvability conditions to ensure that H lies in T =
ker(AT )‚ä•. This may or may not be possible. If so, then the invertible form
of the implicit function theorem is used in the portion of EN where A is
invertible to Ô¨Ånd w‚àó(c, Œª) in S = ker(A)‚ä•. It is straightforward to derive
w‚àóin practice by iteration, but solving the bifurcation equations is usually
diÔ¨Écult.
4.2
Solving Some Bifurcation Equations
Finding u in the invertible case and w‚àóin the noninvertible case are usually
straightforward, and the method of iteration described in the proof gives
a reliable iterative scheme for Ô¨Ånding them. However, in the noninvertible
case, the bifurcation equations must be solved as well, and it is at this
point that the hard work begins. There are no known general methods for
doing this, but Newton‚Äôs polygons are quite useful for solving a variety of
problems.

130
4. Bifurcation and Topological Methods
4.2.1
q = 1: Newton‚Äôs Polygons
When q = 1, the solution takes the form
x = cœÜ + w‚àó(c, Œª),
and the bifurcation equation becomes the scalar equation
0 = œà ¬∑ f[cœÜ + w‚àó(c, Œª), Œª],
which can be expanded in powers of c and Œª. Therefore, we consider the
scalar equation
0 = a0,0 + a1,0c + a0,1Œª + a2,0c2 + a1,1cŒª + a0,2Œª2 + ¬∑ ¬∑ ¬∑ ,
which we want to solve for small real solutions c when Œª is near 0. Here are
some examples:
1. There may be a unique small solution
0 = c + Œªc2 + Œª2c + Œªc = c(1 + Œªc + Œª2 + c).
Here c = 0 is the only small solution for Œª near zero.
2. There may be more than one solution, and both could be an analytic
function of Œª:
0 = Œªc2 + Œª2c = Œªc(c + Œª).
Thus, c = 0 and c = ‚àíŒª are two small solutions.
3. There may be several solutions, but they are analytic in some
(fractional) power of Œª:
0 = Œªc3 ‚àíŒª2c = Œªc(c2 ‚àíŒª).
Here c = 0, c =
‚àö
Œª, and c = ‚àí
‚àö
Œª are small solutions.
These examples suggest that we look for solutions in the form
c = Œ±Œªr + higher order terms in Œª
for some amplitude Œ± and some base exponent r. Thus,
0
=
a1,0Œ±Œªr + a0,1Œª + a2,0Œ±2Œªr2 + a1,1Œ±ŒªrŒª + a0,2Œª2 + ¬∑ ¬∑ ¬∑
=
(¬∑ ¬∑ ¬∑ )Œªr‚àó+ ¬∑ ¬∑ ¬∑ ,
where
r‚àó= min{rj + k = constant: j, k = 0, 1, 2, . . . , for which aj,k Ã∏= 0}.
Terms that will balance to contribute to the coeÔ¨Écient of Œªr‚àócan be deter-
mined by Ô¨Ånding the indices (j, k) of nonzero coeÔ¨Écients for which at least
two lie on the same straight line k = ‚àírj+ constant. This can be done
graphically by plotting k vertically and j horizontally, the points (j, k) for
which aj,k Ã∏= 0. Then the straight lines bounding this set of (j, k) values

4.2. Solving Some Bifurcation Equations
131
 k(  )
 j(c)
Œª
Slope -1/2
Slope -1
Figure 4.2. Newton‚Äôs polygon for Œª2 + Œªc ‚àíc3 + c4 + Œª2c2. There are two relevant
slopes.
from (0, 0) are determined, and the slopes of these lines give the possi-
ble values of ‚àír. Only lines having at least two points are of interest in
obtaining possible equations for Œ±.
For example, consider the equation
0 = Œª2 + Œªc ‚àíc3 + c4 + Œª2c2.
Figure 4.2 shows the indices and the enclosing straight lines.
From Figure 4.2, we see that the two possible cases are
c ‚âàŒª
and
c ‚âà
‚àö
Œª.
Newton‚Äôs polygon method rests on the following theorem, which shows that
corresponding to each of these scalings there is an analytic solution.
Weierstrass Preparation Theorem. Suppose that G(c, Œª) is an an-
alytic function of c and Œª near c = 0, Œª = 0, such that G(0, 0) = 0
and
‚àÇjG
‚àÇcj (0, 0) = 0
for j = 1, . . . , k ‚àí1,
but ‚àÇkG
‚àÇck (0, 0) Ã∏= 0.
Then there are analytic functions A0(Œª), . . . , Ak‚àí1(Œª), and B(c, Œª) with
B(c, Œª) Ã∏= 0 for c and Œª near zero, such that
G(c, Œª) = [ck + Ak‚àí1(Œª)ck‚àí1 + ¬∑ ¬∑ ¬∑ + A0(Œª)]B(c, Œª).
We do not prove this result here [66]. If we wish to solve the equation
G(c, Œª) = 0

132
4. Bifurcation and Topological Methods
for c as a function of Œª, then since B Ã∏= 0, the roots are determined by
solving the polynomial
ck + Ak‚àí1(Œª)ck‚àí1 + ¬∑ ¬∑ ¬∑ + A0(Œª) = 0.
There are exactly k solutions of this equation for each value of Œª near Œª = 0.
Moreover, each root is an analytic function of some (fractional) power of
Œª.
If in the example depicted in Figure 4.2 we set
c = Œ±
‚àö
Œª + BŒª + ¬∑ ¬∑ ¬∑ .
Then
Œ± ‚àíŒ±3
=
0
(1 ‚àí3Œ±2)Œ≤ + (1 + Œ±4)
=
0,
and so on. This shows that Œ± = ¬±1, 0 are the possible candidates for
starting; thereafter, all coeÔ¨Écients are uniquely determined. We will apply
this method in the next examples.
4.3
Examples of Bifurcations
The examples presented in this section illustrate important bifurcation phe-
nomena that arise in many applications. The Ô¨Årst is a system that exhibits
the classical cusp bifurcation of static states, and the second illustrates a
Hopf bifurcation of a periodic solution.
4.3.1
Exchange of Stabilities
Consider the system of diÔ¨Äerential equations
dx
dt
=
Œªx ‚àíayx
dy
dt
=
‚àíby + dyx + fx2,
where a, b, d, and f are positive constants. Small static states of these equa-
tions are to be found by solving the equations dx/dt = 0, dy/dt = 0 for x
and y when Œª is near zero. In this case the matrix A(Œª) = diag(Œª, ‚àíb), and
when Œª = 0, œÜ = œà = (1, 0)‚ä§. Therefore, we seek the solutions of the form
x = c, y = w, in the notation of Section 4.1.3.
The Ô¨Årst step in using the Liapunov‚ÄìSchmidt method involves solving
0 = ‚àíbw + dwc + fc2 + h.o.t.

4.3. Examples of Bifurcations
133
for w as a function of c and Œª, where h.o.t. denotes higher order terms in
c. Obviously,
w = fc2
b
+ h.o.t.
Substituting this result into the Ô¨Årst equation gives the bifurcation equation
0 = Œªc ‚àí(af/b)c3 + c(h.o.t.).
Using Newton‚Äôs polygons, we see that if Œª < 0, then c = 0 is the only
(small) real solution. If Œª > 0, then there are three possible solutions, c = 0
and c = ¬±

Œªb/af + h.o.t.
Therefore, this system has a unique small static state for Œª < 0, namely
x = y = 0, and there are three small static states for Œª > 0. An interesting
aspect of this example is that the solution x = y = 0 is exponentially stable
for Œª < 0, but it is unstable for Œª > 0. However, for Œª > 0, the two new
(nonzero) solutions are both exponentially stable. This behavior is typical
of many stability problems, and it is referred to as being an exchange of
stabilities. Note that when Œª = 0, the solution x = y = 0 is UAS but not
EAS; in fact, the system is stable under persistent disturbances through the
bifurcation, but not structurally stable. This is studied further in Chapter 8
[51, 85].
4.3.2
Andronov‚ÄìHopf Bifurcation
The following example shows how an oscillation can bifurcate from a static
state in such a way that its amplitude grows while its frequency remains
approximately constant. Consider the system
dx
dt
=
[Œª ‚àí(x2 + y2)]x ‚àíœây
dy
dt
=
œâx + [Œª ‚àí(x2 + y2)]y,
where Œª and œâ are constants. Setting r2 = x2 + y2 and Œ∏ = tan‚àí1(y/x),
this problem becomes
dr
dt
=
r(Œª ‚àír2)
dŒ∏
dt
=
œâ.
Dividing the Ô¨Årst equation by the second gives
dr
dŒ∏ = r(Œª ‚àír2)
œâ
.
Thus, the Œªœâ-system reduces easily to a scalar diÔ¨Äerential equation where
the original angle variable becomes a time-like variable. Static states for this
equation (r = r‚àó) correspond to periodic solutions of the original system.

134
4. Bifurcation and Topological Methods
We considered the same static state problem in the preceding section. For
Œª < 0, there is a unique real static state for r, namely r = 0. For Œª > 0
there are three real solutions r = 0 and r = ¬±
‚àö
Œª. The two nonzero static
states for r correspond to periodic solutions, one œÄ out of phase with the
other, so we consider only the positive root. The result for the original
system is that for Œª > 0 there is a unique periodic orbit, namely,
x =
‚àö
Œª cos(œât + œÜ),
y =
‚àö
Œª sin(œât + œÜ),
where œÜ is a free constant.
We say that as Œª increases through Œª = 0, a periodic solution bifurcates
from the static state x = 0, y = 0. Note that this static state is stable for
Œª < 0 but unstable for Œª > 0; however, the periodic solution is orbitally
exponentially stable for Œª > 0. Therefore, an exchange of stabilities can
also occur through the appearance of a periodic solution.
The Œªœâ-system provides a good example of a periodic solution bifur-
cation, and it is typical of a class of problems called Hopf bifurcation
problems. We study these later in Chapter 8 [69].
4.3.3
Saddle-Node on Limit Cycle Bifurcation
The example here shows how an oscillation can bifurcate from a static state
in such a way that the amplitude remains approximately constant while the
frequency increases. Consider the system
dx
dt
=
(R2 ‚àíx2 ‚àíy2)x ‚àí(‚Ñ¶+ x)y
dy
dt
=
(‚Ñ¶+ x)x + (R2 ‚àíx2 ‚àíy2)y,
where R and ‚Ñ¶are some constants.
The bifurcation occurs here as ‚Ñ¶/R increases through the value 1:
1. For ‚Ñ¶/R < 1, Œ∏ ‚Üícos‚àí1 ‚Ñ¶as t ‚Üí‚àû.
2. For ‚Ñ¶/R > 1, Œ∏ ‚Üí‚àûas t ‚Üí‚àû.
The oscillation that appears is close to the circle r = R, but the frequency
increases as the bifurcation point is passed. Further study of this model
shows that as ‚Ñ¶/R ‚Üí1, a stable node and a saddle point coalesce, and as
the bifurcation is passed, they annihilate each other and only the circular
orbit remains.
4.3.4
Cusp Bifurcation Revisited
The appearance and disappearance of two stable static states occurs in
most systems that exhibit hysteresis. The cusp model illustrates this.

4.3. Examples of Bifurcations
135
Consider the scalar equation
dx
dt = ‚àí(x3 ‚àíax + b).
The static states of the system are found by solving
x3 ‚àíax + b = 0
for x. There are either one or three real solutions to this equation, as shown
in Figure 2.20, where the real solutions for x are plotted as functions of
(a, b). The lines where multiple static states appear can be found in the
following way.
As shown in Section 2.4.3, the bifurcation curves are
b = ¬±2a3/2
3
‚àö
3 .
These curves in the ab-plane describe the situation as shown in Figure 2.20.
Between them there are three roots; the top and bottom ones are stable
for the diÔ¨Äerential equation. Outside of this region, there is only one real
root, and it is stable.
In the terminology of singularity theory [133], each of the branches of
the bifurcation curve describes a fold bifurcation where the surface folds
over and under itself. The cusp singularity is at the origin a = 0, b = 0,
and it describes the coming together of two folds.
4.3.5
Canonical Models and Bifurcations
One of the reasons that mathematical results are so eÔ¨Äective in so many
surprising applications is that there are certain canonical models that arise
in a wide variety of applications. A canonical model is the continuous image
of an entire class of models, and so it represents the class. Any results for
the canonical model carry over in some sense to all models of the class even
though not all of these are known [81].
For example, the neuroscientist Hodgkin observed that nerve cells can
Ô¨Åre in two diÔ¨Äerent ways as stronger currents are applied through their
membranes. He classiÔ¨Åed these as Class 1 and Class 2 excitations. In the
Ô¨Årst, an oscillation appears that has approximately constant amplitude but
increasing frequency. In the second case, an oscillation of cell membrane
voltage appears that grows in amplitude but has approximately constant
frequency. The saddle-node on limit cycle and the Andronov‚ÄìHopf models
are canonical models for these two phenomena, respectively [81]. These
same models occur in descriptions of a wide variety of other physical and
biological phenomena.
On the other hand, the cusp bifurcation occurs as a canonical model
of a codimension 2 singularity [133]. It arises later in this book in both
DuÔ¨Éng‚Äôs and van der Pol‚Äôs equations, and it signals the appearance of

136
4. Bifurcation and Topological Methods
bistable behavior. The exchange of stabilities example resides within the
cusp surface as well.
4.4
Fixed-Point Theorems
A periodically forced system has the form
dx
dt = f(t, x),
where x and f are in EN, and we suppose that f is a smooth function of
its arguments having (least) period T in t.
A solution starting at a point x(0) evolves into a point x(T) after T
units, and as in Chapter 3, we can deÔ¨Åne the return mapping, or Poincar¬¥e‚Äôs
mapping, by
P : x(0) ‚Üíx(T).
Fixed points of this mapping correspond to periodic solutions of the original
system of equations, and a study of P helps us to clarify several important
issues. The following lemma shows this.
Lemma. P has a Ô¨Åxed point if and only if the diÔ¨Äerential equation used
to deÔ¨Åne it has a solution of period T (not necessarily its least period).
Proof. If Py = y, then let the solution of the deÔ¨Åning system with initial
value x(0) = y be denoted by x(t). Let z(t) = x(t + T). Note that z(t)
satisÔ¨Åes the same diÔ¨Äerential equation as x, and that z(0) = x(T) = y.
Since solutions of this problem are unique, x and z are identical. This
shows that x(t) = x(t + T) for all t. Conversely, if x(t) is a solution having
period T, then x(0) is a Ô¨Åxed point of P.
This lemma shows that a Ô¨Åxed point of the return mapping corresponds
to a periodic solution of the original system. Therefore, we can establish
the existence of periodic solutions to the system of diÔ¨Äerential equations
by Ô¨Ånding its Ô¨Åxed points. In this section we describe four methods that
are available for doing this.
4.4.1
Contraction Mapping Principle
Suppose that F is a continuous mapping of a closed subset ‚Ñ¶of EN into
itself such that for some constant Œª, 0 ‚â§Œª < 1, we have that
|F(x) ‚àíF(y)| ‚â§Œª|x ‚àíy|
for all x and y in ‚Ñ¶. Then there is a unique Ô¨Åxed point for F in ‚Ñ¶, and it
is asymptotically stable under iterations of F.

4.4. Fixed-Point Theorems
137
The proof of this result begins with showing that the iteration sequence
{xn}, say starting from x0 and for n = 0, 1, . . . deÔ¨Åned by
xn+1 = F(xn),
is a Cauchy sequence. In fact, |xn+1 ‚àíxn| ‚â§Œª|xn ‚àíxn‚àí1|, so |xn+1 ‚àíxn| ‚â§
Œªn|x1 ‚àíx0|. It follows that for any integers k and n,
|xn+k ‚àíxn| ‚â§
 n+k

j=n
Œªj

|x1 ‚àíx0|.
Since the sum in this estimate approaches zero as n and k ‚Üí‚àû, we see
that {xn} is a Cauchy sequence. Therefore, it converges to a limit, say x‚àó.
Passing to the limit n ‚Üí‚àûin the formula
xn+1 = F(xn)
shows that x‚àó= F(x‚àó). Obviously, x‚àóis the only Ô¨Åxed point of F. Finally,
an initial condition, say y0, lying near x0, deÔ¨Ånes an iteration sequence
{yn}. Since
|yn ‚àíxn| ‚â§Œªn|y0 ‚àíx0|,
we see that yn ‚Üíx‚àóas n ‚Üí‚àû. Thus, the equilibrium is asymptotically
stable [32].
Example. A Forced Stable Linear System. The equation
dx
dt = Ax + g(t),
where A is a constant matrix having all eigenvalues in the left half-plane
and g is a vector of smooth functions having period T, has a unique periodic
solution. This can be proved using the contraction mapping principle, as
we show next.
The return mapping is deÔ¨Åned for this system as follows: An initial value
x(0) determines a unique solution of this equation, namely,
x(t) = exp(At)x(0) +
 t
0
exp[A(t ‚àís)]g(s)ds.
The return mapping therefore is
P(Œæ) = exp(AT)Œæ +
 T
0
exp[A(T ‚àís)]g(s)ds,
where we write x(0) = Œæ. Since A is a stable matrix, we have
| exp(ATm)| < 1
for suÔ¨Éciently large values of m. Therefore, P m is a contraction mapping
for suÔ¨Éciently large m. It follows that P m has a unique Ô¨Åxed point, say Œæ‚àó.

138
4. Bifurcation and Topological Methods
Also, Œæ‚àóis a unique Ô¨Åxed point of P, since for any point x(0) we have
P mk[x(0)] ‚ÜíŒæ‚àó
as k ‚Üí‚àû, and so
P(Œæ‚àó) = P[P mk(Œæ‚àó)] = P mk[P(Œæ‚àó)] ‚ÜíŒæ‚àó
as k ‚Üí‚àû. It must be that
Œæ‚àó= [I ‚àíexp(AT)]‚àí1
 T
0
exp[A(T ‚àís)]g(s)ds.
It follows that the diÔ¨Äerential equation has a unique periodic solution,
namely,
x(t) = exp(At)Œæ‚àó+
 t
0
exp[A(t ‚àís)]g(s)ds.
4.4.2
Wazewski‚Äôs Method
A mapping need not be a contraction to have a Ô¨Åxed point. In fact, recall
from calculus that every continuous function attains all values between its
maximum and minimum values. If f is a continuous function mapping a
Ô¨Ånite interval [a, b] into itself, then the graph of f (i.e., the set {[x, f(x)] :
x ‚àà[a, b]}) must intersect the one-to-one line {(x, x)}. Such an intersection
deÔ¨Ånes a Ô¨Åxed point of f.
This observation is based on the intermediate value theorem of calculus,
and it is possible to extend it to dimensions higher than one. Brouwer‚Äôs
Ô¨Åxed-point theorem does this.
Brouwer‚Äôs Fixed-Point Theorem. Suppose that F : EN ‚ÜíEN is a
continuous mapping of a closed ball, say ‚Ñ¶‚äÇEN, into itself. Then F has
a Ô¨Åxed point, say x‚àó, in ‚Ñ¶.
Note that the Ô¨Åxed point given by this result need not be unique, since
the conditions are satisÔ¨Åed by the identity mapping on ‚Ñ¶. This result is
not proved here [32].
Example: Forcing a System That Is Stable Under Persistent
Disturbances. Consider the equation
dx
dt = ‚àíx3 + A cos t.
In the tx-plane, we see that Àôx < 0 on the line x = 2A, but on the line x =
‚àí2A, Àôx > 0. Therefore, solutions starting in the interval ‚àí2A ‚â§x ‚â§2A
remain there for all future times. In particular, this interval is mapped into
itself by the return mapping. It follows from Brouwer‚Äôs theorem that the
return mapping has a Ô¨Åxed point, and so there is a periodic solution of this
equation.

4.4. Fixed-Point Theorems
139
 x
 t
 x = -2A
 x = 2A
Figure 4.3. Example of Brouwer‚Äôs theorem.
A corollary of Brouwer‚Äôs theorem is the No-Retract Theorem.
No-Retract Theorem.
There is no continuous mapping of the unit
ball (or any compact manifold with boundary) in EN onto its boundary that
leaves boundary points Ô¨Åxed.
Proof. If there were such a mapping, say G, of the set
BN = {x ‚ààEN : |x| ‚â§1},
called the unit ball in EN, onto its boundary
SN‚àí1 = {x ‚ààEN : |x| = 1},
then G followed by the antipodal mapping x ‚Üí‚àíx would deÔ¨Åne a con-
tinuous mapping of the unit ball into itself that had no Ô¨Åxed points. This
contradicts Brouwer‚Äôs Theorem, and so the No-Retract Theorem is proved.
Another interesting consequence of this theorem is useful in study-
ing nonlinear diÔ¨Äerential equations. Consider solutions of a system of N
equations
dx
dt = f(t, x)

140
4. Bifurcation and Topological Methods
that have initial values in a smooth, bounded, and simply connected domain
‚Ñ¶. Let c denote the cylinder in tx space that is deÔ¨Åned by
C = {(t, x) : 0 ‚â§t < ‚àû, x in ‚Ñ¶}.
Let ‚àÇC denote the lateral boundary of this cylinder:
‚àÇC = {(t, x) : 0 ‚â§t < ‚àû, x in ‚àÇ‚Ñ¶}.
An egress point of C is a point in ‚àÇC at which a solution of the system
leaves the interior of C. A strict egress point, say (t‚àó, x‚àó), is one where a
solution of the system lies inside C for t‚àó‚àíŒ¥ < t < t‚àóand outside C for
t‚àó< t < t‚àó+ Œ¥ for some small positive number Œ¥. With these deÔ¨Ånitions,
we can discuss Wazewski‚Äôs result:
Wazewski‚Äôs Theorem. Suppose that all egress points of the system are
strict egress points. Then there is a solution of the system that begins in ‚Ñ¶
and remains in C for all t ‚â•0.
Proof. If such a point did not exist, then every point in ‚Ñ¶deÔ¨Ånes an
orbit that eventually leaves C. We deÔ¨Åne a mapping of ‚Ñ¶into ‚àÇC by the
solutions: If p ‚àà‚Ñ¶and x(t, p) is the solution emanating from that point
at t = 0, then the mapping is given by p ‚Üíq = x(t‚àó, p), where t‚àóis the
Ô¨Årst time this solution hits ‚àÇC. This is a continuous mapping, since egress
points are strict. The mapping p ‚Üíq followed by the projection
(t‚àó, x‚àó) ‚Üí(0, x‚àó),
which is called a pull-back mapping, then deÔ¨Ånes a continuous mapping of
‚Ñ¶onto its boundary that leaves boundary points Ô¨Åxed. This contradicts
the No-Retract Theorem and completes the proof of the theorem [32].
The solutions described by this theorem are usually diÔ¨Écult to con-
struct numerically because they are often not stable. Some techniques based
on a related theorem, Sperner‚Äôs lemma, have been devised (see [18] and
Section 4.4.3).
Example. A Forced Unstable System. Consider the equation
dx
dt = x + A cos t.
The cylinder {(t, x) : t ‚â•0, ‚àí2A ‚â§x ‚â§2A} is bounded (in x) by the lines
x = ¬±2A. Since every egress point is a strict egress point for this cylinder,
there is a solution, say x = p(t), that remains within these limits for t ‚â•0.
This must be a periodic solution. Indeed, the general solution is given
by the formula
x(t) = exp(t)x(0) +
 t
0
exp(t ‚àís)A cos s ds.

4.4. Fixed-Point Theorems
141
The return mapping is given by
P(Œæ) = exp(2œÄ)Œæ +
 2œÄ
0
exp(2œÄ ‚àís)A cos s ds,
and it has a unique Ô¨Åxed point
Œæ‚àó= [1 ‚àíexp(2œÄ)]‚àí1
 2œÄ
0
exp(2œÄ ‚àís)A cos s ds.
The solution p(t) is the one with the initial value Œæ‚àó.
4.4.3
Sperner‚Äôs Method
Suppose that f is a mapping of the plane E2 into itself. Thus, for each
point x ‚ààE2, f deÔ¨Ånes a vector
f(x) = [f1(x1, x2), f2(x1, x2)].
Consider a triangle T in E2; select one side of T, say S; and trace the vector
f around T, keeping track of the angle it makes with the side S. The total
angle passed through must be 2œÄN for some integer N. This is called the
index of T relative to f. We denote it by I(f, T). If T is a small triangle,
we see that if f Ã∏= 0 in T and on its boundary, then I(f, T) = 0.
Next consider two triangles, say T1 and T2, that share a side, say S, and
suppose that I(f, T1) = 0 and I(f, T2) = 0. Let the part of the index of f
that occurs on T2 ‚àíS be Œ¥2; then the part on S is ‚àíŒ¥2. Similarly, if the
index on T1 ‚àíS is Œ¥1, then the index on S is ‚àíŒ¥1. It follows that
I(f, T1 ‚à™T2) = Œ¥1 + Œ¥2 = 0.
Finally, consider an arbitrary triangle T. Suppose that I(f, T) Ã∏= 0. Then
there must be a point p ‚ààT such that f(p) = 0. Such a value can be
found by introducing a triangulation of T into subtriangles of size d (i.e.,
the longest side of one of the partitioning triangles is of length d). This
argument shows that for at least one of the triangles in this partition, say
s, we must have I(f, s) Ã∏= 0, or else f Ã∏= 0 in every triangle.
The index and this triangulation construction give a method for Ô¨Ånding a
Ô¨Åxed point of f in a triangle T. First, select a mesh size d and a correspond-
ing triangulation of T. Begin on the boundary of T, successively testing
subtriangles around the boundary for their index. Throw away triangles
t for which I(f, t) = 0. This algorithm, referred to as Sperner‚Äôs method,
leads us to a subtriangle, say s, for which I(f, s) Ã∏= 0. This algorithm can
be quite useful for computations [23, 119].

142
4. Bifurcation and Topological Methods
4.4.4
Measure-Preserving Mappings
Consider an externally forced Hamiltonian system
dx
dt = J‚àáH(x) + g(t),
where x ‚ààE2N, say x = col(p, q), where each of p, q ‚ààEN, J is Jacobi‚Äôs
matrix
J =

0
‚àíI
I
0

,
and H and g are smooth functions. Since the divergence of the right-hand
side of this system is zero, the Ô¨Çow deÔ¨Ånes a volume-preserving transforma-
tion of E2N into itself (see Section 2.3.8). Suppose also that g has period
T. Then the return mapping is deÔ¨Åned by
x(0) ‚Üíx(T),
and it deÔ¨Ånes a measure-preserving transformation of E2N into itself. We
have seen in this chapter that Ô¨Åxed points of this mapping deÔ¨Åne periodic
solutions of the original system. We have seen in Chapter 2, for exam-
ple from Poincar¬¥e‚Äôs Twist Theorem in Section 2.5, that the Ô¨Åxed-point
structure of volume-preserving mappings can be quite complicated. This
problem is the object of intense study [116].
4.5
Exercises
4.1. a. Show that the rank of an N √ó N matrix A and its transpose A‚ä§are
the same. Show that if the equation Ax = f is solvable, then f must
be orthogonal to all null vectors of A‚ä§.
b. Let L = (f ‚àíAx) ¬∑ (f ‚àíAx). Show that the equations ‚àÇL/‚àÇxj = 0
for j = 1, . . . , N are equivalent to the system of equations
A‚ä§Ax = A‚ä§f.
c. Solve the system
2x + y
=
f1
4x + 2y
=
f2,
using the least-squares method described in part b.
d‚àó. Show that the least-squares method works also when A is in EM√óN,
where M Ã∏= N.
4.2.
Show that the sequence of iterates deÔ¨Åned by the iteration in Sec-
tion 4.1.2, say {vj}, deÔ¨Ånes a Cauchy sequence, and so that it
converges. Verify that the limit satisÔ¨Åes the estimate
v = O(|Œª|M+1)
as Œª ‚Üí0.

4.5. Exercises
143
4.3. a. Demonstrate the Weierstrass Preparation Theorem by showing that
the function G deÔ¨Åned in Section 4.2.1 can be written in the form
G(c, Œª) = (ck + l.o.t.)B(c, Œª),
where B(0, 0) Ã∏= 0. Here l.o.t. denotes lower order terms in powers of
c.
b. Construct all small solutions for c and Œª near zero of the equation
c9 ‚àíc4Œª + cŒª2 = 0
using Newton‚Äôs polygon method and the Weierstrass Preparation
Theorem.
4.4.
Exchange of Stabilities. Verify the computations done in Sec-
tions 4.3.1 and 4.3.2.
4.5.
Consider the diÔ¨Äerential-delay equation
dx
dt = Œªx(t ‚àí1)[1 ‚àíx2(t)].
Show that as Œª increases through the value Œª = œÄ/2, a bifurcation
of a periodic solution occurs. Do this by showing that as Œª increases
through the value Œª = œÄ/2, a single pair of eigenvalues of the linear
problem
dx
dt = Œªx(t ‚àí1)
crosses the imaginary axis from left to right. Show that for Œª =
œÄ/2, the function x = sin œÄt/2 solves the linear problem. [See
Exercise 2.14.b.]
4.6.
Verify that the formulas for Œæ‚àóin the examples in Sections 4.4.1
and 4.4.2 lead to the deÔ¨Ånition of solutions having period T.
4.7.
Verify that the graph of a continuous function f that maps an interval
into itself must cross the one-to-one line at some point. Thus, the
function must have a Ô¨Åxed point.
4.8. a. Solve the equation
dx
dt = ‚àíx3 + A cos t
numerically and plot x(2œÄ) versus x(0). Show that there is a unique
periodic solution of this equation and that it is asymptotically stable.
b. Show that the strict egress point mapping in Wazewski‚Äôs theorem is
a continuous mapping of the initial set into the boundary.
c. Use Wazewski‚Äôs theorem to show that the diÔ¨Äerential equation
dx
dt = x3 + A cos t
has a unique periodic solution that starts and remains in the interval
‚àí2A ‚â§x(t) ‚â§2A.

5
Regular Perturbation Methods
Four kinds of perturbation problems are studied in the remainder of this
book. They are illustrated by the following examples:
1. Smooth Data and a Finite Interval. The initial value problem
dx
dt = ‚àíx3 + Œµx,
x(0, Œµ) = Œæ0 + ŒµŒæ1 + O(Œµ2),
where the right-hand side deÔ¨Ånes a smooth function of Œµ at Œµ = 0, arises
frequently in bifurcation problems. The solution x = 0 is stable under
persistent disturbances, but not exponentially asymptotically stable. A so-
lution can be constructed using Taylor‚Äôs formula: First, when Œµ = 0, the
solution is
x0(t) =
Œæ0

1 + 2tŒæ2
0
,
and for Œµ Ã∏= 0 it has the form
x(t) = x0(t) + Œµx1(t) + O(Œµ2),
where x1(t) solves the problem
dx1
dt = (‚àí3x0(t)x1 + 1)x0,
x1(0) = Œæ1,
and so on. This is a regular perturbation problem on a Ô¨Ånite interval [0, T],
and the error estimate O(Œµ2) holds as Œµ ‚Üí0 uniformly for 0 ‚â§t ‚â§T.
However, since the solution x(t) approaches ‚àöŒµ as t ‚Üí‚àû, we cannot
expect Taylor‚Äôs formula to be valid uniformly for 0 ‚â§t < ‚àû.

146
5. Regular Perturbation Methods
2. Smooth Data and an InÔ¨Ånite Interval. The problem
dx
dt = ‚àíx + Œµ,
x(0, Œµ) = Œæ0 + ŒµŒæ1 + O(Œµ2)
has the property that when Œµ = 0 the solution x = 0 is exponentially
asymptotically stable. The solution of this equation is
x(t) = e‚àít[Œæ(Œµ) ‚àíŒµ] + Œµ.
Since this depends smoothly on Œµ uniformly for 0 ‚â§t < ‚àû, the problem is
referred to as a regular perturbation problem on [0, ‚àû).
3. Highly Oscillatory Data. The coeÔ¨Écient of x in the problem
dx
dt = ‚àícos
 t
Œµ

x + Œµ,
x(0, Œµ) = Œæ0 + ŒµŒæ1 + O(Œµ2)
is highly oscillatory, having period 2œÄŒµ. This coeÔ¨Écient has an essential
singularity at Œµ = 0, and the solution is
x(t) = exp

‚àí
 t
0
cos
s
Œµ

ds

x(0, Œµ) + Œµ
 t
0
exp

‚àí
 t
s
cos
s‚Ä≤
Œµ

ds‚Ä≤

ds.
It is not apparent at Ô¨Årst glance how one might extract useful informa-
tion from this explicit formula. However, we will see that the Method of
Averaging (described in Chapter 7) provides for this.
4. Smooth Data‚ÄîSingular Solutions. Solutions might decay or grow
rapidly over short time intervals. For example, the equation
Œµdx
dt = ‚àíx,
x(0, Œµ) = Œæ0 + ŒµŒæ1 + O(Œµ2)
appears to involve a smooth perturbation, but its solution is
x(t) = exp(‚àít/Œµ)x(0, Œµ),
which has an essential singularity at Œµ = 0. Problems of this kind are
studied in Chapter 8 using quasistatic-state approximation methods.
Examples 1 and 2 are regular perturbation problems, and 3 and 4 are
singular perturbation problems, so called because their solutions depend
regularly or singularly on Œµ at Œµ = 0.
Many methods are available for identifying regular perturbation prob-
lems and for constructing their solutions. In this chapter we derive some
methods based on Taylor‚Äôs theorem for constructing approximations to
their solutions. Applications of these methods to oscillation problems are
made in Chapter 6. Singular perturbation problems are studied using
averaging and matching methods in Chapters 7 and 8, respectively.
The regular perturbation methods derived here are based on the Im-
plicit Function Theorem. After introducing some other useful ideas, we
derive methods for constructing solutions to initial value problems in what
corresponds to both the invertible and noninvertible cases.

5.1. Perturbation Expansions
147
5.1
Perturbation Expansions
Perturbation methods have played important roles in the development of
mathematics and physics. Most of the problems studied here come from
electrical circuits or mechanical systems, but the methods were developed
for and used in a variety of applications.
In this chapter we deal primarily with problems that can be solved using
Taylor‚Äôs formula, but methods using Pad¬¥e approximations are also dis-
cussed. Recall that Taylor‚Äôs formula shows how to approximate a smooth
function by a polynomial, and it estimates the error that is made in the
approximation. Pad¬¥e‚Äôs method uses rational function approximations made
up of ratios of polynomials. Using rational functions often makes it possible
to uncover information about singularities of a function and so to obtain
approximations far away from where power series converge.
A variety of notation is used to describe these procedures. Gauge func-
tions are described Ô¨Årst. Next, Taylor‚Äôs and Pad¬¥e‚Äôs formulas are discussed.
Since solutions of diÔ¨Äerential equations can be found using integral formu-
las, like the variation of constants formula, it is important to know Laplace‚Äôs
method and the method of stationary phase, which we also describe brieÔ¨Çy
in this section.
5.1.1
Gauge Functions: The Story of o, O
Suppose that f and g are smooth functions of Œµ for Œµ near zero, say |Œµ| ‚â§Œµ0.
We say that
f(Œµ) = O(g(Œµ))
as Œµ ‚Üí0
if f(Œµ)/g(Œµ) is bounded for all small Œµ. Thus, there is a constant K and a
constant Œµ1 such that
|f(Œµ)| ‚â§K|g(Œµ)|
for all |Œµ| ‚â§Œµ1.
We say that
f(Œµ) = o(g(Œµ))
as Œµ ‚Üí0
if
f(Œµ)
g(Œµ) ‚Üí0
as Œµ ‚Üí0.
Thus, given a tolerance Œ∑, there is a constraint Œ¥ such that |f(Œµ)| ‚â§Œ∑|g(Œµ)|
when |Œµ| ‚â§Œ¥. These are order-of-magnitude or gauge relations between
functions, and they give some idea of the relative sizes between them for
small values of the parameter Œµ. We say that
n

n=0
cnfn(Œµ)

148
5. Regular Perturbation Methods
is an asymptotic expansion of f(Œµ) at Œµ = 0 if the following are true:
1. The sequence {fn}, n = 0, . . . , n + 1, is a gauge sequence; that is,
fn(Œµ) = o(fn‚àí1(Œµ)) as Œµ ‚Üí0 for n = 1, . . . , n + 1, and
2.
f(Œµ) ‚àí
n

n=0
cnfn(Œµ) = O(fn+1(Œµ))
as Œµ ‚Üí0.
Asymptotic expansions need not converge as n ‚Üí‚àû, since convergence
would imply that the function approximated is closely related to an analytic
function. We have no reason to suspect a priori that solutions are that
smooth. Asymptotic expansions do not uniquely determine functions, and
on the other hand, several diÔ¨Äerent asymptotic sequences can be used to
approximate a given function [33].
5.1.2
Taylor‚Äôs Formula
A function f(Œµ) that has n + 1 continuous derivatives at Œµ = 0 can be
approximated by a polynomial of degree n:
f(Œµ) = f(0) + f ‚Ä≤(0)Œµ + ¬∑ ¬∑ ¬∑ + f [n](0)Œµn/n! + O(Œµn+1).
This is Taylor‚Äôs expansion of f about Œµ = 0, and it gives a very useful
approximation to f near Œµ = 0. The error is given explicitly by the formula
f [n+1](Œ∑)Œµn+1
(n + 1)!
,
where Œ∑ is some point near zero, |Œ∑| < Œµ. Here the gauge sequence is fn = Œµn,
and so on.
5.1.3
Pad¬¥e‚Äôs Approximations
Pad¬¥e‚Äôs approximations are rational functions that account for pole sin-
gularities, and they are frequently valid in domains larger than those for
power series expansions. It is often useful to construct an approximation to
a function using Taylor‚Äôs formula and then derive Pad¬¥e‚Äôs approximation to
Taylor‚Äôs polynomial. This is obviously successful when we sum a geometric
series. In general, an analytic function‚Äôs Taylor series has radius of conver-
gence R that is restricted to the largest circle not enclosing a singularity. If
the singularity is a pole, then Pad¬¥e‚Äôs method can be used to continue the
function outside this circle of convergence.
We say that the rational function
Rm,n(Œµ) = Pm(Œµ)
Qn(Œµ) ,

5.1. Perturbation Expansions
149
where Pm is a polynomial of degree m and Qn is one of degree n, is an
[m, n]-Pad¬¥e approximation to f if
f(Œµ) ‚àíRm,n(Œµ) = O(Œµm+n+1)
as Œµ ‚Üí0. Constructing Pad¬¥e‚Äôs approximation Rm,n begins with a power
series expansion of f about Œµ = 0, say
f(Œµ) = f0 + f1Œµ + ¬∑ ¬∑ ¬∑ + fMŒµM + O(ŒµM+1).
We specify m and n such that m + n + 1 ‚â§M, and we seek polynomials
Pm(Œµ) = a0 + a1Œµ + ¬∑ ¬∑ ¬∑ + amŒµm
and
Qn(Œµ) = b0 + b1Œµ + ¬∑ ¬∑ ¬∑ + bnŒµn
such that
f = Pm
Qn
+ O(Œµm+n+1).
Usually, we take m = n ‚àí1, 2n ‚â§M, and b0 = 1. This last can be done
without loss of generality if Q(0) Ã∏= 0, since the numerator and denom-
inator can be multiplied by an arbitrary constant. Since we use a ratio
of polynomials of degrees m and n, we cannot expect to achieve greater
accuracy than order m + n + 1, so we cross multiply to get
Qnf ‚àíPm = O(Œµm+n+1).
Equating coeÔ¨Écients of like powers on both sides gives
order m + n :
b0fm+n
+
¬∑ ¬∑ ¬∑
+
bnfm
= 0
order m + n ‚àí1 :
b0fm‚àí1+n
+
¬∑ ¬∑ ¬∑
+
bnfm‚àí1
= 0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
order m + 1 :
b0f1+m
+
¬∑ ¬∑ ¬∑
+
bnfm‚àín+1
= 0,
starting from order m + n. Here we set fj = 0 if j < 0. This is a system of
n equations for the n unknowns b1, . . . , bn. Recall that b0 = 1, so this is a
nonhomogeneous problem.
We assume the following condition:
Hypothesis H1. This system can be solved for b1, . . . , bn.
Once the coeÔ¨Écients of Qn are known, we can easily Ô¨Ånd the coeÔ¨Écients
of Pm:
order 0 :
a0 = f0
order 1 :
a1 = b1f0 + f1
¬∑ ¬∑ ¬∑
order m :
am = m
j=1 bjfm‚àíj + fm.
Thus, once the coeÔ¨Écients of the denominator are known, the coeÔ¨Écients
of the numerator can be found explicitly.

150
5. Regular Perturbation Methods
There are many surprising and useful extensions of this method [20].
Consider the system of diÔ¨Äerential equations
dx
dt = A(Œµ)x + B(Œµ),
where A and B are matrices that are polynomials in Œµ, say of degrees m
and n, respectively. A static state of this system must solve the equation
A(Œµ)x + B(Œµ) = 0.
If A(0) is invertible, then the unique static state is
x = ‚àíA‚àí1(Œµ)B(Œµ)
for Œµ near 0,
which is a matrix of rational functions of Œµ. Therefore, Taylor‚Äôs method is
more restrictive than Pad¬¥e‚Äôs, for which a matrix form of Pad¬¥e‚Äôs method,
Rm,n(Œµ), gives the exact solution to this problem.
Two ways to proceed with perturbation problems are as follows:
Method I. Substitute Taylor‚Äôs or Pad¬¥e‚Äôs expansion into the problem and
equate coeÔ¨Écients of like powers of Œµ.
Method II. Derive equations for the coeÔ¨Écients in the approximation by
successively diÔ¨Äerentiating the problem with respect to Œµ.
These two methods are used throughout the remainder of this book.
5.1.4
Laplace‚Äôs Methods
The system of diÔ¨Äerential equations
dx
dt = A(Œµ)x + f(t)
has the solution
x(t) = eA(Œµ)tx(0) +
 t
0
eA(Œµ)(t‚àís)f(x)ds.
Evaluating this formula can be diÔ¨Écult.
For example, if A(Œµ) = 1/Œµ, then the solution becomes large if Œµ ‚Üí0+
or if t ‚Üí+‚àû. On the other hand, if A(Œµ) = ‚àí1/Œµ, then the Ô¨Årst term
on the right-hand side of this equation approaches a discontinuous limit
as Œµ ‚Üí0, namely, x(0) if t = 0, but 0 if t > 0. The second term is more
interesting, and the kernel in this integral is closely related to a Dirac delta
function. Finally, if A(Œµ) = i/Œµ, then the integral is highly oscillatory, and
the asymptotic evaluation of it is closely related to the Riemann‚ÄìLebesgue
Lemma [125].
The Ô¨Årst case (A = 1/Œµ) is divergent, and we do not deal with it further.
The second case (A = ‚àí1/Œµ) is treated by using Laplace‚Äôs method, and
the third case (A = i/Œµ) can be evaluated using stationary phase methods.
Since we are dealing primarily with approximations to solutions of diÔ¨Äeren-
tial equations, it is important to know these fundamental methods. In fact,

5.1. Perturbation Expansions
151
they form the basis of the matching and averaging results, respectively,
treated in later chapters. We begin with integration by parts, which is
sometimes referred to as the fundamental theorem of applied mathematics.
Integration by Parts.
Consider the function of t and Œµ deÔ¨Åned by the integral formula
 t
0
exp(is/Œµ)f(s)ds,
where f is a continuously diÔ¨Äerentiable function. Integration by parts gives
an easy way to determine how this function behaves as Œµ ‚Üí0. In particular,
 t
0
exp
is
Œµ

f(s)ds = Œµ
i exp
it
Œµ

f(t) ‚àíŒµ
i f(0) ‚àíŒµ
i
 t
0
exp
is
Œµ

f ‚Ä≤(s)ds.
Integration by parts can be used to generate an asymptotic expansion of
this function, but this short calculation shows that the integral is of order
O(Œµ) (as Œµ ‚Üí0) for any Ô¨Åxed value of t.
Laplace‚Äôs Integral Formula.
Consider the integral
h(Œµ) =
 Œ≤
Œ±
f(s) exp

‚àíg(s)
Œµ

ds.
What is the dominant part of this integral as Œµ ‚Üí0+? Integration by parts
is usually not useful in answering this question.
We suppose that f(t) is continuous and g(t) is a twice-diÔ¨Äerentiable real-
valued function for Œ± ‚â§t ‚â§Œ≤. The place where g is smallest should give
the fastest growing part of the kernel, so let us Ô¨Årst suppose that g has a
minimum at a point œÑ ‚àà(Œ±, Œ≤). Then g‚Ä≤(œÑ) = 0 and g‚Ä≤‚Ä≤(œÑ) > 0. As Œµ ‚Üí0,
h(Œµ) ‚àº
 œÑ+Œ¥
œÑ‚àíŒ¥
f(s) exp

‚àíg(œÑ) + g‚Ä≤‚Ä≤(œÑ)((s ‚àíœÑ)2/2)
Œµ

ds.
Setting u2 = g‚Ä≤‚Ä≤(œÑ)(s ‚àíœÑ)2/2Œµ gives
h(Œµ)
‚àº
2f(œÑ) exp

‚àíg(œÑ)
Œµ

2Œµ
g‚Ä≤‚Ä≤(œÑ)
 Œ¥[g‚Ä≤‚Ä≤(œÑ)/2Œµ]1/2
0
e‚àíu2du
‚àº
2f(œÑ) exp

‚àíg(œÑ)
Œµ

2œÄŒµ
g‚Ä≤‚Ä≤(œÑ)
as Œµ ‚Üí0+.
When the minimum of g is attained at an endpoint of the interval, then
similar calculations show that
h(Œµ) ‚àº
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
f(Œ±) exp[‚àíg(Œ±)/Œµ]

2œÄŒµ
g‚Ä≤‚Ä≤(Œ±)
if g‚Ä≤(Œ±) = 0
Œµf(Œ±) exp[‚àíg(Œ±)/Œµ]
1
g‚Ä≤(Œ±)
if g‚Ä≤(Œ±) > 0,
if the left endpoint is a local minimum for g. Similarly for œÑ = Œ≤.

152
5. Regular Perturbation Methods
The Method of Stationary Phase.
Consider the integral formula
h(Œµ) =
 Œ≤
Œ±
f(s) exp
iœÜ(s)
Œµ

ds,
where |Œµ| ‚â™1 and the phase œÜ is a real-valued function of the real variable
s. Stokes and Kelvin [33] claim that the major contributions to this integral
come from the endpoints Œ± and Œ≤ and from the vicinity of the points where
œÜ is stationary, that is, where œÜ‚Ä≤ = 0, and that to leading order in Œµ,
the stationary points are more signiÔ¨Åcant than the endpoints. This can be
justiÔ¨Åed using the method of steepest descent, which we do not present
here [33].
Suppose that œÜ is stationary at some point œÑ, say that œÜ‚Ä≤(œÑ) = 0 and
œÜ‚Ä≤‚Ä≤(œÑ) > 0. We suppose that the major part of the integral comes from near
œÑ, so we write
h(Œµ) ‚àº
 œÑ+Œµ
œÑ‚àíŒµ
f(t) exp
iœÜ(t)
Œµ

dt.
As before,
h(Œµ) ‚àº

2œÄŒµ
œÜ‚Ä≤‚Ä≤(œÑ)f(œÑ) exp
iœÜ(œÑ)
Œµ
+ iœÄ
4

as Œµ ‚Üí0+.
We use these calculations later in Chapter 7.
5.2
Regular Perturbations of Initial Value
Problems
What conditions ensure that a problem is regular; for example, when will
Taylor‚Äôs formula be successful in Ô¨Ånding approximate solutions? Several
results in this section answer these questions.
5.2.1
Regular Perturbation Theorem
Given smooth data f and Œæ in EN, we consider the initial value problem
dx
dt = f(t, x, Œµ),
x(0) = Œæ(Œµ).
Suppose the following conditions are satisÔ¨Åed:
Hypothesis H2. For Œµ = 0, this system has a unique solution on some
Ô¨Ånite interval 0 ‚â§t ‚â§T. We call this solution x0(t), and it satisÔ¨Åes the
equations
dx0
dt = f(t, x0, 0),
x0(0) = Œæ(0).

5.2. Regular Perturbations of Initial Value Problems
153
Hypothesis H3. f and Œæ(Œµ) are smooth functions of their variables for
0 ‚â§t ‚â§T, x near x0, and Œµ near zero. SpeciÔ¨Åcally, we suppose that f and
Œæ are n + 1 times continuously diÔ¨Äerentiable in all variables, so
Œæ(Œµ) = Œæ0 + Œæ1Œµ + ¬∑ ¬∑ ¬∑ .
We have the following theorem:
Regular Perturbation Theorem. Let conditions H2 and H3 be sat-
isÔ¨Åed. Then for suÔ¨Éciently small Œµ the perturbed problem has a unique
solution, and it is n + 1 times diÔ¨Äerentiable with respect to Œµ. Moreover,
this solution can be expanded in a Taylor expansion
x(t, Œµ) = x0(t) + x1(t)Œµ + ¬∑ ¬∑ ¬∑ + xn(t)Œµn + O(Œµn+1),
where the error estimate holds as Œµ ‚Üí0 uniformly for 0 ‚â§t ‚â§T.
5.2.2
Proof of the Regular Perturbation Theorem
The proof of this result involves three steps. The usual existence theory for
ordinary diÔ¨Äerential equations ensures that a unique solution of the prob-
lem exists for 0 ‚â§t ‚â§T [24]. Denote this by x(t, Œµ). Since this solution is
n + 1 times diÔ¨Äerentiable with respect to Œµ, it can be expanded in a Taylor
polynomial up to order O(Œµn+1). The Ô¨Årst step involves Ô¨Ånding this expan-
sion. The diÔ¨Äerence between the real solution and Taylor‚Äôs approximation
is called the remainder. The second step involves deriving an equation for
the remainder, and the third step involves Ô¨Ånding an a priori estimate of
the remainder.
Step 1. We use Method II for Ô¨Ånding the expansion of the solution:
x(t, Œµ) = x0(t) + x1(t)Œµ + ¬∑ ¬∑ ¬∑ ,
where
x0(t)
=
x(t, 0)
x1(t)
=
‚àÇx(t, Œµ)
‚àÇŒµ

at Œµ = 0, etc.
Therefore,
dx0
dt = f(t, x0, 0),
x0(0) = Œæ0
dx1
dt = fx(t, x0, 0)x1 + fŒµ(t, x0, 0),
x1(0) = Œæ1
¬∑ ¬∑ ¬∑
dxm
dt
= fx(t, x0, 0)xm + {¬∑ ¬∑ ¬∑ }m,
xm(0) = Œæm.
Here fx denotes the Jacobian matrix having components (‚àÇfi/‚àÇxj), fŒµ
denotes the partial derivatives of the components of f with respect to

154
5. Regular Perturbation Methods
Œµ, and the terms {¬∑ ¬∑ ¬∑ }m denote a combination of terms depending on
x0, . . . , xm‚àí1.
The Ô¨Årst equation‚Äôs solution is given by condition H2. The remaining
equations for m = 1, . . . , M are forced linear equations, and their solu-
tions can be obtained from the variation of constants formulas. Thus, the
expansion of x(t, Œµ) is uniquely determined.
Step 2. The remainder is deÔ¨Åned to be
R(t, Œµ) = x(t, Œµ) ‚àíE(t, Œµ, n),
where E denotes the expansion just found for x(t):
E(t, Œµ, n) =
n

j=0
xj(t)Œµj.
This function satisÔ¨Åes the equation
dR
dt = f(t, R + E, Œµ) ‚àídE
dt .
An equivalent way to think of E is that it was found by substituting the
form
E(t, Œµ, n) =
n

j=0
xj(t)Œµj
into the equation and equating like powers of Œµ. Thus,
dE
dt = f(t, E, Œµ) + O(Œµn+1),
and the equation for R becomes
dR
dt = (fx(t, x0, 0) + O(Œµ))R + o(R) + O(Œµn+1),
R(0) = O(Œµn+1).
The second step involves showing that this equation has a unique solution.
This follows from the method of successive approximations presented in
Chapter 4.
Step 3. Finally, we must estimate |R|. It follows from Gronwall‚Äôs in-
equality that R = O(Œµn+1). This completes the proof of the theorem.
The proof of this result is typical of proofs of expansion approximations.
First, the approximation is constructed on faith. Then the diÔ¨Äerence be-
tween it and a real solution of the problem is deÔ¨Åned. An equation for this
diÔ¨Äerence is then analyzed, usually using contraction mapping arguments.
This establishes the existence of a solution to the remainder equation. Fi-
nally, the solution of the remainder equation is estimated, usually using
Gronwall‚Äôs inequality.

5.2. Regular Perturbations of Initial Value Problems
155
5.2.3
Example of the Regular Perturbation Theorem
Consider the equation
dx
dt = ‚àí
x
1 + Œµ,
x(0) = cos Œµ.
The regular perturbation theorem guarantees that the solution of this prob-
lem is a smooth function of Œµ at Œµ = 0. Therefore, it has a Taylor expansion
about Œµ = 0, say
x(t, Œµ) = x0(t) + x1(t)Œµ + ¬∑ ¬∑ ¬∑ + xn(t)Œµn + O(Œµn+1).
These coeÔ¨Écients can be determined directly from the equation by suc-
cessively diÔ¨Äerentiating it, with the result being a sequence of diÔ¨Äerential
equations for the coeÔ¨Écients: (Method II)
dx0
dt
=
‚àíx0,
x0(0) = 1
dx1
dt
=
‚àíx1 + x0,
x1(0) = 0
dx2
dt
=
‚àíx2 + x1 ‚àíx0,
x2(0) = ‚àí1, etc.
Therefore,
x0(t)
=
exp(‚àít)
x1(t)
=
t exp(‚àít)
x2(t)
=
exp(‚àít)(t2/2 ‚àít ‚àí1), etc.
This calculation illustrates the direct Taylor method; of course, Pad¬¥e‚Äôs
method would work better for this problem.
5.2.4
Regular Perturbations for 0 ‚â§t < ‚àû
Two interesting examples are
dx
dt = ‚àíx3 + Œµx
and
dx
dt = Œµx.
Both of these are regular perturbation problems for t restricted to a Ô¨Ånite
interval, but the regular perturbation theorem breaks down for each on
inÔ¨Ånite time intervals. In the Ô¨Årst case, x(t) ‚Üí‚àöŒµ as t ‚Üí‚àû, which is not
a smooth function of Œµ at Œµ = 0, even though the equations are smooth.
In the second, x(t, Œµ) is an entire function of t and Œµ, but the solution is
exp(Œµt), which does not converge uniformly for 0 ‚â§t < ‚àûas Œµ ‚Üí0.

156
5. Regular Perturbation Methods
What conditions ensure that the approximation found in the regular
perturbation theorem is valid uniformly for all 0 ‚â§t < ‚àû? Consider the
initial value problem
dx
dt = f(t, x, Œµ),
x(0) = Œæ(Œµ).
Suppose Ô¨Årst that the problem has a solution when Œµ = 0.
Hypothesis H4. The zero-order problem
dx0
dt = f(t, x0, 0),
x0(0) = Œæ(0)
has a unique solution for 0 ‚â§t < ‚àû.
Next, suppose that the data are smooth near this solution.
Hypothesis H5. Œæ and f are smooth functions of their arguments for
Œµ near zero, 0 ‚â§t < ‚àû, and for (t, x) near (t, x0(t)).
Finally, suppose that the linearized problem is exponentially asymptoti-
cally stable (EAS).
Hypothesis H6. The fundamental solution of the linear problem
dY
dt = fx(t, x0(t), 0)Y,
y(s) = I
satisÔ¨Åes
|Y (t)| ‚â§K exp(‚àíŒ±(t ‚àís))
for all 0 ‚â§t < ‚àû, where K and Œ± are some positive constants.
With these assumptions, we have the following theorem.
Regular Perturbation Theorem on [0, ‚àû). Let conditions H4, H5,
and H6 be satisÔ¨Åed. Then for suÔ¨Éciently small Œµ, the full problem has a
unique solution, and it exists for 0 ‚â§t < ‚àû. Moreover, if
E(t, Œµ, n) =
n

j=0
xj(t)Œµj
is the expansion derived in the Regular Perturbation Theorem, then
x(t, Œµ) ‚àíE(t, Œµ, n) = O(Œµn+1),
where the error estimate holds uniformly for 0 ‚â§t < ‚àû.
The proof of this result proceeds exactly the same as in the Ô¨Ånite-interval
case, except for the last step. In step 3, we must use the estimate provided
by condition H6 in Gronwall‚Äôs inequality to realize a uniform bound on
the function (x ‚àíE)Œµ‚àíM‚àí1 that is valid for 0 ‚â§t < ‚àû. This is the same

5.3. ModiÔ¨Åed Perturbation Methods for Static States
157
argument as used in the Linear Stability Theorem in Chapter 3, and it is
not reproduced here.
Example. Uniformly Valid Regular Perturbation Expansion.
Consider the equation
dx
dt = Ax + Œµf(x),
x(0) = Œæ,
where f(0) = 0 and A is an asymptotically stable matrix (i.e., its
eigenvalues have strictly negative real parts).
Then
x(t, Œµ) ‚àíexp(At)x(0) = O(Œµ)
as Œµ ‚Üí0 uniformly for 0 ‚â§t < ‚àû.
5.3
ModiÔ¨Åed Perturbation Methods for Static
States
Consider now an autonomous (i.e., time-invariant) system of equations
dx
dt = f(x, Œµ).
Static states of the problem are determined by solving the equation
f(x, Œµ) = 0.
We studied this in Chapter 4 using the implicit function theorem.
Recall that we made the following assumptions:
Hypothesis H1. There is a solution for Œµ = 0, say x‚àó
0:
f(x‚àó
0, 0) = 0.
Hypothesis H2. f is a smooth function of x and Œµ for x near x‚àó
0 and Œµ
near zero.
Hypothesis H3. The Jacobian
det(‚àÇf/‚àÇx)(x‚àó
0, 0) Ã∏= 0.
Or:
Hypothesis H3‚Ä≤. The Jacobian matrix has rank r < N.
We refer to problems satisfying conditions H3 as being nondegenerate
and those satisfying H3‚Ä≤ as being degenerate.

158
5. Regular Perturbation Methods
5.3.1
Nondegenerate Static-State Problems Revisited
Under conditions H1, H2, and H3, there is a unique solution of the per-
turbed equation near x‚àó
0, say x‚àó(Œµ), for Œµ near zero. Moreover, this solution
is a smooth function of Œµ. It follows that x‚àó(Œµ) can be expanded in Taylor‚Äôs
expansion about Œµ = 0:
x‚àó(Œµ) = x0 + Œµx1 + ¬∑ ¬∑ ¬∑ + Œµnxn + O(Œµn+1).
Since the coeÔ¨Écients in this expansion are derivatives of x‚àówith respect to
Œµ, we can derive equations for them directly from the deÔ¨Åning equation:
f(x0, 0)
=
0
fx(x0, 0)x1 + fŒµ(x0, 0)
=
0,
and so on. Since x0 is given and the Jacobian condition is satisÔ¨Åed, each of
these equations has a unique solution.
5.3.2
ModiÔ¨Åed Perturbation Theorem
In Chapter 4 the implicit function theorem was used to study problems
where the linear part is not invertible, but we had to derive and deal with
a complicated set of bifurcation equations. In this section we derive a useful
perturbation technique for constructing the bifurcation equations directly.
Consider the static state problem
f(x, Œµ) = 0,
where f(0, 0) = 0 and f is a smooth function near (0, 0) in EN+1. Thus,
we suppose that x0 = 0. Further, suppose that
Hypothesis H3‚Ä≤. The Jacobian matrix
A = (‚àÇf/‚àÇx)(0, 0)
has rank r and nullity q = N ‚àír. Moreover, the null space of this matrix
is spanned by vectors {œÜ1, . . . , œÜq}, and the null space of its adjoint is
spanned by vectors {œà1, . . . , œàq}, where
œàj ¬∑ œÜk
=
Œ¥j,k
œÜj ¬∑ œÜk
=
œàj ¬∑ œàk = Œ¥j,k
for j, k = 1, . . . , q. Here Œ¥j,k is Kronecker‚Äôs delta function; it is equal to 1
if j = k and is zero otherwise.
Finally, we rewrite the equation as
f(x, Œµ) = Ax + G(x, Œµ) = 0,
where G(x, Œµ) = f(x, Œµ) ‚àíAx = O(Œµ + |x|2) for x and Œµ near zero.
We are now in a position to deÔ¨Åne the modiÔ¨Åed problem.
ModiÔ¨Åed Perturbation Problem.

5.3. ModiÔ¨Åed Perturbation Methods for Static States
159
Given small values of c1, . . . , cq and Œµ, Ô¨Ånd functions Œªj(c1, . . . , cq, Œµ) for
j = 1, . . . , q and x(c1, . . . , cq, Œµ) such that
Ax + G(x, Œµ) +
q

j=1
Œªjœàj = 0
œÜk ¬∑ x = ck
for k = 1, . . . , q.
The following result shows that this system is solvable, and that the bifur-
cation equations derived in Chapter 4 are found by setting Œª1 = 0, . . . , Œªq =
0.
ModiÔ¨Åed Perturbation Theorem.
Under conditions H1, H2, and
H3‚Ä≤, the modiÔ¨Åed perturbation problem has a unique solution. That is, there
are unique functions Œªj(c1, . . . , cq, Œµ) and w(c1, . . . , cq, Œµ) that are deÔ¨Åned
for c1, . . . , cq, Œµ near zero such that
1. The functions x = q
j=1 cjœÜj + w and Œª1, . . . , Œªq satisfy the modiÔ¨Åed
perturbation problem.
2. These functions have at least n+1 continuous derivatives with respect
to c1, . . . , cq and Œµ. In particular, these functions can be expanded
using Taylor‚Äôs formula:
w
=
wc1c1 + ¬∑ ¬∑ ¬∑ + wcqcq + wŒµŒµ + h.o.t.
Œªj
=
Œªjc1c1 + ¬∑ ¬∑ ¬∑ + Œªjcqcq + ŒªjŒµŒµ + h.o.t.
This result is quite useful in studying resonance problems. Of course, it
is a clever restatement of the degenerate Implicit Function Theorem [5.4].
Proof of the ModiÔ¨Åed Perturbation Theorem. The proof of this result is
accomplished by showing that the Implicit Function Theorem can be used
to solve the modiÔ¨Åed problem. Writing
x =
q

j=1
cjœÜj + w
we have
Aw + Œ®Œª = ‚àíG(x, Œµ).
The columns of A span the set ker(A‚ä§)‚ä•and the columns of the matrix
Œ® span ker(A‚ä§). Therefore, the linear part on the left-hand side of this
equation is a matrix having N linearly independent columns, and so it is
invertible. Therefore, the modiÔ¨Åed problem can be solved using the nonde-
generate form of the Implicit Function Theorem. In particular, there is a
unique solution for w and Œª, which are smooth functions of c1, . . . , cq, and

160
5. Regular Perturbation Methods
Œµ. In fact, introduce a change of basis
x =
q

j=1
cjœÜj +
N‚àíq

j=1
wjœÜj+q,
where the vectors œÜ1, . . . , œÜq span ker(A) and the vectors œÜq+1, . . . , œÜq+r
complete this set to a basis of EN. Then the modiÔ¨Åed problem becomes
Œªj
=
‚àíœàj ¬∑ G

q

j=1
cjœÜj + w, Œµ

Aw
=
G

q

j=1
cjœÜj + w, Œµ

‚àí
q

j=1
Œªjœàj.
Since the linear problem for Œª and w has full rank, the Implicit Function
Theorem (nondegenerate form) ensures that these equations can be solved
for small solutions Œª and w as functions of c1, . . . , cq, and Œµ for |c| and |Œµ|
near zero.
Thus, there is a unique solution for x and Œª as functions of c and Œµ. Once
this is done, the original problem can be solved by requiring that
Œªj(c, Œµ) = 0
for j = 1, . . . , q.
These are equivalent to the bifurcation equations derived in Chapter 4.
5.3.3
Example: q = 1
Approximations of the modiÔ¨Åed perturbation solution (i.e., x and the func-
tions Œªj) can be found using Taylor‚Äôs formula: We take q = 1 to illustrate
this method. Then
x
=
xŒµŒµ + xcc + ¬∑ ¬∑ ¬∑
Œª
=
ŒªŒµŒµ + Œªcc + ¬∑ ¬∑ ¬∑ .
The Ô¨Årst result is
AxŒµ + GŒµ(0, 0) + Œªeœà
=
0
xŒµ ¬∑ œÜ
=
0.
Thus, ŒªŒµ = ‚àíœà ¬∑ GŒµ(0, 0) and
AxŒµ + GŒµ(0, 0) ‚àí(œà ¬∑ GŒµ(0, 0))œà = 0.
This equation has a unique solution that is orthogonal to œÜ. Next,
Axc + Œªcœà
=
0
xc ¬∑ œÜ
=
1.
Therefore, Œªc = 0 and xc = œÜ.

5.4. Exercises
161
In this way we can continue to construct Taylor‚Äôs expansion of each of
these functions. We return to the original problem by forcing the modiÔ¨Åed
problem to be the same as the original one:
Œª(c, Œµ) = 0.
This equation can be studied using Newton‚Äôs polygons. The modiÔ¨Åed
method applies in the exchange of stabilities problem in Chapter 4, and
we use it in constructing nonlinear oscillations in Chapter 6.
5.4
Exercises
5.1.
Show that x and x+exp(‚àí1/x) have the same asymptotic expansions
as x ‚Üí0+.
5.2.
Show that
1
1 + x
‚àº
‚àí

(‚àíx)n
‚àº
‚àí

(x ‚àí1)x2n
‚àº
‚àí

(x2 ‚àíx + 1)(‚àíx)3n
as x ‚Üí‚àû, where the sums are taken for n = 0, 1, 2, 3, 4, . . . .
5.3. a. Solve the equation
(1 ‚àíŒµ)f(Œµ) = 1
for f using two diÔ¨Äerent methods: Pad¬¥e approximants and Taylor‚Äôs
expansions. Compare the answers you obtain in each case.
b. Construct the [1, 1]-Pad¬¥e approximation to the function
f(Œµ) =
	1 + Œµ/2
1 + 2Œµ

‚àí1/2
.
Construct Taylor‚Äôs approximation of f up to order 2. Compare your
answers for Œµ = 1 and for Œµ = ‚àû.
5.4.
Calculate an asymptotic approximation to the integral
 1
a
exp
	is2
Œµ

cos s ds
as Œµ ‚Üí0 for the two cases a = 0 and a = ‚àí1.
5.5.
Show that R = O(eM+1) in the proof of the regular perturbation
theorem in Section 5.2.2.
5.6.
Use the Ansatz x = exp[P(Œµ)t/Q(Œµ)] to solve the problem
dx
dt = ‚àí
x
1 + Œµ,
x(0) = cos Œµ
in the example in Section 5.2.3 for |Œµ| ‚â™1.
5.7.
Apply the modiÔ¨Åed perturbation method to the exchange of stabilities
problem in Sections 4.3.1 and 5.3.3.

6
Iterations and Perturbations
Forcing a nonlinear oscillation can have complicated consequences, espe-
cially if the forcing period is near the oscillation‚Äôs period. The simplest
question is: Under what conditions will a periodic solution result? The
answers for externally forced systems are quite diÔ¨Äerent from those for
feedback systems that are found in Sections 3.5, 7.4, 7.5, and 8.4.
Problems considered in this chapter have the general form
dx
dt = f(t, x, Œµ),
where x, f ‚ààEN and f is a smooth function of its variables. The case
where f has least period T > 0, say f(t + T, . . . ) = f(t, . . . ) for all t, is
referred to as an externally (periodically) forced system. When f does not
depend explicitly on t, we refer to it as a feedback system.
We suppose that the unperturbed problem
dx
dt = f(t, x, 0)
has a periodic solution x = p(t), say with period T. Conditions on the
data will be found under which the perturbed problem (Œµ Ã∏= 0) will have
a periodic solution that is close to p(t) and has period near T or some
rational multiple of T.

164
6. Iterations and Perturbations
6.1
Resonance
When we try to construct expansions of forced oscillations, we soon dis-
cover diÔ¨Éculties if the forcing frequency is near a natural frequency of the
system. Two results, one describing nonresonance and the other applicable
to resonant cases, are presented in the Ô¨Årst section of this chapter. The
modiÔ¨Åed perturbation method is applied to these cases, and the results
are compared to other perturbation and iteration schemes in the case of
DuÔ¨Éng‚Äôs equation. In the course of this, we discover that chaotic behavior
can occur and that fractals can bound basins of attraction when DuÔ¨Éng‚Äôs
equation is externally forced.
The equation
d2x
dt2 + ¬µ2x = Œµ cos t
is useful for introducing some basic ideas. It can be solved explicitly. If
¬µ2 Ã∏= 1, then
x(t) = A cos(¬µt + œÜ) + Œµ cos t
¬µ2 ‚àí1,
where A and f are arbitrary constants. The Ô¨Årst term gives the general
solution of the free problem. If ¬µ2 = 1, then
x(t) = A cos(t + œÜ) + Œµt sin t
2
;
the last term is referred to as secular, since it grows very slowly.
Since we are interested in Ô¨Ånding solutions that have the same period
as the forcing function (viz., 2œÄ), we specify that x satisfy the periodicity
conditions
x(t + 2œÄ, Œµ) = x(t, Œµ)
for all t ‚â•0.
There are three cases:
1. The problem is nonresonant, or regular, when ¬µ is not an integer. In
this case, the equation has a unique solution of period 2œÄ, namely,
x(t) = Œµ cos t
¬µ2 ‚àí1.
2. Subresonance occurs when ¬µ2 = n2 for some integer n2 Ã∏= 1. In this
case the solution formula shows that both terms in the solution have
period 2œÄ, but the Ô¨Årst, which corresponds to the general solution of
the homogeneous equation, has least period 2œÄ/n. The second term
is a particular solution of the equation.
3. Resonance occurs when ¬µ2 = 1. In this case, there is no solution of
the equation having period 2œÄ.

6.1. Resonance
165
The situation here is quite similar to Fredholm‚Äôs Alternative for solving
systems of linear equations. Recall that when solving the linear system
Lx = f,
where L is a matrix and x and f are vectors, we found that there is a
solution for x if and only if the forcing function f is orthogonal to the null
space of L‚ä§. We follow the same approach for solving equations where now
L represents a diÔ¨Äerential operator and f is a function.
The concept of orthogonality for vectors extends naturally to functions.
Let f and g be two complex valued functions deÔ¨Åned on an interval 0 ‚â§
t ‚â§T. Let tj = jT/N for j = 0, 1, 2, . . . , N, and fj = f(tj) and gj = g(tj).
The vectors f = (f0, . . . , fN) and g = (g0, . . . , gN) have inner product
f ¬∑ g =
N

j=0
fjg‚àó
j ,
where g‚àó
j denotes the complex conjugate of gj. If N is large, this is
approximately the integral
1
N f ¬∑ g = 1
T
N

j=0
fjg‚àó
j
T
N ‚âà1
T
 T
0
f(s)g‚àó(s)ds ‚â°1
T (f, g),
where we deÔ¨Åne the inner product of two square integrable functions f(t)
and g(t) to be
(f, g) =
 T
0
f(t)g‚àó(t)dt.
We say that f and g are orthogonal over [0, T] if (f, g) = 0.
Note that if f is a square-integrable complex-valued function, then (f, f)
exists, and so we can deÔ¨Åne
|f| =

(f, f).
This shows that these integral formulas are natural extensions to functions
of geometric ideas for vectors.
For example, taking the inner product (with T = 2œÄ) of both sides of
our diÔ¨Äerential equation with the general solution of the unforced problem
(Œµ = 0), namely with cos(¬µt + œÜ), we have

cos(¬µt + œÜ), d2x
dt2 + ¬µ2x

= (cos(¬µt + œÜ), Œµ cos t).
Integration by parts shows that the left-hand side of this equation is zero.
Therefore, a necessary condition that this equation have a solution is that
0 = (cos(¬µt + œÜ), Œµ cos t)
for any constant œÜ. In this example, this condition is also suÔ¨Écient to ensure
solvability.

166
6. Iterations and Perturbations
How can these cases be determined directly from the equation? The
unperturbed, or free, equation is
d2x
dt2 + ¬µ2x = 0.
Since we will be forcing this system with period 2œÄ, solutions of this equa-
tion that have period 2œÄ are the important ones. In the resonance case,
the forcing is among these. In the subresonance case, the forcing function
is orthogonal to these; and in the regular case, there are no such solutions
to the free problem.
In our analogy with Fredholm‚Äôs Alternative, the matrix L is replaced by
a diÔ¨Äerential operator
Lx = d2x
dt2 + ¬µ2x
deÔ¨Åned for all square-integrable functions x(t) for which the second
derivative makes sense and that satisfy the periodicity conditions
x(t) = x(t + 2œÄ)
for all t. This operator is self-adjoint in the sense that (Lx, y) = (x, Ly).
6.1.1
Formal Perturbation Expansion of Forced Oscillations
Let us try to construct the solution of the general problem
dx
dt = f(t, x, Œµ),
x(t + T) = x(t)
by a regular perturbation expansion beginning with a known periodic solu-
tion when Œµ = 0, say p(t), without worrying about validity. Let our Ansatz,
or initial guess for the solution, be
x(t) = p(t) + Œµx1 + Œµ2x2 + ¬∑ ¬∑ ¬∑ .
This gives
dp
dt
=
f(t, p, 0)
dx1
dt
=
fx(t, p, 0)x1 + fŒµ(t, p, 0),
and so on, where the subscripts denote diÔ¨Äerentiations with respect to Œµ
and the components of x.
The solution for x1 is given by the formula
x1(t) = Q(t) exp(Rt)x1(0) +
 t
0
Q(t) exp[R(t ‚àís)]Q‚àí1(s)fŒµ(s)ds,
where fŒµ(s) ‚â°fŒµ(s, p(s), 0) and Q(t) exp(Rt) is the fundamental solution
of the linear part given by Floquet‚Äôs theory; x1 is periodic if x1(T) = x1(0).

6.1. Resonance
167
Thus, we have a formula for the initial data of periodic solutions:
[I ‚àíexp(RT)]x1(0) =
 T
0
exp[R(T ‚àís)]Q‚àí1(s)fŒµ(s)ds.
As before, there are three cases:
1. The matrix I ‚àíeRT is invertible (nonresonance).
2. The matrix is not invertible, but the right-hand side is orthogonal to
its adjoint null vectors (subresonance).
3. The matrix I ‚àíeRT is not invertible and the right-hand side is not
orthogonal to its adjoint null vectors (resonance).
In case 3 there is no solution for x1(0), and so the formal perturbation
Ansatz breaks down. In case 2 there is a unique particular solution, plus
an arbitrary linear combination of null vectors of the matrix. This requires
the special accounting supplied by the modiÔ¨Åed perturbation method. In
the Ô¨Årst case there is a unique solution for x1(0), hence a unique periodic
solution of the system.
This calculation illustrates the general situation: SpeciÔ¨Åc results are
described in the sections on nonresonance and resonance, which follow.
6.1.2
Nonresonant Forcing
It is possible to give conditions that ensure nonresonance and to completely
describe perturbation solutions in that case. We refer to this collection of
results as the Nonresonance Theorem.
Consider the forced system
dx
dt = f(t, x, Œµ),
where f is a vector of smooth functions that are periodic in t, say with
least period T. We suppose the following:
Hypothesis H1. There is a solution x = p(t) of the unperturbed
equation
dx
dt = f(t, x, 0)
that has period T.
Hypothesis H2. x, f ‚ààEN. f is a smooth function of its variables for
(t, x) near (t, p(t)) and Œµ suÔ¨Éciently small. Moreover, f has least period
T > 0 in t:
f(t + T, x, Œµ) = f(t, x, Œµ)
for all t ‚â•0.

168
6. Iterations and Perturbations
The problem here is to determine solutions of this system that are
periodic having the same period as the forcing
x(t + T, Œµ) = x(t, Œµ)
for all t ‚â•0.
If œà(t, a, Œµ) is the solution of the diÔ¨Äerential equation that satisÔ¨Åes the
initial condition
œà(0, a, Œµ) = p(0) + a,
then Ô¨Åxed points of the mapping
p(0) + a ‚Üíœà(T, a, Œµ)
deÔ¨Åne periodic solutions of the original problem. Therefore, we consider the
equation
œà(T, a, Œµ) ‚àíp(0) ‚àía = 0.
We can apply the Implicit Function Theorem to this equation to construct
solutions for a = a(Œµ), and so eventually construct periodic solutions of the
forced problem.
The Jacobian matrix of this equation for a is
‚àÇœà
‚àÇa (T, 0, 0) ‚àíI,
where I is the N-dimensional identity matrix. If this matrix is nonsingular,
then we can Ô¨Ånd a unique solution for a = a(Œµ). If the matrix is singular,
then we can use the modiÔ¨Åed perturbation method to study solutions.
The Jacobian matrix is usually not easy to determine, because the fol-
lowing calculation must be done. The function u = ‚àÇœà/‚àÇa is an N √ó N
matrix that solves the linear problem
du
dt = fx(t, p(t), 0)u(t),
u(0) = I.
Since this is a linear equation having periodic coeÔ¨Écients, Floquet‚Äôs theory
shows that u(t) has the form
u(t) = P(t) exp(Rt),
where P(t) has period T in t and R is a constant matrix. Therefore, u(T) =
eRT . Recall that the matrix R is usually diÔ¨Écult to Ô¨Ånd.
It follows that if all eigenvalues of R lie oÔ¨Äthe imaginary axis in the
complex plane, then the regular perturbation method works directly. The
result is a uniquely determined function a(Œµ). This proves the following
theorem.
Nonresonance Theorem. Let conditions H1 and H2 above be satisÔ¨Åed.
Further, suppose that x = 0 is the only solution of the linear problem
dx
dt = fx(t, p(t), 0)x,
x(t + T) = x(t);

6.1. Resonance
169
then, for suÔ¨Éciently small Œµ, the full problem has a unique periodic solution
x = x‚àó(t, Œµ). If f is M + 1 times diÔ¨Äerentiable with respect to its variables,
then x‚àócan be constructed using a regular perturbation expansion:
x‚àó(t, Œµ) = p(t) + x1(t)Œµ + ¬∑ ¬∑ ¬∑ + xM(t)ŒµM + O(ŒµM+1),
where the coeÔ¨Écients are uniquely determined by a hierarchy of linear
problems, and x‚àóis determined by the initial conditions
x‚àó(0, Œµ) = p(0) + a(Œµ),
where a is determined in the preceding argument.
Example of a Conservative System with nonresonant forcing.
Here we try to construct a 2œÄ-periodic solution of the equation
d2x
dt2 + ¬µ2x + Œµf(x) = g(t),
where f is a smooth function, ¬µ is a Ô¨Åxed constant, and Œµ is a small
parameter. The forcing function g is 2œÄ-periodic and has a Fourier series
g(t) =
‚àû

m=‚àí‚àû
gmeimt.
This equation includes DuÔ¨Éng‚Äôs equation, for which
f(x) = Œ±x + Œ≤x3.
We suppose here that ¬µ is not an integer. The unperturbed problem is
obtained by setting Œµ = 0:
d2x
dt2 + ¬µ2x = g(t),
and it has a unique periodic solution that is given by the Fourier series
p(t) =
‚àû

m=‚àí‚àû
gm
eimt
¬µ2 ‚àím2 .
In this case, according to the Regular Perturbation Theorem, we can Ô¨Ånd
x as a smooth function of Œµ. Therefore, we try to Ô¨Ånd the solution of the
equation in the form of a Taylor expansion
x(t, Œµ) = p(t) + Œµx1(t) + ¬∑ ¬∑ ¬∑ .
We obtain a system of equations for the other coeÔ¨Écients:
d2x1
dt2 + ¬µ2x1
=
‚àíf(x0)
d2x2
dt2 + ¬µ2x2
=
‚àí‚àÇf
‚àÇx(x0)x1,

170
6. Iterations and Perturbations
and so on. These equations are to be solved subject to the periodicity
conditions
xj(t + 2œÄ) = xj(t)
for j = 1, 2, . . .
for all t. Obviously, each of these problems has a unique solution, and the
Fourier series of each can be found.
6.1.3
Resonant Forcing
Problems created by resonant forcing are quite similar to those encountered
in the noninvertible case of the Implicit Function Theorem. We can reduce
the problem to a set of bifurcation equations, but the solutions of these
may be quite diÔ¨Écult to Ô¨Ånd.
We consider here systems in the form
dx
dt = Ax + ŒµG(t, x, Œµ),
where x, G ‚ààEN. We say that the forcing function (G) is resonant if it has
period T and if the matrix I ‚àíexp(AT) is not invertible. In this case, the
hypotheses of the Nonresonance Theorem are not satisÔ¨Åed, since the linear
problem
dx
dt = Ax
has a (nonzero) solution of period T.
The following conditions are used in this section:
Hypothesis H1. G is a smooth function for ‚àí‚àû< t < ‚àû, with x
near zero and |Œµ| ‚â™1, and G(t + T, x, Œµ) = G(t, x, Œµ) for all t and some
least T > 0. Moreover, G(t, x, 0) = O(|x|)2 for x near zero uniformly for
0 ‚â§t ‚â§T.
Hypothesis H2. The matrix A has the form
A =
 R
0
0
D

,
where R is a K√óK diagonal matrix, R = diag(r1, . . . , rK), with exp(RT) =
I, and the matrix D is such that exp(DT) ‚àíI is invertible.
This system is not as general as could be considered here (e.g., in general,
R is not diagonalizable), but it enables us to deal with the fundamental
solution exp(At) explicitly. More general equations require evaluation of
the Floquet exponent matrix, which we have discussed earlier.
The existence of periodic solutions can be studied as before: We seek
initial values a such that the solution x = œÜ(t, a, Œµ) has period T. As before,
œÜ(0, a, Œµ) = a,

6.1. Resonance
171
and we want to solve
œÜ(T, a, Œµ) = a
for a.
The function œÜ solves the equivalent integral equation
œÜ(t, a, Œµ) = eAta + Œµ
 t
0
eA(t‚àís)G(s, œÜ, Œµ)ds.
The periodicity condition becomes
(I ‚àíeAT )a = Œµ
 T
0
eA(T ‚àís)G(s, œÜ, Œµ)ds.
Setting ÀÜa = col(a1, . . . , aK) and ¬Øa = col(aK+1, . . . , aN), and similarly for
G, say ÀÜG = (G1, . . . , GK) and ÀúG = col(GK+1, . . . , GN), then we have
0
=
 T
0
e‚àíRs ÀÜG(s, œÜ(s, a, Œµ), Œµ)ds
(I ‚àíeDT )Àúa
=
Œµ
 T
0
eD(T ‚àís) ÀúG(s, œÜ(s, a, Œµ), Œµ)ds.
The Implicit Function Theorem guarantees that for each small Œµ and for
each choice of ÀÜa near zero there is a unique solution for Àúa, say
Àúa = Àúa(ÀÜa, Œµ).
Substituting this solution into the Ô¨Årst K equations gives the bifurcation
equations, which must be solved for ÀÜa. These results are summarized in the
following theorem.
Resonant Forcing Theorem. Let conditions H1 and H2 hold. Then
for each small ÀÜa in EK and Œµ there exists a unique solution Àúa = Àúa(ÀÜa, Œµ) in
EN‚àíK of
(I ‚àíeDT )Àúa = Œµ
 T
0
eD(T ‚àís) ÀúG(s, œÜ(s, a, Œµ), Œµ)ds.
Moreover, for each small solution ÀÜa of the bifurcation equations
0 =
 T
0
e‚àíRs ÀÜG(s, œÜ(s, Àúa(ÀÜa, Œµ), Œµ), Œµ)ds
there is a periodic solution
x = œÜ(t, a, Œµ)
of the original problem where a = col(ÀÜa, Àúa(ÀÜa, Œµ)).
This result is less satisfactory than the Nonresonance Theorem because
it takes us only to the bifurcation equations, and they are in a form that re-
quires knowledge of œÜ(t, a, Œµ). The modiÔ¨Åed perturbation method enables us

172
6. Iterations and Perturbations
to construct the bifurcation equations although there may be no solutions
to them or they might not be solvable using available methods.
6.1.4
ModiÔ¨Åed Perturbation Method for Forced Oscillations
Suppose that the conditions of the resonant forcing theorem are satisÔ¨Åed.
Then we can pursue periodic solutions as we did static states, using the
modiÔ¨Åed perturbation method.
First, we augment the problem by introducing K
new variables
Œª1, . . . , ŒªK, and K new constraints c1, . . . , cK:
dx
dt = Ax + ŒµG(t, x, Œµ) + Œµ
K

j=1
Œªjerjtej,
where ej is the jth standard basis unit
ej = col(Œ¥1,j, . . . , Œ¥N,j)
and rj is the jth eigenvalue of R. Note that because of our choice of basis
in the original problem, the last term can be written as
Œµ exp(At)Œª,
where Œª is the vector
Œª = col(Œª1, . . . , ŒªK, 0, . . . , 0).
Thus, the augmented problem is
dx
dt = Ax + ŒµG(t, x, Œµ) + Œµ exp(At)Œª.
The K additional constraints are speciÔ¨Åed by the projections
(x, exp(rjt)ej) = cj
for j = 1, . . . , K. Finally, we specify that x has period T by requiring that
x(t + T) = x(t)
for all t.
Second, we construct Taylor expansions for x and Œª as functions of Œµ and
c = (c1, . . . , cK), for these quantities near zero. Each of these problems has
a unique solution, and the results are functions
x = x(t, c, Œµ)
and
Œª = Œª(c, Œµ).
The third step gets us back to the original problem. We require that
Œª(c, Œµ) = 0.
These are the bifurcation equations, and when they are satisÔ¨Åed, x(t, c, Œµ)
is a solution of the original problem that has period T.
Rather than present this method as a general algorithm, we consider an
important example in Section 6.2.8.

6.1. Resonance
173
6.1.5
JustiÔ¨Åcation of the ModiÔ¨Åed Perturbation Method
The program prescribed by the ModiÔ¨Åed Perturbation Method has three
steps:
1. Add K variables Œª1, . . . , ŒªK and K constraints c1, . . . , cK to the
problem by
dx
dt
=
Ax + ŒµG(t, x, Œµ) + Œµ exp(At)Œª
cj
=
(x, exp(Œªjt)ej),
where Œª = col(Œª1, . . . , ŒªK, 0, . . . , 0) and ej is the jth standard basis
unit (i.e., a vector with 1 in the jth position and 0 elsewhere). Note
that the right-hand side of the diÔ¨Äerential equation is T-periodic in t,
and we are to solve this system for x and Œª subject to the constraints
and the periodicity condition
x(t + T) = x(t)
for all t.
2. Construct x and Œª as power series in Œµ and the components of
c = col(c1, . . . , cK, 0, . . . , 0). The coeÔ¨Écients in these expansions are
determined uniquely.
3. The equations
Œªj(c, Œµ) = 0,
j = 1, . . . , K,
must be solved for c as functions of Œµ, say c = c(Œµ). For each such so-
lution the function x(t, c(Œµ), Œµ) is a T-periodic solution of the original
problem.
We now show that this approach is equivalent to the one in the previ-
ous section. In fact, let us denote the solution of the modiÔ¨Åed diÔ¨Äerential
equations that satisÔ¨Åes x(0) = b by
y(t, b, Œµ).
This function satisÔ¨Åes the integral equation
y(t, b, Œµ) = eAtb + Œµ
 t
0
eA(t‚àís)G(s, y, Œµ)ds + ŒµteAtŒª.
We observe that y has period T (i.e., y(0) = y(T)) if and only if b satisÔ¨Åes
the equation
(I ‚àíeAT )b = Œµ
 T
0
eA(T ‚àís)G(s, y, Œµ)ds + ŒµTeAT Œª.
Note the presence of secular terms in the equation for y(t, b, Œµ); these are
discussed from another point of view in Section 6.2.3. We write b = col(ÀÜb,Àúb),

174
6. Iterations and Perturbations
where the Ô¨Årst subvector has K components, etc. For each choice of ÀÜb near
zero and |Œµ| ‚â™1 there is a unique solution for Àúb = Àúb(ÀÜb, e) of the equations
(I ‚àíeDT )Àúb = Œµ
 T
0
eD(T ‚àís) ÀúG(s, y, Œµ)ds.
We now replace b by col(ÀÜb,Àúb(ÀÜb, Œµ)). With this, the projection constraints
take the form
cj = (y, exp(rjt)ej) = ÀÜbj + ŒµHj(ÀÜb, Œµ),
where
Hj = 1
T
 T
0
 t
0
e‚àírjs ÀÜGjdsdt + T
2 Œªj
for j = 1, . . . , K. The Implicit Function Theorem guarantees that
c = ÀÜb + ŒµH(ÀÜb, Œµ)
deÔ¨Ånes a one-to-one relation between c and ÀÜb. In particular, we can solve
for ÀÜb, say with the result
ÀÜb = c + Œµh(c, Œµ).
The functions Œªj are deÔ¨Åned by the formulas
Œªj(c, Œµ) = ‚àí1
T
 T
0
e‚àírjs ÀÜGj ds
which results from direct integration of the diÔ¨Äerential equation for y. Fi-
nally, values of c and Œµ for which Œª = 0 result in a solution y that has
period T, but since it now satisÔ¨Åes the same equation as œÜ, we also have
that for such c and Œµ,
y(t, b(c, Œµ), Œµ) = œÜ(t, a, Œµ).
Thus, the results of the Resonant Forcing Theorem and the ModiÔ¨Åed
Perturbation Method agree.
6.2
DuÔ¨Éng‚Äôs Equation
The examples in this section illustrate several interesting aspects of reso-
nant forcing. These are all based on DuÔ¨Éng‚Äôs equation, which is among the
simplest nonlinear conservation equations. Several methods diÔ¨Äerent from
the ModiÔ¨Åed Perturbation Method have been devised to study resonantly
forced systems. Two particularly important ones are DuÔ¨Éng‚Äôs iterative
procedure and the Poincar¬¥e‚ÄìLinstedt perturbation procedure. Rather than
describing these methods in general, we consider an important typical case:
We study the response of DuÔ¨Éng‚Äôs equation to small resonant forcing where

6.2. DuÔ¨Éng‚Äôs Equation
175
the modiÔ¨Åed perturbation method, DuÔ¨Éng‚Äôs iteration, and the Poincar¬¥e‚Äì
Linstedt methods can all be used. One or another of these methods can
work for many other cases.
We consider in this section the problem
d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos œât).
Resonance occurs when the forcing frequency œâ is near 1.
6.2.1
ModiÔ¨Åed Perturbation Method
Consider DuÔ¨Éng‚Äôs equation when the forcing frequency is near resonance,
say
œâ2 = 1 + ¬µŒµ + O(Œµ2).
With the change of variable œât ‚Üít, the equation becomes
(1 + Œµ¬µ)d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos t).
According to the ModiÔ¨Åed Perturbation Method, we augment the equation
by introducing Œª and Œ∑,
(1 + Œµ¬µ)d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos t + Œª cos t + Œ∑ sin t),
and we specify the constraints
(x, cos t) = a
and
(x, sin t) = b.
The ModiÔ¨Åed Perturbation Method is quite clumsy in this case. As a short-
cut, we ignore the fact that a and b are small, and we construct x, Œª, and
Œ∑ in powers of Œµ alone:
d2x0
dt2 + x0 = 0
(x0, cos t) = a
and
(x0, sin t) = b
d2x1
dt2 + x1 = ‚àíŒ±x0 ‚àíŒ≤x3
0 + C cos t ‚àí¬µd2x0
dt2 + Œª0 cos t + Œ∑0 sin t
(x1, cos t) = 0
and
(x1, sin t) = 0,
and so on. Solving the Ô¨Årst problem gives
x0(t) = 2a cos t + 2b sin t.
It will turn out that b = 0, so we use this information now to simplify the
calculations. In order that the second problem be solvable for a function

176
6. Iterations and Perturbations
having period 2œÄ, we must have the coeÔ¨Écient of cos t equal to zero on the
right-hand side of the equation:
‚àíŒ±a ‚àí3Œ≤ a3
4 + C + a¬µ + Œª0 = 0;
this formula relates the forcing frequency (œâ) to the amplitude of response
(a). For further details see [89, 43].
The bifurcation equation is found by setting Œª = 0. To leading order in
Œµ, this is
3Œ≤ a3
4 + (Œ± ‚àí¬µ)a ‚àíC = 0.
Note that there is a cusp singularity at a = 0, Œ± ‚àí¬µ = 0, C = 0, as shown
in Figure 6.1.
6.2.2
DuÔ¨Éng‚Äôs Iterative Method
DuÔ¨Éng introduced an iterative method [31] for approximating the periodic
solutions of the
d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos œât).
Begin with an initial guess suggested by the forcing, namely,
x0(t) = A cos œât,
and consider the iteration
d2xn+1
dt2
+ xn+1 = Œµ(‚àíŒ±xn ‚àíŒ≤x3
n + C cos œât)
for n = 0, 1, . . . . The equation for x1 is
d2x1
dt2 + x1 = Œµ(‚àíŒ±A cos œât ‚àíŒ≤(A cos œât)3 + C cos œât).
The right-hand side can be simpliÔ¨Åed by writing
(cos œât)3 = 3 cos œât
4
+ cos 3œât
4
.
Then
d2x1
dt2 + x1 = Œµ

‚àíŒ±A ‚àí3Œ≤A3
4
+ C

cos œât ‚àíA3Œ≤
4
cos 3œât

.
Integrating this (assuming œâ Ã∏= 1) gives
x1 = Œµ(‚àíŒ±A ‚àí3Œ≤A3/4 + C)
1 ‚àíœâ2
cos œât ‚àí
A3Œ≤
4(1 ‚àí9œâ2) cos 3œât.

6.2. DuÔ¨Éng‚Äôs Equation
177
DuÔ¨Éng‚Äôs idea is that if the initial guess is a good one, then the coeÔ¨Écient
of cos œât in this solution must be close to A. Thus,
(1 ‚àíœâ2)A = Œµ

‚àíŒ±A ‚àí3Œ≤A3
4
+ C

.
If œâ2 = 1 + ¬µŒµ, then
3Œ≤A3
4
+ (Œ± ‚àí¬µ)A ‚àíC = 0.
This is identical to the result obtained in the preceding section using the
ModiÔ¨Åed Perturbation Method.
Once A is determined in this way, it follows that the amplitude of cos œât
in each of the subsequent iterations will also be A.
6.2.3
Poincar¬¥e‚ÄìLinstedt Method
In the two preceding sections, the forcing frequency œâ was given and the
response‚Äôs amplitude was determined. It is possible to reverse the procedure
by specifying the amplitude of the response and asking what frequencies
give a 2œÄ-periodic oscillation having this amplitude.
Again, we consider the equation
d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos œât).
We make the change of variables œât ‚Üít with the result
œâ2 d2x
dt2 + x = Œµ(‚àíŒ±x ‚àíŒ≤x3 + C cos t).
Determine œâ and x such that the periodicity condition
x(t + 2œÄ) = x(t)
is satisÔ¨Åed as well as the initial conditions
x(0) = A,
x‚Ä≤(0) = 0,
which specify that x has A as a (local) maximum value.
We begin by expanding x and œâ in power series:
x
=
x0(t) + x1(t)Œµ + ¬∑ ¬∑ ¬∑
œâ
=
œâ0 + œâ1Œµ + ¬∑ ¬∑ ¬∑ .
Setting Œµ = 0 in the problem gives
œâ2
0
d2x0
dt2 + x0 = 0
x0(t + 2œÄ) = x0(t)
x0(0) = A,
x‚Ä≤
0(0) = 0.

178
6. Iterations and Perturbations
The periodicity condition requires that
œâ0 = 1
and
x0(t) = A cos t.
DiÔ¨Äerentiating the equation with respect to Œµ and setting Œµ = 0 (Method
II in Section 5.1.3) gives
œâ2
0
d2x1
dt2 + x1 = ‚àíŒ±A cos t ‚àíŒ≤A3 cos3 t + C cos t ‚àí2œâ0œâ1
d2x0
dt2 ,
or
d2x1
dt2 + x1 =

C ‚àíŒ±A ‚àí3Œ≤A3
4
+ 2œâ1A

cos t ‚àíŒ≤A3
4
cos 3t.
We choose œâ1 to cancel the remaining part of the coeÔ¨Écient of cos t in the
right-hand side:
œâ1 = Œ±A + 3Œ≤A3/4 ‚àíC
2A
.
Therefore, we have
œâ = 1 + ŒµŒ±A + 3Œ≤A3/4 ‚àíC
2A
+ O(Œµ2).
If we are interested in the frequency near 1, we write
œâ = 1 + Œµ¬µ/2,
which is the expansion of œâ2 = 1 + Œµ¬µ for Œµ near zero, and we get
3Œ≤A3
4
+ (Œ± ‚àí¬µ)A ‚àíC + O(Œµ) = 0.
This relation between forcing frequency and response amplitude agrees with
those derived in the preceding sections.
The Poincar¬¥e‚ÄìLinstedt method is quite similar to the ModiÔ¨Åed Perturba-
tion Method, but its point of view is reversed as noted earlier. The method
is sometimes referred to as the perturbation method, and sometimes it is
called the Method of Suppressing Secular Terms. (Secular terms refer to
nonperiodic solutions that grow slowly.) They can be removed by choosing
constants such that coeÔ¨Écients of terms causing resonance are zero, as we
chose œâ1 here. This is very much in the spirit of the ModiÔ¨Åed Perturba-
tion Method, and its validity is established in our discussion of Averaging
Methods in Chapter 7.
6.2.4
Frequency-Response Surface
Each of the methods used above leads to a frequency-response relation of
the form
A3 ‚àí4(¬µ ‚àíŒ±)A
3Œ≤
‚àí4C
3Œ≤ = 0,

6.2. DuÔ¨Éng‚Äôs Equation
179
 A
 C
¬µ‚àíŒ±
Figure 6.1. Cusp frequency-response surface.
although DuÔ¨Éng‚Äôs iteration method involves the least work in deriving this
formula. This formula deÔ¨Ånes a surface in AC¬µ-space, and it is depicted
in Figure 6.1, where it is assumed that all coeÔ¨Écients in this equation are
positive.
Fixing the forcing amplitude C determines a cross-section of this surface.
Several of these are depicted in Figure 6.2, where we plot the response
amplitude |A|.
The restoring force Œ±x + Œ≤x3 is due to the potential energy
U(x) = Œ±x2
2
+ Œ≤x4
4 .
In analogy with a mechanical spring, the case where Œ≤ > 0 is referred to as
that of a hard spring, since increasing extension (x) increases the restoring
force. The case of Œ≤ < 0 is that of a soft spring, since the restoring force
eventually decreases with increasing extension. These diagrams show that
there is a nonunique response of the system for most parameter values. More
insight into this comes from considering DuÔ¨Éng‚Äôs equation with damping,
as in Section 6.2.6.
6.2.5
Subharmonic Responses of DuÔ¨Éng‚Äôs Equation
The periodic solutions found in the preceding section are harmonics of
the forcing; that is, they have the same period as the forcing function.
Many other periodic solutions of DuÔ¨Éng‚Äôs equation are excited by periodic
forcing. Solutions having least period some integer multiple of the forcing
period are common; these are called subharmonics (lower frequency). Solu-
tions where the least period is the forcing period divided by an integer are

180
6. Iterations and Perturbations
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
 
|A|
Figure 6.2. Frequency-response for hard springs, where Œ≤ > 0. For the soft spring
where Œ≤ < 0, the Ô¨Ågure is reversed to the right. C > 0 is Ô¨Åxed.
called higher harmonics, because their frequencies are integer multiples of
the base frequency.
Subharmonics can be found by using DuÔ¨Éng‚Äôs iterative method and a
suitable initial guess that picks them out. For example, let
x0 = a cos t
3 + A cos t
be our initial guess for the 1/3 harmonic of the equation
œâ2 d2x
dt2 = ‚àíŒ±x ‚àíŒ≤x3 + F cos t,
where œâ is the forcing frequency. Our guess has a least period of 6œÄ. Iter-
ating beginning with x0 and equating coeÔ¨Écients of the iterate to those of
x0 give
a

Œ± ‚àíœâ2
9

+ 3Œ≤
4 (a3 + a2A + 2aA2)
=
0
A(Œ± ‚àíœâ2) + Œ≤
4 (a3 + 6a2A + 3A3)
=
F.
Let us consider this system in the linear case where Œ≤ = 0. There are
nonzero solutions for a and A only if œâ = ‚àöŒ± or œâ = 3‚àöŒ±. There are
nearby solutions of these equations if Œ≤ is near zero and œâ is near these two
values.

6.2. DuÔ¨Éng‚Äôs Equation
181
The important point here is that the linear problem gives the hint of
where to look for subharmonics in the space of all possible parameters (Œ±, Œ≤,
and œâ) appearing in the problems. If the forcing frequency is a rational
multiple of the free frequency of the linear problem, then we might expect
a comparable oscillation for the nonlinear problem. This may or may not
happen, but it usually gives a starting point.
6.2.6
Damped DuÔ¨Éng‚Äôs Equation
Now consider the equation
d2x
dt2 + rdx
dt + Œ±x + Œ≤x3 = a cos œât + b sin œât,
where r > 0 is the damping coeÔ¨Écient. Let us use DuÔ¨Éng‚Äôs method to
study this equation. We begin with an initial guess
x0 = A cos œât
and deÔ¨Åne the iteration
d2x1
dt2 = ‚àírdx0
dt ‚àíŒ±x0 ‚àíŒ≤x3
0 + a cos œât + b sin œât.
Integrating this equation gives
x1(t) = (3Œ≤A3/4) + Œ±A ‚àía
œâ2
cos œât ‚àíb + œâAr
œâ2
sin œât + h.f.t.,
where h.f.t. denotes higher frequency terms. Setting this equal to the initial
guess gives
b
=
‚àíAœâr
a
=
3Œ≤A3
4
+ Œ±A ‚àíAœâ2.
Let us rewrite the forcing term as an amplitude C and a phase lag œÜ,
where C2 = a2 + b2 and œÜ = ‚àítan‚àí1(b/a). With this, a cos œât + b sin œât =
C cos(œât + œÜ), and
C2 = (œâAr)2 +
3Œ≤A3
4
+ (Œ± ‚àíœâ2)A
2
.
Now the cusp singularity is at C2‚àí(œâAr)2. If r is small, the response shown
in Figure 6.3 is similar to that in Figure 6.2, with the important exception
that there is a unique small amplitude oscillation when |œâ2 ‚àía| ‚â´1.
Figure 6.3 explains some interesting physical behavior of electrical cir-
cuits [64]. If we begin with an oscillator with Œ≤ > 0, Œ± > œâ2, then the
system oscillates with small amplitude when Œ±‚àíœâ2 ‚âà4. As the forcing fre-
quency œâ increases, the oscillation remains on the top branch until the fold
is reached. It then jumps to the lower branch, that is, to a small amplitude
oscillation. This kind of discontinuous behavior is observed in what appear

182
6. Iterations and Perturbations
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
0
0.5
1
1.5
2
2.5
 
|A|
Figure 6.3. Frequency-response for damped DuÔ¨Éng equation, in the hard spring.
For the soft spring the Ô¨Ågure is reversed to the right.
to be very smooth systems. We say that there is a hysteretic response of
the system. For example, after this experiment establishes the oscillation
on the lower branch far to the left of the vertical axis, we can begin to
decrease œâ. As we do this, the oscillation remains small in amplitude until
the lower fold is reached near the vertical axis, when the oscillation jumps
to higher amplitude.
6.2.7
DuÔ¨Éng‚Äôs Equation with Subresonant Forcing
The resonant forcing theorem applies as it did in Section 6.1.3 when we
replace ŒµG by
g(t) + ŒµF(t, x, Œµ),
where g is subresonant. This is illustrated here with a calculation using the
ModiÔ¨Åed Perturbation Method (See Section 6.1.4). Consider the equation
d2x
dx2 + ¬µ2x = C cos t ‚àíŒµ(Œ±x + Œ≤x3),
where ¬µ Ã∏= ¬±1 is an integer. The forcing function cos t has period 2œÄ, and
the two linearly independent solutions of the homogeneous equation
d2x
dt2 + ¬µ2x = 0

6.2. DuÔ¨Éng‚Äôs Equation
183
also have period 2œÄ, but the forcing is orthogonal to them.
The modiÔ¨Åed perturbation method gives a way to construct periodic so-
lutions of this equation. We introduce new variables Œª and Œ∑ and constraints
a and b as follows:
d2x
dt2 + ¬µ2x = C cos t ‚àíŒµ(Œ±x + Œ≤x3) + ŒµŒª cos ¬µt + ŒµŒ∑ sin ¬µt
(x, cos ¬µt) = a
and
(x, sin ¬µt) = b
x(t + 2œÄ) = x(t)
for all t.
We successively diÔ¨Äerentiate this problem with respect to Œµ, a, and b, and so
derive a sequence of problems for expansion coeÔ¨Écients (the subscripts here
denote diÔ¨Äerentiations and numbered subscripts denote diÔ¨Äerentiations
with respect to Œµ):
d2x0
dt2 + ¬µ2x0 = C cos t
(x0, cos ¬µt) = 0
and
(x0, sin ¬µt) = 0
d2x1
dt2 + ¬µ2x1 = ‚àíŒ±x0 ‚àíŒ≤x3
0 + Œª0 cos ¬µt + Œ∑0 sin ¬µt
(x1, cos ¬µt) = 0
and
(x1, sin ¬µt) = 0
d2xa
dt2 + ¬µ2xa = 0
(xa, cos ¬µt) = 1
and
(xa, sin ¬µt) = 0
d2xb
dt2 + ¬µ2xb = 0
(xb, cos ¬µt) = 0
and
(xb, sin ¬µt) = 1
d2xaŒµ
dt2
+ ¬µ2xaŒµ = ‚àí(Œ± + 3Œ≤x2
0)xa + Œªa cos ¬µt + Œ∑a sin ¬µt
(xaŒµ, cos ¬µt) = 0
and
(xaŒµ, sin ¬µt) = 0
d2xbŒµ
dt2
+ ¬µ2xbŒµ = ‚àí(Œ± + 3Œ≤x2
0)xb + Œªb cos ¬µt + Œ∑b sin ¬µt
(xbŒµ, cos ¬µt) = 0
and
(xbŒµ, sin ¬µt) = 0,
etc.

184
6. Iterations and Perturbations
The solution of the Ô¨Årst problem is
x0(t) = C cos t
(¬µ2 ‚àí1).
Note that Œª0 is zero if ¬µ Ã∏= 3, but otherwise it is not zero. The other
solutions are
Œ∑0 = 0
xa(t)
=
cos ¬µt
xb(t)
=
sin ¬µt
Œªa
=
Œ±
œÄ +
3Œ≤C2
2(¬µ2 ‚àí1)2
Œ∑b
=
Œ±
œÄ +
3Œ≤C2
2(¬µ2 ‚àí1)2
Œªb
=
0
Œ∑a
=
0,
etc.
We have enough here to demonstrate two important points. First, if ¬µ = 3
and C Ã∏= 0(Œµ), then Œª0 Ã∏= 0, and we have no hope of solving the bifurcation
equation
Œª = 0
for small a, b, and Œµ. Therefore, the method does not produce a solution
because of interference of higher harmonics. Second, if ¬µ = 3 and C = O(Œµ)
or if ¬µ Ã∏= 3, then the Jacobian matrix of the bifurcation equations has
determinant
det‚àÇ(Œª, Œ∑)
‚àÇ(a, b) =
Œ±
œÄ +
3Œ≤C2
2(¬µ2 ‚àí1)2
2
.
If this quantity is not zero, there are unique functions
a = a(Œµ)
and
b = b(Œµ)
that solve the bifurcation equations
Œª(a, b, Œµ)
=
0
Œ∑(a, b, Œµ)
=
0,
and the resulting function x(t, a(Œµ), b(Œµ), Œµ) is a 2œÄ-periodic solution of
DuÔ¨Éng‚Äôs equation.
6.2.8
Computer Simulation of DuÔ¨Éng‚Äôs Equation
The behavior described in Figure 6.3 can be illustrated by direct computa-
tion of the solutions to DuÔ¨Éng‚Äôs equation. Figure 6.4 depicts a portion of

6.2. DuÔ¨Éng‚Äôs Equation
185
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1
0.8
0.6
0.4
0.2
0
‚àí.2
‚àí.4
‚àí.6
‚àí.8
‚àí1
Figure 6.4. Fifty iterations of each of 6 initial points along the Àôx = 0 axis. This
shows one small stable response for 0.5¬®x + 0.01 Àôx + x ‚àíx3 = 0.04 cos t.
the phase plane for (free) DuÔ¨Éng‚Äôs equation. The center is surrounded by
a continuum of periodic orbits that extend to the separatrices that join the
two saddle points. Plotted are values {(x(2œÄn), y(2œÄn))} (where y = x‚Ä≤)
for many choices of n and for many choices of initial conditions x(0), y(0).
Shown in Figure 6.5 is Poincar¬¥e‚Äôs mapping for a damped equation that
is externally forced:
0.81d2x
dt2 + 0.05dx
dt + x ‚àíx3 = cos t.
This plot is obtained by selecting an initial point, say x(0) = Œæ, dx/dt(0) =
Œ∑, solving the equation, and plotting the solution point each time t hits an
integer multiple of 2œÄ. The plot is generated by repeating this simulation
for many initial points.
We found here a unique stable periodic solution of period 2œÄ. In Fig-
ure 6.6 we solve the same equation but with weaker damping. In this case
we see two stable periodic solutions, one corresponding to each of the top
and bottom branches in Figure 6.3.
The basins of attraction of these two oscillations are separated by a
fractal curve that is in some sense random. To understand this, we turn to
a discussion in Section 6.3 of fractal curves and chaotic behavior.

186
6. Iterations and Perturbations
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1
0.8
0.6
0.4
0.2
0
‚àí.2
‚àí.4
‚àí.6
‚àí.8
‚àí1
Figure 6.5. Two stable responses: 0.6¬®x + 0.01 Àôx + x ‚àíx3 = 0.04 cos t.
6.3
Boundaries of Basins of Attraction
When two stable modes of oscillation coexist, as in the cases shown in Fig-
ures 6.3 and 6.5, where the response curve has doubled back on itself, there
comes an important question. Given an initial point, what will happen to
its solution? There are three possibilities: The solution can approach one
of the two stable oscillations or it can remain on a boundary separating
their basins of attraction. This can be important in meeting design speci-
Ô¨Åcations and in determining stability of a designed system. Unfortunately,
the boundary between two basins of attraction can be quite complicated.
In particular, there can be exquisitely sensitive dependence of solutions on
their initial positions. This fact was known to Poincar¬¥e and others in the
last century, and a number of researchers since then have studied aspects
of this phenomenon. It is closely related to the chaotic behavior described
in Chapter 2.
Newton‚Äôs method gives an excellent starting point for a discussion of
basins of attraction. It can exhibit multiple coexisting static states and
complicated boundaries separating their basins of attraction. Next, some
computer simulations are used to illustrate chaotic behavior in DuÔ¨Éng‚Äôs
and van der Pol‚Äôs oscillators. The boundaries between basins of attraction
can be very complicated, and our discussion of fractals provides some in-

6.3. Boundaries of Basins of Attraction
187
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1
0.8
0.6
0.4
0.2
0
‚àí.2
‚àí.4
‚àí.6
‚àí.8
‚àí1
Figure 6.6. One large, stable response: 0.9¬®x + 0.01 Àôx + x ‚àíx3 = 0.04 cos t.
sight into such structures. Finally, a computer simulation to estimate the
nature of the boundary in speciÔ¨Åc cases is described.
6.3.1
Newton‚Äôs Method and Chaos
Newton‚Äôs method gives us one of the best available algorithms for Ô¨Ånding
the zeros of a function. It is a recursive scheme whose iterates move in the
space in which solutions lie. It also presents an interesting introduction to
chaos.
Recall that if f(x) is a smooth real-valued function, and if we seek the
values of x for which f(x) = 0, then we can proceed by successive linear
extrapolations. Let us make an initial guess, say x0, and deÔ¨Åne a sequence
of approximations {xn} by the algorithm
xn+1 = xn ‚àíf(xn)
f ‚Ä≤(xn)
as long as this makes sense. This formula deÔ¨Ånes Newton‚Äôs method. If
f ‚Ä≤(x‚àó) Ã∏= 0, where x‚àóis the zero near x0, then this sequence will converge
to x‚àó, and it will usually do so at a quadratic rate. Thus, quite rapid
convergence results when the method works.
Sarkovski studied what happens when the condition f ‚Ä≤(x‚àó) Ã∏= 0 is not
satisÔ¨Åed [126]. He derived the following interesting results, which were de-

188
6. Iterations and Perturbations
scribed in Section 2.5.1. A computer simulation was designed and carried
out [76] to Ô¨Ånd the zero of the function
f(x) =
|x ‚àír + 1|
x
1/(r‚àí1)
,
where r > 1. This function has a discontinuous derivative at its zero x =
r ‚àí1 (i.e., f(r ‚àí1) = 0). If we derive Newton‚Äôs algorithm for this, we get
for 1 < r ‚â§4
xn+1 = rxn(1 ‚àíxn).
For these values the sequence of iterates lies in the unit interval 0 ‚â§x ‚â§1,
which we studied in Section 2.5.1.
6.3.2
Computer Examples
The one-dimensional mappings do not elucidate the basin-of-attraction
boundary problem, but three other examples are presented here to do this.
Consider a mapping of the plane E2 into itself,
F : x = (x, y) ‚Üí(f(x, y), g(x, y)) = F(x).
Suppose that F has two Ô¨Åxed points, say F(a) = a and F(b) = b. Fur-
thermore, suppose that each of these Ô¨Åxed points is stable. We deÔ¨Åne the
basins of attraction of these points to be
B(a) = (x ‚ààE2 : F n(x) ‚Üía as n ‚Üí‚àû),
where F n(x) = F(F n‚àí1(x)). We deÔ¨Åne B(b) similarly.
The boundary between these two sets might be simple or very compli-
cated. A question of interest is, given a point in E2, does it lie in B(a) or
in B(b)? Points that lie away from the boundary are decidable, but those
that lie on it are not. The following examples illustrate several interesting
facts.
The Roots of z3 ‚àí1.
First, let us consider iterates of Newton‚Äôs method for the complex valued
function f(z) = z3 ‚àí1. There are three zeros of this function, one at each
of the three cubic roots of unity: z = 1, z = ‚àí1/2 ¬± i
‚àö
3/2. Iterates of
this mapping were studied by Fatou, Julia, and Kinney and Pitcher among
many others. It is known that the three domains of attraction, one each
for the three roots, are hopelessly intertwined. In fact, at any point that
bounds any two of these domains there are points of the third domain
arbitrarily close by. The set bounding these three basins of attraction is
now referred to as a Julia set (see Figure 6.7) [36, 93, 107].
DuÔ¨Éng‚Äôs Equation.
Figure 6.5 shows a case where there is a unique periodic response. In
Figure 6.6 two stable responses coexist. The two basins of attraction in

6.3. Boundaries of Basins of Attraction
189
‚àí1.6
0
2
1.6
0
‚àí2
Figure 6.7. Julia set for the basins of attraction of the third roots of unity using
Newton‚Äôs method to solve z3‚àí1 = 0. Each of the three shaded regions is attracted
to one of the roots. The Julia set is in black. These are points on the boundaries
of the three basins of attraction. The real axis points down here. z = 1 is at the
bottom center of this Ô¨Ågure.
this case are separated by a complicated curve. We return later to Ô¨Ånding
out how complicated this curve can be.
van der Pol‚Äôs Equation.
van der Pol‚Äôs equation has the form
Àôx + k(x3/3 ‚àíx) + y
=
0
Àôy
=
x ‚àíB cos t.
If k is large and B = kA, then all of the responses lie in an annulus enclosing
the knee-shaped curve y = k(x ‚àíx3/3), |x| ‚â§
‚àö
3 in Figure 6.8 [132, 98].
The solutions can be described in part by how many times they proceed
around the origin for each oscillation of the forcing function. This measures
the output frequency relative to the input frequency. The resulting ratio is
called the rotation number, and its calculation [38] is shown in Figure 6.9.
We see here that there are overlapping sets where two stable periodic
solutions having diÔ¨Äerent periods coexist. Since these two solutions lie in
the annulus described above, their domains of attraction must be very
mixed up (see [64])!

190
6. Iterations and Perturbations
 x
 y
 dx/dt = 0
Figure 6.8. Phase portrait of a forced van der Pol equation.
6.3.3
Fractal Measures
These examples show that quite complicated basin boundaries can occur
in simple problems. It is therefore of some interest to understand ‚Äúvery
wrinkled‚Äù curves. Figure 6.10 describes the construction of a sequence of
curves that have common endpoints but become successively more complex;
in particular, they become longer even though they keep a recognizable
shape.
In four steps the curve becomes quite complicated, and as the process
continues, we rapidly lose track of it. We can view each step of this process
as a measurement. For example, at Ô¨Årst, we use a rod of length 1; second,
one of length 1/4, and so on. Obviously, the process results in some limiting
set, but the lengths of the approximating curves increase without bound.
Apparently, this problem was observed by cartographers trying to measure
the length of England‚Äôs coastline (see [107]).
We are interested in how the length of the approximating curve changes
with the length of the measure used. For example, if L(Œµ) denotes the length
of the approximating line that is based on using a measure Œµ, then for the
example in Figure 6.10, the length doubles when the measure is quartered:
L(Œµ/4) = 2L(Œµ).
This gives a recursion formula between the lengths using diÔ¨Äerent measures.
We can solve this recursion using the z transform; that is, we set L(Œµ) = ŒµŒ±.
Solving for Œ± in the result gives
Œ± = ‚àílog 2
log 4 = ‚àí1
2.

6.3. Boundaries of Basins of Attraction
191
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xx
xx
xx
xx
xxxxx
xxxxx
xxxxx
xxxxx
xxxxx
1/3
1/2
1/5
1/7
5/9
2/3
3/5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
B
0.01
0.05
0.1
0.15
Œµ
1/9
1/13
Figure 6.9. Rotation number for van der Pol‚Äôs equation (redrawn from [38]).
A trivial example is where we successively partition a straight-line seg-
ment. In this case, the length of the line remains constant no matter what
value of Œµ we take. For example,
L(Œµ/2) = 1
for all Œµ. Thus, Œ± = 0 for a straight-line fractal. However, our intuition is
that a straight-line segment should have dimension 1, so we deÔ¨Åne
dc = 1 ‚àíŒ±.
Thus, for the straight-line fractal dc = 1, and for the curve in Figure 6.10
dc = 3/2. The notation dc is used to indicate that we have computed
the Kolmogorov capacity of the curve [110]. Figure 6.11 shows some other
examples.
6.3.4
Simulation of Fractal Curves
Now, suppose that two sets A and B make up a set whose area is 1.
Moreover, suppose that these two sets are separated by a fractal curve
having dimension dc.

192
6. Iterations and Perturbations
A
B
L   = 1 
= 1 
Œµ
A
B
L =
   = 8 / 4 = 2 
= 1 / 4 
Œµ
A
B
L =
   = 64 / 16 = 4 
= 1 / 16 
Œµ
A
B
L =
   = 256 / 64 = 16 
= 1 / 64 
Œµ
Figure 6.10. Complicated basin boundaries. L denotes the length of the curve.
At each step the linear segments are split into four equal parts and rearranged.
Œµ is the length of the measuring rod at each step.
We now perform calculation in which we place a point in this set and
observe whether it lies in A or B. We suppose that our placement of the
point is accurate only to within some tolerance, say Œ¥. Therefore, our ex-
periment corresponds to placing a disk of radius Œ¥ on the diagram. We say
that the experiment is undecidable if the disk intersects the boundary of
A or the boundary of B, and so the relevant thing here is the area of a
Œ¥-neighborhood of the boundaries of these two sets. This area should be the
length of the boundary of A plus the length of the boundary of B multi-
plied by Œ¥. However, a fractal boundary has inÔ¨Ånite length. Let L(Œ¥) denote
the length of the common boundary of the two sets using a measuring rod
of length Œ¥. Then the area covered by this approximation is proportional
to
L(Œ¥)Œ¥ = Œ¥1+Œ± = Œ¥2‚àídc.

6.3. Boundaries of Basins of Attraction
193
1. Cantor`s middle thirds set:
     =  log 2 / log 3
d c
2.
     =  log 5 / log 3
d c
3. Peano`s curve
     =  log 9 / log 3 = 2
d c
A
B
A
B
Figure 6.11. Capacities of various fractal curves.
For example, if Œ¥ = 1.0 √ó 10‚àí10 and dc = 1.9, then the area of this ‚ÄúÔ¨Çat-
tened‚Äù boundary is comparable to 0.1. Thus, ten orders of magnitude of
precision are lost [35].
Fractals in Higher Dimensions.
Kolmogorov‚Äôs capacity is easy to deÔ¨Åne for sets in an arbitrary Euclidean
space. Consider a bounded set, say S, in EM. Given a number Œµ, let N(Œµ)
denote the number of nonoverlapping (M-dimensional) cubes needed to
cover S. DeÔ¨Åne
dc = lim
Œµ‚Üí0+
log N(Œµ)
log 1/Œµ .
This limit need not exist, but when it does, this is called the fractal
dimension of S.
Note that for small values of Œµ
log N(Œµ) ‚âàdc log(1/Œµ).
This suggests a method for approximating dc for a given curve C. Fix Œµ,
approximate C using measures of length Œµ, determine N(Œµ),and plot the
values of N and 1/Œµ on log‚Äìlog coordinates. Doing this for several values
of Œµ gives a straight line whose slope is dc.
HausdorÔ¨ÄdeÔ¨Åned a measure of complicated sets [117]. Let a set S in E2
be covered by a collection of disks, say Di, which have radii Œµi, respectively.
We deÔ¨Åne a number
‚àÜ(d) =
inf
partitions

i
Œµd
i ,
where the inÔ¨Åmum is taken over all possible coverings of S by disks having
radii Œµi. HausdorÔ¨Äshowed that ‚àÜ= ‚àûwhen d is large, and ‚àÜ= 0 when
d is small. There is a unique value of d for which 0 < ‚àÜ(d) < ‚àû. This
value of d, say dH, is called the HausdorÔ¨Ädimension of S. It is clear that

194
6. Iterations and Perturbations











J
J
J
J
J
J
J
J
J
J
J
J
J
J
J
JJ

















J
J
J
J
J
J
J
J
J
J
J
J
J
J
J
JJ






J
JJ



J
JJ



J
JJ



Figure 6.12. Construction of the Triangle Fractal.
dc ‚â•dH for any set, since dc is obtained by using a special covering of S.
(See [35] for a further discussion of these topics.)
6.4
Exercises
6.1.
Show that (Lx, y) = (x, Ly) when Lx = x‚Ä≤‚Ä≤ + ¬µ2x, x(t) and y(t)
are functions whose second derivatives are square integrable and the
notation (f, g) is the inner product notation deÔ¨Åned in Section 6.1.
Compare this result with the analogous calculation for matrices: De-
termine for which N √ó N matrices A we have Ax ¬∑ y = x ¬∑ Ay for all
N-vectors x and y.
6.2.
Verify that the amplitude A of successive iterates in DuÔ¨Éng‚Äôs method
solves
3Œ≤A3
4
+ (Œ± ‚àí¬µ)A ‚àíC = 0
as in Section 6.2.2.
6.3.
Reproduce Poincar¬¥e‚Äôs mapping for DuÔ¨Éng‚Äôs equation in Figures 6.4,
6.5, and 6.6, using computer simulations.
6.4.
Construct the triangle fractal.
On the left in Figure 6.12 is an equilateral triangle of altitude 1.
Begin with the inscribed triangle on the left. What is its length? Next,
inscribe three triangles in the residual space as shown on the right.
What is the total length of the edges of the four inscribed triangles?
Continue this process. What is the capacity dc of the limit set?

7
Methods of Averaging
Regular perturbation methods are based on Taylor‚Äôs formula and on im-
plicit function theorems. However, there are many problems to which
Taylor‚Äôs formula cannot be applied directly. In these cases perturba-
tion methods based on multiple time or space scales can often be used,
sometimes even for chaotic systems.
Relatively fast and slow scales can occur in many ways. For example,
suppose that A is a 4 √ó 4 matrix having eigenvalues
Œª1 = œÅ + iœâ,
Œª2 = œÅ ‚àíiœâ,
Œª3 = ¬µ,
Œª4 = Œ∑,
where |œÅ| ‚â™œâ and Œ∑ ‚â™¬µ < 0. The spectral decomposition of A is (see
Section 1.2.1)
A =
4

j=1
ŒªjPj,
where Pj, are projection matrices satisfying the conditions
PjPk = 0
if j Ã∏= k and P 2
k = Pk.
The solution of the initial value problem
dx
dt = Ax,
x(0) is given
can be derived using this notation:
x(t) =
4

j=1
eŒªjtPjx(0).

196
7. Methods of Averaging
 œÅ+  œâ
 ¬µ
 Œ∑
i
 œÅ‚àí  œâ
i
Figure 7.1. Spectrum of A with œÅ < 0.
The Ô¨Årst two terms in this sum are modulated at a slower rate than they
oscillate, since their ampliÔ¨Åcation rate œÅ is smaller than their frequency œâ.
The third term is slowly modulated relative to the fourth, which decays
rapidly to zero, since Œ∑ ‚â™¬µ. If we write Œµ = œÅ/œâ, then Œµ ‚â™1 and the Ô¨Årst
two terms can be written as
eœâ(Œµ+i)tP1x(0) + eœâ(Œµ‚àíi)tP2x(0).
We could say that these modes vary on two time scales: œât (oscillation)
and a slower one, Œµœât (modulation).
Similarly, setting Œµ‚Ä≤ = ¬µ/Œ∑ we can write the last two terms as
e(¬µt)P3x(0) + e(¬µt/Œµ‚Ä≤)P4x(0).
This is a sum of transients that decay on two time scales: ¬µt and a faster
one, ¬µt/Œµ‚Ä≤.
Plotting the eigenvalues of A can suggest what scales govern the so-
lution‚Äôs behavior, as shown in Figure 7.1. Eigenvalues lying near the
imaginary axis give rise to slowly modulated oscillations; eigenvalues ly-
ing near the origin correspond to quasistatic modes; and other eigenvalues
lying near the real axis describe rapidly growing or decaying modes.
This example shows that small parameters can appear in linear systems
as ratios of eigenvalues, and so the system‚Äôs solution can include various
scales. There are important implications of this for numerical methods,
since stability and accuracy of numerical schemes depend on step sizes
being appropriate to changes in solutions. Although eigenvalues can be
used to discover various scales in linear problems, they are often diÔ¨Écult
to determine, and there is no reliable method for Ô¨Ånding their analogues
in nonlinear problems. In fact, nonlinear problems typically involve scales

7. Methods of Averaging
197
that change with the system‚Äôs state. Some aspects o f this are studied here
and in Chapter 8.
In this chapter we study highly oscillatory problems, and in the next we
study problems with rapid transients. Highly oscillatory systems are studied
using averaging methods; we Ô¨Årst state and prove a general averaging result
that is used throughout this chapter. The ideas developed here are similar to
those of stability under persistent disturbances(Chapter 3), but in addition
they provide methods for constructing precise approximations to solutions.
A useful approximation shortcut that is based on a multiscale expansion
formalism is also developed in Section 7.1.
The general theory is later specialized and applied to a variety of prob-
lems. First, linear problems are studied in detail in Section 7.2, especially
certain linear feedback problems where detailed computations can be car-
ried out. In Section 7.3 the multiscale methodology is revised and applied
to nearly linear diÔ¨Äerence equations. We study a variety of iterations using
those results.
Weakly nonlinear problems are studied in Section 7.4; in particular, we
revisit van der Pol‚Äôs and DuÔ¨Éng‚Äôs equations as well as more general systems
of weakly coupled harmonic oscillators. Special cases of these are problems
for (angular) phase variables. In Section 7.5 three separate approaches to
phase-amplitude systems are derived: the rotation vector method (RVM),
Bogoliubov‚Äôs near-identity transformation for diÔ¨Äerential equations on a
torus, and the Kolmogorov‚ÄìArnol‚Äôd‚ÄìMoser (KAM) theory for quasiperiodic
solutions. Section 7.6 describes how microscopic spatial structure can be
used to derive macroscopic properties of Ô¨Åne periodic structures in space.
This is the method of homogenization. Computational methods that are
based on averaging are described in Section 7.7. Bogoliubov also developed
averaging methods for dynamical systems that are perturbed by random
noise. The chapter ends with a brief discussion of averaging in systems
perturbed by random processes.
Averaging nonlinear conservative systems can be delicate because small
divisors can occur. These arise through nonlinear feedbacks that create
internal resonances. Many important techniques have been developed to
deal with small divisors, culminating (to date) in the RVM and the KAM
theories. These theories show how to deal with oscillatory problems in
which there is no dissipation. When dissipation is present in a system,
the theorem on nonlinear averaging for mean-stable systems is used to
construct solutions.
Problems encountered in highly oscillatory systems are already appar-
ent in linear systems. The following example is quite useful. Consider the
diÔ¨Äerential equation
dx
dt = U ‚àí1(t)BU(t)x,

198
7. Methods of Averaging
where x is in E2, U is the rotation matrix
U(t) =

cos œât
‚àísin œât
sin œât
cos œât

,
and
B =

a
b
c
d

.
This system was studied in Section 1.4. The change of variables y = Ux
takes this system into
dy
dt = (B + œâJ)y,
where J is Jacobi‚Äôs matrix
J =

0
‚àí1
1
0

.
The matrix U rotates a vector through an angle œât, so we refer to the system
for x as being the result of spinning the system for y. The eigenvalues of
this system are given by the solutions of the equation
Œª2 ‚àí(a + d)Œª + ad ‚àíbc + œâ(c ‚àíb) + œâ2 = 0.
We consider three interesting examples:
Example 1. Spinning Can Destabilize a Stable Linear System.
Let
B =

‚àí1
b
0
‚àí2

.
Then the eigenvalues of U ‚àí1AU are ‚àí1 and ‚àí2, but the eigenvalues of
B + œâJ are
Œª = ‚àí3 ¬±
‚àö
1 + 4bœâ ‚àí4œâ2
2
.
The neutral stability condition for this system (viz., Re Œª = 0) occurs when
the real part of the eigenvalues are zero: This happens when 4bœâ‚àí4œâ2 = 8.
Therefore, there are two regions, S and U, in the plane such that if (œâ, b)
is in S, then y = 0 is a stable spiral node. If (œâ, b) is in U, then it is an
unstable node! Thus, spinning the system 4œâ(b ‚àíœâ) > 8 can destabilize
it. W. A. Coppel [27] showed that in general, slow oscillations (|œâ| ‚â™1)
cannot destabilize the system if the eigenvalues of A have negative real
parts.
Example 2. Spinning Can Stabilize a Hyperbolic System. Let
B =
 0
1
1
0

.

7.1. Averaging Nonlinear Systems
199
The eigenvalues of this matrix are ¬±1, but the eigenvalues of B + œâJ are
¬±

(1 ‚àíœâ2). Therefore, if |œâ| < 1, then y = 0, and hence x = 0, is a
hyperbolic point (i.e., it is conditionally stable), but if œâ2 > 1, it is a stable
node. Thus, spinning can stabilize an unstable node.
Example 3. Rapid Spinning Averages the System. Finally, let us
expand U ‚àí1(t)BU(t) in its Fourier series:
U ‚àí1(t)BU(t) = B0 + h.f.t.,
where h.f.t. denotes higher frequency terms. In this case, the matrix B0 is
B0 =
Ô£´
Ô£¨
Ô£≠
a + d
2
‚àíc ‚àíb
2
c ‚àíb
2
a + d
2
Ô£∂
Ô£∑
Ô£∏,
whose eigenvalues are Œª¬±
0 = (a + d ¬± (c ‚àíb)i)/2.
The system for x is periodic in t, and so Floquet‚Äôs theory, which shows
that such a system can be transformed into a system with constant coeÔ¨É-
cients, leads to the matrix R of Section 1.4; in this case, R = B + œâJ. The
eigenvalues of B + œâJ for large œâ are given by the formulas
Œª¬± = ¬±iœâ + (a + d) ¬± i(c ‚àíb)
2
+ O
 1
œâ

.
Recall that Floquet‚Äôs Theorem states that a fundamental matrix for the
system for x is P(t) exp(Rt). In this case it is
U(t) exp((B + œâJ)t) ‚âàU(t)eB0t.
In summary, we have seen in these examples that spinning can destabilize
a stable system, and it can stabilize an unstable system. Finally, we have
seen that the average of a highly oscillatory system is closely related (by
Floquet‚Äôs theory) to the full problem. We investigate this connection next,
but in a more general setting.
7.1
Averaging Nonlinear Systems
Consider the system of diÔ¨Äerential equations
dx
dt = Œµf(t, Œµt, x, Œµ)
or its slow-time equivalent,
dx
dœÑ = f
œÑ
Œµ , œÑ, x, Œµ

,

200
7. Methods of Averaging
where x, f ‚ààEN, Œµ is a small real parameter, say |Œµ| ‚â™1, and the slow
time variable œÑ = Œµt is restricted to some Ô¨Ånite interval 0 ‚â§Œµt ‚â§T < ‚àû.
Suppose the following hypothesis is true.
Hypothesis H1. f(t, œÑ, x, Œµ) is a smooth function of its arguments for
0 ‚â§t ‚â§T/Œµ, 0 ‚â§œÑ ‚â§T, for x in some domain G lying in EN, and for
Œµ near zero. Moreover, suppose that f is an almost periodic function of
t, uniformly in the other variables. SpeciÔ¨Åcally, we assume that f can be
expanded in a uniformly convergent generalized Fourier series
f(t, œÑ, x, Œµ) = f0(œÑ, x, Œµ) +
‚àû

n=1
fn(œÑ, x, Œµ)eiœânt
where the frequencies œân satisfy 0 < œâ1 < œâ2 < ¬∑ ¬∑ ¬∑ .
The smooth functions fn are the amplitudes of the various modes of rapid
oscillation of f relative to t. When convenient, we write œâ0 = 0.
Functions of this kind can be indistinguishable from chaotic ones in prac-
tice, but there are Ô¨Åne points of distinction between chaotic functions, say
ones that have a power spectrum supported by an interval of frequencies,
and functions that have a convergent generalized Fourier expansion. In
particular, we can average the latter functions to good use.
If we attempt to average f with respect to t, we must evaluate the limit
‚ü®f‚ü©(œÑ, x, Œµ) ‚â°lim
T ‚Üí‚àû
1
T
 T
0
‚àû

n=0
fneiœântdt = f0(œÑ, x, Œµ).
Thus, f0 is the mean value of f with respect to t, provided that the series of
integrals converges. Unfortunately, the series appearing here need not con-
verge. For example, if fn = n‚àí2 and œân = 2‚àín, then although the series can
be integrated termwise, the small numbers œân appear in the denominators
of the integral, and the resulting series does not converge. This illustrates
the small divisor problem that is discussed further in Section 7.5.
7.1.1
The Nonlinear Averaging Theorem
The next condition ensures that the average of f exists.
Hypothesis H2. Let g = f ‚àíf0, and suppose that the integral
 t
0
g(t‚Ä≤, œÑ, x, 0)dt‚Ä≤ =
 t
0
[f(t‚Ä≤, œÑ, x, 0) ‚àíf0(œÑ, x, 0)]dt‚Ä≤
is bounded uniformly for 0 ‚â§t ‚â§T/Œµ (and so for 0 ‚â§œÑ ‚â§T) and for
x ‚ààG.
This condition is not the ‚Äúbest possible‚Äù one, but it suÔ¨Éces for our
purposes here and it is comparable to mixing conditions that arise naturally
in systems perturbed by random noise (see Section 7.8). Condition H2

7.1. Averaging Nonlinear Systems
201
is usually satisÔ¨Åed, although it can be diÔ¨Écult to verify in applications.
However, if f is a periodic function of t or if it is a Ô¨Ånite trigonometric
polynomial in t, then this condition is satisÔ¨Åed. On the other hand, as we
have seen, if the Fourier series for f is suÔ¨Éciently lacunary, say œân = 2‚àín,
then this condition might not be satisÔ¨Åed. More general conditions for
averaging nonlinear problems are given in [58, 77].
We have the following theorem:
Nonlinear Averaging Theorem. Suppose that conditions H1 and H2
are satisÔ¨Åed, and suppose that the averaged system
dX
dœÑ = f0(œÑ, X, 0),
X(0) = Œ∑
(7.1)
has a unique solution lying in G for 0 ‚â§œÑ ‚â§T. If Œµ0 is suÔ¨Éciently small
and if |Œµ| ‚â§Œµ0, then:
1. There is a unique solution of the problem
dx
dt = Œµf(t, Œµt, x, Œµ),
x(0) = Œ∑
(7.2)
for 0 ‚â§t ‚â§T/Œµ.
2. The solution lies in G.
3. There is a constant K depending on T and Œµ0 such that
|x(t) ‚àíX(Œµt)| ‚â§K|Œµ|
for 0 ‚â§t ‚â§T/Œµ.
This result shows that the solution of the averaged equation (7.1) ap-
proximates the solution of the full problem (7.2) having the same initial
data over large (growing) intervals, 0 ‚â§t ‚â§T/Œµ.
Proof of the Nonlinear Averaging Theorem. The theorem is proved in a
more general setting in [77]. The proof here follows from a straightforward
application of Gronwall‚Äôs inequality.
Let z = x(t, Œµ) ‚àíX(Œµt). This function satisÔ¨Åes the initial value problem
dz
dt
=
Œµf(t, Œµt, z + X, Œµ) ‚àíŒµf0(Œµt, X, 0)
=
Œµ[fx(t, Œµt, X, 0)z + f(t, Œµt, X, 0)
‚àíf0(Œµt, X, 0) + O(Œµ) + O(|z|2)]
z(0)
=
0,
where fx denotes the Jacobian matrix of the components of f with respect
to the components of x.

202
7. Methods of Averaging
First, we deÔ¨Åne the linear problem‚Äôs fundamental solution to be U(t, s),
which can be determined from the diÔ¨Äerential equation
dU
dt = Œµfx(t, Œµt, X(Œµt), 0)U,
U(s, s) = I.
Using this, we can convert the initial value problem for x into an equivalent
integral equation for z:
z(t) = Œµ
 t
0
U(t, s)(f(s, Œµs, X(Œµs), 0) ‚àíf0(Œµs, X(Œµs), 0) + O(Œµ) + O(|z|2))ds.
Let us deÔ¨Åne g(s) = f(s, Œµs, X(Œµs), 0) ‚àíf0(Œµs, X(Œµs), 0) and integrate this
formula once by parts. The result is
Œµ
 t
0
U(t, s)g(s)ds = ŒµU(t, s)
 s
0
g(s‚Ä≤)ds‚Ä≤
&&&&
t
0
‚àíŒµ
 t
0
dU
dt
 s
0
g(s‚Ä≤)ds‚Ä≤.
This shows that
Œµ
 t
0
U(t, s)g(s)ds = Œµ
 t
0
g(s)ds ‚àíŒµ2
 t
0
fxU(t, s)
 x
0
g(s‚Ä≤)ds‚Ä≤.
It follows from condition H2 that for 0 ‚â§t ‚â§T/Œµ, this integral is bounded
by K1Œµ, where K1 is a constant that depends on Œµ0 and T. Therefore,
|z(t)| ‚â§O(Œµ) + Œµ
 t
0
|U(t, s)|O(|z|2)ds.
It follows from Gronwall‚Äôs inequality (see Section 3.2.1) that |z(t)| = O(Œµ)
uniformly for 0 ‚â§t ‚â§T/Œµ.
7.1.2
Averaging Theorem for Mean-Stable Systems
Now suppose that T = ‚àûin conditions H1 and H2. Adding a stability
condition can justify an approximation that is valid on the entire half-line
0 ‚â§t < ‚àû. We say that the system is mean stable if condition H3 is true:
Hypothesis H3. The averaged equation
dX
dœÑ = f0(œÑ, X, 0)
has a rest point, say X‚àó, that is exponentially asymptotically stable.
With this additional condition, we have the following theorem.
Averaging Theorem for Mean-Stable Systems. Let f satisfy con-
ditions H1, H2, and H3. Let X denote the solution of the initial value
problem
dX
dœÑ = f0(œÑ, X, 0),
X(0) = Œ∑

7.1. Averaging Nonlinear Systems
203
for 0 ‚â§œÑ < ‚àû. If Œ∑ is near X‚àóand if Œµ > 0 is suÔ¨Éciently small, then the
problem
dx
dt = Œµf(t, Œµt, x, Œµ),
x(0) = Œ∑
has a unique solution for 0 ‚â§t < ‚àû, and
x(t, Œµ) = X(Œµt) + O(Œµ),
where the error estimate here holds as Œµ ‚Üí0+ uniformly for 0 ‚â§t < ‚àû.
This result is quite strong, since it gives a valid approximation of the
system‚Äôs solution over the entire half-line t ‚â•0. The stability condition H3
required for the result is quite restrictive, but it can be relaxed somewhat
to stability under persistent disturbances or uniform asymptotic stability
[15]. Note that the sign of Œµ must be positive in this result to ensure that the
stability estimates hold for t ‚Üí‚àû. This result also highlights the duality
between œÑ ‚Üí‚àûand Œµ ‚Üí0. The solution can be written as x(œÑ/Œµ, Œµ) =
X(œÑ) + O(Œµ), which decomposes x into two components, the behavior as
œÑ ‚Üí‚àûand the behavior as Œµ ‚Üí0.
Proof of the Mean Stable Averaging Theorem. The proof of this result
proceeds exactly like that of the last section except that we must extend
the estimate to the entire line t ‚â•0. First, note that condition H3 ensures
that the fundamental solution U(t, s) satisÔ¨Åes an estimate of the form
|U(t, s)| ‚â§Ke‚àíŒ±(t‚àís)
for 0 ‚â§s ‚â§t < ‚àûand for some positive constants K and Œ±. Then, we
deÔ¨Åne u(t) = |z(t)|eŒ±t as we did in using Gronwall‚Äôs Lemma to prove the
linear stability theorem in Section 3.2. It follows that u(t) increases at most
like a slow exponential function if Œµ is suÔ¨Éciently small. Therefore, |z(t)|
decays at an exponential rate with amplitude at most O(Œµ).
7.1.3
A Two-Time Scale Method for the Full Problem
Based on the last two theorems, we observe that two time scales (t and Œµt)
appear in the solutions, and it is useful to exploit this fact. To do this, we
write a solution of this equation as
x = u(t, œÑ, Œµ),
where t and œÑ = Œµt are treated as independent variables (when convenient).
Keep in mind that this guess for the form of a solution must be validated
a posteriori. We suppose that u is a smooth function of Œµ, so that
u(t, œÑ, Œµ) = u0(t, œÑ) + Œµu1(t, œÑ) + Œµ2u2(t, œÑ) + ¬∑ ¬∑ ¬∑ ,

204
7. Methods of Averaging
where the coeÔ¨Écients are assumed to be bounded functions for 0 ‚â§t, œÑ < ‚àû
to ensure that this expansion makes sense as Œµ ‚Üí0. The problem becomes
‚àÇu
‚àÇt + Œµ‚àÇu
‚àÇœÑ = Œµf(t, œÑ, u, Œµ).
We hope that the dependence of x on the two time scales t and œÑ is correctly
accounted for by this, and we proceed to construct Taylor‚Äôs expansion for
u by successively diÔ¨Äerentiating the equation with respect to Œµ:
‚àÇu0
‚àÇt
=
0
‚àÇu1
‚àÇt
=
f(t, œÑ, u0, 0) ‚àí‚àÇu0
‚àÇœÑ , . . . .
The Ô¨Årst equation implies that u0 does not depend on t, so we write
u0 = U0(œÑ).
The second equation can be integrated, and its solution is
u1(t, œÑ) = U1(œÑ) +
 t
0
f(t‚Ä≤, œÑ, U0(œÑ), 0)dt‚Ä≤ ‚àítdU0
dœÑ ,
where U1 is an unknown function. Since u1 is to be bounded, we divide
both sides by t and pass to the limit t = ‚àû. The result is
dU0
dœÑ
= lim
t‚Üí‚àû
1
t
 t
0
f(t‚Ä≤, œÑ, U0(œÑ), 0)dt‚Ä≤ = f0(œÑ, U0(œÑ), 0).
The process can be continued to determine more coeÔ¨Écients in the expan-
sion of u. At least we see that U0(œÑ) = X(œÑ) as we determined in the
nonlinear averaging theorem.
It is perhaps questionable that we can average over the t variable while
holding the œÑ variable Ô¨Åxed. Still, the results of this nonrigorous calculation
agree with those in the averaging theorems just proved, and so they produce
the correct equations when the conditions of those theorems are satisÔ¨Åed.
The two-time scale scheme provides a good probe to use in studying novel
problems, as we will see in studies of diÔ¨Äerence equations in Section 7.3 and
in homogenization problems in Section 7.6.
7.2
Highly Oscillatory Linear Systems
It is useful to go through the steps of the two averaging theorems for linear
systems.

7.2. Highly Oscillatory Linear Systems
205
7.2.1
dx/dt = ŒµB(t)x
Systems of the form
dx
dt = ŒµB(t)x,
x(0, Œµ) given,
where Œµ is a small parameter and B(t) is a matrix of smooth quasiperiodic
functions, say ones having uniformly convergent generalized Fourier expan-
sions, are diÔ¨Écult to solve in general. Even if B is a matrix of constants,
calculating values of the exponential eŒµBt can pose serious computational
challenges [114], and if B(t) is periodic, determination of Floquet‚Äôs matrix
(R) is usually diÔ¨Écult. Still, systems in this form are quite useful, and we
study next how they can be analyzed using the method of averaging.
Let us Ô¨Årst rewrite the equation as
dx
dt = ŒµB0x + Œµ(B(t) ‚àíB0)x,
where B0 denotes the average of B(t):
B0 = lim
T ‚Üí‚àû
1
T
 T
0
B(t‚Ä≤)dt‚Ä≤.
Thus, B0 is made up of all of the constant terms in B(t). Note that if B(t)
is a matrix of functions that are 2œÄ-periodic in t, then
B0 = 1
2œÄ
 2œÄ
0
B(t)dt.
The averaged equation is
dX
dt = ŒµB0X,
and its solution is
X(Œµt) = eŒµB0tX0.
There are three cases, depending on over what interval we are interested
in approximating. These are summarized in the following theorem:
Averaging Theorem for Linear Systems.
1. 0 ‚â§t ‚â§T (< ‚àû). On this interval,
x(t, Œµ) = x(0, 0) + O(Œµ),
where the error estimates hold as Œµ ‚Üí0 uniformly for t ‚àà[0, T].
2. 0 ‚â§t ‚â§T/Œµ. In this case,
x(t, Œµ) = exp(ŒµB0t)x0 + O(Œµ),

206
7. Methods of Averaging
where the error estimate holds uniformly for 0 ‚â§t ‚â§T/Œµ. Therefore,
we have that
x(t, Œµ) = X(tŒµ) + O(Œµ).
3. 0 ‚â§t < ‚àû. Suppose that there are positive constants K and Œ± such
that
|eŒµB0t| ‚â§Ke‚àíŒ±Œµt
for 0 ‚â§t < ‚àû. Then x(t, Œµ) = X(tŒµ)+O(Œµ), where the error estimate
holds as Œµ ‚Üí0+ uniformly in t, 0 ‚â§t < ‚àû. (For example, if all
eigenvalues of B0 have negative real parts, then the averaged equation
gives an approximation to x that is accurate for all t.)
These results follow from direct applications of the nonlinear averaging
theorems to the linear problem. The Ô¨Årst of these three cases is not very
interesting, since no inÔ¨Çuence of B is felt to the order of accuracy O(Œµ) for
0 ‚â§t ‚â§T. However, the other two cases are of great interest.
7.2.2
Linear Feedback System
A linear feedback system of the form
dx
dt = (A + ŒµC)x,
where A is an oscillatory matrix (i.e., A is a diagonalizable matrix having
purely imaginary eigenvalues ¬±iœâj), can be analyzed using the averaging
theorems. We spin the system as before: Let
x = exp(At)y.
Then
dy
dt = Œµe‚àíAtCeAty,
where now the coeÔ¨Écient matrix is a linear combination of oscillatory
functions that can be found using the spectral decomposition of A.
In fact, if A has the form
A =
2N

j=1
iœâjPj,
we can calculate B0 directly. Since
B(t) = e‚àíAtCeAt

7.3. Averaging Rapidly Oscillating DiÔ¨Äerence Equations
207
is a matrix of trigonometric polynomials, we know that B0 exists, and it is
given by the formula
B0 =

œâj=œâk
PjCPk,
where the sum is taken over all indices j and k for which œâj = œâk.
7.2.3
Averaging and Laplace‚Äôs Method
Laplace studied highly oscillatory integrals, and he derived various
asymptotic expansions for them. A particular example was described in
Section 5.1.4. In general, Laplace‚Äôs methods are based on the method of
steepest descent, but they take a simpler form in the case we considered in
the last section, where
dy
dœÑ = e‚àíAœÑ/ŒµCeAœÑ/Œµy.
Integrating this equation gives an equivalent integral equation for y(œÑ):
y(œÑ)
=
y(0) +
 œÑ
0
e‚àíAs/ŒµCeAs/Œµy(s)ds
=
y(0) + B0
 œÑ
0
y(s)ds
+
 œÑ
0
(e‚àíAs/ŒµCeAs/Œµ ‚àíB0)y(s)ds.
Integrating the last integral once by parts and using the conditions of the
previous section shows that it is O(Œµ). In fact, Laplace‚Äôs method reduces
to the Riemann‚ÄìLebesgue Lemma [125] in this case, since the only terms
surviving in the last integral are those that are oscillatory functions of
s/Œµ multiplied by y(s). The Riemann‚ÄìLebesgue Lemma shows that such
integrals approach zero as Œµ ‚Üí0.
7.3
Averaging Rapidly Oscillating DiÔ¨Äerence
Equations
The two-time scale formalism can be very useful in studying problems that
fall outside the usual diÔ¨Äerential equations systems considered up to this
point. For example, consider the diÔ¨Äerence equation
xn+1 = xn + Œµf(n, xn, Œµ),
where x, f ‚ààEN. Given x0, this recursion deÔ¨Ånes a unique sequence {xn},
which we refer to as being a solution of the recursion. The question asked

208
7. Methods of Averaging
here is: Can we construct an approximate solution that is valid when |Œµ| ‚â™
1?
We can begin to answer this question by using multiple time scales. We
observe that if f is a function of x only, then this recursion is trying hard
to be a diÔ¨Äerential equation. In fact, if X(s) is a smooth function such that
X(nŒµ) = xn, then the recursion deÔ¨Ånes a forward Euler scheme for the
diÔ¨Äerential equation
dX
ds = f(X).
Motivated by this, we make a guess for the solution as a function of the
two time scales n and Œµn:
xn = u(n, Œµn, Œµ) = u0(n, Œµn) + Œµu1(n, Œµn) + Œµ2u2(n, Œµn) + ¬∑ ¬∑ ¬∑ .
We write s = Œµn, and then the recursion takes the form
u(n + 1, s + Œµ, Œµ) = u(n, s, Œµ) + Œµf(n, u, Œµ).
Setting Œµ = 0 in this equation gives
u0(n + 1, s) = u0(n, s).
DiÔ¨Äerentiating the equation with respect to Œµ and setting Œµ = 0 in the result
gives
u1(n + 1, s) = u1(n, s) + f(n, u0(n, s), 0) ‚àí‚àÇu0
‚àÇs ,
and so on.
The Ô¨Årst equation shows that u0 does not depend on n, and so we write
u0(n, s) = U0(s).
The second equation shows that
u1(n, s) = U1(s) +
n‚àí1

j=0
f(j, U0(s), 0) ‚àíndU0
ds .
If u1 remains bounded as n ‚Üí‚àû, then dividing both sides of this equation
by n and passing to the limit n = ‚àûgives
dU0
ds = lim
n‚Üí‚àû
1
n
n‚àí1

j=0
f(j, U0(s), 0).
We take for initial data here the value
U0(0) = x0.
Hypothesis H4. Suppose that the average
f0(U) = lim
n‚Üí‚àû
1
n
n‚àí1

j=0
f(j, U, 0)

7.3. Averaging Rapidly Oscillating DiÔ¨Äerence Equations
209
exists and deÔ¨Ånes a smooth function of U. Moreover, suppose that the
diÔ¨Äerence
n‚àí1

j=0
[f(j, U, 0) ‚àíf0(U)]
remains bounded uniformly in n and U.
With this condition we obtain a useful approximation result for diÔ¨Äerence
equations.
DiÔ¨Äerence Equation Averaging Theorem. Suppose that condition
H4 is satisÔ¨Åed for the recursion
xn+1 = xn + Œµf(n, xn, Œµ)
and that each function f(n, x, Œµ) is a smooth function of its arguments x
(in EN) and Œµ (near zero) for n = 0, 1, 2, . . . . Let U(s) be determined from
initial value problem
dU
ds = f0(U),
U(0) = x0.
Then
xn = U(Œµn) + O(Œµ),
where the error estimate holds (as Œµ ‚Üí0) uniformly for n = 0, 1, . . . , n[1/Œµ].
Here n is any Ô¨Åxed positive integer and [1/Œµ] denotes the integer part of 1/Œµ,
that is, the largest integer less than or equal to 1/Œµ.
Proof of the DiÔ¨Äerence Equation Averaging Theorem. The proof is
accomplished by successfully estimating the diÔ¨Äerence
Œ¥n = xn ‚àíU(Œµn).
We have
Œ¥n+1
=
xn+1 ‚àíU(Œµn + Œµ)
=
xn + Œµf(n, xn, Œµ) ‚àíU(Œµn) ‚àíŒµf0(U(Œµn)) + O(Œµ2)
=
Œ¥n + Œµ(f(n, Œ¥n + U(Œµn), Œµ) ‚àíf0(U(Œµn))) + O(Œµ2)
=
(1 + Œµfx)Œ¥n + Œµ(f(n, U(Œµn), Œµ) ‚àíf0(U(Œµn))) + O(Œµ2) + ŒµO(|Œ¥n|2).
It follows from back substitutions that
Œ¥n = O(Œµ) + Œµ
n‚àí1

k=0
O(|Œ¥k|2).
There is a constant K such that |O(Œµ)| ‚â§KŒµ and O(|Œ¥k|2) ‚â§K|Œ¥k|2 for
k = 0, 1, . . . , n ‚àí1. Now suppose that |Œ¥k| ‚â§2KŒµ for k ‚â§n ‚àí1. Then
|Œ¥n| ‚â§K(Œµ + 4nKŒµ2) ‚â§2KŒµ
if 4KnŒµ < 1.

210
7. Methods of Averaging
Therefore, if Œµ < 1/(4KN), by mathematical induction we have that Œ¥n =
O(Œµ) uniformly for n = 1, . . . , n[1/Œµ] [78].
If U approaches a rest point of the diÔ¨Äerential equation, say U ‚ÜíU ‚àó,
where f0(U ‚àó) = 0 and U ‚àóis exponentially asymptotically stable, then the
results of the averaging theorem hold uniformly for 0 ‚â§n < ‚àû. The proof
of this is analogous to the proof of the Mean Stable Averaging Theorem in
Section 7.1.2, and it is not presented here.
7.3.1
Linear DiÔ¨Äerence Schemes
As we did for linear diÔ¨Äerential equations, we can carry out several
computations in detail for linear recursions. We consider the problem
xn+1 = (A + ŒµB)xn,
x0 given.
Writing xn = Anun gives (if A is invertible)
un+1 = un + ŒµA‚àín‚àí1BAnun.
Applying the analysis of the preceding section, we might hope to Ô¨Ånd that
un = U(nŒµ) + O(Œµ),
where U(s) satisÔ¨Åes the equation
dU
ds = lim
n‚Üí‚àû
1
n
n‚àí1

j=1
A‚àíj‚àí1BAjU.
If the coeÔ¨Écient matrix has all of its eigenvalues with negative real parts,
then we might hope that the error estimate O(Œµ) would hold uniformly for
all n.
Therefore, it is of interest to determine the structure of the matrix
B0 = lim
n‚Üí‚àû
1
n
n‚àí1

j=0
A‚àíj‚àí1BAj
if this limit exists. In order to do this, we assume that A is a discrete
oscillatory matrix.
DeÔ¨Ånition. A is a discrete oscillatory matrix if it has a spectral
decomposition of the form
A =
N

j=1
ŒªjPj
where the eigenvalues Œªj lie on the unit circle in the complex plane (|Œªj| =
1). Therefore, Œªj = exp(iœâj) for some real numbers œâj. We say that A is a

7.3. Averaging Rapidly Oscillating DiÔ¨Äerence Equations
211
discrete stable matrix if all eigenvalues Œªj satisfy
|Œªj| ‚â§1
for j = 1, . . . , N, and it is a discrete asymptotically stable matrix if
|Œªj| < 1
for j = 1, . . . , N.
We can calculate B0 directly when A is a discrete oscillatory matrix. In
fact, then
A‚àín‚àí1BAn =
N

j=1
N

k=1
Œªn
kŒª‚àín‚àí1
j
PjBPk.
Moreover, the geometric sum
1
n
n‚àí1

m=0
Œªm
k Œª‚àím‚àí1
j
=
Œª‚àí1
j
n
(Œªk/Œªj)n ‚àí1
(Œªk/Œªj) ‚àí1 .
Since the eigenvalues Œª lie on the unit circle in the complex plane, this
expression approaches zero as n ‚Üí‚àûunless Œªj = Œªk, in which case it has
the value 1/Œªj.
Therefore, we have
B0 = A‚àí1 
Œªj=Œªk
PjBPk,
where the sum is taken over all indices j and k for which Œªj = Œªk. Thus, B0
contains at least the terms PjBPj. The average of B also exists when the
eigenvalues of A are arbitrary as long as PjBPk = 0 whenever |Œªk/Œªj| > 1.
This result is comparable to the Baker‚ÄìCampbell‚ÄìHausdorÔ¨Ätheorem [9].
The next theorem follows from the DiÔ¨Äerence Equation Averaging
Theorem.
Averaging Theorem for Linear DiÔ¨Äerence Equations. Let A be a
discrete oscillatory matrix, and let {xn} be a sequence determined by the
recursion
xn+1 = (A + ŒµB)xn.
Then, for any Ô¨Åxed number n,
xn = An exp(ŒµB0n)x0 + O(Œµ),
where the error estimate holds as Œµ ‚Üí0 uniformly for n = 1, . . . , n[1/Œµ].
Thus, we can approximate the solution of the recursion using the
fundamental solution of the diÔ¨Äerential equation
dU
ds = B0U,
U(0) = I.

212
7. Methods of Averaging
Example of a Discrete Rotational System. We consider a case
where N = 2 and A = J (Jacobi‚Äôs matrix),
J =
 0
‚àí1
1
0

.
This matrix has spectral decomposition
J = i
2

1
i
‚àíi
1

‚àíi
2
 1
‚àíi
i
1

‚â°iP1 + (‚àíi)P2.
Now, consider the diÔ¨Äerence equation
xn+1 = (J + ŒµB)xn,
where B is an arbitrary 2 √ó 2 matrix.
From the calculations above, we see that the relevant average is given by
the formula
B0 = ‚àíiP1BP1 + iP2BP2.
The result of the averaging theorem for linear diÔ¨Äerence equations shows
that
xn = Jn exp(ŒµnB0)x0 + O(Œµ)
for n = 1, . . . , n[1/Œµ] for any Ô¨Åxed number n.
Examples of Perturbations of Markov Chains. The two-time scale
method can be used to study a random processes. A Markov chain describes
the dynamics of a system that can move among a Ô¨Ånite set of states, say
S1, . . . , SN, in a random way [37]. Let pj,n be the probability that the
system is in state Sj at time n, and let Pj,k denote the probability that
the system moves from state Sj to state Sk in one time step. We suppose
that these transition probabilities (Pjk) do not change with time. As a
result, the probability of state j at the sampling time n + 1 is the sum
of all possible states at the previous step weighted by the probability of
transition to state j in one time step. We write this as
pj,n+1 =
N

k=1
pk,nPk,j
for j = 1, . . . , N. The notation can be simpliÔ¨Åed by introducing the
probability density (row) vector
pn = (p1,n, . . . , pN,n)
and the transition probability matrix
P = (Pk,j).

7.3. Averaging Rapidly Oscillating DiÔ¨Äerence Equations
213
With this, the model becomes
pn+1 = pnP.
The sequence of probability vectors {pn} describes the evolution of the
process.
Two interesting examples of perturbations of cyclic chains are
pn+1 = pn(I + ŒµB),
where I is the identity matrix in EN, and
pn+1 = pn(C + ŒµB),
where C is a cyclic matrix in EN:
C =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
0
1
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
1
1
0
0
0
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
.
We suppose in each case that the row sums of B are all equal to zero and
that the matrices in each case I +ŒµB and C +ŒµB have nonnegative entries,
so they are stochastic matrices. In the Ô¨Årst case, the chain is trivial when
Œµ = 0, and so any distribution will persist in that case. However, when
Œµ > 0, we have from the averaging result that
pn = U0(Œµn) + O(Œµ),
where
dU0
ds = U0B.
Since I + ŒµB is a stochastic matrix, all of B‚Äôs oÔ¨Ä-diagonal elements must
be nonnegative. Thus, B = D + ÀúD, where D is a diagonal matrix and ÀúD is
a matrix of nonnegative elements. It follows that
U0 = p0 exp(ŒµDs)V0,
where V0 has only nonnegative elements.
Since the eigenvalues of a stochastic matrix all lie in or on the unit circle
in the complex plane, the eigenvalues of B must lie at the origin or in the
left half-plane. If B has a single eigenvalue at Œª = 0, then all modes of U0
are damped except for the uniform mode. Thus, if Œµ > 0, the probability
distribution becomes spread out over all possible states.
The second example describes a chain that moves contents regularly from
one state to the next and from state SN to state S1 when Œµ = 0. If B has a

214
7. Methods of Averaging
single eigenvalue at Œª = 0, then the ‚Äúmerry-go-round‚Äù chain changes into
one that results in a uniform distribution of the process over the states.
Finally, it is useful to consider a nonstationary Markov chain
pn+1 = pn(I + ŒµBn),
where Bn is a periodic sequence of matrices, say with period n, for which
I + ŒµBn is a stochastic matrix for each n = 0, . . . , n ‚àí1. Such chains arise
as models in chronobiology where n denotes weeks and n = 52 is a year, or
n is in minutes and n = 1440 is a day. In this case,
pn = U0(Œµn) + O(Œµ),
where
dU0
ds = U0 ¬ØB
and
¬ØB = 1
N
N‚àí1

j=0
Bj.
Therefore, the average inÔ¨Çuence of the sequence {Bn} is dominant to
leading order. The higher order approximations of this chain using the
multiscale method bring the higher moments of the probability distribu-
tion of the sequence {Bn} into the approximation. Although these results
are not surprising, they are useful, since it is frequently possible to estimate
the small parameter Œµ from experiments (see, e.g., [118]).
7.4
Almost Harmonic Systems
Problems of the form
dz
dt = Az + Œµf(t, z, Œµ),
where A is an oscillatory matrix, Œµ is a small parameter, and f is a smooth
function of its arguments, are referred to as almost harmonic systems. (Re-
call that A is an oscillatory matrix if it is a diagonalizable real matrix and
all of its eigenvalues are purely imaginary, say ¬±iœâ1, . . . , ¬±iœâN.)
We suppose that A is real, and so its dimension is even, say z, f ‚ààE2N
and A ‚ààE2N√ó2N. If f is a smooth function of its variables and is almost
periodic in t, then this system can be converted into one to which the
averaging theory applies directly by using the change of variables
z = exp(At)y.
With this,
dy
dt = Œµe‚àíAtf(t, eAty, Œµ).

7.4. Almost Harmonic Systems
215
The new right-hand side has a generalized Fourier expansion in t, and the
averaging theory can be attempted directly as described earlier.
From another point of view, we can introduce a change of variables
that takes the matrix A into its real Jordan canonical form. Since A is
an oscillatory matrix, there is a matrix, say P, such that
P ‚àí1AP = diag

0
‚àíœâ1
œâ1
0

, . . . ,

0
‚àíœâN
œâN
0

.
When Œµ = 0, the problem reduces to one that is equivalent to a system of
harmonic oscillators, which is why we refer to such systems for z as almost
harmonic.
We can rewrite the system for z as
d2xj
dt2 + œâ2
j xj = ŒµFj

t, x, dx
dt , Œµ

for j = 1, . . . , N. For example, xj can be thought of as the location of the
jth particle in an N-particle array each with momentum dxj/dt. Then this
equation is based on Newton‚Äôs law, and it describes the restoring force in
the system (œâ2
j xj) and the coupling between particles ŒµFj.
Almost harmonic systems of this form are studied in [59, 111, 96, 13, 116].
We derive a variety of interesting results for them here.
7.4.1
Phase-Amplitude Coordinates
One advantage of studying harmonic oscillators is that they can be easily
transformed into phase-amplitude coordinates (see Section 2.3.5). Let us
deÔ¨Åne variables rj and Œ∏j by
rjeiŒ∏j = dxj
dt + iœâjxj.
Therefore,
drj
dt + idŒ∏j
dt rj = Œµe‚àíiŒ∏jFj + iœâjrj,
and so the new variables rj, Œ∏j ‚ààEN satisfy the equations
drj
dt
=
Œµfj(t, r, Œ∏, Œµ)
dŒ∏j
dt
=
œâj + Œµgj(t, r, Œ∏, Œµ),
where
fj
=
Re(exp(‚àíiŒ∏j)Fj)
gj
=
Im(exp(‚àíiŒ∏j)Fj)/rj.
The vector of amplitudes r and the vector of phases Œ∏ are the unknowns
in this problem. This change of variables converts the original system into

216
7. Methods of Averaging
phase-amplitude coordinates using those for the harmonic oscillator ( Àôr =
0, ÀôŒ∏ = œâ).
The averaging procedures outlined in Section 7.1 can be applied to this
system, which includes a number of interesting examples. Small divisor
problems are common in these systems, but they can be eliminated in many
useful cases. For example, if the averaged system has an asymptotically
stable rest point, then mean stability overcomes small divisors. Also, if the
free frequencies of the system (œâj) satisfy certain nonresonance conditions
as in the KAM theory in Section 7.5.6, then small divisors can be shown
to be manageable.
7.4.2
Free Oscillations
When there is no external forcing in the system, the equations are
drj
dt
=
Œµfj(r, Œ∏, Œµ)
dŒ∏j
dt
=
œâj + Œµgj(r, Œ∏, Œµ).
When N = 1 and there is no external forcing, we can use Œ∏ as the new
time variable in the problem,
dr
dŒ∏ = Œµ
f(r, Œ∏, Œµ)
œâ + Œµg(r, Œ∏, Œµ),
and treat this using our methods for forced systems. Once this is done, the
additional step of relating Œ∏ to t must be taken. If r = r(Œ∏, Œµ) is known,
then
dŒ∏
dt = œâ + Œµg(r(Œ∏, Œµ), Œ∏, Œµ).
The solution is Œ∏ = Œ∏(t, Œµ). For example, if the period of an oscillation that
has period 2œÄ in Œ∏ is to be found, we must solve the equation 2œÄ = Œ∏(T, Œµ)
for the period T. As we have seen before, the period usually changes as Œµ
does.
When N > 1, since all of the phase variables are timelike (Œ∏j ‚âàœâjt+¬∑ ¬∑ ¬∑ ),
it is not obvious how perturbation methods can be useful.
One approach is to change the basis in EN using œâ = col(œâ1, . . . , œâN)
and its orthogonal complement, so œâ, W2, . . . , WN, is an orthogonal basis.
Then
d(œâ ¬∑ Œ∏)
dt
=
œâ ¬∑ œâ + Œµœâ ¬∑ g(r, Œ∏, Œµ)
d(Wj ¬∑ Œ∏)
dt
=
ŒµWj ¬∑ g(r, Œ∏, Œµ)
for j = 2, . . . , N. Thus, we see that œâ ¬∑ Œ∏ is a timelike variable and all
the other variables in the problem move slowly with respect to it. This

7.4. Almost Harmonic Systems
217
approach is studied in the rotation vector method in Section 7.5.1, and it
is successful if the conditions of the mean-stable averaging theorem are met
by the system
drj
dœÑ
=
Œµ
fj
œâ ¬∑ œâ + Œµœâ ¬∑ g
dœàj
dœÑ
=
Œµ
Wj ¬∑ g
œâ ¬∑ œâ + Œµœâ ¬∑ g ,
where œàj = Wj ¬∑ Œ∏ and œÑ = œâ ¬∑ Œ∏.
On the other hand, if there is no strong stability and the frequencies
œâ1, . . . , œâN are incommensurable, little is known (see Section 7.5.6). The
problem in this case can be partly solved if r can be shown to approach an
invariant manifold, say r ‚ÜíR(œà, Œµ) as Œ∏ ‚Üí‚àû, in which case the problem
reduces to one for œà alone on this manifold, and so to a Ô¨Çow on a torus.
The invariant torus can be stable, but behavior on it might be ergodic.
This is an extension of the ideas of orbital stability (see Section 7.4.3).
If there is an oscillation for this system, then its period probably will
change with Œµ. We can determine its period, or equivalently its frequency,
as we construct a solution. Similar to our discussion of orbit stability of
free oscillations, we attempt here to construct an orbital and its period,
and we disregard the phase lag of solutions. van der Pol‚Äôs equation again
gives a good illustration.
Let us consider the equation
d2x
dt2 + x = Œµ(1 ‚àíx2)dx
dt
from the point of view of two time scales.
We look for the solution of van der Pol‚Äôs equation in the form
x = x0(œÑ, s) + x1(œÑ, s)Œµ + x2(œÑ, s)Œµ2 + ¬∑ ¬∑ ¬∑ ,
where s (= Œµt) and œÑ = œâ(Œµ)t are taken to be independent variables. As
before, we require that the coeÔ¨Écients in this expansion be bounded as
functions of t, and to determine œâ(Œµ) we require that they be 2œÄ-periodic
as functions of œÑ as well. Now, x satisÔ¨Åes the equation
œâ2 ‚àÇ2x
‚àÇœÑ 2 + 2Œµœâ ‚àÇ2x
‚àÇœÑ‚àÇs + Œµ2 ‚àÇ2x
‚àÇs2 + Œµ(x2 ‚àí1)

œâ ‚àÇx
‚àÇœÑ + Œµ‚àÇx
‚àÇs

+ x = 0.
Setting Œµ = 0 in this equation gives
œâ2
0
‚àÇ2x0
‚àÇœÑ 2 + x0 = 0.
The general solution of the Ô¨Årst equation is
x0(œÑ, s) = A(s) cos(œÑ/œâ0) + B(s) sin(œÑ/œâ0),
where A and B are arbitrary functions of s, and we see from the periodicity
condition that œâ0 = 1. It will turn out that we must take œâ1 = 0 to ensure

218
7. Methods of Averaging
that x1 is periodic. We use this transformation at this point to simplify our
calculations. DiÔ¨Äerentiating with respect to Œµ and setting Œµ = 0 gives
‚àÇ2x1
‚àÇœÑ 2 + x1 = ‚àí2 ‚àÇ2x0
‚àÇœÑ‚àÇs + (1 ‚àíx2
0)‚àÇx0
‚àÇœÑ .
The equation for x1 becomes
‚àÇ2x1
‚àÇœÑ 2 + x1
=
‚àí2(‚àíA‚Ä≤ sin œÑ + B‚Ä≤ cos œÑ)
+ (1 ‚àíA2 cos2 œÑ ‚àíAB sin 2œÑ ‚àíB2 sin2 œÑ)(‚àíA sin œÑ + B cos œÑ).
Using the identities
cos3 t
=
(3 cos t)/4 + h.f.t.
sin3 t
=
(3 sin t)/4 + h.f.t.
cos2 t sin t
=
(sin t)/4 + h.f.t.
sin2 t cos t
=
(cos t)/4 + h.f.t.,
we can easily expand the right-hand side of this equation in its Fourier
series.
The functions A and B must be chosen to suppress the resonant terms
in the right-hand side (see Chapter 6). To do this, the coeÔ¨Écients of cos œÑ
and sin œÑ are set equal to zero, and the result is
2dA
ds ‚àíA

1 ‚àíA2
4 ‚àíB2
4

=
0
2dB
ds ‚àíB

1 ‚àíA2
4 ‚àíB2
4

=
0.
This system can be solved using polar coordinates: Setting A = R cos œà
and B = R sin œà, we have that
2RdR
ds = 2AdA
ds + 2B dB
ds = R2

1 ‚àíR2
4

and
R2 dœà
ds = AdB
ds ‚àíB dA
ds = 0.
Therefore,
dR2
ds = R2

1 ‚àíR2
4

.
We see that R(‚àû) = 2, so the result of these computations is that as
t ‚Üí‚àû,
x ‚Üí2 cos(œât + œÜ) + O(Œµ),

7.4. Almost Harmonic Systems
219
where œâ = 1 + O(Œµ2) and œÜ is determined by initial conditions. This ap-
proach to the averaging method is used extensively in [26, 90], which contain
many modern applications.
Almost Harmonic Approximations in Weakly Connected Networks[81].
Systems of van der Pol‚Äôs equations have the form
d2xj
dt2 + ¬µj(x2
j ‚àí1)dxj
dt + œâ2
j xj = Œµfj

t, x, dx
dt

,
where Œµ and ¬µj are parameters, and x and dx/dt denote vectors with
components x1, . . . , xN and dx1/dt, . . . , dxN/dt, respectively.
When Œµ = 0, this system reduces to a system of van der Pol‚Äôs equations
with parameters ¬µj and frequencies œâj. We have just shown that if each
0 < ¬µj ‚â™1, then each oscillator approaches a periodic solution having
amplitude R = 2. Since each equation in this system has a stable periodic
solution, the entire network has a stable invariant torus that is the product
of these orbits. In [141, 81] it is shown that there is a nearby invariant
torus for the full system when 0 < Œµ ‚â™1, so this system can eventually be
reduced to a phase model of the form
Àôœàj = ‚Ñ¶j + ŒµŒ®j.
7.4.3
Conservative Systems
Recall that Hamiltonian systems can be (in theory) converted to phase-
amplitude coordinates, where an amplitude variable is the system‚Äôs energy
and the phase variables are deÔ¨Åned in a complicated way (see Section 2.3.5).
Let us consider a system that has been converted to phase-amplitude
coordinates, say in the form
dr
dt
=
f(r, œà, Œµ)
dœà
dt
=
g(r, œà, Œµ),
where r is a vector of N amplitude variables, œà is a vector of N angular
phase variables, and f and g are smooth functions that are 2œÄ-periodic in
each of the components of œà.
We suppose that r = r‚àócorresponds to the invariant torus for Œµ = 0, so
f(r‚àó, œà, 0) = 0
for all œà. Then the set (r‚àó, œà) deÔ¨Ånes an N torus in E2N as the components
of œà range over [0, 2œÄ]. Moreover, the equation
dœà
dt = g(r‚àó, œà, 0)
deÔ¨Ånes a Ô¨Çow of solutions on the surface of the torus. Linearizing the r
equation about r = r‚àóand replacing r ‚àír‚àóby Œµr brings the system into

220
7. Methods of Averaging
the form
dr
dt
=
Sr + O(Œµ)
dœà
dt
=
g(r‚àó, œà, 0) + O(Œµ).
The special case
g(r‚àó, œà, 0) = œâ(r‚àó),
where œâ(r‚àó) is a function of r‚àóonly, is usually the one considered, since
little is known about the general case. For most of our work we take œâ to
be constant. Note that this system is of the same form as that derived for
the weakly coupled harmonic system except that r can now respond on the
fast time scale t.
We would like to derive an approximation to solutions of this system that
is valid uniformly for 0 ‚â§t < ‚àû. However, we cannot use the mean-stable
averaging theorem, since it would require the œà variable to equilibrate.
This is usually not the case for Ô¨Çows on a torus, especially if the vector œâ
has incommensurable components. The following example illustrates this
problem.
Recall our notation for quasiperiodic functions introduced in Chapter 1.
Let G(x1, . . . , xN) be a smooth function that is 2œÄ-periodic in each of
the variables x1, . . . , xN. Then if the frequencies œâ1, . . . , œâN are ratio-
nally independent (Œ∫ ¬∑ œâ Ã∏= for all integer vectors Œ∫ Ã∏= 0), the function
G(œâ1t, . . . , œâNt) is a quasiperiodic function as described in Section 1.5.1.
This is a special kind of almost-periodic function that is generated (or de-
Ô¨Åned) by a periodic function of a Ô¨Ånite number of variables. We say that its
frequencies are spanned by œâ. In particular, it has an easily derived Fourier
series, and all of its frequencies are linear integer combinations of the Ô¨Ånite
set (œâ1, . . . , œâN). The Fourier series of G has the form
G(œâ1t, . . . , œâNt) =
‚àû

|j|=‚àí‚àû
CjeIj¬∑œât,
where j is a multi-index of integers, j = (j1, . . . , jN), œâ is the vector œâ =
(œâ1, . . . , œâN), and |j| = j1 + ¬∑ ¬∑ ¬∑ + jN.
Small divisor problems arise even in simple systems of weakly coupled
harmonic oscillators. For example, consider the system
d2x
dt2 + x
=
Œµ sin(x ‚àíy)
d2y
dt2 + 2y
=
Œµ sin(y ‚àíx).
We will attempt to use the iteration method to construct oscillatory
solutions of this system (see Section 6.2.2).

7.4. Almost Harmonic Systems
221
Setting Œµ = 0 gives
d2x0
dt2 + x0
=
0
d2y0
dt2 + 2y0
=
0.
The solutions are
x0(t)
=
A cos t + B sin t
y0(t)
=
a cos
‚àö
2t + b sin
‚àö
2t
for some constants A, a, B, and b.
Putting these solutions into the right-hand side of the equation gives
d2x1
dt2 + x1
=
sin(x0 ‚àíy0)
d2y1
dt2 + 2y1
=
sin(y0 ‚àíx0).
Now, the right-hand side is a quasiperiodic function of t, and it has a
generalized Fourier series
sin(x0(t) ‚àíy0(t)) =
‚àû

m,n=‚àí‚àû
Cm,n exp(imt + i
‚àö
2nt),
where the coeÔ¨Écients Cm,n are not zero for inÔ¨Ånitely many values of m and
n.
The solution for x1 is given by the variation of constants formula
x1(t)
=
Œµeit
 t
0

m,n
Cm,n exp(imt‚Ä≤ + in
‚àö
2t‚Ä≤ ‚àíit‚Ä≤)dt‚Ä≤
+ Œµe‚àíit
 t
0

m,n
Cm,n exp(imt‚Ä≤ + in
‚àö
2t‚Ä≤ + it‚Ä≤)dt‚Ä≤ + Œ±eit + Œ≤e‚àíit.
Proceeding as though all sums converge, we have that
x1(t) = Œµ

m,n
Cm,n
eimt+in
‚àö
2t ‚àíeit
im + in
‚àö
2 ‚àíi +

m,n
Cm,n
eimt+in
‚àö
2t ‚àíe‚àíit
im + in
‚àö
2 + i .
The following lemma [60] highlights a problem with this expansion.
Lemma. The set {1 + m + n
‚àö
2 : m, n integers} is dense in E1.
Because of this fact, we see that the terms deÔ¨Åning x1 have arbitrarily
small divisors, and we cannot expect these series to converge. It therefore
appears that the iteration method fails to produce a quasiperiodic solution,
and a subtler approach is needed to resolve this problem of small divisors.

222
7. Methods of Averaging
Since sin(x ‚àíy) is a real analytic function of x and y, we do know that
the coeÔ¨Écients Cm,n decay geometrically to zero: Say, for some number Œæ,
|Cm,n| ‚â§|Œæ|m+n,
|Œæ| < 1
as m+n ‚Üí‚àû. Therefore, if the divisors are not too small, then these series
can be shown to converge, and the iteration process can be continued. It
is possible to set conditions on the divisors that ensure that these series
converge. The following deÔ¨Ånition is useful for dealing with small divisors .
Condition L. Let (œâ1, . . . , œâN) be a set of frequencies. We say that
these frequencies satisfy condition L if there are constants G and Œ≥ with
G > 0 and Œ≥ > N ‚àí1 such that for all multi-indices j = (j1, . . . , jN) with
 |jk| Ã∏= 0 the estimate
|œâ ¬∑ j| > G|j|‚àíŒ≥
holds.
This condition is named for Liouville, who derived a similar condition.
It is very much like a nonresonance condition (see Section 7.5.6). In fact,
condition L holds for the example of this section [115], and so we see that
the terms in the series deÔ¨Åning x1(t) are dominated by those of a convergent
geometric series. Therefore, x1 is determined in the iteration method, and
the process can continue.
When œâ is proportional to a vector of integers, the problem is said to
be resonant and condition L is not satisÔ¨Åed. This is a case to which the
Rotation Vector Method can be applied as shown in Section 7.5.1.
Bogoliubov [13] introduced a method of near-identity transformations
to analyze this system with condition L. The result is described by the
following steps.
We introduce new variables by the transformation
r
=
ŒµV (u, v, Œµ)
œà
=
u + ŒµU(u, v, Œµ),
where U and V are 2œÄ-periodic in the components of u. This transformation
is to be found so that it takes the system into the simpler system
du/dt
=
œâ
dv/dt
=
Sv.
Thus, if the matrix S is asymptotically stable, then the v values equilibrate
to v = 0, and a solution in original variables will be
œà
=
œât + c + ŒµU(œât + c, 0, Œµ)
r
=
ŒµV (œât + c, 0, Œµ).
Therefore, we see that such a solution is a quasiperiodic function of t.

7.5. Angular Phase Equations
223
The idea behind this transformation is to construct an invariant surface
for Œµ Ã∏= 0 that is near the invariant torus of the unperturbed system. Be-
cause of this, the method is sometimes referred to as the method of invariant
manifolds [59].
Since the problem is time-invariant, we expect that a modiÔ¨Åcation of
frequencies will be required to carry out this plan. Therefore, we introduce
a frequency modiÔ¨Åcation Œª, and we consider the modiÔ¨Åed problem
dr
dt
=
Sr + Œµf(r, œà, 0)
dœà
dt
=
œâ + Œª + Œµg(r, œà, 0).
Bogoliubov derived the following theorem for this system.
Bogoliubov‚Äôs Averaging Theorem. Suppose that the frequency vec-
tor œâ satisÔ¨Åes condition L and that S is an asymptotically stable matrix.
Finally, suppose that f and g are analytic functions of their arguments.
Then there exists a smooth function Œª = Œª(Œµ) such that the system above
has a quasiperiodic solution of the form
œà
=
œât + c + ŒµU(œât + c, 0, Œµ)
r
=
ŒµV (œât + c, 0, Œµ),
where the phase shift c is an arbitrary vector of constants.
This theorem will be used decisively in Section 7.5.2 to construct intervals
of phase locking. Unfortunately, it is not always easy to verify the conditions
of this theorem.
7.5
Angular Phase Equations
In this section we consider problems that are described in terms of phase
variables alone. For example, these may arise as descriptions of dissipative
or conservative Ô¨Çows on invariant tori or as models of networks of integrated
circuits that are described by phase variables, as we saw for the PLL and
VCON circuits.
The rotation vector method is useful for studying phase-locking behavior
for various systems. Next, we apply Bogoliubov‚Äôs near-identity transforma-
tion to study a problem to which Denjoy‚Äôs theory applies. This proves
certain other facts about phase locking. Finally, we describe brieÔ¨Çy the
KAM theory of Ô¨Çows on tori.

224
7. Methods of Averaging
7.5.1
Rotation Vector Method
An important system of equations is
dx
dt
=
œâ + Œµf(x, y, Œµ)
(7.3)
dy
dt
=
Œµg(x, y, Œµ),
where y and g are M-vectors and x, œâ, and f are N-vectors, f and g are
periodic in each of the components of x, and Œµ is a small parameter. The
functions f and g are assumed to be smooth functions of their arguments.
As in Section 7.4.2, we suppose that œâ is proportional to a vector of
nonzero integers, and without loss of generality, we take
œâ = col(p1, . . . , pN),
where pk is an integer for each k = 1, . . . , N. This system does not satisfy
condition L, and so Bogoliubov‚Äôs averaging result does not apply directly.
However, vectors of integers W2, . . . , WN
can be found that are
orthogonal to œâ and pairwise orthogonal:
Wj ¬∑ Wk = 0
for j Ã∏= k
and œâ¬∑Wj = 0 for j = 2, . . . , N. We introduce new variables to the problem
through a change of basis in EN. Let
x = vœâ +
N

j=2
ujWj,
where the new coordinates are v, u2, . . . , uN.
This change of variables is similar to Delaunay orbit elements in celestial
mechanics [49]. The diÔ¨Äerential equations are easily transformed into these
new coordinates by taking dot products of the system with œâ, W2, . . . , WN,
successively. The results are
(œâ ¬∑ œâ)dv
dt
=
œâ ¬∑ œâ + œâ ¬∑ f
(Wj ¬∑ Wj)duj
dt
=
ŒµWj ¬∑ f
dy
dt
=
Œµg,
for j = 2, . . . , N. We write œâ2 = œâ ¬∑ œâ. Using v as a timelike variable, we
divide the last M + N ‚àí1 equations by the Ô¨Årst and get
duj
dv
=
Œµ
Wj ¬∑ f
œâ2 + Œµœâ ¬∑ f
dy
dv
=
Œµ
g
œâ2 + Œµœâ ¬∑ f .

7.5. Angular Phase Equations
225
This form of the system is one to which the Mean Stable Averaging
Theorem can be applied.
We rewrite it as
du
dv
=
ŒµF(v, u, y, Œµ)
(7.4)
dy
dv
=
ŒµG(v, u, y, Œµ),
where u denotes the (N ‚àí1)-vector (u2, . . . , uN) and F denotes the vector
whose jth component is
Fj(v, u, y, Œµ) =
Wj ¬∑ f
œâ2 + œâ ¬∑ f .
Since the original function f is periodic in the components of x and the
vectors œâ, W2, . . . , WN, have integer coordinates, the function F is periodic
in v and the components of u. Therefore, F has a convergent Fourier series:
F(v, u, y, Œµ) =
‚àû

|n|=‚àí‚àû
Cn(y, Œµ) exp

i

n1v +
N

j=2
njuj

.
The average of this function over v is given by the Fourier series
‚ü®F‚ü©(U, y) =
‚àû

|n|=‚àí‚àû
Cn(y, 0) exp

i
N

j=2
njUj

,
where the sum is taken over all multi-indices n for which n1 = 0.
If the original function f has the Fourier series
f(x, y, Œµ) =
‚àû

|n|=‚àí‚àû
fn(y, Œµ)ein¬∑x,
then the average of f over v is given by the Fourier series
‚ü®f‚ü©=
‚àû

|n|=‚àí‚àû,n¬∑œâ=0
fn(y, 0)ein¬∑x,
where the sum is taken over all multi-indices n for which n ¬∑ œâ = 0. For
such indices, we have that
n ¬∑ x =
N

j=2
ujn ¬∑ Wj.
Therefore, the coeÔ¨Écients Cn in the expansion for F can be determined
from f.

226
7. Methods of Averaging
Since the system is periodic in v, a near-identity transformation can be
found such that system 7.4 can be rewritten as
du
dv
=
Œµ‚ü®F‚ü©(u, y) + O(Œµ2)
(7.5)
dy
dv
=
Œµ‚ü®G‚ü©(u, y) + O(Œµ2).
We suppose that the following condition is satisÔ¨Åed.
Hypothesis H5. The averaged equation
dU
dv
=
Œµ‚ü®F‚ü©(U, Y )
dY
dv
=
Œµ‚ü®G‚ü©(U, Y )
has an exponentially stable rest point, say (U ‚àó, Y ‚àó).
This condition ensures that the state (U ‚àó, Y ‚àó) is stable under persistent
disturbances. If this condition is satisÔ¨Åed, and if u(0) is near U ‚àó, y(0) is
near Y ‚àó, and if Œµ is suÔ¨Éciently small, then we have that
u = U + O(Œµ)
and
y = Y + O(Œµ)
uniformly for 0 ‚â§v < ‚àûfor this system and all nearby (Carath¬¥eodory)
systems as described in Section 8.1.
This calculation has an important application. Suppose that condition
H5 is satisÔ¨Åed. Then the u components of the system remain bounded, and
we have that
x
v = œâ +
N

j=2
uj
v Wj ‚Üíœâ
as v ‚Üí‚àûfor this system and all nearby ones as above. This can be written
more concisely as
x1 : ¬∑ ¬∑ ¬∑ : xN ‚Üíœâ1 : ¬∑ ¬∑ ¬∑ : œâN,
meaning that the relative ratios of the components of x approach the rel-
ative ratios of frequencies œâ. Thus, the output frequency ratios are locked.
The u variables are, in engineering terminology, phase lags. Since they
converge to near u‚àó, the phase lags also lock.
We say that the system is in phase lock with the frequencies œâ, and
œâ is called the rotation vector in analogy with the two-dimensional case
described by Denjoy. These results are summarized in the next theorem.
Phase Locking Theorem. Suppose that f and g are smooth functions
that are periodic in the components of x and that condition H5 is satisÔ¨Åed.
Then
x1 : ¬∑ ¬∑ ¬∑ : xN ‚Üíœâ1 : ¬∑ ¬∑ ¬∑ : œâN

7.5. Angular Phase Equations
227
as t ‚Üí‚àûfor system (7.2) and all nearby (Carath¬¥eodory) systems.
Phase-locking occurs in many conservative systems. This is due to
dissipation within Hamiltonian systems. For example, if the system
dx
dt = f(x)
phase locks, say according to the rotation vector method, then H(x, p) =
f(x) ¬∑ p deÔ¨Ånes a Hamiltonian for which the dynamics are described by the
equation
dx
dt
=
‚àápH(x, p) = f(x)
dp
dt
=
‚àí‚àáxH(x, p) = ‚àígradx(f(x) ¬∑ p).
In this way any system of phase equations can be embedded in a
Hamiltonian system, so there are many conservative systems that have
exponentially stable subsystems.
7.5.2
Rotation Numbers and Period Doubling Bifurcations
Rotation numbers are very useful for describing phase locking within sys-
tems of oscillators. For example, they can describe stability within systems
where none is expected, such as for conservative systems in the preced-
ing section. On the other hand, rotation numbers do not do everything:
For example, recall the parabolic interval mapping in Section 2.5 (namely,
x ‚Üírx(1 ‚àíx)). We can deÔ¨Åne a ‚Äúrotation number‚Äù to be the number of
times an orbit circles around the rest point (r‚àí1)/r as n increases, divided
by n. Equivalently, we could count the number of times iterates move left
in n steps and deÔ¨Åne the rotation number to be this number divided by n.
With this deÔ¨Ånition, there is rotation number 1/2 for almost all initial
values x0 and for all values of r in the interval, 3 < r < r‚àó‚âà3.57 . . . .
This happens even though there is an inÔ¨Ånite sequence of period doubling
bifurcations over this interval.
Still, rotation numbers can give useful information, ranging from Den-
joy‚Äôs rigorous theory for a Ô¨Çow on a torus to Cartwright and Littlewood‚Äôs
[53, 17, 38] use of rotation numbers to study van der Pol‚Äôs equation, even
when this number is poorly deÔ¨Åned.
7.5.3
Euler‚Äôs Forward Method for Numerical Simulation
We begin this section with a useful observation based on our work on
averaging for diÔ¨Äerence equations. Namely, we show that for a large class
of problems Euler‚Äôs forward diÔ¨Äerence method for solving angular phase
equations is stable. In fact, it yields a Ô¨Årst-order numerical scheme that

228
7. Methods of Averaging
gives the correct value for rotation number of a system that is in phase
lock. We apply this simulation technique to two examples.
The forward Euler algorithm for solving a system of diÔ¨Äerential equations
of the form
dx
dt = œâ + f(x, t),
where x, œâ, f ‚ààEN, and f is a smooth function that satisÔ¨Åes the conditions
of the Nonlinear Averaging Theorem (Section 7.1.1), is given by
xn+1 = xn + h(œâ + f(xn, nh)),
where we have used a step size h and replaced t by nh and the solution
x(t) by the sequence xn ‚âàx(nh). Setting
yn = xn ‚àíœânh
gives the formula
yn+1 = yn + hf(yn + hnœâ, nh)
and y0 = x0.
The diÔ¨Äerence between y(nh) and yn measures the error in using the
algorithm. We say that a scheme is of order p if this diÔ¨Äerence error is
O(hp+1). The scheme for yn is a Ô¨Årst-order algorithm, since
x(h) ‚àíx1
=
 h
0
(œâ + f(x(s), s))ds ‚àíh(œâ + f(x0, 0))
=
 h
0
f(x(s), s)ds ‚àíhf(x0, 0)
=
O(h2),
so p = 1.
According to the averaging theorem for diÔ¨Äerence equations (Section 7.3),
we have
yn = U(hn) + O(h),
where the function U(s) is determined by solving the problem
dU
ds = lim
M‚Üí‚àû
1
M
M‚àí1

j=0
f(U + œâs, s) = f(U + œâs, s)
and U(0) = x0. To study this equation we consider the averaged equation
d ¬ØU
ds = lim
S‚Üí‚àû
1
S
 S
0
f( ¬ØU + œâs, s)ds = ¬Øf( ¬ØU).
If this system has an exponentially asymtotically stable rest point, then
U and ¬ØU should remain close and ¬ØU remains bounded. Therefore, if this

7.5. Angular Phase Equations
229
‚àí.2
‚àí.04
.12
.28
.44
.6
.76
.92
1.08
1.24
1.4
1.4
1.26
1.12
.98
.84
.7
.56
.42
.28
.14
0
Frequency
œÅ
Figure 7.2. N = 2 rotation number vs. œâ.
happens, then
xn
nh = yn + nhœâ
nh
‚Üíœâ
as n ‚Üí‚àû. This result agrees with the phase-locking results of Section 7.5.1.
In particular, the condition used here is the same one used in the rotation
vector method to ensure that the system phase-locks onto the rotation vec-
tor œâ. Thus, we see that stability of this (doubly) averaged system ensures
that the forward Euler algorithm generates a sequence that converges to
the same rotation number as for the original system of equations.
This fact greatly facilitates our simulations of phase equations, since the
forward Euler scheme involves few computations.
7.5.4
Computer Simulation of Rotation Vectors
Coupled oscillators tend to synchronize‚Äîeven Hamiltonian systems of
them. This suggests that in addition to orbital stability (i.e., stability in
amplitudes) there is signiÔ¨Åcant phase-locking, which is asymptotic stability
in angular phases. In fact, phase stability is the rule, and erratic behavior,

230
7. Methods of Averaging
œâ
œâ
œâ 2
1
3
1:1:1
2:1:2
2:1:1
3:1:1
4:1:1
3:2:1
Figure 7.3. A chart of phase locking relations that are observed in Figure 7.4.
like that described by the KAM theory, is the exception in many physical
systems.
The rotation vector method in Section 7.5.1 gives a rigorous method
for demonstrating phase-locking. To illustrate this phenomenon using
computer experiments we consider two interesting examples.
N = 2. This case falls within the scope of Denjoy‚Äôs theory. The rota-
tion number described in Figure 7.2 is calculated using the forward Euler
algorithm for the system
dx
dt
=
œâ ‚àíŒµ(sin(x ‚àíy) + 2 sin(2x ‚àíy))
dy
dt
=
1.0 + Œµ(sin(x ‚àíy) + sin(2x ‚àíy)),
where Œµ = 0.1 and œÅ ‚âàx(50œÄ)/y(50œÄ). This shows plateau regions where
the oscillators are in phase lock. The KAM theory (Section 7.5.6) describes
some of the system‚Äôs behavior oÔ¨Äthese plateaus.
N = 3. The rotation vector in a system of three oscillators can be de-
scribed using triangular coordinates. In the case of three phase equations,
say
dxj
dt = œâj + Œµfj(x1, x2, x3)
for j = 1, 2, 3, if phase-locking occurs, the rotation vector will be the limit
lim
t‚Üí‚àûx1(t) : x2(t) : x3(t) = z1 : z2 : z3,
where the vector (z1, z2, z3) is proportional to a vector of integers. We can
represent this vector of integers using triangular coordinates by introducing
numbers
Œæj =
zj
z1 + z2 + z3

7.5. Angular Phase Equations
231
‚àí1
‚àí0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
œâ1
œâ2
œâ3
p3
p2
p1
Figure 7.4. Simulation of three oscillators described in the text. This shows that
some oscillators go to a vertex, meaning that only one oscillator dominates, but
most are near one of the 2:1:1 locking rotation vectors.
for j = 1, 2, 3. Since these numbers add to one and they are not negative, we
can depict the point (Œæ1, Œæ2, Œæ3) as being in the intersection of the positive
octant with the plane Œæ1 +Œæ2 +Œæ3 = 1. This is illustrated next for the model
dx1
dt
=
œâ1 + Œµ(cos(x1 ‚àíx2) + cos(2x1 ‚àíx3))
dx2
dt
=
œâ2 + Œµ(cos(x2 ‚àíx3) + cos(2x2 ‚àíx1))
dx3
dt
=
œâ3 + Œµ(cos(x3 ‚àíx1) + cos(2x3 ‚àíx2)).
We plot the results of (Œæ1, Œæ2, Œæ3) on triangular (or areal) coordinates for
each of many choices for 0 < œâ1, œâ2 < 2.0. This is shown in Figures 7.3
and 7.4.
7.5.5
Near Identity Flows on S1 √ó S1
Let us return to some work developed in our discussion of Ô¨Çows on a torus.
Consider the equation
dx
dt = œâ(Œµ) + Œµf(t, x, Œµ).

232
7. Methods of Averaging
The change of variables y = x ‚àíœâ(0)t takes this equation into a form to
which the averaging theory applies:
dy
dt = Œµ(œâ1 + œâ2Œµ + ¬∑ ¬∑ ¬∑ + f(t, y + œâ(0)t, Œµ)).
We can analyze this system using Bogoliubov‚Äôs near-identity transforma-
tion: Let us try to Ô¨Ånd a change of variables
y = v(t, Œæ, Œµ) = Œæ + Œµv1(t, Œæ) + Œµ2v2(t, Œæ) + ¬∑ ¬∑ ¬∑
that takes the equation for y into one of the form
dŒæ
dt = ŒµM1(Œæ) + Œµ2M2(Œæ) + ¬∑ ¬∑ ¬∑ .
In this way, we have converted the original problem into one that does not
involve t explicitly, much as the method of averaging converts the problem
into a time-invariant one. In fact, we should Ô¨Ånd that M1 = ¬Øf.
Substituting these formulas (y = v(t, Œæ, Œµ)) into the problem for y gives
‚àÇv
‚àÇt + ‚àÇv
‚àÇŒæ
dŒæ
dt = Œµ[œâ1 + œâ2Œµ + ¬∑ ¬∑ ¬∑ + f(t, œâ0t + v, Œµ)].
Combining this formula with the ones for dx/dt and dv/dt gives
dŒæ
dt
+
Œµ‚àÇv1
‚àÇt + Œµ2 ‚àÇv2
‚àÇt + ¬∑ ¬∑ ¬∑ + (ŒµM1 + Œµ2M2 + ¬∑ ¬∑ ¬∑ )

1 + Œµ‚àÇv1
‚àÇŒæ + Œµ2 ‚àÇv2
‚àÇŒæ + ¬∑ ¬∑ ¬∑

=
Œµ(œâ1 + œâ2Œµ + ¬∑ ¬∑ ¬∑ + f(t, œâ0t + v, Œµ)).
Equating coeÔ¨Écients of like powers of Œµ on both sides gives
M1 + ‚àÇv1
‚àÇt = f(t, Œæ + œâ0t, 0) + œâ1,
so if v1(Œæ, t) is to be periodic in t, we must have
M1(Œæ) = œâ1 + 1
2œÄ
 2œÄ
0
f(t, Œæ + œâ0t, 0)dt ‚â°œâ1 + ¬Øf(Œæ),
and so
v1(Œæ, t) = V1(Œæ) +
 t
0
[f(t‚Ä≤, œâ0t‚Ä≤ + Œæ, 0) ‚àí¬Øf(Œæ)]dt‚Ä≤,
where V1(Œæ) is still not known. Two important points here are that M1 is
indeed the average of f (plus a constant), and the calculation here is quite
similar to the two-time scale expansion method introduced earlier in this
chapter.
This process can continue. For example,
M2 + M1
‚àÇv1
‚àÇŒæ + ‚àÇv2
‚àÇt = ‚àÇf
‚àÇv (t, Œæ + œâ0t, 0)v1(t, Œæ) + f1(t, Œæ + œâ0t) + œâ2,
and so on (see [26, 90]).

7.5. Angular Phase Equations
233
7.5.6
KAM Theory
An important extension of Bogoliubov‚Äôs Theorem was developed by Kol-
mogorov, Arnold, and Moser [115], which is now known as the KAM theory.
This work signiÔ¨Åcantly relaxes the conditions needed by Bogoliubov to es-
tablish the existence of quasiperiodic solutions to the system presented just
above Bogoliubov‚Äôs averaging theorem in Section 7.4.3.
Suppose that the matrix S (Section 7.4.3) is diagonalizable and that its
eigenvalues are denoted by œÉj. Condition L is now replaced by the following
condition.
Condition KAM. Suppose that there are positive constants G and Œ≥,
Œ≥ > N ‚àí1, such that
|i(j ¬∑ œâ) ‚àíœÉk| > G|j|‚àíŒ≥
and
|i(j ¬∑ œâ) ‚àíœÉk + œÉk‚Ä≤| > G|j|‚àíŒ≥
for all nonzero multi-indices j as described in condition L and all indices k,
k‚Ä≤ = 1, . . . , m.
The problem is modiÔ¨Åed by adding new variables ¬µ, M, and Œª as shown
next,
dr
dt
=
Sr + ¬µ(Œµ) + M(Œµ)r + Œµf(r, œà, Œµ)
(7.6)
dœà
dt
=
œâ + Œª(Œµ) + Œµg(r, œà, Œµ),
and we have the following theorem.
KAM Theorem. Suppose that condition KAM is satisÔ¨Åed and that
the functions f and g are analytic. Then there exist functions Œª = Œª(Œµ),
¬µ = ¬µ(Œµ), and M = M(Œµ) such that the system (7.6) has a quasiperiodic
solution of the form
œà
=
œât + c + ŒµU(œât + c, Œµ)
r
=
ŒµV (œât + c, Œµ).
This result is useful in establishing the existence of quasiperiodic solu-
tions to Hamiltonian systems. The proof of this result, a description of the
earlier literature, and several interesting examples are described in [116].
If Œª, ¬µ, and M in this result are zero, then we have found a quasiperiodic
solution of the original system‚Äîin fact, an invariant torus of solutions. In
any case, we see that if the nonresonance Condition KAM is satisÔ¨Åed, then
near the original system is one having quasiperiodic solution.

234
7. Methods of Averaging
7.6
Homogenization
We have considered averaging problems where two time scales appear, but
there are many important problems where two space scales or combinations
of time and space scales appear.
The idea here is to study periodic structures in space. Consider a material
in which there is variation of physical parameters on both microscopic
and macroscopic scales. For example, consider a line of bacterial cells that
are Ô¨Åxed in an agar gel and placed at distances that are comparable to
their size (microns). Suppose also that there is a gradient of nutrient that
is established in the gel‚Äîits variation is on the macroscopic scale that
is related to the size of the agar gel itself. Let a denote some physical
characteristic, such as the local coeÔ¨Écient of diÔ¨Äusivity of nutrients, and
let f be the local uptake rate of nutrients. The data of the system can vary
on the microscopic scale x/Œµ and the macroscopic scale x, where Œµ is the
size of an interval containing each bacterial cell. We suppose that diÔ¨Äusivity
is periodic with period Œµ, so we write a(x/Œµ), where a has period 1.
A model for the static distribution of u is
‚àÇ
‚àÇx

a
x
Œµ
‚àÇu
‚àÇx

+ f

x, x
Œµ

u = 0,
where a(Œæ) and f(x, Œæ) are periodic in Œæ with period one. Setting
v = a
x
Œµ
‚àÇu
‚àÇx
gives
‚àÇv
‚àÇx = ‚àíf

x, x
Œµ

u.
This results in the system
‚àÇ
‚àÇx

u
v

=

0
b
‚àíf
0
 
u
v

,
where b = 1/a. Using the two-scale method developed in Section 7.1, we
replace ‚àÇ/‚àÇx by Œµ‚àÇ/‚àÇx + ‚àÇ/‚àÇy, where y = Œµx. Then the equations become

Œµ ‚àÇ
‚àÇx + ‚àÇ
‚àÇy
  u
v

= Œµ

0
b
‚àíf
0
  u
v

.
Expanding u and v in terms of Œµ, we have
u
=
u0(x, y) + Œµu1(x, y) + ¬∑ ¬∑ ¬∑
v
=
v0(x, y) + Œµv1(x, y) + ¬∑ ¬∑ ¬∑ .

7.7. Computational Aspects of Averaging
235
Substituting these into the equation and equating like powers of Œµ gives
‚àÇ
‚àÇy
 u0
v0

= 0,
so
 u0
v0

=
 U0(x)
V0(x)

,
and
‚àÇ
‚àÇy
 u1
v1

=

0
b
‚àíf0
0
  u0(x)
v0(x)

‚àí‚àÇ
‚àÇx
 u0
v0

.
As a result,
‚àÇ
‚àÇx
 U0
V0

=
Ô£´
Ô£≠
0
'1
a
(
‚àíf0
0
Ô£∂
Ô£∏
 U0(x)
V0(x)

,
where
¬Øb =
'1
a
(
= lim
T ‚Üí‚àû
1
T
 T
0
1
a(s)ds =
 1
0
ds
a(s)
and
f0(x) =
 1
0
f(x, s)ds.
Therefore,
 ‚àÇ
‚àÇx
'1
a
(‚àí1 ‚àÇ
‚àÇx

U0 + f0(x)U0 = 0,
and we see that the ‚Äúmean diÔ¨Äusivity‚Äù is the harmonic mean of the mi-
croscopic diÔ¨Äusivity of the system (see [103] for a detailed treatment of
homogenization).
7.7
Computational Aspects of Averaging
Let us consider now a highly oscillatory system, say having the form
dx
dt = f
 t
Œµ, x

,
x(0) = Œæ,
where f is a vector of functions that satisfy the conditions of the Nonlinear
Averaging Theorem of Section 7.1. We know from that theorem that the
solution of this problem can be written in the form
x = x0(t) + Œµx1(t, t/Œµ) + O(Œµ2),

236
7. Methods of Averaging
where x0 is determined as being the solution of the equation
dx0
dœÑ = ¬Øf(x0) = lim
T ‚Üí‚àû
1
T
 T
0
f(œÑ, x0)dœÑ,
x0(0) = Œæ,
and x1 is given by the formula
x1(t, t/Œµ) = X1(t) +
 t/Œµ
0
[f(œÑ, x0) ‚àí¬Øf(x0)]dœÑ.
Under the assumptions of the theorem, the integral in this formula is
uniformly bounded, and X1 is (as yet) an undetermined function.
We wish to Ô¨Ånd x(h, Œµ) at some step 1 ‚â´h ‚â´Œµ. To do this we use and
compare two methods: computation of averages and extrapolation.
7.7.1
Direct Calculation of Averages
First, to take advantage of the averaging formula, we must evaluate x0,
which entails evaluating of the average of f. This can be done in (at least)
three diÔ¨Äerent ways.
1. Approximation of the Averaging Limit. Since the average exists, given
a tolerance Œ¥ we can Ô¨Ånd a constraint T(Œ¥, x) such that
&&&&
1
T1
 T1
0
fdœÑ ‚àí1
T2
 T2
0
fdœÑ
&&&& < Œ¥
and
&&&&
1
T1
 T1
0
fdœÑ ‚àí¬Øf(x)
&&&& < Œ¥
for all T1 and T2 ‚â•T. If we can Ô¨Ånd such a constraint T(Œ¥, x), then we can
take as an approximation to ‚ü®f‚ü©the integral
¬Øf(x) =
1
T(Œ¥, x)
 T (Œ¥,x)
0
fdœÑ + O(Œ¥).
The tolerance Œ¥ is related to the accuracy of the numerical scheme that we
hope to obtain from this work.
2. Second DiÔ¨Äerence Method. Let us deÔ¨Åne
V (T, x) =
 T
0
f(œÑ, x)dœÑ = ¬Øf(x)T + p(T, x),
which has the form of the average of f times T plus a function p that is
almost periodic in T and has mean zero.
First, note that if we take the second diÔ¨Äerence of V with respect to T,
we get
V (2T, x) ‚àí2V (T, x) = p(2T, x) ‚àí2p(T, x).

7.7. Computational Aspects of Averaging
237
For any tolerance Œ¥ and any value of x, there is a Œ¥ translation number
T(Œ¥) for p such that
p(2T, x) ‚àíp(T, x) ‚àíp(T, x) + p(0, x) = O(Œ¥)
uniformly for x in some domain G. Therefore, if T is such a translation
number, then
V (2T, x) ‚àí2V (T, x) = O(Œ¥).
Therefore, by tabulating values of the second diÔ¨Äerence of the integral,
V (2T, x) ‚àí2V (T, x),
we can Ô¨Ånd candidates for translation numbers. Unfortunately, the fact that
this diÔ¨Äerence is small does not necessarily imply that |p(T, x)| is small.
Of course in some cases this method works exactly, for example, if f is
periodic in t. Experience with this method has been good. It fails when f
has an unfortunate set of frequencies. However, the method proposed here
proceeds by tabulating the values of
V (2T, x) ‚àí2V (T, x)
and selecting the Ô¨Årst value of T away from T = 0 for which the tolerance
|V (2T, x) ‚àí2V (T, x)| < Œ¥
is met. For this value of T we take
¬Øf(x) = V (T, x)
T
.
Various tests can be performed to create a sense of security in this estimate,
but it could be undecidable.
3. Ergodic Theorem. If f is a quasiperiodic function, say
f(œÑ, x) = F(œâ1œÑ, . . . , œâLœÑ, x)
where F is 2œÄ-periodic in each of its Ô¨Årst L variables, then we know from
the weak ergodic theorem [117] that
¬Øf(x) = lim
T ‚Üí‚àû
1
T
 T
0
f(œÑ, x)dœÑ =
1
(2œÄ)L
 2œÄ
0
¬∑ ¬∑ ¬∑
 2œÄ
0
F(Œ∏1, . . . , Œ∏L, x)dŒ∏1 ¬∑ ¬∑ ¬∑ dŒ∏L.
Therefore, we could evaluate the L-fold integral wherever needed. This,
however, can turn out to be a major project in itself if L ‚â•4.
7.7.2
Extrapolation
Once a satisfactory value of T is found, we return to the full problem, where
we set
Œµ‚Ä≤ = h
T .

238
7. Methods of Averaging
Then
2x

h, Œµ‚Ä≤
2

‚àíx(h, Œµ‚Ä≤) = x0(h) + O(Œ¥h) + O
 h
T
2
.
On the other hand,
x(h, Œµ) = x0(h) + O(Œµ).
Therefore,
x(h, Œµ) = 2x

h, Œµ‚Ä≤
2

‚àíx(h, Œµ‚Ä≤) + O(Œµ) + O(Œ¥h) + O
 h
T
2
.
This formula shows how the extrapolation formula depends on the choice
of Œ¥, T, and Œµ‚Ä≤. Usually Œµ‚Ä≤ ‚â´Œµ, so solving the problems for x(h, Œµ‚Ä≤/2) and
x(h, Œµ‚Ä≤) is usually signiÔ¨Åcantly faster than calculating x(h, Œµ) directly. One
of the interesting features of this approach, as with the QSSA method in
Chapter 8, is that the result improves as the stiÔ¨Äness (‚âà1/Œµ) increases
[79, 112].
7.8
Averaging Systems with Random Noise
In addition to his development of averaging methods for studying oscil-
latory nonlinear systems, Bogoliubov also introduced methods for dealing
with systems perturbed by random noise. This work and its successors are
described in [82]. We summarize here some relevant parts of that work,
since they form a natural and useful extension of the methods in this chap-
ter. The ideas are very powerful, and they enable us to further explore the
connections between oscillations, chaos, and randomness.
What does ‚Äúrandom‚Äù mean? What are ‚Äúrandom variables‚Äù? What are
‚Äúrandom signals‚Äù? What are ‚Äúrandom perturbations‚Äù? Many mathemati-
cians, scientists, and engineers have worked on developing mathematical
structures that make these questions, and their answers, precise enough to
enable us to Ô¨Ånd rigorous results. These results have been used to explain
physical observations, like Brownian motion, and to design devices, like FM
radio, that can perform well in the presence of noise.
In this section we Ô¨Årst recall some basic ideas from the theory of proba-
bility theory, and then we describe a basic averaging result for dynamical
systems perturbed by random noise.
7.8.1
Axioms of Probability Theory
Probability theory is based on the idea of a probability space. We write
this as (‚Ñ¶, E, P), where

7.8. Averaging Systems with Random Noise
239
1. ‚Ñ¶is a set, called the sample space, whose elements might correspond
to the possible outcomes of some experiment.
2. E is a collection of subsets of ‚Ñ¶. This is called the collection of events
or observables of ‚Ñ¶. We say event E ‚ààE occurs if the outcome of an
experiment, say œâ, is in E.
3. P is a mapping from E to the interval [0, 1]. For E ‚ààE, P(E) is the
probability of this event. We interpret this in the sense of frequency:
If the experiment is performed a large number of times, the event E
should occur in proportion P(E).
These entities have certain consistency properties required of them to be
useful:
1. E includes both the whole space ‚Ñ¶and the empty set œÜ.
2. If a set A is in E, then so is its complement Ac = {œâ ‚àà‚Ñ¶, œâ /‚ààA}.
3. If a sequence of sets {An} is in E, then so is the union ‚à™‚àû
n=1An ‚ààE.
4. 0 = P(œÜ) ‚â§P(A) ‚â§P(‚Ñ¶) = 1 for any event A ‚ààE.
5. P(‚à™‚àû
n=1An) = ‚àû
n=1 P(An) for any sequence of disjoint sets An ‚ààE.
A random variable can be thought of as being a measurement of a sample
in ‚Ñ¶. Consider a function X : ‚Ñ¶‚ÜíE1 with the property that if x is in
the range of X, i.e., x ‚ààX(‚Ñ¶), then the set {œâ ‚àà‚Ñ¶|X(œâ) = x} = X‚àí1(x)
is an event in E. This ensures that we can determine the probability of a
given measurement: We write P(x) = P(X‚àí1(x)). (In general, X can take
values in any kind of space, which we denote later by Y .)
Using this, we can deÔ¨Åne the moments of a random variable, namely,
E(Xn) =

x‚ààX(‚Ñ¶)
xnP(x)
for any integer n. In particular, the mean value (also known as the average
or expected value) of X refers to E(X), the variance of X is E(X2)‚àíE(X)2,
and so on.
The distribution function of a random variable X is the function FX(x) =
P(X ‚â§x). Since this is a nondecreasing function, it has a derivative (almost
everywhere), and we expect that
FX(x) =
 x
‚àí‚àû
fX(u)du,
where the function fX is referred to as the probability density function
of X. The function fX might be a generalized function, and the rigorous
mathematical theory underlying all of these ideas is quite involved (see
[117, 37] for expositions of this).

240
7. Methods of Averaging
However, in many important cases, the probability density function is
a smooth function. For example, in the case that X is a normal random
variable, we have
fX(x) =
1
‚àö
2œÄœÉ2 e‚àí(x‚àí¬µ)2
2œÉ2 .
For such a random variable, its mean is ¬µ and its variance is œÉ2. Moreover,
the meaning of mean and variance can be made clearer by observing that
for a normal random variable 68+% of samples will be within one standard
deviation of the mean (i.e., within ¬µ ¬± œÉ), 95+% within 2œÉ, and 99+%
within 3œÉ. Thus, the mean locates the values of X, and the variance tells
how spread out they are.
Random Processes.
Of interest to us here are random variables that also depend on time.
We refer to such functions as random processes (or random signals), and we
write them as X(t, œâ) : [t0, t1] √ó ‚Ñ¶‚ÜíY , where Y is some set in E1. These
processes might have some smoothness properties with respect to t, but
they have only the properties of random variables with respect to œâ. For
example, there is no topology in the probability space, so we cannot speak
of X being continuous over ‚Ñ¶. Usually, œâ is not included in the notation
for random processes.
Brownian motion is a good and useful example. We say that a random
process B(t) is Brownian motion if
1. For any sequence of times t0 < t1 < t2 < ¬∑ ¬∑ ¬∑ < tn the diÔ¨Äerences
B(t1) ‚àíB(t0), B(t2) ‚àíB(t1), . . . , B(tn) ‚àíB(tn‚àí1) are independent
random variables.
2. The distribution function for the increments is normal with mean
zero and variance t:
P(B(t1) ‚àíB(t0) ‚â§x) =
1

2œÄ(t1 ‚àít0)
 x
‚àí‚àû
exp

‚àí
u2
2(t1 ‚àít0)

du.
3. B(t) is a continuous function of t for 0 ‚â§t < ‚àû.
A notation for stochastic diÔ¨Äerential equations has been developed. We
write
dX = œÉdB,
and this is interpreted in terms of stochastic integrals that are deÔ¨Åned so
that in this case the process X is Brownian motion with variance œÉ2t [82].
An important point for understanding stochastic diÔ¨Äerential equation
notation is that the probability density function for X, say p(x, t), satisÔ¨Åes
the diÔ¨Äusion equation
‚àÇp
‚àÇt = (œÉ2/2)‚àá2
xp.

7.8. Averaging Systems with Random Noise
241
More generally, we interpret the solution of the stochastic diÔ¨Äerential
equation
dx = f(x)dt + g(x)dB
in terms of the probability density function for x, which in this case is
‚àÇp/‚àÇt = ‚àá¬∑ ((g2/2)‚àáp ‚àífp).
Such processes are referred to as diÔ¨Äusion processes.
We will consider problems involving noise that is described using a
random signal of the form
y(t/Œµ, œâ) : [0, ‚àû] √ó ‚Ñ¶‚ÜíY,
where Œµ is a small dimensionless parameter that will describe the ratio of
the noise time scale to the system‚Äôs response time scale. We require some
natural technical conditions on y.
First, we suppose that y is ergodic: For any measurable function h(s)
and any t
E(h(y(t/Œµ, œâ))) =

Y
h(y)œÅ(dy),
where œÅ is a measure on Y called the ergodic measure for y. Essentially this
says that the average of a function along a noise trajectory is the same as
averaging the function over the whole noise space, with weighting allocated
by œÅ. Hereafter we omit the œâ in this notation and write y(t/Œµ), and so on.
We also suppose that the noise is mixing, meaning that in a probabilistic
sense not made precise here
 t
0
|p(t‚Ä≤, y, C)dt‚Ä≤ ‚àíœÅ(C)|dt‚Ä≤ < ‚àû
for all t, where p(t‚Ä≤, y, C‚Ä≤) is the transition probability density function for
moving from point y to a set C in a time interval of length t‚Ä≤. This is
essentially the same condition that we required in the Nonlinear Averaging
Theorem in Section 7.1. (See [82] details.)
7.8.2
Random Perturbations
Consider a system
Àôx = f(y(t/Œµ), x, t),
where y is an ergodic mixing process. Then it is shown in [82] that
x(t) = ¬Øx(t) + ‚àöŒµx1 + O(Œµ),
where ¬Øx is derived from the averaged system
Àô¬Øx = ¬Øf(¬Øx) ‚â°

Y
f(y, ¬Øx)œÅ(dy)

242
7. Methods of Averaging
and x1 is a diÔ¨Äusion process whose drift (f above) and variance (g2 above)
can be calculated.
Therefore, the solution moves like the averaged system, but the error
accumulates. The Law of the Iterated Logarithm states that
lim
t‚Üí‚àû
‚àöŒµx1(t)
‚àö2t log log t = 1
almost surely. Then if t ‚â§T/Œµ, ‚àöŒµx1(t) should be (approximately) bounded
by ‚àöŒµ. Therefore, on an interval up to T/Œµ, the average system dominates,
but beyond that the noise takes over if there is no other mechanism present
to control it.
7.8.3
Example of a Randomly Perturbed System
Let us consider a random process y(t) that takes values in a Ô¨Ånite set
Y ‚äÇE1, say {S1, . . . , SM}. We suppose that it is a Markov jump process
with switching times determined from an exponential distribution. That is,
for each sample path there is a sequence of random times
0 = œÑ0 < œÑ1 < œÑ2 < ¬∑ ¬∑ ¬∑ < œÑK+1
and a sequence of states in Y , say
y0, y1, . . . ,
such that y(t) = yk for œÑk ‚â§t < œÑk+1. Then to Ô¨Ånd the sample path for x,
we solve
Àôx = f(yk, x, t), x(ŒµœÑk) = x(ŒµœÑ ‚àí
k )
for ŒµœÑk ‚â§t < ŒµœÑk+1 and for k = 1, . . . , K, where ŒµœÑK+1 = T is a Ô¨Åxed
number. We take for the initial condition on this interval the ending value
of x on the preceding interval. This is denoted by x(ŒµœÑk) = x(ŒµœÑ ‚àí
k ).
We suppose that the distribution of switching times is exponential,
P(œÑk+1 ‚àíœÑk > t) = exp(‚àíŒªt),
and that at each switching time, the probability that y jumps from state Si
to state Sj is a given number œÄi,j. The matrix P = (œÄi,j) is the transition
probability matrix for this Markov chain. We suppose that this Markov
chain for y is ergodic. That is, 1 is a unique eigenvalue of P; all others
satisfy |Œª| < 1; and the corresponding left eigenvector, say œÅ, has only
positive entries; i.e., œÅj > 0 for j = 1, . . . , M.
Calculating sample paths is expensive if Œµ is small, since to get to time
T with a sample path for x we must solve the sampled problems on
approximately O(T/Œµ) time intervals.
On the other hand, the averaging theorem for this ensures that we can
use the averaged problem as an approximation:
Àô¬Øx = ¬Øf(¬Øx, t), ¬Øx(0) = x0,

7.9. Exercises
243
where
¬Øf(¬Øx, t) =
M

m=1
f(Sm, ¬Øx, t)œÅm.
This gives a good approximation up to time t = T if Œµ is small.
Beyond time T, our estimate of the diÔ¨Äerence
xŒµ(t) ‚àí¬Øx(t)
diÔ¨Äuses away.
This example highlights the fact that y is a fast process: As Œµ ‚Üí0 the
switching times for y, Œµ(œÑk+1 ‚àíœÑk), get closer to each other, so the jumping
frequency increases. Since y is ergodic, it rapidly visits all states in Y .
How accurate is the approximation? The almost periodic function av-
eraging theory of Bogoliubov for deterministic systems (i.e., nonrandom
ones) creates approximations that are of order Œµ on a time interval [0, T].
The random perturbations are of order ‚àöŒµ on this interval, and even with
stability properties required of the averaged system they eventually wander
oÔ¨Ä: The accuracy diÔ¨Äuses away, even if the averaged system is exponentially
stable. So noise will eventually prevail. The random averaging theory is pre-
sented in detail in [82], where the second order approximation is derived
and the meaning of the approximation is studied in detail.
Another interesting example of this is that in which the averaged system
has an energy surface, as if it were a gradient system. Then the potential
wells dominate behavior for certain periods of time, but eventually noise
will (probably) move the solution through all of the wells in a predictable,
but random, way. In fact, most solutions end up visiting all of the potential
wells in a way that is itself described by a Markov chain [42].
7.9
Exercises
7.1. a. Show that if f(t) is a periodic function or a trigonometric polynomial,
then condition H2 in Section 7.1 is satisÔ¨Åed. That is, show that
 t
0
[f(t) ‚àí‚ü®f‚ü©]dt
is bounded for all œÑ ‚â•0.
b‚àó. If Œæ(t, œâ) is a stationary random process, then its average exists,
lim
T ‚Üí‚àû(1/T)
 T
0
E{|Œæ(t, œâ)|}dt ‚â°‚ü®Œæ‚ü©,
and in addition,
lim
T ‚Üí‚àû(1/T)
 T
0
{Œæ(t, œâ) ‚àí‚ü®Œæ‚ü©}dt ‚â°Œ∑(œâ)

244
7. Methods of Averaging
is a Gaussian random variable [91]. Find such a process Œæ(t, œâ) that
does not have a convergent Fourier series, even one having random
coeÔ¨Écients. This example shows that generalized Fourier analysis is
not suÔ¨Écient to describe random processes.
7.2.
Show that if condition H3 in Section 7.1 is replaced by the system
being stable under persistent disturbances, then the mean averaging
theorem still holds.
7.3.
Averaging linear systems: Let A be an oscillatory matrix.
a. Evaluate ‚ü®B‚ü©‚â°‚ü®e‚àíAtB(t)eAt‚ü©when B is a matrix of periodic
functions.
b. Show that [‚ü®B‚ü©, A] = 0, where [A, C] ‚â°AC ‚àíCA is the commutator,
or Poisson bracket, of A and C.
7.4.
Show that the two-time scale method can be used for linear systems
by carrying out the Ô¨Årst two steps in solving
dx
dt = ŒµC(t)x,
where C is a periodic function or a trigonometric polynomial of t.
7.5. a. Verify the induction argument used in the proof of the DiÔ¨Äerence
Equation Averaging Theorem.
b. Show that if all of the eigenvalues of ‚ü®B‚ü©have negative real parts,
then the conclusion of the diÔ¨Äerence equation averaging theorem in
Section 7.3 holds uniformly for all n = 1, 2, 3, . . . .
7.6.
Verify the spectral decomposition of the cyclic matrix used in
Section 7.3.
7.7.
Apply the method of averaging to the system
dy
dt = Œµ exp(‚àíAt)f(exp(At)y, Œµ)
when A is an oscillatory matrix and the components of f(y, Œµ) are
smooth functions of Œµ and analytic functions of the y variables. Here
y, f are in EN and A is in EN√óN.
7.8. a. Rewrite the equation
u‚Ä≤‚Ä≤ + œâ2u = ŒµF(u)
as an equivalent linear system in E2 whose coeÔ¨Écient matrix (when
Œµ = 0) has eigenvalues ¬±iœâ. Convert this system to phase-amplitude
coordinates using polar coordinates (r, Œ∏). Write the result as a Ô¨Årst-
order equation using Œ∏ as the timelike variable.
b. Consider DuÔ¨Éng‚Äôs equation
x‚Ä≤‚Ä≤ + ax = Œµbx3.
Describe its solution by using the averaging procedure. Compare your
answers with those obtained for DuÔ¨Éng‚Äôs equation in Chapter 6 for
similar parameter values.
c. Apply the near-identity transformation to the system in part a.
7.9.
Let z(t) be a complex-valued function that satisÔ¨Åes the diÔ¨Äerential
equation
dz
dt = iœâz + (a ‚àízz‚àó)z,

7.9. Exercises
245
where z‚àódenotes the complex conjugate of z. Describe the behavior
of solutions for z(t) when a and œâ are real numbers.
7.10.
The gene pool carried by a population of animals that have a genetic
trait (one locus) of one of three possible types (two alleles) can be
described by the diÔ¨Äerence equation
gn+1 =
rng2
n + sn(1 ‚àígn)gn
rng2n + 2sngn(1 ‚àígn) + tn(1 ‚àígn)2 .
Suppose that there is slow selection; that is,
rn = 1 + ŒµœÅn,
sn = 1 + ŒµœÉn,
and
tn = 1 + ŒµœÑn,
where œÅn, œÉn, and œÑn are assumed to be oscillatory functions that
have mean values
‚ü®œÅ‚ü©= lim
n‚Üí‚àû
1
n
n‚àí1

u=0
œÅu,
‚ü®œÉ‚ü©= lim
n‚Üí‚àû
1
n
n‚àí1

u=0
œÉu,
‚ü®œÑ‚ü©= lim
n‚Üí‚àû
1
n
n‚àí1

u=0
œÑu.
With this notation, apply the Nonlinear DiÔ¨Äerence Averaging The-
orem to analyze this system for small values of Œµ in the four cases
œÉ > œÅ, œÑ; œÉ < œÅ, œÑ; œÅ < œÉ < œÑ; and œÑ < œÉ < œÅ.
7.11.
Let f(t, x, Œµ) = cos(x ‚àíœât) + cos t as in Section 7.5.5. Carry out the
near-identity transformation for the resulting equation. Show that
if œâ1 = 0 and œâ0 Ã∏= ¬±1, then M2 = 0 and, if œâ0 = ¬±1, ‚àíM2 =
¬± sin x/2 + œâ2.
7.12‚àó. Consider the initial value problem
‚àÇu
‚àÇt + b
	x
Œµ , t
Œµ, x, t, Œµ

‚àÇu
‚àÇx = 0,
u(x, 0) = U(x)
for real variables x, t and for 0 < Œµ ‚â™1. Let Œæ = x/Œµ and œÑ = t/Œµ, and
suppose (i) U(x) and b(Œæ, œÑ, x, t, Œµ) are smooth (at least C1) functions
of Œæ, œÑ, x, t, and Œµ and (ii) b(Œæ +1, œÑ, . . . ) = b(Œæ, œÑ +1, . . . ) = b(Œæ, œÑ, . . . )
for all values of Œæ, œÑ, x, t, and Œµ. That is, b is doubly periodic in Œæ and
œÑ with period 1. Use the multiple-scale formalism to construct an
approximation to the solution of this problem that is valid for small
values of Œµ: Let
u(x, t) = u0(Œæ, œÑ, x, t) + Œµu1(Œæ, œÑ, x, t) + ¬∑ ¬∑ ¬∑
and determine equations for coeÔ¨Écients, which should be bounded
for all values of Œæ, œÑ.
Show that u0(x, t) solves
‚àÇu0
‚àÇt + œÅ(x, t)‚àÇu0
‚àÇx = 0,
u0(x, 0) = U(x).
(Hint: Consider the ordinary diÔ¨Äerential equation
dŒæ
dœÑ = b0(Œæ, œÑ, x, t)
in which x and t appear as parameters. It is known [30] that for any
choice of initial data œÑ0, Œæ0, the solution, say
Œæ = X(œÑ, œÑ0, Œæ0),

246
7. Methods of Averaging
has the properties that
(i) the limit
lim
œÑ‚Üí‚àûX(œÑ, œÑ0, Œæ0)/œÑ = œÅ(x, t)
exists and is independent of (œÑ0, Œæ0);
(ii) œÅ is a continuous function of (x, t);
(iii) if œÅ is a rational number, then any solution approaches a periodic
solution that is a torus knot;
(iv) if œÅ is irrational, then any solution is dense in the torus.)
7.13‚àó. Consider the problem [122]
du
dt
=
Œµ(B(t)u + C(t)v)
dv
dt
=
Az + Œµ(D(t)u + E(t)v),
where each of B(t), C(t), D(t), and E(t) is a Ô¨Ånite sum of periodic
matrices and the matrix A is stable. Suppose that the average
lim
T ‚Üí‚àû
1
T
 T
0
B(s)ds
is zero. Show that any solution to this system can be constructed in
the form
u
=
u0(t, Œµt, Œµ2t) + O(Œµ)
v
=
v0(t, Œµt) + O(Œµ)
by using the multitime method.
7.14‚àó. Consider the integro-diÔ¨Äerential equation [80]
dx
dt = Œµ

¬µ +
 t
0
K(t ‚àíœÑ)(sin(x(œÑ) + œÑ) + sin x(œÑ))dœÑ

,
where the kernel K satisÔ¨Åes |K(œÑ)| ‚â§Ae‚àíŒ±t for 0 ‚â§œÑ < ‚àû. Here
|Œµ| ‚â™1, 0 < Œ±, and ¬µ is a Ô¨Åxed constant. Use the multitime Ansatz
x = x0(t, s) + Œµx1(t, s) + ¬∑ ¬∑ ¬∑ ,
where s = Œµt, to show that x0(t, s) = X0(s), which satisÔ¨Åes the
equation
dX0(s)
ds
= ¬µ + K‚àósin X0,
where
K‚àó= lim
T ‚Üí‚àû
1
T
 T
0
 t
0
K(t ‚àíœÑ) dœÑ dt.
7.15‚àó. Consider the system of nonlinear diÔ¨Äusion equations [25]
‚àÇu
‚àÇt
=
D1 ‚àÇ2u
‚àÇx2 + ŒµŒª(u2 + v2)u ‚àíœâv
‚àÇv
‚àÇt
=
D2 ‚àÇ2v
‚àÇx2 + œâu + ŒµŒª(u2 + v2)v,

7.9. Exercises
247
where D1, D2, and œâ are positive constants and Œª(r) = 1‚àír. Identify
slowly modulated periodic waves. (Hint: Introduce polar coordinates
to u and v by R2 = u2 + v2 and tan Œò = u/v, average the problem,
and look for (traveling wave) solutions of the result having the form
R(Œµx ‚àícŒµ2t), etc., where c is a constant to be determined.)

8
Quasistatic-State Approximations
The appearance of several time scales in a problem can mean that various
components of the system respond at diÔ¨Äerent rates. Rapidly responding
components can try to reach some equilibrium, while the other components
change hardly at all. It is not surprising that such perturbation problems
can be studied using stability methods, because both deal with how solu-
tions approach equilibria. These problems diÔ¨Äer from those in Chapter 7,
where oscillations occurred on a fast time scale relative to other changes.
In this chapter we study problems that try to equilibrate on a fast time
scale while other components in the system change more slowly.
Gradient systems illustrate many of the methods developed in this
chapter. Consider the system of equations
dy
dt = ‚àí‚àáG(y),
where y is in EN and G maps EN into E1 with
lim
|y|‚Üí‚àûG(y) ‚Üí‚àû.
The solutions of this equation either begin and remain at a critical point of
G, say y‚àówhere ‚àáG(y‚àó) = 0, or they approach such an equilibrium. Since
along such a solution
dG(y(t))
dt
= ‚àí|‚àáG(y)|2,
the solution must approach a critical point of G(y).

250
8. Quasistatic-State Approximations
An analogous singular perturbation problem is
Œµdy
dt = ‚àí‚àáG(y).
The parameter Œµ seems not to be essential, since the change of variables
œÑ = t/Œµ removes it from the equation:
dy
dœÑ = ‚àí‚àáG(y).
(8.1)
Still, in terms of the original time scale t, the variable œÑ approaches ‚àûas
Œµ ‚Üí0+. Let Y (œÑ) be a solution of Equation (8.1) for which ‚àáG(Y (0)) Ã∏= 0.
Then Y (œÑ) ‚Üíy‚àóas œÑ ‚Üí‚àû, where y‚àóis a critical point of G : ‚àáG(y‚àó) = 0.
In terms of the original time scale, we have y = Y (t/Œµ). Therefore, for each
t > 0, Y (t/Œµ) ‚Üíy‚àóas Œµ ‚Üí0+! This calculation illustrates a duality between
stability and perturbations in quasistatic-state problems. Note that y‚àóneed
not be a minimum of G, but in most applications, we restrict attention to
initial data Y (0) that lie in the basin of attraction of a minimum.
This result is robust: Solutions of the perturbed system
Œµdy
dt = ‚àí‚àáG(y) + Œµg(t, y, Œµ),
where Œµ is a small positive parameter, behave in a way similar to Equa-
tion (8.1). Suppose that g satisÔ¨Åes Carath¬¥eodory‚Äôs conditions (g is Lebesgue
integrable with respect to t, continuously diÔ¨Äerentiable with respect to the
components of y, continuous as a function of Œµ, and bounded). Just as in
our discussion of stability under persistent disturbances (see Section 3.4),
we see that the function G(y) is almost a potential function for this system,
but its derivative along solutions is
dG(y(t))
dt
= ‚àí1
Œµ|‚àáG(y)|2 + ‚àáG ¬∑ g(t, y, Œµ).
This is negative for all values of y except those near ‚àáG(y) = 0. Therefore,
if y(0) is not near such a point, then y(t) ‚Üíy‚àó+O(Œµ) as t ‚Üí‚àûor Œµ ‚Üí0+.
It is interesting to note that a similar calculation can work even if g
represents (large deviation) Gaussian noise [91].
Next consider the system of equations
dx
dt
=
f(t, x, y, Œµ)
(8.2)
Œµdy
dt
=
‚àí‚àáG(y) + Œµg(t, x, y, Œµ),
where x, f ‚ààEM. Let us suppose that y‚àóis a minimum of G(y) and that
y(0) is near y‚àó. Then as in the preceding paragraph, the values of the solu-
tion y(t) will lie near y‚àóas well. Suppose also that f satisÔ¨Åes Carath¬¥eodory‚Äôs
conditions and that the solutions of
dx
dt = f(t, x, y‚àó, e)

8. Quasistatic-State Approximations
251
beginning in a bounded domain ‚Ñ¶remain there for 0 ‚â§t ‚â§T. Then the
manifold ‚Ñ¶√ó {y = y‚àó} is referred to as a quasistatic manifold for the
system. It is not invariant with respect to solutions, but solutions of the
full system with y(0) near y‚àócertainly stay near this manifold for small Œµ
and 0 ‚â§t ‚â§T.
Finally, consider the system
dx
dt
=
‚àí‚àáF(x) + Œµf(t, x, y, Œµ),
x(0) = Œæ
(8.3)
Œµdy
dt
=
‚àí‚àáG(y) + Œµg(t, x, y, Œµ),
y(0) = Œ∑.
If y(0) is near y‚àó(a minimum of G), if x(0) is near x‚àó(a minimum of F),
and if f and g satisfy Carath¬¥eodory‚Äôs conditions for y near y‚àóand x near x‚àó
for 0 ‚â§t < ‚àûthen for small positive values of Œµ, y quickly equilibrates to a
neighborhood of y‚àó(a quasistatic manifold) and x eventually equilibrates
to a neighborhood of x‚àó. In particular, the solution is near
x
=
x‚àó+ x‚àó
0(t) + O(Œµ)
y
=
y‚àó+ Y (t/Œµ) + O(Œµ),
where
dx‚àó
0
dt = ‚àí‚àáF(x‚àó+ x‚àó
0),
x‚àó
0(0) = Œæ ‚àíx‚àó
and
dY
dœÑ = ‚àí‚àáG(Y ),
Y (0) = Œ∑ ‚àíy‚àó.
This approximation is valid on the entire interval 0 ‚â§t < ‚àû, and we see
that it is a sum of terms: the equilibrium (x‚àó, y‚àó) of the reduced problem
(Œµ = 0 in Equation (8.3)), two transients, one on the slow time scale t
(for x‚àó
0) and one on the fast time scale œÑ (for Y ), and a small error. The
expression (x‚àó+ x‚àó
0, y‚àó) is called the quasistatic-state approximation, and
Y is the initial transient.
It is clear that level sets of the potential functions F and G in system
(8.3), say
{(x, y) ‚ààEM √ó EN|F(x) = F(x‚àó) + aŒµ
and
G(y) = G(y‚àó) + bŒµ},
where a and b are Ô¨Åxed constants, deÔ¨Åne a tube in [0, ‚àû) √ó Em √ó En that
is of order o(1) about x‚àó, y‚àó. It is ‚Äústicky‚Äù in the sense that solutions of the
system starting near it are attracted to it and once inside the boundary
stay there.
The Ô¨Årst example (8.1) describes a duality between perturbations and
stability under persistent disturbances. The next example shows that be-
havior on various time scales can be peeled oÔ¨Ä. For example, the dynamics
of the Ô¨Årst of Equations (8.2) with y = y‚àóand Œµ = 0 could be chaotic, but

252
8. Quasistatic-State Approximations
still y is attracted to y‚àó. Finally, the third example shows how an approxi-
mation can be constructed for 0 ‚â§t < ‚àûwhen the slow and fast auxiliary
problems following the system (8.3) are both stable. These examples are
useful to keep in mind throughout this chapter, where various results are
derived that extend and apply these observations.
The results just described for perturbed gradient systems can be ex-
tended to more general systems. In fact, a Liapunov function centered at
some state is quite similar, at least locally, to a potential function for a
gradient function. For example, consider the system
Œµdy
dt = G(y),
and suppose that it has a static state, say y = y‚àó, that is asymptotically
stable. Then from Section 3.4 we know that there is a Liapunov function
W(y) that is continuously diÔ¨Äerentiable at and near y‚àó, W(y‚àó) = 0, and
there is a comparison function c such that
dW
dt = 1
Œµ‚àáW ¬∑ G(y) ‚â§‚àí1
Œµc(|y ‚àíy‚àó|)
for y near y‚àó. It follows that W(Y (t/Œµ)) ‚ÜíW(y‚àó) as Œµ ‚Üí0+ for every
t > 0, and so y ‚Üíy‚àóif y(0) is near y‚àó.
Using the idea of stability under persistent disturbances, we can apply
W to study solutions of a nearby (Carath¬¥eodory) system
dy
dt = 1
ŒµG + g(t, y, Œµ),
in which case y(t, Œµ) ‚Üíy‚àó+ o(1) as Œµ ‚Üí0+ for every t > 0 [70]. In
this way Liapunov functions are used later to extend the quasistatic-state
approximation beyond gradient-like systems.
In Chapter 1 we saw that the spectrum of the matrix A determines the
behavior of solutions of the linear system
dx
dt = Ax.
In particular, as in Chapter 7, if the eigenvalues of A are real but widely
separated, then the solution has relatively fast and slow modes. The ratios
of the eigenvalues show what are the fast and slow time scales.
The term ‚Äúsingular-perturbation problem‚Äù applies to any problem whose
solution has a near-singular behavior, in the sense of complex function
theory, relative to a perturbation parameter (Œµ) in the system. Usually,
the solutions have essential singularities at Œµ = 0. The problems studied in
Chapter 7 are singular perturbation problems, since their solutions involve
terms of the form exp(it/Œµ). In this chapter we study problems that have
slowly varying parts and rapidly equilibrating parts‚Äîtypically they include
terms of the form exp(‚àít/Œµ). In contrast to the last chapter, we do not study
highly oscillatory problems here, but only problems whose solutions are the

8. Quasistatic-State Approximations
253
sum of a (slow) quasistatic-state approximation (QSSA) and a rapid initial
transient to it.
Linear problems clearly illustrate these results from the analytic point
of view. For example, the equation
dx
dt = 1
ŒµAx + f(t)
has its solution given explicitly by the formula
x(t) = exp
At
Œµ

x(0) +
 t
0
exp
A(t ‚àís)
Œµ

f(s)ds.
If the eigenvalues of A are real and negative, and if Œµ is a small positive
number, then the kernel exp(A(t ‚àís)/Œµ) behaves like a delta function in
the sense that
x(t) = eAt/Œµ(x(0) + ŒµA‚àí1f(0)) ‚àíŒµA‚àí1f(t) + O(Œµ2A‚àí2).
The Ô¨Årst and last terms are small, so as a useful approximation we write
eAt/Œµ ‚âà‚àíŒµA‚àí1Œ¥(t)
for t ‚â•0 and Œµ near zero. This calculation also shows that the solution x(t)
is approximated by the sum of a rapidly decaying exponential (exp(At/Œµ))
and a slowly varying term (‚àíŒµA‚àí1f(t)). Integrals of this form are frequently
encountered in proofs of quasistatic-state approximations.
A system of ordinary diÔ¨Äerential equations, say
dz
dt = F(t, z)
where z in EM+N, might have two time scales, say t and t/Œµ. It is often
the case that the parameter Œµ can be identiÔ¨Åed and that the system can be
converted to the form
dx
dt
=
f(t, x, y, Œµ)
Œµdy
dt
=
g(t, x, y, Œµ),
where x, f ‚ààEM, y, g ‚ààEN, and Œµ is a small positive parameter. On
the other hand, it is often neither possible nor desirable to preprocess the
system to get it into this form, since the same system might have diÔ¨Äerent
time scales at various points in the state space (z). We describe an example
of this at the end of the chapter. However, we mainly study preprocessed
systems in this chapter, since we have to start somewhere.
It is important to consider only dimensionless parameters Œµ in these
problems; in fact, trivial changes of units, say from nanometers to meters, in
a model should not change a solution‚Äôs behavior. Dimensionless parameters
are also important in models, since they make possible comparison of results
obtained in various experimental settings [19].

254
8. Quasistatic-State Approximations
We begin with a geometric description of fast and slow time scales. This
is followed by an analysis of quasistatic states in a linear feedback sys-
tem that shows how responses on the several time scales can be ‚Äúpeeled
oÔ¨Ä‚Äù in succession, where behavior on the fastest scale is determined Ô¨Årst,
then on the next fastest, and so on. Fully nonlinear initial value problems
are studied next. We show that the fastest time response might equili-
brate to a quasistatic manifold. For example, the manifold might contain
quasistatic chaotic dynamics for which no perturbation theory gives ade-
quate approximate solutions. Still, the solutions remain near the quasistatic
manifold. Additional conditions on the manifold dynamics are shown to
ensure convergence of the solution of the perturbed problem on the in-
terval 0 ‚â§t ‚â§T ‚â§‚àû. It is shown under further conditions that the
solution of the full problem can be approximated using the method of
matched asymptotic expansions. The method of matched asymptotic ex-
pansions (MAE) [71, 120, 136, 26, 90, 132, 44, 137, 139] can be used to
construct a quasistatic-state approximation (QSSA) and its initial tran-
sients for this problem. Extensions of this approach to a variety of other
applications are discussed in later sections.
A theory of singular perturbations of nonlinear oscillations is described in
Section 8.5. This begins with Friedrichs and Wasow‚Äôs theory of quasistatic
oscillations and ends with a brief description of nearly discontinuous oscil-
lations. Boundary value problems are described next, and the chapter ends
with a description of nonlinear stability methods for static and stationary
solutions, an analysis of H2‚ÄìO2 combustion, some computational schemes
based on the QSSA, and some aspects of QSSA, for randomly perturbed
systems.
8.1
Some Geometrical Examples of Singular
Perturbation Problems
It helps to have in mind a few simple pictures of singular perturbation
problems. For example, consider a system of two scalar equations
dx
dt
=
f(x, y),
x(0) = Œæ,
Œµdy
dt
=
g(x, y),
y(0) = Œ∑,
where f and g are smooth functions and Œµ is a small positive parameter.
Setting Œµ = 0 in this system results in an algebraic equation and a
diÔ¨Äerential equation:
dx
dt
=
f(x, y),
x(0) = Œæ,
0
=
g(x, y).

8.1. Some Geometrical Examples of Singular Perturbation Problems
255
 y
 x
y  =   (x  )
0
0
œÜ
Figure 8.1. Solution of the reduced problem.
This problem is called the reduced problem.
Solving the algebraic equation
g(x, y) = 0
may pose signiÔ¨Åcant diÔ¨Éculties, as we have seen in Chapter 4, but we
assume that there is a unique smooth function y = œÜ(x) such that
g(x, œÜ(x)) = 0
for all x. On this branch of solutions, the quasistatic problem becomes
dx
dt = f(x, œÜ(x)),
x(0) = Œæ.
We denote the solution of this problem by x0(t), and we deÔ¨Åne y0(t) =
œÜ(x0(t)). If f < 0, then x0 moves to the left, as depicted in Figure 8.1.
Next, we see that the y component of the solution changes rapidly if g
is not near zero, since then
&&&&
dy
dt
&&&& = |g(x, y)|
Œµ
‚â´1.
To investigate this further, we introduce the fast time variable œÑ = t/Œµ, and
the problem becomes
dx
dœÑ
=
Œµf(x, y),
x(0) = Œæ,
dy
dœÑ
=
g(x, y),
y(0) = Œ∑.

256
8. Quasistatic-State Approximations
 y
 x
y =    (x)
œÜ
(Œæ ,Œ∑)
Figure 8.2. A nice singular perturbation problem. Arrows indicate the direction
that solutions move in the xy-plane. Double arrows indicate fast motions.
Setting Œµ = 0 in this system gives
dx
dœÑ
=
0,
x(0) = Œæ,
dy
dœÑ
=
g(x, y),
y(0) = Œ∑.
Equivalently, for Œµ = 0 and x = Œæ, we have
dy
dœÑ = g(Œæ, y),
y(0) = Œ∑.
This is called the initial transient (or sometimes boundary layer) problem.
The value y = œÜ(Œæ) is an equilibrium value for this equation, and if it is
stable, then we expect y ‚ÜíœÜ(Œæ) as œÑ ‚Üí‚àû. This happens if
gy(Œæ, œÜ(Œæ)) < 0
for all Œæ, and then the solution of the full problem behaves as shown in
Figure 8.2, which we refer to as a nice singular perturbation problem.
In this chapter we determine that some problems are nice singular per-
turbation problems, and we construct approximations to their solutions.
The construction involves two steps: First, there is a special solution of the
full problem that lies near (x0(t), y0(t)), called the quasistatic state. It can
be found using the implicit function theorem. Once it is known, the initial
transient can be constructed using a Taylor‚Äôs series.
Complications arise if gy vanishes somewhere along the solution of the
reduced problem. Figure 8.3 shows a typical case, and a solution of the full
problem that encounters a fold is indicated. The complication here is that a
fast time response occurs sometime after the initial transient, at which time

8.2. Quasistatic-State Analysis of a Linear Problem
257
 y
 x
y =    (x)
œÜ
(Œæ ,Œ∑)
Figure 8.3. A not-nice singular perturbation problem.
the reduced solution falls over a fold, and it is usually diÔ¨Écult to locate in
time when this happens, since it depends on the system‚Äôs state. We deal
with one aspect of these problems later in this chapter, but the fact that
slow and fast time scales alternate in these problems poses a major barrier
to constructing approximate solutions.
8.2
Quasistatic-State Analysis of a Linear Problem
The general idea is that the solution of nice problems is the sum of a
slowly varying part and a rapidly dying part, and each of these parts can
be constructed separately. These can be combined to represent the full
solution by matching free constants. The problems described in this section
are ones for which the two parts of a solution can be constructed as power
series in a single small parameter. Proofs are carried out in detail for a
simple problem to indicate what steps are usually involved in validating
QSSAs. Other problems are dealt with in [71, 120, 44, 137, 41].
QSS methods can be illustrated using a linear problem without many
technical complications. In particular, consider the linear problem
dx
dt
=
Ax + By + f(t),
x(0) = Œæ,
Œµdy
dt
=
Dy + g(t),
y(0) = Œ∑,
where x, f, Œæ ‚ààEM, and y, g, Œ∑ ‚ààEN. The matrices A, B, and D are
constants of appropriate dimensions, A ‚ààEM√óM, B ‚ààEM√óN, and so on.
Finally, we suppose that f and g have n+1 continuous derivatives on some
interval 0 ‚â§t ‚â§T.

258
8. Quasistatic-State Approximations
We will show that if a certain condition (H2) is satisÔ¨Åed, then the solution
of the original problem can be written as
x
=
x0(t) + O(Œµ)
y
=
‚àíD‚àí1g(t) + exp
Dt
Œµ

(Œ∑ + D‚àí1g(0)) + O(Œµ),
where the error estimates hold uniformly for 0 ‚â§t ‚â§T. The result is valid
for the case where T = ‚àûif the matrices A and D are stable, that is, if all
of their eigenvalues have negative real parts.
The quasistatic state is constructed using free constants (Œæ‚àó(Œµ)) that
are chosen at a later stage in the perturbation algorithm to ensure that
matching conditions are met. The calculation of Œæ‚àóshows that using the
initial value Œæ of the full problem does not necessarily give the correct
quasistatic state beyond order Œµ.
8.2.1
Quasistatic Problem
The slowly varying part of the solution can be found by Ô¨Årst solving the
quasistatic problem and then later determining what initial conditions are
appropriate for it. The quasistatic problem is posed in a curious way: Given
any smooth function
Œæ‚àó(Œµ) = Œæ0 + ŒµŒæ1 + ¬∑ ¬∑ ¬∑ + ŒµnŒæn + O(Œµn+1)
Ô¨Ånd functions x‚àóand y‚àósuch that
dx‚àó
dt
=
Ax‚àó+ By‚àó+ f(t),
x(0) = Œæ‚àó(Œµ)
Œµdy‚àó
dt
=
Dy‚àó+ g(t),
and x‚àó, y‚àóare smooth function of Œµ for Œµ near zero. Note that (x‚àó, y‚àó) deÔ¨Ånes
a solution of the system of diÔ¨Äerential equations, but no initial condition is
speciÔ¨Åed for y‚àó; instead, we require that the solution (x‚àó, y‚àó) be expandable
in a power series in Œµ.
The following calculation gives evidence that this problem is solvable for
any choice of Œæ‚àó. We set
x‚àó(t, Œµ)
=
x0 + x1Œµ + ¬∑ ¬∑ ¬∑
y‚àó(t, Œµ)
=
y0 + y1Œµ + ¬∑ ¬∑ ¬∑ ,
and we study the problems that result for the coeÔ¨Écients in this expansion:
dx0
dt
=
Ax0 + By0 + f(t),
x0(0) = Œæ0
0
=
Dy0 + g(t),

8.2. Quasistatic-State Analysis of a Linear Problem
259
and
dx1
dt
=
Ax1 + By1,
x1(0) = Œæ1
dy0
dt
=
Dy1,
and so on. If D is invertible, each of these problems has a unique solution.
Slightly more is needed to make the whole process valid.
Hypothesis H1. Suppose that the matrix D can be decomposed as
D = DS +
M ‚Ä≤

j=1
iŒ≤jPj + DU,
where DS and ‚àíDU are stable and Œ≤j Ã∏= 0 for all j = 1, . . . , M ‚Ä≤. There
are M ‚Ä≤ purely imaginary eigenvalues. Moreover, we suppose that this
decomposition is orthogonal. That is, DsDU = DsPj = 0, and so on.
The numbers iŒ≤j account for the purely imaginary eigenvalues of D,
and a consequence of condition H1 is that the oscillatory part of D is
diagonalizable. With this assumption
y0 = ‚àíD‚àí1g,
and x0 is the unique solution of
dx0
dt = Ax0 + f ‚àíBD‚àí1g(t),
x0(0) = Œæ0.
Once x0 and y0 are known, x1 and y1 are uniquely determined from the
next two equations, and so on. This calculation is justiÔ¨Åed by the following
theorem.
Linear Quasistatic-State Theorem. Suppose that f, g, and Œæ‚àóare
smooth functions having continuous derivatives up to order n + 1 in their
arguments for 0 ‚â§t ‚â§T (T < ‚àû) and for Œµ near zero. Moreover, suppose
that D satisÔ¨Åes condition H1. Then there are functions (x‚àó, y‚àó) that solve
the quasistatic problem and
x‚àó(t, Œµ)
=
n

j=0
xj(t)Œµj + O(Œµn+1)
y‚àó(t, Œµ)
=
n

j=0
yj(t)Œµj + O(Œµn+1),
where the coeÔ¨Écients in this expansion are determined uniquely using the
algorithm described above. The error estimate holds uniformly for 0 ‚â§t ‚â§
T.

260
8. Quasistatic-State Approximations
Proof of the Quasistatic-State Theorem. The remainders
R(t, Œµ) = x‚àó(t, Œµ) ‚àí
n

j=0
xj(t)Œµj
and
Q(t, Œµ) = y‚àó(t, Œµ) ‚àí
n

j=0
yj(t)Œµj
satisfy the equations
dR
dt
=
AR + BQ,
R(0) = O(Œµn+1)
ŒµdQ
dt
=
DQ + G,
where
G = ‚àíDdyn
dt Œµn+1 = O(Œµn+1).
The trick is in Ô¨Ånding correct initial values for Q(0). We use Perron‚Äôs ap-
proach to stable and unstable manifolds (See Section 3.2.3). We write
Q in terms of its projection onto the stable and oscillatory modes of
D1 = D ‚àíDU (i.e., Q1) and onto the unstable modes DU (i.e., QU):
Q = Q1 + QU.
It is easy to verify that the formulas
Q1(t)
=
exp
D1t
Œµ

Q1(0) + 1
Œµ
 t
0
exp
D1(t ‚àís)
Œµ

G1(s)ds
QU(t)
=
‚àí1
Œµ
 œÑ
t
exp
DU(t ‚àís)
Œµ

GU(s)ds
deÔ¨Åne a solution of the remainder system. The function Q1(0) is determined
explicitly by the algorithm, and QU(0) is given explicitly by the last formula
above. Finally, the functions G1 and GU are both of order O(Œµn+1), and
integrating once by parts shows that
|Q1(t)| ‚â§K0|Q1(0)| + K0
 t
0
|G1(s)|ds
and
|QU(t)| ‚â§K1
 T
t
|GU(s)|ds
for some positive constants K0 and K1. Since Q1(0) = O(Œµn+1), Q(t) =
O(Œµn+1) uniformly for 0 ‚â§t ‚â§T. Note that since Q1(0) is otherwise
arbitrary, the quasistatic problem does not necessarily have a unique
solution.

8.2. Quasistatic-State Analysis of a Linear Problem
261
The solution for R is given by the formula
R(t) = eAtR(0) +
 t
0
eA(t‚àís)BQ(s)ds = O(Œµn+1).
In this example the solution of the full problem for the remainders can be
found explicitly.
This result can be extended to the entire half-line 0 ‚â§t < ‚àûif A is a
stable matrix. In that case the upper limit in the formula for QU is replaced
by ‚àû, and the proof proceeds the same way.
8.2.2
Initial Transient Problem
We next derive an algorithm for determining Œæ‚àóso that the diÔ¨Äerences
X(œÑ, Œµ) = x ‚àíx‚àó,
Y (œÑ, Œµ) = y ‚àíy‚àó
are asymptotically equal to zero (up to order n + 1) for any œÑ > 0, where
œÑ = t/Œµ. Moreover, we hope to Ô¨Ånd that X(œÑ, Œµ) and Y (œÑ, Œµ) are smooth
functions of Œµ:
X(œÑ, Œµ)
=
X0(œÑ) + ŒµX1(œÑ) + ¬∑ ¬∑ ¬∑ + Xn(œÑ)Œµn + O(Œµn+1)
Y (œÑ, Œµ)
=
Y0(œÑ) + ŒµY1(œÑ) + ¬∑ ¬∑ ¬∑ + Yn(œÑ)Œµn + O(Œµn+1),
where the error estimates hold uniformly for 0 ‚â§t ‚â§T. The functions X
and Y are referred to as initial transients.
Since œÑ ‚Üí‚àûas Œµ ‚Üí0+, X and Y will be asymptotic to zero as Œµ ‚Üí0+
for each t > 0 if the conditions
Xj(œÑ) ‚Üí0
and
Yj(œÑ) ‚Üí0
(exponentially) as œÑ ‚Üí‚àûare met. These are sometimes called the matching
conditions for the problem, since they ensure that the QSS matches the
initial transient.
The functions X and Y satisfy the system
dX
dœÑ
=
Œµ(AX + BY ),
X(0) = Œæ ‚àíŒæ‚àó
dY
dœÑ
=
DY,
Y (0) = Œ∑ ‚àíy‚àó(0, Œµ),
where Œæ‚àó(Œµ) is still unknown. Therefore,
dX0
dœÑ
=
0,
X0(0) = Œæ ‚àíŒæ0
dY0
dœÑ
=
DY0,
Y0(0) = Œ∑ + D‚àí1g(0).
It follows that X0(‚àû) = 0 only if Œæ0 = Œæ. This shows that there is a
unique choice for Œæ0. In order that no restriction be placed on Œæ and that
Y0(‚àû) = 0, we require that the following condition holds.

262
8. Quasistatic-State Approximations
Hypothesis H2. D is a stable matrix (i.e., all of its eigenvalues have
negative real parts, so D = DS).
With this condition Y0(‚àû) = 0 for any choice of (Œæ, Œ∑). The functions Xj
and Yj solve the problems
dXj
dœÑ
=
AXj‚àí1 + BYj‚àí1,
Xj(0) = ‚àíŒæj
dYj
dœÑ
=
DYj,
Yj(0) = ‚àíyj(0)
for j = 1, 2, . . . . Therefore,
Yj(œÑ) = ‚àíexp(DœÑ)yj(0)
and
Xj(œÑ) = ‚àíŒæj ‚àíBD‚àí1(eDœÑ ‚àíI)yj‚àí1(0) + A
 œÑ
0
Xj‚àí1(s)ds.
To ensure that the matching condition on Xj is satisÔ¨Åed, we specify that
Œæj = BD‚àí1yj‚àí1(0) + A
 ‚àû
0
Xj‚àí1(s)ds.
The integral in this formula exists, since Xj‚àí1 approaches zero exponen-
tially. This can be established by a straightforward induction argument,
and so the expansion of Œæ‚àó(Œµ) is found.
Initial Transient Theorem. Suppose that condition H2 is satisÔ¨Åed.
Then there is a function Œæ‚àó(Œµ), uniquely determined to order O(Œµn+1) by
Œæj, j = 0, . . . , n, such that the solution of
dX
dœÑ
=
Œµ(AX + BY ),
X(0) = Œæ ‚àíŒæ‚àó
dY
dœÑ
=
DY,
Y (0) = Œ∑ ‚àíy‚àó(0, Œµ)
satisÔ¨Åes
X(t/Œµ, Œµ) = O(Œµn+1)
and
Y (t/Œµ, Œµ) = O(Œµn+1)
for each 0 < t ‚â§T as Œµ ‚Üí0+ (note that t = 0 is not included here).
Moreover, these functions have uniquely determined Taylor expansions up
to order n + 1, as derived in the last paragraph.

8.2. Quasistatic-State Analysis of a Linear Problem
263
Proof of the Initial Transient Theorem. Let the functions Xj, Yj be
determined from the algorithm of the last paragraph, and let
R(t, Œµ)
=
X
 t
Œµ, Œµ

‚àí
N

j=0
Xj
 t
Œµ

Œµj
Q(t, Œµ)
=
Y
 t
Œµ, Œµ

‚àí
n

j=0
Yj
 t
Œµ

Œµj.
These functions satisfy the equations
dR
dt
=
AR + BQ
R(0) = O(Œµn+1)
ŒµdQ
dt
=
DQ
Q(0) = O(Œµn+1).
Integrating these equations, we have
Q(t) = exp
Dt
Œµ

O(Œµn+1)
and
R(t)
=
eAtO(Œµn+1) +
 t
0
eA(t‚àís‚Ä≤)BeDs‚Ä≤/ŒµO(Œµn+1)ds‚Ä≤
=
O(Œµn+1).
This completes the proof of the Initial Transient Theorem.
Again, note that the result is valid for T = ‚àûif the matrix A is stable.
8.2.3
Composite Solution
The results of these calculations show that if condition H2 is satisÔ¨Åed, then
the solution of the original problem can be written as
x(t, Œµ)
=
x‚àó(t, Œµ) + X(t/Œµ, Œµ)
y(t, Œµ)
=
y‚àó(t, Œµ) + Y (t/Œµ, Œµ),
where the quasistatic state (x‚àó, y‚àó) and the initial transient (X, Y ) can be
expanded in series in Œµ that are uniquely determined up to order Œµn+1. In
particular,
x
=
x0(t) + O(Œµ)
y
=
‚àíD‚àí1g(t) + exp
Dt
Œµ

(Œ∑ + D‚àí1g(0)) + O(Œµ),
where the error estimates hold uniformly for 0 ‚â§t ‚â§T. The result is valid
for the case where T = ‚àûif the matrix A is stable.

264
8. Quasistatic-State Approximations
In summary, the quasistatic state is constructed using free constants
(Œæ‚àó(Œµ)) that are chosen at a later stage to ensure that the matching con-
ditions are met. The calculation of Œæ‚àóshows that using the initial value Œæ
of the full problem is not necessarily the correct choice for the quasistatic
state beyond the Ô¨Årst term.
8.2.4
Volterra Integral Operators with Kernels Near Œ¥
The calculation in the proof of the Initial Transient Theorem highlights an
important aspect of singular perturbations. Namely, the integrals involved
have kernels that are quite like delta functions. To see this, consider the
Volterra integral formula
g(t) = 1
Œµ
 t
0
k
t ‚àís
Œµ

f(s)ds,
where the kernel is k and Œµ is a small positive parameter. We suppose
that f and f ‚Ä≤ are smooth and bounded functions and that k(t) and K(t) =
 ‚àû
t
k(s)ds are both integrable functions. Then g can be approximated using
integration by parts:
g(t) = 1
Œµ
 t
0
k
s
Œµ

f(t ‚àís)ds = K(0)f(t) +
 t
0
K
s
Œµ

f ‚Ä≤(t ‚àís)ds,
so
g(t) = K(0)f(t) + o(1)
as Œµ ‚Üí0. Thus, we write (1/Œµ)k((t ‚àís)/Œµ) ‚àºK(0)Œ¥(t ‚àís).
8.3
Quasistatic-State Approximation for Nonlinear
Initial Value Problems
Consider the initial value problem
dx
dt
=
f(t, x, y, Œµ),
x(0) = Œæ(Œµ)
(8.4)
Œµdy
dt
=
g(t, x, y, Œµ),
y(0) = Œ∑(Œµ),
where x, f ‚ààEM, y, g ‚ààEN, and Œµ is a small positive parameter.
We consider a domain ‚Ñ¶= I √ó BR √ó BR‚Ä≤ √ó [0, Œµ0], where I = {t : t0 ‚â§
t ‚â§T ‚â§‚àû}, BR = {x in EM : |x| ‚â§R}, BR‚Ä≤ = {y in EN : |y| ‚â§R‚Ä≤}, and
T and Œµ0 are some Ô¨Åxed constants. In what follows, the balls BR and BR‚Ä≤
can be replaced by any sets that are diÔ¨Äeomorphic to them. We suppose
next that the following condition holds.

8.3. Quasistatic-State Approximation for Nonlinear Initial Value Problems
265
Hypothesis H3. f and g are C2(‚Ñ¶), and any solution of the system
(8.4) beginning in BR √ó BR‚Ä≤ remains there for t0 ‚â§t ‚â§T.
Setting Œµ = 0 in this system gives
dx
dt
=
f(t, x, y, 0),
x(0) = Œæ(0)
(8.5)
0
=
g(t, x, y, 0),
which we refer to as the reduced problem. In the following subsection we
place successively more restrictive conditions on the system and obtain as
a result more information about solutions.
8.3.1
Quasistatic Manifolds
We begin with the least restrictive result.
Hypothesis H4. Suppose there is a function y = Œ¶(t, x) such that
g(t, x, Œ¶(t, x), 0) = 0 for t0 ‚â§t ‚â§T and x in BR. Moreover, this function
is smooth, Œ¶ ‚ààC2(I √ó BR), and it has no folds, that is, we suppose that
det(Œ¶x(t, x)) Ã∏= 0 for (t, x) in I √ó BR.
We deÔ¨Åne the manifold
M = {(t, x, Œ¶(t, x)) : (t, x) in I √ó BR},
which we refer to as the quasistatic, or reduced, manifold.
Next, we suppose that the quasistatic manifold is stable:
Hypothesis H5. The system of equations
dY
dœÑ = g(Œ±, Œ≤, Y, 0)
has Y = Œ¶(Œ±, Œ≤) as an equilibrium for each (Œ±, Œ≤) in I √ó BR. We suppose
that this equilibrium is asymptotically stable uniformly in the parameters
(Œ±, Œ≤) in I √ó BR. That is, there is a comparison function a; a positive,
monotonically decreasing function d for which d(‚àû) = 0; and a posi-
tive constant Œ¥ such that if |Y (œÑ0) ‚àíŒ¶(Œ±, Œ≤)| < Œ¥, then the corresponding
solution Y (œÑ, Œ±, Œ≤) exists and satisÔ¨Åes
|Y (œÑ, Œ±, Œ≤) ‚àíŒ¶(Œ±, Œ≤)| ‚â§a(|Y (œÑ0) ‚àíŒ¶(Œ±, Œ≤)|)d(œÑ ‚àíœÑ0)
for 0 < œÑ0 ‚â§œÑ < ‚àû.
With these conditions we have the following lemma:
Lemma. If conditions H3, H4, and H5 are satisÔ¨Åed, then there is a
function W(Œ±, Œ≤, Y ) such that
1. W is twice continuously diÔ¨Äerentiable on I √ó BR √ó BR,
2. a(|y ‚àíŒ¶(Œ±, Œ≤)|) ‚â§W(Œ±, Œ≤, y) ‚â§b(|y ‚àíŒ¶(Œ±, Œ≤)|),

266
8. Quasistatic-State Approximations
3. g(Œ±, Œ≤, y, 0) ¬∑ ‚àáyW(Œ±, Œ≤, y) ‚â§‚àíc(|y ‚àíŒ¶(Œ±, Œ≤)|),
where a, b, and c are comparison functions that are independent of (Œ±, Œ≤)
in I √ó BR.
This lemma is proved in [70]. The proof is quite similar to that created
by Massera [108] to construct Liapunov functions (see Chapter 3). The
function W is a Liapunov function for the quasistatic manifold, and it
serves as the basis for proving the following theorem:
Quasistatic Manifold Theorem. Suppose that conditions H3, H4, and
H5 are satisÔ¨Åed, and let the initial condition be such that Œ∑ is in the domain
of attraction of Œ¶(t0, Œæ0) for the system in H5 with Œ± = t0 and Œ≤ = Œæ0.
Then for each small Œµ, there is a unique solution of Equations (8.4) for
t0 ‚â§t ‚â§T. Moreover, the solution satisÔ¨Åes
dist(y, M) = o(1)
as Œµ ‚Üí0+
uniformly on any interval of the form t0 ‚â§t1 ‚â§t ‚â§T, that is, one not in-
cluding the initial time t0. This result holds for any nearby (Carath¬¥eodory)
system as well.
The result is not proved here (see [70]). However, the main idea is to
use the function W to deÔ¨Åne a ‚Äústicky‚Äù layer that encloses the set M. The
derivative of W along a solution of Equations (8.4) is
ŒµdW
dt = ‚àáW ¬∑ g + Œµ‚àÇW
‚àÇt + Œµ‚àáW ¬∑ f.
The argument proceeds as it did for stability under persistent disturbances.
Since the last two terms are O(Œµ), there is a level set of W that is of diameter
o(1) (for small Œµ) and that deÔ¨Ånes an attractive and invariant layer about
M.
Example. Lorenz‚Äôs system of equations [104] comprises three equations
for the amplitudes of dominant modes in the convective Ô¨Çow of a Ô¨Çuid
in a toroidal container that is heated from below. These equations were
extracted from among all possible modes by an assumption that the system
of three amplitude equations is closed. In fact, the system is embedded in
a system of higher dimension, which we illustrate here with the following
system in E4:
dx1
dt
=
x2x3 ‚àíbx1 + Œµf1(t, x, y, Œµ)
dx2
dt
=
‚àíx1x3 + rx3 ‚àíx2 + Œµf2(t, x, y, Œµ)
dx3
dt
=
œÉ(x2 ‚àíx3) + Œµf3(t, x, y, Œµ)
dy
dt
=
Œªy ‚àíy3 + Œµg(t, x, y, Œµ),

8.3. Quasistatic-State Approximation for Nonlinear Initial Value Problems
267
where f1, f2, f3, and g are Carath¬¥eodory functions. The quasistatic mani-
fold (y = O(
‚àö
Œª) + O(Œµ)) can be constructed when Œª and Œµ are near zero.
When Œµ = 0, the system reduces to Lorenz‚Äôs system, which is known to be
chaotic for various choices of b, r, and œÉ. In this case, if Œª is near 0, then
W(y) = y2, and the previous theorem shows that the solutions remain near
the quasistatic manifold for all t > 0. Since the dynamics on the manifold
are chaotic, we cannot expect the solution of the full problem to remain near
any one particular solution of the reduced problem for Ô¨Åxed t as Œµ ‚Üí0+.
The same result can be extended to vectors y, even inÔ¨Ånite dimensional
ones, using methods derived in [51]. This completes the example.
Let (x0, y0) denote the solution of the reduced problem (8.5) with y0(t) =
Œ¶(t, x0(t)), and we have the following corollary.
Corollary. If the conditions of the previous theorem are satisÔ¨Åed, if T <
‚àû, and if the solution of the reduced problem (x0(t), y0(t)) exists for t0 ‚â§
t ‚â§T, then on any interval of the form t0 < t1 ‚â§t ‚â§T, for suÔ¨Éciently
small Œµ, the solution of the full problem (8.4), (x(t), y(t)), exists, and it
satisÔ¨Åes
x(t) = x0(t) + o(1),
y(1) = Œ¶(t, x0(t)) + o(1)
uniformly as Œµ ‚Üí0+.
The proof of this result follows from the observation that the the-
orem gets this solution near (x0(t1), y0(t1)), and the solutions depend
continuously on the data to leading order in Œµ (see [70] for details).
We next place stability conditions on motions in the quasistatic manifold.
Hypothesis H6. Suppose now that the system (8.5) dx0/dt = f(t, x0,
Œ¶(t, x0), 0) has a solution for t0 ‚â§t < ‚àû, say x‚àó(t), and it is uniformly
asymptotically stable. Furthermore, suppose that Œæ0 is in the domain of
attraction of x‚àó.
With this additional condition, we have the following theorem.
Quasistatic-State Theorem. Let conditions H3, H4, H5, and H6 be
satisÔ¨Åed. Then, for suÔ¨Éciently small values of Œµ, the solution of (8.4) exists
for t0 ‚â§t < ‚àû, and it satisÔ¨Åes
(x, y) = (x0(t), Œ¶(t, x0(t))) + o(1)
as Œµ ‚Üí0+, uniformly on any interval of the form t0 < t1 ‚â§t < ‚àû.
Moreover, this result holds for any nearby (Carath¬¥eodory) system.
Sketch of Proof. Condition H6 implies that there is a Liapunov function
V (t, x) for which
1. V (t, x‚àó(t)) = 0.

268
8. Quasistatic-State Approximations
2. There are comparison functions a1 and b1 such that
a1(|x ‚àíx‚àó|) ‚â§V (t, x) ‚â§b1(|x ‚àíx‚àó|)
for x near x‚àó(t).
3. There is a comparison function c1 such that
‚àÇV
‚àÇt + ‚àáV (t, x) ¬∑ f(t, x, Œ¶(t, x), 0) ‚â§‚àíc1(|x ‚àíx‚àó(t)|)
for 0 ‚â§t < ‚àûand x near x‚àó(t).
The derivative of V along solutions of (8.4) is
dV
dt = ‚àÇV
‚àÇt + ‚àáV (t, x) ¬∑ f(t, x, y, Œµ).
If y = Œ¶(t, x) + o(1) (as from the previous theorem), then
dV
dt ‚â§‚àíc1(|x ‚àíx‚àó(t)|) + o(1).
Proceeding as for stability under persistent disturbances in Section 3.4, we
see that there is a level set of V , having diameter o(1) as Œµ ‚Üí0+, that is
attractive and invariant. This combined with the Quasistatic Manifold The-
orem shows that there is a ‚Äústicky tube‚Äù about (x0(t), Œ¶(t, x0(t))). Details
of these proofs are given in [70].
As an example of this result we consider the system
dx
dt
=
‚àíx3 + Œµf(t, x, y, Œµ)
Œµdy
dt
=
‚àíy3 + Œµg(t, x, y, Œµ),
where f and g are Carath¬¥eodory functions. The functions V = x2 and
W = y2 are the desired Liapunov functions, and so the result applies to
this system, namely,
(x(t), y(t)) = (0, 0) + o(1)
for each t > 0.
8.3.2
Matched Asymptotic Expansions
We turn now to approximating the solutions of system (8.4). For this we
require a condition that is similar to invertibility in the implicit function
theorem.
Denote by gy(t) the Jacobian matrix
gy(t) =
 ‚àÇgi
‚àÇyj
(t, x0(t), y0(t), 0)


8.3. Quasistatic-State Approximation for Nonlinear Initial Value Problems
269
for i, j = 1, . . . , N.
Hypothesis H7. Suppose that system (8.5) has a smooth solution
(x0(t), y0(t)) and all of the eigenvalues of the matrix
gy(t)
satisfy Re Œª(t) ‚â§‚àíŒ¥ < 0 on some Ô¨Ånite interval 0 ‚â§t ‚â§T.
Next, we suppose that the data are smooth near the solution of the
reduced problem.
Hypothesis H8. The functions f and g have continuous derivatives up
to order n+2 with respect to their arguments in some neighborhood of the
points (t, x0(t), y0(t)), 0 ‚â§t ‚â§T, and for 0 ‚â§Œµ < Œµ0. Also, the initial data
(Œæ(Œµ), Œ∑(Œµ)) are smooth functions of Œµ for 0 ‚â§Œµ < Œµ0.
With these conditions we have the main theorem of this section:
Quasistatic-State Approximation Theorem. Let conditions H3, H4,
H7, and H8 be satisÔ¨Åed. Then there is a neighborhood U of (x0(0), y0(0))
such that for each small Œµ > 0 the problem (8.4) has a unique solution for
0 ‚â§t ‚â§T, provided that (Œæ, Œ∑) ‚ààU. Moreover, there is a quasistatic state
x‚àó(t, Œµ), y‚àó(t, Œµ) and an initial transient X(œÑ, Œµ), Y (œÑ, Œµ) such that
x(t, Œµ)
=
x‚àó(t, Œµ) + X
 t
Œµ, Œµ

y(t, Œµ)
=
y‚àó(t, Œµ) + Y
 t
Œµ, Œµ

for 0 ‚â§t ‚â§T and each small Œµ > 0. The quasistatic state has the form
x‚àó(t, Œµ)
=
n

j=0
xj(t)Œµj + O(Œµn+1)
y‚àó(t, Œµ)
=
n

j=0
yj(t)Œµj + O(Œµn+1),
where the error estimates hold uniformly for 0 ‚â§t ‚â§T, and the initial
transient has the form
X
 t
Œµ, Œµ

=
n

j=0
Xj
 t
Œµ

Œµj + O(Œµn+1)
Y
 t
Œµ, Œµ

=
n

j=0
Yj
 t
Œµ

Œµj + O(Œµn+1),

270
8. Quasistatic-State Approximations
where the error estimates hold uniformly for 0 ‚â§t ‚â§T as Œµ ‚Üí0. Finally,
there are positive constants K, Œ±, and Œµ‚Ä≤
0 such that
|X(œÑ, Œµ)| + |Y (œÑ, Œµ)| ‚â§K|Œ∑(Œµ) ‚àíy‚àó(0, Œµ)| exp(‚àíŒ±œÑ)
for 0 ‚â§œÑ ‚â§T/Œµ and 0 < Œµ ‚â§Œµ‚Ä≤
0.
The proof of this result is presented in [71], and it is not presented
here. However, the Ô¨Årst steps in the proof require constructing Taylor‚Äôs
expansions listed in the theorem. Once these have been derived, the proof
is completed by obtaining integral equations for the remainders that are
the diÔ¨Äerences between the solution of the full problem and the proposed
approximations, and then showing that they have unique solutions that are
of order O(Œµn+1). This proof was carried out in detail for linear systems in
Section 8.2.
Vasil‚Äôeva and O‚ÄôMalley [120, 137] developed a method of matched asymp-
totic expansions to solve problem (8.4), and it is from their work that the
present approach grew. Their algorithm is described in [137, 138]. Other re-
Ô¨Ånements and extensions of their method were carried out by their students
and colleagues (see [71]). Roughly, their approach entails the construction
of three expansions, an outer one, an inner one, and a third one used to
match these two. The inner and matching expansions are combined here in
the initial transient of the QSS method. The matching approach has proved
to be quite useful in studying a variety of problems, and [120, 26] describe
many features of the method and its origins and uses in Ô¨Çuid mechanics
and elasticity.
8.3.3
Construction of QSSA
The coeÔ¨Écients in the expansion of the quasistatic state are found by
solving problem (8.5) for x0 and y0, and then for j = 1, . . . , n, by solving
dxj
dt
=
fxxj + fyyj + pj(t),
xj(0) = Œæ‚àó
j
dyj‚àí1
dt
=
gxxj + gyyj + qj(t),
where for each j the functions pj and qj depend on the coeÔ¨Écients xk, yk,
for k = 0, . . . , j ‚àí1. The initial condition for xj (i.e., Œæ‚àó
j ) is determined
at a later step by the matching conditions. These n problems have unique
solutions.
The expansion of the initial transient is determined by solving for j = 0
dX0
dœÑ
=
0,
X0(0) = Œæ0 ‚àíŒæ‚àó
0
dY0
dœÑ
=
g(0, X0, Y0, 0),
Y0(0) = Œ∑0 ‚àíy‚àó(0, 0),

8.3. Quasistatic-State Approximation for Nonlinear Initial Value Problems
271
and for j = 1, . . . , n by solving
dXj
dœÑ
=
Pj(œÑ),
Xj(0) = Œæj ‚àíŒæ‚àó
j
dYj
dœÑ
=
gxXj + gyYj + Qj(œÑ),
Yj(0) = Œ∑j ‚àíy‚àó
j (0).
The functions Pj and Qj are determined by the previous coeÔ¨Écients Xk, Yk
for k = 1, . . . , j ‚àí1.
The values of Œæ‚àó
j are determined at each step to ensure that the matching
conditions Xj(‚àû) = 0, Yj(‚àû) = 0 are satisÔ¨Åed. These are given by the
formulas
Œæ‚àó
j =
 ‚àû
0
Pj(œÑ ‚Ä≤)dœÑ ‚Ä≤ + Œæj.
Each of these n problems has a unique solution. In this way, the expansions
in the theorem are found.
8.3.4
The Case T = ‚àû
The results of the QSSA Theorem are valid over the entire half-line 0 ‚â§
t < ‚àûif the reduced problem satisÔ¨Åes additional stability conditions. We
now assume the following.
Hypothesis H9. The solution of the reduced problem (x0, y0) exists
and remains bounded for all 0 ‚â§t < ‚àû, and the linear problem
dx
dt = [fx(t) ‚àífy(t)g‚àí1
y (t)gx(t)]x
has x = 0 as an exponentially asymptotically stable solution.
If condition H9 is added to the hypotheses of the QSSA Theorem with
T = ‚àû, then the results of that theorem are valid for 0 ‚â§t < ‚àû. In
particular, the quasistatic state and the initial transient can be expanded
in powers of Œµ as indicated there, and the error estimates hold as Œµ ‚Üí0+
uniformly for 0 ‚â§t < ‚àû. This extension of the QSSA Theorem is proved
in [71].
Example of the Michaelis‚ÄìMenten Approximation. A simple en-
zyme reaction involves an enzyme E, a substrate S, a complex C, and
a product P. Such reactions were studied by Michaelis and Menton and
by Haldane and Briggs [14]. Their work is widely used in biochemistry.
Schematically, the reaction is
E + S ‚ÜîC ‚ÜîE + P.
The forward reactions dominate the backward ones. After some prelimi-
nary scaling to remove dimensions from the problem, this reaction can be

272
8. Quasistatic-State Approximations
described by a system of diÔ¨Äerential equations for the (normalized) sub-
strate concentration (x) and the (normalized) complex concentration (y)
as
dx
dt
=
‚àíx + (x + k)y,
x(0) = 1
Œµdy
dt
=
x ‚àí(x + K)y,
y(0) = 0,
where Œµ measures a typical ratio of enzyme to substrate concentration
(O(10‚àí5)) and k and K (k < K) denote normalized (nondimensional)
rate constants.
The QSS method shows that the reduced problem is
dx0
dt = (k ‚àíK)x0
(K + x0)
with the initial condition x0(0) = 1, and
y0 = x0/(K + x0).
The rapid transient is determined from the equation
dY0
dœÑ = ‚àí(1 + K)Y0,
Y0(0) = ‚àí
1
1 + K .
Moreover, since the equation for x0 is exponentially stable, we conclude
that the approximation
x
=
x0(t) + O(Œµ)
y
=
y0(t) + Y0(t/Œµ) + O(Œµ)
is valid uniformly for 0 ‚â§t < ‚àû.
This approximation is known as the Michaelis‚ÄìMenten approximation.
The equation
dx0
dt = (k ‚àíK)x0
K + x0
gives the quasistatic approximation to this reaction, and while the rate
equations are based on the law of mass action, this equation seems to
involve a more complicated rate law. In biochemistry Vmax = K ‚àík is
called the uptake velocity or maximum reaction rate, and K is the reac-
tion‚Äôs saturation constant. These two constants are used by biochemists to
characterize enzyme action.

8.4. Singular Perturbations of Oscillations
273
 y
 x2
 x1
    = 0
 g
Figure 8.4. Cusp.
8.4
Singular Perturbations of Oscillations
Let us now consider the time-invariant problem
dx
dt
=
f(x, y, Œµ)
Œµdy
dt
=
g(x, y, Œµ).
A particularly interesting problem arises when the reduced problem
dx0
dt
=
f(x0, y0, 0)
0
=
g(x0, y0, 0)
has a stable oscillation. Two interesting questions in this case are whether
the full problem has a stable oscillation near this reduced one, and whether
the period of this solution is near the reduced oscillation‚Äôs period.
It is useful to keep an example in mind, say
dx1
dt
=
f1(x, y, Œµ)
dx2
dt
=
f2(x, y, Œµ)
Œµdy
dt
=
g(x, y, Œµ),
in E3, where g has the form of a cusp:
g(x, y, Œµ) = ‚àíy3 + x1y ‚àíx2 + ŒµG(x, y, Œµ).
We see that when Œµ = 0, the solutions of the reduced problem lie on a cusp
surface, as shown in Figure 8.4.

274
8. Quasistatic-State Approximations
 y
 x2
 x1
     =  0
 g 
 Stable oscillation for   = 0
 Reduced oscillation
Œµ /
Figure 8.5. Quasistatic oscillation.
Two interesting cases are described here. In the Ô¨Årst, we suppose that
there is a stable oscillation of the reduced problem that does not hit one of
the bifurcation curves (i.e., folds) of the cusp surface. Such an oscillation
is referred to as a quasistatic oscillation, and a typical case is depicted in
Figure 8.5. In the second, we consider an oscillation that is draped over a
fold in the surface, similar to the one shown in Figure 8.6.
8.4.1
Quasistatic Oscillations
There is an interesting class of oscillation problems in which a periodic
solution can be constructed as a quasistatic solution. In the cusp example,
these are solutions that lie near a reduced oscillation that is on one branch
of the cusp surface, as shown in Figure 8.5. Because of this, computation
of approximation solutions is quite transparent.
Consider the system of equations
dx
dt
=
f(x, y, Œµ)
Œµdy
dt
=
g(x, y, Œµ),
where x, f ‚ààEM and y, g ‚ààEN. We suppose that f and g are smooth func-
tions of their variables in some region of EM+N, and we restrict attention
to that region.
The results of this section are based on the reduced problem
dx
dt
=
f(x, y, 0)
0
=
g(x, y, 0)

8.4. Singular Perturbations of Oscillations
275
 y
 x2
 x1
    = 0
 g
Figure 8.6. A nearly discontinuous oscillation.
having a periodic solution, say (x0, y0), with period T0. Conditions are
found that ensure the existence of a nearby periodic solution of the full
problem and that it can be constructed. The construction involves only a
quasistatic solution of the problem, and so it is quite straightforward.
As we have seen with other time-invariant problems, we must expect the
perturbed problem to have a periodic solution with period slightly diÔ¨Äerent
from T0. Therefore, we introduce an unknown time scaling
t = T(Œµ)s
into the problem with the result that
dx
ds
=
T(Œµ)f(x, y, Œµ)
Œµdy
ds
=
T(Œµ)g(x, y, Œµ).
If the quasistatic solution is periodic, we can construct the oscillation in
the form
x
=
x0(s) + x1(s)Œµ + x2(s)Œµ2 + ¬∑ ¬∑ ¬∑
y
=
y0(s) + y1(s)Œµ + y2(s)Œµ2 + ¬∑ ¬∑ ¬∑ .
In addition to Ô¨Ånding the coeÔ¨Écients in these expansions, we must also show
how the t-period T(Œµ) is to be chosen. The equations for the coeÔ¨Écients
are
dx0
ds
=
T0f(x0, y0, 0)
0
=
T0g(x0, y0, 0),

276
8. Quasistatic-State Approximations
all of which are known, and for j = 1, 2, . . . ,
dxj
ds
=
Tj
dx0
ds + T0(fxxj + fyyj) + pj
dyj‚àí1
ds
=
T0(gxxj + gyyj) + qj,
where the functions pj and qj depend on xk, yk, and Tk for k = 0, . . . , j‚àí1.
Here the functions gx, and so on, denote Jacobian matrices of derivatives,
(‚àÇgi/‚àÇxk)(x0, y0, 0), and so forth.
Hypothesis H10. Suppose the reduced problem has a periodic solution,
x0, y0 = Œ¶(x0(t)), say with period T0, and the matrix gy(x, y, 0) is stable
for (x, y) near (x0, y0).
With this assumption we can solve the second equation for yj, and so
the problem reduces to one for xj:
dxj
ds = Tj
dx0
ds + A(s)xj + Pj(s),
where Pj is a one-periodic function of s that depends on terms with index
k, k < j, and the matrix A is periodic of period 1 given by the formula
A(s) = T0(fx ‚àífyg‚àí1
y gx).
Note that dx0/ds is a periodic solution of the linear problem
du
ds = A(s)u.
Therefore, there is a deÔ¨Åciency in the number of independent state variables
that is made up for by the presence of the period‚Äôs coeÔ¨Écients, similar to
the situation in studying stability and regular perturbations of periodic
solutions to autonomous systems (see Sections 3.5.4 and 6.1.2).
Hypothesis H11. Suppose the linear problem
du
ds = A(s)u
has exactly one characteristic exponent of the form 2œÄi ÀÜN for some integer
ÀÜN.
With this, as before, there is a unique choice of Tj such that each of
the problems for x1, . . . , xn has a periodic solution of period T0. There
remains a free constant in xj that corresponds to the initial phase, but
that can be Ô¨Åxed by taking the Ô¨Årst component of xj to be zero. With such
a condition, xj is uniquely determined. This calculation is justiÔ¨Åed by the
following theorem.
Friedrichs‚ÄìWasow Theorem. Suppose that conditions H10 and H11
are satisÔ¨Åed and that the functions f and g are smooth near (x0, y0). Then,

8.4. Singular Perturbations of Oscillations
277
for suÔ¨Éciently small Œµ > 0 there is a unique periodic solution of the full
problem with period T(Œµ) which lies near (x0, y0). Moreover, T(Œµ) = T0 +
O(Œµ). This solution is orbitally asymptotically stable if (x0, y0) is also.
The proof of this result follows directly from the construction of the peri-
odic quasistatic state. Again, remainders are deÔ¨Åned and integral equations
are derived for them. The proof is presented in [139].
Example of the Friedrichs‚ÄìWasow Theorem. The example stud-
ied in this section uses some results from our earlier work on averaging.
Consider the system in E3
du
dt
=
‚àíŒªv + ¬µ

u ‚àíu3
3

+ Œµf(u, v, y, Œµ, ¬µ)
dv
dt
=
Œªu + ŒµF(u, v, y, Œµ, ¬µ)
Œµdy
dt
=
‚àíDy + g(u, v) + ŒµG(u, v, y),
where u and v are scalars, y, g, G ‚ààEN, Œª is a Ô¨Åxed frequency, and ¬µ is
a Ô¨Åxed small (positive) number in this system. We will consider the case
where
0 < Œµ ‚â™¬µ ‚â™1.
Setting Œµ = 0 in this system gives
du0
dt
=
‚àíŒªv0 + ¬µ

u0 ‚àíu3
0
3

dv0
dt
=
Œªu0
Dy0
=
g(u0, v0).
We see that the Ô¨Årst two equations reduce to van der Pol‚Äôs equation, which
we studied in Sections 2.1.3 and 7.4.2. In particular,
u0(t)
=
r(¬µt) cos Œªt + O(¬µ)
v0(t)
=
r(¬µt) sin Œªt + O(¬µ),
where r(‚àû) = 2. The third equation is easily solved if D is invertible. In
fact, let us suppose that ‚àíD is a stable matrix (i.e., all of its eigenvalues
have negative real parts). Then
y0(t) = D‚àí1g(r(¬µt) cos Œªt + O(¬µ), r(¬µt) sin Œªt + O(¬µ)).
This reduced solution is orbitally asymptotically stable, since solutions ap-
proach the quasistatic manifold at an exponential rate determined by the
eigenvalues of ‚àíD and solutions in the manifold approach van der Pol‚Äôs
oscillation.

278
8. Quasistatic-State Approximations
A periodic quasistatic state can be constructed for this system, but rather
than proceeding with this result, we simply apply the Friedrichs‚ÄìWasow
Theorem and obtain an easy approximation to this oscillation by using the
Ô¨Årst terms of the quasistatic state:
u
=
2 cos(Œªt) + O(¬µ) + O(Œµ)
v
=
2 sin(Œªt) + O(¬µ) + O(Œµ)
y
=
D‚àí1g(2 cos(Œªt), 2 sin(Œªt)) + O(¬µ) + O(Œµ).
If we wish to include the rapid transients in this approximation, we use the
quasistatic state approximation and write
u(t, Œµ)
=
r(¬µt) cos(Œªt) + O(¬µ) + O(Œµ)
v(t, Œµ)
=
r(¬µt) sin(Œªt) + O(¬µ) + O(Œµ)
y(t, Œµ)
=
D‚àí1g(r(¬µt) cos(Œªt), r(¬µt) sin(Œªt))
+ exp(‚àíDt/Œµ)[y(0) ‚àíy0(0)] + O(¬µ) + O(Œµ).
In this way we quickly derive an approximation to the solution of a quite
complicated system of equations.
It is interesting to pursue a slightly diÔ¨Äerent approach to the problem‚Äî
one that enables us to calculate the period of the oscillation. Let us return
to the original problem
du
dt
=
‚àíŒªv + ¬µ

u ‚àíu3
3

+ Œµf(u, v, y, Œµ, ¬µ)
dv
dt
=
Œªu + ŒµF(u, v, y, Œµ, ¬µ)
Œµdy
dt
=
‚àíDy + g(u, v) + ŒµG(u, v, y),
and let us introduce polar coordinates in the Ô¨Årst two equations: Let
‚àö
3u
=
œÅ cos œÜ
‚àö
3v
=
œÅ sin œÜ.
This results in the new system
dœÅ
dt
=
¬µœÅ cos2 œÜ(1 ‚àíœÅ2 cos2 œÜ) + O(Œµ)
dœÜ
dt
=
Œª ‚àí¬µ sin œÜ cos œÜ(1 ‚àíœÅ2 cos2 œÜ) + O(Œµ)
Œµdy
dt
=
‚àíDy + Àúg(œÅ cos œÜ, œÅ sin œÜ) + Œµ ÀúG(œÅ cos œÜ, œÅ sin œÜ, y).
Since œÜ is a timelike variable, we can write
dœÅ
dœÜ
=
¬µœÅ cos2 œÜ1 ‚àíœÅ2 cos2 œÜ
Œª
+ O(¬µ2) + O(Œµ)
Œµ dy
dœÜ
=
‚àíDy + Àúg(œÅ cos œÜ, œÅ sin œÜ)
Œª
+ O(¬µ2) + O(Œµ).

8.4. Singular Perturbations of Oscillations
279
The QSS Theorem can be applied directly to this system to construct its
quasistatic state. Once this is done, we return to compute the period of the
quasistatic state by solving the equation
dœÜ
Œª ‚àí¬µ sin œÜ cos œÜ(1 ‚àíœÅ2 cos2 œÜ) + O(Œµ) = dt.
In particular, we have that
dt =
dœÜ
Œª + O(¬µ, Œµ),
and so
T(Œµ, ¬µ) = 2œÄ
Œª + O(¬µ, Œµ).
The higher order corrections to the period can be calculated directly by
using the corresponding corrections in the integral deÔ¨Åning T.
This last calculation clears up the confusion about having one too few
state variables that often results. When a time-invariant system is converted
to phase-amplitude coordinates, the natural time scale for solutions is the
new phase variable, not t, and the period T results from converting from the
period in the phase variable, for example 2œÄ in the case of polar coordinates,
to 2œÄ/Œª in t.
8.4.2
Nearly Discontinuous Oscillations
Let us now brieÔ¨Çy consider the case where the system
dx
dt
=
f(x, y, 0)
0
=
g(x, y, 0)
has a periodic solution along which condition H1 is not satisÔ¨Åed.
SpeciÔ¨Åcally, we suppose that the following assumption is satisÔ¨Åed.
Hypothesis H12. The reduced problem has a discontinuous solution of
period T0, and det(gy(x0(t), y0(t), 0)) Ã∏= 0 for all but a Ô¨Ånite set of t values.
We suppose that gy(t, x0(t), y0(t), 0) is stable away from these points.
The problem now is similar to that depicted in Figure 8.6, and it is
signiÔ¨Åcantly more diÔ¨Écult than the preceding case.
Oscillations of this kind were studied by van der Pol, Cartwright, and Lit-
tlewood [17], Levinson [100], Pontryagin [123], Mishchenko [113], Kevorkian
and Cole [90], Stoker [132], and Grasman [53], among others. They are
often called relaxation oscillations. Unfortunately, there is no result com-
parable to the Friedrichs‚ÄìWasow Theorem to ensure the existence of stable
oscillations in this case, except for special cases including the ones cited
above.

280
8. Quasistatic-State Approximations
The t values at which gy vanishes are called turning points, and there is
an extensive theory for constructing solutions near them (see [26, 139, 123,
113]). Rather than pursuing a vague general discussion of these problems,
we turn to a speciÔ¨Åc, well-studied example.
van der Pol‚Äôs equation has been studied extensively, so it provides a useful
guide to solving other nearly discontinuous oscillation problems. Consider
the equation
Œµd2x
dt2 + (x2 ‚àí1)dx
dt + x = 0,
where Œµ is a small positive parameter. It is useful to rewrite this equation
as a Ô¨Årst-order system of equations by integrating it once. The result is
Œµdx
dt
=
x ‚àíx3
3 ‚àíy
dy
dt
=
x.
Four questions about this equation that have been answered are as
follows:
1. Does this equation have a periodic solution for small values of Œµ?
2. If so, what is its period?
3. How can such an oscillation be constructed?
4. What is the response of this oscillator to external periodic forcing?
The Ô¨Årst question is answered by our work in Section 3.5 on Lienard‚Äôs
equation. There is a unique periodic solution to this equation for any choice
of the parameter Œµ, and it is globally asymptotically stable.
The phase portrait of this solution is shown in Figure 8.7, where Œµ ‚â™1.
The isocline dx/dt = 0 has been drawn in this portrait for reference. For
small Œµ the oscillation remains quite close to the cubic x isocline except at
the jump (or turning) points. The jumps are traversed quite rapidly, in a
time of order Œµ (double arrow).
The period can be approximated by observing that most of the time is
spent near the cubic curve. This is described by the reduced problem for
this system,
dx0
dt =
x0
(1 ‚àíx2
0),
and the part of this solution lying near half of the oscillation occurs for 1 <
x0 < 2. Therefore, the transit time of this branch is given (approximately)
by
T0 = 2
 1
2
1 ‚àíx2
x
dx = 3 ‚àí2 ln 2.

8.5. Boundary Value Problems
281
 x
 y
 dx/dt = 0
Figure 8.7. Depiction of van der Pol‚Äôs relaxation oscillation, Œµ ‚â™1. This
shrink-wraps onto the isocline as Œµ ‚Üí0+.
It follows that the period is given by
T ‚âà3 ‚àí2 ln 2 + o(1)
as Œµ ‚Üí0. In fact, further work (which is not presented here, see [139])
shows that
T = 3 ‚àí2 ln 2 + 7.014Œµ2/3 ‚àí0.167Œµ ln Œµ ‚àí1.325Œµ + O(ln Œµ/Œµ).
The third question is more diÔ¨Écult to answer, and it is not addressed
here, but the details of the construction can be found in [26]. Finally, the
response of this oscillator to external periodic forcing can be chaotic. The
computer simulation in Section 6.3.2 describes the responses. This was
derived in [38], where further references can be found (see also [17, 100, 53]).
8.5
Boundary Value Problems
Singularly perturbed boundary value problems arise in many important
applications. As we have seen, Ô¨Ånding periodic solutions entails solving a
problem with boundary conditions (e.g., x(0) = x(T)). Models for spatial
structure on whose boundaries physical parameters are set, such as tem-
perature, pressure, Ô¨Çow, electrical current, and so on, and many control
problems are boundary value problems (see, e.g., [120]). An understand-
ing of the role played by Œµ in these problems is helped by understanding
the geometric structure of solutions. Rather than developing a complete
boundary-layer theory here, we study an interesting example that gives

282
8. Quasistatic-State Approximations
the Ô¨Çavor of how the QSS analysis can be used to solve boundary value
problems.
A Josephson junction is a cryogenic device consisting of two supercon-
ductors separated by a thin gap [98]. The electrodynamics of this device
are described in terms of a quantum mechanical wave function that is
quite similar to the phase variable used to describe VCOs. A jump occurs
in the wave function across the gap, and it is denoted by u(x, t), where
x, 0 ‚â§x ‚â§1, describes the location along the gap and t is time. The
sine‚ÄìGordon equation is satisÔ¨Åed by u:
‚àÇ2u
‚àÇt2 + œÉ ‚àÇu
‚àÇt ‚àíŒµ2 ‚àÇ2u
‚àÇx2 + sin u = 0
for 0 ‚â§x ‚â§1 and t ‚â•0.
A magnetic Ô¨Åeld of strength proportional to H is applied at both ends,
and the device is driven by a current I applied at the right end. These
conditions are described by the boundary conditions
‚àÇu
‚àÇx(0, t) = H,
‚àÇu
‚àÇx(1, t) = H + I.
The static states of this conÔ¨Åguration are found by solving the problem
Œµ2 d2u
dx2 ‚àísin u = 0
with the boundary conditions
du
dx(0) = H,
du
dx(1) = H + I.
We consider here only the case where Œµ ‚â™1.
We rewrite this problem as a Ô¨Årst-order system of equations
Œµdu
dx
=
v
Œµdv
dx
=
sin u,
where now v(0) = ŒµH and v(1) = Œµ(H + I). We must construct a solution
of this system that lies on the line v = ŒµH at x = 0 and meets the line
v = Œµ(H + I) when x = 1.
A candidate for a quasistatic solution is u = 0, v = 0. We test this by
trying to construct correcting (transients) at both endpoints (x = 0 and
x = 1) that connect the boundary conditions to this quasistatic state. We
Ô¨Årst seek a left boundary-layer correction u = ŒµU(x/Œµ), v = ŒµV (x/Œµ) such
that
V (0) = H
and
(U(Œæ), V (Œæ)) ‚Üí(0, 0)
as Œæ ‚Üí‚àû.

8.5. Boundary Value Problems
283
 v
 u
 v=  H
Œµ
Starting
Point
 v
 u
 Terminus
 v=  (H+I)
Œµ
QSS
   = 0
   = 1
x
x
Figure 8.8. Geometry of solutions to the boundary value problem.
The problem for (U, V ) is
dU
dŒæ
=
V
dV
dŒæ
=
1
Œµ sin ŒµU = U + O(Œµ).
Figure 8.8 shows the phase portrait of solutions.
U and V can be constructed in power series in Œµ, U = U0 + ŒµU1, and so
on. We see that there is a unique solution for (U0, V0) that satisÔ¨Åes these
two conditions: It is
U0(Œæ) = ‚àíHe‚àíŒæ,
V0(Œæ) = He‚àíŒæ.
This starts at the intersection of the stable manifold for (0, 0) and the
constraint V = H.
Next, we construct the right boundary correction. Let Œ∑ = (1‚àíx)/Œµ. Then
Œ∑ = 0 corresponds to x = 1, and Œ∑ ‚Üí+‚àûcorresponds to (1 ‚àíx)/Œµ ‚Üí‚àû,
so Œ∑ represents a fast scale that moves backward from x = 1. The problem
for u = ŒµU(Œ∑) and v = ŒµV(Œ∑) is
dU
dŒ∑ = ‚àíV
dV
dŒ∑ = ‚àí1
Œµ sin ŒµU = ‚àíU + O(Œµ)
V(0) = H + I
and
(U(Œ∑), V(Œ∑)) ‚Üí(0, 0)
as Œ∑ ‚Üí‚àû.

284
8. Quasistatic-State Approximations
This can be constructed in a power series in Œµ. As before, we see that there
is a unique solution for U0(Œ∑), V0(Œ∑):
U0(Œ∑) = (H + I)e‚àíŒ∑,
V0(Œ∑) = (H + I)e‚àíŒ∑.
Combining these results, we might expect that an approximate solution of
the problem is
u
=
‚àíHe‚àíx/Œµ + (H + I)e(x‚àí1)/Œµ + O(Œµ)
v
=
Œµ(He‚àíx/Œµ + (H + I)e(x‚àí1)/Œµ + O(Œµ)),
which is the sum of a left boundary-layer correction, a right boundary-layer
correction, and a quasistatic state. There remains an important question:
Is there a solution to this boundary value problem that lies near (within
O(Œµ)) the approximation that we constructed? The answer lies in a detailed
analysis of trajectories of the equation. If Œµ is so small that Œµ(H + I) < 1,
then there is such a solution. The proof is constructed as in other problems:
An integral equation is derived for the remainder, that is, the diÔ¨Äerence
between (u, v) and the approximation, and that equation is shown to have
a solution (see [120]).
There are many kinds of boundary value problems involving singular
perturbations that must be studied. The one described here illustrates the
basic geometry involved in many of these problems. However, a survey,
such as [94] should be consulted to get a better feeling for the diversity of
boundary value problems that have been studied.
8.6
Nonlinear Stability Analysis near Bifurcations
The singular perturbation methods described in this chapter are closely
related to stability results, as we have seen repeatedly. Because of this, it is
not surprising that a QSSA can be used to establish stability properties of
various problems. This has been particularly important in problems from
Ô¨Çuid mechanics [51].
8.6.1
Bifurcating Static States
The QSSA method is particularly useful in cases where changing parameter
values results in the appearance of a new static state through bifurcation.
Consider the system
dz
dt = F(z, Œª),
where z, F ‚ààEN and Œª is a real parameter. Suppose that there is a static
state, say z = 0, that loses its stability at some value, say Œª = Œª0. In
particular, assume the following condition.

8.6. Nonlinear Stability Analysis near Bifurcations
285
Hypothesis H13. F(0, Œª) = 0 for all Œª near Œª = Œª0. The Jacobian
matrix Fz(0, Œª) is assumed to be stable for Œª < Œª0, it has a single eigenvalue
equal to zero for Œª = Œª0, and it has a single eigenvalue with positive real
part for Œª > Œª0. All the other eigenvalues are assumed to have negative
real parts for Œª near Œª0.
With assumption H13 we can determine whether a new static state
appears for the problem as Œª increases through Œª0
by using the
Liapunov‚ÄìSchmidt method described in Chapter 4.
Let œÜ denote the null eigenvector of Fz(0, Œª0) and write
z = cœÜ + w,
where w ¬∑ œÜ = 0. The static state problem
F(z, Œª) = 0
becomes
0
=
PF(cœÜ + w, Œª)
0
=
Bw + QF(cœÜ + w, Œª),
where P is the projection of EN onto œÜ and Q is the complementary pro-
jection. The matrix B is a stable matrix found by projecting the Jacobian
matrix Fz(0, 0) onto the complement of œÜ.
As shown in Chapter 4, there is a unique solution for w, say w = w‚àó(c, Œª),
for c near zero and Œª near Œª0, and substituting this into the Ô¨Årst equation
gives the bifurcation equation
0 = PF(cœÜ + w‚àó(c, Œª), Œª).
If this equation has nontrivial small solutions for c as functions of Œª ‚àíŒª0,
then new static states appear through a simple bifurcation.
A general theory for using the QSSA to test the stability of these
new static states is presented in [51]. We consider here an example that
illustrates typical conditions when the method is useful.
Example of Landau‚Äôs Equation for a Canonical Bifurcation
Problem. The problem described in Section 4.3.1 is amenable to nonlinear
stability analysis. Consider the system of equations
dx
dt
=
Œªx ‚àíaxy + h.o.t.
dy
dt
=
‚àíby + dx2 + h.o.t.
Here, a, b, and d are Ô¨Åxed positive numbers, and Œª is a parameter. We saw
that as Œª increases through zero (Œª0 = 0 here), a pair of new static states

286
8. Quasistatic-State Approximations
appears given by
(x, y) =

¬±

bŒª
ad, Œª
a

+ h.o.t.
The stability of these new states is determined in Section 4.3.1 by linearizing
the problem about them and Ô¨Ånding that these linear problems are stable.
The Linear Stability Theorem then applies to show that these are stable
solutions of the nonlinear problem.
One deÔ¨Åciency in using linear stability methods is that they are restricted
to a neighborhood of the static state, possibly to an extent where they do
not accurately describe the domain of attraction of the state. The following
approach based on the QSSA can correct this.
Newton‚Äôs polygon analysis in Section 4.1 indicates the appropriate scal-
ing to use in this problem. For Œª > 0, we set Œµ = Œª, x = ‚àöŒµu, y = Œµv, and
s = Œµt. With these changes of variables, the problem becomes
du
ds
=
u ‚àíauv + h.o.t.
Œµdv
ds
=
‚àíbv + du2 + h.o.t.
Given any initial data (u(0), v(0)), we can determine the behavior of so-
lutions of this system for Œµ near zero by using the QSSA Theorem. The
reduced problem is
du0
ds
=
u0 ‚àíau0v0
0
=
‚àíbv0 + du2
0.
There is a unique solution for v0, and substituting this into the Ô¨Årst
equation gives
du0
ds = u0

1 ‚àíad
b u2
0

,
u0(0) = u(0).
The solution of this problem exists for all 0 ‚â§s < ‚àû, and it approaches
sgn(u(0))

b
ad
as s ‚Üí‚àûat an exponential rate.
The initial transient is determined by the equation
dY0
ds = ‚àíbY0,
Y0(0) = v(0) ‚àíd
b u(0)2.
Therefore, condition H5 of the QSSA Theorem is satisÔ¨Åed, and we conclude
that
u
=
u0(s) + O(Œµ)
v
=
v0(s) + Y0(s/Œµ) + O(Œµ),

8.6. Nonlinear Stability Analysis near Bifurcations
287
where the error estimate holds uniformly for 0 ‚â§s ‚â§‚àû.
Returning to the original question, we can now state that if x(0) = O(‚àöŒµ)
and y(0) = O(Œµ), say x(0) = ‚àöŒµŒæ and y(0) = ŒµŒ∑ where Œæ and Œ∑ are Ô¨Åxed
constants, then
x(t) ‚Üísgn(Œæ)

bŒµ
ad + O(Œµ)
as t ‚Üí‚àû.
The essential diÔ¨Äerence between the nonlinear stability analysis and
the linear stability analysis is that the stability of the quasistatic state
is determined by solving a nonlinear problem. This gives a more realis-
tic description of the domain of attraction of the bifurcated state. Here
there is no further restriction of (x(0), y(0)) other than x(0) = O(‚àöŒµ) and
y(0) = O(Œµ), so the nonlinear stability analysis establishes that the un-
stable static state is in the boundary of the domain of attraction of the
new static state. The equation for u0 is referred to as Landau‚Äôs equation in
applications to Ô¨Çuid dynamics [19]
8.6.2
Nonlinear Stability Analysis of Nonlinear Oscillations
The Hopf theory of bifurcations that was discussed brieÔ¨Çy in Chapter 4
can be studied for stability using the methods of the preceding section. In
fact, consider the problem
dx
dt = f(x, Œª),
which we suppose has a static state, say x = œÜ(Œª). Suppose that for Œª < Œª0,
all of the eigenvalues of the Jacobian matrix
fx(œÜ(Œª), Œª)
lie in the left half of the complex plane, but that there are two eigenvalues,
say œÅ(Œª)¬±iœâ(Œª), for which œÅ(Œª0) = 0, œÅ‚Ä≤(Œª0) > 0, and œâ(Œª0) > 0. The other
eigenvalues are assumed to remain in the left half-plane for Œª near Œª0. In this
situation, which was described in Sections 4.3 and 6.1.4, a Hopf bifurcation
might occur from the static state x = œÜ(Œª) resulting in the appearance of
a new oscillation. The stability analysis of this new oscillation can proceed
in the following steps.
1. Project the problem onto the two modes carrying the purely imag-
inary eigenvalues, and their complement, all of which are damped
modes.
2. Determine the manifold of quasiequilibrium values for the damped
modes. Substitute this into the equations for the two oscillatory
modes, which reduces it to a system for two variables.

288
8. Quasistatic-State Approximations
3. Since the problem carried by the oscillatory modes is described by
a perturbation of a harmonic oscillator, introduce polar coordinates
into these equations.
4. Using the phase variable of this system as a timelike variable, reduce
the system to a single scalar equation.
5. Apply the Mean Stable Averaging Theorem to this equation to derive
the analogue of Landau‚Äôs equation.
For example, consider the problem
x‚Ä≤‚Ä≤ + Œµx‚Ä≤ + œâ2x
=
Œµf(x, y, x‚Ä≤, y‚Ä≤, Œµ)
y‚Ä≤‚Ä≤ + 2ry‚Ä≤ + ¬µ2y
=
Œµg(x, y, x‚Ä≤, y‚Ä≤, Œµ),
where f and g are smooth functions, and r, œâ, and ¬µ are Ô¨Åxed positive
constants. For Œµ = 0 there results a linear problem whose spectrum consists
of the four values
¬±iœâ,
‚àír ¬±

r2 ‚àí¬µ2.
Obviously, the y components will equilibrate on a fast time scale (relative
to Œµt), so we set y‚Ä≤ = y‚Ä≤‚Ä≤ = 0. The result is that the quasistatic manifold is
deÔ¨Åned by
y0 = 0
and so on. Thus, to leading order in Œµ, we have
x‚Ä≤‚Ä≤ + œâ2x = Œµf(x, 0, x‚Ä≤, 0, 0) ‚àíŒµx‚Ä≤.
Introducing polar coordinates into this system using the formula
dx
dt + iœâx = ReiŒ∏
gives
dŒ∏
dt = œâ ‚àíŒµ sin Œ∏ f ‚àíR cos Œ∏
R
,
where f = f[R sin(Œ∏)/œâ, 0, R cos(Œ∏), 0, 0]. Using Œ∏ as a timelike variable, we
have
dR
dŒ∏ =
Œµ cos Œ∏(f ‚àíR cos Œ∏)
œâ ‚àíŒµ sin Œ∏(f/R ‚àícos Œ∏) = Œµ
œâ cos Œ∏(f ‚àíR cos Œ∏) + O(Œµ2).
Averaging this equation gives Landau‚Äôs equation for the problem; namely,
dR
dŒ∏ = ‚àíŒµ
œâ
 1
2œÄ
 2œÄ
0
cos T f
R
œâ sin T, 0, R cos T, 0, 0

dT ‚àíR
2

.
Static states of this equation correspond to periodic solutions of the original
system, and if such a static state is asymptotically stable for the aver-
aged equation, then the oscillation is orbitally asymptotically stable (i.e.,
asymptotically stable in amplitudes).

8.7. Explosion Mode Analysis of Rapid Chemical Reactions
289
Table 8.1. H2‚ÄìO2 elementary reactions.
Reaction
Rate
Type of Reaction
H2 + O2 ‚ÜíH ¬∑ +HO2
k0
Initiation
H2 + OH¬∑ ‚ÜíH ¬∑ +H2O
k1
O2 + H¬∑ ‚ÜíOH ¬∑ +O¬∑
k2
H2 + O¬∑ ‚ÜíOH ¬∑ +H¬∑
k3
Propagation
H ¬∑ +W ‚Üí
a1
Termination
OH ¬∑ +W ‚Üí
a2
Termination
O ¬∑ +W ‚Üí
a3
Termination
This nonlinear stability analysis of bifurcating oscillations uses both qua-
sistatic state and averaging methods. The solutions reduce to a quasistatic
manifold, and the highly oscillatory behavior on it can be described us-
ing averaging methods. The reader should carry out the details of this
calculation for the case of van der Pol‚Äôs equation where
f(x, 0, x‚Ä≤, 0, Œµ) = x2x‚Ä≤.
8.7
Explosion Mode Analysis of Rapid Chemical
Reactions
Explosive chemical reactions are diÔ¨Écult to study using the QSSA because
most of the interesting dynamics occur in transients after an initiation
phase, but before the reaction comes to equilibrium. The example treated
in this section illustrates a case where the QSSA theorem does not give
useful information directly, but it provides some guidance in obtaining a
canonical problem whose solution is relatively simple to Ô¨Ånd. The canonical
problem gives an approximate description of the initial transient‚Äôs behavior,
where most of the interesting dynamics occur.
The combustion of hydrogen involves a chain-branched reaction that is
described in Table 8.1 [2]. Three types of reactions are involved in this
process: initiation, branching (or propagation), and termination. In these
reactions HO2 is taken to be an inactive particle, and W in the termination
reactions indicates collisions with the container‚Äôs wall.
We describe the concentrations of various chemical species by
u = [H2],
v = [O2],
x = [H¬∑],
y = [OH¬∑],
z = [O¬∑],
where the notation H¬∑, O¬∑, etc., denotes highly reactive radicals. Then the
kinetic rate equations can be derived directly from Table 8.1 as H2 ‚àíO2
elementary reactions

290
8. Quasistatic-State Approximations
du
dt
=
‚àí(k1y + k3z)u ‚àík0uv,
u(0) = u0
dv
dt
=
‚àík2xv ‚àík0uv,
v(0) = v0
dx
dt
=
‚àí(k2v + a1)x + k1uy + k3uz + k0uv,
x(0) = 0
dy
dt
=
k2vx ‚àí(k1u + a2)y + k3uz,
y(0) = 0
dz
dt
=
k2vx ‚àí(k3u + a3)z,
z(0) = 0.
It is convenient to rewrite this system using matrices: Let
B =
Ô£´
Ô£≠
‚àík2v
k1u
k3u
k2v
‚àík1u
k3u
k2v
0
‚àík3u
Ô£∂
Ô£∏
and
T =
Ô£´
Ô£≠
a1
0
0
0
a2
0
0
0
a3
Ô£∂
Ô£∏.
The matrix B describes the branching reactions, and the matrix T describes
the termination reactions. If X = col(x, y, z), then
dX
dt = (B ‚àíT)X + col(k0uv, 0, 0),
where the terms on the right-hand side denote branching, termination, and
initiation, respectively, of the radical concentrations.
Spectral Analysis of B ‚àíT. The characteristic polynomial of B ‚àíT is
P(Œª) = ‚àídet(B ‚àíT ‚àíŒªI3),
where I3 is the 3 √ó 3 identity matrix. The roots of this polynomial are the
eigenvalues of B ‚àíT, and P has the form
P(Œª) = Œª3 + œÉŒª2 + ŒΩŒª ‚àíœâ(u, v),
where œÉ = Œ± + Œ≤ + Œ∫ + a1 + a2 + a3 > 0,
ŒΩ = Œ±(a2 + a3) + Œ≤(Œ∫ + a1 + a3) + Œ∫(a1 + a2) + a1a2 + a3a2 + a1a3 > 0,
and
œâ(u, v) = 2Œ±Œ≤Œ∫ ‚àí[a1Œ≤Œ∫ + a1a2Œ∫ + a1a3Œ≤ + a3a2Œ± + a1a2a3],
where Œ± = Œ∫2v, Œ≤ = k1u, and Œ∫ = k3u. Note that
œâ(u, v) = det(B ‚àíT).

8.7. Explosion Mode Analysis of Rapid Chemical Reactions
291
The sign of œâ plays an important role here. First, P is a monotone increasing
function of Œª, since its Ô¨Årst derivative is positive for Œª > 0.
Therefore, if P(0) = ‚àíœâ(u, v) < 0, then there is a unique real, positive
eigenvalue Œª‚àó(u, v). Moreover, if Œª1 and Œª2 denote the other two eigenvalues,
then either (1) they are real and both negative, or (2) they are imaginary.
In the latter case,
Œª‚àó+ Œª2 + Œª1 = ‚àíœÉ < 0,
so 2Re Œª1 < ‚àíŒª‚àó< 0.
Thus, in either case, if œâ(u, v) > 0, then B ‚àíT has one positive, real
eigenvalue and two other eigenvalues that have negative real parts.
Denote by Œª‚àó(u, v) the eigenvalue of B ‚àíT that has the largest real part,
and let œÜ‚àódenote the corresponding eigenvector, so (B‚àíT)œÜ‚àó= Œª‚àóœÜ‚àó. The
vector œà‚àódenotes the adjoint eigenvector
(B ‚àíT)trœà‚àó= Œª‚àóœà‚àó.
These are normalized so that
œà‚àó¬∑ œÜ‚àó= 1
and
œà‚àó¬∑ œà‚àó= 1.
The eigenvector œÜ‚àóis referred to as the explosion mode, and Œª‚àógives its
ampliÔ¨Åcation, or explosion rate. For relevant values of the reaction rates,
œÜ‚àóis observed to remain essentially constant throughout the reaction.
The radical components can be rewritten as
Ô£´
Ô£¨
Ô£¨
Ô£≠
x
y
z
Ô£∂
Ô£∑
Ô£∑
Ô£∏= cœÜ‚àó+ ‚Ñ¶,
where the vector ‚Ñ¶accounts for the Œª1 and Œª2 modes. Since these
modes will be damped, we ignore them and introduce the explosion mode
approximation
Ô£´
Ô£¨
Ô£¨
Ô£≠
x
y
z
Ô£∂
Ô£∑
Ô£∑
Ô£∏= cœÜ‚àó.
Substituting this into the model gives
du
dt
=
‚àí(k1œÜ‚àó
2 + k3œÜ‚àó
3)cu ‚àík0uv
dv
dt
=
‚àí(k2œÜ‚àó
1)cv ‚àík0uv
dc
dt
=
Œª‚àó(u, v)c + Œ±k0uv,
where Œ± gives the projection of the initiation vector col(1, 0, 0) onto œÜ‚àó.
Since the initiation reaction is negligible throughout most of the reaction,

292
8. Quasistatic-State Approximations
we ignore it:
du
dt
=
‚àí(k1œÜ‚àó
2 + k3œÜ‚àó
3)cu
dv
dt
=
‚àí(k2œÜ‚àó
1)cv
dc
dt
=
Œª‚àó(u, v)c.
This is called the canonical Ô¨Årst-order branching problem, and it is solvable;
the equation for du/dv can be easily solved:
v = KuŒæ,
where Œæ =
(k2œÜ‚àó
1)
(k1œÜ‚àó
2 + k3œÜ‚àó
3).
Then the equation for dc/du can be integrated:
c = ¬Øc ‚àí
 u
¬Øu
Œª‚àó(s, KsŒæ)
(k1œÜ‚àó
2 + k3œÜ‚àó
3)
ds
s ,
where ¬Øu and ¬Øc are values after the initiation phase (¬Øu ‚âàu0, ¬Øc ‚âà0).
As the reaction proceeds, u and v are depleted and Œª‚àó(u, v) moves from
being large and positive to being negative. The values of u and v for which
Œª‚àó= 0 deÔ¨Åne the Ô¨Årst explosion limit of the reaction. If u and v are super-
critical (Œª‚àó> 0), then an explosion will ensue that begins termination only
when Œª‚àó= 0. The values of c at termination can be estimated using the
above formula for c [2, 128].
8.8
Computational Schemes Based on QSSA
The numerical solution of singular perturbation problems can be diÔ¨Écult
for several reasons; for example, it is not unusual in applications that Œµ =
O(10‚àí10). Solving for the initial transient can be handled by taking a very
small step size for the numerical algorithm to ensure accuracy, but it is
only the Ô¨Årst problem. The second problem is that any numerical algorithm
constantly makes errors that throw the solution oÔ¨Äof the reduced solution,
and small step sizes might be required throughout the calculation to ensure
that the numerical scheme is stable. This last aspect of the computation is
referred to as stiÔ¨Äness of the system. Various computer packages have been
designed to circumvent this problem of stiÔ¨Äness, such as Gear‚Äôs package
and LSODE, but computation remains expensive [67, 112].
QSS methods can be used to formulate a useful numerical scheme that
takes advantage of solution structure to avoid some of the stiÔ¨Äness prob-
lems. These can signiÔ¨Åcantly reduce the computation time, but they do
require preprocessing the system into a standard form, which may not be
possible in reasonable time.

8.8. Computational Schemes Based on QSSA
293
Consider the initial value problem
dx
dt
=
f(t, x, y, Œµ),
x(0) = Œæ(Œµ) = Œæ0 + O(Œµ)
Œµdy
dt
=
g(t, x, y, Œµ),
y(0) = Œ∑(Œµ) = Œ∑0 + O(Œµ),
where Œµ is a small, positive parameter. Here x, f, Œæ ‚ààEM and y, g, Œ∑ ‚ààEN.
The reduced problem (Œµ = 0 with the initial y condition canceled) is
dx
dt
=
f(t, x, y, 0),
x(0) = Œæ0
0
=
g(t, x, y, 0).
It is assumed to have a smooth solution, x = x0(t), y = y0(t), on some
interval 0 ‚â§t ‚â§T. Moreover, the data f, g, Œæ, and Œ∑ are assumed to be
smooth near this solution and for Œµ near zero. Finally, the Jacobian matrix
gy(t, x0(t), y0(t), 0)
is assumed to be stable (i.e., all of its eigenvalues lie in the left half-plane,
bounded uniformly away from the imaginary axis for all 0 ‚â§t ‚â§T).
Under these conditions, the Quasistatic State Approximation Theorem
shows that the solution of the full problem has the form
x(t, Œµ)
=
x0(t) + x1(t)Œµ + X(t/Œµ, Œµ) + O(Œµ2)
y(t, Œµ)
=
y0(t) + y1(t)Œµ + Y (t/Œµ, Œµ) + O(Œµ2),
where X and Y satisfy
|X(t/Œµ, Œµ)| + |Y (t/Œµ, Œµ)| ‚â§Ke‚àí¬µt/Œµ
for some positive constants K and ¬µ that are independent of Œµ. These
estimates hold uniformly for 0 ‚â§t ‚â§T.
We wish to determine a numerical approximation of (x(h, Œµ), y(h, Œµ)) for
a given step size h ‚â´Œµ. Direct evaluation of this quantity by numerical
packages can be expensive if Œµ is very small, but the QSSA Theorem ensures
that x0(h), y0(h) is a useful approximation.
8.8.1
Direct Calculation of x0(h), y0(h)
Since h/Œµ ‚â´1, we can ignore X and Y terms in the approximation, and
x0(h), y0(h) should give an acceptable approximation to x(h, Œµ), y(h, Œµ). We
begin with a guess for y0(0) and use Newton‚Äôs method to solve the equation
0 = g(0, x(0), y, 0)
for y. This process can be repeated as required up to t = h. Let us next use
a pth-order numerical method for solving the ordinary diÔ¨Äerential equation

294
8. Quasistatic-State Approximations
for x0(h). The result is that
x(h, Œµ)
=
x0(h) + O(Œµ) + O(hp+1)
y(h, Œµ)
=
y0(h) + O(Œµ) + O(hp+1),
where x0(h) and y0(h) are the computed solutions.
8.8.2
Extrapolation Method
It is possible to avoid solving the reduced problem by taking advantage of all
of the information given by the QSSA Theorem. The extrapolation method
described here was derived in [79]. The idea is to identify a value, say Œµ‚Ä≤, that
is substantially larger than Œµ, but for which the solution x(h, Œµ‚Ä≤), y(h, Œµ‚Ä≤)
approximates x(h, Œµ), y(h, Œµ) to the accuracy of the numerical scheme used.
Solving the full problem with a large value of Œµ can be done with greater
accuracy using less eÔ¨Äort.
First, a value T is found such that
K exp(‚àí¬µT) = O(hp+1),
where ¬µ is usually on the order of the largest negative eigenvalue of gy, and
K depends on the size of gy. Next, the value
Œµ‚Ä≤ = h
T
is deÔ¨Åned, and the full problem is solved using a standard integration
method, say of order p, to determine values x(h, Œµ‚Ä≤/2), y(h, Œµ‚Ä≤/2) and
x(h, Œµ‚Ä≤), y(h, Œµ‚Ä≤). From the arguments below, it follows that
x(h, Œµ)
=
2x(h, Œµ‚Ä≤/2) ‚àíx(h, Œµ‚Ä≤) + O(hp+1) + O(Œµ‚Ä≤2) + O(Œµ)
y(h, Œµ)
=
2y(h, Œµ‚Ä≤/2) ‚àíy(h, Œµ‚Ä≤) + O(hp+1) + O(Œµ‚Ä≤2) + O(Œµ).
These formulas are derived by observing that
2x(h, Œµ‚Ä≤/2) = 2x0(h) + x1(h)Œµ‚Ä≤ + 2X(T, Œµ‚Ä≤/2) + O(Œµ‚Ä≤2)
and
x(h, Œµ‚Ä≤) = x0(h) + x1(h)Œµ‚Ä≤ + X(T, Œµ‚Ä≤) + O(Œµ‚Ä≤2).
Subtracting these two expressions gives
2x(h, Œµ‚Ä≤/2) ‚àíx(h, Œµ‚Ä≤) = x0(h) + O(hp+1) + O(Œµ‚Ä≤2).
On the other hand,
x(h, Œµ) = x0(h) + O(Œµ),

8.9. Exercises
295
so the desired result for x(h, Œµ) is found. Similarly for y(h, Œµ). The Ô¨Ånal
result is that
x(h, Œµ)
=
2x(h, Œµ‚Ä≤/2) ‚àíx(h, Œµ‚Ä≤) + O(hp+1) + O(Œµ) + O((h/T)2)
y(h, Œµ)
=
2y(h, Œµ‚Ä≤/2) ‚àíy(h, Œµ‚Ä≤) + O(hp+1) + O(Œµ) + O((h/T)2).
This is referred to as the extrapolation approximation.
In many applications, the number of operations used in these compu-
tations is proportional to 1/Œµ, for x(h, Œµ), and so on, and to 1/Œµ‚Ä≤, for
x(h, Œµ‚Ä≤), and so on. Therefore, the ratio Œµ‚Ä≤/Œµ indicates the relative number
of operations of direct solution compared to the extrapolation solution.
Using the extrapolation approximation can avoid analytical preprocess-
ing of a problem to get it into the form of the full problem for x and y if K
and ¬µ can be estimated. It also avoids having to solve the algebraic equa-
tion g = 0 at each mesh point required for solving the x equation of the
reduced problem. Both preprocessing and solving for the quasistatic state
y can require substantial amounts of time, and a major object of computer
solvers is to avoid preprocessing.
This extrapolation method is based on the QSS perturbation scheme. It
is distinct from straightforward Richardson extrapolation formulas that are
based on Taylor‚Äôs formula and that are not appropriate for stiÔ¨Äproblems.
It is important to note that this method improves as Œµ gets smaller.
8.9
Exercises
8.1.
Use the quasistatic state approximation to construct the solution of
the system
dx
dt
=
ax + by + sin t,
x(0) = 1
Œµdy
dt
=
‚àíy + cos t,
y(0) = 3,
where x and y are scalars and Œµ is a small positive number. Construct
both the quasistatic state and the initial layer corrections through
order Œµ2.
8.2.
Show in the Quasistatic State Approximation Theorem that the
initial layer corrections satisfy Xj ‚Üí0 as œÑ ‚Üí‚àû.
8.3‚àó.
Problems involving two, or more, small parameters can also be
handled using the QSS method. Consider the system
dx
dt
=
Ax + Bu + Cv + f(t)
Œµdu
dt
=
Du + Ev + g(t)
Œµ2 dv
dt
=
Fv + h(t),

296
8. Quasistatic-State Approximations
where x, f are in EM, u, g are in EN, and v, h are in EK. The matrices
A, B, and so on, are of appropriate dimensions. This problem contains
two small parameters Œµ and Œµ2. Suppose that the matrices D and F
are stable and that the functions f, g, and h are smooth functions.
Show that this problem has a quasistatic state (x‚àó, u‚àó, v‚àó) and two
transients
	
X
	 t
Œµ, Œµ

, U
	 t
Œµ, Œµ

, V
	 t
Œµ, Œµ


and
	
X‚àó
	 t
Œµ2 , Œµ

, U ‚àó
	 t
Œµ2 , Œµ

, V ‚àó
	 t
Œµ2 , Œµ


that decay at exponential rates. Moreover, show that the solution is
given by the formula
(x, u, v) = (x‚àó, u‚àó, v‚àó) + (X, U, V ) + (X‚àó, U ‚àó, V ‚àó)
and that the terms in this formula can be expanded in a Taylor
expansion about Œµ = 0.
8.4.
Suppose that the functions f(x, y, Œµ) and g(x, y, Œµ) are smooth func-
tions (at least twice continuously diÔ¨Äerentiable) for all real numbers
x, y, and Œµ. Suppose that the system
dx1
dt
=
f1(x, y, 0)
dx2
dt
=
f2(x, y, 0)
0
=
‚àíy3 + x1y ‚àíx2
has an oscillation, say x‚àó
1(t), x‚àó
2(t), y(t), on which
‚àí3y2 + x1 < 0.
Therefore, the oscillation lies either on the top or bottom branch of
the cusp surface. Let T0 denote the period of this oscillation. The
Friedrichs‚ÄìWasow Theorem shows that there is a unique periodic so-
lution of the full problem lying near the orbit of the reduced problem
and having period near T0. Use the quasistatic state method to derive
the Ô¨Årst-order approximation to the oscillation of the full system.
8.5.
Consider the atoll oscillator, which deÔ¨Ånes a Ô¨Çow on a torus having
interesting periodic solutions:
dx
dt
=
5.0(1.05 + cos x ‚àícos y)
dy
dt
=
0.04(1.0 + cos y + 10.0 cos x).
Prove that there is a periodic solution to this system. Simulate
the solution of this system by calculating the solution and plotting
the results on a toroidal patch (x and y modulo 2œÄ) as shown in
Figure 8.10.

8.9. Exercises
297
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
cos x
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
t
cos y
Figure 8.9. The atoll oscillator solutions.
8.6.
Consider the nonlinear stability problem
dx
dt = A(Œª)x + f(t, x),
where x, f are in EM and A is a matrix of appropriate dimensions.
Suppose that f is a smooth function that satisÔ¨Åes
f(t, x) = o(|x|)
as |x| ‚Üí0 uniformly for 0 ‚â§t < ‚àû. Suppose that a simple eigenvalue
of A passes through the origin from left to right as Œª increases through
zero while the remaining eigenvalues remain in the left half plane,
bounded away from the imaginary axis.
Apply the nonlinear stability analysis described in Section 8.6.1 to
describe solutions. Derive Landau‚Äôs equation for this problem.
8.7.
Consider the damped conservative system
Œµu‚Ä≤‚Ä≤ + ru‚Ä≤ + Uu(u) = 0,
where Œµ is a small dimensionless parameter, r describes the coeÔ¨Écient
of friction, and U(u) describes the system‚Äôs potential energy. We wish
to solve this equation subject to the boundary conditions
u(0) = A,
u(1) = B.
Setting Œµ = 0 in the equation results in the reduced problem
ru‚Ä≤
0 + Uu(u0) = 0.

298
8. Quasistatic-State Approximations
0
1
2
3
4
5
6
0
1
2
3
4
5
6
x
y
Figure 8.10. The atoll oscillator on T 2.
This Ô¨Årst-order diÔ¨Äerential equation‚Äôs solution is uniquely determined
by specifying u0 at one point. Therefore, in general the solution
cannot satisfy both boundary conditions.
a. What is the correct value for u0 to approximate the solution of the
full problem?
b. Solve the problem explicitly in the special case where U(u) = u2/2.
8.8.
Show that if the matrix A in the quasistatic state theorem is expo-
nentially stable, then the error estimates in the result hold uniformly
for 0 ‚â§t < ‚àû.
8.9‚àó.
Consider the stochastic diÔ¨Äerential equation [91, 42]
dx = 1
Œµ F ‚Ä≤(x)dt + œÉdB,
where x(t) is a random variable and dB denotes white noise, as in
Section 7.8. If u(x, t) denotes the probability density function of the
random variable x, then it is known that u solves the Fokker‚ÄìPlanck
equation
‚àÇu
‚àÇt = œÉ2
2
‚àÇ2u
‚àÇx2 ‚àí‚àÇ
‚àÇx
	
F(x)u
Œµ

.
a. Find a static state solution of this. (Hint: Set ‚àÇu/‚àÇt = 0 and solve
the resulting equation for u(x).)

8.9. Exercises
299
b. Determine what happens to this distribution as Œµ and œÉ2 ‚Üí0+. In
particular, explain the role played by the maxima of F(x) in the
distribution of x.
8.10.
Let p denote the proportion of a human population‚Äôs gene pool that is
carried by a single locus that is of type A, and let q = 1‚àíp denote the
proportion that are of type B. We suppose that there are only these
two types. As a result, the population can be broken down into three
groups AA, AB, and BB, by their genetic type. Let D(t), 2H(t), and
R(t) denote the proportions of the population of these types at time
t. Then p = (D + H), q = (H + R). If selection is slow, then the birth
rates and death rates for all of these genotypes are almost the same.
Suppose that the birth rates are identical and that the death rates
are dAA = d + Œµd1, dAB = d + Œµd2, dBB = d + Œµd3. Then
dD
dt
=
b(p2 ‚àíD) + Œµ(d‚àó‚àíd1)D
dH
dt
=
b(pq ‚àíH) + Œµ(d‚àó‚àíd2)H
dR
dt
=
b(q2 ‚àíR) + Œµ(d‚àó‚àíd3)R,
where b > 0 is the population‚Äôs birth rate, and d‚àó= d1D+2d2H+d3R
[73].
a. Derive a diÔ¨Äerential equation for p.
b. Apply the quasistatic state approximation theorem to this system to
obtain the quasistatic state and the initial correction expansions to
Ô¨Årst order in Œµ.
c. Plot your results for (D, 2H, R) using triangular coordinates, as shown
in Figure 8.11.
8.11‚àó. Consider the nonlinear renewal equation [54]
Œµx(t)
=
 0
‚àí1
{(2Œµ ‚àí1)x(t + s) ‚àíx2(t + s)}ds
for t > 0
x(t)
=
Œµv0(t)
for ‚àí1 ‚â§t < 0,
where 0 < Œµ ‚â™1 and where v0 is extended as a periodic function.
Use the multitime algorithm to Ô¨Ånd an approximate solution to this
equation. In particular, let
x = ŒµV (Œæ, t, Œµ) = ŒµV0(Œæ, œÑ) + Œµ2V1(Œæ, œÑ) + ¬∑ ¬∑ ¬∑ ,
where Œæ = (1 ‚àíŒµ)t and œÑ = Œµ2t. Show that
‚àÇV0
‚àÇœÑ + ‚àÇ
‚àÇŒæ (V0 ‚àíV 2
0 ) = 1
2
‚àÇ2V0
‚àÇŒæ2
V0(Œæ, œÑ) = V0(Œæ ‚àí1, œÑ)
and
V0(Œæ, 0) = v0(Œæ).
8.12‚àó. Consider the equation [72]
Œµ‚àÇu
‚àÇt = D ‚àÇ2u
‚àÇx2 + u ‚àíu3

300
8. Quasistatic-State Approximations
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
D
  H
R
Hardy-Weinberg
Equilibria
2
Figure 8.11. Hardy‚ÄìWeinberg quasi-equilibrium.
for 0 ‚â§x ‚â§1 and the auxiliary boundary conditions
‚àÇu
‚àÇx(0, t) = 0,
‚àÇu
‚àÇx(1, t) = 0.
Suppose that u(x, 0) = U(x) is given. Show that there is a qua-
sistatic state for this problem. Construct it and the transient initial
correction.

Supplementary Exercises
S.1.
Consider the forced pendulum equation
x‚Ä≤‚Ä≤ + Œ± sin x = B cos 2œÄt.
Calculate the rotation number of the response as a function of forcing
amplitude B and oscillator tuning Œ± by converting the problem to
polar coordinates and calculating the values Œ∏(100œÄ)/(100œÄ).
S.2.
Solve the Michaelis‚ÄìMenten model in Section 8.3.4 for Œµ = 0.001
and Œµ = 0.00001 and using the extrapolation method described in
Section 8.8.
S.3.
Consider a linear array of masses extending from ‚àí‚àûto +‚àû, say
having masses mj at site j, ‚àí‚àû< j < ‚àû. Suppose that the location
of the jth mass is xj, and that these masses are connected by springs.
Then the dynamics of the ensemble are described by the equations
dxj
dt = f(xj+1 ‚àíxj) ‚àíf(xj ‚àíxj‚àí1),
where f(u) describes the restoring force of a spring that is deÔ¨Çected u
units from rest. Carry out a computer simulation of this system when
f(s) = as, when f(s) = bs + cs3, and when f(s) = e‚àía|s| where a, b,
and c are Ô¨Åxed positive constants. The Ô¨Årst case is a simple diÔ¨Äusion
problem, the second is referred to as the Fermi‚ÄìPasta‚ÄìUlam model,
and the third is the Toda lattice.
S.4.
Simulate the twist mapping
rn+1 = rn ‚àíh cos(tn + rn),
tn+1 = tn + rn.
S.5.
Using a computer simulation, determine the two basins of attraction
of the stable periodic solutions described in Figure 6.6. (Hint: Given

302
8. Quasistatic-State Approximations
a tolerance Œµ, calculate the probability that a line segment having
this length and thrown at random hits the boundary. Calculate this
number for several values of Œµ and extrapolate its value to Œµ = 0.)
S.6.
Consider the array of VCONs
dxj
dt = œâj + Œµ tanh
	
A cos xj + Cj cos ¬µt +
100

k=1
Bj,k cos xk

for j = 1, . . . , 100, where the matrix B is a cyclic matrix. Take œâj =
1.0‚àíj0.8/50.0 for j = 1, . . . , 50 and œâj = œâ101‚àíj for j = 51, . . . , 100,
A = 5.0, Œµ = 0.5, Cj = 5.0, Bjj = 0.0.
a. Simulate this system and plot the resulting rotation vector (œÅ1, . . . , œÅN)
using polar coordinates. (Hint: DeÔ¨Åne œÅj = xj(100œÄ)/x1(100œÄ) and
plot the points (œÅj cos(2œÄj/100), œÅj sin(2œÄj/100)) for j = 1, . . . , 100.)
b. Experiment with various choices of the connection matrix B. For ex-
ample, let it describe nearest neighbor interactions where it is a cyclic
matrix having center row . . . , 1, ‚àí2, 1, . . . , or let B be a permutation
matrix.
S.7.
Consider the semi-implicit scheme
xn+1
=
xn ‚àíhf(yn+1)
yn+1
=
yn + hf(xn),
where f(u) = sin u. Simulate the solution of this iteration for various
choices of initial conditions. Plot your answers by plotting the iterates
of this mapping in the xy-plane.
S.8.
Carry out a Monte Carlo simulation of solutions to the stochastic
diÔ¨Äerential equation
dy = 1
Œµ F ‚Ä≤(y)dt + œÉ
Œµ dB
by computing sample paths using the discrete model
yn+1 = yn + h
Œµ (F ‚Ä≤(yn) + œÉWn),
where Wn is at each step a random variable selected from a nor-
mal distribution and F is a quartic function. Plot your answer as a
histogram of y100. (Hint: Use a random number generator for a uni-
formly distributed random variable on the interval ‚àí1 < y < 1 to
select six values. Add them up and divide by
‚àö
6. The result is a
normally distributed variable (see [62]).)

References
[1] M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions, Dover,
New York, 1972.
[2] P. Alfeld, F.C. Hoppensteadt, Explosion Mode Analysis of H2‚ÄìO2 Combus-
tion, Chemical Physics Series, Springer-Verlag, New York, 1980.
[3] A.A. Andronov, A.A. Witt, S.E. Chaikin, Theory of Oscillators, Dover, New
York, 1966.
[4] V.I. Arnol‚Äôd, Mathematical Methods of Classical Mechanics, Springer-Verlag,
New York, 1978.
[5] V.I. Arnol‚Äôd, A. Avez, Ergodic problems in classical mechanics, W.A.
Benjamin, New York, 1968.
[6] H. Antosiewicz, A survey of Liapunov‚Äôs second method, in Contributions to
the Theory of Nonlinear Oscillations, S. Lefschetz (ed.), Vol. IV, Princeton,
N.J., 1958.
[7] H.T. Banks, F. Kappel, Spline approximations for functional diÔ¨Äerential
equations, J. DiÔ¨Äerential Eqns., 34(1979): 496‚Äì522.
[8] K.G. Beauchamp, Walsh Functions and their Applications, Academic Press,
New York, 1975.
[9] R. Bellman, Perturbation Theory, Holt-Rhinehardt-Winston, New York,
1964.
[10] R. Bellman, K. Cooke, DiÔ¨Äerential-DiÔ¨Äerence Equations, Academic Press,
New York, 1963.
[11] A.S. Besicovitch, Almost Periodic Functions, Dover, New York, 1954.
[12] G.D. BirkhoÔ¨Ä, Dynamical Systems, Vol. IX., American Mathematical
Society, Providence, RI, 1966.

304
References
[13] N.N. BogoliuboÔ¨Ä, Y.A. Mitropolski, Asymptotic Methods in the Theory of
Nonlinear Oscillations, Gordon-Breach, New York, 1961.
[14] G.E. Briggs, J.B.S. Haldane, A note on the kinetics of enzyme action.
Biochem. J. 19(1925): 338‚Äì339.
[15] H. Carrillo, The method of averaging and stability under persistent distur-
bances with applications to phase-locking, Dissertation, University of Utah,
1983.
[16] P.H. Carter, An improvement of the Poincar¬¥e‚ÄìBirkhoÔ¨ÄÔ¨Åxed point theorem,
Trans. AMS 269(1982): 285‚Äì299.
[17] M.A. Cartwright, J.E. Littlewood, Ann. Math. 54(1951): 1‚Äì37.
[18] L. Cesari, Asymptotic Behavior and Stability Problems in Ordinary Dif-
ferential Equations, Ergebnisse der Math. New Series, Vol. 16, 1963, 2nd
ed.
[19] S. Chandrasekhar, Hydrodynamic and Hydromagnetic Stability, Oxford
University Press, 1961.
[20] E.W. Cheney, Introduction to Approximation Theory, McGraw-Hill, New
York, 1966.
[21] E.W. Cheney, D. Kincaid, Numerical Mathematics and Computing, Brooks-
Cole, Monterey, CA, 1980.
[22] W. Chester, The forced oscillations of a simple pendulum, J. Inst. Maths.
Appl. 15(1975): 298‚Äì306.
[23] S.N. Chow, J.K. Hale, Methods of Bifurcation Theory, Springer-Verlag, New
York, 1982.
[24] E.A. Coddington, N. Levinson, Theory of Ordinary DiÔ¨Äerential Equations,
McGraw Hill, New York, 1955.
[25] D.S. Cohen, F.C. Hoppensteadt, R.M. Miura, Slowly modulated oscillations
in nonlinear diÔ¨Äusion processes, SIAM J. Appl. Math. 33 (1977):217‚Äì229.
[26] J.D. Cole, Perturbation Methods in Applied Mathematics, Blaisdale, Waltham,
MA, 1968.
[27] W.A. Coppel, Stability and Asymptotic Behavior of DiÔ¨Äerential Equations,
Heath, Boston, 1965.
[28] R. Courant, D. Hilbert, Methods of Mathematical Physics, Vol. I, Wiley-
Interscience, New York, 1968.
[29] Salvador Dali, The Persistence of Memory. Centre Georges Pompidou, Mus¬¥ee
National d‚ÄôArt Moderne, Paris, 1980.
[30] A. Denjoy, Sur les courbes d¬¥eÔ¨Ånies par les ¬¥equations diÔ¨Ä¬¥erentielles a la surface
du tor, J. Math. Pures Appl. 9(1932):333‚Äì375.
[31] G. DuÔ¨Éng, Erzwungene Schwingungen bie ver¬®anderlicher Eigenfrequeng, F.
Vieweg u. Sohn, Braunschweig, 1918.
[32] J. Dugundji, Fixed Point Theory, Panstwowe Wydawnictwo Naukowe,
Warsaw, 1982.
[33] A. Erdelyi, Asymptotic Expansions, Dover, New York, 1956.
[34] D.K. Fadeev, V.N. Fadeeva, Computational Methods in Linear Algebra, W.H.
Freedman, San Francisco, 1963.

References
305
[35] J.D. Farmer, E. Ott, J.A. Yorke, Physica 7D (1983): 153.
[36] P. Fatou, Sur les equations fonctionelles, Bull. Soc. Math. Fr. 47(1919): 161‚Äì
211; 48(1920): 33‚Äì94, 208‚Äì314.
[37] W. Feller, An Introduction to Probability Theory and its Applications, Wiley,
New York, 1968.
[38] J.E. Flaherty, F.C. Hoppensteadt, Frequency entrainment of a forced van
der Pol oscillator, Studies Appl. Math. 58(1978): 5‚Äì15.
[39] A.T. Fomenko, Integrable systems on Lie algebras and symmetric spaces,
Gordon and Breach, New York, 1988.
[40] J.S. Frame, Applications of Matrices in Engineering, MSU Lecture Notes,
1965.
[41] L.E. Frankel, On the method of matched asymptotic expansions, Proc.Camb.
Phil. Soc. 65(1969): 209‚Äì284.
[42] M.I. Freidlin, A.D. Ventsel, Random Perturbations of Dynamical Systems,
Springer-Verlag, New York, 1984.
[43] K.O. Friedrichs, Lectures on Advanced Ordinary DiÔ¨Äerential Equations,
Gordon and Breach, New York, 1965.
[44] K.O. Friedrichs, Asymptotic phenomena in mathematical physics, Bull.
AMS (1955): 485‚Äì504.
[45] S. Freud, The Interpretation of Dreams, Allen & Unwin, London, 1954.
[46] F.R. Gantmacher, Applications of the Theory of Matrices, Wiley-Interscience,
New York, 1959.
[47] P.R. Garabedian, Partial DiÔ¨Äerential Equations, Wiley, New York, 1964.
[48] J. Glieck, Chaos: Making of a New Science, Viking, 1987.
[49] H. Goldstein, Classical Mechanics, Addison-Wesley, Reading, Mass., 1950.
[50] Preconditioned Conjugate Gradient Methods, Springer-Verlag, New York,
1990.
[51] N. Gordon, F.C. Hoppensteadt, Nonlinear stability analysis of static states
which arise through bifurcation, Comm. Pure Appl. Math. 28(1975): 355‚Äì
373.
[52] Jo. Grasman, E.J.M. Velig, G. Willems, Relaxation oscillations governed by
a van der Pol equation, SIAM J. Appl. Math. 31(1976): 667‚Äì676.
[53] J. Grasman, E.J.M. Velig, G. Willems, Relaxation oscillations governed by
a van der Pol equation, SIAM J. Appl. Math. 31(1976): 667‚Äì676.
[54] J.M. Greenberg, F.C. Hoppensteadt, Asymptotic behavior of solutions to a
population equation, SIAM J. Appl. Math. 17(1975): 662‚Äì674.
[55] J. Guckenheimer, P. Holmes, Nonlinear Oscillations, Dynamical Systems
and Bifurcations of Vector Fields, Applied Mathematical Sciences, Vol. 42,
Springer-Verlag, New York, 1983.
[56] J. Hadamard, Sur l‚Äôit¬¥eration et les solutions asymptotiques des ¬¥equations
dif¬¥erentielles, Bull. Soc. Math. France 29(1901): 224‚Äì228.
[57] W. Hahn, Stability of Motion, Springer-Verlag, New York, 1967.
[58] J.K. Hale, Ordinary DiÔ¨Äerential Equations, Wiley-Interscience, New York,
1971.

306
References
[59] J.K. Hale, Oscillations in Nonlinear Systems, McGraw-Hill, New York, 1963.
[60] P.R. Halmos, Measure Theory, Van Nostrand, Princeton, 1950.
[61] P.R. Halmos, Lectures on Ergodic Theory, Chelsea Publishing Co., New
York, 1956.
[62] J.M. Hammersley, D.C. Handscombe, Monte Carlo Methods, Methuen,
London, 1964.
[63] P. Hartmann, Ordinary DiÔ¨Äerential Equations, Hartmann, Baltimore, 1973.
[64] C. Hayashi, Nonlinear Oscillations in Physical Systems, Princeton Univer-
sity Press, Princeton, 1985.
[65] R.J. Higgins, Electronics with Digital and Analog Integrated Circuits,
Prentice-Hall, Englewood CliÔ¨Äs, New Jersey, 1983.
[66] E. Hille, Analytic Function Theory, Vol. 1 and 2, Ginn, New York, 1959.
[67] A.C. Hindmarsh, Gear‚Äôs ordinary diÔ¨Äerential equation solver, UCID-30001
(rev. 3) Lawrence Livermore Lab, Livermore, CA. Dec. 1974.
[68] M. Hirsch, S. Smale, DiÔ¨Äerential Equations, Dynamical Systems and Linear
Algebra, Academic Press, New York, 1974.
[69] E. Hopf, Ber. Math. Phys. Sachsische Akad. Wiss. Leipzig, 94(1942): 1‚Äì22.
[70] F.C. Hoppensteadt, Singular perturbations on the inÔ¨Ånite interval, Trans.
AMS, 123(1966): 521‚Äì535.
[71] F.C. Hoppensteadt, Properties of solutions of ordinary diÔ¨Äerential equations
with small parameters, Comm. Pure Appl. Math. 24(1971): 807‚Äì840.
[72] F.C. Hoppensteadt, On quasi-linear parabolic equations with a small
parameter, Comm. Pure Appl. Math. 24(1971): 17‚Äì38.
[73] F.C. Hoppensteadt, Mathematical Theories of Populations, SIAM Publica-
tions, 1975.
[74] F.C. Hoppensteadt, Mathematical Methods of Population Biology, Cam-
bridge University Press, New York, 1982.
[75] F.C. Hoppensteadt, Introduction to the Mathematics of Neurons: Modeling
in the Frequency Domain, 2nd ed., Cambridge University Press, New York,
1997.
[76] F.C. Hoppensteadt, J.M. Hyman, Periodic solutions of a logistic diÔ¨Äerence
equation, SIAM J. Appl. Math. 58(1977): 73‚Äì81.
[77] F.C. Hoppensteadt, W.L. Miranker, DiÔ¨Äerential equations having rapidly
changing solutions: Analytic methods for weakly nonlinear systems, J. DiÔ¨Ä.
Eqn. 22 (1976): 237‚Äì249.
[78] F.C. Hoppensteadt, W.L. Miranker, Multi-time methods for systems of
diÔ¨Äerence equations, Studies Appl. Math. 56(1977): 273‚Äì289.
[79] F.C. Hoppensteadt, W.L. Miranker, An extrapolation method for the numer-
ical solution of singular perturbation problems, SIAM J. Sci. Stat. Comp.
4(1983): 612‚Äì625.
[80] F.C. Hoppensteadt, A. SchiaÔ¨Éno, Stable oscillations of weakly nonlin-
ear Volterra integro-diÔ¨Äerential equations, J. reine u. angewandte Math.
353(1984): 1‚Äì13.

References
307
[81] F.C. Hoppensteadt, E.M. Izhikevich, Weakly Connected Neural Networks,
Springer-Verlag, new York, 1997.
[82] F.C. Hoppensteadt, H.S. Salehi, A.V. Skorokhod, Randomly Perturbed
Dynamical Systems, in preparation.
[83] P. Horowitz, W. Hill, The Art of Electronics, 2nd ed., Cambridge University
Press, New York, 1989.
[84] E.L. Ince, Ordinary DiÔ¨Äerential Equations, Dover, New York, 1956.
[85] G. Iooss, D. Joseph, Nonlinear Dynamics and Turbulence, Pitman, Boston,
1983.
[86] H. Jacobowitz, Corrigendum, The existence of the second Ô¨Åxed point, J.
DiÔ¨Äerential Eqns. 25(1977): 148‚Äì149.
[87] E. Jahnke, F. Emde, Tables of Functions, Dover, New York, 1945.
[88] G. Julia, Sur l‚Äôit¬¥eration des fonctions rationelles, J. Math. Pure Appl.
8(1918): 47‚Äì245.
[89] J.B. Keller, Perturbation Theory, Michigan State University, 1968.
[90] J. Kevorkian, J.D. Cole, Perturbation Methods in Applied Mathematics,
Springer-Verlag, New York, 1981.
[91] R.Z. Khas‚Äôminskii, Stochastic Stability of DiÔ¨Äerential Equations, SijthoÔ¨Ä&
NoordhoÔ¨Ä, Rockville, MD, 1980.
[92] A.I. Khinchin, An Introduction to Information Theory, Dover, New York,
1965.
[93] J. Kinney, T. Pitcher, Invariant Measures for Rational Functions. Some con-
nections between ergodic theory and the iteration of polynomials, Ark. Mat.
8(1969): 25‚Äì32.
[94] P. Kokotovic, H. Khalil, Singular Perturbation Methods in Control: Analysis
and Design, Academic, London, 1986.
[95] K. Krohn, J.L. Rhodes, Algebraic Theory of Machines (M.A. Arbib, ed.),
Academic Press, New York, 1968.
[96] N. Krylov, N.N. BogoliuboÔ¨Ä, Introduction to Nonlinear Mechanics, Annals of
Mathematics Studies, No. 11, Princeton University Press, Princeton, 1947.
[97] W. Leveque, Elementary Theory of Numbers, Addison-Wesley, Reading,
Mass., 1962.
[98] M. Levi, F.C. Hoppensteadt, W.L. Miranker, Dynamics of the Josephson
junction, Quart. Appl. Math. (July 1978): 167‚Äì198.
[99] M. Levi, On van der Pol‚Äôs equation, AMS Memoirs, Providence, R.I., 1979.
[100] N. Levinson, A second order diÔ¨Äerential equation with singular solutions,
Ann. Math. 50(19): 127‚Äì152.
[101] T.Y. Li, J.A. Yorke, Period three implies chaos, Amer. Math. Monthly,
82(1975): 985‚Äì992.
[102] W.C. Lindsey, Synchronization Systems in Communication and Control,
Prentice-Hall, Englewood CliÔ¨Äs, N.J., 1972.
[103] J.L. Lions, A. Bensoussan, G. Papanicolaou, Asymptotic analysis for
periodic structures, North-Holland, New York, 1978.

308
References
[104] E.N. Lorenz, Deterministic nonperiodic Ô¨Çow, J. Atoms. Sci. 20(1963): 130‚Äì
141.
[105] W. Magnus, S. Winkler, Hill‚Äôs Equation, Wiley-Interscience, New York,
1966.
[106] I.G. Malkin, Theorie der Stabilit¬®at einer Bewegung, Oldenbourg, Munich,
1959.
[107] B.B. Mandelbrot, The Fractal Geometry of Nature, Updated and Aug-
mented. W.H.Freeman, New York, 1983.
[108] J.L. Massera, Contributions to stability theory, Ann. Math. 64(1956): 182‚Äì
206.
[109] J.L. Massera, J.J. SchaÔ¨Äer, Linear DiÔ¨Äerential Equations and Function
Spaces, Academic Press, New York, 1966.
[110] S.W. McDonald, C. Grebogi, E. Ott, J.A. Yorke, Fractal basin boundaries,
Physica 17D (1985): 125‚Äì153.
[111] N.N. Minorsky, Nonlinear Oscillations, Van Nostrand, Princeton, 1962.
[112] W.L. Miranker, Numerical Methods for StiÔ¨ÄEquations and Singular
Perturbation Problems, D. Reidel, Holland, 1981.
[113] E.F. Mishchenko, Asymptotic calculation of periodic solutions of systems
of diÔ¨Äerential equations containing small parameters in the derivatives, AMS
Transl. Ser. 2, 18(1961): 199‚Äì230.
[114] C. Moler, On the calculation of exponentials of matrices, SIAM Review,
1980.
[115] J. Moser, On the theory of quasi-periodic motions, SIAM Rev. 8(1966):145‚Äì
171.
[116] J. Moser, Stable and Random Motions in Dynamical Systems: With Special
Emphasis on Celestial Mechanics, Princeton University Press, Princeton,
1973.
[117] M.E. Munroe, Introduction to Measure and Integration, Addison-Wesley,
Cambridge, Mss., 1953.
[118] R. Novick, F.C. Hoppensteadt, On plasmid incompatibility, Plasmid
1(1978):421‚Äì434.
[119] R.D. Nussbaum, H.O. Peitgen, Special and spurious solutions of dx/dt =
‚àíaF(x(t ‚àí1)), Mem. AMS 51(310)1984.
[120] R.E. O‚ÄôMalley, Introduction to Singular Perturbations, Academic Press,
New York, 1974.
[121] O. Perron, ¬®Uber stabilit¬®at und asymptotisches verhalten der Integrale von
DiÔ¨Äerential-gleichgungensysteme, Math. Zeit. 29(1929): 129‚Äì160.
[122] S.C. Persek, F.C. Hoppensteadt, Iterated averaging methods for systems of
ordinary diÔ¨Äerential equations with a small parameter, Comm. Pure Appl.
Math. 31(1978): 133‚Äì156.
[123] L.S. Pontryagin, Asymptotic behavior of the solutions of systems of dif-
ferential equations with a small parameter in the higher derivatives, AMS
Transl. Ser. 2, 18(1961): 295‚Äì320.

References
309
[124] Preconditioned Conjugate Gradient Methods, Springer-Verlag, New York,
1990.
[125] W. Rudin, Real and Complex Analysis, McGraw-Hill, 1966.
[126] A.N. Sarkovski, Ukr. Math. Zh. 16(1964): 61‚Äì71. See also P. Stefan, A
theorem of Sarkovskii on the existence of periodic orbits of continuous
endomorphisms of the real line, Comm. Math. Phys. 54(1977): 237‚Äì248.
[127] E.E. Sel‚Äôkov, Stabilization of energy charge, generation of oscillations
and multiple steady states in energy metabolism as a result of purely
stoichiometric regulation, Eur. J. Biochem. 59(1975)197‚Äì220.
[128] N.N. Semenov, Chemical Kinetics and Chain Reactions, Clarendon, Oxford,
1935.
[129] C.L. Siegel, J. Moser, Lectures in Celestial Mechanics, Springer-Verlag, New
York, 1971.
[130] J. Smoller, Shock Waves and Reaction-DiÔ¨Äusion Equations, Springer-
Verlag, New York, 1983.
[131] P.E. Sobolevski, Equations of parabolic type in Banach space, AMS
Translation, 49(1966): 1‚Äì62.
[132] J.J. Stoker, Nonlinear Vibrations, Wiley, New York, 1950.
[133] R. Thom, Structural Stability and Morphogenesis: An Outline of a General
Theory of Models, W.A. Benjamin, Reading, Mass., 1975.
[134] S. Ulam, A Collection of Mathematical Problems, Wiley-Interscience, New
York, 1960.
[135] M.M. Vainberg, V.A. Trenogin, Theory of Branching of Solutions of
Nonlinear Equations, NoordhoÔ¨Ä, Leyden, 1974.
[136] M. van Dyke, Perturbation Methods in Fluid Mechanics, Parabolic Press,
Palo Alto, CA, 1975.
[137] A.B. Vasil‚Äôeva, Asymptotic formulae for the solution of a system of ordinary
diÔ¨Äerential equations containing parameters of diÔ¨Äerent orders of smallness
multiplying the derivatives, Dokl. Akad. Nauk SSSR 128(1959): 1110‚Äì1113.
[138] A.B. Vasil‚Äôeva, V.F. Butuzov, Asymptotic Expansions of Solutions of
Singularlty Perturbed Equations, Nauka, Moscow, 1973 (in Russian).
[139] W. Wasow, Asymptotic Expansions for Ordinary DiÔ¨Äerential Equations,
Interscience, New York, 1965.
[140] N. Wiener, The Fourier Integral and Certain of its Applications, Dover,
New York, 1958.
[141] S. Wiggins, Introduction to Applied Nonlinear Dynamical Systems and
Chaos, Springer-Verlag, New York, 1990.
[142] D.E. Woodward, Phase locking in model neuron networks having group
symmetries, Dissertation, University of Utah, 1988.
[143] T. Yoshizawa, Stability Theory and the Existence of Periodic and Almost
Periodic Solutions, Springer-Verlag, New York, 1975.

Index
Œªœâ-system, 110, 133
H2‚ÄìO2 elementary reactions, 289
œâ-limit set, 104
action integral, 4, 42
active transmission line, 48
almost harmonic system, 214
almost-periodic, 18‚Äì20, 220
ampliÔ¨Åcation rate, 291
angle-action coordinates, 49
angle-action variables, 51
angular phase equations, 40
Approximation of the Averaging
Limit, 236
asymptotic expansion, 148, 161
asymptotically stable, 92, 288
asymptotically stable uniformly in
the parameters, 265
atoll oscillator, 296
Averaging Theorem for Linear
DiÔ¨Äerence Equations, 211
Averaging Theorem for Linear
Systems, 205
Averaging Theorem for Mean-Stable
Systems, 202
Baker‚ÄìCampbell‚ÄìHausdorÔ¨Ätheorem,
211
bifurcation equations, 129, 184
Bogoliubov‚Äôs Averaging Theorem, 223
Bogoliubov‚Äôs Theorem, 233
Brouwer‚Äôs Fixed-Point Theorem, 138
Brownian motion, 240
canonical models, 135
canonical problem, 289
capacity of the curve, 191
Carath¬¥eodory system, 226, 267
Cauchy‚Äôs formula, 9
center frequency, 2
center manifold, 9, 99
chaos, xvi
characteristic equations, 54
characteristic exponent, 14, 112, 276
characteristic multipliers, 14, 112
circle mapping property, 72
col, 112
companion matrix, 6
comparison functions, 99
Condition KAM, 233
Condition L, 222
conservative system, 62
Contraction Mapping Principle, 136

312
Index
cusp, 63, 64, 296
cusp bifurcation, 134
Delaunay orbit elements, 224
delta function, 264
Denjoy‚Äôs Theorem, 39, 116
diagonalizable matrix, 6
DiÔ¨Äerence Equation Averaging
Theorem, 209
diÔ¨Äerential-delay equation, 143
diÔ¨Äerential-diÔ¨Äerence equations, 78
diÔ¨Äusion equation, 240
diÔ¨Äusion processes, 241
discrete oscillatory matrix, 210
discrete stable matrix, 211
dissipative system, 62
distribution function, 239
domain of attraction, 93
DuÔ¨Éng‚Äôs equation, 32, 45, 174, 184,
188
DuÔ¨Éng‚Äôs Iterative Method, 176, 180
entropy, 67, 70, 102, 115
ergodic Markov chain, 242
ergodic measure, 241
Ergodic Theorem, 237
Euler‚Äôs forward diÔ¨Äerence method,
227
Euler‚ÄìLagrange equation, 4, 43
exchange of stabilities, 133, 161
expected value, 239
explosion rate, 291
exponential dichotomy, 97
exponential distribution, 242
exponential integral function, 119
exponentially asymptotically stable,
93, 228, 271
extrapolation, 237
extrapolation approximation, 295
extrapolation method, 294
Fibonacci sequence, xvi
Ô¨Ålters, 3
Ô¨Årst explosion limit, 292
Ô¨Årst-order branching problem, 292
FitzHugh‚ÄìNagumo model, 84
Ô¨Åxed point of period m, 66
Floquet‚Äôs Theorem, 12, 112
Ô¨Çows on a torus, 231
Fokker‚ÄìPlanck equation, 298
fractal curve, 191
Fredholm‚Äôs Alternative, 122, 166
frequency-response relation, 178
Friedrichs‚ÄìWasow Theorem, 276
fundamental matrix, 12
fundamental solution, 23
gauge relations, 147
Gaussian random variable, 244
Gear‚Äôs package, 292
genetic type, 299
gradient method, 101
gradient systems, 101
Gronwall‚Äôs Inequality, 95, 154, 202
Hadamard‚Äôs mappings, 75
Hamilton‚Äôs principle, 4, 42
Hamilton‚ÄìJacobi, 87
Hamilton‚ÄìJacobi equation, 53
Hamiltonian system, 49, 53, 82, 87,
219, 227
hard spring, 179
harmonic mean, 235
harmonic oscillator, 37, 215
HausdorÔ¨Ädimension, 193
Heaviside‚Äôs operator, 3
heteroclinic, 44
higher harmonics, 180
Hill‚Äôs equation, 14, 114
homoclinic, 44
homoclinic points, 78
Homogenization, 234
Hopf bifurcation, 134, 287
Huygen‚Äôs Problem, 116
hyperbolic point, 29
hysteresis, 61, 182
Implicit Function Theorem, 126, 171
index, 141
initial transient, 251, 270
Initial Transient Theorem, 262, 264
inner product, 165
integrable systems, 50
integration by parts, 151, 264
invariant torus, 219
isochrons, 37
iteration histogram, 67

Index
313
Jacobi‚Äôs matrix, 14
Jacobian, 83
Jordan canonical form, 7
Josephson junction, 282
Julia set, 188
KAM Theorem, 233
KAM theory, 233
kernel, 122
kinetic energy, 42
Kronecker Ô¨Çow, 74
l.o.t., 143
Lagrangian, 42
Landau‚Äôs Equation, 285, 287, 297
Laplace inversion formula, 9
Laplace transform, 9
Laplace‚Äôs Integral Formulas, 151
Laplace‚Äôs method, 207
Law of the Iterated Logarithm, 242
least-squares method, 142
least-squares solution, 124
left boundary-layer correction, 282
Liapunov function, 99, 120, 266
Liapunov‚ÄìSchmidt Method, 129, 132
Lie‚Äôs theory, 49
Lienard‚Äôs equation, 104, 110
Linear Quasistatic-State Theorem,
259
linear spline, 80
Linear Stability Theorem, 95, 286
Liouville‚Äôs Theorem, 56
lock-washer oscillator, 65
Lorenz‚Äôs system, 32, 266
low-pass Ô¨Ålter, 10
Malkin‚Äôs Theorem, 107
Markov chain, 69, 212
Markov jump process, 242
Massera‚Äôs Inverse Theorem, 106, 120
matching conditions, 264
Mathieu‚Äôs equation, 15
mean diÔ¨Äusivity, 235
Mean Stable Averaging Theorem,
210, 225, 288
mean value, 239
merry-go-round chain, 214
Method I, 150
Method II, 150
method of invariant manifolds, 223
method of residues, 9
Michaelis‚ÄìMenten Approximation,
271
modes of rapid oscillation, 200
modiÔ¨Åed perturbation method, 172,
173, 175, 178, 182
ModiÔ¨Åed Perturbation Theorem, 159
moments, 239
Monte Carlo simulation, 70
multi-index, 20
multitime algorithm, 299
multitime method, 246
near-identity transformations, 222
Newton‚Äôs method, 187, 293
Newton‚Äôs polygon method, 131, 143
Newton‚Äôs polygons, 161
nilpotent matrix, 7
No-Retract Theorem, 139
node, 80
Nonlinear Averaging Theorem, 201
Nonlinear DiÔ¨Äerence Equation
Averaging Theorem, 245
nonlinear diÔ¨Äusion equation, 246
nonlinear renewal equation, 299
nonresonance, 167
Nonresonance Theorem, 168
nonresonant, 164
normal random variable, 240
Ohm‚Äôs Law, 3
orbit, 66, 109
orbital stability, 109
orbitally asymptotically stable, 109,
114, 288
orbitally stable, 109
orbitally stable under persistent
disturbances, 109, 111
order-of-magnitude, 147
ordinary language model, xiii
orthogonal complement, 122
oscillatory matrix, 206, 214
oscillatory modes, 8
Pad¬¥e approximation, 148
pendulum, 51
period, 281
Perron integral equations, 98, 118

314
Index
Perron‚Äôs Theorem, 98
phase equation, 35
phase locked loop, 40, 57
Phase Locking Theorem, 226
phase-amplitude coordinates, 2, 87,
215, 219
phase-amplitude variables, 51
pitchfork bifurcation, 97
PLL, 223
Poincar¬¥e‚Äôs mapping, 72, 194
Poincar¬¥e‚Äôs Twist Theorem, 74, 83,
142
Poincar¬¥e‚ÄìBendixson Theorem, 31, 84,
110
Poincar¬¥e‚ÄìBendixson‚Äôs Theory, 29
Poincar¬¥e‚ÄìLinstedt method, 178
Poisson bracket, 49
potential energy, 42
probability density function, 239, 240
probability simplex, 102, 115
probability space, 238
projection matrices, 6
pull-back mapping, 140
QSSA, 253
QSSA Theorem with T = ‚àû, 271
quasiperiodic function, 19, 220
quasistatic manifold, 265, 289
Quasistatic Manifold Theorem, 266
quasistatic oscillation, 274
quasistatic state, 264, 284
quasistatic-state approximation, 251
Quasistatic-State Approximation
Theorem, 269, 293
Quasistatic-State Theorem, 267
random processes, 240
random signals, 240
random variable, 239
reduced oscillation, 273
reduced problem, 265, 280
Regular Perturbation Theorem, 152,
156
resonance, 22, 164, 167
Resonant Forcing Theorem, 171
return mapping, 113, 119
return time, 113
Riemann‚ÄìLebesgue Lemma, 207
right boundary-layer correction, 283
RLC circuit, 3
rotation number, 39, 74, 227
rotation vector, 116, 226
rotation vector method, 116, 222
Rotational System, 212
Routh‚ÄìHurwitz Criterion, 88
saddle point, 29
saddle-node on limit cycle bifurcation,
134
saddle-saddle connection, 61
Sarkovski sequence, 66, 187
Schr¬®odinger‚Äôs equation, 14
second diÔ¨Äerence method, 236
secular term, 173
self-adjoint, 166
semi-implicit Euler numerical
algorithm, 83
several small parameters, 295
sine‚ÄìGordon equation, 282
single locus, 299
slow selection, 299
small divisor problem, 23, 200, 222
soft spring, 179
solvability conditions, 123, 129
spectral decomposition, 6, 103
spiral sink, 29
spiral source, 29
stability under persistent
disturbances, xviii, 93, 107,
133, 138, 197, 203, 250, 251,
266, 268
stable, 92
stable Ô¨Åxed point, 66
stable manifold, 8, 78, 98
stable matrix, 262
stable node, 29
stable spiral, 29
stationary phase, 152
steepest descent, 207
sticky layer, 266
stiÔ¨Äness, 292
stochastic diÔ¨Äerential equations, 240
stochastic integral, 240
strange attractor, 88
stroboscopic method, 73
structural stability, xviii, 108
subharmonics, 179
subresonance, 164, 167

Index
315
suppressing secular terms, 178
switching times, 242
Taylor‚Äôs expansion, 148
Theorem on Linear Spline
Approximation, 81
time delays, 78
toroidal clock, 38
toroidal knot, 86
total stability, 93
trace, 25, 28
transfer function, 4, 79
translation number, 18
transpose, 122
transversal homoclinic point, 78
transversal plane, 113
triangle fractal, 194
triangular coordinates, 299
two time scales, 234
two-body problem, 54
two-time scale method, 204
uniformly asymptotically stable, 93,
100, 106, 203
unstable manifold, 9, 78, 98
unstable node, 29
unstable spiral, 29
van der Pol‚Äôs Equation, 35, 57, 64,
189, 217, 280
variance, 239
variation of constants formula, 12, 23,
94
VCO, 38
VCON, 57
voltage-controlled oscillator, 2
volume-preserving mappings, 142
Wazewski‚Äôs Theorem, 140, 143
Weak Ergodic Theorem, 21
weakly connected networks, 219
Weierstrass Preparation Theorem,
131, 143
Wronskian matrix, 14




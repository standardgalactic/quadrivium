Interview with Daniel Dennett conducted by Bill Uzgalis
in Boston, Massachusetts on December 29, 2004
Bill Uzgalis
Published online: 27 April 2006
 Springer Science + Business Media B.V. 2006
Abstract
A taped conversational interview with Daniel Dennett and Bill Uzgalis covers a
wide range of topics arising from Dennett’s thoughts about computing and human beings.
The background of Dennett’s work is explored as are his views about mind-brain identity
theory, artiﬁcial intelligence, functionalism, human exceptionalism, animal culture, lan-
guage, pain, freedom and determinism, and quality of life.
Keywords
Artiﬁcial intelligence Æ Dennett Æ Freedom Æ Functionalism Æ Mind-brain
identity Æ Qualia
Editor’s note: Daniel Dennett accepted the 2003 Barwise Prize at the Eastern American
Philosophical Association meeting in Boston on December 29, 2004. He gave an address at
a special session, following which Bill Uzgalis (Oregon State University) conducted an
interview with Dennett with the intent to focus on aspects of his work related to philosophy
and computing issues. Uzgalis taped the interview, transcribed it, and shared the result with
Dennett who suggested some revisions. Dennett has given many interviews, some on na-
tional television and in popular publications. The following dialogue is the most informed
and insightful of the interviews with Dennett that this editor has read. I am conﬁdent that this
text will serve as an important source in future studies of Dennett’s philosophy.
Uzgalis: Daniel Dennett, you are the recipient of the American Philosophical Association’s
Barwise Prize for lifetime achievement in Computing and Philosophy for 2003.
Dennett: Indeed I am.
Uzgalis: Clearly this means that the APA committee that awarded the prize recognized a
broad range of achievements through a long span of time. So, let’s try and go through some
B. Uzgalis (&)
Department of Philosophy, Oregon State University, Corvallis, OR 97331-4502, USA
E-mail: wuzgalis@oregonstate.edu
123
Mind Mach (2006) 16:7–19
DOI 10.1007/s11023-006-9015-5

of those areas. You were talking earlier in your Barwise lecture about software develop-
ment for classes and I know you got into this very early. How did that happen?
Dennett: Well, I don’t know if I got into software development early. What happened was I
was teaching a course in introductory computer science as part of a team at Tufts in the
early 80’s. And two of us, George Smith and I, saw the opportunity to create software for
use at university level courses that would open up pedagogical bottlenecks where students
had a hard time understanding difﬁcult material. We could make an artifact that would
open up that space for them and help them imagine that in much more detail. So with Steve
Barney’s brilliant help we did a computer, a visible computer, you could watch it work, so
you could really understand how computers work. We did Gene Wright [named in honor of
Sewall Wright, the great population geneticist], which is a population genetics program,
which is available still. This is a wonderful way of looking at population genetics with the
help of a computer
Uzgalis: Now this was for other folks, right?
Dennett: Yeah. None of this had anything to do with philosophy, really. Both of us are
philosophers. In one way it has to do with philosophy because it has to do with how to get
people to imagine things they’re not used to imagining. We did prototypes in statistics, in
geology, in neuroscience, neuroanatomy, and others. It was a labor of love; it was won-
derful work. I’m very proud of the stuff that we did and its fall-out in subsequent years. I
wasn’t writing the programs myself, neither was George. We were the midwives, we were
the ones who would harness the talents of bright young student programmers and make
sure that the content was completely on the level and that the basic design, the pedagogy,
was sound.
Uzgalis: You’ve already mentioned the theme of your Barwise Lecture, which is that
computers aid the imagination in the way that telescopes and microscopes aid the eyes.
When did you discover that and expand that vision to philosophy?
Dennett: Well, I think I discovered the imagination-expanding power of computers back
when I was a graduate student, when Alan Anderson’s little anthology Minds and Ma-
chines came out with some classic papers in it. I devoured that and really got interested,
talked to Anderson a bit, got interested in artiﬁcial intelligence, appreciated right off that
this was another way of doing philosophy. Artiﬁcial intelligence always has been, I think, a
sort of philosophy done with the aid of computers. Artiﬁcial intelligence programs are
thought experiments. People in the ﬁeld have often been uncertain exactly what to say that
they’ve been doing. Are they modeling real phenomena? What’s the difference between
computational neuroscience and artiﬁcial intelligence? The answer is, AI is more like a
thought experiment. It’s addressing a variation on a Kantian theme, actually. That is, the
theme of how could anything possibly have experience, how could anything possibly be
intelligent? You don’t want to restrict yourself to Homo sapiens or Earthlings or any
physical thing at all. How could it be intelligent? And what are the necessary ingredients?
So there is actually an aprioristic turn to it because it’s an attempt to analyze the very idea
of intelligence and see what something would have to be and be able to do to count as
intelligent. And then, don’t just wafﬂe on about it, but actually make one of those things
and prove that it is intelligent or not.
Uzgalis: That was an insight that came to you early on?
Dennett: Back about 1963, yes.
Uzgalis: You continued to follow that line?
Dennett: Yeah, I was very fortunate. I went to Irvine, my ﬁrst job in ‘65, which had a few
good AI people in it—there weren’t that many in the world then—but several of them,
8
B. Uzgalis
123

Julian Feldman and Fred Tonge, in particular, were actually at Irvine. And they took me
under their wing and taught me a lot and introduced me to Allen Newell and other people
in the ﬁeld, and Feldman was the coeditor of the famous Feigenbaum and Feldman
Computers and Thought anthology. So that put me in touch with all those people. And one
day Feldman came to my ofﬁce in a stew, and said, ‘‘What do you make of this?’’ And he
threw down on my desk a preprint of a memo from the RAND Corp. by Hubert Dreyfus,
called ‘‘Alchemy and Artiﬁcial Intelligence,’’ which was basically the backbone of his
famous book, What Computers Can’t Do. He said, ‘‘What do you make of this?’’ I read
through it and said, ‘‘Well it’s clever but it’s wrong, I think.’’ ‘‘Say why, say why.’’ And
so I was enlisted in 1966 by the AI community as a sort of rebutter or refuter of Dreyfus
right from the get-go. And in fact my ﬁrst publication was in the journal Behavioral
Science, in the Computers and Behavioral Science section: ‘‘Machine Traces and Protocol
Statements.’’ It was a critique of the RAND memo of Dreyfus, saying where he went off
base and what was wrong with his critique. Now there were many things that were actually
right about what Bert had to say, but he made what I consider a fundamental philosophical
mistake of inﬂating claims about difﬁculty into claims about impossibility. He was utterly
right about what the hard problems were and about how a lot of the work in AI was on the
wrong problems, was on problems that were impressive in one regard but were easy from
the point of view of AI, whereas the hard ones were about tying your shoes and getting
about in the world, not playing chess or answering questions about baseball. He overstated
the case and the overstatement was what drew the ire and ﬁre of the people in AI. So over
the years I’ve always agreed with Bert about a lot of his diagnoses of the shortcomings and
foibles of AI but not about his grand diagnosis that AI is ﬂat impossible. No, it’s just hard,
and harder than the people in the ﬁeld think, absolutely true, but not impossible.
Uzgalis: At some point in that history, you went to England and studied with Gilbert Ryle.
Dennett: That was before that. I did my graduate with Gilbert Ryle back in ‘63–65, but at
that point I had never touched a computer. I had nothing to do with computers until I got to
Irvine, I mean I had read about them, but I had never touched a computer or tried to write a
program, anything of the sort until ‘65. Of course back then, computers were in large, air-
conditioned rooms. It wasn’t as if computers were on every desk. You had to go to some
trouble to touch computers back in those days.
Uzgalis: Did Hilary Putnam’s work inﬂuence you?
Dennett: Yes, indeed. While I was a graduate student, back in the sixties, I read his classic
‘‘Minds and Machines’’ paper, which I thought was brilliant and a real changer of the
nature of the game. So I began writing my dissertation using that paper as a central pivot
point and he kept writing more papers. There was a whole series of those papers. And no
sooner would I have extrapolated his position into a little new territory, feeling very good
about that, then the next paper would arrive and he would’ve gone that far and farther. So I
was trying to play catch-up with him. Those were, I think, the most important papers. That
and Jack Smart’s ‘‘Sensations and Brain Processes.’’ Those were the most important
papers in philosophy of mind. Not just in the 60s, I think they were the most important
papers in the century because they really did open up new territory. So I did my best to put
my take on those papers and build on them. That was in my dissertation, which eventually
turned into Content and Consciousness.
Uzgalis: The Smart paper sets forth the mind-brain identity theory.
Dennett: That’s right, and it has the lovely idea of topic-neutral sentences in it, the idea that
you can make a machine that can answer the question ‘‘I know this is like that, but I don’t
know in what regard. I just know they’re alike.’’ Jack Smart did that without any particular
Interview with Daniel Dennett
9
123

knowledge about computers, but it was a sure-footed insight into how computers do things.
They have competences that they don’t have to understand. And that’s the way to make
progress on the mind. You have to make a mind out of things that have mind-like com-
petences that they don’t understand, because they’re just simple material things. So, the
idea of making a whole mind out of quasi-, pseudo-, proto-, semi-, hemi-, demi-, pseudo-
minds—the fundamental insight of cognitive science is that by going through a series of
levels you can get mere mechanical matter, as in Leibniz’s mill, to do the work of an
intelligent mind, with comprehension and consciousness in a material thing.
Uzgalis: I take it that of those two strands—the Putnam strand and the mind-brain identity
theory, that you’ve rejected the mind-brain identity theory?
Dennett: I did for roundabout reasons, but it was a central part to what I was up to at the
time. The ﬁrst chapter of Content of Consciousness is about physicalism without identity
theory. And mainly what it’s saying is that the things on the left and right of the equals sign
are not of the same type. It’s not that identity theory is so much false, as it has the wrong
end of the stick. Yes we want to be physicalists, yes we want to be materialists, but there
are more ways of doing that than by lining up mental candidates on the left-hand side of the
equation and physical candidates on the right-hand side of the equation and saying, ‘‘this
equals that.’’ Now, the birth of that idea was already in Jack Smart’s paper. It’s not that the
after-image is identical with something in the brain, it’s the having of the after-image that’s
identical with something in the brain. So, you don’t take the mental phenomenon apart and
say, ‘‘now, which brain process is the left-hand corner of the after-image and which brain
process is the right-hand corner of the after-image.’’ That’s already to be naı¨ve. Yes,
everything that’s true about this thing you say in the mental language is true in virtue of
something you say in the physical language. But the mapping between the mental and
physical language is not going to be straightforward, it’s not going to be one-to-one.
There’s going to be situations on the mental side of the equation which are identical with
situations on the right hand side, and that’s all you need. Well, this is an idea that has more
recently been endlessly written about in terms of supervenience, but the basic idea was
there in Smart and I pushed it a little further. I said, ‘‘Look, by the time you’ve said it’s not
the after-image I’m identifying and it’s not the having of the after image I’m identifying,
it’s the total situation of there being a person who is currently having an after-image of
type such-and-such—it’s that whole situation that is identiﬁed with the person’s body
being in such-and-such a state,’’ the initial point of the identity theory is gone—the initial
point being to take the categories that you ﬁnd in nature and say: ‘‘Oh I see, it’s one of
those.’’ It’s not that simple. You don’t point to whole situations, you point to after-images,
but when you do that you don’t ﬁnd anything on the right-hand side to identify with.
Uzgalis: Functionalism has at its core an analogy between minds and computers as the title
to Putnam’s original paper suggests. It was doing all kinds of things that took the best parts
of other theories, and answered questions other theories couldn’t answer. Dualism, for
example talks about causal interaction. Behaviorists rejected this notion in favor of talking
about the logical relation between inputs and outputs. Functionalists had the insight that
you had a causal stream between inputs and outputs and yet you could also talk about the
relation between inputs and outputs as a logical relationship. It also solved a notorious
problem with behaviorism by allowing other mental states as well as the relation between
inputs and outputs to deﬁne a particular mental state. Functionalism also allowed for
multiple-realizability and thus solved problems that the mind brain identity theory had
difﬁculties with. All of these points were developed from analogies between computers and
minds. When you consider all the problems it could solve this was an impressive
10
B. Uzgalis
123

achievement that seemed to make it the best available theory of mind we had. Then along
came the problem of qualia. How did that come to you and how did you react to it when
you heard it?
Dennett: Well, of course, I already thought I’d dealt with it in Content and Consciousness;
basically by saying that qualia is a trap that philosophers have gotten themselves into and
there really aren’t such things. What we have to explain is why people think there are. That
became more outspoken when I did ‘‘Quining Qualia,’’ which was in the 80s. But in
Brainstorms I had ‘‘Two approaches to mental images’’ and ‘‘Why you can’t make a
computer that feels pain’’ that were pushing a functionalist line and showing how to deal
with qualia. Well, over the years, I’ve always underestimated just how potent the lure of
qualia are. It is one of these fatal attractions to philosophers. They ﬁnd a path to the idea of
qualia and then they won’t let go of it. These paths are all mistaken—of course there are
pains and subjective colors, but they’re not what philosophers think they are. So I’ve
doggedly fought this battle to show people that the idea of qualia is incoherent. There are
neighboring ideas that aren’t incoherent and that aren’t any problem for functionalism; in
fact, the only way to make any sense of them is through functionalism. So, I’ve argued all
along that the phenomena that are misgathered under the rubric of ‘‘qualia’’ make beautiful
sense in functionalism and only in functionalism and that any other approach to them—the
idea that qualia are the stumbling-block of functionalism—that is itself just a huge mistake
and a perennial mistake.
For instance, in Consciousness Explained, to alert people to what was at stake, I made a
sort of joke about ﬁgment—ﬁgment was like pigment, ﬁgment was what qualia were made
of. I pointed out that either what you’re saying is there really is something like ﬁgment or
you have to admit that your view of qualia ﬁts into functionalism just ﬁne. What I’ve found
over the years is that there are a lot of people who want to say there’s ﬁgment! And what I
viewed as a reductio, that’s what they don’t want to give up. And, you know, there’s not
much you can do when you’ve run your reductio and they’ve just embraced the conclusion.
Then you realize that you really do have a deep-seated attraction that’s not going to resist
argument.
David Chalmers and I have discussed this for what seems an indeterminable number of
years, and he candidly grants that he has no arguments in favor of qualia that I haven’t
rebutted to his satisfaction, he just can’t let go of the belief in them. Okay, I’ll take him at
his word; I won’t bother trying to argue with him anymore. I can suggest therapy, maybe,
because he says that it’s not a rationally grounded belief on his part, it’s just a gut intuition
that he can’t escape. I say, all right, maybe if he changes his diet or something, I don’t
know.
Uzgalis: Let me take another turn. At some point, in the late 70s or early 80s, Jerry Fodor
published his book, The Language of Thought.
Dennett: 1975, actually.
Uzgalis: And that was an expression of strong AI.
Dennett: Not really. Jerry wouldn’t let you say that. One of the things that puzzled me for
years was how Jerry could write the Language of Thought and be an ardent, deﬁning
proponent of the language of thought hypothesis and not be interested in strong AI. But he
never was; he always was opposed to it, which is really perplexing when you think about it.
How can he not be interested particularly in Good-Old-Fashioned AI, which was the
attempt really to describe the language of thought—to work it out? Beliefs were sentences
in the head and they were Lisp expressions or something similar, or if you look at a
computer language like Prolog—that’s it, that’s supposed to be the language of thought!
Interview with Daniel Dennett
11
123

And Jerry, from the outset, expressed no interest or enthusiasm about any of this. So, what
on earth was going on? It’s an interesting question. He didn’t want to pursue that ave-
nue—it’s still an interesting question of ‘‘Why not?’’
Uzgalis: The thought I was going down was that, as you said, the language of thought
hypothesis is that underlying sentences is activity in the brain that corresponds to it. And, at
least some folks, like the Churchlands, have just rejected that view altogether. You seem to
hold a kind of middle position.
Dennett: Yeah, ﬁrst of all I certainly would join in dismissing any simplistic language of
thought hypothesis, which supposes that the sentences are like written sentences in the
natural language—that they have subjects and predicates and a linear structure. That thesis
is, I think, bizarrely unrealistic, but now start tempering that thesis and modulating it. Once
a sentence in the language of thought doesn’t have to be linear—it can be multidimen-
sional—and can have variables instead of adjectives and doesn’t have compositional
grammar, but has some other way—though it does have to be compositional in some
way—but it doesn’t have a syntax, well now what you’re saying is—doesn’t there have to
be a system of representation? And I think that’s true. But then it’s not clear that the
Churchlands don’t believe that too. So, the question is ‘‘What kinds of systems of rep-
resentation do we have in the brain?’’ is still a frustratingly unanswered question. People
have been nibbling away at this for 40 years and we’ve only ruled out the most simple-
minded versions, but there’s plenty that we haven’t properly tested because we haven’t
properly formulated them.
Uzgalis: Part of this debate has gone on in terms of computer models, yes?
Dennett: Oh yes, and for a very good reason—without the computer we wouldn’t know
how to take these ideas seriously. With the computer we now do have a general conﬁdence
in our ability to model tremendously intricate, convoluted, and multi-variable phenomena
in a formal way. And in this vast space of possibility, we don’t know what the biologically
plausible ones are and we don’t know what the really powerful ones are. For some people,
it looks as if you have this huge database of fundamentally inert data-structures and then
you have an inference engine that chugs along, maintaining order and consistency and
generating further implications. That’s ‘‘GOFAI,’’ Good Old-Fashioned AI, and it has
many virtues. First of all, at least little bits of it you can actually do. There are models that
actually do the work. You know that the proofs work. You know that they generate
inferences that they can actually accomplish. And that’s not nothing. That’s inference-
engines replacing human thinkers in some pretty impressive domains. That gives us an
existence proof that it’s possible to order mechanical systems so that they do serious
inference, meaning inference in the broadest sense—transition from one state to another
where the information gets put in a new form where it’s more useful for current purposes,
say. On many fronts, we’ve learned how to manipulate representations and extract infor-
mation from them, and a lot of it isn’t really AI, it’s computer graphics or automated map
analysis or data analysis of every imaginable sort.
Out of that work a growing arsenal of tools is being developed, which are the right sort of
tools to do this sort of work of intelligence. How to put them together—we’ve got all these
interesting new competences embodied in tissues we’ve learned to make, but the larger
architecture is still eluding us. It’s as if you want to build an airplane—a ﬂying machine. If
you try to build it out of bricks and mortar—that’s going to be difﬁcult. If you get alu-
minum, that’s pretty good, it’s in the right direction and it’s a material that has a lot of the
properties you’re going to need but it’s not an airplane, it’s just a piece of aluminum.
We’ve got a lot of nice fabrics now, we’ve got great connectionist fabrics, pattern
12
B. Uzgalis
123

recognition fabrics, little inference engine fabrics, we’ve got data structures which have the
right sorts of addressability conditions, we’ve got memories with very nice properties, they
degrade gracefully, they permit learning. We’ve got all kinds of chunks and bits of vision
systems and auditory analysis systems and behavioral control systems, feedback loops.
We’ve got a huge warehouse of gear and now we need to put it all together.
Uzgalis: It seems to me that as the ﬁeld of AI has developed—you started out with things
like theorem proving and chess playing, and now people are talking much more about
action, dynamic systems—is that what this collection of stuff is building towards?
Dennett: This goes back to one of Burt Dreyfus’ correct assertions, namely that embodied
agents that have to deal with complexities of the real world in real time set challenges
which are ill-addressed by the walking-encyclopedia models that AI was developing from
the top down. Let me tell a little autobiographical bit: Zenon Pylyshyn, one of the won-
derful early theorists of cognitive science, wrote a piece for Behavioral and Brain Sciences
about AI, its strengths and weaknesses. In my commentary I said, ‘‘Here’s another ap-
proach entirely. Instead of trying to model some isolated human microcompetence like
playing chess or answering questions about moon rocks, try to model a whole human
organism, but get your simplicity by making it a simple organism. How simple? Bacteria?
No. Starﬁsh? You can go a little higher than that, one hopes. But it doesn’t have to be real,
it could be a three-wheeled Martian iguana, but it has to fend for itself in real-time, it has to
deal with its actual problems but otherwise it can be as unlike us as you like, and it’s just an
interesting challenge. That piece was called, ‘‘Why Not the Whole Iguana?’’ You do the
whole agent, not just parts, and get your simplicity by making it simpler and simpler and
simpler, until you have a simple enough agent that you can do the whole thing. Well, in
fact, that two-page commentary sparked a lot of imaginations and around the world various
people said, ‘‘Right, let’s build the whole iguana.’’
Uzgalis: Rodney Brooks?
Dennett: Rod Brooks and his insects was one example, but there were actually quite a few
others. So many good ones that we had a conference in the Canary Islands a few years ago
called, ‘‘Towards the Whole Iguana’’ and we had people from around the world who, in
their different ways, were trying to make whole iguanas—robots that could survive in their
limited toy worlds and actually accomplish in real-time the sorts of things that one has to
accomplish. The basic insight of it is supposedly that every creature, from the most
mandarin chess-playing philosopher/astronomer to the cockroach, has got to get through
life moment to moment and its got to have an architecture that permits it to preserve itself
and maybe have a little spare time over for doing deep thinking. Let’s ﬁnd out what that
architecture is and then build any deep thinking on top of that and it’ll probably take
advantage of a lot of the competence it has for getting through life simply in order to
understand the fancy things it’s thinking about too. This is very much the embodied
approach. It might be wrong. It might be that you can actually get an adroit, sympathetic,
high-level conscious understanding in an entity which has no body and doesn’t worry about
getting fed at all, doesn’t worry about predators, it just lives completely bed-ridden as a
sort of super-intelligence. But, very few people, I think, believe that.
Uzgalis: Let me pick up another stream. I talked to a couple of my colleagues about you,
and they remarked that amongst the most important things that they thought you had done
was the idea of the intentional stance. And it seems to tie in with what we were just talking
about, because I guess part of the idea is that you can have different hierarchies of
machines with different properties so that, too, ties in with computers, yes?
Dennett: Oh, yes. The basic idea here was to recognize that there were different
Interview with Daniel Dennett
13
123

perspectives that you can adopt to something complicated. You could treat it just as a
physical object obeying the laws of physics. You could treat it as an artifact of some sort,
something that was designed, which means that you can use means/ends analysis. You can
use optimality models, you can think of this as not just matter in motion but as things
which have functions. You can be a functionalist. Then there’s a higher stance still. Higher
in the sense of more abstract, less committed to the details, and taking on an even riskier
assumption: mainly, that the designed thing has been designed to extract information from
the world and use it to improve its ends, whatever they are. So it has beliefs, desires, and
it’s rational. Once you look at it that way, you realize that that perspective is the per-
spective of folk psychology; it is the perspective of mentalism. But my contribution was to
say, notice that this mentalism isn’t anthropomorphism so much as it’s rational-agentism,
and it’s an engineering simpliﬁcation. It can be a short-cut way of getting a grip on the
complexities of a very complex thing by presupposing that this complex thing is a rational
agent with beliefs and desires. Of course, my favorite example is the chess-playing
computer. You don’t have to know what the program is, you don’t have to know what the
implementation is, if you just know it’s a chess-playing computer, you can predict its
moves like a bandit. You can predict that if you leave an unguarded bishop over there, it
will be snatched up at the ﬁrst opportunity because that’s the best move going and it’s
going to see that. This is tremendous predictive leverage, but nothing’s for free—the price
you pay for this is taking on assumptions that could be false if it’s not as rational as you
think it is. But this simplifying move is undeniable, and in fact it’s used by chess playing
computers. How do chess playing computers work? When they’re doing the game tree,
when they’re trying to look ahead, they eliminate the stupid moves that the opponent
would make, and they don’t bother checking out the paths of the responses that are really
stupid, and this is risky. Because maybe a move that they think is stupid is, at some deeper
level, smart. Or maybe it’s just plain stupid but in any case it takes the opponent into a
different part of search space, one that they haven’t investigated. And once you start
looking, you see this strategy of adopting the intentional stance everywhere in nature, and
we’re just a very extreme case of this when we’re folk psychologists, when we become
higher-order intentional systems who attribute to others beliefs and desires—it’s not just
that there are beliefs and desires that we attribute to others. We have beliefs and desires
about the beliefs and desires of others and we have beliefs and desires about the beliefs and
desires about the beliefs and desires, and so forth. And it’s this iteration, this recursion on
the fundamental strategy, that makes us the complicated minds that we are.
Galas: If we go back to Descartes, he thought that there was a huge metaphysical gulf
between human beings and other creatures in nature. The result of a couple of hundred
years of investigating nature and human beings is that we’ve come to the conclusion that
the line is much blurrier. You can ﬁnd different human capacities out there in nature, but it
seems like we have a particular bundle that is unique.
Dennett: Yeah, I think that the level of anxiety about human specialness–exceptional-
ism—is curious, as if there had to be one simple answer to this. Yeah, yeah, we’re
mammals and what’s true of mammals is true of us and we’re not that special in any way,
and then there are some things that are really unique about us. Exceptionalism is true in
some regards and false in others. The main thing is to realize that it’s not at its basis a
fundamental divide in nature. It’s not that one species has an immaterial and eternal soul
and the rest don’t. No, it’s just in virtue of a particular set, a convergence, of competences
that distinguish us from other species. But once that distinction is there, it takes off and it
becomes explosively different. So I think we really are explosively different from other
14
B. Uzgalis
123

species, not metaphysically, but in terms of what makes our minds. It’s what makes us
moral agents and not just moral patients. What makes ‘‘creatures with free will’’ a real and
interesting problem, is that we are capable of so much more than other organisms.
Uzgalis: Take the traditional philosophical distinctions—rationality, ethics, culture—and
it’s turning out that those lines are somewhat blurred too. If you take culture, for example,
primatologists are now arguing that apes actually have culture.
Dennett: Oh yeah, sure they do. But as is usual in biology, these things are not absolute
bright lines, but nevertheless these things are step functions. Animals have culture. Avital
and Jablonka’s Animal Tradition is a lovely book on this, Bonner has a nice book on this,
The Evolution of Culture in Animals, I think is the title.
Uzgalis: Yes, Franz De Waal’s book The Ape and The Sushi Master...
Dennett: Yes, there are other books on this, but that’s like the difference between having a
tin whistle and an orchestra. Yeah, they have a little bit of culture, but it doesn’t go
explosive, it doesn’t build on itself the way ours does. There are a few items of cultural
transmission. Maybe there are a hundred more that we haven’t recognized, maybe two
hundred. But we’ve got millions or billions of items of cultural transmission; no other
species has that.
Uzgalis: Yes, I’m sure that’s right. And I suppose that’s again a function of this unique
collection of capacities and powers.
Dennett: Yes, it’s the generative power that makes all the difference.
Uzgalis: What are your views about language in this regard?
Dennett: I think language is at the heart of it. It is language that gives our minds their
incredible power. I don’t think there’s anything even unobvious about this. Language is
what gives our minds both certain sorts of discipline so that they can simply have habits of
thought that are inaccessible to other creatures, and it gives us, of course, the composi-
tionality to think thoughts that are just unthinkable without language and to transmit them
so that we get to beneﬁt from the discoveries of everybody else automatically. So it makes
possible both the division of labor and the uniﬁcation. You don’t have to depend upon what
you can learn in your own lifetime in your own personal experience.
Uzgalis: Underlying language, there must be a whole series of very strange powers and
capacities again. The capacity to abstract and have concepts—
Dennett: Not so much to have concepts as to manipulate concepts. Does a polar bear have
the concept of snow? Well my gosh, polar bears are pretty competent vis-a-vis snow. You
might say that that in itself implies that they have concepts of snow, but I guess that they
can’t think about snow the way that we can, even if we’ve never seen snow. We need a
whole new vocabulary to talk about concepts, schmoncepts, pseudo-concepts. I think the
term ‘‘concept’’ is a pretty ramshackle theoretical concept.
Uzgalis: I’m thinking of things like perceptual concepts, where you look at a tree from one
angle, or you look at a tree from another angle and somehow you can ﬁgure out that it’s the
same thing. That seems problematic.
Dennett: Yes, I agree. That’s part of what’s ramshackle about the concept of ‘‘concept.’’
For many purposes I think it’s easy enough to say that a concept is just a word without its
pronunciation. It’s what ‘‘chaise’’ and ‘‘chair’’ and ‘‘Stuhl’’ have in common. It’s the
concept of a chair. It’s not the word, it’s the meaning that the words share. And that’s not
bad for getting us into an interesting territory of thought. But to try to elevate that to the
level of ‘‘atom’’ or ‘‘molecule,’’ to treat it as a really good fundamental category—I think
that’s a mistake.
Uzgalis: So, do you think Kant is a little too insistent on that?
Interview with Daniel Dennett
15
123

Dennett: Yes. I think Kant, like every other philosopher before the mid-20th Century has to
work from a base of folk psychology only and is not going to get any help at all from the
brain. There’s just no other way of approaching these issues but from the folk psycho-
logical perspective. We’ve only begun developing other paths into these phenomena.
Uzgalis: If we go back for a second to talking about computers and the kind of organisms
we are, you mentioned one of your Brainstorm essays about why computers can’t feel pain.
That seems like an enormously signiﬁcant difference between biological organisms and
artiﬁcial ones.
Dennett: That’s why I like the title of that piece, because it’s so ironic: people think that
what I’m trying to show is that there’s an enormous difference, and of course, what I’m
trying to show is that there isn’t. The whole point of the essay is to show that the only
reason that computers can’t feel pain is that the concept of pain is subtly incoherent, and,
you know, we can’t feel pain either, given the way pain is deﬁned, and once we get clearer
about the phenomenon of pain we’ll see no problem making a robot that feels pain. And
indeed, I think we’re well on the way to giving COG a good pain system. So the problem is
that it would’ve been criticized, ‘‘That’s not real pain,’’ philosophers might say, and they
would be able to make a pretty good case, because they could ﬁnd some features that they
said were somehow essential or deﬁnitional for pain that were missing, but you could add
those, and when you add those you’d end up with a self-contradictory concept one way or
another, but what they would fail to realize is that that concept is just as self-contradictory
when applied to us. After all, much of that paper is concerned with showing how inco-
herent the everyday concept of pain is. Computers obviously can’t exhibit any phenom-
enon that can’t be coherently described.
Uzgalis: I suppose this is a general problem. When people say, ‘‘Computers can’t do this,’’
often enough what you ﬁnd is they can’t say what ‘‘this’’ is in the ﬁrst place.
Dennett: Yeah, and that’s why it’s such an easy and tempting thing to say—because you
can hide behind the difﬁculties of deﬁnition and cling to your plausibility. And whenever
people respond more constructively and spell out what it is that computers can’t do, the
very spelling-out pretty much guarantees that they can. It’s a recipe for getting computers
to do what you said they can’t do.
Uzgalis: And I suppose that’s one of the lovely things about the whole exercise.
Dennett: I think it is. I think computers call the bluff of philosophers wonderfully in these
regards. You say the computer can’t do X? Well, tell me more exactly what you mean by
X. And once you’ve got it clear—and this is quite independent of the particular com-
puter—just getting really clear what would count as something ‘‘having the compe-
tence’’—once you got really clear about that, I’ll make you something that’s got that
competence.
Uzgalis: In my Introduction to Philosophy class, I often talk to my students about com-
puters and free will (we’ve usually gone through the free will and determinism
part—libertarianism and hard determinism and compatibilism—before we get to the phi-
losophy of mind), and I say ‘‘I’ll bet you think computers don’t have free will.’’ And
they’ll say, ‘‘Yeah, sure.’’ ‘‘Well,’’ I tell them, ‘‘think back through the various theories
that we’ve talked about—libertarianism, hard determinism and compatibilism—if you take
libertarianism—what you say is likely correct because it is a non-deterministic account of
freedom. But libertarianism is a pretty crazy theory even about people. On the other hand,
if you take compatibilism—it’s a deterministic theory and computers are deterministic
systems. So, as long as you can give a computer a desire, you can pretty easily conceive of
it having freedom, in that compatibilist sense.’’
16
B. Uzgalis
123

Dennett: Sure, that’s what I’ve argued in two books.
Uzgalis: So, that is generally the line that you want to go down?
Dennett: In Elbow Room, I gave the example of the robot babysitter, and said, ‘‘You can
have a deterministic or indeterministic one. They will otherwise have the same compe-
tences—these are your kids they’re taking care of—you want them to be responsible, you
want them to have freedom—is there any reason to prefer the one with the indetermin-
istic—genuinely random—control structure as opposed to the pseudo-random one? The
answer is ‘‘No, none at all.’’ If one’s free, the other’s free. The difference couldn’t come
from whether or not quantum indeterminism reigned, that’s just not the right concept of
freedom.
Uzgalis: I guess there is a regularity to freedom that that quantum notion just simply
doesn’t capture.
Dennett: It doesn’t capture anything that you want in freedom. It’s stunning how close to
the surface the systematic futility of quantum physics as a source of the free will problem
is, and people keep routinely forgetting this. If a decision is genuinely quantum
mechanically random then it can’t be your decision, in any interesting sense. You can’t be
responsible for it because it just happened. Now, I go into some length about this looking at
Robert Kane’s model—what’s good about this model is it can be had in a deterministic
variant and there’s no reason to prefer his indeterministic variant over the deterministic
one.
Uzgalis: Well, it seems like we’ve covered a whole range of topics here that are connected
with computers, can you think of others that we’ve missed.
Dennett: I don’t know, let’s see. Well on a darker note I’ve written a bit about the way in
which computer technology can make it harder to lead an interesting life by taking away
some of the thrills and adventures that were otherwise possible. I’ll start with something
trivial and end with something not trivial at all. In my youth, I learned celestial navigation,
how to use a sextant, how to compute HO214 method my position on the surface of the
globe from a few sights and a good chronometer, and the nautical almanac, and I loved
being able to do that. I dreamt of exploiting my hard-won knowledge someday sailing
single-handed across the Atlantic in a sailboat and doing my own navigation. I could still
do that today, but it would be foolish because for $150 you can take along a couple of GPS
units which will do all of that for you to much greater accuracy than you can do on your
own. And if you’ve got your boat insured, they won’t let you go without taking these with
you. Now, you can defy your insurance company and you can brave it, you can go out
there, it’s sort of like using ﬁre by friction to light your campﬁre—if you really want to do
it instead of using matches you can—but it’s sort of weird, it’d be a historical curiosity
today. All of that hard work that’s taken from you. You can still sail across the Atlantic,
it’d be pretty challenging, but part of that thrill is no longer available, practically.
But that’s trivial, as I said. Let’s apply it to medicine. Diagnosticians used to have really
wonderful mental exercises of diagnostics where physicians using Sherlock Homes-like
powers would ﬁgure out what little clues were available and they could make brilliant
diagnoses and, add that to the bedside manner, there was the possibility to be an artful and
creative and ingenious doctor—that’s why a lot of people went into medicine. Nowadays,
more and more, what the doctor does is simply plug you into one machine or another and
wait for the results. The diagnostic skills are much less in use. Neuropsychology used to be
stunning—the cleverness and ingenuity with which neuropsychologists could ﬁgure out
from simply studying the symptomatology of the patient, running a few simple bedside
tests on the patient, could say this patient has brain damage in this particular location of the
Interview with Daniel Dennett
17
123

brain—it’s on the left side, it’s right here or there, not involved in this area, blah blah blah.
And the surgeons would go in and sure enough that would be exactly where the problem
was. Now, of course you just put them in the scanner and you ﬁnd that out immediately.
You don’t have to have that diagnostic skill. Well, if occupations as swashbuckling, as
intellectually-adventurous, as brain surgeon, neuropsychologist, and physician can be
threatened with growing routinization and cutting down the role of human responsibility,
then I think this is actually something to worry about. It reminds me of the doorman at a
fancy apartment building—entirely ceremonial role—you don’t really need a doorman. It’s
sort of nice, I suppose, when he says ‘‘Good morning,’’ and he holds the door, but you can
have a door opener there, it’ll work just ﬁne and it’ll have the cab called and even the
security angles—it’s becoming a sort of ostentatious affectation to pay a doorman, and it’s
not really a nice job. In the past, there was a time when the concierge had quite a lot of
responsibility and could really be clever and would really be important and your life could
depend on him in various ways, not any more. The role has been turned into largely
ceremony. And that could happen to doctors; they could become medical doormen. The
rich would have their personal physicians, the rest of us would just plug into the machine
and get the diagnosis. We should look more closely at the ways in which our still-
exploding-by-orders-of-magnitude-computer-power growing also diminishes the chances
of adventure in life. Believe me, as someone who has undergone a triple bypass, I took
great solace in the fact that I was being treated like a Perdue chicken that was routinized
and this was just another day in the ofﬁce for these people. And I was quite content; I
didn’t want some adventuring aviator-type, you know, here comes the Mad Baron to do my
heart surgery. Heck no! I was quite happy to have this as mechanized and routinized and as
dull as possible. And that’s why, of course, we like all of this of technology. We like GPS
for our boats, but we should tally up the cost in intellectual thrills. It’s sort of too bad that
you can’t really have the intellectual thrill of celestial navigation that your life depends on
anymore. You can do it—going out to sea and deliberately throwing your GPS units
overboard and pulling out your sextant and wind-up watch—yeah you can do that, you can
throw your radio overboard too, and then you can be really roughing it, but it’s not like real
exploring in the wilderness, it’s like roughing it at a national park. You know that there’s a
helicopter to take you away if you break your leg just around the corner.
We’ve had a lot of conversation, I’m happy with that.
Uzgalis: Good! I think our conversation has shown me how central and important com-
puters have been in your philosophical thinking and I want to congratulate you one more
time on receiving the Barwise Prize.
Dennett: I appreciate it, I really do.
References
Avital, E., & Jablonka, E. (2000). Animal traditions: Behavioural inheritance in evolution. Cambridge:
Cambridge University Press.
Andreson, A. R. (1964). Minds and machines. Engelwood Cliffs: Prentice Hall.
Dennett, D. (1968). Machine traces and protocol statements. Behavioral Science, 13(2), 155–161.
Dennett, D. (1969). Content and consciousness. London: Routledge & K. Paul.
Dennett, D. (1978). Brainstorms: Philosophical essays on mind and psychology. Montgomery: Bradford
Books.
Dennett, D. (1984). Elbow room: The varieties of free will worth wanting. Cambridge: MIT Press.
Dennett, D. (1991). Consciousness explained. Boston: Little, Brown and Co.
18
B. Uzgalis
123

De Waal, F. (2001). The Ape and the Sushi Master: Cultural reﬂections of a primatologist. New York: Basic
Books.
Dreyfus, H. (1972). What computers can’t do: A critique of artiﬁcial reason. New York: Harper & Row.
Feigenbaum, E.A., & Feldman, J. (Eds.) (1963). Computers and thought. New York: McGraw-Hill.
Fodor, J. (1975). The language of thought. New York: Crowell.
Pylyshyn, Z. W. (1984). Computation and cognition: Toward a foundation for cognitive science. Cam-
bridge: MIT Press.
Interview with Daniel Dennett
19
123


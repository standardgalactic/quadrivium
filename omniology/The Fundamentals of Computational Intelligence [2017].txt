Studies in Computational Intelligence 652
Mikhail Z. Zgurovsky
Yuriy P. Zaychenko
The Fundamentals 
of Computational 
Intelligence: 
System 
Approach

Studies in Computational Intelligence
Volume 652
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Studies in Computational Intelligence” (SCI) publishes new develop-
ments and advances in the various areas of computational intelligence—quickly and
with a high quality. The intent is to cover the theory, applications, and design
methods of computational intelligence, as embedded in the ﬁelds of engineering,
computer science, physics and life sciences, as well as the methodologies behind
them. The series contains monographs, lecture notes and edited volumes in
computational intelligence spanning the areas of neural networks, connectionist
systems, genetic algorithms, evolutionary computation, artiﬁcial intelligence,
cellular automata, self-organizing systems, soft computing, fuzzy systems, and
hybrid intelligent systems. Of particular value to both the contributors and the
readership are the short publication timeframe and the worldwide distribution,
which enable both wide and rapid dissemination of research output.
More information about this series at http://www.springer.com/series/7092

Mikhail Z. Zgurovsky
• Yuriy P. Zaychenko
The Fundamentals
of Computational
Intelligence: System
Approach
123

Mikhail Z. Zgurovsky
National Technical University of Ukraine
Kiev
Ukraine
Yuriy P. Zaychenko
Institute for Applied System Analysis
National Technical University of Ukraine
“Kiev Polytechnic Institute”
Kiev
Ukraine
ISSN 1860-949X
ISSN 1860-9503
(electronic)
Studies in Computational Intelligence
ISBN 978-3-319-35160-5
ISBN 978-3-319-35162-9
(eBook)
DOI 10.1007/978-3-319-35162-9
Library of Congress Control Number: 2016939572
© Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG Switzerland

Preface
The intention of mankind to reveal the essence of brain intelligence and utilize it in
computers arose already at the initial stage of cybernetics. These aspirations led to
the appearance of new scientiﬁc ﬁeld—artiﬁcial intelligence (AI) in the middle
of the twentieth century. The main goal of new science became the detection of
brain work mechanism and development on this base artiﬁcial systems for creative
processes automation in information processing and decision-making.
In the past 60 years artiﬁcial intelligence has passed complex way of its
evolutionary development. On this way were both signiﬁcant achievements and
failures. During the ﬁrst 40 years of its development in AI several scientiﬁc
branches were formed such as knowledge bases, expert systems, pattern recogni-
tion, neural networks, fuzzy logic systems, learning and self-learning, self-
organization, robotics, etc.
In the 1990s by integrating various AI technologies and methods new scientiﬁc
branch in AI was formed which was called “computational intelligence.” This
branch appeared as a consequence of practical problems which could not be solved
using conventional approaches and methods. In particular these problems have
occured while solving the problems of data mining, problems of decision-making
under uncertainty, so-called ill-structured problems for which the crisp problem
statement was impossible.
The suggested monograph is dedicated to systematic presentation of main trends,
approaches, and technologies of computational intelligence (CI). The introduction
includes brief review of CI history, the authors’ interpretation of CI, the analysis of
main CI components: technologies, models, methods, and applications. The inter-
connections among these components are considered and relations between CI and
soft computing are indicated.
Signiﬁcant attention in the book is paid to analysis of the ﬁrst CI technology—
neural networks. The classical neural network backpropagation (NN BP) is
described, the main training algorithms are considered. The important class of
neural networks—with radial basic functions is described and its properties are
compared with NNBP. The class of neural networks with backfeed—Hopﬁeld and
v

Hamming are described and their properties and methods of weights adjustment are
considered. The results of experimental investigations of these networks in the
problem of images recognition under high level of noise are presented and com-
pared. NN with self-organization by Kohonen are considered, its architecture and
properties are described and various algorithms of self-organization are analyzed.
The application of Kohonen neural networks in the problems of automatic classi-
ﬁcation and multidimensional visualization is considered.
Great attention in monograph is paid to novel important CI technology—fuzzy
logic (FL) systems and fuzzy neural networks (FNN). The general description of
fuzzy logic systems is provided, main stages of fuzzy inference process and fuzzy
logic algorithms are described. The comparative analysis of fuzzy logic systems
properties is presented, their advantages and drawbacks are analyzed. On this base
the integration of two CI technologies—NN and FL was performed and as a result
the new CI technology was created—fuzzy neural networks. Different FNN are
described and their training algorithms are considered and compared.
New class of FNN—cascade neo-fuzzy neural networks (CNFNN) are consid-
ered, its architecture, properties, and training algorithms are analyzed. The appli-
cations of FNN to the forecast in economy and at stock markets are presented. The
most efﬁcient algorithms of fuzzy inference for the problem of forecasting in
economy and ﬁnancial sphere are determined.
The problem of investment portfolio optimization under uncertainty is consid-
ered. The classical portfolio optimization problem by Markovitz is described, its
advantages and drawbacks are analyzed. The new problem statement of portfolio
optimization under uncertainty is considered, which is free of drawbacks of clas-
sical model. The novel theory of fuzzy portfolio optimization is presented. For its
solution corresponding method based on fuzzy sets approach is suggested. The
application of the elaborated theory for investment portfolios determination at
Ukrainian, Russian, and American stock exchanges is presented and analyzed.
The problem of corporations bankruptcy risk forecasting under incomplete and
fuzzy information is considered. The classical method by Altman is described and
analyzed. New methods based on fuzzy sets theory and fuzzy neural networks are
suggested. Results of fuzzy methods application for corporations bankruptcy risk
forecasting are presented, analyzed, compared with Altman method and the most
adequate method was determined.
This approach was extended to the problem of banks bankruptcy risk forecasting
under uncertainty. The experimental results of ﬁnancial state analysis and bank-
ruptcy risk forecasting of Ukrainian and leading European banks with application of
fuzzy neural networks ANFIS, TSK, and fuzzy GMDH are presented and analyzed.
The actual problem of creditability analysis of physical persons and corporations
is considered. The classical scoring method of creditability analysis is described and
its drawbacks are detected. New method of corporations creditability estimation
under uncertainty is suggested based on application of FNN. The comparative
investigations of corporations creditability forecasting using classical scoring
method and FNN are presented and discussed.
vi
Preface

The application fuzzy neural networks in the problem of pattern recognition is
considered. The applications of FNN for pattern recognition of optical images,
handwritten text, are considered.
Great attention in monograph is paid to inductive modeling method, so-called
group method of data handling (GMDH). Its main advantage lies therein it enables
to construct the structure of forecasting model automatically without participation of
an expert. By this possibility GMDH differs from other identiﬁcation methods. The
new fuzzy GMDH method suggested by authors is described which may work
under fuzzy and incomplete information. The problem of inductive models adap-
tation obtained by FGMDH is considered. The results of numerous experimental
investigations of GMDH for forecasting at stock exchanges in Ukraine, Russia, and
USA are presented and analyzed.
The problems of cluster analysis and automatic classiﬁcation are considered in
detail. The classical and new methods of cluster analysis based on fuzzy sets and
FNN are described and compared. The applications of cluster methods for auto-
matic classiﬁcation of UNO countries by indices of sustainable development are
presented and analyzed.
The ﬁnal chapters are devoted to theory and applications of evolutionary
modeling (EM) and genetic algorithms (GA). The general schema and main
mechanisms of GA are considered, their properties and advantages are outlined.
Special section is devoted to new extended GA. The applications of GA for
computer networks structure optimization are considered.
Basic concept and main trends in evolutionary modeling are considered. The
evolutionary strategies for artiﬁcial intelligence problems solution are presented and
discussed. Special attention is paid to the problem of accelerating the convergence
of genetic and evolutionary algorithms.
In the conclusion the perspectives of computer intelligence technologies and
methods development and implementation are outlined.
The distinguishing features of this monograph are a great number of practical
examples of CI technologies and methods and applications for solution of real
problems in economy and ﬁnancial sphere, in particular forecasting, classiﬁcation,
pattern recognition, portfolio optimization, bankruptcy risk prediction of corpora-
tions and banks under uncertainty which were developed by the authors and are
published in the book for the ﬁrst time. Just system analysis of presented experi-
mental and practical results enables to estimate the efﬁciency of the presented
methods and technologies of computational intelligence.
All CI methods and algorithms are considered from the general system approach
and the system analysis of their properties, advantages, and drawbacks is performed
that enables practicians to choose the most adequate method for their own problems
solution.
The proposed monograph is oriented ﬁrst of all to the persons who aspire to
make acquaintance with possibilities of current computer intelligence technologies
and methods and to implement them in practice. It may also serve as inquiry book
on contemporary technologies and methods of CI, it will be useful for students of
corresponding specialties.
Preface
vii

Contents
1
Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Neural Network with Back Propagation. Architecture,
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Gradient Method of Training the Neural Network Back
Propagation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.4
Gradient Training Algorithm for Networks with an Arbitrary
Number of Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.5
The Improved Gradient Training Algorithms for Neural
Network BP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.6
Acceleration of Convergence of Neural Networks Training
Algorithms. The Conjugate Gradient Algorithm. . . . . . . . . . . . .
13
1.7
Genetic Algorithm for Neural Network Training . . . . . . . . . . . .
16
1.8
Neural Networks with Radial Basis Functions
(RBF Networks) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.9
Hybrid Training Algorithm for Radial Networks . . . . . . . . . . . .
30
1.10
Methods of Selecting the Number of Radial Basis Functions. . . .
35
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2
Neural Networks with Feedback and Self-organization . . . . . . . . . .
39
2.1
Neural Network of Hopﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.1.1
Idea of Reccurrency . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.1.2
Binary Networks of Hopﬁeld. . . . . . . . . . . . . . . . . . . .
41
2.1.3
Description of the Algorithm of Asynchronous
Correction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.1.4
Patterns of Behavior of Hopﬁeld’s Network . . . . . . . . .
42
2.1.5
Application of Hopﬁeld’s Network. . . . . . . . . . . . . . . .
43
2.1.6
The Effect of “Cross–Associations” . . . . . . . . . . . . . . .
50
ix

2.2
Neural Network of Hamming. Architecture and Algorithm
of Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.2.1
Algorithm of a Hamming Network Functioning . . . . . . .
52
2.2.2
Experimental Studies of Hopﬁeld’s and Hamming’s
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.2.3
Analysis of Results . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.3
Self-organizing Neural Networks. Algorithms of Kohonen
Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
2.3.1
Learning on the Basis of Coincidence. Law
of Hebb Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
2.3.2
Competitive Learning . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.3.3
Kohonen’s Learning Law . . . . . . . . . . . . . . . . . . . . . .
62
2.3.4
Modiﬁed Competitive Learning Algorithms. . . . . . . . . .
64
2.3.5
Development of Kohonen Algorithm . . . . . . . . . . . . . .
66
2.3.6
Algorithm of Neural Gas . . . . . . . . . . . . . . . . . . . . . .
67
2.4
Application of Kohonen Neural Networks. . . . . . . . . . . . . . . . .
69
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
3
Fuzzy Inference Systems and Fuzzy Neural Networks . . . . . . . . . . .
81
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.2
Algorithms of Fuzzy Inference . . . . . . . . . . . . . . . . . . . . . . . .
82
3.2.1
Mamdani Fuzzy Inference. . . . . . . . . . . . . . . . . . . . . .
85
3.2.2
Tsukamoto Fuzzy Inference Algorithm . . . . . . . . . . . . .
85
3.2.3
Sugeno Fuzzy Inference . . . . . . . . . . . . . . . . . . . . . . .
86
3.2.4
Larsen Fuzzy Inference. . . . . . . . . . . . . . . . . . . . . . . .
87
3.3
Methods of Defuzziﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.4
Fuzzy Approximation Theorems . . . . . . . . . . . . . . . . . . . . . . .
90
3.5
Fuzzy Controller Based on Neural Networks. . . . . . . . . . . . . . .
92
3.6
The Gradient Learning Algorithm of FNN Mamdani
and Tsukamoto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
3.7
Fuzzy Neural Network ANFIS. The Structure and Learning
Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.8
Fuzzy Neural Networks TSK and Wang-Mendel . . . . . . . . . . . .
104
3.9
Adaptive Wavelet-Neuro-Fuzzy Networks. . . . . . . . . . . . . . . . .
112
3.9.1
The Architecture of Adaptive Wavelet Fuzzy
Neural Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
3.9.2
Learning of Adaptive Neuro-Fuzzy Wavelet
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
3.9.3
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
3.10
Neo-Fuzzy-Cascade Neural Networks. . . . . . . . . . . . . . . . . . . .
117
3.11
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
3.12
Conclusions from Experimental Studies . . . . . . . . . . . . . . . . . .
128
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
x
Contents

4
Application of Fuzzy Logic Systems and Fuzzy Neural
Networks in Forecasting Problems in Macroeconomy
and Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4.2
Forecasting in Macroeconomics and Financial Sector Using
Fuzzy Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
4.2.1
A Comparative Analysis of Forecasting Results
in Macroeconomics Obtained with Different FNN . . . . .
134
4.3
FNN Application for Forecasting in the Financial Sector . . . . . .
135
4.4
Predicting the Corporations Bankruptcy Risk Under
Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
4.4.1
Altman Model for Bankruptcy Risk Estimates . . . . . . . .
142
4.4.2
Forecasting the Corporation Bankruptcy Risk
on the Basis of the Fuzzy Sets Theory . . . . . . . . . . . . .
144
4.4.3
The Application of Fuzzy Neural Network in
Bankruptcy Risk Prediction Problem . . . . . . . . . . . . . .
149
4.5
Banks Financial State Analysis and Bankruptcy Forecasting . . . .
160
4.5.1
The Application of Fuzzy Neural Networks
for Financial State Forecasting. . . . . . . . . . . . . . . . . . .
161
4.5.2
The Application of Fuzzy GMDH for Financial State
Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
4.5.3
The Application of Conventional Methods
for Financial State Forecasting. . . . . . . . . . . . . . . . . . .
168
4.5.4
The Generalized Analysis of Crisp and Fuzzy
Forecasting Methods . . . . . . . . . . . . . . . . . . . . . . . . .
169
4.6
Comparative Analysis of Methods of Bankruptcy Risk
Forecasting for European Banks Under Uncertainty . . . . . . . . . .
170
4.6.1
The Application of Fuzzy GMDH for Bank
Financial State Forecasting . . . . . . . . . . . . . . . . . . . . .
173
4.6.2
Application of Linear Regression and Probabilistic
Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
4.6.3
Concluding Experiments. . . . . . . . . . . . . . . . . . . . . . .
177
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
5
Fuzzy Neural Networks in Classiﬁcation Problems . . . . . . . . . . . . .
179
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.2
FNN NEFClass. Architecture, Properties, the Algorithms
of Learning of Base Rules and Membership Functions . . . . . . . .
179
5.3
Analysis NEFClass Properties. The Modiﬁed System
NEFClassM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
5.4
Experimental Studies. Comparative Analysis of FNN
NEFClass and NEFClassM in Classiﬁcation Problems . . . . . . . .
186
5.5
Object Recognition on Electro-Optical Images
Using Fuzzy Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . .
188
Contents
xi

5.5.1
General Characteristics of the System. . . . . . . . . . . . . .
188
5.5.2
The Concept of Multi-spectral Electro-Optical
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
5.5.3
Types of Sensors. Multispectral and Hyperspectral
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
5.5.4
Principles of Imaging Systems. . . . . . . . . . . . . . . . . . .
191
5.6
Application of NEFClass in the Problem of Objects
Recognition at Electro-Optical Images . . . . . . . . . . . . . . . . . . .
192
5.6.1
Experiments to Recognize Objects in the Real Data . . . .
195
5.7
Recognition of Hand-Written Mathematical Expressions
with Application of FNN . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
6
Inductive Modeling Method (GMDH) in Problems
of Intellectual Data Analysis and Forecasting . . . . . . . . . . . . . . . . .
221
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
6.2
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
6.3
The Basic Principles of GMDH. . . . . . . . . . . . . . . . . . . . . . . .
223
6.4
Fuzzy GMDH. Principal Ideas. Interval Model of Regression . . .
225
6.5
The Description of Fuzzy Algorithm GMDH. . . . . . . . . . . . . . .
228
6.6
Analysis of Different Membership Functions . . . . . . . . . . . . . . .
228
6.7
Fuzzy GMDH with Different Partial Descriptions . . . . . . . . . . .
231
6.7.1
Investigation of Orthogonal Polynomials
as Partial Descriptions . . . . . . . . . . . . . . . . . . . . . . . .
231
6.7.2
Investigation of Trigonometric Polynomials
as Partial Descriptions . . . . . . . . . . . . . . . . . . . . . . . .
234
6.7.3
The Investigation of ARMA Models as Partial
Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
6.8
Adaptation of Fuzzy Models Coefﬁcients . . . . . . . . . . . . . . . . .
237
6.8.1
Application of Stochastic Approximation Method
for Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
6.8.2
The Application of Recurrent LSM for Model
Coefﬁcients Adaptation. . . . . . . . . . . . . . . . . . . . . . . .
240
6.9
The Application of GMDH for Forecasting at the Stock
Exchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
6.10
FGMDH Model with Fuzzy Input Data . . . . . . . . . . . . . . . . . .
252
6.10.1
FGMDH with Fuzzy Input Data and Triangular
Membership Functions . . . . . . . . . . . . . . . . . . . . . . . .
253
6.11
Experimental Investigations of GMDH and FGMDH . . . . . . . . .
256
6.12
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
260
xii
Contents

7
The Cluster Analysis in Intellectual Systems . . . . . . . . . . . . . . . . . .
261
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality
and Metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
7.2.1
Hierarchical Algorithms. Agglomerative Algorithms. . . .
265
7.2.2
Divisional Algorithms . . . . . . . . . . . . . . . . . . . . . . . .
267
7.2.3
Not Hierarchical Algorithms . . . . . . . . . . . . . . . . . . . .
267
7.3
Fuzzy C-Means Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
7.4
Deﬁnition of Initial Location of the Centers of Clusters . . . . . . .
273
7.4.1
Algorithm of Peak Grouping . . . . . . . . . . . . . . . . . . . .
273
7.4.2
Algorithm of Differential Grouping . . . . . . . . . . . . . . .
274
7.5
Gustavson-Kessel’s Fuzzy Cluster Analysis Algorithm . . . . . . . .
275
7.6
Adaptive Robust Clustering Algorithms . . . . . . . . . . . . . . . . . .
276
7.6.1
Recurrent Fuzzy Clustering Algorithms . . . . . . . . . . . .
277
7.6.2
Robust Adaptive Algorithms of Fuzzy Clustering. . . . . .
279
7.7
Robust Recursive Algorithm of Possibilistic Fuzzy
Clustering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
7.8
Application of Fuzzy Methods C-Means
and Gustavson-Kessel’s in the Problems of Automatic
Classiﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
7.9
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
8
Genetic Algorithms and Evolutionary Programing . . . . . . . . . . . . .
309
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
8.2
Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
8.2.1
Floating-Point Representations. . . . . . . . . . . . . . . . . . .
312
8.3
Mutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
8.4
Parameters Control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
8.5
Strategy Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
8.6
The Application of Genetic Algorithm in the Problem
of Networks Structural Synthesis . . . . . . . . . . . . . . . . . . . . . . .
329
8.7
Evolutionary Programing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
8.8
Differential Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty and
Its Solution with Application of Computational Intelligence
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
9.2
Direct Problem of Fuzzy Portfolio Optimization . . . . . . . . . . . .
351
9.3
Fuzzy-Set Approach with Triangular Membership Functions . . . .
352
9.4
Fuzzy-Sets Approach with Bell-Shaped Membership
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
9.5
Fuzzy-Sets Approach with Gaussian Membership Functions . . . .
359
Contents
xiii

9.6
The Analysis and Comparison of the Results Received
by Markovitz and Fuzzy-Sets Model . . . . . . . . . . . . . . . . . . . .
360
9.7
The Dual Portfolio Optimization Problem . . . . . . . . . . . . . . . . .
362
9.7.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
9.7.2
Optimality Conditions for Dual Fuzzy Portfolio
Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
9.8
The Application of FGMDH for Stock Prices Forecasting
in the Fuzzy Portfolio Optimization Problem. . . . . . . . . . . . . . .
364
9.9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
xiv
Contents

Introduction
Human desire to improve the efﬁciency of own decisions constantly stimulated the
development of appropriate computational tools and methods. These tools are
characterized by increasing automation and intellectualization of their creative
operations, which were previously considered to be the prerogative exclusively of
human being with the advent of the computer the question had arisen: Can a
machine “think”, is it possible to transfer to a computer a part of human intellectual
work? To this sphere refer modeling of certain functions of the human brain, in
particular, pattern recognition, fuzzy logic, self-learning, and other creative human
activities.
History of work in the ﬁeld of artiﬁcial intelligence counts more than 50 years.
The term artiﬁcial intelligence (AI) ﬁrst appeared in 1956, when at Dartmouth
College USA the symposium entitled “Artiﬁcial Intelligence” (AI) was held, ded-
icated to solving logic rather than computational problems using computers. This
date is the moment of the birth of a new science. Over the years, this branch of
science had passed a difﬁcult and instructive way of development. It has formed a
number of such ﬁelds, as systems based on knowledge, the logical inference, the
search for solutions, the pattern recognition systems, machine translation, machine
learning, action planning, agents and multi-agent systems, self-organization and
self-organizing systems, neural networks, fuzzy logic and fuzzy neural networks,
modeling of emotions and psyche, intellectual games, robots and robotic systems,
and others.
Currently, there are many deﬁnitions of the term Artiﬁcial Intelligence. It makes
no sense to bring them all in this book. In our view, more important is to highlight
the main features and properties that distinguish AI from conventional automation
systems for certain technologies. These properties include:
1. the presence of the goal or group of goals of functioning, the ability to set goals;
2. the ability to plan their actions to achieve the goals, ﬁnding solutions to prob-
lems that arise;
3. the ability to learn and adapt their behavior in the course of work;
4. the ability to work in poorly formalized environment in the face of uncertainty;
xv

5. work with fuzzy instructions;
6. the ability to self-organization and self-development;
7. the ability to understand natural language texts;
8. the ability to generalization and abstraction of accumulated information.
In order to create a machine that would approach in its capabilities to the prop-
erties of the human brain, it is necessary, ﬁrst of all, to understand the nature of
human intelligence, to reveal the mechanisms of human thinking. Over the past
decade, this issue was the subject of many works. Among the works that have
appeared in recent years, it is necessary to appreciate the monograph by Jeff Hawkins
and Sandra Blakeslee “On intelligence,” translated from English.—M.: Ltd. “P.H.
Williams,” 2007, in which the authors, in our opinion, had come closest to the
understanding the basis of human intelligence. In it the authors, at the intersection of
neuroscience, psychology and cybernetics have developed a pioneering theory in
which a model of the human brain, was constructed. The main functions of devel-
oped model are remembering past experiences and predicting by the brain results
of the perception of reality and its own actions. The authors presented many con-
vincing examples of human behavior in different circumstances, which support this
idea. So, Jeff Hawkins writes: “… Forecasting, in my opinion, is not just one of the
functions of the cerebral cortex, it is the primary function of the neocortex, and the
base of intelligence. The cerebral cortex is the organ of vision. If we want to
understand what is the intellect, what is creativity, how our brain works, and how to
build intelligent machines, we need to understand the nature of forecasts and how the
cortex builds them.”
Since many years of research work in the ﬁeld of AI in the early 1990s by
integrating a number of intelligent technologies and methods a new direction in the
ﬁeld of AI, called computational intelligence (CI) was created.
There are several deﬁnitions of the term computational intelligence. For the ﬁrst
time the term “computational intelligence” (CI) was introduced by Bezdek [1]
(1992, 1994), who deﬁned it as: “a system is intelligent in computational sense, if it:
operates only with digital data; has a recognition component; does not use
knowledge in the sense of artiﬁcial intelligence and in addition, when it exhibits:
(a) computing adaptability;
(b) computing fault tolerance;
(c) the error rate that approximates human performance.”
Afterward, this deﬁnition was clariﬁed and expanded. Marx in 1993 in deﬁning
CI focused on technology components of CI [2]: “… neural networks, genetic
algorithms, fuzzy systems, evolutionary programming and artiﬁcial life are the
building blocks of CI.”
Another attempt of CI deﬁnition was made by Vogel [3]: the technologies of
neural networks, fuzzy and evolutionary systems have been integrated under the
guise of “computational intelligence”—a relatively new term suggested for the
general description of the computational methods that can be used to adapt to new
problems solutions and are not based on explicit human knowledge.
xvi
Introduction

Over the years, a large number of works has appeared devoted to various areas in
the ﬁeld of CI, regularly many international conferences and congresses on
Computational Intelligence are held, IEEE International Institute publishes a special
magazine
devoted
to
the
problems
of
computational
intelligence—IEEE
Transactions on Computational Intelligence.
The analysis of these works allows us to give the following deﬁnition of CI [4].
Under computational intelligence (CI) we’ll understand the set of methods,
models, technologies and software designed to deal with informal and creative
problems in various spheres of human activity, using the apparatus of logic, which
identify to some extent mental activity of human, namely, fuzzy reasoning, quali-
tative and intuitive approach, creativity, fuzzy logic, self-learning, classiﬁcation,
pattern recognition and others.
It is worth noting the relationship between artiﬁcial intelligence (AI) and com-
putational intelligence. CI is an integral part of modern AI using special models,
methods, and technologies and focused on solving certain classes of problems.
The structure of CI areas and methods is shown in Fig. 1.
The structure CI includes the following components [4]:
• Technologies;
• Models, methods, and algorithms;
• Applied tasks.
Fig. 1 Structure of computational intelligence
Introduction
xvii

CI Technologies include:
• Neural networks (NN);
• Fuzzy logic systems (FLS) and fuzzy neural networks (FNN);
• Evolutionary modeling (EM).
CI methods and algorithms include:
• Learning methods;
• Methods of self-learning;
• The methods of self-organization;
• Genetic algorithms (GA);
• Particle swarm algorithms;
• Ant colonies algorithms.
CI technologies and techniques are used for solution of relevant applied prob-
lems of AI. It is reasonable to distinguish the following basic classes of CI applied
problems inherent of human mental activity:
• Forecasting and foresight;
• Classiﬁcation and pattern recognition;
• Clustering, spontaneous decomposition of the set of objects into classes of
similar objects;
• Data Mining;
• Decision-Making.
The term CI is close in meaning, which is widely used in foreign literature the
term “soft computing” [5], which is deﬁned as a set of models, methods, and
algorithms based on the application of fuzzy mathematics (fuzzy sets and fuzzy
logic).
The concept of soft computing was ﬁrst mentioned in the work of L. Zadeh to
analyze soft (soft) data in 1981. Soft computing (SC)—a sophisticated computer
methodology based on fuzzy logic (FL), genetic computing, neurocomputing, and
probabilistic calculations. Its components do not compete but create synergies. The
guiding principle of soft computing is accounting errors, uncertainty, and approx-
imation of partial truth to achieve robustness, low-cost solutions which are more
relevant to reality.
Four soft computing components include:
• Fuzzy logic—approximate calculations, information granulation, the calculation
in words;
• Neurocomputing—training, adaptation, classiﬁcation, system modeling, and
identiﬁcation;
• Genetic computing—synthesis, conﬁguration, and optimization using a sys-
tematic random search and evolution;
• Probability calculations—management of uncertainty, belief networks, chaotic
systems, a prediction.
xviii
Introduction

Traditional computer calculations (hard computing) is too accurate for the real
world. There are two classes of problems for soft computing. First, there are
problems, for which complete and accurate information is not available and cannot
be obtained, and, second, the problems whose deﬁnition is not sufﬁciently complete
(uncertain).
Thus, between the computational intelligence (CI) and soft computing are much
in common: common paradigms, principles, methods, technologies, and applied
problems. Differences between them, in our opinion, consist of approach, SC
focuses on methodological, philosophical, and mathematical problems, while CI
focuses mainly on the computer algorithms, technology, and the practical imple-
mentation of the relevant models and methods.
Technologies and methods of CI are widely used for applications. Thus, neural
networks (NN) and fuzzy neural networks are used to predict the nonstationary time
processes, particularly in the economy and the ﬁnancial sector.
NN and GMDH are widely used in problems of classiﬁcation and pattern
recognition problems in diagnostics, including technical and medical spheres.
Neural network self-organization, cluster analysis methods (clear and fuzzy) are
used in the automatic classiﬁcation of objects by their features of similarity—
difference.
Evolutionary modeling, genetic algorithms are used for the synthesis of complex
systems, pattern recognition, classiﬁcation, and optimization of computer networks
structure. In addition, genetic and particle swarms algorithms are widely used in
combinatorial optimization problems, in particular for graphs optimization.
Systems with fuzzy logic and FNN are effectively used for the analysis of the
ﬁnancial state of corporations, prediction of corporations and banks bankruptcy
risk, assessment of the borrowers creditworthiness under uncertainty.
Thus, summing up, it should be noted that modern CI technologies and methods
closely interact with each other. There is a deep interpenetration of methods and
algorithms of computational intelligence into the appropriate technology, and vice
versa.
Further development of computational intelligence apparently will go in several
directions.
First—it is the extension of CI applications problems, new applications in dif-
ferent tasks and subject areas, such as economy, ﬁnance, banking, telecommuni-
cation systems, process control, etc.
Second, it is the development and improvement of CI methods themselves, in
particular, genetic (GA) and evolutionary algorithms (EA), swarm optimization
algorithms, immune algorithms, ant algorithms. One promising area is the adaptation
of learning parameters of genetic and evolutionary algorithms in order to accelerate
convergence and improve the accuracy of solutions in optimization problems.
Relevant is the development and improvement of parallel genetic algorithms.
Third, it is the further integration of various CI technologies, for example, the
integration of fuzzy logic and genetic and evolutionary algorithms in problems of
decision-making and pattern recognition in conditions of incompleteness of
Introduction
xix

information and uncertainty, genetic and swarm algorithms, as well as algorithms
for their learning and self-learning.
The presentation of CI basic techniques and methods, as well as their application
to the solution of numerous application problems in various areas: forecasting,
classiﬁcation, pattern recognition, cluster analysis, and risk forecasting in industrial
and ﬁnancial spheres constitute the main content of this monograph.
xx
Introduction

Chapter 1
Neural Networks
1.1
Introduction
In this chapter the ﬁrst technology of CI-neural networks is considered.
In the Sect. 1.2 neural network Back propagation (NN BP) is considered which
is most widely used, its architecture and functions described. The important theo-
rem on universal approximation for NN BP is considered which grounds its uni-
versal applications in forecasting, pattern recognition, approximation etc. gradient
method of training neural network BP is described, its properties analyzed in
Sect. 1.3. In Sect. 1.4 the extension of gradient algorithm for training NN BP with
arbitrary number of layers is considered.
In Sect. 1.5 some modiﬁcations of gradient algorithm improving its properties
are described. In Sect. 1.6 method of conjugate gradient is considered for network
training which has accelerated convergence as compared with conventional gradient
method. In Sect. 1.7 genetic algorithm for training NNBP is considered, its prop-
erties are analyzed.
In Sect. 1.8 is presented another class of widely used neural networks with radial
basis functions. Its structure and properties are described and analyzed. Methods of
training weights and parameters of radial functions are considered. The more
general class of radial neural networks- so-called Hyper Radial Basis Function
networks (HRBF) is considered, its properties are described and analyzed. In
Sect. 1.9 efﬁcient algorithm of RBF network training- hybrid algorithm is presented
and its properties described. In the Sect. 1.10 some examples of application RBF
neural network and methods of selection the number of radial basis functions are
presented, comparison of back propagation neural networks and RBF NN is
performed.
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_1
1

1.2
Neural Network with Back Propagation. Architecture,
Functions
Most widely used and well-known neural networks are the so-called networks with
reverse propagation (“back propagation”) [1–3].
These networks predict the state of the stock exchange, recognize handwriting,
synthesize speech from text, run a motor vehicle. We will see that the back
propagation rather refers to the training algorithms, and not to the network archi-
tecture. Such a network is more properly described as a feed forward network, with
direct transmission of signals.
In Fig. 1.1 Classic three-level architecture of neural network is presented.
Designate: an input vector X ¼ x1; x2; . . .xN
f
g; an output vector Y ¼ y1;
f
y2; . . .yMg, an output vector of the hidden layer H ¼
h1; h2; . . .hj


. A neural
network executes functional transformation which can be presented as Y ¼ FðXÞ,
where X ¼ xi
f gi ¼ 1; N, Y ¼ yk
f
gk ¼ 1; M, xn þ 1 ¼ 1.
The hidden layer actually can consist of several layers, however it is possible to
suppose that it is enough to examine only three layers for a this type of networks
speciﬁcation.
For a neural network with N input nodes, J nodes of the hidden layer and M
output nodes output yk is determined so:
yk ¼ g
X
J
j¼1
Wo
jkhj
 
!
; k ¼ 1; M
ð1:1Þ
where Wo
jk is output weight of connection from the node j of the hidden layer to the
node k of the output layer; g is a function (which will be deﬁned later), executing a
mapping R1 ! R0.
Fig. 1.1 Architecture of back
propagation neural network
2
1
Neural Networks

The output signals of the hidden layer nodes hj; j ¼ 1; 2; . . .J are deﬁned by
expression:
hj ¼ r
X
N
i¼1
WI
ijxi þ WT
j
 
!
; j ¼ 1; J;
ð1:2Þ
where WI
ij is an input weight of the connection (i, j);
WT
j —threshold value (weight from a node (n + 1), which has a permanent
signal, equal 1 to node j);
the signal r at the output of the jth input node is the so-called “sigmoid”, which
is deﬁned as follows
rðxÞ ¼
1
1 þ ex
ð1:3Þ
The function r in (1.2) is called the activation function (of a neural network),
sometimes called “the function of the ignition of” a neuron.
The function g in Eq. (1.1) may be the same one as rðxÞ or another. In our
presentation we will take g as the function of type r. It’s required that the activation
function was nonlinear and had a limited output. The function rðxÞ is shown in
Fig. 1.2.
As activation functions may be also used other non-decreasing monotonous
continuous and restricted functions, namely f ðxÞ ¼ arctgðxÞ; f ðxÞ ¼ thðxÞ, where
th x is hyperbolic tangens of x and some other. The advantage of use sigmoidal
function y ¼ rðxÞ is simple computation of derivative: r0ðxÞ ¼ rðxÞð1  rðxÞÞ.
Feed Forward Neural Network
The behavior of feed forward network is determined by two factors:
• network architecture;
• the values of the weights.
The number of input and output nodes is determined a priori and, in fact, is
ﬁxed. The number of hidden nodes is variable and can be set (adjusted) by the user.
To date, the choice of hidden nodes number remains “state of art”, though in the
literature different methods of setting their number have been proposed.
Fig. 1.2 Function rðxÞ
1.2
Neural Network with Back Propagation. Architecture, Functions
3

After determining network architecture, just the values of the weights determine
its behavior.
It said that the network “trains” if the weights are modiﬁed so as to achieve the
desired goal. Here it should be borne in mind that the term “training”, borrowed
from biology, for network means simply adjustment of many parameters (con-
nection weights).
Theorem of Universal Approximation
Neuron networks Back Propagation (NNBP) are applied in various problems of
computational intelligence: forecasting, pattern recognition, approximation of
multivariate functions etc. Their wide application possibilities are determined by
their fundamental property—NNBP are universal approximators. The correspond-
ing theorem of universal approximation was proved by Gornik and others in
1989 year [4]. This theorem is generalization of theorem by Kolmogorov and
Arnold and is formulated as follows [2].
Let uð : Þ be limited, non-constant, monotonously increasing continuous
function.
Let Im0 be m0-dimensional unit hypercube ½0; 1m0 and let denote the space of
continuous functions on Im0 by symbol C Im0
ð
Þ. Then for any function f 2 CðIm0Þ
and arbitrary n [ 0 exists such integer m1 and a set of real constants a1; bi; wij
where i ¼ 1; . . .; m1; j ¼ 1; . . .m0, that the function
Fðx1; x2; . . .; xmoÞ ¼
Xm1
i¼1 aiuð
Xm0
j¼1 wijxj þ biÞ
ð1:4Þ
is approximation of function f ðÞ, i.e.
jFðx1; x2; . . .; xm0Þ  f ðx1; x2; . . .; xm0Þj\n
ð1:5Þ
for all x1; x2; . . .; xm0, belonging to the input space.
This theorem is directly applied to networks BP (multilayered perceptron_) by
the following reasons:
1. As an activation function for neurons is used a limited monotonously increasing
function “sigmoid” uðxÞ ¼
1
1 þ ex
2. Neuron network contains m0 input nodes and one hidden layer consisting of m1
neurons.
3. Neuron i of a hidden layer has synaptic weights wi1; wi2; . . .; wim0 and a threshold
bi.
4. The neuron network output represents linear combination of output signals of
hidden layer neurons, weighted with weights a1; a2; . . .; am1.
So the theorem of universal approximation is the existence theorem, i.e. mathe-
matical proof of possibility of any function approximation by neural network BP even
with one hidden layer. But this theorem doesn’t give answer how to construct such
network. For this problem solution in every practical task for given training sample
4
1
Neural Networks

consisting of inputs Xi and desired outputs difðX1; d1Þ; ðX2; d2Þ; . . .; ðXn; dnÞg it’s
necessary to carry out the training of the network using one of the training algorithms.
1.3
Gradient Method of Training the Neural Network
Back Propagation
The ﬁrst algorithm that was developed for training Back Propagation network (BP),
was a gradient method. Let the criterion of training the network with 3 layers (one
hidden layer), be
eðwÞ ¼
X
M
i¼1
di  yiðwÞ
ð
Þ2 ! min
ð1:6Þ
where di is the desired value of the ith output of the neural network, yiðwÞ—the actual
value of the ith output of the neural network BP, for the weight matrix W ¼ WI; W0
½
.
That is, the criterion eðwÞ is the average squared error of approximation.
Let the activation function for the neurons of the hidden layer be hj ¼
r PN
i¼1 xiWI
ij þ WI
N þ 1;j


and output layer yk ¼ r PI
j¼1 hjW0
jk


be the same and be
a function of “SIGMOD”
r x
ð Þ ¼
1
1 þ ex
For such a function the derivative is equal to:
r0 x
ð Þ ¼ r x
ð Þ  1  r x
ð Þ
ð
Þ
Consider a gradient algorithm for neural network (NN BP) training.
1. Let W(n) be the current values of the weights matrix. The gradient algorithm is
as follows:
Wðn þ 1Þ ¼ WðnÞ  cn þ 1rwe WðnÞ
ð
Þ
where cn is the step size at the nth iteration
2. At each iteration, ﬁrst we train (adjust) input weights.
WI
ij n þ 1
ð
Þ ¼ WI
ijðnÞ  cn þ 1
@eðWÞ
@WI
ij
@e WI
ð
Þ
@WI
ij
¼
X
M
k¼1
2 dk  yk WI




 yk W0


 1  yk W0




 hj W
ð
Þ  1  hj W
ð
Þ


 xi
ð1:7Þ
1.2
Neural Network with Back Propagation. Architecture, Functions
5

3. Then we train output weights
@eðW0Þ
@W0
ij
¼ 2 dk  ykðW0Þ


yk W0


1  yk W0




hj
ð1:8Þ
W0
ij n þ 1
ð
Þ ¼ W0
ijðnÞ  cn þ 1
@eðWÞ
@W0
ij
where xi; i ¼ 1; N þ 1 are inputs of NN BP,
yk; k ¼ 1; M are outputs of NN BP,
hj;
j ¼ 1; J—the outputs of the hidden layer.
4. n :¼ n þ 1 and go to the next iteration.
Note: the so-called gradient algorithm with memory is as follows
Wðn þ 1Þ ¼ WðnÞ  kð1  aÞrWeðWðnÞÞ  arWeðWðn  1ÞÞ;
ð1:9Þ
where k—is a training rate, a 2 0; 1
½
 is the forgetting coefﬁcient.
The gradient method is the ﬁrst proposed training algorithm, it is simple to
implement, but has the following disadvantages:
• slow convergence;
• ﬁnds only a local extremum.
1.4
Gradient Training Algorithm for Networks
with an Arbitrary Number of Layers
Recurrent Expression for Calculating of the Error Derivatives of an Arbitrary
Layer
Consider the gradient method of training the neural network BP with an arbitrary
number of layers N. Let it be required to minimize the criterion:
EðwÞ ¼ 1
2
X
p
X
j
ðyðNÞ
jp  djpÞ2
ð1:10Þ
where yðNÞ
jp —is the real output of the jth neuron of the neural network output layer N
the at the input of the pth image; djp—the desired output.
The minimization is performed by stochastic gradient descent, which means the
adjustment of the weights as follows:
wijðtÞ ¼ wijðt  1Þ þ Dwij
6
1
Neural Networks

DwðnÞ
ij
¼ g @E
@wij
ð1:11Þ
where wij is connection weight of the ith neuron of (N−1)- layer to the j—neuron of
the Nth layer; 0\g\1—is a step of gradient search, so-called “learning rate”.
Further
@E
@wij
¼ @E
@yj
 dyj
dsj
 @sj
@wij
;
ð1:12Þ
where yj is the output of jth neuron of the nth layer;
sj—total weighted input signal of the jth neuron.
Obviously
sj ¼
X
i
wijyðn1Þ
i
;
ð1:13Þ
If the activation function of the jth neuron is f ð : Þ, then
yj ¼ f ðsjÞ and dyj
dsj
¼ f 0ðsjÞ
In the particular case if f is sigmoid f ¼ r, then
dyj
dsj
¼ f 0ðsjÞ ¼ yj  ð1  yjÞ:
ð1:14Þ
The third factor is dsj
dwij ¼ yðn1Þ
j
As for the ﬁrst factor @E
@yj in (1.12), it can be easily calculated using the outputs of
the neurons of the next (n + 1)th layer as follows:
@E
@yj
¼
X
k
@E
@yk
 dyk
dsk
 @sk
@yj
¼
X
k
@E
@yk
 dyk
dsk
 wðn þ 1Þ
jk
;
ð1:15Þ
Here summation is performed at the neurons of the (n + 1)th layer (k).
Introducing a new variable
dðnÞ
j
¼ @E
@yj
 dyj
dsj
;
ð1:16Þ
get the recurrence formula for calculating the value dðnÞ
j
of the n-th layer using the
values dðn þ 1Þ
k
of the next layer (see below—Fig. 1.3)
1.4
Gradient Training Algorithm for Networks with an Arbitrary Number of Layers
7

dðnÞ
j
¼
X
K
k¼1
dðn þ 1Þ
k
wðn þ 1Þ
jk
dyj
dsj
:
ð1:17Þ
For the output layer n ¼ N determine dðNÞ
i
directly as
dðNÞ
i
¼
yðNÞ
i
 di

 dyi
dsi
:
ð1:18Þ
Now the gradient descent algorithm may be written in the following form:
DwðnÞ
ij
¼ gdðnÞ
j yðn1Þ
i
:
ð1:19Þ
Sometimes to provide to the process of adjusting weights certain inertia to smooth
out spikes when moving on the surface the value Dwij at the previous iteration t  1
is additionally used. In this case the value DwijðtÞ is determined:
DwðnÞ
ij ðtÞ ¼ gðlDwðnÞ
ij ðt  1Þ  ð1  lÞdðnÞ
j yðn1Þ
i
ðtÞÞ;
ð1:20Þ
where l 2 0; 1
½

Gradient Training Algorithm for Networks with an Arbitrary Number of Layers
Thus the complete algorithm for training the NN BP using back propagation pro-
cedure includes the following steps.
Let one of the possible images. X is entered to NNBP inputs
x ¼ xi
f gi¼1;I
1. Put yð0Þ
i
¼ xi; i ¼ 1; I
2. Calculate sequentially the output values at the nth layer (n ¼ 1; 2. . .N).
Fig. 1.3 Three successive
layers of a neural network
(n −1), n , (n + 1)
8
1
Neural Networks

sðnÞ
j
¼
X
I
i¼1
yðn1Þ
i
wðnÞ
ij
ð1:21Þ
yðnÞ
j
¼ f ðsðnÞ
j Þ
ð1:22Þ
3. Calculate values dðNÞ
i
for the neurons of the output layer by the formula (1.18).
4. Deﬁne DwðNÞ
jk .
5. Using the recurrent formula (1.17), calculate dðn þ 1Þ
k
by dðnÞ
j
and DwðnÞ
ij
for all the
preceding layers n ¼ N  1; N  2; . . .; 1 using formula (1.19).
6. Correct the weights in accordance with the procedure
wðnÞ
ij ðtÞ ¼ wðnÞ
ij ðt  1Þ þ DwðnÞ
ij ðtÞ
ð1:23Þ
In this iteration (t) ends.
7. Calculate E ¼ EðwðtÞÞ. If EðwðtÞÞ\e2
giv, then STOP. Otherwise go to step 1 of
ðt þ 1Þ iteration.
The algorithm of calculation values dðnÞ
j
is illustrated in Figs. 1.4, 1.5, 1.6 and 1.7.
Fig. 1.4 Structure of
network BP
Fig. 1.5 Step 3 of gradient
algorithm for last layer n = N
1.4
Gradient Training Algorithm for Networks with an Arbitrary Number of Layers
9

1.5
The Improved Gradient Training Algorithms
for Neural Network BP
During the implementation of the gradient algorithm for training the neural network
BP a number of difﬁculties inherent to gradient-based optimization algorithms may
arise:
1. If we are far away from the minimum point of a function EðwijÞ and move with
small step gðtÞ, then search process may be delayed. It makes sense to increase
the step size. A indicator of this situation is the constant sign DEðt  1Þ
\ 0;
DEðtÞ\ 0 (see Fig. 1.8a).
2. If we are in the neighborhood of the minimum point w
ij, and the step size g is
large, then we may jump over the point w
ij, after that come back and there the
phenomenon of “oscillation” occurs. In this case it is advisable to gradually
reduce the step size gðtÞ. An indication of this is a change of sign DE, i.e.
DEðt  1Þ\ 0; DEðtÞ [ 0 (see Fig. 1.8b).
Fig. 1.6 Step 5 of gradient
algorithm for layer n
Fig. 1.7 Final step of
gradient algorithm calculating
w(t)
10
1
Neural Networks

Gradient Method with Training Step Correction (Rollback Method)
To overcome the above mentioned difﬁculties associated with the use of the gra-
dient method, the gradient method with correction of a training step was developed.
Here the step size gðtÞ at the ðt þ 1Þth iteration is described using recurrent
expression [1, 2]:
gðt þ 1Þ ¼
gðtÞ  u; if DEðwðtÞÞDEðwðt  1ÞÞ [ 0 and
gðtÞ  d; otherwise

DEðwðtÞ\0
where u [ 1; 0\d\1.
It is recommended to choose u  d  1.
The step correction can be performed if several sequential steps were carried out,
for example, t  2; t  1.
Method with the Ejection from Local Minima (Shock BP)
This method is used in the case of a multiextremal dependence EðwÞ if it’s nec-
essary to search the global minimum (see Fig. 1.9).
Fig. 1.8 a Illustration of
algorithm of gradient descent.
b Illustration of algorithm of
gradient descent
1.5
The Improved Gradient Training Algorithms for Neural Network BP
11

If we get stuck in a local minimum w
ij and the error E for a long time does not
change, it makes sense to make a big step in a random direction n, to jump out of
this shallow and get into the attraction domain of another minimum w
ij. Then
wijðt þ 1Þ ¼ wijðtÞ þ xn
where n—is random, uniformly distributed in the interval [−1; +1], x is a size of
random step. At this point we repeat gradient descend and if we come to another
minimum point then make new random step otherwise increase random step as
follows: x1 ¼ b x, where b [ 1.
Using this procedure we sequentially ﬁnd local minimal points
w
1; w
2; . . .; w
m


out of which we select the global one.
Training Method with Vector Step (Super SAB)
The main drawback of the classical gradient method is that steps in all directions are
the same gðtÞ. It does not take into account the fact that different components wij
may be at different distance from the required point minimum (i.e. one component
is far away, while others are close).
Therefore Almeida and da Silva [1] have developed a method with vector step,
which they called Super SAB. In this method, the search is performed according to
expression
wijðtÞ ¼ wijðt  1Þ  gijðtÞdðnÞ
j yðn1Þ
j
ð1:24Þ
gijðtÞ ¼
gijðt  1Þ  u; if
@EðtÞ
@wij  @Eðt1Þ
@wij
[ 0: and @Eðt1Þ
@wij
\0
gijðt  1Þ  d; if
@EðtÞ
@wij  @Eðt1Þ
@wij
\0 and @Eðt1Þ
@wij
\0 ðdecrease::of ::stepÞ
(
Fig. 1.9 Search of global
minimum
12
1
Neural Networks

where
u [ 1; 0\d\1; u  d ﬃ1
ð1:25Þ
Weights change occurs in accordance with the expression
wijðtÞ ¼
wijðt  1Þ  gijðtÞdðnÞ
j yj; if
@EðtÞ
@wij  @Eðt1Þ
@wij
[ 0 and @Eðt1Þ
@wij
\0
wijðt  1Þ; :otherwise
(
ð1:26Þ
Autonomous Gradient Method with Approximation of the Topography of a
Quadratic Function
Assume that current point is wðtÞ. The value rEðwðtÞÞ and antigradient
rEðwðtÞÞ are calculated and two trial steps are made:
w1 ¼ wðtÞ þ rEðwÞ
w2 ¼ wðtÞ  rEðwÞ
Calculate EðwÞ; Eðw1Þ; Eðw2Þ. Further, assuming that EðwÞ can be approximated by
a parabola, ﬁnd by three points w; w1; w2 a minimum point.
The main drawback of the method is that the function EðwÞ can have a much
more complex form, and so this approximation procedure has to be repeated
multiple times, which requires large computational costs.
1.6
Acceleration of Convergence of Neural Networks
Training Algorithms. The Conjugate Gradient
Algorithm
As was shown above, the algorithm for training networks “back propagation” is an
implementation of the classical method of steepest descent. This training algorithm
is relatively simple to implement and use, which explains its widespread use in the
ﬁeld of neural networks. It has 2 drawbacks:
1. it converges slowly;
2. the method is effective only when ﬁnding points of local minimum.
Therefore, were developed more effective training methods, which are alterna-
tive to the gradient method: conjugate gradient method and the method based on
genetic optimization.
The Method of Conjugate Gradients
The method of conjugate gradients (CG) gives improved convergence rate com-
pared to the method of steepest descent. However, like the method of steepest
descent, it is a local optimization method.
1.5
The Improved Gradient Training Algorithms for Neural Network BP
13

In neural networks the objective function (goal) to be minimized is the average
error on the whole set of training samples. It is equal to
ERðWÞ ¼
X
T
t¼1
X
M
k¼1
dtk  ytkðWÞ

2
ð1:27Þ
where t ¼ 1; 2; . . .T—the set of training samples.
For three-layer network with N input nodes, including the threshold node, J
hidden nodes and M output nodes the weight vector W contains NJ þ MJ
component.
In
the
formula
(1.27)
M—the
number
of
output
nodes;
dt1; dt2; . . .dtM
ð
Þ—the
desired
output
for
the
training
sample
t,
and
ytðWÞ ¼ yt1ðWÞ; yt2ðWÞ; . . .ytMðWÞ
f
g—the response (output signal of the network)
on the sample t.
The CG algorithm as well as a more general conjugate directions algorithm is
used in the ﬁeld of optimization with a wide class of problems for which it ensures
the convergence to the optimal solution in a ﬁnite number of steps. This is a major
improvement compared to the method of steepest descent, which requires an inﬁnite
number of iterations to ﬁnd the minimum of the function.
Conjugate Directions
The name of the method comes from the use of conjugate vectors. In a vector space
of the dimension D a set of vectors
P1; P2; . . .PD
f
g forms a set of conjugate
directions with respect to the matrix A, if [1]
PiAPj ¼ 0; for i 6¼ j
ð1:28Þ
where A is a positive denite matrix of the size D  D. Vector satisfying (1.28) are
called A-conjugate [1].
The question arises: how does the CG algorithm reach the convergence in a ﬁnite
number of steps, and on what tasks? Suppose that we need to minimize the function
FðWÞ ¼ b  AW
ð
ÞT b  AW
ð
Þ
ð1:29Þ
where b and W are D-dimensional vectors, and the matrix ADD is positively—
deﬁned above. So, we have a quadratic function. Suppose that we seek iteratively
optimal vector W that minimizes EðWÞ, and start the search from the starting point
Wo. Choose a nonzero vector P1 which serves as the search direction for the next
iteration, no matter how Wo and P1. Deﬁne W1 as the following vector:
W1 ¼ W0 þ ap1;
ð1:30Þ
where the scalar a is chosen so as to minimize EðW0 þ ap1Þ. Now we come to the
main idea. The optimal direction in which it is necessary to move at the next
iteration is the direction in which you need make only one step straight to the point
of optimal solution W and it must form A-conjugated pair with the vector p1.
14
1
Neural Networks

The optimal direction is therefore W  W1, the condition that ðW  W1Þ is
A-conjugate direction, is equivalent to the statement that it must satisfy the con-
dition [1]
ðW  W1ÞAp1 ¼ 0
Of course, at this point we do not know the optimal solution W, otherwise we
wouldn’t need any algorithm. at all. However, this condition is important for the
following reason. In N-dimensional space, there are exactly N independent vectors
that form A conjugate pairs with the vector p1.
Thus, we need only a ﬁnite number of directions to ﬁnd the optimal solution.
The conjugate direction algorithm systematically constructs the set of A-con-
jugate vectors. After a maximum of N steps, the algorithm will ﬁnd the optimal
direction, and the convergence will be ensured.
Here
we
have
omitted
the
important
question
of
determining
the
one-dimensional problem of minimizing the scalar. For problems in the form
Eq. (1.29) such a minimization is performed directly and forms part of the classic
SG algorithm, although for more general problems, this task is far from trivial.
In the task of training neural network, Eq. (1.29) does not exist in an explicit
form, and in particular we do not have explicit expression for the matrix A, although
the gradient of the error rE can fulﬁll this role.
Note that in Eq. (1.29) A is a factor proportional to the gradient of the function
EðWÞ. For such problems in the general form of EðWÞ the ﬁnite steps convergence
is no longer guaranteed.
You need to realize that the SG algorithm, similar to the method of gradient
descent, ﬁnds only locally optimal solutions. Nevertheless, the method gives a
signiﬁcant acceleration of convergence compared with the method of steepest
descent.
Description of the Algorithm
1. Set K ¼ 0. Initialize the weight vector W
and calculate the gradient
Gk ¼ grad EðWkÞ. Put an initial direction vector pk ¼  Gk
Gk
k
k.
2. Find scalar a [ 0, which minimizes EðW þ apÞ, for which we may use
Fibonacci or Golden section algorithms.
WðK þ 1Þ ¼ WðKÞ þ apðKÞ
3. If EðWðK þ 1ÞÞ\e, where e—is a permissible accuracy of ﬁnding the mini-
mum, then STOP. Otherwise compute a new direction:
Gðk þ 1Þ ¼ gradEðWðk þ 1ÞÞ
4. If modNðK þ 1Þ ¼ 0, where N is the dimension of the weights space W, then
new direction vector is determined as
1.6
Acceleration of Convergence of Neural Networks Training …
15

Pðk þ 1Þ ¼  Gðk þ 1Þ
Gðk þ 1Þ
k
k ;
ð1:31Þ
otherwise put
b ¼ GðK þ 1ÞTGðK þ 1Þ
GðKÞTGðKÞ
ð1:32Þ
and calculate the new direction vector Pk þ 1 ¼
Gðk þ 1Þ þ bpðkÞ
Gðk þ 1Þ þ bpðkÞ
k
k
Replace pðkÞ by pðk þ 1Þ and GðkÞ by Gðk þ 1Þ. Go to step 1 of the next
iteration.
1.7
Genetic Algorithm for Neural Network Training
This algorithm is a global optimization algorithm. It uses the following mechanisms
[1, 5]:
1. Cross-breeding of parental pairs (cross-over) and generation of descendants;
2. mutation (random disturbances);
3. natural selection of the best descendants.
Let the training goal be minimization of a mean squared error (MSE)
EðWÞ ¼ 1
M
X
M
k¼1
ðdk  ykðwÞÞ2;
where W ¼ WI; WO
½

W1 ¼
wI
ij
			
			;
WO ¼
wO
ij
			
			
Let the initial population ½W1ð0Þ; . . .Wið0Þ; . . .WNð0Þ be given., where N is a
population size.
Any specimen i of the N individuals is represented by the corresponding weight
matrix Wi.
Calculate the index of suitability (Fitness Index) of the ith specimen and evaluate
the quality of prediction (or approximation)
FIðWiÞ ¼ C  EðWiÞ
ð1:33Þ
where C is a constant.
16
1
Neural Networks

Crossbreeding of parental pairs. For choosing parents a probabilistic mecha-
nism is used. We determine the probability of selecting the ith parent:
Pi ¼
FIðWið0ÞÞ
P
N
i¼1
FIðWið0ÞÞ
ð1:34Þ
After selecting the next pair of parents crossing-over occurs. You can apply various
mechanisms of crossing. For example: for the ﬁrst descendant odd components of
the vector from the ﬁrst parent, and the even components of the vector of the second
parent are taken and for the second descendant on the contrary—even components
of the vector from the ﬁrst parent, and the odd components of the vector of the
second parent. This can be written thus:
Wið0Þ 	 Wkð0Þ ¼ Wið1Þ [ Wkð1Þ
where Wi ¼ wij


j¼1;R.
wijð1Þ ¼
wijð0Þ;
if
j ¼ 2m
wkjð0Þ; if j ¼ 2m  1

m ¼ 1; R
2
wkjð1Þ ¼
wkjð0Þ; if j ¼ 2m
wijð0Þ; if j ¼ 2m  1

As a whole, N
2 parent pairs are chosen and N descendants are generated.
Then mutation noise is acting on descendants as follows:
w0
ijðnÞ ¼ wijðnÞ þ nðnÞ;
where nðnÞ ¼ a  ean; a ¼ const 2 ½1; þ 1 is an initial mutation rate, a is a
mutation attenuation speed, n is a number of the current iteration.
Selection. Different selection mechanisms can be used, in particular:
1. Replace the old population with the new one.
2. Select N best individuals out of joint population Npar þ Nch ¼ 2N by the
maximum of criterion FI and form the population of the next iteration.
The described iterations are repeated unless the stop condition holds.
As a stop conditions the following variants are used:
1. the number of iterations: n 
 ngiv where ngiv ¼ 103  104 iterations;
2. the achieved value of criterion max FIðWiðnÞÞ 
 FIgiv, where FIgiv is a given
value.
1.7
Genetic Algorithm for Neural Network Training
17

The main advantage of genetic algorithm (GA) is herein it enables to ﬁnd global
optimum if number of iterations n tends to 1. But the main disadvantages of the
genetic algorithm are following:
1. The number of parameters should be determined experimentally, for example:
N—population size; a—attenuation speed of mutations.
2. To implement a genetic algorithm a large amount of computations is demanded.
1.8
Neural Networks with Radial Basis Functions (RBF
Networks)
Multilayer neural networks such as Back Propagation carry out the approximation
of functions of several variables by converting the set of input variables into the set
of output variables [1, 6, 7]. Due to the nature of the sigmoidal activation function
this approximation is of global type. As a result of it a neuron, which was once
switched on (after exceeding by the total input signal a certain threshold), remains
in this state for any value exceeding this threshold. Therefore, every time the value
of the function at an arbitrary point of space is achieved by the combined efforts of
many neurons, which explains the name of the global approximation.
Another way of mapping input into output is to transform by adapting several
single approximating functions to the expected values, and this adaptation is carried
out only in a limited area in the multidimensional space. With this approach, the
mapping of the entire data set is the sum of local transformations. Taking into
account the role played by the hidden neurons, they form the set of basis functions
of local type. Since action of each single function (with non-zero values) is recorded
only in a limited area of the data space—hence the name is local approximation.
The Essence of Radial Neural Networks
Particular family form a networks with radial basis function, in which hidden
neurons implement features, radially varying around the selected center and taking
non-zero values only in the vicinity of the center. Such functions deﬁned in the
form u(x)are called radial basis functions. In such networks, the role of the hidden
neuron contains in displaying sphere space around a given single point or a group
of points forming a cluster. Superposition of signals from all hidden neurons, which
is performed by the output neuron, allows to obtain a mapping of the entire mul-
tidimensional space.
The radial network is a natural complement of sigmoidal networks. Sigmoidal
neuron is represented in a multidimensional space by a hyper-plane that divides the
space into two categories (two classes) in which either of two conditions holds:
Either
X
i
wijxi [ 0; or
X
i
wijxi\0:
18
1
Neural Networks

In turn, the radial neuron represents a hyper-sphere, which carries spherical
division of the space around a central point. From this point of view it is a natural
complement sigmoidal neuron, as in the case of circular symmetry of the data helps
reduce the number of neurons required for the separation of different classes.
The simplest neural network radial type operates on the principle of multidi-
mensional interpolation, which consists in the mapping of p different input vectors
(i ¼ 1; 2; . . .; p) of the input N-dimensional space into the set of rational numbers
p (p ¼ 1; 2; . . .; R). To implement this process, you must use the p hidden neurons
of the radial type and ask such a mapping function F(x) for which the condition
interpolation fulﬁlls:
FðxiÞ ¼ di
ð1:35Þ
Use p hidden neurons, connected by links with weights with linear output
neurons, means forming output signals of the network by summing the weighted
values of the respective basis functions. Consider the radial network with one
output and p training pairs ððx1; d1Þ; ðx2; d2Þ; . . .; ðxp; dpÞÞ. Let us assume that the
coordinates of each of the p centers of the nodes of the network are determined by
one of the vectors, i.e. Xi. In this case, the relationship between input and output
signals of the network can be determined by the system controls, linear relative
weights, which in matrix form looks like
wTuðxÞ ¼ d:
ð1:36Þ
Mathematical basis of the radial network work is a theorem by Cover [2] about
the recognize ability of images. It states that a nonlinear projection of images in a
multidimensional space of higher dimension can be linearly separated more likely
than their projections in a space with lower dimensionality.
If to denote the vector of radial functions uðxÞ ¼ u1ðxÞ; u2ðxÞ. . .uKðxÞ
½
T in
N-dimensional input space by uðxÞ then this space would be split in two spatial
class X þ and X if there exists a vector of weights w such that
wTuðxÞ [ 0 ; if x 2 X þ
ð1:37Þ
wTuðxÞ\ 0 ; if x 2 X
ð1:38Þ
The boundary between these classes is deﬁned by the equation wTuðxÞ ¼ 0.
In [2] it was proved that every set of images, randomly placed in a multidi-
mensional space, is u-divided with probability 1, provided correspondingly large
dimension k of this space. In practice, this means that the application of a sufﬁ-
ciently large number of hidden neurons that implement the radial function uiðxÞ
guarantees that the solution of the problem of classiﬁcation when building only a
two-layer network: in which the hidden layer must implement the vector u(x) and
the output layer may be composed of a single linear neuron that performs the
summation of the output signals from the hidden neurons with weights w.
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
19

The simplest neural network of radial type operates on the principle of multi-
dimensional interpolation, which consists in the mapping of p different input vec-
tors (i ¼ 1; 2; . . .; p) of the input N-dimensional space into the set of rational
numbers p (p ¼ 1; 2; . . .; R). To implement this process, you must use the p hidden
neurons of the radial type and set such a mapping function FðxÞ for which the
condition interpolation holds
FðxiÞ ¼ di :
ð1:39Þ
Use of p hidden neurons, connected by links with weights with linear output
neurons, means forming output signals of the network by summing the weighted
values of the respective basis functions. Consider the radial network with one
output and p training pairs (xi; di). Let us assume that the coordinates of each of the
p centers of the network nodes are deﬁned by one of the vectors xi, i.e. ci ¼ xi. In
this case, the relationship between input and output signals of the network can be
determined by the system of equations, linear related to weights wi, which in matrix
form is written:
u11
u12
. . .
u1p
u21
u22
. . .
u2p
. . .
. . .
. . .
. . .
up1
up2
. . .
upp
2
664
3
775
w1
w2
. . .
wp
2
664
3
775 ¼
d1
d2
. . .
dp
2
664
3
775
ð1:40Þ
where uij ¼
xj  xi
		
		


deﬁnes a radial function centered at the point xi, with the
input vector xj. If we denote the matrix elements uij as u and introduce the notation
vectors w ¼ w1; w2; . . .wp

T, d ¼ d1; d2; . . .dp

T then the system of Eq. (1.40)
can be represented in the reduced matrix form as follows
uw ¼ d
ð1:41Þ
In [2] it was proved that for the some radial functions in the case x1 6¼ x2 6¼
. . . 6¼ xp quadratic interpolation matrix u is non-singular and nonnegative deﬁnite.
Therefore, there exists a solution of Eq. (1.41) in the form
w ¼ u1d
ð1:42Þ
which allows to obtain the vector of weights of the network output neuron.
The theoretical solution to the problem represented by the expression (1.42)
cannot be regarded as absolutely true because of serious limitations in the overall
network properties arising from the assumptions done in the beginning. With very
large number of training samples and equal number of radial functions the problem
from a mathematical point of view becomes inﬁnite (poorly structured), since the
number of equations exceeds the number of degrees of freedom of the physical
process modeled by Eq. (1.40). This means that the result of such an excessive
amount of weight coefﬁcients will be the adaptation of the model to various kinds
20
1
Neural Networks

of noises, or irregularities accompanying the training sample. As a result, inter-
polating these data hyper-plane will not be smooth, and generalizing capabilities
will remain very weak.
That to increase these capabilities, it’s ought to decrease the number of radial
functions and obtain from excess volume data additional information for the reg-
ularization of the problem and improve its conditioning.
Radial Neural Network
Using the decomposition of p basis functions, where p is the number of training
samples, it is also unacceptable from a practical point of view, since usually the
number of samples is very large, and as a result the computational complexity of the
training algorithm becomes excessive. The solution of the system of Eq. (1.40) of
dimension (p  p) for large values of p becomes difﬁcult because a very large matrix
(with the exception orthogonal). Therefore, as well as for multi-layer networks, it is
necessary to reduce the number of weights, which in this case leads to the reduction
of the number of basis functions. So a suboptimal solution in a lower dimension is
searched, which accurately approximates the exact solution. If we restrict ourselves
to K basic functions, then approximating solution can be represented in the form
FðxÞ ¼
X
K
i¼1
wiu
x  ci
k
k
ð
Þ;
ð1:43Þ
where K\ p, and Ci(i ¼ 1; 2; . . .; K) be the set of centers that need to be deﬁned. In
special case, if we take K ¼ p, we can obtain an exact solution Ci ¼ Xi.
The problem of approximation consists in the selection of an appropriate number
of radial functions and their parameters, and in the selection of weights wi
(i ¼ 1; 2; . . .; K) so that the solution of Eq. (1.40) would be the closest to accurate.
Therefore the problem of selection of the parameters of the radial functions and the
values of the weights wi of the network can be reduced to minimization of the
objective function, which if using the Euclid metric is written in the form
E ¼
X
p
i¼1
X
K
j¼1
wju
xi  cj
		
		


 di
"
#2
ð1:44Þ
In this equation K is the number of radial neurons, and p is the number of training
pairs (xi; di), where xi is the input vector, di—corresponding expected value. Let us
denote d ¼ d1; d2; . . .dp

T the vector of expected values, w ¼ w1; w2; . . .wk
½
T—
the vector of network weights, a G—radial matrix is called the matrix of Green [7].
G ¼
u
x1  c1
k
k
ð
Þ
u
x1  c2
k
k
ð
Þ
. . .
u
x1  cK
k
k
ð
Þ
u
x2  c1
k
k
ð
Þ
u
x2  c2
k
k
ð
Þ
. . .
u
x2  cK
k
k
ð
Þ
. . .
. . .
. . .
. . .
u
xp  c1
		
		


u
xp  c2
		
		


. . .
u
xp  cK
		
		


2
664
3
775
ð1:45Þ
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
21

When to restrict K basis functions, the matrix G becomes rectangular with the
number of rows, usually being much larger than the number of columns (p  K).
If we assume that the parameters of the radial functions are known, the opti-
mization problem (1.44) reduces to solution systems of equations, linear with
respect to the weights w [2]
Gw ¼ d:
ð1:46Þ
Due to the squaredness of the matrix G we can determine the vector of weights w
using operations with pseudo-inverse matrix G, i.e.
w ¼ G þ d;
ð1:47Þ
where G þ ¼ GTG
ð
Þ1GT denotes pseudoinverse rectangular matrix G. In compu-
tational practice, pseudoinverse is calculated by applying the SVD decomposition [2].
In considered up to this point the solution it was used representation of the basis
functions with the matrix Green dependent on the Euclidean norm of the vector x
k k.
If to take into account the multivariate function may have a different weight on each
axis, from a practical point of view, it is useful to specify scaling norm by intro-
ducing into the deﬁnition of the Euclidean metric weights in a form of matrix Q:
x
k k2
Q¼ Qx
ð
ÞTðQxÞ ¼ xTQTQx:
ð1:48Þ
The scaling matrix for N-dimensional vector x has the form:
Q ¼
Q11
Q12
. . .
Q1N
Q21
Q22
. . .
Q2N
. . .
. . .
. . .
. . .
QN1
QN2
. . .
QNN
2
664
3
775
In the presentation of matrix QTQ by the correlation matrix C in the general case
we get:
x
k k2
Q¼
X
N
i¼1
X
N
j¼1
Cijxixj:
ð1:49Þ
If the scaling matrix Q is diagonal, we get
x
k k2
Q¼ P
N
i¼1
Cijx2
i . This means that the
norm of the scaling vector x is calculated according to a standard formula of Euclid,
using individual scaling weights for each variable Xi. When Q ¼ 1 the weighted
Euclid metric reduces to the classical (non-scaled) metric
x
k k2
Q¼
x
k k2.
Most often as the radial function the Gaussian function is used. When placing its
center at the point Ci, it can be speciﬁed in abbreviated form as
22
1
Neural Networks

uðxÞ ¼ uð x  ci
k
kÞ ¼ exp  x  ci
k
k2
2r2
i
 
!
:
ð1:50Þ
In this expression ri—is the parameter, the value of which depends on the width of
the function. In the case of Gaussian form of the radial function centered at the
point Ci, and weighted scaling matrix Qi associated with the ith basis function, we
obtain a generalized form of Gaussian function [7]
uðxÞ ¼ uð x  ci
k
kQÞ ¼ exp  x  ci
ð
ÞTQT
i Qiðx  ciÞ


¼ exp  1
2 x  ci
ð
ÞTCiðx  ciÞ


;
ð1:51Þ
where the matrix 1
2 Ci ¼ QT
i Qi plays the role of scalar coefﬁcient
1
2r2
i of the standard
multivariate Gaussian function, given by (1.50).
A solution is obtained, representing the approximating function in the multidi-
mensional space in the form of a weighted sum of the local radial basis functions
(expression (1.50), can be interpreted in a radial neuron network presented in
Fig. 1.10 (for simplicity, this network has only one output), which is ui determined
by the dependence (1.50) or (1.51). The resulting architecture of the radial network
has a structure similar to the multilayer sigmoidal networks with one hidden layer.
The role of hidden neurons are played by radial basis functions, which differ in their
form from the sigmoidal functions. Despite the similarities, these network types are
fundamentally different from each other.
The radial network has a ﬁxed structure with one hidden layer and linear output
neurons, whereas sigmoidal network may contain a different number of layers and
output neurons are linear and nonlinear. Used radial functions may have various
Fig. 1.10 Generalized
structure of the RBF networks
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
23

structure [2, 7]. Nonlinear radial function of each hidden neuron has its own values
for the parameters Ci and ri, whereas in sigmoidal networks are used, as a rule,
standard activation functions with the same parameter b for all neurons. The
argument of the radial function is the Euclidean distance of a sample x from the
center Ci, and in a sigmoidal network the scalar product wTx is used.
Still more large differences between these networks can be noted in a detailed
comparison of their structures. Sigmoidal network has a multilayer structure in
which an arrangement of neurons is repeated from layer to layer. Every neuron in it
sums the signals with subsequent activation. The radial structure of the network is
quite different. In Fig. 1.11 shows a detailed diagram of the RBF network structure
with the radial function of the form (1.50) in the classical sense of the Euclidean
metric. The ﬁgure shows that the ﬁrst layer comprise a non-linear radial function
whose parameters (the centers Ci and the coefﬁcients ri) are speciﬁed in the training
process. The ﬁrst layer does not contain the linear weights unlike sigmoidal
network.
Much more complex is the detailed network structure that implements the scaled
radial function in the form speciﬁed by the expression (1.51). This network is
presented in Fig. 1.12, is called HRBF network (Hyper Radial Basis Function) [7].
Radial neuron in it has a particularly complex structure, containing summing sig-
nals similar to those used in sigmoidal networks, and exponential activation
functions with Parameters to be speciﬁed in the training process. The weights QðkÞ
ij
of the Kth radial neuron of the hidden layer are the elements of the matrix QðkÞ
playing the role of the scaling system. They introduce an additional degree of
freedom, so that it allows to bring the output signal of the network y ¼ f ðxÞ closer
to the desired function dðxÞ.
Fig. 1.11 A detailed diagram of the RBF networks structure
24
1
Neural Networks

In many practical applications the scaling matrix QðkÞ has a diagonal form in
which only the elements QðkÞ
ii
have non-zero values. In such a system there is no
circular mixing signals corresponding to the various components of the vector x,
and the element QðkÞ
ii
plays the role of the individual scaling factor for ith com-
ponent of the vector X of kth neuron. In Fig. 1.13 a simpliﬁed structure of the
HRBF network with diagonal matrices QðkÞ is presented. It should be noted that in
HRBF networks the role of the coefﬁcients r2
i fulﬁll the elements of the matrix Q
which are speciﬁed in the training process.
Fig. 1.12 A detailed diagram of HRBF network structure with the scaling matrix Q of arbitrary
type
Fig. 1.13 A detailed diagram of of radial HRBF network Q with diagonal scaling matrix structure
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
25

Training Methods for Radial Neural Networks
Introduced in the previous subsection, the methods of selection of the weights wi of
the output layer radial RBF network were based on the assumption that the
parameters of the basis functions are known, in connection with Green matrix are
considered to be deﬁned, and therefore the problem is reduced to the solution of the
redundant system of linear equations of the form (1.41). Practically this approach is
possible only in a completely unrealistic case, when K ¼ p, in which the centers
ci ¼ xi are known in advance, and the value of the ri you can easily determine
experimentally with a certain compromise between the monotony and the display
precision. In fact, inequality K  p always holds, so the training process of RBF
network based on the selected type of radial basis function consists of:
• the selection of the centers ci, and parameters ri that form basis functions;
• the selection of the weights of the neurons of the output layer.
The problem of determination the weights of the neurons of the output layer is
greatly simpliﬁed. In accordance with the formula (1.47) weight vector w can be
determined in one step by pseudo-inverse matrix G, w ¼ G þ d.
The matrix G having p rows and K columns, represents the response of neurons
in the hidden layer at the next input vector Xi (i ¼ 1; 2; . . .; p). In practice
pseudo-inverse matrix G is calculated using the decomposition by its eigenvalues,
according to which [7]
G ¼ USVT;
ð1:52Þ
Matrices U and V are orthogonal and have the dimensions (p  p) and (K  K),
respectively, while S is pseudo-diagonal matrix of dimension (p  K). In this case,
K\ p, and the diagonal elements s1 
 s2 
 . . . 
 sk 
 0. Suppose that only the ﬁrst r
elements of si are of signiﬁcant value, and the others can be neglected. Then the
number of columns of the orthogonal matrices U and V can be reduced to r. In this
case the obtained reduced matrix Ur and Vr are of the form:
Ur ¼ u1u2. . .ur
½
;
Vr ¼ v1v2. . .vr
½
;
the matrix sr becomes diagonal (square sr ¼ diag s1; s2; . . .; sr
½
). This matrix
describes the dependence of (1.52) in the form
G ﬃUrSrVT
r
ð1:53Þ
Pseudo-inverse to G matrix G þ is then determined by the expression
G þ ¼ VrS1
r UT
r
ð1:54Þ
26
1
Neural Networks

in which S1
r
¼
1
s1 ; 1
s2 ; . . .; 1
sr
h
i
, and the vector of network weights, undergoing the
training, is deﬁned by the formula
w ¼ VrS1 UT
r d:
ð1:55Þ
The advantage of the formula (1.55) is its simplicity. Output network weights are
selected in one step by a simple multiplication of the corresponding matrices, some
of which (Ur,Vr) are orthogonal and inherently well-ordered. Taking into account
the solution of (1.55), which determines the values of the weights of the output
layer, the main problem of the training radial networks remains the selection of the
parameters of the nonlinear radial functions, especially centers ci.
One of the simplest, albeit not the most effective way of determining the
parameters of the basis functions, is considered to be a random selection. In that
decision, the centers ci of basis functions are chosen randomly based on uniform
distribution. This approach is valid as applied to the classical radial networks,
provided that a uniform distribution the training data corresponds well to the
speciﬁcs of the task. When choosing a Gaussian form of the radial function the
value of the standard deviations si should be given, dependent on location of
randomly selected centers ci
u
x  ci
k
k2


¼ exp  x  ci
k
k2
d2
K
 
!
ð1:56Þ
for i ¼ 1; 2; . . .; K, where d denotes the maximum distance between the centers ci.
From the expression (1.56) implies that the standard deviation of the Gaussian
function, which characterizes the width of the curve is set a random equal to
r ¼
dﬃﬃﬃﬃﬃ
2K
p
and constant for all the basis functions. The width function is proportional
to the maximum centers divergence and decreases with the increase in their number.
Among the many specialized methods of selection centers some of the most
important are the following: self-organizing process of separation into clusters,
hybrid algorithm and supervised training algorithms.
The Application of the Self-organization Process for Adaptation the Parameters
of Radial Functions
Good results of adjustment the parameters of the radial functions can be obtained by
using the algorithm of self-organization. The process of self-organization of the
training data automatically splits the space into so-called “Voronoi regions” [7],
deﬁning different groups of data. An example of this separation is the
two-dimensional space shown in Fig. 1.14. The data are grouped within a cluster
represented by the central point, which determines the average value of all its
elements. The center of the cluster is to be identiﬁed with the center of the corre-
sponding radial functions. For this reason, the number of such functions is equal to
the number of clusters and can be adjusted by the algorithm of self-organization.
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
27

Splitting the data into clusters can be done using the version of the algorithm of
Linde-Buzo-Grey [2, 7], also called the C-means algorithm (this algorithm is
detailly considered in the Chap. 7). In direct (online) version of this algorithm a
reﬁnement of the centers is made after the presentation of each next vector X from
the set of training data. In the cumulative version (off-line) all centers are speciﬁed
at the same time after the presentation of all the elements of the set. In both cases, a
preliminary selection of centers is often random using a uniform distribution.
If the training data represents a continuous function, the initial values of the
centers are primarily placed at points corresponding to all maximum and minimum
values of the function. Data about these points and their immediate environment are
subsequently removed from the training set and the remaining centers will be
uniformly distributed in the area formed by the remaining elements of this set.
In on-line version after the presentation of the kth vector Xk, belonging to the
training set, the centre closest to Xk, relatively to the applied metric is selected. This
center is subjected to reﬁnement in accordance with the WTA (Winner Takes ALL)
algorithm (which is considered in the Chap. 2)
ciðk þ 1Þ ¼ ciðkÞ þ g xk  ciðkÞ
½
;
ð1:57Þ
where g is a coefﬁcient—learning rate, having a small value (typically g  1), and
decreasing in time. Other centers do not change. All training vectors X are pre-
sented several times, usually in a random sequence until the stabilization of their
locations.
A variation of the algorithm is also used, according to which the value of the
center-the winner is speciﬁed in accordance with formula (1.57) while one or more
nearest centers are pushed in the opposite direction [2], and this process is
implemented according to the expression
ciðk þ 1Þ ¼ ciðkÞ  g xk  ciðkÞ
½
:
ð1:58Þ
Fig. 1.14 Illustration of the
method of dividing the data
space into spheres of
inﬂuence of individual radial
functions
28
1
Neural Networks

This modiﬁcation of the algorithm allows to alienate centers located close to each
other that provides the best survey of the entire data space (g1\g).
In cumulative (off-line) versions all training vectors X are presented simulta-
neously and each of them is mapped to a particular center. The set of vectors
assigned to the same center, forms a cluster, a new center which is deﬁned as the
average of the corresponding vectors:
ciðk þ 1Þ ¼ 1
Ni
X
Ni
j¼1
xjðkÞ:
ð1:59Þ
In this expression, N is the number of vectors xðkÞ assigned at the kth cycle to
the ith center. The values of all of the centers are determined in parallel. The process
of presentation of a set of vectors X and adaptation of centers values is repeated
until stabilization of the values of the centers. In practice, most often, we use the
direct algorithm, which has slightly better convergence. However, neither of
algorithms guarantee convergence to the optimal solution in a global sense, and
provides only a local optimization that depends on initial conditions and parameters
of the training process.
In poorly chosen initial conditions, some centers may be stuck in the area where
the number of training data is negligible small or absent, so the centers modiﬁcation
process will slow down or stop. The way to solve this problem is considered to be
simultaneous adjustment of the placement of a large number of centers with ﬁxing
the values g for each of them. The center that is closest the to the current vector X is
modiﬁed most strongly, and the others are modiﬁed inversely proportional to their
distance to the current vector.
Another approach is to use a weighted distance measure from a center to produce
the weighted vector norm which makes the “favorites” those centers that are least
probable to be winners. Both approaches do not guarantee a 100 % optimal solution,
since they represent actually the procedure, introducing the predeﬁned perturbations
into the process of local optimization [7]. The difﬁculty is also a problem of the
selection of training parameter g. When using a constant value g, it should be very
small to guarantee the convergence of the algorithm, which excessively increases the
training time. Adaptive selection methods allow to make its value dependent on time,
i.e. to reduce the growth of the number of iterations. The most known representative of
this group is the algorithm of Darken-Moody [7], according to which
gðkÞ ¼
g0
1 þ k
T
ð1:60Þ
Coefﬁcient T denotes a time constant, which is adjusted individually for each
task. For k\T, the value is almost unchanged, but for k [ T it gradually decreases
to zero. Although adaptive methods of selecting g are more progressive compared
to a constant value, they also can not be considered the best solution, especially
when modeling dynamic processes.
1.8
Neural Networks with Radial Basis Functions (RBF Networks)
29

After ﬁxing the location of centers remains the problem of the selection of the
parameter rj value corresponding to a particular basis function. Setting rj of the
radial function affects the shape and size of the coverage area in which the value of
this function is non-zero (more precisely, exceeds a certain threshold e). Selection
of rj should be performed so that the scope of all radial functions will cover all the
space of the input data, and any two zones may overlap only to a small extent. With
such selection of rj values of the radial network mapping functions will be rela-
tively monotonous.
The easiest way to select the value of rj is to accept the Euclidian distance
between the jth center ci, and his nearest neighbor [7]. In another algorithm that
takes into account the broader neighborhood, on the value of rj affects the distance
between the jth center cj and its p nearest neighbors. In this case the value of rj is
determined by the formula
rj ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
P
X
P
k¼1
cj  ck
		
		2
v
u
u
t
:
ð1:61Þ
In practice, the value of p typically lies in the interval [8–10].
When solving any problem the key issue that determines the quality of mapping
consists in preliminary selection of a number of radial functions (hidden neurons).
As a rule, it’s guided by a general principle: the higher is the dimensionality of the
vector X, the greater the number of radial functions is necessary to obtain a sat-
isfactory solution. Detailed description of the process of selecting the number of
radial functions will be presented in the following subsection.
1.9
Hybrid Training Algorithm for Radial Networks
In the hybrid algorithm, the training process is divided into two stages:
1. selection of a linear network parameters (weights of the output layer) by using
the method of pseudo-inverse;
2. adaptation of the nonlinear parameters of the radial functions (center ci and
width ri of these functions).
Both stages are closely interconnected. When ﬁxing to current values of the centers
and widths of the radial functions (at ﬁrst this will be the initial value), values of the
linear weights of the output layer are selected in one step using SVD decomposition.
This ﬁxation of the parameters of the radial functions allows to determine the values of
the functions FiðxkÞ i ¼ 1; 2; . . .; K, and k ¼ 1; 2; . . .; p, where i is the number (index)
of a radial function, and k is the number of the next training pair (xk; dk). Next input
signals
xk
generate
in
the
hidden
layer
signals
described
by
vectors
uk ¼ 1; u1ðxkÞ; u2ðxkÞ; . . .; uKðxkÞ
½
, where 1 denotes a unit signal of polarization.
30
1
Neural Networks

They are accompanied by an output signal of the network yk; yk ¼ /T
k ðxÞw, and the
vector w contains the weights of the output layer, w ¼ w0; w1; . . .; wK
½
T. In the
presence of the p training pairs, we obtain a system of equations
1
u1ðx1Þ
. . .
uKðx1Þ
1
u1ðx2Þ
. . .
uKðx2Þ
. . .
. . .
. . .
. . .
1
u1ðxpÞ
. . .
uKðxpÞ
2
664
3
775
w0
w1
. . .
wK
2
664
3
775 ¼
y1
y2
. . .
yp
2
664
3
775;
ð1:62Þ
which in vector form can be written as
Gw ¼ y:
ð1:63Þ
Using a hybrid method at the stage of determination of the output weights vector y
is replaced by the vector of expected values d ¼ d1; d2; . . .; dp

T and the system of
equations Gw ¼ d is solved in one step using pseudo-inverse
w ¼ G þ d:
ð1:64Þ
In the algorithm for calculating pseudo-inverse is applied SVD decomposition,
which allows to obtain the current value of the vector w in accordance with
Eq. (1.64) in one step.
At the second stage, when the values of the output weights are known excitatory
signals X are passed through the network until the output layer that enables to
calculate the error criterion for sequence of vectors xk. Then this signal returns back
to the hidden layer (back propagation).
For the next consideration assume that there is the network model of HRBF type
with a diagonal form of the scaling matrix Q. This means that each radial function is
deﬁned in general terms as
uiðxkÞ ¼ exp  1
2 uik


;
ð1:65Þ
where the total signal of a neuron uik is described by the expression
uik ¼
X
N
j¼1
ðxjk  cijÞ2
r2
ij
ð1:66Þ
With availability of p training pairs the goal function can be speciﬁed in the form
E ¼ 1
2
X
p
k¼1
yk  dk
½
2¼ 1
2
X
p
k¼1
X
K
i¼0
wiuðxkÞ  dk
"
#2
ð1:67Þ
1.9
Hybrid Training Algorithm for Radial Networks
31

As a result of differentiation of this function we get:
@E
@cij
¼
X
p
k¼1
yk  dk
ð
Þwi exp  1
2 uik

 ðxjk  cijÞ
r2
ij
"
#
;
ð1:68Þ
@E
@rij
¼
X
p
k¼1
yk  dk
ð
Þwi exp  1
2 uik

 ðxjk  cijÞ2
r3
ij
"
#
;
ð1:69Þ
The application of the steepest descent gradient method allows to perform the
adaptation of the centers and widths of the radial functions according to the
formulae:
cijðn þ 1Þ ¼ cijðnÞ  g @E
@cij
;
ð1:70Þ
rijðn þ 1Þ ¼ rijðnÞ  g @E
@rij
:
ð1:71Þ
Reﬁnement of non-linear parameters of the radial function completes the second
phase (stage) of training. The repetition of both phases leads to a full and quick
training network, especially when the initial values of the parameters of the radial
functions are chosen close to the optimal.
In practice, the stages in varying degrees affect the adaptation of the parameters.
As a rule, algorithm SVD operates faster (it in one step ﬁnds a local minimum of the
function). To align this imbalance one clariﬁcation of linear parameters is usually
accompanied by several cycles of adaptation of non-linear parameters.
Example of a Radial Network Application
Neural networks with radial basis functions are used both in solving problems of
classiﬁcation and approximation of functions, in the prediction, i.e. are applied in
those areas in which sigmoidal networks have already won solid positions for many
years. They perform the same functions as sigmoidal networks, but implement
different data processing methods associated with the local mappings. Due to this
feature a signiﬁcant simpliﬁcation takes place and, therefore, acceleration of the
training process.
As an example, consider a three-dimensional function approximation, which is
described by the formula [7]
d ¼ f ðx1; x2Þ ¼3ð1  x1Þ2 expðx2
1  ðx2 þ 1Þ2Þ
 10 x1
5  x3
1  x5
2


expðx2
1  x2
2Þ
 1
3 expððx1 þ 1Þ2  x2
2Þ
32
1
Neural Networks

Let the variables vary within 3  x1  3 and 3  x2  3. The training data are
uniformly distributed in the areas of deﬁnitions of variables x1 and x2.
In total, for training 625 training samples in the form of data pairs
x1; x2
½
; d
ð
Þ
were used. To solve the problem a radial network with the structure 2-36-1 was
built (2 input x1 and x2, respectively, 36 radial neurons of Gaussian type and one
output linear neuron, The corresponding d-value functions). The hybrid training
algorithm with random initial values of the network parameters was applied. In
Fig. 1.15 a chart of the network training (the curve of change of the error with
increasing number of iterations) is presented. The chart shows that progress in
reducing error is quite large, especially in the early stages of the process (Fig. 1.15).
In Fig. 1.16 a graphical representation of the restored function f(x1, x2) and error
recovery data are shown(the training network). The maximum approximation error
did not exceed the level of 0.06, which is about 1 % of the expected values.
Comparing the speed of training and generalizing abilities of the radial network
with similar indicators of multilayer perceptron is unanimous in favor of the ﬁrst
one. It trains faster and is much less sensitive to the choice of initial parameter
values for basis functions and weights of the output neuron.
Fig. 1.15 Training curve of radial RBF network for recovery of three-dimensional function
1.9
Hybrid Training Algorithm for Radial Networks
33

Fig. 1.16 The results of reconstruction of three-dimensional radial functions of the RBF network:
a the recovered surface; b error of recovery
34
1
Neural Networks

1.10
Methods of Selecting the Number of Radial Basis
Functions
The selection of the number of basis functions, each of which corresponds to one
hidden neuron, is considered the main problem that occurs when searching solution
to the problem of approximation. Like sigmoidal networks, too small number of
neurons doesn’t allow to reduce sufﬁciently the generalization error of the training
data set, whereas a too large number of them increases the output error on the set of
testing data. Selection of the necessary and sufﬁcient number of neurons depends on
many factors, including the dimensionality of the problem, the training data size
and primarily spatial structure of the approximated function. Typically, the number
of basis functions K constitutes a certain percentage of the volume of the training
data p, and the actual value of this fraction depends on the dimension of the vector x
and the variability in the expected value di corresponding to the input vectors for
i ¼ 1; 2; . . .; p.
Heuristic Methods
Due to the impossibility of a priori determination of the exact number of hidden
neurons adaptive methods are used that allow you to add or delete them in the
training process. Many heuristic methods were developed that implement these
operations [2, 7]. As a rule, the training of the network starts with any initially
chosen the number of neurons, and subsequently the degree of reduction of the
RMS error and the change of network parameters values are controlled. If the
average change of weight values after a certain number of training cycles is too
small: P
i
Dwi
h
i\n, two basis functions are added (2 neurons) with centers cor-
responding to the largest and smallest adaptation error, and then the training of the
extended structure continues. At the same time the absolute wi values of the weights
of all the individual neurons are controlled. If they are less than the ﬁrstly set
threshold d, the corresponding neurons are subject to removal from the network.
Adding neurons and their removal as well begins after run of a certain number of
training cycles and can occur throughout the training process until the desired
accuracy of the approximation will be achieved.
Another approach to controlling the number of hidden neurons has been pro-
posed by sir Platt [7]. This is a technique that combines elements of
self-organization and training. After presentation of each next training sample the
Euclidian distance between it and the center of the nearest existing radial functions
is determined. If this distance exceeds the threshold dðkÞ, a new radial function is
generated (i.e. neuron is added), after which the network is subjected to the standard
procedure for training using gradient methods (supervised training). The process of
adding neurons is continued until the desired level of error is achieved. Crucial for
this method is the selection of values dðkÞ, according to which a decision is taken to
expand the network. Usually dðkÞ decreases exponentially with time (depending on
the number of iterations) from the value of dmax at the beginning of the process to
dmin at the end of it. The disadvantage of this approach is the impossibility of
1.10
Methods of Selecting the Number of Radial Basis Functions
35

reducing the number of neurons in processing information, even when as a result of
training some of them degenerate (due to unsuccessful placement centers) or when
several neurons begin to duplicate each other, performing the same function. In
addition, this method is very sensitive to the selection of the parameters of the
training process, especially the values of dmax and dmin.
Comparison of Radial and Sigmoidal Networks
Radial neural networks belong to the same category of networks trained with the
teacher as the multilayer perceptron networks. Compared to multi-layer networks
with sigmoidal activation functions, they differ in some speciﬁc properties which
facilitate the approximation characteristics of the simulated process.
Sigmoidal network in which a non-zero value of the sigmoid function extends
from some point in space to inﬁnity, solves the problem of the global approximation
of the given function. At the same time the radial network based on the functions
that have non-zero values only in a certain small area around their centers,
implements a local approximation type, the scope of which, as a rule, is more
limited. Therefore it is necessary to understand that generalizing ability of radial
networks is slightly worse than sigmoidal networks, especially on the training data.
Because of the global nature of the sigmoidal function a multi-layer network
does not have a built-in mechanism for identifying a data region to which a speciﬁc
neuron most strongly reacts. Due to the physical impossibility to associate the
activity of a neuron with the corresponding area of the training data it’s difﬁcult for
sigmoidal networks to determine the initial position of the training process. Taking
into account poly-modality of an objective function, the achievement of the global
minimum in such a situation becomes extremely difﬁcult even with the most
advanced training methods.
Radial network solve this problem much better. The most commonly used in
practice, the radial functions of the Gaussian type are inherently local in nature and
take non-zero values only in the area around a certain center. This makes it easy to
determine the dependence between the parameters of the basis functions and the
spatial location of the training data in multidimensional space. Therefore it’s rel-
atively easy to ﬁnd a satisfactory initial conditions of the training process with the
teacher. The use of such algorithms with initial conditions close to optimal, greatly
increases the likelihood of achieving a success with the application of radial
networks.
It is believed [2] that the radial networks solve better than sigmoidal networks
the classiﬁcation problems, detection of damages in various systems, pattern
recognition tasks, etc.
An important advantage of radial networks is greatly simpliﬁed training algo-
rithms. If there is only one hidden layer and the close relationship of the activity of
the neuron with the corresponding region of the training data location the starting
point of training may be chosen much closer to the optimal solution than in
multi-layer networks. It is also possible to separate the stage of selection of
parameters of the basis functions from the selection of the values of network
weights (hybrid algorithm), which greatly simpliﬁes and speeds up the training
36
1
Neural Networks

process. Time advantage becomes even greater if we take into account the proce-
dure of forming the optimal (from the point of view of ability to generalize) network
structure. While for multilayer networks it is a very time consuming task that
requires, as a rule, multiple repetition of training. For radial networks, especially
based on orthogonalization, the formation of the optimal structure of a network is a
natural step in the training process, requiring no additional effort.
References
1. Zaychenko, Yu.P.: Fundamentals of Intellectual Systems Design, p. 352. Kiev Publishing
House “Slovo” (2004) (rus)
2. Simon, H.: Neural Networks. Full Course. 2nd edn., p. 1104 (Transl. engl.). Moscow
Publishing House “Williams” (2006) (rus)
3. Anderson, J., Rosenfeld, E. (eds.): Neurocomputing: Foundations of Research. MIT Press,
Cambridge, MA (1988)
4. Hornik, K., Stinchcombe M., White, H.: Multilayer feed forward networks are universal
approximators. Neural Networks 2, 359–366 (1989)
5. Voronovsky, G.K., Makhotilo, K.V., Petrashev, S.N., Sergeyev, S.A.: Genetic Algorithms-
Artiﬁcial Neural Networks and Problems of Virtual Reality, p. 212. Kharkov Publishing
House “Osnova” (1997) (rus)
6. Korotky, S.: Neural networks: Self-learning. http://www.orc.ru (1999)
7. Osovsky, S.: Neural Networks for Information Processing. Finance and Statistics, p. 344
(transl. from pol.). M.: Publishing House (2002) (rus)
8. Batischev, D.I.: Genetic algorithms for extremal problems solution. In: Teaching Manual,
p. 65, Voronezh state Technical University, Voronezh (1995) (rus)
9. Batischev, D.I., Isayev, S.A., Remer, E.K.: Evolutionary-genetic approach for non- convex
problem optimization. In: Inter-High School Scientiﬁc Works Collection Simulation
Optimization in Automated Systems, Voronezh State Technical University, Voronezh,
pp. 20–28 (1998) (rus)
10. Batyrshyn, I.Z.: Methods of presentation and processing of fuzzy information in intellectual
systems. In: Scientiﬁc Review in: News in Artiﬁcial Intelligence. Publishing RAAI Anaharsis.
№2, pp. 9–65 (1996) (rus)
1.10
Methods of Selecting the Number of Radial Basis Functions
37

Chapter 2
Neural Networks with Feedback
and Self-organization
In this chapter another classes of neural networks are considered as compared with
feed-forward NN—NN with back feed and with self-organization.
In Sect. 2.1 recurrent neural network of Hopﬁeld is considered, its structure and
properties are described. The method of calculation of Hopﬁeld network weights is
presented and its properties considered and analyzed. The results of experimental
investigations for application Hopﬁeld network for letters recognition under high
level of noise are described and discussed. In the Sect. 2.2 Hamming neural net-
work is presented, its structure and properties are considered, algorithm of weights
adjusting is described. The experimental investigations of Hopﬁeld and Hamming
networks in the problem of characters recognition under different level of noise are
presented. In the Sect. 2.3 so-called self-organizing networks are considered. At the
beginning Hebb learning law for neural networks is described. The essence of
competitive learning is considered. NN with self-organization by Kohonen are
described. The basic competitive algorithm of Kohonen is considered/its properties
are analyzed. Modiﬁcations of basic Kohonen algorithm are described and ana-
lyzed. The modiﬁed competitive algorithm with neighborhood function is descri-
bed. In the Sect. 2.4 different applications of Kohonen neural networks are
considered: algorithm of neural gas, self-organizing feature maps (SOMs), algo-
rithms of their construction and applications.
2.1
Neural Network of Hopﬁeld
Revival of interest in neural networks is connected with Hopﬁeld’s work (1982) [1].
This work shed light on that circumstance that neuron networks can be used for the
computing purposes. Researchers of many scientiﬁc ﬁelds received incentive for
further researches of these networks; pursuing thus the double aim: the best
understanding of how the brain works and application of properties of these
networks.
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_2
39

2.1.1
Idea of Reccurrency
The neural network of Hopﬁeld is an example of a network which can be deﬁned as
a dynamic system with feedback at which the output of one completely direct
operation serves as an input of the following operation of a network, as shown in
Fig. 2.1.
Networks which work as systems with feedback, are called “recurrent net-
works”. Each direct operation of a network is called an iteration. Recurrent net-
works, like any other nonlinear dynamic systems, are capable to show the whole
variety of different behavior. In particular, one possible pattern of behavior is that
the system can be stable, i.e. it can converge to the only ﬁxed (motionless) point.
When the motionless point is an input to such dynamic system, at the output we
will have the same point. Thus the system remains ﬁxed at the same state. Periodic
cycles or chaotic behavior are also possible.
It was shown that Hopﬁeld’s networks are stable. In general case it may be more
than one ﬁxed point. That depends on the starting point chosen for initial iteration to
which ﬁxed point a network will converge.
Motionless points are called as attractors. The set of points (vectors) which are
attracted to a certain attractor in the course of iterations of a network, is called as
“attraction area” of this attractor. The set of motionless points of Hopﬁeld’s net-
work is its memory. In this case the network can work as associative memory.
Those input vectors which get to the sphere of an attraction of a separate attractor,
are connected (associated) with it.
For example, the attractor can be some desirable image. The area of an attraction
can consist of noisy or incomplete versions of this image. There is a hope that
images which vaguely remind a desirable image will be remembered by a network
as associated with this image.
Fig. 2.1 Binary network of
Hopﬁeld
40
2
Neural Networks with Feedback and Self-organization

2.1.2
Binary Networks of Hopﬁeld
In Fig. 2.1 the binary network of Hopﬁeld is represented. Input and output vectors
consist of “–1” and “+1” (instead of “–1”, “0” can be used). There is a symmetric
weight matrix W ¼
wij

 consisting of integers with zeros (or “–1”) on a diagonal.
The input vector X is multiplied by a weight matrix, using usual matrix and vector
multiplication. The input vector X is fed to corresponding neurons and the output
vector is determined. However, only 1 component of an output vector Y ¼ yj
 
is
used on each iteration. This procedure is known as “asynchronous correction”.
This component which can be chosen incidentally or by turn enters to a threshold
element, whose output is –1, or +1). Corresponding component of an input vector is
replaced with this value and, thus, forms an input vector for the following iteration.
Process proceeds until input and output vectors become identical (that is, the
motionless point will be reached) This algorithm is described below.
2.1.3
Description of the Algorithm of Asynchronous
Correction
At the ﬁrst moment a key k is closed (see Fig. 2.1) so an input vector x is fed with
weight
wij

 to input neurons and the total signal at the input of jth neuron SjðxÞ is
deﬁned. Further, the key k is disconnected and outputs of neurons are fed to their
inputs. The following operations are to be made:
• Calculate components of an output vector yj, j = 1, 2, ..., n, using the formula
yj ¼ T
X
n
i¼1
wijxi
 
!
ð2:1Þ
where
TðxÞ ¼
1;
if x\0
1;
if x [ 0
y is not changed;
if x ¼ 0
8
<
:
• To execute asynchronous correction, i.e. [2–4]:
Step 1:
start with an input vector ðx1; x2; . . .; xnÞ:
Step 2:
ﬁnd yj according to the formula (2.1).
Step 3:
replace ðx1; x2; . . .; xnÞ with ðy1; x1; x2; . . .; xnÞ ¼ Y and a feed Y back to
input X.
Step 4:
repeat process to ﬁnd y2; y3, etc. and replace the corresponding inputs.
2.1
Neural Network of Hopﬁeld
41

Repeat steps 2–3 until the vector: Y ¼ ðy1; y2; . . .; ynÞ ceases to change. It was
proved each such step reduces the value of communications energy E if at least one
of outputs has changed:
E0 ¼ 1
2
X
n
i¼1
X
n
j¼1
wijxixj;
ð2:2Þ
so convergence to a motionless point (attractor) is provided.
Asynchronous correction and zeros on a diagonal of a matrix W guarantee that
power function (2.2) will decrease with each iteration [2, 5]. Asynchronous cor-
rection is especially essential to ensuring convergence to a motionless point. If we
allow whole vector to be corrected on each iteration, it is possible to receive a
network with periodic cycles as terminal states of an attractor, but not with
motionless points.
2.1.4
Patterns of Behavior of Hopﬁeld’s Network
Weight matrix distinguishes behavior of one Hopﬁeld’s network from another so
there is a question: “How to deﬁne this weight matrix?”
The answer is it should be given a set of certain weight vectors which are called
etalons. There is a hope that these etalons will be the ﬁxed points of a resultant
Hopﬁeld’s network, though it is not always so. In order to ensure these etalons to be
attractors, the weight matrix W ¼
wij

 should be calculated so [5]:
wij ¼
X
N
k¼1
ðxki  1Þðxkj  1Þ; if i 6¼ j
0; if i ¼ j;
ð2:3Þ
where N is a number of the etalons, Xk is the kth etalon.
If etalon vectors form a set of orthogonal vectors, it is possible to guarantee that
if the weight matrix is determined as shown above in formula (2.3), each etalon
vector will be a motionless point. However, generally in order that etalons become
motionless points, orthogonality isn’t obligatory.
It should be noted that Holﬁeld network weights aren’t trained like BP or RBF
networks but are calculated in accordance with formula (2.3).
42
2
Neural Networks with Feedback and Self-organization

2.1.5
Application of Hopﬁeld’s Network
Hopﬁeld’s network can be used in particular for images recognition. But the
number of the recognizable images isn’t too great owing to limitation of memory of
Hopﬁeld’s networks. Some results of its work are presented below in the experi-
ment of adjusting network to recognize letters of the Russian alphabet [2].
Initial images are:
Research was conducted thus: consistently increasing noise level each of 4
images, they are fed to inputs of Hopﬁeld’s network. Results of network func-
tioning are given in Table 2.1.
Thus, Hopﬁeld’s neural network perfectly copes with a problem of recognition
of images(pattern) in experiments with distortion of 0–40 %. In this range all
images(pattern) were recognized without mistakes (sometimes there are insigniﬁ-
cant distortions for 40 % level of noise).
At 45–60 % level of noise images(patterns) are recognized unstably, often there
is “entangling” and at the neural network output appears absolutely other image
(pattern) or its negative.
Beginning from 60 % noise level at the system output the negative of the tested
image(pattern) appears which sometimes is partially distorted (starts appearing at
60–70 %).
2.1
Neural Network of Hopﬁeld
43

Table 2.1 Experiments with Hopﬁeld network
The tested image 
Percent of 
image dis-View of the distorted image Result of recognition
tortion
10% 
20% 
30% 
35% 
40% 
45% 
(continued)
44
2
Neural Networks with Feedback and Self-organization

Table 2.1 (continued)
50% 
60% 
70% 
80% 
90% 
100% 
(continued)
2.1
Neural Network of Hopﬁeld
45

Table 2.1 (continued)
10% 
20% 
30% 
40% 
45% 
50% 
(continued)
46
2
Neural Networks with Feedback and Self-organization

Table 2.1 (continued)
60% 
65% 
70% 
80% 
90% 
100% 
(continued)
2.1
Neural Network of Hopﬁeld
47

Table 2.1 (continued)
10% 
20% 
30% 
40% 
45% 
50% 
(continued)
48
2
Neural Networks with Feedback and Self-organization

Table 2.1 (continued)
55% 
60% 
70% 
80% 
90% 
100% 
2.1
Neural Network of Hopﬁeld
49

2.1.6
The Effect of “Cross–Associations”
Let’s complicate a task and train our neural network with one more pattern:
The letter “П” is very similar to letters “И” and “H” which already exist in the
memory (Fig. 2.2). Now Hopﬁeld’s neural network can’t distinguish any of these
letters even in an undistorted state. Instead of correctly recognizable letter it dis-
plays the following image (for distortion of an pattern from 0 to 50 %):
It looks like to each of letters “И”, “H”, “П” but isn’t the correct interpretation of
any of them (Fig. 2.3).
From 50 to 60 % noise level at the output of neural network at ﬁrst appears an
image presented above (Fig. 2.3) in slightly distorted form, and then its negative.
Since 65 % of noise level, at the neural network output steadily there is an image
negative to shown in Fig. 2.3.
The described behavior of a neural network is known as effect of “cross asso-
ciations” [2]. Thus “А” and “Б” letters are recognized unmistakably at noise level
up to 40 %.
At 45–65 % noise level at the network output appear, their slightly noisy
interpretations, the image similar to a negative of a letter “B” (but very distorted), or
a negative of the tested image (pattern). At distortion level of 70 % and more neural
network steadily displays its negative of the tested image.
The experimental investigations had revealed the following shortcomings of
Hopﬁeld’s neural network:
1. existence of cross-associations when some images(patterns) are similar to each
other (like the experiments with letter П);
2. due to storing capacity restrictions the number of the remembered attractors
(patterns) is only (0, 15–0, 2) n, where n is dimension of a weight matrix W.
These circumstances signiﬁcantly limit possibilities of practical use of
Hopﬁeld’s network.
Fig. 2.2 New test symbol
50
2
Neural Networks with Feedback and Self-organization

2.2
Neural Network of Hamming. Architecture
and Algorithm of Work
When there is no need that the network would display an etalon pattern in an explicit
form, that is it is enough to deﬁne, say, a class of a pattern, associative memory is
realized successfully by Hamming’s network. This network is characterized, in
comparison with Hopﬁeld’s network, by smaller costs of memory and volume of
calculations that becomes obvious of its structure and work (Fig. 2.4) [2, 6].
The network consists of two layers. The ﬁrst and second layers have m neurons,
where m is a number of patterns. Neurons of the ﬁrst layer have n synapses
connected to the network inputs (forming a ﬁctitious zero layer). Neurons of the
second layer are connected among themselves by synaptic connections. The only
synopsis with positive feedback for each neuron is connected to his axon.
The idea of network functioning consists in ﬁnding of Hamming distance from
the tested pattern to all patterns represented by their weights. The number of dif-
ferent bits in two binary vectors is called as Hamming distance. The network has to
choose a pattern with the minimum of Hamming distance to an unknown input
signal therefore the only one of a network outputs corresponding to this pattern will
be made active.
At an initialization stage the following values are assigned to weight coefﬁcients
of the ﬁrst layer and a threshold of activation function:
w ¼ xk
i
2 ; i ¼ 1; n; k ¼ 1; m
ð2:5Þ
Tk ¼ n
2 ;
1; m
ð2:6Þ
Fig. 2.3 Output symbol for
tested symbols И, H, П
2.2
Neural Network of Hamming …
51

Here xk
i is the ith an element of the kth pattern.
Weight coefﬁcients of the braking synapses in the second layer are set equal to
some value −e, where 0\e\ 1
m., where m is a number of classes.
The neuron synapse connected with its own axon has a weight (+1).
2.2.1
Algorithm of a Hamming Network Functioning
1. Enter the unknown vector X ¼ fxi : i ¼ 1; ng to a network input and determine
outputs of the ﬁrst layer neurons (the top index in brackets in formula (2.7)
speciﬁes number of a layer):
yð1Þ
j
¼ f ðsð1Þ
j Þ ¼ f
X
n
i¼1
wijxi þ Tj
 
!
; j ¼ 1; m
ð2:7Þ
After that initialize states of axons of the second layer with received values:
yð2Þ
j
¼ yð1Þ
j
j ¼ 1; m
ð2:8Þ
2. Calculate new states of the second layer neurons
Fig. 2.4 Architecture of Hamming network
52
2
Neural Networks with Feedback and Self-organization

sð2Þ
j ðp þ 1Þ ¼ yjðpÞ  e
X
m
k¼1
yð2Þ
k ðpÞ; j ¼ 1; m
ð2:9Þ
And values of their axons:
yð2Þ
j ðp þ 1Þ ¼ f sð2Þ
j ðp þ 1Þ
h
i
; j ¼ 1; m
ð2:10Þ
Activation function f has a threshold, thus the size of a threshold should be
rather big so that any possible values arguments won’t lead to saturation.
3. Check, whether output of the second layer neurons has changed since the last
iteration. If yes, then pass to a step 2 of the next iteration, otherwise—the end.
From the description of algorithm it is evident that the role of the ﬁrst layer is
very conditional, having used once on a step 1 value of its weight coefﬁcients, the
network doesn’t come back to it any more, so that the ﬁrst layer may in general be
excluded from a network and replaced with a matrix of weight coefﬁcients.
Note advantages of neural network of Hamming:
• small costs of memory;
• the network works quickly;
• algorithm of work is extremely simple;
• capacity of a network doesn’t depend on dimension of an input signal (as in
Hopﬁeld’s network) and equals exactly to a number of neurons.
2.2.2
Experimental Studies of Hopﬁeld’s and Hamming’s
Networks
Comparative experimental researches of Hopﬁeld’s and Hamming’s neural net-
works in a problem of symbols recognition were carried out. For learning of a
network input sample of symbols (1, 7, e, q, p) was used. Then generated noisy
patterns from this sample were entered and their recognition was performed. Level
of noise changed from 0 to 50 %. Results of recognition of the speciﬁed symbols
are presented in Fig. 2.5. On the screen 4 images (patterns) are presented (from left
to right, from top to down): the initial image—a etalon, the noisy image, result of
Hamming network, result of Hopﬁeld’s network (Figs. 2.6, 2.7, 2.8, 2.9).
2.2.3
Analysis of Results
Results of experiments with Hopﬁeld’s and Hamming’s networks are presented in
the Table 2.2. A corresponding table element is a result of recognition by a network
2.2
Neural Network of Hamming …
53

Symbol 1
Fig. 2.5 Recognition of the symbol 1
54
2
Neural Networks with Feedback and Self-organization

Symbol 7
Fig. 2.6 Recognition of the symbol 7
2.2
Neural Network of Hamming …
55

Symbol E 
Fig. 2.7 Recognition of the symbol e
56
2
Neural Networks with Feedback and Self-organization

Symbol Q
Fig. 2.8 Recognition of the symbol q
2.2
Neural Network of Hamming …
57

Symbol P
Fig. 2.9 Recognition of a p symbol
58
2
Neural Networks with Feedback and Self-organization

(Hamming; Hopﬁeld) of a symbol at the speciﬁed noise level, and namely: 0—it
isn’t recognized; 0.5—it is recognized with defects; 1—it is recognized correctly.
Hamming’snetwork in general performed well (except for “e” symbol) and
recognized correctly up to the level of noise 40 %), while Hopﬁeld’s network
results of recognition are much worse, at recognition of symbols with a similar
elements (e, p, q) there were difﬁculties—recognition level was less than 30 %, it is
the effect of cross associations.
2.3
Self-organizing Neural Networks. Algorithms of
Kohonen Learning
2.3.1
Learning on the Basis of Coincidence. Law of Hebb
Learning
In 1949 the Canadian psychologist D. Hebb published the book “Organization of
Behaviour” in which he postulated the plausible mechanism of learning at the
cellular level in a brain [2, 7].
The main idea of Hebb consisted therein when the input signal of neuron
arriving through synaptic communications causes activation operation of neuron,
efﬁciency of such input in terms of its ability to cause operation of neuron in the
future has to increase.
Hebb assumed that change of efﬁciency has to happen in a synapse which
transmits this signal to an neuron input. The latest researches conﬁrmed this guess
of Hebb. Though recently other mechanisms of biological learning at the cellular
level were detected, but in recognition of merits of Hebb this law of learning was
called in his honor [7].
The law of Hebb learning belongs to a class of laws of learning by competition.
Linear Associative Elements
In Fig. 2.10 the architecture of the neural network (NN) consisting of m neurons
which are called as “linear associators” is presented.
The input vector in the linear associator is a vector, X ¼ fxig; i ¼ 1; n, which is
taken out of space Rn according to some distribution qðxÞ.
Table 2.2 Comparative results of experiments with Hopﬁeld and Hamming networks
Recognition (Hamm, Hop)
0 %
10 %
20 %
30 %
40 %
50 %
1
(1; 1)
(1; 0.5)
(1; 1)
(1; 0.5)
(1, 0.5)
(0; 0)
7
(1; 1)
(1; 0.5)
(1; 0.5)
(1; 1)
(1, 0.5)
(0; 0)
E
(1; 1)
(1; 0.5)
(1; 1)
(1; 1)
(0; 0)
(0; 0)
Q
(1; 0)
(1; 0)
(1; 0)
(1; 0)
(1; 0)
(0; 0)
P
(1; 1)
(1; 1)
(1; 0)
(1; 0)
(1; 0)
(0; 0)
2.2
Neural Network of Hamming …
59

Fig. 2.10 Network with linear associators–neurons
The output vector is obtained from input X by the following formula:
Y ¼ WX
ð2:11Þ
where W ¼
wij

 is a weight matrix n  m; W ¼ ðW1; W2; . . .; WmÞ, Wj is W-
matrix column, Wj ¼ ðW1j; W2j; . . .; WnjÞT is a weight vector.
We will designate through Y0 ¼ fyjg a desirable output. The main idea of a
linear associative neural network consists that the network has to learn on pairs
input-output:
ðX1; Y1Þ; ðX2; Y2Þ; . . .; ðXL; YLÞ
When to an neural network input the signal Xk is given, desirable output Y0 has
to be equal Yk. If on an network input the vector Xk þ e is given (where e—rather
small), the output has to be equal Yk þ e (i.e. at the output have to receive the vector
close to Yk).
The law of Hebb’s learning is as follows [2, 7]:
wnew
ij
¼ wold
ij þ ykjxki;
ð2:12Þ
Where xki—is the ith vector component Xk; ykj—jth vector component Yk.
In a vector form the expression (2.12) is written so:
Wnew ¼ Wold þ XkYT
k ¼ Wold þ YkXT
k ;
ð2:13Þ
To realize this law in the course of learning the corresponding components
Yk ¼ ykj


, which are shown by dashed lines (arrows) in Fig. 2.10 are entered.
60
2
Neural Networks with Feedback and Self-organization

It is supposed that before learning, all wð0Þ
ij
¼ 0. Then as a result of display of the
learning sample ðX1; Y1Þ; . . .; ðXL; YLÞ the ﬁnal state of a matrix of W is deﬁned so:
W ¼ Y1XT
1 þ Y2XT
2 þ    þ YLXT
L :
ð2:14Þ
This Eq. (2.14) is called “a formula of the sum of external product” for W. This
name comes from that fact that YkXT
k —is an external product.
The reformulation of Hebb law in the form of the sum of external product allows
to conduct additional researches of opportunities of this law to provide associations
of pairs of vectors ðXkYkÞ.
The ﬁrst conclusion consists that if vectors
X1; X2; . . .; XL
f
g are ortogonal and
have unit length, i.e. are orthonormalized, then
Yk ¼ WXk;
ð2:15Þ
In other words, the linear associative neural network will produce desirable
transformation “input-output”.
This is the consequence of an orthonormalization property:
XT
i Xj ¼ dij ¼
0; i 6¼ j
1; i ¼ j

ð2:16Þ
Then
WXk ¼
X
L
r¼1
YrXT
r Xk ¼ Yk:
ð2:17Þ
But the problem consists that an orthonormalization condition is very rigid (ﬁrst
of all it is necessary, that L  n).
Further we are restricted by the requirement that
Xi
k
k ¼ 1. It would be much
more useful if it was succeeded to lift this restriction. These goal can be achieved,
but not in a linear associative neural network. Here, if vector s Xk aren’t
orthonormalized, then a reproduction error Yk appears at the output:
WXk ¼
X
L
r¼1
YrXT
r Xk ¼ Yk þ
X
r6¼k
YrXT
r Xk ¼ Yk þ g:
ð2:18Þ
It is desirable to achieve that η be minimum. To provide g ¼ min or g ¼ 0, it is
necessary to pass to a nonlinear associative network with nonlinear elements.
2.3.2
Competitive Learning
Competitive learning is used in problems of self-learning, when there is no clas-
siﬁcation of the teacher.
2.3
Self-organizing Neural Networks. Algorithms …
61

The laws of learning relating to category competitive, possess that property that
there arises a competitive process between some or all processing elements of a
neural network. Those elements which appear winners of competition, get the right
to change their weights, while the rest the of weights don’t change (or change by
another rule).
Competitive learning is known as “Kohonen’s learning”. Kohonen’s learning
signiﬁcantly differs from Hebb learning and BP algorithm by therein the principle
of self-organization is used (as opposed to the principle of controlled learning with
the teacher).
The competitive law of learning has long and remarkable history [2]. In the late
sixties—the beginning of the 70th Stephen Grossberg suggested the whole set of
competitive learning schemes for neural networks. Another researcher who dealt
with problems of competitive learning was van der Malsburg. The learning law of
van der Malsburg was based on idea that the sum of the weights of one input
element connected with various processing neurons has to remain a constant in the
course of learning i.e. if one of weights (or some) increases, the others have to
decrease.
After considerable researches and studying works of Grossberg, van der
Malsburg and others Toivo Kohonen came to the conclusion that the main goal of
competitive learning has to consist in designing of a set of vectors which form a set
of equiprobable representatives of some ﬁxed function of distribution density qðxÞ
of input vectors. And though learning laws of this type were independently received
by many researchers, T. Kohonen was the ﬁrst who paid attention to a question of
equiprobability. Exactly thanks to this idea and the world distribution of T.
Kohonen book “Self-organization and associative memory” [8] his name began to
associate with this law of learning.
2.3.3
Kohonen’s Learning Law
The basic structure of a layer of Kohonen neurons is given in Fig. 2.11. The layer
consists of N processing elements, each of which receives n input signals
x1; x2; . . .xn from a lower layer which is the direct transmitter of signals. To an input
xi and communication (i, j) we will attribute weight wij.
Each processing element of a layer of Kohonen counts the input intensity Ij in
compliance with a formula [2, 8]:
Ij ¼ DðWj; XÞ;
ð2:19Þ
where Wj ¼ ðw1j; w2j; . . .; wnjÞT and X ¼ ðx1; x2; . . .; xnÞ; DðWj; XÞ—some measure
(metrics) of distance between Wj and X.
62
2
Neural Networks with Feedback and Self-organization

We will deﬁne two most general forms of function DðWj; XÞ:
1. Euclide distance: dðW; XÞ ¼ W  X
k
k;
2. Spherical arc distance:
DðW; XÞ ¼ 1  WTX ¼ 1  cos h
ð2:20Þ
where WTX—the scalar product, and is supposed that
W
k
k ¼ X
k k ¼ 1:
In this statement, unless otherwise stated, we’ll use Euclidean distance dðW; XÞ.
At implementation of the Kohonen law as soon as each processing element (neuron)
counted the function Ij, a competition between them takes place is, whose purpose
is to ﬁnd an element with the smallest value Ij (i.e. Ijmin). As soon as the winner of
such competition is found, his output z is put equal to 1. Output signals of all other
elements remain equal to 0.
At this moment a learning by Kohonen takes place.
The learning data for Kohonen’s layer assumed to consist of sequence of input
vectors fXg, which are taken randomly with the ﬁxed density of probabilities
distribution qðxÞ. As soon as next vector X it is entered into a network, the pro-
cessing Kohonen’s neurons start competing to ﬁnd the winner for whom
min
j
dðX; WjÞ is reached. Then for the winner neuron j output is established
zj ¼ 1, and for all others zj ¼ 0; j 6¼ j.
At this moment a change of weights according to Kohonen learning law is
performed:
Fig. 2.11 Architecture of Kohonen network
2.3
Self-organizing Neural Networks. Algorithms …
63

Wnew
j
¼ Wold
j
þ a X  Wold
j

	
zj;
ð2:21Þ
where 0\a\1.
This law can be written in the following form:
Wnew
j
¼
ð1  aÞWold
j
þ aX; for winner j ¼ j
Wold
j
; 8 j 6¼ j
(
ð2:22Þ
It is evident that at such learning law the weight vector Wj moves to an input
vector X. At the beginning of learning process a ¼ 1 and then in process of learning
it monotonously decreases up to the value a ¼ 0; 1.
This algorithm realizes the principle “The winner takes all” therefore in foreign
literature it is called WTA. Further it should be noted similarity of learning by
Kohonen and statistical process of ﬁnding of “k-means”.
K-means for the ﬁxed set of vectors
X1; X2; . . .; XL
f
g, which are chosen ran-
domly from some population with the ﬁxed density of probabilities distribution
qðxÞ, make a set of k vectors W ¼ ðW1; W2; . . .; WkÞ such that the following
functional is minimized:
min
wi
f
g
X
L
i¼1
D2ðXi; WðXiÞÞ
ð2:23Þ
Where WðXiÞ is a vector W, closest to Xi.
In summary, it is necessary to emphasize that the learning by Kohonen’s
algorithm generally doesn’t generate a set of equiprobable weight vectors, that is a
set of such vectors that X which is chosen randomly, according to density of
probabilities distribution q will have equal probability to be the closest to each of
weight vectors Wj.
2.3.4
Modiﬁed Competitive Learning Algorithms
As it was already noted above, we seek to get vectors Wj, which would be
approximately equally probable in the sense of being the closest to the vectors of X
taken from <n with some density of probability distribution In other words, for any
vector of X taken from <n with probability qðxÞ, it is desirable that the probability
of that X will appear to be the closest to Wi, has to be approximately equal to 1
N for
all i ¼ 1; N.
There are some approaches for the solution of the problems arising at imple-
mentation of a basic learning law of Kohonen [2, 9].
64
2
Neural Networks with Feedback and Self-organization

1. The ﬁrst approach is called as Radial Sprouting. It is the best for Euclidean metrics
and metrics (distances) similar to it. All weight vectors Wi ¼ wij


are originally
set equal to wijð0Þ ¼ 0. All input vectors X at ﬁrst are multiplied by some small
positive scalar b. Process begins with b, close to 0. It provides proximity of input
vectors of X to vectors Wj. In process development b slowly increases, until
reaches the value b ¼ 1. As soon as it occurs, weight vectors “are pushed out”
from the initial values and follow input vectors. This scheme works quite well, but
usually some weight vectors will lag behind process and as a result will be not
involved in the competion process that slows down learning process.
2. Other approach (“noise addition”) consists in adding randomly distributed noise
to data vectors X that facilitates effect of achievement qðXÞ [ 0, in all area Xx
Level of noise is chosen at ﬁrst rather big so that noise vector be much greater,
than a data vector X. But in the process of learning noise level gradually
decreases. This approach works correctly, but it appears even more slowly, than
approach of “Radial Sprouting”. Therefore approaches of “Radial Sprouting”
and “noise addition” solve a problem of presentation of badly representable laws
with small probability of distribution in some area, but they don’t solve a
problem of equiprobable positioning of vectors Wj.
In general, the basic Kohonen’s learning law will bring to a surplus at placement
of vectors Wj in those areas where probabilities distribution density qðXÞ, is large,
and to shortage of vectors Wj in areas where density of probabilities distribution
qðXÞ is small.
3. The third approach which was offered by Duane Desieno, is to build-in “con-
sciousness” (or memory) in each element k to carry out monitoring (control) of
history of successful results (victories) of each neuron. If the processing
Kohonen element wins competition signiﬁcantly more often than 1
N times (time),
then his “consciousness” excludes this element from competition for some time,
thereby giving the chance to elements from the oversaturated area to move to the
next non-saturated areas. Such approach often works very well and is able to
generate good set of equiprobable weight vectors.
The main idea of the consciousness mechanism is a tracking of a share of time
during which the processing element j wins competition. This value can be cal-
culated locally by each processing element by formula:
fjðt þ 1Þ ¼ fjðtÞ þ b zj  fjðtÞ


ð2:24Þ
When competition is ﬁnished and the current value zj (0 or 1) is deﬁned, the
constant b takes a small positive value (typical value b ¼ 104 ¼ 0; 0001) and the
share fj is calculated. Right after it the current shifts value bj is deﬁned
2.3
Self-organizing Neural Networks. Algorithms …
65

bj ¼ cð1
N  fjÞ
ð2:25Þ
where c—positive constant (c  10).
Further the correction of weights is carried out. However, unlike a usual situation
in which weight are adjusted only for one processing element-winner with zi ¼ 1,
here separate competition is being held for ﬁnding of the processing element which
has the smallest value of
DðWj; XÞ  bj
ð2:26Þ
The winner element corrects further the of weight according to the usual law of
Kohonen’s learning.
The role of the shift bj is as follows. For often winning elements j the value
fj [ 1
N and bj\0 therefore for them value DðWj; XÞ  bj increases in comparison
with DðWj; XÞ for seldom winning elements fj  1
N ; bj [ 0 and DðWj; XÞ  bj
decreases that increases their chances to win competiton. Such algorithm realizes
the consciousness mechanism in work of self-organizing neuron networks and
therefore it is called as CWTA (Conscience Winner Takes All) [9].
2.3.5
Development of Kohonen Algorithm
In 1982 T. Kohonen suggested to introduce into the basic rule of competitive learning
information on an arrangement of neurons in an output layer [2, 8, 10]. For this purpose
neurons of an output layer are ordered, forming a one-dimensional or two-dimensional
lattice. The arrangement of neurons in such lattice is marked by a vector index
i ¼ ði1; i2Þ. Such ordering naturally enters distance between neurons i  j
j
j.
The modiﬁed rule of competitive of Kohonen’s learning considers distance of
neurons from winner neuron [2, 8, 9]:
Wjðt þ 1Þ ¼ WjðtÞ þ a ðX  WjÞKðdði; jÞÞ
ð2:27Þ
where K—function of the neighborhood. Kðdði; jÞÞ is equal 1 for winner neuron
with an index j, and gradually decreases in process of increase in distance d, for
example, by function
KðdÞ ¼ ed2=R2
ð2:28Þ
Both rate of learning a, and radius of interaction R gradually decreases in the
course of learning so at a ﬁnal stage of learning we come back to the basic law of
weights adaptation only of winner neurons aðtÞ ¼ a0ekt.
As we can see in this algorithm the principle 2 is realized: winner takes away not
all but maximum (income) therefore in foreign literature it is called as WTM
(Winner Takes MOST).
66
2
Neural Networks with Feedback and Self-organization

Learning by Kohonen modiﬁed algorithm WTM reminds a tension of an elastic
grid of prototypes on a data ﬁle from the learning sample. In process of learning the
elasticity of a network gradually decreases and weights changes also decrease
except winner neuron.
As a result we receive not only quantization of input’s, but also we order input
information in the form of a one-dimensional or two-dimensional topographic map
of Kohonen. On this grid each multidimensional vector has the coordinate, and the
closer are coordinates of two vectors on the grid, the closer they are in the initial
space.
2.3.6
Algorithm of Neural Gas
Acceleration of convergence of the modiﬁed Kohonen WTM’S algorithm and the
best self-organization of a network gas molecules motion can be received with
application of the method offered by M. Martinez, S. Berkovich and K. Shulten [9]
and called by authors “algorithm of neural gas” owing to similarity of its dynamics.
In this algorithm on each iteration all neurons are sorted depending on their distance
to a vector X. After sorting neurons are marked in the sequence corresponding to
increase in their remoteness:
d0\d1\d2\    \dn1;
ð2:29Þ
where di ¼
X  WmðiÞ

 designates remoteness from a vector X of the ith neuron
taking as a result of sorting position m in the sequence begun with neuron winner to
which remoteness d0 is put into compliance. Value of the neighborhood function for
ith neuron is determined by a formula [9]:
Kði; xÞ ¼ exp  mðiÞ
rðtÞ


;
ð2:30Þ
where
mðiÞ
deﬁnes
the
sequence
received
as
a
result
of
sorting
of
(mðiÞ ¼ 0; 1; 2; . . .; n  1), and rðtÞ is the parameter similar to R neighbourhood
level in Kohonen WTM’s algorithm decreasing eventually with t.
At rðtÞ ¼ 0 adaptation only of the winner neuron occurs and the algorithm turns
into usual (basic) Kohonen’s algorithm, and at rðtÞ 6¼ 0 of adaptation are subject
the weights of many neurons neighbors, and the level of change of scales depends
on size Kði; xÞ.
For achievement of good results of self-organization, process of learning has to
begin with rather great value of r, but eventually its size decreases to zero. Change
of rðtÞ can be linear or exponential. In work [9] it was offered to change value
according to expression
2.3
Self-organizing Neural Networks. Algorithms …
67

rðtÞ ¼ rmax
rmin
rmax


t
Tmax
;
ð2:31Þ
where rðtÞ—value on iteration of t; rmin and rmax—the accepted minimum and
maximum values of r. Value Tmax deﬁnes the maximum number of iterations. The
learning coefﬁcient of the ith neuron aiðtÞ can also change both linearly, and
exponentially, and its exponential dependence is deﬁned by expression
aiðtÞ ¼ aið0Þ
amin
aið0Þ


t
Tmax
;
ð2:32Þ
where aið0Þ is an initial value a, and amin is a priori set minimum value corre-
sponding to t ¼ Tmax. In practice the best results of self-organization are reached at
linear change of aðtÞ [9]. For reduction of calculations volume at realization of
neural gas algorithm it is possible to use the simpliﬁcation consisting that adapta-
tions of weights happen only for the ﬁrst k of neurons neighbors in the ordered
sequence of d0\d1\d2\. . .\dk.
Algorithm of neural gas together with Kohonen’s algorithm (CWTA) with
memory considering a share of victories of each neuron of fj are the most effective
tools of neurons self-organization in Kohonen’s network.
Comparative Analysis of Algorithms of Self-organization
Above considered algorithms were compared at the solution of a problem of
recovery of the two-dimensional learned data of difﬁcult structure which are pre-
sented in Fig. 2.12 [9]. For recovery of data 2 sets of the neurons including 40 and
200 elements were used which after ordering positions of neurons will reﬂect data
distribution. They have to locate in areas of the maximum concentration of data.
Results of self-organization of 40 neurons when using three algorithms are
presented in Fig. 2.13 algorithm with memory (CWTA) (Fig. 2.13a), neural gas
(Fig. 2.13b) and basic algorithm of Kohonen (Fig. 2.13c). For comparison in
Fig. 2.15 similar pictures are obtained by Kohonen’s network consisting of 200
neurons (Fig. 2.14).
Fig. 2.12 Data structures to be simulated
68
2
Neural Networks with Feedback and Self-organization

As follows from the given results, irrespective of number of neurons the best
results of self-organization were received with use of algorithms of the
self-organization with memory and neural gas.
For quantitative comparison of results it is possible to use criterion “a quanti-
zation error”:
Eq ¼
X
n
i¼1
Xi  W
i

;
ð2:33Þ
where W
i is winner neuron weight at presentation of a vector Xi.
At 200 neurons the following criterion values were received [9]: Eq = 0,007139
for CWTA; Eq = 0,007050 for algorithm of neural gas and Eq = 0,02539 for basic
Kohonen’s algorithm.
2.4
Application of Kohonen Neural Networks
Neural networks with self-organization are used in two main directions.
1. For automatic classiﬁcation of objects.
2. For visual display of properties of multidimensional space (representation of
multidimensional vectors of features).
Fig. 2.13 Results of self-organization of different algorithms with 40 neurons
Fig. 2.14 Results of self-organization for 200 neurons
2.3
Self-organizing Neural Networks. Algorithms …
69

In the ﬁrst case the problem of automatic splitting a set of the objects represented
by multidimensional vectors in some feature space on similarity—to difference of
feature values is solved. Such task sometimes is called as a task of the cluster
analysis and it is in detail considered in the 7th chapter.
In the second case the problem of visualization of properties of multidimensional
feature vectors on the two-dimensional plane is solved.
So-called “Self-organizing Maps” (SOM) are used for this purpose [10].
For this in multidimensional space the spatial lattice is stretched in which nodes
are processing neurons (a layer of Kohonen’s neurons). Further the next points to
each of these neurons are deﬁned. They deﬁne area of an attraction of this neuron.
Average value of each feature of neurons in the attraction area is deﬁned –Xi cp.
Further nodes of a lattice are mapped on the plane in the form of squares or
hexagons, the corresponding value of a feature for this neuron is painted in the
certain color: from blue (the minimum value of a feature) to red (the maximum
value). As a result we receive the self-organizing feature map similar to a geo-
graphical map. Such maps are built on all features and we receive a certain atlas.
Self-organizing Maps (SOMs) represents a method of design of N-dimensional
input space in discrete output space which makes an effective compression of input
space in a set of the coded (weight) vectors. The output space usually represents
itself a two-dimensional lattice. SOM uses a lattice for approximation of probability
density function of an input space, thus keeping its structure i.e. if two vectors are
close to each other in the input space, they have to be close and on the map as well.
During self-organization process SOM carries out an effective clustering of input
vectors, keeping structure of initial space [10].
Stochastic Algorithm of Learning
Learning of SOM is based on strategy of competitive learning. We will consider
N-dimensional input vectors Xp where the index p designates one learning pattern.
The ﬁrst step of process of learning is deﬁnition of structure of the maps, usually
two-dimensional lattice. Map is usually square, but may be rectangular. The number
of elements (neurons of an output layer) on the map is less, than the number of the
learning patterns (samples). The number of neurons has to be equal in an ideal to
the number of the independent learning patterns. The structure of SOM is given in
Fig. 2.15. Each neuron on the Map is connected with a N-dimensional weight
vector which forms the center of one cluster. Big cluster groups are formed by
grouping together of “similar” next neurons.
Initialization of Weight Vectors Can Be Carried Out in Various Ways
1. To each weight Wkj ¼
Wkj1; Wkj2; . . .; WkjN


, where k is a number of rows and
J is a number of columns, random values are attributed. Initial values are
limited to the range of the corresponding input parameter (variable). Though it
is simple to realize random initialization of weight vectors, such way of ini-
tialization gives a big variation of components into SOM that increases learning
time.
70
2
Neural Networks with Feedback and Self-organization

2. To weight vectors randomly chosen input patterns are attributed, i.e. Wkj ¼ Xp,
where p is a pattern index. Such approach can result in premature convergence
until weight aren’t disturbed with small casual values.
3. To ﬁnd the main components of vectors of input space and to initialize weight
vectors with these values.
4. Other technology of initialization of scales consists in deﬁnition of a hyper cube
of rather big size covering all learning patterns. The algorithm begins work with
ﬁnding of four extreme points by deﬁnition of four extreme learning patterns.
At ﬁrst ﬁnd two patterns with the greatest distance from each other in an
Euclidean metrics. The third pattern is placed in the most remote point from
these two patterns, and the fourth pattern—with the greatest distance from these
three patterns according to Euclid. These four patterns form lattice corners on
the Map (SOM: (1, 1); (1, J); (K, 1); (K, J). Weight vectors of other neurons are
deﬁned by interpolation of four chosen patterns as follows. The weight of
boundary neurons are initialized so [10, 11]:
5. W1j ¼ w1J  w11
J  1
ðj  1Þ þ W11;
6. WKj ¼ wKJ  wK1
J  1
ðj  1Þ þ WK1;
7: Wkj ¼ wKJ  w11
K  1
ðk  1Þ þ W11;
ð2:34Þ
8. WKJ ¼ wKJ  w1J
K  1
ðk  1Þ þ W1J;
9. For all j ¼ 2; 3; . . .; J  1 and k ¼ 2; 3; . . .; K  1.
10. other weight vectors are initialized so:
Fig. 2.15 Structure of SOM
2.4
Application of Kohonen Neural Networks
71

WKj ¼ wKJ  wK1
J  1
ðj  1Þ þ WK1:
ð2:35Þ
The standard algorithm of learning for SOM is stochastic in which weight
vectors adapt after display of each pattern of a network. For each neuron the
related code vector (weight) adapts so:
Wkjðt þ 1Þ ¼ WkjðtÞ þ hmn;kjðtÞ Xp  WkjðtÞ


;
ð2:36Þ
where m, n—index of a line and column of winner neuron correspondingly.
The winner neuron is deﬁned, as usual, by calculation of Euclidean distance
from each weight vector to an input vector and a choice of neuron, the closest to
an input vector, i.e.
Wmn  Xp

2¼ min
Wkj  Xp

2
n
o
ðk;jÞ
;
ð2:37Þ
Function hmn;kjðtÞ in the Eq. (2.36) is considered as function of the neighbor-
hood. Thus, only those neurons which are in the vicinity (in the neighborhood)
of (m, n) winner neuron, change the weights. It is necessary for ensuring
convergence, that hmn;kjðtÞ ! 0 at t ! 1.
Function of the neighborhood usually is function of distance between coordi-
nates of the neurons presented on the map i.e.
hmn;kj tð Þ ¼ h
cmn  ckj

2; t

	
;
ð2:38Þ
where cmn; ckj 2 R, and with increase in distance
cmn  ckj

2,hmn;kjðtÞ ! 0.
The neighbourhood can be determined by a square or a hexagon. However are
most often used a smooth Gaussian kernel:
hmn;kj tð Þ ¼ a tð Þ exp  cmn  ckj

2
2r2 tð Þ
 
!
:
ð2:39Þ
Here aðtÞ is the learning speed, and rðtÞ-kernel width. Both functions aðtÞ and
rðtÞ are monotonously decreasing functions with increase in t.
Creation of SOM in
Batch Mode
Stochastic learning algorithm of SOM is too slow owing to need of weights
adaptation of all neurons after each display of a pattern. The version of SOM
learning algorithm in batch mode was developed. The ﬁrst package SOM
learning algorithm was developed by Kohonen and is described below [11].
72
2
Neural Networks with Feedback and Self-organization

1. To initialize weight vectors by purpose of the ﬁrst KJ of the learning pat-
terns where KJ—total number of patterns on the map.
Until stop condition won’t be satisﬁed,
for each neuron of kj do
make the list of all patterns of Xp which are the closest to a weight vector of this
neuron;
end.
2. For each weight vector of wkj calculate new value of a weight vector as an
average of the corresponding list of patterns.
Also the accelerated version of package SOM learning algorithm was devel-
oped. One of the design problems arising at creation of SOM—determination of
the map sizes. Too many neurons can cause glut when each learning pattern is
attributed to various neurons. Or alternately, ﬁnal SOM can successfully create
good clusters from similar patterns, but many neurons will be with zero or close
to the zero frequency of use where the frequency of neuron means number of
patterns for which the neuron became the winner. Too small number of neu-
rons, on the other hand, leads to clusters with big intra cluster dispersion.
The Accelerated Package Algorithm of Creation of SOM
1. To initialize weight vectors of wkj, using any way of initialization.
2. Yet until stop condition(s) won’t be satisﬁed
for each neuron kj do
calculate an average value for all patterns for which this neuron was the winner;
designate average as wkj;
end.
3. To adapt weight value for each neuron, using expression
Wkj ¼
P
n
P
m Nnmhnm;kj Wnm
P
n
P
m Nnmhnm;kj
;
ð2:40Þ
where indexes m, n are summarized according to all numbers of rows and
columns; Nnm is the number of patterns for which the neuron appeared to be the
winner, and hnm;kj—function of the neighborhood which speciﬁes, whether
neuron of (m, n) gets into area of the neighborhood of (k, j) neuron and in what
degree;
end.
The method of search close to optimum structure of SOM consists in beginning
with small structure and to increase the Maps sizes when the increase in number
of neurons is required. We will notice that development of the map takes place
along with learning process. Consider one of algorithms of SOM structure
development for the rectangular Map [11].
2.4
Application of Kohonen Neural Networks
73

Algorithm of SOM Structure Development
To initialize weight vectors for small SOM,
yet until the stop condition(s) won’t be satisﬁed do;
until a condition of the Map growth won’t be true, do;
to learn SOM for t displays of patterns, using any method of SOM learning;
end.
If the condition of the Map growth is satisﬁed,
1. ﬁnd (k, j) neuron with the greatest error of quantization (intercluster
dispersion);
2. ﬁnd the most distant direct neighbor of (m, n) in rows of the Map;
3. ﬁnd the most remote neuron in Map columns;
4. insert a column between neurons of (k, j) and (r, s), and a line between
neurons of (k, j) and (m, n) (this step keeps rectangular structure of the
Map);
5. for each neuron (a, b) in a new column initialize the corresponding vector of
Wab, using expression
Wab ¼ c Wa;b1 þ Wa;b þ 1


;
ð2:41Þ
and for each neuron in a new row calculate
Wab ¼ c Wa1;b þ Wa þ 1;b


ð2:42Þ
where c 2 0; 1
ð
Þ;
end.
6. To adjust the weights of the ﬁnal structure of SOM, using additional
learning iterations, until convergence will be reached.
The increase in the sizes of the map needs to be stopped when one of the
following criteria is satisﬁed:
• the maximum size of the Map is reached;
• the greatest error of quantization for neuron will become less than threshold
e determined by the user;
• the error of the map quantization converged to a preset value.
Some aspects of this algorithm demand the explanation.
There are constants e, c and the maximum SOM size, and also various stop
conditions. For parameter c a good choice is c ¼ 0:5. The idea of an interpo-
lation step consists in assigning a weight vector to new neuron so that it will
take patterns from (k, j) neuron with the greatest error of quantization to reduce
an error of this neuron. Value c \ 0:5 will locate neuron (a, b) closer to (k, j)
that, perhaps, will lead to that more patterns will be taken by it at kj neuron,
value c [ 0:5 will cause a boomerang effect.
74
2
Neural Networks with Feedback and Self-organization

The threshold of an quantization error e is important to provide the sufﬁcient
size of the SOM Map. Small value e leads to too big SOM size whereas too
great value of e can lead to increase in learning time to reach rather big size of
structure.
It is easy to deﬁne the upper bound of the Map size, it is equal simply to the
number of learning patterns pT. However it, naturally, is undesirable. The
maximum size of the Map can be presented as b pT, where b 2 ð0; 1Þ.
Optimum value b depends on the solved task, and it is necessary to take
measures to provide not too small value b if the increase in the SOM sizes isn’t
provided.
Process of learning of SOM is too slow owing to a large number of iterations of
scales correction. For reduction of computing complexity and acceleration of
convergence of learning some mechanisms are offered. One of them is batch
mode of designing of SOM. Two other mechanisms include control of func-
tions of the neighborhood and learning speed.
If Gaussian neighborhood function is used, all neurons drop to the winner
neuron neighborhood area, but with different degree. Therefore introducing a
certain threshold h, it is possible to limit number of neurons which will get to
this area and by that to reduce computational costs of correction of their
weights. Besides, the width of the neighborhood function r can be changed
dynamically during learning process. For example, it is possible to choose this
function so:
rðtÞ ¼ rð0Þe t
s1;
ð2:43Þ
where s1 [ 0 , is some constant; rð0Þ is the initial rather big variation.
Similarly it is possible to use the learning speed aðtÞ decreasing with increase in
time:
aðtÞ ¼ að0Þe t
s2;
ð2:44Þ
where s2 [ 0 , is some constant; að0Þ is initial, rather big variation.
Clustering and Visualization. Application of SOM
Implementation of SOM learning process consists in a clustering (grouping) of
similar patterns, keeping thus topology of input space. After learning the set of
the learned weights is obtained without obvious borders between clusters.
The additional step for ﬁnding of borders between clusters is required.
One of the ways to deﬁne and visualize these borders between clusters is to
calculate the uniﬁed matrix of distances (U-matrix [11]) which includes geo-
metrical approximation of distribution of weight vectors on the Map. The
U-matrix expresses for each neuron distance to weight vectors of the next
neurons. Great values in a matrix of distances of U indicate location of borders
between clusters.
2.4
Application of Kohonen Neural Networks
75

As an example consider a problem of a clustering of irises. Its statement and the
description are given in Chap. 4. For example, in Fig. 2.16, the U-matrix for a
problem of irises clustering with use of scaling of the SOM Map in shades of
the grey is presented. Thus borders between clusters are marked in more dark
color.
For the same problem in Fig. 2.16b clusters on a full map are visualized.
Borders between them can usually be found by one of clustering methods, for
example, Ward’s method (see Chap. 7). The clustering by Ward’s method uses
approach “from down-up” in which each neuron originally forms an own
cluster. On the subsequent iterations two next clusters merge in one until the
optimum or certain number of clusters is designed. The end result of a clus-
tering is the set of clusters with the minimum intra cluster dispersion and big
inter-cluster dispersion.
For deﬁnition, what clusters need to be united, the metrics (distances) according
to Ward is used (see Chap. 7). The metrics of distance is deﬁned so:
Fig. 2.16 Example of SOM for irises clustering problem
76
2
Neural Networks with Feedback and Self-organization

drs ¼
nrns
nr þ ns
Wr  Ws
k
k2;
ð2:45Þ
where r and s are indexes of clusters; nr and ns are number of patterns in the
corresponding clusters; Ws and Wr are vectors of the gravity centers of these
clusters. Two clusters s and r unite (merge) if their metrics of drs is the smallest.
For the new created cluster q its weight is deﬁned thus:
Wq ¼
1
nr þ ns
nrWr þ nsWs
ð
Þ;
ð2:45Þ
and nq ¼ nr þ ns.
It’s worth to note that for preservation of topological structure two clusters can
be integrated only if they are adjacent (neighbors).
The main advantage of SOM consists in easy visualization and interpretation of
the clusters created on the Map. In addition to visualization of the full map
presented in Fig. 2.17b, separate components of vectors can be visualized, i.e.
for each input feature the separate Map for visualization (display) of values
distribution of this feature in space can be constructed, using the corresponding
color gamut. Such Map and the planes of features can be used for research and
the analysis of data. For example, the marked region on the visualized map can
be designed on the feature plane to ﬁnd distribution of values of the corre-
sponding input parameters (features) for this region. In Fig. 2.16c, d SOM’s for
the third and fourth features for a problem of the irises clustering are presented.
The learned SOM can be also used as the classiﬁer [11] as information on
clusters is inaccessible during learning process, (SOM) it is necessary to
investigate the clusters created on the Map manually and to assign appropriate
tags of classes. Further the vector of input data is entered into the map and the
winner neuron is deﬁned. The corresponding tag of a cluster to which the
entered input vector belongs, is used as a name (a number) of a class.
In the mode of a recall SOM can be used for interpolation of the missed values
in a pattern. When entering the input of such pattern, ignoring inputs with the
Fig. 2.17 Area of violation
of a continuity of mapping
using SOMs
2.4
Application of Kohonen Neural Networks
77

missed values the winner neuron is deﬁned. Further the missed value is
determined by the corresponding feature value of a winner neuron, or by
interpolation of values of the neighbor neurons.
The described topographic maps give an evident presentation of structure of
data in multidimensional input space, which geometry we aren’t able to
imagine otherwise [10]. Visualization of multidimensional information is the
main use of Kohonen’s maps.
Note that in consent with the general everyday principle “free lunches don’t
happen” topographic maps keep the proximity relation only locally, i.e.
neighbors on the map of area are close and in the initial space, but not on the
contrary (Fig. 2.17). Generally there is no mapping cutting the dimension and
keeping the relation of proximity globally.
In Fig. 2.17 the arrow shows the area of violation of a continuity of mapping,
neighbors points on the plane are displayed on the opposite ends of the map.
The convenient instrument of visualization is the coloring of topographic maps
how it used on usual maps. Each feature generates the corresponding coloring
of the map by average value of this feature at the data which got to this cell
[10].
Having collected maps of all of the interesting features, we‘ll receive the
topographical atlas giving an integrated presentation of the structure of multi-
dimensional data. Self-learning Kohonen’s networks are widely used for data
preprocessing at pattern recognition in space of very big dimension. In this
case, that procedure to be effective, it is required to compress at ﬁrst input
information by one or another way:
1. or to lower dimension, having deﬁned signiﬁcant features;
2. or to make quantization of data.
SOM are applied to the solution of a wide range of real problems, including the
analysis of images, recognition of the speech, the analysis of musical patterns,
processing of signals, robotics, telecommunications, data mining of the hidden
knowledge and the analysis of time series [9].
References
1. Hopﬁeld, J.J.: Neural Networks and physical systems with emergent collective computational
abilities. Proc. Natl. Acad. Sci. USA, 79, 2554—2558 (1982)
2. Zaychenko, Y.P.: Fundamentals of intellectual systems design. Kiev. Publishing House,
“Slovo”, pp. 352 (2004) (rus)
3. Chung, F.L., Lee, T.: Fuzzy competitive learning. Neural Netw. 7, 539–552 (1994)
4. Deb, K., Joshi, D., Anand. A.: Real-coded evolutionary algorithms with parent-centric
recombination. In: Proceedings of the IEEE Congress on Evolutionary Computation, pp. 61–
66, (2002)
5. Hopﬁeld, J.J.: Neurons, dynamics and computation. Phys. Today, 47, 40–46 (1994)
78
2
Neural Networks with Feedback and Self-organization

6. Heykin, S.: Neural networks. Full course. (2nd edn). Transl. engl. Moscow.-Publishing House
“Williams”. pp. 1104 (2006). (rus)
7. Hebb, D.O.: The Organization of Behavior: A Neuropsychological Theory. Wiley, New York
(1949)
8. Kohonen, T.: Self-Organization and Associative Memory. (3rd edn). Springer, New York
(1988)
9. Osovsky, S.: Neural networks for information processing, transl. from pol.—M.: Publishing
house Finance and Statistics. pp. 344 (2002). (rus)
10. Kohonen, T.: Self-organized formation of topologically correct feature maps. Biol. Cybern.
43, 59–69 (1982)
11. Engelbrecht, A.: Computational Intelligence. An Introduction (2nd edn). John Wiley & Sons,
Ltd., pp. 630 (2007)
References
79

Chapter 3
Fuzzy Inference Systems and Fuzzy Neural
Networks
3.1
Introduction
In recent years, the attention of many researchers in the ﬁeld of artiﬁcial intelligence
systems attracts the problem of decision making under uncertainty, the incom-
pleteness of the initial data and quality criteria.
There is a new trend in the theory of complex decision-making, which is rapidly
developing—making decisions under uncertainty. A promising approach for solv-
ing many decision-making problems under uncertainty and incomplete information
is based on fuzzy sets and systems theory created by Zadeh [1].
The introduction by Zadeh of the concept of linguistic variables described by fuzzy
sets [2] gave rise to a new class of systems—fuzzy logic systems (FLS), which allows
to formalize fuzzy expert knowledge. The use of fuzzy inference systems (FIS) and
built on the their basis fuzzy neural networks (FNN) has allowed to solve many
problems of decision-making under uncertainty, incompleteness and qualitative
information—forecasting, classiﬁcation, cluster analysis, pattern recognition.
Chapter 3 is devoted to the detailed consideration of FL systems. It discusses the
basic algorithms of fuzzy inference Mamdani, Tsukamoto, Larsen and Sugeno
(Sect. 3.2). In Sect. 3.3 the methods of defuzziﬁcation are described.
In the Sect. 3.4 the important Fuzzy approximation theorem (FAT-theorem) is
considered which is theoretical ground for wide applications of FNN.
Further fuzzy controller (FC) Mamdani and Tsukamoto and classical learning
algorithm on the basis of back-propagation are detailly considered. A new learning
algorithm of FC Mamdani and Tsukamoto for Gaussian membership functions
(MF) of gradient type is described (Sect. 3.6).
Next FNN ANFIS is considered, its architecture and gradient learning algorithm
are presented (Sect. 3.7). Then FNN TSK, the development of FNN ANFIS, is
described and its hybrid training algorithm is reviewed (Sect. 3.8). In the Sect. 3.9
adaptive wavelet-neuro-fuzzy networks are considered and different learning
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_3
81

algorithms in batch and on-line mode are presented. Cascade neo-fuzzy neural
networks (CNFNN) are considered, training algorithms are presented and GMDH
method for its structure synthesis is described and analyzed (Sect. 3.10).
3.2
Algorithms of Fuzzy Inference
Used in different expert and control systems fuzzy inference mechanism is
grounded on a knowledge base, formed by experts in the subject area as a set of
fuzzy rules of the predicate form [3–5]:
P1 : if x is A1; then y is B1
P2 : if x is A2; then y is B2
. . .
Pn : if x is An; then y is Bn
where x, x 2 X—input variable (the name for the known data values);
y; y 2 Y—output variable (name for a data value to be calculated);
Ai and Bi—membership functions deﬁned on the sets X and Y.
Let’s provide a more detailed explanation. Expert knowledge A ! B reﬂects
fuzzy causal relationship between premises and conclusions, so it can be called a
fuzzy relation and denoted as R:
R : A ! B;
where “!” is called fuzzy implication.
Relation R can be regarded as a fuzzy subset of the direct product X  Y
complete set of premises X and conclusions Y. Thus, the process of obtaining a
(fuzzy) output results B0 from the use of this observation A0 and knowledge A ! B
can be represented as the composite fuzzy rule “modus ponens”:
B0 ¼ A0  R ¼ A0  ðA ! BÞ;
where “”—convolution operation.
Both the operation of the composition and the implication operation in the
algebra of fuzzy sets may be implemented in different ways (in this case will be
different and the resultant outcome), but in any case, the total fuzzy inference is
carried out in the following four steps [4].
1. Introduction of fuzziness
(fuzziﬁcation). Membership functions deﬁned on
the input variables are applied to their actual values to determine the degree of
truth of each premise(antecedent) of each rule.
82
3
Fuzzy Inference Systems and Fuzzy Neural Networks

2. The logical inference. The calculated value of the truth of the antecedent for
each rule applies to the conclusion (consequent) of each rule. This results in one
fuzzy subset that will be assigned to each variable output for each rule. The rules
of inference typically use only operations min (minimum) or prod (multiplica-
tion). The inference MINIMUM output membership function “ is cut off” at a
height corresponding to the calculated degree of truth of the rule antecedents
(fuzzy logic “AND”). At the inference by product output membership function
is scaled by calculating the degree of truth of the rule antecedents.
3. Composition. All the fuzzy subsets assigned to each output variable (all rules)
are combined together to form a single fuzzy subset for all output variables.
With such a merge operation normally is used max (maximum) or sum (SUM).
When the composition MAXIMUM is used the combined output of fuzzy rules
is constructed as a point-wise maximum over all fuzzy sets (fuzzy logic “OR”).
When the composition SUM is used combined output of fuzzy rules is formed as
the point-wise sum of all fuzzy subsets assigned to the output variable by
inference rules.
4. Transformation to non-fuzziness (defuzziﬁcation). It’s used when you need
to convert the fuzzy set of conclusions in a crisp number. There is a considerable
number of methods of transferring to non-fuzziness, some of which are dis-
cussed below.
Example 3.1 Suppose that a system is described by the following fuzzy rules:
P1 : if x is A ; then w is D
P2 : if y is B ; then w is E
P3 : if z is C ; then w is F
where x; y and z—the names of the input variables; w—the name of the output
variable, A; B; C; D; E; F—set of membership function (e.g. of triangular shape).
The procedure for obtaining inference is illustrated by Fig. 3.1. It is assumed that
the speciﬁc (clear) values of the input variables are: x0; y0; z0.
At the ﬁrst step, based on these values and on the basis of membership functions
A; B; C, degrees of truth a x0
ð
Þ; a y0
ð
Þ и a z0
ð Þ of prerequisites (antecedents) for each
of the three aforementioned rules are determined. At the second stage, a “cut-off”
membership functions of rules consequence
D; E; F
ð
Þ on levels a x0
ð
Þ; a y0
ð
Þ and
a z0
ð Þ are determined. At the third stage, the output membership functions, truncated
at the previous stage are considered and their composition (uniﬁcation) with
operations max is produced, resulting in a combined fuzzy subset described by the
membership function lRðwÞ corresponding to the output variable w. Finally, in the
fourth step is, if necessary, the crisp value of the output variable, for example, using
3.2
Algorithms of Fuzzy …
83

centroid method: is deﬁned as the center of gravity for the curve lRðwÞ:
w0 ¼
R
X wlRðwÞdw
R
X lRðwÞdw :
Consider the following modiﬁcation of the most commonly used fuzzy inference
algorithm assuming for simplicity that the knowledge base contains two fuzzy rules
of the form:
P1 : if x is A1 and y is B1 ; then z is C1
P2 : if x is A2 and y is B2; then z is C2
where x and y—the names of the input variables, z—the name of the output variable,
A1; B1; C1; A2; B2; C2—some membership functions. At the same time a crisp value z0
must be determined on the basis of the above information and crisp values x0 and y0.
Fig. 3.1 Illustration of logical inference
84
3
Fuzzy Inference Systems and Fuzzy Neural Networks

3.2.1
Mamdani Fuzzy Inference
This algorithm corresponds to the example considered in the Fig. 3.1. In this sit-
uation, it can be formally described as follows [3–5]:
1. Introduction of fuzziness. the truth degrees for premises (antecedents) of each
rule are determined: A1 x0
ð
Þ; A2 x0
ð
Þ; B1 x0
ð
Þ; B2 x0
ð
Þ.
2. The logical inference. The cut-off levels for the premises of each rule are
determined (using operation minimum):
a1 ¼ A1ðx0Þ ^ B1ðy0Þ;
a2 ¼ A2ðx0Þ ^ B2ðy0Þ;
where by “^” denotes the operation of the logical intersection (min).
Then the membership functions (MF) of consequent of each rule are calculated
(operation of fuzzy implication):
C
0
1 ¼ ða1 ^ C1ðzÞÞ;
C
0
2 ¼ ða2 ^ C2ðzÞÞ:
3. Composition. Uniﬁcation of found truncated membership functions for conse-
quences using MAX operation is performed (this operation is designated
hereinafter as “_”), resulting in a fuzzy subset of the ﬁnal output with mem-
bership function:
lRðzÞ ¼ C0
1ðzÞ _ C0
2ðzÞ ¼ ða1 ^ C1ðzÞÞ _ ða2 ^ C2ðzÞÞ
ð3:1Þ
4. Defuzziﬁcation It’s performed to ﬁnd crisp z0, using for example, centroid method.
3.2.2
Tsukamoto Fuzzy Inference Algorithm
Assumptions are the same as in the previous algorithm, but here it is assumed that
the function C1ðzÞ, C2ðzÞ are monotonous (see. Fig. 3.2) [3–5]:
1. Introduction of fuzziness (like Mamdani algorithm).
2. Fuzzy inference. First, “cut-off” levels” a1 and a2 (as in Mamdani algorithm) are
determined and then the following equations are to be solved:
a1 ¼ C1ðz1Þ
and
a2 ¼ C2ðz2Þ
3.2
Algorithms of Fuzzy …
85

as a result clearly deﬁned (non-fuzzy) values z1 and z2 for each of the original
rules are determined.
3. Composition. Determine the clear value of the total output variable (as the
weighted average of z1 and z2):
z0 ¼ a1z1 þ a2z2
a1 þ a2
:
ð3:2Þ
In general, the discrete version of centroid method takes the form:
z0 ¼
Pn
i¼1 aizi
Pn
i¼1 ai
:
ð3:3Þ
3.2.3
Sugeno Fuzzy Inference
Takagi and Sugeno have used a set of rules in the following form (as before, we
give an example of two rules):
P1 : if x is A1 and y is B1 ; then z1 ¼ a1x þ b1y;
P2 : if x is A2 and y is B2; then z2 ¼ a2x þ b2y;
where ai, bi, are some constants.
Description of the algorithm (Fig. 3.3).
Fig. 3.2 Illustration of Tsukamoto algorithm
86
3
Fuzzy Inference Systems and Fuzzy Neural Networks

1. Introduction of fuzziness (like Mamdani algorithm).
2. Fuzzy inference.
The values a1 ¼ A1ðx0Þ ^ B1ðy0Þ, a2 ¼ A2ðx0Þ ^ B2ðy0Þ are determined and
individual outputs of the rules are calculated:
_z1 ¼ a1x0 þ b1y0
_z2 ¼ a2x0 þ b2y0
3. Composition of the output variables and total output determination
z0 ¼ a1_z1 þ a2_z2
a1 þ a2
ð3:4Þ
3.2.4
Larsen Fuzzy Inference
The Larsen fuzzy inference algorithm is modeled using the multiplication operator.
Description of the algorithm (Fig. 3.4).
1. Fuzziﬁcation (as Mamdani algorithm)
2. Fuzzy inference. First, as in the algorithm of Mamdani, values:
a1 ¼ A1ðx0Þ ^ B1ðy0Þ ¼ A1ðx0Þ  B1ðy0Þ
a2 ¼ A2ðx0Þ ^ B2ðy0Þ ¼ A2ðx0Þ  B2ðy0Þ:
Fig. 3.3 Illustration of the algorithm Sugeno
3.2
Algorithms of Fuzzy …
87

are determined. In this algorithm operation ^ is realized as product * and then
output fuzzy subsets for each rule are calculated using fuzzy implication as
product operation (Larsen form):
a1  C1ðzÞ; a2  C2ðzÞ:
3. Composition. The resulting output fuzzy subset is determined (like Mamdani):
lRðzÞ ¼ CðzÞ ¼ ða1C1ðzÞÞ _ ða2C2ðzÞÞ:
ð3:5Þ
(in general for n rules: lRðzÞ ¼ CðzÞ ¼ _
n
i¼1ðaiCiðzÞÞ
4. If necessary, defuzziﬁcation (as in the previously discussed Mamdani algorithm)
is performed.
3.3
Methods of Defuzziﬁcation
1. One of these methods—centroid has already been considered. Here the corre-
sponding formula is presented again in general case, [2, 3]
z0 ¼
R
X z  C zð Þdz
R
X C zð Þdz ;
ð3:6Þ
Fig. 3.4 Illustration of the algorithm Larsen
88
3
Fuzzy Inference Systems and Fuzzy Neural Networks

discrete version
z0 ¼
Pn
i¼1 aizi
Pn
i¼1 ai
2. The ﬁrst maximum (First-of-Maxima-FOM). The crisp output value is the
smallest value at which the maximum of the ﬁnal fuzzy MF is attained
(Fig. 3.5a): z0 ¼ min z CðzÞ ¼ max
U

CðUÞ


.
3. The average maximum (Middle-of-Maxima MOM). Crisp value is given by:
z0 ¼
R
G zdz
R
G dz ;
ð3:7Þ
where G—subset of elements maximizing C (Fig. 2.5b).
For discrete version (set G is discrete)
z0 ¼ 1
n
X
n
i¼1
zi:
ð3:8Þ
4. The criterion of maximum (Max-Criterion). Crisp value is selected randomly
among the many elements, which maximize C
z0 2
z : CðzÞ ¼ max
u
CðUÞ
n
o
:
ð3:9Þ
Fig. 3.5 Illustration of the method of defuzziﬁcation a the ﬁrst maximum; b the average
maximum
3.3
Methods of Defuzziﬁcation
89

5. (Height defuzziﬁcation).
Elements of the domain X, for which membership
function values are less than a certain level a, are not taken into account, and a
crisp value is calculated accordingly:
z0 ¼
R
Ca z  CðzÞdz
R
Ca CðzÞdz ;
ð3:10Þ
3.4
Fuzzy Approximation Theorems
The possibility of wide applications of fuzzy logic is based on the following
results [6]:
1. In 1992, Wang (Wang) showed that a fuzzy system is an universal approxi-
mator, i.e. it can approximate any continuous function on a compact U with
arbitrary precision, when using a set of n n ! 1
ð
Þ rules [1]:
P1: if x is Ai and y is Bi, then z is Ci; i ¼ 1 . . . n; under the following conditions:
• Gaussian membership functions:
AiðxÞ ¼ exp  1
2
x  ai1
bi1

2
"
#
BiðyÞ ¼ exp  1
2
y  ai2
bi2

2
"
#
ð3:11Þ
CiðzÞ ¼ exp  1
2
z  ai3
bi3

2
"
#
;
• intersection as a product: Ai x
ð Þ and Bi y
ð Þ
½
 ¼ Ai x
ð ÞBi y
ð Þ;
• implications in the form of Larsen:
Ai x
ð Þ and Bi y
ð Þ
½
 ! Ci zð Þ ¼ Ai x
ð ÞBi y
ð ÞCi zð Þ;
• centroid method of defuzziﬁcation.
90
3
Fuzzy Inference Systems and Fuzzy Neural Networks

z0 ¼
Pn
i¼1 ai3AiBi
Pn
i¼1 AiBi
;
ð3:12Þ
where ai3—centers Ci.
In other words, Wang proved the theorem: for every real continuous function g,
given on a compact U for an arbitrary e [ 0 there exists a fuzzy system, which
generates the function f x
ð Þ such that
sup
x2U
gðxÞ  f ðxÞ
k
k  e;
where
k k—a symbol of adopted distance between the functions.
2. In 1995, Castro (Castro) showed that the Mamdani fuzzy logic controller is also
universal approximator under the following conditions [7]:
• Number of rules n ! 1,
• symmetrical triangular membership functions:
AiðxÞ ¼
1  ai  x
j
j=ai;
if ai  x
j
j  ai;
0;
if ai  x
j
j [ ai;

;
ð3:13Þ
BiðyÞ ¼
1  bi  y
j
j=bi;
if bi  y
j
j  bi;
0;
if bi  y
j
j [ bi;

:
ð3:14Þ
• intersection using the operation minimum:
Ai x
ð Þ and Bi y
ð Þ
½
 ¼ min Ai x
ð Þ; Bi y
ð Þ
f
g;
• implication in the form of Mamdani and centroid method of defuzziﬁcation:
z0 ¼
Pn
i¼1 ciminfAiðxÞ; BiðyÞg
Pn
i¼1 minfAiðxÞ; BiðyÞg ;
ð3:15Þ
where ci—centers Ci.
3.4
Fuzzy Approximation Theorems
91

In general, fuzzy logic systems it’s reasonable to apply in the following cases
[3, 4]:
• for complex processes where there is no simple mathematical model;
• if the expert knowledge about the object or the process can be formulated only
in linguistic form.
Systems that are based on fuzzy logic, are applied inappropriately:
• If a desired result can be obtained by some other (standard) method;
• When for an object or process an adequate and easily researched mathematical
model has already been or may be found.
As a conclusion note the main advantages of fuzzy logic systems:
1. they enable to work with fuzzy, incomplete and qualitative information; under
uncertainty conditions;
2. they enable to use expert information in the form of fuzzy inference rules. Apart
from great advantages fuzzy system have several drawbacks, namely:
Base of the rules formulated by an expert, may be incomplete or contradictory;
membership functions of linguistic variables may be inadequate to real simulated
processes.
To eliminate these shortcomings it’s needed to use the training of fuzzy logic
systems, i.e., make them adaptive.
3.5
Fuzzy Controller Based on Neural Networks
One of the ﬁrst practical applications of fuzzy systems was the sphere of man-
agement, in which are widely used so-called fuzzy controllers (FC) [3, 8].
For the design of a fuzzy controller the linguistic rules and MF (membership
function) are to be given that to represent linguistic variables. Speciﬁcation of good
linguistic rules depends on expert knowledge of management system. But the
translation of this knowledge into fuzzy sets—this task is not formalized and need
to make a choice on the basis of, for example, forms of MF. The quality of the
fuzzy controller (FC) is achieved by changing the shape of MF.
Artiﬁcial neural network (ANN) is a highly parallel architecture and consist of
similar elements interacting via connections which are deﬁned by weights.
Using ANN, we can not only approximate the function, but also study (investigate)
control objects, applying learning and self-learning procedures. The problem lies
herein training is wasting a lot of time but the result is not always guaranteed. But it
is possible to implement the previously acquired knowledge in the form of rules in
already trained NN to simplify learning.
Connecting NN and fuzzy logic systems (FL) allows you to combine all their
advantages and avoid their weaknesses. To integrate these two technologies
(FL) and ANN the structure of FL system should be presented as a corresponding
92
3
Fuzzy Inference Systems and Fuzzy Neural Networks

NN. This approach uses the NN for optimization of parameters of conventional
neural controller (NC) or for retrieving rules from the data. Choice of MF, which
represents linguistic term, is more or less arbitrary. For example, consider the
linguistic term “approximately zero”. It is obvious that the corresponding fuzzy set
MF is to be unimodal and reach its maximum at the point zero. The correct choice
of membership functions has become the main and most important task of the NC.
NN offers the opportunity to solve this problem. The method offeed propagation (of
signal) in a neural network allows to choose MF form, which depends on several
parameters and can be adjusted in the learning process. As MF one can choose a
symmetrical triangular shape, which depends on two parameters, one of which is the
valueatwhichAFreachesamaximumvalue,andthesecond—thelengthoftheinterval.
Training data should be divided into r cluster R1; . . .; Rr. Each cluster Ri corre-
sponds to some decision rule Ri. Cluster elements are presented as values in the form
of ðX; YÞ, where X ¼ ½x1; . . .; xn—vector of input variables, Y—the output variable.
Consider the dynamic system S, in which the control is carried out by means of a
variable C, and its state can be described by n variables x1; . . .; xn.
Linguistic variables are modeled and manipulated by membership functions, and
control action which leads to the desired system state is described by the fuzzy rule
“if-then”. To get the desired value (i.e. control value), it is necessary to solve the
problem of defuzziﬁcation, for which we use the monotonic membership function
of Tsukamoto, (see Fig. 3.6), where defuzziﬁcation reduced to the application of
the inverse function.
Fig. 3.6 Defuzziﬁcation using monotonous membership function of Tsukamoto
3.5
Fuzzy Controller Based on Neural Networks
93

This membership function l is characterized by two parameters a and b of the
MF, and it is deﬁned as
lðxÞ ¼
ðx þ aÞ=ða  bÞ;
if x 2 ½a; b _ x 2 ½b; a ^ ða [ bÞ
ð
Þ
0;
otherwise

:
ð3:16Þ
Defuzziﬁcation is carried out in such a way:
x ¼ l1ðyÞ ¼ yða  bÞ þ a ¼ a þ yðb  aÞ; y 2 ½0; 1
For our purposes, we must limit the monotonous MF to represent the linguistic
value of the original variable. For the input value is usually used triangular or
trapezoidal MF.
In Fig. 3.7 the structure of fuzzy neural controller is presented [4].
Modules x1 and x2 here are the input variables, and they send their values to their
l-modules that contain the appropriate MF. l-modules are linked to R-modules,
which represent fuzzy rules “if-then”. Every l-module transmits to all R-modules
MF values lijðxiÞ of its input variable xi. each R-module performs the intersection
operation, ﬁnds min
i flijðxiÞg and transmits this value further to m-modules which
comprise MF describing output variables. m-modules using monotonous member-
ship functions, calculate the value ri and m1ðriÞ and transmit them to the C-
modules which calculate the ﬁnal output variable—control action C according to
formula (3.17), that is, uses the centroid defuzziﬁcation algorithm
C ¼
Pn
i¼1 rim1ðriÞ
Pn
i¼1 ri
;
ð3:17Þ
where n—the number of inference rules; ri—is a degree to which the rule Ri is
executed.
Fig. 3.7 The structure of a
fuzzy neural controller
94
3
Fuzzy Inference Systems and Fuzzy Neural Networks

It is easy to see the system in the Fig. 3.7 resembles a sequential multilayer
neural network, where x-, R-и C-modules play the role of neurons, l- and m-
modules play the role of adaptive weights of network connections. The learning
process is determined by the fuzzy error and works in parallel for each fuzzy rule.
After C is generated by the controller and the new state of the object becomes
known, the error E is calculated, which propagates in the opposite direction. Each
rule analyzes its own contribution to the control output and assesses its conclusion
(i.e., it has resulted in an increase or decrease of error). Thus, if the rule acted in the
right direction, it should be made more sensitive and to work out such a conclusion
which increases control. Conversely, if the rule acted in the wrong direction, the
rule should be made less sensitive. taking into account that the MF are described by
two parameters bi; ai
ð
Þ, one of them is ﬁxed, and the second has to change. In this
case, it’s assumed that ai changes.
The learning algorithm. In [8] the following training algorithm FNN was
proposed. It is used to adjust membership functions of rules antecedents and
consequences.
1. Each rule Ri computes the rule error
Ri :
eRi ¼
riE;
signCi ¼ signCopt
riE;
signCi 6¼ signCopt

;
ð3:18Þ
where ri—the level of activation of rule Ri;
Ci is output of vi-module; Copt—optimal control (FNN desired output)
2. A value eRi is transmitted to m- and l-module;
First, a change of MF of m-module is produced;
anew
k
¼
ak  reRi ak  bk
j
j ; ak\bk
ak þ reRi ak  bk
j
j;
ak [ bk

;
ð3:19Þ
where r is a training speed.
If the module mk is used by several R-modules, then their MF should change as
many times as a number of R-modules connected with it.
3. Adjustment of antecedents (parameters of MF):
anew
jk
¼
ajk  reRi ajk  bjk

 ; ajk\bjk
ajk þ reRi ajk  bjk

;
ajk [ bjk

:
ð3:20Þ
Note: the module xj is connected with the module Rk through module ljk. This
algorithm is called the algorithm with reinforcement.
If the learning process has been successful, it means the rule base was built
correctly. It is possible to arrange the learning process so that not only to adjust the
3.5
Fuzzy Controller Based on Neural Networks
95

rules MF, but correct the rules themselves. If the rule output corresponds to the
semantics of the desired control, then the rule is stored in the rule base. If a rule acts
the opposite to the desired output, it changes to the opposite consequence or
generally is completely removed from rule base.
3.6
The Gradient Learning Algorithm of FNN
Mamdani and Tsukamoto
Considered above in Sect. 3.5 Mamdani learning algorithm is empirical, formulas
(3.18)–(3.20) for adjusting membership functions aren’t theoretically grounded.
This comes from the fact that Mamdani and Tsukamoto fuzzy controller uses
triangular MF, and the intersection of the rules antecedents is performed in the form
of min. In the result the corresponding MF are non-differentiable.
In this regard, it is advisable to construct the analytical learning algorithm, which
convergence is strictly proved. For this purpose, it is reasonable to use the Gaussian
MF for the antecedents and consequences [3].
So let MF of the ith l-module associated with the rule Rk be described as
follows:
likðxiÞ ¼ exp
 1
2  ðxi  aikÞ2
r2
ik
(
)
;
ð3:21Þ
where aik, rik are parameters to be set up in the learning process and MF of mk—
module is following,
lkðyiÞ ¼ exp
 1
2  ðyi  akÞ2
r2
k
(
)
;
while the antecedents intersection in rules is speciﬁed as a product;
ak ¼
Y
n
i¼1
likðxiÞ ¼ exp

X
n
i1
1
2  ðxi  aikÞ2
r2
ik
(
)
:
Assume that the centroid defuzziﬁcation method is used, whereas total output is
determined as:
z0 ¼
P
k
zkak
P
k
ak
Let the rules consequences use monotonous MF C(z). Then zk is determined by
solving the following equation (controller Tsukamoto).
96
3
Fuzzy Inference Systems and Fuzzy Neural Networks

CkðzkÞ ¼ ak;
ð3:22Þ
where
CkðzkÞ ¼ exp
 1
2  ðzk  akÞ2
r2
k
(
)
;
ð3:23Þ
Then, solving the Eq. (3.22) exp  1
2  ðzkakÞ2
r2
k
n
o
¼ ak, we ﬁnd two roots:
zk ¼ ak 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 1
a
r
 rk:
The ﬁrst root is z1k ¼ ak 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 1
a  rk
q
. It is located on the site of a mono-
tonically increasing curve zrðakÞ and second z2k is located on site of monotonically
decreasing curve.
Let criterion be EðzÞ ¼ 1
2 ðz0  zÞ2 ! min;
where z* is desired output; z0 is FNN output,
then we ﬁnd the derivative [4]
@E
@ak
¼ @E
@z0
@z0
@zk
@zk
@ak
¼ þ z0  z
ð
Þ
ak
P
K
k¼1
ak
;
ð3:24Þ
@E
@rk
¼ @E0
@z0
@z0
@zk
@zk
@rk
¼  z0  z
ð
Þ
ak
P
K
k¼1
ak

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 1
a
r
;
ð3:25Þ
on the increasing monotonically part of the curve OP lk;
@E
@rk
¼ þ z0  z
ð
Þ
ak
PK
k¼1 ak
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 1
a
r
;
ð3:26Þ
on a monotonically decreasing part of the curve.
For input µ-modules
@E
@aik
¼ @E0
@z0
@z0
@ak
@ak
@aik
¼ þ ðz0  zÞ zk
PK
k ak  PK
k zk  ak
P
k ak

	2

Y
K
k1
exp
 xi  aik
ð
Þ2
2  r2
ik
(
)
 xi  aik
ð
Þ
r2
ik
¼
¼ z0  z
ð
Þ  ak  zk
PK
k ak  PK
k zk  ak
P
k ak

	2
 xi  aik
ð
Þ
r2
ik
3.6
The Gradient Learning …
97

@E
@rik
¼ @E
@z0
@z0
@ak
@ak
@rik
¼
¼
ðz0  zÞak
zk  PK
k¼‘ ak  P
k zkak
P
k ak

	2
 xi  aik
ð
Þ2
r3
ik
 
!
:
ð3:27Þ
then gradient learning algorithm for FNN Mamdani-Tsukamoto is as follows [4]:
1. for the output modules
akðn þ 1Þ ¼ akðnÞ  cn
@E
@ak
¼ akðnÞ  cnðz0  zÞ
ak
P
k ak
;
ð3:28Þ
riðn þ 1Þ ¼rkðnÞ  c0
n
@E
@rk
¼
¼rkðnÞ  cnðz0  z
RÞ
ak
P
k ak

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 1
ak
r
:
ð3:29Þ
for input l-modules
aikðn þ 1Þ ¼ aikðnÞ  cn2
@E
@aik
¼ aikðnÞ  cnðz0  zÞ
 ak zk  P
k ak  P
k zk  ak

	
P
k ak

	2
 ðxi  aikÞ
r2
ik
:
ð3:30Þ
rikðn þ 1Þ ¼ rikðnÞ  cn3ðz0  z
RÞ ak zk  P
k ak  P
k zk  ak

	
P
k ak

	2
 ðxi  aikÞ2
r3
ik
:
ð3:31Þ
here cn; cn1; cn2; cn3—step sizes.
For the convergence of the method the following conditions are to be true [3, 4]:
(a) cn ! 0; n ! 1, (б) P
1
n¼0
cn ¼ 1, P
1
n¼0
c2
n\1.
98
3
Fuzzy Inference Systems and Fuzzy Neural Networks

3.7
Fuzzy Neural Network ANFIS. The Structure
and Learning Algorithm
Consider an adaptive fuzzy system with the inference mechanism proposed by
Sugeno on the basis of IF-THEN rules [3, 4, 9], which is called the network ANFIS
(Adaptive Network Based Fuzzy Inference System). This system can be success-
fully used for adjusting membership functions and the rule base of fuzzy expert
system. Below Sugeno fuzzy model and a block diagram of a network ANFIS (for
two inputs and two rules) are presented (Fig. 3.8.).
f1 ¼ a1x þ b1y þ r1
f2 ¼ a2x þ b2y þ r2
f ¼ w1f1 þ w2f2
w1 þ w2
¼ w1f1 þ w2f2
Fig. 3.8 a Schematic of Sugeno inference, b an equivalent structure of the neural network ANFIS
3.7
Fuzzy Neural Network ANFIS …
99

ANFIS system uses the following rule base:
if x ¼ A1 and y ¼ B1 then f1 ¼ a1x þ b1y þ r1
if x ¼ A2 and y ¼ B2 then f2 ¼ a2x þ b2y þ r2

where Ai and Bi are linguistic variables.
ai; bi; ri—some constants.
The layers of the fuzzy neural network perform the following functions:
Layer 1. Each neuron of this layer converts the input signal x or y using MF
(fuzziﬁcator). The most frequently used membership function is bell-shaped
lAiðxÞ ¼
1
1 þ
xci
ai

2

 ;
ð3:32Þ
or Gaussian function
lAiðxÞ ¼ exp  x  ci
ai

2
"
#
:
ð3:33Þ
Layer 2. Each neuron in this layer, marked as П, performs the intersection of
input signals, simulating a logical AND operation, and sends to the output a value:
wi ¼ lAiðxÞ  lBiðyÞ; i ¼ 1; 2:
ð3:34Þ
Essentially, each neuron determines an activating rule force wi. In fact, any
operator of T-norm, which summarizes the AND operation can be used in these
neurons.
Layer 3. Each neuron in this layer calculates normalized rules weight:
wi ¼
wi
w1 þ w2
; i ¼ 1; 2:
ð3:35Þ
Layer 4. In this layer neurons generate values of the output variables:
O4
i ¼ wifi ¼ wi aix þ biy þ ri
ð
Þ:
ð3:36Þ
Layer 5. In the last layer the output of the neural network is obtained and
defuzziﬁcation is carried out:
O5 ¼ overall output ¼
X
i
wifi ¼
P
i wifi
P
i wi
:
ð3:37Þ
The neural network ANFIS is trained by the method of steepest descent.
100
3
Fuzzy Inference Systems and Fuzzy Neural Networks

Rule Base Construction and Setting the Parameters of the Membership
Function.
In existing systems with fuzzy neural networks one of the most important problems
is the development of an optimum method of building the fuzzy rule base, based on
the training sample. Basically fuzzy rules are described by experts according to
their knowledge and experience of the relevant processes. But in the case of the
development of fuzzy systems it is sometimes quite difﬁcult or almost impossible to
immediately get clear rules and membership functions as a result of ambiguity,
incompleteness or complexity of the systems.
In such cases, the most appropriate is to consider the generation and reﬁnement
of fuzzy rules using special training algorithms. Currently widely used is
back-propagation algorithm for fuzzy networks, described in Chap. 1, which allows
to generate the optimum model of fuzzy systems and the rule base. This algorithm
was proposed independently Ichiashi (Ichihashi), Nomura (Nomura), Wang and
Mendel (Wang and Mendel) [3, 4]. Along with them, Shi Mitsumoto suggested
another method that can be used for practical systems [4].
The main feature of this approach is that adjusting the parameters of fuzzy rules
is carried out without modifying the rules table. Without loss of generality, consider
the algorithm for a model which comprises the two input linguistic variables (x1,
x2) and one output variable y. Network diagram is shown in Fig. 3.9.
Suppose we have a rule base that contains all possible combinations A1i and A2j
(i = 1 … r; j = 1 … k) such that:
Fig. 3.9 Fuzzy neural network ANFIS structure
3.7
Fuzzy Neural Network ANFIS …
101

Rule 1 :
A11; A21 ) y1;
2 :
A11; A22 ) y2;
. . .
k :
A11; A2k ) yk;
k + 1:
A12; A21 ) yk þ 1;
2k:
A12; A2k ) y2k;
Rule
. . .
i  1
ð
Þk þ j :
A1i; A2j ) y i1
ð
Þk þ j;
. . .
Rule r  k:
A1r; A2k ) yrk;
where A1i and A2j are fuzzy sets for variables accordingly X1 and X2,
yði1Þk þ j is real value of Y.
Clearly, this set of rules can be represented in tabular form:
x1\ x2
A21
A22
…
A2j
…
A2k
A11
y1
y2
…
yj
…
yk
A12
yk+1
yk+2
…
yk+j
y2k
…
…
…
…
…
…
…
A1i
…
…
…
y(i−1)k+j
…
…
…
…
…
…
…
…
…
A1r
y(r−1) k+1
…
…
…
…
yrk
So, if we are given a set of values (x1, x2), then, according to the fuzzy rule base,
the output y can be obtained on the basis of fuzzy logic methods.
First of all, we denote the degree of fulﬁllment of the prerequisites as follows:
hði1Þk þ j ¼ A1iðx1ÞA2jðx2Þ:
ð3:38Þ
According to the centroid method network output is determined:
y ¼
Pr
i¼1
Pk
j¼1 hði1Þk þ jyði1Þk þ j
Pr
i¼1
Pk
j¼1 hði1Þk þ j
¼
Pr
i¼1
Pk
j¼1 A1iðx1ÞA2jðx2Þyði1Þk þ j
Pr
i¼1
Pk
j¼1 A1iðx1ÞA2jðx2Þ
:
ð3:39Þ
Assume this learning system uses a training sample (x1, x2; y*), then a system
error can be described as E = (y* −y)2/2.
102
3
Fuzzy Inference Systems and Fuzzy Neural Networks

Based on the description of fuzzy values for A1i denote a1i—center of mem-
bership function, b1i—width for a given function, similarly for A2j denote a2j and
b2. According to the method of gradient descent to minimize the output error E the
formulas for the calculation of coefﬁcients a1i, a2j, b2j и y(i−1)k+j (i = 1, 2, …, r;
j = 1, 2, …, k) take the form [3]:
a1iðt þ 1Þ ¼ a1iðtÞ  a@E=@a1iðtÞ ¼
¼ a1iðtÞ  að@E=@yÞð@y=@hði1Þk þ jÞð@hði1Þk þ j=@A1iÞð@A1i=@a1iðtÞÞ ¼
¼ a1iðtÞ þ
aðy  yÞ½Pk
j¼1 ðyði1Þk þ j  yÞA2jð@A1i=@a1iðtÞÞ
Pr
i¼1
Pk
j¼1 hði1Þk þ j
;
ð3:40Þ
b1iðt þ 1Þ ¼ b1iðtÞ  b@E=@b1iðtÞ ¼
¼ b1iðtÞ  bð@E=@yÞð@y=@hði1Þk þ jÞð@hði1Þk þ j=@A1iÞð@A1i=@b1iðtÞÞ ¼
¼ b1iðtÞ þ
bðy  yÞ½Pk
j¼1 ðyði1Þk þ j  yÞA2jð@A1i=@b1iðtÞÞ
Pr
i¼1
Pk
j¼1 hði1Þk þ j
;
ð3:41Þ
a2jðt þ 1Þ ¼ a2jðtÞ  a@E=@a2jðtÞ ¼
¼ a2jðtÞ  að@E=@yÞð@y=@hði1Þk þ jÞð@hði1Þk þ j=@A2jÞð@A2j=@a2jðtÞÞ ¼ 0
¼ a2jðtÞ þ aðy  yÞ½Pr
i¼1 ðyði1Þk þ j  yÞA1ið@A2j=@a2jðtÞÞ
Pr
i¼1
Pk
j¼1 hði1Þk þ j
;
ð3:42Þ
b2jðt þ 1Þ ¼ b2jðtÞ  b@E=@b2jðtÞ ¼
¼ b2jðtÞ  bð@E=@yÞð@y=@hði1Þk þ jÞð@hði1Þk þ j=@A2jÞð@A2j=@b2jðtÞÞ ¼
¼ b2jðtÞ þ bðy  yÞ½Pr
i¼1 ðyði1Þk þ j  yÞA1ið@A2j=@b2jðtÞÞ
Pr
i¼1
Pk
j¼1 hði1Þk þ j
;
ð3:43Þ
yði1Þk þ jðt þ 1Þ ¼ yði1Þk þ jðtÞ  c@E=@yði1Þk þ jðtÞ ¼
¼ yði1Þk þ jðtÞ  cð@E=@yÞð@y=@yði1Þk þ jðtÞÞ ¼
¼ yði1Þk þ jðtÞ þ
cðy  yÞhði1Þk þ j
Pr
i¼1
Pk
j¼1 hði1Þk þ j
;
ð3:44Þ
where a, b, c is a training speed, t is iteration number in the process of learning.
3.7
Fuzzy Neural Network ANFIS …
103

3.8
Fuzzy Neural Networks TSK and Wang-Mendel
FNN TSK
A generalization of the neural network ANFIS is a fuzzy neural network TSK
(Takagi, Sugeno, Kang’a). The generalized scheme of output in the model using a
TSK with M rules and N variables can be written as follows [3, 4]
R1 : if x1 2 Að1Þ
1 ; x2 2 Að1Þ
2 ; . . .; xn 2 Að1Þ
n ; then y1 ¼ p10 þ
X
N
j¼1
p1jxj;
RM : if x1 2 AðMÞ
1
; x2 2 AðMÞ
2
; . . .; xn 2 AðMÞ
n
; then
yM ¼ pM0 þ
X
N
j¼1
pMjxj
where AðkÞ
i —the value of linguistic variable xi for the rule Rk with MF (membership
function)
lðkÞ
A
xi
ð Þ ¼
1
1 þ
xicðkÞ
i
rðkÞ
i

2bðkÞ
i
:
ð3:45Þ
i ¼ 1; N; k ¼ 1; M:
At the intersection of the TSK network rule conditions Rk MF is deﬁned as a
product
lðkÞ
A
x
ð Þ ¼
Y
N
j¼1
1
1 þ
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
2
66664
3
77775
:
ð3:46Þ
With M inference rules composition results is determined by the following
formula (similar to the inference of Sugeno):
y x
ð Þ ¼
PM
k¼1 wkyk x
ð Þ
PM
k¼1 wk
;
ð3:47Þ
104
3
Fuzzy Inference Systems and Fuzzy Neural Networks

where yk x
ð Þ ¼ pk0 þ P
N
j¼1
pkjxj: The weights in this expression are interpreted as the
degree of fulﬁllment of rule antecedents (conditions): wk ¼ lðkÞ
A ðxÞ; which are given
by (3.46).
The fuzzy neural network TSK, which implements the output in accordance with
(3.47) represents a multilayer structure network shown in Fig. 3.10. In such a
network, 5 layers are present:
1. The ﬁrst layer performs fuzziﬁcation separately for each variable xi; i ¼
1; 2; . . .; N; deﬁning for each rule k value MF lðkÞ
A ðxiÞ in accordance with the
fuzziﬁcation function, which is described, for example, by (3.45). This is a
parametric layer with parameters cðkÞ
j ; rðkÞ
j ; bðkÞ
j ; which are subject to adjustment
in the learning process.
Fig. 3.10 The structure of TSK fuzzy neural network
3.8
Fuzzy Neural Networks TSK …
105

2. The second layer performs the aggregation of individual variables xi; deter-
mining the resulting degree of membership wk ¼ lðkÞ
A ðxÞ for the vector x. This is
not a parametric layer.
3. The third layer is a function generator TSK, wherein the output values are
calculated yk x
ð Þ ¼ pk0 þ P
N
j¼1
pkjxj: At this layer also functions formed on the
previous layer yk x
ð Þ and wk are multiplied. This is a parametric layer, wherein
the adaptation of linear parameters (weight) pk0; pkj for j ¼ 1; N; k ¼ 1; M; is
carried out determining the rules output functions.
4. The fourth layer consists of 2 summing neurons, one of which calculates the
weighted sum of the signals yk x
ð Þ, and the second determines the sum of the
weights P
M
k¼1
wk.
5. The ﬁfth layer is composed of a single output neuron. In it weights normalizing
is performed and the output signal y x
ð Þ is calculated in accordance with the
expression
y x
ð Þ ¼ f1
f2
¼
P
M
k¼1
wkyk x
ð Þ
P
M
k¼1
wk
:
ð3:48Þ
This is also non-parametric layer.
From the above description it follows that the TSK fuzzy network contains only
2 parametric layer (the ﬁrst and third), the parameters of which are speciﬁed in the
learning process. Parameters of the ﬁrst layer
cðkÞ
j ; rðkÞ
j ; bðkÞ
j


we call non-linear,
and the parameters of the third layer
pkj


—linear weights. The general expression
for the functional dependence (3.47) for the network TSK is deﬁned as follows:
yðxÞ ¼
1
P
M
k¼1
Q
M
j¼1
lðkÞ
A
xj
 	
X
M
k¼1
pk0 þ
X
N
j¼1
pkjxj
 
! Y
N
j¼1
lðkÞ
A
xj
 	
ð3:49Þ
If we assume that at any given time the non-linear parameters are ﬁxed, then the
function y x
ð Þ would be linear with respect to the variable xj:
In the presence of N input variables each rule Rk formulates N þ 1
ð
Þ variable pðkÞ
j
linear dependence yk x
ð Þ. If M inference rules are present then M N þ 1
ð
Þ linear
network parameters are obtained. In turn, each MF uses 3 parameters
c; r; b
ð
Þ;
which are subject to adjustment. With M inference rules 3MN nonlinear parameters
are obtained. In total this gives M 4N þ 1
ð
Þ linear and nonlinear parameters that
must be determined in the learning process. This is very large value. In order to
106
3
Fuzzy Inference Systems and Fuzzy Neural Networks

reduce the number of parameters for adaptation, we operate with fewer number of
MF. In particular, it can be assumed that some of the parameters of one function MF
lðkÞ
A ðxjÞ are ﬁxed, e.g. rðkÞ
j
and bðkÞ
j .
The Structure of Wang-Mendel Network
If instead of the rules output functions ykðxÞ we select the output mk, then we obtain
the structure of the FNN, which is called Wang-Mendel neural network (see
Fig. 3.11).
This is the four-layer structure, wherein the ﬁrst layer performs the fuzziﬁcation
of input variables, the second—aggregation (crossing) of the rules conditions, a
third (linear)—composition of outputs of inference rules (the ﬁrst neuron) and
generation of a normalizing signal (second neuron), whereas a last neuron layer
produces an output signal y x
ð Þ, which is calculated as follows:
Fig. 3.11 The structure of the fuzzy neural network of Wang-Mendel
3.8
Fuzzy Neural Networks TSK …
107

y x
ð Þ ¼
PM
k¼1 mk
QN
j¼1 lðkÞ
A
xj
 	
h
i
PM
k¼1 lðkÞ
A
:
ð3:50Þ
Note the great similarity of structures of both networks. Parts deﬁning ante-
cedents in the ﬁrst and second layer are identical (i.e., they are responsible for rule
component “if…”), the differences are manifested in the representation of the
consequences of the rules (“then…”).
TSK output function is represented by a polynomial of the ﬁrst order, and in a
network of Wang-Mendel by constant mk ¼ ck, where the value of ck can be
interpreted as the center of MF. Thus, FNN Wang-Mendel is a special case of
FNN TSK.
The objective of both networks is to ﬁnd such a mapping in data pairs, in which
the expected value d, which corresponds to the input vector x, will be formed by
network output function y x
ð Þ.
The training of fuzzy neural networks, as well as crisp NN may be carried out in
accordance with the supervised algorithm with the teacher, which uses the objective
function E ¼ 1
2
P
p
l¼1
y x
ð ÞðlÞdðlÞ

2
! min, and with the unsupervised algorithm of
self-organization.
Hybrid Learning Algorithm Fuzzy Neural Networks
Consider a hybrid learning algorithm FNN, which is used for both networks TSK
and Wang-Mendel (whose pkj ¼ 0; а pk0 ¼ mk) [3, 10].
In the hybrid algorithm the adapted parameters can be divided into 2 groups. The
ﬁrst group includes linear parameters pkj of the third layer, and the second group—
nonlinear parameters (MF) of the ﬁrst layer. Adaptation occurs in two stages.
In the ﬁrst stage after ﬁxing the individual parameters of the membership
function (in the ﬁrst iteration—the values that are obtained by initializing) by
solving a system of linear equations, linear parameters of polynomial pkj are cal-
culated. With the known values of MF dependence input-output can be represented
as a linear form with respect to the parameters pkj:
yk x
ð Þ ¼
X
M
k¼1
w
0
k
pk0 þ
X
N
j¼1
pkjxj:
 
!
;
ð3:51Þ
108
3
Fuzzy Inference Systems and Fuzzy Neural Networks

where
w
0
k ¼
Q
N
j¼1
lðkÞ
A
xj
 	
P
M
r¼1
Q
N
j¼1
lðrÞ
A
xj
 	 ; k ¼ 1; M:
ð3:52Þ
With the dimension L of training sample
xðlÞ; dðlÞ

	
; ðl ¼ 1; 2; . . .; LÞ and
replacement of the network output by expected value dðlÞ we get a system of L
linear equations of the form:
w
0
11
w
0
11xð1Þ
1 . . .
w
0
11xð1Þ
N . . .
w
0
1M
w
0
21
w
0
21xð2Þ
1 . . .
w
0
21xð2Þ
N . . .
w
0
2M
. . .
. . .
. . .
. . .
w
0
L1
w
0
L1xðLÞ
1
w
0
LxðLÞ
N
w
0
LM
w
0
1Mxð1Þ
1 . . .
w
0
2Mxð2Þ
1 . . .
. . .
w
0
LMxðLÞ
1
w
0
1Mxð1Þ
N
w
0
2Mxð2Þ
N
. . .
w
0
LMxðLÞ
N
2
6664
3
7775 
p10
p11
. . .
p1N
. . .
pM0
pM1
. . .
pMN
2
66666666664
3
77777777775
¼
dð1Þ
dð2Þ
. . .
dðLÞ
2
664
3
775
ð3:53Þ
where w
0
‘i means the level of activation (w) of the ith rule conditions at presentation
of ‘th input vector x‘. This expression can be written in matrix form:
Ap ¼ d:
Matrix A dimension is equal to LðN þ 1ÞM: By thus a number of rows L usually
is much greater than a number of columns ðN þ 1ÞM: The solution of this equations
system may be obtained by conventional methods as well as using pseudo-inverse
matrix A at one step:
p ¼ A þ d;
where A þ —pseudo-inverse matrix.
In the second stage, after ﬁxing the values of linear parameters pkj the actual
output signals yð‘Þ; ‘ ¼ 1; 2; . . .; L, are calculated using a linear equations system:
yðLÞ ¼ Ap:
ð3:54Þ
After
this
error
vector
e ¼ y  d
ð
Þ
and
the
criterion
E
are
calculated
E ¼ 1
2
P
L
‘¼1
y xð‘Þ

	
 dð‘Þ

	2.
3.8
Fuzzy Neural Networks TSK …
109

The error signals are sent through the network in the reverse direction according
to the method of Back Propagation until the ﬁrst layer, at which gradient vector
components of the objective function with respect to parameters
cðkÞ
j ; rðkÞ
j ; bðkÞ
j


can be calculated.
After calculating the gradient vector a step of gradient descent method is made.
The corresponding formulas (for the simplest method of the steepest descent) take
the form:
cðkÞ
j
n þ 1
ð
Þ ¼ cðkÞ
j
n
ð Þ  gc
@E n
ð Þ
@cðkÞ
j
;
ð3:55Þ
rðkÞ
j
n þ 1
ð
Þ ¼ rðkÞ
j
n
ð Þ  gr
@E n
ð Þ
@rðkÞ
j
;
ð3:56Þ
bðkÞ
j
n þ 1
ð
Þ ¼ bðkÞ
j
n
ð Þ  gb
@E n
ð Þ
@bðkÞ
j
;
ð3:57Þ
where n is a number of iteration.
After verifying the nonlinear parameters the process of adaptation of linear
parameters TSK (ﬁrst phase) restarts and nonlinear parameters are further adapted
(second stage). This cycle continues until all the parameters will be stabilized.
Formulas (3.55)–(3.57) require the calculation of the gradient of the objective
function with respect to the parameters of the MF. The ﬁnal form of these formulas
depends on the type of MF. For example, using the generalized bell functions:
lAðxÞ ¼
1
1 þ ðxc
r Þ2b
ð3:58Þ
the corresponding formulas for gradient of the objective function for one pair of
data x; d
ð
Þ take the form [3, 4, 10]:
@E
@cðkÞ
j
¼ y x
ð Þ  d
ð
Þ
X
M
r¼1
pr0 þ
X
N
j¼1
prjxj
 
!
 @w
0
r
@cðkÞ
j
;
ð3:59Þ
@E
@rðkÞ
j
¼ y x
ð Þ  d
ð
Þ
X
M
r¼1
pr0 þ
X
N
j¼1
prjxj
 
!
 @w
0
r
@rðkÞ
j
:
110
3
Fuzzy Inference Systems and Fuzzy Neural Networks

@E
@bðkÞ
j
¼ y x
ð Þ  d
ð
Þ
X
M
r¼1
pr0 þ
X
N
j¼1
prjxj
 
!
 @w
0
r
@bðkÞ
j
:
Derivatives @w
0
r
@cðkÞ
j , @w
0
r
@rðkÞ
j , @w
0
r
@bðkÞ
j , derived from dependency (3.52) take the following
form [3]:
@w
0
r
@cðkÞ
j
¼ drkm xj
 	
 ‘ xj
 	
m xj
 	

2
Y
M
i¼1;i6¼j
2bðkÞ
j
rðkÞ
j
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j 1
"
#
1 þ
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
"
#2 lðkÞ
A
xi
ð Þ;
ð3:60Þ
@w
0
r
@rðkÞ
j
¼ drkm xj
 	
 ‘ xj
 	
m xj
 	

2
Y
M
i¼1;i6¼j
2bðkÞ
j
rðkÞ
j
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
"
#
1 þ
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
"
#2 lðkÞ
A
xi
ð Þ;
ð3:61Þ
@w
0
r
@bðkÞ
j
¼ drkm xj
 	
 ‘ xj
 	
m xj
 	

2
Y
M
i¼1;i6¼j
2
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
ln
xjcðkÞ
j
rðkÞ
j


"
#
1 þ
xjcðkÞ
j
rðkÞ
j

2bðkÞ
j
"
#2

lðkÞ
A
xi
ð Þ
;
ð3:62Þ
for
r ¼ 1; 2; . . .; M,
where
drk—Kronecker
delta,
‘ xj
 	
¼ QN
j¼1 lðkÞ
A
xj
 	
;
m xj
 	
¼ PM
k¼1
QN
j¼1 lðkÞ
A
xj
 	
:
In the practice of the hybrid learning method application the dominant factor in
adaptation is considered to be the ﬁrst step in which weights pkj are selected using
pseudoinverse in one step. To balance its impact the second phase should be
repeated many times in each cycle.
The described hybrid algorithm is one of the most effective ways of learning
fuzzy neural networks. Its characteristic feature is the division of the process into
two stages separated in time. Since the design complexity of each nonlinear opti-
mization algorithm depends non-linear on the number of parameters subject to
optimization, the reduction in the dimensions of optimization signiﬁcantly reduces
the total amount of calculations and increases the speed of convergence of the
algorithm. Due to this hybrid algorithm is one of the most efﬁcient in comparison
with conventional gradient methods.
3.8
Fuzzy Neural Networks TSK …
111

3.9
Adaptive Wavelet-Neuro-Fuzzy Networks
Along with the neuro-fuzzy systems, wavelet transform is becoming more and more
popular as a method that gives a compact representation of signals in both the time
and frequency domains.
Based on the research of neural networks (ANN), and the theory of wavelet
neural networks methods for the analysis of non-stationary processes with signif-
icant non-linear trends have been developed.
The natural step was the following: to make the combination of clarity and
interpretability of fuzzy neural networks, strong approximation and educational
properties of artiﬁcial neural networks (ANN) with the ﬂexibility of wavelet
transforms in the context of hybrid systems of computational intelligence, which
were called the adaptive wavelet-neuro-fuzzy networks (AWNFN) [11].
The key to ensuring the effectiveness of such systems, is a choice of learning
algorithm, which is usually based on gradient procedures to minimize the chosen
criterion. A combination of gradient optimization method and back propagation
(error) signiﬁcantly reduces the rate of learning hybrid CI systems.
In the case when the data processing must be done in real time, and the predicted
sequence is unsteady and distorted by noise, the traditional conventional learning
algorithm of gradient descent, appears to be ineffective.
In this section we consider the problem of synthesis of adaptive predictive
wavelet neural network, which has a higher rate of learning in comparison with
systems using conventional back-propagation algorithm of gradient type.
3.9.1
The Architecture of Adaptive Wavelet Fuzzy Neural
Network
Introduce the ﬁve-layer architecture shown in Fig. 3.12, in a way similar to the
previously considered FNN TSK, which is a learning system with the inference of
Takagi-Sugeno-Kang (Fig. 3.10).
In this case, at the layer 4 values of signals are calculated
wjðkÞðpj0 þ
X
n
i¼1
pjixðk  iÞÞ ¼ wjðkÞpT
j XðkÞ
ð3:63Þ
where XðkÞ ¼ 1; XTðkÞ
ð
ÞT; pj ¼ pj0; pj1; . . .; pjn

	T;.
here hðn þ 1Þ parameters pj; j ¼ 1; 2; . . .:; h; i ¼ 0; 1; 2; :. . ., are to be
determined.
112
3
Fuzzy Inference Systems and Fuzzy Neural Networks

Finally, the output signal (prediction x(k)) is calculated at the 5th output layer
[11]
^xðkÞ ¼
X
h
j¼1
wjðkÞfjðXðkÞÞ ¼
X
h
j¼1
wjðkÞ
Ph
j¼1 wjðkÞ
fjðXðkÞÞ ¼
¼
Qn
i¼1 ujiðxðk  iÞ; cji; rjiÞ
Ph
j¼1
Qn
i¼1 ujiðxðk  iÞ; cji; rjiÞ
fjðXðkÞÞ
ð3:64Þ
This expression vectors by introducing variables
f ðXðkÞÞ ¼
w1ðkÞ; w1ðkÞxðk  1Þ; . . .; w1ðkÞxðk  nÞ; w2ðkÞ;
w2ðkÞxðk  1Þ; . . .; w2ðkÞxðk  nÞ; . . .
. . .; wnðkÞ; wnðkÞxðk  1Þ; . . .; wnðkÞxðk  nÞ
0
B
@
1
C
A
T
and p ¼ ðp10; p11; . . .; p20; p21; . . .; p2n; poh; ph1; . . .; phnÞT dimension h(n + 1)
can be rewritten in a compact form ^xðkÞ ¼ pTf ðXðkÞÞ:
Fig. 3.12 Structure of adaptive wavelet fuzzy neural network
3.9
Adaptive Wavelet-Neuro-Fuzzy Networks
113

Adjustable parameters of the network are located only in the ﬁrst and fourth
hidden layers. These are 2 hn wavelet parameters rji and cji and h(n + 1) linear
autoregression model parameters pji.
They can be identiﬁed throughout the learning process.
3.9.2
Learning of Adaptive Neuro-Fuzzy Wavelet Network
Since conﬁgurable parameter vector is contained in a linear network, to conﬁgure it
is possible to apply any of the algorithms used in the adaptive identiﬁcation, pri-
marily RLSM method. It is written in the form [11]
pðk þ 1Þ ¼ pðkÞ þ PðkÞðxðkÞ  pTðkÞf ðXðkÞÞÞ
a þ f TðXðkÞÞPðkÞf ðXðkÞÞ f ðXðkÞÞ
Pðk þ 1Þ ¼ 1
a
PðkÞ 
PðkÞf ðxðk þ 1ÞÞf TðXðkÞÞPðkÞÞ
a þ f TðXðk þ 1ÞÞPðkÞf ðXðk þ 1ÞÞ


8
>
>
>
<
>
>
>
:
ð3:65Þ
where
xðkÞ  pTðkÞf ðXðkÞÞ ¼ xðkÞ  ^xðkÞ ¼ eðkÞ
is
the
prediction
error,
0  a  1—forgetting factor of outdated information.
For training the optimum speed Kachmarzh algorithm can be used [11]
pðk þ 1Þ ¼ pðkÞ þ xðkÞ  pTðkÞf ðXðkÞÞ
f TðXðkÞÞpðkÞf ðXðkÞÞ f ðXðkÞÞ
ð3:66Þ
or algorithm of Gulvin-Ramadji Keynes [11–18]
pðk þ 1Þ ¼ pðkÞ þ r¼1ðkÞðxðkÞ  pTðkÞf ðXðkÞÞÞf ðXðkÞÞ
rðk þ 1Þ ¼ rðkÞ þ f ðXðk þ 1ÞÞ
k
k2
(
ð3:67Þ
which is the well-known procedure of stochastic approximation.
To adjust the ﬁrst hidden layer parameters in the network AWNFN can be used
learning algorithm back propagation, based on chained differentiation and gradient
descent
EðkÞ ¼ 1
2 e2ðkÞ ¼ 1
2 ð^xðkÞ  xðkÞÞ2
In general, the learning procedure for this layer is given by [11]
cjiðk þ 1Þ ¼ cji  gcðkÞ @EðkÞ
@cjiðkÞ
rjiðk þ 1Þ ¼ rji  grðkÞ @EðkÞ
@rjiðkÞ
8
>
>
>
<
>
>
>
:
ð3:68Þ
114
3
Fuzzy Inference Systems and Fuzzy Neural Networks

Its properties are completely determined by the parameters of speed training
gcðkÞ, grðkÞ, selected from empirical considerations. It should be noted that if the
parameters of the fourth layer can be adjusted more quickly, the speed of operations
is lost on the ﬁrst layer.
Increasing the speed of convergence can be achieved using more complex than
the gradient procedures such as the Marquardt algorithm or Hartley which can be
written as follows [11]
Fðk þ 1Þ ¼ FðkÞ þ kðJðkÞJTðkÞ þ gIÞð1ÞJðkÞeðkÞ
ð3:69Þ
where
FðkÞ ¼ðc11ðkÞ; r1
11 ðkÞ; c21ðkÞ; r1
21 ðkÞ; . . .; cjiðkÞ; r1
ji ðkÞ; . . .::
. . .; chnðkÞ; r1
hn ðkÞÞT
is 2h(n + 1)—vector of adjustable parameters (to reduce the complexity of the
computational procedure not parameter rjiðkÞ, but its reciprocal is used) JðkÞ—is
the vector gradient of the output signal ^xðkÞ of variable parameters, I—identity
matrix 2 hn * 2 hn, g—scalar manipulated variable; a—positive scalar;
JðkÞ ¼
@^xðkÞ
@c11
; @^xðkÞ
@r1
11 ðkÞ ; @^xðkÞ
@c21
; @^xðkÞ
@r1
21 ðkÞ ; . . .; @^xðkÞ
@cji
; @^xðkÞ
@r1
ji ðkÞ ; . . .:
. . .; @^xðkÞ
@chn
; @^xðkÞ
@r1
hn ðkÞ
0
B
B
B
@
1
C
C
C
A
T
To reduce the computational complexity of the algorithm, the following
expression can be used
JJT þ gI

	1¼ g1I  g1IJJTg1I
1 þ JTg1IJ . . .:
ð3:70Þ
using which you can easily obtain the relation
k JJT þ gI

	1J ¼ k
J
g þ J
k k2
Substituting this relation in the algorithm (3.69) we obtain the following algo-
rithm for learning parameters of the ﬁrst layer in the form [11, 14]
Fðk þ 1Þ ¼ FðkÞ þ k JðkÞeðkÞ
g þ J
k k2
ð3:71Þ
It is easily seen that the algorithm (3.71) is a nonlinear multiplicative modiﬁ-
cation of Kachmarzh adaptive algorithm and if k ¼ 1, g ¼ 0 coincides with it in the
structure.
3.9
Adaptive Wavelet-Neuro-Fuzzy Networks
115

To ensure the ﬁltering properties of the training algorithm, introduce an addi-
tional procedure for setting regularizing parameter η in the form
Fðk þ 1Þ ¼ FðkÞ þ k JðkÞeðkÞ
gðkÞ
gðk þ 1Þ ¼ agpðkÞ þ f XðkÞ
ð
Þ
k
k2
(
ð3:72Þ
If a ¼ 0, then this procedure is the same as (3.66) and has a top speed of
convergence, and if a ¼ 1, the procedure to get the properties of stochastic
approximation and is a generalization of the procedure (3.71) in the nonlinear case.
3.9.3
Simulation Results
To study the effectiveness of the proposed neuro-fuzzy wavelet network and its
learning algorithm AWNFN network was trained to predict chaotic time series
Macki-Glass. Forecasting time series Macki-Glass is a standard test widely used to
assess and compare the functioning of neural and neuro-fuzzy networks in time
series forecasting problems. Time series of Macki-Glass describes the differential
equations with delay [11]:
_xðtÞ ¼
0:2xðt  sÞ
1 þ x10ðt  sÞ  0:1xðtÞ
ð3:73Þ
The value of the time series (3.73) were obtained for each integer value t by the
Runge-Kutt fourth order algorithm. Integration time step in this method was 0.1 the
initial condition was taken xð0Þ ¼ 1:2 and the delay s ¼ 17 and xðtÞ calculated for
t ¼ 0; 1; . . .; 51000. The values xðt  18Þ; xðt  17Þ; xðt  16Þ; xðtÞ were used for
the prediction xðt þ 6Þ.
In the online mode network AWNFN was trained on the procedure (3.69) during
50000 iterations (50000 training samples for t ¼ 118; . . .; 50117.
Training algorithm parameters were a ¼ 0:45, kp ¼ 2, k ¼ 1. The initial value
was gð0Þ ¼ 1, gp ¼ 10000. After 50,000 iterations the training stopped and the next
500 points for the values 50118 … 50617 were used as test data for the calculation
of the forecast. As an activation function was chosen wavelet function “Mexican
hat”. The initial value of synaptic weights generated randomly in the interval [−0.1,
0.1]. As a criterion for the quality of the forecast RMSE criterion was used
RMSE ¼ 1
N
X
N
k¼1
xðkÞ  ^xðkÞ
ð
Þ2
116
3
Fuzzy Inference Systems and Fuzzy Neural Networks

In Fig. 3.13 the results of prediction are presented. The two curves represent the
real data (dotted line) and forecast (solid line) are almost indistinguishable.
Table 3.1 presents the results of forecasting adaptive wavelet—neuro-fuzzy
network in comparison with the results of the prediction based on the standard fnn
ANFIS learning algorithm and backpropagation.
As can be easily seen from the experimental results, the proposed prediction
algorithm using adaptive neuro-fuzzy network with wavelet functions and learning
algorithms (3.63), (3.68) provides a signiﬁcantly better quality of forecasting and
increased the speed of learning in comparison with classic network ANFIS.
3.10
Neo-Fuzzy-Cascade Neural Networks
Consider the neo-fuzzy-neuron with multiple inputs and a single output, which is
shown in Fig. 3.14. It is described in the following expression:
^y ¼
X
n
i¼1
fiðxiÞ;
ð3:74Þ
Fig. 3.13 Results of prediction time series Macki-Glass
Table 3.1 The results of the chaotic time series prediction
Neural network/learning algorithm
RMSE
The adaptive wavelet-neuro-fuzzy network/Prop. algorithm (12), (13)
0.0120
Network ANFIS algorithm backpropagation
0.2312
3.9
Adaptive Wavelet-Neuro-Fuzzy Networks
117

where xi is the ith input (i ¼ 1; 2; . . .; N), y is system output. Structural unit of
neo-fuzzy neuron is nonlinear synapse (Nsi), which transforms the ith input signal
into the form:
fiðxiÞ ¼
X
h
j¼1
wjiljiðxiÞ
and performs fuzzy inference: If xi is xji then output is wji, where xji is a fuzzy
number, whose the membership function is lji, wji is a synaptic weight. It is obvious
that the nonlinear synapse actually implements fuzzy Takagi-Sugeno conclusion
zero order.
A
When
the
vector
signal
xðkÞ ¼ ðx1ðkÞ; x2ðkÞ; . . . xnðkÞÞT
(k ¼ 1; 2; . . .
discrete time) is entered into the neo-fuzzy neuron, the output of this neuron is
Fig. 3.14 A structure of Neo-fuzzy neuron
118
3
Fuzzy Inference Systems and Fuzzy Neural Networks

determined by both the membership functions ljiðxiðkÞÞ and adjustable synaptic
weights wjiðk  1Þ, which were obtained in the previous training epoch [11]:
^yðkÞ ¼
X
n
i¼1
fiðxiðkÞÞ ¼
X
n
i¼1
X
h
j¼1
wjiðk  1ÞljiðxiðkÞÞ
ð3:75Þ
thus neo-fuzzy neuron contains h * n the synaptic weights to be determined.
Architecture of Cascade Neo-Fuzzy Neural Network
Architecture CNFNN, is shown in Fig. 3.15 and is characterized by mapping which
has the following form [11, 14]:
• Neo-fuzzy neuron of the ﬁrst cascade
^y½1 ¼
Xn
i¼1
Xh
j¼1 w½1
ji lji xi
ð Þ
ð3:76Þ
Fig. 3.15 Architecture of cascade neo-fuzzy neural network
3.10
Neo-Fuzzy-Cascade Neural Networks
119

• Neo-fuzzy neuron of the second cascade
^y½2 ¼
Xn
i¼1
Xh
j¼1 w½2
ji lji xi
ð Þ þ
Xh
j¼1 w½2
j;n þ 1lj;n þ 1 ^y½1


• Neo-fuzzy neuron of the 3rd cascade
^y½3 ¼
Xn
i¼1
Xh
j¼1 w½3
ji lji xi
ð Þ þ
Xh
j¼1 w½3
j;n þ 1lj;n þ 1 ^y½1


þ
þ
Xh
j¼1 w½3
j;n þ 2lj;n þ 2 ^y½2


• Neo-fuzzy neuron of the mth cascade
^y½m ¼
Xn
i¼1
Xh
j¼1 w½m
ji lji xi
ð Þ þ
Xn þ m1
l¼n þ 1
Xh
j¼1 w½m
jl ljl ^y½ln


ð3:77Þ
Therefore, cascade neo-fuzzy neural network includes hðn þ Pm1
1
lÞ adjustable
parameters and it is important that they are included in the linear description (3.77).
Let h(n + m −1)  1 be vector of membership functions mth neo-fuzzy neuron
l½m ¼ ðl11ðx1Þ; . . .; lh1ðx1Þ; l12ðx2Þ; . . .; lh2ðx2Þ;
. . .; ljiðXiÞ; . . .; lhnðXnÞ; l1;n þ 1ð^y½1Þ; . . .; lh;n þ 1ð^y½1Þ; . . .; lh;n þ m1ð^ym1ÞÞT
And the corresponding vector of synaptic weights:
w½m ¼ ðw½m
11 ; w½m
21 ; . . . ; w½m
h1 ; w½m
12 ; . . .; w½m
h2 ; . . .. . .; w½m
ji . . .; w½m
hn ;
w½m
1;n þ 1; . . .; w½m
h;n þ 1; . . .; w½m
h;n þ m1ÞT;
which has the same dimension. Then we can present the expression (3.77) in a
vector form
y ¼ w½mTl½m
Training of Cascade Neo-Fuzzy Neural Network
Cascade neo-fuzzy neural network training can be executed in a batch mode, or in a
sequential mode (adaptive adjustment of weights).
First, let consider the situation when the training sample is determined a priori,
that is, we have a sample of values x(1), y(1); x(2), y(2); …; x(k), y(k); …; x(N), y
(N).
120
3
Fuzzy Inference Systems and Fuzzy Neural Networks

For the neo-fuzzy neuron of the ﬁrst cascade NFN [1] sample values of the
membership functions are l[1](1), l[1](2), …, l[1](k), …, l[1](N) (hn x 1 vector) It is
deﬁned as follows: l[1](k) = (l11(x1(k)), …, lh1(x1(k)), l12(x2(k)), …, lh2(x2(k)),
……, lji(xi(k)), …, lhn(xn(k)))T.
Then, minimizing the learning criterion
E½1
N ¼ 1
2
XN
k¼1
e½1ðkÞ

2
¼ 1
2
XN
k¼1
yðkÞ  ^y½1ðkÞ

2
;
vector of synaptic weights can be determined as [11]
w½1ðNÞ ¼
XN
k¼1 l½1ðkÞl½1TðkÞ

 þ XN
k¼1 l½1ðkÞyðkÞ ¼
¼ P½1ðNÞ
XN
k¼1 l½1ðkÞyðkÞ
ð3:78Þ
where (•)+ is the Moore-Penrose pseudoinverse.
In the case of sequential data processing the recursive least squares method is used:
w½1ðk þ 1Þ ¼ w½1ðkÞ þ P½1ðkÞ yðk þ 1Þ  w½1TðkÞl½1ðk þ 1Þ

	
1 þ l½1Tðk þ 1ÞP½1ðkÞl½1ðk þ 1Þ
l½1ðk þ 1Þ
P½1ðk þ 1Þ ¼ P½1ðkÞ  P½1ðkÞl½1ðk þ 1Þl½1Tðk þ 1ÞP½1ðkÞ
1 þ l½1Tðk þ 1ÞP½1ðkÞl½1ðk þ 1Þ ; P½1ð0Þ ¼ bI
8
>
>
>
>
<
>
>
>
>
:
ð3:79Þ
where b is a large positive number, I is the unitary matrix with appropriate
dimension.
It is possible to use adaptation algorithms (3.78) or (3.79), which reduce the
computational complexity of the learning process. In any case, the use of proce-
dures (3.78) (3.79) signiﬁcantly reduces training time, compared with the gradient
method, the underlying rules of the delta and backpropagation.
After the ﬁrst cascade of the training synaptic weights of neo-fuzzy neuron NFN
[1] become “frozen”, all values y½1ð1Þ; y½1ð2Þ; . . .Y½1ðkÞ; . . .; Y½1ðNÞ will be
evaluated and a second network cascade is obtained that consists of a single
neo-fuzzy neuron NFN [2]. It has one additional input for the output signal of the
ﬁrst cascade. Then, again the procedure (3.78) is used for adjusting the weight
vector w [2] of dimension hðn þ 1Þ.
In the online method of training neurons are trained sequentially, based on input
signals xðkÞ. Let evaluated synaptic weights of the ﬁrst cascade be w½1ðkÞ and
received output vector be y½1ðkÞ, then using the input vector of the second cascade
(xðkÞ, y½1ðkÞ) weight w½2ðkÞ and outputs y½2ðkÞ of the second cascade are calcu-
lated. For this purpose, the algorithms (3.78) and (3.79) can be used equally well.
The process of the neural network growth (increasing the number of cascades)
continues until we obtain the required accuracy of the solution. To adjust the
3.10
Neo-Fuzzy-Cascade Neural Networks
121

weighting coefﬁcients of the last m-th cascade, the following expression is used
[11]:
w½mðNÞ ¼ ð
X
N
k¼1
l½mðkÞ l½mTðkÞÞ
þ X
N
k¼1
l½mðkÞyðkÞ ¼ P½mðNÞ
X
N
k¼1
l½mðkÞyðkÞ
In a batch mode
w½mðk þ 1Þ ¼ w½mðkÞ þ P½mðkÞðyðk þ 1Þ  w½mTðkÞ w½mðk þ 1Þ
1 þ l½mTðk þ 1Þ P½mðkÞ l½mðk þ 1Þ
l½mðk þ 1Þ;
P½mðk þ 1Þ ¼ P½mðkÞ  P½mðkÞ l½mðk þ 1Þ l½mTðk þ 1Þ P½mðkÞ
1 þ l½mTðk þ 1Þ P½mðkÞ l½mðk þ 1Þ ; P½mð0Þ ¼ bI
8
>
>
>
<
>
>
>
:
;
ð3:80Þ
Or
w½mðk þ 1Þ ¼ w½mðkÞ þ ðr½mðk þ 1ÞÞ
1ðyðk þ 1Þ  w½mTðkÞ l½mðk þ 1ÞÞ l½mðk þ 1Þ
r½mðk þ 1Þ ¼ a r½mðkÞ þ
l½mðk þ 1Þ

2; 0  a  1
8
<
:
ð3:81Þ
in a sequential mode.
Thus, the proposed CNFNN far exceeds conventional cascaded architecture by
training speed and can be trained in batch mode or in sequential (adaptive) mode.
Linguistic interpretation of the results signiﬁcantly expands the functionality of
cascade neo-fuzzy neural network.
3.11
Simulation Results
We investigated the forecasting series of numbers to evaluate the work of the
proposed architecture. Cascade neo-fuzzy neural network was applied to predict
chaotic process Macki-Glass deﬁned by the equation [11]:
y0ðtÞ ¼
0:2xðt  sÞ
1 þ y10ðt  sÞ  0:1yðtÞ
ð3:82Þ
The signal y(t), deﬁned by (3.82), was digitized with step 0.1. A fragment
containing 500 measurements was taken for the training sample. It is necessary to
predict the value of the time series at six steps forward using its values at time k, (k
−6), (k−12) and (k−18). The test sample contains 9500 measurements, i.e. the
122
3
Fuzzy Inference Systems and Fuzzy Neural Networks

value of the time series in the time range from 501 to 1000. In order to evaluate the
results Normalized root mean square error (NRMSE) was used.
The modeling was performed using a network CNFNN with 5 cascades. The ﬁrst
cascade comprises 4 nonlinear synapses for each input value of the time series and
each synapse contains 10 activation functions (i.e. membership functions). Input
signals were presented in the interval [0, 1]. The experimental results of forecasting
time series and their errors are shown in Fig. 3.16a, b. After training, the CNFNN
network error on test sample was 4  104.
Fig. 3.16 The results of CNFNN: a the network outputs and error; b forecasted and actual time
series
3.11
Simulation Results
123

Application of GMDH for Optimal Synthesis of Architecture of Neo-Cascade
Fuzzy Neural Network
An important problem that must be solved using neural cascade neo-fuzzi-networks
is the problem of selecting the number of cascades and the synthesis of the structure
of such a network. Such a task is reasonable to solve using heuristic self-organizing
method GMDH considered in the Chap. 6 [15, 16].
Consider this method and give the corresponding experimental results.
GMDH Algorithm for Synthesis of a Cascade Neo-Fuzzy Network Structure
If to use a neo-fuzzy neurons with two inputs, it is possible to apply the method
GMDH for the synthesis of the optimal structure of the cascade neo-fuzzy neural
network. The basic idea of the algorithm GMDH is that there is a sequential buildup
of layers of the neural network until an external criterion starts to increase.
Description of the algorithm.
1. Form a pair of outputs of neurons of this layer (in the ﬁrst iteration, we use a
number of input signals). Each pair is applied to the inputs of the neo-fuzzy
neuron.
2. Using the training sample, determine a weight for each connection of a
neo-fuzzy neuron.
3. Using the test sample, we calculate the value of external regularity criterion for
every neo-fuzzy neuron
e½s
p ¼
1
Ngiv
X
Ngiv
i¼1
yðiÞ  ^y½s
p ðiÞ

2
ð3:83Þ
where N is the size of the test sample, s is a layer number,
^y½s
p ðiÞ is the output of neuron p at s-th layer at the i-th input signal.
4. Find the minimum value of the external criterion for all neurons in the current
layer
e½s ¼ min
p e½s
p
Testing conditions for stop is: e½s [ e½s10, where e½s; e½s1—criteria value of the
best neuron of the s-th and (s−1)-th layers, respectively.
If the condition is satisﬁed, then go back to the previous layer, ﬁnd the best
neuron with a minimum criterion and go to step 5, otherwise choose the best F
neurons in accordance with the criterion (3.83) and go to step 1 for the construction
of the next layer of neurons.
124
3
Fuzzy Inference Systems and Fuzzy Neural Networks

5. Determine the ﬁnal structure of the network. To do this, move in the opposite
direction from the best neuron along input connections and sequentially move
through all layers of neurons, save in the ﬁnal structure of the network only
those neo-fuzzy neurons, which are used in the next layer.
6. After the end of the algorithm GMDH optimal network structure will be syn-
thesized. It is easy to notice that we get not only the optimal structure of the
network, but also a trained neural network that is ready to handle the new data.
One of the major advantages of using GMDH for the synthesis cascade network
architecture is its ability to use very fast learning procedures for the adjustment
of neo-fuzzy neurons because learning occurs in layers sequentially (layer by
layer).
The Results of Experimental Studies
Experimental studies of neo-fuzzy neural network were carried out in the prediction
problem. The aim of the forecast was predicting RTS index based on the current
share prices of leading Russian companies [18].
Inputs: daily share price RTS index over the period from 5 February to May 5,
2009. Output variable—RTS index next day. The size of the training sample—100
sample values.
Forecast criteria: MSE;
– The mean absolute percentage error (MAPE).
Types of Experiments with Neo-Fuzzy Neural Network
1. Variation ratio training–testing sample in the range: 25:75, 50:50, 75:25;
2. the variation of the layers number: 1-3-5;
3. the variation of the number of iterations: 1000, 10000, 100000;
4. The variation of the number of forecasted points: 1-3-5;
Change the maximum error-stop condition: from 0.01 to 0.09.
Some experimental results are given below
Experiment 1. Variation Ratio Learning/Veriﬁcation Sample.
Experiment (A) ratio of 75:25. Charts of real and predicted values are shown in
Fig. 3.17.
Experiment (B) ratio of 50:50 MSE = 0.053562
Experiment (C) ratio of training/test—25:75. Results are shown in Fig. 3.18
MSE = 0.068489
Experiment Type 2. Comparison of the algorithm by varying the number of layers:
1-3-5-7. The forecast for one point at a ratio of training—test sample 75:25.
Experiment (A) the number of layers—1 MSE = 0.04662. Charts of the fore-
casting results are presented in Fig. 3.19.
Experiment (B) the number of layers—3, MSE = 0.255
Experiment (C) layers number 5, MSE = 0.0446
Experiment (D) layers number 7, MSE = 0.0544
3.11
Simulation Results
125

Experiment Type №3 Variation of iterations-: 1000, 10000, 100000
Experiment (A) The number of iterations—1000. MSE = 0.0588
Experiment (B) the number of iterations—10000, MSE = 0.0575
Experiment (C) the number of iterations—100000, MSE = 0.0525
Experiment Type №4 Comparison of the accuracy of prediction algorithm by
varying the number of forecasted points 1–3–5, using the ratio of training/testing
sample 75:25
Experiment (A) the number of forecasted points—1
Results are shown in Fig. 3.20.
Fig. 3.17 Forecasting results for learning /test samples ratio 75:25
Fig. 3.19 Forecasting results for network with one layer
Fig. 3.18 Forecasting results for learning/test ratio 25:75
126
3
Fuzzy Inference Systems and Fuzzy Neural Networks

MSE = 0.0495
Experiment (B) the number of forecasted points—3
MSE = 0.4469
Experiment (C) the number of forecasted points—5
MSE = 1.0418
Full structure of the neo-fuzzy neural network is shown in Fig. 3.21, and optimal
network structure, synthesized by GMDH-in the Fig. 3.22
• 4 inputs
• 1 output (1 point predicted);
• 1 hidden layer
• 4 inputs; 1 output (1 point forecast); 1 hidden layer
Fig. 3.20 Forecasting results for one point
Fig. 3.21 The structure of the full neo-fuzzy neural network
3.11
Simulation Results
127

3.12
Conclusions from Experimental Studies
After performing a series of experiments with the neo-fuzzy neural network with
full structure and optimal structure synthesized using GMDH following ﬁnal results
were obtained presented in the Table 3.2 [18].
Fig. 3.22 The optimum structure of neo-fuzzy neural network, which was synthesized by GMDH
Table 3.2 Experimental results for synthesized and standard CNFN networks
Comparison CNFNN with full structure and CNFNN
synthesized by GMDH
Type of the experiment
Experiment
parameters
CNFNN synthesized
by GMDH
Full structure
CNFNN
Variation of ratio
training/testing sample
75 %: 25 %
0.0484
0.0501
50 %: 50 %
0.0532
0.0536
25 %: 75 %
0.0608
0.0684
Number of layers variation
1
0.0628
0.0626
3
0.0381
0.0544
5
0.0434
0.0652
Iterations number
1000
0.0588
0.0674
10000
0.0479
0.0485
100000
0.0459
0.0482
Number of forecasted points
1
0.0495
0.0587
3
0.4469
1.0844
5
1.0418
1.3901
128
3
Fuzzy Inference Systems and Fuzzy Neural Networks

It is easy to notice the neo-fuzzy neural network synthesized using GMDH gives
much better results than conventional full network structure. This can be explained
by impact of the network self-organization mechanism used by GMDH. However,
the optimum network structure synthesized by GMDH shows slower convergence.
For convenience of the experimental results analysis number of ﬁgures are
presented below displaying the found dependencies. Figure 3.23 shows the
dependence of the value of MAPE error on the number of predicted points.
As one can see when predicting only one point a very high accuracy is reached:
the average relative error is less than 15 %. By increasing the number of predicted
points accuracy falls, the error is in the range of 15–45 %.
By analyzing the results presented in Fig. 3.24, one can conclude that the best
results are achieved when the number of layers is equal to 3. The average relative
error is less than 10 %. In one hidden layer error is low, but is not uniformly
distributed and may exceed 30 %.
Fig. 3.23 Curves MAPE
error in the forecast of 1.3 or 5
points
Fig. 3.24 Prediction error
(MAPE) depending on the
number of layers (1, 3, 5, 7)
3.12
Conclusions from Experimental Studies
129

In addition, during experimental studies the optimal parameters of algorithm for
the full neo-fuzzy neural network were determined [18]:
• the best ratio of “training/testing sample” −75 to −25 %.;
• the best possible number of layers—3;
• the best results were obtained when the number of training iterations—100000.
References
1. Wang, F.: Neural networks genetic algorithms, and fuzzy logic for forecasting. In:
Proceedings, International Conference on Advanced Trading Technologies, New York, July
1992, pp. 504–532
2. Zadeh, L.A.: The Concept of a Linguistic variable and its application to approximate
reasoning, Part 1 and 2, Information Sciences, vol. 8, pp. 199–249, 301–357 (1975)
3. Zaychenko, Y.P.: Fuzzy Models and Methods in Intellectual Systems. Publisher House
“Slovo”, Kiev, 354 p. (2008). Zaychenko, Y.: Fuzzy group method of data handling under
fuzzy input data. In: System Research and Information Technologies, №3, pp. 100–112
(2007) (in Russian)
4. Zaychenko, Y.P.: Fundamentals of Intellectual Systems Design. Publisher House “Slovo”,
Kiev, 352 p. (2004) (in Russian)
5. Yarushkina, N.G. Fuzzy neural networks. News Artif. Intell. №36, 47–51 (2001) (in Russian)
6. Kosko, B.: Fuzzy systems as universal approximators. IEEE Trans. Comput. (11), 1329–1333
(1994)
7. Castro, J.L., Delgado, M.: Fuzzy systems with defuzziﬁcation are universal approximators.
IEEE Trans. Syst. Man Cybern. 1996 (1), 361–367 (1998)
8. Nauck, D: Building neural fuzzy controllers with NEFCON-I. In: Kruse, R., Gebhardt, J.,
Palm, R. (eds.) Fuzzy Systems in Computer Science. Artiﬁcial Intelligence, pp. 141–151.
Vieweg, Wiesbaden (1994)
9. Rutkovska, D. Pilinsky, M., Rutkovsky, L.: Neural networks, genetic algorithms and fuzzy
systems (Transl. from pol. Rudinsky, I.D.), 452c. Hot Line Telecom (2006) (in Russian)
10. Osovsky, S.: Neural networks for information processing (Transl. from pol.), 344 p. Publisher
House Finance and Statistics (2002) (in Russian)
11. Bodyanskiy, Y., Viktorov, Y., Pliss, I.: The cascade NFNN and its learning algorithm. Bicник
Ужгopoдcькoгo нaцioнaльнoгo yнiвepcитeтy. Cepiя «Maтeмaтикa i iнфopмaтикa», Bип.
17, c. 48–58 (2008)
12. Aleksandr, I., Morton, H.: An Introduction to Neural Computing. Chapman & Hall, London
(1990)
13. Dorigo, M., Stutzle, T.: An experimental study of the simple ant colony optimization
algorithm. In: Proceedings of the WSES International Conference on Evolutionary
Computation, pp. 253–258 (2001)
14. Bodyanskiy, Y., Gorshkov, Y., Kokshenev, I., Kolodyazhniy, V.: Robust recursive fuzzy
clustering algorithms. In: Proceedings of East West Fuzzy Colloquim 2005. HS, Goerlitz,
Zittau, pp. 301–308 (2005)
15. Zaychenko, Y.P., Zayets, I.O. Ю. П. Synthesis and adaptation of fuzzy forecasting models on
the basis of self-organization method. Scientiﬁc Papers of NTUU “KPI”, №3, pp. 34–41
(2001) (in Russian)
16. Zaychenko, Y.P., Kebkal, A.G., Krachkovsky, V.F.: Fuzzy group method of data handling and
its application for macro-economic indicators forecasting. Scientiﬁc Papers of NTUU “KPI”,
№2, pp. 18–26 (2000) (in Russian)
130
3
Fuzzy Inference Systems and Fuzzy Neural Networks

17. Zaychenko, Y.P., Petrosyuk, I.M., Jaroshenko, M.S.: The investigations of fuzzy neural
networks in the problems of electro-optical images recognition. System Research and
Information Technologies, №4, pp. 61–76 (2009) (in Russian)
18. Bodyanskiy, Y., Gorshkov, Y., Kokshenev, I., Kolodyazhniy, V.: Outlier resistant recursive
fuzzy clustering algorithm. In: Reusch, B. (ed.) Computational Intelligence: Theory and
Applications. Advances in Soft Computing, vol. 38, pp. 647–652. Springer, Berlin (2006)
References
131

Chapter 4
Application of Fuzzy Logic Systems
and Fuzzy Neural Networks in Forecasting
Problems in Macroeconomy and Finance
4.1
Introduction
This chapter is devoted to numerous applications of fuzzy neural networks in
economy and ﬁnancial sphere. In the Sect. 4.2 the problem of forecasting
macroeconomic indicators of Ukraine with application of FNN is considered. The
goal of this investigation was to estimate the efﬁciency of different fuzzy inference
algorithms. Fuzzy algorithms of Mamdani, Tsukamoto and Sugeno were compared
in forecasting Consumer Price Index (CPI) and GDP of Ukraine. As result of this
investigation the best forecasting algorithm is detected.
In the Sect. 4.3 the problem of forecasting in the ﬁnancial markets with appli-
cation of FNN is considered. The forecasting variable are share prices of “Lukoil”
at the stock exchange RTS. In these experiments the same FNN are used as in the
Sect. 4.2 and the results of these experiments conﬁrmed the conclusions of
Sect. 4.2.
In the Sect. 4.4 the problem of forecasting corporations bankruptcy risk under
uncertainty is considered and investigated. At ﬁrst classical method by prof. Altman
is presented, its properties and drawbacks are analyzed. As an alternative matrix
method based on fuzzy sets theory is described and discussed. The application of
fuzzy networks with inference algorithms of Mamdani and Tsukamoto for corpo-
rations bankruptcy risk forecasting is considered and the experimental comparative
results of application all the considered methods for solution of this problem for
Ukrainian enterprises are presented and analyzed which conﬁrmed the preference of
FNN.
In the Sect. 4.5 the problem of banks ﬁnancial state analysis and bankruptcy
forecasting under uncertainty is considered. For its solution the application of FNN
TSK and ANFIS is suggested. The experimental results of the efﬁciency of con-
sidered FNN for forecasting are presented and analyzed and the best class of FNN
with the least MSE and MAPE for Ukrainian banks bankruptcy forecasting is
determined. In the Sect. 4.6 the similar problem of banks ﬁnancial state analysis
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_4
133

and bankruptcy forecasting for leading European banks is considered. The appli-
cation of FNN for its solution is investigated and the experimental results are
presented and analyzed.
4.2
Forecasting in Macroeconomics and Financial Sector
Using Fuzzy Neural Networks
Problem Statement
The goal of investigation is to assess the effectiveness of the fuzzy neural networks
(FNN) with different inference algorithms in the forecasting problems in macroe-
conomics and ﬁnancial sphere.
As the initial data macroeconomic indicators Ukraine’s economy were selected,
presented in the form of statistical time series (see Table 4.1) [1].
The task was forecasting of the following macroeconomic indicators: CPI, NGDP
and Volume of industrial production (VIP) by known macroeconomic indicators.
For the construction of rule base signiﬁcant variables and their lags were
identiﬁed. As the degree of the relationship between the input variables
x1; x2; . . .; xn and the output variable Y were used correlation coefﬁcients, by the
value of which were selected essential variables.
4.2.1
A Comparative Analysis of Forecasting Results
in Macroeconomics Obtained with Different FNN
Experimental investigations were conducted comparing the prediction efﬁciency
obtained using the following methods [1, 2]:
• Tsukamoto neuro-fuzzy controller (NFC) with linear membership functions;
• Tsukamoto neuro-fuzzy controller with monotonous membership functions;
• Mamdani neuro-fuzzy controller with Gaussian membership functions;
• Fuzzy neural network ANFIS.
Table 4.2 and Fig. 4.1 show the comparative results of forecasting the CPI
index, obtained by different methods of fuzzy inference.
As it can be easily seen from the presented results, all three fuzzy controllers
coped well with the task. We also see that the network ANFIS provides also
acceptable result, but worse than the neural fuzzy controllers Mamdani, Tsukamoto
and Sugeno. This reﬂects both the effectiveness of controllers learning algorithms
and preference of application fuzzy logic systems to predict macroeconomic indices
[2, 3].
134
4
Application of Fuzzy Logic Systems and Fuzzy …

4.3
FNN Application for Forecasting in the Financial
Sector
To verify these conclusions further experimental studies of fuzzy neural networks
application in forecasting problems in the ﬁnancial markets were carried out. As an
example share prices of “Lukoil” have been chosen for prediction, admitted to
trading at the “Stock Exchange Russian Trading System” (RTS).
For training a sample of 267 daily values of stock prices “Lukoil” was used at
the period since 30.12.2005 to 04.01.2006 [1, 4].
Table 4.1 Macroeconomic indicators in Ukraine
Data
NGDP
VIP
IRIP
CPI
WPI
RIP
M2
MB
TL
1
2358.3
102.2
100.3
103.1
99.6
113.1
1502
840.1
73.7
2
2308.5
102
99.7
101.2
100.3
111
1522.9
846.1
84.6
3
2267.7
103.7
99.9
101.1
99.2
108.1
1562.4
863.5
96.5
4
2428.5
104.3
102.2
101.2
100.7
118
1621.3
917.7
98.2
5
2535.6
102.8
102.5
101.7
102.2
107.8
1686
977.7
118.2
6
2522.8
104.4
103.1
100.5
104.4
105.8
1751.1
1020.7
138.6
7
2956.4
107.8
102.6
100.7
105.4
112.3
1776.1
1019.8
142.6
8
3025.9
103.4
101.7
100.1
105
109
1812.5
1065.6
157.8
9
3074.5
10.55
101.2
100.4
105.3
106.6
1846.6
1067.9
165.5
10
2854.3
103.9
102.1
101.1
105.3
108.5
1884.6
1078.6
158.9
11
2812.5
100.8
101.6
101.6
100.2
107.8
1930
1128.9
163.4
12
2998.4
103.2
99.8
101.5
105.7
106.9
2119.6
1232.6
262.5
13
2725.6
104.9
100.4
102.4
100.5
114.4
2026.5
1140.1
93.8
14
2853.4
1065
101.4
101.6
101.2
116.8
2108
1240.7
110.3
15
2893.1
106.7
101.3
101.1
103.3
115.4
2208.5
1284.5
125.9
16
3014.2
107.1
101.4
101
103.6
109.1
2311.2
1386.8
130.1
17
3102.6
108.5
99.8
100.8
103.9
119.7
2432.4
1505.7
158.8
18
3110.7
107
100.7
100.8
103.9
113.8
2604.5
1534
158.8
19
3192.4
107.1
102.2
100.7
104.9
112.7
2625.4
1510.8
181.9
20
3304.7
105.5
101.4
99.6
105.9
109.8
2683.2
1500.8
185
21
3205.8
108
101.4
100.3
106.9
112.6
2732.1
1484.5
205.8
NGDP—nominal GDP
VIP—volume of industrial production, % of the corresponding period of the previous year
IRIP—the index of real industrial production, % of the corresponding period of the previous year
CPI—consumer price index, % to the corresponding period of the previous year
WPI—the wholesale price index in % to the corresponding period of the previous year
RIP—the real incomes of the population
M2—aggregate M2
MB—monetary base
TL—Total loans, including loans in foreign currency
4.3
FNN Application for Forecasting in the Financial Sector
135

During testing it was found experimentally that the best was to use from three to
ﬁve terms and the rules, since just with these parameters minimum MSE was
obtained. Gradient method was used for training MF with a step of 0.04.
Table 4.2 Comparative results of forecasting CPI index, obtained by different methods
Real
values
real
Network ANFIS
NFC Tsukamoto
with linear MF
NFC Tsukamoto
with monotonous
MF
NFC Mamdani
with Gaussian MF
Forecasting
Error
Forecasting
Error
Forecasting
Error
Forecasting
Error
101.5
101.28
0.22
101.32
0.18
101.32
0.18
101.34
0.16
102.4
101.69
0.71
102.15
0.25
102.16
0.24
102.34
0.06
101.6
101.43
0.17
101.28
0.32
101.30
0.30
101.48
0.12
101.1
101.54
0.44
100.86
0.24
100.89
0.21
101.07
0.03
101.0
100.92
0.08
100.70
0.30
100.71
0.29
100.94
0.06
100.8
100.73
0.07
100.65
0.15
100.65
0.15
100.70
0.10
100.8
99.83
0.97
100.13
0.67
100.24
0.56
100.73
0.07
100.7
99.88
0.82
100.22
0.48
100.22
0.44
100.65
0.05
99.6
98.86
0.74
99.09
0.51
99.13
0.47
99.44
0.16
100.3
99.54
0.76
99.78
0.52
99.80
0.50
100.18
0.12
MSE: 0.3524
MSE: 0.1577
MSE: 0.1309
MSE: 0.0930
Fig. 4.1 Forecasting results by different FNN, row 1-real data, row 2-ANFIS, row 3-NFC
Tsukamoto with linear MF, row 4-NFC Tsukamoto with monotonous MF, row 5-NFC Mamdani
136
4
Application of Fuzzy Logic Systems and Fuzzy …

1. The use of NFC (neuro-fuzzy controller) Mamdani in predicting stock prices.
Using NFC Mamdani with triangular and Gaussian MF the following results
were obtained while predicting the stock price of “Lukoil”. They are presented
in the Table 4.3.
After that prediction using NFC Mamdani for triangular MF was performed. As
it was shown by the experiments, the best proved to be Mamdani controller with
Gaussian MF (MSE on the test sample of 14 points is only 0.03024, the average
relative error is 3.02 %).
2. Further experiments were conducted to predict with NFC Tsukamoto triangular
and Gaussian MF. Forecasting results for NFC Tsukamoto with Gaussian MF
are presented in Table 4.4.
Comparison of forecast errors for NFC Tsukamoto with triangular and
Gaussian MF are shown in Fig. 4.2.
As the second experiment has shown the best is controller Tsukamoto with
Gaussian MF (MSE on the test sample of 14 points is only 0.0667, and the
average relative error of the forecast is 6.67 %).
3. In the next experiment investigations of NFC with Sugeno inference algorithm
were performed. Prediction results using the NFC Sugeno for Gaussian MF are
presented in Table 4.5, and for triangular MF in a Table 4.6.
The third experiment has shown that the best is Sugeno controller with
Gaussian MF as in the two previous cases (MSE on the test sample of 14 points
0.0967).
4. Comparative analysis of stock prices forecasting by different methods.
Table 4.3 Forecasting results for NFC Mamdani with Gaussian MF
Data
Real values
The predicted value
Deviation
The square deviation
01.12.2005
58.1
58.23
0.13
0.0169
02.12.2005
58.7
58.54
0.16
0.0256
05.12.2005
59.4
59.14
0.26
0.0676
06.12.2005
59
59.11
0.11
0.0121
07.12.2005
59.85
59.97
0.12
0.0144
08.12.2005
59.6
59.416
0.184
0.033856
09.12.2005
59.9
60.12
0.22
0.0484
12.12.2005
60.65
60.5
0.15
0.0225
13.12.2005
60.65
60.54
0.11
0.0121
14.12.2005
61.15
61.32
0.17
0.0289
15.12.2005
60.25
60.1
0.15
0.0225
16.12.2005
61
61.2
0.2
0.04
19.12.2005
61.01
61.24
0.23
0.0529
20.12.2005
60.7
60.54
0.16
0.0256
MSE = 0.030239714
4.3
FNN Application for Forecasting in the Financial Sector
137

The results of forecasting by FNC Mamdani, Sugeno and Tsukamoto with
Gaussian MF and the NFC ANFIS are presented in the Fig. 4.3, and with the
triangular MF- in the Fig. 4.4 correspondingly.
As these results demonstrate [4], the best one proved to be Mamdani controller
with Gaussian MF. Its standard deviation is only 0.028236. Next by the quality of
the forecasting is Tsukamoto controller with Gaussian MF. It shows a slightly better
result than NFC Tsukamoto with triangular MF. But in general, their predictions are
very close (MSE = 0.0728 and MSE = 0.0817, respectively). This enables to
assume that the selection of more adequate type of membership functions may
allow to further improve the forecast results. Finally, at the last position are results
obtained using the NFC ANFIS (MSE = 0.34312). Its drawback lies in that this
FNN only MF of input rules are trained but not the rules outputs.
Table 4.4 Forecasting results for NFC Tsukamoto with Gaussian MF
Data
Real values
The predicted value
Deviation
The square deviation
01.12.2005
58.1
58.37
0.27
0.0729
02.12.2005
58.7
58.47
0.23
0.0529
05.12.2005
59.4
59.1
0.3
0.09
06.12.2005
59
59.25
0.25
0.0625
07.12.2005
59.85
60.19
0.34
0.1156
08.12.2005
59.6
59.37
0.23
0.0529
09.12.2005
59.9
60.27
0.37
0.1369
12.12.2005
60.65
60.48
0.17
0.0289
13.12.2005
60.65
60.42
0.23
0.0529
14.12.2005
61.15
61.4
0.25
0.0625
15.12.2005
60.25
60.06
0.19
0.0361
16.12.2005
61
61.22
0.22
0.0484
19.12.2005
61.01
61.28
0.27
0.0729
20.12.2005
60.7
60.48
0.22
0.0484
0.252857143
MSE = 0.0667
Fig. 4.2 Forecasting results for NFC Tsukamoto
138
4
Application of Fuzzy Logic Systems and Fuzzy …

Experimental studies of the prediction in macroeconomics and ﬁnancial sphere
with different classes of fuzzy neural networks enable to make the following
conclusions [1]:
1. Fuzzy Mamdani and Tsukamoto controllers coped with the task of forecasting
the test sample the best. This demonstrates the efﬁciency of the controllers
Table 4.5 Forecasting results for NFC Sugeno with Gaussian MF
Data
Real values
The predicted value
Deviation
The square deviation
01.12.2005
58.1
58.42
0.32
0.1024
02.12.2005
58.7
58.34
0.36
0.1296
05.12.2005
59.4
59.02
0.38
0.1444
06.12.2005
59
59.29
0.29
0.0841
07.12.2005
59.85
60.24
0.39
0.1521
08.12.2005
59.6
59.29
0.31
0.0961
09.12.2005
59.9
60.3
0.4
0.16
12.12.2005
60.65
60.41
0.24
0.0576
13.12.2005
60.65
60.4
0.25
0.0625
14.12.2005
61.15
61.43
0.28
0.0784
15.12.2005
60.25
60.04
0.21
0.0441
16.12.2005
61
61.25
0.25
0.0625
19.12.2005
61.01
61.31
0.3
0.09
20.12.2005
60.7
60.4
0.3
0.09
0.3057142
MSE = 0.0967
Table 4.6 Forecasting results for NFC Sugeno with trianmgular MF
Data
Real values
The predicted value
Deviation
The square deviation
01.12.2005
58.1
58.5
0.4
0.16
02.12.2005
58.7
58.31
0.39
0.1521
05.12.2005
59.4
59.01
0.39
0.1521
06.12.2005
59
59.33
0.33
0.1089
07.12.2005
59.85
60.3
0.45
0.2025
08.12.2005
59.6
59.16
0.44
0.1936
09.12.2005
59.9
60.39
0.49
0.2401
12.12.2005
60.65
60.31
0.34
0.1156
13.12.2005
60.65
60.38
0.27
0.0729
14.12.2005
61.15
61.49
0.34
0.1156
15.12.2005
60.25
59.94
0.31
0.0961
16.12.2005
61
61.3
0.3
0.09
19.12.2005
61.01
61.37
0.36
0.1296
20.12.2005
60.7
60.32
0.38
0.1444
0.370714286
MSE = 0.14096
4.3
FNN Application for Forecasting in the Financial Sector
139

training algorithm, as well as the high representativeness of the sample. As it
was shown by the ﬁrst test, the best proved to be Mamdani controller with
Gaussian membership functions and fuzzy intersection as a product (MSE on
the test sample is 0.09).
2. Analysis of the impact of the inference rules has revealed that the best results are
achieved with comprehensive number of rules, increasing the number of rules
forecasting error (MSE) ﬁrst falls (50 % of the total number of rules), attains
minimum and then begins to rise slightly, indicating existence of optimal
number of rules for the speciﬁc forecasting task.
3. A full set of rules should be used for not large number of variables (n = 4−8).
For more variables, it is advisable to use an incomplete rule base, which greatly
simpliﬁes the structure of the NFC and gives acceptable results in the accuracy
of forecasting.
4. In a whole experiments showed the great potential of NFC and conﬁrmed their
effectiveness in the tasks of macroeconomic and ﬁnancial forecasting.
5. Comparative analysis of the accuracy of prediction using NFC Mamdani,
Tsukamoto, Sugeno and ANFIS shown that FNN ANFIS has signiﬁcantly
poorer performance as compared with FNN Mamdani and Tsukamoto.
Fig. 4.3 Forecasting results
by FNC Mamdani, Sugeno
and Tsukamoto with Gaussian
MF
Fig. 4.4 Forecasting results
by FNC Mamdani, Sugeno
and Tsukamoto with
triangular MF
140
4
Application of Fuzzy Logic Systems and Fuzzy …

4.4
Predicting the Corporations Bankruptcy Risk Under
Uncertainty
Introduction
One of the actual problems related to strategic management is the analysis of
enterprise ﬁnancial state and assessment of the bankruptcy risk. Timely detection of
signs of possible bankruptcy allows managers to take urgent measures to improve
the ﬁnancial state and cut the risk of possible bankruptcy.
For many years, classical statistical methods were widely used to predict the risk
of bankruptcy. These methods also have the name of the one-dimensional
(‘single-period’) classiﬁcation techniques. They include the classiﬁcation proce-
dure, which classiﬁes a company to a group of potential bankrupts, or to a group of
companies with a favorable ﬁnancial position with a certain degree of accuracy.
Using these models, there may be two types of errors.
The ﬁrst type of error occurs when a future bankrupt company is classiﬁed as a
company with a favorable ﬁnancial situation. The error of the second type occurs
when an enterprise with a normal ﬁnancial condition is classiﬁed as a potential
bankrupt. Both mistakes can lead to serious consequences and losses. For example,
if a credit institution refuses to companies with “healthy” ﬁnancial situation to
provide credit (error of type 2), it can lead to the loss of future proﬁts of this
institution. This error is often called a “commercial risk”. Conversely, if the credit
institution makes a decision on granting a loan to a company that is a potential
bankrupt (error type 1), it can lead to the loss of interest on the loan, a signiﬁcant
part of the loan funds, opportunity cost, and other losses. Therefore, this error is
called “credit risk”.
Up to date there are several generally accepted methods and techniques of
bankruptcy risk assessment. The most famous and widely used is professor Altman
method (so-called Z-model) [5].
Z- model of Altman is a statistical model that is based on an assessment of
indicators of ﬁnancial state and solvency of the company that allows to estimate the
risk of bankruptcy and divide business corporations on potential bankrupts and non-
bankrupts. However, Altman’s model has a number of shortcomings, and its
application to the economy of Ukraine is associated with certain difﬁculties.
Therefore, in recent years, alternative approaches and methods were developed
which take into account the speciﬁcity of the analysis and decision- making under
uncertainty. These include the fuzzy sets theory and fuzzy neural networks
technique.
The purpose of this section is to review and analyze the efﬁciency of different
methods, both classical and computational intelligence methods for bankruptcy risk
assessment in relation to the economy of Ukraine.
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
141

4.4.1
Altman Model for Bankruptcy Risk Estimates
The most well-known bankruptcy risk assessment model is professor E. Altman’s
model [5]. Altman’s model is constructed using a multi-variable discriminant
analysis (MDA), which allows to choose such indicators, that the variance between
the groups would be the maximum and within the group be minimum. In this case,
the classiﬁcation is carried out for two groups of companies some of which later
went bankrupt, while others were able to survive and strengthen their ﬁnancial
positions.
As a result, a model by Altman (Z-score)was developed, which has the following
form:
Z ¼ 1:2K1 þ 1:4K2 þ 3:3K3 þ 0:6K4 þ 1:0K5
ð4:3:1Þ
where
K1 = own working capital/sum of assets;
K2 = retained earnings/sum of assets;
K3 = earnings before interest/sum of assets;
K4 = market value of equity/debt capital cost;
K5 = sales/sum of assets.
As a result of calculating the Z—score for a particular company, the following
inference is made:
if Z < 1,81—a very high probability of bankruptcy;
if 1,81  Z  2,7—high probability of bankruptcy;
if 2,7  Z  2,99—perhaps bankruptcy;
if Z  3,0—probability of bankruptcy is very low.
Altman’s model gives a fairly accurate prediction of the probability of bank-
ruptcy within a time interval of 1–2 years.
As a result of discriminant analysis application to a group of companies that
have declared bankruptcy later, using ﬁnancial indicators taken a year before the
possible default this fact was truly conﬁrmed in 31 cases of 33 (94.5 %), and in 2
cases was a mistake (6 %). In the second group of companies—non—bankrupts,
the model predicted a bankruptcy in only 1 case (error 3 %), while in the remaining
32 cases (97 %) was estimated very low probability of bankruptcy, that was con-
ﬁrmed in practice [5].
The corresponding results are given in Table 4.7.
Similar calculations were carried out based on the ﬁnancial indicators for the two
years prior to the possible bankruptcy. As it can be seen in Table 4.8, the results
were not good, especially for a group of companies that have declared the default,
whereas in group 2, the accuracy of the calculations remained approximately at the
same level. The overall accuracy of the classiﬁcation by Altman’s model was 95 %
for one year and 82 % for two years before the bankruptcy.
142
4
Application of Fuzzy Logic Systems and Fuzzy …

The above considered Z-score is applicable only for large companies, whose
shares are traded at the stock exchange. In 1985 E. Altman proposed a new model
that allows to correct this shortcoming. The formula for determining the probability
of bankruptcy prediction for companies whose shares are not represented on the
stock exchange is [5, 6]:
Z ¼ 0:717K1 þ 0:847K2 þ 3:107K3 þ 0:42K4 þ 0:995K5
ð4:3:2Þ
where K1—the value of own equity in relation to the borrowed capital.
If z < 1.23 then the risk of bankruptcy is very large.
Altman’s approach was repeatedly applied and extended by his followers in
many countries (UK, France, Brazil, China, Russia etc.).
An example of application of this model are the research results obtained by
Altman himself for 86 companies-bankrupts in the period 1969–1975 years, 110
companies-bankrupts in the period 1976–1995 years and 120 companies-bankrupts
in the period 1997–1999. Using a threshold value of 2,675, the accuracy of the
method proved to be in the range from 82 to 96 %.
In the last years the investigations were performed on application of Altman’s
model for CIS countries with transient economy. One of the most successful MDA
models was model developed by Davidova and Belikov for Russian economy.
Davidova—Belikov’s model takes the form [7]
R ¼ 8:38K1 þ K2 þ 0:054K3 þ 0:63K4
ð4:3:3Þ
where
K1—ratio own working capital/sum of assets;
K2—ratio return/own equity;
Table 4.7 Forecasting results of Altman’ model one year before possible bankruptcy
Group
Number of
companies
Forecasting:
belonging to group 1
Forecasting
belonging to group 2
Group 1 (the bankrupt
companies)
33
31 (94.0 %)
2 (6.0 %)
Group 2 (the non—
bankrupt companies)
33
1 (3.0 %)
32 (97.0 %)
Table 4.8 The forecasting results of Altman’s model (for two years before possible bankruptcy)
Group
Number of
companies
Prediction:
belonging to group
1
Prediction:
belonging to group
2
Group 1 (the bankrupt
company)
33
23 (72.0 %)
9 (28.0 %)
Group 2 (the non—
bankrupt companies)
33
2 (6.0 %)
31 (94.0 %)
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
143

K3—ratio total sales (cost of sales)/sum of assets; (turn-over coefﬁcient);
K4—ration net return/production costs.
If R < 0- the probability of bankruptcy is maximal (90–100 %); 0 < R < 0, 18
—the probability of bankruptcy is large (60–80 %); 0,18 < R < 0, 32—the prob-
ability of bankruptcy is average (35–50 %); 0, 32 < R < 0, 42—the probability of
bankruptcy is low (15–20 %); R < 0, 42 the probability of bankruptcy is minimal.
Application of MDA model is based on several assumptions [6].
The ﬁrst assumption is that the input data are dichotomous, i.e. groups are
disjoint sets. The other assumptions are the following:
• The independent variables included in the model are normally distributed;
• Variance-covariance matrices of a group of successful companies and bankrupt
group are equal;
• The cost of misclassiﬁcation and the a priori probability of failure are known.
In practice, all three of the above mentioned assumptions are rarely met,
therefore the use of MDA often happens to be inadequate and correctness of the
results obtained by its use, is questionable.
Weakness of Altman’s model lies is therein this model is purely empirical, ﬁtted
on a sample, and has no theoretical basis. In addition, the model coefﬁcients should
be determined for various industries, and will naturally vary.
In Ukrainian economy Altman’s model was not widely used for the following
reasons:
1. It requires the calculation of the relevant coefﬁcients at the terms Ki; i ¼ 1; 5,
which, of course, differ from their values for foreign countries;
2. information about the ﬁnancial condition of the analyzed companies is generally
not trustworthy as managers of many companies often “correct” economic
indices in their ﬁnancial accounts, making it impossible to ﬁnd reliable estimates
of the coefﬁcients in Z- model.
Therefore, the problem of estimating the probability of the bankruptcy risk
should be solved in the conditions of uncertainty and incompleteness of initial
information, and for its solution should be proposed to use an adequate approach:
methods of computational intelligence—fuzzy sets and fuzzy neural networks
(FNN).
4.4.2
Forecasting the Corporation Bankruptcy Risk
on the Basis of the Fuzzy Sets Theory
One of the ﬁrst methods of the corporations bankruptcy risk forecasting under
uncertainty based on fuzzy sets was suggested by O.A Nedosekin known as matrix
method [8, 9]. Consider its main ideas
144
4
Application of Fuzzy Logic Systems and Fuzzy …

1. Expert builds linguistic variable with its term-set values. For example, “The
level of management” can have the following term set of values “ very low, low,
medium, high, very high.”
2. In order to constructively describe linguistic variable, the expert selects the
appropriate quantitative feature—for example, a specially designed indicator of
the level of management, which takes values from zero to one.
3. Further to each value of linguistic variable which is a fuzzy subset of on the
interval [0,1], the expert assigns a corresponding membership function of a
fuzzy set. As a rule, it’s a trapezoidal or triangular membership function. The
upper base of the trapezoid corresponds to the full assurance of the correctness
of the expert classiﬁcation, and the lower- assurance that no other values of the
interval [0,1] belong to the selected fuzzy set (see Fig. 4.5).
The matrix method comprises the following steps
Step 1 (Linguistic variables and fuzzy sets)
a. Linguistic variable E “State enterprise” has ﬁve values
E1—fuzzy subset of states “Limit level of trouble”;
E2—fuzzy subset of states “ high level of trouble”;
E3—fuzzy subset of states “average level”;
E4—fuzzy subset of states “relative prosperity”;
E5—fuzzy subset of states, “the ultimate well-being.”
b. The corresponding to the variable E linguistic variable G “Bankruptcy risk”
also has 5 values:
G1—fuzzy subset of states, “the ultimate risk of bankruptcy”
G2—fuzzy subset of states “high risk of bankruptcy”
G3—fuzzy subset of states “average risk of bankruptcy”
G4—fuzzy subset of states “low risk of bankruptcy”
G5—fuzzy subset of states “negligible risk of bankruptcy.”
Carrier of fuzzy set G—indicator of bankruptcy risk g—takes values from zero to
one by deﬁnition.
Fig. 4.5 Trapezoidal
membership function
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
145

c. For certain ﬁnancial and management indicator Xi assign the linguistic variable Bi
“level of indicator Xi“ in the following term set of values:
Bi1—a subset of the “very low level of indicator Xi”
Bi2—subset of “low level of indicator Xi“
Bi3—a subset of the “average level of indicator Xi”
Bi4—a subset of “high level of indicator Xi“
Bi5—subset of “very high level of indicator Xi“.
Step 2 (Indicators). We construct a set of individual indicators X ¼ fXig total
number N, which according to the expert, on the one hand affect the assessment of
the bankruptcy risk, and on the other hand, reﬂect the different parts of the enter-
prise business and ﬁnancial activity. For example, in the matrix method such
indicators are used [8]:
• X1—autonomy coefﬁcient (the ratio of equity to the balance sheet);
• X2—ratio of current assets to ensure their own needs (the ratio of net working
capital to current assets);
• X3—intermediate liquidity ratio (the ratio of cash and receivables to short-term
liabilities);
• X4—absolute liquidity ratio (the ratio of cash to short-term liabilities);
• X5—the turnover of all assets for the year (the ratio of revenue from sales to the
average the value of assets in the year period;
• X6—Return on total capital (the ratio of net proﬁt to the average assets value for
the period of year).
Step 3 (Signiﬁcant indicators). For each indicator Xi its signiﬁcance ri is
determined. To evaluate this level, it is necessary to range all indicators in order of
decrease their signiﬁcance according to the relation:
r1  r2  . . .  rN:
If the system indicators are ranked in decreasing order of importance, the weight
of the ith indicator should be determined by the Fishburn’s rule:
ri ¼ 2ðN  i þ 1Þ
ðN þ 1ÞN
:
ð4:3:4Þ
If all indicators have the same weight, then ri ¼ 1=N.
Step 4 (Classiﬁcation of risk). A classiﬁcation of the current values of the g
indicator of risk as a criterion for the partition of this set into fuzzy subsets
(Table 4.9).
Step 5 (Classiﬁcation of indicator values). We construct a classiﬁcation of
current values of parameters X as the split criteria to the full set of values into the
fuzzy subset of Bi. One example of such a classiﬁcation is given below in
Table 4.10. The table cells are trapezoidal fuzzy numbers that characterize the
respective membership functions.
146
4
Application of Fuzzy Logic Systems and Fuzzy …

Step 6 (Assessment of indicators level). We will assess the current level of
indicator and put the results in the Table 4.11.
Step 7 (Classiﬁcation level indicators). We carry out a classiﬁcation of the
current values of indicator x using the criterion in table constructed in step 5
(Table 4.10). The result of the classiﬁcation is a table of values kij  ði; jÞ—levels
of membership of xi to fuzzy subsets vj.
Step 8 (Risk Assessment). We perform computing operations to assess the
degree of bankruptcy risk g:
g ¼
X
5
j¼1
gj
X
N
i¼1
rikij;
ð4:3:5Þ
where gj ¼ 0:9  0:2  ðj  1Þ;
ð4:3:6Þ
The sense of formulas (4.3.5) and (4.3.6) is as follows. First, we estimate the
weight of a subset of B in the assessment of the corporation state E and risk
assessment G. These weights participate in the external sum to determine the
average value of the indicator g, where gj is nothing but the average estimate g of
the appropriate range of the Table 4.9 at Step 4.
Step 9 (linguistic recognition). The resulting risk value is classiﬁed on the
database of Table 4.9. The results of the classiﬁcation are the linguistic description
of the bankruptcy risk and the expert level of conﬁdence in the correctness of his
classiﬁcation (Table 4.10).
Table 4.9 The classiﬁcation of risk
The range of
values g
The classiﬁcation level of
the parameter
The degree of conﬁdence (membership
function value)
0  g  0.15
G5
1
0.15 < g < 0.25
G5
l5 = 10  (0.25-g)
G4
1 −l5 = l4
0.25  g  0.35
G4
1
0.35 < g < 0.45
G4
l4 = 10  (0.45-g)
G3
1 −l4 = l3
0.45  g  0.55
G3
1
0.55 < g < 0.65
G3
l3 = 10  (0.65-g)
G2
1 −l3 = l2
0.65  g  0.75
G2
1
0.75 < g < 0.85
G2
l2 = 10  (0.85-g)
G1
1 −l2 = l1
0.85  g  1.0
G1
1
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
147

Table 4.10 Classiﬁcation of certain ﬁnancial indicators
Index
T-number{c} for the value of linguistic variable
“The parameter”:
“very low”
“low”
“average”
“high”
“very high”
X1
(0,0,0.1,0.2)
(0.1,0.2,0.25,0.3)
(0.25,0.3,0.45,0.5)
(0.45,0.5,0.6,0.7)
(0.6,0.7,1,1)
X2
(−1,−1,−0.005,0)
(−0.005,0,0.09,0.11)
(0.09,0.11,0.3,0.35)
(0.3,0.35,0.45,0.5)
(0.45,0.5,1,1)
X3
(0,0,0.5,0.6)
(0.5,0.6,0.7,0.8)
(0.7,0.8,0.9,1)
(0.9,1,1.3,1.5)
(1.3,1.5,∞, ∞)
X4
(0,0,0.02,0.03)
(0.02,0.03,0.08,0.1)
(0.08,0.1,0.3,0.35)
(0.3,0.35,0.5,0.6)
(0.5,0.6,∞, ∞)
X5
(0,0,0.12,0.14)
(0.12,0.14,0.18,0.2)
(0.18,0.2,0.3,0.4)
(0.3,0.4,0.5,0.8)
(0.5,0.8,∞, ∞)
X6
(-∞, -∞,0,0)
(0,0,0.006,0.01)
(0.006,0.01,0.06, 0.1)
(0.06,0.1,0.225, 0.4)
(0.225,0.4,∞,)
148
4
Application of Fuzzy Logic Systems and Fuzzy …

The main advantages of fuzzy matrix method are as follows:
1. the possibility of using not only the quantitative but qualitative factors;
2. taking into account inaccurate fuzzy information on the approximate values of
the factors.
4.4.3
The Application of Fuzzy Neural Network
in Bankruptcy Risk Prediction Problem
Now consider the application of computational intelligence technologies -FNN of
Mamdani and Tsukamoto for the same problem of predicting the risk of bank-
ruptcy. The method comprises the following steps [5, 6, 10].
Stage 1 (Linguistic variables and fuzzy subsets).
Like fuzzy-matrix method deﬁne the fuzzy sets E, G, B.
Step 2 (Performance). Building a set of individual indicators X ¼ fXig (the total
number N), which at the discretion of the expert-analyst affect the assessment of the
bankruptcy risk of enterprises and evaluate different aspects of the business and
ﬁnancial activity. We choose a system of six indicators, the same ones as in
fuzzy-set matrix method
Step 3 (Building the rule base of fuzzy inference system).
An expert builds rules base on the subject area as a set of fuzzy rules of the
predicate type:
Table 4.11 The current level
of performance
Index
Present value
X1
x1
–
–
Xi
xi
–
–
XN
xN
Table 4.12 The results of
the forecast by Altman model
for “enterprises—bankrupts”
Group
Number of
companies
Forecast bankruptcy risk
High
Middle
Low
Two years
before
bankruptcy
26
35 % (9)
27 % (7)
38 % (10)
The year
before
bankruptcy
26
42 % (11)
35 % (9)
23 % (6)
Average for
two years
26
38 %
31 %
31 %
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
149

P1 : if x 2 A1 and y 2 B1 then z 2 C1
P2 : if x 2 A2 and y 2 B2 then z 2 C2
Introduce the following linguistic variables for the implementation of fuzzy
inference algorithms of Mamdani and Tsukamoto.
• X1: (Very Low (VL), Low (L), Medium (M), High (H), Very High (VH));
• X2: (Very Low, Low, Medium, High, Very High);
• X3: (Very Low, Low, Medium, High, Very High);
• X4: (Very Low, Low, Medium, High, Very High);
• X5: (Very Low, Low, Medium, High, Very High);
• X6: (Very Low, Low, Medium, High, Very High);
Set the following levels of bankruptcy: (Very Low, Low, Medium, High, Very
High).
For simplicity, introduce the following abbreviations:
Very Low—(VL), Low—(L), Medium—(M), High—(H), very high—(VH).
Then we can write the following rules, taking into account all the possible
combinations:
• If X1 is “VL” and X2 is “VL” and X3 is “VL” and X4 is “VL” and X5
is “VL” and X6 is “VL” then bankruptcy risk is “ VH”;
• If X1—“M” and X2—“L” and X3—“VL” and X4—“VL” and X5—“VL” and
X6—“VL” then bankruptcy risk—“VH”;
• If
X1 “M” and
X2 “M” and
X3 “L” and
X4 “VL” and
X5 “VL” and
X6 “VL” then bankruptcy risk “VH”;
• If
X1 “M” and
X2 “M” and
X3 “M” and
X4 “L” and
X5 “VL” and
X6 “VL” then bankruptcy risk “H”;
• If X1 “H” and X2 “M” and X3 “L” and X4 “VL” and X5 “L” and X6 “VL” then
bankruptcy risk “L”;
• If X1 “H” and X2 “H” and X3 “H” and X4 “M” and X5 “M” and X6 “H” then
bankruptcy risk “M”;
• If
X1 “VH” and
X2 “VH” and
X3 “H” and
X4 “VH” and
X5 “H” and
X6 “VH” then bankruptcy risk “L”;
• If
X1 “VH” and
X2 “VH” and
X3 “VH” and
X4 “VH” and
X5 “VH” and
X6 “VH”, then bankruptcy risk “VL”;
Since the total number of rules is very high, if we consider all possible options
permutations values, to facilitate perception and brevity of the rules base introduce
scores for linguistic values.
VL ¼ 5;
L ¼ 4;
M ¼ 3;
H ¼ 2;
VH ¼ 1:
150
4
Application of Fuzzy Logic Systems and Fuzzy …

We calculate the marginal performance level of bankruptcy, using the following
boundary rules:
1. If
X1 “VL” and
X2 “VL” and
X3 “VL” and
X4 “VL” and
X5 “VL” and
X6 “VL” then the SCORE = 30;
2. If X1 “L” and X2 “L” and X3 “L” and X4 “L” and X5 “L” and X6 “L” then the
SCORE = 24;
3. If X1 “M” and X2 “M” and X3 “M” and X4 “M” and X5 “M” and X6 “M” then
the SCORE = 18;
4. If X1 “H” and X2 “H” and X3 “H” and X4 “H” and X5 “H” and X6 “H” then
the SCORE = 12;
5. If
X1 “VH” and
X2 “VH” and
X3 “VH” and
X4 “VH” and
X5 “VH” and
X6 “VH” then the SCORE = 6;
Then the new rules to assess the risk of bankruptcy can be written as follows:
If SCORE > 24, the bankruptcy level is VH;
If SCORE  24 and > 18, the bankruptcy level is H;
If SCORE  18 and BAL R > 12, the bankruptcy level is M;
If SCORE  12 and > 6, the bankruptcy level is L;
If the SCORE = 6, the bankruptcy level is VL.
This approach allows us to cover the whole set of rules.
Step 4 (fuzziﬁcation of input parameters). We perform fuzziﬁcation of the
input parameters, or a description of each term set (linguistic variables) using the
membership functions, and ﬁnd the degree of truth for each value in the rules: A1
(x0), A2 (x0), B1 (y0), B2 (y0).
As the membership functions use the triangular functions. For greater clarity of
the membership functions represent them graphically in Fig. 4.6 and assign to them
appropriate actual values.
Step 5 (Inference). Find levels of “cut-off” for each of the rule antecedents using
operation min.
a1 ¼ A11ðx10Þ ^ A21ðx20Þ ^ A31ðx30Þ ^ A41ðx40Þ ^ A51ðx50Þ ^ A61ðx60Þ;
ai ¼ A1iðx10Þ ^ A2iðx20Þ ^ A3iðx30Þ ^ A4iðx40Þ ^ A5iðx50Þ ^ A6iðx60Þ:
And determine “truncated” output membership functions:
C0
1 ¼ ða1 ^ C1ðzÞÞ; C0
i ¼ ðai ^ CiðzÞÞ:
Step 6 (Composition). Perform composition of found output membership
functions using operation max, resulting in a fuzzy subset of the ﬁnal output
variable with the membership function l (z).
Step 7 (Reduction to the crisp value (defuzziﬁcation). Perform the fuzziﬁ-
cation using centroid method
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
151

w0 ¼
R
X
w  lRðwÞdw
R
X
lRðwÞdw
Experimental Studies
In order to analyze the different methods of forecasting the bankruptcy risk a
software package was developed, which implements the classical methods of dis-
criminant analysis by Altman, Davydova—Belikov, Nedosekin’s matrix method
and fuzzy neural networks of Mamdani and Tsukamoto [6, 10].
Using the developed software the experimental investigations were conducted
for predicting bankruptcy of ﬁfty-two enterprises in Ukraine, 26 of which were
declared bankrupt by the arbitral court in 2011.
The input data for the forecasting were ﬁnancial indicators, which were calcu-
lated based on data from accounting statements (balance sheet and income state-
ment) in 2009 and 2010 years. Forecasting was conducted using models of Altman,
the matrix method of Nedosekin, Mamdani and Tsukamoto FNNs. Analysis was
based only on quantitative indicators the same as in the matrix method of
Nedosekin.
Deﬁne the accuracy level of predicting the bankruptcy risk during the considered
period. So Altman model has an accuracy of correct prediction 0.69 for companies
that are potential bankrupts (see Fig. 4.7).
For the second group of companies, which we call “workable”, the following
results were obtained. During the two reporting years prior to the current status, 22
companies are recognized as the company with the “low” level of bankruptcy, 2
companies—“medium” level and 2 companies received the status of a “high” level
of bankruptcy. During one ﬁscal year prior to the present state 18 enterprises
received the status the “low” level of bankruptcy, 5 enterprises—“medium” level of
bankruptcy, three companies—“high” level of bankruptcy (Table 4.13).
Deﬁne the accuracy level of risk prediction for this group of enterprises during
the considered period. So the average prediction accuracy for enterprises—not
bankrupts equals 0.77 (Fig. 4.8).
We generalized these results, and found the average level of forecasting accuracy
for each ﬁscal year, and in the two years together (Table 4.14). After analyzing the
results, one can say that Altman approach is not always able to determine correctly
Fig. 4.6 Membership functions for the variable x1
152
4
Application of Fuzzy Logic Systems and Fuzzy …

ﬁnancial state, especially if you need to detect bankruptcy at an early stage of
ﬁnancial instability. Also, the approach Altman does not allow to trace the
dynamics of enterprise development, i.e. to conduct a ﬁnancial analysis of a
company that to take early measures to prevent bankruptcy.
Fig. 4.7 The level of
accuracy of forecasting
bankrupt enterprises
Table 4.13 The results of the forecast by Altman model for “enterprise—not bankrupts”
Group
Number of companies
Forecast bankruptcy (levels)
High
Middle
Low
In 2009 ﬁscal year
26
7,5 % (2)
7.5 % (2)
85 % (22)
In 2010 ﬁscal year
26
12 % (3)
19 % (5)
69 % (18)
Average for two years
26
10 %
13 %
77 %
Fig. 4.8 The level of
accuracy of forecasting the
state of the
enterprises-non-bankruptcs
Table 4.14 Generalized forecast accuracy of enterprise bankruptcy by Altman’s model
Group
Number of companies
Forecast (%)
True
Error
Two years before bankruptcy
52
77
23
The year before bankruptcy
52
73
27
Average
52
75
25
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
153

Further we analyzed the ﬁnancial state of the same two groups of companies that
were analyzed by Altman’s model using fuzzy-set Nedosekin’s method. In result of
26 bankrupt companies in the two years before the default 20 enterprises were
identiﬁed “very high”, “high” or “medium” level of bankruptcy risk, 6 enterprises
have the status of companies with “low” and “very low” the level of bankruptcy. In
the year before the default 22 enterprises—with a “very high”, “high” or “medium”
level of bankruptcy, 4 enterprises—“low” or “very low” level of bankruptcy [10]
(Table 4.15).
Deﬁne the accuracy level of predicting the risk of bankruptcy during the studied
period. So Nedosekin’s approach gives a correct prediction for companies that are
potential bankrupts with an average accuracy of 0.81 (see Fig. 4.9).
For the second group of enterprises—”workable”, the following results using
Nedosekin’s method were obtained [10]. During the two reporting years prior to the
current status, 21 companies were recognized as the company with a “very low”,
“low” or “medium” level of bankruptcy, ﬁve companies obtained the status of
“high” or “very high” level of bankruptcy. During one ﬁscal year before the present
date 20 enterprises were recognized with “very low”, “low” or “medium” level of
bankruptcy,
six
companies—“high”
or
“very
high”
level
of
bankruptcy
(Table 4.16).
Deﬁne the accuracy level of predicting the bankruptcy risk for working enter-
prises during the survey period. As one can see, the Nedosekin’s model with an
average accuracy of 0.79 gives a correct prediction for enterprises not potential
bankrupts (See Fig. 4.10).
Summarizing the results, ﬁnd the average value of the prediction accuracy for
each year and for the two years together (Table 4.17).
Table 4.15 The results of the forecast by Nedosekin model for “enterprise-bankrupt”
Group
Number of companies
Forecast
Bankrupt
Non-bankrupt
Two years before bankruptcy
26
77 % (20)
23 % (6)
The year before bankruptcy
26
85 % (22)
15 % (4)
Average for two years
26
81 %
19 %
Fig. 4.9 The level of
accuracy of forecasting state
of enterprises-bankrupt)
154
4
Application of Fuzzy Logic Systems and Fuzzy …

Analyzing these results, we can say that Nedosekin’s approach allows not only
to determine the ﬁnancial state of enterprises, but to deﬁne it more precisely thanks
to the linguistic scale consisting of ﬁve grade levels of bankruptcy, “VL, L, M, H,
VH”. In addition, Nedosekin’s approach makes it possible to trace the dynamics
development of the enterprise, i.e. allows to conduct a ﬁnancial analysis of a
company for the previous and current reporting periods and determine the level of
bankruptcy at the “initial” stage, allowing early to take measures to prevent possible
bankruptcy.
We analyzed the ﬁnancial state of the same two groups of companies by
Mamdani approach [10]. After analysis of the companies ‘bankrupt’ in two years
before the default among the 26 companies 22 companies were identiﬁed with a
“very high”, “high” or “medium” level of risk of bankruptcy, four companies
obtained the status of companies with “low” and “very low” level of bankruptcy. In
one year before the default 25 enterprises were identiﬁed with a “very high”, “high”
or “medium” level of bankruptcy, one company—“low” or “very low” level of
bankruptcy (Table 4.18).
Table 4.16 The results of the forecast by Nedosekin model for “enterprises-not bankrupt”
Group
Number of companies
Forecast
Bankrupt
Not-bankrupt
In 2009 ﬁscal year
26
19 % (5)
81 % (21)
In 2010 ﬁscal year
26
23 % (6)
77 % (20)
Average for two years
26
21 %
79 %
Fig. 4.10 The accuracy level
of forecasting for the
workable enterprises by
Nedosekin model
Table 4.17 The overall accuracy of enterprise bankruptcy risk prediction by Nedosekin’s model
Group
Number of companies
Forecast (%)
True
Error
During the two reporting years
52
79
21
During one ﬁscal year
52
81
19
Average
52
80
20
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
155

Determine the average prediction accuracy level of bankruptcy risk using
Mamdani approach (Fig. 4.11).
For the second group of enterprises so-called “workable”, the following results
using Mamdani model were obtained. In 2009 ﬁscal year, 23 companies were
recognized as the company with a “very low”, “low” or “medium” level of
bankruptcy, 3 companies received the status of “high” or “very high” level of
bankruptcy. In 2010, the state of some enterprises has deteriorated, they went a step
below, but the overall picture remains the same (Table 4.19).
The average prediction accuracy of bankruptcy risk level for working enterprizes
using Mamdani approach is presented in the Fig. 4.12 [10].
After analyzing the obtained results, the average prediction accuracy for all
enterprises is presented in Table 4.20. As one can readily see Mamdani approach is
similar to Nedosekin approach. It allows not only to determine the ﬁnancial state of
enterprises, but also to deﬁne it more precisely thanks to the linguistic scale con-
sisting of ﬁve grade levels of bankruptcy: “VL, L, M, H, VH”.
Mamdani’s approach enables to monitor the development dynamics of the
enterprise, i.e. allows to conduct a ﬁnancial analysis of a company for the previous
and current reporting periods and to determine the level of bankruptcy risk at the
Table 4.18 The results of the forecast by Mamdani’s model for “enterprises—bankrupt”
Group
Number of companies
Forecast
Bankrupt
Not bankrupt
Two years before bankruptcy
26
85 % (22)
15 % (4)
The year before bankruptcy
26
96 % (25)
4 % (1)
Average for two years
26
90 %
10 %
Fig. 4.11 The level of
accuracy of forecasting
bankrupt enterprises
Table 4.19 The results of the forecast by Mamdani model for “enterprises—not bankrupts”
Group
Number of companies
Forecast
Bankrupt
Do not bankrupt
Two years before bankruptcy
26
12 % (3)
88 % (23)
The year before bankruptcy
26
12 % (3)
88 % (23)
Average for two years
26
12 %
88 %
156
4
Application of Fuzzy Logic Systems and Fuzzy …

“initial” stage. Therefore Mamdani method allows to make more comprehensive
analysis of the enterprises than the previously considered two methods.
After analyzing the ﬁnancial state of the same companies by Tsukamoto model,
the following results were received [10] (Tables 4.21, 4.22, 4.23).
As can be seen from the tables Tsukamoto approach also enables to analyze the
ﬁnancial state of a company at any stage and with a fairly high percentage of
probability to detect its critical state at an early stage.
We generalized the results for companies ‘bankrupt’, using approaches by
Altman Nedosekin, Mamdani, Tsukamoto, and present them below in Table 4.24
and in Fig. 4.14.
Summarizing results obtained for all the companies, using approaches Altman
Nedosekin, Mamdani, Tsukamoto are presented in the Table 4.25 and in Fig. 4.15
[10].
As one can see, the method of Altman, in presented investigations, correctly
predicted the state of enterprises by an average of 69 %, the method of Nedosekin
Fig. 4.12 The level of
forecasting accuracy for the
working enterprise by
Mamdani model
Fig. 4.13 The level of
accuracy of forecasting the
state of the enterprise
bankrupt by Tsukamoto’s
model
Table 4.20 The average accuracy of the forecast of enterprise ﬁnancial state by Mamdani model
Group
Number of companies
Forecast (%)
True
Error
During the two reporting years
52
89.42
10.58
During one ﬁscal year
52
92.31
7.69
Average
52
87.50
12.50
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
157

correctly predicted by an average of 81 %, and Tsukamoto Mamdani FNN gave
similar results, the forecast was carried out at 90 % correctly. Errors in forecasting
state enterprises may be caused by several reasons. Firstly, we do not take into
account certain social interest of the company or certain individuals in the elimi-
nation of the working enterprise. Second, we do not consider the existence of an
interest in bankrupt enterprises or false bankruptcy. But we can surely say that
Nedosekin fuzzy-set method and fuzzy neural networks of Mamdani and
Tsukamoto allow to detect trends in the development of enterprises and identify the
threat of possible bankruptcy in the early stages.
Table 4.21 The results of the forecast by Tsukamoto model for “enterprises—bankrupts”
Group
Number of companies
Forecast
Bankrupt
Do not bankrupt
Two years before bankruptcy
26
22 (85 %)
4 (15 %)
One year before bankruptcy
26
24 (92 %)
2 (8 %)
Average for two years
26
88 %
12 %
Table 4.22 The results of the forecast by Tsukamoto model for “enterprises—not bankrupts”
Group
Number of companies
Forecast
Bankrupt
Do not bankrupt
Two years before bankruptcy
26
12 % (3)
88 % (23)
The year before bankruptcy
26
15 % (4)
85 % (22)
Average for two years
26
13 %
87 %
Table 4.23 Average forecasting accuracy of bankruptcy risk by Tsukamoto model
Group
Number of companies
Forecast (%)
True
Error
During the two reporting years
52
87
13
During one ﬁscal year
52
88
12
Average
52
87
13
Table 4.24 Forecasting
results for
enterprises-bankrupts by
Altman, Nedosekin,
Mamdani, Tsukamoto models
Method
Number of
companies
Forecast
Right
(%)
Not right
(%)
Altman
26
69.2
30.8
Nedosekin
80.8
19.2
FNN
Mamdani
90.4
9.6
FNN
Tsukamoto
88.5
11.5
158
4
Application of Fuzzy Logic Systems and Fuzzy …

Conclusions
In this section methods of ﬁnancial state analysis and forecasting bankruptcy risk
are considered and analized: the classical method of Altman, fuzzy-set method of
Nedosekin and methods based on the application of fuzzy neural networks with the
inference of Mamdani and Tsukamoto.
Using the developed software package the experimental investigations were
carried out predicting bankruptcy risk for 52 enterprises in Ukraine, among which
26 enterprises were potential bankrupts, and other 26 companies were solvent, (i.e.
the level of bankruptcy is “low”, “very low”, “medium”). Among the 26 companies
—potential bankrupts 24 companies were declared bankrupt on 01.02.2011 by a
court decision. Among the 26 enterprises classiﬁed as having “average” level of
Fig. 4.14 Accuracy of
bankruptcy risk prediction of
bankrupt enterprises by
different methods
Table 4.25 Forecasting
results by Altman, Nedosekin.
Mamdani, Tsukamoto models
for all enterprises
Method
Number of
companies
Forecast
Right
(%)
Right
(%)
Altman
52
73
27
Nedosekin
80
20
Mamdani
89.4
10.6
Tsukamoto
88
13
Fig. 4.15 Forecasting results
of the enterprises bankruptcy
risk by different methods
4.4
Predicting the Corporations Bankruptcy Risk Under Uncertainty
159

bankruptcy 4 enterprises were recognized bankrupts, and 6 companies were reor-
ganized in additional liability and limited liability companies.
Note that in result of the comparative analysis the highest accuracy of bank-
ruptcy prediction have shown computational intelligence methods -FNN of
Mamdani and Tsukamoto (90 %), followed by fuzzy-set method of Nedosekin
(80 %) and ﬁnally, the worst performance prediction accuracy has shown the
classic Altman method of discriminant analysis.
In addition, fuzzy neural networks of Mamdani, Tsukamoto enable to detect the
risk of bankruptcy in the early stages, especially Mamdani approach, as well as to
analyze the development of the company, taking the into account reports for few
quarters or years. Besides, to the contrary to Nedosekin method FNN allow to
utilize expert knowledge in the form of fuzzy inference rules (knowledge base).
4.5
Banks Financial State Analysis and Bankruptcy
Forecasting
Introduction
In a modern economy problem of banks ﬁnancial state analysis and possible
bankruptcy forecasting plays the signiﬁcant role. The timely detection of features of
coming bankruptcy enables top bank managers to adopt urgent measures for sta-
bilizing ﬁnancial bank state and prevent the bankruptcy. Up to the date there are a
lot of methods and techniques of banks state analysis and determination of bank
rating- WEBMoney, CAMEL, Moody’s S&P etc. But their common drawback is
that all of them work with complete and reliable data and cannot give correct results
in case of incomplete and unreliable input data. This is especially important for
Ukrainian bank system where bank managers often provide the incorrect infor-
mation about their ﬁnancial indices that to obtain new credits and loans.
Due to aforesaid it’s very important to develop new approaches and technique
for banks bankruptcy risk forecasting under uncertainty. The main goal of present
investigation is to develop and estimate novel methods of bank ﬁnancial state
analysis and bankruptcy forecasting under uncertainty and compare with classical
methods.
As it is well known 2008 year was the crucial year for bank system of Ukraine.
If the ﬁrst three quarters were periods of fast grow and expansion the last quarter
became the period of collapse in the ﬁnancial sphere. A lot of Ukrainian banks
faced the danger of coming default.
For this research the quarterly accountancy bank reports were used obtained
from National bank of Ukraine site. For analysis the ﬁnancial indices of 170
Ukrainian banks were taken up to the date 01.01. 2008 and 01.07.2009, that is
about two years before crises and just before the start of crises.
The important problem which arouse before start of the investigations is which
ﬁnancial indices are to be used for better forecasting of possible bankruptcy. Thus,
160
4
Application of Fuzzy Logic Systems and Fuzzy …

another goal of this exploration was to determine the most adequate ﬁnancial
indices for attaining maximal accuracy of forecasting.
For analysis the following indices of banks accountancy were taken:
• Assets; Capital; ﬁnancial means and their equivalents; Physical persons entities;
• juridical persons entities; liabilities; Net incomes (losses).
The collected indices were utilized for application by fuzzy neural networks as
well as classic crisp methods—method by Kromonov and method developed by
association of Byelorussian banks As output data of models for Ukrainian banks
were two values:
• 1, if the signiﬁcant worsening of bank ﬁnancial state is not expected in the
nearest future;
• −1, if the bank bankruptcy or introduction of temporary administration in bank
is expected in the nearest future.
4.5.1
The Application of Fuzzy Neural Networks
for Financial State Forecasting
For forecasting of banks bankruptcy risk it was suggested the application of fuzzy
neural networks (FNN) ANFIS and TSK considered in the Chap. 3. The appli-
cation of FNN is determined by following advantages:
1. the possibility to work with incomplete and unreliable information under
uncertainty;
2. the possibility to use expert information in the form of fuzzy inference rules.
A special software kit was developed for FNN application in forecasting prob-
lems enabling to perform Ukrainian banks ﬁnancial state analysis using ANFIS and
TSK.
As input data the ﬁnancial indices of Ukrainian banks in ﬁnancial accountant
reports were used in the period of 2008–2009 years [11]. As the output values were
+1, for bank-non-bankrupt and −1—for bank-bankrupt. In the investigations dif-
ferent ﬁnancial indices were used, different number of rules for FNN and the
analysis of data collection period inﬂuence on forecasting accuracy was performed.
Comparative Analysis of FNN ANFIS and TSK
The results of experimental investigations of FNN application for banks ﬁnancial
state analysis are presented below. In the ﬁrst series of experiments input data at the
period of January 2008 were used (that is for two years before possible bankruptcy)
and possible banks bankruptcy was analyzed at the beginning of 2010 year.
4.5
Banks Financial State Analysis and Bankruptcy Forecasting
161

Experiment №1
• Training sample—120 Ukrainian banks,
• Test sample—50 banks;
• Number of rules = 5.
• Input data—ﬁnancial indices (taken from bank accountant reports):
– Assets;
– Capital;
– Cash (liquid assets);
– Households deposits;
– Liabilities.
The results of application of FNN TSK are presented in Table 4.26.
Experiment №2
• Training sample—120 Ukrainian banks;
• test sample—50 banks; number of rules = 5.
• Input data—ﬁnancial indices the same as in the experiment 1.
The results of application FNN ANFIS are presented in the Table 4.27.
As one can see comparing results of Table 4.26 and 4.27, neural network TSK
gives more accurate results than FNN ANFIS.
Experiment №3 The next experiment was aimed at the detecting of inﬂuence of
rules number on the forecasting results.
• Training sample—120 Ukrainian banks;
• test sample—50 banks;
• number of rules = 10.
• Input data—the same ﬁnancial indices as in experiment 1.
The results of application of FNN TSK are presented in the Table 4.28.
Table 4.26 Results of
FNN TSK forecasting
Results
Total amount of errors
5
%% of errors
10 %
First type of errors
0
Second type of errors
5
Table 4.27 Results of
FNN ANFIS forecasting
Results
Total amount of errors
6
%% of errors
12 %
First type of errors
0
Second type of errors
6
162
4
Application of Fuzzy Logic Systems and Fuzzy …

The similar experiments were carried out with FNN ANFIS
Experiment №4 The next experiment was aimed at the detecting of inﬂuence of
rules number on the forecasting results.
• Training sample—120 Ukrainian banks,
• Test sample—50 banks.
• Number of rules = 10.
• Input data—ﬁnancial indices the same as in experiment 1.
The results of application of FNN ANFIS are presented in the Table 4.29.
The comparative analysis of forecasting results versus the number of rules is
presented in the Table 4.30.
The next experiments were carried out aimed at investigation of inﬂuence of
training and test samples size on accuracy of forecasting.
Experiment №5
• Training sample—120 Ukrainian banks;
• Test sample—50 banks;
• Number of rules = 10.
• Input data—ﬁnancial indices:
– Assets; Entity;
– Cash (liquid assets);
– Households deposits;
– Liabilities.
Table 4.28 Results of
FNN TSK forecasting
Results:
Total amount of errors
6
%% of errors
12 %
First type of errors
1
Second type of errors
5
Table 4.29 Results of
FNN ANFIS forecasting
Results:
Total amount of errors
7
%% of errors
14 %
First type of errors
1
Second type of errors
6
Table 4.30 Comparative analysis of FNN ANFIS and TSK in dependence on rules number
Network/number
of rules
Total amount
of errors
% of
errors (%)
Amount of 1
type errors
Amount of 2
type errors
Anﬁs 5
6
12
0
6
Anﬁs 10
7
14
1
6
TSK 5
5
10
0
5
TSK 10
6
12
1
5
4.5
Banks Financial State Analysis and Bankruptcy Forecasting
163

The results of FNN TSK are presented in the Table 4.31.
The similar experiment was carried out with FNN ANFIS
Experiment №6
• Training sample—120 Ukrainian banks;
• Test sample—50 banks;
• Number of rules = 10.
• Input data—ﬁnancial indices are the same as in the experiment 5.
The results obtained with FNN ANFIS are presented in the Table 4.32.
After analysis of the presented experimental results the following conclusions
were made:
1. FNN TSK gives more accurate forecasting than FNN ANFIS.
2. The variation of the number of rules in the training and test samples doesn’t
inﬂuence on the accuracy of forecasting.
The next series of experiments was aimed at determining optimal input data
(ﬁnancial indices) for forecasting. The period of input data was January 2008.
Experiment №7
• Training sample—120 Ukrainian banks,
• Test sample—50 banks;
• Number of rules = 10.
• Input data—ﬁnancial indices (banks ﬁnancial accountant reports):
– proﬁt of current year; net percentage income; net commission income;
– net expense on reserves;
– net bank proﬁt/losses.
Table 4.31 Results of
FNN TSK forecasting
Results
Total amount of errors
7
%% of errors
10 %
First type of errors
1
Second type of errors
6
Table 4.32 results of
FNN ANFIS forecasting
Results
Total amount of errors
7
%% of errors
10 %
First type of errors
0
Second type of errors
7
164
4
Application of Fuzzy Logic Systems and Fuzzy …

The results of FNN TSK application are presented in the Table 4.33.
Experiment №8
• Training sample-—120 Ukrainian banks;
• Test sample—50 banks;
• Number of rules = 10.
• Input data—ﬁnancial indices (banks ﬁnancial accountant reports):
– General reliability factor (Own capital/Assets);
– Instant Liquidity Factor (Liquid assets/Liabilites);
– Cross-coefﬁcient (Total liabilities/working assets);
– General liquidity coefﬁcient (Liquid assets + defended capital + capitals in
reserve fund/total liabilities);
– Coefﬁcient of proﬁt fund capitalization (Own capital/Charter fund)
The results of application of FNN TSK arte presented in the Table 4.34.
Note that these ﬁnancial indices are used as input data in Kromonov’s method of
banks bankruptcy [12], results of its application are considered below.
Experiment №9
• Training sample—120 Ukrainian banks;
• Test sample—70 banks;
• Number of rules = 5.
• Input data—ﬁnancial indices:
• ROE—Return on Entity (ﬁnancial results/entity);
• ROA—Return on Assets (ﬁnancial results/assets);
• CIN—incomes- expenses ratio (income/expense);
• NIM—net interest margin;
• NI—net income.
The results of application of FNN TSK for forecasting with these input indices
are presented in the Table 4.35.
Table 4.33 Results of
FNN TSK forecasting
Results:
Total amount of errors
13
%% of errors
19 %
First type of errors
6
Second type of errors
7
Table 4.34 Results of
FNN TSK forecasting
Results
Total amount of errors
7
%% of errors
10 %
First type of errors
1
Second type of errors
6
4.5
Banks Financial State Analysis and Bankruptcy Forecasting
165

It should be noted that these indices are used as input in method of EuroMoney
[13].
Experiment №10
• Training sample—120 Ukrainian banks;
• Test sample—70 banks;
• Number of rules = 5.
• Input data—ﬁnancial indices (banks ﬁnancial accountant reports):
• General reliability factor (Own capital/Assets);
• Instant Liquidity Factor (Liquid assets/Liabilities);
• Cross-coefﬁcient (Total liabilities/working assets);
• General liquidity coefﬁcient (Liquid assets + defended capital + capitals in
reserve fund/total liabilities);
• Coefﬁcient of proﬁt fund capitalization (Own capital/Charter fund);
• Coefﬁcient entity security (Secured entity/own entity).
The results of FNN TSK application with these ﬁnancial indices presented in the
Table 4.36.
It should be noted these indices are also used in Kromonov’s method.
The comparative analysis of forecasting results using different sets of ﬁnancial
indices are presented in the Table 4.37.
Further the series of experiments were carried out aimed on determination of
inﬂuence of data collection period on the forecasting results. It was suggested to
consider two periods: January of 2008 (about 2 years before the crisis) and July of
2009 (just before the start of crisis).
Experiment №11
• Training sample—120 Ukrainian banks;
• Test sample—70 banks;
• Number of rules = 10.
• Input data—ﬁnancial indices the same as in the experiment 10.
Table 4.35 Results of
FNN TSK forecasting
Results:
Total amount of errors
12
%% of errors
17 %
First type of errors
5
Second type of errors
7
Table 4.36 Results of
FNN TSK forecasting
Results
Total amount of errors
8
%% of errors
13 %
First type of errors
1
Second type of errors
7
166
4
Application of Fuzzy Logic Systems and Fuzzy …

In the Table 4.38 the comparative results of forecasting in dependence on period
of input data are presented.
4.5.2
The Application of Fuzzy GMDH for Financial State
Forecasting
In the process of investigations fuzzy Group Method of Data Handling (FGMDH)
was also applied for ﬁnancial state of Ukrainian banks forecasting [12]. As input
data the same indices were used as in the experiments with FNN TSK In the
Table 4.39 the corresponding results are presented in dependence on input data
period.
If to compare the results of FGMDH with the results of FNN TSK one can see
that FNN TSK gives better results if we use the input data for one year before
Table 4.37 The dependence of forecasting accuracy on sets of input ﬁnancial indices
Experiment
Total number of
errors
1 type of
errors
2 type of
errors
total % of
errors
Experiment №5
7
1
6
10
Experiment №8
7
0
7
10
Experiment №9
12
5
7
17
Experiment №1
8
1
7
13
Table 4.38 Accuracy of forecasting in dependence on data collection period
Experiment/number of
rules
Total amount of
errors
1 type of
errors
2 type of
errors
Total % of
errors (%)
01.01.2008 5 rules
7
0
7
10
01.07.2009 5 rules
5
0
5
7
01.07.2009 10 rules
7
3
4
10
Table 4.39 Comparative results of forecasting using method FGMDH in dependence on period of
input data collection
Input data
period
Total error
number
%% of errors
(%)
1 type of
errors
2 type of
errors
2004
10
14
3
7
2005
9
13
3
6
2006
8
11.4
3
5
2007
7
10
2
5
2008
6
8.5
1
5
2009
6
8.5
2
4
4.5
Banks Financial State Analysis and Bankruptcy Forecasting
167

possible bankruptcy while FGMDH gives better results using older input data and
has advantages in long-term forecasting (2 or more years).
4.5.3
The Application of Conventional Methods
for Financial State Forecasting
In order to compare the results of application of fuzzy methods for banks ﬁnancial
state forecasting the conventional crisp methods were implemented. As the crisp
methods of banks state analysis the so called Kromonov’s method and aggregated
multilevel index of bank state method developed by the Byelorussian banks
association were used [11].
In the ﬁrst experiment the application of Kromonov’s method was performed
using quarterly data of Ukrainian banks at the beginning of 2008 year and checking
the forecast on the middle of 2009 year using real data of bank bankruptcy. The
goal of the experiments was the exploration of inﬂuence of input data period on the
forecasting quality. The results of analysis are presented in the Table 4.40.
As one may see the Kromonov’s method gives false forecast for 34 banks of
total number 170, that is 20 % of error when using the data on 01.01.2008г. while
false forecast for 24 banks of 170 (15 % of errors) using more fresh data on
01.07.2009. So the considerable improvement of forecasting accuracy using more
fresh data is obtained. The types of errors were distributed uniformly in both cases.
The second experiment in this series was the application of method developed by
Byelorussian bank association (BBA)
for bankruptcy risk forecasting using the
Ukrainian banks quarterly data on the beginning of 2008 year and checking of the
forecasting accuracy in the middle of 2009. Experimental results are presented in
the Table 4.41.
As one may readily see in the Table 4.41 Byelorussian bank association
(BBA) method gives false forecast for 27 banks of 170, that is 16 % of errors. This
result for the data on the beginning of 2008 year is more preferable than
Kromonov’s method which had 20 % of errors on this data. Using data of the
middle of 2009 year BBA method gives the false forecast for 24 banks (15 %) that
coincides with results of Kromonov’ s method.
Table 4.40 Kromonov’s
method results in dependence
on data collection period
01.01.2008
01.07.2009
Total number of errors
34
24
%% of errors
20 %
15 %
1 type errors number
18
12
2 type errors number
16
12
Test sample size
170
170
168
4
Application of Fuzzy Logic Systems and Fuzzy …

4.5.4
The Generalized Analysis of Crisp and Fuzzy
Forecasting Methods
In the concluding experiments the comparative analysis of application of all the
considered methods was carried out. The following methods were considered [14]:
• fuzzy neural network ANFIS;
• fuzzy neural network TSK;
• Kromonov’s method
• Byelorussian bank association method.
As input data the ﬁnancial indices of Ukrainian banks on July 2007 year were
used. The results of application of all methods for bankruptcy risk analysis are
presented in the Table 4.42.
Conclusions
Various methods for banks ﬁnancial state forecasting were considered and
investigated. The following methods were considered [11, 12]:
• fuzzy neural network ANFIS; • fuzzy neural network TSK;
• Kromonov’s method; • Byelorussian bank association method.
As the input data the ﬁnancial indices of Ukrainian banks were considered.
1. While experiments the adequate input ﬁnancial indices were detected with
which the best forecasting results for Ukrainian banks were obtained:
• General reliability factor (Own capital/Assets);
• Instant Liquidity Factor (Liquid assets/Liabilities);
• Cross-coefﬁcient (Total liabilities/working assets);
• General liquidity coefﬁcient (Liquid assets + defended capital + capitals in
reserve fund/total liabilities);
• Coefﬁcient of proﬁt fund capitalization.
Table 4.41 Byelorussian
bank association method
forecasting results
01.01.2008
01.07.2009
Total number of errors
27
24
%% of errors
16 %
15 %
1 type errors number
12
4
2 type errors number
15
20
Test sample size
170
170
Table 4.42 Comparative results analysis of various forecasting methods
Method/period)
Total amount of
errors
%% of errors (%)
1 type
errors
2 type
errors
ANFIS
7
10
1
6
TSK
5
7
0
5
Kromonov’s
method
10
15
5
5
BBA method
10
15
2
8
4.5
Banks Financial State Analysis and Bankruptcy Forecasting
169

2. It was established that FNN TSK gives much more accurate results than FNN
ANFIS The variation of rules number in training sample doesn’t inﬂuence on
forecasting results. The fuzzy GMDH gives better results using older data that
is, more preferable for long-term forecasting (two or more years).
3. In general, the comparative analysis had shown that fuzzy forecasting methods
and techniques give better results than conventional crisp methods for fore-
casting bankruptcy risk. But at the same time the crisp methods are more simple
in implementation and demand less time for their adjustment.
4.6
Comparative Analysis of Methods of Bankruptcy Risk
Forecasting for European Banks Under Uncertainty
Introduction
The results of successful application of fuzzy methods for Bankruptcy risk fore-
casting of Ukrainian banks under uncertainty stimulated the further investigations
of these methods application for ﬁnancial state analysis of European leading banks
The main goal of this exploration was to investigate novel methods of European
banks bankruptcy risk forecasting which may work under uncertainty with
incomplete and unreliable data.
Besides, the other goal of this investigation was to determine which factors
(indices) are to be used in forecasting models that to obtain results close to real data.
Therefore, we used a set of ﬁnancial indicators (factors) of European banks
according to the International accountant standard IFRS [14]. The annual ﬁnancial
indicators of about 300 European banks were collected in 2004–2008 years, pre-
ceding the start of crisis of bank system in Europe in 2009 year. The data source is
the information system Bloomberg[]. The resulting sample included the reports
only from the largest European banks as system Bloomberg contains the ﬁnancial
reports only from such banks. For correct utilization input data were normalized in
interval [0,1].
Application of fuzzy neural networks for European banks bankruptcy risk
forecasting
The period for which the data were collected was 2004–2008 years. The possible
bankruptcy was analyzed in 2009 year. The indicators of 165 banks were consid-
ered among which more than 20 banks displayed the worsening of the ﬁnancial
state in that year. Fuzzy neural networks and Fuzzy Group Method of Data
Handling (FGMDH) were used for bank ﬁnancial state forecasting.
In accordance with the above stated goal the investigations were carried out for
detecting the most informative indicators (factors) for ﬁnancial state analysis and
bankruptcy forecasting. Taking into account incompleteness and unreliability of
170
4
Application of Fuzzy Logic Systems and Fuzzy …

input data FNN ANFIS and TSK were suggested for bankruptcy risk forecasting
[11].
After performing a number of experiments the data set of ﬁnancial indicators was
found using which FNN made the best forecast. These indicators are the following:
• Debt/Assets = (Short-term debt + Long-term debt)/Total Assets
• Loans to Deposits ratio
• Net Interest Margin (NIM) = Net Interest income/Earning Assets
• Return on Equity (ROE) = Net Income/Stockholder Equity
• Return on Assets (ROA) = Net Income/Assets Equity
• Cost/Income = Operating expenses/Operating Income
• Equity/Assets = Total Equity/Total Assets
The series of experiments were carried out for determining the inﬂuence of the
number of rules and period of data collection on forecasting results.
In the ﬁrst series of experiments FNN TSK was used for forecasting.
Experiment №1
• Training sample = 115 banks of Europe;
• Testing sample = 50 banks; number of rules = 5
• Input data period = 2004 year.
The results of FNN TSK application are presented in Table 4.43.
Experiment №2
• Training sample = 115 banks of Europe;
• Testing sample = 50 banks; number of rules = 5
• Input data period = 2005
The results of FNN TSK application are presented in Table 4.44
Table 4.43 Forecasting
results of FNN TSK
Results
Total number of errors
8
%% errors
16 %
ﬁrst type of errors
0
Second type of errors
8
Table 4.44 Forecasting
results of TSK using data of
2005 year
Results
Total number of errors
7
%% errors
14 %
ﬁrst type of errors
0
Second type of errors
7
4.6
Comparative Analysis of Methods of Bankruptcy Risk …
171

Experiment №3
• Input data the same as in the experiment 1.
• Input data period = 2006
The results of FNN TSK application are presented in Table 4.45.
Experiment №4
• Input data the same as in the experiment 1.
• Input data period = 2007
The results of FNN TSK application are presented in Table 4.46.
Further, the similar experiments were performed with FNN ANFIS while the
period of data collection varied since 2004 to 2007 year.
The experiments for detection of rules number inﬂuence on forecasting results
were also carried out. The corresponding results are presented in Table 4.47 for
Table 4.45 Forecasting
results of TSK using data of
2006 year
Results:
Total number of errors
5
%% errors
10 %
ﬁrst type of errors
0
Second type of errors
5
Table 4.46 Forecasting
results of TSK using data of
2007 year
Results:
Total number of errors
1
%% errors
2 %
ﬁrst type of errors
0
Second type of errors
1
Table 4.47 Forecasting results for FNN TSK versus number of rules and data period
Experiment/number
of rules
Total errors
number
%% of
errors (%)
Number of the 1-st
type errors
Number
of the
2-nd type
errors
2004–5
8
16
0
8
2005–5
7
14
0
7
2006–5
5
10
0
5
2007–5
1
2
0
1
2004–10
8
16
0
8
2005–10
8
16
1
7
2006–10
11
22
7
4
2007–10
4
0
4
172
4
Application of Fuzzy Logic Systems and Fuzzy …

FNN TSK. The results for FNN ANFIS are presented in Table 4.48 which display
the inﬂuence of data collection period on forecasting accuracy.
After analysis of these results the following conclusions were made
1. FNN TSK gives better results than ANFIS while forecasting the bankruptcy risk
for European banks.
2. The best input variables (indicators) for European banks bankruptcy risk fore-
casting are the following:
– Debt/Assets = (Short-term debt + Long-term debt)/Total Assets
– Loans to Deposits
– Net Interest Margin (NIM) = Net Interest income/Earning Assets
– Return on Equity (ROE) = Net Income/Stockholder Equity
– Return on Assets (ROA) = Net Income/Assets Equity
– Cost/Income = Operating expenses/Operating Income
– Equity/Assets = Total Equity/Total Assets
Input data collection period (forecasting interval) makes inﬂuence on forecasting
results.
4.6.1
The Application of Fuzzy GMDH for Bank Financial
State Forecasting
In next experiments Fuzzy Group Method of Data Handling (FGMDH)
was
applied for European banks ﬁnancial state forecasting. Fuzzy GMDH enables to
construct forecasting models using experimental data automatically without expert
Table 4.49 Comparative analysis of forecasting results for FGMDH
Input data
period
Total number
of errors
%% of
errors (%)
Number of the ﬁrst
type errors
Number of the second
types of errors
2004
7
14
0
7
2005
6
12
1
5
2006
4
8
1
3
2007
2
4
0
2
Table 4.48 Forecasting results for FNN ANFIS versus data period
Experiment/number
of rules
Total errors
number
%% of errors
(%)
Number of the 1-st
type errors
Number of
the
2-nd type
errors
2004–5
8
16
0
8
2005–5
8
16
1
7
2006–5
8
16
4
4
2007–5
4
8
0
4
4.6
Comparative Analysis of Methods of Bankruptcy Risk …
173

[14]. The additional advantage of FGMDH is possibility to work with fuzzy
information.
As the input data in these experiments were used the same indicators as in
experiments with FNN TSK. In Table 4.49 forecasting results are presented in
dependence on input data period collection for FGMDH.
If to compare the results of FGMDH application with the results of FNN TSK
one can see that neural network gives better results on the short forecasting interval
(one year) while fuzzy GMDH gives better results on greater intervals (2 or more
years) This conclusion coincides with similar conclusion for Ukrainian banks.
In Table 4.50 the comparative results of application of different methods for
bankruptcy risk forecasting are presented.
4.6.2
Application of Linear Regression and Probabilistic
Models
Regression models For analysis of fuzzy methods efﬁciency at the problem of
bankruptcy risk forecasting the comparison with crisp methods: the regression
analysis of linear models was performed. As input data the same indicators were
used which were optimal for FNN. Additionally, the index Net Financial Result,
was also included in the input set. This index makes great impact on forecasting
results. Thus, input data in this experiments were 8 ﬁnancial indicators of 256
European banks according to their reports based on International standards:
• Debt/Assets—X1
• Loans/Deposits—X2
• Net Interest Margin—X3
Table 4.50 Forecasting results of different fuzzy methods
Method
(period)
Total number
of errors
%% of
errors (%)
Number of the ﬁrst
type errors
Number of the second
type of errors
ANFIS
(1 year)
4
8
0
4
TSK
(1 year)
1
2
0
1
FGMDH
(1 year)
2
4
0
2
ANFIS
(2 years)
8
16
4
4
TSK
(2 years)
5
10
0
5
FGMDH
(2 years)
4
8
1
3
174
4
Application of Fuzzy Logic Systems and Fuzzy …

• ROE (Return on Equity)—X4
• ROA (Return on Assets)—X5
• Cost/Income—X6
• Equity/Assets—X7
• Net Financial Result—X8
The input data were normalized before the application. The experiments were
carried out with full regression ARMA model, which used 8 variables and short-
ened models with 6 and 4 variables.
Each obtained model was checked on testing sample consisting of 50 banks. The
comparative forecasting results for all ARMA models are presented in Table 4.51.
As one may see in Table 4.51, the application of all types of linear regression
models gives the same error 18 %, that is much worse than application of fuzzy
neural networks.
Logit-models Further the experiments were performed using logit-models for
bankruptcy forecasting [14]. The training sample consisted of 165 banks and the
testing sample—of 50 banks.
The ﬁrst one was constructed linear logit-model using all the input variables. It
has the following form (estimating and forecasting equations):
IY ¼ Cð1Þ þ Cð2Þ  X1 þ Cð3Þ  X2 þ Cð4Þ  X3 þ Cð5Þ  X4 þ
þ Cð6Þ  X5 þ Cð7Þ  X6 þ Cð8Þ  X7 þ Cð9Þ  X8
Y ¼ 1  @CLOGISTICððCð1Þ þ Cð2Þ  X1 þ Cð3Þ  X2 þ
þ Cð4Þ  X3 þ Cð5Þ  X4 þ Cð6Þ  X5 þ Cð7Þ  X6 þ Cð8Þ  X7 þ
þ Cð9Þ  X8ÞÞ
The next constructed model was a linear probabilistic logit-model with 6 inde-
pendent variables. The ﬁnal table including the forecasting results of all the
logit-models is presented below (Table 4.52).
Probit-models The next experiments were carried out with probit- models [14].
The ﬁrst constructed model was the linear prodbit –model based on 206 banks using
all the input variables. It had the following form
Table 4.51 Comparative analysis of ARMA models
Input data
Testing
sample
I type
errors
II type
errors
Total number of
errors
% of errors
(%)
All variables
(8)
50
5
4
9
18
6 variables
50
5
4
9
18
4 variables
50
5
4
9
18
4.6
Comparative Analysis of Methods of Bankruptcy Risk …
175

IY ¼ Cð1Þ þ Cð2Þ  X1 þ Cð3Þ  X2 þ Cð4Þ  X3 þ Cð5Þ  X4 þ
þ Cð6Þ  X5 þ Cð7Þ  X6 þ Cð8Þ  X7 þ Cð9Þ  X8
Y ¼ 1  @CLOGISTICððCð1Þ þ Cð2Þ  X1 þ Cð3Þ  X2 þ
þ Cð4Þ  X3 þ Cð5Þ  X4 þ Cð6Þ  X5 þ Cð7Þ  X6 þ Cð8Þ  X7 þ
þ Cð9Þ  X8ÞÞ
As the experiments had shown the inputs Net Interest Margin (X3) and Net
Financial Result (X8) very weakly inﬂuence on the results and they were excluded
in the next experiments. The next constructed probit-model included 6 variables.
Further in this model were excluded insigniﬁcant variables Debt/Assets (X1) and
Loans/Deposits (X2) and as result linear probit-model with 4 variables was obtained.
Each of the constructed probit- models was checked on the test sample of 50
banks. The results of application of all probit- models are presented in the
Table 4.53.
As one may see from the Table 4.53, the application of all the probit-models
gives relative error 14–18 %, that is much worse than results obtained by fuzzy
neural networks It is worth to mention the decrease of model forecasting quality
Table 4.52 Comparative analysis of logit-models
Input data
Testing
sample
I type
errors
II type
errors
Total number of
errors
% of errors
(%)
All variables
(8)
50
6
2
8
16
6 variables
50
6
2
8
16
Table 4.53 Forecasting results of probit-models
Input data
Testing
sample
I type
errors
II type
errors
Total number of
errors
% of errors
(%)
All variables
(8)
50
5
2
7
14
6 variables
50
5
2
7
14
4 variables
50
6
3
9
18
Table 4.54 Comparative analysis of methods for banks bankruptcy forecasting
Method
(period)
Total number of
errors
%% of errors
(%)
I type
errors
II type
errors
ANFIS
4
8
0
4
TSK
1
2
0
1
FGMDH
2
4
0
2
ARMA
9
18
4
5
LOGIT
8
16
2
6
PROBIT
7
14
2
5
176
4
Application of Fuzzy Logic Systems and Fuzzy …

after exclusion of insigniﬁcant variables. In particularly, after exclusion of variables
Debt/Assets and Loans/Deposits error value has increased error from 14 % to 18 %.
4.6.3
Concluding Experiments
In the ﬁnal series of experiments investigations and detailed analysis of various
methods for forecasting bankruptcy risk were performed. The following methods
were investigated [14]: FNN ANFIS FNN TSK FGMDH; Regression models
Logit-models; •Probit-models.
Period of input data was—2007 year (for 1 year before possible bankruptcy).
Comparative analysis of all the forecasting methods is presented in the
Table 4.54.
As one may see from this table fuzzy methods and models show much better
results than crisp methods: ARMA, Logit-models and probit-models When fore-
casting by one year prior to current date fuzzy neural network TSK shows better
results than FGMDH. But when forecasting for longer intervals(several years)
FGMDH is the best method.
In a whole the conclusions of experiments with European banks completely
conﬁrmed the conclusions of experiments with Ukrainian banks.
References
1. Zaychenko, Y.P.: Fuzzy models and methods in intellectual systems. Kiev—Publishing House
Slovo, p. 354 (2008). Zaychenko Yu. Fuzzy Group Method of Data Handling under fuzzy
input data. Syst. Res. Inf. Technol. №3, 100–112 (2007) (rus)
2. Zaychenko, Y.P.: Sevae, F., Titarenko, K.M., Titarenko N.V.: Investigations of fuzzy neural
networks in macroeconomic forecasting. Syst. Res. Inf. Technol. №2, 70–86 (2004) (rus)
3. Zaychenko, Y.P., Sevaee F., Matsak A.V.: Fuzzy neural networks for economic data
classiﬁcation. Vestnik of National Technical University of Ukraine KPI, Section Informatic,
Control and Computer Engineering. vol. 42, pp. 121–133 (2004) (rus)
4. Zaychenko Yu.P. Fundamentals of intellectual systems design. Kiev—Publishing House
Slovo, p. 352 (2004) (rus)
5. Zgurovsky, M.Z., Zaychenko, Y.P.: Complex analysis of corporation bankruptcy risk uinder
uncertainty. Part 1. Syst. Res. Inf. Technol. №1, 113–128 (2012) (rus)
6. Zgurovsky, M.Z., Zaychenko, Y.P.: Models and methods of decision-making under
uncertainty. Kiev.-Publ. House Naukova Dumnka, p. 275 c (2011) (rus)
7. Zaychenko, Y.P., Rogoza S.V., Stolbunov, V.A.: Comparative analysis of enterprises
bankruptcy risk. Syst. Res. Inf. Technol. №3, 7–20 (2009) (rus)
8. Nedosekin, A.O.: System of portfolio optimization of Siemens services. Banking technologies,
№5 (2003) (rus)—See also: http://www.ﬁnansy.ru/publ/ﬁn/004.htm
9. Nedosekin, A.O.: The applications of fuzzy sets theory to problems of ﬁnance control.
Section Audit and Financial Analysis, №2 (2000) (rus)—See also: http://www.cﬁn.ru/press/
afa/2000-2/08-3.shtml
4.6
Comparative Analysis of Methods of Bankruptcy Risk …
177

10. Zgurovsky, M.Z., Zaychenko Yu.P. Complex analysis of corporation bankruptcy risk uinder
uncertainty. Part 2 Syst. Res. Inf. Technol. №2, 111–124 (2012) (rus)
11. Aghaei, O.N., Ghamish, A., Yuriy, Z., Voitenko, O.: Analysis of ﬁnancial state and banks
bankruptcy risk forecasting. Syst. Res. Inf. Technol. №2 (2015) (rus)
12. Aghaei, O.N., Ghamish, A., (Iran), Zaychenko, Y., Voitenko, O.: Banks ﬁnancial state
analysis and bankruptcy forecasting. Int. J. Eng. Innov. Technol. (IJEIT) 4(6), 62–67 (2014)
13. Averkin, A.N.: Fuzzy semiotic control systems. Intellectual Control: New Intellectual
Technologies in Control Tasks. pp. 141–145. Moscow.-Nauka. Fizmathlit (1999) (rus)
14. Aghaei, O.N., Ghamish, A., (Iran), Zaychenko, Y.: Comparative analysis of methods of banks
bankruptcy risk forecasting under uncertainty. Int. J. Eng. Innovative Technol. (IJEIT) 4(5)
(2015)
178
4
Application of Fuzzy Logic Systems and Fuzzy …

Chapter 5
Fuzzy Neural Networks
in Classiﬁcation Problems
5.1
Introduction
The purpose of this chapter is consideration and analysis of fuzzy neural networks
in classiﬁcation problems, which have a wide use in industry, economy, sociology,
medicine etc.
In the Sect. 5.2 a basic fuzzy neural network for classiﬁcation—NEFClass is
considered, the learning algorithms of rule base and MF of fuzzy sets are presented
and investigated. Advantages and lacks of the system NEFClass are analyzed and
its modiﬁcation FNN NEFClass M, free of lacks of the system NEFClass is
described in the Sect. 5.3.
The results of numerous comparative experimental researches of the basic and
modiﬁed system NEFClass are described in Sect. 5.4. The important in a practical
sense task of recognition of objects on electro-optical images (EOI) is considered
and its solution with application of FNN NEFClass is presented in the Sect. 5.5.
The comparative analysis of different learning algorithms of FNN NEFClass at the
task of recognition of EOI objects in the presence of noise is carried out. Problem of
hand-written mathematical expressions recognition is considered in the Sect. 5.6
and its solution with application of FNN NEFClass is presented.
5.2
FNN NEFClass. Architecture, Properties,
the Algorithms of Learning of Base Rules
and Membership Functions
A classiﬁcation problem is one of the most actual spheres of application of the
computational intelligence systems. For its decision different approaches and
methods were suggested, among which popular solutions were offered, combining
neural networks and fuzzy inference systems. One of such decisions is the system
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_5
179

NEFClass (NEuro-Fuzzy CLASSiﬁer), based on the generalized architecture of
fuzzy perceptron and suggested by Nauck and Kruse in [1–3].
Both original and modiﬁed model of NEFClass are derivative from the general
model of fuzzy perceptron [4]. A model purpose is a development of fuzzy rules
from a set of data which can be divided into the several non-overlapping classes.
The fuzziness arises up due to the imperfect or incomplete measurings of properties
of objects, subject to classiﬁcation.
Fuzzy rules, describing expert information, have the following form:
if is l1i and x2 is l2i and … and xn is lni,
then pattern ðx1; x2; . . .xnÞ belongs to the class of i,
where l1i; . . .lni; are MF of fuzzy sets.
The goal of NEFClass is to deﬁne these rules, as well as parameters of mem-
bership functions for fuzzy sets. It was assumed here, that intersection of two
different sets is empty.
The system NEFClass has 3-layer successive architecture (see Fig. 5.1). The ﬁrst
layer U1 contains inputs neurons which inputs patterns are fed in. Activating of
these neurons does not change usually input values. The hidden layer U2 contains
fuzzy rules, and the third layer U3 consists of output neurons (classiﬁers).
Fig. 5.1 Structure of FNN NEFCLASS
180
5
Fuzzy Neural Networks in Classiﬁcation Problems

Activations of rule neurons and neurons of output layer with the pattern p are
calculated so:
aðpÞ
R ¼ min
x2U1 Wðx; RÞðaðpÞ
x Þ
n
o
;
ð5:1Þ
aðpÞ
C ¼
X
R2U2
Wðc; RÞ  aðpÞ
R ;
ð5:2Þ
or alternatively
aðpÞ
C ¼ max
R2U2 aðpÞ
R
n
o
;
ð5:3Þ
where Wðx; rÞ is a fuzzy weight of connection of input neuron x with a rule neuron R,
and WðR; cÞ—fuzzy weight of connection of a rule neuron R with the neuron c of
output layer. Instead of application of operations of maximum and minimum it is
possible to use other functions of so-called “t-norm” and “t-conorm” accordingly [1].
A rule base is approximation of unknown function and describes a classiﬁcation
task uðxÞ, such, that ci ¼ 1; cj ¼ 0 ðj ¼ 1. . .m; 8 j 6¼ iÞ, if x belongs to the class of Ci.
Every fuzzy set is marked a linguistic term, such as “large”, “small”, “middle”
etc. Fuzzy sets and linguistic rules present approximation of classifying function
and determine the result of the system NEFClass. They are obtained from a sample
by learning. It’s necessary, that for every linguistic value (for example, “x1 is
positive and large”) there should be only one presentation of fuzzy set.
Learning in the System NEFClass
Learning of Rules Base
The system NEFClass can be built on partial knowledge about patterns. An user
must deﬁne the amount of initial fuzzy sets for each of object features (number of
terms) and set the value kmax that is a maximal number of rule nodes, which can be
created in the hidden layer. For learning triangular MF are used.
Consider the system of NEFClass with n input neurons x1; . . .xn; k ðk  kmaxÞ
rule neurons and m output neurons c1; . . .cm;. The learning sample of patterns is
also given: L ¼
p1; t1
ð
Þ; . . . ps; ts
ð
Þ
f
g, each of which consists of input pattern p 2 Rn
and desired pattern t 2 0; 1
f
gm.
A learning algorithm consists of two stages.
Stage 1. Generation of rule base.
The ﬁrst stage whose purpose is to create rule neurons of the system NEFClass
consists of the followings steps [1–3]:
1. Choose a next pattern (p, t) from sample L.
2. For every input neuron xi 2 U1 ﬁnd such membership li
Ji that
5.2
FNN NEFClass. Architecture, Properties, the Algorithms of Learning …
181

lðpÞ
Ji ¼ max
j21;::q1flðpÞ
ji ðpiÞg;
ð5:4Þ
where xi ¼ pi:
3. If a number of rule nodes k is less than kmax and there is no rule node R such, that
Wðx1; RÞ ¼ lJ1; . . .; Wðxn; RÞ ¼ lJn;
then create such node and connect it with an output node ci, if ti ¼ 1, connect it
with all input neurons and assign the corresponding weights li
Ji to connections.
4. If there are still not-processed patterns in L and k \ kmax, then go to the step 1
and continue learning using next pattern, and otherwise stop.
5. Determine a rule base by one of three procedures:
a. “Simple” rules learning: we leave the ﬁrst k rules only (stop creation of rules,
if it was created k ¼ kmax rules).
b. The “best” learning rules: we process patterns in L and accumulate activating
of every rule neuron for every class of patterns which were entered into
system NEFClass. If rule neuron R shows the greater accumulation of
activating for a class Cj than for a class CR, which was speciﬁed initially for
this rule, then change implication of rule R from CR to Cj, that means
connect R with the output neuron cj. We continue processing of patterns in
L farther and calculate for every rule neuron the function:
VR ¼
X
p2L
aðpÞ
R  ep
ð5:5Þ
where
ep ¼
1
if pattern p is classified correctly
1
otherwise

We leave k rule neurons with the greatest values of VR and delete other rule
neurons from the system NEFClass.
c. The “best for every class” algorithm of learning: we operate as in the pre-
vious case, but leave for each class Cj only those best
k
m
 
rules, the con-
sequences of which relate to the class Cj (where x½  is integer part from x).
Learning of Fuzzy Sets MF
Stage 2
On the second stage learning of parameters of membership functions (MF) of fuzzy
sets. is performed A learning algorithm with teacher of the system NEFClass must
adapt MF of fuzzy sets. The algorithm cyclic runs through all learning patterns of
the sample L, executing the following steps, until one of stop criteria will be
fulﬁlled [1–3].
182
5
Fuzzy Neural Networks in Classiﬁcation Problems

Steps:
1. Choose a next pattern (p, t) from sample L, enter it into FNN NEFclass and
determine an output vector c.
2. For every output neuron ci calculate the value dCi
dCi ¼ ti  aCi;
where ti is a desired output,aci is an real output of neuron ci.
3. For every rule neuron R, for which output is aR [ 0 execute:
a. determine a value dR, equal
dR ¼ aR  1  aR
ð
Þ 
X
C2U3
W R; C
ð
ÞdC
ð5:6Þ
b. Find such x0, that
Wðx0; RÞðax0Þ ¼ min
x2U1 Wðx; RÞðaxÞ


:
ð5:7Þ
c. For fuzzy sets Wðx0; RÞ determine displacement (shift) of parameters of MF
Da; Db; Dc, using learning speed r [ 0:
Db ¼ r  dR  ðc  aÞ  sgnðax0  bÞ;
ð5:8Þ
Da ¼ r  dR  ðc  aÞ þ Db;
ð5:9Þ
Dc ¼ r  dR  ðc  aÞ þ Db:
ð5:10Þ
and execute the changes of Wðx0; RÞ.
d. Calculate an rule error:
E ¼ aR  1  aR
ð
Þ 
X
c2U3
2  WðR; cÞ  1
ð
Þ  dc
j
j:
ð5:11Þ
End of iteration. Repeat the described iterations until condition of stop will be
fulﬁlled. It is possible to use as criteria of stop, for example, such:
1. An error has not decreased during n iterations.
2. Stop learning after achievement of the deﬁned (desirably close to the zero) error
value.
5.2
FNN NEFClass. Architecture, Properties, the Algorithms of Learning …
183

5.3
Analysis NEFClass Properties. The Modiﬁed System
NEFClassM
FNN NEFClass has several obvious advantages, distinguishing it among the other
classiﬁcation systems. The most important are: easiness of implementation,
high-speed algorithms of learning, as well as that is the most important, high
accuracyof data classiﬁcation—at the level of the best systems in this area.
However, the basic system NEFClass has some shortcomings:
1. formulas used for parameters learning are empirical in nature, in addition,
2. it is not clear how to choose in the learning algorithm the speed parameter r.
Therefore, these shortcomings were deleted in the modiﬁcation of basic system
—so-called system NEFClass-M (modiﬁed) developed in [5].
Randomization and careful selection rate constants learning r are inherent
properties of the system NEFCLASS-M. These properties have been designed to
mitigate the impact some of the shortcomings of the original model and have made
it possible to achieve a signiﬁcant improvement in the quality of classiﬁcation.
Randomization. Because of the nature of the training algorithm “simple” rules
base and learning algorithm of fuzzy sets, the outcome of the training network for
these algorithms are highly dependent on order, in which samples are represented in
a learning sample. If, for example, the samples will be sorted by classes, the system
will better classify the patterns of one class and substantially worse—the patterns of
the other class. Ideally, the patterns in the training sample must be randomly mixed,
in order to avoid the negative effect.
Implementation of the system NEFClassM [5] avoids this complexity by “ran-
domization” of patterns order in a learning sample after its boot. Moreover, such an
accidental “randomization” occurs before each iteration of learning algorithm. As
show further experiments, this allows to achieve a more stable and, often, the better
classiﬁcation results, which do not depend on the order in which patterns in a
learning sample has been submitted by a user.
Choice of speed training. In the learning algorithm of fuzzy sets in the model
NEFCLASS is used parameter training speed r. As experiments had shown, carried
out in the course of developing the NEFClass M, this parameter plays a vital role in
the success of the training.
The experiments had shown that, under other parameters being equal, for each
speciﬁc task training there exists a certain value r, which ensures a minimum
percentage of erroneous classiﬁcation after the training. Unfortunately, to obtain
analytical dependence for optimal parameter value is very difﬁcult because learning
algorithm NEFCLASS as a whole is empirical; however, using search and try
method it was found that for many tasks optimal value r lies in the narrow range
[0.06–0.1], in particular it may be equal to 0.07. This value has been set for the
program which implements a modiﬁed model NEFClass M [5].
184
5
Fuzzy Neural Networks in Classiﬁcation Problems

The Modiﬁed Model NEFCLASS
Consider the basic shortcomings in the NEFCLass learning algorithm.
The analysis of the drawbacks of NEFCLASS has shown that their principal
cause lies mostly in an imperfect learning algorithm of fuzzy sets. Therefore, a
natural approach, aimed to correct the situation, was the replacement of empirical
learning algorithm by the strict optimization algorithm with all the ensuing con-
sequences for network architecture and algorithms.
Both the original and modiﬁed model NEFCLASS are based on the architecture
of a fuzzy perceptron [1, 5, 6]. Architectural differences of the original and the
modiﬁed model is in the form of membership functions of fuzzy sets, function
t-norm for calculation rules activations of neurons, as well as aggregating function
(t-conorms), determining the activation of output neurons.
The application of numerical optimization methods requires differentiability of
the membership functions of fuzzy sets—condition to which the triangular mem-
bership functions don’t satisfy. Therefore the modiﬁed model of fuzzy sets uses the
Gaussian membership functions, described as
lðxÞ ¼ exp
 ðx  aÞ2
2b2
(
)
:
This membership function is deﬁned by two parameters—a and b. The
requirement of differentiability also dictates the choice of t-norms (intersections) for
calculating neuron activation rules. In the system NEFCLASS for this operation is
used minimum; in the modiﬁed system NEFCLASS-M-product of the corre-
sponding values.
Finally, the kind of aggregate function (t-conorm) for modiﬁed model is limited
only by the weighted sum. The reason consists in the fact that the maximum
function which is used in the original system also does not satisfy the condition of
differentiability. The main change is obviously relates to a learning algorithm of
fuzzy sets. The objective function in the modiﬁed system NEFClass is minimization
of the mean squared error on the training sample by analogy with the classical
(clear) neural networks:
min E ¼ 1
N
X
N
p¼1
aðpÞ
c
 aðpÞ
c

2
where the N—number of patterns in the training sample, aðpÞ
c
is an activation vector of
neurons in the output layer for the next training sample p, aðpÞ
c
is a target value of this
vector for the pattern p. The components of the target vector for the pattern p are equal:
aðpÞ
ij
¼
0;
i 6¼ j
1;
i ¼ j

5.3
Analysis NEFClass Properties. The Modiﬁed System NEFClassM
185

where j is a number of the real class to which this pattern p belongs, i is classiﬁ-
cation of pattern p by NEFClass. The argument of numerical optimization aimed at
reducing MSE for the training set is the aggregate vector of parameters a and b of
FNN. As a speciﬁc training method can be used any method of unconstrained
optimization such as the gradient method or the conjugate gradient method, these
both methods were implemented in this investigation.
5.4
Experimental Studies. Comparative Analysis of FNN
NEFClass and NEFClassM in Classiﬁcation Problems
Experiments were conducted on the classiﬁcation of the two sets of data IRIS and
WBC [5, 6]. Selection of IRIS and WBC test kits was dictated by two considerations:
ﬁrstly, these sets can be considered standard for classiﬁcation problems, and sec-
ondly, in the original works of authors NEFCLASS model was tested on these data
sets [1, 2]. This allows to compare the results of the base system NEFCLASS with a
modiﬁed NEFCLASS_M and estimate the effect of introduced improvements.
IRIS Data Set
IRIS set contains 150 samples belonging to three different classes (Iris Setosa, Iris
Versicolour, and Iris Virginica), 50 samples of each class. Each sample is char-
acterized by four properties [1–3].
IRIS is the only one set by classiﬁcation simplicity for which even a simple
strategy of rules selection gives good results.
In the ﬁrst experiment, in a modiﬁed model NEFClass-M “simple” rules learning
algorithm was used, and their number was limited to 10 with 3 fuzzy sets per
variable (all other parameters were set to the default values). As a result, the system
has created 10 rules and achieved only 4 classiﬁcation errors of the 150 (i.e. 97.3 %
correct) patterns.
The best result, which was managed to achieve with the “simple” rules learning
algorithm is three rules with two essential variables, x3 and x4, and the same order
of misclassiﬁcation (4 errors) [5]:
R1: IF (any, any, large, large) THEN Class 3
R2: IF (any, any, medium, medium) THEN Class 2
R3: IF (any, any, small, small) THEN Class 1
The same result was achieved for the “better” and “best in class” rules learning
algorithms. However, for the last two algorithms it’s possible further reduction in
the number of fuzzy sets for variable x3 and x4 under the following rules (6 erro-
neous classiﬁcation):
R1: IF (any, any, small, small) THEN Class 1
R2: IF (any, any, large, small) THEN Class 2
R3: IF (any, any, large, large) THEN Class 3
186
5
Fuzzy Neural Networks in Classiﬁcation Problems

The authors of model NEFCLASS obtained the similar results, except that in
their experiments, they used three fuzzy sets (linguistic values) for x3 and x4 [1, 2].
Thus, for a set of data IRIS it was managed to achieve better results than in the
original works—exclusively simple set rules of two variables with only two
decomposing sets for each variable.
Dataset WBC
The next test sample for classiﬁcation was standard data sample Wisconsin Breast
Cancer (WBC). When processing sample Wisconsin Breast Cancer using system
NEFClass-M interesting results were obtained which didn’t always coincide with
the results of the basic model NEFCLASS.
Following the course of the experiments by the authors of NEFCLASS [1, 2] for
system training rule base learning algorithm with the “best in the class” (three sets
in the variable). was used with maximum 4 rules. The resulting error of misclas-
siﬁcation obtained for the system NEFClass-M was 28 patterns of 663 (95.7 %
correct) [5]. Very interesting is the fact that for model NEFClass for similar
parameters correct classiﬁcation value was only 80.4 % (135 misclassiﬁcation).
This is a signiﬁcant advantage of the modiﬁed system NEFClass-M which can
be explained by suggested modiﬁcations that distinguish this model from basic
NEFCLASS model, namely, the use of randomization algorithm, the choice of
learning rate and application of numerical algorithm of optimization (gradient
method for MF learning.
The best result that was managed to obtain for the data set WBC is the rule base
of 8 rules with ﬁve essential variables x1, x2, x4, x6 and x9 (misclassiﬁcation—19
errors) [5]:
R1: IF (small, small, any, small, any, small, any, any, small) THEN Class 1
R2: IF (small, small, any, large, any, small, any, any, small) THEN Class 1
R3: IF (small, small, any, small, any, small, any, any, large) THEN Class 1
R4: IF (large, large, any, small, any, large, any, any, small) THEN Class 2
R5: IF (large, large, any, large, any, small, any, any, small) THEN Class 2
R6: IF (small, large, any, small, any, large, any, any, small) THEN Class 2
R7: IF (large, small, any, small, any, small, any, any, small) THEN Class 2
R8: IF (large, small, any, small, any, small, any, any, large) THEN Class 2
Comparable results (24 misclassiﬁcation) were obtained with the use of a
maximum of 2 rules (“the best in the class”) with all the important variables, except
x5 and x7:
R1: IF (small, small, small, small, any, small, any, small, small) THEN Class 1
R2: IF (large, large, large, small, any, large, any, large, small) THEN Class 2
Thus, the results obtained by NEFCLASS-M are superior over basic model
NEFCLASS both in number of rules/signiﬁcant variables and classiﬁcation accu-
racy. This conﬁrms the efﬁciency of the modiﬁcations made to the model
NEFClass: randomization, the correct choice of speed training and application of
numerical optimization algorithms.
5.4
Experimental Studies. Comparative Analysis of FNN NEFClass …
187

5.5
Object Recognition on Electro-Optical Images Using
Fuzzy Neural Networks
5.5.1
General Characteristics of the System
Today, remote sensing includes a variety of imaging techniques in all ranges of the
electro-magnetic spectrum—from ultraviolet to the infrared. When viewing images
obtained from meteorological geostationary satellites, which cover almost the entire
hemisphere, it can be concluded that remote sensing—is the main method of ﬁnding
qualitative relevant information on the state of the Earth’s surface, as well as of
objects located on it.
Remote probing method, usually includes a set of measurements. Using them not
measured parameters of objects, and some value associated with them are detected.
For example, we need to assess the condition of agricultural crops. However, the
satellite apparatus only responds to the light intensity from these objects to certain
sections of the optical range. In order to “decipher” these data it’s necessary to carry
out preliminary investigations, which include a variety of experiments to study crop
conditions by contact methods; to study on reﬂectance in electro-magnetic spectrum
(water, soil, plants) at different locations of the satellite at a different brightness and
parameter measuring device. Next, you must determine what kind of other objects
are displayed in the image, and only then determine the state of the surface (crop,
water).
The width of the surface scanning inspection is a characteristic feature of satellite
methods for studying the Earth, they enable to obtain high-quality images in a
relatively short period of time.
Information about objects studies comes from the satellite, usually in digital
form, and remote sensing system composed of multiple sensors that are sensitive to
the incoming light or sound waves from the object; and a computer that constantly
reads the data from the sensor and saves them to the hard drive. Such a sensor is
usually mounted in a moving object, so that at rotation to sense the necessary
territory. As an example, the system can be powered with a digital camera to
monitor the sea waves. With this system it is possible to obtain the following types
of data: the depth and morphology of the ocean, and the reasonability of using land,
type of ﬂora. These data are invaluable in monitoring the processes such as erosion
of the mountains, natural changes in the soil, construction of cities and changing the
type of green cover of the Earth.
Remote sensing makes it possible to carry out rapid shooting and accumulating
an extensive archive of aerial-imagery. Today, virtually the entire surface of the
Earth’s land (and a signiﬁcant portion of the water surface) is ﬁxed with space
shooting at different viewing conditions (time of year, time of day, and so on.).
188
5
Fuzzy Neural Networks in Classiﬁcation Problems

5.5.2
The Concept of Multi-spectral Electro-Optical Systems
Basic Deﬁnitions
The concept of multi-spectral system is shown in Fig. 5.2. The system captures an
image of the Earth’s surface using a sensor with a two-dimensional detector grid.
The resulting data are substantially continuous spectrum for each picture element
(pixel). Schematically, the system components can be represented as a cube: 3D
three-dimensional dataset—the images which are characterized by three features,
and coordinates and wavelength (spectrum) [6, 9].
The spectral curve shows the relationship between the wavelength and the
reﬂectivity of the object. The shape of the spectral curve can distinguish objects.
For example, the vegetation is highly reﬂective in the near-infrared region and low
in average, compared with the soil. An example is shown in Fig. 5.3.
Using the on-board computers generated image is transmitted to terrestrial sta-
tions for subsequent processing. Image processing algorithms include calibration,
mapping ﬁxes geometric curvature and classiﬁcation of surfaces. In some cases, the
algorithm is used for data processing in real time.
5.5.3
Types of Sensors. Multispectral and Hyperspectral
Systems
Multispectral system—is a system installed on a board of the aircraft or satellite
(sensor) that takes pictures in a certain spectral range and has from 2 to 15 spectral
frequencies. Frequency bands—deﬁned as a part of the spectrum (color) with a
certain width, for example 10 or 50 nm.
Fig. 5.2 The components of remote sensing as a 3D cube
5.5
Object Recognition on Electro-Optical Images …
189

Figure 5.4 shows an example of the spectral range of multispectral sensor.
Multispectral data set is characterized by 5–10 strips of relatively high spectral
width of 70–400 nm. Typically, in a multispectral systems, frequency band images
are not close to each other, so they are not contiguous. They can be broad or narrow,
but a signiﬁcant number of very narrow (about 10 nm). Modern multispectral
Fig. 5.3 Object recognition in the form of the spectral curve
Fig. 5.4 Frequency band of multispectral sensor
190
5
Fuzzy Neural Networks in Classiﬁcation Problems

images have a high spectral extension. For example, at the sensor Aster spectral
extension for low range is from 0.07 to 0.17 μm. The narrower is the spectral range
to which the sensor is sensitive, the narrower is the spectral extension.
Hyperspectral system operation principle is the same as multispectral, but differ
in the number of spectral frequencies. They have tens of hundreds of narrow
adjacent frequency bands. Generally, such systems have 100–200 frequencies on a
relatively small spectral width (5–10 nm). Figure 5.5 is an example of the spectral
range of hyperspectral sensor.
The spectral curves of the image, usually starting with 400 nm from the blue end of
the visible spectrum. These systems measure the range of up to 1100 or even 2500 nm.
5.5.4
Principles of Imaging Systems
In the multispectral and hyperspectral system, electromagnetic radiation is split by a
prism into a plurality of narrow compatible strips. The energy of each band is
Fig. 5.5 The frequency band of hyperspectral sensor
Fig. 5.6 The concept of constructing the image sensor—spectrometer
5.5
Object Recognition on Electro-Optical Images …
191

measured by a separate detector.. The schematic diagram of the construction of the
image sensor—spectrometer is shown in the Fig. 5.6.
Using a scanning mirror, a prism and a set of optical lenses an image enters in
the detector array from which the image is fed into a computer, recorded on the hard
disk and transferred for further processing.
5.6
Application of NEFClass in the Problem of Objects
Recognition at Electro-Optical Images
Using multi-spectral electro system operating in three ranges—red, green and blue
images were obtained of the ocean and the coastal surface. It was required to recognize
objects in the form of geometric shapes on water surface and on the sand. For these
purposes, accounting the complexity of the problem as well as a large level of noise it
was suggested to use fuzzy neural networks, in particular NEFClass. In order to
organize the training of FNN NEFClass a number of learning algorithms were de-
veloped—gradient, conjugate gradient and genetic ones and their efﬁciency was
investigated and compared to the basic training algorithm of the system NEFClass [7].
Gradient Learning Algorithm for NEFClass
For the ﬁrst stage of the algorithm—learning rule base the ﬁrst phase of the basic
algorithm NEFClass is used. The second stage uses a gradient algorithm for training
the feedforward neural network, which is described below [7].
Let the criterion of training fuzzy neural network, which has 3 layers (one hidden
layer), be as follows:
eðWÞ ¼
X
M
i¼1
ti  NETiðWÞ
ð
Þ2! min;
ð5:12Þ
where ti—the desired value of the ith output of neural network;
NETiðWÞ—the actual value of the ith neural network output for the weight
matrix W ¼ WI; WO
½
, WI ¼ Wðx; RÞ ¼ ljðxÞ, WO ¼ WðR; CÞ.
Let activation function for the hidden layer neurons (neurons rules) be such:
OR ¼
Y
N
i¼1
lðiÞ
ji ðxiÞ;
j ¼ 1; . . .qi;
ð5:13Þ
where ljiðxÞ—membership function, which has the form (Gaussian):
lðiÞ
ji ðxÞ ¼ e

ðxajiÞ2
b2
ji ;
ð5:14Þ
and the activation function of neurons in the output layer (weighted sum):
192
5
Fuzzy Neural Networks in Classiﬁcation Problems

OC ¼
P
R2U2 WðR; CÞ  OR
P
R2U2 WðR; CÞ
;
ð5:15Þ
or maximum function:
OC ¼ max WðR; CÞ  OR:
ð5:16Þ
Consider the gradient learning algorithm of fuzzy perceptron.
1. Let WðnÞ—be the current value of the weights matrix. The algorithm has the
following form:
W n þ 1
ð
Þ ¼ W n
ð Þ  cn þ 1rwe W n
ð Þ
ð
Þ;
ð5:17Þ
where cn—the step size at nth iteration;
rweðWðnÞÞ—gradient (direction), which reduces the criterion (5.12).
2. At each iteration, we ﬁrst train (adjust) the input weight W, which depend on the
parameters a and b (see the expression 5.14)
ajiðn þ 1Þ ¼ ajiðnÞ  cn þ 1
@eðWÞ
@aji
;
ð5:18Þ
bjiðn þ 1Þ ¼ bjiðnÞ  c0
n þ 1
@eðWÞ
@bji
;
ð5:19Þ
where c0
n þ 1—step size for parameter b.
@eðWÞ
@aji
¼ 2
X
M
k¼1
tk  NETkðwÞ
ð
Þ  WðR; CÞ
ð
Þ  OR  ðx  ajiÞ
b2
ji
;
ð5:20Þ
@eðWÞ
@bji
¼ 2
X
M
k¼1
tk  NETkðwÞ
ð
Þ  WðR; CÞ
ð
Þ  OR  ðx  ajiÞ2
b3
ji
:
ð5:21Þ
3. We ﬁnd (train) output weight:
@e WO
ð
Þ
@WðR; CkÞ ¼  tk  NETk WO

	

	
OR;
ð5:22Þ
WO
k ðn þ 1Þ ¼ WO
k ðnÞ  c00
n þ 1
@e WO
ð
Þ
@WðR; CkÞ :
ð5:23Þ
5.6
Application of NEFClass in the Problem of Objects Recognition …
193

4. n :¼ n þ 1 and go to the next iteration.
The gradient method is the ﬁrst proposed learning algorithm, it is easy to
implement, but has the disadvantages:
1. converges slowly;
2. only ﬁnds a local extremum.
Conjugate Gradient Method for the System NEFClass
Conjugate gradient algorithm, as well as more general algorithm of conjugate
directions, was used in the ﬁeld of optimization thanks to a wide class of problems
for which it ensures the convergence to the optimal solution for a ﬁnite number of
steps. Its description is considered in the Chap. 1 and isn’t described here.
Genetic Method for Training System NEFClass
Consider the implementation of a genetic algorithm to train NEFCLASS. This
algorithm is a global optimization algorithm. It uses the following mechanisms [8]:
1. crossing-over pairs of parents and generation of descendants;
2. mutation (random effects of the action);
3. the natural selection of the best (selection).
The purpose of training—to minimize the mean square error:
EðWÞ ¼ 1
M
X
M
k¼1
tk  NETkðWÞ
ð
Þ2;
ð5:30Þ
where M is the number of classes; tk is the desired classiﬁcation;
NETkðWÞ—classiﬁcation result of NEFCLASS;
W ¼ WI; WO
½
, WI ¼
wI
ij

 are inputs weights, WO ¼
wO
ij

—output weights.
Any individual (specimen) is described by the appropriate vector of weights W.
Set the initial population of N individuals WIð0Þ; . . .; Wið0Þ; . . .; WNð0Þ
½
.
Calculate the index of ﬁtness (FI), and evaluate the quality of recognition:
FIðWiÞ ¼ C  EðWiÞ ! max;
ð5:31Þ
where C—a constant.
Next step is the crossing of parental pairs. When selecting parents a probabilistic
mechanism is used. Let Pi be the probability of selecting the ith parent
Pi ¼
FIðWið0ÞÞ
PN
i¼1 FIðWið0ÞÞ
;
ð5:32Þ
194
5
Fuzzy Neural Networks in Classiﬁcation Problems

Then the crossing of selected pairs is performed.
It’s possible to apply different mechanisms of crossing. For example: for the ﬁrst
offspring even components of the vector of the ﬁrst parent and the odd components
of the vector of the other parent are taken, and for the second on the contrary:
Wið0Þ  Wkð0Þ ¼ Wið1Þ þ Wkð1Þ
ð5:33Þ
wijð1Þ ¼
wijð0Þ;
if
j ¼ 2m
wkjð0Þ;
if
j ¼ 2m  1

wkjð1Þ ¼
wkjð0Þ;
if
j ¼ 2m
wijð0Þ;
if
j ¼ 2m  1

ð5:34Þ
where Wi ¼ wij


j¼1;R, m ¼ R=2.
Choose N
2 pairs of parents and generate N descendants.
After generating offsprings, the mutation acts on the new population:
w0
ijðnÞ ¼ wijðnÞ þ nðnÞ
ð5:35Þ
where a ¼ const 2 1; þ 1
½
;
nðnÞ ¼ aean; a—mutation rate of extinction;
a—is selected randomly from the interval [0, 1].
Then, after the effect of mutation selection procedure is performed in a popu-
lation, which allows to choose the “ﬁttest” individuals. Different mechanisms of
selection may be used.
1. Complete replacement of the old to the new population.
2. N Selecting the best of all existing species by the criterion of maximum FI
Npar þ Nch.
After the crossing, mutation and selection the current iteration ends. The itera-
tions are repeated until one of the stop criteria will be fulﬁlled.
5.6.1
Experiments to Recognize Objects in the Real Data
For images processing the electro-optical imaging system ENVI was used and its
ability to map, that is, to combine the images of the check points, obtained from the
different spectral cameras [7]. This enables to get a multispectral image. In the
Fig. 5.7 initial data for mapping are shown.
After selecting the 15 control points in the images in different spectrum (this
function is not automated) images are merged and we get the so-called multispectral
cube. The result is shown in Fig. 5.8.
5.6
Application of NEFClass in the Problem of Objects Recognition …
195

Fig. 5.7 Initial data
Fig. 5.8 Multispectral image
196
5
Fuzzy Neural Networks in Classiﬁcation Problems

On the images there were nine different types of surfaces that need to be clas-
siﬁed. For analysis and processing, so-called ROI (Region of Interest) on images
were used. On the image homogeneous region was determined, for example, sand,
water, foam, target red target white color and so on. The result of this detection can
be seen in Fig. 5.9.
Next, using a processing system the mean value and the variance of the selected
region were received. The data obtained were later tabulated.
These data characterize the nine classes of surface areas:
• white target; red target; green target; blue target; yellow target; foam; water; dry
sand; wet sand.
For classiﬁcation of objects it was suggested to use FNN NEFClass_M [5, 7].
These types of surfaces correspond to nine output nodes in the system
NEFClass_M.
The total number of features used to classify the kinds of surfaces is four,
namely:
• the brightness in the red spectrum (RS);
• the brightness in the blue spectrum (BS);
• the brightness in the green spectrum (GS);
• brightness in the infrared spectrum (IS).
The total number of data patterns is 99, 11 for each class.
Present the main statistical characteristics of the data set obtained by multi-
spectral system “Mantis” (Tables 5.1 and 5.2) [9].
Fig. 5.9 Image of ROI
5.6
Application of NEFClass in the Problem of Objects Recognition …
197

To explore the effectiveness of various learning algorithms in the problem of
electro-optical image recognition using NEFClass software kit was developed
named NEFClass—BGCGG (Basic, Gradient, Conjugate Gradient Genetic) [6].
Further experiments were carried out with the software kit NEFClass-BGCGG.
According to the basic principle of model investigation experiments were carried
out by changing only one parameter each time. Of the available 99 patterns 54
patterns served as a training sample. The other 45 patterns were used for testing.
The values of the basic parameters of the simulation algorithm were set to the
starting positions (see Table 5.3).
During the process of training 15 rules was generated presented in Table 5.4.
The dependence of the quality of training on the number of rules that are
generated in the ﬁrst stage was investigated. For an objective assessment of the
results testing on the test sample was performed. For this purpose we varied the
number of rules, starting from 9 to 14. The results are shown in the Table 5.5.
The obtained result is natural, the more rules, the better the results of the test
classiﬁcation.
We have investigated the effect of the terms number in features on the quality of
classiﬁcation. Comparative table is given below (see Table 5.6).
Very interesting result was obtained in this series of experiments [9].
Table 5.1 Statistical characteristics of multispectral system “Mantis”
Evidence
Minimum
Maximum
Average
Pattern
deviation
The correlation between
the symptoms and the
class
Brightness
in the RS
28.81
255.00
165.40
76.14
−0.46
Brightness
in the BS
72.93
255.00
165.43
68.62
−0.32
Brightness
in the GS
44.34
254.89
121.57
57.64
−0.52
Brightness
in the IS
17.03
255.00
140.84
81.58
−0.49
Table 5.2 The correlation between the features
Brightness in
the RS
Brightness in
the BS
Brightness in
the GS
Brightness in
the IS
Brightness in
the RS
1
0.7
0.58
0.95
Brightness in
the BS
1
0.77
0.7
Brightness in
the GS
1
0.59
Brightness in
the IS
1
198
5
Fuzzy Neural Networks in Classiﬁcation Problems

Table 5.3 The values of the
parameters for the program
Parameter
Value
Algorithm generation rules
The best for
the class
The learning algorithm
Classic
Number of generating rules
Maximum
The aggregation function
Weighted sum
The number of terms (values) for each
feature
5 for all
Limiting the intersection of fuzzy sets
1
Speed training for weight coefﬁcients
between the input nodes and the rule nodes
ra ¼ 0; 1
rb ¼ 0; 1
rc ¼ 0; 1
Speed training for weight coefﬁcients
between the rules layer and the output layer
r ¼ 0; 1
The maximum number of epochs
50
Table 5.4 The rule base of a
fuzzy classiﬁer
№
of
rule
№
Feature 1
value
№
Feature 2
value
№
Feature 3
value
№
Feature 4
value
Class
№
1
4
4
4
4
0
2
4
0
1
4
1
3
4
0
0
4
1
4
4
1
0
4
1
5
2
3
1
1
2
6
1
0
1
0
3
7
4
4
1
4
4
8
3
4
3
3
5
9
3
3
2
3
5
10
4
4
3
3
5
11
0
0
0
0
6
12
3
2
1
2
7
13
1
0
0
1
8
14
1
1
0
1
8
15
1
0
0
0
8
Table 5.5 The dependence
of the quality of classiﬁcation
on the number of rules
Number of rules
MSE
True classiﬁcation %
9
13.071009
24
10
9.545608
15
11
9.910701
15
12
9.705482
15
13
4.769655
4
14
4.739224
4
15
4.751657
4
5.6
Application of NEFClass in the Problem of Objects Recognition …
199

From the Table 5.6 it follows that there exists an optimal number of terms that
can be used to describe a collection of data during training. When the number of
terms exceeds this value the number of misclassiﬁed samples increases, that is, by
increasing the complexity of the model error increases.
System training using classical algorithm with the optimal number of terms in
the features was performed. Forms of membership functions for each feature are
shown in Fig. 5.10.
The total sum of squared errors was 2.852081, the number of erroneous clas-
siﬁcations—zero in the training set, while for the test sample MSE was equal to
4.6252, which is not bad result.
Table 5.6 The dependence
of the quality of classiﬁcation
on the number of terms
Number of terms
MSE
True classiﬁcation %
4
5.928639
4
5
4.626252
4
6
4.957257
4
7
5.228448
4
8
5.633563
4
9
6.797175
4
10
7.897521
7
Fig. 5.10 The result of a classic learning algorithm
200
5
Fuzzy Neural Networks in Classiﬁcation Problems

Experiments with the gradient algorithm. The results are shown in Fig. 5.11
(MF of fuzzy sets for each of the four features).
An error at the end of the training was 2.042015, that a little bit better than for
classical method. When testing MSE was 3.786005, and the portion of misclassi-
ﬁcation was 4 %.
Further, the option automatic speed adjustment of MF parameters was included,
that is, we used the algorithm “golden section” for step value optimization. The
results are shown below (Fig. 5.12).
The same experiments were carried out with a conjugate gradient algorithm. The
results are shown in Fig. 5.13.
Further the method of golden section was added to training algorithm. The
results can be seen in Fig. 5.14.
Finally, experiments with a genetic algorithm with different MF—triangular and
Gaussian were carried out [9].
The results of learning using different algorithms are presented in the compar-
ative charts (Fig. 5.15) and Table 5.7. Note that for the training sample excellent
results by the criterion of the percentage of misclassiﬁcation were obtained for all
algorithms.
For all algorithms, this criterion is zero. However, on the test sample, the results
were worse: at least two samples were misclassiﬁed. Also the sum of squared error
Fig. 5.11 Results of the gradient method
5.6
Application of NEFClass in the Problem of Objects Recognition …
201

Fig. 5.13 The result of training by the conjugate gradient
Fig. 5.12 The result of the gradient algorithm in tandem with the “golden section” algorithm
202
5
Fuzzy Neural Networks in Classiﬁcation Problems

Fig. 5.14 The result of training by conjugate gradient method with the selection step by “golden
section” algorithm
Fig. 5.15 Comparative curves of the convergence rate to the optimal classiﬁcation of different
learning algorithms
5.6
Application of NEFClass in the Problem of Objects Recognition …
203

(MSE) for all, without exception, learning algorithms increased. For ease of
comparison, the number of iterations (epochs) has been limited to 50.
As can be seen, the results are satisfactory, the level of correct classiﬁcation on
the test sample is 96 %. These results may be improved by forming a more rep-
resentative sample.
Analyzing the curves in the Fig. 5.15 it can be clearly seen that the best method
for the rate of convergence is the conjugate gradient method. Then the next is a
genetic algorithm with Gaussian function. Less effective is the gradient method.
Next by rate of convergence is classical algorithm used in the system NEFClass.
And at the end of row the least effective is genetic method with a triangular
membership function.
However, the MSE criterion by which the curves were plotted, displays
ambiguously classiﬁcation quality. An important criterion for evaluation of meth-
ods efﬁciency is the minimum number of misclassiﬁed samples. From Table 5.7
one can see that all algorithms show the same results with respect to this criterion.
Conclusions
1. The chapter describes the FNN NEFClass for the classiﬁcation. Its advantages
and disadvantages are analyzed. A modiﬁed system NEFClass_M is considered
that differs by randomization of learning sample, conﬁguration parameters—the
speed of learning and the use of Gaussian MF instead of triangular, allowing to
implement gradient learning algorithm and its modiﬁcations.
2. Comparative experimental study of FNN NEFClass and NEFClass_M demon-
strated the beneﬁts of the latest in quality classiﬁcation (% correct classiﬁcation).
3. Investigations FNN NEFClass in the problem of classiﬁcation of objects of
electro-optical imaging, which led to the following conclusions:
• In the test data the effect of various parameters on the system efﬁciency was
explored. The results showed that the quality of the training system
NEFClass depends on the rate of learning. An increase in the parameter of
Table 5.7 Comparison table for different learning algorithms
The learning
algorithm of
weighting
coefﬁcients
Training
Testing
MSE
Misclassiﬁcation
(%)
MSE
Misclassiﬁcation
(%)
Classic
6.650668
0
7.285827
4
Gradient
5.9893
0
6.829068
4
Conjugate gradient
1.132871
0
3.314763
4
Genetic with
triangular
membership
functions (MF)
11.110936
0
13.677424
4
Genetic with
Gaussian MF
3.204446
0
4.568338
4
204
5
Fuzzy Neural Networks in Classiﬁcation Problems

speed training for weights between input and rule nodes, may improve the
speed of convergence and attain much closer to the minimum point of error
function. However, during experiments, it was observed setting too large
speed is not appropriate, as the phenomenon of “oscillation” occurs.
• Best algorithm for rules generation is “The best for the class.”
5.7
Recognition of Hand-Written Mathematical
Expressions with Application of FNN
Mathematical expressions are widely-used in scientiﬁc and technical literature but
their entering in PC and I-pads with help of such devices as mouse and key board is
rather difﬁcult. The possibility of entering mathematical expression with help of
stylus and sensor screen enables to make this process more natural, convenient and
simple for user. Therefore last years the problem has arisen of hand-written
mathematical expression recognition with application of FNN.
Goal of presented the work was to develop a method and software for
hand-written mathematical expressions recognition which are entered in computer
in on-line mode.
The problems to be solved were the following [7]:
1. The investigation of properties of hand-written mathematical symbols and
expressions.
2. Development and investigation of approach to selection of informative features
describing the properties of hand-written symbols and utilized for constructing
data base of fuzzy neural classiﬁer.
3. Investigation and analysis of training algorithms of fuzzy classiﬁer NEFCLASS.
4. Development of algorithm of hand-written mathematical expressions recogni-
tion using FNN NEFCLASS.
5. Development of software kit implementing the suggested algorithms.
6. Structural analysis of hand-written mathematical expressions
7. The design of information technology of pattern recognition of hand-written
mathematical expressions in on-line mode
Pattern Recognition of Hand-Written Mathematical Expression in On-line Mode
Problem Statement
While pattern recognition on-line it’s possible to track the trajectory of writing
symbol and deﬁne the point of touching pen (an initial symbol point) and a point of
pen non-touching (ﬁnal point) when writing. The input data for pattern recognition
task is sequence of pen coordinates called segment consisting of touching point,
non-touching point and all points lying between them. The pattern recognition
problem consists of two stages:
5.6
Application of NEFClass in the Problem of Objects Recognition …
205

1. Hand-written symbols recognition;
2. Structural analysis which allows to determine the space relations among
symbols.
The division of problem into two stages has some advantages, in particularly,
sub-problems may be solved independently one from another that enables to ana-
lyze the obtained results and make some improvements at each stage preserving by
this the integrity of the whole system (the list of symbols to be recognized is shown
in Table 5.8).
An
Approach
to
Constructing
Informative
Features
Using
Method
of
Approximating Points
The ﬁrst problem to be solved at hand-written mathematical expressions pattern
recognition is selection or construction of informative features of hand-written
symbols. An approach to constructing informative features was suggested based on
extracting so-called approximating points of linearized line approximating a symbol
curve [10].
For ﬁnding approximating points algorithm of Ramer–Douglas–Pecker was
suggested which enables to cut by several degrees the total number of points
approximating the symbol curve preserving its form with maximal accuracy.
The algorithm ﬁnds the point which is maximal distant from the line passing
through the ﬁrst and the last points on the curve. If this point locates at the distance
lesser than ε, then the obtained line approximates the curve with accuracy not
Table 5.8 Table of symbols used for writing hand-written mathematical expressions
At the stage of pattern recognition 
At the stage of structural
symbols consisting of one part
analysis are reconstructed
are considered
symbols consisting of several 
parts,
206
5
Fuzzy Neural Networks in Classiﬁcation Problems

greater than ε. If this distance is greater than ε, then algorithm recurrently repeats at
the pair initial point-obtained point and the pair obtained point—ﬁnal point.
Experimentally was found optimal coefﬁcient of smoothing accuracy by which the
optimal ratio between the number of left points and maximal preserving the form of
initial symbol curve.
Informative Features
For pattern recognition of hand-written symbols was suggested the set of infor-
mative features which describe the properties of hand-written symbols [10] (see
Fig. 5.16)
1. Maximal angle
Find angles bj, which lie between segments of symbol divided by approximating
points and axis X: bj ¼ arccos ~x 
~vj
~vj
k k


where ~x ¼ ð1; 0Þ, vector vj is deﬁned as: vj ¼ api  api1; 2  i  kðapÞ  1,
kðapÞ—total number of approximating points.
Feature maximal angle is deﬁned by formula max bj
2. Average value of angles
Calculate the difference between sequentially located angles bj:
where 2  i  kðapÞ,
Then the feature average angle value can be calculated by formula:
Fig. 5.16 A set of informative features for symbols pattern recognition
5.7
Recognition of Hand-Written Mathematical Expressions …
207

aaver ¼
1
kðapÞ  2
X
kðapÞ2
n¼1
un
3. Linearity
The value of feature linearity can be calculated as a ratio of number of all angles
such un  ej j to a total number of angles un.
4. Topology
Total number of cases where the sequence of angles un, changes the sign from
plus to minus and from minus to plus.
5. Histogram of angles
Divide coordinate plane on 8 sections by 45 grades each. Every angle bj, lies in
one of the sections. For determination of histogram of angles in ith section
divide the number of angles located in it by the total number of angles:
HAi ¼ number of angles in the section i
total number of angles
;
i ¼ 1; . . .; 8
6. Square of segments
For calculating this feature ﬁnd vectors:
~ui ¼ api þ 1  ap1
~vi ¼ api þ 2  ap1
~ai ¼ api þ 2  ap1 þ 1
where 1  i  kðapÞ  2
The square of segments may be calculated by formula (see Fig. 5.17):
S ¼
X
kðapÞ2
i¼1
~ui ~vi
ð
Þ  sgn ~ui ~vi
ð
Þ
~ai
k k
Fig. 5.17 Example of
segments
208
5
Fuzzy Neural Networks in Classiﬁcation Problems

The example of ﬁnding informative features on base of two methods: approxi-
mating points and dominant points is shown below (Fig. 5.18).
Comparative analysis of methods approximating points and dominant points was
performed consisting of the following steps:
1. Test sample was constructed so: it was created ten random variants of writing
each of 70 symbols.
2. For each symbol values of thirteen features based approximating and dominant
points were found.
3. Foranalysisofobtainedresultstheaveragedeviationfromthemeanvaluewasused
d ¼ 1
n
X
n
i¼1
xi  x
where x is a mean arithmetic for results of ten experiments for each informative
feature.
The experimental results of methods of dominant points and approximating
points for 70 symbols are shown in the Fig. 5.19 and presented in the Table 5.9.
Fig. 5.18 Finding of informative features for methods of dominant and approximating points
5.7
Recognition of Hand-Written Mathematical Expressions …
209

For all 70 symbols the average percentage of improvement average deviation
from the mean value using approximating points method was 32.85 % as compared
with DP-method.
Recognition of Hand-Written Math. Symbols
For the efﬁciency estimation of the suggested approach for mathematical symbols
recognition the experiments were carried out with application of FNN NEFCLASS
with different training algorithms and various MF.фyнкций пpинaдлeжнocти.
As inputs of FNN NEFCLASS were used the suggested 13 informative features
[7, 9]:
x_1—Histogram of angles_T1
x_2—Histogram of angles_T2
x_3—Histogram of angles_T3
x_4—Histogram of angles_T4
x_5—Histogram of angles_T5
x_6—Histogram of angles_T6
x_7—Histogram of angles_T7
x_8—Histogram of angles_T8
x_9—Average angle value
x_10—Maximal angle
x_11—Linearity
x_12—Square of segments
Fig. 5.19 The experimental results of dominant points and approximating points methods
application for recognition
210
5
Fuzzy Neural Networks in Classiﬁcation Problems

x_13—Topology
Outputs were 70 symbols to be recognized.
Learning sample: 280 hand-written symbols,
Testing sample: 140 hand-written symbols
Comparative analysis of different training algorithms of NEFCLASS (Fig. 5.20).
For training Gaussian MF was used: lðxÞ ¼ exp  ðxcÞ2
2r2
h
i
For membership functions parameters training the following algorithms were
investigated [11]:
• Gradient algorithm;
• Conjugated gradient algorithm;
• Genetic algorithm.
Table 5.9 Deviations from the mean for methods of approximating points and dominant points
Dominant point
Deviations
from the
mean
Approximating
point
Deviations
from the
mean
% variation from
the mean deviation
Histogram
angles T1
0.84
Histogram
angles T1
0.5
40.47619048
Histogram
angles T2
0.2
Histogram
angles T2
0.18
10
Histogram
angles T3
0.32
Histogram
angles T3
0.3
6.25
Histogram
angles T4
0.42
Histogram
angles T4
0.21
50
Histogram
angles T5
0.82
Histogram
angles T5
0.54
34.14634146
Histogram
angles T6
0.64
Histogram
angles T6
0.46
28.125
Histogram
angles T7
0.5
Histogram
angles T7
0.18
64
Histogram
angles T8
0.6
Histogram
angles T8
0.5
16.66666667
The average
value of the
angles
15.3673
The average
value of the
angles
22.6521
−47.40455383
The maximum
angle
36.0874
The maximum
angle
17.2534
52.18996104
The straightness
0.122
The straightness
0.03584
70.62295082
Area segments
7670.32
Area segments
2458.68
67.94553552
Topology
0.94
Topology
0.62
34.04255319
Av.%
improvement
32.85081887
5.7
Recognition of Hand-Written Mathematical Expressions …
211

The experimental investigations of training algorithms were carried out. On the
Fig. 5.21 the dependence “percentage error on number of iterations” for all training
algorithms is presented.
Fig. 5.20 Architecture of FNN NEFClass
Fig. 5.21 Experimental investigations results of different training algorithms
212
5
Fuzzy Neural Networks in Classiﬁcation Problems

Comparative analysis of different training algorithms of NEFCLASS using
misclassiﬁcation error (%) for different mathematical symbols are presented below
(Figs. 5.22, 5.23, 5.24 and 5.25).
Structural Analysis of Mathematical Expressions
After the stage of symbols recognition the main problem is the determination of the
space relations among the components of mathematical expression. The approach
for structural analysis of mathematical expressions was suggested which consists of
3 stages [7].
Fig. 5.22 Classiﬁcation
results for symbol “∑”
Fig. 5.23 Classiﬁcation
results for symbol “7”
Fig. 5.24 Classiﬁcation
results for symbol “∫”
5.7
Recognition of Hand-Written Mathematical Expressions …
213

Fig. 5.25 Classiﬁcation results for symbol “S”
Stage of symbol location
Stage of symbol 
reconstruction
Stage of symbols grouping
Recognized symbol
+
+
Fig. 5.26 Block-schema of symbols structural analysis
The schema of structural analysis is presented in the ﬁgure below (Fig. 5.26).
The written mathematical expression is looked through after each entered change
that enables to write components of expression in any order and introduce changes
in already written expression.
The symbols are divided into several groups depending on feasible and obli-
gatory positions of location the other symbols relating to them.
The feasible position is acceptable but not obligatory for ﬁlling symbol position.
Obligatory position is position mandatory for ﬁlling symbol position. In the
Table 5.10 are shown feasible and obligatory positions (Fig. 5.27).
For solution of the problem a dynamic base of heuristic rules is used based on
knowledge on writing order and space relations between symbols shown in the
Table 5.11.
Some symbols allow to group several separate symbols into one group. To these
symbols belong
• Various brackets,
• Fractional feature,
• Sum ∑,
• Product ∏,
• Integral,
214
5
Fuzzy Neural Networks in Classiﬁcation Problems

• Arithmetic root,
• Point, comma.
Symbol group is considered as united symbol, for each symbol exist feasible and
obligatory positions of locating the neighbor symbols (see example in Fig. 5.28).
For testing were selected 50 different mathematical expressions including
trigonometric and logarithmic functions, operations of root extracting, division and
so on. The experimental results are presented in the Table 5.12.
In a result of testing the total value of correctly structured mathematical
expressions was obtained. It equals to 78 % of test sample.
Table 5.10 Feasible and obligatory positions for different math. symbols
Group of symbols
Feasible positions
Obligatory positions
0–9, π, ∞
1, 2, 6
∫, ∏, ∑
4, 6, 8
2
√
1, 6, 7
2
Thrigonom. functions
1, 6
2
Logarithmic functions
1, 3, 6
2
Letters
1, 2, 3, 6, 8
Opening brackets
6
2
Closing brackets
2
6
S
M
F
1
2
6
7
8
3
4
5
Fig. 5.27 Examples of obligatory, feasible and not-feasble positions
5.7
Recognition of Hand-Written Mathematical Expressions …
215

The information system for hand-written math. expressions recognition was
developed based on the suggested methods of recognition and structural analysis
[12].
1. Information system is implemented as client-server architecture and contents a
set of software and hardware enabling gathering, storage, processing and
transmission of obtained information.
Table 5.11 Writing order and space positions betweeen symbols
Current
symbol Si
Symbol
Si−1
Position of si
relating to si−1
Symbol
Si−2
Position of si−1
relating to si−2
Result
–
∨
M
8
–
+
4
±
.
ɭ
8
i
–
ɭ
S
t
–
⌠
S
f
.
ȷ
8
j
/
\
M
x
\
/
M
x
>
–
2 or F
→
|
–
F or M
|
S
π
ɭ
–
F or M
ȷ
S
π
ɭ
*
F or M
ȷ
S
π
n
1
6
s
6
sin
s
0
6
c
6
cos
m
i
6
l
6
lim
Fig. 5.28 Examples of obligatory and feasible positions for locating neighbor symbol
Table 5.12 Experimental results of structuring mathematical expressions
General number of
math expressions
Number of erroneously structured
math. expressions
% of erroneously structured
math. expressions
50
11
22
216
5
Fuzzy Neural Networks in Classiﬁcation Problems

2. Information system is used for hand-written mathematical expression recogni-
tion in on-line mode based on suggested methods and algorithms.
3. It is represented in Fig.5.29.
General Results of Experimental Investigations
The total test sample consisted of 140 mathematical expressions.
Total time of recognition of each symbol after training is 150 ms.
The results of ﬁnal experiments are presented in the following Table 5.13.
CLIENTS
Database
WEB Service
Input Data 
Processing
Training
GA Training
CGA 
Training
Event Processing
Event 
Handler
Synchronizer
Recognition
Classifier 
NEFCLASS
Features 
Processing
Structural Analysis
Symbol 
Arrangement
Reconstruction
Grouping 
Expression 
Analyzer
Database Layer
TEX 
Convertor
Output Data 
Processing
Fig. 5.29 Architecture of information system for hand-written math. expressions recognition
5.7
Recognition of Hand-Written Mathematical Expressions …
217

Field “Recognition of separate segments” shows the percentage of correctly
classiﬁed symbols written without break of pen with one script and enables to
estimate recognition efﬁciency (Table 5.13).
Field “Symbols reconstruction” shows the percentage of correctly reconstructed
symbols of correctly recognized segments.
Field “Structural analysis of mathematical expressions” shows the percentage of
correctly structured expressions.
Conclusion
1. The problem hand-written mathematical symbols and expressions recognition in
on-line mode was considered.
2. The properties of hand-written math. symbols were explored and method of
selection of informative features describing the properties of hand-written
symbols was developed and utilized for constructing data base of fuzzy neural
classiﬁer.
3. For classiﬁcation of hand-written math. symbols FNN NEFCLASS was
suggested.
4. The algorithm of hand-written mathematical expressions recognition using
FNN NEFCLASS was developed, the investigation and analysis of training
algorithms of fuzzy classiﬁer were carried out.
5. Structural analysis of hand-written mathematical expressions was performed.
6. Software kit implementing the suggested algorithms was developed and design
of information technology of pattern recognition of hand-written mathematical
expressions in on-line mode was performed.
References
1. Nauck, D., Kruse, R.: Generating classiﬁcation rules with the neuro-fuzzy system
NEFCLASS. In: Proceedings of Biennial Conference of the North American Fuzzy
Information Processing Society (NAFIPS’96). Berkeley (1996)
2. Nauck, D., Kruse, R.: New learning strategies for NEFCLASS. In: Proceedings of Seventh
International Fuzzy Systems Association World Congress IFSA’97, vol. IV, pp. 50–55.
Academia Prague (1997)
3. Nauck, D., Kruse, R.: What are neuro-fuzzy classiﬁers? In: Proceedings of Seventh
International Fuzzy Systems Association World Congress IFSA’97, vol. IV, pp. 228–233.
Academia Prague (1997)
Table 5.13 Results of experimental investigations of hand-written mathematical expressions
recognition
Recognition of
separate segments
Symbols reconstruction (for
correctly recognized segments)
Structural analysis of
mathematical expressions
91.54 %
95.32 %
71.29 %
218
5
Fuzzy Neural Networks in Classiﬁcation Problems

4. Nauck, D.: Building neural fuzzy controllers with NEFCON-I. In: Kruse, R., Gebhardt, J.,
Palm, R. (eds.) Fuzzy Systems in Computer Science. Artiﬁcial Intelligence, pp. 141–151.
Vieweg, Wiesbaden (1994)
5. Zaychenko, Yu.P., Sevaee, F., Matsak, A.V.: Fuzzy neural networks for economic data
classiﬁcation. In: Informatic, Control and Computer Engineering, vol. 42, pp. 121–133.
Vestnik of National Technical University of Ukraine “KPI” (2004) (rus)
6. Zaychenko, Yu.P.: Fuzzy Models and Methods in Intellectual Systems, 354 pp. Kiev
Publishing House, “Slovo” (2008). Zaychenko, Yu.P.: Fuzzy group method of data handling
under fuzzy input data. Syst. Res. Inf. Technol. (3), 100–112 (2007) (rus)
7. Zaychenko, Yu.P., Naderan, E.: Structural analysis of hand-written mathematical expressions.
In: Informatics, Control and Computer Engineering, №54, pp. 12–17. NTUU “KPI” Herald
(2011) (rus)
8. Zaychenko, Yu.P.: Fundamentals of Intellectual Systems Design, 352 pp. Kiev Publishing
House, “Slovo” (2004) (rus)
9. Zaychenko, Yu.P., Petrosyuk, I.M., Jaroshenko, M.S.: The investigations of fuzzy neural
networks in the problems of electro-optical images recognition. Syst. Res. Inf. Technol. (4),
61–76 (2009) (rus)
10. Naderan, E., Zaychenko, Yu.: The extraction of informative features of hand-written
mathematical symbols. In: Informatics, Control and Computer Engineering, №57, pp. 124–
128. Herald of NTUU “KPI” (2012) (rus)
11. Zaychenko, Yu.P., Naderan, E.: The application of fuzzy classiﬁer NEFClass to the problem of
on-line recognition of hand-written mathematical expressions. Herald of Cherkassy State
Technical University, №1, pp. 3–7 (2012) (rus)
12. Naderan, E., Zaychenko, Yu.: Approach to development of the hand-written mathematical
expressions recognition system which are entered in on-line mode. In: Informatics, Control
and Computer Engineering, №58, pp. 56–60. Herald of NTUU “KPI” (2013) (rus)
References
219

Chapter 6
Inductive Modeling Method (GMDH)
in Problems of Intellectual Data Analysis
and Forecasting
6.1
Introduction
This chapter is devoted to the investigation and application of fuzzy inductive
modeling method known as Group Method of Data Handling (GMDH) in problems
of intellectual data analysis (Data Mining), in particularly its application in the
forecasting problem in macroeconomy and ﬁnancial sphere.
The problem consists in forecasting models construction and ﬁnding unknown
functional dependence between given set of macroeconomic indices and forecasted
variable using experimental data.
The advantage of inductive modeling method GMDH is a possibility of con-
structing adequate model directly in the process of algorithm run. The speciﬁcity of
fuzzy GMDH is getting the interval estimates for forecasting variables.
In this chapter the review of main results concerning GMDH and fuzzy GMDH
is presented, analysis of application of various membership functions (MF) and
perspectives of fuzzy GMDH application for forecasting in macroeconomy and
ﬁnancial sphere are estimated.
The Sect. 6.2 contains the problem formulation.
In the Sect. 6.3 main principles and ideas of GMDH are considered. In the
Sect. 6.4 the generalization of GMDH in case of uncertainty—new method fuzzy
GMDH suggested by authors is described which enables to construct fuzzy models
almost automatically. The Sect. 6.5 contains the algorithm of fuzzy GMDH. In the
Sect. 6.6 the fuzzy GMDH with Gaussian and bell-wise membership functions MF
are considered and their similarity with triangular MF is shown. In the Sect. 6.7.
fuzzy GMDH with different partial descriptions in particular orthogonal polyno-
mials of Chebyshev and Fourier are considered.
In the Sect. 6.8 the problem of adaptation of fuzzy models obtained by GMDH is
considered and the corresponding adaptation algorithms are described. The
Sect. 6.9 contains the results of numerous experiments of GMDH and fuzzy
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_6
221

GMDH application for forecasting share prices and Dow Jones index at New York
stock exchange (NYSE). The extension and generalization of fuzzy GMDH in case
of fuzzy inputs is considered and its properties are analyzed in the Sect. 6.11.
6.2
Problem Formulation
A set of initial data is given inclusive: input variables
Xð1Þ; Xð2Þ; . . .; XðNÞ
f
g and
output variables
Yð1Þ; Yð2Þ; . . .; YðNÞ
f
g, where X ¼ x1; x2; . . .; xn
½
 is n-tuple vec-
tor, N is a number of observations.
The task is to synthesize an adequate forecasting model Y ¼ F x1; x2; . . .; xn
ð
Þ,
and besides, the obtained model should have the minimal complexity. In particu-
larly, while solving forecasting problem as an output variable Y a forecasting model
is used X N þ K
ð
Þ ¼ f Xð1Þ; . . .; XðNÞ
ð
Þ, where K is a value of a forecasting interval.
The constructed model should be adequate according to the initial set of data,
and should have the least complexity (Fig. 6.1).
The distinguishing features of the problem are the following:
1. Form of functional dependence is unknown and only model class is determined,
for example, polynomial of any degree or Fourier time series.
2. short data samples;
3. time series xiðtÞ in general case is non-stationary.
In this case the application of conventional methods of statistical analysis (e.g.
regressional analysis) is impossible and it’s necessary to utilize methods based on
computational intelligence (CI). To this class belongs Group Method of Data
Handling (GMDH) developed by acad. Ivakhnenko [1] and extended by his col-
leagues. GMDH is a method of inductive modeling. The method inherits ideas of
biological evolution and its mechanisms:
1. crossing-over of parents and offsprings generation;
2. selection of the best offsprings.
GMDH method belongs to self-organizing methods and allows to discover
internal hidden laws in the appropriate object area.
Fig. 6.1 Graphical
representation of the problem
222
6
Inductive Modeling Method (GMDH) in Problems …

The advantages of GMDH algorithms is the possibility of constructing optimal
models with a small number of observations and unknown dynamics among
variables. This method doesn’t demand to know the model structure a priori, the
model is constructed by algorithm itself in the process of its run.
6.3
The Basic Principles of GMDH
Let’s remind the fundamental principles of GMDH [1, 2]. The full interconnection
between input XðiÞ and output YðiÞ in the class of polynomial models may be
presented by so-called generalized polynomial of Kolmogorov-Gabor:
Y ¼ a0 þ
Xn
i¼1 aixi þ
X
n
j¼1
X
i  j
aijxixj
X
n
i¼1
X
j  i
X
k  j
aijkxixjxk þ   
ð6:1Þ
where all the coefﬁcients a0; ai; aij; are unknown.
While constructing model (search coefﬁcients values) as a criterion of adequacy
the so-called regularity criterion (mean squared error—MSE) is used
e2 ¼ 1
N 
X
N
i¼1
ðyi  f ðXiÞÞ2;
ð6:2Þ
where N is a sample size (number of observations).
It’ s demanded to ﬁnd minimum e2.
GMDH method is based on the following principles [1].
The principle of multiplicity of models. There is a great number of models
providing zero error on a given sample. It’s enough simply to raise the degree of the
polynomial model. If N nodes of interpolation are available, then it’s possible to
construct the family of models each of which gives zero error on experimental
points e2 ¼ 0.
The principle of self-organization. Denote S as model complexity. The value
of an error depends on the complexity of a model. As the level of complexity S
grows the error ﬁrst drops, attains minimum value and then begins to rise (see
Fig. 6.2).
We need to ﬁnd such level of complexity for which the error would be minimal.
In addition if to take into account the action of noise we may make the following
conclusions concerning e:
1. With the increase of noise the optimal complexity s0 ¼ arg min e2 shifts to the
left.
2. With the increase of noise level the value of optimal criterion min e2ðsÞ grows.
Theorem of incompleteness by Geodel: In any formal logical system there are
some statements which cannot be proved or refuted using the given system of
6.2
Problem Formulation
223

axioms and staying in the margins of this system. That to prove or refute such
statement one need go out this system and use some external information (meta
information) which is called “external complement”. In our case as external
information stands additional sample of data which wasn’t used for the ﬁnding
unknown coefﬁcients of the model.
So one way to overcome incompleteness of sample is to use principle of ex-
ternal complement which means that the whole sample should be divided into two
parts—training subsample and test subsample. The search of optimal model is
performed in such a way:
• at the training sample Ntrain the estimates a0; ai; aij; are determined.
• at the test sample Ntest the best models are selected.
The Principle of Freedom of Choice
For each pair of inputs xi and xj partial descriptions are being built (all in all C2
n)
of the form:

Ys ¼ uðxi; xjÞ ¼ a0 þ aixi þ ajxj;
s ¼ 1. . .C2
n linear
ð
Þ;
ð6:3Þ

or Ys ¼ uðxi; xjÞ ¼ a0 þ xi þ ajxj þ aiix2
i þ aijxixj þ ajjx2
j ;
s ¼ 1. . .C2
n quadratic
ð
Þ:
Fig. 6.2 Dependence of
criterion e2 on model
complexity S
224
6
Inductive Modeling Method (GMDH) in Problems …

1. Determine the coefﬁcients of these model using LSM (least square method) at
the training sample (i.e. ﬁnd estimates a0; a1; . . .; aj; . . .; aN; a11; . . .; aij; . . .; aNN.
2. Further at the test sample for each of these models calculate the value of reg-
ularity criterion:
d2
s ¼
1
Ntest

X
Ntestx
k¼1
½YðkÞ  YsðkÞ2
ð6:4Þ
(where YðkÞ is real output value of the kth point of test; YsðkÞ is a value of this
criterion on kth point obtained by model, Ntest is a number of points at the test
sample); as alternate criterion “unbiasedness” criterion may be used:
Nub ¼
1
N1 þ N2
X
N
k¼1
y
k  y
k

2;
ð6:5Þ
where the sample is also divided in two parts N1 and N2, y
k are outputs of the
model built on the subsample N1, y
k are outputs of model built on subsample
N2, N ¼ N1 þ N2.
3. Determine F (this number is called a freedom of choice) best models using one
of these criteria. The selected models yi are then transferred to the second row of
model construction. We search coefﬁcients of new partial descriptions:
zI ¼ uð2Þðyi; yjÞ ¼ að2Þ
0 þ að2Þ
1 yi þ að2Þ
2 yj þ að2Þ
3 y2
i þ að2Þ
4 yiyj þ að2Þ
5 y2
j :
The process at the second row runs in the same way. The selection of the best
models is carried out similarly, but F2\F1. The process of rows construction
repeats more and more till MSE (regularity criterion) falls. If at the mth layer the
increase of the error e2 occurs, the algorithm stops. In this case we ﬁnd the best
model at the preceding layer and then moving backward by its connections ﬁnd
models of preceding layer and successfully passing all the used connections at the
end we’ ll reach the ﬁrst layer and ﬁnd the analytical form of the optimal model
(with minimal complexity).
6.4
Fuzzy GMDH. Principal Ideas. Interval Model
of Regression
The drawbacks of GMDH are following:
1. GMDH utilizes least squared method (LSM) for ﬁnding the model coefﬁcients
but matrix of linear equations may be close to degenerate and the corresponding
6.3
The Basic Principles of GMDH
225

solution may appear non-stable and very volatile. Therefore, the special methods
for regularisation should be used;
2. after application of GMDH point-wise estimations are obtained but in many
cases it’s needed ﬁnd interval value for coefﬁcient estimates;
3. GMDH doesn’t work in case of incomplete or fuzzy input data.
Therefore, in the last 10 years the new variant of GMDH—fuzzy GMDH was
developed and reﬁned which may work with fuzzy input data and is free of classical
GMDH drawbacks [3].
In works [3, 4] the linear interval model regression was considered:
Y ¼ A0Z0 þ A1Z1 þ    þ AnZn;
ð6:6Þ
where Ai is a fuzzy number of triangular form described by pair of parameters
Ai ¼ ai; ci
ð
Þ, where ai is interval center, ci is its width, ci  0.
Then Y is a fuzzy number, parameters of which are determined as follows:
the interval center
ay ¼
X
aizi ¼ aT  z;
ð6:7Þ
the interval width
cy ¼
X
ci  zi
j j ¼ cT zj j:
ð6:8Þ
In order that the interval model be correct it’s necessary that real value of output
should belong to the interval of uncertainty described by the following constraints:
aTz  cT  zj j  y
aTz þ cT  zj j  y
(
:
ð6:9Þ
For example, for the partial description of the kind
f ðxi; xjÞ ¼ A0 þ A1xi þ A2xj þ A3xixj þ A4x2
i þ A5x2
j
ð6:10Þ
it’s necessary to assign in the general model (6.6)
z0 ¼ 1 z1 ¼ xi z2 ¼ xj z3 ¼ xixj z4 ¼ x2
i z5 ¼ x2
j
Let the training sample be fz1; z2; . . .; zMg, fy1; y2; . . .; yMg. Then for the model
(6.10) to be adequate its necessary to ﬁnd such parameters ai; ci
ð
Þ i ¼ 1; n, which
satisfy the following inequalities:
226
6
Inductive Modeling Method (GMDH) in Problems …

aTzk  cT  zk
j j  yk
aTzk þ cT  zk
j j  yk

;
k ¼ 1; M:
ð6:11Þ
Let’s formulate the basic requirements for the linear interval model of partial
description of a kind (6.10).
It’s necessary to ﬁnd such values of the parameters ai; ci
ð
Þ of fuzzy coefﬁcients
for which:
1. real values of the observed outputs yk would drop in the estimated interval for
Yk;
2. the total width of the estimated interval for all sample points would be minimal.
These requirements lead to the following linear programming problem:
min C0  M þ C1
X
M
k¼1
xki
j
j þ C2
X
M
k¼1
xkj

 þ C3
X
M
k¼1
xkixkj

 þ
 
þ C4
X
x2
ki

 þ C5
X
M
k¼1
x2
kj


!
ð6:12Þ
under constraints:
a0 þ a1xki þ a2xkj þ a3xkixkj þ a4x2
ki þ a5x2
kj  C0 þ C1 xki
j
j þ C2 xkj



þ C3 xkixkj

 þ C4 x2
ki

 þ C5 x2
kj



 yk
ð6:13Þ
a0 þ a1xki þ a2xkj þ a3xkixkj þ a4x2
ki þ a5x2
kj þ C0 þ C1 xki
j
j þ C2 xkj



þ
þ C3 xkixkj

 þ C4 x2
ki

 þ C5 x2
kj



 yk
k ¼ 1; M;
Cp  0;
p ¼ 0; 1; . . .; 5
ð6:14Þ
where k is an index of a point.
As we can easily see the task (6.12)–(6.14) is linear programing (LP) problem.
However, the inconvenience of the model (6.12)–(6.14) for the application of
standard LP methods is that there are no constraints of non-negativeness for vari-
ables ai. Therefore for its solution it’s reasonable to pass to the dual LP problem by
introducing dual variables fdkg and fdk þ Mg, k ¼ 1; M. Using simplex-method for
the dual problem and after ﬁnding the optimal values for the dual variables fdkg the
optimal solutions ðai; ciÞ of the initial direct problem will be also found.
6.4
Fuzzy GMDH. Principal Ideas …
227

6.5
The Description of Fuzzy Algorithm GMDH
Let’s present the brief description of the algorithm FGMDH [2, 3].
1. Choose the general model type by which the sought dependence will be
described.
2. Choose the external criterion of optimality (criterion of regularity or
non-biasedness).
3. Choose the type of partial descriptions (for example, linear or quadratic one).
4. Divide the sample into training Ntrain and test Ntest subsamples.
5. Put zero values to the counter of model number k and to the counter of rows r
(iterations number).
6. Generate a new partial model fk (6.10) using the training sample. Solve the LP
problem (6.12)–(6.14) and ﬁnd the values of parameters ai, ci.
7. Calculate using test sample the value of external criterion (NðrÞ
ub or dð2Þ
k ðrÞ).
8. k ¼ k þ 1. If k [ C2
N for r = 1 or k [ C2
F for r > 1, then k ¼ 1, r ¼ r þ 1 and
go to step 9, otherwise go to step 6.
9. Calculate the best value of the criterion for models of rth iteration. If r ¼ 1,
then go to step 6 otherwise, go to step 10.
10. If NubðrÞ  Nubðr  1Þ

  e or dð2ÞðrÞ  dð2Þðr  1Þ, then go 11,
otherwise select F best models and assigning r ¼ r þ 1, k ¼ 1, go to step 6 and
execute (r + 1)th iteration.
11. Select the best model out of models of the previous row (iteration) using
external criterion.
6.6
Analysis of Different Membership Functions
In the ﬁrst papers devoted to fuzzy GMDH [2, 3] the triangular membership
functions (MF) were considered. But as fuzzy numbers may also have the other
kinds of MF it’s important to consider the other classes of MF in the problems of
modeling using FGMDH. In the paper [4] fuzzy models with Gaussian and
bell-shaped MF were investigated.
Consider a fuzzy set with MF of the form:
lBðxÞ ¼ e1
2ðxaÞ2
c2 ;
ð6:15Þ
B is a fuzzy number with Gaussian MF. Such fuzzy number is given by pair
b ¼ ða; cÞ, where a-is a center and c is a value determining the width of the interval
(Fig. 6.3).
228
6
Inductive Modeling Method (GMDH) in Problems …

Let the linear interval model for partial description of FGMDH take the form
(6.15). Then the problem is formulated as follows: ﬁnd such fuzzy numbers Bi, with
parameters ai; ci
ð
Þ, that:
1. the observation yk would belong to a given estimate interval for the set YðkÞ
with degree not less than a, 0\a\1;
2. the width of estimated interval of the degree a would be minimal;
The width of the interval of degree a is equal (see Fig. 6.3):
da ¼ y2  y1 ¼ 2ðy2  aÞ;
where ðy2  aÞ is determined from the condition:
a ¼ exp
 1
2  y2  a
ð
Þ2
c2
(
)
ð6:16Þ
Hence da ¼ 2c 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
.
So the goal function may be written as follows:
min
X
M
k¼1
dk
a ¼ min
X
M
k¼1
2Ck
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
¼ 2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
min
X
M
k¼1
X
n
i¼1
Ci  zik
j
j:
ð6:17Þ
As 2 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
is a positive constant not inﬂuencing on the set of the values ci,
which minimize (6.17), it’s possible to divide the goal function by this constant and
transform the goal function to the initial form (6.12).
Now consider the ﬁrst demand: lðykÞ  a.
It’s equivalent to exp  1
2  ðykakÞ2
c2
k
n
o
 a.
Fig. 6.3 Subset of a level
6.6
Analysis of Different Membership Functions
229

This inequality may be transformed to the following system of inequalities:
ak þ ck
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
 yk
ak  ck
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
 yk
:
ð6:18Þ
Taking into account ak ¼ P
n
i¼1
aizki; ck ¼ P
n
i¼1
ci zki
j
j, the problem of ﬁnding optimal
fuzzy model will be ﬁnally transform to the following LP problem:
min C0  M þ C1
X
M
k¼1
xki
j
j þ C2
X
M
k¼1
xkj

 þ C3
X
M
k¼1
xkixkj

 þ
 
þ C4
X
M
k¼1
x2
ki

 þ C5
X
M
k¼1
x2
kj


!
;
ð6:19Þ
under constraints:
a0 þ a1xki þ    þ a5x2
kj þ ðC0 þ C1 xki
j
j þ    þ C5 x2
kj

Þ 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
 yk
a0 þ a1xki þ    þ a5x2
kj  ðC0 þ C1 xki
j
j þ    þ C5 x2
kj

Þ 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
 yk
9
>
=
>
;
k ¼ 1; M
ð6:20Þ
That to solve this problem like in the case with triangular MF pass to the dual LP
problem of the form:
max
X
M
k¼1
yk  dk þ M 
X
M
k¼1
yk  dk
 
!
;
ð6:21Þ
with constraints of equalities and inequalities:
X
M
k¼1
dk þ M 
X
M
k¼1
dk ¼ 0;
X
M
k¼1
Xki  dk þ M 
X
M
k¼1
Xkii  dk ¼ 0
. . .
X
M
k¼1
X2
kj  dk þ M 
X
M
k¼1
X2
kj  dk ¼ 0
ð6:22Þ
230
6
Inductive Modeling Method (GMDH) in Problems …

P
M
k¼1
dk þ P
M
k¼1
dk þ M 
M
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
P
M
k¼1
Xki
j
j  dk þ M þ P
M
k¼1
Xki
j
j  dk 
P
M
k¼1
Xki
j
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
. . .
P
M
k¼1
X2
kj

  dk þ M þ P
M
k¼1
X2
kj

  dk 
P
M
k¼1
X2
kj
j
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln a
p
9
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
;
ð6:23Þ
dk  0; k ¼ 1; 2M:
Thus, fuzzy GMDH allows to construct fuzzy models and has the following
advantages:
1. The problem of optimal model determination is transferred to the problem of
linear programming, which is always solvable.
2. There is interval regression model built as the result of method work;
3. There is a possibility of the obtained model adaptation.
6.7
Fuzzy GMDH with Different Partial Descriptions
6.7.1
Investigation of Orthogonal Polynomials as Partial
Descriptions
As it well-known from the general GMDH theory, models pretendents are gener-
ated on the base of so-called partial description-elementary models of two variables
(e.g. linear polynomials) which utilize as arguments the partial descriptions of
preceding stages for optimal model synthesis. But the choice of partial description
form is not automated and is to be solved the expert.
Usually as partial descriptions linear or quadratic polynomials are used. The
alternative to this class of models is application of orthogonal polynomials. The
choice of orthogonal polynomials as partial descriptions is determined by the fol-
lowing advantages:
• Owing to orthogonal property the calculation of polynomial coefﬁcients which
approximates the simulated process goes faster than for non-orthogonal
polynomials.
• The coefﬁcients of polynomial approximating equation don’t depend on the
degree of initial polynomial model so if apriori the real polynomial degree is not
known we may check the polynomials of various degrees and by this property
the coefﬁcients obtained for polynomials of lower degrees remain the same after
transfer to higher polynomial degrees. This property is the most important
during investigation of real degree of approximating polynomial. One of the
properties of orthogonal polynomials widely used in this work is the property of
6.6
Analysis of Different Membership Functions
231

almost equal errors. This property means that approximation errors oscillates in
the middle of estimation interval between two almost equal bounds (margins).
Owing to this the very large errors do not happen, on the contrary, in most cases
the error values are small. Therefore the damping of approximation errors
occurs.
Main Concepts
Consider the approximating equation for one-dimensional system
yðxÞ ¼ b0F0ðxÞ þ b1F1ðxÞ þ    þ bmFmðxÞ;
ð6:24Þ
where y is initial (real) value which is estimated,
FmðxÞ is orthogonal polynomial of degree v, which has the orthogonal property,
i.e.
X
r
i¼0
FlðxiÞFmðxiÞ ¼ 0;
8l 6¼ m;
ð6:25Þ
r is a length of training sample.
In the general case (for continuous variables)
Zb
a
xðxÞFlðxÞFmðxÞdx ¼ 0;
ð6:26Þ
where µ, m are nonnegative integer numbers (polynomial degree);
xðxÞ is a weighting coefﬁcient.
Investigation of Chebyshev’s Orthogonal Polynomials as Partial Descriptions
Chebyshev’s orthogonal polynomials in general case have the following form [5]:
FmðnÞ ¼ TmðnÞ ¼ cosðm  arccos nÞ; 1  n  1;
ð6:27Þ
These polynomials have the following orthogonality property
Z1
1
TlðnÞTmðnÞdn
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  n2
p
¼
0
if
l 6¼ n;
p
2
if
l ¼ n 6¼ 0;
p
if
l ¼ n ¼ 0:
8
<
:
;
ð6:28Þ
where
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  n2
p
is a weighting coefﬁcient xðnÞ in the Eq. (6.26).
Let’s present several Chebyshev’s orthogonal polynomials of lower degrees:
232
6
Inductive Modeling Method (GMDH) in Problems …

T0ðnÞ ¼ 1
T1ðnÞ ¼ n
T2ðnÞ ¼ 2n2  1
Tm þ 1ðnÞ ¼ 2n  TmðnÞ  Tm1ðnÞ
The approximating Chebyshev’s orthogonal polynomial for y is obtained on the
base of function S minimization.
S ¼
Z1
1
xðnÞ yðnÞ 
X
m
i¼0
biTiðnÞ
 
!2
dn:
ð6:29Þ
where from it follows:
bk ¼
1
p
R1
1
yðnÞ
ﬃﬃﬃﬃﬃﬃﬃﬃ
1n2
p
dn;
k ¼ 0
2
p
R1
1
yðnÞTkðnÞ
ﬃﬃﬃﬃﬃﬃﬃﬃ
1n2
p
dn;
k 6¼ 0
8
>
>
>
<
>
>
>
:
:
ð6:30Þ
Hence the approximating equation is obtained
yðnÞ ¼
X
m
k¼0
bkTkðnÞ:
ð6:31Þ
As it may be readily seen from the presented expressions coefﬁcient bk in the
Eq. (6.30) doesn’t depend on choice of degree m. Thus, the variable m doesn’t
demand recalculation of bj; 8j  m, while such recalculation is necessary for
non-orthogonal approximation.
Determining the Best Degree of Approximating Polynomial
The best degree m of approximating may be obtained on the base of hypothesis
that investigation results yðiÞ; i ¼ 1; 2; . . .; r, have independent Gaussian distribu-
tion in the bounds of some polynomial function y of deﬁnite degree for example,
m þ l, where
Table 6.1 Testing sample for BP plc (ADR)
Data
BP plc (ADR)
Nov 15, 2011
43.04
Nov 16, 2011
41.78
Nov 17, 2011
41.60
6.7
Fuzzy GMDH with Different Partial Descriptions
233

ym þ lðxiÞ ¼
X
m þ l
j¼0
bjx j
i ;
ð6:32Þ
and a dispersion r2 of distribution y  y doesn’t depend on l.
It’s clear that for very small m (m = 0, 1, 2, …) r2
m decreases as m grows.
As in accordance with previously formulated hypothesis dispersion doesn’t
depend on m, the best degree m is a minimal m, for which rm ﬃrm þ 1.
For determining m it’s necessary to calculate the approximating polynomials of
various degrees. As coefﬁcients bj in the Eq. (6.32) don’t depend on m, the
determination of the best degree of polynomial is accelerated.
Constructing Linear Interval Model
Let we have the forecasted variable Y and input variables x1; x2; . . .xn. Let’s search
the relation between them in the following form
Y ¼ A1f1ðx1Þ þ A2f2ðx2Þ þ    þ AnfnðxnÞ;
ð6:33Þ
where Ai is a fuzzy number of triangular type given as Ai ¼ ai; ci
ð
Þ, functions fi are
determined so [2]:
fiðxiÞ ¼
X
mi
j¼0
bi jTjðxiÞ:
ð6:34Þ
The degree mi of function fi is determined using hypothesis deﬁned in the
preceding section. So if denote zi ¼ fiðxiÞ, we’ll get linear interval model in its
classical form.
6.7.2
Investigation of Trigonometric Polynomials as Partial
Descriptions
General Concepts
Let we have periodic function with period P, P [ 0:
gðx þ PÞ ¼ gðxÞ
ð6:35Þ
In, general we may consider any periodic function which has period 2p:
f ðxÞ ¼ g Px
2p

	
;
f ðx þ 2pÞ ¼ g Px
2p þ P

	
¼ f ðxÞ
ð6:36Þ
234
6
Inductive Modeling Method (GMDH) in Problems …

Let function f ðxÞ be periodic with period 2p deﬁned at the interval
½p; p, and its derivative f 0ðxÞ is also deﬁned at the interval ½p; p. Then the
following equality holds
SðxÞ ¼ f ðxÞ; 8x 2 ½p; p
ð6:37Þ
where
SðxÞ ¼ a0
2 þ
X
1
j¼1
ðaj cosðjxÞ þ bj sinðjxÞÞ:
ð6:38Þ
Coefﬁcients aj, bj are calculated by Euler formulas
aj ¼ 1
p
Zp
p
f ðxÞ cosðjxÞdx;;
bj ¼ 1
p
Zp
p
f ðxÞ sinðjxÞdx;
ð6:39Þ
Trigonometric Polynomials
Deﬁnition. A trigonometric polynomial of the degree M is called the following
polynomial
TMðxÞ ¼ a0
2 þ
X
M
j¼1
ðaj cosðjxÞ þ bj sinðjxÞÞ:
ð6:40Þ
The following theorem is true stating that exists such 9M; 2M\N, which
minimizes the expression:
X
N
j¼1
ðf ðxiÞ  TMðxiÞÞ2:
ð6:41Þ
Hence the coefﬁcients of corresponding trigonometric polynomial are deter-
mined by formulas:
aj ¼ 2
N
X
N
i¼1
f ðxiÞ cosðjxiÞ;
bj ¼ 2
N
X
N
i¼1
f ðxiÞ sinðjxiÞ:
ð6:42Þ
6.7
Fuzzy GMDH with Different Partial Descriptions
235

Constructing a Linear Interval Model
Let it be the variable Y to be forecasted and input variables x1; x2; . . .xn. Let’s search
the dependence among them in the form:
Y ¼ A1f1ðx1Þ þ A2f2ðx2Þ þ    þ AnfnðxnÞ;
ð6:43Þ
where Ai is a fuzzy number of triangular type given as Ai ¼ ai; ci
ð
Þ, functions fi are
determined in such a way:
fiðxiÞ ¼ TMiðxiÞ:
ð6:44Þ
The degree Mi of function fi is determined by the theorem described in the
preceeding section.
Therefore if to assign zi ¼ fiðxiÞ, the linear interval model will be obtained in its
classical form.
Note before start of modeling initial data should be normalized at the interval
½p; p
6.7.3
The Investigation of ARMA Models as Partial
Descriptions
General Concepts
The choice autoregression models with moving average (ARMA) for investi-
gation is based on their wide applications for modeling and forecasting in economy.
Consider the classical ARMA ðn; mÞ-model. As a rule it is described in the form
of difference equation:
yðkÞ þ a1yðk  1Þ þ    þ anyðk  nÞ ¼
¼ b1uðk  1Þ þ    þ bmuðk  mÞ þ mðkÞ:
ð6:45Þ
or in the form:
yðkÞ ¼ b1uðk  1Þ þ    þ bmuðk  mÞ
 a1yðk  1Þ      anyðk  nÞ:
ð6:46Þ
In vector form this model may be presented, as follows
yðkÞ ¼ hTwðkÞ þ mðkÞ;
ð6:47Þ
where hT ¼ a1. . .an b1. . .bm
½
 is vector of model parameters;
wTðkÞ ¼ yðk  1Þ  yðk  2Þ      yðk  nÞ uðk  1Þ. . .uðk  mÞ
½

is
mea-
surement vector;
236
6
Inductive Modeling Method (GMDH) in Problems …

yðkÞ is dependent variable, uðkÞ is independent variable, vðkÞ is random noise.
Model parameter vector is estimated with any identiﬁcation method.
Constructing Linear Interval Model
Let h be a vector of fuzzy parameters (triangular form). Then in the Eq. (6.47) the
noise is missing and linear interval model in its classical form is obtained
Y ¼ A0z0 þ A1z1 þ    þ Apzp;
where p ¼ n þ m;
Ai ¼
ai;
i ¼ 1; . . .; n
bin;
i ¼ n þ 1; . . .; n þ m

:
For constructing the model of optimal complexity (in our case it’s the deter-
mination of n, m and model parameters) the multilevel polynomial GMDH algo-
rithm may be used.
At the ﬁrst level a set fuzzy model-pretendents is generated in the class ARMA
ði; jÞ, where i ¼ 1; . . .; n0; j ¼ 1; . . .; m0, n0, m0 are varied. From this set using
selection procedure the best models are selected and transferred to the next level of
synthesis etc. Evidently, if to use ARMA—models as arguments in the next level
model also ARMA-models will be obtained.
As a result of GMDH algorithm application ARMA-model (n; m) will be
obtained, where n* and m* are optimal values in the sense of GMDH.
6.8
Adaptation of Fuzzy Models Coefﬁcients
While forecasting by self-organizing methods (fuzzy GMDH, in particular) the
problem arises connected with necessity of huge amount of repetitive calculations
in case of the training sample size increase or while forecasting in real time when
it’s needed to correct the obtained model in accordance with new available data. In
this work for solution of this problem the following adaptation methods are sug-
gested: stochastic approximation and recurrent LSM method. The possibility of
choice one of several adaptation methods enables to carry out more wide explo-
rations and make grounded recommendations concerning the properties and
application sphere of each method.
Taking into account new information obtained while forecasting may be done by
two approaches. The ﬁrst one is to correct parameters of a forecasting model with new
data assuming that model structure didn’t change. The second approach consists in
adaptation of not only model parameters but its optimal structure as well. This way
demands the repetitive use of full GMDH algorithm and is connected with huge
volume of calculations. The second approach is used if adaptation of parameters
doesn’t provide good forecast and the new real output values don’t drop in the cal-
culated interval for its estimate.
6.7
Fuzzy GMDH with Different Partial Descriptions
237

In our consideration the ﬁrst approach is used based on adaptation of FGMDH
model parameters with new available data. Here the recursive identiﬁcation
methods are preferably used, especially the recursive least squared method (LSM).
In this method the parameters estimates on the next step are determined on the base
of estimates on the previous step, model error and some information matrix which is
modiﬁed during all estimation process and therefore contains data which may be
used at the next steps of adaptation process.
Hence, model coefﬁcients adaptation will be simpliﬁed substantially. If to store
information matrix obtained while identiﬁcation of optimal model using fuzzy
GMDH then for model parameters adaptation it will be enough to fulﬁll only one
iteration by recursive LSM method.
6.8.1
Application of Stochastic Approximation Method
for Adaptation
Consider a discrete stationary system
yk ¼ UðUk; pÞ;
ð6:48Þ
where Uk is a vector of input model variables,
p is a parameters vector to be estimated,
U is a given function.
The vector estimate p
_
n þ 1 on the step(n + 1) is deﬁned in such a way:
p
_
n þ 1 ¼ p
_  qnWn
where Wn is a function which depends on Un and y,
qn is a sequence of scalar correcting coefﬁcients.
Let Jnð^pnÞ be scalar quality index of identiﬁcation which is deﬁned so [4]:
Jnð^pnÞ ¼ 1
2 ðyn UðUn; ^pnÞÞ2
ð6:49Þ
Then
Wn ¼ @Jnðp
_
nÞ
@pn
¼
@Jnðp
_
nÞ
@pn1
. . .
@Jnðp
_
nÞ
@pnm
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
:
ð6:50Þ
238
6
Inductive Modeling Method (GMDH) in Problems …

Here p
_
n1; . . .; p
_
nm are components of a vector p
_
n.
From the expression (6.50) we obtain:
@Jnðp
_
nÞ
@pn
¼ ðUðUn; p
_
nÞ  ynÞ  gðUn; p
_
nÞ;
ð6:51Þ
where
gðUn; p
_
nÞ ¼ @UðUn; p
_
nÞ
@p
_
n
:
ð6:52Þ
In the case of linear interval model we have [6]:
UðUn; p
_
nÞ ¼ p
_T
nUn
ð6:53Þ
where p
_
n ¼ a1 a2. . . am
ð
Þ or p
_
n ¼ C1 C2. . . Cm
ð
Þ;
Un ¼ z1 z2. . . zm
ð
Þ or Un ¼ jz1j jz2j. . . jzmj
ð
Þ;
for adaptation of ai or Ci correspondingly.
Then
gðUn; p
_
nÞ ¼ @UðUn; p
_
nÞ
@p
_
n
¼ Un;
ð6:54Þ
and we obtain:
Wn ¼
UðUn; p
_
nÞ  yn


 Un:
ð6:55Þ
The ﬁrst of scalar correcting coefﬁcients is determined so:
q1 ¼
1
gðUn; p
_
nÞ

2 :
ð6:56Þ
In the case of linear interval model:
q1 ¼
1
Un
k
k2 ¼
1
UT
n Un
:
ð6:57Þ
Thus, stochastic approximation represents the convenient tools for recursive
identiﬁcation of model parameters which may be linearized and applied for model
coefﬁcients adaptation obtained by fuzzy GMDH. Adaptation procedure applies to
parameters ai, as well as to Ci.
In software implementation coefﬁcients adaptation is fulﬁlled by expert in
step-mode forecasting or by automatic forecasting procedure for one or several
steps ahead using the following principle: the adaptation takes place only in the case
6.8
Adaptation of Fuzzy Models Coefﬁcients
239

when the real value of output variable drops out of interval bounds for estimate
obtained by a model.
The main disadvantage of this algorithm is that while constructing the ﬁrst of the
sequence of correcting coefﬁcients the information obtained in the process of
parameters estimation using fuzzy GMDH algorithm is not used.
Therefore it’s desirable to modify the algorithm of parameters estimation so that
output of fuzzy GMDH algorithm for optimal model altogether with the data
accumulated during the process of estimation were used. Such approach is con-
sidered in the next section.
6.8.2
The Application of Recurrent LSM for Model
Coefﬁcients Adaptation
Consider the following model:
yðkÞ ¼ hTWðkÞ þ vðkÞ;
ð6:58Þ
where yðkÞ is a dependent (output) variable,
WðkÞ is a measurements vector,
vðkÞ are random disturbances,
h—is a parameters vector to be estimated.
The parameters estimate h at the step N is performed due to such formula [2, 4, 7]:
h
_
ðNÞ ¼ h
_
ðN  1Þ þ cðNÞ½yðNÞ  h
_T
ðN  1ÞWðNÞ;
ð6:59Þ
where cðNÞ—is a coefﬁcients vector which is determined by formula:
cðNÞ ¼
PðN  1ÞWðNÞ
1 þ WTðNÞPðN  1ÞWðNÞ ;
ð6:60Þ
where PðN  1Þ is so-called an “information matrix”, determined by formula:
PðN  1Þ ¼ PðN  2Þ  PðN  2ÞwðN  1Þ wTðN  1ÞPðN  2Þ
1 þ wTðN  1ÞPðN  2ÞwðN  1Þ
:
ð6:61Þ
Linear Interval Model Parameters Adaptation
As one can easily see in (6.61), the information matrix may be obtained indepen-
dent on parameters estimation process and parallel to it. The adaptation of two
parameter vectors hT
1 ¼ a1; . . .; am
½
; hT
2 ¼ C1; . . .; Cm
½
; is performed in such a way
using the formulas [4]:
240
6
Inductive Modeling Method (GMDH) in Problems …

^h1ðNÞ ¼ ^h1ðN  1Þ þ c1ðNÞ yðNÞ  ^hT
1ðN  1ÞW1ðNÞ
j
k
^h2ðNÞ ¼ ^h2ðN  1Þ þ c2ðNÞ ycðNÞ  ^hT
2ðN  1ÞW2ðNÞ
h
i ;
ð6:62Þ
where WT
1 ¼ ½z1; . . .; zm;
WT
2 ¼ ½jz1j; . . .; jzmj;
ycðNÞ ¼ jyðNÞ  hT
1ðN  1ÞW1ðNÞj:
6.9
The Application of GMDH for Forecasting
at the Stock Exchange
Consider the application of GMDH and fuzzy GMDH for forecasting at the stock
exchange NYSE. As input variables the following stock prices at NYSE were
chosen: close prices of companies Hess Corporation, Repsol YPF, S.A. (ADR),
Eni S.p.A. (ADR), Exxon Mobil
Corporation, Chevron Corporation,
and
TOTAL S.A. (ADR) [5].
As an output variable close prices of British Petroleum—BP plc (ADR) were
chosen.
The initial data for training were taken in the period from 20 September
2011 year till 14 November 2011 year. For testing were used the data of company
BP plc (ADR) since 15 November 2011 to 17 November 2011 year presented in the
Table 6.1.
The input variables were chosen after correlation matrix analysis.
The second problem was forecasting industrial index Dow-Jones Average. As
the input variables in this problem were taken close stock prices of the following
companies which form it: American Express Company, Bank of America,
Coca-cola, McDonald’s, Microsoft Corp., Johnson & Johnson, Intel Corp.
The training sample data were also taken in the period since 20 September to
14 November 2011 year. For test sample were taken data of Dow Jones
Industrial Average since 15 November 2011 year to 17 November 2011 year
(Table 6.2).
For experimental investigations classical GMDH and fuzzy GMDH were used.
The comparison with cascade neo-fuzzy neural network [5] was also carried out
during the experiments. For investigations the percentage of training sample was
chosen 50 %, 70 %, 90 %. Freedom of choice was taken such F ¼ 5; 6.
Table 6.2 Testing data for
Dow Jones Industrial Average
Data
Dow Jones Industrial Average
Nov 15 2011
12096.16
Nov 16 2011
11905.59
Nov 17 2011
11770.73
6.8
Adaptation of Fuzzy Models Coefﬁcients
241

For fuzzy GMDH triangular, Gaussian and bell-shaped membership functions
were used. For Gaussian and bell-shaped membership functions the following level
values were taken a ¼ 0:3; 0:5; 0:7; 0:9.
For constructing models the following four partial descriptions were used:
• a linear model of the form:
f ðxi; xjÞ ¼ A01 þ A11  xi þ A12  xj;
Table 6.3 Forecast criteria for shares of BP plc (ADR)
Step of forecast
Criterion
Percentage of training sample
50 %
70 %
90 %
1
MSE test
0.851172
0.195661
0.222304
MSE
0.556938
0.285995
0.269439
MAPE test
1.828592
0.952620
1.122068
MAPE
1.405313
1.034865
0.994513
DW
0.604306
2.171417
2.222481
R-square
0.650841
0.969484
0.906763
AIC
2.185818
1.590313
1.514696
BIC
−3.023414
−2.349042
−2.244520
SC
2.235525
1.640021
1.564404
2
MSE test
1.734720
0.433501
1.052307
MSE
0.974776
0.425016
0.391586
MAPE test
2.199273
1.274355
1.718034
MAPE
1.412474
1.227474
1.090535
DW
0.587306
1.913641
2.219508
R-square
0.706036
1.046596
0.906839
AIC
2.223931
1.874082
1.514855
BIC
−3.059720
−2.698769
−2.244745
SC
2.273639
1.923790
1.564562
3
MSE test
2.981124
1.093896
3.238221
MSE
1.723194
0.675693
0.839414
MAPE test
2.411189
1.788249
2.352890
MAPE
1.571035
1.496220
1.241341
DW
0.587306
1.952832
2.219508
R-square
0.706036
1.023470
0.906839
AIC
2.223931
1.923849
1.514855
BIC
−3.059720
−2.754263
−2.244745
SC
2.273639
1.973556
1.564562
242
6
Inductive Modeling Method (GMDH) in Problems …

• a squared model:
f ðxi; xjÞ ¼ A01 þ A11  xi þ A12  xj þ A21  x2
i þ A22  x2
j þ A23xi  xj;
• Fourier polynomial of the ﬁrst degree:
f ðxi; xjÞ ¼ ðA01Þ=2 þ A11  cosðxiÞ þ A12  sinðxjÞ;
• Chebyshev’s polynomial of the second degree:
Table 6.4 Forecast criteria for Dow Jones Industrial Average
Step of forecast
Criterion
Percentage of training sample
50 %
70 %
90 %
1
MSE test
40494.427565
33109.754270
40286.725033
MSE
26900.763617
62553.804951
26432.081096
MAPE test
1.462066
1.363148
1.405372
MAPE
1.149183
1.809130
1.191039
DW
1.917430
1.013536
1.862203
R-square
0.990922
0.804774
0.849329
AIC
12.753417
13.551054
12.729001
BIC
−7.360808
−7.482613
−7.356959
SC
12.796073
13.593710
12.771656
2
MSE test
41546.293300
31602.995442
43907.693743
MSE
27793.341807
61328.007065
32746.387552
MAPE test
1.481603
1.374860
1.521950
MAPE
1.167355
1.819287
1.280730
DW
1.917020
1.013536
1.669879
R-square
0.989976
0.804774
0.811488
AIC
12.753849
13.551054
12.935378
BIC
−7.360875
−7.482613
−7.389255
SC
12.796504
13.593710
12.978034
3
MSE test
58651.409938
34923.660449
56028.016943
MSE
37306.640205
61736.892393
34792.320367
MAPE test
1.584580
1.394577
1.534979
MAPE
1.230185
1.830302
1.288520
DW
1.917430
1.013536
1.669879
R-square
0.990922
0.804774
0.811488
AIC
12.753417
13.551054
12.935378
BIC
−7.360808
−7.482613
−7.389255
SC
12.796073
13.593710
12.978034
6.9
The Application of GMDH for Forecasting at the Stock Exchange
243

Fig. 6.4 Forecast for share prices of BP plc (ADR) at 3 steps ahead
Table 6.5 Forecast criteria for shares BP plc (ADR) closing prices for fuzzy GMDH for each
forecast step
Step of forecast
Criterion
Percentage of training sample
50 %
70 %
90 %
1
MSE test
1.248355
0.793523
0.317066
MSE
0.748864
0.612827
0.485599
MAPE test
2.041366
2.065155
1.386096
MAPE
1.452656
1.723657
1.505965
DW
0.686478
1.763043
1.839065
R-square
1.064526
0.938058
1.008042
AIC
2.456985
2.228267
2.053752
BIC
−3.268440
−3.063808
−2.892253
SC
2.506693
2.277974
2.103459
2
MSE test
1.256828
1.871883
2.440575
MSE
0.728499
0.989650
0.795699
MAPE test
2.072085
2.367006
2.426847
MAPE
1.468987
1.845798
1.656084
DW
0.686478
1.795021
1.839065
R-square
1.064526
0.874163
1.008042
AIC
2.456985
2.147717
2.053752
BIC
−3.268440
−2.986448
−2.892253
(continued)
244
6
Inductive Modeling Method (GMDH) in Problems …

f ðxi; xjÞ ¼ ðA01Þ þ A11  xi þ A12  ðx2
j  1Þ;
where Ai is a fuzzy number with triangular, Gaussian and bell-shaped
membership function.
Experimental Investigations of Inductive Modeling Methods
1. In the ﬁrst experiments the investigations of classical GMDH were carried out.
As partial descriptions were taken linear model, squared model, Chebyshev’s
polynomial and Fourier polynomial.
All the forecasts were carried out with change of training sample size, namely
50 %, 70 %, 90 %. The freedom of choice varied so F ¼ 5; 6; 7. But as the results
of experiments had shown the ﬂuctuations of forecast values differed only at the 3rd
Fig. 6.5 Forecasts ﬂow charts for fuzzy GMDH with linear partial descriptions, 3 steps ahead
Table 6.5 (continued)
Step of forecast
Criterion
Percentage of training sample
50 %
70 %
90 %
SC
2.506693
2.197424
2.103459
3
MSE test
1.317349
5.408313
6.385595
MSE
0.788241
2.388737
1.625459
MAPE test
2.129226
3.095381
3.169584
MAPE
1.496548
2.149238
1.834261
DW
0.686478
1.762895
1.839065
R-square
1.064526
0.945517
1.008042
AIC
2.456985
2.229614
2.053752
BIC
−3.268440
−3.065077
−2.892253
SC
2.506693
2.279321
2.103459
6.9
The Application of GMDH for Forecasting at the Stock Exchange
245

or 4th digit after comma with various F. Thefore, in the succeeding experiments the
freedom of choice was taken equal to F ¼ 6.
For forecasts accuracy analysis the errors were calculated and below in the
Tables 6.3 and 6.4 the following criteria of forecast quality are presented: MSE for
test sample, MSE for full sample, MAPE for full and test sample, Durbin-Wattson
criterion (DW), R-squared, Akaike criterion (AIC), Bayes information criterion
(BIC), and Shwarz criterion (SC). MAPE criterion was taken in percentages.
In the Fig. 6.4 the forecast of BP plc (ADR) shares prices at 3 steps for F = 6
and 70 % of training sample is presented (Table 6.4).
Table 6.6 Forecast criteria for each forecast step Dow Jones Industrial Average for fuzzy GMDH
Step of forecast
Criterion
Percentage of training sample
50 %
70 %
90 %
1
MSE test
37900.205497
37035.864923
37221.489676
MSE
25676.116414
62162.341245
43498.250784
MAPE test
1.379057
1.207524
1.425909
MAPE
1.137631
1.705561
1.627755
DW
1.910164
1.265273
1.338295
R-square
0.920628
0.928216
0.769091
AIC
12.708769
13.569508
13.219191
BIC
−7.353765
−7.485345
−7.432833
SC
12.751424
13.612164
13.261847
2
MSE test
37541.139628
39456.166212
38884.456009
MSE
25793.209060
61020.777256
43549.202548
MAPE test
1.379904
1.230402
1.457562
MAPE
1.143661
1.711295
1.628665
DW
1.910164
1.265273
1.338295
R-square
0.920628
0.928216
0.769091
AIC
12.708769
13.569508
13.219191
BIC
−7.353765
−7.485345
−7.432833
SC
12.751424
13.612164
13.261847
3
MSE test
44442.485544
44478.491577
79402.073988
MSE
29782.905175
63478.044185
49364.835751
MAPE test
1.430837
1.289476
1.678009
MAPE
1.176753
1.719494
1.651875
DW
1.910164
1.265273
1.338295
R-square
0.920628
0.928216
0.769091
AIC
12.708769
13.569508
13.219191
BIC
−7.353765
−7.485345
−7.432833
SC
12.751424
13.612164
13.261847
246
6
Inductive Modeling Method (GMDH) in Problems …

Table 6.7 Comparative forecasting results for BP plc (ADR) share prices
Step of
forecast
Criteria
Neo-fuzzy neural
network
Forecast results for GMDH
Partial description (PD)
Forcast results for fuzzy GMDH
Partial description
Linear
Quadratic
Fourier
polynomial
Chebyshev’s
polynomial
Linear
Quadratic
Fourier
polynomial
Chebyshev’s
polynomial
1
MSE
5.231
0.285
1.905
0.859
0.365
0.481
0.130
1.691
0.757
MAPE
4.521
1.034
1.965
1.624
1.114
1.374
0.813
2.960
1.459
2
MSE
6.111
0.425
3.090
1.094
0.366
0.498
0.150
1.742
1.029
MAPE
5.398
1.227
2.916
1.814
1.115
1.481
0.818
2.977
1.584
3
MSE
7.490
0.675
4.978
2.144
0.523
0.572
0.308
2.183
1.505
MAPE
6.521
1.496
4.434
2.050
1.320
1.494
0.908
3.024
1.681
6.9
The Application of GMDH for Forecasting at the Stock Exchange
247

Fuzzy GMDH
In this section the forecasting results for fuzzy GMDH are presented. As partial
descriptions linear model, squared model, Chebyshev’s polynomial and Fourier
polynomial were taken. As membership functions triangular, Gaussian and
bell-shaped membership function were used. All the forecasts were carried out with
change of training sample size, namely 50 %, 70 %, 90 %.
In the Fig. 6.5 the ﬂow charts of forecasts made by fuzzy GMDH are presented
for 3 steps ahead with F = 6, training sample size—70 %, and Gaussian MF.
Signiﬁcance level a ¼ 0:7.
Fig. 6.6 Index Dow Jones I. A. forecast results at 3 steps ahead with FGMDH
Fig. 6.7 Forecast results for BP plc (ADR shares by FGMDH with quadratic partial descriptions,
3 steps ahead
248
6
Inductive Modeling Method (GMDH) in Problems …

Table 6.8 Comparative forecasting results for index Dow Jones I. A
Step of
forecast
Criteria
Neo-fuzzy
neural
network
Forecast results for GMDH
Partial description (PD)
Forcast results for fuzzy GMDH
Partial description
Linear
Quadratic
Fourier
polynomial
Chebyshev’s
polynomial
Linear
Quadratic
Fourier
polynomial
Chebyshev’s
polynomial
1
MSE
514561
26900
38225
40142
23818
25176
21332
42205
24464
MAPE
5.231
1.149
1.298
1.445
1.111
1.137
1.046
1.487
1.125
2
MSE
584371
27793
39460
40930
23978
25793
223491
59059
24767
MAPE
5.992
1.167
1.322
1.445
1.119
1.143
1.098
1.614
1.144
3
MSE
624501
37306
50471
41720
27337
29782
38291
63900
24910
MAPE
6.179
1.230
1.386
1.460
1.157
1.176
1.099
1.623
1.160
6.9
The Application of GMDH for Forecasting at the Stock Exchange
249

Table 6.9 MSE comparison for different methods of experiment 1
GMDH
FGMDH
FGMDH with fuzzy inputs,
Triangular MF
FGMDH with fuzzy inputs,
Gaussian MF
MSE
0.1129737
0.0536556
0.055557
0.028013
250
6
Inductive Modeling Method (GMDH) in Problems …

For forecasts accuracy analysis the errors were calculated and in the Tables 6.5
and 6.6 the following criteria of forecast quality are presented: MSE for test sample,
MSE for full sample, MAPE for full and test sample, Durbin-Wattson criterion,
R-squared, Akaike criterion, Bayes information criterion (BIC), and Shwartz cri-
terion. Criteria were calculated for each forecast step using a test sample. The
results for BP plc (ADR) shares are presented in the Table 6.5 and for Dow Jones
Industrial Average in the Table 6.6.
On the Figs. 6.6 and 6.7 the ﬂow charts of forecasts for shares BP plc (ADR) and
Dow Jones I. A obtained by fuzzy GMDH are presented for 3 steps ahead with
F ¼ 6, a training sample size 70 % and Gaussian MF.
Signiﬁcance level a ¼ 0:7.
Further experiments for forecasting share prices of BP plc (ADR) and Dow
Jones I. A. were carried out with application of GMDH and fuzzy GMDH with
different partial descriptions: linear model, squared model, Chebyshev’s polyno-
mials and Fourier polynomials and with application of cascade neo-fuzzy neural
networks as well. [6].
The ﬁnal experimental results of forecasts at 1, 2 and 3 steps ahead with
aforesaid methods for share prices of British Petroleum—BP plc (ADR) are pre-
sented in the Table 6.7, and for index Dow Jones Industrial Average—in the
Table 6.8.
Judging from presented criteria estimates the best results were obtained with
fuzzy GMDH with quadratic partial descriptions (PD), 70 % training sample and
Gaussian membership function. The worst results were obtained using Fourier
polynomials as PD. The both GMDH methods, classical and fuzzy, have shown the
high forecast accuracy. If to compare the accuracy of both methods with linear
partial descriptions then linear model by GMDH has shown more accurate results.
But with all used PD more accurate forecasts were obtained using fuzzy GMDH
with quadratic partial descriptions. At the same time cascade neo-fuzzy neural
network gave the worse results than both GMDH methods.
The best results were obtained with fuzzy GMDH using quadratic partial
descriptions with bell-shaped membership functions and 50 % training sample size.
The worst results were obtained with Fourier polynomial as partial descriptions.
The use of Chebyshev’s polynomial as PD in classical GMDH has shown the best
results while in fuzzy GMDH the most accurate estimates were obtained with linear
and quadratic MF.
6.9
The Application of GMDH for Forecasting at the Stock Exchange
251

6.10
FGMDH Model with Fuzzy Input Data
Let’s consider the generalization of fuzzy GMDH for case when input data are also
fuzzy. Then a linear interval regression model takes the following form [8–10]:
Y ¼ A0Z0 þ A1Z1 þ    þ AnZn;
ð6:63Þ
where
Ai
is
a
fuzzy
number,
which
is
described
by
three
parameters
Ai ¼ ðAi; A
^
i; AiÞ, A
^
i is an interval center, Ai is an upper border of the interval, Ai is a
lower border of the interval, and Zi is also a fuzzy number, which is determined by
parameters
Zi; Z
^
i; Zi


, Zi is a lower border, Z
^
i is a center, Zi is an upper border of a
fuzzy number.
Then Y is a fuzzy number, which parameters are deﬁned as follows (in accor-
dance with L-R numbers multiplying formulas):
Center of interval y
^ ¼ P A
^
i  Z
^
i,
Deviation in the left part of the membership function:
y
^  y ¼
X
ð A
^
i

  ðZ
^
i  ZiÞ þ ðA
^
i  AiÞ  Z
^
i

Þ;
ð6:64Þ
Lower border of the interval
y ¼
X
ðA
^
i  Z
^
i  A
^
i

  ðZ
^
i  ZiÞ  ðA
^
i  AiÞ  Z
^
i

Þ
Deviation in the right part of the membership function:
y  y
^ ¼
X
ð A
^
i

  ðZi  Z
^
iÞ þ Z
^
i

  ðAi  A
^
iÞÞ:
ð6:65Þ
Upper border of the interval
y ¼
X
ð A
^
i

  ðZi  Z
^
iÞ þ Z
^
i

  ðAi  A
^
iÞ þ A
^
i  Z
^
iÞ:
ð6:66Þ
For the interval model to be correct, the real value of input variable Y should lay
in the interval got by the method application.
So, the general requirements to estimation linear interval model are to ﬁnd such
values of parameters Ai ¼ ðAi; A
^
i; AiÞ of fuzzy coefﬁcients, which allow [8–10]:
1. observed values yk should lay in the estimation interval for Yk;
2. total width of the estimation interval should be minimal.
Input data for this task is Zk ¼ Zki
½
-input training sample, and output values yk
252
6
Inductive Modeling Method (GMDH) in Problems …

are known, k ¼ 1; M, M is the number of observation points.
Triangular membership functions are considered:
Quadratic partial descriptions were chosen:
f ðxi; xjÞ ¼ A0 þ A1xi þ A2xj þ A3xixj þ A4x2
i þ A5x2
j :
6.10.1
FGMDH with Fuzzy Input Data and Triangular
Membership Functions
There are more than ten typical forms of curves to specify the membership func-
tions. But the most wide spread are triangular, trapezoidal and Gaussian member-
ship functions. Triangular membership function is described by three numbers (a, b,
c), and its value in point x is computed as the following expression shows [6]:
lðxÞ ¼
1  b  x
b  a ;
a  x  b
x  c
b  c ;
b  x  c
0;
in other cases
8
>
>
>
<
>
>
>
:
If b  a ¼ c  b, then it is the case of symmetric triangular membership
function.
Triangular membership function is a special case of trapezoidal membership
function, which is described by four parameters ða; b; c; dÞ while b ¼ c. Its value in
point x is computed as follows [6]:
lðxÞ ¼
1  b  x
b  a ;
a  x  b
1;
b  x  c
1  x  c
d  c ;
c  x  d
0;
in other cases
8
>
>
>
>
>
<
>
>
>
>
>
:
Now let’s consider the linear interval regression model:
Y ¼ A0Z0 þ A1Z1 þ    þ AnZn;
ð6:67Þ
where Ai is a fuzzy number of triangular shape, which is described by three
parameters Ai ¼ ðAi; A
^
i; AiÞ, where ai is a center of the interval, Ai—its upper
border, Ai—its lower border.
Consider the case of symmetrical membership function for parameters Ai, so
they can be described by the pair of parameters (ai, ci).
Ai ¼ ai  ci, Ai ¼ ai þ ci, ci—interval width, ci  0,
6.10
FGMDH Model with Fuzzy Input Data
253

Zi is also a fuzzy number of triangular shape, which is deﬁned by parameters
ðZi; Z
^
i; ZiÞ, Zi is a lower border, Z
^
i is a center, Zi—an upper border of fuzzy number.
Then Y is a fuzzy number, which parameters are deﬁned as follows:
the center of the interval y
^ ¼ P ai  Z
^
i,
the deviation in the left part of the membership function:
y
^  y ¼
X
ðai  ðZ
^
i  ZiÞ þ ci Z
^
i

Þ;
thus, the lower border of the interval
y ¼
X
ðai  Zi  ci Z
^
i

Þ:
ð6:68Þ
Deviation in the right part of the membership function:
y  y
^ ¼
X
ðai  ðZi  Z
^
iÞ þ ci Z
^
i

Þ ¼
X
aiZi  aiZ
^
i þ ci Z
^
i

; so
The upper border of the interval
y ¼
X
ðai  Zi þ ci Z
^
i

Þ:
ð6:69Þ
For the interval model to be correct, the real value of input variable Y should lay
in the interval obtained by the method FGMDH.
It can be described in such a way:
P ðai  Zik  ci Z
^
ik

Þ  yk
P ðai  Zik þ ci Z
^
ik

Þ  yk
8
<
:
; k ¼ 1; M;
ð6:70Þ
where Zk ¼ Zki
½
 is an input training sample, yk is known output value, k ¼ 1; M, M
—number of observation points.
So, the general requirements to a linear interval model are the following: to ﬁnd
such values of parameters (ai, ci) of fuzzy coefﬁcients, which ensure [6]:
1. observed values yk should locate in the estimation interval for Yk;
2. total width of the estimation interval be minimal.
These requirements may be redeﬁned as a task of linear programming:
min
ai;ci
X
M
k¼1
ð
X
ðai  Zi þ ci Z
^
i

Þ 
X
ðai  Zi  ci Z
^
i

ÞÞ
ð6:71Þ
under conditions
254
6
Inductive Modeling Method (GMDH) in Problems …

X
ðai  Zik  ci Z
^
ik

Þ  yk
X
ðai  Zki þ ci Z
^
ik

Þ  yk; k ¼ 1; M
8
>
<
>
:
ð6:72Þ
Let’s consider partial description of the form
f ðxi; xjÞ ¼ A0 þ A1xi þ A2xj þ A3xixj þ A4x2
i þ A5x2
j
ð6:73Þ
Rewriting it in accordance with the model (6.63) needs such substitutions
z0 ¼ 1 z1 ¼ xi z2 ¼ xj z3 ¼ xixj z4 ¼ x2
i z5 ¼ x2
j
Then math model takes the following form
min
ai;ci ð2Mc0 þ a1
X
M
k¼1
ðxik  xikÞ þ 2c1
X
M
k¼1
x
^
ik

 þ a2
X
M
k¼1
ðxjk  xjkÞ þ 2c2
X
M
k¼1
x
^
jk

 þ
þ a3
X
M
k¼1
ð x
^
ik

ðxjk  xjkÞ þ x
^
jk

ðxik  xikÞÞ þ 2c3
X
M
k¼1
x
^
ikx
^
jk

 þ 2a4
X
M
k¼1
x
^
ik

ðxik  xikÞ þ
þ 2c4
X
M
k¼1
x
^2
ik þ 2a5
X
M
k¼1
x
^
jk

ðxjk  xjkÞ þ 2c5
X
M
k¼1
x
^2
jkÞ
ð6:74Þ
with the following constraints
a0 þ a1xik þ a2xjk þ a3ð x
^
ik

ðx
^
jk  xjkÞ  x
^
jk

ðx
^
ik  xikÞ þ x
^
ikx
^
jkÞ þ
þ a4ð2 x
^
ik

ðx
^
ik  xikÞ þ x
^2
ikÞ þ a5ð2 x
^
jk

ðx
^
jk  xjkÞ þ x
^2
jkÞ 
 c0  c1 x
^
ik

  c2 x
^
jk

  c3 x
^
ikx
^
jk

  c4x
^2
ik  c5x
^2
jk  yk
a0 þ a1xik þ a2xjk þ a3ð x
^
ik

ðxjk  x
^
jkÞ þ x
^
jk

ðxik  x
^
ikÞ  x
^
ikx
^
jkÞ þ
þ a4ð2 x
^
ik

ðxik  x
^
ikÞ  x
^2
ikÞ þ a5ð2 x
^
jk

ðxjk  x
^
jkÞ  x
^2
jkÞ þ c0 þ
þ c1 x
^
ik

 þ c2 x
^
jk

 þ c3 x
^
ikx
^
jk

 þ c4x
^2
ik þ c5x
^2
jk  yk
cl  0; l ¼ 0; 5:
As one can see, this is the linear programing problem, but there are still no
constraints for non-negativity of variables ai, so it’s reasonable to pass to a dual
problem, introducing dual variables
dk
f
g and
dk þ M
f
g.
6.10
FGMDH Model with Fuzzy Input Data
255

6.11
Experimental Investigations of GMDH and FGMDH
Experiment 1. Forecasting of RTS index (opening price)
Let’s consider the results of experiments of forecasting RTS index using data from
Russian stock-exchange.
RTS index is an ofﬁcial indicator of RTS stock exchange. It is calculated during
trade session with every change of price of instruments, which are included in the
list of its calculation. First value of index is the opening price, and the last is the
closing price. RTS index is calculated as a ratio of total capitalization of securities
included in list of its calculation to total market capitalization of these securities on
the initial date and then multiplying this value by the value of the index on the
initial date and correction coefﬁcient:
In ¼ Zn  I1  MCn
MC1
;
where MCn—sum of market capitalizations of shares on current time, USD.
MCn ¼ PN
i¼1 Wi  Pi  Qi  Ci, where Wi—correction coefﬁcient, which takes into
account the number of securities of ith type in free circulation;
Ci—coefﬁcient, which limits share of capitalization of securities of ith type;
Qi—quantity of securities of particular type, which were created by issuer till the
current date;
Pi—price of ith security in USD on the calculation moment t;
N—number of types of securities in list, which is used for index calculation.
The list of securities, which is used for calculation of RTS index, consists of the
most liquid shares of Russian companies chosen by Informational Committee and
based on expert judgment. The number of securities may not exceed 50.
Information about index is covered on RTS web-site http://www.rts.ru/rtsindex.
There are also the methods of index calculation, the list of securities for index
calculation, index value, information about every security allotment into total
capitalization of all securities in list.
The experiment contains 5 fuzzy input variables, which are the stock prices of
leading Russian energetic companies included into the list of RTS:
• index LKOH—shares of “ЛУКOЙЛ” joint-stock company,
• EESR—shares of “PAO EЭC Poccии” joint-stock company,
• YUKO—shares of “ЮКOC” joint-stock company,
• SNGSP—privileged shares of “Cypгyтнeфтeгaз” joint-stock company,
• SNGS—common shares of “Cypгyтнeфтeгaз” joint-stock company.
• Output variable is the value of RTS index (opening price) of the same period
(03.04.2006–18.05.2006).
256
6
Inductive Modeling Method (GMDH) in Problems …

Sample size contains 32 values. Training sample size has 18 values (optimal size
of the training sample for current experiment). The following results were obtained
[6].
Experiment 1 For normalized input when using Gaussian MF in group method of
data handling with fuzzy input data the results of experiment are presented in
Fig. 6.8
and
Table 6.9:
MSE
for
GMDH = 0.1129737,
MSE
for
FGMDH = 0.0536556.
As the results of experiment 1 show, fuzzy group method of data handling with
fuzzy input data gives more accurate forecast than GMDH and FGMDH. In case of
triangular MF FGMDH with fuzzy inputs gives a little worse forecast than FGMDH
with Gaussian MF (Fig. 6.9).
Experiment 2 RTS-2 index forecasting (opening price)
Sample size—32 values. Training sample size—18 values
For normalized input data when using triangular MF in FGMDH with fuzzy
input data the experimental results are presented in Fig. 6.10.
-0,2000
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
1,4000
1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132
Y
Estimation by distinct
GMDH
Estimation by fuzzy
GMDH
Upper border by fuzzy
GMDH
Lower border by fuzzy
GMDH
Fig. 6.8 Experiment 1 results using GMDH and FGMDH
Table 6.10 Comparison of different methods in experiment 2
GMDH
FGMDH
FGMDH with fuzzy
inputs, triangular MF
FGMDH with fuzzy
inputs, Gaussian MF
MSE
0.051121
0.063035
0.061787
0.033097
6.11
Experimental Investigations of GMDH and FGMDH
257

For normalized input data using Gaussian MF in FGMDH with fuzzy input data
the following experimental results were obtained (see Fig. 6.11).
MSE for GMDH = 0.051121, MSE for FGMDH = 0.063035.
As the results of the experiment 2 show (Table 6.10), fuzzy group method of
data handling with fuzzy input data gives better result than GMDH and FGMDH in
case of both Gaussian and triangular membership function (Fig. 6.12).
-0,2
0
0,2
0,4
0,6
0,8
1
1,2
1
3
5
7
9 11 13 15 17 19 21 23 25 27 29 31
FGMDH with fuzzy inputs, triangular MF
FGMDH with fuzzy inputs, Gaussian MF
Real values
GMDH
FGMDH
Fig. 6.9 GMDH, FGMDH,
and FGMDH with fuzzy
inputs comparison result
-0,4
-0,2
0
0,2
0,4
0,6
0,8
1
1,2
1,4
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
Left
border
Center
Right
border
Real
values
Fig. 6.10 Experiment 2
results for triangular MF
258
6
Inductive Modeling Method (GMDH) in Problems …

6.12
Conclusions
In this chapter inductive modeling method GMDH is considered.
The main principles of GMDH are presented. This method enables to construct
complex process models using experimental data. Opposite to classical identiﬁca-
tion methods GMDH has advantage to synthesize model structure automatically in
the process of algorithm run.
-0,4000
-0,2000
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132
Y
Estimation by distinct GMDH
Estimation by fuzzy GMDH
Upper border by fuzzy GMDH
Lower border by fuzzy GMDH
Fig. 6.11 Experiment 2 results using GMDH and FGMDH
Table 6.11 Comparison of different methods in experiment 2
GMDH
FGMDH
FGMDH with fuzzy
inputs, triangular MF
FGMDH with fuzzy
inputs, Gaussian MF
MSE
0.051121
0.063035
0.061787
0.033097
-0,4
-0,2
0
0,2
0,4
0,6
0,8
1
1,2
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29 31
FGMDH with
fuzzy inputs,
triangular MF
FGMDH with
fuzzy inputs,
Gaussian MF
Real values
GMDH
Fig. 6.12 GMDH, FGMDH
and FGMDH with fuzzy
inputs (center of estimation)
results
6.12
Conclusions
259

Two different GMDH versions were presented and discussed: classical GMDH
developed by prof. A.G. Ivakhnenko and new fuzzy GMDH which works with
indeﬁnite and fuzzy input information and constructs fuzzy models.
The advantage of fuzzy GMDH lies herein it doesn’t use least square method for
search of unknown model coefﬁcients opposite to classical GMDH and therefore
the problem of possible ill-conditioned matrix doesn’t exist for it. Besides, fuzzy
GMDH enables to ﬁnd not point-wise forecast estimates but interval estimates for
forecast values which allow to determine forecast accuracy. The generalization of
fuzzy GMDH FGMDH with fuzzy inputs was also considered and analyzed.
The experimental investigations of GMDH and fuzzy GMDH in problems of
share prices forecast at NYSE and Russian stock market RTS are presented and
discussed. The comparative results analysis has conﬁrmed the high accuracy of
fuzzy GMDH in problems of forecasting in ﬁnancial sphere.
References
1. Ivakhnenko, A.G., Zaychenko, Yu.P., Dimitrov, V.D.: Decision-Making on the Basis of
Self-Organization, 363 pp. Moscow Publishing House “Soviet Radio’” (1976) (rus)
2. Zaychenko, Yu.P., Zayets, I. O. Ю.: П. Synthesis and adaptation of fuzzy forecasting models
on the basis of self-organization method. Scientiﬁc papers of NTUU “KPI”, №3, pp. 34–41
(2001) (rus)
3. Zaychenko, Yu.P., Kebkal, A.G., Krachkovsky, V.F.: Fuzzy group method of data handling
and its application for macro-economic indicators forecasting. Scientiﬁc papers of NTUU
“KPI”, №2, pp. 18–26 (2000) (rus)
4. Zaychenko, Yu.P., Zayets, I.O., Kamotsky, O.V., Pavlyuk, O.V.: The investigations of
different membership functions. In: Fuzzy Group Method of Data Handling. Control Systems
and Machines, №2, pp. 56–67 (2003) (rus)
5. Zaychenko, Yu.: Fuzzy group method of data handling in forecasting problems at ﬁnancial
markets. Int. J. Inf Models Anal. 1(4), 303–317 (2012) (rus)
6. Zaychenko, Yu.P.: Fuzzy Models and Methods in Intellectual Systems, 354 pp. Kiev
Publishing House “Slovo” (2008).
7. Zaychenko, Yu.: The fuzzy group method of data handling and its application for economical
processes forecasting. Scientiﬁc Inquiry 7(1), 83–98 (2006)
8. Zaychenko, Yu.: The fuzzy group method of data handling with fuzzy input variables.
Scientiﬁc Inquiry, 9(1), 61–76 (2008)
9. Zaychenko, Yu.P., Petrosyuk, I.M., Jaroshenko, M.S.: The investigations of fuzzy neural
networks in the problems of electro-optical images recognition. Syst. Res. Inf. Technol. (4),
61–76 (2009) (rus)
10. Zaychenko, Yu.P.: Fuzzy group method of data handling under fuzzy input data. Syst. Res.
Inf. Technol. (3), 100–112 (2007) (rus)
260
6
Inductive Modeling Method (GMDH) in Problems …

Chapter 7
The Cluster Analysis in Intellectual
Systems
7.1
Introduction
Term cluster analysis (introduced by Tryon, 1939 for the ﬁrst time) actually
includes a set of various algorithms of classiﬁcation without teacher [1]. The
general question asked by researchers in many areas is how to organize observed
data in evident structures, i.e. to develop taxonomy. For example, biologists set the
purpose to divide animals into different types that to describe distinctions between
them. According to the modern system accepted in biology, the person belongs to
primacies, mammals, vertebrate and an animal. Notice that in this classiﬁcation, the
higher is aggregation level, the less is the similarity between members in the
corresponding class. The person has more similarity to other primacies (i.e. with
monkeys), than with the “remote” members of family of mammals (for example,
dogs), etc.
The clustering is applied in the most various areas. For example, in the ﬁeld of
medicine the clustering of diseases, treatments of diseases or symptoms of diseases
leads to widely used taksonomy. In the ﬁeld of psychiatry the correct diagnostics of
clusters of symptoms, such as paranoia, schizophrenia, etc., is decisive for suc-
cessful therapy. In archeology by means of the cluster analysis researchers try to
make taxonomy of stone tools, funeral objects, etc. Broad applications of the cluster
analysis in market researches are well known. Generally, every time when it is
necessary to classify “mountains” of information to groups, suitable for further
processing, the cluster analysis is very useful and effective. In recent years the
cluster analysis is widely used in the intellectual analysis of data (Data Mining), as
one of the principal methods [1].
The purpose of this chapter is the consideration of modern methods of the cluster
analysis, crisp methods(a method of C-means, Ward’s method, the next neighbor,
the most distant neighbor), and fuzzy methods, robust probabilistic and possibilistic
clustering methods. In the Sect. 7.2 problem of cluster analysis is formulated, main
criteria and metrics are considered and discussed. In the Sect. 7.3 classiﬁcation of
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_7
261

cluster analysis methods is presented, several crisp methods are considered, in
particular hard C-means method and Ward’s method. In the Sect. 7.4 fuzzy
C-means method is described. In the Sect. 7.5 the methods of initial location of
cluster centers are considered: peak and differential grouping and their properties
analyzed. In the Sect. 7.6 Gustavsson-Kessel’s method of cluster analysis is con-
sidered which is a generalization of fuzzy C-means method when metrics of dis-
tance differs from Euclidian.
In the Sect. 7.7 adaptive robust clustering algorithms are presented and analyzed
which are used when initial data are distorted by high level of noise, or by outliers.
In the Sect. 7.8 robust probabilistic algorithms of fuzzy clustering are considered.
Numerous results of pilot studies of fuzzy methods of a cluster analysis are
presented in the Sect. 7.9 among them is a problem of UN countries clustering by
indicators of sustainable development.
7.2
Cluster Analysis, Problem Deﬁnition. Criteria
of Quality and Metrics
Let the set of observations c1 be given, where Xi ¼ fxijg; j ¼ 1; N. It is required to
divide a set X into not intersected K subsets—clusters S1; . . .; SK so that to provide
extremum of some criterion (functional of quality), that is:
to ﬁnd such S ¼ ðS1; . . .; SKÞ that f ðSÞ ! minðmaxÞ.
Different types of criteria (functional) of splitting are possible. It’s worth to note
that this task is closely connected with deﬁnition of some metrics in a feature space.
Consider the most widely used functionals of splitting quality [2]:
1. Coefﬁcient of splitting F which is deﬁned as follows:
F ¼
XK
j¼1
Xn
i¼1
w2
ij
n ;
ð7:1Þ
where wij 2 ½0; 1—some degree of membership of the ith object to the jth cluster.
Change range is F 2
1
k ; 1


, where n—number of objects, K—number of clusters.
2. Non-fuzziness index:
NFI ¼ KF  1
k  1 ;
NFI 2 ½0; 1;
ð7:2Þ
where K—number of classes (clusters); F—splitting coefﬁcient.
262
7
The Cluster Analysis in Intellectual Systems

3. Entropy of splitting:
H ¼ 
X
K
j¼1
X
n
i¼1
wijlnðwijÞ
n
; H 2 ð0; ln KÞ:
ð7:3Þ
4. The normalized entropy of splitting:
H1 ¼
H
1  K=n ; H1 2
0; n ln K
n  K


;
ð7:4Þ
where n is a number of points.
5. The modiﬁed entropy:
H2 ¼ H
ln K ; H2 2 ð0; 1Þ
ð7:5Þ
6. Second functional of Rubens:
F2 ¼ 1
2
1
n
X
n
i¼1
max
j
Wij þ min
i
max
j
Wij
 
!
;
F2 2
1
K ; 1


:
ð7:6Þ
7. Third functional of Rubens (second index of Non-fuzziness) :
NF2I ¼ KF2  1
K  1 ; NF2I 2 ð0; 1Þ:
ð7:7Þ
As initial information is set in the form of a matrix X, there is a metrics choice
problem. Metrics choice—the most important factor inﬂuencing results of a cluster
analysis. Depending on type of features various measures of distance (metrics) are
used.
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality and Metrics
263

Let be samples Xi and XK in N-dimensional feature space.
The main metrics of clustering are given in the Table 7.1
There is a large number of clustering algorithms which use various metrics and
criteria of splitting.
Classiﬁcation of Algorithms of Cluster Analysis
When performing a clustering it is important to know, how many clusters contains
an initial sample It is supposed that the clustering has to reveal natural local
grouping of objects. Therefore the number of clusters is the parameter which is
often signiﬁcantly complicates an algorithm if it is supposed to be unknown and
signiﬁcantly inﬂuencing quality of result if it is known.
The problem of a choice of clusters number is very nontrivial. It is enough to tell
that for obtaining the satisfactory theoretical decision often it is required to make in
advance very strong assumptions of properties of some family of distributions. But
about what assumptions one can make when, especially at the beginning of
research, of data practically it isn’t known? Therefore algorithms of a clustering
usually are constructed as some way of search clusters number and determination of
its optimum value in the course of search.
The number of methods of splitting a set of objects into clusters is quite great.
All of them can be subdivided on hierarchical and not hierarchical.
Table 7.1 Clustering metrics
No
Name of a metrics
Type of
features
Formula for an assessment of a measure
of proximity (metrics)
1
Euclidean distance
The
quantitative
dik ¼
P
N
j¼1
xij  xkj

2
 
!1=2
2
Measure of similarity of
Hamming
Nominal
(qualitative)
lH
ij ¼ nik
N ,
where nik—number of coinciding
features in samples Xi and XK
3
Measure of similarity of
Rogers-Tanimoto
Nominal
scales
lRT
ij
¼ n00
ikðn0
i þ n00
k  n00
ikÞ
where n00
ik—number of coinciding unit
features at samples Xi and XK;
n0
i, n00
k—total number of unit features at
samples Xi and XK respectively
4
Manhattan metrics
The
quantitative
dð1Þ
ik ¼ P
N
j¼1
xij  xkj


5
Makhalonobis’s distance
The
quantitative
dM
ik ¼ xij  xkj

TW1 xij  xkj


W—covariance matrix of sample
X ¼ X1; X2; . . .Xn
f
g
6
Zhuravlev’s distance
The mixed
dik ¼ P
N
j¼1
I j
ik,
where
I j
ik ¼
1; if xij  xkj

\e
0; otherwise
	
264
7
The Cluster Analysis in Intellectual Systems

In not hierarchical algorithms their work and conditions of stop need to be
regulated in advance often with large number of parameters that is sometimes
difﬁcult, especially at the initial stage of investigation. But in such algorithms big
ﬂexibility in a variation of a clustering is reached and usually the number of clusters
is deﬁned.
On the other hand, when objects are characterized by a large number of features
(parameters), a task of grouping features is important. Initial information contains in
a square matrix of features interconnections, in particular, in a correlation matrix.
Basis of the successful solution of a grouping task is the informal hypothesis of a
small number of the hidden factors which deﬁne structure of an interconnection
between features.
In hierarchical algorithms one actually refuses to deﬁne a number of clusters,
building a full tree of the enclosed clusters (so-called dendrogram). The number of
clusters is deﬁned from the assumptions, in principle, which aren’t relating to work
of algorithms, for example on dynamics of change of a threshold of splitting
(merge) of clusters. Difﬁculties of such algorithms are well studied: choice of
measures of proximity of clusters, problem of inversions of indexation in the
dendrogramms, inﬂexibility of hierarchical classiﬁcations which is sometimes
undesirable. Nevertheless, representation of a clustering in the form of a dendro-
gramm allows to gain the most complete display of structure of clusters.
Hierarchical algorithms are connected with dendrogramms construction and
divided on:
1. agglomerative, characterized by consecutive merge of initial elements and the
corresponding reduction of number of clusters (creation of clusters from below
to top);
2. divisional (divided) in which the number of clusters increases, starting with one
cluster therefore the sequence of the splitting groups is constructed (creation of
clusters from top to down).
7.2.1
Hierarchical Algorithms. Agglomerative Algorithms
On the ﬁrst step all the set of objects is represented as a set of clusters:
c1 ¼ fi1g; c2 ¼ fi2g; . . .; cm ¼ fimg
On the following step two closest one to another clusters are chosen (for
example, Cp and Cq) and unite in one joint cluster. The new set consisting already
of m −1 of clusters will be such:
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality and Metrics
265

c1 ¼ fi1g; c2 ¼ fi2g; . . .; cp ¼ fip; iqg; . . .; cp ¼ fimg
Repeating process, we obtain step by step the consecutive sets consisting of
(т −2), (m −3), (m −4) and etc. clusters.
At the end of procedure the cluster consisting of m of objects and coinciding
with an initial set I will be obtained.
For determination of distance between clusters it is possible to choose different
metrics. Depending on it algorithms with various properties exist.
There are some methods of recalculation of distances with use of old values of
distances for the united clusters differing in coefﬁcients in a formula [1]:
drs ¼ apdps þ aqdqs þ bdpq þ c dps  dqs


If clusters p and q unite in one cluster of r and it is required to calculate distance
from a new cluster to cluster say, s, application of this or that method depends on a
way of determination of distance between clusters, these methods differ with values
of coefﬁcients ap; aq; b; c.
Coefﬁcients of recalculation of distances between clusters are speciﬁed in
Table 7.2 ap; aq; b; c.
Table 7.2 Coefﬁcients of distances recalculation between clusters
Name of a method
ap
aq
b
c
Distance between the closest neighbours—the
closest objects of clusters (Nearest neighbour)
1/2
1/2
0
−1/2
Distance between the farthest neighbours
(Furthest neighbour)
1/2
1/2
0
1/2
The method of medians—the same centroid
method, but the centre of the integrated cluster
is calculated as an average of all objects
(Median clustering)
1/2
1/2
−1/4
0
Average distance between clusters
(Between—groups linkage)
1/2
1/2
0
0
Average distance between all objects of couple
of clusters taking into account distances inside
clusters (intra-groups linkage)
kp
kp þ kq
kq
kp þ kq
0
0
Distance between centres clusters (Centroid
clustering), or centroid method. A lack of this
method is that the centre of the integrated cluster
is calculated as an average of the centres of the
united clusters, without their volume
kp
kp þ kq
kp
kp þ kq
kpkq
kp þ kq
0
Ward’s method.
As distance between clusters the gain of the sum
of squares of distances of objects to the centres
of clusters received as a result of their
association is calculated
kr þ kp
kr þ kq þ kp
kr þ kp
kr þ kq þ kp
kr
kr þ kq þ kp
0
266
7
The Cluster Analysis in Intellectual Systems

7.2.2
Divisional Algorithms
Divisional cluster algorithms, unlike agglomerative, on the ﬁrst step represent all set
of elements I as the only cluster. On each step of algorithm one of the existing
clusters is recursively divided into two afﬁliated. Thus, clusters from top to down
are iteratively formed. This approach isn’t so in detail described in literature de-
voted to the cluster analysis, as agglomerative algorithms. It is applied when it is
necessary to divide all set of objects on rather small amount clusters.
One of the ﬁrst the divisional algorithms was offered by Smith Maknaoton in
1965 [1].
All elements are located on the ﬁrst step in one cluster C1 ¼ I.
Then the element, at which average value of distance from other elements in this
cluster is the greatest is selected.
The chosen element is removed from a cluster of C1 and becomes the ﬁrst
member of the second cluster C2.
On each subsequent step an element in a cluster of C1 for which the difference
between average distance to the elements which are in C2, and average distance to
the elements remaining in C1 is the greatest is transferred to C2.. Transfer of
elements from C1 in C2 proceed until the corresponding differences of averages
become negative, i.e. so far there are elements located to elements of a cluster of C2
closer than to cluster elements of C1.
As a result one cluster is divided into two afﬁliated ones which will be split at the
following level of hierarchy. Each subsequent level procedure of division is applied
to one of the clusters received at the previous level. The choice of cluster to be split
can be carried out differently.
In 1990 Kauffman and Rouzeuv suggested to choose at each level a cluster for
splitting with the greatest diameter which is calculated on a formula [1]
DC ¼ max dðip; iqÞ8ip; iq 2 C
Recursive division of clusters proceeds, so far all clusters or won’t become
sigleton (i.e. consisting of one object), or so far all members of one cluster won’t
have zero difference from each other.
7.2.3
Not Hierarchical Algorithms
The great popularity at the solution of clustering problems was acquired by the
algorithms based on search of splitting a data set into clusters (groups). In many
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality and Metrics
267

tasks algorithms of splitting are used owing to the advantages. These algorithms try
to group data (in clusters) so that criterion function of splitting algorithm reaches an
extremum (minimum). We’ll consider three main algorithms of a clustering based
on splitting methods. In these algorithms the following basic concepts are used:
• the training set (an input set of data) of M on which splitting is based;
• distance metrics:
d2
Aðmj; cðiÞÞ ¼
mj  cðiÞ




2¼ ðmj  cðiÞÞtAðmj  cðiÞÞ
ð7:6Þ
where the matrix A deﬁnes a way of distance calculation. For example, for a
singular matrix distance according to Euclid metrics is used;
• vector of the centers of clusters C;
• splitting matrix on clusters U;
• goal function J ¼ JðM; d; C; UÞ;
• set of restrictions.
Algorithm K-means (Hard C-means)
Consider in more detail algorithm for the example of clustering data on irises. For
descriptive reasons the presentation is limited to only two parameters (instead of
four)—length and width of a sepal. It allows to present this problem in
two-dimensional space (Fig. 7.1). Points are noted by numbers of objects.
At the beginning k of any points in space of all objects are taken as possible
centers. It isn’t really critical which points will be initial centers, procedure of a
choice of starting points will affect, mainly, on calculation time. For example, it
may be the ﬁrst k of objects of the set I. In this example it is points 1, 2 and 3.
Further iterative operation of two steps is carried out.
On the ﬁrst step all objects were split into k = 3 groups, the closest to one of the
centers. The proximity is deﬁned by distance which is calculated by one of the ways
Fig. 7.1 Example of
clustering data on irises by
algorithm hard C-means
268
7
The Cluster Analysis in Intellectual Systems

described earlier (for example, the Euclidean distance is used). Figure 7.2 illustrates
splitting irises into three clusters.
On the second step the new centers of clusters are calculated. The centers can be
calculated as average values of the variable objects referred to the created groups.
The new centers, naturally, can differ from the previous ones.
In Fig. 7.3 the new centers and new division according to them are marked. It is
natural that some points which earlier were related to one cluster, at new splitting
are related to another (in this case such points are 1, 2, 5, etc.). The new centers in
the Fig. 7.3 are marked with symbol “x”, inside a circle.
The considered operation repeats recursively until the centers of clusters (re-
spectively, and borders between them) cease to change. In our example the second
splitting was the last. If to look at results of a clustering, it is possible to notice that
the lower cluster completely coincides with the class Iris setosa. In other clusters
there are representatives of both classes. In this case it was the result of a clustering
with only two parameters, but not with all four.
Fig. 7.2 Example of
clustering data on irises by
algorithm hard C-means
Fig. 7.3 Example of
clustering data on irises by
algorithm hard C-means
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality and Metrics
269

After objects are mapped in points of multidimensional space, procedure of
automatic splitting into clusters becomes very simple. A problem is that initial
objects can not always be presented in the form of points. Introduction of a metrics,
distances between categorial variables or the relations is more difﬁcult.
In summary it should be noted that the method K-means works well if data may
be separated on compact, approximately spherical groups.
This algorithm is a prototype practically of all algorithms of fuzzy clustering, and
its consideration will help the best understanding of the principles underlying in
more complex algorithms.
Description of K-means Algorithm
Basic deﬁnitions and concepts within this algorithm are following:
• the training set M ¼ fmjgd
j¼1 d—number of points (vectors) of data;
• the distance metrics counted by a formula (7.6);
• vector of the centers of clusters C ¼ fcðiÞgc
i¼1
where
cðiÞ ¼
Pd
j¼1 uijmj
Pd
j¼1 uij
; 1  i  c
ð7:7Þ
• splitting matrix U ¼ fuijg
where
uðlÞ
ij ¼
1; if dðmj; cðlÞ
i Þ ¼ min1  k  c dðmj; cðlÞ
k Þ
0; otherwise
	
ð7:8Þ
Object function
JðM; U; CÞ ¼
X
c
i¼1
X
d
j¼1
uijd2
Aðmj; cðiÞÞ
• set of restrictions
fuijg 2 f0; 1g;
X
c
i¼1
uij ¼ 1; 0\
X
d
j¼1
uij\d
ð7:10Þ
which deﬁnes that each vector of data can belong only to one cluster and doesn’t
belong to the rest. Each cluster contains not less than one point, but less than a total
number of points.
270
7
The Cluster Analysis in Intellectual Systems

Structurally the algorithm represents the following iterative procedure [1].
Step 1. To initialize initial splitting (for example, in a random way), to choose
accuracy value d (it is used in a condition of end of an algorithm), to initialize a
number of iteration l = 0.
Step 2. To deﬁne the centers of clusters by the following formula:
cðlÞ
i
¼
Pd
j¼1 uðl1Þ
ij
mj
Pd
j¼1 uðl1Þ
ij
; 1  i  c
ð7:11Þ
Step 3. To update a splitting matrix to minimize squares of errors, using a
formula
uðl1Þ
ij
¼
1; if dðmj; cðlÞ
i Þ ¼ min1  k  c dðmj; cðlÞ
k Þ
0; otherwise
	
ð7:12Þ
Step 4. To check a condition
UðlÞ  Uðl1Þ




\d. If the condition is satisﬁed,
ﬁnish process if it isn’t true then pass to a step 2 with number of iteration l ¼ l þ 1.
The main shortcoming inherent to this algorithm owing to discrete character of
elements of a splitting matrix is the big size of splitting space. One way to over-
come this shortcoming is the choice of elements of a splitting matrix by numbers
from a unit interval. That is, belonging of a data element to a cluster has to be
deﬁned by membership function— the element of data can belong to several
clusters with various degree of membership. In that case we come to a problem of
fuzzy clustering. This approach found the embodiment in algorithm of fuzzy
clustering—fuzzy method of K-means (Fuzzy C-Means).
7.3
Fuzzy C-Means Method
Consider a neural network with self-organization where training is performed without
a teacher. The algorithm of self-organization refers a vector x to the corresponding
cluster of data which is presented by its center, using a competitive training.
The basic form of algorithm of self-organization allows to ﬁnd precisely position
of the centers of the relevant groups (clusters) into which the output multidimen-
sional space is split. These centers can be used further in hybrid algorithm of
training of FNNs as initial values that considerably accelerates process of training
and guarantees convergence to a global minimum.
Algorithm of Fuzzy C-means
Let’s assume that in a network exists m fuzzy neurons with the centers in points
cj; ðj ¼ 1; 2; . . .; mÞ. Initial values of these centers can be chosen randomly from
areas of admissible values of the corresponding components of vectors xk; ðk ¼
1; 2; . . .; NÞ used for training. Let function of a fuzziﬁcation be set in the form of the
generalized Gauss function expressed by a formula (3.45) in Chap. 3.
7.2
Cluster Analysis, Problem Deﬁnition. Criteria of Quality and Metrics
271

The vector entered in a network input xk will belong to various groups repre-
sented by the centers cj, with degree wkj, and 0\wkj\1, and total degree of
membership to all groups, is obviously, equal 1. Therefore
X
m
j¼1
wkj ¼ 1;
ð7:13Þ
for all wkj ðk ¼ 1; 2; . . .; NÞ.
The function of an error corresponding to such representation can be deﬁned as
the sum of individual errors of membership to the centers ci taking into account
fuzziness degree b. Therefore,
E ¼
X
m
j¼1
X
N
k¼1
wb
kj cj  xk




2
ð7:14Þ
where b is a weight coefﬁcient which accepts values from an interval ð1; 1Þ. The
training goal of self-organization consists in such selection of the centers cj, that for
the whole set of the training vectors xk—achievement of a minimum of function
(7.14) at simultaneous fulﬁllment of conditions (7.13) is attained. Thus it is a
problem of minimization of nonlinear function (7.14) with N constraints of type
(7.13). The solution of this task can be transferred to minimization of Lagrange
function deﬁned by the form [3].
LE ¼
X
m
j¼1
X
N
k¼1
wb
kj cj  xk




2 þ
X
N
k¼1
kk
X
m
j¼1
wkj  1
 
!
ð7:15Þ
where kk ðk ¼ 1; 2; . . .; NÞ are Lagrange’s multipliers. In [4] it is proved that the
solution of a task (7.15) can be presented in the form
cj ¼
PN
k¼1 wb
kjxk
PN
k¼1 wb
kj
;
ð7:16Þ
wkj ¼
1
P
m
i¼1
d2
kj
d2
ij
  1
b1 ;
ð7:17Þ
where
dkj—is
Euclidean
distance
between
the
center
cj
and
vector
xk,
dkj ¼
cj  xk




. As exact values of the centers cj at the beginning of process aren’t
known, the training algorithm has to be iterative. It can be formulated in the
following way:
1. To execute random initialization of coefﬁcients wkj, choosing their values from
an interval [0, 1] so that the condition (7.13) be satisﬁed.
272
7
The Cluster Analysis in Intellectual Systems

2. To deﬁne К centers cj, in accordance with (7.16).
3. To calculate value of the error function according to expression (7.14). If its
value appears below the established threshold or if reduction of this error of
previous iteration is negligible, to ﬁnish calculations. The last values of the
centers represent the required decision. Otherwise, go to step 4.
4. To calculate new values wkj in a formula (7.17) and to pass to step 2.
We‘ll call such procedure the fuzzy self-organization algorithm C-means.
Repetition of iterative procedure leads to achievement of a minimum of function
E which won’t be a global minimum. The quality of the found centers estimated by
value of an error function E essentially depends on preliminary selection of values
wkj and centers cj. As the best will be such placement of the centers at which they
settle down in the areas containing the greatest number of the shown vectors xj. At
such selection of the centers they will represent vectors of data xj with the smallest
total error.
Therefore the beginning of iterative procedure of calculation of optimum values
of the centers has to be preceded by procedure of their initialization. Algorithms of
peak and differential grouping of data belong to the most known algorithms of
initialization.
7.4
Deﬁnition of Initial Location of the Centers of Clusters
7.4.1
Algorithm of Peak Grouping
The algorithm of peak grouping was offered by Jager and Filev [3, 5].
When using N input vectors the special grid which evenly covers space of these
vectors is constructed. Nodes of this grid are considered as potential centers #, for
each of which peak function is calculated:
mð#Þ ¼
X
N
k¼1
exp
 #  xk
k
k2b
2r2
(
)
ð7:18Þ
where r is some constant which is selected separately for each speciﬁc task.
Value mð#Þ is considered as an assessment of height of peak function. It is
proportional to quantity of vectors xj, which get to the vicinity of the potential
center #. Great value mð#Þ testiﬁes to that the center # locates in the area in which
the greatest number of vectors is concentrated fxkg.
The coefﬁcient of r inﬂuences ﬁnal proportions between mð#Þ and # slightly.
After calculation of values mð#Þ for all potential centers the ﬁrst center is
selected c1, which has the greatest value mð#Þ. For a choice of the following centers
it is necessary to exclude c1 and nodes which are placed in close proximity to c1.
7.3
Fuzzy C-Means Method
273

It can be done by redeﬁnition of peak function at the expense of separation of
Gauss function from it with the center in a point c1. Having designated this new
function through mnewð#Þ, we receive:
mnewð#Þ ¼ mð#Þ  mðc1Þ exp
 #  c1
k
k2b
2r2
(
)
ð7:19Þ
Note that this function has zero in a point c1.
Then the same procedure repeats with the next center c2, etc.
Process of ﬁnding of the following centers c2, c3 is realized consistently on the
modiﬁed values mnewð#Þ, which turn out at an exception of the next neighbors of
the center which was found at the previous stage. It comes to an end at the moment
of localization of all the centers.
The method of peak grouping is effective at not really big dimension of a vector
of X. Otherwise number of the potential centers increases as avalanche.
7.4.2
Algorithm of Differential Grouping
The algorithm of differential grouping is a modiﬁcation of the previous algorithm,
in which vectors xj are considered as the potential centers #. Peak function DðxiÞ in
this case takes the form [2]:
DðxiÞ ¼
X
N
j¼1
exp
 xi  xj




2b
ra=2
ð
Þ2
(
)
;
ð7:20Þ
where value of coefﬁcient ra deﬁnes the sphere of the neighborhood. On value
DðxiÞ considerably inﬂuence only vectorsxj, which are inside this sphere.
At the big density of points near xi function value DðxiÞ is large. After calcu-
lation of values of peak function for each point xi, the vector x is found, for
which peak function DðxÞ will appear to be the greatest. This point becomes the ﬁrst
center c1.
Choice of the following center c2 is performed after an exception of the previous
center and all points which lie in its vicinity
As well as in the previous case peak function is redeﬁned so
DnewðxiÞ ¼ DðxiÞ  Dðc1Þ exp
 xi  c1
k
k2b
rb=2
ð
Þ2
(
)
At new deﬁnition of function D coefﬁcients rb designate new values of a constant
which sets the sphere of the neighborhood of the following center. Usually a
condition rb  ra is used.
274
7
The Cluster Analysis in Intellectual Systems

After modiﬁcation of value of peak function a search of a new point x, for which
DnewðxiÞ ! max is performed. It becomes the new center.
Process of ﬁnding of the next center is resumed after the exception of all already
selected points. Initialization comes to an end at the time of ﬁxing of all centers
which are provided by entry conditions.
7.5
Gustavson-Kessel’s Fuzzy Cluster Analysis Algorithm
In classical algorithm fuzzy C = means elements of error function E are obtained by
means of usual Euclid distance between a vector x and the center of a cluster c with:
dðx; cÞ ¼ x  c
k
k ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx  cÞTðx  cÞ
q
At such metrics of distance between two vectors the set of the points equidistant
from the center represents a sphere with an identical scale on all axes. But if data form
groups which form differs from spherical or if scales of separate coordinates of a
vector strongly differ, such metrics becomes inadequate. In this case quality of a
clustering can be increased considerably at the expense of the improved version of the
self-organization algorithm which is called as Gustavson-Kessel’s algorithm [2, 3].
The main changes of basic algorithm fuzzy C-means consist in introduction to a
metrics calculation formula of the scaling matrix A. At such scaling the distance
between the center c and vectors x is deﬁned by a formula:
dðx; cÞ ¼ x  c
k
k ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx  cÞTAðx  cÞ
q
ð7:21Þ
As scaling usually the positive-deﬁnite matrix is used, that is a matrix, at which
all eigenvalues are real and positive.
Similar to the basic algorithm C-means the training goal of Gustavson-Kessel
algorithm lies in such placement of the centers at which the criterion E is
minimized:
E ¼
X
k
X
j
wb
kjd2ðxk; cjÞ
ð7:22Þ
Description of Gustavson-Kessel algorithm:
1. To carry out initial placement of the centers in data space. To create an ele-
mentary form of the scaling matrix A.
7.4
Deﬁnition of Initial Location of the Centers of Clusters
275

2. To create a matrix of membership coefﬁcients of all vectors x to the centers by a
formula:
wkj ¼
1
P
m
i¼1
d2
kj
d2
ij
  1
b1
ð7:23Þ
3. To calculate new placement of the centers according to a formula:
cj ¼
PN
k¼1 wb
kjxk
PN
k¼1 wb
kj
;
ð7:24Þ
4. To generate a covariance matrix for each vector:
Sj ¼
X
N
k¼1
wb
kjðxk  cjÞðxk  cjÞT
ð7:25Þ
5. To calculate a new scaling matrix for each ith centre by a formula:
Aj ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
detðSjÞ
nq
S1
j
ð7:26Þ
6. If the last changes of centers and a covariance matrix are rather small in relation
to the previous values (don’t exceed the set values), ﬁnish iterative process,
otherwise go to step 2.
7.6
Adaptive Robust Clustering Algorithms
Possibilistic Clustering Algorithms
Major drawbacks associated with a probabilistic approach (Fuzzy C-means algo-
rithm)are connected with constraints (7.13). In the simplest case of two clusters
(m ¼ 2) is easy to see that the observation xk, equally owned by both clusters and
observation xp, not belonging to any of them, may have the same levels of mem-
bership wðk;1Þ ¼ wðk;2Þ ¼ wðp;1Þ ¼ wðp;2Þ ¼ 0:5. Naturally, this fact decreasing the
accuracy of classiﬁcation, led to a possibilistic approach to the fuzzy classiﬁcation
[6]. In the possibilistic clustering algorithm goal function has the form
276
7
The Cluster Analysis in Intellectual Systems

Eðwk;j; cjÞ ¼
X
m
j¼1
X
N
k¼1
wb
k;jd2ðxk; cjÞ þ
X
m
j¼1
lj
X
N
k¼1
ð1  wk;jÞb
 
!
;
ð7:27Þ
where scalar parameter lj [ 0 determines the distance on which membership level
takes the value 0.5, that is if d2ðxk; cjÞ ¼ lj, then wk;j ¼ 0:5.
Minimization (7.27) by wk;j, cj, lj gives evident solution
wk;j ¼
1 þ
d2ðxk; cjÞ
lj
 
! 1
b1
0
@
1
A
1
;
ð7:28Þ
cj ¼
PN
k¼1 wb
kjxk
PN
k¼1 wb
kj
;
ð7:29Þ
lj ¼
PN
k¼1 wb
k;jd2ðxk; cjÞ
PN
k¼1 wb
k;j
;
ð7:30Þ
It can be seen that the possibilistic and probabilistic algorithms are very similar
and pass one into other by replacing the expression (7.27) to the formula (7.15), and
vice versa. A common disadvantage of the considered algorithms is their compu-
tational complexity and the inability to work in real time. The algorithm (7.15)
−(7.17) begins with the initial task of normal random partitions matrix W0. On the
basis of its values initial set of prototypes c0
j is calculated which then is used to
calculate a new matrix W1. Then this procedure is continued and sequence of
solutions c1
j W2; . . .; Wt; ct
jWt þ 1 etc. is obtained., until the difference
Wt þ 1  Wt




is less than a preassigned threshold e.
Therefore, all available data sample is processed repeatedly.
The solution obtained using a probabilistic algorithm, is recommended as the
initial conditions for possibilistic algorithm (7.28)−(7.30) [6, 7]. Parameter distance
lj is initialized in accordance with (7.30) on the results of the probabilistic
algorithm.
7.6.1
Recurrent Fuzzy Clustering Algorithms
Analysis of (7.15) shows that, for the calculation of membership levels wk;j instead
of the Lagrangian (7.15) can be used its local modiﬁcation:
7.6
Adaptive Robust Clustering Algorithms
277

Lðwk;j; cj; kkÞ ¼
X
m
j¼1
wb
k;jd2 xk; cj


þ kk
X
m
j¼1
wk;j  1
 
!
ð7:31Þ
Optimization
of
the
expression
(7.31)
by
the
procedure
of
the
Arrow-Hurwicz-Uzawa leads to an algorithm
wk;j ¼
d2ðxk; ck;jÞ
1
b1
Pm
l¼1 d2ðxk; ck;lÞ

 1
b1
ð7:32Þ
ck þ 1;j ¼ ck;j  gkrcjLkðwk;j; ck;j; kkÞ ¼
¼ ck;j  gkwb
k;jdðxk þ 1; ck;jÞrcj
ð7:33Þ
Procedure (7.32), (7.33) is close to the learning algorithm Chang-Lee [8], and for
b ¼ 2 coincides with the gradient procedure clustering Park-Degger [9]:
wk;j ¼
xk  ck;j




2
Pm
l¼1 xk  ck;l




2
ð7:34Þ
ck þ 1;j ¼ ck;j  gkw2
k;jðxk þ 1  ck;jÞ
ð7:35Þ
Within the framework of possibilistic approach local criterion takes the form
Ekðwk;j; cjÞ ¼
X
m
j¼1
wb
k;jd2 xk; cj


þ
X
m
j¼1
ljð1  wk;jÞb
ð7:36Þ
and the result of its optimization has the form
wk;j ¼
1 þ
d2ðxk; ck;jÞ
lj
 
! 1
b1
0
@
1
A
1
ð7:37Þ
ck þ 1;j ¼ ck;j  gkwb
k;jdðxk þ 1; ck;jÞrcj
ð7:38Þ
where the distance parameter lj initialized according to (7.30).
In this case, N in Eq. (7.30) is a volume of data set used for initialization.
In the quadratic case, the algorithm (7.37), (7.38) is converted into a rather
simple procedure and optimization result is of the form
wk;j ¼
lj
lj þ xk  ck;j




2
ð7:39Þ
278
7
The Cluster Analysis in Intellectual Systems

wherein lj is the distance parameter which is initialized by the results of the
probabilistic clustering (for example, using an algorithm Fuzzy C-means (7.15)
−(7.17) according to the equation:
lj ¼
PN
k¼1 w2
k;j xk  cj




2
PN
k¼1 w2
k;j
;
ð7:41Þ
7.6.2
Robust Adaptive Algorithms of Fuzzy Clustering
The considered above clustering methods can effectively solve the problem of
classiﬁcation with a substantial intersection of the clusters, however, it assumes that
the data within each cluster are located compactly enough without sharp (abnormal)
outliers.
However, it should be noted that the actual data are usually distorted by outliers,
the share of which according to some estimates [10], is up to 20 % so that to speak
of a compact placement of data is not always correct.
In this regard, recently, much attention was paid to problems of fuzzy cluster
analysis of the data, the density distribution of which differs from the normal by
presence of “heavy tails” [11, 12].
Robust Recursive Algorithm for Probabilistic Fuzzy Clustering
After standardization of feature vectors components so that all source vectors would
belong to the unit hypercube ½0; 1n, the objective function is constructed
Eðwk;j; cjÞ ¼
X
m
j¼1
X
N
k¼1
wb
k;jDðxk; cjÞ
ð7:42Þ
under constraints
X
m
j¼1
wk;j ¼ 1; k ¼ 1; . . .; N;
ð7:43Þ
0\
X
N
k¼1
wk;j  N; j ¼ 1; . . .; m:
ð7:44Þ
Here Dðxk; cjÞ is a distance between xk and cj in adopted metric. The result of
clustering is assumed to be N  m matrix W ¼ fwk;jg, called “matrix of fuzzy
decomposition.” Typically, as the distance function Dðxk; cjÞ Minkowski metric Lp
is applied [12]
7.6
Adaptive Robust Clustering Algorithms
279

Dðxk; cjÞ ¼
Xn
i¼1 xk;i  cj;i

p

1
p; p  1;
ð7:45Þ
where xk;i; cj;i are the ith components of ðn  1Þ—vectors xk; cj correspondingly.
Estimates relating to the quadratic objective functions are optimal when the data
belong to the class of distributions with ﬁnite variance, the most famous member of
which is a Gaussian.
Varying parameter p allows to improve the properties of the robustness of
clustering procedures, however, the quality of assessment is determined by the type
of data distribution. Thus, the estimates with p = 1 are optimal for the Laplacian
data distribution, but their construction involves great computational expense. Quite
realistic is the class of approximate normal distributions [12].
Approximately normal distributions are mixture of Gaussian density and dis-
tribution of some arbitrary density, which distorts with outliers the normal distri-
bution. The optimal objective function in this case is the quadratic-linear, and tends
to linear type as the distance from the minimum grows.
The most prominent representative of the approximate normal distribution
density function is
pðxi; ciÞ ¼ Seðci; siÞ ¼ 1
2si
sec h2 xi  ci
si
;
ð7:46Þ
where ci,si are parameters, determining a center and a width of the distribution.
This function resembles a Gaussian in the vicinity of the center, however, has a
more heavy tails. With the distribution (7.46) is associated an objective function
fiðxi; ciÞ ¼ bi ln cosh xi  ci
bi
;
ð7:47Þ
where the parameter bi deﬁnes steepness of this function, while in the vicinity of the
minimum this function is very close to the quadratic, tending with the growth of
X to a linear one.
Also interesting is the fact that the derivative of this function
f 0
i ðxiÞ ¼ uðxiÞ ¼ tanh xi
bi
;
ð7:48Þ
is a standard activation function of artiﬁcial neural networks (Back propagation, see
Chap. 1). Using as a metric the following structure
DRðxk; cjÞ ¼
Xn
i¼1 fi xk;i; cj;i


¼
X
n
i¼1
bi ln cosh xk;i  cj;i
bi
;
ð7:49Þ
is possible to introduce the objective function of robust classiﬁcation [12]
280
7
The Cluster Analysis in Intellectual Systems

ERðwk;j; cjÞ ¼
X
N
k¼1
X
m
j¼1
wb
k;jD2 xk; cj


¼
¼
X
N
k¼1
X
m
j¼1
wb
k;j
X
n
i¼1
bi ln cosh xk;i  cj;i
bi
ð7:50Þ
and a corresponding Lagrangian
L ¼
X
N
k¼1
X
m
j¼1
wb
k;j
X
n
i¼1
bi ln cosh xk;i  cj;i
bi
þ
X
N
k¼1
kk
X
m
j¼1
wk;j  1
 
!
ð7:51Þ
where kk—is indeﬁnite Lagrange multiplier, ensuring fulﬁllment of constraints
(7.43), (7.44). The saddle point of the Lagrangian (7.51) can be found by solving
the equations system of Kuhn-Tucker
@Lðwk;j;cj;kkÞ
@wk;j
¼ 0;
@Lðwk;j;cj;kkÞ
@kk
¼ 0;
rcjLðwk;j; cj; kkÞ ¼ 0:
8
>
<
>
:
ð7:52Þ
Solutions of the ﬁrst and second equations lead to well-known results
wk;j ¼
DRðxk;cjÞ
ð
Þ
1
1b
Pm
l¼1 DRðxk;clÞ
ð
Þ
1
1b
kk ¼  Pm
l¼1 bDRðxk; clÞ
ð
Þ
1
1b

1b
8
>
>
<
>
>
:
ð7:53Þ
But the third equation
rcjLðwk;i; cj; kkÞ ¼
X
N
k¼1
wb
k;jrcjDR xk; cj


¼ 0;
ð7:54Þ
evidently has no analytic solution. The solution of Eq. (7.54) can be obtained with
the help of local modiﬁcation of Lagrangian and recurrent fuzzy clustering algo-
rithm [14].
Search of the Lagrangian local saddle point
Lkðwk;j; cj; kkÞ ¼
X
m
j¼1
wb
k;jDR xk; cj


þ kk
X
m
j¼1
wk;j  1
 
!
ð7:55Þ
using procedures Arrow-Hurwitz-Udzawa leads to an algorithm
7.6
Adaptive Robust Clustering Algorithms
281

wpr
k;j ¼
DR xk;cj
ð
Þ
ð
Þ
1
1b
Pm
l¼1 DRðxk;clÞ
ð
Þ
1
1b
ck þ 1;j;i ¼ ck;j;i  gk
@Lðwk;j;cj;kkÞ
@cj;i
¼ ck;j;i þ gkwb
k;j tanh xk;ick;j;i
bi
8
>
<
>
:
ð7:56Þ
where gkis a parameter of learning rate, ck;j;i is the ith component of the jth pro-
totype calculated at the kth step.
But despite low computational complexity this algorithm (7.56) has the disad-
vantage inherent to all probabilistic clustering algorithm.
7.7
Robust Recursive Algorithm of Possibilistic Fuzzy
Clustering
For possibilistic fuzzy clustering algorithms the criterion is the following expres-
sion [14]
ERðwk;j; cj; liÞ ¼
X
N
k¼1
X
m
j¼1
wb
k;jD2 xk; cj


þ
X
m
j¼1
li
X
N
k¼1
ð1  wk;jÞb
ð7:57Þ
Minimization of (7.57) by parameters wk;j, cj and li leads to equations system
@ERðwk;j;cj;ljÞ
@wk;j
¼ 0;
@ERðwk;j;cj;ljÞ
@kk
¼ 0;
rcjERðwk;j; cj; ljÞ ¼ 0:
8
>
>
<
>
>
:
ð7:58Þ
The solution of the ﬁrst two equations of (7.58) leads to the well-known result
wpos
k;j ¼
1 þ
DR xk;cj
ð
Þ
lj

 1
b1
 
!
lj ¼
P
N
k¼1
wb
k;jDR xk;cj
ð
Þ
P
N
k¼1
wb
k;j
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
1
ð7:59Þ
while the third one
rcjERðwk;j; cj; ljÞ ¼
X
N
k¼1
wb
k;jrcjDR xk; cj


¼ 0
ð7:60Þ
fully corresponds to (7.54).
282
7
The Cluster Analysis in Intellectual Systems

Introducing the local modiﬁcation of (7.57)
ER
k ¼
X
m
j¼1
wb
k;jDR xk; cj


þ
X
m
j¼1
ljð1  wk;jÞb ¼
¼
X
m
j¼1
wb
k;j
X
n
i¼1
bi ln cosh xk;i  cj;i
bi
þ
X
m
j¼1
ljð1  wk;jÞb
ð7:61Þ
and optimizing it we obtain:
wpos
k;j ¼
1 þ
DR xk;cj
ð
Þ
lj

 1
b1
 
!
ck þ 1;j;i ¼ ck;j;i  gk
@ER
k ðwk;j;cj;ljÞ
@cj;i
¼ ck;j;i þ gkwb
k;j tanh xkck;j;i
bi
8
>
>
<
>
>
:
ð7:62Þ
where the distance parameter lkj may be determined according to the second
equation of the system (7.59) for k observations rather than the entire sample
volume N.
It should be noted that the last equation of system (7.52) and (7.58) are identical
and are determined only by choice of metrics. This makes possible to use any
suitable metric for a particular case, which will determine only the setup procedure
of prototypes if the equation for calculating the weights still remains the same.
Considered robust recursive methods may be used in a batch mode and in the
on-line mode as well. In the last case the number of observation k represents a
discrete time.
Experiments with a repository of data, distorted by abnormal outliers (emis-
sions), have shown high efﬁciency of the proposed algorithms in the processing of
the information given in the form of tables “object-property” [10, 11] and in the
form of time series [14].
In particular, the problem of data classiﬁcation of specially artiﬁcially generated
sample containing three-dimensional cluster of data was considered, whose
observations are marked the symbols “o”, “x” and “ + ” [12] (see. Fig. 7.4). Points
in each cluster sampling are distributed according to the density of Laplace dis-
tribution having “heavy tails”
pðxiÞ ¼ rð1 þ ðxi  cÞ2Þ1
ð7:63Þ
where r and c are width and center correspondingly.
The sample includes 9,000 observations (3,000 in each cluster) and is divided
into training (7200 cases) and testing (1800 cases) subsamples [12, 14].
It should be noted that some observations are very far away from the centers of
the clusters (Fig. 7.4a). Prototypes of the clusters are located in the central region of
the data as shown in Fig. 7.4b. In order to ﬁnd the correct prototypes clustering
algorithm should be insensitive to outliers.
7.7
Robust Recursive Algorithm of Possibilistic Fuzzy Clustering
283

For all of the algorithms involved in the comparison, the procedure of the
experiment was performed as follows. At the beginning of training a sample was
clustered by appropriate algorithms and prototypes of clusters have been found.
Then, training and testing samples were classiﬁed according to the results of clus-
tering. Observations belonging to each cluster in the classiﬁcation process are cal-
culated in accordance with Eqs. (7.17), (7.56) or (7.62) depending on the type of
clustering algorithm. The cluster, to which the observation belongs with a maximum
membership degree, deﬁnes the class of this observation. Classiﬁcation and training
is performed in the on-line mode of receiving observations, where b ¼ 2;
b1 ¼ b2 ¼ b3 ¼ 1;
gðkÞ ¼ 0:01. The results are shown in Table 7.3 [12]
In the Fig. 7.5 it can be easily seen that the centers of the clusters (prototypes)
produced by the algorithm « fuzzy C-means » by Bezdek, are shifted from the
visual centers of the clusters, due to the presence of “heavy tails” of the data
distribution density, in contrast to the robust methods with objective function (7.57)
Fig. 7.4 Full sample (a) and
its central part (b)
284
7
The Cluster Analysis in Intellectual Systems

and (7.61) in which prototypes are found more precisely, which is conﬁrmed by the
less classiﬁcation error (see Table 7.3).
Continuous growth in the successful application of computational intelligence
technologies in the areas of data analysis conﬁrms the versatility of this approach.
At the same time, real problems that arise in the processing of very large databases
(Big Data), complicate the use of existing algorithms and tools and demand to be
improved to meet the challenges of data mining in real time using the paradigms of
CI and soft computing.
7.8
Application of Fuzzy Methods C-Means
and Gustavson-Kessel’s in the Problems of Automatic
Classiﬁcation
Consider
application
of fuzzy
methods
of
cluster
analysis
C-means
and
Gustavson-Kessel’s in the test and practical problems of automatic classiﬁcation [3].
Table 7.3 Results of classiﬁcation. Classiﬁcation error
Algorithm
Training sample
Testing sample
Fuzzy C-means
17.1 % (1229 obs.)
16.6 % (299 obs.)
Robust probabilistic
15.6 % (1127 obs.)
15.6 % (281 obs.)
Robust possibilistic
15.2 % (1099 obs.)
14.6 % (263 obs.)
Fig. 7.5 Prototypes of clusters
7.7
Robust Recursive Algorithm of Possibilistic Fuzzy Clustering
285

Input data: set of points, number of clusters.
Output: Membership matrix U, cluster centers.
Example 7.1 Initial sampling points are shown in Fig. 7.6. The centers of the
clusters are obtained using the methods:
1. peak grouping;
2. differential grouping.
Further, the method of ﬁnding clusters Gustafson-Kessel is used.
Placement of clusters in this example is shown in Fig. 7.6.
In the case of two clusters under the previous sample data, the results of clus-
tering are given below (see. Fig. 7.7)
Example 7.2 Suppose that the original set of points is given in Fig. 7.8.
Consider ﬁrst the case of two clusters. The corresponding results are presented in
Fig. 7.8.
For three clusters results are shown in Fig. 7.9.
For four clusters: K = 4 corresponding solution is given below (Fig. 7.10).
Fig. 7.6 Example of clustering by Gustafson-Kessel algorithm
286
7
The Cluster Analysis in Intellectual Systems

Fig. 7.7 Example of clustering by Gustafson-Kessel algorithm
Fig. 7.8 Example of clustering by Gustafson-Kessel algorithm, k = 2
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
287

Fig. 7.9 Example of clustering by Gustafson-Kessel algorithm, k = 3
Fig. 7.10 Example of clustering by Gustafson-Kessel algorithm, k = 4
Example 7.3 Classiﬁcation of the UN countries
These UN Millennium Indicators are presented in the Table 7.4.
In this experiment it was required to perform a clustering of the United Nations
countries into 4 clusters by the above indicators. As a result of the clustering
algorithm of Gustavson –Kessel application the following results were obtained
(Tables 7.5, 7.6, 7.7, 7.8, and 7.9). As can be seen from the table in the ﬁrst cluster
are countries with relatively high rates of all indicators (compared to other countries
in the sample). These are the countries of CIS, Eastern and western Europe, USA,
Canada, the Balkans and Latin America countries.
288
7
The Cluster Analysis in Intellectual Systems

Table 7.4 UN millenium indicators for countries
Population
percent
below the
poverty line
Percent of
children for
5 years with an
insufﬁcient
weight
Literacy
Gender equality.
Percent of women
among workers of the
non-agricultural
sphere
Afghanistan
70
48
50
17.8
Albania
25.4
14.3
99.4
40.3
Algeria
12.2
6
89.9
15.5
Angola
70
30.5
71.4
26.4
Argentina
15
5.4
98.6
47.6
Armenia
53.7
2.6
99.8
47
Azerbaijan
49.6
6.8
99.9
48.5
Bahrain
15
8.7
97
13.4
Bangladesh
49.8
47.7
49.7
24.2
Belize
40
6.2
84.2
44.4
Belarus
41.9
2
99.8
55.9
Butane
70
18.7
80
12
Benin
33
22.9
55.5
46
Bolivia
62.7
7.5
97.3
36.5
Bosnia Herzegovina
19.5
4.1
99.6
35.8
Botswana
70
12.5
89.1
47
Brazil
17.4
5.7
96.3
46.7
Bulgaria
12.8
2
99.7
52.2
Burundi
70
45.1
72.3
13.3
Burkina Faso
45.3
34.3
19.4
15.2
Cambodia
36.1
45.2
80.3
52.6
Cameroon
40.2
21
81.1
20.7
Verde’s cap
40
13.5
89.1
39.1
It is central the
African Republic
70
24.3
58.5
30.4
Fumes
64
28.1
37.3
5.5
Chile
17
0.7
99
37.3
China
4.6
10
98.9
39.5
Colombia
64
6.7
97.2
48.8
Congo
50
13.9
97.8
26.1
Costa Rica
22
5.1
98.4
39.5
Côte d’Ivoire
59
21.2
59.8
20.2
Croatia
20
0.6
99.6
46.3
Cuba
60
4.1
99.8
37.7
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
289

Table 7.4 (continued)
Population
percent
below the
poverty line
Percent of
children for
5 years with an
insufﬁcient
weight
Literacy
Gender equality.
Percent of women
among workers of the
non-agricultural
sphere
Czech Republic
10
1
99.8
45.8
Democratic
Republic of Congo
70
31.1
68.7
25.9
Djibouti
45.1
18.2
73.2
25
Dominican Republic
28.6
5.3
94
34.9
Ecuador
35
11.6
96.4
41.1
Egypt
16.7
8.6
73.2
21.6
El Salvador
48.3
10.3
88.9
31.1
Equatorial Guinea
50
18.6
92.7
10.5
Eritrea
53
39.6
60.9
35
Estonia
8.9
1
99.8
51.5
Ethiopia
44.2
47.2
57.4
39.9
Fiji
40
7.9
99.3
35.9
Gabon
40
11.9
59
37.7
Gambia
64
17
42.2
20.9
Georgia
11.1
3.1
99.8
45.2
Ghana
50
24.9
81.8
56.5
Guatemala
56.2
22.7
80.1
38.7
Guinea
40
23.2
50
30.3
Guinea-Bissau
48.7
25
44.1
10.8
Guyana
35
13.6
80
37.4
Haiti
45
17.3
66.2
39.5
Honduras
53
16.6
88.9
50.5
Hungary
17.3
3
99.5
47.1
India
28.6
47
64.3
17.5
Indonesia
27.1
26.1
98
30.8
Iran
30
10.9
86.3
17.2
Iraq
35
15.9
41
11.9
Jamaica
18.7
3.6
94.5
48
Jordan
11.7
4.4
99.4
24.9
Kazakhstan
34.6
4.2
99.8
48.7
Laos
38.6
40
78.5
42.1
Kenya
52
20.2
95.8
38.5
Democratic People’s
Republic of Korea
60
20.8
99.8
40.7
(continued)
290
7
The Cluster Analysis in Intellectual Systems

Table 7.4 (continued)
Population
percent
below the
poverty line
Percent of
children for
5 years with an
insufﬁcient
weight
Literacy
Gender equality.
Percent of women
among workers of the
non-agricultural
sphere
Kuwait
20
9.8
93.1
24.1
Kyrgyzstan
64.1
11
99.7
44.1
Lebanon
20
3
92.1
25.9
Lesotho
50
17.9
87.2
24.7
Liberia
60
26.4
70.8
23.6
Libya
30
4.7
97
15
Madagascar
71.3
33.1
70.1
24.2
Malawi
65.3
21.9
63.2
12.5
Malaysia
40
12.4
97.2
38
Maldives
70
30.4
99.2
36.1
Mali
63.8
33.2
24.2
35.9
Mauritania
46.3
31.8
49.6
37
Mexico
30
7.5
96.6
37.4
Mozambique
69.4
23.7
62.8
11.4
Mongolia
36.3
12.7
97.7
49.4
Morocco
19
8.9
69.5
26.2
Nepal
42
48.3
70.1
11.8
Nicaragua
47.9
9.6
86.2
41.1
Niger
63
39.6
25.6
8.6
Nigeria
34.1
28.7
88.6
34
Pakistan
32.6
38
53.9
8.7
Panama
37.3
6.8
96.1
44
Papua New Guinea
37.5
7
68.6
35.4
Paraguay
21.8
4.6
96.3
42
Peru
49
7.1
96.6
37.2
Philippines
36.8
30.6
95.1
41.1
Poland
23.8
3
99.8
47.7
Moldova
23.3
3.2
98.7
54.6
Romania
21.5
5.7
97.8
45.3
Russian Federation
30.9
3
99.8
50.1
Rwanda
51.2
27.2
76.5
14.6
Senegal
33.4
22.7
51.5
25.7
Sri Lanka
25
29.4
97
43.2
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
291

In the second cluster are countries with smaller values of indicators, it’s coun-
tries of North Africa and Middle East. In this cluster, is the lowest level of gender
equality.
In the third cluster are the poorest countries with the lowest levels of literacy, as
well as the low level of gender equality. Mainly it’s African countries.
In the fourth cluster are poor countries with the most unfavorable conditions for
the growth of children (See Table 7.6).
Example 7.4 Classiﬁcation of the United Nations countries on sustainable devel-
opment indicators.
Table 7.4 (continued)
Population
percent
below the
poverty line
Percent of
children for
5 years with an
insufﬁcient
weight
Literacy
Gender equality.
Percent of women
among workers of the
non-agricultural
sphere
Serbia and
Montenegro
30
1.9
99.8
44.9
Swaziland
40
10.3
88.1
31.3
Thailand
13.1
18.6
98
46.9
Trinidad and Tobago
21
5
99.8
41.3
Turkey
25
8.3
95.5
20.6
Turkmenistan
30
12
99.8
20
Tunisia
7.6
4
94.3
25.3
Uganda
44
22.8
80.2
35.6
Ukraine
31.7
3
99.9
53.6
United Arab Emirates
20
14.4
91.4
14.4
Tanzania
35.7
29.4
91.6
28.5
USA
5
1.4
99.1
48.8
Uzbekistan
27.5
7.9
99.7
41.5
Vietnam
50.9
33.1
94.1
51.8
Yemen
41.8
45.6
67.9
6.1
Zambia
72.9
28.1
81.2
29.4
Zimbabwe
34.9
13
97.6
21.8
Table 7.5 The centers of the clusters
28.25
5.96
97.86
43.77
29.85
12.51
82.14
23.67
59.72
26.00
65.40
24.48
40.39
34.25
74.51
31.35
292
7
The Cluster Analysis in Intellectual Systems

Table 7.6 The matrix of belonging coefﬁcients to different clusters (membership functions)
Afghanistan
0.055
0.162
0.495
0.288
Albania
0.599
0.114
0.073
0.214
Algeria
0.174
0.733
0.031
0.062
Angola
0.045
0.102
0.789
0.064
Argentina
0.939
0.024
0.016
0.021
Armenia
0.731
0.067
0.116
0.087
Azerbaijan
0.757
0.058
0.103
0.082
Bahrain
0.204
0.687
0.030
0.080
Bangladesh
0.031
0.071
0.187
0.712
Belize
0.251
0.252
0.266
0.231
Belarus
0.761
0.056
0.087
0.096
Butane
0.241
0.156
0.479
0.124
Benin
0.057
0.282
0.371
0.290
Bolivia
0.645
0.087
0.191
0.077
Bosnia Herzegovina
0.764
0.157
0.029
0.050
Botswana
0.530
0.130
0.215
0.124
Brazil
0.920
0.033
0.020
0.027
Bulgaria
0.914
0.033
0.025
0.028
Burundi
0.178
0.171
0.445
0.206
Burkina Faso
0.020
0.249
0.495
0.236
Cambodia
0.120
0.050
0.163
0.667
Cameroon
0.115
0.522
0.095
0.268
Verde’s cap
0.428
0.197
0.160
0.215
It is central the African Republic
0.048
0.287
0.471
0.194
Fumes
0.021
0.117
0.681
0.180
Chile
0.758
0.158
0.034
0.049
China
0.800
0.091
0.040
0.070
Colombia
0.703
0.075
0.136
0.086
Congo
0.435
0.275
0.157
0.133
Costa Rica
0.930
0.039
0.012
0.019
Côte d’Ivoire
0.013
0.091
0.819
0.077
Croatia
0.928
0.032
0.018
0.022
Cuba
0.631
0.092
0.183
0.093
Czech Republic
0.919
0.040
0.018
0.022
Democratic Republic of Congo
0.038
0.108
0.784
0.069
Djibouti
0.036
0.604
0.203
0.158
Dominican respubl_ka
0.642
0.234
0.049
0.074
Ecuador
0.839
0.053
0.038
0.070
Egypt
0.065
0.791
0.068
0.076
El Salvador
0.389
0.243
0.219
0.148
Ecvatorial Guinea
0.347
0.331
0.140
0.182
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
293

Table 7.6 (continued)
Eritrea
0.035
0.076
0.246
0.642
Estonia
0.909
0.037
0.026
0.028
Ethiopia
0.048
0.060
0.174
0.718
Fiji
0.716
0.130
0.067
0.087
Gabon
0.038
0.428
0.351
0.183
Gambia
0.035
0.200
0.509
0.256
Georgia
0.934
0.032
0.015
0.020
Ghana
0.180
0.107
0.264
0.448
Guatemala
0.100
0.143
0.577
0.181
Guinea
0.020
0.401
0.397
0.182
Guinea-Bissau
0.021
0.193
0.572
0.214
Guyana
0.126
0.435
0.206
0.233
Haiti
0.040
0.353
0.366
0.241
Honduras
0.368
0.122
0.275
0.235
Hungary
0.961
0.017
0.010
0.013
India
0.087
0.070
0.098
0.745
Indonesia
0.220
0.099
0.077
0.604
Iran
0.106
0.785
0.035
0.074
Iraq
0.034
0.306
0.464
0.196
Jamaica
0.815
0.077
0.051
0.057
Jordan
0.322
0.588
0.029
0.061
Kazakhstan
0.920
0.024
0.025
0.031
Laos
0.057
0.029
0.086
0.828
Kenya
0.375
0.131
0.318
0.177
Democratic People’s Republic of Korea
0.293
0.128
0.451
0.127
Kuwait
0.232
0.689
0.023
0.055
Kyrgyzstan
0.609
0.093
0.213
0.085
Lebanon
0.240
0.656
0.040
0.063
Lesotho
0.276
0.360
0.204
0.160
Liberia
0.013
0.056
0.893
0.038
Libya
0.302
0.489
0.069
0.141
Madagascar
0.051
0.111
0.764
0.074
Malawi
0.044
0.120
0.715
0.120
Malaysia
0.725
0.100
0.069
0.106
Maldives
0.203
0.133
0.557
0.108
Mali
0.027
0.438
0.231
0.304
Mauritania
0.028
0.206
0.350
0.415
Mexico
0.900
0.054
0.017
0.029
Mozambique
0.050
0.117
0.721
0.112
(continued)
294
7
The Cluster Analysis in Intellectual Systems

Table 7.6 (continued)
Mongolia
0.648
0.068
0.107
0.177
Morocco
0.055
0.757
0.101
0.086
Nepal
0.108
0.079
0.110
0.703
Nicaragua
0.299
0.206
0.290
0.204
Niger
0.020
0.170
0.533
0.276
Nigeria
0.096
0.040
0.052
0.813
Pakistan
0.065
0.172
0.163
0.600
Panama
0.930
0.023
0.021
0.025
Papua New Guinea
0.066
0.411
0.334
0.189
Paraguay
0.930
0.036
0.014
0.020
Peru
0.748
0.081
0.100
0.070
Philippines
0.119
0.045
0.096
0.740
Poland
0.966
0.013
0.009
0.012
Moldova
0.873
0.038
0.041
0.048
Romania
0.979
0.009
0.005
0.007
Russian Federation
0.924
0.023
0.024
0.029
Rwanda
0.138
0.315
0.240
0.308
Senegal
0.027
0.466
0.322
0.185
Sri Lanka
0.186
0.058
0.099
0.657
Serbia and Montenegro
0.921
0.031
0.021
0.027
Swaziland
0.293
0.433
0.125
0.150
Thailand
0.550
0.094
0.106
0.250
Trinidad and Tobago
0.931
0.035
0.013
0.021
Turkey
0.133
0.806
0.018
0.043
Turkmenistan
0.197
0.656
0.037
0.110
Tunisia
0.369
0.537
0.035
0.059
Uganda
0.089
0.145
0.241
0.525
Ukrain
0.867
0.036
0.044
0.053
United Arab Emirates
0.255
0.584
0.039
0.122
Tanzania
0.151
0.072
0.070
0.706
USA
0.905
0.043
0.025
0.027
Uzbekistan
0.889
0.046
0.023
0.042
Vietnam
0.190
0.077
0.308
0.426
Yemen
0.117
0.106
0.118
0.659
Zambia
0.147
0.125
0.659
0.069
Zimbabwe
0.249
0.585
0.046
0.120
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
295

Table 7.7 Indicators of the sustainable development
Albania
0.58387
0.54820
0.57383
0.57353
Algeria
0.56706
0.48474
0.26571
0.35047
Argentina
0.35296
0.63011
0.76377
0.49466
Armenia
0.65586
0.51200
0.33448
0.47481
Australia
0.56994
0.81968
0.78295
0.86663
Austria
0.68752
0.72971
0.80098
0.8154
Azerbaijan
0.60008
0.53918
0.35928
0.40572
Bangladesh
0.65282
0.21060
0.20669
0.17571
Belgium
0.614967
0.748405
0.817869
0.676777
Benin
0.498461
0.178393
0.334484
0.208119
Bolivia.
0.172206
0.433633
0.492253
0.298884
Bosnia and Herzegovina
0.548856
0.534668
0.492253
0.354646
Botswana
0.13233
0.418817
0.464936
0.347845
Brazil
0.216163
0.518054
0.626328
0.60656
Bulgaria
0.361631
0.583907
0.699515
0.518325
Cambodia
0.380226
0.23689
0.157964
0.204641
Cameroon
0.376477
0.201684
0.143939
0.2182
Canada
0.62295
0.77159
0.72201
0.84180
Chile
0.243463
0.64134
0.546863
0.731782
China
0.437748
0.463597
0.384858
0.332749
Colombia
0.157988
0.502915
0.546863
0.618515
Costa Rica
0.271513
0.557193
0.600368
0.724376
Croatia
0.601288
0.61875
0.676009
0.603607
Cyprus
0.962098
0.678039
0.600368
0.681226
Czech Republic
0.744175
0.717602
0.722017
0.702571
Denmark
0.761651
0.747263
0.743451
0.816827
Dominican Republic
0.303631
0.463597
0.464936
0.412767
Ecuador
0.294138
0.512001
0.600368
0.403071
Egypt
0.631237
0.399777
0.265716
0.419674
El Salvador
0.331064
0.457576
0.437827
0.504924
Estonia
0.553215
0.680678
0.699515
0.689583
Ethiopia
0.676386
0.101967
0.143939
0.201635
Finland
0.72688
0.752941
0.699515
0.85815
France
0.619443
0.754066
0.817869
0.794635
Gambia
0.324594
0.141861
0.265716
0.235642
Georgia
0.441675
0.516541
0.411085
0.47656
Germany
0.702424
0.768379
0.800988
0.806349
(continued)
296
7
The Cluster Analysis in Intellectual Systems

Table 7.7 (continued)
Greece
0.588835
0.73447
0.743451
0.517882
Ґватемала
0.219163
0.316496
0.437827
0.325601
Honduras
0.167474
0.376757
0.359283
0.269843
Hungary
0.64978
0.67139
0.763774
0.663185
Iceland
0.962098
0.75068
0.743451
0.87829
India
0.536571
0.265352
0.225227
0.253416
Indonesia
0.537406
0.371083
0.359283
0.268663
Ireland
0.588616
0.778987
0.743451
0.752281
Israel
0.486303
0.754066
0.651581
0.644542
Italy
0.552527
0.733286
0.782957
0.668513
Jamaica
0.357959
0.5014
0.546863
0.405747
Japan
0.759359
0.767299
0.800988
0.793311
Yordaniya
0.517332
0.490799
0.359283
0.445905
Kazakhstan
0.655486
0.540692
0.265716
0.434827
Kenya
0.31728
0.211616
0.28762
0.234049
Korea, Republic
0.641912
0.759641
0.743451
0.610841
Kyrgyzstan
0.605705
0.36826
0.310566
0.317857
Latvia
0.558807
0.621604
0.699515
0.655443
Lithuania
0.520474
0.64134
0.676009
0.6509
Luxembourg
0.962098
0.73091
0.546863
0.802207
Madagascar
0.32533
0.178393
0.206696
0.19195
Malawi
0.490073
0.138213
0.28762
0.23739
Malaysia
0.344576
0.585379
0.437827
0.527639
Mexico
0.248442
0.594174
0.651581
0.552042
Moldova, Republic
0.510835
0.404146
0.437827
0.336335
Mongolia
0.542408
0.402688
0.334484
0.305619
Morocco
0.451213
0.325741
0.464936
0.427675
Mozambique
0.356033
0.080015
0.206696
0.241402
Namibia
0.047323
0.379606
0.464936
0.449997
Nepal
0.324226
0.172261
0.28762
0.3035
Niderlandi
0.655107
0.773727
0.782957
0.775775
Zealand is new
0.549644
0.79125
0.833616
0.854503
Nicaragua
0.239314
0.323085
0.334484
0.315336
Niger
0.40908
0.167985
0.157964
0.181214
Norway
0.744654
0.820582
0.763774
0.851428
Pakistan
0.619443
0.232537
0.130966
0.172761
Panama
0.239162
0.601457
0.573832
0.600787
Paraguay
0.245166
0.429174
0.546863
0.583011
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
297

Table 7.7 (continued)
Peru
0.312213
0.554201
0.310566
0.522702
Filippini
0.386772
0.426208
0.310566
0.340265
Poland
0.589972
0.657887
0.722017
0.625403
Portugal
0.501937
0.657887
0.782957
0.667165
Rumuniya
0.650353
0.61875
0.600368
0.547301
Russian Federation
0.42253
0.548206
0.310566
0.515363
Senegal
0.486509
0.158067
0.28762
0.195048
Slovakia
0.744335
0.688527
0.651581
0.687639
Slovenia
0.650353
0.70137
0.763774
0.673112
South Africa
0.166191
0.366851
0.464936
0.29966
Spain
0.580784
0.743814
0.743451
0.724533
Sri Lanka
0.464124
0.456073
0.359283
0.450402
Sweden
0.757052
0.768379
0.699515
0.874694
Switzerland
0.600644
0.756307
0.722017
0.851068
Tajikistan
0.602294
0.343272
0.189316
0.253371
Tanzania, United Republic
0.520265
0.147865
0.225227
0.255087
Thailand
0.220891
0.450068
0.411085
0.400544
Trinidad і Tobago
0.463915
0.573568
0.464936
0.385968
Tunisia
0.452668
0.493827
0.464936
0.492711
Turkey
0.47499
0.487771
0.492253
0.455756
Uganda
0.381611
0.16714
0.244906
0.22529
Ukraine
0.716261
0.534668
0.464936
0.361609
United Arab Emirates
0.962098
0.684616
0.225227
0.51641
Great Britain
0.55376
0.72732
0.800988
0.780447
USA
0.452668
0.786202
0.937406
0.805513
Uruguay
0.419463
0.615887
0.743451
0.687183
Uzbekistan
0.53824
0.395423
0.225227
0.17794
Venezuela. Bolivar Republic
0.397572
0.513515
0.265716
0.30664
Vjetnam
0.520474
0.332429
0.265716
0.317299
Zambia
0.264436
0.145589
0.310566
0.254877
Zimbabwe
0.264436
0.035073
0.10799
0.1072
298
7
The Cluster Analysis in Intellectual Systems

Table 7.8 Degrees of membership to clusters
3
Albania
0.40165
0.10648
0.49186
3
Algeria
0.08839
0.43869
0.47292
3
Argentina
0.38845
0.11707
0.49448
3
Armenia
0.17773
0.27465
0.54762
1
Australia
0.90018
0.02894
0.07086
1
Austria
0.95403
0.01378
0.03218
3
Azerbaijan
0.13099
0.24733
0.62168
2
Bangladesh
0.04774
0.8121
0.14015
1
Belgium
0.95258
0.01291
0.0345
2
Benin
0.017346
0.91288
0.069771
3
Bolivia.
0.086566
0.27213
0.6413
3
Bosnia and Herzegovina
0.1149
0.16889
0.71621
3
Botswana
0.090011
0.27373
0.63626
3
Brazil
0.21422
0.13172
0.65406
3
Bulgaria
0.29843
0.10605
0.59552
2
Cambodia
0.015168
0.92082
0.064017
2
Cameroon
0.017754
0.91106
0.071185
1
Canada
0.94729
0.01499
0.03771
3
Chile
0.34211
0.1277
0.53019
3
China
0.04406
0.22574
0.7302
3
Colombia
0.17747
0.15295
0.66959
3
Costa Rica
0.33409
0.12403
0.54188
1
Croatia
0.75759
0.052825
0.18958
1
Cyprus
0.68146
0.11573
0.2028
1
Czech Republic
0.94321
0.016686
0.0401
1
Denmark
0.92516
0.023355
0.051486
3
Dominican Republic
0.023839
0.05861
0.91755
3
Ecuador
0.08807
0.10437
0.80756
2
Egypt
0.10324
0.48662
0.41014
3
El Salvador
0.021848
0.041038
0.93711
1
Estonia
0.91918
0.018352
0.062473
2
Ethiopia
0.063802
0.77286
0.16334
1
Finland
0.91557
0.025895
0.058533
1
France
0.95921
0.011702
0.029093
2
Gambia
0.023284
0.87702
0.099697
3
Georgia
0.023663
0.038418
0.93792
1
Germany
0.94903
0.015455
0.03551
1
Greece
0.74098
0.062566
0.19646
3
Ґватемала
0.067026
0.40502
0.52795
3
Honduras
0.071116
0.45306
0.47582
1
Hungary
0.95232
0.012626
0.03505
1
Iceland
0.76693
0.084196
0.14888
2
India
0.012701
0.93374
0.053564
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
299

Table 7.8 (continued)
2
Indonesia
0.050291
0.64789
0.30182
1
Ireland
0.97523
0.006589
0.018182
1
Israel
0.75846
0.049725
0.19181
1
Italy
0.93061
0.01727
0.052117
3
Jamaica
0.037423
0.05336
0.90922
1
Japan
0.92541
0.023439
0.051152
3
Yordaniya
0.070166
0.17151
0.75832
3
Kazakhstan
0.16399
0.33124
0.50477
2
Kenya
0.020158
0.8778
0.10204
1
Korea, Republic
0.91524
0.02221
0.062547
2
Kyrgyzstan
0.066312
0.62744
0.30625
1
Latvia
0.82602
0.037702
0.13627
1
Lithuania
0.76748
0.046179
0.18634
1
Luxembourg
0.6903
0.11288
0.19682
2
Madagascar
0.021309
0.89052
0.088166
2
Malawi
0.014653
0.92942
0.055931
3
Malaysia
0.058977
0.05684
0.88418
3
Mexico
0.23705
0.117
0.64595
3
Moldova, Republic
0.064556
0.31673
0.61872
2
Mongolia
0.057925
0.56437
0.3777
3
Morocco
0.062452
0.26329
0.67426
2
Mozambique
0.029408
0.86369
0.1069
3
Namibia
0.1218
0.28352
0.59468
2
Nepal
0.025167
0.84944
0.12539
1
Niderlandi
0.97497
0.007208
0.017824
1
Zealand is new
0.8935
0.030837
0.075666
2
Nicaragua
0.051662
0.54775
0.40058
2
Niger
0.017697
0.91677
0.065531
1
Norway
0.90361
0.030465
0.065925
2
Pakistan
0.044592
0.82182
0.13359
3
Panama
0.21163
0.11324
0.67513
3
Paraguay
0.12109
0.13045
0.74846
3
Peru
0.0764
0.13887
0.78473
3
Filippini
0.043558
0.41156
0.54488
1
Poland
0.8745
0.028966
0.096534
1
Portugal
0.83716
0.037676
0.12517
1
Rumuniya
0.57831
0.094633
0.32706
3
Russian Federation
0.076257
0.13511
0.78863
2
Senegal
0.013131
0.93672
0.050145
1
Slovakia
0.89028
0.031444
0.078275
1
Slovenia
0.97281
0.007269
0.019919
3
South Africa
0.08185
0.34815
0.57
1
Spain
0.98221
0.004552
0.013235
(continued)
300
7
The Cluster Analysis in Intellectual Systems

Table 7.8 (continued)
3
Sri Lanka
0.043578
0.13993
0.8165
1
Sweden
0.89026
0.034693
0.075048
1
Switzerland
0.93911
0.017102
0.043793
2
Tajikistan
0.045867
0.78327
0.17087
2
Tanzania, United Republic
0.015506
0.92838
0.056119
3
Thailand
0.055589
0.17785
0.76656
3
Trinidad і Tobago
0.059986
0.083263
0.85675
3
Tunisia
0.026507
0.033837
0.93966
3
Turkey
0.039097
0.051998
0.90891
2
Uganda
0.01028
0.94557
0.044149
3
Ukraine
0.23303
0.27103
0.49594
1
United Arab Emirates
0.36305
0.28152
0.35543
1
Great Britain
0.94643
0.014397
0.039177
1
USA
0.79335
0.060098
0.14655
1
Uruguay
0.666
0.069556
0.26445
2
Uzbekistan
0.043994
0.77022
0.18578
3
Venezuela. Bolivar Republic
0.06372
0.38883
0.54745
2
Vjetnam
0.028441
0.81113
0.16043
2
Zambia
0.03963
0.78231
0.17806
2
Zimbabwe
0.061394
0.75431
0.18429
Number of a cluster
1
2
3
GINI
Ihd
Iql
Isd
0.63499
0.724
0.73749
0.73457
0.44656
0.236
0.25218
0.24782
0.38459
0.493
0.45717
0.45412
Criterion
9.8738
Hi-Beni
0.438
Table 7.9 Degrees of membership of the countries to clusters
4
Albania
0.28424
0.3116
0.072582
0.33157
4
Algeria
0.027483
0.071321
0.10306
0.79814
2
Argentina
0.22755
0.53851
0.067084
0.16686
4
Armenia
0.070525
0.11933
0.091308
0.71884
1
Australia
0.86029
0.070031
0.022093
0.047588
1
Austria
0.94636
0.0251
0.008865
0.019672
4
Azerbaijan
0.033017
0.073697
0.051201
0.84209
3
Bangladesh
0.042206
0.084422
0.63857
0.2348
1
Belgium
0.92169
0.039019
0.011551
0.027739
3
Benin
0.017031
0.046297
0.82568
0.111
2
Bolivia
0.061197
0.50498
0.18867
0.24515
4
Bosnia and Herzegovina
0.057183
0.1661
0.071758
0.70496
2
Botswana
0.060793
0.52973
0.18418
0.22529
2
Brazil
0.068824
0.80042
0.041411
0.089343
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
301

2
Bulgaria
0.14689
0.65894
0.051013
0.14315
3
Cambodia
0.009822
0.028878
0.90035
0.060951
3
Cameroon
0.009908
0.028354
0.90497
0.056772
1
Canada
0.92961
0.034214
0.010931
0.025244
2
Chile
0.18624
0.58983
0.067466
0.15646
4
China
0.019842
0.10456
0.078945
0.79666
2
Colombia
0.066882
0.7671
0.056491
0.10953
2
Costa Rica
0.16698
0.6328
0.060572
0.13965
1
Croatia
0.61489
0.18965
0.045698
0.14976
1
Cyprus
0.57037
0.14719
0.085695
0.19675
1
Czech Republic
0.91329
0.037491
0.013955
0.03526
1
Denmark
0.90361
0.042127
0.016682
0.037581
2
Dominican Republic
0.032441
0.67082
0.072181
0.22456
2
Ecuador
0.040697
0.79243
0.045605
0.12127
4
Egypt
0.04537
0.097315
0.16448
0.69284
2
El Salvador
0.03225
0.70707
0.054953
0.20573
1
Estonia
0.8311
0.091463
0.020293
0.057147
3
Ethiopia
0.050831
0.094557
0.63814
0.21647
1
Finland
0.88749
0.050062
0.019109
0.043343
1
France
0.94945
0.024991
0.007902
0.017653
3
Gambia
0.011694
0.037745
0.88941
0.061153
4
Georgia
0.038094
0.26718
0.053444
0.64128
1
Germany
0.94112
0.027184
0.009862
0.021834
1
Greece
0.60917
0.19229
0.051299
0.14724
2
Guatemala
0.052387
0.36066
0.31644
0.27051
3
Honduras
0.054833
0.31568
0.35575
0.27374
1
Hungary
0.90577
0.04526
0.013551
0.035419
1
Iceland
0.68922
0.12112
0.063855
0.12581
3
India
0.019765
0.051379
0.75142
0.17743
4
Indonesia
0.030765
0.093396
0.26095
0.61489
1
Ireland
0.96099
0.019763
0.005589
0.013659
1
Israel
0.61194
0.22343
0.041591
0.12304
1
Italy
0.86916
0.06992
0.017282
0.043638
2
Jamaica
0.038649
0.71492
0.050335
0.19609
1
Japan
0.90616
0.041501
0.016386
0.035957
4
Jordan
0.009514
0.034653
0.018891
0.93694
4
Kazakhstan
0.069722
0.12096
0.11663
0.69268
3
Kenya
0.013179
0.047963
0.85807
0.080792
1
Korea. Republic
0.85087
0.070721
0.021213
0.057195
4
Kyrgyzstan
0.03518
0.085876
0.22919
0.64975
1
Latvia
0.69365
0.16621
0.035339
0.1048
1
Lithuania
0.61283
0.2234
0.040208
0.12357
1
Luxembourg
0.58655
0.1471
0.084004
0.18234
(continued)
302
7
The Cluster Analysis in Intellectual Systems

3
Madagascar
0.010267
0.031539
0.90351
0.054684
3
Malawi
0.011209
0.029699
0.89214
0.066953
2
Malaysia
0.049112
0.71574
0.043663
0.19148
2
Mexico
0.079655
0.78924
0.03832
0.092786
4
Moldova, Republic
0.024628
0.095535
0.092677
0.78716
4
Mongolia
0.022514
0.068667
0.14794
0.76088
4
Morocco
0.048106
0.2518
0.17008
0.53001
3
Mozambique
0.014971
0.041838
0.87376
0.069428
2
Namibia
0.075055
0.54377
0.17888
0.20229
3
Nepal
0.017185
0.062621
0.81948
0.10071
1
Netherlands
0.9755
0.011725
0.003857
0.008913
1
New Zealand
0.84821
0.07761
0.023867
0.050316
3
Nicaragua
0.042242
0.23973
0.4443
0.27372
3
Niger
0.008523
0.022791
0.92278
0.045911
1
Norway
0.87596
0.055127
0.021828
0.047085
3
Pakistan
0.039064
0.080179
0.66428
0.21648
2
Panama
0.067097
0.81058
0.034832
0.087487
2
Paraguay
0.039361
0.83131
0.041076
0.088257
2
Peru
0.065334
0.46554
0.10782
0.3613
4
Philippine
0.029716
0.15589
0.21217
0.60223
1
Poland
0.76646
0.12003
0.028969
0.084542
1
Portugal
0.71572
0.16448
0.033888
0.085911
1
Romania
0.42991
0.22585
0.068687
0.27555
4
Russian Federation
0.058523
0.27694
0.090317
0.57422
3
Senegal
0.00987
0.026109
0.90456
0.059459
1
Slovakia
0.82228
0.073597
0.027661
0.076458
1
Slovenia
0.94376
0.027013
0.008162
0.02106
2
South Africa
0.060062
0.42837
0.25723
0.25433
1
Spain
0.96064
0.020311
0.005394
0.013656
4
Sri Lanka
0.015491
0.078825
0.040186
0.8655
1
Sweden
0.85291
0.064036
0.025878
0.057175
1
Switzerland
0.91403
0.042608
0.013135
0.03023
3
Tajikistan
0.040625
0.091683
0.48922
0.37848
3
Tanzania
0.013224
0.032817
0.87388
0.080083
2
Thailand
0.045151
0.57189
0.13572
0.24724
4
Trinidad and Tobago
0.052559
0.26387
0.06346
0.62011
4
Tunisia
0.049968
0.38186
0.056285
0.51189
4
Turkey
0.050238
0.3103
0.058481
0.58098
3
Uganda
0.002341
0.00716
0.97664
0.013863
4
Ukraine
0.11937
0.15783
0.11911
0.60369
4
United Arab Emirates
0.25172
0.18882
0.17458
0.38488
1
Great Britain
0.91502
0.044743
0.012211
0.028022
1
USA
0.69788
0.16177
0.047239
0.093105
(continued)
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
303

Investigations of fuzzy clustering method C-means by indicators of sustainable
development for the countries of the United Nations were carried out. For this, the
data of the World Data Center in Ukraine (WDC) were used (See Table 7.7).
As sustainable development indicators the following indices were taken:
• Index GINI-GINI
• Ihd—index of health status
• Iql—standard of living index
• Isd—index of sustainable development.
As algorithm of initial centers placement the algorithm of differential grouping
was applied. Clustering was carried out for a different number of clusters K = 3, 4,
5. Besides the value of optimized criterion the quality of splitting will be evaluated
by the indicator of Hi-Beni:
v ¼ dav
Dav
;
where dav is the average intra-cluster distance, Dav—average inter-cluster distance.
This indicator should be minimized.
Experiment 1. K = 3
Let us analyze the results (See Table 7.8). The ﬁrst cluster contains countries with
the highest values of all parameters. These are the countries of Western Europe, as
well as some other. Namely, Australia, Austria, Belgium, Great Britain, Hungary,
Denmark, Iceland, Ireland, Israel, Italy, Cyprus, Latvia, Lithuania, Luxembourg,
Netherlands, New Zealand, Norway, Poland, Portugal. USA, Slovakia, Slovenia,
Croatia, Czech Republic, Sweden, Switzerland, Uruguay.
The second cluster contains countries with an average value of the index GINI,
and minimum values of all other indicators. These are the countries of Africa and
1
Uruguay
0.48376
0.33651
0.051159
0.12857
3
Uzbekistan
0.038792
0.098392
0.4853
0.37751
4
Venezuela
0.039499
0.16547
0.19342
0.6016
4
Vietnam
0.027619
0.082803
0.4324
0.45718
3
Zambia
0.026225
0.093996
0.75376
0.12602
3
Zimbabwe
0.040152
0.099806
0.71609
0.14395
Number of a cluster
1
2
3
4
Centers of clusters
0.64429
0.73332
0.74677
0.74678
0.29515
0.51657
0.53844
0.51317
0.419
0.19409
0.23834
0.22998
0.51726
0.46924
0.3707
0.39482
Criterion 1
9.4268
Hi-Beni
0.39492
304
7
The Cluster Analysis in Intellectual Systems

South-East Asia. These include: Bangladesh, Egypt, Zambia, Zimbabwe, India,
Indonesia, Cambodia, Cameroon, Kyrgyzstan, Nicaragua, Niger, Pakistan, Uganda,
Senegal, Tajikistan, Tanzania, and others.
The third cluster contains countries with average values of all the indicators and
the small value of the index GINI. It includes the CIS countries, Latin America and
some of the most developed countries of Asia and Africa. Namely, Armenia,
Albania, Algeria, Argentina, Brazil, Bolivia, Bulgaria, Bosnia and Herzegovina,
Venezuela, Honduras, Guatemala, Georgia, Jordan, Kazakhstan, China, Costa Rica,
Colombia, Mexico, Moldova, Peru, Paraguay, Russian Federation, Trinidad and
Tobago, Tunisia, Turkey, Ukraine, Chile, South Africa, Jamaica.
Experiment 2. K = 4
It is interesting to analyse dynamics of changes of clusters after transition from
K = 3 to K = 4 (See Table 7.9).
The countries with the greatest values of all indicators fall to the ﬁrst cluster. The
structure of this cluster practically didn’t change. In the second cluster there are
countries with the minimum value of an index GINI and average values of all other
indicators. Here are the countries of Latin America: Argentina, Brazil, Panama,
Paraguay, Peru, Uruguay, etc.
The countries with the minimum values of all indicators except GINI index fall
to the third cluster. Here the countries from the second cluster of the previous
clustering at K = 3 fall. Namely, Bangladesh, Benin, Zambia, Zimbabwe, India,
Cambodia,
Cameroon,
Kenya,
Mozambique,
Nepal,
Pakistan,
Senegal,
Tadzhikistan, Tanzania, Uzbekistan.
The countries with average values of all indicators fall to the fourth cluster. Here
the countries from the third cluster of the previous clustering fall, namely:
Venezuela, Vietnam, Ukraine, the Russian Federation, Azerbaijan, Georgia,
Indonesia, Jordan, Kyrgyzstan, Sri-Lanka. Value of an indicator of Chi- Beni
decreased from 0.438 to 0.39492.
Experiment 3. К = 5
For this experiment we present only the average data for cluster centers (see below)
Centers of clusters:
0.52645
0.45648
0.35255
0.37995
0.56042
0.67078
0.70818
0.64717
0.41643
0.18501
0.23361
0.22605
0.68519
0.75586
0.75276
0.80007
0.27997
0.47653
0.48824
0.45777
Criterion 1
9.0011
Hi Beni
0.38816
7.8
Application of Fuzzy Methods C-Means and Gustavson-Kessel’s …
305

Consider the dependence of the index Hi-Beni on the number of clusters K
(Fig. 7.11).
As the chart above shows, the value of Hi-Beni index signiﬁcantly decreases
when K = 2 −4, then its value is changing slightly. Therefore, the optimal number
of clusters lies in vicinity of K = 4.
7.9
Conclusions
Cluster analysis includes a set of different classiﬁcation algorithms. In general,
whenever it is necessary to classify the “mountains” of information to suitable for
further processing groups, cluster analysis is very useful and effective. Cluster
analysis is needed for the classiﬁcation of information, it can be used in a certain
way to structure the variables and to ﬁnd out which variables should be combined in
the ﬁrst place, and which should be considered separately.
A great advantage of the cluster analysis is that it allows to split the objects not
only by one parameter but by a set of attributes as well. In addition, cluster analysis
unlike most mathematical and statistical methods do not impose any restrictions on
the form of these objects, and allows to treat a variety of raw data of almost
arbitrary nature. This is important, for example, in the situation when indicators are
diverse views, and it’s impossible to use traditional econometric approaches.
As any other method, cluster analysis has certain disadvantages and limitations:
in particular, the content and the number of clusters depend on the criteria selected
for partition. For the reduction of the original data set to a more compact form there
may be some distortion, and characteristics of individual objects may be lost by
replacing them with the characteristics of parameters of the cluster center.
The main disadvantage of the considered methods of fuzzy clustering C-means
and Gustavson-Kessel is that they can only be used when the number of clusters K
is known. But usually, the number of clusters is unknown, and visual observations
in the multidimensional case simply don’t lead to a success.
Fig. 7.11 Hi-Beni index versus number of clusters
306
7
The Cluster Analysis in Intellectual Systems

References
1. Durant, B., Smith, G.: Cluster analysis.- M.Statistica 289 p. (rus)
2. Zaychenko, Y.P.: Fundamentals of Intellectual Systems Design, 352 p. Kiev Publ. house,
Slovo (2004). (in Russ.)
3. Zaychenko, Y.P.: Fuzzy models and methods in intellectual systems, 354 p. Publ. House
“Slovo”, Kiev (2008). Zaychenko, Y.: Fuzzy Group Method of Data Handling under fuzzy
input data. In: System research and information technologies, №3, pp. 100–112 (2007). (in
Russ.)
4. Osovsky, S.: Neural networks for information processing, transl. from pol. M Publ. house
Finance and Statistics, 344 p. (2002). (in Russ.)
5. Yager, R.R., Filev, D.P.: Approximate clustering via the mountain method. IEEE Trans. Syst.
Man Cybern. 24, 1279–1284 (1994)
6. Krishnapuram, R., Keller, J.: A possibilistic approach to clustering. IEEE Trans. Fuzzy Syst. 1,
98–110 (1993)
7. Krishnapuram, R., Keller, J.: Fuzzy and possibilistic clustering methods for computer vision.
IEEE Trans. Fuzzy Syst. 1, 98–110 (1993)
8. Chung, F.L., Lee, T.: Fuzzy competitive learning. Neural Netw. 7, 539–552 (1994)
9. Park, D.C., Dagher, I.: Gradient based fuzzy C-means (GBFCM) algorithm. In: Proceedings of
IEEE Inernational Conference on Neural Networks, pp. 1626−1631 (1984)
10. Bodyanskiy, Y., Gorshkov Y., Kokshenev, I., Kolodyazhniy, V.: Robust recursive fuzzy
clustering algorithms. In: Proceedings of East West Fuzzy Colloquim 2005, pp. 301–308, HS,
Zittau/Goerlitz (2005)
11. Bodyanskiy, Y., Gorshkov, Y., Kokshenev, I., Kolodyazhniy, V.: Outlier resistant recursive
fuzzy clustering algorithm. In: Reusch, B. (ed.) Computational Intelligence: Theory and
Applications. Advances in Soft Computing, vol. 38, pp. 647–652. Springer, Berlin (2006)
12. Bodyanskiy, Y.: Computational intelligence techniques for data analysis. In: Lecture Notes in
Informatics, vol. P-72, pp. 15–36. GI, Bonn (2005)
13. Vasiliyev, V.I.: Pattern-recognition systems. Naukova Dumka, Kiev (1988). (in Russ)
14. Bodyanskiy, Y., Gorshkov, Y., Kokshenev, I., Kolodyazhniy, V., Shilo, O.: Robust recursive
fuzzy clustering-based segmentation of biomedical time series. In: Proceedings of 2006
International Symposium on Evolving Fuzzy Systems, pp. 101–105, Lancaster, UK (2006)
References
307

Chapter 8
Genetic Algorithms and Evolutionary
Programing
8.1
Introduction
The chapter is devoted to widely used genetic algorithms (GA) and deeply con-
nected with them evolutionary programing. In Sect. 8.2 genetic algorithms are
considered and their properties analyzed. The different variants of main GA oper-
ators: cross-over are considered, various presentations of individuals- binary and
ﬂoating—point are considered and different cross-over operators for them are
described.
In the Sect. 8.3 various implementations of mutation operator are presented and
their properties are discussed. Parameters of GA are considered, deterministic,
adaptive and self-adaptive parameters modiﬁcations are described and discussed. In
the Sect. 8.5 different selection strategies are considered and analyzed. In the
Sect. 8.6 the application of GA for solution of practical problem of computer
network structural synthesis is described. The Sect. 8.7 is devoted to detail
description and analysis of evolutionary programing (EP). Main operators of EP:
mutations and selection are considered and their properties analyzed. Different
variants of algorithms EP are described. In the Sect. 8.8 differential evolution is
considered, its properties discussed and the algorithm of DE is presented.
8.2
Genetic Algorithms
Genetic algorithms (GAs) are the ﬁrst algorithmic models developed to simulate
genetic systems. First proposed by Fraser [1], it was the extensive work done by
Holland [2] that popularized GAs. It is also due to his work that Holland was
generally recognized as the father of genetic alorithms.
GAs model genetic evolution, where the characteristics of individuals are
expressed using genotypes. The main driving operators of GA are selection (to
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_8
309

model survival of the ﬁttest) and recombination through application of a crossover
operator (to model reproduction).
Consider the general description of genetic algorithm.
Preliminary stage. Create and initialize the initial population Cð0Þ.
Choose the stop condition, assign populations counter t ¼ 0.
Iteration t
1. Estimate ﬁtness index f XiðtÞ
ð
Þ of every individual XiðtÞ.
2. Execute reproduction to create offspring.
3. Choose new population Cðt þ 1Þ.
4. Check the stop condition, if it fulﬁlls then stop otherwise t ¼ t þ 1 and go to the
next iteration.
Canonical Genetic Algorithm
The canonical GA (CGA) proposed by Holland [2] follows the general algorithm
descripted above, with the following implementation speciﬁcity:
• A bit string representation of individuals was used.
• Proportional selection was used to select parents for recombination.
• One-point crossover (see Sect. 8.2) was used as the primary method to
• produce offspring.
• Uniform mutation (refer to Sect. 8.3) was proposed as a background operator of
small importance.
It is worth to note that mutation was not considered as an important operator in
the original GA implementations. Only in later implementations the explorative
power of mutation was used to improve the search capabilities of GAs.
Since CGA, several variations of the GA have been developed that differ in
representation scheme, selection operator, crossover operator and mutation operator.
Consider the main approaches to implementation of GA illustrating all its diversity.
Crossover
Crossover operators can be divided into three main categories based on the arity
(i.e. the number of parents used) of the operator. This results in three main classes
of crossover operators []:
• asexual, where an offspring is generated from one parent.
• sexual, where two parents are used to produce one or two offspring.
• multi-recombination, where more than two parents are used to produce one or
more offspring.
Each pair (or group) of parents have a probability, pc of producing offspring.
Usually, a high crossover probability is used.
In selection of parents, the following questions need to be considered:
• Due to probabilistic selection of parents, it may happen that the same individuals
will be selected as both parents, in which case the generated offspring will be a
copy of the parent. The parent selection process should therefore incorporate a
test to prevent such non-desirable case.
310
8
Genetic Algorithms and Evolutionary Programing

• It is also possible that the same individual takes part in more than one appli-
cation of the crossover operator. This becomes a problem when ﬁtness-pro-
portional selection schemes are used. In addition to parent selection and the
recombination process, the crossover operator considers a replacement policy. If
one offspring is generated, the offspring may replace the worst parent. Such
replacement can be based on the restriction that the offspring must be more ﬁt
than the worst parent, or it may be rejected. Crossover operators have also been
implemented where the offspring replaces the worst individual of the population.
In the case of two offspring, similar replacement strategies can be used.
Binary Representations
Most of the crossover operators for binary representations are sexual, being applied
to two selected parents. If x1ðtÞ and x2ðtÞ denote the two selected parents, then the
recombination process is described in Algorithm 8.1. In this algorithm, mðtÞ is a
mask that speciﬁes which bits of the parents should be swapped to generate the
offsprings, ~x1ðtÞ and ~x2ðtÞ. Several crossover operators have been developed to
compute the mask:
• One-point crossover: Holland [2] suggestedthat segments of genes be swapped
between the parents to create their offspring, and not single genes. A one-point
crossover operator was developed that randomly selects a crossover point, and
the bitstrings after that point are swapped between the two parents. One-point
crossover is illustrated in Fig. 8.1b. The mask is computed using Algorithm 8.1.
• Two-point crossover: In this case two bit positions are randomly selected, and
the bitstrings between these points are swapped as illustrated in Fig. 8.1c. The
mask is calculated using Algorithm 8.2. This operator can be generalized to an
n-point crossover [3].
• Uniform crossover: The nx-dimensional mask is created randomly [3] as
summarized in Algorithm 8.3. Here, px is the bit-swapping probability. If
px ¼ 0:5, then each bit has an equal chance to be swapped. Uniform crossover is
illustrated in Fig. 8.1a.
Algorithm 8.1. One-Point Crossover Mask Calculation
Select the crossover point n  Uð1; nx  1Þ;
Initialize the mask: mjðtÞ ¼ 0, for all j ¼ 1; . . .; nx;
for j ¼ n þ 1 to nx do
mjðtÞ ¼ 1;
end
Algorithm 8.2. Two-Point Crossover Mask Calculation
Select the two crossover points, n1; n2  Uð1; nxÞ;
Initialize the mask: mjðtÞ ¼ 0 for all j ¼ 1; . . .; nx
for j ¼ n1 þ 1 to n2 do
mjðtÞ ¼ 1
end
8.2
Genetic Algorithms
311

Algorithm 8.3. Uniform Crossover Mask Calculation
Initialize the mask: mjðtÞ ¼ 0 for all j ¼ 1; . . .; nx
for j ¼ 1 to nx do
if Uð0; 1Þ  px then
mjðtÞ ¼ 1 end
end
Bremermann [4] proposed the ﬁrst multi-parent crossover operators for binary
representations. Given nl parent vectors, x1ðtÞ; . . .; xglðtÞ, majority mating gener-
ates one offspring using
~xljðtÞ ¼
0; if n0
l [ nl
2 ; l ¼ 1; . . .; nx
1; otherwise
8
<
:
ð8:1Þ
where n0
l is the number of parents with xljðtÞ ¼ 0.
A multi-parent version of n-point crossover was also proposed, where nl  1
identical crossover points are selected in the nl parents. One offspring is generated
by selecting one segment from each parent.
8.2.1
Floating-Point Representations
The crossover operators discussed above (excluding majority mating) can also be
applied to ﬂoating-point representations as discrete recombination strategies. In
contrast to these discrete operators where information is swapped between parents,
intermediate recombination operators, developed speciﬁcally for ﬂoating-point
representations, blend components across the selected parents.
Fig. 8.1 Crossover operators for binary representations. a Uniform crossover, b one-point
crossover, c two-point crossover
312
8
Genetic Algorithms and Evolutionary Programing

One of the ﬁrst ﬂoating-point crossover operators is the linear operator proposed
by Wright [3]. From the parents, x1ðtÞ and x2ðtÞ; three candidate offspring are
generated as x1ðtÞ þ x2ðtÞ
ð
Þ; 1:5x1ðtÞ  0:5x2ðtÞ
ð
Þ and 0:5x1ðtÞ þ 1:5x2ðtÞ
ð
Þ.
The two best solutions are selected as the offspring. Wright also proposed a
directional heuristic crossover operator where one offspring is created from two
parents using
~xijðtÞ ¼ Uð0; 1Þðx2jðtÞ  x1jðtÞÞ þ x2jðtÞ
ð8:2Þ
subject to the constraint that parent x2ðtÞ cannot be worse than parent x1ðtÞ.
The arithmetic crossover was suggested in [5], which is a multi-parent recom-
bination strategy that takes a weighted average over two or more parents. One
offspring is generated using
~xijðtÞ ¼
X
nl
l¼1
cljxljðtÞ ;
ð8:3Þ
where Pnl
l¼1 clj ¼ 1.
A special case of the arithmetic crossover operator is obtained for nl ¼ 2, in
which
~xijðtÞ ¼ ð1  cÞx1jðtÞ þ cx2jðtÞ
ð8:4Þ
with c 2 0; 1
½
. If c ¼ 0:5, the effect is that each component of the offspring is
simply the average of the corresponding components of the parents.
Eshelman and Schaffer [6] developed a variation of the weighted average given
in Eq. (8.4), referred to as the blend crossover (BLX-a), where
~xijðtÞ ¼ ð1  cjÞx1jðtÞ þ cjx2jðtÞ
ð8:5Þ
with cj ¼ ð1 þ 2aÞUð0; 1Þ  a:
The BLX-a operator randomly picks, for each component a random value in the
range
x1jðtÞ  a x2jðtÞ  x1jðtÞ


; x2jðtÞ þ a x2jðtÞ  x1jðtÞ




ð8:6Þ
BLX-a assumes that x1jðtÞ\x2jðtÞ: It was found that a = 0.5 works well.
The BLX-a has the property that the location of the offspring depends on the
distance between the parents. If this distance is large, then the distance between the
offspring and its parents will be also large. The BLX-a allows a bit more explo-
ration than the weighted average of Eq. (8.3), due to the stochastic component in
producing the offspring.
Michalewicz et al. [5] developed the two-parent geometrical crossover to pro-
duce a single offspring as follows:
8.2
Genetic Algorithms
313

~xijðtÞ ¼ x1jx2j

0:5
ð8:7Þ
The geometrical crossover can be generalized to multi-parent recombination as
follows:
~xijðtÞ ¼ ðxa1
ij ; xa2
2j ; . . .; xan
nljÞ;
ð8:8Þ
where nl is the number of parents, and
X
nl
l¼1
al ¼ 1; al [ 0
ð8:9Þ
Deb and Agrawal [7] developed the binary crossover (SBX) to simulate the
behavior of the one-point crossover operator for binary representations. Two par-
ents, x1ðtÞ and x2ðtÞ are used to produce two offspring, where for j ¼ 1; . . .; nx.
~x1jðtÞ ¼ 0:5 ð1 þ cjÞx1jðtÞ þ ð1  cjÞx2jðtÞ


ð8:10Þ
~x2jðtÞ ¼ 0:5
1  cj


x1jðtÞ þ 1 þ cj


x2jðtÞ


ð8:11Þ
where
cj ¼
ð2rjÞ
1
g þ 1; if rj  0; 5
1
2ð1  rj


1
g þ 1
; otherwise
8
>
<
>
:
ð8:12Þ
rj  Uð0; 1Þ и g [ 0—is the distribution index.
It was suggested to take g ¼ 1 [7].
The SBX operator generates offspring symmetrically about the parents, which
prevents bias towards any of the parents. For large values of g there is a higher
probability that offspring will be created near the parents. For small g values,
offspring will be more far from the parents.
Now consider multi-parent crossover operators. The main objective of these
multi-parent operators is to intensify the explorative capabilities compared to
two-parent operators.
The unimodal distributed (UNDX) operator was developed where two or more
offspring are generated using three parents. The offspring are created by an ellip-
soidal probability distribution, with one of the axes formed along the line that
connects two of the parents. The extent of the orthogonal direction is determined
from the perpendicular distance of the third parent from the axis. The UNDX
operator can be generalized to work with any number of parents, with 3  nl  ns.
For the generalization, nl  1 parents are randomly selected and their center of
mass (mean), xðtÞ, is calculated, where xjðtÞ ¼ P
nl1
l¼1
xljðtÞ.
314
8
Genetic Algorithms and Evolutionary Programing

From the mean, nl  1 direction vectors, dlðtÞ ¼ xlðtÞ  xðtÞ are computed, for
l ¼ 1; . . .; nl  1.
Using the direction vectors, the direction cosines are computed as
elðtÞ ¼ dlðtÞ= dlðtÞ
j
j;
ð8:13Þ
where dlðtÞ
j
j is the length of vector dlðtÞ. A random parent, with index nl is
selected.
Let
xnlðtÞ  xðtÞ
be
the
vector
orthogonal
to
all
elðtÞ,
and
d ¼ xnlðtÞ  xðtÞ

.
Let elðtÞ, l ¼ nl; . . .; ns be the orthonormal basis of the subspace orthogonal to
the subspace spanned by the direction cosines, elðtÞ, l ¼ 1; . . .; nl  1.
Offspring are then generated using
~xiðtÞ ¼ xðtÞ þ
X
nl1
l¼1
Nð0; r2
1Þjjdljjel þ
X
ns
l¼nl
Nð0; r2
2ÞdelðtÞ
ð8:14Þ
where
r1 ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
nl  2
p
and r2 ¼
0; 35
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ns  nl  2
p
:
ð8:15Þ
Using Eq. (8.14) any number of offsprings can be created, sampled around the
center of mass of the selected parents. A higher probability is assigned to create
offspring near the center rather than near the parents. The effect of the UNDX
operator is illustrated in Fig. 8.2a for nl ¼ 4.
Tsutsui and Goldberg [8] proposed the simplex crossover (SPX) operator as
another center of mass approach to recombination. nl ¼ nx þ 1 Parents are chosen
independently for nx—dimensional search space. These nl parents form simplex.
Simplex is extended in each of nl directions as shown in Fig. 8.2. For nx ¼
2; nl ¼ 3 and xðtÞ ¼ P
nl
l¼1
xlðtÞ extended simplex is determined by points
ð1 þ cÞðxlðtÞ  xðtÞÞ
ð8:16Þ
for l ¼ 1; nl ¼ 3; и c  0. The offsprings are generated uniformly in extended
simplex.
In [9] a variation of the UNDX operator was proposed, which was called as
parent-centric crossover (PCX). Instead of generating offspring around the center of
mass of the selected parents, offsprings are generated around selected parents. PCX
selects nl parents and computes their center of mass, xðtÞ. For each offspring to be
generated one parent is selected uniformly from the nl parents. A direction vector is
calculated for each offspring as
dlðtÞ ¼ xlðtÞ  xðtÞ
ð8:17Þ
8.2
Genetic Algorithms
315

where xlðtÞ is the randomly selected parent. From all the other nl  1 parents
perpendicular distances, dl, for i ¼ l ¼ 1; . . .; nl, are calculated to the line diðtÞ. The
average over these distances is calculated, i.e.
d ¼
X
nl
l ¼ 1
l 6¼ 1
dl
nl  1
ð8:18Þ
Fig. 8.2 Illustration of crossover operators based on center of mass for several parents (points
represents potential offsprings)
316
8
Genetic Algorithms and Evolutionary Programing

Then offspring is generated using
~xiðtÞ ¼ xðtÞ þ
X
nl1
l¼1
Nð0; r2
1ÞjjdlðtÞjjel þ
X
ns
l¼nl
Nð0; r2
2ÞdelðtÞ
ð8:19Þ
where xiðtÞ is the randomly selected parent of offspring ~xiðtÞ, and felðtÞg are the
orthonormal basis of the subspace orthogonal dlðtÞ. The effect of the PCX operator
is illustrated in Fig. 8.2c.
The diagonal crossover operator is a generalization of n-point crossover for more
than two parents: n  1 crossover points are selected and applied to all of the
nl ¼ n þ 1 parents. One or n þ 1 offspring can be generated by selecting segments
from the parents along the diagonals as illustrated in Fig. 8.3, for n ¼ 2, nl ¼ 3.
8.3
Mutations
The aim of mutation is to introduce new genetic material into an existing individual;
that is, to add diversity to the genetic characteristics of the population. Mutation is
used in support of crossover to ensure that the full range of allele is accessible for
each gene. Mutation is applied at a certain probability, pm, to each gene of the
offspring, ~xiðtÞ, to produce the mutated offspring, xiðtÞ. The mutation probability,
also referred to as the mutation rate, is usually a small value, pm 2 0; 1
½
, to ensure
that good solutions would not be distorted too much. Given that each gene is
mutated at probability pm, the probability that an individual will be mutated is given
by Pr obðmutation ~xiðtÞÞ ¼ 1  ð1  PmÞnx.
Fig. 8.3 Diagonal crossover
8.2
Genetic Algorithms
317

Assuming binary representations, if H ~xiðtÞ; xðtÞ
ð
Þ is the Hamming distance
between offspring ~xiðtÞ and its mutated version xiðtÞ, then the probability that the
mutated
version
resembles
the
original
offspring
is
given
by
Pr obðx0
iðtÞ  ~xiðtÞÞ ¼ P
Hð~xiðtÞ;x0
iðtÞÞ
m
ð1  PmÞnxHð~xiðtÞ;x0
iðtÞÞ.
Binary Representations
For binary representations, the following mutation operators have been
developed:
• Uniform (random) mutation, where bit positions are chosen randomly and the
corresponding bit values are negated as illustrated in Fig. 8.4a
• Inorder mutation, where two mutation points are randomly selected and only
the bits between these mutation points undergo random mutation. Inorder
mutation is illustrated in Fig. 8.4b
• Gaussian mutation: For binary representations of ﬂoating-point decision
variables, it was proposed that the bit-string that represents a decision variable
be converted back to a ﬂoating-point value and mutated with Gaussian noise.
For each chromosome random numbers are obtained from a Poisson distribution
to determine the genes to be mutated. The bit-strings representing these genes
are then converted. To each of the ﬂoating-point values is added the stepsize
N 0; rj


, where rj is 0.1 of the range of that decision variable. The mutated
ﬂoating-point value is then converted back to a bitstring. Hinterding showed
[10] that Gaussian mutation on the ﬂoating-point representation of decision
variables provided superior results to bit ﬂipping.
For large dimensional bit-strings, mutation may signiﬁcantly increase the com-
putational cost of the GA. In order to reduce computational complexity, was pro-
posed to divide the bit-string of each individual into a number of bins [3]. The
mutation probability is applied and if a bin is to be mutated, one of its bits is
randomly selected and ﬂipped.
Floating-Point Representations
As indicated by Hinterding [10] and Michalewicz [5], better performance is
obtained by using a ﬂoating-point representation when decision variables are
ﬂoating-point values and by applying appropriate operators to these representations,
Fig. 8.4 Mutation operators for binary presentation. a Random mutation, b Inorder mutation
318
8
Genetic Algorithms and Evolutionary Programing

than to convert to a binary representation. This resulted in the development of
mutation operators for ﬂoating-point representations. One of the ﬁrst proposals was
a uniform mutation, where
x0
ijðtÞ ¼
~xijðtÞ þ D t; xmax  x0
ijðtÞ


; if a random digit is 0;
~xijðtÞ þ D t;~xijðtÞ  xminjðtÞ


; if a random digit is 1
(
ð8:20Þ
where D(t, x) returns random values from the range [0, x].
8.4
Parameters Control
In addition to the population size, the performance of a GA is inﬂuenced by the
mutation rate, pm, and the crossover rate, pc. In early GA studies very low values for
pm and relatively high values for pc were proposed. Usually, the values for pm and
pc are kept static. It is, however, widely accepted that these parameters have a
signiﬁcant inﬂuence on performance, and optimal settings for pm and pc can sig-
niﬁcantly improve performance. To obtain such optimal settings through empirical
parameter tuning is a time consuming process. A solution to the problem of ﬁnding
best values for these control parameters is to use dynamically changing parameters.
Although
dynamic,
and
self-adjusting
parameters
have
been
used
for
Evolutionary programing (EP) and ES (see Sects. 8.5) already in the 1960s, Fogarty
[11] provided one of the ﬁrst studies of dynamically changing mutation rates for
GAs. In this study, Fogarty concluded that performance can be signiﬁcantly
improved using dynamic mutation rates. Fogarty used the following schedules
where the mutation rate exponentially decreases with generation number.
Dynamic genetic algorithms were a continuation of the idea of generational
algorithms that are changing the size of the population. In contrast to genetic
algorithms with ﬁxed parameters, dynamic genetic algorithms are able to modify
and adjust their parameters, such as the size of the population, the essence of
genetic operators, the probability of their use, and even the genotype of the algo-
rithm in the process. The following main strategic parameters are used that can be
changed during operation of the genetic algorithm [11]:
• The operator and mutation probability of mutation;
• The operator crossover and the probability of a crossover;
• The mechanism of selection and its pressure (selective pressure);
• Population size.
The researcher is always able to determine the sub-optimum values of the
algorithm parameters empirically, but as genetic search process is inherently
dynamic and adaptive process, it makes sense to assume that the optimal parameters
of the algorithm may change during operation. Often, to improve the application of
genetic algorithm it uses dynamic mutation and crossover, as well as changes the
size of the population. There are two basic approaches to the determination of the
8.3
Mutations
319

parameters of the genetic algorithm: adjustment of parameters (parameter tuning)
and control parameters (Fig. 8.5) [12].
Parameters adjusting consists in running a number of tests to determine at which
parameter values genetic algorithm behaves well. Thus, the optimal probabilities of
crossover and mutation, population size, and other parameters are experimentally
investigated.
De Jong (de Jong) [13] conducted a large-scale study (for a number of test
functions) to identify the optimal parameters of the genetic algorithm in the tra-
ditional formulation and recommended to use: population size 50, single-point
crossover probability of 0.6 and mutation probability: 0.001. But, of course, these
values are not universal. Control of the parameters of the algorithm involves
starting with some initial values of parameters may deﬁned by the ﬁrst method.
Further, during operation of the algorithm, these parameters are modiﬁed in various
ways.
Depending on the method of modifying parameters such genetic algorithms are
divided into several categories [13]:
1. deterministic (deterministic)
2. adaptive
3. self-adapting (self-adaptive).
The deterministic genetic algorithms parameters are changed during operation of
the algorithm on some predetermined law, which often depends on the time (the
number of generations, or the amount of computation of the objective function).
Fig. 8.5 Classiﬁcation of methods of GA parameters setting (control)
320
8
Genetic Algorithms and Evolutionary Programing

The main problem here is to ﬁnd the form of the aforementioned law for maximum
performance of the genetic algorithm as part of the task.
Consider how the concept of deterministic genetic algorithms can be applied in
practice. For example, in the genetic algorithm with non-ﬁxed population size each
individual is attributed to the maximum age, i.e. the number of generations, after
which the individual dies [14]. The introduction of the new algorithm parameter—
the age enables to include operator selection in a new population. Age of each
individual is different and depends on its adaptability. In every generation at step
crossover in the usual way additional population of descendants is created. The size
of the additional population is proportional to the size of the general population
V AðtÞ ¼ size AðtÞ  p, where p—the probability of reproduction.
For reproducing the individuals are selected from the general population with
equal probability, regardless of their ﬁtness. After applying the mutation and
crossover to offspring is attributed age according to the value of their ﬁtness. Then
from the general population are removed those individuals whose lifespan has
expired, and added the descendants of the intermediate population. Thus, the size of
the population after one iteration of the algorithm is calculated according to the
formula:
size Aðt þ 1Þ ¼ size AðtÞ þ size BðtÞ  DðtÞ;
ð8:21Þ
where DðtÞ—the number of individuals who die in the generation t.
To organize deterministic dynamic mutations Back (Back) and Schutz (Schuts)
[3] used a decreasing function of time of the following form:
p ðtÞ ¼ ð2 þ l  2
T  1 tÞ1:
ð8:22Þ
where T—the number of iterations of the genetic algorithm, and l—length of the
chromosome. The probability of mutation obtained by this formula is bounded in
the interval ð0; 1=2.
One of the ﬁrst dynamically varying intensity parameters mutations was pro-
posed by Fogarty [11]. Fogarty proposed the following scheme in which the
mutation rate decreases exponentially with the number of generation t:
PmðtÞ ¼ 1
240 þ 0; 11375
2t
ð8:23Þ
Alternatively, Fogarty also proposed for the binary representation the intensity of
the mutations on the 1-bit:
PmðjÞ ¼ 0; 3528
2j1
; j ¼ 1; 2; . . .; nb; nb;
ð8:24Þ
where nb—indicates how the least signiﬁcant bit.
8.4
Parameters Control
321

These two schemes were combined and the following dependence for Pm was
obtained:
Pmðj; tÞ ¼
28
1905 	 2j1 þ 0; 4026
2t þ j1
ð8:25Þ
Various schemes may be used to reduce the intensity of mutations. In the above
scheme is suggested exponential version.
An alternative may be a linear scheme, which provides a slow change, allowing
a large number of experiments. However, a slow reduction in the intensity of
mutations can also be devastating for found good solutions.
Virtually all such formulas include the condition 1=t, where t—number of
generations. In this case, the initial probability of mutation is quite high and
decreases as the algorithm runs. Thus, in the early stages of a genetic algorithm
mutations play a key role in identifying the most promising areas of the search, but
later for their study a crossover is used.
Such formulas don’t eliminate the need to set parameters, but signiﬁcantly
increase the efﬁciency of the genetic algorithm by researchers estimates.
Note that the probability of mutations in these studies deterministically changes
in time and does not depend on the actual search results. The question of the
optimality of this approach is controversial. Alternatively, the current search results
can be used to adjust the parameters of the genetic algorithm.
In adaptive genetic algorithms information about the algorithm results through
feedback effect on the values of its parameters. However, unlike the self-adapting
algorithms, the values of these parameters are common to all individuals of a
population. But since the essence of the mechanism is determined by the feedback
researcher, the question how to make it the optimal remains open.
This method of control algorithm was ﬁrst used by Rechenberg (Rechenberg)
[12]. He postulated that one of the ﬁve mutations should lead to the emergence of a
better individual. Further, he linked the likelihood of mutations with a frequency of
good mutations. Thus, if the frequency of good mutations is above 1=5, the
probability of mutation increases and vice versa. In the formalized form
Rechenberg rule usually is such:
if uðkÞ\1=5 ! ðpm=kÞ
if uðkÞ ¼ 1=5 ! ðpmÞ
if uðkÞ [ 1=5 ! ðkpmÞ;
where uðkÞ—the success rate of mutations in k iterations, and -k is training coef-
ﬁcient. (usually k ¼ 1:1).
To improve efﬁciency of the genetic algorithm Terence (Thierens) proposed
using two adaptive mutations change the scheme. (Constant Gain scheme) has the
form [12]:
322
8
Genetic Algorithms and Evolutionary Programing

1. The individual ðx; pmÞ is mutated in 3 different ways
Mðx; pm=xÞ ! ðx1Þ
Mðx; pmÞ ! ðx2Þ
Mðx; xpmÞ ! ðx3Þ
2. Selection of the ﬁttest individual and the probabilities of new mutations
MAXfðx; pmÞ; ðx1; pm=kÞ; ðx2; pmÞ; ðx3; kpmÞg
In expressions above k is a training coefﬁcient, and x- factor of study. Usually k
and x have different values (1\k\x). In order to avoid ﬂuctuations in the
learning process, it is limited as x [ k. Typical values of coefﬁcients are: k ¼ 1:1
and x ¼ 1:5.
Declining scheme promotes the use of more aggressive step size, at the same
time suppressing the random ﬂuctuations that may occur at a high learning rate.
1. The individual is mutated in 3 different ways
Mðx; xpmÞ ! ðx1Þ; Mðx; pmÞ ! ðx2Þ; Mðx; pm=xÞ ! ðx3Þ
2. The decrease of parent mutation probability ðx; pmÞ ! ðx; cpmÞ
3. Selection of the ﬁttest individual and new mutation probability
MAXfðx; cpmÞ; ðx1; kpmÞ; ðx2; pmÞ; ðx3; kpmÞg
The difference between the two schemes is that the second scheme has no ability
to increase the probability of mutations and current mutation probability will
decrease, until a next successful descendant. Used coefﬁcients lie within the fol-
lowing bounds: k [ 1,x [ 1 and 0:9\c\1.
Typical coefﬁcients values are k ¼ 2:0, x ¼ 2:0 and c ¼ 0:95. To adapt the
probabilities of crossover and mutation Shi (Shi) used a set of fuzzy logic rules.
Table 8.1 describes a set of fuzzy rules relating to the probability of mutations
(MP) [12].
In the expressions above, BF—best ﬁtness value, UN—number of iterations
since the last change BF, MP—the probability of a mutation. This set of rules, of
course, provides the giving of membership functions for different metrics and
mutation probabilities. For example, Sugeno fuzzy inference may be used. In order
8.4
Parameters Control
323

to organize the dynamic changes of the crossover or mutation operator different
probabilities, constant or linear functions are selected for each fuzzy inference.
In the original formulation of a set of rules by Shi (Shi) as the metrics was used
the variance of ﬁtness with high, medium and low fuzzy values.
For optimization problems with constraints penalty functions are frequently
used. This idea can be applied to the organization of the adaptive crossover and
mutation. So, for solving graph coloring problem Eiben and van der Hauw sug-
gested the use of an evaluation function with stepwise adaptation of weights: when
a certain constraint is violated, the value of penalty for the corresponding variable
increases evalðxÞ ¼ f ðxÞ þ W  penaltyðxÞ, where f ðxÞ is the objective function,
penaltyðxÞ—the penalty function, and w—weighting factor that determines weight
of penalty to be appointed in violation of restrictions. If all constraints are satisﬁed,
then penaltyðxÞ ¼ 0.
For the organization of the mechanism of self-adaptive population size
Greffenstet used a meta-approach based on the use of other genetic algorithm. Thus,
each iteration of meta-genetic algorithm has initiated the launch of the target genetic
algorithm to adapt the parameters. The result of the meta-algorithm was optimal
population size for the target algorithm.
The idea of organizing self-adapting mutation operator was ﬁrst implemented by
Scheffel (Shaffel) in evolutionary strategy, where he tried to control the step
mutations. Back (Back) disseminated his ideas on genetic algorithms. He added to
the individual extra bits that coded the probability of mutation. Each coded prob-
ability of mutation applies both to itself and to the corresponding gene (Table 8.2):
Experimental studies have shown that by using this approach, a signiﬁcant
increase in performance of the genetic algorithm is achieved.
Of course, it is also possible to combine the scheme adapting the parameters of
the genetic algorithm in a frame of one task. For example, the organization of the
mechanism of adaptation both a crossover and mutation can signiﬁcantly increase
the performance of the genetic algorithm.
Thus, control of parameters allows to use a variety of the most suitable
parameter values in different stages of genetic research to maximize the perfor-
mance of the algorithm. Adaptive and self-adapting genetic algorithms complicate
Table 8.1 Set of fuzzy
adaptive rules (Shi) for
mutations
1. If BF is low or middle MP is low
2. If BF is middle and UN is low then, MP is low
3. If BF is middle and UN is middle then, then MP is middle
4. If BF is high and UN is low then MP is low
5. If BF is high and UN is middle then, MP is high
6. If UN is high then, MP is chosen randomly (high, middle or
low)
Table 8.2 Extended
chromosome with coded
mutations probabilities
x1 x2 x3 . . . xn1 xn . . . xm p1 p2 p3 . . . pn1 pn . . . pm
x1 x2 x3 . . . xn1 xn . . . xm p0
1 p0
2 p0
3 . . . p0
n1 p0
n . . . p0
m
324
8
Genetic Algorithms and Evolutionary Programing

the algorithm itself, delegating him the task of determining the value of strategic
options, but at the same time, they eliminate the need for researchers to adjust
parameters manually.
The next step in the development of dynamic algorithms is the emergence of
multi-level dynamic genetic algorithms (nGA) [12]. The lower level of the algo-
rithm itself solves the task of improving the decisions population. The upper levels
are genetic algorithms that solve the optimization problem to improve the param-
eters of the algorithm of the lower level as the objective function is usually used the
speed of the algorithm of the lower level and the rate of improvement of their
population from generation to generation.
A good strategy is to link the intensity of mutations with the individual ﬁtness
index: the higher the index of ﬁtness of the individual, the less must be the prob-
ability of mutations of its gene, and conversely the lower is ﬁtness index the greater
is the probability of mutations.
For ﬂoating-point representations, the performance of the GA also affects the
step size of mutations. The ideal strategy is to start with the large size mutations to
provide more random jumps in the search space. The step size of mutations
decreases with time, so that a very small change is produced at the end of the search
process. The step size may also be proportional to the individual ﬁtness index, so
that less suitable individuals had a greater step size than the more suitable.
The intensity of the crossover Pc also has a signiﬁcant impact on the perfor-
mance of GA. Since the optimum value of Pc depends on the task for dynamic
adjustment may be used the same alternative strategy that for mutations.
In addition to crossover choice of the best operator of evolution is also a
problem-oriented. In general, the deﬁnition of the best set of operators and control
parameters values are essentially the problems of multi-criteria optimization.
8.5
Strategy Selection
The above GA greatly differ from biological evolution in that the population size is
ﬁxed. It allows to describe the selection process consisting of two steps:
• Selection of the parents;
• Replacement strategy, which decides whether to replace a descendant of parents
and what kind of parents should be replaced.
There are two main classes of GA, based on the strategy used for replacement
parents: generational GA and steady state GA [3]. In the generational GA
replacement policy replaces all parents to their descendants, after the descendants
were generated and subjected to mutations. As a result, there is no overlay (inter-
section) between the current population and new population (assuming that elitism
is not used).
8.4
Parameters Control
325

In a steady state GA after generation and mutation of descendant it’s immedi-
ately decided: to keep a parent or offspring for the next generation. Thus, there is
overlap between the current and next generations.
The value of the intersection between the current and the new generation is
called “generational gap”. Generational GA have zero gap, while the steady-state
GA have large gaps between generations.
A large number of strategies for GA steady state were developed:
• To replace the worst, when the descendant replaces the worst individual of the
current generation;
• Random replacement when the descendant replaces a randomly selected indi-
vidual from the current generation;
• “Replacement of the oldest” strategy (ﬁrst-in-ﬁrst-out), which should replace the
oldest of the individual from the current population. This strategy has a high
probability of replacing one of the best individuals.
• Tournament strategy that is randomly selected group of individuals, and the
worst individual in the group is replaced by a descendant. Alternatively, the size
of the tournament is two, and the worst individual is replaced by a descendant
with a probability 0; 5  pr  1:
• Conservative strategy, combines the strategy to replace the oldest one with the
upgraded deterministic tournament strategy. The size of the tournament consists
of two individuals, one of which is the oldest individual in the current popu-
lation. The worst of the two is replaced by a descendant. This approach ensures
that the older individual is not lost, if he has the best ﬁtness index.
• A strategy of “elitism” in which the best parents are excluded from the selection
and automatically transferred to the next generation.
• Competition “parent-child”, which uses a strategy of selection to decide whether
to replace by a descendant of one of their parents.
Island Genetic Algorithms
GA is well suited for a parallel implementation. Three main categories are distin-
guished in parallel GA:
• Uniform GA type (master-slave), in which the evaluation of the ﬁtness index is
distributed across multiple processors;
• uniform cell GA, where each individual is assigned to one processor and each
processor is assigned to only one individual. For each of the individual is
determined the scope of a small neighborhood, breeding and reproduction are
limited only by the scope of the neighborhood.
• Multi-population or island GA, which use a lot of populations, each on a sep-
arate processor. Information exchange between populations is performed
through migration policy. Although island GA have been developed for a
parallel implementation of many processors, they can also be implemented in a
uniprocessor system.
326
8
Genetic Algorithms and Evolutionary Programing

Let us consider in more detail the island GA. In the island GA, several popu-
lations are developing in parallel, in a cooperative structure. In such a model, the
GA has lots of islands, each island has one population. Selection, crossover and
mutation occurs in each population independently of the other. In addition, indi-
viduals are allowed to migrate between islands (or populations) as shown in
Fig. 8.6.
An integral part of the island GA is migration policy governing the exchange of
information between the islands. Migration policy determines [3]:
• The communication topology, which deﬁnes the way of individuals migration
between the islands. For example, a ring topology (as shown in Fig. 8.6) allows
the exchange of information between the neighboring islands. The communi-
cation topology determines how fast (or slow) good decisions are propagating to
other populations. For loosely coupled structure (such as a ring topology)
islands are more isolated from one another, and dissemination of good decisions
is carried slower. Loosely topology also contributes to the appearance of the
solution set. Tightly structure have a greater speed of information dissemination,
which can lead to premature convergence.
• The intensity of migration, which determines the frequency of migrations.
Closely related to the intensity of the migration is the question when the
migration should take place. If migration occurs too early, the number of good
building blocks for migrants may be too small to have any effect on the ﬁnal
results. Typically, migration occurs when each population converged to some
Fig. 8.6 Routes of migration and visits in island genetic algorithms
8.5
Strategy Selection
327

decision. After an exchange of individuals, the development process of the
population is restarted again.
• Selection mechanism used to decide exactly which individuals will migrate.
• Replacement strategy is used to decide which individual destination node will
be replaced by a migrant.
On the basis of breeding strategies and replacement island GAs can be grouped
into 2 classes of algorithms: namely, static and dynamic island GAs. For statistical
island GAs deterministic strategy of selection and replacement is used, for example,
[3]
• Good migrant replaces the bad individual;
• A good migrant replaces a randomly selected individual;
• Randomly selected migrant replaces a bad individual;
• Randomly selected migrant replaces a randomly selected individual.
For a good selection of the migrant any of the ﬁtness-proportional selection
operators can be used. For example, elitism strategy means the best individual of
one population moves to another population.
Gordon [10] used the tournament selection, considering the two randomly
selected individuals. The best of the two migrates, while the worst is replaced by an
individual which won the tournament from the neighboring population.
Dynamic models do not use the topology to determine the migration routes.
Instead, decisions about migration are performed in a probabilistic way. Migration
takes place with a certain probability. If migration from the island takes place, the
island destination is also determined probabilistically. It may be used tournament
selection using the average values of the population ﬁtness index. Additionally, we
can use the strategy of decision-making to decide whether immigrants to be
accepted. For example, immigrant can be accepted with probability if his ﬁtness
index (FI) is higher than the values of the other FI in this island.
Another important aspect to be considered in the island GA, it must be initialized
as a separate population. Of course, it is possible to use a random approach which
may lead to that different populations will cover the same part of the search space.
The best approach is to initialize different populations so that they would cover
different parts of the search space, and contribute to the formation of a sort of niche
in the individual islands. Next, you step on the meta-level to combine solutions
from different islands.
Another type of “island” is a cooperative co-evolutionary GA (CCGA) [3]. In
this case, instead of allocating all individuals for several (sub) populations, each
population is given one or more genes (one decision variable) to optimize.
Subpopulations are mutually exclusive, each is intended to develop one or more
genes. Each population therefore optimizes one (or several) genes, i.e. no sub-
population does have the necessary information to solve the problem indepen-
dently. Conversely, information from all populations is to be combined to construct
a solution.
328
8
Genetic Algorithms and Evolutionary Programing

Stop conditions. GA may use different conditions stop Among the most com-
mon should be mentioned:
• The number of iterations of the algorithm;
• Achieved ﬁtness value of the index of the best individual;
• If the value of FI of the best individual does not change for a predetermined
number of iterations
Multi-criteria Optimization (MCO)
Much attention is paid to the use of GA for solving multi-criteria optimization
problems:
min
x f
i
ðx; ciÞ; i ¼ 1; k
ð8:26Þ
Algorithm VEGA. One of the ﬁrst GA algorithms for solving problems of the
MCO is the vector genetic algorithm (VEGA—Vector Evaluated Genetic
Algorithm) [3].
Each sub-population is associated with one objective function. Further the
selection is applied to each subpopulation to create a pool of applicants. As a result
of selection process the best individuals in each of the objective function are
included in the pool of pretendents. Next crossover is performed by selecting
parents from a pool of pretenders.
8.6
The Application of Genetic Algorithm in the Problem
of Networks Structural Synthesis
The important application of GA are combinatorial optimization problems, in
particularly computer `networks structural synthesis problems. Consider the for-
mulation of computer networks with technology MPLS synthesis problem [12].
There are given a set of network nodes X ¼
xj
 
j ¼ 1; n—MPLS routers (so-
called LRS—Label Switching Routers), their locations over regional area, channels
capacities D ¼ d1; d2; . . .; dk
f
g and their costs per unit length C ¼ c1; c2; . . .; ck
f
g,
classes of service (CoS) are determined, matrices of input demands for the kth CoS
are known HðkÞ ¼
hijðkÞ

 i; j ¼ 1; n; k ¼ 1; 2; . . .; K, where hijðkÞ is the inten-
sity of ﬂow of the kth CoS which is to transfer from node i to node j in sec (Mbits/s).
Additionally the constraints are introduced on the Quality of Service (QoS) for
each class k as constraint on mean PTD (packets transfer delay) Tinp;k, k ¼ 1; K and
packets loss ratio (PLR).
It’s demanded to ﬁnd network structure as a channels set E ¼
r; s
ð
Þ
f
g, choose
channels capacities
lrs
f
g and ﬁnd ﬂows distributions of all classes so that ensure
the transmission demands of all classes HðkÞ in full volume with mean delays Tav,
8.5
Strategy Selection
329

not exceeding the given values Tinp;k and by this the constraint on packets loss ratio
(PLR) should be fulﬁlled and total network cost be minimal [15].
Let’s construct the mathematical model of this problem.
It’s demanded to ﬁnd such network structure E for which
min
lrs
f
g CR M
ð
Þ ¼
X
ðr;sÞ2E
Crs
lrs
f
g
ð
Þ
ð8:27Þ
under constraints
Tav
lrs
f
g; frs
f
g
ð
Þ  Tinp;k k ¼ 1; K
ð8:28Þ
frs\lrs for all
ðr; sÞ
ð8:29Þ
lrs 2 D
ð8:30Þ
PLRk
lrs
f
g; frs
f
g
ð
Þ  PLRk inp
ð8:31Þ
where PLRkðflrsg; ffrsgÞ—is packets loss ratio for the kth ﬂow, PLRk inp—given
constraint on its value.
This problem belongs to so-called NP-complete optimization problems. For its
solution general genetic algorithm was developed using two operators: crossover
and mutation [12].
Deﬁne matrix of channels K ¼
kij

, where
kij ¼
1; i f 9ði  jÞ
0; otherwise; i:e::9ði  jÞ

;
ð8:32Þ
for each network structure. Then initial population of different structures in given
class of multi-connected structures is generated with connectivity coefﬁcient 2. For
synthesis we’ll use semi-uniform crossover which is grounded for small population
size.
Parents (structures Ei(k) are chosen randomly with probability inverse propor-
tional to cost CRðEiðkÞÞ, each parent is determined by matrix Ki; i ¼ 1; 2. In the
process of semi-uniform crossover each descendant receives exactly a half of
quantity of parents genes. The crossover mask is presented as the following matrix
M ¼
mij


where mij ¼
0; if p  p0
1; if p\ p0

;
where parameter p0 ¼ 0:5 and p 2 ½0; 1—is a random value.
330
8
Genetic Algorithms and Evolutionary Programing

This process of crossover may be written so:
EðkÞ0 ¼
eðkÞ0
ij

 ¼
ði  jÞ1; k1
ij ¼ 1; if mij ¼ 0;
ði  jÞ2; k2
ij ¼ 1; if mij ¼ 1:
(
In this crossover only one descendant is generated that to maximize the algo-
rithm productivity. In case if after crossover isolated sub-graphs were generated
then with direct channels they are connected to a root. Further for generated
descendant-structure EðkÞ0 capacities assignment and ﬂows distribution problems
are solved [15] and network total cost is calculated. Then using cost value CR
decision is made whether to include new structure EðkÞ0 in the set of local optimal
structures (new population) П or not.
After crossover operation the mutations are performed. Note that basic GA
algorithms use unconditional mutations. Mutations consist in deleting or adding
new channels into network structure. During the improvement of basic GA algo-
rithm in tandem with semi-uniform crossover the following schemes mutations
probability change were investigated [12]: deterministic and adaptive.
In deterministic case mutation probability is determined using function
depending on time. Thus we ‘ll change mutation probability according to the law:
rðtÞ ¼ 1  ct=T, where 0  t  T, a c 2 ð0; 1Þ—is a coefﬁcient. Note that as time
increases the mutation probability will decrease.
Note that main properties of this approach are:
1. mutation probability change doesn’t depend on success of its application in the
process of genetic search;
2. the researcher fully controls the mutation probability change by deﬁnite
formula;
3. mutation probability change is fully predictable.
For implementation of adaptive version of mutation probability change
Rechenberg rule was used [12]:
rðtÞ ¼
rðt  1Þ=k; uðt  1Þ\1=5
rðt  1Þk; uðt  1Þ [ 1=5
rðt  1Þ; uðt  1Þ ¼ 1=5
8
<
:
;
ð8:33Þ
where uðtÞ is good mutation percentage, and k ¼ 1:1 is a learning coefﬁcient.
Note that main properties of such approach are the following:
1. mutation probability change depends on success of its application in process of
genetic search;
2. the researcher has information how mutation probability will be changed:
3. mutation probability change is non-predictable.
Self-adaptive mutation may be organized at the level of chromosomes (network
structures) and at the level of genes(channels). For this should be given the law of
8.6
The Application of Genetic Algorithm in the Problem …
331

mutation probability change. But implementation of this approach leads to signif-
icant cut in algorithm productivity and in this problem it isn’t used [12]. As
alternative to the scheme of unconditional crossover and dynamic mutation the
scheme with unconditional mutation and dynamic crossover was explored in this
work.
In the process of algorithm perfection in pair with unconditional mutation the
following schemes of crossover probability change were applied and investigated:
• Deterministic. Implementation of deterministic scheme of crossover probability
change is based on hypothesis that at different stages of genetic search crossover
may be more or less signiﬁcant, therefore as a function of crossover probability
change should be used non-monotonic function. That’s why in this problem we
used the function rðtÞ ¼ sinðtÞ
j
j, where 0  t  T.
• Adaptive. Deﬁne adaptive crossover as an operator whose probability decreases
if population is sufﬁciently homogeneous and increases in case if the population
is heterogeneous. As the measure of homogeneity/heterogeneity we chose the
value
CD ¼ maxðCRðEiðkÞÞ  CRðEjðkÞÞ; i 2 ½1; . . .; n; j 2 ½1; . . .; n; i 6¼ j;
where n is a population size (in this case n = 3). It’s reasonable to suggest that in
case of similar individuals in population the crossover operation will be inefﬁcient
and vice versa. Therefore in adaptive crossover the law of crossover probability
change takes the form:
rðtÞ ¼
rðt  1Þk; CD [ C
rðt  1Þ=k; CD\C
rðt  1Þ; CD ¼ C
8
<
:
;
ð8:34Þ
where C is threshold value, k ¼ 1:1 is a learning coefﬁcient.
• Self-adaptive. The implementation of self-adaptive crossover is unreasonable
from algorithm productivity criterion as well as the implementation of
self-adaptive mutation.
The above-considered versions of GA were applied and investigated in the
project of Ukrainian global MPLS network design [12]. After series of experiments
with different initial input data and parameters, such as volumes of trafﬁc, given sets
of channels capacities and their costs per unit length the following results were
obtained:
• a combination of unconditional crossover and dynamic deterministic mutation:
with c ¼ 0:3 this realization proved to be the one of the most successful (per-
formance increase by 15 %) that conﬁrms the hypothesis about distribution of
participation between crossover and mutations at the different stages of genetic
search.
332
8
Genetic Algorithms and Evolutionary Programing

Thus, mutations play key role at the initial stage of search when exploring the
search space while at the ﬁnal stage it’s more efﬁcient to use crossover for ﬁnding
optimal solution.
• a combination of unconditional crossover and dynamic adaptive mutation: this
realization didn’t allow to achieve stable decrease of algorithm run time, that
conﬁrms idea about inefﬁciency of this approach for the considered problem;
• a combination of unconditional crossover and dynamic self-adaptive mutation:
this realization is unreasonable as it signiﬁcantly complicates the genetic search
and leads to algorithm performance decrease (productivity cut);
• a combination of dynamic deterministic crossover and unconditional mutation:
this realization didn’t ensure the stable performance increase, perhaps it’s nee-
ded more ﬁne adjustment of crossover probability change;
• a combination of dynamic adaptive crossover and unconditional mutation: such
realization proved to be the most successful and ensured the performance
increase by 20–22 %. Thus, the hypothesis about some positive properties of
crossover which mutation operator doesn’t have was conﬁrmed. Note that
application of crossover is grounded only if a population is heterogeneous, i.e.
the individuals differ from each other sufﬁciently.
• a combination of dynamic self-adaptive crossover and unconditional mutation:
such realization proved to be inefﬁcient due to complication of search process.
As illustration of the above described experimental results in the Fig. 8.7 the
initial structure of Ukrainian global network is presented, and in the Fig. 8.8 the
optimized structure obtained with application of modiﬁed genetic algorithm which
realizes a combination of dynamic adaptive crossover and unconditional mutation.
Note that cut in total network cost for optimized structure in comparison with
initial structure equals to 14,250 thousand $ −10,023 thousand $ = 4227 thousand
Fig. 8.7 Initial structure of global MPLS network
8.6
The Application of Genetic Algorithm in the Problem …
333

$, i.e. the cost after optimization is by 30 % less than the cost of initial network
structure. Additionally the suggested algorithm productivity increased by 22 % in
comparison with basic genetic algorithm [12].
8.7
Evolutionary Programing
Evolutionary programing (EP) appeared owing to the pioneer work of Vogel, Owen
and Walsh [16],who used the modeling of evolution for the development of arti-
ﬁcial intelligence (AI). Though the EP has the same overall objective of the natural
process of evolution simulation with GA and genetic programing, it differs sig-
niﬁcantly from them by that EP focuses on the development of behavioral patterns,
rather than genetic, i.e. EP considers so- called “phenotypic” evolution rather than
genotypic. Iteratively EA uses two of three evolution operators, namely mutations
that provide the necessary variety and selection. Recombination operators are
generally not used within the EP.
EP was invented in the early 60s as an alternative approach to AI that focused on
models of human intelligence. Because of its conclusion that intelligence can be
regarded as “property which allows the system to adapt its behavior to achieve the
desired goal in a wide range of environments”, a model was developed that sim-
ulates the evolution of behavior traits. Therefore, in contrast to other evolutionary
algorithms, ﬁtness—index (FI) measures the “behavioral noise” of the individual
with respect to its environment.
Fig. 8.8 Optimized structure of global MPLS network
334
8
Genetic Algorithms and Evolutionary Programing

EP uses 4 main components of evolutionary algorithms (EA) [16]:
(1) Initialization: like other paradigms initialized starting position of individuals
so as to cover uniformly the area of optimization.
(2) Mutations: mutation operator aims to introduce variation into the population,
i.e. generate new candidates. Each parent produces one or more offsprings by
mutation operator. A large number of mutation operators have been
developed.
(3) Rating: ﬁtness function is used to quantify the “behavioral noise” of indi-
viduals. While the ﬁtness function provides an absolute measure of ﬁtness, to
assess how well the individual solves the problem of optimization, survival in
the EP is normally based on the relative measure ﬁtness index. This measure
allows you to compare a group of individuals between themselves and the
individuals that survive and move to the next generation, are selected on the
basis of this measure.
(4) Selection: The main aim of the operator—is to select individuals who are
moving into the next generation. Selection—is a competitive process in which
the parents and their offsprings are competing among themselves for survival.
Operators of mutation and selection are used to iteratively until no stop condition
is satisﬁed.
They can be used in any of the stopping conditions considered for GA.
Comparing GA and EP are the following differences should be noted:
• EP focuses on phenotypic rather than genotypic evolution;
• In accordance with the above EP uses no operators of recombination; there is no
exchange of genetic material.
• EP uses relative ﬁtness function, to quantify with respect to the functioning of a
randomly selected group of individuals;
• Selection based on competition. Those individuals who are functioning better
than group of rivals receive greater probability of inclusion into the next
generation;
• Parents and offspring are struggling for survival;
• The behavior of individuals inﬂuences strategy parameters that determine the
amount of variations between parents and descendants.
Operators EP
Operators of mutations
Since the mutations is the only means of introducing variability into the population
EP, it is very important to design the mutation operator so that to provide a com-
promise between the phases the study—use.
After the initial phase of the study, individuals should be allowed to use the
information about the search space for ﬁne-tuning decisions.
For this discussion, assume that the objective is to minimize the continuous
function without limitation: f : Rnx ! R: If xiðtÞ denotes the pretendent for a
decision presented by the ith individual, where t is time, every xiðtÞ 2 R; j ¼ 1; nx.
8.7
Evolutionary Programing
335

In general, the mutation is described as x0
ijðtÞ ¼ xijðtÞ þ DxijðtÞ, wherein x0
ijðtÞ—a
descendant of the parent xijðtÞ generated by adding the step size DxijðtÞ to the
parent.
On the step size acts noise with some probability distribution, where the devi-
ation of the noise is determined by the strategic option rij. In general, the step size is
computed as
DxijðtÞ ¼ UðrijÞnijðtÞ
ð8:35Þ
where U : R ! R is a function, which scales the effect of noise nijðtÞ.
On the basis of the characteristics of the scaling function U, EP algorithms can
be divided into the following three categories [3]:
• Non-adaptive EP, in this case UðrÞ ¼ r. Otherwise, variations in step size—
static.
• Dynamic EP, in which the deviation in the step value changes over time, using
some deterministic function U.
• Self-learning EP in which variations in step size change dynamically. The
system is trained to ﬁnd the best values rij in parallel with the decision variables.
Since deviations rij affect the behavior of individuals in the case of dynamic and
self-learning EP, these deviations are considered as strategic parameters. As in
the case with all of evolutionary algorithms (EA), EP utilizes a stochastic search
process.
Stochastic is introduced by calculating the step size as a function of the noise
nijðtÞ, generated using a probability distribution. The following distributions are
used for EP:
1. Uniform. Noise is sampled from a uniform distribution
nijðtÞ  uðxminj; xmaxjÞ
ð8:36Þ
It should be noted that the E nij


¼ 0 in order to prevent any displacement by
noise where E 
f g operator of mathematical expectation. Wong and Yurevich
offered uniform mutation operator
DxijðtÞ ¼ Uð0; 1Þ ^yjðtÞ  xijðtÞ


This operator causes all individuals to do casual steps toward better individual
that is very similar to the social component used in the swarm optimization algo-
rithms [17].
2. Gaussian. For Gaussian mutation operators, noise is generated from a normal
distribution with zero mean: c.
336
8
Genetic Algorithms and Evolutionary Programing

In comparison with other distributions, Gaussian distribution density is deﬁned
as
fGðxÞ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
e x2
2r2
ð8:37Þ
3. Cauchy distribution. For operators mutation Cauchy distribution
nijðtÞ  C 0; t
ð
Þ
ð8:38Þ
where t—a parameter of the scale, the probability density function of Cauchy is
given
fcðxÞ ¼ 1
a
t
t þ x2
ð8:39Þ
where t [ 0. The corresponding distribution function is given by
FcðxÞ ¼ 1
2 þ 1
p arctg x
t

 
ð8:40Þ
Cauchy distribution has wider tails than a Gaussian distribution and therefore
produces more mutations.
4. Levi distribution. For Levi distribution [3] fijðtÞ  LðvÞ. Levi distribution with
center in coordinates beginning is given so
FL;U;cðxÞ ¼ 1
p
Z1
0
ecqv cosðqxÞdq;
ð8:41Þ
where c [ 0 is a scale coefﬁcient and 0\v\2 regulates distribution form. If v ¼ 1,
then obtain Cauchy distribution, if v ¼ 2, then obtain Gaussian distribution. For
xj j 
 1 Levi distribution may be approximated so: fLðxÞ  axðt þ 1Þ.
5. Exponential. In this case nijðtÞ  Eð0; nÞ, and the density function of the ex-
ponential distribution is given as
fE;nðxÞ ¼ n
2 en xj j
ð8:42Þ
where n [ 0 regulates variance (which is equal 2
n2).
8.7
Evolutionary Programing
337

Random numbers can be calculated so
x ¼
1
n ln 2y
ð
Þ; if y  0:5
 1
n ln 2 1  y
ð
Þ; if y [ 0:5
8
>
<
>
:
where y  Uð0; 1Þ.
It’s easy to note that Eð0; nÞ ¼ 1
n Eð0; 1Þ.
6. Chaotic. Chaotic distribution is used for noise generation according to.
nijðtÞ  Rð0; 1Þ,
where Rð0; 1Þ presents a chaotic sequence in the space (1;1) generated according to
recurrence [3]
xt þ 1 ¼ sin
2
xt
 
xt;
t ¼ 0; 1; 2; . . .
ð8:43Þ
7. Combined distribution. The average mutation operator, was proposed which is a
linear combination of Gaussian and Cauchy distributions [3]. In this case
nijðtÞ ¼ nN;ijðtÞ þ nC;ijðtÞ
ð8:44Þ
where nN;ijðtÞ  Nð0; 1Þ and nC;ijðtÞ  Cð0; 1Þ.
The resulting distribution generates more very small and large mutations com-
pared to a Gaussian distribution. It has been proposed also an adaptive averaged
mutations operator
DxijðtÞ ¼ cijðtÞ cijð0; 1Þ þ tijðtÞNð0; 1Þ


ð8:45Þ
where cijðtÞ ¼ rijðtÞ—general scaling, tijðtÞ ¼ r1ij
r2ij—determines the form of distri-
bution probability function.
Thus the Cauchy distribution, thanks to the large tail, generates more large
mutations than the Gaussian distribution.
Levy distribution has tails between Gauss and Cauchy distributions. Thus, the
Cauchy distribution should be used when solutions-pretendents are far from the
optimum, and the Gauss—near the optimum for ﬁne-tuning. Distribution of Levi as
a combination provides a good balance between exploration and optimization.
Another factor that plays an important role in the balance between the study area
and the optimization of this process is a way strategic parameters calculated and
adjusted as these parameters directly affect the step size
Selection Operators
Selection operators are used for determination of those offsprings who had survived
and go to the next generation. In the initial work on EP and the majority of their
338
8
Genetic Algorithms and Evolutionary Programing

variations (develop) a new generation was selected from all the parents and their
offsprings, i.e. parents and offsprings are competing for survival. Unlike GA
competition is based on the relative ﬁtness index, but not on the absolute one.
Absolute measures FIs are real ﬁtness function, which estimate how closely is a
candidate to the optimal solution. On the other hand, the relative performance index
of ﬁtness determines how much better the individual is relatively (as compared with
a group of randomly selected applicants from parents and children).
In this presentation we use l to denote the number of parents and k—number of
offsprings.
The ﬁrst step in the process is to count index FI relative to each parent xiðtÞ and
descendant xiðtÞ.
Let
PðtÞ ¼ CðtÞ [ C00ðtÞ
ð
Þ
is
competing
pool
and
let
uiðtÞ 2 PðtÞ;
i ¼
1; . . .; l þ k be an individual in a competitive game. Then for each uiðtÞ 2 PðtÞ a
group of competing np individuals randomly selected from PðtÞnuiðtÞ. For each
uiðtÞ is calculated index
SiðtÞ ¼
Xnp
l¼1 sieðtÞ
ð8:46Þ
where sieðtÞ ¼
1; if f ðuiðtÞÞ\f ðueððtÞÞ
0; otherwise

ð8:47Þ
The alternative strategy was suggested in which [3]
sieðtÞ ¼
1; if r1\
f ðulðtÞÞ
f ðuiðtÞÞ þ f ðulððtÞÞ
0; otherwise
8
<
:
ð8:48Þ
where np is chosen as np ¼ 2lr2 þ 1
½
; r1; r2  Uð0; 1Þ.
In this case f uiðtÞ\\f ulðtÞ
ð
Þ
ð
Þ, this will be if index FI for uiðtÞ is much better
than for ulðtÞ then for individual uiðtÞ is assigned greater probability of getting
winning score 1.
On the basis of the index Si assigned to each individual uiðtÞ any of the methods
of selection can be used.
Elitism: l best individuals from the combined population PðtÞ move to the next
population Cðt þ 1Þ.
Selection: M best individuals are stochastically selected from the tournament.
The proportional selection: Each individual is attributed to the likelihood of
being selected
PsðuiðtÞÞ ¼
siðtÞ
Pl þ k
l1 slðtÞ
ð8:49Þ
Roulette method can also be used to select l individuals for the next generation.
Various methods have been proposed for the solution which of the parents or
descendants will move to the next generation. In particular, it was suggested that the
8.7
Evolutionary Programing
339

number of offsprings generated by each parent is determined by its FI. The higher is
the FI of parent, more offsprings it produces. The best of the descendants generated
by parent is chosen and competes with the parent for survival.
Competition between parent and descendant is based on the annealing method
[3]. The descendant x0
lðtÞ survives and proceeds to the next generation,
if f x0
lðtÞ


\f xlðtÞ
ð
Þ; or if eðf x0
lðtÞ
ð
Þf ðxilðtÞÞÞ=sðtÞ [ Uð0; 1Þ
ð8:50Þ
otherwise parent survives, where sðtÞ—so-called temperature coefﬁcient, wherein
sðtÞ ¼ csðt  1Þ; 0\c\1.
Strategic Parameters
As mentioned above, the step sizes are dependent on strategic parameters that form
part of EP mutation operators.
For further discussion it is assumed that each component has its own strategic
option and individuals are presented as
xiðtÞ ¼ xiðtÞ; riðtÞ
ð
Þ
Static Strategic Options
The simplest way to manage the strategic options is to use a record of their value. In
this case, the function of strategic options Uðr; XÞ- is linear, i.e.
U rijðtÞ


¼ rij
ð8:51Þ
The offsprings then are calculated as
x0
ijðtÞ ¼ xijðtÞ þ Nijð0; rijÞ
ð8:52Þ
where DxijðtÞ ¼ Nijð0; rijÞ.
The disadvantage of this approach is that too low rij limits search ranges and
slows convergence. On the other hand, too large a value for rij limits possibility of
ﬁne tuning solutions.
Dynamic Strategy
One of the ﬁrst approaches to change the values of the strategic options with the
time—is to link them with the values of the FI
cijðtÞ ¼ ciðtÞ ¼ cf ðxiðtÞÞ
ð8:53Þ
In this case offsprings are generated according to
x0
ijðtÞ ¼ XijðtÞ þ N 0; riðtÞ
ð
Þ ¼ xijðtÞ þ riðtÞNð0; 1Þ;
ð8:54Þ
where c 2 Nð0; 1Þ.
If you have information about the global optimum, individual error can be used
instead of the absolute error of the individual index FI. However, such information
340
8
Genetic Algorithms and Evolutionary Programing

is generally unavailable. As an alternative, it may be used phenotypic distance from
the best individual in accordance
r0
ijðtÞ ¼ riðtÞ ¼ f ð^yÞ  f ðxiÞ
j
j
ð8:55Þ
where ^y—is the most proper individual.
A distance in solutions space may be also used r0
ijðtÞ ¼ riðtÞ ¼ dð^y; xiÞ ¼
^y  xi
k
k, where dð; Þ—determines Euclid norm between vectors.
The advantage of this approach is that the worse is an individual, the more it
should be mutated. On the other hand, the greater (better) is the individual, the less
the child must be differed from the parent for ﬁne adjustment of good solutions.
This approach has several drawbacks:
1. If the value of FI is very large, the size of the steps will also be too large, which
may lead to the fact that the individual will jump over a good minimum.
2. The problem is further complicated if the optimal value of F—large
non-negative quantity. If the value FI from “good” individuals is large, it will
lead to large steps and the resulting offspring is removed from the good deci-
sions. In such cases, if the information about the optimal value of the function is
available, using the error value is more adequate.
Many proposals have been made to control the step size depending on the value
of the ﬁtness function. Fogel suggested an additive approach that
x0
ijðtÞ ¼ xijðtÞ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
bijðtÞf ðxiÞ þ cij
q
þ Nijð0; 1Þ
ð8:56Þ
where bij and cij are proportionality constant and displacement parameter.
For training of recurrent neural networks it has been proposed [3]
x0
ijðtÞ ¼ xijðtÞ þ brijNijð0; 1Þ;
ð8:57Þ
where b is proportionality coefﬁcient and
rij ¼ Uð0; 1Þ 1  f ðxiðtÞÞ
f ðxmaxðtÞÞ


;
ð8:58Þ
fmax—is maximal FI value for current population.
In this case, the problem of maximizing f is solved and f(xi(t)) takes on positive
values. It was suggested that the deviation be proportional to the normalized values
of the FI
x0
ijðtÞ ¼ xijðtÞ þ bijriNijð0; 1Þ;
ð8:59Þ
8.7
Evolutionary Programing
341

where b is proportionality coefﬁcient and deviation is calculated as
riðtÞ ¼
f ðxiðtÞÞ
Pl
l¼1 f ðxiðtÞÞ ;
ð8:60Þ
l is population size. In this approach it’s assumed that function f is minimized.
It was suggested that deviation value was proportional to distance from the best
individual [3]
rijðtÞ ¼ bij ^yj  xijðtÞ

 þ c;
ð8:61Þ
where c [ 0—is small value, and coefﬁcient bij is determined from condition
bij ¼
b
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d xmax;xmin
ð
Þ
p
p
, b 2 ½0; 2, d xmax; xmin
ð
Þ determines search space width as
Euclidian distance between xmin and xmax.
Adaptation (Self-organization) of EP
Two major problems concerning strategic parameters are the amount of noise
mutation to be added, and a step size of mutations. In order to provide a truly
self-organizing behavior, strategic parameters should be developed (or study) in
parallel with variables of EP, which use such a mechanism called self-learning For
the ﬁrst time the idea of self-learning and self-organization in relation to neural
networks was formulated by Ivakhnenko [18] and Rozenblatt [19]. As for the EP,
the ﬁrst proposals for the creation of a self- learning EP were made by Vogel et al.
authors in [20]. These methods can be divided into the following three categories:
1. Additive methods. The ﬁrst self-learning algorithm EP was proposed by Vogel
[16] is additive, in which
rijðt þ 1Þ ¼ rijðtÞ þ grijðtÞNijð0; 1Þ
ð8:62Þ
where g is called a learning speed (rate).
In the ﬁrst application of this approach g ¼ 1=6. If rijðtÞ  0, then rij ¼ c,where
c is small positive constant (usually c ¼ 0:001), that ensure non-zero deviations.
As an alternative Fogel suggested
rijðt þ 1Þ ¼ rijðtÞ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
fr rijðtÞ


q
þ Nijð0; 1Þ
ð8:63Þ
where fr rijðtÞ


¼
rijðtÞ; if rijðtÞ [ 0;
c; if rijðtÞ\0:

:
342
8
Genetic Algorithms and Evolutionary Programing

2. The multiplicative method. Janda and Wang proposed a multiplicative method in
which
rijðt þ 1Þ ¼ rð0Þ 
k1ek2 t
nt þ k3


ð8:64Þ
where k1; k2; k3—adjustable parameters and nt—maximum number of iterations.
1. Lognormal methods. They are taken from the literature on evolutionary strate-
gies [3]
rijðt þ 1Þ ¼ rijðtÞ es Nið0;1Þ þ s
0 N
0
ijð0;1Þ
ð8:65Þ
where s ¼
1ﬃﬃﬃﬃﬃﬃﬃ
2nx
p
; s0 ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
ﬃﬃﬃﬃﬃ
nx
p
p
;
nx—is vector x dimension.
Self-learning EP exhibit undesirable behavior of stagnation due to the tendency
that the strategy parameters converge quickly. Because of this deviation are quickly
becoming too small, thus limiting the search. Search fades for a while, until the
parameters of the strategy will not grow strongly enough, thanks to random variations.
Consider the most famous implementation of algorithms EP.
Classic EP
According to [3] Classic EP uses Gaussian mutations More speciﬁcally, the clas-
sical EP uses log-normal self-learning by the Eq. (8.65) and generates offspring,
using Eq. (8.5). For the construction of a new population from the current popu-
lation of the parent and the children the strategy of elitism is used.
Fast EP
A number of authors [21] adapted the classical EPO to create a fast EP (fast EP) by
replacing the Gaussian noise with mutations by Cauchy distribution, according to
the expression (8.37) where v ¼ 1. A descendant is generated using
x0
ijðtÞ ¼ xijðtÞ þ rijðtÞCijð0; 1Þ
ð8:66Þ
which uses self-lognormal self-learning.
Wider tails of Cauchy distribution produce larger step sizes, and as a result
accelerate the convergence. Analysis of fast EP showed that the size of the steps
may be too large for the appropriate functioning, whereas mutation of the Gaussian
distribution are the best opportunities for ﬁne-tuning decisions. This has led to the
development of a fast algorithm EP. In this algorithm, each parent produces two
children, one using a Gaussian mutation and the other—the Cauchy mutation. The
best descendant is chosen, who competes with their parents for survival.
An alternative approach is to start the search using mutation by Cauchy distri-
bution, and after approaching the optimum to switch to a Gaussian distribution.
However, this strategy creates the problem of determining the moment of switching.
8.7
Evolutionary Programing
343

Exponential EP
EP algorithm was proposed with a double exponential probability distribution [3].
Descendants are generated using the expression
x0
ijðtÞ ¼ xijðtÞ þ rij
1
n Eijð0; 1Þ;
ð8:67Þ
where rij is learning parameter, and the variation of the distribution is controlled by
parameter n. The less n, the greater the variation, and vice versa. To provide good
properties search at the beginning n must be initialized by small value, which
constantly increases over time.
Fast EP
In order to further improve the rate of convergence of the EP it was suggested to
accelerate EP, which uses variation operators [3]:
• Direction operator to determine the direction of the search based on the values of
ﬁtness-function;
• Gaussian mutation operator speciﬁed in (8. 5).
Individuals are presented as
xiðtÞ ¼ xiðtÞ; qiðtÞ; aiðtÞ
ð
Þ;
ð8:68Þ
where
qiðtÞ 2 1; 1
f
g
j ¼ 1; . . .; nx
determines
search
direction
for
each
component
jth individual, and ai contains age of individual.
Age is used to boost the broader research (search) if the descendants are worse
than their parents.
Generation of descendants occurs in 2 steps. At the ﬁrst step s the age of each
individual is adapted and the direction of the search is determined(assuming
minimization).
If the individual’s FI improved, the search continues in the direction of
improvement. If the FI has not improved, the age is incremented, which results in
an large step size, according to the following expression
aiðtÞ ¼
1; if f ðxiðtÞÞ\f ðxiðt  1ÞÞ
aiðt  1Þ þ 1; otherwise

;
ð8:69Þ
and
qijðtÞ ¼
sign ðxijðtÞ  xijðt  1ÞÞ; if f ðxiðtÞÞ\f ðxiðt  1ÞÞ
qijðt  1Þ; otherwise

ð8:70Þ
344
8
Genetic Algorithms and Evolutionary Programing

If aiðtÞ ¼ 1, then
riðtÞ ¼ c1f ðxiðtÞÞ
x0
ijðtÞ ¼ xijðtÞ þ qijðtÞNð0; riðtÞÞ
ð8:71Þ
Otherwise, if aiðtÞ [ 1, then
riðtÞ ¼ c2f ðxiðtÞÞ
x0
ijðtÞ ¼ xijðtÞ þ Nð0; riðtÞÞ
where c1,c2 are positive constants.
Selection procedure is done by a descendant of the competition directly from its
parent using the absolute index FI.
8.8
Differential Evolution
The positions of individuals in the search space provide valuable information about
the ﬁtness landscape. Under condition of the use of good random initialization
method with uniform distribution to generate a population, individuals give a good
presentation of the whole search space with a relatively large distance between
individuals. Over time, as search process proceeds the distance between the indi-
viduals becomes less and all individuals converge to the same decision. If distances
between individuals are large, the individuals have to make large steps to explore as
much as possible the search space.
On the other hand, if the distances between individuals are small, the step sizes
must also be small for study local areas. This behavior is achieved by so- called
differential evolution [22] when calculating the size of the step mutations as the
weighted distances between randomly selected individuals. Using the vector dif-
ference has many advantages.
Firstly, information about the ﬁtness landscape features presented by the current
population may be used to control the search.
Secondly, according to the central limit theorem, the step sizes mutation
approaches a Gaussian distribution, assuming that the population is sufﬁciently
large to provide a large number of difference vector.
Mutations
DE mutation operator makes a trial vector for each individual from the current
population by mutations in the target vector with a weighted difference. This trial
vector then will be used by the crossover operator to generate offspring. For each
parent xiðtÞ test vector uiðtÞ is generated as follows: [22].
Select target vector xi1ðtÞ from the population such that i 6¼ i1. Next, randomly
select two individuals xi2 and xi3 of the population, such that i 6¼ i1 6¼ i2 where
8.7
Evolutionary Programing
345

i2; i3  Uð1; uDÞ. Using these individuals, then the trial vector is calculated by the
perturbation of the target vector:
uiðtÞ ¼ xi1ðtÞ þ bðxi2ðtÞ  xi3ðtÞÞ
ð8:72Þ
where b 2 ½0; 1—a scaling factor, which regulates the value of differential
variation.
Crossover
The operator implements discrete crossover recombination of the trial vector uiðtÞ
and the parental vector xiðtÞ to generate a descendant xiðtÞ. Crossover is imple-
mented as follows
x0
ijðtÞ ¼
uijðtÞ; if j 2 I
xijðtÞ; otherwise

ð8:73Þ
where xijðtÞ denotes the jth element of the vector xiðtÞ and I—set of indices of
elements that should be subjected to changes (in other words, multipoint crossover).
To determine the set I may be used a variety of methods, of which the following
two approaches are used most frequently:
The binomial crossover. Crossover points are randomly selected from a set of
possible crossover points
1; 2; . . .; nx
f
g, where nx—dimension of the problem. In
ﬁnal form, this process is described in DE algorithm.
In this algorithm pr—the likelihood that considered crossover point will be
included. The larger the pr, the more crossover points will be included. This means
that more elements of the test vector are used to generate a descendant and the less
of the parent vector. To provide a difference of at least one element from the parent
multipoint crossover or initialization includes randomly selected point jx.
Exponential crossover. Starting from randomly selected index exponential
crossover operator selects a sequence of adjacent crossover points, considering the
list of potential crossover points as a circular array. By this at least one crossover
point is selected, and since this index the next is selected until such condition:
Uð0; 1Þ  pr or Ij j ¼ nx will be true.
Selection
Selection is used to determine which individuals take part in the mutation operation
to generate trial vector and which of the parents or the children go into the next
generation. Regarding the mutation operator a variety of methods of selection may
be used.
Random selection is generally used to select parents from which difference
vector calculated. For most algorithms DE target vector is selected or randomly, or
the best individual s selected. To construct a new generation of the population a
deterministic selection is used: descendant replaces parent if its FI is better than the
parent’s, otherwise parent goes to the next generation. This ensures that the average
FI of generation is not deteriorated.
346
8
Genetic Algorithms and Evolutionary Programing

Describe a general DE algorithm [3]
Set the counter of generation t ¼ 0.
Initialize control parameters b and pr;
Create and initialize a population of Cð0Þ of xs individuals
until another stop condition for each individual xiðtÞ 2 CðtÞ do
estimate FI f ðxiðtÞÞ;
Create a test vector uiðtÞ using a mutation operator;
create a descendant x0
iðtÞ, using a crossover operator;
if f ðx0
iðtÞÞ is better than f ðxiðtÞÞ, then,
add a descendant x0
iðtÞ 2 Cðt þ 1Þ;
end;
otherwise xiðtÞ 2 Cðt þ 1Þ;
end; end; end.
Choose the best individual.
The Control Parameters
In addition to a population size, the performance of DE is affected by two control
parameters: a scale factor b and the recombination probability pr. Consider the
effect of these parameters. The size of the population as indicated in the Eq. (8.64)
has a direct impact on the properties of DE algorithms. The more individuals in the
population, the more available the differential vectors and more directions can be
investigated. However, it should be born in mind that the more individuals are in
the population, the greater is the computational cost of one generation.
Empirical studies provide the following relation ns ¼ 10nx [3].
The nature of the mutations provides a lower bound on the number of indi-
viduals: ns [ nt þ 1, where nt—the number of used differentials (differences). For
nt differences it’s required 2nt different individuals, two for each difference.
The scaling factor. The scaling factor b 2 0; 1
ð
Þ adjusts the gain variation of
the difference ðxi2  xi1Þ. Less is b, the smaller is the mutation and the longer time
is required for convergence of the algorithm. Large values of b improve search
capabilities of the algorithm, but can cause skipping over a good optimum point.
Therefore the value of b must be small enough to allow to explore narrow valleys
using differences and on the other hand, large enough to provide the necessary
diversity.
The probability of recombination. The probability of recombination pr directly
affects the diversity of DE. This parameter controls the number of elements of the
parent xiðtÞ, which vary. The higher the probability of the recombination, the more
variability is introduced into a new population, thereby increasing the diversity of
properties and research. Increase in pr often leads to faster convergence, while
decreasing pr increases robustness search. Most implementations strategies of DE
accept control parameters constant. To ﬁnd the optimal parameters self-learning
strategies DE were developed [3].
8.8
Differential Evolution
347

References
1. Fraser, A.S.: Simulation of genetic systems by automatic digital computers I: introduction.
Aust. J. Biol. Sci. 10, 484–491 (1957)
2. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems. University of Michigan Press,
Ann Arbor (1975)
3. Engelbrecht, A.: Computational Intelligence. An Introduction, 2nd edn. John Wiley & Sons,
Ltd., pp. 630 (2007)
4. Bremermann, H.J.: Optimization through Evolution and Recombination. In: Yovits, M.C.,
Jacobi, G.T., Goldstine, G.D. (eds.) Self-organization Systems, pp. 93–106. Spartan Books
(1962)
5. Michalewicz, Z.: Genetic Algorithms + Data Structures = Evolutionary Programs, 3rd edn.
Springer, Berlin (1996)
6. Eshelman, L.J., Schaffer, J.D.: Real-Coded Genetic Algorithms and Interval Schemata. In:
Whitley, D. (ed.) Foundations of Genetic Algorithms, vol. 2, pp. 187–202. Morgan Kaufmann,
San Mateo (1993)
7. Deb, K., Agrawal, R.B.: Simulated binary crossover for continuous space. Complex Syst. 9,
115–148 (1995)
8. Yager, R.R., Filev, D.P.: Approximate clustering via the mountain method. IEEE Trans. Syst.
Man Cybern. 24, 1279–1284 (1994)
9. Deb, K., Joshi, D., Anand, A.: Real-coded evolutionary algorithms with parent-centric
recombination. In: Proceedings of the IEEE Congress on Evolutionary Computation, pp. 61–
66 (2002)
10. Hinterding, R.: Gaussian mutation and self-adaption for numeric genetic algorithms. In:
Proceedings of the International Conference on Evolutionary Computation, vol. 1, pp. 384
(1995)
11. Fogarty, T.C.: Varying the Probability of Mutation in the Genetic Algorithm. In: Schaffer, J.D.
(ed.) Proceedings of the Third International Conference on Genetic Algorithms, pp. 104–109.
Morgan Kaufmann, San Mateo, C.A. (1989)
12. Zaychenko, H.Y., Anikiev, A.S.: Efﬁciency of genetic algorithm application for MPLS
network structure synthesis. Vestnik of ChNTU №1.- Chercassy, pp. 176–182 (2008). (rus)
13. de Jong, K.A., Morrison, R.W.: A test problem generator for non-stationary environments. In:
Proceedings of the IEEE Congress on Evolutionary Computation, pp. 2047–2053 (1999)
14. Michalewicz, Z.: Genetic algorithms, numerical optimization, and constraints. In: Proceedings
of the 6th International Conference on Genetic Algorithms, pp. 1–158 (1995)
15. Zaychenko, H.Y., Zaychenko, Y.P.: MPLS Networks: Modeling, analysis and optimization.
Kiev. NTUU KPI, pp. 240 (2008). (rus)
16. Fogel, L.J., Owens, A., Walsh, M.: Artiﬁcial intelligence through simulated evolution. John
Wiley & Sons (1966)
17. Eberhart, R.C., Shi, Y.: Particle swarm optimization: developments, applications and
resources. In: Proceedings of the IEEE Congress on Evolutionary Computation, vol. 1,
pp. 27–30 (2001)
18. Ivakhnenko, A.G.: Self-learning systems of pattern recognition and automatic control.
Publishing House Technika, Kiev (1969). (rus)
19. Rosenblatt, F.: Principles of Neurodynamics. Perceptrons and Theory of Brain Mechanisms.
M.: Publishing house, Mir (1965). (rus)
20. Fogel, D.: Review of Computational intelligence: imitating life. IEEE Trans. Neural Netw. 6,
1562–1565 (1995)
21. Yuryevich, J., Wong, K.P.: Evolutionary programming based optimal power ﬂow algorithm.
IEEE Trans. Power Syst. 14(4), 1245–1250 (1999)
22. Storn, R., Price, K.: Differential evolution—a simple and efﬁcient heuristic for global
optimization over continuous spaces. J. Global Optim. 11(4), 341–359 (1997)
348
8
Genetic Algorithms and Evolutionary Programing

Chapter 9
Problem of Fuzzy Portfolio Optimization
Under Uncertainty and Its Solution
with Application of Computational
Intelligence Methods
9.1
Introduction
The problem of constructing an optimal portfolio of securities under uncertainty is
considered in this chapter. The main objective of portfolio investment is to improve
the investment environment, giving securities such investment characteristics that
are only possible in their combination. The global market crisis of recent years has
shown that the existing theory of investment portfolio optimization and forecasting
stock indices exhausted and revision of the basic theory of portfolio management is
needed. Therefore in this work the novel theory of investment portfolio optimiza-
tion under uncertainty is presented based on fuzzy set theory and efﬁcient fore-
casting methods.
The direct problem of fuzzy portfolio optimization and dual problem are con-
sidered. In the direct problem structure of a portfolio is determined which will
provide the maximum proﬁtableness at the given risk level (Sect. 9.3). In dual
problem structure of a portfolio is determined which will provide the minimum risk
level at the set level of critical proﬁtableness (Sect. 9.7). In the Sect. 9.3 fuzzy
portfolio model was constructed for triangular membership functions and algorithm
of its solution was considered. In the Sect. 9.4 fuzzy portfolio model was con-
structed for bell-shaped membership functions and in the Sect. 9.5 for Gaussian
membership functions.
In the Sect. 9.6 the experimental investigations of the suggested fuzzy portfolio
model were considered and comparison with classic Markovitz model was presented.
That to increase groundness of portfolio optimization and to decrease the possible
risk it was suggested to use forecasting share prices for portfolio model. The input
data for the optimization system are predicted by Fuzzy Group Method of Data
Handling (FGMDH) (Sect. 9.8). The comparative analysis of optimal portfolio
obtained by using different membership functions was fulﬁlled (Sect. 9.8). The
experimental investigations of the suggested theory were carried out and comparison
with classical portfolio model was performed (Sects. 9.7 and 9.8).
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9_9
349

The suggested portfolio optimization system is an effective tool for the opera-
tional management of portfolio investments.
The problem of investment in securities had arisen with appearance of the ﬁrst
stock markets. The main objective of portfolio investment is to improve the
investment environment, giving securities such investment characteristics that are
only possible in their combination. Careful processing and accounting of invest-
ment risks have become an integral and important part of the success of each
company. However, more and more companies have to make decisions under
uncertainty, which may lead to undesirable results. Particularly serious conse-
quences may have the wrong decisions at long-term investments. Therefore, early
detection, adequate and the most accurate assessment of risk is one of the biggest
problems of modern investment analysis.
Historically, the ﬁrst and the most common way to take account of uncertainty
was the use of probability theory. The beginning of modern investment theory was
put in the article H. Markowitz, “Portfolio Selection”, which was released in 1952.
In this article mathematical model of optimal portfolio of securities was ﬁrst pro-
posed. Methods of constructing such portfolios under certain conditions are based
on theoretical and probabilistic formalization of the concept proﬁtability and risk.
For many years the classical theory of Markowitz was the main theoretical tool for
optimal investment portfolio construction, after which most of the novel theories
were only modiﬁcations of the basic theory.
New approach in the problem of investment portfolio construction under
uncertainty is connected with fuzzy sets theory. Fuzzy sets theory was created about
half a century ago in the fundamental work of LotﬁZadeh [1, 2]. This theory came
into use in the economy in the late 70s. By using fuzzy numbers in the forecast
parameters decision-making person was not required to form probability estimates.
The application of fuzzy sets technique enabled to create a novel theory of fuzzy
portfolio optimization under uncertainty and risk deprived of drawbacks of classical
portfolio theory by Markovitz. The main source of uncertainty is changing stock
prices of securities at the stock market as the decision on portfolio is based on
current stock prices while the implementation of portfolio is performed in future
and portfolio proﬁtablenes depends on future prices which are unknown at the
moment of decision making. Therefore that to raise the reliability of decision
concerning portfolio and cut possible risk it’s needed to forecast future prices of
stocks. For this the application of inductive modelling method, so-called Fuzzy
Froup Method of Data Handling (FGMDH) seems to be very perspective. And
thanks to the results obtained using FGMDH system is devoid of subjective expert
risk. The main goals of this chapter are to review the main results in fuzzy portfolio
optimization theory, to consider and analyze so-called direct and dual problem of
optimization, to estimate the application of FGMDH for stock prices forecasting
while constructing investment portfolio and to carry out experimental investigations
for estimation of the efﬁciency of the elaborated theory.
350
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

9.2
Direct Problem of Fuzzy Portfolio Optimization
Problem Statement
The purpose of the analysis and optimization of an investment portfolio is a
research in the area of portfolio optimization, and also the comparative analysis of
structure of the effective portfolios received with the use of classical model of
Markovitz and fuzzy-set model of a investment portfolio optimization.
Let us consider a share portfolio of N components and its expected behavior at
time interval ½0; T. Each of a portfolio component i ¼ 1; N at the moment T is
characterized by it’s ﬁnancial proﬁtableness ri (evaluated at a point T as a relative
increase in the price of the asset for this period) [3]. The holder of a share portfolio
—the private investor, or an investment company—operates the investments, being
guided by certain reasons. On one hand, the investor tries to maximize the prof-
itableness. On the other hand, he ﬁxes maximum permissible risk of an inefﬁciency
of the investments.
Assume the capital of the investor be equal 1. The problem of share portfolio
optimization consists in a ﬁnding of a vector of share prices distribution in a
portfolio x ¼ fxigi ¼ 1; N maximizing the expected income at the set risk level.
In process of practical application of Markovitz model its drawbacks were
detected:
1. The hypothesis about normality of proﬁtableness distributions in practice does
not prove to be true.
2. Stationarity of price processes is not always conﬁrmed in practice.
3. At last, the risk of stocks is considered as a dispersion (standard deviation of the
prices from expected value) or its volatility, i.e. the decrease in proﬁtableness of
securities in relation to the expected value, and proﬁtableness increase are
estimated in this model absolutely the same. While for the proprietor of secu-
rities these events are absolutely different. These weaknesses of Markovitz
theory caused necessity of development essentially new approach for deﬁnition
of an optimum investment portfolio.
Let’s consider the main principles and ideas of a fuzzy method for portfolio
optimization [3–6].
The risk of a portfolio is not its volatility, but possibility that expected prof-
itableness of a portfolio will appear below some pre-established planned value.
• Correlation of stock prices in a portfolio is not considered and not taken into
account.
• Proﬁtableness of each security is not random, but fuzzy number. Similarly,
restriction on extremely low level of proﬁtableness can be both usual scalar and
fuzzy number of any kind. Therefore, to optimize a portfolio in such statement
may mean, in that speciﬁc case, the requirement to maximize expected prof-
itableness of a portfolio in time moment T at the ﬁxed risk level of a portfolio.
9.2
Direct Problem of Fuzzy Portfolio Optimization
351

• Proﬁtableness of a security on termination of ownership term is expected to be
equal r and lies in a settlement range.
For ith security denote:
ri—the expected proﬁtableness of the ith security;
r1i—the lower border of proﬁtableness of the ith security;
r2i—the upper border of proﬁtableness of the ith security.
ri ¼ ðr1i;ri; r2iÞ—proﬁtableness of the ith security is a triangular or Gaussian fuzzy
number.
Then proﬁtableness of a portfolio:
r ¼ ðrmin ¼
X
N
i¼1
xir1;r ¼
X
N
i¼1
xiri; rmax ¼
X
N
i¼1
xir2iÞ
ð9:1Þ
where xi is the weight of the ith security in a portfolio (its portion), and
X
N
i¼1
xi ¼ 1;
0  xi  1
ð9:2Þ
Critical level of proﬁtableness of a portfolio at the moment of T given by an
investor may be fuzzy triangular type number r ¼ ðr
1; r  r
2Þ or may be crisp value.
9.3
Fuzzy-Set Approach with Triangular Membership
Functions
To deﬁne structure of a portfolio which provides the maximum proﬁtableness at the
set risk level, it is required to solve the following problem [4]:
fxoptg ¼ fxgjr ! max; b ¼ const
ð9:3Þ
where r is a portfolio proﬁtableness, b is a desired risk, vector x satisﬁes (9.2).
Let us consider a risk estimation of portfolio investments. On Fig. 9.1 mem-
bership function r and criterion value r are shown.
Point with ordinate a1—the crossings point of two membership functions. Let us
choose any level of membership a and deﬁne corresponding intervals ½r1; r2 and
½r
1; r
2. At a [ a1, r1 [ r
2, intervals are not crossed, and the risk level equals to
zero. Level a1 is a top border of risk zone. At 0  a  a1 intervals are crossed.
Calculate the risk value b. For this calculate shaded area of the phase space Sa
(see Fig. 9.2).
352
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

Sa ¼
0; if r1  r
2
ðr
2r1Þ2
2
; if r
2 [ r1  r
1; r2  r
2
ðr
1r1Þ þ ðr
2r1Þ
2
 ðr
2  r
1Þ; if r1\r
1; r2 [ r
2
ðr
2  r
1Þðr2  r1Þ  ðr2r
1Þ2
2
; if r1\r
1  r2; r2\r
2
ðr
2  r
1Þðr2  r1Þ; if r2  r
1
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
:
ð9:4Þ
Since all realizations r; r
ð
Þ at set membership level u a
ð Þ are equally possible,
so the degree of inefﬁciencies risk u a
ð Þ is geometrical probability of event to drop
into any point r; r
ð
Þ in the zone of inefﬁcient distribution of the capital [4, 5]:
Fig. 9.1 Membership functions of r and r*
Fig. 9.2 Phase space r; r
ð
Þ
9.3
Fuzzy-Set Approach with Triangular Membership Functions
353

u a
ð Þ ¼
Sa
ðr
2  r
1Þðr2  r1Þ
Then total value of risk level of portfolio inefﬁciency equal to:
b ¼
Z a1
0
uðaÞ@a
ð9:5Þ
When the criterion of investor is deﬁned as crisp level r limiting transition at
r
2 ! r
1 ! r gives:
uðaÞ ¼
0; if r\r1
ðr  r1Þ
ðr2  r1Þ ; if r1  r  r2; a 2 0; 1
½

1; if r [ r2
8
>
>
>
<
>
>
>
:
:
ð9:6Þ
Then expected value risk degree of a portfolio is deﬁned so [4]:
b ¼
0; if r\rmin
R 1 þ 1a1
a1 lnð1  a1Þ


; if rmin  r ~r
1  ð1  RÞ 1 þ 1a1
a1 lnð1  a1Þ


; if ~r  r  rmax
1; if r  rmax
8
>
>
>
<
>
>
>
:
;
ð9:7Þ
where
R ¼
r  rmin
rmax  rmin
; if r\rmax
1;
if r  rmax
8
<
:
:
ð9:8Þ
a1 ¼
0;
if r\rmin
r  rmin
~r  rmin
;
if rmin  r\~r
1;
if r ¼ ~r
rmax  r
rmax  ~r ;
if ~r\r\rmax
0;
if r  rmax
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
Taking into account also that proﬁtableness of a portfolio is equal to:
r ¼ ðrmin ¼
X
N
i¼1
xir1i;r ¼
X
N
i¼1
xiri; rmax ¼
X
N
i¼1
xir2iÞ
354
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

where ðr1i;ri; r2iÞ is the proﬁtableness of ith security, we obtain the following direct
optimization problem (9.9)—(9.11) [4, 7]:
r ¼
X
N
i¼1
xiri ! max
ð9:9Þ
b ¼ const
ð9:10Þ
X
N
i¼1
xi ¼ 1; xi  0; i ¼ 1; N
ð9:11Þ
At a risk level variation b 3 cases are possible. Consider in detail each of them.
1. b ¼ 0
From (9.7) it is evident, that this case is possible when r\ P
N
i¼1
xir1i.
The following problem of linear programming is obtained:
r ¼
X
N
i¼1
xiri ! max
ð9:12Þ
X
N
i¼1
xir1i [ r;
ð9:13Þ
X
N
i¼1
xi ¼ 1; xi  0; i ¼ 1; N
ð9:14Þ
The solution of the problem (9.12)−(9.14)—vector x ¼ fxig i ¼ 1; N is a
required structure of the optimum portfolio for the given risk level.
2. b ¼ 1. This case practically is not accepted and is considered only for com-
pleteness of analysis. From (9.7) it follows, that this case is possible when
r  P
N
i¼1
xiri2.
Then we get the following problem
r ¼
X
N
i¼1
xiri ! max
X
N
i¼1
xir2i  r
9.3
Fuzzy-Set Approach with Triangular Membership Functions
355

X
N
i¼1
xi ¼ 1; xi  0; i ¼ 1; N
Found result of the problem (9.9)−(9.11) is a solution vector x ¼ fxig i ¼ 1; N
which determines a required structure of an optimum portfolio for the given risk
level.
3. 0\b\1
From (9.7) it is evident, that this case is possible when P
N
i¼1
xir1i  r  P
N
i¼1
xiri or
when P
N
i¼1
xiri  r  P
N
i¼1
xiri2.
• Assume P
N
i¼1
xiri1  r  P
N
i¼1
xiri. Then using (9.7)−(9.8) the problem (9.9)−(9.11)
is reduced to the following nonlinear programming problem:
r ¼
X
N
i¼1
xiri ! max
ð9:15Þ
1
P
N
i¼1
xiri2  P
N
i¼1
xiri1
ð r 
X
N
i¼1
xiri1
 
!
þ
X
N
i¼1
xiri  r
 
!

 ln
P
N
i¼1
xiri  r
P
N
i¼1
xiri  P
N
i¼1
xiri1
0
B
B
B
@
1
C
C
C
AÞ ¼ b
ð9:16Þ
X
N
i¼1
xiri1  r;
ð9:17Þ
X
N
i¼1
xiri [ r
ð9:18Þ
X
N
i¼1
xi ¼ 1; xi  0; i ¼ 1; N
ð9:19Þ
356
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

• Let P
N
i¼1
xiri  r  P
N
i¼1
xiri2. Then the problem (9.9)−(9.11) is reduced to the
following nonlinear programming problem:
r ¼
X
N
i¼1
xiri ! max
ð9:20Þ
r 
X
N
i¼1
xiri1
 
!

r 
X
N
i¼1
xiri
 
!
 ln
r  P
N
i¼1
xiri
P
N
i¼1
xiri2  P
N
i¼1
xiri1
0
B
B
B
@
1
C
C
C
A;

1
P
N
i¼1
xiri2  P
N
i¼1
xiri1
¼ b
ð9:21Þ
X
N
i¼1
xiri2 [ r;
ð9:22Þ
X
N
i¼1
xiri  r;
ð9:23Þ
X
N
i¼1
xi ¼ 1; xi  0; i ¼ 1; N:
ð9:24Þ
The R-algorithm of minimization of not differentiated functions is suggested to ﬁnd
the solution of problems (9.15)−(9.19) and (9.20)−(9.24). Let both problems: (9.15)
−(9.19) and (9.20)−(9.24) be solvable. Then to the structure of a required optimum
portfoliowillcorrespondsuchvector—x ¼ fxig i ¼ 1; N the solution that of problems
(9.15)−(9.19), (9.20)−(9.24) the criterion function value of which will be greater.
9.4
Fuzzy-Sets Approach with Bell-Shaped Membership
Functions
In case of using bell-shaped membership functions (MF) we should solve the
problem of portfolio optimization where parameter ri ¼ ðr1i;ri; r2iÞ, the proﬁtability
of the ith asset, is a fuzzy number with bell-shaped form lðxÞ ¼
1
1 þ
xa
c
ð
Þ
2, as shown
in Fig. 9.3.
9.3
Fuzzy-Set Approach with Triangular Membership Functions
357

In this case:
a1 ¼
1
1 þ
r  r
rmax  rmin

2
where rmin\r\rmax
0;
if rmin [ r and r [ rmax
1;
if r ¼ r
8
>
>
>
>
<
>
>
>
>
:
ð9:25Þ
b ¼
0; if r\rmin
1
2 a1 þ 1
2
r  r
rmax  rmin
 L; if rmin  r r
1 
1
2 a1  1
2
r  r
rmax  rmin
 LÞ


; if r  r\rmax
1; if r [ rmax
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
ð9:26Þ
where we can ﬁnd a1 from (9.25), L ¼ arcsin
ﬃﬃﬃﬃﬃ
a1
p



ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a1  a2
1
p


In order to determine the structure of the portfolio, which will provide maximum
return with the given level of risk, we need to solve the problem (9.3) where a1 and
b are determined from formulas (9.27), (9.28) [8].
b ¼
0;
if r  rmin
1
2 ða1  a0Þ þ 1
2
ðr  rÞ
ðrmax  rminÞ  L þ ðr  rminÞ
ðrmax  rminÞ  a0; if rmin  r r
1 
1
2 ða1  a0Þ  1
2
ðr  rÞ
ðrmax  rminÞ  L þ ðrmax  rÞ
ðrmax  rminÞ  a0


; if r\r  rmax
1;
if r [ rmax
8
>
>
>
>
>
<
>
>
>
>
>
:
ð9:27Þ
Fig. 9.3 Efﬁciency criterion
for the bell-shaped MF
358
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

where
a1 ¼
1
1 þ
r  rmin
rmax  rmin

2
where r1  r r
1
1 þ
rmax  r
rmax  rmin

2
wherer\r  r2
8
>
>
>
>
>
<
>
>
>
>
>
:
ð9:28Þ
L ¼ arcsin
ﬃﬃﬃﬃﬃ
a1
p
ð
Þ  arcsin
ﬃﬃﬃﬃﬃ
a0
p
ð
Þ 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a1  a2
1
q



ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a0  a2
0
q




ð9:29Þ
Optimization problem consists of tasks (9.3), (9.25)−(9.29).
9.5
Fuzzy-Sets Approach with Gaussian Membership
Functions
In case of using Gaussian MF we should solve the portfolio optimization problem
where parameter ri ¼ ðr1i;ri; r2iÞ, the proﬁtability of the ith stock is a fuzzy number
with Gaussian form lðxÞ ¼ e1
2
xa
c
ð
Þ
2
, as shown in Fig. 9.4.
In this case [8]:
a1 ¼
exp
 1
2
r  r
rmax  rmin

2
(
)
0;
if r\rmin and r [ rmax
1; if r ¼ r
8
>
>
>
<
>
>
>
:
ð9:30Þ
Fig. 9.4 Efﬁciency criterion
for the Gaussian MF
9.4
Fuzzy-Sets Approach with Bell-Shaped Membership Functions
359

b ¼
0;
if r  rmin
1
2 ða1  a0Þ þ
ﬃﬃﬃp
p
2
ﬃﬃﬃ
2
p R Uð
ﬃﬃﬃﬃﬃﬃﬃ
1
a0
Þ
r
 U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ln 1
a1
r




; if rmin  r r
1 
1
2 ða1  a0Þ 
ﬃﬃﬃp
p
2
ﬃﬃﬃ
2
p R Uð
ﬃﬃﬃﬃﬃ
1
a0
r
Þ  U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ln 1
a1
r






; if r\r  rmax
1;
if r [ rmax
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
ð9:31Þ
where a0 is very small, a1 we can ﬁnd from (9.30),
R is
R ¼
r  r
rmax  rmin
ð9:32Þ
UðxÞ is the Laplace function: UðxÞ ¼
2ﬃﬃp
p Rx
0
et2dt
In order to determine the structure of the portfolio, which will provide maximum
return with the given level of risk, we need to solve the problem (9.3) where b and
a1 are determined from formulas (9.30), (9.31).
b ¼
0;
if r  rmin
1
2 ða1  a0Þ þ
ﬃﬃﬃp
p
2
ﬃﬃﬃ
2
p R Uð
ﬃﬃﬃﬃﬃﬃﬃ
1
a0
Þ
r
 U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ln 1
a1
r




þ r  rmin
rmax  rmin
 a0; if rmin  r r
1 
1
2 ða1  a0Þ 
ﬃﬃﬃp
p
2
ﬃﬃﬃ
2
p R Uð
ﬃﬃﬃﬃﬃ
1
a0
r
Þ  U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ln 1
a1
r




þ
rmax r
rmax  rmin
 a0


; if r\r  rmax
1;
if r [ rmax
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
ð9:33Þ
where
a0 ¼
exp
 1
2
r  rmin
rmax  rmin

2
(
)
; where rmin  r  r
exp
 1
2
rmax  r
rmax  rmin

2
(
)
; where r\r  rmax
8
>
>
>
>
<
>
>
>
>
:
ð9:34Þ
Optimization problem consists of tasks (9.3), (9.30), (9.31)−(9.33).
9.6
The Analysis and Comparison of the Results Received
by Markovitz and Fuzzy-Sets Model
For the comparative analysis of investigated methods of a share portfolio optimi-
sation real data on share prices of the companies RAO » EES (EERS2) and
Gazprom (GASP), were taken from February, 2000 till May, 2006 [4, 7].
360
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

In Markovitz model expected proﬁtableness of a share is calculated as a mean
m ¼ Mfrg and risk of an asset is considered as a dispersion of the proﬁtableness
value r2 ¼ M
m  r
ð
Þ2
h
i
, i.e. level of variability of expected incomes.
In the fuzzy-sets model obtained from a situation at the share market we
conclude:
• shares proﬁtableness of EERS2 lies in a settlement corridor [−1.0: 3.9], the most
expected value of proﬁtableness is 2.1 %
• shares proﬁtableness of GASP lies in a settlement corridor [−4.1: 5.7], the most
expected value of proﬁtableness is 4.8 %
Let critical proﬁtableness of a portfolio be 3.5 % i.e. portfolio investments which
bring the income below 3.5 %, are considered as the inefﬁcient.
Expected proﬁtableness of the optimum portfolios received by Markovitz model,
is higher, than proﬁtableness of optimum portfolios, received by means of fuzzy-set
model because in Markovitz model the calculation of expected share proﬁtableness
is based on indicators for the preceding periods and the situation in the share market
at the moment of decision-making by the investor is not accounted. As prof-
itableness of shares EERS2 and GASP till July 2006 was much more higher than at
the present moment, Markovitz model gives unfairly high estimate.
In the fuzzy-set model the proﬁtableness of each asset is a fuzzy number. Its
expected value is calculated not from statistical data, but from analysis of the
market at the moment of decision-making by the investor. Thus, in the considered
case, the expected proﬁtableness of a portfolio is not too high.
The structures of an optimum portfolio which are obtained by both methods for
the same risk levels are quite different too. To understand the reason of this consider
following dependences obtained for both models (Fig. 9.5) [3, 4].
Dependence of expected proﬁtableness on risk degree of the portfolio is pre-
sented on Fig. 9.5.
Dependences of expected proﬁtableness on degree of risk of the portfolio,
received by the above speciﬁed methods, are practically opposite. The reason of
such result is the various understanding of a portfolio risk.
Fig. 9.5 Dependence of the expected proﬁtableness on risk degree for considered models. a By
fuzzy-set method b by Markovitz model
9.6
The Analysis and Comparison of the Results …
361

In the fuzzy-set method the risk is understood as a situation when expected prof-
itableness of a portfolio falls below the set critical level, so with decrease of expected
proﬁtableness risk of portfolio investments to be less than the critical value, increases.
In Markovitz model the risk is considered as the degree of expected income
variability of a portfolio, in both cases of smaller and greater income that contra-
dicts common sense. The various understanding of portfolio risk level is also the
reason of difference of a portfolio structure, received by different methods.
For share EERS2, with the growth of portion of low proﬁtable securities in a
portfolio, even in spite of the fact that the settlement corridor for EERS2 is nar-
rower, rather than a settlement corridor for GASP, expected proﬁtableness of a
portfolio in general falls and the risk of an inefﬁciency portfolio grows.
Level of volatility of expected incomes for shares EERS2 obtained from data
2000−2006 is much more lower, than for shares GASP. Therefore, in Markovitz
model which consider it as risk of portfolio investments, with the increase of
portion of share EERS2 the risk of a portfolio decreases.
From the point of view of the fuzzy-set approach, the greater is the portion of
GASP shares in a portfolio, the less is the risk of that efﬁciency of share investments
will appear below the critical level which is in our case 3.5 %.
From the point of view of Markovitz model, average mean deviation from
average value for GASP shares is great enough, therefore with growth of this share
the risk of a portfolio increases. It leads to that portion of highly proﬁtable assets in
the share portfolio received by Markovitz model, is unfairly small.
According to Markovitz model, thanks to correlation between assets it is possible
to receive a portfolio with a risk level less than volatility of the least risk security.
In this task after investing 96 % of the capital in EERS2 shares and 4 % in
GASP shares, the investor received portfolio with expected proﬁtableness of 2.4 %
and degree of risk 0.19. However investments with expected proﬁtableness of
2.4 % in the fuzzy-set model are considered as the inefﬁcient. If to set critical value
of expected portfolio proﬁtableness equal to 2.4 % the risk of inefﬁcient invest-
ments will decrease, too.
9.7
The Dual Portfolio Optimization Problem
9.7.1
Problem Statement
The initial portfolio optimization problem which is naturally to be called as direct
has the following form [8]: optimize the expected portfolio proﬁtableness
r ¼
X
N
i¼1
rlxi ! max
ð9:35Þ
362
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

under constraints on risk
0  b  1
ð9:36Þ
X
N
i¼1
xi ¼ 1;
xi  0
ð9:37Þ
Let’s consider the case when the criterion value r* satisﬁes the conditions
X
N
i¼1
xiri1  r 
X
N
i¼1
rlxi ¼ r
ð9:38Þ
Then the risk value is equal
bðxÞ ¼
1
P
N
i¼1
xiri2  P
N
i¼1
xiri1
½ r 
X
N
i¼1
xiri1
 
!
þ
X
N
i¼1
xirl  r
 
!

 ln
P
N
i¼1
xirl  r
P
N
i¼1
xirl  P
N
i¼1
xiri1
0
B
B
B
@
1
C
C
C
A
ð9:39Þ
Nowconsidertheportfoliooptimizationproblemdualtotheproblem(9.35)−(9.37).
To minimize
bðxÞ
ð9:40Þ
under conditions
r ¼
X
N
i¼1
xirl  rgiven ¼ r
ð9:41Þ
X
N
i¼1
xi ¼ 1;
xi  0
ð9:42Þ
9.7.2
Optimality Conditions for Dual Fuzzy Portfolio
Problem
As it was proved in [9] function bðxÞ is convex thefore the dual portfolio problem
(9.40)−(9.42) is a convex programming problem. Taking into account that con-
straints (9.38) are linear compose Lagrangian function:
9.7
The Dual Portfolio Optimization Problem
363

Lðx; k; lÞ ¼ bðxÞ þ kðr 
X
N
i¼1
xirlÞ þ l
X
N
i¼1
xi  1
 
!
ð9:43Þ
The optimality conditions by Kuhn-Tucker are such [3, 8]
@L
@xi
¼ @bðxÞ
@xi
 krl þ l  0;
1  i  N;
ð9:44Þ
@L
@k ¼ 
X
N
i¼1
xirl þ r  0; @L
@l ¼
X
N
i¼1
xi  1 ¼ 0
ð9:45Þ
and conditions of complementary slackness
@L
@xi
xi ¼ 0; 1  i  N;
@L
@k k ¼ k 
X
N
i¼1
xirl þ r
 
!
¼ 0; xi  0; k  0
ð9:46Þ
where k  0 and l are indeﬁnite Lagrange multpliers.
This problem may be solved by standard methods of convex programming, for
example method of feasible directions or method of penalty functions.
9.8
The Application of FGMDH for Stock Prices
Forecasting in the Fuzzy Portfolio Optimization
Problem
The proﬁtableness of leading companies at NYSE in the period from 03.09.2013 to
17.01.2014 were used as the input data in experimental investigations. The com-
panies included: Canon Inc. (CAJ), McDonald’s Corporation (MCD), PepsiCo, Inc
(PEP), The Procter & Gamble Company (PG), SAP AG (SAP). The corresponding
data is presented in the Table 9.1 [8]:
For forecasting we have used the Fuzzy GMDH method with triangular mem-
bership functions, linear partial descriptions, training sample of 70 %, forecasting
for 1 step.
The fuzzy GMDH allows to construct forecasting model using experimental data
automatically without participation of an expert. Besides, it may work under
uncertainty conditions with fuzzy input data or data given as intervals.
This method is considered and explored in the Chap. 6. The next proﬁtableness
values on date 17.01.2014 were obtained (Table 9.2):
364
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

Thus, as the result of application of FGMDH the shares proﬁtableness values
were forecasted to the end of 20th week (17.01.2014) as follows [8]:
• Proﬁtableness of CAJ shares lies in the calculated corridor [−1.484; −1.008], the
expected value is—1.246 %;
• Proﬁtableness of MCD shares lies in the calculated corridor [−0.347; 0.111], the
expected value is—0.118 %;
Table 9.1 The proﬁtableness of shares, %
Companies
CAJ
MCD
PEP
PG
SAP
Dates
06.09.13
0.510
−1.841
1.172
0.772
2.692
13.09.13
−0.880
−0.933
−1.184
−1.139
−0.945
20.09.13
0.990
0.829
−0.889
0.961
−1.509
27.09.13
1.110
0.164
1.012
2.611
−0.582
04.10.13
0.060
1.569
−0.151
−0.569
0.649
11.10.13
−0.410
−0.403
−2.239
−3.741
−3.190
18.10.13
−1.230
−0.507
−2.368
−0.851
−0.477
25.10.13
2.300
−0.201
−1.190
−1.304
−3.979
01.11.13
2.620
−1.961
0.059
0.185
2.494
08.11.13
0.190
0.308
−1.754
−1.451
−0.879
15.11.13
−3.600
0.175
−0.679
−3.136
−1.675
22.11.13
−0.180
−0.635
0.140
−0.449
−0.329
29.11.13
−1.830
1.567
1.066
1.393
−0.510
06.12.13
1.950
−0.300
0.657
−1.416
−0.636
13.12.13
1.650
1.337
2.128
2.843
1.566
20.12.13
−0.060
−1.111
−1.000
−0.184
−2.247
27.12.13
1.560
−0.633
−1.038
−0.861
−1.132
03.01.14
0.880
0.484
0.808
1.890
2.655
10.01.14
1.770
0.052
−1.483
0.422
1.042
17.01.14
−1.270
−0.105
0.206
0.162
0.843
Table 9.2 The proﬁtableness of shares on date 17.01.2014, %
Companies
Proﬁtableness
MAPE test
sample
MSE test
sample
Real
value
Low
bound
Forecasted
value
Upper
bound
CAJ
−1.270
−1.484
−1.246
−1.008
2.2068
0.0295
MCD
−0.105
−0.347
−0.118
0.111
2.5943
0.0091
PEP
0.206
0.001
0.242
0.483
3.0179
0.0177
PG
0.162
0.041
0.170
0.299
1.6251
0.0197
SAP
0.843
0.675
0.867
1.059
2.3065
0.0164
9.8
The Application of FGMDH for Stock Prices Forecasting…
365

• Proﬁtableness of PEP shares lies in the calculated corridor [0.001; 0.483], the
expected value is 0.242 %;
• Proﬁtableness of PG shares lies in the calculated corridor [0.041; 0.299], the
expected value is 0.17 %;
• Proﬁtableness of SAP shares lies in the calculated corridor [0.675; 1.059], the
expected value is 0.867 %.
In this way the portfolio optimization system stops to be dependent on factor of
expert subjectivity. Besides, we can get data for this method automatically, without
expert’s estimates.
Let the critical proﬁtableness level be 0.7 %. Varying the risk level we obtain the
following results at the end of 2nd week (17.01.2014) for triangular MF. The results
are presented in the Tables 9.3, 9.4 and the Fig. 9.6.
Table 9.3 Distribution of components of the optimal portfolio for triangular MF with critical
level r ¼ 0:7 %
CAJ
MCD
PEP
PG
SAP
0.05482
0.00196
0.0027
0.00234
0.93818
0.06145
0.00113
0.00606
0.0039
0.92746
0.0698
0.00577
0.00235
0.00219
0.91989
0.06871
0.00228
0.0057
0.00244
0.92087
0.07567
0.00569
0.00106
0.00094
0.91664
0.07553
0.00002
0.0029
0.00208
0.91947
0.06774
0.00121
0.006
0.00234
0.92271
0.0764
0.001
0.00612
0.00464
0.91184
0.09072
0.00849
0.00655
0.0039
0.89034
Table 9.4 Parameters of the optimal portfolio for triangular MF with critical level r ¼ 0:7 %
Low bound
Expected proﬁtableness
Upper bound
Risk level
0.55133
0.74591
0.94049
0.2
0.53462
0.72954
0.92446
0.25
0.51544
0.71084
0.90624
0.3
0.51894
0.71431
0.90968
0.35
0.5045
0.70018
0.89587
0.4
0.50877
0.70425
0.89973
0.45
0.522
0.71731
0.91262
0.5
0.50197
0.69752
0.89308
0.55
0.46358
0.66014
0.8567
0.6
366
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

As we can see in Fig. 9.6, the dependence proﬁtableness—risk has descending
type, the greater is the risk the lesser is proﬁtableness which is opposite to classical
probabilistic method. It may be explained so that at fuzzy approach by risk is meant
Table 9.5 Distribution of components of the optimal portfolio for bell-shaped MF with critical
level r ¼ 0:7 %
CAJ
MCD
PEP
PG
SAP
0.00129
0.0026
0.00279
0.00316
0.99016
0.00264
0.00249
0.0028
0.0037
0.98837
0.00249
0.00268
0.00246
0.00238
0.98999
0.00209
0.00148
0.00039
0.00024
0.9958
0.00102
0.00105
0.0021
0.00222
0.99361
0.00425
0.00325
0.00343
0.0032
0.98587
0.00128
0.00125
0.00207
0.002
0.9934
0.00084
0.0022
0.0018
0.00163
0.99353
0.00165
0.00105
0.00198
0.00137
0.99395
Table 9.6 Parameters of the optimal portfolio for bell-shaped MF with critical level r ¼ 0; 7 %
Low bound
Expected proﬁtableness
Upper bound
Risk level
0.66568
0.85777
1.04986
0.2
0.66252
0.85464
1.04676
0.25
0.66372
0.8559
1.04808
0.3
0.6612
0.85335
1.04551
0.35
0.6594
0.85145
1.0435
0.4
0.65816
0.85044
1.04272
0.45
0.63574
0.82782
1.0199
0.5
0.62921
0.82131
1.01341
0.55
0.59079
0.78291
0.97504
0.6
Fig. 9.6 Dependence of the expected portfolio proﬁtableness versus risk level for triangular MF
9.8
The Application of FGMDH for Stock Prices Forecasting…
367

the situation when the expected proﬁtableness happens to be less than the given
criteria level. When the expected proﬁtableness decreases, the risk grows.
The proﬁtableness of the real portfolio is 0.7056 %. This value falls in calcu-
lated corridor of proﬁtableness for optimal portfolio [0.5346, 0.7295, 0.9245] built
with application of forecasting, indicating the high accuracy of the forecast.
Now consider the same portfolio using bell-shaped MF (Tables 9.5 and 9.6 and
Fig. 9.7).
The proﬁtableness of the real portfolio is 0.8339 %. This value also falls in the
calculated corridor of proﬁtableness for optimal portfolio [0.6657; 0.8578; 1.0499]
[8].
Now consider the same portfolio using Gaussian MF (results are presented in
Tables 9.7 and 9.8 and Fig. 9.8).
The proﬁtableness of the real portfolio is 0.8316 %. This value also falls in
calculated corridor of proﬁtableness for optimal portfolio [0.6833; 0.8756; 1.0677].
In the above presented results the optimal portfolio corresponds to the ﬁrst row
of tables. As it can be seen from these data, the proﬁtableness obtained using
Fig. 9.7 Dependence of expected portfolio proﬁtableness versus risk for bell-shaped MF
Table 9.7 Distribution of components of the optimal portfolio for Gaussian MF with critical level
r ¼ 0:7 %
CAJ
MCD
PEP
PG
SAP
0.0028
0.00277
0.00221
0.0021
0.99012
0.0009
0.00126
0.00153
0.00162
0.99469
0.00028
0.00189
0.00232
0.00213
0.99338
0.00193
0.00243
0.00284
0.00278
0.99002
0.00144
0.00096
0.00088
0.00138
0.99534
0.00083
0.001
0.00225
0.00144
0.99448
0.00223
0.0024
0.003
0.00209
0.99028
0.0013
0.00124
0.00129
0.0019
0.99427
0.00261
0.00191
0.00204
0.00239
0.99105
368
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

Gaussian and bell-shaped MF is higher than the proﬁtableness obtained using
triangular MF. The reason is the form of used MF curves. The bell-shaped and
Gaussian function are more convex, so an area of inefﬁcient assets is bigger, and the
risk of getting into this area is higher.
The optimal portfolios obtained with different MF actually have the same
structure, the main portion belongs to the company SAP shares, due to high rates of
return as compared with other companies.
Let’s consider the results obtained by solving the dual problem using triangular
MF. In this case, the investor sets the rate of return and the problem is to minimize
the risk.
The optimal portfolio is presented in Tables 9.9 and 9.10, and Fig. 9.9:
From these results one may see that the curve “dependence risk—given critical
level of proﬁtability” has a ascending character, because with the growth of the
critical value of proﬁtability increases the probability that the expected return would
be lower than a given critical value [8].
Fig. 9.8 Dependence of expected portfolio proﬁtableness versus risk for Gaussian MF
Table 9.8 Parameters of the optimal portfolio for Gaussian MF with critical level r ¼ 0:7 %
Low bound
Expected proﬁtableness
Upper bound
Risk level
0.6833
0.87551
1.06772
0.2
0.66972
0.86178
1.05384
0.25
0.66955
0.86161
1.05368
0.3
0.66468
0.85682
1.04896
0.35
0.64944
0.8415
1.03356
0.4
0.65975
0.85185
1.04394
0.45
0.63439
0.8266
1.0188
0.5
0.63184
0.82389
1.01594
0.55
0.62452
0.81666
1.0088
0.6
9.8
The Application of FGMDH for Stock Prices Forecasting…
369

Table 9.10 Parameters of the optimal portfolio (dual task)
Low
bound
Expected
proﬁtableness
Upper
bound
Risk
level
Critical rate of return
0.58944
0.78264
0.97584
0.00025
0.6
0.59846
0.79141
0.98437
0.01468
0.65
0.61478
0.80735
0.99991
0.04973
0.7
0.6229
0.81531
1.00772
0.13347
0.75
0.63606
0.82822
1.02037
0.26399
0.8
0.64945
0.84181
1.03417
0.49937
0.85
0.63712
0.82933
1.02153
0.72631
0.86
0.63382
0.82612
1.01843
0.8333
0.87
0.62559
0.81805
1.01052
0.91214
0.88
Fig. 9.9 Dependence of the risk level on a given critical return
Table 9.9 Distribution of
components of the optimal
portfolio (dual task)
CAJ
MCD
PEP
PG
SAP
0.01627
0.02083
0.02226
0.02231
0.91833
0.01112
0.02085
0.02391
0.02383
0.92029
0.00333
0.01992
0.02517
0.02476
0.92682
0.0021
0.01579
0.02457
0.02344
0.9341
0.00004
0.00921
0.02423
0.02135
0.94517
0.00224
0.00144
0.01825
0.01095
0.96712
0.00044
0.00682
0.02508
0.02058
0.94708
0.0011
0.00917
0.02448
0.02039
0.94486
0.00294
0.01206
0.02533
0.02154
0.93813
370
9
Problem of Fuzzy Portfolio Optimization Under Uncertainty …

9.9
Conclusion
The problem of optimization of the investment portfolio under uncertainty is
considered in this chapter. The fuzzy-set approach for solving the direct and dual
portfolio optimization problems was suggested and explored. In the direct problem
triangular, bell-shaped and Gaussian membership functions were used. The results
of solving the problems were presented. The optimal portfolios for the ﬁve assets at
NYSE stock market were constructed and analyzed.
The problem of stock prices forecasting for portfolio optimization was also
investigated. The fuzzy GMDH was proposed for its solution and its experimental
investigations are presented. The fuzzy GMDH was applied for stocks proﬁtable-
ness forecasting at NYSE stock market in the problem of fuzzy portfolio opti-
mization. The application of fuzzy GMDH enabled to decrease risk of the wrong
decisions concerning portfolio content.
After
analysis
of
experiments
it
was
detected
that
the
dependence
“proﬁtableness-risk” has descending type, the greater risk the lesser is proﬁtable-
ness that is opposite to classical probabilistic methods.
The dependence “risk versus given critical level of proﬁtability” has ascending
type, because with the growth of the critical level of proﬁtability increases the
probability that the expected return appears to be lower than a given critical value.
As the main result of this research the fundamentals of theory of fuzzy portfolio
optimization under uncertainty have been developed.
References
1. Zadeh, L.A.: Fuzzy sets. Inf. Control 8(3), 338–353 (1965)
2. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. N1, 3–28 (1978)
3. Zaychenko, Y.P.: Fuzzy models and methods in intellectual systems, 354 p. Publ. House, Slovo,
Kiev (2008). Zaychenko, Y.: Fuzzy group method of data handling under fuzzy input data. In:
System research and information technologies, №. 3, pp. 100–112 (2007) (in Russ.)
4. Zaychenko, Y.P., Maliheh Esfandiyarifard. Analysis and comparison results of invest portfolio
optimization using Markovitz model and fuzzy sets method. In: XIII-th International
Conference KDS-2007, vol. 1, pp. 278–286, Soﬁa (2007). (in Russ.)
5. Nedosekin, A.O.: System of portfolio optimization of Siemens services. Banking Technol. №5
(2003). (in Russ.) http://www.ﬁnansy.ru/publ/ﬁn/004.htm
6. Nedosekin, A.O.: Corporation business portfolio optimization (in Russ.). Ayдит caйтe: http://
sedok.narod.ru/s_ﬁles/2003/Art_070303.doc
7. Zaychenko, Y.P.: Maliheh Esfandiyarifard. Analysis of invest portfolio for different member-
ship functions. Syst. Res. Inf. Technol. 1(2) 59–76 (2008). (in Russ.)
8. Zaychenko, Y.: Inna Sydoruk. Direct and dual problem of investment portfolio optimization
under uncertainty. Int. J. Inf. Technol. Knowl. 8(3), 225–242 (2014)
9. Zgurovsky, M.Z., Zaychenko, Y.P.: Models and methods of decision-making under uncertainty,
275 c. Publ. House, Naukova Dumnka, Kiev (2011). (in Russ.)
9.9
Conclusion
371

Index
A
Algorithms of fuzzy neural networks learning
Gulvin-Ramadji Kaynes, 114
Kachmarzh, 114, 115
Asynchronous correction, 41
B
Bankruptcy risk forecasting
Altman model, 142, 152
ARMA model, 175
Byelorussian bank association method, 161,
168, 169
Davidova—Belikov’s model, 143
FNN ANFIS, 140, 162, 163, 170, 173, 177
FNN TSK, 133, 162, 164, 165, 167, 177
fuzzy neural networks, 152, 161, 176
fuzzy sets (matrix) method, 144, 149
GMDH, 173
Kromonov’s method, 165, 166, 168, 169
linear regression, 175
logit-models, 175, 177
Mamdani, 149, 152, 155, 159
Nedosekin’s matrix method, 144, 152, 154,
157
probit-models, 175–177
regression models, 174, 177
Tsukamoto, 134, 138, 149, 160
C
Cluster analysis algorithms
agglomerative, 265, 267
algorithms of fuzzy inference, 81, 84
defuzziﬁcation, 81, 83, 85, 88, 90, 93, 96,
100
differential grouping, 261, 262, 273, 274,
286
divisional, 265, 267
fuzziﬁcation, 82, 105
fuzzy C-means, 261, 262, 271, 275, 276
Gustavson-Kessel, 261, 262, 275, 285, 286,
306
K-means, 268, 270
Larsen, 81, 87, 88
Mamdani, 81, 85, 87, 91, 96
metrics, 261–264, 268, 275
not hierarchical, 264, 267
peak grouping, 273, 274, 286
possibilistic, 261, 276, 277, 282
recurrent fuzzy, 277, 281
robust, 261, 262, 279, 280, 282
Sugeno, 81, 86, 99
Tsukamoto, 85, 96
Ward’s method, 261, 267
Competitive learning, 39, 61, 62, 66, 70
Conjugate gradients (CG) method, 13
Credit risk, 141
Criterion
Hi-Beni, 304, 306
non-fuzziness, 262, 263
quality of cluster analysis, 262, 264, 273,
275
Cross-associations effect, 50, 59
Cross-over
asexual, 310
blend, 312, 313
center of mass, 314, 315
diagonal, 317
double point, 311
exponential, 346
multi-point, 312–314, 346
multi-recombination, 310
one point, 310, 311, 314
sexual, 310
uniform, 311
D
Defuzziﬁcation, 88, 89, 91, 93, 94
Differential evolution
© Springer International Publishing Switzerland 2016
M.Z. Zgurovsky and Y.P. Zaychenko, The Fundamentals of Computational
Intelligence: System Approach, Studies in Computational Intelligence 652,
DOI 10.1007/978-3-319-35162-9
373

algorithms modiﬁcations
cross-over, 309
general, 309, 310, 330
mutations, 309, 317, 335, 345
E
EP algorithms
Cauchy, 337
Chaotic, 338
combined, 338
distribution, 336–338
dynamic EP, 336
elitism strategy, 328
exponential, 337, 344
Gaussian, 336
Levi, 337
mutation operator, 309, 310, 318, 319, 324,
335
proportional selection, 310, 311, 339
Roulette method, 339
selection operator, 310, 328, 338
self-learning, 336, 342, 343
Evolutionary algorithm, 334, 336
Evolutionary programming (EP), 309, 334,
335, 342
F
Forecasting in macroeconomy and ﬁnancial
sphere
fuzzy GMDH, 167, 173, 174
fuzzy neural networks, 135, 141, 161
GMDH, 170, 173
neural-fuzzy networks, 134, 135, 139, 144,
159, 170
Fuzziﬁcation, 82, 87, 107
Fuzzy approximation theorem (FAT-theorem),
81
Fuzzy GMDH
algorithm, 124
Chebyshev’s orthogonal polynomials, 232
fourier polynomials, 251
fuzzy models coefﬁcients, 237
linear interval model, 226, 227, 229, 234,
236, 237, 240, 252
model
with bell-shaped MF, 100
with Gaussian MF, 81, 96
recurrent LSM for model coefﬁcients, 237,
240
stochastic approximation method, 238
trigonometric polynomials, 234, 235
with fuzzy inputs, 221, 222, 257, 260
Fuzzy neural network
adaptive wavelet-neuro-fuzzy, 112
ANFIS, 81, 99, 100, 104, 117
Mamdani, 85, 91
Neo-fuzzy-cascade, 81, 117, 119, 120, 124
TSK, 81, 104–106, 108, 110
Tsukamoto, 81, 96, 98
Wang-Mendel, 104, 107, 108
Fuzzy neural network for classiﬁcation
algorithm for rule base learning, 205
algorithm of membership functions
learning, 179, 185, 200, 211
classical, 200, 204
FNN NEFClass, 179, 192, 204, 210, 218
modiﬁed NEFClass, 186
G
Genetic algorithms
adaptive, 320, 322, 324
canonical, 310
cross-over, 309–311
deterministic, 320, 321
dynamic, 319, 325
for neural network, 16
island, 326, 328
selection, 309–311, 319, 325, 327, 328,
335, 338, 346
self-adjusting, 319
self-learning parameters, 336, 342
VEGA, 329
Group method of data handling (GMDH)
algorithm of GMDH, 81, 124, 125
and FGMDH, experimental investigations
of, 241, 245, 256, 260
drawbacks of, 225
FGMDH model with fuzzy input data, 252
Fuzzy GMDH, 82
principles of
external complement, 224
freedom of choice, 224, 225, 245
multiplicity of models, 223
polynomial of Kolmogorov-Gabor, 223
self-organization, 223
theorem of incompleteness, 223
H
Hybrid Algorithm
for radial network, 30
Hyper radial basis function
networks (HRBF), 1
374
Index

I
Invest portfolio optimization
direct fuzzy portfolio optimization problem,
349, 351
dual fuzzy portfolio optimization problem,
349, 362, 363, 369
fuzzy-set approach with triangular
membership functions, 349, 352,
364
fuzzy-sets approach with bell-shaped
membership functions, 349, 357
fuzzy-sets approach with Gaussian
membership functions, 349, 359
fuzzy portfolio problem with application of
forecasting method, 349, 364
Markovitz model, 349–351, 360, 362
K
Kohonen’s learning. See Competitive learning
M
Multi-criteria optimization (MCO), 325, 329
Mutation
adaptive, 322, 331
Cauchy, 343
exponential, 319, 321
Gaussian, 318, 336, 343, 344
Levi, 338
self-adjusting, 319
with fuzzy rules, 323
N
Network learning algorithms
competitive learning, 62
conjugate gradient, 1, 13, 186, 192, 194,
198, 201, 204
genetic, 1, 13, 16, 18, 194, 201, 204, 211
gradient, 1, 5, 6, 8, 10, 13, 15, 186, 187,
192–194
hybrid, 1, 27, 30, 31, 33, 36
modiﬁed competitive learning, 64
rollback method, 11
shock BP method, 11
stochastic algorithm of learning, 70
with step correction, 11
with vector step, 12
Neural networks
back propagation, 1, 4, 5
feed forward, 2, 3
Hamming, 39, 51–53
Hebb, 39, 59
Hopﬁeld, 39, 40–50, 53
Kohonen, 39, 59, 62, 63, 67–69, 78
radial basis functions, 1, 18, 23, 32
R
Radial neural networks
and sigmoidal networks, 36
essence of, 18
heuristic methods, 35
hybrid algorithm for, 30
self-organization process, 27
training methods for, 26
Randomization, 184, 187, 204
Region of interest (ROI), 197
S
Self-organizing maps (SOMs), 70
T
Theorem about universal approximation
fuzzy neural network, 4
neural network back propagation, 4
Index
375


Problem Solving and Data 
Analysis using Minitab 
www.allitebooks.com

Problem Solving and  
Data Analysis  
using Minitab 
A clear and easy guide  
to Six Sigma methodology
Rehman M. Khan
Chartered Chemical Engineer and Six Sigma
Black Belt, Loughborough, UK
A John Wiley & Sons, Ltd., Publication
www.allitebooks.com

This edition ﬁrst published 2013 
© 2013 John Wiley & Sons, Ltd.
Registered ofﬁce 
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply 
for permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with 
the Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or trans-
mitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, 
except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission 
of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may 
not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand 
names and product names used in this book are trade names, service marks, trademarks or registered 
trademarks of their respective owners. The publisher is not associated with any product or vendor men-
tioned in this book. This publication is designed to provide accurate and authoritative information in 
regard to the subject matter covered. It is sold on the understanding that the publisher is not engaged in 
rendering professional services. If professional advice or other expert assistance is required, the services 
of a competent professional should be sought.
Portions of information contained in this publication/book are printed with permission of Minitab Inc. 
All such material remains the exclusive property and copyright of Minitab Inc. All rights reserved.
MINITAB® and all other trademarks and logos for the Company’s products and services are the exclusive 
property of Minitab Inc. All other marks referenced remain the property of their respective owners. See 
minitab.com for more information.
Library of Congress Cataloging-in-Publication Data
Khan, Rehman M.
  Problem solving and data analysis using Minitab : a clear and easy guide to six sigma methodology / 
Rehman M. Khan.
       pages cm
  Includes index.
  ISBN 978-1-118-30757-1 (hardback)
 1.  Mathematical statistics--Data processing. 2.  Problem solving--Statistical methods. 3.  Minitab. 
4.  Six sigma (Quality control standard)  I. Title. 
  QA276.45.M56K43 2013
  658.4'013028553--dc23
                                                            2012035286
A catalogue record for this book is available from the British Library.
ISBN: 978-1-118-30757-1
Typeset in 10/13 Frutiger LT Std by Laserwords Private Limited, Chennai, India
www.allitebooks.com

Contents
Acknowledgements
ix
1
Introduction
1
2
Minitab Navigation
6
2.1
Windows
6
2.2
Dropdown Menus
8
2.3
Importing Data
13
2.4
Column Formats
15
2.5
The Calculator
18
2.6
Basic Graphs
20
2.7
Adding Detail to Graphs
25
2.8
Saving Graphs
27
2.9
Dotplots
28
2.10
Using the Brush
32
2.11
Boxplots
34
2.12
Bar Charts
40
2.13
The Layout Tool
42
2.14
Producing Graphs with the Assistant
44
2.15
Producing Reports
46
2.16
Creating a New Project/Worksheet Button
49
3
Basic Statistics
50
3.1
Types of Data
50
3.2
Central Location
50
3.3
Dispersion
51
3.4
Descriptive Statistics
52
3.5
Inferential Statistics
59
3.6
Conﬁdence Intervals
60
3.7
Normal Distribution
62
3.8
Deviations from Normality
63
3.9
Central Limit Theorem
70
4
Hypothesis Testing
71
4.1
The Problem Statement
72
4.2
Null and Alternate Hypotheses
72
4.3
Establishing the Risks
73
4.4
Power and Sample Size
76
4.5
Conducting the Test and Evaluating the Results
80
4.6
One Sample t Test
81
4.7
Paired t Test
105
www.allitebooks.com

vi
Contents
4.8
Two Variance Test
118
4.9
Two Sample t Test
130
5
Analysis of Variance
150
5.1
How ANOVA Works
150
5.2
One Way ANOVA (Classic)
152
5.3
One Way ANOVA with the Assistant
164
5.4
ANOVA General Linear Model
192
6
Measurement System Analysis
209
6.1
The Importance of Measurement Systems
209
6.2
How Measurement Systems Affect Data
209
6.3
Analysing the Appropriate Systems
210
6.4
Types of Measurement Systems Error
211
6.5
Measurement Systems Toolbox
213
6.6
Type 1 Gage Study
214
6.7
Gage Repeatability and Reproducibility Studies
217
6.8
Create Gage R&R Study Worksheet
219
6.9
Gage R&R (Crossed)
221
6.10
Gage R&R Crossed Studies
221
6.11
Gage R&R (Crossed) Study
222
6.12
Gage R&R (Nested)
247
6.13
Gage Bias and Linearity Study
255
7
Statistical Process Control
261
7.1
The Origins of Statistical Process Control
261
7.2
Common Cause and Special Cause Variation
262
7.3
Detection Rules for Special Causes
263
7.4
False Alarms
266
7.5
When Should We Use SPC Charts?
267
7.6
Subgrouping
268
7.7
The Appropriate Chart
268
7.8
The I-MR Chart
269
7.9
The Xbar-R Chart
291
7.10
The Xbar-S Chart
299
7.11
SPC Exercise
307
7.12
The I-MR-R/S Chart
310
8
Process Capability
313
8.1
The Basics of Process Capability
313
8.2
Short Term and Overall Capability
318
8.3
Capability Analysis for Normal Data
319
8.4
Capability Analysis for Non Normal Data
329
8.5
Capability Comparison using the Assistant
340
9
Correlation and Regression
344
9.1
What are Correlation and Regression?
344
9.2
Correlation
346
9.3
Multiple Correlations
349
9.4
Introduction to Regression
354
www.allitebooks.com

Contents
vii
9.5
Single Predictor Regression
355
9.6
Introduction to Multiple Predictor Regression
372
9.7
Multiple Predictor Regression
373
9.8
Predictor Selection Procedure
396
9.9
Nonlinear Regression
400
10
Design of Experiment
407
10.1
Why Use Design of Experiment?
407
10.2
Types of DOE
407
10.3
DOE Terminology
408
10.4
Two Level Factorial Designs
412
10.5
Fractional Factorial Designs
439
11
Help
456
11.1
Help Overview
456
11.2
Help! Help!
457
11.3
Tutorials
461
11.4
StatGuide
463
11.5
Methods and Formulas
464
11.6
Meet Minitab
466
11.7
Help on the Web
466
11.8
Help on the Web and Datasets
468
11.9
Datasets
469
Index
471
www.allitebooks.com

Acknowledgements
Firstly, I would like to thank God for giving me the ability and circumstance to write this book.
I would like to thank my wife, Mahwish, and my children, Iqra, Humzah and Raeesa, for being very 
patient with me and accepting that I would be spending all of my spare time outside of work on this 
project. Mahwish and Humzah (12 years old at that time) also helped me with the proof-reading and 
transferring the script from Power Points to Microsoft Word. 
I would like to acknowledge my parents’ effort to continually challenge me and my brother in our edu-
cation. I think my dad would have been very proud at the release of this book and I know my mum is.
I would like to thank our family down in London, particularly, my mum and wife’s parents who did not 
see us very much during the production of the manuscript but still continued to support us. 
I want to thank all my family and friends around the world for their support. There are too many peo-
ple to name so I will just say where you live, Loughborough Bedford, London, Preston, Birmingham, 
Shefﬁeld, Pakistan, North America, Australia, Saudi Arabia. If I haven’t mentioned where you live please 
feel free to associate yourself with one of the places I did mention.
I also want to mention Springﬁelds Fuels Limited near Preston. I spent most of my working life there 
and they introduced me to Six Sigma. I also want to mention my current employer, British Gypsum, and 
thank them for my continued Six Sigma training. In particular, Gary Pilcher who supported my training 
and then the start of my training course within British Gypsum. There is a fantastic team at East Leake, I 
want to thank everyone there for their moral support, in particular the other members of the Melnik 6 
(Greg Bere, Paul Brauer, Matt Carey, Lee Chaplin & Gary Parkins). 
I want to thank the good people at Wiley for publishing this book. I also want to thank the good people 
at Minitab for letting me use their excellent software.
Some teachers you never forget. So I would also like to thank Mr Crowley who was my Mathematics 
teacher at Hayes Manor School. He took me through my ‘O’, ‘AO’ and ‘A’ Level Maths.
Finally, I would like to thank the people that have bought this book. I am hoping that this is the start of 
the journey for you as well as me.
www.allitebooks.com

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 1
Introduction
Confucius said ‘I hear and I forget. I see and I remember. I do and I understand’. This proverb sums up 
the spirit of this book as it is very practical and it involves the reader. Every chapter contains examples 
and exercises that will capture the reader and ensure the information is passed on in a memorable way. 
This book is aimed at numerical professionals, students or academics who wish to learn and apply sta-
tistical techniques for problem solving, process improvement or data analysis without getting bogged 
down in theory. In fact anyone that wants to be data driven in their decision making should use this 
book to understand how to use Minitab. 
The vehicle for the statistical techniques is Minitab version 16. However, most of it will also be applicable 
to version 15. The notes will indicate if a feature is in Minitab 16 only by stating ‘M16 only’. At this point 
I could give you a list of the new features that are incorporated within Minitab 16 but that would be a 
bit pointless, so I will attempt to point out the important new features as we go through the modules. 
You will deﬁnitely need to have access to a copy of Minitab when going through the book and it would 
be advantageous to have a copy of Microsoft Excel as well. 
Minitab has developed considerably between versions 15 and 16. One of the main enhancements is the 
Assistant which helps users select the appropriate test, enter the data and interpret the results. The 
Assistant is available for a number of the key test procedures. Users of older versions of Minitab and also 
Minitab 16 have the option of using the non-Assistant methods which are accessed via the traditional 
drop down menus. For convenience, within the book I will refer to this method as the Classic method. 
I am a chartered Chemical Engineer and Six Sigma Black Belt. I have worked in the nuclear industry, 
cosmetics industry and in construction products manufacturing. My main role has always been process 
improvement and the projects that I have completed have won awards and saved millions of pounds. 
Whilst honing my problem solving and data analysis skills I recognised that there was a huge gap in the 
availability of appropriate training materials and yet there was a huge demand from numerical profes-
sionals to learn the skills. I was disappointed not to be able to ﬁnd the right sort of books to help me 
learn and understand how to use Minitab. However, I was fortunate enough to have my employers put 
me through my Six Sigma training. 
My aim is not to blind the reader with mathematical theory but to teach problem solving and data anal-
ysis through the use of statistical analysis in a very graphical and accessible way. The book uses example 
based learning that each reader can work through at their own pace. Each example is broken down into 
the very exact steps that must be followed in order to work through the complex analysis. After the 
examples there is usually an exercise so that the reader can be assured that they have understood the 
key learning points. However, even the answers do not leave the reader cold with just a single numerical 
solution. The exercise answers show graphic milestones that the reader must achieve in order to reach 
the endpoint of the analysis. The example and exercise data sets can be downloaded from the Wiley 
publishing website, www.wiley.com/go/six_sigma_methodology.
www.allitebooks.com

2
Problem Solving and Data Analysis using Minitab 
I have found that other books teaching this subject make themselves inaccessible to beginners either 
because complex statistical theory is put before practical learning or there is an attempt to cover every-
thing that Minitab can do. The main strengths of this book are that it is a training course in book form 
and it teaches a much sought after skill set. It teaches the reader using a logical and stepwise methodol-
ogy. The examples and exercises take the reader through key learning points and, because they are so 
very easy to follow, they build the reader’s conﬁdence. The book does not cover every possible topic. 
For instance it does not cover the handling of attribute data. This is due to space limitations and the 
fact that in a numerical world we come across this type of data set less and less. However, if there are 
procedures that you wish to learn with attribute data I suggest that you learn the equivalent test for 
continuous data and then use the help system to understand how to handle attribute data. Also, we will 
not be learning Six Sigma project methodology as it would probably double the page count. 
I would like to introduce a metaphor for problem solving. I am standing in Field A but when I solve my 
problem I will be able to move over to Field B which is a better place. In between is a rather large wall, 
and this wall represents the problem. The wall has a gate and when I ﬁnd the key, which can be repre-
sented by the answer, I can open the gate and step through. The problem is that I need to ﬁnd the key 
and it can be anywhere in Field A. However, to help me ﬁnd the key I have a metal detector. The thing 
that determines how well I use the metal detector is my own skill. Well, in the real world the metal 
detector is Minitab and to ﬁnd the solution to numerical problems I must ensure that I can utilise this 
fantastic tool. This book’s sole purpose is to increase your skill level with Minitab. The intention is to do 
this by giving you a bit of theory and then getting you to solve problems using Minitab. Luckily, Minitab 
will make our life easy by doing all the complicated maths. All we need to do is tell it what to do and 
then understand what it is telling us. 
Let’s have a look at what we are going to be learning throughout this book. If you are beginner then 
I strongly recommend that you start from the very beginning and at least go through Chapters 2–4
before you start skipping forward to topics of interest. If you are an experienced user of Minitab then 
feel free to launch into any of the topics that are of interest to you. 
1. Introduction
This is the wordiest chapter in the book and you’re doing well to get through it. In the rest of the 
book we are going to keep you busy in front of your computer working on Minitab. 
2. Minitab Navigation 
In this chapter you are going to get the feel of Minitab. We will start by discussing the Minitab 
windows and then the drop down menus. As discussion is not a very good way of learning we will 
then import some data and look at the data formatting system used within Minitab. For additional 
learning we will put together some simple graphs and we will learn to add additional content to 
those graphs. This has a twofold purpose in that the reader will experience some of the stand alone 
graphing tools and will become familiar with Minitab navigation. Later you will see that a lot of 
Minitab’s statistical procedures produce their own graphs which are very useful for data analysis. 
We also need to learn how to use the report pad so that we can send reports to Microsoft Word 
and PowerPoint. 
3. Basic Statistics 
This is the chapter where we learn the basic background statistics that we need to know to help 
us understand what is coming up in the more demanding chapters. If you are new to statistical 
methods you have probably only been introduced to descriptive statistics. In this chapter we learn 
one of the most important concepts in the course and that is the difference between descriptive 
www.allitebooks.com

Introduction
3
statistics and inferential statistics and how that links to the concept of the entire population and a 
sample set from that population. 
4. Hypothesis Testing 
You could say that hypothesis testing is one of the foundational chapters of the statistical proce-
dures that we are going to be using. Once we get to grips with the theory we will start using some 
of the test procedures within Minitab. We will start by looking at a single set of data and determin-
ing whether the mean of the population could be a particular value. We will then compare two 
data sets and attempt to infer conclusions about them. 
5. Analysis of Variance 
The analysis of variance (ANOVA) procedure builds on the last chapter and lets us make compari-
sons between two or more groups. We start by looking at the one way ANOVA procedure and 
move onto the ANOVA General Linear Model (GLM). The GLM allows us to model multiple factors 
and multiple levels. 
6. Measurement System Analysis
This chapter teaches us procedures to help us understand whether our measurement systems are 
adequate and reliable. The measurement system can be an instrument, like a weigh scale, or it 
could be a person making a judgment about a product or process. Either way we want to know if 
the measurement systems are free from human failings and whether they can be relied upon. 
7. Statistical Process Control 
Statistical process control (SPC) is about monitoring your process and looking for unusual occur-
rences by using control charts. Finding out the type of unusual behaviour and when it occurs helps 
us to identify the initiating events. We can then put controls in place that will eliminate the initiat-
ing events and thereby making the process more stable. 
8. Process Capability 
We use process capability to measure our customer requirements against what the process is actu-
ally delivering. This is often done at the start of the project to measure the gaps and then again at 
the end of the project to demonstrate the improvement. 
9. Correlation and Regression
These two terms are often confused. In the world of Minitab correlation is about establishing 
whether two parameters have a linear relationship and checking the strength of that relationship. 
Regression goes further by trying to ﬁt an equation to model the relationship. We will learn pro-
cedures for single and multiple factor regression. 
10. Design of Experiment 
Don’t worry. Design of experiment (DOE) is not always about being in a laboratory and conducting 
experiments. DOE is actually an optimised methodology for checking selected inputs and check-
ing whether they have a statistically signiﬁcant effect on a selected response. DOE tells us how 
strongly each factor is affecting the response and whether any of the interactions are important 
to the response. 
There are many types of DOE and we will concentrate on two of the more commonly used types. 
DOE comes with its own terminology so we will spend a bit of time getting to grips with the termi-
nology before we get into DOE proper. 
www.allitebooks.com

4
Problem Solving and Data Analysis using Minitab 
11. Help
Minitab has a number of comprehensive help systems that can help you should you get stuck. 
This chapter will introduce the help systems and show you what is available so you will be able to 
quickly get the information that you need. 
This book originally started out as a course that I ran within British Gypsum. I then decided that I could 
do with less time with my wife and family and it would be a good idea to convert the course into a book. 
One of the concepts that I introduced during the course was the Statistical Charter. The charter was a 
self imposed set of rules designed to protect the stakeholders, customers and no doubt our own reputa-
tions. The charter has the following functions: 
t Stops us from turning ﬁction into fact. 
t Warns stakeholders regarding the certainty of our conclusions. 
t Sets limits so that we can safely execute the statistical procedures. 
You will see that the charter manifests itself within the care that we take to set up the test procedures. 
I can give you guidance that, for example, we should try and aim for a power of between 80–90% but 
when it comes to using the tests within the real world you may choose to ignore the advice. However, 
most of the advice given here is derived from the Minitab White Papers and gives a robust methodology 
for conducting the procedures. 
Statistics can have a bad reputation. It was Benjamin Disraeli who said ‘There are lies, damn lies and sta-
tistics’ and we don’t want to perpetuate that reputation. Personally, I think a particular quote from the 
Toby McGuire Spiderman movie is more apt for the exponents of statistical analysis: ‘With great power 
comes great responsibility’. 
The statistical techniques taught within this book are most commonly found within Six Sigma. They are 
used for problem solving, data analysis and the reduction of variation within the DMAIC Framework. 
There are other business improvement frameworks such as World Class Manufacturing (WCM) and these 
tools and techniques sit equally well within the Focussed Improvement (FI) Pillar of WCM. However, the 
use of Six Sigma is far more widespread and it is more commonly associated with Process Improvement 
and Minitab.
The table below shows how the statistical toolset can be used within the DMAIC Framework. The tools 
used are not set in stone as the methodology allows the use of any tool which gets the job done. 
For each of the chapters I would strongly recommend that you initially read the theory section in the 
chapter and then, when it comes to examples in the book, follow the instructions to work through each 
example on a PC with a copy of Minitab. When you are happy that you understand the methodology 
applied to solve the example problems then try a single exercise without using the book. After complet-
ing the exercise go back and review the solution shown within the book. If you are happy that you have 
successfully completed the exercise then complete all the remaining exercises in the same way. If not, 
review the notes and the solution to establish where you went wrong and then repeat the exercise. As 
a gift to the reader for reading this far into the introduction I will tell you that the exercises are usually 
easier than the examples. 

Introduction
5
In case you are wondering, all of the data sets used are ﬁctitious. They were designed to show particular 
aspects of the methodology being taught. 
And ﬁnally, if you are in need of some inspiration go onto YouTube and search for the ‘Minitab song’, 
it keeps me motivated! 
Deﬁne
Used to set Measures 
of performance for 
project, Deﬁne gap 
between process 
outputs and customer 
requirements.
Tools used: Project 
Charter, Critical To 
Tree, SIPOC.
Measure
Used to start 
understanding
the process. Carry 
out Measurement 
Systems Analysis 
(MSA) on the Key 
Output Process 
Variables (KOPV) and 
captute the process 
baseline.
Statistical Tools 
used: SPC, Process 
Capability, MSA.
Tools used: Project 
Charter, Critical To 
Tree, SIPOC.
Analyse
Used to deﬁne the 
Key Process Input 
Variables (KPIVs). 
Need to identify 
sources of variation 
and screen potential 
causes. MSA of KPIVs.
Tools used: 
Process Mapping, 
Brainstorming, 
Fishbone Diagram, 
Cause and Effect 
Matrix, FMEA.
Statistical Tools used: 
MSA, SPC, Hypothesis 
Testing, ANOVA, 
Correlation and 
Regression, DOE
Improve
Optimises settings 
of KPIVs to deliver 
the required output. 
Statistically vali-
dates new operating 
conditions.
Tools used: Updated 
FMEA, Cost/Beneﬁt 
Analysis, Error 
Prooﬁng
Statistical Tools used: 
MSA, SPC, Hypothesis 
Testing, ANOVA, 
Correlation and 
Regression, DOE
Control
Locks in and 
documents optimised 
parameters. Measures 
improvement and 
sustainability.
Tools used: Updated 
FMEA, Control Plan, 
Procedures reviewed, 
engineering and 
operaating, Success 
Sheets/Best Practise 
Sheets, Project Report 
and Presentations.
Statistical Tools 
used: SPC, Process 
Capability,

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 2 
Minitab Navigation
2.1 Windows
In this Module we want you to get the feel of Minitab. We are going to do this by ﬁrst discussing the 
Minitab windows and then the dropdown menus. As discussion is not a very good way of learning we 
will then import some data and get you to start working with that data.
1. Open Minitab.
You will be presented with the screen below. 
At the top of the screen there are the dropdown menus and toolbars, which work like most Microsoft 
applications. Below that we have two windows: the session window and the project window. The graphs 
window is not shown at start up but it is also commonly used.
Toolbars
and
Dropdown
Menus
Session
Window
Project
Window

Minitab Navigation
7
The project window is mainly used to enter data into Minitab and is used in a similar way to Microsoft 
Excel. However, the main difference is that Minitab wants you to present data in columns and is quite 
strict about having the same data format within a column.
The session window is where the user is given numerical outputs. You will have guessed that the graphs 
window is where the user is given graphical outputs. Graphs can exist in their own windows but it’s best 
to keep them minimised and only call them up when you need them from the graphs folder.
Now that we know what the windows do, let’s learn to move between the windows.
2. Locate the three folder icons in the tool bar as shown in the above, left ﬁgure.
3. Single click between the live icons a couple of times. Notice how the folder icons not only open 
the corresponding window but also a project manager window. The window icons, as shown in the 
central ﬁgure above will open the particular window and not the project manager. The window 
you select will cover the complete screen if the windows were previously maximised. Give that a go 
now and ﬁnish up on the project window.
4. If you forget what the icons are for, you can hover over them with the mouse and a pop-up box will 
open to tell you. As shown in the above, right ﬁgure.
The ﬁgure above shows the dropdown menu and default toolbars for Minitab 16 (M16). The ﬁgure 
below the next paragraph is for Minitab 15 (M15). In fact all the dropdown menus are virtually the same 
between M15 and M16, with the exception of the Assistant, which is a new feature on M16. There are 
many new features within M16 but these are better explored as we go through the modules because 
the majority of them are for experienced users.
Next we will look at the dropdown menus. Later we will explore the key icons within the toolbar by 
working with some data which we will import.

8
Problem Solving and Data Analysis using Minitab 
2.2 Dropdown Menus
The File dropdown should be familiar as it is similar to 
most Microsoft programs.
Project and Worksheet Description allow the user to 
enter information about either item. This can be very 
useful when you refer back to the work in the future.
Open Worksheet and Other Files both allow the import 
of data from various common ﬁle types.
Query Database is a facility that allows the user to 
import information from a database.
There are options for printing presented here, how-
ever, from my experience printing for the purpose of 
producing reports is best done via the Report Pad.
The Edit tab is not used that often. This is because 
right clicking on an object also gives custom editing 
options.
Cut, Copy and Paste are more easily done with Ctrl+X, 
C and V, respectively.
Worksheet Links can be useful. They allow the user to 
link a section of a spreadsheet to the data within the 
project window. Depending on how the link is man-
aged, changes to the spreadsheet will automatically 
be applied to the data within the project window. 
The Edit Last Dialog facility is tremendously useful but 
I ﬁnd it easier to use directly from the toolbar. 

Minitab Navigation
9
The top half of the toolset is used to arrange the data that is 
in the worksheet window. On occasion subsets may need to be 
moved from the same column to adjacent columns and vice 
versa. Copy contains predeﬁned copying procedures that can 
be used as shortcuts.
As in Excel, Transpose Columns will spin the selected data by 
90°. We need to be wary of data types though.
Code works to replace selected items throughout a column 
and lets you convert data types in the process.
Change Data Type is useful in correcting the data type.
Concatenate is the same as the Excel function and adds text 
strings together. 
Using Calc will be covered in the module as it is quite useful. 
Column and Row Statistics pretty much do as expected and 
give you an output in the session window.
The central and bottom sections are used to generate differ-
ent types of data distributions. Making mesh data can be quite 
fun in a nerdy sort of way.

10
Problem Solving and Data Analysis using Minitab 
The Stat menu is the heart of Minitab 
and contains the majority of the sta-
tistical tests.
This book will focus on a number of 
these menus and submenus. We will 
be going into Regression, ANOVA, 
DOE, Control Charts and Quality Tools 
submenus in later modules.
Basic Statistics will be the most fre-
quently used submenu.
Using graphs to display data is usu-
ally a quick and easy way to see trends 
and should be done in the early stages 
of analysis.
The Graph menu in Minitab supplies a 
large variety of graphing tools which 
can provide a very different insight 
into the data than you would nor-
mally see with standard spreadsheet 
programs.
Minitab will generate a large number 
of these graphs when statistical tests 
are carried out. 
We are going to look at a number of 
these graphs within this module.

Minitab Navigation
11
The Editor dropdown menu contains a number of standard 
editing functions. A number of these are available by right 
clicking on the live worksheet.
Column formatting is done here.
Deﬁne Custom Lists is quite interesting. It can be used to 
deﬁne a standard list that when set up can then be added 
into the worksheet by entering the starting element of the 
list and then pulling down on the handle of the active cell. 
This is particularly useful if you have a custom list that you 
regularly want to add to the project window.
The three applications listed at the top of the Tools menu 
can be launched from here.
Toolbars can be used to set which toolbars are active. For 
example, the 3D Graph Editing tool bar can be activated 
from here.
Customize can be used to change a large number of fea-
tures. It is best fully explored by the reader in their own 
time.
Amongst other things Options is used to set:

X the root directory for saving work, 

X the background colour on all graphs,

X the output options on statistical tests.
File Security can be used to set password protection for our 
ﬁles.

12
Problem Solving and Data Analysis using Minitab 
The icons in Window are most useful when you are working on 
a large project whilst utilising all the windows. They can be used 
to quickly organise your windows and graphs.
We will show how to use the Update All Graphs Now function 
later in this module.
The Help menu should be your ﬁrst option when faced with 
problems. We will look at how to use the Help system in much 
greater detail in a separate module.
Minitab actually comes supplied with a number of sample data 
sets which are utilised within the help systems in order to dem-
onstrate statistical procedures. 
Licensing is a M16 only option. It contains information about 
your Minitab License and also a clever feature to Move License. If 
you are mobile or considering working from home it would allow 
you to transfer your product activation to another location.

Minitab Navigation
13
The Assistant dropdown is M16 only and 
is one of the key advances in the update. 
The Assistant will lead you through the 
tool selection and ensure that the pre-
requisites are correct. It will also pro-
vide you with an interpretation of the 
results. It could make guide texts and 
training redundant!
2.3 Importing Data
We are now going to look at importing some data into Minitab. We will then carry out a number of 
exercises on that data. These will include checking the data format within the columns and, if we need 
to, we will change the data format. We will then manipulate the data by adding columns together and 
then produce a number of different types of graphs and add content to those graphs. Once we have a 
few graphs we will look at how we can export our data so that others can view it. We will ﬁnish the ses-
sion by carrying out some Minitab customisation and also using the Assistant to produce some graphs.
The table below shows the data that we are going to work with. The data concerns a piece of equip-
ment that requires both gas and electricity to run. The equipment has two zones. The Total Cost row 
relates to how much it costs to run the equipment. Person relates to the operator who was running the 
equipment and Stage refers to a particular event that we want to monitor; in this case it relates to when 
maintenance was conducted on the machine. Stage changes from condition Before to Stage1 so we can 
tell when the maintenance occurred.
The data currently lives within Microsoft Excel and it is formatted in rows not columns. Each row has a 
title cell and then 38 data cells.
1. Open File 02 Minitab Navigation.xls using 
Microsoft Excel.
2. Go to ‘Data in rows’ worksheet. 
Minitab analyses data in columns. We need to 
change the arrangement of this data so that it 
is in columns. Minitab has a Transpose function 
but it is better to prepare the data in Excel and 
then transfer it to Minitab. This is due to want-
ing to maintain both the column headings and the 
mixed data types in the example data.
www.allitebooks.com

14
Problem Solving and Data Analysis using Minitab 
3. In order to shift the table around so 
that the data is in columns ﬁrst copy 
target cells A1 to AM10.
4. Go to worksheet ‘Sheet2’.
5. Right click on cell A1 and use the Paste 
Special command from within the 
dropdown menu.
6. Select Transpose from the selection 
box that opens and then click OK.
If you had trouble you can use the table that I have already transposed. It is in the ‘Data in Columns’ 
worksheet. There are two easy ways to transfer the data from an Excel worksheet into Minitab.
7. Copy cells A1 to J39 of the trans-
posed cells by highlighting and 
then pressing Ctrl+C.
8. Then Paste the data, by pressing 
Ctrl+V, into Minitab with Cell A1 
being copied  into the title row 
directly under ‘C1’ and above 
Row1 in the shaded row. Minitab 
uses these rows for titles. Pasting 
under this row would cause the 
title row to be counted as data 
and this would upset the column 
formatting.

Minitab Navigation
15
The alternate method of importing 
data into Minitab is shown below.
9. From within Minitab click File 
<<Open Worksheet.
10. Navigate to the folder that 
contains the data ﬁle. Ensure 
that the correct ﬁle type has 
been selected. Then double 
click on the ﬁle that you want 
to open.
11. Minitab will put the data from 
each Excel worksheet into a 
separate Minitab Worksheet. 
Notice how the formatting of 
the Data in Rows sheet is in 
disarray.
12. Close all the Minitab work-
sheets with the exception 
of the Data in Columns 
worksheet.
2.4 Column Formats
Notice the column numbering system and column formatting as shown within the above ﬁgure.

16
Problem Solving and Data Analysis using Minitab 
Minitab uses:

X D for Date/Time, 

X T for Text, 

X A blank for numeric.
There is only one type of format allowed in each column. 
Notice that column C8 is incorrectly formatted as 
text due to a typing error.
1. Click into the cell and remove the ‘a’.
Notice that Minitab does not automatically cor-
rect the formatting. Also Minitab will not handle 
the data as numeric so we have to correct the 
formatting.

Minitab Navigation
17
2. Click Data <<Change Data Type 
<<Text to Numeric
3. The Text to Numeric menu box 
opens. Minitab uses the top input 
box labelled Change Text Columns 
to ask which column needs chang-
ing. You can select Feedrate either 
by double clicking on it or by high-
lighting it with a single click and 
then pressing Select. The destina-
tion for our numeric column is going 
to be C8. We are going to overwrite 
our old text column by selecting C8. 
C8 is typed directly into the bottom 
input box. Then click OK.
4. Click the Show Info icon. This gives 
us a summary of the data that is 
within the current worksheet. We 
can see that the format of column 
8 has been changed to numeric. 
Additionally, we can see that col-
umn 9 has the title Stage. It con-
tains 38 cells of data and none of 
the data is missing. The column 
format is text.

18
Problem Solving and Data Analysis using Minitab 
2.5 The Calculator
In the next part of the exercise we are going to use the Calculator to add the values in two columns 
together. To show a different way of adding columns together we are then going to use the Assign 
Formula function to add two different columns together. The ﬁgure below shows the layout of the 
calculator.
1. Click Calc <<Calculator.
The calculator menu box opens. The calculator menu is used to mathe-
matically manipulate columns of data. The output is stored in the current 
worksheet.
Results will be
stored here
Quick selection of
input columns from
here
We have the option
of assigning a 
formula

Minitab Navigation
19
2. Input the same expression as 
shown.
3. Store results in C11.
4. Click OK.
5. C11 will now contain the 
result of adding together 
each of the rows in the stated 
columns within the expres-
sion. Initially the resultant 
column will not have a head-
ing. Double click within the 
heading area of C11 and 
insert ‘Total Elec’ as the col-
umn heading.
6. Move the cursor to C12 row 1.
7. Click on the Assign Formula 
button icon in the toolbar.
8. Notice the Formula box that 
opens. It is similar to the 
Calculator; however there is 
nowhere to assign the des-
tination of the data as that 
is preselected by placing the 
active cell.

20
Problem Solving and Data Analysis using Minitab 
9. Input the same expression 
as shown.
10. Click ‘OK’.
11. Rename C12 heading as 
‘Total Gas’. Minitab has 
added together each row 
in the Zone1 Gas and Zone2 
Gas columns and stored the 
results in C12. The green 
cross with the black back-
ground, within the col-
umn heading, means that 
C12 will be updated if any 
cells in columns C6 and C7 
are changed. This type of 
updating does not occur if 
you use the calculator.  
2.6 Basic Graphs
We are going to continue analysing our example data. This time we are going to carry out a batch of 
statistical analysis that Minitab calls the Graphical Summary. This battery of tests is an excellent ﬁrst 
choice when examining data.
1. Click Stat <<Basic Statistics 
<<Graphical Summary
2. From the Graphical Summary menu box 
Enter Total Gas as the variable to be 
investigated.
3. Then click OK.
The Graphical Summary is the most commonly 
used tool in Minitab. It provides a lot of statis-
tical information in one quick procedure.

Minitab Navigation
21
Looking at the boxplot we see 
that there is an outlier. This is 
shown as an asterisk and is 
separated from the tails of 
the boxplot.
4. Hover the mouse pointer 
over the outlier in the 
boxplot to see the value 
of the point and the 
data row. We see that 
this data has come from 
Row 25 of the Total Gas 
Data.
In the real world we would 
investigate 
outliers 
thor-
oughly. For the example we 
are going to say it has been 
conﬁrmed that this point is 
low because the machine suf-
fered a stop during this data 
collection period.
Hover over
outlier.
Normality test
Mean and Median confidence
intervals
Histogram of
data with
normal curve
Numerical data
on confidence
intervals
Numerical
statistical data
Boxplot of data

22
Problem Solving and Data Analysis using Minitab 
5. Click on the project window icon.
6. We see that it is not just the data in C12 that is the prob-
lem. Due to the stop the rest of the data in the row is 
also lower than expected. Click on the actual Row num-
ber for row 25. This will highlight all of the row.
7. Right click on the Row number and select Delete Cells. 
Row 25 is deleted. And all the rows move up one slot 
automatically.
Note: if a single cell is deleted, all the data points below 
would still shift up one row. Beware that this would misalign 
adjacent rows within the  data. 
We are now going to produce exactly the same Graphical Summary again to highlight a number of 
learning points. One is to show that instead of navigating through Stat <<Basic Statistics <<Graphical 
Summary we can just click on the Edit Last Dialog Box icon. This remembers the last dialog box we went 
to and takes us straight back to it. The second is that the dialog box remembers our last entries. This is 
useful most of the time but can occasionally be problematic as all sublevel settings will also be stored.  
The dialog box can be reset to initial settings by pressing F3 but don’t do this now. The ﬁnal learning 
point is to do with updating graphs. Follow the lesson through to ﬁnd out about that.
8. Click on the Edit Last dialog 
box icon or press Ctrl+E.
9. Ensure ‘Total Gas’ is still 
selected and then click OK. 
This produces another copy 
of the Graphical Summary 
for Total Gas.
With the outlier removed a new 
graphical summary is shown for 
Total Gas. Notice how the graph 
has changed.

Minitab Navigation
23
10. Click on the Show Graphs 
Folder icon. This shows that we 
have two copies of the graph. 
11. Cycle between the two graphs 
a couple of times by double 
clicking on the top and bottom 
text.
Notice that there is a difference in 
the type of cross in the top left cor-
ner of each graph.
The white diagonal cross on the 
original graph indicates that it is not 
current.
The green cross indicates that the 
graph is up to date with the data in 
the project window.
These charts cannot be updated. 
They must be redrawn.
12. Close the Graphical Summary 
that is not up to date.
www.allitebooks.com

24
Problem Solving and Data Analysis using Minitab 
Some charts, usually Control Charts, can be updated and they 
have slightly different indicators in the top left corner.
The green cross in the white circle means that this chart is current. 
Additionally, if the source data changes the chart can be updated.
The yellow circle with the horizontal yellow bar means that the 
chart is not current. Right clicking on the chart and selecting 
‘Update Graph Automatically’ will update it. An alternative way 
of doing this is to go Window <<Update All Graphs Now.
The ﬁgure below shows how some of the regions within charts can be edited.  Double clicking on a spe-
ciﬁc region or item will open up an input box that allows the region or item to be edited. Each region or 
item has its own set or options for editing. The following is a list of some of the items that can be edited:

X Graph scales,

X Graph bars,

X Background region,

X Titles,

X Data.
Try double clicking on each of the regions shown within the rings or the items to open the options box. 
Change some of the options if you wish.

Minitab Navigation
25
When you make changes to a chart they won’t be carried through to the next chart that you produce. 
There is a way to make some global changes to charts. I like to do my bit for the environment by setting 
the background colour to my charts as white, thereby saving printer ink. I will show you how to access 
these global options.
13. Click on Tools <<Options
14. Double click on each of the following: Graphics, then Regions 
and then Graph.
15. Amongst the options that are available will be background col-
our. This can then be set to white.  Then click OK.
Note that if you want to change other global settings they can be 
accessed via this menu.
2.7 Adding Detail to Graphs
We are now going to learn how to add additional content to the graphs, such as reference lines and 
text.
1. Select the second of the Graphical 
Summaries that we produced.
2. Right click on the graph on the histo-
gram graph region.
3. Go to <<Add
<<Reference Lines.
4. We will show a reference line at 6300 
on this graph as this indicates the 
target level for Total Gas. Enter 6300 
into the lower cell. Then click OK.

26
Problem Solving and Data Analysis using Minitab 
Within the toolbars there is a 
drawing toolbar. This is similar 
to other programs so the icons 
should be familiar.
1. Click on the Text icon to 
add text to the graph.
2. Drag an area on the his-
togram where you want 
the text to be placed. This 
can be adjusted later.
3. When you release the 
mouse button after drag-
ging, a text box will open 
allowing you to enter the 
text shown. Then click 
OK.
4. In order to change the 
look of the text, double 
click on it. Go to the Font 
tab and change the text 
colour to red. There are 
also other text options 
available here.

Minitab Navigation
27
2.8 Saving Graphs
We can save our current graph as a separate ﬁle. We have a number of options such as setting the ﬁle 
type and we can even save it as a picture ﬁle. 
1. Click File <<Save Graph 
As…
A normal save box will appear 
where you can select the des-
tination and ﬁle type.
For the highest compatibility 
use .jpg and .tif ﬁle types.
Saving the graph as a .mgf 
ﬁle will then require Minitab 
to open the graph.

28
Problem Solving and Data Analysis using Minitab 
2.9 Dotplots
We will now look at producing Dotplots and Boxplots. Both graphs use almost the same options for 
displaying groups. For each graph that we produce we will look at displaying the inputs with different 
Groupings so you can see how the selection works. The ﬁrst distinction made is whether the graph is to 
contain One or Multiple Variables (One Y or Multiple Ys). We will start with the simplest selection, which 
is one variable (One Y) and no groups. You will see a similar selection method for other types of graphs.
1. To produce a Dotplot select 
Graph <<Dotplot.
2. At the Dotplot menu select 
One Y <<Simple.
3. Then click OK.
4. Select ‘Total Elec’ as the 
Graph variables.
5. Then click OK.

Minitab Navigation
29
6. For this particular data set 
the dotplot shows that there 
are two distinct distributions 
within the Total Elec data. 
However, at this stage we can-
not make conclusions about 
what is separating the data.
We will now produce a dotplot 
from a single variable (One Y). We 
will use the Stage column to sepa-
rate the data into two groups. You 
might remember earlier we said that 
the Stage column contains categori-
cal data and we were using it to 
tell us when maintenance occurred. 
This would help us tell if mainte-
nance had any effect on the other 
variables.
1. Select Graph <<Dotplot.
2. At the Dotplot menu select One 
Y <<Stack Groups. Click OK.
3. Select 
‘Total 
Elec’ 
as 
the 
variable.
4. Select ‘Stage’ as the Categorical 
variable. Note that the tick box 
for Stack dots of last categori-
cal variable is ticked.
5. Click OK.

30
Problem Solving and Data Analysis using Minitab 
The following graph is dis-
played.  It can be seen that the 
two stages of the data produce 
two separate distributions. We 
could conclude that our elec-
tricity usage has reduced after 
the maintenance activity.
The graph (right) shows the 
same data in a slightly differ-
ent way. This graph would 
have been produced if we had 
unticked Stack dots of last cat-
egorical variable. It can also be 
produced by selecting the graph 
type as One Y, with Groups.
We will now break the rules 
that I just told you. We are 
going to display two variables 
using a One Y graph option.
6. Select Graph <<Dotplot.
7. At the Dotplot menu select 
One Y <<Stack Groups.

Minitab Navigation
31
8. Select ‘Total Elec’ and 
then ‘Total Gas’ as 
the variables.
9. Select ‘Stage’ as the 
Categorical variable.
10. Click Multiple Graphs.
11. Select ‘In separate 
Panels of the same 
graph.
12. ‘Then click OK and 
OK again.
The following two graphs 
will be displayed together, 
with each graph within its 
own panel.

32
Problem Solving and Data Analysis using Minitab 
2.10 Using the Brush
We will now learn how to use the Brush to quickly obtain data pertaining to data points on graphs.
1. Click on the Brush Mode icon.
2. Using the left mouse button, drag the 
pointer over a selection of data points on 
the graph.
Notice how the pop up box tells you the row 
number of the data. Also notice that data from 
the same rows will be highlighted in other areas 
of the graph.
The Brush tool can also be used to display data 
from other data columns.
3. Ensure Brush Mode is selected.

Minitab Navigation
33
4. Select Editor <<Set 
ID Variables.
5. Select ‘Date’, ‘Time’ 
and ‘Feedrate’ as the 
required variables.
6. Then click OK. 
7. Again use the Brush 
to drag over a num-
ber of data points. 
Notice how the data 
from 
our 
selected 
variables is displayed.
www.allitebooks.com

34
Problem Solving and Data Analysis using Minitab 
2.11 Boxplots
We will now look at Multiple Y graphs but this time we are going to produce boxplots.
1. Select Graph <<Boxplot.
2. At the Boxplot menu select 
Multiple Ys <<Simple
3. Click OK.
4. Select ‘Zone 1 Gas’ and 
then ‘Zone 2 Gas’ as the 
variables.
5. Then click OK.

Minitab Navigation
35
The boxplot is displayed. From 
the boxplot we can begin to con-
clude that Zone1 uses more gas 
than Zone2. This is something 
we could conﬁrm using hypoth-
esis testing. We will learn more 
about this in Chapter 4.
Notice the key features of the 
boxplot:

X The line in the box denotes 
the median.

X The data is split into quar-
tiles between the tails and 
the two sections of the box. 

X Outliers are represented by 
asterisks.
This is the last of the boxplots 
and we are going to select 
Multiple Ys, With Groups. Then 
we are going to add some addi-
tional content to the graph.
6. Select Graph <<Boxplot.
7. At the Boxplot menu select 
Multiple Ys <<With Groups.
8. Click OK.
Outliers
Data is split
in quartiles
Median

36
Problem Solving and Data Analysis using Minitab 
9. Select ‘Zone1 Gas’ 
and then ‘Zone2 Gas’ 
as the variables.
10. Select Stage as the 
Categorical Variable
11. Then click OK.
The boxplot is displayed. 
From the graph we could 
conclude that the main-
tenance activity has not 
impacted gas usage in the 
same way it did electricity 
usage.
12. Right click on the 
graph panel.
13. Go to <<Add 
<<Reference Lines.
14. Enter 3100 into the 
upper cell to produce 
a horizontal refer-
ence line on the box-
plot. Then Click OK. 
A value of 3100 has 
been selected to be 
the target for the 
individual zone gas 
usage.

Minitab Navigation
37
The following steps change the 
colour of the reference line to 
red and also its thickness.
15. Right click on the refer-
ence line. Click on Edit Y 
Ref Line.
16. Alternatively, double click 
on the reference line.
17. On the Attributes tab 
change the radio button 
to Custom.

38
Problem Solving and Data Analysis using Minitab 
18. Select red as the colour and 
5 as the size.
19. Click OK.
We will add the mean values of 
the data onto the graph together 
with a symbol which denotes the 
position of the mean.
20. Right click on the graph 
panel.
21. Click on Add <<Data Labels.
22. Click onto the Means tab.

Minitab Navigation
39
23. Click on ‘Use y-value 
labels’.
24. Click OK.
25. Right click onto the graph. 
26. Select Add <<Data Display.
The Add Data Display menu 
box opens. It shows all the 
items that can be added onto 
the graph. The default items 
for this chart are already ticked.

40
Problem Solving and Data Analysis using Minitab 
27. Select Mean symbol.
28. Click OK.
The graph should look like 
this. Notice that boxplots dis-
play the median symbol auto-
matically but not the mean. 
2.12 Bar Charts
Let’s now have a look at Minitab’s Bar Charts. They are especially useful when we start to use the inbuilt 
functions. Let’s say that we wanted to produce a basic Pareto chart showing the standard deviation 
(I will abbreviate this to StDev) of total gas usage from each of the operators. Let’s have a look how we 
would do this in Minitab and see why it is probably easier than producing the same graph in a spread-
sheet program.
1. Click on Graph <<Bar 
Chart

Minitab Navigation
41
2. Change the selector that appears 
to A function of a variable.
3. The bar graph that we will 
be producing will be a One Y, 
Simple.
4. Click OK. 
5. From the Function selector box 
select StDev.
6. Select Total Gas as the Graph 
variables.
7. Select 
the 
column 
heading 
Person for Categorical variable.
8. Then Click on chart options.

42
Problem Solving and Data Analysis using Minitab 
9. In order to make this 
a Pareto chart, where 
the value of the bars 
decreases as you read 
from left to right, 
we need to select 
Decreasing Y. 
10. Click OK and OK again 
in order to produce 
the Pareto Chart.
Note that Minitab has a 
specialised 
Pareto 
chart 
which is listed under Stat 
<<Quality Tools.
The Pareto chart of Total 
Gas standard deviation by 
operator is displayed. It 
can be seen that Woody 
has the highest StDev and 
Pilch had the lowest for 
this period of operation.
2.13 The Layout Tool
We have produced a lot of graphs within the same worksheet. Minitab gives a tool to display and print 
multiple graphs on the same sheet. It is called the Layout Tool. It is a very useful tool for the collation 
of graphs.
1. Click on the Show Graphs Folder icon.
2. Select any graph by double clicking it within the Project 
Manager. Then left click onto the graph within the Graph 
Window to select it.

Minitab Navigation
43
3. Select Editor 
<<Layout Tool.
Note: if the graph is not 
selected the Layout Tool 
will not be selectable. 
The layout tool will 
appear. It already con-
tains the graph that you 
just selected. 
In the top left is the grid 
selection for the layout.
Below that is a list of 
graphs 
that 
can 
be 
selected for the layout.
4. Select three other 
graphs 
for 
the 
Layout.
5. Then click on 
Finish.
A new graph appears 
with our four selected 
graphs 
as 
panels. 
It 
should be noted that 
the graphs can still be 
edited but they cannot 
be updated.
The name of the graph 
appears as Layout in the 
Project Manager. It can 
be renamed by right 
clicking on Layout within 
the Project Manager.
www.allitebooks.com

44
Problem Solving and Data Analysis using Minitab 
2.14 Producing Graphs with the Assistant
In this section we are going to have a quick look at using the Assistant to produce graphs. As stated earlier 
this is an M16 feature. If you have M15 don’t feel too left out as all of the graphs and procedures accessed 
via the Assistant have their equivalents which are available via the dropdown menus. The Assistant offers 
a very graphical method of selection which also gives a lot of guidance about the selection.
1. Click on Assistant 
<<Graphical Analysis
The selector menu for  Graphical 
Analysis opens. There are three 
sections and so far we have only 
looked at graphs within the 
Distribution of Data section. We 
will be covering Variables over 
Time and Relationships between 
Variables in later chapters. If you 
know the type of graph that you 
want, you can click on the graph 
type and open the data entry 
menu box straight away. Let’s 
pretend we need help to decide.
2. Click on Help me choose 
within the Distribution of 
Data section.
We are presented with a decision 
tree to help us choose the type of 
graph that we want. However, if 
we don’t understand the terms 
being used within the decision 
tree the Assistant will give us fur-
ther help by explaining the terms 
and taking us through the deci-
sion tree.
3. Click on Click to start above 
the Data type decision icon.

Minitab Navigation
45
In the previous ﬁgure, the 
ﬁrst decision that had to 
be made was whether the 
data that we were going 
to graph was Continuous 
Data or whether it was 
Count of Summary Data. 
The Assistant explains both 
choices within the decision. 
We must click Next below 
the appropriate choice.
4. Follow the decisions 
through to get to Bar 
Chart. You will need to 
select Count/Summary 
Data, Compare data 
across categories and 
then Bars.
5. Note: even at this 
point we can press the 
cross icons to get addi-
tional information if 
we don’t understand 
any of the terms or 
requirements. 
Press 
the icon to create the 
graph.

46
Problem Solving and Data Analysis using Minitab 
Using this menu we will pro-
duce a Pareto chart of Total 
Gas. Note that our selection 
in Bar Charts does not include 
the options that we used pre-
viously, that is Functions of a 
variable.
6. Complete the menu box 
as shown and click OK. 
A Pareto chart of Total Gas is 
produced.
2.15 Producing Reports
In this section we are going to see how information can be exported to a Word Processing program such 
as Microsoft Word in order to produce reports and increase accessibility for others. You will need to have 
the appropriate program available on your PC.

Minitab Navigation
47
1. Select any graph and 
right click on the graph 
panel.
2. Click on Append Graph 
to Report. We can then 
start 
selecting 
other 
items for the Report Pad. 
Data/calculations 
from 
the Session window can 
also be appended to 
the Report pad by right 
clicking.
3. When you have ﬁn-
ished sending items to 
the Report Pad click the 
Show Report Pad icon.
4. Right click onto the 
Report Pad folder icon 
and then select Copy to 
Word Processor or Save 
Report As…

48
Problem Solving and Data Analysis using Minitab 
5. The contents of the Report 
Pad can now be copied into 
a word processer in .rtf for-
mat and subsequently con-
verted to .doc format for 
Microsoft Word.
If you are lucky enough to have Minitab 16 you can export directly to Microsoft Word or PowerPoint. 
You will need to have the appropriate program available on your PC.
6. Select any graph and right 
click on the graph panel.
7. Click 
on 
Send 
Graph 
to Microsoft Word or 
PowerPoint. This action 
will export the graph or 
data immediately. 
I have put some data into the 
Session Window and shown 
how that is exported as well. 
If you have not got Word or 
Powerpoint open, Minitab will 
open it for you.

Minitab Navigation
49
2.16 Creating a New Project/Worksheet Button
Finally, after every example and exercise you will need to start afresh and open a new project. I am not 
going to tell you how to save your work because that is the same as in any other work based program. 
However, to make things easier we will install the New Project/Worksheet button into our toolbars.
1. Click Tools <<Customize
2. On the Commands tab ensure File is 
selected in Categories. Find the New 
icon and simply drag it into your toolbar. 
I have placed it next to the Open Project 
icon on my copy of Minitab.
3. Click on the New icon and select Minitab 
Project. If you have not already saved 
your work you will be prompted to do 
so. After which you will begin afresh 
with a blank project.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 3
Basic Statistics
3.1 Types of Data
When we start looking at any system we need to get a quick grasp of the data that the system is work-
ing with, as this dictates the tools and techniques that we will use to analyse the system. We may even 
decide that the quality of the data is too low to understand process behaviour and then the initial step 
would be to change how we monitor the process.
Continuous data gives us the most value and that is why most modern measuring systems will produce 
continuous data to the highest possible resolution. We will only focus on integers and real numbers for 
that reason. 
3.2 Central Location
In order to demonstrate the next two concepts, Measures of Central Location and Measures of Dispersion, 
we are going to use the following continuous data set:
7.2, 3.0, 3.0, 9.1, 4.0, 5.0, 9.0, 6.3, 2.0 
There are N = 9 numbers within our data set. The data set is our complete set of values so we call it 
our population. We say that we are using descriptive statistics because we have the values of the entire 
population within our calculations.
Mean, Median and Mode are three very common measures of central location that you will know already.
Mean: The mean, µ, is the result when all of the values are added together and then divided by the 
number of values within the data set, N.
Discrete
Continuous
Real Number, e.g. 3.14
Integer, e.g. 3
Categorical or
Attribute
Binary, e.g. Pass or Fail 
>2 Categories, e.g. Strong,
Medium, Weak.

Basic Statistics 51
m =
+
+
+
+
+
+
+
+
=
7 2
3 0
3
9 1
4 0
5 0
9 0
6 3
2 0
9
5 4
.
.
.
.
.
.
.
.
.
Equation (3.1)
Median: The midpoint in a string of sorted data. If we sort our data in ascending order we get:
2.0, 3.0, 3.0, 4.0, 5.0, 6.3, 7.2, 9.0, 9.1
K
5.0 is the median as it is at the midpoint of the ordered data. If there is an even number of elements 
within the data set then the mean of the middle two numbers is taken to be the median.
Mode: The most frequently occurring value. The mode is 3.0 as it occurs twice within the data set and all 
the other numbers only occur once.
Depending upon which measure of central location that we decide to use, our value for central location 
could be 5.4, 5.0 or 3.0.
3.3 Dispersion
Moving onto Measures of Dispersion. The easiest of these is Range.
Range is the difference between the largest and the smallest of the elements within the data set.
Data Set = 7.2, 3.0, 3.0, 9.1, 4.0, 5.0, 9.0, 6.3, 2.0 
Range = Max Value – Min Value = 9.1 – 2.0 = 7.1
Equation (3.2)
Let’s have a look at how data is distributed about the mean, µ.
The deviation is the distance 
from each point to the mean. 
By deﬁnition the sum of the 
deviations is zero.
Deviation = (X – µ)
Equation (3.3)
 
Sum of       
Deviations 
Squared    
2
=
=∑(
)
xi
x
N
_ µ
1
Equation (3.4)
5
5.4
6
7
8
9
4
3
2

52 Problem Solving and Data Analysis using Minitab 
The Sum of Deviations Squared may also be called the sum of squares. Note that we are using the sum 
of squares procedure on the deviations.
Variance
2
2
=
= ∑
σ
µ
(
)
xi
N
_
=
x
N
1
Equation (3.5)
StDev.P
2
=
= ∑
σ
µ
(
)
x
N
i
x
N
_
=1
Equation (3.6)
Remember these are equations for population parameters. StDev.P stands for population standard 
deviation.
The table below shows how the StDev.P for our data set is derived manually.
Data Set
Mean
Deviation
Deviation 
Sqr
Sum of Sqrs  
of  
Deviations
Variance
StDev.P
7.2
5.4
1.8
3.24
55.9
6.21
2.49
3.0
–2.4
5.76
3.0
–2.4
5.76
9.1
3.7
13.69
4.0
–1.4
1.96
5.0
–0.4
0.16
9.0
3.6
12.96
6.3
0.9
0.81
2.0
–3.4
11.56
3.4 Descriptive Statistics
Descriptive statistics are for the case where we have data on a complete population. We analyse that 
complete data set which then gives us information to describe the population. Let’s have a look at 
descriptive statistics within Minitab.
Describe
population
Analyse
Finite
population

Basic Statistics
53
This is a short exercise on Descriptive Statistics. We 
are going to import some data into Minitab and 
then demonstrate how to use Descriptive Statistics. 
We will be using this data for the remainder of the 
chapter so you do not need to start a new project 
after every example/exercise. 
1. Open the Excel ﬁle ‘03 Basic Stats.xls’ and go 
to sheet 1.
2. Import the four columns into Minitab by copy-
ing and pasting.
3. Check that the data has been correctly trans-
ferred into Minitab. All columns should be 
numeric.
4. Click on the column information button.
5. The data type for all four columns is numeric, 
as indicated by the N under Type.
www.allitebooks.com

54
Problem Solving and Data Analysis using Minitab 
6. Click Stat <<Basic Statistics 
<<Display Descriptive 
Statistics.
7. Complete the menu box as 
shown.
8. Click on Statistics
9. The menu box shows the 
range of statistics that are 
available. The default sta-
tistics are already ticked. 
Change the selection to the 
one shown.
10. Click OK and then click on 
Graphs.

Basic Statistics
55
11. The menu box shows 
the four graphs that 
are available. On this 
occasion we are not 
going to produce a 
graph.
12. Then click OK and OK 
again.
13. The results for the 
selected statistics are 
shown in the Session 
Window which opens 
automatically.
Notice that the data is 
too wide for the Session 
Window and Minitab has 
to start a new table. We 
can change the default 
session window width.
14. Click on Tools 
<<Options.
15. Double click on 
‘Session Window’.

56
Problem Solving and Data Analysis using Minitab 
16. Then double click on ‘Output’.
17. Change the Line width from 79 
to 132, where 132 is the maxi-
mum. Then click OK.
18. Click on the Edit Last Dialog icon.
19. Then click OK to display the same 
selection of Descriptive Statistics.
Notice that Minitab can now display 
all the data in a single table.

Basic Statistics
57
Now that we know how to use Descriptive Statistics within Minitab 
we will use this procedure on the data set that we used when learn-
ing about measures of dispersion and central location.
1. Open the Excel ﬁle ‘03 Basic Stats.xls’ and go to sheet2.
2. Import the single columns into Minitab. Instead of opening a 
new project just put it into column 5 of the current worksheet.
3. Click on the Edit Last Dialog icon.
4. Press F3 to clear the previous input. Then select Data Set as the 
required Variables.

58
Problem Solving and Data Analysis using Minitab 
5. Click on Statistics.
6. Change the selection to that 
shown. This will calculate 
the items that we manually 
calculated for our Data Set.
7. Then click OK and OK again.
8. Look at the results produced 
within the Session Window 
and compare those with the 
numbers that we calculated 
earlier.
We see that the mean is the same; 
however, all the other terms that 
were calculated are different. It is 
worth trying to understand why 
this is. The sum of squares (SS) is 
different because initially we cal-
culated SS for the deviations and 
now we have just calculated the 
SS of the data set, not the devia-
tions. The variance and StDev 
are different because we initially 
calculated them as population 
parameters; however, Minitab 
has calculated them as Sample 
parameters. This leads us on 
nicely to look at the difference 
between population and sample 
statistics and also descriptive and 
inferential statistics. 
Mean
Sum of Sqrs 
of Deviations
Variance
StDev.P
5.4
55.9
6.21
2.49

Basic Statistics
59
3.5 Inferential Statistics
Imagine that we have a factory where we are monitoring parameters like weight, ﬂow, concentration 
and so on. When we start to capture data we will probably not take the complete data set but only take 
a small fraction of it. We will then analyse our captured data and then we will want to infer something 
about the population. This is where sample or inferential statistics come in. For example, let’s say that 
our factory is making 100 000 widgets every day. We take a sample of 500 widgets in the day. We weigh 
all the widgets and ﬁnd that the average weight is 25 g. The average weight of the 500 samples widgets 
is 25 g; and that’s descriptive statistics. We then say that all the widgets weighed by the factory have an 
average weight of 25 g; and that’s inferential statistics because we have not checked the weight of the 
complete population.
Usually it helps us to randomly select our sample from our population. However, when you use sample 
statistics to infer something about the population there is a chance that you will be wrong. Therefore, 
we use conﬁdence intervals to let us know how likely we are to be correct. 
Take sample
of population
Analyse
sample data
Use results to infer
something about
population
Population
data

60
Problem Solving and Data Analysis using Minitab 
3.6 Conﬁdence Intervals
If you worked your way through Chapter 2 you will have seen conﬁdence intervals within the Graphical 
Summaries that were produced. In the ﬁgure above, Minitab is showing us conﬁdence intervals for the 
Mean, Median and Standard Deviation. This means that from the 37 data points provided Minitab is 
calculating that the population will have a Mean between 6300.6 and 6407.9. It is 95% conﬁdent that 
this estimate for the population is correct. The mean for the 37 data points is 6354.2.
Conﬁdence Interval for Mean = Mean r Conﬁdence Factor u Variability
Equation (3.7)
Minitab will calculate all the required conﬁdence intervals for you. It is useful to know how a conﬁdence 
interval is put together so you can appreciate the factors that affect the size of the conﬁdence interval, 
see Equation (3.7).
The Conﬁdence Factor is based on our initial statement of how conﬁdent we want to be in our estimate. 
The default for Minitab is 95% (there will be a lot more on this in the next chapter) but the Conﬁdence 
Factor is also based on sample size. The greater the sample size the lower the value of the Conﬁdence 
Factor.
The Variability is based on the sample standard deviation divided by the number of samples square 
rooted.
Assuming we keep our conﬁdence level at 95%, you can see that a large sample size and low standard 
deviation within the sample will make the conﬁdence interval smaller.
Increasing the conﬁdence level, to say 99%, and keeping everything else the same will result in a larger 
conﬁdence interval. 
Statistical data
Data with
confidence
intervals
Mean and Median confidence intervals

Basic Statistics
61
Example of Conﬁdence Intervals
At the S Todd Beans factory they ﬁll beans into cans so that the average amount of beans per can is 
nearly 100 g. They decide to weigh a random sample of 20 cans, in order to determine what the aver-
age of all beans produced might be. Let’s say that the data for 20 randomly selected cans is listed in the 
column called Exercise3 in the data that you previously imported into Minitab earlier this chapter.
What is a 95% conﬁdence interval for the true mean ﬁll using the data provided? 
1. Click Stat <<Basic Statistics <<Graphical 
Summary.
2. Select Exercise3 as the variable, then 
press OK.

62
Problem Solving and Data Analysis using Minitab 
The 20 data points have a mean of 101.68. The 95% conﬁdence interval for the mean is between 99.69 
and 103.67. This means that we can be 95% conﬁdent that the mean for all cans of beans will be 
between 99.69 and 103.67 g.
Before learning about inferential statistics the incorrect procedure would have been to sample 20 cans 
and then state that the mean of the population was 101.68 g.
3.7 Normal Distribution
One of the fundamentals within 
statistics is to consider the type 
of distribution that we get from 
our data. Distribution means the 
shape of the histogram we would 
get where the y axis is frequency.
The 
Normal 
Distribution 
(or 
Gaussian distribution) is particu-
larly important because it forms 
the basis for a number of test 
procedures, it shows up in the 
natural world and manmade 
world and it is an outcome of the 
Central Limit Theorem. Another 
important point to remember is 
that the Normal Distribution is 
not the only type of distribution. 

Basic Statistics
63
Slight departures from the normal distribution, like skewness and kurtosis, can give us important 
information about why our process is not preforming as expected.
The normal distribution shown in the ﬁgure has a mean of 200 and a standard deviation of 10. 
Additionally, the normal distribu-
tion can be described by a num-
ber of generic rules.

X It is described by its mean 
and standard deviation.

X The tails extend to ± inﬁnity.

X All observations will appear 
under the curve.

X The curve is symmetrical.

X The 
mean, 
mode 
and 
median are almost equal.
68.27% of the data is within one 
standard deviation of the mean.
95.45% of the data is within two 
standard deviations of the mean.
99.73% of the data is within 
three standard deviations of the 
mean.
3.8 Deviations from Normality
Left or Negative skewed data.
Right or Positive skewed data.
The mean will be to the left of the Median.
The mean will be to the right of the Median.
Above, two distributions are shown where there have been slight departures from normality, for what-
ever reason. One of the distributions is left skewed and the other right skewed. If we wanted to improve 
the process one of the ﬁrst steps might be to ﬁnd out what is causing the skewed distributions if a nor-
mal distribution was expected.
68.27%
95.45%
99.73%
www.allitebooks.com

64
Problem Solving and Data Analysis using Minitab 
Another departure from normality that we sometimes see is the bimodal distribution. This is actually two 
normal distributions joined together. It could be that during the data collection a change has occurred 
that resulted in two different processes. The data should only contain one distribution. Sometimes this 
can be hard to see if the two separate distributions are close together.
Kurtosis is also a measure of how 
a distribution can deviate from 
normality. 
Kurtosis is a measure of how ﬂat or 
peaked a distribution is. We previ-
ously said that a normal distribu-
tion has 68.27% of the population 
within 1 standard deviation of the 
mean. A ﬂat distribution will have 
less than 68.0% and a peaked distri-
bution will have more.
A normal distribution will have a 
kurtosis of 0. A totally ﬂat distribu-
tion will have a kurtosis of –1.2.
Norm
al distribution
Kurtosis = 0.02
Flat Distribution
Kurtosis = –1.22

Basic Statistics
65
It is important for us to be able to recognise a normal distribution as it tells us how to advance with a 
number of test procedures. There are two main ways of doing this; one is to use a Normal Probability 
Plot and the other is to use the Anderson–Darling (AD) Normality Test. Both the Normal Probability Plot 
and the AD test result are given in Minitab’s Normality Test Procedure. Additionally, the result of the AD 
test is given in the top left corner of the Graphical Summary.
The normal probability plot is a graphical technique. In order to establish if the distribution is normally 
distributed, check the straightness of the line in the plot by applying ‘the Fat Pencil Test’. If you can cover 
all of the points on the line using a fat pencil then the distribution is deemed to be normal.
As stated earlier the results of the AD Normality Test are given on the Graphical Summary, which again 
highlights the usefulness of the Graphical Summary. If the P value from the Anderson–Darling test is 
greater than our signiﬁcance level (usually 0.05 by default), then the data can be treated as normal.
There are some rules:
The Anderson–Darling test is not robust for small sample sizes. For samples less than 20, use ‘the Fat 
Pencil’ test. If a fat pencil can cover all of the points on the normal probability plot, the data can be 
treated as normal; but be wary of the results. Remember always protect the customer.
Peaked Distribution
Kurtosis = 3.83

66
Problem Solving and Data Analysis using Minitab 
We are now going to run through a few normality 
tests and apply the rules that we have just learnt. 
Check that you have still got the data shown overleaf 
loaded in the project window. If not, open Excel ﬁle 
03 Basic Stats.xls and import the data into Minitab.

Basic Statistics
67
1. Click Stat <<Basic Statistics 
<<Normality Test.
2. Select 
Normal 
as 
the 
Variable to be tested.
3. Then click OK.
The data looks reasonably 
straight and we are happy that 
we can cover all the points 
with a ‘Fat Pencil’. Therefore, 
we conclude that data within 
the column labelled Normal is 
likely to have come from a nor-
mally distributed population.
The AD P value is also greater 
than 0.05, also indicating nor-
mality. As an exercise test the 
other columns. Carry out the 
normality test for the remain-
ing three columns, then pro-
duce a graphical summary for 
all four columns. Are they nor-
mally distributed? Does the 
assumption of normality carry 
any risk?

68 Problem Solving and Data Analysis using Minitab 
Non Normal
Distribution
Non Normal
Distribution
Passes F at Pencil &
Anderson-Dar
ling

Basic Statistics
69
Bim
odal
Distributions
Positive Skewed
Distributions

70 Problem Solving and Data Analysis using Minitab 
3.9 Central Limit Theorem
The Central Limit Theorem (CLT) is the foundation of Hypothesis Testing. The CLT states that, if you have 
a non normal distribution and you take the mean of a set number of samples and keep repeating this 
process to build up a distribution of means, the resultant distribution of means will be more normally 
distributed and have a narrower distribution than the parent.
S
N
=
s
Equation (3.8)
Where S = sample StDev, V = population StDev and N = the number of samples taken each time. We are 
not going to be using the CLT directly but it is worth knowing of its existence.
Normal but carries small risk.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 4
Hypothesis Testing
Hypothesis Testing is one of the foundational chapters of the statistical procedures that we are going 
to be using.
In the last chapter we learnt how we could take a sample from a population, analyse that sample, apply 
conﬁdence intervals and then infer some properties of the population from our sample data. Hypothesis 
Testing builds on this by helping us make speciﬁc decisions about our population from the sample data.
Hypothesis Testing is vital because in the real world we should be much more interested in the popula-
tion even though it is the sample data that helps us understand the population. It is essential in problem 
solving to know what the population is doing. Imagine claiming a major breakthrough in a project 
based on sample data without having made any checks on the population. What if the conclusions you 
made about the sample data did not apply to the population data. You would then have to explain to 
the Boss why the project was not delivering as you claimed.
Using the Hypothesis Testing procedure we are going to learn how to test a population is performing 
against a speciﬁc mean. Then we are going to start comparing variances and means for two populations 
in the same manner. 
All the data ﬁles for this chapter are in ﬁle ‘04 Hypothesis Testing.xls’
Hypothesis Testing follows the procedure shown above. We will start by looking at the main steps in the 
procedure to build our knowledge before we go into the examples. When we start looking at the exam-
ples our journey within the procedure will be highlighted. After a couple of goes it will become second 
nature and we will stop using the ﬂowchart. For Minitab 16 users the Assistant feature instantly does it 
all for you, including the power and sample size procedure, but it is still worth knowing the traditional 
way or Classic Method. We need to be aware that sometimes the Assistant will use a slightly different 
test procedure to the one used within the Classic Method and we need to be aware that our assump-
tions and test limitations may be slightly different in each case.
1. Define the
problem and
state
objectives.
2. State a
Null and
Alternate
Hypothesis.
4. Conduct
test and
calculate
P value.
3. Establish
risks, then
conduct
Power and
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.

72
Problem Solving and Data Analysis using Minitab 
Steps 1 and 2 are fairly straightforward and will become second nature once practiced a few times.
It was stated in the last chapter that there is a risk of being wrong about the decision you make because 
the sample you took was not representative of the population. We will be developing the risk model in 
more detail by looking at a procedure called Power and Sample Size (P&SS) in Step 3.
Step 4 is the actual test procedure. This will vary slightly from test to test and you will even need to use 
a test within a test when comparing the means of two populations. Don’t worry though, by then it will 
be second nature.
Finally, Step 5 is making the decision with regard to the result. You will learn the phrase ‘If the P is low, 
the null must go.’ You will use this mantra throughout your statistical career, so it is worth mentioning 
early within the text.
4.1 The Problem Statement
This is a statement of the problem that you are looking to solve and it will have only two possible out-
comes. It will show that you have knowledge of the test procedure that you are going to apply and the 
type of data that you have. A succinct sentence is sufﬁcient. Examples are:
t Is the mean of Group 1 equal to that of Group 2, or is it different?
t Could the variance of Group 1 be the same as that of Group 2, or is it different?
t Could the mean of this population be equal to 230, or is it different?
All the problem statements shown are for two sided tests. If we take the top example, we want to know 
if the means are the same or different. To be different, the mean of Group 2 has to be either greater 
than or less than that of Group 1. An example of a one sided test would be: is the mean of Group 1 less 
than or equal to that of Group 2, or is it greater than the mean of Group 2? The advantage of a one 
sided test is that it will have a greater power than the equivalent two sided test.
4.2 Null and Alternate Hypotheses
There are two possible outcomes to our Problem Statement and at this point we don’t know which is 
correct. From our Problem Statement we must decide which of the outcomes is the Null Hypothesis and 
which is the Alternate Hypothesis. The Null Hypothesis (Ho) states that either there is no difference, or 
the parameters being investigated are equal. The Alternate Hypothesis (Ha) states that there is a differ-
ence, or the parameters being investigated are not equal. Previously, one of the examples of a problem 
statement was ‘Is the mean of Group 1 equal to that of Group 2, or different?’
1. Define the
problem and
state
objectives.
2. State a
Null and
Alternate
Hypothesis.
4. Conduct
test and
calculate
P value.
3. Establish
risks, then
conduct
Power and
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.

Hypothesis Testing
73
The Null Hypothesis would be the means of Group 1 and Group 2 are equal.
Or   Ho :  µGroup1 = µGroup2
The Alternate Hypothesis would be the means of Group 1 and Group 2 are not equal.
Or   Ha : µGroup1 ≠ µGroup2
Consider a one sided test with the following problem statement: ‘Is the mean of Group 1 less than or 
equal to that of Group 2, or is it greater than that of Group 2?’
The Null Hypothesis would be the mean of Group 1 is less than or equal to that of Group 2.
Or   Ho : µGroup1 ≤ µGroup2
The Alternate Hypothesis would be the mean of Group 1 is greater than that of Group 2.
Or   Ha : µGroup1 > µGroup2
4.3 Establishing the Risks
Let’s say the two possible outcomes to our Problem Statement are: (1) no difference (aligned to Ho)
and (2) difference (Ha). We need to assign a risk, D, of incorrectly saying there was a difference when 
there was none. D is called the signiﬁcance and 1 – D is called the conﬁdence. The default value used in 
Minitab for D is 0.05 and this is where we get our 95% conﬁdence as a default. There will be an option 
within the test menus for you to change this value. Put another way, the D risk is the risk of the instance 
when the data supports the rejection of the Ho, but it is wrong. This risk of a false positive or Type I error 
is also sometimes called the ‘Producer’s Risk’.
1. Define the
problem and
state
objectives.
2. State a
Null and
Alternate
Hypothesis.
4. Conduct
test and
calculate
P value.
3. Establish
risks, then
conduct
Power and
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.
1. Define the
problem and
state
objectives.
4. Conduct test
and calculate
P value.
3. Establish
risks, then
conduct Power
and Sample
Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.
2. State a Null
and Alternate
Hypothesis.

74
Problem Solving and Data Analysis using Minitab 
The E risk relates to the instance when there really is a difference. The E risk is the chance of the test 
incorrectly concluding that there was not a difference when there actually was a difference. The risk of 
a false negative or Type II error is also sometimes called the ‘Consumer’s Risk’. Statistically speaking, the 
data states that we should not reject the Ho, but in reality we should reject the Ho. The term ‘Power’ 
is given by 1 – E. Typically, we want a power of around 80–90% for our test procedures. The power of 
the test is the probability of correctly concluding there was a difference when in reality there was a 
difference.
The Signiﬁcance, D risk, will be set within the test procedure. Changing the D risk will change the conﬁ-
dence intervals of the test statistic.
The E risk or Power is calculated prior to testing within the Power and Sample Size Procedure.
It is easier to see how the D and E risks are derived in the following matrix.
Just to reinforce a couple of the terms from the matrix above:
x D risk:  Risk of ﬁnding a difference when there really isn’t one (false positive).
x E risk:  Risk of not ﬁnding a difference when there really is one (false negative).
Reality, or  the Truth
There is a
difference
The decision you make, or your opinion
There is not a
difference
There is a
difference
Type I Error
a risk or Producers risk
Correct Decision
Power = 1 – β
There is
not a
difference
Right
Right
Wrong
Wrong
Type II Error
β risk or Consumers risk
Correct Decision
Confidence = 1 – α

Hypothesis Testing
75
t Unless we test the complete population we will never know the truth. Therefore, we try and stack 
the odds in our favour by having high values of conﬁdence and power.
The ﬁgure below shows the same matrix with the axis now referring to the null hypothesis versus our 
decision.
Remember the D and E risk are set by you. The default or recommended values need to be changed if the 
consequence of the risk is unacceptable. If injury was the consequence I would hope that a conﬁdence 
level of 95% was too low.
We will now go into an example showing the Power and Sample Size Procedure. Prior to showing the 
test procedures for the ﬁrst time there will be a generic table detailing what the procedure is used for 
and its assumptions and limitations.
False
The decision you make, or your opinion
Fail to Reject
Reject
Type I Error
α risk or Producers risk
Correct Decision
Power = 1 – β
True
Right
Right
Wrong
Wrong
Type II Error
β risk or Consumers risk
Correct Decision
Confidence = 1 – α
Null Hypothesis

76
Problem Solving and Data Analysis using Minitab 
4.4 Power and Sample Size
Example 1. Power and Sample Size for a 1 sample t test.
Prawn video games want to establish the mean time to ﬁnish the ﬁrst level in one of their games. They 
will use a 1 sample t test but initially they want to calculate the power of the test.
At a signiﬁcance level of 5% they want a power of 80% and want to be able to detect a difference of ±5 
min. They have resources to conduct 14 independent tests. And they know that the historical standard 
deviation is 7 min.
No ﬁle required. Note: for the purposes of demonstrating P&SS we will not deﬁne the problem state-
ment and will not list the null and alternate hypothesis for this problem.
1. Click Stat <<P&SS << 1-Sample t…
Procedure
t Power & Sample Size Testing for a 1 sample t test.
What's it 
used for?
t Given the standard deviation and another two out of the three 
following factors; the number of samples required, the power 
of the test or the detectable difference the procedure is used to 
calculate the third missing factor.
Assumptions
and
Limitations
t The standard deviation is required for this procedure. Use 
historical data to calculate the StDev. If that is not available it can 
be calculated from test data. Failing that an estimate will have to 
be made which would then be checked against the test data when 
that is available.

Hypothesis Testing
77
2. Complete the menu box 
as shown. Note that 
‘Differences’ refers to 
detectable 
difference. 
Also note that, as long 
as StDev is available, 
only two out of the 
other three factors are 
required and the third is 
calculated. In this exam-
ple we will be calculat-
ing power so we leave 
it blank.
3. Then click OK.
Minitab gives us a graphical 
output and also has the same 
information reﬂected within 
the session window. 
Power is on the y axis and 
the 
detectable 
difference 
is on the x axis. The curve 
shows how the power would 
change if we speciﬁed a dif-
ferent ‘difference’, that is a 
difference of 6 or –6 would 
give a power greater than 
80%. It can also be seen that 
the power curve is two sided 
as it caters for both a positive 
difference and a negative 
difference. 
Hovering the mouse over the 
red dot, which represents our 
point of interest, displays the 
actual power with a differ-
ence of ±5.
Example 1 continued. Power and Sample size for a 1 sample t-test.
Prawn video games realise that only a power of 70% can be achieved with the 14 tests but they don’t 
have the resources to conduct more tests. Someone from the team then suggests that they are only 
really interested if the level is taking longer to complete. Therefore, they should look at using a one 
sided test. 

78
Problem Solving and Data Analysis using Minitab 
They decide that they are only really interested in looking at a difference of +5 min. 
Under the same conditions what power can they achieve now?
1. Click on the Edit Last icon. It should open 
the menu box for P&SS for 1 Sample t.
2. Complete the menu box as shown. 
Remember to change the selection for 
difference. 
3. Then click Options.
4. For the Alternative Hypothesis select 
Greater than. It is by using these radio 
buttons that we make our choice for a 
one sided test. The default selection is 
‘Not equal’ (two sided).
5. Then click OK and OK again.

Hypothesis Testing
79
Prawn video games can achieve a power of over 80% using 14 tests with a one sided test. This means 
that if there is a difference there is an 80% chance of detecting it correctly. As we did not change the 
conﬁdence of the test, that remains at the default level of 95%. Which means that if the null hypothesis 
is correct and there is no difference we will have a 95% chance of detecting it correctly.
The power of a test is as important as the signiﬁcance and should be stated as a part of the results.
It is worth having an understanding of the parameters that affect the power.
Over 80%
Note - this
is a one
sided test.

80
Problem Solving and Data Analysis using Minitab 
The interaction chart shows the effects on Power of changing a number of the key parameters available 
within test procedures.
4.5 Conducting the Test and Evaluating the Results
Now that we understand about power and sample size we are going to look at how to conduct the tests. 
We will learn: 
t 1 Sample t Test for comparing the mean of a population to a target value.
t Paired t Test for comparing the means of two dependent populations.
t 2 Variances Test for comparing the levels of variation of two independent populations.
t 2 Sample t Test for comparing the means of two independent populations.
1. Define the
problem and
state
objectives.
2. State a
Null and
Alternate
Hypothesis
4. Conduct
test and
calculate
P value.
3. Establish
risks, then
conduct
Power and
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.

Hypothesis Testing
81
We will use examples to consider each of the tests using the standard procedure shown above. Then for 
M16 users we will show how the Assistant would tackle the example. There will of course also be some 
exercises along the way.
Prior to conducting the hypothesis tests it is always wise to take the opportunity to view the data using 
the Graphical Summary. It’s good to get a feel for the numbers and the shape of the distribution and the 
impact that these will have on the accuracy of the test that you are about to conduct.
The result of the test will be shown in the session window. The result will take the form of a value again. 
In statistical terms:
t If the P value is greater than our signiﬁcance level (usually 0.05) we conclude that there is insuf-
ﬁcient evidence to reject the null hypothesis. 
t If the P value is less than our signiﬁcance level (usually 0.05) we conclude that there is sufﬁcient 
evidence to reject the null hypothesis and accept the alternate hypothesis.
I was taught ‘If the P is low, the null must go’ and this works well in the practical world.
Now that we know how to evaluate the test results let’s go back to our example of the video game 
manufacturer.
4.6 One Sample t Test
Procedure
t1 Sample t Test
What's it 
used for?
t This test is used to assess a population against a target mean.
Assumptions
and
Limitations
t The following assumptions must be true about our sample data
t The sample data must be randomly selected from the 
population.
t Data must be continuous and uni model (a single distribution)
t Sample data is likely to be from a normally distributed 
population
t This test is fairly robust to non normal data as long as the sample 
size is greater than 20.

82
Problem Solving and Data Analysis using Minitab 
Example 2. 1 Sample t-test.
Prawn video games want to establish if the mean time to ﬁnish the ﬁrst level from one of their games 
is greater than 200 min. They have already conducted the power and sample size procedure and were 
satisﬁed that the power of the 1 sample t test was going to be 80%. That is with a one sided test, sample 
size of 14, detectable difference of 5 min and standard deviation of 7 min.
The manufacturer conducts the 14 independent play tests and records the results. Establish if the mean 
completion time of the population is likely to be more than 200 min from the results of the 14 play tests?
Worksheet: Level Time.
Problem Statement
Establish if the mean completion time of the population is likely to be more than 200 min from the 14 
play test results?
Null Hypothesis: Population Mean ≤ 200 min
Alternate Hypothesis: Population Mean > 200 min
We have already established ` and a
Prior to conducting the test procedure we must ensure that 
the sample data meets the requirements for the test. The 
best way of doing this is to look at the Graphical Summary.
1. Open worksheet Level Times and transfer the data into 
Minitab.
2. Examine the Graphical Summary by clicking Stat <<Basic 
Statistics <<Graphical Summary. 

Hypothesis Testing
83
3. Select C1 and then click 
OK.
The mean for the sample is 
206.8 and we can see the 95% 
CI for the mean.
The AD P value is 0.137 but we 
should be wary as the sample 
size is small so we will carry out 
a normality test.
4. Click Stat <<Basic Statistics 
<<Normality Test.
5. Select C1 as the Variable. 
6. Then click OK.

84
Problem Solving and Data Analysis using Minitab 
The dots can just be cov-
ered with a Fat Pencil so we 
will accept that the sam-
ple came from a normal 
distribution.
Finally we can now start 
the actual test procedure.
7. Click Stat <<Basic 
Statistics <<1-Sample t.
8. Select C1 as the data 
column.
9. Enter 
200 
as 
the 
Hypothesized 
mean, 
this is the value that 
we are testing against, 
200 min.
10. Then click Graphs to 
specify 
the 
output 
graph that we would 
like. If we don’t select 
a graph then the out-
put will only be in the 
session window.

Hypothesis Testing
85
11. Select Boxplot of data.
12. Then click OK.
13. Click Options.
14. Use the dropdown menu to select 
‘greater than’ as the Alternative 
hypothesis. The default is always 
‘not equal’. 
15. Then click OK and OK again.
16. To have a look at the result 
of the test click on the Show 
Session Folder icon.

86
Problem Solving and Data Analysis using Minitab 
17. Locate our test result and 
look at the P value. If it 
is less than the D (0.05) 
then we can reject the 
null hypothesis. Our P
value is 0.00 so we reject 
the null hypothesis that 
the 
mean 
completion 
time is 200 min or less (if 
the P is low the null must 
go!). Note that mean of 
the samples was 206.8 
and the lower 95% inter-
val is 204.46.
18. In order to see this result 
graphically go to the 
graph folder and look at 
the boxplot.
Graphically it can be see that 
Ho is not within the conﬁdence 
interval for the mean. It is for 
this reason we have rejected 
the null hypothesis. We can 
state that the mean comple-
tion time will be greater than 
200 min.
As we requested a one sided 
test, which tested whether the 
mean of the population was 
greater than 200 min, only a 
lower bound conﬁdence inter-
val is shown on the graph.

Hypothesis Testing
87
We are going to have a look at using the Assistant to repeat the 1 Sample t Test. As stated earlier the 
Assistant is an M16 feature. The Assistant offers a graphical selection for the appropriate hypothesis 
test. The Assistant will also give guidance on interpreting the result of the test and additional relevant 
information such as P&SS, normality and outliers within the data.
1. Click on Assistant <<Hypothesis 
Tests.
The tests within the Assistant are 
broken down into three groups: (1) 
tests relating to a single sample, (2) 
tests which have two samples and 
(3) the comparison of more than 
two samples.
2. Click on ‘Help me choose’ 
within the ‘Compare one sam-
ple with a target’ group. 

88
Problem Solving and Data Analysis using Minitab 
We are presented with a decision 
tree to help us choose the type of 
test that we want. We can click on 
the diamonds in order to get more 
information about our selection.
As a reminder, this book does 
not deal with Attribute data as 
Continuous data is far more com-
mon and useful. 
3. Click on the button for the 1 
Sample t Test.

Hypothesis Testing
89
4. Complete 
the 
menu 
box 
as 
shown and click 
OK.
The menu box takes 
the same inputs as the 
previous menus for 
the 1 sample t test.
Note that there is not 
a selection menu for 
the type of graphical 
output that we will 
get. Also, the Power 
and Sample Size selec-
tion only takes in a 
single parameter. This 
is because StDev will 
be calculated from 
the sample data and 
so will sample size.
The Assistant gives 
three output pages: 
the Summary Report, 
the Diagnostic Report 
and the Report Card. 
For the purposes of 
navigation each of 
these pages can be 
handled as a graph.

90
Problem Solving and Data Analysis using Minitab 
On the top left quarter of the summary report the P value is compared to the signiﬁcance level. There is 
an orange marker on a sliding scale indicating the actual P value. 
Below this we see a dotplot of the actual sample data. Graphically we see that 200 is not greater than the 
95% lower bound conﬁdence level for the population mean, 204.46. If 200 had been greater than the 95% 
lower bound conﬁdence level we would not have rejected the null hypothesis. 
On the top right we see the statistics from the test. Note that the StDev of the sample data was 4.94. As 
we are conducting a one sided test Minitab uses a 90% conﬁdence interval to describe the population 
mean. If we conducted a two sided test Minitab would use a 95% CI. 
On the bottom right, within the comments section, you can read the conclusion of the test procedure 
and edit the text.

Hypothesis Testing
91
On the top of the Diagnostic Report we are presented with an SPC chart. The data point that is coloured 
red is deemed to be unusual and may have occurred under unusual circumstances. Minitab is warning us 
that these kinds of events could affect our conclusion. We will be covering SPC in detail within Chapter 7.
The bottom left and right sections of this panel deal with Power and Sample Size. If you relied only upon 
the Assistant you would get these results after you had potentially conducted test runs and had taken 
samples. That could be wasteful if you then found your power to be inadequate.
As we previously stated that a difference of ﬁve was of interest to us, we are told that our power is 
97.3%. This is much higher than the menu driven test because originally we used a historical StDev of 
7.0. Here, the Assistant is calculating the StDev from the samples and using a value of 4.94. To make 
the correct decision about which ﬁgure to quote and use always think about the application and think 
about protecting the stakeholders.

92
Problem Solving and Data Analysis using Minitab 
Finally, we have the Report Card. We are again told that we had one unusual data point. In the real 
world we would investigate if there were any unusual circumstances behind that point.
We get a warning due to our small sample size as it could lead to an inaccurate assessment of normality.
Finally, we get a satisfactory indicator relating to Power and Sample Size. This is because there is a suf-
ﬁcient difference between the mean and the target.
Let’s move onto the next example. We are going to go through that from beginning to end using the 
hypothesis testing procedure. Initially we will use the classic menus. We will then analyse the data with 
the Assistant.
Example 3. 1 Sample t Test.
Parky Bakery wants to know if the mean weight of the buns that they bake is 99 g. They take 50 samples 
and weigh them. If the difference they want to detect is 1.0 g and the historical standard deviation is 
1.5 g what is the power of the 1 sample t test?
Establish if the mean weight of the buns is 99 g?
Worksheet: Buns.
1. Define the
problem and
state
objectives.
2. State a
Null and
Alternate
Hypothesis
4. Conduct
test and
calculate
P value.
3. Establish
risks, then
conduct
Power and
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.

Hypothesis Testing
93
The ﬁgure above shows the Hypothesis Testing procedure again. We will be using the steps to guide 
ourselves through the test procedure.
Problem Statement
Establish if the mean weight of the buns is 99 g.
(We can see that this is a two sided test.)
Null Hypothesis
Population Mean = 99 g
Alternate Hypothesis
Population Mean ≠ 99 g
Calculating the Power
1. Start a new project.
2. Click Stat <<P&SS << 1-Sample t.
3. Complete the menu box as shown.
4. Then click OK.

94
Problem Solving and Data Analysis using Minitab 
We have a power of 0.996. 
This means that if there is a 
difference there is a 99.6% 
chance of detecting it cor-
rectly. As we did not change 
the conﬁdence of the test 
that remains at the default 
level of 95%. Which means 
that if the null hypothesis is 
correct and there is no dif-
ference we will have a 95% 
chance of detecting that 
correctly.
Calculating the Test Statistic
5. Open worksheet Buns 
and transfer the data 
into Minitab.
6. Examine the Graphical 
Summary by clicking 
Stat <<Basic Statistics 
<<Graphical Summary. 
7. Select C1 and then 
click OK.

Hypothesis Testing
95
The mean for the samples is 
100.38 and we can see the 95% 
CI for the mean.
The AD P value is 0.103 which 
indicates this data is from a nor-
mally distributed population
As the sample size is sufﬁciently 
large we do not need to use a 
separate normality test.
We can now conduct the test 
procedure.
8. Click Stat <<Basic Statistics 
<<1-Sample t.
9. Select C1 as the data column. 
10. Enter 99 as the Hypothesized 
Mean.
11. Then click Graphs.

96
Problem Solving and Data Analysis using Minitab 
12. Select Boxplot of Data.
13. Then click OK and OK 
again.
Decision Time
14. Click on the Show 
Session Folder icon.
15. Locate our test result 
and look at the P 
value. If it is less than 
the D (0.05) then we 
can reject the null 
hypothesis.
16. Go to the graph 
folder and look at the 
boxplot.
Graphically it can be see 
that Ho is not within the 
conﬁdence interval for the 
mean.
Since we have rejected the 
null hypothesis we can state 
that the mean bun weight 
is not 99 g. We can say it is 
higher.
We 
will 
now 
run 
this 
example again using the 
Assistant.

Hypothesis Testing
97
1. Click on Assistant 
<<Hypothesis Tests.
2. Click on 1-Sample t within 
the ‘Compare one sample 
with a target’ group.
3. Complete the menu box 
as shown and click OK. 
The menu box takes the 
same inputs as the previ-
ous menus for the 1 sam-
ple t test.

98
Problem Solving and Data Analysis using Minitab 
From the P value indicator in the top left quarter of the Summary Report we see that the P value = 0. 
Therefore the real mean will differ from our target value.
Below this we see the histogram of the actual sample data. Graphically we see that 99 is outside the 
95% conﬁdence interval for the population mean. You get a histogram rather than a dotplot when you 
have 20 or more data points. 
On the top right we see the statistics from the test. Note that the StDev of the sample data was 1.16 
compared to the 1.5 we used in the Classic Method. 
On the bottom right, within the comments section, you can read the conclusion of the test procedure. 
The comments can be edited to add additional information.

Hypothesis Testing
99
On the top of the Diagnostic Report we are presented with the SPC chart. There are three data points 
that Minitab deems to be outliers.
The power achieved using the Assistant is again higher than previously calculated. It is again due to the 
fact that a lower StDev was used in the calculation.

100
Problem Solving and Data Analysis using Minitab 
Finally, we have the Report Card. We are again told that we had some unusual data points. 
We don’t get any further warnings. 
It’s time for an exercise. Complete the question using the classic method. Use the full procedure for 
hypothesis testing. Then if you have M16 use the Assistant and compare the answer.
Exercise 1. 1 Sample t test. 
Bear is a keen cyclist and likes to buy all the latest equipment. He wants to buy the latest lights for his 
bike and therefore must show that his current lights are failing. He keeps a spreadsheet showing the 
recharge time for his lights. If the time becomes different to the recharge time stated by the manufac-
turer he will have the excuse he needs to buy new lights.
Bear has 75 data points. If the difference he wants to detect is 0.5 h and the historical standard deviation 
is 0.8 h what is the power of the 1 sample t test?
Use the graphical summary to check the sample data. Establish if the mean recharge time of the lights is 
different to 17 h? Display the comparison of the null hypothesis to the 95% CI for the population mean 
using a boxplot.
The data is in Worksheet: Lights.
Problem Statement
Establish if the mean recharge time of the lights is different to 17 h.
(As we are only checking to see if there is a difference a two sided test is required.)

Hypothesis Testing
101
Null Hypothesis
Population Mean = 17 h
Alternate Hypothesis
Population Mean ≠ 17 h
Based on the historical 
StDev the power for 
the test is very high. 
The 
default 
signiﬁ-
cance level will be used 
for the test.

102
Problem Solving and Data Analysis using Minitab 
The graphical sum-
mary shows that the 
sample distribution 
is uni modal and nor-
mally distributed.
The boxplot shows 
that the estimated 
mean lies within the 
95% CI for the pop-
ulation mean.

Hypothesis Testing
103
As the P value is above the alpha value we cannot reject the null hypothesis. We cannot say the mean 
recharge time of the lights is different to 17 h.
Bear didn’t like the result and bought some new lights anyway.
Exercise 1. Using the Assistant.
The P value = 0.354. Therefore the target is not signiﬁcantly different from the real mean.
Within the histogram we see how the target mean is within the 95% conﬁdence interval for the popula-
tion mean.
On the top right we see the statistics from the test. Note that the StDev of the sample data was 0.76 
compared to the 0.8 we used in the Classic Method. 

104
Problem Solving and Data Analysis using Minitab 
On the top of the Diagnostic Report we are presented with the SPC chart. There are two data points that 
Minitab deems to be outliers.
The power achieved using the Assistant is again higher than previously calculated. It is again due to the 
fact that a lower StDev was used in the calculation.

Hypothesis Testing
105
Finally, we have the Report Card. We are again told that we had some unusual data points. 
We don’t get any further warnings.
4.7 Paired t Test
Procedure
tPaired t Test
What's it 
used for?
t A paired t test is used to test the mean difference between two 
sets of dependent samples.
t They usually involve a human element that gives the dependency 
between the two sets of data.
Assumptions
and
Limitations
t The difference of the data sets should form a normally distributed 
population, the test is fairly robust to non normal data when the 
sample size is large (>20).
t M15 users can use Power and Sample Size for the 1 Sample t test.

106
Problem Solving and Data Analysis using Minitab 
The Paired t Test
The Paired t Test is used to check the mean difference between two sets of dependent data. To help 
understand what is meant by dependent data think of using a set of people to test two types of running 
shoe. The test will be to see if 100 m times are different using each type of shoe. There are two obvious 
ways of arranging the test
1. Split the runners into two groups, one group uses shoe A and the other shoe B. Then compare the 
times.
2. Have all the runners run with shoe A and then repeat the run with shoe B. Look at the difference 
in time for each person.
For method 1, if the ability of the runners was to vary greatly between the groups the difference 
between the shoes might get lost within the difference of the groups. Method 2 seems a better choice 
because we can ﬁlter out the variation from our system by using the system twice and then looking at 
the difference.
In fact, the Paired t Test is the same as conducting a 1 Sample t Test on the difference of the two data 
sets and using 0 as the hypothesised mean.
We will learn more about the Paired t Test by going straight into the examples. We will use the same 
procedure as we have used previously for hypothesis testing.
Example 4. Paired t Test.
An archery equipment manufacturer has developed a new arrow release, the Sureshot2. They want to 
know if it performs differently to the Sureshot1. The company gets 50 archers of varying ability and 
measures their average score with both types of release. With a historical StDev of 5, what is the power 
of the test? 
Is the Sureshot2 an improvement over the previous model by an average score of 2?
Worksheet: Archery. 
Problem Statement
Establish if the Sureshot2 gives a better archery score than the Sureshot1. 
Null Hypothesis
Score with Sureshot2 ≤ Score with Sureshot1.
1. Define the
problem and
state
objectives.
2. State a
Null &
Alternate
Hypothesis
4. Conduct
test and
calculate P-
value.
3. Establish
risks, then
conduct
Power &
Sample Size
procedure.
5. DECIDE:
Reject Ho or
Fail to reject
Ho.

Hypothesis Testing
107
Alternate Hypothesis
Score with Sureshot2 > Score with Sureshot1.
(If the Sureshot2 is better it will give a higher score.)
We will now establish the power.
1. Click Stat <<P&SS << Paired t (M15 users can 
use P&SS for the 1 Sample t test).
2. Complete the menu box as shown.
3. Click on Options.
4. Select the radio button for setting the alter-
nate hypothesis to Greater than. As we are 
using Greater than, this is a one sided test.
5. Click OK and OK again.

108
Problem Solving and Data Analysis using Minitab 
Two power curves are 
shown here; the one spe-
ciﬁcally for the Paired t
Test and the one for the 
1 Sample t Test. It can be 
seen that they are iden-
tical and both give the 
same result for a Power 
of 87.3%.
For our test, this means 
that if there is a differ-
ence we have an 87.3% 
chance of correctly stat-
ing that there was a 
difference.
6. Go to Worksheet: 
Archery and trans-
fer the data into 
Minitab.
7. Click Stat <<Basic 
Statistics <<Paired t

Hypothesis Testing
109
8. Select the columns as shown. Ensure 
the order is correctly entered.
9. Then click on Graphs.
10. Select the Boxplot of data and then 
click on OK.
11. Click on Options.
12. Change the Alternative drop down 
menu to ‘greater than’.
13. Click on OK and OK again.

110
Problem Solving and Data Analysis using Minitab 
Graphically it can be seen 
that Ho exceeds the lower 
end of the conﬁdence inter-
val for the mean.
From the session window 
we see that the mean dif-
ference was –1.924. This 
indicates that the Sureshot2 
actually produced a lower 
score than the Sureshot1. 
However, we cannot say 
if it is statistically signiﬁ-
cant. With the test which 
we have just done we can 
only say that the Sureshot2 
was not better than the 
Sureshot1 as the P value 
was 0.994. 
We 
will 
now 
use 
the 
Assistant to conduct the 
same test (M16 only).
1. Click on Assistant 
<<Hypothesis Tests.
2. Click 
on 
Paired 
t 
within the ‘Compare 
two samples with each 
other’ group.
3. Complete the menu 
box as shown and click 
OK.

Hypothesis Testing
111

112
Problem Solving and Data Analysis using Minitab 
From the P value indicator in the top left quarter of the Summary Report we see that the P value = 0.994. 
Therefore we cannot say that the Sureshot2 is better than the Sureshot1.
The histogram gives us our ﬁrst indication that the Sureshot2 was actually worse than the Surehot1. It 
can be seen that the zero is actually outside the 90% CI. (Minitab uses a 90% CI because it was a one 
sided test).
On the top right we see the statistics from the test. Note that the StDev of the sample data was 5.16 
compared to the 5.0 we used in the Classic Method.
On the top of the Diagnostic Report we are presented with the SPC chart which is now based on the 
difference of the points. None of our points are deemed to be unusual.
This is the ﬁrst time that the Assistant has calculated a lower power than the initial check. Again this is 
due to the StDev.

Hypothesis Testing
113
On the Report Card we are told that we did not have any unusual points within the data and that nor-
mality is not an issue.
We are given a warning that we may have the data sets the wrong way round, but we haven’t.

114
Problem Solving and Data Analysis using Minitab 
It’s time for another exercise. Complete the exercise using the Classic Method. Use the full procedure for 
hypothesis testing. Then if you have M16 use the Assistant and compare the answer.
Exercise 2. Paired t test.
A motoring magazine wants to know if parking sensors have an effect on parking time. The magazine 
gets 50 drivers of varying ability and measures parking time (seconds) with and without sensors on the 
same car. 
If they want a power >90% and the historical standard deviation is 8, what is the maximum difference 
that can be measured (as an integer)? 
Do parking sensors change parking time?
Display the comparison of the data to the null hypothesis using a boxplot.
Worksheet: Parking Times. 
Problem Statement
Establish if parking sensors change the parking time.
Null Hypothesis
Parking time With Sensors = Without Sensors
Alternate Hypothesis
Parking time With Sensors ≠ 
Without Sensors
(This is a two sided test.)
In order to ﬁnd the values of 
difference to give us a power 
of >90% we must guess a cou-
ple of values. I have chosen 3 
and 4. As long as I put a space 
between the two numbers they 
will both be displayed on the 
graph together.
A difference of 4 s gives us a 
power of 93.3%.

Hypothesis Testing
115
Graphically it can 
be seen that Ho
sits within the 
conﬁdence inter-
val for the mean. 
This means that 
there is not a dif-
ference in park-
ing times with or 
without sensors.
From the Session 
window we see 
that we have P
value of 0.822. 
‘As the P is not 
low the null can-
not go’. Meaning 
that we cannot 
reject the null 
hypothesis.
We will now use 
the 
Assistant 
to conduct the 
same test (M16 
only).

116
Problem Solving and Data Analysis using Minitab 
From the P value indicator in the top left quarter of the summary report we see that the P value = 0.882. 
Again, this means that we cannot reject the null hypothesis.
The histogram shows us that the zero target clearly lies within the conﬁdence interval.
On the top right we see the statistics from the test. Note that the StDev of the sample data was 8.7 com-
pared to the 8.0 we used in the Classic Method.
On the top of the Diagnostic Report we are presented with the SPC chart which is now based on the 
difference of the points. None of our points are deemed to be unusual.
The Assistant has calculated a lower power than the initial check. Again this is due to the StDev.

Hypothesis Testing
117
On the report card we are told that we did not have any unusual points within the data and that nor-
mality is not an issue.
We are given a warning as the power is calculated to be 88.8% with the Assistant.

118
Problem Solving and Data Analysis using Minitab 
We have looked at comparing a single population to a mean and we have looked at comparing the 
means of two dependent populations using the Paired t Test. We are now going to move on and look 
at comparing two independent populations. We will start by comparing the variance of the populations 
and then move on to comparing the means.
4.8 Two Variance Test
It is worth spending a bit of time and effort understanding the options available to different Minitab 
users for this test. 
M15 users do not have the option of conducting Power and Sample Size for this test. M15 users have 
slightly different input menus for the 2 Variances Test. The options for the output graph are ﬁxed but 
the graph contains the P values for the F test and Levene’s test.
M16 users have the option of conducting Power and Sample Size and have the option of selecting one 
or more of four output graphs using the Classic Method during the test procedure. The P values for the 
F test and Levene’s test are presented in the session window.
For both Classic Methods it is recommended the data be checked for normality using the Graphical 
Summary as M15 and M16 users are always presented with the results of both the F test and Levene’s 
test. The user can then use the appropriate P value when making the decision regarding equal variances. 
Procedure
t 2 Variance Test
What's it 
used for?
t The 2 Variance test is used to determine whether the variances or 
StDevs of two populations are different.
Assumptions
and
Limitations
t Power and Sample Size for this test was introduced in M16.
t There are two procedures used within Minitab. These are the F
test, for data that is likely to have come from a normally distrib-
uted population, and the Levene (Levene/Brown–Forsythe) Test, is 
for all continuous distributions.
t The Assistant uses the Levene test for all distributions.
t The recommendation for the Assistant is to have a sample size of 
at least 20 within the smallest group to ensure the test performs 
well.
t Sample sizes can be unequal.

Hypothesis Testing 119
The Classic Method for checking two variances can be used in the Classic Method of the 2 Sample t Test, 
which we will be looking at later in this section. 
M16 users also have the option of using the Assistant, which will always use Levene’s test.
We will now go through the same example using the M16 Classic Method and using the Assistant. M15 
users should be able to work through the latter half of the ﬁrst example as the differences in the menus 
are minor. 
Exercise 5. 2 Variances Test.
Scotty, the maintenance engineer, has repaired worn parts on a feeder system that regulates the amount 
of additives to a process. The control system on the feeder will try and maintain the same mean but 
there is always some variation from the feeder. Scotty understands the process metrics and he wants to 
know if he has changed the variation of the feeder with the work that he has done.
Before conducting the test procedure, help Scotty understand the power that he can expect from the 
test. He thinks that 75 data points before and after the work will be sufﬁcient. Scotty feels that if the 
StDev decreased by a ratio (Before/After) of less than 0.75 he would be concerned. 
Worksheet: Feeder.
Problem Statement
Establish if the StDev from the feeder increased after maintenance 
Null Hypothesis StDev before
StDev After
(
)
(
)  1
Alternate Hypothesis StDev before
StDev After
(
)
(
) 1
We need to maintain a consistent system of how we arrange our data between the numerator and the 
denominator. We will put the before data as the numerator and the after data as the denominator. This 
means that an increase in post maintenance StDev from the feeder will result in ratio of less than 1. As 
we are only interested in a worsening StDev we can employ a one sided test to increase the power.
The P&SS Test for 2 Variances is available in M16 only.
1. Click Stat <<P&SS << 2 Variances.

120
Problem Solving and Data Analysis using Minitab 
2. Complete the menu box 
as shown. Ensure the 
uppermost selection box 
is set to StDevs and not 
Variances. 
3. Then click on Options to 
set the type of test.
4. Set the Alternate 
Hypothesis to Less 
Than. Note that the 
method has been set for 
Levene’s Test.
5. Click OK and OK again.
6. Look at the graph that 
is produced. We see that 
we would have a power 
of 73% with Levene’s 
test. We will repeat the 
Power and Sample Size 
procedure but select the 
F test.

Hypothesis Testing
121
7. Click on the Edit Last 
icon.
8. Click on Options and 
set the Method to 
the F Test.
9. Click OK and OK 
again.
From the graph that was 
produced we see that we 
get a higher power if our 
data was applicable for 
use with the F test.
Usually we would conduct 
P&SS prior to conduct-
ing the test and collect-
ing data. Therefore we 
would not know whether 
our data was likely to 
come from a normally dis-
tributed population and 
we would not know if 
the F test was applicable. 
However, after collecting 
the data it is worth check-
ing due to the increased 
power of the F test.
10. Open 
worksheet 
Feeder and trans-
fer the data into 
Minitab. It’s not rele-
vant but notice that 
Scotty has provided 
more data than orig-
inally planned.
11. Examine the 
Graphical Summary 
by selecting Stat 
<<Basic Statistics 
<<Graphical
Summary. We want 
to establish if we 
can apply the F test 
or whether we have 
to use Levene’s test.

122
Problem Solving and Data Analysis using Minitab 
12. Select both columns of 
data and then click OK.
It can be seen that both the 
P values are below 0.05 and 
therefore the samples are 
not likely to have come from 
normal 
distributions. 
This 
means that we need to use 
Levene’s test results. Also, 
note the StDev’s. The ratio of 
the Before to After StDev is 
112.86/105.8 = 1.067

Hypothesis Testing
123
13. Click Stat <<Basic Statistics 
<< 2 Variances.
14. Complete the menu box as 
shown. Ensure the upper-
most selection box is set to 
Samples in different col-
umns. Click on Graphs.
15. M16 allows the selection of 
any of the graphs shown. 
M15 will always display two 
plots on a single page which 
also displays the P values.
16. Click OK and then click 
onto Options. Change the 
Alternative hypothesis to 
less than. Click OK and OK 
again.

124
Problem Solving and Data Analysis using Minitab 
From the dotplot it can be 
seen that there is slightly 
less variation in the After 
group. It can also be seen 
that the variation is almost 
equal on either side of the 
mean symbol, although the 
before group appears to be 
slightly more loaded on the 
tails.
The interval plot shows 
the 95% CIs for the StDevs 
of both groups. Note that 
Minitab has swapped the 
order of the groups. The 
interval for the After group 
does appear to be lower. 

Hypothesis Testing
125
M15 users will be able to see the P values 
from both tests on the output graph. 
M16 users need to go to the session 
window. 
As our data was not normally distrib-
uted we need to use the results of the 
Levene’s test. The P value for Levene’s 
test is 0.676 so we cannot reject the null 
hypothesis. This means that the StDev is 
not greater after the maintenance.
Scotty can be satisﬁed that his mainte-
nance work has not adversely affected 
the feeder variation.
We will now conduct the same proce-
dure using the Assistant.

126
Problem Solving and Data Analysis using Minitab 
1. Click on Assistant 
<<Hypothesis Tests.
2. Click on 2-Sample 
Standard Deviation 
within the ‘Compare 
two samples with 
each other’ group. 
3. Complete the menu 
box as shown and 
click OK. Ensure the 
Test Setup is done 
correctly. We want 
to test if the StDev 
of before is less than 
After. With a bit of 
simple maths we can 
work out that we 
need to check the 
power for a 25% 
difference.

Hypothesis Testing
127
The test delivers a P value = 0.676, which is the same as the results for Levene’s test we calculated using 
the Classic Method. 
We are given an additional chart which shows the CIs for StDev. We can see that they clearly overlap 
indicating that there isn’t a difference. 
The histogram shows the spread of the data. Again we can see that After data actually has less spread.
In the comments we are told that we cannot reject the null hypothesis.

128
Problem Solving and Data Analysis using Minitab 
The SPC charts show that none of our points are deemed to be unusual because none of the points are 
marked in red.
The power reported for the test is 79.3% using the actual sample sizes of 99 and 76.

Hypothesis Testing
129
On the report card we are told that we did not have any unusual points within the data and that nor-
mality is not an issue.
We are given a warning regarding the power of the test. We would need both sample sizes to be 123 to 
obtain a power of 90% or 89 to get a power of 80%.
The last procedure that we are going to look at in this chapter is the 2 Sample t Test. This is used to 
compare the mean of two populations. As such it is widely used to understand the effects of changes 
on processes.
We will go straight into looking at worked examples to learn the test procedure. When using the Classic 
Method we need to know if our two groups have equal variances prior to conducting the 2 Sample t
Test. Again, M16 users can also use the Assistant for the 2 Sample t Test.

130
Problem Solving and Data Analysis using Minitab 
4.9 Two Sample t Test
Example 6. 2 Sample t test.
Two CPU manufacturers both make chips that are rated at 2200 MHz. Overclocking is running the CPU 
at higher speed than its rated speed in order to get more performance from the computer. A sample of 
CPUs is taken from each manufacturer. Each chip is overclocked to run at its highest possible speed. The 
maximum speed and manufacturer for each chip is recorded. 
The historical StDev is known to be 20 and 50 samples are to be tested from each manufacturer. If we 
want a power of 90% from the 2 sample t test what is the maximum difference that can be detected? 
Using the data provided in Worksheet: CPU Speed, establish if there is a difference between the chips 
in terms of overclocking to the highest speed. Then establish which manufacturer’s chips can be over-
clocked to the highest speed.
Problem Statement
Establish which chips can be overclocked to the highest speeds.
Null Hypothesis
Pintel Speed = AMB Speed
Procedure
t 2 Sample t Test
What's it 
used for?
t A 2 sample t test is used to determine whether the means of two 
populations are different.
Assumptions
and
Limitations
t The Classic Method performs well with non normal data as long 
as the data is continuous, reasonably symmetrical and uni modal. 
We check these assumptions prior to the procedure using the 
graphical summary.
t Checking for equal variances within the Classic Method increases 
test reliability. The Graphical Summaries can also be used to make 
conclusions as to whether the populations have equal variances.
t The method used by the Assistant does not require the user to test 
for equal variances prior to the test.
t For both methods sample size is important for test validation and 
power.

Hypothesis Testing
131
Alternate Hypothesis
Pintel Speed ≠ AMB Speed
(This will be a two sided test. If we detect a difference we will use the graphical outputs to establish 
which chips are faster.)
1. Click Stat <<P&SS 
<<2-Sample t.
2. Complete 
the 
menu 
box 
as 
shown. There are 
a couple of points 
to note here. The 
sample size listed 
should be for the 
smallest group as 
this will give us 
a safety margin. 
The estimate for 
StDev should be 
for the pooled 
value.
3. Then click OK.
It can be seen that 
under the test condi-
tions stated we would 
be able to detect a dif-
ference of about 13 
with a power of 90%.
This is acceptable so 
we can continue with 
the test procedure.

132
Problem Solving and Data Analysis using Minitab 
4. Open worksheet CPU 
Speed and transfer the 
data into Minitab. 
5. We need to check the 
underlying 
assump-
tions about the data 
by using the Graphical 
summary. It is also a 
good idea to get a feel 
for the data by check-
ing the Graphical sum-
mary before the test 
procedure is started. 
Click 
Stat 
<<Basic 
Statistics <<Graphical 
Summary.
6. Select Pintel and AMB. 
Then click OK.
From the graphical sum-
maries it can be seen that 
our data is reasonably sym-
metrical and unimodal. The 
P values indicate normal-
ity. The StDevs are 21.9 and 
18.2 and the 95% CIs for 
StDevs overlap. Using our 
knowledge and experience 
we could assume equal 
variances with this data. 
However we could also use 
the 2 Variances Test, the 
session window results are 
shown below.

Hypothesis Testing
133
Now that we know the 
assumptions are valid re-
garding the sample data 
and we can assume equal 
variances let’s conduct the 
actual test.
7. Click Stat <<Basic 
Statistics << 2-Sample t.
8. Complete the menu 
box as shown. Make 
sure Assume equal var-
iances is ticked.

134
Problem Solving and Data Analysis using Minitab 
9. Then click on Graphs 
and select Boxplots.
10. Click OK and OK again.
From the test results in the 
session window we can see 
that the P value is zero and 
the mean speed for AMB 
chips is faster. Therefore, we 
can say AMB chips can be 
overclocked to higher speeds 
than Pintel chips. The esti-
mate for the difference was 
actually 17.
The boxplot conﬁrms that the 
AMB chips were faster than 
Pintel. However, we would 
not know from the boxplot 
alone if the difference was 
signiﬁcant.
We will now conduct the 
equivalent procedure with 
the Assistant.

Hypothesis Testing
135
1. Click on Assistant 
<<Hypothesis Tests.
2. Click on 2-Sample t 
within the ‘Compare 
two samples with 
each other’ group. 
3. For Test Setup we 
want to check if the 
mean of Pintel is dif-
ferent to AMB. For 
P&SS we will enter 
a Difference of 13. 
Complete the menu 
box as shown and 
click OK.

136
Problem Solving and Data Analysis using Minitab 
The test delivers a P value = 0.0, telling us that the populations have different means.
We are given an additional chart which shows the 95% CI for the difference. If the interval had gone 
through the zero point we would have had to conclude that there was not a difference between the 
means of the populations. The difference is negative only due to the order that we entered the columns 
into the input menu box. Under Statistics in the top left we are told that ‘The difference is deﬁned as 
Pintel–AMB’.
Within the histograms we see the 95% I for the means. Again they do not overlap.
In the comments we are told that we can conclude that the means differ at the 5% signiﬁcance level.

Hypothesis Testing
137
The SPC charts show that none of our points are deemed to be unusual because none of the points are 
marked in red.
The power reported using the pooled StDev for the sample data was 89.2% in order to be able to detect 
a difference of 13.

138
Problem Solving and Data Analysis using Minitab 
On the Report Card we are told that we did not have any unusual points within the data and that nor-
mality is not an issue and we had sufﬁcient data points.
We are given information stating that the test method used within the Assistant does not require equal 
variances to be conﬁrmed.
Example 7. 2 Sample t Test.
A city has two driving test centres. They both conduct a large number of tests every month and the pass 
rate is recorded for each of the sites every month.
It has already been established that to detect a difference of 2% with the pooled standard deviation of 
3%, then 49 samples are required to achieve a power of 90%.
Use the data provided to establish if the mean pass rates from the two centres are different. Use the 
Classic Method and the Assistant to carry out the analysis.
Worksheet: Test Centre.
1. Open worksheet Test Centre and transfer the 
data into Minitab. 
2. We need to check the underlying assumptions 
about the data by using the Graphical summary. 
Click Stat <<Basic Statistics <<Graphical Summary.

Hypothesis Testing
139
3. Select SiteA and SiteB. 
Then click OK.
The distribution for SiteA is 
normal, contains continu-
ous data and is uni modal. 
Unfortunately, SiteB has a 
bi modal distribution. This 
means that the test results 
will be unreliable and we 
should not proceed. We 
could take this as an oppor-
tunity to explore why there 
is a bi modal distribution 
and see if this can increase 
our 
understanding 
of 
how SiteB is working. The 
problem solver within you 
should feel some excite-
ment at this opportunity. 
Let’s see how the Assistant 
gets on with this problem.

140
Problem Solving and Data Analysis using Minitab 
1. Click on Assistant 
<<Hypothesis Tests.
2. Click on 2-Sample t 
within the ‘Compare 
two samples with 
each other’ group. 
3. Complete the menu 
box as shown and 
click OK. For P&SS 
we have entered a 
Difference of 3.

Hypothesis Testing
141
The Assistant has carried out the full test and reported that we cannot reject the null hypothesis due to 
a P value of 0.265.
As we did using the Classic Method it would be up to the user to recognise that the data did not meet 
the test criteria by looking at the histograms. On this occasion the Assistant does not warn us regarding 
the betrayal of our data.

142
Problem Solving and Data Analysis using Minitab 
The Assistant does provide further investigative information in the form of the SPC charts. On this 
occasion the chart is not highlighting any unusual data points but having been presented with a bi 
modal distribution it would be the next logical step in the investigation to use SPC charts.
Even the report card does not show any warnings.
This example has not been used to underline weaknesses of the Assistant. It has been used to remind the 
problem solver that diligence is required when assessing the test data. With the Classic Method there 
are more checks as you work through the procedure. The Assistant makes the procedure very easy and 
this might make it harder for the problem solver to remain diligent. Remember, problems missed could 
actually be opportunities.
To ﬁnish off the chapter there are a couple of exercises for you to do.

Hypothesis Testing
143
Exercise 3. 2 Sample t Test.
Hawk, the process engineer, works on a plant where they manufacture plasterboard. Hawk is involved 
with improving dryer efﬁciency. He believes that by adjusting inlet pressure he can improve the dryer 
efﬁciency. He intends to run the inlet pressure at two levels and then compare the dryer efﬁciency at 
those two levels. The levels are the normal setting of 10 mbar and the proposed new setting of 8 mbar.
If he wants to be able to detect a difference of 15 kWh/te and if the pooled StDev is 17, what power can 
be achieved with 30 samples in each group?
Calculate the power and then establish if the efﬁciency has changed between the two levels of inlet 
pressure used. Use both the Classic Method and the Assistant.
Worksheet: Efﬁciency. 
A 
power 
of 
about 92% can 
be achieved.

144
Problem Solving and Data Analysis using Minitab 
The data is reasonably 
symmetrical and uni-
modal and the P values 
indicate normality. The 
StDevs are 11 and 21 and 
the 95% CIs for StDevs 
do not overlap, indicat-
ing unequal variances. 
The 2 Variances Test con-
ﬁrms unequal variances 
(results shown below), 
which means that the 
tick box for ‘Assume 
Equal Variances’ in the 
2 Sample t Test input 
menu box must be left 
blank.

Hypothesis Testing
145
As the higher mean 
value indicates a poorer 
efﬁciency, it appears the 
change has made the 
dryer more inefﬁcient. 
However we need to 
check the P value in the 
session window.
The P value is 0.084 and 
the 95% CI for the dif-
ference 
contains 
the 
zero value. Therefore 
we cannot say that the 
populations are differ-
ent. The dryer did not 
change 
its 
efﬁciency 
when the inlet pressure 
was changed from 10 to 
8 mbar. 
Next we will look at the 
solution obtained using 
the Assistant.

146
Problem Solving and Data Analysis using Minitab 
Within the Summary Report the Assistant 
also delivers a P value of 0.084 and we see 
that zero is within the 95% CI for the differ-
ence between the groups.
Within the Diagnostic Report we see that 
we would have a power of 91.8% using 
the sample data StDev. Also, there are no 
outliers.
The Report Card also conﬁrms that there 
were no problems with Normality, Unusual 
Data and Sample size/Power.

Hypothesis Testing
147
Exercise 4. 2 Sample t Test.
Beefy buys chips (fries) from two different chip shops. The chips at the two different shops cost the same 
but Beefy believes he gets a bigger portion at Chappo’s chips. Over the next two weeks Beefy buys and 
eats a lot of chips so he can collect data for a 2 sample t test.
Beefy knows that he can achieve a power about 98% with 25 samples in each group, a pooled StDev of 
13 g if the difference he wants to be able to detect is 15 g. 
Establish which chip shop gives the biggest portion of chips.
Worksheet: Chips.

148
Problem Solving and Data Analysis using Minitab 
From the graphical summaries it can be seen that the data is reasonably symmetrical and uni modal and 
the P values indicate normality. The StDevs are 11.2 and 10.7 and the 95% CIs for StDevs overlap indicat-
ing equal variances. The 2 Variances Test conﬁrms equal variances (results shown below), which means 
that the tick box for ‘Assume Equal Variances’ in the 2 Sample t Test input menu box must be ticked.
The boxplot indicates 
that Chappo’s Chips 
provide a bigger por-
tion 
than 
the 
OK 
Haddock.
The P value is 0.00 and 
the 95% CI for the dif-
ference does not con-
tain zero. Therefore, 
we can conclude that 
the populations are 
different 
and 
that 
Chappo’s Chips gives a 
bigger portion.
We will look at the 
solution obtained from 
using the Assistant.

Hypothesis Testing
149
Within the Summary Report the Assistant 
also delivers a P value of 0.00 and we see 
that zero is not within the 95% CI for the 
difference between the groups.
Within the Diagnostic Report we see that 
we would have a power of 99.8% using 
the sample data StDev. Also, there are no 
outliers.
The Report Card also conﬁrms that 
there were no problems with Normality, 
Unusual Data and Sample size/Power.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 5
Analysis of Variance
In the last chapter we used the 2 Sample t Test to analyse if there was a 
difference between two population means. In this chapter we are going 
to be using predominately the ANOVA procedure to test if there is a 
difference between two or more population means. ANOVA stands for 
ANalysis Of VAriance. We will start by looking at how ANOVA works. 
Then we will look at the One Way ANOVA procedure and look at how 
that is handled in the Classic menus and then within the Assistant. 
Amongst the examples we will also look at how to check for differences 
in Variances between multiple groups. 
We will then move onto the ANOVA General Linear Model which allows 
the comparison of multiple factors at two or more levels.
All of the data sets for this chapter are in the spreadsheet 05 ANOVA.xls.
5.1 How ANOVA Works
In order to understand how ANOVA works let’s pretend that we are investigating how yield responds 
to varying levels of concentration. We feel that yield is a function of concentration. We have looked at 
four levels of concentration and taken three samples at each level.
We will split this data into two parts as this will help us understand if there is going to be a difference 
in the populations of the response, Yield, as we alter the factor, Concentration.
Overall
Variation
Concentration
Yield

Analysis of Variance 151
First, we will quantify the amount of variation at each level of concentration. This is usually called the 
Within Group Variation.
We then take the aver-
age value of each group. 
The variation of the aver-
ages is called the Between 
Group Variation
Concentration
Within
Group Variation
Yield
Concentration
Average of
each guoup
Yield
Concentration
Between
Group Variation
Yield
Between
Group Variation
Within
Group Variation
Overall
Variation

152
Problem Solving and Data Analysis using Minitab 
We say that Overall Variation is the sum of the Within Group and the Between Group Variation. ANOVA 
looks at the ratio of the Between/Within variation.
If the ratio is small then the Within Group Variation is larger and a differ-
ence between the group means does not exist.
If the ratio is large then the Between Group Variation is larger and a differ-
ence between the group means exists.
This ratio is called the F Statistic and within ANOVA the P value tells us if 
the ratio is signiﬁcant.
Let’s see how this works within the ANOVA procedure.
5.2 One Way ANOVA (Classic)
Procedure
t One Way ANOVA (Classic)
What's it 
used for?
t It is used directly to assess the signiﬁcance of one factor or 
variable  at two or more levels.
Assumptions
and
Limitations
t The underlying assumption for the procedure is that the data 
within the groups will come from a normally distributed popula-
tion. The procedure is robust to slight departures from normality 
when the design is balanced (same number of samples at each 
level).
t ANOVA is not robust to unequal variances when the design is 
balanced.
t As a guide a balanced design with a large number of samples at 
each level (>20) will protect the validity of the results.
t At the end of the procedure the residuals are analysed. They will 
show if the normality assumption and equal variance assumption 
were suitably met. Patterns in the residuals will point to problems 
with the analysis and any conclusions that were made. 
t The residuals must show Independence, a reasonably normally 
distribution and equal variance.
Within
Group Variation
Between
Group Variation

Analysis of Variance
153
Example 1. One Way ANOVA (Classic).
Parky’s crayon factory has seven machines making crayons. The ultimate aim of the exercise is to see 
if any of the machines are producing crayons with differing diameters and then to compare the mean 
diameters to a tolerance. We also want to check if the machines are delivering products with the same 
variance.
Initially, we will calculate the power of the test. The max difference of mean diameter that we want to 
be able to detect between any two machines is 0.25 mm, the estimated StDev is 0.3 mm and the inten-
tion is to take 40 samples. 
After calculating the power, open Worksheet: Crayon and use this data to check the underlying assump-
tions required for the One Way ANOVA test. Then use One Way ANOVA to assess whether any of the 
machines are producing crayons with a different diameter.
The ﬁnal part of the exercise is to check each of the machines against the tolerance for diameter. The 
required crayon diameter is actually 6.0 mm with a tolerance of ±0.25 mm. Any machine outside of this 
tolerance value should be targeted for improvement activities.
1. Click Stat <<P&SS << One Way ANOVA
2. Complete the menu box as shown. There 
are a couple of points to note here. 
The sample size listed should be for 
the smallest group as this will give us a 
safety margin. The StDev should be esti-
mated from knowledge of the process, 
typically, the average value expected 
within one of the groups. The number 
of levels is seven due to the number of 
machines.
3. Then click OK.

154
Problem Solving and Data Analysis using Minitab 
It can be seen that with a 
sample size of 40, a StDev of 
0.3 and being able to detect 
a difference of about 0.25 
between groups the power 
that we would achieve is 
79.83%.
This is acceptable so we 
can continue with the test 
procedure.
As we said in the last chap-
ter it is always a good idea 
to see the data before you 
start any kind of analy-
sis. We are going to use 
an Individual Value Plot in 
order to see the levels of the 
data and their distribution.
4. Import the data from 
Worksheet: Crayon into 
Minitab.
5. Click Graph 
<<Individual Value 
Plot. Select Multiple 
Ys, Simple.
6. Then Click OK. 
7. Select 
Machine1 
to 
Machine7 as the Graph 
variables. Then click 
OK.

Analysis of Variance
155
From the plot we can see how the sample data is distributed. It would appear that Machine3 is pro-
ducing crayons of the smallest diameter and Machines 5 and 6 are producing crayons with the largest 
diameter.
We now need to check the underlying assumptions of normality and equal variance for the test.

156
Problem Solving and Data Analysis using Minitab 
8. Produce a Graphical Summary 
for each of the Machines.
The graphical summary for Machine1 
is shown and we can see that the dis-
tribution is deemed to be normal as 
the AD P value is >0.05. The StDev is 
0.29. The table below summarises P 
values and StDevs.
From the table we see that Machine7 
is not normally distributed. However, 
as the remaining groups are normal 
we will continue with the analysis. 
The StDevs vary from 0.21 to 0.31. 
We will need to test whether these 
groups have signiﬁcantly differing 
variances.
Machine
P-value
StDev
1
0.178
0.29
2
0.225
0.27
3
0.594
0.28
4
0.392
0.21
5
0.318
0.22
6
0.070
0.25
7
0.039
0.31

Analysis of Variance
157
The worksheet that was copied 
into Minitab was in the unstacked 
format. This means that the labels 
or subscripts were at the top of 
each column of data. Minitab will 
also handle data in the stacked 
format. This is where the labels or 
subscripts are in one column and 
the data is in the adjacent column. 
When using the Classic Method to 
test for Equal Variances amongst 
groups the data needs to be in the 
stacked format so we must convert 
it.
9. Click Data <<Stack <<Columns.
10. Complete the menu box as 
shown and click OK. We had 
the option of putting the 
stacked data into a new work-
sheet but we will put it into 
C9 and C10 of the existing 
worksheet.
Notice how Minitab stacks all the 
data. C9 contains the labels C10 
contains all the data.
11. Enter new labels for columns 
C9 and C10 as shown.
12. Click Stat <<ANOVA <<Test 
for Equal Variances
13. Complete the menu box as 
shown and click OK.

158
Problem Solving and Data Analysis using Minitab 
Minitab carries out a 
standard hypothesis 
test and also gives a 
graphical display.
Bartlett’s Test result 
is for normally dis-
tributed 
data 
and 
Levene’s for non nor-
mal data. As our data 
was mostly normally 
distributed we will 
use Bartlett’s P value 
to say that the groups 
have equal variances.
The fact that the con-
ﬁdence intervals are 
all overlapping does 
not always indicate 
equal variances with 
this test.
We will now conduct the One Way ANOVA test. There are two options for using One Way ANOVA, one 
for stacked data and one for unstacked. In this example we will use the stacked version
14. Click Stat <<ANOVA << One Way.
15. Complete the menu box as shown.

Analysis of Variance
159
16. Then click Graphs and select 
the Four in one residual plots. 
If the Unstacked version of the 
test had been selected then we 
would not get the option of 
selecting the Residuals versus 
order plot and we would have 
a three in one plot.
17. Click OK and OK again.
18. Go to the session window to 
check the results.
Subscripts is the Between Group var-
iation. The F value is our Between to 
Within ratio. The P value tells us if 
the F value is signiﬁcant. In this case 
it is signiﬁcant and we can reject the 
null hypothesis. This means that one 
or more of the groups are different. 
Error is the Within Group variation.
The R-Sq(adj) tells us that 64.5% of 
the variation in diameter is due to 
changes in machine.
We can ﬁnd out which groups are different by looking at the interval plot within the session window. 
The interval plot shows the 95% CI for the mean crayon diameter. If any of the intervals do not overlap 
then we can deﬁnitely say that those groups are different, the converse is not always true. We can see 
that Machine3 is producing crayons with the lowest diameter.  Machine5 and Machine6 are producing 
crayons with the highest diameter and because their CIs signiﬁcantly overlap we cannot say that the 
mean diameter of these machines is different. It can also be seen that Machine1 and Machine7 could be 
paired. Also Machine2 and Machine4 can be paired. Occasionally, the CIs can be overlapping only very 
slightly but the P value indicates a difference. In these instances we go with the P value.
The original question asked us which of the machines were making crayons within a particular toler-
ance. To obtain this information we need to produce an Interval plot of our own.

160
Problem Solving and Data Analysis using Minitab 
There are two methods of 
navigating to the Interval 
plot.
19. Click Stat <<ANOVA 
<<Interval Plot.
Or:
20. Click Graph <<Interval 
Plot.
21. Select Multiple Ys 
<<Simple. Click OK.
22. Select all the machines 
as the Graph variables. 
23. Click OK.
We get the same interval plot as previously produced within the session window, however, on this occa-
sion the response data is on the y axis.
24. Right click onto the graph area.
25. Click on Add <<Reference Lines.
26. Enter the required tolerance values for the crayons.
As the CIs for machines 3, 5 and 6 are outside of the reference lines  we know that their mean crayon diam-
eter is not between 5.75 and 6.25 mm. Machines 3, 5 and 6 should be targeted for improvement activities.
or

Analysis of Variance
161
We will now check the residual plot and establish whether it validates our test procedure.
As shown earlier, as a part of the ANOVA calculation the mean of each group is calculated. The residual 
is the distance from a data point to the mean value. The residual for each point is calculated.
If our original data was not suitable for the One Way ANOVA test in terms of normality and equal vari-
ance this would show up as unusual patterns within the residuals. We need to check for patterns within 
the residuals to decide if the test is valid. If you look back we have requested the four in one residual 
plot from the Graphs menu when we conducted the One Way ANOVA test. 
27. From the Graphs window ﬁnd the four in one residual plot.
Within the test Assumptions and Limitations we said ‘At the end of the procedure the residuals are 
analysed. They will show if the normality assumption and equal variance assumption were suitably met. 
Patterns in the residuals will point to problems with the analysis and any conclusions that were made. 
The residuals must show Independence, a reasonably normally distribution and equal variance.’
Concentration
Average of each
group.
Residual.
Yield

162
Problem Solving and Data Analysis using Minitab 
We will now examine the residual plots in greater detail.
We are only looking for departures from nor-
mality from the residuals within the Normal 
Probability plot. In this case the residuals 
would pass the ‘fat pencil’ test so there are 
no issues.
From the histogram we are looking to see if 
there is extreme skewness. Our distribution 
for the residuals looks quite good so there 
are no issues. 

Analysis of Variance
163
Within the Versus Fits plot we are looking 
to see if the residuals are equally spaced 
around the zero line. The residuals for 
our study are equally distant from the 
centre line.
Within the Versus Order we are looking 
for unusual patterns or lack of random-
ness within the residuals. No patterns can 
be seen within the residuals from our 
study.
Honesty and discipline are required when checking the residuals. Don’t ignore issues within the residuals 
which indicate that the results are ﬂawed. Asking yourself why issues are occurring and then resolving 
the issues will probably lead to a more robust solution.
Now that we are satisﬁed that the residuals do not indicate any problems within our study and our 
objectives have been met we will repeat this problem using the Assistant.

164
Problem Solving and Data Analysis using Minitab 
5.3 One Way ANOVA with the Assistant
Procedure
tOne Way ANOVA (Assistant)
What's it 
used for?
t It is used directly to asses the signiﬁcance of one factor or variable 
at two or more levels.
Assumptions
and
Limitations
t The One Way ANOVA test within the Assistant uses a different 
method of calculation to the Classic Method.
t It is robust to unequal variances but has a slightly lower power 
than the Classic Method when variances are equal.
t After carrying out the test, if the null hypothesis was not rejected 
the Assistant will state whether the power was adequate. If the 
power was not sufﬁcient the Assistant will recommend a sample 
size. The information provided by the Assistant will be more useful 
if you provide a value for the difference under investigation.
t The test is robust to the normality assumption when the sample 
size is greater than 15 samples per group.
t Residuals are not relied upon to validate the test results.

Analysis of Variance
165
1. Click 
on 
Assistant 
<<Hypothesis Tests.
2. Click on One Way 
ANOVA within the 
‘Compare more than 
two samples’ group. 
3. The Assistant caters 
for 
stacked 
and 
unstacked data, this 
is set within the ﬁrst 
drop down menu. I 
have opted to use 
the unstacked data. 
We will not change 
the signiﬁcance level 
and we enter 0.25 
as 
the 
difference 
between means that 
we are interested in. 
Then Click OK.

166
Problem Solving and Data Analysis using Minitab 
A number of graphical report pages are produced. Go to the Summary Report as shown in the ﬁgure 
below. 
The P value indicator in the top left indicates that there are differences between at least two of the 
groups.
The Means Comparison Chart shows the mean conﬁdence intervals of the groups. Again, intervals that 
do not overlap indicate a difference between groups.  However, on occasion intervals that do slightly 
overlap can also be different.  
On the top right the Assistant provides a comparison table which makes it very easy to see which groups 
actually differ.
On the bottom right, within the comments section, you can read the conclusion of the test procedure. 
This can be edited.

Analysis of Variance
167
The next report page that we are going to look at is the Power Report. The power indicator on the top 
left gives a conﬁdence interval for the power value. We have 78.44–98.3% chance of detecting a differ-
ence of 0.25 mm. This compares to a power of 79.83% obtained when using the Classic Method.
The bottom of the screen shows a table which breaks down the individual data based on each group.

168
Problem Solving and Data Analysis using Minitab 
On the left of the Diagnostic Report we are presented with a histogram for each of the groups so that 
we can see spread, shape and location of each group.
On the right hand side we are given SPC charts. Unusual data points are marked as red dots. These could 
inﬂuence the results of the test so we are given the opportunity to locate the data points and then 
investigate them.

Analysis of Variance
169
Finally, we have the Report Card. We are told that we had no unusual data points. 
We are told that the test managed to detect a difference even though the power was low.
Finally, we are told that normality was not an issue as the sample size within each group was above 15. 
We are going to move onto another example which will help to reinforce our learning.
Example 2. One Way ANOVA.
A tyre manufacturer wants to investigate how the sulfur content of rubber affects the shear strength of 
the rubber. They vary the sulfur concentration at four different levels and check the shear strength of a 
number of samples at each level. 
Initially calculate the power of the test. The max difference of mean strength that we want to be able to 
detect between any two levels is 2.5 kN, the estimated StDev is 3.2 kN and we intend to take 40 samples 
at each level.
After calculating the power, open Worksheet: Tyre and use this data to check the underlying assump-
tions required for the One Way ANOVA test. Then use One Way ANOVA within the Classic Method and 
the Assistant to assess whether changing the concentration has a statistically signiﬁcant effect upon the 
shear strength of the rubber.

170
Problem Solving and Data Analysis using Minitab 
1. Click Stat <<P&SS << 
One Way ANOVA.
2. Complete the menu 
box as shown. The 
sample 
size 
listed 
should be for the 
smallest group as this 
will give us a safety 
margin. The StDev 
should be estimated 
from knowledge of 
the process, typically, 
the average value 
expected within one 
of the groups.
3. Then click OK.
It can be seen that with a 
sample size of 40, a StDev 
of 3.2 and being able to 
detect a difference of 
about 2.5 between groups 
the power that we would 
achieve is 83.7%.
This is acceptable so we 
can continue with the test 
procedure.

Analysis of Variance
171
We want to see the data before we 
start the analysis. Last time we used 
an Individual Value Plot but this time 
we are going to use a boxplot for a 
change.
4. Import the data from Worksheet: 
Tyre into Minitab.
5. Click Graph <<Boxplot. Select 
Multiple Ys, Simple.
6. Click OK. 
7. Select 10–40 ppm as the Graph 
variables. Then click OK.
From the plot we can see how the 
sample data is distributed. It would 
appear that there is a positive cor-
relation between concentration and 
shear strength. However, we don’t 
know if there is actually a signiﬁcant 
change within the populations.
Before we carry out the One-Way 
ANOVA we need to check the under-
lying assumptions of normality and 
equal variance for the test.

172
Problem Solving and Data Analysis using Minitab 
8. Produce 
a 
Graphical 
Summary for each of 
the concentrations.
The table below summa-
rises the P values for the 
Anderson–Darling Normality 
Test and StDevs.
The data clearly meets the 
normality requirement. The 
StDevs vary from 2.93 to 3.94, 
which appears to be a signif-
icant difference. However, 
we need to check using the 
equal variances test.
As our original data is in the 
unstacked format we need 
to stack the data to use the 
Equal Variances test.
9. Click Data <<Stack 
<<Columns.
10. Complete 
the 
menu 
box as shown. On this 
occasion we are going 
to name the columns 
within the menu box 
and we will let Minitab 
locate them on the 
worksheet.
11. Click OK.
12. Check the location of 
the stacked data it 
should be in columns 
C5 and C6.
Concentration
P-value
StDev
10 ppm
0.74
3.94
20 ppm
0.91
2.94
30 ppm
0.63
2.93
40 ppm
0.95
3.3

Analysis of Variance
173
13. Click Stat 
<<ANOVA 
<<Test for Equal 
Variances.
14. Enter Strength as 
the Response and 
Concentration as 
the Factors.
15. Click OK.
From the output graph 
we see that the P
value for Barlett’s Test 
(which is for normal 
data) is 0.193, indicat-
ing that we cannot 
reject the null hypoth-
esis and we cannot say 
that the groups have 
unequal variances.

174
Problem Solving and Data Analysis using Minitab 
Last time we used the stacked 
version of One Way ANOVA. This 
time we will use the unstacked 
version just for a change.
16. Click Stat <<ANOVA<< One 
Way (Unstacked).
17. Complete the menu box as 
shown.
18. Then click Graphs and select 
the Three in one residual 
plots.
19. Click OK and OK again.
20. Go to the session window 
to check the results.
The P value for the test is low so 
the null must go, meaning that 
we can reject the null hypoth-
esis. A difference has been 
detected between at least two 
of the groups. 
The interval plot tells us that 
the 10 ppm level is deﬁnitely 
different to all the other levels. 
The 20 and 30 ppm levels signiﬁ-
cantly overlap so they must be 
the same. There is a bit of doubt 
about the 30 and 40 ppm levels 
as they could be slightly over-
lapping. We will repeat the One 
Way ANOVA in order to demon-
strate a slightly different com-
parison method.

Analysis of Variance
175
21. Click on the Edit Last 
icon.
22. Click on Comparisons.
23. Select Tukey’s.
24. Click OK and OK again.
25. Go to the session win-
dow to interpret the test 
results.
The Grouping Information 
table gives us an efﬁcient 
summary of the results, it is 
an M16 enhancement. 
As the 40 ppm level is the 
only member of Group A it is 
different to all other levels. 
Similarly the 10 ppm level is 
the only member of Group 
C. The 20 and 30 ppm levels, 
as stated earlier, are the same 
and are both in group B.

176
Problem Solving and Data Analysis using Minitab 
Tukey’s method of comparison is a step wise method. One of the levels is subtracted from the group and 
the remainders are compared to the subtracted level. Initially, the 10 ppm level is removed the remain-
ing levels are compared to the 10 ppm level. If their CI does not include zero then they are different to 
10 ppm. 
After that 20 ppm is removed and the remaining two levels are compared to 20 ppm. 20 ppm and 30 
ppm are deemed to be the same as the CI contains zero, whereas 20 ppm and 40 ppm are different.
In the last comparison it can be seen that 30 ppm and 40 ppm are different.
Before checking the residuals, let’s have a quick look at the other com-
parison methods we could have used.
Family and Individual error rate relate to Type I Errors when multiple 
comparisons are involved, which is incorrectly rejecting the null hypoth-
esis. A family signiﬁcance level of 5% for 20 elements is equivalent to 
5/20 or 0.25% individual error rate. As family error rates take less alpha 
risk their conﬁdence intervals are wider. The error rate that you select 
should be appropriate for the risk that you are willing to take. 
The remaining comparison methods will be demonstrated. This is done only to show how they work not 
to help us with the example.

Analysis of Variance
177
Tukey’s and Fisher’s com-
parison 
methods 
are 
both pairwise methods. 
The difference is that 
Tukey’s uses a family 
error rate and Fisher’s 
uses an individual error 
rate. As Tukey’s method 
uses a lower alpha risk 
the conﬁdence intervals 
are wider. For example, 
when 
comparing 
the 
10 ppm level to the 20 
ppm level using Tukey’s 
method the CI was 0.168 
to 4.001 whereas using 
Fisher’s method we have 
0.625 to 3.543. However, 
for this example the 
ﬁndings are identical 
using either of these 
two methods, that is the 
Grouping 
Information 
tables are the same 
using either method.

178
Problem Solving and Data Analysis using Minitab 
When using Dunnett’s 
comparison 
method 
we are ﬁrst required to 
select a control group. 
For the ﬁgure shown the 
20 ppm level has been 
selected as the control 
group and this means 
that all other levels will 
be compared to 20 ppm. 
The Group Information 
table only tells us which 
groups are the same as 
the control and which 
are not.
Even the Interval plot 
only allows us to com-
pare to the 20 ppm 
level. We cannot see 
that the 30 and 40 ppm 
groups are different.
When using Hsu’s MCB 
comparison 
method 
we are ﬁrst required to 
select the best group. 
There is a radio but-
ton for stating whether 
this has the Largest or 
Smallest of the values. 
For the example ‘Largest 
is best’ has been selected. 
This means that the 40 
ppm is selected as the 
control group and all 
other groups are com-
pared to it. For this case, 
we can conﬁrm that all 

Analysis of Variance
179
the other groups are different to the control group because none of the other intervals extend into positive 
values.
We can now complete the example using the Classic Method by reviewing the residuals and checking 
whether our test assumptions were met.
The residuals follow a fairly straight line 
so they can conﬁdently be said to follow 
a normal distribution.
The histogram does not appear to show 
any skewness.
The Versus Fits plots shows that the resid-
uals are equally distributed about the 
zero line. The residuals do not show any 
departures from normality that would 
lead us to question the validity of the 
test.
Let’s now look at using the Assistant to 
solve this problem.

180
Problem Solving and Data Analysis using Minitab 
Example 2 with the Assistant
1. Click on Assistant 
<<Hypothesis Tests.
2. Click on One Way 
ANOVA within the 
‘Compare more than 
two samples’ group. 
3. Complete the menu 
box as shown and 
then Click OK. Note 
that we have again 
used the unstacked 
data and entered 
2.5 as the difference 
that we are inter-
ested in detecting.

Analysis of Variance
181
The Summary Report is shown below. The P value indicator in the top left indicates that there are differ-
ences between at least two of the groups.
The Means Comparison Chart and the Group Comparison Table both show the same results for the 
groups as previously calculated using the Classic Method.
On the bottom right, within the comments section, you can read the conclusion of the test procedure.

182
Problem Solving and Data Analysis using Minitab 
On the Power Report shown below, the power indicator on the top left gives a conﬁdence interval for 
the power value. We have 71.39–89.26% chance of detecting a difference of 2.5. This compares to a 
power of 83.7% obtained when using the Classic Method.
The bottom of the screen shows a table which breaks down the individual data based on each group. 
On the left of the Diagnostic Report we are presented with a histogram for each of the groups so that 
we can see spread, shape and location of each group.
The SPC charts on the right hand side do not show any outliers.

Analysis of Variance
183
Finally, we have the Report Card. We are told that we had no unusual data points. 
We are told that the test managed to detect a difference even though the power was low.
Finally, we are told that Normality is not an issue as the sample size within each group was above 15. 

184
Problem Solving and Data Analysis using Minitab 
There is a short exercise for you to try. Initially use the Classic Method and then use the Assistant. Check 
the assumptions for the Classic Method prior to conducting the test. When both test methods have been 
completed check the answers against each other.
Exercise 1. One Way ANOVA.
At the plasterboard plant, Beefy has been varying the starch level within the plasterboard and monitor-
ing board strength as his response. Beefy wants to know if the strength of the plasterboard has changed 
whilst he has been carrying out his starch alterations.
Initially calculate the power of the test. The max difference of mean strength that we want to be able 
to detect between any two levels is 8 N, the estimated StDev is 17.5 N, there are four levels and Beefy 
intends to take 81 samples. 
After calculating the power, open Worksheet: Starch and use this data to check the underlying assump-
tions required for the One Way ANOVA test. Check the spread of all groups together using an Individual 
Value Plot. Then use One Way ANOVA to assess whether any of the levels of Starch used have a different 
mean strength. Use Tukey’s comparison method to check how the levels differ. Don’t forget to check the 
residuals to see if the test results are valid.
Use the Assistant to repeat the test if using M16.
It can be seen that 
with a sample size of 
81, a StDev of 17.5 and 
being able to detect a 
difference of about 8 
between groups the 
power that we would 
achieve is 67.6%. Even 
though the power is 
low Beefy wants to 
go ahead with the 
analysis.

Analysis of Variance
185
From the Individual Value 
Plot we can see how the 
sample data is distrib-
uted. It looks like starch 
at the 40 ppm level gives 
the highest strength but 
it is not clear which group 
has the lowest strength. 
It could be 50 or 20 ppm.
We produce all the graph-
ical summaries. The table 
below shows the P values 
for the Anderson–Darling 
Test and the StDevs.
The data clearly meets 
the normality require-
ment. The StDev varies 
from 15.56 to 20.24.
Concentration
P-value
StDev
20 ppm
0.959
17.52
30 ppm
0.317
17.50
40 ppm
0.918
20.24
50 ppm
0.711
15.56

186
Problem Solving and Data Analysis using Minitab 
The levels have equal 
variances as the P value 
for Barlett’s Test is 0.133. 
The normality and the 
equal variance assump-
tions have both been 
met.
The P value for the One 
Way ANOVA test is zero. 
This means that a differ-
ence has been detected 
between at least two of 
the groups. The inter-
val plot shows that the 
20 and 40 ppm groups 
and the 50 and 40 ppm 
groups are deﬁnitely 
different.

Analysis of Variance
187
In addition to what we 
have already established 
from the Interval Plot 
from Tukey’s method 
we see that the 20 and 
30 ppm groups are also 
different. This can be 
seen from the Grouping 
Information table.
The last section of the 
pairwise 
comparison 
also shows that the 30 
and 20 ppm groups are 
different as the conﬁ-
dence interval does not 
include zero.

188
Problem Solving and Data Analysis using Minitab 
The residuals follow a fairly straight line so 
they can conﬁdently be said to follow a nor-
mal distribution.
The histogram does not appear to show any 
skewness.
The Versus Fits plots shows that the residuals 
are equally distributed about the zero line.  
The residuals do not show any departures 
from normality that would lead us to ques-
tion the validity for the test.
Let’s now look at using the Assistant to solve 
this problem.

Analysis of Variance
189
The P value indicator in the top left indicates that there are differences in at least two of the groups.
The Means Comparison Chart and the Group Comparison Table both show the same results for the 
groups as previously calculated using the Classic Method. Although, be careful as Minitab has reordered 
our groups.
On the bottom right, within the comments section, you can read the conclusion of the test procedure.

190
Problem Solving and Data Analysis using Minitab 
The power indicator on the top left gives a conﬁdence interval for the power value. We have 59.62–
72.54% chance of detecting a difference of eight. 
The bottom of the screen shows a table which breaks down the individual data based on each group. 
On the left of the Diagnostic Report we are presented with a histogram for each of the groups so that 
we can see spread, shape and location of each group.
The SPC charts on the right hand side show a couple of outliers. However, two outliers, which are not 
extreme, in all this data is nothing to worry about.

Analysis of Variance
191
Finally, we have the Report Card and we are informed about unusual data points. 
We are told that the test managed to detect a difference even though the power was low. On this occa-
sion, Beefy’s gamble of using a low power paid off.
Finally, we are told that normality is not an issue as the sample size within each group was above 15. 

192
Problem Solving and Data Analysis using Minitab 
We are going on to look at the ANOVA General Linear Model for the remainder of this chapter.
5.4 ANOVA General Linear Model
The Terminology of ANOVA GLM
The GLM can handle both balanced and unbalanced designs. A balanced design has the same number 
of runs for each combination of factors.
The GLM can handle multiple comparisons.
Predictors can be ﬁxed factors or random factors. Fixed factors are discrete variables (e.g. low, medium, 
high). The GLM will check whether the means of the response data is different at these levels. Random 
factors are also discrete variables but they are selected from a larger population. 
Continuous predictors can be entered as covariates. Covariate predictors can either be items that are 
controllable or items that are not controllable. The addition of a covariate can change the results of a 
study, as seen within the Rotary Kiln example.
Example 3. ANOVA GLM.
Aman wants to buy a new car stereo. He decides to carry out some research into prices of stereo systems. 
He varies three factors that he is interested in and then records the price of the systems. The factors are 
number of speakers, number of CDs the CD player can hold and whether or not the stereo has an MP3 
input. Study the data in Worksheet: Car and decide which of the factors are signiﬁcant to Cost. Do any 
Procedure
t ANOVA General Linear Model (GLM)
What's it 
used for?
t It is used directly to assess the signiﬁcance of multiple factors at 
two or more levels.
Assumptions
and
Limitations
t There must always be continuous response data.
t For our purposes the ANOVA GLM replaces the Two Way ANOVA 
and the Balanced ANOVA.
t The ANOVA GLM is only available via the Classic Method.
t Residuals are used to validate the study.

Analysis of Variance
193
of the factors interact with each other?  Are the levels of each of the factors signiﬁcantly different from 
each other? Finally, check the residuals from the study to establish if there are any problems with the 
validity of the study.
The discrete levels for these factors are as follows:
Speakers: 4, 8, 12
CD: Single, Double, Multi
MP3 Input: Yes, No
Previously, it was said that it always 
a good idea to view the data prior 
to conducting the test procedure 
and we used the Graphical Summary 
to do this. For the ANOVA GLM we 
use the Main Effects & Interactions 
plots to look at our data before we 
conduct the GLM procedure. 
1. Transfer the data from 
Worksheet: Car into Minitab.
2. Click Stat <<ANOVA <<Main 
Effects Plot.
3. Enter Cost as the response and 
Speaker, CD and MP3 Input as 
the Factors. Click on OK.
The Main Effects Plot isolates each 
of the factors and shows how the 
response variable changes due to 
only changes in the factor being 
studied. The steeper the line the 
greater the effect of the change. 
However, we do not know if the 
factors are signiﬁcant at this point.
4. Click Stat <<ANOVA 
<<Interactions Plot.

194
Problem Solving and Data Analysis using Minitab 
5. Enter Cost as the response and 
Speaker, CD and MP3 Input 
as the Factors. Click on OK 
(the full interaction plot just 
reverses the axis of the graphs 
and displays them again). 
An interaction between the factors 
produces a change different to the 
level of change we were expecting. 
No interaction manifests itself as par-
allel lines. There does not appear to 
be an interaction between Speakers 
and CD because the lines are almost 
parallel. The same can be said for 
Speakers and MP3 Input. However, 
there does appear to be an interac-
tion between CD and MP3 Input and 
we will ﬁnd out if this is signiﬁcant 
when we carry out the analysis.
Now that we have had a look at the 
data we will carry out the ANOVA 
GLM procedure. 
6. Click Stat <<ANOVA <<General 
Linear Model
7. Complete the menu box as 
shown. Note there are two 
ways of adding the factors. The 
vertical bar indicates the main 
effect as well as all the interac-
tions and using it saves a lot of 
typing. If typing out in full we 
use a * to indicate an interac-
tion term.
8. Click OK.

Analysis of Variance
195
Within the Session Window we see 
the factors and interactions listed 
down the left hand side. Reading 
along the table we see the P value 
for each factor. The factors with a 
P value below our signiﬁcance level 
of 0.05 are signiﬁcant. The R-Sq(adj) 
value means that 86.88% of the 
changes within our response can be 
explained by changes within our fac-
tors. R-Sq(adj) is a value that shows 
the ﬁt and allows equal comparison 
of models with different numbers 
of terms. We will now reduce our 
model which means remove the 
terms that are not signiﬁcant. 
As we will only reduce the model once 
we will produce our residual plot and 
also the Grouping Information Table 
with the reduced model.
9. Click the Edit last Dialog icon.
10. Complete the menu box as 
shown, on this occasion it is 
not worth using the vertical 
bar shortcut.
11. Click on Comparisons.
We are going to use pairwise com-
parisons to see how the levels within 
our factors differ. We are going to 
look at the Grouping Information 
Tables to check the comparison. M15 
users will have to use Conﬁdence 
Intervals for the comparisons.
12. Select Speakers, CD and MP3 
Input as the factors for which 
we are going to display tables. 
Click OK and then click on 
graphs.

196
Problem Solving and Data Analysis using Minitab 
13. Click Four in One for Residual 
plots.
14. Click OK and OK again.
From the session window we can 
conﬁrm that the remaining factors 
and single interaction are signiﬁ-
cant. Notice that the R-Sq(adj) has 
increased as we now have a better 
model.
Each of our factors is 
given its own Grouping 
Information Table so 
we can see if the lev-
els within the factors 
are actually different 
from each other. The 
Main Effects plot for 
each factor has been 
placed next to its rel-
evant table. All the 
levels within the CD 
and MP3 Input factor 
are different. For the 
speaker 
group, 
the 
factors that cannot be 
said to be different 
have been grouped.
Now that we have 
seen which factors and 
levels are different we 
will have a look at the 
residuals.

Analysis of Variance
197
We check the validity of the model by looking for patterns in the 4 in 1 Residual Plot. Starting from the 
top and going from left to right:
t The Normal Probability Plot shows that the Residuals are normally distributed.
t The Versus Fits Plot shows that the Residuals are equally distributed about the centre line.
t The Histogram Plot shows that the Residuals do not show extreme skewness.
t The Versus Order Plot shows that the Residuals do not display any patterns or ordered behaviour.   
In summary, there are no issues with the residuals within this study.

198
Problem Solving and Data Analysis using Minitab 
Aman can conclude that Speakers, CD and MP3 Input are all signiﬁcant factors when it comes to stereo 
system cost. The cost of a system with both Multi CD and MP3 Input goes up considerably, this comes 
from the interaction term. 
There is not a signiﬁcant cost difference if you buy a system with four or eight speakers whereas there 
is a difference if you buy a system with 12 speakers.
Example 4. ANOVA GLM.
Within the nuclear industry UF6 gas is converted to UO2 powder inside a rotary kiln. The UF6 is reacted 
with steam at high temperature and pressure. The UO2 produced is a powder but other non uranic 
impurities are present. The quality of the UO2 is measured by checking for residual ﬂuorides within the 
product. Using the factors of steam pressure, kiln temperature and revolutions per minute (rpm) assess 
which factors are signiﬁcant in the production of ﬂuorides. Are there any interactions present?
Worksheet: Kiln.
We will look at the data before we carry out 
the GLM procedure by producing Main Effects 
and Interactions plots. We will not be using the 
RoomTemp data immediately.
1. Transfer the data from Worksheet: Kiln into 
Minitab.
2. Click Stat <<ANOVA <<Main Effects Plot.

Analysis of Variance
199
3. Enter Fluorides as 
the response and 
SteamPress, RPM 
and KilnTemp as the 
Factors. Click on OK.
The Main Effects Plot shows 
that all the factors appear 
to be having an effect 
upon the response variable. 
KilnTemp appears to be 
having the greatest effect 
and then SteamPress. 
4. Click Stat <<ANOVA 
<<Interactions Plot.
5. Enter Fluorides as 
the response and 
SteamPress, RPM 
and KilnTemp as the 
Factors. Click on OK.

200
Problem Solving and Data Analysis using Minitab 
On the Interaction Plot lines 
that are not parallel indicate an 
interaction. There only appears 
to be a slight interaction pre-
sent for the interactions involv-
ing KilnTemp. 
Now that we have had a look at 
the data we will carry out the 
ANOVA GLM procedure.
6. Click Stat <<ANOVA 
<<General Linear Model
7. Complete the menu box as 
shown.
8. Click OK. Then go to the 
Session window to look at 
the results table.
All of the factors and all of the 
two way interactions are sig-
niﬁcant. The three way interac-
tion is not signiﬁcant.
We will now reduce the model 
and remove the nonsigniﬁcant 
three way interaction.

Analysis of Variance 201
9. Click the Edit Last Dialog icon.
10. Complete the menu box as shown. Note, that we can use the minus sign to remove the three way 
interaction.  
11. Click on Graphs and select 
the Four in one residual plots.
12. Click OK and OK again. Then 
go to the session window.
Make a note of the signiﬁcant fac-
tors and interactions because in 
the next exercise we are going to 
look at the effects of adding in a 
continuous factor as a covariate.
For now we still need to check the 
residuals. 
or

202
Problem Solving and Data Analysis using Minitab 
As in the earlier example we check the validity of the model by looking for patterns in the Four in one 
Residual Plot. 
Starting from the top and going from left to right:
t The Normal Probability Plot shows that the Residuals are normally distributed.
t The Versus Fits Plot shows that the Residuals are equally distributed about the centre line.
t The Histogram Plot shows that the Residuals do not show extreme skewness.
t The Versus Order Plot shows that the Residuals are not displaying any patterns or ordered behaviour.  
In summary, there are no issues with the residuals within this study.
Example 4a. ANOVA  GLM.
We have assessed factors, with ﬁxed levels, that affect residual ﬂuorides within UO2 powder. Let’s say 
for the purposes of illustrating a point that after carrying out this work it becomes apparent that the 
room temperature may also be related to the presence of residual ﬂuorides. We will repeat the exer-
cise with the original data but on this occasion we are going to use the continuous data within the 
RoomTemp column as a covariate. Will using RoomTemp as a covariate change the results of our study?
Worksheet: Kiln.

Analysis of Variance
203
Continuous factors are added to the study as covariates. However, Covariates must have a linear rela-
tionship with the response. We are going to be covering Correlation and Regression in C9.
1. In order to conﬁrm the linear relationship click Stat 
<<Regression <<Fitted Line Plot.
2. Enter Fluorides as the Response and RoomTemp as 
the Predictor.
3. Click OK.
The graph shows a roughly linear relationship and the 
R-sq(adj) value indicates a strong linear ﬁt between the 
variables. This conﬁrms that we can use RoomTemp as a 
covariate within this study.
4. Click Stat <<ANOVA <<General Linear Model.
5. Complete the menu box as shown. 
6. Click on Covariates.

204
Problem Solving and Data Analysis using Minitab 
7. Select RoomTemp as the 
Covariate.
8. Click OK and OK again. 
(We will ignore the resid-
ual plot from this run as 
we still need to reduce 
the model.)
With the covariate all three of 
our factors are still signiﬁcant. 
However, with the covariate 
two of our two way interac-
tions are now not signiﬁcant.
We will remove nonsigniﬁcant 
items from the model.
9. Click the Edit last Dialog 
icon.
10. Complete the menu box 
as shown.
11. Click on OK and OK 
again. Go to the Session 
window to check the 
results.
Above is the table from the 
study using the RoomTemp 
as a covariate and below is 
the original study without the 
covariate.
It is always important to iden-
tify all the input factors that 
are affecting the response. 
This exercise has highlighted 
how identifying a covariate 
will alter the factors that you 
might think are signiﬁcant 
and change the results of a 
study.

Analysis of Variance
205
We need to ﬁnish this example 
off by checking the residuals.
There are no issues with the residuals within this study.

206
Problem Solving and Data Analysis using Minitab 
Exercise 2. ANOVA GLM.
Bear wants to talk about buying a new digital camera. He wants to discuss start up time and the fac-
tors that affect it. He collects start up time data for different cameras and records the following factors: 
Make, Number of Sensors and Cost.
Produce a Main Effects plot and Interaction plot. Use the ANOVA GLM to establish which factors and 
interactions are signiﬁcant. Explore if the levels within the factors are actually different. 
Worksheet: Camera. 
The Main Effects Plot shows that the number of sensors and cost of the camera appear to have quite a 
linear relationship with camera start up time. It would appear that Pony cameras take longer to start 
than the other makes.
There appears to be interactions within each pane but it is not yet possible to tell if they are signiﬁcant.

Analysis of Variance
207
The top results table is 
produced on the initial 
run and the table below is 
the reduced model show-
ing only signiﬁcant items.
The bottom table shows 
that only the factors of 
Make and Cost were sig-
niﬁcant. None of the inter-
actions were signiﬁcant.
The Grouping Information 
Table for Make conﬁrms 
that Pony cameras are dif-
ferent to the other makes 
in terms of start up speed.
For Cost we see that each 
of the levels is different.

208
Problem Solving and Data Analysis using Minitab 
The four in one residual plot validates the study. We don’t see anything within any of the plots that 
indicates a problem with the GLM procedure.
We have successfully explored the factors that affect camera start up speed and given Bear plenty to 
talk about.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 6
Measurement System Analysis
6.1 The Importance of Measurement Systems
Whatever our process, our measurement systems are the equivalent of our senses. They measure the 
process and bring us data.
At the start of every project you should check and verify the measurement systems related to your key 
output. If you do this it will create a solid foundation for the project which you can then build upon. If 
you don’t do this you will always be on shaky ground.
Here are a couple of predictions for you.
1. At the start of every project your measurement systems will be inadequate, most of the time.
2. Even if the measurement system is deemed to be acceptable at some point in the project you will 
go back and question the measurement system again.
Even though you probably won’t heed my warning, until it has happened to you a couple of times, do 
not rush into a project without ﬁrst having checked the measurement system as thoroughly as possible. 
Rushing in is usually a false economy.
6.2 How Measurement Systems Affect Data
The variation within the measurement system is usually incorporated within process variation. 
Consider the following two processes; 

X One has a small part to part variance with a higher measurement system variance. 

X The other has a high part to part variance but a low measurement system variance.
If we look at the output measurements of the processes they both appear to be identical. Without 
Measurement System Analysis (MSA) we won’t know whether we should be trying to reduce the meas-
urement system variation or the process variation.

210
Problem Solving and Data Analysis using Minitab 
6.3 Analysing the Appropriate Systems
Earlier it was said that, at the start of the project, you should conﬁrm that your measurement systems 
are adequate. At the start of the project it should be easy enough to ﬁnd the systems that are measur-
ing the output that you are interested in. However, at that point in the project you may not know which 
inputs affect the output. I would suggest that you only conﬁrm the input measurement systems later in 
the project when you have established which inputs are signiﬁcant to the output.
Let’s have a deeper look at the types of measurement system error and learn some of the terminology. 
Process A
Process B
=
+
Variation in Part
σ2
σ2
σ2
Part
Variation in
Measurement
MS
Total Variation
Total
Temperature
Pressure
Flow
Inputs
Output
Weight
Process

Measurement System Analysis 211
6.4 Types of Measurement Systems Error
Traditionally the terms accuracy and precision are explained by showing shots ﬁred at the centre of a 
target. Precision relates to the spread of the shots and accuracy relates to how well the mean of the 
shots aligns to the centre of the target. The ﬁrst four shots are accurate and precise because the spread 
is low and the mean is aligned to the centre. The second four shots have a tight spread so they are 
described as precise but the mean value is not aligned to the centre so the shots are not accurate. 
Let’s have a look how precision and accuracy relate to measurement systems.
Precision errors can be fur-
ther broken down into the 
categories of repeatability 
and reproducibility.
Repeatability errors relate 
to the measurement varia-
tion generated by the same 
person repeatedly measur-
ing the same item.
Reproducibility errors relate 
to the measurement varia-
tion generated by different 
people measuring the same 
part.
If you are learning the terms a good way of remembering the terms is that ‘it takes more than one to 
reproduce’. 
1. Precise and accurate
2. Precise but not
accurate
3. Not Precise but
accurate
4. Not Precise and not
accurate
 
Repeatability (same person)
Reproducibility (different people)

212 Problem Solving and Data Analysis using Minitab 
Accuracy errors can be further broken down 
into the categories of linearity, bias and 
stability.
A linearity error displays an offset between 
the measured value and the real value. 
However, it does not have a constant offset 
or bias. 
A bias error has a constant offset between 
the measured value and the real value over 
the range of the measurement. It could be 
argued that bias is a special type of linearity 
error.
Stability errors manifest themselves as an off-
set; however, they appear over time rather 
than range. The time that stability errors take 
to appear will obviously vary depending on 
the process and type of measurement system. 
Resolution or Discrimination is also a type of measurement error. On occasions it is overlooked in sta-
tistical texts as it is something that should have been considered when setting up or purchasing the 
measurement system. 
Imagine that you have to time an event. You could employ the following techniques:
1. Use a clock.
2. Use an analogue stopwatch with a second hand.
3. Use a digital stopwatch which gives times to one 100th of a second.
Each successive method gives a better resolution. 
Range
Linearity
Range
Bias
Real Value
Measured Value
Stability
Time

Measurement System Analysis
213
There is a rule of thumb for deciding what the resolution for your measurement system should be. It 
brings in another term that we must learn: Tolerance.
To explain tolerance, let’s pretend that we are making ring doughnuts and there is a speciﬁcation for 
the size of the hole. The speciﬁcation for the minimum diameter of the hole is 20 mm and the maximum 
diameter is 30 mm. We say that the Tolerance or Tolerance Width is 30 – 20 = 10 mm.
The rule of thumb is that the resolution of the measurement system should be a tenth of the tolerance. 
In this case 1 mm. The measurement system should be able to measure clearly between every 1 mm 
change. We say it should have a resolution of 1 mm.
6.5 Measurement Systems Toolbox
Underneath toolbox you can see the techniques that are available in Minitab and that we will be going 
through within this chapter. You might notice that Stability is not present. This is because, if all other 
sources of error had been eliminated and stability was thought to be an issue, it would be investigated 
by using observational techniques.
People familiar with U.K. English will have noticed that Minitab uses the U.S. spelling of gage. Even 
though this course is written in U.K. English we will spell ‘Gauge’ as ‘Gage’.
In the next section we will begin to work through MSA examples and exercises. All of the example data 
is within separate worksheets in the ﬁle 06 MSA.xls.
Repeatability & Reproducibility
Linearity & Bias
Repeatability & Bias
Toolbox
Type 1 Gage Study
Gage R&R
Gage Linearity & Bias Study

214
Problem Solving and Data Analysis using Minitab 
6.6 Type 1 Gage Study
Example 1. Type 1 Gage Study.
Bear decides to buy a new digital vernier gauge. He wants to conﬁrm how well his new vernier measures 
objects so he takes two metal blocks and measures their lengths on his CPU controlled laser proﬁling 
equipment. The ﬁrst object has a length of 50 mm and Bear gives himself a tolerance width of 0.5 mm 
for measuring this block in the future. Bear measures the length of this object 50 times and records the 
results in Worksheet: Type 1GS. The results are in the Block 1 column.
Use the results and carry out a Type 1 Gage Study to establish if the measurement system is repeatable 
and if a bias is present.
1. Copy the data from column A of theExcel ﬁle and copy it into 
Minitab.
2. Click on Stat <<Quality Tools <<Gage Study <<Type 1 Gage Study.
Procedure
tType 1 Gage Study
What's it 
used for?
t A reference part is measured repeatedly to make an initial assess-
ment of Repeatability and Bias Error of a measurement system.
Assumptions
and
Limitations
t The term reference part means that parameter being measured 
within the part has been conﬁrmed using highly precise and accu-
rate methods.
t The test runs should be conducted by a single person as no distinc-
tion is made for the Reproducibility term.

Measurement System Analysis
215
3. Within the menu box select Block 1 as the measure-
ment data. Bear has accurately measured the part 
and knows the reference length is 50. The tolerance 
width that he is using is 0.5. Therefore, acceptable 
parts in the future will be within 50 ± 0.25 mm.
4. Click on Gage Info. It is always a good idea to enter 
relevant information about the study. It will prob-
ably prove useful at some point in the future. Click 
OK and OK again.

216
Problem Solving and Data Analysis using Minitab 
The output of the Type 1 Gage Study is given wholly within the graph shown. 
The Run chart shows the 50 measurements that were recorded. The control limits are calculated by add-
ing and subtracting 10% of the tolerance to the reference value (not the mean of the measurements). 
The percentage used can be adjusted within the options menu of the test.
We gave the reference measurement for the part as 50. It turns out that the mean of the 50 measure-
ments with the vernier is also 50.001 mm. 
The StDev of the 50 measurements is 0.0308.
The tolerance width, which was stated by Bear, for this part being measured was 0.5mm.
The Bias is given as 0.001. The P value of 0.841 conﬁrms that the bias is not signiﬁcant. 
We will discuss process capability in a later chapter, where the terms Cp and Cpk will be discussed and 
deﬁned. Cp and Cpk are analogous to Cg and Cgk. 
Cg relates to 20% of the tolerance divided by 6*StDev (0.1847). Its value would be >1 if the gage preci-
sion was suitable for the given tolerance.
Cgk relates to the distance to the closest control limit divided by 3*StDev. Its value would be >1 if the 
gage precision and accuracy was suitable for the given tolerance.
The %Var values tells us how much of the tolerance is wasted by Repeatability and then by Repeatability 
and Bias. This value should be less than 10%, ideally.
Exercise 1. Type 1 Gage Study.
The second object has a length of 64.5 mm and Bear gives himself a tolerance width of 0.8 mm for meas-
uring this block in the future. Bear measures the length of this object 75 times and records the results in 
Worksheet: Type 1GS. The results are in the Block 2 column.
Use the results and carry out a Type 1 Gage study to establish if the measurement system is repeatable 
and if a bias is present. Is the measurement system suitable?

Measurement System Analysis
217
We see that there is a signiﬁcant Bias within this study of 0.063 mm. The Repeatability takes up 35.95% 
of the tolerance. What really makes this measurement system a failure is the Bias which contributes an 
additional 133% to the %Variation.
We can conclude that, although Bear’s new vernier was poor at measuring Block 1, it is absolutely dire 
at measuring Block 2.
Next, we are going to move on to look at Gage R&R studies.
6.7 Gage Repeatability and Reproducibility Studies

218
Problem Solving and Data Analysis using Minitab 
As we are now going to concentrate on Gage Repeatability and Reproducibility (Gage R&R) studies 
we need to understand a bit more about the two types of study. Initially we will look at the Gage R&R 
Crossed Study. The diagram above shows how the total gage error is broken down into its components 
for a crossed study. The other type of study is the Nested Study. It has almost the same breakdown of 
components as a Crossed Study but we don’t get the Operator*Part Interaction term. You will get to see 
why later in the section. 
In order to understand the difference between 
the Crossed and the Nested Study take a look at 
the ﬁgure on the right. The diagram shown is for 
illustrative purposes and the number of parts and 
operators shown are not as they would be in a real 
study. However, what it does show is that each 
operator measures each part. That is why this is a 
Crossed Study.
On occasion the testing is destructive and the part 
being measured is altered or destroyed by the 
measuring process, as in breaking strength tests, 
for example. For these types of tests we still need 
to be able to estimate repeatability.
We do this by making batches of parts that are identical. If they are identical then all the measurement 
variation can be attributed to repeatability. In order to use the Crossed Study for destructive testing the 
batch of parts has to be big enough so that different operators can measure from the same batch and 
essentially repeat measurements can be made. 
On occasion it is not possible to produce a batch 
where there are enough samples or pieces for 
every operator to be able to measure the sample. 
On these occasions we use a Nested Study.
The diagram shows that the parts are nested 
within the operators. Operator A will measure the 
two parts from the ﬁrst batch and Operator B two 
parts from the second batch.
If batches, with the same properties, cannot be 
produced a Gage R&R study cannot be carried out.
The parts that you select for the study can have 
a massive impact on the results of the study. The 
parts that you select should be parts that are 
deemed to be good parts with respect to your process speciﬁcation/tolerance. If you use reject parts you 
may exaggerate the part to part variation and that will make your study metric better than it should be.
On occasion it may seem like a good idea to make yourself a tester in order to increase the number of 
operators you have. If you are not a regular tester/operator you may inadvertently increase both types 
of measurement error.
Operator
A
Operator
B
Operator
A
Operator
B

Measurement System Analysis
219
If you can, keep the part identiﬁcations hidden from the operators/testers and randomise the studies. 
This is because operators will use their judgement as well as the measurement system.
Apart from the Assistant there are a couple of upgrades under the bonnet of M16 when using the Classic 
method. The ﬁrst is the option of calculating the conﬁdence intervals for the study metrics. Give this a 
go when you are carrying out the exercises. You will ﬁnd the button for using conﬁdence intervals in the 
Gage R&R input menu. However, a word of warning you will ﬁnd the results frightening!
The other addition is the Gage R&R Study (Expanded). This procedure allows you to add additional fac-
tors to the analysis and check if they impact on the measurement system.
The ﬁrst example that we are going to look at is how to create a simple worksheet which can be used 
to collect data for a Gage R&R Crossed Study.
6.8 Create Gage R&R Study Worksheet
Example 2. Gage R&R Study Worksheet.
My three kids are not very good at baking cakes. Before we address the problem we need to ensure the 
measurement system is satisfactory. 
Create a randomised Study Worksheet so we can collect data for a Gage R&R study; there are four parts.
No Data File
1. Click Stat <<Quality 
Tools <<Gage Study 
<<Create Gage R&R 
Study Worksheet.
2. Complete the page as 
shown. Note we must 
have ‘Replicates’ >1 
for the Repeatability 
term and more than 
one operator for our 
Reproducibility term. 
3. Then click Options. 

220
Problem Solving and Data Analysis using Minitab 
4. Select ‘Randomize runs within 
operators’.
5. Then Click OK and OK.
Normally, you would randomise the 
runs as far as possible but this depends 
on the time and effort required to set 
up the study.
Minitab creates the study worksheet. 
If we were using this worksheet we 
would carry out the study as per the 
run order.

Measurement System Analysis
221
6.9 Gage R&R (Crossed)
6.10 Gage R&R Crossed Studies
We have just shown you how to create a study worksheet but that example did not use the recom-
mended number of parts. The Automotive Industry Action Group recommend using 10 parts, two or 
three operators and two or three replicates when carrying out a Gage R&R Crossed Study. However, 10 
parts may not give a precise representation of the actual part to part variation. Therefore, it is strongly 
recommended within the Assistant that a historical standard deviation of the measurement is entered.
We will now start looking at examples of the Gage R&R Crossed Study. The study examples will be cov-
ered using both the Classic Method and the Assistant. The ﬁrst example will show a simple study and 
will introduce the basic analysis. Once we have understood the basic analysis we will move onto a more 
in depth example, where we can look in more detail at the metrics that we can choose to evaluate our 
measurement system. You can expect some exercises along the way as well.
If you are using the Classic method there are ﬁve possible metrics you could use to assess the meas-
urement system. These are %Contribution, %Study Variation, %Tolerance, %Process and Number of 
Distinct Categories (NoDC). NoDC should not be used as a metric only as an indicator. %Tolerance will 
only be given within the results if a tolerance value is entered. %Process will only be given within the 
results if a historical StDev is entered.
If you are using the Assistant there are only two possible metrics: %Process and %Tolerance. The part 
to part variation used to calculate the %Process will be calculated from the parts within the study or 
a historical StDev of the measurements. Minitab strongly recommends you enter a historical StDev. 
%Tolerance will only be given within the results if a tolerance value is entered. 
Procedure
tGage R&R (Crossed)
What's it 
used for?
t It is used to compare the reproducibility and repeatability 
elements of the measurement system with the overall study 
variation and, if required, tolerance.
Assumptions
and
Limitations
t Parts selection for the study can be critical. The parts should not 
be deliberately selected to be at the extremes of the process. The 
parts should be selected so that they represent normal operations.
t Human behavior can affect the results. Therefore, it is best if the 
runs can be randomised and the parts kept blind.
t The crossed study is used to assess systems where all operators can 
measure all parts.

222
Problem Solving and Data Analysis using Minitab 
For both the Assistant and the Classic Methods:
If we wanted to improve parts to a speciﬁcation we would use %Tolerance as our metric.
If we wanted to reduce part to part variation we would use %Study Variation or %Process as the metric.
6.11 Gage R&R (Crossed) Study
Example 3. Gage R&R Study.
My three kids, Iqra, Humzah and Raeesa are not very good at baking cakes. I give them 10 packets of 
ﬂour and ask them to weigh the packets as a part of a Gage R&R Study. Each of my kids measures each 
packet three times, so there are 10 parts, three operators, three repeats of each measurement making 90 
runs in total. My kids have short attention spans so I did not need to randomise the study. The cake bak-
ing process at my house does not have a speciﬁcation so I choose not to use a tolerance for this analysis.
Using the results table in Worksheet: Cakes carry out a Gage R&R (Crossed) study.
Use the study results to learn if our measurement system is suitable.
1. Copy the data from the Excel ﬁle into 
Minitab. There were 10 bags of ﬂour and 
each is designated a number from 1 to 10, 
this can be seen in the part column. The 
measurement column is used to record 
the value of the measurement operation, 
weighing. Person is simply the name for 
the person carrying out the measurement. 
I always ﬁnd it easier to use column labels 
that Minitab will be expecting within its 
menus, it’s one less thing to think about.
2. As stated in earlier chapters it is always a 
good idea to get a feel for your data before 
conducting the test procedure. For Gage 
R&R studies we are going to use a Gage Run 
Chart to do this. Click Stat <<Quality Tools 
<<Gage Study <<Gage Run Chart.
3. Complete the menu box as shown.
4. Click on Gage Info. When carrying out a lot 
of studies it’s worth recording some extra 
information for the future.

Measurement System Analysis
223
5. Then click OK and OK again.
What we get is a graph showing all the results of the measurements. The measurements are initially split 
by part and then by operator. It can be seen that the variability within the operators is low, indicating a 
good repeatability. The measurement variability between operators is also low indicting good reproduc-
ibility. The range of the part to part variation appears to be given by parts two and nine. Both types of 
operator measurement variability appears much lower than the part to part variation.
To carry out the actual test procedure we are ﬁrst going to use the Classic Method and then are going 
to use the Assistant.
6. Now that we have got a feel for the 
measurement data we are going to 
start the test procedure. Click Stat 
<<Quality Tools <<Gage Study <<Gage 
R&R Study (Crossed).

224
Problem Solving and Data Analysis using Minitab 
7. Complete the menu as shown. I have already 
gone into Gage Info and entered some 
details about this study. We will always use 
the ANOVA method for this procedure. 
8. Click OK to execute the procedure.
Minitab produces this set of six graphs and also information within the session window. We shall look at 
the results in detail and look at how they helps us to understand our measurement system.
Four sets of bar graphs are 
within the Components of 
Variation bar chart. The ﬁrst set 
is the Gage R&R bar graph. This 
is then broken into its separate 
components of Reproducibility 
and Repeatability. The Part to 
Part variation is shown as the 
fourth set of bars.
Each Bar graph is broken down into two components %Contribution and %Study Variation. Normally, 
we would choose %Study Variation to decide if the measurement system was acceptable by comparing 
it to the values shown in the table below.

Measurement System Analysis
225
If we wanted to improve 
parts to a speciﬁcation we 
would use %Tolerance as 
our metric. In order to dis-
play 
%Tolerance 
within 
the graph we would have 
needed to enter a tolerance 
ﬁgure within the options 
menu. As I did not provide 
a tolerance ﬁgure for my 
kids this is not applicable 
within this example.
Therefore, we will use %Study Variation as our metric in deciding whether our measurement system is 
acceptable.
In our study we can see that 
the Gage R&R error is prob-
ably acceptable using the 
%Study Variation metric. 
Hovering over the bars with 
the mouse pointer will actu-
ally show the value of each 
of the bars.
The repeatability and the 
reproducibility errors appear 
to be about the same.
The gage error is much 
smaller than the part to part 
variation. This is good news 
as we would want the varia-
tion between the parts to be 
much greater than our Gage 
R&R error.
The R chart shows the range of measurements for each part in turn by each person. This chart highlights 
repeatability issues.

226
Problem Solving and Data Analysis using Minitab 
All points should be within the control lines and the pattern of the data should be the same.
A point outside the control lines indicates that a person is having trouble measuring a particular part. If 
this occurs, the problems with the measurement system should be studied and then corrected. The study 
should then be repeated. 
Control lines are calculated using the average range of each part. Essentially, this chart and the next are 
an Xbar-R chart of the measurement data. You will learn more about this type of chart within the SPC 
module.
The XBar chart shows the 
average measurements for 
each part in turn for each 
person.
The Control lines on this 
chart represent the repeat-
ability variation of the 
study.
The data points should be 
outside the control lines 
and the pattern of the data should be the same. Having the points outside the control lines indicates 
that the part to part variation is greater than the measurement system repeatability. Minitab suggests 
that at least 50% of the data points should be outside the control lines.
The Measure by Part chart 
shows all the measured 
values for each part and 
the average value for each 
part.
From this chart it would be 
easy to see if the operators 
were having trouble meas-
uring any particular part. 
The range of values around 
that part would be greater 
than any other. This prob-
lem does not seem to be 
apparent in our study.
The Measure by Operator 
chart shows the spread of 
measured values for each 
person and the average 
value for each person.
From this graph it can be 
seen if any of my kids are 

Measurement System Analysis
227
getting different results to any of the others. That is, you would be able to tell if one of them was meas-
uring consistently higher or lower than the others or if their spread of measurements was greater.
In our study it can just be seen that Iqra is measuring a slightly higher average value than Humzah or 
Raeesa.
For the Operator* Part 
interaction chart we want 
all the lines to be the 
same and overlaid on top 
of each other.
If one line is consistently 
higher or lower than the 
rest this indicates that 
the person is consistently 
over measuring or under 
measuring.
If the lines cross this may indicate an operator part interaction i.e. the person’s ability to measure is 
dependent upon the part. The ANOVA table within the Session Window would conﬁrm the strength of 
the interaction.
Since my kids did such a splendid job of measuring the lines are overlaid and we don’t seem to be seeing 
any Operator*Part interaction.
We are now going to look at the out-
put within the Session Window. This is 
where we get the numbers to be able 
to judge our measurement system.
Minitab uses the ANOVA procedure to 
calculate the components of variance 
and then uses those components to cal-
culate the percentage variation of the 
different components of the measure-
ment system.
In the ﬁrst table the P value for the 
interaction term is calculated at 0.395. 
As this is above 0.25, Minitab removes 
the interaction term from the ANOVA 
calculation and carries out the test 
again. In effect, Minitab is saying that 
there is no Operator*Part interaction.
As the P value for person is zero it 
means that the difference of measure-
ments is statistically signiﬁcant.

228 Problem Solving and Data Analysis using Minitab 
As %Contribution is based on Variance 
the Part to Part variation and Total 
Gage R&R adds up to 100%. When the 
tables are based on StDevs the total 
will no longer add up to 100%. 
Total Gage R&R is made up of 
Repeatability and Reproducibility. Re- 
producibility is made up of Operator 
(Person) and the Operator*Part inter-
action. However, as the P value for 
the Operator*Part interaction was 
above 0.25 it was removed from the 
calculations.
In terms of %Study Variation we see 
that the Total Gage R&R is 8.24% which 
means that our measurement system is 
acceptable. The Repeatability compo-
nent is 6.05% and the Reproducibility 
component is slightly lower at 5.6%.
Number of Distinct Categories gives an overall grade to the measurement system but lacks detail. This 
value should be 6 or above. Sometimes this value is taken to indicate the number of bins that your meas-
urement system can resolve a part into. 
So if Number of Distinct Categories was two, the measurement system could only grade the item as a 
pass or fail.
Only use this as an indicator not a metric for assessing the measuring system.
Now we will have a look at using the Assistant to handle this analysis.
%Contribution
%Study
Variation
Acceptable
<1%
<10%
Marginal
≤9%
≤ 30%
Unacceptable
>9%
>30%

Measurement System Analysis
229
1. Click on Assistant <<Measurement Systems Analysis.
2. The selection ﬂowchart appears. We will only be look-
ing at the Measurement side which deals with contin-
uous data, which is on the left. Click on the button for 
Gage R&R Study Crossed. 
3. Complete the menu as shown. Note, that the Assistant strongly recommends that we use a histori-
cal StDev. Also, we will not be entering a tolerance as I did not specify one for my kids. We will rely 
on Percentage of Process as our metric for improving the measurement system, in this example.
4. Click OK.

230
Problem Solving and Data Analysis using Minitab 
Starting in the top left, the Summary Report tells us that 8.2% of the observed variation is coming from 
the measurement system. Since this is below the 10% level the measurement system is deemed to be 
acceptable.
Below that is a bar chart showing the Variation Breakdown. We can see that Repeatability and 
Reproducibility are very similar in terms of percentage of process variation.
The comments section gives us a more precise breakdown of the Repeatability and Reproducibility com-
ponents. Remember, the components won’t add to 100% as they are based on StDevs.
Within the Variation Report we are given the relevant information presented within the same for-
mat as the Classic method. We see that the XBar Chart, R Chart, Operator*Part Interaction Chart and 
Measurement by Person Chart all appear again. On the bottom right we see the breakdown of the com-
ponents of variation. %Process is similar to %Study Variation which we saw within the Classic Method. 
Note, the ‘(data)’ below the %Process heading indicates that the Assistant calculated the Process 
Variation from the measurement data, that is 10 parts.

Measurement System Analysis
231
On the Report Card we are given three pieces of key information about this type of study.
The ﬁrst is regarding our choice to ask the Assistant to calculate the part to part variation from the study 
data, 10 parts, and not provide a historical ﬁgure based on many more parts. The general advice would 
be to enter a historical StDev wherever possible, otherwise the data for 10 parts, three operators and 
two or three repeats is the minimum that you should have.
Let’s have a look at the next example where we enter a tolerance and calculate the part to part variation 
from historical data.

232
Problem Solving and Data Analysis using Minitab 
Example 4. Gage R&R Study.
Bads has set up a study examining the measurement system used when measuring the caliper of plaster-
board. He uses three operators and 10 parts. The parts represent the variation seen within the process 
and not the extremes of the process. The tolerance for the caliper is 0.6 mm.This MSA is part of a project 
which is intended to reduce process waste when the product goes out of the caliper tolerance required 
by the customer. Bads has also collected historical data on these measurements to allow a more precise 
calculation of part to part variation. The study data was originally randomised but has been ordered for 
our convenience. Incidentally, Minitab does not need the data to be ordered.
Using the results table carry out a Gage R&R Study to learn if the measurement system is suitable. Use 
the data within Worksheet: Caliper
1. Copy the data from the Excel ﬁle into 
Minitab.
2. We are going to use a Gage Run Chart 
to view our data prior to the test proce-
dure. Click Stat <<Quality Tools <<Gage 
Study <<Gage Run Chart.
3. Select the following columns into the 
appropriate section of the Gage Run 
Chart menu. 
4. Record some appropriate information 
into the Gage Info menu and then click 
OK and OK again. 

Measurement System Analysis
233
For this example it can be seen that there are both repeatability and reproducibility issues. It is hard to 
say which is greater. It is also difﬁcult to say whether any particular operator is consistently having the 
same type of problem. 
To carry out the actual test procedure we are ﬁrst going to use the Classic Method and then are going 
to use the Assistant.
There are 500 values of measurement data in the col-
umn labeled Historical Data. We need to calculate the 
StDev of this data.
5. Click Stat <<Basic Statistics <<Display Descriptive 
Statistics.
6. Select the Historical Data column.
7. Click on Statistics.
8. Clear the default selection and then select 
Standard Deviation.
9. Then click OK and OK again.
10. The result is given within the session window. 
This is the value we will use for historical stand-
ard deviation. 

234
Problem Solving and Data Analysis using Minitab 
11. In order to conduct the test procedure 
click Stat <<Quality Tools <<Gage 
Study <<Gage R&R Study (Crossed).
12. Complete the menu as shown. I have 
already gone into Gage Info and 
entered some details about this study. 
13. Click on Options.
14. Enter the tolerance of the process and 
the Historical Standard Deviation. 
Note, that from within this menu we 
can also change the number of StDevs 
that are used within the study, nor-
mally we would leave this at 6. We also 
have the option of changing when we 
will remove the Operator*Part inter-
action from the ANOVA table. 
15. Click OK and OK again to execute the 
procedure.

Measurement System Analysis
235
Minitab produces this set of six graphs and also information within the session window. We shall look at 
the results in detail and look at how it helps us to understand our measurement system.
On this occasion it can be 
seen that the components 
of variation bar graph has 
expanded to include new 
metrics. As we entered a 
tolerance we get a bar for 
%Tolerance so we can see 
the components of vari-
ation as a percentage of 
the tolerance. We also 
entered a historical StDev 
and from this we get the 
components of variation 
as a percentage of the 
process variation.
On this occasion we want 
to improve parts to a 
speciﬁcation, 
therefore, 
we would use %Tolerance 
as our metric. 

236
Problem Solving and Data Analysis using Minitab 
We see that %Study Variation, %Process and %Tolerance all use the same scale to grade the measure-
ment system.
The 
value 
of 
the 
%Tolerance within the 
total Gage R&R chart can 
be seen by clicking onto 
or hovering over the 
relevant bar within the 
graph.
The R chart shows the 
range of measurements 
for each part in turn by 
each operator.
All points should be 
within the control lines 
and the pattern of the 
data should be the same.
All of the points are 
within the control lines 
so we can say that none 
of the operators are hav-
ing a problem measuring 
a particular part.
The Xbar chart shows the 
average 
measurements 
for each part in turn for 
each operator.
It can be seen that the 
Xbar values are inside 
and outside of the con-
trol lines, which is what 
we want. Looking more 
closely we can see a trend 
in that Aamir appears to 
be measuring a higher 
average value than the 
other two operators. This 
looks like the beginnings 
of a reproducibility issue.

Measurement System Analysis
237
The Measure by Part chart shows all the measured values for each part and the average value for each 
part.
We can see that Part 2 has the least spread of measurements. To ﬁnd the part with the greatest spread 
of measurements you can hover over the measurements at the extremes for each part and take the dif-
ference as the range for that part. 
The Measure by Operator 
chart shows the spread of 
measured values for each 
operator and the average 
value for each operator.
We can again see that Aamir 
is on average reporting a 
higher value than the other 
two operators. We cannot 
say that Aamir has a positive 
bias as that would imply we 
know the real values for the parts. It is plausible that Aamir is reading the correct value and Amin and 
Fakhar have a negative bias but we just don’t know. And that is why we carry out Bias and Linearity 
Studies as discussed later.
For the Part * Operator interaction chart we want all the lines to be the same and overlaid on top of 
each other.
Again we see that 
Aamir has a higher 
average reading. 
We don’t see many 
lines that are not 
parallel. This indi-
cates that the Part 
* Operator interac-
tion may be low.

238
Problem Solving and Data Analysis using Minitab 
In the Session Window Minitab 
uses the ANOVA procedure to 
calculate the components of 
variance and then uses those 
components to calculate the per-
centage variation in the meas-
urement system.
In the ﬁrst table the P value for 
the interaction term is calculated 
at 0.395. As this is above 0.25, 
Minitab removes the interac-
tion term from the ANOVA cal-
culation and carries out the test 
again.
We are concerned with improv-
ing parts to a tolerance within 
this exercise and so we are using 
%Tolerance as the metric for this 
MSA. The value of %Tolerance 
is 25.9% and therefore our 
measurement system is graded 
as Marginal. The Repeatability 
component is slightly less than 
the Reproducibility component. 
All of the Reproducibility is 
made up from the differences 
between operators and none 
of it is made up from the Part * 
Operator interaction.
If our objective had been 
to improve part to part 
variation we would have 
used %Process as our 
metric. If we had done 
that we would have said 
that the measurement 
system was graded as 
bad.
If this was your measurement system would you try and improve it or would you proceed with the pro-
ject? Remember, at best the measurement system has been graded as marginal. Again, it comes down 
to the risk that you are willing to take and the potential consequence.

Measurement System Analysis
239
Let’s have a look at how to handle this study within the Assistant.
1. Click on Assistant 
<<Measurement
Systems Analysis.
2. Click on the button 
for Gage R&R Study 
Crossed.
3. Complete the menu 
as shown. We enter 
our historical stand-
ard deviation that 
we previously calcu-
lated at 0.0762. We 
also enter the toler-
ance of 0.6 mm.
4. Then click OK.

240
Problem Solving and Data Analysis using Minitab 
Starting in the top left, the Summary Report tells us that 34% of the process variation is coming from 
the measurement system.
As we entered a tolerance there is also a scale which asks whether we can tell good parts from bad. We 
are told that the measurement system variation is 25.9% of the tolerance. This is within the marginal 
band.
We see the breakdown of the Total Gage variation in the Variation Breakdown Chart. The comments 
section gives us a more precise breakdown of the Repeatability and Reproducibility components. 
Remember, the components won’t add to 100% as they are based on StDevs.
Within the Variation Report we are given the relevant information presented within the same format as 
the Classic Method. We see that the Xbar Chart, R Chart, Operator by Part Interaction Chart and meas-
urement by person (Operator Main Effects) chart all appear again. 
In the bottom right we see the Variation Breakdown table. Notice that there are separate columns for 
%Process based on the historical StDev we provided and that measured from the 10 parts used within 
the study.

Measurement System Analysis
241
On the Report Card we are given three pieces of key information about this type of study.
We are given information that our estimate of Reproducibility may not be that precise. Our value for 
Reproducibility was 25.95% under %Process (hist) which was not particularly large.
It’s exercise time again. 

242
Problem Solving and Data Analysis using Minitab 
Exercise 2. Gage R&R Study.
Omer is running an improvement project where he wants to improve a metric against speciﬁcation 
levels. He sets up a study examining the measurement system used when measuring the density of fuel 
pellets. He uses three operators and 10 parts. The parts represent the variation seen within the process. 
The tolerance for the products is (USL-LSL) 0.6 kg/m3. The historical StDev is given as 0.0753.
Check the results using a Gage Run Chart. Then using the results given in Worksheet: Fuel Density 
carry out a Gage R&R to establish if the measurement system is deemed to be acceptable for tolerance. 
Marginal is not good enough within the nuclear industry. Use the Classic Method and the Assistant, if 
available.
From the Gage Run Chart the initial assessment is that the measurement system is displaying repeatabil-
ity and reproducibility issues, a lot of them!

Measurement System Analysis
243
The Components of Variation 
graph tells us that the %Tolerance 
is classed as bad if we hover over 
the %Tolerance bar relating to 
Gage R&R.
Although the Gage Run chart 
showed us issues with both 
repeatability and reproducibility 
the R chart shows that the repeat-
ability variation is within control.
The Xbar chart shows points both 
inside and outside the control 
lines. We appear to be on the 
50% limit.

244
Problem Solving and Data Analysis using Minitab 
The Measure by Part chart shows 
that Part 5 had the greatest 
spread of measurements and Part 
1 had the least spread.
The Measurement by Operator 
chart shows that on average Aasim 
has the highest measurements.
Interactions in terms of lines 
crossing can be seen within the 
Part * Operator interaction chart. 
However, we don’t know if they 
are signiﬁcant.

Measurement System Analysis
245
For the ﬁrst time we see the Part * Operator interaction is signiﬁcant and has been carried through into 
the Gage R&R table.
The %Tolerance for the Total Gage R&R is 31.11%. This means that 31.11% of the tolerance is taken up 
by measurement system variation. This means that this measurement system is graded as unacceptable. 
Had we used %Process or %Study Variation as the study metric the results would have been even worse.

246
Problem Solving and Data Analysis using Minitab 
The Assistant tells us 
that this measure-
ment system is unac-
ceptable for process 
improvement or for 
improving the parts 
to the tolerance.
Again the Variation Report reﬂects the information presented within the Classic Method. The one dif-
ference being that it is possible to see the difference between %Process(data) and %Process(hist).

Measurement System Analysis
247
6.12 Gage R&R (Nested)
Example 5. Gage R&R Study.
Mahwish has set up a study examining the measurement system used when measuring the ﬁre resist-
ance of plasterboard. The measurement given is time taken for the plasterboard to fail when exposed to 
a set ﬂame. She uses three operators and six parts, each part is split into three pieces. The same operator 
tests the pieces from the same part. The parts represent typical parts seen within the process. 
Using the results table carry out a Gage R&R Nested study to learn if the measurement system is suitable. 
The data is in Worksheet: Fire Test.
1. Copy the data into Minitab.
Procedure
tGage R&R (Nested)
What's it 
used for?
t It is used to compare the reproducibility and repeatability 
elements of the measurement system with the overall study 
variation and, if required, tolerance.
Assumptions
and
Limitations
t The Nested study is used to assess measurement systems associated 
with destructive testing where large batches of identical parts 
cannot be produced. However, small batches must be available to 
allow a single operator to make repeated measurements from the 
same batch.
t Parts are randomly assigned to operators.
t It is better to have a large number of parts within the study as this 
reduces the chance that a particular operator will receive a part at 
the extreme of the measurement range.
t There is not an option with the Assistant for Nested Studies.

248
Problem Solving and Data Analysis using Minitab 
2. Click Stat <<Quality Tools 
<<Gage Study <<Gage R&R 
Study (Nested).
3. Complete the menu box as 
shown. I have entered some 
relevant information about 
the test into Gage Info.
4. Then click OK. 
Minitab produces this set of ﬁve graphs and also information within the session window. Note: the 
Operator * Part graph is not available as all operators do not measure all parts.

Measurement System Analysis
249
The Components of Variation 
Chart shows us that the Gage 
variation is made up completely 
of Repeatability. However, do 
not assume that Nested studies 
will not show Reproducibility 
errors.
We can also see that the Gage 
Error is smaller than the Part to 
Part error.
As we did not enter a tolerance 
we will not see any tolerance 
metrics.
The R chart shows that the 
measurement data for each 
part is under control. 
As the parts were distributed 
randomly we would expect 
the ranges for the operators to 
be almost the same. A higher 
range 
from 
one 
operator 
would indicate a Repeatability 
issue.
It can be seen that a number 
of the points are outside of 
the control lines. This indicates 
that the part to part variation 
is greater than the repeatabil-
ity variation.

250
Problem Solving and Data Analysis using Minitab 
This chart shows the individual plots for each part by operator. Minitab compares the between batch 
variation to the within batch.
If one operator is meas-
uring values higher than 
the others it could be 
due to the parts that 
they received. We there-
fore need to assign parts 
randomly to check if 
the differences are due 
to operators. The more 
parts we have the better.
In our graph Maryam 
appears to be measur-
ing a lower value than 
the other operators. This 
could be just due to parts 
A and B.
The %Study Variation 
is 17.81% for the total 
GR&R. All of this is from 
Repeatability Errors.
Number of Distinct 
Categories is seven. 
We can say that our 
measurement system is 
Marginal.
Next, there is an exer-
cise for you to complete.

Measurement System Analysis
251
Exercise 3. Gage R&R (Nested) Study.
Han has set up a study examining the measurement system used when measuring the impact resist-
ance of steel. He uses three operators and nine parts, and each part is split into three pieces. The same 
operator tests the pieces from the same part. The parts represent the variation seen within the process. 
The tolerance for this measurement system is 3 kN and the aim of this study is to improve parts to the 
tolerance.
Using the results table carry out a Gage R&R Nested Study to learn if the measurement system is suitable. 
The data is in Worksheet: Impact Test.
Again the Gage variation 
is made up completely of 
Repeatability and the same 
warning goes out again: 
do not assume that nested 
studies 
will 
not 
show 
Reproducibility errors. 
We see that the part to 
part variation and the total 
gage variation have similar 
values. This is a bad sign. 

252
Problem Solving and Data Analysis using Minitab 
The %Study Variation 
looks very high but this 
study is focused around 
the %Tolerance value. 
The R chart shows that 
the measurement data 
for each part is under 
control.
Only one of the parts is 
outside of the control 
lines. This indicates that 
the part to part variation 
is almost as great as the 
repeatability variation.

Measurement System Analysis
253
We see that the spread of measurements around Parts 4, 5 and 6 is quite large compared to the other 
parts. Iman may be having trouble measuring consistently.
The spread shown by the operators here may also be due to the parts.

254
Problem Solving and Data Analysis using Minitab 
In terms of %Tolerance 
this measurement sys-
tem is almost marginal. 
The %Tolerance ﬁgure 
is only close to being 
Marginal as the toler-
ance is quite wide. This 
is something that Han 
should consider.
Number of Distinct Cat-
egories is one. 
We can say that the 
measurement system is 
unacceptable in terms of 
%Tolerance and needs 
to be improved.

Measurement System Analysis
255
6.13 Gage Bias and Linearity Study
Example 6. Gage Bias and Linearity Study.
Uzzy wants to carry out a Gage Bias and Linearity study on the measurement system used to measure the 
caliper of plasterboard. The reference parts representing each of the standard sizes of board are used. 
The reference parts are 7.5, 9.5, 12.5, 15.0 and 19.0 mm. Each one is measured 12 times in a randomised 
study. It is known that six times the historical StDev is 0.38.
Using the results table carry out a Gage Bias and Linearity Study to learn if the measurement system is 
suitable.
Worksheet: Caliper B&L
Procedure
tGage Bias and Linearity Study
What's it 
used for?
t It is used to assess the Bias and Linearity of a Gage over its 
operating range.
Assumptions
and
Limitations
t At least ﬁve parts that cover the operating range of the gage 
must be used. There must be at least 10 readings from each part.
t The parts must be reference parts with known readings for each 
part.
t It is best if the runs can be randomised and the parts kept blind.
t There isn’t an option within the Assistant for Gage Bias and 
Linearity Studies.

256
Problem Solving and Data Analysis using Minitab 
1. Copy the data from 
the Excel ﬁle and 
paste it into Minitab.
2. Click Stat <<Quality 
Tools <<Gage Study 
<<Gage Linearity 
and Bias Study.
3. Complete the menu 
box as shown.
4. Then click OK. 

Measurement System Analysis
257
Minitab produces this output window with no information within the session window. We shall look at 
the results in detail and see how it helps us to understand our measurement system.
The scatterplot shows the bias 
(measured value – reference value) 
for each part. The red dot shows 
the average for each part. 
Using Linear regression a line is ﬁt-
ted through the bias data points on 
the scatter plot.
The regression line has a conﬁdence 
band around it. If the zero line is 
outside of the conﬁdence band 
then we have bias in our measure-
ment system when there are no lin-
earity issues.
As all points in this study are above 
the zero line we have a positive 
bias.

258
Problem Solving and Data Analysis using Minitab 
Minitab carries out two hypothesis 
tests on the regression line, one to 
check whether the constant (inter-
cept) of the line is zero and the 
other to check whether the slope 
of the line is zero.
We ﬁnd that we can reject that the 
intercept of the line is zero; that is 
it does not go through the origin.
More importantly we cannot reject 
that the slope is zero. This tells us 
that the study does not have a lin-
earity issue.
The Linearity as a percentage of 
process variation is 0.1%.
The gage bias table shows the 
average bias for each part and 
the values as a percentage of pro-
cess variation. The P values shows 
whether the bias was signiﬁcant 
for that value.
To check for overall Bias we would 
check the P value for the average 
of the points.
Finally, we get a graphical display 
of linearity and bias as a percent-
age of process variation.
In summary, the real issue for this 
measurement system is bias.

Measurement System Analysis
259
Exercise 4. Gage Bias and Linearity Study.
Parky wants to carry out a Gage Bias and Linearity study on a weigh scale. Reference weights are used 
which cover the range of the weigh scale. These are 100, 200, 300, 400, and 500 kg. Each one is measured 
10 times in a randomised study. Process Variation is 1.5 kg.
Using the results table carry out a Gage Bias and Linearity Study to learn if the measurement system is 
suitable.
Worksheet: Weights B&L
The slope of the regression 
line is signiﬁcant so we have a 
linearity issue. This is also seen 
in the scatter plot.

260
Problem Solving and Data Analysis using Minitab 
For the Average of the reference 
values we ﬁnd that the P value is 
signiﬁcant. This means that if the 
linearity issue were resolved there 
would still be a bias of 0.03 on 
average.
As a percentage of process varia-
tion Bias is the bigger issue.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 7
Statistical Process Control
7.1 The Origins of Statistical Process Control
Dr Walter A. Shewart is credited as being the father of Statistical Process Control (SPC). It was in the 
1920s when Shewart wrote to his boss and proposed his ideas. At that time he worked for the Western 
Electric Company at the Hawthorne Works. (Incidentally, this is the same Hawthorne Electrical Works 
where the famous Hawthorne Effect which relates to industrial psychology was recognised.)
Later William Demming applied the Statistical Control techniques to the production of munitions and 
essentials during WWII. Demming also worked in postwar Japan applying the same techniques that 
helped shape Japan as an industrial giant.
It was not until much later that control charts were used in nonmanufacturing environments, beginning 
with Computer Software.
Dr Shewart recognised the 
importance of reducing vari-
ation in manufacturing pro-
cesses. He also concluded that 
continual process adjustments 
by operators would in all like-
lihood increase variation and 
result in more defects.
He looked at problems in 
terms of common cause and 
special cause variation. He 
concluded that every process 
displays common cause vari-
ation and set limits to when 
the variation was caused by 
new or additional factors 
which he called special cause variation.
Dr Shewart used ±3 StDevs as the control limits to separate common cause variation from special cause 
variation. This ﬁgure shows a typical control chart indicating a process that is in control.

262
Problem Solving and Data Analysis using Minitab 
7.2 Common Cause and Special Cause Variation

Statistical Process Control
263
The control chart provides a simple way of detecting special cause variation. Usually the Control Limits 
that are calculated will be within the tolerance limits of the process. The aim is to react when the control 
limits are exceeded by investigating and rectifying the special cause. If we do this before we go out of 
the tolerance limits we avoid defects and waste.
7.3 Detection Rules for Special Causes
When using the Classic Method 
Minitab provides eight separate 
tests for detecting special cause 
variation. Test 1, which is the orig-
inal used by Dr Shewart, is always 
on by default. However, you can 
change the settings to your indi-
vidual needs as can be seen from 
the menu. Minitab also allows 
you to set the value of K which 
will alter the settings at which 
the individual Test works. You can 
change the default options within 
the Tools <<Options menu.
When using the Assistant the tests 
used to detect special causes are set by default. In fact, the Assistant only uses Tests 1, 2 and a modiﬁed 
version of Test 7 for the charts it produces. You can read about the tests used within the Assistant in the 
Minitab White Paper for Control Charts. 

264
Problem Solving and Data Analysis using Minitab 
Test 2: nine points in a row, all on the same side 
of the centre line.
In order to show the special causes that Minitab 
can detect we are going to use the following 
control chart. It has a mean of 100 and StDev 
of 10. Note, where there is a sequence of test 
points involved in a test that has failed Minitab 
only highlights the last point in the sequence.
Test 1: one point more than three StDevs from 
the centre line (outside the control limits).
Test 3: six points in a row all increasing or all 
decreasing.

Statistical Process Control
265
Test 4: 14 points in a row alternating up and 
down.
Test 5: two of three points more than two StDevs 
from the centre line on the same side.
Test 6: four of ﬁve points more than one StDev 
from the centre line on the same side.

266
Problem Solving and Data Analysis using Minitab 
Test 7: 15 points in a row within one StDev of the 
centre line (either side)
Test 8: eight points in a row more than one StDev 
from the centre line (either side).
7.4 False Alarms
You may have already realised that there could be a slight problem. Even when you have a normally 
distributed process by deﬁnition only 99.73% of the points will be within three StDevs of the mean. 
Therefore, if you produce a control chart of a normally distributed process 0.27% of the points will be 
outside the control limits, we call these false alarms. This is because these points are not the result of a 
special cause but due to the intrinsic variation within the process that has resulted in the point being 
outside the control limits. 0.27% is the equivalent of one point in 370. If the process has extended tails or 
we add Tests 2–8 there will be additional false alarms. And remember the expectation is that all special 
cause variation will be investigated. 
Therefore, we must set a balance between the detection rules and generating alarms so that we don’t 
miss any important special causes and we don’t generate too many false alarms.

Statistical Process Control
267
7.5 When Should We Use SPC Charts?
There is a massive number of potential uses for control charts. Most of the uses can be broken down into 
four main categories. 
Monitoring This is the obvious one. We want to be able to monitor our process and identify special 
cause variation. When we see a control chart we can be conﬁdent and feel secure that our process is 
being monitored and should things go out of control a reaction plan will come into play. This will alert 
the right people so that they can investigate and rectify the situation. It should be an awful lot more 
than a display of data.
Process Improvement Typically when we start improvement work our system is out of control. Initially 
we should try to achieve stability before we try to understand our system and then ﬁnally improve. The 
control chart can be an invaluable tool when it comes to bringing stability to the system and it helps 
us to understand and eliminate special cause variation. As stated in the Minitab White papers for tests 
related to Hypothesis Testing, including ANOVA, outliers or data related to special cause variation can 
have a strong inﬂuence on the results of the tests. Therefore, if we are working on an improvement 
project we should aim to make the process stable, in control, before we use hypothesis testing and other 
characterisation procedures.
Characterisation As we are trying to achieve stability and start to identify special cause variation we can 
trend multiple variables to identify the root cause of the special cause variation. This type of characteri-
sation work is shown in Exercise 1.
Defect Reduction Usually our control limits will be within our tolerance limits for the process. Therefore, 
if we are using control charts to monitor the process we can react and rectify the process before the 
speciﬁcation limits are breached. This will reduce defects.
68.27%
95.45%
99.73%

268
Problem Solving and Data Analysis using Minitab 
7.6 Subgrouping
Before we start to discuss the types of chart we will be using we need to understand subgrouping. 
Imagine a process where we are ﬁlling cans at a rate of 5000 per hour and require a consistent ﬁll 
level. In order to monitor the process we decide to take six consecutive cans every hour and check their 
ﬁll level. This gives us a subgroup size of six. From our process knowledge we know that the variation 
between these six cans represents the short term variation of the process and the variation within the 
subgroup is probably the best we can achieve. 
Due to the deliberate manner in which we are taking six consecutive samples from the line we can 
assume that when special cause variation takes place it will occur in the time period between sampling. 
It is due to the design of our sampling/subgrouping that we can make the link between Special Cause 
Variation and Between Group Variation. 
Remember in the ANOVA chapter we said that ‘Overall Variation is the sum of the Within Group and 
the Between Group Variation’. Due to our foresight we have set up our sampling regime so that the 
variation within the subgroup will be representative of common cause variation. The variation between 
subgroups will capture the special cause variation.
7.7 The Appropriate Chart
We will be looking at two common types of Control Chart which use subgroups. They are the Xbar-R 
chart and the Xbar-S chart. Both of these charts are split into two sections. The Xbar chart is always at 
the top and this displays the average of the subgroup. The Xbar chart is usually accompanied by either 
the R chart or the S chart, where R denotes the range of the subgroup and S denotes the StDev of the 
subgroup. It can be seen that on both charts the Xbar is displaying the average of the subgroup and the 
R or S charts are attempting to display the within subgroup variation.
Charts which use subgroups give greater protection from false alarms generated by non normal distribu-
tions. As the subgroup size increases the greater the level of protection.
When it has not been possible to sample and analyse subgroups we use the I-MR chart. This is made up 
of an individual value chart and a Moving Range chart. The I-MR chart would probably be used where 
the sampling or analysis was particularly complex or onerous. 

Statistical Process Control
269
Having a subgroup size of one indicates individual data points without any subgrouping. If N = 1, then 
the choice is simple use an I-MR chart. 
On occasion there will be more than the within variation trapped inside the subgroup. On these occa-
sions we use an I-MR R/S chart.
If we have established that we have subgrouping and that it only contains Within variation the recom-
mendation has always been that if you have a subgroup of less than nine you would use the Xbar-R 
chart and for anything above that use the Xbar-S chart. Note that the R chart only uses the points at 
the extremes to represent within group variation whereas the S chart uses all the points within the 
subgroup.
7.8 The I-MR Chart
Procedure
tI-MR Chart (Classic Method)
What's it 
used for?
t It is used to detect changes in time ordered data where individual 
values are being examined.
Assumptions
and
Limitations
t I-MR charts are usually used where it is not possible to sample and 
analyse subgroups.
t The false alarm rate can be four to ﬁve times higher when the 
distribution is not normal.
t Using optional additional tests is likely to increase the false alarm 
rate.
t Control limits are calculated using the average moving range.
t Help <<Methods and Formulas shows how the control limits are 
calculated.

270
Problem Solving and Data Analysis using Minitab 
Example 1. I-MR Chart.
Willy Winker is developing a new Chocolate within his R&D laboratory. He takes a sample from the line 
every 5 min and tests it for crunchiness. Testing for crunchiness is a difﬁcult and time consuming process 
therefore subgroups are not analysed.
Produce an I-MR Chart of Crunchiness and explore any process changes.
All data sets for this module are in ﬁle 08 SPC.xls. This example uses Worksheet: Crunchiness.
1. Copy all the data from 
Worksheet: Crunchiness 
into a new Minitab pro-
ject worksheet. 
2. Click on the Information 
icon so we can check 
the amount of data and 
whether the format-
ting is correct.
Procedure
tI-MR Chart (Assistant)
What's it 
used for?
t It is used to detect changes in time ordered data where individual 
values are being examined.
Assumptions
and
Limitations
t The recommendation from the Minitab White Paper on Variable 
control charts is to have at least 100 data points.
t The Assistant will only conduct Tests 1 and 2 for the I Chart and 
only Test 1 for the MR Chart.
t If the data is generating a lot of false alarms and certain condi-
tions are met, the Assistant will offer to transform the data for 
you; see example 2.

Statistical Process Control
271
3. We see that there is no missing 
data and the column formatting 
is correct. There are 121 rows of 
data, which are for 121 samples.
4. Click Stat <<Control Charts 
<<Variables for Individuals 
<<I-MR.
The I-MR Chart produces the Individual 
and Moving Range charts on the 
same sheet. As can be seen within the 
full menu there are options available 
which will allow the production of 
only one of these charts if required. 
This ﬂexibility is only available using 
the Classic Method.
5. Select Crunchiness as the vari-
able to be plotted.
6. Then Click on Scale. The Scale 
option allows us to use our own 
labels for the x axis. The default 
setting is just an indexed num-
ber for each point.
7. For the X Scale click on the 
Stamp radio button.
8. Select Time ﬁrst then Date. We 
can add three layers of labels. 
Minitab is telling us to put the 
innermost ﬁrst, in our case we 
want Time to be immediately 
next to the x axis.
9. Click OK and OK again.

272
Problem Solving and Data Analysis using Minitab 
Our I-MR chart is split into two sections. The top shows the individual values, there should be 121 of 
these. The bottom is the Moving Range chart. This shows the difference in range of sequential points. 
As it takes the difference of sequential points there are 120 points.
If we look at the data within the charts we see that none of the points are out of control. We also see a 
period of lower variability towards the end of the run. We can hover over points to get the time when 
this period started and then set up new control limits for the end of the run.

Statistical Process Control
273
10. In order to set up new control limits 
for the period of lower variability, 
which was at the end of the run, click 
on the Edit Last Dialog Box icon.
11. Click on I-MR Options.
12. Click on the Stages tab.
13. Use Time to deﬁne the Stage.
14. Use 16:45 as the time to start a new 
stage. 16:45 is the time that the period 
of lower variability is estimated to 
have started. 
15. Click OK and OK again.

274
Problem Solving and Data Analysis using Minitab 
We now have two sets of control lines and two mean lines on each chart as Minitab has calculated new 
control values for each period. However, the values for Xbar, MRbar, UCL and LCL only relate to the lat-
ter part of the chart. We can ask Minitab to display these parameters for each stage.
16. Click on the edit last dialog box icon.
17. Click on I-MR Options.

Statistical Process Control
275
18. Click on the Display tab.
19. In order to display all the stage 
data click into the box for Display 
control limit/center line labels for 
all stages.
20. Click OK and OK again.
We now have control limits and centre line values for both sections of the chart. If it does look untidy 
or you cannot read the values as they are being obscured you can always drag the values to a more suit-
able location.
We have identiﬁed a trend within the common cause region and we will use additional I-MR charts to 
investigate additional factors that might be involved with the period of lower variability for Crunchiness.

276
Problem Solving and Data Analysis using Minitab 
Next to the Crunchiness column there are also columns of data for the following factors: Nougat, 
Caramel, Sugar, Water and Gelling Agent. Data was also collected on these parameters at the same time 
that data was collected on Crunchiness. 
We will use I-MR charts to inves-
tigate each of the factors in turn 
and check to see if any changes 
can be seen after the 16:45 index. 
We will use the Assistant to pro-
duce an I-MR chart for Nougat.
1. Click 
Assistant 
<<Control 
Charts.
2. The Assistant will produce 
charts for continuous and 
attribute data but we are 
only going to focus on con-
tinuous data. Click on the 
Icon for the I-MR chart.
3. Select Nougat as the Data 
column.
4. Notice that the Assistant 
gives us the option of specify-
ing control limits from known 
values. We might want to use 
these if we were producing 
charts for an already estab-
lished process with estab-
lished control limits. As we 
don’t want this we will ask 
Minitab to calculate control 
limits from the Data.
5. Prior to producing the chart 
the Assistant scans the data 
and alerts us if any points are 
out of control or unstable 
and gives us the option of 
omitting them from the data. 
We will leave these points in. 
The Assistant actually carries 
out a number of checks these 

Statistical Process Control
277
include checks for normality, Auto correlation and amount of data. The results of which will be 
shown in the Report Card
6. Click OK to produce the chart.
On the top left of the Summary Report we see an indicator bar that should now be familiar to users of 
the Assistant. It is asking whether the process mean is stable and the answer is no because 13.2% of the 
points are out of control. 
On the right hand side we see the I and MR charts. We can see that there has been a shift in the Nougat 
level near the time we were interested in. Disappointingly, we cannot set the index within the Assistant.

278
Problem Solving and Data Analysis using Minitab 
The Stability Report enlarges the I-MR charts allowing us to look for patterns more easily. At the bottom 
of the page we see the points that were deemed to be unstable are listed.
The Report Card follows a familiar format as well. We are told that normality was not an issue because 
less than 2% of the points were outside of the control limits. If this condition was not true the Assistant 
would carry out a normality check on the data. If appropriate it would offer to transform our data if we 
required.
As the time when the Nougat level reduced was similar to the reduction in variation in Crunchiness we 
have discovered a possible link that merits further investigation. In the real world we would carry out 
further trials in order to investigate if there was a link.

Statistical Process Control
279
Exercise 1. I-MR Chart.
We wanted to investigate the change that we noticed in Crunchiness and we have produced an I-MR 
chart for Nougat. The I-MR chart for Nougat showed us that the level of Nougat reduced during the 
period we were interested in. There are four remaining factors that need to be investigated; Caramel, 
Sugar, Water and Gelling Agent. Use the Classic Method to produce I-MR charts for Caramel and Sugar, 
use Time as the x axis scale. Use the Assistant to produce I-MR charts for Water and Gelling Agent. Look 
for trends within the charts that could help explain the change in Crunchiness. 
Worksheet: Crunchiness.
The I-MR chart for Caramel is consistently in control. No unusual trends can be seen within the Individuals 
chart or the Moving Range chart.
 No Change at 16:45

280
Problem Solving and Data Analysis using Minitab 
The I-MR chart for Sugar is consistently in control. No unusual trends can be seen within the Individuals 
chart or the Moving Range chart.
 No Change at 16:45

Statistical Process Control
281
The Summary Report for Water states that the process mean is stable with no points going out of 
control. The I-MR chart for Water is consistently in control. No unusual trends can be seen within the 
Individuals chart or the Moving Range chart.

282
Problem Solving and Data Analysis using Minitab 
The Summary Report for Gelling Agent states that the process mean is stable with no points going out 
of control. The MR chart does show two points that are out of control. Within the I chart we see that 
there is reduced variability during our target time which we could only identify by looking at the charts.
In Summary we have used the I-MR charts not to investigate process stability but to explore a trend that 
was of interest. We managed to link the factors of Crunchiness with Nougat and Gelling Agent. In the 
real world this would only be a start and we then might want to set up further experiments or further 
investigate the link within our factors by using hypothesis testing.
Example 2. I-MR Chart.
As an experiment Humzah decides to check how long it takes him to roll two sixes. He starts a timer and 
then continuously rolls the dice until he throws two sixes. He repeats this 120 times. Produce an I-MR 
chart of the data to assess whether the process is in control. 
Worksheet: Dice.
1. Copy the single column of data from Worksheet: Dice into a new Minitab project 
worksheet.

Statistical Process Control
283
2. Click Stat <<Control Charts <<Variables for 
Individuals <<I-MR.
3. Select Time as the variable to be charted.
4. Click on OK to produce the chart.
5. Inspect the graph that has been produced.
A couple of things stand out about the graph. The ﬁrst is that there are a number of points out of con-
trol on both sections of the graph. 
We need to have a look at the type of distribution we are dealing with so we can make some better 
decisions. We will produce a histogram of the data.
6. Select Graph <<Histogram.

284
Problem Solving and Data Analysis using Minitab 
7. Select the option for 
producing a histogram 
With Fit.
8. Select Time as the vari-
able to be plotted. Click 
OK.
9. Inspect the histogram.
It can be seen that the histo-
gram is right skewed. There 
are now two possibilities, one 
is that the distribution should 
be normal and the out of 
control points that have been 
identiﬁed will help us correct 
the process. The other possi-
bility is that the process does 
not follow a normal distribu-
tion and the out of control 
points are false alarms. 
Due to the fact that we are dealing with a very simple process with a fairly large sample size we can 
assume that the skewed distribution is representative of the process. After making this decision we can 
use a Box–Cox Transformation to transform this distribution into a normally distributed one. For this 
transformation to work the distribution must be positively (or right) skewed and all values must be 
greater than zero.
We should not take the decision to transform lightly. We should only transform the data if we are sure 
that out of control points are due to the natural distribution of the process and not due to special causes 
inﬂuencing the data. This means that we should not transform the data if we were expecting a normally 
distributed process.

Statistical Process Control
285
The transformation process is 
two step. Initially we need to 
ﬁnd the optimal value of the 
conversion factor, lambda.
10. Click Stat <<Control 
Charts <<Box–Cox 
Transformation. Ensure 
the selector is set to 
all values in one col-
umn. Select Time as the 
Variable and Subgroup 
size as 1.
11. Click on OK.
12. The Box–Cox Plot is pro-
duced. We are given the 
95% conﬁdence interval 
for the optimal conver-
sion factor. The value 
used in the chart is the 
Rounded 
Value. 
The 
rounded value is usually 
a value that is easy to 
use within the transfor-
mation, such as 0 which 
corresponds to the natu-
ral log or 0.5 which cor-
responds to the square 
root. We have been 
given a rounded value of 
0.00 for this conversion.
13. Click Stat <<Control Charts <<Variables for 
Individuals <<I-MR.
14. Click on I-MR Options
15. Click on the Box–Cox tab. 

286
Problem Solving and Data Analysis using Minitab 
16. Activate the Use a Box–Cox Transformation 
tick box. Having analysed for the optimal 
value of Lambda we know that we want to 
use the default value of Lambda = 0.
17. Click on OK and OK again. 
Minitab produces a control chart using the transformed data. We can see that both sections of this chart 
are in control. Notice that the control limits are also in equivalent transformed units.

Statistical Process Control
287
1. In order to produce 
the Control Charts 
for this data using 
the Assistant click 
Assistant <<Control 
Charts.
2. Click on the Icon for 
the I-MR chart.
3. In the Menu box that 
opens select Time as 
the 
Data 
Column 
and set the selector 
box to estimate con-
trol limits and centre 
line from the data.
4. Minitab will ask if 
we wish to omit any 
of the out of con-
trol points from the 
data but we want 
to leave them all in. 
Click OK.
Due to the number of 
out of control points the 
Assistant carries out a nor-
mality check on our data. 
We are warned that our 
data is non normal and 
due to this we may see an 
increased number of false 
alarms. We are asked if 
we want to transform our 
data with the guidance 
that we should not trans-
form the data if we were 
expecting our process to 
produce normal data.
5. Click on Yes to trans-
form the data.

288
Problem Solving and Data Analysis using Minitab 
The Stability Report shows us the same charts that we saw previously when we used the Classic Method.
The Report Card gives us the all clear in terms of the checks that it carries out.
Exercise 2. I-MR Chart.
Iqra measures the concentration of micro-organisms on industrial air conditioning ﬁlters. From historical 
data she knows that the distribution is naturally skewed. Help Iqra produce an I-MR chart of her results. 
Initially produce a histogram of this data and visually check if the distribution is positively skewed and 
all values are greater than zero. Then use the Classic Method and the Assistant to produce an I-MR chart 
of the transformed data.
Worksheet: Air.

Statistical Process Control
289
From the histogram it can be seen that the distribution is positively skewed. A check of the data shows 
that all values are greater than zero.
The Box–Cox Plot is 
produced and gives 
us a Rounded Value 
for Lambda of 0.26. 
When you go to the 
Box–Cox tab and 
engage the selector 
for the transforma-
tion also select the 
radio button for the 
Optimal lambda.

290
Problem Solving and Data Analysis using Minitab 
With the transformed data we can see that both the I chart and the MR chart are in control.
When we use the Assistant to complete the same procedure the Stability Report should show us the 
same charts that we saw previously when we used the classic method. 

Statistical Process Control
291
The Report Card 
should not give 
us any warnings.
7.9 The Xbar-R Chart
Example 3. Xbar-R Chart.
On the chocolate bunny line Willy Winker is taking ﬁve samples at the start of every hour and weighing 
them. Initially four sets of samples were taken in the startup phase but these are not deemed to be a 
part of the production run.
Produce an Xbar-R chart of bunny weight and explore any process changes.
Worksheet: Bunny.
Procedure
tXbar-R Chart
What's it 
used for?
t It is used to detect changes in time ordered data where subgroups 
are being examined. Subgroup sizes should be between two and 
eight, inclusive.
Assumptions
and
Limitations
t Used when there are subgroups and not individual data points.
t The subgroup (within group) variability must be in control before 
examining the Xbar values (between groups). This is because the 
subgroup variability is used to calculate the control limits for the 
Xbar control chart.
t Help <<Methods and Formulas shows how the control limits are 
calculated.

292
Problem Solving and Data Analysis using Minitab 
1. Copy all columns of data from 
Worksheet: Bunny into a new 
Minitab project worksheet. The 
hourly times that the samples 
were taken are in the Time col-
umn and within the same row 
are the ﬁve weights that were 
recorded. Therefore, we will 
consistently have a subgroup 
size of ﬁve within this chart.
2. Click Stat <<Control Charts 
<<Variables for Subgroups 
<<Xbar-R.
3. The Xbar-R chart menu opens. 
Normally Minitab takes data in 
columns so we must tell it that 
the subgroup data is across 
the rows. Select ‘Subgroup 
Observations in Rows’
4. Select columns Weight1 to 
Weight5 as the data to be 
charted.
5. Then Click on Scale. For the X 
Scale click on the Stamp radio 
button. Select Time.
6. Click OK and OK again.

Statistical Process Control
293
We have produced an ini-
tial Xbar-R chart but really 
we should have removed 
the startup data before we 
started. We actually did 
this to demonstrate the use 
of the Data Options menu 
which can be used within 
any of our chart types when 
using the Classic Method. 
We are going to use the Data 
Options Menu to remove 
the ﬁrst four subgroups.
7. Click on the edit last 
dialog box icon.
8. Then click on the Data 
Options button.
The menu works by either 
specifying which rows to 
include or exclude, depend-
ing on which is easier for the 
user. There are also a num-
ber of options on specifying 
the rows. It can be done by 
row number, brushed rows 
or we can set up logical 
conditions. When specify-
ing items Minitab uses the 
same system all the time. 1:4 
means 1 to 4 inclusive. 2:6 
10 would mean 2 to 6 inclu-
sive plus 10.
9. Select the radio button 
to specify which rows 
to exclude.
10. Select the radio to 
specify selected row 
numbers. Enter 1:4.
11. Click OK and OK again.

294
Problem Solving and Data Analysis using Minitab 
There is a single point on the R chart that is out of control. We must address this ﬁrst before we consider 
the Xbar chart. This is because the within subgroup variation is used to calculate the Xbar control limits 
on the Xbar-R and Xbar-S charts.
We will now have a look at the data within the 01:00 subgroup.
12. Click on the Show Worksheets icon.
We see that there is an unusual value 
for Weight1 at 01:00 of 7.0. After a 
thorough and exhausting investigation 
we found that the value was a typing 
error; it should be 27.
13. Correct the value from 7 to 27 by 
editing the value on the worksheet.
14. We are going to update the graph 
using a shortcut rather than go 
through the menus again. Click on 
the Show Graphs icon.

Statistical Process Control
295
15. Right click on the graph and select Update Graph Automatically.
Having selected this option the graph will now automatically reﬂect the data within the worksheet. 
There is also the option to Update Graph Now which is a single update.

296
Problem Solving and Data Analysis using Minitab 
We see that the R chart is now in control. However, a different point on the Xbar chart is now out of 
control. The reason for this is that the high subgroup range generated by the data at 01:00 widened the 
control limits for the Xbar chart. Since that has been corrected the Xbar control limit width has become 
smaller.
We now need to investigate why the subgroup average at 20:00 is high.
Looking at the 20:00 subgroup we 
see that all the values are at the top 
end of the range giving a high aver-
age value. Although the point is out of control it is deemed to be acceptable by Willy Winker.
This example is intended to highlight that the correct procedure is always to address the control limits 
on the R chart prior to addressing issues on the Xbar graph. This is because the within subgroup varia-
tion is used to calculate the Xbar control limits on Xbar-R and Xbar-S charts. On I-MR charts the average 
moving range is used to calculate the control limits on the Individuals chart.
We will now chart the same data set using the Assistant. As we have changed the data set it would prob-
ably be a good idea to reload the data set into a new Minitab project.

Statistical Process Control
297
1. Click Assistant <<Control 
Charts.
2. Click on the Icon for the 
Xbar-R chart.
3. Again, we must tell Minitab 
how the data is arranged. 
Select Data are in Multiple 
Columns from the drop 
down box.
4. Select the Data Columns as 
Weight1 to Weight5 and 
use the next selector to tell 
Minitab that we want con-
trol limits to be Estimated 
from the Data.
As soon as we tell Minitab how 
we want the control limits to 
be calculated we are asked to 
omit the points which are out of 
control.
5. Click the selector to omit 
the ﬁrst four points which 
were not part of the pro-
duction run.
6. Click OK.

298
Problem Solving and Data Analysis using Minitab 
On the Stability Report 
we see both the Xbar 
and R charts. It looks as 
though Minitab has not 
omitted the ﬁrst four 
points. In actual fact it 
has omitted the ﬁrst four 
points from the calcula-
tions of the control limits 
and the mean. In order to 
tidy up the chart we will 
delete the data points 
that we don’t want from 
the actual worksheet.
7. Select the ﬁrst four rows 
of data by left clicking 
and dragging.
8. Click Edit <<Delete Cells.
9. Correct the Typo, change 
7 to 27, which is on the 
ﬁrst column of the 01:00 
subgroup.
The Assistant does not allow the chart to be updated as per the Classic Method. When using the Assistant 
the chart must be redrawn from the Assistant menu.
10. Redraw the Xbar-R chart using the Assistant menu.

Statistical Process Control
299
Once the chart is redrawn the Stability Report should look like the one shown here. This is exactly the 
same as the one we produced using the Classic Method.
7.10 The Xbar-S Chart
Procedure
tXbar-S Chart
What's it 
used for?
t It is used to detect changes in time ordered data where subgroups 
are being examined. The chart is for subgroup sizes of nine and 
greater.
Assumptions
and
Limitations
t Used when there are subgroups and not individual data points.
t The subgroup (within group) variability must be in control before 
examining the Xbar values (between groups). This is because the 
subgroup variability is used to calculate the control limits for the 
Xbar control chart.
t Help <<Methods and Formulas shows how the control limits are 
calculated.

300
Problem Solving and Data Analysis using Minitab 
Example 4. Xbar-S Chart.
Willy Winker ‘s secret to making a good doughnut is the internal hole diameter of the doughnut. This is 
carefully monitored and controlled at the plant using Xbar-S charts. Ten doughnuts of Type A are sam-
pled on the hour. At half past the hour, ten doughnuts of Type B are sampled.
Produce an Xbar-S chart of doughnut hole diameter and explore any process changes.
Worksheet: Donut.
1. Copy all columns of data 
from Worksheet: Donut 
into a new Minitab pro-
ject worksheet. 
2. Click onto the Information 
icon and check the data 
formatting.
3. We see that there are 48 
rows of data. There are 
ten columns of data on 
internal doughnut hole 
diameter which will form 
the subgroups. The Shift 
column contains the times 
that the doughnuts were 
sampled.
4. Click Stat <<Control Charts 
<<Variables 
Charts 
for 
Subgroups <<Xbar-S.
5. Select ‘Observations for a 
subgroup are in one row’.

Statistical Process Control
301
6. Select the data from Dia01 to 
Dia10 to form the subgroups to 
be plotted.
7. Then Click on Scale. 
8. Click on the Stamp radio but-
ton and then select Shift as the 
Stamp. Click OK and OK again.

302
Problem Solving and Data Analysis using Minitab 
On the S chart we see one point that is out of control and we see an unusual pattern within the Sbar 
chart.
We will produce the chart again but on this occasion we will switch the additional tests on.
9. Click onto the edit last dialog 
box icon.
10. Click onto Xbar-S Options.

Statistical Process Control
303
11. Click onto the Tests tab.
12. Click onto the selector tab and choose 
Perform all tests for special causes.
13. Click OK and OK again.
We see that the Sbar chart fails Test 4 in addition to Test 1. Test 4 is failed due to 14 points alternating 
up and down.
We need to investigate the reason for the additional failure of Test 4.

304
Problem Solving and Data Analysis using Minitab 
14. Click on the Session win-
dow icon. More detail of 
the test failures can be 
seen. We are given a list of 
the points that failed each 
test. For Test 4, these are 
the points that appear at 
the end of the consecu-
tive sequence of 14 points, 
which alternate up and 
down.
In order to try and understand 
the failure we are going to pro-
duce separate charts for both 
Type A and Type B doughnuts.
15. Click on the Edit Last 
Dialog box icon.
16. Click on Data Options.
We are going to use a logical 
condition to select the type 
of donut that we are going to 
chart.
17. Under Specify Which 
Rows to Include, click on 
the radio button for Rows 
that match. Then click on 
Condition.

Statistical Process Control
305
18. Within the Condition box enter 
Type = ‘A’ as shown. This means 
that only rows of type A dough-
nut will be selected for the chart. 
The conditional selector is quite 
powerful and allows the logical 
conditions to be extended by 
using And, Or and Not.
19. Click OK, OK and OK again.
20. Having multiple charts with the same title can get confusing so double click onto the chart title and 
edit it to include the doughnut type.
Both the Xbar and S chart are in control for Type A.
21. As a mini exercise produce the Xbar-S for Type B.

306
Problem Solving and Data Analysis using Minitab 
We see that most of our problems have been cured by separating Type A and Type B doughnuts. The 
problem was simply having two different items on the chart. The two types of doughnut trapped what 
appeared to be too much special cause variation between the subgroups.
For Type B doughnuts we still see one point that is out of control, this is the subgroup for 13:30. We must 
investigate this before making conclusions about the Xbar graph.
The S chart is out of control on a single point. However, since the Xbar chart control limits will be based 
on the within group variation, we must ﬁrst address the issues on the S chart before we continue.
Looking at the data for time 13:30 Willy ﬁnds another typing error. Dia6 should be 30 not 40.
22. Change the value of Dia6 and update the chart for Type B doughnuts and check if there are any 
further points out of control.

Statistical Process Control
307
With the correction the S chart is now under control and we can now conclude that the Xbar chart is 
also under control.
Within the example we saw the importance of correctly setting up subgroups. Although the doughnuts 
of Type A and Type B have a similar average hole diameter, the StDev within the subgroups is very dif-
ferent. Type A has an Sbar of 2.861 and Type B of 1.465.
We will not repeat this example using the Assistant as the approach would be quite different and there 
would not be any new learning.
7.11 SPC Exercise
Exercise 3.
Willy Winker is developing a new chocolate coin and wants to understand the properties of the foil 
that the coins will be wrapped in. He understands that the pulse time of the electroplaters (P_Time) 
and concentration of the electrolyte (Conc) both affect the wear resistance of the gold plate on the foil 
wrapping.
In order to ensure both of these properties are in control and predictable before further development 
work is done Willy Winker wants conﬁrmation using the appropriate control chart.
Produce control charts for both P_Time and Conc using the data provided within Worksheet: Coin. The 
data has already been put into subgroups. Use the Classic Method and the Assistant to produce the 
charts.

308
Problem Solving and Data Analysis using Minitab 
As P_Time has a subgroup 
size of 10 we should use an 
Xbar-S chart to check if the 
data is in control. We see that 
for P_Time both the Xbar 
and S charts are in control. 
Using the Classic Method or 
the Assistant produces the 
same charts.

Statistical Process Control
309
As Conc has a subgroup size of 
ﬁve we should use an Xbar-R 
chart to check if the data is in 
control. For Conc we see that 
both the Xbar and R charts 
are in control.
Knowing that there isn’t any spe-
cial cause variation that affects 
these products will help produce 
a stable foundation for further 
development work.

310
Problem Solving and Data Analysis using Minitab 
7.12 The I-MR-R/S Chart
Example 5. I-MR-R/S Chart.
In a bid to break Willy Winker’s monopoly on cream cakes, Mahwish starts to mass produce her double 
jammed cream scone. She carefully monitors her process by checking one scone every hour. The diam-
eter of the sample is checked in three planes, 120° apart and the results are recorded. Produce an Xbar-R 
chart of scone diameter and establish if the diameter is out of control. Repeat the exercise using an 
I-MR-R/S chart.
Worksheet: Scone.
1. Copy all columns of data from Work-
sheet: Scone into a new Minitab pro-
ject worksheet.
2. Click Stat <<Control Charts <<Variables 
for Subgroups <<Xbar-R
Procedure
tI-MR-R/S Chart
What's it 
used for?
t It is used to detect changes in time ordered data where subgroups 
are being examined.
Assumptions
and
Limitations
t The I-MR-R/S chart is used when the data contains subgroups. 
However, the subgroups contain more variation than just the 
Within variation.
t Using a Xbar-R or Xbar-S with this type of data could hide out of 
control points as the Xbar variation is based on the exaggerated 
within Subgroup variation.
t The I-MR-R/S chart is not available in the Assistant.

Statistical Process Control
311
3. Select ‘Subgroup Observations in Rows’ 
and then select the three columns of data 
as shown.
4. Click OK to produce the chart.
We see that the Xbar-R chart shows that the process is control in terms of the sample range and the 
subgroup mean.
However, on this occasion we know that the subgroup is capturing common cause variation and the vari-
ation between the three positions where diameter is measured. Therefore, the Within Group variation 
estimate is exaggerated. As we know that the limits on the Xbar chart are calculated using the Within 
Group Variation we know they will also be exaggerated. That is why we use the I-MR-R/S chart and it 
also highlights the need to be mindful when selecting subgroups.
5. To produce the I-MR-R/S chart Click 
Stat <<Control Charts <<Variables 
for Subgroups <<I-MR-R/S.

312
Problem Solving and Data Analysis using Minitab 
6. Select ‘Observations for a subgroup 
are in one row’ and then select the 
three columns of data as shown. 
Notice that the menu and the sub-
menus still carry the same format as 
the other charts we have produced 
using the Classic Method.
7. Click OK to produce the chart.
The I-MR-R/S chart shows that the process is out of control. The chart is made up of three subgraphs. The 
top one shows the subgroup mean. The middle one shows the moving range of the subgroup mean. It is 
the short term variation of the moving mean that is used to calculate the control limits of the top graph.
The bottom chart gives the subgroup sample range. It is the same graph at the bottom of the Xbar R 
chart we previously produced. Minitab displays an R chart or an S chart depending on the chosen estima-
tion method and the size of the subgroup.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 8 
Process Capability
8.1 The Basics of Process Capability
Process Capability studies are necessary to ensure that our process outputs meet our customer 
speciﬁcations.
We want to ensure the distribution produced by our process, Voice Of the Process (VOP) lies completely 
within the speciﬁcation of the customer, Voice Of the Customer (VOC).
Capability
Voice of the Customer
Voice of the Process
VOC
VO
=
=
P
Equation (8.1)
We use the following to measure this type of Process Capability:
Cp, Cpk, Pp, Ppk
Other measures include DPMO (%).
In order to introduce the basic concepts we will work through the ﬁrst example and learn the terminol-
ogy used within Process Capability. 
Example 1. Dam Busters.
Sir Barnes Wallis developed the bouncing bomb in order to attack the Ruhr dams. There was a lot of 
development work done to ensure the best design was used. For this example let’s pretend that we 
want to explore the process capability of the bomb design. We will consider the cylindrical bomb and 
the spherical bomb.
Explore the capability of both types of dam busting bomb when ﬁred from a Lancaster bomber. 
The distribution of bomb spread is given for the optimal ﬁring distance, therefore, we need only con-
sider the x dimension.
No data ﬁle.

314 Problem Solving and Data Analysis using Minitab 
Let’s start by looking at the cylindrical bomb. Let’s say that the width of the dam is 30 m; we can say 
that Upper Speciﬁcation Limit (USL) – Lower Speciﬁcation Limit (LSL) is 30 m. This is the Voice Of the 
Customer (VOC).
Following extensive development work it has been found that there is a spread of impact locations that 
the bomb will produce when it is ﬁred from the optimal ﬁring distance. This spread or distribution has 
been checked and it has been found that the StDev is 3 m. Let’s say that the process we are looking at is 
normally distributed. This means that the Voice of the Process (VOP) is represented by 6V. If this was not 
a normally distributed process we would not easily be able to predict the process spread. For this reason 
we are going to stick to normally distributed processes for the ﬁrst half of this module.
C
VOC
VOP
USL
LSL
6
6
3
p =
=
-
=
=
s
30
1 67
¥
.
Equation (8.2)
Since we know the VOC and the VOP we can now calculate the capability. We get a value of 1.67.
We will now calculate the capability of the spherical bomb. The VOC remains at 30 m because the dam 
width stays the same.
Dam
Width =
30
USL
LSL
Distribution of  spread
when fired, σ = 3 
LSL
USL

Process Capability 315
For the spherical bomb the distribution is checked and it is found that the StDev is 5 m. As the VOP is 
represented by 6V it also has a value of 30 m.
C
VOC
VOP
USL
LSL
6
6
5
p =
=
-
=
=
s
30
1 0
¥
.
Equation (8.3)
We can calculate the capability and we get a value of 1.0. As VOC and VOP have the same value this 
means that the distribution just ﬁts into the customer requirements. We are just going to break out of 
the example for a moment and give you some more detail on Cp levels and what they mean.
The diagram above shows a normal process distribution with a mean of 200 and StDev of 10. The Upper 
Speciﬁcation Limit (USL) for this process is 260 and the LSL is 140. These both happen to be six StDevs 
from the mean. If we were lucky enough to own such a process we could declare our process to be 6V 
capable or call it a 6V process. By deﬁnition a 6V process has a Cp of 2 and will produce 3.4 defects per 
million opportunities. 
The table shows additional ﬁgures of varying sigma level and the corresponding changes in Cp and 
DPMO. Many industries use 1.33 as the target for Cpk or Ppk and this is a good starting target but the 
target should be process and industry speciﬁc. 
Sigma 
Level
Cp
DPMO
2
0.67
308 450
3
1.0
66 800
4
1.33
6 210
5
1.67
233
6
2.0
3.4
6s
LSL
USL
µ
USL
LSL
Distribution of
spread when
fired, σ = 5

316 Problem Solving and Data Analysis using Minitab 
We will get back into our example and consider what happens when the plane is not ﬂying along the 
centre of the dam.
We are now going to go back to the cylindrical bomb and are going to consider what happens when the 
plane is no longer ﬂying along the centre of the dam.
The width of the distribution is the same but as it is now off centre it cuts the lower speciﬁcation limit.
C
VOC
VOP
USL
LSL
6
6
3
p =
=
-
=
=
s
30
1 67
¥
.
Equation (8.4)
However, the Capability Value is the same. This isn’t good; we have a good capability value but as the 
process is not centred it will give results that are outside the speciﬁcation limits. We need a new meas-
urement for processes that are not centred.
 
C
US L x
3
pu =
-
s
 
C
x
LSL
3
pl =
-
s
 
Equation (8.5) 
Equation (8.6)
In order to get a more appropriate measure for processes that are not centred we split the calculation 
and measure the capability to each of the speciﬁcation limits.
C
(C
,C )
pk
pu
pl
= min
Equation (8.7)
Dam
Width =
30
 
 
USL
LSL
Distribution of  spread
when fired, σ = 3

Process Capability 317
Cpk then takes the value of whichever is lower out of Cpu is Cpl. As Cpk is a measure that looks at the nearest 
speciﬁcation limit it is more appropriate when the distribution is not centered. 
Let’s have a look at some simple distributions and have a look at how Cp and Cpk vary for these distributions.
The diagram shows the case when the distribu-
tion just ﬁts within the spec limits. Both Cp and 
Cpk are equal to 1.
The diagram now shows the case 
when the mean of distribution is in 
line with the Upper Spec Limit. Cp is 
still equal to 1 because the VOP and 
VOC have not changed. However, Cpk 
is now equal to 0 because it reports 
the minimum value of either (USL – 
Xbar)/3V or (Xbar – LSL)/3V. In the 
case of the diagram the difference 
between the mean and the USL is 
zero. When Cpk is zero, yield is 50%.
The diagram now shows 
that the distribution has 
gone just beyond the 
USL. Cpk is now –1 because 
the mean is 3V away from 
the USL. Therefore, USL – 
Xbar = –3V. 
At this point everything 
that is being made is 
rejected.
LSL
USL
Cp = 1
Cpk = 1
LSL
USL
Cp = 1
Cpk = 1
Cp = 1
Cpk = −1
LSL
USL
3σ

318
Problem Solving and Data Analysis using Minitab 
8.2 Short Term and Overall Capability
Process Capability has strong links with SPC, indeed you will see a lot of control charts used within this 
module. One of the main principles that is carried over is that of common cause and special cause vari-
ation. As stated earlier due to our clever sampling using subgroups we will see the short term variation 
captured within the subgroups and the special cause variation should be captured between the groups.
When measuring Process Capability Cp and Cpk are measures of short term capability and Pp and Ppk are 
measures of overall capability. 
If a process is displaying only common cause variation with no special cause variation then Cpk = Ppk,
approximately. If the process is displaying special cause variation then Cp and Cpk ﬁgures are only achiev-
able if we manage to eliminate all the special cause variation.
We use Process Capability to benchmark our processes and sometimes make estimates of the yield we 
can expect. If there is special cause variation then the process is not stable and not predictable. In which 
case the capability ﬁgures only apply to the data set they were derived from. To make predictions there 
must be predictability within the process and that means no special cause variation.

Process Capability
319
8.3 Capability Analysis for Normal Data
Example 2. Normal Capability.
Willy Winker’s caramel ﬁlled chocolate hearts must have a size between 32 and 28 mm, inclusive, so that 
they can ﬁt into the box correctly. A number of samples have been taken from the line and measured. 
The data has a subgroup size of three and is time ordered.
Use the capability Sixpack and then Normal Capability Analysis to assess the process capability. Use both 
the Classic Method and the Assistant.
All data sets are in spreadsheet 08 Process Capablity.xls
Worksheet: Hearts.
1. Open the Excel ﬁle and transfer the data into a new 
Minitab project worksheet.
Initially, we do not know if the data has a normal distri-
bution. Luckily the Capability Sixpack contains a normal 
probability plot as well as control charts to tell us if the 
process is in control. Therefore, it makes a good starting 
point for the Capability Analysis when using the Classic 
Method.
Procedure
tCapability Sixpack and Analysis (Normal)
What's it 
used for?
t The Sixpack is used in order to help us understand whether the 
process data is normal and in control.
t Capability Analysis (Normal) is used to calculate capability metrics 
for normal data.
Assumptions
and
Limitations
t The data should be time ordered.
t Having the data in subgroups may give a better understanding of 
within and overall capability.
t The Sixpack uses Control Charts and Probability plots to ensure 
the validity of the capability results. It is usually used prior to the 
Capability Analysis procedure to ensure the process is normal and 
in control.
t In order to use the results for a given data set to predict future 
performance the process must be in control.

320
Problem Solving and Data Analysis using Minitab 
2. Click Stat <<Quality Tools << 
Capability Sixpack << Normal.
3. Our data is laid out with the sub-
groups within rows so we need 
to select the appropriate radio 
button and then select our data 
columns.
4. We enter the values of the lower 
and upper spec limits and then 
click OK to produce the Capability 
Sixpack.
The Sixpack has been split into two sec-
tions to allow the inclusion of the expla-
nation against the sections. We see an 
Xbar and R chart of our data. We can see 
that our data is in control. As the data 
is in control the overall variation should 
contain predominantly short term varia-
tion. This means that the values for the 
short term metrics and overall metrics 
should be similar.
Below the control charts we have a 
chart displaying the actual values of the 
points within the last 25 subgroups. We 
can inspect for outliers within the sub-
groups and whether they are equally 
spaced around the mean. We can also 
check for changes in distribution within 
the groups.

Process Capability
321
The Capability Histogram shows our distri-
bution which is within the Spec limits. The 
distribution appears to be centred within 
the spec limits. These are both good signs. 
There are also two lines of ﬁt, one is a 
black dashed line which represents the 
overall estimate and the red unbroken 
line represents the within estimate. 
The probability plot shows that we are 
dealing with a normal distribution as 
the P value above the plot is 0.615. This 
validates our initial decision to use the 
Sixpack for normal distributions. 
The summary data gives us our capabil-
ity metrics. In the centre we see lines rep-
resenting the width of the process and 
the specs which are representative of the 
widths seen on the Capability Histogram. 
We then have a box on the left contain-
ing the within metrics and one on the 
right containing the overall metrics. The 
within StDev is 0.4948 which is estimated 
from the within subgroup variation. The 
Cp and Cpk are calculated from this. The 
PPM is the number of defective parts per 
million we would get based on the within 
variation. The overall StDev is higher as 
it is calculated globally and therefore Ppk
is lower than Cpk. As it stands, we would 
expect to have 821 defective parts per 
million but we don’t know whether they 
would fail the upper or the lower speciﬁ-
cation limit.
Now that we know we have a normal distribution we can use the Capability Analysis for normal distribu-
tions to get more detailed information on the capability metrics.
5. Click Stat <<Quality Tools <<Capability 
Analysis << Normal.

322
Problem Solving and Data Analysis using Minitab 
6. Our data is laid out with the 
subgroups within rows so we 
need to select the appropriate 
radio button and then select 
our data columns.
7. Enter the LSL and USL. Notice 
that here we have the oppor-
tunity to set either of the 
specs as a Boundary, we didn’t 
have that option with the 
Sixpack. Ticking the Lower 
spec Boundary tick box will 
tell Minitab that it is not phys-
ically possible to have parts 
below 28, therefore it won’t 
calculate defective parts for 
the boundary. It won’t cal-
culate any capability metrics 
associated with that limit 
either.
8. Then click on options.
9. Under Display set the radio 
button onto Percents (this is a 
personal preference as I pre-
fer percentage data over PPM 
but I suppose the choice is also 
dependent on how well your 
processes work). There is also 
an option here to include the 
conﬁdence intervals for Ppk
and Cpk. We can also enable 
the Cpm metric which is a new 
overall metric which meas-
ures the central location our 
distribution.
10. Then click OK and OK again.

Process Capability
323
We will now use the Assistant to carry 
out the same capability study. The 
Assistant offers two types of analysis 
for continuous data. It offers Capability 
Analysis which gives results similar to 
those we have just seen. It also offers 
Capability Comparison which is used 
to compare two sets of capability data. 
This is obviously very useful when 
showing the improvements made 
within a project.
1. Click Assistant <<Capability 
Analysis.
2. Click on the Capability Analysis 
Icon.
3. We have a choice of two types of 
analysis. For now we will select 
the Snapshot.
4. Select Data in multiple columns 
then enter the columns where 
the data is stored. The snapshot 
does not calculate the within 
metrics so it will treat the three 
columns as one.
5. Enter the LSL and USL. Then click 
OK.
The detailed capability anal-
ysis is given here. The main 
difference between this and 
the Capability Sixpack is that 
information 
on 
Observed 
and Expected performance 
is given. We also see the 
breakdown of whether most 
of our losses are at the LSL 
or USL. It appears that the 
expected 
overall 
perfor-
mance attributes the losses 
equally between the specs. It 
is interesting to note that the 
observed losses were 0% at 
either spec. We should also 
note that the expected per-
formance is a valid predic-
tion of future performance 
as our process is in control.

324
Problem Solving and Data Analysis using Minitab 
The Snapshot shows us a histogram of our data within the spec limits. Below that is a Normality Plot with 
the results of the AD normality test.
The Process Characterization gives us all the metrics that we have seen previously with the exception 
of Z.Bench. This is in simple terms deﬁned as the capability experienced by the customer in units of V.
Where 6 is excellent and 0 is 100% defects.
The Snapshot also generates a report card which is not shown here. For a more detailed analysis we will 
use the Complete Analysis which will calculate Within Metrics, as shown next.
1. Click Assistant <<Capability Analysis.
2. Click on the Capability Analysis Icon.

Process Capability
325
3. In order to complete the 
comparison we will use 
the Complete Analysis 
on this occasion.
4. Select Data in multiple 
columns then enter the 
columns where the data 
is stored. 
5. Enter the LSL and USL. 
Then click OK.
The Summary Report is very similar to the Snapshot Summary Report. The exception is the Z.Bench 
indicator. We see that our process is about average on this indicator. The Summary Report again gives 
metrics on the Overall Capability.

326
Problem Solving and Data Analysis using Minitab 
The Process Performance Report shows the histogram again with both the Overall and Within ﬁts. 
Capability Metrics for both Within and Overall are given on the opposite side.
The Diagnostic Report which is not shown here shows us the SPC charts and Normality test results.
The Report Card gives information about the validity of the test procedure. As we have provided more 
than 100 observations in 40 subgroups Minitab is satisﬁed with the likely precision of our results.
Exercise 1. Normal Capability.
Omer checks the weight of nuclear fuel rods. He collects data from two samples every hour for a capabil-
ity study. LSL = 19.50 kg and USL = 20.50 kg.

Process Capability
327
Use the capability Sixpack to establish if the process is in control and normally distributed. Then conduct 
the full Capability Analysis using the Classic Method. Then carry out the same task using the Complete 
Analysis on the Assistant.
Worksheet: Fuel Rods.
As indicated in the question the subgroup size needs to be set to two. From the Sixpack we see that 
there are no issues with the process being out of control. The Capability Histogram looks worrying as 
the right edge of the distribution intersects with the USL.
We also see that the process is normally distributed with a P value of 0.562. 
The Capability Plot also indicates that the process may not be centred. 

328
Problem Solving and Data Analysis using Minitab 
It can be seen that both Pp and Cp are less than one, indicating that the distribution will not ﬁt within 
the speciﬁcation limits even if it was centred. The Observed Performance is better than the Expected 
Performance. The Expected Overall Performance indicates that 2.06% of parts would be rejected due to 
the USL and 0.07% would be rejected due to the LSL.
The Process Performance Report shows the histogram again with both the Overall and Within ﬁts. 
Capability Metrics for both Within and Overall are given on the opposite side. As our Cpk is signiﬁcantly 
less than the Cp this conﬁrms that the distribution is not centered between the spec limits.
The Diagnostic Report, which is not shown here, shows us the SPC charts and Normality test results.

Process Capability
329
On this occasion the Report Card gives us a warning about the number of observations. We are also 
given information about the number of subgroups being slightly low. Stability and Normality are not 
an issue.
8.4 Capability Analysis for Non Normal Data
As we saw in the SPC module if we are faced 
with non normal data we ﬁrst have a critical 
decision to make. Using our skill, judgement 
and experience we must make a decision 
about the nature of our process. Is it a pro-
cess that should produce normal data but 
it has some special causes acting on it or 
does this process align to a different type of 
distribution?
If we think that our data should be normal 
and it isn’t close to being normal, as in the 
diagram shown, we should not be calculat-
ing capability metrics for it. We should be 
ﬁnding out what is doing this to the process.
To highlight the point, let’s say you are using non normal data techniques to calculate capability metrics 
within an improvement project. Without making any major changes to the process you collect more 
data and then ﬁnd that process requires a different transformation than the one you started with or the 
data turns normal. This means you probably got it wrong to start with. You have special cause variation 
inﬂuencing your process. 
The remainder of this section deals with distributions that produce non normal data inherently.
The problem with the Normal Capability checks performed on non normal data is that to calculate the 
Voice of the Process the data is ﬁtted with a normal curve. This means that the non normal distribution 
will be a ﬁtted with a curve that does not ﬁt. 

330
Problem Solving and Data Analysis using Minitab 
In order to get past this we have a cou-
ple of options, we can transform the data 
and then try and ﬁt the data with a nor-
mal curve. Minitab uses the Johnson and 
the Box–Cox transformations. The Johnson 
transformation is a complex function but 
it usually ﬁnds a suitable transformation. 
The Box–Cox transformation is much sim-
pler but it does not always ﬁnd a suitable 
transformation.
Or we have the option of applying a dif-
ferent type of distribution to our data. 
Both methods will be illustrated within the 
examples.
Example 3. Non Normal Capability.
Prawn is carrying out a project to improve the yield of ﬁre rated plasterboard. He has collected data at 
the start of the project and wants to conduct an initial capability analysis. The thickness speciﬁcation of 
ﬁre rated plasterboard is very strict and is between 12.80 and 13.00 mm. 
It is known from previous studies that this process does not produce normal data.
Initially, use the Assistant and check whether that will transform the data. Also use the Classic Method 
to ﬁnd the appropriate transformation and then conduct a Non Normal Capability Analysis.
Worksheet: PB Thickness.
1. Open the Excel ﬁle and transfer the data into a new Minitab 
Project Worksheet.
2. For this example we are going to begin by using the Assistant. 
Click Assistant <<Capability Analysis.
3. Click on the Capability Analysis button. 

Process Capability
331
4. Select 
the 
Complete 
Analysis. Then complete 
the menu box as shown. 
Note, the data was not 
collected in subgroups, 
therefore the Subgroup 
size is listed as 1.
5. Enter the Spec limits and 
then click OK.
6. Notice that the Assistant 
has not transformed the 
data for this procedure. 
We know this as it did 
not display a conﬁrma-
tion box for the trans-
formation. Also, look at 
the Normality Plot on 
the Diagnostic Report, 
the data is clearly not 
normal. The Assistant 
did not transform the 
data as the Box–Cox 
transformation 
would 
not have delivered a 
suitable transformation.
The Report Card also generates the warning message displayed in the ﬁgure above. The warning mes-
sage is not strictly true as it only refers to the Box–Cox transformation. 
We will now use the Individual Distribution 
Identiﬁcation (IDI) procedure using the 
Classic Method. This procedure tests the ﬁt 
of a number of distribution models and tests 
the two common transformations.
1. Click Stat <<Quality Tools <<Individual 
Distribution Identiﬁcation

332
Problem Solving and Data Analysis using Minitab 
2. Complete the IDI menu as 
shown. Note, that the radio 
button to try all available dis-
tributions is ticked by default. 
There are separate buttons for 
the common transformations 
in order to allow the user 
more control in the transfor-
mation settings and allow the 
storage of transformed data.
3. Click on OK.
4. Click on the session window 
icon so we can see the main 
results table ﬁrst.
The P values given here are for the 
Goodness of Fit Test for the listed 
distribution models. These include 
the two common transformations. 
We can see that the Johnson trans-
formation gives the best P value, 
0.671. Initially, we will use this 
transformation for the capability 
analysis. Although, there are other 
models which give an acceptable ﬁt 
like the three parameter Weibull 
which has a P value of >0.500.
The Likelihood Ratio Test (LRT) is a 
hypothesis test used to differenti-
ate between distributions which 
have a different number of param-
eters. For example, a P value of 0.0 
indicates that the three parameter 
Weibull was better than the two 
parameter.
5. Click on the Graphs icon to see 
the graphical output of the IDI 
procedure.

Process Capability
333
There are four pages of graphical output. Only one of the pages is shown here.
It can be seen that the data has an almost normal ﬁt after the Johnson transformation.
Now that we have identiﬁed that we wish to use the Johnson transformation we will carry out an initial 
assessment using the Capability Sixpack.
If you are using M16 you will ﬁnd the 
Transformations within the Normal menus. 
M15 users will ﬁnd the Transformations 
with the non normal distribution models 
in the Non normal menus.
6. Click Stat <<Quality Tools << Capability 
Sixpack << Normal (for M16).
7. Complete the menu box as shown. 
Subgroup size is 1, as there was no 
indication of any subgrouping. Enter 
the speciﬁcation limits.

334
Problem Solving and Data Analysis using Minitab 
8. Click on the Transformations button.
9. Select the radio button for Johnson 
transformation. Then click OK and 
OK again.
It is impressive to see the complexity of the Johnson transformation but you would not want to work 
out the transformation values with a calculator. 
When we get to the Capability Histogram we begin to see that there is a problem, there are no speciﬁca-
tion limits. Looking into the Session Window we see that the problem was that the speciﬁcation limits 
were outside of the transformation function. We will have to go plan C and try the three parameter 
Weibull.

Process Capability
335
M15 and M16 both put the Non normal distribu-
tions in the Non normal menu.
10. Click Stat <<Quality Tools << Capability 
Sixpack << Non normal.
11. Complete the menu box as shown. Ensure 
the selector for Fit distribution is changed to 
3-parameter Weibull. Then click OK.
Having ﬁtted the three parameter Weibull distribution we see that the process is in control. 
The capability histogram displays our speciﬁcation limits with our data and the ﬁtted distribution.
As our Ppk is signiﬁcantly less than the Pp this means that the distribution is not centered between the 
spec limits. 
The PPM defect of 9594 indicates that 0.96% of the product would breach the USL.

336
Problem Solving and Data Analysis using Minitab 
12. In order to carry out a detailed Capability 
Analysis click Stat <<Quality Tools <<Capability 
Analysis <<Non Normal.
13. Complete the menu box as shown. Ensure the 
selector for Fit distribution is set to 3-param-
eter Weibull.
14. In order to change the data display into per-
centages rather than parts per million click 
on Options and then select the Percents radio 
button. Then click OK and OK again.
The overall capability is 0.65. The observed performance, from the given data set does not produce any 
defects. However, from the expected overall performance we would expect 0.96% of product to be 
above the USL.
Within capability metrics are not given for non normal data distributions.

Process Capability
337
Exercise 2. Non Normal Capability
The Quality Department want to introduce a speciﬁcation for banana curvature. Bananas with an out-
side curvature of 20–50° will be subject to an internal quality defect.
Assess the historical data and establish how many bananas will be deemed defective due to the new 
speciﬁcation.
Initially, use the Assistant and establish if the Box–Cox transformation within the Assistant suitably trans-
forms the data. Then use the Classic Method and use the IDI procedure to identify the best three possible 
ﬁts/transformations. Use the top three to conduct the capability study (Sixpack only). If the Box–Cox is 
one of the top three you do not need to repeat the work already done by the Assistant.
Worksheet: Banana.
We know the data has been transformed as we pressed the Yes button at the prompt which asked us if 
we wanted to transform the data.
From the Summary Report we see that 0.44% of our data would be out of spec due to the top spec limit. 
If the distribution had been centred it would have ﬁtted within the spec limits.

338
Problem Solving and Data Analysis using Minitab 
The Diagnostic Report shows that the data is in control. Therefore, we can use this data to make 
predictions about future performance.
We also see the P values of the original and transformed data. The Normality Plot header tells us that a 
lamda of –0.5 was used in the transformation.
The Individual Distribution Identi-
ﬁcation procedure indicates that the 
best transformations/ﬁts would be:
Johnson transformation, P value = 
0.402
Box–Cox transformation, P value = 
0.263
Loglogistic distribution ﬁt, P value 
> 0.25
From the top three we need to check 
the Johnson transformation and the 
Loglogistic distribution ﬁt using the 
Classic Method. 

Process Capability
339
If you were using M16, did you remember to access the Johnson transformation via the Normal menu?
The Sixpack using the Johnson transformation is shown. Again, the data is in control. Therefore, we can 
use this data to make predictions about future performance. 
Using the Johnson transformation we predict that 0.81% of Bananas will be defective as they breach 
the upper spec limit.
The Sixpack using the Loglogistic ﬁt is shown. This time we see that one data point is out of control on 
the MR chart. In order to be able to make future predictions we would need to investigate the cause of 
this variation.
Using the Loglogistic ﬁt the procedure predicts that 0.67% of bananas will be defective as they will 
breach the upper spec limit.

340
Problem Solving and Data Analysis using Minitab 
8.5 Capability Comparison using the Assistant
Procedure
tCapability Comparison (Assistant)
What's it 
used for?
t It is used to compare capability metrics for two sets of data, ideally 
before and after a process change.
Assumptions
and
Limitations
t The procedure will calculate if there has been a statistically signiﬁ-
cant shift in mean or StDev.
t The data should be time ordered and normally distributed.
t In order to use the results for a given data set to predict future 
performance the process must be in control.

Process Capability
341
Example 4. Capability Comparison.
Zaf has almost completed his ﬁrst 6V project. He has been reducing the order lifetime within a busy 
industrial components supplier. His target has been to reduce the order lifetime to between 65 and 75 
days. This target has been in place for a long period of time but never achieved.
Data on order lifetimes is in two columns one labelled Before and the other After. Due to the nature of 
the process subgrouping was not attempted.
Use the Capability Comparison to establish if Zaf has been successful in his project. How have the capa-
bility metrics changed? 
Worksheet: Order Time.
1. Open the Excel ﬁle and transfer the data 
into a new Minitab Project Worksheet.
2. Click Assistant <<Capability Analysis.
3. Click on the Capability Comparison Icon.
We enter the Baseline and the Improved 
data as before. Minitab gives us the option 
of having subgroups in rows again. We need 
to make sure that the selector is set to Data 
are in one Column for both sets of data.
4. Enter the Column Data
5. Enter the LSL and USL as 65 and 75, 
respectively. Note that we only have the 
option of entering one set of speciﬁ-
cation limits and they will be used for 
both sets of data. 
6. Click on OK.

342
Problem Solving and Data Analysis using Minitab 
The Summary Report statistically conﬁrms that Zaf has not reduced the process StDev but has changed 
the mean. Zaf has reduced %Out of Spec from 2.26 to 0%.
The histograms show how the distribution has shifted so that it is more centred and not crossing the 
USL.
On the right hand side we can see the overall capability metrics and how they have changed.
Pp has reduced from 1.64 to 1.58. But Ppk has gone from 0.67 to 1.52.
The Diagnostic Report looks at the normality and stability requirements of the data. We see that both the 
Before and the After data is in control. This allows us to use these metrics to predict future performance.
Notice, that both SPC charts are using the same y scale.
We also see that both sets of data passes the normality test.

Process Capability
343
The Process Performance Report gives us more detail on the capability metrics. We get Overall and 
Within metrics and we also get observed and expected metrics.
Finally, we get the report card, which is not shown here. That gives us the all clear on stability, normality, 
sample size and number of subgroups.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 9
Correlation and Regression
9.1 What are Correlation and Regression?
We use correlation to check whether two variables have a linear relationship. We use the correlation 
coefﬁcient (r) to check the strength of the relationship.
The correlation coefﬁcient, r, varies from –1 to 1. Zero implies that there is no linear correlation. However, 
there can be nonlinear patterns when the correlation coefﬁcient is zero. Therefore, it is advisable to 
graphically check the data before carrying out the test procedure.
Strong
Positive
Correlation,
r =0.97
Weak
Positive
Correlation,
r =0.6
No
Correlation,
r =0
Weak
Negative
Correlation,
r =−0.6
Strong
Negative
Correlation,
r =−0.97
No Correlation, r =0

Correlation and Regression
345
Regression is more about building a mathematical model which describes the relationship between one 
or more predictors and a single response variable.
The graph shows the data points collected as tyre pressure is varied and then fuel efﬁciency is meas-
ured. A line of best ﬁt has been drawn through the points using a cubic relationship to form the model. 
Even though we have used a cubic relationship there is only one factor/predictor and one dependent 
response.
Once you have a regression equation it can be used to predict the response when the predictor is known. 
Regression goes beyond correlation by adding this prediction capability.
Unfortunately, we can’t expect our equations to perfectly model the real world so there will always be 
some error. 
All data sets for this chapter are in the Excel ﬁle 09 Correlation and Regression.xls. 

346
Problem Solving and Data Analysis using Minitab 
9.2 Correlation
Example 1. Correlation.
Spud has been collecting data on the sugar content of cake versus the taste rating (0–100) of a number 
of samples of cake.
Analyse the data Spud has collected and check whether there is a linear correlation between the vari-
ables. What is the strength of the correlation?
Worksheet: Cake.
1. The ﬁrst thing we need to do before we conduct 
the procedure is to ask ourselves if we believe there 
is a real world link between our predictor and our 
response. It is our responsibility to ensure ‘damn lies 
and statistics’ are not linked. In this case the link 
between the predictor and the response does sound 
plausible.
2. Open the Excel ﬁle and transfer the data into Minitab.
3. We need to view the graphical relationship between 
the parameters in case a nonlinear relationship is 
present. Click Graph <<Scatterplot.
Test
tCorrelation
What's it 
used for?
t It is used to test whether two sets of variables have a linear 
relationship.
Assumptions
and
Limitations
t The data should be checked using a scatter plot prior to conduct-
ing the test in order to establish if a non linear relationship is 
present.
t It is up to the tester to ensure there is a real world relationship 
between the factor and the response. Correlation does not mean 
causation.
t Data for the study needs to be carefully collected so that it is rep-
resentative of the population.
t Testing for Correlation is only available via the Classic Menus.

Correlation and Regression
347
4. Select Simple and then click OK. 
5. Since Taste Rating is the response we will 
use that as the Y variable. Enter Sugar 
Content as the X variable and click OK.
6. From the scatterplot there appears to be a positive correlation but we don’t know if it is statistically 
signiﬁcant. Note: we conduct a graphical check because checking the correlation coefﬁcient alone 
would not detect if there was a strong nonlinear relationship.

348
Problem Solving and Data Analysis using Minitab 
7. To check the strength of the correlation 
and whether it is statistically signiﬁcant 
click Stat <<Basic Statistics <<Correlation.
8. Select both the Variables and then click 
OK.
9. Click on the session window icon so we can 
see the results. The Pearson Coefﬁcient (r)
indicates a strong positive relationship, 
r = 0.988. The P value indicates the coef-
ﬁcient is signiﬁcant.
Exercise 1. Correlation.
Aman is a keen gamer, however, his kids keep beating him on his favourite games. Aman is worried that 
reaction time declines with age. He collects data on age versus the completion time of a new video game 
to test the theory.
Analyse the data and check whether there is a linear correlation between the variables. What is the 
strength of the correlation? Should Aman be worried?
Worksheet: Reactions.

Correlation and Regression
349
The scatter plot does appear 
to show a linear relationship. 
It would appear that as age 
increases the time taken to 
ﬁnish the game is longer.
The results in the session win-
dow are shown below the 
graph. The Pearson coefﬁ-
cient indicates a strong posi-
tive relationship, r = 0.946. 
The P value suggest the coef-
ﬁcient is signiﬁcant.
Aman is so worried by the 
results that he bans his chil-
dren from playing further 
video games.
9.3 Multiple Correlations
Example 2. Multiple Correlations.
Eight presses are used to manufacture steel nuts. Samples are taken every hour and the across ﬂats 
dimension is measured. If there is a strong correlation between any of the presses it may give an oppor-
tunity to reduce sampling.
Analyse the data and check whether there is a linear correlation between any of the presses. What is 
the strength of the correlation?
Worksheet: Nut.
1. Open the Excel ﬁle and transfer 
the data into Minitab.
2. As with single correlations we 
need to view the graphical rela-
tionship between the parameters 
in case of a nonlinear relation-
ship. Click Graph <<Matrix Plot.

350
Problem Solving and Data Analysis using Minitab 
3. Select Simple and then 
click OK. 
4. Select all the data col-
umns as the graph var-
iables. Then click on 
Matrix Options.
5. Under Matrix Display 
select the radio button 
for Lower Left. (This is 
a personal preference 
which I think makes 
the graph easier on 
the eye.)
6. Click OK and OK again.

Correlation and Regression
351
All possible combinations of scatterplot are presented here.
We can visually check for nonlinear relationships.
We can see that Press1 & Press5 and Press2 & Press8 appear to be correlated. As before we need to check 
if the correlation is signiﬁcant.
1. To check the strength of the correlation and whether 
it is statistically signiﬁcant click Stat <<Basic Statistics 
<<Correlation.

352
Problem Solving and Data Analysis using Minitab 
2. Select all the Variables and then click OK.
3. Select the Session Window icon to see the 
results.
All combinations of correlation coefﬁcient with their corresponding P value are shown here.
We can conﬁrm that Press1 & Press5 and Press2 & Press8 are correlated and that correlation is statistically 
signiﬁcant.
Knowing which presses are correlated may allow us to reduce sampling.

Correlation and Regression
353
Exercise 2. Multiple Correlations.
Ten machines are used to manufacture 50 µF capacitors. Samples are taken every hour and the capacitance 
is measured. If there is a strong correlation between any of the machines it may give an opportunity to 
reduce sampling.
Analyse the data and check whether there is a linear correlation between any of the machines. What is 
the strength of the correlation?
Worksheet: Capacitor
We can see that all combinations of machines appear to be correlated. As before we need to check if 
the correlation is signiﬁcant.

354
Problem Solving and Data Analysis using Minitab 
The P value tells us that all the machines are correlated.
This may be good in terms of reducing sampling but in process terms it would be very beneﬁcial to 
understand why the machines are behaving in this way.
9.4 Introduction to Regression
As stated earlier, Regression is more about building 
a mathematical model which describes the relation-
ship between one or more predictors and a single 
response variable.
This course will split the topic of Regression into 
Single Predictor Regression and Multiple Predictor 
Regression. You saw the example given earlier 
when we took Tyre Pressure as a single factor and 
showed a cubic equation of how the response, Fuel 
Efﬁciency, would vary with Tyre Pressure. This was 
single predictor regression. If we wanted to reduce 

Correlation and Regression
355
the error in the model we could add additional predictors such as vehicle weight, engine size and drag 
factor. That would be multiple predictor regression.
Later we will also take a quick look at another type of regression which is Nonlinear Regression. This is 
M16 only.
For Single Predictor Regression Minitab will work with the user to try and ﬁt an equation. You have 
already seen the cubic equation, the other types are linear and quadratic. We measure how well these 
equations model our data and we call that the ‘ﬁt’.
On occasion, there isn’t much difference in the achieved ﬁt between the different types of equation. 
Sometimes, the simpler model is preferred as it is easier for subsequent work.
When using the Classic Menus we will use the Fitted Line Plot procedure. In M16, the Assistant offers us 
Single Predictor Regression but not Multiple Predictor Regression. 
Another key difference is that when using the Classic Method we validate the model using the Residuals. 
The Assistant validates the model using the methods described in the Minitab White Paper for Regression. 
That is by checking the sample size and checking for unusual data points. Advice on getting a valid 
model can be obtained when starting the Assistant procedure. After the test procedure the summary of 
the validity checks are given in the Report Card, as always. 
The White Paper recommends a sample size of ≥40 in order to obtain a precise estimate of the strength 
of the relationship and states that for this sample size the normality of the residuals is not an issue. 
There is no reason for us not to use these guidelines to help ourselves when using the Classic Method.
9.5 Single Predictor Regression
Test
tSingle Predictor Regression
What's it 
used for?
t It is used to model an independent predictor variable against a 
dependent response variable.
Assumptions
and
Limitations
t Assumptions should not be made about data that is extrapolated 
outside the study range.
t Data for the study needs to be carefully collected so that it is rep-
resentative of the population.
t The residuals are used to validate the model when using the 
Classic Method. The Assistant informs us of the reliability of our 
model within the Report Card.

356
Problem Solving and Data Analysis using Minitab 
Example 3. Fitted Line Plot.
Researchers believe that the diameter of an apple determines its sugar concentration. 
Analyse the data to establish if any kind of relationship exists. What is the strength of the relationship?
For an apple of 90 mm diameter could the mean sugar content be 32 mg/g?
For an apple of 120 mm diameter could the sugar content be 32 mg/g?
For an apple of 50 mm diameter could the sugar content be 24 mg/g?
Worksheet: Apple.
1. Open the Excel ﬁle and transfer the data 
into a new Minitab project worksheet.
2. Click on Stat <<Regression <<Fitted Line 
plot.
3. Select Sugar Content as the Response 
and Diameter as the Predictor. We have a 
one in three chance of guessing the cor-
rect model. If we want to stack the odds 
in our favour we should select the Cubic 
model ﬁrst. Click OK.

Correlation and Regression
357
4. The Cubic model gives 
an R-Sq(adj) of 81.9%. 
This means that 81.9% of 
changes in the response 
can be explained by 
changes in the predictor.
5. Go to the Session Window 
and check the results of 
the Sequential Analysis of 
Variance. We see that the 
Cubic term is not actually 
signiﬁcant. As the next 
highest term, the quad-
ratic, is signiﬁcant we 
should select that model 
instead.
6. Click on the Edit Last 
button.
7. Select the radio button 
for the Quadratic model.
8. Click on Graphs and select 
the radio button for Four 
in one Residual Plots. 
9. Click on OK and OK again.

358
Problem Solving and Data Analysis using Minitab 
10. Find the graph with the quadratic model on it. Note that the quadratic model has a slightly lower 
R-Sq(adj) than the cubic model of 81.5%. 
The R-Sq(adj) of 81.5% indicates that there is a strong relationship between Diameter and Sugar Content.
We will now move onto the second part of the question. 
11. Click on the Edit Last button.
12. Then click on the Options button.
13. Select the tick boxes for displaying the 
conﬁdence and prediction intervals.
14. Click OK and OK again.

Correlation and Regression
359
15. The red dashed line is the 95% conﬁdence interval for the mean. The prediction interval gives a 
conﬁdence boundary for individual points. We can be 95% conﬁdent that an individual point will 
fall within the outer green dashed line. We can be 95% conﬁdent that if apple diameter was kept 
constant the mean would fall within the inner red dashed line.
16. In order to help us answer the questions about 
predicting means and individual points we can 
add reference lines to the graph. Right click on 
the graph panel and select Add <<Reference 
Lines
17. Enter the values as shown and then click OK. 
Reference lines appear on the quadratic plot and we can use them to help us answer the second part of 
the question.
We can say with 95% conﬁdence that a diameter of 90 can give a mean sugar content of 32 as the inter-
section of the relevant reference lines lies within the conﬁdence interval for the mean.

360
Problem Solving and Data Analysis using Minitab 
We can also say that a 
diameter of 120 is not 
likely give a single sugar 
content of 32. The inter-
section of the relevant 
reference lines does not 
lie within the prediction 
interval.
An apple of 50 mm diam-
eter falls outside the 
range of our data and 
we would be extrapo-
lating. Therefore, we 
must decline to answer 
the third part of the 
problem.
Finally we must check 
the residuals to estab-
lish the validity of the 
model.

Correlation and Regression
361
Starting from the top and going from left to right:
t The Normal Probability Plot shows that the Residuals are normally distributed.
t The Versus Fits Plot shows that the Residuals are equally distributed about the centre line.
t The Histogram Plot shows that the Residuals do not show extreme skewness.
t The Versus Order Plot shows that the Residuals are not displaying any patterns or ordered behaviour. 
This is only relevant if the results were recorded in the order that the experiment was carried out.
In summary, there are no issues with the residuals within this study.
Example 4. Regression with the Assistant.
Researchers believe that the diameter of an apple determines its sugar concentration. Assume the data 
has been collected and recorded in time order.
Use the Assistant to analyse the data in order to establish if any kind of relationship exists. What is the 
strength of the relationship?
(The Assistant has it strengths but it does not have as many extra options available as the Classic Method. 
At present the Assistant does not provide Conﬁdence and Prediction Intervals as part of the analysis so 
we will have to omit this section from the examples when using the Assistant.)
Worksheet: Apple.
1. Click on Assistant << Regression.
The main screen for regression with the Assistant is shown. It contains advice on collecting the data, 
evaluating the results and using the model. Clicking on any of the crosses will expand the selection and 
gives us the advice we want. The hidden information gives us the rules for ensuring the model is correctly 
validated and the rules to ensure we do not commit any of the common mistakes within Regression.

362
Problem Solving and Data Analysis using Minitab 
An example is shown here in which we have expanded the very bottom selection. 
2. Click on the button 
in the top left cor-
ner labelled ‘Click to 
perform analysis’.
3. Complete the menu 
box as shown. As 
we are saying that 
the Sugar Content 
is dependent upon 
the Diameter of the 
apple, the Diameter 
is the predictor and 
the Sugar Content 
is 
the 
response. 
Traditionally, 
the 
response is entered 
on the y axis.
4. Select 
the 
tick 
box for ‘Data are 
recorded in time 
order’. If we don’t 
tick the box we will 
not get Diagnostic 
Report 2.
5. Unlike the Classic 
Method we have 
the option of let-
ting Minitab check 
for the most suita-
ble model or we can 
make the selection 
ourselves. We will 
let Minitab choose 
for us.
6. Then click OK.

Correlation and Regression
363
On the top left hand side of the Summary Report we are told that the P value is zero and that there is a 
relationship between X and Y. Below that we are told that the R-Sq (adj) is 81.5% meaning that 81.5% 
of the variation in Sugar Content can be explained by changes in Diameter.
The Assistant uses the same method as we did to select the best model. As it is the same method it has 
also selected the quadratic model as shown in the plot on the top right.

364
Problem Solving and Data Analysis using Minitab 
The Model Selection Report shows us the plot of the data with the selected ﬁt. It has two points that 
are deemed to be unusual.
At the bottom of the page we have the model selection statistics shown in a table. The cubic model was 
rejected because the cubed term was not signiﬁcant, with a P value of 0.187. The quadratic term in the 
model was the highest order signiﬁcant term so it was selected to be the basis of the model.

Correlation and Regression
365
Diagnostic Report 1 displays the plot of Residual versus Fitted Values. When this plot was shown in the 
four in one residual plot we checked to make sure the Residuals were evenly distributed about the zero 
line. Additionally, the Assistant gives us examples of patterns that would indicate problems with our 
model.

366
Problem Solving and Data Analysis using Minitab 
Diagnostic Report 2 displays the plot of Residual versus Observation Order. This plot is only displayed 
when we indicate that the data was in time order in the data entry menu box.
When this plot was shown in the four in one residual plot we checked to make sure the Residuals did 
not show any patterned behaviour.
Additionally, the Assistant gives us examples of patterns that would indicate problems with our model.
The Report Card validates our test procedure. We are told we had enough data to obtain a precise esti-
mate of the strength of the relationship and for the normality of residuals not to be an issue. 

Correlation and Regression
367
We are given a warning that we had two unusual data points and that we should try and ﬁnd the reason 
why these data points were unusual.
Exercise 3. Fitted Line Plot.
Scientists have discovered three new materials. A pulling force is applied to each material and then 
the length of the material is measured, both the force and the extension are recorded. This process is 
repeated with a continually increasing force until the material breaks.
Use the Classic Method and the Assistant to establish if any kind of relationship exists between the force 
applied and the extension of the material. What is the strength of the relationship?
Using the Classic Method establish if:
t A force of 1500 N on material A could give an increase in length of 150 mm?
t A force of 800 N on material C could give an increase in length of 250 mm?
Worksheet: Materials.
Material A
Initially, the cubic model was selected but the results in 
the session window showed that the cubic term was not 
signiﬁcant. The quadratic model was then selected and 
this gave an R-Sq(adj) of 98.9%. We see that a force of 
1500 N cannot give an increase in length of 150 mm as 
this point lies outside the prediction interval. The resid-
ual plots do not show any extreme patterns or behaviour.

368
Problem Solving and Data Analysis using Minitab 
Material B
The results in the session window show that the lin-
ear model should be selected. With the linear model 
selected an R-Sq(adj) of 99.9% is achieved. The residual 
plots do not show any extreme patterns or behaviour.

Correlation and Regression
369
Material C
On this occasion the cubic model was selected and it 
turned out to be the correct choice. The cubic model 
gives an R-Sq(adj) of 99.5%. We see that a force of 800 N 
can give an increase in length of 250 mm as this point lies 
clearly within the prediction interval. 
The residual plots do not show any extreme patterns or 
behaviour.

370
Problem Solving and Data Analysis using Minitab 
Material A
On the Summary Report we see that the Assistant also selected the quadratic model. We see that the 
relationship is signiﬁcant and the R-Sq(adj) is the same as that calculated in the Classic Method. The 
Assistant has also ﬁtted the same equation as the Classic Method. 
The Model Selection Report, not shown, had one residual which was deemed to be unusual.

Correlation and Regression
371
Material B
On the Summary Report we see that the Assistant also selected the linear model. We see that the rela-
tionship is signiﬁcant and the R-Sq(adj) is the same as that calculated in the Classic Method. The Assistant 
has also ﬁtted the same equation as the Classic Method. 
As the model is linear the Assistant displays the value of the correlation coefﬁcient. The result is r = 1.

372
Problem Solving and Data Analysis using Minitab 
Material C
The Assistant has selected the cubic model. The key results and the ﬁtted equation are again the same 
as the Classic Method.
The Model Selection Report shows that two residuals were deemed to be unusual. 
9.6 Introduction to Multiple Predictor Regression
We have used the Fitted Line Plot and the Assistant to form models for single factor regression. If we 
wanted to we could also have used the full regression models. However, in this course we have saved the 
full regression procedures for their intended purpose which is Multiple Predictor Regression.
Life would be too easy if there was only one version of this procedure so we must rejoice that there are 
two versions. M15 users must use the standard regression procedure found within Stat <<Regression 
<< Regression. M16 users have both versions available to them but the version found under Stat 
<<Regression << General Regression is the updated version. It can handle categorical predictors and can 
directly model polynomial predictors and predictors with interaction. It is suggested that M16 users use 
this version. This text will also use the General Regression procedure but we will on one occasion show 
the M15 and M16 methodologies separately. 

Correlation and Regression
373
9.7 Multiple Predictor Regression
Example 5.
The Admiralty have asked Captain Pilch to study factors which could affect the range of a standard naval 
cannon used in galleons.
Analyse the data to establish which of the predictors are signiﬁcant to the response? Then form the 
regression equation.
The Admiralty have supplied nine sets of preset values of the predictors in columns K to P of the Excel 
ﬁle. They want these values to be used to produce nine estimates of the ﬁring distance using the regres-
sion equation. Also, they want the 95% conﬁdence limits (CL) and 95% prediction limits (PL) for the 
Firing Distance in each of the nine cases.
Worksheet: Cannon.
1. Open the Excel ﬁle and transfer all of the data into Minitab. Columns K to P contain nine rows of 
data that we will use to produce estimates of the ﬁring distance after we have formed the regres-
sion equation.
Test
tRegression
What's it 
used for?
t It is used to model multiple predictor variables against a single 
dependent response variable
Assumptions
and
Limitations
t Assumptions should not be made about data that is extrapolated 
outside the study range.
t Data for the study needs to be carefully collected so that it is 
representative of the population.
t If the predictors are dependent upon each other multi-co-linearity 
may exist and it may be challenging to reduce the model.
t The Regression procedure will not automatically search for 
quadratic or cubic relationships.

374
Problem Solving and Data Analysis using Minitab 
2. We need to view the graphical rela-
tionship between the predictors to 
check whether they are independ-
ent. Click Graph <<Matrix Plot.
3. Select Simple and then click OK. 
4. Select all the data columns as the 
Graph variables.
5. Click on Matrix Options.
6. Under Matrix Display select the 
radio button for Lower left (per-
sonal preference).
7. Click OK and OK again.

Correlation and Regression
375
Distance Fired is our response variable.
It is okay if we see patterns between the response and the predictors. However, all the predictors should 
be independent from each other. 
Graphically, it can be seen that the predictors are independent but we will check the correlation coef-
ﬁcients as well.
8. To check the strength of the correlation and whether 
it is statistically signiﬁcant click Stat <<Basic Statistics 
<<Correlation.

376
Problem Solving and Data Analysis using Minitab 
9. Select all the Variables and then click OK.
10. Select the session window icon to see the 
results.
We see that Charge_Wt and Ball_Wt have a signiﬁcant correlation. Also, Barrel_Len and Wadding_Thk 
have a signiﬁcant correlation. This means that there may be instability within our model. When it comes 
to reducing our model we should only remove one predictor at a time.
We are now ready to carry out the regression analysis.

Correlation and Regression
377
11. Click on Stat <<Regression 
<<General Regression. M15 
users will have to use Stat 
<<Regression <<Regression. 
The results of this example 
should be virtually the same 
for both test procedures. 
12. Select Distance Fired as the 
Response and C1 to C6 as 
the predictors.
13. Click OK.
14. As yet there are no graphi-
cal outputs from this pro-
cedure. Click on the session 
window icon as all the results 
are in the session window. 
The regression equation is 
shown below.
15. The regression table shows all 
the predictors and whether 
they are signiﬁcant or not 
in terms of the P value. We 
see that Wadding_Thk and 
Barrel_Len are not signiﬁ-
cant. Previously, we found 
these predictors to be corre-
lated so we should be care-
ful and remove them one at 
a time. We will go through 
the regression exercise again 
and remove Barrel_Len as 
a predictor. Also, note the 
R-Sq(adj) value, this is a good 
ﬁt for the process.
16. Click on the Edit Last button. 

378
Problem Solving and Data Analysis using Minitab 
17. Change the selection for the 
predictors so that Barrel_Len is 
not included and Click OK .
18. If multi colinearity had existed 
Wadding_Thk 
would 
have 
become signiﬁcant but this has 
not happened. We can proceed 
and remove Wadding_Thk as 
well. Click on the Edit Last icon.
19. Change the selection for the 
predictors so that both Barrel_
Len and Wadding_Thk are not 
included.
20. Click OK.
21. In the session window look at 
the regression table, it now 
only includes signiﬁcant pre-
dictors and we have a very 
good ﬁt of 99.5%.

Correlation and Regression
379
The ﬁnal regression equation with only signiﬁcant 
predictors is shown above. As the R-Sq(adj) is high we 
can be conﬁdent that our regression equation will 
give us an accurate model to predict distance ﬁred.
22. We can now ask Minitab to use the regression 
equation and our preselected values for the 
predictors to calculate what ﬁring distance they 
will give us including the conﬁdence and predic-
tion limits. Click on the Edit Last icon and then 
the prediction button. M15 users need to click 
on the Options button.
23. In the menu box for New Observations for con-
tinuous predictors enter the pre-deﬁned values 
for our signiﬁcant predictors. Under storage 
select the tick boxes for Conﬁdence limits (CL) 
and Prediction limits (PL).
24. Then click OK.
25. Again we must check the validity of the model 
by looking at the residuals. To do this click on 
Graphs.
26. Then select Four in one under Residual Plots. 
Click OK and OK again. 
27. Since we had asked for the results of the CLs 
and PLs to be stored they appear in the current 
worksheet. They also appear in the session win-
dow. To view them click on the Session Window 
icon.
M16
M15

380
Problem Solving and Data Analysis using Minitab 
There were nine rows of pre-deﬁned predictor values and we were asked to ﬁnd the estimated Distance 
Fired and the CIs for the mean and PIs for additional points. Each row in the session window corresponds 
to each row of data, in order. 
For our ﬁrst set of data the Distance Fired would be 316.772, the mean ﬁring distance would be between 
314.3 and 319.3. The Prediction Interval for additional points is 310.1 and 323.4.
We will now check the residual plots to conﬁrm the validity of the model.

Correlation and Regression
381
Starting from the top and going from left to right:
t The Normal Probability Plot shows that the Residuals are normally distributed.
t The Versus Fits Plot shows that the Residuals are equally distributed about the centre line.
t The Histogram Plot shows that the Residuals do not show extreme skewness.
t The Versus Order Plot shows that the Residuals are not displaying any patterns or ordered behaviour. 
In summary, there are no issues with the residuals within this study.
Example 6.
Prawn wants to buy a car and is very interested in the power output of the engine. He knows that the 
power delivered by a car engine is affected by a number of factors and sets about collecting data on four 
key predictors that he feels will be signiﬁcant.
Analyse the data Prawn has collected in order to establish which of the predictors are signiﬁcant to the 
response?
Form the regression equation.
Worksheet: Engine Power.
1. Open the Excel ﬁle and transfer 
the data into Minitab.
2. Click Graph <<Matrix Plot.
3. Select Simple and then click OK. 

382
Problem Solving and Data Analysis using Minitab 
4. Select all the data columns as the graph 
variables.
5. Click on Matrix options.
6. Under Matrix Display select the radio but-
ton for Lower left (personal preference).
7. Click OK and OK again.
All the predictors appear to be independent from each other. Comp ratio and Power appear to have a 
strong positive correlation.
We will also check the correlation coefﬁcients.

Correlation and Regression
383
8. To check the strength of the correlation 
and whether it is statistically signiﬁcant 
click Stat <<Basic Statistics <<Correlation.
9. Select all the Variables and then click OK.
10. Select the session window icon to see the 
results.
We see that Comp ratio 
and Power have a sig-
niﬁcant correlation. 
None of the predictors 
are correlated so when 
it comes to reducing our 
model we can remove 
more than one predic-
tor at a time.
We are now ready to 
carry out the regression 
analysis.

384
Problem Solving and Data Analysis using Minitab 
11. Click on Stat <<Regression 
<<General Regression.
12. Select Power as the response 
and all the other variables 
as predictors. Click OK.
13. Click on the session window 
icon. The regression table 
shows that EngTemp and 
MGP are not signiﬁcant. We 
can remove them from the 
model. As they were not 
correlated we can remove 
them in one iteration.
14. Click on the Edit Last icon. 
15. Change the selection of the 
predictors to Comp ratio 
and Air Density. Then click 
OK.
16. Click on the session window 
icon. The regression table 
only includes signiﬁcant 
parameters. The R-Sq(adj) 
has improved from 98.1 to 
98.2%.
17. In order to check the residu-
als click on the Edit Last icon. 

Correlation and Regression
385
18. Then click on Graphs.
19. Under Residual Plots select 
Four in one. 
20. Click OK and OK again.
Starting from the top and going from left to right:
t The Normal Probability Plot shows that the Residuals are normally distributed.
t The Versus Fits Plot is showing patterned behaviour. We need to investigate why this is happening. 
Strong curvature indicates that the data is not well explained by the regression model.
t The Histogram Plot shows that the Residuals do not show extreme skewness.
t The Versus Order Plot shows that the Residuals are not displaying any patterns or ordered behaviour. 
In summary, the Versus Fits plot is indicates that the model does not ﬁt well. We need to go back and 
investigate why this is happening.

386 Problem Solving and Data Analysis using Minitab 
If we go back to the matrix plot we can see 
that the relationship is not quite linear. It 
may be quadratic. We can explore this in the 
regression model. The General Regression 
procedure in M16 has made this very easy. 
Initially, we will go through the method 
that M15 users need to use to investigate 
interactions and higher powers as predic-
tors. The start of each step will indicate 
whether it is for M15 or M16 in order to 
highlight the version methodology.
21. M15. Change the title of column C8 to 
Comp Ratio^2. ‘^’ is to designate rais-
ing Comp ratio to the power of two.
22. M15. Click on Calc <<Calculator. 
23. M15. In the Calculator menu store the 
results in column C8 and enter the 
expression Comp ratio*Comp ratio.
24. M15. Then click OK.
25. M15. We have created our Comp ratio 
squared term in C8. 
26. M15. Click on Stat <<Regression 
<<Regression.
Comp Ratio
Power

Correlation and Regression
387
27. M15. Change the selection of the predictors 
to Comp Ratio, Comp Ratio^2 and Air Density. 
Then click OK.
For M15 we have to manually create a column if 
we want to enter a polynomial or an interaction 
term. With M16’s General Regression menu we can 
enter the terms directly into the menu as shown 
next.
28. M16. Click on Stat <<Regression << General 
Regression.
29. M16. Change the selection of the predictors 
to Comp ratio, Comp ratio*Comp ratio and 
Air Density. Then click OK.
The results in the session window are the same for both methods. The regression table shows that the 
quadratic term is signiﬁcant. Also note the improvement in R-Sq(Adj).
Let’s check the residuals and see if they have improved.

388
Problem Solving and Data Analysis using Minitab 
The pattern in the Versus Fits plot has gone. We have successfully produced a model with a quadratic term.
Example 7. Multi Colinearity
Miff also wants to buy a car and is interested in the power output of the engine. Miff believes that 
engine power is most affected by a different set of parameters to Prawn.
Analyse Miff’s data to establish which of the predictors are signiﬁcant. 
Form the regression equation.
Worksheet: Engine Power2.
1. Open the Excel ﬁle and transfer 
the data into Minitab. Note that 
Miff has changed the predictors 
for this example.

Correlation and Regression
389
2. Click Graph <<Matrix plot.
3. Select Simple and then click OK. 
4. Select all the data columns as the 
Graph variables.
5. Click on Matrix Options.
6. Under Matrix Display select the 
radio button for Lower left (per-
sonal preference).
7. Click OK and OK again.

390
Problem Solving and Data Analysis using Minitab 
All the predictors appear to be independent from each other apart from Air Density and Outside Temp. 
MPG is about the only predictor that does not appear to be correlated with Power.
We will also check the correlation coefﬁcients.
8. To check the strength of the correlation and whether 
it is statistically signiﬁcant click Stat <<Basic Statistics 
<<Correlation.
9. Select all the Variables and then click OK.
10. Select the session window icon to see the results.

Correlation and Regression
391
As suspected, out of the predictors, Air Density and Outside Temp are strongly correlated.
We will now carry out the regression analysis.
11. Click on Stat <<Regression <<General Regression.
12. Select Power as the response and all the other vari-
ables as predictors. Click OK.

392
Problem Solving and Data Analysis using Minitab 
13. Click on the session window icon. The 
regression table shows that Outside 
Temp and Air Density are not signiﬁ-
cant. We should be able to remove 
them both from the model.
14. Click on the Edit Last icon. 
15. Change the selection of the pre-
dictors to EngTemp, MPG and Air 
Density. Then click OK.
16. Click on the session window icon. 
After removing Outside Temp notice that 
Air Density has become signiﬁcant. This 
is because the two parameters were cor-
related and they were causing instability 
within the model. This is known as multi 
colinearity and the best way to ﬁnd the 
best combination of predictors is to use 
a procedure like Best Subsets or Stepwise 
Regression. We will look at Predictor 
Selection Procedures after the exercise.
Exercise 4. Multiple Predictor Regression.
The drying rate of plasterboard within an industrial dryer is affected by a number of factors. Hawk 
wants to understand which factors are signiﬁcant and then wants to form a model which can be used 
to optimise the drying rate.
Analyse the data to establish which of the predictors are signiﬁcant to the response. 
Form the regression equation and then check the residuals to establish if the model is valid.
Worksheet: Drying Rate.

Correlation and Regression
393
All the predictors appear to be independent of each other. 
Humidity and Drying Rate appear to have a strong negative correlation.

394
Problem Solving and Data Analysis using Minitab 
None of the predictors are correlated so when it comes to reducing our model we can remove more than 
one predictor at a time.
On the ﬁrst run Air Pressure is the only 
predictor that is not signiﬁcant.

Correlation and Regression
395
On the second run all 
the predictors are sig-
niﬁcant. The regression 
equation for this model 
is straightforward.
We need to check the 
residuals to ensure the 
model is valid.
There do not appear to be any issues with the residuals.
Hawk now understands that the parameters affect drying rate are temperature, Humidity, Board Area 
and Air Speed. Air Pressure does not affect the drying rate.

396
Problem Solving and Data Analysis using Minitab 
9.8 Predictor Selection Procedure
Example 8. Predictor Selection Procedure.
Using the cannon data set from Example 5, use Best Subsets and Stepwise selection to evaluate the best 
combination of predictors. 
Worksheet: Cannon.
1. Open the Excel ﬁle and 
transfer the data into 
Minitab.
2. Click Stat <<Regression 
<<Best Subsets
Test
tBest Subsets and Step Wise Regression
What's it 
used for?
t They are used to select the best predictor variables when ﬁtting a 
model. They do not carry out the regression.
Assumptions
and
Limitations
t Both procedures usually produce consistent results.
t They become particularly useful when multi colinearity is causing 
problems in reducing the model.
t A combination of metrics is used to show the best predictors for 
the model.
t Neither procedure will attempt to ﬁnd models with interaction or 
higher order terms..

Correlation and Regression
397
3. Select Distance Fired as the 
Response and C1 to C6 as the 
Free predictors. Then click 
OK.
4. Select the session window 
icon to see the results. 
Minitab works through a number 
of models using different combi-
nations of predictors. We select 
the best combination of predic-
tors by looking at the results table.
Generally, we are looking for the 
highest R-Sq and R-Sq(adj) values. 
We use R-Sq when comparing 
models with the same number of 
predictors and R-Sq(adj) for dif-
fering numbers. We also want the 
lowest Mallows Cp and the lowest 
S value(S can be thought of as the 
standard deviation of residuals).
For the example we would select 
the model with Mallows Cp = 3.5 
as this row gives us the best 
results for R-Sq, R-Sq (adj) and S. 
Incidentally, this gives the same 
combination of predictors as the 
result in Example 5.
Now we will carry out the same 
procedure but this time using 
Stepwise selection.
5. Click Stat <<Regression 
<<Stepwise.

398
Problem Solving and Data Analysis using Minitab 
6. Select Distance Fired as the 
Response and C1 to C6 as the 
Predictors. Then click OK.
7. Select the session window 
icon to see the results. 
Again, the procedure works 
through a different combination 
of predictors until it ﬁnds the 
best model. 
The Stepwise procedure can be 
modiﬁed in the initial set-up. It 
can be set to add predictors or sub-
tract them. In the default mode it 
will add and then subtract. It can 
also be asked to stop after a num-
ber of iterations.
The result is the same as that for 
Best Subsets and the combina-
tion of predictors we selected in 
Example 5.

Correlation and Regression
399
Exercise 5. Predictor Selection Procedure.
After we discovered the multi colinearity in Example 7 we did not complete the exercise. Using the data 
set from Example 7 use the Best Subsets and Stepwise selection procedures to evaluate the best combi-
nation of predictors for the regression model. 
Worksheet: Engine Power2.
Using Best Subsets, the best model 
appears to be one with EngTemp, MPG 
and Outside Temp as predictors. It does 
not select Air Density as a predictor.
The Stepwise procedure selects the 
same predictors as the Best Subsets 
procedure.

400
Problem Solving and Data Analysis using Minitab 
9.9 Nonlinear Regression
Example 9. Nonlinear Regression.
Zaf, the chemical engineering consultant, is conducting evaluation trials on a new granular active car-
bon (GAC) ﬁlter. He is checking its ability to remove sulfur dioxide (SO2) from a gas stream. A gas stream 
with a constant 200 ppm SO2 content is passed through the ﬁlter at varying ﬂowrates. The exhaust con-
centration (Conc) of SO2 is recorded against the ﬂowrate.
Zaf has conducted a literature search and has found that a Power (concave) function will be the most 
appropriate model to ﬁt.
Help zaf by forming a model for the relationship between Flow and Conc. Can a ﬂow of 30 kg/min give 
a concentration of 20 ppm.
Worksheet: GAC Filter.
1. Open the ﬁle GAC ﬁlter and transfer the data into a new Minitab project 
window.
As a comparison we will display the results of using the Regression <<Fitted Line 
Plot procedure on this data. We have used a cubic model and from the session 
window we can see that the cubic term is signiﬁcant. 
Test
tNonlinear Regression
What's it 
used for?
t It is used when you cannot model the relationship with linear 
parameters. Linear models have only additive terms with only one 
parameter in each term.
Assumptions
and
Limitations
t Response variables must be continuous, predictors must be con-
tinuous or indicator variables (i.e. 0, 1).
t The procedure uses an iterative algorithm to ﬁt the best model 
and therefore requires starting values for the iteration.
t Prior to running the procedure the expectation function needs to 
be speciﬁed. Due to the almost inﬁnite number of nonlinear func-
tions prior knowledge of the model is almost mandatory.
t Nonlinear Regression is M16 only.

Correlation and Regression
401
The Fitted Line Plot shows a fairly good ﬁt with an 
R-Sq(adj) of 92.9% which also indicates a good ﬁt.
The Four in One residual 
plot indicates problems with 
the model. The histogram is 
extremely skewed and the 
versus ﬁt plot shows that 
the residuals are not evenly 
distributed around the zero 
line. In fact we are seeing 
strong curvature indicating 
the data is not well explained 
by the model. Clearly, we 
need to attack this problem 
in a different way so we turn 
to Nonlinear Regression.

402
Problem Solving and Data Analysis using Minitab 
2. Click on Stat <<Regression <<Nonlinear Regression.
3. Enter Conc as the Response variable.
4. We have the option of using the calculator to form 
our own function or we can use a predeﬁned function 
by clicking on Use Catalog. Click on Use Catalog.
5. Select the Power (concave) function from the list. We 
see that it uses one predictor and two parameters. 
6. Click on OK.
7. We can see the algebraic form of the equa-
tion we have selected. We must now let the 
procedure know which column represents 
X in the equation. Select Flow and then 
click OK.
8. As the Nonlinear Regression procedure uses 
an iterative process to produce the model 
we need to estimate starting values. Click 
on the Parameters button to enter the 
starting values for the iteration.
9. Looking at the proposed equation and the 
data that has been generated I guessed the 
starting values. (Actually, on the ﬁrst occa-
sion I used 0.01 but the iteration stopped 
after four loops.) Enter 0.1 as the starting val-
ues for Theta1 and Theta2 and then click OK.

Correlation and Regression
403
10. Click on the Graphs Button. Click on 
the tick box for Display prediction 
intervals as this will help us to answer 
a part of the question. 
11. Select the Four in one residuals plot.
12. Select OK and OK again.
13. Within the Session Window we see the 
Lack of Fit test. We only get the results 
of this test if there are replicates 
within our data. If the P value was 
below our D level then there would be 
evidence to show that model did not 
ﬁt the data. However, this is not the 
case.
14. Go to the Fitted Line 
Plot. We can see the 
equation 
that 
has 
been developed and 
the Prediction Interval 
for new points has 
been plotted.

404
Problem Solving and Data Analysis using Minitab 
15. Right Click on the graph area and then 
select Add <<Reference Lines. Complete 
the menu box as shown with reference 
lines at X = 30 and Y = 20.
As the intersection point of the two lines lies outside of the prediction interval we can be conﬁdent that 
a ﬂow of 30 kg/min will not give a concentration of 20 ppm.
We need to check the residuals to validate the model. 

Correlation and Regression
405
There do not appear to be any issues with the residuals.
Exercise 6. Nonlinear Regression.
The reaction rate of a new enzyme is being evaluated. The concentration of the substrate (Conc) is var-
ied and the resulting rate of reaction (RoR) is measured. 
For this type of study it is known that the Michaelis–Menten equation is usually appropriate to model 
enzyme kinetics.
Form a model for the relationship between Conc and RoR. 
Worksheet: Enzyme.
Within the session window we see the 
Lack of Fit test. We see that the P value 
is above 0.05 which indicates that we 
cannot say that our model does not ﬁt 
the data.

406
Problem Solving and Data Analysis using Minitab 
The Fitted Line Plot shows the param-
eters within the Michaelis–Menton 
equation that have been developed 
for this study.
Starting values of 0.1 were again used 
for theta1 and theta2.
Although the residuals could be better we don’t have enough abnormality to reject the model.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 10
Design of Experiment
10.1 Why Use Design of Experiment?
Design of experiment (DOE) is used to assess the input factors and interactions that are important to our 
response. In a series of designed experiments, where the number of experimental runs are optimised, 
the important factors and interactions are identiﬁed. We can then look at optimising the signiﬁcant 
input settings in order to achieve our desired output.
10.2 Types of DOE
The type of DOE and the number of runs it uses depends on our objectives.
Screening DOE
Characterisation DOE
Response Surface 
Designs
Taguchi Designs
Used to reduce a large 
number of factors to the few 
signiﬁcant factors that are 
relevant.
Used to evaluate main factors 
and their interactions. Leading 
us to a prediction equation.
Used to model curvature 
within the design
Used to reduce variation 
and reduce sensitivity to 
noise factors.
Weight
 Temperature
Flow
Concentration
Viscosity
Length
Energy
Process
Noise Variables

408
Problem Solving and Data Analysis using Minitab 
10.3 DOE Terminology
In order to explain some 
of the DOE terminology 
we are going to look at 
an example of a Two Level 
Full Factorial DOE. 
We are going to look at 
a mopping process. We 
are interested in how the 
detergent concentration 
and water temperature 
affect the grime concen-
tration 
after 
mopping. 
Our factors are Dconc and 
Wtemp and the response 
is Grime.
We carry out four experi-
ments using low and high 
levels for each factor as 
shown. As each combina-
tion of factors is run only 
once we say there are no 
Replicates. It is preferable 
to have Replicates within 
a DOE as it allows Minitab 
to calculate the variability 
within the experiment. (In 
reality our example DOE 
would not work without 
replicates.)
One of the main beneﬁts 
of using DOE is that it 
optimises the number of 
experimental runs that 
are needed.
Let’s just look at how many 
runs would be required if 
we had not used a Two 
Level Factorial approach 
but decided to use six lev-
els for each factor.
Dconc
Low = 15
High = 30
Wtemp
Low = 10
High = 20

Design of Experiment
409
From needing four experimen-
tal runs we have now gone to 
needing 36. 
The experimenter needs to 
decide if the cost and time 
required to do the extra runs 
is beneﬁcial in terms of the 
information that is likely to be 
gained and how much value 
that information will have.
As we are carrying out a Two 
Level Factorial DOE we look at 
each factor at a high and low 
levels only. We call these our 
Corner Points and we assume 
a linear response between the 
corner points. Other types of 
DOE have additional points to 
look for curvature. 
With this type of DOE:
t For two factors we need 
four runs.
t For three factors we need 
eight runs.
t For four factors we need 
16 runs.
We call this a 2k factorial, where 
k is the number of factors.
2 Factor Design
3 Factor Design
4 Factor Design

410
Problem Solving and Data Analysis using Minitab 
Let’s have a look at the Design Matrix for 
our experiment.
We can change the values into Coded 
units where the high values are repre-
sented by +1 and the low values by –1. 
We can also add the interaction term. 
Notice that the interaction term is the 
product of the coded units of the factors.
Our design is Balanced because it con-
tains the same number or runs at each 
high value and low value for each factor, 
that is two.
The design is Completely Orthogonal
because the sum of the products of the 
coded factors is zero. Also, the Pearson 
correlation coefﬁcient for the coded fac-
tors and interaction term is zero, that is 
No Correlation.
Let’s say we have carried out the experi-
ment. The results are shown in the table. 
The higher the grime value the dirtier the sample was after mopping.
The Cube plot shows the results in a very graphical and straight forward manner.
Let’s have a look at the Main Effects plot and Interaction plot and see how the results were calculated.
Wtemp
Dconc
10
15
10
30
20
15
20
30
Wtemp
Dconc
Wtemp*Dconc
1
1
1
1
1
1
1
1
1
1
1
1
Sum of Column = 0
Wtemp
Dconc
Grime
10
15
212
10
30
135
20
15
187
20
30
90
−1
−1
+1
+1

Design of Experiment 411
Let’s start with the Main Effects plot and see how the Main Effects are calculated.
Factor Effect
Average  of
responses at
high levels
Average  
=
-
of
responses at
low levels
Dconc Effect
 
=
+
-
+
= -
135
90
2
212
187
2
87
Equation (10.1)
Wtemp Effect
 
=
+
-
+
= -
187
90
2
212
135
2
35
Equation (10.2)
Let’s have a look at the Interaction plot and see how the Interaction is calculated.
Interaction =
-
-
-
= -
212
135
2
187
90
2
10
Equation (10.3)
Now that we understand more about the terminology let’s work through the full example. 
212−135
187−90

412
Problem Solving and Data Analysis using Minitab 
10.4 Two Level Factorial Designs
Example 1. Setting up the experimental design.
We are going to expand on the mopping example. We are interested in how the detergent concentra-
tion, water temperature and material of the sponge on the mop affect the grime concentration after 
mopping. Our factors are Dconc, Wtemp and Material and the response is Grime. Initially eight runs 
were carried out on day1 and then the eight runs were replicated on day2.
If the historical standard deviation is 10 and the effect of the factors is at least 20 what is the minimum 
power for this experiment?
Also, set up the experimental worksheet.
No File.
1. Click Stat <<Power and Sample Size <<2 Level Factorial 
Design.
Procedure
tTwo Level Factorial Designs
What's it 
used for?
t The design of experiment process is used to understand the effect 
of process inputs and their interactions on the outputs of the 
process.
Assumptions
and
Limitations
t A two level factorial DOE evaluates the changes in response when 
combinations of high and low levels of each factor are tested.
t This type of DOE cannot detect curvature within a single run.
t This type of DOE can be used for characterisation or screening but 
of the two it is more likely to be used for characterisation.

Design of Experiment
413
2. Complete the menu box as shown. 
Standard deviation and Effects are 
estimated from historical knowledge 
of the process. 
3. Click on Designs.
4. In order to highlight how to use 
Blocks we said that the experiment 
was carried out over two days. We 
need to know if carrying out half the 
runs on one day and half on the next 
affects the results. In order to check 
if this has changed the experiment 
we click on Include blocks in model 
and we specify 2 as the number 
of blocks. In this way using Blocks 
allows us to assess whether a noise 
variable is affecting the experiment.
5. Click OK and OK again.
6. The graph that pops open shows the Power Curve against Effect. With the data that we speciﬁed 
we will have a 92.7% chance of recognising a difference if one exists.

414
Problem Solving and Data Analysis using Minitab 
2. Go to the session window to see 
the summary of the data. As we 
are happy with the power we 
can now set up the worksheet 
for the experiment.
3. Click Stat <<DOE <<Factorial 
<<Create Factorial Design…
4. We are going to use a ‘2-level 
factorial’ design which should 
be selected by default. Change 
the Number of factors to 3 
and click on Display Available 
Designs.
A matrix of available two level facto-
rial designs is shown here. The roman 
numerals are the resolution of frac-
tional designs, which we will cover 
later. We do not need to select any-
thing from this menu. Click OK and 
then Designs.

Design of Experiment
415
5. Ensure Full Factorial is high-
lighted. Complete the remain-
der of the menu box as shown. 
Click OK.
6. Notice how the remaining but-
tons become selectable. Click 
on the Factors button.
7. This is where we enter the name, 
type and values of high and low 
settings for our factors. Notice 
that we are using a categori-
cal value for Material. Copy the 
menu box and click OK.
8. Click on the Options button. 
Folding a design is concerned 
with Fractional designs. For the 
example we are going to dese-
lect Randomise runs so we can 
see the structure of the experi-
mental runs in the project 
window. Normally we would 
randomise the runs to reduce 
human inﬂuence on the experi-
ment. Click on OK and OK again 
to produce the worksheet.

416
Problem Solving and Data Analysis using Minitab 
9. Go to the project window and 
look at how the sheet is laid 
out. Notice that there are eight 
runs within each block. Each 
block contains all the possible 
combinations of factors. This is 
the sheet that will be used to 
collect the response data.
Example 2. Analysing the experiment.
Just in case you forgot, our factors are Dconc, Wtemp and Material and the response is Grime. We now 
have the experimental results. Eight runs were carried out on day1 and then the eight runs were repli-
cated on day2.
The researchers want to know which factors have affected the response and whether carrying out the 
replicates the next day affected the experiment. Also, are any of the interactions signiﬁcant?
Use the worksheet window from example 1.
All data sets for this chapter are within Excel ﬁle 10 DOE.xls.
Worksheet: Mopping.
1. Copy the data from column 
H, which is the response 
data, into column C8.

Design of Experiment 417
2. Click Stat <<DOE <<Factorial 
<<Analyze Factorial Design
3. For Responses, select Grime.
4. Click on the Terms button.
5. Ensure 
that 
blocks 
are 
included in the model and 
the selected terms appear as 
shown.
6. Click OK then click on the 
Graph button.
7. Under Effects Plots select 
Pareto. We won’t select the 
residual plots until the model 
has been reduced.
8. Click OK and OK again.
The Pareto Chart that is gener-
ated shows the signiﬁcance of the 
terms. If any of the bars represent-
ing the terms go beyond the red 
line they are signiﬁcant.

418
Problem Solving and Data Analysis using Minitab 
Within the session window ﬁnd the 
Estimated Effects table. The P values 
listed will tell us which of the factors 
and interactions are signiﬁcant. We 
also see that our Block was not sig-
niﬁcant, so carrying out the experi-
ment over two days had no effect.
We will now discuss reducing the 
model.
This ﬂow sheet shows the procedure that should be followed when removing insigniﬁcant terms from 
DOE models. 
It is good practice to follow the procedure for all DOEs but it is not required for orthogonal designs with 
replicates.
Although not highlighted within the ﬂow sheet we can remove all nonsigniﬁcant terms of the same 
order at the same time.
Identify highest order
nonsignificant term.
e.g. 3-Way interaction.
Remove Effect from
Model and refit model.
Is the next lowest
term a main effect?
Is the main effect
present in any
significant
interactions?
Keep Main effect.
Remove term from
Model and refit model.
Y
Y
N
N
Select the next
lowest nonsignificant
term.
Model reduction
is complete.

Design of Experiment
419
9. We are now going to 
reduce the model as per 
the ﬂow sheet shown on 
the previous slide. Click 
on the Edit Last button.
10. Click on the Terms button.
11. Deselect Blocks and the 
3-Way interaction from 
the model.
12. Click OK and OK again.
13. The three way interaction 
has been removed from 
the Pareto Chart and we 
can see that all of the two 
way interactions are also 
not signiﬁcant. 
14. Within the session win-
dow the effects table 
also shows that the two 
way interactions are not 
signiﬁcant.

420
Problem Solving and Data Analysis using Minitab 
15. Click on the Edit Last button.
16. Click on the Terms button.
17. Remove the two way interactions 
from the selected terms. As only 
the signiﬁcant terms remain we 
can now produce the Four in One 
Residual Plot. 
18. Click OK.
19. Click on the Graphs button. 
20. Select the Four in one Residual 
Plot.
21. Click on OK and OK again.
22. With the two way interactions 
removed from the Pareto chart 
only the main factors are show-
ing as signiﬁcant.

Design of Experiment
421
23. In the session window we 
see conﬁrmation that the 
main factors are signiﬁcant 
in the Estimated Effects 
table. R-Sq(adj) is 98.4. This 
means that 98.4% of the var-
iation seen within Grime can 
be explained by the changes 
within our factors.
Also listed within the session win-
dow we have a group of numbers 
listed under the line ‘Estimated 
Coefﬁcients for Grime using data 
in uncoded units’.
These are the coefﬁcients that can 
used to build a prediction equa-
tion for the Response. The full 
prediction equation for Grime is 
shown here for your convenience.
We also need to conﬁrm the validity of the model by checking the residuals. 
Grime = 285.9 – 2.74*Dconc – 3.35*Wtemp – 6.5*Material
where Material would have values of 1 or –1
Equation (10.4)

422
Problem Solving and Data Analysis using Minitab 
t Residuals are normally distributed on the normal probability plot.
t Residuals are equally distributed about the centre line on the versus ﬁts plot.
t Histogram of residuals does not show extreme skewness.
t If the original data was collected in time order then the residuals should not show any ordered 
behaviour on the versus order plot.
As there are no issues with the Residuals we will now produce the Main Effects and Cube Plots.
24. Click Stat <<DOE <<Factorial << Factorial Plots

Design of Experiment
423
25. As there were no signiﬁcant 
interactions we will only look 
at the Main Effect and Cube 
Plots. Go into Setup for Main 
Effects.
26. Select Grime in the Responses 
and then select the three main 
effects as the factors within the 
plots. Then click OK. After set-
ting up the Cube Plot in exactly 
the same way click OK and OK 
again.
27. We want to minimise the Grime 
response. The Main Effect Plots 
show the levels we need to 
use in our factors to minimise 
Grime. As the y axis units are 
the same we can see the Main 
Effect that has the steepest 
line has the greatest effect on 
the response.

424
Problem Solving and Data Analysis using Minitab 
28. The cube plot shows us all the fac-
tors together. The corners show 
the average response for the cor-
responding factors. The corner rep-
resenting Material = B, Dconc = 30 
and Wtemp = 20 gives the lowest 
Grime value and this is marked on 
the cube.
Example 3. The Gamer.
Humzah has bought the latest Cawl of Duty game for his PC. Unfortunately, the game does not run fast 
enough so he buys some new hardware for his PC. He runs a two level factorial DOE in order to optimise 
his hardware settings. His response is frames per second (FPS), however he realises that on occasions his 
CPU overheats. This means that he needs CPUtemp as a response as well. The factors that he is trying to 
optimise are:
FSB = Front Side Bus speed (200, 220)
CPUX = CPU Multiplier (10, 12)
Memspeed = Memory speed (333, 400)
GPUspeed = Graphics Processing Unit speed (500, 600).
Establish which of the factors and interactions are signiﬁcant. Use the Results Optimizer to establish the 
settings to use to minimise CPUTEMP and maximise FPS. CPUTEMP is more important and the target for 
that is 65 with an upper limit of 70. He wants a target of 200 for FPS with a lower limit of 180. 
Worksheet: Gamer.
As there are two responses we must carry out the DOE analysis twice to decide what the signiﬁcant 
factors are for each response before we use the Results Optimizer. Also, as we are going to be import-
ing the complete results table into Minitab we must ﬁrst initialise the design. This is done by using the 
Deﬁne Custom Factorial Design procedure. Basically, we need to tell Minitab that the results we have 
imported in are from a DOE procedure.

Design of Experiment
425
1. Copy the data from the Excel ﬁle into 
a new Minitab project window.
2. Click Stat <<DOE <<Factorial <<Deﬁne 
Custom Factorial Design. 
3. Select the four factors. Two level fac-
torial should be selected by default.
4. Click on the Low/High button.
5. Minitab will automatically pick up 
the high and low values of all of the 
factors. Note that uncoded units is 
selected by default. 
6. Click OK and then click on the Designs 
button.

426
Problem Solving and Data Analysis using Minitab 
7. We don’t need to change anything but 
we could specify the data order, center 
points and blocks within this menu.
8. Click OK and OK again. Now that we have 
deﬁned our factorial design Minitab will 
be happy to analyse it for us.
9. Click Stat <<DOE 
<<Factorial <<Analyse 
Factorial Design. Select 
FPS as the response. 
We are going to carry 
out the DOE analysis 
procedure for FPS ﬁrst 
and then CPUTEMP.
10. Click on the Terms button. All terms up to the fourth 
order should be preselected. Click OK.
11. Click on Graphs.
12. Under Effects Plots select Pareto.
13. Click OK and OK again.

Design of Experiment
427
14. On the Pareto Chart 
all the bars that go 
beyond the red line 
are signiﬁcant.
15. In the session window 
the Analysis of Variance 
table shows the break-
down of the terms that 
are signiﬁcant. It can 
be seen that the three 
way and four way inter-
actions are not signiﬁ-
cant. However, we will 
use the recommended 
stepwise procedure to 
reduce the model.
16. Click on the Edit Last 
icon.
17. Then click on the Terms 
button.
18. Deselect the ABCD term 
or change the selec-
tion so that only terms 
up to order three are 
considered.
19. Click OK and OK again.

428
Problem Solving and Data Analysis using Minitab 
20. The Pareto chart shows 
that we can also remove 
all three way interactions 
so we will do that next.
21. Click on Edit Last.
22. Then click on the Terms 
button.
23. Change the selection so 
that only terms up to 
order two are considered 
or manually remove all 
four of the three way 
interactions.
24. Click OK and OK again.
25. The Pareto chart shows 
that we can also remove 
all two way interactions 
with the exception of CD 
which is signiﬁcant.

Design of Experiment
429
26. Click on the Edit Last icon.
27. Then click on Terms button.
28. Change the selection so that only CD 
and the main effects are selected. Click 
OK.
29. As this is the last iteration we will also 
produce the residuals plot. Click on 
the Graphs button and then select the 
Four in one residual plot. Click OK and 
OK again.
30. The Pareto chart shows only signiﬁcant 
terms as expected.
31. In the Session window 
the Estimated Effects 
table shows that the 
main effects are signif-
icant and the CD term 
which is the interaction 
between 
MemSpeed 
and GPUSpeed.
We can conﬁrm the validity 
of the model by looking at 
the residuals for FPS. After 
which we will look at the 
Main Effect and Cube plots.

430
Problem Solving and Data Analysis using Minitab 
t Residuals are normally distributed on the normal probability plot.
t Residuals are equally distributed about the centre line on the versus ﬁts plot.
t Histogram of residuals does not show extreme skewness.
t If the original data was collected in time order then the residuals should not show any ordered 
behaviour on the versus order plot.
As there are no issues with the Residuals we will now produce the Main Effects and Cube Plots.

Design of Experiment
431
32. Click Stat <<DOE <<Factorial <<Factorial Plots. 
Select Main Effects Plot and Cube Plot. We 
won’t plot the Interaction Plot to see our single 
interaction term.
33. Both plots need to be set up in exactly the 
same way before they can be displayed. Click 
on Setup then select FPS as the Response and 
select all the main effects as factors to be plot-
ted. Carry out the same setup for both plots 
and then click OK and OK again. 
34. The Main effects plot shows that MemSpeed and GPUSpeed are having a relatively large effect on 
the response.

432
Problem Solving and Data Analysis using Minitab 
35. The Cube plots show the mean response that we got at the different levels. The changing levels 
of the main effects can be seen to be changing the response, bearing in mind that there is also an 
interaction present but it is hard to see in this cube plot.
We will now analyse the other response.
36. Click Stat <<DOE <<Factorial <<Analyse Factorial 
Design. We are going to carry out the DOE analysis 
procedure for CPUTEMP so that needs to be selected 
as the Response.
37. Click on the Terms button. Ensure all terms up to the 
fourth order are preselected. Click OK.
38. Click on the Graphs button.
39. Deselect the Residual Plots. The Pareto plot remains 
selected.
40. Click OK and OK again.

Design of Experiment
433
41. Again, on the Pareto 
Chart all the bars that 
go beyond the red line 
are signiﬁcant.
42. In the session window 
the Analysis of Variance 
table shows the break-
down of the terms that 
are signiﬁcant. It can 
be seen that the four 
way interaction is not 
signiﬁcant 
but 
one 
three way interaction 
is signiﬁcant. 
43. As a mini exercise I 
will leave it to you to 
reduce the model.
44. The ﬁnal Pareto chart 
shows that the main 
effects were all signiﬁ-
cant with the excep-
tion of D (GPUSpeed), 
however, this is kept 
in the model because 
there are signiﬁcant 
interactions that con-
tain GPUSpeed.

434
Problem Solving and Data Analysis using Minitab 
45. The Estimated Effects 
table also shows the 
same information.
46. As another mini exercise 
plot the Main Effects 
and Cube Plots. Do not 
put GPUSpeed into the 
cube plot.
47. The Main effects plot 
shows that CPUX and 
MemSpeed have a rela-
tively large effect on the 
response, whereas FSB 
has slightly less effect 
and GPUSpeed has no 
effect at all.
48. Part of the ABC inter-
action 
can 
be 
seen 
within the cube plot. As 
GPUSpeed did not have 
an effect we did not put 
it into the cube plot as 
an additional factor.

Design of Experiment
435
We now need to use the Results Optimizer 
to establish the levels of the factors that we 
want to use to minimise CPUTEMP and max-
imise FPS. It was said that CPUTEMP was more 
important and the target for that was 65 with 
an upper limit of 70. The target for FPS was 
200 with a lower limit of 180.
49. Click Stat <<DOE <<Factorial <<Response 
Optimizer. 
50. Select both of the responses. Then click 
on the Setup button. 
51. Complete the header of the menu box 
as shown. Note how the menus are 
completed with our required response 
information.
52. Click OK and OK again.

436
Problem Solving and Data Analysis using Minitab 
Minitab will open the Response 
Optimizer showing the Optimal 
settings. Along the top are the 
four main effects. The red value 
in square brackets shows their 
current settings and these val-
ues represent the best settings 
we can use to achieve the goals 
that have been set. However, if 
the user wishes to explore new 
values they can be changed 
by dragging the correspond-
ing vertical line. The Optimal is 
given as a graphical matrix, on 
the left hand axis we have the 
Responses. An FPS of 193.6 and 
CPUTEMP of 67.5 are the opti-
mal response values we can 
achieve. ‘d’ is the desirability 
factor for each response. The 
Composite Desirability is 0.614, 
as shown in the top left.
The lower plot shows the 
effects on the responses and 
desirability when the values of 
the Main Effects are changed 
by dragging the red lines. Have 
a go yourself and see how the 
values change.
Exercise 1. Artiﬁcial Diamonds.
Princess Raeesa has developed a new press to make artiﬁcial diamonds. Pure carbon powder is placed 
into the press and then exposed to extreme pressure and temperature for a set amount of time to 
produce artiﬁcial diamonds. The settings of time, pressure, temperature and particle size of the carbon 
need to be optimised to give a yield of 50%. 
If the yield is higher than 55% the diamonds lose clarity; and any lower than 45% and the process 
becomes uneconomic. A series of experiments were carried out at the following levels of each of the 
factors and the yield was measured.
Particle size = Partsize (10, 40)
Time = Time (400, 600)
Pressure = Press(30, 60)
Temperature = Temp(800, 900)
Establish the settings required to obtain a yield between 45 and 55%. 
Worksheet: Diamond.

Design of Experiment
437
Using the Pareto Chart the 
model can be reduced to sig-
niﬁcant terms only.
It can be seen that all the 
main terms are signiﬁcant as 
is the Partsize*Press interac-
tion. Although, not part of 
the question we will have a 
look at the main effects and 
interactions plot for the AC 
term.
The main effects plots show 
that all the main effects have 
a positive effect on yield. 

438
Problem Solving and Data Analysis using Minitab 
The interaction plot shows 
that there is a strong inter-
action between particle size 
and pressure.
We can now check the resid-
uals to establish the validity 
of the model.
t Residuals are normally distributed on the normal probability plot.
t Residuals are equally distributed about the centre line on the versus ﬁts plot.
t Histogram of residuals does not show extreme skewness.

Design of Experiment 439
t If the original data was collected in time order then the residuals should not show any ordered 
behaviour on the versus order plot.
We must now use the Response Optimizer to establish the settings we need to get a yield of 50%.
Minitab gives us a solu-
tion to get a yield of 50%.
However, this plot shows 
a different solution. There 
is still freedom within 
the other parameters to 
change the settings and 
still obtain a Yield of 50%. 
(The accuracy of the slider 
stops us getting exactly 
50%.)
10.5 Fractional Factorial Designs
As can be seen from Minitab’s table of available designs the experiment must have at least three factors 
for a fractional factorial design to be employed.
With Fractional designs we say the number of runs required is 2k–n, where k is the number of factors and 
n indicates the fractional level of the design. Where Fractional Level = 1
2n  (e.g. when n = 2 we have a 
quarter fractional design).

440
Problem Solving and Data Analysis using Minitab 
A, 26–3 would be an eight fractional 
design with six factors and it would 
require eight runs. Notice that we 
don’t actually subtract one power 
from the other in order to show the 
type of experiment we are running. 
Locate the design in the table shown 
above. The ‘III’ indicates that the 
design has a resolution of three; this 
will be explained soon.
If we wanted to run a Full Factorial 
experiment with three factors it 
would require eight runs, whereas 
a Half Factorial with three factors 
would require four runs, (23–1 = 4).
Both of these experiments can be 
represented by the cube plots shown 
here. On the Half Factorial the points 
represented by the grey dots would 
not be tested.
Let’s say that the three factors were 
A, B and C. Let’s have a look at the 
Coded levels we would be using in 
the experiment.
The table opposite shows the coded 
levels that would be used for the full 
factorial experiment. The table also 
contains interactions and the results 
of multiplying out the coded units. 
In order to form the half fractional 
experiment Minitab takes the lines 
where the A*B*C interaction is 1 
and uses them to form the fractional 
experiment.
However, there is now a problem in 
that column A is now the same as 
B*C. This means that our estimation 
of the effect of A will be the same as 
the B*C interaction.
Full
Factorial
Half
Factorial
A
B
C
A*B
A*C
B*C
A*B*C
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
A
B
C
A*B
A*C
B*C
A*B*C
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Design of Experiment
441
We say that the main effects are ‘confounded’ or ‘aliased’ with the two way interactions as the same can 
be said for B & A*C and C & A*B.
It can be seen that the advantage of fractional designs is the reduced number of runs, which reduces 
time and no doubt cost. However, the assumption is made that lower order terms are responsible for the 
changes and not the higher order interactions.
We previously stated that this experiment had resolution III. This means that the ﬁrst order terms (main 
effects) are confounded with the second order terms (two way interactions).   As III = 1 + 2.
Similarly, for a resolution V design the following terms would be confounded:
First and fourth orders
Second and third orders.
To understand what the term 
Design Generator means it is 
better to look at things from 
the other direction. If we had 
a two level full factorial with 
two factors it would look like 
the table shown here. 
For a half fractional design we 
essentially squeeze another 
factor in which is identical 
to A*B. The combination of 
1s and –1s are the same for C 
and A*B. This means that the 
estimate of the main effect 
will be the same. However, 
We assume that A*B will be 
negligible when compared to 
the effect of C.
We say that the Design 
Generator is C = AB because C 
is aliased with A*B. This term 
is the Design Generator even 
though other terms are con-
founded. In more complex 
designs the Design Generator 
can be manually selected.
Negligible

442
Problem Solving and Data Analysis using Minitab 
Example 4. The column.
Engineers want to understand the 
operation of their sulphur diox-
ide (SO2) scrubbing column. They 
want to know what factors are 
important and affect the concen-
tration of SO2 (SO2Conc) leaving 
the column in the gas stream. 
The SO2 is removed from the gas 
going into the column by spraying 
water into a column containing 
a packing material. The packing 
material provides a high surface 
area for contact between the gas 
stream and the water. The number 
of nozzles within the column can 
be adjusted. The engineers also 
monitor the pressure within the 
column and the pH of the water. 
The engineers can actually set the 
pH of the water using a separate 
control system.
Procedure
tFractional Factorial Designs
What's it 
used for?
t The design of experiment process is used to understand the effect 
of process inputs and their interactions on the outputs of the 
process.
Assumptions
and
Limitations
t Fractional designs will tend to be used for screening important 
factors.
t Fractional designs are used when it is thought that higher order 
terms are not having an impact on the response.
t Fractional Designs will contain terms that are confounded.
t This type of DOE cannot detect curvature within a single run.
t This type of DOE is not orthogonal.
Gas in
Gas to
atmosphere
Height of
packing
Scrubbing
water circuit

Design of Experiment 443
The factors that the engineers will be considering are:
GasFlow (20, 30)
GasPressIn (4, 8), gas inlet pressure
Nozzles (3, 4)
HofPacking (22, 30), height of packing
FRWater (100,150), ﬂow rate of water
pH (3, 7)
ColPress (20, 50), column pressure.
A two level factorial would require 128 runs which would take far too long. The engineers are willing 
to consider a half fractional DOE. Set up the worksheet for them and note which terms are confounded.
No Data File.
1. Click Stat <<DOE <<Factorial <<Create 
Factorial Design…
2. In the menu box that appears change 
the Number of factors to 7.
3. Click on the Display Available Designs 
button. We see that a half factorial 
design with seven factors has a resolu-
tion of VII. This means the following 
order terms will be confounded: ﬁrst 
and sixth, second and ﬁfth, third and 
fourth order terms. Click OK.
4. Click on the Designs button. Select the 
half fraction design then click on OK.

444
Problem Solving and Data Analysis using Minitab 
5. Click on the Factors button. Enter 
the details of all seven factors. Note 
that we are entering Nozzles as a 
text value as we can only have three 
or four nozzles physically within the 
column. Click OK when complete.
6. Click on the Options button. Ensure 
that Randomise runs is not selected. 
Normally, it would be better to ran-
domise runs to reduce human inﬂu-
ence but we want to be able to copy 
in the table of results later. Click OK 
to produce the experimental work-
sheet in the project window, as 
shown below.
The session window gives the design 
summary. The design generator is G 
= ABCDEF. The deﬁning relation I = 
ABCDEFG allows the formation of 
the Alias Structure.
For example, if we want to know 
which terms are aliased with BC we 
multiply BC by I + ABCDEFG:
BC (I + ABCDEFG) = BC + AB2C2DEFG.
As B2 = 1 and C2 = 1, then
BC (I + ABCDEFG) = BC + ADEFG.
Therefore the BC interaction will be 
aliased with ADEFG interaction.

Design of Experiment
445
Example 4. The column, continued.
With the trial sheet set up the engineers carry out all of the trial runs. Copy the results from the Excel ﬁle 
(column SO2Conc) and paste them into C12 of the Minitab sheet you have just created.
Find out which of the factors are signiﬁcant and establish if any of the interactions are signiﬁcant.
Produce the Main Effects plots and interaction plots if applicable.
Worksheet: Column.
The procedure to analyse fractional designs is the same as that for two level designs so we will not fully 
repeat the repetitive elements of the procedure.
1. Click Stat <<DOE <<Factorial 
<<Analyse Factorial Design. 
Then select SO2Conc as the 
Response.
2. Click on the Terms button. 
Note that Minitab has auto-
matically set the Selected 
Terms to only include up to 
third order terms. This is so 
that pairs of confounded 
terms are not included in the 
model by default. Click OK.

446
Problem Solving and Data Analysis using Minitab 
3. Click on the Graphs button.
4. Under Effects Plots select Pareto. We don’t want residual 
plots until we have fully reduced the model.
5. Click OK and OK again.
6. The Pareto plot shows us that none of the third order terms 
are signiﬁcant so we can eliminate them from the model. As 
the design did not contain any replicates we cannot use the 
ANOVA table in the session window to reduce the model. 
We will only use the Pareto chart.
7. To reduce the model click on Edit Last icon.
8. Then click on the Terms button. Change the selec-
tion so only second order terms are included. 
Click on OK and OK again.

Design of Experiment
447
9. It can be seen that EF is 
the only second order 
term that is signiﬁcant. 
10. Reduce the model again. 
The easiest way to do 
this is to change the 
selector so that only ﬁrst 
order terms are included 
and then manually select 
EF so that it is included. 
The Pareto plot shows that 
three of the main effects are 
not signiﬁcant. As these are 
not included in any of the 
interactions they can also be 
removed from the model.
11. Click on the Edit Last 
icon and then the Terms 
button.
12. Change the ﬁnal selec-
tion of terms so it is the 
same as that shown. 
Click OK.

448
Problem Solving and Data Analysis using Minitab 
13. Click on the Graphs button 
and select the Four in one 
Residual Plots.
14. Click OK and OK again.
15. The ﬁnal Pareto plot shows 
all the terms that are 
signiﬁcant.
16. Within the session window 
the Effects table shows the 
T and P values of the terms 
that are signiﬁcant. We also 
see that 96.89% of the vari-
ation within the response 
can be explained by the 
changes within our factors. 
Finally, we see the coefﬁ-
cients that can be used to 
form a prediction equation. 

Design of Experiment
449
t Residuals are normally distributed on the normal probability plot.
t Residuals are equally distributed about the centre line on the versus ﬁts plot.
t Histogram of residuals does not show extreme skewness.
t If the original data was collected in time order then the residuals should not show any ordered 
behaviour on the versus order plot.
As there are no issues with the Residuals we will now produce the Main Effects and Interaction Plots.

450
Problem Solving and Data Analysis using Minitab 
17. Click Stat <<DOE <<Factorial <<Factorial Plots. 
Select Main Effects Plot and Interaction Plot. 
We don’t want a cube plot as the number of 
terms is fairly high.
18. Click on Setup for the Main Effects Plot. Select 
SO2Conc as the Response. Select the main 
effects as shown. Click OK. 
19. Then click on Setup for the Interactions Plot. 
Select SO2Conc as the Response. Select the 
E:FRWater and F:pH as the variables to be plot-
ted. EF was the only signiﬁcant interaction. 
Click OK & OK again.

Design of Experiment
451
The interaction plot shows 
that when FRWater is low 
there is not much impact in 
changing the pH. However, 
when FRWater is high there 
is a much greater impact in 
changing the pH.
The Main Effect plots show the relative strength of the change caused by each of the factors. Note that 
HofPacking has a negative effect.

452
Problem Solving and Data Analysis using Minitab 
Exercise 2. The Reactor.
Engineers want to understand the operation of 
their reactor which they are using to convert 
hexane into perﬂuorohexane. They want to 
know what factors are important and how they 
affect the yield of the product, perﬂuorohex-
ane, leaving the reactor. 
Fluorine gas is fed into a heated reactor. The 
reactor contains a bed of cobalt tri-ﬂuoride 
(CoF3). The CoF3 captures the ﬂuorine gas until 
it can react with the heated hexane feed. The 
hexane is converted to perﬂuorohexane only if 
all of the hydrogen molecules within hexane 
are replaced with ﬂuorine atoms. The percent-
age of fully ﬂuorinated perﬂuorohexane in the 
product stream speciﬁes the yield.
The reactor is kept at a very slight negative 
pressure so that unreacted gases can be drawn 
into the gas scrubbing system.
The factors that the engineers considered to be 
important are:
Particle Size of CoF3, Partsize (80, 120)
Reactor Temperature, RTemp (300, 400)
Reactor Pressure, RPress (–10, –2) 
Hexane Feed Temperature, HexTemp (30, 60)
Feedrate of Fluorine, FRF2 (30, 45) 
Feedrate of Hexane, FRHex(12,18) 
The engineers carried out a half fractional factorial DOE. Analyse the experiment and decide which of 
the terms are signiﬁcant.
Produce the Main Effects plots and Interaction plots if applicable.
Worksheet: Reactor.

Design of Experiment 453
Run 1.
Run 2. All third
order
interactions are
removed.
Run 3. All second order
interactions are
removed, except BF.
Run 4. Main
Effect C is
removed.

454
Problem Solving and Data Analysis using Minitab 
t Residuals are normally distributed on the normal probability plot.
t Residuals are equally distributed about the centre line on the versus ﬁts plot.
t Histogram of residuals does not show extreme skewness.
t If the original data was collected in time order then the residuals should not show any ordered 
behaviour on the versus order plot.
There are no issues with the Residuals.

Design of Experiment
455
The Main Effects plot shows 
the relative strength of the 
change caused by each of the 
factors.
Although the interaction is signiﬁcant it can be seen that the lines appear to be being close to parallel.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
CHAPTER 11
Help
11.1 Help Overview
The ﬁnal task of this course is to leave you with a method of helping yourself should you get stuck and 
also a method for you to increase your knowledge and understanding of Minitab. We are going to do 
that by showing you the Help system.
The Help system within Minitab offers a comprehensive library of information on Minitab. It includes 
information on the data you need to run a procedure, how to execute the procedure and how to 
interpret the results. It includes a glossary and a section to show you how calculations are carried out. 
In fact you will probably be hard pushed to ask a question that is not covered within the Help system. 
Therefore, we need to learn how to get to the answer.
Initially, navigation within the Help system may seem a bit confusing as there are multiple ways of get-
ting to the same point. In fact, there are ways of going round in an endless loop as well. I will show you 
what I think are the straightforward routes.
I made the decision to use an independent data set in this course. This does mean that you have to 
download the data but the advantage of having an independent data set within Minitab is enormous. 
This allows the reader to run the examples again whilst using the Minitab help system as the guide. This 
will allow the reader to see the procedures from another perspective and effectively give them two ways 
of learning.
Clicking on the Help within the Toolbar will open the Help Drop Down menu as shown below.
Help: The main menu can be used to navigate to most of the other 
help systems but this is also the main route to get help into all 
things Minitab. 
Meet Minitab: This option takes you to the website and allows you 
to download the Meet Minitab document, or you may have received 
one with your copy of Minitab.
StatGuide: This option is another help system that focuses on inter-
pretation of test results.
Tutorials: Think of these tutorials as the front end of the procedures 
that are used within the StatGuide.

Help
457
Glossary: Explanation of terms used within Minitab.
Methods and Formulas: This option explains the maths that Minitab uses behind the scenes. 
Knowledgebase/FAQ: This option takes the user onto the Minitab website for web based help.
11.2 Help! Help!
In order to get to the main help menu click Help <<Help. Or press F1 or click on the Help icon in the tool 
bar.
In the middle section of the Help menu you can see some of the previous options that we discussed as 
being within the drop down menu.
In order to show how to navigate using the help menu we will pretend that we wish to know more 
about One Way ANOVA .

458
Problem Solving and Data Analysis using Minitab 
The diagram shows three ways to navigate to the help 
section on One Way ANOVA. Two of the ways are from 
the main help menu. You can use the menu boxes on the 
right hand side of the help screen or either of the three 
search systems on the left hand side: Contents, Index or 
Search. The third way of getting to the help page is via 
clicking the help button on the menu box for One-Way 
ANOVA (all menu boxes have the help button in the bot-
tom left corner). Let’s have a look at the help page in 
greater detail.

Help
459
The subject speciﬁc help sections are usually 
further broken down into ﬁve subcategories. 
We will go through these.
The Navigation bar in the top left corner is 
always available to move back and forward 
through the different levels of help.
Overview gives a summary of the procedure. 
Key terms are explained in dialog boxes if the 
users clicks on the link or they are taken to 
different help topics.
How to is a very brief memory jogger on how 
to carry out the analysis.

460
Problem Solving and Data Analysis using Minitab 
Example is a fully worked example with a prob-
lem scenario, brief explanation of the procedure 
and then the results with full interpretation. It 
is important to note that the data set is given in 
EH_AOV.MTW for this example. We will look at 
how to retrieve these later in the module.
Data tells us the data requirements for the test. 
How data should be laid out and the type of 
data required.
See Also is a collection of links to related topics.

Help 461
11.3 Tutorials
Tutorials and StatGuide work together to give another set of worked examples. This set is separate from 
those given within the Help topic speciﬁc menus. The Tutorial section deals with the front end of the 
procedure up to execution. However, StatGuide deals only with interpretation. The tutorials section in 
M15 looks different to that for M16. The M16 section is shown below.
To access the tutorial click <<Help 
<<Tutorials. Alternatively, click 
on Tutorials from the Main Help 
screen.
Navigate to the One Way ANOVA 
Tutorial by clicking on the con-
tents tab and then select ANOVA 
<<One Way ANOVA.
The Tutorials have three main sections which are accessed via three discrete looking tabs. The discrete 
tabs are Uses, Data and How To. The Back and Forward navigation buttons are also available.
Uses tells us how the procedure can be applied 
and then gives an example scenario.
or

462
Problem Solving and Data Analysis using Minitab 
Data gives us the data type requirements 
for the procedure and it also gives us a 
graphical display of how the data should 
be laid out.
How To is a detailed explanation for setting up the test procedure. However, as stated earlier it only 
goes up to the point of execution. The interpretation of results is given within the StatGuide and there 
is a link to the StatGuide at the bottom of the How To section. 
Again, to allow you to work through the example Minitab tells you that the data set is PAINT.MTW. 
This is a different ﬁle to that given within the Help menus. As stated earlier, we will look at getting the 
example data sets later in the module. We will now have a look at the StatGuide.

Help 463
11.4 StatGuide
The main ways of getting to 
the StatGuide for One Way 
ANOVA are to click Help 
<<StatGuide or the StatGuide 
icon and then ﬁnd the rel-
evant topic from any of the 
tabs. The other is to get there 
via the relevant Tutorial.
The StatGuide gives a very 
detailed explanation of the 
test results. Initially, it begins 
with a summary of the proce-
dure but then it goes through 
the interpretation of the 
results for the example started 
within the Tutorial. For One 
Way ANOVA it continues with 
the PAINT.MTW example.
or

464 Problem Solving and Data Analysis using Minitab 
On the top right corner of the StatGuide 
window are the navigation arrows to 
move through different sections of the 
results. If you click on Topic a dialog box 
opens showing the breakdown of the 
procedure explanation. The window 
in the background is Graph Residuals 
versus Fits. Clicking the left arrow will 
make the section go back one stage 
onto Graphs Normal Probability Plot of 
Residuals.
It is also possible to use the Contents tab 
to open all the topics and select them by 
clicking on the section required. 
11.5 Methods and Formulas
To navigate to Methods and Formulas 
click on Help <<Method and Formulas 
or select it from the main help menu.
or

Help
465
In order to then Navigate to the Methods and Formulas for One Way 
ANOVA click on the links as shown. Again, it is a very logical way of 
navigating to the required procedure.
All of the Methods and Formulas for One Way ANOVA are shown under subheadings.
A single click on one of the links will open the requested information in the window below the head-
ings. We have opened the formula for calculation R-Sq(adj).

466
Problem Solving and Data Analysis using Minitab 
11.6 Meet Minitab
Meet Minitab is a booklet that 
helps the new user get started 
with Minitab. You may have got 
a copy with the Disc of Minitab 
or you can download the latest 
copy from the Minitab website 
by clicking Help <<Meet Minitab 
(on an internet enabled PC). 
This link will take you straight 
to the download page for Meet 
Minitab. Meet Minitab also con-
tains fully worked examples and 
graphics. The folder of data sets 
for Meet Minitab will be with 
the Minitab sample data ﬁles.
11.7 Help on the Web
Minitab also offers exten-
sive support on the inter-
net. This is mainly available 
on their website at www.
Minitab.com . This can be 
accessed directly with a 
web browser or by clicking 
Help << Knowledgebase/
FAQ.
Minitab is also has a 
presence on Twitter and 
Facebook in case you were 
interested. We are going 
to stick with the help on 
their website for now.
Navigate to the 
Knowledgebase and 
enter One Way ANOVA 
into the Search Facility 
and click on Search.
or
WWW.Minitab.com
Home
Support
Knowledgebase/FAQ

Help
467
The Knowledgebase brings up a list of documents and FAQs relating to my query. 
If I feel a document is relevant I can click on the link in the header to open it. ID 1267 has been opened 
to show you a part of the solution. 
If you register with Minitab you can also send questions to the Help Team which they will answer 
promptly. 

468
Problem Solving and Data Analysis using Minitab 
I want to show a couple of the more interesting FAQs. This one is ID-2613 and contains links to the 
Minitab White Papers for the Assistant. The White Papers contain the results of research conducted by 
Minitab into the Methods used by the Assistant and what conditions are required to validate the tests. 
They contain interesting background reading.
11.8 Help on the Web and Datasets
This one is ID-2241. It contains useful information about the Minitab data sets. It also contains a ﬁle that 
is worth downloading. The ﬁle contains the names of the example data sets used within the StatGuide 
and Help system. A section of the spreadsheet is shown below. You can see the ﬁle names relating to 
One Way ANOVA.

Help
469
11.9 Datasets
To open one of the example datasets 
click on File <<Open Worksheet. You can 
then either click on the Look in Minitab 
Sample Folder button or you can manu-
ally navigate to where the sample data 
ﬁles are stored. 
I have shown the location where my sam-
ple data ﬁles are stored and the location 
where the Meet Minitab data ﬁles are 
stored. However, depending on how you 
installed Minitab you may have a differ-
ent location.
This ends the chapter and the book. 
I hope I have completed my task and 
opened the world of statistical problem 
solving using Minitab for you.

Problem Solving and Data Analysis using Minitab: A clear and easy guide to Six Sigma methodology, First Edition. Rehman M. Khan.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
1 Sample t test
81–105
2 sample t test
130–49
2 variance test
118–29
Accuracy of Measurement Systems
211–12
Alternate hypothesis
72–3
Analysis of variance (ANOVA)
150
general linear model (GLM)
192–208
one-way
152–63
one-way using the Assistant
164–91
principle
150–2
Anderson–Darling (AD) Normality Test
65
Assistant
1 sample t test
87–92
2 sample t test
140–9
dropdown menu
13
graphs
44–6
one-way ANOVA
164–91
paired t test
110–18
process capability
340–3
single predictor regression
363–7
Assumptions & Limitations
2 sample t test
130
2 variance test
118
ANOVA GLM
192
best subsets
373
capability analysis (Normal)
319
capability comparison (Assistant)
340
capability sixpack (Normal)
319
correlation
346
fraction factorial designs
442
Gage bias and linearity study
255
Gage R&R (Crossed)
221
Gage R&R (Nested)
247
I-MR chart (Assistant)
270
I-MR chart (Classic)
269
I-MR-R/S chart
310
nonlinear regression
400
one way ANOVA (Assistant)
164
one way ANOVA (classic)
152
paired t test
105
power and sample size for 1 sample t test
76
Regression
373
single predictor regression
355
stepwise regression
373
two level factorial designs
412
type 1 Gage Study
214
Xbar-R chart
291
Xbar-S chart
299
Bar charts
40–2
Bartlett’s Test
158
Between group variation
151–2
Bias errors
212
Bimodal distribution
64
Boxplots
34–40
Brush tool
32–3
Calc dropdown menu
9
Calculator
18–20
Central Limit Theorem (CLT)
70
Central location
50–1
Characterisation DOE
407
Column formats
15–17
Common cause variation
262–3
Conﬁdence factor
60
Conﬁdence intervals
60–2
Correlation
346–9
deﬁnition
344
multiple
349–54
Data
importing
13–15
types
50
Data dropdown menu
9
Descriptive statistics
52–9
Design of experiment (DOE)
407
fractional factorial designs
439–55
Index

472
Index
Design of experiment (DOE) (continued)
terminology
408–12
two level factorial designs
412–39
types
407
Deviation
51–2
Differences between Minitab 15 (M15) and Minitab 16 
(M16)
7, 12, 44, 118, 175, 219
Discrimination
212
Dispersion
51–2
Dotplots
28–31
Dropdown menus see menus 
Dunnet’s comparison method
178
Edit dropdown menu
8
Editor dropdown menu
11
F statistic
152
F test
121
File dropdown menu
8
Fisher’s comparison method
177
Formatting columns
15–17
Gage Bias and Linearity Study
255–260
Gaussian (Normal) Distribution
62–3
deviations
63–70
Graph dropdown menu
10
Graphs
adding detail
25–7
bar charts
40–2
basic
20–5
boxplots
34–40
dotplots
28–31
layout tools
42–3
saving
27
using the Assistant
44–6
Help dropdown menu
12
Help system
accessing
457–60
datasets
478–9
meet Minitab
466
methods and Formulas
464–5
overview
456–7
StatGuide
463–4
tutorials
461–2
web-based
466–8
Hsu’s MCB comparison method
178
Hypothesis testing
71–2
1 sample t test
81–105
2 sample t test
130–49
2 variance test
118–29
conducting tests and evaluating results
80–1
null and alternate hypotheses
72–3
paired t test
105–18
power and sample size testing
76–80
problem statement
72
risks
73–5
Importing data
13–15
I-MR chart
269–91
I-MR-R/S chart
310–12
Inferential statistics
59
Kurtosis
64
Layout tools
42–3
Levene’s Test
158
Linearity errors
212
Mean
50–1
Measurement system analysis (MSA)
analysing appropriate systems
210
effect on data
209–10
error types
211–13
gage R&R crossed
221–46
gage R&R nested
247–54
gage R&R study worksheet
219–20
gage repeatability and reproducibility 
study
217–19
importance of
209
toolbox
213
type 1 gage study
214–17
Median
51
Meet Minitab
466
Mode
51
Multiple correlation
349–54
Multiple predictor regression
372–95
Navigation
menus
8–13
windows
6–7
New Project/Worksheet
49–50
Nonlinear regression
400–6
Normal (Gaussian) Distribution
62–3
deviations
63–70
Normal Probability Plot
65
Null hypothesis
72–3
One-way ANOVA
152–63
using Assistant
164–91
Overall capability
318
Overall variation
152
P value
152
Paired t test
105–18
Power and sample size procedures
1 sample t test
76
2 Level factorial designs
412
2 sample t test
131

Index
473
2 variance test
119
one way ANOVA (classic)
153, 170
paired t test
107
Precision of Measurement Systems
211–12
Problem statement
72
Process capability
313–17
comparison using the Assistant
340–3
non-normal data
329–40
normal data
319–29
short term and overall capability
318
Producing reports
46–8
Range
51
Regression
354–5
deﬁnition
345
multiple predictor
372–95
nonlinear
400–6
predictor selection
396–9
single predictor
355–72
Repeatability errors
211
Reproducibility errors
211
Resolution
212
Response surface designs
407
Risks
73–5
Screening DOE
407
Short term capability
318
Single predictor regression
355–72
Special cause variation
262–3
detection rules
263–6
Stability errors
212
Stat dropdown menu
10
StatGuide
463–4
Statistical process control (SPC)
appropriate chart
268–9
common cause and special cause variation
262–3
exercise
307–9
false alarms
266–7
I-MR chart
269–91
I-MR-R/S chart
310–12
origins
261
special cause detection rules
263–6
subgrouping
268
Xbar-R chart
291–9
Xbar-S chart
299–307
Statistics
Central Limit Theorem (CLT)
70
central location
50–1
conﬁdence intervals
60–2
data types
50
descriptive statistics
52–9
deviation from normality
63–70
dispersion
51–2
inferential statistics
59
Normal (Gaussian) Distribution
62–3
Taguchi designs
407
Tolerance
213
Tools dropdown menu
11
Tukey’s comparison method
176–7
Two level full factorial DOE
408–12
Voice of the Customer (VOC)
313, 314
Voice of the Process (VOP)
313, 314
Window dropdown menu
12
Windows
6–7
Within group variation
151–2
Xbar-R chart
291–9
Xbar-S chart
299–307


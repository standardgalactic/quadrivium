C H A P T E R  9
Induction
9.1 Introduction
Induction is reasoning from the specific to the general. If various instances of a schema are true
and there are no counterexamples, we are tempted to conclude a universally quantified version of
the schema.
p(a) ⇒ q(a)
p(b) ⇒ q(b)
→
∀x.p(x) ⇒ q(x)
p(c) ⇒ q(c)
Incomplete induction is induction where the set of instances is not exhaustive. From a reasonable
collection of instances, we sometimes leap to the conclusion that a schema is always true even
though we have not seen all instances. Consider, for example, the function f where f(1)=1, and
f(n+1)=f(n) + 2n + 1. If we look at some values of this function, we notice a certain regularity - the
value of f always seems to be the square of its input. From this sample, we are tempted to leap to
the conclusion that f(n)=n2. Lucky guess. In this case, the conclusion happens to be true; and we
can prove it.
n
f(n)
n2
1
1
1
2
4
22
3
9
32
4
16
42
5
25
52
Here is another example. This one is due to the mathematician Fermat (1601-1665). He looked at
values of the expression 22n + 1 for various values of n and noticed that they were all prime. So, he
concluded the value of the expression was prime number. Unfortunately, this was not a lucky
guess. His conjecture was ultimately disproved, in fact with the very next number in the sequence.
n
22n+1
Prime?
1
5
Yes
17
4
Yes
257
9
Yes
65537
16
Yes
For us, this is not so good. In Logic, we are concerned with logical entailment. We want to derive

only conclusions that are guaranteed to be true when the premises are true. Guesses like these are
useful in suggesting possible conclusions, but they are not themselves proofs. In order to be sure
of universally quantified conclusions, we must be sure that all instances are true. This is called
complete induction. When applied to numbers, it is usually called mathematical induction.
The techniques for complete induction vary with the structure of the language to which it is
applied. We begin this chapter with a discussion of domain closure, a rule that applies when the
Herbrand base of our language is finite. We then move on to linear induction, i.e. the special case
in which the ground terms in the language form a linear sequance. After that, we look at tree
induction, i.e. the special case in which the ground terms of the language form a tree. Finally, we
look at structural induction, which applies to all languages.
9.2 Finite Induction
First let's consider the case for finite languages. Induction for finite languages is simple. If we
believe a schema is true for every instance, then we can infer a universally quantified version of
that schema. In other words, we have the following rule of inference, called Finite Domain
Closure.
For a language with object constants σ1, ... , σn, the rule is written as shown below. If we believe a
schema is true for every instance, then we can infer a universally quantified version of that
schema.
φ[σ1]
   ...
φ[σn]
∀ν.φ[ν]
Recall that, in our formalization of the Sorority World, we have just four constants - abby, bess,
cody, and dana. For this language, we would have the following Finite Domain Closure rule.
Finite Domain Closure (DC)
φ[abby]
φ[bess]
φ[cody]
φ[dana]
∀ν.φ[ν]
The following proof shows how we can use this rule to derive an inductive conclusion. Given the
premises we considered earlier in this book, it is possible to infer that Abby likes someone, Bess
likes someone, Cody likes someone, and Dana likes someone. Taking these conclusions as
premises and using our Finite Domain Closure rule, we can then derive the inductive conclusion
∀x.∃y.likes(x,y), i.e. everybody likes somebody.

1. ∃y.likes(abby,y)  Premise
2. ∃y.likes(bess,y)  Premise
3. ∃y.likes(cody,y)  Premise
4. ∃y.likes(dana,y)  Premise
5. ∀x.∃y.likes(x,y)  DC: 1, 2, 3, 4
Unfortunately, this technique does not work when there are infinitely many ground terms.
Suppose, for example, we have a language with ground terms σ1, σ2, ... A direct generalization of
the Finite Domain Closure rule is shown below.
φ[σ1]
φ[σ2]
   ...
∀ν.φ[ν]
This rule is sound in the sense that the conclusion of the rule is logically entailed by the premises
of the rule. However, it does not help us produce a proof of this conclusion. To use the rule, we
need to prove all of the rule's premises. Unfortunately, there are infinitely many premises. So, the
rule cannot be used in generating a finite proof.
All is not lost. It is possible to write rules that cover all instances without enumerating them
individually. The method depends on the structure of the language. The next sections describe how
this can be done for languages with different structures.
9.3 Linear Induction
Consider a language with a single object constant a and a single unary function constant s. There
are infinitely many ground terms in this language, arranged in what resembles a straight line. See
below. Starting from the object constant, we move to a term in which we apply the function
constant to that constant, then to a term in which we apply the function constant to that term, and
so forth.
a → s(a) → s(s(a)) → s(s(s(a))) → ...
In this section, we concentrate on languages that have linear structure of this sort. Hereafter, we
we call these linear languages. In all cases, there is a single object constant and a single unary
function constant. In talking about a linear language, we call the object constant the base element
of the language, and we call the unary function constant the successor function.
Although there are infinitely many ground terms in any linear language, we can still generate finite
proofs that are guaranteed to be correct. The trick is to use the structure of the terms in the
language in expressing the premises of an inductive rule of inference known as linear induction.
See below for a statement of the induction rule for the language introduced above. In general, if
we know that a schema holds of our base element and if we know that, whenever the schema holds
of an element, it also holds of the successor of that element, then we can conclude that the schema
holds of all elements.

Linear Induction
φ[a]
∀μ.(φ[μ] ⇒ φ[s(μ)]
∀ν.φ[ν]
A bit of terminology before we go on. The first premise in this rule is called the base case of the
induction, because it refers to the base element of the language. The second premise is called the
inductive case. The antecedent of the inductive case is called the inductive hypothesis, and the
consequent is called, not surprisingly, the inductive conclusion. The conclusion of the rule is
sometimes called the overall conclusion to distinguish it from the inductive conclusion.
For the language introduced above, our rule of inference is sound. Suppose we know that a schema
is true of a and suppose that we know that, whenever the schema is true of an arbitrary ground
term τ, it is also true of the term s(τ). Then, the schema must be true of everything, since there are
no other terms in the language.
The requirement that the signature consists of no other object constants or function constants is
crucial. If this were not the case, say there were another object constant b, then we would have
trouble. It would still be true that φ holds for every element in the set {a, s(a), s(s(a)),...}.
However, because there are other elements in the Herbrand universe, e.g. b and s(b), ∀x.φ(x)
would no longer be guaranteed.
In order to understand the intuition behind linear induction, imagine an infinite set of dominoes
placed in a line so that, when one falls, the next domino in the line also falls. If the first domino
falls, then the second domino falls. If the second domino falls, then the third domino falls. And so
forth. By continuing this chain of reasoning, it is easy for us to convince ourselves that every
domino eventually falls.
Now, let's formalize this example and prove this result using induction. Let a represent the first
domino. And let s represent the relationship between a domino and the next domino in line. Let's
use the unary relation constant falls to express the fact that the domino given as argument
eventually falls.
A one-step proof of our conclusion that all of the dominoes eventually fall is shown below. We
start with the premise that the first domino in the line falls. We also have the premise that the
dominoes are so placed so that each domino knocks over the next domino. These two sentences
match the premises of the linear induction rule, and so we can derive the conclusion.
1.  falls(a)
  Premise
2.  ∀x.(falls(x) ⇒ falls(s(x))  Premise
3.  ∀x.(falls(x))
  Ind: 1, 2
Here is a more interesting example. Recall the formalization of Arithmetic introduced in Chapter
6. Using the object constant 0 and the unary function constant s, we represent each number n by
applying the function constant to 0 exactly n times. We also have a single ternary relation constant
plus to represent the addition table.
The following axioms describe plus in terms of 0 and s. The first sentence here says that adding 0

to any element produces that element. The second sentence states that adding the successor of a
number to another number yields the successor of their sum.
∀y.plus(0,y,y)
∀x.∀y.∀z.(plus(x,y,z) ⇒plus(s(x),y,s(z)))
It is easy to see that any table that satisfies these axioms includes all of the usual addition facts.
The first axioms ensures that all cases with 0 as first arg are included. From this fact and the
second axiom, we can see that all cases with s(0) as first argument are included. And so forth. In
order for our description to be a complete definition, we would also need an axiom limiting the
value of plus (its third argument) to a single value for each pair on inputs (its first two arguments).
However, we do not need that axiom for the purposes of this example; and so, for the sake of
simplicity, we skip it.
The first axiom above tells us that 0 is a left identity for addition - 0 added to any number produces
that number as result. As it turns out, given these definitions, 0 must also be a right identity, i.e it
must be the case that ∀x.plus(x,0,x).
We can use induction to prove this result as shown below. We start with our premises. We use
Universal Elimination on the first premise to derive the sentence on line 3. This takes care of the
base case of our induction. We then start a subproof and assume the antecedent of the inductive
case. We then use three applications of Universal Elimination on the second premise to get the
sentence on line 5. We use Implication Elimination on this sentence and our assumption to derive
the conclusion on line 6. We then discharge our assumption and form the implication shown on
line 7 and then universalize this to get the result on line 8. Finally, we use linear induction to
derive our overall conclusion.
1.  ∀y.plus(0,y,y)
  Premise
2.  ∀x.∀y.∀z.(plus(x,y,z) ⇒plus(s(x),y,s(z)))  Premise
3.  plus(0,0,0)
  UE: 1
4.  
 plus(x,0,x)
  Assumption
5.  
 plus(x,0,x) ⇒plus(s(x),0,s(x))
  3 x UE: 2
6.  
 plus(s(x),0,s(x))
  IE: 5, 4
7.  plus(x,0,x) ⇒ plus(s(x),0,s(x))
  II: 4, 6
8.  ∀x.(plus(x,0,x) ⇒ plus(s(x),0,s(x)))
  UI: 7
9.  ∀x.plus(x,0,x)
  Ind: 3, 8
9.4 Tree Induction
Tree languages are a superset of linear languages. While, in linear languages, the terms in the
language form a linear sequence, in tree languages the structure is more tree-like. Consider a
language with an object constant a and two unary function constants f and g. Some of the terms in
this language are shown below.

a
↙
↘
f(a)
g(a)
↙
↘
↙
↘
f(f(a))
g(f(a))
f(g(a))
g(g(a))
As with linear languages, we can write an inductive rule of inference for tree languages. The tree
induction rule of inference for the language just described is shown below. Suppose we know that
a schema φ holds of a. Suppose we know that, whenever the schema holds of any element, it holds
of the term formed by applying f to that element. And suppose we know that, whenever the
schema holds of any element, it holds of the term formed by applying g to that element. Then, we
can conclude that the schema holds of every element.
Tree Induction
φ[a]
∀μ.(φ[μ] ⇒ φ[f(μ)])
∀μ.(φ[μ] ⇒ φ[g(μ)])
∀ν.φ[ν]
In order to see an example of tree induction in action, consider the ancestry tree for a particular
dog. We use the object constant rex to refer to the dog; we use the unary function constant f map
an arbitrary dog to its father; and we use g map a dog to its mother. Finally, we have a single
unary relation constant purebred that is true of a dog if and only if it is purebred.
Now, we write down the fundamental rule of dog breeding - we say that a dog is purebred if and
only if both its father and its mother are purebred. See below. (This is a bit oversimplified on
several grounds. Properly, the father and mother should be of the same breed. Also, this
formalization suggests that every dog has an ancestry tree that stretches back without end.
However, let's ignore these imperfections for the purposes of our example.)
∀x.(purebred(x) ⇔ purebred(f(x)) ∧ purebred(g(x)))
Suppose now that we discover the fact that our dog rex is purebred. Then, we know that every dog
in his ancestry tree must be purebred. We can prove this by a simple application of tree induction.
A proof of our conclusion is shown below. We start with the premise that Rex is purebred. We
also have our constraint on purebred animals as a premise. We use Universal Elimination to
instantiate the second premise, and then we use Biconditional Elimination on the biconditional in
line 3 to produce the implication on line 4. On line 5, we start a subproof with the assumption the x
is purebred. We use Implication Elimination to derive the conjunction on line 6, and then we use
And Elimination to pick out the first conjunct. We then use Implication Introduction to discharge
our assumption, and we Universal Introduction to produce the inductive case for f. We then repeat
this process to produce an analogous result for g on line 14. Finally, we use the tree induction rule
on the sentences on lines 1, 9, and 14 and thereby derive the desired overall conclusion.

1.   purebred(rex)
  Premise
2.   ∀x.(purebred(x) ⇔ purebred(f(x)) ∧ purebred(g(x)))  Premise
3.   purebred(x) ⇔ purebred(f(x)) ∧ purebred(g(x))
  UE: 2
4.   purebred(x) ⇒ purebred(f(x)) ∧ purebred(g(x))
  BE: 3
5.   
 purebred(x)
  Assumption
6.   
 purebred(f(x)) ∧ purebred(g(x))
  IE: 4, 6
7.   
 purebred(f(x))
  AE: 6
8.   purebred(x) ⇒ purebred(f(x))
  II: 5, 7
9.   ∀x.purebred(x) ⇒ purebred(f(x))
  UI: 8
10.  
 purebred(x)
  Assumption
11.  
 purebred(f(x)) ∧ purebred(g(x))
  IE: 4, 6
12.  
 purebred(g(x))
  AE: 7
13.  purebred(x) ⇒ purebred(g(x))
  II: 6, 8
14.  ∀x.purebred(x) ⇒ purebred(g(x))
  UI: 9
15.  ∀x.purebred(x)
  Ind: 1, 9, 14
9.5 Structural Induction
Structural induction is the most general form of induction. In structural induction, we can have
multiple object constants, multiple function constants, and, unlike our other forms of induction, we
can have function constants with multiple arguments. Consider a language with two object
constants a and b and a single binary function constant c. See below for a list of some of the terms
in the language. We do not provide a graphical rendering in this case, as the structure is more
complicated than a line or a tree.
a, b, c(a,a), c(a,b), c(b,a), c(b,b), c(a,c(a,a)), c(c(a,a),a), c(c(a,a),c(a,a)), ...
The structural induction rule for this language is shown below. If we know that φ holds of our
base elements a and b and if we know ∀μ.∀ν.((φ[μ] ∧ φ[ν]) ⇒ φ[c(μ,ν)]), then we can conclude
∀ν.φ[ν] in a single step.
Structural Induction
φ[a]
φ[b]
∀λ.∀μ.((φ[λ] ∧ φ[μ]) ⇒ φ[c(λ,μ)])
∀ν.φ[ν]
In order to visualize a world where structural induction is appropriate, let's transport ourselves to a
primitive chemistry laboratory. In the lab, we have two liquids, and we have enough equipment to

combine the liquids in various ways, to combine the combinations with each other, and so forth.
Back in Logic land, our goal is to prove things about the acidity of the combinations we can create
in the laboratory.
Let's use the object constant a to represent one liquid, and let's use b to represent the second liquid.
Let the binary function constant c represent the result of combining two liquids designated by its
arguments. Finally, let's use the unary relation constant acid to express the fact that a liquid is an
acid; let's use the unary relation constant neut express the fact that a liquid is neutral; let's use the
unary relation constant base express the fact that a liquid is a base.
Now for our premises. We know that mixing two acids yields an acid. We know that mixing an
acid and a neutral liquid yields an acid, and vice versa. We know that mixing two neutral liquids
yields a neutral result. We know that mixing a base and a neutral liquid yields a base, and vice
versa. We know that mixing two bases yields a base. It is unclear what happens when we mix
acids and bases; it depends on their concentrations.
∀x.∀y.(acid(x) ∧ acid(y) ⇒ acid(c(x,y))
∀x.∀y.(acid(x) ∧ neut(y) ⇒ acid(c(x,y))
∀x.∀y.(neut(x) ∧ acid(y) ⇒ acid(c(x,y))
∀x.∀y.(neut(x) ∧ neut(y) ⇒ neut(c(x,y))
∀x.∀y.(base(x) ∧ base(y) ⇒ base(c(x,y))
∀x.∀y.(base(x) ∧ neut(y) ⇒ base(c(x,y))
∀x.∀y.(neut(x) ∧ base(y) ⇒ base(c(x,y))
As a simple example of structural induction in action, let's prove that, if both of our liquids are
acids, then every combination is an acid as well. In other words, given acid(a) and acid(b), we
want to prove the conclusion ∀x.acid(x).
The proof in this case is trivial. We have our specific facts about a and b, and we have our general
fact about the combination of two acids. These three items match the premises of the structural
induction rule, and so we can derive our conclusion in one step.
1.  acid(a)
  Premise
2.  acid(b)
  Premise
3.  ∀x.∀y.(acid(x) ∧ acid(y) ⇒ acid(c(x,y))  Premise
2.  ∀x.acid(x)
  Ind: 1, 2, 3
Now, let's see what happens when one of our liquids is an acid and the other is a neutral liquid.
Clearly, we can now have pure acids or pure neutral liquids or any mix of the two. The result is
guaranteed to be either an acid or neutral, but that's all we can say. In other words, if we know that
acid(a) and neut(b), then the best we can do is to prove ∀x.(acid(x) ∨ neut(x)).
As with our last example, we can prove this result using structural induction. The proof in this
case is a lot messier, but it is worth seeing as it suggests the sort of complexity that is involved in
inductive proofs with disjunctive conclusions. Because the proof is so messy, we show it in
segments. As usual, we start with our premises. Note that we could include all of our facts about
combinations; but, since we do not have any bases, we can omit our general facts about

combinations involving bases.
1. acid(a)
Premise
2. neut(b)
Premise
3. ∀x.∀y.(acid(x) ∧ acid(y) ⇒ acid(c(x,y))
Premise
4. ∀x.∀y.(acid(x) ∧ neut(y) ⇒ acid(c(x,y))
Premise
5. ∀x.∀y.(neut(x) ∧ acid(y) ⇒ acid(c(x,y))
Premise
6. ∀x.∀y.(neut(x) ∧ neut(y) ⇒ neut(c(x,y))
Premise
As before, we deal with the base cases first. We use Or Introduction to disjoin our first two
premises with other sentences so that the resulting disjunctions match the first two premises of
structural induction for our desired result.
7. acid(a) ∨ neut(a)
OI: 1
8. acid(b) ∨ neut(b)
OI: 2
Now, we prove ∀x.∀y.((acid(x) ∨ neut(x)) ∧ (acid(y) ∨ neut(y)) ⇒ acid(c(x,y)) ∨ neut(c(x,y))). To
this end, we start a new proof and assume the inductive hypothesis, and we use And Elimination to
split the conjunction into its components.
9.
 (acid(x) ∨ neut(x)) ∧ (acid(y) ∨ neut(y))
Assumption
10.
 acid(x) ∨ neut(x)
AE: 9
11.
 acid(y) ∨ neut(y)
AE: 9
In order to prove the inductive case, using our assumption, we need to show the intermediate
conclusion. acid(c(x,y)) ∨ neut(c(x,y)). Our plan for doing this to show that this conclusion follows
from acid(x) and that it follows from neut(x). Then we can use Or Elimination on the disjunction
in line 10 to derive the inductive conclusion.
In the segment below, we see the proof for the first of these two cases. We assume acid(x); and,
using that assumption, we derive acid(c(x,y)) ∨ neut(c(x,y)). We then use Implication Introduction
to derive the first result we need for our application of Or Elimination.

12.
 acid(x)
Assumption
13.
 acid(y)
Assumption
14.
 acid(x) ∧ acid(y)
AI: 12, 13
15.
 acid(x) ∧ acid(y) ⇒ acid(c(x,y))
2 x UE: 3
16.
 acid(c(x,y))
IE: 15, 14
17.
 acid(y) ⇒ acid(c(x,y))
II: 13, 16
18.
 neut(y)
Assumption
19.
 acid(x) ∧ neut(y)
AI: 12, 18
20.
 acid(x) ∧ neut(y) ⇒ acid(c(x,y))
2 x UE: 4
21.
 acid(c(x,y))
IE: 20, 19
22.
 neut(y) ⇒ acid(c(x,y))
II: 18, 20
23.
 acid(c(x,y))
OE:11,17,22
24.
 acid(c(x,y)) ∨ neut(c(x,y))
OI: 23
25.
 acid(x) ⇒ acid(c(x,y)) ∨ neut(c(x,y))
II: 12, 24
In the next segment below, we see the proof for the second of the cases needed for Or Elmination.
We assume neut(x); and, using that assumption, we derive acid(c(x,y)) ∨ neut(c(x,y)). We again
use Implication Introduction to derive the necessary result.

26.
 neut(x)
Assumption
27.
 acid(y)
Assumption
28.
 neut(x) ∧ acid(y)
AI: 26, 27
29.
 neut(x) ∧ acid(y) ⇒ acid(c(x,y))
2 x UE: 5
30.
 acid(c(x,y))
IE: 29, 28
31.
 acid(c(x,y)) ∨ neut(c(x,y))
OI: 30
32.
 acid(y) ⇒ acid(c(x,y)) ∨ neut(c(x,y))
II: 27, 31
33.
 neut(y)
Assumption
34.
 neut(x) ∧ neut(y)
AI: 26, 33
35.
 neut(x) ∧ neut(y) ⇒ neut(c(x,y))
2 x UE: 6
36.
 neut(c(x,y))
IE: 35, 34
37.
 acid(c(x,y)) ∨ neut(c(x,y))
OI: 36
38.
 neut(y) ⇒ acid(c(x,y)) ∨ neut(c(x,y))
II: 33, 36
39.
 acid(c(x,y)) ∨ neut(c(x,y))
OE:11,32,38
40.
 neut(x) ⇒ acid(c(x,y)) ∨ neut(c(x,y))
II: 26, 39
Now, we have the pieces we need - the disjunction on line 10 and the implications on lines 25 and
40. Using OE, we derive the result on line 42, and then we use UI to universalize this result.
41.
 acid(c(x,y)) ∨ neut(c(x,y))
OE:10,25,40
42.(acid(x) ∨ neut(x)) ∧ (acid(y) ∨ neut(y)) ⇒ acid(c(x,y)) ∨ neut(c(x,y))
II: 9, 41
43.∀x.∀y.((acid(x) ∨ neut(x)) ∧ (acid(y) ∨ neut(y)) ⇒ acid(c(x,y)) ∨ neut(c(x,y)))2 x UI: 42
Finally, we put all of the pieces together - the two base cases and this inductive case - and we
apply the structural induction rule to produce the desired conclusion.
44. ∀x.(acid(x) ∨ neut(x))
Ind: 7, 8, 43
Although our proof in this case is messier than in the last case, the basic inductive structure is the
same. Importantly, using induction, we can are able to prove this result where otherwise it would
not be possible.
Recap
Induction is reasoning from the specific to the general. Complete induction is induction where the
set of instances is exhaustive. Incomplete induction is induction where the set of instances is not
exhaustive. Linear induction is a type of complete induction for languages with a single object
constants and a single unary function constant. Tree induction is a type of complete induction for

languages with a single object constants and multiple unary function constants. Structural
induction is a generalization of both linear and tree induction that works even in the presence of
multiple object constants and multiple n-ary function constants.


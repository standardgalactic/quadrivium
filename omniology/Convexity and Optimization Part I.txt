Lars-Ã…ke Lindahl
Convexity
Convexity and Optimization â€“ Part I
Download free books at

ii
 
LARS-Ã…KE LINDAHL
CONVEXITY
CONVEXITY AND 
OPTIMIZATION â€“ PART I
Download free eBooks at bookboon.com

iii
Convexity: Convexity and Optimization â€“ Part I
1st edition
Â© 2016 Lars-Ã…ke Lindahl & bookboon.com
ISBN 978-87-403-1382-6
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
iv
Contents
iv
CONTENTS
	
Preface	
viii
	
List of symbols	
x
1	
Preliminaries	
1
2	
Convex sets	
21
2.1	
Affine sets and affine maps	
21
2.2	
Convex sets	
27
2.3	
Convexity preserving operations	
28
2.4	
Convex hull	
34
2.5	
Topological properties	
36
2.6	
Cones	
40
2.7	
The recession cone	
47
	
Exercises	
55
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
v
Contents
3	
Separation	
57
3.1	
Separating hyperplanes	
57
3.2	
The dual cone	
65
3.3	
Solvability of systems of linear inequalities	
69
	
Exercises	
74
4	
More on convex sets	
77
4.1	
Extreme points and faces	
77
4.2	
Structure theorems for convex sets	
83
	
Exercises	
88
5	
Polyhedra	
90
5.1	
Extreme points and extreme rays	
90
5.2	
Polyhedral cones	
94
5.3	
The internal structure of polyhedra	
96
5.4	
Polyhedron preserving operations	
99
5.5	
Separation	
100
	
Exercises	
103
6	
Convex functions	
104
6.1	
Basic definitions	
104
6.2	
Operations that preserve convexity	
113
6.3	
Maximum and minimum	
119
6.4	
Some important inequalities	
122
6.5	
Solvability of systems of convex inequalities	
126
6.6	
Continuity	
129
6.7	
The recessive subspace of convex functions	
131
6.8	
Closed convex functions	
135
6.9	
The support function	
137
6.10	
The Minkowski functional	
140
	
Exercises	
142
7	
Smooth convex functions	
144
7.1	
Convex functions on R	
144
7.2	
Differentiable convex functions	
151
7.3	
Strong convexity	
153
7.4	
Convex functions with Lipschitz continuous derivatives	
156
	
Exercises	
161
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
vi
Contents
8	
The subdifferential	
163
8.1	
The subdifferential	
163
8.2	
Closed convex functions	
169
8.3	
The conjugate function	
173
8.4	
The direction derivative	
180
8.5	
Subdifferentiation rules	
183
	
Exercises	
188
	
Bibliografical and historical notices	
189
	
References	
190
	
Answers and solutions to the exercises	
192
	
Index	
201
	
Endnotes	
204
	
Part II. Linear and Convex Optimization
9	
Optimization	
Part II
9.1	
Optimization problems	
Part II
9.2	
Classification of optimization problems	
Part II
9.3	
Equivalent problem formulations	
Part II
9.4	
Some model examples	
Part II
10	
The Lagrange function	
Part II
10.1	
The Lagrange function and the dual problem	
Part II
10.2	
Johnâ€™s theorem	
Part II
11	
Convex optimization	
Part II
11.1	
Strong duality	
Part II
11.2	
The Karush-Kuhn-Tucker theorem	
Part II
11.3	
The Lagrange multipliers	
Part II
12	
Linear programming	
Part II
12.1	
Optimal solutions	
Part II
12.2	
Duality	
Part II
13	
The simplex algorithm	
Part II
13.1	
Standard form	
Part II
13.2	
Informal description of the simplex algorithm	
Part II
13.3	
Basic solutions	
Part II
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
vii
Contents
13.4	
The simplex algorithm	
Part II
13.5	
Blandâ€™s anti cycling rule	
Part II
13.6	
Phase 1 of the simplex algorithm	
Part II
13.7	
Sensitivity analysis	
Part II
13.8	
The dual simplex algorithm	
Part II
13.9	
Complexity	
Part II
	
Part III. Descent and Interior-point Methods
14	
Descent methods	
Part III
14.1	
General principles	
Part III
14.2	
The gradient descent method	
Part III
15	
Newtonâ€™s method	
Part III
15.1	
Newton decrement and Newton direction	
Part III
15.2	
Newtonâ€™s method	
Part III
15.3	
Equality constraints	
Part III
16	
Self-concordant functions	
Part III
16.1	
Self-concordant functions	
Part III
16.2	
Closed self-concordant functions	
Part III
16.3	
Basic inequalities for the local seminorm	
Part III
16.4	
Minimization	
Part III
16.5	
Newtonâ€™s method for self-concordant functions	
Part III
17	
The path-following method	
Part III
17.1	
Barrier and central path	
Part III
17.2	
Path-following methods	
Part III
18	
The path-following method with self-concordant barrier	
Part III
18.1	
Self-concordant barriers	
Part III
18.2	
The path-following method	
Part III
18.3	
LP problems	
Part III
18.4	
Complexity	
Part III
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
viii
Preface
Preface
Mathematical optimization methods are today used routinely as a tool for
economic and industrial planning, in production control and product de-
sign, in civil and military logistics, in medical image analysis, etc., and the
development in the ï¬eld of optimization has been tremendous since World
War II. In 1945, George Stigler studied a diet problem with 77 foods and 9
constraints without being able to determine the optimal diet âˆ’today it is
possible to solve optimization problems containing hundreds of thousands of
variables and constraints. There are two factors that have made this pos-
sible âˆ’computers and eï¬ƒcient algorithms. It is the rapid development in
the computer area that has been most visible to the common man, but the
algorithm development has also been tremendous during the past 70 years,
and computers would be of little use without eï¬ƒcient algorithms.
Maximization and minimization problems have of course been studied and
solved since the beginning of the mathematical analysis, but optimization
theory in the modern sense started around 1948 with George Dantzig, who
introduced and popularized the concept of linear programming and proposed
an eï¬ƒcient solution algorithm, the simplex algorithm, for such problems.
The type of optimization problems to be discussed by us are problems
that can be formulated as the problem to maximize (or minimize) a given
function over a somehow given subset of Rn.
In order to obtain general
results of interest we need to make some assumptions about the function
and the set, and it is here that convexity enters into the picture. The ï¬rst
part in this series of three on convexity and optimization therefore deals
with ï¬nite dimensional convexity theory. Since convexity plays an important
role in many areas of mathematics, signiï¬cantly more about convexity is
included than is used in the subsequent two parts on optimization, where
Part II provides the basic classical theory for linear and convex optimization,
and Part III describes Newtonâ€™s algorithm, self-concordant functions and an
interior point method with self-concordant barriers.
Parts II and III present a number of algorithms, but the emphasis is al-
ways on the mathematical theory, so we do not describe how the algorithms
should be implemented numerically. Anyone who is interested in these im-
portant aspects should consult specialized literature in the ï¬eld.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
ix
Preface
The embryo of this book is a compendium written by Christer Borell and
myself 1978â€“79, but various additions, deletions and revisions over the years,
have led to a completely diï¬€erent text, the most signiï¬cant addition being
Part III.
The presentation in this book is complete in the sense that all theorems
are proved. Some of the proofs are quite technical, but none of them re-
quires more previous knowledge than a good knowledge of linear algebra and
calculus of several variables.
Uppsala, April 2016
Lars-Ëš
Ake Lindahl
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
x
List of symbols
List of symbols
aï¬€X
aï¬ƒne hull of X, p. 22
bdry X
boundary of X, p. 11
cl f
closure of the function f, p. 172
cl X
closure of X, p. 12
con X
conic hull of X, p. 43
cvx X
convex hull of X, p. 34
dim X
dimension of X, p. 24
dom f
the eï¬€ective domain of f: {x | âˆ’âˆ< f(x) < âˆ}, p. 3
epi f
epigraph of f, p. 104
exr X
set of extreme rays of X, p. 79
ext X
set of extreme points of X, p. 77
int X
interior of X, p. 11
lin X
recessive subspace of X, p. 51
rbdry X
relative boundary of X, p. 37
recc X
recession cone of X, p. 47
rint X
relative interior of X, p. 37
sublevÎ± f
Î±-sublevel set of f, p. 104
ei
ith standard basis vector (0, . . . , 1, . . . , 0), p. 4
f â€²
derivate or gradient of f, p. 17
f â€²(x; v)
direction derivate of f at x in direction v, p. 180
f â€²â€²
second derivative or hessian of f, p. 19
f âˆ—
conjugate function of f, p. 173
B(a; r)
open ball centered at a with radius r, p. 10
B(a; r)
closed ball centered at a with radius r, p. 10
Df(a)[v]
diï¬€erential of f at a, p. 17
D2f(a)[u, v]
n
i,j=1
âˆ‚2f
âˆ‚xiâˆ‚xj (a)uivj, p. 19
D3f(a)[u, v, w]
n
i,j,k=1
âˆ‚3f
âˆ‚xiâˆ‚xjâˆ‚xk (a)uivjwk, p. 20
R+, R++
{x âˆˆR | x â‰¥0}, {x âˆˆR | x > 0}, p. 1
Râˆ’
{x âˆˆR | x â‰¤0}, p. 1
R, R, R
R âˆª{âˆ}, R âˆª{âˆ’âˆ}, R âˆª{âˆ, âˆ’âˆ}, p. 1
SX
support function of X, p. 137
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
xi
List of symbols
SÂµ,L(X)
class of Âµ-strongly convex functions on X with
L-Lipschitz continuous derivative, p. 157
X+
dual cone of X, p. 65
1
the vector (1, 1, . . . , 1), p. 4
âˆ‚f(a)
subdiï¬€erential of f at a, p. 163
Ï†X
Minkowski functional of X, p. 140
âˆ‡f
gradient of f, p. 17
âˆ’â†’x
ray from 0 through x, p. 40
[x, y]
line segment between x and y, p. 7
]x, y[
open line segment between x and y, p. 7
âˆ¥Â·âˆ¥1, âˆ¥Â·âˆ¥2, âˆ¥Â·âˆ¥âˆ
â„“1-norm, Euclidean norm, maximum norm, p. 10
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
1
Preliminaries
Chapter 1
Preliminaries
The purpose of this chapter is twofold âˆ’to explain certain notations and
terminologies used throughout the book and to recall some fundamental con-
cepts and results from calculus and linear algebra.
Real numbers
We use the standard notation R for the set of real numbers, and we let
R+ = {x âˆˆR | x â‰¥0},
Râˆ’= {x âˆˆR | x â‰¤0},
R++ = {x âˆˆR | x > 0}.
In other words, R+ consists of all nonnegative real numbers, and R++ de-
notes the set of all positive real numbers.
The extended real line
Each nonempty set A of real numbers that is bounded above has a least
upper bound, denoted by sup A, and each nonempty set A that is bounded
below has a greatest lower bound, denoted by inf A. In order to have these
two objects deï¬ned for arbitrary subsets of R (and also for other reasons)
we extend the set of real numbers with the two symbols âˆ’âˆand âˆand
introduce the notation
R = R âˆª{âˆ},
R = R âˆª{âˆ’âˆ}
and
R = R âˆª{âˆ’âˆ, âˆ}.
We furthermore extend the order relation < on R to the extended real
line R by deï¬ning, for each real number x,
âˆ’âˆ< x < âˆ.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
2
Preliminaries
The arithmetic operations on R are partially extended by the following
â€naturalâ€ deï¬nitions, where x denotes an arbitrary real number:
x + âˆ= âˆ+ x = âˆ+ âˆ= âˆ
x + (âˆ’âˆ) = âˆ’âˆ+ x = âˆ’âˆ+ (âˆ’âˆ) = âˆ’âˆ
x Â· âˆ= âˆÂ· x =
ï£±
ï£´
ï£²
ï£´
ï£³
âˆ
if x > 0
0
if x = 0
âˆ’âˆ
if x < 0
x Â· (âˆ’âˆ) = âˆ’âˆÂ· x =
ï£±
ï£´
ï£²
ï£´
ï£³
âˆ’âˆ
if x > 0
0
if x = 0
âˆ
if x < 0
âˆÂ· âˆ= (âˆ’âˆ) Â· (âˆ’âˆ) = âˆ
âˆÂ· (âˆ’âˆ) = (âˆ’âˆ) Â· âˆ= âˆ’âˆ.
It is now possible to deï¬ne in a consistent way the least upper bound
and the greatest lower bound of an arbitrary subset of the extended real line.
For nonempty sets A which are not bounded above by any real number, we
deï¬ne sup A = âˆ, and for nonempty sets A which are not bounded below
by any real number we deï¬ne inf A = âˆ’âˆ. Finally, for the empty set âˆ…we
deï¬ne inf âˆ…= âˆand sup âˆ…= âˆ’âˆ.
Sets and functions
We use standard notation for sets and set operations that are certainly well
known to all readers, but the intersection and the union of an arbitrary family
of sets may be new concepts for some readers.
So let {Xi | i âˆˆI} be an arbitrary family of sets Xi, indexed by the set
I; their intersection, denoted by

{Xi | i âˆˆI}
or

iâˆˆI
Xi,
is by deï¬nition the set of elements that belong to all the sets Xi. The union

{Xi | i âˆˆI}
or

iâˆˆI
Xi
consists of the elements that belong to Xi for at least one i âˆˆI.
We write f : X â†’Y to indicate that the function f is deï¬ned on the set
X and takes its values in the set Y . The set X is then called the domain
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
3
Preliminaries
3
of the function and Y is called the codomain. Most functions in this book
have domain equal to Rn or to some subset of Rn, and their codomain is
usually R or more generally Rm for some integer m â‰¥1, but sometimes we
also consider functions whose codomain equals R, R or R.
Let A be a subset of the domain X of the function f. The set
f(A) = {f(x) | x âˆˆA}
is called the image of A under the function f. If B is a subset of the codomain
of f, then
f âˆ’1(B) = {x âˆˆX | f(x) âˆˆB}
is called the inverse image of B under f. There is no implication in the
notation f âˆ’1(B) that the inverse f âˆ’1 exists.
For functions f : X â†’R we use the notation dom f for the inverse image
of R, i.e.
dom f = {x âˆˆX | âˆ’âˆ< f(x) < âˆ}.
The set dom f thus consists of all x âˆˆX with ï¬nite function values f(x),
and it is called the eï¬€ective domain of f.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
4
Preliminaries
The vector space Rn
The reader is assumed to have a solid knowledge of elementary linear algebra
and thus, in particular, to be familiar with basic vector space concepts such
as linear subspace, linear independence, basis and dimension.
As usual, Rn denotes the vector space of all n-tuples (x1, x2, . . . , xn) of
real numbers. The elements of Rn, interchangeably called points and vec-
tors, are denoted by lowercase letters from the beginning or the end of the
alphabet, and if the letters are not numerous enough, we provide them with
sub- or superindices. Subindices are also used to specify the coordinates of
a vector, but there is no risk of confusion, because it will always be clear
from the context whether for instance x1 is a vector of its own or the ï¬rst
coordinate of the vector x.
Vectors in Rn will interchangeably be identiï¬ed with column matrices.
Thus, to us
(x1, x2, . . . , xn)
and
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xn
ï£¹
ï£ºï£ºï£ºï£»
denote the same object.
The vectors e1, e2, . . . , en in Rn, deï¬ned as
e1 = (1, 0, . . . , 0),
e2 = (0, 1, 0, . . . , 0),
. . . ,
en = (0, 0, . . . , 0, 1),
are called the natural basis vectors in Rn, and 1 denotes the vector whose
coordinates are all equal to one, so that
1 = (1, 1, . . . , 1).
The standard scalar product âŸ¨Â· , Â·âŸ©on Rn is deï¬ned by the formula
âŸ¨x, yâŸ©= x1y1 + x2y2 + Â· Â· Â· + xnyn,
and, using matrix multiplication, we can write this as
âŸ¨x, yâŸ©= xTy = yTx,
where T denotes transposition. In general, AT denotes the transpose of the
matrix A.
The solution set to a homogeneous system of linear equations in n un-
knowns is a linear subspace of Rn. Conversely, every linear subspace of Rn
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
5
Preliminaries
can be presented as the solution set to some homogeneous system of linear
equations:
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
a11x1 + a12x2 + Â· Â· Â· + a1nxn = 0
a21x1 + a22x2 + Â· Â· Â· + a2nxn = 0
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = 0
Using matrices we can of course write the system above in a more compact
form as
Ax = 0,
where the matrix A is called the coeï¬ƒcient matrix of the system.
The dimension of the solution set of the above system is given by the
number n âˆ’r, where r equals the rank of the matrix A. Thus in particular,
for each linear subspace X of Rn of dimension n âˆ’1 there exists a nonzero
vector c = (c1, c2, . . . , cn) such that
X = {x âˆˆRn | c1x1 + c2x2 + Â· Â· Â· + cnxn = 0}.
Sum of sets
If X and Y are nonempty subsets of Rn and Î± is a real number, we let
X + Y = {x + y | x âˆˆX, y âˆˆY },
X âˆ’Y = {x âˆ’y | x âˆˆX, y âˆˆY },
Î±X = {Î±x | x âˆˆX}.
The set X + Y is called the (vector) sum of X and Y , X âˆ’Y is the (vector)
diï¬€erence and Î±X is the product of the number Î± and the set X.
It is convenient to have sums, diï¬€erences and products deï¬ned for the
empty set âˆ…, too. Therefore, we extend the above deï¬nitions by deï¬ning
X Â± âˆ…= âˆ…Â± X = âˆ…
for all sets X, and
Î±âˆ…= âˆ….
For singleton sets {a} we write a + X instead of {a} + X, and the set
a + X is called a translation of X.
It is now easy to verify that the following rules hold for arbitrary sets X,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
6
Preliminaries
6
Y and Z and arbitrary real numbers Î± and Î²:
X + Y = Y + X
(X + Y ) + Z = X + (Y + Z)
Î±X + Î±Y = Î±(X + Y )
(Î± + Î²)X âŠ†Î±X + Î²X .
In connection with the last inclusion one should note that the converse
inclusion Î±X + Î²X âŠ†(Î± + Î²)X does not hold for general sets X.
Inequalites in Rn
For vectors x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) in Rn we write x â‰¥y
if xj â‰¥yj for all indices j, and we write x > y if xj > yj for all j. In
particular, x â‰¥0 means that all coordinates of x are nonnegative.
The set
Rn
+ = R+ Ã— R+ Ã— Â· Â· Â· Ã— R+ = {x âˆˆRn | x â‰¥0}
is called the nonnegative orthant of Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
7
Preliminaries
The order relation â‰¥is a partial order on Rn. It is thus, in other words,
reï¬‚exive (x â‰¥x for all x), transitive (x â‰¥y & y â‰¥z â‡’x â‰¥z) and
antisymmetric (x â‰¥y & y â‰¥x â‡’x = y). However, the order is not a
complete order when n > 1, since two vectors x and y may be unrelated.
Two important properties, which will be used now and then, are given
by the following two trivial implications:
x â‰¥0 & y â‰¥0 â‡’âŸ¨x, yâŸ©â‰¥0
x â‰¥0 & y â‰¥0 & âŸ¨x, yâŸ©= 0 â‡’x = y = 0.
Line segments
Let x and y be points in Rn. We deï¬ne
[x, y] = {(1 âˆ’Î»)x + Î»y | 0 â‰¤Î» â‰¤1}
and
]x, y[ = {(1 âˆ’Î»)x + Î»y | 0 < Î» < 1},
and we call the set [x, y] the line segment and the set ]x, y[ the open line
segment between x and y, if the two points are distinct. If the two points
coincide, i.e. if y = x, then obviously [x, x] =]x, x[= {x}.
Linear maps and linear forms
Let us recall that a map S : Rn â†’Rm is called linear if
S(Î±x + Î²y) = Î±Sx + Î²Sy
for all vectors x, y âˆˆRn and all scalars (i.e. real numbers) Î±, Î². A linear
map S : Rn â†’Rn is also called a linear operator on Rn.
Each linear map S : Rn â†’Rm gives rise to a unique m Ã— n-matrix ËœS
such that
Sx = ËœSx,
which means that the function value Sx of the map S at x is given by
the matrixproduct ËœSx. (Remember that vectors are identiï¬ed with column
matrices!) For this reason, the same letter will be used to denote a map and
its matrix. We thus interchangeably consider Sx as the value of a map and
as a matrix product.
By computing the scalar product âŸ¨x, SyâŸ©as a matrix product we obtain
the following relation
âŸ¨x, SyâŸ©= xTSy = (STx)Ty = âŸ¨STx, yâŸ©
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
8
Preliminaries
between a linear map S : Rn â†’Rm (or m Ã— n-matrix S) and its transposed
map ST : Rm â†’Rn (or transposed matrix ST).
An n Ã— n-matrix A = [aij], and the corresponding linear map, is called
symmetric if AT = A, i.e. if aij = aji for all indices i, j.
A linear map f : Rn â†’R with codomain R is called a linear form.
A
linear form on Rn is thus of the form
f(x) = c1x1 + c2x2 + Â· Â· Â· + cnxn,
where c = (c1, c2, . . . , cn) is a vector in Rn. Using the standard scalar product
we can write this more simply as
f(x) = âŸ¨c, xâŸ©,
and in matrix notation this becomes
f(x) = cTx.
Let f(x) = âŸ¨c, yâŸ©be a linear form on Rm and let S : Rn â†’Rm be a
linear map with codomain Rm. The composition f â—¦S is then a linear form
on Rn, and we conclude that there exists a unique vector d âˆˆRn such that
(f â—¦S)(x) = âŸ¨d, xâŸ©for all x âˆˆRn. Since f(Sx) = âŸ¨c, SxâŸ©= âŸ¨STc, xâŸ©, it
follows that d = STc.
Quadratic forms
A function q: Rn â†’R is called a quadratic form if there exists a symmetric
n Ã— n-matrix Q = [qij] such that
q(x) =
n

i,j=1
qijxixj,
or equivalently
q(x) = âŸ¨x, QxâŸ©= xTQx.
The quadratic form q determines the symmetric matrix Q uniquely, and this
allows us to identify the form q with its matrix (or operator) Q.
An arbitrary quadratic polynomial p(x) in n variables can now be written
in the form
p(x) = âŸ¨x, AxâŸ©+ âŸ¨b, xâŸ©+ c,
where x â†’âŸ¨x, AxâŸ©is a quadratic form determined by a symmetric operator
(or matrix) A, x â†’âŸ¨b, xâŸ©is a linear form determined by a vector b, and c is
a real number.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
9
Preliminaries
9
Example. In order to write the quadratic polynomial
p(x1, x2, x3) = x2
1 + 4x1x2 âˆ’2x1x3 + 5x2
2 + 6x2x3 + 3x1 + 2x3 + 2
in this form we ï¬rst replace the terms dxixj for i < j with 1
2dxixj + 1
2dxjxi.
This yields
p(x1, x2, x3) = (x2
1 + 2x1x2 âˆ’x1x3 + 2x2x1 + 5x2
2 + 3x2x3 âˆ’x3x1 + 3x3x2)
+ (3x1 + 2x3) + 2 = âŸ¨x, AxâŸ©+ âŸ¨b, xâŸ©+ c
with A =
ï£®
ï£°
1
2
âˆ’1
2
5
3
âˆ’1
3
0
ï£¹
ï£», b =
ï£®
ï£°
3
0
2
ï£¹
ï£»and c = 2.
A quadratic form q on Rn (and the corresponding symmetric operator
and matrix) is called positive semideï¬nite if q(x) â‰¥0 and positive deï¬nite if
q(x) > 0 for all vectors x Ì¸= 0 in Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
10
Preliminaries
Norms and balls
A norm âˆ¥Â·âˆ¥on Rn is a function Rn â†’R+ that satisï¬es the following three
conditions:
âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥yâˆ¥
for all x, y
(i)
âˆ¥Î»xâˆ¥= |Î»| âˆ¥xâˆ¥
for all x âˆˆRn, Î» âˆˆR
(ii)
âˆ¥xâˆ¥= 0 â‡”x = 0.
(iii)
The most important norm to us is the Euclidean norm, deï¬ned via the
standard scalar product as
âˆ¥xâˆ¥=

âŸ¨x, xâŸ©=

x2
1 + x2
2 + Â· Â· Â· + x2
n.
This is the norm that we use unless the contrary is stated explicitely. We
use the notation âˆ¥Â·âˆ¥2 for the Euclidean norm whenever we for some reason
have to emphasize that the norm in question is the Euclidean one.
Other norms, that will occur now and then, are the maximum norm
âˆ¥xâˆ¥âˆ= max
1â‰¤iâ‰¤n |xi|,
and the â„“1-norm
âˆ¥xâˆ¥1 =
n

i=1
|xi|.
It is easily veriï¬ed that these really are norms, that is that conditions (i)â€“(iii)
are satisï¬ed.
All norms on Rn are equivalent in the following sense: If âˆ¥Â·âˆ¥and âˆ¥Â·âˆ¥â€² are
two norms, then there exist two positive constants c and C such that
câˆ¥xâˆ¥â€² â‰¤âˆ¥xâˆ¥â‰¤Câˆ¥xâˆ¥â€²
for all x âˆˆRn.
For example, âˆ¥xâˆ¥âˆâ‰¤âˆ¥xâˆ¥2 â‰¤âˆšn âˆ¥xâˆ¥âˆ.
Given an arbitrary norm âˆ¥Â·âˆ¥we deï¬ne the corresponding distance between
two points x and a in Rn as âˆ¥x âˆ’aâˆ¥. The set
B(a; r) = {x âˆˆRn | âˆ¥x âˆ’aâˆ¥< r},
consisting of all points x whose distance to a is less than r, is called the open
ball centered at the point a and with radius r. Of course, we have to have
r > 0 in order to get a nonempty ball. The set
B(a; r) = {x âˆˆRn | âˆ¥x âˆ’aâˆ¥â‰¤r}
is the corresponding closed ball.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
11
Preliminaries
The geometric shape of the balls depends on the underlying norm. The
ball B(0; 1) in R2 is a square with corners at the points (Â±1, Â±1) when the
norm is the maximum norm, it is a square with corners at the points (Â±1, 0)
and (0, Â±1) when the norm is the â„“1-norm, and it is the unit disc when the
norm is the Euclidean one.
If B denotes balls deï¬ned by one norm and Bâ€² denotes balls deï¬ned by a
second norm, then there are positive constants c and C such that
(1.1)
Bâ€²(a; cr) âŠ†B(a; r) âŠ†Bâ€²(a; Cr)
for all a âˆˆRn and all r > 0. This follows easily from the equivalence of the
two norms.
All balls that occur in the sequel are assumed to be Euclidean, i.e. deï¬ned
with respect to the Euclidean norm, unless otherwise stated.
Topological concepts
We now use balls to deï¬ne a number of topological concepts. Let X be an
arbitrary subset of Rn. A point a âˆˆRn is called
â€¢ an interior point of X if there exists an r > 0 such that B(a; r) âŠ†X;
â€¢ a boundary point of X if X âˆ©B(a; r) Ì¸= âˆ…and âˆX âˆ©B(a; r) Ì¸= âˆ…for all
r > 0;
â€¢ an exterior point of X if there exists an r > 0 such that Xâˆ©B(a; r) = âˆ….
Observe that because of property (1.1), the above concepts do not depend
on the kind of balls that we use.
A point is obviously either an interior point, a boundary point or an
exterior point of X. Interior points belong to X, exterior points belong to
the complement of X, while boundary points may belong to X but must not
do so. Exterior points of X are interior points of the complement âˆX, and
vice versa, and the two sets X and âˆX have the same boundary points.
The set of all interior points of X is called the interior of X and is denoted
by int X. The set of all boundary points is called the boundary of X and is
denoted by bdry X.
A set X is called open if all points in X are interior points, i.e. if int X =
X.
It is easy to verify that the union of an arbitrary family of open sets is
an open set and that the intersection of ï¬nitely many open sets is an open
set. The empty set âˆ…and Rn are open sets.
The interior int X is a (possibly empty) open set for each set X, and
int X is the biggest open set that is included in X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
12
Preliminaries
12
A set X is called closed if its complement âˆX is an open set. It follows
that X is closed if and only if X contains all its boundary points, i.e. if and
only if bdry X âŠ†X.
The intersection of an arbitrary family of closed sets is closed, the union
of ï¬nitely many closed sets is closed, and Rn and âˆ…are closed sets.
For arbitrary sets X we set
cl X = X âˆªbdry X.
The set cl X is then a closed set that contains X, and it is called the closure
(or closed hull) of X. The closure cl X is the smallest closed set that contains
X as a subset.
For example, if r > 0 then
cl B(a; r) = {x âˆˆRn | âˆ¥x âˆ’aâˆ¥â‰¤r} = B(a; r),
which makes it consistent to call the set B(a; r) a closed ball.
For nonempty subsets X of Rn and numbers r > 0 we deï¬ne
X(r) = {y âˆˆRn | âˆƒx âˆˆX : âˆ¥y âˆ’xâˆ¥< r}.
The set X(r) thus consists of all points whose distance to X is less than r.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
13
Preliminaries
A point x is an exterior point of X if and only if the distance from x to
X is positive, i.e. if and only if there is an r > 0 such that x /âˆˆX(r). This
means that a point x belongs to the closure cl X, i.e. x is an interior point
or a boundary point of X, if and only if x belongs to the sets X(r) for all
r > 0. In other words,
cl X =

r>0
X(r).
A set X is said to be bounded if it is contained in some ball centered at
0, i.e. if there is a number R > 0 such that X âŠ†B(0; R).
A set X that is both closed and bounded is called compact.
An important property of compact subsets X of Rn is given by the
Bolzanoâ€“Weierstrass theorem: Every inï¬nite sequence (xn)âˆ
n=1 of points xn
in a compact set X has a subsequence (xnk)âˆ
k=1 that converges to a point in
X.
The cartesian product XÃ—Y of a compact subset X of Rm and a compact
subset Y of Rn is a compact subset of Rm Ã— Rn
(= Rm+n).
Continuity
A function f : X â†’Rm, whose domain X is a subset of Rn, is deï¬ned to be
continuous at the point a âˆˆX if for each Ïµ > 0 there exists an r > 0 such
that
f(X âˆ©B(a; r)) âŠ†B(f(a); Ïµ).
(Here, of course, the left B stands for balls in Rn and the right B stands
for balls in Rm.) The function is said to be continuous on X, or simply
continuous, if it is continuous at all points a âˆˆX.
The inverse image f âˆ’1(I) of an open interval under a continuous function
f : Rn â†’R is an open set in Rn. In particular, the sets {x | f(x) < a} and
{x | f(x) > a}, i.e. the sets f âˆ’1(]âˆ’âˆ, a[) and f âˆ’1(]a, âˆ[), are open for all
a âˆˆR. Their complements, the sets {x | f(x) â‰¥a} and {x | f(x) â‰¤a}, are
thus closed.
Sums and (scalar) products of continuous functions are continuous, and
quotients of real-valued continuous functions are continuous at all points
where the quotients are well-deï¬ned. Compositions of continuous functions
are continuous.
Compactness is preserved under continuous functions, that is the image
f(X) is compact if X is a compact subset of the domain of the continuous
function f. For continuous functions f with codomain R this means that
f is bounded on X and has a maximum and a minimum, i.e. there are two
points x1, x2 âˆˆX such that f(x1) â‰¤f(x) â‰¤f(x2) for all x âˆˆX.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
14
Preliminaries
Lipschitz continuity
A function f : X â†’Rm that is deï¬ned on a subset X of Rn, is called
Lipschitz continuous with Lipschitz constant L if
âˆ¥f(y) âˆ’f(x)âˆ¥â‰¤Lâˆ¥y âˆ’xâˆ¥
for all x, y âˆˆX.
Note that the deï¬nition of Lipschitz continuity is norm independent, since
all norms on Rn are equivalent, but the value of the Lipschitz constant L is
obviously norm dependent.
Operator norms
Let âˆ¥Â·âˆ¥be a given norm on Rn. Since the closed unit ball is compact and
linear operators S on Rn are continuous, we get a ï¬nite number âˆ¥Sâˆ¥, called
the operator norm, by the deï¬nition
âˆ¥Sâˆ¥= sup
âˆ¥xâˆ¥â‰¤1
âˆ¥Sxâˆ¥.
That the operator norm really is a norm on the space of linear opera-
tors, i.e. that it satisï¬es conditions (i)â€“(iii) in the norm deï¬nition, follows
immediately from the corresponding properties of the underlying norm on
Rn.
By deï¬nition, S(x/âˆ¥xâˆ¥) â‰¤âˆ¥Sâˆ¥for all x Ì¸= 0, and consequently
âˆ¥Sxâˆ¥â‰¤âˆ¥Sâˆ¥âˆ¥xâˆ¥
for all x âˆˆRn.
From this inequality follows immediately that
âˆ¥STxâˆ¥â‰¤âˆ¥Sâˆ¥âˆ¥Txâˆ¥â‰¤âˆ¥Sâˆ¥âˆ¥Tâˆ¥âˆ¥xâˆ¥,
which gives us the important inequality
âˆ¥STâˆ¥â‰¤âˆ¥Sâˆ¥âˆ¥Tâˆ¥
for the norm of a product of two operators.
The identity operator I on Rn clearly has norm equal to 1. Therefore,
if the operator S is invertible, then, by choosing T = Sâˆ’1 in the above
inequality, we obtain the inequality
âˆ¥Sâˆ’1âˆ¥â‰¥1/âˆ¥Sâˆ¥.
The operator norm obviously depends on the underlying norm on Rn,
but again, diï¬€erent norms on Rn give rise to equivalent norms on the space
of operators. However, when speaking about the operator norm we shall in
this book always assume that the underlying norm is the Euclidean norm
even if this is not stated explicitely.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
15
Preliminaries
15
Symmetric operators, eigenvalues and norms
Every symmetric operator S on Rn is diagonizable according to the spectral
theorem. This means that there is an ON-basis e1, e2, . . . , en consisting of
eigenvectors of S. Let Î»1, Î»2, . . . , Î»n denote the corresponding eigenvalues.
The largest and the smallest eigenvalue Î»max and Î»min are obtained as
maximum and minimum values, respectively, of the quadratic form âŸ¨x, SxâŸ©
on the unit sphere âˆ¥xâˆ¥= 1:
Î»max = max
âˆ¥xâˆ¥=1âŸ¨x, SxâŸ©
and
Î»min = min
âˆ¥xâˆ¥=1âŸ¨x, SxâŸ©.
For, by using the expansion x = n
i=1 Î¾iei of x in the ON-basis of eigen-
vectors, we obtain the inequality
âŸ¨x, SxâŸ©=
n

i=1
Î»iÎ¾2
i â‰¤Î»max
n

i=1
Î¾2
i = Î»maxâˆ¥xâˆ¥2,
and equality prevails when x is equal to the eigenvector ei that corresponds
to the eigenvalue Î»max. An analogous inequality in the other direction holds
for Î»min, of course.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
16
Preliminaries
The operator norm (with respect to the Euclidean norm) moreover satis-
ï¬es the equality
âˆ¥Sâˆ¥= max
1â‰¤iâ‰¤n |Î»i| = max{|Î»max|, |Î»min|}.
For, by using the above expansion of x, we have Sx = n
i=1 Î»iÎ¾iei, and
consequently
âˆ¥Sxâˆ¥2 =
n

i=1
Î»2
i Î¾2
i â‰¤max
1â‰¤iâ‰¤n |Î»i|2
n

i=1
Î¾2
i = ( max
1â‰¤iâ‰¤n |Î»i|)2 âˆ¥xâˆ¥2,
with equality when x is the eigenvector that corresponds to maxi |Î»i|.
If all eigenvalues of the symmetric operator S are nonzero, then S is in-
vertible, and the inverse Sâˆ’1 is symmetric with eigenvalues Î»âˆ’1
1 , Î»âˆ’1
2 , . . . , Î»âˆ’1
n .
The norm of the inverse is given by
âˆ¥Sâˆ’1âˆ¥= 1/ min
1â‰¤iâ‰¤n |Î»i|.
A symmetric operator S is positive semideï¬nite if all its eigenvalues are
nonnegative, and it is positive deï¬nite if all eigenvalues are positive. Hence,
if S is positive deï¬nite, then
âˆ¥Sâˆ¥= Î»max
and
âˆ¥Sâˆ’1âˆ¥= 1/Î»min.
It follows easily from the diagonizability of symmetric operators on Rn
that every positive semideï¬nite symmetric operator S has a unique positive
semideï¬nite symmetric square root S1/2. Moreover, since
âŸ¨x, SxâŸ©= âŸ¨x, S1/2(S1/2x)âŸ©= âŸ¨S1/2x, S1/2xâŸ©= âˆ¥S1/2xâˆ¥
we conclude that the two operators S and S1/2 have the same null space
N(S) and that
N(S) = {x âˆˆRn | Sx = 0} = {x âˆˆRn | âŸ¨x, SxâŸ©= 0}.
Diï¬€erentiability
A function f : U â†’R, which is deï¬ned on an open subset U of Rn, is called
diï¬€erentiable at the point a âˆˆU if the partial derivatives
âˆ‚f
âˆ‚xi exist at the
point x and the equality
(1.2)
f(a + v) = f(a) +
n

i=1
âˆ‚f
âˆ‚xi
(a) vi + r(v)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
17
Preliminaries
holds for all v in some neighborhood of the origin with a remainder term r(v)
that satisï¬es the condition
lim
vâ†’0
r(v)
âˆ¥vâˆ¥= 0.
The linear form Df(a)[v], deï¬ned by
Df(a)[v] =
n

i=1
âˆ‚f
âˆ‚xi
(a) vi,
is called the diï¬€erential of the function f at the point a. The coeï¬ƒcient
vector
 âˆ‚f
âˆ‚x1
(a), âˆ‚f
âˆ‚x2
(a), . . . , âˆ‚f
âˆ‚xn
(a)

of the diï¬€erential is called the derivative or the gradient of f at the point a
and is denoted by f â€²(a) or âˆ‡f(a). We shall mostly use the ï¬rst mentioned
notation.
The equation (1.2) can now be written in a compact form as
f(a + v) = f(a) + Df(a)[v] + r(v),
with
Df(a)[v] = âŸ¨f â€²(a), vâŸ©.
A function f : U â†’R is called diï¬€erentiable (on U) if it is diï¬€erentiable
at each point in U. In particular, this implies that U is an open set.
For functions of one variable, diï¬€erentiability is clearly equivalent to the
existence of the derivative, but for functions of several variables, the mere
existence of the partial derivatives is no longer a guarantee for diï¬€erentiabil-
ity. However, if a function f has partial derivatives and these are continous
on an open set U, then f is diï¬€erentiable on U.
The Mean Value Theorem
Suppose f : U â†’R is a diï¬€erentiable function and that the line segment
[a, a + v] lies in U. Let Ï†(t) = f(a + tv). The function Ï† is then deï¬ned and
diï¬€erentiable on the interval [0, 1] with derivative
Ï†â€²(t) = Df(a + tv)[v] = âŸ¨f â€²(a + tv), vâŸ©.
This is a special case of the chain rule but also follows easily from the deï¬ni-
tion of the derivative. By the usual mean value theorem for functions of one
variable, there is a number s âˆˆ]0, 1[ such that Ï†(1) âˆ’Ï†(0) = Ï†â€²(s)(1 âˆ’0).
Since Ï†(1) = f(a + v), Ï†(0) = f(a) and a + sv is a point on the open line
segment ]a, a + v[, we have now deduced the following mean value theorem
for functions of several variables.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
18
Preliminaries
18
Theorem 1.1.1. Suppose the function f : U â†’R is diï¬€erentiable and that
the line segment [a, a + v] lies in U. Then there is a point c âˆˆ]a, a + v[ such
that
f(a + v) = f(a) + Df(c)[v].
Functions with Lipschitz continuous derivative
We shall sometimes need more precise information about the remainder term
r(v) in equation (1.2) than what follows from the deï¬nition of diï¬€erentiabil-
ity. We have the following result for functions with a Lipschitz continuous
derivative.
Theorem 1.1.2. Suppose the function f : U â†’R is diï¬€erentiable, that its
derivative is Lipschitz continuous, i.e. that âˆ¥f â€²(y) âˆ’f â€²(x)âˆ¥â‰¤Lâˆ¥y âˆ’xâˆ¥for
all x, y âˆˆU, and that the line segment [a, a + v] lies in U. Then
|f(a + v) âˆ’f(a) âˆ’Df(a)[v]| â‰¤L
2 âˆ¥vâˆ¥2.
Proof. Deï¬ne the function Î¦ on the interval [0, 1] by
Î¦(t) = f(a + tv) âˆ’t Df(a)[v].
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
19
Preliminaries
Then Î¦ is diï¬€erentiable with derivative
Î¦â€²(t) = Df(a + tv)[v] âˆ’Df(a)[v] = âŸ¨f â€²(a + tv) âˆ’f â€²(a), vâŸ©,
and by using the Cauchyâ€“Schwarz inequality and the Lipschitz continuity,
we obtain the inequality
|Î¦â€²(t)| â‰¤âˆ¥f â€²(a + tv) âˆ’f â€²(a)âˆ¥Â· âˆ¥vâˆ¥â‰¤Lt âˆ¥vâˆ¥2.
Since f(a + v) âˆ’f(a) âˆ’Df(a)[v] = Î¦(1) âˆ’Î¦(0) =
 1
0 Î¦â€²(t) dt, it now follows
that
|f(a + v) âˆ’f(a) âˆ’Df(a)[v]| â‰¤
 1
0
|Î¦â€²(t)| dt â‰¤Lâˆ¥vâˆ¥2
 1
0
t dt = L
2 âˆ¥vâˆ¥2.
Two times diï¬€erentiable functions
If the function f together with all its partial derivatives âˆ‚f
âˆ‚xi are diï¬€erentiable
on U, then f is said to be two times diï¬€erentiable on U. The mixed partial
second derivatives are then automatically equal, i.e.
âˆ‚2f
âˆ‚xiâˆ‚xj
(a) =
âˆ‚2f
âˆ‚xjâˆ‚xi
(a)
for all i, j and all a âˆˆU.
A suï¬ƒcient condition for the function f to be two times diï¬€erentiable on
U is that all partial derivatives of order up to two exist and are continuous
on U.
If f : U â†’R is a two times diï¬€erentiable function and a is a point in U,
we deï¬ne a symmetric bilinear form D2f(a)[u, v] on Rn by
D2f(a)[u, v] =
n

i,j=1
âˆ‚2f
âˆ‚xiâˆ‚xj
(a)uivj,
u, v âˆˆRn.
The corresponding symmetric linear operator is called the second derivative
of f at the point a and it is denoted by f â€²â€²(a). The matrix of the second
derivative, i.e. the matrix
 âˆ‚2f
âˆ‚xiâˆ‚xj
(a)
n
i,j=1,
is called the hessian of f (at the point a). Since we do not distinguish between
matrices and operators, we also denote the hessian by f â€²â€²(a).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
20
Preliminaries
The above symmetric bilinear form can now be expressed in the form
D2f(a)[u, v] = âŸ¨u, f â€²â€²(a)vâŸ©= uTf â€²â€²(a)v,
depending on whether we interpret the second derivative as an operator or
as a matrix.
Let us recall Taylorâ€™s formula, which reads as follows for two times dif-
ferentiable functions.
Theorem 1.1.3. Suppose the function f is two times diï¬€erentiable in a neigh-
borhood of the point a. Then
f(a + v) = f(a) + Df(a)[v] + 1
2D2f(a)[v, v] + r(v)
with a remainder term that satisï¬es lim
vâ†’0 r(v)/âˆ¥vâˆ¥2 = 0.
Three times diï¬€erentiable functions
To deï¬ne self-concordance we also need to consider functions that are three
times diï¬€erentiable on some open subset U of Rn.
For such functions f
and points a âˆˆU we deï¬ne a trilinear form D3f(a)[u, v, w] in the vectors
u, v, w âˆˆRn by
D3f(a)[u, v, w] =
n

i,j,k=1
âˆ‚3f
âˆ‚xiâˆ‚xjâˆ‚xk
(a)uivjwk.
We leave to the reader to formulate Taylorâ€™s formula for functions that
are three times diï¬€erentiable. We have the following diï¬€erentiation rules,
which follow from the chain rule and will be used several times in the ï¬nal
chapters:
d
dtf(x + tv) = Df(x + tv)[v]
d
dt

Df(x + tv)[u]

= D2f(x + tv)[u, v],
d
dt

D2f(x + tw)[u, v]

= D3f(x + tw)[u, v, w].
As a consequence we get the following expressions for the derivatives of
the restriction Ï† of the function f to the line through the point x with the
direction given by v:
Ï†(t) = f(x + tv),
Ï†â€²(t) = Df(x + tv)[v],
Ï†â€²â€²(t) = D2f(x + tv)[v, v],
Ï†â€²â€²â€²(t) = D3f(x + tv)[v, v, v].
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
21
Convex sets
Chapter 2
Convex sets
2.1
Aï¬ƒne sets and aï¬ƒne maps
Aï¬ƒne sets
Deï¬nition. A subset of Rn is called aï¬ƒne if for each pair of distinct points
in the set it contains the entire line through the points.
Thus, a set X is aï¬ƒne if and only if
x, y âˆˆX, Î» âˆˆR â‡’Î»x + (1 âˆ’Î»)y âˆˆX.
The empty set âˆ…, the entire space Rn, linear subspaces of Rn, singleton
sets {x} and lines are examples of aï¬ƒne sets.
Deï¬nition. A linear combination y = m
j=1 Î±jxj of vectors x1, x2, . . . , xm is
called an aï¬ƒne combination if m
j=1 Î±j = 1.
Theorem 2.1.1. An aï¬ƒne set contains all aï¬ƒne combination of its elements.
Proof. We prove the theorem by induction on the number of elements in the
aï¬ƒne combination. So let X be an aï¬ƒne set. An aï¬ƒne combination of one
element is the element itself. Hence, X contains all aï¬ƒne combinations that
can be formed by one element in the set.
Now assume inductively that X contains all aï¬ƒne combinations that can
be formed out of m âˆ’1 elements from X, where m â‰¥2, and consider an
arbitrary aï¬ƒne combination x = m
j=1 Î±jxj of m elements x1, x2, . . . , xm in
X. Since m
j=1 Î±j = 1, at least one coeï¬ƒcient Î±j must be diï¬€erent from 1;
assume without loss of generality that Î±m Ì¸= 1, and let s = 1âˆ’Î±m = mâˆ’1
j=1 Î±j.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
22
Convex sets
Then s Ì¸= 0 and mâˆ’1
j=1 Î±j/s = 1, which means that the element
y =
mâˆ’1

j=1
Î±j
s xj
is an aï¬ƒne combination of m âˆ’1 elements in X. Therefore, y belongs to X,
by the induction assumption. But x = sy+(1âˆ’s)xm, and it now follows from
the deï¬nition of aï¬ƒne sets that x lies in X. This completes the induction
step, and the theorem is proved.
Deï¬nition. Let A be an arbitrary nonempty subset of Rn. The set of all aï¬ƒne
combinations Î»1a1 + Î»2a2 + Â· Â· Â· + Î»mam that can be formed of an arbitrary
number of elements a1, a2, . . . , am from A, is called the aï¬ƒne hull of A and
is denoted by aï¬€A .
In order to have the aï¬ƒne hull deï¬ned also for the empty set, we put
aï¬€âˆ…= âˆ….
Theorem 2.1.2. The aï¬ƒne hull aï¬€A is an aï¬ƒne set containing A as a
subset, and it is the smallest aï¬ƒne subset with this property, i.e. if the set X
is aï¬ƒne and A âŠ†X, then aï¬€A âŠ†X.
Proof. The set aï¬€A is an aï¬ƒne set, because any aï¬ƒne combination of two
elements in aï¬€A is obviously an aï¬ƒne combination of elements from A,
and the set A is a subset of its aï¬ƒne hull, since any element is an aï¬ƒne
combination of itself.
If X is an aï¬ƒne set, then aï¬€X âŠ†X, by Theorem 2.1.1, and if A âŠ†X,
then obviously aï¬€A âŠ†aï¬€X. Thus, aï¬€A âŠ†X whenever X is an aï¬ƒne set
and A is a subset of X.
Characterisation of aï¬ƒne sets
Nonempty aï¬ƒne sets are translations of linear subspaces. More precisely, we
have the following theorem.
Theorem 2.1.3. If X is an aï¬ƒne subset of Rn and a âˆˆX, then âˆ’a+X is a
linear subspace of Rn. Moreover, for each b âˆˆX we have âˆ’b+X = âˆ’a+X.
Thus, to each nonempty aï¬ƒne set X there corresponds a uniquely deï¬ned
linear subspace U such that X = a + U.
Proof. Let U = âˆ’a + X. If u1 = âˆ’a + x1 and u2 = âˆ’a + x2 are two elements
in U and Î±1, Î±2 are arbitrary real numbers, then the linear combination
Î±1u1 + Î±2u2 = âˆ’a + (1 âˆ’Î±1 âˆ’Î±2)a + Î±1x1 + Î±2x2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
23
Convex sets
23
a
X
0
U = âˆ’a + X
Figure 2.1. Illustration for Theorem 2.1.3: An aï¬ƒne
set X and the corresponding linear subspace U.
is an element in U, because (1âˆ’Î±1âˆ’Î±2)a+Î±1x1+Î±2x2 is an aï¬ƒne combination
of elements in X and hence belongs to X, according to Theorem 2.1.1. This
proves that U is a linear subspace.
Now assume that b âˆˆX, and let v = âˆ’b + x be an arbitrary element in
âˆ’b + X. By writing v as v = âˆ’a + (a âˆ’b + x) we see that v belongs to
âˆ’a + X, too, because a âˆ’b + x is an aï¬ƒne combination of elements in X.
This proves the inclusion âˆ’b + X âŠ†âˆ’a + X. The converse inclusion follows
by symmetry. Thus, âˆ’a + X = âˆ’b + X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
24
Convex sets
Dimension
The following deï¬nition is justiï¬ed by Theorem 2.1.3.
Deï¬nition. The dimension dim X of a nonempty aï¬ƒne set X is deï¬ned as
the dimension of the linear subspace âˆ’a+X, where a is an arbitrary element
in X.
Since every nonempty aï¬ƒne set has a well-deï¬ned dimension, we can
extend the dimension concept to arbitrary nonempty sets as follows.
Deï¬nition. The (aï¬ƒne) dimension dim A of a nonempty subset A of Rn is
deï¬ned to be the dimension of its aï¬ƒne hull aï¬€A.
The dimension of an open ball B(a; r) in Rn is n, and the dimension of
a line segment [x, y] is 1.
The dimension is invariant under translation i.e. if A is a nonempty subset
of Rn and a âˆˆRn then
dim(a + A) = dim A,
and it is increasing in the following sense:
A âŠ†B â‡’dim A â‰¤dim B.
Aï¬ƒne sets as solutions to systems of linear equations
Our next theorem gives a complete description of the aï¬ƒne subsets of Rn.
Theorem 2.1.4. Every aï¬ƒne subset of Rn is the solution set of a system of
linear equations
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
c11x1 + c12x2 + Â· Â· Â· + c1nxn = b1
c21x1 + c22x2 + Â· Â· Â· + c2nxn = b2
...
cm1x1 + cm2x2 + Â· Â· Â· + cmnxn = bm
and conversely. The dimension of a nonempty solution set equals nâˆ’r, where
r is the rank of the coeï¬ƒcient matrix C.
Proof. The empty aï¬ƒne set is obtained as the solution set of an inconsistent
system. Therefore, we only have to consider nonempty aï¬ƒne sets X, and
these are of the form X = x0 + U, where x0 belongs to X and U is a linear
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
25
Convex sets
subspace of Rn. But each linear subspace is the solution set of a homogeneous
system of linear equations. Hence there exists a matrix C such that
U = {x | Cx = 0},
and dim U = n âˆ’rank C. With b = Cx0 it follows that x âˆˆX if and only
if Cx âˆ’Cx0 = C(x âˆ’x0) = 0, i.e. if and only if x is a solution to the linear
system Cx = b.
Conversely, if x0 is a solution to the above linear system so that Cx0 = b,
then x is a solution to the same system if and only if the vector z = x âˆ’x0
belongs to the solution set U of the homogeneous equation system Cz = 0.
It follows that the solution set of the equation system Cx = b is of the form
x0 + U, i.e. it is an aï¬ƒne set.
Hyperplanes
Deï¬nition. Aï¬ƒne subsets of Rn of dimension n âˆ’1 are called hyperplanes.
Theorem 2.1.4 has the following corollary:
Corollary 2.1.5. A subset X of Rn is a hyperplane if and only if there exist
a nonzero vector c = (c1, c2, . . . , cn) and a real number b so that
X = {x âˆˆRn | âŸ¨c, xâŸ©= b}.
It follows from Theorem 2.1.4 that every aï¬ƒne proper subset of Rn can
be expressed as an intersection of hyperplanes.
Aï¬ƒne maps
Deï¬nition. Let X be an aï¬ƒne subset of Rn. A map T : X â†’Rm is called
aï¬ƒne if
T(Î»x + (1 âˆ’Î»)y) = Î»Tx + (1 âˆ’Î»)Ty
for all x, y âˆˆX and all Î» âˆˆR.
Using induction, it is easy to prove that if T : X â†’Rm is an aï¬ƒne map
and x = Î±1x1 + Î±2x2 + Â· Â· Â· + Î±mxm is an aï¬ƒne combination of elements in
X, then
Tx = Î±1Tx1 + Î±2Tx2 + Â· Â· Â· + Î±mTxm.
Moreover, the image T(Y ) of an aï¬ƒne subset Y of X is an aï¬ƒne subset of
Rm, and the inverse image T âˆ’1(Z) of an aï¬ƒne subset Z of Rm is an aï¬ƒne
subset of X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
26
Convex sets
26
The composition of two aï¬ƒne maps is aï¬ƒne. In particular, a linear map
followed by a translation is an aï¬ƒne map, and our next theorem shows that
each aï¬ƒne map can be written as such a composition.
Theorem 2.1.6. Let X be an aï¬ƒne subset of Rn, and suppose the map
T : X â†’Rm is aï¬ƒne. Then there exist a linear map C : Rn â†’Rm and
a vector v in Rm so that Tx = Cx + v for all x âˆˆX.
Proof. Write the domain of T in the form X = x0 + U with x0 âˆˆX and U
as a linear subspace of Rn, and deï¬ne the map C on the subspace U by
Cu = T(x0 + u) âˆ’Tx0.
Then, for each u1, u2 âˆˆU and Î±1, Î±2 âˆˆR we have
C(Î±1u1 + Î±2u2) = T(x0 + Î±1u1 + Î±2u2) âˆ’Tx0
= T

Î±1(x0 + u1) + Î±2(x0 + u2) + (1 âˆ’Î±1 âˆ’Î±2)x0

âˆ’Tx0
= Î±1T(x0 + u1) + Î±2T(x0 + u2) + (1 âˆ’Î±1 âˆ’Î±2)Tx0 âˆ’Tx0
= Î±1

T(x0 + u1) âˆ’Tx0

+ Î±2

T(x0 + u2) âˆ’Tx0

= Î±1Cu1 + Î±2Cu2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
27
Convex sets
So the map C is linear on U and it can, of course, be extended to a linear
map on all of Rn.
For x âˆˆX we now obtain, since x âˆ’x0 belongs to U,
Tx = T(x0 + (x âˆ’x0)) = C(x âˆ’x0) + Tx0 = Cx âˆ’Cx0 + Tx0,
which proves the theorem with v equal to Tx0 âˆ’Cx0.
2.2
Convex sets
Basic deï¬nitions and properties
Deï¬nition. A subset X of Rn is called convex if [x, y] âŠ†X for all x, y âˆˆX.
In other words, a set X is convex if and only if it contains the line segment
between each pair of its points.
x
y
x
y
Figure 2.2. A convex set and a non-convex set
Example 2.2.1. Aï¬ƒne sets are obviously convex. In particular, the empty
set âˆ…, the entire space Rn and linear subspaces are convex sets. Open line
segments and closed line segments are clearly convex.
Example 2.2.2. Open balls B(a; r) (with respect to arbitrary norms âˆ¥Â·âˆ¥) are
convex sets. This follows from the triangle inequality and homogenouity, for
if x, y âˆˆB(a; r) and 0 â‰¤Î» â‰¤1, then
âˆ¥Î»x + (1 âˆ’Î»)y âˆ’aâˆ¥= âˆ¥Î»(x âˆ’a) + (1 âˆ’Î»)(y âˆ’a)âˆ¥
â‰¤Î»âˆ¥x âˆ’aâˆ¥+ (1 âˆ’Î»)âˆ¥y âˆ’aâˆ¥< Î»r + (1 âˆ’Î»)r = r,
which means that each point Î»x+(1âˆ’Î»)y on the segment [x, y] lies in B(a; r).
The corresponding closed balls B(a; r) = {x âˆˆRn | âˆ¥x âˆ’aâˆ¥â‰¤r} are of
course convex, too.
Deï¬nition. A linear combination y = m
j=1 Î±jxj of vectors x1, x2, . . . , xm is
called a convex combination if m
j=1 Î±j = 1 and Î±j â‰¥0 for all j.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
28
Convex sets
Theorem 2.2.1. A convex set contains all convex combinations of its ele-
ments.
Proof. Let X be an arbitrary convex set.
A convex combination of one
element is the element itself, and hence X contains all convex combinations
formed by just one element of the set.
Now assume inductively that X
contains all convex combinations that can be formed by m âˆ’1 elements of
X, and consider an arbitrary convex combination x = m
j=1 Î±jxj of m â‰¥2
elements x1, x2, . . . , xm in X. Since m
j=1 Î±j = 1, some coeï¬ƒcient Î±j must
be strictly less than 1, and assume without loss of generality that Î±m < 1,
and let s = 1 âˆ’Î±m = mâˆ’1
j=1 Î±j. Then s > 0 and mâˆ’1
j=1 Î±j/s = 1, which
means that
y =
mâˆ’1

j=1
Î±j
s xj
is a convex combination of mâˆ’1 elements in X. By the induction hypothesis,
y belongs to X. But x = sy+(1âˆ’s)xm, and it now follows from the convexity
deï¬nition that x belongs to X. This completes the induction step and the
proof of the theorem.
2.3
Convexity preserving operations
We now describe a number of ways to construct new convex sets from given
ones.
Image and inverse image under aï¬ƒne maps
Theorem 2.3.1. Let T : V â†’Rm be an aï¬ƒne map.
(i) The image T(X) of a convex subset X of V is convex.
(ii) The inverse image T âˆ’1(Y ) of a convex subset Y of Rm is convex.
Proof. (i) Suppose y1, y2 âˆˆT(X) and 0 â‰¤Î» â‰¤1. Let x1, x2 be points in X
such that yi = T(xi). Since
Î»y1 + (1 âˆ’Î»)y2 = Î»Tx1 + (1 âˆ’Î»)Tx2 = T(Î»x1 + (1 âˆ’Î»)x2)
and Î»x1 + (1 âˆ’Î»)x2 lies X, it follows that Î»y1 + (1 âˆ’Î»)y2 lies in T(X). This
proves that the image set T(X) is convex.
(ii) To prove the convexity of the inverse image T âˆ’1(Y ) we instead assume
that x1, x2 âˆˆT âˆ’1(Y ), i.e. that Tx1, Tx2 âˆˆY , and that 0 â‰¤Î» â‰¤1. Since Y
is a convex set,
T(Î»x1 + (1 âˆ’Î»)x2) = Î»Tx1 + (1 âˆ’Î»)Tx2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
29
Convex sets
29
is an element of Y , and this means that Î»x1 + (1 âˆ’Î»)x2 lies in T âˆ’1(Y ).
As a special case of the preceding theorem it follows that translations
a + X of a convex set X are convex.
Example 2.3.1. The sets
{x âˆˆRn | âŸ¨c, xâŸ©â‰¥b}
and
{x âˆˆRn | âŸ¨c, xâŸ©â‰¤b},
where b is an arbitrary real number and c = (c1, c2, . . . , cn) is an arbirary
nonzero vector, are called opposite closed halfspaces. Their complements, i.e.
{x âˆˆRn | âŸ¨c, xâŸ©< b}
and
{x âˆˆRn | âŸ¨c, xâŸ©> b},
are called open halfspaces.
The halfspaces {x âˆˆRn | âŸ¨c, xâŸ©â‰¥b} and {x âˆˆRn | âŸ¨c, xâŸ©> b} are inverse
images of the real intervals [b, âˆ[ and ]b, âˆ[, respectively, under the linear
map x â†’âŸ¨c, xâŸ©. It therefore follows from Theorem 2.3.1 that halfspaces are
convex sets.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
30
Convex sets
Intersection and union
Theorem 2.3.2. Let {Xi | i âˆˆI} be a family of convex subsets of Rn. The
intersection {Xi | i âˆˆI} is a convex set.
Proof. Suppose x, y are points in the intersection Y . The deï¬nition of an
intersection implies that x and y lie in Xi for all indices i âˆˆI, and convexity
implies that [x, y] âŠ†Xi for all i âˆˆI. Therefore, [x, y] âŠ†Y , again by the
deï¬nition of set intersection. This proves that the intersection is a convex
set.
A union of convex sets is, of course, in general not convex. However, there
is a trivial case when convexity is preserved, namely when the sets can be
ordered in such a way as to form an â€increasing chainâ€.
Theorem 2.3.3. Suppose {Xi | i âˆˆI} is a family of convex sets Xi and that
for each pair i, j âˆˆI either Xi âŠ†Xj or Xj âŠ†Xi. The union {Xi | i âˆˆI}
is then a convex set.
Proof. The assumptions imply that, for each pair of points x, y in the union
there is an index i âˆˆI such that both points belong to Xi. By convexity, the
entire segment [x, y] lies in Xi, and thereby also in the union.
Example 2.3.2. The convexity of closed balls follows from the convexity of
open balls, because B(a; r0) = {B(a; r) | r > r0}.
Conversely, the convexity of open balls follows from the convexity of closed
balls, since B(a; r0) = {B(a; r) | r < r0} and the sets B(a; r) form an
increasing chain.
Deï¬nition. A subset X of Rn is called a polyhedron if X can be written as
an intersection of ï¬nitely many closed halfspaces or if X = Rn.â€ 
Polyhedra are convex sets because of Theorem 2.3.2, and they can be
represented as solution sets to systems of linear inequalities. By multiplying
some of the inequalities by âˆ’1, if necessary, we may without loss of generality
assume that all inequalities are of the form c1x1 +c2x2 +Â· Â· Â·+cnxn â‰¥d. This
means that every polyedron is the solution set to a system of the following
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
31
Convex sets
Figure 2.3. A polyhedron in R2
form
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
c11x1 + c12x2 + Â· Â· Â· + c1nxn â‰¥b1
c21x1 + c22x2 + Â· Â· Â· + c2nxn â‰¥b2
...
cm1x1 + cm2x2 + Â· Â· Â· + cmnxn â‰¥bm,
or in matrix notation
Cx â‰¥b.
The intersection of ï¬nitely many polyhedra is clearly a polyhedron. Since
each hyperplane is the intersection of two opposite closed halfspaces, and
each aï¬ƒne set (except the entire space) is the intersection of ï¬nitely many
hyperplanes, it follows especially that aï¬ƒne sets are polyhedra. In particular,
the empty set is a polyhedron.
Cartesian product
Theorem 2.3.4. The Cartesian product X Ã— Y of two convex sets X and Y
is a convex set.
Proof. Suppose X lies in Rn and Y lies in Rm. The projections
P1 : Rn Ã— Rm â†’Rn
and
P2 : Rn Ã— Rm â†’Rm,
deï¬ned by P1(x, y) = x and P2(x, y) = y, are linear maps, and
X Ã— Y = (X Ã— Rm) âˆ©(Rn Ã— Y ) = P âˆ’1
1 (X) âˆ©P âˆ’1
2 (Y ).
The assertion of the theorem is therefore a consequence of Theorem 2.3.1
and Theorem 2.3.2.
Sum
Theorem 2.3.5. The sum X + Y of two convex subsets X and Y of Rn is
convex, and the product Î±X of a number Î± and a convex set X is convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
32
Convex sets
32
Proof. The maps S : Rn Ã— Rn â†’Rn and T : Rn â†’Rn, deï¬ned by S(x, y) =
x + y and Tx = Î±x, are linear. Since X + Y = S(X Ã— Y ) and Î±X = T(X),
our assertions follow from Theorems 2.3.1 and 2.3.4.
Example 2.3.3. The set X(r) of all points whose distance to a given set X
is less than the positive number r, can be written as a sum, namely
X(r) = X + B(0; r).
Since open balls are convex, we conclude from Theorem 2.3.5 that the set
X(r) is convex if X is a convex set.
Image and inverse image under the perspective map
Deï¬nition. The perspective map P : Rn Ã— R++ â†’Rn is deï¬ned by
P(x, t) = tâˆ’1x
for x âˆˆRn and t > 0.
The perspective map thus ï¬rst rescales points in RnÃ—R++ so that the last
coordinate becomes 1 and then throws the last coordinate away. Figure 2.4
illustrates the process.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
33
Convex sets
R++
1
Rn
(x, t)
P(x, t)
( 1
t x, 1)
y
P âˆ’1({y})
Figure 2.4. The perspective map P. The
inverse image of a point y âˆˆRn is a halï¬‚ine.
Theorem 2.3.6. Let X be a convex subset of Rn Ã— R++ and Y be a convex
subset of Rn. The image P(X) of X and the inverse image P âˆ’1(Y ) of Y
under the perspective map P : Rn Ã— R++ â†’Rn are convex sets.
Proof. To prove that the image P(X) is convex we assume that y, yâ€² âˆˆP(X)
and have to prove that the point Î»y + (1 âˆ’Î»)yâ€² lies in P(X) if 0 < Î» < 1.
To achieve this we ï¬rst note that there exist numbers t, tâ€² > 0 such that the
points (ty, t) and (tâ€²yâ€², tâ€²) belong to X, and then deï¬ne
Î± =
Î»tâ€²
Î»tâ€² + (1 âˆ’Î»)t.
Clearly 0 < Î± < 1, and it now follows from the convexity of X that the point
z = Î±(ty, t) + (1 âˆ’Î±)(tâ€²yâ€², tâ€²) =
ttâ€²(Î»y + (1 âˆ’Î»)yâ€²)
Î»tâ€² + (1 âˆ’Î»)t
,
ttâ€²
Î»tâ€² + (1 âˆ’Î»)t

lies X. Thus, P(z) âˆˆP(X), and since P(z) = Î»y + (1 âˆ’Î»)yâ€², we are done.
To prove that the inverse image P âˆ’1(Y ) is convex, we instead assume
that (x, t) and (xâ€², tâ€²) are points in P âˆ’1(Y ) and that 0 < Î» < 1. We will
prove that the point Î»(x, t) + (1 âˆ’Î»)(xâ€², tâ€²) lies in P âˆ’1(Y ).
To this end we note that the the points 1
tx and 1
tâ€²xâ€² belong to Y and that
Î± =
Î»t
Î»t + (1 âˆ’Î»)tâ€²
is a number between 0 and 1. Thus,
z = Î±1
t x + (1 âˆ’Î±)1
tâ€²xâ€² = Î»x + (1 âˆ’Î»)xâ€²
Î»t + (1 âˆ’Î»)tâ€²
is a point in Y by convexity, and consequently

(Î»t+(1âˆ’Î»)tâ€²)z, Î»t+(1âˆ’Î»)tâ€²
is a point in P âˆ’1(Y ). But

(Î»t + (1 âˆ’Î»)tâ€²)z, Î»t + (1 âˆ’Î»)tâ€²
= Î»(x, t) + (1 âˆ’Î»)(xâ€², tâ€²)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
34
Convex sets
and this completes the proof.
Example 2.3.4. The set {(x, xn+1) âˆˆRn Ã— R | âˆ¥xâˆ¥< xn+1} is the inverse
image of the unit ball B(0; 1) under the perspective map, and it is therefore
a convex set in Rn+1 for each particular choice of norm âˆ¥Â·âˆ¥. The following
convex sets are obtained by choosing the â„“1-norm, the Euclidean norm and
the maximum norm, respectively, as norm:
{x âˆˆRn+1 | xn+1 > |x1| + |x2| + Â· Â· Â· + |xn| },
{x âˆˆRn+1 | xn+1 > (x2
1 + x2
2 + Â· Â· Â· + x2
n)1/2 }
and
{x âˆˆRn+1 | xn+1 > max
1â‰¤iâ‰¤n |xi| }.
2.4
Convex hull
Deï¬nition. Let A be a nonempty set in Rn. The set of all convex combina-
tions Î»1a1+Î»2a2+Â· Â· Â·+Î»mam of an arbitrary number of elements a1, a2, . . . , am
in A is called the convex hull of A and is denoted by cvx A.
Moreover, to have the convex hull deï¬ned for the empty set, we deï¬ne
cvx âˆ…= âˆ….
Theorem 2.4.1. The convex hull cvx A is a convex set containing A, and it
is the smallest set with this property, i.e. if X is a convex set and A âŠ†X,
then cvx A âŠ†X.
Proof. cvx A is a convex set, because convex combinations of two elements
of the type m
j=1 Î»jaj, where m â‰¥1, Î»1, Î»2, . . . , Î»m â‰¥0, m
j=1 Î»j = 1 and
a1, a2, . . . , am âˆˆA, is obviously an element of the same type.
Moreover,
A âŠ†cvx A, because each element in A is a convex combination of itself
(a = 1a).
A convex set X contains all convex combinations of its elements, accord-
ing to Theorem 2.2.1. If A âŠ†X, then in particular X contains all convex
combinations of elements in A, which means that cvx A âŠ†X.
The convex hull of a set in Rn consists of all convex combinations of
an arbitrary number of elements in the set, but each element of the hull is
actually a convex combination of at most n + 1 elements.
Theorem 2.4.2. Let A âŠ†Rn and suppose that x âˆˆcvx A. Then A contains
a subset B with at most n + 1 elements such that x âˆˆcvx B.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
35
Convex sets
35
A
cvx A
Figure 2.5. A set and its convex hull
Proof. According to the deï¬nition of convex hull there exists a ï¬nite subset
B of A such that x âˆˆcvx B. Choose such a subset B = {b1, b2, . . . , bm} with
as few elements as possible. By the minimality assumption, x = m
j=1 Î»jbj
with m
j=1 Î»j = 1 and Î»j > 0 for all j.
Let cj = bj âˆ’bm for j = 1, 2, . . . , m âˆ’1.
We will show that the
set C = {c1, c2, . . . , cmâˆ’1} is a linearly independent subset of Rn, and this
obviously implies that m â‰¤n + 1.
Suppose on the contrary that the set C is linearly dependent. Then there
exist real numbers Âµj, not all of them equal to 0, such that mâˆ’1
j=1 Âµjcj = 0.
Now let Âµm = âˆ’mâˆ’1
j=1 Âµj; then m
j=1 Âµj = 0 and m
j=1 Âµjbj = 0. Moreover,
at least one of the m numbers Âµ1, Âµ2, . . . , Âµm is positive.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
36
Convex sets
Consider the numbers Î½j = Î»j âˆ’tÂµj for t > 0. We note that
m

j=1
Î½j =
m

j=1
Î»j âˆ’t
m

j=1
Âµj = 1
and
m

j=1
Î½jbj =
m

j=1
Î»jbj âˆ’t
m

j=1
Âµjbj = x.
Moreover, Î½j â‰¥Î»j > 0 if Âµj â‰¤0, and Î½j â‰¥0 if Âµj > 0 and t â‰¤Î»j/Âµj.
Therefore, by choosing t as the smallest number of the numbers Î»j/Âµj with
positive denominator Âµj, we obtain numbers Î½j such that Î½j â‰¥0 for all j and
Î½j0 = 0 for at least one index j0. This means that x is a convex combination
of elements in the set B \ {bj0}, which consists of m âˆ’1 elements. This
contradicts the minimality assumption concerning the set B, and our proof
by contradiction is ï¬nished.
2.5
Topological properties
Closure
Theorem 2.5.1. The closure cl X of a convex set X is convex.
Proof. We recall that cl X = 
r>0 X(r), where X(r) denotes the set of all
points whose distance from X is less than r. The sets X(r) are convex when
the set X is convex (see Example 2.3.3), and an intersection of convex sets
is convex.
Interior and relative interior
The interior of a convex set may be empty. For example, line segments in
Rn have no interior points when n â‰¥2. A necessary and suï¬ƒcient condition
for nonempty interior is given by the following theorem.
Theorem 2.5.2. A convex subset X of Rn has interior points if and only if
dim X = n.
Proof. If X has an interior point a, then there exists an open ball B = B(a; r)
around a such that B âŠ†X, which implies that dim X â‰¥dim B = n, i.e.
dim X = n.
To prove the converse, let us assume that dim X = n; we will prove that
int X Ì¸= âˆ…. Since the dimension of a set is invariant under translations and
int (a + X) = a + int X, we may assume that 0 âˆˆX.
Let {a1, a2, . . . , am} be a maximal set of linearly independent vectors in X;
then X is a subset of the linear subspace of dimension m which is spanned by
these vectors, and it follows from the dimensionality assumption that m = n.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
37
Convex sets
The set X contains the convex hull of the vectors 0, a1, a2, . . . , an as a subset,
and, in particular, it thus contains the nonempty open set
{Î»1a1 + Î»2a2 + Â· Â· Â· + Î»nan | 0 < Î»1 + Â· Â· Â· + Î»n < 1, Î»1 > 0, . . . , Î»n > 0}.
All points in this latter set are interior points of X, so int X Ì¸= âˆ….
A closed line segment [a, b] in the two-dimensional space R2 has no in-
terior points, but if we consider the line segment as a subset of a line and
identify the line with the space R, then it has interior points and its interior
is equal to the corresponding open line segment ]a, b[. A similar remark holds
for the convex hull T = cvx{a, b, c} of three noncolinear points in three-space;
the triangle T has interior points when viewed as a subset of R2, but it lacks
interior points as a subset of R3. This conï¬‚ict is unsatisfactory if we want a
concept that is independent of the dimension of the surrounding space, but
the dilemma disappears if we use the relative topology that the aï¬ƒne hull of
the set inherits from the surrounding space Rn.
Deï¬nition. Let X be an m-dimensional subset of Rn, and let V denote the
aï¬ƒne hull of X, i.e. V is the m-dimensional aï¬ƒne set that contains X.
A point x âˆˆX is called a relative interior point of X if there exists an
r > 0 such that B(x; r) âˆ©V âŠ†X, and the set of all relative interior points
of X is called the relative interior of X and is denoted by rint X.
A point x âˆˆRn is called a relative boundary point of X if, for every r > 0,
the intersection B(x; r) âˆ©V contains at least one point from X and at least
one point from V \ X. The set of all relative boundary points is called the
relative boundary of X and is denoted by rbdry X.
The relative interior of X is obviously a subset of X, and the relative
boundary of X is a subset of the boundary of X. It follows that
rint X âˆªrbdry X âŠ†X âˆªbdry X = cl X.
Conversely, if x is a point in the closure cl X, then for each r > 0
B(x, r) âˆ©V âˆ©X = B(x, r) âˆ©X Ì¸= âˆ….
Thus, x is either a relative boundary point or a relative interior point of X.
This proves the converse inclusion, and we conclude that
rint X âˆªrbdry X = cl X,
or equivalently, that
rbdry X = cl X \ rint X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
38
Convex sets
38
It follows from Theorem 2.5.2, with Rn replaced by aï¬€X, that every
nonempty convex set has a nonempty relative interior.
Note that the relative interior of a line segment [a, b] is the corresponding
open line segment ]a, b[, which is consistent with calling ]a, b[ an open seg-
ment. The relative interior of a set {a} consisting of just one point is the set
itself.
Theorem 2.5.3. The relative interior rint X of a convex set X is convex.
Proof. The theorem follows as a corollary of the following theorem, since
rint X âŠ†cl X.
Theorem 2.5.4. Suppose that X is a convex set, that a âˆˆrint X and that
b âˆˆcl X. The entire open line segment ]a, b[ is then a subset of rint X.
Proof. Let V = aï¬€X denote the aï¬ƒne set of least dimension that includes
X, and let c = Î»a + (1 âˆ’Î»)b, where 0 < Î» < 1, be an arbitrary point on the
open segment ]a, b[. We will prove that c is a relative interior point of X by
constructing an open ball B which contains c and whose intersection with V
is contained in X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
39
Convex sets
a
b
bâ€²
c
X
B(a; r)
B
Figure 2.6.
Illustration of the proof of Theorem 2.5.4.
The convex hull of the ball B(a; r) and the point bâ€² forms
a â€coneâ€ with nonempty interior that contains the point c.
To this end, we choose r > 0 such that B(a; r) âˆ©V âŠ†X and a point
bâ€² âˆˆX such that âˆ¥bâ€² âˆ’bâˆ¥< Î»r/(1 âˆ’Î»); this is possible since a is a relative
interior point of X and b is a point in the closure of X. Let
B = Î»B(a; r) + (1 âˆ’Î»)bâ€²,
and observe that B = B(Î»a + (1 âˆ’Î»)bâ€²; Î»r). The open ball B contains the
point c because
âˆ¥c âˆ’(Î»a + (1 âˆ’Î»)bâ€²)âˆ¥= âˆ¥(1 âˆ’Î»)(b âˆ’bâ€²)âˆ¥= (1 âˆ’Î»)âˆ¥b âˆ’bâ€²âˆ¥< Î»r.
Moreover, B âˆ©V = Î»(B(a; r) âˆ©V ) + (1 âˆ’Î»)bâ€² âŠ†Î»X + (1 âˆ’Î»)X âŠ†X, due
to convexity. This completes the proof.
Theorem 2.5.5. Let X be a convex set. Then
(i)
cl (rint X) = cl X;
(ii)
rint (cl X) = rint X;
(iii)
rbdry (cl X) = rbdry (rint X) = rbdry X.
Proof. The equalities in (iii) for the relative boundaries follow from the other
two and the deï¬nition of the relative boundary.
The inclusions cl (rint X) âŠ†cl X and rint X âŠ†rint (cl X) are both trivial,
because it follows, for arbitrary sets A and B, that A âŠ†B implies cl A âŠ†cl B
and rint A âŠ†rint B.
It thus only remains to prove the two inclusions
cl X âŠ†cl (rint X) and
rint (cl X) âŠ†rint X.
So ï¬x a point x0 âˆˆrint X.
If x âˆˆcl X, then every point on the open segment ]x0, x[ lies in rint X, by
Theorem 2.5.4, and it follows from this that the point x is either an interior
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
40
Convex sets
point or a boundary point of rint X, i.e. a point in the closure cl (rint X).
This proves the inclusion cl X âŠ†cl (rint X).
To prove the remaining inclusion rint (cl X) âŠ†rint X we instead assume
that x âˆˆrint (cl X) and deï¬ne yt = (1 âˆ’t)x0 + tx for t > 1. Since yt â†’x
as t â†’1, the points yt belong to cl X for all t suï¬ƒciently close to 1. Choose
a number t0 > 1 such that yt0 belongs to cl X. According to Theorem 2.5.4,
all points on the open segment ]x0, yt0[ are relative interior points in X, and
x is such a point since x =
1
t0yt0 + (1 âˆ’1
t0)x0. This proves the implication
x âˆˆrint (cl X) â‡’x âˆˆrint X, and the proof is now complete.
Compactness
Theorem 2.5.6. The convex hull cvx A of a compact subset A in Rn is com-
pact.
Proof. Let S = {Î» âˆˆRn+1 | Î»1, Î»2, . . . , Î»n+1 â‰¥0, n+1
j=1 Î»j = 1}, and let
f : S Ã— Rn Ã— Rn Ã— Â· Â· Â· Ã— Rn â†’Rn be the function
f(Î», x1, x2, . . . , xn+1) =
n+1

j=1
Î»jxj.
The function f is of course continuous, and the set S is compact, since it is
closed and bounded. According to Theorem 2.4.2, every element x âˆˆcvx A
can be written as a convex combination x = n+1
j=1 Î»jaj of at most n + 1
elements a1, a2, . . . , an+1 from the set A. This means that the convex hull
cvx A coincides with the image f(SÃ—AÃ—AÃ—Â· Â· Â·Ã—A) under f of the compact
set S Ã— A Ã— A Ã— Â· Â· Â· Ã— A.
Since compactness is preserved by continuous
functions, we conclude that the convex hull cvx A is compact.
2.6
Cones
Deï¬nition. Let x be a point in Rn diï¬€erent from 0. The set
âˆ’â†’x = {Î»x | Î» â‰¥0}
is called the ray through x, or the halï¬‚ine from the origin through x.
A cone X in Rn is a non-empty set which contains the ray through each
of its points.
A cone X is in other words a non-empty set which is closed under multi-
plication by nonnegative numbers, i.e. which satisï¬es the implication
x âˆˆX, Î» â‰¥0 â‡’Î»x âˆˆX.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
41
Convex sets
41
In particular, all cones contain the point 0.
We shall study convex cones. Rays and linear subspaces of Rn are convex
cones, of course. In particular, the entire space Rn and the trivial subspace
{0} are convex cones. Other simple examples of convex cones are provided
by the following examples.
Example 2.6.1. A closed halfspace {x âˆˆRn | âŸ¨c, xâŸ©â‰¥0}, which is bounded
by a hyperplane through the origin, is a convex cone and is called a conic
halfspace.
The union {x âˆˆRn | âŸ¨c, xâŸ©> 0} âˆª{0} of the corresponding open half-
space and the origin is also a convex cone.
Example 2.6.2. The nonnegative orthant
Rn
+ = {x = (x1, . . . , xn) âˆˆRn | x1 â‰¥0, . . . , xn â‰¥0}
in Rn is a convex cone.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
42
Convex sets
Deï¬nition. A cone that does not contain any line through 0, is called a proper
cone.â€¡
That a cone X does not contain any line through 0 is equivalent to the
condition
x, âˆ’x âˆˆX â‡’x = 0.
In other words, a cone X is a proper cone if and only if X âˆ©(âˆ’X) = {0}.
0
0
Figure 2.7. A plane cut through two proper convex cones in R3
Closed conic halfspaces in Rn are non-proper cones if n â‰¥2. The non-
negative orthant Rn
+ is a proper cone. The cones {x âˆˆRn | âŸ¨c, xâŸ©> 0} âˆª{0}
are also proper cones.
We now give two alternative ways to express that a set is a convex cone.
Theorem 2.6.1. The following three conditions are equivalent for a nonempty
subset X of Rn:
(i) X is a convex cone.
(ii) X is a cone and x + y âˆˆX for all x, y âˆˆX.
(iii) Î»x + Âµy âˆˆX for alla x, y âˆˆX and all Î», Âµ âˆˆR+.
Proof. (i) â‡’(ii): If X is a convex cone and x, y âˆˆX, then z = 1
2x + 1
2y
belongs to X because of convexity, and x + y (= 2z) belongs to X since X
is cone.
(ii) â‡’(iii): If (ii) holds, x, y âˆˆX and Î», Âµ âˆˆR+, then Î»x and Âµy belong to
X by the cone condition, and Î»x + Âµy âˆˆX by additivity.
(iii) â‡’(i): If (iii) holds, then we conclude that X is a cone by choosing y = x
and Âµ = 0, and that the cone is convex by choosing Î» + Âµ = 1.
Deï¬nition. A linear combination m
j=1 Î»jxj of vectors x1, x2, . . . , xm in Rn
is called a conic combination if all coeï¬ƒcients Î»1, Î»2, . . . , Î»m are nonnegative.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
43
Convex sets
Theorem 2.6.2. A convex cone contains all conic combinations of its ele-
ments.
Proof. Follows immediately by induction from the characterization of convex
cones in Theorem 2.6.1 (iii).
Cone preserving operations
The proofs of the four theorems below are analogous to the proofs of the cor-
responding theorems on convex sets, and they are therefore left as exercises.
Theorem 2.6.3. Let T : Rn â†’Rm be a linear map.
(i) The image T(X) of a convex cone X in Rn is a convex cone.
(ii) The inverse image T âˆ’1(Y ) of a convex cone in Rm is a convex cone.
Theorem 2.6.4. The intersection 
iâˆˆI Xi of an arbitrary family of convex
cones Xi in Rn is a convex cone.
Theorem 2.6.5. The Cartesian product X Ã— Y of two convex cones X and
Y is a convex cone.
Theorem 2.6.6. The sum X + Y of two convex cones X and Y in Rn is a
convex cone, and âˆ’X is a convex cone if X is a convex cone.
Example 2.6.3. An intersection
X =
m

i=1
{x âˆˆRn | âŸ¨ci, xâŸ©â‰¥0}
of ï¬nitely many closed conic halfspaces is called a polyhedral cone or a conic
polyhedron.
By deï¬ning C as the m Ã— n-matrix with rows c T
i , i = 1, 2, . . . , m, we can
write the above polyhedral cone X in a more compact way as
X = {x âˆˆRn | Cx â‰¥0}.
A polyhedral cone is in other words the solution set of a system of homoge-
neous linear inequalities.
Conic hull
Deï¬nition. Let A be an arbitrary nonempty subset of Rn. The set of all
conic combinations of elements of A is called the conic hull of A, and it is
denoted by con A. The elements of A are called generators of con A.
We extend the concept to the empty set by deï¬ning con âˆ…= {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
44
Convex sets
44
Theorem 2.6.7. The set con A is a convex cone that contains A as a subset,
and it is the smallest convex cone with this property, i.e. if X is a convex
cone and A âŠ†X, then con A âŠ†X.
Proof. A conic combination of two conic combinations of elements from A
is clearly a new conic combination of elements from A, and hence con A
is a convex cone. The inclusion A âŠ†con A is obvious. Since a convex cone
contains all conic combinations of its elements, a convex cone X that contains
A as a subset must in particular contain all conic combinations of elements
from A, which means that con A is a subset of X.
Theorem 2.6.8. Let X = con A be a cone in Rn, Y = con B be a cone in
Rm and T : Rn â†’Rm be a linear map. Then
(i)
T(X) = con T(A);
(ii)
X Ã— Y = con

(A Ã— {0}) âˆª({0} Ã— B)

;
(iii)
X +Y = con(AâˆªB),
provided that m = n so that the sum X +Y
is well-deï¬ned.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
45
Convex sets
Proof. (i) The cone X consists of all conic combinations x = p
j=1 Î»jaj of
elements aj in A. For such a conic combination Tx = p
j=1 Î»jTaj. The
image cone T(X) thus consists of all conic combinations of the elements
Taj âˆˆT(A), which means that T(X) = con T(A).
(ii) The cone X Ã— Y consists of all pairs (x, y) =
p
j=1 Î»jaj, q
k=1 Âµkbk

of conic combinations of elements in A and B, respectively. But
(x, y) =
p

j=1
Î»j(aj, 0) +
q

k=1
Âµk(0, bk),
and hence (x, y) is a conic combination of elements in (AÃ—{0}) âˆª({0}Ã—B),
that is (x, y) is an element of the cone Z = con

(AÃ—{0})âˆª({0}Ã—B)

. This
proves the inclusion X Ã— Y âŠ†Z.
The converse inclusion Z âŠ†X Ã— Y follows at once from the trivial in-
clusion (A Ã— {0}) âˆª({0} Ã— B) âŠ†X Ã— Y , and the fact that X Ã— Y is a
cone.
(iii) A typical element of X + Y has the form p
j=1 Î»jaj + q
k=1 Âµkbk,
which is a conic combination of elements in AâˆªB. This proves the assertion.
Finitely generated cones
Deï¬nition. A convex cone X is called ï¬nitely generated if X = con A for
some ï¬nite set A.
Example 2.6.4. The nonnegative orthant Rn
+ is ï¬nitely generated by the
standard basis e1, e2, . . . , en of Rn.
Theorem 2.6.8 has the following immediate corollary.
Corollary 2.6.9. Cartesian products X Ã— Y , sums X + Y and images T(X)
under linear maps T of ï¬nitely generated cones X and Y , are themselves
ï¬nitely generated cones.
Intersections X âˆ©Y and inverse images T âˆ’1(Y ) of ï¬nitely generated cones
are ï¬nitely generated, too, but the proof of this fact has to wait until we
have shown that ï¬nitely generated cones are polyhedral, and vice versa. See
Chapter 5.
Theorem 2.6.10. Suppose that x âˆˆcon A, where A is a subset of Rn. Then
x âˆˆcon B for some linearly independent subset B of A.
The number of
elements in B is thus at most equal to n.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
46
Convex sets
Proof. Since x is a conic combination of elements of A, x is per deï¬nition a
conic combination of ï¬nitely many elements chosen from A. Now choose a
subset B of A with as few elements as possible and such that x âˆˆcon B. We
will prove that the set B is linearly independent.
If B = âˆ…(i.e. if x = 0), then we are done, because the empty set is
linearly independent. So assume that B = {b1, b2, . . . , bm}, where m â‰¥1.
Then x = m
j=1 Î»jbj, where each Î»j > 0 due to the minimality assumption.
We will prove our assertion by contradiction. So, suppose that the set
B is linearly dependent.
Then there exist scalars Âµ1, Âµ2, . . . , Âµm, at least
one of them being positive, such that m
j=1 Âµjbj = 0, and it follows that
x = m
j=1(Î»j âˆ’tÂµj)bj for all t âˆˆR.
Now let t0 = min Î»j/Âµj, where the minimum is taken over all indices such
that Âµj > 0, and let j0 be an index where the minimum is achieved. Then
Î»j âˆ’t0Âµj â‰¥0 for all indices j, and Î»j0 âˆ’t0Âµj0 = 0. This means that x belongs
to the cone generated by the set B \ {bj0}, which contradicts the minimality
assumption about B. Thus, B is linearly independent.
Theorem 2.6.11. Every ï¬nitely generated cone is closed.
Proof. Let X be a ï¬nitely generated cone in Rn so that X = con A for some
ï¬nite set A.
We ï¬rst treat the case when A = {a1, a2, . . . , am} is a linearly independent
set. Then m â‰¤n, and it is possible to extend the set A, if necessary, with
vectors am+1, . . . , an to a basis for Rn. Let (c1(x), c2(x), . . . , cn(x)) denote
the coordinates of the vector x with respect to the basis a1, a2, . . . , an, so
that x = n
j=1 cj(x) aj. The coordinate functions cj(x) are linear forms on
Rn.
A vector x belongs to X if and only if x is a conic combination of the
ï¬rst m basis vectors, and this means that
X = {x âˆˆRn | c1(x) â‰¥0, . . . , cm(x) â‰¥0, cm+1(x) = Â· Â· Â· = cn(x) = 0}.
We conclude that X is equal to the intersection of the closed halfspaces
{x âˆˆRn | cj(x) â‰¥0}, 1 â‰¤j â‰¤m, and the hyperplanes {x âˆˆRn | cj(x) = 0},
m + 1 â‰¤j â‰¤n. This proves that X is a closed cone in the present case and
indeed a polyhedral cone.
Let us now turn to the general case. Let A be an arbitrary ï¬nite set.
By the previous theorem, there corresponds to each x âˆˆcon A a linearly
independent subset B of A such that x âˆˆcon B, and this fact implies that
con A =  con B, where the union is to be taken over all linearly independent
subsets B of A. Of course, there are only ï¬nitely many such subsets, and
hence con A is a union of ï¬nitely many cones con B, each of them being
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
47
Convex sets
47
closed, by the ï¬rst part of the proof. A union of ï¬nitely many closed sets is
closed. Hence, con A is a closed cone.
2.7
The recession cone
The recession cone of a set consists of the directions in which the set is
unbounded and in this way provides information about the behavior of the
set at inï¬nity. Here is the formal deï¬nition of the concept.
Deï¬nition. Let X be a subset of Rn and let v be a nonzero vector in Rn.
We say that the set X recedes in the direction of v and that v is a recession
vector of X if X contains all halï¬‚ines with direction vector v that start from
an arbitrary point of X.
The set consisting of all recession vectors of X and the zero vector is called
the recession cone and is denoted by recc X. Hence, if X is a nonempty set
then
recc X = {v âˆˆRn | x + tv âˆˆX for all x âˆˆX and all t > 0},
whereas recc âˆ…= {0}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
48
Convex sets
X
X
X
a
a
a
Figure 2.8.
Three convex sets X and the corresponding translated
recession cones a + recc X.
Theorem 2.7.1. The recession cone of an arbitrary set X is a convex cone
and
X = X + recc X.
Proof. That recc X is a cone follows immediately from the very deï¬nition of
recession cones, and the same holds for the inclusion X + recc X âŠ†X. The
converse inclusion X âŠ†X +Y is trivially true for all sets Y containing 0 and
thus in particular for the cone recc X.
If v and w are two recession vectors of X, x is an arbitrary point in X and
t is an arbitrary positive number, then ï¬rst x+tv belongs to X by deï¬nition,
and then x + t(v + w) = (x + tv) + tw belongs to X. This means that the
sum v + w is also a recession vector. So the recession cone has the additivity
property v, w âˆˆrecc X â‡’v + w âˆˆrecc X, which implies that the cone is
convex according to Theorem 2.6.1.
Example 2.7.1. Here are some simple examples of recession cones:
recc(R+ Ã— [0, 1]) = recc(R+Ã— ]0, 1[) = R+ Ã— {0},
recc(R+Ã— ]0, 1[ âˆª{(0, 0)}) = {(0, 0)},
recc{x âˆˆR2 | x2
1 + x2
2 â‰¤1} = {(0, 0)},
recc{x âˆˆR2 | x2 â‰¥x2
1} = {0} Ã— R+,
recc{x âˆˆR2 | x2 â‰¥1/x1, x1 > 0} = R2
+.
The computation of the recession cone of a convex set is simpliï¬ed by the
following theorem.
Theorem 2.7.2. A vector v is a recession vector of a nonempty convex set
X if and only if x + v âˆˆX for all x âˆˆX.
Proof. If v is a recession vector, then obviously x + v âˆˆX for all x âˆˆX.
To prove the converse, assume that x + v âˆˆX for all x âˆˆX, and let x
be an arbitrary point in X. Then, x + nv âˆˆX for all natural numbers n,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
49
Convex sets
by induction, and since X is a convex set, we conclude that the closed line
segment [x, x+nv] lies in X for all n. Of course, this implies that x+tv âˆˆX
for all positive numbers t, and hence v is a recession vector of X.
Corollary 2.7.3. If X is a convex cone, then recc X = X.
Proof. The inclusion recc X âŠ†X holds for all sets X containing 0 and thus
in particular for cones X. The converse inclusion X âŠ†recc X is according to
Theorem 2.7.2 a consequence of the additivity property x, v âˆˆX â‡’x+v âˆˆX
for convex cones.
Example 2.7.2. recc R2
+ = R2
+,
recc(R2
++ âˆª{(0, 0)}) = R2
++ âˆª{(0, 0)}.
The recession vectors of a closed convex set are characterized by the
following theorem.
Theorem 2.7.4. Let X be a nonempty closed convex set. The following three
conditions are equivalent for a vector v.
(i) v is a recession vector of X.
(ii) There exists a point x âˆˆX such that x + nv âˆˆX for all n âˆˆZ+.
(iii) There exist a sequence (xn)âˆ
1 of points xn in X and a sequence (Î»n)âˆ
1
of positive numbers such that Î»n â†’0 and Î»nxn â†’v as n â†’âˆ.
Proof. (i) â‡’(ii): Trivial, since x + tv âˆˆX for all x âˆˆX and all t âˆˆR+, if v
is a recession vector of X.
(ii) â‡’(iii): If (ii) holds, then condition (iii) is satisï¬ed by the points xn =
x + nv and the numbers Î»n = 1/n.
(iii) â‡’(i): Assume that (xn)âˆ
1 and (Î»n)âˆ
1 are sequences of points in X and
positive numbers such that Î»n â†’0 and Î»nxn â†’v as n â†’âˆ, and let x be
an arbitrary point in X. The points zn = (1 âˆ’Î»n)x + Î»nxn then lie in X for
all suï¬ƒciently large n, and since zn â†’x + v as n â†’âˆand X is a closed
set, it follows that x + v âˆˆX. Hence, v is a recession vector of X according
to Theorem 2.7.2.
Theorem 2.7.5. The recession cone recc X of a closed convex set X is a
closed convex cone.
Proof. The case X = âˆ…is trivial, so assume that X is a nonempty closed
convex set. To prove that the recession cone recc X is closed, we assume that
v is a boundary point of the cone and choose a sequence (vn)âˆ
1 of recession
vectors that converges to v as n â†’âˆ.
If x is an arbitrary point in X,
then the points x + vn lie in X for each natural number n, and this implies
that their limit point x + v lies in X, since X is a closed set. Hence, v is a
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
50
Convex sets
50
recession vector, i.e. v belongs to the recession cone recc X. This proves that
the recession cone contains all its boundary points.
Theorem 2.7.6. Let {Xi | i âˆˆI} be a family of closed convex sets, and as-
sume that their intersection is nonempty. Then recc(
iâˆˆI Xi) = 
iâˆˆI recc Xi.
Proof. Let x0 be a point in 
i Xi. By Theorem 2.7.4, v âˆˆrecc(
i Xi) if and
only if x0 + nv lies in Xi for all positive integers n and all i âˆˆI, and this
holds if and only if v âˆˆrecc Xi for all i âˆˆI.
The recession cone of a polyhedron is given by the following theorem.
Theorem 2.7.7. If X = {x âˆˆRn | Cx â‰¥b} is a nonempty polyhedron, then
recc X = {x âˆˆRn | Cx â‰¥0}.
Proof. The recession cone of a closed halfspace is obviously equal to the cor-
responding conical halfspace. The theorem is thus an immediate consequence
of Theorem 2.7.6.
Note that the recession cone of a subset Y of X can be bigger than the
recession cone of X. For example,
recc R2
++ = R2
+ âŠ‹R2
++ âˆª{(0, 0)} = recc(R2
++ âˆª{(0, 0)}).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
51
Convex sets
However, this cannot occur if the superset X is closed.
Theorem 2.7.8. (i) Suppose that X is a closed convex set and that Y âŠ†X.
Then recc Y âŠ†recc X.
(ii) If X is a convex set, then recc(rint X) = recc(cl X).
Proof. (i) The case Y = âˆ…being trivial, we assume that Y is a nonempty
subset of X and that y is an arbitrary point in Y . If v is a recession vector of
Y , then y +nv are points in Y and thereby also in X for all natural numbers
n. We conclude from Theorem 2.7.4 that v is a recession vector of X.
(ii) The inclusion recc(rint X) âŠ†recc(cl X) follows from part (i), because
cl X is a closed convex subset.
To prove the converse inclusion, assume that v is a recession vector of
cl X, and let x be an arbitrary point in rint X. Then x + 2v belongs to cl X,
so it follows from Theorem 2.5.4 that the open line segment ]x, x + 2v[ is
a subset of rint X, and this implies that the point x + v belongs to rint X.
Thus, x âˆˆrint X â‡’x + v âˆˆrint X, and we conclude from Theorem 2.7.2
that v is a recession vector of rint X.
Theorem 2.7.9. Let X be a closed convex set. Then X is bounded if and
only if recc X = {0}.
Proof. Obviously, recc X = {0} if X is a bounded set. So assume that X
is unbounded. Then there exists a sequence (xn)âˆ
1 of points in X such that
âˆ¥xnâˆ¥â†’âˆas n â†’âˆ. The bounded sequence (xn/âˆ¥xnâˆ¥)âˆ
1 has a convergent
subsequence, and by deleting elements, if necessary, we may as well assume
that the original sequence is convergent. The limit v is a vector of norm 1,
which guarantees that v Ì¸= 0. With Î»n = 1/âˆ¥xnâˆ¥we now have a sequence
of points xn in X and a sequence of positive numbers Î»n such that Î»n â†’0
and Î»nxn â†’v as n â†’âˆ, and this means that v is a recession vector of X
according to Theorem 2.7.4. Hence, recc X Ì¸= {0}.
Deï¬nition. Let X be an arbitary set. The intersection recc X âˆ©(âˆ’recc X) is
a linear subspace, which is called the recessive subspace of X and is denoted
lin X.
A closed convex set is called line-free if lin X = {0}. The set X is in
other words line-free if and only if recc X is a proper cone.
If X is a nonempty closed convex subset of Rn and x âˆˆX is arbitrary,
then clearly
lin X = {x âˆˆRn | a + tx âˆˆX for all t âˆˆR}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
52
Convex sets
The image T(X) of a closed convex set X under a linear map T is not
necessarily closed. A counterexample is given by X = {x âˆˆR2
+ | x1x2 â‰¥1}
and the projection T(x1, x2) = x1 of R2 onto the ï¬rst factor, the image being
T(X) = ]0 , âˆ[. The reason why the image is not closed in this case is the
fact that X has a recession vector v = (0, 1) which is mapped on 0 by T.
However, we have the following general result, where N(T) denotes the
null space of the map T, i.e. N(T) = {x | Tx = 0}.
Theorem 2.7.10. Let T : Rn â†’Rm be a linear map, let X be a closed convex
subset of Rn, and suppose that
N(T) âˆ©recc X âŠ†lin X.
The image T(X) is then a closed set, and
recc T(X) = T(recc X).
In particular, the image T(X) is closed if X is a closed convex set and
x = 0 is the only vector in recc X such that Tx = 0.
Proof. The intersection
L = N(T) âˆ©lin X = N(T) âˆ©recc X
is a linear subspace of Rn. Let LâŠ¥denote its orthogonal complement. Then
X = X âˆ©L + X âˆ©LâŠ¥, and
T(X) = T(X âˆ©LâŠ¥),
since Tx = 0 for all x âˆˆL.
Let y be an arbitrary boundary point of the image T(X). Due to the
equality above, there exists a sequence (xn)âˆ
1 of points xn âˆˆX âˆ©LâŠ¥such
that limnâ†’âˆTxn = y.
We claim that the sequence (xn)âˆ
1
is bounded.
Assume the contrary.
The sequence (xn)âˆ
1 has then a subsequence (xnk)âˆ
1 such that âˆ¥xnkâˆ¥â†’âˆ
as k â†’âˆand the bounded sequence (xnk/âˆ¥xnkâˆ¥)âˆ
1 converges. The limit v
is, of course, a vector of norm 1 in the linear subspace LâŠ¥. Moreover, since
xnk âˆˆX and 1/âˆ¥xnkâˆ¥â†’0, it follows from Theorem 2.7.4 that v âˆˆrecc X.
Finally,
Tv = lim
kâ†’âˆT(xnk/âˆ¥xnkâˆ¥) = lim
kâ†’âˆâˆ¥xnkâˆ¥âˆ’1Txnk = 0 Â· y = 0,
and hence v belongs to N(T), and thereby also to L.
This means that
v âˆˆL âˆ©LâŠ¥, which contradicts tha fact that v Ì¸= 0, since L âˆ©LâŠ¥= {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
53
Convex sets
53
The sequence (xn)âˆ
1 is thus bounded. Let (xnk)âˆ
1 be a convergent subse-
quence, and let x = limkâ†’âˆxnk. The limit x lies in X since X is closed, and
y = limkâ†’âˆTxnk = Tx, which implies that y âˆˆT(X). This proves that the
image T(X) contains its boundary points, so it is a closed set.
The inclusion T(recc X) âŠ†recc T(X) holds for all sets X. To prove this,
assume that v is a recession vector of X and let y be an arbitrary point in
T(X). Then y = Tx for some point x âˆˆX, and since x+tv âˆˆX for all t > 0
and y + tTv = T(x + tv), we conclude that the points y + tTv lie in T(X)
for all t > 0, and this means that Tv is a recession vector of T(X).
To prove the converse inclusion recc T(X) âŠ†T(recc X) for closed convex
sets X and linear maps T fulï¬lling the assumptions of the theorem, we assume
that w âˆˆrecc T(X) and shall prove that there is a vector v âˆˆrecc X such
that w = Tv.
We ï¬rst note that there exists a sequence (yn)âˆ
1 of points yn âˆˆT(X) and
a sequence (Î»n)âˆ
1 of positive numbers such that Î»n â†’0 and Î»nyn â†’w as
n â†’âˆ. For each n, choose a point xn âˆˆX âˆ©LâŠ¥such that yn = T(xn).
The sequence (Î»nxn)âˆ
1 is bounded. Because assume the contrary; then
there is a subsequence such that âˆ¥Î»nkxnkâˆ¥â†’âˆand (xnk/âˆ¥xnkâˆ¥)âˆ
1 converges
to a vector z as k â†’âˆ. It follows from Theorem 2.7.4 that z âˆˆrecc X,
because the xnk are points in X
and âˆ¥xnkâˆ¥â†’âˆas k â†’âˆ. The limit z
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
54
Convex sets
belongs to the subspace LâŠ¥, and since
Tz = lim
kâ†’âˆT(xnk/âˆ¥xnkâˆ¥) = lim
kâ†’âˆT(Î»nkxnk/âˆ¥Î»nkxnkâˆ¥)
= lim
kâ†’âˆÎ»nkynk/âˆ¥Î»nkxnkâˆ¥= 0 Â· w = 0,
we also have z âˆˆN(T) âˆ©recc X = L.
Hence, z âˆˆL âˆ©LâŠ¥= {0}, which
contradicts the fact that âˆ¥zâˆ¥= 1.
The sequence (Î»nxn)âˆ
1 , being bounded, has a subsequence that converges
to a vector v, which belongs to recc X according to Theorem 2.7.4. Since
T(Î»nxn) = Î»nyn â†’w, we conclude that Tv = w. Hence, w âˆˆT(recc X).
Theorem 2.7.11. Let X and Y be nonempty closed convex subsets of Rn and
suppose that
x âˆˆrecc X & y âˆˆrecc Y & x + y = 0
â‡’
x âˆˆlin X & y âˆˆlin Y.
The sum X + Y is then a closed convex set and
recc(X + Y ) = recc X + recc Y.
Remark. The assumption of the theorem is fulï¬lled if recc X and âˆ’recc Y
have no common vector other than the zero vector.
Proof. Let T : Rn Ã— Rn â†’Rn be the linear map T(x, y) = x + y. We leave
as an easy exercise to show that recc(X Ã— Y ) = recc X Ã— recc Y and that
lin(XÃ—Y ) = lin XÃ—lin Y . Since N(T) = {(x, y) | x+y = 0}, the assumption
of the theorem yields the inclusion N(T) âˆ©recc(X Ã— Y ) âŠ†lin(X Ã— Y ), and
it now follows from Theorem 2.7.10 that T(X Ã— Y ), i.e. the sum X + Y , is
closed and that
recc(X + Y ) = recc T(X Ã— Y ) = T(recc(X Ã— Y )) = T(recc X Ã— recc Y )
= recc X + recc Y.
Corollary 2.7.12. The sum X + Y of a nonempty closed convex set X and
a nonempty compact convex set Y is a closed convex set and
recc(X + Y ) = recc X.
Proof. The assumptions of Theorem 2.7.11 are trivially fulï¬lled, because
recc Y = {0}.
Theorem 2.7.13. Suppose that C is a closed convex cone and that Y is a
nonempty compact convex set. Then recc(C + Y ) = C.
Proof. The corollary is a special case of Corollary 2.7.12 since recc C = C.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
55
Convex sets
Exercises
2.1 Prove that the set {x âˆˆR2
+ | x1x2 â‰¥a} is convex and, more generally, that
the set {x âˆˆRn
+ | x1x2 Â· Â· Â· xn â‰¥a} is convex.
Hint: Use the inequality xÎ»
i y1âˆ’Î»
i
â‰¤Î»xi + (1 âˆ’Î»)yi; see Theorem 6.4.1.
2.2 Determine the convex hull cvx A for the following subsets A of R2:
a) A = {(0, 0), (1, 0), (0, 1)}
b) A = {x âˆˆR2 | âˆ¥xâˆ¥= 1}
c) A = {x âˆˆR2
+ | x1x2 = 1} âˆª{(0, 0)}.
2.3 Give an example of a closed set with a non-closed convex hull.
2.4 Find the inverse image P âˆ’1(X) of the convex set X = {x âˆˆR2
+ | x1x2 â‰¥1}
under the perspective map P : R2 Ã— R++ â†’R2.
2.5 Prove that the set {x âˆˆRn+1 |
n
j=1 x2
j
1/2 â‰¤xn+1} is a cone.
2.6 Let e1, e2, . . . , en denote the standard basis in Rn and let e0 = âˆ’n
1 ej.
Prove that Rn is generated as a cone by the n + 1 vectors e0, e1, e2, . . . , en.
2.7 Prove that each conical halfspace in Rn is the conic hull of a set consisting
of n + 1 elements.
2.8 Prove that each closed cone in R2 is the conic hull of a set consisting of at
most three elements.
2.9 Prove that the sum of two closed cones in R2 is a closed cone.
2.10 Find recc X and lin X for the following convex sets:
a) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¥2, x2 â‰¥âˆ’1}
b) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¤2, x2 â‰¥âˆ’1}
c) X = {x âˆˆR3 | 2x1 + x2 + x3 â‰¤4, x1 + 2x2 + x3 â‰¤4}
d) X = {x âˆˆR3 | x2
1 âˆ’x2
2 â‰¥1, x1 â‰¥0}.
2.11 Let X and Y be arbitrary nonempty sets. Prove that
recc(X Ã— Y ) = recc X Ã— recc Y
and that
lin(X Ã— Y ) = lin X Ã— lin Y .
2.12 Let P : Rn Ã— R++ â†’Rn be the perspective map. Suppose X is a convex
subset of Rn, and let c(X) = P âˆ’1(X) âˆª{(0, 0)}.
a) Prove that c(X) is a cone and, more precisely, that c(X) = con(X Ã—{1}).
b) Find an explicit expression for the cones c(X) and cl(c(X)) if
(i) n = 1 and X = [2, 3];
(ii) n = 1 and X = [2, âˆ[;
(iii) n = 2 and X = {x âˆˆR2 | x1 â‰¥x2
2}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
56
Convex sets
56
c) Find c(X) if X = {x âˆˆRn | âˆ¥xâˆ¥â‰¤1} and âˆ¥Â·âˆ¥is an arbitrary norm on
Rn.
d) Prove that cl(c(X)) = c(cl X) âˆª(recc(cl X) Ã— {0}).
e) Prove that cl(c(X)) = c(cl X) if and only if X is a bounded set.
f) Prove that the cone c(X) is closed if and only if X is compact.
2.13 Y = {x âˆˆR3 | x1x3 â‰¥x2
2, x3 > 0} âˆª{x âˆˆR3 | x1 â‰¥0, x2 = x3 = 0} is a
closed cone. (Cf. problem 2.12 b) (iii)). Put
Z = {x âˆˆR3 | x1 â‰¤0, x2 = x3 = 0}.
Show that
Y + Z = {x âˆˆR3 | x3 > 0} âˆª{x âˆˆR3 | x2 = x3 = 0},
with the conclusion that the sum of two closed cones in R3 is not necessarily
a closed cone.
2.14 Prove that the sum X + Y of an arbitrary closed set X and an arbitrary
compact set Y is closed.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
57
Separation
Chapter 3
Separation
3.1
Separating hyperplanes
Deï¬nition. Let X and Y be two sets in Rn. We say that the hyperplane H
separarates the two sets if the following two conditionsâ€  are satisï¬ed:
(i) X is contained in one of the two opposite closed halvspaces deï¬ned by
H and Y is contained in the other closed halfspace;
(ii) X and Y are not both subsets of the hyperplane H.
The separation is called strict if there exist two parallell hyperplanes to
H, one on each side of H, that separates X and Y .
The hyperplane H = {x | âŸ¨c, xâŸ©= b} thus separates the two sets X and
Y , if âŸ¨c, xâŸ©â‰¤b â‰¤âŸ¨c, yâŸ©for all x âˆˆX and all y âˆˆY and âŸ¨c, xâŸ©Ì¸= b for some
element x âˆˆX âˆªY .
The separation is strict if there exist numbers b1 < b < b2 such that
âŸ¨c, xâŸ©â‰¤b1 < b2 â‰¤âŸ¨c, yâŸ©for all x âˆˆX, y âˆˆY .
X
Y
H
Figure 3.1. A strictly separating hyperplane H
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
58
Separation
The existence of separating hyperplanes is in a natural way connected to
extreme values of linear functions.
Theorem 3.1.1. Let X and Y be two nonempty subsets of Rn.
(i) There exists a hyperplane that separates X and Y if and only if there
exists a vector c such that
sup
xâˆˆX
âŸ¨c, xâŸ©â‰¤inf
yâˆˆY âŸ¨c, yâŸ©
and
inf
xâˆˆXâŸ¨c, xâŸ©< sup
yâˆˆY
âŸ¨c, yâŸ©.
(ii) There exists a hyperplane that separates X and Y strictly if and only if
there exists a vector c such that
sup
xâˆˆX
âŸ¨c, xâŸ©< inf
yâˆˆY âŸ¨c, yâŸ©.
Proof. A vector c that satisï¬es the conditions in (i) or (ii) is nonzero, of
course.
Suppose that c satisï¬es the conditions in (i) and choose the number b
so that supxâˆˆXâŸ¨c, xâŸ©â‰¤b â‰¤infyâˆˆY âŸ¨c, yâŸ©. Then âŸ¨c, xâŸ©â‰¤b for all x âˆˆX and
âŸ¨c, yâŸ©â‰¥b for all y âˆˆY . Moreover, âŸ¨c, xâŸ©Ì¸= b for some x âˆˆX âˆªY because
of the second inequality in (i). The hyperplane H = {x | âŸ¨c, xâŸ©= b} thus
separates the two sets X and Y .
If c satisï¬es the condition in (ii), we choose instead b so that
supxâˆˆXâŸ¨c, xâŸ©< b < infyâˆˆY âŸ¨c, yâŸ©,
and now conclude that the hyperplane H separates X and Y strictly.
Conversely, if the hyperplane H separates X and Y , then, by changing the
signs of c and b if necessary, we may assume that âŸ¨c, xâŸ©â‰¤b for all x âˆˆX and
âŸ¨c, yâŸ©â‰¥b for all y âˆˆY , and this implies that supxâˆˆXâŸ¨c, xâŸ©â‰¤b â‰¤infyâˆˆY âŸ¨c, yâŸ©.
Since H does not contain both X and Y , there are points x1 âˆˆX and y1 âˆˆY
with âŸ¨c, x1âŸ©< âŸ¨c, y1âŸ©, and this gives us the second inequality in (i).
If the separation is strict, then there exist two parallel hyperplanes Hi =
{x | âŸ¨c, xâŸ©= bi} with b1 < b < b2 that separate X and Y . Assuming that X
lies in the halfspace {x | âŸ¨c, xâŸ©â‰¤b1} , we conclude that
supxâˆˆXâŸ¨c, xâŸ©â‰¤b1 < b < b2 â‰¤infyâˆˆY âŸ¨c, yâŸ©,
i.e. the vector c satisï¬es the condition in (ii).
The following simple lemma reduces the problem of separating two sets
to the case when one of the sets consists of just one point.
Lemma 3.1.2. Let X and Y be two nonempty sets.
(i) If there exists a hyperplane that separates 0 from the set X âˆ’Y , then
there exists a hyperplane that separates X and Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
59
Separation
59
(ii) If there exists a hyperplane that strictly separates 0 from the set X âˆ’Y ,
then there exists a hyperplane that strictly separates X and Y .
Proof. (i) If there exists a hyperplane that separates 0 from X âˆ’Y , then by
Theorem 3.1.1 there exists a vector c such that
ï£±
ï£´
ï£²
ï£´
ï£³
0 = âŸ¨c, 0âŸ©â‰¤
inf
xâˆˆX, yâˆˆY âŸ¨c, x âˆ’yâŸ©= inf
xâˆˆXâŸ¨c, xâŸ©âˆ’sup
yâˆˆY
âŸ¨c, yâŸ©
0 = âŸ¨c, 0âŸ©<
sup
xâˆˆX, yâˆˆY
âŸ¨c, x âˆ’yâŸ©= sup
xâˆˆX
âŸ¨c, xâŸ©âˆ’inf
yâˆˆY âŸ¨c, yâŸ©
i.e. supyâˆˆY âŸ¨c, yâŸ©â‰¤infxâˆˆXâŸ¨c, xâŸ©and infyâˆˆY âŸ¨c, yâŸ©< supxâˆˆXâŸ¨c, xâŸ©, and we con-
clude that there exists a hyperplane that separates X and Y .
(ii) If instead there exists a hyperplane that strictly separates 0 from
X âˆ’Y , then there exists a vector c such that
0 = âŸ¨c, 0âŸ©<
inf
xâˆˆX, yâˆˆY âŸ¨c, x âˆ’yâŸ©= inf
xâˆˆXâŸ¨c, xâŸ©âˆ’sup
yâˆˆY
âŸ¨c, yâŸ©
and it now follows that supyâˆˆY âŸ¨c, yâŸ©< infxâˆˆXâŸ¨c, xâŸ©, which shows that Y and
X can be strictly separated by a hyperplane.
Our next theorem is the basis for our results on separation of convex sets.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
60
Separation
Theorem 3.1.3. Suppose X is a convex set and that a /âˆˆcl X. Then there
exists a hyperplane H that strictly separates a and X.
Proof. The set cl X is closed and convex, and a hyperplane that strictly
separates a and cl X will, of course, also strictly separate a and X, since X
is a subset of cl X. Hence, it suï¬ƒces to prove that we can strictly separate a
point a from each closed convex set that does not contain the point.
We may therefore assume, without loss of generality, that the convex set
X is closed and nonempty. Deï¬ne d(x) = âˆ¥x âˆ’aâˆ¥2, i.e. d(x) is the square
of the Euclidean distance between x and a. We start by proving that the
restriction of the continuous function d(Â·) to X has a minimum point.
To this end, choose a positive real number r so big that the closed ball
B(a; r) intersects the set X. Then d(x) > r2 for all x âˆˆX \ B(a; r), and
d(x) â‰¤r2 for all x âˆˆX âˆ©B(a; r). The restriction of d to the compact set
X âˆ©B(a; r) has a minimum point x0 âˆˆX âˆ©B(a; r), and this point is clearly
also a minimum point for d restricted to X, i.e. the inequality d(x0) â‰¤d(x)
holds for all x âˆˆX.
Now, let c = a âˆ’x0.
We claim that âŸ¨c, x âˆ’x0âŸ©â‰¤0 for all x âˆˆX.
Therefore, assume the contrary, i.e. that there is a point x1 âˆˆX such that
âŸ¨c, x1 âˆ’x0âŸ©> 0. We will prove that this assumption yields a contradiction.
Consider the points xt = tx1+(1âˆ’t)x0. They belong to X when 0 â‰¤t â‰¤1
because of convexity. Let f(t) = d(xt) = âˆ¥xt âˆ’aâˆ¥2. Then
f(t) = âˆ¥xt âˆ’aâˆ¥2 = âˆ¥t(x1 âˆ’x0) + (x0 âˆ’a)âˆ¥2 = âˆ¥t(x1 âˆ’x0) âˆ’câˆ¥2
= t2âˆ¥x1 âˆ’x0âˆ¥2 âˆ’2t âŸ¨c, x1 âˆ’x0âŸ©+ âˆ¥câˆ¥2.
The function f(t) is a quadratic polynomial in t, and its derivative at 0
satisï¬es f â€²(0) = âˆ’2 âŸ¨c, y1 âˆ’y0âŸ©< 0. Hence, f(t) is strictly decreasing in a
neighbourhood of t = 0, which means that d(xt) < d(x0) for all suï¬ƒciently
small positive numbers t.
This is a contradiction to x0 being the minimum point of the function
and proves our assertion that âŸ¨c, x âˆ’x0âŸ©â‰¤0 for all x âˆˆX. Consequently,
x0
a
x1
xt
c
X
Figure 3.2. Illustration for the proof of Theorem 3.1.3.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
61
Separation
âŸ¨c, xâŸ©â‰¤âŸ¨c, x0âŸ©= âŸ¨c, aâˆ’câŸ©= âŸ¨c, aâŸ©âˆ’âˆ¥câˆ¥2 for all x âˆˆX, and this implies that
supxâˆˆXâŸ¨c, xâŸ©< âŸ¨c, aâŸ©. So there exists a hyperplane that strictly separates a
from X according to Theorem 3.1.1.
Deï¬nition. Let X be a subset of Rn and let x0 be a point in X. A hyperplane
H through x0 is called a supporting hyperplane of X, if it separates x0 and
X. We then say that the hyperplane H supports X at the point x0.
The existence of a supporting hyperplane of X at x0 is clearly equivalent
to the condition that there exists a vector c such that
âŸ¨c, x0âŸ©= inf
xâˆˆXâŸ¨c, xâŸ©
and
âŸ¨c, x0âŸ©< sup
xâˆˆX
âŸ¨c, xâŸ©.
The hyperplane {x | âŸ¨c, xâŸ©= âŸ¨c, x0âŸ©} is then a supporting hyperplane.
X
x0
H
Figure 3.3. A supporting hyperplane of X at the point x0
If a hyperplane supports the set X at the point x0, then x0 is necessarily
a relative boundary point of X. For convex sets the following converse holds.
Theorem 3.1.4. Suppose that X is a convex set and that x0 âˆˆX is a relative
boundary point of X. Then there exists a hyperplane H that supports X at
the point x0.
Proof. First suppose that the dimension of X equals the dimension of the
surrounding space Rn. Since x0 is then a boundary point of X, there exists
a sequence (xn)âˆ
1 of points xn /âˆˆcl X that converges to x0 as n â†’âˆ, and
by Theorem 3.1.3 there exists, for each n â‰¥1, a hyperplane which strictly
separates xn and X. Theorem 3.1.1 thus gives us a sequence (cn)âˆ
1 of vectors
such that
(3.1)
âŸ¨cn, xnâŸ©< âŸ¨cn, xâŸ©
for all x âˆˆX
and all n â‰¥1, and we can obviously normalize the vectors cn so that âˆ¥cnâˆ¥= 1
for all n.
The unit sphere {x âˆˆRn | âˆ¥xâˆ¥= 1} is compact. Hence, by the Bolzanoâ€“
Weierstrass theorem, the sequence (cn)âˆ
1 has a subsequence (cnk)âˆ
k=1 which
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
62
Separation
62
converges to some vector c of length âˆ¥câˆ¥= 1. Clearly limkâ†’âˆxnk = x0, so by
going to the limit in the inequality (3.1) we conclude that âŸ¨c, x0âŸ©â‰¤âŸ¨c, xâŸ©for
all x âˆˆX. The set X is therefore a subset of one of the two closed halfspaces
determined by the hyperplane H = {x âˆˆRn | âŸ¨c, xâŸ©= âŸ¨c, x0âŸ©}, and X is
not a subset of H, since dim X = n. The hyperplane H is consequently a
supporting hyperplane of X at the point x0.
Next suppose that dim X < n. Then there exists an aï¬ƒne subspace a+U
that contains X, where U is a linear subspace of Rn and dim U = dim X.
Consider the set Y = X + U âŠ¥, where U âŠ¥is the orthogonal complement of
U. Compare with ï¬gure 3.4. Y is a â€cylinderâ€ with X as â€baseâ€, and each
y âˆˆY has a unique decomposition of the form y = x + v with x âˆˆX and
v âˆˆU âŠ¥.
The set Y is a convex set of dimension n with x0 as a boundary point.
By the already proven case of the theorem, there exists a hyperplane which
supports Y at the point x0, i.e. there exists a vector c such that
âŸ¨c, x0âŸ©= inf
yâˆˆY âŸ¨c, yâŸ©=
inf
xâˆˆX, vâˆˆUâŠ¥âŸ¨c, x + vâŸ©= inf
xâˆˆXâŸ¨c, xâŸ©+ inf
vâˆˆUâŠ¥âŸ¨c, vâŸ©
and
âŸ¨c, x0âŸ©< sup
yâˆˆY
âŸ¨c, yâŸ©=
sup
xâˆˆX, vâˆˆUâŠ¥âŸ¨c, x + vâŸ©= sup
xâˆˆX
âŸ¨c, xâŸ©+ sup
vâˆˆUâŠ¥âŸ¨c, vâŸ©.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
63
Separation
X
X + UâŠ¥
x0
UâŠ¥
Figure 3.4. Illustration for the proof of Theorem 3.1.4.
It follows from the ï¬rst equation that infvâˆˆUâŠ¥âŸ¨c, vâŸ©is a ï¬nite number, and
since U âŠ¥is a vector space, this is possible if and only if âŸ¨c, vâŸ©= 0 for all
v âˆˆU âŠ¥. The conditions above are therefore reduced to the conditions
âŸ¨c, x0âŸ©= inf
xâˆˆXâŸ¨c, xâŸ©
and
âŸ¨c, x0âŸ©< sup
xâˆˆX
âŸ¨c, xâŸ©,
which show that X has indeed a supporting hyperplane at x0.
We are now able to prove the following necessary and suï¬ƒcient condition
for separation of convex sets.
Theorem 3.1.5. Two convex sets X and Y can be separated by a hyperplane
if and only if their relative interiors are disjoint.
Proof. A hyperplane that separates two sets A and B clearly also separates
their closures cl A and cl B and thereby also all sets C and D that satisfy the
inclusions A âŠ†C âŠ†cl A and B âŠ†D âŠ†cl B.
To prove the existence of a hyperplane that separates the two convex
sets X and Y provided rint X âˆ©rint Y = âˆ…, it hence suï¬ƒces to prove that
there exists a hyperplane that separates the two convex sets A = rint X and
B = rint Y , because rint X âŠ†X âŠ†cl(rint X) = cl X, and the corresponding
inclusions are of course also true for Y .
Since the sets A and B are disjoint, 0 does not belong to the convex set
A âˆ’B. Thus, the point 0 either lies in the complement of cl(A âˆ’B) or
belongs to cl(A âˆ’B) and is a relative boundary point of cl(A âˆ’B), because
cl(A âˆ’B) \ (A âˆ’B) âŠ†cl(A âˆ’B) \ rint(A âˆ’B)
= rbdry(A âˆ’B) = rbdry(cl(A âˆ’B)).
In the ï¬rst case it follows from Theorem 3.1.3 that there is a hyperplane that
strictly separates 0 and A âˆ’B, and in the latter case Theorem 3.1.4 gives us
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
64
Separation
a hyperplane that separates 0 from the set cl(A âˆ’B), and thereby afortiori
also 0 from A âˆ’B. The existence of a hyperplane that separates A and B
then follows from Lemma 3.1.2.
Now, let us turn to the converse. Assume that the hyperplane H separates
the two convex sets X and Y . We will prove that there is no point that is
a relative interior point of both sets. To this end, let us assume that x0 is a
point in the intersection X âˆ©Y . Then, x0 lies in the hyperplane H because X
and Y are subsets of opposite closed halfspaces determined by H. According
to the separability deï¬nition, at least one of the two convex sets, X say,
has points that lie outside H, and this clearly implies that the aï¬ƒne hull
V = aï¬€X is not a subset of H. Hence, there are points in V from each side
of H. Therefore, the intersection V âˆ©B(x0; r) between V and an arbitrary
open ball B(x0; r) centered at x0 also contains points from both sides of H,
and consequently surely points that do not belong to X. This means that x0
must be a relative boundary point of X.
Hence, every point in the intersection X âˆ©Y is a relative boundary point
of either of the two sets X and Y . The intersection rint X âˆ©rint Y is thus
empty.
Let us now consider the possibility of strict separation. A hyperplane that
strictly separates two sets obviously also strictly separates their closures, so
it suï¬ƒces to examine when two closed convex subsets X and Y can be strictly
separated. Of course, the two sets have to be disjoint, i.e. 0 /âˆˆX âˆ’Y is a
necessary condition, and Lemma 3.1.2 now reduces the problem of separating
X strictly from Y to the problem of separating 0 strictly from X âˆ’Y . So it
follows at once from Theorem 3.1.3 that there exists a separating hyperplane
if the set X âˆ’Y is closed. This gives us the following theorem, where the
suï¬ƒcient conditions follow from Theorem 2.7.11 and Corollary 2.7.12.
Theorem 3.1.6. Two disjoint closed convex sets X and Y can be strictly
separated by a hyperplane if the set Xâˆ’Y is closed, and a suï¬ƒcient condition
for this to be the case is recc X âˆ©recc Y = {0}. In particular, two disjoint
closed convex set can be separated strictly by a hyperplane if one of the sets
is bounded.
We conclude this section with a result that shows that proper convex
cones are proper subsets of conic halfspaces. More precisely, we have:
Theorem 3.1.7. Let X Ì¸= {0} be a proper convex cone in Rn, where n â‰¥2.
Then X is a proper subset of some conic halfspace {x âˆˆRn | âŸ¨c, xâŸ©â‰¥0},
whose boundary {x âˆˆRn | âŸ¨c, xâŸ©= 0} does not contain X as a subset.
Proof. The point 0 is a relative boundary point of X, because no point on
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
65
Separation
65
the line segment ]0, âˆ’a[ belongs to X when a is a point Ì¸= 0 in X. Hence, by
Theorem 3.1.4, there exists a hyperplane H = {x âˆˆRn | âŸ¨c, xâŸ©= 0} through
0 such that X lies in the closed halfspace K = {x âˆˆRn | âŸ¨c, xâŸ©â‰¥0} without
X being a subset of H. K is a conic halfspace, and the proper cone X must
be diï¬€erent from K, since no conic halfspaces in Rn are proper cones when
n â‰¥2.
3.2
The dual cone
To each subset A of Rn we associate a new subset A+ of Rn by letting
A+ = {x âˆˆRn | âŸ¨a, xâŸ©â‰¥0 for all a âˆˆA}.
In particular, for sets {a} consisting of just one point we have
{a}+ = {x âˆˆRn | âŸ¨a, xâŸ©â‰¥0},
which is a conic closed halfspace. For general sets A, A+ = 
aâˆˆA{a}+, and
this is an intersection of conic closed halfspaces. The set A+ is thus in general
a closed convex cone, and it is a polyhedral cone if A is a ï¬nite set.
Deï¬nition. The closed convex cone A+ is called the dual cone of the set A.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
66
Separation
A
0
A+
0
Figure 3.5. A cone A and its dual cone A+.
The dual cone A+ of a set A in Rn has an obvious geometric interpreta-
tion when n â‰¤3; it consists of all vectors that form an acute angle or are
perpendicular to all vectors in A.
Theorem 3.2.1. The following properties hold for subsets A and B of Rn.
(i)
A âŠ†B
â‡’B+ âŠ†A+;
(ii)
A+ = (con A)+;
(iii)
A+ = (cl A)+.
Proof. Property (i) is an immediate consequence of the deï¬nition of the dual
cone.
To prove (ii) and (iii), we ï¬rst observe that
(con A)+ âŠ†A+ and (cl A)+ âŠ†A+,
because of property (i) and the obvious inclusions A âŠ†con A and A âŠ†cl A.
It thus only remains to prove the converse inclusions. So let us assume
that x âˆˆA+. Then
âŸ¨Î»1a1 + Â· Â· Â· + Î»kak, xâŸ©= Î»1âŸ¨a1, xâŸ©+ Â· Â· Â· + Î»kâŸ¨ak, xâŸ©â‰¥0
for all conic combinations of elements ai in A. This proves the implication
x âˆˆA+ â‡’x âˆˆ(con A)+, i.e. the inclusion A+ âŠ†(con A)+.
For each a âˆˆcl A there exists a sequence (ak)âˆ
1 of elements in A such that
ak â†’a as k â†’âˆ. If x âˆˆA+, then âŸ¨ak, xâŸ©â‰¥0 for all k, and it follows, by
passing to the limit, that âŸ¨a, xâŸ©â‰¥0. Since a âˆˆcl A is arbitrary, this proves
the implication x âˆˆA+ â‡’x âˆˆ(cl A)+ and the inclusion A+ âŠ†(cl A)+.
Example 3.2.1. Clearly, (Rn)+ = {0} and {0}+ = Rn.
Example 3.2.2. Let, as usual, e1, e2, . . . , en denote the standard basis of Rn.
Then
{ej}+ = {x âˆˆRn | âŸ¨ej, xâŸ©â‰¥0} = {x âˆˆRn | xj â‰¥0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
67
Separation
Since Rn
+ = con{e1, . . . , en}, it follows that
(Rn
+)+ = {e1, . . . , en}+ =
n
j=1
{ej}+ = {x âˆˆRn | x1 â‰¥0, . . . , xn â‰¥0} = Rn
+.
The bidual cone
Deï¬nition. The dual cone A+ of a set A in Rn is a new set in Rn, and we
may therefore form the dual cone (A+)+ of A+. The cone (A+)+ is called
the bidual cone of A, and we write A++ instead of (A+)+.
Theorem 3.2.2. Let A be an arbitrary set in Rn. Then
A âŠ†con A âŠ†A++.
Proof. The deï¬nitions of dual and bidual cones give us the implications
a âˆˆA â‡’âŸ¨x, aâŸ©= âŸ¨a, xâŸ©â‰¥0 for all x âˆˆA+
â‡’a âˆˆA++,
which show that A âŠ†A++. Since A++ is a cone and con A is the smallest
cone containing A, we conclude that con A âŠ†A++.
Because of the previous theorem, it is natural to ask when con A = A++.
Since A++ is a closed cone, a necessary condition for this to be the case is
that the cone con A be closed. Our next theorem shows that this condition
is also suï¬ƒcient.
Theorem 3.2.3. Let X be a convex cone. Then X++ = cl X, and conse-
quently, X++ = X if and only if the cone X is a closed.
Proof. It follows from the inclusion X âŠ†X++ and the closedness of the
bidual cone X++ that cl X âŠ†X++.
To prove the converse inclusion X++ âŠ†cl X, we assume that x0 /âˆˆcl X
and will prove that x0 /âˆˆX++.
By Theorem 3.1.3, there exists a hyperplane that strictly separates x0
from cl X. Hence, there exist a vector c âˆˆRn and a real number b such
that the inequality âŸ¨c, xâŸ©â‰¥b > âŸ¨c, x0âŸ©holds for all x âˆˆX. In particular,
tâŸ¨c, xâŸ©= âŸ¨c, txâŸ©â‰¥b for all x âˆˆX and all numbers t â‰¥0, since X is a cone,
and this clearly implies that b â‰¤0 and that âŸ¨c, xâŸ©â‰¥0 for all x âˆˆX. Hence,
c âˆˆX+, and since âŸ¨c, x0âŸ©< b â‰¤0, we conclude that x0 /âˆˆX++.
By Theorem 2.6.11, ï¬nitely generated cones are closed, so we have the
following immediate corollary.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
68
Separation
68
Corollary 3.2.4. If the cone X is ï¬nitely generated, then X++ = X.
Example 3.2.3. The dual cone of the polyhedral cone
X =
m

i=1
{x âˆˆRn | âŸ¨ai, xâŸ©â‰¥0}
is the cone
X+ = con{a1, a2, . . . , am}.
This follows from the above corollary and Theorem 3.2.1, for
X = {a1, a2, . . . , am}+ = (con{a1, a2, . . . , am})+.
If we use matrices to write the above cone X as {x âˆˆRn | Ax â‰¥0},
then the vector ai corresponds to the ith column of the transposed matrix
AT (cf. Example 2.6.3), and the dual cone X+ is consequently generated by
the columns of AT. Thus,
{x âˆˆRn | Ax â‰¥0}+ = {ATy | y âˆˆRm
+}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
69
Separation
3.3
Solvability of systems of linear inequali-
ties
Corollary 3.2.4 can be reformulated as a criterion for the solvability of systems
of linear inequalities. The proof of this criterion uses the following lemma
about dual cones.
Lemma 3.3.1. Let X and Y be closed convex cones in Rn. Then
(i)
X âˆ©Y = (X+ + Y +)+;
(ii)
X + Y = (X+ âˆ©Y +)+, provided that the cone X + Y is closed.
Proof. We have X+ âŠ†(X âˆ©Y )+ and Y + âŠ†(X âˆ©Y )+, by Theorem 3.2.1 (i).
Hence, X+ + Y + âŠ†(X âˆ©Y )+ + (X âˆ©Y )+ = (X âˆ©Y )+.
Another application of Theorem 3.2.1 in combination with Theorem 3.2.3
now yields X âˆ©Y = (X âˆ©Y )++ âŠ†(X+ + Y +)+.
To obtain the converse inclusion we ï¬rst deduce from X+ âŠ†X+ + Y +
that (X+ + Y +)+ âŠ†X++ = X, and the inclusion (X+ + Y +)+ âŠ†Y is of
course obtained in the same way. Consequently, (X+ + Y +)+ âŠ†X âˆ©Y . This
completes the proof of property (i).
By replacing X and Y in (i) by the closed cones X+ and Y +, we obtain
the equality X+ âˆ©Y + = (X++ + Y ++)+ = (X + Y )+, and since the cone
X + Y is assumed to be closed, we conclude that
X + Y = (X + Y )++ = (X+ âˆ©Y +)+.
We are now ready for the promised result on the solvability of certain
systems of linear inequalities, a result that will be used in our proof of the
duality theorem in linear programming.
Theorem 3.3.2. Let U be a ï¬nitely generated cone in Rn, V be a ï¬nitely
generated cone in Rm, A be an mÃ—n-matrix and c be an nÃ—1-matrix. Then
the system
(S)
ï£±
ï£´
ï£²
ï£´
ï£³
Ax âˆˆV +
x âˆˆU +
cTx < 0
has a solution x if and only if the system
(Sâˆ—)

c âˆ’ATy âˆˆU
y âˆˆV
has no solution y.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
70
Separation
Proof. The system (Sâˆ—) clearly has a solution if and only if c âˆˆ(AT(V ) + U),
and consequently, there is no solution if and only if c /âˆˆ(AT(V ) + U). There-
fore, it is worthwhile to take a closer look at the cone AT(V ) + U.
The cones AT(V ), U and AT(V ) + U are closed, since they are ï¬nitely
generated. We may therefore apply Lemma 3.3.1 with
AT(V ) + U =

AT(V )+ âˆ©U ++
as conclusion. The condition c /âˆˆ(AT(V )+U) is now seen to be equivalent to
the existence of a vector x âˆˆAT(V )+ âˆ©U + satisfying the inequality cTx < 0,
i.e. to the existence of an x such that
(â€ )
ï£±
ï£´
ï£²
ï£´
ï£³
x âˆˆAT(V )+
x âˆˆU +
cTx < 0 .
It now only remains to translate the condition x âˆˆAT(V )+; it is equivalent
to the condition
âŸ¨y, AxâŸ©= âŸ¨ATy, xâŸ©â‰¥0
for all y âˆˆV ,
i.e. to Ax âˆˆV +. The two systems (â€ ) and (S) are therefore equivalent, and
this observation completes the proof.
By choosing U = {0} and V = Rm
+ with dual cones U + = Rn and
V + = Rm
+, we get the following special case of Theorem 3.3.2.
Corollary
3.3.3 (Farkasâ€™s lemma). Let A be an m Ã— n-matrix and c be an
n Ã— 1-matrix, and consider the two systems:
(S)

Ax â‰¥0
cTx < 0
and
(Sâˆ—)

ATy = c
y â‰¥0
The system (S) has a solution if and only if the system (Sâˆ—) has no solution.
Example 3.3.1. The system
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
x1 âˆ’x2 + 2x3 â‰¥0
âˆ’x1 + x2 âˆ’
x3 â‰¥0
2x1 âˆ’x2 + 3x3 â‰¥0
4x1 âˆ’x2 + 10x3 < 0
has no solution, because the dual system
ï£±
ï£²
ï£³
y1 âˆ’y2 + 2y3 =
4
âˆ’y1 + y2 âˆ’y3 = âˆ’1
2y1 âˆ’y2 + 3y3 = 10
has a nonnegative solution y = (3, 5, 3).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
71
Separation
71
Example 3.3.2. The system
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
2x1 + x2 âˆ’x3 â‰¥0
x1 + 2x2 âˆ’2x3 â‰¥0
x1 âˆ’x2 + x3 â‰¥0
x1 âˆ’4x2 + 4x3 < 0
is solvable, because the solutions of the dual system
ï£±
ï£²
ï£³
2y1 + y2 + y3 =
1
y1 + 2y2 âˆ’y3 = âˆ’4
âˆ’y1 âˆ’2y2 + y3 =
4
are of the form y = (2 âˆ’t, âˆ’3 + t, t), t âˆˆR, and none of those is nonnegative
since y1 < 0 for t > 2 and y2 < 0 for t < 3.
The following generalization of Example 3.2.3 will be needed in Chapter
10 in Part II.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
72
Separation
Theorem 3.3.4. Let a1, a2, . . . , am be vectors in Rn, and let I, J be a parti-
tion of the index set {1, 2, . . . , m}, i.e. I âˆ©J = âˆ…and I âˆªJ = {1, 2, . . . , m}.
Let
X =

iâˆˆI
{x âˆˆRn | âŸ¨ai, xâŸ©â‰¥0} âˆ©

iâˆˆJ
{x âˆˆRn | âŸ¨ai, xâŸ©> 0},
and suppose that X Ì¸= âˆ…. Then
X+ = con{a1, a2, . . . , am}.
Proof. Let
Y =
m

i=1
{x âˆˆRn | âŸ¨ai, xâŸ©â‰¥0}.
The set Y is closed and contains X, and we will prove that Y = cl X by
showing that every neighborhood of an arbitrary point y âˆˆY contains points
from X.
So, ï¬x a point x0 âˆˆX, and consider the points y + tx0 for t > 0. These
points lie in X, for
âŸ¨ai, y + tx0âŸ©= âŸ¨ai, yâŸ©+ tâŸ¨ai, x0âŸ©=

â‰¥0
if i âˆˆI
> 0
if i âˆˆJ,
and since y +tx0 â†’y as t â†’0, there are indeed points in X arbitrarily close
to y.
Hence X+ = (cl X)+ = Y +, by Theorem 3.2.1, and the conclusion of the
theorem now follows from the result in Example 3.2.3.
How do we decide whether the set X in Theorem 3.3.4 is nonempty? If
just one of the m linear inequalities that deï¬ne X is strict (i.e. if the index
set J consists of one element), then Farkasâ€™s lemma gives a necessary and
suï¬ƒcient condition for X to be nonempty. A generalization to the general
case reads as follows.
Theorem 3.3.5. The set X in Theorem 3.3.4 is nonempty if and only if
ï£±
ï£´
ï£²
ï£´
ï£³
m

i=1
Î»iai = 0
Î»i â‰¥0 for all i
â‡’Î»i = 0 for all i âˆˆJ.
Proof. Let the vectors Ë†ai in Rn+1 (= Rn Ã— R) be deï¬ned by
Ë†ai =

(ai, 0)
if i âˆˆI
(ai, 1)
if i âˆˆJ.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
73
Separation
Write Ëœx = (x, xn+1), and let ËœX be polyhedral cone
ËœX =
m

i=1
{Ëœx âˆˆRn+1 | âŸ¨Ë†ai, ËœxâŸ©â‰¥0} = (con{Ë†a1, . . . , Ë†am})+.
Since
âŸ¨Ë†ai, (x, xn+1)âŸ©=

âŸ¨ai, xâŸ©
if i âˆˆI
âŸ¨ai, xâŸ©+ xn+1
if i âˆˆJ,
and âŸ¨ai, xâŸ©> 0 for all i âˆˆJ if and only if there exists a negative real number
xn+1 such that âŸ¨ai, xâŸ©+ xn+1 â‰¥0 for all i âˆˆJ, we conclude that the point
x lies in X if and only if there exists a negative number xn+1 such that
âŸ¨Ë†ai, (x, xn+1)âŸ©â‰¥0 for all i, i.e. if and only if there exists a negative number
xn+1 such that (x, xn+1) âˆˆËœX. This is equivalent to saying that the set X is
empty if and only if the implication Ëœx âˆˆËœX â‡’xn+1 â‰¥0 is true, i.e. if and
only if ËœX âŠ†Rn Ã— R+. Using the results on dual cones in Theorems 3.2.1
and 3.2.3 we thus obtain the following chain of equivalences:
X = âˆ…â‡”ËœX âŠ†Rn Ã— R+
â‡”{0} Ã— R+ = (Rn Ã— R+)+ âŠ†ËœX+ = con{Ë†a1, Ë†a2, . . . , Ë†am}
â‡”(0, 1) âˆˆcon{Ë†a1, Ë†a2, . . . , Ë†am}
â‡”there are numbers Î»i â‰¥0 such that
m

i=1
Î»iai = 0 and

iâˆˆJ
Î»i = 1
â‡”there are numbers Î»i â‰¥0 such that
m

i=1
Î»iai = 0 and Î»i > 0 for
some i âˆˆJ.
(The last equivalence holds because of the homogenouity of the condition
m
i=1 Î»iai = 0. If the condition is fulï¬lled for a set of nonnegative numbers
Î»i with Î»i > 0 for at least one i âˆˆJ, then we can certainly arrange so that

iâˆˆJ Î»i = 1 by multiplying with a suitable constant.)
Since the ï¬rst and the last assertion in the above chain of equivalences are
equivalent, so are their negations, and this is the statement of the theorem.
The following corollary is an immediate consequence of Theorem 3.3.5.
Corollary
3.3.6. The set X in Theorem 3.3.4 is nonempty if the vectors
a1, a2, . . . , am are linearly independent.
The following equivalent matrix version of Theorem 3.3.5 is obtained by
considering the vectors ai, i âˆˆI and ai, i âˆˆJ in Theorems 3.3.4 and 3.3.5 as
rows in two matrices A and C, respectively.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
74
Separation
74
Theorem 3.3.7. Let A be a pÃ—n-matrix and C be qÃ—n-matris. Then exactly
one of the two dual systems

Ax â‰¥0
Cx > 0
and

ATy + CTz = 0
y, z â‰¥0, z Ì¸= 0
has a solution.
Theorem 3.3.7 will be generalized in Chapter 6.5, where we prove a the-
orem on the solvability of systems of convex and aï¬ƒne inequalities.
Exercises
3.1 Find two disjoint closed convex sets in R2 that are not strictly separable by
a hyperplane (i.e. by a line in R2).
3.2 Let X be a convex proper subset of Rn. Show that X is an intersection of
closed halfspaces if X is closed, and an intersection of open halfspaces if X
is open.
3.3 Prove the following converse of Lemma 3.1.2: If two sets X and Y are
(strictly) separable, then X âˆ’Y and 0 are (strictly) separable.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
75
Separation
3.4 Find the dual cones of the following cones in R2:
a) R+ Ã—{0}
b) RÃ—{0}
c) RÃ— R+
d) (R++ Ã— R++)âˆª{(0, 0)}
e) {x âˆˆR2 | x1 + x2 â‰¥0, x2 â‰¥0}
3.5 Prove for arbitrary sets X and Y that (X Ã— Y )+ = X+ Ã— Y +.
3.6 Determine the cones X, X+ and X++, if X = con A and
a) A = {(1, 0), (1, 1), (âˆ’1, 1)}
b) A = {(1, 0), (âˆ’1, 1), (âˆ’1, âˆ’1)}
c) A = {x âˆˆR2 | x1x2 = 1, x1 > 0}.
3.7 Let A = {a1, a2, . . . , am} be a subset of Rn, and suppose 0 /âˆˆA. Prove that
the following three conditions are equivalent:
(i) con A is a proper cone.
(ii) m
j=1 Î»jaj = 0, Î» = (Î»1, Î»2, . . . , Î»m) â‰¥0 â‡’Î» = 0.
(iii) There is a vector c such that âŸ¨c, aâŸ©> 0 for all a âˆˆA.
3.8 Is the following system consistent?
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
x1 âˆ’2x2 âˆ’7x3 â‰¥0
5x1 + x2 âˆ’2x3 â‰¥0
x1 + 2x2 + 5x3 â‰¥0
18x1 + 5x2 âˆ’3x3 < 0
3.9 Show that
ï£±
ï£²
ï£³
x1 + x2 âˆ’x3 â‰¥2
x1 âˆ’x2
â‰¥1
x1
+ x3 â‰¥3
â‡’
6x1 âˆ’2x2 + x3 â‰¥11.
3.10 For which values of the parameter Î± âˆˆR is the system
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
x1 +
x2 + Î±x3 â‰¥0
x1 + Î±x2 +
x3 â‰¥0
Î±x1 +
x2 +
x3 â‰¥0
x1 + Î±x2 + Î±2x3 < 0
solvable?
3.11 Let A be an m Ã— n-matrix. Prove that exactly one of the two systems (S)
and (Sâˆ—) has a solution if
(S)
ï£±
ï£´
ï£²
ï£´
ï£³
Ax = 0
x â‰¥0
x Ì¸= 0
and
(Sâˆ—)
ATy > 0
a)
(S)

Ax = 0
x > 0
and
(Sâˆ—)

ATy â‰¥0
ATy Ì¸= 0 .
b)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
76
Separation
76
3.12 Prove that the following system of linear inequalities is solvable:
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
Ax = 0
x â‰¥0
ATy â‰¥0
ATy + x > 0 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
77
More on convex sets
Chapter 4
More on convex sets
4.1
Extreme points and faces
Extreme point
Polyhedra, like the one in ï¬gure 4.1, have vertices. A vertex is characterized
by the fact that it is not an interior point of any line segment that lies entirely
in the polyhedron. This property is meaningful for arbitrary convex sets.
Figure 4.1. A polyhedron
with vertices.
Figure 4.2. Extreme points of a line
segment, a triangle and a circular disk.
Deï¬nition. A point x in a convex set X is called an extreme point of the set
if it does not lie in any open line segment joining two points of X, i.e. if
a1, a2 âˆˆX & a1 Ì¸= a2
â‡’x /âˆˆ]a1, a2[.
The set of all extreme points of X will be denoted by ext X.
A point in the relative interior of a convex set is clearly never an extreme
point, except when the convex set consists just one point.â€  With an exception
for this trivial case, ext X is consequently a subset of the relative boundary
of X. In particular, open convex sets have no extreme points.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
78
More on convex sets
78
Example 4.1.1. The two endpoints are the extreme points of a closed line
segment. The three vertices are the extreme points of a triangle. All points
on the boundary {x | âˆ¥xâˆ¥2 = 1} are extreme points of the Euclidean closed
unit ball B(0; 1) in Rn.
Extreme ray
The extreme point concept is of no interest for convex cones, because non-
proper cones have no extreme points, and proper cones have 0 as their only
extreme point. Instead, for cones the correct extreme concept is about rays,
and in order to deï¬ne it properly we ï¬rst have to deï¬ne what it means for a
ray to lie between to rays.
Deï¬nition. We say that the ray R = âˆ’â†’a lies between the two rays R1 = âˆ’â†’
a1
and R2 = âˆ’â†’
a2 if the two vectors a1 and a2 are linearly independent and there
exist two positive numbers Î»1 and Î»2 so that a = Î»1a1 + Î»2a2.
It is easy to convince oneself that the concept â€lie betweenâ€ only depends
on the rays R, R1 and R2, and not on the vectors a, a1 and a2 chosen to
represent them. Furthermore, a1 and a2 are linearly independent if and only
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
79
More on convex sets
if the rays R1 and R2 are diï¬€erent and not opposite to each other, i.e. if and
only if R1 Ì¸= Â±R2.
Deï¬nition. A ray R in a convex cone X is called an extreme ray of the cone
if the following two conditions are satisï¬ed:
(i) the ray R does not lie between any rays in the cone X;
(ii) the opposite ray âˆ’R does not lie in X.
The set of all extreme rays of X is denoted by exr X.
The second condition (ii) is automatically satisï¬ed for all proper cones,
and it implies, as we shall see later (Theorem 4.2.4), that non-proper cones
have no extreme rays.
R3
R1
R2
Figure 4.3. A polyhedral cone in R3 with three extreme rays.
It follows from the deï¬nition that no extreme ray of a convex cone with
dimension greater than 1 can pass through a relative interior point of the
cone. The extreme rays of a cone of dimension greater than 1 are in other
words subsets of the relative boundary of the cone.
Example 4.1.2. The extreme rays of the four subcones of R are as follows:
exr{0} = exr R = âˆ…, exr R+ = R+ and exr Râˆ’= Râˆ’.
The non-proper cone R Ã— R+ in R2 (the â€upper halfplaneâ€) has no ex-
treme rays, since the two boundary rays R+ Ã— {0} and Râˆ’Ã— {0} are dis-
qualiï¬ed by condition (ii) of the extreme ray deï¬nition.
Face
Deï¬nition. A subset F of a convex set X is called a proper face of X if
F = X âˆ©H for some supporting hyperplane H of X. In addition, the set X
itself and the empty set âˆ…are called non-proper faces of X.â€¡
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
80
More on convex sets
The reason for including the set itself and and the empty set among the
faces is that it simpliï¬es the wording of some theorems and proofs.
X
H
F
Figure 4.4. A convex set X with F as one its faces.
The faces of a convex set are obviously convex sets. And the proper faces
of a convex cone are cones, since the supporting hyperplanes of a cone must
pass through the origin and thus be linear subspaces.
Example 4.1.3. Every point on the boundary {x | âˆ¥xâˆ¥2 = 1} is a face of
the closed unit ball B(0; 1), because the tangent plane at a boundary point
is a supporting hyperplane and does not intersect the unit ball in any other
point.
Example 4.1.4. A cube in R3 has 26 proper faces: 8 vertices, 12 edges and
6 sides.
Theorem 4.1.1. The relative boundary of a closed convex set X is equal to
the union of all proper faces of X.
Proof. We have to prove that rbdry X =  F, where the union is taken over
all proper faces F of X. So suppose that x0 âˆˆF, where F = X âˆ©H is a
proper face of X, and H is a supporting hyperplane. Since H supports X at
x0, and since, by deï¬nition, X is not contained in H, it follows that x0 is a
relative boundary point of X. This proves the inclusion  F âŠ†rbdry X.
Conversely, if x0 is a relative boundary point of X, then there exists a
hyperplane H that supports X at x0, and this means that x0 lies in the
proper face X âˆ©H.
Theorem 4.1.2. The intersection of two faces of a convex set is a face of the
set.
Proof. Let F1 and F2 be two faces of the convex set X, and let F = F1 âˆ©F2.
That F is a face is trivial if the two faces F1 and F2 are identical, or if they
are disjoint, or if one of them is non-proper.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
81
More on convex sets
81
H1
H2
H
F1
F2
X
F
Figure 4.5. Illustration for the proof of Theorem 4.1.2.
So suppose that the two faces F1 and F2 are distinct and proper, i.e. that
they are of the form Fi = X âˆ©Hi, where H1 och H2 are distinct supporting
hyperplanes of the set X, and F Ì¸= âˆ…. Let
Hi = {x âˆˆRn | âŸ¨ci, xâŸ©= bi},
where the normal vectors ci of the hyperplanes are chosen so that X lies
in the two halfspaces {x âˆˆRn | âŸ¨ci, xâŸ©â‰¤bi}, and let x1 âˆˆX be a point
satisfying the condition âŸ¨c1, x1âŸ©< b1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
82
More on convex sets
The hyperplanes H1 and H2 must be non-parallel, since X âˆ©H1 âˆ©H2 =
F Ì¸= âˆ…. Hence c2 Ì¸= âˆ’c1, and we obtain a new hyperplane
H = {x âˆˆRn | âŸ¨c, xâŸ©= b}
by deï¬ning c = c1 + c2 and b = b1 + b2. We will show that H is a supporting
hyperplane of X and that F = X âˆ©H, which proves our claim that the
intersection F is a face of X.
For all x âˆˆX, we have the inequality
âŸ¨c, xâŸ©= âŸ¨c1, xâŸ©+ âŸ¨c2, xâŸ©â‰¤b1 + b2 = b,
and the inequality is strict for the particular point x1 âˆˆX, since
âŸ¨c, x1âŸ©= âŸ¨c1, x1âŸ©+ âŸ¨c2, x1âŸ©< b1 + b2 = b.
So X lies in one of the two closed halfspaces determined by H without being
a subset of H. Moreover, for all x âˆˆF = X âˆ©H1 âˆ©H2,
âŸ¨c, xâŸ©= âŸ¨c1, xâŸ©+ âŸ¨c2, xâŸ©= b1 + b2 = b.
which implies that H is a supporting hyperplane of X and that F âŠ†X âˆ©H.
Conversely, if x âˆˆX âˆ©H, then âŸ¨c1, xâŸ©â‰¤b1, âŸ¨c2, xâŸ©â‰¤b2 and
âŸ¨c1, xâŸ©+ âŸ¨c2, xâŸ©= b1 + b2,
and this implies that âŸ¨c1, xâŸ©= b1 and âŸ¨c2, xâŸ©= b2. Hence, x âˆˆX âˆ©H1 âˆ©H2 =
F. This proves the inclusion X âˆ©H âŠ†F.
Theorem 4.1.3.
(i) Suppose F is a face of the convex set X. A point x in
F is an extreme point of F if and only if x is an extreme point of X.
(ii) Suppose F is a face of the convex cone X. A ray R in F is an extreme
ray of F if and only if R is an extreme ray of X.
Proof. Since the assertions are trivial for non-proper faces, we may assume
that F = X âˆ©H for some supporting hyperplane H of X.
No point in a hyperplane lies in the interior of a line segment whose
endpoints both lie in the same halfspace, unless both endpoints lie in the
hyperplane, i.e. unless the line segment lies entirely in the hyperplane.
Analogously, no ray in a hyperplane H (through the origin) lies between
two rays in the same closed halfspace determined by H, unless both these
rays lie in the hyperplane H. And the opposite ray âˆ’R of a ray R in a
hyperplane clearly lies in the same hyperplane.
(i) If x âˆˆF is an interior point of some line segment with both endpoints
belonging to X, then x is in fact an interior point of a line segment whose
endpoints both belong to F. This proves the implication
x /âˆˆext X â‡’x /âˆˆext F.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
83
More on convex sets
x
X
H
F
Figure 4.6.
The two endpoints of an open line segment that intersects the
hyperplane H must either both belong to H or else lie in opposite open halfspaces.
The converse implication is trivial, since every line segment in F is a line
segment i X. Hence, x /âˆˆext X â‡”x /âˆˆext F, and this is of course equivalent
to assertion (i).
(ii) Suppose R is a ray in F and that R is not an extreme ray of the cone X.
Then there are two possibilities: R lies between two rays R1 and R2 in X, or
the opposite ray âˆ’R lies in X. In the ï¬rst case, R1 and R2 will necessarily
lie in F, too. In the second case, the ray âˆ’R will lie in F. Thus, both cases
lead to the conclusion that R is not an extreme ray of the cone F, and this
proves the implication R /âˆˆexr X â‡’R /âˆˆexr F.
The converse implication is again trivial, and this observation concludes
the proof of assertion (ii).
4.2
Structure theorems for convex sets
Theorem 4.2.1. Let X be a line-free closed convex set with dim X â‰¥2. Then
X = cvx(rbdry X).
Proof. Let n = dim X. By identifying the aï¬ƒne hull of X with Rn, we may
without loss of generality assume that X is a subset of Rn of full dimension,
so that rbdry X = bdry X. To prove the theorem it is now enough to prove
that every point in X lies in the convex hull of the boundary bdry X, because
the inclusionen cvx(bdry X) âŠ†X is trivially true.
The recession cone C = recc X is a proper cone, since X is supposed to
be line-free. Hence, there exists a closed halfspace
K = {x âˆˆRn | âŸ¨c, xâŸ©â‰¥0},
which contains C as a proper subset, by Theorem 3.1.7. Since C is a closed
cone, we conclude that the corresponding open halfspace
K+ = {x âˆˆRn | âŸ¨c, xâŸ©> 0}.
contains a vector v that does not belong to C. The opposite vector âˆ’v, which
lies in the opposite open halfspace, does not belong to C, either. Compare
ï¬gure 4.7.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
84
More on convex sets
84
K+
v
x
X
x1
x2
0
C
Figure 4.7. An illustration for the proof of Theorem 4.2.1.
We have produced two opposite vectors Â±v, both lying outside the reces-
sion cone. The two opposite halï¬‚ines x + âˆ’â†’v and x âˆ’âˆ’â†’v from a point x âˆˆX
therefore both intersect the complement of X.
The intersection between X and the line through x with direction vector
v, which is a closed convex set, is thus either a closed line segment [x1, x2]
containing x and with endpoints belonging to the boundary of X, or the
singleton set {x} with x belonging to the boundary of X. In the ï¬rst case, x
is a convex combination of the boundary points x1 and x2. So, x lies in the
convex hull of the boundary in both cases. This completes the proof.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
85
More on convex sets
It is now possible to give a complete description of line-free closed convex
sets in terms of extreme points and recession cones.
Theorem 4.2.2. A nonempty closed convex set X has extreme points if and
only if X is line-free, and if X is line-free, then
X = cvx(ext X) + recc X.
Proof. First suppose that the set X is not line-free. Its recessive subspace
will then, by deï¬nition, contain a nonzero vector y, and this implies that
the two points x Â± y lie in X for each point x âˆˆX. Therefore, x being the
midpoint of the line segment ]x âˆ’y, x + y[, is not an extreme point. This
proves that the set ext X of extreme points is empty.
Next suppose that X is line-free. We claim that ext X Ì¸= âˆ…and that X =
cvx(ext X) + recc X, and we will prove this by induction on the dimension
of the set X.
Our claim is trivially true for zero-dimensional sets X, i.e. sets consisting
of just one point. If dim X = 1, then either X is a halï¬‚ine a + âˆ’â†’v with one
extreme point a and recession cone equal to âˆ’â†’v , or a line segment [a, b] with
two extreme points a, b and recession cone equal to {0}, and the equality in
the theorem is clearly satisï¬ed in both cases.
Now assume that n = dim X â‰¥2 and that our claim is true for all line-
free closed convex sets X with dimension less than n. By Theorems 4.1.1
and 4.2.1,
X = cvx
 F

,
where the union is taken over all proper faces F of X. Each proper face F is
a nonempty line-free closed convex subset of a supporting hyperplane H and
has a dimension which is less than or equal to n âˆ’1. Therefore, ext F Ì¸= âˆ…
and
F = cvx(ext F) + recc F,
by our induction hypothesis.
Since ext F âŠ†ext X (by Theorem 4.1.3), it follows that ext X Ì¸= âˆ…. More-
over, recc F is a subset of recc X, so we have the inclusion
F âŠ†cvx(ext X) + recc X
for each face F. The union  F is consequently included in the convex set
cvx(ext X) + recc X. Hence
X = cvx
 F

âŠ†cvx(ext X) + recc X âŠ†X + recc X = X,
so X = cvx(ext X) + recc X, and this completes the induction and the proof
of the theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
86
More on convex sets
The recession cone of a compact set is equal to the null cone, and the
following result is therefore an immediate corollary of Theorem 4.2.2.
Corollary 4.2.3. Each nonempty compact convex set has extreme points and
is equal to the convex hull of its extreme points.
We shall now formulate and prove the anaologue of Theorem 4.2.2 for
convex cones, and in order to simplify the notation we shall use the following
convention: If A is a family of rays, we let con A denote the cone
con
 
RâˆˆA
R

,
i.e. con A is the cone that is generated by the vectors on the rays in the
family A. If we choose a nonzero vector aR on each ray R âˆˆA and let
A = {aR | R âˆˆA}, then of course con A = con A.
The cone con A is clearly ï¬nitely generated if A is a ï¬nite family of rays,
and we obtain a set of generators by choosing one nonzero vector from each
ray.
Theorem 4.2.4. A closed convex cone X has extreme rays if and only if the
cone is proper and not equal to the null cone {0}. If X is a proper closed
convex cone, then
X = con(exr X).
Proof. First suppose that the cone X is not proper, and let R = âˆ’â†’x be an
arbitrary ray in X. We will prove that R can not be an extreme ray.
Since X is non-proper, there exists a nonzero vector a in the intersection
X âˆ©(âˆ’X). First suppose that R is equal to âˆ’â†’a or to âˆ’âˆ’â†’a . Then both R and
its opposite ray âˆ’R lie in X, and this means that R is not an extreme ray.
Next suppose R Ì¸= Â±âˆ’â†’a . The vectors x and a are then linearly indepen-
dent, and the two rays R1 = âˆ’âˆ’âˆ’â†’
x + a and R2 = âˆ’âˆ’â†’
xâˆ’a are consequently distinct
and non-opposite rays in the cone X. Since x = 1
2(x + a) + 1
2(x âˆ’a), we
conclude that R lies between R1 and R2. Thus, R is not an extreme ray in
this case either, and this proves that non-proper cones have no extreme rays.
The equality X = con(exr X) is trivially true for the null cone, since
exr{0} = âˆ…and con âˆ…= {0}. To prove that the equality holds for all non-
trivial proper closed convex cones and that these cones do have extreme rays,
we only have to modify slightly the induction proof for the corresponding part
of Theorem 4.2.2.
The start of the induction is of course trivial, since one-dimensional proper
cones are rays. So suppose our assertion is true for all cones of dimension less
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
87
More on convex sets
87
than or equal to n âˆ’1, and let X be a proper closed n-dimensional convex
cone. X is then, in particular, a line-free set, whence X = cvx
 F

, where
the union is taken over all proper faces F of the cone. Moreover, since X is
a convex cone, cvx
 F

âŠ†con
 F

âŠ†con X = X, and we conclude that
(4.1)
X = con
 F

.
We may of course delete the trivial face F = {0} from the above union
without destroying the identity, and every remaining face F is a proper closed
convex cone of dimension less than or equal to n âˆ’1 with exr F Ì¸= âˆ…and
F = con(exr F), by our induction assumption. Since exr F âŠ†exr X, it now
follows that the set exr X is nonempty and that F âŠ†con(exr X).
The union  F of the faces is thus included in the cone con(exr X), so
it follows from equation (4.1) that X âŠ†con(exr X).
Since the converse
inclusion is trivial, we have equality X = con(exr X), and the induction step
is now complete.
The recession cone of a line-free convex set is a proper cone. The following
structure theorem for convex sets is therefore an immediate consequence of
Theorems 4.2.2 and 4.2.4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
88
More on convex sets
Theorem 4.2.5. If X is a nonempty line-free closed convex set, then
X = cvx(ext X) + con(exr(recc X)).
The study of arbitrary closed convex sets is reduced to the study of line-
free such sets by the following theorem, which says that every non-line-free
closed convex set is a cylinder with a line-free convex set as its base and with
the recessive subspace lin X as its â€axisâ€.
Theorem 4.2.6. Suppose X is a closed convex set in Rn. The intersection
X âˆ©(lin X)âŠ¥is then a line-free closed convex set and
X = lin X + X âˆ©(lin X)âŠ¥.
0
(lin X)âŠ¥
X
lin X
Figure 4.8. Illustration for Theorem 4.2.6.
Proof. Each x âˆˆRn has a unique decomposition x = y + z with y âˆˆlin X
and z âˆˆ(lin X)âŠ¥. If x âˆˆX, then z lies in X, too, since
z = x âˆ’y âˆˆX + lin X = X.
This proves the inclusion X âŠ†lin X+Xâˆ©(lin X)âŠ¥, and the converse inclusion
follows from lin X + X âˆ©(lin X)âŠ¥âŠ†lin X + X = X.
Exercises
4.1 Find ext X and decide whether X = cvx(ext X) when
a) X = {x âˆˆR2
+ | x1 + x2 â‰¥1}
b) X =

[0, 1] Ã— [0, 1[

âˆª

[0, 1
2] Ã— {1}

c) X = cvx

{x âˆˆR3 | (x1 âˆ’1)2 + x2
2 = 1, x3 = 0} âˆª{(0, 0, 1), (0, 0, âˆ’1)}

.
4.2 Prove that ext(cvx A) âŠ†A for each subset A of Rn.
4.3 Let X = cvx A and suppose the set A is minimal in the following sense: If
B âŠ†A och X = cvx B, then B = A. Prove that A = ext X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
89
More on convex sets
4.4 Let x0 be a point in a convex set X. Prove that x0 âˆˆext X if and only if
the set X \ {x0} is convex.
4.5 Give an example of a compact convex subset of R3 such that the set of
extreme points is not closed.
4.6 A point x0 in a convex set X is called an exposed point if the singleton set
{x0} is a face, i.e. if there exists a supporting hyperplane H of X such that
X âˆ©H = {x0}.
a) Prove that every exposed point is an extreme point of X.
b) Give an example of a closed convex set in R2 with an extreme point that
is not exposed.
4.7 There is a more general deï¬nition of the face concept which runs as follows:
A face of a convex set X is a convex subset F of X such that every closed
line segment in X with a relative interior point in F lies entirely in F, i.e.
(a, b âˆˆX & ]a, b[ âˆ©F Ì¸= âˆ…) =â‡’a, b âˆˆF.
Let us call faces according to this deï¬nition general faces in order to distin-
guish them from faces according to our old deï¬nition, which we call exposed
faces, provided they are proper, i.e. diï¬€erent from the faces X and âˆ….
The empty set âˆ…and X itself are apparently general faces of X, and all
extreme points of X are general faces, too.
Prove that the general faces of a convex set X have the following properties.
a) Each exposed face is a general face.
b) There is a convex set with a general face that is not an exposed face.
c) If F is a general face of X and F â€² is a general face of F, then F â€² is a
general face of X, but the coresponding result is not true in general for
exposed faces.
d) If F is a general face of X and C is an arbitrary convex subset of X such
that F âˆ©rint C Ì¸= âˆ…, then C âŠ†F.
e) If F is a general face of X, then F = X âˆ©cl F. In particular, F is closed
if X is closed.
f) If F1 and F2 are two general faces of X and rint F1 âˆ©rint F2 Ì¸= âˆ…, then
F1 = F2.
g) If F is a general face of X and F Ì¸= X, then F âŠ†rbdry X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
90
Polyhedra
Chapter 5
Polyhedra
We have already obtained some isolated results on polyhedra, but now is the
time to collect these and to complement them in order to get a complete
description of this important class of convex sets.
5.1
Extreme points and extreme rays
Polyhedra and extreme points
Each polyhedron X in Rn, except for the entire space, is an intersection of
ï¬nitely many closed halfspaces and may therefore be written in the form
X =
m

j=1
Kj,
with
Kj = {x âˆˆRn | âŸ¨cj, xâŸ©â‰¥bj}
for suitable nonzero vectors cj in Rn and real numbers bj. Using matrix
notation,
X = {x âˆˆRn | Cx â‰¥b},
where C is an m Ã— n-matrix with cjT as rows, and b =

b1
b2
. . .
bm
T.
Let
Kâ—¦
j = {x âˆˆRn | âŸ¨cj, xâŸ©> bj} = int Kj,
and
Hj = {x âˆˆRn | âŸ¨cj, xâŸ©= bj} = bdry Kj.
The sets Kâ—¦
j are open halfspaces, and the Hj are hyperplanes.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
91
Polyhedra
If b = 0, i.e. if all hyperplanes Hj are linear subspaces, then X is a
polyhedral cone.
The polyhedron X is clearly a subset of the closed halfspace Kj, which
is bounded by the hyperplane Hj. Let
Fj = X âˆ©Hj.
If there is a point in common between the hyperplane Hj and the polyhedron
X, without X being entirely contained in H, then H is a supporting hyper-
plane of X, and the set Fj is a proper face of X. But Fj is a face of X also
in the cases when X âˆ©Hj = âˆ…or X âŠ†Hj, due to our convention regarding
non-proper faces. Of course, the faces Fj are polyhedra.
All points of a face Fj (proper as well as non-proper) are boundary points
of X. Since
X =
m

j=1
Kâ—¦
j âˆª
m

j=1
Fj,
and all points in the open set m
j=1 Kâ—¦
j are interior points of X, we conclude
that
int X =
m

j=1
Kâ—¦
j
and
bdry X =
m

j=1
Fj.
The set ext X of extreme points of the polyhedron X is a subset of the
boundary m
j=1 Fj, and the extreme points are characterized by the following
theorem.
Theorem 5.1.1. A point x0 in the polyhedron X = m
j=1 Kj is an extreme
point if and only if there exists a subset I of the index set {1, 2, . . . , m} such
that 
jâˆˆI Hj = {x0}.
Proof. Suppose there exists such an index set I. The intersection
F =

jâˆˆI
Fj = X âˆ©

jâˆˆI
Hj = {x0}
is a face of X, by Theorem 4.1.2, and x0 is obviously an extreme point of F.
Therefore, x0 is also an extreme point of X, by Theorem 4.1.3.
Now suppose, conversely, that there is no such index set I, and let J be
an index set that is maximal with respect to the property x0 âˆˆ
jâˆˆJ Hj.
(Remember that the intersection over an empty index set is equal to the
entire space Rn, so J = âˆ…if x0 is an interior point of X.) The intersection

jâˆˆJ Hj is an aï¬ƒne subspace, which by assumption consists of more than
one point and, therefore, contains a line {x0 + tv | t âˆˆR} through x0. The
line is obviously also contained in the larger set 
jâˆˆJ Kj.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
92
Polyhedra
92
Since x0 is an interior point of the halfspace Kj for all indices j /âˆˆJ,
we conclude that the points x0 + tv belong to all these halfspaces for all
suï¬ƒciently small values of |t|. Consequently, there is a number Î´ > 0 such
that the line segment [x0 âˆ’Î´v, x0 +Î´v] lies in X = 
jâˆˆJ Kj âˆ©
j /âˆˆJ Kj, which
means that x0 is not an extreme point.
The condition 
jâˆˆI Hj = {x0} means that the corresponding system of
linear equations
âŸ¨cj, xâŸ©= bj,
j âˆˆI,
in n unknowns has a unique solution. A necessary condition for this to be
true is that the index set I contains at least n elements. And if the system
has a unique solution and there are more than n equations, then it is always
possible to obtain a quadratic subsystem with a uniqe solution by eliminating
suitably selected equations.
Hence, the condition m â‰¥n is necessary for the polyhedron X = m
j=1 Kj
to have at least one extreme point. (This also follows from Theorem 2.7.7,
for if m < n, then
dim lin X = dim{x âˆˆRn | Cx = 0} = n âˆ’rank C â‰¥n âˆ’m > 0,
which means that X is not line-free.)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
93
Polyhedra
Theorem 5.1.1 gives us the following method for ï¬nding all extreme points
of the polyhedron X when m â‰¥n:
Solve for each subset J of {1, 2, . . . , m} with n elements the corresponding
linear system âŸ¨cj, xâŸ©= bj, j âˆˆJ. If the system has a unique solution x0, and
the solution lies in X, i.e. satisï¬es the remaining linear inequalities âŸ¨cj, xâŸ©â‰¥
bj, then x0 is an extreme point of X.
The number of extreme points of X is therefore bounded by
m
n

, which
is the number of subsets J of {1, 2, . . . , m} with n elements. In particular,
we have proved the following theorem.
Theorem 5.1.2. Polyhedra have ï¬nitely many extreme points.
Polyhedral cones and extreme rays
A polyhedral cone in Rn is an intersection X = m
j=1 Kj of conic halfspaces
Kj which are bounded by hyperplanes Hj through the origin, and the faces
Fj = X âˆ©Hj are polyhedral cones. Our next theorem is a direct analogue of
Theorem 5.1.1.
Theorem 5.1.3. A point x0 in the polyhedral cone X generates an extreme
ray R = âˆ’â†’
x0 of the cone if and only if âˆ’x0 /âˆˆX and there exists a subset I of
the index set {1, 2, . . . , m} such that 
jâˆˆI Hj = {tx0 | t âˆˆR}.
Proof. Suppose there exists such an index set I and that âˆ’x0 does not belong
to the cone X. Then

jâˆˆI
Fj = X âˆ©

jâˆˆI
Hj = R.
By Theorem 4.1.2, this means that R is a face of the cone X. The ray R
is an extreme ray of the face R, of course, so it follows from Theorem 4.1.3
that R is an extreme ray of X.
If âˆ’x0 belongs to X, then X is not a proper cone, and hence X has no
extreme rays according to Theorem 4.2.4.
It remains to show that R is not an extreme ray in the case when âˆ’x0 /âˆˆX
and there is no index set I with the property that the intersection 
jâˆˆI Hj is
equal to the line through 0 and x0. So let J be a maximal index set satisfying
the condition x0 âˆˆ
jâˆˆJ Hj. Due to our assumption, the intersection 
jâˆˆJ Hj
is then a linear subspace of dimension greater than or equal to two, and
therefore it contains a vector v which is linearly independent of x0. The
vectors x0 + tv and x0 âˆ’tv both belong to 
jâˆˆJ Hj, and consequently also
to 
jâˆˆJ Kj, for all real numbers t. When |t| is a suï¬ƒciently small number,
the two vectors also belong to the halfspaces Kj for indices j /âˆˆJ, because
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
94
Polyhedra
x0 is an interior point of Kj for these indices j. Therefore, there exists a
positive number Î´ such that the vectors x+ = x0 + Î´v and xâˆ’= x0 âˆ’Î´v both
belong to the cone X. The two vectors x+ and xâˆ’are linearly independent
and x0 = 1
2x+ + 1
2xâˆ’, so it follows that the ray R = âˆ’â†’
x0 lies between the two
rays âˆ’â†’
x+ and âˆ’â†’
xâˆ’in X, and R is therefore not an extreme ray.
Thus, to ï¬nd all the extreme rays of the cone
X =
m

j=1
{x âˆˆRn | âŸ¨cj, xâŸ©â‰¥0}
we should proceed as follows. First choose an index set J consisting of n âˆ’1
elements from the set {1, 2, . . . , m}.
This can be done in
 m
nâˆ’1

diï¬€erent
ways. Then solve the corresponding homogeneous linear system âŸ¨cj, xâŸ©= 0,
j âˆˆJ. If the solution set is one-dimensional, than pick a solution x0. If x0
satisï¬es the remaining linear inequalities and âˆ’x0 does not, then R = âˆ’â†’
x0 is
an extreme ray. If, instead, âˆ’x0 satisï¬es the remaining linear inequalities and
x0 does not, then âˆ’R is an extreme ray. Since this is the only way to obtain
extreme rays, we conclude that the number of extreme rays is bounded by
the number
 m
nâˆ’1

. In particular, we get the following corollary.
Theorem 5.1.4. Polyhedral cones have ï¬nitely many extreme rays.
5.2
Polyhedral cones
Theorem 5.2.1. A cone is polyhedral if and only if it is ï¬nitely generated.
Proof. We ï¬rst show that every polyhedral cone is ï¬nitely generated.
By Theorem 4.2.6, every polyhedral cone X can be written in the form
X = lin X + X âˆ©(lin X)âŠ¥,
and X âˆ©(lin)âŠ¥is a line-free, i.e. proper, polyhedral cone. Let B be a set
consisting of one point from each extreme ray of X âˆ©(lin X)âŠ¥; then B is a
ï¬nite set and
X âˆ©(lin X)âŠ¥= con B,
according to Theorems 5.1.4 and 4.2.4.
Let e1, e2, . . . , ed be a basis for the linear subspace lin X, and put e0 =
âˆ’(e1 + e2 + Â· Â· Â· + ed).
The cone lin X is generated as a cone by the set
A = {e0, e1, . . . , ed}, i.e.
lin X = con A.
Summing up,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
95
Polyhedra
95
X = lin X + X âˆ©(lin X)âŠ¥= con A + con B = con(A âˆªB),
which shows that the cone X is ï¬nitely generated by the set A âˆªB.
Next, suppose that X is a ï¬nitely generated cone so that X = con A for
some ï¬nite set A. We start by the observation that the dual cone X+ is
polyhedral. Indeed, if A Ì¸= âˆ…then
X+ = A+ = {x âˆˆRn | âŸ¨x, aâŸ©â‰¥0 for all a âˆˆA} =

aâˆˆA
{x âˆˆRn | âŸ¨a, xâŸ©â‰¥0}
is an intersection of ï¬nitely many conical halfspaces, i.e. a polyhedral cone.
And if A = âˆ…, then X = {0} and X+ = Rn.
The already proven part of the theorem now implies that the dual cone
X+ is ï¬nitely generated. But the dual cone of X+, i.e. the bidual cone X++,
is then polyhedral, too. Since the bidual cone X++ coincides with the original
cone X, by Corollary 3.2.4, we conclude the X is a polyhedral cone.
We are now able two prove two results that were left unproven in Chap-
ter 2.6; compare Corollary 2.6.9.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
96
Polyhedra
Theorem 5.2.2.
(i) The intersection X âˆ©Y of two ï¬nitely generated cones
X and Y is a ï¬nitely generated cone.
(ii) The inverse image T âˆ’1(X) of a ï¬nitely generated cone X under a linear
map T is a ï¬nitely generated cone.
Proof. The intersection of two conical polyhedra is obviously a conical poly-
hedron, and the same holds for the inverse image of a conical polyhedron un-
der a linear map. The theorem is therefore a corollary of Theorem 5.2.1.
5.3
The internal structure of polyhedra
Polyhedra are by deï¬nition intersections of ï¬nite collections of closed half-
spaces, and this can be viewed as an external description of polyhedra. We
shall now give an internal description of polyhedra in terms of extreme points
and extreme rays, and the following structure theorem is the main result of
this chapter.
Theorem 5.3.1. A nonempty subset X of Rn is a polyhedron if and only if
there exist two ï¬nite subsets A and B of Rn with A Ì¸= âˆ…such that
X = cvx A + con B.
The cone con B is then equal to the recession cone recc X of X. If the
polyhedron is line-free, we may choose for A the set ext X of all extreme points
of X, and for B a set consisting of one nonzero point from each extreme ray
of the recession cone recc X.
Proof. We ï¬rst prove that polyhedra have the stated decomposition.
So
let X be a polyhedron and put Y = X âˆ©(lin X)âŠ¥. Then, Y is a line-free
polyhedron, and
X = lin X + Y = lin X + recc Y + cvx(ext Y ),
by Theorems 4.2.6 and 4.2.2. The two polyhedral cones lin X and recc Y
are, according to Theorem 5.2.1, generated by two ï¬nite sets B1 and B2,
respectively, and their sum is generated by the ï¬nite set B = B1 âˆªB2. The
set ext Y is ï¬nite, by Theorem 5.1.2, so the representation
X = cvx A + con B
is now obtained by taking A = ext Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
97
Polyhedra
e1
b1
b2
0
a1
a2
lin X
recc(X âˆ©(lin X)âŠ¥)
X
X âˆ©(lin X)âŠ¥
Figure 5.1. An illustration for Theorem 5.3.1. The right part of the ï¬gure depicts
an unbounded polyhedron X in R3. Its recessive subspace lin X is one-dimensional
and is generated as a cone by e1 and -e1. The intersection X âˆ©(lin X)âŠ¥, which
is shadowed, is a line-free polyhedron with two extreme points a1 and a2. The
recession cone recc(X âˆ©(lin X)âŠ¥) is generated by b1 and b2. The representation
X = cvx A + con B is obtained by taking A = {a1, a2} and B = {e1, âˆ’e1, b1, b2}.
The cone con B is closed and the convex set cvx A is compact, since the
sets A and B are ï¬nite. Hence, con B = recc X by Corollary 2.7.13.
If X is a line-free polyhedron, then
X = cvx(ext X) + con(exr(recc X)),
by Theorems 4.2.2 and 4.2.4, and this gives us the required representation of
X with A = ext X and with B as a set consisting of one nonzero point from
each extreme ray of recc X.
To prove the converse, suppose that X = cvx A + con B, where A =
{a1, . . . , ap} and B = {b1, . . . , bq} are ï¬nite sets. Consider the cone Y in
Rn Ã— R that is generated by the ï¬nite set (A Ã— {1}) âˆª(B Ã— {0}). The cone
Y is polyhedral according to Theorem 5.2.1, which means that there is an
m Ã— (n + 1)-matrix C such that
(5.1)
(x, xn+1) âˆˆY
â‡”C
 x
xn+1

â‰¥0.
(Here
 x
xn+1

denotes the vector (x1, . . . , xn, xn+1) written as a column ma-
trix.)
Let Câ€² denote the submatrix of C which consists of all columns but the
last, and let câ€² be the last column of the matrix C. Then
C
 x
xn+1

= Câ€²x + xn+1câ€²,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
98
Polyhedra
98
which means that the equivalence (5.1) may be written as
(x, xn+1) âˆˆY
â‡”Câ€²x + xn+1câ€² â‰¥0.
By deï¬nition, a vector (x, 1) âˆˆRn Ã— R belongs to the cone Y if and only if
there exist nonnegative numbers Î»1, Î»2, . . . , Î»p and Âµ1, Âµ2, . . . , Âµq such that

x = Î»1a1 + Î»2a2 + Â· Â· Â· + Î»pap + Âµ1b1 + Âµ2b2 + Â· Â· Â· + Âµqbq
1 = Î»1 + Î»2 + Â· Â· Â· + Î»p
i.e. if and only if x âˆˆcvx A + con B. This yields the equivalences
x âˆˆX
â‡”(x, 1) âˆˆY
â‡”Câ€²x + câ€² â‰¥0,
which means that X = {x âˆˆRn | Câ€²x â‰¥âˆ’câ€²}. Thus, X is a polyhedron.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
99
Polyhedra
5.4
Polyhedron preserving operations
Theorem 5.4.1. The intersection of ï¬nitely many polyhedra in Rn is a poly-
hedron.
Proof. Trivial.
Theorem 5.4.2. Suppose X is a polyhedron in Rn and that T : Rn â†’Rm is
an aï¬ƒne map. The image T(X) is then a polyhedron in Rm.
Proof. The assertion is trivial if the polyhedron is empty, so suppose it is
nonempty and write it in the form
X = cvx A + con B,
where A = {a1, . . . , ap} and B = {b1, . . . , bq} are ï¬nite sets. Each x âˆˆX has
then a representation of the form
x =
p

j=1
Î»jaj +
q

j=1
Âµjbj =
p

j=1
Î»jai +
q

j=1
Âµjbj âˆ’(
q

j=1
Âµj)0
with nonnegative coeï¬ƒcients Î»j and Âµj and p
j=1 Î»j = 1, i.e. each x âˆˆX is
an aï¬ƒne combination of elements in the set A âˆªB âˆª{0}. Since T is an aï¬ƒne
map,
Tx =
p

j=1
Î»jTaj +
q

j=1
ÂµjTbj âˆ’(
q

j=1
Âµj)T0 =
p

j=1
Î»jTaj +
q

j=1
Âµj(Tbj âˆ’T0).
This shows that the image T(X) is of the form
T(X) = cvx Aâ€² + con Bâ€²
with Aâ€² = T(A) and Bâ€² = âˆ’T0 + T(B) = {Tb1 âˆ’T0, . . . , Tbq âˆ’T0}. So the
image T(X) is a polyhedron, by Theorem 5.3.1.
Theorem 5.4.3. Suppose Y is a polyhedron in Rm and that T : Rn â†’Rm
is an aï¬ƒne map. The inverse image T âˆ’1(Y ) is then a polyhedron in Rn.
Proof. First assume that Y is a closed halfspace in Rm (or the entire space
Rm), i.e. that Y = {y âˆˆRm | âŸ¨c, yâŸ©â‰¥b}. (The case Y = Rm is obtained by
c = 0 and b = 0.) The aï¬ƒne map T can be written in the form Tx = Sx+y0,
with S as a linear map and y0 as a vector in Rm. This gives us
T âˆ’1(Y ) = {x âˆˆRn | âŸ¨c, TxâŸ©â‰¥b} = {x âˆˆRn | âŸ¨STc, xâŸ©â‰¥b âˆ’âŸ¨c, y0âŸ©}.
So T âˆ’1(Y ) is a closed halfspace in Rn if STc Ì¸= 0, the entire space Rn if
STc = 0 and b â‰¤âŸ¨c, y0âŸ©, and the empty set âˆ…if STc = 0 and b > âŸ¨c, y0âŸ©.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
100
Polyhedra
In the general case, Y = p
j=1 Kj is an intersection of ï¬nitely many closed
halfspaces. Since Sâˆ’1(Y ) = p
j=1 Sâˆ’1(Kj), the inverse image Sâˆ’1(Y ) is an
intersection of closed halfspaces, the empty set, or the entire space Rn. Thus,
Sâˆ’1(Y ) is a polyhedron.
Theorem 5.4.4. The Cartesian product X Ã— Y of two polyhedra X and Y is
a polyhedron.
Proof. Suppose X lies in Rm and Y lies in Rn. The set X Ã— Rn is a poly-
hedron since it is the inverse image of X under the projection (x, y) â†’x,
and Rm Ã— Y is a polyhedron for a similar reason. It follows that X Ã— Y is a
polyhedron, because X Ã— Y = (X Ã— Rn) âˆ©(Rm Ã— Y ).
Theorem 5.4.5. The sum X + Y of two polyhedra in Rn is a polyhedron.
Proof. The sum X + Y is equal to the image of X Ã— Y under the linear map
(x, y) â†’x + y, so the theorem is a consequence of the previous theorem and
Theorem 5.4.2.
5.5
Separation
It is possible to obtain sharper separation results for polyhedra than for
general convex sets. Compare the following two theorems with Theorems
3.1.6 and 3.1.5.
Theorem 5.5.1. If X and Y are two disjoint polyhedra, then there exists a
hyperplane that strictly separates the two polyhedra.
Proof. The diï¬€erence X âˆ’Y of two polyhedra X and Y is a closed set, since
it is a polyhedron according to Theorem 5.4.5. So it follows from Theorem
3.1.6 that there exists a hyperplane that strictly separates the two polyhedra,
if they are disjoint.
Theorem 5.5.2. Let X be a convex set, and let Y be a polyhedron that is
disjoint from X. Then there exists a hyperplane that separates X and Y and
does not contain X as a subset.
Proof. We prove the theorem by induction over the dimension n of the sur-
rounding space Rn.
The case n = 1 is trivial, so suppose the assertion of the theorem is true
when the dimension is n âˆ’1, and let X be a convex subset of Rn that is
disjoint from the polyhedron Y . An application of Theorem 3.1.5 gives us
a hyperplane H that separates X and Y and, as a consequence, does not
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
101
Polyhedra
101
contain both sets as subsets. If X is not contained in H, then we are done.
So suppose that X is a subset of H. The polyhedron Y then lies in one of the
two closed halfspaces deï¬ned by the hyperplane H. Let us denote this closed
halfspace by H+, so that Y âŠ†H+, and let H++ denote the corresponding
open halfspace.
If Y âŠ†H++, then Y and H are disjoint polyhedra, and an application
of Theorem 5.5.1 gives us a hyperplane that strictly separates Y and H. Of
course, this hyperplane also strictly separates Y and X, since X is a subset
of H.
This proves the case Y âŠ†H++, so it only remains to consider the case
when Y is a subset of the closed halfspace H+ without being a subset of the
corresponding open halfspace, i.e. the case
Y âŠ†H+, Y âˆ©H Ì¸= âˆ….
Due to our induction hypothesis, it is possible to separate the nonempty
polyhedron Y1 = Y âˆ©H and X inside the (n âˆ’1)-dimensional hyperplane H
using an aï¬ƒne (n âˆ’2)-dimensional subset L of H which does not contain X
as a subset. L divides the hyperplane H into two closed halves L+ and Lâˆ’
with L as their common relative boundary, and with X as a subset of Lâˆ’
and Y1 as a subset of L+.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
102
Polyhedra
Let us denote the relative interior of Lâˆ’by Lâˆ’âˆ’, so that Lâˆ’âˆ’= Lâˆ’\ L.
The assumption that X is not a subset of L implies that X âˆ©Lâˆ’âˆ’Ì¸= âˆ….
Observe that Y âˆ©Lâˆ’= Y1âˆ©L. If Y1âˆ©L = âˆ…, then there exists a hyperplane
that strictly separates the polyhedra Y och Lâˆ’, by Theorem 5.5.1, and since
X âŠ†Lâˆ’, we are done in this case, too.
What remains is to treat the case Y1 âˆ©L Ì¸= âˆ…, and by performing a
translation, if necessary, we may assume that the origin lies in Y1 âˆ©L, which
implies that L is a linear subspace. See ï¬gure 5.2.
X
Y
L+
Lâˆ’
Y1
H
H+
L
Figure 5.2. Illustration for the proof of Theorem 5.5.2.
Note that the set H++ âˆªL+ is a cone and that Y is a subset of this cone.
Now, consider the cone con Y generated by the polyhedron Y , and let
C = L + con Y.
C is a cone, too, and a subset of the cone H++ âˆªL+, since Y and L are both
subsets of the last mentioned cone. The cone con Y is polyhedral, because
if the polyhedron Y is written as Y = cvx A + con B with ï¬nite sets A and
B, then con Y = con(A âˆªB) due to the fact that 0 lies in Y . Since the
sum of two polyhedral cones is polyhedral, it follows that the cone C is also
polyhedral.
The cone C is disjoint from the set Lâˆ’âˆ’, since the sets Lâˆ’âˆ’and H++âˆªL+
are disjoint.
Now write the polyhedral cone C as an intersection  Ki of ï¬nitely many
closed halfspaces Ki which are bounded by hyperplanes Hi through the origin.
Each halfspace Ki is a cone containing Y as well as L. If a given halfspace Ki
contains in addition a point from Lâˆ’âˆ’, then it contains the cone generated
by that point and L, that is all of Lâˆ’.
Therefore, since C =  Ki and
C âˆ©Lâˆ’âˆ’= âˆ…, we conclude that there exists a halfspace Ki that does not
contain any point from Lâˆ’âˆ’. In other words, the corresponding boundary
hyperplane Hi separates Lâˆ’and the cone C and is disjoint from Lâˆ’âˆ’. Since
X âŠ†Lâˆ’, Y âŠ†C and X âˆ©Lâˆ’âˆ’Ì¸= âˆ…, Hi separates the sets X and Y and
does not contain X. This completes the induction step and the proof of the
theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
103
Polyhedra
103
Exercises
5.1 Find the extreme points of the following polyhedra X:
a) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¥2, x2 â‰¥âˆ’1}
b) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¤2, x2 â‰¥âˆ’1}
c) X = {x âˆˆR3 | 2x1 + x2 + x3 â‰¤4, x1 + 2x2 + x3 â‰¤4, x â‰¥0}
d) X = {x âˆˆR4 | x1 + x2 + 3x3 + x4 â‰¤4, 2x2 + 3x3 â‰¥5, x â‰¥0}.
5.2 Find the extreme rays of the cone
X = {x âˆˆR3 | x1 âˆ’x2 + 2x3 â‰¥0, x1 + 2x2 âˆ’2x3 â‰¥0, x2 + x3 â‰¥0, x3 â‰¥0}.
5.3 Find a matrix C such that
con{(1, âˆ’1, 1), (âˆ’1, 0, 1), (3, 2, 1), (âˆ’2, âˆ’1, 0)} = {x âˆˆR3 | Cx â‰¥0}.
5.4 Find ï¬nite sets A and B such that X = con A + cvx B for the following
polyhedra:
a) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¥2, x2 â‰¥âˆ’1}
b) X = {x âˆˆR2 | âˆ’x1 + x2 â‰¤2, x1 + 2x2 â‰¤2, x2 â‰¥âˆ’1}
c) X = {x âˆˆR3 | 2x1 + x2 + x3 â‰¤4, x1 + 2x2 + x3 â‰¤4, x â‰¥0}
d) X = {x âˆˆR4 | x1 + x2 + 3x3 + x4 â‰¤4, 2x2 + 3x3 â‰¥5, x â‰¥0}.
5.5 Suppose 0 lies in the polyhedron X = cvx A + con B, where A and B are
ï¬nite sets. Prove that con X = con(A âˆªB).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
104
Convex functions
Chapter 6
Convex functions
6.1
Basic deï¬nitions
Epigraph and sublevel set
Deï¬nition. Let f : X â†’R be a function with domain X âŠ†Rn and codomain
R, i.e. the real numbers extended with âˆ. The set
epi f = {(x, t) âˆˆX Ã— R | f(x) â‰¤t}
is called the epigraph of the function.
Let Î± be a real number. The set
sublevÎ± f = {x âˆˆX | f(x) â‰¤Î±}
is called a sublevel set of the function, or more precisely, the Î±-sublevel set.
The epigraph is a subset of Rn+1, and the word â€™epiâ€™ means above. So
epigraph means above the graph.
Rn
R
Î±
sublevÎ± f
epi f
Figure 6.1. Epigraph and a sublevel set
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
105
Convex functions
x
y
Figure 6.2. The graph of a convex function
We remind the reader of the notation dom f for the eï¬€ective domain of
f, i.e. the set of points where the function f : X â†’R is ï¬nite. Obviously,
dom f = {x âˆˆX | f(x) < âˆ}
is equal to the union of all the sublevel sets of f, and these form an increasing
family of sets, i.e.
dom f =

Î±âˆˆR
sublevÎ± f
and
Î± < Î² â‡’sublevÎ± f âŠ†sublevÎ² f.
This implies that dom f is a convex set if all the sublevel sets are convex.
Convex functions
Deï¬nition. A function f : X â†’R is called convex if its domain X and
epigraph epi f are convex sets.
A function f : X â†’R is called concave if the function âˆ’f is convex.
Example 6.1.1. The epigraph of an aï¬ƒne function is a closed halfspace. All
aï¬ƒne functions, and in particular all linear functions, are thus convex and
concave.
Example 6.1.2. The exponential funcion ex with R as domain of deï¬nition
is a convex function.
To see this, we replace x with xâˆ’a in the elementary inequality ex â‰¥x+1
and obtain the inequality ex â‰¥(xâˆ’a)ea+ea, which implies that the epigraph
of the exponential function can be expressed as the intersection

aâˆˆR
{(x, y) âˆˆR2 | y â‰¥(x âˆ’a)ea + ea}
of a family of closed halfspaces in R2. The epigraph is thus convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
106
Convex functions
106
Theorem 6.1.1. The eï¬€ective domain dom f and the sublevel sets sublevÎ± f
of a convex function f : X â†’R are convex sets.
Proof. Suppose that the domain X is a subset of Rn and consider the pro-
jection P1 : Rn Ã— R â†’Rn of Rn Ã— R onto its ï¬rst factor, i.e. P1(x, t) = x.
Let furthermore KÎ± denote the closed halfspace {x âˆˆRn+1 | xn+1 â‰¤Î±}.
Then sublevÎ± f = P1(epi f âˆ©KÎ±), for
f(x) â‰¤Î± â‡”âˆƒt: f(x) â‰¤t â‰¤Î± â‡”âˆƒt: (x, t) âˆˆepi f âˆ©KÎ±
â‡”x âˆˆP1(epi f âˆ©KÎ±).
The intersections epi f âˆ©KÎ± are convex sets, and since convexity is preserved
by linear maps, it follows that the sublevel sets sublevÎ± f are convex. Con-
sequently, their union dom f is also convex.
Quasiconvex functions
Many important properties of convex functions are consequences of the mere
fact that their sublevel sets are convex. This is the reason for paying special
attention to functions with convex sublevel sets and motivates the following
deï¬nition.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
107
Convex functions
Deï¬nition. A function f : X â†’R is called quasiconvex if X and all its
sublevel sets sublevÎ± f are convex.
A function f : X â†’R is called quasiconcave if âˆ’f is quasiconvex.
Convex functions are quasiconvex since their sublevel sets are convex. The
converse is not true, because a function f that is deï¬ned on some subinterval
I of R is quasiconvex if it is increasing on I, or if it is decreasing on I, or
more generally, if there exists a point c âˆˆI such that f is decreasing to the
left of c and increasing to the right of c. There are, of course, non-convex
functions of this type.
Convex extensions
The eï¬€ective domain dom f of a convex (quasiconvex) function f : X â†’R
is convex, and since
epi f = {(x, t) âˆˆdom f Ã— R | f(x) â‰¤t}
and
sublevÎ± f = {x âˆˆdom f | f(x) â‰¤Î±},
the restriction f|dom f of f to dom f is also a convex (quasiconvex) function,
and the restriction has the same epigraph and the same Î±-sublevel sets as f.
So what is the point of allowing âˆas a function value of a convex func-
tion? We are of course primarily interested in functions with ï¬nite values but
functions with inï¬nite values arise naturally as suprema or limits of sequences
of functions with ï¬nite values.
Another beneï¬t of allowing âˆas a function value of (quasi)convex func-
tions is that we can without restriction assume that they are deï¬ned on the
entire space Rn. For if f : X â†’R is a (quasi)convex function deï¬ned on a
proper subset X of Rn, and if we deï¬ne the function Ëœf : Rn â†’R by
Ëœf(x) =

f(x)
if x âˆˆX
âˆ
if x /âˆˆX,
then f and Ëœf have the same epigraphs and the same Î±-sublevel sets. The
extension Ëœf is therefore also (quasi)convex. Of course, dom Ëœf = dom f.
(Quasi)concave functions have an analogous extension to functions with
values in R = R âˆª{âˆ’âˆ}.
Alternative characterization of convexity
Theorem 6.1.2. A function f : X â†’R with a convex domain of deï¬nition
X is
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
108
Convex functions
(a) convex if and only if
(6.1)
f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y)
for all points x, y âˆˆX and all numbers Î» âˆˆ]0, 1[;
(b) quasiconvex if and only if
(6.2)
f(Î»x + (1 âˆ’Î»)y) â‰¤max{f(x), f(y)}
for all points x, y âˆˆX and all numbers Î» âˆˆ]0, 1[.
Proof. (a) Suppose f is convex, i.e. that the epigraph epi f is convex, and
let x and y be two points in dom f. Then the points (x, f(x)) and (y, f(y))
belong to the epigraph, and the convexity of the epigraph implies that the
convex combination

Î»x + (1 âˆ’Î»)y, Î»f(x) + (1 âˆ’Î»)f(y)

of these two points also belong to the epigraph. This statement is equivalent
to the inequality (6.1) being true. If any of the points x, y âˆˆX lies outside
dom f, then the inequality is trivially satisï¬ed since the right hand side is
equal to âˆin that case.
To prove the converse, we assume that the inequality (6.1) holds. Let
(x, s) and (y, t) be two points in the epigraph, and let 0 < Î» < 1. Then
f(x) â‰¤s and f(y) â‰¤t, by deï¬nition, and it therefore follows from the
inequality (6.1) that
f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y) â‰¤Î»s + (1 âˆ’Î»)t,
so the point (Î»x+(1âˆ’Î»)y, Î»s+(1âˆ’Î»)t), i.e. the point Î»(x, s)+(1âˆ’Î»)(y, t),
lies in the epigraph. In orther words, the epigraph is convex.
(b) The proof is analogous and is left to the reader.
A function f : X â†’R is clearly (quasi)convex if and only if the restriction
f|L is (quasi)convex for each line L that intersects X. Each such line has
an equation of the form x = x0 + tv, where x0 is a point in X and v is a
vector in Rn, and the corresponding restriction is a one-variable function
g(t) = f(x0 + tv) (with the set {t | x0 + tv âˆˆX} as its domain of deï¬nition).
To decide whether a function is (quasi)convex or not is thus essentially a
one-variable problem.
Deï¬nition. Let f : X â†’R be a function deï¬ned on a convex cone X. The
function is called
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
109
Convex functions
109
â€¢ subadditive if f(x + y) â‰¤f(x) + f(y) for all x, y âˆˆX;
â€¢ positive homogeneous if f(Î±x) = Î±f(x) for all x âˆˆX and all Î± âˆˆR+.
Every positive homogeneous, subadditive function is clearly convex. Con-
versely, every convex, positive homogeneous function f is subadditive, be-
cause
f(x + y) = 2f( 1
2x + 1
2y) â‰¤2(1
2f(x) + 1
2f(y)) = f(x) + f(y).
A seminorm on Rn is a function f : Rn â†’R, which is subadditive,
positive homogeneous, and symmetric, i.e. satisï¬es the condition
f(âˆ’x) = f(x)
for all x âˆˆRn.
The symmetry and homogenouity conditions may of course be merged to
the condition
f(Î±x) = |Î±|f(x) for all x âˆˆRn and all Î± âˆˆR.
If f is a seminorm, then f(x) â‰¥0 for all x, since
0 = f(0) = f(x âˆ’x) â‰¤f(x) + f(âˆ’x) = 2f(x).
A seminorm f is called a norm if f(x) = 0 implies x = 0. The usual
notation for a norm is âˆ¥Â·âˆ¥.
Seminorms, and in particular norms, are convex functions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
110
Convex functions
Example 6.1.3. The Euclidean norm and the â„“1-norm, that were deï¬ned in
Chapter 1, are special cases of the â„“p-norms âˆ¥Â·âˆ¥p on Rn. They are deï¬ned
for 1 â‰¤p < âˆby
âˆ¥xâˆ¥p =
 n

i=1
|xi|p1/p,
and for p = âˆby
âˆ¥xâˆ¥âˆ= max
1â‰¤iâ‰¤n |xi|.
The maximum norm âˆ¥Â·âˆ¥âˆis a limiting case, because âˆ¥xâˆ¥p â†’âˆ¥xâˆ¥âˆas p â†’âˆ.
The â„“p-norms are obviously positive homogeneous and symmetric and
equal to 0 only if x = 0. Subadditivity is an immediate consequence of the
triangle inequality |x + y| â‰¤|x| + |y| for real numbers when p = 1 or p = âˆ,
and of the Cauchyâ€“Schwarz inequality when p = 2. For the remaining values
of p, subadditivity will be proved in Section 6.4 (Theorem 6.4.3).
Strict convexity
By strengthening the inequalities in the alternative characterization of con-
vexity, we obtain the following deï¬nitions.
Deï¬nition. A convex function f : X â†’R is called strictly convex if
f(Î»x + (1 âˆ’Î»)y) < Î»f(x) + (1 âˆ’Î»)f(y)
for all pairs of distinct points x, y âˆˆX and all Î» âˆˆ]0, 1[.
A quasiconvex function f is called strictly quasiconvex if inequality (6.2)
is strict for all pairs of distinct points x, y âˆˆX and all Î» âˆˆ]0, 1[.
A function f is called strictly concave (strictly quasiconcave) if the func-
tion âˆ’f is strictly convex (strictly quasiconvex).
Example 6.1.4. A quadratic form q(x) = âŸ¨x, QxâŸ©= n
i,j=1 qijxixj on Rn is
convex if and only if it is positive semideï¬nite, and the form is strictly convex
if and only if it is positive deï¬nite. This follows from the identity

Î»xi+(1âˆ’Î»)yi

Î»xj+(1âˆ’Î»)yj

= Î»xixj+(1âˆ’Î»)yiyjâˆ’Î»(1âˆ’Î»)(xiâˆ’yi)(xjâˆ’yj)
which after multiplication by qij and summation yields the equality
q(Î»x + (1 âˆ’Î»)y) = Î»q(x) + (1 âˆ’Î»)q(y) âˆ’Î»(1 âˆ’Î»)q(x âˆ’y).
The right hand side is â‰¤Î»q(x) + (1 âˆ’Î»)q(y) for all 0 < Î» < 1 if and only if
q(xâˆ’y) â‰¥0, which holds for all x Ì¸= y if and only if q is positive semideï¬nite.
Strict inequality requires q to be positive deï¬nite.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
111
Convex functions
Jensenâ€™s inequality
The inequalities (6.1) and (6.2) are easily extended to convex combinations
of more than two points.
Theorem 6.1.3. Let f be a function and suppose x = Î»1x1+Î»2x2+Â· Â· Â·+Î»mxm
is a convex combination of the points x1, x2, . . . , xm in the domain of f.
(a) If f is convex, then
(6.3)
f(x) â‰¤
m

j=1
Î»jf(xj).
(Jensenâ€™s inequality)
If f is strictly convex and Î»j > 0 for all j, then equality prevails in (6.3)
if and only if x1 = x2 = Â· Â· Â· = xm.
(b) If f is quasiconvex, then
(6.4)
f(x) â‰¤max
1â‰¤jâ‰¤m f(xj).
If f is strictly quasiconvex and Î»j > 0 for all j, then equality prevails in
(6.4) if and only if x1 = x2 = Â· Â· Â· = xm.
Proof. (a) To prove the Jensen inequality we may assume that all coeï¬ƒcients
Î»j are positive and that all points xj lie in dom f, because the right hand
side of the inequality is inï¬nite if some point xj lies outside dom f. Then

x,
m

j=1
Î»jf(xj)

=
m

j=1
Î»j

xj, f(xj)

,
and the right sum, being a convex combination of elements in the epigraph
epi f, belongs to epi f. So the left hand side is a point in epi f, and this gives
us inequality (6.3).
Now assume that f is strictly convex and that we have equality in Jensenâ€™s
inequality for the convex combination x = m
j=1 Î»jxj, with positive coeï¬ƒ-
cents Î»j and m â‰¥2. Let y = m
j=2 Î»j(1âˆ’Î»1)âˆ’1xj. Then x = Î»1x1+(1âˆ’Î»1)y,
and y is a convex combination of x2, x3, . . . , xm, so it follows from Jensenâ€™s
inequality that
m

j=1
Î»jf(xj) = f(x) â‰¤Î»1f(x1) + (1 âˆ’Î»1)f(y)
â‰¤Î»1f(x1) + (1 âˆ’Î»1)
m

j=2
Î»j(1 âˆ’Î»1)âˆ’1f(xj) =
m

j=1
Î»jf(xj).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
112
Convex functions
112
Since the left hand side and right hand side of this chain of inequalities
and equalities are equal, we conclude that equality holds everywhere. Thus,
f(x) = Î»1f(x1) + (1 âˆ’Î»1)f(y), and since f is strictly convex, this implies
that x1 = y = x.
By symmetri, we also have x2 = x, . . . , xm = x, and hence x1 = x2 =
Â· Â· Â· = xm.
(b) Suppose f is quasiconvex, and let Î± = max1â‰¤jâ‰¤m f(xj). If any of the
points xj lies outside dom f, then there is nothing to prove since the right
hand side of the inequality (6.4) is inï¬nite. In the opposite case, Î± is a ï¬nite
number, and each point xj belongs to the convex sublevel set sublevÎ± f, and
it follows that so does the point x. This proves inequality (6.4).
The proof of the assertion about equality for strictly quasiconvex func-
tions is analogous with the corresponding proof for strictly convex func-
tions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
113
Convex functions
6.2
Operations that preserve convexity
We now describe some ways to construct new convex functions from given
convex functions.
Conic combination
Theorem 6.2.1. Suppose that f : X â†’R and g: X â†’R are convex func-
tions and that Î± and Î² are nonnegative real numbers. Then Î±f + Î²g is also
a convex function.
Proof. Follows directly from the alternative characterization of convexity in
Theorem 6.1.2.
The set of convex functions on a given set X is, in other words, a convex
cone. So every conic combination Î±1f1+Î±2f2+Â· Â· Â·+Î±mfm of convex functions
on X is convex.
Note that there is no counterpart of this statement for quasiconvex func-
tions âˆ’a sum of quasiconvex functions is not necessarily quasiconvex.
Pointwise limit
Theorem 6.2.2. Suppose that the functions fi : X â†’R, i = 1, 2, 3, . . . , are
convex and that the limit
f(x) = lim
iâ†’âˆfi(x)
exists as a ï¬nite number or âˆfor each x âˆˆX. The limit function f : X â†’R
is then also convex.
Proof. Let x and y be two points in X, and suppose 0 < Î» < 1. By passing
to the limit in the inequality fi(Î»x + (1 âˆ’Î»)y) â‰¤Î»fi(x) + (1 âˆ’Î»)fi(y) we
obtain the following inequality
f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y),
which tells us that the limit function f is convex.
Using Theorem 6.2.2, we may extend the result in Theorem 6.2.1 to in-
ï¬nite sums and integrals. For example, a pointwise convergent inï¬nite sum
f(x) = âˆ
i=1 fi(x) of convex functions is convex.
And if f(x, y) is a function that is convex with respect to the variable
x on some set X for each y in a set Y , Î± is a nonnegative function deï¬ned
on Y , and the integral g(x) =

Y Î±(y)f(x, y) dy exists for all x âˆˆX, then
g is a convex function on X. This follows from Theorem 6.2.2 by writing
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
114
Convex functions
the integral as a limit of Riemann sums, or more directly, by integrating the
inequalites that characterize the convexity of the functions f(Â·, y).
Composition with aï¬ƒne maps
Theorem 6.2.3. Suppose A: V â†’Rn is an aï¬ƒne map, that Y is a convex
subset of Rn, and that f : Y â†’R is a convex function. The composition
f â—¦A is then a convex function on its domain of deï¬nition Aâˆ’1(Y )
Proof. Let g = f â—¦A. Then, for x1, x2 âˆˆAâˆ’1(Y ) and 0 < Î» < 1,
g(Î»x1 + (1 âˆ’Î»)x2) = f(Î»Ax1 + (1 âˆ’Î»)Ax2) â‰¤Î»f(Ax1) + (1 âˆ’Î»)f(Ax2)
= Î»g(x1) + (1 âˆ’Î»)g(x2),
which shows that the function g is convex.
The composition f â—¦A of a quasiconvex function f and an aï¬ƒne map A
is quasiconvex.
Example 6.2.1. The function x â†’ec1x1+Â·Â·Â·+cnxn is convex on Rn since it is a
composition of a linear map and the convex exponential function t â†’et.
Pointwise supremum
Theorem 6.2.4. Let fi : X â†’R, i âˆˆI, be a family of functions, and deï¬ne
the function f : X â†’R for x âˆˆX by
f(x) = sup
iâˆˆI
fi(x).
Then
(i) f is convex if the functions fi are all convex;
(ii) f is quasiconvex if the functions fi are all quasiconvex.
Proof. By the least upper bound deï¬nition, f(x) â‰¤t if and only if fi(x) â‰¤t
for all i âˆˆI, and this implies that
epi f =

iâˆˆI
epi fi
and
sublevt f =

iâˆˆI
sublevt fi
for all t âˆˆR. The assertions of the theorem are now immediate consequences
of the fact that intersections of convex sets are convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
115
Convex functions
115
f
f1
f2
f3
Figure 6.3. f = sup fi for a family consisting of three functions.
Example 6.2.2. A pointwise maximum of ï¬nitely many aï¬ƒne functions, i.e.
a function of the form
f(x) = max
1â‰¤iâ‰¤m(âŸ¨ci, xâŸ©+ ai),
is a convex function and is called a convex piecewise aï¬ƒne function.
Example 6.2.3. Examples of convex piecewise aï¬ƒne functions f on Rn are:
(a) The absolute value of the i:th coordinate of a vector
f(x) = |xi| = max{xi, âˆ’xi}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
116
Convex functions
(b) The maximum norm
f(x) = âˆ¥xâˆ¥âˆ= max
1â‰¤iâ‰¤n |xi|.
(c) The sum of the m largest coordinates of a vector
f(x) = max{xi1 + Â· Â· Â· + xim | 1 â‰¤i1 < i2 < Â· Â· Â· < im â‰¤n}.
Composition
Theorem 6.2.5. Suppose that the function Ï†: I â†’R is deï¬ned on a real
intervall I that contains the range f(X) of the function f : X â†’R. The
composition Ï† â—¦f : X â†’R is convex
(i) if f is convex and Ï† is convex and increasing;
(ii) if f is concave and Ï† is convex and decreasing.
Proof. The inequality
Ï†

f(Î»x + (1 âˆ’Î»)y)

â‰¤Ï†

Î»f(x) + (1 âˆ’Î»)f(y)

holds for x, y âˆˆX and 0 < Î» < 1 if either f is convex and Ï† is increasing, or
f is concave and Ï† is decreasing. If in addition Ï† is convex, then
Ï†

Î»f(x) + (1 âˆ’Î»)f(y)

â‰¤Î»Ï†(f(x)) + (1 âˆ’Î»)Ï†(f(y)),
and by combining the two inequalities above, we obtain the inequality that
shows that the function Ï† â—¦f is a convex.
There is a corresponding result for quasiconvexity: The composition Ï†â—¦f
is quasiconvex if either f is quasiconvex and Ï† is increasing, or f is quasi-
concave and Ï† is decreasing.
Example 6.2.4. The function
x â†’ex2
1+x2
2+Â·Â·Â·+x2
k,
where 1 â‰¤k â‰¤n, is convex on Rn, since the exponential function is convex
and increasing, and positive semideï¬nite quadratic forms are convex.
Example 6.2.5. The two functions t â†’1/t and t â†’âˆ’ln t are convex and
decreasing on the interval ]0, âˆ[.
So the function 1/g is convex and the
function ln g is concave, if g is a concave and positive function.
Inï¬mum
Theorem 6.2.6. Let C be a convex subset of Rn+1, and let g be the function
deï¬ned for x âˆˆRn by
g(x) = inf{t âˆˆR | (x, t) âˆˆC},
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
117
Convex functions
with the usual convention inf âˆ…= +âˆ. Suppose there exists a point x0 in the
relative interior of the set
X0 = {x âˆˆRn | g(x) < âˆ} = {x âˆˆRn | âˆƒt âˆˆR: (x, t) âˆˆC}
with a ï¬nite function value g(x0). Then g(x) > âˆ’âˆfor all x âˆˆRn, and
g: Rn â†’R is a convex function with X0 as its eï¬€ective domain.
Proof. Let x be an arbitrary point in X0. To show that g(x) > âˆ’âˆ, i.e. that
the set
Tx = {t âˆˆR | (x, t) âˆˆC}
is bounded below, we ï¬rst choose a point x1 âˆˆrint X0 such that x0 lies on
the open line segment ]x, x1[, and write x0 = Î»x + (1 âˆ’Î»)x1 with 0 < Î» < 1.
We then ï¬x a real number t1 such that (x1, t1) âˆˆC, and for t âˆˆTx deï¬ne
the number t0 as t0 = Î»t + (1 âˆ’Î»)t1. The pair (x0, t0) is then a convex
combination of the points (x, t) and (x1, t1) in C, so
g(x0) â‰¤t0 = Î»t + (1 âˆ’Î»)t1,
by convexity and the deï¬nition of g. We conclude that
t â‰¥1
Î»

g(x0) âˆ’(1 âˆ’Î»)t1

,
and this inequality shows that the set Tx is bounded below.
So the function g has R as codomain, and dom g = X0. Now, let x1 and
x2 be arbitrary points in X0, and let Î»1 and Î»2 be two positive numbers with
sum 1. To each Ïµ > 0 there exist two real numbers t1 and t2 such that the
two points (x1, t1) and (x2, t2) lie in C and t1 < g(x1) + Ïµ and t2 < g(x2) + Ïµ.
The convex combination (Î»1x1 + Î»2x2, Î»1t1 + Î»2t2) of the two points lies in
C, too, and
g(Î»1x1 + Î»2x2) â‰¤Î»1t1 + Î»2t2 â‰¤Î»1g(x1) + Î»2g(x2) + Ïµ.
This means that the point Î»1x1 + Î»2x2 lies in X0, and by letting Ïµ tend to 0
we conclude that g(Î»1x1 + Î»2x2) â‰¤Î»1g(x1) + Î»2g(x2). Hence, the set X0 is
convex, and the function g is convex.
We have seen that the pointwise supremum f(x) = supiâˆˆI fi(x) of an
arbitrary family of convex functions is convex. So if f : X Ã— Y â†’R is a
function with the property that the functions f(Â·, y) are convex on X for
each y âˆˆY , and we deï¬ne the function g on X by g(x) = supyâˆˆY f(x, y),
then g is convex, and this is true without any further conditions on the set
Y . Our next theorem shows that the corresponding inï¬mum is a convex
function, provided f is convex as a function on the product set X Ã— Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
118
Convex functions
118
Theorem 6.2.7. Suppose f : X Ã— Y â†’R is a convex function, and for each
x âˆˆX deï¬ne
g(x) = inf
yâˆˆY f(x, y).
If there is a point x0 âˆˆrint X such that g(x0) > âˆ’âˆ, then g(x) is a ï¬nite
number for each x âˆˆX, and g: X â†’R is a convex function.
Proof. Suppose X is a subset of Rn and let
C = {(x, t) âˆˆX Ã— R | âˆƒy âˆˆY : f(x, y) â‰¤t}.
C is a convex subset of Rn+1, because given two points (x1, t1) and (x2, t2)
in C, and two positive numbers Î»1 and Î»2 with sum 1, there exist two points
y1 and y2 in the convex set Y such that f(xi, yi) â‰¤ti for i = 1, 2, and this
implies that
f(Î»1x1 + Î»2x2, Î»1y1 + Î»2y2) â‰¤Î»1f(x1, y1) + Î»2f(x2, y2) â‰¤Î»1t1 + Î»2t2,
which shows that the convex combination Î»1(x1, t1) + Î»2(x2, t2) lies in C.
Moreover, g(x) = inf{t | (x, t) âˆˆC}, so the corollary follows immediately
from Theorem 6.2.6.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
119
Convex functions
Perspective
Deï¬nition. Let f : X â†’R be a function deï¬ned on a cone X in Rn. The
function g: X Ã— R++ â†’R, deï¬ned by
g(x, s) = sf(x/s),
is called the perspective of f.
Theorem 6.2.8. The perspective g of a convex function f : X â†’R with a
convex cone X as domain is a convex function.
Proof. Let (x, s) and (y, t) be two points in X Ã— R++, and let Î±, Î² be two
positive numbers with sum 1. Then
g

Î±(x, s) + Î²(y, t)

= g(Î±x + Î²y, Î±s + Î²t) = (Î±s + Î²t)f
Î±x + Î²y
Î±s + Î²t

= (Î±s + Î²t)f

Î±s
Î±s + Î²t Â· x
s +
Î²t
Î±s + Î²t Â· y
t

â‰¤Î±sf
x
s

+ Î²tf
y
t

= Î±g(x, s) + Î²g(y, t).
Example 6.2.6. By the previous theorem, f(x) = xnq(x/xn) is a convex
function on Rnâˆ’1 Ã— R++ whenever q(x) is a positive semideï¬nite quadratic
form on Rnâˆ’1. In particular, by choosing the Euclidean norm as quadratic
form, we see that the function
x â†’(x2
1 + x2
2 + Â· Â· Â· + x2
nâˆ’1)/xn
is convex on the open halfspace Rnâˆ’1 Ã— R++.
6.3
Maximum and minimum
Minimum points
For an arbitrary function to decide whether a given point is a global minimum
point is an intractable problem, but there are good numerical methods for
ï¬nding local minimum points if we impose some regularity conditions on the
function.
This is the reason why convexity plays such an important role
in optimization theory. A local minimum of a convex function is namely
automatically a global minimum.
Let us recall that a point x0 âˆˆX is a local minimum point of the function
f : X â†’R if there exists an open ball B = B(x0; r) with center at x0 such
that f(x) â‰¥f(x0) for all x âˆˆX âˆ©B. The point is a (global) minimum point
if f(x) â‰¥f(x0) for all x âˆˆX.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
120
Convex functions
Theorem 6.3.1. Suppose that the function f : X â†’R is convex and that
x0 âˆˆdom f is a local minimum point of f. Then x0 is a global minimum
point. The minimum point is unique if f is strictly convex.
Proof. Let x âˆˆX be an arbitrary point diï¬€erent from x0. Since f is a convex
function and Î»x + (1 âˆ’Î»)x0 â†’x0 as Î» â†’0, the following inequalities hold
for Î» > 0 suï¬ƒciently close to 0:
f(x0) â‰¤f(Î»x + (1 âˆ’Î»)x0) â‰¤Î»f(x) + (1 âˆ’Î»)f(x0)
(with strict inequality in the last place if f is strictly convex). From this
follows at once that f(x) â‰¥f(x0) (and f(x) > f(x0), respectively), which
proves that x0 is a global minimum point (and that there are no other mini-
mum points if the convexity is strict)
Theorem 6.3.2. The set of minimum points of a quasiconvex function is
convex.
Proof. The assertion is trivial for functions with no minimum point, since
the empty set is convex, and for the function which is identically equal to âˆ
on X. So, suppose that the quasiconvex function f : X â†’R has a minimum
point x0 âˆˆdom f. The set of minimum points is then equal to the sublevel
set {x âˆˆX | f(x) â‰¤f(x0)}, which is convex by deï¬nition.
Maximum points
Theorem 6.3.3. Suppose X = cvx A and that the function f : X â†’R is
quasiconvex. Then
sup
xâˆˆX
f(x) = sup
aâˆˆA
f(a).
If the function has a maximum, then there is a maximum point in A.
Proof. Let x âˆˆX. Since x is a convex combination x = m
j=1 Î»jaj of ele-
ments aj âˆˆA,
f(x) = f(
m

j=1
Î»jaj) â‰¤max
1â‰¤jâ‰¤m f(aj) â‰¤sup
aâˆˆA
f(a),
and it follows that
sup
xâˆˆX
f(x) â‰¤sup
aâˆˆA
f(a).
The converse inequality being trivial, since A is a subset of X, we conclude
that equality holds.
Moreover, if x is a maximum point, then f(x) â‰¥max1â‰¤jâ‰¤m f(aj), and
combining this with the inequality above, we obtain f(x) = max1â‰¤jâ‰¤m f(aj),
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
121
Convex functions
121
which means that the maximum is certainly attained at some of the points
aj âˆˆA.
Thus, we can ï¬nd the maximum of a quasiconvex function whose domain
is the convex hull of a ï¬nite set A, by just comparing ï¬nitely many function
values. Of course, this may be infeasible if the set A is very large.
Since compact convex sets coincide with the convex hull of their extreme
points, we have the following corollary of the previous theorem.
Corollary 6.3.4. Suppose that X is a compact convex set and that f : X â†’R
is a quasiconvex function. If f has a maximum, then there is a maximum
point among the extreme points of X.
Example 6.3.1. The quadratic form f(x1, x2) = x2
1 + 2x1x2 + 2x2
2 is strictly
convex, since it positive deï¬nite. The maximum of f on the traingle with
vertices at the points (1, 1), (âˆ’2, 1) and (0, 2) is attained at some of the
vertices. The function values at the vertices are 5, 2, and 8, respectively.
The maximum value is hence equal to 8, and it is attained at (0, 2).
A non-constant realvalued convex function can not attain its maximum
at an interior point of its domain, because of the following theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
122
Convex functions
Theorem 6.3.5. A convex function f : X â†’R that attains its maximum at
a relative interior point of X, is necessarily constant on X.
Proof. Suppose f has a maximum at the point a âˆˆrint X, and let x be an
arbitrary point in X. Since a is a relative interior point, there exists a point
y âˆˆX such that a belongs to the open line segment ]x, y[, i.e. a = Î»x+(1âˆ’Î»)y
for some number Î» satisfying 0 < Î» < 1. By convexity and since f(y) â‰¤f(a),
f(a) = f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y) â‰¤Î»f(x) + (1 âˆ’Î»)f(a),
with f(x) â‰¥f(a) as conclusion. Since the converse inequality holds trivially,
we have f(x) = f(a). The function f is thus equal to f(a) everywhere.
6.4
Some important inequalities
Many inequalities can be proved by convexity arguments, and we shall give
three important examples.
Arithmetic and geometric mean
Deï¬nition. Let Î¸1, Î¸2, . . . , Î¸n be given positive numbers with n
j=1 Î¸j = 1.
The weighted arithmetic mean A and the weighted geometric mean G of n pos-
itive numbers a1, a2, . . . , an with the given numbers Î¸1, Î¸2, . . . , Î¸n as weights
are deï¬ned as
A =
n

j=1
Î¸jaj
and
G =
n

j=1
a
Î¸j
j .
The usual arithmetic and geometric means are obtained as special cases
by taking all weights equal to 1/n.
We have the following well-known inequality between the arithmetic and
the geometric means.
Theorem 6.4.1. For all positive numbers a1, a2, . . . , an
G â‰¤A
with equality if and only if a1 = a2 = Â· Â· Â· = an.
Proof. Let xj = ln aj, so that aj = exj = exp(xj). The inequality G â‰¤A is
now transformed to the inequality
exp
 n

j=1
Î¸jxj

â‰¤
n

j=1
Î¸j exp(xj),
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
123
Convex functions
which is Jensenâ€™s inequality for the strictly convex exponential function, and
equality holds if and only if x1 = x2 = Â· Â· Â· = xn, i.e. if and only if a1 = a2 =
Â· Â· Â· = an.
Example 6.4.1. A lot of maximum and minimum problems can be solved
by use of the inequality of arithmetic and geometric means. Here follows a
general example.
Let f be a function of the form
f(x) =
m

i=1
ci
 n

j=1
x
Î±ij
j

,
x âˆˆRn
where ci > 0 and Î±ij are real numbers for all i, j.
The function g(x) = 16x1 +2x2 +xâˆ’1
1 xâˆ’2
2 , corresponding to n = 2, m = 3,
c = (16, 2, 1) and
Î± = [Î±ij] =
ï£®
ï£°
1
0
0
1
âˆ’1
âˆ’2
ï£¹
ï£»,
serves as a typical example of such a function.
Suppose that we want to minimize f(x) over the set {x âˆˆRn | x > 0}.
This problem can be attacked in the following way.
Let Î¸1, Î¸2, . . . , Î¸m be
positive numbers with sum equal to 1, and write
f(x) =
m

i=1
Î¸i
ci
Î¸i
n

j=1
x
Î±ij
j

.
The inequality of arithmetic and geometric means now gives us the following
inequality
(6.5)
f(x) â‰¥
m

i=1
ci
Î¸i
Î¸i n

j=1
x
Î¸iÎ±ij
j

= C(Î¸) Â·
n

j=1
x
Î²j
j ,
with
C(Î¸) =
m

i=1
ci
Î¸i
Î¸i
and
Î²j =
m

i=1
Î¸iÎ±ij.
If it is possible to choose the weights Î¸i > 0 so that m
i=1 Î¸i = 1 and
Î²j =
m

i=1
Î¸iÎ±ij = 0
for all j,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
124
Convex functions
124
then inequality (6.5) becomes
f(x) â‰¥C(Î¸),
and equality occurs if and only if all the products ci
Î¸i
n

j=1
x
Î±ij
j
are equal, a
condition that makes it possible to determine x.
HÂ¨olderâ€™s inequality
Theorem 6.4.2 (HÂ¨olderâ€™s inequallity). Suppose 1 â‰¤p â‰¤âˆand let q be the
dual index deï¬ned by the equality
1
p + 1
q = 1.
Then
|âŸ¨x, yâŸ©| =

n

j=1
xjyj
 â‰¤âˆ¥xâˆ¥pâˆ¥yâˆ¥q
for all x, y âˆˆRn. Moreover, to each x there corresponds a y with norm
âˆ¥yâˆ¥q = 1 such that âŸ¨x, yâŸ©= âˆ¥xâˆ¥p.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
125
Convex functions
Remark. Observe that q = 2 when p = 2. Thus, the Cauchyâ€“Schwarz in-
equality is a special case of HÂ¨olderâ€™s inequality.
Proof. The case p = âˆfollows directly from the triangle inequality for sums:

n

j=1
xjyj
 â‰¤
n

j=1
|xj||yj| â‰¤
n

j=1
âˆ¥xâˆ¥âˆ|yj| = âˆ¥xâˆ¥âˆâˆ¥yâˆ¥1.
So assume that 1 â‰¤p < âˆ.
Since
n
1 xjyj
 â‰¤n
1|xj||yj|, and the
vector (|x1|, . . . , |xn|) has the same â„“p-norm as (x1, . . . , xn) and the vector
(|y1|, . . . , |yn|) has the same â„“q-norm as (y1, . . . , yn), we can without loss of
generality assume that the numbers xj and yj are positive.
The function t â†’tp is convex on the interval [0, âˆ[. Hence,
(6.6)
 n

j=1
Î»jtj
p
â‰¤
n

j=1
Î»jtp
j.
for all positive numbers t1, t2, . . . , tn and all positive numbers Î»1, Î»2, . . . , Î»n
with n
1 Î»j = 1. Now, let us make the particular choice
Î»j =
yq
j
n
j=1 yq
j
and
tj = xjyj
Î»j
.
Then
Î»jtj = xjyj
and
Î»jtp
j = xp
jyp
j
y(pâˆ’1)q
j
 n

j=1
yq
j
pâˆ’1
= xp
j
 n

j=1
yq
j
pâˆ’1
,
which inserted in the inequality (6.6) gives
 n

j=1
xjyj
p
â‰¤
p

j=1
xp
j
 n

j=1
yq
j
pâˆ’1
,
and we obtain HÂ¨olderâ€™s inequality by raising both sides to 1/p.
It is easy to verify that HÂ¨olderâ€™s inequality holds with equality and that
âˆ¥yâˆ¥q = 1 if we choose y as follows:
x = 0 :
All y with norm equal to 1.
x Ì¸= 0, 1 â‰¤p < âˆ:
yj =

âˆ¥xâˆ¥âˆ’p/q
p
|xj|p/xj
if xj Ì¸= 0,
0
if xj = 0.
x Ì¸= 0, p = âˆ:
yj =

|xj|/xj
if j = j0,
0
if j Ì¸= j0,
where j0 is an index such that |xj0| = âˆ¥xâˆ¥âˆ.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
126
Convex functions
Theorem 6.4.3 (Minkowskiâ€™s inequality). Suppose p â‰¥1 and let x and y be
arbitrary vectors in Rn. Then
âˆ¥x + yâˆ¥p â‰¤âˆ¥xâˆ¥p + âˆ¥yâˆ¥p.
Proof. Consider the linear forms x â†’fa(x) = âŸ¨a, xâŸ©for vectors a âˆˆRn
satisfying âˆ¥aâˆ¥q = 1. By HÂ¨olderâ€™s inequality,
fa(x) â‰¤âˆ¥aâˆ¥qâˆ¥xâˆ¥p â‰¤âˆ¥xâˆ¥p,
and for each x there exists a vector a with âˆ¥aâˆ¥q = 1 such that HÂ¨olderâ€™s
inequality holds with equality, i.e. such that fa(x) = âˆ¥xâˆ¥p. Thus
âˆ¥xâˆ¥p = sup{fa(x) | âˆ¥aâˆ¥q = 1},
and hence, f(x) = âˆ¥xâˆ¥p is a convex function by Theorem 6.2.4. Positive
homogenouity is obvious, and positive homogeneous convex functions are
subadditive, so the proof of Minkowskiâ€™s inequality is now complete.
6.5
Solvability of systems of convex inequal-
ities
The solvability of systems of linear inequalities was discussed in Chapter 3.
Our next theorem is kind of a generalization of Theorem 3.3.7 and treats the
solvability of a system of convex and aï¬ƒne inequalities.
Theorem 6.5.1. Let fi : â„¦â†’R, i = 1, 2, . . . , m, be a family of convex
functions deï¬ned on a convex subset â„¦of Rn.
Let p be an integer in the interval 1 â‰¤p â‰¤m, and suppose if p < m that
the functions fi are restrictions to â„¦of aï¬ƒne functions for i â‰¥p + 1 and
that the set
{x âˆˆrint â„¦| fi(x) â‰¤0 for i = p + 1, . . . , m}
is nonempty. The following two assertions are then equivalent:
(i) The system
fi(x) < 0,
i = 1, 2, . . . , p
fi(x) â‰¤0,
i = p + 1, . . . , m
has no solution x âˆˆâ„¦.
(ii) There exist nonnegative numbers Î»1, Î»2, . . . , Î»m, with at least one of the
numbers Î»1, Î»2, . . . , Î»p being nonzero, such that
m

i=1
Î»ifi(x) â‰¥0
for all x âˆˆâ„¦.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
127
Convex functions
127
Remark. The system of inequalities must contain at least one strict inequality,
and all inequalities are allowed to be strict (the case p = m).
Proof. If the system (i) has a solution x, then the sum in (ii) is obviously
negative for the same x, since at least one of its terms is negative and the
others are non-positive. Thus, (ii) implies (i).
To prove the converse implication, we assume that the system (i) has no
solution and deï¬ne M to be the set of all y = (y1, y2, . . . , ym) âˆˆRm such
that the system
fi(x) < yi,
i = 1, 2, . . . , p
fi(x) = yi,
i = p + 1, . . . , m
has a solution x âˆˆâ„¦.
The set M is convex, for if yâ€² and yâ€²â€² are two points in M, 0 â‰¤Î» â‰¤1, and
xâ€², xâ€²â€² are solutions in â„¦of the said systems of inequalities and equalities with
yâ€² and yâ€²â€², respectively, as right hand members, then x = Î»xâ€² +(1âˆ’Î»)xâ€²â€² âˆˆâ„¦
will be a solution of the system with Î»yâ€²+(1âˆ’Î»)yâ€²â€² as its right hand member,
due to the convexity and aï¬ƒnity of the functions fi for i â‰¤p and i > p,
respectively.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
128
Convex functions
Our assumptions concerning the system (i) imply that M âˆ©Rm
âˆ’= âˆ…. Since
Rm
âˆ’is a polyhedron, there exist, by the separation theorem 5.5.2, a nonzero
vector Î» = (Î»1, Î»2, . . . , Î»m) and a real number Î± such that the hyperplane
H = {y | âŸ¨Î», yâŸ©= Î±} separates M and Rm
âˆ’and does not contain M as subset.
We may assume that
Î»1y1 + Î»2y2 + Â· Â· Â· + Î»mym

â‰¥Î±
for all y âˆˆM,
â‰¤Î±
for all y âˆˆRm
âˆ’.
By ï¬rst choosing y = 0, we see that Î± â‰¥0, and by then choosing y = tei,
where ei is the i:th standard basis vector in Rm, and letting t tend to âˆ’âˆ,
we conclude that Î»i â‰¥0 for all i.
For each x âˆˆâ„¦and Ïµ > 0,
y = (f1(x) + Ïµ, . . . , fp(x) + Ïµ, fp+1(x), . . . , fm(x))
is a point in M. Consequently,
Î»1(f1(x) + Ïµ) + Â· Â· Â· + Î»p(fp(x) + Ïµ) + Î»p+1fp+1(x) + Â· Â· Â· + Î»mfm(x) â‰¥Î± â‰¥0,
and by letting Ïµ tend to zero, we obtain the inequality
Î»1f1(x) + Î»2f2(x) + Â· Â· Â· + Î»mfm(x) â‰¥0
for all x âˆˆâ„¦.
If p = m, we are done since the vector Î» = (Î»1, Î»2, . . . , Î»m) is then
nonzero, but it remains to prove that some of the coeï¬ƒcients Î»1, Î»2, . . . , Î»p is
nonzero when p < m. Assume the contrary, i.e. that Î»1 = Î»2 = Â· Â· Â· = Î»p = 0,
and let
h(x) =
m

i=p+1
Î»ifi(x).
The function h is aï¬ƒne, and h(x) â‰¥0 for all x âˆˆâ„¦. Furthermore, by the
assumptions of the theorem, there exists a point x0 in the relative interior
of â„¦such that fi(x0) â‰¤0 for all i â‰¥p + 1, which implies that h(x0) â‰¤0.
Thus, h(x0) = 0. This means that the restriction h|â„¦, which is a concave
function since h is aï¬ƒne, attains its minimum at a relative interior point,
and according to Theorem 6.3.5 (applied to the function âˆ’h|â„¦), this implies
that the function h is constant and equal to 0 on â„¦.
But to each y âˆˆM there corresponds a point x âˆˆâ„¦such that yi = fi(x)
for i = p + 1, . . . m, and this implies that
âŸ¨Î», yâŸ©= m
i=p+1 Î»ifi(x) = h(x) = 0.
We conclude that Î± = 0 and that the hyperplane H contains M, which is a
contradiction. Thus, at least one of the coeï¬ƒcients Î»1, Î»2, . . . , Î»p has to be
nonzero, and the theorem is proved.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
129
Convex functions
6.6
Continuity
A real-valued convex function is automatically continuous at all relative in-
terior points of the domain. More precisely, we have the following theorem.
Theorem 6.6.1. Suppose f : X â†’R is a convex function and that a is
a point in the relative interior of dom f. Then there exist a relative open
neighborhood U of a in dom f and a constant M such that
|f(x) âˆ’f(a)| â‰¤Mâˆ¥x âˆ’aâˆ¥
for all x âˆˆU. Hence, f is continuous on the relative interior of dom f.
Proof. We start by proving a special case of the theorem and then show how
to reduce the general case to this special case.
1. So ï¬rst assume that X is an open subset of Rn, that dom f = X, i.e. that
f is a real-valued convex function, that a = 0, and that f(0) = 0. We will
show that if we choose the number r > 0 such that the closed hypercube
K(r) = {x âˆˆRn | âˆ¥xâˆ¥âˆâ‰¤r}
is included in X, then there is a constant M such that
(6.7)
|f(x)| â‰¤Mâˆ¥xâˆ¥
for all x in the closed ball B(0; r) = {x âˆˆRn | âˆ¥xâˆ¥â‰¤r}, where âˆ¥Â·âˆ¥is the
usual Euclidean norm.
The hypercube K(r) has 2n extreme points (vertices). Let L denote the
largest of the function values of f at these extreme points. Since the convex
hull of the extreme points is equal to K(r), it follows from Theorem 6.3.3
that
f(x) â‰¤L
for all x âˆˆK(r), and thereby also for all x âˆˆB(0; r), because B(0; r) is a
subset of K(r).
We will now make this inequality sharper.
To this end, let x be an
arbitrary point in B(0; r) diï¬€erent from the center 0. The halï¬‚ine from 0
through x intersects the boundary of B(0; r) at the point
y =
r
âˆ¥xâˆ¥x,
and since x lies on the line segment [0, y], x is a convex combination of its
end points. More precisely, x = Î»y + (1 âˆ’Î»)0 with Î» = âˆ¥xâˆ¥/r. Therefore,
since f is convex,
f(x) â‰¤Î»f(y) + (1 âˆ’Î»)f(0) = Î»f(y) â‰¤Î»L = L
r âˆ¥xâˆ¥.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
130
Convex functions
130
The above inequality holds for all x âˆˆB(0; r). To prove the same inequal-
ity with f(x) replaced by |f(x)|, we use the fact that the point âˆ’x belongs
to B(0; r) if x does so, and the equality 0 = 1
2x + 1
2(âˆ’x). By convexity,
0 = f(0) â‰¤1
2f(x) + 1
2f(âˆ’x) â‰¤1
2f(x) + L
2râˆ¥xâˆ¥,
which simpliï¬es to the inequality
f(x) â‰¥âˆ’L
r âˆ¥âˆ’xâˆ¥= âˆ’L
r âˆ¥xâˆ¥.
This proves that inequality (6.7) holds for x âˆˆB(0; r) with M = L/r.
2. We now turn to the general case. Let n be the dimension of the set dom f.
The aï¬ƒne hull of dom f is equal to the set a + V for some n-dimensional
linear subspace V , and since V is isomorphic to Rn, we can obtain a bijective
linear map T : Rn â†’V by choosing a coordinate system in V .
The inverse image Y of the relative interior of dom f under the map
y â†’a + Ty of Rn onto aï¬€(dom f) is an open convex subset of Rn, and Y
contains the point 0. Deï¬ne the function g: Y â†’R by
g(y) = f(a + Ty) âˆ’f(a).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
131
Convex functions
Then, g is a convex function, since g is composed by a convex function and
an aï¬ƒne function, and g(0) = 0.
For x = a + Ty âˆˆrint(dom f) we now have f(x) âˆ’f(a) = g(y) and
x âˆ’a = Ty, so in order to prove the general case of our theorem, we have to
show that there is a constant M such that |g(y)| â‰¤Mâˆ¥Tyâˆ¥for all y in some
neighborhood of 0. But the map y â†’âˆ¥Tyâˆ¥is a norm on Rn, and since all
norms are equivalent, it suï¬ƒces to show that there is a constant M such that
|g(y)| â‰¤Mâˆ¥yâˆ¥
for all y in some neighborhood of 0, and that is exactly what we did in step 1
of the proof. So the theorem is proved.
The following corollary follows immediately from Theorem 6.6.1, because
aï¬ƒne sets have no relative boundary points.
Corollary
6.6.2. A convex function f : X â†’R with an aï¬ƒne subset X as
domain is continuous.
For functions f with a closed interval I = [a, b] as domain, convexity
imposes no other restrictions on the function value f(b) than that it has
to be greater than or equal to limxâ†’bâˆ’f(x). Thus, a convex function need
not be continuous at the endpoint b, and a similar remark holds for the left
endpoint, of course. For example, a function f, that is identically equal to
zero on I \ {a, b}, is convex if f(a) â‰¥0 and f(b) â‰¥0. Cf. exercise 7.6.
6.7
The recessive subspace of convex func-
tions
Example 6.7.1. Let f : R2 â†’R be the convex function
f(x1, x2) = x1 + x2 + e(x1âˆ’x2)2.
The restrictions of f to lines with direction given by the vector v = (1, 1) are
aï¬ƒne functions, since
f(x + tv) = f(x1 + t, x2 + t) = x1 + x2 + 2t + e(x1âˆ’x2)2 = f(x) + 2t.
Let V = {x âˆˆR2 | x1 = x2} be the linear subspace of R2 spanned by the
vector v, and consider the the orthogonal decomposition R2 = V âŠ¥+V . Each
x âˆˆR2 has a corresponding unique decomposition x = y + z with y âˆˆV âŠ¥
and z âˆˆV , namely
y = 1
2(x1 âˆ’x2, x2 âˆ’x1) and z = 1
2(x1 + x2, x1 + x2).
Moreover, since z = 1
2(x1 + x2)v = z1v,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
132
Convex functions
f(x) = f(y + z) = f(y) + 2z1 = f|V âŠ¥(y) + 2z1.
So there is a corresponding decomposition of f as a sum of the restriction
of f to V âŠ¥and a linear function on V . It is easily veriï¬ed that the vector
(v, 2) = (1, 1, 2) spans the recessive subspace lin(epi f), and that V is equal
to the image P1(lin(epi f)) of lin(epi f) under the projection P1 of R2 Ã— R
onto the ï¬rst factor R2.
The result in the previous example can be generalized, and in order to
describe this generalization we need a deï¬nition.
Deï¬nition. Let f : X â†’R be a function deï¬ned on a subset X of Rn.
The linear subspace Vf = P1(lin(epi f)), where P1 : Rn Ã— R â†’Rn is the
projection of Rn Ã— R onto its ï¬rst factor Rn, is called the recessive subspace
of the function f.
Theorem 6.7.1. Let f be a convex function with recessive subspace Vf.
(i) A vector v belongs to Vf if and only if there is a unique number Î±v such
that (v, Î±v) belongs to the recessive subspace lin(epi f) of the epigraph
of the function.
(ii) The map g: Vf â†’R, deï¬ned by g(v) = Î±v for v âˆˆVf, is linear.
(iii) dom f = dom f + Vf.
(iv) f(x + v) = f(x) + g(v) for all x âˆˆdom f and all v âˆˆVf.
(v) If the function f is diï¬€erentiable at x âˆˆdom f then g(v) = âŸ¨f â€²(x), vâŸ©
for all v âˆˆVf.
(vi) Suppose V is a linear subspace, that h: V â†’R is a linear map, that
dom f + V âŠ†dom f, and that f(x + v) = f(x) + h(v) for all x âˆˆdom f
and all v âˆˆV . Then, V âŠ†Vf.
Proof. (i) By deï¬nition, v âˆˆVf if and only if there is a real number Î±v such
that (v, Î±v) âˆˆlin(epi f). To prove that the number Î±v is uniquely determined
by v âˆˆVf, we assume that the pair (v, Î²) also lies in lin(epi f).
The point (x+tv, f(x)+tÎ±v) belongs to the epigraph for each x âˆˆdom f
and each t âˆˆR, i.e.
(6.8)
x + tv âˆˆdom f
and
f(x + tv) â‰¤f(x) + tÎ±v.
Hence, (x + tv, f(x + tv)) is a point in the epigraph, and our assumption
(v, Î²) âˆˆlin(epi f) now implies that (x + tv âˆ’tv, f(x + tv) âˆ’tÎ²) is a point in
i epi f, too. We conclude that
(6.9)
f(x) â‰¤f(x + tv) âˆ’tÎ²
for all t âˆˆR. By combining the two inequalities (6.8) and (6.9), we obtain
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
133
Convex functions
133
the inequality f(x) â‰¤f(x) + (Î±v âˆ’Î²)t, which holds for all t âˆˆR. This is
possible only if Î² = Î±v, and proves the uniqueness of the number Î±v.
(ii) Let, as before, P1 be the projection of Rn Ã— R onto Rn, and let P2 be
the projection of Rn Ã— R onto the second factor R. The uniqueness result
(i) implies that the restriction of P1 to the linear subspace lin(epi f) is a
bijective linear map onto Vf. Let Q denote the inverse of this restriction; the
map g is then equal to the composition P2 â—¦Q of the two linear maps P2 and
Q, and this implies that g is a linear function.
(iii) The particular choice of t = 1 in (6.8) yields the implication
x âˆˆdom f & v âˆˆVf â‡’x + v âˆˆdom f,
which proves the inclusion dom f + Vf âŠ†dom f, and the converse inclusion
is of course trivial.
(iv) By choosing t = 1 in the inequalities (6.8) and(6.9) and using the fact
that Î±v = Î² = g(v), we obtain the two inequalities f(x + v) â‰¤f(x) + g(v)
and f(x) â‰¤f(x + v) âˆ’g(v), which when combined prove assertion (iv).
(v) Consider the restriction Ï†(t) = f(x + tv) of the function f to the line
through the point x with direction v âˆˆVf. By (iii), Ï† is deï¬ned for all t âˆˆR,
and by (iv), Ï†(t) = f(x) + tg(v). Hence, Ï†â€²(0) = g(v). But if f is diï¬€erenti-
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
134
Convex functions
able at x, then we also have Ï†â€²(0) = âŸ¨f â€²(x), vâŸ©according to the chain rule,
and this proves our assertion (v).
(vi) Suppose v âˆˆV . If (x, s) is an arbitrary point in the epigraph epi f,
then f(x + tv) = f(x) + h(tv) â‰¤s + th(v), which means that the point
(x + tv, s + th(v)) lies in epi f for every real number t. This proves that
(v, h(v)) belongs to lin(epi f) and, consequently, that v is a vector in Vf.
By our next theorem, every convex function is the sum of a convex func-
tion with a trivial recessive subspace and a linear function.
Theorem 6.7.2. Suppose that f is a convex function with recessive subspace
Vf. Let Ëœf denote the restriction of f to dom f âˆ©V âŠ¥
f , and let g: Vf â†’R be
the linear function deï¬ned in Theorem 6.7.1. The recessive subspace V Ëœf of Ëœf
is then trivial, i.e. equal to {0}, dom f = dom f âˆ©V âŠ¥
f + Vf, and
f(y + z) = Ëœf(y) + g(z)
for all y âˆˆdom f âˆ©V âŠ¥
f
and all z âˆˆVf.
Proof. Each x âˆˆRn has a unique decomposition x = y + z with y âˆˆV âŠ¥
f and
z âˆˆVf, and if x âˆˆdom f then y = x âˆ’z âˆˆdom f + Vf = dom f, by Theorem
6.7.1, and hence y âˆˆdom f âˆ©V âŠ¥
f . This proves that dom f = dom f âˆ©V âŠ¥
f +Vf.
The equality f(y + z) = Ëœf(y) + g(z) now follows from (iv) in Theorem
6.7.1, so it only remains to prove that V Ëœf = {0}. Suppose v âˆˆV Ëœf, and let x0
be an arbitrary point in dom Ëœf. Then x0 + v lies in dom Ëœf, too, and since
dom Ëœf âŠ†V âŠ¥
f and V âŠ¥
f is a linear subspace, we conclude that v = (x0 +v)âˆ’x0
is a vector in V âŠ¥
f . This proves the inclusion V Ëœf âŠ†V âŠ¥
f .
Theorem 6.7.1 gives us two linear functions g: Vf â†’R and Ëœg: V Ëœf â†’R
such that f(x + v) = f(x) + g(v) for all x âˆˆdom f and all v âˆˆVf, and
Ëœf(y + w) = Ëœf(y) + Ëœg(w) for all y âˆˆdom f âˆ©V âŠ¥
f and all w âˆˆV Ëœf.
Now, let w be an arbitrary vector in V Ëœf and x be an arbitrary point in
dom f, and write x as x = y +v with y âˆˆdom f âˆ©V âŠ¥
f and v âˆˆVf. The point
y + w lies in dom f âˆ©V âŠ¥
f , and we get the following identities:
f(x + w) = f(y + v + w) = f(y + w + v) = f(y + w) + g(v)
= Ëœf(y + w) + g(v) = Ëœf(y) + Ëœg(w) + g(v)
= f(y) + g(v) + Ëœg(w) = f(x) + Ëœg(w).
Therefore, V Ëœf âŠ†Vf, by Theorem 6.7.1 (v). Hence, V Ëœf âŠ†V âŠ¥
f âˆ©Vf = {0},
which proves that V Ëœf = {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
135
Convex functions
6.8
Closed convex functions
Deï¬nition. A convex function is called closed if it has a closed epigraph.
Theorem 6.8.1. A convex function f : X â†’R is closed if and only if all its
sublevel sets are closed.
Proof. Suppose that X is a subset of Rn and that f is a closed function. Let
XÎ± = sublevÎ± f = {x âˆˆX | f(x) â‰¤Î±}
be an arbitrary nonempty sublevel set of f, and deï¬ne YÎ± to be the set
YÎ± = epi f âˆ©{(x, xn+1) âˆˆRn Ã— R | xn+1 â‰¤Î±}.
The set YÎ± is closed, being the intersection between the closed epigraph epi f
and a closed halfspace, and XÎ± = P(YÎ±), where P : Rn Ã— R â†’Rn is the
projection P(x, xn+1) = x.
Obviously, the recession cone recc YÎ± contains no nonzero vector of the
form v = (0, vn+1), i.e. no nonzero vector in the null space N(P) = {0}Ã—R of
the projection P. Hence, (recc YÎ±)âˆ©N(P) = {0}, so it follows from Theorem
2.7.10 that the sublevel set XÎ± is closed.
To prove the converse, assume that all sublevel sets are closed, and let
(x0, y0) be a boundary point of epi f. Let

(xk, yk)
âˆ
1 be a sequence of points
in epi f that converges to (x0, y0), and let Ïµ be an arbitrary positive number.
Then, since yk â†’y0 as k â†’âˆ, f(xk) â‰¤yk â‰¤y0 + Ïµ for all suï¬ƒciently large
k, so the points xk belong to the sublevel set {x âˆˆX | f(x) â‰¤y0 + Ïµ} for all
suï¬ƒciently large k. The sublevel set being closed, it follows that the limit
point x0 lies in the same sublevel set, i.e. x0 âˆˆX and f(x0) â‰¤y0 + Ïµ, and
since Ïµ > 0 is arbitrary, we conclude that f(x0) â‰¤y0. Hence, (x0, y0) is a
point in epi f. So epi f contains all its boundary points and is therefore a
closed set.
Corollary
6.8.2. Continuous convex functions f : X â†’R with closed do-
mains X are closed functions.
Proof. Follows immediately from Theorem 6.8.1, because the sublevel sets of
real-valued continuous functions with closed domains are closed sets.
Theorem 6.8.3. All nonempty sublevel sets of a closed convex function have
the same recession cone and the same recessive subspace. Hence, all sublevel
sets are bounded if one of the nonempty sublevel sets is bounded.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
136
Convex functions
136
Proof. Let f : X â†’R be a closed convex function, and suppose that x0 is
a point in the sublevel set XÎ± = {x âˆˆX | f(x) â‰¤Î±}. Since XÎ± and epi f
are closed convex sets and (x0, Î±) is a point in epi f, we obtain the following
equivalences:
v âˆˆrecc XÎ± â‡”x0 + tv âˆˆXÎ±
for all t âˆˆR+
â‡”f(x0 + tv) â‰¤Î±
for all t âˆˆR+
â‡”(x0 + tv, Î±) âˆˆepi f
for all t âˆˆR+
â‡”(x0, Î±) + t(v, 0) âˆˆepi f
for all t âˆˆR+
â‡”(v, 0) âˆˆrecc(epi f),
with the conclusion that the recession cone
recc XÎ± = {v âˆˆRn | (v, 0) âˆˆrecc(epi f)}
does not depend on Î± as long as XÎ± Ì¸= âˆ…. Of course, the same is then true
for the recessive subspace
lin XÎ± = recc XÎ± âˆ©(âˆ’recc XÎ±) = {v âˆˆRn | (v, 0) âˆˆlin(epi f)}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
137
Convex functions
The statement concerning bounded sublevel sets follows from the fact
that a closed convex set is bounded if and only if its recession cone is equal
to the zero cone {0}.
Theorem 6.8.4. A convex function f, which is bounded on an aï¬ƒne subset
M, is constant on M.
Proof. Let M = a + U, where U is a linear subspace, and consider the
restriction g = f|M of f to M. The function g is continuous since all points
of M are relative interior points, and closed since the domain M is a closed
set. Let Î± = sup{g(x) | x âˆˆM}; then {x | g(x) â‰¤Î±} = M, so by the
previous theorem, all nonempty sublevel sets of g has lin M, that is the
subspace U, as their recessive subspace.
Let now x0 be an arbitrary point in M. Since the recessive subspace of
the particular sublevel set {x | g(x) â‰¤g(x0} is equal to U, we conclude that
g(x0 + u) â‰¤g(x0) for all u âˆˆU. Hence, g(x) â‰¤g(x0) for all x âˆˆM, which
means that x0 is a maximum point of g. Since x0 âˆˆM is arbitrary, all points
in M are maximum points, and this implies that g is constant on M.
6.9
The support function
Deï¬nition. Let A be a nonempty subset of Rn. The function SA : Rn â†’R,
deï¬ned by
SA(x) = sup{âŸ¨y, xâŸ©| y âˆˆA}
(with the usual convention that SA(x) = âˆif the function y â†’âŸ¨y, xâŸ©is
unbounded above on A) is called the support function of the set A.
Theorem 6.9.1. (a) The support function SA is a closed convex function.
(b) Suppose A and B are nonempty subsets of Rn, that Î± > 0 and that
C : Rn â†’Rm is a linear map. Then
SA = Scvx A = Scl(cvx A)
(i)
SÎ±A = Î±SA
(ii)
SA+B = SA + SB
(iii)
SA âˆªB = max {SA, SB}
(iv)
SC(A) = SA â—¦CT.
(v)
Proof. (a) The support function SA is closed and convex, because its epi-
graph
epi SA = {(x, t) | âŸ¨y, xâŸ©â‰¤t for all y âˆˆA} =

yâˆˆA
{(x, t) | âŸ¨y, xâŸ©â‰¤t}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
138
Convex functions
is closed, being the intersection of a family of closed halfspaces in Rn Ã— R.
(b) Since linear forms are convex, it follows from Theorem 6.3.3 that
SA(x) = sup{âŸ¨x, yâŸ©| y âˆˆA} = sup{âŸ¨x, yâŸ©| y âˆˆcvx A} = Scvx A(x)
for all x âˆˆRn. Moreover, if a function f is continuous on the closure of
a set X, then supyâˆˆX f(y) = supyâˆˆcl X f(y), and linear forms are of course
continuous. Therefore, Scvx A(x) = Scl(cvx A)(x) for all x.
This proves the identity (i), and the remaining identities are obtained as
follows:
SÎ±A(x) = sup
yâˆˆÎ±A
âŸ¨y, xâŸ©= sup
yâˆˆA
âŸ¨Î±y, xâŸ©= Î± sup
yâˆˆA
âŸ¨y, xâŸ©= Î±SA(x).
SA+B(x) = sup
yâˆˆA+B
âŸ¨y, xâŸ©=
sup
y1âˆˆA, y2âˆˆB
âŸ¨y1 + y2, xâŸ©
=
sup
y1âˆˆA, y2âˆˆB
(âŸ¨y1, xâŸ©+ âŸ¨y2, xâŸ©) = sup
y1âˆˆA
âŸ¨y1, xâŸ©+ sup
y2âˆˆB
âŸ¨y2, xâŸ©
= SA(x) + SB(x).
SA âˆªB(x) =
sup
yâˆˆ(A âˆªB)
âŸ¨y, xâŸ©= max {sup
yâˆˆA
âŸ¨y, xâŸ©, sup
yâˆˆB
âŸ¨y, xâŸ©}
= max {SA(x), SB(x)}.
SC(A)(x) = sup
yâˆˆC(A)
âŸ¨y, xâŸ©= sup
zâˆˆA
âŸ¨Cz, xâŸ©= sup
zâˆˆA
âŸ¨z, CTxâŸ©= SA(CTx).
Example 6.9.1. The support function of a closed interval [a, b] on the real
line is given by
S[a,b](x) = S{a,b}(x) = max{ax, bx},
since [a, b] = cvx{a, b}.
Example 6.9.2. In order to ï¬nd the support function of the closed unit ball
Bp = {x âˆˆRn | âˆ¥xâˆ¥p â‰¤1} with respect to the â„“p-norm, we use HÂ¨olderâ€™s
inequality, obtaining
SBp(x) = sup{âŸ¨x, yâŸ©| âˆ¥yâˆ¥p â‰¤1} = âˆ¥xâˆ¥q,
where the relation between p and q is given by the equation 1/p+1/q = 1.
Closed convex sets are completely characterized by their support func-
tions, due to the following theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
139
Convex functions
139
Theorem 6.9.2. Suppose that X1 and X2 are two nonempty closed convex
subsets of Rn with support functions SX1 and SX2, respectively. Then
X1 âŠ†X2 â‡”SX1 â‰¤SX2
(a)
X1 = X2 â‡”SX1 = SX2.
(b)
Proof. Assertion (b) is an immediate consequence of (a), and the implication
X1 âŠ†X2 â‡’SX1 â‰¤SX2 is trivial, so it only remains to prove the converse
implication, or equivalently, the implication X1 Ì¸âŠ†X2 â‡’SX1 Ì¸â‰¤SX2.
To prove the latter implication we assume that X1 Ì¸âŠ†X2, i.e. that there
exists a point x0 âˆˆX1\X2. The point x0 is strictly separable from the closed
convex set X2, which means that there exist a vector c âˆˆRn and a number
b such that âŸ¨x, câŸ©â‰¤b for all x âˆˆX2 while âŸ¨x0, câŸ©> b. Consequently,
SX1(c) â‰¥âŸ¨x0, câŸ©> b â‰¥sup{âŸ¨x, câŸ©| x âˆˆX2} = SX2(c),
which shows that SX1 Ì¸â‰¤SX2.
By combining the previous theorem with property (i) of Theorem 6.9.1,
we obtain the following corollary.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
140
Convex functions
Corollary 6.9.3. Let A and B be two nonempty subsets of Rn. Then,
SA = SB â‡”cl(cvx A) = cl(cvx B).
6.10
The Minkowski functional
Let X be a convex subset of Rn with 0 as an interior point of X. Consider
the sets tX for t â‰¥0. This is an increasing family of sets, whose union equals
all of Rn, i.e. 0 â‰¤s < t â‡’sX âŠ†tX and 
tâ‰¥0 tX = Rn.
The family is increasing, because using the convexity of the sets tX and
the fact that they contain 0, we obtain the following inclusions for 0 â‰¤s < t:
sX = s
t (tX) + (1 âˆ’s
t ) 0 âŠ†s
t (tX) + (1 âˆ’s
t )(tX) âŠ†tX.
That the union equals Rn only depends on 0 being an interior point of
X. For let B(0; r0) be a closed ball centered at 0 and contained in X. An
arbitrary point x âˆˆRn will then belong to the set râˆ’1
0 âˆ¥xâˆ¥X since r0âˆ¥xâˆ¥âˆ’1x
lies in B(0; r0).
Now ï¬x x âˆˆRn and consider the set {t â‰¥0 | x âˆˆtX}. This set is an
unbounded subinterval of [0, âˆ[, and it contains the number râˆ’1
0 âˆ¥xâˆ¥. We
may therefore deï¬ne a function
Ï†X : Rn â†’R+
by letting
Ï†X(x) = inf{t â‰¥0 | x âˆˆtX}.
Obviously,
Ï†X(x) â‰¤râˆ’1
0 âˆ¥xâˆ¥
for all x.
Deï¬nition. The function Ï†X : Rn â†’R+ is called the Minkowski functional
of the set X.
Theorem 6.10.1. The Minkowski functional Ï†X has the following properties:
(i) For all x, y âˆˆRn and all Î» âˆˆR+,
(a) Ï†X(Î»x) = Î»Ï†X(x),
(b) Ï†X(x + y) â‰¤Ï†X(x) + Ï†X(y).
(ii) There exists a constant C such that
|Ï†X(x) âˆ’Ï†X(y)| â‰¤Câˆ¥x âˆ’yâˆ¥
for all x, y âˆˆRn.
(iii) int X = {x âˆˆRn | Ï†X(x) < 1} and
cl X = {x âˆˆRn | Ï†X(x) â‰¤1}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
141
Convex functions
The Minkowski functional is, in other words, positive homogeneous, subaddi-
tive, and Lipschitz continuous. So it is in particular a convex function.
Proof. (i) The equivalence x âˆˆtX â‡”Î»x âˆˆÎ»tX, which holds for Î» > 0,
together with the fact that Ï†X(0) = 0, implies positive homogenouity.
To prove subadditivity we choose, given Ïµ > 0, two positive numbers
s < Ï†X(x) + Ïµ and t < Ï†X(y) + Ïµ such that x âˆˆsX and y âˆˆtX. The point
1
s + t(x + y) =
s
s + t
x
s +
t
s + t
y
t
is a point in X, by convexity, and it follows that the point x + y belongs to
the set (s + t)X. This implies that
Ï†X(x + y) â‰¤s + t < Ï†X(x) + Ï†X(y) + 2Ïµ,
and since this inequality is true for all Ïµ > 0, we conclude that
Ï†X(x + y) â‰¤Ï†X(x) + Ï†X(y).
(ii) We have already noted that the inequality Ï†X(x) â‰¤Câˆ¥xâˆ¥holds for all x
with C = râˆ’1
0 . By subadditivity,
Ï†X(x) = Ï†X(x âˆ’y + y) â‰¤Ï†X(x âˆ’y) + Ï†X(y),
and hence
Ï†X(x) âˆ’Ï†X(y) â‰¤Ï†X(x âˆ’y) â‰¤Câˆ¥x âˆ’yâˆ¥.
For symmetry reasons
Ï†X(y) âˆ’Ï†X(x) â‰¤Câˆ¥y âˆ’xâˆ¥= Câˆ¥x âˆ’yâˆ¥,
and hence |Ï†X(x) âˆ’Ï†X(y)| â‰¤Câˆ¥x âˆ’yâˆ¥.
(iii)
The sets {x âˆˆRn | Ï†X(x) < 1} and {x âˆˆRn | Ï†X(x) â‰¤1} are open
and closed, respectively, since Ï†X is continuous. Therefore, to prove assertion
(iii) it suï¬ƒces, due to the characterization of int X as the largest open set
contained in X and of cl X as the smallest closed set containing X, to prove
the inclusions
int X âŠ†{x âˆˆRn | Ï†X(x) < 1} âŠ†X âŠ†{x âˆˆRn | Ï†X(x) â‰¤1} âŠ†cl X.
Suppose x âˆˆint X. Since tx â†’x as t â†’1, the points tx belong to the
interior of X for all numbers t that are suï¬ƒciently close to 1. Thus, there
exists a number t0 > 1 such that t0x âˆˆX, i.e. such that x âˆˆtâˆ’1
0 X, which
means that Ï†X(x) â‰¤tâˆ’1
0
< 1, and this proves the inclusion
int X âŠ†{x âˆˆRn | Ï†X(x) < 1}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
142
Convex functions
The implications Ï†X(x) < t â‡’x âˆˆtX â‡’Ï†X(x) â‰¤t are direct con-
sequences of the deï¬nition of Ï†X(x), and by choosing t = 1 we obtain the
inclusions
{x âˆˆRn | Ï†X(x) < 1} âŠ†X âŠ†{x âˆˆRn | Ï†X(x) â‰¤1}.
To prove the remaining inclusion it is now enough to prove the inclusion
{x âˆˆRn | Ï†X(x) = 1} âŠ†cl X.
So, suppose Ï†X(x) = 1. Then there is a sequence (tn)âˆ
1 of numbers > 1 such
that tn â†’1 as n â†’âˆand x âˆˆtnX for all n. The points tâˆ’1
n x belong to X
for all n, and since tâˆ’1
n x â†’x as n â†’âˆ, x is a point in the closure cl X.
Exercises
6.1 Find two quasiconvex functions f1, f2 with a non-quasiconvex sum f1 + f2.
6.2 Prove that the following functions f : R3 â†’R are convex:
a) f(x) = x2
1 + 2x2
2 + 5x2
3 + 3x2x3
b) f(x) = 2x2
1 + x2
2 + x2
3 âˆ’2x1x2 + 2x1x3
c) f(x) = ex1âˆ’x2 + ex2âˆ’x1 + x2
3 âˆ’2x3.
6.3 For which values of the real number a is the function
f(x) = x2
1 + 2x2
2 + ax2
3 âˆ’2x1x2 + 2x1x3 âˆ’6x2x3
convex and strictly convex?
6.4 Prove that the function f(x) = x1x2 Â· Â· Â· xn with Rn
+ as domain is quasicon-
cave, and that the function g(x) = (x1x2 Â· Â· Â· xn)âˆ’1 with Rn
++ as domain is
convex.
6.5 Let x[k] denote the k:th biggest coordinate of the point x = (x1, x2, . . . , xn).
In other words, x[1], x[2], . . . , x[n] are the coordinates of x in decreasing order.
Prove for each k that the function f(x) = k
i=1 x[i] is convex.
6.6 Suppose f : R+ â†’R is convex. Prove that
f(x1) + f(x2) + Â· Â· Â· + f(xn) â‰¤f(x1 + x2 + Â· Â· Â· + xn) + (n âˆ’1)f(0)
for all x1, x2, . . . , xn â‰¥0. Note the special case f(0) = 0!
6.7 The function f is deï¬ned on a convex subset of Rn.
Suppose that the
function f(x) + âŸ¨c, xâŸ©is quasiconvex for each c âˆˆRn.
Prove that f is
convex.
6.8 We have derived Corollary 6.2.7 from Theorem 6.2.6. Conversely, prove that
Theorem 6.2.6 follows easily from Corollary 6.2.7.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
143
Convex functions
143
6.9 X is a convex set in Rn with a nonempty interior, and f : X â†’R is a
continuous function, whose restriction to int X is convex. Prove that f is
convex.
6.10 Suppose that the function f : X â†’R is convex. Prove that
inf {f(x) | x âˆˆX} = inf {f(x) | x âˆˆrint(dom f)}.
6.11 Use the method in Example 6.4.1 to determine the minimum of the function
g(x1, x2) = 16x1 + 2x2 + xâˆ’1
1 xâˆ’2
2
over the set x1 > 0, x2 > 0.
6.12 Find the Minkowski functional of
a) the closed unit ball B(0; 1) in Rn with respect to the â„“p-norm âˆ¥Â·âˆ¥p;
b) the halfspace {x âˆˆRn | x1 â‰¤1}.
6.13 Let X be a convex set with 0 as interior point and suppose that the set
is symmetric with respect to 0, i.e. x âˆˆX â‡’âˆ’x âˆˆX.
Prove that the
Minkowski functional Ï†X is a norm, i.e. that
(i) Ï†X(x + y) â‰¤Ï†X(x) + Ï†X(y);
(ii) Ï†X(Î»x) = Î»Ï†X(x) for all Î» âˆˆR;
(iii) Ï†X(x) = 0 â‡”x = 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
144
Smooth convex functions
Chapter 7
Smooth convex functions
This chapter is devoted to the study of smooth convex functions, i.e. convex
functions that are diï¬€erentiable. A prerequisite for diï¬€erentiability at a point
is that the function is deï¬ned and ï¬nite in a neighborhood of the point.
Hence, it is only meaningful to study diï¬€erentiability properties at interior
points of the domain of the function, and by passing to the restriction of
the function to the interior of its domain, we may as well assume from the
beginning that the domain of deï¬nition is open.
That is the reason for
assuming all domains to be open and all function values to be ï¬nite in this
chapter.
7.1
Convex functions on R
Let f be a real-valued function that is deï¬ned in a neighborhood of the point
x âˆˆR. The one-sided limit
f â€²
+(x) = lim
tâ†’0+
f(x + t) âˆ’f(x)
t
,
if it exists, is called the right derivative of f at the point x. The left derivative
f â€²
âˆ’(x) is similarly deï¬ned as the one-sided limit
f â€²
âˆ’(x) = lim
tâ†’0âˆ’
f(x + t) âˆ’f(x)
t
.
The function is obviously diï¬€erentiable at the point x if and only if the right
and the left derivatives both exist and are equal, and the derivative f â€²(x) is
in that case equal to their common value.
The left derivative of the function f : I â†’R can be expressed as a right
derivative of the function Ë‡f, deï¬ned by
Ë‡f(x) = f(âˆ’x)
for all x âˆˆâˆ’I,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
145
Smooth convex functions
because
f â€²
âˆ’(x) = lim
tâ†’0+
f(x âˆ’t) âˆ’f(x)
âˆ’t
= âˆ’lim
tâ†’0+
Ë‡f(âˆ’x + t) âˆ’Ë‡f(âˆ’x)
t
and hence
f â€²
âˆ’(x) = âˆ’Ë‡f â€²
+(âˆ’x).
Observe that the function Ë‡f is convex if f is convex.
The basic diï¬€erentiability properties of convex functions are consequences
of the following lemma, which has an obvious interpretation in terms of slopes
of various chords. Cf. ï¬gure 7.1.
Lemma 7.1.1. Suppose f is a real-valued convex function that is deï¬ned on
a subinterval of R containing the points x1 < x2 < x3. Then
f(x2) âˆ’f(x1)
x2 âˆ’x1
â‰¤f(x3) âˆ’f(x1)
x3 âˆ’x1
â‰¤f(x3) âˆ’f(x2)
x3 âˆ’x2
.
The above inequalities are strict if f is strictly convex.
x1
x2
x3
A
B
C
Figure 7.1.
A geometric interpretation of Lemma 7.1.1: If kPQ
denotes the slope of the chord PQ, then kAB â‰¤kAC â‰¤kBC.
Proof. Write x2 = Î»x3 + (1 âˆ’Î»)x1; then Î» = x2 âˆ’x1
x3 âˆ’x1
is a number in the
interval ]0, 1[. By convexity,
f(x2) â‰¤Î»f(x3) + (1 âˆ’Î»)f(x1),
which simpliï¬es to f(x2) âˆ’f(x1) â‰¤Î»(f(x3) âˆ’f(x1)), and this is equivalent
to the leftmost of the two inequalities in the lemma.
The rightmost inequality is obtained by applying the already proven in-
equality to the convex function Ë‡f. Since âˆ’x3 < âˆ’x2 < âˆ’x1,
f(x2) âˆ’f(x3)
x3 âˆ’x2
=
Ë‡f(âˆ’x2) âˆ’Ë‡f(âˆ’x3)
âˆ’x2 âˆ’(âˆ’x3)
â‰¤
Ë‡f(âˆ’x1) âˆ’Ë‡f(âˆ’x3)
âˆ’x1 âˆ’(âˆ’x3)
= f(x1) âˆ’f(x3)
x3 âˆ’x1
,
and multiplication by âˆ’1 gives the desired result.
The above inequalities are strict if f is strictly convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
146
Smooth convex functions
146
The diï¬€erentiability properties of convex one-variable functions are given
by the following theorem.
Theorem 7.1.2. Suppose f : I â†’R is a convex function with an open subin-
terval I of R as its domain. Then:
(a) The function f has right and left derivatives at all points x âˆˆI, and
f â€²
âˆ’(x) â‰¤f â€²
+(x).
(b) If f â€²
âˆ’(x) â‰¤a â‰¤f â€²
+(x), then
f(y) â‰¥f(x) + a(y âˆ’x)
for all y âˆˆI.
The above inequality is strict for y Ì¸= x, if f is strictly convex.
(c) If x < y, then f â€²
+(x) â‰¤f â€²
âˆ’(y), and the inequality is strict if f is strictly
convex.
(d) The functions f â€²
+ : I â†’R and f â€²
âˆ’: I â†’R are increasing, and they are
strictly increasing if f is strictly convex.
(e) The set of points x âˆˆI where the function is not diï¬€erentiable, is ï¬nite
or countable.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
147
Smooth convex functions
Proof. Fix x âˆˆI and let
F(t) = f(x + t) âˆ’f(x)
t
.
The domain of F is an open interval Jx with the point 0 removed.
We start by observing that if s, t, u âˆˆJx and u < 0 < t < s, then
(7.1)
F(u) â‰¤F(t) â‰¤F(s)
(and the inequalities are strict if f is strictly convex).
The right inequality F(t) â‰¤F(s) follows directly from the left inequality
in Lemma 7.1.1 by choosing x1 = x, x2 = x + t and x3 = x + s, and the left
inequality F(u) â‰¤F(t) follows from the inequality between the extreme ends
in the same lemma by instead choosing x1 = x + u, x2 = x and x3 = x + t.
It follows from inequality (7.1) that the function F(t) is increasing for
t > 0 (strictly increasing if f is strictly convex) and bounded below by
F(u0), where u0 is an arbitrary negative number in the domain of F. Hence,
the limit
f â€²
+(x) = lim
tâ†’0+ F(t)
exists and
F(t) â‰¥f â€²
+(x)
for all t > 0 in the domain of F (with strict inequality if f is strictly convex).
By replacing t with y âˆ’x, we obtain the following implication for a â‰¤f â€²
+(x):
(7.2)
y > x â‡’f(y) âˆ’f(x) â‰¥f â€²
+(x)(y âˆ’x) â‰¥a(y âˆ’x)
(with strict inequality if f is strictly convex).
The same argument, applied to the function Ë‡f and the point âˆ’x, shows
that Ë‡f â€²
+(âˆ’x) exists, and that
âˆ’y > âˆ’x â‡’Ë‡f(âˆ’y) âˆ’Ë‡f(âˆ’x) â‰¥âˆ’a(âˆ’y âˆ’(âˆ’x))
if âˆ’a â‰¤Ë‡f â€²
+(âˆ’x). Since f â€²
âˆ’(x) = âˆ’Ë‡f â€²
+(âˆ’x), this means that the left derivative
f â€²
âˆ’(x) exists and that the implication
(7.3)
y < x â‡’f(y) âˆ’f(x) â‰¥a(y âˆ’x)
is true for all constants a satisfying a â‰¥f â€²
âˆ’(x). The implications (7.2) and
(7.3) are both satisï¬ed if f â€²
âˆ’(x) â‰¤a â‰¤f â€²
+(x), and this proves assertion (b).
Using inequality (7.1) we conclude that F(âˆ’t) â‰¤F(t) for all suï¬ƒciently
small values of t. Hence
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
148
Smooth convex functions
f â€²
âˆ’(x) = lim
tâ†’0+ F(âˆ’t) â‰¤lim
tâ†’0+ F(t) = f â€²
+(x),
and this proves assertion (a).
As a special case of assertion (b), we have the two inequalities
f(y) âˆ’f(x) â‰¥f â€²
+(x)(y âˆ’x)
and
f(x) âˆ’f(y) â‰¥f â€²
âˆ’(y)(x âˆ’y),
and division by y âˆ’x now results in the implication
y > x â‡’f â€²
+(x) â‰¤f(y) âˆ’f(x)
y âˆ’x
â‰¤f â€²
âˆ’(y).
(If f is strictly convex, we may replace â‰¤with < at both places.) This proves
assertion (c).
By combining (c) with the inequality in (a) we obtain the implication
x < y â‡’f â€²
+(x) â‰¤f â€²
âˆ’(y) â‰¤f â€²
+(y),
which shows that the right derivative f â€²
+ is increasing. That the left derivative
is increasing is proved in a similar way. (And the derivatives are strictly
increasing if f is strictly convex.)
To prove the ï¬nal assertion (e) we deï¬ne Ix to be the open interval
]f â€²
âˆ’(x), f â€²
+(x)[. This interval is empty if the derivative f â€²(x) exists, and it is
nonempty if the derivative does not exist, and intervals Ix and Iy belonging
to diï¬€erent points x and y are disjoint because of assertion (c). Now choose,
for each point x where the derivative does not exist, a rational number rx
in the interval Ix. Since diï¬€erent intervals are pairwise disjoint, the chosen
numbers will be diï¬€erent, and since the set of rational numbers is countable,
there are at most countably many points x at which the derivative does not
exist.
Deï¬nition. The line y = f(x0) + a(x âˆ’x0) is called a supporting line of the
function f : I â†’R at the point x0 âˆˆI if
(7.4)
f(x) â‰¥f(x0) + a(x âˆ’x0)
for all x âˆˆI.
A supporting line at the point x0 is a line which passes through the point
(x0, f(x0)) and has the entire function curve y = f(x) above (or on) itself. It
is, in other words, a (one-dimensional) supporting hyperplane of the epigraph
of f at the point (x0, f(x0)). The concept will be generalized for functions
of several variables in the next chapter.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
149
Smooth convex functions
149
x0
x
y
y = f(x0) + a(x âˆ’x0)
y = f(x)
Figure 7.2.
A supporting line.
Assertion (b) of the preceding theorem shows that convex functions with
open domains have supporting lines at each point, and that the tangent is a
supporting line at points where the derivative exists. By our next theorem,
the existence of supporting lines is also a suï¬ƒcient condition for convexity.
Theorem 7.1.3. Suppose that the function f : I â†’R, where I is an open
interval, has a supporting line at each point in I. Then, f is a convex func-
tion.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
150
Smooth convex functions
Proof. Suppose that x, y âˆˆI and that 0 < Î» < 1, and let a be the constant
belonging to the point x0 = Î»x+(1âˆ’Î»)y in the deï¬nition (7.4) of a supporting
line. Then we have f(x) â‰¥f(x0) + a(x âˆ’x0) and f(y) â‰¥f(x0) + a(y âˆ’x0).
By multiplying the ï¬rst inequality by Î» and the second inequality by (1âˆ’Î»),
and then adding the two resulting inequalities, we obtain
Î»f(x) + (1 âˆ’Î»)f(y) â‰¥f(x0) + a

Î»x + (1 âˆ’Î»)y âˆ’x0

= f(x0).
So the function f is convex.
Observe that if the inequality (7.4) is strict for all x Ì¸= x0 and for all
x0 âˆˆI, then f is strictly convex.
For diï¬€erentiable functions we now obtain the following necessary and
suï¬ƒcient condition for convexity.
Theorem 7.1.4. A diï¬€erentiable function f : I â†’R is convex if and only
if its derivative f â€² is increasing. And it is strictly convex if and only if the
derivative is strictly increasing.
Proof. Assertion (d) in Theorem 7.1.2 shows that the derivative of a (strictly)
convex function is (strictly) increasing.
To prove the converse, we assume that the derivative f â€² is increasing. By
the mean value theorem, if x and x0 are distinct points in I, there exists a
point Î¾ between x and x0 such that
f(x) âˆ’f(x0)
x âˆ’x0
= f â€²(Î¾)

â‰¥f â€²(x0)
if x > x0,
â‰¤f â€²(x0)
if x < x0.
Multiplication by x âˆ’x0 results, in both cases, in the inequality
f(x) âˆ’f(x0) â‰¥f â€²(x0)(x âˆ’x0),
which shows that y = f(x0) + f â€²(x0)(x âˆ’x0) is a supporting line of the
function f at the point x0. Therefore, f is convex by Theorem 7.1.3.
The above inequalites are strict if the derivative is strictly increasing, and
we conclude that f is strictly convex in that case.
For two times diï¬€erentiable functions we obtain the following corollary.
Corollary
7.1.5. A two times diï¬€erentiable function f : I â†’R is convex
if and only if f â€²â€²(x) â‰¥0 for all x âˆˆI. The function is strictly convex if
f â€²â€²(x) > 0 for all x âˆˆI.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
151
Smooth convex functions
Proof. The derivative f â€² is increasing (strictly increasing) if the second deri-
vative f â€²â€² is nonnegative (positive). And the second derivative is nonnegative
if the derivative is increasing.
Remark. A continuous function f : J â†’R with a non-open interval J as
domain is convex if (and only if) the restriction of f to the interior of J is
convex. Hence, if the derivative exists and is increasing in the interior of J,
or if the second derivative exists and f â€²â€²(x) â‰¥0 for all interior points x of
the interval, then f is convex on J. Cf. exercise 7.7.
Example 7.1.1. The functions x â†’ex, x â†’âˆ’ln x and x â†’xp, where
p > 1, are strictly convex on their domains R, ]0, âˆ[ and [0, âˆ[, respectively,
because their ï¬rst derivatives are strictly increasing functions.
7.2
Diï¬€erentiable convex functions
A diï¬€erentiable one-variable function f is convex if and only if its derivative
is an increasing function. In order to generalize this result to functions of
several variables it is necessary to express the condition that the derivative is
increasing in a generalizable way. To this end, we note that the derivative f â€²
is increasing on an interval if and only if f â€²(x + h)h â‰¥f â€²(x)h for all numbers
x and x+h in the interval, and this inequality is also meaningful for functions
f of several variables if we interpret f â€²(x)h as the value of the linear form
Df(x) at h. The inequality generalizing that the derivative of a function of
several variables is increasing will thus be written Df(x + h)[h] â‰¥Df(x)[h],
or using gradient notation, âŸ¨f â€²(x + h), hâŸ©â‰¥âŸ¨f â€²(x), hâŸ©.
Theorem 7.2.1. Let X be an open convex subset of Rn, and let f : X â†’R
be a diï¬€erentiable function. The following three conditions are equivalent:
(i) f is a convex function.
(ii) f(x + v) â‰¥f(x) + Df(x)[v] for all x, x + v âˆˆX.
(iii) Df(x + v)[v] â‰¥Df(x)[v] for all x, x + v âˆˆX.
The function f is strictly convex if and only if the inequalities in (ii) and
(iii) can be replaced by strict inequalities when v Ì¸= 0.
Proof. Let us for given points x and x + v in X consider the restriction Ï†x,v
of f to the line through x with direction v, i.e. the one-variable function
Ï†x,v(t) = f(x + tv)
with the open interval Ix,v = {t âˆˆR | x + tv âˆˆX} as domain. The functions
Ï†x,v are diï¬€erentiable with derivative Ï†â€²
x,v(t) = Df(x+tv)[v], and f is convex
if and only if all restrictions Ï†x,v are convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
152
Smooth convex functions
152
(i) â‡’(ii)
So if f is convex, then Ï†x,v is a convex function, and it follows
from Theorem 7.1.2 (b) that Ï†x,v(t) â‰¥Ï†x,v(0)+Ï†â€²
x,v(0)t for all t âˆˆIx,v, which
means that f(x + tv) â‰¥f(x) + Df(x)[v] t for all t such that x + tv âˆˆX. We
now obtain the inequality in (ii) by choosing t = 1.
(ii) â‡’(iii)
We obtain inequality (iii) by adding the two inequalities
f(x + v) â‰¥f(x) + Df(x)[v]
and
f(x) â‰¥f(x + v) + Df(x + v)[âˆ’v].
(iii) â‡’(i) Suppose (iii) holds, and let y = x+sv and w = (tâˆ’s)v. If t > s,
then
Ï†â€²
x,v(t) âˆ’Ï†â€²
x,v(s) = Df(x + tv)[v] âˆ’Df(x + sv)[v]
= Df(y + w)[v] âˆ’Df(y)[v]
=
1
t âˆ’s

Df(y + w)[w] âˆ’Df(y)[w]

â‰¥0,
which means that the derivative Ï†â€²
x,v is increasing. The functions Ï†x,v are
thus convex.
This proves the equivalence of assertions (i), (ii) and (iii), and by replacing
all inequalities in the proof by strict inequalities, we obtain the corresponding
equivalent assertions for strictly convex functions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
153
Smooth convex functions
The derivative of a diï¬€erentiable function is equal to zero at a local min-
imum point. For convex functions, the converse is also true.
Theorem 7.2.2. Suppose f : X â†’R is a diï¬€erentiable convex function.
Then Ë†x âˆˆX is a global minimum point if and only if f â€²(Ë†x) = 0.
Proof. That the derivative equals zero at a minimum point is a general fact,
and the converse is a consequence of property (ii) in the previous theorem,
for if f â€²(Ë†x) = 0, then f(x) â‰¥f(Ë†x) + Df(Ë†x)[x âˆ’Ë†x] = f(Ë†x) for all x âˆˆX.
Convexity can also be expressed by a condition on the second derivative,
and the natural substitute for the one-variable condition f â€²â€²(x) â‰¥0 is that
the second derivative should be positive semideï¬nite.
Theorem 7.2.3. Let X be an open convex subset of Rn, and suppose that
the function f : X â†’R is two times diï¬€erentiable. Then f is convex if and
only if the second derivative f â€²â€²(x) is positive semideï¬nite for all x âˆˆX.
If f â€²â€²(x) is positive deï¬nite for all x âˆˆX, then f is strictly convex.
Proof. The one-variable functions Ï†x,v(t) = f(x + tv) are now two times
diï¬€erentiable with second derivative
Ï†â€²â€²
x,v(t) = D2f(x + tv)[v, v] = âŸ¨v, f â€²â€²(x + tv)vâŸ©.
Since f is convex if and only if all functions Ï†x,v are convex, f is convex if
and only if all second derivatives Ï†â€²â€²
x,v are nonnegative functions .
If the second derivative f â€²â€²(x) is positive semideï¬nite for all x âˆˆX, then
Ï†â€²â€²
x,v(t) = âŸ¨v, f â€²â€²(x+tv)vâŸ©â‰¥0 for all x âˆˆX and all v âˆˆRn, which means that
the second derivatives Ï†â€²â€²
x,v are nonnegative funtions. Conversely, if the second
derivatives Ï†â€²â€²
x,v are nonnegative, then in particular âŸ¨v, f â€²â€²(x)âŸ©= Ï†â€²â€²
x,v(0) â‰¥0
for all x âˆˆX and all v âˆˆRn, and we conclude that the second derivative
f â€²â€²(x) is positive semideï¬nite for all x âˆˆX.
If the second derivatives f â€²â€²(x) are all positive deï¬nite, then Ï†â€²â€²
x,v(t) > 0
for v Ì¸= 0, which implies that the functions Ï†x,v are strictly convex, and then
f is strictly convex, too.
7.3
Strong convexity
The function surface of a convex functions bends upwards, but there is no
lower positive bound on the curvature. By introducing such a bound we
obtain the notion of strong convexity.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
154
Smooth convex functions
Deï¬nition. Let Âµ be a positive number. A function f : X â†’R is called Âµ-
strongly convex if the function f(x)âˆ’1
2Âµâˆ¥xâˆ¥2 is convex, and the function f is
called strongly convex if it is Âµ-strongly convex for some positive number Âµ.
Theorem 7.3.1. A diï¬€erentiable function f : X â†’R with a convex domain
is Âµ-strongly convex if and only if the following two mutually equivalent in-
equalities are satisï¬ed for all x, x + v âˆˆX:
Df(x + v)[v] â‰¥Df(x)[v] + Âµâˆ¥vâˆ¥2
(i)
f(x + v) â‰¥f(x) + Df(x)[v] + 1
2Âµâˆ¥vâˆ¥2.
(ii)
Proof. Let g(x) = f(x) âˆ’1
2Âµâˆ¥xâˆ¥2 and note that gâ€²(x) = f â€²(x) âˆ’Âµx and that
consequently Df(x)[v] = Dg(x)[v] + ÂµâŸ¨x, vâŸ©.
If f is Âµ-strongly convex, then g is a convex function, and so it follows
from Theorem 7.2.1 that
Df(x + v)[v] âˆ’Df(x)[v] = Dg(x + v)[v] âˆ’Dg(x)[v] + ÂµâŸ¨x + v, vâŸ©âˆ’ÂµâŸ¨x, vâŸ©
â‰¥ÂµâŸ¨v, vâŸ©= Âµâˆ¥vâˆ¥2,
i.e. inequality (i) is satisï¬ed.
(i) â‡’(ii):
Assume (i) holds, and deï¬ne the function Î¦ for 0 â‰¤t â‰¤1 by
Î¦(t) = f(x + tv) âˆ’f(x) âˆ’Df(x)[v] t.
Then Î¦â€²(t) = Df(x + tv)[v] âˆ’Df(x)[v] = 1
t

Df(x + tv)[tv] âˆ’Df(x)[tv]

,
and it now follows from inequality (i) that
Î¦â€²(t) â‰¥tâˆ’1Âµâˆ¥tvâˆ¥2 = Âµâˆ¥vâˆ¥2 t.
By integrating the last inequality over the interval [0, 1] we obtain
Î¦(1) = Î¦(1) âˆ’Î¦(0) â‰¥1
2Âµâˆ¥vâˆ¥2,
which is the same as inequality (ii).
If inequality (ii) holds, then
g(x + v) = f(x + v) âˆ’1
2Âµâˆ¥x + vâˆ¥2 â‰¥f(x) + Df(x)[v] + 1
2Âµâˆ¥vâˆ¥2 âˆ’1
2Âµâˆ¥x + vâˆ¥2
= g(x) + 1
2Âµâˆ¥xâˆ¥2 + Dg(x)[v] + ÂµâŸ¨x, vâŸ©+ 1
2Âµâˆ¥vâˆ¥2 âˆ’1
2Âµâˆ¥x + vâˆ¥2
= g(x) + Dg(x)[v].
The function g is thus convex, by Theorem 7.2.1, and f(x) = g(x) + 1
2Âµâˆ¥xâˆ¥2
is consequently Âµ-strongly convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
155
Smooth convex functions
155
Theorem 7.3.2. A twice diï¬€erentiable function f : X â†’R with a convex
domain is Âµ-strongly convex if and only if
(7.5)
âŸ¨v, f â€²â€²(x)vâŸ©= D2f(x)[v, v] â‰¥Âµâˆ¥vâˆ¥2
for all x âˆˆX and all v âˆˆRn.
Remark. If A is a symmetric operator, then
min
vÌ¸=0
âŸ¨v, AvâŸ©
âˆ¥vâˆ¥2
= Î»min,
where Î»min is the smallest eigenvalue of the operator. Thus, a two times
diï¬€erentiable function f with a convex domain is Âµ-strongly convex if and
only if the eigenvalues of the hessian f â€²â€²(x) are greater than or equal to Âµ for
each x in the domain.
Proof. Let Ï†x,v(t) = f(x + tv). If condition (7.5) holds, then
Ï†â€²â€²
x,v(t) = D2f(x + tv)[v, v] â‰¥Âµâˆ¥vâˆ¥2
for all t in the domain of the function. Using Taylorâ€™s formula with remainder
term, we therefore conclude that
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
156
Smooth convex functions
Ï†x,v(t) = Ï†x,v(0) + Ï†â€²
x,v(0)t + 1
2Ï†â€²â€²
x,v(Î¾)t2 â‰¥Ï†x,v(0) + Ï†â€²
x,v(0)t + 1
2Âµâˆ¥vâˆ¥2 t2.
For t = 1 this amounts to inequality (ii) in Theorem 7.3.1, and hence f is a
Âµ-strongly convex function.
Conversely, if f is Âµ-strongly convex, then by Theorem 7.3.1 (i)
Ï†â€²
x,v(t) âˆ’Ï†â€²
x,v(0)
t
= Df(x + tv)[tv] âˆ’Df(x)[tv]
t2
â‰¥Âµâˆ¥vâˆ¥2.
Taking the limit as t â†’0 we obtain
D2f(x)[v, v] = Ï†â€²â€²
x,v(0) â‰¥Âµâˆ¥vâˆ¥2.
7.4
Convex functions with Lipschitz continu-
ous derivatives
The rate of convergence of classical iterative algorithms for minimizing func-
tions depends on the variation of the deriviative âˆ’the more the derivative
varies in a neighborhood of the minimum point, the slower the convergence.
The size of the Lipschitz constant is a measure of the variation of the deriva-
tive for functions with a Lipschitz continuous derivative. Therefore, we start
with a result which for arbitrary functions connects Lipschitz continuity of
the ï¬rst derivative to bounds on the second derivative.
Theorem 7.4.1. Suppose f is a twice diï¬€erentiable function and that X is a
convex subset of its domain.
(i) If âˆ¥f â€²â€²(x)âˆ¥â‰¤L for all x âˆˆX, then the derivative f â€² is Lipschitz con-
tinuous on X with Lipschitz constant L.
(ii) If the derivative f â€² is Lipschitz continuous on the set X with constant
L, then âˆ¥f â€²â€²(x)âˆ¥â‰¤L for all x âˆˆint X.
Proof. (i) Suppose that âˆ¥f â€²â€²(x)âˆ¥â‰¤L for all x âˆˆX, and let x and y be two
points in X. Put v = y âˆ’x, let w be an arbitrary vector with âˆ¥wâˆ¥= 1, and
deï¬ne the function Ï† for 0 â‰¤t â‰¤1 by
Ï†(t) = Df(x + tv)[w] = âŸ¨f â€²(x + tv), wâŸ©.
Then Ï† is diï¬€erentiable with derivative
Ï†â€²(t) = D2f(x + tv)[w, v] = âŸ¨w, f â€²â€²(x + tv)vâŸ©
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
157
Smooth convex functions
so it follows from the Cauchy-Schwarz inequality that
|Ï†â€²(t)| â‰¤âˆ¥wâˆ¥âˆ¥f â€²â€²(x + tv)vâˆ¥â‰¤âˆ¥f â€²â€²(x + tv)âˆ¥âˆ¥vâˆ¥â‰¤Lâˆ¥vâˆ¥,
since x + tv is a point in X. By the mean value theorem, Ï†(1) âˆ’Ï†(0) = Ï†â€²(s)
for some point s âˆˆ]0, 1[. Consequently,
|âŸ¨f â€²(y) âˆ’f â€²(x), wâŸ©| = |Ï†(1) âˆ’Ï†(0)| = |Ï†â€²(s)| â‰¤Lâˆ¥y âˆ’xâˆ¥.
Since w is an arbitrary vector of norm 1, we conclude that
âˆ¥f â€²(y) âˆ’f â€²(x)âˆ¥= sup
âˆ¥wâˆ¥=1
âŸ¨f â€²(y) âˆ’f â€²(x), wâŸ©â‰¤Lâˆ¥y âˆ’xâˆ¥,
i.e. the derivative f â€² is Lipschitz continuous on X with constant L.
(ii) Assume conversely that the ï¬rst derivative f â€² is Lipschitz continuous on
the set X with constant L. Let x be a point in the interior of X, and let v
and w be arbitrary vectors with norm 1. The function
Ï†(t) = Df(x + tv)[w] = âŸ¨f â€²(x + tv, wâŸ©
is then deï¬ned and diï¬€erentiable and the point x + tv lies in X for all t in a
neighborhood of 0, and it follows that
|Ï†(t) âˆ’Ï†(0)| = |âŸ¨f â€²(x + tv) âˆ’f â€²(x), wâŸ©| â‰¤âˆ¥f â€²(x + tv) âˆ’f â€²(x)âˆ¥âˆ¥wâˆ¥
â‰¤Lâˆ¥tvâˆ¥= L|t|.
Division by t and passing to the limit as t â†’0 results in the inequality
|âŸ¨w, f â€²â€²(x)vâŸ©| = |Ï†â€²(0)| â‰¤L
with the conclustion that
âˆ¥f â€²â€²(x)âˆ¥= sup
âˆ¥vâˆ¥=1
âˆ¥f â€²â€²(x)vâˆ¥=
sup
âˆ¥vâˆ¥,âˆ¥wâˆ¥=1
âŸ¨w, f â€²â€²(x)vâŸ©â‰¤L.
Deï¬nition. A diï¬€erentiable function f : X â†’R belongs to the class SÂµ,L(X)
if f is Âµ-strongly convex and the derivative f â€² is Lipschitz continuous with
constant L. The quotient Q = L/Âµ is called the condition number of the
class.
Due to Theorem 7.3.1, a diï¬€erentiable function f with a convex domain
X belongs to the class SÂµ,L(X) if and only if it satisï¬es the following two
inequalities for all x, x + v âˆˆX:
âŸ¨f â€²(x + v) âˆ’f â€²(x), vâŸ©â‰¥Âµâˆ¥vâˆ¥2
and
âˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥â‰¤Lâˆ¥vâˆ¥.
If we combine the ï¬rst of these two inequalities with the Cauchyâ€“Schwarz
inequality, we obtain the inequality Âµâˆ¥vâˆ¥â‰¤âˆ¥f â€²(x+v)âˆ’f â€²(x)âˆ¥, so we conclude
that Âµ â‰¤L and Q â‰¥1.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
158
Smooth convex functions
158
Example 7.4.1. Strictly convex quadratic functions
f(x) = 1
2âŸ¨x, PxâŸ©+ âŸ¨q, xâŸ©+ r
belong to the class SÎ»min,Î»max(Rn), where Î»min and Î»max denote the smallest
and the largest eigenvalue, respectively, of the positive deï¬nite matrix P.
For f â€²(x) = Px + q and f â€²â€²(x) = P, whence
D2f(x)[v, v] = âŸ¨v, PvâŸ©â‰¥Î»minâˆ¥vâˆ¥2
and
âˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥= âˆ¥Pvâˆ¥â‰¤âˆ¥Pâˆ¥âˆ¥vâˆ¥= Î»maxâˆ¥vâˆ¥.
The condition number of the quadratic function f is thus equal to the
quotient Î»max/Î»min between the largest and the smallest eigenvalue.
The sublevel sets {x | f(x) â‰¤Î±} of a strictly convex quadratic function
f are ellipsoids for all values of Î± greater than the minimum value of the
function, and the ratio of the longest and the shortest axes of any of these
ellipsoids is equal to

Î»max/Î»min, i.e. to the square root of the condition
number Q. This ratio is obviously also equal to the ratio of the radii of the
smallest ball containing and the largest ball contained in the ellipsoid. As
we shall see, something similar applies to all functions in the class SÂµ,L(Rn).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
159
Smooth convex functions
Theorem 7.4.2. Let f be a function in the class SÂµ,L(Rn) with minimum
point Ë†x, and let Î± be a number greater than the minimum value f(Ë†x). Then
B(Ë†x; r) âŠ†{x âˆˆX | f(x) â‰¤Î±} âŠ†B(Ë†x; R),
where r =

2Lâˆ’1(Î± âˆ’f(Ë†x)) and R =

2Âµâˆ’1(Î± âˆ’f(Ë†x)).
Remark. Note that R/r =

L/Âµ = âˆšQ.
Proof. Since f â€²(Ë†x) = 0 we obtain the following inequalities from Theorems
1.1.2 and 7.3.1 (by replacing a and x respectively with Ë†x and v with x âˆ’Ë†v):
f(Ë†x) + 1
2Âµâˆ¥x âˆ’Ë†xâˆ¥2 â‰¤f(x) â‰¤f(Ë†x) + 1
2Lâˆ¥x âˆ’Ë†xâˆ¥2.
Hence, x âˆˆS = {x âˆˆX | f(x) â‰¤Î±} implies
1
2Âµâˆ¥x âˆ’Ë†xâˆ¥2 â‰¤f(x) âˆ’f(Ë†x) â‰¤Î± âˆ’f(Ë†x) = 1
2ÂµR2,
which means that âˆ¥x âˆ’Ë†xâˆ¥â‰¤R and proves the inclusion S âŠ†B(Ë†x; R).
And if x âˆˆB(Ë†x; r), then f(x) â‰¤f(Ë†x) + 1
2Lr2 = Î±, which means that
x âˆˆS and proves the inclusion B(Ë†x; r) âŠ†S.
Convex functions on Rn with Lipschitz continuous derivatives are char-
acterized by the following theorem.
Theorem 7.4.3. A diï¬€erentiable function f : Rn â†’R is convex and its
derivative is Lipschitz continuous with Lipschitz constant L if and only if the
following mutually equivalent inequalities are fulï¬lled for all x, v âˆˆRn:
f(x) + Df(x)[v] â‰¤f(x + v) â‰¤f(x) + Df(x)[v] + L
2 âˆ¥vâˆ¥2
(i)
f(x + v) â‰¥f(x) + Df(x)[v] + 1
2Lâˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥2
(ii)
Df(x + v)[v] â‰¥Df(x)[v] + 1
Lâˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥2
(iii)
Proof. That inequality (i) has to be satisï¬ed for convex functions with a
Lipschitz continuous derivative is a consequence of Theorems 1.1.2 and 7.2.1.
(i) â‡’(ii):
Let w = f â€²(x + v) âˆ’f â€²(x), and apply the right inequality in
(i) with x replaced by x + v and v replaced by âˆ’Lâˆ’1w; this results in the
inequality
f(x + v âˆ’Lâˆ’1w) â‰¤f(x + v) âˆ’Lâˆ’1Df(x + v)[w] + 1
2Lâˆ’1âˆ¥wâˆ¥2.
The left inequality in (i) with v âˆ’Lâˆ’1w instead of v yields
f(x + v âˆ’Lâˆ’1w) â‰¥f(x) + Df(x)[v âˆ’Lâˆ’1w].
By combining these two new inequalities, we obtain
f(x + v) â‰¥f(x) + Df(x)[v âˆ’Lâˆ’1w] + Lâˆ’1Df(x + v)[w] âˆ’1
2Lâˆ’1âˆ¥wâˆ¥2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
160
Smooth convex functions
= f(x) + Df(x)[v] + Lâˆ’1
Df(x + v)[w] âˆ’Df(x)[w]

âˆ’1
2Lâˆ’1âˆ¥wâˆ¥2
= f(x) + Df(x)[v] + Lâˆ’1âŸ¨f â€²(x + v) âˆ’f â€²(x), wâŸ©âˆ’1
2Lâˆ’1âˆ¥wâˆ¥2
= f(x) + Df(x)[v] + Lâˆ’1âŸ¨w, wâŸ©âˆ’1
2Lâˆ’1âˆ¥wâˆ¥2
= f(x) + Df(x)[v] + 1
2Lâˆ’1âˆ¥wâˆ¥2,
and that is inequality (ii).
(ii) â‡’(iii):
Add inequality (ii) to the inequality obtained by changing x to
x + v and v to âˆ’v. The result is inequality (iii).
Let us ï¬nally assume that inequality (iii) holds. The convexity of f is then
a consequence of Theorem 7.2.1, and by combining (iii) with the Cauchyâ€“
Schwarz inequality, we obtain the inequality
1
Lâˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥2 â‰¤Df(x + v)[v] âˆ’Df(x)[v] = âŸ¨f â€²(x + v) âˆ’f â€²(x), vâŸ©
â‰¤âˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥Â· âˆ¥vâˆ¥,
which after division by âˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥gives us the desired conclusion:
the derivative is Lipschitz continuous with Lipschitz constant L.
Theorem 7.4.4. If f âˆˆSÂµ,L(Rn), then
Df(x + v)[v] â‰¥Df(x)[v] +
ÂµL
Âµ + Lâˆ¥vâˆ¥2 +
1
Âµ + Lâˆ¥f â€²(x + v) âˆ’f â€²(x)âˆ¥2
for all x, v âˆˆRn.
Proof. Let g(x) = f(x) âˆ’1
2Âµâˆ¥xâˆ¥2; the function g is then convex, and since
Dg(x)[v] = Df(x)[v] âˆ’ÂµâŸ¨x, vâŸ©, it follows from Theorem 1.1.2 that
g(x + v) = f(x + v) âˆ’1
2Âµâˆ¥x + vâˆ¥2
â‰¤f(x) + Df(x)[v] + 1
2Lâˆ¥vâˆ¥2 âˆ’1
2Âµâˆ¥x + vâˆ¥2
= g(x) + 1
2Âµâˆ¥xâˆ¥2 + Dg(x)[v] + ÂµâŸ¨x, vâŸ©+ 1
2Lâˆ¥vâˆ¥2 âˆ’1
2Âµâˆ¥x + vâˆ¥2
= g(x) + Dg(x)[v] + 1
2(L âˆ’Âµ)âˆ¥vâˆ¥2.
This shows that g satisï¬es condition (i) in Theorem 7.4.3 with L replaced by
L âˆ’Âµ. The derivative gâ€² is consequently Lipschitz continuous with constant
L âˆ’Âµ. The same theorem now gives us the inequality
Dg(x + v)[v] â‰¥Dg(x)[v] +
1
L âˆ’Âµâˆ¥gâ€²(x + v) âˆ’gâ€²(x)âˆ¥2,
which is just a reformulation of the inequality in Theorem 7.4.4.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
161
Smooth convex functions
Exercises
7.1 Show that the following functions are convex.
a) f(x1, x2) = ex1 + ex2 + x1x2,
x1 + x2 > 0
b) f(x1, x2) = sin(x1 + x2),
âˆ’Ï€ < x1 + x2 < 0
c) f(x1, x2) = âˆ’

cos(x1 + x2),
âˆ’Ï€
2 < x1 + x2 < Ï€
2 .
7.2 Is the function f(x1, x2) = x2
1/x2+x2
2/x1 convex in the ï¬rst quadrant x1 > 0,
x2 > 0?
7.3 Show that the function f(x) = nâˆ’1
j=1 x2
j/xn is convex in the halfspace xn > 0.
7.4 Show that the following function is concave on the set [0, 1[Ã—[0, 1[Ã—[0, 1[:
f(x1, x2, x3) = ln(1 âˆ’x1) + ln(1 âˆ’x2) + ln(1 âˆ’x3)
âˆ’(x2
1 + x2
2 + x2
3 + x1x2 + x1x3 + x2x3).
7.5 Let I be an interval and suppose that the function f : I â†’R is convex. Show
that f is either increasing on the interval, or decreasing on the interval, or
there exists a point c âˆˆI such that f is decreasing to the left of c and
increasing to the right of c.
7.6 Suppose f : ]a, b[ â†’R is a convex function.
a) Prove that the two one-sided limits limxâ†’a+ f(x) and limxâ†’bâˆ’f(x) exist
(as ï¬nite numbers or Â±âˆ).
b) Suppose that the interval is ï¬nite, and extend the function to the closed
interval [a, b] by deï¬ning f(a) = Î± and f(b) = Î². Prove that the extended
function is convex if and only if Î± â‰¥limxâ†’a+ f(x) and Î² â‰¥limxâ†’bâˆ’f(x).
7.7 Prove that a continuous function f : [a, b] â†’R is convex if and only if its
restriction to the open interval ]a, b[ is convex.
7.8 F is a family of diï¬€erentiable functions on Rn with the following two prop-
erties:
(i) f âˆˆF â‡’f + g âˆˆF for all aï¬ƒne functions g: Rn â†’R.
(ii) If f âˆˆF and fâ€²(x0) = 0, then x0 is a minimum point of f.
Prove that all functions in F are convex.
7.9 Suppose that f : X â†’R is a twice diï¬€erentiable convex function. Prove
that its recessive subspace Vf is a subset of N(fâ€²â€²(x)) for each x âˆˆX.
7.10 Let f : X â†’R be a diï¬€erentiable function with a convex domain X. Prove
that f is quasiconvex if and only if
f(x + v) â‰¤f(x) â‡’Df(x)[v] â‰¤0
for all x, x + v âˆˆX.
[Hint: It suï¬ƒces to prove the assertion for functions on R; the general result
then follows by taking restrictions to lines.]
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
162
Smooth convex functions
162
7.11 Let f : X â†’R be a twice diï¬€erentiable function with a convex domain X.
Prove the following assertions:
a) If f is quasiconvex, then
Df(x)[v] = 0 â‡’D2f(x)[v, v] â‰¥0
for all x âˆˆX and all v âˆˆRn.
b) If
Df(x)[v] = 0 â‡’D2f(x)[v, v] > 0
for all x âˆˆX and all v Ì¸= 0, then f is quasiconvex.
[Hint: It is enough to prove the results for functions deï¬ned on R.]
7.12 Prove that the function Î±1f1 + Î±2f2 is (Î±1Âµ1 + Î±2Âµ2)-strongly convex if f1
is Âµ1-strongly convex, f2 is Âµ2-strongly convex and Î±1, Î±2 > 0.
7.13 Prove that if a diï¬€erentiable Âµ-strongly convex function f : X â†’R has a
minimum at the point Ë†x, then âˆ¥x âˆ’Ë†xâˆ¥â‰¤Âµâˆ’1âˆ¥fâ€²(x)âˆ¥for all x âˆˆX.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
163
The subdifferential
Chapter 8
The subdiï¬€erential
We will now generalize a number of results from the previous chapter to
convex functions that are not necessarily diï¬€erentiable everywhere. However,
real-valued convex functions with open domains can not be too irregular âˆ’
they are, as already noted, continuous, and they have direction derivatives.
8.1
The subdiï¬€erential
If f is a diï¬€erentiable function, then y = f(a) + âŸ¨f â€²(a), x âˆ’aâŸ©is the equation
of a hyperplane that is tangent to the surface y = f(x) at the point (a, f(a)).
And if f is also convex, then f(x) â‰¥f(a) + âŸ¨f â€²(a), x âˆ’aâŸ©for all x in the
domain of the function (Theorem 7.2.1), so the tangent plane lies below the
graph of the function and is a supporting hyperplane of the epigraph.
The epigraph of an arbitrary convex function is a convex set, by deï¬nition.
Hence, through each boundary point belonging to the epigraph of a convex
function there passes a supporting hyperplane. The supporting hyperplanes
of a convex one-variable function f, deï¬ned on an open interval, are given
by Theorem 7.1.2, which says that the line y = f(x0) + a(x âˆ’x0) supports
the epigraph at the point (x0, f(x0)) if (and only if) f â€²
âˆ’(x0) â‰¤a â‰¤f â€²
+(x0).
The existence of supporting hyperplanes characterizes convexity, and this
is a reason for a more detailed study of this concept.
Deï¬nition. Let f : X â†’R be a function deï¬ned on a subset X of Rn. A
vector c âˆˆRn is called a subgradient of f at the point a âˆˆX if the inequality
(8.1)
f(x) â‰¥f(a) + âŸ¨c, x âˆ’aâŸ©
holds for all x âˆˆX.
The set of all subgradients of f at a is called the subdiï¬€erential of f at a
and is denoted by âˆ‚f(a).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
164
The subdifferential
y = |x|
y = ax
y
x
Figure 8.1. The line y = ax is a supporting line of
the function f(x) = |x| at the origin if âˆ’1 â‰¤a â‰¤1.
Remark. The inequality (8.1) is of course satisï¬ed by all points a âˆˆX and
all vectors c âˆˆRn if x is a point in the set X \ dom f. Hence, to verify that
c is a subgradient of f at a it suï¬ƒces to verify that the inequality holds for
all x âˆˆdom f.
The inequality (8.1) does not hold for any vector c if a is a point in
X \dom f and x is a point in dom f. Hence, âˆ‚f(a) = âˆ…for all a âˆˆX \dom f,
except in the trivial case when dom f = âˆ…, i.e. when f is equal to âˆon
the entire set X. In this case we have âˆ‚f(a) = Rn for all a âˆˆX since the
inequality (8.1) is now trivially satisï¬ed by all a, x âˆˆX and all c âˆˆRn.
Example 8.1.1. The subdiï¬€erentials of the one-variable function f(x) = |x|
are
âˆ‚f(a) =
ï£±
ï£´
ï£²
ï£´
ï£³
{âˆ’1}
if a < 0,
[âˆ’1, 1]
if a = 0,
{1}
if a > 0.
Theorem 8.1.1. The subdiï¬€erentials of an arbitrary function f : X â†’R are
closed and convex sets.
Proof. For points a âˆˆdom f,
âˆ‚f(a) = 
xâˆˆdom f{c âˆˆRn | âŸ¨c, x âˆ’aâŸ©â‰¤f(x) âˆ’f(a)}
is convex and closed, since it is an intersection of closed halfspaces, and the
case a âˆˆX \ dom f is trivial.
Theorem 8.1.2. A point a âˆˆX is a global minimum point of the function
f : X â†’R if and only if 0 âˆˆâˆ‚f(a).
Proof. The assertion follows immediately from the subgradient deï¬nition.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
165
The subdifferential
165
Our next theorem tells us that the derivative f â€²(a) is the only subgradient
candidate for functions f that are diï¬€erentiable at a.
Geometrically this
means that the tangent plane at a is the only possible supporting hyperplane.
Theorem 8.1.3. Suppose that the function f : X â†’R is diï¬€erentiable at the
point a âˆˆdom f. Then either âˆ‚f(a) = {f â€²(a)} or âˆ‚f(a) = âˆ….
Proof. Suppose c âˆˆâˆ‚f(a). By the diï¬€erentiability deï¬nition,
f(a + v) âˆ’f(a) = âŸ¨f â€²(a), vâŸ©+ r(v)
with a remainder term r(v) satisfying the condition
lim
vâ†’0
r(v)
âˆ¥vâˆ¥= 0,
and by the subgradient deï¬nition, f(a + v) âˆ’f(a) â‰¥âŸ¨c, vâŸ©for all v such that
a + v belongs to X. Consequently,
(8.2)
âŸ¨c, vâŸ©
âˆ¥vâˆ¥â‰¤âŸ¨f â€²(a), vâŸ©+ r(v)
âˆ¥vâˆ¥
for all v with a suï¬ƒciently small norm âˆ¥vâˆ¥.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
166
The subdifferential
Let ej be the j:th unit vector. Then âŸ¨c, ejâŸ©= cj and âŸ¨f â€²(a), ejâŸ©= âˆ‚f
âˆ‚xj
(a),
so by choosing v = tej in inequality (8.2), noting that âˆ¥tejâˆ¥= |t|, and letting
t â†’0 from the right and from the left, respectively, we obtain the following
two inequalities
cj â‰¤âˆ‚f
âˆ‚xj
(a)
and
âˆ’cj â‰¤âˆ’âˆ‚f
âˆ‚xj
(a),
which imply that cj = âˆ‚f
âˆ‚xj
(a). Hence, c = f â€²(a), and this proves the inclusion
âˆ‚f(a) âŠ†{f â€²(a)}.
We can now reformulate Theorem 7.2.1 as follows: A diï¬€erentiable func-
tion with a convex domain is convex if and only if it has a subgradient (which
is then equal to the derivative) everywhere. Our next theorem generalizes
this result.
Theorem 8.1.4. Let f : X â†’R be a function with a convex domain X.
(a) If dom f is a convex set and âˆ‚f(x) Ì¸= âˆ…for all x âˆˆdom f, then f is a
convex function.
(b) If f is a convex function, then âˆ‚f(x) Ì¸= âˆ…for all x âˆˆrint(dom f).
Proof. (a) Let x and y be two arbitrary points in dom f and consider the
point z = Î»x+(1âˆ’Î»)y, where 0 < Î» < 1. By assumption, f has a subgradient
c at the point z. Using the inequality (8.1) at the point a = z twice, one
time with x replaced by y, we obtain the inequality
Î»f(x) + (1 âˆ’Î»)f(y) â‰¥Î»

f(z) + âŸ¨c, x âˆ’zâŸ©

+ (1 âˆ’Î»)

f(z) + âŸ¨c, y âˆ’zâŸ©

= f(z) + âŸ¨c, Î»x + (1 âˆ’Î»)y âˆ’zâŸ©= f(z) + âŸ¨c, 0âŸ©= f(z),
which shows that the restriction of f to dom f is a convex function, and this
implies that f itself is convex.
(b) Conversely, assume that f is a convex function, and let a be a point in
rint(dom f). We will prove that the subdiï¬€erential âˆ‚f(a) is nonempty.
The point (a, f(a)) is a relative boundary point of the convex set epi f.
Therefore, there exists a supporting hyperplane
H = {(x, xn+1) âˆˆRn Ã— R | âŸ¨c, x âˆ’aâŸ©+ cn+1(xn+1 âˆ’f(a)) = 0}
of epi f at the point (a, f(a)), and we may choose the normal vector (c, cn+1)
in such a way that
(8.3)
âŸ¨c, x âˆ’aâŸ©+ cn+1(xn+1 âˆ’f(a)) â‰¥0
for all points (x, xn+1) âˆˆepi f. We shall see that this implies that cn+1 > 0.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
167
The subdifferential
By applying inequality (8.3) to the point (a, f(a)+1) in the epigraph, we
ï¬rst obtain the inequality cn+1 â‰¥0.
Now suppose that cn+1 = 0, and put L = aï¬€(dom f). Since epi f âŠ†LÃ—R
and the supporting hyperplane H = {(x, xn+1) âˆˆRn Ã— R | âŸ¨c, xâˆ’aâŸ©= 0} by
deï¬nition does not contain epi f as a subset, it does not contain LÃ—R either.
We conclude that there exists a point y âˆˆL such that âŸ¨c, yâˆ’aâŸ©Ì¸= 0. Consider
the points yÎ» = (1 âˆ’Î»)a + Î»y for Î» âˆˆR; these points lie in the aï¬ne set L,
and yÎ» â†’a as Î» â†’0. Since a is a point in the relative interior of dom f,
the points yÎ» lie in dom f if |Î»| is suï¬ƒciently small, and this implies that
the inequality (8.3) can not hold for all points (yÎ», f(yÎ»)) in the epigraph,
because the expression âŸ¨c, yÎ» âˆ’aâŸ©(= Î»âŸ¨c, y âˆ’aâŸ©) assumes both positive and
negative values depending on the sign of Î».
This is a contradiction and proves that cn+1 > 0, and by dividing inequal-
ity (8.3) by cn+1 and letting d = âˆ’(1/cn+1)c, we obtain the inequality
xn+1 â‰¥f(a) + âŸ¨d, x âˆ’aâŸ©
for all (x, xn+1) âˆˆepi f.
In particular, f(x) â‰¥f(a) + âŸ¨d, x âˆ’aâŸ©for all
x âˆˆdom f, which means that d is a subgradient of f at a.
It follows from Theorem 8.1.4 that a real-valued function f with an open
convex domain X is convex if and only if âˆ‚f(x) Ì¸= âˆ…for all x âˆˆX.
Theorem 8.1.5. The subdiï¬€erential âˆ‚f(a) of a convex function f is a com-
pact nonempty set if a is an interior point of dom f.
Proof. Suppose a is a point in int(dom f). The subdiï¬€erential âˆ‚f(a) is closed
by Theorem 8.1.1 and nonempty by Theorem 8.1.4, so it only remains to
prove that it is a bounded set.
Theorem 6.6.1 yields two positive constants M and Î´ such that the closed
ball B(a; Î´) lies in dom f and
|f(x) âˆ’f(a)| â‰¤Mâˆ¥x âˆ’aâˆ¥
for x âˆˆB(a; Î´).
Now suppose that c âˆˆâˆ‚f(a) and that c Ì¸= 0. By choosing x = a + Î´c/âˆ¥câˆ¥
in inequality (8.1), we conclude that
Î´âˆ¥câˆ¥= âŸ¨c, x âˆ’aâŸ©â‰¤f(x) âˆ’f(a) â‰¤Mâˆ¥x âˆ’aâˆ¥= Î´M
with the bound âˆ¥câˆ¥â‰¤M as a consequence. The subdiï¬€erential âˆ‚f(a) is thus
included in the closed ball B(0; M).
Theorem 8.1.6. The sublevel sets of a strongly convex function f : X â†’R
are bounded sets.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
168
The subdifferential
168
Proof. Suppose that f is Âµ-strongly convex. Let x0 be a point in the relative
interior of dom f, and let c be a subgradient at the point x0 of the convex
function g(x) = f(x) âˆ’1
2Âµâˆ¥xâˆ¥2. Then, for each x belonging to the sublevel
set S = {x âˆˆX | f(x) â‰¤Î±},
Î± â‰¥f(x) = g(x) + 1
2Âµâˆ¥xâˆ¥2 â‰¥g(x0) + âŸ¨c, x âˆ’x0âŸ©+ 1
2Âµâˆ¥xâˆ¥2
= f(x0) âˆ’1
2Âµâˆ¥x0âˆ¥2 + âŸ¨c, x âˆ’x0âŸ©+ 1
2Âµâˆ¥xâˆ¥2
= f(x0) + 1
2Âµ

âˆ¥x + Âµâˆ’1câˆ¥2 âˆ’âˆ¥x0 + Âµâˆ’1câˆ¥2
,
which implies that
âˆ¥x + Âµâˆ’1câˆ¥2 â‰¤âˆ¥x0 + Âµâˆ’1câˆ¥2 + 2Âµâˆ’1(Î± âˆ’f(x0)).
The sublevel set S is thus included in a closed ball with center at the point
âˆ’Âµâˆ’1c and radius R =

âˆ¥x0 + Âµâˆ’1câˆ¥2 + 2Âµâˆ’1(Î± âˆ’f(x0)) .
Corollary 8.1.7. If a continuous and strongly convex function has a nonempty
closed sublevel set, then it has a unique minimum point.
In particular, every strongly convex function f : Rn â†’R has a unique
minimum point.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
169
The subdifferential
Proof. Let f be a continuous, strongly convex function with a nonempty
closed sublevel set S. Then S is compact by the previous theorem, so the
restriction of f to S assumes a minimum at some point in S, and this point
is obviously a global minimum point of f. The minimum point is unique,
because strongly convex functions are strictly convex.
A convex function f : Rn â†’R is automatically continuous, and continu-
ous functions on Rn are closed. Hence, all sublevel sets of a strongly convex
function on Rn are closed, so it follows from the already proven part of the
theorem that there is a unique minimum point.
8.2
Closed convex functions
In this section, we will use the subdiï¬€erential to supplement the results on
closed convex functions in chapter 6.8 with some new results. We begin with
an alternative characterization of closed convex functions.
Theorem 8.2.1. A convex function f : X â†’R is closed if and only if, for
all convergent sequences (xk)âˆ
1 of points in dom f with limit x0,
(8.4)
lim
kâ†’âˆ
f(xk)

â‰¥f(x0)
if x0 âˆˆdom f,
= +âˆ
if x0 âˆˆcl(dom f) \ dom f.
Proof. Suppose that f is closed, i.e. that epi f is a closed set, and let (xk)âˆ
1
be a sequence in dom f which converges to a point x0 âˆˆcl(dom f), and put
L = lim
kâ†’âˆ
f(xk).
Let a be an arbitrary point in the relative interior of dom f and let c be a
subgradient of f at the point a. Then f(xk) â‰¥f(a) + âŸ¨c, xk âˆ’aâŸ©for all k,
and since the right hand side converges (to f(a) + âŸ¨c, x0 âˆ’aâŸ©) as k â†’âˆ, we
conclude that the sequence (f(xk))âˆ
1 is bounded below. Its least limit point,
i.e. L, is therefore a real number or +âˆ.
Inequality (8.4) is trivially satisï¬ed if L = +âˆ, so assume that L is a
ï¬nite number, and let (xkj)âˆ
j=1 be a subsequence of the given sequence with
the property that f(xkj) â†’L as j â†’âˆ. The points (xkj, f(xkj)), which
belong to epi f, then converge to the point (x0, L), and since the epigraph is
assumed to be closed, we conclude that the limit point (x0, L) belongs to the
epigraph, i.e. x0 âˆˆdom f and L â‰¥f(x0).
So if x0 does not belong to dom f but to cl(dom f)\dom f, then we must
have L = +âˆ. This proves that (8.4) holds.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
170
The subdifferential
Conversely, suppose (8.4) holds for all convergent sequences, and let
((xk.tk))âˆ
1 be a sequence of points in epi f which converges to a point (x0, t0).
Then, (xk)âˆ
1 converges to x0 and (tk)âˆ
1 converges to t0, and since f(xk) â‰¤tk
for all k, we conclude that
lim
kâ†’âˆ
f(xk) â‰¤lim
kâ†’âˆ
tk = t0.
In particular, limkâ†’âˆf(xk) < +âˆ, so it follows from inequality (8.4) that
x0 âˆˆdom f and that f(x0) â‰¤t0. This means that the limit point (x0, t0)
belongs to epi f. Hence, epi f contains all its boundary points and is therefore
a closed set, and this means that f is a closed function.
Corollary
8.2.2. Suppose that f : X â†’R is a convex function and that
its eï¬€ective domain dom f is relative open. Then, f is closed if and only
if limkâ†’âˆf(xk) = +âˆfor each sequence (xk)âˆ
1
of points in dom f that
converges to a relative boundary point of dom f.
Proof. Since a convex function is continuous at all points in the relative
interior of its eï¬€ective domain, we conclude that limkâ†’âˆf(xk) = f(x0) for
each sequence (xk)âˆ
1 of points in dom f that converges to a point x0 in dom f.
Condition (8.4) of the previous theorem is therefore fulï¬lled if and only if
limkâ†’âˆf(xk) = +âˆfor all sequences (xk)âˆ
1
in dom f that converge to a
point in rbdry(dom f).
So a convex function with an aï¬ƒne set as eï¬€ective domain is closed (and
continuous), because aï¬ƒne sets lack relative boundary points.
Example 8.2.1. The convex function f(x) = âˆ’ln x with R++ as domain is
closed, since lim
xâ†’0 f(x) = +âˆ.
Theorem 8.2.3. If the function f : X â†’R is convex and closed, then
f(x) = lim
Î»â†’1âˆ’f(Î»x + (1 âˆ’Î»)y)
for all x, y âˆˆdom f.
Proof. The inequality
lim
Î»â†’1âˆ’f(Î»x + (1 âˆ’Î»)y) â‰¤lim
Î»â†’1âˆ’(Î»f(x) + (1 âˆ’Î»)f(y)) = f(x)
holds for all convex functions f, and the inequality
lim
Î»â†’1âˆ’f(Î»x + (1 âˆ’Î»)y) â‰¥f(x)
holds for all closed convex functions f according to Theorem 8.2.1.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
171
The subdifferential
171
Theorem 8.2.4. Suppose that f and g are two closed convex functions, that
rint(dom f) = rint(dom g)
and that
f(x) = g(x)
for all x âˆˆrint(dom f). Then f = g.
We remind the reader that the equality f = g should be interpreted as
dom f = dom g and f(x) = g(x) for all points x in the common eï¬€ective
domain.
Proof. If rint(dom f) = âˆ…, then dom f = dom g = âˆ…, and there is nothing to
prove, so suppose that x0 is a point in rint(dom f). Then, Î»x + (1 âˆ’Î»)x0 lies
in rint(dom f), too, for each x âˆˆdom f and 0 < Î» < 1, and it follows from
our assumptions and Theorem 8.2.3 that
g(x) = lim
Î»â†’1âˆ’g(Î»x + (1 âˆ’Î»)x0) = lim
Î»â†’1âˆ’f(Î»x + (1 âˆ’Î»)x0) = f(x).
Hence, g(x) = f(x) for all x âˆˆdom f, and it follows that dom f âŠ†dom g.
The converse inclusion holds by symmetry, so dom f = dom g.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
172
The subdifferential
Theorem 8.2.5. Let f : X â†’R and g: Y â†’R be two closed convex func-
tions with X âˆ©Y Ì¸= âˆ…. The sum f + g: X âˆ©Y â†’R is then a closed convex
function.
Proof. The theorem follows from the characterization of closedness in Theo-
rem 8.2.1. Let (xk)âˆ
1 be a convergent sequence of points in dom(f + g) with
limit point x0. If x0 belongs to dom(f + g) (= dom f âˆ©dom g), then
lim
kâ†’âˆ
(f(xk) + g(xk)) â‰¥lim
kâ†’âˆ
f(xk) + lim
kâ†’âˆ
g(xk) â‰¥f(x0) + g(x0),
and if x0 does not belong to dom(f + g), then we use the trivial inclusion
cl(A âˆ©B) \ A âˆ©B âŠ†(cl A \ A) âˆª(cl B \ B),
with A = dom f and B = dom g, to conclude that the sum f(xk) + g(xk)
tends to +âˆ, because one of the two sequences (f(xk))âˆ
1 and (g(xk))âˆ
1 tends
to +âˆwhile the other either tends to +âˆor has a ï¬nite limes inferior.
The closure
Deï¬nition. Let f : X â†’R be a function deï¬ned on a subset of Rn and
deï¬ne (cl f)(x) for x âˆˆRn by
(cl f)(x) = inf{t | (x, t) âˆˆcl(epi f)}.
The function cl f : Rn â†’R is called the closure of f.
Theorem 8.2.6. The closure cl f of a convex function f, whose eï¬€ective
domain is a nonempty subset of Rn, has the following properties:
(i)
The closure cl f : Rn â†’R is a convex function.
(ii)
dom f âŠ†dom(cl f) âŠ†cl(dom f).
(iii)
rint(dom(cl f)) = rint(dom f).
(iv)
(cl f)(x) â‰¤f(x) for all x âˆˆdom f.
(v)
(cl f)(x) = f(x) for all x âˆˆrint(dom f).
(vi)
epi(cl f) = cl(epi f).
Proof. (i) Let x0 be an arbitrary point in rint(dom f), and let c be a sub-
gradient of f at the point x0.
Then f(x) â‰¥f(x0) + âŸ¨c, x âˆ’x0âŸ©for all
x âˆˆdom f, which means that the epigraph epi f is a subset of the closed
set K = {(x, t) âˆˆcl(dom f) Ã— R | âŸ¨c, x âˆ’x0âŸ©+ f(x0) â‰¤t}. It follows that
cl(epi f) âŠ†K, and hence
(cl f)(x) = inf{t | (x, t) âˆˆcl(epi f)} â‰¥inf{t | (x, t) âˆˆK}
= f(x0) + âŸ¨c, x âˆ’x0âŸ©> âˆ’âˆ
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
173
The subdifferential
for all x âˆˆRn. So R is a codomain of the function cl f, and since cl(epi f)
is a convex set, it now follows from Theorem 6.2.6 that cl f : Rn â†’R is a
convex function.
(ii), (iv) and (v) It follows from the inclusion epi f âŠ†cl(epi f) âŠ†K that
(cl f)(x)

â‰¤inf{t | (x, t) âˆˆepi f} = f(x) < +âˆ
if x âˆˆdom f,
â‰¥inf{t | (x, t) âˆˆK} = inf âˆ…= +âˆ
if x /âˆˆcl(dom f),
(cl f)(x0) â‰¥inf{t | (x0, t) âˆˆK} = f(x0).
This proves that dom f âŠ†dom(cl f) âŠ†cl(dom f), that (cl f)(x) â‰¤f(x) for
all x âˆˆdom f, and that (cl f)(x0) = f(x0), and since x0 is an arbitrary point
in rint(dom f), we conclude that (cl f)(x) = f(x) for all x âˆˆrint(dom f).
(iii) Since rint(cl X) = rint X for arbitrary convex sets X, it follows in par-
ticular from (ii) that
rint(dom f) âŠ†rint(dom(cl f)) âŠ†rint(cl(dom f)) = rint(dom f),
with the conclusion that rint(dom(cl f)) = rint(dom f).
(vi) The implications
(x, t) âˆˆcl(epi f) â‡’(cl f)(x) â‰¤t â‡’(x, t) âˆˆepi(cl f)
follow immediately from the closure and epigraph deï¬nitions. Conversely,
suppose that (x, t) is a point in epi(cl f), i.e. that (cl f)(x) â‰¤t, and let
U Ã— I be an open neighborhood of (x, t). The neighborhood I of t contains
a number s such that (x, s) âˆˆcl(epi f), and since U Ã— I is also an open
neighborhood of (x, s), it follows that epi f âˆ©(U Ã— I) Ì¸= âˆ…. This proves that
(x, t) âˆˆcl(epi f), so we have the implication
(x, t) âˆˆepi(cl f) â‡’(x, t) âˆˆcl(epi f).
Thus, epi(cl f) = cl(epi f).
Theorem 8.2.7. If f is a closed convex function, then cl f = f.
Proof. We have rint(dom(cl f)) = rint(dom f) and (cl f)(x) = f(x) for all
x âˆˆrint(dom f), by the previous theorem. Therefore it follows from Theorem
8.2.4 that cl f = f.
8.3
The conjugate function
Deï¬nition. Let f : X â†’R be an arbitrary function deï¬ned on a subset X
of Rn and deï¬ne a function f âˆ—on Rn by
f âˆ—(y) = sup{âŸ¨y, xâŸ©âˆ’f(x) | x âˆˆX}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
174
The subdifferential
for y âˆˆRn. The function f âˆ—is called the conjugate function or the Fenchel
transform of f.
We use the shorter notation f âˆ—âˆ—for the conjugate function of f âˆ—, i.e.
f âˆ—âˆ—= (f âˆ—)âˆ—.
The conjugate function f âˆ—of a function f : Rn â†’R with a nonempty
eï¬€ective domain is obviously a function Rn â†’R, and
f âˆ—(y) = sup{âŸ¨y, xâŸ©âˆ’f(x) | x âˆˆdom f}.
There are two trivial cases: If the eï¬€ective domain of f : X â†’R is empty,
then f âˆ—(y) = âˆ’âˆfor all y âˆˆRn, and if f : X â†’R assumes the value âˆ’âˆ
at some point, then f âˆ—(y) = +âˆfor all y âˆˆRn.
x
y
(0, âˆ’fâˆ—(c))
y = cx
y = f(x)
Figure 8.2.
A graphical illustration of the conjugate function fâˆ—when f
is a one-variable function. The function value fâˆ—(c) is equal to the maximal
vertical distance between the line y = cx and the curve y = f(x). If f is
diï¬€erentiable, then fâˆ—(c) = cx0 âˆ’f(x0) for some point x0 with fâ€²(x0) = c.
Example 8.3.1. The support functions that were deï¬ned in Section 6.9, are
conjugate functions.
To see this, deï¬ne for a given subset A of Rn the
function Ï‡A : Rn â†’R by
Ï‡A(x) =

0
if x âˆˆA,
+âˆ
if x /âˆˆA.
The function Ï‡A is called the indicator function of the set A, and it is a
convex function if A is a convex set. Obviously,
Ï‡âˆ—
A(y) = sup{âŸ¨y, xâŸ©| x âˆˆA} = SA(y)
for all y âˆˆRn, so the support function of A coincides with the conjugate
function Ï‡âˆ—
A of the indicator function of A.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
175
The subdifferential
175
We are primarily interested in conjugate functions of convex functions
f : X â†’R, but we start with some general results.
Theorem 8.3.1. The conjugate function f âˆ—of a function f : X â†’R with a
nonempty eï¬€ective domain is convex and closed.
Proof. The epigraph epi f âˆ—consists of all points (y, t) âˆˆRn Ã— R that satisfy
the inequalities âŸ¨x, yâŸ©âˆ’t â‰¤f(x) for all x âˆˆdom f, which means that it is
the intersection of a family of closed halfspaces in Rn Ã— R. Hence, epi f âˆ—is
a closed convex set, so the conjugate function f âˆ—is closed and convex.
Theorem 8.3.2 (Fenchelâ€™s inequality). Let f : X â†’R be a function with a
nonempty eï¬€ective domain. Then
âŸ¨x, yâŸ©â‰¤f(x) + f âˆ—(y)
for all x âˆˆX and all y âˆˆRn. Moreover, the two sides are equal for a given
x âˆˆdom f if and only if y âˆˆâˆ‚f(x).
Proof. The inequality follows immediately from the deï¬nition of f âˆ—(y) as a
least upper bound if x âˆˆdom f, and it is trivially true if x âˆˆX \ dom f.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
176
The subdifferential
Moreover, by the subgradient deï¬nition, if x âˆˆdom f then
y âˆˆâˆ‚f(x) â‡”f(z) âˆ’f(x) â‰¥âŸ¨y, z âˆ’xâŸ©
for all z âˆˆdom f
â‡”âŸ¨y, zâŸ©âˆ’f(z) â‰¤âŸ¨y, xâŸ©âˆ’f(x)
for all z âˆˆdom f
â‡”f âˆ—(y) â‰¤âŸ¨y, xâŸ©âˆ’f(x)
â‡”f(x) + f âˆ—(y) â‰¤âŸ¨x, yâŸ©,
and by combining this with the already proven Fenchel inequality, we obtain
the equivalence y âˆˆâˆ‚f(x) â‡”f(x) + f âˆ—(y) = âŸ¨x, yâŸ©.
By the previous theorem, for all points y in the set {âˆ‚f(x) | x âˆˆdom f}
f âˆ—(y) = âŸ¨xy, yâŸ©âˆ’f(xy),
where xy is a point satisfying the condition y âˆˆâˆ‚f(xy). For diï¬€erentiable
functions f we obtain the points xy as solutions to the equation f â€²(x) = y.
Here follows a concrete example.
Example 8.3.2. Let f : ]âˆ’1, âˆ[â†’R be the function
f(x) =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
âˆ’x(x + 1)âˆ’1
if âˆ’1 < x â‰¤0,
2x
if 0 â‰¤x < 1,
(x âˆ’2)2 + 1
if 1 â‰¤x < 2,
2x âˆ’3
if x â‰¥2.
Its graph is shown in the left part of Figure 8.3.
âˆ’1
1
2
3
x
1
2
3
y
1
2
3
âˆ’1
âˆ’2
âˆ’3
âˆ’4
1
2
+âˆ
Figure 8.3. To the left the graph of the function f : ]âˆ’1, âˆ[â†’R,
and to the right the graph of the conjugate function fâˆ—: R â†’R.
A look at the ï¬gure shows that the curve y = f(x) lies above all lines
that are tangent to the curve at a point (x, y) with âˆ’1 < x < 0, lies above
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
177
The subdifferential
all lines through the origin with a slope between f â€²
âˆ’(0) = âˆ’1 and the slope of
the chord that connects the origin and the point (2, 1) on the curve, and lies
above all lines through the point (2, 1) with a slope between 1
2 and f â€²
+(2) = 2.
This means that

{âˆ‚f(x)} =

âˆ’1<x<0
{f â€²(x)} âˆªâˆ‚f(0) âˆªâˆ‚f(2) âˆª

x>2
{f â€²(x)}
= ]âˆ’âˆ, âˆ’1[ âˆª[âˆ’1, 1
2] âˆª[ 1
2, 2] âˆª{2} = ]âˆ’âˆ, 2].
The equation f â€²(x) = c has for c < âˆ’1 the solution x = âˆ’1 +

âˆ’1/c in
the interval âˆ’1 < x < 0. Let
xc =
ï£±
ï£´
ï£²
ï£´
ï£³
âˆ’1 +

âˆ’1/c
if c < âˆ’1,
0
if âˆ’1 â‰¤c â‰¤1
2,
2
if 1
2 â‰¤c â‰¤2.
Then c âˆˆâˆ‚f(xc), and it follows from the remark after Theorem 8.3.2 that
f âˆ—(c) = cxc âˆ’f(xc) =
ï£±
ï£´
ï£²
ï£´
ï£³
âˆ’c âˆ’2âˆšâˆ’c + 1
if c < âˆ’1,
0
if âˆ’1 â‰¤c â‰¤1
2,
2c âˆ’1
if 1
2 â‰¤c â‰¤2.
Since
f âˆ—(c) = sup
x>âˆ’1{cx âˆ’f(x)} â‰¥sup
xâ‰¥2
{cx âˆ’f(x)} = sup
xâ‰¥2
{(c âˆ’2)x + 3} = +âˆ
if c > 2, we conclude that dom f âˆ—=]âˆ’âˆ, 2]. The graph of f âˆ—is shown in the
right part of Figure 8.3.
Theorem 8.3.3. Let f : X â†’R be an arbitrary function. Then
f âˆ—âˆ—(x) â‰¤f(x)
for all x âˆˆX. Furthermore, f âˆ—âˆ—(x) = f(x) if x âˆˆdom f and âˆ‚f(x) Ì¸= âˆ….
Proof. If f(x) = +âˆfor all x âˆˆX, then f âˆ—â‰¡âˆ’âˆand f âˆ—âˆ—â‰¡+âˆ, according
to the remarks following the deï¬nition of the conjugate function, so the
inequality holds with equality for all x âˆˆX in this trivial case.
Suppose, therefore, that dom f Ì¸= âˆ….
Then âŸ¨x, yâŸ©âˆ’f âˆ—(y) â‰¤f(x) for
all x âˆˆX and all y âˆˆdom f âˆ—because of Fenchelâ€™s inequality, and hence
f âˆ—âˆ—(x) = sup{âŸ¨x, yâŸ©âˆ’f âˆ—(y) | y âˆˆdom f âˆ—} â‰¤f(x).
If âˆ‚f(x) Ì¸= âˆ…, then Fenchelâ€™s inequality holds with equality for y âˆˆâˆ‚f(x).
This means that f(x) = âŸ¨x, yâŸ©âˆ’f âˆ—(y) â‰¤f âˆ—âˆ—(x) and implies that f(x) =
f âˆ—âˆ—(x).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
178
The subdifferential
178
The following corollary follows immediately from Theorem 8.3.3, because
convex functions have subgradients at all relative interior points of their
eï¬€ective domains.
Corollary 8.3.4. If f : X â†’R is a convex function, then f âˆ—âˆ—(x) = f(x) for
all x in the relative interior of dom f.
We will prove that f âˆ—âˆ—= cl f if f is a convex function, and for this purpose
we need the following lemma.
Lemma 8.3.5. Suppose that f is a convex function and that (x0, t0) is a
point in Rn Ã— R which does not belong to cl(epi f). Then there exist a vector
c âˆˆRn and a real number d such that the â€non-verticalâ€ hyperplane
H = {(x, xn+1) | xn+1 = âŸ¨c, xâŸ©+ d}
strictly separates the point (x0, t0) from cl(epi f).
Proof. By the Separation Theorem 3.1.3, there exists a hyperplane
H = {(x, xn+1) | cn+1xn+1 = âŸ¨c, xâŸ©+ d}
which strictly separates the point from cl(epi f). If cn+1 Ì¸= 0, we can without
loss of generality assume that cn+1 = 1, and there is nothing more to prove.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
179
The subdifferential
So assume that cn+1 = 0, and choose the signs of c and d so that âŸ¨c, x0âŸ©+
d > 0 and âŸ¨c, xâŸ©+ d < 0 for all x âˆˆdom f.
Using the subgradient câ€² at some point in the relative interior of dom f
we obtain an aï¬ƒne function âŸ¨câ€², xâŸ©+ dâ€² such that f(x) â‰¥âŸ¨câ€², xâŸ©+ dâ€² for all
x âˆˆdom f. This implies that
f(x) â‰¥âŸ¨câ€², xâŸ©+ dâ€² + Î»(âŸ¨c, xâŸ©+ d) = âŸ¨câ€² + Î»c, xâŸ©+ dâ€² + Î»d
for all x âˆˆdom f and all positive numbers Î», while
âŸ¨câ€² + Î»c, x0âŸ©+ dâ€² + Î»d = âŸ¨câ€², x0âŸ©+ dâ€² + Î»(âŸ¨c, x0âŸ©+ d) > t0
for all suï¬ƒciently large numbers Î».
So the epigraph epi f lies above the
hyperplane
HÎ» = {(x, xn+1) | xn+1 = âŸ¨câ€² + Î»c, xâŸ©+ dâ€² + Î»d}.
and the point (x0, t0) lies strictly below the same hyperplane, if the number
Î» is big enough.
By moving the hyperplane HÎ» slightly downwards, we
obtain a parallel non-vertical hyperplane which strictly separates (x0, t0) and
cl(epi f).
Lemma 8.3.6. If f : X â†’R is a convex function, then
rint(dom f âˆ—âˆ—) = rint(dom f).
Proof. Since rint(dom f) = rint(cl(dom f)), it suï¬ƒces to prove the inclusion
dom f âŠ†dom f âˆ—âˆ—âŠ†cl(dom f).
The left inclusion follows immediately from the inequality in Theorem 8.3.3.
To prove the right inclusion, we assume that x0 /âˆˆcl(dom f) and shall prove
that this implies that x0 /âˆˆdom f âˆ—âˆ—.
It follows from our assumption that the points (x0, t0) do not belong
to cl(epi f) for any number t0.
Thus, given t0 âˆˆR there exists, by the
previous lemma, a hyperplane H = {(x, xn+1) âˆˆRn Ã— R | xn+1 = âŸ¨c, xâŸ©+ d}
which strictly separates (x0, t0) and cl(epi f). Hence, t0 < âŸ¨c, x0âŸ©+ d and
âŸ¨c, xâŸ©+ d < f(x) for all x âˆˆdom f. Consequently,
âˆ’d â‰¥sup{âŸ¨c, xâŸ©âˆ’f(x) | x âˆˆdom f} = f âˆ—(c),
and hence
t0 < âŸ¨c, x0âŸ©+ d â‰¤âŸ¨c, x0âŸ©âˆ’f âˆ—(c) â‰¤f âˆ—âˆ—(x0).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
180
The subdifferential
Since this holds for all real numbers t0, it follows that f âˆ—âˆ—(t0) = +âˆ, which
means that x0 /âˆˆdom f âˆ—âˆ—.
Theorem 8.3.7. If f is a convex function, then f âˆ—âˆ—= cl f.
Proof. It follows from Lemma 8.3.6 and Theorem 8.2.6 (iii) that
rint(dom f âˆ—âˆ—) = rint(dom(cl f)),
and from Theorem 8.3.4 and Theorem 8.2.6 (v) that
f âˆ—âˆ—(x) = (cl f)(x)
for all x âˆˆrint(dom f âˆ—âˆ—). So the two functions f âˆ—âˆ—and cl f are equal, ac-
cording to Theorem 8.2.4, because both of them are closed and convex .
Corollary 8.3.8. If f is a closed convex function, then f âˆ—âˆ—= f.
8.4
The direction derivative
Deï¬nition. Suppose the function f : X â†’R is deï¬ned in a neighborhood of
x, and let v be an arbitrary vector in Rn. The limit
f â€²(x; v) = lim
tâ†’0+
f(x + tv) âˆ’f(x)
t
,
provided it exists, is called the direction derivative of f at the point x in the
direction v.
If f is diï¬€erentiable at x, then obviously f â€²(x; v) = Df(x)[v].
Example 8.4.1. If f is a one-variable function, then
f â€²(x; v) =
ï£±
ï£´
ï£²
ï£´
ï£³
f â€²
+(x)v
if v > 0,
0
if v = 0,
f â€²
âˆ’(v)v
if v < 0.
So, the direction derivative is a generalization of left- and right derivatives.
Theorem 8.4.1. Let f : X â†’R be a convex function with an open domain.
The direction derivative f â€²(x; v) exists for all x âˆˆX and all directions v, and
f(x + v) â‰¥f(x) + f â€²(x; v)
if x + v lies in X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
181
The subdifferential
181
Proof. Let Ï†(t) = f(x + tv); then f â€²(x; v) = Ï†â€²
+(0), which exists since con-
vex one-variable functions do have right derivatives at each point by Theo-
rem 7.1.2. Moreover,
Ï†(t) â‰¥Ï†(0) + Ï†â€²
+(0) t
for all t in the domain of Ï†, and we obtain the inequality of the theorem by
choosing t = 1.
Theorem 8.4.2. The direction derivative f â€²(x; v) of a convex function is a
positively homogeneous and convex function of the second variable v, i.e.
f â€²(x; Î±v) = Î±f â€²(x; v)
if Î± â‰¥0
f â€²(x; Î±v + (1 âˆ’Î±)w) â‰¤Î±f â€²(x; v) + (1 âˆ’Î±)f â€²(x; w)
if 0 â‰¤Î± â‰¤1.
Proof. The homogenouity follows directly from the deï¬nition (and holds for
arbitrary functions). Moreover, for convex functions f
f(x + t(Î±v + (1 âˆ’Î±)w)) âˆ’f(x) = f(Î±(x + tv) + (1 âˆ’Î±)(x + tw)) âˆ’f(x)
â‰¤Î±

f(x + tv) âˆ’f(x)

+ (1 âˆ’Î±)

f(x + tw) âˆ’f(x)

.
The required convexity inequality is now obtained after division by t > 0 by
passing to the limit as t â†’0+.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
182
The subdifferential
Theorem 7.1.2 gives a relation between the subgradient and the direction
derivative for convex one-variable functions f âˆ’the number c is a subgradient
at x if and only if f â€²
âˆ’(x) â‰¤c â‰¤f â€²
+(x). The subdiï¬€erential âˆ‚f(x) is in other
words equal to the interval [f â€²
âˆ’(x), f â€²
+(x)].
We may express this relation using the support function of the subdiï¬€er-
ential. Let us recall that the support function SX of a set X in Rn is deï¬ned
as
SX(x) = sup{âŸ¨y, xâŸ©| y âˆˆX}.
For one-variable functions f this means that
Sâˆ‚f(x)(v) = S[fâ€²
âˆ’(x),fâ€²
+(x)](v) = max{f â€²
+(x)v, f â€²
âˆ’(x)v} =
ï£±
ï£´
ï£²
ï£´
ï£³
f â€²
+(x)v
if v > 0,
0
if v = 0,
f â€²
âˆ’(x)v
if v < 0
= f â€²(x; v).
We will generalize this identity, and to achieve this we need to consider the
subgradients of the function v â†’f â€²(x; v). We denote the subdiï¬€erential of
this function at the point v0 by âˆ‚2f â€²(x; v0).
If the function f : X â†’R is convex, then so is the function v â†’f â€²(x; v),
according to our previous theorem, and the subdiï¬€erentials âˆ‚2f â€²(x; v) are
thus nonempty sets for all x âˆˆX and all v âˆˆRn.
Lemma 8.4.3. Let f : X â†’R be a convex function with an open domain X
and let x be a point in X. Then:
c âˆˆâˆ‚2f â€²(x; 0) â‡”f â€²(x; v) â‰¥âŸ¨c, vâŸ©
for all v âˆˆRn
(i)
âˆ‚2f â€²(x; v) âŠ†âˆ‚2f â€²(x; 0)
for all v âˆˆRn
(ii)
c âˆˆâˆ‚2f â€²(x; v) â‡’f â€²(x; v) = âŸ¨c, vâŸ©
(iii)
âˆ‚f(x) = âˆ‚2f â€²(x; 0).
(iv)
Proof. The equivalence (i) follows directly from the deï¬nition of the subgra-
dient and the fact that f â€²(x; 0) = 0.
(ii) and (iii): Suppose c âˆˆâˆ‚2f â€²(x; v) and let w âˆˆRn be an arbitrary vector.
Then, by homogenouity and the deï¬nition of the subgradient, we have the
following inequality for t â‰¥0:
tf â€²(x; w) = f â€²(x; tw) â‰¥f â€²(x; v) + âŸ¨c, tw âˆ’vâŸ©= f â€²(x; v) + tâŸ¨c, wâŸ©âˆ’âŸ¨c, vâŸ©,
and this is possible for all t > 0 only if f â€²(x; w) â‰¥âŸ¨c, wâŸ©. So we conclude from
(i) that c âˆˆâˆ‚2f â€²(x; 0), and this proves the inclusion (ii). By choosing t = 0
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
183
The subdifferential
we obtain the inequality f â€²(x; v) â‰¤âŸ¨c, vâŸ©, which implies that f â€²(x; v) = âŸ¨c, vâŸ©,
and completes the proof of the implication (iii).
(iv)
Suppose c âˆˆâˆ‚2f â€²(x; 0). By (i) and Theorem 8.4.1,
f(y) â‰¥f(x) + f â€²(x; y âˆ’x) â‰¥f(x) + âŸ¨c, y âˆ’xâŸ©
for all y âˆˆX, which proves that c is a subgradient of f at the point x and
gives us the inclusion âˆ‚2f â€²(x; 0) âŠ†âˆ‚f(x).
Conversely, suppose c âˆˆâˆ‚f(x). Then f(x + tv) âˆ’f(x) â‰¥âŸ¨c, tvâŸ©= tâŸ¨c, vâŸ©
for all suï¬ƒciently small numbers t. Division by t > 0 and passing to the limit
as t â†’0+ results in the inequality f â€²(x; v) â‰¥âŸ¨c, vâŸ©, and it now follows from
(i) that c âˆˆâˆ‚2f â€²(x; 0). This proves the inclusion âˆ‚f(x) âŠ†âˆ‚2f â€²(x; 0), and the
proof is now complete.
Theorem 8.4.4. Suppose that f : X â†’R is a convex function with an open
domain. Then
f â€²(x; v) = Sâˆ‚f(x)(v) = max{âŸ¨c, vâŸ©| c âˆˆâˆ‚f(x)}
for all x âˆˆX and all v âˆˆRn.
Proof. It follows from (i) and (iv) in the preceding lemma that
âŸ¨c, vâŸ©â‰¤f â€²(x; v)
for all c âˆˆâˆ‚f(x), and from (ii), (iii) and (iv) in the same lemma that âŸ¨c, vâŸ©
is equal to f â€²(x; v) for all subgradients c in the nonempty subset âˆ‚2f â€²(x; v)
of âˆ‚f(x).
8.5
Subdiï¬€erentiation rules
Theorem 8.5.1. Let X be an open convex set, and suppose that fi : X â†’R
are convex functions and Î±i are nonnegative numbers for i = 1, 2 . . . , m.
Deï¬ne
f =
m

i=1
Î±ifi.
Then
âˆ‚f(x) =
m

i=1
Î±iâˆ‚fi(x).
Proof. A sum of compact, convex sets is compact and convex. Therefore,
m
i=1 Î±iâˆ‚fi(x) is a closed and convex set, just as the set âˆ‚f(x). Hence, by
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
184
The subdifferential
184
Theorem 6.9.2 it suï¬ƒces to prove that the two sets have the same support
function. And this follows from Theorems 8.4.4 and 6.9.1, according to which
Sâˆ‚f(x)(v) = f â€²(x; v) =
m

i=1
Î±if â€²
i(x; v) =
m

i=1
Î±iSâˆ‚fi(x) = Sm
i=1 Î±iâˆ‚fi(x)(v).
Theorem 8.5.2. Suppose that the functions fi : X â†’R are convex for i = 1,
2,..., m, and that their domain X is open, and let
f = max
1â‰¤iâ‰¤m fi.
Then
âˆ‚f(x) = cvx
 
iâˆˆI(x)
âˆ‚fi(x)

,
for all x âˆˆX, where I(x) = {i | fi(x) = f(x)}.
Proof. The functions fi are continuous at x and fj(x) < f(x) for all j /âˆˆI(x).
Hence, for all suï¬ƒciently small numbers t,
f(x + tv) âˆ’f(x) = max
iâˆˆI(x) fi(x + tv) âˆ’f(x) = max
iâˆˆI(x)(fi(x + tv) âˆ’fi(x)),
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
185
The subdifferential
and it follows after division by t and passing to the limit that
f â€²(x; v) = max
iâˆˆI(x) f â€²
i(x; v).
We use Theorem 6.9.1 to conclude that
Sâˆ‚f(x)(v) = f â€²(x; v) = max
iâˆˆI(x) f â€²
i(x; v) = max
iâˆˆI(x) Sâˆ‚fi(x)(v) = S
iâˆˆI(x) âˆ‚fi(x)(v)
= Scvx(
iâˆˆI(x) âˆ‚fi(x))(v),
and the equality for âˆ‚f(x) is now a consequence of Theorem 6.9.2.
Our next theorem shows how to compute the subdiï¬€erential of a compo-
sition with aï¬ƒne functions.
Theorem 8.5.3. Suppose C is a linear map from Rn to Rm, that b is a vector
in Rm, and that g is a convex function with an open domain in Rm, and let f
be the function deï¬ned by f(x) = g(Cx + b). Then, for each x in the domain
of f,
âˆ‚f(x) = CT(âˆ‚g(Cx + b)).
Proof. The sets âˆ‚f(x) and CT(âˆ‚g(Cx + b)) are convex and compact, so it
suï¬ƒces to show that their support functions are identical.
But for each
v âˆˆRn
f â€²(x; v) = lim
tâ†’0+
g(C(x + tv) + b) âˆ’g(Cx + b)
t
= lim
tâ†’0+
g(Cx + b + t Cv) âˆ’g(Cx + b)
t
= gâ€²(Cx + b; Cv),
so it follows because of Theorem 6.9.1 that
Sâˆ‚f(x)(v) = f â€²(x; v) = gâ€²(Cx + b; Cv) = Sâˆ‚g(Cx+b)(Cv) = SCT(âˆ‚g(Cx+b))(v).
The Karushâ€“Kuhnâ€“Tucker theorem
As an application of the subdiï¬€erentiation rules we now prove a variant of a
theorem by Karushâ€“Kuhnâ€“Tucker on minimization of convex functions with
convex constraints. A more thorough treatment of this theme will be given
in Part II.
Theorem 8.5.4. Suppose that the functions f, g1, g2, . . . , gm are convex and
deï¬ned on an open convex set â„¦, and let
X = {x âˆˆâ„¦| gi(x) â‰¤0 for i = 1, 2, . . . , m.}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
186
The subdifferential
Moreover, suppose that there exists a point x âˆˆâ„¦such that gi(x) < 0 for
i = 1, 2, . . . , m. (Slaterâ€™s condition)
Then, Ë†x âˆˆX is a minimum point of the restriction f|X if and only if
for each i = 1, 2, . . . , m there exist a subgradient ci âˆˆâˆ‚gi(Ë†x) and a scalar
Ë†Î»i âˆˆR+ with the following properties:
âˆ’
m

i=1
Ë†Î»ici âˆˆâˆ‚f(Ë†x)
and
(i)
Ë†Î»igi(Ë†x) = 0
for i = 1, 2, . . . , m.
(ii)
Remark. If the functions are diï¬€erentiable, then condition (i) simpliï¬es to
âˆ‡f(Ë†x) +
m

i=1
Ë†Î»iâˆ‡gi(Ë†x) = 0.
Cf. Theorem 11.2.1 in Part II.
Proof. Let Ë†x be a point in X and consider the convex function
h(x) = max {f(x) âˆ’f(Ë†x), g1(x), . . . , gm(x)}
with â„¦as its domain. Clearly, h(Ë†x) = 0. By deï¬ning
I(Ë†x) = {i | gi(Ë†x) = 0},
we obtain I(Ë†x) = {i | gi(Ë†x) = h(Ë†x)}, and it follows from Theorem 8.5.2 that
âˆ‚h(Ë†x) = cvx

âˆ‚f(Ë†x) âˆª

{âˆ‚gi(Ë†x) | i âˆˆI(Ë†x)}

.
Now assume that Ë†x is a minimum point of the restriction f|X.
Then
h(x) = f(x) âˆ’f(Ë†x) â‰¥0 for all x âˆˆX with equality when x = Ë†x. And if
x /âˆˆX, then h(x) > 0 since gi(x) > 0 for at least one i. Thus, Ë†x is a global
minimum point of h.
Conversely, if Ë†x is a global minimum point of h, then h(x) â‰¥0 for all
x âˆˆâ„¦. In particular, for x âˆˆX this means that h(x) = f(x)âˆ’f(Ë†x) â‰¥0, and
hence Ë†x is a mimimum point of the restriction f|X, too.
Using Theorem 8.1.2 we therefore obtain the following equivalences:
Ë†x is a minimum point of f|X
â‡”
Ë†x is a minimum point of h
â‡”
0 âˆˆâˆ‚h(Ë†x)
â‡”
0 âˆˆcvx

âˆ‚f(Ë†x) âˆª

{âˆ‚gi(Ë†x) | i âˆˆI(Ë†x)}

â‡”
0 = Î»0c0 +

iâˆˆI(Ë†x)
Î»ici
â‡”
Î»0c0 = âˆ’

iâˆˆI(Ë†x)
Î»ici,
(8.5)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
187
The subdifferential
187
where c0 âˆˆâˆ‚f(Ë†x), ci âˆˆâˆ‚gi(Ë†x) for i âˆˆI(Ë†x), and the scalars Î»i are nonnegative
numbers with sum equal to 1.
We now claim that Î»0 > 0. To prove this, assume the contrary. Then

iâˆˆI(Ë†x) Î»ici = 0, and it follows that

iâˆˆI(Ë†x)
Î»igi(x) â‰¥

iâˆˆI(Ë†x)
Î»i

gi(Ë†x) + âŸ¨ci, x âˆ’Ë†xâŸ©

= âŸ¨

iâˆˆI(Ë†x)
Î»ici, x âˆ’Ë†xâŸ©= 0,
which is a contradiction, since gi(x) < 0 for all i and Î»i > 0 for some i âˆˆI(Ë†x).
We may therefore divide the equality in (8.5) by Î»0, and conditions (i)
and (ii) in our theorem are now fulï¬lled if we deï¬ne Ë†Î»i = Î»i/Î»0 for i âˆˆI(Ë†x),
and Ë†Î»i = 0 for i /âˆˆI(Ë†x), and choose arbitrary subgradients ci âˆˆâˆ‚gi(Ë†x) for
i /âˆˆI(Ë†x).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
188
The subdifferential
188
Exercises
8.1 Suppose f : Rn â†’R is a strongly convex function. Prove that
lim
âˆ¥xâˆ¥â†’âˆf(x) = âˆ
8.2 Find âˆ‚f(âˆ’1, 1) for the function f(x1, x2) = max(|x1|, |x2|).
8.3 Determine the subdiï¬€erential âˆ‚f(0) at the origin for the following functions
f : Rn â†’R:
a) f(x) = âˆ¥xâˆ¥2
b) f(x) = âˆ¥xâˆ¥âˆ
c) f(x) = âˆ¥xâˆ¥1.
8.4 Determine the conjugate functions of the following functions:
a) f(x) = ax + b, dom f = R
b) f(x) = âˆ’ln x, dom f = R++
c) f(x) = ex, dom f = R
d) f(x) = x ln x, dom f = R++
e) f(x) = 1/x, dom f = R++.
8.5 Use the relation between the support function SA and the indicator function
Ï‡A and the fact that SA = Scl(cvx A) to prove Corollary 6.9.3, i.e. that
cl(cvx A) = cl(cvx B) â‡”SA = SB.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
189
Bibliogracal and historical notices
Bibliograï¬cal and historical
notices
Basic references in convex analysis are the books by Rockafellar [1] from 1970
and Hiriart-Uruttyâ€“Lemarechal [1] from 1993. Almost all results in Part I
and in Chapters 9â€“10 in Part II of this series can be found in one form
or another in Rockafellarâ€™s book, which also contains a historical overview
with references to the original works in the ï¬eld. A more accessible book
on the same subject is Webster [1]. Among textbooks in convexity with an
emphasis on polyhedra, one should mention Stoerâ€“Witzgall [1] and the more
combinatorially oriented GrÂ¨unbaum [1].
The general convexity theory was founded around the turn of the century
1900 by Hermann Minkowski [1, 2] as a byproduct of his number theoretic
studies. Among other things, Minkowski introduced the concepts of sepa-
ration and extreme point, and he showed that every compact convex set is
equal to the convex hull of its extreme points and that every polyhedron is
ï¬nitely generated, i.e. one direction of Theorem 5.3.1) âˆ’the converse was
noted later by Weyl [1].
The concept of dual cone was introduced by Steinitz [1], who also showed
basic results about the recession cone.
The theory of linear inequalities is surprisingly young âˆ’a special case
of Theorem 3.3.7 (Exercise 3.11a) was proved by Gordan [1], the algebraic
version of Farkasâ€™s lemma, i.e. Corollary 3.3.3, can be found in Farkas [1],
and a closely related result (Exercise 3.11b) is given by Stiemke [1]. The
ï¬rst systematic treatment of the theory is given by Weyl [1] and Motzkin [1].
Signiï¬cant contributions have also been provided by Tucker [1]. The proof
in Chapter 3 of Farkasâ€™s lemma has a geometrical character; an alternative
algebraic induction proof of the lemma has been given by Kuhn [1].
Extreme points and faces are treated in detail in Klee [1,2].
Jensen [1] studied convex functions of one real variable and showed that
convex functions with R as domain are continuous and have one-sided deriva-
tives everywhere. Jensenâ€™s inequality, however, was shown earlier for func-
tions with positive second derivative by HÂ¨older [1].
The conjugate function was introduced by Fenchel [1], and a modern
treatment of the theory of convex cones, sets and functions can be found
in Fenchel [2], which among other things contains original results about the
closure of convex functions and about the subdiï¬€erential.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
190
References
References
Farkas, J.
[1] Theorie der einfachen Ungleichungen, J. reine angew. Math. 124 (1902),
1â€“27.
Fenchel, W.
[1] On conjugate convex functions. Canad. J. Math. 1 (1949), 73-77.
[2] Convex Cones, Sets and Functions. Lecture Notes, Princeton University,
1951.
Gordan, P.
[1] Â¨Uber die Auï¬‚Â¨osung linearer Gleichungen mit reellen Coeï¬ƒcienten, Math.
Ann. 6 (1873), 23â€“28.
GrÂ¨unbaum, B.
[1] Convex Polytopes. Interscience publishers, New York, 1967.
Hiriart-Urruty, J.-B. & LemarÂ´echal, C.
[1] Convex Analysis and Minimization Algorithms. Springer, 1993.
HÂ¨older, O.
[1] Â¨Uber einen Mittelwertsatz, Nachr. Ges. Wiss. GÂ¨ottingen, 38â€“47, 1989.
Jensen, J.L.W.V.
[1] Sur les fonctions convexes et les inÂ´egalitÂ´es entre les valeur moyennes,
Acta Math. 30 (1906), 175â€“193.
Klee, V.
[1] Extremal structure of convex sets, Arch. Math. 8 (1957), 234â€“240.
[2] Extremal structure of convex sets, II, Math. Z. 69 (1958), 90â€“104.
Klee, V. & Minty, G.J.
[1] How Good is the Simplex Algorithm? Pages 159â€“175 in Shisha, O. (ed.),
Inequalities, III, Academic Press, 1972.
Koopmans, T.C., ed.
[1] Activity Analysis of Production and Allocation.
John Wiley & Sons,
1951.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
191
References
Kuhn, H.W.
[1] Solvability and Consistency for Linear Equations and Inequalities, Amer.
Math. Monthly 63 (1956), 217â€“232.
Minkowski, H.
[1] Geometrie der Zahlen. Teubner, Leipzig, 1910.
[2] Gesammelte Abhandlungen von Hermann Minkowski, Vol. 1, 2. Teub-
ner, Leibzig, 1911
Motzkin, T.
[1] BeitrÂ¨age zur Theorie der linearen Ungleichungen.
Azviel, Jerusalem,
1936.
Rockafellar, R.T.
[1] Convex Analysis. Princeton Univ. Press., 1970
Steinitz, E.
[1] Bedingt konvergente Reihen und konvexe Systeme, I, II, III, J. Reine
Angew. Math. 143 (1913), 128â€“175; 144 (1914), 1â€“40; 146 (1916), 1â€“52.
Stiemke, E.
[1] Â¨Uber positive LÂ¨osungen homogener linearer Gleichungen, Math. Ann.
76 (1915), 340â€“342.
Stoer, J. & Witzgall, C.
[1] Convexity and Optimization in Finite Dimensions I. Springer-Verlag,
1970.
Tucker, A.W.
[1] Dual Systems of Homogeneous Linear Relations. Pages 3â€“18 in Kuhn,
H.W. & Tucker, A.W. (eds.), Linear Inequalities and Related Systems,
Princeton Univ. Press, 1956.
Webster, R.
[1] Convexity. Oxford University Press, 1954.
Weyl, H.
[1] Elementare Theorie der konvexen Polyeder, Comment. Math. Helv. 7
(1935), 290â€“306.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
192
Answers and solutions to the exercises
Answers and solutions to the
exercises
Chapter 2
2.2 a) {x âˆˆR2 | 0 â‰¤x1 + x2 â‰¤1, x1, x2 â‰¥0}
b) {x âˆˆR2 | âˆ¥xâˆ¥â‰¤1}
c) R2
++ âˆª{(0, 0)}
2.3 E.g. {(0, 1)} âˆª(R Ã— {0}) in R2.
2.4 {x âˆˆR3
++ | x2
3 â‰¤x1x2}
2.5 Use the triangle inequality
n
1(xj + yj)21/2 â‰¤
n
1 x2
j
1/2 +
n
1 y2
j
1/2
to show that the set is closed under addition of vectors. Or use the
perspective map; see example 2.3.4.
2.6 Follows from the fact that âˆ’ek is a conic combination of the vectors
e0, e1, . . . , en.
2.7 Let X be the halfspace {x âˆˆRn | âŸ¨c, xâŸ©â‰¥0}. Each vector x âˆˆX is a
conic combination of c and the vector y = x âˆ’âŸ¨c, xâŸ©âˆ¥câˆ¥âˆ’2c, and y lies
in the (n âˆ’1)-dimensional subspace Y = {x âˆˆRn | âŸ¨c, xâŸ©= 0}, which
is generated by n vectors as a cone according to the previous exercise.
Hence, x is a conic combination of these n vectors and c.
2.8 The intersection between the cone X and the unit circle is a closed
circular arc with endpoints x and y, say. The length of the arc is either
less than Ï€, equal to Ï€, or equal to 2Ï€. The cone X is proper and
generated by the two vectors x and y in the ï¬rst case. It is equal to a
halfspace in the second case and equal to R2 in the third case, and it
is generated by three vectors in both these cases.
2.9 Use exercise 2.8.
2.10 a) recc X = {x âˆˆR2 | x1 â‰¥x2 â‰¥0},
lin X = {(0, 0)}
b) recc X = lin X = {(0, 0)}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
193
Answers and solutions to the exercises
2.10 c) recc X = {x âˆˆR3 | 2x1 + x2 + x3 â‰¤0, x1 + 2x2 + x3 â‰¤0},
lin X = {(t, t, âˆ’3t) | t âˆˆR}
d) recc X ={x âˆˆR3 | x1 â‰¥|x2|},
lin X ={x âˆˆR3 | x1 = x2 = 0}.
2.12 b) (i)
c(X) = {x âˆˆR2 | 0 â‰¤1
3x1 â‰¤x2 â‰¤1
2x1} = cl(c(X))
(ii) c(X) = {x âˆˆR2 | 0 < x2 â‰¤1
2x1} âˆª{(0, 0)},
cl(c(X) = {x âˆˆR2 | 0 â‰¤x2 â‰¤1
2x1},
(iii) c(X) = {x âˆˆR3 | x1x3 â‰¥x2
2, x3 > 0} âˆª{(0, 0, 0)},
cl(c(X)) = c(X) âˆª{(x1, 0, 0) | x1 â‰¥0}.
c) c(X) = {(x, xn+1) âˆˆRn Ã— R | âˆ¥xâˆ¥â‰¤xn+1}.
2.14 Let zn = xn + yn, n = 1, 2, . . . be a convergent sequence of points in
X + Y with xn âˆˆX and yn âˆˆY for all n and limit z0. The sequence
(yn)âˆ
1 ha a convergent subsequence (ynk)âˆ
k=1 with limit y0 âˆˆY , since Y
is compact. The corresponding subsequence (znk âˆ’ynk)âˆ
k=1 of points in
X converges to z0 âˆ’y0, and the limit point belongs to X since X is a
closed set. Hence, z0 = (z0 âˆ’y0) + y0 lies in X + Y , and this means
that X + Y is a closed set.
Chapter 3
3.1 E.g. {x âˆˆR2 | x2 â‰¤0} and {x âˆˆR2 | x2 â‰¥ex1}.
3.2 Follows from Theorem 3.1.3 for closed sets and from Theorem 3.1.5 for
open sets.
3.4 a) R+ Ã— R
b) {0} Ã— R
c) {0} Ã— R+
d) R+ Ã— R+
e) {x âˆˆR2 | x2 â‰¥x1 â‰¥0}
3.6 a) X = X++ = {x âˆˆR2 | x1 + x2 â‰¥0, x2 â‰¥0},
X+ = {x âˆˆR2 | x2 â‰¥x1 â‰¥0}
b) X = X++ = R2,
X+ = {(0, 0)}
c) X = R2
++ âˆª{(0, 0)},
X+ = X++ = R2
+
3.7 (i) â‡’(iii): Since âˆ’aj /âˆˆcon A, there is, for each j, a vector cj such
that âˆ’âŸ¨cj, ajâŸ©< 0 and âŸ¨cj, xâŸ©â‰¥0 for all x âˆˆcon A, which implies that
âŸ¨cj, ajâŸ©> 0 and âŸ¨cj, akâŸ©â‰¥0 for all k. It follows that c = c1+c2+Â· Â· Â·+cm
works.
(iii) â‡’(ii): Suppose that âŸ¨c, ajâŸ©> 0 for all j. Then m
1 Î»jaj = 0
implies 0 = âŸ¨c, 0âŸ©= m
1 Î»j âŸ¨c, ajâŸ©, so if Î»j â‰¥0 for all j then Î»j âŸ¨c, ajâŸ©=
0 for all j, with the conclusion that Î»j = 0 for all j.
(ii) â‡’(i): If there is a vector x such that x = m
1 Î»jaj and âˆ’x =
m
1 Âµjaj with nonnegative scalars Î»j, Âµj, then by addition we obtan
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
194
Answers and solutions to the exercises
194
the equality m
1 (Î»j + Âµj)aj = 0 with the conclusions Î»j + Âµj = 0,
Î»j = Âµj = 0 and x = 0.
3.8 No solution.
3.10 Solvable for Î± â‰¤âˆ’2, âˆ’1 < Î± < 1 and Î± > 1.
3.11 a) The systems (S) and (Sâˆ—) are equivalent to the systems
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
Ax â‰¥0
âˆ’Ax â‰¥0
Ex â‰¥0
1Tx > 0
and
AT(yâ€² âˆ’yâ€²â€²) + Ez + 1t = 0
yâ€², yâ€²â€², z â‰¥0, t > 0,
respectively (with y corresponding to yâ€²â€² âˆ’yâ€²). The assertion therefore
follows from Theorem 3.3.7.
b) The systems (S) and (Sâˆ—) are equivalent to the systems
ï£±
ï£²
ï£³
Ax â‰¥0
âˆ’Ax â‰¥0
Ex > 0
and
AT(yâ€² âˆ’yâ€²â€²) + Ez = 0
yâ€², yâ€²â€², z â‰¥0, z Ì¸= 0,
respectively. Now apply Theorem 3.3.7.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
195
Answers and solutions to the exercises
3.12 By Theorem 3.3.7, the system is solvable if and only if the dual system
ï£±
ï£²
ï£³
AT(yâ€² âˆ’yâ€²â€²) + z + u = 0
A(w + u) = 0
yâ€², yâ€²â€², z, w, u â‰¥0, u Ì¸= 0
has no solution. It follows from the two equations of the dual system
that:
0 = âˆ’(w + u)TAT = âˆ’(w + u)TAT(yâ€² âˆ’yâ€²â€²) = (w + u)T(z + u)
= wTz + wTu + uTz + uTu,
and all the four terms in the last sum are nonnegative. We conclude
that uTu = 0, and hence u = 0. So the dual system has no solution.
Chapter 4
4.1 a) ext X = {(1, 0), (0, 1)}
b) ext X = {(0, 0), (1, 0), (0, 1), ( 1
2, 1)}
c) ext X = {(0, 0, 1), (0, 0, âˆ’1)} âˆª{(x1, x2, 0) | (x1 âˆ’1)2 + x2
2 = 1} \
{(0, 0, 0)}
4.2 Suppose x âˆˆcvx A \ A; then x = Î»a + (1 âˆ’Î»)y where a âˆˆA, y âˆˆcvx A
and 0 < Î» < 1. It follows that x /âˆˆext(cvx A).
4.3 We have ext X âŠ†A, according to the previous exercise. Suppose that
a âˆˆA \ ext X. Then a = Î»x1 + (1 âˆ’Î»)x2, where x1, x2 âˆˆX, x1 Ì¸= x2
and 0 < Î» < 1. We have xi = Âµia + (1 âˆ’Âµi)yi, where 0 â‰¤Âµi < 1 and
yi âˆˆcvx(A \ {a}). It now follows from the equality
a = (1 âˆ’Î»Âµ1 âˆ’(1 âˆ’Î»)Âµ2)âˆ’1(Î»(1 âˆ’Âµ1)y1 + (1 âˆ’Î»)(1 âˆ’Âµ2)y2),
that a lies in cvx(A \ {a}).Therefore, cvx(A \ {a}) = cvx A = X, which
contradicts the minimality of A. Hence, ext X = A.
4.4 The set X \ {x0} is convex if and only if ]a, b[ âŠ†X \ {x0} for all a, b âˆˆ
X \ {x0}, i.e. if and only if x0 /âˆˆ]a, b[ for all a, b âˆˆX \ {x0}, i.e. if and
only if x0 âˆˆext X.
4.5 E.g. the set in exercise 4.1 c).
4.6 a) Follows directly from Theorem 4.1.3.
b) The extreme point (1, 0) of

x âˆˆR2 | x2 â‰¤

1 âˆ’x2
1 , |x1| â‰¤1

is
not exposed.
4.7 b) A zero-dimensional general face is an extreme point, and a zero-
dimensional exposed face is an exposed point. Hence, exercise 4.6 b)
contains an example of a general face which is not an exposed face.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
196
Answers and solutions to the exercises
c) Suppose that a, b âˆˆX and that the open line segment ]a, b[ intersects
F â€². Since F â€² âŠ†F, the same line segment also intersects F, so it follows
that a, b âˆˆF.
But since F â€² is a general face of F, it follows that
a, b âˆˆF â€². So F â€² is indeed a general face of X.
The set X in exercise 4.6 b) has F = {1}Ã—]âˆ’âˆ, 0] as an exposed face,
and F â€² = {(1, 0)} is an exposed face of F but not of X.
d) Fix a point x0 âˆˆF âˆ©rint C. To each x âˆˆC there is a point y âˆˆC
such that x0 lies on the open line segment ]x, y[, and it now follows
from the deï¬nition of a general face that x âˆˆF.
e) Use the result in d) on the set C = X âˆ©cl F. Since rint C contains
rint F as a subset, F âˆ©rint C Ì¸= âˆ…, so it follows that C âŠ†F. The
converse inclusion is of course trivial.
f) Use the result in d) with F = F1 och C = F2, which gives us the
inclusion F2 âŠ†F1. The converse inclusion is obtained analogously.
g) If F is a general face and F âˆ©rint X Ì¸= âˆ…, then X âŠ†F by d) above.
For faces F Ì¸= X we therefore have F âˆ©rint X = âˆ…, which means that
F âŠ†rbdry X.
Chapter 5
5.1 a) (âˆ’2
3, 4
3), and (4, âˆ’1)
b) (âˆ’2
3, 4
3), (4, âˆ’1), and(âˆ’3, âˆ’1)
c) (0, 0, 0), (2, 0, 0), (0, 2, 0), (0, 0, 4), and (4
3, 4
3, 0)
d) (0, 4, 0, 0), (0, 5
2, 0, 0), (3
2, 5
2, 0, 0), (0, 1, 1, 0), and (0, 5
2, 0, 3
2)
5.2 The extreme rays are generated by (âˆ’2, 4, 3), (1, 1, 0), (4, âˆ’1, 1), and
(1, 0, 0).
5.3 C =
ï£®
ï£°
1 âˆ’2
1
âˆ’1
2
3
âˆ’3
2
5
ï£¹
ï£»
5.4 a) A = {(1, 0), (0, 1)}, B = {(âˆ’2
3, 4
3), (4, âˆ’1)}
b) A = âˆ…, B = {(âˆ’2
3, 4
3), (4, âˆ’1), (âˆ’3, âˆ’1)}
c) A = {(1, 1, âˆ’3), (âˆ’1, âˆ’1, 3), (4, âˆ’7, âˆ’1), (âˆ’7, 4, âˆ’1)},
B = {(0, 0, 0), (2, 0, 0), (0, 2, 0), (0, 0, 4), (4
3, 4
3, 0)}
d) A = âˆ…,
B = {(0, 4, 0, 0), (0, 5
2, 0, 0), (3
2, 5
2, 0, 0), (0, 1, 1, 0), (0, 5
2, 0, 3
2)}.
5.5 The inclusion X = cvx A+con B âŠ†con A+con B = con(AâˆªB) implies
that con X âŠ†con(A âˆªB). Obviously, A âŠ†cvx A âŠ†X. Since cvx A
is a compact set, recc X = con B, so using the assumption 0 âˆˆX, we
obtain the inclusion B âŠ†con B âŠ†X. Thus, A âˆªB âŠ†X, and it follows
that con(A âˆªB) âŠ†con X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
197
Answers and solutions to the exercises
197
Chapter 6
6.1 E.g. f1(x) = x âˆ’|x| and f2(x) = âˆ’x âˆ’|x|.
6.3 a â‰¥5 and a > 5, respectively.
6.4 Use the result of exercise 2.1.
6.5 Follows from f(x) = max(xi1 + xi2 + Â· Â· Â· + xik), where the maximum
is taken over all subsets {i1, i2, . . . , ik} of {1, 2, . . . , n} consisting of k
elements.
6.6 The inequality is trivial if x1 + x2 + Â· Â· Â· + xn = 0, and it is obtained by
adding the n inequalities
f(xi) â‰¤
xi
x1 + Â· Â· Â· + xn
f(x1 + Â· Â· Â· + xn) +

1 âˆ’
xi
x1 + Â· Â· Â· + xn

f(0)
if x1 + Â· Â· Â· + xn > 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
198
Answers and solutions to the exercises
6.7 Choose
c = f(x2) âˆ’f(x1)
âˆ¥x2 âˆ’x1âˆ¥2
(x1 âˆ’x2),
to obtain f(x1) + âŸ¨c, x1âŸ©= f(x2) + âŸ¨c, x2âŸ©. By quasiconvexity,
f(Î»x1 + (1 âˆ’Î»)x2) + âŸ¨c, Î»x1 + (1 âˆ’Î»)x2âŸ©â‰¤f(x1) + âŸ¨c, x1âŸ©,
which simpliï¬es to
f(Î»x1 + (1 âˆ’Î»)x2) â‰¤Î»f(x1) + (1 âˆ’Î»)f(x2).
6.8 Let f : Rn Ã— R â†’R be the function deï¬ned by
f(x, t) =

t
if (x, t) âˆˆC
+âˆ
if (x, t) /âˆˆC.
Then inf{t âˆˆR | (x, t) âˆˆC} = inf{f(x, t) | t âˆˆR}, and Theorem 6.2.6
now follows from Corollary 6.2.7.
6.9 Choose, given x, y âˆˆX, sequences (xk)âˆ
1 , (yk)âˆ
1 of points xk, yk âˆˆint X
such that xk â†’x and yk â†’y as k â†’âˆ. Since the points Î»xk+(1âˆ’Î»)yk
belong to int X,
f(Î»xk + (1 âˆ’Î»)yk) â‰¤Î»f(xk) + (1 âˆ’Î»)f(yk),
and since f is continuous on X, we now obtain the desired inequality
f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y) by passing to the limit.
6.10 Let m = inf{f(x) | x âˆˆrint(dom f)} and ï¬x a relative interior point x0
of dom f. If x âˆˆdom f is arbitrary and 0 < Î» < 1, then Î»x + (1 âˆ’Î»)x0
is a relative interior point of dom f, and it follows that
m â‰¤f(Î»x + (1 âˆ’Î»)x0) â‰¤Î»f(x) + (1 âˆ’Î»)f(x0).
The inequality f(x) â‰¥m now follows by letting Î» â†’1.
6.11 Minimum 8 at x = ( 1
8, 2).
6.12 a) âˆ¥xâˆ¥p
b) max(x1, 0).
Chapter 7
7.2 Yes.
7.5 Let J be a subinterval of I. If f â€²
+(x) â‰¥0 for all x âˆˆJ, then
f(y) âˆ’f(x) â‰¥f â€²
+(x)(y âˆ’x) â‰¥0
for all y > x in the interval J, i.e. f is increasing on J. If instead
f â€²
+(x) â‰¤0 for all x âˆˆJ, then f(y) âˆ’f(x) â‰¥f â€²
+(x)(y âˆ’x) â‰¥0 for all
y < x, i.e. f is decreasing on J.
Since the right derivative f â€²
+(x) is increasing on I, there are three dif-
ferent cases to consider. Either f â€²
+(x) â‰¥0 for all x âˆˆI, and f is then
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
199
Answers and solutions to the exercises
increasing on I, or f â€²
+(x) â‰¤0 for all x âˆˆI, and f is then decreasing
on I, or there is a point c âˆˆI such that f â€²
+(x) â‰¤0 to the left of c and
f â€²
+(x) > 0 to the right of c, and f is in this case decreasing to the left
of c and increasing to the right of c.
7.6 a) The existence of the limits is a consequence of the results of the
previous exercise.
b) Consider the epigraph of the extended function.
7.7 Follows directly from exercise 7.6 b).
7.8 Suppose that f âˆˆF. Let x0 âˆˆRn be an arbitrary point, and consider
the function g(x) = f(x) âˆ’âŸ¨f â€²(x0), x âˆ’x0âŸ©. The function g belongs to
F and gâ€²(x0) = 0. It follows that g(x) â‰¥g(x0) for all x, which means
that f(x) â‰¥f(x0) + âŸ¨f â€²(x0), x âˆ’x0âŸ©for all x. Hence, f is convex by
Theorem 7.2.1.
7.9 Ï†(t) = f(x + tv) = f(x) + tâŸ¨f â€²(x), vâŸ©for v âˆˆVf by Theorem 6.7.1.
Diï¬€erentiate two times to obtain D2f(x)[v, v] = Ï†â€²â€²(0) = 0, with the
conlusion that f â€²â€²(x)v = 0.
7.13 By combining Theorem 7.3.1 (i) with x replaced by Ë†x and v = xâˆ’Ë†x with
the Cauchy-Schwarz inequality, we obtain the inequality Âµâˆ¥x âˆ’Ë†xâˆ¥2 â‰¤
âŸ¨f â€²(x), x âˆ’Ë†xâŸ©â‰¤âˆ¥f â€²(x)âˆ¥âˆ¥x âˆ’Ë†xâˆ¥.
Chapter 8
8.1 Suppose that f is Âµ-strongly convex, where Âµ > 0, and let c be a
subgradient at 0 of the convex function g(x) = f(x) âˆ’1
2Âµâˆ¥xâˆ¥2. Then
f(x) â‰¥f(0) + âŸ¨c, xâŸ©+ 1
2Âµâˆ¥xâˆ¥2 for all x, and the right-hand side tends
to âˆas âˆ¥xâˆ¥â†’âˆ. Alternatively, one could use Theorem 8.1.6.
8.2 The line segment [âˆ’e1, e2], where e1 = (1, 0) and e2 = (0, 1).
8.3 a) B2(0; 1) = {x | âˆ¥xâˆ¥2 â‰¤1}
b) B1(0; 1) = {x | âˆ¥xâˆ¥1 â‰¤1}
c) Bâˆ(0; 1) = {x | âˆ¥xâˆ¥âˆâ‰¤1}.
8.4 a) dom f âˆ—= {a}, f âˆ—(a) = b
b) dom f âˆ—= {x | x < 0}, f âˆ—(x) = âˆ’1 âˆ’ln(âˆ’x)
c) dom f âˆ—= R+, f âˆ—(x) = x ln x âˆ’x, f âˆ—(0) = 0
d) dom f âˆ—= R, f âˆ—(x) = exâˆ’1
e) dom f âˆ—= Râˆ’, f âˆ—(x) = âˆ’2âˆšâˆ’x.
Download free eBooks at bookboon.com

Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
201
Index
Index
aï¬ƒne
combination, 21
dimension, 24
hull, 22
map, 25
piecewise â€”, 115
set, 21
ball
closed â€”, 10
open â€”, 10
bidual cone, 67
boundary, 11
point, 11
relative â€”, 37
bounded set, 13
closed
ball, 10
convex function, 135
halfspace, 29
hull, 12
set, 12
closure, 12
of function, 172
codomain, 3
compact set, 13
concave function, 105
strictly â€”, 110
condition number, 157
cone, 40
bidual â€”, 67
dual â€”, 65
ï¬nitely generated â€”, 45
cone
polyhedral â€”, 43
proper â€”, 42
recession â€”, 47
conic
combination, 42
halfspace, 41
hull, 43
polyhedron, 43
conjugate function, 174
continuous function, 13
convex
combination, 27
function, 105
hull, 34
set, 27
strictly â€” function, 110
derivative, 17, 180
diï¬€erence of sets, 5
diï¬€erentiable, 16
diï¬€erential, 17
dimension, 24
direction derivative, 180
distance, 10
domain, 2
dual cone, 65
eï¬€ective domain, 3
epigraph, 104
Euclidean norm, 10
exposed point, 89
exterior point, 11
extreme point, 77
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
202
Index
extreme ray, 79
face, 79, 89
Farkasâ€™s lemma, 70
Fenchel transform, 174
Fenchelâ€™s inequality, 175
ï¬nitely generated cone, 45
form
linear â€”, 8
quadratic â€”, 8
generator, 43
gradient, 17
halï¬‚ine, 40
halfspace, 29
conic â€”, 41
hessian, 19
hull
aï¬ƒne â€”, 22
conic â€”, 43
convex â€”, 34
hyperplane, 25
separating â€”, 57
supporting â€”, 61
HÂ¨olderâ€™s inequality, 124
image, 3
inverse â€”, 3
indicator function, 174
interior, 11
point, 11
relative â€”, 37
intersection, 2
inverse image, 3
Jensenâ€™s inequality, 111
Karushâ€“Kuhnâ€“Tucker theorem, 185
â„“1-norm, 10
â„“p-norm, 110
lie between, 78
line segment, 7
open â€”, 7
line-free, 51
linear
form, 8
map, 7
operator, 7
Lipschitz
constant, 14
continuous, 14
maximum norm, 10
mean value theorem, 17
Minkowski functional, 140
Minkowskiâ€™s inequality, 126
norm, 10, 109
Euclidean â€”, 10
â„“1 â€”, 10
â„“p â€”, 110
maximum â€”, 10
operator â€”, 14
open
ball, 10
halfspace, 29
line segment, 7
set, 11
operator norm, 14
orthant, 6
perspective, 119
map, 32
piecewise aï¬ƒne, 115
polyhedral cone, 43
polyhedron, 30
conic â€”, 43
positive
deï¬nite, 9
homogeneous, 109
semideï¬nite, 9
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
203
Index
proper
cone, 42
face, 79
quadratic form, 8
quasiconcave, 107
strictly â€”, 110
quasiconvex, 107
strictly â€”, 110
ray, 40
recede, 47
recession
cone, 47
vector, 47
recessive subspace, 51
of a function, 132
relative
boundary, 37
boundary point, 37
interior, 37
interior point, 37
second derivative, 19
seminorm, 109
separating hyperplane, 57
strictly â€”, 57
Slaterâ€™s condition, 186
standard scalar product, 4
strictly
concave, 110
convex, 110
quasiconcave, 110
quasiconvex, 110
strongly convex, 154
subadditive, 109
subdiï¬€erential, 163
subgradient, 163
sublevel set, 104
sum of sets, 5
support function, 137
supporting
hyperplane, 61
line, 148
symmetric linear map, 8
Taylorâ€™s formula, 20
translation, 5
transposed map, 8
union, 2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION â€“ PART I
204
Endnotes
â€ The intersection of an empty family of sets is usually deï¬ned as the entire space,
and using this convention the polyhedron Rn can also be viewed as an intersection of
halfspaces.
â€¡The terminology is not universal. A proper cone is usually called a salient cone, while
the term proper cone is sometimes reserved for cones that are closed, have a nonempty
interior and do not contain any lines through the origin.
â€ The second condition is usually not included in the deï¬nition of separation, but we
have included it in order to force a hyperplane H that separates two subsets of a hyperplane
Hâ€² to be diï¬€erent from Hâ€².
â€ For if X = {x0}, then rint X = {x0}, rbdry X = âˆ…and ext X = {x0}.
â€¡There is an alternative and more general deï¬nition of the face concept, see exercise 4.7.
Our proper faces are called exposed faces by Rockafellar in his standard treatise Convex
Analysis. Every exposed face is also a face according to the alternative deï¬nition, but the
two deï¬nitions are not equivalent, because there are convex sets with faces that are not
exposed.
ENDNOTES
Chapter 1
Chapter 4
Chapter 3
Download free eBooks at bookboon.com

To see Part II, download:
Linear and Convex Optimization
Convexity and Optimization â€“ Part II
Download free eBooks at bookboon.com


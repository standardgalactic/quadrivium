Lars-Åke Lindahl
Convexity
Convexity and Optimization – Part I
Download free books at

ii
 
LARS-ÅKE LINDAHL
CONVEXITY
CONVEXITY AND 
OPTIMIZATION – PART I
Download free eBooks at bookboon.com

iii
Convexity: Convexity and Optimization – Part I
1st edition
© 2016 Lars-Åke Lindahl & bookboon.com
ISBN 978-87-403-1382-6
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
iv
Contents
iv
CONTENTS
	
Preface	
viii
	
List of symbols	
x
1	
Preliminaries	
1
2	
Convex sets	
21
2.1	
Affine sets and affine maps	
21
2.2	
Convex sets	
27
2.3	
Convexity preserving operations	
28
2.4	
Convex hull	
34
2.5	
Topological properties	
36
2.6	
Cones	
40
2.7	
The recession cone	
47
	
Exercises	
55
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
v
Contents
3	
Separation	
57
3.1	
Separating hyperplanes	
57
3.2	
The dual cone	
65
3.3	
Solvability of systems of linear inequalities	
69
	
Exercises	
74
4	
More on convex sets	
77
4.1	
Extreme points and faces	
77
4.2	
Structure theorems for convex sets	
83
	
Exercises	
88
5	
Polyhedra	
90
5.1	
Extreme points and extreme rays	
90
5.2	
Polyhedral cones	
94
5.3	
The internal structure of polyhedra	
96
5.4	
Polyhedron preserving operations	
99
5.5	
Separation	
100
	
Exercises	
103
6	
Convex functions	
104
6.1	
Basic definitions	
104
6.2	
Operations that preserve convexity	
113
6.3	
Maximum and minimum	
119
6.4	
Some important inequalities	
122
6.5	
Solvability of systems of convex inequalities	
126
6.6	
Continuity	
129
6.7	
The recessive subspace of convex functions	
131
6.8	
Closed convex functions	
135
6.9	
The support function	
137
6.10	
The Minkowski functional	
140
	
Exercises	
142
7	
Smooth convex functions	
144
7.1	
Convex functions on R	
144
7.2	
Differentiable convex functions	
151
7.3	
Strong convexity	
153
7.4	
Convex functions with Lipschitz continuous derivatives	
156
	
Exercises	
161
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
vi
Contents
8	
The subdifferential	
163
8.1	
The subdifferential	
163
8.2	
Closed convex functions	
169
8.3	
The conjugate function	
173
8.4	
The direction derivative	
180
8.5	
Subdifferentiation rules	
183
	
Exercises	
188
	
Bibliografical and historical notices	
189
	
References	
190
	
Answers and solutions to the exercises	
192
	
Index	
201
	
Endnotes	
204
	
Part II. Linear and Convex Optimization
9	
Optimization	
Part II
9.1	
Optimization problems	
Part II
9.2	
Classification of optimization problems	
Part II
9.3	
Equivalent problem formulations	
Part II
9.4	
Some model examples	
Part II
10	
The Lagrange function	
Part II
10.1	
The Lagrange function and the dual problem	
Part II
10.2	
John’s theorem	
Part II
11	
Convex optimization	
Part II
11.1	
Strong duality	
Part II
11.2	
The Karush-Kuhn-Tucker theorem	
Part II
11.3	
The Lagrange multipliers	
Part II
12	
Linear programming	
Part II
12.1	
Optimal solutions	
Part II
12.2	
Duality	
Part II
13	
The simplex algorithm	
Part II
13.1	
Standard form	
Part II
13.2	
Informal description of the simplex algorithm	
Part II
13.3	
Basic solutions	
Part II
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
vii
Contents
13.4	
The simplex algorithm	
Part II
13.5	
Bland’s anti cycling rule	
Part II
13.6	
Phase 1 of the simplex algorithm	
Part II
13.7	
Sensitivity analysis	
Part II
13.8	
The dual simplex algorithm	
Part II
13.9	
Complexity	
Part II
	
Part III. Descent and Interior-point Methods
14	
Descent methods	
Part III
14.1	
General principles	
Part III
14.2	
The gradient descent method	
Part III
15	
Newton’s method	
Part III
15.1	
Newton decrement and Newton direction	
Part III
15.2	
Newton’s method	
Part III
15.3	
Equality constraints	
Part III
16	
Self-concordant functions	
Part III
16.1	
Self-concordant functions	
Part III
16.2	
Closed self-concordant functions	
Part III
16.3	
Basic inequalities for the local seminorm	
Part III
16.4	
Minimization	
Part III
16.5	
Newton’s method for self-concordant functions	
Part III
17	
The path-following method	
Part III
17.1	
Barrier and central path	
Part III
17.2	
Path-following methods	
Part III
18	
The path-following method with self-concordant barrier	
Part III
18.1	
Self-concordant barriers	
Part III
18.2	
The path-following method	
Part III
18.3	
LP problems	
Part III
18.4	
Complexity	
Part III
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
viii
Preface
Preface
Mathematical optimization methods are today used routinely as a tool for
economic and industrial planning, in production control and product de-
sign, in civil and military logistics, in medical image analysis, etc., and the
development in the ﬁeld of optimization has been tremendous since World
War II. In 1945, George Stigler studied a diet problem with 77 foods and 9
constraints without being able to determine the optimal diet −today it is
possible to solve optimization problems containing hundreds of thousands of
variables and constraints. There are two factors that have made this pos-
sible −computers and eﬃcient algorithms. It is the rapid development in
the computer area that has been most visible to the common man, but the
algorithm development has also been tremendous during the past 70 years,
and computers would be of little use without eﬃcient algorithms.
Maximization and minimization problems have of course been studied and
solved since the beginning of the mathematical analysis, but optimization
theory in the modern sense started around 1948 with George Dantzig, who
introduced and popularized the concept of linear programming and proposed
an eﬃcient solution algorithm, the simplex algorithm, for such problems.
The type of optimization problems to be discussed by us are problems
that can be formulated as the problem to maximize (or minimize) a given
function over a somehow given subset of Rn.
In order to obtain general
results of interest we need to make some assumptions about the function
and the set, and it is here that convexity enters into the picture. The ﬁrst
part in this series of three on convexity and optimization therefore deals
with ﬁnite dimensional convexity theory. Since convexity plays an important
role in many areas of mathematics, signiﬁcantly more about convexity is
included than is used in the subsequent two parts on optimization, where
Part II provides the basic classical theory for linear and convex optimization,
and Part III describes Newton’s algorithm, self-concordant functions and an
interior point method with self-concordant barriers.
Parts II and III present a number of algorithms, but the emphasis is al-
ways on the mathematical theory, so we do not describe how the algorithms
should be implemented numerically. Anyone who is interested in these im-
portant aspects should consult specialized literature in the ﬁeld.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
ix
Preface
The embryo of this book is a compendium written by Christer Borell and
myself 1978–79, but various additions, deletions and revisions over the years,
have led to a completely diﬀerent text, the most signiﬁcant addition being
Part III.
The presentation in this book is complete in the sense that all theorems
are proved. Some of the proofs are quite technical, but none of them re-
quires more previous knowledge than a good knowledge of linear algebra and
calculus of several variables.
Uppsala, April 2016
Lars-˚
Ake Lindahl
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
x
List of symbols
List of symbols
aﬀX
aﬃne hull of X, p. 22
bdry X
boundary of X, p. 11
cl f
closure of the function f, p. 172
cl X
closure of X, p. 12
con X
conic hull of X, p. 43
cvx X
convex hull of X, p. 34
dim X
dimension of X, p. 24
dom f
the eﬀective domain of f: {x | −∞< f(x) < ∞}, p. 3
epi f
epigraph of f, p. 104
exr X
set of extreme rays of X, p. 79
ext X
set of extreme points of X, p. 77
int X
interior of X, p. 11
lin X
recessive subspace of X, p. 51
rbdry X
relative boundary of X, p. 37
recc X
recession cone of X, p. 47
rint X
relative interior of X, p. 37
sublevα f
α-sublevel set of f, p. 104
ei
ith standard basis vector (0, . . . , 1, . . . , 0), p. 4
f ′
derivate or gradient of f, p. 17
f ′(x; v)
direction derivate of f at x in direction v, p. 180
f ′′
second derivative or hessian of f, p. 19
f ∗
conjugate function of f, p. 173
B(a; r)
open ball centered at a with radius r, p. 10
B(a; r)
closed ball centered at a with radius r, p. 10
Df(a)[v]
diﬀerential of f at a, p. 17
D2f(a)[u, v]
n
i,j=1
∂2f
∂xi∂xj (a)uivj, p. 19
D3f(a)[u, v, w]
n
i,j,k=1
∂3f
∂xi∂xj∂xk (a)uivjwk, p. 20
R+, R++
{x ∈R | x ≥0}, {x ∈R | x > 0}, p. 1
R−
{x ∈R | x ≤0}, p. 1
R, R, R
R ∪{∞}, R ∪{−∞}, R ∪{∞, −∞}, p. 1
SX
support function of X, p. 137
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
xi
List of symbols
Sµ,L(X)
class of µ-strongly convex functions on X with
L-Lipschitz continuous derivative, p. 157
X+
dual cone of X, p. 65
1
the vector (1, 1, . . . , 1), p. 4
∂f(a)
subdiﬀerential of f at a, p. 163
φX
Minkowski functional of X, p. 140
∇f
gradient of f, p. 17
−→x
ray from 0 through x, p. 40
[x, y]
line segment between x and y, p. 7
]x, y[
open line segment between x and y, p. 7
∥·∥1, ∥·∥2, ∥·∥∞
ℓ1-norm, Euclidean norm, maximum norm, p. 10
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
1
Preliminaries
Chapter 1
Preliminaries
The purpose of this chapter is twofold −to explain certain notations and
terminologies used throughout the book and to recall some fundamental con-
cepts and results from calculus and linear algebra.
Real numbers
We use the standard notation R for the set of real numbers, and we let
R+ = {x ∈R | x ≥0},
R−= {x ∈R | x ≤0},
R++ = {x ∈R | x > 0}.
In other words, R+ consists of all nonnegative real numbers, and R++ de-
notes the set of all positive real numbers.
The extended real line
Each nonempty set A of real numbers that is bounded above has a least
upper bound, denoted by sup A, and each nonempty set A that is bounded
below has a greatest lower bound, denoted by inf A. In order to have these
two objects deﬁned for arbitrary subsets of R (and also for other reasons)
we extend the set of real numbers with the two symbols −∞and ∞and
introduce the notation
R = R ∪{∞},
R = R ∪{−∞}
and
R = R ∪{−∞, ∞}.
We furthermore extend the order relation < on R to the extended real
line R by deﬁning, for each real number x,
−∞< x < ∞.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
2
Preliminaries
The arithmetic operations on R are partially extended by the following
”natural” deﬁnitions, where x denotes an arbitrary real number:
x + ∞= ∞+ x = ∞+ ∞= ∞
x + (−∞) = −∞+ x = −∞+ (−∞) = −∞
x · ∞= ∞· x =





∞
if x > 0
0
if x = 0
−∞
if x < 0
x · (−∞) = −∞· x =





−∞
if x > 0
0
if x = 0
∞
if x < 0
∞· ∞= (−∞) · (−∞) = ∞
∞· (−∞) = (−∞) · ∞= −∞.
It is now possible to deﬁne in a consistent way the least upper bound
and the greatest lower bound of an arbitrary subset of the extended real line.
For nonempty sets A which are not bounded above by any real number, we
deﬁne sup A = ∞, and for nonempty sets A which are not bounded below
by any real number we deﬁne inf A = −∞. Finally, for the empty set ∅we
deﬁne inf ∅= ∞and sup ∅= −∞.
Sets and functions
We use standard notation for sets and set operations that are certainly well
known to all readers, but the intersection and the union of an arbitrary family
of sets may be new concepts for some readers.
So let {Xi | i ∈I} be an arbitrary family of sets Xi, indexed by the set
I; their intersection, denoted by

{Xi | i ∈I}
or

i∈I
Xi,
is by deﬁnition the set of elements that belong to all the sets Xi. The union

{Xi | i ∈I}
or

i∈I
Xi
consists of the elements that belong to Xi for at least one i ∈I.
We write f : X →Y to indicate that the function f is deﬁned on the set
X and takes its values in the set Y . The set X is then called the domain
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
3
Preliminaries
3
of the function and Y is called the codomain. Most functions in this book
have domain equal to Rn or to some subset of Rn, and their codomain is
usually R or more generally Rm for some integer m ≥1, but sometimes we
also consider functions whose codomain equals R, R or R.
Let A be a subset of the domain X of the function f. The set
f(A) = {f(x) | x ∈A}
is called the image of A under the function f. If B is a subset of the codomain
of f, then
f −1(B) = {x ∈X | f(x) ∈B}
is called the inverse image of B under f. There is no implication in the
notation f −1(B) that the inverse f −1 exists.
For functions f : X →R we use the notation dom f for the inverse image
of R, i.e.
dom f = {x ∈X | −∞< f(x) < ∞}.
The set dom f thus consists of all x ∈X with ﬁnite function values f(x),
and it is called the eﬀective domain of f.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
4
Preliminaries
The vector space Rn
The reader is assumed to have a solid knowledge of elementary linear algebra
and thus, in particular, to be familiar with basic vector space concepts such
as linear subspace, linear independence, basis and dimension.
As usual, Rn denotes the vector space of all n-tuples (x1, x2, . . . , xn) of
real numbers. The elements of Rn, interchangeably called points and vec-
tors, are denoted by lowercase letters from the beginning or the end of the
alphabet, and if the letters are not numerous enough, we provide them with
sub- or superindices. Subindices are also used to specify the coordinates of
a vector, but there is no risk of confusion, because it will always be clear
from the context whether for instance x1 is a vector of its own or the ﬁrst
coordinate of the vector x.
Vectors in Rn will interchangeably be identiﬁed with column matrices.
Thus, to us
(x1, x2, . . . , xn)
and


x1
x2
...
xn


denote the same object.
The vectors e1, e2, . . . , en in Rn, deﬁned as
e1 = (1, 0, . . . , 0),
e2 = (0, 1, 0, . . . , 0),
. . . ,
en = (0, 0, . . . , 0, 1),
are called the natural basis vectors in Rn, and 1 denotes the vector whose
coordinates are all equal to one, so that
1 = (1, 1, . . . , 1).
The standard scalar product ⟨· , ·⟩on Rn is deﬁned by the formula
⟨x, y⟩= x1y1 + x2y2 + · · · + xnyn,
and, using matrix multiplication, we can write this as
⟨x, y⟩= xTy = yTx,
where T denotes transposition. In general, AT denotes the transpose of the
matrix A.
The solution set to a homogeneous system of linear equations in n un-
knowns is a linear subspace of Rn. Conversely, every linear subspace of Rn
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
5
Preliminaries
can be presented as the solution set to some homogeneous system of linear
equations:









a11x1 + a12x2 + · · · + a1nxn = 0
a21x1 + a22x2 + · · · + a2nxn = 0
...
am1x1 + am2x2 + · · · + amnxn = 0
Using matrices we can of course write the system above in a more compact
form as
Ax = 0,
where the matrix A is called the coeﬃcient matrix of the system.
The dimension of the solution set of the above system is given by the
number n −r, where r equals the rank of the matrix A. Thus in particular,
for each linear subspace X of Rn of dimension n −1 there exists a nonzero
vector c = (c1, c2, . . . , cn) such that
X = {x ∈Rn | c1x1 + c2x2 + · · · + cnxn = 0}.
Sum of sets
If X and Y are nonempty subsets of Rn and α is a real number, we let
X + Y = {x + y | x ∈X, y ∈Y },
X −Y = {x −y | x ∈X, y ∈Y },
αX = {αx | x ∈X}.
The set X + Y is called the (vector) sum of X and Y , X −Y is the (vector)
diﬀerence and αX is the product of the number α and the set X.
It is convenient to have sums, diﬀerences and products deﬁned for the
empty set ∅, too. Therefore, we extend the above deﬁnitions by deﬁning
X ± ∅= ∅± X = ∅
for all sets X, and
α∅= ∅.
For singleton sets {a} we write a + X instead of {a} + X, and the set
a + X is called a translation of X.
It is now easy to verify that the following rules hold for arbitrary sets X,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
6
Preliminaries
6
Y and Z and arbitrary real numbers α and β:
X + Y = Y + X
(X + Y ) + Z = X + (Y + Z)
αX + αY = α(X + Y )
(α + β)X ⊆αX + βX .
In connection with the last inclusion one should note that the converse
inclusion αX + βX ⊆(α + β)X does not hold for general sets X.
Inequalites in Rn
For vectors x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) in Rn we write x ≥y
if xj ≥yj for all indices j, and we write x > y if xj > yj for all j. In
particular, x ≥0 means that all coordinates of x are nonnegative.
The set
Rn
+ = R+ × R+ × · · · × R+ = {x ∈Rn | x ≥0}
is called the nonnegative orthant of Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
7
Preliminaries
The order relation ≥is a partial order on Rn. It is thus, in other words,
reﬂexive (x ≥x for all x), transitive (x ≥y & y ≥z ⇒x ≥z) and
antisymmetric (x ≥y & y ≥x ⇒x = y). However, the order is not a
complete order when n > 1, since two vectors x and y may be unrelated.
Two important properties, which will be used now and then, are given
by the following two trivial implications:
x ≥0 & y ≥0 ⇒⟨x, y⟩≥0
x ≥0 & y ≥0 & ⟨x, y⟩= 0 ⇒x = y = 0.
Line segments
Let x and y be points in Rn. We deﬁne
[x, y] = {(1 −λ)x + λy | 0 ≤λ ≤1}
and
]x, y[ = {(1 −λ)x + λy | 0 < λ < 1},
and we call the set [x, y] the line segment and the set ]x, y[ the open line
segment between x and y, if the two points are distinct. If the two points
coincide, i.e. if y = x, then obviously [x, x] =]x, x[= {x}.
Linear maps and linear forms
Let us recall that a map S : Rn →Rm is called linear if
S(αx + βy) = αSx + βSy
for all vectors x, y ∈Rn and all scalars (i.e. real numbers) α, β. A linear
map S : Rn →Rn is also called a linear operator on Rn.
Each linear map S : Rn →Rm gives rise to a unique m × n-matrix ˜S
such that
Sx = ˜Sx,
which means that the function value Sx of the map S at x is given by
the matrixproduct ˜Sx. (Remember that vectors are identiﬁed with column
matrices!) For this reason, the same letter will be used to denote a map and
its matrix. We thus interchangeably consider Sx as the value of a map and
as a matrix product.
By computing the scalar product ⟨x, Sy⟩as a matrix product we obtain
the following relation
⟨x, Sy⟩= xTSy = (STx)Ty = ⟨STx, y⟩
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
8
Preliminaries
between a linear map S : Rn →Rm (or m × n-matrix S) and its transposed
map ST : Rm →Rn (or transposed matrix ST).
An n × n-matrix A = [aij], and the corresponding linear map, is called
symmetric if AT = A, i.e. if aij = aji for all indices i, j.
A linear map f : Rn →R with codomain R is called a linear form.
A
linear form on Rn is thus of the form
f(x) = c1x1 + c2x2 + · · · + cnxn,
where c = (c1, c2, . . . , cn) is a vector in Rn. Using the standard scalar product
we can write this more simply as
f(x) = ⟨c, x⟩,
and in matrix notation this becomes
f(x) = cTx.
Let f(x) = ⟨c, y⟩be a linear form on Rm and let S : Rn →Rm be a
linear map with codomain Rm. The composition f ◦S is then a linear form
on Rn, and we conclude that there exists a unique vector d ∈Rn such that
(f ◦S)(x) = ⟨d, x⟩for all x ∈Rn. Since f(Sx) = ⟨c, Sx⟩= ⟨STc, x⟩, it
follows that d = STc.
Quadratic forms
A function q: Rn →R is called a quadratic form if there exists a symmetric
n × n-matrix Q = [qij] such that
q(x) =
n

i,j=1
qijxixj,
or equivalently
q(x) = ⟨x, Qx⟩= xTQx.
The quadratic form q determines the symmetric matrix Q uniquely, and this
allows us to identify the form q with its matrix (or operator) Q.
An arbitrary quadratic polynomial p(x) in n variables can now be written
in the form
p(x) = ⟨x, Ax⟩+ ⟨b, x⟩+ c,
where x →⟨x, Ax⟩is a quadratic form determined by a symmetric operator
(or matrix) A, x →⟨b, x⟩is a linear form determined by a vector b, and c is
a real number.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
9
Preliminaries
9
Example. In order to write the quadratic polynomial
p(x1, x2, x3) = x2
1 + 4x1x2 −2x1x3 + 5x2
2 + 6x2x3 + 3x1 + 2x3 + 2
in this form we ﬁrst replace the terms dxixj for i < j with 1
2dxixj + 1
2dxjxi.
This yields
p(x1, x2, x3) = (x2
1 + 2x1x2 −x1x3 + 2x2x1 + 5x2
2 + 3x2x3 −x3x1 + 3x3x2)
+ (3x1 + 2x3) + 2 = ⟨x, Ax⟩+ ⟨b, x⟩+ c
with A =


1
2
−1
2
5
3
−1
3
0

, b =


3
0
2

and c = 2.
A quadratic form q on Rn (and the corresponding symmetric operator
and matrix) is called positive semideﬁnite if q(x) ≥0 and positive deﬁnite if
q(x) > 0 for all vectors x ̸= 0 in Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
10
Preliminaries
Norms and balls
A norm ∥·∥on Rn is a function Rn →R+ that satisﬁes the following three
conditions:
∥x + y∥≤∥x∥+ ∥y∥
for all x, y
(i)
∥λx∥= |λ| ∥x∥
for all x ∈Rn, λ ∈R
(ii)
∥x∥= 0 ⇔x = 0.
(iii)
The most important norm to us is the Euclidean norm, deﬁned via the
standard scalar product as
∥x∥=

⟨x, x⟩=

x2
1 + x2
2 + · · · + x2
n.
This is the norm that we use unless the contrary is stated explicitely. We
use the notation ∥·∥2 for the Euclidean norm whenever we for some reason
have to emphasize that the norm in question is the Euclidean one.
Other norms, that will occur now and then, are the maximum norm
∥x∥∞= max
1≤i≤n |xi|,
and the ℓ1-norm
∥x∥1 =
n

i=1
|xi|.
It is easily veriﬁed that these really are norms, that is that conditions (i)–(iii)
are satisﬁed.
All norms on Rn are equivalent in the following sense: If ∥·∥and ∥·∥′ are
two norms, then there exist two positive constants c and C such that
c∥x∥′ ≤∥x∥≤C∥x∥′
for all x ∈Rn.
For example, ∥x∥∞≤∥x∥2 ≤√n ∥x∥∞.
Given an arbitrary norm ∥·∥we deﬁne the corresponding distance between
two points x and a in Rn as ∥x −a∥. The set
B(a; r) = {x ∈Rn | ∥x −a∥< r},
consisting of all points x whose distance to a is less than r, is called the open
ball centered at the point a and with radius r. Of course, we have to have
r > 0 in order to get a nonempty ball. The set
B(a; r) = {x ∈Rn | ∥x −a∥≤r}
is the corresponding closed ball.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
11
Preliminaries
The geometric shape of the balls depends on the underlying norm. The
ball B(0; 1) in R2 is a square with corners at the points (±1, ±1) when the
norm is the maximum norm, it is a square with corners at the points (±1, 0)
and (0, ±1) when the norm is the ℓ1-norm, and it is the unit disc when the
norm is the Euclidean one.
If B denotes balls deﬁned by one norm and B′ denotes balls deﬁned by a
second norm, then there are positive constants c and C such that
(1.1)
B′(a; cr) ⊆B(a; r) ⊆B′(a; Cr)
for all a ∈Rn and all r > 0. This follows easily from the equivalence of the
two norms.
All balls that occur in the sequel are assumed to be Euclidean, i.e. deﬁned
with respect to the Euclidean norm, unless otherwise stated.
Topological concepts
We now use balls to deﬁne a number of topological concepts. Let X be an
arbitrary subset of Rn. A point a ∈Rn is called
• an interior point of X if there exists an r > 0 such that B(a; r) ⊆X;
• a boundary point of X if X ∩B(a; r) ̸= ∅and ∁X ∩B(a; r) ̸= ∅for all
r > 0;
• an exterior point of X if there exists an r > 0 such that X∩B(a; r) = ∅.
Observe that because of property (1.1), the above concepts do not depend
on the kind of balls that we use.
A point is obviously either an interior point, a boundary point or an
exterior point of X. Interior points belong to X, exterior points belong to
the complement of X, while boundary points may belong to X but must not
do so. Exterior points of X are interior points of the complement ∁X, and
vice versa, and the two sets X and ∁X have the same boundary points.
The set of all interior points of X is called the interior of X and is denoted
by int X. The set of all boundary points is called the boundary of X and is
denoted by bdry X.
A set X is called open if all points in X are interior points, i.e. if int X =
X.
It is easy to verify that the union of an arbitrary family of open sets is
an open set and that the intersection of ﬁnitely many open sets is an open
set. The empty set ∅and Rn are open sets.
The interior int X is a (possibly empty) open set for each set X, and
int X is the biggest open set that is included in X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
12
Preliminaries
12
A set X is called closed if its complement ∁X is an open set. It follows
that X is closed if and only if X contains all its boundary points, i.e. if and
only if bdry X ⊆X.
The intersection of an arbitrary family of closed sets is closed, the union
of ﬁnitely many closed sets is closed, and Rn and ∅are closed sets.
For arbitrary sets X we set
cl X = X ∪bdry X.
The set cl X is then a closed set that contains X, and it is called the closure
(or closed hull) of X. The closure cl X is the smallest closed set that contains
X as a subset.
For example, if r > 0 then
cl B(a; r) = {x ∈Rn | ∥x −a∥≤r} = B(a; r),
which makes it consistent to call the set B(a; r) a closed ball.
For nonempty subsets X of Rn and numbers r > 0 we deﬁne
X(r) = {y ∈Rn | ∃x ∈X : ∥y −x∥< r}.
The set X(r) thus consists of all points whose distance to X is less than r.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
13
Preliminaries
A point x is an exterior point of X if and only if the distance from x to
X is positive, i.e. if and only if there is an r > 0 such that x /∈X(r). This
means that a point x belongs to the closure cl X, i.e. x is an interior point
or a boundary point of X, if and only if x belongs to the sets X(r) for all
r > 0. In other words,
cl X =

r>0
X(r).
A set X is said to be bounded if it is contained in some ball centered at
0, i.e. if there is a number R > 0 such that X ⊆B(0; R).
A set X that is both closed and bounded is called compact.
An important property of compact subsets X of Rn is given by the
Bolzano–Weierstrass theorem: Every inﬁnite sequence (xn)∞
n=1 of points xn
in a compact set X has a subsequence (xnk)∞
k=1 that converges to a point in
X.
The cartesian product X×Y of a compact subset X of Rm and a compact
subset Y of Rn is a compact subset of Rm × Rn
(= Rm+n).
Continuity
A function f : X →Rm, whose domain X is a subset of Rn, is deﬁned to be
continuous at the point a ∈X if for each ϵ > 0 there exists an r > 0 such
that
f(X ∩B(a; r)) ⊆B(f(a); ϵ).
(Here, of course, the left B stands for balls in Rn and the right B stands
for balls in Rm.) The function is said to be continuous on X, or simply
continuous, if it is continuous at all points a ∈X.
The inverse image f −1(I) of an open interval under a continuous function
f : Rn →R is an open set in Rn. In particular, the sets {x | f(x) < a} and
{x | f(x) > a}, i.e. the sets f −1(]−∞, a[) and f −1(]a, ∞[), are open for all
a ∈R. Their complements, the sets {x | f(x) ≥a} and {x | f(x) ≤a}, are
thus closed.
Sums and (scalar) products of continuous functions are continuous, and
quotients of real-valued continuous functions are continuous at all points
where the quotients are well-deﬁned. Compositions of continuous functions
are continuous.
Compactness is preserved under continuous functions, that is the image
f(X) is compact if X is a compact subset of the domain of the continuous
function f. For continuous functions f with codomain R this means that
f is bounded on X and has a maximum and a minimum, i.e. there are two
points x1, x2 ∈X such that f(x1) ≤f(x) ≤f(x2) for all x ∈X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
14
Preliminaries
Lipschitz continuity
A function f : X →Rm that is deﬁned on a subset X of Rn, is called
Lipschitz continuous with Lipschitz constant L if
∥f(y) −f(x)∥≤L∥y −x∥
for all x, y ∈X.
Note that the deﬁnition of Lipschitz continuity is norm independent, since
all norms on Rn are equivalent, but the value of the Lipschitz constant L is
obviously norm dependent.
Operator norms
Let ∥·∥be a given norm on Rn. Since the closed unit ball is compact and
linear operators S on Rn are continuous, we get a ﬁnite number ∥S∥, called
the operator norm, by the deﬁnition
∥S∥= sup
∥x∥≤1
∥Sx∥.
That the operator norm really is a norm on the space of linear opera-
tors, i.e. that it satisﬁes conditions (i)–(iii) in the norm deﬁnition, follows
immediately from the corresponding properties of the underlying norm on
Rn.
By deﬁnition, S(x/∥x∥) ≤∥S∥for all x ̸= 0, and consequently
∥Sx∥≤∥S∥∥x∥
for all x ∈Rn.
From this inequality follows immediately that
∥STx∥≤∥S∥∥Tx∥≤∥S∥∥T∥∥x∥,
which gives us the important inequality
∥ST∥≤∥S∥∥T∥
for the norm of a product of two operators.
The identity operator I on Rn clearly has norm equal to 1. Therefore,
if the operator S is invertible, then, by choosing T = S−1 in the above
inequality, we obtain the inequality
∥S−1∥≥1/∥S∥.
The operator norm obviously depends on the underlying norm on Rn,
but again, diﬀerent norms on Rn give rise to equivalent norms on the space
of operators. However, when speaking about the operator norm we shall in
this book always assume that the underlying norm is the Euclidean norm
even if this is not stated explicitely.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
15
Preliminaries
15
Symmetric operators, eigenvalues and norms
Every symmetric operator S on Rn is diagonizable according to the spectral
theorem. This means that there is an ON-basis e1, e2, . . . , en consisting of
eigenvectors of S. Let λ1, λ2, . . . , λn denote the corresponding eigenvalues.
The largest and the smallest eigenvalue λmax and λmin are obtained as
maximum and minimum values, respectively, of the quadratic form ⟨x, Sx⟩
on the unit sphere ∥x∥= 1:
λmax = max
∥x∥=1⟨x, Sx⟩
and
λmin = min
∥x∥=1⟨x, Sx⟩.
For, by using the expansion x = n
i=1 ξiei of x in the ON-basis of eigen-
vectors, we obtain the inequality
⟨x, Sx⟩=
n

i=1
λiξ2
i ≤λmax
n

i=1
ξ2
i = λmax∥x∥2,
and equality prevails when x is equal to the eigenvector ei that corresponds
to the eigenvalue λmax. An analogous inequality in the other direction holds
for λmin, of course.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
16
Preliminaries
The operator norm (with respect to the Euclidean norm) moreover satis-
ﬁes the equality
∥S∥= max
1≤i≤n |λi| = max{|λmax|, |λmin|}.
For, by using the above expansion of x, we have Sx = n
i=1 λiξiei, and
consequently
∥Sx∥2 =
n

i=1
λ2
i ξ2
i ≤max
1≤i≤n |λi|2
n

i=1
ξ2
i = ( max
1≤i≤n |λi|)2 ∥x∥2,
with equality when x is the eigenvector that corresponds to maxi |λi|.
If all eigenvalues of the symmetric operator S are nonzero, then S is in-
vertible, and the inverse S−1 is symmetric with eigenvalues λ−1
1 , λ−1
2 , . . . , λ−1
n .
The norm of the inverse is given by
∥S−1∥= 1/ min
1≤i≤n |λi|.
A symmetric operator S is positive semideﬁnite if all its eigenvalues are
nonnegative, and it is positive deﬁnite if all eigenvalues are positive. Hence,
if S is positive deﬁnite, then
∥S∥= λmax
and
∥S−1∥= 1/λmin.
It follows easily from the diagonizability of symmetric operators on Rn
that every positive semideﬁnite symmetric operator S has a unique positive
semideﬁnite symmetric square root S1/2. Moreover, since
⟨x, Sx⟩= ⟨x, S1/2(S1/2x)⟩= ⟨S1/2x, S1/2x⟩= ∥S1/2x∥
we conclude that the two operators S and S1/2 have the same null space
N(S) and that
N(S) = {x ∈Rn | Sx = 0} = {x ∈Rn | ⟨x, Sx⟩= 0}.
Diﬀerentiability
A function f : U →R, which is deﬁned on an open subset U of Rn, is called
diﬀerentiable at the point a ∈U if the partial derivatives
∂f
∂xi exist at the
point x and the equality
(1.2)
f(a + v) = f(a) +
n

i=1
∂f
∂xi
(a) vi + r(v)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
17
Preliminaries
holds for all v in some neighborhood of the origin with a remainder term r(v)
that satisﬁes the condition
lim
v→0
r(v)
∥v∥= 0.
The linear form Df(a)[v], deﬁned by
Df(a)[v] =
n

i=1
∂f
∂xi
(a) vi,
is called the diﬀerential of the function f at the point a. The coeﬃcient
vector
 ∂f
∂x1
(a), ∂f
∂x2
(a), . . . , ∂f
∂xn
(a)

of the diﬀerential is called the derivative or the gradient of f at the point a
and is denoted by f ′(a) or ∇f(a). We shall mostly use the ﬁrst mentioned
notation.
The equation (1.2) can now be written in a compact form as
f(a + v) = f(a) + Df(a)[v] + r(v),
with
Df(a)[v] = ⟨f ′(a), v⟩.
A function f : U →R is called diﬀerentiable (on U) if it is diﬀerentiable
at each point in U. In particular, this implies that U is an open set.
For functions of one variable, diﬀerentiability is clearly equivalent to the
existence of the derivative, but for functions of several variables, the mere
existence of the partial derivatives is no longer a guarantee for diﬀerentiabil-
ity. However, if a function f has partial derivatives and these are continous
on an open set U, then f is diﬀerentiable on U.
The Mean Value Theorem
Suppose f : U →R is a diﬀerentiable function and that the line segment
[a, a + v] lies in U. Let φ(t) = f(a + tv). The function φ is then deﬁned and
diﬀerentiable on the interval [0, 1] with derivative
φ′(t) = Df(a + tv)[v] = ⟨f ′(a + tv), v⟩.
This is a special case of the chain rule but also follows easily from the deﬁni-
tion of the derivative. By the usual mean value theorem for functions of one
variable, there is a number s ∈]0, 1[ such that φ(1) −φ(0) = φ′(s)(1 −0).
Since φ(1) = f(a + v), φ(0) = f(a) and a + sv is a point on the open line
segment ]a, a + v[, we have now deduced the following mean value theorem
for functions of several variables.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
18
Preliminaries
18
Theorem 1.1.1. Suppose the function f : U →R is diﬀerentiable and that
the line segment [a, a + v] lies in U. Then there is a point c ∈]a, a + v[ such
that
f(a + v) = f(a) + Df(c)[v].
Functions with Lipschitz continuous derivative
We shall sometimes need more precise information about the remainder term
r(v) in equation (1.2) than what follows from the deﬁnition of diﬀerentiabil-
ity. We have the following result for functions with a Lipschitz continuous
derivative.
Theorem 1.1.2. Suppose the function f : U →R is diﬀerentiable, that its
derivative is Lipschitz continuous, i.e. that ∥f ′(y) −f ′(x)∥≤L∥y −x∥for
all x, y ∈U, and that the line segment [a, a + v] lies in U. Then
|f(a + v) −f(a) −Df(a)[v]| ≤L
2 ∥v∥2.
Proof. Deﬁne the function Φ on the interval [0, 1] by
Φ(t) = f(a + tv) −t Df(a)[v].
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
19
Preliminaries
Then Φ is diﬀerentiable with derivative
Φ′(t) = Df(a + tv)[v] −Df(a)[v] = ⟨f ′(a + tv) −f ′(a), v⟩,
and by using the Cauchy–Schwarz inequality and the Lipschitz continuity,
we obtain the inequality
|Φ′(t)| ≤∥f ′(a + tv) −f ′(a)∥· ∥v∥≤Lt ∥v∥2.
Since f(a + v) −f(a) −Df(a)[v] = Φ(1) −Φ(0) =
 1
0 Φ′(t) dt, it now follows
that
|f(a + v) −f(a) −Df(a)[v]| ≤
 1
0
|Φ′(t)| dt ≤L∥v∥2
 1
0
t dt = L
2 ∥v∥2.
Two times diﬀerentiable functions
If the function f together with all its partial derivatives ∂f
∂xi are diﬀerentiable
on U, then f is said to be two times diﬀerentiable on U. The mixed partial
second derivatives are then automatically equal, i.e.
∂2f
∂xi∂xj
(a) =
∂2f
∂xj∂xi
(a)
for all i, j and all a ∈U.
A suﬃcient condition for the function f to be two times diﬀerentiable on
U is that all partial derivatives of order up to two exist and are continuous
on U.
If f : U →R is a two times diﬀerentiable function and a is a point in U,
we deﬁne a symmetric bilinear form D2f(a)[u, v] on Rn by
D2f(a)[u, v] =
n

i,j=1
∂2f
∂xi∂xj
(a)uivj,
u, v ∈Rn.
The corresponding symmetric linear operator is called the second derivative
of f at the point a and it is denoted by f ′′(a). The matrix of the second
derivative, i.e. the matrix
 ∂2f
∂xi∂xj
(a)
n
i,j=1,
is called the hessian of f (at the point a). Since we do not distinguish between
matrices and operators, we also denote the hessian by f ′′(a).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
20
Preliminaries
The above symmetric bilinear form can now be expressed in the form
D2f(a)[u, v] = ⟨u, f ′′(a)v⟩= uTf ′′(a)v,
depending on whether we interpret the second derivative as an operator or
as a matrix.
Let us recall Taylor’s formula, which reads as follows for two times dif-
ferentiable functions.
Theorem 1.1.3. Suppose the function f is two times diﬀerentiable in a neigh-
borhood of the point a. Then
f(a + v) = f(a) + Df(a)[v] + 1
2D2f(a)[v, v] + r(v)
with a remainder term that satisﬁes lim
v→0 r(v)/∥v∥2 = 0.
Three times diﬀerentiable functions
To deﬁne self-concordance we also need to consider functions that are three
times diﬀerentiable on some open subset U of Rn.
For such functions f
and points a ∈U we deﬁne a trilinear form D3f(a)[u, v, w] in the vectors
u, v, w ∈Rn by
D3f(a)[u, v, w] =
n

i,j,k=1
∂3f
∂xi∂xj∂xk
(a)uivjwk.
We leave to the reader to formulate Taylor’s formula for functions that
are three times diﬀerentiable. We have the following diﬀerentiation rules,
which follow from the chain rule and will be used several times in the ﬁnal
chapters:
d
dtf(x + tv) = Df(x + tv)[v]
d
dt

Df(x + tv)[u]

= D2f(x + tv)[u, v],
d
dt

D2f(x + tw)[u, v]

= D3f(x + tw)[u, v, w].
As a consequence we get the following expressions for the derivatives of
the restriction φ of the function f to the line through the point x with the
direction given by v:
φ(t) = f(x + tv),
φ′(t) = Df(x + tv)[v],
φ′′(t) = D2f(x + tv)[v, v],
φ′′′(t) = D3f(x + tv)[v, v, v].
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
21
Convex sets
Chapter 2
Convex sets
2.1
Aﬃne sets and aﬃne maps
Aﬃne sets
Deﬁnition. A subset of Rn is called aﬃne if for each pair of distinct points
in the set it contains the entire line through the points.
Thus, a set X is aﬃne if and only if
x, y ∈X, λ ∈R ⇒λx + (1 −λ)y ∈X.
The empty set ∅, the entire space Rn, linear subspaces of Rn, singleton
sets {x} and lines are examples of aﬃne sets.
Deﬁnition. A linear combination y = m
j=1 αjxj of vectors x1, x2, . . . , xm is
called an aﬃne combination if m
j=1 αj = 1.
Theorem 2.1.1. An aﬃne set contains all aﬃne combination of its elements.
Proof. We prove the theorem by induction on the number of elements in the
aﬃne combination. So let X be an aﬃne set. An aﬃne combination of one
element is the element itself. Hence, X contains all aﬃne combinations that
can be formed by one element in the set.
Now assume inductively that X contains all aﬃne combinations that can
be formed out of m −1 elements from X, where m ≥2, and consider an
arbitrary aﬃne combination x = m
j=1 αjxj of m elements x1, x2, . . . , xm in
X. Since m
j=1 αj = 1, at least one coeﬃcient αj must be diﬀerent from 1;
assume without loss of generality that αm ̸= 1, and let s = 1−αm = m−1
j=1 αj.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
22
Convex sets
Then s ̸= 0 and m−1
j=1 αj/s = 1, which means that the element
y =
m−1

j=1
αj
s xj
is an aﬃne combination of m −1 elements in X. Therefore, y belongs to X,
by the induction assumption. But x = sy+(1−s)xm, and it now follows from
the deﬁnition of aﬃne sets that x lies in X. This completes the induction
step, and the theorem is proved.
Deﬁnition. Let A be an arbitrary nonempty subset of Rn. The set of all aﬃne
combinations λ1a1 + λ2a2 + · · · + λmam that can be formed of an arbitrary
number of elements a1, a2, . . . , am from A, is called the aﬃne hull of A and
is denoted by aﬀA .
In order to have the aﬃne hull deﬁned also for the empty set, we put
aﬀ∅= ∅.
Theorem 2.1.2. The aﬃne hull aﬀA is an aﬃne set containing A as a
subset, and it is the smallest aﬃne subset with this property, i.e. if the set X
is aﬃne and A ⊆X, then aﬀA ⊆X.
Proof. The set aﬀA is an aﬃne set, because any aﬃne combination of two
elements in aﬀA is obviously an aﬃne combination of elements from A,
and the set A is a subset of its aﬃne hull, since any element is an aﬃne
combination of itself.
If X is an aﬃne set, then aﬀX ⊆X, by Theorem 2.1.1, and if A ⊆X,
then obviously aﬀA ⊆aﬀX. Thus, aﬀA ⊆X whenever X is an aﬃne set
and A is a subset of X.
Characterisation of aﬃne sets
Nonempty aﬃne sets are translations of linear subspaces. More precisely, we
have the following theorem.
Theorem 2.1.3. If X is an aﬃne subset of Rn and a ∈X, then −a+X is a
linear subspace of Rn. Moreover, for each b ∈X we have −b+X = −a+X.
Thus, to each nonempty aﬃne set X there corresponds a uniquely deﬁned
linear subspace U such that X = a + U.
Proof. Let U = −a + X. If u1 = −a + x1 and u2 = −a + x2 are two elements
in U and α1, α2 are arbitrary real numbers, then the linear combination
α1u1 + α2u2 = −a + (1 −α1 −α2)a + α1x1 + α2x2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
23
Convex sets
23
a
X
0
U = −a + X
Figure 2.1. Illustration for Theorem 2.1.3: An aﬃne
set X and the corresponding linear subspace U.
is an element in U, because (1−α1−α2)a+α1x1+α2x2 is an aﬃne combination
of elements in X and hence belongs to X, according to Theorem 2.1.1. This
proves that U is a linear subspace.
Now assume that b ∈X, and let v = −b + x be an arbitrary element in
−b + X. By writing v as v = −a + (a −b + x) we see that v belongs to
−a + X, too, because a −b + x is an aﬃne combination of elements in X.
This proves the inclusion −b + X ⊆−a + X. The converse inclusion follows
by symmetry. Thus, −a + X = −b + X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
24
Convex sets
Dimension
The following deﬁnition is justiﬁed by Theorem 2.1.3.
Deﬁnition. The dimension dim X of a nonempty aﬃne set X is deﬁned as
the dimension of the linear subspace −a+X, where a is an arbitrary element
in X.
Since every nonempty aﬃne set has a well-deﬁned dimension, we can
extend the dimension concept to arbitrary nonempty sets as follows.
Deﬁnition. The (aﬃne) dimension dim A of a nonempty subset A of Rn is
deﬁned to be the dimension of its aﬃne hull aﬀA.
The dimension of an open ball B(a; r) in Rn is n, and the dimension of
a line segment [x, y] is 1.
The dimension is invariant under translation i.e. if A is a nonempty subset
of Rn and a ∈Rn then
dim(a + A) = dim A,
and it is increasing in the following sense:
A ⊆B ⇒dim A ≤dim B.
Aﬃne sets as solutions to systems of linear equations
Our next theorem gives a complete description of the aﬃne subsets of Rn.
Theorem 2.1.4. Every aﬃne subset of Rn is the solution set of a system of
linear equations









c11x1 + c12x2 + · · · + c1nxn = b1
c21x1 + c22x2 + · · · + c2nxn = b2
...
cm1x1 + cm2x2 + · · · + cmnxn = bm
and conversely. The dimension of a nonempty solution set equals n−r, where
r is the rank of the coeﬃcient matrix C.
Proof. The empty aﬃne set is obtained as the solution set of an inconsistent
system. Therefore, we only have to consider nonempty aﬃne sets X, and
these are of the form X = x0 + U, where x0 belongs to X and U is a linear
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
25
Convex sets
subspace of Rn. But each linear subspace is the solution set of a homogeneous
system of linear equations. Hence there exists a matrix C such that
U = {x | Cx = 0},
and dim U = n −rank C. With b = Cx0 it follows that x ∈X if and only
if Cx −Cx0 = C(x −x0) = 0, i.e. if and only if x is a solution to the linear
system Cx = b.
Conversely, if x0 is a solution to the above linear system so that Cx0 = b,
then x is a solution to the same system if and only if the vector z = x −x0
belongs to the solution set U of the homogeneous equation system Cz = 0.
It follows that the solution set of the equation system Cx = b is of the form
x0 + U, i.e. it is an aﬃne set.
Hyperplanes
Deﬁnition. Aﬃne subsets of Rn of dimension n −1 are called hyperplanes.
Theorem 2.1.4 has the following corollary:
Corollary 2.1.5. A subset X of Rn is a hyperplane if and only if there exist
a nonzero vector c = (c1, c2, . . . , cn) and a real number b so that
X = {x ∈Rn | ⟨c, x⟩= b}.
It follows from Theorem 2.1.4 that every aﬃne proper subset of Rn can
be expressed as an intersection of hyperplanes.
Aﬃne maps
Deﬁnition. Let X be an aﬃne subset of Rn. A map T : X →Rm is called
aﬃne if
T(λx + (1 −λ)y) = λTx + (1 −λ)Ty
for all x, y ∈X and all λ ∈R.
Using induction, it is easy to prove that if T : X →Rm is an aﬃne map
and x = α1x1 + α2x2 + · · · + αmxm is an aﬃne combination of elements in
X, then
Tx = α1Tx1 + α2Tx2 + · · · + αmTxm.
Moreover, the image T(Y ) of an aﬃne subset Y of X is an aﬃne subset of
Rm, and the inverse image T −1(Z) of an aﬃne subset Z of Rm is an aﬃne
subset of X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
26
Convex sets
26
The composition of two aﬃne maps is aﬃne. In particular, a linear map
followed by a translation is an aﬃne map, and our next theorem shows that
each aﬃne map can be written as such a composition.
Theorem 2.1.6. Let X be an aﬃne subset of Rn, and suppose the map
T : X →Rm is aﬃne. Then there exist a linear map C : Rn →Rm and
a vector v in Rm so that Tx = Cx + v for all x ∈X.
Proof. Write the domain of T in the form X = x0 + U with x0 ∈X and U
as a linear subspace of Rn, and deﬁne the map C on the subspace U by
Cu = T(x0 + u) −Tx0.
Then, for each u1, u2 ∈U and α1, α2 ∈R we have
C(α1u1 + α2u2) = T(x0 + α1u1 + α2u2) −Tx0
= T

α1(x0 + u1) + α2(x0 + u2) + (1 −α1 −α2)x0

−Tx0
= α1T(x0 + u1) + α2T(x0 + u2) + (1 −α1 −α2)Tx0 −Tx0
= α1

T(x0 + u1) −Tx0

+ α2

T(x0 + u2) −Tx0

= α1Cu1 + α2Cu2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
27
Convex sets
So the map C is linear on U and it can, of course, be extended to a linear
map on all of Rn.
For x ∈X we now obtain, since x −x0 belongs to U,
Tx = T(x0 + (x −x0)) = C(x −x0) + Tx0 = Cx −Cx0 + Tx0,
which proves the theorem with v equal to Tx0 −Cx0.
2.2
Convex sets
Basic deﬁnitions and properties
Deﬁnition. A subset X of Rn is called convex if [x, y] ⊆X for all x, y ∈X.
In other words, a set X is convex if and only if it contains the line segment
between each pair of its points.
x
y
x
y
Figure 2.2. A convex set and a non-convex set
Example 2.2.1. Aﬃne sets are obviously convex. In particular, the empty
set ∅, the entire space Rn and linear subspaces are convex sets. Open line
segments and closed line segments are clearly convex.
Example 2.2.2. Open balls B(a; r) (with respect to arbitrary norms ∥·∥) are
convex sets. This follows from the triangle inequality and homogenouity, for
if x, y ∈B(a; r) and 0 ≤λ ≤1, then
∥λx + (1 −λ)y −a∥= ∥λ(x −a) + (1 −λ)(y −a)∥
≤λ∥x −a∥+ (1 −λ)∥y −a∥< λr + (1 −λ)r = r,
which means that each point λx+(1−λ)y on the segment [x, y] lies in B(a; r).
The corresponding closed balls B(a; r) = {x ∈Rn | ∥x −a∥≤r} are of
course convex, too.
Deﬁnition. A linear combination y = m
j=1 αjxj of vectors x1, x2, . . . , xm is
called a convex combination if m
j=1 αj = 1 and αj ≥0 for all j.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
28
Convex sets
Theorem 2.2.1. A convex set contains all convex combinations of its ele-
ments.
Proof. Let X be an arbitrary convex set.
A convex combination of one
element is the element itself, and hence X contains all convex combinations
formed by just one element of the set.
Now assume inductively that X
contains all convex combinations that can be formed by m −1 elements of
X, and consider an arbitrary convex combination x = m
j=1 αjxj of m ≥2
elements x1, x2, . . . , xm in X. Since m
j=1 αj = 1, some coeﬃcient αj must
be strictly less than 1, and assume without loss of generality that αm < 1,
and let s = 1 −αm = m−1
j=1 αj. Then s > 0 and m−1
j=1 αj/s = 1, which
means that
y =
m−1

j=1
αj
s xj
is a convex combination of m−1 elements in X. By the induction hypothesis,
y belongs to X. But x = sy+(1−s)xm, and it now follows from the convexity
deﬁnition that x belongs to X. This completes the induction step and the
proof of the theorem.
2.3
Convexity preserving operations
We now describe a number of ways to construct new convex sets from given
ones.
Image and inverse image under aﬃne maps
Theorem 2.3.1. Let T : V →Rm be an aﬃne map.
(i) The image T(X) of a convex subset X of V is convex.
(ii) The inverse image T −1(Y ) of a convex subset Y of Rm is convex.
Proof. (i) Suppose y1, y2 ∈T(X) and 0 ≤λ ≤1. Let x1, x2 be points in X
such that yi = T(xi). Since
λy1 + (1 −λ)y2 = λTx1 + (1 −λ)Tx2 = T(λx1 + (1 −λ)x2)
and λx1 + (1 −λ)x2 lies X, it follows that λy1 + (1 −λ)y2 lies in T(X). This
proves that the image set T(X) is convex.
(ii) To prove the convexity of the inverse image T −1(Y ) we instead assume
that x1, x2 ∈T −1(Y ), i.e. that Tx1, Tx2 ∈Y , and that 0 ≤λ ≤1. Since Y
is a convex set,
T(λx1 + (1 −λ)x2) = λTx1 + (1 −λ)Tx2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
29
Convex sets
29
is an element of Y , and this means that λx1 + (1 −λ)x2 lies in T −1(Y ).
As a special case of the preceding theorem it follows that translations
a + X of a convex set X are convex.
Example 2.3.1. The sets
{x ∈Rn | ⟨c, x⟩≥b}
and
{x ∈Rn | ⟨c, x⟩≤b},
where b is an arbitrary real number and c = (c1, c2, . . . , cn) is an arbirary
nonzero vector, are called opposite closed halfspaces. Their complements, i.e.
{x ∈Rn | ⟨c, x⟩< b}
and
{x ∈Rn | ⟨c, x⟩> b},
are called open halfspaces.
The halfspaces {x ∈Rn | ⟨c, x⟩≥b} and {x ∈Rn | ⟨c, x⟩> b} are inverse
images of the real intervals [b, ∞[ and ]b, ∞[, respectively, under the linear
map x →⟨c, x⟩. It therefore follows from Theorem 2.3.1 that halfspaces are
convex sets.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
30
Convex sets
Intersection and union
Theorem 2.3.2. Let {Xi | i ∈I} be a family of convex subsets of Rn. The
intersection {Xi | i ∈I} is a convex set.
Proof. Suppose x, y are points in the intersection Y . The deﬁnition of an
intersection implies that x and y lie in Xi for all indices i ∈I, and convexity
implies that [x, y] ⊆Xi for all i ∈I. Therefore, [x, y] ⊆Y , again by the
deﬁnition of set intersection. This proves that the intersection is a convex
set.
A union of convex sets is, of course, in general not convex. However, there
is a trivial case when convexity is preserved, namely when the sets can be
ordered in such a way as to form an ”increasing chain”.
Theorem 2.3.3. Suppose {Xi | i ∈I} is a family of convex sets Xi and that
for each pair i, j ∈I either Xi ⊆Xj or Xj ⊆Xi. The union {Xi | i ∈I}
is then a convex set.
Proof. The assumptions imply that, for each pair of points x, y in the union
there is an index i ∈I such that both points belong to Xi. By convexity, the
entire segment [x, y] lies in Xi, and thereby also in the union.
Example 2.3.2. The convexity of closed balls follows from the convexity of
open balls, because B(a; r0) = {B(a; r) | r > r0}.
Conversely, the convexity of open balls follows from the convexity of closed
balls, since B(a; r0) = {B(a; r) | r < r0} and the sets B(a; r) form an
increasing chain.
Deﬁnition. A subset X of Rn is called a polyhedron if X can be written as
an intersection of ﬁnitely many closed halfspaces or if X = Rn.†
Polyhedra are convex sets because of Theorem 2.3.2, and they can be
represented as solution sets to systems of linear inequalities. By multiplying
some of the inequalities by −1, if necessary, we may without loss of generality
assume that all inequalities are of the form c1x1 +c2x2 +· · ·+cnxn ≥d. This
means that every polyedron is the solution set to a system of the following
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
31
Convex sets
Figure 2.3. A polyhedron in R2
form









c11x1 + c12x2 + · · · + c1nxn ≥b1
c21x1 + c22x2 + · · · + c2nxn ≥b2
...
cm1x1 + cm2x2 + · · · + cmnxn ≥bm,
or in matrix notation
Cx ≥b.
The intersection of ﬁnitely many polyhedra is clearly a polyhedron. Since
each hyperplane is the intersection of two opposite closed halfspaces, and
each aﬃne set (except the entire space) is the intersection of ﬁnitely many
hyperplanes, it follows especially that aﬃne sets are polyhedra. In particular,
the empty set is a polyhedron.
Cartesian product
Theorem 2.3.4. The Cartesian product X × Y of two convex sets X and Y
is a convex set.
Proof. Suppose X lies in Rn and Y lies in Rm. The projections
P1 : Rn × Rm →Rn
and
P2 : Rn × Rm →Rm,
deﬁned by P1(x, y) = x and P2(x, y) = y, are linear maps, and
X × Y = (X × Rm) ∩(Rn × Y ) = P −1
1 (X) ∩P −1
2 (Y ).
The assertion of the theorem is therefore a consequence of Theorem 2.3.1
and Theorem 2.3.2.
Sum
Theorem 2.3.5. The sum X + Y of two convex subsets X and Y of Rn is
convex, and the product αX of a number α and a convex set X is convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
32
Convex sets
32
Proof. The maps S : Rn × Rn →Rn and T : Rn →Rn, deﬁned by S(x, y) =
x + y and Tx = αx, are linear. Since X + Y = S(X × Y ) and αX = T(X),
our assertions follow from Theorems 2.3.1 and 2.3.4.
Example 2.3.3. The set X(r) of all points whose distance to a given set X
is less than the positive number r, can be written as a sum, namely
X(r) = X + B(0; r).
Since open balls are convex, we conclude from Theorem 2.3.5 that the set
X(r) is convex if X is a convex set.
Image and inverse image under the perspective map
Deﬁnition. The perspective map P : Rn × R++ →Rn is deﬁned by
P(x, t) = t−1x
for x ∈Rn and t > 0.
The perspective map thus ﬁrst rescales points in Rn×R++ so that the last
coordinate becomes 1 and then throws the last coordinate away. Figure 2.4
illustrates the process.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
33
Convex sets
R++
1
Rn
(x, t)
P(x, t)
( 1
t x, 1)
y
P −1({y})
Figure 2.4. The perspective map P. The
inverse image of a point y ∈Rn is a halﬂine.
Theorem 2.3.6. Let X be a convex subset of Rn × R++ and Y be a convex
subset of Rn. The image P(X) of X and the inverse image P −1(Y ) of Y
under the perspective map P : Rn × R++ →Rn are convex sets.
Proof. To prove that the image P(X) is convex we assume that y, y′ ∈P(X)
and have to prove that the point λy + (1 −λ)y′ lies in P(X) if 0 < λ < 1.
To achieve this we ﬁrst note that there exist numbers t, t′ > 0 such that the
points (ty, t) and (t′y′, t′) belong to X, and then deﬁne
α =
λt′
λt′ + (1 −λ)t.
Clearly 0 < α < 1, and it now follows from the convexity of X that the point
z = α(ty, t) + (1 −α)(t′y′, t′) =
tt′(λy + (1 −λ)y′)
λt′ + (1 −λ)t
,
tt′
λt′ + (1 −λ)t

lies X. Thus, P(z) ∈P(X), and since P(z) = λy + (1 −λ)y′, we are done.
To prove that the inverse image P −1(Y ) is convex, we instead assume
that (x, t) and (x′, t′) are points in P −1(Y ) and that 0 < λ < 1. We will
prove that the point λ(x, t) + (1 −λ)(x′, t′) lies in P −1(Y ).
To this end we note that the the points 1
tx and 1
t′x′ belong to Y and that
α =
λt
λt + (1 −λ)t′
is a number between 0 and 1. Thus,
z = α1
t x + (1 −α)1
t′x′ = λx + (1 −λ)x′
λt + (1 −λ)t′
is a point in Y by convexity, and consequently

(λt+(1−λ)t′)z, λt+(1−λ)t′
is a point in P −1(Y ). But

(λt + (1 −λ)t′)z, λt + (1 −λ)t′
= λ(x, t) + (1 −λ)(x′, t′)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
34
Convex sets
and this completes the proof.
Example 2.3.4. The set {(x, xn+1) ∈Rn × R | ∥x∥< xn+1} is the inverse
image of the unit ball B(0; 1) under the perspective map, and it is therefore
a convex set in Rn+1 for each particular choice of norm ∥·∥. The following
convex sets are obtained by choosing the ℓ1-norm, the Euclidean norm and
the maximum norm, respectively, as norm:
{x ∈Rn+1 | xn+1 > |x1| + |x2| + · · · + |xn| },
{x ∈Rn+1 | xn+1 > (x2
1 + x2
2 + · · · + x2
n)1/2 }
and
{x ∈Rn+1 | xn+1 > max
1≤i≤n |xi| }.
2.4
Convex hull
Deﬁnition. Let A be a nonempty set in Rn. The set of all convex combina-
tions λ1a1+λ2a2+· · ·+λmam of an arbitrary number of elements a1, a2, . . . , am
in A is called the convex hull of A and is denoted by cvx A.
Moreover, to have the convex hull deﬁned for the empty set, we deﬁne
cvx ∅= ∅.
Theorem 2.4.1. The convex hull cvx A is a convex set containing A, and it
is the smallest set with this property, i.e. if X is a convex set and A ⊆X,
then cvx A ⊆X.
Proof. cvx A is a convex set, because convex combinations of two elements
of the type m
j=1 λjaj, where m ≥1, λ1, λ2, . . . , λm ≥0, m
j=1 λj = 1 and
a1, a2, . . . , am ∈A, is obviously an element of the same type.
Moreover,
A ⊆cvx A, because each element in A is a convex combination of itself
(a = 1a).
A convex set X contains all convex combinations of its elements, accord-
ing to Theorem 2.2.1. If A ⊆X, then in particular X contains all convex
combinations of elements in A, which means that cvx A ⊆X.
The convex hull of a set in Rn consists of all convex combinations of
an arbitrary number of elements in the set, but each element of the hull is
actually a convex combination of at most n + 1 elements.
Theorem 2.4.2. Let A ⊆Rn and suppose that x ∈cvx A. Then A contains
a subset B with at most n + 1 elements such that x ∈cvx B.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
35
Convex sets
35
A
cvx A
Figure 2.5. A set and its convex hull
Proof. According to the deﬁnition of convex hull there exists a ﬁnite subset
B of A such that x ∈cvx B. Choose such a subset B = {b1, b2, . . . , bm} with
as few elements as possible. By the minimality assumption, x = m
j=1 λjbj
with m
j=1 λj = 1 and λj > 0 for all j.
Let cj = bj −bm for j = 1, 2, . . . , m −1.
We will show that the
set C = {c1, c2, . . . , cm−1} is a linearly independent subset of Rn, and this
obviously implies that m ≤n + 1.
Suppose on the contrary that the set C is linearly dependent. Then there
exist real numbers µj, not all of them equal to 0, such that m−1
j=1 µjcj = 0.
Now let µm = −m−1
j=1 µj; then m
j=1 µj = 0 and m
j=1 µjbj = 0. Moreover,
at least one of the m numbers µ1, µ2, . . . , µm is positive.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
36
Convex sets
Consider the numbers νj = λj −tµj for t > 0. We note that
m

j=1
νj =
m

j=1
λj −t
m

j=1
µj = 1
and
m

j=1
νjbj =
m

j=1
λjbj −t
m

j=1
µjbj = x.
Moreover, νj ≥λj > 0 if µj ≤0, and νj ≥0 if µj > 0 and t ≤λj/µj.
Therefore, by choosing t as the smallest number of the numbers λj/µj with
positive denominator µj, we obtain numbers νj such that νj ≥0 for all j and
νj0 = 0 for at least one index j0. This means that x is a convex combination
of elements in the set B \ {bj0}, which consists of m −1 elements. This
contradicts the minimality assumption concerning the set B, and our proof
by contradiction is ﬁnished.
2.5
Topological properties
Closure
Theorem 2.5.1. The closure cl X of a convex set X is convex.
Proof. We recall that cl X = 
r>0 X(r), where X(r) denotes the set of all
points whose distance from X is less than r. The sets X(r) are convex when
the set X is convex (see Example 2.3.3), and an intersection of convex sets
is convex.
Interior and relative interior
The interior of a convex set may be empty. For example, line segments in
Rn have no interior points when n ≥2. A necessary and suﬃcient condition
for nonempty interior is given by the following theorem.
Theorem 2.5.2. A convex subset X of Rn has interior points if and only if
dim X = n.
Proof. If X has an interior point a, then there exists an open ball B = B(a; r)
around a such that B ⊆X, which implies that dim X ≥dim B = n, i.e.
dim X = n.
To prove the converse, let us assume that dim X = n; we will prove that
int X ̸= ∅. Since the dimension of a set is invariant under translations and
int (a + X) = a + int X, we may assume that 0 ∈X.
Let {a1, a2, . . . , am} be a maximal set of linearly independent vectors in X;
then X is a subset of the linear subspace of dimension m which is spanned by
these vectors, and it follows from the dimensionality assumption that m = n.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
37
Convex sets
The set X contains the convex hull of the vectors 0, a1, a2, . . . , an as a subset,
and, in particular, it thus contains the nonempty open set
{λ1a1 + λ2a2 + · · · + λnan | 0 < λ1 + · · · + λn < 1, λ1 > 0, . . . , λn > 0}.
All points in this latter set are interior points of X, so int X ̸= ∅.
A closed line segment [a, b] in the two-dimensional space R2 has no in-
terior points, but if we consider the line segment as a subset of a line and
identify the line with the space R, then it has interior points and its interior
is equal to the corresponding open line segment ]a, b[. A similar remark holds
for the convex hull T = cvx{a, b, c} of three noncolinear points in three-space;
the triangle T has interior points when viewed as a subset of R2, but it lacks
interior points as a subset of R3. This conﬂict is unsatisfactory if we want a
concept that is independent of the dimension of the surrounding space, but
the dilemma disappears if we use the relative topology that the aﬃne hull of
the set inherits from the surrounding space Rn.
Deﬁnition. Let X be an m-dimensional subset of Rn, and let V denote the
aﬃne hull of X, i.e. V is the m-dimensional aﬃne set that contains X.
A point x ∈X is called a relative interior point of X if there exists an
r > 0 such that B(x; r) ∩V ⊆X, and the set of all relative interior points
of X is called the relative interior of X and is denoted by rint X.
A point x ∈Rn is called a relative boundary point of X if, for every r > 0,
the intersection B(x; r) ∩V contains at least one point from X and at least
one point from V \ X. The set of all relative boundary points is called the
relative boundary of X and is denoted by rbdry X.
The relative interior of X is obviously a subset of X, and the relative
boundary of X is a subset of the boundary of X. It follows that
rint X ∪rbdry X ⊆X ∪bdry X = cl X.
Conversely, if x is a point in the closure cl X, then for each r > 0
B(x, r) ∩V ∩X = B(x, r) ∩X ̸= ∅.
Thus, x is either a relative boundary point or a relative interior point of X.
This proves the converse inclusion, and we conclude that
rint X ∪rbdry X = cl X,
or equivalently, that
rbdry X = cl X \ rint X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
38
Convex sets
38
It follows from Theorem 2.5.2, with Rn replaced by aﬀX, that every
nonempty convex set has a nonempty relative interior.
Note that the relative interior of a line segment [a, b] is the corresponding
open line segment ]a, b[, which is consistent with calling ]a, b[ an open seg-
ment. The relative interior of a set {a} consisting of just one point is the set
itself.
Theorem 2.5.3. The relative interior rint X of a convex set X is convex.
Proof. The theorem follows as a corollary of the following theorem, since
rint X ⊆cl X.
Theorem 2.5.4. Suppose that X is a convex set, that a ∈rint X and that
b ∈cl X. The entire open line segment ]a, b[ is then a subset of rint X.
Proof. Let V = aﬀX denote the aﬃne set of least dimension that includes
X, and let c = λa + (1 −λ)b, where 0 < λ < 1, be an arbitrary point on the
open segment ]a, b[. We will prove that c is a relative interior point of X by
constructing an open ball B which contains c and whose intersection with V
is contained in X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
39
Convex sets
a
b
b′
c
X
B(a; r)
B
Figure 2.6.
Illustration of the proof of Theorem 2.5.4.
The convex hull of the ball B(a; r) and the point b′ forms
a ”cone” with nonempty interior that contains the point c.
To this end, we choose r > 0 such that B(a; r) ∩V ⊆X and a point
b′ ∈X such that ∥b′ −b∥< λr/(1 −λ); this is possible since a is a relative
interior point of X and b is a point in the closure of X. Let
B = λB(a; r) + (1 −λ)b′,
and observe that B = B(λa + (1 −λ)b′; λr). The open ball B contains the
point c because
∥c −(λa + (1 −λ)b′)∥= ∥(1 −λ)(b −b′)∥= (1 −λ)∥b −b′∥< λr.
Moreover, B ∩V = λ(B(a; r) ∩V ) + (1 −λ)b′ ⊆λX + (1 −λ)X ⊆X, due
to convexity. This completes the proof.
Theorem 2.5.5. Let X be a convex set. Then
(i)
cl (rint X) = cl X;
(ii)
rint (cl X) = rint X;
(iii)
rbdry (cl X) = rbdry (rint X) = rbdry X.
Proof. The equalities in (iii) for the relative boundaries follow from the other
two and the deﬁnition of the relative boundary.
The inclusions cl (rint X) ⊆cl X and rint X ⊆rint (cl X) are both trivial,
because it follows, for arbitrary sets A and B, that A ⊆B implies cl A ⊆cl B
and rint A ⊆rint B.
It thus only remains to prove the two inclusions
cl X ⊆cl (rint X) and
rint (cl X) ⊆rint X.
So ﬁx a point x0 ∈rint X.
If x ∈cl X, then every point on the open segment ]x0, x[ lies in rint X, by
Theorem 2.5.4, and it follows from this that the point x is either an interior
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
40
Convex sets
point or a boundary point of rint X, i.e. a point in the closure cl (rint X).
This proves the inclusion cl X ⊆cl (rint X).
To prove the remaining inclusion rint (cl X) ⊆rint X we instead assume
that x ∈rint (cl X) and deﬁne yt = (1 −t)x0 + tx for t > 1. Since yt →x
as t →1, the points yt belong to cl X for all t suﬃciently close to 1. Choose
a number t0 > 1 such that yt0 belongs to cl X. According to Theorem 2.5.4,
all points on the open segment ]x0, yt0[ are relative interior points in X, and
x is such a point since x =
1
t0yt0 + (1 −1
t0)x0. This proves the implication
x ∈rint (cl X) ⇒x ∈rint X, and the proof is now complete.
Compactness
Theorem 2.5.6. The convex hull cvx A of a compact subset A in Rn is com-
pact.
Proof. Let S = {λ ∈Rn+1 | λ1, λ2, . . . , λn+1 ≥0, n+1
j=1 λj = 1}, and let
f : S × Rn × Rn × · · · × Rn →Rn be the function
f(λ, x1, x2, . . . , xn+1) =
n+1

j=1
λjxj.
The function f is of course continuous, and the set S is compact, since it is
closed and bounded. According to Theorem 2.4.2, every element x ∈cvx A
can be written as a convex combination x = n+1
j=1 λjaj of at most n + 1
elements a1, a2, . . . , an+1 from the set A. This means that the convex hull
cvx A coincides with the image f(S×A×A×· · ·×A) under f of the compact
set S × A × A × · · · × A.
Since compactness is preserved by continuous
functions, we conclude that the convex hull cvx A is compact.
2.6
Cones
Deﬁnition. Let x be a point in Rn diﬀerent from 0. The set
−→x = {λx | λ ≥0}
is called the ray through x, or the halﬂine from the origin through x.
A cone X in Rn is a non-empty set which contains the ray through each
of its points.
A cone X is in other words a non-empty set which is closed under multi-
plication by nonnegative numbers, i.e. which satisﬁes the implication
x ∈X, λ ≥0 ⇒λx ∈X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
41
Convex sets
41
In particular, all cones contain the point 0.
We shall study convex cones. Rays and linear subspaces of Rn are convex
cones, of course. In particular, the entire space Rn and the trivial subspace
{0} are convex cones. Other simple examples of convex cones are provided
by the following examples.
Example 2.6.1. A closed halfspace {x ∈Rn | ⟨c, x⟩≥0}, which is bounded
by a hyperplane through the origin, is a convex cone and is called a conic
halfspace.
The union {x ∈Rn | ⟨c, x⟩> 0} ∪{0} of the corresponding open half-
space and the origin is also a convex cone.
Example 2.6.2. The nonnegative orthant
Rn
+ = {x = (x1, . . . , xn) ∈Rn | x1 ≥0, . . . , xn ≥0}
in Rn is a convex cone.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
42
Convex sets
Deﬁnition. A cone that does not contain any line through 0, is called a proper
cone.‡
That a cone X does not contain any line through 0 is equivalent to the
condition
x, −x ∈X ⇒x = 0.
In other words, a cone X is a proper cone if and only if X ∩(−X) = {0}.
0
0
Figure 2.7. A plane cut through two proper convex cones in R3
Closed conic halfspaces in Rn are non-proper cones if n ≥2. The non-
negative orthant Rn
+ is a proper cone. The cones {x ∈Rn | ⟨c, x⟩> 0} ∪{0}
are also proper cones.
We now give two alternative ways to express that a set is a convex cone.
Theorem 2.6.1. The following three conditions are equivalent for a nonempty
subset X of Rn:
(i) X is a convex cone.
(ii) X is a cone and x + y ∈X for all x, y ∈X.
(iii) λx + µy ∈X for alla x, y ∈X and all λ, µ ∈R+.
Proof. (i) ⇒(ii): If X is a convex cone and x, y ∈X, then z = 1
2x + 1
2y
belongs to X because of convexity, and x + y (= 2z) belongs to X since X
is cone.
(ii) ⇒(iii): If (ii) holds, x, y ∈X and λ, µ ∈R+, then λx and µy belong to
X by the cone condition, and λx + µy ∈X by additivity.
(iii) ⇒(i): If (iii) holds, then we conclude that X is a cone by choosing y = x
and µ = 0, and that the cone is convex by choosing λ + µ = 1.
Deﬁnition. A linear combination m
j=1 λjxj of vectors x1, x2, . . . , xm in Rn
is called a conic combination if all coeﬃcients λ1, λ2, . . . , λm are nonnegative.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
43
Convex sets
Theorem 2.6.2. A convex cone contains all conic combinations of its ele-
ments.
Proof. Follows immediately by induction from the characterization of convex
cones in Theorem 2.6.1 (iii).
Cone preserving operations
The proofs of the four theorems below are analogous to the proofs of the cor-
responding theorems on convex sets, and they are therefore left as exercises.
Theorem 2.6.3. Let T : Rn →Rm be a linear map.
(i) The image T(X) of a convex cone X in Rn is a convex cone.
(ii) The inverse image T −1(Y ) of a convex cone in Rm is a convex cone.
Theorem 2.6.4. The intersection 
i∈I Xi of an arbitrary family of convex
cones Xi in Rn is a convex cone.
Theorem 2.6.5. The Cartesian product X × Y of two convex cones X and
Y is a convex cone.
Theorem 2.6.6. The sum X + Y of two convex cones X and Y in Rn is a
convex cone, and −X is a convex cone if X is a convex cone.
Example 2.6.3. An intersection
X =
m

i=1
{x ∈Rn | ⟨ci, x⟩≥0}
of ﬁnitely many closed conic halfspaces is called a polyhedral cone or a conic
polyhedron.
By deﬁning C as the m × n-matrix with rows c T
i , i = 1, 2, . . . , m, we can
write the above polyhedral cone X in a more compact way as
X = {x ∈Rn | Cx ≥0}.
A polyhedral cone is in other words the solution set of a system of homoge-
neous linear inequalities.
Conic hull
Deﬁnition. Let A be an arbitrary nonempty subset of Rn. The set of all
conic combinations of elements of A is called the conic hull of A, and it is
denoted by con A. The elements of A are called generators of con A.
We extend the concept to the empty set by deﬁning con ∅= {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
44
Convex sets
44
Theorem 2.6.7. The set con A is a convex cone that contains A as a subset,
and it is the smallest convex cone with this property, i.e. if X is a convex
cone and A ⊆X, then con A ⊆X.
Proof. A conic combination of two conic combinations of elements from A
is clearly a new conic combination of elements from A, and hence con A
is a convex cone. The inclusion A ⊆con A is obvious. Since a convex cone
contains all conic combinations of its elements, a convex cone X that contains
A as a subset must in particular contain all conic combinations of elements
from A, which means that con A is a subset of X.
Theorem 2.6.8. Let X = con A be a cone in Rn, Y = con B be a cone in
Rm and T : Rn →Rm be a linear map. Then
(i)
T(X) = con T(A);
(ii)
X × Y = con

(A × {0}) ∪({0} × B)

;
(iii)
X +Y = con(A∪B),
provided that m = n so that the sum X +Y
is well-deﬁned.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
45
Convex sets
Proof. (i) The cone X consists of all conic combinations x = p
j=1 λjaj of
elements aj in A. For such a conic combination Tx = p
j=1 λjTaj. The
image cone T(X) thus consists of all conic combinations of the elements
Taj ∈T(A), which means that T(X) = con T(A).
(ii) The cone X × Y consists of all pairs (x, y) =
p
j=1 λjaj, q
k=1 µkbk

of conic combinations of elements in A and B, respectively. But
(x, y) =
p

j=1
λj(aj, 0) +
q

k=1
µk(0, bk),
and hence (x, y) is a conic combination of elements in (A×{0}) ∪({0}×B),
that is (x, y) is an element of the cone Z = con

(A×{0})∪({0}×B)

. This
proves the inclusion X × Y ⊆Z.
The converse inclusion Z ⊆X × Y follows at once from the trivial in-
clusion (A × {0}) ∪({0} × B) ⊆X × Y , and the fact that X × Y is a
cone.
(iii) A typical element of X + Y has the form p
j=1 λjaj + q
k=1 µkbk,
which is a conic combination of elements in A∪B. This proves the assertion.
Finitely generated cones
Deﬁnition. A convex cone X is called ﬁnitely generated if X = con A for
some ﬁnite set A.
Example 2.6.4. The nonnegative orthant Rn
+ is ﬁnitely generated by the
standard basis e1, e2, . . . , en of Rn.
Theorem 2.6.8 has the following immediate corollary.
Corollary 2.6.9. Cartesian products X × Y , sums X + Y and images T(X)
under linear maps T of ﬁnitely generated cones X and Y , are themselves
ﬁnitely generated cones.
Intersections X ∩Y and inverse images T −1(Y ) of ﬁnitely generated cones
are ﬁnitely generated, too, but the proof of this fact has to wait until we
have shown that ﬁnitely generated cones are polyhedral, and vice versa. See
Chapter 5.
Theorem 2.6.10. Suppose that x ∈con A, where A is a subset of Rn. Then
x ∈con B for some linearly independent subset B of A.
The number of
elements in B is thus at most equal to n.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
46
Convex sets
Proof. Since x is a conic combination of elements of A, x is per deﬁnition a
conic combination of ﬁnitely many elements chosen from A. Now choose a
subset B of A with as few elements as possible and such that x ∈con B. We
will prove that the set B is linearly independent.
If B = ∅(i.e. if x = 0), then we are done, because the empty set is
linearly independent. So assume that B = {b1, b2, . . . , bm}, where m ≥1.
Then x = m
j=1 λjbj, where each λj > 0 due to the minimality assumption.
We will prove our assertion by contradiction. So, suppose that the set
B is linearly dependent.
Then there exist scalars µ1, µ2, . . . , µm, at least
one of them being positive, such that m
j=1 µjbj = 0, and it follows that
x = m
j=1(λj −tµj)bj for all t ∈R.
Now let t0 = min λj/µj, where the minimum is taken over all indices such
that µj > 0, and let j0 be an index where the minimum is achieved. Then
λj −t0µj ≥0 for all indices j, and λj0 −t0µj0 = 0. This means that x belongs
to the cone generated by the set B \ {bj0}, which contradicts the minimality
assumption about B. Thus, B is linearly independent.
Theorem 2.6.11. Every ﬁnitely generated cone is closed.
Proof. Let X be a ﬁnitely generated cone in Rn so that X = con A for some
ﬁnite set A.
We ﬁrst treat the case when A = {a1, a2, . . . , am} is a linearly independent
set. Then m ≤n, and it is possible to extend the set A, if necessary, with
vectors am+1, . . . , an to a basis for Rn. Let (c1(x), c2(x), . . . , cn(x)) denote
the coordinates of the vector x with respect to the basis a1, a2, . . . , an, so
that x = n
j=1 cj(x) aj. The coordinate functions cj(x) are linear forms on
Rn.
A vector x belongs to X if and only if x is a conic combination of the
ﬁrst m basis vectors, and this means that
X = {x ∈Rn | c1(x) ≥0, . . . , cm(x) ≥0, cm+1(x) = · · · = cn(x) = 0}.
We conclude that X is equal to the intersection of the closed halfspaces
{x ∈Rn | cj(x) ≥0}, 1 ≤j ≤m, and the hyperplanes {x ∈Rn | cj(x) = 0},
m + 1 ≤j ≤n. This proves that X is a closed cone in the present case and
indeed a polyhedral cone.
Let us now turn to the general case. Let A be an arbitrary ﬁnite set.
By the previous theorem, there corresponds to each x ∈con A a linearly
independent subset B of A such that x ∈con B, and this fact implies that
con A =  con B, where the union is to be taken over all linearly independent
subsets B of A. Of course, there are only ﬁnitely many such subsets, and
hence con A is a union of ﬁnitely many cones con B, each of them being
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
47
Convex sets
47
closed, by the ﬁrst part of the proof. A union of ﬁnitely many closed sets is
closed. Hence, con A is a closed cone.
2.7
The recession cone
The recession cone of a set consists of the directions in which the set is
unbounded and in this way provides information about the behavior of the
set at inﬁnity. Here is the formal deﬁnition of the concept.
Deﬁnition. Let X be a subset of Rn and let v be a nonzero vector in Rn.
We say that the set X recedes in the direction of v and that v is a recession
vector of X if X contains all halﬂines with direction vector v that start from
an arbitrary point of X.
The set consisting of all recession vectors of X and the zero vector is called
the recession cone and is denoted by recc X. Hence, if X is a nonempty set
then
recc X = {v ∈Rn | x + tv ∈X for all x ∈X and all t > 0},
whereas recc ∅= {0}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
48
Convex sets
X
X
X
a
a
a
Figure 2.8.
Three convex sets X and the corresponding translated
recession cones a + recc X.
Theorem 2.7.1. The recession cone of an arbitrary set X is a convex cone
and
X = X + recc X.
Proof. That recc X is a cone follows immediately from the very deﬁnition of
recession cones, and the same holds for the inclusion X + recc X ⊆X. The
converse inclusion X ⊆X +Y is trivially true for all sets Y containing 0 and
thus in particular for the cone recc X.
If v and w are two recession vectors of X, x is an arbitrary point in X and
t is an arbitrary positive number, then ﬁrst x+tv belongs to X by deﬁnition,
and then x + t(v + w) = (x + tv) + tw belongs to X. This means that the
sum v + w is also a recession vector. So the recession cone has the additivity
property v, w ∈recc X ⇒v + w ∈recc X, which implies that the cone is
convex according to Theorem 2.6.1.
Example 2.7.1. Here are some simple examples of recession cones:
recc(R+ × [0, 1]) = recc(R+× ]0, 1[) = R+ × {0},
recc(R+× ]0, 1[ ∪{(0, 0)}) = {(0, 0)},
recc{x ∈R2 | x2
1 + x2
2 ≤1} = {(0, 0)},
recc{x ∈R2 | x2 ≥x2
1} = {0} × R+,
recc{x ∈R2 | x2 ≥1/x1, x1 > 0} = R2
+.
The computation of the recession cone of a convex set is simpliﬁed by the
following theorem.
Theorem 2.7.2. A vector v is a recession vector of a nonempty convex set
X if and only if x + v ∈X for all x ∈X.
Proof. If v is a recession vector, then obviously x + v ∈X for all x ∈X.
To prove the converse, assume that x + v ∈X for all x ∈X, and let x
be an arbitrary point in X. Then, x + nv ∈X for all natural numbers n,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
49
Convex sets
by induction, and since X is a convex set, we conclude that the closed line
segment [x, x+nv] lies in X for all n. Of course, this implies that x+tv ∈X
for all positive numbers t, and hence v is a recession vector of X.
Corollary 2.7.3. If X is a convex cone, then recc X = X.
Proof. The inclusion recc X ⊆X holds for all sets X containing 0 and thus
in particular for cones X. The converse inclusion X ⊆recc X is according to
Theorem 2.7.2 a consequence of the additivity property x, v ∈X ⇒x+v ∈X
for convex cones.
Example 2.7.2. recc R2
+ = R2
+,
recc(R2
++ ∪{(0, 0)}) = R2
++ ∪{(0, 0)}.
The recession vectors of a closed convex set are characterized by the
following theorem.
Theorem 2.7.4. Let X be a nonempty closed convex set. The following three
conditions are equivalent for a vector v.
(i) v is a recession vector of X.
(ii) There exists a point x ∈X such that x + nv ∈X for all n ∈Z+.
(iii) There exist a sequence (xn)∞
1 of points xn in X and a sequence (λn)∞
1
of positive numbers such that λn →0 and λnxn →v as n →∞.
Proof. (i) ⇒(ii): Trivial, since x + tv ∈X for all x ∈X and all t ∈R+, if v
is a recession vector of X.
(ii) ⇒(iii): If (ii) holds, then condition (iii) is satisﬁed by the points xn =
x + nv and the numbers λn = 1/n.
(iii) ⇒(i): Assume that (xn)∞
1 and (λn)∞
1 are sequences of points in X and
positive numbers such that λn →0 and λnxn →v as n →∞, and let x be
an arbitrary point in X. The points zn = (1 −λn)x + λnxn then lie in X for
all suﬃciently large n, and since zn →x + v as n →∞and X is a closed
set, it follows that x + v ∈X. Hence, v is a recession vector of X according
to Theorem 2.7.2.
Theorem 2.7.5. The recession cone recc X of a closed convex set X is a
closed convex cone.
Proof. The case X = ∅is trivial, so assume that X is a nonempty closed
convex set. To prove that the recession cone recc X is closed, we assume that
v is a boundary point of the cone and choose a sequence (vn)∞
1 of recession
vectors that converges to v as n →∞.
If x is an arbitrary point in X,
then the points x + vn lie in X for each natural number n, and this implies
that their limit point x + v lies in X, since X is a closed set. Hence, v is a
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
50
Convex sets
50
recession vector, i.e. v belongs to the recession cone recc X. This proves that
the recession cone contains all its boundary points.
Theorem 2.7.6. Let {Xi | i ∈I} be a family of closed convex sets, and as-
sume that their intersection is nonempty. Then recc(
i∈I Xi) = 
i∈I recc Xi.
Proof. Let x0 be a point in 
i Xi. By Theorem 2.7.4, v ∈recc(
i Xi) if and
only if x0 + nv lies in Xi for all positive integers n and all i ∈I, and this
holds if and only if v ∈recc Xi for all i ∈I.
The recession cone of a polyhedron is given by the following theorem.
Theorem 2.7.7. If X = {x ∈Rn | Cx ≥b} is a nonempty polyhedron, then
recc X = {x ∈Rn | Cx ≥0}.
Proof. The recession cone of a closed halfspace is obviously equal to the cor-
responding conical halfspace. The theorem is thus an immediate consequence
of Theorem 2.7.6.
Note that the recession cone of a subset Y of X can be bigger than the
recession cone of X. For example,
recc R2
++ = R2
+ ⊋R2
++ ∪{(0, 0)} = recc(R2
++ ∪{(0, 0)}).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
51
Convex sets
However, this cannot occur if the superset X is closed.
Theorem 2.7.8. (i) Suppose that X is a closed convex set and that Y ⊆X.
Then recc Y ⊆recc X.
(ii) If X is a convex set, then recc(rint X) = recc(cl X).
Proof. (i) The case Y = ∅being trivial, we assume that Y is a nonempty
subset of X and that y is an arbitrary point in Y . If v is a recession vector of
Y , then y +nv are points in Y and thereby also in X for all natural numbers
n. We conclude from Theorem 2.7.4 that v is a recession vector of X.
(ii) The inclusion recc(rint X) ⊆recc(cl X) follows from part (i), because
cl X is a closed convex subset.
To prove the converse inclusion, assume that v is a recession vector of
cl X, and let x be an arbitrary point in rint X. Then x + 2v belongs to cl X,
so it follows from Theorem 2.5.4 that the open line segment ]x, x + 2v[ is
a subset of rint X, and this implies that the point x + v belongs to rint X.
Thus, x ∈rint X ⇒x + v ∈rint X, and we conclude from Theorem 2.7.2
that v is a recession vector of rint X.
Theorem 2.7.9. Let X be a closed convex set. Then X is bounded if and
only if recc X = {0}.
Proof. Obviously, recc X = {0} if X is a bounded set. So assume that X
is unbounded. Then there exists a sequence (xn)∞
1 of points in X such that
∥xn∥→∞as n →∞. The bounded sequence (xn/∥xn∥)∞
1 has a convergent
subsequence, and by deleting elements, if necessary, we may as well assume
that the original sequence is convergent. The limit v is a vector of norm 1,
which guarantees that v ̸= 0. With λn = 1/∥xn∥we now have a sequence
of points xn in X and a sequence of positive numbers λn such that λn →0
and λnxn →v as n →∞, and this means that v is a recession vector of X
according to Theorem 2.7.4. Hence, recc X ̸= {0}.
Deﬁnition. Let X be an arbitary set. The intersection recc X ∩(−recc X) is
a linear subspace, which is called the recessive subspace of X and is denoted
lin X.
A closed convex set is called line-free if lin X = {0}. The set X is in
other words line-free if and only if recc X is a proper cone.
If X is a nonempty closed convex subset of Rn and x ∈X is arbitrary,
then clearly
lin X = {x ∈Rn | a + tx ∈X for all t ∈R}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
52
Convex sets
The image T(X) of a closed convex set X under a linear map T is not
necessarily closed. A counterexample is given by X = {x ∈R2
+ | x1x2 ≥1}
and the projection T(x1, x2) = x1 of R2 onto the ﬁrst factor, the image being
T(X) = ]0 , ∞[. The reason why the image is not closed in this case is the
fact that X has a recession vector v = (0, 1) which is mapped on 0 by T.
However, we have the following general result, where N(T) denotes the
null space of the map T, i.e. N(T) = {x | Tx = 0}.
Theorem 2.7.10. Let T : Rn →Rm be a linear map, let X be a closed convex
subset of Rn, and suppose that
N(T) ∩recc X ⊆lin X.
The image T(X) is then a closed set, and
recc T(X) = T(recc X).
In particular, the image T(X) is closed if X is a closed convex set and
x = 0 is the only vector in recc X such that Tx = 0.
Proof. The intersection
L = N(T) ∩lin X = N(T) ∩recc X
is a linear subspace of Rn. Let L⊥denote its orthogonal complement. Then
X = X ∩L + X ∩L⊥, and
T(X) = T(X ∩L⊥),
since Tx = 0 for all x ∈L.
Let y be an arbitrary boundary point of the image T(X). Due to the
equality above, there exists a sequence (xn)∞
1 of points xn ∈X ∩L⊥such
that limn→∞Txn = y.
We claim that the sequence (xn)∞
1
is bounded.
Assume the contrary.
The sequence (xn)∞
1 has then a subsequence (xnk)∞
1 such that ∥xnk∥→∞
as k →∞and the bounded sequence (xnk/∥xnk∥)∞
1 converges. The limit v
is, of course, a vector of norm 1 in the linear subspace L⊥. Moreover, since
xnk ∈X and 1/∥xnk∥→0, it follows from Theorem 2.7.4 that v ∈recc X.
Finally,
Tv = lim
k→∞T(xnk/∥xnk∥) = lim
k→∞∥xnk∥−1Txnk = 0 · y = 0,
and hence v belongs to N(T), and thereby also to L.
This means that
v ∈L ∩L⊥, which contradicts tha fact that v ̸= 0, since L ∩L⊥= {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
53
Convex sets
53
The sequence (xn)∞
1 is thus bounded. Let (xnk)∞
1 be a convergent subse-
quence, and let x = limk→∞xnk. The limit x lies in X since X is closed, and
y = limk→∞Txnk = Tx, which implies that y ∈T(X). This proves that the
image T(X) contains its boundary points, so it is a closed set.
The inclusion T(recc X) ⊆recc T(X) holds for all sets X. To prove this,
assume that v is a recession vector of X and let y be an arbitrary point in
T(X). Then y = Tx for some point x ∈X, and since x+tv ∈X for all t > 0
and y + tTv = T(x + tv), we conclude that the points y + tTv lie in T(X)
for all t > 0, and this means that Tv is a recession vector of T(X).
To prove the converse inclusion recc T(X) ⊆T(recc X) for closed convex
sets X and linear maps T fulﬁlling the assumptions of the theorem, we assume
that w ∈recc T(X) and shall prove that there is a vector v ∈recc X such
that w = Tv.
We ﬁrst note that there exists a sequence (yn)∞
1 of points yn ∈T(X) and
a sequence (λn)∞
1 of positive numbers such that λn →0 and λnyn →w as
n →∞. For each n, choose a point xn ∈X ∩L⊥such that yn = T(xn).
The sequence (λnxn)∞
1 is bounded. Because assume the contrary; then
there is a subsequence such that ∥λnkxnk∥→∞and (xnk/∥xnk∥)∞
1 converges
to a vector z as k →∞. It follows from Theorem 2.7.4 that z ∈recc X,
because the xnk are points in X
and ∥xnk∥→∞as k →∞. The limit z
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
54
Convex sets
belongs to the subspace L⊥, and since
Tz = lim
k→∞T(xnk/∥xnk∥) = lim
k→∞T(λnkxnk/∥λnkxnk∥)
= lim
k→∞λnkynk/∥λnkxnk∥= 0 · w = 0,
we also have z ∈N(T) ∩recc X = L.
Hence, z ∈L ∩L⊥= {0}, which
contradicts the fact that ∥z∥= 1.
The sequence (λnxn)∞
1 , being bounded, has a subsequence that converges
to a vector v, which belongs to recc X according to Theorem 2.7.4. Since
T(λnxn) = λnyn →w, we conclude that Tv = w. Hence, w ∈T(recc X).
Theorem 2.7.11. Let X and Y be nonempty closed convex subsets of Rn and
suppose that
x ∈recc X & y ∈recc Y & x + y = 0
⇒
x ∈lin X & y ∈lin Y.
The sum X + Y is then a closed convex set and
recc(X + Y ) = recc X + recc Y.
Remark. The assumption of the theorem is fulﬁlled if recc X and −recc Y
have no common vector other than the zero vector.
Proof. Let T : Rn × Rn →Rn be the linear map T(x, y) = x + y. We leave
as an easy exercise to show that recc(X × Y ) = recc X × recc Y and that
lin(X×Y ) = lin X×lin Y . Since N(T) = {(x, y) | x+y = 0}, the assumption
of the theorem yields the inclusion N(T) ∩recc(X × Y ) ⊆lin(X × Y ), and
it now follows from Theorem 2.7.10 that T(X × Y ), i.e. the sum X + Y , is
closed and that
recc(X + Y ) = recc T(X × Y ) = T(recc(X × Y )) = T(recc X × recc Y )
= recc X + recc Y.
Corollary 2.7.12. The sum X + Y of a nonempty closed convex set X and
a nonempty compact convex set Y is a closed convex set and
recc(X + Y ) = recc X.
Proof. The assumptions of Theorem 2.7.11 are trivially fulﬁlled, because
recc Y = {0}.
Theorem 2.7.13. Suppose that C is a closed convex cone and that Y is a
nonempty compact convex set. Then recc(C + Y ) = C.
Proof. The corollary is a special case of Corollary 2.7.12 since recc C = C.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
55
Convex sets
Exercises
2.1 Prove that the set {x ∈R2
+ | x1x2 ≥a} is convex and, more generally, that
the set {x ∈Rn
+ | x1x2 · · · xn ≥a} is convex.
Hint: Use the inequality xλ
i y1−λ
i
≤λxi + (1 −λ)yi; see Theorem 6.4.1.
2.2 Determine the convex hull cvx A for the following subsets A of R2:
a) A = {(0, 0), (1, 0), (0, 1)}
b) A = {x ∈R2 | ∥x∥= 1}
c) A = {x ∈R2
+ | x1x2 = 1} ∪{(0, 0)}.
2.3 Give an example of a closed set with a non-closed convex hull.
2.4 Find the inverse image P −1(X) of the convex set X = {x ∈R2
+ | x1x2 ≥1}
under the perspective map P : R2 × R++ →R2.
2.5 Prove that the set {x ∈Rn+1 |
n
j=1 x2
j
1/2 ≤xn+1} is a cone.
2.6 Let e1, e2, . . . , en denote the standard basis in Rn and let e0 = −n
1 ej.
Prove that Rn is generated as a cone by the n + 1 vectors e0, e1, e2, . . . , en.
2.7 Prove that each conical halfspace in Rn is the conic hull of a set consisting
of n + 1 elements.
2.8 Prove that each closed cone in R2 is the conic hull of a set consisting of at
most three elements.
2.9 Prove that the sum of two closed cones in R2 is a closed cone.
2.10 Find recc X and lin X for the following convex sets:
a) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≥2, x2 ≥−1}
b) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≤2, x2 ≥−1}
c) X = {x ∈R3 | 2x1 + x2 + x3 ≤4, x1 + 2x2 + x3 ≤4}
d) X = {x ∈R3 | x2
1 −x2
2 ≥1, x1 ≥0}.
2.11 Let X and Y be arbitrary nonempty sets. Prove that
recc(X × Y ) = recc X × recc Y
and that
lin(X × Y ) = lin X × lin Y .
2.12 Let P : Rn × R++ →Rn be the perspective map. Suppose X is a convex
subset of Rn, and let c(X) = P −1(X) ∪{(0, 0)}.
a) Prove that c(X) is a cone and, more precisely, that c(X) = con(X ×{1}).
b) Find an explicit expression for the cones c(X) and cl(c(X)) if
(i) n = 1 and X = [2, 3];
(ii) n = 1 and X = [2, ∞[;
(iii) n = 2 and X = {x ∈R2 | x1 ≥x2
2}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
56
Convex sets
56
c) Find c(X) if X = {x ∈Rn | ∥x∥≤1} and ∥·∥is an arbitrary norm on
Rn.
d) Prove that cl(c(X)) = c(cl X) ∪(recc(cl X) × {0}).
e) Prove that cl(c(X)) = c(cl X) if and only if X is a bounded set.
f) Prove that the cone c(X) is closed if and only if X is compact.
2.13 Y = {x ∈R3 | x1x3 ≥x2
2, x3 > 0} ∪{x ∈R3 | x1 ≥0, x2 = x3 = 0} is a
closed cone. (Cf. problem 2.12 b) (iii)). Put
Z = {x ∈R3 | x1 ≤0, x2 = x3 = 0}.
Show that
Y + Z = {x ∈R3 | x3 > 0} ∪{x ∈R3 | x2 = x3 = 0},
with the conclusion that the sum of two closed cones in R3 is not necessarily
a closed cone.
2.14 Prove that the sum X + Y of an arbitrary closed set X and an arbitrary
compact set Y is closed.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
57
Separation
Chapter 3
Separation
3.1
Separating hyperplanes
Deﬁnition. Let X and Y be two sets in Rn. We say that the hyperplane H
separarates the two sets if the following two conditions† are satisﬁed:
(i) X is contained in one of the two opposite closed halvspaces deﬁned by
H and Y is contained in the other closed halfspace;
(ii) X and Y are not both subsets of the hyperplane H.
The separation is called strict if there exist two parallell hyperplanes to
H, one on each side of H, that separates X and Y .
The hyperplane H = {x | ⟨c, x⟩= b} thus separates the two sets X and
Y , if ⟨c, x⟩≤b ≤⟨c, y⟩for all x ∈X and all y ∈Y and ⟨c, x⟩̸= b for some
element x ∈X ∪Y .
The separation is strict if there exist numbers b1 < b < b2 such that
⟨c, x⟩≤b1 < b2 ≤⟨c, y⟩for all x ∈X, y ∈Y .
X
Y
H
Figure 3.1. A strictly separating hyperplane H
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
58
Separation
The existence of separating hyperplanes is in a natural way connected to
extreme values of linear functions.
Theorem 3.1.1. Let X and Y be two nonempty subsets of Rn.
(i) There exists a hyperplane that separates X and Y if and only if there
exists a vector c such that
sup
x∈X
⟨c, x⟩≤inf
y∈Y ⟨c, y⟩
and
inf
x∈X⟨c, x⟩< sup
y∈Y
⟨c, y⟩.
(ii) There exists a hyperplane that separates X and Y strictly if and only if
there exists a vector c such that
sup
x∈X
⟨c, x⟩< inf
y∈Y ⟨c, y⟩.
Proof. A vector c that satisﬁes the conditions in (i) or (ii) is nonzero, of
course.
Suppose that c satisﬁes the conditions in (i) and choose the number b
so that supx∈X⟨c, x⟩≤b ≤infy∈Y ⟨c, y⟩. Then ⟨c, x⟩≤b for all x ∈X and
⟨c, y⟩≥b for all y ∈Y . Moreover, ⟨c, x⟩̸= b for some x ∈X ∪Y because
of the second inequality in (i). The hyperplane H = {x | ⟨c, x⟩= b} thus
separates the two sets X and Y .
If c satisﬁes the condition in (ii), we choose instead b so that
supx∈X⟨c, x⟩< b < infy∈Y ⟨c, y⟩,
and now conclude that the hyperplane H separates X and Y strictly.
Conversely, if the hyperplane H separates X and Y , then, by changing the
signs of c and b if necessary, we may assume that ⟨c, x⟩≤b for all x ∈X and
⟨c, y⟩≥b for all y ∈Y , and this implies that supx∈X⟨c, x⟩≤b ≤infy∈Y ⟨c, y⟩.
Since H does not contain both X and Y , there are points x1 ∈X and y1 ∈Y
with ⟨c, x1⟩< ⟨c, y1⟩, and this gives us the second inequality in (i).
If the separation is strict, then there exist two parallel hyperplanes Hi =
{x | ⟨c, x⟩= bi} with b1 < b < b2 that separate X and Y . Assuming that X
lies in the halfspace {x | ⟨c, x⟩≤b1} , we conclude that
supx∈X⟨c, x⟩≤b1 < b < b2 ≤infy∈Y ⟨c, y⟩,
i.e. the vector c satisﬁes the condition in (ii).
The following simple lemma reduces the problem of separating two sets
to the case when one of the sets consists of just one point.
Lemma 3.1.2. Let X and Y be two nonempty sets.
(i) If there exists a hyperplane that separates 0 from the set X −Y , then
there exists a hyperplane that separates X and Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
59
Separation
59
(ii) If there exists a hyperplane that strictly separates 0 from the set X −Y ,
then there exists a hyperplane that strictly separates X and Y .
Proof. (i) If there exists a hyperplane that separates 0 from X −Y , then by
Theorem 3.1.1 there exists a vector c such that





0 = ⟨c, 0⟩≤
inf
x∈X, y∈Y ⟨c, x −y⟩= inf
x∈X⟨c, x⟩−sup
y∈Y
⟨c, y⟩
0 = ⟨c, 0⟩<
sup
x∈X, y∈Y
⟨c, x −y⟩= sup
x∈X
⟨c, x⟩−inf
y∈Y ⟨c, y⟩
i.e. supy∈Y ⟨c, y⟩≤infx∈X⟨c, x⟩and infy∈Y ⟨c, y⟩< supx∈X⟨c, x⟩, and we con-
clude that there exists a hyperplane that separates X and Y .
(ii) If instead there exists a hyperplane that strictly separates 0 from
X −Y , then there exists a vector c such that
0 = ⟨c, 0⟩<
inf
x∈X, y∈Y ⟨c, x −y⟩= inf
x∈X⟨c, x⟩−sup
y∈Y
⟨c, y⟩
and it now follows that supy∈Y ⟨c, y⟩< infx∈X⟨c, x⟩, which shows that Y and
X can be strictly separated by a hyperplane.
Our next theorem is the basis for our results on separation of convex sets.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
60
Separation
Theorem 3.1.3. Suppose X is a convex set and that a /∈cl X. Then there
exists a hyperplane H that strictly separates a and X.
Proof. The set cl X is closed and convex, and a hyperplane that strictly
separates a and cl X will, of course, also strictly separate a and X, since X
is a subset of cl X. Hence, it suﬃces to prove that we can strictly separate a
point a from each closed convex set that does not contain the point.
We may therefore assume, without loss of generality, that the convex set
X is closed and nonempty. Deﬁne d(x) = ∥x −a∥2, i.e. d(x) is the square
of the Euclidean distance between x and a. We start by proving that the
restriction of the continuous function d(·) to X has a minimum point.
To this end, choose a positive real number r so big that the closed ball
B(a; r) intersects the set X. Then d(x) > r2 for all x ∈X \ B(a; r), and
d(x) ≤r2 for all x ∈X ∩B(a; r). The restriction of d to the compact set
X ∩B(a; r) has a minimum point x0 ∈X ∩B(a; r), and this point is clearly
also a minimum point for d restricted to X, i.e. the inequality d(x0) ≤d(x)
holds for all x ∈X.
Now, let c = a −x0.
We claim that ⟨c, x −x0⟩≤0 for all x ∈X.
Therefore, assume the contrary, i.e. that there is a point x1 ∈X such that
⟨c, x1 −x0⟩> 0. We will prove that this assumption yields a contradiction.
Consider the points xt = tx1+(1−t)x0. They belong to X when 0 ≤t ≤1
because of convexity. Let f(t) = d(xt) = ∥xt −a∥2. Then
f(t) = ∥xt −a∥2 = ∥t(x1 −x0) + (x0 −a)∥2 = ∥t(x1 −x0) −c∥2
= t2∥x1 −x0∥2 −2t ⟨c, x1 −x0⟩+ ∥c∥2.
The function f(t) is a quadratic polynomial in t, and its derivative at 0
satisﬁes f ′(0) = −2 ⟨c, y1 −y0⟩< 0. Hence, f(t) is strictly decreasing in a
neighbourhood of t = 0, which means that d(xt) < d(x0) for all suﬃciently
small positive numbers t.
This is a contradiction to x0 being the minimum point of the function
and proves our assertion that ⟨c, x −x0⟩≤0 for all x ∈X. Consequently,
x0
a
x1
xt
c
X
Figure 3.2. Illustration for the proof of Theorem 3.1.3.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
61
Separation
⟨c, x⟩≤⟨c, x0⟩= ⟨c, a−c⟩= ⟨c, a⟩−∥c∥2 for all x ∈X, and this implies that
supx∈X⟨c, x⟩< ⟨c, a⟩. So there exists a hyperplane that strictly separates a
from X according to Theorem 3.1.1.
Deﬁnition. Let X be a subset of Rn and let x0 be a point in X. A hyperplane
H through x0 is called a supporting hyperplane of X, if it separates x0 and
X. We then say that the hyperplane H supports X at the point x0.
The existence of a supporting hyperplane of X at x0 is clearly equivalent
to the condition that there exists a vector c such that
⟨c, x0⟩= inf
x∈X⟨c, x⟩
and
⟨c, x0⟩< sup
x∈X
⟨c, x⟩.
The hyperplane {x | ⟨c, x⟩= ⟨c, x0⟩} is then a supporting hyperplane.
X
x0
H
Figure 3.3. A supporting hyperplane of X at the point x0
If a hyperplane supports the set X at the point x0, then x0 is necessarily
a relative boundary point of X. For convex sets the following converse holds.
Theorem 3.1.4. Suppose that X is a convex set and that x0 ∈X is a relative
boundary point of X. Then there exists a hyperplane H that supports X at
the point x0.
Proof. First suppose that the dimension of X equals the dimension of the
surrounding space Rn. Since x0 is then a boundary point of X, there exists
a sequence (xn)∞
1 of points xn /∈cl X that converges to x0 as n →∞, and
by Theorem 3.1.3 there exists, for each n ≥1, a hyperplane which strictly
separates xn and X. Theorem 3.1.1 thus gives us a sequence (cn)∞
1 of vectors
such that
(3.1)
⟨cn, xn⟩< ⟨cn, x⟩
for all x ∈X
and all n ≥1, and we can obviously normalize the vectors cn so that ∥cn∥= 1
for all n.
The unit sphere {x ∈Rn | ∥x∥= 1} is compact. Hence, by the Bolzano–
Weierstrass theorem, the sequence (cn)∞
1 has a subsequence (cnk)∞
k=1 which
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
62
Separation
62
converges to some vector c of length ∥c∥= 1. Clearly limk→∞xnk = x0, so by
going to the limit in the inequality (3.1) we conclude that ⟨c, x0⟩≤⟨c, x⟩for
all x ∈X. The set X is therefore a subset of one of the two closed halfspaces
determined by the hyperplane H = {x ∈Rn | ⟨c, x⟩= ⟨c, x0⟩}, and X is
not a subset of H, since dim X = n. The hyperplane H is consequently a
supporting hyperplane of X at the point x0.
Next suppose that dim X < n. Then there exists an aﬃne subspace a+U
that contains X, where U is a linear subspace of Rn and dim U = dim X.
Consider the set Y = X + U ⊥, where U ⊥is the orthogonal complement of
U. Compare with ﬁgure 3.4. Y is a ”cylinder” with X as ”base”, and each
y ∈Y has a unique decomposition of the form y = x + v with x ∈X and
v ∈U ⊥.
The set Y is a convex set of dimension n with x0 as a boundary point.
By the already proven case of the theorem, there exists a hyperplane which
supports Y at the point x0, i.e. there exists a vector c such that
⟨c, x0⟩= inf
y∈Y ⟨c, y⟩=
inf
x∈X, v∈U⊥⟨c, x + v⟩= inf
x∈X⟨c, x⟩+ inf
v∈U⊥⟨c, v⟩
and
⟨c, x0⟩< sup
y∈Y
⟨c, y⟩=
sup
x∈X, v∈U⊥⟨c, x + v⟩= sup
x∈X
⟨c, x⟩+ sup
v∈U⊥⟨c, v⟩.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
63
Separation
X
X + U⊥
x0
U⊥
Figure 3.4. Illustration for the proof of Theorem 3.1.4.
It follows from the ﬁrst equation that infv∈U⊥⟨c, v⟩is a ﬁnite number, and
since U ⊥is a vector space, this is possible if and only if ⟨c, v⟩= 0 for all
v ∈U ⊥. The conditions above are therefore reduced to the conditions
⟨c, x0⟩= inf
x∈X⟨c, x⟩
and
⟨c, x0⟩< sup
x∈X
⟨c, x⟩,
which show that X has indeed a supporting hyperplane at x0.
We are now able to prove the following necessary and suﬃcient condition
for separation of convex sets.
Theorem 3.1.5. Two convex sets X and Y can be separated by a hyperplane
if and only if their relative interiors are disjoint.
Proof. A hyperplane that separates two sets A and B clearly also separates
their closures cl A and cl B and thereby also all sets C and D that satisfy the
inclusions A ⊆C ⊆cl A and B ⊆D ⊆cl B.
To prove the existence of a hyperplane that separates the two convex
sets X and Y provided rint X ∩rint Y = ∅, it hence suﬃces to prove that
there exists a hyperplane that separates the two convex sets A = rint X and
B = rint Y , because rint X ⊆X ⊆cl(rint X) = cl X, and the corresponding
inclusions are of course also true for Y .
Since the sets A and B are disjoint, 0 does not belong to the convex set
A −B. Thus, the point 0 either lies in the complement of cl(A −B) or
belongs to cl(A −B) and is a relative boundary point of cl(A −B), because
cl(A −B) \ (A −B) ⊆cl(A −B) \ rint(A −B)
= rbdry(A −B) = rbdry(cl(A −B)).
In the ﬁrst case it follows from Theorem 3.1.3 that there is a hyperplane that
strictly separates 0 and A −B, and in the latter case Theorem 3.1.4 gives us
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
64
Separation
a hyperplane that separates 0 from the set cl(A −B), and thereby afortiori
also 0 from A −B. The existence of a hyperplane that separates A and B
then follows from Lemma 3.1.2.
Now, let us turn to the converse. Assume that the hyperplane H separates
the two convex sets X and Y . We will prove that there is no point that is
a relative interior point of both sets. To this end, let us assume that x0 is a
point in the intersection X ∩Y . Then, x0 lies in the hyperplane H because X
and Y are subsets of opposite closed halfspaces determined by H. According
to the separability deﬁnition, at least one of the two convex sets, X say,
has points that lie outside H, and this clearly implies that the aﬃne hull
V = aﬀX is not a subset of H. Hence, there are points in V from each side
of H. Therefore, the intersection V ∩B(x0; r) between V and an arbitrary
open ball B(x0; r) centered at x0 also contains points from both sides of H,
and consequently surely points that do not belong to X. This means that x0
must be a relative boundary point of X.
Hence, every point in the intersection X ∩Y is a relative boundary point
of either of the two sets X and Y . The intersection rint X ∩rint Y is thus
empty.
Let us now consider the possibility of strict separation. A hyperplane that
strictly separates two sets obviously also strictly separates their closures, so
it suﬃces to examine when two closed convex subsets X and Y can be strictly
separated. Of course, the two sets have to be disjoint, i.e. 0 /∈X −Y is a
necessary condition, and Lemma 3.1.2 now reduces the problem of separating
X strictly from Y to the problem of separating 0 strictly from X −Y . So it
follows at once from Theorem 3.1.3 that there exists a separating hyperplane
if the set X −Y is closed. This gives us the following theorem, where the
suﬃcient conditions follow from Theorem 2.7.11 and Corollary 2.7.12.
Theorem 3.1.6. Two disjoint closed convex sets X and Y can be strictly
separated by a hyperplane if the set X−Y is closed, and a suﬃcient condition
for this to be the case is recc X ∩recc Y = {0}. In particular, two disjoint
closed convex set can be separated strictly by a hyperplane if one of the sets
is bounded.
We conclude this section with a result that shows that proper convex
cones are proper subsets of conic halfspaces. More precisely, we have:
Theorem 3.1.7. Let X ̸= {0} be a proper convex cone in Rn, where n ≥2.
Then X is a proper subset of some conic halfspace {x ∈Rn | ⟨c, x⟩≥0},
whose boundary {x ∈Rn | ⟨c, x⟩= 0} does not contain X as a subset.
Proof. The point 0 is a relative boundary point of X, because no point on
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
65
Separation
65
the line segment ]0, −a[ belongs to X when a is a point ̸= 0 in X. Hence, by
Theorem 3.1.4, there exists a hyperplane H = {x ∈Rn | ⟨c, x⟩= 0} through
0 such that X lies in the closed halfspace K = {x ∈Rn | ⟨c, x⟩≥0} without
X being a subset of H. K is a conic halfspace, and the proper cone X must
be diﬀerent from K, since no conic halfspaces in Rn are proper cones when
n ≥2.
3.2
The dual cone
To each subset A of Rn we associate a new subset A+ of Rn by letting
A+ = {x ∈Rn | ⟨a, x⟩≥0 for all a ∈A}.
In particular, for sets {a} consisting of just one point we have
{a}+ = {x ∈Rn | ⟨a, x⟩≥0},
which is a conic closed halfspace. For general sets A, A+ = 
a∈A{a}+, and
this is an intersection of conic closed halfspaces. The set A+ is thus in general
a closed convex cone, and it is a polyhedral cone if A is a ﬁnite set.
Deﬁnition. The closed convex cone A+ is called the dual cone of the set A.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
66
Separation
A
0
A+
0
Figure 3.5. A cone A and its dual cone A+.
The dual cone A+ of a set A in Rn has an obvious geometric interpreta-
tion when n ≤3; it consists of all vectors that form an acute angle or are
perpendicular to all vectors in A.
Theorem 3.2.1. The following properties hold for subsets A and B of Rn.
(i)
A ⊆B
⇒B+ ⊆A+;
(ii)
A+ = (con A)+;
(iii)
A+ = (cl A)+.
Proof. Property (i) is an immediate consequence of the deﬁnition of the dual
cone.
To prove (ii) and (iii), we ﬁrst observe that
(con A)+ ⊆A+ and (cl A)+ ⊆A+,
because of property (i) and the obvious inclusions A ⊆con A and A ⊆cl A.
It thus only remains to prove the converse inclusions. So let us assume
that x ∈A+. Then
⟨λ1a1 + · · · + λkak, x⟩= λ1⟨a1, x⟩+ · · · + λk⟨ak, x⟩≥0
for all conic combinations of elements ai in A. This proves the implication
x ∈A+ ⇒x ∈(con A)+, i.e. the inclusion A+ ⊆(con A)+.
For each a ∈cl A there exists a sequence (ak)∞
1 of elements in A such that
ak →a as k →∞. If x ∈A+, then ⟨ak, x⟩≥0 for all k, and it follows, by
passing to the limit, that ⟨a, x⟩≥0. Since a ∈cl A is arbitrary, this proves
the implication x ∈A+ ⇒x ∈(cl A)+ and the inclusion A+ ⊆(cl A)+.
Example 3.2.1. Clearly, (Rn)+ = {0} and {0}+ = Rn.
Example 3.2.2. Let, as usual, e1, e2, . . . , en denote the standard basis of Rn.
Then
{ej}+ = {x ∈Rn | ⟨ej, x⟩≥0} = {x ∈Rn | xj ≥0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
67
Separation
Since Rn
+ = con{e1, . . . , en}, it follows that
(Rn
+)+ = {e1, . . . , en}+ =
n
j=1
{ej}+ = {x ∈Rn | x1 ≥0, . . . , xn ≥0} = Rn
+.
The bidual cone
Deﬁnition. The dual cone A+ of a set A in Rn is a new set in Rn, and we
may therefore form the dual cone (A+)+ of A+. The cone (A+)+ is called
the bidual cone of A, and we write A++ instead of (A+)+.
Theorem 3.2.2. Let A be an arbitrary set in Rn. Then
A ⊆con A ⊆A++.
Proof. The deﬁnitions of dual and bidual cones give us the implications
a ∈A ⇒⟨x, a⟩= ⟨a, x⟩≥0 for all x ∈A+
⇒a ∈A++,
which show that A ⊆A++. Since A++ is a cone and con A is the smallest
cone containing A, we conclude that con A ⊆A++.
Because of the previous theorem, it is natural to ask when con A = A++.
Since A++ is a closed cone, a necessary condition for this to be the case is
that the cone con A be closed. Our next theorem shows that this condition
is also suﬃcient.
Theorem 3.2.3. Let X be a convex cone. Then X++ = cl X, and conse-
quently, X++ = X if and only if the cone X is a closed.
Proof. It follows from the inclusion X ⊆X++ and the closedness of the
bidual cone X++ that cl X ⊆X++.
To prove the converse inclusion X++ ⊆cl X, we assume that x0 /∈cl X
and will prove that x0 /∈X++.
By Theorem 3.1.3, there exists a hyperplane that strictly separates x0
from cl X. Hence, there exist a vector c ∈Rn and a real number b such
that the inequality ⟨c, x⟩≥b > ⟨c, x0⟩holds for all x ∈X. In particular,
t⟨c, x⟩= ⟨c, tx⟩≥b for all x ∈X and all numbers t ≥0, since X is a cone,
and this clearly implies that b ≤0 and that ⟨c, x⟩≥0 for all x ∈X. Hence,
c ∈X+, and since ⟨c, x0⟩< b ≤0, we conclude that x0 /∈X++.
By Theorem 2.6.11, ﬁnitely generated cones are closed, so we have the
following immediate corollary.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
68
Separation
68
Corollary 3.2.4. If the cone X is ﬁnitely generated, then X++ = X.
Example 3.2.3. The dual cone of the polyhedral cone
X =
m

i=1
{x ∈Rn | ⟨ai, x⟩≥0}
is the cone
X+ = con{a1, a2, . . . , am}.
This follows from the above corollary and Theorem 3.2.1, for
X = {a1, a2, . . . , am}+ = (con{a1, a2, . . . , am})+.
If we use matrices to write the above cone X as {x ∈Rn | Ax ≥0},
then the vector ai corresponds to the ith column of the transposed matrix
AT (cf. Example 2.6.3), and the dual cone X+ is consequently generated by
the columns of AT. Thus,
{x ∈Rn | Ax ≥0}+ = {ATy | y ∈Rm
+}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
69
Separation
3.3
Solvability of systems of linear inequali-
ties
Corollary 3.2.4 can be reformulated as a criterion for the solvability of systems
of linear inequalities. The proof of this criterion uses the following lemma
about dual cones.
Lemma 3.3.1. Let X and Y be closed convex cones in Rn. Then
(i)
X ∩Y = (X+ + Y +)+;
(ii)
X + Y = (X+ ∩Y +)+, provided that the cone X + Y is closed.
Proof. We have X+ ⊆(X ∩Y )+ and Y + ⊆(X ∩Y )+, by Theorem 3.2.1 (i).
Hence, X+ + Y + ⊆(X ∩Y )+ + (X ∩Y )+ = (X ∩Y )+.
Another application of Theorem 3.2.1 in combination with Theorem 3.2.3
now yields X ∩Y = (X ∩Y )++ ⊆(X+ + Y +)+.
To obtain the converse inclusion we ﬁrst deduce from X+ ⊆X+ + Y +
that (X+ + Y +)+ ⊆X++ = X, and the inclusion (X+ + Y +)+ ⊆Y is of
course obtained in the same way. Consequently, (X+ + Y +)+ ⊆X ∩Y . This
completes the proof of property (i).
By replacing X and Y in (i) by the closed cones X+ and Y +, we obtain
the equality X+ ∩Y + = (X++ + Y ++)+ = (X + Y )+, and since the cone
X + Y is assumed to be closed, we conclude that
X + Y = (X + Y )++ = (X+ ∩Y +)+.
We are now ready for the promised result on the solvability of certain
systems of linear inequalities, a result that will be used in our proof of the
duality theorem in linear programming.
Theorem 3.3.2. Let U be a ﬁnitely generated cone in Rn, V be a ﬁnitely
generated cone in Rm, A be an m×n-matrix and c be an n×1-matrix. Then
the system
(S)





Ax ∈V +
x ∈U +
cTx < 0
has a solution x if and only if the system
(S∗)

c −ATy ∈U
y ∈V
has no solution y.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
70
Separation
Proof. The system (S∗) clearly has a solution if and only if c ∈(AT(V ) + U),
and consequently, there is no solution if and only if c /∈(AT(V ) + U). There-
fore, it is worthwhile to take a closer look at the cone AT(V ) + U.
The cones AT(V ), U and AT(V ) + U are closed, since they are ﬁnitely
generated. We may therefore apply Lemma 3.3.1 with
AT(V ) + U =

AT(V )+ ∩U ++
as conclusion. The condition c /∈(AT(V )+U) is now seen to be equivalent to
the existence of a vector x ∈AT(V )+ ∩U + satisfying the inequality cTx < 0,
i.e. to the existence of an x such that
(†)





x ∈AT(V )+
x ∈U +
cTx < 0 .
It now only remains to translate the condition x ∈AT(V )+; it is equivalent
to the condition
⟨y, Ax⟩= ⟨ATy, x⟩≥0
for all y ∈V ,
i.e. to Ax ∈V +. The two systems (†) and (S) are therefore equivalent, and
this observation completes the proof.
By choosing U = {0} and V = Rm
+ with dual cones U + = Rn and
V + = Rm
+, we get the following special case of Theorem 3.3.2.
Corollary
3.3.3 (Farkas’s lemma). Let A be an m × n-matrix and c be an
n × 1-matrix, and consider the two systems:
(S)

Ax ≥0
cTx < 0
and
(S∗)

ATy = c
y ≥0
The system (S) has a solution if and only if the system (S∗) has no solution.
Example 3.3.1. The system







x1 −x2 + 2x3 ≥0
−x1 + x2 −
x3 ≥0
2x1 −x2 + 3x3 ≥0
4x1 −x2 + 10x3 < 0
has no solution, because the dual system



y1 −y2 + 2y3 =
4
−y1 + y2 −y3 = −1
2y1 −y2 + 3y3 = 10
has a nonnegative solution y = (3, 5, 3).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
71
Separation
71
Example 3.3.2. The system







2x1 + x2 −x3 ≥0
x1 + 2x2 −2x3 ≥0
x1 −x2 + x3 ≥0
x1 −4x2 + 4x3 < 0
is solvable, because the solutions of the dual system



2y1 + y2 + y3 =
1
y1 + 2y2 −y3 = −4
−y1 −2y2 + y3 =
4
are of the form y = (2 −t, −3 + t, t), t ∈R, and none of those is nonnegative
since y1 < 0 for t > 2 and y2 < 0 for t < 3.
The following generalization of Example 3.2.3 will be needed in Chapter
10 in Part II.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
72
Separation
Theorem 3.3.4. Let a1, a2, . . . , am be vectors in Rn, and let I, J be a parti-
tion of the index set {1, 2, . . . , m}, i.e. I ∩J = ∅and I ∪J = {1, 2, . . . , m}.
Let
X =

i∈I
{x ∈Rn | ⟨ai, x⟩≥0} ∩

i∈J
{x ∈Rn | ⟨ai, x⟩> 0},
and suppose that X ̸= ∅. Then
X+ = con{a1, a2, . . . , am}.
Proof. Let
Y =
m

i=1
{x ∈Rn | ⟨ai, x⟩≥0}.
The set Y is closed and contains X, and we will prove that Y = cl X by
showing that every neighborhood of an arbitrary point y ∈Y contains points
from X.
So, ﬁx a point x0 ∈X, and consider the points y + tx0 for t > 0. These
points lie in X, for
⟨ai, y + tx0⟩= ⟨ai, y⟩+ t⟨ai, x0⟩=

≥0
if i ∈I
> 0
if i ∈J,
and since y +tx0 →y as t →0, there are indeed points in X arbitrarily close
to y.
Hence X+ = (cl X)+ = Y +, by Theorem 3.2.1, and the conclusion of the
theorem now follows from the result in Example 3.2.3.
How do we decide whether the set X in Theorem 3.3.4 is nonempty? If
just one of the m linear inequalities that deﬁne X is strict (i.e. if the index
set J consists of one element), then Farkas’s lemma gives a necessary and
suﬃcient condition for X to be nonempty. A generalization to the general
case reads as follows.
Theorem 3.3.5. The set X in Theorem 3.3.4 is nonempty if and only if





m

i=1
λiai = 0
λi ≥0 for all i
⇒λi = 0 for all i ∈J.
Proof. Let the vectors ˆai in Rn+1 (= Rn × R) be deﬁned by
ˆai =

(ai, 0)
if i ∈I
(ai, 1)
if i ∈J.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
73
Separation
Write ˜x = (x, xn+1), and let ˜X be polyhedral cone
˜X =
m

i=1
{˜x ∈Rn+1 | ⟨ˆai, ˜x⟩≥0} = (con{ˆa1, . . . , ˆam})+.
Since
⟨ˆai, (x, xn+1)⟩=

⟨ai, x⟩
if i ∈I
⟨ai, x⟩+ xn+1
if i ∈J,
and ⟨ai, x⟩> 0 for all i ∈J if and only if there exists a negative real number
xn+1 such that ⟨ai, x⟩+ xn+1 ≥0 for all i ∈J, we conclude that the point
x lies in X if and only if there exists a negative number xn+1 such that
⟨ˆai, (x, xn+1)⟩≥0 for all i, i.e. if and only if there exists a negative number
xn+1 such that (x, xn+1) ∈˜X. This is equivalent to saying that the set X is
empty if and only if the implication ˜x ∈˜X ⇒xn+1 ≥0 is true, i.e. if and
only if ˜X ⊆Rn × R+. Using the results on dual cones in Theorems 3.2.1
and 3.2.3 we thus obtain the following chain of equivalences:
X = ∅⇔˜X ⊆Rn × R+
⇔{0} × R+ = (Rn × R+)+ ⊆˜X+ = con{ˆa1, ˆa2, . . . , ˆam}
⇔(0, 1) ∈con{ˆa1, ˆa2, . . . , ˆam}
⇔there are numbers λi ≥0 such that
m

i=1
λiai = 0 and

i∈J
λi = 1
⇔there are numbers λi ≥0 such that
m

i=1
λiai = 0 and λi > 0 for
some i ∈J.
(The last equivalence holds because of the homogenouity of the condition
m
i=1 λiai = 0. If the condition is fulﬁlled for a set of nonnegative numbers
λi with λi > 0 for at least one i ∈J, then we can certainly arrange so that

i∈J λi = 1 by multiplying with a suitable constant.)
Since the ﬁrst and the last assertion in the above chain of equivalences are
equivalent, so are their negations, and this is the statement of the theorem.
The following corollary is an immediate consequence of Theorem 3.3.5.
Corollary
3.3.6. The set X in Theorem 3.3.4 is nonempty if the vectors
a1, a2, . . . , am are linearly independent.
The following equivalent matrix version of Theorem 3.3.5 is obtained by
considering the vectors ai, i ∈I and ai, i ∈J in Theorems 3.3.4 and 3.3.5 as
rows in two matrices A and C, respectively.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
74
Separation
74
Theorem 3.3.7. Let A be a p×n-matrix and C be q×n-matris. Then exactly
one of the two dual systems

Ax ≥0
Cx > 0
and

ATy + CTz = 0
y, z ≥0, z ̸= 0
has a solution.
Theorem 3.3.7 will be generalized in Chapter 6.5, where we prove a the-
orem on the solvability of systems of convex and aﬃne inequalities.
Exercises
3.1 Find two disjoint closed convex sets in R2 that are not strictly separable by
a hyperplane (i.e. by a line in R2).
3.2 Let X be a convex proper subset of Rn. Show that X is an intersection of
closed halfspaces if X is closed, and an intersection of open halfspaces if X
is open.
3.3 Prove the following converse of Lemma 3.1.2: If two sets X and Y are
(strictly) separable, then X −Y and 0 are (strictly) separable.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
75
Separation
3.4 Find the dual cones of the following cones in R2:
a) R+ ×{0}
b) R×{0}
c) R× R+
d) (R++ × R++)∪{(0, 0)}
e) {x ∈R2 | x1 + x2 ≥0, x2 ≥0}
3.5 Prove for arbitrary sets X and Y that (X × Y )+ = X+ × Y +.
3.6 Determine the cones X, X+ and X++, if X = con A and
a) A = {(1, 0), (1, 1), (−1, 1)}
b) A = {(1, 0), (−1, 1), (−1, −1)}
c) A = {x ∈R2 | x1x2 = 1, x1 > 0}.
3.7 Let A = {a1, a2, . . . , am} be a subset of Rn, and suppose 0 /∈A. Prove that
the following three conditions are equivalent:
(i) con A is a proper cone.
(ii) m
j=1 λjaj = 0, λ = (λ1, λ2, . . . , λm) ≥0 ⇒λ = 0.
(iii) There is a vector c such that ⟨c, a⟩> 0 for all a ∈A.
3.8 Is the following system consistent?







x1 −2x2 −7x3 ≥0
5x1 + x2 −2x3 ≥0
x1 + 2x2 + 5x3 ≥0
18x1 + 5x2 −3x3 < 0
3.9 Show that



x1 + x2 −x3 ≥2
x1 −x2
≥1
x1
+ x3 ≥3
⇒
6x1 −2x2 + x3 ≥11.
3.10 For which values of the parameter α ∈R is the system







x1 +
x2 + αx3 ≥0
x1 + αx2 +
x3 ≥0
αx1 +
x2 +
x3 ≥0
x1 + αx2 + α2x3 < 0
solvable?
3.11 Let A be an m × n-matrix. Prove that exactly one of the two systems (S)
and (S∗) has a solution if
(S)





Ax = 0
x ≥0
x ̸= 0
and
(S∗)
ATy > 0
a)
(S)

Ax = 0
x > 0
and
(S∗)

ATy ≥0
ATy ̸= 0 .
b)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
76
Separation
76
3.12 Prove that the following system of linear inequalities is solvable:











Ax = 0
x ≥0
ATy ≥0
ATy + x > 0 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
77
More on convex sets
Chapter 4
More on convex sets
4.1
Extreme points and faces
Extreme point
Polyhedra, like the one in ﬁgure 4.1, have vertices. A vertex is characterized
by the fact that it is not an interior point of any line segment that lies entirely
in the polyhedron. This property is meaningful for arbitrary convex sets.
Figure 4.1. A polyhedron
with vertices.
Figure 4.2. Extreme points of a line
segment, a triangle and a circular disk.
Deﬁnition. A point x in a convex set X is called an extreme point of the set
if it does not lie in any open line segment joining two points of X, i.e. if
a1, a2 ∈X & a1 ̸= a2
⇒x /∈]a1, a2[.
The set of all extreme points of X will be denoted by ext X.
A point in the relative interior of a convex set is clearly never an extreme
point, except when the convex set consists just one point.† With an exception
for this trivial case, ext X is consequently a subset of the relative boundary
of X. In particular, open convex sets have no extreme points.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
78
More on convex sets
78
Example 4.1.1. The two endpoints are the extreme points of a closed line
segment. The three vertices are the extreme points of a triangle. All points
on the boundary {x | ∥x∥2 = 1} are extreme points of the Euclidean closed
unit ball B(0; 1) in Rn.
Extreme ray
The extreme point concept is of no interest for convex cones, because non-
proper cones have no extreme points, and proper cones have 0 as their only
extreme point. Instead, for cones the correct extreme concept is about rays,
and in order to deﬁne it properly we ﬁrst have to deﬁne what it means for a
ray to lie between to rays.
Deﬁnition. We say that the ray R = −→a lies between the two rays R1 = −→
a1
and R2 = −→
a2 if the two vectors a1 and a2 are linearly independent and there
exist two positive numbers λ1 and λ2 so that a = λ1a1 + λ2a2.
It is easy to convince oneself that the concept ”lie between” only depends
on the rays R, R1 and R2, and not on the vectors a, a1 and a2 chosen to
represent them. Furthermore, a1 and a2 are linearly independent if and only
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
79
More on convex sets
if the rays R1 and R2 are diﬀerent and not opposite to each other, i.e. if and
only if R1 ̸= ±R2.
Deﬁnition. A ray R in a convex cone X is called an extreme ray of the cone
if the following two conditions are satisﬁed:
(i) the ray R does not lie between any rays in the cone X;
(ii) the opposite ray −R does not lie in X.
The set of all extreme rays of X is denoted by exr X.
The second condition (ii) is automatically satisﬁed for all proper cones,
and it implies, as we shall see later (Theorem 4.2.4), that non-proper cones
have no extreme rays.
R3
R1
R2
Figure 4.3. A polyhedral cone in R3 with three extreme rays.
It follows from the deﬁnition that no extreme ray of a convex cone with
dimension greater than 1 can pass through a relative interior point of the
cone. The extreme rays of a cone of dimension greater than 1 are in other
words subsets of the relative boundary of the cone.
Example 4.1.2. The extreme rays of the four subcones of R are as follows:
exr{0} = exr R = ∅, exr R+ = R+ and exr R−= R−.
The non-proper cone R × R+ in R2 (the ”upper halfplane”) has no ex-
treme rays, since the two boundary rays R+ × {0} and R−× {0} are dis-
qualiﬁed by condition (ii) of the extreme ray deﬁnition.
Face
Deﬁnition. A subset F of a convex set X is called a proper face of X if
F = X ∩H for some supporting hyperplane H of X. In addition, the set X
itself and the empty set ∅are called non-proper faces of X.‡
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
80
More on convex sets
The reason for including the set itself and and the empty set among the
faces is that it simpliﬁes the wording of some theorems and proofs.
X
H
F
Figure 4.4. A convex set X with F as one its faces.
The faces of a convex set are obviously convex sets. And the proper faces
of a convex cone are cones, since the supporting hyperplanes of a cone must
pass through the origin and thus be linear subspaces.
Example 4.1.3. Every point on the boundary {x | ∥x∥2 = 1} is a face of
the closed unit ball B(0; 1), because the tangent plane at a boundary point
is a supporting hyperplane and does not intersect the unit ball in any other
point.
Example 4.1.4. A cube in R3 has 26 proper faces: 8 vertices, 12 edges and
6 sides.
Theorem 4.1.1. The relative boundary of a closed convex set X is equal to
the union of all proper faces of X.
Proof. We have to prove that rbdry X =  F, where the union is taken over
all proper faces F of X. So suppose that x0 ∈F, where F = X ∩H is a
proper face of X, and H is a supporting hyperplane. Since H supports X at
x0, and since, by deﬁnition, X is not contained in H, it follows that x0 is a
relative boundary point of X. This proves the inclusion  F ⊆rbdry X.
Conversely, if x0 is a relative boundary point of X, then there exists a
hyperplane H that supports X at x0, and this means that x0 lies in the
proper face X ∩H.
Theorem 4.1.2. The intersection of two faces of a convex set is a face of the
set.
Proof. Let F1 and F2 be two faces of the convex set X, and let F = F1 ∩F2.
That F is a face is trivial if the two faces F1 and F2 are identical, or if they
are disjoint, or if one of them is non-proper.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
81
More on convex sets
81
H1
H2
H
F1
F2
X
F
Figure 4.5. Illustration for the proof of Theorem 4.1.2.
So suppose that the two faces F1 and F2 are distinct and proper, i.e. that
they are of the form Fi = X ∩Hi, where H1 och H2 are distinct supporting
hyperplanes of the set X, and F ̸= ∅. Let
Hi = {x ∈Rn | ⟨ci, x⟩= bi},
where the normal vectors ci of the hyperplanes are chosen so that X lies
in the two halfspaces {x ∈Rn | ⟨ci, x⟩≤bi}, and let x1 ∈X be a point
satisfying the condition ⟨c1, x1⟩< b1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
82
More on convex sets
The hyperplanes H1 and H2 must be non-parallel, since X ∩H1 ∩H2 =
F ̸= ∅. Hence c2 ̸= −c1, and we obtain a new hyperplane
H = {x ∈Rn | ⟨c, x⟩= b}
by deﬁning c = c1 + c2 and b = b1 + b2. We will show that H is a supporting
hyperplane of X and that F = X ∩H, which proves our claim that the
intersection F is a face of X.
For all x ∈X, we have the inequality
⟨c, x⟩= ⟨c1, x⟩+ ⟨c2, x⟩≤b1 + b2 = b,
and the inequality is strict for the particular point x1 ∈X, since
⟨c, x1⟩= ⟨c1, x1⟩+ ⟨c2, x1⟩< b1 + b2 = b.
So X lies in one of the two closed halfspaces determined by H without being
a subset of H. Moreover, for all x ∈F = X ∩H1 ∩H2,
⟨c, x⟩= ⟨c1, x⟩+ ⟨c2, x⟩= b1 + b2 = b.
which implies that H is a supporting hyperplane of X and that F ⊆X ∩H.
Conversely, if x ∈X ∩H, then ⟨c1, x⟩≤b1, ⟨c2, x⟩≤b2 and
⟨c1, x⟩+ ⟨c2, x⟩= b1 + b2,
and this implies that ⟨c1, x⟩= b1 and ⟨c2, x⟩= b2. Hence, x ∈X ∩H1 ∩H2 =
F. This proves the inclusion X ∩H ⊆F.
Theorem 4.1.3.
(i) Suppose F is a face of the convex set X. A point x in
F is an extreme point of F if and only if x is an extreme point of X.
(ii) Suppose F is a face of the convex cone X. A ray R in F is an extreme
ray of F if and only if R is an extreme ray of X.
Proof. Since the assertions are trivial for non-proper faces, we may assume
that F = X ∩H for some supporting hyperplane H of X.
No point in a hyperplane lies in the interior of a line segment whose
endpoints both lie in the same halfspace, unless both endpoints lie in the
hyperplane, i.e. unless the line segment lies entirely in the hyperplane.
Analogously, no ray in a hyperplane H (through the origin) lies between
two rays in the same closed halfspace determined by H, unless both these
rays lie in the hyperplane H. And the opposite ray −R of a ray R in a
hyperplane clearly lies in the same hyperplane.
(i) If x ∈F is an interior point of some line segment with both endpoints
belonging to X, then x is in fact an interior point of a line segment whose
endpoints both belong to F. This proves the implication
x /∈ext X ⇒x /∈ext F.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
83
More on convex sets
x
X
H
F
Figure 4.6.
The two endpoints of an open line segment that intersects the
hyperplane H must either both belong to H or else lie in opposite open halfspaces.
The converse implication is trivial, since every line segment in F is a line
segment i X. Hence, x /∈ext X ⇔x /∈ext F, and this is of course equivalent
to assertion (i).
(ii) Suppose R is a ray in F and that R is not an extreme ray of the cone X.
Then there are two possibilities: R lies between two rays R1 and R2 in X, or
the opposite ray −R lies in X. In the ﬁrst case, R1 and R2 will necessarily
lie in F, too. In the second case, the ray −R will lie in F. Thus, both cases
lead to the conclusion that R is not an extreme ray of the cone F, and this
proves the implication R /∈exr X ⇒R /∈exr F.
The converse implication is again trivial, and this observation concludes
the proof of assertion (ii).
4.2
Structure theorems for convex sets
Theorem 4.2.1. Let X be a line-free closed convex set with dim X ≥2. Then
X = cvx(rbdry X).
Proof. Let n = dim X. By identifying the aﬃne hull of X with Rn, we may
without loss of generality assume that X is a subset of Rn of full dimension,
so that rbdry X = bdry X. To prove the theorem it is now enough to prove
that every point in X lies in the convex hull of the boundary bdry X, because
the inclusionen cvx(bdry X) ⊆X is trivially true.
The recession cone C = recc X is a proper cone, since X is supposed to
be line-free. Hence, there exists a closed halfspace
K = {x ∈Rn | ⟨c, x⟩≥0},
which contains C as a proper subset, by Theorem 3.1.7. Since C is a closed
cone, we conclude that the corresponding open halfspace
K+ = {x ∈Rn | ⟨c, x⟩> 0}.
contains a vector v that does not belong to C. The opposite vector −v, which
lies in the opposite open halfspace, does not belong to C, either. Compare
ﬁgure 4.7.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
84
More on convex sets
84
K+
v
x
X
x1
x2
0
C
Figure 4.7. An illustration for the proof of Theorem 4.2.1.
We have produced two opposite vectors ±v, both lying outside the reces-
sion cone. The two opposite halﬂines x + −→v and x −−→v from a point x ∈X
therefore both intersect the complement of X.
The intersection between X and the line through x with direction vector
v, which is a closed convex set, is thus either a closed line segment [x1, x2]
containing x and with endpoints belonging to the boundary of X, or the
singleton set {x} with x belonging to the boundary of X. In the ﬁrst case, x
is a convex combination of the boundary points x1 and x2. So, x lies in the
convex hull of the boundary in both cases. This completes the proof.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
85
More on convex sets
It is now possible to give a complete description of line-free closed convex
sets in terms of extreme points and recession cones.
Theorem 4.2.2. A nonempty closed convex set X has extreme points if and
only if X is line-free, and if X is line-free, then
X = cvx(ext X) + recc X.
Proof. First suppose that the set X is not line-free. Its recessive subspace
will then, by deﬁnition, contain a nonzero vector y, and this implies that
the two points x ± y lie in X for each point x ∈X. Therefore, x being the
midpoint of the line segment ]x −y, x + y[, is not an extreme point. This
proves that the set ext X of extreme points is empty.
Next suppose that X is line-free. We claim that ext X ̸= ∅and that X =
cvx(ext X) + recc X, and we will prove this by induction on the dimension
of the set X.
Our claim is trivially true for zero-dimensional sets X, i.e. sets consisting
of just one point. If dim X = 1, then either X is a halﬂine a + −→v with one
extreme point a and recession cone equal to −→v , or a line segment [a, b] with
two extreme points a, b and recession cone equal to {0}, and the equality in
the theorem is clearly satisﬁed in both cases.
Now assume that n = dim X ≥2 and that our claim is true for all line-
free closed convex sets X with dimension less than n. By Theorems 4.1.1
and 4.2.1,
X = cvx
 F

,
where the union is taken over all proper faces F of X. Each proper face F is
a nonempty line-free closed convex subset of a supporting hyperplane H and
has a dimension which is less than or equal to n −1. Therefore, ext F ̸= ∅
and
F = cvx(ext F) + recc F,
by our induction hypothesis.
Since ext F ⊆ext X (by Theorem 4.1.3), it follows that ext X ̸= ∅. More-
over, recc F is a subset of recc X, so we have the inclusion
F ⊆cvx(ext X) + recc X
for each face F. The union  F is consequently included in the convex set
cvx(ext X) + recc X. Hence
X = cvx
 F

⊆cvx(ext X) + recc X ⊆X + recc X = X,
so X = cvx(ext X) + recc X, and this completes the induction and the proof
of the theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
86
More on convex sets
The recession cone of a compact set is equal to the null cone, and the
following result is therefore an immediate corollary of Theorem 4.2.2.
Corollary 4.2.3. Each nonempty compact convex set has extreme points and
is equal to the convex hull of its extreme points.
We shall now formulate and prove the anaologue of Theorem 4.2.2 for
convex cones, and in order to simplify the notation we shall use the following
convention: If A is a family of rays, we let con A denote the cone
con
 
R∈A
R

,
i.e. con A is the cone that is generated by the vectors on the rays in the
family A. If we choose a nonzero vector aR on each ray R ∈A and let
A = {aR | R ∈A}, then of course con A = con A.
The cone con A is clearly ﬁnitely generated if A is a ﬁnite family of rays,
and we obtain a set of generators by choosing one nonzero vector from each
ray.
Theorem 4.2.4. A closed convex cone X has extreme rays if and only if the
cone is proper and not equal to the null cone {0}. If X is a proper closed
convex cone, then
X = con(exr X).
Proof. First suppose that the cone X is not proper, and let R = −→x be an
arbitrary ray in X. We will prove that R can not be an extreme ray.
Since X is non-proper, there exists a nonzero vector a in the intersection
X ∩(−X). First suppose that R is equal to −→a or to −−→a . Then both R and
its opposite ray −R lie in X, and this means that R is not an extreme ray.
Next suppose R ̸= ±−→a . The vectors x and a are then linearly indepen-
dent, and the two rays R1 = −−−→
x + a and R2 = −−→
x−a are consequently distinct
and non-opposite rays in the cone X. Since x = 1
2(x + a) + 1
2(x −a), we
conclude that R lies between R1 and R2. Thus, R is not an extreme ray in
this case either, and this proves that non-proper cones have no extreme rays.
The equality X = con(exr X) is trivially true for the null cone, since
exr{0} = ∅and con ∅= {0}. To prove that the equality holds for all non-
trivial proper closed convex cones and that these cones do have extreme rays,
we only have to modify slightly the induction proof for the corresponding part
of Theorem 4.2.2.
The start of the induction is of course trivial, since one-dimensional proper
cones are rays. So suppose our assertion is true for all cones of dimension less
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
87
More on convex sets
87
than or equal to n −1, and let X be a proper closed n-dimensional convex
cone. X is then, in particular, a line-free set, whence X = cvx
 F

, where
the union is taken over all proper faces F of the cone. Moreover, since X is
a convex cone, cvx
 F

⊆con
 F

⊆con X = X, and we conclude that
(4.1)
X = con
 F

.
We may of course delete the trivial face F = {0} from the above union
without destroying the identity, and every remaining face F is a proper closed
convex cone of dimension less than or equal to n −1 with exr F ̸= ∅and
F = con(exr F), by our induction assumption. Since exr F ⊆exr X, it now
follows that the set exr X is nonempty and that F ⊆con(exr X).
The union  F of the faces is thus included in the cone con(exr X), so
it follows from equation (4.1) that X ⊆con(exr X).
Since the converse
inclusion is trivial, we have equality X = con(exr X), and the induction step
is now complete.
The recession cone of a line-free convex set is a proper cone. The following
structure theorem for convex sets is therefore an immediate consequence of
Theorems 4.2.2 and 4.2.4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
88
More on convex sets
Theorem 4.2.5. If X is a nonempty line-free closed convex set, then
X = cvx(ext X) + con(exr(recc X)).
The study of arbitrary closed convex sets is reduced to the study of line-
free such sets by the following theorem, which says that every non-line-free
closed convex set is a cylinder with a line-free convex set as its base and with
the recessive subspace lin X as its ”axis”.
Theorem 4.2.6. Suppose X is a closed convex set in Rn. The intersection
X ∩(lin X)⊥is then a line-free closed convex set and
X = lin X + X ∩(lin X)⊥.
0
(lin X)⊥
X
lin X
Figure 4.8. Illustration for Theorem 4.2.6.
Proof. Each x ∈Rn has a unique decomposition x = y + z with y ∈lin X
and z ∈(lin X)⊥. If x ∈X, then z lies in X, too, since
z = x −y ∈X + lin X = X.
This proves the inclusion X ⊆lin X+X∩(lin X)⊥, and the converse inclusion
follows from lin X + X ∩(lin X)⊥⊆lin X + X = X.
Exercises
4.1 Find ext X and decide whether X = cvx(ext X) when
a) X = {x ∈R2
+ | x1 + x2 ≥1}
b) X =

[0, 1] × [0, 1[

∪

[0, 1
2] × {1}

c) X = cvx

{x ∈R3 | (x1 −1)2 + x2
2 = 1, x3 = 0} ∪{(0, 0, 1), (0, 0, −1)}

.
4.2 Prove that ext(cvx A) ⊆A for each subset A of Rn.
4.3 Let X = cvx A and suppose the set A is minimal in the following sense: If
B ⊆A och X = cvx B, then B = A. Prove that A = ext X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
89
More on convex sets
4.4 Let x0 be a point in a convex set X. Prove that x0 ∈ext X if and only if
the set X \ {x0} is convex.
4.5 Give an example of a compact convex subset of R3 such that the set of
extreme points is not closed.
4.6 A point x0 in a convex set X is called an exposed point if the singleton set
{x0} is a face, i.e. if there exists a supporting hyperplane H of X such that
X ∩H = {x0}.
a) Prove that every exposed point is an extreme point of X.
b) Give an example of a closed convex set in R2 with an extreme point that
is not exposed.
4.7 There is a more general deﬁnition of the face concept which runs as follows:
A face of a convex set X is a convex subset F of X such that every closed
line segment in X with a relative interior point in F lies entirely in F, i.e.
(a, b ∈X & ]a, b[ ∩F ̸= ∅) =⇒a, b ∈F.
Let us call faces according to this deﬁnition general faces in order to distin-
guish them from faces according to our old deﬁnition, which we call exposed
faces, provided they are proper, i.e. diﬀerent from the faces X and ∅.
The empty set ∅and X itself are apparently general faces of X, and all
extreme points of X are general faces, too.
Prove that the general faces of a convex set X have the following properties.
a) Each exposed face is a general face.
b) There is a convex set with a general face that is not an exposed face.
c) If F is a general face of X and F ′ is a general face of F, then F ′ is a
general face of X, but the coresponding result is not true in general for
exposed faces.
d) If F is a general face of X and C is an arbitrary convex subset of X such
that F ∩rint C ̸= ∅, then C ⊆F.
e) If F is a general face of X, then F = X ∩cl F. In particular, F is closed
if X is closed.
f) If F1 and F2 are two general faces of X and rint F1 ∩rint F2 ̸= ∅, then
F1 = F2.
g) If F is a general face of X and F ̸= X, then F ⊆rbdry X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
90
Polyhedra
Chapter 5
Polyhedra
We have already obtained some isolated results on polyhedra, but now is the
time to collect these and to complement them in order to get a complete
description of this important class of convex sets.
5.1
Extreme points and extreme rays
Polyhedra and extreme points
Each polyhedron X in Rn, except for the entire space, is an intersection of
ﬁnitely many closed halfspaces and may therefore be written in the form
X =
m

j=1
Kj,
with
Kj = {x ∈Rn | ⟨cj, x⟩≥bj}
for suitable nonzero vectors cj in Rn and real numbers bj. Using matrix
notation,
X = {x ∈Rn | Cx ≥b},
where C is an m × n-matrix with cjT as rows, and b =

b1
b2
. . .
bm
T.
Let
K◦
j = {x ∈Rn | ⟨cj, x⟩> bj} = int Kj,
and
Hj = {x ∈Rn | ⟨cj, x⟩= bj} = bdry Kj.
The sets K◦
j are open halfspaces, and the Hj are hyperplanes.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
91
Polyhedra
If b = 0, i.e. if all hyperplanes Hj are linear subspaces, then X is a
polyhedral cone.
The polyhedron X is clearly a subset of the closed halfspace Kj, which
is bounded by the hyperplane Hj. Let
Fj = X ∩Hj.
If there is a point in common between the hyperplane Hj and the polyhedron
X, without X being entirely contained in H, then H is a supporting hyper-
plane of X, and the set Fj is a proper face of X. But Fj is a face of X also
in the cases when X ∩Hj = ∅or X ⊆Hj, due to our convention regarding
non-proper faces. Of course, the faces Fj are polyhedra.
All points of a face Fj (proper as well as non-proper) are boundary points
of X. Since
X =
m

j=1
K◦
j ∪
m

j=1
Fj,
and all points in the open set m
j=1 K◦
j are interior points of X, we conclude
that
int X =
m

j=1
K◦
j
and
bdry X =
m

j=1
Fj.
The set ext X of extreme points of the polyhedron X is a subset of the
boundary m
j=1 Fj, and the extreme points are characterized by the following
theorem.
Theorem 5.1.1. A point x0 in the polyhedron X = m
j=1 Kj is an extreme
point if and only if there exists a subset I of the index set {1, 2, . . . , m} such
that 
j∈I Hj = {x0}.
Proof. Suppose there exists such an index set I. The intersection
F =

j∈I
Fj = X ∩

j∈I
Hj = {x0}
is a face of X, by Theorem 4.1.2, and x0 is obviously an extreme point of F.
Therefore, x0 is also an extreme point of X, by Theorem 4.1.3.
Now suppose, conversely, that there is no such index set I, and let J be
an index set that is maximal with respect to the property x0 ∈
j∈J Hj.
(Remember that the intersection over an empty index set is equal to the
entire space Rn, so J = ∅if x0 is an interior point of X.) The intersection

j∈J Hj is an aﬃne subspace, which by assumption consists of more than
one point and, therefore, contains a line {x0 + tv | t ∈R} through x0. The
line is obviously also contained in the larger set 
j∈J Kj.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
92
Polyhedra
92
Since x0 is an interior point of the halfspace Kj for all indices j /∈J,
we conclude that the points x0 + tv belong to all these halfspaces for all
suﬃciently small values of |t|. Consequently, there is a number δ > 0 such
that the line segment [x0 −δv, x0 +δv] lies in X = 
j∈J Kj ∩
j /∈J Kj, which
means that x0 is not an extreme point.
The condition 
j∈I Hj = {x0} means that the corresponding system of
linear equations
⟨cj, x⟩= bj,
j ∈I,
in n unknowns has a unique solution. A necessary condition for this to be
true is that the index set I contains at least n elements. And if the system
has a unique solution and there are more than n equations, then it is always
possible to obtain a quadratic subsystem with a uniqe solution by eliminating
suitably selected equations.
Hence, the condition m ≥n is necessary for the polyhedron X = m
j=1 Kj
to have at least one extreme point. (This also follows from Theorem 2.7.7,
for if m < n, then
dim lin X = dim{x ∈Rn | Cx = 0} = n −rank C ≥n −m > 0,
which means that X is not line-free.)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
93
Polyhedra
Theorem 5.1.1 gives us the following method for ﬁnding all extreme points
of the polyhedron X when m ≥n:
Solve for each subset J of {1, 2, . . . , m} with n elements the corresponding
linear system ⟨cj, x⟩= bj, j ∈J. If the system has a unique solution x0, and
the solution lies in X, i.e. satisﬁes the remaining linear inequalities ⟨cj, x⟩≥
bj, then x0 is an extreme point of X.
The number of extreme points of X is therefore bounded by
m
n

, which
is the number of subsets J of {1, 2, . . . , m} with n elements. In particular,
we have proved the following theorem.
Theorem 5.1.2. Polyhedra have ﬁnitely many extreme points.
Polyhedral cones and extreme rays
A polyhedral cone in Rn is an intersection X = m
j=1 Kj of conic halfspaces
Kj which are bounded by hyperplanes Hj through the origin, and the faces
Fj = X ∩Hj are polyhedral cones. Our next theorem is a direct analogue of
Theorem 5.1.1.
Theorem 5.1.3. A point x0 in the polyhedral cone X generates an extreme
ray R = −→
x0 of the cone if and only if −x0 /∈X and there exists a subset I of
the index set {1, 2, . . . , m} such that 
j∈I Hj = {tx0 | t ∈R}.
Proof. Suppose there exists such an index set I and that −x0 does not belong
to the cone X. Then

j∈I
Fj = X ∩

j∈I
Hj = R.
By Theorem 4.1.2, this means that R is a face of the cone X. The ray R
is an extreme ray of the face R, of course, so it follows from Theorem 4.1.3
that R is an extreme ray of X.
If −x0 belongs to X, then X is not a proper cone, and hence X has no
extreme rays according to Theorem 4.2.4.
It remains to show that R is not an extreme ray in the case when −x0 /∈X
and there is no index set I with the property that the intersection 
j∈I Hj is
equal to the line through 0 and x0. So let J be a maximal index set satisfying
the condition x0 ∈
j∈J Hj. Due to our assumption, the intersection 
j∈J Hj
is then a linear subspace of dimension greater than or equal to two, and
therefore it contains a vector v which is linearly independent of x0. The
vectors x0 + tv and x0 −tv both belong to 
j∈J Hj, and consequently also
to 
j∈J Kj, for all real numbers t. When |t| is a suﬃciently small number,
the two vectors also belong to the halfspaces Kj for indices j /∈J, because
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
94
Polyhedra
x0 is an interior point of Kj for these indices j. Therefore, there exists a
positive number δ such that the vectors x+ = x0 + δv and x−= x0 −δv both
belong to the cone X. The two vectors x+ and x−are linearly independent
and x0 = 1
2x+ + 1
2x−, so it follows that the ray R = −→
x0 lies between the two
rays −→
x+ and −→
x−in X, and R is therefore not an extreme ray.
Thus, to ﬁnd all the extreme rays of the cone
X =
m

j=1
{x ∈Rn | ⟨cj, x⟩≥0}
we should proceed as follows. First choose an index set J consisting of n −1
elements from the set {1, 2, . . . , m}.
This can be done in
 m
n−1

diﬀerent
ways. Then solve the corresponding homogeneous linear system ⟨cj, x⟩= 0,
j ∈J. If the solution set is one-dimensional, than pick a solution x0. If x0
satisﬁes the remaining linear inequalities and −x0 does not, then R = −→
x0 is
an extreme ray. If, instead, −x0 satisﬁes the remaining linear inequalities and
x0 does not, then −R is an extreme ray. Since this is the only way to obtain
extreme rays, we conclude that the number of extreme rays is bounded by
the number
 m
n−1

. In particular, we get the following corollary.
Theorem 5.1.4. Polyhedral cones have ﬁnitely many extreme rays.
5.2
Polyhedral cones
Theorem 5.2.1. A cone is polyhedral if and only if it is ﬁnitely generated.
Proof. We ﬁrst show that every polyhedral cone is ﬁnitely generated.
By Theorem 4.2.6, every polyhedral cone X can be written in the form
X = lin X + X ∩(lin X)⊥,
and X ∩(lin)⊥is a line-free, i.e. proper, polyhedral cone. Let B be a set
consisting of one point from each extreme ray of X ∩(lin X)⊥; then B is a
ﬁnite set and
X ∩(lin X)⊥= con B,
according to Theorems 5.1.4 and 4.2.4.
Let e1, e2, . . . , ed be a basis for the linear subspace lin X, and put e0 =
−(e1 + e2 + · · · + ed).
The cone lin X is generated as a cone by the set
A = {e0, e1, . . . , ed}, i.e.
lin X = con A.
Summing up,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
95
Polyhedra
95
X = lin X + X ∩(lin X)⊥= con A + con B = con(A ∪B),
which shows that the cone X is ﬁnitely generated by the set A ∪B.
Next, suppose that X is a ﬁnitely generated cone so that X = con A for
some ﬁnite set A. We start by the observation that the dual cone X+ is
polyhedral. Indeed, if A ̸= ∅then
X+ = A+ = {x ∈Rn | ⟨x, a⟩≥0 for all a ∈A} =

a∈A
{x ∈Rn | ⟨a, x⟩≥0}
is an intersection of ﬁnitely many conical halfspaces, i.e. a polyhedral cone.
And if A = ∅, then X = {0} and X+ = Rn.
The already proven part of the theorem now implies that the dual cone
X+ is ﬁnitely generated. But the dual cone of X+, i.e. the bidual cone X++,
is then polyhedral, too. Since the bidual cone X++ coincides with the original
cone X, by Corollary 3.2.4, we conclude the X is a polyhedral cone.
We are now able two prove two results that were left unproven in Chap-
ter 2.6; compare Corollary 2.6.9.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
96
Polyhedra
Theorem 5.2.2.
(i) The intersection X ∩Y of two ﬁnitely generated cones
X and Y is a ﬁnitely generated cone.
(ii) The inverse image T −1(X) of a ﬁnitely generated cone X under a linear
map T is a ﬁnitely generated cone.
Proof. The intersection of two conical polyhedra is obviously a conical poly-
hedron, and the same holds for the inverse image of a conical polyhedron un-
der a linear map. The theorem is therefore a corollary of Theorem 5.2.1.
5.3
The internal structure of polyhedra
Polyhedra are by deﬁnition intersections of ﬁnite collections of closed half-
spaces, and this can be viewed as an external description of polyhedra. We
shall now give an internal description of polyhedra in terms of extreme points
and extreme rays, and the following structure theorem is the main result of
this chapter.
Theorem 5.3.1. A nonempty subset X of Rn is a polyhedron if and only if
there exist two ﬁnite subsets A and B of Rn with A ̸= ∅such that
X = cvx A + con B.
The cone con B is then equal to the recession cone recc X of X. If the
polyhedron is line-free, we may choose for A the set ext X of all extreme points
of X, and for B a set consisting of one nonzero point from each extreme ray
of the recession cone recc X.
Proof. We ﬁrst prove that polyhedra have the stated decomposition.
So
let X be a polyhedron and put Y = X ∩(lin X)⊥. Then, Y is a line-free
polyhedron, and
X = lin X + Y = lin X + recc Y + cvx(ext Y ),
by Theorems 4.2.6 and 4.2.2. The two polyhedral cones lin X and recc Y
are, according to Theorem 5.2.1, generated by two ﬁnite sets B1 and B2,
respectively, and their sum is generated by the ﬁnite set B = B1 ∪B2. The
set ext Y is ﬁnite, by Theorem 5.1.2, so the representation
X = cvx A + con B
is now obtained by taking A = ext Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
97
Polyhedra
e1
b1
b2
0
a1
a2
lin X
recc(X ∩(lin X)⊥)
X
X ∩(lin X)⊥
Figure 5.1. An illustration for Theorem 5.3.1. The right part of the ﬁgure depicts
an unbounded polyhedron X in R3. Its recessive subspace lin X is one-dimensional
and is generated as a cone by e1 and -e1. The intersection X ∩(lin X)⊥, which
is shadowed, is a line-free polyhedron with two extreme points a1 and a2. The
recession cone recc(X ∩(lin X)⊥) is generated by b1 and b2. The representation
X = cvx A + con B is obtained by taking A = {a1, a2} and B = {e1, −e1, b1, b2}.
The cone con B is closed and the convex set cvx A is compact, since the
sets A and B are ﬁnite. Hence, con B = recc X by Corollary 2.7.13.
If X is a line-free polyhedron, then
X = cvx(ext X) + con(exr(recc X)),
by Theorems 4.2.2 and 4.2.4, and this gives us the required representation of
X with A = ext X and with B as a set consisting of one nonzero point from
each extreme ray of recc X.
To prove the converse, suppose that X = cvx A + con B, where A =
{a1, . . . , ap} and B = {b1, . . . , bq} are ﬁnite sets. Consider the cone Y in
Rn × R that is generated by the ﬁnite set (A × {1}) ∪(B × {0}). The cone
Y is polyhedral according to Theorem 5.2.1, which means that there is an
m × (n + 1)-matrix C such that
(5.1)
(x, xn+1) ∈Y
⇔C
 x
xn+1

≥0.
(Here
 x
xn+1

denotes the vector (x1, . . . , xn, xn+1) written as a column ma-
trix.)
Let C′ denote the submatrix of C which consists of all columns but the
last, and let c′ be the last column of the matrix C. Then
C
 x
xn+1

= C′x + xn+1c′,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
98
Polyhedra
98
which means that the equivalence (5.1) may be written as
(x, xn+1) ∈Y
⇔C′x + xn+1c′ ≥0.
By deﬁnition, a vector (x, 1) ∈Rn × R belongs to the cone Y if and only if
there exist nonnegative numbers λ1, λ2, . . . , λp and µ1, µ2, . . . , µq such that

x = λ1a1 + λ2a2 + · · · + λpap + µ1b1 + µ2b2 + · · · + µqbq
1 = λ1 + λ2 + · · · + λp
i.e. if and only if x ∈cvx A + con B. This yields the equivalences
x ∈X
⇔(x, 1) ∈Y
⇔C′x + c′ ≥0,
which means that X = {x ∈Rn | C′x ≥−c′}. Thus, X is a polyhedron.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
99
Polyhedra
5.4
Polyhedron preserving operations
Theorem 5.4.1. The intersection of ﬁnitely many polyhedra in Rn is a poly-
hedron.
Proof. Trivial.
Theorem 5.4.2. Suppose X is a polyhedron in Rn and that T : Rn →Rm is
an aﬃne map. The image T(X) is then a polyhedron in Rm.
Proof. The assertion is trivial if the polyhedron is empty, so suppose it is
nonempty and write it in the form
X = cvx A + con B,
where A = {a1, . . . , ap} and B = {b1, . . . , bq} are ﬁnite sets. Each x ∈X has
then a representation of the form
x =
p

j=1
λjaj +
q

j=1
µjbj =
p

j=1
λjai +
q

j=1
µjbj −(
q

j=1
µj)0
with nonnegative coeﬃcients λj and µj and p
j=1 λj = 1, i.e. each x ∈X is
an aﬃne combination of elements in the set A ∪B ∪{0}. Since T is an aﬃne
map,
Tx =
p

j=1
λjTaj +
q

j=1
µjTbj −(
q

j=1
µj)T0 =
p

j=1
λjTaj +
q

j=1
µj(Tbj −T0).
This shows that the image T(X) is of the form
T(X) = cvx A′ + con B′
with A′ = T(A) and B′ = −T0 + T(B) = {Tb1 −T0, . . . , Tbq −T0}. So the
image T(X) is a polyhedron, by Theorem 5.3.1.
Theorem 5.4.3. Suppose Y is a polyhedron in Rm and that T : Rn →Rm
is an aﬃne map. The inverse image T −1(Y ) is then a polyhedron in Rn.
Proof. First assume that Y is a closed halfspace in Rm (or the entire space
Rm), i.e. that Y = {y ∈Rm | ⟨c, y⟩≥b}. (The case Y = Rm is obtained by
c = 0 and b = 0.) The aﬃne map T can be written in the form Tx = Sx+y0,
with S as a linear map and y0 as a vector in Rm. This gives us
T −1(Y ) = {x ∈Rn | ⟨c, Tx⟩≥b} = {x ∈Rn | ⟨STc, x⟩≥b −⟨c, y0⟩}.
So T −1(Y ) is a closed halfspace in Rn if STc ̸= 0, the entire space Rn if
STc = 0 and b ≤⟨c, y0⟩, and the empty set ∅if STc = 0 and b > ⟨c, y0⟩.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
100
Polyhedra
In the general case, Y = p
j=1 Kj is an intersection of ﬁnitely many closed
halfspaces. Since S−1(Y ) = p
j=1 S−1(Kj), the inverse image S−1(Y ) is an
intersection of closed halfspaces, the empty set, or the entire space Rn. Thus,
S−1(Y ) is a polyhedron.
Theorem 5.4.4. The Cartesian product X × Y of two polyhedra X and Y is
a polyhedron.
Proof. Suppose X lies in Rm and Y lies in Rn. The set X × Rn is a poly-
hedron since it is the inverse image of X under the projection (x, y) →x,
and Rm × Y is a polyhedron for a similar reason. It follows that X × Y is a
polyhedron, because X × Y = (X × Rn) ∩(Rm × Y ).
Theorem 5.4.5. The sum X + Y of two polyhedra in Rn is a polyhedron.
Proof. The sum X + Y is equal to the image of X × Y under the linear map
(x, y) →x + y, so the theorem is a consequence of the previous theorem and
Theorem 5.4.2.
5.5
Separation
It is possible to obtain sharper separation results for polyhedra than for
general convex sets. Compare the following two theorems with Theorems
3.1.6 and 3.1.5.
Theorem 5.5.1. If X and Y are two disjoint polyhedra, then there exists a
hyperplane that strictly separates the two polyhedra.
Proof. The diﬀerence X −Y of two polyhedra X and Y is a closed set, since
it is a polyhedron according to Theorem 5.4.5. So it follows from Theorem
3.1.6 that there exists a hyperplane that strictly separates the two polyhedra,
if they are disjoint.
Theorem 5.5.2. Let X be a convex set, and let Y be a polyhedron that is
disjoint from X. Then there exists a hyperplane that separates X and Y and
does not contain X as a subset.
Proof. We prove the theorem by induction over the dimension n of the sur-
rounding space Rn.
The case n = 1 is trivial, so suppose the assertion of the theorem is true
when the dimension is n −1, and let X be a convex subset of Rn that is
disjoint from the polyhedron Y . An application of Theorem 3.1.5 gives us
a hyperplane H that separates X and Y and, as a consequence, does not
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
101
Polyhedra
101
contain both sets as subsets. If X is not contained in H, then we are done.
So suppose that X is a subset of H. The polyhedron Y then lies in one of the
two closed halfspaces deﬁned by the hyperplane H. Let us denote this closed
halfspace by H+, so that Y ⊆H+, and let H++ denote the corresponding
open halfspace.
If Y ⊆H++, then Y and H are disjoint polyhedra, and an application
of Theorem 5.5.1 gives us a hyperplane that strictly separates Y and H. Of
course, this hyperplane also strictly separates Y and X, since X is a subset
of H.
This proves the case Y ⊆H++, so it only remains to consider the case
when Y is a subset of the closed halfspace H+ without being a subset of the
corresponding open halfspace, i.e. the case
Y ⊆H+, Y ∩H ̸= ∅.
Due to our induction hypothesis, it is possible to separate the nonempty
polyhedron Y1 = Y ∩H and X inside the (n −1)-dimensional hyperplane H
using an aﬃne (n −2)-dimensional subset L of H which does not contain X
as a subset. L divides the hyperplane H into two closed halves L+ and L−
with L as their common relative boundary, and with X as a subset of L−
and Y1 as a subset of L+.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
102
Polyhedra
Let us denote the relative interior of L−by L−−, so that L−−= L−\ L.
The assumption that X is not a subset of L implies that X ∩L−−̸= ∅.
Observe that Y ∩L−= Y1∩L. If Y1∩L = ∅, then there exists a hyperplane
that strictly separates the polyhedra Y och L−, by Theorem 5.5.1, and since
X ⊆L−, we are done in this case, too.
What remains is to treat the case Y1 ∩L ̸= ∅, and by performing a
translation, if necessary, we may assume that the origin lies in Y1 ∩L, which
implies that L is a linear subspace. See ﬁgure 5.2.
X
Y
L+
L−
Y1
H
H+
L
Figure 5.2. Illustration for the proof of Theorem 5.5.2.
Note that the set H++ ∪L+ is a cone and that Y is a subset of this cone.
Now, consider the cone con Y generated by the polyhedron Y , and let
C = L + con Y.
C is a cone, too, and a subset of the cone H++ ∪L+, since Y and L are both
subsets of the last mentioned cone. The cone con Y is polyhedral, because
if the polyhedron Y is written as Y = cvx A + con B with ﬁnite sets A and
B, then con Y = con(A ∪B) due to the fact that 0 lies in Y . Since the
sum of two polyhedral cones is polyhedral, it follows that the cone C is also
polyhedral.
The cone C is disjoint from the set L−−, since the sets L−−and H++∪L+
are disjoint.
Now write the polyhedral cone C as an intersection  Ki of ﬁnitely many
closed halfspaces Ki which are bounded by hyperplanes Hi through the origin.
Each halfspace Ki is a cone containing Y as well as L. If a given halfspace Ki
contains in addition a point from L−−, then it contains the cone generated
by that point and L, that is all of L−.
Therefore, since C =  Ki and
C ∩L−−= ∅, we conclude that there exists a halfspace Ki that does not
contain any point from L−−. In other words, the corresponding boundary
hyperplane Hi separates L−and the cone C and is disjoint from L−−. Since
X ⊆L−, Y ⊆C and X ∩L−−̸= ∅, Hi separates the sets X and Y and
does not contain X. This completes the induction step and the proof of the
theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
103
Polyhedra
103
Exercises
5.1 Find the extreme points of the following polyhedra X:
a) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≥2, x2 ≥−1}
b) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≤2, x2 ≥−1}
c) X = {x ∈R3 | 2x1 + x2 + x3 ≤4, x1 + 2x2 + x3 ≤4, x ≥0}
d) X = {x ∈R4 | x1 + x2 + 3x3 + x4 ≤4, 2x2 + 3x3 ≥5, x ≥0}.
5.2 Find the extreme rays of the cone
X = {x ∈R3 | x1 −x2 + 2x3 ≥0, x1 + 2x2 −2x3 ≥0, x2 + x3 ≥0, x3 ≥0}.
5.3 Find a matrix C such that
con{(1, −1, 1), (−1, 0, 1), (3, 2, 1), (−2, −1, 0)} = {x ∈R3 | Cx ≥0}.
5.4 Find ﬁnite sets A and B such that X = con A + cvx B for the following
polyhedra:
a) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≥2, x2 ≥−1}
b) X = {x ∈R2 | −x1 + x2 ≤2, x1 + 2x2 ≤2, x2 ≥−1}
c) X = {x ∈R3 | 2x1 + x2 + x3 ≤4, x1 + 2x2 + x3 ≤4, x ≥0}
d) X = {x ∈R4 | x1 + x2 + 3x3 + x4 ≤4, 2x2 + 3x3 ≥5, x ≥0}.
5.5 Suppose 0 lies in the polyhedron X = cvx A + con B, where A and B are
ﬁnite sets. Prove that con X = con(A ∪B).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
104
Convex functions
Chapter 6
Convex functions
6.1
Basic deﬁnitions
Epigraph and sublevel set
Deﬁnition. Let f : X →R be a function with domain X ⊆Rn and codomain
R, i.e. the real numbers extended with ∞. The set
epi f = {(x, t) ∈X × R | f(x) ≤t}
is called the epigraph of the function.
Let α be a real number. The set
sublevα f = {x ∈X | f(x) ≤α}
is called a sublevel set of the function, or more precisely, the α-sublevel set.
The epigraph is a subset of Rn+1, and the word ’epi’ means above. So
epigraph means above the graph.
Rn
R
α
sublevα f
epi f
Figure 6.1. Epigraph and a sublevel set
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
105
Convex functions
x
y
Figure 6.2. The graph of a convex function
We remind the reader of the notation dom f for the eﬀective domain of
f, i.e. the set of points where the function f : X →R is ﬁnite. Obviously,
dom f = {x ∈X | f(x) < ∞}
is equal to the union of all the sublevel sets of f, and these form an increasing
family of sets, i.e.
dom f =

α∈R
sublevα f
and
α < β ⇒sublevα f ⊆sublevβ f.
This implies that dom f is a convex set if all the sublevel sets are convex.
Convex functions
Deﬁnition. A function f : X →R is called convex if its domain X and
epigraph epi f are convex sets.
A function f : X →R is called concave if the function −f is convex.
Example 6.1.1. The epigraph of an aﬃne function is a closed halfspace. All
aﬃne functions, and in particular all linear functions, are thus convex and
concave.
Example 6.1.2. The exponential funcion ex with R as domain of deﬁnition
is a convex function.
To see this, we replace x with x−a in the elementary inequality ex ≥x+1
and obtain the inequality ex ≥(x−a)ea+ea, which implies that the epigraph
of the exponential function can be expressed as the intersection

a∈R
{(x, y) ∈R2 | y ≥(x −a)ea + ea}
of a family of closed halfspaces in R2. The epigraph is thus convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
106
Convex functions
106
Theorem 6.1.1. The eﬀective domain dom f and the sublevel sets sublevα f
of a convex function f : X →R are convex sets.
Proof. Suppose that the domain X is a subset of Rn and consider the pro-
jection P1 : Rn × R →Rn of Rn × R onto its ﬁrst factor, i.e. P1(x, t) = x.
Let furthermore Kα denote the closed halfspace {x ∈Rn+1 | xn+1 ≤α}.
Then sublevα f = P1(epi f ∩Kα), for
f(x) ≤α ⇔∃t: f(x) ≤t ≤α ⇔∃t: (x, t) ∈epi f ∩Kα
⇔x ∈P1(epi f ∩Kα).
The intersections epi f ∩Kα are convex sets, and since convexity is preserved
by linear maps, it follows that the sublevel sets sublevα f are convex. Con-
sequently, their union dom f is also convex.
Quasiconvex functions
Many important properties of convex functions are consequences of the mere
fact that their sublevel sets are convex. This is the reason for paying special
attention to functions with convex sublevel sets and motivates the following
deﬁnition.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
107
Convex functions
Deﬁnition. A function f : X →R is called quasiconvex if X and all its
sublevel sets sublevα f are convex.
A function f : X →R is called quasiconcave if −f is quasiconvex.
Convex functions are quasiconvex since their sublevel sets are convex. The
converse is not true, because a function f that is deﬁned on some subinterval
I of R is quasiconvex if it is increasing on I, or if it is decreasing on I, or
more generally, if there exists a point c ∈I such that f is decreasing to the
left of c and increasing to the right of c. There are, of course, non-convex
functions of this type.
Convex extensions
The eﬀective domain dom f of a convex (quasiconvex) function f : X →R
is convex, and since
epi f = {(x, t) ∈dom f × R | f(x) ≤t}
and
sublevα f = {x ∈dom f | f(x) ≤α},
the restriction f|dom f of f to dom f is also a convex (quasiconvex) function,
and the restriction has the same epigraph and the same α-sublevel sets as f.
So what is the point of allowing ∞as a function value of a convex func-
tion? We are of course primarily interested in functions with ﬁnite values but
functions with inﬁnite values arise naturally as suprema or limits of sequences
of functions with ﬁnite values.
Another beneﬁt of allowing ∞as a function value of (quasi)convex func-
tions is that we can without restriction assume that they are deﬁned on the
entire space Rn. For if f : X →R is a (quasi)convex function deﬁned on a
proper subset X of Rn, and if we deﬁne the function ˜f : Rn →R by
˜f(x) =

f(x)
if x ∈X
∞
if x /∈X,
then f and ˜f have the same epigraphs and the same α-sublevel sets. The
extension ˜f is therefore also (quasi)convex. Of course, dom ˜f = dom f.
(Quasi)concave functions have an analogous extension to functions with
values in R = R ∪{−∞}.
Alternative characterization of convexity
Theorem 6.1.2. A function f : X →R with a convex domain of deﬁnition
X is
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
108
Convex functions
(a) convex if and only if
(6.1)
f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y)
for all points x, y ∈X and all numbers λ ∈]0, 1[;
(b) quasiconvex if and only if
(6.2)
f(λx + (1 −λ)y) ≤max{f(x), f(y)}
for all points x, y ∈X and all numbers λ ∈]0, 1[.
Proof. (a) Suppose f is convex, i.e. that the epigraph epi f is convex, and
let x and y be two points in dom f. Then the points (x, f(x)) and (y, f(y))
belong to the epigraph, and the convexity of the epigraph implies that the
convex combination

λx + (1 −λ)y, λf(x) + (1 −λ)f(y)

of these two points also belong to the epigraph. This statement is equivalent
to the inequality (6.1) being true. If any of the points x, y ∈X lies outside
dom f, then the inequality is trivially satisﬁed since the right hand side is
equal to ∞in that case.
To prove the converse, we assume that the inequality (6.1) holds. Let
(x, s) and (y, t) be two points in the epigraph, and let 0 < λ < 1. Then
f(x) ≤s and f(y) ≤t, by deﬁnition, and it therefore follows from the
inequality (6.1) that
f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) ≤λs + (1 −λ)t,
so the point (λx+(1−λ)y, λs+(1−λ)t), i.e. the point λ(x, s)+(1−λ)(y, t),
lies in the epigraph. In orther words, the epigraph is convex.
(b) The proof is analogous and is left to the reader.
A function f : X →R is clearly (quasi)convex if and only if the restriction
f|L is (quasi)convex for each line L that intersects X. Each such line has
an equation of the form x = x0 + tv, where x0 is a point in X and v is a
vector in Rn, and the corresponding restriction is a one-variable function
g(t) = f(x0 + tv) (with the set {t | x0 + tv ∈X} as its domain of deﬁnition).
To decide whether a function is (quasi)convex or not is thus essentially a
one-variable problem.
Deﬁnition. Let f : X →R be a function deﬁned on a convex cone X. The
function is called
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
109
Convex functions
109
• subadditive if f(x + y) ≤f(x) + f(y) for all x, y ∈X;
• positive homogeneous if f(αx) = αf(x) for all x ∈X and all α ∈R+.
Every positive homogeneous, subadditive function is clearly convex. Con-
versely, every convex, positive homogeneous function f is subadditive, be-
cause
f(x + y) = 2f( 1
2x + 1
2y) ≤2(1
2f(x) + 1
2f(y)) = f(x) + f(y).
A seminorm on Rn is a function f : Rn →R, which is subadditive,
positive homogeneous, and symmetric, i.e. satisﬁes the condition
f(−x) = f(x)
for all x ∈Rn.
The symmetry and homogenouity conditions may of course be merged to
the condition
f(αx) = |α|f(x) for all x ∈Rn and all α ∈R.
If f is a seminorm, then f(x) ≥0 for all x, since
0 = f(0) = f(x −x) ≤f(x) + f(−x) = 2f(x).
A seminorm f is called a norm if f(x) = 0 implies x = 0. The usual
notation for a norm is ∥·∥.
Seminorms, and in particular norms, are convex functions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
110
Convex functions
Example 6.1.3. The Euclidean norm and the ℓ1-norm, that were deﬁned in
Chapter 1, are special cases of the ℓp-norms ∥·∥p on Rn. They are deﬁned
for 1 ≤p < ∞by
∥x∥p =
 n

i=1
|xi|p1/p,
and for p = ∞by
∥x∥∞= max
1≤i≤n |xi|.
The maximum norm ∥·∥∞is a limiting case, because ∥x∥p →∥x∥∞as p →∞.
The ℓp-norms are obviously positive homogeneous and symmetric and
equal to 0 only if x = 0. Subadditivity is an immediate consequence of the
triangle inequality |x + y| ≤|x| + |y| for real numbers when p = 1 or p = ∞,
and of the Cauchy–Schwarz inequality when p = 2. For the remaining values
of p, subadditivity will be proved in Section 6.4 (Theorem 6.4.3).
Strict convexity
By strengthening the inequalities in the alternative characterization of con-
vexity, we obtain the following deﬁnitions.
Deﬁnition. A convex function f : X →R is called strictly convex if
f(λx + (1 −λ)y) < λf(x) + (1 −λ)f(y)
for all pairs of distinct points x, y ∈X and all λ ∈]0, 1[.
A quasiconvex function f is called strictly quasiconvex if inequality (6.2)
is strict for all pairs of distinct points x, y ∈X and all λ ∈]0, 1[.
A function f is called strictly concave (strictly quasiconcave) if the func-
tion −f is strictly convex (strictly quasiconvex).
Example 6.1.4. A quadratic form q(x) = ⟨x, Qx⟩= n
i,j=1 qijxixj on Rn is
convex if and only if it is positive semideﬁnite, and the form is strictly convex
if and only if it is positive deﬁnite. This follows from the identity

λxi+(1−λ)yi

λxj+(1−λ)yj

= λxixj+(1−λ)yiyj−λ(1−λ)(xi−yi)(xj−yj)
which after multiplication by qij and summation yields the equality
q(λx + (1 −λ)y) = λq(x) + (1 −λ)q(y) −λ(1 −λ)q(x −y).
The right hand side is ≤λq(x) + (1 −λ)q(y) for all 0 < λ < 1 if and only if
q(x−y) ≥0, which holds for all x ̸= y if and only if q is positive semideﬁnite.
Strict inequality requires q to be positive deﬁnite.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
111
Convex functions
Jensen’s inequality
The inequalities (6.1) and (6.2) are easily extended to convex combinations
of more than two points.
Theorem 6.1.3. Let f be a function and suppose x = λ1x1+λ2x2+· · ·+λmxm
is a convex combination of the points x1, x2, . . . , xm in the domain of f.
(a) If f is convex, then
(6.3)
f(x) ≤
m

j=1
λjf(xj).
(Jensen’s inequality)
If f is strictly convex and λj > 0 for all j, then equality prevails in (6.3)
if and only if x1 = x2 = · · · = xm.
(b) If f is quasiconvex, then
(6.4)
f(x) ≤max
1≤j≤m f(xj).
If f is strictly quasiconvex and λj > 0 for all j, then equality prevails in
(6.4) if and only if x1 = x2 = · · · = xm.
Proof. (a) To prove the Jensen inequality we may assume that all coeﬃcients
λj are positive and that all points xj lie in dom f, because the right hand
side of the inequality is inﬁnite if some point xj lies outside dom f. Then

x,
m

j=1
λjf(xj)

=
m

j=1
λj

xj, f(xj)

,
and the right sum, being a convex combination of elements in the epigraph
epi f, belongs to epi f. So the left hand side is a point in epi f, and this gives
us inequality (6.3).
Now assume that f is strictly convex and that we have equality in Jensen’s
inequality for the convex combination x = m
j=1 λjxj, with positive coeﬃ-
cents λj and m ≥2. Let y = m
j=2 λj(1−λ1)−1xj. Then x = λ1x1+(1−λ1)y,
and y is a convex combination of x2, x3, . . . , xm, so it follows from Jensen’s
inequality that
m

j=1
λjf(xj) = f(x) ≤λ1f(x1) + (1 −λ1)f(y)
≤λ1f(x1) + (1 −λ1)
m

j=2
λj(1 −λ1)−1f(xj) =
m

j=1
λjf(xj).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
112
Convex functions
112
Since the left hand side and right hand side of this chain of inequalities
and equalities are equal, we conclude that equality holds everywhere. Thus,
f(x) = λ1f(x1) + (1 −λ1)f(y), and since f is strictly convex, this implies
that x1 = y = x.
By symmetri, we also have x2 = x, . . . , xm = x, and hence x1 = x2 =
· · · = xm.
(b) Suppose f is quasiconvex, and let α = max1≤j≤m f(xj). If any of the
points xj lies outside dom f, then there is nothing to prove since the right
hand side of the inequality (6.4) is inﬁnite. In the opposite case, α is a ﬁnite
number, and each point xj belongs to the convex sublevel set sublevα f, and
it follows that so does the point x. This proves inequality (6.4).
The proof of the assertion about equality for strictly quasiconvex func-
tions is analogous with the corresponding proof for strictly convex func-
tions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
113
Convex functions
6.2
Operations that preserve convexity
We now describe some ways to construct new convex functions from given
convex functions.
Conic combination
Theorem 6.2.1. Suppose that f : X →R and g: X →R are convex func-
tions and that α and β are nonnegative real numbers. Then αf + βg is also
a convex function.
Proof. Follows directly from the alternative characterization of convexity in
Theorem 6.1.2.
The set of convex functions on a given set X is, in other words, a convex
cone. So every conic combination α1f1+α2f2+· · ·+αmfm of convex functions
on X is convex.
Note that there is no counterpart of this statement for quasiconvex func-
tions −a sum of quasiconvex functions is not necessarily quasiconvex.
Pointwise limit
Theorem 6.2.2. Suppose that the functions fi : X →R, i = 1, 2, 3, . . . , are
convex and that the limit
f(x) = lim
i→∞fi(x)
exists as a ﬁnite number or ∞for each x ∈X. The limit function f : X →R
is then also convex.
Proof. Let x and y be two points in X, and suppose 0 < λ < 1. By passing
to the limit in the inequality fi(λx + (1 −λ)y) ≤λfi(x) + (1 −λ)fi(y) we
obtain the following inequality
f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y),
which tells us that the limit function f is convex.
Using Theorem 6.2.2, we may extend the result in Theorem 6.2.1 to in-
ﬁnite sums and integrals. For example, a pointwise convergent inﬁnite sum
f(x) = ∞
i=1 fi(x) of convex functions is convex.
And if f(x, y) is a function that is convex with respect to the variable
x on some set X for each y in a set Y , α is a nonnegative function deﬁned
on Y , and the integral g(x) =

Y α(y)f(x, y) dy exists for all x ∈X, then
g is a convex function on X. This follows from Theorem 6.2.2 by writing
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
114
Convex functions
the integral as a limit of Riemann sums, or more directly, by integrating the
inequalites that characterize the convexity of the functions f(·, y).
Composition with aﬃne maps
Theorem 6.2.3. Suppose A: V →Rn is an aﬃne map, that Y is a convex
subset of Rn, and that f : Y →R is a convex function. The composition
f ◦A is then a convex function on its domain of deﬁnition A−1(Y )
Proof. Let g = f ◦A. Then, for x1, x2 ∈A−1(Y ) and 0 < λ < 1,
g(λx1 + (1 −λ)x2) = f(λAx1 + (1 −λ)Ax2) ≤λf(Ax1) + (1 −λ)f(Ax2)
= λg(x1) + (1 −λ)g(x2),
which shows that the function g is convex.
The composition f ◦A of a quasiconvex function f and an aﬃne map A
is quasiconvex.
Example 6.2.1. The function x →ec1x1+···+cnxn is convex on Rn since it is a
composition of a linear map and the convex exponential function t →et.
Pointwise supremum
Theorem 6.2.4. Let fi : X →R, i ∈I, be a family of functions, and deﬁne
the function f : X →R for x ∈X by
f(x) = sup
i∈I
fi(x).
Then
(i) f is convex if the functions fi are all convex;
(ii) f is quasiconvex if the functions fi are all quasiconvex.
Proof. By the least upper bound deﬁnition, f(x) ≤t if and only if fi(x) ≤t
for all i ∈I, and this implies that
epi f =

i∈I
epi fi
and
sublevt f =

i∈I
sublevt fi
for all t ∈R. The assertions of the theorem are now immediate consequences
of the fact that intersections of convex sets are convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
115
Convex functions
115
f
f1
f2
f3
Figure 6.3. f = sup fi for a family consisting of three functions.
Example 6.2.2. A pointwise maximum of ﬁnitely many aﬃne functions, i.e.
a function of the form
f(x) = max
1≤i≤m(⟨ci, x⟩+ ai),
is a convex function and is called a convex piecewise aﬃne function.
Example 6.2.3. Examples of convex piecewise aﬃne functions f on Rn are:
(a) The absolute value of the i:th coordinate of a vector
f(x) = |xi| = max{xi, −xi}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
116
Convex functions
(b) The maximum norm
f(x) = ∥x∥∞= max
1≤i≤n |xi|.
(c) The sum of the m largest coordinates of a vector
f(x) = max{xi1 + · · · + xim | 1 ≤i1 < i2 < · · · < im ≤n}.
Composition
Theorem 6.2.5. Suppose that the function φ: I →R is deﬁned on a real
intervall I that contains the range f(X) of the function f : X →R. The
composition φ ◦f : X →R is convex
(i) if f is convex and φ is convex and increasing;
(ii) if f is concave and φ is convex and decreasing.
Proof. The inequality
φ

f(λx + (1 −λ)y)

≤φ

λf(x) + (1 −λ)f(y)

holds for x, y ∈X and 0 < λ < 1 if either f is convex and φ is increasing, or
f is concave and φ is decreasing. If in addition φ is convex, then
φ

λf(x) + (1 −λ)f(y)

≤λφ(f(x)) + (1 −λ)φ(f(y)),
and by combining the two inequalities above, we obtain the inequality that
shows that the function φ ◦f is a convex.
There is a corresponding result for quasiconvexity: The composition φ◦f
is quasiconvex if either f is quasiconvex and φ is increasing, or f is quasi-
concave and φ is decreasing.
Example 6.2.4. The function
x →ex2
1+x2
2+···+x2
k,
where 1 ≤k ≤n, is convex on Rn, since the exponential function is convex
and increasing, and positive semideﬁnite quadratic forms are convex.
Example 6.2.5. The two functions t →1/t and t →−ln t are convex and
decreasing on the interval ]0, ∞[.
So the function 1/g is convex and the
function ln g is concave, if g is a concave and positive function.
Inﬁmum
Theorem 6.2.6. Let C be a convex subset of Rn+1, and let g be the function
deﬁned for x ∈Rn by
g(x) = inf{t ∈R | (x, t) ∈C},
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
117
Convex functions
with the usual convention inf ∅= +∞. Suppose there exists a point x0 in the
relative interior of the set
X0 = {x ∈Rn | g(x) < ∞} = {x ∈Rn | ∃t ∈R: (x, t) ∈C}
with a ﬁnite function value g(x0). Then g(x) > −∞for all x ∈Rn, and
g: Rn →R is a convex function with X0 as its eﬀective domain.
Proof. Let x be an arbitrary point in X0. To show that g(x) > −∞, i.e. that
the set
Tx = {t ∈R | (x, t) ∈C}
is bounded below, we ﬁrst choose a point x1 ∈rint X0 such that x0 lies on
the open line segment ]x, x1[, and write x0 = λx + (1 −λ)x1 with 0 < λ < 1.
We then ﬁx a real number t1 such that (x1, t1) ∈C, and for t ∈Tx deﬁne
the number t0 as t0 = λt + (1 −λ)t1. The pair (x0, t0) is then a convex
combination of the points (x, t) and (x1, t1) in C, so
g(x0) ≤t0 = λt + (1 −λ)t1,
by convexity and the deﬁnition of g. We conclude that
t ≥1
λ

g(x0) −(1 −λ)t1

,
and this inequality shows that the set Tx is bounded below.
So the function g has R as codomain, and dom g = X0. Now, let x1 and
x2 be arbitrary points in X0, and let λ1 and λ2 be two positive numbers with
sum 1. To each ϵ > 0 there exist two real numbers t1 and t2 such that the
two points (x1, t1) and (x2, t2) lie in C and t1 < g(x1) + ϵ and t2 < g(x2) + ϵ.
The convex combination (λ1x1 + λ2x2, λ1t1 + λ2t2) of the two points lies in
C, too, and
g(λ1x1 + λ2x2) ≤λ1t1 + λ2t2 ≤λ1g(x1) + λ2g(x2) + ϵ.
This means that the point λ1x1 + λ2x2 lies in X0, and by letting ϵ tend to 0
we conclude that g(λ1x1 + λ2x2) ≤λ1g(x1) + λ2g(x2). Hence, the set X0 is
convex, and the function g is convex.
We have seen that the pointwise supremum f(x) = supi∈I fi(x) of an
arbitrary family of convex functions is convex. So if f : X × Y →R is a
function with the property that the functions f(·, y) are convex on X for
each y ∈Y , and we deﬁne the function g on X by g(x) = supy∈Y f(x, y),
then g is convex, and this is true without any further conditions on the set
Y . Our next theorem shows that the corresponding inﬁmum is a convex
function, provided f is convex as a function on the product set X × Y .
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
118
Convex functions
118
Theorem 6.2.7. Suppose f : X × Y →R is a convex function, and for each
x ∈X deﬁne
g(x) = inf
y∈Y f(x, y).
If there is a point x0 ∈rint X such that g(x0) > −∞, then g(x) is a ﬁnite
number for each x ∈X, and g: X →R is a convex function.
Proof. Suppose X is a subset of Rn and let
C = {(x, t) ∈X × R | ∃y ∈Y : f(x, y) ≤t}.
C is a convex subset of Rn+1, because given two points (x1, t1) and (x2, t2)
in C, and two positive numbers λ1 and λ2 with sum 1, there exist two points
y1 and y2 in the convex set Y such that f(xi, yi) ≤ti for i = 1, 2, and this
implies that
f(λ1x1 + λ2x2, λ1y1 + λ2y2) ≤λ1f(x1, y1) + λ2f(x2, y2) ≤λ1t1 + λ2t2,
which shows that the convex combination λ1(x1, t1) + λ2(x2, t2) lies in C.
Moreover, g(x) = inf{t | (x, t) ∈C}, so the corollary follows immediately
from Theorem 6.2.6.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
119
Convex functions
Perspective
Deﬁnition. Let f : X →R be a function deﬁned on a cone X in Rn. The
function g: X × R++ →R, deﬁned by
g(x, s) = sf(x/s),
is called the perspective of f.
Theorem 6.2.8. The perspective g of a convex function f : X →R with a
convex cone X as domain is a convex function.
Proof. Let (x, s) and (y, t) be two points in X × R++, and let α, β be two
positive numbers with sum 1. Then
g

α(x, s) + β(y, t)

= g(αx + βy, αs + βt) = (αs + βt)f
αx + βy
αs + βt

= (αs + βt)f

αs
αs + βt · x
s +
βt
αs + βt · y
t

≤αsf
x
s

+ βtf
y
t

= αg(x, s) + βg(y, t).
Example 6.2.6. By the previous theorem, f(x) = xnq(x/xn) is a convex
function on Rn−1 × R++ whenever q(x) is a positive semideﬁnite quadratic
form on Rn−1. In particular, by choosing the Euclidean norm as quadratic
form, we see that the function
x →(x2
1 + x2
2 + · · · + x2
n−1)/xn
is convex on the open halfspace Rn−1 × R++.
6.3
Maximum and minimum
Minimum points
For an arbitrary function to decide whether a given point is a global minimum
point is an intractable problem, but there are good numerical methods for
ﬁnding local minimum points if we impose some regularity conditions on the
function.
This is the reason why convexity plays such an important role
in optimization theory. A local minimum of a convex function is namely
automatically a global minimum.
Let us recall that a point x0 ∈X is a local minimum point of the function
f : X →R if there exists an open ball B = B(x0; r) with center at x0 such
that f(x) ≥f(x0) for all x ∈X ∩B. The point is a (global) minimum point
if f(x) ≥f(x0) for all x ∈X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
120
Convex functions
Theorem 6.3.1. Suppose that the function f : X →R is convex and that
x0 ∈dom f is a local minimum point of f. Then x0 is a global minimum
point. The minimum point is unique if f is strictly convex.
Proof. Let x ∈X be an arbitrary point diﬀerent from x0. Since f is a convex
function and λx + (1 −λ)x0 →x0 as λ →0, the following inequalities hold
for λ > 0 suﬃciently close to 0:
f(x0) ≤f(λx + (1 −λ)x0) ≤λf(x) + (1 −λ)f(x0)
(with strict inequality in the last place if f is strictly convex). From this
follows at once that f(x) ≥f(x0) (and f(x) > f(x0), respectively), which
proves that x0 is a global minimum point (and that there are no other mini-
mum points if the convexity is strict)
Theorem 6.3.2. The set of minimum points of a quasiconvex function is
convex.
Proof. The assertion is trivial for functions with no minimum point, since
the empty set is convex, and for the function which is identically equal to ∞
on X. So, suppose that the quasiconvex function f : X →R has a minimum
point x0 ∈dom f. The set of minimum points is then equal to the sublevel
set {x ∈X | f(x) ≤f(x0)}, which is convex by deﬁnition.
Maximum points
Theorem 6.3.3. Suppose X = cvx A and that the function f : X →R is
quasiconvex. Then
sup
x∈X
f(x) = sup
a∈A
f(a).
If the function has a maximum, then there is a maximum point in A.
Proof. Let x ∈X. Since x is a convex combination x = m
j=1 λjaj of ele-
ments aj ∈A,
f(x) = f(
m

j=1
λjaj) ≤max
1≤j≤m f(aj) ≤sup
a∈A
f(a),
and it follows that
sup
x∈X
f(x) ≤sup
a∈A
f(a).
The converse inequality being trivial, since A is a subset of X, we conclude
that equality holds.
Moreover, if x is a maximum point, then f(x) ≥max1≤j≤m f(aj), and
combining this with the inequality above, we obtain f(x) = max1≤j≤m f(aj),
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
121
Convex functions
121
which means that the maximum is certainly attained at some of the points
aj ∈A.
Thus, we can ﬁnd the maximum of a quasiconvex function whose domain
is the convex hull of a ﬁnite set A, by just comparing ﬁnitely many function
values. Of course, this may be infeasible if the set A is very large.
Since compact convex sets coincide with the convex hull of their extreme
points, we have the following corollary of the previous theorem.
Corollary 6.3.4. Suppose that X is a compact convex set and that f : X →R
is a quasiconvex function. If f has a maximum, then there is a maximum
point among the extreme points of X.
Example 6.3.1. The quadratic form f(x1, x2) = x2
1 + 2x1x2 + 2x2
2 is strictly
convex, since it positive deﬁnite. The maximum of f on the traingle with
vertices at the points (1, 1), (−2, 1) and (0, 2) is attained at some of the
vertices. The function values at the vertices are 5, 2, and 8, respectively.
The maximum value is hence equal to 8, and it is attained at (0, 2).
A non-constant realvalued convex function can not attain its maximum
at an interior point of its domain, because of the following theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
122
Convex functions
Theorem 6.3.5. A convex function f : X →R that attains its maximum at
a relative interior point of X, is necessarily constant on X.
Proof. Suppose f has a maximum at the point a ∈rint X, and let x be an
arbitrary point in X. Since a is a relative interior point, there exists a point
y ∈X such that a belongs to the open line segment ]x, y[, i.e. a = λx+(1−λ)y
for some number λ satisfying 0 < λ < 1. By convexity and since f(y) ≤f(a),
f(a) = f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) ≤λf(x) + (1 −λ)f(a),
with f(x) ≥f(a) as conclusion. Since the converse inequality holds trivially,
we have f(x) = f(a). The function f is thus equal to f(a) everywhere.
6.4
Some important inequalities
Many inequalities can be proved by convexity arguments, and we shall give
three important examples.
Arithmetic and geometric mean
Deﬁnition. Let θ1, θ2, . . . , θn be given positive numbers with n
j=1 θj = 1.
The weighted arithmetic mean A and the weighted geometric mean G of n pos-
itive numbers a1, a2, . . . , an with the given numbers θ1, θ2, . . . , θn as weights
are deﬁned as
A =
n

j=1
θjaj
and
G =
n

j=1
a
θj
j .
The usual arithmetic and geometric means are obtained as special cases
by taking all weights equal to 1/n.
We have the following well-known inequality between the arithmetic and
the geometric means.
Theorem 6.4.1. For all positive numbers a1, a2, . . . , an
G ≤A
with equality if and only if a1 = a2 = · · · = an.
Proof. Let xj = ln aj, so that aj = exj = exp(xj). The inequality G ≤A is
now transformed to the inequality
exp
 n

j=1
θjxj

≤
n

j=1
θj exp(xj),
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
123
Convex functions
which is Jensen’s inequality for the strictly convex exponential function, and
equality holds if and only if x1 = x2 = · · · = xn, i.e. if and only if a1 = a2 =
· · · = an.
Example 6.4.1. A lot of maximum and minimum problems can be solved
by use of the inequality of arithmetic and geometric means. Here follows a
general example.
Let f be a function of the form
f(x) =
m

i=1
ci
 n

j=1
x
αij
j

,
x ∈Rn
where ci > 0 and αij are real numbers for all i, j.
The function g(x) = 16x1 +2x2 +x−1
1 x−2
2 , corresponding to n = 2, m = 3,
c = (16, 2, 1) and
α = [αij] =


1
0
0
1
−1
−2

,
serves as a typical example of such a function.
Suppose that we want to minimize f(x) over the set {x ∈Rn | x > 0}.
This problem can be attacked in the following way.
Let θ1, θ2, . . . , θm be
positive numbers with sum equal to 1, and write
f(x) =
m

i=1
θi
ci
θi
n

j=1
x
αij
j

.
The inequality of arithmetic and geometric means now gives us the following
inequality
(6.5)
f(x) ≥
m

i=1
ci
θi
θi n

j=1
x
θiαij
j

= C(θ) ·
n

j=1
x
βj
j ,
with
C(θ) =
m

i=1
ci
θi
θi
and
βj =
m

i=1
θiαij.
If it is possible to choose the weights θi > 0 so that m
i=1 θi = 1 and
βj =
m

i=1
θiαij = 0
for all j,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
124
Convex functions
124
then inequality (6.5) becomes
f(x) ≥C(θ),
and equality occurs if and only if all the products ci
θi
n

j=1
x
αij
j
are equal, a
condition that makes it possible to determine x.
H¨older’s inequality
Theorem 6.4.2 (H¨older’s inequallity). Suppose 1 ≤p ≤∞and let q be the
dual index deﬁned by the equality
1
p + 1
q = 1.
Then
|⟨x, y⟩| =

n

j=1
xjyj
 ≤∥x∥p∥y∥q
for all x, y ∈Rn. Moreover, to each x there corresponds a y with norm
∥y∥q = 1 such that ⟨x, y⟩= ∥x∥p.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
125
Convex functions
Remark. Observe that q = 2 when p = 2. Thus, the Cauchy–Schwarz in-
equality is a special case of H¨older’s inequality.
Proof. The case p = ∞follows directly from the triangle inequality for sums:

n

j=1
xjyj
 ≤
n

j=1
|xj||yj| ≤
n

j=1
∥x∥∞|yj| = ∥x∥∞∥y∥1.
So assume that 1 ≤p < ∞.
Since
n
1 xjyj
 ≤n
1|xj||yj|, and the
vector (|x1|, . . . , |xn|) has the same ℓp-norm as (x1, . . . , xn) and the vector
(|y1|, . . . , |yn|) has the same ℓq-norm as (y1, . . . , yn), we can without loss of
generality assume that the numbers xj and yj are positive.
The function t →tp is convex on the interval [0, ∞[. Hence,
(6.6)
 n

j=1
λjtj
p
≤
n

j=1
λjtp
j.
for all positive numbers t1, t2, . . . , tn and all positive numbers λ1, λ2, . . . , λn
with n
1 λj = 1. Now, let us make the particular choice
λj =
yq
j
n
j=1 yq
j
and
tj = xjyj
λj
.
Then
λjtj = xjyj
and
λjtp
j = xp
jyp
j
y(p−1)q
j
 n

j=1
yq
j
p−1
= xp
j
 n

j=1
yq
j
p−1
,
which inserted in the inequality (6.6) gives
 n

j=1
xjyj
p
≤
p

j=1
xp
j
 n

j=1
yq
j
p−1
,
and we obtain H¨older’s inequality by raising both sides to 1/p.
It is easy to verify that H¨older’s inequality holds with equality and that
∥y∥q = 1 if we choose y as follows:
x = 0 :
All y with norm equal to 1.
x ̸= 0, 1 ≤p < ∞:
yj =

∥x∥−p/q
p
|xj|p/xj
if xj ̸= 0,
0
if xj = 0.
x ̸= 0, p = ∞:
yj =

|xj|/xj
if j = j0,
0
if j ̸= j0,
where j0 is an index such that |xj0| = ∥x∥∞.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
126
Convex functions
Theorem 6.4.3 (Minkowski’s inequality). Suppose p ≥1 and let x and y be
arbitrary vectors in Rn. Then
∥x + y∥p ≤∥x∥p + ∥y∥p.
Proof. Consider the linear forms x →fa(x) = ⟨a, x⟩for vectors a ∈Rn
satisfying ∥a∥q = 1. By H¨older’s inequality,
fa(x) ≤∥a∥q∥x∥p ≤∥x∥p,
and for each x there exists a vector a with ∥a∥q = 1 such that H¨older’s
inequality holds with equality, i.e. such that fa(x) = ∥x∥p. Thus
∥x∥p = sup{fa(x) | ∥a∥q = 1},
and hence, f(x) = ∥x∥p is a convex function by Theorem 6.2.4. Positive
homogenouity is obvious, and positive homogeneous convex functions are
subadditive, so the proof of Minkowski’s inequality is now complete.
6.5
Solvability of systems of convex inequal-
ities
The solvability of systems of linear inequalities was discussed in Chapter 3.
Our next theorem is kind of a generalization of Theorem 3.3.7 and treats the
solvability of a system of convex and aﬃne inequalities.
Theorem 6.5.1. Let fi : Ω→R, i = 1, 2, . . . , m, be a family of convex
functions deﬁned on a convex subset Ωof Rn.
Let p be an integer in the interval 1 ≤p ≤m, and suppose if p < m that
the functions fi are restrictions to Ωof aﬃne functions for i ≥p + 1 and
that the set
{x ∈rint Ω| fi(x) ≤0 for i = p + 1, . . . , m}
is nonempty. The following two assertions are then equivalent:
(i) The system
fi(x) < 0,
i = 1, 2, . . . , p
fi(x) ≤0,
i = p + 1, . . . , m
has no solution x ∈Ω.
(ii) There exist nonnegative numbers λ1, λ2, . . . , λm, with at least one of the
numbers λ1, λ2, . . . , λp being nonzero, such that
m

i=1
λifi(x) ≥0
for all x ∈Ω.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
127
Convex functions
127
Remark. The system of inequalities must contain at least one strict inequality,
and all inequalities are allowed to be strict (the case p = m).
Proof. If the system (i) has a solution x, then the sum in (ii) is obviously
negative for the same x, since at least one of its terms is negative and the
others are non-positive. Thus, (ii) implies (i).
To prove the converse implication, we assume that the system (i) has no
solution and deﬁne M to be the set of all y = (y1, y2, . . . , ym) ∈Rm such
that the system
fi(x) < yi,
i = 1, 2, . . . , p
fi(x) = yi,
i = p + 1, . . . , m
has a solution x ∈Ω.
The set M is convex, for if y′ and y′′ are two points in M, 0 ≤λ ≤1, and
x′, x′′ are solutions in Ωof the said systems of inequalities and equalities with
y′ and y′′, respectively, as right hand members, then x = λx′ +(1−λ)x′′ ∈Ω
will be a solution of the system with λy′+(1−λ)y′′ as its right hand member,
due to the convexity and aﬃnity of the functions fi for i ≤p and i > p,
respectively.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
128
Convex functions
Our assumptions concerning the system (i) imply that M ∩Rm
−= ∅. Since
Rm
−is a polyhedron, there exist, by the separation theorem 5.5.2, a nonzero
vector λ = (λ1, λ2, . . . , λm) and a real number α such that the hyperplane
H = {y | ⟨λ, y⟩= α} separates M and Rm
−and does not contain M as subset.
We may assume that
λ1y1 + λ2y2 + · · · + λmym

≥α
for all y ∈M,
≤α
for all y ∈Rm
−.
By ﬁrst choosing y = 0, we see that α ≥0, and by then choosing y = tei,
where ei is the i:th standard basis vector in Rm, and letting t tend to −∞,
we conclude that λi ≥0 for all i.
For each x ∈Ωand ϵ > 0,
y = (f1(x) + ϵ, . . . , fp(x) + ϵ, fp+1(x), . . . , fm(x))
is a point in M. Consequently,
λ1(f1(x) + ϵ) + · · · + λp(fp(x) + ϵ) + λp+1fp+1(x) + · · · + λmfm(x) ≥α ≥0,
and by letting ϵ tend to zero, we obtain the inequality
λ1f1(x) + λ2f2(x) + · · · + λmfm(x) ≥0
for all x ∈Ω.
If p = m, we are done since the vector λ = (λ1, λ2, . . . , λm) is then
nonzero, but it remains to prove that some of the coeﬃcients λ1, λ2, . . . , λp is
nonzero when p < m. Assume the contrary, i.e. that λ1 = λ2 = · · · = λp = 0,
and let
h(x) =
m

i=p+1
λifi(x).
The function h is aﬃne, and h(x) ≥0 for all x ∈Ω. Furthermore, by the
assumptions of the theorem, there exists a point x0 in the relative interior
of Ωsuch that fi(x0) ≤0 for all i ≥p + 1, which implies that h(x0) ≤0.
Thus, h(x0) = 0. This means that the restriction h|Ω, which is a concave
function since h is aﬃne, attains its minimum at a relative interior point,
and according to Theorem 6.3.5 (applied to the function −h|Ω), this implies
that the function h is constant and equal to 0 on Ω.
But to each y ∈M there corresponds a point x ∈Ωsuch that yi = fi(x)
for i = p + 1, . . . m, and this implies that
⟨λ, y⟩= m
i=p+1 λifi(x) = h(x) = 0.
We conclude that α = 0 and that the hyperplane H contains M, which is a
contradiction. Thus, at least one of the coeﬃcients λ1, λ2, . . . , λp has to be
nonzero, and the theorem is proved.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
129
Convex functions
6.6
Continuity
A real-valued convex function is automatically continuous at all relative in-
terior points of the domain. More precisely, we have the following theorem.
Theorem 6.6.1. Suppose f : X →R is a convex function and that a is
a point in the relative interior of dom f. Then there exist a relative open
neighborhood U of a in dom f and a constant M such that
|f(x) −f(a)| ≤M∥x −a∥
for all x ∈U. Hence, f is continuous on the relative interior of dom f.
Proof. We start by proving a special case of the theorem and then show how
to reduce the general case to this special case.
1. So ﬁrst assume that X is an open subset of Rn, that dom f = X, i.e. that
f is a real-valued convex function, that a = 0, and that f(0) = 0. We will
show that if we choose the number r > 0 such that the closed hypercube
K(r) = {x ∈Rn | ∥x∥∞≤r}
is included in X, then there is a constant M such that
(6.7)
|f(x)| ≤M∥x∥
for all x in the closed ball B(0; r) = {x ∈Rn | ∥x∥≤r}, where ∥·∥is the
usual Euclidean norm.
The hypercube K(r) has 2n extreme points (vertices). Let L denote the
largest of the function values of f at these extreme points. Since the convex
hull of the extreme points is equal to K(r), it follows from Theorem 6.3.3
that
f(x) ≤L
for all x ∈K(r), and thereby also for all x ∈B(0; r), because B(0; r) is a
subset of K(r).
We will now make this inequality sharper.
To this end, let x be an
arbitrary point in B(0; r) diﬀerent from the center 0. The halﬂine from 0
through x intersects the boundary of B(0; r) at the point
y =
r
∥x∥x,
and since x lies on the line segment [0, y], x is a convex combination of its
end points. More precisely, x = λy + (1 −λ)0 with λ = ∥x∥/r. Therefore,
since f is convex,
f(x) ≤λf(y) + (1 −λ)f(0) = λf(y) ≤λL = L
r ∥x∥.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
130
Convex functions
130
The above inequality holds for all x ∈B(0; r). To prove the same inequal-
ity with f(x) replaced by |f(x)|, we use the fact that the point −x belongs
to B(0; r) if x does so, and the equality 0 = 1
2x + 1
2(−x). By convexity,
0 = f(0) ≤1
2f(x) + 1
2f(−x) ≤1
2f(x) + L
2r∥x∥,
which simpliﬁes to the inequality
f(x) ≥−L
r ∥−x∥= −L
r ∥x∥.
This proves that inequality (6.7) holds for x ∈B(0; r) with M = L/r.
2. We now turn to the general case. Let n be the dimension of the set dom f.
The aﬃne hull of dom f is equal to the set a + V for some n-dimensional
linear subspace V , and since V is isomorphic to Rn, we can obtain a bijective
linear map T : Rn →V by choosing a coordinate system in V .
The inverse image Y of the relative interior of dom f under the map
y →a + Ty of Rn onto aﬀ(dom f) is an open convex subset of Rn, and Y
contains the point 0. Deﬁne the function g: Y →R by
g(y) = f(a + Ty) −f(a).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
131
Convex functions
Then, g is a convex function, since g is composed by a convex function and
an aﬃne function, and g(0) = 0.
For x = a + Ty ∈rint(dom f) we now have f(x) −f(a) = g(y) and
x −a = Ty, so in order to prove the general case of our theorem, we have to
show that there is a constant M such that |g(y)| ≤M∥Ty∥for all y in some
neighborhood of 0. But the map y →∥Ty∥is a norm on Rn, and since all
norms are equivalent, it suﬃces to show that there is a constant M such that
|g(y)| ≤M∥y∥
for all y in some neighborhood of 0, and that is exactly what we did in step 1
of the proof. So the theorem is proved.
The following corollary follows immediately from Theorem 6.6.1, because
aﬃne sets have no relative boundary points.
Corollary
6.6.2. A convex function f : X →R with an aﬃne subset X as
domain is continuous.
For functions f with a closed interval I = [a, b] as domain, convexity
imposes no other restrictions on the function value f(b) than that it has
to be greater than or equal to limx→b−f(x). Thus, a convex function need
not be continuous at the endpoint b, and a similar remark holds for the left
endpoint, of course. For example, a function f, that is identically equal to
zero on I \ {a, b}, is convex if f(a) ≥0 and f(b) ≥0. Cf. exercise 7.6.
6.7
The recessive subspace of convex func-
tions
Example 6.7.1. Let f : R2 →R be the convex function
f(x1, x2) = x1 + x2 + e(x1−x2)2.
The restrictions of f to lines with direction given by the vector v = (1, 1) are
aﬃne functions, since
f(x + tv) = f(x1 + t, x2 + t) = x1 + x2 + 2t + e(x1−x2)2 = f(x) + 2t.
Let V = {x ∈R2 | x1 = x2} be the linear subspace of R2 spanned by the
vector v, and consider the the orthogonal decomposition R2 = V ⊥+V . Each
x ∈R2 has a corresponding unique decomposition x = y + z with y ∈V ⊥
and z ∈V , namely
y = 1
2(x1 −x2, x2 −x1) and z = 1
2(x1 + x2, x1 + x2).
Moreover, since z = 1
2(x1 + x2)v = z1v,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
132
Convex functions
f(x) = f(y + z) = f(y) + 2z1 = f|V ⊥(y) + 2z1.
So there is a corresponding decomposition of f as a sum of the restriction
of f to V ⊥and a linear function on V . It is easily veriﬁed that the vector
(v, 2) = (1, 1, 2) spans the recessive subspace lin(epi f), and that V is equal
to the image P1(lin(epi f)) of lin(epi f) under the projection P1 of R2 × R
onto the ﬁrst factor R2.
The result in the previous example can be generalized, and in order to
describe this generalization we need a deﬁnition.
Deﬁnition. Let f : X →R be a function deﬁned on a subset X of Rn.
The linear subspace Vf = P1(lin(epi f)), where P1 : Rn × R →Rn is the
projection of Rn × R onto its ﬁrst factor Rn, is called the recessive subspace
of the function f.
Theorem 6.7.1. Let f be a convex function with recessive subspace Vf.
(i) A vector v belongs to Vf if and only if there is a unique number αv such
that (v, αv) belongs to the recessive subspace lin(epi f) of the epigraph
of the function.
(ii) The map g: Vf →R, deﬁned by g(v) = αv for v ∈Vf, is linear.
(iii) dom f = dom f + Vf.
(iv) f(x + v) = f(x) + g(v) for all x ∈dom f and all v ∈Vf.
(v) If the function f is diﬀerentiable at x ∈dom f then g(v) = ⟨f ′(x), v⟩
for all v ∈Vf.
(vi) Suppose V is a linear subspace, that h: V →R is a linear map, that
dom f + V ⊆dom f, and that f(x + v) = f(x) + h(v) for all x ∈dom f
and all v ∈V . Then, V ⊆Vf.
Proof. (i) By deﬁnition, v ∈Vf if and only if there is a real number αv such
that (v, αv) ∈lin(epi f). To prove that the number αv is uniquely determined
by v ∈Vf, we assume that the pair (v, β) also lies in lin(epi f).
The point (x+tv, f(x)+tαv) belongs to the epigraph for each x ∈dom f
and each t ∈R, i.e.
(6.8)
x + tv ∈dom f
and
f(x + tv) ≤f(x) + tαv.
Hence, (x + tv, f(x + tv)) is a point in the epigraph, and our assumption
(v, β) ∈lin(epi f) now implies that (x + tv −tv, f(x + tv) −tβ) is a point in
i epi f, too. We conclude that
(6.9)
f(x) ≤f(x + tv) −tβ
for all t ∈R. By combining the two inequalities (6.8) and (6.9), we obtain
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
133
Convex functions
133
the inequality f(x) ≤f(x) + (αv −β)t, which holds for all t ∈R. This is
possible only if β = αv, and proves the uniqueness of the number αv.
(ii) Let, as before, P1 be the projection of Rn × R onto Rn, and let P2 be
the projection of Rn × R onto the second factor R. The uniqueness result
(i) implies that the restriction of P1 to the linear subspace lin(epi f) is a
bijective linear map onto Vf. Let Q denote the inverse of this restriction; the
map g is then equal to the composition P2 ◦Q of the two linear maps P2 and
Q, and this implies that g is a linear function.
(iii) The particular choice of t = 1 in (6.8) yields the implication
x ∈dom f & v ∈Vf ⇒x + v ∈dom f,
which proves the inclusion dom f + Vf ⊆dom f, and the converse inclusion
is of course trivial.
(iv) By choosing t = 1 in the inequalities (6.8) and(6.9) and using the fact
that αv = β = g(v), we obtain the two inequalities f(x + v) ≤f(x) + g(v)
and f(x) ≤f(x + v) −g(v), which when combined prove assertion (iv).
(v) Consider the restriction φ(t) = f(x + tv) of the function f to the line
through the point x with direction v ∈Vf. By (iii), φ is deﬁned for all t ∈R,
and by (iv), φ(t) = f(x) + tg(v). Hence, φ′(0) = g(v). But if f is diﬀerenti-
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
134
Convex functions
able at x, then we also have φ′(0) = ⟨f ′(x), v⟩according to the chain rule,
and this proves our assertion (v).
(vi) Suppose v ∈V . If (x, s) is an arbitrary point in the epigraph epi f,
then f(x + tv) = f(x) + h(tv) ≤s + th(v), which means that the point
(x + tv, s + th(v)) lies in epi f for every real number t. This proves that
(v, h(v)) belongs to lin(epi f) and, consequently, that v is a vector in Vf.
By our next theorem, every convex function is the sum of a convex func-
tion with a trivial recessive subspace and a linear function.
Theorem 6.7.2. Suppose that f is a convex function with recessive subspace
Vf. Let ˜f denote the restriction of f to dom f ∩V ⊥
f , and let g: Vf →R be
the linear function deﬁned in Theorem 6.7.1. The recessive subspace V ˜f of ˜f
is then trivial, i.e. equal to {0}, dom f = dom f ∩V ⊥
f + Vf, and
f(y + z) = ˜f(y) + g(z)
for all y ∈dom f ∩V ⊥
f
and all z ∈Vf.
Proof. Each x ∈Rn has a unique decomposition x = y + z with y ∈V ⊥
f and
z ∈Vf, and if x ∈dom f then y = x −z ∈dom f + Vf = dom f, by Theorem
6.7.1, and hence y ∈dom f ∩V ⊥
f . This proves that dom f = dom f ∩V ⊥
f +Vf.
The equality f(y + z) = ˜f(y) + g(z) now follows from (iv) in Theorem
6.7.1, so it only remains to prove that V ˜f = {0}. Suppose v ∈V ˜f, and let x0
be an arbitrary point in dom ˜f. Then x0 + v lies in dom ˜f, too, and since
dom ˜f ⊆V ⊥
f and V ⊥
f is a linear subspace, we conclude that v = (x0 +v)−x0
is a vector in V ⊥
f . This proves the inclusion V ˜f ⊆V ⊥
f .
Theorem 6.7.1 gives us two linear functions g: Vf →R and ˜g: V ˜f →R
such that f(x + v) = f(x) + g(v) for all x ∈dom f and all v ∈Vf, and
˜f(y + w) = ˜f(y) + ˜g(w) for all y ∈dom f ∩V ⊥
f and all w ∈V ˜f.
Now, let w be an arbitrary vector in V ˜f and x be an arbitrary point in
dom f, and write x as x = y +v with y ∈dom f ∩V ⊥
f and v ∈Vf. The point
y + w lies in dom f ∩V ⊥
f , and we get the following identities:
f(x + w) = f(y + v + w) = f(y + w + v) = f(y + w) + g(v)
= ˜f(y + w) + g(v) = ˜f(y) + ˜g(w) + g(v)
= f(y) + g(v) + ˜g(w) = f(x) + ˜g(w).
Therefore, V ˜f ⊆Vf, by Theorem 6.7.1 (v). Hence, V ˜f ⊆V ⊥
f ∩Vf = {0},
which proves that V ˜f = {0}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
135
Convex functions
6.8
Closed convex functions
Deﬁnition. A convex function is called closed if it has a closed epigraph.
Theorem 6.8.1. A convex function f : X →R is closed if and only if all its
sublevel sets are closed.
Proof. Suppose that X is a subset of Rn and that f is a closed function. Let
Xα = sublevα f = {x ∈X | f(x) ≤α}
be an arbitrary nonempty sublevel set of f, and deﬁne Yα to be the set
Yα = epi f ∩{(x, xn+1) ∈Rn × R | xn+1 ≤α}.
The set Yα is closed, being the intersection between the closed epigraph epi f
and a closed halfspace, and Xα = P(Yα), where P : Rn × R →Rn is the
projection P(x, xn+1) = x.
Obviously, the recession cone recc Yα contains no nonzero vector of the
form v = (0, vn+1), i.e. no nonzero vector in the null space N(P) = {0}×R of
the projection P. Hence, (recc Yα)∩N(P) = {0}, so it follows from Theorem
2.7.10 that the sublevel set Xα is closed.
To prove the converse, assume that all sublevel sets are closed, and let
(x0, y0) be a boundary point of epi f. Let

(xk, yk)
∞
1 be a sequence of points
in epi f that converges to (x0, y0), and let ϵ be an arbitrary positive number.
Then, since yk →y0 as k →∞, f(xk) ≤yk ≤y0 + ϵ for all suﬃciently large
k, so the points xk belong to the sublevel set {x ∈X | f(x) ≤y0 + ϵ} for all
suﬃciently large k. The sublevel set being closed, it follows that the limit
point x0 lies in the same sublevel set, i.e. x0 ∈X and f(x0) ≤y0 + ϵ, and
since ϵ > 0 is arbitrary, we conclude that f(x0) ≤y0. Hence, (x0, y0) is a
point in epi f. So epi f contains all its boundary points and is therefore a
closed set.
Corollary
6.8.2. Continuous convex functions f : X →R with closed do-
mains X are closed functions.
Proof. Follows immediately from Theorem 6.8.1, because the sublevel sets of
real-valued continuous functions with closed domains are closed sets.
Theorem 6.8.3. All nonempty sublevel sets of a closed convex function have
the same recession cone and the same recessive subspace. Hence, all sublevel
sets are bounded if one of the nonempty sublevel sets is bounded.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
136
Convex functions
136
Proof. Let f : X →R be a closed convex function, and suppose that x0 is
a point in the sublevel set Xα = {x ∈X | f(x) ≤α}. Since Xα and epi f
are closed convex sets and (x0, α) is a point in epi f, we obtain the following
equivalences:
v ∈recc Xα ⇔x0 + tv ∈Xα
for all t ∈R+
⇔f(x0 + tv) ≤α
for all t ∈R+
⇔(x0 + tv, α) ∈epi f
for all t ∈R+
⇔(x0, α) + t(v, 0) ∈epi f
for all t ∈R+
⇔(v, 0) ∈recc(epi f),
with the conclusion that the recession cone
recc Xα = {v ∈Rn | (v, 0) ∈recc(epi f)}
does not depend on α as long as Xα ̸= ∅. Of course, the same is then true
for the recessive subspace
lin Xα = recc Xα ∩(−recc Xα) = {v ∈Rn | (v, 0) ∈lin(epi f)}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
137
Convex functions
The statement concerning bounded sublevel sets follows from the fact
that a closed convex set is bounded if and only if its recession cone is equal
to the zero cone {0}.
Theorem 6.8.4. A convex function f, which is bounded on an aﬃne subset
M, is constant on M.
Proof. Let M = a + U, where U is a linear subspace, and consider the
restriction g = f|M of f to M. The function g is continuous since all points
of M are relative interior points, and closed since the domain M is a closed
set. Let α = sup{g(x) | x ∈M}; then {x | g(x) ≤α} = M, so by the
previous theorem, all nonempty sublevel sets of g has lin M, that is the
subspace U, as their recessive subspace.
Let now x0 be an arbitrary point in M. Since the recessive subspace of
the particular sublevel set {x | g(x) ≤g(x0} is equal to U, we conclude that
g(x0 + u) ≤g(x0) for all u ∈U. Hence, g(x) ≤g(x0) for all x ∈M, which
means that x0 is a maximum point of g. Since x0 ∈M is arbitrary, all points
in M are maximum points, and this implies that g is constant on M.
6.9
The support function
Deﬁnition. Let A be a nonempty subset of Rn. The function SA : Rn →R,
deﬁned by
SA(x) = sup{⟨y, x⟩| y ∈A}
(with the usual convention that SA(x) = ∞if the function y →⟨y, x⟩is
unbounded above on A) is called the support function of the set A.
Theorem 6.9.1. (a) The support function SA is a closed convex function.
(b) Suppose A and B are nonempty subsets of Rn, that α > 0 and that
C : Rn →Rm is a linear map. Then
SA = Scvx A = Scl(cvx A)
(i)
SαA = αSA
(ii)
SA+B = SA + SB
(iii)
SA ∪B = max {SA, SB}
(iv)
SC(A) = SA ◦CT.
(v)
Proof. (a) The support function SA is closed and convex, because its epi-
graph
epi SA = {(x, t) | ⟨y, x⟩≤t for all y ∈A} =

y∈A
{(x, t) | ⟨y, x⟩≤t}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
138
Convex functions
is closed, being the intersection of a family of closed halfspaces in Rn × R.
(b) Since linear forms are convex, it follows from Theorem 6.3.3 that
SA(x) = sup{⟨x, y⟩| y ∈A} = sup{⟨x, y⟩| y ∈cvx A} = Scvx A(x)
for all x ∈Rn. Moreover, if a function f is continuous on the closure of
a set X, then supy∈X f(y) = supy∈cl X f(y), and linear forms are of course
continuous. Therefore, Scvx A(x) = Scl(cvx A)(x) for all x.
This proves the identity (i), and the remaining identities are obtained as
follows:
SαA(x) = sup
y∈αA
⟨y, x⟩= sup
y∈A
⟨αy, x⟩= α sup
y∈A
⟨y, x⟩= αSA(x).
SA+B(x) = sup
y∈A+B
⟨y, x⟩=
sup
y1∈A, y2∈B
⟨y1 + y2, x⟩
=
sup
y1∈A, y2∈B
(⟨y1, x⟩+ ⟨y2, x⟩) = sup
y1∈A
⟨y1, x⟩+ sup
y2∈B
⟨y2, x⟩
= SA(x) + SB(x).
SA ∪B(x) =
sup
y∈(A ∪B)
⟨y, x⟩= max {sup
y∈A
⟨y, x⟩, sup
y∈B
⟨y, x⟩}
= max {SA(x), SB(x)}.
SC(A)(x) = sup
y∈C(A)
⟨y, x⟩= sup
z∈A
⟨Cz, x⟩= sup
z∈A
⟨z, CTx⟩= SA(CTx).
Example 6.9.1. The support function of a closed interval [a, b] on the real
line is given by
S[a,b](x) = S{a,b}(x) = max{ax, bx},
since [a, b] = cvx{a, b}.
Example 6.9.2. In order to ﬁnd the support function of the closed unit ball
Bp = {x ∈Rn | ∥x∥p ≤1} with respect to the ℓp-norm, we use H¨older’s
inequality, obtaining
SBp(x) = sup{⟨x, y⟩| ∥y∥p ≤1} = ∥x∥q,
where the relation between p and q is given by the equation 1/p+1/q = 1.
Closed convex sets are completely characterized by their support func-
tions, due to the following theorem.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
139
Convex functions
139
Theorem 6.9.2. Suppose that X1 and X2 are two nonempty closed convex
subsets of Rn with support functions SX1 and SX2, respectively. Then
X1 ⊆X2 ⇔SX1 ≤SX2
(a)
X1 = X2 ⇔SX1 = SX2.
(b)
Proof. Assertion (b) is an immediate consequence of (a), and the implication
X1 ⊆X2 ⇒SX1 ≤SX2 is trivial, so it only remains to prove the converse
implication, or equivalently, the implication X1 ̸⊆X2 ⇒SX1 ̸≤SX2.
To prove the latter implication we assume that X1 ̸⊆X2, i.e. that there
exists a point x0 ∈X1\X2. The point x0 is strictly separable from the closed
convex set X2, which means that there exist a vector c ∈Rn and a number
b such that ⟨x, c⟩≤b for all x ∈X2 while ⟨x0, c⟩> b. Consequently,
SX1(c) ≥⟨x0, c⟩> b ≥sup{⟨x, c⟩| x ∈X2} = SX2(c),
which shows that SX1 ̸≤SX2.
By combining the previous theorem with property (i) of Theorem 6.9.1,
we obtain the following corollary.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
140
Convex functions
Corollary 6.9.3. Let A and B be two nonempty subsets of Rn. Then,
SA = SB ⇔cl(cvx A) = cl(cvx B).
6.10
The Minkowski functional
Let X be a convex subset of Rn with 0 as an interior point of X. Consider
the sets tX for t ≥0. This is an increasing family of sets, whose union equals
all of Rn, i.e. 0 ≤s < t ⇒sX ⊆tX and 
t≥0 tX = Rn.
The family is increasing, because using the convexity of the sets tX and
the fact that they contain 0, we obtain the following inclusions for 0 ≤s < t:
sX = s
t (tX) + (1 −s
t ) 0 ⊆s
t (tX) + (1 −s
t )(tX) ⊆tX.
That the union equals Rn only depends on 0 being an interior point of
X. For let B(0; r0) be a closed ball centered at 0 and contained in X. An
arbitrary point x ∈Rn will then belong to the set r−1
0 ∥x∥X since r0∥x∥−1x
lies in B(0; r0).
Now ﬁx x ∈Rn and consider the set {t ≥0 | x ∈tX}. This set is an
unbounded subinterval of [0, ∞[, and it contains the number r−1
0 ∥x∥. We
may therefore deﬁne a function
φX : Rn →R+
by letting
φX(x) = inf{t ≥0 | x ∈tX}.
Obviously,
φX(x) ≤r−1
0 ∥x∥
for all x.
Deﬁnition. The function φX : Rn →R+ is called the Minkowski functional
of the set X.
Theorem 6.10.1. The Minkowski functional φX has the following properties:
(i) For all x, y ∈Rn and all λ ∈R+,
(a) φX(λx) = λφX(x),
(b) φX(x + y) ≤φX(x) + φX(y).
(ii) There exists a constant C such that
|φX(x) −φX(y)| ≤C∥x −y∥
for all x, y ∈Rn.
(iii) int X = {x ∈Rn | φX(x) < 1} and
cl X = {x ∈Rn | φX(x) ≤1}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
141
Convex functions
The Minkowski functional is, in other words, positive homogeneous, subaddi-
tive, and Lipschitz continuous. So it is in particular a convex function.
Proof. (i) The equivalence x ∈tX ⇔λx ∈λtX, which holds for λ > 0,
together with the fact that φX(0) = 0, implies positive homogenouity.
To prove subadditivity we choose, given ϵ > 0, two positive numbers
s < φX(x) + ϵ and t < φX(y) + ϵ such that x ∈sX and y ∈tX. The point
1
s + t(x + y) =
s
s + t
x
s +
t
s + t
y
t
is a point in X, by convexity, and it follows that the point x + y belongs to
the set (s + t)X. This implies that
φX(x + y) ≤s + t < φX(x) + φX(y) + 2ϵ,
and since this inequality is true for all ϵ > 0, we conclude that
φX(x + y) ≤φX(x) + φX(y).
(ii) We have already noted that the inequality φX(x) ≤C∥x∥holds for all x
with C = r−1
0 . By subadditivity,
φX(x) = φX(x −y + y) ≤φX(x −y) + φX(y),
and hence
φX(x) −φX(y) ≤φX(x −y) ≤C∥x −y∥.
For symmetry reasons
φX(y) −φX(x) ≤C∥y −x∥= C∥x −y∥,
and hence |φX(x) −φX(y)| ≤C∥x −y∥.
(iii)
The sets {x ∈Rn | φX(x) < 1} and {x ∈Rn | φX(x) ≤1} are open
and closed, respectively, since φX is continuous. Therefore, to prove assertion
(iii) it suﬃces, due to the characterization of int X as the largest open set
contained in X and of cl X as the smallest closed set containing X, to prove
the inclusions
int X ⊆{x ∈Rn | φX(x) < 1} ⊆X ⊆{x ∈Rn | φX(x) ≤1} ⊆cl X.
Suppose x ∈int X. Since tx →x as t →1, the points tx belong to the
interior of X for all numbers t that are suﬃciently close to 1. Thus, there
exists a number t0 > 1 such that t0x ∈X, i.e. such that x ∈t−1
0 X, which
means that φX(x) ≤t−1
0
< 1, and this proves the inclusion
int X ⊆{x ∈Rn | φX(x) < 1}.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
142
Convex functions
The implications φX(x) < t ⇒x ∈tX ⇒φX(x) ≤t are direct con-
sequences of the deﬁnition of φX(x), and by choosing t = 1 we obtain the
inclusions
{x ∈Rn | φX(x) < 1} ⊆X ⊆{x ∈Rn | φX(x) ≤1}.
To prove the remaining inclusion it is now enough to prove the inclusion
{x ∈Rn | φX(x) = 1} ⊆cl X.
So, suppose φX(x) = 1. Then there is a sequence (tn)∞
1 of numbers > 1 such
that tn →1 as n →∞and x ∈tnX for all n. The points t−1
n x belong to X
for all n, and since t−1
n x →x as n →∞, x is a point in the closure cl X.
Exercises
6.1 Find two quasiconvex functions f1, f2 with a non-quasiconvex sum f1 + f2.
6.2 Prove that the following functions f : R3 →R are convex:
a) f(x) = x2
1 + 2x2
2 + 5x2
3 + 3x2x3
b) f(x) = 2x2
1 + x2
2 + x2
3 −2x1x2 + 2x1x3
c) f(x) = ex1−x2 + ex2−x1 + x2
3 −2x3.
6.3 For which values of the real number a is the function
f(x) = x2
1 + 2x2
2 + ax2
3 −2x1x2 + 2x1x3 −6x2x3
convex and strictly convex?
6.4 Prove that the function f(x) = x1x2 · · · xn with Rn
+ as domain is quasicon-
cave, and that the function g(x) = (x1x2 · · · xn)−1 with Rn
++ as domain is
convex.
6.5 Let x[k] denote the k:th biggest coordinate of the point x = (x1, x2, . . . , xn).
In other words, x[1], x[2], . . . , x[n] are the coordinates of x in decreasing order.
Prove for each k that the function f(x) = k
i=1 x[i] is convex.
6.6 Suppose f : R+ →R is convex. Prove that
f(x1) + f(x2) + · · · + f(xn) ≤f(x1 + x2 + · · · + xn) + (n −1)f(0)
for all x1, x2, . . . , xn ≥0. Note the special case f(0) = 0!
6.7 The function f is deﬁned on a convex subset of Rn.
Suppose that the
function f(x) + ⟨c, x⟩is quasiconvex for each c ∈Rn.
Prove that f is
convex.
6.8 We have derived Corollary 6.2.7 from Theorem 6.2.6. Conversely, prove that
Theorem 6.2.6 follows easily from Corollary 6.2.7.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
143
Convex functions
143
6.9 X is a convex set in Rn with a nonempty interior, and f : X →R is a
continuous function, whose restriction to int X is convex. Prove that f is
convex.
6.10 Suppose that the function f : X →R is convex. Prove that
inf {f(x) | x ∈X} = inf {f(x) | x ∈rint(dom f)}.
6.11 Use the method in Example 6.4.1 to determine the minimum of the function
g(x1, x2) = 16x1 + 2x2 + x−1
1 x−2
2
over the set x1 > 0, x2 > 0.
6.12 Find the Minkowski functional of
a) the closed unit ball B(0; 1) in Rn with respect to the ℓp-norm ∥·∥p;
b) the halfspace {x ∈Rn | x1 ≤1}.
6.13 Let X be a convex set with 0 as interior point and suppose that the set
is symmetric with respect to 0, i.e. x ∈X ⇒−x ∈X.
Prove that the
Minkowski functional φX is a norm, i.e. that
(i) φX(x + y) ≤φX(x) + φX(y);
(ii) φX(λx) = λφX(x) for all λ ∈R;
(iii) φX(x) = 0 ⇔x = 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
144
Smooth convex functions
Chapter 7
Smooth convex functions
This chapter is devoted to the study of smooth convex functions, i.e. convex
functions that are diﬀerentiable. A prerequisite for diﬀerentiability at a point
is that the function is deﬁned and ﬁnite in a neighborhood of the point.
Hence, it is only meaningful to study diﬀerentiability properties at interior
points of the domain of the function, and by passing to the restriction of
the function to the interior of its domain, we may as well assume from the
beginning that the domain of deﬁnition is open.
That is the reason for
assuming all domains to be open and all function values to be ﬁnite in this
chapter.
7.1
Convex functions on R
Let f be a real-valued function that is deﬁned in a neighborhood of the point
x ∈R. The one-sided limit
f ′
+(x) = lim
t→0+
f(x + t) −f(x)
t
,
if it exists, is called the right derivative of f at the point x. The left derivative
f ′
−(x) is similarly deﬁned as the one-sided limit
f ′
−(x) = lim
t→0−
f(x + t) −f(x)
t
.
The function is obviously diﬀerentiable at the point x if and only if the right
and the left derivatives both exist and are equal, and the derivative f ′(x) is
in that case equal to their common value.
The left derivative of the function f : I →R can be expressed as a right
derivative of the function ˇf, deﬁned by
ˇf(x) = f(−x)
for all x ∈−I,
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
145
Smooth convex functions
because
f ′
−(x) = lim
t→0+
f(x −t) −f(x)
−t
= −lim
t→0+
ˇf(−x + t) −ˇf(−x)
t
and hence
f ′
−(x) = −ˇf ′
+(−x).
Observe that the function ˇf is convex if f is convex.
The basic diﬀerentiability properties of convex functions are consequences
of the following lemma, which has an obvious interpretation in terms of slopes
of various chords. Cf. ﬁgure 7.1.
Lemma 7.1.1. Suppose f is a real-valued convex function that is deﬁned on
a subinterval of R containing the points x1 < x2 < x3. Then
f(x2) −f(x1)
x2 −x1
≤f(x3) −f(x1)
x3 −x1
≤f(x3) −f(x2)
x3 −x2
.
The above inequalities are strict if f is strictly convex.
x1
x2
x3
A
B
C
Figure 7.1.
A geometric interpretation of Lemma 7.1.1: If kPQ
denotes the slope of the chord PQ, then kAB ≤kAC ≤kBC.
Proof. Write x2 = λx3 + (1 −λ)x1; then λ = x2 −x1
x3 −x1
is a number in the
interval ]0, 1[. By convexity,
f(x2) ≤λf(x3) + (1 −λ)f(x1),
which simpliﬁes to f(x2) −f(x1) ≤λ(f(x3) −f(x1)), and this is equivalent
to the leftmost of the two inequalities in the lemma.
The rightmost inequality is obtained by applying the already proven in-
equality to the convex function ˇf. Since −x3 < −x2 < −x1,
f(x2) −f(x3)
x3 −x2
=
ˇf(−x2) −ˇf(−x3)
−x2 −(−x3)
≤
ˇf(−x1) −ˇf(−x3)
−x1 −(−x3)
= f(x1) −f(x3)
x3 −x1
,
and multiplication by −1 gives the desired result.
The above inequalities are strict if f is strictly convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
146
Smooth convex functions
146
The diﬀerentiability properties of convex one-variable functions are given
by the following theorem.
Theorem 7.1.2. Suppose f : I →R is a convex function with an open subin-
terval I of R as its domain. Then:
(a) The function f has right and left derivatives at all points x ∈I, and
f ′
−(x) ≤f ′
+(x).
(b) If f ′
−(x) ≤a ≤f ′
+(x), then
f(y) ≥f(x) + a(y −x)
for all y ∈I.
The above inequality is strict for y ̸= x, if f is strictly convex.
(c) If x < y, then f ′
+(x) ≤f ′
−(y), and the inequality is strict if f is strictly
convex.
(d) The functions f ′
+ : I →R and f ′
−: I →R are increasing, and they are
strictly increasing if f is strictly convex.
(e) The set of points x ∈I where the function is not diﬀerentiable, is ﬁnite
or countable.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
147
Smooth convex functions
Proof. Fix x ∈I and let
F(t) = f(x + t) −f(x)
t
.
The domain of F is an open interval Jx with the point 0 removed.
We start by observing that if s, t, u ∈Jx and u < 0 < t < s, then
(7.1)
F(u) ≤F(t) ≤F(s)
(and the inequalities are strict if f is strictly convex).
The right inequality F(t) ≤F(s) follows directly from the left inequality
in Lemma 7.1.1 by choosing x1 = x, x2 = x + t and x3 = x + s, and the left
inequality F(u) ≤F(t) follows from the inequality between the extreme ends
in the same lemma by instead choosing x1 = x + u, x2 = x and x3 = x + t.
It follows from inequality (7.1) that the function F(t) is increasing for
t > 0 (strictly increasing if f is strictly convex) and bounded below by
F(u0), where u0 is an arbitrary negative number in the domain of F. Hence,
the limit
f ′
+(x) = lim
t→0+ F(t)
exists and
F(t) ≥f ′
+(x)
for all t > 0 in the domain of F (with strict inequality if f is strictly convex).
By replacing t with y −x, we obtain the following implication for a ≤f ′
+(x):
(7.2)
y > x ⇒f(y) −f(x) ≥f ′
+(x)(y −x) ≥a(y −x)
(with strict inequality if f is strictly convex).
The same argument, applied to the function ˇf and the point −x, shows
that ˇf ′
+(−x) exists, and that
−y > −x ⇒ˇf(−y) −ˇf(−x) ≥−a(−y −(−x))
if −a ≤ˇf ′
+(−x). Since f ′
−(x) = −ˇf ′
+(−x), this means that the left derivative
f ′
−(x) exists and that the implication
(7.3)
y < x ⇒f(y) −f(x) ≥a(y −x)
is true for all constants a satisfying a ≥f ′
−(x). The implications (7.2) and
(7.3) are both satisﬁed if f ′
−(x) ≤a ≤f ′
+(x), and this proves assertion (b).
Using inequality (7.1) we conclude that F(−t) ≤F(t) for all suﬃciently
small values of t. Hence
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
148
Smooth convex functions
f ′
−(x) = lim
t→0+ F(−t) ≤lim
t→0+ F(t) = f ′
+(x),
and this proves assertion (a).
As a special case of assertion (b), we have the two inequalities
f(y) −f(x) ≥f ′
+(x)(y −x)
and
f(x) −f(y) ≥f ′
−(y)(x −y),
and division by y −x now results in the implication
y > x ⇒f ′
+(x) ≤f(y) −f(x)
y −x
≤f ′
−(y).
(If f is strictly convex, we may replace ≤with < at both places.) This proves
assertion (c).
By combining (c) with the inequality in (a) we obtain the implication
x < y ⇒f ′
+(x) ≤f ′
−(y) ≤f ′
+(y),
which shows that the right derivative f ′
+ is increasing. That the left derivative
is increasing is proved in a similar way. (And the derivatives are strictly
increasing if f is strictly convex.)
To prove the ﬁnal assertion (e) we deﬁne Ix to be the open interval
]f ′
−(x), f ′
+(x)[. This interval is empty if the derivative f ′(x) exists, and it is
nonempty if the derivative does not exist, and intervals Ix and Iy belonging
to diﬀerent points x and y are disjoint because of assertion (c). Now choose,
for each point x where the derivative does not exist, a rational number rx
in the interval Ix. Since diﬀerent intervals are pairwise disjoint, the chosen
numbers will be diﬀerent, and since the set of rational numbers is countable,
there are at most countably many points x at which the derivative does not
exist.
Deﬁnition. The line y = f(x0) + a(x −x0) is called a supporting line of the
function f : I →R at the point x0 ∈I if
(7.4)
f(x) ≥f(x0) + a(x −x0)
for all x ∈I.
A supporting line at the point x0 is a line which passes through the point
(x0, f(x0)) and has the entire function curve y = f(x) above (or on) itself. It
is, in other words, a (one-dimensional) supporting hyperplane of the epigraph
of f at the point (x0, f(x0)). The concept will be generalized for functions
of several variables in the next chapter.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
149
Smooth convex functions
149
x0
x
y
y = f(x0) + a(x −x0)
y = f(x)
Figure 7.2.
A supporting line.
Assertion (b) of the preceding theorem shows that convex functions with
open domains have supporting lines at each point, and that the tangent is a
supporting line at points where the derivative exists. By our next theorem,
the existence of supporting lines is also a suﬃcient condition for convexity.
Theorem 7.1.3. Suppose that the function f : I →R, where I is an open
interval, has a supporting line at each point in I. Then, f is a convex func-
tion.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
150
Smooth convex functions
Proof. Suppose that x, y ∈I and that 0 < λ < 1, and let a be the constant
belonging to the point x0 = λx+(1−λ)y in the deﬁnition (7.4) of a supporting
line. Then we have f(x) ≥f(x0) + a(x −x0) and f(y) ≥f(x0) + a(y −x0).
By multiplying the ﬁrst inequality by λ and the second inequality by (1−λ),
and then adding the two resulting inequalities, we obtain
λf(x) + (1 −λ)f(y) ≥f(x0) + a

λx + (1 −λ)y −x0

= f(x0).
So the function f is convex.
Observe that if the inequality (7.4) is strict for all x ̸= x0 and for all
x0 ∈I, then f is strictly convex.
For diﬀerentiable functions we now obtain the following necessary and
suﬃcient condition for convexity.
Theorem 7.1.4. A diﬀerentiable function f : I →R is convex if and only
if its derivative f ′ is increasing. And it is strictly convex if and only if the
derivative is strictly increasing.
Proof. Assertion (d) in Theorem 7.1.2 shows that the derivative of a (strictly)
convex function is (strictly) increasing.
To prove the converse, we assume that the derivative f ′ is increasing. By
the mean value theorem, if x and x0 are distinct points in I, there exists a
point ξ between x and x0 such that
f(x) −f(x0)
x −x0
= f ′(ξ)

≥f ′(x0)
if x > x0,
≤f ′(x0)
if x < x0.
Multiplication by x −x0 results, in both cases, in the inequality
f(x) −f(x0) ≥f ′(x0)(x −x0),
which shows that y = f(x0) + f ′(x0)(x −x0) is a supporting line of the
function f at the point x0. Therefore, f is convex by Theorem 7.1.3.
The above inequalites are strict if the derivative is strictly increasing, and
we conclude that f is strictly convex in that case.
For two times diﬀerentiable functions we obtain the following corollary.
Corollary
7.1.5. A two times diﬀerentiable function f : I →R is convex
if and only if f ′′(x) ≥0 for all x ∈I. The function is strictly convex if
f ′′(x) > 0 for all x ∈I.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
151
Smooth convex functions
Proof. The derivative f ′ is increasing (strictly increasing) if the second deri-
vative f ′′ is nonnegative (positive). And the second derivative is nonnegative
if the derivative is increasing.
Remark. A continuous function f : J →R with a non-open interval J as
domain is convex if (and only if) the restriction of f to the interior of J is
convex. Hence, if the derivative exists and is increasing in the interior of J,
or if the second derivative exists and f ′′(x) ≥0 for all interior points x of
the interval, then f is convex on J. Cf. exercise 7.7.
Example 7.1.1. The functions x →ex, x →−ln x and x →xp, where
p > 1, are strictly convex on their domains R, ]0, ∞[ and [0, ∞[, respectively,
because their ﬁrst derivatives are strictly increasing functions.
7.2
Diﬀerentiable convex functions
A diﬀerentiable one-variable function f is convex if and only if its derivative
is an increasing function. In order to generalize this result to functions of
several variables it is necessary to express the condition that the derivative is
increasing in a generalizable way. To this end, we note that the derivative f ′
is increasing on an interval if and only if f ′(x + h)h ≥f ′(x)h for all numbers
x and x+h in the interval, and this inequality is also meaningful for functions
f of several variables if we interpret f ′(x)h as the value of the linear form
Df(x) at h. The inequality generalizing that the derivative of a function of
several variables is increasing will thus be written Df(x + h)[h] ≥Df(x)[h],
or using gradient notation, ⟨f ′(x + h), h⟩≥⟨f ′(x), h⟩.
Theorem 7.2.1. Let X be an open convex subset of Rn, and let f : X →R
be a diﬀerentiable function. The following three conditions are equivalent:
(i) f is a convex function.
(ii) f(x + v) ≥f(x) + Df(x)[v] for all x, x + v ∈X.
(iii) Df(x + v)[v] ≥Df(x)[v] for all x, x + v ∈X.
The function f is strictly convex if and only if the inequalities in (ii) and
(iii) can be replaced by strict inequalities when v ̸= 0.
Proof. Let us for given points x and x + v in X consider the restriction φx,v
of f to the line through x with direction v, i.e. the one-variable function
φx,v(t) = f(x + tv)
with the open interval Ix,v = {t ∈R | x + tv ∈X} as domain. The functions
φx,v are diﬀerentiable with derivative φ′
x,v(t) = Df(x+tv)[v], and f is convex
if and only if all restrictions φx,v are convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
152
Smooth convex functions
152
(i) ⇒(ii)
So if f is convex, then φx,v is a convex function, and it follows
from Theorem 7.1.2 (b) that φx,v(t) ≥φx,v(0)+φ′
x,v(0)t for all t ∈Ix,v, which
means that f(x + tv) ≥f(x) + Df(x)[v] t for all t such that x + tv ∈X. We
now obtain the inequality in (ii) by choosing t = 1.
(ii) ⇒(iii)
We obtain inequality (iii) by adding the two inequalities
f(x + v) ≥f(x) + Df(x)[v]
and
f(x) ≥f(x + v) + Df(x + v)[−v].
(iii) ⇒(i) Suppose (iii) holds, and let y = x+sv and w = (t−s)v. If t > s,
then
φ′
x,v(t) −φ′
x,v(s) = Df(x + tv)[v] −Df(x + sv)[v]
= Df(y + w)[v] −Df(y)[v]
=
1
t −s

Df(y + w)[w] −Df(y)[w]

≥0,
which means that the derivative φ′
x,v is increasing. The functions φx,v are
thus convex.
This proves the equivalence of assertions (i), (ii) and (iii), and by replacing
all inequalities in the proof by strict inequalities, we obtain the corresponding
equivalent assertions for strictly convex functions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
153
Smooth convex functions
The derivative of a diﬀerentiable function is equal to zero at a local min-
imum point. For convex functions, the converse is also true.
Theorem 7.2.2. Suppose f : X →R is a diﬀerentiable convex function.
Then ˆx ∈X is a global minimum point if and only if f ′(ˆx) = 0.
Proof. That the derivative equals zero at a minimum point is a general fact,
and the converse is a consequence of property (ii) in the previous theorem,
for if f ′(ˆx) = 0, then f(x) ≥f(ˆx) + Df(ˆx)[x −ˆx] = f(ˆx) for all x ∈X.
Convexity can also be expressed by a condition on the second derivative,
and the natural substitute for the one-variable condition f ′′(x) ≥0 is that
the second derivative should be positive semideﬁnite.
Theorem 7.2.3. Let X be an open convex subset of Rn, and suppose that
the function f : X →R is two times diﬀerentiable. Then f is convex if and
only if the second derivative f ′′(x) is positive semideﬁnite for all x ∈X.
If f ′′(x) is positive deﬁnite for all x ∈X, then f is strictly convex.
Proof. The one-variable functions φx,v(t) = f(x + tv) are now two times
diﬀerentiable with second derivative
φ′′
x,v(t) = D2f(x + tv)[v, v] = ⟨v, f ′′(x + tv)v⟩.
Since f is convex if and only if all functions φx,v are convex, f is convex if
and only if all second derivatives φ′′
x,v are nonnegative functions .
If the second derivative f ′′(x) is positive semideﬁnite for all x ∈X, then
φ′′
x,v(t) = ⟨v, f ′′(x+tv)v⟩≥0 for all x ∈X and all v ∈Rn, which means that
the second derivatives φ′′
x,v are nonnegative funtions. Conversely, if the second
derivatives φ′′
x,v are nonnegative, then in particular ⟨v, f ′′(x)⟩= φ′′
x,v(0) ≥0
for all x ∈X and all v ∈Rn, and we conclude that the second derivative
f ′′(x) is positive semideﬁnite for all x ∈X.
If the second derivatives f ′′(x) are all positive deﬁnite, then φ′′
x,v(t) > 0
for v ̸= 0, which implies that the functions φx,v are strictly convex, and then
f is strictly convex, too.
7.3
Strong convexity
The function surface of a convex functions bends upwards, but there is no
lower positive bound on the curvature. By introducing such a bound we
obtain the notion of strong convexity.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
154
Smooth convex functions
Deﬁnition. Let µ be a positive number. A function f : X →R is called µ-
strongly convex if the function f(x)−1
2µ∥x∥2 is convex, and the function f is
called strongly convex if it is µ-strongly convex for some positive number µ.
Theorem 7.3.1. A diﬀerentiable function f : X →R with a convex domain
is µ-strongly convex if and only if the following two mutually equivalent in-
equalities are satisﬁed for all x, x + v ∈X:
Df(x + v)[v] ≥Df(x)[v] + µ∥v∥2
(i)
f(x + v) ≥f(x) + Df(x)[v] + 1
2µ∥v∥2.
(ii)
Proof. Let g(x) = f(x) −1
2µ∥x∥2 and note that g′(x) = f ′(x) −µx and that
consequently Df(x)[v] = Dg(x)[v] + µ⟨x, v⟩.
If f is µ-strongly convex, then g is a convex function, and so it follows
from Theorem 7.2.1 that
Df(x + v)[v] −Df(x)[v] = Dg(x + v)[v] −Dg(x)[v] + µ⟨x + v, v⟩−µ⟨x, v⟩
≥µ⟨v, v⟩= µ∥v∥2,
i.e. inequality (i) is satisﬁed.
(i) ⇒(ii):
Assume (i) holds, and deﬁne the function Φ for 0 ≤t ≤1 by
Φ(t) = f(x + tv) −f(x) −Df(x)[v] t.
Then Φ′(t) = Df(x + tv)[v] −Df(x)[v] = 1
t

Df(x + tv)[tv] −Df(x)[tv]

,
and it now follows from inequality (i) that
Φ′(t) ≥t−1µ∥tv∥2 = µ∥v∥2 t.
By integrating the last inequality over the interval [0, 1] we obtain
Φ(1) = Φ(1) −Φ(0) ≥1
2µ∥v∥2,
which is the same as inequality (ii).
If inequality (ii) holds, then
g(x + v) = f(x + v) −1
2µ∥x + v∥2 ≥f(x) + Df(x)[v] + 1
2µ∥v∥2 −1
2µ∥x + v∥2
= g(x) + 1
2µ∥x∥2 + Dg(x)[v] + µ⟨x, v⟩+ 1
2µ∥v∥2 −1
2µ∥x + v∥2
= g(x) + Dg(x)[v].
The function g is thus convex, by Theorem 7.2.1, and f(x) = g(x) + 1
2µ∥x∥2
is consequently µ-strongly convex.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
155
Smooth convex functions
155
Theorem 7.3.2. A twice diﬀerentiable function f : X →R with a convex
domain is µ-strongly convex if and only if
(7.5)
⟨v, f ′′(x)v⟩= D2f(x)[v, v] ≥µ∥v∥2
for all x ∈X and all v ∈Rn.
Remark. If A is a symmetric operator, then
min
v̸=0
⟨v, Av⟩
∥v∥2
= λmin,
where λmin is the smallest eigenvalue of the operator. Thus, a two times
diﬀerentiable function f with a convex domain is µ-strongly convex if and
only if the eigenvalues of the hessian f ′′(x) are greater than or equal to µ for
each x in the domain.
Proof. Let φx,v(t) = f(x + tv). If condition (7.5) holds, then
φ′′
x,v(t) = D2f(x + tv)[v, v] ≥µ∥v∥2
for all t in the domain of the function. Using Taylor’s formula with remainder
term, we therefore conclude that
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
156
Smooth convex functions
φx,v(t) = φx,v(0) + φ′
x,v(0)t + 1
2φ′′
x,v(ξ)t2 ≥φx,v(0) + φ′
x,v(0)t + 1
2µ∥v∥2 t2.
For t = 1 this amounts to inequality (ii) in Theorem 7.3.1, and hence f is a
µ-strongly convex function.
Conversely, if f is µ-strongly convex, then by Theorem 7.3.1 (i)
φ′
x,v(t) −φ′
x,v(0)
t
= Df(x + tv)[tv] −Df(x)[tv]
t2
≥µ∥v∥2.
Taking the limit as t →0 we obtain
D2f(x)[v, v] = φ′′
x,v(0) ≥µ∥v∥2.
7.4
Convex functions with Lipschitz continu-
ous derivatives
The rate of convergence of classical iterative algorithms for minimizing func-
tions depends on the variation of the deriviative −the more the derivative
varies in a neighborhood of the minimum point, the slower the convergence.
The size of the Lipschitz constant is a measure of the variation of the deriva-
tive for functions with a Lipschitz continuous derivative. Therefore, we start
with a result which for arbitrary functions connects Lipschitz continuity of
the ﬁrst derivative to bounds on the second derivative.
Theorem 7.4.1. Suppose f is a twice diﬀerentiable function and that X is a
convex subset of its domain.
(i) If ∥f ′′(x)∥≤L for all x ∈X, then the derivative f ′ is Lipschitz con-
tinuous on X with Lipschitz constant L.
(ii) If the derivative f ′ is Lipschitz continuous on the set X with constant
L, then ∥f ′′(x)∥≤L for all x ∈int X.
Proof. (i) Suppose that ∥f ′′(x)∥≤L for all x ∈X, and let x and y be two
points in X. Put v = y −x, let w be an arbitrary vector with ∥w∥= 1, and
deﬁne the function φ for 0 ≤t ≤1 by
φ(t) = Df(x + tv)[w] = ⟨f ′(x + tv), w⟩.
Then φ is diﬀerentiable with derivative
φ′(t) = D2f(x + tv)[w, v] = ⟨w, f ′′(x + tv)v⟩
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
157
Smooth convex functions
so it follows from the Cauchy-Schwarz inequality that
|φ′(t)| ≤∥w∥∥f ′′(x + tv)v∥≤∥f ′′(x + tv)∥∥v∥≤L∥v∥,
since x + tv is a point in X. By the mean value theorem, φ(1) −φ(0) = φ′(s)
for some point s ∈]0, 1[. Consequently,
|⟨f ′(y) −f ′(x), w⟩| = |φ(1) −φ(0)| = |φ′(s)| ≤L∥y −x∥.
Since w is an arbitrary vector of norm 1, we conclude that
∥f ′(y) −f ′(x)∥= sup
∥w∥=1
⟨f ′(y) −f ′(x), w⟩≤L∥y −x∥,
i.e. the derivative f ′ is Lipschitz continuous on X with constant L.
(ii) Assume conversely that the ﬁrst derivative f ′ is Lipschitz continuous on
the set X with constant L. Let x be a point in the interior of X, and let v
and w be arbitrary vectors with norm 1. The function
φ(t) = Df(x + tv)[w] = ⟨f ′(x + tv, w⟩
is then deﬁned and diﬀerentiable and the point x + tv lies in X for all t in a
neighborhood of 0, and it follows that
|φ(t) −φ(0)| = |⟨f ′(x + tv) −f ′(x), w⟩| ≤∥f ′(x + tv) −f ′(x)∥∥w∥
≤L∥tv∥= L|t|.
Division by t and passing to the limit as t →0 results in the inequality
|⟨w, f ′′(x)v⟩| = |φ′(0)| ≤L
with the conclustion that
∥f ′′(x)∥= sup
∥v∥=1
∥f ′′(x)v∥=
sup
∥v∥,∥w∥=1
⟨w, f ′′(x)v⟩≤L.
Deﬁnition. A diﬀerentiable function f : X →R belongs to the class Sµ,L(X)
if f is µ-strongly convex and the derivative f ′ is Lipschitz continuous with
constant L. The quotient Q = L/µ is called the condition number of the
class.
Due to Theorem 7.3.1, a diﬀerentiable function f with a convex domain
X belongs to the class Sµ,L(X) if and only if it satisﬁes the following two
inequalities for all x, x + v ∈X:
⟨f ′(x + v) −f ′(x), v⟩≥µ∥v∥2
and
∥f ′(x + v) −f ′(x)∥≤L∥v∥.
If we combine the ﬁrst of these two inequalities with the Cauchy–Schwarz
inequality, we obtain the inequality µ∥v∥≤∥f ′(x+v)−f ′(x)∥, so we conclude
that µ ≤L and Q ≥1.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
158
Smooth convex functions
158
Example 7.4.1. Strictly convex quadratic functions
f(x) = 1
2⟨x, Px⟩+ ⟨q, x⟩+ r
belong to the class Sλmin,λmax(Rn), where λmin and λmax denote the smallest
and the largest eigenvalue, respectively, of the positive deﬁnite matrix P.
For f ′(x) = Px + q and f ′′(x) = P, whence
D2f(x)[v, v] = ⟨v, Pv⟩≥λmin∥v∥2
and
∥f ′(x + v) −f ′(x)∥= ∥Pv∥≤∥P∥∥v∥= λmax∥v∥.
The condition number of the quadratic function f is thus equal to the
quotient λmax/λmin between the largest and the smallest eigenvalue.
The sublevel sets {x | f(x) ≤α} of a strictly convex quadratic function
f are ellipsoids for all values of α greater than the minimum value of the
function, and the ratio of the longest and the shortest axes of any of these
ellipsoids is equal to

λmax/λmin, i.e. to the square root of the condition
number Q. This ratio is obviously also equal to the ratio of the radii of the
smallest ball containing and the largest ball contained in the ellipsoid. As
we shall see, something similar applies to all functions in the class Sµ,L(Rn).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
159
Smooth convex functions
Theorem 7.4.2. Let f be a function in the class Sµ,L(Rn) with minimum
point ˆx, and let α be a number greater than the minimum value f(ˆx). Then
B(ˆx; r) ⊆{x ∈X | f(x) ≤α} ⊆B(ˆx; R),
where r =

2L−1(α −f(ˆx)) and R =

2µ−1(α −f(ˆx)).
Remark. Note that R/r =

L/µ = √Q.
Proof. Since f ′(ˆx) = 0 we obtain the following inequalities from Theorems
1.1.2 and 7.3.1 (by replacing a and x respectively with ˆx and v with x −ˆv):
f(ˆx) + 1
2µ∥x −ˆx∥2 ≤f(x) ≤f(ˆx) + 1
2L∥x −ˆx∥2.
Hence, x ∈S = {x ∈X | f(x) ≤α} implies
1
2µ∥x −ˆx∥2 ≤f(x) −f(ˆx) ≤α −f(ˆx) = 1
2µR2,
which means that ∥x −ˆx∥≤R and proves the inclusion S ⊆B(ˆx; R).
And if x ∈B(ˆx; r), then f(x) ≤f(ˆx) + 1
2Lr2 = α, which means that
x ∈S and proves the inclusion B(ˆx; r) ⊆S.
Convex functions on Rn with Lipschitz continuous derivatives are char-
acterized by the following theorem.
Theorem 7.4.3. A diﬀerentiable function f : Rn →R is convex and its
derivative is Lipschitz continuous with Lipschitz constant L if and only if the
following mutually equivalent inequalities are fulﬁlled for all x, v ∈Rn:
f(x) + Df(x)[v] ≤f(x + v) ≤f(x) + Df(x)[v] + L
2 ∥v∥2
(i)
f(x + v) ≥f(x) + Df(x)[v] + 1
2L∥f ′(x + v) −f ′(x)∥2
(ii)
Df(x + v)[v] ≥Df(x)[v] + 1
L∥f ′(x + v) −f ′(x)∥2
(iii)
Proof. That inequality (i) has to be satisﬁed for convex functions with a
Lipschitz continuous derivative is a consequence of Theorems 1.1.2 and 7.2.1.
(i) ⇒(ii):
Let w = f ′(x + v) −f ′(x), and apply the right inequality in
(i) with x replaced by x + v and v replaced by −L−1w; this results in the
inequality
f(x + v −L−1w) ≤f(x + v) −L−1Df(x + v)[w] + 1
2L−1∥w∥2.
The left inequality in (i) with v −L−1w instead of v yields
f(x + v −L−1w) ≥f(x) + Df(x)[v −L−1w].
By combining these two new inequalities, we obtain
f(x + v) ≥f(x) + Df(x)[v −L−1w] + L−1Df(x + v)[w] −1
2L−1∥w∥2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
160
Smooth convex functions
= f(x) + Df(x)[v] + L−1
Df(x + v)[w] −Df(x)[w]

−1
2L−1∥w∥2
= f(x) + Df(x)[v] + L−1⟨f ′(x + v) −f ′(x), w⟩−1
2L−1∥w∥2
= f(x) + Df(x)[v] + L−1⟨w, w⟩−1
2L−1∥w∥2
= f(x) + Df(x)[v] + 1
2L−1∥w∥2,
and that is inequality (ii).
(ii) ⇒(iii):
Add inequality (ii) to the inequality obtained by changing x to
x + v and v to −v. The result is inequality (iii).
Let us ﬁnally assume that inequality (iii) holds. The convexity of f is then
a consequence of Theorem 7.2.1, and by combining (iii) with the Cauchy–
Schwarz inequality, we obtain the inequality
1
L∥f ′(x + v) −f ′(x)∥2 ≤Df(x + v)[v] −Df(x)[v] = ⟨f ′(x + v) −f ′(x), v⟩
≤∥f ′(x + v) −f ′(x)∥· ∥v∥,
which after division by ∥f ′(x + v) −f ′(x)∥gives us the desired conclusion:
the derivative is Lipschitz continuous with Lipschitz constant L.
Theorem 7.4.4. If f ∈Sµ,L(Rn), then
Df(x + v)[v] ≥Df(x)[v] +
µL
µ + L∥v∥2 +
1
µ + L∥f ′(x + v) −f ′(x)∥2
for all x, v ∈Rn.
Proof. Let g(x) = f(x) −1
2µ∥x∥2; the function g is then convex, and since
Dg(x)[v] = Df(x)[v] −µ⟨x, v⟩, it follows from Theorem 1.1.2 that
g(x + v) = f(x + v) −1
2µ∥x + v∥2
≤f(x) + Df(x)[v] + 1
2L∥v∥2 −1
2µ∥x + v∥2
= g(x) + 1
2µ∥x∥2 + Dg(x)[v] + µ⟨x, v⟩+ 1
2L∥v∥2 −1
2µ∥x + v∥2
= g(x) + Dg(x)[v] + 1
2(L −µ)∥v∥2.
This shows that g satisﬁes condition (i) in Theorem 7.4.3 with L replaced by
L −µ. The derivative g′ is consequently Lipschitz continuous with constant
L −µ. The same theorem now gives us the inequality
Dg(x + v)[v] ≥Dg(x)[v] +
1
L −µ∥g′(x + v) −g′(x)∥2,
which is just a reformulation of the inequality in Theorem 7.4.4.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
161
Smooth convex functions
Exercises
7.1 Show that the following functions are convex.
a) f(x1, x2) = ex1 + ex2 + x1x2,
x1 + x2 > 0
b) f(x1, x2) = sin(x1 + x2),
−π < x1 + x2 < 0
c) f(x1, x2) = −

cos(x1 + x2),
−π
2 < x1 + x2 < π
2 .
7.2 Is the function f(x1, x2) = x2
1/x2+x2
2/x1 convex in the ﬁrst quadrant x1 > 0,
x2 > 0?
7.3 Show that the function f(x) = n−1
j=1 x2
j/xn is convex in the halfspace xn > 0.
7.4 Show that the following function is concave on the set [0, 1[×[0, 1[×[0, 1[:
f(x1, x2, x3) = ln(1 −x1) + ln(1 −x2) + ln(1 −x3)
−(x2
1 + x2
2 + x2
3 + x1x2 + x1x3 + x2x3).
7.5 Let I be an interval and suppose that the function f : I →R is convex. Show
that f is either increasing on the interval, or decreasing on the interval, or
there exists a point c ∈I such that f is decreasing to the left of c and
increasing to the right of c.
7.6 Suppose f : ]a, b[ →R is a convex function.
a) Prove that the two one-sided limits limx→a+ f(x) and limx→b−f(x) exist
(as ﬁnite numbers or ±∞).
b) Suppose that the interval is ﬁnite, and extend the function to the closed
interval [a, b] by deﬁning f(a) = α and f(b) = β. Prove that the extended
function is convex if and only if α ≥limx→a+ f(x) and β ≥limx→b−f(x).
7.7 Prove that a continuous function f : [a, b] →R is convex if and only if its
restriction to the open interval ]a, b[ is convex.
7.8 F is a family of diﬀerentiable functions on Rn with the following two prop-
erties:
(i) f ∈F ⇒f + g ∈F for all aﬃne functions g: Rn →R.
(ii) If f ∈F and f′(x0) = 0, then x0 is a minimum point of f.
Prove that all functions in F are convex.
7.9 Suppose that f : X →R is a twice diﬀerentiable convex function. Prove
that its recessive subspace Vf is a subset of N(f′′(x)) for each x ∈X.
7.10 Let f : X →R be a diﬀerentiable function with a convex domain X. Prove
that f is quasiconvex if and only if
f(x + v) ≤f(x) ⇒Df(x)[v] ≤0
for all x, x + v ∈X.
[Hint: It suﬃces to prove the assertion for functions on R; the general result
then follows by taking restrictions to lines.]
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
162
Smooth convex functions
162
7.11 Let f : X →R be a twice diﬀerentiable function with a convex domain X.
Prove the following assertions:
a) If f is quasiconvex, then
Df(x)[v] = 0 ⇒D2f(x)[v, v] ≥0
for all x ∈X and all v ∈Rn.
b) If
Df(x)[v] = 0 ⇒D2f(x)[v, v] > 0
for all x ∈X and all v ̸= 0, then f is quasiconvex.
[Hint: It is enough to prove the results for functions deﬁned on R.]
7.12 Prove that the function α1f1 + α2f2 is (α1µ1 + α2µ2)-strongly convex if f1
is µ1-strongly convex, f2 is µ2-strongly convex and α1, α2 > 0.
7.13 Prove that if a diﬀerentiable µ-strongly convex function f : X →R has a
minimum at the point ˆx, then ∥x −ˆx∥≤µ−1∥f′(x)∥for all x ∈X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
163
The subdifferential
Chapter 8
The subdiﬀerential
We will now generalize a number of results from the previous chapter to
convex functions that are not necessarily diﬀerentiable everywhere. However,
real-valued convex functions with open domains can not be too irregular −
they are, as already noted, continuous, and they have direction derivatives.
8.1
The subdiﬀerential
If f is a diﬀerentiable function, then y = f(a) + ⟨f ′(a), x −a⟩is the equation
of a hyperplane that is tangent to the surface y = f(x) at the point (a, f(a)).
And if f is also convex, then f(x) ≥f(a) + ⟨f ′(a), x −a⟩for all x in the
domain of the function (Theorem 7.2.1), so the tangent plane lies below the
graph of the function and is a supporting hyperplane of the epigraph.
The epigraph of an arbitrary convex function is a convex set, by deﬁnition.
Hence, through each boundary point belonging to the epigraph of a convex
function there passes a supporting hyperplane. The supporting hyperplanes
of a convex one-variable function f, deﬁned on an open interval, are given
by Theorem 7.1.2, which says that the line y = f(x0) + a(x −x0) supports
the epigraph at the point (x0, f(x0)) if (and only if) f ′
−(x0) ≤a ≤f ′
+(x0).
The existence of supporting hyperplanes characterizes convexity, and this
is a reason for a more detailed study of this concept.
Deﬁnition. Let f : X →R be a function deﬁned on a subset X of Rn. A
vector c ∈Rn is called a subgradient of f at the point a ∈X if the inequality
(8.1)
f(x) ≥f(a) + ⟨c, x −a⟩
holds for all x ∈X.
The set of all subgradients of f at a is called the subdiﬀerential of f at a
and is denoted by ∂f(a).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
164
The subdifferential
y = |x|
y = ax
y
x
Figure 8.1. The line y = ax is a supporting line of
the function f(x) = |x| at the origin if −1 ≤a ≤1.
Remark. The inequality (8.1) is of course satisﬁed by all points a ∈X and
all vectors c ∈Rn if x is a point in the set X \ dom f. Hence, to verify that
c is a subgradient of f at a it suﬃces to verify that the inequality holds for
all x ∈dom f.
The inequality (8.1) does not hold for any vector c if a is a point in
X \dom f and x is a point in dom f. Hence, ∂f(a) = ∅for all a ∈X \dom f,
except in the trivial case when dom f = ∅, i.e. when f is equal to ∞on
the entire set X. In this case we have ∂f(a) = Rn for all a ∈X since the
inequality (8.1) is now trivially satisﬁed by all a, x ∈X and all c ∈Rn.
Example 8.1.1. The subdiﬀerentials of the one-variable function f(x) = |x|
are
∂f(a) =





{−1}
if a < 0,
[−1, 1]
if a = 0,
{1}
if a > 0.
Theorem 8.1.1. The subdiﬀerentials of an arbitrary function f : X →R are
closed and convex sets.
Proof. For points a ∈dom f,
∂f(a) = 
x∈dom f{c ∈Rn | ⟨c, x −a⟩≤f(x) −f(a)}
is convex and closed, since it is an intersection of closed halfspaces, and the
case a ∈X \ dom f is trivial.
Theorem 8.1.2. A point a ∈X is a global minimum point of the function
f : X →R if and only if 0 ∈∂f(a).
Proof. The assertion follows immediately from the subgradient deﬁnition.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
165
The subdifferential
165
Our next theorem tells us that the derivative f ′(a) is the only subgradient
candidate for functions f that are diﬀerentiable at a.
Geometrically this
means that the tangent plane at a is the only possible supporting hyperplane.
Theorem 8.1.3. Suppose that the function f : X →R is diﬀerentiable at the
point a ∈dom f. Then either ∂f(a) = {f ′(a)} or ∂f(a) = ∅.
Proof. Suppose c ∈∂f(a). By the diﬀerentiability deﬁnition,
f(a + v) −f(a) = ⟨f ′(a), v⟩+ r(v)
with a remainder term r(v) satisfying the condition
lim
v→0
r(v)
∥v∥= 0,
and by the subgradient deﬁnition, f(a + v) −f(a) ≥⟨c, v⟩for all v such that
a + v belongs to X. Consequently,
(8.2)
⟨c, v⟩
∥v∥≤⟨f ′(a), v⟩+ r(v)
∥v∥
for all v with a suﬃciently small norm ∥v∥.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
166
The subdifferential
Let ej be the j:th unit vector. Then ⟨c, ej⟩= cj and ⟨f ′(a), ej⟩= ∂f
∂xj
(a),
so by choosing v = tej in inequality (8.2), noting that ∥tej∥= |t|, and letting
t →0 from the right and from the left, respectively, we obtain the following
two inequalities
cj ≤∂f
∂xj
(a)
and
−cj ≤−∂f
∂xj
(a),
which imply that cj = ∂f
∂xj
(a). Hence, c = f ′(a), and this proves the inclusion
∂f(a) ⊆{f ′(a)}.
We can now reformulate Theorem 7.2.1 as follows: A diﬀerentiable func-
tion with a convex domain is convex if and only if it has a subgradient (which
is then equal to the derivative) everywhere. Our next theorem generalizes
this result.
Theorem 8.1.4. Let f : X →R be a function with a convex domain X.
(a) If dom f is a convex set and ∂f(x) ̸= ∅for all x ∈dom f, then f is a
convex function.
(b) If f is a convex function, then ∂f(x) ̸= ∅for all x ∈rint(dom f).
Proof. (a) Let x and y be two arbitrary points in dom f and consider the
point z = λx+(1−λ)y, where 0 < λ < 1. By assumption, f has a subgradient
c at the point z. Using the inequality (8.1) at the point a = z twice, one
time with x replaced by y, we obtain the inequality
λf(x) + (1 −λ)f(y) ≥λ

f(z) + ⟨c, x −z⟩

+ (1 −λ)

f(z) + ⟨c, y −z⟩

= f(z) + ⟨c, λx + (1 −λ)y −z⟩= f(z) + ⟨c, 0⟩= f(z),
which shows that the restriction of f to dom f is a convex function, and this
implies that f itself is convex.
(b) Conversely, assume that f is a convex function, and let a be a point in
rint(dom f). We will prove that the subdiﬀerential ∂f(a) is nonempty.
The point (a, f(a)) is a relative boundary point of the convex set epi f.
Therefore, there exists a supporting hyperplane
H = {(x, xn+1) ∈Rn × R | ⟨c, x −a⟩+ cn+1(xn+1 −f(a)) = 0}
of epi f at the point (a, f(a)), and we may choose the normal vector (c, cn+1)
in such a way that
(8.3)
⟨c, x −a⟩+ cn+1(xn+1 −f(a)) ≥0
for all points (x, xn+1) ∈epi f. We shall see that this implies that cn+1 > 0.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
167
The subdifferential
By applying inequality (8.3) to the point (a, f(a)+1) in the epigraph, we
ﬁrst obtain the inequality cn+1 ≥0.
Now suppose that cn+1 = 0, and put L = aﬀ(dom f). Since epi f ⊆L×R
and the supporting hyperplane H = {(x, xn+1) ∈Rn × R | ⟨c, x−a⟩= 0} by
deﬁnition does not contain epi f as a subset, it does not contain L×R either.
We conclude that there exists a point y ∈L such that ⟨c, y−a⟩̸= 0. Consider
the points yλ = (1 −λ)a + λy for λ ∈R; these points lie in the aﬁne set L,
and yλ →a as λ →0. Since a is a point in the relative interior of dom f,
the points yλ lie in dom f if |λ| is suﬃciently small, and this implies that
the inequality (8.3) can not hold for all points (yλ, f(yλ)) in the epigraph,
because the expression ⟨c, yλ −a⟩(= λ⟨c, y −a⟩) assumes both positive and
negative values depending on the sign of λ.
This is a contradiction and proves that cn+1 > 0, and by dividing inequal-
ity (8.3) by cn+1 and letting d = −(1/cn+1)c, we obtain the inequality
xn+1 ≥f(a) + ⟨d, x −a⟩
for all (x, xn+1) ∈epi f.
In particular, f(x) ≥f(a) + ⟨d, x −a⟩for all
x ∈dom f, which means that d is a subgradient of f at a.
It follows from Theorem 8.1.4 that a real-valued function f with an open
convex domain X is convex if and only if ∂f(x) ̸= ∅for all x ∈X.
Theorem 8.1.5. The subdiﬀerential ∂f(a) of a convex function f is a com-
pact nonempty set if a is an interior point of dom f.
Proof. Suppose a is a point in int(dom f). The subdiﬀerential ∂f(a) is closed
by Theorem 8.1.1 and nonempty by Theorem 8.1.4, so it only remains to
prove that it is a bounded set.
Theorem 6.6.1 yields two positive constants M and δ such that the closed
ball B(a; δ) lies in dom f and
|f(x) −f(a)| ≤M∥x −a∥
for x ∈B(a; δ).
Now suppose that c ∈∂f(a) and that c ̸= 0. By choosing x = a + δc/∥c∥
in inequality (8.1), we conclude that
δ∥c∥= ⟨c, x −a⟩≤f(x) −f(a) ≤M∥x −a∥= δM
with the bound ∥c∥≤M as a consequence. The subdiﬀerential ∂f(a) is thus
included in the closed ball B(0; M).
Theorem 8.1.6. The sublevel sets of a strongly convex function f : X →R
are bounded sets.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
168
The subdifferential
168
Proof. Suppose that f is µ-strongly convex. Let x0 be a point in the relative
interior of dom f, and let c be a subgradient at the point x0 of the convex
function g(x) = f(x) −1
2µ∥x∥2. Then, for each x belonging to the sublevel
set S = {x ∈X | f(x) ≤α},
α ≥f(x) = g(x) + 1
2µ∥x∥2 ≥g(x0) + ⟨c, x −x0⟩+ 1
2µ∥x∥2
= f(x0) −1
2µ∥x0∥2 + ⟨c, x −x0⟩+ 1
2µ∥x∥2
= f(x0) + 1
2µ

∥x + µ−1c∥2 −∥x0 + µ−1c∥2
,
which implies that
∥x + µ−1c∥2 ≤∥x0 + µ−1c∥2 + 2µ−1(α −f(x0)).
The sublevel set S is thus included in a closed ball with center at the point
−µ−1c and radius R =

∥x0 + µ−1c∥2 + 2µ−1(α −f(x0)) .
Corollary 8.1.7. If a continuous and strongly convex function has a nonempty
closed sublevel set, then it has a unique minimum point.
In particular, every strongly convex function f : Rn →R has a unique
minimum point.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
169
The subdifferential
Proof. Let f be a continuous, strongly convex function with a nonempty
closed sublevel set S. Then S is compact by the previous theorem, so the
restriction of f to S assumes a minimum at some point in S, and this point
is obviously a global minimum point of f. The minimum point is unique,
because strongly convex functions are strictly convex.
A convex function f : Rn →R is automatically continuous, and continu-
ous functions on Rn are closed. Hence, all sublevel sets of a strongly convex
function on Rn are closed, so it follows from the already proven part of the
theorem that there is a unique minimum point.
8.2
Closed convex functions
In this section, we will use the subdiﬀerential to supplement the results on
closed convex functions in chapter 6.8 with some new results. We begin with
an alternative characterization of closed convex functions.
Theorem 8.2.1. A convex function f : X →R is closed if and only if, for
all convergent sequences (xk)∞
1 of points in dom f with limit x0,
(8.4)
lim
k→∞
f(xk)

≥f(x0)
if x0 ∈dom f,
= +∞
if x0 ∈cl(dom f) \ dom f.
Proof. Suppose that f is closed, i.e. that epi f is a closed set, and let (xk)∞
1
be a sequence in dom f which converges to a point x0 ∈cl(dom f), and put
L = lim
k→∞
f(xk).
Let a be an arbitrary point in the relative interior of dom f and let c be a
subgradient of f at the point a. Then f(xk) ≥f(a) + ⟨c, xk −a⟩for all k,
and since the right hand side converges (to f(a) + ⟨c, x0 −a⟩) as k →∞, we
conclude that the sequence (f(xk))∞
1 is bounded below. Its least limit point,
i.e. L, is therefore a real number or +∞.
Inequality (8.4) is trivially satisﬁed if L = +∞, so assume that L is a
ﬁnite number, and let (xkj)∞
j=1 be a subsequence of the given sequence with
the property that f(xkj) →L as j →∞. The points (xkj, f(xkj)), which
belong to epi f, then converge to the point (x0, L), and since the epigraph is
assumed to be closed, we conclude that the limit point (x0, L) belongs to the
epigraph, i.e. x0 ∈dom f and L ≥f(x0).
So if x0 does not belong to dom f but to cl(dom f)\dom f, then we must
have L = +∞. This proves that (8.4) holds.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
170
The subdifferential
Conversely, suppose (8.4) holds for all convergent sequences, and let
((xk.tk))∞
1 be a sequence of points in epi f which converges to a point (x0, t0).
Then, (xk)∞
1 converges to x0 and (tk)∞
1 converges to t0, and since f(xk) ≤tk
for all k, we conclude that
lim
k→∞
f(xk) ≤lim
k→∞
tk = t0.
In particular, limk→∞f(xk) < +∞, so it follows from inequality (8.4) that
x0 ∈dom f and that f(x0) ≤t0. This means that the limit point (x0, t0)
belongs to epi f. Hence, epi f contains all its boundary points and is therefore
a closed set, and this means that f is a closed function.
Corollary
8.2.2. Suppose that f : X →R is a convex function and that
its eﬀective domain dom f is relative open. Then, f is closed if and only
if limk→∞f(xk) = +∞for each sequence (xk)∞
1
of points in dom f that
converges to a relative boundary point of dom f.
Proof. Since a convex function is continuous at all points in the relative
interior of its eﬀective domain, we conclude that limk→∞f(xk) = f(x0) for
each sequence (xk)∞
1 of points in dom f that converges to a point x0 in dom f.
Condition (8.4) of the previous theorem is therefore fulﬁlled if and only if
limk→∞f(xk) = +∞for all sequences (xk)∞
1
in dom f that converge to a
point in rbdry(dom f).
So a convex function with an aﬃne set as eﬀective domain is closed (and
continuous), because aﬃne sets lack relative boundary points.
Example 8.2.1. The convex function f(x) = −ln x with R++ as domain is
closed, since lim
x→0 f(x) = +∞.
Theorem 8.2.3. If the function f : X →R is convex and closed, then
f(x) = lim
λ→1−f(λx + (1 −λ)y)
for all x, y ∈dom f.
Proof. The inequality
lim
λ→1−f(λx + (1 −λ)y) ≤lim
λ→1−(λf(x) + (1 −λ)f(y)) = f(x)
holds for all convex functions f, and the inequality
lim
λ→1−f(λx + (1 −λ)y) ≥f(x)
holds for all closed convex functions f according to Theorem 8.2.1.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
171
The subdifferential
171
Theorem 8.2.4. Suppose that f and g are two closed convex functions, that
rint(dom f) = rint(dom g)
and that
f(x) = g(x)
for all x ∈rint(dom f). Then f = g.
We remind the reader that the equality f = g should be interpreted as
dom f = dom g and f(x) = g(x) for all points x in the common eﬀective
domain.
Proof. If rint(dom f) = ∅, then dom f = dom g = ∅, and there is nothing to
prove, so suppose that x0 is a point in rint(dom f). Then, λx + (1 −λ)x0 lies
in rint(dom f), too, for each x ∈dom f and 0 < λ < 1, and it follows from
our assumptions and Theorem 8.2.3 that
g(x) = lim
λ→1−g(λx + (1 −λ)x0) = lim
λ→1−f(λx + (1 −λ)x0) = f(x).
Hence, g(x) = f(x) for all x ∈dom f, and it follows that dom f ⊆dom g.
The converse inclusion holds by symmetry, so dom f = dom g.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
172
The subdifferential
Theorem 8.2.5. Let f : X →R and g: Y →R be two closed convex func-
tions with X ∩Y ̸= ∅. The sum f + g: X ∩Y →R is then a closed convex
function.
Proof. The theorem follows from the characterization of closedness in Theo-
rem 8.2.1. Let (xk)∞
1 be a convergent sequence of points in dom(f + g) with
limit point x0. If x0 belongs to dom(f + g) (= dom f ∩dom g), then
lim
k→∞
(f(xk) + g(xk)) ≥lim
k→∞
f(xk) + lim
k→∞
g(xk) ≥f(x0) + g(x0),
and if x0 does not belong to dom(f + g), then we use the trivial inclusion
cl(A ∩B) \ A ∩B ⊆(cl A \ A) ∪(cl B \ B),
with A = dom f and B = dom g, to conclude that the sum f(xk) + g(xk)
tends to +∞, because one of the two sequences (f(xk))∞
1 and (g(xk))∞
1 tends
to +∞while the other either tends to +∞or has a ﬁnite limes inferior.
The closure
Deﬁnition. Let f : X →R be a function deﬁned on a subset of Rn and
deﬁne (cl f)(x) for x ∈Rn by
(cl f)(x) = inf{t | (x, t) ∈cl(epi f)}.
The function cl f : Rn →R is called the closure of f.
Theorem 8.2.6. The closure cl f of a convex function f, whose eﬀective
domain is a nonempty subset of Rn, has the following properties:
(i)
The closure cl f : Rn →R is a convex function.
(ii)
dom f ⊆dom(cl f) ⊆cl(dom f).
(iii)
rint(dom(cl f)) = rint(dom f).
(iv)
(cl f)(x) ≤f(x) for all x ∈dom f.
(v)
(cl f)(x) = f(x) for all x ∈rint(dom f).
(vi)
epi(cl f) = cl(epi f).
Proof. (i) Let x0 be an arbitrary point in rint(dom f), and let c be a sub-
gradient of f at the point x0.
Then f(x) ≥f(x0) + ⟨c, x −x0⟩for all
x ∈dom f, which means that the epigraph epi f is a subset of the closed
set K = {(x, t) ∈cl(dom f) × R | ⟨c, x −x0⟩+ f(x0) ≤t}. It follows that
cl(epi f) ⊆K, and hence
(cl f)(x) = inf{t | (x, t) ∈cl(epi f)} ≥inf{t | (x, t) ∈K}
= f(x0) + ⟨c, x −x0⟩> −∞
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
173
The subdifferential
for all x ∈Rn. So R is a codomain of the function cl f, and since cl(epi f)
is a convex set, it now follows from Theorem 6.2.6 that cl f : Rn →R is a
convex function.
(ii), (iv) and (v) It follows from the inclusion epi f ⊆cl(epi f) ⊆K that
(cl f)(x)

≤inf{t | (x, t) ∈epi f} = f(x) < +∞
if x ∈dom f,
≥inf{t | (x, t) ∈K} = inf ∅= +∞
if x /∈cl(dom f),
(cl f)(x0) ≥inf{t | (x0, t) ∈K} = f(x0).
This proves that dom f ⊆dom(cl f) ⊆cl(dom f), that (cl f)(x) ≤f(x) for
all x ∈dom f, and that (cl f)(x0) = f(x0), and since x0 is an arbitrary point
in rint(dom f), we conclude that (cl f)(x) = f(x) for all x ∈rint(dom f).
(iii) Since rint(cl X) = rint X for arbitrary convex sets X, it follows in par-
ticular from (ii) that
rint(dom f) ⊆rint(dom(cl f)) ⊆rint(cl(dom f)) = rint(dom f),
with the conclusion that rint(dom(cl f)) = rint(dom f).
(vi) The implications
(x, t) ∈cl(epi f) ⇒(cl f)(x) ≤t ⇒(x, t) ∈epi(cl f)
follow immediately from the closure and epigraph deﬁnitions. Conversely,
suppose that (x, t) is a point in epi(cl f), i.e. that (cl f)(x) ≤t, and let
U × I be an open neighborhood of (x, t). The neighborhood I of t contains
a number s such that (x, s) ∈cl(epi f), and since U × I is also an open
neighborhood of (x, s), it follows that epi f ∩(U × I) ̸= ∅. This proves that
(x, t) ∈cl(epi f), so we have the implication
(x, t) ∈epi(cl f) ⇒(x, t) ∈cl(epi f).
Thus, epi(cl f) = cl(epi f).
Theorem 8.2.7. If f is a closed convex function, then cl f = f.
Proof. We have rint(dom(cl f)) = rint(dom f) and (cl f)(x) = f(x) for all
x ∈rint(dom f), by the previous theorem. Therefore it follows from Theorem
8.2.4 that cl f = f.
8.3
The conjugate function
Deﬁnition. Let f : X →R be an arbitrary function deﬁned on a subset X
of Rn and deﬁne a function f ∗on Rn by
f ∗(y) = sup{⟨y, x⟩−f(x) | x ∈X}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
174
The subdifferential
for y ∈Rn. The function f ∗is called the conjugate function or the Fenchel
transform of f.
We use the shorter notation f ∗∗for the conjugate function of f ∗, i.e.
f ∗∗= (f ∗)∗.
The conjugate function f ∗of a function f : Rn →R with a nonempty
eﬀective domain is obviously a function Rn →R, and
f ∗(y) = sup{⟨y, x⟩−f(x) | x ∈dom f}.
There are two trivial cases: If the eﬀective domain of f : X →R is empty,
then f ∗(y) = −∞for all y ∈Rn, and if f : X →R assumes the value −∞
at some point, then f ∗(y) = +∞for all y ∈Rn.
x
y
(0, −f∗(c))
y = cx
y = f(x)
Figure 8.2.
A graphical illustration of the conjugate function f∗when f
is a one-variable function. The function value f∗(c) is equal to the maximal
vertical distance between the line y = cx and the curve y = f(x). If f is
diﬀerentiable, then f∗(c) = cx0 −f(x0) for some point x0 with f′(x0) = c.
Example 8.3.1. The support functions that were deﬁned in Section 6.9, are
conjugate functions.
To see this, deﬁne for a given subset A of Rn the
function χA : Rn →R by
χA(x) =

0
if x ∈A,
+∞
if x /∈A.
The function χA is called the indicator function of the set A, and it is a
convex function if A is a convex set. Obviously,
χ∗
A(y) = sup{⟨y, x⟩| x ∈A} = SA(y)
for all y ∈Rn, so the support function of A coincides with the conjugate
function χ∗
A of the indicator function of A.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
175
The subdifferential
175
We are primarily interested in conjugate functions of convex functions
f : X →R, but we start with some general results.
Theorem 8.3.1. The conjugate function f ∗of a function f : X →R with a
nonempty eﬀective domain is convex and closed.
Proof. The epigraph epi f ∗consists of all points (y, t) ∈Rn × R that satisfy
the inequalities ⟨x, y⟩−t ≤f(x) for all x ∈dom f, which means that it is
the intersection of a family of closed halfspaces in Rn × R. Hence, epi f ∗is
a closed convex set, so the conjugate function f ∗is closed and convex.
Theorem 8.3.2 (Fenchel’s inequality). Let f : X →R be a function with a
nonempty eﬀective domain. Then
⟨x, y⟩≤f(x) + f ∗(y)
for all x ∈X and all y ∈Rn. Moreover, the two sides are equal for a given
x ∈dom f if and only if y ∈∂f(x).
Proof. The inequality follows immediately from the deﬁnition of f ∗(y) as a
least upper bound if x ∈dom f, and it is trivially true if x ∈X \ dom f.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
176
The subdifferential
Moreover, by the subgradient deﬁnition, if x ∈dom f then
y ∈∂f(x) ⇔f(z) −f(x) ≥⟨y, z −x⟩
for all z ∈dom f
⇔⟨y, z⟩−f(z) ≤⟨y, x⟩−f(x)
for all z ∈dom f
⇔f ∗(y) ≤⟨y, x⟩−f(x)
⇔f(x) + f ∗(y) ≤⟨x, y⟩,
and by combining this with the already proven Fenchel inequality, we obtain
the equivalence y ∈∂f(x) ⇔f(x) + f ∗(y) = ⟨x, y⟩.
By the previous theorem, for all points y in the set {∂f(x) | x ∈dom f}
f ∗(y) = ⟨xy, y⟩−f(xy),
where xy is a point satisfying the condition y ∈∂f(xy). For diﬀerentiable
functions f we obtain the points xy as solutions to the equation f ′(x) = y.
Here follows a concrete example.
Example 8.3.2. Let f : ]−1, ∞[→R be the function
f(x) =









−x(x + 1)−1
if −1 < x ≤0,
2x
if 0 ≤x < 1,
(x −2)2 + 1
if 1 ≤x < 2,
2x −3
if x ≥2.
Its graph is shown in the left part of Figure 8.3.
−1
1
2
3
x
1
2
3
y
1
2
3
−1
−2
−3
−4
1
2
+∞
Figure 8.3. To the left the graph of the function f : ]−1, ∞[→R,
and to the right the graph of the conjugate function f∗: R →R.
A look at the ﬁgure shows that the curve y = f(x) lies above all lines
that are tangent to the curve at a point (x, y) with −1 < x < 0, lies above
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
177
The subdifferential
all lines through the origin with a slope between f ′
−(0) = −1 and the slope of
the chord that connects the origin and the point (2, 1) on the curve, and lies
above all lines through the point (2, 1) with a slope between 1
2 and f ′
+(2) = 2.
This means that

{∂f(x)} =

−1<x<0
{f ′(x)} ∪∂f(0) ∪∂f(2) ∪

x>2
{f ′(x)}
= ]−∞, −1[ ∪[−1, 1
2] ∪[ 1
2, 2] ∪{2} = ]−∞, 2].
The equation f ′(x) = c has for c < −1 the solution x = −1 +

−1/c in
the interval −1 < x < 0. Let
xc =





−1 +

−1/c
if c < −1,
0
if −1 ≤c ≤1
2,
2
if 1
2 ≤c ≤2.
Then c ∈∂f(xc), and it follows from the remark after Theorem 8.3.2 that
f ∗(c) = cxc −f(xc) =





−c −2√−c + 1
if c < −1,
0
if −1 ≤c ≤1
2,
2c −1
if 1
2 ≤c ≤2.
Since
f ∗(c) = sup
x>−1{cx −f(x)} ≥sup
x≥2
{cx −f(x)} = sup
x≥2
{(c −2)x + 3} = +∞
if c > 2, we conclude that dom f ∗=]−∞, 2]. The graph of f ∗is shown in the
right part of Figure 8.3.
Theorem 8.3.3. Let f : X →R be an arbitrary function. Then
f ∗∗(x) ≤f(x)
for all x ∈X. Furthermore, f ∗∗(x) = f(x) if x ∈dom f and ∂f(x) ̸= ∅.
Proof. If f(x) = +∞for all x ∈X, then f ∗≡−∞and f ∗∗≡+∞, according
to the remarks following the deﬁnition of the conjugate function, so the
inequality holds with equality for all x ∈X in this trivial case.
Suppose, therefore, that dom f ̸= ∅.
Then ⟨x, y⟩−f ∗(y) ≤f(x) for
all x ∈X and all y ∈dom f ∗because of Fenchel’s inequality, and hence
f ∗∗(x) = sup{⟨x, y⟩−f ∗(y) | y ∈dom f ∗} ≤f(x).
If ∂f(x) ̸= ∅, then Fenchel’s inequality holds with equality for y ∈∂f(x).
This means that f(x) = ⟨x, y⟩−f ∗(y) ≤f ∗∗(x) and implies that f(x) =
f ∗∗(x).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
178
The subdifferential
178
The following corollary follows immediately from Theorem 8.3.3, because
convex functions have subgradients at all relative interior points of their
eﬀective domains.
Corollary 8.3.4. If f : X →R is a convex function, then f ∗∗(x) = f(x) for
all x in the relative interior of dom f.
We will prove that f ∗∗= cl f if f is a convex function, and for this purpose
we need the following lemma.
Lemma 8.3.5. Suppose that f is a convex function and that (x0, t0) is a
point in Rn × R which does not belong to cl(epi f). Then there exist a vector
c ∈Rn and a real number d such that the ”non-vertical” hyperplane
H = {(x, xn+1) | xn+1 = ⟨c, x⟩+ d}
strictly separates the point (x0, t0) from cl(epi f).
Proof. By the Separation Theorem 3.1.3, there exists a hyperplane
H = {(x, xn+1) | cn+1xn+1 = ⟨c, x⟩+ d}
which strictly separates the point from cl(epi f). If cn+1 ̸= 0, we can without
loss of generality assume that cn+1 = 1, and there is nothing more to prove.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
179
The subdifferential
So assume that cn+1 = 0, and choose the signs of c and d so that ⟨c, x0⟩+
d > 0 and ⟨c, x⟩+ d < 0 for all x ∈dom f.
Using the subgradient c′ at some point in the relative interior of dom f
we obtain an aﬃne function ⟨c′, x⟩+ d′ such that f(x) ≥⟨c′, x⟩+ d′ for all
x ∈dom f. This implies that
f(x) ≥⟨c′, x⟩+ d′ + λ(⟨c, x⟩+ d) = ⟨c′ + λc, x⟩+ d′ + λd
for all x ∈dom f and all positive numbers λ, while
⟨c′ + λc, x0⟩+ d′ + λd = ⟨c′, x0⟩+ d′ + λ(⟨c, x0⟩+ d) > t0
for all suﬃciently large numbers λ.
So the epigraph epi f lies above the
hyperplane
Hλ = {(x, xn+1) | xn+1 = ⟨c′ + λc, x⟩+ d′ + λd}.
and the point (x0, t0) lies strictly below the same hyperplane, if the number
λ is big enough.
By moving the hyperplane Hλ slightly downwards, we
obtain a parallel non-vertical hyperplane which strictly separates (x0, t0) and
cl(epi f).
Lemma 8.3.6. If f : X →R is a convex function, then
rint(dom f ∗∗) = rint(dom f).
Proof. Since rint(dom f) = rint(cl(dom f)), it suﬃces to prove the inclusion
dom f ⊆dom f ∗∗⊆cl(dom f).
The left inclusion follows immediately from the inequality in Theorem 8.3.3.
To prove the right inclusion, we assume that x0 /∈cl(dom f) and shall prove
that this implies that x0 /∈dom f ∗∗.
It follows from our assumption that the points (x0, t0) do not belong
to cl(epi f) for any number t0.
Thus, given t0 ∈R there exists, by the
previous lemma, a hyperplane H = {(x, xn+1) ∈Rn × R | xn+1 = ⟨c, x⟩+ d}
which strictly separates (x0, t0) and cl(epi f). Hence, t0 < ⟨c, x0⟩+ d and
⟨c, x⟩+ d < f(x) for all x ∈dom f. Consequently,
−d ≥sup{⟨c, x⟩−f(x) | x ∈dom f} = f ∗(c),
and hence
t0 < ⟨c, x0⟩+ d ≤⟨c, x0⟩−f ∗(c) ≤f ∗∗(x0).
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
180
The subdifferential
Since this holds for all real numbers t0, it follows that f ∗∗(t0) = +∞, which
means that x0 /∈dom f ∗∗.
Theorem 8.3.7. If f is a convex function, then f ∗∗= cl f.
Proof. It follows from Lemma 8.3.6 and Theorem 8.2.6 (iii) that
rint(dom f ∗∗) = rint(dom(cl f)),
and from Theorem 8.3.4 and Theorem 8.2.6 (v) that
f ∗∗(x) = (cl f)(x)
for all x ∈rint(dom f ∗∗). So the two functions f ∗∗and cl f are equal, ac-
cording to Theorem 8.2.4, because both of them are closed and convex .
Corollary 8.3.8. If f is a closed convex function, then f ∗∗= f.
8.4
The direction derivative
Deﬁnition. Suppose the function f : X →R is deﬁned in a neighborhood of
x, and let v be an arbitrary vector in Rn. The limit
f ′(x; v) = lim
t→0+
f(x + tv) −f(x)
t
,
provided it exists, is called the direction derivative of f at the point x in the
direction v.
If f is diﬀerentiable at x, then obviously f ′(x; v) = Df(x)[v].
Example 8.4.1. If f is a one-variable function, then
f ′(x; v) =





f ′
+(x)v
if v > 0,
0
if v = 0,
f ′
−(v)v
if v < 0.
So, the direction derivative is a generalization of left- and right derivatives.
Theorem 8.4.1. Let f : X →R be a convex function with an open domain.
The direction derivative f ′(x; v) exists for all x ∈X and all directions v, and
f(x + v) ≥f(x) + f ′(x; v)
if x + v lies in X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
181
The subdifferential
181
Proof. Let φ(t) = f(x + tv); then f ′(x; v) = φ′
+(0), which exists since con-
vex one-variable functions do have right derivatives at each point by Theo-
rem 7.1.2. Moreover,
φ(t) ≥φ(0) + φ′
+(0) t
for all t in the domain of φ, and we obtain the inequality of the theorem by
choosing t = 1.
Theorem 8.4.2. The direction derivative f ′(x; v) of a convex function is a
positively homogeneous and convex function of the second variable v, i.e.
f ′(x; αv) = αf ′(x; v)
if α ≥0
f ′(x; αv + (1 −α)w) ≤αf ′(x; v) + (1 −α)f ′(x; w)
if 0 ≤α ≤1.
Proof. The homogenouity follows directly from the deﬁnition (and holds for
arbitrary functions). Moreover, for convex functions f
f(x + t(αv + (1 −α)w)) −f(x) = f(α(x + tv) + (1 −α)(x + tw)) −f(x)
≤α

f(x + tv) −f(x)

+ (1 −α)

f(x + tw) −f(x)

.
The required convexity inequality is now obtained after division by t > 0 by
passing to the limit as t →0+.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
182
The subdifferential
Theorem 7.1.2 gives a relation between the subgradient and the direction
derivative for convex one-variable functions f −the number c is a subgradient
at x if and only if f ′
−(x) ≤c ≤f ′
+(x). The subdiﬀerential ∂f(x) is in other
words equal to the interval [f ′
−(x), f ′
+(x)].
We may express this relation using the support function of the subdiﬀer-
ential. Let us recall that the support function SX of a set X in Rn is deﬁned
as
SX(x) = sup{⟨y, x⟩| y ∈X}.
For one-variable functions f this means that
S∂f(x)(v) = S[f′
−(x),f′
+(x)](v) = max{f ′
+(x)v, f ′
−(x)v} =





f ′
+(x)v
if v > 0,
0
if v = 0,
f ′
−(x)v
if v < 0
= f ′(x; v).
We will generalize this identity, and to achieve this we need to consider the
subgradients of the function v →f ′(x; v). We denote the subdiﬀerential of
this function at the point v0 by ∂2f ′(x; v0).
If the function f : X →R is convex, then so is the function v →f ′(x; v),
according to our previous theorem, and the subdiﬀerentials ∂2f ′(x; v) are
thus nonempty sets for all x ∈X and all v ∈Rn.
Lemma 8.4.3. Let f : X →R be a convex function with an open domain X
and let x be a point in X. Then:
c ∈∂2f ′(x; 0) ⇔f ′(x; v) ≥⟨c, v⟩
for all v ∈Rn
(i)
∂2f ′(x; v) ⊆∂2f ′(x; 0)
for all v ∈Rn
(ii)
c ∈∂2f ′(x; v) ⇒f ′(x; v) = ⟨c, v⟩
(iii)
∂f(x) = ∂2f ′(x; 0).
(iv)
Proof. The equivalence (i) follows directly from the deﬁnition of the subgra-
dient and the fact that f ′(x; 0) = 0.
(ii) and (iii): Suppose c ∈∂2f ′(x; v) and let w ∈Rn be an arbitrary vector.
Then, by homogenouity and the deﬁnition of the subgradient, we have the
following inequality for t ≥0:
tf ′(x; w) = f ′(x; tw) ≥f ′(x; v) + ⟨c, tw −v⟩= f ′(x; v) + t⟨c, w⟩−⟨c, v⟩,
and this is possible for all t > 0 only if f ′(x; w) ≥⟨c, w⟩. So we conclude from
(i) that c ∈∂2f ′(x; 0), and this proves the inclusion (ii). By choosing t = 0
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
183
The subdifferential
we obtain the inequality f ′(x; v) ≤⟨c, v⟩, which implies that f ′(x; v) = ⟨c, v⟩,
and completes the proof of the implication (iii).
(iv)
Suppose c ∈∂2f ′(x; 0). By (i) and Theorem 8.4.1,
f(y) ≥f(x) + f ′(x; y −x) ≥f(x) + ⟨c, y −x⟩
for all y ∈X, which proves that c is a subgradient of f at the point x and
gives us the inclusion ∂2f ′(x; 0) ⊆∂f(x).
Conversely, suppose c ∈∂f(x). Then f(x + tv) −f(x) ≥⟨c, tv⟩= t⟨c, v⟩
for all suﬃciently small numbers t. Division by t > 0 and passing to the limit
as t →0+ results in the inequality f ′(x; v) ≥⟨c, v⟩, and it now follows from
(i) that c ∈∂2f ′(x; 0). This proves the inclusion ∂f(x) ⊆∂2f ′(x; 0), and the
proof is now complete.
Theorem 8.4.4. Suppose that f : X →R is a convex function with an open
domain. Then
f ′(x; v) = S∂f(x)(v) = max{⟨c, v⟩| c ∈∂f(x)}
for all x ∈X and all v ∈Rn.
Proof. It follows from (i) and (iv) in the preceding lemma that
⟨c, v⟩≤f ′(x; v)
for all c ∈∂f(x), and from (ii), (iii) and (iv) in the same lemma that ⟨c, v⟩
is equal to f ′(x; v) for all subgradients c in the nonempty subset ∂2f ′(x; v)
of ∂f(x).
8.5
Subdiﬀerentiation rules
Theorem 8.5.1. Let X be an open convex set, and suppose that fi : X →R
are convex functions and αi are nonnegative numbers for i = 1, 2 . . . , m.
Deﬁne
f =
m

i=1
αifi.
Then
∂f(x) =
m

i=1
αi∂fi(x).
Proof. A sum of compact, convex sets is compact and convex. Therefore,
m
i=1 αi∂fi(x) is a closed and convex set, just as the set ∂f(x). Hence, by
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
184
The subdifferential
184
Theorem 6.9.2 it suﬃces to prove that the two sets have the same support
function. And this follows from Theorems 8.4.4 and 6.9.1, according to which
S∂f(x)(v) = f ′(x; v) =
m

i=1
αif ′
i(x; v) =
m

i=1
αiS∂fi(x) = Sm
i=1 αi∂fi(x)(v).
Theorem 8.5.2. Suppose that the functions fi : X →R are convex for i = 1,
2,..., m, and that their domain X is open, and let
f = max
1≤i≤m fi.
Then
∂f(x) = cvx
 
i∈I(x)
∂fi(x)

,
for all x ∈X, where I(x) = {i | fi(x) = f(x)}.
Proof. The functions fi are continuous at x and fj(x) < f(x) for all j /∈I(x).
Hence, for all suﬃciently small numbers t,
f(x + tv) −f(x) = max
i∈I(x) fi(x + tv) −f(x) = max
i∈I(x)(fi(x + tv) −fi(x)),
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
185
The subdifferential
and it follows after division by t and passing to the limit that
f ′(x; v) = max
i∈I(x) f ′
i(x; v).
We use Theorem 6.9.1 to conclude that
S∂f(x)(v) = f ′(x; v) = max
i∈I(x) f ′
i(x; v) = max
i∈I(x) S∂fi(x)(v) = S
i∈I(x) ∂fi(x)(v)
= Scvx(
i∈I(x) ∂fi(x))(v),
and the equality for ∂f(x) is now a consequence of Theorem 6.9.2.
Our next theorem shows how to compute the subdiﬀerential of a compo-
sition with aﬃne functions.
Theorem 8.5.3. Suppose C is a linear map from Rn to Rm, that b is a vector
in Rm, and that g is a convex function with an open domain in Rm, and let f
be the function deﬁned by f(x) = g(Cx + b). Then, for each x in the domain
of f,
∂f(x) = CT(∂g(Cx + b)).
Proof. The sets ∂f(x) and CT(∂g(Cx + b)) are convex and compact, so it
suﬃces to show that their support functions are identical.
But for each
v ∈Rn
f ′(x; v) = lim
t→0+
g(C(x + tv) + b) −g(Cx + b)
t
= lim
t→0+
g(Cx + b + t Cv) −g(Cx + b)
t
= g′(Cx + b; Cv),
so it follows because of Theorem 6.9.1 that
S∂f(x)(v) = f ′(x; v) = g′(Cx + b; Cv) = S∂g(Cx+b)(Cv) = SCT(∂g(Cx+b))(v).
The Karush–Kuhn–Tucker theorem
As an application of the subdiﬀerentiation rules we now prove a variant of a
theorem by Karush–Kuhn–Tucker on minimization of convex functions with
convex constraints. A more thorough treatment of this theme will be given
in Part II.
Theorem 8.5.4. Suppose that the functions f, g1, g2, . . . , gm are convex and
deﬁned on an open convex set Ω, and let
X = {x ∈Ω| gi(x) ≤0 for i = 1, 2, . . . , m.}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
186
The subdifferential
Moreover, suppose that there exists a point x ∈Ωsuch that gi(x) < 0 for
i = 1, 2, . . . , m. (Slater’s condition)
Then, ˆx ∈X is a minimum point of the restriction f|X if and only if
for each i = 1, 2, . . . , m there exist a subgradient ci ∈∂gi(ˆx) and a scalar
ˆλi ∈R+ with the following properties:
−
m

i=1
ˆλici ∈∂f(ˆx)
and
(i)
ˆλigi(ˆx) = 0
for i = 1, 2, . . . , m.
(ii)
Remark. If the functions are diﬀerentiable, then condition (i) simpliﬁes to
∇f(ˆx) +
m

i=1
ˆλi∇gi(ˆx) = 0.
Cf. Theorem 11.2.1 in Part II.
Proof. Let ˆx be a point in X and consider the convex function
h(x) = max {f(x) −f(ˆx), g1(x), . . . , gm(x)}
with Ωas its domain. Clearly, h(ˆx) = 0. By deﬁning
I(ˆx) = {i | gi(ˆx) = 0},
we obtain I(ˆx) = {i | gi(ˆx) = h(ˆx)}, and it follows from Theorem 8.5.2 that
∂h(ˆx) = cvx

∂f(ˆx) ∪

{∂gi(ˆx) | i ∈I(ˆx)}

.
Now assume that ˆx is a minimum point of the restriction f|X.
Then
h(x) = f(x) −f(ˆx) ≥0 for all x ∈X with equality when x = ˆx. And if
x /∈X, then h(x) > 0 since gi(x) > 0 for at least one i. Thus, ˆx is a global
minimum point of h.
Conversely, if ˆx is a global minimum point of h, then h(x) ≥0 for all
x ∈Ω. In particular, for x ∈X this means that h(x) = f(x)−f(ˆx) ≥0, and
hence ˆx is a mimimum point of the restriction f|X, too.
Using Theorem 8.1.2 we therefore obtain the following equivalences:
ˆx is a minimum point of f|X
⇔
ˆx is a minimum point of h
⇔
0 ∈∂h(ˆx)
⇔
0 ∈cvx

∂f(ˆx) ∪

{∂gi(ˆx) | i ∈I(ˆx)}

⇔
0 = λ0c0 +

i∈I(ˆx)
λici
⇔
λ0c0 = −

i∈I(ˆx)
λici,
(8.5)
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
187
The subdifferential
187
where c0 ∈∂f(ˆx), ci ∈∂gi(ˆx) for i ∈I(ˆx), and the scalars λi are nonnegative
numbers with sum equal to 1.
We now claim that λ0 > 0. To prove this, assume the contrary. Then

i∈I(ˆx) λici = 0, and it follows that

i∈I(ˆx)
λigi(x) ≥

i∈I(ˆx)
λi

gi(ˆx) + ⟨ci, x −ˆx⟩

= ⟨

i∈I(ˆx)
λici, x −ˆx⟩= 0,
which is a contradiction, since gi(x) < 0 for all i and λi > 0 for some i ∈I(ˆx).
We may therefore divide the equality in (8.5) by λ0, and conditions (i)
and (ii) in our theorem are now fulﬁlled if we deﬁne ˆλi = λi/λ0 for i ∈I(ˆx),
and ˆλi = 0 for i /∈I(ˆx), and choose arbitrary subgradients ci ∈∂gi(ˆx) for
i /∈I(ˆx).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
188
The subdifferential
188
Exercises
8.1 Suppose f : Rn →R is a strongly convex function. Prove that
lim
∥x∥→∞f(x) = ∞
8.2 Find ∂f(−1, 1) for the function f(x1, x2) = max(|x1|, |x2|).
8.3 Determine the subdiﬀerential ∂f(0) at the origin for the following functions
f : Rn →R:
a) f(x) = ∥x∥2
b) f(x) = ∥x∥∞
c) f(x) = ∥x∥1.
8.4 Determine the conjugate functions of the following functions:
a) f(x) = ax + b, dom f = R
b) f(x) = −ln x, dom f = R++
c) f(x) = ex, dom f = R
d) f(x) = x ln x, dom f = R++
e) f(x) = 1/x, dom f = R++.
8.5 Use the relation between the support function SA and the indicator function
χA and the fact that SA = Scl(cvx A) to prove Corollary 6.9.3, i.e. that
cl(cvx A) = cl(cvx B) ⇔SA = SB.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
189
Bibliogracal and historical notices
Bibliograﬁcal and historical
notices
Basic references in convex analysis are the books by Rockafellar [1] from 1970
and Hiriart-Urutty–Lemarechal [1] from 1993. Almost all results in Part I
and in Chapters 9–10 in Part II of this series can be found in one form
or another in Rockafellar’s book, which also contains a historical overview
with references to the original works in the ﬁeld. A more accessible book
on the same subject is Webster [1]. Among textbooks in convexity with an
emphasis on polyhedra, one should mention Stoer–Witzgall [1] and the more
combinatorially oriented Gr¨unbaum [1].
The general convexity theory was founded around the turn of the century
1900 by Hermann Minkowski [1, 2] as a byproduct of his number theoretic
studies. Among other things, Minkowski introduced the concepts of sepa-
ration and extreme point, and he showed that every compact convex set is
equal to the convex hull of its extreme points and that every polyhedron is
ﬁnitely generated, i.e. one direction of Theorem 5.3.1) −the converse was
noted later by Weyl [1].
The concept of dual cone was introduced by Steinitz [1], who also showed
basic results about the recession cone.
The theory of linear inequalities is surprisingly young −a special case
of Theorem 3.3.7 (Exercise 3.11a) was proved by Gordan [1], the algebraic
version of Farkas’s lemma, i.e. Corollary 3.3.3, can be found in Farkas [1],
and a closely related result (Exercise 3.11b) is given by Stiemke [1]. The
ﬁrst systematic treatment of the theory is given by Weyl [1] and Motzkin [1].
Signiﬁcant contributions have also been provided by Tucker [1]. The proof
in Chapter 3 of Farkas’s lemma has a geometrical character; an alternative
algebraic induction proof of the lemma has been given by Kuhn [1].
Extreme points and faces are treated in detail in Klee [1,2].
Jensen [1] studied convex functions of one real variable and showed that
convex functions with R as domain are continuous and have one-sided deriva-
tives everywhere. Jensen’s inequality, however, was shown earlier for func-
tions with positive second derivative by H¨older [1].
The conjugate function was introduced by Fenchel [1], and a modern
treatment of the theory of convex cones, sets and functions can be found
in Fenchel [2], which among other things contains original results about the
closure of convex functions and about the subdiﬀerential.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
190
References
References
Farkas, J.
[1] Theorie der einfachen Ungleichungen, J. reine angew. Math. 124 (1902),
1–27.
Fenchel, W.
[1] On conjugate convex functions. Canad. J. Math. 1 (1949), 73-77.
[2] Convex Cones, Sets and Functions. Lecture Notes, Princeton University,
1951.
Gordan, P.
[1] ¨Uber die Auﬂ¨osung linearer Gleichungen mit reellen Coeﬃcienten, Math.
Ann. 6 (1873), 23–28.
Gr¨unbaum, B.
[1] Convex Polytopes. Interscience publishers, New York, 1967.
Hiriart-Urruty, J.-B. & Lemar´echal, C.
[1] Convex Analysis and Minimization Algorithms. Springer, 1993.
H¨older, O.
[1] ¨Uber einen Mittelwertsatz, Nachr. Ges. Wiss. G¨ottingen, 38–47, 1989.
Jensen, J.L.W.V.
[1] Sur les fonctions convexes et les in´egalit´es entre les valeur moyennes,
Acta Math. 30 (1906), 175–193.
Klee, V.
[1] Extremal structure of convex sets, Arch. Math. 8 (1957), 234–240.
[2] Extremal structure of convex sets, II, Math. Z. 69 (1958), 90–104.
Klee, V. & Minty, G.J.
[1] How Good is the Simplex Algorithm? Pages 159–175 in Shisha, O. (ed.),
Inequalities, III, Academic Press, 1972.
Koopmans, T.C., ed.
[1] Activity Analysis of Production and Allocation.
John Wiley & Sons,
1951.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
191
References
Kuhn, H.W.
[1] Solvability and Consistency for Linear Equations and Inequalities, Amer.
Math. Monthly 63 (1956), 217–232.
Minkowski, H.
[1] Geometrie der Zahlen. Teubner, Leipzig, 1910.
[2] Gesammelte Abhandlungen von Hermann Minkowski, Vol. 1, 2. Teub-
ner, Leibzig, 1911
Motzkin, T.
[1] Beitr¨age zur Theorie der linearen Ungleichungen.
Azviel, Jerusalem,
1936.
Rockafellar, R.T.
[1] Convex Analysis. Princeton Univ. Press., 1970
Steinitz, E.
[1] Bedingt konvergente Reihen und konvexe Systeme, I, II, III, J. Reine
Angew. Math. 143 (1913), 128–175; 144 (1914), 1–40; 146 (1916), 1–52.
Stiemke, E.
[1] ¨Uber positive L¨osungen homogener linearer Gleichungen, Math. Ann.
76 (1915), 340–342.
Stoer, J. & Witzgall, C.
[1] Convexity and Optimization in Finite Dimensions I. Springer-Verlag,
1970.
Tucker, A.W.
[1] Dual Systems of Homogeneous Linear Relations. Pages 3–18 in Kuhn,
H.W. & Tucker, A.W. (eds.), Linear Inequalities and Related Systems,
Princeton Univ. Press, 1956.
Webster, R.
[1] Convexity. Oxford University Press, 1954.
Weyl, H.
[1] Elementare Theorie der konvexen Polyeder, Comment. Math. Helv. 7
(1935), 290–306.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
192
Answers and solutions to the exercises
Answers and solutions to the
exercises
Chapter 2
2.2 a) {x ∈R2 | 0 ≤x1 + x2 ≤1, x1, x2 ≥0}
b) {x ∈R2 | ∥x∥≤1}
c) R2
++ ∪{(0, 0)}
2.3 E.g. {(0, 1)} ∪(R × {0}) in R2.
2.4 {x ∈R3
++ | x2
3 ≤x1x2}
2.5 Use the triangle inequality
n
1(xj + yj)21/2 ≤
n
1 x2
j
1/2 +
n
1 y2
j
1/2
to show that the set is closed under addition of vectors. Or use the
perspective map; see example 2.3.4.
2.6 Follows from the fact that −ek is a conic combination of the vectors
e0, e1, . . . , en.
2.7 Let X be the halfspace {x ∈Rn | ⟨c, x⟩≥0}. Each vector x ∈X is a
conic combination of c and the vector y = x −⟨c, x⟩∥c∥−2c, and y lies
in the (n −1)-dimensional subspace Y = {x ∈Rn | ⟨c, x⟩= 0}, which
is generated by n vectors as a cone according to the previous exercise.
Hence, x is a conic combination of these n vectors and c.
2.8 The intersection between the cone X and the unit circle is a closed
circular arc with endpoints x and y, say. The length of the arc is either
less than π, equal to π, or equal to 2π. The cone X is proper and
generated by the two vectors x and y in the ﬁrst case. It is equal to a
halfspace in the second case and equal to R2 in the third case, and it
is generated by three vectors in both these cases.
2.9 Use exercise 2.8.
2.10 a) recc X = {x ∈R2 | x1 ≥x2 ≥0},
lin X = {(0, 0)}
b) recc X = lin X = {(0, 0)}
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
193
Answers and solutions to the exercises
2.10 c) recc X = {x ∈R3 | 2x1 + x2 + x3 ≤0, x1 + 2x2 + x3 ≤0},
lin X = {(t, t, −3t) | t ∈R}
d) recc X ={x ∈R3 | x1 ≥|x2|},
lin X ={x ∈R3 | x1 = x2 = 0}.
2.12 b) (i)
c(X) = {x ∈R2 | 0 ≤1
3x1 ≤x2 ≤1
2x1} = cl(c(X))
(ii) c(X) = {x ∈R2 | 0 < x2 ≤1
2x1} ∪{(0, 0)},
cl(c(X) = {x ∈R2 | 0 ≤x2 ≤1
2x1},
(iii) c(X) = {x ∈R3 | x1x3 ≥x2
2, x3 > 0} ∪{(0, 0, 0)},
cl(c(X)) = c(X) ∪{(x1, 0, 0) | x1 ≥0}.
c) c(X) = {(x, xn+1) ∈Rn × R | ∥x∥≤xn+1}.
2.14 Let zn = xn + yn, n = 1, 2, . . . be a convergent sequence of points in
X + Y with xn ∈X and yn ∈Y for all n and limit z0. The sequence
(yn)∞
1 ha a convergent subsequence (ynk)∞
k=1 with limit y0 ∈Y , since Y
is compact. The corresponding subsequence (znk −ynk)∞
k=1 of points in
X converges to z0 −y0, and the limit point belongs to X since X is a
closed set. Hence, z0 = (z0 −y0) + y0 lies in X + Y , and this means
that X + Y is a closed set.
Chapter 3
3.1 E.g. {x ∈R2 | x2 ≤0} and {x ∈R2 | x2 ≥ex1}.
3.2 Follows from Theorem 3.1.3 for closed sets and from Theorem 3.1.5 for
open sets.
3.4 a) R+ × R
b) {0} × R
c) {0} × R+
d) R+ × R+
e) {x ∈R2 | x2 ≥x1 ≥0}
3.6 a) X = X++ = {x ∈R2 | x1 + x2 ≥0, x2 ≥0},
X+ = {x ∈R2 | x2 ≥x1 ≥0}
b) X = X++ = R2,
X+ = {(0, 0)}
c) X = R2
++ ∪{(0, 0)},
X+ = X++ = R2
+
3.7 (i) ⇒(iii): Since −aj /∈con A, there is, for each j, a vector cj such
that −⟨cj, aj⟩< 0 and ⟨cj, x⟩≥0 for all x ∈con A, which implies that
⟨cj, aj⟩> 0 and ⟨cj, ak⟩≥0 for all k. It follows that c = c1+c2+· · ·+cm
works.
(iii) ⇒(ii): Suppose that ⟨c, aj⟩> 0 for all j. Then m
1 λjaj = 0
implies 0 = ⟨c, 0⟩= m
1 λj ⟨c, aj⟩, so if λj ≥0 for all j then λj ⟨c, aj⟩=
0 for all j, with the conclusion that λj = 0 for all j.
(ii) ⇒(i): If there is a vector x such that x = m
1 λjaj and −x =
m
1 µjaj with nonnegative scalars λj, µj, then by addition we obtan
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
194
Answers and solutions to the exercises
194
the equality m
1 (λj + µj)aj = 0 with the conclusions λj + µj = 0,
λj = µj = 0 and x = 0.
3.8 No solution.
3.10 Solvable for α ≤−2, −1 < α < 1 and α > 1.
3.11 a) The systems (S) and (S∗) are equivalent to the systems







Ax ≥0
−Ax ≥0
Ex ≥0
1Tx > 0
and
AT(y′ −y′′) + Ez + 1t = 0
y′, y′′, z ≥0, t > 0,
respectively (with y corresponding to y′′ −y′). The assertion therefore
follows from Theorem 3.3.7.
b) The systems (S) and (S∗) are equivalent to the systems



Ax ≥0
−Ax ≥0
Ex > 0
and
AT(y′ −y′′) + Ez = 0
y′, y′′, z ≥0, z ̸= 0,
respectively. Now apply Theorem 3.3.7.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
195
Answers and solutions to the exercises
3.12 By Theorem 3.3.7, the system is solvable if and only if the dual system



AT(y′ −y′′) + z + u = 0
A(w + u) = 0
y′, y′′, z, w, u ≥0, u ̸= 0
has no solution. It follows from the two equations of the dual system
that:
0 = −(w + u)TAT = −(w + u)TAT(y′ −y′′) = (w + u)T(z + u)
= wTz + wTu + uTz + uTu,
and all the four terms in the last sum are nonnegative. We conclude
that uTu = 0, and hence u = 0. So the dual system has no solution.
Chapter 4
4.1 a) ext X = {(1, 0), (0, 1)}
b) ext X = {(0, 0), (1, 0), (0, 1), ( 1
2, 1)}
c) ext X = {(0, 0, 1), (0, 0, −1)} ∪{(x1, x2, 0) | (x1 −1)2 + x2
2 = 1} \
{(0, 0, 0)}
4.2 Suppose x ∈cvx A \ A; then x = λa + (1 −λ)y where a ∈A, y ∈cvx A
and 0 < λ < 1. It follows that x /∈ext(cvx A).
4.3 We have ext X ⊆A, according to the previous exercise. Suppose that
a ∈A \ ext X. Then a = λx1 + (1 −λ)x2, where x1, x2 ∈X, x1 ̸= x2
and 0 < λ < 1. We have xi = µia + (1 −µi)yi, where 0 ≤µi < 1 and
yi ∈cvx(A \ {a}). It now follows from the equality
a = (1 −λµ1 −(1 −λ)µ2)−1(λ(1 −µ1)y1 + (1 −λ)(1 −µ2)y2),
that a lies in cvx(A \ {a}).Therefore, cvx(A \ {a}) = cvx A = X, which
contradicts the minimality of A. Hence, ext X = A.
4.4 The set X \ {x0} is convex if and only if ]a, b[ ⊆X \ {x0} for all a, b ∈
X \ {x0}, i.e. if and only if x0 /∈]a, b[ for all a, b ∈X \ {x0}, i.e. if and
only if x0 ∈ext X.
4.5 E.g. the set in exercise 4.1 c).
4.6 a) Follows directly from Theorem 4.1.3.
b) The extreme point (1, 0) of

x ∈R2 | x2 ≤

1 −x2
1 , |x1| ≤1

is
not exposed.
4.7 b) A zero-dimensional general face is an extreme point, and a zero-
dimensional exposed face is an exposed point. Hence, exercise 4.6 b)
contains an example of a general face which is not an exposed face.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
196
Answers and solutions to the exercises
c) Suppose that a, b ∈X and that the open line segment ]a, b[ intersects
F ′. Since F ′ ⊆F, the same line segment also intersects F, so it follows
that a, b ∈F.
But since F ′ is a general face of F, it follows that
a, b ∈F ′. So F ′ is indeed a general face of X.
The set X in exercise 4.6 b) has F = {1}×]−∞, 0] as an exposed face,
and F ′ = {(1, 0)} is an exposed face of F but not of X.
d) Fix a point x0 ∈F ∩rint C. To each x ∈C there is a point y ∈C
such that x0 lies on the open line segment ]x, y[, and it now follows
from the deﬁnition of a general face that x ∈F.
e) Use the result in d) on the set C = X ∩cl F. Since rint C contains
rint F as a subset, F ∩rint C ̸= ∅, so it follows that C ⊆F. The
converse inclusion is of course trivial.
f) Use the result in d) with F = F1 och C = F2, which gives us the
inclusion F2 ⊆F1. The converse inclusion is obtained analogously.
g) If F is a general face and F ∩rint X ̸= ∅, then X ⊆F by d) above.
For faces F ̸= X we therefore have F ∩rint X = ∅, which means that
F ⊆rbdry X.
Chapter 5
5.1 a) (−2
3, 4
3), and (4, −1)
b) (−2
3, 4
3), (4, −1), and(−3, −1)
c) (0, 0, 0), (2, 0, 0), (0, 2, 0), (0, 0, 4), and (4
3, 4
3, 0)
d) (0, 4, 0, 0), (0, 5
2, 0, 0), (3
2, 5
2, 0, 0), (0, 1, 1, 0), and (0, 5
2, 0, 3
2)
5.2 The extreme rays are generated by (−2, 4, 3), (1, 1, 0), (4, −1, 1), and
(1, 0, 0).
5.3 C =


1 −2
1
−1
2
3
−3
2
5


5.4 a) A = {(1, 0), (0, 1)}, B = {(−2
3, 4
3), (4, −1)}
b) A = ∅, B = {(−2
3, 4
3), (4, −1), (−3, −1)}
c) A = {(1, 1, −3), (−1, −1, 3), (4, −7, −1), (−7, 4, −1)},
B = {(0, 0, 0), (2, 0, 0), (0, 2, 0), (0, 0, 4), (4
3, 4
3, 0)}
d) A = ∅,
B = {(0, 4, 0, 0), (0, 5
2, 0, 0), (3
2, 5
2, 0, 0), (0, 1, 1, 0), (0, 5
2, 0, 3
2)}.
5.5 The inclusion X = cvx A+con B ⊆con A+con B = con(A∪B) implies
that con X ⊆con(A ∪B). Obviously, A ⊆cvx A ⊆X. Since cvx A
is a compact set, recc X = con B, so using the assumption 0 ∈X, we
obtain the inclusion B ⊆con B ⊆X. Thus, A ∪B ⊆X, and it follows
that con(A ∪B) ⊆con X.
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
197
Answers and solutions to the exercises
197
Chapter 6
6.1 E.g. f1(x) = x −|x| and f2(x) = −x −|x|.
6.3 a ≥5 and a > 5, respectively.
6.4 Use the result of exercise 2.1.
6.5 Follows from f(x) = max(xi1 + xi2 + · · · + xik), where the maximum
is taken over all subsets {i1, i2, . . . , ik} of {1, 2, . . . , n} consisting of k
elements.
6.6 The inequality is trivial if x1 + x2 + · · · + xn = 0, and it is obtained by
adding the n inequalities
f(xi) ≤
xi
x1 + · · · + xn
f(x1 + · · · + xn) +

1 −
xi
x1 + · · · + xn

f(0)
if x1 + · · · + xn > 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
198
Answers and solutions to the exercises
6.7 Choose
c = f(x2) −f(x1)
∥x2 −x1∥2
(x1 −x2),
to obtain f(x1) + ⟨c, x1⟩= f(x2) + ⟨c, x2⟩. By quasiconvexity,
f(λx1 + (1 −λ)x2) + ⟨c, λx1 + (1 −λ)x2⟩≤f(x1) + ⟨c, x1⟩,
which simpliﬁes to
f(λx1 + (1 −λ)x2) ≤λf(x1) + (1 −λ)f(x2).
6.8 Let f : Rn × R →R be the function deﬁned by
f(x, t) =

t
if (x, t) ∈C
+∞
if (x, t) /∈C.
Then inf{t ∈R | (x, t) ∈C} = inf{f(x, t) | t ∈R}, and Theorem 6.2.6
now follows from Corollary 6.2.7.
6.9 Choose, given x, y ∈X, sequences (xk)∞
1 , (yk)∞
1 of points xk, yk ∈int X
such that xk →x and yk →y as k →∞. Since the points λxk+(1−λ)yk
belong to int X,
f(λxk + (1 −λ)yk) ≤λf(xk) + (1 −λ)f(yk),
and since f is continuous on X, we now obtain the desired inequality
f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) by passing to the limit.
6.10 Let m = inf{f(x) | x ∈rint(dom f)} and ﬁx a relative interior point x0
of dom f. If x ∈dom f is arbitrary and 0 < λ < 1, then λx + (1 −λ)x0
is a relative interior point of dom f, and it follows that
m ≤f(λx + (1 −λ)x0) ≤λf(x) + (1 −λ)f(x0).
The inequality f(x) ≥m now follows by letting λ →1.
6.11 Minimum 8 at x = ( 1
8, 2).
6.12 a) ∥x∥p
b) max(x1, 0).
Chapter 7
7.2 Yes.
7.5 Let J be a subinterval of I. If f ′
+(x) ≥0 for all x ∈J, then
f(y) −f(x) ≥f ′
+(x)(y −x) ≥0
for all y > x in the interval J, i.e. f is increasing on J. If instead
f ′
+(x) ≤0 for all x ∈J, then f(y) −f(x) ≥f ′
+(x)(y −x) ≥0 for all
y < x, i.e. f is decreasing on J.
Since the right derivative f ′
+(x) is increasing on I, there are three dif-
ferent cases to consider. Either f ′
+(x) ≥0 for all x ∈I, and f is then
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
199
Answers and solutions to the exercises
increasing on I, or f ′
+(x) ≤0 for all x ∈I, and f is then decreasing
on I, or there is a point c ∈I such that f ′
+(x) ≤0 to the left of c and
f ′
+(x) > 0 to the right of c, and f is in this case decreasing to the left
of c and increasing to the right of c.
7.6 a) The existence of the limits is a consequence of the results of the
previous exercise.
b) Consider the epigraph of the extended function.
7.7 Follows directly from exercise 7.6 b).
7.8 Suppose that f ∈F. Let x0 ∈Rn be an arbitrary point, and consider
the function g(x) = f(x) −⟨f ′(x0), x −x0⟩. The function g belongs to
F and g′(x0) = 0. It follows that g(x) ≥g(x0) for all x, which means
that f(x) ≥f(x0) + ⟨f ′(x0), x −x0⟩for all x. Hence, f is convex by
Theorem 7.2.1.
7.9 φ(t) = f(x + tv) = f(x) + t⟨f ′(x), v⟩for v ∈Vf by Theorem 6.7.1.
Diﬀerentiate two times to obtain D2f(x)[v, v] = φ′′(0) = 0, with the
conlusion that f ′′(x)v = 0.
7.13 By combining Theorem 7.3.1 (i) with x replaced by ˆx and v = x−ˆx with
the Cauchy-Schwarz inequality, we obtain the inequality µ∥x −ˆx∥2 ≤
⟨f ′(x), x −ˆx⟩≤∥f ′(x)∥∥x −ˆx∥.
Chapter 8
8.1 Suppose that f is µ-strongly convex, where µ > 0, and let c be a
subgradient at 0 of the convex function g(x) = f(x) −1
2µ∥x∥2. Then
f(x) ≥f(0) + ⟨c, x⟩+ 1
2µ∥x∥2 for all x, and the right-hand side tends
to ∞as ∥x∥→∞. Alternatively, one could use Theorem 8.1.6.
8.2 The line segment [−e1, e2], where e1 = (1, 0) and e2 = (0, 1).
8.3 a) B2(0; 1) = {x | ∥x∥2 ≤1}
b) B1(0; 1) = {x | ∥x∥1 ≤1}
c) B∞(0; 1) = {x | ∥x∥∞≤1}.
8.4 a) dom f ∗= {a}, f ∗(a) = b
b) dom f ∗= {x | x < 0}, f ∗(x) = −1 −ln(−x)
c) dom f ∗= R+, f ∗(x) = x ln x −x, f ∗(0) = 0
d) dom f ∗= R, f ∗(x) = ex−1
e) dom f ∗= R−, f ∗(x) = −2√−x.
Download free eBooks at bookboon.com

Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
201
Index
Index
aﬃne
combination, 21
dimension, 24
hull, 22
map, 25
piecewise —, 115
set, 21
ball
closed —, 10
open —, 10
bidual cone, 67
boundary, 11
point, 11
relative —, 37
bounded set, 13
closed
ball, 10
convex function, 135
halfspace, 29
hull, 12
set, 12
closure, 12
of function, 172
codomain, 3
compact set, 13
concave function, 105
strictly —, 110
condition number, 157
cone, 40
bidual —, 67
dual —, 65
ﬁnitely generated —, 45
cone
polyhedral —, 43
proper —, 42
recession —, 47
conic
combination, 42
halfspace, 41
hull, 43
polyhedron, 43
conjugate function, 174
continuous function, 13
convex
combination, 27
function, 105
hull, 34
set, 27
strictly — function, 110
derivative, 17, 180
diﬀerence of sets, 5
diﬀerentiable, 16
diﬀerential, 17
dimension, 24
direction derivative, 180
distance, 10
domain, 2
dual cone, 65
eﬀective domain, 3
epigraph, 104
Euclidean norm, 10
exposed point, 89
exterior point, 11
extreme point, 77
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
202
Index
extreme ray, 79
face, 79, 89
Farkas’s lemma, 70
Fenchel transform, 174
Fenchel’s inequality, 175
ﬁnitely generated cone, 45
form
linear —, 8
quadratic —, 8
generator, 43
gradient, 17
halﬂine, 40
halfspace, 29
conic —, 41
hessian, 19
hull
aﬃne —, 22
conic —, 43
convex —, 34
hyperplane, 25
separating —, 57
supporting —, 61
H¨older’s inequality, 124
image, 3
inverse —, 3
indicator function, 174
interior, 11
point, 11
relative —, 37
intersection, 2
inverse image, 3
Jensen’s inequality, 111
Karush–Kuhn–Tucker theorem, 185
ℓ1-norm, 10
ℓp-norm, 110
lie between, 78
line segment, 7
open —, 7
line-free, 51
linear
form, 8
map, 7
operator, 7
Lipschitz
constant, 14
continuous, 14
maximum norm, 10
mean value theorem, 17
Minkowski functional, 140
Minkowski’s inequality, 126
norm, 10, 109
Euclidean —, 10
ℓ1 —, 10
ℓp —, 110
maximum —, 10
operator —, 14
open
ball, 10
halfspace, 29
line segment, 7
set, 11
operator norm, 14
orthant, 6
perspective, 119
map, 32
piecewise aﬃne, 115
polyhedral cone, 43
polyhedron, 30
conic —, 43
positive
deﬁnite, 9
homogeneous, 109
semideﬁnite, 9
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
203
Index
proper
cone, 42
face, 79
quadratic form, 8
quasiconcave, 107
strictly —, 110
quasiconvex, 107
strictly —, 110
ray, 40
recede, 47
recession
cone, 47
vector, 47
recessive subspace, 51
of a function, 132
relative
boundary, 37
boundary point, 37
interior, 37
interior point, 37
second derivative, 19
seminorm, 109
separating hyperplane, 57
strictly —, 57
Slater’s condition, 186
standard scalar product, 4
strictly
concave, 110
convex, 110
quasiconcave, 110
quasiconvex, 110
strongly convex, 154
subadditive, 109
subdiﬀerential, 163
subgradient, 163
sublevel set, 104
sum of sets, 5
support function, 137
supporting
hyperplane, 61
line, 148
symmetric linear map, 8
Taylor’s formula, 20
translation, 5
transposed map, 8
union, 2
Download free eBooks at bookboon.com

CONVEXITY: CONVEXITY AND 
OPTIMIZATION – PART I
204
Endnotes
†The intersection of an empty family of sets is usually deﬁned as the entire space,
and using this convention the polyhedron Rn can also be viewed as an intersection of
halfspaces.
‡The terminology is not universal. A proper cone is usually called a salient cone, while
the term proper cone is sometimes reserved for cones that are closed, have a nonempty
interior and do not contain any lines through the origin.
†The second condition is usually not included in the deﬁnition of separation, but we
have included it in order to force a hyperplane H that separates two subsets of a hyperplane
H′ to be diﬀerent from H′.
†For if X = {x0}, then rint X = {x0}, rbdry X = ∅and ext X = {x0}.
‡There is an alternative and more general deﬁnition of the face concept, see exercise 4.7.
Our proper faces are called exposed faces by Rockafellar in his standard treatise Convex
Analysis. Every exposed face is also a face according to the alternative deﬁnition, but the
two deﬁnitions are not equivalent, because there are convex sets with faces that are not
exposed.
ENDNOTES
Chapter 1
Chapter 4
Chapter 3
Download free eBooks at bookboon.com

To see Part II, download:
Linear and Convex Optimization
Convexity and Optimization – Part II
Download free eBooks at bookboon.com


Data Structures
Universal Hash 
Functions: Motivation
Design and Analysis 
of Algorithms I

Purpose : maintain a (possibly evolving) set of stuff.
(transactions, people + associated data, IP addresses, etc.)
Insert : add new record
Hash Table: Supported Operations
Delete : delete existing record
easier/more common with chaining than open addressing
Lookup : check for a particular record
( a “dictionary” )
* 1. properly implemented     2. non-pathological data
Using a “key”
AMAZING 
GUARANTEE
All operations in 
O(1) time ! *

Resolving Collisions
Collision : distinct x,y in U such that h(x) = h(y).
Solution#1: (separate) chaining.
-- keep linked list in each bucket
i
k
/ bj
f
/
l
/
k
i
h
-- given a key/object x, perform Insert/Delete/Lookup in the
list in A[h(x)]
Solution#2 : open addressing. (only one object per bucket)
-- hash function now specifies probe sequence h1(x), h2(x), …
(keep trying till find open slot)
-- examples : linear probing (look consecutively), double hashing
bucket for x
linked list for x
use 2 hash functions

The Load of a Hash Table
Definition : the load factor of a hash table is
: =     # of objects in hash table
# f b
k t
f h
h t bl
Tim Roughgarden
# of buckets of hash table

Which hash table implementation strategy is feasible for load 
factors larger than 1?
Both chaining and open addressing
Only chaining
Only open addressing

The Load of a Hash Table
Definition : the load factor of a hash table is
: =     # of objects in hash table
# of buckets of hash table
Tim Roughgarden
# of buckets of hash table
Note : 1.)        = O(1) is necessary condition for 
operations to run in constant time.
2.) with open addressing, need       <<  1.
Upshot#1 : good HT performance, need to control load.

Upshot#2 : for good HT performance, need a good hash function.
Ideal : user super-clever hash function guaranteed
to spread every data set out evenly.
bl
O S
O
S ! (f
h
h f
i
h
i
Pathological Data Sets
Tim Roughgarden
Problem : DOES NOT EXIST!  (for every hash function, there is a 
pathological data set)
Reason : fix a hash function h : U -> {0,1,2,…,n-1}
⇒a la Pigeonhole Principle, there exist bucket i such that at least 
|u|/n elements of U hash to I under h.
⇒if data set drawn only from these, 
everything collides !

Pathological Data in the Real World
Preference : Crosby and Wallach, USENIX 2003.
Main Point : can paralyze several real-world systems (e.g., 
Tim Roughgarden
p
y
y
g
network intrusion detection) by exploiting badly designed 
hash functions.
-- open source
-- overly simplistic hash function
( easy to reverse engineer a pathological data set )

Solutions
1. Use a cryptographic hash function (e.g., SHA-2)
-- infeasible to reverse engineer a pathological data set
2
Use randomization
In next 2 videos
Tim Roughgarden
2. Use randomization.    In next 2 videos
-- design a family H of hash functions such that for all 
data sets S, “almost all” functions 
spread S 
out “pretty evenly”.
(compare to QuickSort guarantee)

Overview of Universal Hashing
Next : details on randomized solution (in 3 parts).
Part 1 : proposed definition of a “good random hash function”.
(“universal family of hash functions”)
Tim Roughgarden
(
y
)
Part 3 : concrete example of simple + practical such functions
Part 4 : justifications of definition : “good functions” lead to “good
performance”


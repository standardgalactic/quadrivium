Machine Learning Specialization 
Handling missing data 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
3 
©2015-2016 Emily Fox & Carlos Guestrin 
Decision tree review 
T(xi) = Traverse decision tree 
 
 
 
 
 
 
Loan 
Application 
Input:  xi 
ŷi 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
5 
So far: data always completely observed 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
Known x and y 
values for all  
data points  

Machine Learning Specialization 
6 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan application 
may be  
3 or 5 years 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
? 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
? 
high 
safe 
Missing data 

Machine Learning Specialization 
7 
Missing values impact training and predictions 
©2015-2016 Emily Fox & Carlos Guestrin 
 
1.  Training data: Contains “unknown” 
values 
2.  Predictions: Input at prediction time 
contains “unknown” values 

Machine Learning Specialization 
8 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Safe 
excellent 
Income? 
poor 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 
xi = (Credit = poor, Income = ?, Term = 5 years) 
Missing values during prediction 
What do 
we do??? 
Start 
Credit? 
Income? 

Machine Learning Specialization 
10 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
h(x) 
T(x) 
x 
ŷ 
Missing values 
during training 
Missing values 
during prediction 

Machine Learning Specialization 
Handling missing data 
Strategy 1: Puriﬁcation by skipping 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
13 
Idea 1: Puriﬁcation by skipping/removing 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
h(x) is a subset of the data 
without any missing values 
x 
Feature 
extraction 
h(x) 
Remove/skip 
missing values 
May contain data points 
where one (or more) 
features are missing 

Machine Learning Specialization 
15 
Idea 1: Skip data points  
with missing values 
©2015-2016 Emily Fox & Carlos Guestrin 
N = 9, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
low 
risky 
poor 
3 yrs 
? 
safe 
fair 
? 
high 
safe 
N = 6, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
low 
risky 
h(x) 
x 
Skip data 
points with 
missing values 

Machine Learning Specialization 
16 
The challenge with Idea 1 
©2015-2016 Emily Fox & Carlos Guestrin 
N = 9, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
? 
high 
risky 
excellent 
? 
low 
risky 
fair 
? 
high 
safe 
poor 
3 yrs 
low 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
N = 3, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
3 yrs 
high 
safe 
poor 
3 yrs 
low 
risky 
h(x) 
x 
Skip data 
points with 
missing values 
Warning: More than 
50% of the loan 
terms are unknown!  

Machine Learning Specialization 
17 
Idea 2: Skip features  
with missing values 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
? 
high 
risky 
excellent 
? 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
? 
high 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
N = 9, 3 features 
x 
N = 9, 2 features 
h(x) 
Credit 
Income 
y 
excellent 
high 
safe 
fair 
low 
risky 
fair 
high 
safe 
poor 
high 
risky 
excellent 
low 
risky 
fair 
high 
safe 
poor 
high 
risky 
poor 
low 
safe 
fair 
high 
safe 
Skip features 
with many 
missing values 

Machine Learning Specialization 
19 
Missing value skipping: Ideas 1 & 2 
©2015-2016 Emily Fox & Carlos Guestrin 
Idea 1: Skip data points where any 
feature contains a missing value 
-  Make sure only a few data points are 
skipped 
Idea 2: Skip an entire feature if it’s 
missing for many data points 
-  Make sure only a few features are 
skipped 
 
 

Machine Learning Specialization 
20 
Missing value skipping: Pros and Cons 
©2015-2016 Emily Fox & Carlos Guestrin 
Pros 
•  Easy to understand and implement 
•  Can be applied to any model  
(decision trees, logistic regression, linear 
regression,…) 
Cons 
•  Removing data points and features may 
remove important information from data 
•  Unclear when it’s better to remove  
data points versus features 
•  Doesn’t help if data is missing at 
prediction time 

Machine Learning Specialization 
Handling missing data 
Strategy 2: Priﬁcation by imputing 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
23 
Main drawback of skipping strategy 
©2015-2016 Emily Fox & Carlos Guestrin 
N = 9, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
low 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
N = 6, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
low 
risky 
h(x) 
x 
Skip data points with 
missing values 
Data is precious, 
don’t throw  
it away! 

Machine Learning Specialization 
25 
Can we keep all the data? 
©2015-2016 Emily Fox & Carlos Guestrin 
credit 
term 
income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
Use other data points 
in x to “guess” the “?” 

Machine Learning Specialization 
26 
Idea 2: Puriﬁcation by imputing 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Same number of data 
points as the original data 
x 
Feature 
extraction 
h(x) 
Impute/
Substitute 
missing values 

Machine Learning Specialization 
28 
Idea 2: Imputation/Substitution 
©2015-2016 Emily Fox & Carlos Guestrin 
N = 9, 3 features 
Fill in each missing 
value with a 
calculated guess 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
3 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
3 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
N = 9, 3 features 

Machine Learning Specialization 
29 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
3 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
3 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
Example: Replace ? with  
most common value 
©2015-2016 Emily Fox & Carlos Guestrin 
# 3 year loans: 4 
# 5 year loans: 2 
Puriﬁcation by 
imputing 

Machine Learning Specialization 
30 
Common (simple) rules for  
puriﬁcation by imputation 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
? 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
high 
safe 
poor 
3 yrs 
high 
risky 
poor 
? 
low 
safe 
fair 
? 
high 
safe 
Impute each feature with missing values: 
1.  Categorical features use mode: Most 
popular value (mode) of non-missing xi 
2.  Numerical features use average or median: 
Average or median value of non-missing xi 
Many advanced methods exist,  
e.g., expectation-maximization (EM) algorithm 

Machine Learning Specialization 
32 
Missing value imputation: Pros and Cons 
©2015-2016 Emily Fox & Carlos Guestrin 
Pros 
•  Easy to understand and implement 
•  Can be applied to any model  
(decision trees, logistic regression, linear regression,…) 
•  Can be used at prediction time: use same 
imputation rules 
Cons 
•  May result in systematic errors  
Example: Feature “age” missing in all banks in 
Washington by state law  

Machine Learning Specialization 
Handling missing data 
Strategy 3: Adapt learning algorithm  
to be robust to missing values 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
34 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Safe 
excellent 
Income? 
poor 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 
xi = (Credit = poor, Income = ?, Term = 5 years) 
Missing values during prediction: revisited 
What do 
we do??? 
Start 
Credit? 
Income? 

Machine Learning Specialization 
35 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Safe 
excellent 
Income? 
poor 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 
xi = (Credit = poor, Income = ?, Term = 5 years) 
Add missing values to the tree deﬁnition 
Associate missing 
values with a branch 
Start 
Credit? 
Income? 
or unknown 
Risky 

Machine Learning Specialization 
37 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Safe 
excellent 
Income? 
poor 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 
Add missing value choice to every decision node 
or unknown 
Every decision node includes choice 
of response to missing values 
or unknown 
or unknown 
or unknown 

Machine Learning Specialization 
38 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Term? 
Safe 
fair 
5 years 
Prediction with missing values becomes simple 
xi = (Credit = ?, Income = high, Term = 5 years) 
or unknown 
Safe 
excellent 
Income? 
poor 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 
or unknown 
or unknown 
Risky 
3 years 
or unknown 

Machine Learning Specialization 
39 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Income? 
poor 
Term? 
Safe 
high 
5 years 
Prediction with missing values becomes simple 
xi = (Credit = poor, Income = high, Term = ?) 
or unknown 
Safe 
excellent 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Risky 
3 years 
or unknown 
or unknown 
or unknown 

Machine Learning Specialization 
41 
Explicitly handling missing data by 
 learning algorithm: Pros and Cons 
©2015-2016 Emily Fox & Carlos Guestrin 
Pros 
•  Addresses training and prediction time 
•  More accurate predictions 
Cons 
•  Requires modiﬁcation of learning algorithm 
-  Very simple for decision trees 

Machine Learning Specialization 
Feature split selection  
with missing data 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
44 
Pick feature split 
leading to lowest 
classiﬁcation error 
Greedy decision tree learning 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Start with an empty tree 
 
•  Step 2: Select a feature to split data 
 
•  For each split of the tree: 
•  Step 3: If nothing more to,           
make predictions 
 
•  Step 4: Otherwise, go to Step 2 & 
continue (recurse) on this split 
Must select feature &  
branch for missing values! 

Machine Learning Specialization 
46 
Feature split (without missing values) 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
excellent 
fair 
poor 
Credit? 
Split on the 
feature Credit 
Data points with  
Credit = poor 
go here 

Machine Learning Specialization 
47 
Feature split (with missing values) 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
excellent 
fair 
poor  
or unknown 
Credit? 
Data points with  
Credit = poor or unknown 
go here 
Split on the 
feature Credit 

Machine Learning Specialization 
48 
Missing value handling in threshold splits 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
< $50K  
or unknwon 
>= $50K 
Income  
Data points with  
income < $50K or unknown 
Split on the 
feature Income 

Machine Learning Specialization 
50 
Should missing go left, right, or middle? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
excellent  
or unknown 
fair 
poor 
Credit? 
Choice 1: Missing values go 
with Credit=excellent 
Root 
excellent 
fair  
or unknown 
poor 
Credit? 
Choice 2: Missing values  
go with Credit=fair 
Root 
excellent 
fair 
poor  
or unknown 
Credit? 
Choice 3: Missing values 
go with Credit=poor 
Choose branch that leads to  
lowest classiﬁcation error! 

Machine Learning Specialization 
51 
Computing classiﬁcation error of  
decision stump with missing data 
©2015-2016 Emily Fox & Carlos Guestrin 
N = 40, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
? 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
? 
3 yrs 
low 
risky 
? 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
… 
… 
… 
… 
Root 
 22    18 
excellent  
or unknown 
 
fair 
 
poor 
 
Credit? 
Observed values 
9    0 
8    4 
4    12 
“Unknown” 
1    2 
Total 
    10   2 
8    4 
4    12 
Error =            . 
                
         = 

Machine Learning Specialization 
52 
Use classiﬁcation error to decide 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
 22    18 
excellent  
or unknown 
 10    2 
 
fair 
 
 8    4 
 
poor 
 
 4    12 
Credit? 
Root 
 22    18 
 
excellent 
 
 9    0 
fair  
or unknown 
 9    6 
 
poor 
 
4    12 
Credit? 
Root 
 22    18 
 
excellent 
 
 9    0 
 
fair 
 
 8    4 
poor  
or unknown 
 5    14 
Credit? 
Choice 1: error = 0.25  
Choice 2: error = 0.25  
Choice 3: error = 0.225 
WINNER 

Machine Learning Specialization 
54 
Feature split selection algorithm  
with missing value handling 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Given a subset of data M (a node in a tree) 
 
•  For each feature hi(x): 
1.  Split data points of M where hi(x) is not 
“unknown” according to feature hi(x)  
 
2.  Consider assigning data points with “unknown” 
value for hi(x) to each branch 
A.  Compute classiﬁcation error split & branch 
assignment of “unknown” values 
•  Chose feature h*(x) & branch assignment of 
“unknown” with lowest classiﬁcation error 

Machine Learning Specialization 
Summary of handling missing data 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
56 
What you can do now… 
©2015-2016 Emily Fox & Carlos Guestrin 
Describe common ways to handling missing data: 
1. 
Skip all rows with any missing values 
2. 
Skip features with many missing values 
3. 
Impute missing values using other data points 
 
Modify learning algorithm (decision trees) to handle 
missing data: 
1. 
Missing values get added to one branch of split 
2. 
Use classiﬁcation error to determine where missing 
values go 

Machine Learning Specialization 
57 
Thank you to Dr. Krishna Sridhar  
Dr. Krishna Sridhar 
Staﬀ Data Scientist, Dato, Inc. 
©2015-2016 Emily Fox & Carlos Guestrin 


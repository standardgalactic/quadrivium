Lecture Notes in Artiﬁcial Intelligence
5641
Edited by R. Goebel, J. Siekmann, and W. Wahlster
Subseries of Lecture Notes in Computer Science

Anna Esposito Robert Vích (Eds.)
Cross-Modal Analysis
ofSpeech,Gestures,Gaze
and Facial Expressions
COST Action 2102 International Conference
Prague, Czech Republic, October 15-18, 2008
Revised Selected and Invited Papers
1 3

Series Editors
Randy Goebel, University of Alberta, Edmonton, Canada
Jörg Siekmann, University of Saarland, Saarbrücken, Germany
Wolfgang Wahlster, DFKI and University of Saarland, Saarbrücken, Germany
Volume Editors
Anna Esposito
Second University of Naples, Department of Psychology
and IIASS, International Institute for Advanced Scientiﬁc Studies
Via G. Pellegrino 19, 84019 Vietri sul Mare (SA), Italy
E-mail: iiass.annaesp@tin.it
Robert Vích
Institute of Photonics and Electronics
Academy of Sciences of the Czech Republic
Chaberská 57, 182 52 Prague 8, Czech Republic
E-mail: vich@ufe.cz
Library of Congress Control Number: 2009931057
CR Subject Classiﬁcation (1998): I.5, H.5, I.2.7, I.2.10, I.4
LNCS Sublibrary: SL 7 – Artiﬁcial Intelligence
ISSN
0302-9743
ISBN-10
3-642-03319-9 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-03319-3 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2009
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
SPIN: 12731275
06/3180
5 4 3 2 1 0

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This book is dedicated to: 
 
Maria Marinaro  
A person of exceptional human and ethical qualities  
and a scientist of outstanding value. 
 
and to all those:   
 
who posit questions whose answers raise new questions  
 driving  the emotional sense of any scientific work 
 

 
Preface 
This volume brings together the peer-reviewed contributions of the participants at the 
COST 2102 International Conference on “Cross-Modal Analysis of Speech, Gestures, 
Gaze and Facial Expressions” held in Prague, Czech Republic, October 15–18, 2008. 
The conference was sponsored by COST (European Cooperation in the Field of 
Scientific and Technical Research, www.cost.esf.org/domains_actions/ict) in the do-
main of Information and Communication Technologies (ICT) for disseminating the 
research advances developed within COST Action 2102: “Cross-Modal Analysis of 
Verbal and Nonverbal Communication” http://cost2102.cs.stir.ac.uk. 
COST 2102 research networking has contributed to modifying the conventional 
theoretical approach to the cross-modal analysis of verbal and nonverbal communica-
tion changing the concept of face to face communication with that of body to body 
communication as well as developing the idea of embodied information. Information 
is no longer the result of a difference in perception and is no longer measured in terms 
of quantity of stimuli, since the research developed in COST 2102 has proved that 
human information processing is a nonlinear process that cannot be seen as the sum of 
the numerous pieces of information available. Considering simply the pieces of infor-
mation available, results in a model of the receiver as a mere decoder, and produces a 
huge simplification of the communication process. What has emerged from COST 
2102 research is that human information processing does rely on several communica-
tion modes but also, more importantly, strongly depends on the context in which the 
communication process is instantiated. The implications are a change of perspective 
where the research focus moves from “communicative tools” to “communicative in-
stances” and ask for investigations that take into account the environment and the 
context in which communicative acts take place. The consequences in ICT research 
should lead to the development of instantiated interactive dialogue systems and instan-
tiated intelligent avatars able to act by exploiting contextual and environmental signals 
and to process them by combining previous experience (memory) adapted to the prob-
lem instance. 
Currently, advances in COST 2102 research have shown the implementation of in-
teractive systems such as: 
• 
Visitors controlled an avatar in a lively multi-user 3D environment 
       where characters follow natural and realistic behavior patterns. 
• 
Virtual trainer that monitors the user's behavior. 
• 
Demonstration of the use of motion capture, physical simulation and 
       kinematics on a single body. 
• 
Multimodal signal processing for dance synthesis by analysis. 
• 
Alternative augmentative communication systems. 
• 
Infant voice-controlled robot. 

VIII 
Preface 
• 
Data of interactions and vocal and facial emotional expressions that 
       are exploited for developing new algorithms and mathematical 
       models for vocal and facial expression recognition. 
• 
Software showing progress made in HMI, as far as spoken dialogue 
       is concerned. 
• 
Remote health monitoring. 
• 
Telecommunication. 
Some of these implementations and results were presented at ICT 2008 in Lyon, 
France, during November 23–25, 2008 
http://www.cost.esf.org/about_cost/cost_stories/ICT-2008 
http://www.cost.esf.org/events/ICT-2008-I-s-to-the-Future 
The conference in Prague was developed around the COST 2102 main themes and  
benefited from a special session on “Emotions and ICT” jointly organized with COST 
298 (http://www.cost298.org/).  
This book is roughly arranged into three sections, according to a thematic classifi-
cation, even though the COST 2102 research field is largely interdisciplinary and 
complex, involving expertise in computer graphics, animation, artificial intelligence, 
natural language processing, cognitive and psychological modeling of human–human 
and human–machine interaction, linguistics, communication, and artificial life and  
cross-fertilization between social sciences and engineering (psychology, sociology, 
linguistic, neuropsychology). 
The first section “Emotion and ICT,” deals with themes related to the cross-
fertilization between studies on ICT practices of use and cross-modal analysis of ver-
bal and nonverbal communication.  
The second section, “Verbal and Nonverbal Features of Computational Phonetics,” 
presents original studies devoted to the modeling of verbal and nonverbal phonetics.  
The third section, “Algorithmic and Theoretical Analysis of Multimodal Inter-
faces,” presents theoretical and practical implementations of original studies devoted 
to the analysis of speech, gestures, face and head movements as well as to learning 
issues in human–computer interaction and to algorithmic solutions for noise environ-
ments in human–machine exchanges.  
The editors would like to thank the COST- ICT Programme for supporting the re-
alization of the conference and the publication of this volume, and in particular the 
COST Science Officers Gian Mario Maggio, Francesca Boscolo and Sophie Beaubron 
for their constant help, and guidance. 
Our gratitude goes to the staff of the Charles University in Prague, and in particular 
to Jan Volin for making available the space and people to help in the conference 
organization. The Prague Academia of Science, Institute of Photonics and Electronics, 
is deeply acknowledged for contributing to the event, in particular, Petr Horák for his 
hard and invaluable work. 
Special appreciation goes to the International Institute for Advanced Scientific Studies, 
and in particular to Tina Marcella Nappi, Michele Donnarumma, and Antonio Natale, for 
their invaluable editorial and technical support in the organization of this volume .   
 

Preface IX 
The editors are extremely grateful to the contributors and the keynote speakers, 
whose work stimulated an extremely interesting interaction with the attendees, and to 
COST 2102 International Scientific Committee for the accurate review work, for their 
dedication, and their valuable selection process.  
 
May 2009 
 
 
 
 
 
 
 Anna Esposito 
Robert Vích  
 

 
Organization 
International Advisory and Organizing Committee 
Robert Vích  
Institute of Photonics and Electronics, Prague,  
Czech Republic 
Anna Esposito  
Second University of Naples and IIASS, Italy  
Eric Keller 
University of Lausanne, Switzerland  
Macos Faundez-Zanuy  
University of Mataro, Barcelona, Spain  
Petr Horák  
Institute of Photonics and Electronics, Prague,  
Czech Republic 
Amir Hussain  
University of Stirling, UK  
Dagmar Dvořáková  
Media Communication Department,  Prague,  
Czech Republic 
Jitka Veroňková  
Institute of Phonetics, Charles University, Prague,  
Czech Republic 
Jitka Pečenková  
Institute of Photonics and Electronics, Prague,  
Czech Republic 
Irena Vítková  
Media Communication Department, Prague,  
Czech Republic 
Jan Volín  
Institute of Phonetics, Charles University, Prague,  
Czech Republic 
International Scientific Committee 
Uwe Altmann 
Technische Universität Dresden, Germany  
Hicham Atassi 
Brno University of Technology,  Czech Republic 
Nikos Avouris 
University of Patras, Greece 
Ruth Bahr 
University of South Florida, USA  
Gérard Bailly 
ICP, Grenoble, France  
Marian Bartlett 
University of California, San Diego, USA 
Štefan Beňuš 
Constantine the Philosopher University, Nitra, Slovakia  
Niels Ole Bernsen 
University of Southern Denmark, Denmark  
Jonas Beskow 
Royal Institute of Technology, Sweden                
Horst Bishof 
Technical University Graz, Austria   
Peter Birkholz 
Aachen University, Germany 
Jean-Francois Bonastre 
Universitè d'Avignon, France  
Nikolaos Bourbakis 
ITRI, Wright State University, Dayton, USA  
Maja Bratanić 
University of Zagreb, Croatia  
Antonio Calabrese 
Istituto di Cibernetica – CNR, Naples, Italy 
Paola Campadelli 
Università di Milano, Italy  
Nick Campbell 
ATR Human Information Science Labs, Kyoto, Japan  
Antonio Castro Fonseca 
Universidade de Coimbra, Portugal  

 
Organization 
XII 
Aleksandra Cerekovic 
Faculty of Electrical Engineering, Croatia  
Josef Chaloupka 
Technical University of Liberec, Czech Republic 
Mohamed Chetouani 
Universitè Pierre et Marie Curie, France  
Gerard Chollet 
CNRS-LTCI, Paris, France  
Muzeyyen Ciyiltepe 
Gulhane Askeri Tip Academisi, Ankara, Turkey  
Anton Čižmár 
Technical University of Košice, Slovakia  
Nicholas Costen 
Manchester Metropolitan University, UK  
Vlado Delic 
University of Novi Sad, Serbia 
Marion Dohen 
ICP, Grenoble, France 
Francesca D’Olimpio 
Second University of Naples, Italy  
Thierry Dutoit 
Faculté Polytechnique de Mons, Belgium  
Laila Dybkjær 
University of Southern Denmark, Denmark  
Matthias Eichner 
Technische Universität Dresden, Germany 
Aly El-Bahrawy 
Faculty of Engineering, Cairo, Egypt  
Engin Erzin 
Koc University, Istanbul, Turkey 
Anna Esposito 
Second University of Naples, and IIASS, Italy  
Joan Fàbregas Peinado 
Escola Universitaria de Mataro, Spain 
Sascha Fagel 
Technische Universität Berlin, Germany  
Nikos Fakotakis 
University of Patras, Greece  
Marcos Faundez-Zanuy 
Escola Universitaria de Mataro, Spain  
Dilek Fidan 
Ankara University, Turkey  
Leopoldina Fortunati 
Università di Udine, Italy 
Carmen García-Mateo 
University of Vigo, Spain 
Björn Granström 
Royal Institute of Technology (KTH), Sweden  
Marco Grassi 
Università Politecnica delle Marche, Italy 
Maurice Grinberg 
New Bulgarian University, Bulgaria 
 
Mohand Said Hacid 
Universitè Claude Bernard Lyon 1, France  
Jaakko Hakulinen 
 
University of Tampere, Finland  
Ioannis Hatzilygeroudis 
University of Patras, Greece  
Immaculada Hernaez 
University of the Basque Country, Spain  
Javier Hernando 
Technical University of Catalonia, Spain  
Wolfgang Hess 
Universität Bonn, Germany  
Dirk Heylen 
University of Twente, The Netherlands  
Rüdiger Hoffmann 
Technische Universität Dresden, Germany  
David House 
Royal Institute of Technology (KTH), Sweden  
Amir Hussain 
University of Stirling, UK  
Ewa Jarmolowicz  
Adam Mickiewicz University, Poznan, Poland 
Kristiina Jokinen 
University of Helsinki, Finland 
Jozef Juhár 
Technical University Košice, Slovak Republic 
Zdravko Kacic 
University of Maribor, Slovenia  
Maciej Karpinski 
Adam Mickiewicz University, Poznan, Poland  
Eric Keller 
Université de Lausanne, Switzerland  
Adam Kendon 
University of Pennsylvania, USA  
Stefan Kopp 
University of Bielefeld, Germany 
Jacques Koreman 
University of Science and Technology, Norway 
Robert Krauss 
Columbia University, New York, USA 
Maria Koutsombogera 
Inst. for Language and Speech Processing, Greece  

 
  
                                                  Organization  
XIII 
Bernd Kröger 
Aachen University, Germany 
Gernot Kubin 
Graz University of Technology, Austria 
Alida Labella 
Second University of Naples, Italy  
Yiannis Laouris 
Cyprus Neuroscience and Technology Institute, Cyprus 
Børge Lindberg 
Aalborg University, Denmark  
Wojciech Majewski 
Wroclaw University of Technology, Poland  
Pantelis Makris 
Neuroscience and Technology Institute, Cyprus  
Raffaele Martone 
Second University of Naples, Italy  
Dominic Massaro 
University of California - Santa Cruz, USA             
David McNeill  
University of Chicago, USA  
Nicola Melone 
Second University of Naples, Italy  
Katya Mihaylova 
University of National and World Economy, Sofia, 
Bulgaria 
Michal Mirilovič 
Technical University of Košice, Slovakia 
Peter Murphy 
University of Limerick, Ireland  
Antonio Natale 
Salerno University and IIASS, Italy  
Eva Navas 
Escuela Superior de Ingenieros, Bilbao, Spain  
Delroy Nelson 
University College London, UK 
Géza Németh 
Budapest University of Technology, Hungary  
Friedrich Neubarth 
Research Inst. Artificial Intelligence, Austria 
Giovanna Nigro 
Second University of Naples, Italy  
Anton Nijholt 
University of Twente, The Netherlands        
Jan Nouza 
Technical University of Liberec, Czech Republic 
Igor Pandzic 
Faculty of Electrical Engineering, Croatia  
Harris Papageorgiou 
Inst. for Language and Speech Processing, Greece  
Ana Pavia 
Spoken Language Systems Laboratory, Portugal  
Catherine Pelachaud 
Université de Paris 8, France  
Bojan Petek 
University of Ljubljana, Slovenia  
Harmut R. Pfitzinger 
University of Munich, Germany  
Francesco Piazza 
Università Politecnica delle Marche, Italy  
Neda Pintaric 
University of Zagreb, Croatia  
Isabella Poggi 
Università di Roma 3, Italy 
Jiří Přibil 
Academy of Sciences, Czech Republic  
Anna Přibilová 
Slovak University of Technology, Slovakia 
Michael Pucher 
Telecommunications Research Center Vienna, Austria 
Jurate Puniene 
Kaunas University of Technology, Lithuania 
Giuliana Ramella 
Istituto di Cibernetica – CNR, Naples, Italy 
Kari-Jouko Räihä 
University of Tampere, Finland  
José Rebelo 
Universidade de Coimbra, Portugal  
Luigi Maria Ricciardi 
Università di Napoli “Federico II”, Italy  
Matej Rojc 
University of Maribor, Slovenia 
Algimantas Rudzionis 
Kaunas University of Technology, Lithuania 
Vytautas Rudzionis 
Kaunas University of Technology, Lithuania 
Milan Rusko 
Slovak Academy of Sciences, Slovak Republic 
Zsófia Ruttkay 
Pazmany Peter Catholic University, Hungary 
Bartolomeo Sapio 
Fondazione Ugo Bordoni, Rome, Italy 
Yoshinori Sagisaka 
Waseda University, Tokyo, Japan  

 
Organization 
XIV 
Silvia Scarpetta 
Salerno University, Italy  
Ralph Schnitker 
Aachen University, Germany 
Jean Schoentgen 
Université Libre de Bruxelles, Belgium  
Stefanie  
Shattuck-Hufnagel 
 
MIT, Cambridge, USA  
Zdeněk Smékal 
Brno University of Technology,  Czech Republic  
Stefano Squartini 
Università Politecnica delle Marche, Italy  
Piotr Staroniewicz 
Wroclaw University of Technology, Poland  
Vojtěch Stejskal 
Brno University of Technology, Czech Republic  
Marian Stewart-Bartlett 
University of California, San Diego, USA 
Jianhua Tao 
Chinese Academy of Sciences, P.R. China  
Jure F. Tasič 
University of Ljubljana, Slovenia  
Murat Tekalp 
Koc University, Istanbul, Turkey 
Kristinn Thórisson 
Reykjavík University, Iceland  
Isabel Trancoso 
Spoken Language Systems Laboratory, Portugal  
Luigi Trojano 
Second University of Naples, Italy  
Wolfgang Tschacher 
University of Bern, Switzerland  
Markku Turunen 
University of Tampere, Finland  
Henk Van Den Heuvel 
Radboud University Nijmegen,The Netherlands 
Robert Vích 
Academy of Sciences, Czech Republic  
Klára Vicsi 
Budapest University of Technology, Hungary  
Leticia  
Vicente-Rasoamalala 
 
Alchi Prefectural Univesity, Japan   
Hannes Högni 
Vilhjálmsson 
 
Reykjavík University, Iceland  
Jane Vincent 
University of Surrey, Guilford, UK 
Vogel 
University of Dublin, Ireland  
Jan Volín 
Charles University, Czech Republic 
Rosa Volpe 
Université De Perpignan Via Domitia, France  
Yorick Wilks 
University of Sheffield, UK 
Matthias Wimmer 
Technische Universiät München, Germany 
Matthias Wolf 
Technische Universität Dresden, Germany 
Bencie Woll 
University College London, UK 
Bayya Yegnanarayana 
Institute of Information Technology, India 
Jerneja Žganec Gros  
Alpineon Development and Research, Slovenia 
Goranka Zoric 
Faculty of Electrical Engineering, Croatia 
Sponsors  
• COST - European Science Foundation: COST ACTION 2102: “Cross-Modal 
Analysis of Verbal and Nonverbal Communication”  
• Institute of Photonics and Electronics, Academy of Sciences, Prague, Czech 
Republic 
• Media and Communication, Academy of Sciences, Prague, Czech Republic  
• Institute of Phonetics, Faculty of Philosophy and Arts, Charles University, 
Prague, Czech Republic  

 
  
                                                  Organization  
XV 
• Institute of Applied Physics, Johann Wolfgang University, Frankfurt/Main, 
Germany 
• Institute of Phonetics, Johann Wolfgang University, Frankfurt/Main, Germany  
• Institute of Acoustics and Speech Communication, Dresden University of 
Technology, Dresden, Germany 
• International Institute for Advanced Scientific Studies, Italy 
• Second University of Naples, Caserta, Italy 
• Regione Campania, Italy 
• Provincia di Salerno, Italy 
 
 

Table of Contents
I Emotions and ICT
Cross-Fertilization between Studies on ICT Practices of Use and
Cross-Modal Analysis of Verbal and Nonverbal Communication . . . . . . . .
1
Leopoldina Fortunati, Anna Esposito, and Jane Vincent
Theories without Heart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
Leopoldina Fortunati
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot
Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
ˇStefan Beˇnuˇs and Milan Rusko
Aﬃliations, Emotion and the Mobile Phone . . . . . . . . . . . . . . . . . . . . . . . . .
28
Jane Vincent
Polish Emotional Speech Database – Recording and Preliminary
Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
Piotr Staroniewicz and Wojciech Majewski
Towards a Framework of Critical Multimodal Analysis: Emotion in a
Film Trailer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
Maria Bortoluzzi
Biosignal Based Emotion Analysis of Human-Agent Interactions . . . . . . .
63
Evgenia Hristova, Maurice Grinberg, and Emilian Lalev
Emotional Aspects in User Experience with Interactive Digital
Television: A Case Study on Dyslexia Rehabilitation . . . . . . . . . . . . . . . . .
76
Filomena Papa and Bartolomeo Sapio
Investigation of Normalised Time of Increasing Vocal Fold Contact as a
Discriminator of Emotional Voice Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
Peter J. Murphy and Anne-Maria Laukkanen
Evaluation of Speech Emotion Classiﬁcation Based on GMM and Data
Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
Martin Vondra and Robert V´ıch
Spectral Flatness Analysis for Emotional Speech Synthesis and
Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
Jiˇr´ı Pˇribil and Anna Pˇribilov´a

XVIII
Table of Contents
II Verbal and Nonverbal Features of Computational
Phonetics
Voice Pleasantness of Female Voices and the Assessment of Physical
Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Vivien Zuta
Technical and Phonetic Aspects of Speech Quality Assessment: The
Case of Prosody Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
Jana Tuˇckov´a, Jan Holub, and Tom´aˇs Dubˇeda
Syntactic Doubling: Some Data on Tuscan Italian . . . . . . . . . . . . . . . . . . . .
133
Anna Esposito
Perception of Czech in Noise: Stability of Vowels . . . . . . . . . . . . . . . . . . . . .
149
Jitka Veroˇnkov´a and Zdena Palkov´a
Challenges in Segmenting the Czech Lateral Liquid . . . . . . . . . . . . . . . . . . .
162
Radek Skarnitzl
Implications of Acoustic Variation for the Segmentation of the Czech
Trill /r/ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Pavel Machaˇc
Voicing in Labial Plosives in Czech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
Annett B. Jorschick
Normalization of the Vocalic Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
Jan Vol´ın
III Algorithmic and Theoretical Analysis of
Multimodal Interfaces
Gaze Behaviors for Virtual Crowd Characters . . . . . . . . . . . . . . . . . . . . . . .
201
Helena Grillon, Barbara Yersin, Jonathan Ma¨ım, and
Daniel Thalmann
Gestural Abstraction and Restatement: From Iconicity to Metaphor . . . .
214
Nicla Rossini
Preliminary Prosodic and Gestural Characteristics of Instructing Acts
in Polish Task-Oriented Dialogues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
Maciej Karpi´nski
Polish Children’s Gesticulation in Narrating (Re-telling) a Cartoon . . . . .
239
Ewa Jarmolowicz-Nowikow
Prediction of Learning Abilities Based on a Cross-Modal Evaluation of
Non-verbal Mental Attributes Using Video-Game-Like Interfaces . . . . . . .
248
Yiannis Laouris, Elena Aristodemou, and Pantelis Makris

Table of Contents
XIX
Automatic Sentence Modality Recognition in Children’s Speech, and
Its Usage Potential in the Speech Therapy . . . . . . . . . . . . . . . . . . . . . . . . . .
266
D´avid Sztah´o, Katalin Nagy, and Kl´ara Vicsi
Supporting Engagement and Floor Control in Hybrid Meetings . . . . . . . .
276
Rieks op den Akker, Dennis Hofs, Hendri Hondorp,
Harm op den Akker, Job Zwiers, and Anton Nijholt
Behavioral Consistency Extraction for Face Veriﬁcation . . . . . . . . . . . . . . .
291
Hui Fang and Nicholas Costen
Protecting Face Biometric DCT Templates by Means of Pseudo-random
Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
Marcos Faundez-Zanuy
Facial Expressions Recognition from Image Sequences . . . . . . . . . . . . . . . .
315
Zahid Riaz, Christoph Mayer, Michael Beetz, and Bernd Radig
Czech Artiﬁcial Computerized Talking Head George . . . . . . . . . . . . . . . . . .
324
Josef Chaloupka and Zdenek Chaloupka
An Investigation into Audiovisual Speech Correlation in Reverberant
Noisy Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
Simone Cifani, Andrew Abel, Amir Hussain, Stefano Squartini, and
Francesco Piazza
Articulatory Speech Re-synthesis: Proﬁting from Natural Acoustic
Speech Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344
Dominik Bauer, Jim Kannampuzha, and Bernd J. Kr¨oger
A Blind Source Separation Based Approach for Speech Enhancement
in Noisy and Reverberant Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
Alessio Pignotti, Daniele Marcozzi, Simone Cifani,
Stefano Squartini, and Francesco Piazza
Quantitative Analysis of the Relative Local Speech Rate . . . . . . . . . . . . . .
368
Jan Janda
Czech Spontaneous Speech Collection and Annotation: The Database
of Technical Lectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
Josef Rajnoha and Petr Poll´ak
BSSGUI – A Package for Interactive Control of Blind Source Separation
Algorithms in MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
386
Jakub Petkov and Zbynˇek Koldovsk´y
Accuracy Analysis of Generalized Pronunciation Variant Selection in
ASR Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
V´aclav Hanˇzl and Petr Poll´ak

XX
Table of Contents
Analysis of the Possibilities to Adapt the Foreign Language
Speech Recognition Engines for the Lithuanian Spoken Commands
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
Rytis Maskeliunas, Algimantas Rudzionis, and Vytautas Rudzionis
MLLR Transforms Based Speaker Recognition in Broadcast Streams . . .
423
Jan Silovsky, Petr Cerva, and Jindrich Zdansky
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433

 
A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 1–4, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Cross-Fertilization between Studies on ICT Practices of 
Use and Cross-Modal Analysis of Verbal and Nonverbal 
Communication 
Leopoldina Fortunati1, Anna Esposito2, and Jane Vincent3 
1 Faculty of Education, Multimedia Science and Technology, University of Udine  
Via Prasecco, 3 – 33170 Pordenone, Italy 
fortunati.deluca@tin.it 
2 Second University of Naples, Department of Psychology, and IIASS, Italy 
anna.esposito@unina2.it 
3 Faculty of Arts and Human Sciences, University of Surrey, 
Stag Hill, Guildford, Surrey GU2 7XH 
jane.vincent@surrey.ac.uk 
Abstract. The following are comments and considerations on how the 
Information Communication Technology (ICT) will exploits research on cross 
modal analysis of verbal and nonverbal communication.   
Keywords: Communication behaviors, multimodal signals, human-machine 
interaction. 
1   Introduction  
In the first place let us to propose some reflections on how the cross-fertilization 
between the field of studies on ICT use practices and the cross-modal analysis of 
Verbal and Nonverbal Communication [1-4] could take place in a reciprocal and 
productive way.  One notion that might be corrected in the current studies is that of 
face-to-face communication with body-to-body communication. In sociological 
studies the expression face-to-face, that is still used in many technical studies, has 
been criticized and overcome in favour of the expression body-to-body [6]. In fact, as 
all the tradition of studies on non-verbal language points out, individuals talk with all 
their body not only with their face, although the face is a very strategic site of the 
communication process. Also in the terminology one should consider, or at least be 
aware of, all the variables that compose the communication process.  
Another notion that might be more deeply problematised is that of information. 
There is an approach to information which can be summarized with Bateson’s words 
“Information is the perception of a difference”. Maybe the result of a perception can 
be measured and counted. But the possibility of measuring it might give the illusion 
that it is possible to construct a scientific analysis only by reducing the problem to the 
measurement of the perception result. In effect this would be a wrong approach, 
because the reality of information is much more complex since it concerns human 
beings. If one enlarges the notion of information through the lens of a sociological 
contribution and one claims that information is also a relational concept, the 

2 
L. Fortunati, A. Esposito, and J. Vincent 
 
measurability of information becomes complicated, since in the field of research of 
cross-modal analysis of Verbal and Nonverbal Communication it might be necessary 
to activate also the notion of e-actors and their power[9, 7]. So, looking at the 
problem of information from a user’s point of view, it comes out that the interest of 
the receiver/e-actor is not simply to have the greatest possible amount of information. 
This means that it not true that the more information one has, the better it is, because 
considering only the amount of information has the consequence of conceptualising 
the role of the receiver only as being a mere decoder of information and to propose a 
reductionist vision of the communication process. On the contrary, the amount of 
information which one tends to obtain is related to several aspects of the 
communication process. Let us focus on two of these aspects: the power of the 
receiver and the quality of information. In regard to the first aspect, compare, for 
example, reading a book and seeing a movie on TV. In the first case the only channel 
involved, writing, gives to the reader information that is limited in detail, but this 
means that the reader can handle the information better. In this case the reader is able 
to exploit his/her imagination and co-operate with the writer to a great extent. In the 
second case, the co-operation by the audience is reduced, since the product that is 
consumed is formalized at a much higher level. What is at the stake in these two 
examples is the difference in the audience’s power over the consumption of the 
product, which in the latter case is in a certain sense reduced by the greater amount of 
information. But the problem of course is not only of the power of e-actors on the 
product, otherwise one should not be able to understand why audiences celebrated the 
advent of television. The exercise of this power by the e-actor is maybe addressed in 
some contexts and situations by other characteristics of the product, others than the 
amount of information contained in it, such as, for example, its pleasantness and 
relaxing qualities. These are the cases in which e-actors like to consume products 
which require less commitment and involvement from them. This has to do with the 
ambiguous essence of the consumption which actually should be understood as 
productive labour and that often e-actors aspire to reduce to a minimum. This 
aspiration has been often misunderstood by scholars studying patterns and styles of 
TV consumption through the optics of “audiences’ passivity”.  
Another aspect of the information process is which setting we can study the issue 
of information in regard to its quality, that is its efficacy. An indirect measurement of 
the efficacy of the communication process might be memory. In the middle and long 
terms one remembers what one sees much better than what one reads [5].  This means 
that more detailed information is more effective than less detailed information in the 
memorization process.  
Third, continuing our attempt to cross-fertilize the field of cross-modal analysis of 
Verbal and Nonverbal Communication with the main tenets of the field of ICT users’ 
studies, we recall that the communication process should always be studied in its multiple 
social contexts, because it is shaped also by the social organization of relationships and 
so it is intelligible only in concrete situations, local practices and contexts. It is the same 
concern which was expressed by de Saussure with regard to language. Meaning is not 
understandable if it is not situated in a broad context, which maybe is the sentence, as 
minimal unit of a text. Take, for example, the research carried out on audio and visual 
cues in interpersonal communication: their results might be fully understandable only 
when they will be situated in a social context. But this would imply, at the same time, 

Cross-Fertilization between Studies on ICT Practices of Use and Cross-Modal Analysis 
3 
 
that we reflect on the need to bring together different methodological approaches. To 
what extent does it make sense to continue to study these issues only in laboratory?  Does 
it, instead, make more sense to study them in laboratory but also in social contexts and so 
try to design a completely different research? 
Fourth, another important and recent tenet of psycho-sociological studies has been 
to see the fundamental role of emotion in the communication process and in the use of 
ICTs [8]. This approach is very important to overcome the implicit premise that the 
communication process is rational, without taking into account the role played by 
emotions in it. This is a debate, that of the electronic, mediated emotion, which is still 
in its infancy, although it is very vivid and ever-growing. Now it is recognized quite 
broadly that emotion always accompanies the process using technologies, their 
practices and the transformations of social organizations in which the technologies are 
used. This should also inspire research projects in physical or engineering studies. 
Fifth, another important issue concerns the signal. If it is acceptable that for the 
operationalisation of a research study a complex concept such as communication has 
to be reduced to a more simple notion such as a signal, then later on it is always 
necessary to come back to the complex notion of communication. This is the only 
way to avoid dangerous shortcomings in the design of the research. Following the 
same line of reasoning, when the signal is chosen for operational reasons it should not 
be seen as part of a communicative act, but rather as part of a communication process, 
constituted by immaterial labour and of a message which is its product and which has 
an economic and normative impact, affects social roles and power, the organization 
and structure of social relationships, and so on. In this case it would be more wise to 
start from the multi-dimensionality of the signal and then to declare that, given the 
difficulty in analysing this multi-dimension nature, only one aspect is selected. In that 
way, the researcher would be more easily aware that it is possible to arrive only at a 
partial and limited conclusion, avoiding a metonymical conclusion (in which a part is 
understood and presented for the all). Moreover it would be better to consider all the 
variables and then to decide on a post-selection of the significant variables by means 
of a factor analysis. So, in this case the choice of the considered variables in the 
design of a research might be justified, otherwise not.  
Finally, just to concluding this attempt to cross-fertilize the field of cross-modal 
analysis of Verbal and Nonverbal Communication with the main tenets of the field of 
ICT users’ studies, we would propose some points coming out from the first 
interdisciplinary dialogue. When research on emotion is designed, it is always the case 
to remember that a) emotions are a continuum; b) different cultural perceptions of 
emotion derive from the fact that emotion have different  archetypical, symbolic and 
metaphorical history; c) in our multicultural society more inter-cultural experiments on 
and studies of emotion are needed; d) one should problematise more emotions: fear is 
both positive and negative (relating it to different contexts). 
References 
1. Esposito, A., Hussain, A., Marinaro, M., Martone, R. (eds.): Multimodal Signals: Cognitive 
and Algorithmic Issues. LNCS, vol. 5398. Springer, Heidelberg (2009), http://www. 
springer.com/computer/artificial/book/978-3-642-00524-4 

4 
L. Fortunati, A. Esposito, and J. Vincent 
 
2. Esposito, A., Bourbakis, N., Avouris, N., Hatzilygeroudis, I. (eds.): HH and HM Interaction. 
LNCS, vol. 5042. Springer, Heidelberg (2008) 
3. Esposito, A., Faundez-Zanuy, M., Keller, E., Marinaro, M. (eds.): COST Action 2102. 
LNCS, vol. 4775. Springer, Heidelberg (2007) 
4. Esposito, A.: The Amount of Information on Emotional States Conveyed by the Verbal and 
Nonverbal Channels: Some Perceptual Data. In: Stilianou, Y., et al. (eds.) COST 277. 
LNCS, vol. 4391, pp. 249–268. Springer, Heidelberg (2007) 
5. Fortunati, L.: Gli italiani al telefono. Angeli, Milano (1995) 
6. Fortunati, L.: Is Body-to-body communication still the prototype? The Information 
Society 21(1), 1–9 (2005) 
7. Fortunati, L., Vincent, J., Gebhardt, J., Petrovčič, A. (eds.): Interaction in Broadband 
Society. Peter Lang, Berlin (2009) 
8. Fortunati, L., Vincent, J.: Introduction. In: Vincent, J., Fortunati, L. (eds.) Electronic 
Emotion. The Mediation of Emotion via Information and Communication Technologies. 
Peter Lang, Oxford (2009) 
9. Haddon, L., Mante, E., Sapio, B., Kommonen, K.-H., Fortunati, L., Kant, A. (eds.): 
Everyday Innovators. Researching the Role of Users in Shaping ICTs. Springer, Heidelberg 
(2005) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 5–17, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Theories without Heart 
Leopoldina Fortunati 
Faculty of Education, Multimedia Science and Technology, University of Udine  
Via Prasecco, 3 – 33170 Pordenone, Italy 
fortunati.deluca@tin.it 
Abstract. In general, sociological theories are, or at least seem, to be without 
heart. However many fundamental sociological notions such as solidarity, 
social cohesion and identity are highly emotional. In the field of information 
and communication technologies studies there is a specific theory, that of 
domestication (Silverstone and Hirsch [58], Silverstone and Haddon [59], 
Haddon and Silverstone [31]), inside which several research studies on the 
emotional relationship with ICTs have flourished (Fortunati [19], [20]; Vincent 
[63]).  In this paper I will focus on this theory, which is one of the frameworks 
most commonly applied to understand the integration of ICTs into everyday 
life. I argue that emotion empowers sociological theories when its analysis is 
integrated into them. To conclude, I will discuss the seminal idea proposed by 
Star and Bowker [62] to consider emotion as an infrastructure of body-to-body 
communication.  
Keywords: Domestication theory, emotion, information and communication 
technologies, infrastructure, body, body-to-body communication, mediated 
communication. 
1   Introduction 
In general, sociological theories do not worry about emotions, they are theories 
without heart. Nevertheless, if one examines seminal sociological notions such as 
solidarity, social cohesion, identity, what are they if not a “bundle of emotion, binding 
social actors to the central symbols of society”, as Illuz ([36]: 2) writes? So, 
sociological theories are apparently without heart as they are inhabited by emotions 
although not in any explicit way. Theories on new media are not an exception at this 
subject. However, there is a  theory among those currently applied in the studies on 
ICTs (Information and Communication Technologies) – domestication theory 
(Silverstone and Hirsch [58], Silverstone and Haddon [59], Haddon and Silverstone 
[31]) - that is in a certain sense an exception, since it includes several research starting 
from the observation of emotional reactions to the diffusion and integration in the 
domestic setting of ICT. It is not by chance that this theory has allowed the 
development of several studies which have taken into account also the emotional 
aspects of the integration of ICTs in the domestic sphere (Fortunati [19], [20]; 
Haddon [33]). Apart from these studies related to domestication theory, another 
seminal approach to emotion is that advanced by Star and Bowker [62] who see 

6 
L. Fortunati 
emotion and the body as an infrastructure of body-to-body communication. 
Considering emotion as an infrastructure maybe allows us also to reflect in a more 
sophisticated way on the role of emotion in mediated communication.  
The aim of this article is to show how the explicit integration of emotion into 
sociological theorization cannot but lead to a more powerful conceptualisation. 
Domestication theory, which through cross-fertilization with domesticity theory has 
included emotion in its conceptual framework, is a good example of the capacity of 
theories to acquire symbolic and conceptual strength when they integrate emotion. In 
the next section I will analyze the development of sociological theorization between 
rationality and emotion. Then, I will examine domestication theory as the theory that 
has allowed the development –even if slowly - of emotion as one of the key elements 
in the understanding of the integration of information and communication 
technologies in everyday life. To conclude, I will discuss the seminal idea to consider 
emotion as an infrastructure of body-to-body communication (Star and Bowker [62]).  
2   Sociological Theory between Rationality and Emotions 
Sociological analysis is generally based on the development of a discourse which 
focuses on the rational layer of social behaviour. However this rational aspect often 
remains in the background or it presents itself like a taken for granted premise. 
Sociological theoretical activity in particular represents itself as an even more 
radically rational undertaking. However, Roger Silverstone, who was one of the main 
theorists of “domestication” approach, reminds us ([61]: 229) that “all concepts are 
metaphors. They stand in place of the world. And in so doing they mask as well they 
reveal it”. 
The approach proposed by Silverstone opens up a lot of issues: the first is that 
theoretical activity is much less rational than it represents itself. In fact metaphors 
introduce tension between two aspects of meaning which might be contradictory or 
which stress a latent or an unexpected relation (Lakoff and Johnson [40]). Metaphors, 
as well as symbols (Jung [37]), are the rhetoric tools that convey in the language the 
ambiguity that psychologically one needs to express. The need to express ambiguity 
derives from the fact that we are emotionally ambiguous, since, as Freud pointed out, 
we love and hate at the same time. Metaphors and symbols are two of the features that 
Umberto Eco characterises as forms of hyper-codification [11]. They are such since 
they require a supplementary effort from the speaker or writer to be formulated and 
from the receiver to be understood. The tension of metaphors and symbols corresponds 
to an emotional dimension which enters in the rational sphere breaking up the game 
(Marchese [46]). The game is always that when one speaks, what one says and what 
one does not say (or what one says in an allusive way) are both relevant.  
Sociological theorization is in particular an activity where a new statement 
accompanies people in conceptual territories that need to be explored, where the old 
encounters the new, which rarely is clear, as in the case of domestication theory. But 
this element, far from being a limitation, represents instead an advantage, as 
Silverstone himself underlines, allowing multiple interpretations and empirical 
experimentation and research. The success of a sociological theory is exactly given by 
the precarious equilibrium between what is said and what is not said, between the 

 
Theories without Heart 
7 
powerfulness of logos and the perturbing presence and energy of emotion. But this 
equilibrium is very much strengthened by the simplicity of a theory. Only simple 
concepts survive over time and domestication is not an exception, argues Silverstone 
([61]: 229). 
The second aspect of the question is that the major part of our conceptual system is 
on the one hand partially structured in terms of other concepts, that is it is structured 
in a metaphoric sense (Lakoff and Johnson [40]), and that, on the other hand, 
metaphors are mainly a conceptual construction. Even the very name of this theory, 
‘domestication’, is a metaphor, which was inspired by the work of the anthropologist 
Kopytoff [39]. Kopytoff proposed that we should see cultural objects as biological 
beings and to analyse their ‘life’ as such. He was in fact convinced that they cease to 
be mere commodities when they are culturally integrated into cultivated home 
environments. When the object is a technological artefact, this taming is an even more 
necessary requirement. In fact, technology has a specific strength, consisting in its 
capacity to produce movement. Movement is the watershed between beings and 
inanimate objects (Fortunati [13]). It is a capacity which qualifies technology as a 
particularly powerful object. It is exactly this power that needs to be domesticated to 
avoid a situation where the technologies revolt against those who use them. When, 
then, the technologies in question are ICTs, the power of the movement technology 
brings with it is particularly strong. In fact ICTs deal with the intellectual and 
communicative processes of individuals. Not by chance Maldonado [44] calls them 
“intellective machines”, Rheingold [55] “tools for thought”. It is the particular power 
of ICTs that has in a certain sense pushed scholars to compare them with ‘wild 
animals’ which needed to be tamed. The process of their integration in the 
household’s context was termed as domestication. In Kopytoff’s analysis of the 
taming of cultural objects seen as “wild animals” there is an implicit emotional 
narrativity. What is the emotion one feels towards wild animals if not fear, terror? The 
process of domestication has enabled  human beings to overcome the fear by building 
a process of familiarity and intimacy with wild animals, by taming them. This feeling 
of fear (and terror) is one of the classical emotions that humankind felt when 
confronted by technology. The other one is wonder or what Mosco [49] calls the 
“digital sublime”. The sense of wonder as an emotional reaction to the technology is 
well documented since ancient times and has found a new reformulation in the 
Weberian notion of enchantment for technology. 
This opposition between the two main emotions tracing the mood towards 
technology has inspired the debate on technology and society until the second world 
war and continues to present itself again in within many of the studies on society and 
technology in the guise of technological determinism. Silverstone [61], in his last 
attempt to take stock of the domestication theory, analyses the technological 
determinist approach, arguing that its internal logic is based on the most important 
emotional pole - wonder and fear - that has traditionally surrounded the attitude of 
society towards technology. Recently Sally Waytt [66] points out that technological 
determinism continues to live and be revived in those mainstream studies and theories 
that want to refuse it in principle. It is a matter of fact that technological determinism 
is strong since technologies, as I mentioned above, are perceived as the powerful 
extension of the human body and are humanized. Machines are human projections 
and become human creatures (Katz [38]). 

8 
L. Fortunati 
The relevant evidence for my discourse is that in the first period when we encounter 
a technology the mainstream narrativity on domestication describes a path in which the 
technology is integrated into everyday life, becomes invisible and a kind of normativity 
elbows in with the consequence that emotion disappears from the majority of research 
frameworks referring to domestication theory. Second, one cannot understand the 
persistence of technological determinism if one does not analyze technologies in their 
complex reality, which is constituted not only by rationality and science, but also by 
emotions (Haddon et al. [32], Vincent and Fortunati [64]), symbols, metaphors, myths 
(Mumford [51], [52]; Mosco [49]; Fortunati [13]; Fortunati, Katz, Riccini, [22]) and 
narrativity (Carey [4]; Silverstone [60]). If one shares Susan Langer’s idea that 
technique is a way to handle problems, one should conclude that technologies have an 
emotional technique. 
However in the quantitative research projects I have done with other colleagues both 
in Italy and in Europe, it emerges that fear and wonder have disappeared from the 
emotional repertoires which people activate in their relationship with technologies. This 
relationship is changed and transformed from an episodic and extraordinary event to the 
spread of ownership of these devices and practices of use based on the daily routine of 
an increasing number of people. One can distinguish two emotional dimensions –one is 
satisfaction, made up by emotions such as interest, joy, relaxation, amusement, 
satisfaction, curiosity, enthusiasm and surprise, and the other is dissatisfaction, made up 
by emotions such as indifference, irritation, boredom, anger, frustration, anxiety/stress, 
unpleasantness, embarrassment (Fortunati [17]). So, the emotional repertoires can by 
now be conceptualized in term of an assessment of consumption. Clearly the 
domestication of ICTs has been the process which has also accompanied the 
disenchantment for the technologies of information and communication, which has 
occurred because of standardized operativity in the domestic setting. 
In effect Kopytoff’s approach and the metaphor of domestication proposed by 
Silverstone have captured the meaning of the process of “technological 
naturalization” that has taken place with the ICT. As Contarello et al. [8] have 
showed, in a research on social representations of the human body and the ICTs 
carried out at international level (including countries such Italy, Netherlands, Spain, 
Romania and Russia), the fact that the technologies of information and 
communication get closer and penetrate the human body is interpreted by respondents 
as a process of naturalization of technologies and thus “a naturalization of what is 
artificial, rather than the other way around”.  So, far from a rhetoric that makes the 
ICTs appearing as a ‘deus-ex-machina’, this research stresses instead the necessity to 
see them as imitating the human body, which is the model that has inspired all 
machines and technologies.  The results of this  research do not surprise since the 
most usual examples of ontological metaphors are those in which physical objects are 
considered as if they were persons (Lakoff and Johnson [40]: 53-54). The 
personification of objects allows humans to understand a wide range of experiences 
with non-human entities in terms of human motivations, characteristics and activities. 
So, this behaviour describes the fact that many people are able to  develop a meaning 
of technological artefacts only by conceptualising them in human terms. In the 
particular case of the domestication theory, the shift is even more complex, because it 
happens through an intermediate stage which passes by means of a metaphor with 
wild species.  

 
Theories without Heart 
9 
3   Domestication Theory and the Coming to Light of Studies on 
Emotion and ICT 
Domestication theory, which is one of the theories widely drawn upon to understand 
and describe the use and the integration of ICTs in everyday life (Silverstone and 
Hirsch [58], Silverstone and Haddon [59]), is also the one which has allowed and 
encouraged in a second stage the coming to light of a series of studies on the 
emotional relationship with information and communication technologies. For this 
reason I will focus here on domestication theory, trying to expose its main tenets and 
to show through which paths and theoretical cross-fertilization this theory has created 
a fertile terrain to develop a discourse and an empirical tradition of research on 
emotion and ICT.   
Domestication, argues Roger Silverstone ([61]: 232), has to be seen as a 
consumption process in which consumption is framed as “linked to invention and 
design, and to the public framing of technologies as symbolic objects of value and 
desire”. The inspirers were Jean Baudrillard [1], Michel de Certeau [10] and Daniel 
Miller [48], who at the end of 1980’s were all observing that the classical boundary 
between production and consumption was blurring. The world was changing under 
the mobilization and struggles not only of the working class but also of other new 
social actors, such women and youth. The hierarchical structures and the strategies of 
separation among the old economic and social structures were losing their strength. 
Their studies showed that in the consumption process commodities are subject to an 
intense activity of attribution and the transformation of particular meanings as well of 
symbolic and affective production. The domestic sphere emerged from their studies 
not as the place where the command scheme embodied in the commodity was merely 
executed, but as the place where the subjectivity and agency of consumers/users 
reinterpreted the commodity, often in an unexpected way. Of course the space of 
rebellion or at least of non-cooperation with the logics of the capitalistic process in 
the consumption sphere remained of limited influence until the moment in which 
capital discovered that not only workers but also consumers could be an independent 
variable in the process. The recognition by these authors of the consumption sphere as 
a productive sphere in an economic sense and also as a sphere where struggles, 
conflicts and autonomous determination by a multitude of actors take place has been 
of a great importance.  
The merit of domestication theory is that it has linked this discourse about 
consumption to several other strands such as the innovation theory (Rogers [56]), 
showing in a certain sense that the diffusion and process is not linear as it was 
imagined by Rogers. Another strand which domestication theory included under its 
umbrella is the question of the design of technology, which should consequently be 
seen as an on-going activity which does not stop at the factory walls, but which 
continues in the sphere of everyday life and which should consequently be seen as an 
activity which also involves consumers/users (Schuler and Namioka [57]).  These two 
issues – innovation and design – have been developed considerably over the past 
years, reflecting a large amount of empirical research and debate. Users/consumers 
have been conceptualised as e-actors including in this notion the complexity of the 
rich debate on the role and agency of buyers/users/consumers/stakeholders (Fortunati 
et al [24], Gebhardt et al [27]). Further strands that in principle can be linked to this 

10 
L. Fortunati 
theory include that of Goffman’s tradition of the public frame [29] and that of social 
psychologist tradition of social thinking which has been explored in the framework of 
research on social representations (Moscovici [50]).  
This latter framework of research on social representations has constituted the 
theoretical background of a series of studies on the new media that have been carried 
out in the last fifteen years (Fortunati and Manganelli [23], Contarello and Fortunati 
[7]). This group of studies is important, as, I will discuss below, they add an important 
dimension regarding the integration of technologies of communication and 
information into the household context: the dimension of the integration of new 
technologies at the socio-cognitive level, in the system of social thought as developed 
in the public sphere. In these studies the process of domestication is investigated by 
exploring how these technologies are socially elaborated and shared with the purpose 
of constructing a common understanding of them. To better understand the role of  
new media, Joachin Höfflich [35] has recently proposed combining the powerful 
notion of ‘frame’ with that other powerful notion of ‘social representation’. A final 
connection can be made again with the analysis of Baudrillard regarding the system of 
objects [2] and their symbolic and cultural value. 
In practice, domestication theory describes the adoption and use of ICTs in four 
dimensions: the first is appropriation, the second objectivation, the third is integration  
and the fourth is conversion. Appropriation is the process that involves human agency 
and which describes the interaction between the human and the technological in a 
constant dialectic of change. Objectivation is constituted by tactics of placing the new 
technology inside the domestic sphere, by reorganizing the house space and 
restructuring the micro-politics of gender and generation relationships and the 
command over the domestic space. Integration is the process of injecting the practices 
of using the new media into the rhythms and pauses of domestic life, inside and 
outside the formal boundaries of the household. In practice, time management has 
been found to be in Europe one of the three main reasons for using the ICTs 
(Fortunati and Manganelli [21]). Conversion, which implies recognition, is made up 
of “the perpetuation of the helix of the design-domestication interface” (Silverstone 
[61]: 234) and involves display and the development of skills, competences and 
literacies. This last element of domestication has also results in many studies and 
research  (Chiaro and Fortunati [5]: Williams, Stewart, Slack [65]). The late 
Silverstone stressed that consumption includes five dimensions: commodification, 
appropriation, objectivation, integration and conversion. These different dimensions 
are often confused and they are somehow inspired by the two stages of the social 
integration at cognitive level depicted by Moscovici [50] in his theory on social 
representation. These two stages are anchorage and objectivisation: the former 
enables people to incorporate something with which they are not familiar into their 
network of categories, integrating it cognitively in the pre-existing system of thought. 
Objectivisation (whose name recalls the second element of domestication)  helps to 
make something abstract concrete, to give ideas a material consistency and to embody 
conceptual schemes. By the way, these respective stages have the purpose of 
describing the conceptual and social integration of technologies in everyday life. 
While domestication is sensitive to the material and immaterial part of the integration 
process, understood as practical behaviour, but also as attitudes, opinions and 

 
Theories without Heart 
11 
values/symbols, the social representation approach is developed mainly on the 
cognitive side.  
In both theories, domestication and social representations, however, although 
emotion plays an important role, this role was not explicitly discussed, at least in a 
first stage of research in this field. And yet almost all human cognition depends on 
and uses concretes structures such as the sense-motor system and emotion (Lakoff 
and Johnson [40]). This initial ignoring of the emotion role in the domestication 
process is even more surprising since the domestication metaphor develops, as I 
showed above, from the starting point of the conceptualisation of the new 
technologies as being like wild animals and so implicitly it include a noticeable 
emotional tension inside it. This indifference towards emotion however was already 
overcome in the early 1990’s when domestication theory was cross-fertilized by 
domesticity theory (Fortunati [12]). It was through the contribution coming from 
domesticity theory that domestication theory found the way to develop further its 
approach to consumption, including the immaterial part of it, which is constituted by 
emotion, affection, communication, information and so on. In effect the approach 
formulated by domestication theory to consumption was in part new and in part old. It 
was new since this theory understood the consumption sphere as a production sphere, 
it was old because for the first period of research it did not try to understand what this 
could imply. The questions which have been avoided in this phase were: production 
of what? By whom? For which purposes? With what effects? Only when the analysis 
on the social functioning of the labour-force reproduction has been made merge with 
domestication theory (Fortunati [19], [20]) has it been possible to answer all these 
question in an appropriate way. Fortunati’s analysis showed that the consumption 
process should be seen as part of the process of reproducing the labour force which 
takes place on the domestic sphere and which has a specific worker: women. The 
cross-fertilization between domestication and domesticity theory allowed us to 
understand that production inside consumption should be seen as the production of 
value, of surplus value, and that this process is the main and productive process of the 
whole economic system (Fortunati [15], [16]; Hochschild [34]). In this framework, 
emotion represents a substantive part of the immaterial labour carried out in the 
reproductive process and technology represents a tool to make women intensify their 
labour. Studying the short-circuit of the emotional relationship with ICTs and the 
social role of electronic emotion allowed the blossoming of a series of studies on this 
topic (Vincent [63]; Lasen [41]). The specific role of emotion has been implemented 
in several studies that have been carried out later both at quantitative level and 
qualitative level and which now constitute an important strand of domestication 
studies (Vincent and Fortunati [64]). 
4   Emotions as Infrastructure of Body-to-Body Communication 
On top of this cross-fertilization between domestication and domesticity theories, 
another original approach to the understanding of emotion in the communicative 
process is that proposed by Star and Bowker [62]. These two scholars define emotion 
and the human body as components of the infrastructure in face-to-face, or even better 
body-to-body,  communication (Fortunati [14]).  Their approach allows us to capture 

12 
L. Fortunati 
better the role of emotion in accompanying the process of communication in co-
presence, but also in accompanying the practices of using information and 
communication technologies as well as the transformation of social organizations in 
which these technologies are used.  
What transformation of emotion do we experience as an infrastructure of body-to-
body communication when the communication process is mediated? For 
understanding and depicting this transformation I refer to a classical concept which 
was introduced by Marx [47] in the Economic and Philosophical Manuscripts of 
1844, and which is ‘alienation’. Many intellectuals and scholars of 20th century have 
reflected on the issue of alienation and offered valid contributions. Among them I cite 
here, for example, Gorz [30] who stated that the loss of control by individuals of their 
selves has meant the ever-growing loss of control of their needs, desires, thoughts and 
the image that they have of their selves. I argue that in the 20th century alienation has 
also involved the communication sphere, where by means of the technologies of 
information and communication the separation of the body from the communication 
of emotion, words and non-verbal signals has been produced. The mediation of an 
artificial transformer (such as the mobile phone and the internet) has in fact 
strengthened the communication process but at the same time has provoked 
structurally a separation between the body and the personal and social capacity of 
individuals to communicate verbally or by written and also visual representations.  
This aspect of alienation is inevitable since the development of the capital system 
has implied the rupture of the unit between mind and body and consequently a 
separate development of the mind from the body. This separation means that the mind 
has more chances than the body to be protagonist in the communication process. As 
Manovich [45] argues, the main point of tele-presence is not in individuals’ presence 
through ICTs, but their absence through ICTs (anti-presence). In fact it is no more 
necessary to be physically present in a certain place to affect the surrounding reality. 
As consequence, the distance between the subject and the object or another subject 
becomes the crucial point, since it is the distance that shapes perception. The tele-
absence of the body confines it not only to a kind of secondary role in mediated 
communication but also to a condition of being a minority. Its affordances, needs and 
desires are mainly ignored. With the advent of the tele-absence of the body, the 
physical and emotional infrastructures of the communicative process become 
separated and have a different destiny. While emotions, for their specific essence of 
inner energy which simultaneously implicate cognition, affect, evaluation, motivation 
(Illouz [36]), adapt themselves to mediated communication in various ways, the 
physical infrastructure of the body expresses more inertia towards the change in the 
communication sphere. In fact, the separation of emotions from the body leads them 
to a better destiny, since this separation does not automatically imply that individuals 
are destined to live emotion in a way that is worse. On the contrary, the destiny of the 
body with mediated communication is that to be ignored in its potency and 
peculiarities and to be treated as in absentia. The body in computer mediated 
communication is expected to be steady, sit down on a chair. See all the health 
problems affect the body and especially some parts of it such as the neck, the arm, the 
wrist and so on that have pain. The role of the body is less sacrificed, of course, in 
mobile communication in which it can move. However, in both cases, the script of the 
body is in fact reduced to micro-gestures, often to wrong postures, to the use of only 

 
Theories without Heart 
13 
two senses, mainly sight and hearing, and also the voice. This limited use of the body 
cannot help but also distort seriously  the communication of emotions, which, 
although they are linked to social and cultural contexts and shared norms, remain 
“body matters” (Frijda [26]). Here a question is inevitable: to what extent are 
emotions sacrificed in their separation from the body? When the body is separated 
from the communication in a certain sense a sort of alienation of emotion is produced, 
since these simultaneously implicate cognition, affect, evaluation, motivation and the 
body. As consequence, conceding a growing share of our own communicative activity 
to mediated communication might involve risks in terms of psychophysical well-
being (Contarello [6]). The problem is that on many occasions one cannot choose the 
best channel but rather use the channel that one has at one’s disposal to communicate 
and that often ICTs are precious tools to overcome spatial, temporal and economic 
limitations, but also to explore other dimensions of oneself. The definition of 
electronic emotion advanced by Fortunati and Vincent [25] might help to figure out 
what these dimensions are. For them an electronic emotion is: “an emotion felt, 
narrated or showed, which is produced or consumed, for example in a telephone or 
mobile phone conversation, in a film or a TV program or in a website, in other words 
mediated by a computational electronic device. Electronic emotions are emotions 
lived, re-lived or discovered through machines. Through ICTs, emotions are on one 
hand amplified, shaped, stereotyped, re-invented and on the other sacrificed, because 
they must submit themselves to the technological limits and languages of a machine. 
Mediated emotions are emotions which are expressed at a distance from the 
interlocutor or the broadcaster, and which consequently take place during the break up 
of the unitary process which usually provides the formation of attitudes and which 
consists of cognition, emotion and behaviour”. 
A specific study on the social representation of the human body and ICTs has 
investigated the relationship between the human body and new technologies (namely 
the mobile phone and the computer/internet) with the purpose of understanding how 
this relationship is socially conceptualized in this period that has been named ‘mass 
prosthetization’. The results indicate that social thought still sees a clear opposition 
between the human body and the new media (Contarello and Fortunati [7]), which 
seem to be destined to a divergent development. For respondents, however, the 
importance of the body remains central, although in absentia. Its centrality emerges in 
an oblique but constant way in many studies carried out on the use of 
telecommunications. But here I argue that the importance of the body resonates also 
from the importance that respondents attribute to the dimension of convenience. 
Elsewhere I argued that convenience might be considered as a major need and at the 
same time a true principle which governs the use of ICT. In the majority of 
quantitative and qualitative studies carried out in these two decades on the 
technologies of information and communication, convenience was a recurrent 
motivation to use these tools (Fortunati [19], [20]; Chiaro and Fortunati [5]). But what 
is there behind convenience if not the reasons of the body, that is, the concern to 
avoid useless efforts and fatigue caused to it? A concern regarding bodily efficiency 
and health? The notion of convenience in the use of ICTs represents the application of 
Occam’s Law. The convenience law, which is the systematic behavior to save energy 
and avoid causing fatigue to the body through the use of these technologies, is 
connected with the reasons of the body, which in the context of the decision making 

14 
L. Fortunati 
process regarding the question of whether to use a device or to resort to body-to-body 
communication, which device to use and how much to use the device in question 
becomes a priority. This law is particularly evident in the research carried out in the 
period of the diffusion and appropriation of these devices, that is for the mobile phone 
and the internet the second part of 1990’s. This law puts in motion, however, a chain 
of contradictions in the sense that the body, which in principle inspires actions and 
strategies to save it from fatigue and efforts, often ends up, in reality, being sacrificed. 
5   Conclusion 
By presenting and discussing the example of domestication theory I tried to show 
that: 1) domestication theory is powerful because it is highly metaphorical and so 
emotional; 2) the cross-fertilization of this theory with domesticity theory has allowed 
the development of a variety of research studies both qualitative and quantitative on 
the emotional integration of ICTs in everyday life. Furthermore, by presenting and 
discussing the notion of emotion as an infrastructure of body-to-body communication, 
I tried to show to what extent this perspective proposed by Star and Bowker [62] is 
seminal in detecting what happens to emotion in mediated communication. 
Body-to-body and mediated communication represent broad fields which 
increasingly require multidisciplinary approaches and challenges traditional methods 
of research. The problem is that it is not easy to merge different traditions of 
investigation, theories and methodologies. Recently there have been some attempts to 
merge inside the same strand different sociological approaches – namely the 
Sociology of Technology and Science, Communication studies, Mass Media studies, 
ICT Users’ studies (Boczkowski and Lievrouw [3]; Lievrouw and Livingstone [42]; 
Fortunati [18]). And this paper represents another attempt to cross-fertilize the 
tradition of ICT user studies with the field of cross-modal analysis of Verbal and 
Nonverbal Communication. Only the future will show if these operations will open a 
fruitful dialogue among different disciplines. However, it is a matter of fact that it is 
really necessary to merge different traditions in order to mutually correct 
inconsistencies and errors. 
From this analysis has emerged the necessity to strengthen the investigation of 
electronic emotion both at a theoretical and an empirical level and to develop further 
this cross-fertilization between these two fields of research, which until yesterday 
were very disconnected. Theories with heart are needed in order to understand 
properly processes so complex as body-to-body and mediated communication. 
References 
1. Baudrillard, J.: Selected Writings: Jean Baudrillard. Poster, M. (ed.). Polity Press, 
Cambridge (1988) 
2. Baudrillard, J.: The system of objects. Verso, London (2005) 
3. Boczkowski, P., Lievrouw, L.A.: Bridging STS and Communication Studies: Research on 
Media and Information Technologies. In: Hackett, E.J., Amsterdamska, O., Lynch, M., 
Wajcman, J. (eds.) New Handbook of Science and Technologies Studies. MIT Press, 
Cambridge (2008) 

 
Theories without Heart 
15 
4. Carey, J.W.: Media, Myths, and Narratives. In: Television and the Press. Sage, London 
(1988) 
5. Chiaro, M., Fortunati, L.: Nouvelles technologies et compétences des usagers. 
Réseaux 17(96), 147–182 (1999) 
6. Contarello, A.: Body to Body: Copresence in Communication. In: Fortunati, L., Katz, J., 
Riccini, R. (eds.) Mediating the Human Body: Technology, Communication and Fashion, 
pp. 123–133. Erlbaum, Mahwah (2003) 
7. Contarello, A., Fortunati, L.: ICTs and The Human Body. A Social Representation 
Approach. In: Law, P., Fortunati, L., Yang, S. (eds.) New Technologies in Global 
Societies, pp. 51–74. World Scientific Publisher, Singapore (2006) 
8. Contarello, A., Fortunati, L., Gomez Fernandez, P., Mante-Meijer, E., Versinskaya, O., 
Volovici, D.: ICTs and the human body: An empirical study in five countries. In: Loos, E., 
Haddon, L., Mante-Meijer, E. (eds.) The Social Dynamics of Information and 
Communication Technology, pp. 25–38. Ashgate, Aldershot (2008) 
9. Damasio, A.: Descartes’ Error. In: Emotion, Reason, and the Human Brain. Avon Books, 
New York (1994) 
10. De Certeau, M.: The Practice of Everyday Life. California University Press, Berkeley 
(1984) 
11. Eco, U.: Trattato di semiotica generale. Bompiani, Milano (1975) 
12. Fortunati, L.: L’arcano della riproduzione. Marsilio, Venezia (1981) (English Tr.: The 
Arcane of Reproduction. Autonomedia, New York, 1995)  
13. Fortunati, L.: Real People, Artificial Bodies. In: Fortunati, L., Katz, J.E., Riccini, R. (eds.) 
Mediating the Human Body: Technology, Communication and Fashion, pp. 61–74. 
Erlbaum, Mahwah (2003) 
14. Fortunati, L.: Is Body-to-body communication still the prototype? The Information 
Society 21(1), 1–9 (2005) 
15. Fortunati, L.: User Design and the Democratization of the Mobile Phone. First Monday 7 
(2006) 
16. Fortunati, L.: Immaterial Labor. and its Machinization, Ephemera. Theory & Politics in 
Organization 7(1), 139–157 (2007) 
17. Fortunati, L.: Old and New Media, Old Emotions. In: Vincent, J., Fortunati, L. (eds.) 
Electronic Emotion. The Mediation of Emotion via Information and Communication 
Technologies. Peter Lang, Oxford (2009) 
18. Fortunati, L.: A Discourse around Theories on New Media (forthcoming) 
19. Fortunati, L. (ed.): Gli italiani al telefono. Angeli, Milano (1995) 
20. Fortunati, L. (ed.): Telecomunicando in Europa. Angeli, Milano (1998) 
21. Fortunati, L., Manganelli, A.: La comunicazione tecnologica: Comportamenti, opinioni ed 
emozioni degli Europei. In: Fortunati, L. (ed.) Telecomunicando in Europa. Angeli, 
Milano (1998) 
22. Fortunati, L., Katz, J.E., Riccini, R. (eds.): Mediating the Human Body: Technology, 
Communication and Fashion. Erlbaum, Mahwah (2003) 
23. Fortunati, L., Manganelli, A.: The social representations of Telecommunications. Personal 
and Ubiquitous Computing 12(6), 421–431 (2008) 
24. Fortunati, L., Vincent, J., Gebhardt, J., Petrovčič, A. (eds.): Interaction in Broadband 
Society. Peter Lang, Berlin (2009) 
25. Fortunati, L., Vincent, J.: Introduction. In: Vincent, J., Fortunati, L. (eds.) Electronic 
Emotion. The Mediation of Emotion via Information and Communication Technologies. 
Peter Lang, Oxford (2009) 
26. Frijda, N.H.: The Emotions. Cambridge University Press, Cambridge (1986) 

16 
L. Fortunati 
27. Gebhardt, J., Greif, H., Raycheva, L., Lobet-Maris, C., Lasen, A. (eds.): Experiencing 
Broadband Society. Peter Lang, Berlin (2009) 
28. Gibbs, R.W. (ed.): The Cambridge Handbook of Metaphor and Thought. C.U.P., 
Cambridge (2008) 
29. Goffman, E.: Frame Analysis: An essay on the organization of experience. Northeastern 
University Press, Boston (1986) 
30. Gorz, A.: Ecologica. Jaka Book, Milano (2008) 
31. Haddon, L., Silverstone, R.: Information and Communication Technologies and the Young 
Elderly, SPRU/CICT report series no.13. University of Sussex, Sussex (1996) 
32. Haddon, L., Mante, E., Sapio, B., Kommonen, K.-H., Fortunati, L., Kant, A. (eds.): 
Everyday Innovators. Researching the Role of Users in Shaping ICTs. Springer, Dordrect 
(2005) 
33. Haddon, L.: Empirical Studies Using the Domestication Framework. In: Berker, T., 
Hartmann, M., Punie, Y., Ward, K.J. (eds.) Domestication of Media and Technology, pp. 
103–122. Open University Press, Maidenhead (2006) 
34. Hochschild, A.R.: Emotion Work, Feeling Rules, and Social Structure. American Journal 
of Sociology (85), 551–575 (1979) 
35. Höfflich, J.: Reinventing the Old? New Communication Technologies and Changing 
Practices of Communication. Paper presented at the conference The Role of New 
Technologies in Global Society, Hong Kong, July 30-31 (2008) 
36. Illouz, E.: Cold Intimacies. In: The Making of Emotional Capitalism. Polity Press, 
Cambridge (2007) 
37. Jung, C.G. (ed.): The Man and his symbols. Doubleday, New York (1964) 
38. Katz, J. (ed.): Machines that Become us. Transaction, New Brunswick, New Jersey (2003) 
39. Kopytoff, I.: The cultural biography of things: commoditization as a process. In: 
Appadurai, A. (ed.) The Social Life of Things: Commodities in Cultural Perspective. 
Cambridge University Press, Cambridge (1986) 
40. Lakoff, G., Johnson, M.: Metaphors We Live By. University of Chicago Press, Chicago 
(1980) 
41. Lasen, A.: Affective Technologies: Emotion and Mobile Phone. Receiver. Vodaphone 
(2004) 
42. Lievrouw, L.A., Livingstone, S. (eds.): The Handbook of New Media. Sage, London 
(2002) (new edition, 2006) 
43. Ling, R.: New Tech, New Ties: How Mobile Communication is Reshaping Social 
Cohesion. The MIT Press, Cambridge (2008) 
44. Maldonado, T.: Critica della ragione informatica. Feltrinelli, Milano (1997) 
45. Manovich, L.: The Language of New Media. The MIT Press, Cambridge (2001) 
46. Marchese, A.: Dizionario di retorica e stilistica. Mondatori, Milano (1992) 
47. Marx, K.: Economic and Philosophical Manuscripts of 1844. Prometheus Books, Buffalo 
(1988) 
48. Miller, D.: Material Culture and Mass Consumption. Blackwell, Oxford (1987) 
49. Mosco, V.: The Digital Sublime. Myth, Power, and Cyberspace. The MIT Press, 
Cambridge (2004)  
50. Moscovici, S.: La psychanalyse. Son Image et son Public. PUF, Paris (1961/1976) 
51. Mumford, L.: The Myth of the Machine. Technics and Human Development, vol. I. 
Harcourt, New York (1967) 
52. Mumford, L.: The Myth of the Machine. The Pentagon of Power, vol. II. Harcourt, New 
York (1970) 

 
Theories without Heart 
17 
53. Oudshoorn, N., Pinch, T.: How Users Matter. In: The Co-Construction of Users and 
Technologies. The MIT Press, Cambridge (2003) 
54. Oudshoorn, N., Pinch, T.: User-Technology Relationships: Some Recent Developments. 
In: Hackett, E.J., Amsterdamska, O., Lynch, M., Wajcman, J. (eds.) New Handbook of 
Science and Technologies Studies. MIT Press, Cambridge (2008) 
55. Rheingold, H.: Tools for Thought. The MIT Press, Cambridge (2000) 
56. Rogers, E.M.: Diffusion of Innovations. Free Press, New York (1993) 
57. Schuler, D., Namioka, A.: Participatory design: Principles and practices. Erlbaum, 
Hillsdale (1993) 
58. Silverstone, R., Hirsch, E.: Consuming Technologies: Media and Information in Domestic 
Space. Routledge, London (1992) 
59. Silverstone, R., Haddon, L.: Design and the Domestication of Information and 
Communication Technologies: Technical Change and Everyday Life. In: Mansell, R., 
Silverstone, R. (eds.) Communication by Design. The Politics of Information and 
Communication Technologies. Oxford University Press, Oxford (1996) 
60. Silverstone, R.: Television Myth and Culture. In: Carey, J.W. (ed.) Myth, Media and 
Narratives. Sage, London (1988) 
61. Silverstone, R.: Reflections on the Life of a Concept. In: Berker, T., Hartmann, M., Punie, 
Y., Ward, K.J. (eds.) Domestication of Media and Technology. Open University Press, 
Maidenhead (2006) 
62. Star, S.L., Bowker, G.C.: The infrastructure of the new media. In: Lievrouw, L.A., 
Livingstone, S. (eds.) The Handbook of New Media. Sage, London (2002) 
63. Vincent, J.: Emotions and Mobile Phones. In: Nyiri, K. (ed.) Mobile Democracy. Essays 
On Society, Self and Politics. Passagen Verlag, Vienna (2003) 
64. Vincent, J., Fortunati, L. (eds.): Electronic Emotion. The Mediation of Emotion via 
Information and Communication Technologies. Peter Lang, Oxford (2009) 
65. Williams, R., Stewart, J., Slack, R.: Social Learning in Technological. Innovation. Edward 
Elgar, Cheltenham (2005) 
66. Wyatt, S.: Technological Determinism is Dead; Long Life Technological Determinism. In: 
Hackett, E.J., Amsterdamska, O., Lynch, M., Wajcman, J. (eds.) New Handbook of 
Science and Technologies Studies. MIT Press, Cambridge (2008) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 18–27, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Prosodic Characteristics and Emotional Meanings of 
Slovak Hot-Spot Words 
Štefan Beňuš1,2 and Milan Rusko2 
1 Constantine the Philosopher University, Štefánikova 67, 94974 Nitra, Slovakia 
2 Institute of Informatics of the Slovak Academy of Sciences, Dúbravská cesta 9,  
845 07 Bratislava, Slovakia 
sbenus@ukf.sk, milan.rusko@savba.sk 
Abstract. In this paper, we investigate emotionally charged hot-spot jVj-words 
from a corpus that is based on recordings of puppet plays in Slovak. The poten-
tial of these hot-spot words for detecting emotion in larger utterances was 
tested. More specifically, we tested the effect of prosodic and voice quality 
characteristics and the presence or absence of lexical context on the perception 
of emotions that the jVj-words convey. We found that the lexical cues present in 
the context are better predictors for the perception of emotions than the pro-
sodic and voice quality features in the jVj-words themselves. Nevertheless, both 
prosodic as well as voice quality features are useful and complementary in de-
tecting emotion of individual words from the speech signal as well as of larger 
utterances. Finally, we argue that a corpus based on recordings of puppet plays 
presents a novel and advantageous approach to the collection of data for emo-
tional speech research. 
Keywords: Hot-spot words, emotional speech, prosody. 
1   Introduction 
A comprehensive model of communication should incorporate both visual and auditory 
characteristics. One of the goals of such a cross-modal model is to reflect the key char-
acteristics of human-human interactions, and use this knowledge for improving the 
naturalness and effectiveness of applications based on human-machine communication 
such as spoken dialogue systems. One partial aspect of such enterprise is better under-
standing of cues signaling the internal emotional state of a speaker. These cues, in turn, 
could then be used in the recognition and synthesis of emotionally varied speech.  
Extracting information about the emotional state of the speaker may be facilitated 
when so called hot-spot segments are taken into consideration [1]. The presence and 
acoustic characteristics of these segments may facilitate the detection of high involve-
ment in meetings and thus point to important information for automatic retrieval. In the 
stretches of conversation from 30 to 60 seconds, [1] showed that humans are able to 
discriminate between involved and non-involved speech, and, that automatically ex-
tracted acoustic and prosodic features together with machine learning techniques are 
able to perform this discrimination automatically. 

 
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot Words 
19 
In addition to longer stretches of speech, individual words may also be considered 
as hot-spots. For example, [2] showed that the prosodic features of individual emo-
tionally ambiguous hot-spot words such as whatever can signal emotional valence of 
the speaker, that is, if the speaker’s emotions are neutral or negative. Additionally, it 
has long been known that the meaning of words with seemingly clear lexical polarity 
such as yes, no, yeah, sure, no way, etc. can be changed or made ambiguous depend-
ing on their prosodic characteristics. 
In short, we believe that even single words can provide strong indicators for the 
emotional state of the speaker or his/her attitudes toward the propositions expressed in 
communicative interactions. In this paper, we investigate the influence of prosodic 
characteristics, voice quality features, and lexical context, on the perception of emo-
tions in a particular group of Slovak hot-spot words that we call jVj-words.  
There are four core members in the group of jVj-words: [jej], [jaj], [joj], and [juj]. 
Significant variation occurs in the duration of the vowels as well as the degree of redu-
plication. We hypothesize that the presence and the acoustic and prosodic characteris-
tics of hot-spot words may cue the emotional characteristics of larger stretches of 
speech such as utterances or speaker turns. If emotion detection of a speaker utterance 
can be facilitated by using automatically extractable cues (such as acoustic ones) from 
a single target word, this understanding may help to increase the effectiveness of 
automated cross-modal dialogue systems. Additionally, we want to improve our under-
standing of the role of lexical context for the emotional perception of hot-spot words. 
2   Prosody and Emotions 
In order to investigate the relationship between prosody and emotions, both concepts 
should be anchored within a theoretical framework [3]. It should be made clear at the 
start that prosody and emotions are not necessarily linked: there are many linguistic or 
paralinguistic meanings signaled with prosody that can be claimed to be emotionally 
empty or neutral. On the other hand, emotions can be expressed visually, and could be 
reliably detected from facial expressions only [4].  
Moreover, both emotions and prosody can be described with continuous features as 
well as discrete labels produced by human annotators that should to a certain extent be 
derivable from measurable features of the signal. Continuum of emotions can be dis-
cretized into basic emotions [4] and their combinations, but it can also be characterized 
using continuous scales [6]. Prosody or voice quality can also be described with con-
tinuous, measurable, and thus more objective and automatically extractable features, 
such as pitch range or jitter. Nevertheless, the perception of prosody by humans might 
not be linearly related to the physical properties of the produced signal, and thus dis-
crete categories labeled by humans with reasonable agreement, such as the ones used 
in the ToBI framework [7], can also provide a useful description of data. 
One of the promising frameworks for studying the relationship between prosody and 
emotions is to define emotions on a two-dimensional space of activation and valence, 
and then look for acoustic correlates of that space, e.g. in [8].1 Referring to basic emo-
tions, ANGER or JOY have high activation values while SADNESS has low values. [10] 
                                                           
1 The term activation is sometimes replaced by arousal. Also, some researchers suggest a third 
dimension for emotional description: power [9]. 

20 
Š. Beňuš and M. Rusko 
found that activation correlates well with continuous features such as pitch range and 
energy that can be automatically extracted from the signal. Emotional valence marks 
the continuum between positive and negative emotions. For example, ANGER and 
FEAR are negative emotions while HAPPINESS or JOY are positive ones. [8] argued that 
discrete features characterizing the type of intonational contour (using the ToBI 
framework of prosody description, [7]) may signal emotional valence: plateau F0 con-
tour correlated with negative emotions while falling contours with positive ones. How-
ever, [10] found little evidence for the hypothesis that qualitatively different contours 
signal different emotions. Finally, several studies suggested that emotional valence 
might be cued by voice quality characteristics [11], [12].  
Another crucial issue is the methodological approach concerning the collection of 
the data for emotion-prosody research. Designing a corpus suitable for the experimen-
tal investigation of emotional speech leads to significant challenges [12]. Natural 
speech recorded in emotionally loaded situations provides seemingly ideal data be-
cause the emotional involvement of speakers correlates well with physiological and 
acoustic changes of the signal. However, [12] also notes that data obtained from natu-
ral speech are not robust enough (few speakers and/or short utterances), typically have 
poor signal quality, and the determination of the underlying emotion may not be 
straightforward. Speech, in which emotions are elicited (portrayed) by actors produc-
ing semantically neutral utterances such as series of numbers, has been a dominant 
paradigm for data collection in this area in recent years. This approach allows for a 
rich source of emotions and the control of other variables such as linguistic context. 
Finally, the emotions produced in this way tend to be recognized by judges with high 
degree of reliability. 
In our approach we concentrate on the potential of continuous acoustic and voice-
quality features for the perception of discrete emotional categories. Data for our re-
search come from recordings of a puppeteer, which, in our minds, represents a useful 
combination of advantages of both natural speech and speech elicited for the purpose 
of portraying emotions. Our corpus is described in more detail in the following section.  
3   Corpus of Hot-Spot Words 
The data for our study come from a recording of a play acted by a single puppet-
player [13]2. There are several advantages to using this data for research in emotional 
speech. First, this corpus is ‘emotionally loaded’ since the visual cues to emotions are 
significantly limited for characters in a puppet play. This allows for a natural control-
ling effect of some potentially confounding factors such as facial expression. Second, 
the performance is directed to young audience, which requires a clear presence, some-
times even an exaggeration, of emotional expressions. [14] showed that although 
acted emotions are more exaggerated than real-life ones, the relationship of these two 
types of emotions to acoustic correlates is not contradictory. Finally, the emotions in 
our corpus are expressed within a clear context of the play. We believe that this 
                                                           
2 Several recordings were made, some with the puppets and the audience, some with the pup-
pets and without the audience, and some without the puppets or the audience. An auditory 
analysis of the speech in the three conditions did not reveal any salient differences; hence, we 
used the third type due to the best quality of the acoustic signal.  

 
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot Words 
21 
makes our corpus more natural than the corpora based on professional actors who act 
emotions on semantically neutral utterances such as series of numbers. On the other 
hand, the presence of the script and the lexical choice of words and expressions 
clearly participate in conveying the emotions, which allows us to investigate the role 
of prosody and lexical context in disambiguating the emotional meanings of individ-
ual words. 
Our earlier research of non-verbal vocal gestures in puppet plays [15] showed that 
words of the [jVj] shape, where V represents one of four vowels {i, e, a, u}, express a 
rich variety of emotions and can be realized with multiple acoustic-prosodic charac-
teristics. Lexically, jVj-words are categorized as particles with similar meanings. For 
example, the Short Dictionary of Slovak [16] uses the same definition for jaj, joj, and 
juj: “used to express feeling of physical pain, or used in exclamations expressing 
various emotions such as dislike, sorrow, surprise, and others”. The same dictionary 
defines jej as “expressing surprise, astonishment”.3 From these definitions, it is clear 
that not only the words themselves are ambiguous, but that there is also a significant 
semantic overlap across the words. 
In terms of phonetic characteristics, all four jVj-words can contain a short or long 
vowel and can be reduplicated as jVjVj, jVjVjVj, etc. Vowel length is phonemically 
contrastive in Slovak, however, the difference between short and long vowels in jVj-
words does not present a clear minimal pair.4 Similarly, there is no established differ-
ence between jaj and reduplicated jajaj, or between joj and jojoj. Rather, it seems that 
these cases of re-iteration and lengthening fall under the notion of iconicity. More 
specifically, the principle of quantity – “more of form is more of content” [17] – ap-
plied to these particles predicts that longer vowels or more extensive reduplication 
should signal stronger emotions of surprise, despair, sorrow, etc. 
Finally, jVj-words can also be expressed in multiple voice quality modes because the 
puppeteer tries to distinguish among the characters in the play with the use of different 
voice settings such as pitch register, falsetto, modal voice, etc [13]. Hence, jVj-words in 
our corpus present variation on at least three levels: emotional meaning, prosodic char-
acteristic, and voice quality setting. Our corpus thus provides an interesting testing 
ground for studying the relationship between these levels. 
For this study we extracted all jVj-words from a single puppet play. There were 16 
jaj tokens, 3 jej tokens, 8 joj tokens, and 15 juj tokens. This provided us with 42  
tokens in total. 
4   Prosody and Context in Perceiving Emotions of jVj-Words  
4.1   Perception Experiment 
A perception experiment was designed to determine the effects of the form of the jVj-
words and the context in which they occur on the emotions that they convey. From our 
corpus we created three sets of stimuli, and asked the subjects to judge the presence of 
                                                           
3 The word [jej] has also another meaning that is in fact more frequent: personal and possessive 
pronoun ‘her’. 
4 However, several restrictions can be identified; for example long [e:] is required for pleasant 
surprise, or long [a:] for surprised acknowledgement.   

22 
Š. Beňuš and M. Rusko 
five basic emotions ANGER, JOY, SADNESS, FEAR, and SURPRISE in the stimuli. The first 
set of stimuli consisted of the sound files of the jVj-words only. The second set con-
tained the sound files and transcripts of the complete utterance from which the jVj-
word was extracted. The utterances were semantically and intonationally complete 
sentences but could only span a single speaker’s turn. The third stimuli set contained 
the transcripts of these utterances only. We will refer to the three sets as JVJ-ONLY, 
FULL, and TEXT respectively. 
We then asked the subjects to judge the presence of each of the five emotions in 
the stimuli on a scale of 0 (no emotion) to 3 (very strong emotion). Each rating was 
normalized by subject using z-scores normalization in order to account for the varia-
tion the subjects might have produced in the use of the scale. We then calculated the 
mean and standard deviation values for each token and emotion. Additionally, as a 
general measure of expressive load, we calculated the sum of the mean z-score values 
for each token and emotion (the higher the value, the greater the expressive load). We 
also calculated the sum of the standard deviations for each token and emotion, which 
is a measure of agreement among the subjects (the higher the value, the worse the 
agreement). 
In addition to collecting data on the emotional value of the jVj-words, we also ex-
tracted basic features describing their prosodic and voice quality characteristics. Pro-
sodic features included duration, minimum, maximum, and mean values of pitch and 
energy, and features capturing the F0 contour such as the F0 slope over the whole 
word, over the second half of the word, or the slope calculated from the first and the 
last pitch value after the stylization of F0 contour. Voice quality features included jit-
ter, shimmer, and harmonics-to-noise ratio. Using the software for the acoustic analysis 
of speech Praat [18] we designed a script that extracted these features automatically.   
Three non-overlapping groups of college students participated in the experiment. 
Each group judged the presence of emotions with different set of stimuli. In total, 
there were 53 judges; 20 for JVJ-ONLY, 20 for FULL, and 13 for TEXT condition. The 
sound files during the experiment in the first and second group were played through 
loudspeakers and each sound file was played twice. 
4.2   Observations, Results, and Discussion 
Expressive load. The results show that our subjects agreed more on the less expres-
sive tokens than on the more expressive ones. We found significant correlation be-
tween the general measures of expressive load and agreement in JVJ-ONLY; r(40) = 
0.52, p < 0.01, and FULL; r(40) = 0.37, p < 0.05.5 The correlation in the TEXT condi-
tion was positive but not significant. Hence, it seems that the presence of acoustic 
cues facilitates the identification of the absence rather than presence of emotions. 
Based on the values of the general measure of expressive load, jVj-words were per-
ceived on average as less emotionally loaded in JVJ-ONLY condition than in the other 
two conditions, but this difference was not significant. The difference could be ex-
plained by the brevity of the linguistic material on which the emotions could be 
                                                           
5 All correlation tests reported in this paper were Pearson’s test, and we report p values for two-
tailed tests of significance with an alpha level of 0.05. Adjustments for the alpha levels were 
not made. Since we had 42 tokens, degrees of freedom equals 40. 

 
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot Words 
23 
judged in this condition, or the presence of other emotionally loaded words in the con-
text that was present only in the FULL and TEXT conditions.  
Next, we looked at the differences among the jVj-words and the general measure of 
expressive load in the three experimental conditions. The mean values for expressive 
load for the four words and three conditions are illustrated in Fig. 1 below. An 
ANOVA test found no significant difference between the four words; neither when 
data from all conditions were pooled, nor within the conditions. Although the plot 
does not present any clear pattern, we see that jaj-words seem to cue the absence 
rather than presence of emotions, and juj-words are highly emotionally expressive but 
only in the full context condition. 
Relations among emotions. A rather surprising finding was that the ratings for 
emotions did not correlate significantly among themselves. A notable exception is 
JOY that was well defined with respect to ANGER, SADNESS, and FEAR in the FULL 
condition due to significant negative correlations; r(40) = -0.41, p = 0.007, r(40) = -
0.64, p < 0.001, and r(40) = -0.56, p < 0.001 respectively. However, we also observed 
a significant positive correlation between JOY and SURPRISE; in the JVJ-ONLY 
condition; r(40) = 0.5, p < 0.001. These observations support the view that basic 
emotional categories are rather uniform and homogeneous [12]. 
 
Fig. 1. Mean expressive load (sum of mean z-scored values for five basic emotions) for four 
lexical types of jVj-words in each of the three experimental conditions 
Gender. The gender of the judges did affect the perception of emotions. In JVJ-ONLY 
condition, females perceived the emotions as stronger than males for all five 
emotions. The gender differences in the other two conditions were minimal and not 

24 
Š. Beňuš and M. Rusko 
significant. This observation is in line with recent findings in [19] that females are 
more attuned to emotional prosody than males in word processing. However the fact 
that this gender difference is significant only in JVJ-ONLY condition and not in FULL 
condition seems to point to the greater masking effect of the lexical context on 
emotional decoding for females than for males. 
The sound of jVj-words vs. their lexical context. Table 1 shows the pair-wise 
correlations between the three conditions of the experiment: JVJ-ONLY, FULL, and 
TEXT. If we take the subjects’ ratings in the condition with both transcript and audio 
stimuli (FULL) as the gold standard, textual cues were better indicators of emotions 
than the prosody of the jVj-words only. This is because the correlation between FULL 
and TEXT conditions (4th row) are better than between JVJ-ONLY and FULL (2nd row) 
for five out of six emotional categories. The only emotion for which the acoustic cues 
of jVj-word are a better predictor of emotion than the textual cues in the context is 
SADNESS. 
Nevertheless, the acoustic features of jVj-words possess useful cues for emotion 
detection. This is because the judges perceived the emotions cued by jVj-words as 
significantly similar to the cues in the full context in which they occurred for four out 
of six emotional categories (2nd row). In all four cases, the correlation between the 
two conditions is significant at p < 0.01 and r-values are around 0.5, which is consid-
ered as a relatively strong correlation in emotion research. 
Table 1. Pair-wise Pearson correlations of ratings for the three types of stimuli. ‘*’ denotes 
significance at p < 0.05, ‘**’ at p < 0.01. 
 
Anger 
Joy 
Sadness 
Surprise
Fear 
ExpLoad 
JVJ-ONLY & FULL .507** 
.162 
.533** 
.239 
.485** 
.476** 
JVJ-ONLY & TEXT .331* 
.020 
-.024 
.079 
.243 
.131 
FULL & TEXT 
.783** 
.697** .350* 
.792** 
.758** 
.590** 
 
Finally, the table also shows that the acoustic features of jVj-words and the textual 
cues of the contexts transcripts add different cues for emotion perception. This is be-
cause there is virtually no correlation between the ratings of the judges in these two 
conditions.  
Jej-words and their dictionary definitions. Despite the underrepresentation of jej-
words in our corpus, we expected that they would be perceived significantly 
differently from the other three jVj-words. Our expectation was based on the lexical 
entries provided for the jVj-words found in the Slovak dictionary and discussed in 
Section 3. In the JVJ-ONLY condition, there were no significant differences among the 
jVj-words for any emotion. In both FULL and TEXT conditions, the lexical type of jVj-
word was significant in perceiving JOY; F(3,38) = 7.86, p < 0.001 and F(3,38) = 
11.79, p < 0.001 respectively. A post-hoc Tukey HSD test showed that in both 
conditions jej-words were perceived as significantly more joyful than each of the 
remaining jVj-words. Hence, jej-words are indeed different from the other three jVj-
words, but not in the perception of SURPRISE, as suggested by the dictionary 

 
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot Words 
25 
definitions, but in the perception of JOY. Note, however, that the perception of JOY 
and SURPRISE correlated in the JVJ-ONLY condition.  
Aligning this finding with the picture in Fig. 1, the primary emotion signaled by 
[jej] is JOY and [jaj] tends to be the least emotionally loaded of the four jVj-words. 
This leaves [joj] and [juj] as the most promising lexical items for further potential use 
in emotional speech research.  
Emotions and acoustics of jVj-words. Since we found that the acoustic signal of jVj-
words contains useful cues for emotion detection, our next step was to identify the 
continuous features that are most useful in this detection. Hence, we ran correlation 
tests between the mean values of emotional categories for each token and the acoustic 
features describing that token.  
There are two features describing the length of the jVj-words, one is plain duration, 
the other is the number of syllables; recall that jVj-words can be reduplicated. Follow-
ing the iconicity principle of length discussed in Section 3, the prediction was that the 
longer the word, the stronger the emotions perceived on that token. However, we did 
not find strong support for this prediction. The number of syllables positively corre-
lated only with JOY, and plain duration correlated only with the general expressive 
load. Moreover, both correlations were close to 0.05 level: r(40) = 0.35, p = 0.023 for 
the former, and r(40) = 0.32, p = 0.041 for the later. Hence, we may conclude that the 
iconicity principle of duration in cueing emotions is not robustly displayed in our data 
of hot-spot words. 
Prosodic features describing the overall pitch and energy values were predicted to 
signal emotional activation (arousal). This prediction was mostly supported in the F0 
features minimum, maximum, and mean. All three features correlated positively with 
the high activation emotions JOY, SURPRISE, and FEAR, and negatively with the low 
activation emotion SADNESS. Energy features showed a similar, but much less robust, 
trend. Surprisingly, supposedly high-activation ANGER correlated with neither general 
F0 nor Energy features. This might be related to the observation in [20] that a general 
label like ANGER might actually include different families of emotions such as HOT 
ANGER and COLD ANGER. Of these two, only the first one is supposed to be a high-
activation emotion. In our data, the only prosodic feature that correlated with ANGER 
was the pitch slope calculated from the stylized pitch targets; r(40) = 0.49, p < 0.001.  
Interestingly, voice quality features shimmer and harmonics-to-noise ratio showed 
the most robust correlation with ANGER; r(40) = 0.61, p < 0.001 for both. In addition 
to ANGER, voice quality features were useful in perceiving SADNESS, but in a much 
less robust way than for ANGER. 
Finally, the prosodic features describing F0 slope correlated with SADNESS and 
FEAR. JVj-words with falling pitch tended to be perceived as sad while jVj tokens 
with rising pitch tended to signal FEAR.  
To summarize, our findings in the corpus of Slovak jVj-words support the observa-
tions from literature that both prosody and voice quality features are useful for detect-
ing emotions from speech. Importantly, they also complement each other since voice 
quality features are useful for ANGER while prosodic features are more useful for 
other emotions. 

26 
Š. Beňuš and M. Rusko 
5   Conclusion and Future Research 
In this paper, we studies the effect of prosodic and voice quality characteristics of jVj-
words in Slovak and the presence or absence of lexical context on the perception of 
emotions they convey. We argued that a corpus based on recordings of puppet plays 
presents a novel and advantageous approach to the methodological issues connected 
with the collection of data for emotional speech research.  
The main finding is that the lexical cues present in the context provide better pre-
dictors for the perception of emotions than the prosodic and voice quality features 
present in the acoustic signal of the jVj-words themselves. Nevertheless, both pro-
sodic as well as voice quality features of Slovak jVj-words are useful and complemen-
tary for the detection of the emotional state of the speaker, and are crucially different 
from the semantic cues.  
It is possible that the observed finding arises from the difference in the two tested 
conditions – perceiving emotions from sentence transcripts and from audio signal of 
jVj-words only. It might seem that transcripts provide more data and represent mean-
ingful language parsing, which makes the task more natural for the subjects compared 
to the other task. However, one might also argue that the emotional load of the  
jVj-words could have been better perceived without the potentially masking effect of 
semantic information. It seems to us that the selection of hot-spot words themselves 
and the restriction to a single word are two obvious avenues for further testing of the 
usefulness of the hot-spot concept for emotion perception. Hence, we plan to test the 
paradigm with other hot-spot words in Slovak such as no ‘well’, or polarity items 
such as ano ‘yes’ and nie ‘no’, and we plan to experiment with including immediately 
preceding and following words in an effort to achieve more robust results for the 
acoustic features. In future work, we would also like to include more acoustic features 
in our experiments following the observation in [21] that cues such as long-term aver-
age spectrum (LTAS), formant and bandwidth features, and more sophisticated char-
acteristics of the glottal pulse and the vocal tract may increase the discriminatory 
power of acoustic features for emotion decoding.  
Acknowledgments. We would like to thank the subjects in our perception experiments. 
This work was supported by the Ministry of Education of the Slovak Republic, KEGA 
grant number 3/6399/08, Scientific Grant Agency project number 2/0138/08, Applied 
Research project number AV 4/0006/07, and by the European Education, Audiovisual 
and Culture Executive Agency LLP project EURONOUNCE. 
 
               
 
 
This project has been funded with support from the European Commission. This 
publication reflects the views only of the authors, and the Commission cannot be held 
responsible for any use which may be made of the information contained therein. 

 
Prosodic Characteristics and Emotional Meanings of Slovak Hot-Spot Words 
27 
References 
1. Wrede, B., Shriberg, E.: Spotting “Hotspots” in Meetings: Human Judgments and Prosodic 
Cues. In: Proceedings of European Conference on Speech Communication and Technol-
ogy, pp. 2805–2808 (2003) 
2. Benus, S., Gravano, A., Hirschberg, J.: Prosody, Emotions, and...‘whatever’. In: Proceed-
ings of International Conference on Speech Communication and Technology, pp. 2629–
2632 (2007) 
3. Mozziconacci, S.: Prosody and Emotions. In: Bel, B., Marlien, I. (eds.) Proceedings of 1st 
International Conference on Speech Prosody, pp. 1–9 (2002) 
4. Ekman, P.: Emotion in the Human Face. Cambridge University Press, Cambridge (1982) 
5. Ladd, D.R.: Intonational Phonology. Cambridge University Press, Cambridge (1996) 
6. Schlosberg, H.: Three dimensions of emotion. Psychological Review 61, 81–88 (1954) 
7. Beckman, M.E., Hirschberg, J., Shattuck-Hufnagel, S.: The Original ToBI System and the 
Evolution of the ToBI Framework. In: Jun, S.-A. (ed.) Prosodic Typology: The Phonology 
of Intonation and Phrasing, pp. 9–54. Oxford University Press, Oxford (2005) 
8. Liscombe, J., Venditti, J., Hirschberg, J.: Classifying Subject Ratings of Emotional Speech 
Using Acoustic Features. In: Proceedings of European Conference on Speech Communica-
tion and Technology, pp. 725–728 (2003) 
9. Zei, B.: A place for affective prosody in a unified model of cognition and emotion. In: Bel, 
B., Marlien, I. (eds.) Proceedings of 1st International Conference on Speech Prosody, pp. 
17–22 (2002) 
10. Banziger, T., Scherer, K.R.: The role of intonation in emotional expressions. Speech 
Communication 46, 252–267 (2005) 
11. Gobl, C., Chasaide, A.N.: The role of voice quality in communicating emotion, mood and 
attitude. Speech Communication 40, 189–212 (2003) 
12. Scherer, K.: Vocal communication of emotion: A review of research paradigms. Speech 
Communication 40, 227–256 (2003) 
13. Rusko, M., Hamar, J.: Character Identity Expression in Vocal Performance of Traditional 
Puppeteers. In: Sojka, P., Kopeček, I., Pala, K. (eds.) TSD 2006. LNCS, vol. 4188, pp. 
509–516. Springer, Heidelberg (2006) 
14. Williams, C.E., Stevens, K.N.: Emotions and speech: Some acoustical factors. Journal of 
the Acoustical Society of America 52, 1238–1250 (1972) 
15. Beňuš, Š., Rusko, M.: Vocal gestures in Slovak: Emotions and prosody. In: Esposito, A., 
Hussain, A., Marinaro, M. (eds.) Multimodal Signals: Cognitive and Algorithmic Issues. 
LNCS (LNAI), vol. 5398, pp. 223–231. Springer, Heidelberg (2009) 
16. Kačala, J., et al.: Krátky slovník slovenského jazyka [Short dictionary of Slovak]. Veda 
Bratislava (2003) 
17. Lakoff, G., Johnson, M.: Metaphors we live by. University of Chicago Press, Chicago 
(1980) 
18. Boersma, P., Weenink, D.: Praat: Doing phonetics by computer,  
  http://www.praat.org 
19. Schirmer, A., Kotz, S.A., Friederici, A.D.: Sex differentiates the role of emotional prosody 
during word processing. Cognitive Brain Research 14, 228–233 (2002) 
20. Ekman, P.: An argument for basic emotions. Cognitive Emotion 6(3/4), 169–200 (1992) 
21. Johnstone, T., Scherer, K.R.: Vocal communication of emotion. In: Lewis, M., Haviland, J. 
(eds.) The handbook of emotions, pp. 226–235. Guilford, New York (2000) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 28–41, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Affiliations, Emotion and the Mobile Phone 
Jane Vincent 
Faculty of Arts and Human Sciences, University of Surrey, 
Stag Hill, Guildford, Surrey GU2 7XH 
jane.vincent@surrey.ac.uk 
Abstract. Over half of the world's population is expected to be using mobile 
phones by 2009 and many have become attached to and even dependent on this 
small electronic communications device. Drawing on seven years of research 
into the affective aspects of mobile phone usage this chapter will examine ways 
that people in the UK have incorporated mobile phones into their lives. It will 
explore what they use it for, their affiliations and their emotional attachment to 
the mobile phone and how it appears to have gained such an extraordinary role 
in maintaining close relationships with family and friends. 
Keywords: Mobile Phone, Emotion, Social Practices, Users, Affiliations. 
1   Introduction 
In this chapter I aim to explore why so many people appear to have a passionate and 
emotional response to this small electronic computational communications device – 
their mobile phone. I will address this question of why some people seem to feel so 
strongly about their mobile phone by examining research in four areas. Firstly what 
people use their mobile phones for; secondly whether mobile phone technology might 
be shaping what we do; thirdly the strategies people adopt to pay for mobile phones 
and fourthly emotion in relation to mobile phones.  I will explore these topics by 
examining the results of research conducted in the UK by myself and colleagues at 
the Digital World Research Centre1 in the UK, [1-6], and by reference to the research 
of others [7-11]. In these various studies the research has explored how some people’s 
social practices are shaping and being shaped by this small mobile phone device. 
The extraordinary properties of the mobile phone have emerged from a long 
history of telecommunications that has evolved and shaped our twenty–first century 
wireless mobile world. Since the latter part of the nineteenth century, and well into 
the late twentieth century the British Post Office, and its successor in 1980 British 
Telecom, played a key role in the start of a massive revolution in telecommunications 
that was instrumental in developing the telegraph and wireless telecommunications 
services.  Similar transitions were taking place in other countries across the globe and 
this history of engineering developments is well documented by its institutions [12]. 
The public use of the mobile phone came after many years of wireless telegraphy use 
                                                           
1 Digital World Research Centre, Faculty of Arts and Human Sciences, University of Surrey, 
UK. 

 
Affiliations, Emotion and the Mobile Phone 
29 
in military and emergency services and although available as a public service since 
the late 1970’s it was not until the mid 1990’s that the mobile phone was adopted by a 
large proportion of the population in the UK. Young people and children who have 
grown up with mobile phones in their households now take it for granted as part  
of their communications repertoire. However, for many people, it is still quite 
remarkable that one can take a mobile phone to almost any location in the world, use 
it successfully and be billed for it on one bill by their home service provider.  
The early pioneers of telecommunications show us that solutions to tricky 
problems such as making a telephone call from anywhere can come from quite 
unusual sources such as the association of previously unrelated topics. Dr.Martin 
Cooper claims his invention of the portable mobile phone owes something to seeing 
the communicator used by a certain Captain James T. Kirk of the Starship Enterprise, 
a science fiction creation that also manifested in the StarTAC Motorola mobile phone 
in the 1990’s. Indeed, this is perhaps less surprising when one considers that the work 
of science fiction is on occasion ahead of the futuristic ideas of research and 
development teams of the most advanced technology organisations, [13, 14]. Not 
withstanding this science fiction legacy it would appear that even from the first 
realisation of the existence of wireless communication there has been a kind of magic 
or enchantment about the mobile phone device and about how it works and the instant 
connections it enables [13]. Even for the engineers who know how it works there 
remains an excitement of being able to send and receive messages from any location, 
in almost any country and at any time of the day. There is also a thrill of being able to 
conduct conversations by voice, text or image that are private and intimate in very 
public places, and a strong desire to keep checking and to keep hold of ‘my mobile 
phone’ [15]. 
2   Research Methodologies and Theoretical Perspectives 
Examining social practices with regard to mobile phones presents some difficulties 
for the researcher, not least because of the paucity of academic literature on the social 
and human aspects of mobile communications prior to the first decade of the twenty 
first century.  This was due in part to the speed of adoption of this means of 
communication as well as to the restrictions on access to user data due to competition 
and shareholder confidentiality within the mobile communications industry. Mobile 
phones became popular in Japan and Western Europe some years before the USA, 
and this is reflected in the source of research material, particularly prior to the early 
twenty first century. The Digital World Research Centre began its research into the 
social practices of mobile phone users in 1997 [17], and since that date various studies 
have been carried out, mostly funded by the mobile communications industry. Each of 
these studies has used the qualitative methods of observation, diaries, focus groups, 
questionnaires and interviews and none of the studies involved less than 40 
respondents. The common theme throughout has been to understand how the 
respondents were using their mobile phones on a day to day basis. In some studies 
particular aspects of mobile phone use were explored such as how people could afford 
to maintain their mobile phone [4]; how children use mobile phones [3, 5], and how 
existing mobile phone use might shape the ways new mobile communications 

30 
J. Vincent 
technologies would be used, [1-2]. In common with other researchers such as Ling in 
Norway [7], Fortunati [16] in Italy and Höflich and Hartmann [10] in Germany, the 
UK based studies refer, among others, to the work of the interactionist theorists – 
particularly Goffman [18] – to research the social practices of mobile phone users. 
Exploration of human and technology interaction has been researched with regard to 
other information and communications technology media such as computers and the 
walkman [19], to name but two and there is a broad literature on the adoption of 
technologies. The seminal work by MacKenzie and Wajcman [20] is an important 
contribution to understanding the theoretical discourse on technological and social 
shaping, although it is only in recent times that the social shaping of technologies was 
considered to be a realistic proposition [21]. Prior to this the technological determinist 
approach had prevailed.  More recently the domestication theory of Haddon and 
Silverstone, developed prior to the omnipresence of mobile phones in society, has 
been used to examine the adoption practices of mobile phone users [22]. They 
developed a theory of domestication with regard to the use of technologies in 
everyday life; there is a point at which the technology simply becomes part of the 
everyday domestic processes of a household and is no longer a complicated and alien 
objects such as digital broadcasting (DAB) or high definition television (HDTV), or 
even mobile phones are for some people today.  In a different approach to the matter 
Katz and Aakhus [8] have postulated their apparatgeist theory in which they suggest 
our social practices can be explained by the combination of the benefits of the 
apparatus (that is the mobile phone) and our sense of whole life well being. More 
recent studies have explored the role of the human as an electronic actor in this 
technologically rich environment, in particular Vershinskaya [23] and the work of the 
COST 298 research Work Group 2 [13] who expound emerging theories in this 
relatively new area of research. Understanding the human interactions between the 
mobile phone and the multiplicity of the content it conveys and retains, as well as the 
numerous social, business and commercial contacts it enables, continues to be a 
conundrum, and a theoretical proposition that fully explains this human interaction is 
still some way off.    
3   Examining the Research Findings 
In this section of the chapter I explore the four topics outlined in the introduction, 
illustrating them with references to the findings in the various aforementioned 
research studies.  
3.1   Mobile Phone Affiliations and Users  
Although there are many similarities of use every individual mobile phone user will 
have created a way of using their own mobile phone that is personal to them. The 
combination of where they keep it, how, when and where they use it; what 
information it contains; the services they access – the varieties are endless. Consider 
for a moment these matters with regard to a series of questions: Where is your mobile 
phone now?  Is it in your hand, are you touching it? How close is it to you? Is it 
switched on silent, vibrate, meeting, switched off?  Have you personalised it? What 

 
Affiliations, Emotion and the Mobile Phone 
31 
do you use it for? Can you resist it for the next two hours or even two minutes? Can 
you live with out it? These are the kind of questions that have been addressed by the 
various respondents in the aforementioned studies and their answers were diverse.  
Some people, for example, have no qualms about not backing up their phone 
directory: […]‘It’s all on my PC’ or ‘I can easily get the really important numbers 
because they are people I know’, whereas others keep paper copies to ensure that 
these numbers, so vital to their everyday lives, are not lost. Learning from his 
experience with a personal digital organiser (pda) one respondent said of his mobile 
phone:  
 
[…]I always back up on paper and have a back up memory chip too - this 
pda lost everything once and I was stranded. [2]   
 
Some people keep their phone about their person in a pocket or handbag at all times, 
and for most it is an essential item placed alongside their house keys and wallet or 
handbag so they do not leave home without it. 
Many of the respondents said that the safety and security the mobile phone offers is 
really important, even if they never actually find a need for it and it stays unused in 
their handbag.  
 
We thought it would be […] quite nice to be in a situation when there 
was an emergency …to be able to use it … if you needed to.  Sharon [2] 
 
Some used it to ensure their child could text them to say they have arrived when out 
unaccompanied, or for a parent to be able to text to say they are on their way to collect 
a child, or that they are late. People of all generations are using their mobiles in this 
way. Interestingly the children interviewed in the studies of 11-16 year olds [3, 5], 
preferred to phone rather than text parents for vital things such as lifts because this was 
the only way they could be certain that their parent would get the message and that the 
lift was assured.   It was also found in all studies that many calls were short calls and 
were frequently about making social arrangements. 
Furthermore most people use their mobile phones for person to person 
communication with people they already know – numbers stored on mobile phones 
had been placed there for a purpose as a result of direct or third party contact. The 
daily routine of work supports this type of use but of course there are also some niche 
applications now targeted at specific groups of customers such as location services, 
remote monitoring and specialist medical applications. The point about using the 
mobile for social arrangements applies to the ways business users make use of their 
mobiles as well as personal users – in other words many business calls are merely 
catching up or making social arrangements with close business colleagues. 
Looking back to the past when new telecommunications products were first made 
available they were often pushed onto an unsuspecting public with little consideration 
for the practicalities of the device or the relevance of the products and services to their 
lives. The mobile phone has been no exception and in the early research projects  
[1, 3], it was found that people were less willing to use the person to information 
services, although these were widely available. Later research projects found, 
however, that as mobile phone and other ICT services converged there was more use 
of the mobile phone for information services to while away time waiting for transport 

32 
J. Vincent 
or while on the train for example. Camera and video on the mobile phone were used 
to record special family moments, the unexpected sighting of a celebrity or a 
catastrophic event or emergency such as a road accident or terror attack - the so called 
citizen journalism. 
For most people their mobile phone use arises out of their relationships with 
friends, family and business and commercial interests. These are all person to person 
contacts with communication by voice or text, maybe email or image too.  However 
the mobile phone does enable people to do other things, music downloads, sports 
training, games, gambling and many more besides. It also links people with other non 
mobile phone devices like their personal computer or lap top via Bluetooth, or the 
(wireless) internet using email or web searching. These are the ‘me to machine’ 
interfaces that involve connecting with technology and infrastructure rather than other 
people [15].  
As people go about their every day activities they do not just use their mobile 
phones to communicate with each other in these ways but also to communicate their 
image and affiliations and this has had an influence on the design of the mobile phone 
and the take up of services. Indeed the mobile phone has become a fashion item 
which functions like clothes to communicate aspects of people’s identity. In this sense 
there is a kind of mobile phone communication going on, which is both non-verbal 
and co-present, which is not mediated by the wireless technology but by the visual 
impact of the casing and how people interact with the mobile and all that it engenders. 
[9]. As found in the study for the UMTS Forum in 2003 [1], the mobile is used for 
flânerie;  being cool and having the right mobile is used to make a personal statement; 
walking out with the mobile phone, showing off the design, its ring tone and other 
functionality displays it as a symbol for its user.  ‘I only got one because my mates 
had one’ was a common response in research, especially among the children [3].  For 
many people the device is a display of their identity, their affiliations and image.  A 
group of girls were arranging a night out with a new friend and when they got out 
their mobiles to exchange numbers the new member of the group was quickly offered 
a spare phone by one of the others: […]‘If you are coming out with us you can’t take 
that old phone with you’ she was told.  Being accepted into their group meant 
dressing and behaving in their style and this included the mobile phone [2].   
These various uses for the mobile phone and the affiliations that accompany them 
such as buying a mobile phone that is socially acceptable within one’s peer group, 
using similar ring tones, exchanging tunes between phones, taking photos of one 
another’s clothes, even checking make up is on correctly, all show how the mobile 
phone has many, many uses.  
Unfortunately not all mobile phone affiliations are beneficial and the effects of 
having mobile phones, particularly it would appear amongst some segments of the 
youth market, can be harmful to some. Bullying became a problem some years ago 
and as this young woman’s experience shows: […]‘my phone number has changed 
because my ex–boyfriend wouldn’t stop texting me’, this was not just confined to the 
younger children in the research studies. One particularly unpleasant trait has been 
happy slapping which of course is a totally unacceptable social practice for the 
recipient but for the perpetrators there is clearly a thrill of taking and showing images 
of their actions. Video social networking internet sites are full of less criminal 

 
Affiliations, Emotion and the Mobile Phone 
33 
examples of mobile phone videos which none-the-less border on unacceptable social 
behaviour.  
Notwithstanding the negative affiliations of mobile phones their everyday use has 
resulted in the very beneficial development of multitudes of personal mobile worlds 
that keep people connected globally [25], These personal mobile worlds are ethereal 
places where people are connected to their close friends and family or work 
colleagues via the touch of their mobile phone [26]. However, these personal mobile 
worlds have created social silence.  Teachers and lecturers no longer receive feedback 
from pupils after class as the chatter that used to follow lessons is replaced with the 
silent motions of accessing texts and voice mail that arrived during the class. 
Research by others such Katz [8] Rheingold [27] and Licoppe [26] has explored the 
curious phenomenon of what has been referred to as ‘Buddy Space’ or 
‘Technospace’; in other words the ethereal worlds that contain the links between 
people who know each other but who are often connected only by their mobile phone.  
The absorbed texter is tuned into the world he is texting rather than the one in 
which he is physically located and in cars, hands free kit is used to maintain contact at 
all times with others in one’s buddy space. Watching television, video, listening to 
music and playing games has now entered the repertoire of the buddy space with 
downloads and internet access becoming more simple.   This constant connectivity to 
one’s buddy space afforded by the mobile phone has now caused a blurring of the 
boundaries between public and private behaviours and today it would appear that 
almost anything goes.  In the early days of mobile phones people did not feel 
comfortable using them in public places and were stilted and awkward about it but 
slowly etiquette developed. Now it is acceptable for the most intimate of 
conversations to be conducted in very public places and of course contact can be 
made with people in private places wherever they are and at any time. [28, 29] It is 
not only etiquettes that relate to proximity of the user that affect mobile phone use but 
those that apply to the different services and functions of the device.  Camera phones 
were quickly provided with an audible click, not just a stylish design feature but also 
to help avoid inappropriate use – but this does not prevent it. The etiquette for the use 
of mobiles does appear to exist but as with any behaviour, not everyone keeps to the 
rules.  Overheard conversations seem to exercise the patience of many and strategies 
emerge for dealing with this; mobile phones are banned in some places.  There is 
more than one incidence reported of irritated people texting fellow travellers to ‘shut 
up’ after they have audibly given out their mobile number to a caller. In theatres 
actors have stopped a performance to demand an audience member to leave after their 
mobile phone audibly rung interrupting the production. [30] People are provided with 
etiquette guidelines such as in train carriages on the Washington Metro where a poster 
exclaims alongside an image of a man speaking with a wide open mouth and his 
mobile phone to his ear, ‘Yes we are all interested in what you are having for dinner 
tonight. (Please keep your phone conversations to yourself)’.  Although people do 
complain about others they also admitted to taking calls in inappropriate places 
themselves [1]. Some of these different reactions to mobile phone use in public places 
can be attributed to cultural differences that add further complexity to these public 
behaviours. In her study of mobile phone use in London, Paris and Madrid, Lasen 
[11] found some interesting variations in usage. For example in Madrid people used 
their mobile phones much more, sharing conversations or being together while on 

34 
J. Vincent 
separate calls.  In London people were observed using their mobile phones alone but 
in similar locations at the same time – such as on a street corner whereas in Paris 
people would seek out a more solitary space and spend time alone with their caller. 
In summary the research findings have shown a diverse multiplicity of uses for 
mobile phones and for many users there are no boundaries to when, where and for 
what a mobile phone might be used, even in a public place. Furthermore the mobile 
phone would appear to be demonstrative of one’s affiliations and identity so that it is 
not just any old device but rather it has some special meaning as ‘my mobile phone’. 
3.2   Technological Shaping of Mobile Phone Use 
Observing and researching what people do with their mobile phones highlights the 
close attachment that people have for the device as well as the mystery and 
excitement that is associated with the technology, but does the technology really 
shape what we do with the mobile phone, or could it simply be a new way of doing 
old things? In the late 1890’s day trips by train from London to the seaside were often 
recorded in a portrait photograph taken by a commercial photographer. Made into a 
post card the same day the pictures were sent by postal mail to family members who 
could not join the day trip. Today the same message and an accompanying photo can 
be sent by text or video, and a call might be made many times during a day. Camera 
phones have taken the place of digital cameras for some. Many are used in a very 
similar way to the first cameras of the early twentieth century that transformed 
photography from a formal pose to the everyday snapshot images that are more 
familiar today.  People have not stopped sending postcards nor buying formal 
photographic portraits while on seaside visits, but they do also use a mobile phone to 
record and communicate the special moments of the day too.   
It is not unusual for technologies to be used in different ways from that intended by 
their creators and there are some very clear examples of how society has embraced or 
rejected particular technological innovations made for the mobile phone. In particular 
WAP (wireless application protocol) was rejected when offered as a product to access 
the internet whereas SMS (short messaging service) was not only embraced but 
demanded by users before it had been fully developed as a product [31, 32]. The 
capabilities of WAP technology are at last finding applications in the context of the 
new third generation mobile services (but it is no longer called WAP) and its launch 
in the 1990s is now acknowledged to have been premature. Lessons learned from this 
experience were used to caution early launch of 3G [1] such as noted in an article 
about one of the UK’s leading network operators: 
 
O2’s caution about a consumer launch of 3G is born of experience. 
During the late 90s, while still part of BT, the operator made grandiose 
claims for new data services using technology known as WAP. 
Although its advertising promised customers they could "surf the net, 
surf the BT Cellnet", the experience was not even remotely like using 
the internet on a PC.  The Guardian Richard Wray 27 Oct 2004 [33] 
 
Today email and the internet are easily accessible over mobile phones (although 
some more successfully than others), but no claim is made that it is ‘like using a 

 
Affiliations, Emotion and the Mobile Phone 
35 
personal computer (pc)’ for it is now acknowledged that mobile phones will never be 
a substitute for a lap top or a pc, rather it is a complementary device.  
 
You can go on the house computer and it’s like the entire Internet. If 
you go on your mobile one, it’s like a little…it’s like about a third of 
what you could do on the normal Internet  Tim (15) [5] 
 
SMS was an entirely different story with the early users shaping the product with 
their demand for service and the development of a new argot. (32)  Although how 
new is the argot?  Short hand, speed writing and other methods of shortening words 
have existed for well over a century. An eighty year old grandfather quickly adopted 
text messaging when he realised he could use the abbreviated shorthand he learned as 
a lawyer in the 1950s to text his 11 year granddaughter. The texting argot is certainly 
new and inventive for the majority of its younger users but these links with past short 
hand and abbreviated message writing have enabled people with this prior experience 
to also use texting, some with alacrity.  
Unwittingly the lack of technological capabilities in the form of the inability to be 
able to bill customers for text messaging in the early days of its use stimulated its take 
up. Not only has an argot emerged for use in texting it has extended beyond into 
everyday life and text messages have been used to inform people of their redundancy, 
to convey a multitude of important messages as well to enable the current blogging 
phenomenon of Twitter.com that uses text and its limit of 160 characters to provide 
news, commentary and diary records of every day lives. 
Although these examples show that people do like to hold on to familiar practices 
in the end some of the old ways of doing things do fall out of use and in the UK the 
use of payphones and paging are two examples of technologies that have been 
superseded by the omnipresence of mobile phones.  Telephone boxes are now 
retained as listed buildings in some locations, or paid for by local communities, and 
most paging networks have shut down. 
Consideration of whether the popularity of mobile phones can be attributed to 
social and/or technological shaping is complex as has been demonstrated by the 
attempts to make the mobile phone a fashion object.  The affiliations referred to 
earlier were created by the peer groups for themselves but when mobile phone 
products were made into a fashion object by manufacturers [9], without adequate 
reference to the peer group behaviour they found very few customers. However, 
manufacturers of mobile phones do now produce mass market mobile phone products 
that are coloured rather than black, and they have produced slim polished chrome 
devices or made exclusive limited edition diamond studied phones.  Some mobile 
phones have product names that use the text argot such as ‘pebl’ and ‘razr’ [14] 
reflecting the way people have appropriated text and developed the SMS argot.  
It would appear that a new telecommunications ecology is emerging that takes 
account of social practices in the design of products and the applications developed 
from new technologies. Whilst it is clear that people do hang on to old ways of doing 
things it does not mean they are not willing to adopt new products.  Similarity of user 
behaviours is not the same as similarity of products and the ability to personalise a 
mobile phone allows for these individual differences to manifest. In this way the 
mobile phone appears to have developed a unique place in people’s communications 

36 
J. Vincent 
repertoire and they are now an inclusive part of most people’s lives. Mobile phones 
have created new ways of doing things but have also been adapted to incorporate 
familiar practices, and as found in one of the early studies. […] ‘having a mobile 
phone is virtually a social necessity’ [1]. 
3.3   Strategies for Affording a Mobile Phone 
Throughout all the studies mobile phone use has been influenced by how much 
money it costs to maintain service. One study in particular explored the topic of how 
people afford a mobile phone [4] from which it was clear that people are unwilling to 
give up their mobile phone under any circumstances.  If they do find themselves with 
no money to pay for it they will ensure they at least have a working pay as you go 
SIM card and an old mobile phone so that they can be contacted, even if they cannot 
make calls or send texts themselves. It was found that most people know their running 
costs and are frugal with their use of the mobile phone optimizing tariffs. The children 
studied in 2004 and 2006 were particularly aware of the cost of running a mobile 
phone and were very price sensitive. Voice calls were often deliberately short such  
as […] ‘Mum I’m at the station’ (meaning come and collect me), or, […] ‘I’m at 
Jennies’ (meaning I’m OK and it doesn’t matter that you don’t know Jenny or where 
she lives). Most children spend £10 or less per month on their mobile phone and their 
bills are sometimes paid by parents but most pay it all or jointly. One child 
demonstrated how she kept her costs down, writing in her diary: […] ‘Phone call with 
my sister: I phoned her then she rang back as I was low on credit’ and another shared 
the cost of keeping in touch with his girl friend:  
 
I phone my girlfriend every night for 45minutes; we take it in turns, …I 
only phone her on my mobile when she isn’t at home and is on her’s 
because I am not allowed to make long calls to mobiles from the home 
phone. [3] 
 
Some respondents using pay and go tariffs in particular would wait to make longer 
calls on house phones at the expense of the household or might instead use voice over 
internet such as on their MSN or other instant messaging service or using skype.  
The household expenditure on telecommunications is factored into how people use 
their mobile, changing tariffs not just on mobiles to accommodate charges. One child 
phoned his dad to speak to his mum. This was because he could call his dad more 
cheaply than he could call his mum; his dad then phoned his wife using his work 
phone and his wife called her son back on her mobile within her free minutes quota 
[5]. In fact, many people spend very little on mobiles, either their employer pays or 
maybe they just do not use them [34]. Some respondents were deterred by the 
complexity and apparent high cost of data tariffs – most people are not familiar with 
the file size of an image for example.  
Thus although the strategies for managing the cost of running a mobile phone were 
elaborate it remained an essential item and people of all ages would ensure they had a 
functioning mobile phone [35].  But why is it that these respondents felt they had to 
find the money to keep their mobile phone going, why did they have these elaborate 
strategies for keeping in touch and why indeed has the mobile phone become such 
social necessity? Some of the reasons have already been discussed with regard to the 

 
Affiliations, Emotion and the Mobile Phone 
37 
various uses and affiliations associated with having a mobile phone and in the final 
section I will explore perhaps the most compelling reason, that of emotion and the 
mobile phone. 
3.4   Emotion and the Mobile Phone 
Thus far in this chapter I have explored the ways that people use their mobile phones, 
how their social practices have both influenced and been influenced by the design and 
capabilities of the mobile phone and how they can afford to keep their mobile phone. 
I now move on to explore the emotional aspects of the human interaction with this 
device. The concept of emotion and the mobile phone refers to the relationship its 
user has with the device as well as with the content it conveys and the people it 
connects them with.  
Building emotion into the design of a device is a vital factor in the success of many 
products today. The aesthetic of mobile phones has to be good – they have got to feel 
right, look good and reflect the emotional needs of the user.  However as these 
children commented: 
 
Jed: You don’t want to show off your mobile too much because some 
people will think that you’re like…boasting about it… it just makes you 
look like a bit of a… a bit spoilt in a way. 
Paula: I think you can show off a bit but you can’t go too far   
(Focus group 1:13–14yrs) [5]  
 
The early mobile phones were large, mostly black or grey and with little finesse. 
Since then size, weight and battery life continued to be the driving force for mobile 
phone manufacturers for many years until more recently the addition of the style and 
the design of the casing was recognised as an important selling point. More mobile 
phone suppliers are now employing ethnographers to better understand the social 
practices of their customers – not just their demographics but deep down into why 
they do the things they do – these needs are reflected back into the device design, as 
well as in the design of new products and services.  
The mobile phone supports perpetual contact between friends and family as well as 
business colleagues and in so doing it brings about an emotional attachment not only 
with the device in the ways discussed earlier but to all that it engenders. Knowing that 
you are able to maintain contact with loved ones at all times has led to a dependency 
on the mobile phone for some of the respondents.  
 
Sharon: ‘It’s just so easy to keep in contact, so convenient…you get to a 
stage when you couldn’t do without it…I’d feel really, really lost 
without my phone now’. [2]  
 
Times of utmost need and crisis are situations that might never occur but because 
they could it means that the phone has become an essential tool, particularly for 
teenagers. The situation that occurred after a terrorist attack in London in 2005 
highlighted the importance of mobile phones for reassurance and safety; they were so 
much in demand that the UK mobile phone networks temporarily failed under the 
sheer weight of calls, [36]. Many of these calls were between emergency services 

38 
J. Vincent 
staff coordinating their activities as well as between friends and families to check 
people were safe as was reported by a news web site at the time. 
 
The United Kingdom's major mobile networks are struggling under the 
strain of London residents rushing to call friends and family as news of 
a series of explosions spread Thursday morning.  News.com 7 July 05 
 
Ironically, this dependency has lead to the value paradox of a mobile phone being 
too valuable to lose and so at times, it is actually left at home. Interestingly 
respondents would give reasons for why they did not have their mobile phone with 
them but they did not feel the need to explain why they had it with them. Being 
without a mobile phone, even if this was because the battery had run out, was an 
emotional event but having it with you was simply an everyday normal occurrence.  
 
Sometimes she forgot to charge it up and the battery ran down. We 
often have panic situations. [2] 
 
Adults say they do not always take a mobile when out clubbing and children do not 
take them anywhere they cannot keep them with them at all times.  The emotional 
way people talk and express their feelings about mobile phones shows that it has a 
unique place in their communications repertoire. For example they are emotional 
about the conversations by voice or text with loved ones who are distant being 
conducted in a public place where strangers are co-present. A major strength of a 
mobile phone is that it provides the comfort of feeling near to loved ones when you 
are apart. They hold happy and sad memories.  I referred earlier to not wanting to let 
go of phones; an example of why this might be is the young girl who was told by her 
mother to pass her mobile phone on to her brother as she had a new one and he was 
due an upgrade too.  After a few weeks, she still had not done so and when quizzed 
about this she said she had special memories about a boyfriend to do with that mobile 
phone and she couldn’t bear to think of her brother using it. She even kept the mobile 
phone under her pillow at night [37]. Keeping text messages on the phone memory 
from friends and family who have died are another example of why people will not 
give up a phone.  The mobile phone is certainly integral to all types of relationships 
and also, as already learned, to the feelings and emotions of the users. 
 In social groups a friend who cannot make it to a social gathering might well be 
included by the sharing of a phone call – the handing round of the mobile so people 
can talk with them, or the sharing of an image or a text explaining why they are not 
there. When meeting up friends will call each other until they are actually together 
although they know that they will be seeing each other. 
 
I call my friends…stupid calls…I’m meeting them in half an hour and 
I’ll call them, speak to them…until I meet them [1]  
 
This constant contact is not always appreciated, however, and as one respondent 
explained it spoiled a relationship for him. 
 
If I wanted to speak to my girlfriend any time of the day I know that I 
can and it kind of takes the fun out of it when I’m seeing her [1]  
 

 
Affiliations, Emotion and the Mobile Phone 
39 
Mobile phone coverage extends to some of the most remote places on earth. 
However, there has been criticism that people are turning their back on who they are 
with – the co-present, in favour of the being with the absent present, those people with 
whom they want to share the moment but from whom they are separated. You can be 
wherever you want to be with a mobile phone just so long as you can connect with 
others, and you need never be alone. 
Emotion is thus embedded in most of the various uses for the mobile phone, from 
photos, texts, and other personal data recorded on the mobile phone to the touch and 
feel of the device, and the association a particular mobile might have with a special 
event or person, the mobile phone is imbued with emotion.  
4   Conclusions  
In this chapter I have explored how some people use their mobile phones, as well as 
examining their affiliations and their emotional attachment to the mobile phone.  This 
small electronic communications device seems to have gained an extraordinary role in 
maintaining close relationships with family and friends and more recently has become 
the repository of a great deal of personalised data and media content. It would appear 
that the social practices of our everyday life will continue to influence the design and 
development of future mobile communications but that the creation of new technologies 
delivered by the mobile phone will also shape how people use them. It would also seem 
that people will continue to develop elaborate strategies for optimizing the whole 
repertoire of their communications portfolio whether or not they have the income to 
support it. Finally I would conclude that the greater the omnipresence of mobile phones 
nationally and globally the greater will be the emotional attachment to them. This is 
because so many people find they cannot do without their mobile phone as it has 
become an essential part of their life and integral to their social arrangements, their child 
care, their work, their leisure time and their personal safety and security. Having the 
right mobile phone that displays one’s affiliations and one’s identity is now an 
indispensable prerequisite of many people’s lives after the age of 11, (or younger), and 
it will not be many years before this will apply to most people in the UK. 
References 
1. Vincent, J., Harper, R.: Social Shaping of UMTS - Preparing the 3G Customer, UMTS 
Forum Report 26 (2003), http://www.umts-forum.org 
2. Vincent, J., Haddon, L.: Informing Suppliers about User Behaviours to better prepare them 
for their 3G/UMTS Customers. UMTS Forum Report 34 (2004),  
       http://www.umts-forum.org/  
3. Vincent, J.: ‘11 16 Mobile’ Examining mobile phone and ICT use amongst children aged 
11 to 16, Report for Vodafone (2004), http://www.dwrc.surrey.ac.uk 
4. Hamill, L., Haddon, L., Vincent, J., Rickman, N., Mendoza-Contreras, E.: How much can I 
afford to spend on my mobile? DWRC Report for Vodafone (2004) 
5. Haddon, L., Vincent, J.: Growing up with a Mobile Phone – Learning from the 
Experiences of Some Children in the UK. DWRC Report for Vodafone (2007) 

40 
J. Vincent 
6. Vincent, J.: Emotion, My Mobile, My Identity. In: Vincent, J., Fortunati, L. (eds.) 
Electronic Emotion, The Mediation of Emotion via Information and Communication 
Technologies. Peter Lang, Oxford (in print) 
7. Ling, R.: New Tech, New Ties How Mobile Communications is Reshaping Social 
Cohesion. MIT Press, Cambridge (2008) 
8. Katz, J.E., Aakhus, M. (eds.): Perpetual Contact Mobile Communication, Private Talk, 
Public Performance. Cambridge University Press, Cambridge (2002) 
9. Fortunati, L., Katz, J., Riccini, R. (eds.): Mediating the Human Body: Technology, 
Communication and Fashion. Erlbaum, Mahwah (2003) 
10. Höflich, J.R., Hartmann, M. (eds.): Mobile Communication in Everyday Life: Ethnographic 
Views, Observations and Reflections. Frank & Timme, Berlin (2006) 
11. Lasen, A.: Understanding Mobile Phone Users and Usage. Vodafone Group R&D, 
Newbury (2005) 
12. Ward, K.: A History of the Journal. The Journal of The Institute of Telecommunications 
Professionals 2, 7–18 (2008) 
13. Fortunati, L., Vincent, J.: Introduction. In: Vincent, J., Fortunati, L. (eds.) Electronic 
Emotion, The Mediation of Emotion via Information and Communication Technologies. 
Peter Lang, Oxford (in print) 
14. Levinson, P.: Cellphone. In: The story of the world’s most mobile medium and how it has 
transformed everything! Palgrave Macmillan, New York (2004) 
15. Vincent, J.: Me and My Mobile Phone. In: Fortunati, L., Vincent, J., Gebhardt, J., 
Petrovčič, A., Vershinskaya, O. (eds.) Interacting in Broadband Society. Peter Lang, Berlin 
(in print) 
16. Fortunati, L.: Mobile Telephone and the Presentation of Self. In: Ling, R., Pedersen, P.E. 
(eds.) Mobile communications Re-negotiation of the Social Sphere, pp. 203–218. Springer, 
London (2005) 
17. Brown, B., Green, N., Harper, R. (eds.): Wireless World Social and Interactional Aspects 
of the Mobile Age. Springer, London (2002) 
18. Goffman, E.: The Presentation of Self in Everyday Life. Penguin Books (1959) (Middlesex 
Edition, 1969) 
19. du Gay, P., Hall, S., Janes, L., Mackay, H., Negas, K.: Doing Cultural Studies: The Story 
of the Sony Walkman. Sage, London (1997) 
20. MacKenzie, D., Wajcman, J.: The Social Shaping of Technology. OUP, Buckingham 
(1999) 
21. Bendelow, G., Williams, S.J. (eds.): Emotions and Social Life Critical Themes and 
Contemporary Issues. Routledge, London (1998) 
22. Haddon, L.: Domestication and Mobile Telephony. In: Katz, J. (ed.) Machines that 
Become Us: The Social Context of Personal Communication Technology, pp. 43–56. 
Transaction Publishers, New Brunswick (2003) 
23. Vershinskaya, O.: Theoretical Approach to the concept of Humans as E–Actors. In: 
Fortunati, L., Vincent, J., Gebhardt, J., Petrovčič, A., Vershinskaya, O. (eds.) Interacting in 
Broadband Society. Peter Lang, Berlin (in print) 
24. Daniels, G., Channing, I.: Mobile Planet, Connecting the World. Decisive Media, London 
(2008) 
25. Fortunati, L., Vincent, J., Gebhardt, J., Petrovčič, A., Vershinskaya, O. (eds.): Interacting 
in Broadband Society. Peter Lang, Berlin (in print) 
26. Licoppe, C.: Connected Presence: The emergence of a new repertoire for managing social 
relationships in a changing communications technoscape. In: Environment and Planning 
D: Society and Space, vol. 22, pp. 135–156 (2004) 

 
Affiliations, Emotion and the Mobile Phone 
41 
27. Rheingold, H.: Smartmobs The Next Social Revolution. Perseus, Cambridge (2003) 
28. Vincent, J.: Emotional Attachment and Mobile Phones. In: Knowledge Technology and 
Policy, vol. 19, pp. 29–44. Springer, Netherlands (2006) 
29. DWRC Focus Group Research: Mobile Phone Etiquette. University of Surrey (2002) 
30. Vincent, J.: Why do some people love and loathe their mobile phones? Porthcurno 
Museum Connected Earth Communications Spring Lecture Series (2008),  
 http://www.porthcurno.org.uk 
31. Vincent, J.: The Social Shaping of the Mobile Communications Repertoire. The Journal of 
the Communications Network 3 (2004) 
32. Taylor, A., Vincent, J.: SMS A History. In: Hamill, L., Lasen, A. (eds.) Mobile World Past 
Present and Future. Springer, UK (2005) 
33. Wray, R.: The Guardian Newspaper, UK, October 27 (2004) 
34. Haddon, L., Vincent, J.: Children’s Broadening Use of Mobile Phones. In: Goggin, G., 
Hjorth, L. (eds.) Mobile Technologies from Telecommunications to Media. Routledge, 
New York (2009) 
35. Haddon, L., Vincent, J.: Making the Most of the Communications Repertoire. In: Nyíri, K. 
(ed.) A Sense of Place. The Global and the Local in Mobile Communication, pp. 231–240. 
Passagen Verlag, Vienna (2005) 
36. London Assembly. Report of the 7 July Review Committee. Greater London Authority, 
London (2006) 
37. Vincent, J.: Emotion and the Mobile Phone, Presentation (2004),  
       http://www.dwrc/surrey.ac.uk/Publications/DigiPlay3.pdf 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 42–49, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Polish Emotional Speech Database – Recording and 
Preliminary Validation 
Piotr Staroniewicz and Wojciech Majewski 
Wroclaw University of Technology 
Institute of Telecommunications, Teleinformatics and Acoustics 
Wybrzeze Wyspianskiego 27, 50-370 Wroclaw, Poland 
piotr.staroniewicz@pwr.wroc.pl 
Abstract. The paper presents the state of the art review on emotional speech 
databases and designing of Polish database. The principles set for naturalness, 
choice of emotions, speakers selection, text material and validation procedure 
are presented. The six simulated emotional states: anger, sadness, happiness, 
fear, disgust, surprise plus neutral state were chosen for recording by speakers 
from three groups: professional actors, amateur actors and amateurs. Linguistic 
material consists of ten short everyday life sentences. Amateurs database 
recordings are completed and next step will be recordings of professional 
actors. The database of over two thousand amateur speakers utterances was 
recorded and validated by perception listeners forced choice. The results of 
emotion recognition and the influence of musical education, gender and 
nationality for the group of over two hundred listeners are presented. 
Keywords: emotional speech, speech database. 
1   Introduction 
In speech communication linguistic information is only a small part of the spoken 
message. What is also very significant is the information related to an extra-linguistic 
message about the identity of the speaker and his affective state, emotions, attitudes, 
intention etc. Among them, emotions are important factors in speech-computer 
communication (i.e. speech and speaker recognition, speech synthesis). A speech 
emotion recognition system can be applied by disabled people or actors for emotion 
speech consistency, in modern and more natural speech synthesizers etc. Nowadays 
the leading speech laboratories in the world try to develop efficient algorithms for 
emotion speech synthesis and emotion speech recognition. This problem also lies in 
the field of interest of the European project COST Action 2102 “Cross-modal 
Analysis of Verbal and Non-verbal Communication” [1]. The indispensable condition 
of achieving such goals is the collection of the emotional speech dataset. The paper 
briefly presents the state of the art review on emotional speech databases, the design 
of a Polish database which is currently recorded and validated in our Lab and partial 
validation results. 
Most of emotional speech databases [2,3,4,5,6] were used for automatic recognition 
and speech synthesis. For the purpose of synthesis, it may be enough (in some 

 
Polish Emotional Speech Database – Recording and Preliminary Validation 
43 
realisations) to study a single speaker (only his method of expressing emotion would 
be then modelled), whereas research aimed at recognising emotion needs databases 
encompassing as many signs by which a given emotion may be expressed as possible. 
The most common emotions are anger, sadness, happiness, fear, disgust and surprise. 
Since natural emotions sometimes cannot be easily classified by humans, the majority 
of the databases include simulated emotional speech. Professional actors, drama 
students or amateurs express these emotional utterances. Despite the fact that there 
exists quite a big number of emotional speech datasets collected in the world, most 
work has been done on Germanic languages [3]. The coverage for other language 
groups is rather sparse [4]. As a result, it is difficult to gauge how many of the 
relationships described in the literature are specific to a single language or have a more 
universal nature. For example, in the Japanese society, an open display of emotion may 
be considered anti-social and it is considered normal to show a smile when angry or 
embarrassed [3]. Polish, being one of the biggest European languages, with some 
exceptions for non-emotional speech [7], is still not very often considered in spoken 
language corpuses. The goal of recording a Polish emotional speech database is to 
evaluate people’s and machines’ possibilities of identifying emotional states. A very 
crucial and difficult problem during a database realisation is also selecting conditions 
and demands by selecting proper sets of categories or dimensions that will enable later 
a data integration across studies or the comparison of the obtained results. 
2   Database 
Despite all disadvantages of acted emotions in comparison to natural and elicited ones 
(which means recordings of spontaneous speech), only recording simulated (or semi-
natural) emotions can guarantee the control of recordings which fulfils [3,5]: 
- 
reasonable number of subjects to act all emotions to enable the generalization 
over a target group, 
- 
all subjects uttering the same verbal content to allow the comparison across 
emotions and speakers, 
- 
high quality recordings to enable later  proper speech features extraction, 
- 
unambiguous emotional states (only one emotion per utterance). 
During emotional speech recordings it was planned to rely on speakers’ ability of self-
induction by remembering a situation when certain emotion was felt (known as the 
Stanislavski method [2]). For some limited recordings professional actors will be 
engaged who, according to literature [3], can generate speech that listeners classify 
very reliably (up to 78%). The next step in designing an emotional speech database is 
the selection of categories. The literature describes them by emotion-dimensions (i.e. 
pleasure, activation, etc.) or discrete concepts (i.e. anger, fear etc.) [2]. Distinct terms 
which are easily understood by the speakers are usually chosen in acted emotions. In 
order to be able to compare the results with older studies and because they are 
generally considered as the most common ones, it was decided to use six basic 
emotional states plus the neutral state (despite the fact that there is no definitive list of 
basic emotions, there exists a general agreement on so-called “the big six” [8,9]): 
anger, sadness, happiness, fear, disgust, surprise and neutral state. Since skilled actors 

44 
P. Staroniewicz and W. Majewski 
can simulate emotions in a way that could be confused with truly natural behaviour, 
they are very often used in emotional speech databases recordings [3]. On the other 
hand, sometimes actors could express emotions in quite an exaggerated way [2]. 
Therefore, it was decided that the speakers will be selected from three groups: 
professional actors, amateur actors and amateurs. The speakers will be sex balanced. 
All subjects will be recorded in separate sessions to prevent their influencing each 
other’s speaking styles. The speakers will be asked to use their own every day way of 
expressing emotional states, not from stage acting. The decision of selecting 
simulated emotional states enabled a free choice of utterances to be recorded. The 
most important condition is that all selected texts should be interpretable according to 
emotions and not containing an emotional bias. Two kinds of material could be used: 
nonsense text material or everyday life sentences. Despite the fact that nonsense 
material is guaranteed to be emotionally neutral, it would be difficult to produce 
natural emotional speech spontaneously, which can lead to overacting. The usage of 
everyday life speech has some important advantages: 
- 
it is the natural form of speech under emotional arousal, 
- 
actors can immediately speak it from memory, 
- 
no need for memorising and reading it, which could lead to a lecturing style. 
The ten everyday life phonetically balanced sentences in Polish and their English 
translation are listed in Table 1. 
Table 1. Ten everyday life sentences in Polish and their English translation 
No 
Sentence (in Polish) 
Sentence (English translation) 
1 
Jutro pojdziemy do kina. 
Tomorrow we’ll go to the cinema. 
2 
Musimy sie spotkac. 
We have to meet. 
3 
Najlepsze miejsca sa już zajete. 
The best seats are already taken. 
4 
Powinnas zadzwonic wieczorem. 
You should call in the evening. 
5 
To na pewno sie uda. 
It must work out. 
6 
Ona koniecznie chce wygrac. 
She simply must win. 
7 
Nie pij tyle kawy. 
Don’t drink so much coffee. 
8 
Zasun za soba krzeslo. 
Put the chair back. 
9 
Dlaczego on nie wrocil. 
Why hasn’t he come back. 
10 
Niech się pan zastanowi. 
Think about it. 
 
The database recording were carried out in our recording studio with T-Bone SCT 
700 microphone, Yamaha 03D digital mixer, where the analogue/digital conversion 
was done (44.1kHz, 16bit, mono). The data in S/PDIF standard was then transferred 
to the Soundmax Integrated Digital Audio PC sound card. At the moment the 
recordings of the first group of speakers (i.e. amateurs) are completed. The group of 
amateur speakers consisted of 13 subjects, 6 women and 7 men each recorded 10 
sentences in 7 emotional states in several repetitions. Altogether 2351 utterances were 
recorded, 1168 with women and 1183 with male voice. An average duration of a 
single utterance was around 1 second. After a preliminary validation, some doubtful  
 

 
Polish Emotional Speech Database – Recording and Preliminary Validation 
45 
Table 2. Tested emotional speech database 
Emotional state 
Women 
Men 
Together 
Training set 
Happiness 
69 
83 
152 
Anger 
77 
78 
155 
Fear 
63 
68 
131 
Sadness 
86 
71 
157 
Surprise 
88 
76 
164 
Disgust 
83 
81 
164 
Neutral state 
77 
99 
176 
Together 
543 
556 
1099 
Testing set 
Happiness 
68 
79 
147 
Anger 
88 
69 
157 
Fear 
61 
66 
127 
Sadness 
79 
70 
149 
Surprise 
73 
67 
140 
Disgust 
66 
78 
144 
Neutral state 
74 
81 
155 
Together 
509 
510 
1019 
Altogether 
1052 
1066 
2118 
 
emotional states and recordings with poor acoustical quality were rejected. The Final  
number of 2118 utterances was then divided into training and testing sets for a later 
automatic recognition of emotional states (Table 2). The subjective tests were carried 
out on the testing set. 
3   Database Validation 
For ensuring a high reliability and naturalness of the material chosen for the database, 
perception listener tests have to be carried out. Before the tests some basic principles 
were set up. Firstly, the listeners were presented with the acoustic material in random 
order and listened to each sample, not being allowed to go back to compare them with 
earlier utterances. Each time the decision was made in which emotional state the 
speaker was and how convincing his or her performance was. The automated tests 
were done on a personal computer. The important factors which have to be considered 
during the tests and which can, according to literature [3], influence the results are 
also the listeners’ emotional intelligence, their musical education or even their gender. 
Personal questionnaire of application for subjective recognition tests included age, sex 
and question about musical education of listener. 202 listeners participated in the 
tests. 33 of them were musically educated. Additionally the tests were carried out on 
the group of 27 foreigners not knowing the Polish language. They were students, 
newcomers before starting Polish language lessons from England, Czech Republic, 
France, Slovakia, Switzerland, USA, Ukraine, Hungary and Italy. The Table 3  
 

46 
P. Staroniewicz and W. Majewski 
presents the results of emotional state recognition for all the listeners. It includes the 
mean recognition scores and the number of the listeners who took part in the tests in 
each group. 
 
Table 3. Emotional state recognition for all listeners groups 
 
Mean 
value 
Min. 
value 
Max. 
value 
Standard 
deviation 
No of 
subjects 
Musically non-educated women 
60.41%
44.90%
77.55%
7.77% 
25 
Musically non-educated men 
55.59%
30.61%
77.55%
8.87% 
117 
Musically educated women 
65.17%
51.02%
77.55%
7.01% 
15 
Musically educated men 
60.77%
42.86%
81.63%
9.91% 
18 
Poles altogether 
57.63%
30.61%
81.63%
9.18% 
175 
Foreigners 
54.80%
40.82%
69.39%
7.29% 
27 
Altogether 
57.25%
30.61%
81.63%
8.98% 
202 
 
These general scores indicate that sex and musical education can be significant 
factors in an emotional states classification. The scores for the women are about 5% 
higher than for the men and also very similar difference can be noticed between the 
musically educated and non-educated listeners (Fig.1). Further on some more detailed 
results will be presented. 
48,00%
50,00%
52,00%
54,00%
56,00%
58,00%
60,00%
62,00%
64,00%
66,00%
Musically
non-
educated
women 
Musically
non-
educated
men
Musically
educated
women
Musically
educated
men
Poles
Foreighners
Altogether
 
Fig. 1. Emotional state recognition for all listeners groups 
For the musically non-educated listeners the women tend to recognise emotionally 
marked utterances better, whereas the neutral state was better recognised by men 
(Fig.2).  
 

 
Polish Emotional Speech Database – Recording and Preliminary Validation 
47 
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
Happiness
Anger
Fear
Sadness
Surprise
Disgust
Neutral state 
Fig. 2. Results for musically non-educated listeners (white-women and grey-men) 
In Figure 3 results for the musically educated listeners are presented. And again the 
women recognise most of the emotional states slightly better. 
 
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
Happiness
Anger
Fear
Sadness
Surprise
Disgust
Neutral state
 
Fig. 3. Results for musically educated listeners (white-women and grey-men) 
The results for the groups of Poles and foreigners (Fig.4) are surprisingly similar. 
Despite the fact that the foreigner listeners were newcomers some of them could have 
contact with the Polish language before or their mother tongue belongs to the same 
group of Western Slavonic languages (Czech and Slovak). The foreigners group was 
however too sparse for grouping according to the language families and on this basis 
we could not draw any significant conclusions. 

48 
P. Staroniewicz and W. Majewski 
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
Happiness
Anger
Fear
Sadness
Surprise
Disgust
Neutral state 
Fig. 4. Results for Poles (white) and foreigners (grey) 
The confusion matrix for the listeners altogether is presented in Table 4. The worse 
results had disgust. It could be caused by the fact that the speakers found it extremely 
difficult to express disgust naturally and of course sometimes this emotional state can 
be better expressed by face than by voice. Fear and sadness also caused troubles in a 
proper identification. In this case also the expression of fear can be very difficult for 
speakers. The listeners were confusing sadness with the neutral state. The best results 
were obtained for the recognition of the neutral state. 
Table 4. Confusion matrix for all listeners 
 
Happiness 
Anger 
Fear 
Sadness 
Surprise 
Disgust 
Neutral 
state 
Happiness 
68.18% 
4.38% 
2.76% 
2.33% 
3.61% 
5.73% 
13.01% 
Anger 
2.40% 
71.07% 
1.98% 
0.64% 
2.90% 
8.70% 
12.31% 
Fear 
5.37% 
11.17% 
40.52% 
10.33% 
14.00% 
9.48% 
9.12% 
Sadness 
2.12% 
2.19% 
7.21% 
44.70% 
3.32% 
17.68% 
22.77% 
Surprise 
3.47% 
6.01% 
3.82% 
0.78%
72.49% 
9.41% 
4.03% 
Disgust 
3.82% 
13.58% 
3.47% 
12.02% 
7.71% 
30.41% 
29.00% 
Neutral 
state 
0.50% 
3.68% 
1.63% 
10.82% 
2.76% 
7.21% 
73.41% 
4   Concluding Remarks 
Firstly, our goal was to fill the gap in emotional databases for the Polish language 
since so far no emotional database has existed for this language. 
The Polish database is the first step in the planned research but when finished, it 
will soon let us answer some crucial questions, i.e. emotion recognition for Polish and 
its relations to other languages. The database can be easily expanded in the future to 
other kinds of material or other emotional categories. 
Currently the Polish database of emotional speech is still in the process of subject 
recording. The process of preliminary validation proceeds in parallel. The presented 

 
Polish Emotional Speech Database – Recording and Preliminary Validation 
49 
preliminary results were obtained only for amateur speakers. They revealed influence 
of listeners’ gender and their musical education on subjective recognition scores 
(which confirms the results of some earlier studies [10,11]). 
Next steps will be recording professional and amateur actors which should 
heighten the recognition results. 
 
Acknowledgments. This work was partially supported by COST Action 2102 “Cross-
modal Analysis of Verbal and Non-verbal Communication” and by the grant from the 
Polish Minister of Science and Higher Education (decision nr 115/N-COST/2008/0). 
References 
1. COST Action 2102, Cross-Modal Analysis of Verbal and Non-verbal Communication, 
Memorandum of Understanding, Brussels, July 11 (2006) 
2. Burkhard, F., Paeschkhe, A., Rolfes, M., Sendlmeier, W., Weiss, B.: A Database of 
German Emotional Speech. In: Proc. of Interspeech 2005, Lissabon, Portugal (2005) 
3. Douglas-Cowie, E., Campbell, N., Cowie, R., Roach, P.: Emotional speech: Towards a 
new generation of databases. Speech Communication 40, 33–60 (2003) 
4. Jovcic, S.J., Kasic, Z., Dordevic, M., Rajkovic, M.: Serbian emotional speech database: 
design, processing and evaluation. In: Proc. SPECOM 2004, St. Petersburg, Russia (2004) 
5. Ververdis, D., Kotropoulos, C.: A State of the Art on Emotional Speech Databases. In: 
Proc. of 1st Richmedia Conf. Laussane, Switzerland, October 2003, pp. 109–119 (2003) 
6. Staroniewicz, P.: Polish emotional speech database–design. In: Proc. of 55th Open 
Seminar on Acoustics, Wroclaw, Poland, pp. 373–378 (2008) 
7. Staroniewicz, P., Sadowski, J., Majewski, W.: The SpeechDat(E) database for Polish – 
final remarks and quality valuation (in Polish). In: Proc. of 48th Open Seminar on 
Acoustics 2001, pp. 165–168 (2001) 
8. Cowie, R.: Describing the Emotional States Expressed in Speech. In: Proc. of ISCA, 
Belfast 2000, pp. 11–18 (2000) 
9. Scherer, K.R.: Vocal communications of emotion: A review of research paradigms. Speech 
Communication 40, 227–256 (2003) 
10. Thompson, W.F., Schellenberg, E.G., Husain, G.: Decoding Speech Prosody: Do Music 
Lessons Help? Emotion 4(1), 46–64 (2004) 
11. Nilsonne, A., Sunberg, J.: Group differences in ability to judge emotional states from voice 
fundamental frequency. STL-QPSR 24(2-3), 108–118 (1983) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 50–62, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Towards a Framework of Critical Multimodal  
Analysis: Emotion in a Film Trailer 
Maria Bortoluzzi 
Faculty of Education, Multimedia Science and Technology, University of Udine  
Via Prasecco, 3 – 33170 Pordenone, Italy 
maria.bortoluzzi@uniud.it 
Abstract. This paper presents a pilot study in the analysis of emotion 
integrating approaches from traditionally separate theoretical backgrounds: 
socio-semiotic studies and cognitive studies. The general aim is to identify a 
flexible and comprehensive framework for critical multimodal analysis of 
visual, verbal (oral, written and a blend of the two), kinetic, sound, music and 
graphic aspects. The case-study, exemplifying this kind of integrated approach 
in its initial stages, identifies the voices of emotion as expressed in the trailer of 
the film An Inconvenient Truth (2006) and discusses their intertextual and 
interdiscoursal characteristics. Aspects of the on-going research projects are 
discussed.  
Keywords: Multimodality, Emotion, Socio-semiotic Analysis, Critical Discourse 
Studies. 
1   Introduction 
The analysis of emotion in multimodal texts is particularly complex because it 
includes variables which are usually studied in separate disciplines (linguistics, 
sociolinguistics, neurolinguistics, discourse analysis, pragmatics, iconology, film 
studies, sound and music studies, studies of design, sociology, psychology, neurology, 
etc). This paper presents a pilot study in the analysis of emotion which attempts to 
integrate frameworks derived from traditionally separate and often contrasting 
theoretical backgrounds, such as approaches derived from socio-semiotics and 
cognitive studies. The general aim is to identify variables of analysis and means to 
interpret and evaluate them that can blend into a comprehensive framework visual, 
verbal (oral, written and hybrid), kinetic, sound, music, graphic aspects and their 
affordances. The difficulty lies in the fact that in naturally occurring multimodal texts 
the different aspects become re-semiotised one by the others: ‘Resemiotization is 
about how meaning shifts from context to context, from practice to practice, or from 
one stage of a practice to the next’ (Iedema [31]: 41).  
In particular, this paper will focus on identifying emotion as expressed in multimodal 
texts by different ‘voices’ contributing to it. The term ‘voice’ is used here in as an 
element of Bakhtinian ‘dialogism’ (Bakhtin [4]): ‘Dialogism is the characteristic 
epistemological mode of a world dominated by heteroglossia. Everything means, is 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
51 
understood, as a part of a greater whole – there is a constant interaction between 
meanings, all of which have the potential of conditioning others. Which will affect the 
other, how it will do so and in what degree is what is actually settled at the moment of 
utterance. This dialogic imperative, mandated by pre-existence of the language world 
relative to its current inhabitants, insures that there can be no actual monologue.’ 
(Holquist [29]: 426)  Bakhtin was referring to verbal communication (applying to 
written communication and literature a default characteristic of face-to-face 
communication); ‘dialogism’ can be also expanded from the verbal sphere to 
multimodal aspects of communication triggering processes of resemiotization (see 
Iedema [31]). 
The study outlines the work-in-progress towards a possible multimodal framework 
analysis of emotion; its main aim is assessing some comprehensive analytical tools 
within a multimodal perspective for complex texts such as videos.  The presentation 
of the issue in the following section will be followed by an outline of the theoretical 
studies underlying the analytical framework (section 2.2, 2.3); section 2.4 presents the 
case study and its working hypothesis; section 2.5 reports the methodological 
framework; section 3 discusses and interprets some insights derived from the analysis 
and assesses the framework proposed. 
2   Emotion in Multimodal Voices 
Emotion has been widely investigated in a range of research areas and across 
disciplines. In this section I will briefly outline some of the main research areas in 
linguistics and socio-semiotics which have offered insights into the study of emotion; 
then I will present the theoretical frameworks at the basis of the present study. 
2.1   The Open-Ended Issue of Defining Emotions 
Defining emotion is no easy task, given that over a hundred definitions can be found 
in the literature (see an overview in Oatley et al. [49]); however, there seems at least 
to be consensus in present-day research on the fact that some aspects of emotion are 
universal because biologically determined (Oatley et al. [49]: Ch 6; Ekman [13]; 
Gallois [23]), while other aspects are culturally determined by socialization and 
cultural schemata (Ekman [12]; Turner and Stets [55]; Gallois [23]). 
As Scherer argues: ‘The definition of emotions, distinguishing them from other 
affective states or traits, and measuring them in a comprehensive and meaningful way 
have been a constant challenge for emotion researchers in different disciplines of the 
social and behavioral sciences over a long period of time.’ (Scherer [54]: 31) Among 
the many definitions of emotion, ground-breaking studies in the field of neuroscience  
have been those of Damasio [7, 8, 9] and his group. He has suggested that, as the five 
senses connect the external world to the brain activating nerve patterns, emotions are 
nerve activation patterns that correspond to inner states.  
In psychology, LeDoux’s [41] seminal study of emotion offers a working 
definition thus summarised by Fedeli [19]: feeling an emotion consists in 
physiological, behavioural and thought alterations happening simultaneously with the 
following characteristics: ‘a) [emotions] are based on body and behaviour alterations; 

52 
M. Bortoluzzi 
b) they tend to be outside the voluntary control of the individual since they are 
mediated by the limbic system; c) they appear before the cerebral cortex has a 
conscious representation of what is happening.’ (Fedeli [19]: 23; my translation).  
Although there is a variety of definitions, most studies in different areas remark 
that emotion includes ‘an eliciting condition, a cognitive evaluation, physiological 
activation, a change of action readiness, and finally an action’ (Johnson Laird & 
Oatley [32]: 82). In communication all this can be overtly or covertly expressed, and 
directness or indirectness varies with culture and sub-culture variations, group 
variations, even personal idiosyncratic variations; additionally there are variation 
patterns across text-types and communicative events. 
2.2   Linguistic Approaches to Emotions 
The linguistic literature about emotions in communication is vast and has been 
explored in the following areas of research as reported by Bednarek ([5]: 7-9 for an 
overview of main studies in these fields): cognitive, cross-linguistic, linguistic-
anthropological, 
diachronic, 
functional, 
syntactic, 
conversation 
analytic, 
stylistic/literary, psycholinguistic, pragmatic/textlinguistic, systemic-functional, etc. 
Many cross-cultural and interdisciplinary studies on emotions fall beyond Benarek’s 
list; among many others, Fussell [22] includes contributions about the relation 
between verbal and non-verbal communication, cross-cultural studies, everyday and 
therapeutic communication, etc.; Robinson [52] is a seminal study on emotions in 
literature, music and art; Vincent and Fortunati [64] an interdisciplinary analysis of 
emotion in ICTs; Esposito et al. [14] which focuses on multimodal signals and their 
interaction (including oral and written language). 
Bednarek mentions that a fundamental distinction cuts across all these approaches: 
the study of language about emotions (called by Bednarek emotion talk) that is all the 
expressions that denote emotions/affect, and the study or language as emotions 
(called by Bednarek emotional talk) that is all those constituents that signal 
emotions/affect (e.g. paralinguistic features, some metaphoric expressions, emphasis, 
repetitions, etc) (Bednarek [5]: 11). The present study, because of its multimodal 
nature, will necessarily deal with both aspects, as will be shown in the next sections. 
Another fundamental distinction in linguistic research relates to how emotions are 
portrayed by the speaker/writer and how texts create an emotional response in the 
adressee (listener/reader). This is a pilot study exploring possible frameworks of 
analysis which can include both aspects; however, the latter is only touched upon as 
the early stages of tentative analysts’ response, while an audience-response study has 
not yet been set in place at the time of writing. 
2.3   Theoretical Studies for an Integrated Approach to Multimodal Analysis 
The main tenet at the basis of the study is that the construction of voices in multimodal 
texts for publics and communities of practice1  in situated actions is influenced by overt 
and covert ideologies2 conveyed by the powerful interaction of verbal and non–verbal 
                                                           
1 Wenger [65], Riley [51], Cortese and Duszak [6]. 
2 See, among others, van Dijk [56, 57, 58, 59]; Fairclough [15, 16, 17, 18]; Weiss and Wodak, 
[64]; Martin and Wodak [45]; Wodak [69, 70]. 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
53 
communication and constructed at times as non-negotiable common sense. The study is 
work-in-progress which aims at identifying flexible and complementary tools of 
analysis for further research in sets (or, rather, corpora) of naturally occurring 
multimodal texts. Given the complexity of naturally occurring data of this kind, the pilot 
study presented here attempts at integrating approaches which are traditionally kept 
separate in research: recent socio-semiotic developments contribute to identifying 
situated actions and explore multimodal communicative practices; recent developments 
in cognitive approaches to metaphor contribute to bridging the gap between culturally 
and cognitively influenced communication. It is thus hoped to adopt critical tools  
to uncover and discuss underlying ideologies which tend to become ‘transparent’  
(i.e. not easily noticeable) when mediated through a variety of communicative 
strategies multimodally co-deployed. The more ‘transparent’ and ‘commonsensical’ 
the communication is, the lower the degree of negotiation the text-user adopts towards 
the text while the critical stance the reader/user has towards the text tends to become 
less effective (see Fairclough [16, 17]; van Dijk [59]). 
The theoretical studies at the basis of the integrated approach draw, on the one 
hand, on recent developments of systemic functional linguistics in a social semiotic 
perspective, including its applications to the analysis of visual grammar and 
multimodality in general, as well as on seminal studies in the field of Critical 
Discourse Studies and Appraisal Theory. 
The framework of analysis for multisemiotic/multimodal texts (i.e. combining 
verbal and non-verbal meaning-making resources which are co-deployed to produce 
an overall textual meaning) is based on the multisemiotic studies of Kress and van 
Leeuwen [36, 37, 38], Iedema [31], van Leeuwen [60, 61], Baldry [1, 2], O’Halloran 
[50], Baldry and Thibault [3]. These studies have at their basis a systemic functional 
view of linguistics (Halliday [25]; Halliday and Matthiessen [26]; Martin [43]; Davies 
and Ravelli [10]; Hasan, Matthiessen and Webster [27]). 
Another development stemming from systemic functional studies is Critical 
Discourse Analysis (CDA), also called Critical Discourse Studies, the branch of 
linguistics and social semiotics that offers a fundamental contribution to the 
exploration of ideology. As van Dijk points out, CDA is not a new school, but rather 
it is a necessary 'critical' perspective which contextualises, interprets and evaluates 
linguistic and multimodal data as never value-free but as the result of power struggles 
(Wodak and Meyer [71]; Fairclough [15, 16, 17, 18]; Wodak [69, 70]; Lassen et al., 
[40]; van Dijk [58, 59]).  
In connection to CDA, but also to the wider area of Discourse Analysis, the concepts 
of intertextuality and interdiscursivity are central to this study. Intertextuality has been 
investigated over the years as one of the pervasive characteristics of communication and 
connected to what has been called by Bakhtin heteroglossic and dialogic aspects of texts 
[4]. For the present study I will adopt Fairclough’s definition of intertextuality (derived 
from Bakhtin [4]) and the related concept of interdiscursivity, while I will extend both 
to visual and aural communication: 
‘The intertextuality of a text is the presence within it of elements of other texts 
(and therefore potentially other voices than the author’s own) which may be related to 
(dialogued with, assumed, rejected, etc.) in various ways [...].’ (Fairclough [17]: 218). 

54 
M. Bortoluzzi 
‘Analysis of the interdiscursivity of a text is analysis of the particular mix of genres, 
of discourses, and of styles upon which it draws, and of how different genres, discourses 
or styles are articulated (or ‘worked’) together in the text.’ (Fairclough [17]: 218). 
A recent development of systemic functional linguistics and closely related to the 
investigation of emotion is Appraisal Theory: it focuses on the devices used to 
construe (overtly or covertly) an evaluative stance in texts (White [66, 67]; Martin 
and White [44]; see below). The novelty of the present study is exploring the use of 
this framework for multimodal naturally occurring data. 
On the other hand, the exploration is complemented by using insights from a 
disciplinary area usually kept separate from socio-semiotic approaches: latest 
developments in cognitive discourse studies and, in particular, the cognitive analysis 
of metaphorical expressions (Kővecses [33, 34]; Goatly [24], Semino [53]; Forceville 
[20, 21]). As Semino summarises: ‘Conceptual metaphors are defined as systematic 
sets of correspondences, or, ‘mappings’, across conceptual domains, whereby a 
‘target’ domain (e.g. our knowledge about arguments) is partly structured in terms of 
a different ‘source’ domain (e.g. our knowledge about war) [...]. Conceptual domains 
are rich mental representations: they are portions of our background knowledge that 
relate to particular experiences of phenomena [...].’ (Semino [53]: 5). 
The analysis of metaphors as related to emotion within cognitive linguistics is the 
focus of Kővecses [33, 34] and, more importantly for the present study, Goatly [24], a 
study based on large corpora of data. Cognitive linguistics focuses on verbal 
conceptual metaphors and studies referring to other communicative modes are not 
many (Robinson [52], Forceville [20, 21]).  
Metaphors will be analysed not only in language but also in moving images and 
sounds trying to capture the combined effect of  resemiotised affordances.  
2.4   A Multimodal Case-Study 
The case-study presented here is used to exemplify the kind of work-in-progress 
being done and to assess the integrated approach of analysis; the choice is a short but 
complex multimodal text: the trailer of the film An Inconvenient Truth (2006) [73], an 
urgent call-for-action against global warming by Al Gore and his collaborators. The 
discussion that follows necessitates viewing the short trailer (2.30 minutes [73], on 
the official website of the film [72]). 
The choice of this text is due to its multimodal complexity as naturally occurring 
data (with visual, kinetic, sound, verbal affordances superimposing one to the other) 
and the variety of ostensible and covert voices interacting and even, as we shall see 
‘competing’ to convey complex messages in a very limited span of time. This trailer 
is therefore an interesting touchstone to test the possibilities and the limits offered by 
the integrated framework of analysis.    
The trailer promotes a film/documentary committed to denounce a human problem 
at the world level: global warming. Ostensibly the film and, as a consequence, its 
trailer belong to socially committed texts promoting a more responsible attitude 
towards ecological issues. Since social campaigns tend to be perceived by the general 
public as either ‘neutral’ and ‘objective’ or ideologically-correct by default, my aim 
was to analyse the underlying ideological tenets multimodally conveyed and check 
whether values and voices portrayed in the trailer through communication related to 
emotions are conveyed and construed (depending also on the communities of 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
55 
stakeholders directly or indirectly addressed) in ways which are consistent or in 
contradiction with the ostensible aims and general principles of the film which is 
promoted. A starting hypothesis is that the promotional function of the trailer heavily 
influences the message conveyed because some of the film voices are edited and 
transformed into a highly emotional advertising hybrid text. 
2.5   Methodological Issues: The Framework of Multimodal Analysis 
The framework of analysis for multimodal texts developed by Baldry and Thibault  
[3: 195-200] and Baldry [1, 2] has been adapted and changed to focus on variables 
conveying emotion and voices through the different interacting modes. The trailer has 
been divided into shots (using Adobe Premiere) and the intersemiosis of different 
affordances was qualitatively analysed shot by shot throughout the video (see Table 
1). The first sets of variables marked with # are derived and adapted from Baldry and 
Thibault [3], whereas the categories in the lower part of the table derive from the 
insights offered by the first four sets of variables and integrated with other studies and 
approaches (see Sections 2.3 and 3). 
Table 1. Framework of multimodal analysis 
# SHOT AND VISUAL FRAME 
Progressive number of shot (a single, uninterrupted run of the camera). Span 
of time in seconds 
# CAMERA POSITION AND MOVEMENT 
Point of view offered to the viewer 
Stationary camera: distance, perspective, camera angle 
Moving camera (different categories of movement): distance, perspective, vector of 
movement, camera angle 
# KINESIC ACTION AND LOCATION OF ELEMENTS/PARTICIPANTS 
Movement or stationary position of participants/elements (human, non-human, objects, 
shapes, etc) and type of interaction among them (environmental events) 
Salience of elements/participants 
# SOUNDTRACK 
Sounds 
Music 
Transcript of auditory verbal soundtrack (off screen voice and voice of participants) of 
shot 
 
MULTIMODAL METAPHORS 
Interaction of verbal and non-verbal metaphors 
EVALUATIVE STRATEGIES 
Verbal (Appraisal Theory) and non-verbal devices  
INTERTEXTUALITY AND INTERDISCURSIVITY 
Quotes (clips/images/sound/verbal communication, etc) from the film (visual, verbal, 
sounds, etc) and changes in use and context of these quotes 
Discoursal conventions referring to the film An Inconvenient Truth or/and to other 
multimodal texts (use of images, editing, sound, verbal communication, etc) 

56 
M. Bortoluzzi 
The detailed qualitative analysis, which cannot be reported here in its entirety, 
yielded a wealth of insights and different strands for interpretation in context of the 
trailer. It also revealed a series of problematic issues related to its use for larger scale 
texts (much longer videos and multimodal corpora) due to the time-consuming 
procedure and the need of cross-checking by different analysts.  
In the following sections I will summarise some of the results offered by the data 
analysis as integrated into the last three sets of categories. 
3   Voices and Emotion 
The sections that follow are based on the close analysis of the trailer which needs to 
be watched to follow the interpretation proposed (see website [73]). 
3.1   Multimodal Metaphors 
Verbal studies in conceptual metaphors of emotions refer to physical and sound 
aspects which are metaphorically represented in videos in a variety of affordances. 
Goatly [24] identifies the following metaphorical areas related to emotions: 
‘EMOTION IS FORCE, EMOTION IS MOVEMENT, EMOTION IS A CURRENT 
IN A LIQUID, EMOTION IS WEATHER’ (ibid.: 197-206). In a wider sense, 
‘EMOTION IS SENSE IMPRESSION (IMPACT / TOUCH > HEAT / 
EXPLOSION)’ (Goatly [24]: 224).  
In the trailer this is verbally substantiated by the following expressions:  
 
standing ovations, shake you to your core; global warming; hurricane Katrina 
slammed into New Orleans; A film that shocked audiences; If this were to go, sea 
levels would go up twenty feet; Think of the impact of a hundred thousand refugees 
and then imagine a hundred million 
 
The most powerful metaphors of sense impression, however, are expressed in the 
fast succession of moving images edited in the trailer, in which visual metaphors of 
force, weather, liquid, current, impact, heat and movement blend with sounds 
representing and evoking impact, movement, speed and extreme weather conditions. 
Fast and emphatic background music reminiscent of disaster movies further increases 
the emotional level of communication; this alternates with shots accompanied by 
silence or ‘auditory bullets’ contributing to an atmosphere of emotional suspense. 
Visual and auditory metaphors enhance the most noticeable feature of the 
metaphorical verbal expressions related to emotions, that is the direct involvement of 
the viewer as the participant who undergoes the experience (it will shake you to your 
core), while the Actor (in Halliday’s grammar [25], the participant who is responsible 
for setting the event in motion) tends to be the weather or the film:  
 
hurricane Katrina slammed into New Orleans; a film that shocked audiences; if 
this were to go sea levels would go up twenty feet. 
 
By using a variety of semiotic codes, the trailer widely exploits the metaphor 
EMOTION IS WEATHER and powerfully communicates the message WEATHER 
IS EMOTION. 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
57 
The second set of emotional metaphoric expressions is related to moral values: 
 
MORAL VALUE: did the planet betray us or did we betray the planet? Threat; at 
stake. 
 
Verbal communication is carefully woven into the trailer through graphics, voice 
quality and editing to enhance the emotional involvement of the viewers but also to 
expand on the second set of metaphoric expressions, MORAL VALUE, linked to 
evaluative strategies, as shown in the next section. 
3.2    Emotion through Evaluative Strategies 
The verbal communication of the trailer is evaluative throughout, thus Appraisal 
Theory (Martin and White [44], White [66, 67]) is adopted as framework of analysis. 
The most common device used by the voices in the trailer can be inscribed into the 
category ‘Judgement’ which is the language ‘which criticizes or praises, which 
condemns or applauds the behaviour – the actions, deeds, sayings, beliefs, 
motivations etc – of human individuals and groups.’ (White [66]). In the text, the 
overt Judgement expressed as a generalised statement Scientific consensus is that WE 
are causing global warming is highlighted and sanctioned by the applause that 
follows and directs the viewer to identify with the audience in the lecture theatre. The 
exemplification of how we cause global warming is achieved by editing black and 
white images of traffic jams and congestion in cities and roads. From then on, the 
reference to apparently ‘neutral’ weather changes are all referred to human action and 
therefore cannot be analysed as Appreciation (which evaluates the result itself), but 
rather as inscribed (overt) or provoked (not explicitly stated but rather clear in 
linguistic terms) Judgement. 
Verbal communication also contains some expression of Affect, that is expressions 
more directly concerned with emotions and with positive and negative emotional 
responses and dispositions (White [67]). The very few instances of Affect are either 
included in promotional features or they are related to the second metaphorical field 
of MORAL VALUES, e.g. the metaphorical use of the verb betray repeated twice, 
highlighted by ‘auditory bullets’ expressed in a rhetorical question: 
 
DID THE PLANET BETRAY US... 
OR DID WE BETRAY THE PLANET?  
 
The analysis of evaluative language shows that Judgement is by far the most 
frequent function in the script of the trailer. These expressions are both emotional and 
overtly linked to moral values which, as a metaphorical target domain, is rather more 
difficult to render in images and sound than in words. The multimodal analysis points 
towards a tendency (in this specific text) for semiotic codes to specialize: verbal 
communication expresses more easily judgemental aspects, as well as emotional 
involvement, whereas the editing of images, sounds (music and sound ‘bullet points’, 
voice quality and paralinguistic features), graphic devices (which powerfully 
emphasise some verbal elements) tend to arouse basic emotions based on sense 
impression and movement. 

58 
M. Bortoluzzi 
3.3   Blending Multimodal Voices 
The main tenet of the analysis is that the device of arousing emotion in the film has 
the aim of personally involving the viewer and making the call-to-action more 
effective. The basic hypothesis of the present study is that enhancing the arousal of 
powerful emotions in the viewer by editing the trailer’s intertextuality and 
interdiscursivity differently from its original source (the film) has the backlash of 
inscribing the film into categories which belong to fiction rather than a documentary 
to raise social responsibility. This may have the positive effect, ostensibly clear in the 
promotional function of the genre ‘trailer’ (see Maier [42]), of attracting more 
viewers, but also the negative effect of importing the impression of ‘unreality’ or dejà 
vu typical of action and disaster movies.  
One of the most noticeable characteristic of the film An Inconvenient Truth  is 
intertextuality, interdiscursivity and genre mixing (e.g. documentary of popularization 
of science, journalistic report, newscasting, personal narrative, and so on). The trailer 
takes this feature to the extreme, enhancing mainly those aspects related to heightened 
emotions. I will not linger here on the complex hybrid quality of the film, but rather 
focus on the way the trailer, a hybrid text by default (Dusi [11]), conveys heightened 
emotions. 
Summarising a complex analysis of voices (verbal and non-verbal), it is possible to 
identify three main clusters of overlapping ostensible and non-ostensible 
addressers/narrators which the masterful editing of the trailer seamlessly blend into 
one intertextual and interdiscoursal hybrid text. 
The cluster related to the voiceover narrator is represented by oral and written 
voices, variety of images, editing, music (intertitles, ‘auditory bullets’, silence, rapid 
successions of images in editing, soundtrack underlining speed, suspense, impact, 
deep and emphatic male narrators voice, etc) closely interacts with the other main 
ostensible addresser represented by Al Gore who identifies a set of discoursal and 
social roles (offscreen voice, main character, storyteller, spokesperson for the 
scientific community, the former politician, the embodiment of a moral commitment 
for a cause of public concern, the public speaker, media expert, etc). The third 
overlapping cluster of voices comes from the news world and global media (televised 
news reels, television reports, weather forecast, satellite images, etc). 
Two apparently contradictory tendencies to globalise/generalise and personalise 
blend seamlessly in the final product: the message is addressed globally to the 
communities of practice mainly belonging to the industrialised world or to the fast 
developing world (China, for instance) (mainly the moral message as embodied by the 
‘Al Gore cluster’), but it is also construed in such a way as to involve emotionally the 
individual viewer, by placing him or her in the position of being directly addressed by 
these voices (mainly the highly emotional instances as expressed by the ‘voiceover 
narrator cluster’). The ‘global media cluster’ tends to be a cross-over and convey both 
judgemental, moral and highly emotional messages as resemiotised by the editing in 
the trailer. 
The inherently intertextual and interdiscursive quality of the trailer blends the 
discourse of popularization of science and the genres ‘documentary’, ‘journalistic 
investigation’, ‘report’ and ‘public lecture’ derived from the film, but it does so with a 
heightened emotional quality of involvement. See for instance the overtly emotional 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
59 
over emphasised language directly related to the film (underlined by graphics and 
powerful sound and music) which is intended as an overtly promotional section in the 
trailer: 
 
BY FAR THE MOST TERRIFYING FILM YOU WILL EVER SEE 
And the end of the trailer: 
NOTHING IS SCARIER 
THAN THE TRUTH 
Our ability to live is what is at stake. 
AN INCONVENIENT TRUTH 
 
The implication is that the trailer promotes the most terrifying of movies because 
the issue at stake is not fiction, but a real world situation; however, this is 
discoursively a rather commonly used device in horror and disaster movies to arouse 
fear and attract audiences presenting situations as ‘real’ whilst it is actually fictional 
narrative. In the case of a real threat for humanity, the interdiscursivity of fiction used 
to enhance the impact of promotional communication on wide audiences might have 
both the positive effect of attracting the prospective viewer and conveying to large 
audiences the alarming message put across by the film, but also it might create the 
effect of dejà vu and fiction, which would confirm the starting hypothesis of the study 
(see section 2.4). 
4   Concluding Remarks 
The main aim of this paper was to report about the initial stages of an on-going study 
to identify a framework of critical multimodal analysis for complex texts such as 
videos focusing on the communication of emotion through verbal and non-verbal 
devices. The study has presented a single case-study and cannot be generalised, but it 
has confirmed the necessity of a framework integrating socio-semiotic and cognitive 
approaches within critical discourse studies. A corpus of different genres and 
multimodal text-types is being collected in order to assess the tools of analysis on a 
variety of texts, to verify whether these tools can be used for different multimodal 
text-types.  
The complexity of interpreting how voices blend and how affordances resemiotise 
meaning in specific contexts and for specific communities of practice remains an 
open-ended issue as well as the question whether such an integrated analytical 
approach can be transformed into a quantitative study on a large corpus of data. These 
are the areas that will be explored in the on-going project. 
References 
1. Baldry, A. (ed.): Multimodality and Multimediality in the Distance Learning Age. 
Palladino, Campobasso (2000) 
2. Baldry, A.: A Multimodal Approach to Text Studies in English. Palladino, Campobasso 
(2005) 

60 
M. Bortoluzzi 
3. Baldry, A., Thibault, P.J.: Multimodal Transcription and Text Analysis Equinox, London 
(2006) 
4. Bakhtin, M.: The Dialogical Imagination. University of Texas Press, Austin (1981) 
5. Bednarek, M.: Emotion Talk across Corpora. Palgrave Macmillan, Basingstoke (2008) 
6. Cortese, G., Duszak, A. (eds.): Identity, Community, Discourse. English in Intercultural 
Settings. Peter Lang, Bern (2005) 
7. Damasio, A.R.: Looking for Spinoza: Joy, Sorrow and the Feeling Brain. Harvest Books, 
Washington (2003) 
8. Damasio, A.R.: Emotion in the perspective of an integrated nervous system. Brain 
Research Reviews 26(2-3), 83–86 (1998) 
9. Damasio, A.R.: The Feeling of What Happens: Body, Emotion and the Making of 
Consciousness, London, Heinemann (1999) 
10. Davies, M., Ravelli, L. (eds.): Advances in Systemic Linguistics. Recent Theory and 
Practice. Pinter, London (1992) 
11. Dusi, N.: Le forme del trailer come manipolazione intrasemiotica. In: Pezzini, I. (ed.) 
Trailer, spot, clip, siti, banner, Le forme brevi della comunicazione audiovisiva, pp. 31–66. 
Meltemi, Roma (2002) 
12. Ekman, P.: Should we call it expression or communication? Social Science 
Research 10(4), 333–344 (1997) 
13. Ekman P.: Basic Emotions (1999), http://www.paulekman.com/pdfs/basic_ 
emotions.pdf (last accessed, 20-1-2009)  
14. Esposito, A., et al. (eds.): Multimodal Signals: Cognitive and Algorithmic Issues. Springer, 
Berlin (2009) 
15. Fairclough, N. (ed.): Critical Language Awareness. Longman, London (1992) 
16. Fairclough, N.: Language and Power, 2nd edn. Pearson Education, London (2001) 
17. Fairclough, N.: Analysing Discourse: Textual Analysis for Social Research. Routledge, 
London (2003) 
18. Fairclough, N.: Language and Globalization. Routledge, London (2006) 
19. Fedeli, D.: Emozioni e successo scolastico. Carocci, Roma (2006) 
20. Forceville, C.: Pictorial Metaphor in Advertising. Routledge, London (1996) 
21. Forceville, C.: Metaphor in Pictures and Multimodal Representations. In: Gibbs, R. (ed.) 
The Cambridge Handbook of Metaphor and Though, pp. 462–481. C.U.P., Cambridge 
(2008) 
22. Fussell, S.R.: The Verbal Communication of Emotions. In: An Interdisciplinary 
Perspective. Lawrence Erlbaum Associates, Mahwah (2002) 
23. Gallois, C.: Group membership, social rules, and power: a socio-psychological perspective 
on emotional communication. Journal of Pragmatics 22, 301–324 (1994) 
24. Goatly, A.: Washing the Brain – Metaphor and Hidden Ideology. John Benjamins, 
Amsterdam (2007) 
25. Halliday, M.A.K.: An Introduction to Functional Grammar, 2nd edn. Edward Arnold, 
London (1994) 
26. Halliday, M.A.K., Matthiessen, C.M.I.M.: An Introduction to Functional Grammar, 3rd 
edn. Edward Arnold, London (2004) 
27. Hasan, R., Matthiessen, C., Webster, J. (eds.): Continuing Discourse on Language. A 
Functional Perspective. Continuum, London (2005) 
28. Hodge, R., Kress, G.: Language as Ideology, 2nd edn. Routledge, London (1993) 
29. Holquist, M.: Glossary. In: Bakhtin, M. (ed.) The Dialogical Imagination, pp. 423–434. 
University of Texas Press, Austin (1981) 

 
Towards a Framework of Critical Multimodal Analysis: Emotion in a Film Trailer 
61 
30. Hunston, S., Thompson, G. (eds.): Evaluation in Text: Authorial Stance and the 
Construction of Discourse. Oxford University Press, Oxford (2000) 
31. Iedema, R.: Multimodality, resemiotization: extending the analysis of discourse as multi-
semiotic practice. Visual Communication 2(1), 29–57 (2003) 
32. Johnson-Laird, P.N., Oatley, K.: The language of emotions: An analysis of a semantic 
field. Cognition and Emotion 3, 81–123 (1898) 
33. Kővecses, Z.: Metaphor and emotion. Cambridge University Press, Cambridge (2000) 
34. Kővecses, Z.: Metaphor in Culture. Cambridge University Press, Cambridge (2005) 
35. Kress, G.: Literacy in the New Media Age. Routledge, London (2003) 
36. Kress, G., van Leeuwen, T.: Multimodal Discourse. Edward Arnold, London (2001) 
37. Kress, G., van Leeuwen, T.: Colour as a semiotic mode: notes for a grammar of colour. 
Visual Communication 1(3), 343–368 (2002) 
38. Kress, G., van Leeuwen, T.: Reading Images: The Grammar of Visual Design. Routledge, 
London (2006) 
39. Lakoff, G., Johnson, M.: Metaphors We Live By. The Chicago University Press, Chicago 
(1980)  
40. Lassen, I., Strunck, J., Vestergaard, T. (eds.): Mediating Ideology in Text and Image. John 
Benjamins, Amsterdam (2006) 
41. Ledoux, J.: Il cervello emotivo. Baldini and Castoldi, Milano (1998) 
42. Maier, C.D.: The Promotional Genre of Film Trailers: Persuasive Structures in a 
Multimodal Form. Unpublished Ph.D Thesis, Aarhus school of Business, University of 
Aarhus, Denmark (2006) 
43. Martin, J.R.: English Text: System and Structure. John Benjamins, Amsterdam (1992) 
44. Martin, J.R., White, P.R.R.: The Language of Evaluation: Appraisal in English. Palgrave, 
London (2005) 
45. Martin, J.R., Wodak, R. (eds.): Re/reading the past. Critical and functional perspectives on 
time and value. John Benjamins, Amsterdam (2003) 
46. Niemeier, S., Dirven, R. (eds.): The Language of Emotions. John Benjamins, Amsterdam 
(1997) 
47. Norris, S.: Analysing Multimodal Interaction: A Methodological Framework. Routledge, 
London (2004) 
48. Norris, S., Jones, R.H. (eds.): Discourse in Action. Introducing Mediated Discourse 
Analysis. Routledge, London (2005) 
49. Oatley, K., Keltner, D., Jenkins, J.: Understanding Emotions. Blackwell, Oxford (2006) 
50. O’Halloran, K. (ed.): Multimodal Discourse Analysis. Systemic-Functional Perspectives. 
Continuum, London (2004) 
51. Riley, P.: Epistemic Communities: The Social Knowledge System, Discourse and Identity. 
In: Cortese, G., Riley, P. (eds.) Domain-specific English. Textual Practices across 
Communities and Classrooms, pp. 41–64. Peter Lang, Bern (2002) 
52. Robinson, J.: Deeper than Reason: Emotion and Its Role in Literature, Music, and Art. 
O.U.P., Oxford (2005) 
53. Semino, E.: Metaphor in Discourse. C.U.P., Cambridge (2008) 
54. Scherer, K.R.: What are Emotions? And how can they be measured? Social Science 
Information 44(4), 695–729 (2005) 
55. Turner, J., Stets, J.E.: The Sociology of Emotions. C.U.P., Cambridge (2005) 
56. Van Dijk, T.A.: Critical Discourse Analysis. In: Schriffrin, D., Tannen, D., Hamilton, H.E. 
(eds.) The Handbook of Discourse Analysis, pp. 352–371. Blackwell, Oxford (2001) 
57. Van Dijk, T.A.: The Discourse-Knowledge Interface. In: Weiss, G., Wodak, R. (eds.) 
Methods of Critical Discourse Analysis, pp. 95–120. Sage, London (2003) 

62 
M. Bortoluzzi 
58. Van Dijk, T.A.: Contextual Knowledge Management in Discourse Production. A CDA 
Perspective. In: Wodak, R., Chilton, P.A. (eds.) New Agenda in CDA, pp. 71–100. 
Benjamins, Amsterdam (2005) 
59. Van Dijk, T.A.: Discourse and Power. Palgrave Macmillan, Basingstoke (2008) 
60. Van Leeuwen, T.: Moving English: The Visual Language of Film. In: Goodman, S., 
Graddol, D. (eds.) Redesigning English: New Texts, New Identities, pp. 81–105. 
Routledge, London (1996) 
61. Van Leeuven, T.: Speech, Music, Sound. Macmillan, London (1999) 
62. Van Leeuwen, T., Machin, D.: Global Media Discourse. Routledge, London (2007) 
63. Vincent, J., Fortunati, L. (eds.): Electronic Emotion, the Mediation of Emotion via 
Information and Communication Technologies. Peter Lang, Oxford (2009) 
64. Weiss, G., Wodak, R. (eds): Critical Discourse Analysis. Theory and Interdisciplinarity. 
Palgrave Macmillan, Basingstoke (2002)  
65. Wenger, E.: Communities of Practice, Learning, Meaning and Identity, 2nd edn. 
Cambridge University Press, Cambridge (1999) 
66. White, P.R.R.: Appraisal – The Language of Evaluation and Intersubjective Stance (2005), 
http://www.grammatics.com/appraisal/ (last accessed, 20-1-2009)  
67. White, P.R.R.: Evaluative semantics and ideological positioning in journalistic discourse. 
A new framework for analysis. In: Lassen, I., Strunck, J., Vestergaard, T. (eds.) Mediating 
Ideology in Text and Image, pp. 37–67. John Benjamins, Amsterdam (2006) 
68. Wierzbicka, A.: Emotions across languages and cultures: Diversity and Universals. 
Cambridge University Press, Cambridge (1999) 
69. Wodak, R.: What CDA is about – a summary of its history, important concepts and its 
developments. In: Wodak, R., Meyer, M. (eds.) Methods in Critical Discourse Analysis, 
pp. 1–13. Sage, London (2001) 
70. Wodak, R.: Images in/and news in a globalised world. In: Lassen, I., Strunck, J., 
Vestergaard, T. (eds.) Mediating Ideology in Text and Image, pp. 1–16. John Benjamins, 
Amsterdam (2006) 
71. Wodak, R., Meyer, M. (eds.): Methods in Critical Discourse Analysis. Sage, London 
(2001) 
72. An Inconvenient Truth. A Global Warning. Paramount Classics, directed by Davis 
Guggenheim, U.S.A. (2006), http://www.climatecrisis.net (last accessed, 3-4-
2009)  
73. An Inconvenient Truth. A Global Warning. Paramount Classics, directed by Davis 
Guggenheim, U.S.A. (2006), http://www.climatecrisis.net/trailer/ (last 
accessed, 3-4-2009)  

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 63–75, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Biosignal Based Emotion Analysis of Human-Agent 
Interactions 
Evgenia Hristova, Maurice Grinberg, and Emilian Lalev 
Central and East European Center for Cognitive Science, New Bulgarian University, 
1618 Sofia, Bulgaria 
ehristova@cogs.nbu.bg, mgrinberg@nbu.bg, elalev@cogs.nbu.bg 
Abstract. A two-phase procedure, based on biosignal recordings, is applied in 
an attempt to classify the emotion valence content in human-agent interactions. 
In the first phase, participants are exposed to a sample of pictures with known 
valence values (taken from IAPS dataset) and classifiers are trained on selected 
features of the biosignals recorded. During the second phase, biosignals are 
recorded for each participant while watching video clips with interactions with 
a female and male ECAs. The classifiers trained in the first phase are applied 
and a comparison between the two interfaces is carried on based on the 
classifications of the emotional response from the video clips. The results 
obtained are promising and are discussed in the paper together with the 
problems encountered, and the suggestions for possible future improvement. 
1   Introduction 
Emotional reactions and users’ satisfaction are an important factor in usability 
evaluation of human-agent interactions. A commonly used method for gathering such 
information is by self-administered questionnaires. An additional method that is used to 
assess the emotional reactions is by means of biosignals. Traditionally, these measures 
are used to study the attitudes and emotional experiences in various situations [1], [2]. 
Recently, they have been more and more often used to study the emotional experiences 
of users during game playing, video watching, web site browsing or while interacting 
with software applications [3], [4], [5], [6].  Among the most commonly used biosignals 
are the galvanic skin response, cardiovascular measures, respiration, and facial electro-
myogram. Using such measures, one can gain information not only about stronger 
emotions experienced consciously, but also about weaker emotions which would not 
have been reported by participants in human-agent interaction. The latter stresses the 
importance of biosignal recording based methods in gathering objective information 
about emotional states without relying on introspection or retrospection. 
At the same time, the problem of identifying emotions using biosignals is 
notoriously difficult, although recently there have been many attempts to build 
automatic emotion recognition systems [7], [8]. In refs. [7], [8], [9], [10] various data 
processing and classification methods have been compared trying to achieve high 
sensitivity and precision of emotion classification. While the results for one subject 
seem quite good with over 95% recognition rate, subject-independent classification 

64 
E. Hristova, M. Grinberg, and E. Lalev 
seldom reaches 70 %. Another unsolved problem is generalization over more than one 
task, i.e. applying classifiers trained in one situation (e.g. music listening) to another 
situation (e.g. video watching). 
In the present paper, we have two goals. First of all, we propose and assess a 
methodology for emotion valence assessment based on classifier training (calibration) 
in one task (phase 1) and application of the classifiers in another task (phase 2). The 
former task has a known emotional response, while the latter is the task for which the 
emotional response is to be determined. The second goal is to apply this methodology 
in a real study aimed at analyzing the emotional content of the interactions with an 
embodied conversational agent (ECA) and to choose the interface with the more 
positive valence of the elicited emotions.  
In order to achieve these goals, we recorded biosignals and collected subjective 
ratings. We accounted for the above mentioned difficulties in interpreting biosignals 
by trying to classify emotional responses only with respect to their valence – positive, 
negative, or neutral. The other known dimensions of the emotional space (such as 
arousal or strength of the emotions), or specific emotional categories (e.g. fear, 
disgust, etc.) were not considered. 
The ECA interfaces used have been developed for the RASCALLI multi-agent 
architecture and are male and female digital characters [11]. Via a multimodal system, 
the agents can interact with the user in various ways – text, speech, and gesture. The 
believability of the agents and the quality of the interaction are crucial for the 
usability of the platform as users are expected to interact with the agent for long 
periods of time using them as personal assistants. 
2   Biosignals in the Study of Emotions 
Self-report measures like the user satisfaction questionnaire are typically administered 
at the end of an interaction session. As such, they have at least two possible 
disadvantages [3], [4]. First, sometimes it is not possible to consciously recollect the 
exact emotional experience during the interaction. Second, the questionnaires provide 
measures only at the end of the interaction – so we have only one measure for the 
whole interaction process. Other self-report studies conducted during the interaction 
with the ECA, like thinking aloud, interfere with natural task completion. 
Biosignals, if reliably interpreted, could in principle help to overcome the above 
drawbacks. Their first advantage is that they provide continuous measures of the 
experienced emotions and can provide automatically information about them over 
time. Their second advantage is the potential to register weak emotional states which 
would not be reported verbally by participants either because they are not consciously 
experienced or because they are too weak and/or judged non significant. 
Therefore, the large body of existing and on-going research on biosignals-based 
study of emotions is not surprising. However, the attempts to use only one or two 
types of signals or features have encountered serious problems so now the efforts 
have been redirected towards finding patterns of many features, extracted from 
various biosignals [3], [7], [8], [10], [12], [13]. In these studies, various statistical 
methods for feature extraction, feature combination, dimensional projection, etc. have 
been applied. Despite the use of powerful statistical methods, the main difficulties in 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
65 
identifying emotions, based on biosignals, remain. One reason for this are the huge 
inter-individual differences, evinced by the fact that one and the same emotion can 
lead to different patterns of physiological reactions and vice-versa in different 
participants. Due to the latter, it is very difficult to find patterns of features (or even 
classifiers) that are applicable for emotion identification which is independent from 
the particular individual [8] and can be generalized for any user (e.g. without having 
to train a classifier for this specific user). 
3   Design of the Study 
The testing procedure consists of two major phases. During the first phase, called the 
calibration phase, subjects were presented with stimuli with known normative 
ratings of emotional impact. Simultaneously, biosignals were recorded. Individual 
physiological patterns that differentiate best between different emotional categories, 
for the particular participant, were identified. 
In the second phase, called the main phase, we studied the emotional reaction of 
the users while interacting with two interfaces of the ECA. We recorded the same 
kind of biosignals as the ones in the calibration phase. The emotional responses were 
classified using the classifiers trained during the calibration phase. 
3.1   Stimuli and Procedure: Calibration Phase 
During the calibration phase, users were presented with several pictures with 
emotional content. In order to have standardized measures of the valence of the 
elicited emotions, these pictures were chosen from the International Affective Pictures 
System (IAPS) [14]. We used 32 pictures from IAPS – among them 16 with average 
valence ratings defining the ‘neutral’ condition, 8 with high positive valence ratings 
defining the ‘positive’ condition, and 8 with high negative valence ratings defining 
the ‘negative’ condition. In Table 1, the picture identification numbers, valence, and 
arousal ratings, as given in the IAPS database [14], are presented. 
Pictures were arranged in blocks of 4 pictures of the same valence. As a result we 
used 2 negative, 2 positive, and 4 neutral blocks. During the calibration phase positive 
and negative blocks were always separated by a neutral block in order to account for 
contrast and/or assimilation effects and thus to achieve a sharper distinction of the 
biosignals. 
The study began after users gave their informed consent and were acquainted with 
the instructions. Each picture was presented for 20 seconds. The participants were 
asked to look at the picture, imagining the emotional content and trying to experience 
it. After the 20 s interval, two 5-point rating scales appeared on the screen one after the 
other: the first one stood for rating the valence of the emotion (from 1 = ‘completely 
negative’ to 5 = ‘completely positive’) and the second scale was for rating the arousal 
(from 1 = ‘very low’ to 5 = ‘very high’). Additionally to the labels, we used the 
corresponding SAM scales [14] which consist of anthropomorphic figures depicting 
the levels of emotional response along each dimension (valence or arousal). 
One reason to use subjective ratings was to compare them with the normative ratings 
(taken from the IAPS database). This gives a measure of the consistency between the 
participants’ ratings and the normative ones and respectively, of the quality of the 

66 
E. Hristova, M. Grinberg, and E. Lalev 
calibration phase. Another reason is the possibility to discard the participant’s data in 
the case of discrepancy between the subjective and normative ratings. Moreover, the 
task of rating each picture for emotional valence and arousal was a pressure on the 
participants to focus on trying to experience the associated emotion. 
The biosignals described in Section 3.3. were recorded during the calibration 
phase. Only the data obtained during picture presentation were analyzed (signals 
recorded during pictures’ rating were excluded from the analysis). 
Table 1. Pictures taken from the IAPS data base [14] used in the calibration phase. The 
standardized normative ratings from the IAPS database are on two 9-point scales: valence (1 = 
‘extremely negative’ to 9=’extremely positive’) and arousal (1 = ’extremely calm’ to 9 = 
‘extremely aroused’). 
Condition 
IAPS mean 
rating on 
'valence' 
dimension 
IAPS mean 
rating on 
'arousal' 
dimension 
Pictures used 
Negative 
2,35 
6,42 
2095; 2352,2; 2683; 2730; 6313; 6550; 
8485; 9050 
Positive 
7,66 
5,63 
1710; 2216; 2345; 2352,1; 5623; 5833; 
7502; 8210 
Neutral 
5,00 
2,65 
2102; 2235; 2440; 2575; 7000; 7004; 
7010; 7020; 7035; 7059; 7080; 7175; 
7217; 7224; 7235; 7491 
3.2   Stimuli and Procedure: Main Phase 
During the main phase, the testing was performed on the two agent interfaces, 
developed in the RASCALLI multi-agent architecture [11] (see Fig. 1). The interaction 
with the RASCALLI ECA can be carried out by using text (the user and the agent), 
speech (only the agent), and gestures (only the agent). The agent is a personal assistant, 
acting in a scenario where it helps the human user in gathering and organizing 
information related to music. 
 
    a)  
 
b) 
 
Fig. 1. The two versions of the ECA interface used in the study: a) male and b) female agent 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
67 
The stimuli in the main phase were video clips representing interactions between a 
user and the RASCALLI ECA. We used two versions of the agent: male and female 
(see Fig. 1). For each agent, six videos representing interactions with duration 20-30s 
each were presented. The first video shows the agent just sitting while the user enters 
the system. The next 5 videos had the following structure: the user asks a question, 
the agent answers, the user attends to the given answer. In three of these videos the 
agent answers correctly. In two of them the agent answers incorrectly. The agent 
answers by text (in the dialogue bar) and by voice. The agent also uses various 
gestures to emphasize something or to point at information presented on the screen. 
The questions and answers were the same for the male and the female ECA. The use 
of video clips instead of real interactions allowed for a more controlled environment 
with the same behavior of the agent for all users. 
Each user watched 6 videos with either the male or the female ECA. The users 
were instructed to imagine they were interacting with the system while watching the 
videos. After the end of each video, the participants had to rate the experienced 
emotions on the same two rating scales (for valence and arousal) used in the 
calibration phase. 
While users were watching the videos, the same type of biosignals as the ones used 
in the calibration phase and described in Section 3.3, were recorded. 
3.3   Biosignals Used in the Study  
The following biosignals were recorded during the calibration and the main phases: 
electrocardiogram (ECG), photoplethysmogram (PPG), galvanic skin reaction (GSR), 
and electromyogram (EMG). 
ECG was recorded using the Biopac's ECG100 amplifier with sampling rate 200 
sample/s. Two LEAD110S electrode leads with EL503 electrodes were attached to 
the participant’s left and right hand. 
PPG was recorded using the Biopac's PPG100C amplifier with the TSD200 
transducer. The TSD200 consists of an infrared source and photo diode, which 
records the infrared reflectance sensitive to varying blood flow. The sampling rate 
was 200 samples/s. 
GSR was recorded using Biopac GSR100C amplifier with sampling rate of 200 
samples/s. The GSR100C uses a constant voltage (0.5 V) technique to measure skin 
conductance. The GSR100C amplifier was connected to one set of the TSD203 Ag-
AgCl, unpolarizable, finger electrodes.   
Facial EMG was measured over Corrugator Supercilii and Zygomaticus Major 
using surface electrodes placed following the scheme suggested by [15]. The EMG 
was recorded with sampling rate of 250 samples/s. 
3.4   Participants 
Nineteen participants – 7 men and 12 women (aged between 20 and 45 years) – took 
part in the study. Two male participants were not able to follow the instructions in the 
calibration phase, so their data was excluded from further analysis. 

68 
E. Hristova, M. Grinberg, and E. Lalev 
4   Classification of Biosignals: Calibration Phase 
The data collected during this session was used in order to extract patterns of 
physiological responses that differentiate between emotions with different valence. As 
large inter-individual differences were expected, the analysis was performed for each 
user separately. As a first step, we used the Augsburg Biosignal Toolbox (AuBT), 
which provides a set of tools to analyze physiological signals for emotion recognition 
[16]. With the help of AuBT, a total of 208 features were extracted from the 5 types 
of recorded biosignals: 80 features from the ECG signal, 67 features from the PPG 
signal, 19 features from the GSR signal, and 21 features from each of the EMG 
signals. For each signal, after preprocessing, features such as the maximal and 
minimal amplitude, mean, standard deviation, rate, variability, etc. were calculated. 
(The complete list of the features extracted by AuBT can be found in [16].)  
To extract interpretable information from this very large dataset (208 features for 
32 pictures for each subject), we applied the Fisher’s Linear Discriminant Analysis 
(LDA) [17] as a dimension reduction technique. In such a way we obtain a reduced 
two-dimensional representation of the high-dimensional data set of biosignals’ 
features. As pointed out in [17], using the reduced data representation, classification 
procedures can very often give better results. Fisher’s LDA looks for a low-
dimensional data representation that gives the best separation between a set of 
categories [18]. 
After applying the Fisher’s LDA on the calibration phase data, we obtained 
projection weights for each of the dimensions (for each extracted feature 
respectively). Using the projection weights, we were able to project the set of features 
for the IAPS pictures onto a 2-dimensional space. The result of this procedure for an 
individual user is presented in Fig. 2. 
 
 
 
Fig. 2. Fisher projection (LDA) of physiological data for three classes of emotional pictures – 
negative, positive, and neutral (the data presented are from a single user) 
The projection weights are different for each user and we did not try to identify 
neither common dimensions between the participants nor dimensions typically 
attributed in the literature to the emotion space such as valence or arousal. We just used 
the projection weights as a basis for representation of the physiological data features 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
69 
obtained during video watching in the space of the IAPS pictures using the latter as a 
reference for evaluation of the videos with respect to valence (see next section). 
To assess the quality of the classification achieved, we evaluated how well the 
pictures are classified by Fisher’s LDA compared to the IAPS categorization based on 
normative ratings (see Table 1). The percentage of correctly classified pictures using 
LDA of extracted biosignal features is 99.3% correct for the negative pictures, 97.4% 
for the positive pictures, and 93.4 % for the neutral pictures (96.9% on average), for 
all participants. The discriminant functions for each user were saved for further use in 
the main phase, described in the next section. 
5   Classification of Biosignals: Main Phase  
The biosignals recorded during the main phase are analyzed individually for each user 
as in the calibration phase. First, for each video we extracted the same features as the 
ones selected in the calibration phase. Then, we used the projection weights for each 
participants to project both pictures’ and videos’ data in the same 2-dimensional 
space. The results of this procedure for one participant are presented in Fig. 3. 
 
 
Fig. 3. Fisher’ LDA of physiological data for three classes of emotional pictures (negative, 
positive, and neutral) and for videos with interaction with ECA (the presented data are for a 
single user) 
Next, using the Fisher’s LDA functions from the calibration phase (see Section 4), 
we classified the emotions experienced during each video in three categories – 
positive, negative, or neutral. The summarized results for all participants are presented 
in Table 2. 
As seen from Table 2, the female ECA video clips were classified more often as 
positive (31,5 %) compared to the male ECA videos (12,8 %). On the other hand, the 
male ECA videos were classified more often as neutral (55,3 %). Qualitatively, the 
classification patterns for the male and female ECAs seem different – more uniform 
for the female agent and concentrated on the classification ‘neutral’ for the male 
agent. Classification patterns were compared using a chi-square test that showed  
 

70 
E. Hristova, M. Grinberg, and E. Lalev 
Table 2. Classification of videos using biosignals. Data are in percentages over all users and all 
videos for the corresponding agent (male or female) 
Classification of videos based on biosignals 
Agent 
Negative 
(%) 
Neutral 
(%) 
Positive 
(%) 
Female 
37,0 
31,5 
31,5 
Male  
31,9 
55,3 
12,8 
 
 
significant difference between the classification for the male and the female ECA 
(χ2(2) = 7.4, p = 0.025). 
To assess the quality of the biosignal based classification shown in Table 2, we 
compared it with classification based on subjective ratings. Subjective ratings are 
made using the scale: 1 = ‘completely negative’, 2 = ‘somewhat negative’, 3 = 
‘neutral’, 4 = ‘somewhat positive’, and 5 = ‘completely positive’. For the purpose of 
the analysis ratings ‘1’ and ‘2’ were coded as ‘negative’, rating ‘3’ was coded as 
‘neutral’, and ratings ‘4’ and ‘5’ were coded as ‘positive’. On the basis of these 
subjective ratings, 32 % of the videos (male and female ECA) are classified as 
‘negative’, 28 % – as ‘neutral’, and 40 % – as ‘positive’. 
The comparison between these two classification methods (subjective ratings and 
biosignals) is shown in Table 3. In total, about one third (36,6 %) of the videos are 
classified with one and the same valence label (negative, neutral, or positive) using 
the two classification methods.  The largest matching was observed for the neutral 
category (46,4 %). 
Table 3. Comparison between the classifications of videos based on subjective ratings (SR) and 
on biosignal data (BS) 
Classification of videos based on biosignals (BS) 
Classification 
based on subjective 
ratings (SR) 
Negative 
(%) 
Neutral 
(%) 
Positive 
(%) 
Negative 
34,4 
50,1 
15,6 
Neutral 
35,7 
46,4 
17,8 
Positive 
34,1 
34,2 
31,7 
 
One reason for the relatively small overlap between the two classifications can be 
related to the overall neutrality of the video stimuli presented during the main phase. 
The emotions elicited during the video watching were weak. This can explain ‘low’ 
physiological response and higher error in the assignment due to noise. The second 
reason could lie in the choice of reference pictures from the IAPS dataset which cover 
a large range of emotions reaching highly negative and positive ones. This large range 
decreases the sensitivity of the method for stimuli with small differences due to the 
lack of enough reference stimuli in the appropriate range. On the other hand, 
biosignals are related to the participants’ experience during an extended period of 
time, while the ratings are given at the end and can influenced by additional factors 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
71 
like judgment, reasoning, etc. The important question is to what extent the results 
obtained by both approaches are comparable and consistent, and can we use just one 
of them when appropriate.  
To answer this question, three classification distributions over the categories 
‘negative’, ‘neutral’, and ‘positive’, were analyzed and compared for the female and 
male ECAs: classification using biosignals (BS), classification using subjective 
ratings (SR), and classification according to the cases for which there is a match 
between BS and SR classifications (i.e. the same valence labels – ‘negative’, 
‘neutral’, or ‘positive’ – were assigned), denoted further ‘SR∩BS’. Summarized 
classification data for the male and female ECA, are presented in Table 4 and Table 5, 
respectively.  
Table 4. Classification of videos with the male virtual character using biosignals (BS) and 
subjective ratings (SR). (Note: Classification which was the same by both methods is denoted 
as SR∩BS.) 
Male ECA 
Measure 
Negative 
(%) 
Neutral 
(%) 
Positive 
(%) 
Number of 
classifications 
BS 
31,9 
55,3 
12,8 
47 
SR 
34,0 
36,2 
29,8 
47 
SR∩BS 
23,5 
58,8 
17,6 
17 
 
These three classification distributions were compared using chi-square statistics. 
The results for the male ECA (see Table 4) are the following. There were no 
significant differences between the BS or SR (χ2(4) = 2.39, p = 0.66), between BS and 
SR∩BS (χ2(2) = 0.73, p = 0.69), and between the SR and SR∩BS (χ2(2) = 3.8, p = 0.15) 
classifications. 
Table 5. Classification of videos with the female virtual character using biosignals (BS) and 
subjective ratings (SR). (Note: Classification which was the same by both methods is denoted 
as SR∩BS.) 
Female ECA 
Measure 
Negative 
(%) 
Neutral 
(%) 
Positive 
(%) 
Number of 
classifications 
BS 
37,0 
31,5 
31,5 
54 
SR 
29,6 
20,4 
50,0 
54 
SR∩BS 
35,0 
15,0 
50,0 
20 
 
The results for the female ECA (see Table 5) are quite similar as the ones for the 
male ECA. There were no significant differences between the BS and SR (χ2(4) = 1.74, 
p = 0.78), between the BS and SR∩BS (χ2(2) = 3.93, p = 0.14), and between the SR 
and SR∩BS (χ2(2) = 0.48, p = 0.79) classifications. 
 

72 
E. Hristova, M. Grinberg, and E. Lalev 
a) 
 
b) 
 
c) 
 
 
Fig. 4. Classification of videos based on: a) subjective ratings (SR), b) biosignals (BS), and  
c) the cross section of SR and BS (SR∩BS). (Note: the percentages are calculated over all users 
and all videos for the corresponding agent interface and the corresponding method. See also 
Table 4 and 5.) 
 
The results from the statistical analysis, discussed above, seem to show that the 
three methods of classification give similar results and information. Therefore, it is 
interesting to see if the three methods of classification can differentiate the two 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
73 
interfaces and if the comparison given would be similar and leading to the same 
conclusions. This would mean that we can use any of them to estimate the emotional 
impact of interfaces and the biosignal based method in particular. 
The results from the three methods are summarized in Fig. 4 (see also Tables 4 and 5). 
For more convenient comparison, the BS classification, shown in Fig. 4b, is between the 
two other classifications (Fig. 4a and 4c). It is seen from Fig. 4 that the three distributions 
are similar with some differences. The larger proportion of positive classification for the 
female interface is common feature of the three classification methods. For the SR and 
SR∩BS classifications (both involving subjective ratings), the distribution of 
classification is bimodal (positive and negative) for the female virtual character which is 
less pronounced for the BS classification. For the male virtual character the distribution 
seem to be centered around the neutral for all three classifications (see Fig. 4). 
Chi-square tests were performed to see to what extent the SR, BS, and SR∩BS 
based classification patterns differ for the female and male ECA interfaces. All three 
classification methods yield a significant difference (marginally significant for the SR 
classification) between the classification patterns for the male and female ECA: (χ2(2) 
= 7.4, p = 0.025) for BS, (χ2(2) = 8.2, p = 0.017) for SR∩BS, and  (χ2(2) = 4.95,  
p = 0.084) for SR. 
The above results seem to indicate that the BS based classification method shows 
closer results to the SR∩BS method and higher selectivity compared to the SR 
method (if we take the SR∩BS method as a reference). 
6   Conclusion 
In this paper, we proposed a two-phase procedure, based on biosignals, aimed at 
classifying users’ emotional experience while interacting with embodied conversational 
agents. In the first phase of the procedure images from IAPS, standardized with respect 
to emotional response, were shown to participants. In the second phase, participants 
watched a sequence of video clips with interaction episodes with a female and a male 
ECA. Biosignals were recorded during both phases. Additionally, subjective ratings 
were gathered using a five-point scale. 
The general goal of this paper was to explore the applicability of this two-phase 
procedure for assessing patterns of emotional response with respect to valence as part 
of usability and human-agent interaction studies. Additionally, we wanted to 
investigate to what extent the proposed two-phase procedure can be used for automatic 
emotion valence assessment. Our specific goal was to assess and differentiate two 
interface versions of ECAs (female and male) based on the valence of the elicited 
emotions. 
In order to achieve these goals, the results from the biosignals’ processing were 
compared with subjective ratings classifications given by the same participants for the 
same video clips. In the comparison, special attention was paid to video clips for 
which the classification given by the two methods coincided. Classification based on 
this latter case was taken as a reference for evaluation of the biosignal based and the 
subjective ratings based classifications. The results from this comparison seem to 
show that the biosignal based method gave a better distinction between the female and 
male interface, than the subjective rating based (as suggested by a chi-square tests). 

74 
E. Hristova, M. Grinberg, and E. Lalev 
Moreover, the classification patterns obtained by the biosignal processing were closer 
to the intersection of both methods. These results imply that the proposed procedure 
can be used successfully for emotion valence assessment at least at the level of 
individual subjects.  
The application of the procedure for evaluation of the female and male ECAs 
showed that the classification pattern for the female virtual character is bimodal with 
relatively large number of negative and positive classifications, while for the male 
virtual character the neutral classification predominated. Furthermore, there were 
more positive classifications for the female than for the male ECA.  
Although the procedure seems to provide useful information about the valence of 
emotional response several questions need to be answered by further research. One 
such question is related to the relatively low overlap between biosignal based and 
subjective rating based classification (about 37%). A possible way to improve these 
results would be to choose the pictures’’ calibration set closer to the emotional 
response expected during interaction with the ECA. Another question is related to 
general connection between reported emotions, which may be influenced by many 
external factors, and biosignal responses. Work along these lines is in progress. 
Acknowledgments. The work presented in this paper was supported by the EC FP6 
project RASCALLI and COST action 2102. We would like to thank the AuBT team 
(University of Augsburg) and especially Johannes Wagner, for making available the 
AuBT program for data analysis and for the help and advices he provided concerning 
our study. We gratefully acknowledge the help of Alexander Gerganov, Ivo 
Popivanov, Gergana Kuzmova, and Dr. Dimiter Atanasov in the acquisition, 
processing and statistical analyses of the data. 
References 
1. Bradley, M.: Emotion and motivation. In: John, T., Cacioppo, J., Tassinary, L.G., Berntson, G. 
(eds.) Handbook of Psychophysiology. Cambridge University Press, Cambridge (2000) 
2. Levenson, R.: Autonomic nervous system differences among emotions. Psychological 
Science 3(1) (1992) 
3. Mandryk, R., Atkins, M.: A fuzzy physiological approach for continuously modeling 
emotion during interaction with play technologies. International Journal of Human 
Computer Studies 65(4), 329–347 (2006) 
4. Benedek, J., Hazlett, R.: Incorporating facial EMG emotion measures as feedback in the 
software design process. In: Proc. Human Computer Interaction Consortium (2005) 
5. Ward, R., Mardseen, P.: Psychological responses to different WEB page designs. 
International Journal of Human-Computer Studies 59, 199–212 (2003) 
6. Wilson, G., Sasse, M.: Do users always know what’s good for them? Utilizing 
physiological responses to assess media quality. In: McDonald, S., Waern, Y., Cockton, G. 
(eds.) People and Computers XIV - Usability or Else! Proceedings of HCI 2000, 
Sunderland, UK, September 5- 8. Springer, Heidelberg (2000) 
7. Haag, A., Goronzy, S., Schaich, P., Williams, J.: Emotion recognition using bio-sensors: 
First steps towards an automatic system. In: André, E., Dybkjær, L., Minker, W., 
Heisterkamp, P. (eds.) ADS 2004. LNCS, vol. 3068, pp. 36–48. Springer, Heidelberg 
(2004) 

 
Biosignal Based Emotion Analysis of Human-Agent Interactions 
75 
8. Kim, J., André, E.: Emotion Recognition Based on Physiological Changes in Listening 
Music. IEEE Trans. on Pattern Analysis and Machine Intelligence 30(12), 2067–2083 
(2008) 
9. Wagner, J., Kim, J., André, A.: From Physiological Signals to Emotions: Implementing 
and Comparing Selected Methods for Feature Extraction and Classification. In: IEEE 
International Conference on Multimedia & Expo. (2005) 
10. Nasoz, F., Alvarez, K., Lisetti, C., Finkelstein, N.: Emotion recognition from physiological 
signals for presence technologies. International Journal of Cognition, Technology, and 
Work – Special Issue on Presence 6(1) (2003) 
11. Krenn, B.: RASCALLI. Responsive Artificial Situated Cognitive Agents Living and 
Learning on the Internet. In: Proc. of the International Conference on Cognitive 
Systems/University of Karlsruhe, Karlsruhe, Germany, April 2–4 (2008) 
12. Picard, R.W., Vyzas, E., Healey, J.: Toward machine emotional intelligence: analysis of 
affective physiological state. IEEE Transactions on Pattern Analysis and Machine 
Intelligence 23(10), 1175–1191 (2001) 
13. Christie, I.C., Friedman, B.H.: Autonomic specificity of discrete emotion and dimensions 
of affective space: A multivariate approach. International Journal of Psychophysiology 51, 
143–153 (2004) 
14. Lang, P.J., Bradley, M.M., Cuthbert, B.N.: International affective picture system (IAPS): 
Digitized photographs, instruction manual and affective ratings. Technical Report A-6. 
University of Florida, Gainesville, FL (2005) 
15. Tassinary, L., Cacioppo, J., Geen, T.: A psychometric study of surface electrode 
placements for facial electromyographic recording: I. The brow and cheek muscle regions. 
Psychophysiology 26(1), 1–16 (1989) 
16. Wagner, J.: Augsburg Biosignal Toolbox (AuBT): User Guide (2005) 
17. Cunningham, P.: Dimension reduction. Technical report UCD-CSI-2007-7, August 8, 
2007, University College Dublin (2007) 
18. Fukunaga, K.: Introduction to statistical pattern recognition. Academic Press, Inc., London 
(1990) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 76–89, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Emotional Aspects in User Experience with Interactive 
Digital Television:  
A Case Study on Dyslexia Rehabilitation 
Filomena Papa and Bartolomeo Sapio 
Fondazione Ugo Bordoni, Via Baldassarre Castiglione 59, 00142 Rome, Italy 
{fpapa,bsapio}@fub.it 
Abstract. This work explores the emotional response of users in front of 
Information and Communication Technologies and the aspects triggering 
emotional reactions. One of the opportunities provided by interactive services 
delivered through Digital Television (DTV) is to promote the use of socially 
relevant ICT-based services by large groups of people who have neither 
Internet access nor the needed skills to use the Internet, but are familiar with 
television and its remote control. The pilot research programme on DTV in 
Italy has been developed through a number of initiatives, some of them issued 
to explore the potential impact on population of the new digital services 
associated with the broadcasted TV channels. Fondazione Ugo Bordoni (FUB) 
co-funded six T-government projects with the main objective to experiment 
high interactivity services involving real users. The T-islessia project 
concentrated on the rehabilitation of children at risk of dyslexia using 
interactive exercises through the TV channel. Structured listening sessions were 
used to investigate the young users’ attitudes and their emotional reactions 
towards the technological tool. During these sessions, drawings and interviews 
were adopted in a complementary way. Positive results derived from the field 
investigation with regard to the effectiveness of the rehabilitation through the 
DTV platform: the greater the number of interactive sessions, the higher the 
level of acquired phonetic skills. 
Keywords: Emotional aspects, interactive public services, digital television 
(DTV), end-users evaluation, field study, T-government, usability, usage, use 
behaviour, user experience.  
1   Introduction 
This work explores the emotional response of users in front of Information and 
Communication Technologies and the aspects triggering emotional reactions. User 
experience is dealing with all aspects of the users’ interaction with the product: how it 
is perceived, learned and used [1]. 
There are different components in user experience in the interaction with an ICT 
service. One of these components is focused on tasks and goals, their efficient 

 
Emotional Aspects in User Experience with Interactive Digital Television 
77 
achievement and the cognitive information processing involved. This component 
includes aspects as perception of usefulness and ease of use. A further component is 
related to visual aesthetic and symbolic quality of the technology. Another component 
is concerned with emotional user reactions (only subjective feelings [2]).  
Affective/emotional components can be on the one hand immediate, unmediated 
affective reactions and on the other hand more complex emotional consequences that 
result from a cognitive appraisal process [3].  
Different methods are used to measure emotions: for instance some instruments are 
used to measure facial expressions or vocal expressions, other instruments to measure 
physiological reactions, other instruments to measure subjective feelings including 
verbal rating scales or pictograms [4]. 
The affective/emotional aspects have been considered in one of the T-government 
projects promoted by Fondazione Ugo Bordoni (FUB) (Section 2): the T-islessia 
Project. 
The T-islessia project concentrated on the rehabilitation of children at risk of 
dyslexia using interactive exercises through the TV channel (Section 3). In this project 
a case study was conducted to investigate about children’s reactions to rehabilitation 
through Digital Terrestrial Television (DTV) and qualitative data were collected. 
The method and the first results here presented are part of a larger work still in 
progress, moreover having the objective to investigate the usability and economic 
factors affecting adoption and usage of T-government services [5, 6].  
T-government stands for a wide set of services addressed to citizens (e.g. about 
health, education, tourism, payment of bills), delivered by a Public Administration 
and accessible by Digital Television.  
The piloting research program on DTV in Italy has been developed through a 
number of initiatives, some of them issued to explore the potential impact on 
population of the new digital services associated with the broadcasted TV channels. In 
that field of investigation, T-government interactive services were introduced to be 
early tested with a real sample of residential private adopters of the digital TV decoder. 
2   T-Government Services  
T-government services are herewith categorised as “informative services” (e.g. 
provision of general information) and “interactive services” (e.g. searching for 
specific information, e-mail, chat, form filling and form sending, payments).  
Most of T-government services that have been so far developed are informative 
services.  
One of the opportunity given by T-government is to promote the use of ICT-based 
public services by large groups of people (e.g. the elderly), who have not Internet 
access or the required skills. For those people, the past experience with TV and remote 
control may be a key qualification to become effective users of the above services. 
On the other hand some studies [7] make evident the gap of knowledge about the 
human aspects of T-government services: usage and usability, user satisfaction, 
capability of services to reach all citizens and capability to include them in the 
benefits of the information society.  

78 
F. Papa and B. Sapio 
On these topics some field investigations have been developed in Italy in the 
framework of the T-government projects promoted by FUB. 
One of the objectives of the six T-government projects co-funded by FUB was to 
experiment high interactivity T-government services, realised by Digital Terrestrial 
Television (DTV), involving real users. 
The high interactivity T-government services provide the user with some of the 
following features: 
• 
remote interactivity using the return channel;  
• 
high performance return channel (e.g. broadband or wireless); 
• 
user authentication using a smart card (e.g. electronic identity card, services 
regional card or other kind of smart cards); 
• 
on line payments.  
The experimented services belong to different application areas: demographics, 
utilities and fines, education, T-health, T-learning, employment, T-commerce,  
T-banking. 
In order to investigate usage, usability and socio-economic aspects of T-
government services, an explorative field investigation was developed in each of the 
six projects. 
The field investigations were realised using a common framework referring to 
human factors discipline [8]. The following main usability aspects related to 
interactive services were identified [9]: perceived usefulness, perceived ease of use 
and attractiveness, training and user support (human support, user manual, support 
provided by DTV, call center), user perception of technical disturbances and troubles 
(due to television signal, set top box, return channel), security and privacy perception 
(confidentiality of personal data, security of payments), impact of the equipment in 
the house, users’ satisfaction about the service including the comparison of different 
channels to perform the same task (e.g. DTV versus Internet, DTV versus traditional 
office desk). 
Service usage is related to whether a given service is used or not and to the service 
utilisation level. Service usage is evaluated collecting subjective data provided by the 
user. The adopted indicators for service utilisation level are [10]: frequency of use, time 
duration of the session, kind of use (shallow or intensive). Socio-economic aspects 
included: user profile (including income and social network information), TV and 
Internet usage (including other entertainment technologies), and scenarios (including 
interest levels, willingness to pay for equipment and services, decision factors). 
3   The Field Investigation Developed in the “T-Islessia” Project  
In the following, the field experiment carried out in the T-islessia project is described. 
T-islessia is a T-health project involving a particular class of DTV users: children 
six years old at risk of dyslexia, performing rehabilitation exercises on a daily basis.  
Usually children realise a rehabilitation program in adequate centres. DTV 
provides the opportunity for children to perform exercises at home or at school 
without the necessity to go to the rehabilitation centre. 

 
Emotional Aspects in User Experience with Interactive Digital Television 
79 
It is evident that, for this particular kind of users, emotional aspects and pleasantness 
of use of ICT play a very important role in the success of the rehabilitation program. 
 
Aims 
The main aims of the project were: 
• 
to investigate the effectiveness of rehabilitation exercises delivered by DTV; 
• 
to compare rehabilitation exercises delivered at distance (at home or at 
school) by DTV with traditional techniques (e.g. going to a rehabilitation 
centre).  
• 
to evaluate user reactions and usability aspects. 
The project was realised involving some schools in the areas of Bologna and 
Rimini. 
The rehabilitation process was developed involving children, parents and teachers.  
In order to investigate user reactions and usability aspects, the “Donini” school in 
Bologna was selected as a case study. 
 
Phases 
The rehabilitation project developed in three main phases: 
 
• 
First screening test administration (to about 1000 children of the first class of 
primary school) to identify children at risk of dyslexia. 
• 
Realisation of the rehabilitation program (two months time duration) in 
controlled environment (at school) and at home. 
• 
Second screening test to verify the actual improvement of the children’s 
skills in comparison to the first screening test. 
 
Application  
The project developed an interactive service named “Magicadabra”.  
The offered service delivered high interactivity games with a rehabilitation value. 
The games allowed children to develop skills in reading and writing. A proprietary 
smart card was employed for user identification. 
The application was designed using an “interaction metaphor” to make the 
rehabilitation experience pleasant for the child and to create a common play context 
among the different exercises. Due to the user target, a “magic world” was selected 
for the rehabilitation context. In this fantastic world a character (named Gia) led the 
child among the different games (exercises) and became his/her interlocutor during 
the different rehabilitation sessions.  
Each rehabilitation session last about half an hour and included eight different 
exercises. During the development of the rehabilitation program some variants were 
introduced in the exercises in order to increase their complexity. 
At the end of each session, data related to the child performance in the different 
exercises were sent, through the return channel, to the DTV service centre. 
A screen shot of an exercise named “Lulù” is presented in Fig. 1. 
 

80 
F. Papa and B. Sapio 
 
Fig. 1. Application screenshot 
Users’ panel 
In total 84 children six years old at risk of dyslexia effectively participated in the 
rehabilitation through DTV. 45 performed exercises at school and 39 at home (Table 1). 
Table 1. Number of children participating in the DTV rehabilitation program at home and at 
school (in the areas of Rimini and Bologna) 
 
Delivery at school Delivery at home
Total for area 
Bologna area 
21 
27 
48 
Rimini area 
24 
12 
36 
Total for place of delivery 
45 
39 
84 
 
In the case study at the “Donini” school two kinds of “users” were involved: 
children as end users of the application; teachers as “privileged observers” of the 
rehabilitation program by DTV.  
 
Procedure 
The “Magicadabra” application was implemented and broadcasted in the areas of 
Bologna and Rimini.  
Set top boxes were placed in selected schools and in children’s homes. 
A technician installed equipment in schools and trained the users as well.  
At home self-installation was adopted: families were provided with the necessary 
equipment (Set top box, smart card, etc.). A user manual was also released to aid 
home installation and service use. 
A call centre was available to the users for any information and help in the 
utilisation of the service during the project. 
 
 

 
Emotional Aspects in User Experience with Interactive Digital Television 
81 
Tools and techniques for data collection 
Specific tools for data collection were adopted in this project, in comparison to other 
FUB projects, due to the particular class of users involved. 
Objective data were collected at the DTV service centre about: 
• 
 the service degree of utilisation;  
• 
 the results of the rehabilitation exercises.  
These data were collected for all the children involved in the project. 
Qualitative data about users’ reactions and service usability were collected in the 
case study developed at “Donini” school in Bologna [11]. In the case study two main 
techniques were adopted for data collection: 
• 
“listening sessions” in which children were asked to produce a drawing and 
to give oral clarifications about the drawing (see below); 
• 
semi-structured interviews to teachers supervising the rehabilitation at the 
school. 
Data about young users’ attitudes and their emotional reactions towards the 
technological tool were collected mainly setting up “structured listening sessions”. 
These session were supervised by a team of psychologists. 
The choice of using drawings as a tool for usability analysis derived from the 
peculiarity of users - about 6 years old - and their limited reading/writing skills. 
Given the young age of the service users, tests or structured interviews could be 
inadequate tools in the context of the trial. In order to detect the attitude of those 
young users towards the application and the testing activities, researchers chose to ask 
the kids to produce a drawing on the theme "Magicadabra and I". 
The objective of the analysis was to measure the usability of the application in its 
individual components and to analyse the emotional attitude of the young user while 
interacting with Magicadabra. Starting from the emotions expressed by the child in 
the production of the drawing, an attempt was done to highlight the level of 
satisfaction/frustration produced by the rehabilitation experience and the use of the 
application. 
Elements of context and interaction that could be decisive and conditioning the 
expressed attitude were traced in the drawings. Finally, some assessments were made 
about the general level of satisfaction expressed by users and any problems emerged, 
comparing them with non-structured interviews to teachers. 
In order to support the interpretation of the drawings, verbal interactions occurred 
between researchers and children both during and after the delivery of the test were 
taken into consideration. 
In psychology research, drawings belong to the category of projective tests, 
because the child proposes (projects) a story to the blank sheet: the story of his 
feelings, his emotions, his fears and so on.  
The children were asked to produce a drawing titled "Magicadabra and I", leaving 
them free to use the colors they wanted and the time they felt necessary. 
At the end they were asked to sign the drawing with a pseudonym of their choice. 
When they delivered their drawings, chidren were asked what they wanted to 
represent in order to gather the necessary information for a "correct" interpretation. 

82 
F. Papa and B. Sapio 
Since the task required the representation of themselves, the test can be rightly 
considered as a variant of the projective test Human Figure Drawing (HFD). In 
representing themselves, children unwittingly bring into the scene their perception of 
their own characteristics, their own body scheme and their own fears, wishes and 
experiences. 
As often indicated in the literature on children drawing tests, the material provided 
included a pencil, a rubber, a pencil sharpener, colors (pencils or felt-tip pens), non- 
striped sheets of paper 21cm x 29,7 cm. 
The dimensions indicated in the scientific literature as descriptors of the meaning 
of what is represented in the drawing are: 
• 
space occupied or not occupied by the drawing 
• 
stroke 
• 
pressure exerted on the sheet by the child 
• 
shapes (curved lines, broken lines, and so on) 
• 
colors 
In addition, the following extra elements were taken into account: 
• 
presence of metaphoric elements referring directly to the games and user 
interface of Magicadabra 
• 
absence of metaphoric elements denying the presence of Magicadabra 
despite the task 
• 
presence of highly symbolic elements (e.g. dragon, plane, animals and so on) 
not in relation with the games of Magicadabra 
• 
presence of emotional elements related to family and/or relational dynamics 
• 
willingness to communicate both at the time of delivery (with questions like 
"What did you want to represent? Who/what is this? Where are you? Where 
is Magicadabra?") and at the time of drawing. 
The interpretation of drawings tried to take into account all these components, in 
order to understand the experience of children in relation to the process of 
rehabilitation through DTV. 
The analysis and interpretation of each drawing followed three directions: 
1. Attitude of the subject: it is derived both from cognitive elements related to 
the interaction with Magicadabra and from emotional/relational elements 
emerging from the drawings and the communication expressed 
2. Remarks about the interaction: these are the elements related to the 
interaction with the Magicadabra world, the seized metaphors, the attribution 
of meaning that the child assigns to his cognitive and emotional experiences 
3. Notes: additional comments and notes for the development of improved 
releases of the application 
The teachers supervising the rehabilitation program have been considered as 
“privileged observers” of the experimentation at school, since they were also aware of 
the parents’ reactions to the rehabilitation at home. As a consequence they were 
involved in a semi-structured group interview in which different items were considered: 
technical problems with the equipment at school and at home, management of 
rehabilitation in relation to usual didactic activities, children’s reactions to the 

 
Emotional Aspects in User Experience with Interactive Digital Television 
83 
rehabilitation by DTV, problems of children in the interaction with the equipment (e.g. 
in the use of the remote control), pleasantness of the exercises for children, comparison 
with video games, parents’ attitudes, opinions about the technological tool used in the 
project, general opinions about the use of learning technologies. 
4   Results 
Two kinds of results [11] were obtained: quantitative results about effectiveness of 
rehabilitation through DTV and qualitative results (from the case study) about 
emotional aspects. 
The quantitative results, from data collected at the DTV service centre, suggest that 
the percentage of success, i.e. the number of children improving their skills in reading 
and writing after the rehabilitation, is increasing depending on the number of sessions 
(Fig. 2), thus indicating the effectiveness of the interactive rehabilitation tool. 
 
38%
67%
73%
100%
from 0 to 7
from 8 to 14 
from 15 to 20
from 21 to 28
Number of sessions
Percentage of rehabilitation success
 
Fig. 2. Percentage of rehabilitation success in dependence of the number of sessions (schools in 
the Bologna area) 
The analysis of drawings suggests a few considerations. The representations show 
elements of different types: 
Realistic: representing the physical context of the application and the interaction 
between the user and Magicadabra;  
Symbolic: representing the significance of the interaction and the relationship 
between the child and the application; 
Emotional: representing the psychological conditions and the child’s attitude 
towards the interactive experience. 

84 
F. Papa and B. Sapio 
Among the performances drawing the scene in a realistic way the interaction 
scene, we can distinguish two points of view: 
• 
Spectator: users represent the scene of rehabilitation depicting themselves 
and the set-top box; on the TV screen a game is always depicted; 
• 
Personal: drawings represent a game on the screen. 
The spectator point of view reveals a general positive attitude: the characters (user 
and other observing or supporting persons) are represented with serene and smiling 
expressions. In a particular case the player/user represents himself in a moment of 
gratification: during a "victory" in the game. 
 
 
Fig. 3. Drawing by harripoter2 
In the case of the subjective point of view, the drawing by harripoter2 (Fig. 3) 
depicts a smiling game hero, while in silente2 (Fig. 4) there are no human figures to 
suggest more information about the state of mind of the child. 
Both types of drawings are linked by the absence of interface elements not 
belonging to the scene and the game action. About this absence one can assume that 
the level of attention and concentration is fully addressed to the exercise, without any 
special representation of the cognitive effort made to learn the game interactions. 
In the group of drawings defined symbolic (Fig. 5-6-7), the set-top box and the 
Magicadabra application are not depicted, and the child’s attention focuses on human 
figures. Again the drawings adopt a spectator (mirmo-camilla, dori) and subjective 
point of view (ermione), and human figures are represented in serene attitude. The 
three drawings are linked by the presence of a figure with a magic wand recalling 
Magicadabra’s interaction agent Gia. 

 
Emotional Aspects in User Experience with Interactive Digital Television 
85 
 
Fig. 4. Drawing by silente2 
 
Fig. 5. Drawing by dori 
The choice to portray the magic element as a salient trait in the interactive 
experience reveals a more direct interest for the communication metaphor, the 
narrated story and the atmosphere generated by the graphic style and the script.  

86 
F. Papa and B. Sapio 
 
Fig. 6. Drawing by ermione 
 
Fig. 7. Drawing by mirmo-camilla 
Even in their diversity, the three drawings seem to have grasped that through the 
magical element the producers attempted to give a relational dimension to the 
interaction, through a character who plays a supporting role in the game. 

 
Emotional Aspects in User Experience with Interactive Digital Television 
87 
The function of the magic element and of the interaction agent had been originally 
designed to make less "cold" the relationship with the technology behind the 
application. By characterising the interaction with the application as a "magical 
action", or better as a “relation with a magical character," the presence of the 
application itself became less "physical". The absence of references to the control 
panel in all the drawings and the accent placed on the relation, which appears 
especially in the drawings made by girls in the observation group, suggests that the 
issue of the transparency-of-technology effect has been effective in most cases. 
However, the role of the interaction agent can be strengthened, involving him also in 
games, in order to improve the involvement strategy even in those subjects heavily 
concentrated on the games. 
 
 
Fig. 8. Drawing by 1Harrypotter 
One notable exception is represented by 1Harrypotter (Fig. 8), who depicts the 
interaction with the application as a highly problematic experience. The general 
impression is that the rehabilitation experience for 1Harrypotter is frustrating or 
demanding a great amount of energies (preponderant presence of yellow). The 
drawing communicates the attention effort to complete the task, not mitigated by any 
helpful figure. 
The remarks emerged from the drawings are reflected in the semi-structured 
interview to teachers of the Donini school. As privileged observers of the trial in the 
school, teachers confirm the easy learning curve of the application, with some minor 
difficulties at the start, limited to the first days of use of Magicadabra. A critical 
element for the correct development of the rehabilitation was the audio quality. The 
children made some errors in the rehabilitation exericises due to the diffucilties in the  
 

88 
F. Papa and B. Sapio 
comprehension of the auditory stimuli. Overall, teachers evaluated the rehabilitation 
experience using DTV to be pleasant and interesting for children in comparison to 
traditional rehabilitation methods usually considered boring and repetitive. 
5   Conclusions 
The aim of this work was to explore the emotional response of young children in front 
of an application of DTV for dyslexia rehabilitation and the aspects triggering 
emotional reactions. 
The methodology chosen to evaluate young users’ attitudes and their emotional 
reactions towards the technological tool involved structured listening sessions 
including drawings. This approach proved to be very effective given the very young 
age of users and their limited reading/writing skills.  
Starting from the emotions expressed by children in the production of the drawing, 
an attempt was done to highlight the level of satisfaction/frustration produced by the 
rehabilitation experience and the use of the application. 
The overall satisfaction level of the young users encourages further developments 
of DTV interactive applications targeted at children with special needs.  
 
Acknowledgments. The T-government projects promoted by FUB were co-funded 
via a competitive call coming from a joint initiative of the Italian Ministry of 
Communication and the Italian Ministry for Innovation and Technology. The field 
investigation of the Project “T-islessia” was developed in the framework of the 
agreement between Fondazione Ugo Bordoni and the project partners: Cineca, Cresm, 
Indire, Università di Urbino. The authors would like to thank: Sebastiano Trigila 
(Chief of the FUB T-government program) and Gaetano Bruno for the supervision of 
the projects; Roberto Azzano (ANFoV) and Giulia Berni (ANFoV) for their 
contribution to the supervision of user trials. Team researchers in the perception 
study: Laura Parigi (University of Florence), Rudi Bartolini (University of Florence), 
Chiara Paganuzzi (Indire), Silvia Panzavolta (Indire). 
References 
1. Norman, D.A.: Emotional Design: Why We Love (or Hate) Everyday Things. Basic 
Books, New York (2004) 
2. Mahlke, S.: Usability, Aestetics, Emotions and User Experience. Hot Topics 6(2) (2007) 
3. Mahlke, S.: Studying Affect and Emotions as Important Parts of the User Experience. In: 
Workshop on the Role of Emotion in Human-Computer Interaction, HCI conference 
(2005) 
4. Desmet, P.M.A.: Measuring Emotions. Development and Application of An Instrument to 
Measure Emotional Responses to Producs. In: Blythe, M.A., Monk, A.F., Overbeeke, K., 
Wright, P.C. (eds.) Funology: from Usability to Enjoyment. Kluwer Academic Publishers, 
Norwell (2003) 
5. Cornacchia, M., Papa, F., Livi, S., Sapio, B., Nicolò, E., Bruno, G.: Factors Affecting the 
Usage of T-Government Services: an Exploratory Study. In: ICETE 2008 International 
Joint Conference on E-Business and Telecommunications, pp. 315–322. Institute for 
Systems and Technologies of Information, Control and Comunication, Porto (2008) 

 
Emotional Aspects in User Experience with Interactive Digital Television 
89 
6. Turk, T., Sapio, B., Cornacchia, M., Livi, S., Nicolò, E., Papa, F.: Microsimulating the 
Adoption of Digital Television and T-Government Services. In: Workshop on Digital 
Television Revisited: Linking Users, Markets and Policies, pp. 3–14. Cost Action 298, 
Budapest (2008) 
7. Damodaran, L. (ed.): Analogue to Digital Switch-over: a Scoping Study for the Digital 
Television Project. Report CRSP456, Digital Television Project (UK) (2002) 
8. Papa, F., Spedaletti, S.: Broadband Cellular Radio Telecommunication Technologies in 
Distance Learning: a Human Factors Field Study. Personal and Ubiquitous Computing 5, 
231–242 (2001) 
9. Papa, F., Spedaletti, S.: Methodology to Evaluate the Usability of Interactive Services 
Offered through Terrestrial Digital Television (Metodologia per la Valutazione 
dell’Usabilità dei Servizi Interattivi Offerti dalla Televisione Digitale Terrestre). Internal 
Report, Fondazione Ugo Bordoni (2004) 
10. Davis, F.: User Acceptance of Information Technology: System Characteristics, User 
Perceptions and Behavioral Impacts. Int. J. Man-Machine Studies 38, 475–487 (1993) 
11. Venturi, A. (ed.): Experimenting Services with Real Users (Sperimentazione dei Servizi 
con l’Utenza Reale). Technical Report, T-Islessia Project (2006) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 90–97, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Investigation of Normalised Time of Increasing Vocal 
Fold Contact as a Discriminator of Emotional Voice Type 
Peter J. Murphy1 and Anne-Maria Laukkanen2 
1 Department of Electronic and Computer Engineering,  
University of Limerick, Limerick, Ireland 
peter.murphy@ul.ie 
2 Department of Speech Communication and Voice Research,  
University of Tampere, Tampere, Finland 
anne-maria.laukkanen@uta.fi 
Abstract. To date, descriptions of the categorisation of emotional voice type 
have mostly been provided in terms of fundamental frequency (f0), amplitude 
and duration. It is of interest to seek additional cues that may help to improve 
recognition of emotional colouring in speech, and, expressiveness in speech 
synthesis. The present contribution examines a specific laryngeal measure - the 
normalised time of increasing contact of the vocal folds (NTIC) i.e. increasing 
contact time divided by cycle duration - as estimated from the electroglottogram 
signal. This preliminary study, using a single female speaker, analyses the sus-
tained vowel [a:], produced when simulating the emotional states anger, joy, 
neutral, sad and tender. The results suggest that NTIC may not be ideally suited 
for emotional voice discrimination. Additional measures are suggested to fur-
ther characterise the emotional portrayals.   
1   Introduction 
The study of emotion as expressed in speech has been examined in terms of funda-
mental frequency (f0), syllable duration and waveform intensity in a number of stud-
ies; an overview of the literature is provided in [1]. Speaker-independent automatic 
recognition of emotion, using four or five emotions, has been reported to be in the 
range of 50%-60% accuracy [2]. The use of other cues, such as glottal characteristics, 
may allow for improved recognition rates or improved synthesis of emotional attrib-
utes. Relatively few studies have focussed on glottal characteristics of emotional 
voice [1, 3-6]. In the present study a measure of vocal fold contact closing time is 
derived from the electroglottogram for the sustained vowel [a:]. A measure of the 
time for the vocal folds to go from minimum to peak contact is chosen as it is hy-
pothesized that it may discern differences along the activation dimension of emotions 
- anger and joy having high activation levels while sad and tender have low activation 

Investigation of Normalised Time of Increasing Vocal Fold Contact as a Discriminator 
91 
levels. In the following section the electroglottogram and the measurements taken 
from this signal are briefly described.   
2   Electroglottography 
The electroglottograph consists of two electrodes placed external to the larynx (Fig.1). 
A high frequency current is passed between the electrodes and the output signal varies 
depending on the impedance of the substance between the electrodes. As the vocal 
folds vibrate they move through high impedance (open glottis) to low impedance 
(closed glottis) values. As the impedance decreases with contact the electroglottogram 
(EGG) signal provides a measure of vocal fold contact [7,8] (Fig.2 – top row). The 
electroglottogram provides complimentary information to the glottal flow; the maxi-
mum in the electroglottogram occurs when contact is maximum while the maximum in 
the glottal flow occurs during the open phase. An important aspect of the EGG signal 
is that it is essentially free of supra-glottal acoustic influences, which can produce 
source–filter interaction, making glottal flow determination and interpretation difficult. 
 
 
Fig. 1. Electroglottogram (EGG) recording  
2.1   Measurements Taken from the EGG Waveform  
The EGG signal provides a measure of vocal fold contact (Fig.2, row 1) and the first 
(DEGG, row 2) and second derivatives (D2EGG, row3) provide estimates of the 
speed and acceleration of contact, respectively. Fig.2 and Table 1 indicate how the 
measures amplitude, cycle duration (and hence fundamental frequency) and NTIC 
are estimated. 
Estimation of time domain events such as zero crossings can be problematic as 
noise can easily offset the measurement (c.f. [9], e.g. point of maximum contact indi-
cated by the zero crossing in row 2, Fig.2). Using clearly defined peaks in the signal is 
preferable. NTIC is estimated using the cycle peaks of the EGG and D2EGG signals. 

92 
P.J. Murphy and A.-M. Laukkanen 
The point of maximum contact is taken from the EGG signal (this is the positive peak 
per glottal cycle). The point of the start of increasing contact is estimated using the 
D2EGG signal. The positive peak in the D2EGG signal occurs during the rapidly 
rising edge of the peak in the DEGG signal. One sample point back from the D2EGG 
peak is taken as the beginning of increasing contact – this point was initially deter-
mined empirically by viewing the corresponding point on the EGG signal which indi-
cated the beginning of closure. The difference between these times is taken as the 
time of increasing vocal fold contact, which is used as an approximate estimate of 
vocal fold collision closing time. This estimate is then time normalised by dividing by 
the cycle duration to produce the normalised time of increasing vocal fold contact 
(NTIC). This measure contrasts with measures of closing quotient (ClQ) (estimated 
using normalised amplitude quotient – NAQ [9], for example) which estimate the 
time of peak glottal flow to a point of zero or minimum flow. NAQ gives a measure 
of the closing event during the open phase whereas NTIC provides a measuring of 
closing which takes place essentially during the closed phase. 
 
 
Fig. 2. EGG analysis (neutral phonation) (y-axis amplitude – arbitrary units - indicating in-
creasing vocal fold contact, x-axis – sample number – indicating time) top row EGG with 
(nearest sample to) zero crossing points indicated (negative to positive – first asterisk – in the 
glottal cycle, positive to negative – second asterisk – in the glottal cycle), 2nd row DEGG (rate 
of change of vocal fold contact) peaks at closure (asterisk at positive peak) and opening (aster-
isk at negative peak), 3rd row D2EGG (acceleration of vocal fold contact) peak (asterisk at 
positive peak). Measurement symbols (A – amplitude, T0 - cycle duration and NTIC – normal-
ized time of increasing contact – time indicated by the arrows between the parallel lines, di-
vided by T0, are described in Table 1). 
 
 

Investigation of Normalised Time of Increasing Vocal Fold Contact as a Discriminator 
93 
The following measures were estimated from the EGG (and derived) signal(s).  
Table 1. Measures estimated from the electroglottogram (EGG), first derivative of the elec-
troglottogram (DEGG) and the second derivative of the electroglottogram (D2EGG) signals 
Measure 
Symbol 
Description 
Method of Measurement  
T0  
glottal cycle duration 
measured between points of glottal closure as determined from 
the positive peaks in the DEGG signal 
f0 
fundamental frequency  
1/T0 
A 
Amplitude 
Amplitude difference between maximum and minimum in a 
glottal cycle 
NTIC 
normalised time of 
increasing contact – 
closing time divided by 
cycle duration      
closing time is measured from the beginning of contact – one 
sample point before the peak in D2EGG to the max point in the 
EGG signal   
3   Measurement 
The EGG and speech signals were recorded in a sound treated room in the Department 
of Speech Communication and Voice Research, University of Tampere, Tampere, 
Finland. The sustained vowel [a:] was phonated while simulating the emotions anger, 
joy, neutral, sad and tender by a single speaker who was experienced in phonation 
types and emotional portrayals in voice. One hundred and thirty eight cycles of a 
steady portion of the vowel were selected for analysis. The five emotions chosen have 
been analysed in previous studies [1,3]; they reflect positive and negative valence, and 
high and low psycho-physiological activity levels. Sadness and tenderness have low 
activity levels, while joy and anger have a high level of activity. Sadness and anger  
 
 
Fig. 3. EGG segment of the emotion anger (y-axis amplitude – arbitrary units - indicating in-
creasing vocal fold contact, x-axis – sample number – indicating time) top row EGG with 
(nearest sample to) zero crossing points indicated (negative to positive – first asterisk – in the 
glottal cycle, positive to negative – second asterisk – in the glottal cycle), 2nd row DEGG peaks 
at closure (asterisk at positive peak) and opening (asterisk at negative peak), 3rd row D2EGG 
peak (asterisk at positive peak). Measurement symbols (NTIC – normalized time of increasing 
contact, A – amplitude and T0 - cycle duration, are described in Table 1). 

94 
P.J. Murphy and A.-M. Laukkanen 
 
Fig. 4. EGG segment of the emotion joy (y-axis amplitude – arbitrary units - indicating increas-
ing vocal fold contact, x-axis – sample number – indicating time) top row EGG with (nearest 
sample to) zero crossing points indicated (negative to positive – first asterisk – in the glottal 
cycle, positive to negative – second asterisk – in the glottal cycle), 2nd row DEGG peaks at 
closure (asterisk at positive peak) and opening (asterisk at negative peak), 3rd row D2EGG peak 
(asterisk at positive peak). Measurement symbols (NTIC – normalized time of increasing con-
tact, A – amplitude and T0 - cycle duration, are described in Table 1). 
 
Fig. 5. EGG segment of the emotion sad (y-axis amplitude – arbitrary units - indicating increas-
ing vocal fold contact, x-axis – sample number – indicating time) top row EGG with (nearest 
sample to) zero crossing points indicated (negative to positive – first asterisk – in the glottal 
cycle, positive to negative – second asterisk – in the glottal cycle), 2nd row DEGG peaks at 
closure (asterisk at positive peak) and opening (asterisk at negative peak), 3rd row D2EGG peak 
(asterisk at positive peak). Measurement symbols (NTIC – normalized time of increasing con-
tact, A – amplitude and T0 - cycle duration, are described in Table 1). 
have negative valence while joy and tenderness represent positive valence. Fig.3 shows 
the electroglottogram and its first and second derivatives for anger. Figures 4, 5 and 6 
show the same information for joy, sad and tender, respectively. 

Investigation of Normalised Time of Increasing Vocal Fold Contact as a Discriminator 
95 
 
Fig. 6. EGG segment of the emotion tender (y-axis amplitude – arbitrary units - indicating 
increasing vocal fold contact, x-axis – sample number – indicating time) top row EGG with 
(nearest sample to) zero crossing points indicated (negative to positive – first asterisk – in the 
glottal cycle, positive to negative – second asterisk – in the glottal cycle), 2nd row DEGG peaks 
at closure (asterisk at positive peak) and opening (asterisk at negative peak), 3rd row D2EGG 
peak (asterisk at positive peak). Measurement symbols (NTIC – normalized time of increasing 
contact, A – amplitude and T0 - cycle duration, are described in Table 1). 
NTIC is estimated as described in section 2.1 and values are taken for all the emo-
tional portrayals.  
4   Results 
Fig.7 shows NTIC versus cycle number for the emotions anger (a), joy (j), neutral (n), 
sad (s) and tender (t). Average values and standard deviations are provided in Table 2.  
Table 2. Electroglottogram based measures averaged over 138 glottal cycles (mean, std. dev.) 
EGG  
Measure/ 
Emotion 
f0 (fundamental 
frequency – mean, 
std. dev.) 
NTIC (normalised time of 
increasing contact – 
mean, std. dev.) 
Anger  
180, 2 
0.20, 0.03 
Joy 
178, 3 
0.17, 0.02 
Neutral 
177, 2 
0.14, 0.01 
Sad 
175, 2 
0.17, 0.02 
Tender 
176, 2 
0.15, 0.01 
 
Measure/ 
Emotion  
Fundamental 
Frequency (f0) 
mean (std. dev.) 
Normalised Time of Increasing 
Contact (NTIC) 
mean (std. dev.) 
Anger 
180 (2) 
0.20 (0.03) 
Joy  
178 (3) 
0.16 (0.02) 
Neutral 
177 (2) 
0.14 (0.01) 
Sad 
175 (2) 
0.17 (0.02) 
Tender  
176 (2) 
0.15 (0.01) 

96 
P.J. Murphy and A.-M. Laukkanen 
0
0,05
0,1
0,15
0,2
0,25
0,3
0,35
1
11
21
31
41
51
61
71
81
91
101
111
121
131
cycle no. 
NTIC (normalised time of increasing vocal fold 
contact - time of increasing contact/cycle length - 
dimensionless ratio)
NTIC_a
NTIC_j
NTIC_n
NTIC_s
NTIC_t
 
Fig. 7. Normalised time of increasing vocal fold contact (NTIC) versus glottal cycle number for 
the emotional portrayals anger, joy, neutral, sad and tender 
From Fig.7 and Table 2 it can be seen that NTIC is highest for anger and lowest 
for neutral.   
5   Discussion 
Normalised time of increasing contact (NTIC) does not show discriminatory ability along 
the activation dimension. For higher/lower activity NTIC might be expected to be ordered 
as anger and joy>neutral>sad and tender (with higher activity indicated by a reduced time 
of increasing contact). However, the ordering is anger>joy=sad>tender>neutral. Some 
reasons for the higher values for anger include (i) the portrayal was cold anger (so not 
particularly high in intensity in the acoustic signal), (ii) amplitude of contact is greater, (iii) 
the adductory configuration (as indicated by the greater closed quotient, Fig.3) insures 
more pressing of the folds at closure and (iv) velocity of contact at closure is not as great 
as for joy or neutral (comparing the peak amplitudes in row 2 of Fig.2, 3 and 4). Although 
f0 revealed a trend as expected from an activation viewpoint, i.e. greater for the higher 
activity emotions, this was very minor and hence the f0 variation could be ignored in the 
NTIC evaluation (even though NTIC is normalised for glottal cycle duration, changes in 
the cycle duration can give rise to additional changes in glottal waveform structure [10]). 
A previous study of emotional voice portrayals indicates that anger can be separated 
along the activation dimension using a measure of closing quotient (ClQ) [1]. As stated 
previously ClQ is primarily an open glottis measure whereas NTIC is essentially a 
closed glottis measure. If the closed quotient increases (as shown in Fig.3 for anger) 
then the shorter open phase contributes to a shorter glottal flow closing time. 
6   Conclusion 
This preliminary study of a single female speaker portraying different emotions sug-
gests that the normalised time of increasing contact (NTIC) does not discriminate 

Investigation of Normalised Time of Increasing Vocal Fold Contact as a Discriminator 
97 
between emotions along the activation dimension. However, other measures of the 
glottal closure event (c.f. [1] or as outlined in [11]) may show promise for the auto-
matic recognition of emotion e.g. contacting quotient, speed quotient, velocity and 
acceleration of contact at closure may distinguish between the different emotions. 
Future work will examine other glottal measures and will involve a more detailed 
statistical analysis with more speakers.        
Acknowledgements 
The recording and initial analysis development was performed during a COST 2103 
(Advanced Voice Function Assessment) supported short-term scientific mission to the 
Department of Speech Communication and Voice Research, University of Tampere, 
Tampere, Finland in December, 2007.      
References 
1. Airas, M., Alku, P.: Emotions in vowel segments of continuous speech: analysis of the 
glottal flow using the normalized amplitude quotient. Phonetica 63, 26–46 (2006) 
2. McGilloway, S., Cowie, R., Douglas-Cowie, E., Gielen, S., Westerdijk, M., Stroeve, S.: 
Approaching automatic recognition of emotion from voice: a rough benchmark. In: Pro-
ceedings of the ISCA work-shop on Speech and Emotion (Belfast), pp. 207–212 (2000) 
3. Toivanen, J., Waaramaa, T., Alku, P., Laukkanen, A.-M., Seppänen, T., Väyrynen, E., 
Airas, M.: Emotions in [a]: A perceptual and acoustic study. Logopedics Phoniatrics Vo-
cology 31, 43–48 (2006) 
4. Gobl, C., Ní Chasaide, A.: The role of voice quality in communicating emotion, mood and 
attitude. Speech Communication 40, 189–212 (2003) 
5. Laukkanen, A.-M., Vilkman, E., Alku, P., Oksanen, H.: Physical variations related to 
stress and emotional state: a preliminary study. J. Phonetics 24, 313–335 (1996) 
6. Cummings, K.E., Clements, M.A.: Analysis of the glottal excitation of emotionally styled 
and stressed speech. J. Acoust. Soc. Am. 98, 88–98 (1995) 
7. Rothenberg, M., Mashie, J.J.: Monitoring vocal fold abduction through vocal fold contact 
area. J. Speech Hear Res. 31, 338–351 (1988) 
8. Titze, I.: Interpretation of the electroglottographic signal. J. Voice 4, 1–9 (1990) 
9. Alku, P., Bäckström, T., Vilkman, E.: Normalised amplitude quotient for parameterization 
of the glottal flow. J. Acoust. Soc. Am. 112, 701–710 (2002) 
10. Murphy, P.: Voice source change during fundamental frequency variation. In: Esposito, 
A., Faundez-Zanuy, M., Keller, E., Marinaro, M. (eds.) COST Action 2102. LNCS 
(LNAI), vol. 4775, pp. 165–173. Springer, Heidelberg (2007) 
11. Murphy, P., Laukkanen, A.-M.: Electroglottogram analysis of emotionally styled phona-
tion. In: Esposito, A., Hussain, A., Marinaro, M., Martone, R. (eds.) Multimodal Signals 
2008. LNCS (LNAI), vol. 5398, pp. 264–270. Springer, Heidelberg (2008) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 98–105, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Evaluation of Speech Emotion Classification Based on 
GMM and Data Fusion 
Martin Vondra and Robert Vích 
Institute of Photonics and Electronics, Academy of Sciences of the Czech Republic  
Chaberská 57, CZ 18251 Prague 8, Czech Republic 
{vondra,vich}@ufe.cz 
Abstract. This paper describes continuation of our research on automatic emotion 
recognition from speech based on Gaussian Mixture Models (GMM). We use 
similar technique for emotion recognition as for speaker recognition. From 
previous research it seems to be better to use a lesser number of GMM 
components than is used for speaker recognition and better results are also 
achieved for a greater number of speech parameters used for GMM modeling. In 
previous experiments we used suprasegmental and segmental parameters 
separately and also together, which can be described as fusion on feature level. 
The experiment described in this paper is based on an evaluation of score level 
fusion for two GMM classifiers used separately for segmental and suprasegmental 
parameters. We evaluate two techniques of score level fusion – dot product of 
scores from both classifiers and maximum selection and maximum confidence 
selections. 
1   Introduction 
Speech carries several types of information. The first is the semantic information – 
the meaning, the second is the information about the speaker – who is speaking and 
last but not least, it is the style of speaking used by the speaker. Style of speaking is 
mainly related to the expression of emotions.  
Automatic emotion classification from speech can be utilized for example in call-
centers to identify nonstandard events in the automatic system which can be switched over 
to a human operator, or in computer dialog systems for improving naturalness of 
communication. Emotion classification can also improve interactive computer games, etc. 
In the literature several techniques for recognition of emotions from speech were 
proposed. The problem can be divided into two main tasks – feature extraction and 
their classification. There are two basic approaches for feature extraction. The first 
approach is based on the estimation of some statistics from the fundamental frequency 
contour, intensity contour, formant frequencies contours, speaking rate, etc. [1], [2]. 
The most general statistics used are the mean, median, variance, maximum, 
minimum, range, etc. Researchers also use some statistics of the first derivative of the 
contours. More parameters usually contribute to better recognition score, but also 
increase computation requirements. Feature selection algorithms are used to select the 
best features. The second approach of feature extraction is the classical short-time 

 
Evaluation of Speech Emotion Classification Based on GMM and Data Fusion 
99 
parameterization of speech. Here are mainly used segmental parameters, like LPCC 
(Linear Prediction Cepstral Coefficients), MFCC (Mel-Frequency Cepstral 
Coefficients), RASTA-PLP (Relative Spectral Transform – Perceptual Linear 
Prediction) [3], LFPC (Log Frequency Power Coefficients) [4], etc. The second part 
of automatic recognition of emotions from speech is the classifier. Here the statistical 
classifier – HMM (Hidden Markov Models) [4], GMM [3] or machine learning 
algorithms – SVM (Support Vector Machines), KNN (K Nearest Neighbor), decision 
trees, neural networks [2], etc. can be used. To improve recognition accuracy the 
combination or fusion of classifier begins to appear. 
In [5] we described our first experiments with the recognition of emotions based on 
GMM. We tested two parameter sets – suprasegmental (fundamental frequency F0, 
energy and their first and second differences) and segmental (13 Mel-Frequency 
Cepstral Coefficients and their first and second differences). We also tested the 
combination of these two parameter sets. This technique is known as feature level 
fusion. We examined also the optimum number of GMM components, which seems 
to be a more significant feature than the used parameter set, but the optimum number 
of components for the used parameter set has to be selected. In summary, it is better 
to use a lesser number of GMM components than it is used for speaker recognition. In 
[6] we enlarged the parameter sets using parameters computed from the Teager 
energy operator [7], but almost without any influence on the recognition score. 
Now we have decided to test also the capability of two separate GMM classifiers, 
first for suprasegmental parameters and second for segmental parameters and to apply 
fusion on score level. 
2   Configuration of Experiment 
2.1   Speech Database 
For our experiments the German emotional speech database [8] was used. It contains 
7 simulated emotions (anger, boredom, disgust, fear, joy, neutral, and sadness), 
simulated by 10 actors (5 male, 5 female) and 10 sentences (5 short, 5 longer). 
Sampling frequency of this database is 16 kHz. 
The complete database was evaluated in a perception test by the authors of the 
database. Utterances, for which the emotion with which they were spoken were 
recognized better than 80% and judged as natural by more than 60% of the listeners, 
were used for the experiment. The speech material used for our experiment contains 
535 utterances. The structure of the used speech material is summarized in Table 1. 
The test was performed as cross validated and speaker independent. Configuration 
of the training and testing sets is obvious from Table 2. Speakers are marked by 
numbers, which mean: 
03 – 1st male speaker, 
08 – 1st female speaker, 
09 – 2nd female speaker, 
10 – 2nd female speaker, 
11 – 3rd male speaker, 
 

100 
M. Vondra and R. Vích 
12 – 4th male speaker, 
13 – 3rd female speaker, 
14 – 4th female speaker, 
15 – 5th male speaker, 
16 – 5th female speaker. 
The final results for all speakers and all sentences were obtained in three iterations by 
changing the speaker groups for training and testing. 
Table 1. Structure of the German emotional speech database 
Emotions 
Number of 
utterances 
Total time 
[min] 
Anger 
127 
5.59 
Boredom 
81 
3.75  
Disgust 
46 
2.57  
Fear 
69 
2.57  
Joy 
71 
3.01  
Neutral 
79 
3.11  
Sadness 
62 
4.19  
2.2   Speech Parameters 
For our experiment we have chosen two parameter sets. The parameters were 
computed for short-time speech frames similarly as in speech recognition – one 
feature vector for each frame. The frame length was set to 25ms with frame shift 
10ms. The parameter sets for training of GMMs were obtained by concatenation of 
feature vectors from all training sentences. This parameterization for emotion 
recognition is somewhat different from the experiment described e.g. in [2], where 
global parameters from each utterance are used, e.g. the mean, maximum and 
minimum values of F0, the maximum steepness and dispersion of F0, etc. We assume 
that these characteristics are covered in our parameter sets and can be caught by the 
GMM model. 
Our first parameter set contains only suprasegmental features – the fundamental 
frequency (F0) and the intensity contours. We use mean subtracted natural logarithm 
of these parameters. The second parameter set contains only segmental features – 12 
MFCC. We do not use mean subtraction of MFCC. The last parameter set is based on 
feature level fusion and contains the combination of the first and second parameter 
sets, i.e. the F0 and intensity contours and 12 MFCCs. To each parameter set also the 
delta and delta-delta coefficients of basic parameters were added. 
For F0 estimation the wavesurfer [9] script was used. In unvoiced parts of speech 
F0 was replaced by values obtained by piecewise cubic Hermite interpolation (see 
Matlab function “pchip”). For MFCC computation the Matlab voicebox toolbox [10] 
was used. 
 

 
Evaluation of Speech Emotion Classification Based on GMM and Data Fusion 
101 
Table 2. Configuration of cross validated training and testing sets 
 Step   
Emotion 
Training set 
Testing set 
Speakers 
  
10, 11, 12, 13, 14, 15, 16 
03, 08, 09 
Anger 
88 utt.,   3.91 min.  
39 utt.,   1.68 min. 
Boredom 
62 utt.,   2.84 min. 
19 utt.,   0.91 min. 
Disgust 
37 utt.,   2.06 min. 
 9 utt.,    0.51 min. 
Fear 
58 utt.,   2.14 min. 
11 utt.,   0.43 min. 
Joy 
49 utt.,   2.13 min. 
22 utt.,   0.88 min. 
Neutral 
49 utt.,   1.89 min. 
30 utt.,   1.22 min. 
1 
Number of 
utterances 
and total 
time 
Sadness 
42 utt.,   2.74 min. 
20 utt.,   1.45 min. 
Speakers 
  
03, 08, 09, 10, 14, 15, 16  
11, 12, 13 
Anger 
92 utt.,   4.03 min. 
35 utt.,   1.56 min. 
Boredom 
58 utt.,   2.66 min. 
23 utt.,   1.09 min. 
Disgust 
34 utt.,   1.95 min. 
12 utt.,   0.62 min. 
Fear 
46 utt.,   1.73 min. 
23 utt.,   0.84 min. 
Joy 
51 utt.,   2.10 min. 
20 utt.,   0.91 min. 
Neutral 
57 utt.,   2.23 min. 
22 utt.,   0.88 min. 
2 
Number of 
utterances 
and total 
time 
Sadness 
46 utt.,   3.10 min. 
16 utt.,   1.09 min. 
Speakers 
  
03, 08, 09, 11, 12, 13 
10, 14, 15, 16 
Anger 
74 utt.,   3.24 min. 
53 utt.,   2.35 min. 
Boredom 
42 utt.,   2.00 min. 
39 utt.,   1.75 min. 
Disgust 
21 utt.,   1.14 min. 
25 utt.,   1.43 min. 
Fear 
34 utt.,   1.27 min. 
35 utt.,   1.30 min. 
Joy 
42 utt.,   1.79 min. 
29 utt.,   1.22 min. 
Neutral 
52 utt.,   2.10 min. 
27 utt.,   1.01 min. 
3 
Number of 
utterances 
and total 
time 
Sadness 
36 utt.,   2.54 min. 
26 utt.,   1.65 min. 
2.3   Gaussian Mixture Models  
For recognition of emotions we use GMMs with full covariance matrix [11]. The 
GMM modeling was implemented in C programming language. For training of 
GMMs we used the Expectation Maximization (EM) algorithm with 10 iteration 
steps. The iteration stop based on the difference between the previous and actual 
probabilities has shown to be ineffective, because the magnitude value depends on the 
number of training data and we have found out that relative probability in the training 
has a very similar behavior for different input data. For several initial iterations the 
probability that GMM parameters belong to training data steeply grows up, then 
becomes less and after 8 to 10 iterations the probability reaches the limit value. For 
initialization the Vector Quantization (VQ) – K-means algorithm was used. 

102 
M. Vondra and R. Vích 
3   Score Level Fusion 
Two methods of fusion have been considered in this paper [12]. Each GMM classifier 
returns probabilities that the tested utterance belongs to the GMM model, which is 
trained for each emotion category. This probability is called as score.  
The first technique of score level fusion is based on the dot product of scores from 
GMM estimation. We use two GMM classifiers, the first for the suprasegmental 
parameter set and the second for the segmental parameter set. For the first classifier 
we have trained models 
)
(
1 i
S
 and for the second classifier we have trained models 
)
(
2 i
S
, where i = 1,…, N (N is the number of emotions, in our case 7). If we denote 
the score that returns the first GMM classifier for model 
)
(
1 i
S
 and tested utterance X 
as 
(
))
(
,
1
1
i
S
X
score
 and the score that returns the second GMM classifier for model 
)
(
2 i
S
 and tested utterance X as 
(
))
(
,
2
2
i
S
X
score
 then the overall score is given by 
(
)
(
)
(
))
(
,
)
(
,
,
2
2
1
1
i
S
X
score
i
S
X
score
i
X
scoretotal
⋅
=
. 
(1) 
The identification of emotion 
*i  is given by the maximum overall score for the given 
emotion 
(
)i
X
score
i
total
N
i
,
max
arg
1
*
≤
≤
=
. 
(2) 
The second method of score level fusion, which we have tested, is based on 
maximum confidence selection. Confidence measure gives information how distinctive 
is the assessment of the given classifier. The confidence is high when the score for one 
model is significantly higher than for the other models. On the other hand the 
confidence is small when the score is very similar for each model. Based on this idea, 
the confidence measure can be defined as 
max
2
max
1
score
score
c
−
=
, 
(3) 
where 
max
score
 and 
2
max
score
 are the highest and second highest score. In practice 
we compute the confidence measures for each GMM classifier and the estimated 
emotion is determined from that GMM model which gives the higher confidence.  
4   Results 
The overall recognition score for various configuration of emotion recognition based 
on GMM is shown in Fig. 1. In this figure the overall recognition score is depicted for 
GMM classification for the suprasegmental parameter set (Fundamental frequency F0 
+ Energy E), for segmental parameter set (MFCC + Energy E), for feature level 
fusion (combinations of previous parameter sets) and for score level fusion (separate 
GMM classifier for segmental and suprasegmental parameter set). Numbers on the x 
axis represent the number of GMM components used for the appropriate classifier.  
 
 

 
Evaluation of Speech Emotion Classification Based on GMM and Data Fusion 
103 
0,00
10,00
20,00
30,00
40,00
50,00
60,00
70,00
80,00
90,00
100,00
8
16
32
4
6
8
16
32
4
8
16
32
F0E32 & CCE4
F0E16 & CCE4
F0E32 & CCE4
F0E8 & CCE4
F0 + E
CC+E
feature level fusion
CC+E+F0
score level
fusion -
dot
score level
fusion -
confidence
 
Fig. 1. Overall recognition scores in per cent for various configurations of emotion recognition 
based on GMM classifiers 
Table 3. Confusion matrix for the best configuration of score level fusion. The numbers are the 
percentages of recognized utterances of the category in the top line versus the number of 
utterances for emotions in the left column.  
  
Anger 
Bored. 
Disgust 
Fear 
Joy 
Neutral 
Sad. 
Anger 
92.91 
0.00 
0.00 
1.57 
5.51 
0.00 
0.00 
Boredom 
2.47 
75.31 
4.94 
0.00 
0.00 
12.35 
4.94 
Disgust 
6.52 
0.00 
80.43 
0.00 
2.17 
6.52 
4.35 
Fear 
20.29 
0.00 
1.45 
33.33 
18.84 
18.84 
7.25 
Joy 
36.62 
0.00 
4.23 
2.82 
53.52 
2.82 
0.00 
Neutral 
0.00 
11.39 
3.80 
2.53 
2.53 
77.22 
2.53 
Sadness 
0.00 
8.06 
3.23 
0.00 
0.00 
0.00 
88.71 
 
There are many more combinations for score level fusion (two parameter sets and 
various number of GMM components), but in Fig. 1 only the best results are depicted. 
It can be seen from Fig. 1 that the best score is achieved for score level fusion. 
Slightly better results give the dot product of scores and maximum selections. It is 
interesting to note that the best result of score level fusion does not result from the 
best individual classifiers.  

104 
M. Vondra and R. Vích 
Confusion matrices can provide more detailed results than is given by the overall 
score, see Table 3. It can be said that the results for score level fusion have similar 
tendency as the individual classifier. Confusion matrices for one GMM classifier and 
also for feature level fusion are given in [5] or [6]. Here in Table 3 are the results for 
the best configuration of the score level fusion, i.e. the dot product of the score for the 
suprasegmental parameter set (16 GMM components) and the score for the segmental 
parameter set (4 GMM components). The best recognized emotions, equally with [5] 
or [6] are here anger and sadness, followed by boredom and neutral. More difficult 
are disgust and joy and the most difficult seems to be fear. Joy generates most 
confusion and is recognized as anger, and fear is often recognized as anger. 
5   Conclusion 
The score level fusion outperforms feature level fusion by approximately 4%. The 
improvement in comparison with individual GMM is about 6%. We examined two 
methods of score level fusion: 
• 
based on dot product of scores from individual GMM classifiers and maximum 
selections and 
• 
based on maximum confidence selections. 
Both techniques offer almost the same results, but dot product of scores and maximum 
selections give slightly better results. 
Similar conclusions like in [5] and [6] can be also made here. The score level 
fusion for suprasegmental and segmental parameter sets and GMM classifiers has a 
good differentiation property between emotions with high stimulation (anger, fear, 
disgust, joy) and between emotions with small stimulation (boredom, neutral, 
sadness), but shows poor differentiation between emotions with similar stimulation, 
like fear and anger.  
The results achieved in our experiment are very comparable to those obtained in 
[3], where the same database was used and those in [2], where another emotional 
speech database was used. 
Future work will be focused on fusion of more different classifiers, e.g. SVM, 
KNN or on selection of better feature extraction. We want also to examine separation 
of classification into two stages. The first stage would separate emotions with high 
and small stimulation and the second stage would then decide from a lesser group of 
emotions, which could possibly contribute to higher recognition accuracy. Another 
future aim is the application of GMM for conversion of neutral speech into emotive 
speech for enrichment of speaking styles of our TTS system.  
Acknowledgement 
This paper has been supported within the framework of COST2102 by the Ministry of 
Education, Youth and Sport of the Czech Republic, project number OC08010 and by 
the National research program “Targeted Research” of the Academy of Sciences of 
the Czech Republic, project number 1QS108040569. 

 
Evaluation of Speech Emotion Classification Based on GMM and Data Fusion 
105 
References 
1. Dellaert, F., Polzin, T., Waibel, A.: Recognizing Emotion in Speech. In: The Fourth 
International Conference on Spoken Language Processing ICSLP 1996, Philadelphia, pp. 
1970–1973 (1996) 
2. Morrison, D., Wang, R., De Silva, L.C.: Ensemble methods for spoken emotion recognition 
in call-centers. Speech Communication 49, 98–112 (2007) 
3. Truong, K.P., Leeuven, D.A.: An ‘open-set’ detection evaluation methology for automatic 
emotion recognition in speech. In: ParaLing 2007: Workshop on Paralinguistic Speech - 
between models and data, Saarbrücken, Germany (2007) 
4. Nwe, T.L., Foo, S.W., DeSilva, L.C.: Speech emotion recognition using hidden Markov 
models. Speech Communication 41, 603–623 (2003) 
5. Vondra, M., Vích, R.: Recognition of Emotions in German Speech using Gaussian Mixture 
Models. In: Esposito, A., Hussain, A., Marinaro, M., Martone, R. (eds.) Multimodal 
Signals 2008. LNCS, vol. 5398, pp. 256–263. Springer, Heidelberg (2008) 
6. Vondra, M., Vích, R.: Evaluation of Automatic Speech Emotion Recognition Based on 
Gaussian Mixture Models. In: Proc. 19. Konferenz Elektronische Sprachsignalverarbeitung, 
Frankfurt am Main, September 8-10, pp. 172–176 (2008) 
7. Vích, R., Vondra, M.: Experimente mit dem Teager Energie Operator. In: Proc. 19. 
Konferenz Elektronische Sprachsignalverarbeitung, Frankfurt am Main, September 8-10, 
pp. 29–36 (2008) 
8. Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W., Weiss, B.: A Database of 
German Emotional Speech. In: Proc. Interspeech 2005, Lisbon, Portugal, September 4-8 
(2005) 
9. Sjölander, K., Beskow, J.: Wavesurfer, 
 http://www.speech.kth.se/wavesurfer/  
10. Brookes, M.: VOICEBOX: Speech Processing Toolbox for MATLAB,  
 http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html 
11. Reynolds, D.A.: Speaker identification and verification using Gaussian mixture speaker 
models. Speech Communication 17, 91–108 (1995) 
12. Kinnunen, T., Hautamäki, V., Fränti, P.: On the fusion of dissimilarity- based classifiers 
for speaker identification. In: Proc. 8th European Conference on Speech Communication 
and Technology (Eurospeech 2003), Geneva, Switzerland, pp. 2641–2644 (2003) 
13. Shami, M., Verhelst, W.: An evaluation of the robustness of existing supervised machine 
learning approaches to the classification of emotions in speech. Speech Communication 49, 
201–212 (2007) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 106–115, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Spectral Flatness Analysis for Emotional Speech 
Synthesis and Transformation 
Jiří Přibil1 and Anna Přibilová2 
1 Institute of Photonics and Electronics, Academy of Sciences CR, v.v.i., 
Chaberská 57, CZ-182 51 Prague 8, Czech Republic 
and 
Institute of Measurement Science, SAS, 
Dúbravská cesta 9, SK-841 04 Bratislava, Slovakia 
Jiri.Pribil@savba.sk 
2 Slovak University of Technology, Faculty of Electrical Engineering & Information 
Technology, Dept. of Radio Electronics, Ilkovičova 3, SK-812 19 Bratislava, Slovakia 
Anna.Pribilova@stuba.sk 
Abstract. According to psychological research of emotional speech different 
emotions are accompanied by different spectral noise. We control its amount by 
spectral flatness according to which the high frequency noise is mixed in voiced 
frames during cepstral speech synthesis. Our experiments are aimed at 
statistical analysis of spectral flatness in three emotions (joy, sadness, anger), 
and a neutral state for comparison. Calculated histograms of spectral flatness 
distribution are visually compared and modelled by Gamma probability 
distribution. Obtained statistical parameters and emotional-to-neutral ratios of 
their mean values show good correlation for both male and female voices and 
all three emotions. 
Keywords: spectral flatness, speech analysis and synthesis, emotional speech. 
1   Introduction 
Spectral flatness (SF) is a useful measure to distinguish between voiced and unvoiced 
speech [1]. Its usage in speech processing can be extended to empty speech pauses 
identification [2], whispered speech recognition in noisy environment [3], or voicing 
transition frequency determination in harmonic speech modelling [4]. In cepstral 
speech synthesis the spectral flatness measure was used to determine voiced/unvoiced 
energy ratio in voiced speech [5]. According to psychological research of emotional 
speech different emotions are accompanied by different spectral noise [6]. We control 
its amount by spectral flatness measure according to which the high frequency noise 
is mixed in voiced frames during cepstral speech synthesis [7]. We perform the 
statistical analysis of spectral flatness values in voiced speech for four emotional 
states: joy, sadness, anger, and a neutral state. Obtained statistical results of the 
spectral flatness ranges and values are shown also in the form of histograms in a way 
similar to that used by other authors for prosodic emotional features [8], [9]. 

 
Spectral Flatness Analysis for Emotional Speech Synthesis and Transformation 
107 
2   Subject and Method 
As follows from the experiments, the SF values depend on a speaker, but they do not 
depend on nationality (it was confirmed that it holds for the Czech and Slovak 
languages). Therefore the created speech database consists of neutral and emotional 
sentences uttered by each of several speakers (extracted from the Czech and Slovak 
stories performed by professional actors). Analysis must be preceded by classification 
and sorting process of the SF values in dependence on voice type (male / female) and 
speaking style (neutral / emotional). The performed statistical analysis of spectral 
flatness values consists of the two parts: 
1. determination of basic statistical parameters of the SF values, 
2. calculation and building of histograms. 
Practical evaluation of obtained results is further processed in three ways: 
1. determination of mean ratio between neutral and emotional states, 
2. visual comparison of histogram figures, 
3. histograms fitting and modelling by Gamma distribution – comparison of 
parameters α, λ and Root Mean Square (RMS) approximation error. 
2.1   Spectral Flatness Calculation Overview 
The spectral flatness measure SF calculated during the cepstral speech analysis is 
defined as 
,
ln
exp
2
1
2
2
2
1
2
2
∑
∑
=
=
⎥⎦
⎤
⎢⎣
⎡
=
FFT
FFT
FFT
FFT
N
k
k
N
N
k
k
N
F
S
S
S
 
(1) 
where the values 
2
k
S
 represent the magnitude of the complex spectrum, and NFFT is 
number of points of the Fast Fourier Transform (FFT) [10]. The SF values lie 
generally in the range of (0 ÷ 1) − the zero value represents totally voiced signal (for 
example pure sinusoidal signal); in the case of SF = 1, the totally unvoiced signal is 
classified (for example white noise signal). According to the statistical analysis of the 
Czech and Slovak words the ranges of SF = (0 ÷ 0.25) for voiced speech frames and 
SF = (0 ÷ 0.75) for unvoiced frames were evaluated. 
For voiceness frame classification, the value of detected pitch-period L was used. If 
the value L ≠ 0, the processed speech frame is determined as voiced, in the case of L = 0 
the frame is marked as unvoiced – see Fig. 1. On the border between voiced and 
unvoiced part of speech signal a situation can occur that the frame is classified as voiced, 
but the SF value corresponds to the unvoiced class. For correction of this effect, the 
output values of the pitch-period detector are filtered by a 3-point recursive median filter. 
The demonstration example in Fig. 2 shows the input speech signal with detected 
pitch frequency F0 (pitch period reciprocal) and calculated SF values with voiceness 
classification. The influence of median filtration applied to the L values is documented 
in Fig. 3. 

108 
J. Přibil and A. Přibilová 
 
Voiceness 
classification
{SF} 
All frames 
Voiced (L ≠ 0)
Unvoiced (L = 0) 
Segmentation
|FFT| 
Windowing
Pitch period 
detection
Speech signal 
L
Spectral 
flatness 
Median filtration
{SF v}
{SF u}
Fig. 1. Partial block diagram of speech analysis with spectral flatness values calculation 
0
50
100
150
200
250
- 00
0
50
100
150
—› s*100 [-]/F0 [Hz]
—› N [frames]
s(n)
F0, NF = 272
0
50
100
150
200
250
0
1
2
3
4
5
—› SF [-]
—› N [frames]
Voiced frames
Unvoiced frames
-50
1
a)
0.
0.
0.
0.
0.
b)
 
Fig. 2. Demonstration of spectral flatness calculation process: input speech signal – sentence 
“Lenivý si a zle gazduješ” (“You are lazy and you keep your house ill”) pronounced in angry 
emotional style, male Slovak speaker with F0 contour (a), spectral flatness for voiced and 
unvoiced frames (b) 

 
Spectral Flatness Analysis for Emotional Speech Synthesis and Transformation 
109 
 
Male 
{SFv} Spectral flatness values 
(of voiced frames only) 
Emotion style 
classification 
All Neutral 
Joy 
Sadness
Anger
Female 
Emotion style 
classification 
All 
Neutral
Joy 
Sadness
Anger
Voice type 
classification
Classified 
spectral flatness 
values 
60
80
100
120
140
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
F
—› N [frames]
Voiced frames
Unvoiced frames
60
80
100
120
140
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
—› SF [-]
—› N [frames]
Voiced frames
Unvoiced frames
—› S  [-]
 
Fig. 3. Influence of median filtration pitch-period values on the SF voiceness classification 
process (detail of frames 65÷140): without filtration (left), applied median filtration (right) 
2.2   Statistical Analysis of Spectral Flatness Values 
We compute the SF values of the sentences in the basic (“Neutral”) speech style and the 
SF values of the sentences pronounced in the emotional states (“Joy”, “Sadness”, and 
“Anger”) and perform statistical analysis of these values. In our algorithm, the SF values 
obtained from the speech frames classified as voiced are separately processed in 
dependence on voice type (male/female). For every voice type the SF values are 
subsequently sorted by emotional styles and stored in separate stacks. These 
classification operations are performed manually, by subjective listening method – see 
the block diagram in Fig. 4. Next operations with the stacks were performed 
automatically – calculation of statistical parameters: minimum, maximum, mean values 
and standard deviation (STD). From the mean SF values the ratio between emotional and 
neutral states is subsequently calculated. As the graphical output used for visual 
comparison (subjective method), the histogram of sorted SF values for each of the stacks 
is also calculated. These histograms can also be fitted and modelled by the Gamma 
distribution (objective evaluation method). For the summary comparison the stack with 
all emotional styles is filled and processed – see the block diagram in Fig. 5. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4. Block diagram of used manual classification method of the SF values 

110 
J. Přibil and A. Přibilová 
{SFv}
Statistical parameters
Stack “All”
Sorting
Histogram
{SFv neutral}
Stack “Neutral”
Classified
pectral flatness
values
{SFv joy}
Stack “Joy”
{SFv sadness}
Stack “Sadness”
{SFv anger}
Stack “Anger”
Sorting
Histogram
Sorting
Min
Max
Std
Mean
Histogram
Ratio of
SF mean
X:Neutral
Histograms
visual
comparison
Histograms
fitting &
modeling
by Gamma
distribution
Min
Max
Std
Mean
Min
Max
Std
Mean
Results evaluation
s
 
Fig. 5. Block diagram of used automatically processed operations with the stack filled with 
classified SF values of voiced frames 
2.3   Histograms Fitting and Modelling by Gamma Distribution 
The generalized Gamma distribution of the random variable X is given by the 
probability density function (PDF) [11], [12] 
( )
( )
,0
,0
,0
1
>
>
≥
Γ
=
−
−
λ
α
α
λ
λ
α
x
e
x
x
f
x
a
 
(2) 
where α is a shape parameter and λ is a scale parameter. The Gamma function is 
defined by 
 
1
0
dx
e
x
x


f³
 
*
D
D
. 
(3) 
The graphs of the PDFs for different parameters α, λ are shown in Fig. 6. 
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
—› x [-]
—› f(x) [-]
alpha=0.5,lambda=0.5
alpha=1.5,lambda=0.5
alpha=5.5,lambda=0.5
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
—› x [-]
—› f(x) [-]
alpha=0.5,lambda=1.5
alpha=1.5,lambda=1.5
alpha=5.5,lambda=1.5
0
 
Fig. 6. Example of the Gamma probability density functions for λ = 0.5 (left), λ = 1.5 (right) 

 
Spectral Flatness Analysis for Emotional Speech Synthesis and Transformation 
111 
The shape and scale parameters of the Gamma distribution enable easy and rather 
accurate modelling of obtained histograms of SF values. It means finding of α  and λ 
parameters for minimum RMS error between the histogram envelope curve and the 
Gamma PDF. Simultaneous control of two parameters represents a two-dimensional 
regulation process. Its practical realization with sufficient precision is a difficult task. 
Therefore, a simplified control method was used – only one parameter is changed and 
the second one has a constant value. The developed algorithm can be divided into 
three phases: 
1. Initialization phase: 
− Fitting the histogram bars by the envelope curve 
− Rough estimation of α, λ parameters 
− Calculation of the Gamma PDF 
− Calculation of the RMS error, storing this value to the memory 
2. Finding the RMS minimum by change of α parameter: 
− Modification of α parameter with constant value of λ parameter 
− Calculation of the Gamma PDF and the RMS error, storing to the memory 
− Comparison of the current RMS error with the last value from the memory 
(repeating of steps in this phase until the minimum of RMS) 
3. Finding the RMS minimum by change of λ parameter: 
− Modification of λ parameter with constant value of α parameter 
− Calculation of the Gamma PDF and the RMS error, storing to the memory 
− Comparison of the current RMS error with the last value from the memory 
(repeating of steps in this phase until the minimum of RMS) 
3   Material, Experiments and Results 
The speech material for spectral flatness analysis was collected in two databases 
(separately of male – 134 sentences, and female voice – 132 sentences) consisting of 
sentences with duration from 0.5 to 5.5 seconds. The sentences of four emotional states – 
“Neutral”, “Joy”, “Sadness”, and “Anger” were extracted from the Czech and Slovak 
stories narrated by professional male and female actors. Pitch-contours given with the 
help of the PRAAT program [13] were used for segment determination as voiced or 
unvoiced. The PRAAT internal settings for F0 values determination were experimentally 
chosen by visual comparison of testing sentences (one typical sentence from each of 
emotions and voice classes) as follows: cross-correlation analysis method [14], pitch-
range 35÷250 Hz for male and 105÷350 Hz for female voices. 
Speech signal analysis was performed for total number of 25988 frames (8 male 
speakers) and 24017 frames (8 female speakers). The spectral flatness values were 
determined only from the voiced frames (totally 11639 of male and 13464 of female 
voice) – see statistical results in Tab. 1 (male), Tab. 2 (female). The main result – mean 
spectral flatness values ratios between different emotional states and a neutral state – is 
given in Tab. 3. Summary histograms of SF values for different emotions in dependence 
on the speaker gender are shown in Fig. 7 (male) and Fig. 8 (female). For comparison, 
the histograms of unvoiced frames (male voice) are shown in Fig. 9. Tab. 4 (male) and 
Tab. 5 (female) contain parameters α, λ of the Gamma distribution for histogram fitting 
and modelling together with the resulting RMS approximation errors. 

112 
J. Přibil and A. Přibilová 
Table 1. Summary results of statistical analysis of the spectral flatness values: male voice, 
voiced frames 
Emotion 
frames 
mean 
min 
max 
std 
Neutral 
3300 
0.00286 
3.78⋅10-5 
0.03215 
0.00364 
Joy 
2183 
0.00662 
1.36⋅10-4 
0.04327 
0.00650 
Sadness 
3503 
0.00444 
1.12⋅10-4 
0.05540 
0.00462 
Anger 
2707 
0.00758 
2.28⋅10-4 
0.04228 
0.00614 
Table 2. Summary results of statistical analysis of the spectral flatness values: female voice, 
voiced frames 
Emotion 
frames 
mean 
min 
max 
std 
Neutral 
3056 
0.00274 
3.15⋅10-5 
0.03731 
0.00346 
Joy 
3473 
0.00784 
2.07⋅10-4 
0.05414 
0.00726 
Sadness 
3690 
0.00506 
9.48⋅10-5 
0.06694 
0.00674 
Anger 
3245 
0.00807 
1.41⋅10-4 
0.05129 
0.00692 
 
Table 3. Mean spectral flatness values ratios between different emotional states and a neutral 
state (for voiced frames only) 
mean SF ratio 
joy: neutral 
sadness: neutral 
anger: neutral 
Male voice 
2.31 
1.55 
2.65 
Female voice 
2.86 
1.85 
2.94 
Female to Male ratio 
1.24 
1.19 
1.11 
 
Table 4. Evaluated parameters α, λ of Gamma distribution for histogram fitting and modelling 
together with resulting RMS approximation error: male voice, voiced frames 
Emotion 
α *) 
λ *) 
RMS 
Neutral 
2.05 
0.48 
0.70 
Joy 
4.15 
0.50 
0.67 
Sadness 
2.55 
0.54 
1.35 
Anger 
5.40 
0.56 
0.84 
                                  *) Values for minimum RMS error 
Table 5. Evaluated parameters α, λ of Gamma distribution for histogram fitting and modelling 
together with resulting RMS error: female voice, voiced frames 
Emotion 
α *) 
λ *) 
RMS 
Neutral 
1.95 
0.51 
1.48 
Joy 
4.85 
0.51 
0.54 
Sadness 
2.35 
0.54 
0.75 
Anger 
6.15 
0.51 
0.67 
                                 *) Values for minimum RMS error 

 
Spectral Flatness Analysis for Emotional Speech Synthesis and Transformation 
113 
0
0.01
0.02
0.03
0.04
0.05
0.06
0
—› Relative occurence [%
—› SF values [-]
"Neutral" (3300 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
10
20
30
—› Relative occurence [%]
—› SF values [-]
"Sad" (3503 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
—› Relative occurence [%]
—› SF values [-]
"Joy" (2183 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
10
20
30
—› Relative occurence [%]
—› SF values [-]
"Anger" (2707 frames)
Gamma Distribution
]
30
20
10
a)
b)
30
20
10
c)
d)
 
Fig. 7. Histograms of spectral flatness values together with fitted and modelled curves of 
Gamma distribution - determined from the speech signal with emotions: “neutral” (a), 
“sadness” (b), “joy” (c), and “anger” (d) - male voice, voiced frames 
0
0.01
0.02
0.03
0.04
0.05
0.06
0
—› Relative occurence [%]
—› SF values [-]
"Neutral" (3056 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
10
20
30
—› Relative occurence [%]
—› SF values [-]
"Sad" (3690 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
—› Relative occurence [%]
—› SF values [-]
"Joy" (3473 frames)
Gamma Distribution
0
0.01
0.02
0.03
0.04
0.05
0.06
0
10
20
30
—› Relative occurence [%]
—› SF values [-]
"Anger" (3245 frames)
Gamma Distribution
30
20
10
a)
b)
30
20
10
c)
d)
 
Fig. 8. Histograms of spectral flatness values together with fitted and modelled curves of 
Gamma distribution - determined from the speech signal with emotions: “neutral” (a), 
“sadness” (b), “joy” (c), and “anger” (d) - female voice, voiced frames 

114 
J. Přibil and A. Přibilová 
0
0.1
0.2
0.3
0.4
0.5
0.6
0
30
—› Relative occurenc
—› SF values [-]
(3921 unvoiced frames)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
10
20
30
—› Relative occurence [%]
—› SF values [-]
(2381 unvoiced frames)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
—› Relative occurence [%]
 SF values [-
(4459 unvoiced frames)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
10
20
30
—› Relative occurence [%]
 SF values [-]
(3588 unvoiced frames)
e [%]
20
10
a)
b)
30
20
10
—›
]
—›
c) 
d) 
 
Fig. 9. Histograms of spectral flatness values calculated from the unvoiced frames (male 
voice): “neutral” style (a), and emotions - “joy” (b), “sadness” (c), and “anger” (d) 
4   Conclusion 
The statistical analysis of spectral flatness values was performed. Obtained statistical 
results of the spectral flatness ranges and values show good correlation for both types 
of voices and all three emotions. The greatest mean SF value is observed in “Anger” 
style for both voices – compare Tab. 1 and Tab. 2. From Tab. 3 follows that the ratio of 
mean values is 1.18 times higher for female voice than for male voice. Similar shape of 
SF histograms can be seen in Fig. 7 and Fig. 8 comparing corresponding emotions for 
male and female voices. This subjective result is confirmed by the objective method – 
histogram modelling with the help of the Gamma distribution. Given vales of α and λ 
parameters – showed in Tab. 4 and Tab. 5 – are also in correlation with previously 
obtained results. On the other hand, it was confirmed that only SF values calculated 
from voiced frames of speech give sufficient information – in Fig. 9 it is evident that 
the histograms are practically the same for all three emotions. 
Our final aim was to obtain the ratio of mean values, which can be used to control 
the high frequency noise component in the mixed excitation during cepstral speech 
synthesis of voiced frames. This parameter can be applied directly to the text-to-
speech system enabling expressive speech production [15], or it can be used in 
emotional speech transformation (conversion) method based on cepstral speech 
description for modification of degree of voicing in voiced frames [4], [16]. 
Out next aim will be to find out how to use obtained statistical parameters of 
spectral flatness for evaluation of different emotional states in speech. Further these 
results can be used for determination of voicing transition frequency (for speech 
synthesis based on the harmonic speech model) [4].  

 
Spectral Flatness Analysis for Emotional Speech Synthesis and Transformation 
115 
Acknowledgments. The work has been done in the framework of the COST 2102 Action. 
It has also been supported by the Ministry of Education, Youth, and Sports of the Czech 
Republic (OC08010), the Grant Agency of the Czech Republic (GA102/09/0989), and the 
Ministry of Education of the Slovak Republic (COST2102/STU/08). 
References 
1. Gray Jr., A.H., Markel, J.D.: A Spectral-Flatness Measure for Studying the Autocorrelation 
Method of Linear Prediction of Speech Analysis. IEEE Transactions on Acoustics, Speech, 
and Signal Processing ASSP-22, 207–217 (1974) 
2. Esposito, A., Stejskal, V., Smékal, Z., Bourbakis, N.: The Significance of Empty Speech 
Pauses: Cognitive and Algorithmic Issues. In: Proceedings of the 2nd International 
Symposium on Brain Vision and Artificial Intelligence, Naples, pp. 542–554 (2007) 
3. Ito, T., Takeda, K., Itakura, F.: Analysis and Recognition of Whispered Speech. Speech 
Communication 45, 139–152 (2005) 
4. Přibil, J., Přibilová, A.: Voicing Transition Frequency Determination for Harmonic Speech 
Model. In: Proceedings of the 13th International Conference on Systems, Signals and 
Image Processing, Budapest, pp. 25–28 (2006) 
5. Přibil, J., Madlová, A.: Two Synthesis Methods Based on Cepstral Parameterization. 
Radioengineering 11(2), 35–39 (2002) 
6. Scherer, K.R.: Vocal Communication of Emotion: A Review of Research Paradigms. 
Speech Communication 40, 227–256 (2003) 
7. Vích, R.: Cepstral Speech Model, Padé Approximation, Excitation, and Gain Matching in 
Cepstral Speech Synthesis. In: Proceedings of the 15th Biennial International EURASIP 
Conference Biosignal, Brno, pp. 77–82 (2000) 
8. Paeschke, A.: Global Trend of Fundamental Frequency in Emotional Speech. In: 
Proceedings of Speech Prosody, Nara, Japan, pp. 671–674 (2004) 
9. Bulut, M., Lee, S., Narayanan, S.: A Statistical Approach for Modeling Prosody Features 
Using POS Tags for Emotional Speech Synthesis. In: Proceedings of IEEE International 
Conference on Acoustics, Speech, and Signal Processing, Honolulu, Hawai, pp. 1237–
1240 (2007) 
10. Markel, J.D., Gray Jr., A.H.: Linear Prediction of Speech. Springer, Heidelberg (1976) 
11. Suhov, Y., Kelbert, M.: Probability and Statistics by Example. Basic Probability and 
Statistics, vol. I. Cambridge University Press, Cambridge (2005) 
12. Everitt, B.S.: The Cambridge Dictionary of Statistics, 3rd edn. Cambridge University 
Press, Cambridge (2006) 
13. Boersma, P., Weenink, D.: Praat: Doing Phonetics by Computer (Version 5.0.32) 
[Computer Program], http://www.praat.org/ (retrieved August 12, 2008)  
14. Boersma, P., Weenink, D.: Praat - Tutorial, Intro 4. Pitch analysis (September 5, 2007), 
http://www.fon.hum.uva.nl/praat/manual/Intro_4__Pitch_ 
analysis.html 
15. Přibil, J., Přibilová, A.: Application of Expressive Speech in TTS System with Cepstral 
Description. In: Esposito, A., Bourbakis, N.G., Avouris, N., Hatzilygeroudis, I. (eds.) HH 
and HM Interaction. LNCS (LNAI), vol. 5042, pp. 200–212. Springer, Heidelberg (2008) 
16. Přibilová, A., Přibil, J.: Spectrum Modification for Emotional Speech Synthesis. In: 
Esposito, A., et al. (eds.) Multimodal Signals: Cognitive and Algorithmic Issues. LNCS 
(LNAI), vol. 5398, pp. 232–241. Springer, Heidelberg (2009) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 116–125, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Voice Pleasantness of Female Voices and the Assessment 
of Physical Characteristics 
Vivien Zuta 
FIAS, Frankfurt Institute for Advanced Studies, Frankfurt, Germany 
and 
Institute of Phonetics, Goethe University, Frankfurt, Germany 
zuta@fias.uni-frankfurt.de 
Abstract. It has been demonstrated that there seem to be non-linguistic patterns 
from which listeners refer to the appearance of a speaker. This study completes 
the collected voice parameters, which count so far as indicators of physical 
attributes and as factors for voice attractiveness. Since scientists tend to prefer 
male voices for such analyses this one is based on female voices and both-
gendered judges. 20 female voices were played to 102 listeners, 52 male and 50 
female. Because of different rating strategies of female and male listeners, the 
group specific rating strategies were compared; a bimodal rating dispersion was 
assumed and mostly proved. 
Keywords: voice attractiveness, voice production, voice perception, vocal tract. 
1   Introduction 
Most people have had the experience of surprise, by the appearance of a person they 
had hitherto known only from his or her voice. However, the surprised reaction 
indicates that there must be exact cues in speech that lead the listener to create a 
certain picture of the speaker. In previous studies [1] the author examined phonetic 
criteria of male voices and found out, that female listeners usually assign attractive 
body characteristics to speakers with attractive sounding voices. The rating of 
whether a voice was judged as attractive or not was carried out independently of F0, 
which was a big surprise, since 100% of the listeners declared to prefer deep sounding 
male voices. The cliché that attractive male voices need to have low F0 values was 
not proven. The reactions were splendorous and led to big media interest. (In a 
broader, more popular sense, see also [2]). In order to find out more interesting details 
about listeners judgements in terms of voices a more extensive study was carried out, 
which is still in progress and includes female voices as well. Some of the results are 
presented in the following article. 
A closer look at the publications on this topic show how discordant the results are 
and how complex the research question is designed. 
Several studies, mostly undertaken outside of the field of phonetics, deal with the 
topic of how feasible it is to infer the speakers’ appearance from their voice. Some of 
these studies concentrate on the correlation of assumed physical attributes and their 

 Voice Pleasantness of Female Voices and the Assessment of Physical Characteristics 
117 
equivalents in vocal attractiveness. Collins & Missing [3] found that vocal 
attractiveness and the attractiveness of female faces are related and that female voices 
with higher fundamental frequency were judged as more attractive and as likely to be 
younger. Feinberg et al. [4] report a positive correlation between facial-metric 
femininity and vocal attraction, the latter being connected with higher pitched voices.  
Actual physical attributes and rated voice attractiveness were examined for example 
by Hughes et al. [5] who found a surprising correlation between Waist-to-Hip-Ratio 
(WHR) for women and Shoulder-to-Hip-Ratio (SHR) for men and the opposite-sex 
voice ratings. Voices of females with low WHRs and males with big SHRs were 
found to be consistently rated as more attractive. In 2002 Hughes et al. [6] found a 
correlation between the measurable bilateral symmetry of a speaker and his or her 
rated voice attractiveness. Additionally Evans et al. [7] discovered negative 
correlations between F0 for male voices and male body measures such as SHR. 
Furthermore they found that men with large body size and shape tend to have smaller 
formant dispersion than shorter men do. However they refer to the findings of Fitch 
and Giedd [8] whereby the spreading of overweight in western society weakens any 
statistical findings. 
Other examinations that are more closely related to the field of phonetic science, 
deal with the question of whether physical body characteristics are related to 
measurable physical parameters in voice at all. So did Künzel [9] examine the 
correlation between the average fundamental frequency and speaker height and 
weight. He assumed contrarily to other examinations [10], that there is no acoustical 
parameter that could be counted as an indicator for height and weight estimations. 
Furthermore he postulates that speech signals do not contain any information about 
speaker’s height or weight at all. Van Dommelen & Moxness [11] concluded that 
listeners correctly used speech rate for speaker weight estimations whereas low F0 
was wrongly taken for large body estimations. 
Lass & Davis [10] found that the accuracy of estimating the height of male and 
female subjects (in agreement with van Dommelen & Moxness [11]) and male weight 
on the basis of speech signals was higher than would be attributed to chance. Lass & 
Davis, however, were not able to determine the exact cues in the speech signals from 
which these conclusions were derived. Another investigation was performed by 
Bruckert et al. [12] who found that females were consistent in their judgment of 
pleasantness and in their height, weight and age estimations. Bruckert et al. concluded 
that pleasantness judgments were based mainly on intonation. Women tend to prefer 
voices with raising pitch. Another result was that female listeners were able to 
correctly estimate age and weight but not height. For the age estimation women 
would use lower-frequency formants with little formant dispersion as a sign for an 
older age. Bruckert et al. however were not able to determine which acoustic 
parameters were used to estimate the subjects weight correctly. 
It is very interesting to see that some experiments do find correlations between body 
size and voice pitch and some do not. Mostly these differences are attributed to the 
varying experimental set ups and measurement methods. The hypothesis that a positive 
correlation between body size and F0 exists is due to the assumption that the lengths of 
the vocal folds increases with body size [13]. This however is not the case within the 
group of mammal species [14]. Another kind of experiment handles the question of 
whether the suspected body characteristics are consistent and/or close to the actual 

118 
V. Zuta 
body characteristics of the speaker. Krauss et al. [15] examined the possibility to refer 
from ones voice to his/her photo (one out of two). The accuracy was better than chance 
and naïve listeners were able to estimate a speaker’s age, height, and weight from 
voice samples “nearly as well as they can from a photograph”. 
This topic is highly interesting since the correlation between body characteristics 
and measured values is usually quite low. It seems evident that there must be some 
cues in the speech signal to be identified, because, as it will be shown, correlations 
between actual and estimated characteristics were found. 
2   Experiment 
2.1   Material 
The recordings for the experiment were conducted using a set of 40 native German 
speakers (20 female, 20 male) who were requested to narrate the fairy tale “Little Red 
Riding Hood”. The fairy tale was spoken in standard-German at speaker specific 
speed, articulation and intonation. No guidelines were given. (Little Red Riding Hood 
was chosen because of its widespread familiarity.) The task for the listeners was to 
judge in reference to the sound they were listening to but not in reference to the 
content of what the speakers were saying. It was ensured that everybody who took 
part in the investigation was familiar with the fairy tale. Speakers were told to narrate 
the story in a natural way and they were recorded in a sound treated environment with 
professional equipment (see 2.6.1. Recordings). 
2.2   Subjects 
None of the subjects had a strong dialect, a speech disorder or was affected by an illness 
that could have had an effect on the voice quality. All of them were healthy and were 
recruited by the author and her co-worker. The participants were asked about their 
height, weight, age, hair and eye colour, education, smoking habits and if they ever had 
their nose broken. (Female subjects were also asked about hormonal contraceptives.) 
Since only female voices are relevant for this article, the male speakers descriptive 
statistics is left aside. 
Table 1. Descriptive statistics for subjects 
female subjects 
(N=20) 
mean 
range 
  
  
min 
max 
age (years) 
25 (5,29) 
21 
44 
height (cm) 
168 (6,30) 
152 
178 
weight (kg) 
62,4 (11,7) 
50 
100 
 
The Voice Listeners were 107 adults. They were recruited through a couple of 
advertisements, which were placed all around the university, different institutes and 
via the Multimedia-System of the Goethe University Frankfurt. We also contacted 

 Voice Pleasantness of Female Voices and the Assessment of Physical Characteristics 
119 
different schools and companies directly with covering letters. They were allured with 
a little lottery, cookies, cakes and coffee. 
5 listeners did not continue until the end of the evaluation and were not included in 
the analysis. 8 listeners were younger than 18years old. The statistic analysis was 
carried out twice, including and excluding those 8 listeners. Since the results did not 
differ significantly the age did not matter for this analysis. Leaving 52 male and 50 
female listeners. 
Table 2. Descriptive statistics for listeners 
  
male listeners (N=52) 
female listeners (N=50) 
  
mean 
range 
mean 
range 
  
  
min 
max 
  
min 
max 
age 
(years) 
24,76 
(11,75) 
16 
63 
24,52 
(8,22) 
17 
56 
height 
(cm) 
181,89 
(7,19) 
165 
195 
168,14 
(6,45) 
170 
185 
weight 
(kg) 
78,19 
(11,33) 
58 
110 
62,67 
(10,49) 
45 
90 
2.3   Stimuli Selection 
Originally, it was planned to present the whole 40 stimuli to all listeners, but soon it 
became clear that this would take way too much time for each listener. So it was 
decided to present only 10 male and 10 female speakers to each listener and it already 
took them about 60 min. to go through the twenty voices. In order to get an equally 
distributed evaluation a script was computed which randomised the stimuli so that an 
equally distributed evaluation was assured. This randomisation guaranteed also that a 
fatiguing effect could be disregarded.  
2.4   Procedure 
The participants were all tested in the same manner. Each participant was seated in 
front of one computer and was told to follow the instructions that were written on the 
first page of the questionnaire. The participants were lead through the experiment on-
screen and had to answer 20 questionnaires about 20 different voices. (For this article 
we refer to the female voices N=20 and all participants N=102 only.) The 
experimenter was seated in the same room to take care that participants would not 
communicate with each other. Since the experiment was undertaken in the rooms of 
the Institute of Phonetics and in different offices in the Frankfurt area, the experiment 
was run on Macintosh and Window PCs, depending on the availability of each 
company. It was made sure that the experimental premises were as analogue as 
possible. During the time of the experiment, there were no other activities, such as 
usual office business or university business, which could have sidetracked the 
participants. No time limit was given. 

120 
V. Zuta 
2.5   Preparing the Data 
Recordings. Recording the subjects in a sound treated environment using the 
Microtrack 24/96 Professional 2-Channel Mobile Digital Recorder (M-Audio) created 
the stimuli. The voices were recorded 44,1 kHz, 16 bit mono. 
Firstly the data was adjusted. In order to avoid extreme effects, such as too loud or 
too silent speech, all audio files were rescaled by setting the peak scale to 0,99 and the 
intensity to 70 dB. Afterwards the audio signals were cleared of laughs, coughs and 
abruptions since these particles are absolutely irrelevant for what was sought after. 
Given that we tend to investigate speech and voices these behavioural particles were 
not given further attention.  
Using PRAAT software1 all signals were segmented. Each signal obtained 
segmentation into two tiers. The first tier for F0, in which five segments2 of each 5 
seconds was measured in order to calculate average F0. Since a fairy tale was told it 
was not possible to measure F0 simply through the whole signal, because of some 
unwanted variations due to direct speech within the fairy tale. In order to measure F0 
equally for all audio signals a script was used, to calculate F0 for all segments in a 
signal. The average of these results was taken respectively as F0 for the subjects. As 
reference served the PRAAT Voice Report function. 
To be able to compare formant frequencies a second tier was created in which [a:], 
[e:], [i:], [o:] and [u:] were segmented. Each vowel was segmented five times, if this 
was not possible, it was marked in the statistics. A script ran through the tiers and 
calculated the mean formant values in the segmented parts of the signal. The average 
values were calculated and analysed. 
3   Results 
For the voices ANOVAs and/or Nominal Logistics were carried out. According to 
pre-experiments, male and female listeners are judging so differently that we are 
dealing with a bimodal distribution.  
3.1   Pleasantness of Female Voices  
For both listener-groups a nominal logistic3 using Chi-Square analysis and REML 
(Restricted Maximum Likelihood) method with pleasantness of voice as dependent 
variable and actual age, height, weight, education, smoking habits, broken nose, F0, 
estimated age, estimated weight and estimated height as independent variables and 
randomised subjects was carried out.  
For female listeners interactions were found between pleasantness and: actual age 
χ2=16.24, p=0.0001; actual height χ2=12.41, p=0.0004; education χ2=22.49, 
p=0.0004; broken nose χ2=8.09, p=0.0045; smoking habits χ2=8.12, p=0.0044 and 
estimated height χ2=6.12, p=0.0469. No interactions were found for weight χ2=2.59 
                                                           
1 www.praat.org 
2 Two segments at the beginning, one segment in the middle and two segments at the end of each 
signal. 
3 All Chi-Square-Tests in this article are Effect Wald Tests. 

 Voice Pleasantness of Female Voices and the Assessment of Physical Characteristics 
121 
p=0.1071, F0 χ2=1.91, p=0.1665; estimated weight χ2=4.70, p=0.4530 and estimated 
age χ2=0.49, p=0.9219. 
For male listeners interactions were found between pleasantness and: actual age 
χ2=11.14, p=0.008; smoking habits χ2=6.18, p=0.0129 and education χ2=17.58, 
p=0.0035. All estimated factors were significant as well: estimated age χ2=16.28, 
p=0.0003, estimated height χ2=6.22, p=0.0446, estimated weight χ2=17.43, p=0.0038. 
No interactions were found for actual height χ2=1.89, p=0.1696; actual weight 
χ2=1.03, p=0.3095, broken nose χ2=1.54, p=0.2144  and F0 χ2=1.01, p=0.3140. 
Table 3. Dependencies for characteristics of female voices and listeners of both gender 
  
female listeners (N=50)
male listeners (N=52) 
actual age 
0,0001 
0.008 
actual height 
0,0004 
n.s. 
actual weight 
n.s. 
n.s. 
education 
0,0004 
0,0035 
smoking habits 
0,0044 
0,0129 
broken nose 
0,0045 
n.s. 
F0 
n.s. 
n.s. 
estimated age 
n.s. 
0,0003 
estimated height 
0,0469 
0,0446 
estimated weight 
n.s. 
0,0038 
3.2   Listeners’ Judgments and Speakers’ Actual Characteristics 
For both listener-groups four separate nominal logistics using Chi-Square analysis 
and REML method with estimated age, estimated height, estimated weight and 
estimated F0 (high, low, average) as dependent variable and actual age, height, 
weight, education, smoking habits, broken nose and F0 as independent variables were 
carried out.  
For female listeners interactions were found between estimated age and: actual age 
χ2=14.62, p=0.0022; weight χ2=11.05, p=0.0115 and smoking habits χ2=14.90, 
p=0.0019. All other factors and interactions were not significant. Between estimated 
height, correlations were found for: actual height χ2=10.0, p=0.0069; weight 
χ2=10.05, p=0.0066; smoking habits χ2=10.42, p=0.0055; education χ2=23.2, 
p=0.0100; broken nose χ2=7.27, p=0.0264; F0 χ2=9.29, p=0.0096. Actual age did not 
show significant correlations with height estimations (χ2=6.88, p=0.3324). Weight 
estimation was predicted only by F0 χ2=17.4353, p=0.0037 and Age χ2=11.41, 
p=0.0438. For estimated F0 as dependent variable, interactions were found for Age 
χ2=8.08, p=0.0176, education χ2=24.83, p=0.0057 and F0 χ2=18.40, p=0.0001.  
For male listeners interactions were found only between estimated age and: 
smoking habits χ2=20.52, p<0.0001 and F0 χ2=27.46, p<0.0001. No other significant 
interactions for this dependent factor were found. Interactions with estimated height 
were found for following independent factors: height χ2=9.26, p=0.0097, weight 
χ2=15.51, p=0.0004, education χ2=35, p=0.0001, broken nose χ2=12,36, p=0.0021, F0 

122 
V. Zuta 
χ2=11.15, p=0.0038. All other factors and interactions were not significant. Weight 
estimation was made using actual age χ2=15.12, p=0.0084, and height χ2=21.69, 
p=0.0168 only. For estimated F0 as dependent variable, Age χ2=20.30, p<0.0001, 
education χ2=28.75, p<0.0001 and smoking habits χ2=12.61, p=0.0004, showed 
significant relations. All other factors were insignificant. 
Table 4. Interactions between actual and estimated characteristics for female voices and female 
listeners; independent variables vertically aligned 
  
estimated age
estimated 
height 
estimated 
weight 
estimated F0 
actual age 
0,0022 
n.s. 
0,0438 
0,0176 
actual height 
0,0115 
0,0069 
n.s. 
n.s. 
actual weight
n.s. 
0,0066 
n.s. 
n.s. 
education 
n.s. 
0,01 
n.s. 
0,0057 
smoking 
habits 
0,0019 
0,0055 
n.s. 
n.s. 
broken nose 
n.s. 
0,0264 
n.s. 
n.s. 
F0 
n.s. 
0,0096 
0,0037 
0,0001 
Table 5. Interactions between actual and estimated characteristics for female voices and male 
listeners; independent variables vertically aligned 
 
estimated 
age 
estimated 
height 
estimated 
weight 
estimated F0 
actual age 
n.s. 
n.s. 
0,0084 
<0,0001 
actual height 
n.s. 
0,0097 
0,0168 
n.s. 
actual weight 
n.s. 
0,0004 
n.s. 
n.s. 
education 
n.s. 
0,0001 
n.s. 
<0,0001 
smoking habits 
<0,0001 
n.s. 
n.s. 
0,0004 
broken nose 
n.s. 
0,0021 
n.s. 
n.s. 
F0 
<0,0001 
0,0038 
n.s. 
n.s. 
4   Discussion 
Inconsistent to previous studies of preferences for women’s voices the results show, 
that the evaluation of pleasantness of female voices occurs independently of voice 
pitch for both, male and female listeners. The results furthermore indicate that for male 
voice listeners F0 plays even a much smaller role than expected. Male listeners are not 
able to correctly assign a F0 description (low, average, high) to a woman’s voice. F0-
judgements of male listeners do not show any significant correlation with actual F0 
χ2=3.3688, p=0.1856 F0. Male listeners use F0 for height estimations and age 
estimations only and affiliate a lower F0 with higher age. Several studies ([3], [16]) 

 Voice Pleasantness of Female Voices and the Assessment of Physical Characteristics 
123 
have shown, that high-pitched female voices are indicators for female fertility and with 
that to be preferred. But as it is shown in the table below, men prefer average 
judgments instead of making a decision when the frequencies fall below 205Hz. Since 
men tend to have difficulties in correctly estimating F0 in general, they seem only to 
make differences between high and not high pitch. The pleasantness judgments show 
that men prefer younger estimated voices to older sounding ones, but that does not go 
along with F0. Actual F0 is no indicator for pleasantness (χ2=1.01, p=0.3140). 
Table 6. Pleasantness, age and F0 estimations compared with actual values for male listeners 
average 
estimated 
age 
average 
actual age 
estimated F0 
actual 
F0 
pleasantness 
42% high 
54% average 
20 
24 
4% deep 
205 Hz 
68% 
29% high 
66% average 
20-30 
25 
5% deep 
200 Hz 
68% 
27% high 
64% average 
30-40 
28 
7% deep 
187 Hz 
34% 
 
It is imaginable, that men subconsciously prefer voices above 200 Hz, (which is 
rather a normal than a significant high pitched female fundamental frequency) and 
affiliate youth with these voices. But fact is, that men do not necessarily prefer high-
pitched voices, more importantly they put their focus on estimated age and favor 
young sounding voices. Another interpretation is that men think that young sounding 
voices have to have a certain high frequency. Possibly they are influenced by a cliché 
that leads to the judgments listed above. (Voice sounds young, must be high.) Male 
listeners cannot reliably differentiate between high/average and deep female voices. 
The physical parameter that is used by the male listeners of this experiment in order to 
judge female voices’ age needs to be found out in further analysis of this data. 
Female listeners however are able to correctly assign F0 estimates to the actual F0 
value with χ2=18.4987, p=0.0001, but in agreement to male listeners do not use F0 for 
pleasantness judgments. 
Female listeners used F0 for weight estimations but no correlation between 
estimated and actual weight was detectable. A deeper F0 leads female listeners to 
higher weight estimations. Creating two groups, the lighter judged group (50-60kg) 
had an  average weight of 62kg and a mean F0 of 201 Hz. The heavier judged group 
of female speakers (60-75kg) had an actual average weight of 62kg and a mean F0 of 
197 Hz. Since the actual average weights are exactly the same for both groups, weight 
estimations are correspondingly caused by the lower F0 value of one group. 
Both, male and female participants equally referred from F0 to estimated height. It 
was shown in the statistical analysis that correlations between estimated and actual  
 

124 
V. Zuta 
Table 7. Pleasantness, age and F0 estimations compared with actual values for female listeners 
average 
estimated 
age 
average 
actual age 
estimated F0 
actual F0 
pleasantness 
30% high 
64% average 
20 
24 
6% deep 
207 Hz 
49% 
28% high 
65% average 
20-30 
25 
7% deep 
200 Hz 
60% 
16% high 
73% average 
30-40 
28 
11% deep 
188 Hz 
57% 
 
Table 8. Height estimations and corresponding F0 values for both listener groups 
  
estimated height 
actual F0 
N 
tall 
194 Hz 
10% 
average 
198 Hz 
62% 
male listeners 
short 
202 Hz 
28% 
tall 
193 Hz 
15% 
average 
198 Hz 
60% 
female listeners 
short 
203 Hz 
25% 
 
height were found. Interestingly, the judgments for this parameter were the only ones, 
which are consistent between both gender-groups. 
Looking at the average F0 values it is obvious that the estimated height groups are 
extremely mixed. This is the only explanation for such low average F0 values for 
female speaker groups. 
It can be summarized, that not only the pleasantness of ones voices but also the 
assessment of physical characteristics by ones voice is depending on many different 
factors. The complexity of a voice impression has to be seen in total and cannot be 
examined in disregard of interacting single factors. 
As mentioned in the beginning, the work is still in progress. For instance the 
statistical analyses for Acoustic Parameters and Speakers’ Characteristics are 
currently carried out and of course other examinations about the physical parameters 
that lead listeners to certain judgments about the owner of a voice will be undertaken. 
Since the corpus contains both (male and female) voices many more interesting 
results are to be expected. 

 Voice Pleasantness of Female Voices and the Assessment of Physical Characteristics 
125 
References 
1. Zuta, V.: Phonetic Criteria of Attractive Male Voices. In: Proc. 16th ICPhS Saarbrücken, 
pp. 1837–1840 (2007) 
2. Zuta, V.: Warum tiefe Männerstimmen doch nicht sexy sind – Das Geheimnis unserer 
Stimme. Scherz Verlag, Frankfurt (2008) 
3. Collins, S.A., Missing, C.: Vocal and visual attractiveness are related in women. Animal 
Behaviour 65, 997–1004 (2003) 
4. Feinberg, D.R., Jones, B.C., DeBruine, L.M., Moore, F.R., Law Smith, M.J., Cornwell, E., 
Tiddeman, B.P., Boothroyd, L.G., Perrett, D.I.: The voice and face of woman: One 
ornament that signals quality? Evolution and Human Behavior 26, 398–408 (2005) 
5. Hughes, S.M., Harrison, M.A., Gallup Jr., G.G.: The sound of symmetry: voice as a 
marker of developmental instability. Evolution and Human Behavior 23, 173–180 (2002) 
6. Hughes, S.M., Dispenza, F., Gallup Jr., G.G.: Ratings of voice attractiveness predict sexual 
behavior and body configuration. Evolution and Human Behavior 25, 295–304 (2004) 
7. Evans, S., Neave, N., Wakelin, D.: Relationships between vocal characteristics and body 
size and shape in human males: An evolutionary explanation for a deep male voice. 
Biological Psychology 72, 160–163 (2006) 
8. Fitch, W.T., Giedd, J.: Morphology and development of the human vocal tract: a study 
using magnetic resonance imaging. Journal of the Acoustical Society of America 106, 
1511–1522 (1999) 
9. Künzel, H.J.: How Well Does Average Fundamental Frequency Correlate with Speaker 
Height and Weight? Phonetica 46, 117–125 (1989) 
10. Lass, N.J., Davis, M.: An investigation of speaker height and weight identification. Journal 
of the Acoustical Society of America 60(3), 700–703 (1976) 
11. van Dommelen, W.A., Moxness, B.H.: Acoustic parameters in speaker height and weight 
identification: sex- specific behaviour. Language and Speech 38, 267–287 (1995) 
12. Bruckert, L., Lienard, J.S., Lacroix, A., Kreutzer, M., Lebourcher, G.: Women use voice 
parameters to assess men’s characteristics. In: Proceedings of Biological sciences, 
vol. 273(1582), pp. 83–293. The Royal Society of London (2006) 
13. Titze, I.R.: Principles of voice production. Journal of the Acoustical Society of 
America 104(3), 1148 (1994) 
14. McComb, K.E.: Female choice for high roaring rate in red deer (Cervus elaphus). Animal 
Behaviour 41, 79–88 (1991) 
15. Krauss, R.M., Freyberg, R., Morsella, E.: Inferring speakers’ physical attributes from their 
voices. Journal of Experimental Social Psychology 38, 618–625 (2002) 
16. Jones, B.C., et al.: Integrating cues of social interest and voice pitch in men’s preferences 
for women’s voices. Biology Letters 4, 192–194 (2008) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 126–132, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Technical and Phonetic Aspects of Speech Quality 
Assessment: The Case of Prosody Synthesis 
Jana Tučková1, Jan Holub2, and Tomáš Duběda3 
1 Czech Technical University, Faculty of Electrical Engineering, Dept. of Circuit Theory, 
Technická 2, 166 27 Prague 6, Czech Republic 
tuckova@fel.cvut.cz 
2 Czech Technical University, Faculty of Electrical Engineering, Dept. of Measurement, 
Technická 2, 166 27 Prague 6, Czech Republic 
holubjan@fel.cvut.cz 
3 Charles University in Prague, Faculty of Arts and Philosophy,  
Institute of Translation Studies, Hybernská 3, 110 00 Praha 1 
dubeda@ff.cuni.cz 
Abstract. The present paper proposes a discussion of methods used for 
subjective assessment of speech quality in technical sciences and in linguistics. 
Stressing the fact that purely mathematical evaluation of synthetic speech is not 
sufficient, we try to show that the perspectives and approaches used in the two 
scientific domains are not necessarily the same. Next we proceed to a pilot 
experiment consisting in the assessment of synthetic sentences as generated by 
five different prosodic models, by means of the MOS formalism (ITU-T P.800). 
The two groups of listeners involved (students of engineering vs. linguistics) 
provide rather similar results. 
Keywords: Speech synthesis, speech quality assessment, listening tests, neural 
networks, data mining, pruning. 
1   Introduction 
Speech as a means of human communication should be not only intelligible, but also 
comfortable to listen to. Several factors influence speech quality. Objective factors 
include measurable parameters of technical devices and systems, e.g. recorders, 
transmission lines, end terminals and speech synthesizers. However, speech is 
primarily a human attribute and should be thus assessed subjectively as well. In the 
present paper, we first discuss methods of auditory evaluation of synthetic speech, 
with special focus on perspectives, approaches and methods used in technical sciences 
and humanities, respectively. Next we describe a brief listening assessment of 
different prosodic models, carried out with two types of listeners. 
Prosody is one of the crucial components of synthetic speech generation. In our 
approach, it is controlled by an artificial neural network (ANN), whose topology may 
be optimized in various ways. Listening tests are inevitable in testing different 
pruning methods, which often return similar results. 

 
Technical and Phonetic Aspects of Speech Quality Assessment 
127 
2   Technical Approach 
The perceived quality of (transmitted) speech can be measured with subjective tests 
according to ITU-T P.800 [1]. Humans evaluate the quality according to a 
standardized quality assessment process, in terms of mean opinion score (MOS) 
values, which range from 1 (bad) to 5 (excellent). There are several methods to assess 
the subjective quality of speech signals [2]. Generally speaking, they fall into two 
main classes: (a) conversational tests and (b) listening-only tests. Conversational tests, 
in which two subjects have to talk and listen interactively via the transmission system 
under test, provide a more realistic test environment. However, they are rather time 
consuming, and often suffer from low reproducibility; as a consequence, listening-
only tests are often recommended as more convenient. These are mostly based on 
speech samples which have a length ranging from 6 to 12 s. 
It should be noted here that, due to their dependence on human voting, one-to-one 
comparison between subjective MOS scores from different subjective tests is difficult 
with tests conducted according to ACR LQ. This is because subjective votes are 
affected by factors such as cultural variation, individual and personal experience 
variation and testing conditions. Hence, it is unreasonable to expect results from 
different subjective tests to be strictly identical. However, if the tests are well 
designed and consistent, the relationship between them should be monotonic. A 
monotonic mapping function can therefore be applied between two tests to make sure 
that their scales fit to each other. 
Objective voice quality metrics replace the human panel by a computational model or 
an algorithm that computes MOS values by observing a sample of the speech in 
question. The aim of objective measures is to predict MOS values that are as close as 
possible to the ratings obtained from subjective tests for various adverse speech 
conditions. The accuracy, effectiveness and performance of such an objective measure 
is, therefore, determined by the correlation of its scores with the subjective MOS scores. 
If an objective method has a high correlation, typically greater than 0.8, it is deemed to 
be an effective measure of perceived voice quality, at least for the speech data and 
transmission systems with the same characteristics as those in the test experiment. 
3   Linguistic Approach 
Perception tests (or listening tests) have been a standard part of phonetic research for a 
long time [3, 4]. As compared to tests used in signal processing and telecommunications, 
they are less standardized and their goal is often rather theoretical than application-
oriented. By experience, we know that test designs used in language engineering are 
sometimes judged “too technocratic”, and those used in linguistics, “too vague”. Also, 
when using listeners with linguistic education, one may obtain results that are different 
from those obtained from “average” listeners. 
The test design should be elaborated with respect to the hypothesis tested, type of 
speech data, implicitness vs. explicitness of the task, testing conditions, the listeners’  
 

128 
J. Tučková, J. Holub, and T. Duběda 
skills and several other aspects. It is apparent that phonetic findings should not be 
neglected when building formal frameworks for speech quality assessment (cf. the 
participation of trained phoneticians in the COCOSDA project [5]). 
There is a fundamental difference in objective and subjective assessment of 
processed speech: in the objective approach, formal distances from a reference are 
measured, while in subjective assessment, we evaluate the distance of the stimulus 
from the listener’s linguistic, paralinguistic and extralinguistic experience, where not 
all “deviations” from the reference have the same impact. 
The choice of listeners is a relevant factor in speech assessment: naive listeners are 
mostly representative of the target group, whereas trained listeners may be more 
attentive (and sometimes more speculative). Also, perception thresholds, which may 
be speaker-specific, should be controlled for. 
The assessment of speech used in practical applications should be situation-
oriented, i.e. we should not ask the question “How good does this sound?” or “Is this 
the best quality?”, but rather “How adequate is this for the given situation?”. Varying 
only one parameter at a time (e.g. method of unit selection, intonation, rhythm, 
speaking rate) permits to access the different variables constituting speech quality 
independently (cf. the experiment described below). 
4   Application to Synthetic Speech Assessment 
The pilot experiment described in this section is an application of the MOS procedure 
to a small sample of synthetic speech where the prosodic component is the observed 
variable. We use two different populations of listeners with different training: 
students of engineering (not familiar with auditory analysis or phonetics), and 
students of humanities (partly familiar with auditory phonetic analysis). Natural 
speech is not directly included in this testing, i.e. the different prediction methods are 
only compared between them. Of course, all the listeners’ judgments are necessarily 
made with respect to a general reference, represented by natural speech. 
4.1   Material 
We used six sentences with authentic wording taken from radio weather forecasts 
(Table 1), generated by the ARTIC synthesizer (University of West Bohemia in Pilsen 
[6]). Their prosody was predicted by a multi-layer neural network with one hidden 
layer [7, 8]. The number of neurons in the input layer was 70: this number 
corresponds to the set of language parameters used (silent pause type, stress unit 
boundary, syllable nucleus, punctuation mark, phoneme type, vowel height, vowel 
length, consonant voicing, manner of articulation of the consonant, number of 
phonemes in the stress group), analyzed in seven different positions (up to 3 positions 
before and 3 positions after the focus position). The ANN output is fundamental 
frequency F0 and speech unit duration Du. For the purposes of training, the target 
values of prosodic parameters were extracted from natural speech signal. 
 
 

 
Technical and Phonetic Aspects of Speech Quality Assessment 
129 
Table 1. List of the sentences 
Sentence 
Text of the sentence 
   a     
Předpověď počasí na noc a zítřek. 
            
Weather forecast for tonight and tomorrow. 
   b     
Východní vítr: 2 až 5 metrů za sekundu. 
            
Easterly wind: from 2 to 5 metres in a second. 
   c     
Zítra očekáváme většinou oblačno, místy přeháňky. 
                     Tomorrow, we expect mostly cloudy weather, with sporadic showers. 
   d     
Tlaková tendence: slabý vzestup, odpoledne slabý pokles.  
 
 
Pressure trends: slightly increasing, slightly decreasing again in the afternoon.
   e 
 
Maximální hodnota UV indexu: 5,8. 
            
Maximal value of the UV index: 5.8. 
   f    
To byly zprávy Českého rozhlasu 1 – Radiožurnálu. 
            
This was Czech Radio One News – Radio Journal. 
 
We used the feed-forward recall algorithm (using heuristic techniques) which had 
been found as optimal in previous experiments. The transfer functions were a sigmoid 
function in the hidden layer and a linear function in the output layer. The output 
corresponds to real values of F0 in [kHz] and Du in [s]; the choice of kHz and seconds 
is conditioned by the ANN activation function which has a scale (0,1). The batch 
mode was used in the back-propagation algorithm. 
An important problem in stochastic prediction of prosody is the ANN topology. 
The ultimate goal is to find an optimal number of neurons and their mutual 
connections so that the ANN has a good ability for generalization. No complete 
theory concerning the minimization of ANN is known. Four different methods were 
used to obtain the canonical ANN: the GUHA method [9], classical pruning of ANN 
weights, the weight sensitivities method and, finally, a combined method which 
consists in pruning neurons identified by at least two other methods [10]. 
In the GUHA method (General Unary Hypotheses Automaton), the processed data 
form a rectangular matrix where the rows correspond to speech units and the columns 
to different investigated attributes (e.g. fundamental frequency, duration of speech 
units and characteristic language features). The attributes are split into antecedents 
(e.g. characteristic language features) and succedents (e.g. fundamental frequency and 
duration of phonemes). The program generates and evaluates the hypotheses of an 
association of the form A Æ S, where A is an elementary conjunction of antecedents, 
S is an elementary conjunction of succedents and Æ is a quantifier of implication. 
The implication quantifier estimates the conditional probability P (S | A). The user has 
to specify the number of elements in antecedent and succedent conjunctions; after 
that, the program generates all possible hypotheses in sequence. According to the 
number of the verified hypotheses we can determine the importance of individual 
input parameters. 
The “classical pruning” method corresponds to the simplest algorithm for the 
pruning of neurons in input and hidden layers. It is based on the comparison of all 
sums of absolute values of synaptic weights for all neurons after the training: 

130 
J. Tučková, J. Holub, and T. Duběda 
 
⎟
⎠
⎞
⎜
⎝
⎛
=
∑
i
ik
k
prun
w
sum
min
 
 
 
 
      (1) 
where wik are the values of synaptic weights for the i-th input of the k-th neuron. 
Neurons with minimal values can be pruned. Neurons with maximum threshold 
values, operating in the saturation domain, can be pruned as well. 
The last pruning method is based on the sensitivities theory. This approach was 
first introduced in the domain of linear electronic circuit in the seventies, to be used in 
a lot of other domains later, e.g. in data mining. This method is based on the 
comparison of absolute values of output signals with small changes of input signals. 
The sensitivity of the n-th output yn to small changes of the k-th input xk: 
        
(
)
k
n
x
k
n
nk
x
y
x
y
S
S
k
Δ
Δ
=
=
→
Δ
0
lim
,
 
 
 
      (2) 
In practical applications, the main problem of this approach is the determination of 
the optimal Δxk value. 
4.2   Listening Test 
16 listeners (9 students of the Faculty of Electrical Engineering, Czech Technical 
University, and 7 students of the Faculty of Philosophy and Arts, Charles University 
in Prague) took part in the listening tests. The listeners were all naive, i.e. they had 
not been acquainted with the aim or the structure of the test before assessing. 
Utterances were played back in random order. The ITU-T Recommendation P.800 
has been used for the evaluation of speech quality [1]. The results of evaluation are 
given by MOS (Mean Opinion Score), where the following opinion scale is 
recommended: excellent – 5, good – 4, fair – 3, poor – 2, bad – 1. 
4.3   Results 
Figures 1 and 2 give arithmetic means and standard deviations for the five different 
predictions, as assessed by the two groups of listeners. 
 
0,00
1,00
2,00
3,00
4,00
5,00
nonpruned
combination
GUHA
weights
sensitivities
MOS
 
Fig. 1. Results of the listening test: MOS for five types of synthetic utterances obtained from 
students of engineering (N = 9). The error bars correspond to standard deviations. 

 
Technical and Phonetic Aspects of Speech Quality Assessment 
131 
0
1
2
3
4
5
nonpruned
combination
GUHA
weights
sensitivities
MOS
 
Fig. 2. Results of the listening test: MOS for five types of synthetic utterances obtained from 
students of linguistics (N = 7). The error bars correspond to standard deviations. 
In both groups, the ordering of the methods is the same: the method based on 
sensitivities seems to lead the best prosody, whereas the unpruned network offers the 
worst quality. In the engineering group, there is almost no difference of assessment 
between the combination method and GUHA, and in the linguistic group, unpruned 
networks and combination method were evaluated similarly. Generally speaking, the 
linguistic group had a higher tolerance threshold (overall MOS are higher), and did 
not show visibly higher perceptual sensitivity than the engineering group (the 
difference between the best and worst average score for the five methods is 1.06 in 
the engineering group, and 1.14 in the linguistic group). None of the differences 
between adjacent columns in either of the graphs is significant (pairwise t-test), 
except for unpruned/combination difference in the engineering group, which is nearly 
significant (p = 0,053). These results are at least partly counterbalanced by the fact 
that the ordering of methods is stable in both groups. Also, in prosodic optimization 
by means of pruning, we usually operate near the quality ceiling of prediction 
performance, which makes the significance criteria more relative. When comparing 
individual pruning methods to the approach where pruning is not used, we obtain 
significant differences in the expected direction for GUHA, weights and sensitivities 
in the engineering group, and for weights and sensitivities in the linguistic group. 
5   Conclusion 
The aim of the introductory parts of the present article was to discuss different aspects 
of speech quality assessment with respect to methods, perspectives and standards 
existing in technical sciences on the one hand, and in humanities on the other. 
Bridging the methodological gap between the two domains may be helpful as far as 
common research areas are involved. Also, the perceptual potential of linguistically 
educated listeners may be different from the rest of the population. 
A listening experiment whose goal was to assess the acceptability of synthetic 
stimuli generated by different prosodic models showed that both groups (students of 
engineering vs. humanities) returned the same ordering of variants, where the method 
based on sensitivities was assessed the best, and the unpruned approach the worst. 
This result provides, among other things, perceptual evidence for the generally 
accepted hypothesis that pruning is a viable method of prosodic optimization in 
speech synthesis. Interestingly, the “combined” pruning method, based on 

132 
J. Tučková, J. Holub, and T. Duběda 
information provided by at least two other methods, thus putatively more robust, did 
not lead to visible improvement.  
MOS seem to be an acceptable framework for the assessment of synthetic speech, 
which should be done in parallel with purely mathematical evaluation. Physical 
properties of the acoustic wave, perceived as a sound, undergo a complex, multi-level 
transformation in the human organism (audition, neural system, brain). Therefore, the 
sound perception does not fully correlate with the mathematical model of the acoustic 
patterns. However, it is the perceived speech quality which is decisive for human 
listeners. 
Future work will focus on using larger groups of listeners and testing listeners with 
different kinds of skills. 
 
Acknowledgments. This work was supported by the research program II MSM 
6840770012 and MSM 6840770014 of the Czech Technical University in Prague, and 
by the GAČR 405/07/0126 grant. 
References 
1. ITU-T P.800. Telecommunication standardization sector of ITU, Methods for objective 
and subjective assessment of transmission quality, ITU (1996)  
2. Hoene, C.: Internet Telephony over Wireless Links. Ph.D Thesis. TU Berlin (2005) 
3. Pisoni, D.B., Remez, R.E. (eds.): The Handbook of Speech Perception. Blackwell 
Publishing, Malden (2005) 
4. Kohler, K.J.: Paradigms in experimental prosodic analysis: from measurement to function. 
In: Sudhoff, S., et al. (eds.) Methods in Empirical Prosody Research, pp. 123–152. Walter 
de Gruyter, Berlin (2006) 
5. COCOSDA International Committee for Co-ordination and Standardisation of Speech 
Databases, http://www.cocosda.org/ 
6. Matoušek, J., Tihelka, D., Romportl, J.: Current state of Czech text-to-speech system 
ARTIC. In: Sojka, P., Kopeček, I., Pala, K. (eds.) TSD 2006. LNCS (LNAI), vol. 4188, 
pp. 439–446. Springer, Heidelberg (2006) 
7. Riedi, M.: A neural-network-based model of segmental duration for speech synthesis. In: 
Proc. Eurospeech 1995, vol. 1, pp. 599–602. European Speech Communication Association 
(1997) 
8. Traber, C.: F0 generation with a database of natural F0 patterns and with a neural network. 
In: Bailly, G., Benoît, C., Sawallis, T.R. (eds.) Talking Machines: Theories, Models, and 
Design, pp. 287–304. Elsevier Science Publishers, Amsterdam (1992) 
9. Hájek, P., Sochorová, A., Zvárová, J.: GUHA for personal computers. Computational 
Statistics and Data Analysis 19, 149–153 (1995) 
10. Tučková, J., Šebesta, V.: The Prosody Optimisation of the Czech Language Synthesizer. 
In: Novák, M. (ed.) International Journal on Neural and Mass-Parallel Computing and 
Information Systems “Neural Network World", ICS AS CR and CTU, FTS, vol. 18(4),  
pp. 291–308 (2008) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 133–148, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Syntactic Doubling: Some Data on Tuscan Italian 
Anna Esposito 
Second University of Naples, Department of Psychology, and IIASS, Italy 
iiass.annaesp@tin.it 
Abstract. The data of an experimental study on Syntactic Doubling 
(Raddoppiamento Sintattico (RS)) in Tuscan Italian are reported and discussed. 
(a) Consonant not subject to RS with consonant possibly subject to RS in 
adjacent stressed syllables; (b) with a phonological boundary intervening 
between trigger and target; and (c) with no phonological phrase boundary 
intervening are compared. We consistently find lengthening in (b) (short RS), 
but more lengthening in (c) (long RS). Our data indicate that (short) RS applies 
across phonological phrases. The publication of these data was motivated by the 
recent interest raised by the ESF-funded project Edisyn [1] on dialect syntax 
aiming, among the other goals, to investigate doubling phenomena in European 
languages and dialects for cross-linguistic comparisons. 
Keywords: Syntactic Doubling, phonological phrase. 
1   Introduction 
There has been a surge of interest in syntactic doubling thanks to the Edisyn project 
that has created a European network of researchers studying doubling phenomena in 
several European languages and dialects [1-2]. This was because doubling can 
provide highlights on the structure of languages as well as help in formulating 
hypotheses explaining natural language changes and dialectal differences. Several 
questions remains open about syntactic doubling among those its usefulness since its 
linguistic realization contains a component that could be considered semantically 
redundant. Provided explanations argue that such redundancy is a linguistic tool to 
facilitate communication or emphasizes clause’s constituents [18] failing to justify its 
pervasive use in dialects more than in standard language varieties. It is clear that more 
data and more cross-linguistic comparisons are needed to ascertain the role of 
syntactic doubling and it is in this spirit that we are providing the data below. 
In this paper we report some data concerning Syntactic Doubling and phonological 
phrasing in Tuscan Italian (Raddoppiamento Sintattico (RS)).  
The measurements reported are not in agreement with a suggestion that is 
prominent on this topic but we are not offering an alternative explanation, since 
further experiments are required to assess and evaluate these data in comparison with 
those already reported in the literature [3-6]. For now, we confine ourselves to present 
and discuss the data hoping that they can contribute to a part of the puzzle. 
Raddoppiamento Sintattico (RS) is a rule of lengthening across words. In Tuscan 
Italian, the dialect in which RS has primarily been studied, RS lengthens the initial 

134 
A. Esposito 
consonant of a word w2 when w2 is preceded by a word w1 with a final stressed 
vowel. In 1a and 1b are reported two language environment when RS apply. 
 
 (1) 
 w1    w2 
a. 
caffè caldo  ->  caffè [kk]aldo 
 
'warm coffee' 
b. 
té freddo     ->  té [ff]reddo 
 
'cold tee' 
but: 
c. 
mólto caldo  ->  mólto [k]aldo 
 
'very warm' 
 
Vogel [18] and Chierchia [3] represent RS as double linking of a word-initial onset 
consonant to a coda-position of a preceding, stressed syllable.  
Previous measurements of RS were published by Korzen [11-12], Vogel [18] and 
Marotta [13]. Detailed phonological accounts of RS can be found in Vogel [18], 
Chierchia [3], and Nespor and Vogel [14-16],  Borrelli [3], and Poletto [17]. Nespor 
and Vogel [14-16] maintain that RS applies within phonological phrase (Φs) as the 
sentence in 2a, but not across them as in 2b. 
 
(2)   a. (Avrá  [tt]rovato) Φ (la via) Φ  (He must have found the street) 
   b. (Troverai dei posti in cittá) Φ ([m]olto vecchi) Φ (You will find very old city 
places) 
 
They offer a rule of RS that stipulates that RS only apply within Φ.  Our findings 
suggest that RS is not restricted to the phonological phrase but also applies across Φ-
boundaries. We also identified two varieties of RS: short RS and long RS. While short 
RS applies across Φ-boundaries, long RS seems not to occur across Φ-boundaries.  
This paper is presenting the data of an experiment that brings to the above findings, 
leaving to the phoneticians the duty to provide a phonetic/phonological explanation. 
This is because the author personal involvement was in the experimental set up, the 
data analysis,  and data collection but also because it is the author’s opinion that the 
data are worth to be published in order to emphasize differences among languages. The 
list of all measurements and some examples of spectrogram is given in Appendix 1. 
2   Experimental Set-Up and Data Collection 
2.1   Participants 
To compare our findings with Nespor and Vogel's [14-16] proposals, subjects were 
chosen among speakers from Florence. F, a woman in her mid-twenties, and R, a man 
in his early forties, both grew up in Florence, and with parents living there at the time 
of data collection. The data were collected at the Massachusetts Institute of 
Technology in Cambridge, in USA where F has been staying for two month, and R 
for one and a half years. 
2.2   Materials 
Each subject read 105 Italian sentences three times, in different randomized orders. 
The first reading discarded and was intended to allow the subject to get used to the 

 
Syntactic Doubling: Some Data on Tuscan Italian 
135 
recording’s situation, and to the material. We asked our subjects to read each sentence 
as an answers to a question1. The questions were read by the author, native speaker of 
Italian. If a pronunciation error occurred or if the subject felt that she/he did not 
answered in a natural way, the question-answer pairs were repeated. The crucial 
sentences for the issue at hand are shown in 3. 
 
(3) /s/      Un re salta fuori          Un papà salta fuori    Un caribù salta fuori 
    /m/      Un re merita                  Un papà merita 
     Un caribù merita 
   /d/      Un re dona un regalo    Un papà dona un regalo   Un caribù dona un regalo 
  /p/      Un re parte 
         Un papà parte 
               Un caribù parte 
             A king ...  
     A father ... 
 
  A caribou  is jumping out 
                                                                                                         is worthy 
                                                                                                         is giving a   present 
                                                                                                         is leaving 
 
 
   
As it can be seen, they are of the form Noun Phrase Verb Phrase [NP VP], 
consisting of permutations of three preverbal subjects (NP in the following) with four 
verb phrases (VP in the following) which have the initial consonants /s/, /m/, /d/, and 
/p/. Here the question whether RS applies across NP and VP is of interest: On the one 
hand, the phonological environment for RS is given in these examples by final stress 
on the subject nouns. On the other hand, a phonological phrase boundary, separating 
NP and the VP in Italian (see below) separates the trigger of RS (the final stressed 
vowel of the NP) from the target (the initial consonant of the verb).  
These sentences allow us to determine whether RS applies across a Φ-boundary or 
not and are compared with sentences in which RS does not apply for independent 
reasons, and with sentences in which RS would apply with a high degree of certainty. 
The sentences in which RS do not apply are given in 4.  
 
(4)  /s/ Un papa salta fuori  
 Un'amica salta fuori 
 
 /m/ Un papa merita  
  Un'amica merita 
 
 /d/ 
Un papa dona un regalo 
  Un'amica dona un regalo 
/p/ 
Un papa parte 
                Un'amica parte 
         A pope ... 
 
A friend-FEM 
is jumping out 
                                                                                 is worthy 
                                                                                 is giving a present 
                                                                                 is leaving 
 
The subjects nouns in the above sentences do not have final stress, thus making RS of 
the verb-initial consonant impossible (see 1 above). Otherwise the sentences in 4 are 
parallel to the ones in 3. The sentences for comparison where we are confident that 
RS would apply are shown in 5. 
 
                                                           
1 This allowed to control the focus in the answered sentence for some purposes that we were 
initially interested in (see below for some more comments). 

136 
A. Esposito 
(5) 
   a. 
 
 
                                   b. 
 /s/ 
Ho visto tre soli                                      Ho bevuto un caffè super 
/d/ 
Ho contato tre dita 
                     Ho bevuto un caffè denso 
/m/ 
Aveva tre mani  
                      Ho bevuto un caffè marcio 
/p/ 
Ho comprato tre pani 
                      Ho bevuto un caffè puro 
I have seen three suns; I have drunk a super coffee; I have counted three fingers; I 
have drunk a dense coffee; He had three hands;  I have drunk a rotten coffee; I have 
bought three breads; I have drunk a pure coffee 
 
These are of the form [V tré N] and [Ho bevuto un caffè A]. Here RS would apply 
between the last two words in each sentence due to the final stressed vowel in tré and 
caffè. In these cases, no Φ-boundary would normally intervene between the trigger of 
RS and the target. 
The question of interest is then whether the cases in 3, where there is a 
phonological environment for RS, but a phonological phrase boundary intervening, 
show consonant-length comparable to 4 with no RS, or to 5a and 5b with RS (see 
examples of spectrograms in Appendix 1). 
Each of the sentences in 3, 4, and 5a-5b, occurred more than once in our list of 
105, each occurrence having a different focus, as prompted by the preceding question. 
The sentences in 3 and 4 occurred two times each in our list, one with focus on the 
NP, and another one with focus on the VP. The sentences in 5a-5b occurred three 
each in our sample: one with focus on the verb, one with focus on the trigger for RS 
(tre, caffè), and one with focus on the final element. Our sample thus contained 24 
question-answer pairs for 3, 16 for 42, and 24 for 5a-5b. This accounts for 64 of the 
105 question-answer pairs in our list. Each of these was read twice by our two 
subjects. Thus the total number of measurements is 96 for the pattern in 3, 64 for the 
pattern in 4, and 96 for the pattern in 5a-5b. 
The recordings were made in the Speech Communication Laboratory (MIT), in a 
sound-treated room using a high-quality magnetic tape recording system. The 
measurements of consonant length were performed using the KLSPEC93 speech 
analysis program (see Klatt [8-10]), which accepts user commands to read in 
waveform files and generates spectral displays of various types. The spectral 
representations used for the analysis of our data include a DFT (Discrete Fourier 
Transform) magnitude spectrum and a smoothed version of DFT called Spectrogram-
like Spectrum. The analysis window (Hamming window) was set to a default of 50 
samples which corresponds to 3.1 msec at a sampling rate of 16000 Hz. The first step 
in the analysis procedure was to process the speech signals by a low-pass filter with a 
cutoff frequency of 7500 Hz3.  The output of the filter was sampled at the sampling 
rate of 16000 Hz and stored in a computer memory. Sound spectrograms of all 
utterances and visual displays of the corresponding waveform were also made. On 
these data, we measured the consonant duration. The temporal sampling points 
                                                           
2 Initially 24, but eight of them turned out not to be useful for our purposes: We found that the 
preposition per, used as Un "per" ...  systematically triggers RS of the following consonant: 
un "per" da problemi -> ..."per" [dd]a ... . This raises some interesting questions about RS. 
However, we will not pursue them here. 
3 This kind of filter is appropriate for speech. 

 
Syntactic Doubling: Some Data on Tuscan Italian 
137 
defined below were identified by visual inspection of the spectrogram, the waveform, 
and the spectrum.  
2.3   Measurements 
Consonant duration is defined in this study as the duration of the time interval between 
the offset of the previous vowel and the consonantal release in the case of the stop 
consonants /d, p/, and as the duration of the time interval between the offset of the 
previous vowel and the onset of the following vowel in the case of the consonants /m, s/. 
The waveform, the spectrogram and the spectrum were examined in parallel to 
identify the temporal sampling point corresponding to the release of the consonant, 
and the offset and onset of the vowel.  
 
The release. The oral release of /d, p/ is usually marked in the spectrogram by an 
abrupt onset of energy at high frequencies. There are, however, cases in which it is 
not possible to identify the release by looking at the spectrogram. In such cases, the 
smoothed spectrum and the DFT spectrum were computed and superimposed every 
1ms by moving the cursor along the waveform and generating a spectral slice each 
time. The release of the consonant was identified as the instant in which there is an 
abrupt onset of energy in the spectral slices at frequencies higher than 3kHz. 
Nevertheless, there are a few cases for /d/ in which the consonant is released into the 
vowel, i.e. there is no release prior to the vowel onset. In such cases the temporal 
sampling points corresponding to the release of the consonant and the vowel onset are 
considered coincident. 
 
The vowel offset. The temporal sampling point defined as the vowel offset was 
identified as the temporal instant in which the second formant energy abruptly drops 
in intensity. This point was determined with the help of the spectral slices and the 
spectrogram. This procedure was also checked for errors by looking at the waveform, 
where the vowel offset correlates with an abrupt change in the characteristics of the 
waveform and a clear reduction of the glottal pulsing. /m/ sometimes showed weak 
second formant structure, in which case a drop in intensity of the second formant 
energy identified the vowel offset. Where possible, this latter procedure was checked 
for errors by taking the offset of F3  and F4  energy into account. In the case of /s/, the 
second formant energy of the vowel would sometimes cease more gradually. In such 
cases the vowel offset was considered to be the point at which the high frequency 
energy of /s/ became stronger in amplitude than the second formant energy of the 
vowel in the spectrum.  
 
The vowel onset. The temporal sampling point defining the vowel onset was 
identified as the time at which the second formant energy of the vowel appears in the 
spectrogram (or shows a sudden increase of intensity, in case a preceding /m/ showed 
weak second formant structure). For /m/, this point was identified with the help of the 
spectrogram and the spectrum. In the case of /s/ which shows no low-frequency  
 

138 
A. Esposito 
energy, this point was also clearly discernible in the waveform where the first glottal 
pulsing coincides with the onset of second formant energy of the vowel. 
3   Results 
An example from each of the patterns in 3, 4, and 5a-5b is reproduced in 6a-6c with 
phonological phrase boundaries indicated. The consonants of interest are underlined. 
In 6a, RS was not expected regardless of the Φ-boundary, since the previous vowel is 
not stressed. In 6c, RS would apply, since a stressed vowel is followed by a consonant 
in the same Φ. 6b is the case we are interested in, with the preceding vowel stressed 
but a Φ-boundary intervening between the stressed vowel and the following 
consonant. 
 
(6)  a. 
(Un pápa) (merita) Φ 
 
A pope is worthy 
      b. 
(Un papà) (merita) Φ 
 
A father  is worthy) 
      c. 
(Ho bevuto) (un caffè marcio) Φ 
I have drunk a rotten coffee 
 
In the following, the notation (a), (b) and (c) will be used to refer to the patterns of 
sentences exemplified in 6a-6c, respectively. 
 
The comparison of the three patterns shows that consonant length in (b) is neither 
comparable to (a), nor like (c). The consonants in (b) are consistently longer than 
those in (a), and consistently shorter than those in (c). The overall percentage of 
consonant lengthening in (b) and (c) in relation to (a), broken down by speakers, is 
shown in Table 1.  
Table 1. Overall consonant’s lengthening in the environments (b) and (c) in relation to the 
consonant’s lengthening in the (a) environment 
Conditions 
Speaker F 
Speaker R 
(a) No RS 
100% 
100% 
(b) RS across a Φ−boundary 
144% 
160% 
(c) RS 
220% 
201% 
 
There is a certain amount of variation among the four consonants /s/, /m/, /d/ and 
/p/ in our sample. Each consonant, of course, had a different length in its normal form 
in (a). Furthermore, each speaker would lengthen each consonant to a different extent 
in (b) and in (c). However, the general pattern of Table 1 is stable across speakers and 
consonants. Each speaker lengthened consonants in (b) relative to (a), and consonants 
in (c) more than in (b) as reported in Table 2. 
Each value in Table 2 is an average (in msec) over either 12 or 8 measurements (12 
in the rows labeled (b) and (c), 8 in the rows labeled (a)).  
Nespor and Vogel [14-16] maintain that RS is optional in certain (other) 
environments, and one might wonder whether RS applies optionally in the (b)-cases.  

 
Syntactic Doubling: Some Data on Tuscan Italian 
139 
Table 2. Consonant’s lengthening in the three environments under examination  
Conditions 
Speaker F 
Speaker R 
 
average length/ms  % of (a)  
average length/ms    % of (a)  
/s/ 
 
 
 
 
(a) 
85 
 
86 
 
(b) 
101 
118 
122 
142 
(c) 
186 
218 
171 
199 
/m/ 
 
 
 
 
(a) 
63 
 
71 
 
(b) 
91 
144 
123 
172 
(c) 
131 
207 
131 
185 
/d/ 
 
 
 
 
(a) 
49 
 
41 
 
(b) 
63 
130 
73 
180 
(c) 
109 
224 
106 
259 
/p/ 
 
 
 
 
(a) 
59 
 
86 
 
(b) 
108 
182 
127 
148 
(c) 
136 
230 
139 
162 
 
 
Fig. 1. Length of verb-initial /s/ for the speakers F and R in the (a) (no RS) vs. (b) (short   RS) 
environment 
Figure 1 displays (for each subject) the distribution of the durations that determine 
the averages reported in the first two rows of Table 2, for the consonant /s/ in the (a) 
and (b) conditions. The durations for (a) (no RS) and for (b) (RS across subject and 
verb) are reported along the x-axis whereas the occurrences of these durations are 
displayed for (a) along the negative values and for (b) along the positive values of the 
y-axis. Each mark in the figure corresponds to one duration. 
Figure 1 suggests that for speaker F RS might be interpreted as optional in (b). 
There are some occurrences in (b) between 70 and 90 msec, around the average length 
of the non-elongated consonants in (a). Other occurrences of /s/ for F in (b) are 
longer. For speaker R, instead, the durations reported in the upper half of Figure 1 are 

140 
A. Esposito 
consistently longer than the non-RS values in the lower half and do not suggest 
optionality of RS in (b).  
We integrate the other consonants into the picture in order to check which part of 
the partial data displayed in Figure 1 represents the general pattern, and which an 
accidental deviation. To do so we applied to the distribution of (a) and (b) for each 
consonants and each speaker as reported in Figure 1 a linear transformation that leave 
the occurrences of the different duration in (a) and (b) unaltered and change the  x-
axis scale the mean duration values (for each speaker and each consonant)  of (a) 
maps into 0 and that of  (b) maps into 1 (0 and 1 are chosen arbitrarily) obtaining the 
general distributions displayed in Figure 2.  
 
 
Fig. 2. Distribution of consonant’s length with a trigger for RS (top) relative to consonant 
length without a trigger  for RS (bottom) on the preceding word 
 
Figure 2 displays the overall distribution of (b) relative to (a) showing that RS in 
(b) cannot be considered optional.  
If RS is optional in (b) and obligatory in (c), we would likewise expect that there 
should be a peak in the distribution of (b) around the average of (c). This is again not 
the case. Figure 3 shows the relative distribution of (b) and (c).  The values for (b) and 
(c) were combined in a single diagram using the same linear transformation applied in 
Figure 2, such that it was possible to abstract away, for each speaker and consonant, 
from the different average length in (b) as well as from the relative differences of 
lengthening in (b) vs. (c). The values of (b) are shown above the scale, the ones of (c) 
below the scale. There are a few cases of (b) around the average of (c), but they are 
clearly exceptions, not the general pattern. Figure 3 makes it clear that the shorter 

 
Syntactic Doubling: Some Data on Tuscan Italian 
141 
average of  consonant length in (b) in comparison to (c) that was observed in Tables 1 
and 2 reflects a case by case difference in length.  
Figures 2 and 3 show that the intermediate average length of (b) in comparison to 
(a) and (c) is not the result of (b) being sometimes like (a), sometimes like (c), 
demonstrating that the consonants in question are consistently longer in pattern (b) 
than in pattern (a) and consistently longer in pattern (c) than in pattern (b). The 
complete list of measurements is given in Appendix 1.  
 
Fig. 3. Two amounts of lengthening in RS-environments: distribution of consonant’s length 
across subject and verb (top) relative to consonant’s length in other RS environments (bottom) 
3.1   The Irrelevance of Focus 
We controlled for focus in our experiment to be able to determine to what extent, if 
any, focus would affect phonological phrasing and RS. It appears that focus does not 
interact with RS in a systematic way in the cases we investigated. In the cases of an 
intervening Φ-boundary, we found the small amount of lengthening regardless of 
whether the focus is on the NP or on the VP. For each subject F and R, there is some 
consonant where the subject lengthened the consonant slightly more with noun-phrase 
focus than with verb-phrase focus, and another consonant where the subject showed 
more lengthening with verb-phrase focus than with noun-phrase focus. Such variation 
is minor: when it occurs, the average of consonant length for NP-focus and the 
average of consonant length for VP-focus do not differ by more than 10msec4. There 
                                                           
4 Here again, the /s/ of subject F are an exception: the exceptionally short /s/mentioned above 
all occurred in examples with focus on VP, thus leading to a difference greater than 10msec 
between focus on NP and focus on VP for this consonant and subject.  

142 
A. Esposito 
is no clear pattern in this variation, and it averages out over all occurrences. Similar 
remarks apply to the sentences with no RS and to the ones with long RS: the variation 
with focus is small and apparently randomly distributed. 
3.2   Interpretation: No RS vs. Short RS 
As far as the sentences are concerned, the only difference between the examples in (a) 
and the examples in (b) is whether or not the NP ended in a stressed vowel. 
Everything else was held constant. In particular, the crucial consonants are in the 
same syntactic environment (VP-initial) and in the same sentence-phonological 
environment (Φ-initial, see below) in both cases. If these factors alone have an impact 
on the length of consonants, they affect the consonants in (a) and in (b) alike.  
We conclude, therefore, that RS, which cannot apply in (a) has applied in (b). RS, 
then, has applied obligatorily across subjects and verbs in our data (one insignificant 
irregularity aside).  
4   Conclusions  
We have presented measurements that suggest that RS applies across as well as within 
the phonological phrase. The data provided highlight this phenomenon for the first 
time, even though RS has been observed and reported by several authors [3-6]. We did 
not provide an explanation for this phenomenon, leaving to the phoneticians this duty. 
Together with a phonetic and/or phonological explanation for RS across a Φ-
boundary, a number of interesting questions remain open. One of them is how to 
explain differences in lengthening of both short and long RS reported some authors 
[13-14] with respect to those observed in our experiments. Are there any dialectal 
influences. What would happen for southern Italian speakers? 
  
Acknowledgments. This work was developed in collaboration with Hubert 
Truckenbrodt, when we were both students at MIT. I was personally involved in the 
experimental set-up, the data collections, the analysis and the measurements. I am 
extremely grateful to Hubert, since what is reported here is the result of long night 
hours spend together in the lab for taking the measurements. These data do exists 
thanks to Hubert’s invaluable and priceless collaboration. All mistakes are obviously 
my own. Furthermore, Hubert suggested a phonetic/phonological explanation of the 
short and long RS based on  a Stress Clash Removal hypothesis  reported Esposito & 
Truckenbrodt [5]. This work has been partially funded by COST 2102 “Cross Modal 
Analysis of Verbal and Nonverbal Communication”, http://cost2102.cs.stir.ac.uk/ and 
by Regione Campania, L.R. N.5 del 28.03.2002, Project ID N. BRC1293, Bando Feb. 
2006. We are grateful to Miss Tina Marcella Nappi for her editorial help. 
References 
1. http://www.dialectsyntax.org/index.php/network-mainmenu-58 
2. Workshop on Syntactic Doubling in European Dialects, Amsterdam, March 16-18 (2006), 
http://www.dialectsyntax.org/index.php/dialect-syntax-
archive-mainmenu-66 

 
Syntactic Doubling: Some Data on Tuscan Italian 
143 
3. Borrelli, D.: Raddoppiamento Sintattico in Italian: A Synchronic and Diachronic Cross-
Dialectical Study. Dissertations in Linguistics, Routledge (2002) ISBN 0415942071 
4. Chierchia, G.: Length, Syllabification and the Phonological Cycle in Italian. Journal of 
Italian Linguistics 8, 5–34 (1986) 
5. Esposito, A., Truckenbrodt, H.: A Note on Raddoppiamento Sintattico and the 
Phonological Phrase in Italian. IIASS Technical Report I0001,1-45, Via Pellegrino 19, 
Vietri sul Mare (Sa) I84019, Italy (2000) 
6. Fanciullo, F.: Syntactic Reduplication and the Italian Dialects of the Centre-South. Journal 
of Italian Linguistics 8, 67–104 (1986) 
7. Ghini, M.: Φ-formation in Italian: A New Proposal. In: Dyck, C. (ed.), Toronto, Working 
Papers in Linguistics, vol. 12(3), pp. 41–78 (1993) 
8. Klatt, D.H.: Software for a Cascade/Parallel Formant Synthesizer. Journal of the 
Acoustical Society of America 67, 971–995 (1980) 
9. Klatt, D.H.: HARSYN: An Additive Harmonic Speech Synthesizer. Speech Communication 
Group Working Papers 1, 46–70 (1982) 
10. Klatt, D.H.: MIT Speech Vax User’s Guide. Copyright 1984 by Dennis Klatt (1984) 
11. Korzen, I.: Il Raddoppiamento Sintattico e la Geminata nella Variante Toscana dell’Italiano-
Standard: Risultati di un’indagine Sperimentale. Studi Italiani di Linguistica Teorica ed 
Applicata 9, 333–366 (1980a) 
12. Korzen, I.: Il Prolungamento Intervocalico di Alcune Consonanti Iniziali e Finali di Parola 
nella Variante Toscana dell’ Italiano-Standard: Risultati di un’Indagine Sperimentale. 
Studi Italiani di Linguistica Teorica ed Applicata 9, 367–384 (1980b) 
13. Marotta, G.: Rhythmical constraints on Syntactic Doubling. Journal of Italian Linguistics 8, 
35–52 (1983) 
14. Nespor, M., Vogel, I.: Prosodic Domains of External Sandhi Rules. In: van der Hulst, H. 
(ed.) The structure of phonological representations, pp. 225–265. Foris, Dortrecht (1982) 
15. Nespor, M., Vogel, I.: Prosodic Phonology. Foris, Dortrecht (1986) 
16. Nespor, M., Vogel, I.: On Clashes and Lapses. Phonology 6, 69–116 (1989) 
17. Poletto, C.: Doubling as Economy. Paper presented to Workshop on Syntactic Doubling in 
European Dialects, Amsterdam, March 16-18 (2006),  
  http://www.dialectsyntax.org/index.php/dialect-syntax- 
 archive-mainmenu-66 
18. Vogel, I.: La Sillaba come Unità Fonologica. Zanichelli, Bologna (1982) 
Appendix: 1 
The following Tables pages show the single measurements that this paper is based on. 
The columns are structured as follows: the large columns are for the two subjects, F 
and R. The columns subordinate to these distinguish focus on the subject NP ('Chi 
...?') and focus on the VP ('Cosa ...?'). 'r1' and 'r2' are recording 1 and recording 2 for 
each of our subjects. 
 
length of /s/ 
 
F
G 
 
Chi ...?
Cosa fa ...?
Chi ...?
Cosa fa ...? 
(a) no RS 
r1 
r2 
r1 
r2 
r1 
r2 
r1 
r2 
Chi salta fuori?/Cosa fa un papa? 
Un papa salta 
fuori 
88 
89
76
88
90
87
72 
87 

144 
A. Esposito 
Chi salta fuori?/Cosa fa un’amica? 
Un’amica salta 
fuori 
81 
102
72
87
83
89
84 
93 
(b) RS across Φ 
 
  
Chi salta fuori?/Cosa fa un re? 
Un re  salta fuori 
120 
109
106
(80)
138
123
125 
138 
Chi salta fuori?/Cosa fa un papà? 
Un papà salta 
fuori 
107 
131
(82)
107
115
112
116 
105 
Chi salta fuori?/Cosa fa un caribù? 
Un caribù salta 
fuori 
104 
92
105
(69)
126
131
120 
111 
 
F
G 
(c) RS within Φ 
r1
r2
r1
r2 
Quanti soli hai visto? 
Ho visto tre soli 
205
218
191
172 
 
Quali erano le tre cose che hai visto? 
Ho visto tre soli 
218
196
182
179 
Hai visto o hai letto di tre soli? 
Ho visto tre soli 
184
198
178
176 
Che cosa hai bevuto di super? 
Ho bevuto un 
caffè super 
172
172
162
153 
Che tipo di caffè hai bevuto? 
Ho bevuto un 
caffè super 
154
159
165
149 
Hai bevuto o volevi bere un caffè super? 
Ho bevuto un 
caffè super 
178
175
172
175 
 
 
length of /d/ 
 
 
F
G 
 
 
Chi ...?
Cosa fa ...?
Chi ...?
Cosa fa ...? 
(a) no RS 
r1
r2
r1
r2
r1
r2
r1 
r2 
 
 
 
Chi dona un regalo?/Cosa fa un papa? 
Un papa dona un regalo 
50
47
49
43
37
27
37 
31 
Chi dona un regalo?/Cosa fa un’amica? 
 
Un’amica dona un regalo 
46
57
57
40
38
56
58 
44 
(b) RS across Φ 
  
Chi dona un regalo?/Cosa fa  un re? 
Un re dona un regalo 
76
59
101
71
88
75
73 
85 
Chi dona un regalo?/Cosa fa un papà? 
 

 
Syntactic Doubling: Some Data on Tuscan Italian 
145 
Un papà dona un regalo 
52
54
57
62
76
71
62 
68 
Chi dona un regalo?/Cosa fa un caribù? 
Un caribù dona un regalo 
63
55
54
56
84
57
77 
63 
 
 
F
G 
(c) RS within Φ 
r1
r2
r1
r2 
Quante dita hai contato? 
Ho contato tre dita 
94
114
98
108 
Quali erano le tre cose che hai contato? 
Ho contato tre dita 
106
128
96
102 
Aveva, o hai contato tre dita? 
Ho contato tre dita 
97
110
113
109 
Che cosa hai bevuto di denso? 
Ho bevuto un caffè denso 
110
96
115
103 
Che tipo di caffè hai bevuto? 
Ho bevuto un caffè denso 
135
115
110
102 
Hai bevuto o volevi bere un caffè denso? 
Ho bevuto un caffè denso 
107
94
110
111 
 
 
length of /p/ 
 
F
G 
 
Chi ...?
Cosa fa ...?
Chi ...?
Cosa fa ...? 
 
(a) no RS 
r1 
r2
r1
r2
r1
r2
r1 
r2 
Chi parte?/Cosa fa un papa? 
Un papa parte  
62 
44
53
61
91
90
93 
82 
Chi  parte?/Cosa fa un’amica? 
Un’amica parte 
83 
58
56
58
84
89
73 
84 
(b) RS across Φ 
 
  
Chi parte?/Cosa fa  un re? 
Un re parte 117 
102
111
96
134
131
140 
132 
Chi parte?/Cosa fa un papà? 
Un papà parte 102 
102
115
122
131
108
123 
124 
Chi parte?/Cosa fa un caribù? 
Un caribù parte 104 
112
97
118
131
109
142 
114 
 
F
G 
(c) RS within Φ 
r1
r2
r1
r2 
Quanti pani hai comprato? 
Ho comprato tre pani 
135
120
140
139 
Quali erano le tre cose che hai comprato? 
Ho comprato tre pani 
143
126
131
149 
Hai rubato, o comprato tre pani? 
Ho comprato tre pani 
145
139
138
102 
Che cosa hai bevuto di puro? 
Ho bevuto un caffè puro 
140
141
142
142 

146 
A. Esposito 
Che tipo di caffè hai bevuto? 
Ho bevuto un caffè puro 
136
128
142
140 
Hai bevuto o volevi bere un caffè puro? 
Ho bevuto un caffè puro 
134
149
141
165 
 
 
 
length of /m/ 
 
F
G 
 
Chi ...?
Cosa fa ...?
Chi ...?
Cosa fa ...? 
(a) no RS 
r1
r2
r1
r2
r1
r2
r1 
r2 
Chi merita?/Cosa fa un papa? 
Un papa merita 
50
77
73
63
68
63
70 
73 
Chi  merita?/Cosa fa un’amica? 
Un’amica merita 
58
65
60
61
81
55
79 
83 
(b) RS across Φ 
  
Chi merita?/Cosa fa  un re? 
Un re merita 
87
90
103
95
124
125
127 
129 
Chi merita?/Cosa fa un papà? 
Un papà merita 
93
85
105
97
122
116
107 
135 
Chi merita?/Cosa fa un caribù? 
Un caribù merita 
81
77
83
94
129
125
111 
123 
 
F
G 
(c) RS within Φ 
r1
r2
r1
r2 
Quante mani  aveva il mostro? 
Aveva tre mani 
153
142
125
128 
Quali erano le tre cose che aveva il mostro? 
Aveva tre mani 
139
153
139
138 
Il mostro aveva, o hai creduto che avesse tre mani? 
Aveva tre mani 
135
122
137
128 
Che cosa hai bevuto di marcio? 
Ho bevuto un caffè marcio 
114
113
122
129 
Che tipo di caffè hai bevuto? 
Ho bevuto un caffè marcio 
138
137
140
130 
Hai bevuto o volevi bere un caffè marcio? 
Ho bevuto un caffè marcio 
111
118
126
128 
 

 
Syntactic Doubling: Some Data on Tuscan Italian 
147 
 
 
 
 

148 
A. Esposito 
 
 
 
 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 149–161, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Perception of Czech in Noise: Stability of Vowels 
Jitka Veroňková and Zdena Palková 
Institute of Phonetics, Faculty of Arts and Philosophy, nám. J. Palacha 2, 
11638 Prague, Czech Republic 
{Jitka.Veronkova,Zdena.Palkova}@ff.cuni.cz 
Abstract. The paper is based on results of perceptual tests focused on 
recognition of Czech words in noise. Identification of vowels in the syllabic 
nuclei proves most consistent. Resistance of individual phonemes and 
regularities in their substitution are examined. The primary focus concerns the 
position in a word. It shows that in Czech the position in the stressed syllable is 
not decisive for correct identification of vowels by listeners. Also the relations 
to duration and formant characteristics (F2/F1) bring negative results. 
Identification rates distinctly vary even among particular occurrences of 
individual phonemes. These differences do not have a verifiable relation to 
particular acoustic or structural feature. The assumption is vindicated that in the 
recognition of the acoustic word in adverse conditions a complex of features is 
in operation where these can act separately, in cooperation or in competition. 
Keywords: perception of speech, noise, vowels, Czech, word stress. 
1   Introduction 
Noise makes speech communication more difficult and much effort is spent to find 
procedures to minimize the negative impact of the adverse conditions. Perception 
tests which exploit speech in noise are accepted as a legitimate source of valuable 
information in speech communication research [1], [2]. With respect to listeners’ 
skills in speech, the success of perception in noise could inform us both about the 
listeners’ ability to discriminate items in the first language and about perceptive skills 
in the acquisition of the second language (see e.g. [3], [4], [5]). With respect to the 
speech material, the robustness of sounds and their features in the listeners’ 
discrimination of the speech items could be observed (see e.g. [6], [7], [8]). In this 
paper the second topic will be discussed with respect to some data on the perception 
of Czech samples in noise.   
1.1   Perceptual Test Based on the Czech Material: Aims and Method  
The presented discussion is based on the listening in which the listeners tried to 
identify simple samples of Czech masked with white noise on gradually increasing 
levels. The test was created in the 1970s [9] and it has been administered until today. 
Its primary objective was the evaluation of perceptual skills of listeners. Despite this 
initial objective, however, the large range of the collected data enables to evaluate 
some aspects of the speech material from the point of the resistance to noise. 

150 
J. Veroňková and Z. Palková 
Listeners. For the purpose of this research, judgements of 300 native Czech listeners 
in total are presented. Three homogeneous groups of native Czech listeners were 
chosen from different periods of 1979-1980, 1996-1997, and 2006-2007; there are 
100 listeners in each group. They were all students of the Faculty of Arts, Charles 
University in Prague, 19 – 20 years old on average, their study major was mostly 
Czech language on the Bc level. The test was administered in groups of 10 – 20 
people in the same sound-treated lecture room via tape recorder and loudspeakers. In 
case of the latest group (2006-2007) the digitized version of the test was used. 
Design of the Test. Three kinds of speech material were used in the test, all read by a 
female speaker with standard pronunciation. a) 25 numerals from 0 – 9 for 
familiarization with the task; b) 25 short sentences (questions and commands); c) 100 
isolated Czech words arranged in 5 subsets of 20 words; the task for the listeners was 
to write the word they hear. In all three parts of the test, five increasing levels of noise 
(L1 – L5) were used (see Table 1).  
Table 1. Levels of noise used in the perceptual test. SNR: Signal-to-noise ratio. 
Noise Level 
L1 
L2 
L3 
L4 
L5 
SNR (dB) 
+9 
+6 
+3 
0 
-3 
Subsets of 20 Words. The third part of the listening test built by subsets of isolated 
words provides the most valuable results of the experiment. The subsets of words 
were selected following these main criteria: a) All items are nouns in the nominative 
singular; b) The frequency of one half of the words belonged to the 1,000 range, the 
other half to the 10,000 range [10]; c) Each subset is balanced according to the word 
length in syllables (see Table 2); d) Each subset is relatively balanced according to the 
types of syllable structure. There are 9 types of CV combinations in the syllable (see 
also Table 9 in 2.2.). The choice was made in accordance with the syllable structure 
frequency given for Czech in [11]. These combinations within a syllable build 21 
different CV patterns on the word level. 
Table 2. Length of words in syllables and their total used in the perceptual test 
No of syll./word 
No of words in subset 
Total (5 subsets) 
1 syll. 
5 
25 
2 syll. 
7 
35 
3 syll. 
6 
30 
4 syll. 
1 
5 
5 syll. 
1 
5 
Total 
20 
100 
1.2   Success Rate of the Listeners in the Test in Overall 
The performance of listeners proves the good applicability of the test. The success score 
of the word identification on individual levels of noise is relatively stable, with a slight 

 
Perception of Czech in Noise: Stability of Vowels 
151 
tendency to the gradual worsening of the average score in more recent collections of 
data [comp. 12]. The average results for all 300 listeners in each subset of 20 words with 
increasing noise levels L1 – L5 is given in Table 3. According to the expectation the 
average number of errors continuously increases with increasing noise level. 
Table 3. Success rate of the listeners in the subsets of 20 words according to the noise level. 
Row 2: absolute number of errors. Row 3: percentage of errors within the subset of 20 words.  
1 
Noise level 
L1 
L2 
L3 
L4 
L5 
2 
Number of errors 
1.3 
2.6 
7.5 
9.1 
13.6 
3 
Number of errors in % 
6.5 
13.0 
37.5 
45.5 
68.0 
1.3   Resistance of the Word Sets in Noise in Overall 
The data discussed here are based on listeners’ evaluation of monosyllabic (W1), 
disyllabic (W2) and trisyllabic words (W3), in total 27 000 answers (90 items x 300 
listeners), see also Table 4-a. The longer words (W4 and W5) were omitted because 
of their limited number in the test. 
Main Parametres. Five main parameters were examined in the material: correctly 
identified words (Wid); no answer or incomplete answer (0); incorrect answer, but 
correctly determined number of syllables (nSid); incorrect answer and incorrectly 
determined number of syllables (xS); words used by listeners in their incorrect 
answers (word candidates, Cand). 
Table 4-a gives numbers of listeners’ judgements in the parameters mentioned 
above distinguishing the number of syllables in word (W1 – W3), Table 4-b gives the 
percentage related to the total of possible answers. Table 4-c shows the percentage of 
judgements considering the different noise levels L1 – L5. 
Results in Table 4 indicate two tendencies in relation between the number of 
correct answers, word length and noise level: a) The number of errors increases with 
increasing level of noise; relevant worsening starts on L3. b) The number of errors 
increases with decreasing length of word; relevant worsening is shown in the W1 
group. Fig. 1 shows both the relations together. 
The Most Stable Characteristics of Samples. In the conditions of the presented test 
the number of syllables in the word is a quality which proved to be relatively stable, 
i.e. consistently recognised by the listeners (see Table 4).  
The set of correct answers (Wid) and the incorrect answers with correctly 
determined number of syllables (nSid) forms a solid base for locating further features 
supporting the word recognition in noise. The total amount of Wid + nSid makes 
23,364 words and 48,112 syllables.  
In this material, the relation between the amount of correctly identified speech 
items and main structural features of their syllables were examined. According to our 
results, two other features seem to represent relatively stable factors in noise 
conditions in Czech: the quality of the nucleus and the differentiation in the open vs. 
closed syllable dichotomy (see also Table 10 in 2.2). It is vital to add that these global  
 

152 
J. Veroňková and Z. Palková 
Table 4. Listener’s judgements. Parametres examined: see paragraph Main parametres above. 
W1 – W3: number of syllabes in word. L1 – L5:  noise levels. 
4-a 
Wid 
0 
nSid 
xS 
Cand
4-c, %
Wid
0 
nSid 
xS Cand 
W1 
3489
740 
3067 
204 
692 
L1 
93 
1 
4 
1 
1 
W2 
7003
1255 1865 
377 
744 
L2 
87 
2 
11 
0 
3 
W3 
7079
832 
861 
228 
458 
L3 
61 
6 
31 
1 
7 
Total 17571 2827 5793 
809 
1894
L4 
52 
14 
30 
4 
10 
4-b, %  
  
  
  
  
L5 
33 
29 
31 
8 
15 
W1 
47 
10 
41 
3 
46 
W2 
67 
12 
18 
4 
35 
W3 
79 
9 
10 
3 
25 
 
Correct answers (Wid)
0
20
40
60
80
100
L1       L2         L3         L4        L5
Wid in %
W1
W2
W3
 
Fig. 1. Listeners’ judgements in overall: correctly identified words (Wid) in per cent 
distinguishing length of word in syllables (W1 – W3) and noise level (L1 – L5). 
 
features function equally in all the three mutually distant sets of responses according 
to the time period of the data collection. Potential changes in the language standards 
had no considerable influence on them. 
The overall data provide reasonable but only general assumptions on the topic in 
question. More detailed look shows noticeable large-scale variability in the listeners’ 
evaluation of single items. Searching the sources of this variability, the analyses of 
phenomena showing relative stability in our material could offer helpful hints. 
2   Stability of Nucleus 
In the set of the word items in our test, the vowels in their role as syllabic nuclei 
should be count among the best recognizable components. Differences in resistance of 
individual phonemes in noisy environment and trends in their substitutions are 
followed. The main attention is paid to the influence which the position in the word 
could eventually bring to the identification of vowels by the listener.  

 
Perception of Czech in Noise: Stability of Vowels 
153 
2.1   Differences in Identification of Individual Vowels 
For this part of analysis the amount of suitable data was selected as follows: 
a) All the phoneme realisations in the nucleus of all the syllables in nSid were 
found. Because of correctly identified number of syllables the set nSid enables to 
determine the direct correspondence between the syllables of the tested item and the 
syllables of the candidate. The number of correctly identified vowels (Nid) in material 
nSid and its relation to the amount of vocalic destinations made by listeners in total is 
used in the analyses.  
The further amount of accurately identified vowels offers then the set (collection) 
of correctly identified words (NWid). 
b) For this part of research, only data of monosyllabic (W1), disyllabic (W2) and 
trisyllabic words (W3) are used, obtained on noise levels L4 and L5 (see Table 1 in 1.1). 
The longer words (W4 and W5) were omitted as well as the words with nSid < 10. 
The analyses are limited to the data of noise levels L4 and L5 (without further 
distinguishing), since there are rare errors in the identification of vowels on the lower 
noise level.  
The delimited set contains 62 words, i.e. tested items (18,600 judgements). In this 
set seven different vowels and both liquids /r/ and /l/, which can be syllabic in Czech, 
create the nucleus. The frequency (F) of the single phonemes varies. It means the total 
of listeners’ judgements (J) is different for individual nuclei. See Table 5. 
Table 5. Identification of syllable nuclei. F – the frequency of the phoneme in nucleus in the 
tested items, J – the total of judgements, NWid  – the number of nuclei in correctly recognized 
words, nSid  – the incorrect answers with correctly determined number of syllables, Nid – the 
number of correctly identified nucleus within nSid. 
1 Nucleus 
a 
a: 
e 
i 
i: 
o 
u 
r 
l 
Tot 
2 Frequency (F) 
9 
7 
20 
6 
5 
11 
2 
1 
1 
62 
3 Judgements (J) 
2700 2100 6000 1800 1500 3300 600
300
300 18600 
4 NWid 
905 1264 2403 674
439 1297 378
31 
240 7631 
5 nSid 
755 403 2015 503
373 1079 166
158
25 5477 
6 Nid  
539 377 1531 246
275
843
92 
119
0 
4022 
7 NWid+Nid 
1444 1641 3934 920
714 2140 470
150
240 11653 
8 Nid in nSid (%) 
71.4 93.5 76.0 48.9 73.7 78.1 55.4 75.3
0 
73.4 
9 NWid+Nid in J (%)  53.5 78.1 65.6 51.1 47.6 64.8 78.3 50.0 80.0 62.7 
The rows 2-7 of the table show the data for the individual phonemes in the nucleus. 
Row 7 gives the number of correctly identified nucleus within nSid together with the 
number of NWid, i.e. the amount of correctly identified vowels in total.  
The rows 8-9 show the total of the correctly identified nucleus in per cent. Row 8 
shows correctly identified nuclei (Nid) within nSid (cf. also rows 5 and 6). Row 9 
shows correctly identified nuclei included nuclei in correctly identified words NWid 
(the basis is the total of the judgements). 

154 
J. Veroňková and Z. Palková 
The confusion matrix (Table 6) shows the more detailed data about the phonemes 
in nucleus observed by listeners and the direction of substitutions with respect to their 
correct identifications.  
The relative comparison of the stability of individual phonemes in the noise test 
conditions is based on the set of nSid, i.e. the set of incorrect answers with correctly 
determined number of syllables. 
In the inventory of Czech, there are 5 short and 5 long single vowels and 3 
diphthongs. In the delimited set of words examined in present discussion all the short 
Czech vowels appear, but only two of the long ones. That is why the differences in 
quantity could not be observed any closer. Table 6 presents in rows all the phonemes 
which appeared in all word-candidates suggested by listeners, i.e. both the quality and 
quantity are distinguished. In spite of this, Table 7 shows the identification of the 
nucleus with respect just to the quality of vowels, i.e. results of phonemes with the 
same quality but differentiated in quantity are processed together.  
According to the results presented in Table 6 the highest agreement in listeners’ 
observation of nuclei is the correct identification of all phonemes occurring in the 
analyzed part of our material. This trend remains even with elimination of differences 
of vowel quantity too (see Table 7). The only exception is the value obtained for /l/. 
The substitution of /l/ by /o/ showed the highest agreement (see also futher). 
These results confirm the relative stability of the vowel quality in the test. The 
direction of more frequented substitution is mostly based on the acoustic structure, 
especially the mutual substitution between the phonemes /u/ and /i/ which share F1. 
The higher stability of long phonemes comparing to short phonemes is not 
surprising. A more noticeable difference concerns the pair /i/ and /i:/. It reflects the 
fact that the members of this pair differ not just by the quantity but also to certain 
extent by the quality, a phenomenon unique in the system of pairs of vowels in Czech. 
The phoneme /i/ is a little bit more open than /i:/.  
Although we focus on the vowels, it seems worthwhile to comment on the different 
kinds of agreement of the syllabic sonorants /l/ and /r/. In the examined set nSid (the 
incorrect answers with correctly determined number of syllables), the nucleus /r/ was 
identified correctly with the highest agreement. On the contrary, the nucleus /l/ was 
not in nSid identified at all. But with respect to Wid (correctly identified words), the 
tested word containing /l/ in nucleus was recognized correctly as the whole with high 
agreement (cf. Table 4, rows 3 and 4, 8 and 9). This example shows the necessity to 
examine the results gained for individual phonemes in detail and from various points 
of view. 
Certain disadvantage appears due to the fact that the test being designed to screen 
the skills of listeners is not balanced enough with regard to the linguistic structure of 
its items. It can be presupposed that the identification of the vowels in the noise 
conditions is influenced not only by the noise level, the quality and quantity of an 
individual vowel phoneme, but also by the position of the vowel in the word and the 
consonantal context (see e.g. [13], [14]). The concrete consonantal environment in the 
tested items is very variable. However, there is the possibility to examine the role that 
the position in a word may play. 
 

 
Perception of Czech in Noise: Stability of Vowels 
155 
Table 6. The identification of nuclei and their substitutions (differences on both quality and 
quantity of vowels are considered) 
Sounds in nucleus 
 
% 
a 
a: 
e 
i 
i: 
o 
u 
r 
l 
 
 
a 
71.4 14.4 
4.5 
1.8
0.5
4.1
0.6
7.0
 
 a: 
2.4 84.6 
0.0 
 
1.3
1.3
 
 
 
e 
6.0
0.2 75.1 11.9
5.4
5.6
3.0
5.7
4.0
 e: 
 
  
1.3 
 
 
0.2
 
 
 
i 
4.9
0.2 
7.7 41.7
5.9
2.1 14.5
5.7 20.0
 i: 
4.2
0.2 
2.8 
7.2 71.3
3.7 10.2
3.8
4.0
 
o 
10.3
0.2 
7.0 16.5
0.3 78.3
1.2
 68.0
Substitutions 
 o: 
  
  
0.3
> 10% 
u 
0.7
  
0.2 15.3
1.9
2.2 55.4
1.3
4.0
a > o 
 u: 
0.1
  
0.0 
2.6 12.6
0.6
7.2
1.3
 
a: > a 
ou 
 
  
0.3 
 
0.5
0.2
 
 
 
i > e, o, u 
r 
 
  
0.6 
2.6
0.3
0.9
6.6 75.3
 
i: > u: 
Listener’s judgement 
l 
 
  
0.1 
0.4
 
0.5
1.2
 
 
u > i, i: 
Total 
100
100 100 100
100
100
100
100
100
l > o, i 
Table 7. The identification of nuclei and their substitutions with respect just to the quality 
Sounds in nucleus
% 
a 
e 
i 
o 
u 
r 
l 
 
 
a 
82.6 
4.6 
1.8
5.4
0.6
7.0
 
e 
4.0 76.5 
9.1
5.7
3.0
5.7
4.0
 
i
6.1 10.6 61.0
5.8 24.7
9.5 24.0
o 
6.8 
7.0 
9.6 78.6
1.2
68.0
Substitutions 
u
0.5 
0.2 16.4
2.8 62.7
2.5
4.0
> 10% 
ou 
 
0.3 
0.2
0.2
e > i 
r 
 
0.6 
1.6
0.9
6.6 75.3
i > u 
Listener’s judgement 
l 
 
0.1 
0.2
0.5
1.2
u > i 
Total 
100 100 100
100
100
100
100
   l > o, i 
2.2   The Recognizing of Vowels in the Relation to the Position in the Word 
The stressed syllable in many languages is supposed to be the most stable part of the 
word shape and the quality of the vowel is often evaluated as the most relevant stress 
bearer. The Czech language has got the so-called fixed word-stress, which falls on the 
first syllable, and there are no vowel reductions, i.e. its norm requires a stable form of 
vowels in all positions. Even in analyses of spontaneous speech, where natural 
reductions of speech sounds result from higher speech rate or casual pronunciation, 

156 
J. Veroňková and Z. Palková 
eventual deformations both in the vowel in the first syllable and any other syllable of 
a word can be found. 
In the description of the prosody of the Czech language, the unit commonly used at 
the word level is the stress-group or foot while the prominence is referred to as word 
stress. However, no regular pattern in prosodic properties bound to the stressed 
syllable has been proved [15]. The more realistic approach seems to base the unit 
delimitation on the linear course of the overall sound properties throughout the unit. 
Extensive perceptual experiments confirm the relevance of the F0 contour for the 
inner cohesion of the foot [16]. 
That is why it might be worthy to raise a question whether differences between 
identification of vowels forming the nucleus of the first syllable in the test words and 
identification of vowels in other syllables emerge. The possibility of such a result may 
be strengthened by the fact that the test words were read in isolation. 
The number of single vowels according to the position in a word differs in the tested 
material. Therefore, only three groups of data are distinguished in the following 
analyses: the data regarding monosyllabic words (W1) in separate, the data regarding 
vowels in the stressed syllable of di- and trisyllabic words collectively (INI), and the 
results regarding other syllables of di- and trisyllabic words are also presented 
collectively (Rest). For a correctly identified vowel we consider the vowel where its 
quality is correctly identified. Table 8-a presents the occurrence of a given vowel in a 
given position. Table 8-b presents the proportional share of correctly identified vowels 
in the set of substitutions (nSid), Table 8-c presents the share of all correctly identified 
vowels (Nid + NWid) in the overall number of possible judgements (the Fig. 2 presents 
the main facts from Table 8-c). 
Despite the varying number of vocalic phonemes in the distinguished positions it 
can be observed: 
a) The position of the vowel in the stressed (initial) syllable vs. non-stressed 
syllables does not have in the condition of our material the decisive influence on the 
identification of the vowel by the listeners. 
b) Comparing the identification of the vowel in the relation to its position, the 
results are distinctly different for single vowels. There are also some differences 
between the results obtained from the whole set of correct identification of nucleus 
(Nid + NWid, see Table-c) and the results obtained from the incorrectly identified 
words (nSid, see Table 8-b). In the set Nid + NWid (see Table 8-c), four of six vowels 
tested both in INI and Rest positions (all the listed vowels except /i/) are identified 
less successfully in the initial position.  On the other hand in the set nSid four vowels 
are identified more successfully in the initial position (but the differences are not as 
pronounced as in the set Nid + NWid). 
On the basis of the results presented above a conclusion can be made that next to 
the quality of a vowel, the features of segmental context in a word or a syllable are 
also influential and their influence seems to be stronger than that of the position of the 
syllable in a prosodic unit. 
In agreement with the above-mentioned hypothesis is the fact that vowels in 
monosyllabic words (W1) are identified better that the vowels in di- and trisyllabic words 
(both in comparison with initial and other positions (see Table 8-b and Table 8-c)), 
although the identification of a monosyllabic words as a whole (Wid) is less successful 
than that of the longer words (see Fig. 1 in 1.3). In monosyllabic words, mostly built by  
 

 
Perception of Czech in Noise: Stability of Vowels 
157 
Table 8. The identification of nucleus with respect to the position in a word 
8-a 
Frequency of nucleus (F) 
8-b Identification of nucleus (%): Nid in nSid 
V 
a á 
e 
i 
í o u Tot
V 
a 
á 
e 
i 
í 
o 
u 
Tot 
W1 1 1 5 1 0 1 0 
9 
W1 97
100
96
37
0 
100 
0 
90 
INI 3 4 4 0 3 6 1 21
INI
79
88 
66
0 
59
86 
58 
77 
Rest 5 2 11 5 2 4 1 31
Rest
52
99 
58
53
93
47 
49 
59 
 
 
 
 
 
 
 
 
 
Tot 71
94 
76
49
74
78 
55 
76 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
8-c 
Identification of nucleus in F (%): 
Nid in nSid + NWid 
 
 
 
 
 
 
 
 
 
V 
a 
á 
e 
i 
í 
o 
u 
Tot 
 
 
 
 
 
 
 
 
 
W1 74
91 
90
27
0 
96 
0 
87 
 
 
 
 
 
 
 
 
 
INI
73
71 
47
0 
58
58 
67 
61 
 
 
 
 
 
 
 
 
 
Rest
38
86 
61
56
32
67 
89 
56 
 
 
 
 
 
 
 
 
 
Tot 53
78 
66
51
48
65 
78 
63 
0
20
40
60
80
100
a          a:          e          i           i:         o          u           ø
% Nid
W1
INI
Rest
 
Fig. 2. The correct identification of vowels in the relation to the position in a word 
closed syllables in our test, the narrow consonantal context can help to the vowel 
identification, although the consonants themselves are confused. 
With regard to the structure of syllable from the point of view open vs. closed may 
play an important role. Table 9 shows the representation of the syllable structures in 
the material discussed. The monosyllabic words are built just by closed syllables, on 
the other hand, in case of di- and trisyllabic words the least number of closed syllables 
occurs in the INI position. 
Analysing the whole set of incorrectly identified words with correctly identified 
number of syllables (nSid), the difference of the kind of syllable from the point of 
absence vs. presence of coda becomes evident as one of the most stable characteristics 
within the material (see Table 10). Number of consonants or their quality shows a 
smaller resistance to noise. However, it is possible that the transition of the vowel to 
coda supports the identification of the vowel because of its relative stability. 

158 
J. Veroňková and Z. Palková 
Table 9. Syllable structures according to CV combination used 
Comb. CVC CCVC CCVCC CCCVC
CVCC CCV V
CV CCCV Total 
W1 
1 
4 
3 
2 
  
  
  
  
  
10 
INI 
1 
1 
  
  
  
7 
2
8 
2 
21 
Rest 
14 
1 
  
  
2 
2 
  
12 
  
31 
Table 10. The identification of open vs. closed syllable 
% 
Correct answers 
L4 and L5
Open syll. Closed syll.
W1 
  
95 
W2 
83 
81 
W3 
89 
53 
3   Individual Differences in Evaluation 
The overall results of the listening test suggest the most successful recognition of a-
vowels and the least successful recognition of i-vowels (Tables 5-8). When 
comparing the identification of particular words, however, there are notable 
differences in recognition of various occurrences of individual vowel phonemes. At 
the same time it is not possible to find a regular tendency in the relation of the 
differences to the acoustic characteristics of these concrete realisations. 
3.1   Occurrences of the Phoneme /e/  
For illustration we present a detailed analysis of the occurrences of the phoneme /e/ 
which is amply represented in our material. We will focus on the set of examples W2 
and W3 on the noise levels L4 and L5. To examine the mutual relation of the 
recognition of vowels to two acoustic features we use the coefficient of rank-
correlation rs (Spearman) whose significance can be tested with t-test (Student).  
Regarding the listeners’ evaluation, both the relation of correct identifications in 
nSid and the relation of all correct identifications (Nid + NWid) to the whole volume 
of judgements are applied. In addition to the results for all occurrences of /e/, the data 
for the subset Rest are presented separately. Concerning the acoustic characteristics, 
the duration of vowels (t) and the formant structure by means of the ratio F2/F1 were 
examined (measurement made using [17]). 
The results in Table 11 show that in all the ways of processing, the values of 
testing characteristics t are notably lower than the critical values on the level of 
significance 0.05. The interdependence between the evaluation of the vowels and the 
examined acoustic parameters cannot be assumed. This result is in accordance with 
the above mentioned finding that the first, structurally stressed syllable, is not 
grounded on acoustic characteristics of its own in Czech.  
Two occurrences of the phoneme /e/ in the first syllable of disyllabic words (zřetel 
and květen) can serve as an illustration of individual differences between specific 
items. Both vowels have very similar structural features as well as acoustic features 
but significantly different recognizability for listeners. See Table 12. 

 
Perception of Czech in Noise: Stability of Vowels 
159 
Table 11. Correlation between the correct identification and acoustic characteristics. J – the 
total of judgements, NWid  – the number of nuclei in correctly recognized words, nSid  – the 
incorrect answers with correctly determined number of syllables, Nid – the number of correctly 
identified nucleus within nSid, N1 – N3 – the position of the nucleus within the word (N1 – in 
the 1st syllable etc.). 
/e/: W2+W3, 
L4+L5 
 
 
n 
rs 
t-test 
sign. 
0.05 
Relation to t 
% in nSid 
N1+N2+N3 
15 
0.30 
1.12 
2.16 
Relation to t 
% in nSid 
N2+N3 
11 
0.36 
1.16 
2.26 
Relation to t 
% Nid+NWid in J
N1+N2+N3 
15 
0.19 
0.71 
2.16 
Relation to t 
% Nid+NWid in J
N2+N3 
11 
0.36 
1.15 
2.26 
Relation to F2/F1 
% in nSid 
N1+N2+N3 
15 
-0.21 
-0.78 
2.16 
Relation to F2/F1 
% in nSid 
N2+N3 
11 
-0.47 
-1.60 
2.26 
Relation to F2/F1 % Nid+NWid in J
N1+N2+N3 
15 
-0.38 
-1.49 
2.18 
Relation to F2/F1 % Nid+NWid in J
N2+N3 
11 
-0.29 
-0.54 
2.26 
Table 12. Phoneme /e/ in N1 in two examples 
 
N1
Dur (s) 
F2/F1 
(Hz) 
Wid 
nSid
Nid 
% Nid 
in nSid 
%Nid+NWid 
in tot  
květen 
e 
0.092 
4.5 
172 
60 
58 
97 
77 
zřetel 
e 
0.096 
4.2 
59 
67 
34 
51 
31 
3.2   A Comment as a Contribution to Discussion 
The phoneme /e/ in the material W2 + W3 is represented by only four occurrences in 
the first syllable. Therefore the position INI can not be put into a separate subset. Only 
indirect speculation may be possible, e.g. by comparing the results of all cases N1-3 to 
the results of the subset Rest (N2-3). At the same time it may be worthwhile to 
examine similarities or differences between the results obtained by means of two 
different evaluations of the correctly identified vowels in the material. 
In the part dedicated to the relation between the identification of vowels and their 
duration, the subset Rest shows approximately same results both in the material nSid 
and Nid + NWid. The results of subset N1-3 differs in this regard. In the processing of 
Nid + NWid material they show even looser relation to the duration of vowels than in 
the complex nSid. Differences of similar kind can be also found in the values related 
to formant ratio, but these show different relations. The loosest relation to the timbre 
of vowels is suggested by the processing of nSid while using the whole set of W1-3. 
It needs to be pointed out that all the above mentioned relations leading to 
particular aspects of acoustic structure remain under the level of statistic relevance. 
However, a closer look at individual differences brings about a valuable point of view 
that might be taken in consideration in future. The processing of data using the sum 
Nid + NWid might be influenced by the information resulting from the correct 
identification of a word as the whole. The processing in the framework of the material 
nSid may also comprise such information, but the alternate words do not in most cases 

160 
J. Veroňková and Z. Palková 
occur frequently, cf. the number of candidates in Table 4 in 1.3. It might be implied 
that acoustic characteristics will assert themselves in a more conspicuous way in the 
material nSid. In this regard it is not useless to further examine the vowels in their 
initial position. Once again it has been confirmed that successful perception in 
adverse conditions can be influenced by more factors which may be mutually 
independent, or which may complement one another, or on the contrary which may be 
in competition to one another. See e.g. [4], [14], [18], [19]. 
4   Conclusion and Perspectives 
The material of the test examining the skills of listeners to recognise samples of 
speech in noise provided us with an opportunity to also gain certain information about 
features of the used material. In the conditions of the experiment, in the set of isolated 
Czech words the most stable features are those of the number of syllables in a word, 
the quality of a syllable nucleus and the dichotomy open/closed syllable. 
The study focused on the issue of preservation of a vowel as the syllable nucleus 
and its eventual substitution. As far as the differences among individual phonemes are 
concerned considering the success in their identification, there are certain tendencies 
which, however, are not dominant regarding the success in the recognition of whole 
words. Out of vowel phonemes in Czech, the vowel /a:/ is relatively best preserved 
and the vowel /i/ is the least easy to be preserved. 
During the analysis the question was examined whether the vowel in the stressed 
(the first) syllable of a polysyllabic word preserves in adverse conditions of speech 
communication its quality better than vowels in other syllables. The results did not 
show this difference and they confirmed the assumption that the quality of vowels in 
Czech is not actually influenced by their position in a word. 
The difference between the results for vowels in the first syllable of polysyllabic 
words and for the other vowels were examined from the point of view of their relation 
to the duration and formant structure F2/F1. Even here the result is negative, and 
again in accordance with the assumption that the so called word stress is mainly the 
phenomenon of a structural description and the rhythmical unit on the word level in 
Czech is not primarily characterised by acoustic prominence of the first syllable. 
Identification of vowels on the side of listeners distinctly varies even among 
particular occurrences of individual phonemes. It needs to be taken in consideration 
that these differences do not have a verifiable relation to particular acoustic (e.g. 
duration or spectrum) or structural feature (position in a stressed syllable). In some 
cases an influence of morphological or semantic aspects can be felt. 
The experience gained from the analysis of our material confirms the earlier 
stipulated assumption that in recognition of the acoustic word unit in adverse 
conditions a complex of features is in operation where these can act independently or 
in cooperation or in competition. The results of this research may perhaps enable to 
continue the examination of their mutual interaction when an appropriate choice of 
language material is made. 
Acknowledgement. The research is supported by VZ MSM 0021620825 and the 
European Union grant MRTN-CT-2006-035561. 

 
Perception of Czech in Noise: Stability of Vowels 
161 
References 
1. Assmann, P., Summerfield, Q.: The Perception of Speech under Adverse Conditions. In: 
Greenberg, S., et al. (eds.) Speech Processing in the Auditory System. Springer, Heidelberg 
(2004) 
2. Barker, J., Cooke, M.: Modelling Speaker Intelligibility in Noise. Speech Communication 49, 
402–417 (2007) 
3. Dees, T.M., et al.: Effects of Noise on Lexical Tone Perception by Native and Non-native 
Listeners. In: Proceedings of 16th ICPhS, pp. 817–820. Saarbrücken (2007) 
4. Lecumberri, M.L., Cooke, M.: Effect of Cross-word Context on Plosive Identification in 
Noise for Native and Non-native Listeners. In: Proceedings of 16th ICPhS, pp. 205–210 
(2007) 
5. Lecumberri, M.L., Cooke, M.: Effect of Masker Type on Native and Non-native 
Consonant perception in Noise. Journal of the Acoustical Society of America 119(4), 
2445–2454 (2006) 
6. Alwan, A., Lo, J., Zhu, Q.: Human and Machine Recognition of Nasal Consonants in 
Noise. In: Proceedings of 14th ICPhS, San Francisco, pp. 167–170 (1999) 
7. Chen, W., Alwan, A.: Perception of the Place of Articulation Feature for Plosives and 
Fricatives in Noise. In: Proceedings of 15th ICPhS, vol. 2, pp. 1497–1500. UAB, 
Barcelona (2003) 
8. Meyer, G.F., Morse, R.P.: The intelligibility of Consonants in Noisy Vowel-Consonant-
Vowel Sequences when the Vowels are Selectively Enhanced. In: Proceedings of 15th 
ICPhS, pp. 2313–2316. UAB, Barcelona (2003) 
9. Janota, P., Palková, Z.: The Analysis of Erroneous Identifications of Speech Segments 
under Noise-test Conditions. In: Janota, P., Schulzová, O. (eds.) AUC, Philologica 3, 
Phonetica Pragensia VII, pp. 43–66. Univerzita Karlova, Praha (1985) 
10. Jelínek, J., Bečka, J.V., Těšitelová, M.: Frekvence slov, slovních druhů a tvarů v českém 
jazyce. Praha (1961) 
11. Kučera, H., Monroe, G.K.: A Comparative Quantitative Phonology of Russian. Czech and 
German, New York (1968) 
12. Palková, Z., Janota, P.: Testing Speech Perception in Noise. In: 32nd Czech Conference on 
Acoustics. Speech – Music – Hearing, pp. 55–58. VÚZORT, Praha (1995) 
13. Miller, G.A., Heise, G.A., Lichten, W.: The Intelligibility of Speech as a Function of the 
Context of the Test Materials. Journal of Experimental Psychology 41, 329–335 (1951) 
14. Warren, R.M.: Processing of Speech and Other Auditory Patterns: Some Similarities and 
Differences. In: Ainsworth, W., Greenberg, S. (eds.) Proceedings of the Workshop on the 
Auditory Basis of Speech Perception, pp. 226–231. Keele University, United Kingdom (1996) 
15. Palková, Z.: Intonatorische Merkmale in der Perzeption der Wortgrenzen im Satz. In: 
Proceedings of the 11th ICPhS, vol. 1, pp. 296–299. ASE S. S. R., Tallin (1987) 
16. Palková, Z., Volín, J.: The Role of F0 Contours in Determining Foot boundaries in Czech. 
In: Proceedings of 15th ICPhS, vol. 2, pp. 1783–1786. UAB, Barcelona (2003) 
17. Boersma, P., Weenink, D.: Praat, doing phonetics by computer - 4.3.,  
  http://www.praat.org 
18. Nguyen, N., Hawkins, S.: Implications for Word Recognition of Phonetic Dependencies 
between Syllable Onsets and Codas. In: Proceedings of 14th ICPhS, San Francisco, pp. 
647–650 (1999) 
19. Felty, R.: Confusion Patterns and Response bias in Spoken Word Recognition of German 
Disyllabic Words and Nonwords. In: Proceedings of 16th ICPhS, pp. 1957–1960. 
Saarbrücken (2007) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 162–172, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Challenges in Segmenting the Czech Lateral Liquid 
Radek Skarnitzl 
Institute of Phonetics, Charles University in Prague 
radek.skarnitzl@ff.cuni.cz 
Abstract. The study is part of a larger project focused on defining criteria for 
manual segmentation. Liquids are typically the most problematic sounds in this 
respect. We examine various acoustic correlates of the Czech alveolar lateral 
liquid /l/ and their role in isolating this sound in the acoustic stream. Relative 
formant intensity turned out to be the most reliable criterion. The results 
indicate a surprisingly high contribution of F3, whose intensity is often stronger 
in than in neighbouring vowels. This appears to be a new finding. 
Keywords: segmentation, Czech, lateral, liquid. 
1   Introduction 
Phonetic investigations, whether of segmental or suprasegmental nature, require a 
large amount of annotated and segmented speech material. Unfortunately, automatic 
or semi-automatic tools designed to facilitate segmentation are not accurate enough 
for the purposes of phonetic research [1], [2], [3]. Manual correction of HMM-
generated segmentation is therefore necessary. 
Purely manual segmentation is known to be time-consuming and demanding with 
respect to labeller expertise. Moreover, it has been criticized as inherently subjective. 
Our previous experiments have shown, however, that relatively simple segmentation 
guidelines can considerably reduce the subjective element (e.g., [4], [5]). The 
introduction of guidelines for the boundary placement of intervocalic plosives, 
fricatives, and nasals led to very low inter-labeller deviations of 2-3 milliseconds. 
This compares very favourably with results reported in literature: for example, [6], 
who also analyzed inter-labeller agreement with respect to individual boundary types, 
report mean deviations between 6 and 12 milliseconds for these segments. 
Similar endeavours at specifying segmentation criteria have been pursued before 
(cf. the Buckeye corpus ([7], [8]). However, their segmentation instructions are rather 
crude, do not contain visual (spectrogram) examples, and do not seem to account for 
implicit pronunciation. Our guidelines try to cover the variability in speechsound 
production. It should be pointed out that our attempts do not aspire at specifying the 
precise location of a boundary between two speechsounds. The objective is to create 
easy-to-follow guidelines, which will allow for a uniform segmentation approach of a 
greater number of labellers. When specifying the guidelines, we try to make use of 
phonetic features - articulatorily and perceptually motivated - which are inherent to 
the individual speechsounds or speechsound classes (see [4] for more detail). 

 
Challenges in Segmenting the Czech Lateral Liquid 
163 
1.1   The Lateral Liquid in Czech 
Laterals are defined as sounds which are articulated with a closure along the mid-
sagittal line of the oral cavity with the sides of the tongue lowered, so that air can 
flow along them (or along one of them). In most languages of the world, lateral 
speechsounds are alveolar or dental [9]; the same applies for the Czech /l/, which is 
defined as a voiced lateral alveolar liquid (e.g., [10: 232]). Acoustic data on Czech /l/ 
is very rare, with the exception of [11] who deal mostly with formant frequencies. 
Liquids in general are known to be problematic from the segmentation viewpoint. 
First, as they frequently undergo coarticulation [12], their acoustic qualities “seep” 
into neighbouring sounds, making the boundaries indistinct. Second, they are often 
characterized by high acoustic variability. This variability can be universal: for 
instance, considerable variability is to be expected in the 2.5-4 kHz region [13: 547]. 
In addition, some degree of variability may exist within the language itself. In 
Czech, the last two decades have seen quite a dramatic increase in the production of 
velarized [lÚ] [14]. In extreme (but documented) cases, the alveolar contact can be lost, 
leading to the vocalization of /l/ (see, e.g., [15] for examples from English). 
These sources of variability make the segmentation of the Czech /l/ quite a 
daunting task. It is very difficult to specify segmentation criteria which would be 1) 
comprehensive (i.e., which would apply for all representations of /l/) and 2) 
hierarchical (i.e., acoustic cue A will stand higher than cue B etc.). 
The aim of this study is to try to define criteria for the placement of boundaries of 
Czech /l/ in all possible contexts, and to see how useful these criteria turn out to be. 
Naturally, such criteria may be useful for segmentation in other languages with /l/. 
2   Method 
The speech material which we used for defining the criteria for boundary placement 
was drawn from two corpora. First, we used the GAP corpus, which comprises 
recordings of 75 Czech university students who were asked to read a short story of 
nine sentences (see [3] for more detail). The speakers had been evaluated as to their 
overall speaking performance and divided into equal groups good (G), average (A), 
and poor (P). Assuming some of the quality would be reflected on the segmental 
level, we hoped to obtain a large variety of /l/-items and thus to capture a lot of the 
variability in the production of /l/. The drawback of the GAP corpus is the fact that /l/ 
only appears nine times in the text (675 items in total), and the segmental context of it 
was quite limited. 
The second sample was taken from news bulletins read by two female newsreaders 
of the public station Czech Radio, each approximately four minutes long. This 
sample, which contains 279 items of /l/, is richer as far as segmental context is 
concerned, but comparable in quality (as is to be expected from newsreaders of a 
public broadcaster). For that reason, both corpora were used to determine the criteria 
for boundary placement. Praat was used for labelling [16]. 

164 
R. Skarnitzl 
3   Acoustic Cues of Intervocalic /l/ 
The first objective was to search for inherent features of /l/ which set it apart from 
neighbouring vowels, for acoustic cues which would produce as high an acoustic 
contrast between vowels and /l/ as possible. These cues are discussed in this section. 
All boundaries are placed at zero crossings. 
We first focused on formant frequencies. However, due to the high variability in 
the production of /l/, these were not very helpful for the purposes of segmentation. 
The second acoustic cue which tends to be mentioned in manuals of acoustic 
phonetics is the antiformant (or more precisely, a pole-zero pair), the result of the 
bifurcation of the air stream in the oral cavity. The antiformant tends to appear 
between 2 and 3 kHz, depending on the length of the side branch. Figure 1 shows an 
intervocalic /l/ with the antiformant clearly visible as a very light stripe around 2 kHz. 
Obviously, there are more precise ways of pole-zero estimation, but not readily 
available for annotators who work in, for example, Praat. 
 
Fig. 1. Spectrogram of [ala] with a clearly visible antiformant in the [l] around 2 kHz 
The presence of an antiformant typically co-occurs with lower intensity in the 
spectrum above the antiformant [17]. Theoretically, then, the contrast in high-
frequency intensity could be exploited for isolating /l/ from neighbouring vowels. In 
Figure 1, the contrast is not apparent, but Figure 2 shows an illustrative example of 
intensity contrast in the upper frequency range: spectral amplitude is visibly lower in 
frequencies above approximately 4 kHz in the segment identified as /l/. The lower 
intensity of high frequencies in /l/ has been associated with the presence of the 
antiformant. However, we found that the intensity drop may occur even without any 
antiformant visible throughout the /l/-sound. That is why we treat the presence of the 
antiformant and lower intensity in high frequencies as separate acoustic cues. 
Apart from the spectrogram, /l/ may be manifested by some changes in the shape 
of the waveform. We have found that the waveform may contribute to isolating /l/ 
from the acoustic stream in two ways. First, lower intensity of high frequencies might 
mean that the waveform will be “simpler” in shape compared to the neighbouring 

 
Challenges in Segmenting the Czech Lateral Liquid 
165 
vowel (see Figure 3). Second, the periods corresponding to /l/ in the waveform may 
have lower amplitude than in the neighbouring speechsound (Figure 4). In both 
Figures 3 and 4, the spectrogram cues are also available. 
 
Fig. 2. Spectrogram of [alE] with a marked intensity drop in high frequencies 
 
Fig. 3. Waveform and spectrogram of [Ili˘] illustrating the “simplification” of the waveform 
Continuing with the focus on the waveform, it appears that sometimes, and it need 
not be a case of extremely explicit pronunciation, the release phase of the /l/ is visible. 
Obviously, this will greatly facilitate the specification of the right boundary of the 
speechsound. While the aforementioned acoustic cues appeared in all segmental 
contexts, the plosion seems to be limited to cases when the following vowel is [I] or 
[i˘]. In Figure 5, we see an exceptionally strong plosion corresponding in frequency to 
an alveolar sound. The release (if visible at all) is usually weaker, though. 

166 
R. Skarnitzl 
 
Fig. 4. Waveform and spectrogram of [Ela] illustrating a dip in the amplitude in the waveform 
 
Fig. 5. Spectrogram of [alI] with a strong plosion with a typical frequency distribution of an 
alveolar sound 
The examination of our spectrograms led to the discovery of another acoustic cue 
which appeared quite frequently, but whose existence we had not been aware of from 
literature. It has been mentioned that high frequencies above approximately 2 kHz are 
often weaker in the lateral sound than in neighbouring vowels. However, we 
frequently found F3 to be stronger than in the neighbouring vowels. Figure 6 shows 
a good example of this. 
Naturally, differences in formant intensity may concern not only F3, but also 
other formants. Most of the times, the given formant will be weaker in /l/, as 
illustrated for example by a weaker F2 in Figure 7. In section 5 below, formant 
intensity will be considered as a whole group of cues. However, we will also keep the 
information relating to stronger F3 separately, as this cue appears to be new (although 
see discussion in section 6). 

 
Challenges in Segmenting the Czech Lateral Liquid 
167 
 
Fig. 6. Spectrogram of [ola] with a strong F3 relative to the neighbouring vowels 
 
Fig. 7. Spectrogram of [ElE˘] with a weaker F2 in /l/ 
So far we have been essentially relying on visual inspection of spectrograms, 
without explicitly referring to what may seem to be the most important aspect of 
manual labelling, namely auditory analysis of the speechsounds.  
Since we have mentioned that /l/ belongs to sounds which pose a challenge for 
labellers, it is not surprising that sometimes we will have to resort to what we may 
call “cross-modal” analysis of the speech signal. It is especially in cases of implicit 
pronunciation that the acoustic cues described above may not be available at all, or 
may not be sufficiently robust to allow reliable placement of one or both boundaries 
of /l/. Figure 8 shows the waveform and spectrogram of one such item. Although we 
cannot hear differences of a few milliseconds around the transition between two 
speechsounds - that is why we try to formulate segmentation rules mostly on the 
visual basis - we have to fall back on listening in examples like this. 
To summarize this section, /l/ in the intervocalic position may be distinguished 
from the neighbouring vowels with the help of several acoustic cues: the presence of 
the antiformant slightly above 2 kHz, lower intensity in higher frequencies, different 
formant intensity, changes in the shape of the waveform, and the visibility of the  
 
 

168 
R. Skarnitzl 
 
Fig. 8. Waveform and spectrogram of [olE] in which visual cues are not sufficient for reliable 
boundary placement 
release phase. Obviously, we can often exploit more acoustic cues in a given item 
than only one. 
4   Acoustic Cues of /l/ Preceded by a Voiceless Obstruent 
All the acoustic cues mentioned in section 3 for intervocalic /l/ also apply in clusters 
with voiced sounds. However, /l/ often behaves quite differently in items when it is 
preceded by a voiceless obstruent, especially when this obstruent is also alveolar. In 
these contexts, /l/ may become partially or even completely devoiced. 
In [sl8] clusters, we are essentially talking about a combination of two alveolar 
fricatives, differing only in stridency, which should be manifested as a difference in 
the intensity of the noise (see Figure 9). It may be necessary to view frequencies 
above 5 kHz. 
The cluster /sl/ may be realized with an epenthetic /t/-like sound, due to a slightly 
different overlap of articulatory gestures. As this is essentially a parasitic sound, the 
plosions is not marked as a segment; we decided to classify it as part of the fricative 
sound, as shown in Figure 10. 
The real challenge for labeller comes in [tl8] clusters. Here, the plosive is very often 
released laterally (that is, the alveolar closure is preserved, and the sides of the tongue 
are lowered). As is illustrated in Figure 11, it may be very difficult to specify where 
exactly the release changes into voiceless lateral noise. Therefore, in order to facilitate 
consistent and straightforward segmentation, we decided to treat the onset of formant 
structure as the real beginning of /l/. The voiceless part of /l/ is thus marked as part of 
the plosive. 

 
Challenges in Segmenting the Czech Lateral Liquid 
169 
 
Fig. 9. Spectrogram of [sl8o] with a clear boundary between the two types of noise 
 
Fig. 10. Spectrogram of [sl8u] with an epenthetic plosion 
 
Fig. 11. Spectrogram of [tl8a] with the boundary set to the beginning of formant structure 
5   Reliability of Acoustic Cues 
Having defined as many acoustic cues which could contribute to the isolation of /l/ in 
the speech signal as possible, we wanted to see how reliable these cues are, in other 

170 
R. Skarnitzl 
words, how often and to what extent they may be exploited when labelling. The 
following data will refer only to those items of /l/ which are not preceded by a 
voiceless obstruent. 
The contribution of all the cues mentioned in section 3 was rated as 
• 1 when the given acoustic cue allowed for clear separation of /l/ from the 
neighbouring speechsounds; 
• 0 when the given acoustic cue was not available; 
• 0.5 when the acoustic cue was weak, or when it indicated the boundary of /l/ on 
one of its sides only. 
The results for all the instances of /l/ analyzed in this study are summarized in Figure 12. 
As we can see, although the presence of an antiformant is an integral part of all 
acoustic descriptions of /l/ in the relevant literature, in our corpora it turned out to be 
one of the least stable cues: the antiformant could be exploited for segmentation 
purposes only in 26 % of all /l/ items. 
It may be perhaps expected that the most robust acoustic cue is the relative 
intensity of formants - this composite cue could be used in 81 % of all cases. What I 
consider to be the most interesting result of this study, however, is how reliable the 
stronger F3 is in indicating the boundaries of /l/: F3 was stronger than in the 
neighbouring vowel in more than one third of all the analyzed cases. 
Interestingly, lower intensity in higher frequencies appears to be more robust than 
the presence of the antiformant (which should, theoretically, induce the low intensity 
above it). 
antiformant
reliability of acoustic cues
0.0
high
frequencies
relative
formant
intensity
waveform
release
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.44
0.26
0.81
0.36
0.27
0.11
 
Fig. 12. Reliability of the analyzed acoustic cues. The bars indicate how often a given cue 
could be exploited for segmentation. 
 

 
Challenges in Segmenting the Czech Lateral Liquid 
171 
The data in the chart indicate that we cannot ignore changes in the waveform - they 
turned out to be helpful in segmentation more than one fourth of the items. Given the 
fact that we are talking about a sonorant sound, the contribution of a visible release is 
also relatively strong. 
In 14 % of all items, it was necessary to adjust the boundaries of the target 
speechsound with the help of listening. 
6   Discussion and Conclusions 
The aims of this study were to summarize all possible acoustic cues which may 
contribute to the segmentation of /l/ in Czech, and to see to what extent these cues are 
actually useful when segmenting. 
The study shows that in connected speech, a visible occurrence of the antiformant 
is comparatively rare, while relative differences in the intensity of formants can be 
used most frequently. 
We consider the high contribution of F3 to be the most interesting result of the 
present study. Its intensity is often significantly higher than in the neighbouring 
sounds. Ladefoged and Maddieson mention that F3 typically has “a relatively strong 
amplitude” ([9: 193]). However, I have not come across a finding saying that F3 is 
actually stronger than in the neighbouring vowels. Stevens’ modelling in [13] nor real 
data indicate such a change in formant intensity, quite the contrary. 
We might find a possible explanation for the stronger F3 in the articulation of /l/ 
itself. If we look at the vocal tract during the articulation of /l/ (see Figure 13), the 
tongue is pulled forward quite a lot. As F3 is the second resonance of the back cavity, 
this articulatory gesture expands the space in which F3 resonates, which may lead to 
relative strengthening of F3. 
 
Fig. 13. Articulation of the Czech lateral liquid (cf. [18]) 
By way of conclusion, I would like to point out once again that this research is part 
of a concentrated effort at defining guidelines for persons who want to have some 
basis for their phone segmentation. The title of this paper includes the word 
“challenges”. I believe that /l/ indeed is one of the most challenging speechsounds for 
labellers. Perhaps, this study has showed that it is possible to meet this challenge. 

172 
R. Skarnitzl 
Acknowledgement 
This research was supported by the grants VZ MSM 0021620825 and MRTN-CT-
2006-035561 (European grant, Sound to Sense). 
References 
1. Pauws, S., Kamp, Y., Willems, L.: A hierarchical method of automatic speech segmentation 
for synthesis applications. Speech Communication 19, 207–220 (1996) 
2. Kominek, J., Bennett, C., Black, A.W.: Evaluating and Correcting Phoneme Segmentation 
for Unit Selection Synthesis. In: Proceedings of Eurospeech 2003, pp. 313–316. ISCA, 
Geneva (2003) 
3. Volín, J., Skarnitzl, R., Pollák, P.: Confronting HMM-based Phone Labelling with Human 
Evaluation of Speech Production. In: Proceedings of Interspeech 2005, pp. 1541–1544. 
ISCA, Lisbon (2005) 
4. Machač, P., Skarnitzl, R., Volín, J.: Inter-labeller agreement in segmental boundary 
placement. In: Vích, R. (ed.) 17th Czech-German Workshop - Speech Processing, pp. 57–61. 
ASCR, Prague (2007) 
5. Volín, J., Skarnitzl, R., Machač, P., Janoušková, J., Veroňková, J.: Reliabilita a validita 
popisných kategorií v Pražském fonetickém korpusu. In: Kopřivová, M., Waclawičová, M. 
(eds.) Čeština v mluveném korpusu, pp. 249–254. Nakladatelství Lidové noviny / Ústav 
českého národního korpusu, Praha (2008) 
6. Wesenick, M.-B., Kipp, A.: Estimating the quality of phonetic transcriptions and 
segmentations of speech signals. In: Proceedings of ICSLP 1996, pp. 129–132. ISCA, 
Philadelphia (1996) 
7. Pitt, M., Johnson, K., Hume, E., Kiesling, S., Raymond, W.: The Buckeye corpus of 
conversational speech: labeling conventions and a test of transcriber reliability. Speech 
Communication 45, 89–95 (2005) 
8. Kiesling, S., Dilley, L., Raymond, W.D.: The Variation in Conversation (ViC) Project: 
Creation of the Buckeye Corpus of Conversational Speech (2008),  
  http://buckeyecorpus.osu.edu/BuckeyeCorpusmanual.pdf 
9. Ladefoged, P., Maddieson, I.: The sounds of the world’s languages. Blackwell, Oxford 
(1996) 
10. Palková, Z.: Fonetika a fonologie češtiny. Karolinum, Praha (1994) 
11. Borovičková, B., Maláč, V.: The spectral analysis of Czech sound combinations. Academia, 
Praha (1967) 
12. Recasens, D., Fontdevila, J., Pallares, M.D.: Linguopalatal coarticulation and 
alveolarpalatal correlations for velarized and nonvelarized /l/. Journal of Phonetics 24, 
165–185 (1996) 
13. Stevens, K.: Acoustic phonetics. MIT Press, Cambridge (1998) 
14. Volín, J.: Čtyři scénáře vývoje české laterály. Čeština doma a ve světě 1(2002), 7–13 
(2002) 
15. Sproat, R., Fujimura, O.: Allophonic variation in English /l/ and its implications for 
phonetic implementation. Journal of Phonetics 21, 291–311 (1993) 
16. Boersma, P., Weenink, D.: Praat, version 5.0.27 (2008), http://www.praat.org 
17. Fant, G.: Acoustic Theory of Speech Production. Mouton, The Hague (1960) 
18. Polland, B., Hála, B.: Artikulace českých zvuků v roentgenových obrazech (skiagramech). 
Česká akademie věd a umění, Praha (1926) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 173–181, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Implications of Acoustic Variation 
for the Segmentation of the Czech Trill /r/ 
Pavel Machač 
Faculty of Philosophy and Arts, Charles University, Prague 
pavel.machac@ff.cuni.cz 
Abstract. The Czech alveolar sonorant trill /r/, like liquids generally, 
constitutes a challenge from the point of view of locating its boundaries in the 
acoustic stream. As it is desirable to label and segment a phonetic corpus 
uniformly and also to facilitate a high degree of inter-labeller agreement, rules 
for specifying speechsound boundaries should be unambiguous and as 
straightforward as possible. In this study, we examined various acoustic forms 
of Czech /r/ – from the trill and a flap to strongly reduced instances – and their 
implications for segmentation. The above-mentioned requirements resulted in 
the necessity to treat the segmentation of intervocalic items of /r/ differently 
from items occurring in consonant clusters.  
Keywords: segmentation, segment boundary, Czech /r/, trill. 
1   Introduction 
Much phonetic research tends to be based on the acoustic analysis of a large, labelled 
speech material. We need to divide the acoustic signal into speech segments for many 
different research purposes, for example when examining temporal properties of 
speech sounds, acoustic changes in their microstructure, or their resistance to changes 
of articulation rate. 
However, it is evident that an “accurate” placement of segment boundary is often 
an illusion. The goal of our research is therefore not to determine an exact place 
where a segment begins and ends, because differences in the timing of glottal and 
supraglottal activities, in the mobility of individual articulators, or in articulatory 
effort lead to more or less smooth changes in the acoustic signal. The ultimate 
objective is to formulate concise and straightforward criteria for phone segmentation, 
respecting inherent phonetic features of the speechsounds (see section 2.2), which will 
allow consistent manual labelling to be conducted by a larger number of annotators. 
The topic of this paper concerns the Czech alveolar sonorant trill /r/ from the point 
of view of locating its boundaries in the acoustic stream. This is part of a broader 
project, whose first results have already been presented (see below), which should 
eventually result in guidelines for setting boundaries of speechsounds in connected 
speech. Although we focus on the segmentation of the Czech /r/, the results of this 
study will be relevant for segmentation in any language where /r/ is a trill. /r/ sounds 
of an approximative or fricative character would require different criteria. 

174 
P. Machač 
1.1   Guidelines for Setting Segment Boundaries 
The demarcation of phonetic units – segments or others – can proceed in two ways: 
automatically or manually. Automatic labelling, most frequently based on HMM [1], 
[2], is not accurate enough and must be manually corrected [3]. Manual labelling can 
be described, and also criticized, as time consuming, demanding and unfortunately 
subjective [4], [5].  
It is not infrequent that the acoustic signal does not allow a fully acceptable 
decision, because there are intelligent reasons for considerably different boundary 
placements. Also, the “aerosol” of the acoustic signal may offer no obvious segment 
boundaries: acoustic contrasts may not be obvious and there will be a “grey” area. It 
is quite likely that two labellers will arrive at different decisions, and even one 
labeller can label differently on different days. 
Guidelines should help as much as possible to increase objectivity, that means 
intra- and interlabeller agreement, and to reduce time expenditure. In this direction, 
we have had good experience with the demarcation of Czech intervocalic plosives, 
fricatives and nasals [6]. Simple rules led to mean deviations in boundary placement 
below 3 ms, which is considerably lower than that reported in literature [4], [5], [7].  
At this time, we are investigating other speechsound combinations and formulating 
preferably clear, simple and unambiguous rules for uniform segmentation. One of the 
main goals is to cover cases where segmentation is not obvious, for example due to 
implicit pronunciation (not careful articulation). 
2   Method  
2.1   Material 
The criteria for boundary placement were formulated based on work with two 
corpora. First, we used the GAP corpus [2], which comprises recordings of 75 Czech 
university students reading a short story. The speakers had been evaluated as to their 
overall speaking performance and divided into three groups – good (G), average (A), 
and poor (P). This ensured a larger variability in the production of /r/. Second, we 
used news bulletins read by two female newsreaders of Czech Radio, each 
approximately four minutes long. We thus had at our disposal utterances from 
professional and non-professional speakers, pronounced in different speech styles and 
with different degrees of articulatory effort. The rules were created based on the 
analysis of approximately 300 items of /r/ in the Praat software [8]. The items were 
selected from all the recordings based on their usefulness for the formulation of rules. 
The procedure when formulating segmentation criteria was to observe the acoustic 
forms of /r/ adjacent to different types of speechsounds, to compare the items, and 
finally to generalize the findings. It should be pointed out that we dealt primarily with 
differences in the segmental context. As the Czech stressed syllable has no specific 
properties [9], the position of /r/ with respect to the stressed syllable was irrelevant for 
our purposes in this study. 

 
Implications of Acoustic Variation for the Segmentation of the Czech Trill /r/ 
175 
2.2   Phonetic Features as Signals for Boundary Placement 
Our rules for boundary setting are based on phonetic features [10]. Every 
speechsound may be characterized by certain inherent phonetic features, whether 
articulatory, acoustic, or perceptual. We are concentrating on those inherent phonetic 
features which are currently relevant for the discrimination of neighbouring 
speechsounds. Those features which overlap from one speechsound to another are not 
inherent features anymore. Labelling is easier when the acoustic contrast between 
segments is high, for example between vowels and voiceless obstruents. The degree 
of acoustic contrast and the degree of overlap of phonetic features depend for example 
on articulatory effort, on the precision of articulatory movements. 
However, there are speechsounds whose inventory of inherent phonetic features is 
partly similar, for example vowels and liquids, that means /l/ and /r/ in Czech, and the 
acoustic contrast is relatively low. 
In the following section, the rules will be accompanied by waveform and wideband 
spectrogram examples. The spectrograms all show the frequency range of 0-5 kHz. 
3   Results 
3.1   Phases of the Czech /r/ 
The Czech /r/-sound has, from the articulatory point of view, at least three parts:  
1. relatively open vocal tract;  
2. the cycle itself (/r/-cycle), that means the approximation of the tongue to the 
alveolar ridge and its withdrawal;  
3. relatively open vocal tract. 
 
The pronunciation can involve more cycles demarcated by phases of open vocal 
tract, but this is rather sporadic in Czech. 
From the acoustic viewpoint, we can define the following corresponding stages:  
 
1. vocalic part;  
2. decay of acoustic energy;  
3. vocalic part (Fig. 1 and 2).  
 
The vocalic part has usually one of two types of quality: 
 
a) approximately schwa [] or  
b) the quality of the neighbouring vowel, not specifically /r/ quality. 
 
We can find the first possibility (like-schwa-sound) 
 
a) in combination with a consonant or a pause (Fig. 1); the schwa-sound then 
separates the neighbouring consonant or pause from the /r/-cycle, 
b) between two cycles within the /r/-sound (Fig. 2); the Czech /r/ usually has only 
one cycle, though. 
 

176 
P. Machač 
 
Fig. 1. The vocalic part (VP) separating the /r/-cycle (C) from a consonant and glottal stop 
 
Fig. 2. Two vocalic parts between three /r/-cycles 
The greatest issue with respect to segmentation is how to treat the vocalic part. As 
will become obvious, there is a conflict between simplicity and reliability of 
segmentation on the one hand, and what may be called phonetic accuracy on the other 
hand. 
Figures 1 and 2 indicate an essential difference between those /r/-items adjacent to 
consonants and those adjacent to vowels. The setting of the boundary between /r/ and 
a consonant, except for /l/ in some cases, is relatively uncomplicated, because the 
vocalic part tends to be clearly visible (Fig. 1). However, the situation is quite 
different in the combination of /r/ with a vowel. In the rest of this paper, we will 
therefore focus on the location of a boundary between /r/ and neighbouring vowels. 
3.2   The Vocalic Part of /r/ 
It is probably not possible to create a general rule for isolating the vocalic part, 
because the acoustic contrast between the vowel and /r/ is transparent in the cycle part 
only (for more detail see section 4). The vocalic part of /r/ cannot be accurately 
separated from the vowel, because the transition between them is often not transparent 
or is fully missing (see Fig. 3, 4 for examples).  

 
Implications of Acoustic Variation for the Segmentation of the Czech Trill /r/ 
177 
As we stated in the introduction, one of the main goals of our segmentation criteria 
is for them to be reliable and applicable by a large number of labellers. Since 
straightforward isolation of intervocalic /r/ seems feasible only for the cycle-part, our 
segmentation rules will be different depending on the type of neighbouring segments. 
The /r/-sound will contain the /r/-cycle and the vocalic part next to a consonant, but 
only the cycle next to a vowel. 
To separate /r/ from a neighbouring vowels, we can exploit the following inherent 
phonetic features: 
 
1. full formant structure of the vowel; 
2. weakened or missing formant structure in the /r/-cycle (Fig. 3, 4); 
3. decrease of amplitude in the /r/-cycle. 
 
 
Fig. 3. Weakened formant structure in a intervocalic /r/-cycle 
 
Fig. 4. Missing formant structure in a intervocalic /r/-cycle 
3.3   Basic Rules for Setting Boundaries of the Intervocalic /r/ 
Vowels are characterized by a full formant structure. The dark vertical areas in the 
spectrogram, representing the peaks of acoustic energy in each glottal pulse, are 
called formant columns.  
 

178 
P. Machač 
 
Fig. 5. Exploited inherent phonetic features of /r/ and neighbouring vowels; formant column; 
zero crossing 
The end of the vowel before a consonant is situated on the right side of the last 
formant column and the beginning of the following vowel on the left side of the first 
formant column. In general, we stick to the rule that the boundary is always placed at 
the nearest zero crossing (Fig. 5). 
In subsections 3.4-3.7, we will examine in more detail some aspects of the 
pronunciation of /r/ in Czech, including implicit pronunciation, and their implications 
for segmentation. 
3.4   The “Grey Area” Rule 
In cases with a lower acoustic contrast between /r/ and neighbouring vowels, we 
identify the “transition” or “grey area”, and the segment boundary should be placed at 
the nearest zero crossing from the middle of this area (Fig. 6). That concerns for 
example those /r/ realizations which have a full formant structure. It is necessary to 
check the segmentation also by listening in these cases. 
 
 
Fig. 6. The boundary placement of an intervocalic /r/ in the middle of the transition part (“grey” 
area) 

 
Implications of Acoustic Variation for the Segmentation of the Czech Trill /r/ 
179 
3.5   Symmetry of Decay and Onset of Acoustic Energy 
The next rule exploits the waveform, and it is especially useful when it is not clear 
from the spectrogram which formant column is the last in the preceding vowel or first 
in the following vowel. This is caused by the fact that the decay and onset of acoustic 
energy tends to be symmetrical. The vowel will comprise only those formant columns 
with “full acoustic energy”. The glottal cycles with lower acoustic energy will be 
regarded as part of the /r/-cycle, as shown in Fig. 7. 
 
 
Fig. 7. Intervocalic /r/ with symmetrical decay and onset of acoustic energy (the relevant 
portion of the waveform is enlarged on the right) 
3.6   Low Acoustic Contrast 
Implicit pronunciation can lead to very low acoustic contrast between segments in the 
spectrogram, with very gradual changes in the formant structure. However, the /r/-
sound remains clearly audible. The left part of Figure 8 shows an item with a very low 
acoustic contrast between /r/ and the following vowel. The right part of Figure 8 
shows a complete lack of visible contrast. In order to locate the boundary, it is 
necessary to listen to the sound. 
 
    
 
Fig. 8. Intervocalic /r/ with very low acoustic contrast 
3.7   Devoiced Intervocalic /r/ 
It is possible to encounter relatively frequently those realizations of /r/ which are 
partly or fully devoiced. F0 and the formant structure are missing in these instances, 
and the /r/-cycle is often accompanied by noise similar to a fricative or plosive. The 

180 
P. Machač 
/r/-sound thus loses sonority and possibly formant structure as its inherent phonetic 
features, and obtains noisiness as a non-inherent phonetic feature. Boundary 
placement is usually not complicated, because the acoustic contrast is actually higher 
(Figure 9). 
 
 
Fig. 9. Devoiced intervocalic /r/ with a noise-sound 
4   Discussion and Conclusion 
The Czech sonorant trill /r/ belongs to the more challenging speechsounds from the 
viewpoint of segmentation. There are several reasons for this. First, /r/ is 
characterized in the articulatory domain by one (and only rarely more) fast 
approximation of the tongue tip towards the alveolar ridge (the /r/-cycle). The aim of 
this articulatory gesture is presumably a short-time decrease in acoustic energy and a 
disintegration of formant structure. This occurs automatically when /r/ is adjacent to a 
vowel. When /r/ appears next to a consonant or a pause, the necessary acoustic 
contrast apparently requires the presence of a short segment with a full vocalic 
structure between the /r/ and the consonant or pause. The resulting vocalic part of 
neutral quality (a schwa sound), which leads to a sufficient contrast with the /r/-cycle, 
is regarded as part of the sonorant trill, since full formant structure is not an inherent 
phonetic feature of consonants (the vocalic part appears almost exclusively even when 
/r/ is adjacent to sonorant consonants). Moreover, the vocalic part is present between 
the individual cycles of /r/ with more trills. 
When /r/ is adjacent to a vowel, the vowel itself assumes the function of the 
contrastive vocalic part. However, it is not possible to formulate unambiguous rules 
for the isolation of the vocalic part of /r/. In connected speech, vowels are often so 
short that subdivision into “vowel” and “vocalic part of /r/” is not feasible. In vowels 
of normal duration, the variability of formant transitions, if they are visible at all, is so 
high that they do not allow for isolating the vocalic part of /r/. 
For these reasons, it was necessary to specify two ways of locating the boundary of 
/r/, depending on the neighbouring sounds: 

 
Implications of Acoustic Variation for the Segmentation of the Czech Trill /r/ 
181 
a) next to a consonant or pause, what is segmented as the /r/-speechsound 
comprises the /r/-cycle and the vocalic part (which always separates the cycle from 
the consonant or pause); 
b) next to a vowel, what is segmented as the /r/-speechsound is only the /r/-cycle 
itself.  
To conclude, we have attempted to define criteria for the location of boundaries of 
the Czech sonorant trill /r/. When formulating these rules, we are led by two main 
principles: to find rules which 
1. are preferably clear, simple and unambiguous to achieve uniform segmentation; 
2. respect as much as possible the physical reality of the speech signal. 
The proposed criteria for the segmentation of the Czech /r/ could be employed for 
segmenting in other languages which have a trill-like /r/ sound. Segmentation 
guidelines for other speechsound combinations have already been formulated and will 
be available in the foreseeable future. 
References 
1. Wester, M., Kessens, J.M., Cucchiarini, C., Strik, H.: Obtaining phonetic transcriptions: a 
comparison between expert listeners and a continuous speech recognizer. Language and 
Speech 44, 377–403 (2001) 
2. Volín, J., Skarnitzl, R., Pollák, P.: Confronting HMM-based Phone Labelling with Human 
Evaluation of Speech Production. In: Proceedings of Interspeech 2005, pp. 1541–1544. 
ISCA, Lisbon (2005) 
3. Kominek, J., Bennett, C., Black, A.W.: Evaluating and Correcting Phoneme Segmentation 
for Unit Selection Synthesis. In: Proceedings of Eurospeech 2003, pp. 313–316. ISCA, 
Geneva (2003) 
4. Pitt, M., Johnson, K., Hume, E., Kiesling, S., Raymond, W.: The Buckeye corpus of 
conversational speech: labeling conventions and a test of transcriber reliability. Speech 
Communication 45, 89–95 (2005) 
5. Wesenick, M.-B., Kipp, A.: Estimating the quality of phonetic transcriptions and 
segmentations of speech signals. In: Proceedings of ICSLP 1996, pp. 129–132. ISCA, 
Philadelphia (1996) 
6. Machač, P., Skarnitzl, R., Volín, J.: Inter-labeller agreement in segmental boundary 
placement. In: Vích, R. (ed.) 17th Czech-German Workshop - Speech Processing, Prague, 
pp. 57–61 (2007) 
7. Pauws, S., Kamp, Y., Willems, L.: A hierarchical method of automatic speech 
segmentation for synthesis applications. Speech Communication 19, 207–220 (1996) 
8. Boersma, P., Weenink, D.: Praat, version 4.4.20 (2006), http://www.praat.org 
9. Palková, Z., Volín, J.: The role of F0 contours in determining foot boundaries in Czech. In: 
Proceedings of the 15th ICPhS, pp. 1783–1786. Organizing Committee, Barcelona (2003) 
10. Machač, P.: Stabilita zvukových charakteristik fonémů ve spontánních mluvených projevech. 
In: Hladká, Z., Karlík, P. (eds.) Čeština - univerzália a specifika 5, pp. 427–435. Lidové 
noviny, Praha (2004) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 182–189, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Voicing in Labial Plosives in Czech 
Annett B. Jorschick 
Charles University, Institute of Phonetics, nám. Jana Palacha 2, 
116 38 Praha 1, Czech Republic 
annett.jorschick@ff.cuni.cz 
Abstract. Traditional phonetic descriptions of Czech consonants assume that 
voiced segments are always realized with complete vocal fold vibration. The 
present research addresses the importance of voicing in Czech labial plosives 
using a subset of the Prague Phonetic Corpus. Corpus studies provide a power-
ful tool to study variation in actual realizations of speech and offer material to 
investigate its impact on perception. In the first study the extent of vocal fold 
vibration in voiced plosive /b/ in intervocalic position was examined. The 
results show that a considerable number of these plosives were realized with 
devoiced portions. In the second study it was tested whether Czech phonetically 
experienced listeners were able to recognize differences in voicing. A strong 
relationship was found between the amount of vocal fold vibration in the targets 
and the ratings by the participants indicating that voicing is a good predictor for 
the phonological voice/voiceless distinction in Czech.  
Keywords: voicing, vocal fold vibration, voice production, Czech. 
1   Introduction 
This paper addresses the importance of voicing in pronunciation and perception of 
Czech bilabial plosives. 
In phonological terms voicing refers to broad categories of phonemes distinguish-
ing between voiced (e.g. /b, d, g/) and voiceless (e.g. /p, t, k/) segments. However, 
that does not imply that phonologically voiced phonemes (e.g. /b/) are always realized 
with phonetic voicing. Phonetic voicing refers to vibration of the vocal folds during 
the realization of a sound. In many languages such as English [e.g. 1] or German [2], 
voiced plosives are often realized without vocal fold vibration. In these languages 
other phonetic cues, for example aspiration, are important for the phonological 
voiced-voiceless distinction.   
The investigation of the phonological voiced-voiceless contrast is especially prob-
lematic since there are numerous cues potentially indicating this distinction. For 
example, Lisker [3] identified 16 cues relevant to the voiced-voiceless contrast in 
English. Moreover, the relevance of these cues for the voicing contrast is highly lan-
guage dependent. While in some languages, i.e. English or German, voiced plosives 
are seldom realized with vocal fold vibration, it is assumed for Czech that voiced 
obstruents are always realized with complete vocal fold vibration [4].  

 
Voicing in Labial Plosives in Czech 
183 
The use of speech corpora is a promising opportunity for the investigation of 
varieties in speech pronunciation and its consequences for perception. Speech pro-
duction and perception have been studied separately in psycholinguistics for several 
decades. A similar gap exists for the study of the realization of speech in phonetics 
and the conclusions drawn in phonology [see 5 for an overview].  One reason for 
these separations was the use of different methodologies. Corpus studies offer a lin-
kage between the study of pronunciation and perception by providing materials for 
both. Using materials from the same corpus might facilitate to find similar patterns in 
both the realization and perception of speech. 
The aim of this paper is to shed some light on the importance of vocal fold vibra-
tion in pronunciation and perception in Czech. In the first study I investigated the 
traditional assumption that voiced plosives in Czech are always realized with com-
plete phonetic voicing, i.e. complete vocal fold vibration. In the second study the 
importance of phonetic voicing in perception of sounds was examined. 
Throughout the remainder of this paper I will refer to phonological voicing always 
as voice/voiceless contrast while the term voicing will only be used for phonetic 
voicing.  
2   The Realization of /b/ in Vocalic Contexts 
According to the traditional view, Czech obstruents are always realized with complete 
vocal fold vibration. However, the actual detail of voicing realization in Czech 
obstruents has not been investigated yet. The aim of the first study was to examine the 
variation of vocal fold vibration during the realization of voiced labial plosives in 
Czech. 
For the investigation of voicing an intervocalic position of the plosive was chosen. 
In such a context it is very likely that the targets are realized with complete vocal fold 
vibration [3]. Furthermore, effects of adjacent consonants such as regressive or pro-
gressive voice assimilation can be ruled out as an explanation of variation. If vocal 
fold vibration in intervocalic position is incomplete, the traditional view cannot be 
maintained and the importance of vocal fold vibration in Czech obstruents is to be 
analyzed. Otherwise, if vocal fold vibration is complete in such context, it might serve 
as a perfect and exclusive predictor for the phonological voiced-voiceless contrast.  
2.1   Methods  
All materials were obtained from a subset of the Prague Phonetic Corpus [6]. This 
subset consists of spoken utterances of 75 speakers (48 female and 27 male) reading 
nine sentences and covers approximately 30 minutes. All speakers were native Czech 
with standard Bohemian dialect and aged from 18 to 40 years.  
The position of the phone might affect the realization of voicing; therefore, the 
investigation was restricted to voiced labial plosives in vowel context. To this end all 
occurrences of /b/ in the phrase “červené boty” (red shoes) which was part of a longer 
sentence were selected using Emu2prev2.0 [7].   
The subset of the Prague Phonetic Corpus is manually transcribed at phone level as 
described in [6]. According the transcription rules presented in this paper, the begin-
ning and end of a plosive are set to the closest zero crossing at a significant second 
formant change. 

184 
A.B. Jorschick 
The presence or absence of voice in the target plosives was manually labeled using 
visual and auditory cues as displayed by PRAAT software [8]. Voice was defined as 
periodicity in the signal with audible amplitude. The beginnings and ends of vocal 
fold vibration were always set to the closest zero crossings. Fortunately, Czech labial 
plosives do not display any profound bursts at release, so there was no danger of a 
masking effect of the burst. Figure 1 gives an example of the voice labels together 
with the visual cues from waveform and spectrogram. The Voicing Ratio of the plo-
sive was defined as the ratio between the time from the beginning of the phone to the 
end of the vocal fold vibration plus the time from the beginning of the vocal fold 
vibration after the burst until the end of the phone divided by the overall phone dura-
tion of the plosive. 
 
 
Fig. 1. Example of plosive boundary and voice labels in Praat 
2.2   Results and Discussion 
Deviating from the assumption that voiced plosives in Czech are realized with com-
plete vocal fold vibration, a large proportion of the targets contained devoiced por-
tions (41%). An exact two-sided binomial test revealed no significant difference in the 
occurrence of completely (n = 44) and incompletely (n = 31) voiced plosives: pi = 
0.5, p = 0.17. The absence of a significant difference might result from a lack of sta-
tistical power due to the small number of targets. However, it should be kept in mind 
that the realization of voiced plosives with devoiced portions occurred in vowel con-
text, where complete vocal fold vibration is very likely to occur. The ratio between 
completely and incompletely voiced targets behaved exactly the same whether the 
speaker was male (16, 11) or female (28, 20): Χ2(1) = 0.028, p = 0.87. 
A closer inspection of the devoiced targets revealed two facts. First, the range of 
devoiced portions was very narrow. Only three out of 75 investigated plosives (4%) 
were realized with Voicing Ratios of less than 50% and only two targets (3%) were 
realized with less than 40% voicing. Secondly, devoiced parts always occurred at the 
area around the burst. This is not surprising since vocal fold vibration is especially 
hard to realize during constrictions. Furthermore, vocal fold vibration is harder to 
sustain the longer the closure. Indeed, there was a negative correlation between the 

 
Voicing in Labial Plosives in Czech 
185 
duration of the closure (time from the beginning of the phone until the onset of the 
burst) and the Voicing Ratio: r = -0.30 showing that longer closures tend to be rea-
lized with less voicing. 
In summary, voiced plosives were often realized with devoiced portions in Czech 
despite the intervocalic position of the targets. Hence the traditional view of vocal 
fold vibration in Czech should be corrected. However, the devoiced portions were 
strictly limited in range as well as in position within the plosive.  
3   The Recognition of Plosive Voicing in Czech 
The first study pointed out that the vocal folds do not always vibrate continuously 
during the realization of voiced labial plosives, even in intervocalic position. This 
questions the role of vocal fold vibration in Czech obstruents as the most important 
and reliable cue of the phonological voiced-voiceless contrast. However, the first 
study also revealed that devoicing was limited to the area around the burst and to a 
certain range. Given these restrictions of devoiced portions, vocal fold vibration might 
still serve as a highly reliable linguistic cue for the phonological voiced-voiceless 
contrast in Czech. 
Another often used measure of the voiced-voiceless contrast is voice onset time 
(VOT). Unlike the vocal fold vibration, VOT is mainly an indicator of oral and glottal 
gesture coordination. However, it may reflect various other phonetic-acoustic fea-
tures. A further cue of the voiced-voiceless contrast which is reflected in VOT and 
whose importance varies across languages is aspiration. For Czech it is assumed that 
aspiration plays a negligible role in the realization of plosives [4]. Even voiceless 
plosives are hardly ever aspirated. Given the assumption that for the Czech phono-
logical voiced-voiceless contrast only vocal fold vibration is important but not other 
parameters which are reflected in VOT, VOT should not add to the explanation of the 
voiced-voiceless contrast. The question that arises here is: Are there important cues to 
the voiced-voiceless contrast in Czech that VOT additionally measures?  
The second study concerned the question of the influence in variation of the vocal 
fold vibration on the perception of labial plosives by native Czech listeners. Given the 
traditional assumption and the limitations of variation in voicing found in the first 
study, the amount of vocal fold vibration should be a strong predictor for the percep-
tion of voicing in the plosives. Moreover, if the degree of vocal fold vibration is the 
most prominent cue of the voiced-voiceless contrast, it should be a better predictor 
than VOT for the perception of plosive voicing in Czech listeners. 
3.1   Methods 
Material. Speech material was drawn from the subset of the Prague Phonetic Corpus 
which was used in the first study. All realizations of voiced and voiceless labial 
plosives in intervocalic position were selected from this corpus using Emu2prev2.0 
[7]. To restrict the material set, the plosives were assigned to one of five categories 
with respect to the amount of vocal fold vibration in the plosive: (a) completely 
voiced, (b) largely voiced but devoiced bursts, (c) voiced and voiceless portions, (d) 
largely voiceless but some vocal fold vibration in the beginning of the plosive, and (e) 
completely voiceless. In order to cover a wide range of variation in voicing, five 

186 
A.B. Jorschick 
targets were chosen for female and five for male voices from all categories. For male 
speakers there were only four targets for one of the categories as it was not possible to 
find a sufficient number of labial plosives with voiced and voiceless portions. Thus, 
the experiment was conducted with 25 targets from female speakers and 24 targets 
from male speakers. 
All targets were composed of vowel-plosive-vowel sequences (VCV). To control 
for some of the factors which are known to influence the perception of the voiced-
voiceless contrast [e.g. 3], care was taken to select only plosives with a similar range 
of phone duration (67 to 112 ms) and which were clearly realized with a burst. Fur-
thermore, the length of the preceding and the following vowel was manipulated 
(lengthened or shortened) to cover ranges between 40 to 50 and 45 to 55 ms respec-
tively. To lengthen a vowel, single cycles of the waveform were duplicated. Shorten-
ing was done by cutting the vowel at an appropriate zero crossing. All ranges were 
chosen to limit manipulations and enhance the use of the natural VCV occurrences. 
Targets were normalized in amplitude to achieve a similar volume and 10 ms of the 
onsets and offsets of the dynamic envelope of all VCV-targets were smoothed to 
make them sound more natural. Stimulus preparation (cutting, lengthening, smooth-
ing, and normalization) was done using the Cool Edit Software [11]. 
The same procedure as in the first study was used to measure the amount of vocal 
fold vibration in the plosives as Voicing Ratio (the portion of vocal fold vibration in 
relation to the duration of a phone). Furthermore, VOT was measured for each target. 
VOT was defined as the time from the beginning of the burst to the onset of vocal fold 
vibration in ms. For the 10 targets with complete vocal fold vibration a negative VOT 
was measured from the beginning of the burst to the beginning of the phone. Acoustic 
measurements were conducted with the help of PRAAT software [8]. 
Participants and Procedure. Twelve native Czech listeners with background in 
phonetics participated in the experiment. Participants listened to the targets via 
headphones (KOSS UR-15C) in a quiet room. Before the start of the experiment there 
was a learning session, where the participants listened to examples from each of the 
five categories with the instruction to observe the fine differences in voicing within 
and between them. After this learning phase, they listened to the VCV-targets only 
one at a time. Their task was to indicate at a 5-point scale whether a target sounded 
more like a voiced /b/ or a voiceless /p/. 
The targets were presented in two blocks, one for male and one for female speak-
ers, with starting block randomly assigned. Within each block the 25 (24) targets were 
randomly distributed. There were four different random sequences for each block to 
avoid sequence effects. The sequence of a block used in the experiment was assigned 
to participants in a Latin-square like order. The experiment was conducted with the 
help of PRAAT [8]. 
3.2   Results and Discussion 
Trials in which the rating deviated by two or more values from the mean of the ratings 
in that item were excluded from further analysis. This was done because during the 
experiment participants showed problems with the direction of the rating scale sug-
gesting that those values were marked erroneously. This affected six out of 588  
ratings (1%).  

 
Voicing in Labial Plosives in Czech 
187 
The Average Perception Score was calculated as the mean of the ratings for each 
target. Figure 2 gives the Average Perception Scores as a function of the Voicing 
Ratio and VOT.  
 
            2a)                                                          2b) 
 
Fig. 2. Average Perception Scores of the targets as a function of the Voicing Ratio (2a) and 
VOT (2b) in the plosive 
A linear regression model was build with the Average Perception Score as depen-
dent variable and Voicing Ratio and VOT as predictors. This analysis yielded signifi-
cant main effects for the Voicing Ratio (β = 1.048, t(46) = 15.61, p < 0.001) and VOT 
(β = 0.162, t(46) = 2.41, p = 0.02. The model could explain a large portion of the 
variance of the Average Perception Score (adjusted R2 = 0.887, F(2,46) = 188.55,  
p < 0.001). 
The results show that the proportion of vocal fold vibration in a phone (Voicing 
Ratio) is a valuable and reliable indicator for the perception ratings of the participants. 
Moreover, the discrepancy in β–weights between the Voicing Ratio and VOT demon-
strate that VOT is far less important to explain the perception of the phonological 
voiced-voiceless contrast in Czech labial plosives. VOT indicates more than one pho-
netic cue relevant to the phonological voiced-voiceless contrast [9, 10]. Phonological 
voicing, similar to what is measured by the Voicing Ratio, is one of the cues which 
VOT represents. Hence, the small but significant contribution of VOT might result 
from properties of the plosive beyond vocal fold vibration, such as aspiration. It also 
shows that the amount of vocal fold vibration is not the only cue relevant to the Czech 
voiced/voiceless contrast in labial plosives but the most relevant.  
The aim of using the vocal fold vibration of the complete plosive and not only of 
the closure to calculate the Voicing Ratio was to get simple but reliable indicators of 
voicing for automatic search in larger corpora. Further research to explain the me-
chanics of plosives should minimize susceptibility to errors due to additional manual 
or automatically labeling, such as burst onsets. 

188 
A.B. Jorschick 
Figure 2a) also indicates that the relationship between the Voicing Ratio and the 
perception of plosives is not linear but a categorical one. Figure 3 displays the density 
functions of targets rated as more voiceless (below 3) or more voiced (above 3). The 
intersection between both functions is at 41.7% of the Voicing Ratio. This shows that 
up to a Voicing Ratio of approximately 42% the plosives were perceived as voiceless. 
Targets realized with a larger proportion of vocal fold vibration were perceived as 
voiced.  
 
 
Fig. 3. Density functions for targets with Average Perception Scores above and below ratings of 3 
4   General Discussion 
The paper presents two studies in which the importance of vocal fold vibration in 
labial plosives in Czech production and perception was examined. The first study 
investigated the traditional assumption that voiced labial plosives are realized with 
complete phonetic voicing in Czech. The present study did not maintain this assump-
tion. A considerable number of voiced labial plosives were realized with devoiced 
portions. Devoicing occurred even though only plosives in intervocalic position were 
investigated where complete voicing is very likely [3]. However, devoiced portions 
were always restricted to areas around the burst and were limited to a certain range. 
Realizations with less than 40% of voicing rarely occurred. 
The second study addressed the question of the importance of voicing in percep-
tion. Keeping other factors known to affect the phonological voiced-voiceless contrast 
relatively constant, the amount of vocal fold vibration in the plosive became apparent 
as strong predictor of the perception of the targets as voiced or voiceless.   
Moreover, the relationship between the proportion of vocal fold vibration in a plosive 
and the ratings of the participants showed a categorical perception pattern. Categorical 

 
Voicing in Labial Plosives in Czech 
189 
perception has been shown for many perception phenomena in speech before [12]. In 
this study, approximately 42% of voicing marked the boundary between two perceptual 
categories (voiced and voiceless). Interestingly, this boundary is confirmed by both the 
results of the perception experiment and the production data in the first study since 
voicing below that boundary rarely occurred in the first study. This indicates that Czech 
listeners know about the variation in range and area of vocal fold vibration in voiced 
and voiceless plosives in their language and use this knowledge in perception. 
Although the results suggest vocal fold vibration as important cue for the phono-
logical voiced-voiceless contrast in Czech, the interplay between vocal fold vibration 
and other factors relevant to the voiced-voiceless contrast are beyond the scope of this 
paper and have to be addressed in future research. 
 
Acknowledgments. This research was supported by the European Union grant 
MRTN-CT-2006-035561 (Sound to Sense). The author would also like to thank  
Dr. Jan Volín for his comments on the draft of this paper.  
References 
1. Lisker, L., Abramson, A.S.: A cross-language study of voicing in initial stops: Acoustical 
measurements. Word 20, 384–422 (1964) 
2. Jessen, M.: Phonetics and phonology of tense and lax obstruents in German. John 
Benjamins, Amsterdam (1998) 
3. Lisker, L.: Voicing in English: A catalogue of acoustic features signaling /b/ versus /p/ in 
trochees. Language and Speech 29, 3–11 (1986) 
4. Palková, Z.: Fonetika a fonologie češtiny. Karolinum, Praha (1997) 
5. Ernestus, M., Baayen, H.: Corpora and exemplars in phonology. To be published in: 
Goldsmith, J.A., Yu, A., Riggle, J. (eds.) Handbook of Phonology. Blackwell Publishers, 
Malden 
6. Volín, J., Skarnitzl, R., Machač, P., Janoušková, J., Veroňková, J.: Reliabilita a validita 
popisných kategorií v Pražském fonetickém korpusu. In: Kopřivová, M., Waclawičová, M. 
(eds.) Čeština v mluveném korpusu, pp. 249–254. Nakladatelství LN / ÚČNK, Praha 
(2008) 
7. Bombien, L., Cassidy, S., Harrington, J., John, T., Palethorpe, S.: Recent Developments in 
the Emu Speech Database System. In: Proceedings of the Australian Speech Science and 
Technology Conference, Auckland (2006) 
8. Boersma, P., Weenink, D.: Praat: doing phonetics by computer (Version 5.0.23) 
[Computer program] (2008) 
9. Cho, T., Ladefoged, P.: Variations and universals in VOT: evidence from 18 languages. 
Journal of Phonetics 27, 207–229 (1999) 
10. Keating, P.A.: Phonetic and phonological representation of stop consonant voicing. 
Language 60(2), 286–319 (1984) 
11. Johnston, D.: Cool Edit 2000 (Version 1.1). Syntrillium Software Corporation, Phönix 
(2000) 
12. Liberman, A.M., Harris, K.S., Hoffman, H.S., Griffith, B.C.: The discrimination of speech 
sounds within and across phoneme boundaries. Journal of Experimental Psychology 54, 
358–368 (1957) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 190–200, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Normalization of the Vocalic Space 
Jan Volín 
Institute of Phonetics, Faculty of Philosophy and Arts, Charles University in Prague 
jan.volin@ff.cuni.cz 
Abstract. Phonetic units which constitute natural continuous speech display 
immense variation due to a substantial number of factors. Consequently, one of 
the key questions for speech scientists concerns the translation of individual 
bundles of acoustic features into conventional linguistic meanings or types. 
Although the problem of normalization of acoustic data is common to many 
areas of speech science, its solutions depend on particular applicational 
objectives. An overview of the development in the field of normalization is 
presented from the perspective of the phonetic understanding of speech 
communication. The explanatory value of individual methodological outcomes 
is discussed. Both indexical (related to the speaker identity) and contextual 
(related to the linguistic form) factors are considered and several normalization 
algorithms are compared with each other. Recent findings indicate that human 
listeners exploit not only visual cues but also their cumulated social experience 
when processing sounds of speech. 
Keywords: Classification, discrimination, vowels, acoustic space, variation. 
1   Introduction 
Natural continuous speech is the most common and most effective channel of human 
communication. We can choose to describe it as a multitude of layers of phonetic 
units (e.g., phones, syllables, stress-units, prosodic units, etc.) which host phonetic 
phenomena (e.g., formant configurations, stresses, melodic accents, articulation rate 
variations, intonational primitives, etc.). Each of the units which constitute speech as 
well as each of the phonetic phenomena can be characterized in a multidimensional 
acoustic space. However, individual acoustic dimensions of a phonetic object display 
enormous variation. Interestingly, this variation largely escapes attention of an 
ordinary person, yet it is convincingly revealed by acoustic measurements. 
The factors causing the immense variation of phonetic objects can be roughly 
classified as indexical (sex, age, dialect, sociolect, size of the vocal tract, health, etc.) 
or contextual (the influence of neighbouring units due to coarticulation, the effect of 
the position within a higher unit, speaking style, etc.). For the present purpose we will 
disregard external factors such as various types of environmental noise, which also 
contribute to the acoustic variation of the speech signal. 
Our everyday experience suggests that despite considerable variation, recognition 
of phonetic objects by human listeners is highly successful. Similarly to the 
recognition of a visual object such as a particular house or person from various 

 
Normalization of the Vocalic Space 
191 
angles, distances and under different light conditions, we can recognize speech units 
produced by unknown speakers or even items distorted by unusual circumstances. 
One of the possibilities to explain this fact is to utilize the concept of normalization. 
In this context, normalization could be loosely defined as a procedure by which 
unique or circumstantial qualities of an object are modified into typical features of a 
class of objects. This guarantees a correct assignment of the object into the 
appropriate class. According to normalization approach, all percepts are mentally pre-
processed in order to converge their properties with the properties of the model 
(template) which serves for their classification as a type. Whether this concept is 
correct for speech perception or not still remains to be seen and we will address this 
question again later in this article. 
In the vast area of speech research, normalization can be referred to in at least three 
wider frameworks. The first is the technological one. Certain automatic speech 
recognition (ASR) procedures may involve pre-processing of the signal so that the 
target units which are being recognized fit better the templates representing them. 
Whether the chosen procedure relates to human perception or not is immaterial within 
this framework: the important criterion is the effectiveness of the procedure in the 
technological process. 
The second framework is linguistically descriptive. Phonetic data may require 
particular degree of normalization in order to facilitate further research. Such 
normalization should neutralize unimportant factors of variation while preserving 
features which are of the research interest. If melodic configurations are studied by 
intonologists, the natural register of the speaker should be normalized, since it hinders 
comparison of individual items collected for the research. For certain tasks, even pitch 
range (the span given by relevant intonational minima and maxima) has to be 
normalized as well. It can be seen quite clearly, that within the linguistically 
descriptive framework there is no need for ‘absolute normalization’. We may want to 
preserve information about, e.g., coarticulation, sociolinguistic factors, etc. Some 
interesting detail may explain speech patterning even with its shortcomings. Thus, 
only the selected differences are removed. As in the previous framework, procedures 
that are used do not necessarily model the way in which human brains operate. 
That is the major concern of the third, psychophonetic framework. In psycho-
phonetics (and neurophonetics) the chief objective is to describe perceptual processes 
in human brains. The effectiveness of normalization is important only to the extent in 
which it is present in true speech perception by humans. Therefore, if we find out that 
human perception does not use any normalization as such, strange as it may appear at 
the current state of knowledge, we have to be brave enough to stop modelling it. This 
thought should be borne in mind for the rest of this paper. Although we often describe 
speech as consisting of phonetic units which can be assembled to constitute larger 
units, we must not forget that the intellectual descriptions of speech reflect the way 
our conscious cognitive mechanisms work. It is not necessarily the way our 
unconscious (or subconscious) brain processes function. 
More specifically, the idea that our everyday speech communication is based on 
assembling words from correctly identified phonemes is as suspicious as the idea that 
we recognize our closest friends or family members after we have assembled their 
faces from their eyes, eyebrows, noses, ears, lips, cheeks, etc. Intellectual descriptions 
of reality often rely on decomposition of the wholes into pieces as small as possible. 

192 
J. Volín 
This does not mean that fast unconscious operations performed by our brains do the 
same. Although we are obliged to talk about linguistic units if we want to discuss 
language and speech as we understand it, we should allow for the possibility that 
these may not take any part in real speech production and perception. 
2   Vowel Normalization 
From the more general account of the topic we will now shift to the normalization of 
vowels. This is because more work has been done in the area of vowel normalization 
than elsewhere and it is often instructive to other fields of interest in the phonetic 
domain of speech. Also, in their basic form vowels constitute a relatively homo-
genous group of speech sounds in most languages and comprise about 40-50 percent 
of all the speech segments in continuous speech. Last but not least, models of vocalic 
acoustic structure are relatively transparent. One of its forms is captured in fig. 1. 
[i]
[e]
[a]
[o]
[u]
time
frequency
F3
F2
F1
 
Fig. 1. A simple model of the vocalic space with three formants and a typical five-term system. 
Trajectories of the formants at certain moments produce configurations which correspond with 
conventional vowel sounds. 
2.1   Internal Ratio Models 
One of the earliest approaches to vowel formant normalizations evokes the notion of 
musical chords. It was claimed that mutual ratios of formant frequencies are sufficient 
for a vowel definition since they result in a specific configuration of stimulations on 
the basilar membrane in the cochlea and this pattern is recognized regardless of its 
position on the membrane. Similarly, the value of major, minor, diminished and other 
musical chords is recognized whether they are played in the low, mid or high register. 
This idea is captured, for instance, in the model suggested by Sussman [1]: 
F
F
F
F
F
F
F
F
F
norm
norm
norm
ˆ
3
ln
3
;
ˆ
2
ln
2
;
ˆ
1
ln
1
=
=
=
 . 
(1) 
Subscript norm refers to normalized values, ln to natural logarithm, the Fˆ indicates 
geometric mean, which is calculated from the values of the three formants of the 
vowel that is being normalized.  
In his article, Sussman refers to a conference paper claiming that by 1980 there had 
been over a hundred normalization procedures tested in an attempt to model 
perceptual constancy of speech units [1: 16]. Many of these must have been inspired 
by the musical cord analogy. Sussman’s own inspiration, however, comes from 

 
Normalization of the Vocalic Space 
193 
biology, specifically, from the discovery of combination-sensitive neurons in the 
brains of bats. Neurobiological research had suggested that apart from neurons 
specialized in pitch estimation and neurons sensitive to pitch movements, the bats’ 
brains also possessed neurons which were only active when specific combinations of 
frequencies were present in the signal, but not when individual components of the 
combination were exposed  [1: 14]. Such combination-sensitive neurons were also 
discovered in frogs. Although frogs are not mammals and bats use sounds in 
frequencies of 30-120 kHz to monitor the space around them, the idea to model vowel 
perception as a multi-layered process in which combination-sensitive neurons play an 
essential role is very attractive. 
Sussman does not explain individual aspects of his formulae, but we can guess that 
he used the geometric (rather than arithmetic) mean to reduce the effect of F3 and the 
logarithm to reflect general properties of pitch perception. The author demonstrated 
his formula on vowels [i], [a] and [u] from carefully pronounced monosyllables with 
satisfactory results. However, when we used it to Czech vowels from continuous texts 
read by 75 speakers, the outcome was quite poor (for information about the material 
see [2]). Table 1 indicates that data normalized by (1) do not improve the results of 
LDA (Linear Discrimination Analysis). Specifically, it is the vowel [u] which suffers 
from the transformation. Moreover, if we plot the normalized data into a traditional 
F1-F2 scattergram, the shape of the vocalic space is distorted and would be quite 
inconvenient to use in linguistic research. Interestingly, Sussman is confident that 
simple mathematical formulae could reflect human normalization [1: 13]. 
Table 1. Confusion matrices from discrimination analysis of original data in hertz (Hz) and the 
same data normalized by the procedure proposed in [1] (Suss). Vowels as intended by speakers 
are in the rows, the classification outcome is in the columns. 
Hz 
i 
e 
a 
o 
u 
success 
[i] 
533 
62 
1 
0 
4 
88.8 % 
[e] 
47 
389 
108 
34 
22 
64.6 % 
[a] 
0 
56 
457 
82 
5 
76.2 % 
[o] 
2 
18 
50 
344 
186 
57.3 % 
[u] 
55 
6 
0 
111 
428 
71.3 % 
Suss 
 
 
 
 
 
 
[i] 
511 
87 
1 
0 
1 
85.2 % 
[e] 
43 
402 
125 
15 
15 
67.0 % 
[a] 
0 
50 
474 
72 
4 
79.0 % 
[o] 
4 
34 
59 
331 
172 
55.2 % 
[u] 
92 
26 
3 
130 
349 
58.2 % 
 
A similar procedure was suggested by Miller ([3], in [4]): 
2
3
ln
3
;
1
2
ln
2
;
0
ˆ
1
ln
1
F
F
F
F
F
F
F
F
F
norm
norm
norm
=
=
=
 . 
(2) 

194 
J. Volín 
In Miller’s formula formants are compared with their lower neighbours rather than 
with a quasi-centroid of the vocalic space. The first formant is normalized against the 
geometric mean of F0, which is calculated from F0 values over a stretch of speech. 
This brings two very important aspects into play. First, it has been shown decades ago 
that F0 values influence perception of vowels. Second, calculating a parameter from a 
stretch of speech departs slightly from the idea that normalization should be purely 
segment-internal and that it could disregard the neighbourhood of the vowel. 
However, even Miller’s proposal did not bring about any desirable results in 
normalization of 3000 vowels from [2], which is indicated in table 2. 
Table 2. Confusion matrix from discrimination analysis of data normalized by the procedure 
proposed in [3]. Intended vowels are in the rows, the classification outcome is in the columns. 
Mill 
i 
e 
a 
o 
u 
success 
[i] 
510 
89 
1 
0 
0 
85.0 % 
[e] 
50 
410 
125 
10 
5 
68.3 % 
[a] 
0 
55 
488 
56 
1 
81.3 % 
[o] 
4 
31 
53 
351 
161 
58.5 % 
[u] 
90 
15 
1 
106 
388 
64.7 % 
 
The influence of F0 on perception of vowels is well documented in [5], indicating 
that an octave change of F0 may lead to shifts of perceptual boundaries between vowel 
as large as 10 % of the value of F1 or F2. The same formant values are perceived as 
different vowels if combined with different F0 values. A newer experiment by Johnson 
also confirmed that with higher F0, higher formants are required to preserve the same 
vowel quality [6]. However, Johnson’s experiment also showed that it was not F0 of 
every single vowel which caused the effect, as long as the speaker did not change. The 
listeners were influenced by F0 only when they were adjusting to new speakers  
[6: 235]. This might mean that the information about F0 is only used to estimate the 
vowel space when listening to a new speaker and it is not utilized every time a vowel 
sound occurs. Be that as it may, Miller’s idea to introduce acoustic information from 
the outside of the inspected segment already points in the direction of external scaling 
models, some of which are discussed in the following section. 
2.2   External Scaling Models 
Some normalization procedures employ the information about the acoustic environment 
of individual phones rather than merely the information contained by the phones 
themselves. This concept leads to the scaling of a speaker’s vowel space using its 
particular characteristics possibly linked to the speaker’s vocal tract size. 
Since it is generally known that the correlation of speaker’s mean F0 with the size 
of the vocal tract is not particularly strong, other ways of external scaling were 
sought. Nordstöm & Lindblom [7], for example, believed that the vowel space can be 
normalized in relation to the average F3 of low vowels. They suggested simple 

 
Normalization of the Vocalic Space 
195 
scaling constant k calculated as the ratio between the mean F3 in the population 
(F3ref) and average F3 of the speaker whose vowels were to be normalized (F3sp): 
sp
ref
F
F
k
3
3
=
 . 
(3) 
Applied to our data this procedure brings about mild improvement against the non-
normalized data (see table 3, cf. table 1) and exceeds the previous models in that it 
does not cause unnatural distortion of the vocalic space in the area of the vowel [u]. 
Table 3. Confusion matrix from discrimination analysis of data normalized by a procedure 
proposed in [7]. Intended vowels are in the rows, the classification outcome is in the columns. 
 N&L 
i 
e 
a 
o 
u 
success 
[i] 
534 
63 
1 
0 
2 
89.0 % 
[e] 
42 
401 
115 
24 
18 
66.8 % 
[a] 
0 
59 
472 
65 
4 
78.7 % 
[o] 
2 
17 
44 
366 
171 
61.0 % 
[u] 
46 
8 
0 
118 
428 
71.3 % 
 
Normalization with the use of the information from the outside of an individual 
vowel was also suggested by Gerstman ([8], in [9]), who related the frequency of each 
inspected formant to the speaker’s minimum and maximum for the given formant:  
min
,
max
,
min
,
,
f
sp
f
sp
f
sp
f
sp
norm
F
F
F
F
F
−
−
=
 . 
(4) 
The subscript sp,f refers to the respective formant of the respective speaker. The 
resulting ratio could be optionally multiplied by a constant to produce values easier to 
remember (Gerstman allegedly used 999). 
It is useful to note that this external scaling is a common procedure without any 
special phonetic motivation. Interestingly, applied to our data it produces better 
results than all the methods mentioned above (see table 4). The disadvantage of this 
method is clearly the fact that the correct calculation hinges on the values of the 
minimum and maximum. Given the current state of formant extraction techniques, 
there is a fair possibility of one of these values to be incorrectly measured. The whole 
normalization procedure could therefore be spoiled by one incorrect measurement! 
Another common statistical normalization procedure is that of Lobanov [10], known 
also as z-score normalization. Lobanov’s idea to use it with the arithmetic mean and 
standard deviation of all the vowels produced by the given speaker is simple and 
effective. It is also more robust than Gerstman’s procedure, since the arithmetic mean 
and standard deviation in larger samples are not seriously influenced by individual 
inaccuracies. The normalization effect for our data is about the same as that of 
Nearey’s proposal ([11], in [9]) to use log-transformed values with a speaker-dependent 

196 
J. Volín 
correction term, namely the arithmetic mean of all the Fsp,f values produced by that 
speaker (the chosen base of the logarithm is unimportant): 
(log)
,
,
log
f
sp
f
sp
norm
x
F
F
−
=
 . 
(5) 
The results for both methods are displayed in table 5. It is apparent that the greatest 
improvement against the previous normalization method was recorded for the front 
mid [e] and the back mid [o]. 
Table 4. Confusion matrix from discrimination analysis of data normalized by a procedure 
proposed in [8]. The vowels as intended by speakers are in the rows, the classification outcome 
is in the columns. 
Ger 
i 
e 
a 
o 
u 
success 
[i] 
544 
55 
0 
0 
1 
90.7 % 
[e] 
42 
421 
117 
9 
11 
70.2 % 
[a] 
0 
70 
476 
50 
4 
79.3 % 
[o] 
1 
10 
44 
384 
161 
64.0 % 
[u] 
42 
6 
1 
87 
464 
77.3 % 
Table 5. Confusion matrices from discrimination analyses of 3000 vowels normalized by 
procedures proposed by Lobanov (Lob) and Nearey (Nea). Vowels as intended by speakers are 
in the rows, the classification outcome is in the columns. 
Lob 
i 
e 
a 
o 
u 
success 
[i] 
549 
49 
0 
0 
2 
91.5 % 
[e] 
27 
462 
99 
8 
4 
77.0 % 
[a] 
0 
61 
502 
37 
0 
83.7 % 
[o] 
1 
8 
30 
421 
140 
70.2 % 
[u] 
34 
6 
0 
86 
474 
79.0 % 
Nea 
 
 
 
 
 
 
[i] 
523 
76 
1 
0 
0 
87.2 % 
[e] 
37 
460 
94 
5 
4 
76.7 % 
[a] 
0 
73 
499 
27 
1 
83.2 % 
[o] 
1 
10 
30 
431 
128 
71,8 % 
[u] 
48 
2 
0 
90 
460 
76,7 % 
 
At this point we can compare the methods used so far in one summarizing table. 
Such an overview of success rates is offered in table 6, where the first column of 
figures presents the results pertaining to raw data in Hertz (Hz). The next two 
columns display transformations to psychoacoustic ERB and Bark units. Sussman’s 

 
Normalization of the Vocalic Space 
197 
column (Suss) represents internal ratio models and so does Miller’s, although it 
already introduces external information about F0. The four columns on the right stand 
for more successful external scaling models. Of these, Lobanov’s (Lob) and Nearey’s 
(Nea) are the best. However, when T. Nearey carried out carefully prepared and 
thorough experiments, he found out that neither vowel intrinsic nor extrinsic factors 
can explain existing variation on their own and should perhaps be combined [12]. 
Table 6. Comparison of success rates (in percent) of the tested normalization procedures in 
discrimination analyses of 3000 vowels. For abbreviations see text above. 
Vowel 
Hz 
ERB 
Bark 
Suss 
Mill 
N&L 
Gerst 
Nea 
Lob 
[i] 
88.8 
85.5 
86.2 
85.2 
85.0 
89.0 
90.7 
87.2 
91.5 
[e] 
64.6 
70.2 
68.7 
67.0 
68.3 
66.8 
70.2 
76.7 
77.0 
[a] 
76.2 
78.8 
79.0 
79.0 
81.3 
78.7 
79.3 
83.2 
83.7 
[o] 
57.3 
57.7 
57.8 
55.2 
58.5 
61.0 
64.0 
71,8 
70.2 
[u] 
71.3 
69.2 
70.0 
58.2 
64.7 
71.3 
77.3 
76,7 
79.0 
All 
71.7 
72.3 
72.3 
68.9 
71.6 
73.4 
76.3 
79.1 
80.3 
2.3   The Effects of Extralinguistic Contexts 
When speech communication is under way, the participants perceive much more than 
just acoustic information about the segments and their neighbourhood. Quite 
interestingly, with our 3000 vowels we increased the discrimination success rate of 
the original 71.7 % to 76.1 % (comparable to Gerstman method – see above) without 
much sophistication. Having the extra-linguistic knowledge about the sex of the 
speaker, we merely decreased the formant values by 10 % for every female speaker. 
This one simple step alone caused an average improvement by almost 5 %. That is not 
to say that female vocal tracts are smaller versions of male vocal tracts. It is well 
established that the whole geometry of the male versus female vocal tracts differs. For 
instance, men have proportionally larger larynges and longer pharynges than women. 
Scaling for each vowel would then require different ratios for each formant. In our 
experiment with constant scaling, vowel [i] only improved by 0.5 % while [a] 
improved by 5.8 %.  
However, we could question practicality of looking for scaling constants for each 
formant and each vowel separately for two reasons. First, to use them in larger sets of 
vowels, we would have to know the identity of the vowels in advance, while the 
common requirement is often to normalize speech material before the identities of 
vowels are known. Second, even if we estimate differences in the geometries of vocal 
tracts correctly, we still do not have procedures which treat the gender differences 
acquired through learning. There are studies showing that in many cultures men learn 
to talk differently from women straight from the childhood. The content and degree of 
such differences is clearly culture dependent. 

198 
J. Volín 
The idea of gender and cultural differences suggests a possible influence of a-priori 
expectations, which might take part in perceptual processes. This concept could, 
indeed, lead to a plausible explanation of one of the results in the study by Eklund and 
Traunmüller [13]. Respondents in this study were asked to recognize the gender of the 
speaker and the vowel that was whispered. (In other parts of the study phonated 
vowels were used as well.) For correctly recognized gender the success rate of 
whispered vowel recognition was about 94.8 %. If, however, the respondent did not 
recognize the gender correctly and, subsequently, had false expectations concerning 
the speaker, the success rate of vowel recognition dropped to only about 75 % [13: 8]. 
It follows that if we have a wrong idea about the speaker as a person, we interpret his 
or her articulatory intentions with errors. Clearly, mental processing of the acoustic 
information is not independent of our previous experience with male and female 
individuals we have encountered and talked to. 
Similar conclusion may be drawn from the study by Donald Rubin [14] who asked 
62 university students to listen to about a 4-minute long, clearly spoken text of a 
lecture style. Half of the students were listening to the speech while looking at a 
photograph of a lecturer of their own ethnicity. The other half watched a photograph 
of a lecturer of Asian origin. Although they were listening to the same recording, 
these two groups of respondents differed significantly in comprehension. Listeners 
who believed they were listening to an Asian lecturer achieved worse scores in 
comprehension tests which followed the presentation of the talk [14: 516]. Yet again, 
it is obvious that what we hear is not determined purely by the acoustic information 
passed onto our ears. 
Effects in the same vein are reported in [4: 372] where several studies on vowel 
recognition and vocalic space mapping are mentioned. They generally involve vowel 
identification using stimuli spoken by an ambiguous voice that could belong either to 
a man or to a woman. The listeners are either verbally or visually led to assume that 
they know the gender of the speaker. The vowel fields of those who believe they are 
listening to a woman differ from those who think the speaker is a man. Psycho-
phonetic normalization seems to take into account the image of the speaker. 
3   Conclusion 
We have discussed a few ideas which emerged from hundreds of studies relevant to the 
area of vowel normalization.  Some of the procedures suggested there were tested on 
our data in an attempt to demonstrate that methods which rely on segment-internal 
acoustic structure do not perform as well as methods that take into account information 
from the outside of the normalized segment. We have also illustrated the notion of 
extra-linguistic contexts that seem to play substantial role in speech perception. 
A few points still remain to be raised. First, the role of phonetic contextual 
influences was left out completely so far. However, we do not underestimate their 
impact in the least. Both prosodic and segmental, i.e., coarticulatory conditioning 
must not be neglected. In [2] we showed that immediate segmental contexts seem to 
have greater impact on vocalic formant structure than prosodic influences. In the same 
study we demonstrated that, for instance, the vowel [o] can be recognized with 92% 
precision in one position while in a different location it is recognized with the success 
rate of only 37 %. This outcome led us to the notion of enhancing and degrading 

 
Normalization of the Vocalic Space 
199 
environments. Our set of 3000 vowels (produced by 75 speakers) offered the best 
disciminability of [i] next to palatal [j], [e] between two alveolar occlusives, [a] after 
[r] and before silence, [o] with the closest vowels to it also [o]s plus a neighbouring 
labiodental consonant, and [u] surrounded by labial consonants flanked by labialized 
vowels. Naturally, these findings reflect the logic of articulation as we know it and 
call for further generalization. That is also the aim of our research at present. As it is 
clear that such research requires a larger speech corpus we decided to triple our 
original data set in the immediate future. 
As pointed out in the introduction our research is focused on normalization which 
should serve further linguistic inquiries. We labelled this approach as linguistically 
descriptive and we limited its objectives to transformations that allow comparisons of 
vowels from various speakers while preserving interesting information about their 
personal articulatory habits and linguistically relevant detail. It does not mean that we 
do not want to investigate the psychophonetic mechanisms of vowel perception. We 
are currently collecting perceptual data using the same vowels which were used in [2]. 
At the same time, we do not want to overrate the importance of individual vowels. 
The simple fact that consonantal alphabets can be found in the world while no vowel 
alphabet has been discovered so far indicates that the contrastivity and distributional 
constraints on vowels predetermine them to have a lower identification importance 
than consonants. Moreover, as pointed out above, assembling speech messages from 
single consonants and vowels in natural speech communication is a dubious concept. 
It seems to be the less usual mode to which our perceptual mechanisms might switch 
if the recognition of higher units proves too difficult. In addition, there are reports that 
up to 60 % of a vowel can be deleted in consonantal contexts without affecting vowel 
identification [15: 60]. The deleted part is the stable portion of the vowel, i.e., exactly 
the portion which is usually used in normalization experiments. 
With highly successful automatic recognition applications based on MFCC or PLP 
coefficients on the one hand and psychophonetic and neurophonetic revelations on the 
other hand, we might wonder if linguistically descriptive normalization can still hold 
its place in the field of speech research. Our answer is positive. To explain 
articulatory mechanisms from coarticulation phenomena requires techniques of 
comparison, in which certain features of speakers’ identity are neutralized while 
others remain intact. The same is true about the research in dialectal variation, in 
sociophonetics, in the study of speech styles, the research of affective speech etc., etc. 
We will, therefore, continue in the search for such techniques to offer them to the 
speech research community for further utilization. 
 
Acknowledgement. This research was supported by the grant of Czech Ministry of 
Education MSM0021620825 and the European Union grant MRTN-CT-2006-035561. 
References 
1. Sussman, H.M.: A neuronal model of vowel normalization and representation. Brain & 
Language 28, 12–23 (1986) 
2. Volín, J., Studenovský, D.: Normalization of Czech vowels from continuous read texts.  
In: Proceedings of the 16th ICPhS, pp. 185–190. IPA & UDS, Saarbrücken (2007) 

200 
J. Volín 
3. Miller, J.D.: Auditory-perceptual interpretation of the vowel. Journal of the Acoustical 
Soc. Am. 85, 2114–2134 (1989) 
4. Johnson, K.: Speaker normalization in speech perception. In: Pisoni, D.B., Remez, R.E. 
(eds.) The Handbook of Speech Perception, pp. 363–389. Blackwell, Oxford (2005) 
5. Slawson, A.W.: Vowel quality and musical timbre as functions of spectrum envelope and 
fundamental frequency. Journal of the Acoustical Soc. Am. 43(1), 87–101 (1968) 
6. Johnson, K.: Contrast and normalization in vowel perception. Journal of Phonetics 18, 
229–254 (1990) 
7. Nordstöm, P., Lindblom, B.: Normalization procedure for vowel formant data. In: 
Proceedings of the 8th ICPhS. IPA, Leeds (1975) 
8. Gerstman, 
L.: 
Classification 
of 
self-normalized 
vowels. 
IEEE 
Trans. 
Audio 
Electroacoust AU-16, 78–80 (1968) 
9. Adank, P., Smits, R., van Hout, R.: A comparison of vowel normalization procedures for 
language variation research. Journal of the Acoustical Soc. Am. 116(5), 3099–3107 (2004) 
10. Lobanov, B.M.: Classification of Russian vowels spoken by different speakers. Journal of 
the Acoustical Soc. Am. 49, 606–608 (1971) 
11. Nearey, T.M.: Phonetic Feature Systems for Vowels. Indiana University Linguistics Club, 
Indiana (1978) 
12. Nearey, T.M.: Static, dynamic, and relational properties in vowel perception. Journal of the 
Acoustical Soc. Am. 85(5), 2088–2113 (1989) 
13. Eklund, I., Traunmüller, H.: Comparative study of male and female whispered and 
phonated versions of the long vowels of Swedish. Phonetica 54, 1–21 (1997) 
14. Rubin, D.L.: Non-language factors affecting undergraduate’s judgments of non-native 
English speaking teaching assistants. Research in Higher Education 33(4), 511–531 (1992) 
15. Rosenblum, L.D.: Primacy of multimodal speech perception. In: Pisoni, D.B., Remez, R.E. 
(eds.) The Handbook of Speech Perception, pp. 51–78. Blackwell, Oxford (2005) 

Gaze Behaviors for Virtual Crowd Characters
Helena Grillon, Barbara Yersin, Jonathan Ma¨ım, and Daniel Thalmann
EPFL, Lausanne, Switzerland
{helena.grillon,barbara.yersin,jonathan.maim,daniel.thalmann}@epfl.ch
Abstract. Nowadays, crowds of virtual characters are used in many do-
mains such as neurosciences, psychology, and computer sciences. Since as
human beings, we are natural experts in human being representation and
movement, it makes it that much harder to correctly model and animate
virtual characters. This becomes even more challenging when consider-
ing crowds of virtual characters. Indeed, in addition to the representation
and animation, there is the mandatory trade-oﬀbetween rich, realistic
behaviors and computational costs. In this paper, we present a crowd en-
gine, to which we introduce and extra layer which allows its characters
to produce gaze behaviors. We thus enhance crowd realism by allowing
the characters composing it to be aware of their environment and other
characters and/or a user.
1
Introduction
Real-time crowd simulation has become a topic of increasingly high interest.
Virtual crowds are used in many domains ranging from the movie or gaming
industries to neurology and psychology. Simulating real-time crowds is a very
challenging topic. First, as humans, we are natural experts when it comes to
human representation and animation. It makes it that much harder to create
realistic virtual characters, both in aspect and movement. In addition, dealing
with crowds of characters rises a mandatory trade-oﬀbetween rich and realis-
tic representations and behaviors, and computational costs. Individual character
animation may provide realistic results but is computationally expensive. Con-
versely, global crowd behavior design is much faster but results in a loss of
character individuality.
We believe that an important aspect which can greatly enhance crowd sim-
ulation realism is for the characters to be aware of their environment, other
characters and/or a user. This can partly be achieved with the navigation and
path planning algorithms present in our crowd engine.
In this paper, we propose a method, integrated in the crowd simulation
pipeline, to obtain more advanced behaviors than what navigation can provide.
To add attentional behaviors to crowds, we are confronted to two issues. The
ﬁrst one is to detect the points of interest for characters to look at. The second
one is to edit the character motions for them to perform the gaze behavior.
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 201–213, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

202
H. Grillon et al.
Fig. 1. Examples of gaze behaviors for virtual crowd characters
2
Related Work
2.1
Real-Time Crowd Simulation
Simulating crowds in real time is a challenging task that is usually divided into
the following topics: rendering, navigation, and behavior.
To render large crowds, the best approach is to use a level-of-detail ap-
proach [1]: high quality rendering techniques are used for virtual humans close
to the camera, while faster, less accurate methods are exploited at farther dis-
tances. Many solutions use impostors for their eﬃciency. Impostors are 2D an-
imated images, mapped onto a quad formed by 2 triangles. One pioneer work
in this domain was achieved by Dobbyn et al. [2]: they used pre-computed rigid
meshes in front of the camera, and switched seamlessly to impostors for more
distant characters. Later, Millan and Rudomin [3] presented a solution combin-
ing impostors and instanced geometries, maximizing the use of the GPU. More
recently, Kavan et al. [4], presented a virtual human new level of detail, called
polypostor: they are composed of a small set of 2D polygons, and animated by
displacing their vertices. To avoid crowds of cloned entities, much work has been
achieved on color variety, or how to modulate the colors of body parts for each
instance of a same characters [5,2,6]. A recent study [7] has demonstrated that
variety at the color level has much more impact on the user than varying the
animations solely.
In the domain of navigation and behavior, many approaches have been studied
to eﬃciently simulate large crowds. The pioneer work of Reynolds [8,9] presented
simple rules to easily steer individuals for performing basic actions, such as
“seek”, “ﬂee”, etc. Other important branches of study on crowd navigation were
introduced: approaches can be Physics-based [10,11],
sociology/psychology-
based [12,13], example-based [14,15,16], and hybrid [13,17].
2.2
Simulation of Attentional Behaviors
Attention Models. The synthesis of human vision and perception is a complex
problem which has been tackled in many diﬀerent ways. Models of synthetic
vision based on memory have been developed for the navigation of characters
[18,19]. Other proposed models relied on bottom-up, stimulus-driven rules [20,21]
and saliency maps [25,26,27,28]. Similarly, much work has been conducted in
the simulation of visual attention and gaze in Embodied Conversational Agents

Gaze Behaviors for Virtual Crowd Characters
203
[22,23,24]. On a diﬀerent note, [29] described an eye movement model based
on saccade empirical models and statistical models of eye-tracking data, and
[30] proposed a head-neck model based on biomechanics. All these methods give
very convincing results but are prohibitive in terms of computational times or
not applicable to crowd simulations.
Motion Editing. A large category of motion editing techiniques use analytic
Inverse Kinematics (IK) [31,32]. In this domain, [33] proposed a method to edit a
pre-existing animation to satisfy a set of user deﬁned constraints for human like
ﬁgures. [34] extended this method to remove footskate from motion captured an-
imations. [35] used Kalman ﬁlters and a set of rules to assign varying importance
to a set of tasks which they then solved with a dedicated IK solver. [36] pro-
posed a hierarchical Cyclic Coordinate Descent algorithm to deal with spacetime
constraints for human-like ﬁgures. These analytic methods are dedicated to the
positionning of end-eﬀectors, whereas we are interested in controlling the ﬁnal
orientation of the eyes, head, and spinal joints over time. Several other methods
use Jacobian based IK solvers to edit motions [37,38]. While these methods are
generic enough to possibly use any kind of constraints, the use of Jacobian in-
version causes prohibitive computational costs that are not compatible with our
framework.
3
System Overview
Our crowd engine is composed of 3 main elements, illustrated in Figure 2: the
variety component, the navigation component, and the real-time component.
The variety and navigation components are processed oﬀ-line, before launching
the simulation, while the real-time component represents our pipeline, which is
run at each frame of the simulation.
The variety component takes care of the visual appearance of pedestrians com-
posing the crowd: it generates the human instances, vary their color, shape, and
height. This component is also responsible for generating all the animations that
will be used at runtime. Many locomotion animations are generated at various
speeds with a locomotion engine (see next section), and some quieter actions,
such as talking, standing, sitting, are hand-designed. Variations are applied to
these animations, e.g., hand on the hip, in the pocket, and ﬁnally, they are all
stored in a dedicated database that is later uploaded at runtime.
The navigation component is responsible for analyzing and structuring the
environment. From the geometry of an environment, we create a navigation
graph [39,40], which is then used to compute many paths for pedestrians to
follow. Semantic information can also be input in the navigation graph to locally
trigger special behaviors in the crowd.
Finally, the real-time component is our main pipeline. It is composed of 4
steps, which are successively processed as follows:
– The Scaler applies scores to each vertex of our navigation graph in order to
know which level of detail is applied to which humans.

204
H. Grillon et al.
– The Simulator navigates each individual towards its next waypoint, using
diﬀerent algorithms, depending on the score obtained in the Scaler.
– The Animator has to update the posture of each virtual human. Once again,
depending on the score obtained by each virtual human, the animation pro-
cess is diﬀerent.
– The Renderer ﬁrst takes care of computing the shadows for the characters as
well as the scene. In a second pass, it displays the crowds and environment.
The attentional behaviors are added to the walking character motion after the
Animator part of the pipeline, in the Gaze module depicted in Figure 2. The ﬁrst
step is to deﬁne the interest points, i.e. the points in space which we consider
interesting and which therefore attract the characters’ attention. We use several
diﬀerent methods to do this depending on the result we want to obtain:
– The interest points can be deﬁned as regions in space which have been de-
scribed as interesting (see Section 4). In this case, they will be static.
– They can be deﬁned as characters evolving in space. All characters may
then potentially attract the attention of other characters as long as they are
in their ﬁeld of view. In this case, we have dynamic constraints, since the
characters move around.
Fig. 2. System overview

Gaze Behaviors for Virtual Crowd Characters
205
– They can be deﬁned as a user if we track a user interacting with the system.
A coupled head- and eye-tracking setup allows us to deﬁne the position of
the user in the 3D space as depicted in Figure 3. Characters may then look
at the user.
Fig. 3. A user is tracked in a CAVE environment. His position and orientation are used
to interact with the characters walking in the virtual environment.
The second step to obtain the desired attentional behaviors consists in com-
puting the displacement map which allows for the current character posture to
achieve the gaze posture, i.e. to satisfy the gaze constraints. Once the displace-
ment map has been computed, it is dispatched to the various joints composing
the eyes, head, and spine in order for each to contribute to the ﬁnal posture. Fi-
nally, this displacement is propagated in time in order for the looking or looking
away motions to be smooth, natural, and human-like.
4
Crowd Engine
As introduced in Section 3, our crowd engine is divided into 3 important ele-
ments: the variety and navigation components, that are pre-processed, and the
real-time pipeline, which is run at each frame of the simulation.
Variety. To generate thousands of individuals, a naive approach is to design as
many humans as there are people in the crowd. Obviously, such an approach is
impossible, since it would require armies of designers, and an inﬁnite memory.
The common and more reasonable approach is to use human templates. A human
template is a virtual human deﬁned by its skeleton, its mesh, which is skinned
to the skeleton, and its set of textures. To create large crowds, a small group of
human templates are instantiated several times. For each instance, one texture
is randomly chosen within the template’s available set. Then, color and shape
variety techniques are applied, so that instances of a same template and using the
same texture are still diﬀerent [6]. To generate many locomotion animations for
each human template, we use a motion-capture-based locomotion engine [41,42].

206
H. Grillon et al.
This allows us to obtain adapted locomotion cycles that will be used at runtime
to animate the characters. Using an IK system, upper body variations are also
applied to further vary the crowd: the characters are thus able to have a hand in
their pocket, or next to their ear to make a phone call. Note that the generated
animations are skeletal, i.e., they can be applied to meshes that are deformed
at runtime. In our crowd engine, we use such meshes only at the forefront of the
camera, for the animation process is very costly. The second and third levels of
detail we use, i.e., static meshes and impostors, are pre-computed and saved in
the database too.
Navigation. Given an environment geometry, a navigation graph can be com-
puted [39,40]: the vertices of the graph represent circular zones where a pedes-
trian can walk freely without the risk of colliding with any static object
composing the environment. Graph edges represent connections between ver-
tices. In the environment, they are viewed as intersections (or gates) between
two circular zones (vertices). From a navigation graph, path requests can be
issued from one vertex to another. Using an algorithm based on Dijkstra’s, we
are able to devise many diﬀerent paths that join one point of the environment
to another one. It is possible to provide the navigation graph with a second
model of the environment, which usually has a much more simple geometry,
annotated with information. This second model is automatically analyzed, its
meta-information retrieved, and associated to the corresponding vertices. Us-
ing this technique allows to make virtual humans look at speciﬁc points in the
environment, for instance. We further detail this technique in Section 5.
Real-Time Pipeline. The ﬁrst stage of our pipeline is the Scaler. The Scaler
receives in input the navigation graph, and the view frustum of the camera.
Based on this data, each vertex of the graph, and thus each pedestrian situated
inside this vertex, is provided with two scores. The ﬁrst one determines which
navigation algorithm should be used for simulating the characters, but is beyond
the scope of this paper. The second score determines which level of detail (LOD)
will be used to render the characters in the vertex: deformable mesh (skeletal
animation skinned at runtime), static mesh (pre-computed animations of the
same mesh), or impostors (2D images representing frames of animation of the
character). The second stage, the simulator, employs the ﬁrst score to make
each virtual human progress on its path. The Animator uses the second score to
correctly update each pedestrian’s position:
– If the LOD is the highest, the Animator needs to update the skeleton pos-
ture of human instances. The skinning of the mesh is later achieved in a
shader, at the Renderer phase. With this high-level representation, it is also
possible to add more detailed animations, that can be either pre-computed,
or procedural. Procedural gaze variations are inserted in the Gaze part of
the pipeline, after the Animator, but only to those pedestrians having the
highest LOD. We further detail how this is done in the next Section.
– If the LOD is lower (static mesh or impostor), the job of the Animator
is much diﬀerent: it consists in retrieving the next frame of animation of

Gaze Behaviors for Virtual Crowd Characters
207
Fig. 4. Various examples depicting the crowd simulation engine at work
the LOD, i.e., a static and already deformed mesh, or a 2D image of the
same posture, and send it to the Renderer. Note that with such LOD, it is
impossible to provide humans with procedural animations. The gaze system
presented below is thus limited to deformable meshes. This limitation is
however not too troubling, because characters using these representations
are farther from the camera, and thus, won’t be visible since they will be
either hidden by other characters or too far away for the user to be able to
see their eyes. Moreover, it is possible to deﬁne the range from the camera
at which the highest LOD will remain.
The last stage of the pipeline is the Renderer. It takes care of rendering the
environment and the crowd, as well as their respective shadows. Once again, the
rendering process for a human instance varies, depending on the LOD it uses. If
the representation is a deformable mesh, then the skeleton posture is sent to the
GPU, where the deformation of the mesh will be achieved. If it is a static mesh,
the deformation has been pre-computed and the work is limited to rendering the
vertices of the mesh. Finally, in the case of impostors, the shader receives the
correct coordinates of the texture that should be mapped onto two triangles.
Figure 4 depicts the results obtained with this crowd engine. It enables us to
realistically simulate hundreds of characters in real-time.
5
Gaze Behaviors for Crowds
In the previous section, we explained the overall crowd engine pipeline. In the
present section, we explain how we adapt the character motions to obtain the
desired attentional behaviors. As mentioned, this motion adaptation takes place
at the end of the Animator part of the pipeline, in the Gaze module depicted in
Figure 2.
5.1
Interest Points
The ﬁrst step is to deﬁne where and when each character should look. As men-
tioned in Section 3, the method to do this varies depending on the setup and
the results we want to obtain.

208
H. Grillon et al.
The ﬁrst is to use the meta-information present in the environment model as
described in Section 4. This meta-information allows to describe the various envi-
ronmental elements as “interesting” to look at. If this is the case, they will attract
character attention. Characters will perform the gaze behavior in proximity of
these elements and as long as they are in their ﬁeld of view. These types of inter-
est points are always at the same position in the scene; they are static elements.
The second method we use to deﬁne interest points is by assigning them to other
characters in the scene. We may assign diﬀerent levels of interest to diﬀerent char-
acters. In this way, some of them will attract attention more than others. As for
the environmental elements, characters will attract the attention of other char-
acters when in their proximity and as long as they remain in their ﬁeld of view.
These types of interest points are moving entities; they are therefore dynamic.
Finally, the third type of interest point is the user. A person using our system
may be tracked using a coupled head- and eye-tracker. In this way, we can track
the user position and gaze in the crowd engine 3D space. In this case, the user
will be the interest point. Moreover, since we can track the user’s gaze, the user’s
interest points (where he looks at) may become interest points for the characters
in the environment. They will thus seem to unconsciously imitate the user. They
will seem to try and ﬁnd out what the user is looking at.
5.2
Motion Adaptation
In order to obtain convincing results, we then have to adapt the character mo-
tions in order for them to look at the interest points. Since the characters and
the interest points may be dynamic, we have to compute the joint displacements
to be applied to the base motion at each timestep.
The skeletons we use are composed of 86 joints. Our method adjusts 10 of
them: 5 spinal cord joints, 2 cervical joints, 1 head joint and 2 eye joints, in
order for the characters to align their gaze to the interest points. By considering
only this subset of the full skeleton, we greatly reduce the complexity of our
algorithm. This allows us to have very small computational times and thus to
animate a large number of characters.
Our method consists of two distinc phases. The ﬁrst one computes the dis-
placement maps to be applied to the various joints in order to satisfy the gaze
constraints. We name this spatial resolution. The second component is the tem-
poral resolution. This allows to propagate the displacement maps over an auto-
matically deﬁned number of frames onto the original motion. We thus ensure a
smooth and continuous ﬁnal movement.
Spatial Resolution. The purpose of the spatial resolution is to ﬁnd a displace-
ment map that modiﬁes the initial motion in order to satisfy a given gaze con-
straint At each timestep, and for each deformable mesh in the crowd, if there
is an active constraint, we launch an iterative loop starting with the bottom of
the kinematic chain (lumbar verterbrae) and ending with the top of the kine-
matic chain (the eyes). At each iteration, we calculate the total remaining rota-
tion to be done by the average eyes position (global eye) to satisfy the constraint.

Gaze Behaviors for Virtual Crowd Characters
209
Fig. 5. Various examples depicting the types of possible gaze behaviors
However, since the eyes are not the only joints to adjust, we dispatch this rotation
to the other recruited joints.
To determine the contribution of each joint to the complete rotation we take
inspiration from the work of [43]. The authors proposed a set of formulae to
deﬁne the angle proportions to be assigned to the spine joints depending on
the type of rotation to be done (pitch, yaw or roll). We use the formula they
propose for the linearly increasing rotation distribution around the vertical axis.
In our model, the rotations around the sagittal and frontal axes are very small
in comparison to those in the vertical axis. We therefore keep the same formula
for all types of rotations:
ci = (−(i −n))(
2
n(n −1)) i = 1...9
(1)
where n is total number of joints through which to iterate and i is the joint
index with i = 1 the lowest lumbar vertebra and i = 9 the global eye. At each
step, ci determines the percentage of remaining rotation to be assigned to joint
i. The total rotation to be done by each joint for the character to satisfy the
constraint may then by calculated by spherical linear interpolation (slerp) using
these contribution values.
The remaining rotation to be done by each eye joint is then computed in order
for them to converge on the interest point. Moreover, for interest points in the
30◦composing the central foveal area, only the eye joints are recruited. For the
15◦farther on each side composing the central vision area, only the eye, head,
and cervical joints are recruited. Small movements therefore do not recruit the
heavier joints.
Temporal Resolution. The time it takes to look at something or someone
depends, amongst other things, on where it is placed in the ﬁeld of view. It will
take less time to perform the gaze movement to look at something just in front
than at something in the periphery.
When initiating a gaze movement, we therefore deﬁne how long it will take
to perform it. We ﬁrst deﬁne the upper and lower bounds for the movement
duration. The lower bound is set to 0, which corresponds to no movement,
i.e. the character already facing the interest point. The upper bound is set to

210
H. Grillon et al.
2 seconds, which corresponds to a 180◦movement. We then simply interpolate
between the two to obtain the duration of the gaze movement.
Moreover, this duration is diﬀerent if considering the eye joints, the head and
cervical joints, or the joints composing the remainder of the spine. The duration
for the head to move is 2 times smaller than for the spine. Similarly, the duration
for the eyes to converge is 5 times smaller than for the head. In this way, we allow
for the lighter joints to move more rapidly than the others. The eyes therefore
converge on the interest point well before any of the other joints attain their
ﬁnal posture. Our ﬁnal movement therefore allows for the eyes to converge on
the interest point and then recenter with respect to the head as the remainder
of the joints move to satisfy the constraint.
As previously mentioned, some of the gaze constraints may be dynamic, i.e. in
the case where the constraint is either another moving character or the system
user. We therefore recompute the displacement map to satisfy the constraint at
each timestep. We can assume that the constraint’s position from one frame to
the next one does not change much. We therefore recompute the rotation to
be done at each frame but maintain the total contribution ci to apply which
we calculated before the initiation of the gaze motion. However, we reset the
contributions to 0 if the gaze constraint changes. More speciﬁcally, when the
current constraint location is farther than a pre-determined threshold from the
constraint location at the previous frame. The newly calculated rotations to
be performed by the joints to attain the new constraint’s position are then
distributed over the appropriate number of frames.
Finally, if the gaze behavior is deactivated, either because the character has
been looking at a point of interest for too long or if there are no more interest
points, the character will look back in front of him.
The added gaze motions strongly increase the human-like behaviors of crowd
characters. By adding this functionality to the crowd simulation engine, we ob-
tain a crowd of characters which seem to be aware of their environment and of
other characters, as depicted in Figure 5.
6
Conclusion
In this paper, we have described how to improve the realism of a crowd simulation
by allowing its pedestrians to be aware of their environment and of the other
characters present in this environment. They can even seem to be aware of
a user interacting with this environment. We have explained the various setups
which allow for crowd characters to gaze at environment objects, other characters
or even a user. Finally, we have described a method to add these attentional
behaviors in order for crowd characters to seem more individual.
Acknowledgements
The authors would like to thank Mireille Clavien for the design of characters
and environments. They would also like to thank the Swiss National Research
Foundation for sponsoring this research.

Gaze Behaviors for Virtual Crowd Characters
211
References
1. Ryder, G., Day, A.M.: Survey of Real-Time Rendering Techniques for Crowds.
Computer Graphics Forum 24(2), 203–215 (2005)
2. Dobbyn, S., Hamill, J., O’Conor, K., O’Sullivan, C.: Geopostors: a real-time geom-
etry / impostor crowd rendering system. In: SI3D 2005: Proceedings of the 2005
symposium on Interactive 3D graphics and games, pp. 95–102 (2005)
3. Millan, E., Rudomin, I.: Impostors and pseudo-instancing for GPU crowd ren-
dering. In: GRAPHITE 2006: Proceedings of the 4th international conference on
Computer graphics and interactive techniques in Australasia and Southeast Asia,
pp. 49–55 (2006)
4. Kavan, L., Dobbyn, S., Collins, S., Zara, J., O’Sullivan, C.: Polypostors: 2D polygo-
nal impostors for 3D crowds. In: 2008 ACM SIGGRAPH Symposium on Interactive
3D Graphics and Games, pp. 149–155 (2008)
5. Tecchia, F., Loscos, C., Chrysanthou, Y.: Image-based crowd rendering. IEEE
Computer Graphics and Applications 22(2), 36–43 (2002)
6. Ma¨ım, J., Yersin, B., Thalmann, D.: Unique Instances for Crowds. In: IEEE Com-
puter Graphics and Applications (to appear, 2009)
7. McDonnell, R., Larkin, M., Dobbyn, S., Collins, S., O’Sullivan, C.: Clone attack!
Perception of crowd variety. ACM Transactions on Graphics 27(3), 1–8 (2008)
8. Reynolds, C.W.: Flocks, Herds, and Schools: A Distributed Behavioral Model. In:
SIGGRAPH 1987: Proceedings of the 14th International Conference on Computer
Graphics and Interactive Techniques, vol. 21(4), pp. 25–34 (1987)
9. Reynolds, C.W.: Steering Behaviors for Autonomous Characters. In: Game Devel-
opers Conference (1999)
10. Helbing, D., Molnar, P.: Phys. Rev. E51, 4282 (1995)
11. Treuille, A., Cooper, S., Popovi´c, Z.: Continuum Crowds. ACM Transactions on
Graphics 25(3), 1160–1168 (2006)
12. Musse, S.R., Thalmann, D.: A Model of Human Crowd Behavior: Group Inter-
Relationship and Collision Detection Analysis Computer Animation and Simula-
tion. In: Proc. Workshop of Computer Animation and Simulation of Eurographics
1997, pp. 39–51 (1997)
13. Pelechano, N., Allbeck, J.M., Badler, N.I.: Controlling individual agents in high-
density crowd simulation. In: Proceedings of the ACM SIGGRAPH/Eurographics
symposium on Computer animation (2007)
14. Lerner, A., Chrysanthou, Y., Lischinski, D.: Crowds by example. Computer Graph-
ics Forum (Eurographics 2007) 26(3), 655–664 (2007)
15. Paris, S., Pettr´e, J., Donikian, S.: Pedestrian steering for crowd simulation: A
predictive approach. Computer Graphics Forum 26(3), 665–675 (2007)
16. Lee, K.H., Choi, M.G., Hong, Q., Lee, J.: Group behavior from video: a data-driven
approach to crowd simulation. In: Proceedings of the 2007 ACM SIGGRAPH Eu-
rographics symposium on Computer animation, pp. 109–118 (2007)
17. Yersin, B., Ma¨ım, J., Morini, F., Thalmann, D.: Real-Time Crowd Motion Plan-
ning: Scalable Avoidance and Group Behavior. The Visual Computer Jour-
nal 24(10), 859–870 (2008)
18. Kuﬀner Jr., J.J., Latombe, J.-C.: Fast Synthetic Vision, Memory, and Learning
Models for Virtual Humans. In: Proceedings of Computer Animation, pp. 118–127
(1999)
19. Peters, C., O’Sullivan, C.: Synthetic Vision and Memory for Autonomous Virtual
Humans. Computer Graphics Forum 21(4), 743–752 (2002)

212
H. Grillon et al.
20. Hill, R.: Modeling Perceptual Attention in Virtual Humans. In: Proceedings of
Computer Generated Forces and Behavioral Representation (1999)
21. Chopra Khullar, S., Badler, N.I.: Where to Look? Automating Attending Behaviors
of Virtual Human Characters. Autonomous Agents and Multi-Agent Systems 4(1-
2), 9–23 (2001)
22. Peters, C., Pelachaud, C., Bevacqua, E., Mancini, M., Poggi, I.: A Model of At-
tention and Interest Using Gaze Behavior. In: Panayiotopoulos, T., Gratch, J.,
Aylett, R.S., Ballin, D., Olivier, P., Rist, T. (eds.) IVA 2005. LNCS, vol. 3661, pp.
229–240. Springer, Heidelberg (2005)
23. Gu, E., Badler, N.: Visual Attention and Eye Gaze During Multiparty Conversa-
tions with Distractors. In: Gratch, J., Young, M., Aylett, R.S., Ballin, D., Olivier,
P. (eds.) IVA 2006. LNCS, vol. 4133, pp. 193–204. Springer, Heidelberg (2006)
24. Lance, B., Marsella, S.: Emotionally Expressive Head and Body Movement During
Gaze Shifts. In: Pelachaud, C., Martin, J.-C., Andr´e, E., Chollet, G., Karpouzis,
K., Pel´e, D. (eds.) IVA 2007. LNCS, vol. 4722, pp. 72–85. Springer, Heidelberg
(2007)
25. Itti, L., Dhavale, N., Pighin, F.: Realistic Avatar Eye and Head Animation Using
a Neurobiological Model of Visual Attention. In: Proceedings of the Symposium
on Optical Science and Technology, vol. 5200, pp. 64–78 (2003)
26. Peters, C., O’Sullivan, C.: Bottom-up visual attention for virtual human animation.
In: Proceedings of Computer Animation and Social Agents, pp. 111–117 (2003)
27. Marchand, E., Courty, N.: Controlling a camera in a virtual environment. The
Visual Computer 18(1), 1–19 (2002)
28. Kim, Y., Hill Jr., R.W., Traum, D.R.: A Computational Model of Dynamic Per-
ceptual Attention for Virtual Humans. In: Proceedings of Behavior Representation
in Modeling and Simulation (2005)
29. Lee, S.P., Badler, J.B., Badler, N.I.: Eyes alive. In: Proceedings of ACM SIG-
GRAPH. Annual Conference Series, pp. 637–644 (2002)
30. Lee, S.-H., Terzopoulos, D.: Heads up!: biomechanical modeling and neuromuscu-
lar control of the neck. In: Proceedings of ACM SIGGRAPH. Annual Conference
Series, pp. 1188–1198 (2006)
31. Badler, N.I., Korein, J.D., Korein, J.U., Radack, G.M., Shapiro Brotman, L.: Po-
sitioning and animating human ﬁgures in a task-oriented environment. The Visual
Computer 1(4), 212–220 (1985)
32. Tolani, D., Goswami, A., Badler, N.I.: Real-Time Inverse Kinematics Techniques
for Anthropomorphic Limbs. Graphical models 62(5), 353–388 (2000)
33. Lee, J., Shin, S.Y.: A hierarchical approach to interactive motion editing for human-
like ﬁgures. In: Proceedings of ACM SIGGRAPH. Annual Conference Series, pp.
39–48 (1999)
34. Kovar, L., Schreiner, J., Gleicher, M.: Footskate cleanup for motion capture editing.
In: Proceedings of the ACM SIGGRAPH/Eurographics symposium on Computer
animation, pp. 97–104 (2002)
35. Shin, H.J., Lee, J., Shin, S.Y., Gleicher, M.: Computer puppetry: An importance-
based approach. ACM Transactions on Graphics 20, 67–94 (2001)
36. Kulpa, R., Multon, F., Arnaldi, B.: Morphology-independent representation of mo-
tions for interactive human-like animation. In: EURORAPHICS 2005, vol. 24(3),
pp. 343–352 (2005)
37. Choi, K.-J., Ko, H.-S.: Online motion retargetting. The Journal of Visualization
and Computer Animation 11(5), 223–235 (2000)

Gaze Behaviors for Virtual Crowd Characters
213
38. Le Callennec, B., Boulic, R.: Interactive motion deformation with prioritized con-
straints. In: Proceedings of the ACM SIGGRAPH/Eurographics symposium on
Computer animation, pp. 163–171 (2004)
39. Pettr´e, J., de Heras Ciechomski, P., Ma¨ım, J., Yersin, B., Laumond, J.-P., Thal-
mann, D.: Real-time navigating crowds: scalable simulation and rendering. Com-
puter Animation and Virtual Worlds 17(34), 445–455 (2006)
40. Pettr´e, J., Grillon, H., Thalmann, D.: Crowds of Moving Objects: Navigation Plan-
ning and Simulation. In: Proceedings of IEEE International Conference on Robotics
and Automation (2007)
41. Glardon, P., Boulic, R., Thalmann, D.: PCA-based walking engine using motion
capture data. In: Proc. of Computer Graphics International (2004)
42. Glardon, P., Boulic, R., Thalmann, D.: A coherent locomotion engine extrapolating
beyond experimental data. In: Proc. of Computer Animation and Social Agent
(2004)
43. Boulic, R., Ulicny, B., Thalmann, D.: Versatile Walk Engine. Journal of Game
Development 1(1), 29–43 (2004)

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 214–226, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Gestural Abstraction and Restatement: 
From Iconicity to Metaphor 
Nicla Rossini 
Dipartimento di Studi Umanistici, Università del Piemonte Orientale; BPSE Program, 
Université du Luxembourg; Dipartimento di Linguistica, Università di Pavia 
nicla.rossini@unipv.it, nicla.rossini@uni.lu 
Abstract. The question of abstraction and metaphor in gesture is particularly 
controversial. Some scholars such as David McNeill, who first introduced this 
concept for gestures in a systematic way, think that gesture can convey abstract 
meaning and metaphoric thought, while others believe that gestures can only be 
considered to be iconic representations. This question will be addressed here by 
means of an analysis of cases of “on-line” abstraction in the gestural production 
concurrent with restatements of path descriptions.  
Keywords: Gestural abstraction, Gestural metaphor, Iconicity, Metaphoric 
thought, Restatement. 
1   Introduction 
The notion of metaphoricity and abstraction in gestures occurring during speech is 
particularly controversial, because it concerns important theoretical outcomes about 
the roles and functions of gesture and speech within the economy of human language.  
Some scholars who concern themselves with gesture studies are in fact convinced 
that gestures have a role relative to language and communication that is perfectly similar 
to that of speech, but with different pragmatic and referential functions (see [1] as one of 
the initiators of this theoretical position), while others deny the idea that gestures are 
closely linked to speech and integral to human language (see for instance [2]). Within 
this debate is the question of metaphoricity and abstraction in speech and gesture. 
While metaphoricity in speech has been widely addressed from a variety of 
different perspectives (see e.g. [3]), less is known about the processes of abstraction1 
as conveyed by gestures. The idea of metaphoricity in gesture has been put forward 
and maintained by David McNeill (see e.g. [4] and [5]), but questioned by other 
scholars who see McNeill’s “metaphoric gestures”, or “metaphors” as ultimately 
iconic (see for instance [6]).  
                                                           
1 Abstraction is here intended both from a cognitive and semiotic point of view, as a shift from 
a concrete and complete depiction of the referent to a more abstract, i.e., less hypostatized and 
concrete semiosis in which the content of information related to a concept is reduced. In this 
sense, abstraction is determinant for the evolution from iconicity to metaphor in thought, 
speech, and gestural representation. 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
215 
This question is addressed here by means of an analysis of cases of restatement in 
route direction and description, where an object or path is described twice. The cases 
discussed here are available from both face-to-face quasi-spontaneous conversational 
and “blind” map-task activities, so called because participants are asked to interact 
with each other on a map, in close proximity with one another, but without the 
possibility of seeing the other person (for further information about the “map-task 
project”, please refer to [7]). 
The data available from quasi-spontaneous descriptions of objects or paths, in 
coincidence with a linguistic error that forces the speaker to rephrase his/her description 
are particularly interesting for several reasons, among which are the observed “gestural 
rephrasing” that perfectly co-occur with restatements in speech, and the observed 
increasing in abstraction in gesture – but rarely in speech – during restatement. 
The iconic gestures usually accompanying speech during the expression of spatial 
concepts such as objects and paths have been in fact found to undergo a sort of on-
line process of abstraction from first performance to restatement, even if the 
restatement in question immediately follows the first performance. 
Such phenomena of “on-line” abstraction are suitable to serve as indices of the 
emergence and application of metaphoric thought during spontaneous interactions 
between individuals and are thus evidence in favour of the hypothesis of a substantial 
differentiation and resilience of metaphoric thought put forward by McNeill [4, 5], 
not to mention the potential applications of these findings to ECAs in order to achieve 
a more “natural” linguistic and interactive performance.  
1.1   Metaphoric Thought, Abstraction, and Gesture 
The enquiry into metaphoric thought is particularly broad and involves a wide number 
of research fields, the totality of which should be taken into account. For the purposes 
of this chapter, we will focus on the emergence of metaphorical thought, both from a 
physiological and a psychological perspective. 
Several theories on the emergence of metaphor and abstraction have been put 
forward over the years. Despite the number of frameworks by which metaphor and 
abstraction are treated, the functional hypothesis reckoning that metaphoric thought 
stems from the Economy Principle of Language2 appears to be shared – even if tacitly 
– by the great majority of scholars who have concerned themselves with this topic 
(see, e.g., [8], [9], [10]). Such a concept of language, that relates linguistic phenomena 
to two basic contrastive phenomena (such as the Principle of Least Effort on the one 
hand and linguistic variation on the other), also seems to find corroboration in recent 
discoveries about the neural structure of the human brain. In other words, the need for 
creating new means of categorization and expression starting from finite resources 
may be illuminated by focusing on the structure of human brain itself, whose neurons 
have been found to be inter-wired when firing simultaneously (see, e.g., [11]). The 
fact that even when not in physical reciprocity neurons tend to create connections 
when solicited at the same time could be regarded as insignificant with respect to the 
topic of abstraction and metaphor in language. Nevertheless, the conjunction between 
this universal trait of language and the universal physiology of the brain may allow 
                                                           
2 The Economy Principle of Language, together with the Principle of Least Effort, was first put 
forward by the linguist André Martinet [12] within the domain of Phonetics. 

216 
N. Rossini 
further speculation: it is in fact rather possible that the very structure of the human 
brain allows – probably, even requires – metaphoric thought as a key function of 
cognition, perception [13], and production. 
If these premises are true, it is then possible to hypothesize that the study of 
gesture, both in terms of meaning-conveying strategies and in terms of abstraction, 
may highlight some interesting steps of the emergence of abstract and metaphoric 
thought. 
In particular, the analysis of spontaneous gesture and speech production during adult 
driven collaborative interactions is likely to highlight the underlying mechanisms of 
thought and language. 
Some interesting phenomena involving spontaneous repetition of iconic descriptions 
by adults intent on a given task shall be hereby presented and analysed. Specifically, 
the observed increased abstraction and reinterpretation in on-line restatements shall be 
examined as a key to the understanding of the emergence of abstract thought. 
The question of metaphor in gesture shall also be addressed, and hopefully 
resolved in the light of a more general and interdisciplinary interpretation of thought 
and language processes. Before addressing this topic, the scenery of the present 
enquiry needs to be briefly introduced: the following section summarizes a set of 
questions relative to abstraction, metaphor, and their relationship to gesture and 
speech. Afterwards, the results of the enquiry on some processes of abstraction in 
multi-modal spontaneous communication shall provide some answers for the 
problems thus far posed within this peculiar research topic. 
1.2   Metaphor and Gesture 
The question of metaphor in gesture has been put forward by McNeill [1], as a result 
of the semiotic approach to the enquiry on co-verbal gestures. In particular, he states 
that the gestures accompanying speech can be analyzed in terms of the inherent 
relationship they show between form (or signifier) and meaning (or signified). In this 
interpretation, that is somehow a static representation of the inner semiotic 
mechanisms of gestural production, he singles out some dimensions [4] of 
signification, among these being the emblematic – or symbolic – iconic, and 
metaphoric ones. In his 1992 book, McNeill relates metaphoricty to a particular 
recurring co-verbal gesture that represents abstract mental contents with the physical 
representation of a container, i.e., the conduit gesture. 
Later on, some scholars, on the one hand, have further investigated metaphor in 
gestural production, also by focusing on other typologies of metaphor, such as the 
recurrent metonymies in mathematical gestures, or on the implicit metaphorical nature 
of the majority of symbolic gestures, such as the “thumbs up” [14]. Others, on the 
other hand, tend to reduce the diverse types of co-verbal gestures to a mere iconic 
relationship3 between the representation and the represented. 
Those accepting the hypothesis of a metaphoric relationship in some co-verbal 
gestures’ signification domain have focused on methods for gesturally conveying of 
verbal metaphors: the “good as up” metaphor, for instance, is expressed in both 
                                                           
3 For a wider discussion, please refer to [15]. 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
217 
gestural and verbal production in a number a languages, including English4, and 
would thus have a metaphoric scent [14]. 
Nevertheless, this type of metaphoricity in gestural representation can also be 
reduced to an iconic significance, insofar as the gesture imitates some traits of the 
embodied semiotic association underlying the linguistic metaphor (in this case, the 
[+up] trait). 
Conversely, if one reduces the metaphor to the culturally determined and fixed 
association of imagistic and concrete traits of objects, one can easily appreciate that 
the ultimate semiotic bias of metaphor is an iconic one. 
Because the question of metaphor in gesture is not an obvious one, some 
investigation on spontaneous mechanisms of abstraction in gesture is desirable. The 
study here presented is aimed at contributing to this field by providing empirical data 
on the subject. 
2   Metaphoric Thought and Gestural Production: A Case Study 
The problem so far discussed can be better understood by focusing on some cases of 
gestural abstraction recorded during driven sessions of data collection. The data 
hereby presented are available from several projects conducted in order to assess 
different functions of language and gesture.  
Nevertheless, the heterogeneity of this material should not be considered a flaw, at 
least insomuch as the present research is concerned: the spontaneity of restatements 
due to either false starts or other cognitive and pragmatic needs, such as self-
orientation, self-interpretation, reinterpretation and understanding of the object to be 
described, or the synchronization with one’s interlocutor, provide the data so far 
available with due reliability, especially to the extent that the gestures performed by 
all the participants are spontaneous, and only partially experimentally evoked. 
The data shown here are available from two research projects focused on the 
intentionality of co-verbal gestures, and the function of gesture and language during a 
map-task, respectively. 
The experiment devoted to assessing the intentionality of co-verbal gestures was 
conducted at the Department of Psychology, the University of Chicago, in 2001. 
During this experiment, five Italian subjects were asked to engage in face-to-face 
interactions of 5 minutes each, following a given situational scheme. The “guessing 
game”, one of the schemes introduced during the experiment, had the subjects 
reconstruct a story from the story’s concluding scenario. Because this particular 
framework produced some interesting imagistic representations, as well as false starts 
and restatements, it is also possible to analyse the gestural strategies adopted by the 
subjects in order to convey their target concepts. 
The map-task data-collection [7] also produced some interesting instances of 
abstraction in the gestural production during restatement events. In the following 
pages, some instances shall be presented and analysed. 
                                                           
4 Nevertheless, the abovementioned study [14] fails to recognize that the metaphor is question 
goes back to the Romans, being it used both during the Republican and – especially – during 
the Roman Empire. Most probably, it diffused as a linguistic loan in a number or cultures ever 
since then.  

218 
N. Rossini 
2.1   Cases of Gestural Abstraction during Restatement 
The gestures performed during spontaneous face-to-face interactions serve diverse 
functions, among which are the referential [4], pragmatic [16], and self-orienting [7] 
ones. Mostly, such basic functions of language can be found to merge and interlace 
within the same linguistic act. As already pointed out [4, 5], gestures share the same 
functions already claimed for language, and these can conflate within the same 
gestural act, as happens with language acts in general. 
The phenomena here presented are cases of spontaneous online restatement aimed 
at synchronizing the speaker with his interactant: these are recorded when the speaker 
is trying to convey his idea without receiving desired feedback from his interlocutor, 
or when the speaker is trying to understand the pieces of information provided by his 
interlocutor. 
In Figs. 1 and 2, representative cases of restatement in face-to-face interaction are 
provided. As already mentioned, these cases are recorded during the third phase of a 
three-stage-experiment aimed at assessing the degree of intentionality and awareness 
of gestures.  
During the third phase, subjects in dyads were asked to resolve a problem given by 
the interviewer: because the data provided by the interviewer were constant, the 
content of the discussion was partially experimentally induced, which trait makes 
possible the isolation of recurrent concepts during the entire length of the recording: 
these concepts included an open window, some shards and water on the ground, and 
two dead characters, simply introduced as Romeo and Juliet. 
The game itself is also suited for the analysis of culturally-motivated 
conceptualizations of everyday objects, such as an open window in a room. Fig. 1 
shows the typical gestural representation of a window by an Italian subject (in 
transcripts, C2). 
Of course, the subject in question refers to the window constantly during this 
session, but his first conceptualizations of it are focused on the window’s shutters, or 
on the window’s state (i.e., being open). Because the hypotheses of C2 and his partner 
are not successful, he tries to synchronize with the interviewer (in this case, located at 
the camcorder) and asks for clarifications about the initial scene. 
Probably because of a sense of frustration combined with the sensation that the 
interviewer might have provided false items, C2 adopts an aggressive behavior with 
the interviewer, and tries to review the whole scene. 
At point 00:03:36, the first gestural depiction of the window is recorded in 
concurrence with the sentence “In alto!... Davanti alla finestra!” (Engl: Up! Beside the 
window!), where “up!” is a recognition of a speech error previously occurred, while 
“beside the window!” is a restatement. In this case, the gesture referring to the 
window is a small square-like space that C2 delineates with a complex mirror gesture 
of both hands starting as joined in the upper part, departing horizontally and going 
slightly down. The gesture in question is thus “sloppy” [17], and it is also 
immediately followed by a conduit palm-up gesture, performed with both hands. 
As the interviewer replies and adds confusion to C2’s idea of the scenario, he says 
“sì, ma c’è la finestra vicino!” (yes, but there’s the window in the nearby!), where 
“sì!” (yes!) has a pragmatic function: in his formal agreement with the interviewer, 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
219 
the subject in fact expresses a divergent interpretation of the scene, in which the 
window is close to Juliet and Romeo’s bodies. 
In this attempt to synchronize with the interviewer, C2 performs a complex 
gestural utterance composed of a metaphor, an iconic depiction of the window, and, 
again, a palm-up conduit showing the evidence of his claim. 
 
 
Fig. 1. Instance of reformulation during a face-to-face guessing game. In this case, the 
participant’s (C2) gestures undergo a process leading from abstraction to iconicity, probably 
due to the need for a detailed representation of the scene. 
Notice that the second gesture referring to the window is rather more iconic than 
the first one: here, despite the continuing “sloppy” performance typical of gestural 
representation, both hands start joined from the upper part of the semiotic space, 
depart horizontally, then horizontally and vertically at the same time, and finally 
perform some kind or horizontal mirror movement towards the lower side of the 
semiotic space.  
As the interviewer confirms this hypothesis, the subject (C2) repeats an 
enumeration of the basic objects in the scene and says “c’è la finestra, c’è l’acqua” 
(there’s the window, there’s the water 5 ), while his gestures depict the spatial 
relationship between the objects referred to in speech. This time, in synchronization 
                                                           
5 Note that the use of the determinative article has, in this case, a function of both deixis and 
rhematisation. In other words, “the window” and “the water” are not any kind of window or 
water, but the elements presented by the interviewer at the beginning of the experiment. 

220 
N. Rossini 
with “c’è la finestra”, C2 performs the same gesture described above, but with a more 
iconic and precise gestural performance. 
This is in fact the only case of three subsequent repetitions of a given element in 
which the gestures acquire greater iconicity rather than the contrary. C2’s gestures, in 
fact, show an inverse abstraction process, probably due to the exigency of 
synchronizing precisely with the interviewer. 
Conversely, other subjects show interesting phenomena of abstraction during 
restatement. In particular, it is important to underline the fact that the process of 
abstraction is only seen in the gestural performance. 
 
 
Fig. 2. Instance of reformulation during a face-to-face guessing game. In this case, the 
participant’s (C4) gestures undergo a process leading from iconicity to abstraction in gesture 
while the speech content is altered. 
Fig. 2 shows an example of this phenomenon. In this case, the subject (C4), while 
listening to the data provided by the interviewer, interrupts her and asks “ma è acqua 
proprio?” (but, is it water, precisely?), following the interviewer’s positive feedback, 
she asks “non può essere veleno?” ( can’t it be poison?). In concurrence with these two 
phrases, she performs the same gesture. The first time, she performs an iconic gesture 
in concurrence with the segment of speech “è acqua proprio?” The gesture in question 
represents C4’c conceptualization of a spot of water on the ground. When she further 
tries to assess the nature of the liquid on the ground, in suggesting it could be poison, 
she performs the same gesture as before, with more abstract features: the circular form 
already presented in the previous gesture is simplified and “sloppier” in configuration. 
Moreover, a metaphoric gesture is superimposed when the left hand of the subject 
performs some slight oscillatory movements towards the interlocutor, as if involving 
the interlocutor in her hypothesis. Thus, in this case the gestural performance is kept 
consistent and is anchored to the previous performance – a catchment in McNeill’s 
sense – while the speech content is completely altered. 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
221 
Other interesting phenomena of abstraction in gesture are available from the map-
task experiment. Because the experiment in question was exclusively focused on the 
description of paths and objects, it elicited in all the participants a higher number of 
restatements related to iconic representation. Due to space exigency, only a few 
examples will be hereby shown. 
 
 
Fig. 3. Instance of reformulation during “blind” map-task activity. The participant (M1) needs 
to reformulate a complex direction several times before his interlocutor gives positive feedback. 
The first time the path is described, the direction is segmented into two different and 
subsequent steps, diagonal and straight north (Plates A-B). During the third restatement, the 
subject conflates both paths (Plate C). 
Fig. 3, for instance, shows a complex restatement. A participant (M1) is 
experiencing problems in conveying the path to be followed by his partner. During 
the first description of the path in question, he divides the route into its main features, 
a diagonal movement and a straight path towards north. 
During his first description of the route in question, he performs particularly iconic 
and concrete gestures, such as movement with the index finger following the path to 
be described (Fig. 3, Plate A), and a more abstract gesture conveying the idea of 
going straight forward, performed with a sloppy B-shape of the hand – fingers and 
hand away from body, palm facing right – moving up and forward (Fig. 3, Plate B). 
As the subject is asked to rephrase his message, after a long discussion with his 
interactant aimed at assessing common landmarks on the map, he decides to convey 
the path jointly, in both speech and gestural performance (Fig. 3, Plate C). As he 
utters the sentence “e poi dritto diagonale verso il basso fino al leone” (and then 

222 
N. Rossini 
straight diagonal down towards the lion) he performs a complex more abstract 
gesture that can be divided into two phases: the first phase is a rather emphatic 
version of the gesture already shown in Plate B. The hand goes up towards the head in 
order to convey the idea of going straight forward; subsequently, the hand goes down 
in a slightly diagonal path in order to convey the idea of the down-diagonal direction.  
 
 
  
 
Fig. 4. Instance of reformulation during a “blind” map-task activity. Participant (M2) spontaneously 
reformulating a semi-circle path around a landmark. 
In Fig. 3, this complex gesture phrase is analysed as composed of two gestures, 
although the utterance in question can also be interpreted as a single complex gestural 
performance.  
Apart from questions of interpretation, the gesture is evidently more abstract, and 
acts as a global and more abstract cognitive reinterpretation of the path to be 
described, that is now conveyed successfully to the interlocutor.  
Another instance of this phenomenon is shown in Fig. 4. In this case, the subject 
spontaneously reformulates the description of a semi-circular path around a given 
landmark. During her first description of the route to be followed, she utters the 
sentence “La giri” (you turn around it) and performs an iconic gesture: the left hand, 
D-shape, depicts a semicircle away from body. As she is proceeding with her path 
description, she interrupts herself and decides to restate the previous segment: “cioè, 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
223 
la giri e curvi verso destra” (that is, you turn around it and bend towards right). 
Concurrently with this restatement, she performs a complex gestural performance 
with both hands. While her left hand is still holding an iconic gesture for the forward 
direction, her right hand, with a “sloppy” D-shape, depicts a semicircle as she says “la 
giri” (Engl.: You turn around it) and spreads completely while moving forward and 
left as she says “e” (and). The same gesture is repeated four times, each time with 
decreasing emphasis, in concurrence with the main accents of the rest of the sentence. 
 
 
Fig. 5. Instance of reformulation of spatial relationships during “blind” map-task activity in M5. 
The iconic gesture recorded during the first mention of a given landmark on the map is repeated 
as it is during the second attempt of synchronization with the interactant (Plates A-B). The 
strategy being apparently unsuccessful, a third reformulation is provided (Plate C) during 
which the subject shows a higher degree of abstraction in his gestural performance.  
 
Fig. 6. Detail of the map provided to M5 

224 
N. Rossini 
The last instance of this phenomenon is presented in Fig. 5. Here, it is possible to 
clearly observe the development of the participant’s representation of a landmark (i.e., 
Via Splendida) through a laborious and extensive attempt to synchronize with his 
partner. Because of the complexity of this subject’s spatial reinterpretation, a graphic 
rendering of his map is provided in Fig. 6. 
 Soon after the first attempt at describing the path shown in Fig. 6, M5 encounters 
problems of synchronization with his partner and is forced to negotiate the basic 
landmarks on the map before proceeding. The negotiation phase has a total length of 
two minutes, during which M5 describes briefly the initial path, the two participants 
realize that their maps are not totally congruent, and discuss their landmarks one by 
one in order to find a successful way to complete the task. 
The first time M5 refers to Via Splendida (00:00:09), his speech is particularly 
vague and expresses the idea of making a turn through the landmark in question. The 
first problems of synchronization arise, since his interactant immediately provides 
negative feedback. After the renegotiation of major landmarks, and having assessed 
that his partner’s target landmark has a different name, M5 decides to take that 
landmark as good and says “vabè, allora passa da lì” (ok, then go from there). During 
both the first and the second attempt, M5’s speech, besides being colloquial, is also 
rather vague and unclear. Both times his gesture is particularly iconic, in following 
exactly the path he has drawn on his map. Because the communication is once again 
unsuccessful, M5 tries to state better the direction (00:01:54) and attempts an 
objective orientation with respect to the landmark in common with his interactant’s 
map. The utterance he pronounces shows one false start (“allora, tra la Via 
Splendida…” – Ok, between Via Splendida), a filled pause, a speech error and a 
restatement (“praticamente passi sulla sin…*destra di Via Spledida” – practically, 
you go on the lef…* right side of Via Splendida”).  
The gestural performance synchronized with this segment is particularly abstract, 
and shows a good reinterpretation of the space relationships by the speaker: after 
having pointed at the precise location in his map where Via Splendida is, he performs 
a metaphor index of a new plan, i.e., a “palm-down-flap” [7], a metaphor expressing 
the online process of judgment about distance, size, and other spatial relationships, 
and again several “palm-down-flaps”, the last of which synchronized with the speech 
segment expressing the landmark. 
This is probably one of the best instances of abstraction during restatement, as far 
as it leads from an indexing-iconic anchoring to a complete metaphoric gestural 
production. All the subjects intent in map-task have shown to some extent the same 
phenomena exposed in these pages.  
3   Conclusions 
Despite the fact that the problem of metaphoric expression in gesture is still a debated 
one, the data here presented and analyzed should lead to the conclusion that it is 
possible to claim some abstraction properties in gestural expression.  
Granted, the basic semiotic property of gestures stems from perception and 
cognitive reinterpretation of imagery, and is thus essentially iconic. Nevertheless, 
both cognitive and linguistic processes show to be based on associative mechanisms 
that allow for metonymic and metaphoric thought. 

 
Gestural Abstraction and Restatement: From Iconicity to Metaphor 
225 
The fact that our gestural production responds to metaphoric thought should not be 
assumed as evidence of the non-communicativeness of gesture; neither can it be 
interpreted as supporting an exclusive iconic function of gestural representation. 
The results of analysis of the gestures performed during tasks concerning the 
description of objects, thus eliciting a considerable number of iconic representations, 
has in fact shown that, apart from serving representational and iconic functions, co-
verbal gestures also show a high degree of abstraction, especially if performed during 
spontaneous restatement. 
As a consequence, not only do gestures resemble in some iconic manner the objects 
to be described, but they also show abstract semiotic features. In particular, the same 
gesture referring to the same object in the real world, if repeated immediately during 
online restatement, can show a significantly more abstract performance, and even, in 
some cases, a complete metaphoric shift. These results appear to be outstandingly 
consistent with the studies already conducted on speech and gesture (see e.g. [18]). 
The data so far acquired should hopefully encourage the inclusion of gesture 
studies in the analysis of linguistic phenomena, without disregarding the enquiry on 
metaphoric thought and embodiment, its potentials, and its biological bases. 
Finally, the observed tendency towards abstraction in gesture during repetition, 
especially as synchronized, combined with and opposed to a sic repetition in the 
spoken signal, or a more adherent (i.e., iconic with respect to the real world) 
description in speech, may provide interesting pieces of knowledge for the 
development of ECAs whose behavior might be judged to be more reliable and 
natural by its final users. 
 
Acknowledgments. Thanks to Fey Parrill for discussing with me this study at an 
early stage, and to Karl-Erik McCullough for providing insightful comments to its 
final version. The responsibility of any errors or inaccuracies remains exclusively 
with the author. 
References 
1. Kendon, A.: Abstraction in gesture. Semiotica 90, 225–250 (1992) 
2. Butterworth, B., Hadar, U.: Gesture, speech and computational stages: A reply to McNeill. 
Psychological Review 96(1), 168–174 (1989) 
3. Ortony, A. (ed.): Metaphor and Thought. Cambridge University Press, Cambridge (1979) 
4. McNeill, D.: Hand and Mind. In: What Gestures Reveal about Thought. The University of 
Chicago Press, Chicago (1992) 
5. McNeill, D.: Gesture and Thought. The University of Chicago Press, Chicago (2005) 
6. Krauss, M.R., Hadar, U.: The role of speech-related arm/hand gestures in word retrieval. 
In: Campbell, R., Messing, L. (eds.) Gesture, Speech, and Sign, pp. 93–116. Oxford 
University Press, Oxford (1999) 
7. Rossini, N.: “Unseen” gestures and the Speaker’s Mind. An analysis of co-verbal gestures 
in map-task activities. In: Esposito, A., et al. (eds.) Fundamentals of Verbal and Nonverbal 
Communication and the Biometric Issue. NATO Security through Science Series E: 
Human and Societal Dynamics, vol. 18, pp. 58–65. IOS Press, Berlin (2007) 
8. Lakoff, G., Johnson, M.: Metaphors we live by. University of Chicago Press, Chicago 
(1980) 

226 
N. Rossini 
9. Fauconnier, G.: Mental spaces. In: Aspects of meaning construction in natural language. 
MIT Press, Cambridge (1985) 
10. Ortony, A. (ed.): Metaphor and Thought. Cambridge University Press, Cambridge (1979) 
11. Pulvermüller, F.: The Neuroscience of language. In: On brain circuits of words and serial 
order. Cambridge University Press, Cambridge (2002) 
12. Martinet, A.: Economie des changements phonétique: traité de phonologie diachronique. A 
Francke, Bern (1955) 
13. Massaro, D.W.: Perceiving talking faces: From speech perception to a behavioral principle. 
MIT Press, Cambridge (1998) 
14. Cienki, A.J.: Why study metaphor and gesture? In: Cienki, A.J., Müller, C. (eds.) 
Metaphor and gesture, pp. 5–25. John Benjamins Publishing Company, Amsterdam (2008) 
15. Müller, C.: Iconicity and gesture. In: Santi, S., et al. (eds.) Oralité et Gestualité: 
Communication Multimodale, Interaction, pp. 321–328. L’Harmattan, Montréal (1998) 
16. Kendon, A.: Gesture: Visible Action as Utterance. Cambridge University Press, Cambridge 
(2004) 
17. Kita, S., van Gijn, I., van der Hulst, H.: The non-linguistic status of the Symmtery 
Condition in signed languages: Evidence from a comparison from signs and speech-
accompanying representational gestures (in preparation) 
18. Parrill, F.: Form, meaning and convention: An experimental examination of metaphoric 
gestures. In: Cienki, A., Müller, C. (eds.) Metaphor and Gesture, pp. 195–217. John 
Benjamins, Amsterdam (2008) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 227–238, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Preliminary Prosodic and Gestural Characteristics of 
Instructing Acts in Polish Task-Oriented Dialogues 
Maciej Karpiński 
Institute of Linguistics and Center for Speech and Language Processing,  
Adam Mickiewicz University, Al. Niepodleglosci 4, 61-874 Poznan, Poland 
maciej.karpinski@amu.edu.pl 
Abstract. In the present study, selected properties of multimodal instructing acts 
are discussed. Realisations of the instructing acts extracted from a corpus of task-
oriented dialogues are analysed in terms of their syntactic structure, prosodic 
properties and accompanying gestures. The syntactic structures found in the 
material are similar to those found in earlier studies on map task dialogues. 
Deictic vocabulary is more frequent in gesture-supported instructions. The mean 
relative pitch range is similar to the values obtained for instructions in earlier 
studies and different from the values for syntactically similar questions. As 
opposite to verbally ill-formed instructions, the well-formed ones tend to contain 
at least one gestural stroke. It is shown that the relative range of pitch frequency 
is higher in the gesture-accompanied instructing acts. It is also noticed that 
prosody and gesture may play similar roles in utterances. 
Keywords: task-oriented dialogue, instructing acts, gesture, intonation. 
1   Multimodal Dialogue Analysis and Dialogue Acts 
The aim of the present study is to formulate a preliminary description of multimodal 
instructing dialogue acts in Polish task-oriented dialogues. The analysis is focused on 
the intonational features of utterances and hand gestures but it also includes some 
aspects of syntactic and lexical realisation. This research forms a part of the DiaGest 
project [1] confessed to the study of gestural, prosodic, grammatical and lexical 
components of task-oriented dialogues. It is meant as a step towards an application-
oriented holistic, multidisciplinary study of dialogue. 
The idea of a comprehensive approach to the studies on interpersonal communication, 
in which all of its relevant aspects would be paid the attention they deserve is not new. 
However, it gained more influence in the twentieth century with the contributions of 
great philosophers of language, psycho- and sociolinguistics and other researchers 
interested both in interpersonal and man-machine communication. Also the impact of 
technology cannot be overestimated. It created demand for formal models of human 
behaviour, simultaneously providing means for capturing and analysing it. 
As Wahlster [2] points out, multimodal dialogue systems exploit the coordinated 
use of different modalities as one of the major characteristics of human-human 

228 
M. Karpiński 
interaction. He acknowledges that all the modalities in use refer and depend upon 
each other which is the key to the richness of multimodal communication. 
Nevertheless, as Karpinski [3] mentions, this introduces much complexity to the 
description of dialogues. First of all, the information coming from various modalities 
cannot be simply “summed up”. Each “modal contribution” should be analysed both 
as providing context for the others and as occurring in the context of the others. The 
meaning of a given “verbal utterance” may largely depend on the accompanying 
gesture or facial expression. Gestures, similarly as prosody, may attribute it a certain 
value. On the other hand, the same gestural configuration may have different 
meanings when accompanied by different words. An efficient, formal representation 
of all these phenomena may pose extreme difficulties. While the problem of analysing 
an utterance in the context of the preceding text and, then, incorporating it into this 
context, was carefully studied and described (e.g., [4]), with simultaneous multiple 
modal contributions, new theoretical obstacles arise. 
Therefore, to make reasonable steps forwards, certain limitations are frequently put 
on the studied material and the ways of its analysis. First of all, most of the present 
studies are limited to the auditory and visual modalities and, most frequently, they are 
focused on speech, intentional body movements and facial expressions. Secondly, a 
number of factors (including the contextual ones) must be simplified or rejected. 
Consequently, it seems reasonable to base on controlled communication environments 
and carefully selected topics of communication. 
Task-oriented dialogues form a peculiar category which is extensively studied by 
researchers from a number of fields, including linguistics, man-machine communication, 
and psychology. They provide research material which is influenced by the 
characteristics of the task itself and the features of the environment but preserves a 
certain dose of spontaneity in the behaviour of the subjects. The findings from the studies 
on task-oriented dialogues are especially valuable for the design of man-machine 
communication but they also provide rich and manageable material for linguistic 
analyses of communication. 
The functions of utterances in dialogue interaction are frequently referred to as 
“dialogue acts” (e.g., [5]). While dialogue acts are sometimes regarded simply as 
speech acts realised in the context of dialogue, some researchers provide more precise 
definitions. For example, Bunt & Romary [6] describe them as the ways in which  
utterances are meant to change the information state of an interpreting system upon 
understanding them. Bunt & Girard [7] stress that “the whole idea of dialogue acts is 
that they are a way to characterize dialogue behaviour; therefore, they should have an 
empirical basis: every dialogue act type should have some reflection in observable 
features of communicative behaviour.” A commonly agreed aspect of dialogue acts is 
that they are realisations of human intentions (as explained in [8]). Still, as intentions 
cannot account for all observable dialogue phenomena, other components are also 
considered and analysed. This especially applies to the contextual factors. Context can 
be understood as the environment in which communication occurs as well as a 
product of the process of communication. Previous utterances create context for the 
subsequent ones and may be used to express and impose obligations which are put on 
the dialogue parties [9]. For example, questions impose the obligation to give answers 
and instructions impose the obligation to follow them. 

 
Preliminary Prosodic and Gestural Characteristics of Instructing Acts 
229 
Popescu-Belis [5] stresses that the automatic recognition of dialogue acts plays an 
important role not only in human-computer dialogue, but also in interpersonal 
dialogue understanding applications. It is obvious, however, that this research 
framework can be equally useful for many other linguistic, psycho- or sociolinguistic 
applications. Besides the lexical-syntactic content, gestures and prosody are major 
sources of information in the process of identifying categories of dialogue acts. 
1.1   Dialogue Acts and Gestures 
Although the role of gestures in particular types of dialogue act has received relatively 
little explicit attention so far, the gestural (visual) component of dialogue has been 
extensively studied from various viewpoints and for various purposes. Bavelas and 
Chovil stress that the tendency to recognize the importance of the visible component 
of dialogue is not new and it can be traced back to the middle of the 20th century [10]. 
It seems that certain gestures are typical of dialogue interaction and rarely occur in 
other communicational contexts. Bavelas et al. [11] distinguished between 
“interactional gestures” and “topic gestures.” The latter refer directly to the addressee 
and to the process of dialogue. Bavelas found that the ratio of the interactive to the 
topical gestures reached 1:3 in dialogues and 1:9 in monologues. Interactive gestures 
can contribute to dialogue flow management, especially to turn-taking. In a more 
recent study, Bavelas et al. [12] showed independent effects of mutual visibility and 
the dialogue interaction on gesturing. They found that face-to-face speakers were 
more likely to put information in their gestures that was not in their words as well as 
to make verbal references to their gestures, and to use more gestures referring to the 
interaction itself. Allwood and Cerrato [13] analysed the realisation of gestures that 
accompanied verbal feedback expressions in Swedish dialogues. They found that the 
feedback was mostly expressed simultaneously by vocal and gestural means. 
Moreover, they noticed that complex feedback expressions and certain attitudinal 
reactions are often accompanied by more complex head movements. The attitudinal 
expressions were found to be simultaneously marked by some phonological and 
prosodic phenomena. Much research on the gestural and gesture-supported 
communication is currently confessed to man-machine interaction phenomena.  
1.2   Dialogue Acts and Prosody 
While numerous attempts have been made to determine the intonational properties of 
sentential categories or sentence modalities, the grammatical category of a sentence 
only partially accounts for its intonational realisation. Nevertheless, a number of 
studies proved fruitful in finding prosodic cues to interrogativity (vs. declarativity). 
For example, House carried out extensive studies on Swedish (e.g., [14]), extending 
them with some multimodal aspects (e.g., [15]). Much hope was risen by early works 
on the prosodic properties of the realisations of selected dialogue acts [16,17,18]. 
Intonation seemed to offer especially valuable cues to the identification of dialogue 
acts. In a number of later studies, this approach was followed and elaborated. 
Fernandez & Pickard [19] declared the preliminary recognition rates of 47.3% (with 
respect to a 20.4% chance-level rate) in their system using prosodic cues. Tamarit & 
Martinez-Hinarejos [20] applied the pitch range and energy as additional cues to 

230 
M. Karpiński 
HMM-based dialogue act identification in their study of Spanish (DIHANA Corpus). 
They obtained only “a slight improvement” in the automatic tagging. Venkataraman 
et al. [21] studied the use of unlabelled data in HMM-based automatic dialogue act 
tagger. With the introduction of prosodic cues, the tagging error dropped by 12% to 
16%. A comprehensive approach was proposed by Rangarajan, Bangalore and 
Narayanan [22]. They offered two models of integrating prosody in the identification 
of speech acts. They reported the improvement of 11.8% for a system that, besides 
lexical and syntactic cues, used also prosody. Preliminary results from Coria & 
Pineda [23] show that intonation is useful to recognize sentence mood. In their study, 
they use CART-style decision trees trained on manually labelled data. Ang, Liu and 
Shriberg [24] showed that prosodic features can be also employed as cues to the 
detection of the boundaries between the realisations of subsequent dialogue acts. 
For languages like Polish, where the same sequence of words can be used as a 
statement, as a question, or even as an order, prosody would be an especially valuable 
source of information for the identification of dialogue acts. As shown by Karpinski 
[25], while certain tendencies can be found in the intonational realisations of dialogue 
acts belonging to a given category, the intonation alone is a relatively weak indicator 
of dialogue act category for spontaneous Polish. One of reasons for this is that the 
contextual factors frequently override the factors related to the basic intention behind 
an utterance. 
The prosodic cues to dialogue act identification are worth further investigation. 
Even though prosody may be misleading when used in isolation as the only cue to the 
category of a dialogue act, it cannot be omitted as powerful modifier to the pragmatic 
meaning and as a source of valuable information when the others (vocabulary, syntax, 
context) fall short. 
2   Instructing Acts in Polish Task-Oriented Dialogues 
A very limited number of studies has been confessed to the problem of dialogue act 
characteristics and identification in Polish. Sequential occurrence of dialogue acts in 
Polish task-oriented dialogues was studied by Karpinski [25]. He also made an 
attempt to provide basic intonational characteristics for a number of dialogue act 
categories. In some  works, the prosodic realisation of Polish questions was studied 
from the contrastive perspective  (e.g., [26, 27]). An attempt  was was also made to 
describe the prosodic properties of instructing acts in Polish task-oriented dialogues 
[28]. The study was based on twelve map task dialogues from the Pol'n'Asia Corpus. 
Instructing acts realised as single well-formed phrases were analysed for their 
intonational contours. Three types of context were distinguished: the instruction  
followed by an utterance by the same speaker, (a) immediately, or (b) after a pause, or 
(c) followed by an utterance produced by the other party. It turned out that the 
contextual factors strongly influenced the intonational realisation of instructions. 
When a number of instructions was realised in a sequence, only the ultimate one had a 
falling nuclear melody while, most frequently, all the previous ones were rising, 
presumably in order to signal the speaker's intention to continue (cf. “open” vs. 
“closed” utterances; e.g., [29]). It was also found that the relative pitch frequency 
range (both for entire utterances and for their nuclear melodies) was the highest for 

 
Preliminary Prosodic and Gestural Characteristics of Instructing Acts 
231 
the instructions syntactically built as declarative sentences. It was proposed that, as 
they did not contain any grammatical nor lexical cues to directivity, prosody was used 
to compensate for their absence. This kind of compensation was described for Polish 
questions by Grabe and Karpinski [26] as well as by Karpinski [25]. While the 
absence of a one-to-one relationship between dialogue act categories and  the classes 
of intonational contours was confirmed, it was shown again that intonation still 
provided valuable cues to the dialogue act category. 
2.1   Instructing Acts in DiaGest Taxonomy 
Instructions (commands, orders, requests) often play the role of the engine which 
gives momentum to subsequent steps of dialogues. Nevertheless, they form a very 
heterogeneous collection and vary in terms of their syntactic, lexical, and prosodic 
realisation, as well in terms of contextual appropriateness or the degree of politeness. 
While in some schemes of dialogue acts “Instruction” is already a defined category 
(e.g., Bunt's DIT++; [7]), here it will be used in a slightly different way. In the 
DiaGest scheme of dialogue acts [3], four dimensions are available: Task Action 
Control (TAC), Information Flow Control (IFC), Dialogue Flow Control (DFC), and 
External Attitude Expression (EAE). For the current study, only the TAC dimension 
is taken into account and the acts aimed at the realisation of task-related actions are 
considered (“RequestAction”). For the purpose of this text, they will be referred to as 
“Instructing Dialogue Acts” (IDAs).  
2.2   Material under Study: Recordings and Labelling 
Six task-oriented “origami” dialogues [1] were transcribed and labelled on multiple 
tiers for the lexical, prosodic and gestural components of utterances. Although both 
instruction givers and followers were recorded, only the behaviour of the instruction 
givers is analysed in this study. In the studied material, 223 G-Phrases consisting of 
713 G-Phases were labelled. The labels were based mostly on Kendon's model of the 
G-Phrase (as described by McNeill [30]) adopted and slightly modified for the 
purposes of the DiaGest project. In spoken utterances, 773 major intonational phrases 
were found, 539 of them being categorised as “well-formed.” (Gesture tagging: 
courtesy of E. Jarmolowicz-Nowikow). 
The realisations of instructing acts were tagged according to the system described 
in 3.1. Consequently, acts of various pragmatic meaning and realisations were 
included in the studied set. The total of 145 tokens were collected (their number 
ranged from 17 to 36 per a session). 
Basic syllable-level segmentation, phonemic transcription and pitch frequency 
analyses were carried out using Praat [31]. Gestures were labelled independently in 
ELAN using a number of specialised tiers [1]. The transcriptions and annotations 
from Praat were imported into ELAN (by MPI) which offered convenient tools for 
multi-tier annotation analysis.  
2.3   The Grammatical Form of Polish Instructing Dialogue Acts 
The word order in Polish is very flexible and the syntactic structure of sentences often 
must be decoded on the basis of the inflectional forms of its components. Nevertheless, 

232 
M. Karpiński 
some word order patterns are more frequent than others and regarded as “unmarked”. 
The grammatical form of a sentence certainly provides some useful cues to the 
dialogue act category. On the other hand, one has to remember that, for example, 
Polish polar questions and statements may have the same lexical and syntactic form 
and, as mentioned above, sometimes can be distinguished only on the basis of their 
intonation (internal cues), linguistic or situational context (external cues). 
IDAs may take various grammatical forms in Polish. The grammatical form may 
be influenced by a number of factors, including the degree of politeness or the kind or 
relationship between the interlocutors. The material under study was dominated by 
the following types of syntactic realisations: 
• 
Declarative clause, 2nd pers. sing. (“you're folding the sheet in two”); 
• 
Declarative clause, 1st pers. plur. (“we're folding the sheet in two”); 
• 
Imperative clause, using an imperative verb form (“fold it in half”); 
• 
Expressions with no verb (“straight ahead”); 
• 
Modal verb clause (“you must go straight ahead”). 
The same forms were found most frequent in Polish map task dialogues [28]. 
Although the nature of the dialogue task was different (e.g., the origami task involved 
more manual activity on the side of the instruction follower), the inventory of 
grammatical forms of instructions remained identical as they seemed to be typical of 
colloquial dialogues. 
2.4   Selected Gestural and Prosodic Features of Instructing Acts  
In the material under study, 145 realisations of IDAs were found. Many of them 
occurred in sequences or were followed by bounded clarifications. Sixty-three percent 
of the utterances labelled as IDAs were accompanied by gestures but there was a 
meaningful variation among the speakers (values ranging from 20% to 89.5%). Only 
48% of the realisations contained at least one stroke within their temporal limits. Also 
in this case, the deviation was substantial as the proportion ranged from 10% to 
84.5% in the six speakers under study. It is worth noticing that 95% of well-formed 
IDAs overlapped with strokes of respective G-Phrases. “Stroke-less” IDAs were 
usually realised as ill-formed intonational phrases or they immediately followed other 
realisations of IDAs that were accompanied by strokes. 
On the other hand, up to four strokes occurred within gesture-supported IDAs. It 
reflects the fact that single dialogue acts may be both gesturally and prosodically 
complex as they may consist of a number of gestural and intonational phrases. 
Although the number of questions realised by the instruction givers in the studied 
material was too limited for conclusive statements, it should be mentioned that 
questions were rarely accompanied by gesture. Accordingly, one may hypothesize 
that the presence of a gesture may help to decide on the category of the dialogue act in 
a given utterance. 
The gestures accompanying the realisations of IDAs were tagged for their type. In the 
taxonomy based mostly on [30], iconic, deictic, metaphorical gestures and beats were 
distinguished. As some gestures were too difficult to categorize them unequivocally, an  
 

 
Preliminary Prosodic and Gestural Characteristics of Instructing Acts 
233 
additional category “others” was introduced. Nevertheless, even gestures put into the 
basic, initial categories not always were of “pure types”. As it is shown in Fig. 1, the 
iconic gestures accounted for 58% of all the occurrences, the deictic ones for 16%, 
and beats for 10%. Only 8% of gestures were categorised as metaphorical. A 
relatively high proportion of gestures (10%) proved to be too difficult for an 
unequivocal classification. Some of them might have been regarded as “mixed types”, 
i.e. showing certain features of at least two basic categories (cf. [34]). The proportions 
of gesture categories may be influenced by the nature of the task, although it is 
obviously difficult to find a “neutral” form of communication which could be 
regarded as a reference in this respect. However, there is no doubt that “topical” 
gestures (e.g., [11]) were predominant in the realisations of IDAs. 
 
iconic 57%
deictic 16%
beats 10%
metaphoric 8%
other/unindetified 10%
 
Fig. 1. The proportions of basic gesture types in the realisations of instructing dialogue acts 
The gestural structure of IDAs was studied in terms of accompanying gesture 
sequences. A number of instructions were accompanied by more than one gesture (G-
Phrase). Over 10% of the gesture-supported IDAs were realised using a sequence of a 
deictic and an iconic gesture. The former was used to describe the object of the action 
while the latter depicted the action itself or the expected final shape of the 
manipulated object. Some speakers tended to use different hands for these two stages 
of the gestural utterance (Example 1). Many iconic gestures that described the desired 
shape of the object or the performed action involved two hands. However, the 
simultaneous use of both hands for relatively independent purposes occurred rarely 
(Example 2). 
 
Example 1. 
speech:  and now your left square... you've got to fold it... 
left hand: 
[                G1                     ] 
right hand: 
 
 
 
[     G2    ] 
 
G1 – pointing at the objects on the instruction follower's desk with her entire palm 
G2 – showing how to fold the sheet of paper 
 

234 
M. Karpiński 
Example 2. 
speech:  
and this part on the left... this little beak... to the center... 
left hand: 
[ 
 
 
G1 
 
 
    ] 
right hand: 
       [                              G2                      ][         G3     ]  
G1: pointing at the object on her desk (not visible to the instruction follower) and then 
holding this position till the end of the phrase 
G2: pointing to the respective part of the object on the desk of the instruction follower  
G3: showing the way the paper should be folded 
 
When both the (verbally expressed) object and verb were simultaneously expressed 
using gestures but words were not replaced with gestures (nor vice-versa), a kind of  
“parallel syntax” was in use. This should not be regarded as opposite to Slama-
Casacu's “mixed syntax” [32] but rather a special case of it where syntactic slots are 
filled both with verbal and non-verbal components. The parallelism should be 
understood not as producing two parallel syntactic structures but as the making 
syntactic slots of a single structure available for both speech and gesture (cf. also 
[10,11,33]). This, in turn, remains in accordance with McNeill's [30,34] hypothesis on 
the common source of audible and visible components of utterances. 
Another function of gestures in the analysed material was related to the coherence 
of multimodal utterances. When a disfluency or a pause (filled or silent) occurred, 
subjects tended to hold the gesturing (usually freezing at the pre- or post-stroke hold 
phase). Those holds certainly may be interpreted in a number of ways and may play a 
number of different roles. On one hand, this phenomenon seems to support the 
hypothesis of the common source for gesture and speech (gestures “must wait” for 
speech). On the other hand, the holds seemed to be used consciously to upkeep the 
attention of the instruction follower during the time of disfluency (as well as to 
prevent her or him from grabbing the turn). A prolonged post-stroke hold also 
occurred in sequences of IDAs and following clarifications. A hypothetical role of 
this behaviour might have been to keep the coherence by showing that subsequent 
clarifications applied to the same instructing act. In fact, G-Phrases were usually 
closed after the last relevant clarification. In this particular context, “the gestures that 
were held longer than necessary to convey information” were not “kinesically held 
questions” (Bavelas citing Kendon's suggestion in [33]) but rather a means to 
maintain the coherence. 
The problem of redundancy in multimodal utterances has been addressed more or 
less directly in a variety of contexts [35,36,37,38]. For example, Chovil [39] found 
that of the 405 semantic displays by speakers, 60% were to some degree redundant 
with verbal content, while 40% conveyed information that was not in the 
accompanying words. Nevertheless, the degree of redundancy remains difficult to 
judge in an objective and precise way. It is widely accepted that many gesture convey 
basically the same meaning as the accompanying words. However, De Ruiter [38] 
argues that even iconic gestures are never truly redundant with speech. They always 
may add certain information from a social perspective. Bavelas et al. [40] point to the 
fact that while interactive gestures are usually non-redundant with words, the topic 
gestures are normally highly redundant. These claims seem to be supported by the 
results of the current study. All the gestures that occurred in the realisations of IDAs 
were, in their basic meaning, redundant to the spoken component of the message. 

 
Preliminary Prosodic and Gestural Characteristics of Instructing Acts 
235 
However, it is necessary to stress that certain parameters of gesture realisation itself 
added something to the message (for example, emotional or attitudinal meaning). 
Moreover, as it was described above, prolonged holds and, were most probably used 
to maintain coherence and upkeep listeners attention. Another fact that must be taken 
into account is that gestures have also a certain value to their producer and they are 
produced even when the parties do not face each other. 
The usage of beats followed (in the sense of co-occurrence) the rhythmic structure 
of utterances. Beats did not occur frequently and their usage was typical of more 
emotional or expressive utterances. It was observed that their strength (amplitude) 
differed within utterances and increased in their most important parts. This complies 
with McNeill's [30] observations on the functions of beats. 
The lexical and grammatical differences between gesture-supported and gestureless 
utterances are not striking. It was found, however, that deictic words occurred in 62% 
of the gesture-supported and  only in 49% of “gesture-less” IDAs. 
The intonational contours of the IDAs' realisations were analysed using Praat for 
the extraction of basic pitch parameters. Independently, the utterances were also 
analysed by ear for the shape of their intonational contours. The proportions of rising 
and falling nuclear melodies were almost equal in the studied material. However, as 
mentioned above, the rises could be frequently attributed to the contextual factors, not 
only to the dialogue act category. 
The mean value of the relative pitch range calculated as (f0max – f0min)/f0max was 
43%. The same value was obtained for this category of dialogue acts in [28] where 
the calculation was based on a larger sample from a different dialogue corpus. 
Instructing utterances involving gestures had a substantially wider relative pitch range 
than those produced without parallel gesturing (43% vs. 32%). A few explanations are 
possible. For example, one may expect that the prosodic arousal co-occurs with the 
gestural arousal. This solution would be in accord with the common source hypothesis 
(cf. [30,41]).  
3   Conclusion and Further Research 
Despite the limited size of the material under study, an impressive variety of forms 
was found in the gestural and prosodic realisations of instructing dialogue acts. It 
turned out that  individual treats may be very strong in gesturing even if the speakers 
are put in the same situation and assigned the same task. Nevertheless, this variety 
was realised within relatively clear frames. To a certain extent, it was due to the 
nature of the task in which certain types of movements or shapes were more relevant 
than others.  
Utterances marked as the realisations of IDAs may be simultaneously functioning 
in other dimensions, including those related to dialogue flow management. Therefore, 
it is not surprising that some of the topical gestures were found to contain certain 
features that allowed them to play the role of the interactive ones. Analogical 
phenomena may also may partially account for the form of the prosodic realisations. 
Gestures and prosody in the analysed dialogues share some functional 
characteristics. They contribute to the meaning and these contributions are usually in 
accordance with what the lexical and grammatical components convey. However, 

236 
M. Karpiński 
they can be independently manipulated for a number of purposes. Therefore, while 
the information they provide may be valuable in the recognition of dialogue acts, it 
still should be treated cautiously.  
In a number of tokens, gestures clearly reflected the propositional content of IDAs 
and, in some of them, the content of the instruction could be read solely from 
gestures. However, gestures did not provide sufficient information on the modality of 
the utterance, its communicational aspects or its grammatical category. 
The gestures found in the IDAs (a) frequently supported deictic functions of 
utterances; (b) were often used to describe shapes of objects and gesture trajectories 
(c) supported focusing (for components of the spoken utterances); (d) were used to 
catch and keep attention; (e) supported coherence of utterances. At least some of these 
functions are  very similar to what prosody and voice quality may contribute. While it 
is obvious in the case of focusing, coherence or attention mechanisms, it is also 
possible that prosody and voice quality may be used to reflect, even if less precisely, 
some features related to the visual qualities of objects. 
The present study is based on a relatively limited set of utterances and it is 
mentioned as a preliminary step towards a more comprehensive description of 
selected multimodal dialogue acts in Polish task-oriented dialogues. The number of 
sessions under study will be increased. More detailed annotation of intonational and 
gestural phenomena is planned, with the focus on the gestural trajectories and 
intonational contours. A parallel corpus of origami dialogues will be recorded in non-
facing condition (i.e., the speakers will not be able to see each other). 
References 
[1] Jarmolowicz, E., Karpinski, M., Malisz, Z., Szczyszek, M.: Gesture, Prosody and 
Lexicon in Task-Oriented Dialogues: Multimedia Corpus Recording and Labelling. In: 
Esposito, A., Faundez-Zanuy, M., Keller, E., Marinaro, M. (eds.) COST Action 2102. 
LNCS, vol. 4775, pp. 99–110. Springer, Heidelberg (2007) 
[2] Wahlster, W.: Dialogue Systems Go Multimodal: The SmartKom Experience. In: 
Wahlster, W. (ed.) SmartKom: Foundations of Multimodal Dialogue Systems. Cognitive 
Technologies Series, pp. 3–27. Springer, Heidelberg (2006) 
[3] Karpinski, M.: From Speech and Gestures to Dialogue Acts. In: Esposito, A., Hussain, A., 
Marinaro, M., Martone, R. (eds.) Multimodal Signals 2008. LNCS, vol. 5398, pp. 164–
169. Springer, Heidelberg (2009) 
[4] Kamp, H., Reyle, U.: From Discourse to Logic. In: Introduction to Model-theoretic 
Semantics of Natural Language, Formal Logic and Discourse Representation Theory. 
Springer, Heidelberg (1993) 
[5] Popescu-Belis, A.: Abstracting a Dialog Act Tagset for Meeting Processing. In: 
Proceedings of LREC 2004, Lisbon, Portugal, vol. IV, pp. 1415–1418 (2005) 
[6] Bunt, H.C., Romary, L.: Standardization in Multimodal Content Representation: Some 
Methodological Issues. In: Proceedings of LREC 2004, Lisbon, pp. 2219–2222 (2004) 
[7] Bunt, H., Girard, Y.: Designing an Open, Multidimensional Dialogue Act Taxonomy. In: 
Proceedings of DIALOR 2005 Workshop, Nancy (2005) 
[8] Traum, D.R.: 20 Questions for Dialogue Act Taxonomies. Journal of Semantics 17(1),  
7–30 (2000) 

 
Preliminary Prosodic and Gestural Characteristics of Instructing Acts 
237 
[9] Kreutel, J., Matheson, C.: Obligations, Intentions, and the Notion of Conversational 
Games. In: Proc. Gotalog, 4th Workshop on the Semantics and Pragmatics of Dialogue 
(2000) 
[10] Bavelas, J.B., Chovil, N.: Visible acts of meaning. An Integrated Message Model of 
Language in Face-to-Face Dialogue. Journal of Language and Social Psychology 19(2), 
163–194 (2000) 
[11] Bavelas, J.B., Chovil, N., Coates, L., Roe, L.: Gestures specialized for dialogue. 
Personality and Social Psychology Bulletin 21, 394–405 (1995) 
[12] Bavelas, J., Gerwing, J., Sutton, C., Prevost, D.: Gesturing on the telephone: Independent 
effects of dialogue and visibility. Journal of Memory and Language (2007) 
[13] Allwood, J., Cerrato, L.: A study of gestural feedback expressions. In: Paggio, P., 
Jokinen, K., Jönsson, A. (eds.) First Nordic Symposium on Multimodal Communication, 
pp. 7–22 (2003) 
[14] House, D.: Final rises and Swedish question intonation. In: Proceedings of Fonetik 2004, 
pp. 56–59 (2004) 
[15] House, D.: Perception of question intonation and facial gesture. In: Proceedings of 
Fonetik 2002, pp. 41–44 (2002) 
[16] Mast, M., Kompe, R., Harbeck, S., Kiessling, A., Niemann, H., Nöth, E., Schukat-
Talamazzini, E.G., Warnke, V.: Dialog act classification with the help of prosody.  
In: Proceedings of ICSLP 1996, pp. 1732–1735 (1996) 
[17] Shriberg, E., Bates, E., Stolcke, A., Taylor, P., Jurafsky, D., Ries, K., Coccaro, N., 
Martin, R., Meteer, M., Van Ess-Dykema, C.: Can Prosody Aid the Automatic 
Classification of Dialogue Acts in Conversational Speech. Language and Speech 41(34), 
439–487 (1998) 
[18] Wright, H., Poesio, M., Isard, S.: Using high level dialogue information for dialogue act 
recognition using prosodic features. In: Proceedings of DIAPRO 1999, pp. 139–143 
(1999) 
[19] Fernandez, R., Picard, R.W.: Dialog act classification from prosodic features using 
support vector machines. In: Proceedings of Speech Prosody 2002, pp. 291–294 (2002) 
[20] Tamarit, V., Martinez-Hinarejos, C.-D.: Dialogue act labelling in the DIHANA corpus 
using prosody information. Jornadas en Tecnologia del Habla V, 183–186 (2008) 
[21] Venkataraman, A., Ferrer, L., Stolcke, A., Shriberg, E.: Training a prosody-based dialog 
act tagger from unlabeled data. In: Proceedings of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, vol. 1, pp. I-272–I-275 (2003) 
[22] Rangarajan, V., Bangalore, S., Narayanan, S.S.: Exploiting prosodic features for dialog 
act tagging in a discriminative modeling framework. In: Proceedings of INTERSPEECH 
2007, pp. 150–153 (2007) 
[23] Coria, S., Pineda, L.: Predicting Dialogue Acts from Prosodic Information. In: Gelbukh, 
A. (ed.) CICLing 2006. LNCS, vol. 3878, pp. 355–365. Springer, Heidelberg (2006) 
[24] Ang, J., Liu, Y., Shriberg, E.: Automatic dialog act segmentation and classification in 
multiparty meetings. In: Proceedings IEEE International Conference on Acoustics, 
Speech and Signal Processing, pp. 1061–1064 (2005) 
[25] Karpinski, M.: Struktura i intonacja polskiego dialogu zadaniowego (The Structure and 
Intonation of Polish Task-Oriented Dialogues). Wydawnictwo Naukowe UAM, Poznan 
(2006) 
[26] Grabe, E., Karpinski, M.: Universal and Language-specific Aspects of Intonation in 
English and Polish. In: Grabe, E., Wright, D.G.S. (eds.) Oxford University Working 
Papers in Linguistics, Philology & Phonetics, vol. 8 (2003) 

238 
M. Karpiński 
[27] Karpinski, M., Szalkowska, E.: On intonation of question-type dialogue moves in Korean 
and Polish task-oriented dialogues. In: Proceedings of Speech Signal Annotation, 
Processing and Synthesis, Poznan (2006) 
[28] Karpinski, M.: The Intonational Realization of Requests in Polish Task-Oriented 
Dialogues. In: Matoušek, V., Mautner, P. (eds.) TSD 2007. LNCS (LNAI), vol. 4629,  
pp. 556–563. Springer, Heidelberg (2007) 
[29] Fox, A.: Prosodic features and Prosodic Structures: The Phonology of Suprasegmentals. 
OUP, Oxford (2000) 
[30] McNeill, D.: Hand and Mind. In: What Gesture Reveals about Thought? University of 
Chicago Press, Chicago (1992) 
[31] Boersma, P., Wenink, D.: Doing Phonetics by Computer (A computer program; version 
5.1) (2008) 
[32] Slama-Casacu, T.: Nonverbal components in message sequence: “Mixed syntax”. In: 
McCormack, W.C., Wurm, S.A. (eds.) Language and man: Anthropological issues,  
pp. 217–227. Mouton, The Hague (1976) 
[33] Bavelas, J.B.: Gestures as Part of Speech: Methodological Implications. Research on 
Language and Social Interaction 27(3), 201–221 (1994) 
[34] McNeill, D.: Gesture and Thought. University of Chicago Press, Chicago (2007) 
[35] Kendon, M.: Gesture: Visible Action as Utterance. CUP, Cambridge (2004) 
[36] Bergmann, K., Kopp, S.: Co-expressivity of speech and gesture: Lessons for models of 
aligned speech and gesture production. In: Olivier, P., Kray, C. (eds.) AISB Annual 
Convention: Language, Speech and Gesture for Expressive Characters, pp. 153–158 
(2007) 
[37] Buisine, S., Martin, J.-C.: The effects of speech–gesture cooperation in animated agents’ 
behavior in multimedia presentations. Interacting with Computers 19, 484–493 (2007) 
[38] De Ruiter, J.P.: Some multimodal signals in humans. In: Van Der Sluis, I., Theune, M., 
Reiter, E., Krahmer, E. (eds.) Proceedings of the Workshop on Multimodal Output 
Generation MOG 2007, pp. 141–148 (2007) 
[39] Chovil, N.: Discourse-oriented facial displays in conversation. Research on Language and 
Social Interaction 25, 163–194 (1991/1992) 
[40] Bavelas, J.B., Kenwood, C., Johnson, T., Phillips, B.: An experimental study of when and 
how speakers use gestures to communicate. Gesture 2(1), 1–18 (2002) 
[41] Kendon, A.: Gesticulation and speech: two aspects of the process of utterance. In: Key, 
M.R. (ed.) The Relationship of Verbal and Nonverbal Communication, pp. 207–227. 
Mouton, The Hague (1980) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 239–247, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Polish Children's Gesticulation in Narrating (Re-telling) 
a Cartoon 
Ewa Jarmołowicz-Nowikow 
 
Center for Speech and Language Processing and Institute of Linguistics, 
Adam Mickiewicz University, Al. Niepodległości 4, 
61-874 Poznań  
ewa@jarmolowicz.art.pl  
Abstract. The study was aimed at a preliminary analysis of the nonverbal com-
ponent of utterances produced by nine year old Polish children in the task of re-
telling a cartoon. Most research on children's gestures so far has been confessed 
to younger subjects. The analyses presented in the article concern the relations 
between semantic contributions from the visual and auditory modalities (ges-
tures and speech) as well as the viewpoint of gestures and the use of gesture 
space. Gestural phrases were tagged for their internal structure (phases) and 
gestures were categorized into basic types.   
Keywords: Gesture phrase, viewpoint, type of gestures, gesture space. 
1   Introduction 
Even though there is a substantial amount of literature concerning the nonverbal as-
pects of children communication, many problems in this field remain unsolved. Most 
research concerning children's nonverbal communication is focused on the very early 
period of children's language development. The period of time from the early months 
until the child is three years old seems to be the most thoroughly examined period in 
the individual development of language. According to Kendon [1], “we know rather 
little about the further development of gesture use in children. After children have 
reached the age of two or three years, studies of the further development of linguistic 
expression, whether in speech or gesture, become much less frequent. (…) it would be 
most interesting to undertake further studies of the development of gesture use in 
children throughout all the ages of childhood, up to and including adolescence.”   
1.1   Some Aspects of Studies on Children's Gesturing  
The main direction of childhood gesture studies concerns the development of non-
verbal competence. Most researchers [2], [3], [4], [5], [6] agree that gestures are cru-
cial for the very early stages of language development. Observations of children  

240 
E. Jarmołowicz-Nowikow 
before the one-word period show that the first medium of children's conscious com-
munication is gesture [7], [5]. However, to emphasize the common source of gesture 
and speech McNeill [4] refers to studies suggesting a close relation between proto-
gestures and protospeech. According to McNeill, simultaneous hand movements and 
vocalizations in the first minutes of life and raising the hands above the head accom-
panying speech articulatory movements within the first weeks of a child's life sug-
gest a biological connection between the vocal and manual systems. Even though 
there is a common agreement that the gesture development precedes the advent of 
spoken words, there is no certainty about the communicative conditions under which 
gesture emerges. It was observed that even blind children at very early stages of lan-
guage acquisition have the same type of gestures in their repertoire as sighted chil-
dren, however the absence of vision considerably decreases the frequency of gestures 
as a gesture is not an effective means of communication for them [8]. This observa-
tion provoked questions about the role of gesture in language development. There 
has been a number of studies proving that gesture use in children communication 
foreshadows progress in speech development. According to Butcher & Goldin-
Meadow [7] it is possible to distinguish certain stages in language development in 
which gestures play a vital part. Initially, children create gestures as alternative to 
words. In the one-word period, children begin to combine gestures and speech, al-
though there is no synchrony between the two modalities. The integration between 
gesture and speech forebodes the entry to the two-word stage. According to Iverson 
& Goldin-Meadow [8] “gesture is a harbinger of change in the child's developing 
language system”. However, it was observed that in case of blind children less fre-
quent gestures did not lead to delays in language development.     
2   The Course of the Experiment   
The subjects (nine year old children) were asked to watch a 7-minute cartoon about 
Sylvester & Tweety and re-tell the story. Before the experiment the entire group of 
children spent some time in the room where the experiment took place getting ac-
quainted with the experimenter and playing with the recording equipment. Just before 
each of the children started watching the cartoon individually, the experimenter tried 
to make the child feel more comfortable and relaxed and again entered into a few 
minutes conversation them. After watching the cartoon the subject was asked to re-tell 
it and was informed that the experimenter will re-tell the story on the base of subject's 
narration.  
2.1   The Material under Study 
The material under study comprises video recordings of six nine year old native 
speakers of Polish re-telling the cartoon about Sylvester & Tweety. The children  
were recorded at their schools. The durations of the recordings range from 4 to 7 min-
utes. From the total of 28 recordings, six were selected for further analyses on the  
 

 
Polish Children's Gesticulation in Narrating (Re-telling) a Cartoon 
241 
basis of the amount of gestures realized by children. The video recordings were tran-
scribed and annotated in ELAN. The annotation included a number of tiers and only 
some of them were further employed. The present study covers the structure of the 
Gesture phrase (G-phrase), the types of gestures produced, the viewpoint of the ges-
ture, gesture space and the semantic correlation between gesture and speech.  
3   The Results of Preliminary Analysis 
3.1   The Structure of the Children's Gesture phrase  
For the purpose of the study, the model of the G-phrase proposed by Kendon was 
applied. It consists of the obligatory Stroke phase, and optional Preparation phase 
and Recovery phase [1], [5]. The model was extended by the Pre-stroke hold and 
Post-stroke hold added by Kita [9] as well as the Stroke hold distinguished by Duncan 
[5]. A simple probabilistic model of the G-phrase for nine year old children was built 
on the basis of the quantitative analysis of the material. In the material under study, 
137 G-phrases (gestures) were found. The number of Strokes (137) is equal to the 
number of analyzed G-phrases as each Stroke forms one G-phrase  while the number 
of optional phases may vary. The Pre-stroke hold occurred eight times, being the 
most seldom optional category, while the Preparation (111) was the most frequent 
one. The proportions of respective G-phases were similar to those obtained by Kar-
pinski et al. [10] in the study of adults. 
 
Fig. 1. Frequencies of particular G-phases that occur within G-phrase in the studied material  
The distribution of G-phrases in time was also analyzed. To describe the average 
layout of G-phrases in time, the initial four minutes of all the recordings were taken 
into consideration. The length of the recordings differed, however each of them lasted 
at least four minutes. The amount of gestures occurring within each minute of each 
child's narration is similar. Only seven gestures were noticed within the first minute of 
all the recorded sessions, whereas an abrupt increase in gesture frequency was ob-
served in the second minute (42 gestures were realized by all the children in the sec-
ond minute of the studied material). In the third and the fourth minutes the amount of 
gestures decreased.  
 
Preparation
Pre-str. hold
Stroke
Post-str. hold
Retraction
0
20
40
60
80
100
120
140
160
111
8
137
25
91

242 
E. Jarmołowicz-Nowikow 

	

	

	

	








 
Fig. 2. The distribution of G-phrases in the initial four minutes of the children's narration  
The detection of the temporal boundaries of G-phrases, as well as finding their on-
sets and ends of each gesture, did not pose serious problems for the vast majority of 
G-phrases. According to McNeill's [4] comparative study, this situation is typical for 
adults but not for six year old children narrating the cartoon story because their ges-
tures often blend together. It was noticed that the nine year old children's narration of 
the story is often accompanied by sequences of iconic gestures. The separate iconics 
in each sequence are realized generally exactly one after another (in a few cases a 
short hold of the averaged length 400 msec were observed) and refer to some aspects 
of objects and actions verbally described by the child. Twenty gesture sequences con-
sisting of 65 gestures were found in the the analyzed material. The shortest sequence 
consists of two different iconic gestures while the longest contains ten of them.  
 
Example 1. (five gesture sequence): 
Na linie (1) 
i coś tam pisał (2)  
i w wtedy pojechał na linie (3) 
[On the rope] 
[and was writing something] [and then he went on the line] 
aż w mur (4) 
trafił.(5)  
[until the wall]  
[hit].  
 
1. Both palms tightened, one over the other as if holding a rope. 
2. Both hands move as if typing something. 
3. Both hands are rising, one over the other, as if the character would like to start swinging on 
the line. 
4. Both hands simultaneously rapidly move forward. 
5. The same gesture as above but realized by one hand. 
 
Example 2. (four gesture sequence): 
Spadł(1)  
i pobiegł,(2)  
a ten ciężarek opuścił się(3)    
[Fall down] 
[and ran]  
[and this weight went down]  
i mu tak później wyprostował głowę(4). 
[and him so later straightened the head] 
 
1. The hand rapidly moves down. 
2. The hand moves forward. 
3. The hand moves down. 
4. Both hands move up and than both hands move down along the head. 
 
At the age of nine, children still seem to use gestures extensively as a support for 
more precise description of objects and actions. The iconic gestures within the se-
quences are quite clear and often combine with each other. That is why even though 

 
Polish Children's Gesticulation in Narrating (Re-telling) a Cartoon 
243 
they are simultaneous with speech they may call up associations with pantomime (see 
Kendon's Continuum in [11]).    
3.2   The Types of Gestures Used in Narration 
The categorization of gestures in this study is based on McNeill ideas [5]. The ges-
tures that could not be assigned a single category were labelled with as many tags as 
necessary to reflect all their possible assignments (e.g., iconic/deictic). Iconicity is a 
dominant feature of most of the gestures in the material under study, however some of 
them contain deixis and beat features as well. No cases of metaphoricity were noticed. 
A vast majority of the iconics used by children may to some extent be the result of the 
character of the utterance almost free of persuasion and metanarrative statements (re-
ferring to the structure of the cartoon). The majority of gestures were produced in 
conjunction with narrative statements having to do with the plot which move the 
story line forward. The proportion of metanarrative and narrative statement usage is 
the attribute that differentiates child and adult gestures [12].  
	
	 
	 !







 
Fig. 3. The frequency of occurrence of gesture types used in the six children's narration  
Most of the nine year old children's gestures are co-expressive with speech: They refer 
to the same event or idea as speech but present a different aspects of it [4]. Only a few 
cases of complementarity between speech and gestures, which is characterized by present-
ing different meanings in different modes, were found in the analyzed recordings. 
 
Example of complementarity:                                                                                                                       
Tak liną tak. 
[And with the line so.] 
1. The hand shows the swinging movement. 
 
Complementarity seems to be peculiar to the children's gestures at the beginning of 
speech and gesture combination while co-expressiveness is typical of adults [5]. Al-
most all the gestures were realized by either the right or left hand or both hands 
together forming G-phrases. However, in three cases, both hands simultaneously real-
ized different gestures presenting actions in a complex and detailed way.  
3.3   The Viewpoint of the Gesture 
According to McNeill [4], two gestural viewpoints may be distinguished. When the 
gesture is realized from the Character Viewpoint (CVPT) the speaker's body as a  
 

244 
E. Jarmołowicz-Nowikow 
 
Fig. 4. The overlap of different gestures made by different hands (a screenshot of annotation in 
ELAN) 
 
Fig. 5. Proportion of OVPT, CVPT and BARP gestures  
whole or just partially plays the role of the character. In case of the Observer View-
point (OVPT) the hands are symbols representing the character as a whole. A special 
type of Character Viewpoint named “BARP-gestures” (Body as Reference Point) was 
distinguished by Holler and Beattie [13]. In this case, narrators use their bodies as 
direct points of reference. They point to themselves, indicating thereby character's 
body parts.  
Most of gestures produced by adults belong to the OVPT category. Children be-
tween the twelfth and eighteenth month of life produce mainly gestures which are 
whole-body CVPT enactments. A two and a half year old child narrating the cartoon 
story uses both CVPT and OVPT gestures, however children up to twelve are charac-
terized by viewpoint sequestering. That means that the manner and path describing 
movement (e. g. Sylvester rolling down the street) are sequestered into two different 
viewpoints. The path tends to be shown from observer's perspective, while the man-
ner from the perspective of the character [4], [5]. Most of the gestures in the studied 
material are produced from the Observer Point of View. It was observed that subjects 
of the analysis may change the gesture viewpoints within the framework of one ges-
ture sequence. It happens that two subsequent gestures forming one gesture sequence 
are represented from a different point of view. The children shift the point of view in 
the gestures but they do not shift the perspective in the simultaneous spoken dis-
course. It was also noted that in almost all of the CVPT cases the hands were the only  
 
 
OVPT
CVPT
BARP
0
20
40
60
80
100
120
100
35
2

 
Polish Children's Gesticulation in Narrating (Re-telling) a Cartoon 
245 
parts of the body “identified” with the character. The rest of the body stayed at rest. 
Only twice was the CVPT realized by a different part of the body e. g. the head in one 
case or almost the whole body in another. It was also observed that CVPT gestures in 
the studied material generally refer to the activity performed by the hands such as: 
opening, taking, throwing, beating, throwing down, cranking. 
3.4   Gesture Space    
To describe the space where the nine year old children's gestures are produced 
McNeill's [4] panel presenting a division of the gesture space was used.  
 
Fig. 6. The schema of gesture space proposed by McNeill. Drawing based on the one presented 
in [4].  
In order to analyze the usage of the gesture space, the locations of onsets and ends 
of Strokes were taken under consideration. Very large differences are noticeable in the 
individual use of the gesture space. For example one of the subjects produced Strokes 
which were rather limited in space (more than 50% of Strokes within the scope of one 
periphery lower panel), while the span between the beginning and end of majority of 
the Strokes realized by another subject was much wider and stretched across four pan-
els (e.g. the Stroke started in the periphery lower and ended at center upper). 
4   Conclusions 
The results of the preliminary analyses revealed that the nine year olds' gestures com-
bine child as well as adult features. The structure of analyzed children's G-phrases 
resembles the adults' one, however the dominant gesture feature is iconicity. The 
beats and deictic gestures were uncommon while the metaphorical ones were absent.  
 

246 
E. Jarmołowicz-Nowikow 
It was also noticed that gestures produced by children form gesture sequences consist-
ing of successive iconic gestures. Children make most of the gestures from the  
observer's perspective and may change the viewpoints within one gesture sequence. 
Children strongly varied in their use of the gesture space, so it was impossible to find 
any dominating trend. The study is a part of a comparative research project which is 
aimed at the studies of gestures of children and adults. It is meant to contribute mate-
rial for a multilingual, multicultural database prepared by the participants of the 
COST2102 Action. The findings presented here are preliminary and concern only 
some aspects of children's narration. Further research will provide data on adult's ges-
ticulation as well as on the prosodic properties of child and adult speech. Then, more 
detailed studies of co-expressiveness and synchronization between gesture and speech 
of children and adults will become possible. 
 
Acknowledgement. This study is a part of the project 199/N-COST/2008/0 funded by 
the Polish Ministry of Research and Higher Education.  
References  
1. Kendon, A.: Some Topics in Gesture Studies. In: Esposito, A., Bratanic, M., Keller, E., 
Marinaro, M. (eds.) Fundamentals of Verbal and Nonverbal Communication and the Bio-
metric Issue, pp. 3–19. IOS Press, Amsterdam (2007) 
2. Goldin-Meadow, S.: The Role of Gesture in Communication and Thinking. Trends in Cog-
nitive Sciences 3(11), 419–429 (1999) 
3. Iverson, J.M., Goldin-Meadow, S.: Gesture Paves the Way for Language Development. 
American Psychological Society 16(5), 367–371 (2005) 
4. McNeill, D.: Hand and Mind: What Gestures Reveal about Thought. The University of 
Chicago Press, Chicago (1995) 
5. McNeill, D.: Gesture and Thought. The University of Chicago Press, Chicago (2005) 
6. Riseborough, M.G.: Meaning in Movement: An Investigation into the Interrelationship of 
Physiographic Gestures and Speech in Seven-Year-Olds. British Journal of Psychology 73, 
497–503 (1982) 
7. Butcher, C., Goldin-Meadow, S.: When Hand and Mouth Come Together. In: McNeill, D. 
(ed.) Language and Gesture, pp. 235–258. Cambridge University Press, Cambridge (2000) 
8. Iverson, J.M., Tencer, H.L., Lany, J., Goldin-Meadow, S.: The Relation Between Gesture 
and Speech in Congenitally Blind and Sighted Language-Learners. Journal of Nonverbal 
Behavior 4(2), 105–130 (2000) 
9. Kita, S., Gijn van, I., Hulst van der, H.: Movements phases in signs and co-speech ges-
tures, and their transcription by human coders. In: Wachsmuth, I., Fröhlich, M. (eds.) GW 
1997. LNCS, vol. 1371, pp. 23–35. Springer, Heidelberg (1998) 
10. Karpinski, M., Jarmołowicz-Nowikow, E., Malisz, Z.: Aspects of Gestural and Prosodic 
Structure of Multimodal Utterances in Polish Task-Oriented Dialogues. The paper pre-
sented at the conference Analiza, Synteza i Rozpoznawanie Mowy, Zastosowania w Sys-
temach Bezpieczeństwa Publicznego and accepted for publication 
11. Kendon, A.: Gesture, Visible Action as Utterance. Cambridge University Press, Cam-
bridge (2005) 

 
Polish Children's Gesticulation in Narrating (Re-telling) a Cartoon 
247 
12. Cassell, J.: The Development of metanarrative speech and gesture in children’s storytel-
ling. Paper presented at the Biennial Meeting of the society for Research on Child Devel-
opment, Kansas City, MO (1989) 
13. Holler, J., Beattie, G.: A Micro-Analytic Investigation on How Iconic Gestures and Speech 
Represent Core Semantic Features in Speech. Semiotica 142(1/4), 31–69 (2002) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 248–265, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Prediction of Learning Abilities Based on a Cross-Modal 
Evaluation of Non-verbal Mental Attributes Using  
Video-Game-Like Interfaces 
Yiannis Laouris1, Elena Aristodemou1, and Pantelis Makris1,2 
1 Cyprus Neuroscience & Technology Institute, 5 Promitheos, 
1065 Nicosia, Cyprus 
{laouris,elena}@cnti.org.cy 
2 Ministry of Education, Nicosia, Cyprus 
makrisp@cytanet.com.cy 
Abstract. The authors propose the thesis that today’s children immersed in 
cyberspace need to rely on different skills and mental attributes in order to 
interact successfully with knowledge. It is argued that learning pedagogies as 
well as corresponding assessment tools must comply with the multi-modality 
principle. The paper describes a multimodal evaluation of the learning potential 
and reading/learning abilities of young children’s brains. The method is based 
on the assessment of non-verbal abilities using video-game-like interfaces. The 
results show that the ability to orientate and navigate, to sequence or categorize 
objects or events, as well as to discriminate visual and auditory stimuli and the 
short-term visual and auditory memory can predict reading and learning 
abilities. Moreover, the combined assessment of several independent modalities 
significantly increases the predictive power.  
Keywords: Cognitive profile, multimodal, neuroscience, instructional design, 
navigation, 
categorization, 
sequencing, 
lateralization, 
auditory-visual 
discrimination, computer assessment, complex system, video game. 
1   Introduction 
For millions of years knowledge has been transferred from generation to generation 
using primarily the auditory modality, namely oral language processing pathways. 
Except for a few cases like for example the 40,000 year old cave drawings on 
aboriginals in Australia, which indicate that the visual modality had its share in 
information transfer, it is only recent in human evolution, that imprinted drawings and 
text manifested our ability to transfer experiences and know-how to future generations 
more authentically, massively and reliably. For the last 20-30 centuries, the acquisition 
of knowledge as well as the documentation of it has relied almost exclusively on 
written forms of communication and thus on visual modalities. The past century has 
however marked the launch of a new era in human communication and learning. The 
dominance of written text and the “book” as the primary source of information and 
meaning-making technology has reached a turning point. New media such as the 
television, the Internet and mobile technologies in connection with the introduction of 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
249 
multimedia in education expanded our options to interact with information and acquire 
knowledge. Although we are now immersed in a technology-rich multimodal 
environment, we often continue to treat perception as a modular function [1]. The 
dominant thesis remains more often than not that different sensory modalities operate 
as separate and independent processes. However, not only electrophysiological, but 
also neuroanatomical and more recently neuroimaging studies revealed that 
multisensory interactions occur throughout many processing pathways [2], [3], [4]. 
Neuroscience has identified “multisensory” brain regions as convergence zones, where 
neurons receive afferent inputs from several senses. Many reports indicate that such 
interactions are the rule rather than the exception in processing of sensory information 
[5], [2]. For example, Schroeder et al. [6] discovered multisensory convergence in low-
level cortical structures (generally believed to be multisensory in function) in both 
monkeys and humans. Despite these findings, many contemporary studies in the field 
of instructional design and educational technologies have focused on a single sensory 
modality. Similarly, assessment of learning abilities has also focused mainly on visual 
and auditory modalities. 
Learning is a complex process and although it is hard to operationalize, researching 
difficulties in learning has led to an understanding on what kind of processes may be 
related with it. Learning disabilities (or difficulties), “refer to a number of disorders, 
which may affect the acquisition, organization, retention, understanding or use of 
verbal or nonverbal information” [7]. Such disorders may be diagnosed through 
assessment of difficulties in reading, low performance on verbal IQ and/or nonverbal 
IQ measures, difficulties in oral and/or written speech, etc. For the purposes of this 
paper, two of the above parameters that may relate to learning have been used; 
reading measures and nonverbal IQ.  
Contemporary teaching and learning methodologies and pedagogies exploit multi-
sensorial stimulation engaging multiple modalities [8] through the use of multimedia. 
The idea that multisensory training can enhance learning goes back to the beginning 
of the 20th century (see for example the influential historical reports of Montessori 
[9], Fernald and Keller [10], Orton [11], Neményi, Szendrei-Radnai and Varga [12]. 
Also an unknown author stated “A picture is worth a thousand words.” Emergent 
neuroscience and recent advances and research in the field of visualization have 
revealed some interesting aspects of the scientific grounding of this (see below). Yet, 
despite the plethora of educational and instructional multimedia grounded on this 
hypothesis and related empirical research, we still lack a comprehensive 
understanding of the underlying complex mechanisms. For example, we assume that 
visualization is important because of neuroanatomical and neurophysiological data 
which reveal that our brains are wired to process visual input very differently from 
text and sound. Recent technological advances through functional Magnetic 
Resonance Imaging scans have confirmed the dual coding system first proposed by 
Paivio [11], [13]. Visual and textual information is processed in separate channels 
from auditory. What remains unknown is how learners with different processing 
power for each channel or preference of one over the other pathway benefit from 
learning environments that exploit text, sound and animation. An on-going research 
project of our group aspires to correlate learners with visual- vs. auditory preference 
in processing of information with educational environments that use predominantly 
voice, text, static images and/or visualizations [14]. With such knowledge, one could 

250 
Y. Laouris, E. Aristodemou, and P. Makris 
design learning environments that can dynamically adapt to the differential 
preferences and abilities of learners giving preference to those modalities that are 
more effective for a particular individual.  
We have explained above, how millennial kids immersed within technology-rich 
environments are challenged to process at least the auditory and visual modalities in 
parallel. However, they also need to engage additional modalities. For example, they 
are also required to navigate through cyberspace, quickly categorize objects or 
concepts in order to create meaning, keep record in their short-term memory of 
sequence events, tasks or learning objectives. The challenge for educators is to design 
instructional environments that engage all modalities and capitalize on their 
integration. On the other hand, researchers face the challenge to discover how 
multiple modalities interact and co-operate within the complex system we call brain 
to produce meaning. The application of multimodal stimulation of the brain offers a 
challenge to augment our learning capacity, but one has to account for individual 
differences. The “one-curriculum fits all” hypothesis can only apply if we as 
researchers manage to match the characteristics of the learning environment to the 
learning style and mental characteristics of the learner. To achieve this goal we need 
assessment of all channels. We have hypothesized earlier that mental attributes 
beyond the auditory and visual would also correlate with their ability to learn [15]. A 
preliminary battery of eight cognitive tests known as Mental Attributed Profiling 
System (MAPS) has been validated [16], [17] and found to predict reading ability 
[18] when considered as a whole. 
The aim of the study reported here was primarily to test the following hypotheses: 
 
Hypothesis 1: The combined assessment of multiple modalities, i.e., measures of 
navigation, orientation, sequencing, categorization, auditory and visual short-
term memory and discrimination abilities could serve as predictor for reading 
abilities/difficulties. 
 
Hypothesis 2: The non-verbal tests of MAPS can predict non-verbal IQ. 
 
In the case in which the experimental data supported the first hypothesis, our 
secondary aim was to investigate whether different combinations of modalities would 
provide different levels of confidence in their predictive power. 
Reading performance is generally considered as an important prerequisite for learning. 
It is therefore very usual in practice to use reading measures in screening tests and other 
assessment tests of learning difficulties (e.g., WISC, Raven’s Progressive Matrices, etc). 
This study has therefore also examined whether reading performance is predicted from 
the different MAPS modalities. Along the same line of thought, non-verbal IQ is also 
usually considered as an adequate measure of learning abilities/difficulties. Therefore, 
this study examined also the second hypothesis stated above. 
2   Method  
2.1   Subjects 
The participants were elementary school children attending 16 different public 
elementary schools. They were equally sampled from urban, suburban, and rural 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
251 
public school districts. Subjects were from three age groups: The second, the fourth, 
and the sixth Grades. The researchers aimed at 45 subjects per age group. The parents 
of the participants had predominantly low levels of education (less than one quarter of 
the sample had parents, who were college educated). 
2.2   The Mental Attributes Profiling System 
The MAPS (Mental Attributes Profiling System) cognitive test is a battery of more 
than 12 validated computer-based video-game-like tests that assess the learning 
abilities of pre-elementary and elementary age school children. The non-verbal group 
consists of language-independent tests. Snap shots of three tests are shown in Fig. 1.  
 
 
Fig. 1. Examples of three out of the 8 interfaces used in the study reported here to evaluate non-
verbal mental attributes. Categorization ability (left) orientation (center) and navigation (right).  
The following eight tests have been used for the multimodal evaluation reported here: 
Categorization 
The test (Fig. 1; Left) presents an object on the lower part of the screen and invites the 
subject to “drag” it in one of three squares that represent different “worlds,” for which 
there is a match. The following “categories” were tested: (1) Objects of different color 
to be placed in one of three possible “colored worlds,” i.e., squares of three different 
colors; (2) One geometrical object, (i.e., triangular, circular, or rectangular), to be 
placed in its corresponding world, i.e., the squares contained triangular, circular or 
rectangular shapes respectively; (3) A plant matched as a vegetable, a tree, or a 
flower. (4) An animal (or fish) to be placed in one of three possible environments: 
sea, sky, or open fields. (5) Objects usually found in a home environment, to be 
“placed” in one of three rooms of a house (office, kitchen, or bathroom), i.e., as in the 
example snapshot shown in Fig. 1 (left). The software measured the time to response 
and whether the move was correct or false. For this study we used only the average 
response time (i.e., the cumulative response time of each test divided by 5: 
(ctt1+ctt2+...ctt5)/5 in Table 1), as it has been shown previously to have a higher 
correlation than the Correct/False values. 
Orientation: “Left-Right Game” Lateral awareness Test 
This test (Fig 1; Center) provides two types of measures. One, it evaluates the child’s 
ability to make left-right discriminations on his own body. During the first part, the 
test shows a child “sitting” in the same orientation as the subject (i.e., the subject sees 
on the screen the back of the child) in front of two objects, one on his/her upper left 
and the other on his/her upper right visual field. The computer asks the subject to 

252 
Y. Laouris, E. Aristodemou, and P. Makris 
“grab” the left or right object using his/her left or right hand. What is measured is (1) 
the time taken to select an arm (by clicking at the shoulder), and (2) whether the 
selected arm was correct or wrong. The same procedure is repeated during the second 
part of the test, in which the orientation of the child on the screen is reversed, i.e., the 
child on the screen is facing the subject. The second type of measures are derived 
from Piaget’s [19] tests to evaluate awareness of right-left relations outside our own 
body, i.e., in the environment. To evaluate this ability, the test measures (1) the time 
taken by the subject to decide which object to “grab” and (2) the number of 
correct/wrong decisions.  
Navigation 
The Navigation test (Fig 1; Right) consists of an 8x8 matrix of small pieces of cheese 
and a mouse. The computer instructs the subject to move the mouse in one of eight 
possible directions to “eat” the corresponding piece of cheese: Up, Right-up, Right, 
Right-Down, Down, Left-down, Left, and Left-up. The instructions are given both 
visually (with the aid of a small arrow) and/or orally. For the tests reported here the 
instructions were given only orally. The software measured the total number of correct 
responses and the total number of correct responses divided by total number of trials. 
Sequencing 
This test was inspired by the “connect the puzzle” games. Different objects or animals 
appear in two-, three-, four-, or five pieces and the subject is requested to “drag” the 
pieces and place them in the right order to complete the picture. The second part of 
the test presents pictures, which represent different stages of a temporal process. The 
subject is expected to put them in the correct chronological order. The test measured 
the total time required to complete each sub-section of the test (the test was comprised 
of six different types of exercises divided by six (i.e., (stt1+stt2+...stt6)/6 in Tabl.1). 
Visual memory 
This test is based on the well-known card game called “Memory,” in which the 
subject uncovers two cards at a time. If the two cards featured the same picture, they 
would remain uncovered and thus visible. Otherwise they are turned back as they 
were, and the game continues until all cards are uncovered. The test keeps records of 
many parameters, including the number of iterations that have passed since the 
subject “saw” a certain card. For the purpose of this study, only VMtt: Total time to 
complete the test, and VMnoT: Total number of trials, were used. 
Auditory memory 
The test was modeled using the digital phone metaphor. The computer instructor 
invites the subject to “dial” a telephone number. Two sets of two-digit numbers are 
followed by four sets of three-, four-, five-, and six digit numbers, respectively. The 
test concludes by presenting a set of two seven-digit numbers. The feedback is 
positive/neutral giving no clue regarding the correctness of the response. The test is 
terminated if the subject makes 3 consecutive errors.  For the purposes of the research 
reported here, AMTc: Total number of correct responses and AMTcw: weighted 
number of correct digits (i.e., all correctly types digits even when the sequence was 
wrong) were used as measures of auditory digit span. 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
253 
Auditory discrimination test 
The main screen of the test features two human-like figures, who “speak” a word, one 
after the other. The subject is asked to decide whether the two words are the same or 
different by clicking on an “8” or a “9” sign. Each word includes consonants, which 
sound similar and are therefore confused by weak readers and especially by dyslexics 
in the Greek language. The following letter combinations were tested: (1) φ-β, (2) δ-θ, 
(3) ζ-σ, (4) χ-γ, (5) τ-ντ, (6) κ-γγ/γκ, (7) π-μπ, (8) τσ-στ, (9) γ-γγ/γκ, (10) ξ-κς. In 
additional sets, the test also evaluates the ability of the child to differentiate between 
the same letter combinations when they are embedded in non-sense words. The test 
kept record of the time and correctness of each instance. In Tabl.1 adtt(1), 
adtt(2),...adtt(n) are used to denote the time taken to respond to each card presented. 
Visual discrimination 
Modeled using the popular “Find the differences” game, this test presents three 
pictures very similar to one another and one that must be matched. The exercise is 
repeated four times with four different pictures. The total number of correct responses 
is used as an index of visual discrimination.  
For more details regarding the structure of the video game interfaces used in the 
MAPS cognitive tests and the parameters measured refer to Laouris & Makris [16]. 
2.3   Measures Used for Predicting Learning and Reading Performance 
Tests and Measures used to evaluate IQ 
The following verbal and one non-verbal test were used to evaluate the IQ: 
• 
Verbal: WISC-III V IQ – Vocabulary 
• 
Verbal: WISC-III V IQ – Similarities 
The Weschler Intelligence Scale for Children (WISC) is a measure for testing 
intelligence in children aged six to sixteen years old. It is composed of ten core 
subtests and five supplemental one which verbal and performance abilities are tested. 
The verbal subtests are administered orally and include information, similarities, 
vocabulary and comprehension. The performance subtests are comprised of picture 
completion, coding picture arrangement and block. For the purposes of this study, two 
verbal subtests were used from the WISC to assess verbal IQ in participating children; 
vocabulary and similarities. The vocabulary subtest asks oral questions about the 
meaning of words and the similarities’ test is asks children how two concepts can be 
alike. The tests were used to ensure that the distribution of scores was normal and that 
all subjects’ scores fall within ±1 SD from the average. 
• 
Non-verbal: IQ – Matrices 
The Raven’s Progressive Matrices is a test that measures non-verbal IQ using 
questions on abstract reasoning with multiple choice answers. Children on Raven’s 
Matrices are asked to identify the missing part to complete the pattern presented. 
Raven’s Matrices were used to assess non-verbal IQ in participating children. IQ can 
be considered as one measure for predicting learning difficulties. 

254 
Y. Laouris, E. Aristodemou, and P. Makris 
Tests and Measures used to evaluate reading abilities/disabilities 
Rapid Naming of Pictures. This test, modeled after Wimmer et al. [20], consisted of 
two tasks. For each task, a random sequence of 20 pictured objects (5 different icons, 
each repeated four times) had to be named. The icons for each task were presented on 
a single page, with four lines of 5 objects per page. Order of icons changed from one 
line to the other. The words of the first task started with the same single consonant 
cluster (καπέλο, καρέκλα, κεράσι καρότο, κλειδί) whereas the words of the second task 
started with different consonant clusters (φράουλα, πλυντήριο, σκύλος, σταυρός, 
μπανάνα). Children’s score was the ratio score of number of correct items identified 
in time taken, per task, to name all the letters. 
Rapid Naming of Letters. This test was constructed following the same logic as in the 
RAN pictures test. In each task, the children were asked to name as fast as possible a 
random sequence of 20 letters appearing on a single page (5 different letters, each 
repeated four times). The letters of the first task were only vowels (α, η, ε, ο, υ); and 
the letters of the second task were only consonants which share similar characteristics 
and which are usually confused from poor readers in Greek (π, τ, σ, δ, θ). The child 
had to say the name of the letter and not the sound that it makes, for an answer to be 
recorded as correct. Again, children’s score was the ratio score of number of correct 
items identified in time taken, per task, to name all the letters.  
 
The participants’ reading ability was assessed through two different tasks involving 
the reading of real words and pseudo-words. Both reading measures were Greek 
adaptations of Woodcock’s Reading Mastery Test-Revised [21], and have been used 
in previous studies [22], [23]. In both tests, the participants’ score was the number of 
words read correctly within 60 sec.  
 
• 
Word Identification: This test consisted of 85 words forming a 2 x 2 x 2 
factorial design in terms of frequency (high/low), orthographic regularity 
(regular/exception), and length (bi-syllable/tri-syllable). Due to the absence 
of standard frequency counts in Greek, half of the words were sampled from 
the first-to-second grade language books, and the other half taken from third-
to-fourth grade language books. The stimulus words were mainly nouns with 
a few adjectives and verbs.  
• 
Word Attack: This test consisted of 45 pronounceable non-words that were 
derived from real words after changing two or three letters (either by 
substituting them or using them backwards). The task started with bi-syllabic 
words and ended with five-syllabic words. 
2.4   Design of Experiments and Administration of the Tests 
Letters were sent to all parents, whose children were selected, seeking their 
permission. No parent withheld permission. All participating children attended two 
individual testing sessions lasting from 30 to 40 minutes each. All testing took place 
during school hours in a private room in the participants' respective schools. All 
paper-and-pen tests were administered and scored by the participating educators. 
Also, all MAPS tests were administered in their presence. The scores were written on 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
255 
the computer automatically by the software. The educators copied those files on disks 
and delivered them to the authors.  
Before launching the experiments, the authors secured special permission by the 
Ministry of Education authorizing them to contact schools directly. Sixteen regular 
elementary schools, which were equally sampled from urban, suburban, and rural 
public school districts, in Cyprus, were chosen for the experiments. Twenty four 
educators (where possible, preference was given to those employed in participating 
schools) were identified to serve as administrators of the tests. Almost all of them are 
qualified and experienced teachers who participated in similar exercises in other 
occasions. Before the experiments, these twenty four experts received an 8-hour 
training (spread over two sessions) in how to conduct individual assessment 
interviews with children and how to administer the paper-and-pen as well as the 
MAPS computer-based video-game-like tests. 
3   Results 
Eight public schools tested a total of 134 children. Forty-four (25 females & 19 
males) were 7-8 years old, 44 (22 females & 24 males) were 9-10, and 46 participants 
(24 females & 22 males) were 11-12 years old. The mean age for the younger group 
was 7.40 years (SD = 0.45), for the middle group was 9.49 years (SD = 0.47), and for 
the older group was 11.27 (SD = 0.39). There were no significant differences in age 
between males and females across groups.  
In a previous study, the results were analyzed using the Statistical Package for 
Social Sciences using linear multiple regression to identify the best predictors of 
reading performance. Five measures which provided the best prediction are used in 
this analysis. The model which includes the five measures shown in Table 1 explains 
54% of the variance and it is significant (F=29.979, df=5, p<.001). Table 1 documents 
the coefficients for the five strongest predictors of the MAPS cognitive tests for 
reading performance.  
As it can be seen, the strongest predictor appears to be lateralization same 
orientation (Beta=.274, t(128)=3.883, p<.001). This result suggests that reading 
performance can be predicted from the number of correct responses the participant had 
when asked to note left and right while the orientation was the same with his/her own 
body. As the data in Table 1 suggest, the second strongest predictor appears to be 
auditory memory (Beta=.268, t(128)=3.362, p=.001). This result suggests that the total 
of correctly typed digits (despite the order on which they were put) appears as a good 
predictor of reading performance. The third good predictor of reading performance  
as shown from the regression analysis is acoustic discrimination (Beta=-.235,  
t(128)=-3.156, p<.01). The fourth best predictor of reading performance as shown in 
table 1 is categorization (Beta=.188, t(128)=-3.006, p<.01). This suggests that the total 
time taken to categorize objects can predict reading performance. Last, sequencing is 
also found to be a predictor of reading performance (Beta=-.177, t(128)=-2.396, 
p<.05). The total time taken for a participant to put objects in the correct sequence 
appears as a good predictor of reading abilities. 
In order to examine the above predictors in line with the multimodality principle, 
we have tested them in groups of 2, 3 and 4. Table 2 summarizes the results. 

256 
Y. Laouris, E. Aristodemou, and P. Makris 
Table 1. Regression coefficients of five MAPS scores tested for their ability to predict reading 
performance. The scores from MAPS tests are shown on the left column. For details regarding 
the formulas used to calculate these scores refer to the respective tests‘ descriptions in the 
Method section. The composite measure used for reading performance was the sum of  wid: 
Word Identification and wat: Word Attack.  All MAPS measures shown here were found to be 
good predictors of reading performance (refer to text for further details). 
 
 MAPS measures 
Unstandardized 
Coefficients 
Standardized 
Coefficients 
t 
Sig. 
  
  
B 
Std. 
Error 
Beta 
  
  
1 
Lateralization 
same 
orientation  total correct = 
(lsoc1+lsoc2+...lsoc8) 
.320 
.082 
.274 
3.883 
.000*** 
2 
Categorization total time = 
(ctt1+ctt2+...ctt5)/5 
1.62E-
006 
.000 
.188 
3.006 
.003** 
3 
Sequencing total time = 
(stt1+stt2+...stt6)/6 
.000 
.000 
-.177 
-
2.396 
.018* 
4 
Acoustic 
discrimination 
total 
time 
= 
(adtt1+adtt2+...adtt10)/10 
-.003 
.001 
-.235 
-
3.156 
.002** 
5 
Acoustic memory total 
correct weighted number 
of correct digits (2x2; 3x3 
etc.) 
.030 
.009 
.268 
3.362 
.001** 
Dependent Variable: reading composite score = wid+wat 
*Significant at .05 level 
**Significant at .01 level 
***Significant at .001 level 
Table 2. Examination of combinations of two, three, or four tests to predict reading performance 
(refer to text for details) 
 
 
R2 
F 
df 
p 
Lateralization, 
acoustic 
discrimination 
.434 
50.309 
2 
<.001 
Lateralization, acoustic memory 
.430 
49.507 
2 
<.001 
Lateralization, sequencing 
.425 
48.486 
2 
<.001 
Acoustic discrimination, acoustic 
memory 
.411 
45.760 
2 
<.001 
Categorization, acoustic memory 
.377 
39.633 
2 
<.001 
Sequencing, acoustic memory 
.372 
38.766 
2 
<.001 
Sequencing, 
acoustic 
discrimination 
.348 
35.021 
2 
<.001 
Categorization, 
acoustic 
discrimination 
.325 
31.502 
2 
<.001 
Lateralization, categorization 
.309 
29.258 
2 
<.001 
Two tests 
Categorization, sequencing 
.262 
23.255 
2 
<.001 
             

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
257 
Table 2. (continued) 
lateralization, acoustic memory, 
acoustic discrimination 
.488 
41.249 
3 
<.001 
lateralization, sequencing, acoustic 
discrimination 
.478 
39.645 
3 
<.001 
lateralization, sequencing, acoustic 
memory 
.474 
39.089 
3 
<.001 
categorization, 
acoustic 
discrimination, acoustic memory 
.466 
37.874 
3 
<.001 
lateralization, 
categorization, 
acoustic memory 
.456 
36.380 
3 
<.001 
lateralization, 
categorization, 
sequencing 
.440 
34.073 
3 
<.001 
lateralization, 
categorization, 
acoustic discrimination 
.430 
35.412 
3 
<.001 
categorization, 
sequencing, 
acoustic memory 
.428 
32.485 
3 
<.001 
sequencing, 
acoustic 
discrimination, acoustic memory 
.428 
32.388 
3 
<.001 
Three tests 
categorization, 
sequencing, 
acoustic discrimination 
.390 
27.741 
3 
<.001 
Lateralization, 
categorization, 
acoustic discrimination, acoustic 
memory 
.519 
34.762 
4 
<.001 
Lateralization, 
sequencing, 
acoustic discrimination, acoustic 
memory 
.507 
33.149 
4 
<.001 
Lateralization, 
categorization, 
sequencing, acoustic memory 
.504 
32.711 
4 
<.001 
Lateralization, 
categorization, 
sequencing, 
acoustic 
discrimination 
.499 
32.084 
4 
<.001 
Four tests 
Categorization, 
sequencing, 
acoustic discrimination, acoustic 
memory 
.485 
30.387 
4 
<.001 
 
In order to investigate which combinations of test would provide sufficient and 
optimal information we have calculated the R2 and F values of combined two-, three- 
and four-test combinations. In all cases, the combinations of tests reported in Table 2 
have p values smaller than <001. The combination Lateralization & Acoustic 
Discrimination has R2 = .434. Adding to this combination Acoustic Memory brings 
R2 = .488. The combination Lateralization, Categorization, Acoustic Discrimination 
and Acoustic Memory features the highest R2 = .519 and it appears as the best model 
for predicting reading performance. In conclusion, the data support Hypothesis 1. 
 

258 
Y. Laouris, E. Aristodemou, and P. Makris 
Table 3. Stepwise regression coefficients for predicting non-verbal IQ in children. The dependet 
variable used was the total score in the Raven’s Progressive Matrices test. (Refer to text for 
details). 
Unstandardized 
Coefficients 
Standardized 
Coefficients 
Model 
B 
Std. 
Error 
Beta 
t 
Sig. 
1 
Acoustic memory 
(total correct) 
.152 
.025 
.458 
5.958
.000*** 
Acoustic memory 
(total correct)  
.109 
.027 
.327 
3.953
.000*** 
2 
Visual 
auditory 
discrimination 
(total correct) 
.517 
.149 
.287 
3.471
.001*** 
Acoustic memory  
(total correct)  
.071 
.030 
.215 
2.390
.018* 
Visual 
auditory 
discrimination 
(total correct) 
.476 
.146 
.265 
3.264
.001*** 
3 
Sequencing  
(total time)  
-.001
.000 
-.239 
-2.829 
.005** 
Dependent Variable: ravens total 
*Significant at .05 level 
**Significant at .01 level 
***Significant at .001 level 
 
 
 
 
 
A further analysis was conducted to examine Hypothesis 2 stated in the 
Introduction, i.e., whether MAPS subtests provide good predictors of non-verbal IQ in 
children. Stepwise regression on the subtests of MAPS showed that a 32% of the 
variance is accounted by the model. The regression model was found to be significant 
(F=20.379, df=3, p<.001) and the three MAPS tests that were found to serve as good 
predictors to non-verbal IQ  are acoustic memory, visual auditory discrimination and 
sequencing. Table 3 documents the stepwise regression coefficients for predicting 
non-verbal IQ in children. 
The stepwise regression shows that Model 3 provides the best set for predicting 
non-verbal IQ in children. It includes only three out of the eight MAPS subtests. The 
three subtests are by order of significance: (1) the total number of correct responses in 
visual auditory discrimination test (Beta=.265, t(132)=3.264, p=.001), (2) the total time 
taken from participants to complete the sequencing subtest (Beta=-.239, t(132)=-2.829, 
p=.005) and (3) the total number of correct responses in acoustic memory (Beta=.215, 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
259 
t(132)=2.390, p=.018). This finding is discussed further in the Discussion section with 
respect to its importance in designing screening tests which can be delivered in 
reasonably short time and which would have reasonably reliable confidence. 
4   Discussion 
4.1   Discussion of the Experimental Results 
The purpose of this study was to examine whether a combination of video-game-like 
non verbal tests that evaluate different modalities could predict reading and learning 
abilities/disabilities of children 6-12 years of age. The first part of the analysis 
reported here has demonstrated that the combined assessment of a number of 
modalities can serve as a good predictor for reading. The second part of the analysis, 
which was conducted in order to examine whether MAPS subtests can predict non-
verbal IQ has also revealed interesting results. The stepwise regression with all MAPS 
subtests showed that auditory visual discrimination, sequencing and acoustic memory 
serve as good predictors of non-verbal IQ. Therefore, we argue that MAPS tests can 
be used in various combinations of their subtests to evaluate learning parameters and 
possibly diagnose learning difficulties. MAPS tests have been shown to predict both 
verbal (i.e., measures related to reading performance) and non-verbal (i.e., Raven’s 
Progressive Matrices) abilities, which in combination may provide a reasonably good 
index for learning abilities. 
The experimental results have therefore provided support for both the first and 
second hypothesis. Interestingly, the combined measures of lateralization and 
sequencing alone can explain 42.5% of the variance at a very high level of 
significance (F=48.486, df=2, p<.001). Three measures, i.e., of lateralization, 
sequencing and acoustic discrimination can explain 47.8% of the variance (F=39.645, 
df=3, p<.001). Lateralization, categorization, acoustic discrimination, acoustic 
memory take the number up to 51.9% (F=34.762, df=4, p<.001) and all five to 54% 
(F=29.979, df=5, p<.001).  
The regression analysis for different combinations of tests revealed that some test 
combinations provide better prediction models than others. Therefore, the analysis of 
the experimental data supported the first hypothesis and provided scientific grounding 
for the construction of a smaller battery of tests, yet capable of explaining significant 
part of the variance. We know that human performance on diverse tests of intellect is 
impacted by a "general" regulatory factor that accounts for up to 50% of the variance 
between individuals on intelligence tests [24]. We have similar findings also from 
animal experiments. Metzel et al. [24] assessed mice ability to learn new tasks using 
multiple test batteries. Indicative of a common source of variance, positive 
correlations were found between individuals' performance on all tasks, in a way 
analogous to our findings here. Factor analysis of learning performance variables 
determined that a single factor accounted for 38% of the total variance. Their results 
indicate that diverse learning abilities of laboratory mice are influenced by a common 
source of variance. Collectively, these results highlight the value of multimodal 
assessment, but at the same time underline the need to discover the underlying 
pathways and mechanisms of interactions between the various modalities.  

260 
Y. Laouris, E. Aristodemou, and P. Makris 
Some mental attributes have not been found to relate to the control set of tests. For 
example, the navigation ability did not turn out to contribute to the variance of the 
model in predicting reading performance, as we would have expected based on our 
introductory discussion. However, it is important to note that the tests used here 
feature very simple graphical interfaces and in no ways resemble virtual spaces like 
the ones encountered by learners today. We hypothesize that visual-spatial navigation 
in familiar and unfamiliar environments is an essential requirement not only of daily 
life, but also for the “browsing” and the “acquisition” of knowledge when immersed 
in cyberspace equivalent environments. For example, we know from animal 
experiments the importance of the hippocampus for navigation [25], which constitutes 
at the same time the single most important circuit for any new knowledge to take 
place. Ohnishi [25] has shown task related activity in the hippocampus, 
parahippocampal gyrus, posterior cingulate cortex, precuneus, parietal association 
areas, and the visual association areas. More work engaging cyberspace equivalent 
learning environments will be required to investigate the role of navigation and other 
analogous non-verbal abilities in learning. 
4.2   Relevance to Instructional Design 
The work reported here which uses video-game like interfaces to assess various 
modalities is relevant to contemporary challenges in instructional design and 
assessment. The ways in which we interact with other people, with our environment, 
with past and current experiences, with information, know-how and knowledge are 
changing. In particular, the appearance of mobile technologies in conjunction with the 
global connectivity between humans as well as with inanimate worlds through the 
Internet marks the transition to a new evolutionary stage in our development. Many 
use the term “mobile” as synonym to mobile phone and the term “internet” as 
synonym to cyberspace. These are over simplifications. We have proposed previously 
the thesis that the appearance of mobile technologies has signaled a major milestone 
in human evolution. While the computer constitutes the first human construction that 
aspired to amplify mental rather than physical human powers (in contrast to all 
previous human constructs [26], [27], mobile technologies and cyberspace mark the 
appearance of new “organs” in the time line of human evolution: (a) one that extends 
the hearing system, both on the receiving (i.e., hearing) and the sending (i.e., speech) 
side and (b) one that extends the visual system enabling it to read and interact with 
books and see and interact with things and/or events that happen(ed) remotely. 
Together, they demolish distance, they demolish boundaries (private or public), and 
they will soon even demolish the very concept of what it means to be here or there. 
These cyber-organs enable considerable extension of the auditory and visual 
modalities. At the same time they require the engagement of additional brain circuits 
to be able to “navigate” through knowledge. Immersed within a plethora of spaces at 
the same time they will need to engage for example circuits involved in navigation, 
orientation, categorization and sequencing. The significance of the above changes for 
those working in instructional design in particular and more generally in education is 
enormous. It is within this context that we have defined mobile learning as “an 
evolutionary opportunity to reconsider the role and the methods of education, in light 
of relevant technological advances and pressing social priorities” [27].  Consider for 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
261 
example the educational challenges of children with learning difficulties or 
differences or the physically disadvantaged. When used correctly, mobile 
technologies and the Internet could make their problem(s) disappear [28]. However, 
in order to be able to design efficient learning environments tailored to the needs of 
the individual, we will need to know a lot more about the learner; mental attributes 
beyond those used in classic assessment become important. This why the authors 
propose MAPS as an instrument to evaluate non-verbal abilities which we 
hypothesize are relevant to learning in new media environments. Knowledge of the 
individual’s non-verbal abilities as well as knowledge of how such modalities might 
interact is a pre-requisite for the design of more efficient learning environments, in 
line with Schnotz’s [29] statement that “the successful use of technologies in learning 
and instruction is dependent on a better "understanding of the interplay between 
different forms of external representations and the learners' internal mental structures 
and cognitive processes.” 
 
Multimodal approach and multimedia: For example, Meyer, in his ‘Cognitive 
Theory of Multimedia Learning,’ promoted for years that animations that are co-
presented with narrations facilitate learning of facts and concepts [30]. He bases his 
arguments on the principle of ‘dual coding’ [31], which claims that information 
entering the system through multiple processing channels helps circumvent the 
limited processing capabilities of each individual channel. The hypothesis is that the 
total amount of information that can be processed is greater when it is spread between 
multiple senses. Furthermore, multimodal processing reduces cognitive load because 
information from different modalities is processed differently [32], [8]. In sum, these 
arguments can be collectively interpreted to claim that the more modalities we 
exploit, the better. This approach is somewhat analogous to the prescription of 
broadband antibiotics against an unknown infection. A much better approach would 
be to be able to prescribe a learning approach tailored to the strengths and weaknesses 
of the learner. Along these lines, parallel work in our lab takes this research one step 
further [15]. We hypothesize that learners with different processing powers and/or 
preferences for one over another modality will benefit more if information is 
presented to them in ways that exploit predominantly the preferred pathways. This 
however assumes, as explained also above, that we have adequate methods to assess 
the different modalities and that our learning environments exploit these modalities 
differently for different learners. In a preliminary report we argued that learning 
materials that are designed according to the Meyer’s multimedia principle, the 
modality principle (i.e., students learn better from animation and auditory narration 
than from animation and on-screen text), and the redundancy principle (i.e., students 
learn better from animation and auditory narration than from animation, narration, and 
text) may compensate for ability deficits of students with reading difficulties [15]. In 
that study we have used the cognitive MAPS tests to assess the differential abilities of 
the learners and identify preference for one over another modality.  
4.3   Methodological Issues Related to Multimodal Assessment 
The approach adopted is somewhat analogous to the approach used in medical image 
segmentation to obtain a labeled image where each label corresponds to the real 

262 
Y. Laouris, E. Aristodemou, and P. Makris 
anatomy of the patient. However, the work described here lacks any data regarding 
the possible corresponding pathways in the brain and the neuronal correlates of the 
scores obtained in the MAPS tests. Zhang [33] distinguished evaluation methods 
between empirical (based on the study of the results) and analytical (based only on 
intrinsic features of the methods). The MAPS is an empirical method. Typically, 
empirical methods are divided into goodness and discrepancy methods, where the first 
type are based on the study of the results themselves, and the second compare the 
results with a “reference or ground” truth. The MAPS belongs to the second category 
because it found high correlations with well-established measures that describe 
reading and learning such as Rapid Naming of Letters, Reading Measures, Word 
Identification and Word Attack. 
The multimodality approach to evaluation and assessment has found application 
also in the evaluation of medical conditions. For example Ishibashi et al. [34] have 
presented a case of a histo-pathologically-confirmed Rasmussen's encephalitis, which 
they evaluated using a multimodal process that explored the physical and functional 
aspects of the associated epilepsy. The process included magnetic resonance imaging, 
single photo emission computed tomography, electroencephalography, and magneto-
encephalography. They argued that the multimodal approach greatly assisted the 
surgical treatment decision-making process. Similarly, Cardenes et al. [35] applied 
multimodal segmentation evaluation and showed experimentally that the combination 
of various measures improved the quality of their evaluation. 
In line with more recent scientific findings, we propose to move away from the 
older thesis that different sensory modalities operate as separate and independent 
processes. Loyal to its principles, the science of bionics1 led in previous centuries to 
the construction of computers that complied with the modularity principle: i.e., first-
generation robotics simulated no interactions between their “sensory” systems.  
Current technologies, exploit multimodal sensing in order to design reliable 
environment sensors [36]. The questions surrounding the principles and mechanisms 
of the fusion of multiple modalities are becoming an increasingly important research 
domain. Models range from framework able to accept different single and multi 
modality systems and to automatically adapt the fusion algorithm [37] to models 
based on complex systems theory [38]. For example, Fang and McKenzie [39] were 
inspired by multifunctional neural networks discovered in biological brain to devise 
artificial neural networks with multifunctional learning abilities related to the ability 
of navigation: they evaluated their system on its ability to learn two kinds of 
navigation abilities at the same time: to explore unknown environments as far as 
possible, and to reach designated goals in the environments. They hypothesized that 
since these two functions share network mechanisms, learning of one function can 
benefit learning of another. Their experiment supported the concept. 
5   Conclusions and Future Directions 
The long-term goal of this line of research is to develop instruments to assess various 
non-verbal attributes of the brain and explore their possible interactions with the 
                                                           
1 Application of biological systems found in nature to the design of modern technology. 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
263 
learning process. The purpose is to facilitate the development of learning 
environments and methodologies that can take into account the specific preferences 
and abilities of the learner. To achieve these goals we will need to (a) optimize the 
assessment; (b) discover the mechanisms of multi-modal integration. 
 
Test optimization: The correlations reported here do not prove causality. In order to 
establish the predictive power required for developing educational design guidelines, 
it would be necessary to show that improving the investigated and (in correlation) 
relevant non-reading skills results in improvement of the reading skills or other 
learning abilities. This would require a longitudinal study of the development of the 
non-reading and reading (or other learning) skills of individual students. It would also 
be interesting to conduct research on the effect on reading and or other learning 
related attributes of developing the aforementioned non-reading skills. 
The combination of Lateralization, Categorization, Acoustic Discrimination and 
Acoustic Memory tests featured the highest R2 = .519. In order to actually decide for 
an optimal combination of tests to be used in practice, it would be appealing to utilize 
a model that takes into account also the time required to deliver and complete each 
test. It is conceivable that a minor sacrifice in the R2 value could result to a battery 
that can be completed in less time thus making it more attractive for the practitioner. 
For example, since the Acoustic Discrimination test takes the longest to complete, one 
might settle for the Lateralization, Categorization, Sequencing, Acoustic Memory 
combination, which features R2 value = .504. 
 
Multi-modal integration: This paper describes preliminary efforts to contribute 
towards the design of future learning environments, which will match a multi-
sensorial delivery of information to the diverse processing capabilities of the learner. 
Future work needs to address in more detail the interplay between modalities, answer 
questions such as how general is the multisensory benefit to learning and whether is it 
is restricted to certain types of information. Furthermore, we need to uncover the 
mechanisms by which multisensory stimulation modulates activity of other pathways. 
Finally, the greatest challenge is to facilitate dialogue between educators and 
technologists so that the theoretically grounded learning environments can indeed be 
implemented. 
 
Acknowledgments. The research was funded by a Research Promotion Foundation 
grant awarded to Cyprus Neuroscience & Technology Institute. The authors 
acknowledge Dr. Costas Apostolides and Mrs. Dina Zakou, past and current Presidents 
of the Cyprus Dyslexia Association, Dr. Charles Haynes, past-Vice President of the 
International Dyslexia Association, Dr. Timotheos Papadopoulos and Mrs. Christiane 
El-Haddad for their contributions in the experiments, analysis and dissemination. An 
unknown reviewer is also acknowledged for valuable comments and suggestions. 
References 
1. Broadbent, D.E.: Perception and Communication. Pergamon, Oxford (1958) 
2. Calvert, G.: The Handbook of Multisensory Processes. MIT Press, New York (2004) 

264 
Y. Laouris, E. Aristodemou, and P. Makris 
3. Ghazanfar, A.: Is neocortex essentially multisensory? Trends in Cognitive Science 10, 
278–285 (2006) 
4. Driver, J.: Multisensory interplay reveals crossmodal influences on ‘sensory-specific’ 
brain regions, neural responses, and judgments. Neuron 11, 11–23 (2008) 
5. Shimojo, S.: Sensory modalities are not separate modalities: plasticity and interactions. 
Curr. Opin. Neurobiol. 11, 231–239 (2001) 
6. Schroeder, T.A., Smiley, C.E., Fu, J., McGinnis, K.G., O’Connell, T., Hackett, M.N.: 
Anatomical mechanisms and functional implications of multisensory convergence in early 
cortical processing. Int. J. Psychophysiol. 50, 5–17 (2003) 
7. Learning Disabilities Association of Canada, http://www.ldac-taac.ca 
8. Mayer, R.E.: Cognitive constraints on multimedia learning: when presenting more material 
results in less understanding. J. Educ. Psychol. 93, 187–198 (2001) 
9. Montessori, M.: The Montessori Method. Frederick Stokes (1912) 
10. Fernald, G.: The effect of kinesthetic factors in development of word recognition in the 
case of non-readers. J. Educ. Res. 4, 355–377 (1921) 
11. Orton, S.T.: Specific reading disability–strephosymbolia. J. Am. Med. Assoc. 90, 1095–
1099 (1928) 
12. Nemenyi, E., Szendrei-Radnai, J., Varga, T.: The curriculum of mathematics for the grades 
from 1 to 8 of Hungary’s general educating schools. The Jhos Bolyai Math. Society, 
Budapest (1974) 
13. Paivio, A.: Mental representations: a dual coding approach. Oxford University Press, 
Oxford (1986) 
14. Paivio, A.: Mind and Its Evolution; A Dual Coding Theoretical Approach. Psychology 
Press, San Diego (2006) 
15. Taraszow, T., Wahl, J., Laouris, Y., Scheiter, K., Gerjets, P.: Using Dynamic Visualization 
with Written Explanation to By-Pass Information Processing Deficits of Children with 
Reading Difficulties. In: EARLI Conference, Budapest (2007) 
16. Laouris, Y., Makris, P.: M.A.P.S. Mental Attributes Profiling System computerized battery 
for dyslexia assessment. In: Multilingual & Cross-Cultural Perspectives on Dyslexia, 
Omni Shoreham Hotel, Washington, D.C. (2002) 
17. Papadopoulos, T.C., Laouris, Y., Makris, P.: The validation of a computerised cognitive 
battery of tests for the diagnosis of children with dyslexia. In: IDA 54th Annual 
Conference, San Diego (2003) 
18. Aristodemou, E., Taraszow, T., Laouris, Y., Papadopoulos, T., Makris, P.: Prediction of 
Reading Performance Using the MAPS (Mental Attributes Profiling System) Multimodal 
Interactive ICT Application. In: 7th European Conference on e-learning, Ayia Napa (2008) 
19. Piaget, J.: Judgement and reasoning of the child. Paul Kegan, London (1928) 
20. Wimmer, H., Mayringer, H., Landerl, K.: The double deficit hypothesis and difficulties in 
learning to read a regular orthography. Journal of Educational Psychology 92, 668–680 
(2002) 
21. Woodcock, R.W.: Woodcock reading mastery tests – Revised NU: Examiner’s manual. 
American Guidance Service, Circle Pines (1998) 
22. Papadopoulos, T.C.: Phonological and cognitive correlates of word-reading acquisition 
under two different instructional approaches. European Journal of Psychology of 
Education 26, 549–568 (2001) 
23. Papadopoulos, T.C., Charalambous, A., Kanari, A., Loizou, M.: Kindergarten intervention 
for dyslexia: The PREP remediation in Greek. European Journal of Psychology of 
Education 19, 79–105 (2004) 

 
Prediction of Learning Abilities Based on a Cross-Modal Evaluation 
265 
24. Matzel, L.D., Han, Y.R., Grossman, H., Karnik, M.S., Patel, D., Scott, N., Specht, S.M., 
Gandhi, C.C.: Individual Differences in the Expression of a “General” Learning Ability in 
Mice. The Journal of Neuroscience 23, 6423–6433 (2003) 
25. Ohnishi, T., Matsuda, H., Hirakata, M., Ugawab, Y.: Navigation ability dependent neural 
activation in the human brain: An fMRI study. Neuroscience Research 55, 361–369 (2006) 
26. Laouris, Y., Laouri, R.: Can Information and mobile technologies serve close the 
economic, educational, digital and social gaps and accelerate development? World 
Futures 64, 254–275 (2008) 
27. Laouris, Y., Eteokleous, N.: We need an educationally relevant definition of mobile 
learning. In: 4th World Conference on Mobile Learning, Cape Town (2005) 
28. Nyiri, K.: The mobile phone in 2005: Where are we now? In: Proceedings, Seeing 
Understanding, Learning in the Mobile Age, Budapest (2005) 
29. Schnotz, W.: Visual learning with new technologies: Introduction. European Journal of 
Psychology of Education XIV, 245–265 (1999) 
30. Harp, S.F., Mayer, R.E.: How seductive details do their damage: a theory of cognitive 
interest in science learning. J. Educ. Psychol. 93, 187–198 (1998) 
31. Clark, J.M., Paivio, A.: Dual coding theory and education. Educ. Psychol. 37, 250–263 
(1991) 
32. Bagui, S.: Reasons for increased learning using multimedia. J. Educ. Multimed. 
Hypermedia 7, 3–18 (1998) 
33. Zhang, Y.: A review of recent evaluation methods for image segmentation. In: Int. 
Symposium on Signal Proc. and its Applications (ISSPA) (2001) 
34. Ishibashi, H., et al.: Multimodality functional imaging evaluation in a patient with 
Rasmussen’s encephalitis. Brain and Development 24, 239–244 (2001) 
35. Cardenes, R., et al.: Multimodal Evaluation for Medical Image Segmentation. In: 
Kropatsch, W.G., Kampel, M., Hanbury, A. (eds.) CAIP 2007. LNCS, vol. 4673, pp. 229–
236. Springer, Heidelberg (2007) 
36. Stillman, S., Essa, I.: Towards reliable multimodal sensing in aware environments. In: 
ACM International Conference Proceeding Series, Orlando (2001) 
37. Paleari, M., Lisetti, C.L.: Toward multimodal fusion of affective cues. In: International 
Multimedia Conference Proceedings of the 1st ACM international workshop on Human-
centered multimedia, Santa Barbara (2006) 
38. Thórisson, K.R.: Modeling Multimodal Communication as a Complex System. In: 
Wachsmuth, I., Knoblich, G. (eds.) ZiF Research Group International Workshop. LNCS 
(LNAI), vol. 4930, pp. 143–168. Springer, Heidelberg (2008) 
39. Wang, F., McKenzie, E.: Multifunctional learning of a multi-agent based evolutionary 
artificial neural network with lifetime learning. In: Proceedings of IEEE International 
Symposium on Computational Intelligence in Robotics and Automation. CIRA (1999) 

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 266–275, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Automatic Sentence Modality Recognition in Children’s 
Speech, and Its Usage Potential in the Speech Therapy 
Dávid Sztahó, Katalin Nagy, and Klára Vicsi 
Laboratory of Speech Acoustics, Budapest University of Technology and Economics, 
Department of Telecommunications and Media Informatics,  
Stoczek u. 2, 1111 Budapest, Hungary 
sztaho@tmit.bme.hu, menjus@gmail.com, vicsi@tmit.bme.hu 
Abstract. In the Laboratory of Speech Acoustics prosody recognition 
experiments have been prepared, in which, among the others, we were 
searching for the possibilities of the recognition of sentence modalities. Due to 
our promising results in the sentence modality recognition, we adopted the 
method for children modality recognition, and looked for the possibility, how it 
can be used as an automatic feedback in an audio - visual pronunciation 
teaching and training system. Our goal was to develop a sentence intonation 
teaching and training system for speech handicapped children, helping them to 
learn the correct prosodic pronunciation of sentences. In the experiment basic 
sentence modality models have been developed and used. For the training of 
these models, we have recorded a speech prosody database with correctly 
speaking children, processed and segmented according to the types of 
modalities. At the recording of this database, 59 children read a text of one 
word sentences, simple and complex sentences. HMM models of modality 
types were built by training the recognizer with this correctly speaking children 
database. The result of the children sentence modality recognition was not 
adequate enough for the purpose of automatic feedback in case of pronunciation 
training. Thus another way of classification was prepared. This time the 
recordings of the children were sorted rigorously by the type of the intonation 
curves of sentences, which were different in many cases from the sentence 
modality classes. With the new classes, further tests were carried out. The 
trained HMM models were used, not for the recognition of the modality of 
sentences, but checking the correctness of the intonation of sentences 
pronounced by speech handicapped children. Therefore, an initial database, 
consisting of the recordings of the voices of two speech handicapped children 
had been prepared, similar to the database of healthy children.  
Keywords: Speech Prosody Recognition, Automatic Speech Recognition, 
Prosody Database, Speech Technology, Hidden Markov Models. 
1   Introduction 
The latest results in computer technology and digital speech processing made possible 
the construction of computer aided systems that improve the quality of speech 

 
Automatic Sentence Modality Recognition in Children’s Speech 
267 
learning and training. Speech training using this speech technology is useful for 
speech handicapped persons, especially hard of hearing and deaf children. These 
children partially or fully cannot learn the way of correct pronunciation, including not 
only the phonemes, but also larger segmental features, like prosody because of the 
distortion of auditory feedback. Recently, speech technology has appeared in second 
language learning too. The capabilities of the technology include measuring and 
displaying the dynamic characteristics of speech parameters, using auditive, visual 
and automatic feedback [1]. Experiments have shown, that for example a visual F0 
display of supra-segmental features combined with audio feedback is more effective 
than audio feedback alone [2][3], especially if student’s F0 contour is displayed along 
with a reference model. The feasibility of this type of visual feedback has been 
demonstrated by a number of simple prototypes [14] [15]. 
Our main goal is to develop a speech teaching system for speech handicapped 
children through a computer-aided system. This way, we are going to teach the 
correct pronunciation of the different sentence modalities by visual and automatic 
feedback. Institutions that treat speech handicapped children, for example with hard 
of hearing disability, will be able to use the method for improving the children’s 
pronunciation. To determine the final use of the method, like frequency of use and 
exact usage, needs discussion with experts on speech therapy of children. 
Until now, teaching systems that have automatic feedback are based only on 
segmental features, not on supra-segmental features. One group of these systems takes 
advantage of the acoustic similarity between the trainee’s acoustic production and a 
template to measure the correctness of trainee’s production. In SPECO [4] and ISTRA 
[5] similar metric was estimated. In the other group of speech training systems, 
phoneme-based Hidden Markov Models are applied [16] [17]. 
In this article we looked for the possibility, how an automatic prosodic recognizer, 
actually a sentence modality recognizer can be used for the automatic feedback in a 
pronunciation teaching system.   
In the first part of our research, a prosodic database of children was prepared. It 
consisted of the recordings of healthy children. The recorded database was segmented 
and labeled manually according to the Hungarian sentence modality classes.  
After the database preparation a sentence modality recognition test was carried out 
by an automatic prosodic recognizer, developed earlier in our Laboratory [6]. 
The result of the children sentence modality recognition was not adequate enough 
for the purpose of automatic feedback in case of pronunciation training. 
Thus another way of classification was prepared. This time the recordings of the 
children were sorted rigorously by the type of the intonation curves of sentences, 
which were different in many cases from the sentence modality classes. With the new 
classes, further tests were carried out, by training and testing the prosodic recognizer. 
This way the recognition correctness increased to a great extent. 
In order to test the real usage of the recognizer, a small database was made, 
consisting of recordings of two speech impaired children. The structure of this 
database corresponded with the first database. A method was examined with which 
the recognizer can be used for teaching prosody. 

268 
D. Sztahó, K. Nagy, and K. Vicsi 
2   Former Recognition System 
For sentence modality recognition a formerly developed automatic prosodic classifier 
was used [6]. This recognition system is based on prosodic Hidden Markov Models. 
The features used for the training of these prosodic models, and for the recognition 
are fundamental frequency and energy values, their derivates and second derivates. 
The prosodic HMM models were built using HTK toolkit [8].  
To train this recognizer, speech databases were processed according to the types of 
modalities that were used. An HMM was assigned to each modality, then the aligned 
modality sequence of the incoming speech was searched for.  
For the extraction of fundamental frequency and energy values the Snack toolkit 
was used [7]. The calculation was done with 150 ms window size and 10 ms time 
step. Fundamental frequency values were corrected by anti-octave filtering and 
median filtering. The recognition was done for six sentence modality classes – 
declarative, question to complement, yes-no question, imperative, sentence with wish 
(“if only…”) and clause (not closing) – and one silence model. The recognizer was 
trained with MRBA [9] and BABEL [10] databases.  
3   Database 
It is necessary to have a correctly built database, for every statistical-based 
recognition system, which consists of the acoustic signals needed to train and test the 
recognition system. Because our goal was to develop a sentence modality recognizer, 
we needed a database consisting of recordings of the appropriate sentence modality 
classes. 
Two text materials were prepared. The first text contained on average 10 individual 
sentences from each modality class. These sentences were obtained from SPECO 
[11], software developed in our laboratory for children speech therapy. The second 
text contained 3 short dialogues, made from the sentences of the first text. All of the 
sentences were used. Some more sentences were needed to be added in order to build 
appropriate dialogues. These added sentences were not used during the recognition 
experiments. 
The recording condition is shown on figure 1. Children were sitting in front of a 
MONACOR ECM-100 microphone and read both texts. Every sentence was recorded 
twice, in order to ensure at least one good recording from each sentence. The 
microphone was connected to a CREATIVE Sound Blaster Audigy 2 NX external 
sound card which was further connected to a portable computer.  
The database was recorded in an elementary school at Budapest [Elementary 
School of Farkasrét in Budapest] with 60 healthy and correctly speaking children. The 
age of the children was between 8 and 13. This database was used for training the 
recognition system and evaluating it. For further use, the database was manually 
annotated by marking the sentence modalities for each recording. After annotating all 
sentences, the average number of samples per each class was 1600. 

 
Automatic Sentence Modality Recognition in Children’s Speech 
269 
 
Fig. 1. Recording condition 
4   Adaptation 
At the evaluation phase, 70 % of the samples were used for the training of the 
recognizer, and 30% was used for testing. The results are shown on table 1. 
Table 1. Sentence modality recognition results. Classes: S: Declarative K: Question to 
complemented E: Yes-no question FF: Imperative O: Sentence with wish (“if only…”) T: 
Clause (not closing) U: Silence 
 
S 
K 
E 
FF 
O 
T 
U 
Corr (%) 
S 
895 
279 
91 
353 
240 
41 
6 
47 
K 
49 
579 
50 
103 
68 
36 
1 
65,3 
E 
65 
158 
862 
95 
110 
34 
2 
65 
FF 
69 
98 
52 
505 
126 
44 
4 
56,2 
O 
25 
12 
6 
4 
131 
7 
0 
70,8 
T 
9 
35 
14 
21 
51 
255 
12 
64,2 
U 
106 
56 
45 
51 
42 
46 
5791 
94,4 
Without the quiet parts, the overall result was 61.42 percent. This performance is 
good for a system which is for example an added system to a speech recognizer, but it 
is not acceptable for a pronunciation teaching system. Two major difficulties occurred 
during the evaluation tests. First, in some cases there were differences in intonation 
contours between samples belonging to the same class, because not all the children 
said the same modality class with the same prosodic characteristics. Second, the 
complexity of the sentences was different, there were one-word, simple and complex 
sentences in the case of some classes. 
In order to solve these problems, new classes were created, replacing the former 
ones. The new classes were based strictly on the intonation contour of the sentences. 
The new classes are shown in table 2. Examples of fundamental frequency contours 
can be seen on figures 2 and 3. 
External Sound 
Card 
Monacor ECM-100 
Microphone 
Portable Computer 
Creative Sound Blaster 
Audigy 2 NX 

270 
D. Sztahó, K. Nagy, and K. Vicsi 
Table 2. New classes 
TYPE OF 
INTONATION 
TYPE OF SENTENCE 
EXAMPLE 
FORM OF 
INTONATION 
Descending 
declarative sentence 
“Anikó is standing at 
the gate” 
 
Falling 
question to be 
complemented 
“Why is Anikó standing 
there?” 
 
Ascending-falling 
yes-no questions 
“Is Anikó standing at 
the gate?” 
 
Falling-descending 
imperative and 
exclamation sentences 
“Come here quickly!” 
 
Floating 
clauses (not closing) 
“Anna is standing at the 
corridor, …” 
 
Rising 
one word questions 
“No?” 
 
 
Fig. 2. Examples for classes of table 2. (A: descending; B: falling; C: ascending-falling) 
For the new recognition tests the sentences were selected as follows. From each 
class there was 1 given sentence selected per child. On average, each child said each 
sentence two times. If a given child did not read a sentence correctly, another 
sentence was selected from the same child. Because there were 60 children in the 
database, there were on average 120 sentences selected per class. As a result of this,  
 

 
Automatic Sentence Modality Recognition in Children’s Speech 
271 
 
Fig. 3. Examples for classes of Table 2. (A: falling-descending; B: floating; C: rising) 
we got a sentence set, that hereafter we will call “sentence set 1”. The above process 
was repeated on different sentences per class, and this way we got another sentence 
set, that hereafter we will call “sentence set 2”. With these two different sentence sets 
it was possible to tests grammatically similar and diverse sentences by mixing them 
together. The two sentence sets are shown in Table 3. 
Table 3. Sentences of the two sentence sets 
Class 
Sentence set 1 
Sentence set 2 
A 
David has the croissant. 
Peti has got the hat. 
B 
Oh, how am I glad! 
If only he was standing there! 
C 
Does he have got the hat? 
Did you get a mark today? 
D 
You must stand there too! 
Come here quickly! 
E 
Anna is standing at the corridor, (…) 
Anna goes with you, (…) 
F 
Want some? 
But? 
Table 4 shows, that during the initial tests, in case of some classes, the automatic 
recognizer could not separate classes correctly, in the case of classes that have similar 
prosodic characteristics. These classes were the “Falling” and the “Falling-
descending”. Further on these classes were contracted. 

272 
D. Sztahó, K. Nagy, and K. Vicsi 
Table 4. Recognition results with sentence set 1 for six classes. Dark fields show, that in case 
of classes B and D the system could not decide unequivocally. (Classes: A: descending;  
B: falling; C: ascending-falling; D: falling-descending; E: floating; F: rising) 
 
A 
B 
C 
D 
E 
F 
CORR [%]
A 
40 
0 
1 
1 
1 
0 
93.0 
B 
0 
25 
0 
7 
0 
0 
78.1 
C 
0 
0 
28 
1 
0 
0 
96.6 
D 
1 
15 
0 
13 
1 
0 
43.3 
E 
5 
2 
1 
1 
30 
0 
76.9 
F 
0 
0 
0 
0 
0 
36 
100.0 
With the reduced class set (B and D are contracted) the recognition results were 
above 90 percent (table 5). When the two sentence sets were mixed, the recognition 
went below 87 percent. In order to see the effect of more diverse training data, a third 
sentence set was chosen in the same way as the other two. Figure 4 shows, what kind  
 
Table 5. Recognition results with sentence set 1 for reduced class set 
 
A 
B 
C 
E 
F 
CORR [%] 
A 
42 
1 
0 
0 
0 
97.7 
B 
1 
51 
5 
4 
1 
82.3 
C 
0 
0 
27 
1 
1 
93.1 
E 
1 
1 
1 
36 
0 
92.3 
F 
0 
0 
0 
0 
36 
100 
 
Fig. 4. Effect of the diverse training data on automatic recognition 

 
Automatic Sentence Modality Recognition in Children’s Speech 
273 
of effect has the more diverse training data on the recognition. It shows that the best 
recognition can be achieved by using less diverse training and testing data. Therefore 
in the real teaching system, only a specific data set should be used at a time. 
5   Testing Method with Hard of Hearing Children 
The main goal of the research is to develop a recognition system that can be used in a 
teaching system for hard of hearing children. In order to do this, the recognition 
system built in the previous chapter was used for a small database of speech 
handicapped children’s recordings, which was recorded in a special institute for 
hearing handicapped children. This database consisted of recordings from two speech 
impaired children. The correct operation of the recognizer means that if the child says 
the sentence with bad pronunciation, the recognizer will classify it falsely. On the 
other hand, if the pronunciation is good, the recognizer has to classify it correctly. On 
Figure 5, there is an example shown, how the recognition system can be used in a real 
environment. The first fundamental frequency contour is from a healthy child, the 
second is from a hard of hearing child, and both of them were classified correctly. The 
third recording is from a hard of hearing child, and the recognizer did not classify it 
correctly to the same class as the others. 
 
Fig. 5. Recognition of recording of hard of hearing children illustrated by fundamental 
frequency. (Description: (a) recording from healthy child; (b) a correctly classified recording 
from hard of hearing child; (c) a falsely classified recording from hard of hearing child) 
It can be seen from the examples presented above that the problem of the automatic 
feedback is that we do not know when the recognizer has to refuse the pronunciation, 
and when the recognizer can accept the actual one. It must be worked out in the 
future. Otherwise the strictness can be adjusted by modifying the probability of the 
acceptance. 

274 
D. Sztahó, K. Nagy, and K. Vicsi 
6   Conclusion 
In the paper we presented an automatic sentence intonation recognition method, that 
can be used as an automatic feedback in a prosody teaching system in the future. First, 
a database was created that consisted of recordings from healthy children. The text 
material of the database was assembled from two texts. The first consisted sentences 
of six Hungarian modality classes. The sentences were selected from formerly 
developed software, SPECO. The second text was made from the first text, making 
three dialogues using the same sentences. The built database was segmented and 
annotated marking the sentence modality classes. 
After the initial tests it was clear that the original six sentences modality classes 
used for speech recognition purposes were not appropriate, therefore a new annotation 
was made. Sentences were categorized strictly according to their intonation, and thus 
the recognition increased to a great extent. 
Verifying tests were made with the database of the healthy children in order to 
prepare the recognizer to its real usage. The results showed that the recognition ratio 
decreased in the case of more diverse training data. Thus it is clear, that for 
pronunciation teaching purposes only well selected training data are acceptable for the 
contraction of the reference sentence intonation models in the recognizer. 
In the future, it is necessary to develop an optimization technique on the base of 
which we can adjust the probability of the acceptance of the recognizer. Thus we plan 
to prepare a large sentence intonation database of impaired children. Moreover for 
tuning the probability of the acceptance of the automatic recognizer we plan a 
perception experiment, in which human listeners will have to categorize the different 
sentences by listening to them. This perception experiment makes us able to examine 
the listener’s acceptance of different sentences. 
Acknowledgement 
We would like to thank to “Dr. Béla Török” Kindergarten, Elementary School and 
Special School, and Elementary School of Farkasrét making available for us to make 
recordings with the children.  
References 
1. Vicsi, K.: Computer-Assisted Pronunciation Teaching and Training Methods Based on the 
Dynamic Spectro-Temporal Characteristics of Speech. In: Dynamics of Speech Production 
and Perception, pp. 283–304. IOS Press, Amsterdam (2006) 
2. de Bot, K.: Visual feedback of intonation: Effectiveness and induced practice behavior. 
Lang. Speech 26(4), 331–335 (1983) 
3. James, E.: The acquisition of prosodic features of speech using a speech visualizer. 
IRAL 14(3), 227–243 (1976) 
4. Vicsi, K., Csatári, F., Bakcsi, Z., Tantos, A.: Distance score evaluation of the visualized 
speech spectra at audio-visual articulation training. In: Proc. Eurospeech, pp. 1911–1914 
(1999) 

 
Automatic Sentence Modality Recognition in Children’s Speech 
275 
5. ISTRA Indiana Speech Training Aid Features. Bloomington, IN: Communication Disorders 
Technology, Inc. (2003),  
  http://www.comdistec.com/istra_faq.shtml 
6. Vicsi, K., Szaszák, G.: Using Prosody for the Imporvement of ASR - Sentence Modality 
Recognition. In: Proc. of Interspeech 2008, Bristol (2008), ISCA Archive,  
  http://www.isca-speech.org/archive 
7. The Snack Sound Toolkit, http://www.speech.kth.se/snack/ 
8. HTK Speech Recognition Toolkit, http://htk.eng.cam.ac.uk/ 
9. Vicsi, K., Velkei, S., Szaszák, G., Borostyán, G., Gordos, G.: Speech recognizer for 
preparing medical reports. In: Development experiences of a Hungarian speaker 
independent continous speech recognizer, Híradástechnika, 2006/7, pp. 22–27 (2006); 
Hungarian Reference Speech Database (MRBA),  
  http://alpha.tmit.bme.hu/speech/hdbMRBA.php  
10. Roach, P., Vicsi, K., Gordos, G.: Report on BABEL, an Eastern European Multi-language 
database. In: COST 249 meeting, Zurich, October 17-18 (1996) 
11. Vicsi, K., Váry, Á.: Distinctive training methods and Evaluation of a Multilingual, 
Multimodal Speech Tranining System. In: SPECO Proc. 4th Intl. Conf. Disability, Virtual 
Reality and Assoc. Tech., Veszprém, Hungary, pp. 47–52 (2002) 
12. Szaszák, G., Vicsi, K.: Speech recognition supported by prosodic information for fixed 
stress languages. In: Proceeding of TSD Conference, Brno, pp. 262–269 (2007) 
13. Szaszák, G., Vicsi, K.: Using Prosody in Fixed Stress Languages for Improvement of 
Speech Recognition. In: Workshop Vietri (2007) 
14. Anderson-Hsieh, J.: Interpreting visual feedback in computer-assisted instruction on 
suprasegmentals. CALICO Journal 11(4), 5–22 (1994) 
15. Hiller, S., Rooney, E., Lefevre, J.P., Jack, M.: SPELL: An automated system for computer-
aided pronunciation teaching. In: Proc. Eurospeech (1993) 
16. Kawai, G., Hirose, K.A.: CALL system using speech recognition to train the pronunciation 
of Japanese long vowels, the mora nasal and mora obstruents. In: Proc. Eurospeech (1997) 
17. Narusa, J.: Computer-aided spoken language training with enhanced visual and auditory 
feedback. In: Proc. Eurospeech, pp. 183–186 (1999) 

Supporting Engagement and Floor Control in
Hybrid Meetings
Rieks op den Akker, Dennis Hofs, Hendri Hondorp,
Harm op den Akker, Job Zwiers, and Anton Nijholt
Human Media Interaction Group
University of Twente
P.O. Box 217, 7500 AE Enschede, The Netherlands
infrieks@ewi.utwente.nl
Abstract. Remote participants in hybrid meetings often have problems
to follow what is going on in the (physical) meeting room they are con-
nected with. This paper describes a videoconferencing system for partic-
ipation in hybrid meetings. The system has been developed as a research
vehicle to see how technology based on automatic real-time recognition of
conversational behavior in meetings can be used to improve engagement
and ﬂoor control by remote participants. The system uses modules for
online speech recognition, real-time visual focus of attention as well as
a module that signals who is being addressed by the speaker. A built-in
keyword spotter allows an automatic meeting assistant to call the remote
participant’s attention when a topic of interest is raised, pointing at the
transcription of the fragment to help him catch-up.
1
Introduction
AMIDA is a research project funded by the EU on automatic analysis and recog-
nition of activities in meetings and the understanding of their outcomes. It is a
follow-up of the AMI and M4 projects that targeted the development of meeting
support technology, multi-modal meeting browsers, as well as automatic audio-,
and video-based summarization systems1. AMIDA is aiming at two new targets:
the development of real-time processing (for online speech recognition, online ges-
ture recognition, etc.), and the application of these real-time methods in cross-
modal conversational scene analysis for supporting meetings. Meeting support in-
cludes the (semi-) automatic retrieval of information needed for decision processes
in meetings as well as technology in the form of meeting assistants that support
remote participation in hybrid meetings, in which one or more participants are re-
mote and others are present in a shared room. AMI delivered a multi-modal, multi-
layered annotated corpus containing 100 hours of audio and video recordings of
face-to-face meetings [1],[2]. Besides natural small group meetings, the corpus con-
tains about 35 series of 4 meetings of design groups that consist of 4 people. These
1 (AMI(DA) stands for Augmented Multi-party Interaction (Distant Access) -
www.amiproject.org
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 276–290, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Supporting Engagement and Floor Control in Hybrid Meetings
277
are scenario-based meetings, the people play a role in a group that has the task to
design a new remote tv control [3]. The same scenario was used in the follow-up
project that collected a new corpus of meetings. In each of the series of 4 design
group meetings, now three include a remote participant. In two of them 3 people
are located in the same room, and one has a video and audio link with the room.
Speech is recorded by individual lapel microphones as well as by a microphone ar-
ray. Speech has been transcribed by hand. Words have been time-aligned. Words
of a speaker have been segmented into intentional units and hand-annotated with
dialogue act tags. A part of the corpus was annotated with visual information:
hand and head gestures, focus of attention (who or what are they looking at),
as well as with addressee information, telling whom the speaker was talking to
during a stretch of talk. Other content annotations include abstractive and ex-
tractive summaries, topic segmentation, named entities, and subjectivity [4]. The
annotations are organized using NXT in layers, all directly or indirectly referring
to the time line, a requirement for cross-modal analysis as well as for replaying
the meeting. (For NXT and NXT based tools see [5], [6]). The annotated corpora
have been used to develop models of multi-modal multi-party meeting behaviors
as well as for training and testing automatic recognizers of these behaviors or as-
pects thereof. They are also used for automatic generation of summaries, and for
generation of behaviors of synthetic (conversational) characters in virtual smart
meeting rooms [7].
This paper presents the ﬁrst version of a system that demonstrates how these
recognition and generation modules can be used to support remote meeting par-
ticipation. This User Engagement and Floor Control (UEFC) demo is meant
to show how AMIDA research can contribute to technology that makes remote
meetings more engaging by giving remote participants more control in discus-
sions and decision making processes. The UEFC demo is one system developed
in a general Meeting Recorder Framework that is being used as a research ve-
hicle for experimental studies of how outcomes and processes in remote meet-
ings depend on properties of communication channels and how engagement and
eﬃciency are aﬀected by meeting support technology. The Meeting Recorder
Framework contains a package for media streaming. Audio and video streams
can be produced real-time by devices (in on-line use in a life meeting) as well
as from ﬁles. This makes it possible to exploit the MRF for building systems
that play back recorded audio and video ﬁles and that use annotation layers of
ﬁled meetings in what we call “oﬀ-line” systems, as well as for building real life,
“on-line”, tele-meeting systems.
Basic constraints of tele-meeting systems are the capacities of the communi-
cation network and the capacities of the audio and video displays. They con-
strain how information can be presented on the “user’s interfaces” display and
what the aﬀordances of the interface can oﬀer the user/remote participant to
control the meeting. An earlier oﬀ-line system (based on play back of a hand-
annotated meeting) for remote meeting support developed within the AMIDA
project demonstrated how a mobile remote participant could be informed about
the meeting by means of a 2D or 3D visualisation of the meeting room [8].

278
R. op den Akker et al.
The UEFC system that we present in this paper is made to be used by a remote
participant that has a fast internet connection and a desktop computer screen.
The use case scenario is that of a participant that cannot or has chosen not to
attend the meeting continuously, who may have a special role in a project group
and who is interested in some agenda items more than in others and who will
either devote “continuous partial attention” to the meeting or is multi-tasking.
This scenario becomes more and more common practice in our meeting culture,
where also people that meet physically in the same room are often distributing
their attention between reading their mails and participating in local meeting
activities. Moreover, people sometimes choose to attend a meeting staying in
their own oﬃce because it makes it easier to selectively attend only parts of the
meeting, even if travel time can be neglected.
2
Mediated Communication Problems
Despite its frequency of use, communication in audio and video conferencing
systems still suﬀers from a number of limitations that have been shown to im-
pact the engagement in and the eﬀectiveness of remote meetings. Among these
are noisy lines or background noise, diﬃculties hearing the speaker, not knowing
who is speaking, and a lack of the experience of “social presence”. In face-to-face
meetings the audio and visual “channel” are both used in turn-taking, control-
ling the conversational ﬂow, or ﬂoor control. Floor denotes a cognitively shared
attentive space that mediates in the sequential or simultaneous organization of
participants’ contributions: turn-taking as well as topic management (cf. Edel-
sky [9] and Hayashi [10]). There can be several ﬂoors at the same time as we see
in cocktail parties, and in larger group meetings, [11]. Although a ﬂoor mostly
contains one main speaker at a time, verbal and non-verbal listener feedbacks
(laughters, head nods) that occur in the back channel control turn-taking and
are important for keeping the pace in the conversation. The physical and psy-
chological barriers that exist in hybrid meetings make that it is often hard for
remote partners to attend to a selected ﬂoor of which the participants share the
same room. It makes it harder for them to take turn, and to initiate a new topic
and a new ﬂoor.
Ideas about how technology could prevent speciﬁc problems in computer me-
diated communication no doubt rest on a theory of communication. From such
a theory we may expect that it explains (or at least makes believable) the ef-
fectiveness of introducing technological means. Some theories of communication
are information-theoretic or cognitive. The most popular is the sender-message-
receiver model (see for example [12]). Other theories of communication empha-
size the social dimension. Politeness, dominance, ﬁght for the ﬂoor, but also
commitment, empathy, social presence and social signals are key elements of
these theories. Central is the obligation that co-partners have towards the oth-
ers to be clear, and to follow and provide feedback (see [13]). On the level of
conversation the roles change with the obligation: those between speaker and se-
lected addressees diﬀer from those between speaker and overhearers. Monk et al.

Supporting Engagement and Floor Control in Hybrid Meetings
279
[14] emphasize that on the more global task level “peripheral participants”, who
are monitoring communicative behavior not explicitly directed at themselves,
need to be considered when designing equipment for video mediated coopera-
tive work. A complete theory of human communication will encompass both,
the information- and cognitive- processing aspects as well as the social aspects,
as well as their relations. These relations concern the various ways that sender,
receiver and message are related and involves notions as authenticity, trust, and
credibility. Notions that are important when considering group decision making
as well as making persuasive technology [15].
In [16] the authors give a list of problems that people experience with com-
munication in hybrid meetings (i.e. meetings where some people are local and
some are remote).
– Audio problems:
• Poor quality speakerphones
• Too much background noise
• Multiple speakers speaking at the same time can be diﬃcult to under-
stand
• People speaking too far from microphones
– Remote attendee problems:
• Inability to conduct side conversations.
• In-room attendees forget about remote people
• Challenging to break into lively conversation
• Diﬃcult to detect in-room speaker changes
• Hard to identify people currently in the meeting room
• Hard to identify the current speaker
• Diﬃcult to participate in brain-storming sessions
• Cannot see in-room demonstrations or artifacts
– Meeting room problems:
• Local people are more emotionally salient than remote participants.
• Easy to forget about remote participants
• Often local people do not know who is still connected
Key point is that subjects in mediated communication suﬀer from increased
uncertainty, caused by the spatial and temporal distance between co-participants.
Uncertainties are expressed in questions as “Did he receive my message?” or “Is
what I made of this message what my partner meant?” Speakers and listeners
work parallel in cooperation. While speaking a speaker will be interested in how
the message is received and the receiver will send signals if and how he received the
message. So they work on shared beliefs in a “grounding” process. What changes
with the physical circumstances and the channel properties is the sample time
and the granularity of the information units, the packages that are send back-
and-forth. Units and sample times are not determined in advance by grammatical
rules, they are “interactively determined” by the ﬂoor participants. ([17], p.726-
727.) When people get used to audio delay they adapt sample time and package
size, and explicit verbal control messages become more frequent.

280
R. op den Akker et al.
Our analysis of the remote meeting corpus are in line with earlier ﬁndings:
in remote meetings participants refrain from using verbal backchannel signals,
which makes that speakers often experience that they talk in a void. Visual con-
tact may help here. See also [18] for similar conclusions. Addressing in remote
meetings is more explicit, since speakers are often not convinced that the remote
partner is paying attention. The visual channel is important when people discuss
objects, or documents. Moreover, it helps to identify who is speaking and to sig-
nal focus of attention of the speaker, which helps understanding verbal referring
expressions. The above ﬁndings have been leading for the development of the
UEFC demonstrator for the scenario of use we discussed earlier.
3
The User Engagement and Floor Control Demo
Figure 1 shows the HMI meeting room and a remote meeting participant using
the system described in this paper. This section describes the modules of the
UEFC demo, and the graphical user interface that presents the view on the
co-participants and the remote meeting room. The remote participant (RP) is
present in the meeting room on a large screen. A smaller screen shows the view
that the RP has of the meeting room, so that both sides can always see what
the other side sees.
(a)
(b)
Fig. 1. A remote meeting in which the UEFC system is being used
The modules that receive media input streams, send their respective outputs
to a central database application known as The Hub, which sends it through
to the modules that rely on the data. Figure 2 shows the dependencies of all
modules between each other. The Media Streamer records all video and audio
from the local and remote participants; the ASR, VFOA and KWS modules (see
below) process video or audio data, which is used by the Dialogue Act Recognizer
and the Addressing Module. The Media Streamer is a video conference tool
developed at the University of Twente. It runs on all participant’s computers
and that of the meeting room itself. It reads out the data from the webcam

Supporting Engagement and Floor Control in Hybrid Meetings
281
Fig. 2. Dependencies between the modules of the UEFC demonstrator
and microphone, and can stream the data to another PC for further processing.
It also takes care of compressing the video stream. The Hub serves as a central
point of communication for diﬀerent software modules developed within AMIDA.
Modules can subscribe to the Hub as a producer or consumer (or both) of speciﬁc
types of data. In our UEFC example, the Visual Focus of Attention Module
produces “focus” data, which is consumed by the Addressing module, which in
its turn produces “addressing” data. The Hub makes sure that every module is
aware of new data arriving from other modules. The sections below will explain
the details of the other modules in the system.
3.1
Automatic Speech Recognition
The ASR system receives the incoming audio streams from all participants on
diﬀerent sockets, which allows the system to be split between Windows and Linux
systems easily. A Java wrapper allows the results of recognition to be streamed
via the Java middleware to the Hub. From the Hub, metadata is available to all
other consumers. Thus the demonstrator runs on several computers. Audio is
captured on one machine in real time from a single microphone or a microphone
array and on-line beamformer. For every audio stream it generates the words
that are being spoken. It does this in spurts; there needs to be a short silence
before the system starts to process the stream. The word data that is send
to the Hub, includes start- and end time information, and from which of the
participants it came. The system that is used within the UEFC Demonstrator
is the webASR system from the University of Sheﬃeld. For the details on this
system please refer to [19]2.
2 webASR is located at the following website: http://webasr.dcs.shef.ac.uk/.

282
R. op den Akker et al.
3.2
Dialogue Act Recognition
The Dialogue Act Recognition module segments the words from the ASR module
into Dialogue Act segments and classiﬁes them with a Dialogue Act Tag from
the AMI tag set. At the time of writing the segmentation is done using so-called
spurts, meaning that a segment boundary is inserted whenever there is a pause
of a certain size between two words. In the future, the segmentation algorithm
described in [20] will be used. The Dialogue Act Classiﬁcation, or tagging, is
done using the system described in [21].
3.3
On-Line Keyword Spotting
The Keyword Spotting module analyses the audio input stream for the occur-
rence of certain keywords. The acoustic keyword-spotter is based on an esti-
mation of phone posterior probabilities by neural networks and on the classical
tandem of target word model and background model, (cf. [22]). It can be given a
list of keywords, that can be modiﬁed on the ﬂy, for which it will look. Whenever
it detects one of the keywords, it sends a signal to the Hub, indicating the word
and the time in the audio stream at which it recognized it. The module can han-
dle a list of up to 100 words, and is, for these words, much more reliable than
the standard ASR system. The keywords for which spotter looks are inserted
by the Remote Participant, so that he can be warned whenever a topic of his
interest is being discussed.
3.4
Visual Focus of Attention Recognition
Visual focus of attention (VFOA) of participants provides important cues to rec-
ognize interactions in meetings. But, recognizing the VFOA directly is a diﬃcult
task. The main cue to recognize VFOA is the head pose. The Visual Focus of At-
tention module analyses the video streams of each individual meeting participant.
It tracks the pose of the head in terms of tilt (vertical movement) and pan (hori-
zontal movement) and maps these values to predeﬁned targets. The system then
sends for every 2 frames of video data (e.g. 15 times per second) the best matching
target to the Hub. In the current setup we are only interested in who is looking at
the Remote Participant’s screen, so there are two targets: remote participant and
other. The system that is used is based on work in [23]. In the UEFC demo the
main consumer of the VFOA data is the Addressee Detection Module.
3.5
Addressee Detection
The addressee detection module (ADR) that is used in the UEFC system iden-
tiﬁes the addressee of the speaker. In particular ADR will tell if the remote
participant is being addressed. This information is used to call the RP’s atten-
tion when he is not actively participating in the conversation. See Figure 2 for
the input the ADR module depends on.
The addressee classiﬁer is trained on statistical models of patterns of address-
ing behavior in the annotated AMI and Amida meeting corpora. Jovanovi´c build

Supporting Engagement and Floor Control in Hybrid Meetings
283
a classiﬁer for addressee prediction trained on the AMI corpus using Dynamic
Bayesian Network technology: for predicting the addressee of the current dialogue
act it uses the information about the addressee of previous dialogue acts. The
method assumes that four participants meet face-to-face and sit at ﬁxed positions
at a table. For use in the remote setting with a variable number of local partic-
ipants, who may move around, a more general addressee detection module was
made. The idea is that each participant in the meeting has a dedicated addressee
predictor, a binary classiﬁer that tells whether the participant is addressed by the
speaker or not. The features used are of the following three types.
1. lexical features, in particular the use of personal pronouns, the number of
words of the dialogue act,
2. contextual features, how active was the participant in the last turns and how
often was he addressed in the last turns?
3. visual focus of attention of the speaker and other participants, in particular
do they gaze at the participant?
A supervised classiﬁer trained and tested on the hand-annotated AMI corpus
has an accuracy of 92,53%. The baseline for this classiﬁcation task is 89.2%,
the percentage of all dialogue acts in the test set that are not addressed to one
single distinguished participant (group addressed dialogue acts counts as no),
so a classiﬁer that labels every dialogue acts as such will receive that score. For
more details about the addressee classiﬁer we refer to [24].
3.6
The Graphical User Interface
The GUI of the UEFC demo shows close up view of each of the local partners
and an overview of the meeting room. The positions of the video frames reﬂect
positions at the meeting room table. The central overview frame has a border that
changes color when the user is called, or addressed by a speaker in the meeting
room. An audible signal will catch his attention. The user can indicate his state
of attendance. He can add or remove keywords to and from the keyword list. The
keywords are send to the Hub which forwards them to the keyword spotter. If
the RP is not actively attending the discussion a visual and audible signal will be
generated that a keyword of interest has been detected. The keyword is highlighted
in the scrollable frame that contains the online produced transcript of the meeting.
This allows the RP to easily catch-up with the ongoing meeting. The GUI also
indicates whether there are problems with the audio or video channel.
4
The Meeting Recorder Framework
The UEFC demonstrator is an application that uses a software architecture and
framework that we developed for experimenting with remote meetings, one-to-
one or hybrid. But it can also be used for oﬀ-line applications and for building
software agents, and virtual characters. There are three kinds of client appli-
cations: one for the remote participant, one for the meeting room, the location

284
R. op den Akker et al.
where the overview of the meeting room is recorded (and the remote participant
is presented), and ﬁnally there is an application for each local participant.
The user interface is actually implemented in an integrated application, named
meeting recorder, which can also be run stand-alone.
4.1
Meeting Recorder
A meeting recorder is a video conferencing application that can record all video
and audio to ﬁles, and supports the controller marks on the Hub for a synchro-
nized start of all sites. The application can be customized in various ways using
an XML conﬁguration ﬁle.
First of all, the conﬁguration ﬁle describes exactly what streams should be
broadcast and what streams should be received. It can broadcast from ﬁles or
from capture devices. For capture broadcasts, one can specify the media format
and the recorded media can be saved to an AVI or WAV ﬁle. Video streams
can be compressed with DivX or stored as uncompressed RGB video. For each
broadcasts stream, you can add custom stream receivers, which are simple Java
classes. This is used for example to send an audio stream to the ASR. Such
classes can also be added for received streams.
In the choice of the media formats in the UEFC demo, we had to consider lim-
ited network bandwidth (high bandwidth but still limited) and the high quality
demands of signal processing modules. The ASR and keyword spotter require
16 kHz, 16 bits, uncompressed PCM audio. It appeared no problem to transmit
this audio format over the available network, although in the future we may
want to compress the audio before transmitting it. The video is another matter.
The VFOA requires 15 Hz, 320x240, uncompressed RGB video. This cannot be
transmitted over the network and we chose DivX for the compression of trans-
mitted video streams. DivX oﬀers good image quality while the compression is
good enough for high bandwidth networking.
The user interface is basically a desktop containing titled frames that can
be moved and resized like normal windows. Each broadcast and received video
stream has its own frame. With plugin code the frames could be automatically
positioned and sized, while any additional frames (not just video frames) can be
added as well. An example of the user interface is shown in Figure 3.
The media streaming is done using a separate software package.
4.2
Media Streaming
An open package for low-latency high-bandwidth video and audio streaming has
been developed. It uses DirectShow and has basic interfaces in C++ and Java.
The Java interface uses JNI to access the C++ interface. The actual network
streaming and other functionalities are implemented in Java only on top of the
base framework. The base framework stays close to DirectShow concepts with
some simpliﬁcations. DirectShow leans on Microsoft’s Component Object Model
(COM). DirectShow has no built-in support to obtain the media data outside
COM for custom processing or network transmission. For those purposes, it is
necessary to build a DirectShow ﬁlter (COM object), which needs to be installed

Supporting Engagement and Floor Control in Hybrid Meetings
285
Fig. 3. Meeting recorder
in Windows. This is rather complicated and it can be very diﬃcult to integrate
existing software. This media framework hides COM, which allows for more
simple interfaces.
5
Does It Work?
The UFCD system shows the working of the various technical modules developed
in the AMIDA project, such as the real-time head pose tracker that informs the
system about the visual focus of attention and the addressee detection module
that decides whether the remote participant is being addressed. Moreover, the
media streaming software allows for synchronized audio and video streaming
over the internet with an acceptable delay of less than 1 sec. People that have
worked with the system appreciated that you can see the faces of the individuals
and also have an overview of the meeting room. The overview makes it easier
to follow discussions performed in the remote meeting location, whereas the
individual videos improve one-to-one contact in a dialogue between the remote
participant and someone located in the meeting room. The participants in the
meeting room found it hard to establish mutual gaze in a one-to-one interaction
with a remote participant. This is caused by parallax, the distance between the
positions of the cameras and the screen position where the faces are shown.
The main question, however, concerning evaluation is if the system can be
used so that it indeed improves the engagement of remote participants in hybrid
meetings and their control over the ﬂoor. To answer these questions requires more

286
R. op den Akker et al.
ﬁeld tests in realistic settings where participants do have personal interests in the
outcomes of a meeting. Another issue is whether people will use a teleconference
system more when it supports them in distributing their attention over diﬀerent
tasks by automatic attention call and the possibility to catch-up. Do these tech-
niques improve eﬃciency of meetings? More engagement doesn’t always imply
improved eﬃciency in terms of task performance and vice versa. In an interesting
study about the eﬀect of various forms of interfaces in a video mediated coopera-
tive task with a shared tabletop display Hauber et al. [25] used a variety of social
and performance measures to see how the interface design aﬀects social presence
and eﬃciency. In this study, the face-to-face condition is included as a gold-
standard control. They conclude that “there were important diﬀerences between
the 2D and 3D interfaces. In particular, the 3D interface positively inﬂuenced
social- and co-presence measures in comparison to 2D, but the task measures
favored the two-dimensional interfaces.” The important lesson is that we need
to be careful in deﬁning what we are aiming at: improve engagement and social
interaction or eﬃciency of task completion. Moreover, the impact that meeting
support technology has on engagement and eﬀectiveness of task completion will
depend on the type of task that the group has to perform. McGraths group task
classiﬁcation system separates tasks into the following four quadrants: (a) gen-
erating, (b) choosing, (c) negotiating, and (d) executing. The quadrants and the
tasks they contain are related to one another within a two-dimensional space.
One dimension reﬂects the degree to which the task entails cognitive (choosing)
versus behavioral performance requirements (executing). The other dimension
reﬂects the degree and form of interdependence among group members (gener-
ating versus negotiating). It has been found that a “communication medium is
more likely to aﬀect group outcomes when there is a need for the expression
and perception of emotions, when tasks require coordination and timing among
members activities, when one is attempting to persuade others, or when tasks
require consensus on issues that are aﬀected by attitudes or values of the group
members.” (citation from [26]). The intellective and decision making tasks and
the cognitive conﬂict and mixed-motive tasks thus seem to require a greater
amount of coordination and group member interaction than the other tasks. If
this holds, it is a real challenge to apply meeting support technology for use
in decision making tasks and cognitive conﬂict tasks and to design experiments
to measure the impact of these technologies on engagement and eﬀectiveness of
remote meetings.
6
Conclusion and Future Work
The latest results in automatic conversational scene analysis in face-to-face meet-
ings - see [27] for an up-to-date overview - make it possible to build systems that
select automatically the best camera view for meeting observers. These could
be remote meeting participants in a teleconference situation or users of meeting
archive systems ([28], [29], [30]). Because of the real-time constraint the most
challenging is the use of these technologies by remote participants in an ongo-
ing meeting. What is the best view presented to the remote participant and

Supporting Engagement and Floor Control in Hybrid Meetings
287
how does user control over meeting support technology aﬀect the cognitive load
and distract the attention from the real issues so that it hampers engagement
and participation? Meeting assistants enter smart meeting rooms in the form
of software agents that aim to assist the meeting process and to facilitate more
eﬀective and eﬃcient meetings. An investigation by Rienks et al. discusses how
“pro-active meeting assistants” could be exploited [31]. As soon as these new
technologies get into use they change meeting practices. Feedback to the group
during meetings about the talkativity of participants has shown to aﬀect so-
cial behavior [31], people change their language and use more explicit signals
if they believe that helps the technology to identify it. Ehlen et al. [32] give
an example of the latter, where participants started to mention “action items”
explicitly in meetings, after they found out that the meeting browser they used
and that identiﬁed these items automatically in previous meetings listed them
under this name. In remote meetings people use more explicit procedures to
make clear whom they want to address, so that it becomes easier for outside ob-
servers, including meeting assistants, to identify these signals. As soon as meeting
technology, based on studying regularities in human conversational behavior in
meeting practice, becomes a part of that practice (“technology in the loop”),
people adapt their behavior in sometimes unforseen ways.
We presented the Meeting Recorder Framework, a general framework for me-
dia streaming and for building oﬀ-line and on-line remote meeting support tools.
An application of the MRF is the User Engagement and Floor Control demo for
demonstrating technology for conversational scene identiﬁcation that has been
developed in the AMIDA project, in a conﬁguration that is meant to support
engagement of a “multi-tasking” remote participant in a hybrid meeting. The
framework is currently used to study the eﬀect of audio delay on the one-to-one
audio only interaction where subjects have to negotiate about the best meeting
date. Another aim of this study is to see if visual feedback about the transfer
rate of the audio signal aﬀects the time that it costs subjects to adjust their in-
teractive behavior to the audio delay. Future work will consist of the evaluation
of the remote meeting support technology in decision and negotiation tasks as
we discussed in the last section.
One of the outcomes of a market assessment investigation that was carried out
in the AMIDA project says that potential users would welcome a remote meeting
assistant that helps the user to catch-up with a meeting when they arrive late, or
miss a part of it for other reasons. Users also like to see that the system provides
links to relevant documents or other information resources related to the issues
being discussed in the meeting. These functions have been implemented in the
Automatic Content Linking Device (ACLD), a real-time query-free document
retrieval system [33]. The ACLD is a system that constantly retrieves items from
a document repository, which includes meeting related documents together with
excerpts from previous meetings of the group, and displays them to a participant
or to all of them; the device can be used online, during a meeting, or oﬀ-line,
integrated in a meeting browser. The idea is to see how functionalities of the
ACLD could be usefully integrated with the UEFC system. Further research

288
R. op den Akker et al.
should reveal where engagement and participation in meetings is still improved
by information and communication technology and where the technology stands
in the way instead of supporting interaction.
Acknowledgments
We are gratefull to the anonymous reviewers for their comments on an earlier
version of this paper. We acknowledge our AMIDA partners from IDIAP in
Martigny, DFKI in Saarbr¨ucken and the Universities of Brno, Sheﬃeld, and
Edinburgh for their contributions to the UEFC demonstrator. The work reported
in this paper is sponsored by the European IST Programme Project FP6-0033812
(AMIDA). This paper only reﬂects the authors views and funding agencies are
not liable for any use that may be made of the information contained herein.
References
1. McCowan, I., Carletta, J., Kraaij, W., Ashby, S., Bourban, S., Flynn, M., Guille-
mot, M., Hain, T., Kadlec, J., Karaiskos, V., Kronenthal, M., Lathoud, G., Lincoln,
M., Lisowska, A., Post, W., Reidsma, D., Wellner, P.: The AMI Meeting Corpus.
In: Proceedings of Measuring Behavior 2005, the 5th International Conference on
Methods and Techniques in Behavioral Research, Wageningen, Netherlands (2005)
2. Carletta, J.C.: Unleashing the killer corpus: experiences in creating the multi-
everything AMI meeting corpus. Resources and Evaluation 41(2), 181–190 (2007)
3. Post, W., Cremers, A., Henkemans, O.: A research environment for meeting be-
havior. In: Nijholt, A., Nishida, T., Fruchter, R., Rosenberg, D. (eds.) Social Intel-
ligence Design, Enschede, The Netherlands (2004)
4. Wilson, T.: Annotating subjective content in meetings. In: Proceedings of LREC
(2008)
5. Carletta, J., Evert, S., Heid, U., Kilgour, J., Robertson, J., Voormann, H.: The nite
xml toolkit: Flexible annotation for multi-modal language data. Behavior Research
Methods, Instruments, and Computers 35(3), 353–363 (2003)
6. Reidsma, D., Hofs, D., Jovanovic, N.: A presentation of a set of new annotation
tools based on the nxt api. In: Proceedings of Measuring Behavior, Wageningen,
The Netherlands (2005)
7. Nijholt, A., Rienks, R., Zwiers, J., Reidsma, D.: Online and oﬀ-line visualization
of meeting information and meeting support. Visual Comput. 22, 965–976 (2006)
8. Matena, L., Jaimes, A., Popescu-Belis, A.: Graphical representation of meetings on
mobile devices. In: MobileHCI 2008 Demonstrations (10th International Conference
on Human-Computer Interaction with Mobile Devices and Services), Amsterdam
(2008)
9. Edelsky, C.: Who’s got the ﬂoor? Language in Society 10, 383–421 (1981)
10. Hayashi, R.: Floor structure of english and japanese conversation. Journal of Prag-
matics 16, 1–30 (1991)
11. Aoki, P., Romaine, M., Szymanski, M., Thornton, J., Wilson, D., Woodruﬀ, A.:
The mad hatter’s cocktail party: A social mobile audio space supporting multiple
conversations. In: Proc. ACM SIGCHI Conf. on Human Factors in Computing
Systems (CHI 2003), pp. 425–432 (2003)

Supporting Engagement and Floor Control in Hybrid Meetings
289
12. Bettinghaus, E.P., Cody, M.J.: Persuasive Communication. In: Wadsworth Thom-
son Learning, 5th edn. (1994)
13. Clark, H.H., Schaefer, F.E.: Dealing with overhearers. In: Arenas of language use.
University of Chicago Press, Chicago (1992)
14. Monk, A., Watts, L.: Peripheral participation in video-mediated communication.
Int. J. Human-Computer Studies 52, 933–958 (2000)
15. Fogg, B., Tseng, H.: The elements of computer credibility. In: Proceeding of CHI
1999, pp. 80–87 (1999)
16. Yankelovich, N., Kaplan, J., Simpson, N., Provino, J.: Porta-person: telepresence
for the connected meeting room. In: Proceedings of CHI 2007, pp. 2789–2794 (2007)
17. Sacks, H., Schegloﬀ, E., Jeﬀerson, G.: A simplest systematics for the organization
of turn-taking for conversation. Language 50, 696–735 (1974)
18. Donath, J.S.: Mediated faces. In: Beynon, M., Nehaniv, C.L., Dautenhahn, K.
(eds.) CT 2001. LNCS, vol. 2117, pp. 373–390. Springer, Heidelberg (2001)
19. Hain, T., El Hannani, A., Wrigley, S.N., Wan, V.: Automatic speech recognition
for scientiﬁc purposes - webasr. In: Proceedings of the international conference on
spoken language processing (Interspeech 2008) (2008)
20. Op den Akker, H., Schulz, C.: Exploring features and classiﬁers for dialogue act
segmentation. In: Popescu-Belis, A., Stiefelhagen, R. (eds.) MLMI 2008. LNCS,
vol. 5237, pp. 196–207. Springer, Heidelberg (2008)
21. Germesin, S., Becker, T., Poller, P.: Determining latency for on-line dialog act
classiﬁcation. In: Poster Session for the 5th International Workshop on Machine
Learning for Multimodal Interaction, vol. 5237 (2008)
22. Sz¨oke, I., Schwarz, P., Burget, L., Fapˇso, M., Karaﬁ´at, M., ˇCernock´y, J., Matˇejka,
P.: Comparison of keyword spotting approaches for informal continuous speech. In:
Interspeech 2005 - Eurospeech - 9th European Conference on Speech Communica-
tion and Technology, pp. 633–636 (2005)
23. Ba, S., Odobez, J.M.: Recognizing human visual focus of attention from head pose
in meetings. IEEE Transaction on Systems, Man, and Cybernetics, Part B (Trans.
SMC-B) 39, 16–33 (2009)
24. Op den Akker, H.: On addressee prediction for remote hybrid meeting settings.
Master’s thesis, University of Twente (2009)
25. Hauber, J., Regenbrecht, H., Billinghurst, M., Cockburn, A.: Spatiality in video-
conferencing: Trade-oﬀs between eﬃciency and social presence. In: Proceedings
ACM Conference on computer-supported cooperative work, CSCW 2006 (Novem-
ber 2004)
26. Baltes, B.B., Dickson, M.W., Sherman, M.P., Bauer, C.C., LaGanke, J.S.:
Computer-mediated communication and group decision making: A meta-analysis.
Organizational Behavior and Human Decision Processes 87(1), 156–179 (2002)
27. Popescu-Belis, A., Stiefelhagen, R. (eds.): MLMI 2008. LNCS, vol. 5237. Springer,
Heidelberg (2008)
28. Takemae, Y., Otsuka, K., Yamato, J., Ozawa, S.: The subjective evaluation ex-
periments on an automatic video editing system using vision-based head tracking
for multiparty conversations. IEEJ Transactions on Electronics, Information and
Systems 126(4), 435–442 (2006)
29. Takemae, Y., Otsuka, K., Yamato, J.: Eﬀects of automatic video editing system
using stereo-based head tracking for archiving meetings. In: IEEE International
Conference on Multimedia and Expo., ICME 2005, July 2005, pp. 185–188 (2005)

290
R. op den Akker et al.
30. Otsuka, K., Araki, S., Ishizuka, K., Fujimoto, M., Heinrich, M., Yamato, J.: A
realtime multimodal system for analyzing group meetings by combining face pose
tracking and speaker diarization. In: Proceedings of ICMI 2008 - International
Conference on Multimodal Interfaces (2008)
31. Rienks, R., Nijholt, A., Barthelmess, P.: Pro-active meeting assistants: Attention
please? In: Proceedings of the 5th workshop on Social Intelligence Design (2006)
32. Ehlen, P., Fernandez, R., Frampton, M.: Designing and evaluating meeting assis-
tants, keeping humans in mind. In: Popescu-Belis, A., Stiefelhagen, R. (eds.) MLMI
2008. LNCS, vol. 5237, pp. 309–314. Springer, Heidelberg (2008)
33. Popescu-Belis, A., Boertjes, E., Kilgour, J., Poller, P., Castronovo, S., Wilson, T.,
Jaimes, A., Carletta, J.: The amida content linking device: Just-in-time document
retrieval in meetings. In: Popescu-Belis, A., Stiefelhagen, R. (eds.) MLMI 2008.
LNCS, vol. 5237, pp. 272–283. Springer, Heidelberg (2008)

Behavioral Consistency Extraction for Face
Veriﬁcation
Hui Fang and Nicholas Costen
Manchester Metropolitan University
Department of Computing and Mathematics,
Manchester, U.K.
{H.Fang,N.Costen}@mmu.ac.uk
Abstract. In this paper we investigate how the use of computational
statistical models, derived from moving images, can take part in the face
recognition process. As a counterpart to psychological experimental re-
sults showing a signiﬁcant beneﬁcial eﬀect of facial non-rigid movement,
two features obtained from face sequences, the central tendency and type
of movement variation, are associated to improve face veriﬁcation com-
pared with single static images. By using General Group-wise Registra-
tion algorithm, the correspondences across the sequences are captured
to build a combined shape and appearance model, parameterizing the
face sequences. The parameters are projected to an identity-only space
to ﬁnd the central tendency of each subject. In addition, facial movement
consistencies across diﬀerent behaviors exhibited by the same subjects
are recorded. These two features are fused by a conﬁdence-based de-
cision system for authentication applications. Using the BANCA video
database, the results show that the extra information extracted from
moving images signiﬁcantly and eﬃciently improves performance.
1
Introduction
In recent decades, a great deal of attention has been paid to modeling static face
recognition. A large number of algorithms, including Principle Components Anal-
ysis (PCA) [1], Elastic Bunch Graphic Matching [2], Independent Component
Analysis [3], kernel-based algorithms [4], Linear Discriminant Analysis (LDA) [5]
and Active Appearance Model (AAM) [6] have been developed in an attempt to
model the recognition process. Although face identiﬁcation systems based on them
have achieved high level of performance in standardized face recognition or ver-
iﬁcation tests such as FERET [7], FRVT [8] and BANCA [9], and the combined
processing of a number of images of the same person is known to very signiﬁcantly
reduce recognition errors [10], it is still expected that performance will be im-
proved by the use of video sequences rather than from still images.
At the same time, psychological evidence has been accumulating, which in-
dicates that the dynamic information available from moving faces also makes a
signiﬁcant contribution to face recognition. There are three major, non-exclusive
hypotheses about the role of facial movements in face recognition [11]. The ﬁrst
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 291–305, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

292
H. Fang and N. Costen
depends on the observation that the 3D structure of the face can be recovered
via the Structure-from-Motion (SfM) algorithm. The second suggests that robust
conﬁrmation by multiple frames is possible in poor viewing conditions (together
these are referred to as “structural enhancement”) and the third uses “charac-
teristic motion patterns” to distinguish individual faces. Based on these psy-
chological ﬁndings, it is interesting to see if computer vision can exploit similar
additional information from moving images to help in modeling computational
recognition and improve recognition performance.
The application of the Structure-from-Motion algorithm [12] to face mod-
eling is an ill-posed problem, although a number of non-rigid SfM algorithms
[13, 14] have been proposed. SfM assumes the object undergoes relatively large
rigid movements compared with small deformations. However, this is implausible
with faces, as large rotations induce self-occlusions. This means the correspon-
dences between parts cannot be tracked consistently, breaking the fundamental
assumptions of the algorithm. In addition, the non-rigid movement of the face
(as when the subject speaks or shows emotions) further degrades the 3D struc-
ture recovered. One feasible solution [15] is to build 3D face models by acquiring
data from 3D scanner devices and parameterizing the face instance by reducing
the projection error to the sequence. However, this would be highly dependent
on the 3D data acquisition process.
In this paper, we investigate the other two factors mentioned in the psycho-
logical experiments. A generic combined 2D Appearance Model (AM) [6] is built
from the ensemble (the set of faces used to provide the system with generic fa-
cial knowledge) using correspondences obtained from the General Group-wise
Registration algorithm (GGR). The use of the AM is known to improve facial
representation and recognition relative to systems which do not explicitly model
face shape [16]. All the facial instances are parameterized on this model and the
consistency of the parameters is explored, projecting onto an identity sub-space
(obtained by LDA performed on the ensemble), using sequences of the same and
diﬀerent subjects. This yields representative measures of the central tendencies
for identity parameters. At the same time, the common characteristic motions
of the same subjects are extracted by ﬁnding a maximally correlated sub-set of
the eigenvectors derived from the parameters of a single sequence. Finally, the
features extracted from moving faces are fused by a reliability-based framework
to verify the true clients. Through the experiments, a conclusion in accordance
with psychological work by Lander et al. [17] is obtained, which shows that the
beneﬁts of moving faces are mainly from non-rigid movements.
The originalities of our contributions can be highlighted as: (1) General Group-
wise Registration (GGR) algorithm to extend combined appearance model from
modeling static image to moving images; (2) a new way to represent the type of
moving variations of the same subjects; (3) better recognition from moving images
which is accordant with the corresponding psychological experimental results by
fusing the two types of moving information.
This paper is structured as follows. In the next section the relevant work is
introduced and some diﬀerences between them and the proposed algorithm are

Behavioral Consistency Extraction for Face Veriﬁcation
293
mentioned. In section 3, the method used is presented in detail. Experimental
results are shown in section 4. Finally in section 5 conclusions are drawn and
future work is introduced.
2
Relevant Work
Normally a tracking and recognition scheme is used for face recognition in video
sequence. Zhou et al. [18] investigate a probabilistic approach to video-based
recognition by using a particle ﬁlter. In principle, this algorithm assumes that
the identity of a person in a sequence is constant and searches for the maximum
probability solution. Although this is similar to the central tendency concept in
our proposed algorithm, the motion patterns were not explored in this paper.
A similar system was proposed by Benedikt et al [19], using high-speed 3D
videos of short sequences of speaking. After parameterization by Principal Com-
ponent Analysis, maximum discrimination between individuals was obtained us-
ing Dynamic Time Warping; this was signiﬁcantly better than a Hidden Markov
Model (HMM), perhaps because of the relatively small amount of data available.
It should be noted that in this case, all of the sequences were restricted to the
same behaviour (saying the word “Puppy”).
Liu and Chen [20] present a HMM based algorithm which ﬁnds the highest
score of the state transition probability from the facial motion in the sequence.
They project the moving images into a PCA subspace and deﬁne states from
which to train the HMM. Although the transitions between these states represent
the motion patterns of the subjects to some extent, it is quite diﬃcult to deﬁne
the discrete states and quantize the parameters of the images.
Combined shape and texture model has been used for analyzing the face mo-
tion by AAM. Bettinger et al. [21] present a generative model of facial behavior.
All of the faces sequences are registered by AAM and are projected into a com-
bined shape and texture model. Fragments of sequences with the same behavior,
such as shaking the head or smiling, are grouped, followed by modeling using
a variable length Markov model to ﬁnd the temporal relationships. This work
assumes that same facial behaviors of diﬀerent subjects share the same char-
acteristics while we are seeking consistencies across diﬀerent behaviors, such as
speaking diﬀerent sentences, exhibited by the same subjects. In addition, as with
Liu and Chen [20], very considerable sequence lengths are required to allow ef-
fective training of the Markov models; these are not available here. We also ﬁnd
that the GGR algorithm is more suitable for solving the correspondence problem
for registration tasks where a large diverse data set is used.
Edwards et al. [22] propose an online scheme of learning a linear relationship
between the parameters in an identity space and its non-identity space, em-
bedded within a shape and texture model space. These relationships are used to
correct the identity parameters, improving the recognition from video sequences.
The framework of our proposed algorithm is similar; however we ﬁnd the mo-
tion patterns of subjects directly in the non-identity space and fuse the features
in identity space with it for recognition instead of looking for the relationship

294
H. Fang and N. Costen
between them. Our scheme is more suitable to ﬁnd similarities of the patterns
when presented with diﬀerent behaviors in relatively short sequences and gives
a much clearer perspective to connect with psychological experiments.
Yamaguchi et al. [23] report a system similar to that proposed here, although
using an eigenface encoding. Relatively long sequences, where the predominate
form of variation involves facial orientation, are encoded and the similarity of their
distributions are compared using a Mutual Subspace Method (MSM), measuring
the relative angle at which they lie within a larger space. This was extended by
Arandjelovi´c and Cipolla [24] and notably high recognition performance was ob-
served when mean face encoding was also included, but this depended upon the
use of a procedure which compensates for diﬀerent proportions of facial orienta-
tions in the sequences by physical modeling these changes. It is unclear whether
this latter process could be applied to sequences where the major variations are
plastic facial changes (typically expressive or speech deformations).
3
Proposed Algorithm
In the proposed framework, the two factors mentioned in the psychological exper-
iments, structural enhancement and characteristic motion patterns are simulated
in computer vision. The two correspondences, named as identity central tendency
and principle component based motion patterns are fused by a conﬁdence-based
system to synthesize the improvement in human face perception when presented
with video sequences rather than static images. The overall paradigm is shown
in Figure 1. It should however be noted that there are potentially other eﬀects
dependent upon the order in which sequence frames are presented [25], which
will not be simulated in this study.
3.1
General Group-Wise Registration
Pair-wise (template-based) registration is a traditional technique for ﬁnding the
correspondences in a group of images. However, it is unlikely that the reference
image represents the whole set of images best; this makes the corresponded struc-
tures unreliable. A group-wise registration scheme may improve performance by
expanding the set of references.
GGR is an automatic registration algorithm which ﬁnds the correspondences
across a set of images. It shares similar ideas with others [26, 27] which seek
to model sets eﬃciently, representing the image set and iteratively ﬁtting this
model to each image. Operational details can be found elsewhere [28].
The implementation of the group-wise registration has a number of steps. First,
one image is selected as a reference template and all other images are registered
using a traditional template match. Next, a statistical shape model and texture
model is build to represent the image set. Each image is represented in the model
and the correspondences are reﬁned by minimizing a cost function. Finally the
statistical models are undated and the ﬁtting repeated until convergence.
The model used in this paper is a simple mean shape and texture built by
warping all the faces to the mean shape using a triangular Delauney mesh.

Behavioral Consistency Extraction for Face Veriﬁcation
295
F a c e  S e q u e n c e
G r o u p - W i s e
Registration
C o m b i n e d  S h a p e
a n d
T e x t u r e  M o d e l
P C  B a s e d
M o t i o n  P a t t e r n s
L D A  I d e n t i t y
C e n t r a l
T e n d e n c y
C o n f i d e n c e - b a s e d
F u s i o n  S y s t e m
True Client or
I m p o s t e r
Fig. 1. The stages of processing on a se-
quence to reach an identity decision
Fig. 2. Varying +/−3s.d. from individual
subject means two signiﬁcant modes
Fig. 3. Representative stages of the iterated updating of the mean face model for a
single individual. Later stages are to the right; note the increased level of detail.
Figure 3 shows how the registration improves and the model represents the
face structure more accurately as diﬀerent subjects are progressively aligned
together. The matching step deforms each image by minimizing a cost function.
A coarse-to-ﬁne scheme is applied to increase the number of control points and
optimize their position. In the ﬁnal iterations, the points are moved individually
to minimize the cost. The cost function includes both shape and texture parts,
E = λ

i

c −0.5∥di −(Δdi + dneig)∥
σ2s

−|r|
σr
(1)
where r is the residue between the model and the current image after defor-
mation, σr and σs are the standard deviations of the residue and shape, c is a
constant, di is the position of the ith control point, dneig is the average position

296
H. Fang and N. Costen
of the neighborhood around point i and Δdi represents the oﬀset of the point
from the average mean shape.
The initialization of the sparse correspondent feature points is set manually
on the mean image of the image set. When GGR ﬁnds the dense correspondences
across the images, all the sparse feature points are warped to each image using
the triangulation mesh. This helps avoid errors made by the correspondence
matching step and allows concentration upon investigating the importance of
the motion patterns.
It should be noted that while GGR is a possible description of the process
of consolidating faces, with its repeated iteration across the sequence, it is not
necessarily an adequate description of the perception of new face in a sequence.
One possible model of this process centres on building a model of the variation
within the sequence, which then constrains the interpretation of the next frame
[29]. It is notable that this may also provide an explanation of the eﬀects of
jumbling frame-order in the image-sequence.
3.2
Combined Shape and Texture Model
The faces are encoded using an Appearance Model [6]; this takes the output
of the GGR and approximates the manifold, or high dimensional surface, on
which the faces lie. This allows accurate coding, recognition and reproduction
of previously unseen examples. Pixels deﬁned by the GGR points as part of the
face are warped to a standard shape, ensuring that the image-wise and face-wise
coordinates of images are equivalent. If a rigid transformation to remove scale,
location and orientation eﬀects is performed on the point-locations, they can
then be treated in the same way as the grey-levels, as again identical values for
corresponding points on diﬀerent faces will have the same meaning.
Redundancies between feature-point location and grey-level values can be
removed and dimensionality estimated by Principal Components Analysis. Given
a set of N vectors qi (either the pixel grey-levels, or the feature-point locations)
sampled from the images, the covariance matrix C of the images is calculated,
C = 1
N
N

i=1
(qi −¯q)(qi −¯q)T ,
(2)
and orthogonal unit eigenvectors Φ and a vector of eigenvalues λ are extracted
from C.
Redundancies between conﬁguration and pigmentation are removed by per-
forming separate PCAs upon the shape and grey-levels, providing shape pa-
rameters wsi and texture parameters wti. These are combined to form a single
vector for each image on which second PCA is performed [30]. This gives a single
feature vectors
x = Φc
T
WsΦs
T (qs −¯qs)
Φt
T (qt −¯qt)

(3)
for each image, assuming zero-mean weights and where Ws is a diagonal vector
of weights compensating for the characteristic scales of the shape and texture

Behavioral Consistency Extraction for Face Veriﬁcation
297
parameters. This ‘appearance model’ allows the description of the face in terms
of true, expected variation [16].
3.3
Central Tendency
In the same sequence, regardless of parameter change due to diﬀerent poses, light-
ing and expressions, the representation of the identity can be expected to be con-
stant. Stabilization of representation by taking the mean of a number of samples is
known to improve recognition performance [10]. However, in this case, the model
will encode (even after averaging) both identity and non-identity variation. To re-
move the latter, a Linear Discriminate Analysis subspace [5] is used ensure that
only identity information is encoded. LDA is a linear subspace calculated from the
n frames available of each of the p individuals in the ensemble, using
CB = 1
p
p

i=1
(¯xi −¯x)(¯xi −¯x)T
(4)
and
CW = 1
np
p

i=1
n

j=1
(xij −¯xi)(xij −¯xi)T
(5)
to give
CD = CBC−1
W .
(6)
Eigenvectors are then taken to provide a subspace which maximizes variation
between individuals and minimizes that within them. Each frame in a sequence
is projected onto this subspace to give a descriminative encoding d, before taking
the mean and assessing similarity with other sequences,
Sc =
¯di
|¯di| ·
¯dj
|¯dj|.
(7)
3.4
Motion Patterns
When the moving images in one sequence are projected into the combined shape
and appearance subspace, the variation of these encoded representations reveal
the movement of the face; given the psychology it should be possible to perform
recognition on this variation. The characteristic motion patterns are captured by
using PCA performed upon all of the encodings x of a single sequence to yield
a single set of eigenvectors Φ. Although there is no direct relationship between
the motion patterns (a series of eigenvectors) and the speciﬁc behaviors of faces,
some of the more important of variations actually represent some meaningful
changes. Examples for two sequences are shown in Figure 2. In this ﬁgure, the
images are synthesized by adding 3 standard deviations of change on two modes
of the individuals; they are obviously related to some kinds of mouth and eye
orientation changes.

298
H. Fang and N. Costen
The similarity of pairs of spaces can be compared as
Sm = max(trace(colperm(Φi · Φj
T ))),
(8)
where it should be noted that the ordering of the components extracted by the
PCA will be determined by their associated eigenvalues and so the value of Sm
is liable to be reduced by variation in the magnitude of diﬀerent behavioural
tendencies in diﬀerent sequences of the same individual. This is overcome here
by permuting the columns of the matrix describing the relationship between any
two sequences before taking the trace so as to maximize Sm. It should be noted
that this will involve calculating diﬀerent permutions for a single probe sequence
when comparing it with multiple gallery members. In addition, Φi and Φj may
be (identically) truncated to remove noisy, low-variance eigenvectors.
3.5
Fusion of Facial Identity Characteristics
Although recognition may be performed on both the central tendency and motion
patterns independently, neither shows perfect performance and it is possible
to combine the two, balancing one against another. A conﬁdence-based fusion
system [31] is adopted, based on how deﬁnite is each type of information. The
fused similarity can be calculated,
Sf =
CSc
CSc + αCSm
Sc −μSc
σSc

+
αCSm
CSc + αCSm
Sm −μSm
σSm

.
(9)
Here μ and σ are the average and standard deviation of the similarity measures,
allowing normalization. C is a conﬁdence value associated with the two type
of information, shown in Figure 4 and is calculated as the absolute diﬀerence
between the False Acceptance and False Rejection rates; this is dependent upon
the similarity between probe and gallery for that information type and gives a
greater weighting to parameters which will yield conﬁdent decisions to either
accept or reject and individual. Finally α is a weight value, trading between
0.42
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0
10
20
30
40
50
60
70
80
90
100
Probe − Gallery Similarity
Value Percentage
 
 
False Rejection Rate
False Acceptance Rate
Confidence
Fig. 4. The conﬁdence of motion pattern
based on the FA and FR rates
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
90
100
False Accept Rate
True Accept Rate
 
 
G2 motion
G1 motion
Fig. 5. ROC curves for motion
patterns with 41 eigenvectors

Behavioral Consistency Extraction for Face Veriﬁcation
299
the central tendency and movement measures. All of these parameters can be
derived from a set of calibration sequences.
4
Experiment
4.1
Setup
We evaluate the framework by testing performance for facial veriﬁcation. This
is an application of face recognition which seeks to determine whether an indi-
vidual’s real and claimed identity (as determined by some other means such as
a password, identity card, or, as here, a spoken name) coincide. Thus, centering
on one-to-one matches, it diﬀers from facial familiarity or identiﬁcation. Both
of these involve one-to-many matches but the latter requires recording which
gallery face is most similar to the probe, while the former concerns the degree
of similarity to any and all of the gallery. The same features and combination
methods could be used in these cases.
The BANCA database used here includes 52 subjects, half male and half fe-
male, each of which (verbally) claims the identity of two individuals four times
(thus each individual can be used as both a true client and an imposter). Each
such claim sequence has approximately 500 frames of frontal images, of which 4
representative frames of one individual are shown in Figure 6. The database is
divided into two groups, g1 and g2, intended for a leave-some-out design. The 26
subjects in g1 and an extra 10 subjects (so totaling 36 subjects) form an ensemble
and are used to build model, which allows testing of the encoding of the 26 sub-
jects in g2 and vice versa. This both avoids overﬁtting of the model (individuals
to be recognized are not present in the ensemble) and un-representativeness of
the model (faces swap between ensemble and gallery). One of the correct claims
for each test individual is used in the gallery (those frame which are encoded
to give speciﬁc knowledge about particular, identiﬁed individuals), all the other
claims are probes and are so used to test recognition performance.
Because of the size of the database (about 20 seconds for each person, thus
over 250,000 images in the ensemble), representative frames are selected for each
ensemble subject by building an appearance model of that individual, encoding
Fig. 6. Representative frames from a BANCA individual. The left most image is the
ﬁrst frame, used the “static image” condition. The images have the landmark positions
indicated on them; these are not present when the texture is processed by the algorithm.

300
H. Fang and N. Costen
each fame on this AM and using k-means clustering to divide the encodings into
approximately 10 groups (one for each 50 frames). The frames most representa-
tive for each group are then selected and used to build both the combined AM
and LD model.
The model yields a similarity value for each probe and its claimed identity,
which will be either a True Entry or an Imposter Attack. Across the entire set
of probes, each similarity can be used as a threshold, so generating a Receiver
Operating Characteristic (ROC) curve, which summarizes its performance by
trading between the False Acceptance (FA) and False Rejection (FR) rates as
shown in Figures 5, 9 and 10. Two performance criteria, Equal Error Rate (EER)
and the Area Under the Curve (AUC) are used to present the results. In a typical
authentication application, the EER is the best tradeoﬀ, at the point of equal
FA and FR. However, this parameter only concerns a single point on the ROC
curve, and so is not necessarily representative of all changes (particularly those
associated with the extremes of FA and FR). AUC overcomes this, measuring
the total discrimination, and ranging from 1 (for perfect discrimination) to 0 (for
perfect reversed discrimination). If building an actual application, an individual
threshold can be chosen based on the relative risks of the two types of error.
4.2
Experimental Results
All of the probe and gallery frames are encoded on the world combined AM
and LD model and are used to derive the central tendency and motion pattern
parameters. We evaluate the performance of the static veriﬁcation system by
two means of selecting images. Firstly, we selecting the very ﬁrst frame from
each gallery and probe sequence and encode those alone, on the SC measure.
Secondly, we randomly select one image from each sequence and encode those.
Motion Parameters. The individual eigenmodels which provide the motion-
pattern parameters are each derived from approximately 500 observations, each
of which has 41 dimensions. The lower of these ﬁgures limits the number of
parameters which can be derived from the PCA, but some, particularly those
with small eigenvalues may be spurious. The eﬀects of varying the number of
parameters extracted from the gallery images (diﬀerent lines) and the number
of probe-gallery parameter pairings to include in the similarity measure (the
axilla) for AUC on g2 are shown in Figure 7. There is no noticeable interaction
between the means of controlling the number of parameters, but performance
continues increasing until the maximum is reached. This pattern is probably
an unfortunate consequence of the relatively small ensemble (36 individuals)
and the high similarity between gallery and probe sequences. It is possible that
with more variation between the gallery and probes some signiﬁcant truncation
would be required. For comparison purposes, the AUC for the MSM similarity
[23] is also plotted; this is signiﬁcantly lower. Quantitative comparisons for the
two groups, with 41 eigenvectors are shown in Table 1, including two-tailed
probability diﬀerences [32].

Behavioral Consistency Extraction for Face Veriﬁcation
301
Table 1. Error rates and AUCs for movement using the proposed algorithm and MSM.
Probabilities are two-tailed paired-sample tests of similarity of the AUCs.
EER
AUC g1
AUC g2
Data Type
g1
g2
AUC
Z
Prob
AUC
Z
Prob
Motion Patterns 10.50 16.43 0.9072 3.71 0.0002 0.9644 4.82 0.0001
MSM
29.84 28.24 0.7563
0.8061
0
5
10
15
20
25
30
35
40
45
0.7
0.75
0.8
0.85
0.9
0.95
1
Number of Maximum Correlations
Area Under ROC Curve
 
 
06 modes
12 modes
18 modes
24 modes
30 modes
36 modes
41 modes
msm
Fig. 7. Variation in motion-pattern AUC
with gallery and probe dimension
0
0.05
0.1
0.15
0.2
0.25
0.3
0.993
0.994
0.995
0.996
0.997
0.998
0.999
1
weight of motion pattern
Area Under ROC Curve
 
 
G1 weight vs AUC
G2 weight vs AUC
Fig. 8.
Variation
in
combined
AUC
motion-pattern weighting function
Parameter Fusion Weighting. Although the conﬁdence measures and means
and standard deviations can be simply read from the similarity values of the
calibration set (g1 for g2 veriﬁcation and vice versa), the weight value α must be
selected given the classiﬁcation performance of the fused parameters. The AUC
measure for the two groups of sequences is shown in Figure 8. Note that a weight
of 0 implies the use of central tendency alone, and that an α value in the range 0 –
0.2 improves classiﬁcation performance (the ﬁnal weighting is set to 0.08 for both
groups). This relatively low importance of movement is understandable, given the
relatively low discrimination shown by the motion patterns (all 41 eigenvectors)
in Figure 5, and corresponds with the relative diﬃculty humans ﬁnd in learning
or recognizing faces from motion alone, without grey-level information [33].
Combined Recognition. The overall results for the two groups are shown in
Figures 9 and 10 (note that these only show the top 15%), while the EER and
AUC values are given in Table 2, including one-tailed probabilities of signiﬁcant
diﬀerences from the “Random Image” condition. In both cases, the combined
central tendency and motion pattern gives a much lower EER and higher AUC
than the static images, with intermediate performance on the central tendency.
As can be seen from Figure 5, performance on the motion alone is much weaker.
The variation in the results between the two groups suggests that while the initial
frames for g1 are unusually representative, there are more signiﬁcant consisten-
cies in the motion than are seen in g2, which probably lacks motion variation

302
H. Fang and N. Costen
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
False Accept Rate
True Accept Rate
 
 
Static Random Selection
Static First Frame
Central Tendency
Fusion with Motion Pattern
Fig. 9. ROCs for group 1 images
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
False Accept Rate
True Accept Rate
 
 
Static Random Selection
Static First Frame
Central Tendency
Fusion with Motion Pattern
Fig. 10. ROCs for group 2 images
Table 2. Error rates and AUCs for two sorts of static images, central appearance
tendency and fusion of the sequential information. Probabilities are one-tailed paired-
sample tests of similarity of the AUCs relative to the “Random Frame” condition, as
we are consistently adding information to the system.
EER
AUC g1
AUC g2
Data Type
g1
g2
AUC
Z
Prob
AUC
Z
Prob
Random Frame
4.41 3.94 0.9869
0.9965
First Frame
3.29 4.65 0.9965 1.6657 0.0485 0.9958 0.4328 0.3336
Central Tendency 3.93 1.92 0.9972 1.7774 0.0375 0.9991 1.6988 0.0455
Fusion of motion 1.68 1.68 0.9995 1.9063 0.0281 0.9994 1.6250 0.0516
within the sequences. Thus the static images are slightly easier to recognize
and so the diﬀerence with combined recognition is (just) non-signiﬁcant. This
appears to be a peculiarity of the (random but gender-balanced) selection of
sequences in the two groups.
5
Conclusions and Future Work
We have described a framework to use the motion information in facial sequences
to improve face authentication. Two kinds of motion-derived features, the mean
parameters in identity space and major correlated variations across the sequences
of the same subjects, are captured and fused to allow veriﬁcation of the identity
of the face. The associated features from the moving faces achieve signiﬁcantly
better performance than using the static images. In addition, the motion patterns
themselves are signiﬁcantly more useful than the comparable MSM measure [23].
One interpretation of this matrix permution eﬀect is that we are reproducing
the ballencing eﬀect of repopulating the face-space [24], without the need to
use a physical model to hypothesize views. Thus we can extend this advantage
into situation where most of the motion is plastic deformation, as here. The

Behavioral Consistency Extraction for Face Veriﬁcation
303
fusion technique allows an automatic trade-oﬀbetween the use of appearance
and motion-based information as a function of the composition of the dataset.
All of the parameters (with the partial exception of the relatively stable mean –
motion weighting factor) are determined directly from the dataset. However it
should be noted that the need to use all of the possible dimensions in the motion
variation features to get peak performance does suggest a degree of overﬁtting
imposed by the design of the dataset. Diﬀerent sets of sequences and diﬀerent
recognition protocols will yield slightly diﬀerent patterns of results but should
not alter their general tenor.
The results also reﬂect how faces are recognized by the human cognitive sys-
tem when presented with video sequences. As the cognitive system accumulates
the stabilized appearance information and characteristic motion from the se-
quence, it predominately recognizes the face based on the appearance. However,
if the appearance features are indistinct but the conﬁdence in the characteristic
motion patterns is high, the decision will be made based on the latter feature.
This is a possible reason why face recognition can be improved using video se-
quences in both human and computer vision.
In future work, we will investigate how the characteristic motion patterns can
be expanded to include explicit information on the sequence of facial conﬁgura-
tions and test the framework by direct comparison with human performance.
Acknowledgments
We would like to thank Prof. Tim Cootes and Prof. Chris Taylor of Manch-
ester University for providing comments and helpful suggestions. This work was
supported by EPSRC grants EP/D056942 and EP/D054818.
References
[1] Turk, M., Pentland, A.: Face recognition using eigenfaces. In: Computer Vision
and Pattern Recognition Conference, pp. 586–591 (1991)
[2] Wiskott, L., Fellous, J.M., Knueuger, N., Malsburg, C.: Face recognition by elas-
tic bunch graph matching. IEEE Transations on Pattern Analysis and Machine
Intelligence 19(7), 775–779 (1997)
[3] Bartlett, M., Movellan, J., Sejnowski, T.: Face recognition by independent com-
ponent analysis. IEEE Transations on Neural Networks 13(6), 1450–1464 (2002)
[4] Lu, J., Plataniotis, K., Venetsanopoulos, A.: Face recognition using kernel direct
discriminant analysis algorithms. IEEE Transations on Neural Networks 14(1),
117–126 (2003)
[5] Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J.: Eigenfaces vs. Fisherfaces:
Recognition using class speciﬁc linear projection. IEEE Transations on Pattern
Analysis and Machine Intelligence 19, 711–720 (1997)
[6] Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active appearance models. IEEE Tran-
sations on Pattern Analysis and Machine Intelligence 23(6), 681–685 (2001)
[7] Phillips, P., Moon, H., Rizvi, S., Rauss, P.: The feret evaluation methodology for
face recognition algorithms. IEEE Transations on Pattern Analysis and Machine
Intelligence 22(10), 1090–1104 (2000)

304
H. Fang and N. Costen
[8] Phillips, P., Flynn, P., Scruggs, T., Bowyer, K., Chang, J., Hoﬀman, K., Marques,
J., Min, J., Worek, W.: Overview of the face recognition grand challenge. In:
Computer Vision and Pattern Recognition Conference, pp. 947–954 (2005)
[9] Bailly-Bailliere, E., Bengio, S., Bimbot, F., et al.: The banca database and evalua-
tion protocol. In: International Conference on Audio- and Video-Based Biometric
Person Authentication, pp. 625–638 (2003)
[10] Jenkins, R., Burton, A.M.: 100% accuracy in automatic face recognition. Sci-
ence 319(5862), 435 (2008)
[11] O’Toole, A., Roark, D., Abdi, H.: Recognizing moving faces: A psychological and
neural synthesis. Trends in Cognitive Sciences 6(6), 261–266 (2002)
[12] Tomasi, C., Kanade, T.: Shape and motion from image streams under orthog-
raphy: a factorization method. International Journal of Computer Vision 9(2),
137–154 (1992)
[13] Torresani, L., Hertzmann, A., Bregler, C.: Learning non-rigid 3D shape from 2D
motion. In: Neural Information Processing Systems, pp. 1555–1562 (2003)
[14] Xiao, J., Chai, J.X., Kanade, T.: A closed-form solution to non-rigid shape and
motion recovery. International Journal of Computer Vision 67(2), 233–246 (2006)
[15] Blanz, V., Vetter, T.: Face recognition based on ﬁtting a 3D morphable model.
IEEE Transations on Pattern Analysis and Machine Intelligence 25(9), 1–12
(2003)
[16] Craw, I.G., Costen, N.P., Kato, T.: How should we represent face for auto-
matic recognition? IEEE Transactions on Pattern Analysis and machine In-
teligence 21(8), 725–736 (1999)
[17] Lander, K., Chuang, L.: Why are moving faces easier to recognize? Visual Cogni-
tion 23(3), 429–442 (2005)
[18] Zhou, S., Krueger, V., Chellappa, R.: Probabilistic recognition of human faces
from video. Computer Vision and Image Understanding 91, 214–245 (2003)
[19] Benedikt, L., Kajic, V., Cosker, D., Rosin, P.L., Marshall, D.: Facial dynamics in
biomedtric identiﬁcation. In: British Machine Vision Conference, pp. 1065–1075
(2008)
[20] Liu, X., Chen, T.: Video-based face recognition using adaptive hidden markov
models. In: Computer Vision and Pattern Recognition Conference, pp. 26–33
(2003)
[21] Bettinger, F., Cootes, T., Taylor, C.: Modelling facial behaviours. In: British Ma-
chine Vision Conference, pp. 797–806 (2002)
[22] Edwards, G., Taylor, C., Cootes, T.: Improving identiﬁcation performance by in-
tegrating evidence from sequences. In: Computer Vision and Pattern Recognition
Conference, pp. 1486–1491 (1999)
[23] Yamaguchi, O., Fukui, K., Maeda, K.: Face recognition using temporal sequence.
In: IEEE International Conference on Automatic Face and Gesture Recognition,
vol. 10, pp. 318–323 (1998)
[24] Arandjelovi´c, O., Cipolla, R.: An information-theoretic approach to face recogni-
tion from face motion manifolds. Image and Vision Computing 24, 639–647 (2006)
[25] Knappmeyer, B., Thornton, I.M., B¨ulthoﬀ, H.H.: The use of facial motion and
facial form during the processing of identity. Vision Research 43, 1921–1936 (2003)
[26] Baker, S., Matthews, I., Schneider, J.: Automatic construction of active appear-
ance models as an image coding problem. IEEE Transations on Pattern Analysis
and Machine Intelligence 26(10), 1380–1384 (2004)

Behavioral Consistency Extraction for Face Veriﬁcation
305
[27] Cootes, T.F., Marsland, S., Twining, C.J., Smith, K., Taylor, C.J.: Groupwise
diﬀeomorphic non-rigid registration for automatic model building. In: Pajdla, T.,
Matas, J(G.) (eds.) ECCV 2004. LNCS, vol. 3024, pp. 316–327. Springer, Heidel-
berg (2004)
[28] Cootes, T.F., Twining, C.J., Petrovic, V., Schestowitz, R., Taylor, C.J.: Groupwise
construction of appearance model using piece-wise aﬃne deformations. In: British
Machine Vision Conference, pp. 879–888 (2005)
[29] Fang, H., Costen, N.P.: Tracking face localization with a hierarchical progressive
face model. In: Gonz`aliz, J., Moeslund, T.B., Wang, L. (eds.) Tracking Humans
for the Evaluation of their Motion in Image Sequences, pp. 89–99 (2008)
[30] Edwards, G.J., Lanitis, A., Taylor, C., Cootes, T.F.: Modelling the variability in
face images. In: IEEE International Conference on Automatic Face and Gesture
Recognition, pp. 328–333 (1996)
[31] Poh, N., Bengio, S.: Improving fusion with margin-derived conﬁdence in biometric
authentication tasks. In: Kanade, T., Jain, A., Ratha, N.K. (eds.) AVBPA 2005.
LNCS, vol. 3546, pp. 474–483. Springer, Heidelberg (2005)
[32] Hanley, J.A., McNeil, B.J.: A method for comparing the areas under Receiver
Operating Characteristic curves derived from the same cases. Radiology 148, 839–
843 (1983)
[33] Hill, H., Johnston, A.: Categorizing sex and identity from the biological motion
of faces. Current Biology 11, 880–885 (2001)

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 306–314, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Protecting Face Biometric DCT Templates by Means of 
Pseudo-random Permutations 
Marcos Faundez-Zanuy 
Escola Universitària Politècnica de Mataró (Adscrita a la UPC) 
08303 MATARO (BARCELONA), Spain 
faundez@eupmt.es 
http://www.eupmt.es/veu 
Abstract. Biometric template security and privacy is a great concern of biome-
tric systems, because unlike passwords and tokens, compromised biometric 
templates cannot be revoked and reissued. In this paper we present a protection 
scheme based on a user dependent pseudo-random ordering of the DCT tem-
plate coefficients. In addition to privacy enhancement, this scheme also lets to 
increase the biometric recognition performance, because a hacker can hardly 
match a fake biometric sample without knowing the pseudo-random ordering. 
1   Introduction 
Biometric template security is an important issue because unlike passwords and to-
kens, compromised biometric templates cannot be revoked and reissued. Thus, there 
is a strong interest on the possibility to cancel and replace a given biometric data 
when compromised. If a biometric is lost once (illegally acquired by a hacker), it is 
compromised forever, and sometimes even for the next generations: as an example 
[1], information on our DNA has implications on the DNA of our children and grand-
children, hence this information may need to be protected for a very long time. 
An ideal biometric template protection scheme should possess the following prop-
erties [2-3]: 
1) Diversity: the secure template must not allow cross-matching across databases, 
thereby ensuring the user’s privacy. 
2) Revocability: it should be straightforward to revoke a compromised template 
and reissue a new one based on the same biometric data. 
3) Security: it must be computationally hard to obtain the original biometric tem-
plate from the secure template. This property prevents a hacker from creating a 
physical spoof of the biometric trait from a stolen template. 
4) Performance: the biometric template protection scheme should not degrade the 
recognition performance (identification or verification rates). 
For the next years, we are evolving towards ambient intelligence, pervasive network-
ing or ubiquitous computing, which have special characteristics [1]. In addition there 
is a gradual erosion of the computational difficulty of the mathematical problems on 

 Protecting Face Biometric DCT Templates by Means of Pseudo-random Permutations 
307 
which cryptology is based, due to developments in computation (progress in electron-
ics and in the future in optical and maybe even quantum computing). This increases 
the vulnerability [4] of biometric systems [5]. 
Encryption is not a smooth function [3] and a small difference in the values of the 
feature sets extracted from the raw biometric data would lead to a very large differ-
ence in the resulting encrypted features. While it is possible to decrypt the template 
and perform matching between the query and decrypted template, such an approach is 
not secure because it leaves the template exposed during every authentication attempt. 
Thus, standard encryption techniques are not useful for securing biometric templates. 
The solutions proposed in the literature can be split into two categories [3]:  
1) Feature transformation. 
2) Biometric Cryptosystems. 
We will describe this categories in the next sections. 
1.1   Feature Transformation 
A transformation function ܻൌ݂ሺݔሻ is applied to the biometric information and only 
the transformed template is stored in the database. The parameters of the transforma-
tion function are typically derived from a random key or password. The same trans-
formation function is applied to the test signal and the transformed query is directly 
matched against the transformed template. 
Feature transformation can be divided into salting and non-invertible transforms. In 
salting ܻൌ݂ሺݔሻ is invertible. Thus, if a hacker knows the key and the transformed 
template, he can recover the original biometric template, and the security is based on 
the secrecy of the key or password. This is the unique approach that requires a secret 
information (key). This is not necessary in the other categories. The second group is 
based on noninvertible transformation systems. They apply a one-way function on the 
template and it is computationally hard to invert a transformed template even if the 
key (transform function) is known. The main drawback of this approach is the trade-
off between discriminability and noninvertibility of the transformation function. 
Transformed features belonging to the same user should have high similarity after 
transformation, while features from different users should be quite different. 
A recent paper [6] proposes a noninvertible (cancelable) transformation. Instead of 
storing the original biometric information, it is transformed using a one-way function. 
The transformed biometric and the transformation are stored either distributed on a 
smart-card on centrally in a database. The transformation can be performed either in 
the signal domain (such as by means of the distortion shown in figure 1) or in the 
feature domain (see fig. 2). 
This construct preserves privacy since it will be computationally very hard to recov-
er the original biometric template using such a transformed version. If a biometric is 
compromised, it can be simply reenrolled using another transformation function, thus 
providing revocability. The construct also prevents cross-matching between the data-
bases, since each application using the same biometric uses a different transformation. 
While our intuition seems to suggest that it is very easy to design a function that is 
“easy” to compute but “hard” to invert, so far the best theoretical result can prove that 
there exist functions that are twice as hard to invert as to compute [7]. It is clear that 
such functions would be completely useless to practical cryptology [1]. 

308 
M. Faundez-Zanuy 
 
Fig. 1. If the face is distorted in the signal domain prior to feature extraction, the distorted ver-
sion does not match with the original biometric, while the two instances of distorted faces match 
among themselves. 
 
Fig. 2. Each feature can be transformed using a noninvertible function ܻൌ݂ሺݔሻ. In this case 
an ݔଵ value is mapped to ܻଵൌ݂ሺݔଵሻ. In this case, if we know ܻଵ, the inverse mapping is a 
many-to-one transformation. ݔଵ, ݔଶ, ݔଷ, ݔସ , ڮ , ݔ଼ are all valid inverse mappings to ܻଵ . The 
complexity of the inverse mapping is exponential in the number of features, making the trans-
form practically noninvertible. 
1.2   Biometric Cryptosystems 
In this approach some public information about the biometric template is stored. This 
public information is called helper data. For this reason they are also known as helper 
data-based methods. While the helper data does not reveal any significant information 
about the original biometric template, it is needed during the matching to extract a 
cryptographic key from the input signal. Matching is performed indirectly by verify-
ing the correctness of the extracted key. In order to cope with intra-user variations, 
error correction coding techniques are usual. 
Y=f(x) 
x1 
x3 
x4 x5 
x6 
x7 
x8 
x 
x2 
MATCH 
MATCH
NO MATCH
NO MATCH
Original 
Distorted
Repeatable 
distortion
Y1 

 Protecting Face Biometric DCT Templates by Means of Pseudo-random Permutations 
309 
Biometric cryptosystems can also be split into two groups: key binding and key gen-
eration systems depending on how the helper data is obtained. When the helper data is 
obtained by binding a key (that is independent of the biometric features) with the bio-
metric template, it is known as key-binding biometric cryptosystem. In the second case, 
the helper data is derived only from the biometric template and the cryptographic key is 
directly generated from the helper data and the query biometric features. This second 
approach is known as key generation biometric cryptosystem. Figure 3 summarizes the 
scheme of key generation biometric cryptosystems [8]. Authentication in a key-binding 
biometric cryptosystem is similar except that the helper data is a function of both the 
template and the key K. 
Hybrid techniques combining the previous four approaches are also possible. 
 
 
Fig. 3. Scheme for biometric cryptosystems. In the enrollment phase, the biometric template ܺ௡ 
is used for the derivation of a secret S and helper data W. A hashed version ܨሺܵሻ of the secret 
and the helper data W are stored in the database. In the test phase a noisy version  ܻ௡ of the 
biometric template is acquired. The helper data W is used to derive a secret V from ܻ௡. If 
ܨሺܸሻൌܨሺܵሻ, the user is accepted. Otherwise he/she is rejected. It is important to emphasize 
that in contrast to classical biometric systems, an exact match is required. 
2   Pseudo-random Permutations 
In this paper, in order to secure the templates, we use a different random permutation of 
template coefficients for each person. Thus, our proposal corresponds to salting feature 
transformation described in section 1.1. Figure 4 shows the diagram for the proposed 
approach. The template coefficients are equal to the DCT components of the two di-
mensional transform of the face image [9]. The permutation order is different for reach 
person (although more than one permutation per person is possible) and it is given by a 
Key, which must be kept secret. The advantages of this strategy are the following: 
• There is an increase on privacy, because it is impossible (computationally very 
hard) to obtain the face image without knowing the permutation order. Figure 5 
shows the number of different permutations that can be done for a given feature 
vector length. If a feature vector template has N coefficients, the number of 
permutations of these coefficients is equal to N! 
• There is an improvement on the recognition rates because an impostor does not 
know the correct permutation order. This is similar to the privacy achieved by 
CDMA (Code Division Multiple Access) used in some mobile telephone standards 
Xn 
enc 
F
S 
F(S)
Data 
base 
accept/ 
reject 
MATCHING
W
Yn 
dec 
F
V 
F(V)
enrollment
test
W
F(S)

310 
M. Faundez-Zanuy 
in order to secure the communications. If you do not know the correct order (pro-
vided by the pseudo-random frequency hopper order) you cannot decode the mes-
sage. Obviously if the impostor knows the permutation order then he can sort 
his/her feature vector and then the protection is equal to the biometric system 
without permutation. Anyway, it is as difficult to get the Key as in other security 
systems (VISA number, password, PIN, etc.). 
• In contrast to some encryption systems, where the template must be decrypted 
before comparison, in our approach there is no need to re-order the coefficients. 
They can be directly compared. 
 
 
Fig. 4. Scheme for proposed system. In the enrollment phase, the biometric template ܺ௡ coeffi-
cients are permuted according to the secret key K. This permuted template is stored in the data-
base.  In the test phase a noisy version  ܻ௡ of the biometric template is acquired, and the user 
provides his/her Key. The permuted template ܨሺܸሻ is matched with the template of the claimed 
identity. If ݀݅ݏݐܽ݊ܿ݁൫ܨሺܸሻ, ܨሺܵሻ൯൏ݐ݄ݎ݁ݏ݄݋݈݀, the user is accepted. Otherwise he/she is 
rejected. 
 
 
Fig. 5. Number of permutations for a feature vector of # coefficients 
 
0
20
40
60
80
100
10
0
10
50
10
100
10
150
10
200
# permutations
# coeficients
Xn 
Pseudo-random 
permutation 
F(S)
Data 
base 
accept/ 
reject 
MATCHING 
& Threshold 
K 
Yn 
Pseudo-random 
permutation 
F(V)
enrollment
test
F(S)
K 

 Protecting Face Biometric DCT Templates by Means of Pseudo-random Permutations 
311 
Although this is a very simple approach, it has a main advantage: experimental re-
sults are always better or equal than the baseline biometric system without encryption. 
This is not the case with a large majority of existing systems in the literature, where 
intra-user variability is hard to manage and provides an increase in False Rejection 
Rates. 
3   Experimental Results for Face Recognition 
In this paper we will use the AR database [10], which consists of 126 individuals, 
with 26 images for each, taken in two different sessions at a time distance of two 
weeks, varying the lighting and the facial expression. We have used 10 of the 26 im-
ages, excluding the ones overexposed and the ones in which the face was partially 
occluded by sunglasses or scarves. Nine individuals were not complete or not availa-
ble. Thus, we have only used 117. We have used five images as enrollment templates 
and 5 different ones for testing the system. The classifier consists of a nearest neigh-
bor  with a Mean Absolute Difference (MAD) and Mean Squared Error (MSE) error 
criterion. Figure 6 shows some snapshots for the first user in the database. 
 
 
 
Fig. 6. Sample images of one person from the AR database 
Figure 7 shows the verification error rates with and without random permutation 
(one different random permutation for each person) using the AR database. We have 
used as verification error the minimum Detection Cost Function (minDCF), which is 
defined [11] as 
, where Cmiss is the cost 
of a miss (rejection), Cfa is the cost of a false alarm (acceptance), Ptrue is the a priori 
probability of the target, and Pfalse = 1 − Ptrue , Cmiss= Cfa =1. 
When trying to synthesize again a face from the random permuted coefficients, a 
hacker would achieve the result shown in figure 8. Taking into account that in a nor-
mal DCT image the energy is concentrated in the low frequency area, a possible re-
ordering scheme could consist of natural ordering according to energy. Figure 8 on 
the left shows the inverse DCT of a set of coefficients:  on the left without permuta-
tion, on the center with a permuted set of coefficients, and on the right with natural re-
ordering of DCT coefficients (ordered according to their energy). 
miss
miss
true
fa
fa
false
DCF
C
P
P
C
P
P
=
×
×
+
×
×

312 
M. Faundez-Zanuy 
While the transformation function of the non-invertible transform methods should 
be dependent on the features used in the specific application, our proposed approach 
can be directly applied, for instance, to our online signature recognition system [12] 
based on signature normalization to 100 points [13]. 
 
  
Fig. 7. MinDCF with and without coefficient permutation for MAD and MSE error criterion. 
Perm corresponds to secure key (only the genuine user knows the correct Key that provides the 
correct permutation user). The other case with higher minimum detection cost function corres-
ponds to the unsecure key (all the impostors know the keys of each other). 
  
 
 
Fig. 8. Inverse DCT of a template of 1024 coefficients. Without permutation (on the left), with 
permutation (on the center) and trying to reorder according to energy (on the right). 
This paper has presented preliminary experimental results that can be enhanced in 
several ways, such as user-dependent feature selection. In fact, we have used a zonal 
coding, while our last development, Biometric Dispersion Matcher [14] is well suited 
to this kind of feature selection, which can be seen as threshold coding with discrimi-
nability criterion, rather than representability criterion used in transform image coding 
methods. In the case of BDM, probably, the selected features would not make possi-
ble to recover the face image when performing an inverse transform. 
10
0
10
1
10
2
10
3
10
4
0
10
20
30
40
# coefficients
%
Verification: Minimum Detection Cost Function
 
 
DCT-MAD
DCT-MSE
DCT-MAD-perm
DCT-MSE-perm

 Protecting Face Biometric DCT Templates by Means of Pseudo-random Permutations 
313 
4   Conclusions 
In this paper we have presented a simple approach for biometric template protection 
that belongs to the salting feature transformation. The unique drawback of this ap-
proach is that it requires a secret key. If it is compromised, the biometric template can 
be exactly recovered. Nevertheless, this should be as difficult as to obtain the VISA 
number, PIN, or password of a classical security system. The advantages of this ap-
proach are the following: 
a) There is always an improvement on the biometric verification errors which are 
dramatically reduced. This is not the case of a large majority of the proposed 
methods in the literature, where privacy implies a degradation of biometric 
performance (False Rejection Rate is increased). 
b) If the key is kept secret the number of different permutations equals N!, which 
is a very large value for typical face template sizes. 
c) Comparison between enrollment samples and test sample can be done exactly 
in the same way than the classical system without encryption. 
Although few efforts have been devoted to feature transformation by means of salting 
due to the risk of compromising the face image if the secret key is cracked, we should 
consider that: 
a) In fact the face image is not secret and easy to obtain using a webcam, mobile 
phone, etc. 
b) The chance to crack the secret Key should be comparable to obtain PIN and 
VISA number. People would agree that although imperfect, it is a secure pay-
ment method. 
c) This work can be extended to other versions without the face image cannot be 
derived by means of inverse transform of the template. 
Acknowledgements 
This work has been supported by COST2102, FEDER and MEC, TEC2006-13141-
C03-02/TCM. 
References 
1. Preneel, B.: ENCRYPT: the Cryptographic research challenges for the next decade. In: Blun-
do, C., Cimato, S. (eds.) SCN 2004. LNCS, vol. 3352, pp. 1–15. Springer, Heidelberg (2005) 
2. Maltoni, D., Maio, D., Jain, A.K., Prabhakar, S.: Handbook of Fingerprint Recognition. 
Springer, Heidelberg (2003) 
3. Jain, A.K., Nandakumar, K., Nagar, A.: Biometric template security. Eurasip journal on 
Advances in Signal Processing. Special issue on Biometrics, 1–20 (January 2008) 
4. Faundez-Zanuy, M.: On the vulnerability of biometric security systems. IEEE Aerospace 
and Electronic Systems Magazine 19(6), 3–8 (2004) 
5. Faundez-Zanuy, M.: Biometric security technology. IEEE Aerospace and Electronic Sys-
tems Magazine 21(6), 15–26 (2006) 

314 
M. Faundez-Zanuy 
6. Ratha, N.K., Chikkerur, S., Connell, J.H., Bolle, R.M.: Generating cancellable fingerprint 
templates. IEEE Trans. on Pattern Analysis and Machine Intelligence 29(4), 561–572 (2007) 
7. Hiltgen, A.P.L.: Constructions of feebly-one-way families of permutations. In: Zheng, Y., 
Seberry, J. (eds.) AUSCRYPT 1992. LNCS, vol. 718, pp. 422–434. Springer, Heidelberg 
(1993) 
8. Tuyls, P., Goseling, J.: Capacity and examples of template-protecting biometric authenti-
cation systems. In: Maltoni, D., Jain, A.K. (eds.) BioAW 2004. LNCS, vol. 3087, pp. 158–
170. Springer, Heidelberg (2004) 
9. Faundez-Zanuy, M., Roure-Alcobe, J., Espinosa-Duró, V., Ortega, J.A.: An efficient face 
verification method in a transformed domain. Pattern Recognition Letters 28(7), 854–858 
(2007) 
10. Martinez, A.M.: Recognizing Imprecisely Localized, Partially Occluded, and Expression 
Variant Faces from a Single Sample per Class. IEEE Transaction on Pattern Analysis and 
Machine Intelligence 24(6), 748–763 (2002) 
11. Martin, A., Doddington, G., Kamm, T., Ordowski, M., Przybocki, M.: The DET curve in 
assessment of detection performance. In: European speech Processing Conference Euros-
peech 1997, vol. 4, pp. 1895–1898 (1997) 
12. Faundez-Zanuy, M.: Signature recognition state-of-the-art. IEEE Aerospace and Electronic 
Systems Magazine 20(7), 28–32 (2005) 
13. Vivaracho, C., Faundez-Zanuy, M., Gaspar, J.M.: An efficient low cost approach for On-
Line signature recognition based on length normalization and fractional distances. Pattern 
Recognition 42(1), 183–193 (2009) 
14. Fàbregas, J., Faundez-Zanuy, M.: Biometric Dispersion Matcher. Pattern Recognition 41(11), 
3412–3426 (2008) 

Facial Expressions Recognition from Image Sequences
Zahid Riaz, Christoph Mayer, Michael Beetz, and Bernd Radig
Department of Informatics, Technische Universit¨at M¨unchen,
D-85748 Garching, Germany
Abstract. Human machine interaction is one of the emerging ﬁelds for the com-
ing years. Interacting with others in our daily life is a face to face interaction.
Faces are the natural way of interaction between humans and hence also useful in
human machine interaction.
This paper describes a novel technique to recognize the human facial ex-
pressions and manipulating this task for human machine interaction. We use 2D
model based approach for human facial expression recognition. An active shape
model (ASM) is ﬁtted to the face image and texture information is extraced. This
shape and texture information is combined with optical ﬂow based temporal in-
formation of the image sequences to form a feature vector for the image. We ex-
perimented on image sequences of 97 different persons of Cohn-Kanade-Facial
Expression Database. A classiﬁcation rate of 92.4% is obtained using a binary
decision tree classiﬁer, whereas a classiﬁcation rate of 96.4% is obtained using
pairwise classiﬁer based on support vector machines. This system is capable to
work in realtime.
Keywords: Face Modeling, Active Appearance Models, Facial Expressions
Recognition, Face Recognition.
1
Introduction
Human face analysis plays an important role for the development of social robots. Such
scenarios arise when humans and robots are working on some common task. Existing
methods for human-machine interaction are often considered unintuitive. Especially for
social robots which are operating in various systems in daily life. They require timely
human intervention and sometimes result in unfriendly mannered. On the other hand
it requires a lot of time and effort to train human to interact with machines which is
still not intuitive. However, in this paper we have tried to use those characteristics of
the humans which could be intuitively interpreted by the machines and hence result-
ing in a an autonomous system working without human interaction. To participate in
natural human-machine interaction, machines must be able to derive information from
human communication channels, such as spoken language, gestures or facial expres-
sions. We evaluate human face information in order to classify the facial expressions
which could be easily embedded in advanced systems to study human facial behaviors.
We used model based image interpretation techniques to extract information about fa-
cial changes. Models impose knowledge about the object of interest and reduce high
dimensional image data to a small number of expressive model parameters. The model
used by our system is a point distribution model (PDM) [3]. A combination of the fa-
cial features is used for binary decision tree to classify six basic facial expressoins i.e.
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 315–323, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

316
Z. Riaz et al.
anger, fear, surprise, saddness, laugh and disgust. Unlike many competing systems, the
approach is fully automated and requires no human intervention.
This work focuses on one of the aspects of natural human-computer interfaces: our
goal is to build a real-time system for facial expressions recognition that could robustly
run in real-world environments. We develop it using model-based image interpretation
techniques, which have proven its great potential to fulﬁll current and future requests
on real-world image understanding.
2
Related Work
We explain facial expressions recognition as a three step process consisting of face
detection, features extraction, and expressions classiﬁcation [1]. The ﬁrst step aims at
determining the position and shape of the face in the image by ﬁtting a model. Features
descriptive for facial expressions or head gestures are extracted in the second step. In
the third step a classiﬁer is applied to the features to identify the expressions. Cootes
et al. [5] introduced modelling shapes with Active Contours. Further enhancements in-
cluded the idea of expanding shape models with texture information [6]. In order to ﬁt
a model to an image, Van Ginneken et al. learned local objective functions from an-
notated training images [14]. In this work, image features are obtained by approximat-
ing the pixel values in a region around a pixel of interest The learning algorithm used
to map images features to objective values is a k-Nearest-Neighbor classiﬁer (kNN)
learned from the data. We used similar methodology developed by Wimmer et al. [4]
which combines multitude of qualitatively different features [15], determines the most
relevant features using machine learning [14], and learns objective functions from an-
notated images [14]. In order to ﬁnd features, Michel et al. [10] extracted the location
of 22 feature points within the face and determine their motion between an image that
shows the neutral state of the face and an image that represents a facial expression.
The very similar approach of Cohn et al. [11] uses hierarchical optical ﬂow in order to
determine the motion of 30 feature points.
A set of training data hence formed from the extracted features is learned on a clas-
siﬁer. Some approaches infer the facial expression from rules stated by Ekman and
Friesen [9]. This approach is applied by Kotsia et al. [12] to design Support Vec-
tor Machines (SVM) for classiﬁcation. Michel et al. [10] train a Support Vector Ma-
chine (SVM) that determines the visible facial expression within the video sequences
of the Cohn-Kanade Facial Expression Database by comparing the ﬁrst frame with the
neutral expression to the last frame with the peak expression. The features we used here
are quite efﬁcient not only for facial expression recognition but also for face recognition
against facial expressions [13].
3
Our Approach
This paper describes the classiﬁcation of facial expresions for the image sequences. We
derive features which are suitable not only for person identity but also for classifying
six basic facial expressions. The face feature vector consists of the shape, texture and
temporal variations, which sufﬁciently deﬁnes global and local variations of the face.

Facial Expressions Recognition from Image Sequences
317
All the subjects in the database are labeled for different facial expressions. A ﬁtted
face model, on the training images is then used for deﬁning the reference shape in our
experiments. This reference shape is calculated by ﬁnding the mean shape of all the
shapes in the database.
In this paper a robustly ﬁtted model is coupled with temporal information of the im-
age sequences. A point distribution model (PDM) consisting of 134 landmarks deﬁned
on various location on the face is used as an active shape model. An objective function
is learned for ﬁtting this model to the faces. After ﬁtting the model to the example face
image, texture information is extracted from the example image on a reference shape
which is the mean shape of all the shapes of database. Image texture is extracted using
planar subdivisions of the reference and the example shapes. Texture warping between
the subdivisions is performed using afﬁne transformation. Principal Components Anal-
ysis (PCA) is used to obtain the texture and shape model parameters of the example
image. This approach is similar to extracting Active Appearance Model (AAM) param-
eters. In addition to AAM parameters, temporal features of the facial changes are also
calculated. Local motion of the feature points is observed using optical ﬂow. We use
reduced descriptors by trading off between accuracy and runtime performance. These
features are then used for binary decision tree (BDT) for facial expressions recognition.
A detailed process ﬂow of our approach is shown in Figure 1. Our approach achieves
real-time performance. This computer vision task comprises of various phases for which
Fig. 1. Our Approach: Different modules working independently towards facial expressions
recognition

318
Z. Riaz et al.
it exploits model-based techniques that accurately localize facial features, seamlessly
track them through image sequences, and ﬁnally infer facial expressions.
The remainder of this paper is divided in four sections. In section 4 model based im-
age interpretation is described along with the model ﬁtting process. Sections 5 discusses
about the model based feature extraction technique comprising of shape and appearance
along with the temporal features. Section 6 deals with feature classiﬁcation along with
experimentations and ﬁnally the conclusions in section 7.
4
Model-Based Image Interpretation
Model-based techniques consist of four components: the model, the initialization algo-
rithm, the objective function, and the ﬁtting algorithm. Our approach makes use of a
statistics-based deformable model, as introduced by Cootes et al. [5]. The model con-
tains a parameter vector pthat represents its possible conﬁgurations, such as position,
orientation, scaling, deformation and texture. Models are mapped onto the surface of an
image via a set of feature points, a contour, a textured region, etc. Referring to [2], de-
formable models are highly suitable for analyzing human faces with all their individual
variations. Its parameters p comprise the translation, scaling factor, rotation, and a vec-
tor of deformation parameters b. The latter component describes the conﬁguration of the
face, such as the opening of the mouth, roundness of the eyes, raising of the eye brows.
4.1
Model Initialization
The initialization algorithm automatically localizes the object roughly to interpret and
computes an initial estimate of the translation and scaling parameters. We integrate the
approach of Viola and Jones to detect frontal faces. In order to obtain higher accuracy,
we again apply this algorithm to detect facial components, such as eyes and mouth
within the face bounding box and derive a rough estimation of the shape parameters as
well. Usually resulting detectors are not able to robustly localize the eyes or the mouth
in a complex image, because it usually contains a lot of information that was not part of
the training data. However, it is highly appropriate to determine the location of the eyes
within a pure face image or within the face region of a complex image.
4.2
Model Fitting
The model parameters determined in the initialization step are only a rough guess and
therefore need reﬁnement. An objective function f(I, p) that yields a comparable value to
measure how accurately a parameterized model p describes an image I is utilized in this
challenge. It is also known as the likelihood,similarity, energy,cost, goodness, or quality
function. Without losing generality, we consider lower values to denote a better model
ﬁt. Traditionally, objective functions are manually speciﬁed by ﬁrst selecting a small
number of simple image features, such as edges or corners, and then formulating math-
ematical calculation rules. Afterwards, the appropriateness is subjectively determined
by inspecting the result on example images and example model parameterization. If the
result is not satisfactory the function is tuned or redesigned from scratch. This heuristic

Facial Expressions Recognition from Image Sequences
319
approach relies on the designer’s intuition about a good measure of ﬁtness. Earlier works
by Wimmer et al. [4] show that this methodology is erroneous and tedious.
The ﬁtting algorithm searches for the model that best describes the face visible in the
image. Therefore, it aims at ﬁnding the model parameters that minimize the objective
function. Fitting algorithms have been the subject of intensive research and evaluation,
e.g. Simulated Annealing,Genetic Algorithms,Particle Filtering, RANSAC, CONDEN-
SATION, and CCD, see [16] for a recent overview and categorization. We propose to
adapt the objective function rather than the ﬁtting algorithm to the speciﬁcs of our appli-
cation. Therefore, we are able to use any of these standard ﬁtting algorithms, the char-
acteristics of which are well-known, such as termination criteria, runtime, and accuracy.
To avoid these drawbacks, we recently proposed an approach that learns the objec-
tive function from annotated example images [4]. It splits up the generation of the
objective function into several tasks partly automated. This provides several beneﬁts:
ﬁrstly, automated steps replace the labor-intensive design of the objective function. Sec-
ondly, this approach is less error prone, because giving examples of good ﬁt is much
easier than explicitly specifying rules that need to cover all examples. Thirdly, this ap-
proach does not rely on expert knowledge and therefore it is generally applicable and
not domain-dependent. The bottom line is that this approach yields more robust and
accurate objective functions, which greatly facilitate the task of the ﬁtting algorithm.
5
Face Model Based Feature Extraction
In order to classify facial expressions, we require a reasonable set of descriptive fea-
tures. We apply two kinds of facial features to represent the face state and face move-
ment: global features and local features. The ﬁrst represent face properties of single
images, such as the opening of the mouth or rising of the eye-brows. We combine them
with information about movement within the face region to form an invariant feature
vector. As our experimental evalution will prove, these extracted features are efﬁcient
for face and facial expressions recognition.
The shape x is parameterized by using mean shape xm and matrix of eigenvectors
Ps to obtain the parameter vector Ps [7].
x = xm + Psbs
(1)
Shape information is extracted to reﬂect changes in facial expression. This informa-
tion determines the state of various facial features such as eye brows or the mouth. For
texture extraction, we calculate a reference shape which we chose to be the mean shape,
obtained by taking the mean of all the shapes of all persons in our database. Figure 2
shows shape information interpretation.
Since the points distribution deﬁnes a convex hull of points in space so a suitable pla-
nar subdivision could be deﬁned for the reference shape to map image texture. Delaunay
triangulation is used to divide the shape into a set of distinct facets. The extracted texture
is parameterized using PCA by using mean texture gmand matrix of eigenvectors Pgto
obtain the parameter vector bg [7]. Figure 3 shows texture extracted from face image.
g = gm + Pgbg
(2)

320
Z. Riaz et al.
Fig. 2. Shape information is extracted to represent the state of various facial feature such as the
rising of the eye brows or the opening angle of the mouth
Fig. 3. Texture information is represented by an appearance model. Model parameters of the
ﬁtted model are extracted to represent single image information.
Since facial expressions emerge from muscle activity, the motion of particular feature
points within the face gives evidence about the facial expression. These features further
help the classiﬁer to learn the motion activity and their relative location is connected to
the structure of the face model. Note that we do not specify these locations manually,
because this assumes a good experience of the designer in analyzing facial expressions.
In contrast, we automatically generate T feature points that are uniformly distributed.
We expect these points to move descriptively and predictably in the case of a particular
facial expression. We sum up the motion tx,i and ty,i of each point 1 ≤i ≤T during
a short time period. We set this period to 2 sec to cover slowly expressed emotions
as well. The motion of the feature points is normalized by the afﬁne transformation
of the entire face in order to separate the facial motion from the rigid head motion.
In order to determine robust descriptors, PCA determines the H most relevant motion
patterns (principal components) visible within the set of training sequences. A linear
combination of these motion patterns describes each observation approximately correct.
This reduces the number of descriptors by enforcing robustness towards outliers as well.
As a compromise between accuracy and runtime performance, we set the number of
feature points to T = 140 and the number of motion patterns bt to H = 14 containing.
Figure 4 shows motion pattern withing an image.

Facial Expressions Recognition from Image Sequences
321
Fig. 4. Motion patterns within the image are extracted and the temporal features are calculated
from them. These features are descriptive for a sequence of images rather than single images.
We compile all extracted features into a single feature vector. Single image infor-
mation is considered by the structural and textural features whereas image sequence
information is considered by the temporal features. The overall feature vector then be-
comes:
u = (bs1, ...., bsm, bt1, ...., btm, bg1, ...., bgm)
(3)
Where bs, bt and bg are shape, temporal and textural parameters respectively.
6
Experimental Evaluation
Experiments have been performed on Cohn-Kanade-Facial-Expression database
(CKFE-DB) for human faces. The CKFE-DB contains 488 short image sequences of
97 different persons performing the six universal facial expressions [8]. It provides
researchers with a large dataset for experimenting and benchmarking purpose. Each
sequence shows a neutral face at the beginning and then develops into the peak ex-
pression. Therefore, a decision has to be made at which point the face changes from a
neutral state to an acctual expression and we label the ﬁrst half of every sequence to be
neutral and the second half to depict expression.
Note that this database does not contain natural facial expressions, but volunteers
were asked to act. Furthermore, the image sequences are taken in a laboratory environ-
ment with predeﬁned illumination conditions, solid background and frontal face views.
Algorithms that perform well with these image sequences are not immediately appro-
priate for real-world scenes.
We apply machine learning techniques to derive calculation rules for facial expres-
sion that are calculated from the complete assembled feature vector as well as from
subsets of the features. The classiﬁers determine the facial expression for every image
separately. Note that the information provided by the temporal features is still depen-
dent on the predecent images and therefore takes into account not only the information
of one image but of all previous images in the sequence. However, In order to avoid
overﬁtting, we used 10-fold cross validation in both cases.
We apply support vector machines (SVM) for pairwise classiﬁcation.

322
Z. Riaz et al.
In a ﬁrst experiment SVMs is trained with the temporal features only and achieved
an accuracy of 79.5%. This result is improved to 92.2% by additionally providing the
structural features. This is due to the fact that these features provide information about
both, the single image and the image sequence. The best result is gained by providing
full set of features and the accuracy raised to 96.4%
In order to create a classiﬁer that is applicable in real-world environments as well, we
train a classiﬁer that is both, fast and capable of considering several class labels in the
data. We apply decision trees for this task and provide them with the completely assem-
bled data vector. The accuracy measured is 92.6%. Note that in contrast to the SVMs
the decision tree does not utilize previous information on the image label. Since its ac-
curacy is only 3.8% below the best accuracy of SVM, this proves the high descriptive
strength of the features provided.
7
Conclusions
We introduced an idea to develop a feature vector which consists of three types of fa-
cial variations. The features extracted from the images are descriptive for both, single
images and image sequences. A model based approach is applied to extract shape in-
formation and forms the basis for the extraction of textural information and motion
information. The assembled features are provided to train a support vector machine
that derives the facial expression currently visible. Our experimental evaluation demon-
strates that the classiﬁers are well able to handle this task and therefore proves that the
extracted features are higly descriptive. We demonstrated that on two different clas-
siﬁers, support vector machines and binary decision trees. However the benchmarked
database consists of only frontal view of faces. In the future, experiments will be per-
formed to test the capability of the system to work in real time environment.
References
1. Pantic, M., Rothkrantz, L.J.M.: Automatic analysis of facial expressions: The state of the art.
IEEE Transactions on Pattern Analysis and Machine Intelligence 22(12), 1424–1445 (2000)
2. Edwards, G.J., Taylor, C.J., Cootes, T.F.: Interpreting Face Images using Active Appearance
Models. In: Proceedings of International Conference on Automatic Face and Gesture Recog-
nition, pp. 300–305 (1998)
3. Wimmer, M., Riaz, Z., Mayer, C., Radig, B.: Recognizing Facial Expressions Using Model-
Based Image Interpretation. In: Advances in Human-Computer Interaction
4. Wimmer, M., Stulp, F., Tschechne, S., Radig, B.: Learning Robust Objective Functions for
Model Fitting in Image Understanding Applications. In: Proceedings of the 17th British Ma-
chine Vision Conference, pp. 1159–1168. BMVA, Edinburgh (2006)
5. Cootes, T.F., Taylor, C.J.: Active shape models – smart snakes. In: Proceedings of the 3rd
British Machine Vision Conference, pp. 266–275. Springer, Heidelberg (1992)
6. Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active Appearance Models. In: Burkhardt, H., Neu-
mann, B. (eds.) ECCV 1998. LNCS, vol. 1407, pp. 484–498. Springer, Heidelberg (1998)
7. Li, S.Z., Jain, A.K.: Handbook of Face recognition. Springer, Heidelberg (2005)
8. Kanade, T., Cohn, J.F., Tian, Y.: Comprehensive database for facial expression analysis.
In: Proceedings of Fourth IEEE International Conference on Automatic Face and Gesture
Recognition (FGR 2000), Grenoble, France, pp. 46–53 (2000)

Facial Expressions Recognition from Image Sequences
323
9. Ekman, P., Friesen, W.: The Facial Action Coding System: A Technique for The Measure-
ment of Facial Movement. Consulting Psychologists Press, San Francisco (1978)
10. Michel, P., Kaliouby, R.E.: Real time facial expression recognition in video using support
vector machines. In: Fifth International Conference on Multimodal Interfaces, Vancouver,
pp. 258–264 (2003)
11. Cohn, J., Zlochower, A., Lien, J.-J.J., Kanade, T.: Feature-point tracking by optical ﬂow
discriminates subtle differences in facial expression. In: Proceedings of the 3rd IEEE Inter-
national Conference on Automatic Face and Gesture Recognition, April 1998, pp. 396–401
(1998)
12. Kotsia, I., Pitaa, I.: Facial expression recognition in image sequences using geometric defor-
mation features and support vector machines. IEEE Transaction on Image Processing 16(1)
(2007)
13. Riaz, Z., et al.: A Model Based Approach for Expression Invariant Face Recognition. In: 3rd
International Conference on Biometrics, Italy (June 2009)
14. Ginneken, B., Frangi, A., Staal, J., Haar, B., Viergever, R.: Active shape model segmentation
with optimal features. IEEE Transactions on Medical Imaging 21(8), 924–933 (2002)
15. Romdhani, S.: Face Image Analysis using a Multiple Feature Fitting Strategy. Ph.D thesis,
University of Basel, Computer Science Department, Basel, CH (January 2005)

Czech Artiﬁcial Computerized Talking Head
George
Josef Chaloupka1 and Zdenek Chaloupka2
1 Institute of Information Technology, Technical University of Liberec,
Studentska 2, 461 17 Liberec, Czech Republic
josef.chaloupka@tul.cz
2 Institute of Photonics and Electronics, Czech Academy of Sciences, Czech Republic
chaloupka@ufe.cz
http://www.ite.tul.cz/speechlab/
Abstract. This contribution is about a computer-implemented Czech
speaking talking head called “George”. This talking head is based on a
fully parametric photo-realistic 3D model of human head. The creation
and development of this talking head is described here in detail. The
talking head George produces realistic animation of face, lips and jaw
movements synchronized with either synthetic speech from Czech text
to speech synthesis system. The potential applications with this artiﬁcial
talking head are described in the last part of this paper.
Keywords: Audio-visual speech synthesis, Czech talking head.
1
Introduction
Audio-visual speech synthesis belongs to the dynamically evolving research areas
of audio-visual speech processing and recognition. Artiﬁcial talking heads are
one of the interesting examples of audio-visual speech synthesis systems. The
model consists of an acoustic Text To Speech (TTS) synthesis system and of
a two-dimensional (2D) or three-dimensional (3D) model of human head. The
movements of lips, tongue, lower jaw and mimic muscles are animated in this
model. The models of artiﬁcial talking head exist for diﬀerent languages - for
English [1], French [2], Swedish [3], Finish [4] and so on. Two models of talking
heads have been developed for Czech language too. First model has been created
in the Department of Cybernetics in Pilsen [5] and second model (George) [6]
has been developed in our Laboratory of computer speech processing and this
model is described in this paper. It is a model of the third generation and it looks
like real human. The previous model of taking head called Chatter [7] (second
generation) was developed in the year 2003 and it was created completely in 3D
content creation suite (tool) therefore it looks like from cartoon.
A modiﬁcation of 3D artiﬁcial talking head Baldi [1] was our ﬁrst attempt
to create a Czech-speaking talking head [8] (ﬁrst generation). The 3D model of
talking head Baldi was created at University of Santa Cruz and it has been pos-
sible to use this model for non-commercial purposes and it has been distributed
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 324–330, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Czech Artiﬁcial Computerized Talking Head George
325
with source codes. We have used original English visemes from this model ac-
cording to similarity with Czech visemes and Czech TTS system was connected
to this model. A special test of comprehensibility of this modiﬁed talking head
was done for Czech native speaker [9]. The results from this test were that it
would be better to developed the talking directly for Czech language therefore
the Czech computerized talking heads Chatter and George have been developed
and created.
2
The Development of Artiﬁcial Computerized Talking
Head
The development of model of talking head is possible to divide into four tasks:
Firstly, it is necessary to have a good acoustic Text To Speech (TTS) synthesis
system. The 2D or 3D model of human (or animal, robot...) head is created in a
second task. The real data - video recordings of human speaker are provided and
processed thirdly and the animation of talking head is created from processed
video recordings in the last task.
2.1
The TTS System for Talking Head
The audio-visual system uses a Text-To-Speech interface called Epos. The Epos
system is a multilanguage client-server oriented program easily controlled through
Text-To-Speech Control Protocol (TTSCP) [10]. TTSCP is both human and ma-
chine readable communication protocol, remotely similar to the File Transfer Pro-
tocol. It is primarily designed to run atop TCP. Operations, needed for the text
synthesis (like transcribing, parsing, speciﬁc language rules application), could be
proceeded both separately or at once.
This makes Epos the best option for speech synthesis oriented applications. The
Epos system has several synthesizing methods based on diﬀerent algorithms like
Linear Predictive Coding (LPC) - excited by synthetic or residual signals, LPC
with codebook - vector quantization, Time Domain Pitch-Synchronous Over Lap
and Add (TD PSOLA) synthesis. Epos speech synthesis is highly adjustable - a
speech rate, F0 and a type of prosody modulation (language rules, neural net, LPC
smoothed model) could be set. The system also oﬀers a MBROLA-like synthesis,
which synthesize a text phone by phone and thus provides better speciﬁcation of
each phone. MBROLA contains information about a length and prosody points
[11], hence prosody of speech is much smoother and natural.
The last version of Epos supports MS Windows Speech Application Interface
(MS SAPI), which makes Epos highly user friendly. Speech synthesis that is
applied in the presented audio-visual system is based on TD PSOLA algorithm
synthesized phone by phone (MBROLA-like synthesis). This algorithm uses parts
of the speech recordings called triphones for speech synthesis. A triphone is com-
posed of a central phone and its corresponding previous and following phones.
Czech language contains about seventy thousand triphones, but the set of about
two thousand diﬀerent triphones is suﬃcient for an intelligible and acceptably

326
J. Chaloupka and Z. Chaloupka
natural speech. Triphones were originally synthesized as single units, thus prop-
erties of a central phone could not be set directly. Nowadays, triphones have
speciﬁc marks that show the position of the start/end of the central phone. This
allows to set exact length of each synthesized triphone, but brings problems
with some speech sounds like plosives. Changing of the speech rate consequently
increases/decreases the original length of the central phone, hence the synthesiz-
ing algorithm has to repeat/omit some segments. If the algorithm repeats/omits
an articulation part of the plosive sound, the produced speech will not sound
naturally. The articulation part of the plosive has to be marked separately. The
properties of the MBROLA-like synthesis is crucial for audio-video systems, be-
cause it allows to keep the time information about the phone borders and thus
precise synchronization between a video and audio signal is easily gained.
2.2
The 3D Model of Talking Head
The second generation of our 3D model of talking head Chatter was manually
created by using 3D tool. Data were two photos (from the front and from the
proﬁle) of human face but this model didn’t look like real human (ﬁg. 1) therefore
we wanted to created better model.
Fig. 1. 3D model of artiﬁcial talking head Chatter (left) and new photo - realistic 3D
model of artiﬁcial talking head George
It is possible to use a 3D scanner or some methods for reconstruction of 3D
object from two diﬀerent images of this object if we want to obtain real model
of human head. We have used software Faceworx (http://www.looxis.com) for
creation of model of our talking head. This software makes it possible to create
3D model of human head from two images (from the front and from the proﬁle) of
real human head. A creation of 3D model takes several minutes and texture of 3D
model of head is created automatically from images. Advantage of this solution
is that it is possible to create 3D models of head of some well known peoples
(politicians, actors, celebrities...) if we have the photo of their heads from the

Czech Artiﬁcial Computerized Talking Head George
327
front and from the proﬁle. The 3D model from Faceworx software is improved by
the 3D toolkit Blender (http://www.blender.org/) where the eyes, jawbones and
tongue are added and this model is imported to our system which uses DirectX
for animation of 3D model of talking head. We used OpenGL in previous version
of our talking head Chatter but they were some problems with OpenGL under
operation system Windows Vista therefore we have chosen DirectX for our new
talking head.
2.3
The Audio-Visual Speech Data and Processing
An audio-visual speech database was created for controlling the talking head.
One human speaker was camera scanned and this speaker narrated several tens
phonetic-rich sentences. We wanted to create 3D model of talking head therefore
it was necessary to use two video cameras which scan human face from two
diﬀerent positions. A bend mirror was placed next to human head in original
video recordings therefore human head was scanned in one video picture from
the font and from the proﬁle but the problem was that a part of the image with
the head from the proﬁle was more or less distorted. Methods and algorithms
for image geometric transformation were used for solving this problem.
Our second generation of talking head Chatter was created with the help
of these video recordings. New audio-visual database where two video cameras
were used was created for the new generation of talking head George. An acous-
tic signal (0.5 s, 440Hz) was played in the beginning of each sentence therefore
synchronization between these two cameras was done with the help of this syn-
chronizing acoustic pulse. Audio signal and visual stream (video pictures) was
separated from each video recording. The time boundaries of single phonemes
was found in the audio signal [12] in the next step. The hypothesis was that
visems correspond to single phonemes. This assumption was just taken as an
technical simpliﬁcation. The video pictures were separated to visems group ac-
cording to these time boundaries. The model of our talking had thus represents a
parametric approach where single parameters. 11 control facial parameters have
been used like in [13].
2.4
The Animation of Talking Head
The principle of animation of talking head George is the following: Some text is
realized by our text to speech synthesis system. The output from TTS system
are an audio signal, a chain of phonemes, and a set of time boundaries of single
phonemes. The phonemes are remapped to visemes and the single features for
deformation of talking head are prepared from visemes. Audio signal is played
and timer for animation is started in next step. The deformation parameters are
interpolated and changed in the time according to time boundaries which were
obtained from our TTS system.
Small moves of eyes and eyebrow are generated too in the model of talking
head and the model winks sometimes. A time period (from 2 to 5 second) of wink
is controlled by random generator. The moves of eye, eyebrow and wink were
added that the model of talking head has been more “natural”. The real moves

328
J. Chaloupka and Z. Chaloupka
of tongue are not solved because it is relatively diﬃcult to create recordings of
tongue moves from speech pronunciation.
There exist two ways how to create tongue moves: The tongue moves are
created manually by some rules for training of pronunciation or some technical
methods are used in second way - several sensors are placed in tongue (contact
methods) or a device with some kind of ray is used (contactless methods). We
would like to use ﬁrst way for creation of tongue movements in the real features.
The tongue is moved similar like bottom jawbone in our talking head at present.
3
The Applications with Talking Head
The previous talking head Chatter was connected to our graphic platform of
voice interaction systems “Lotos” [14] therefore it was relatively easy to connect
this system Lotos to our new version of talking head George. System Lotos
was developed for easy-creation of dialogs between a human and a computer.
It is possible to created very complex dialogs with the help of this system. The
dialog is created by a computer mouse actions from diﬀerent graphic blocks for
audio-visual TTS, speech recognition, question and answer, recognition settings,
switch, jump, time delay, expression and database query. Any dialog is created
from these eight diﬀerent blocks and it is very easy to change the properties of
these blocks.
The system Lotos was developed mainly for creation of telephone information
systems based on dialog between a human and a computer therefore we have
used (in 2001) this system for creation of telephone information system which
contains important information about our city Liberec (Infocity Liberec). A
calling person speciﬁes his (her) query and the computer with the help of dialog
system give the information (by TTS) about public transport, sport, culture and
so on in the city Liberec. This information dialog system with Infocity Liberec
was partly changed and it is connected to our talking head at present so that
information kiosk with artiﬁcial communication agent has been created.
Our next application with talking head was Chat with virtual character [15]
where talking head, continuous speech recognition and artiﬁcial intelligence were
used. Well-known (in middle Europe) novel person good soldier Svejk was repre-
sented like virtual character. The dialog between a human and the virtual per-
son (Svejk) were controlled with the help of key-words from continuous speech
recognition where system chosen answers or utterances according to recognized
key-words. The virtual novel character Svejk was humorous person therefore the
virtual person tells about something funny if the question was not well recog-
nized so that the dialog was relatively natural. It is not so serious application
but it is very good for presentation of automatic continuous speech recognition,
artiﬁcial intelligence and audio-visual speech synthesis to general public.
This system was developed in the year 2002 but we have fundamentally better
continuous speech recognizer now and it is relatively easy to change the character
of our talking head - it takes less than 30 minutes. It is only necessary to have
the photo of some human face from the front and the proﬁle. Therefore we are

Czech Artiﬁcial Computerized Talking Head George
329
Fig. 2. Graphic Platform for designing and developing practical voice interaction sys-
tems Lotos
preparing a new system for chat with virtual character - with politics, scientists
or celebrities and we are developing new universal dialog system based on key-
words for these purposes.
4
Conclusion
Czech artiﬁcial computerized talking head George has been presented in this
paper. It is second generation of talking head which was developed in our lab-
oratory SpeechLab. The animation of this talking head is based on the same
principle like previous version but 3D model of talking head looks more natural
now because near-photo realistic model has been created. It is relatively easy to
change this 3D model for diﬀerent people. The creation of new character takes
from 10 to 15 minutes therefore we plan to create some well-known people (pol-
itics, scientists, actors ...). New Czech TTS was used in new model of talking
head. This TTS contains male and female voice so we would like to create female
character in the near future. We would like to create a test for comparison of
the ﬁrst and the second generation of our talking heads in the near future.
Acknowledgments
The research was supported partly by the Grant Agency of the Czech Academy
of Sciences (grant no.1QS108040569) and by the grant MMT OC8010 (project
COST 2102).

330
J. Chaloupka and Z. Chaloupka
References
1. Cole, et al.: Intelligent Animated Agents for Interactive Language Training. In:
Proc. of STiLL 1998, Stockholm, pp. 163–166 (1998)
2. Benoit, C., Le Goﬀ, B.: Audio-visual speech synthesis from French text: Eight years
of models, design and evaluation at the ICP. Speech Communication 26, 117–129
(1998)
3. Beskow, J., Edlund, J., Nordstrand, M.: A model for multi-modal dialogue system
output applied to an animated talking head. In: Minker, W., B¨uhler, D., Dybkjaer,
L. (eds.) Spoken Multimodal Human-Computer Dialogue in Mobile Environments,
Text, Speech and Language Technology, pp. 93–113. Kluwer Academic Publishers,
Dordrecht (2005)
4. Frydrych, M., Ktsyri, J., Dobsik, M., Sams, M.: Toolkit for animation of Finnish
talking head. In: Proceedings of International Conference on Auditory-Visual
Speech Processing AVSP 2003, St. Joroiz, France, September 4-7, pp. 199–204
(2003)
5. Krnoul, Z., Zelezny, M.: Realistic face animation for a Czech Talking Head. In:
Sojka, P., Kopeˇcek, I., Pala, K. (eds.) TSD 2004. LNCS, vol. 3206, pp. 603–610.
Springer, Heidelberg (2004)
6. Chaloupka, J.: New Version of Czech Computerized Talking Head. In: 17th Czech-
German Workshop Speech Processing, Prague, Czech Republic, September 2007,
pp. 173–176 (2007) ISBN 978-80-86269-00-9
7. Chaloupka, J.: The Czech Computerized Talking Head “Chatter”. In: Proc. of
7th World Multiconference on Systemics, Cybernetics and Informatics, USA, July
2003, vol. IV, pp. 320–323 (2003) ISBN 980-6560-01-9
8. Chaloupka, J., Nouza, J., Pribil, J.: Czech-Speaking Artiﬁcial Face. In: Proc. of
Biosignal 2002, Brno, June 2002, pp. 403–405 (2002) ISBN 80-214-2120-7
9. Chaloupka, J.: Talking Head: How Much Comprehensible Is It? In: Proc. of Ra-
dioelektronika 2002, Bratislava, May 2002, pp. 202–205 (2002) ISBN 80-227-1700-2
10. Bozkurt, B., Bagein, M., Dutoit, T.: From MBROLA to NU-MBROLA (cited,
11-20-2007), http://www.multitel.be/TTS/Papers/bagein2001.pdf
11. Chaloupka, Z., Horak, P.: Current State of the Czech TTS System Epos. In: 17th
Czech-German Workshop (2007) ISBN 978-80-86269-00-9
12. Nouza, J., Myslivec, M.: Methods and Application of Phonetic Label Alignment.
Speech Processing Tasks. Radioengineering 9(4), 1–7 (2000)
13. Cohen, M.M., Massaroa, D.W., Clark, R.: Training a talking head. In: V Fourth
IEEE International Conference on Multimodal Interfaces (ICMI 2002), Pittsburgh,
Pennsylvania, str. 499 (2002)
14. Nouza, T., Nouza, J.: Graphic Platform for designing and developing practical
voice interaction systems. In: Proc. of Eurospeech 2001, Aalborg, September 2001,
pp. 1287–1290 (2001) ISBN 87-90834-09-7, ISSN 1018-4074
15. Nouza, J., Kolar, P., Chaloupka, J.: Voice Chat with a Virtual Character: The
Good Soldier Svejk Case Project. In: Sojka, P., Kopeˇcek, I., Pala, K. (eds.) TSD
2002. LNCS, vol. 2448, pp. 445–448. Springer, Heidelberg (2002)

An Investigation into Audiovisual Speech
Correlation in Reverberant Noisy Environments
Simone Cifani2, Andrew Abel1, Amir Hussain1, Stefano Squartini2,
and Francesco Piazza2
1 Dept. of Computing Science, University of Stirling, Scotland, UK
{aka,ahu}@cs.stir.ac.uk
2 3MediaLabs, DIBET, Universit`a Politecnica delle Marche, Ancona, Italy
{s.cifani,s.squartini,f.piazza}@univpm.it
Abstract. As evidence of a link between the various human commu-
nication production domains has become more prominent in the last
decade, the ﬁeld of multimodal speech processing has undergone signiﬁ-
cant expansion. Many diﬀerent specialised processing methods have been
developed to attempt to analyze and utilize the complex relationship be-
tween multimodal data streams. This work uses information extracted
from an audiovisual corpus to investigate and assess the correlation be-
tween audio and visual features in speech. A number of diﬀerent feature
extraction techniques are assessed, with the intention of identifying the
visual technique that maximizes the audiovisual correlation. Addition-
ally, this paper aims to demonstrate that a noisy and reverberant audio
environment reduces the degree of audiovisual correlation, and that the
application of a beamformer remedies this. Experimental results, carried
out in a synthetic scenario, conﬁrm the positive impact of beamform-
ing not only for improving the audio-visual correlation but also in a
complete audio-visual speech enhancement scheme. Thus, this work in-
evitably highlights an important aspect for the development of future
promising bimodal speech enhancement systems.
1
Introduction and Background
The bimodal nature of human speech is long established. This bimodality in-
volves production as well as perception: indeed, speech is produced by the vi-
bration of the vocal cord and the conﬁguration of the vocal tract that is composed
of articulatory organs. The well-known McGurk eﬀect empirically demonstrates
that speech perception is inherently bimodal [1]. The high degree of correlation
between audio and visual speech has been deeply investigated in literature [2],
[3], [4], showing that facial measures provide enough information to reasonably
estimate related speech acoustics. This plays a central role in the development
of human-computer interfaces (HCI), especially in automatic speech recognition
(ASR) where traditional algorithms can be extended to deal with audio-visual
features [5]. However, development speciﬁcally into multimodal speech enhance-
ment is much more limited. Since pioneering work by Girin et al. [6], which uses
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 331–343, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

332
S. Cifani et al.
visual information to estimate the ”cleaned” spectral parameters, very little ad-
ditional research has been published. Recently, Almajai et al. [4] investigated
the degree of correlation between a set of audio (MFCC and the ﬁrst four for-
mant frequencies) and visual (2D-DCT, Cross-DCT, Active Appearance Models
(AAM)) features, showing that MFCC and AAM (or 2D-DCT) provides the best
results. Later this correlation was exploited to devise a visually-derived Wiener
ﬁlter for speech enhancement, which estimates the PSD of clean audio from the
visual features using an HMM-based approach [7]. Up to authors knowledge,
the correlation behavior when dealing with noisy audio has never been inves-
tigated in literature. However, this seems to be an interesting study, especially
in conjunction with beamforming applications. In fact, these constitute a more
general basis for speech enhancement with respect to traditional single-channel
techniques, operative with diﬀuse as well as directional additive noise and in
presence of reverberation. Moreover, the use of source localization algorithms,
based on beamforming criteria, seems to be compelled in real applications to per-
form camera steering. This paper describes work showing audiovisual correlation
between audio and visual features in reverberant noisy environments. A selection
of sentences from the VidTIMIT corpus are tested using MFCCs and two forms
of DCT. The diﬀerence in correlation when using the two visual techniques is
assessed in order to corroborate the work by Almajai et al. detailed above. Re-
sults are similar to other published work, and ﬁnd that the ideal dimensionality
of the audio vector is around eight to twelve, and the ideal visual vector size
is around thirty to forty. A range of noises, including additive machine noises
and convolutive speech babble, are added to the corpus in order to investigate
the performance of a beamformer with regard to multimodal correlation in noisy
reverberant environments. The diﬀerence between noisy speech and speech en-
hanced with a beamformer is assessed. It is found that speech enhanced with
a beamformer always leads to a higher audiovisual correlation between the en-
hanced speech and the visual information, than between noisy speech and facial
information. However it is found that this diﬀerence in correlation is noticeably
more pronounced in additive noise than in convolutive mixtures. Future related
research is also outlined in this paper.
2
Techniques And Corpus
2.1
Audio Feature Extraction
In this work we consider the well-known MFCC (Mel Frequency Cepstral Coeﬃ-
cients) as the unique audio feature. Among the numerous implementations of the
MFCC, we choose the one provided in the framework of the Cambridge Hidden
Markov Models (HMM) Toolkit [8], known as HTK. The proposed implemen-
tation, namely the HTK MFCC FB-24, assumes a ﬁlterbank of I = 24 ﬁlters,
as recommended by Young for speech bandwidth of 8 kHz. The relationship
between linear and Mel frequency is given by,
ˆfmel = 2595 · log10

1 + flin
700

(1)

An Investigation into Audiovisual Speech Correlation
333
In this implementation, the limits of the frequency range are the parameters
that deﬁne the basis for the ﬁlter bank design. Speciﬁcally, the unit interval Δ ˆf
is determined by the lower and the higher boundaries of the frequency range of
the entire ﬁlter bank, ˆfhigh and ˆflow, as follows,
Δ ˆf =
ˆfhigh −ˆflow
I + 1
(2)
The centre frequency ˆfci of the ith ﬁlter is given by,
ˆfci = ˆflow + iΔ ˆf,
i = 1, ..., I −1
(3)
where I is the total number of ﬁlters in the ﬁlterbank. The conversion of the
centre frequencies of the ﬁlters to linear frequency (Hz) is given by,
ˆfci = 700 ·

10
ˆ
fci/2595 −1

(4)
The shape of the ith triangular ﬁlter is deﬁned by
Hi(k) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
k < fbi−1
k−fbi−1
fbi −fbi−1 fbi−1 ≤k ≤fbi
i = 1, ..., I
fbi+1 −k
fbi+1−fbi fbi ≤k ≤fbi+1
0
k > fbi+1
(5)
where fbi are the boundary points of the ﬁlters and k = 1, ..., K corresponds to
the kth coeﬃcient of the K-points DFT. The boundary points fbi are expressed in
terms of position, which depends on the sampling frequency Fs and the number
of points K in the DFT:
fbi =
 K
Fs
· fci

(6)
For computing the MFCC parameters, the magnitude spectrum |X(k)| of the
input signal x(t) acts as input for the ﬁlterbank Hi(k). Next, the ﬁlterbank
output is logarithmically compressed,
Xi = ln

K−1

k=0
|X(k)| · Hi(k)

(7)
The MFCC parameters are obtained by means of DCT on the log-energy output
Xi:
Fa(j) =
I

i=1
Xi · cos

j · (i −1/2) · π
I

j = 1, ..., J
(8)
where J is the number of cepstral coeﬃcients which are computed, usually
(J < I).

334
S. Cifani et al.
2.2
Visual Feature Extraction
This paper focuses on using two forms of Discrete Cosine Transform (DCT)
for visual feature extraction. The DCT was originally developed in 1974 by
Ahmed, Natarajan, and Rao [9], and is a close relative of the discrete Fourier
transform (DFT). This was extended for application with image compression
by Chen and Pratt [10]. The one-dimensional DCT is capable of processing
one-dimensional signals such as speech waveforms. However, for analysis of two
dimensional signals such as images, a 2D-DCT version is required. For an U x
V matrix P of pixel intensities, the 2D-DCT is computed in a simple way: the
1D-DCT is applied to each row of P and then to each column of the result.
Thus, the transform of P is given by,
Cm,n = WmWn
U−1

u=0
V −1

v=0
Pu,v cos

m(2u + 1) · π
2U

cos

n(2v + 1) · π
2V

(9)
with 0 ≤m ≤U −1, 0 ≤n ≤V −1,
Wn =

1/V
if n = 0

2/V
otherwise and Wm =

1/U if m = 0

2/U otherwise
(10)
Since the 2D-DCT can be computed by applying 1D transforms separately to the
rows and columns, we say that the 2D-DCT is separable in the two dimensions.
To be used as a feature vector, the 2D-DCT matrix C is then vectorized in a
zigzag order to get the Q-dimensional visual features vector Fv(q). An alterna-
tive approach to consider consists of taking only the central horizontal row and
vertical column of the matrix of pixel intensities p and then applying 1D-DCT
to each vector. This is a much simpler method, and arguably contains adequate
information for lipreading because the former captures the width of the mouth
while the latter capture the height of the mouth. The two vectors are truncated
and concatenated to get the visual features vector Fv(q).
2.3
Beamforming
Multiple microphone techniques such as beamforming can improve speech quality
and intelligibility by exploiting the spatial diversity of speech and noise sources.
Upon these techniques, one can diﬀerentiate between ﬁxed and adaptive beam-
formers. The former combine the noisy signals by a time-invariant ﬁlter-and-sum
operation, the latter combine the spatial focusing of ﬁxed beamformers with
adaptive noise suppression, such that they are able to adapt to changing acous-
tic environments and generally exhibit a better noise reduction performance
than ﬁxed beamformers [11]. The generalized sidelobe canceller is probably the
most widely used structure for adaptive beamformers and a number of algo-
rithms have been developed based on it [12]. Among them, the general transfer
function generalized sidelobe canceler (TF-GSC) suggested by Gannot et al. [13],

An Investigation into Audiovisual Speech Correlation
335
has shown impressive noise reduction abilities in a directional noise ﬁeld, while
maintaining low speech distortion. To summarise this concept, assume M mi-
crophone signals, z1(t), · · · , zM(t) record a source x(t) and M uncorrelated noise
interfering signals d1(t), · · · , dM(t). Thus, the mth microphone signal is given by,
Zm(t) = am(t) ∗x(t) + dm(t), 1 ≤m ≤M
(11)
where am(t) is the impulse response of the mth sensor to the desired source, and
∗denotes convolution. In the frequency domain convolutions become multipli-
cations. Furthermore, since we are not interested in balancing the channels, we
redeﬁne the source so that the ﬁrst channel becomes unity. Hence, applying the
short-time Fourier Transform (STFT) to the above, we have,
Zm(k, l) = Am(k, l)X(k, l) + Dm(k, l), 1 ≤m ≤M
(12)
where k is the frequency bin index, and l the time-frame index. Thus, we have
a set of M equations that can be written in a compact matrix form as,
Z(k, l) = A(k, l)X(k, l) + D(k, l)
(13)
with,
Z(k, l) = [Z1(k, l)Z2(k, l)...ZM(k, l)]T
(14)
A(k, l) = [A1(k, l)Z2(k, l)...AM(k, l)]T
(15)
D(k, l) = [D1(k, l)D2(k, l)...DM(k, l)]T
(16)
The general GSC structure is composed of three main parts: a ﬁxed beamformer
(FBF) W(k) , a blocking matrix (BM) B(k) , and a multichannel adaptive noise
canceller (ANC) H(k, l) . The FBF is an array of weighting ﬁlters that suppresses
or enhances signals arriving from unwanted directions. The column of the BM
can be regarded as a set of spatial ﬁlters suppressing any component impinging
from the direction of the signal of interest, thus yielding M −1 reference noise
signals U(k, l) . These signals are used by the ANC to construct a noise signal to
be subtracted to the FBF output, attempting to eliminate stationary noise that
pass through the ﬁxed beamformer, yielding an enhanced signal Y (k, l). Thus,
the beamformer (BF) output Y (k, l) can be written as,
Y (k, l) = YF BF (k, l) −YNC(k, l)
(17)
where,
YF BF (k, l) = WH(k, l)Z(k, l)
(18)
YNC(k, l) = HH(k, l)U(k, l)
(19)
U(k, l) = BH(k, l)Z(k, l)
(20)
The FBF and BM matrices are constructed using the ATF ratios as follows,
W(k, l) =
A(k, l)
∥A(k, l)∥2
(21)

336
S. Cifani et al.
B(k, l) =
⎡
⎢⎢⎢⎣
−A∗
2(k,l)
A∗
1(k,l) −A∗
3(k,l)
A∗
1(k,l) ... −A∗
M(k,l)
A∗
1(k,l)
1
0
...
0
0
1
...
0
0
0
. . .
1
⎤
⎥⎥⎥⎦
(22)
Note that the computation of both W(k) and B(k) requires the knowledge of
the ATFs ratios. In this work, for simplicity, we directly transform the true
impulse responses am(t) in the frequency domain. The BF output Y (k, l) can be
transformed back to the time domain through an inverse STFT (ISTFT) and
then fed into the previously described MFCC ﬁlterbank.
3
Experimentation Results
3.1
Comparison of Visual Feature Extraction Techniques
This work has chosen two visual feature extraction techniques to compare, 2D-
DCT and Cross-DCT. These are described in more detail in section 2. The
diﬀerence in performance of these techniques is assessed in this section, with the
goal of identifying the technique that returns the highest audiovisual correlation.
In order to accomplish this, multiple linear regression is used. This is a multi-
variate approach that assesses the relationship between audio and visual vectors
[14]. In the present work, experiments have been carried out by using an audio
frame of 25ms and a video frame of 100ms. This implies that the same visual
features are used for four consecutive audio frames. For a speech sentence, each
component Fa(l, j) of the audio feature vector is predicted by means of multiple
linear regression using the entire visual features vector Fv(l, q), where l is the
time-frame index. This approach mirrors that taken by Almajai [4], and means
that using Q + 1 regression coeﬃcients {bj,0, ..., bj,q, ..., bj,Q}, the jth component
of the audio features vector can be represented by the visual features vector
Fv(q) = [Fv (l, 0) , ..., Fv (q) , ..., Fv (Q −1)]:
ˆFa (l, j) = bj,0 + bj,1Fv (l, 0) + ... + bj,QFv (l, Q −1) + ϵl
(23)
with ϵl representing an error term. The multiple correlation between the jth
component of the audio features vector and the visual vector, calculated over L
frames, is given by R, and is found by calculating the squared value:
R (j)2 = 1 −
L
l=0

Fa (j) −ˆFa (j)
2
L
l=0

Fa (j) −¯Fa (j)
2
(24)
¯Fa (j) represents the mean of the jth component of the audio features vector.
Twelve sentences were chosen from the multimodal VidTIMIT corpus, and the
audio signal of each sentence was sampled at 8000Hz, and processed at 100fps.
This was converted into MFCCs with 6 components used for correlation analysis.
The matching visual signal for each sentence was recorded at 25fps, and was in-
terpolated to 100fps to match the input audio signal, before having a Cross-DCT

An Investigation into Audiovisual Speech Correlation
337
Fig. 1. Plot of audiovisual correlation for twelve sentences from the VidTIMIT corpus.
Diﬀerence between using 2D-DCT and Cross-DCT as visual features.
and 2D-DCT transform performed, with 30 components used. The correlation
of the MFCC component for each selected sentence to the appropriate match-
ing 2D-DCT vector is compared to the MFCC to Cross-DCT correlation of the
same sentence. This is shown in ﬁg.1. This graph plots matching sentence pairs
of audiovisual correlation when using the two diﬀerent methods of visual feature
DCT. The left side of ﬁg.1 shows the correlation found for each sentence when
using 2D-DCT. The right side shows the correlation found for each sentence
when using Cross-DCT. Matching sentences are linked by a line showing the dif-
ference in correlation. Fig.1 shows that the audiovisual correlation of a sentence
from the VidTIMIT corpus that is found when comparing MFCC correlation to
2D-DCT visual features is greater than for the equivalent sentence when using
Cross-DCT. This was an expected result. As described in section 2, 2D-DCT
makes use of all the visual information present in the mouth region of a speaker,
whereas Cross-DCT only takes a limited sample of the available visual infor-
mation. It should also be noted that the results shown in ﬁg.1 are lower than
those published by Almajai et al. [4]. This work makes use of a diﬀerent Corpus
(VidTIMIT), and there is a lot of background noise present in VidTIMIT, with
some sentences having a poor audio quality. This has the eﬀect of producing
lower levels of audiovisual correlation than would be found when using a cleaner
multimodal speech corpus.
3.2
Maximising Correlation
This section describes recently completed experiments carried out to investigate
the ideal audio and visual feature vector dimensionalities to use for performing
multimodal correlation analysis. In addition, the performance of the GSC beam-
former (as described in section 2) was also assessed. This was done by adding
white noise to sentences from the VidTIMIT corpus in order to produce noisy
speech with a SNR of -3dB, and making use of the beamformer to remove the
added noise and produce enhanced speech. For the experiments described in this
paper, four microphones were used. In order to ﬁnd the right combination of au-
dio and visual vectors that maximise correlation, the white noise was removed

338
S. Cifani et al.
with the beamformer to produce enhanced speech. The correlation of this en-
hanced speech, found by performing MLR regression as described in section 3.1,
when varying the audio (MFCC) and visual (DCT) vector sizes used is shown in
ﬁg.2. The correlation of a single enhanced sentence is shown in this graph, plot-
ted against an audio vector varying between a dimensionality of one and twenty
three, and a visual vector that varies in size between one and seventy. As can
be seen in ﬁg. 2.a, a very clear pattern can be seen. Increasing the visual vector
increases the correlation between MFCC(audio) and DCT (visual) vectors, and
reducing the size of the audio vector produces a similar eﬀect, peaking at very
high (70) visual and very low(3) audio vector dimensionalities. However, this is
a misleading result. Fig. 2.b shows the graph of the same sentence, but with the
correlation of the speech mixed with background white noise plotted. This noisy
speech has not been cleaned up with the beamformer and this results in a lower
level of correlation being found. This conﬁrms the eﬀectiveness of the beam-
former for cleaning speech and increasing correlation, as a diﬀerence between
noisy and enhanced audiovisual correlation is always found. However, a visual
comparison of the noisy and enhanced graphs show that they have a similar
shape, and where the enhanced correlation is very large, the noisy correlation is
also very large. Therefore, it becomes important to ﬁnd the audio and visual di-
mensionalities that maximise the diﬀerence between noisy and enhanced speech.
Both plots of Fig.2 show that with a very small visual vector, the diﬀerence in
audiovisual correlation is very small, and that an initial increase results in an
increased diﬀerence. However, this increase tails oﬀwhen the visual vector is in-
creased above thirty, with only a very small rate of increase is correlation found.
These results show that the ideal visual vector dimensionality to maximises the
diﬀerence in audiovisual correlation between noisy and enhanced speech is above
twenty ﬁve, and that there is no signiﬁcant gain to be achieved from increasing
this above thirty. Additionally, both graphs show that increasing the size of the
MFCC dimensionality results in a lower diﬀerence in audiovisual correlation and
that the highest diﬀerence is found with an audio vector size of less than ﬁve.
However, this is not a practical value to use. A very low MFCC dimensional-
ity does not contain adequate spectral information about the input speech to
be of practical use, and a compromise between maximising correlation and po-
tential practical application has to be found. Overall, it was decided that the
most suitable audio vector size was between the range of six and twelve. These
ﬁndings are very similar to those achieved by Almajai et al. [4]. Almajai et al.
performed similar experiments with a diﬀerent corpus and found that although
increasing the dimensionality of the visual vector initially led to a rapid increase
in audiovisual correlation, there was very little additional improvement in corre-
lation to be gained with the use of very large visual vectors for performing MLR
analysis. Also, Almajai et al. came to similar conclusions regarding appropriate
audio vector dimensionality. Their results show a slight drop in correlation as
the audio vector increases, and that a very low audio vector produces a very high
correlation. Furthermore, they also mentioned the need for a trade-oﬀbetween
high correlation and adequate spectral information. Although diﬀerent values

An Investigation into Audiovisual Speech Correlation
339
Fig. 2. MFCC correlation to 2D-DCT of noisy and enhanced speech for a single sen-
tence by a single speaker. MFCC dimensionality varies from 1 to 23 and visual vector
size varies from 1 to 70.
for correlation were found, thanks mainly to the use of a diﬀerent corpus; these
experiments produced results which matched the pattern of those published by
Almajai et al. quite closely.
3.3
Investigation of Noisy Environments
The previous section identiﬁed the audio and visual vector dimensionalities that
produced the greatest diﬀerence in correlation between noisy and enhanced
speech signals. These values were used to investigate the audiovisual correla-
tion in a range of reverberant noisy environments. As with the previous exper-
iments, multiple linear regression was used to measure audiovisual correlation,
as described in section 3.1. Based on the results of the maximisation correlation
experiments (section 3.2), it was decided to make use of an MFCC vector di-
mensionality of six, and a visual vector of thirty. To investigate, four types of
noise were added to a selection of sentences from the VidTIMIT corpus. Three
mechanical noises were chosen, white machine noise, ﬁltered pink noise, and real
world aircraft cockpit noise. Finally, an incoherent human babble mixture was
used to simulate a busy social environment. The results of these experiments are
shown in Table 1. Each row shows the mean and the variance of the audiovisual
correlation for eight sentences from the VidTIMIT corpus, corrupted with diﬀer-
ent noises, in the noisy and enhanced case. Table 1 shows a consistent and visibly
signiﬁcant diﬀerence between noisy and enhanced speech correlation in all four

340
S. Cifani et al.
Table 1. Comparison of audiovisual correlation in noisy and enhanced sentences from
VidTIMIT corpus
NOISE
MEAN
VARIANCE
noisy
enhanced
noisy
enhanced
White
0.191
0.321
0.002
0.007
Pink
0.178
0.324
0.002
0.007
F-16
0.194
0.323
0.002
0.007
Babble
0.255
0.336
0.006
0.005
types of noise. In every case, the enhanced audio vector for a sentence produces
a higher correlation with visual information than the noisy signal does. This was
an expected result, and conﬁrms the usefulness of the beamformer for cleaning
up an audio signal before audiovisual correlation analysis is performed. These ex-
periments also showed a much greater diﬀerence in correlation between noisy and
enhanced artiﬁcial machine noise and speech mixtures (white, pink and aircraft
noise), than for speech and incoherent babble mixtures. This smaller diﬀerence
between noisy and enhanced values in environments containing signiﬁcant levels
of background speech may be explained by the relative similarity of sentences
from the corpus and the background speech mixture. In the three machine noise
graphs, it can be seen that there is a much larger drop in correlation due to
the noisy audiovisual correlation being much lower than enhanced. However, in
the case of babble, while the enhanced correlation is similar to other results,
the noisy speech correlation is much higher for all sentences, demonstrating a
lower diﬀerence between noisy and enhanced audiovisual speech correlation. It
is possible that the similarity between the speech and noise is causing confusion
with regard to visual information, and so an inaccurate correlation is produced.
This warrants further investigation with more detailed experimentation and the
use of intelligibility testing in order to validate this hypothesis.
4
Future Developments
4.1
Additional Corpus Experimentation
One signiﬁcant feature of these results is that while correlation between audio
and visual features has been found, the values are lower than what might have
been expected. It is hypothesised that this is due to VidTIMIT containing sig-
niﬁcant levels of background noise, which distort the speech signal and lead to
lower correlation values. Future development will investigate other corpora such
as the Grid corpus [15], which contains lower levels of background noise, in order
to check this hypothesis.
4.2
Active Appearance Models
This work has made use of Cross-DCT and 2D-DCT for visual feature extrac-
tion, and can be improved by comparing these results with those produced by

An Investigation into Audiovisual Speech Correlation
341
using Active Appearance Models. AAMs were originally developed by Cootes
et al. [16], and create models of visual features by making use of shape and tex-
ture information. This makes them suitable for tasks such as the generation of
appearance parameters for speech processing. Existing work [7] has found that
AAMs are suitable for use as a visual feature extraction technique. The gen-
eralisation capabilities of facial models will be assessed, in addition to ﬁnding
the ideal audio and visual vector sizes for maximising multimodal correlation,
and comparing these results to those reported in this paper with the Cross and
2D DCT techniques. Preliminary investigation has conﬁrmed that AAMs can be
successfully used and this work has potential for future expansion.
4.3
Speech Segmentation
One important requirement of a multimodal speech enhancement system is to
implement appropriate speech segmentation. In laboratory work, pre-recorded
sentences are often used as input, but in a real time system, speech is continuous
and needs to be automatically segmented. Various methods have been developed
such as dividing by phoneme or vowels [17], [18], and work by Almajai et al.
[4] showed increased audiovisual correlation when hand annotated individual
phonemes were used for comparison, rather than using complete sentences. It
is intended to carry out experiments with key events in speech such as stressed
vowels and syllables, and assess audiovisual correlation. It is hoped that the
result of these experiments can be used to develop software that will function
as one component of a full speech enhancement system, by automating the real
time segmentation of speech.
4.4
Audiovisual Speech Enhancement System
The long term aim of this research is to develop a useful and eﬀective real time
implementation of an audiovisual speech enhancement system. Current systems
such as those developed by Barker and Shao [19] and Rivet et al. [20] are fo-
cused primarily on research, and lack real world implementation potential. Also,
research such as that published by Hazen el al. [21] and other developments
reviewed by Potamianos et al. [5] are focused on automatic speech recognition,
rather than intelligibility and enhancement. It is hoped that a system can be
developed to show the practical feasibility of audiovisual speech enhancement.
It is expected that this system will make use of the feature extraction methods
described in this paper, and that advanced techniques such as beamforming, in-
telligent automated speech segmentation, camera steering and facial tracking will
converge on it to produce more intelligible human speech. The aforementioned
system is intended to extend the recently proposed audio-based multichannel
speech enhancement system [22].
5
Conclusion
This paper has presented work that conﬁrms the existence of audiovisual
correlation between audio and visual features in a range of reverberant noisy

342
S. Cifani et al.
environments. MFCCs were chosen to represent the audio features, and 2D-DCT
and Cross-DCT were both tested as potential visual feature extraction techniques.
A number of sentences from the VidTIMIT corpus were compared, and in every
testedcase,2D-DCTproduceda slightlyhigher audiovisualcorrelationthanCross-
DCT. A range of noises were also added to sentences from the corpus in order to
test the diﬀerence in correlation between noisy speech and cleaned up speech that
was enhanced with a beamformer. It was found that the beamformer cleaned up the
noisy signal successfully and led to a greater diﬀerence in correlation in all cases.
The ideal audio and visual vector dimensionalities that maximised the diﬀerence
in bimodal correlation between noisy and clean audio information and visual data
were also found. These were found to be in the range of 8 to 12 for the audio di-
mensionality, and 30-40 for the visual vector size. However, it was found that the
diﬀerence in audiovisual correlation was much larger when additive noise such as
white noise or aircraft cockpit noise was added to the corpus, than when convolu-
tive noise, such as human speech babble was added. The addition of speech led to
a much higher correlation in noisy speech, suggesting that audiovisual speech en-
hancement using a beamformer is potentially much more feasible in environments
containing signiﬁcant levels of additive background noise than in those contain-
ing convolutive speech mixtures. This work validates results published by [4], as
it used a diﬀerent corpus but similar experiments to ﬁnd the ideal visual and au-
dio vector dimensionalities produced similar values to existing work. The overall
correlation levels were found to be lower than other work, but it was hypothesised
that this was due to the level of background noise in the chosen corpus. This work
is expected to form the basis of future audiovisual speech enhancement work, with
planned future developments including additional corpus investigation, using ad-
ditional visual techniques, and speech segmentation.
Acknowledgements
This work was partly funded with the aid of a COST 2102 Short Term Scientiﬁc
Mission, and also with a Departmental Scholarship from the University of Stirling.
References
1. McGurk, H., MacDonald, J.: Hearing lips and seeing voices. Nature 264, 746–748
(1976)
2. Yehia, H., Rubin, P., Vatikiotis Bateson, E.: Quantitative association of vocal tract
and facial behavior. Speech Communication 26(1), 23–43 (1998)
3. Barker, J.P., Berthommier, F.: Evidence of correlation between acoustic and visual
features of speech. In: ICPhS 1999, San Francisco (August 1999)
4. Almajai, I., Milner, B.: Maximising Audio-Visual Speech Correlation. Accepted for
AVSP 2007, paper P16 (2007)
5. Potamianos, G., Neti, C., Gravier, G., Garg, A., Senior, A.W.: Recent Advances
in the Automatic Recognition of Audiovisual Speech. Proceedings - IEEE 91, part
9, 1306–1326 (2003)

An Investigation into Audiovisual Speech Correlation
343
6. Girin, L., Feng, G., Schwartz, J.L.: Fusion of auditory and visual information for
noisy speech enhancement: a preliminary study of vowel transition. In: ICASSP
1998, Seattle, WA, USA (1998)
7. Almajai, I., Milner, B., Darch, J., Vaseghi, S.: Visually-Derived Wiener Filters for
Speech Enhancement. In: ICASSP 2007, vol. 4, pp. IV-585–IV-588 (2007)
8. Young, S.J., Odell, J., Ollason, D., Valtchev, V., Woodland, P.: The HTK Book.
Version 2.1 Department of Engineering. Cambridge University, UK (1995)
9. Ahmed, N., Natarajan, T., Rao, K.R.: On image processing and a discrete cosine
transform. IEEE Transactions on Computers C-23(1), 90–93 (1974)
10. Chen, W.H., Pratt, W.K.: Scene adaptive coder. IEEE Transactions on Commu-
nications 32(3), 225–232 (1984)
11. Brandstein, M.S., Ward, D.B.: Microphone Arrays. Springer, New York (2001)
12. Griﬃths, L.J., Jim, C.W.: An alternative approach to linearly constrained adaptive
beamforming. IEEE Trans. Antennas Propagat. AP-30, 27–34 (1982)
13. Gannot, S., Burshtein, D., Weinstein, E.: Signal enhancement using beamforming
and nonstationarity with applications to speech. IEEE Trans. Signal Processing 49,
1614–1626 (2001)
14. Chatterjee, S., Hadi, A.S., Price, B.: Regression analysis by example. John Wiley
and Sons, Canada (2000)
15. Cooke, M., Barker, J., Cunningham, S., Shao, X.: An audio-visual corpus for speech
perception and automatic speech recognition. J. Acoust. Soc. Amer. 120(5), 2421–
2424 (2006)
16. Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active Appearance Models. IEEE Trans.
on Pattern Analysis and Machine Intelligence 23(6), 681–685 (2001)
17. Ringeval, F., Chetouani, M.: A Vowel Based Approach for Acted Emotion Recog-
nition. In: Interspeech 2008 (2008)
18. Wang, D., Lu, L., Zhang, H.J.: Speech Segmentation without Speech Recognition.
In: ICASSP 2003, vol. 1, pp. 468–471 (2003)
19. Barker, J., Shao, X.: Audio-Visual Speech Fragment Decoding. Accepted for AVSP
2007, paper L5-2 (2007)
20. Rivet, B., Girin, L., Jutten, C.: Mixing Audiovisual Speech Processing and Blind
Source Separation for the Extraction of Speech Signals From Convolutive Mixtures.
IEEE Trans. on Audio, Speech, and Lang. Processing 15(1), 96–108 (2007)
21. Hazen, J.T., Saenko, K., La, C.H., Glass, J.R.: A Segment Based Audio-Visual
Speech Recognizer: Data Collection, Development, and Initial Experiments. In:
ICMI 2004: Proceedings of the 6th international conference on Multimodal inter-
faces, pp. 235–242 (2004)
22. Hussain, A., Cifani, S., Squartini, S., Piazza, F., Durrani, T.: A Novel Psychoa-
coustically Motivated Multichannel Speech Enhancement System. In: Esposito,
A., Faundez-Zanuy, M., Keller, E., Marinaro, M. (eds.) COST Action 2102. LNCS,
vol. 4775, pp. 190–199. Springer, Heidelberg (2007)

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 344–355, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Articulatory Speech Re-synthesis: Profiting from Natural 
Acoustic Speech Data 
Dominik Bauer, Jim Kannampuzha, and Bernd J. Kröger 
Department of Phoniatrics, Pedaudiology, and Communication Disorders, 
University Hospital Aachen and RWTH Aachen University, Aachen, Germany 
{dobauer,jkannampuzha,bkroeger}@ukaachen.de 
Abstract. The quality of static phones (e.g. vowels, fricatives, nasals, laterals) 
generated by articulatory speech synthesizers has reached a high level in the last 
years. Our goal is to expand this high quality to dynamic speech, i.e. whole 
syllables, words, and utterances by re-synthesizing natural acoustic speech data. 
Re-synthesis means that vocal tract action units or articulatory gestures, 
describing the succession of speech movements, are adapted spatio-temporally 
with respect to a natural speech signal produced by a natural “model speaker” of 
Standard German. This adaptation is performed using the software tool SAGA 
(Sound and Articulatory Gesture Alignment) that is currently under development 
in our lab. The resulting action unit scores are stored in a database and serve as 
input for our articulatory speech synthesizer. This technique is designed to be the 
basis for a unit selection articulatory speech synthesis in the future. 
Keywords: speech, articulatory speech synthesis, articulation, re-synthesis, vocal 
tract action units. 
1   Introduction 
Articulatory speech synthesizers are able to generate an acoustic speech signal from an 
articulatory description. (Kröger 1993, Birkholz 2005, Birkholz et al. 2007, Engwall 
2005, Badin et al. 2002). Our articulatory speech synthesizer (Birkholz et al. 2007, 
Kröger and Birkholz 2007) is controlled by a set of temporally coordinated vocal tract 
actions (action-based control concept) where the complete acticulatory control 
information is stored in the “action unit score”. Typical examples for vocal tract 
actions are bilabial closing actions, vocalic tract forming actions, consonantal closing 
actions, glottal opening and velopharyngeal (or velic) opening actions (Kröger and 
Birkholz 2007). If it is the goal to integrate an articulatory speech synthesizer into a 
TTS-system the system must be able to generate vocal tract action scores 
automatically. In our TTS approach the dual-route phonetic encoding idea (Levelt and 
Wheeldon 1994 and Levelt 1999) is integrated. It says that action scores for frequent 
syllables are stored as one object, whereas the action scores for non-frequent syllables 
are generated by rule. The complete TTS-system would then consist of a database, 
where all articulatory action scores of frequent syllables are saved and it will also have 
a rule-based mechanism to create non-frequent syllables not included in the database. 
The rules in addition have to account for prosodic and paralinguistic variation. 

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
345 
The extraction of articulatory movement parameters from the acoustic speech 
signal is often solved in a speaker-dependent way by using acoustic-to-articulatory 
inversion procedures (e.g. Dang and Honda 2002). In our approach many spatio-
temporal parameters of vocal tract action units are specified with respect to speaker-
independent mean values stemming from articulatory measurement data from 
speakers of different Indo-European languages (Draper et al. 1959, Moll and Daniloff 
1971, Löfqvist and Yoshioka 1980, Yoshioka et al. 1981, Adams et al. 1993, Löfqvist 
and Gracco 1997, Wrench 1999, Löfquist 2005, Birkholz et al. 2007, Deterding and 
Nolan 2007). This leads to important constraints on the level of the articulatory 
control model (on the level of the action unit score). The remaining vocal tract action 
parameters can be estimated easily from the acoustic signal.  
2   Control of an Articulatory Speech Synthesizer 
In our control concept each syllable realization is considered to consist of one or more 
vocal tract action units distributed over gestural tiers. These tiers are named ‘vocalic’, 
‘consonantal’, ‘velic’, ‘glottal’ and ‘subglottal pressure’ (Kröger and Birkholz 2007 and 
Birkholz et al. 2006, and see Fig. 1). All actions are realized as goal- or target-directed 
movements. In the case of vocalic actions, which occur on the vocalic tier, the goal is to 
reach a vowel specific vocal tract shape. In the case of consonantal closing actions, 
which occur on the consonantal tier, the goal is to reach a consonant specific full-
closure for realizing plosives or nasals or to reach a consonantal near-closure in order to 
realize a fricative. Since some phoneme realizations have the same kind of oral 
constriction, a single consonantal action unit can correspond with more than one phone. 
The disambiguation is done by combination with other action units on the velic tier or 
on the glottal tier. For example an apical full-closing action on the consonantal tier can 
be combined with a glottal opening action (opgl) for producing a voiceless plosive. For 
example a labial full-closing action (clla) on the consonantal tier can be combined with 
glottal closing action (clgl) on the glottal tier to result in a fully voiced bilabial plosive 
or with glottal closing action and velopharyngeal or velic opening action to produce a 
bilabial nasal (Note that clgl is a default gesture and thus not indicated in action scores, 
see Kröger and Birkholz 2007). The set of consonantal action units consists of full-
closing actions (cl) for the production of stops and near-closing actions (nc) for the 
production of fricatives. Beside the manner of articulation, the consonantal action units 
also contain information about the place of articulation. Full closings can be labial (clla), 
apical (clap) and dorsal (cldo). Near closings can be labio-dental (ncld), alveolar (ncal) 
and postalveolar (ncpo) (Kröger and Birkholz 2007). 
All action units comprise an onset, steady-state, and offset time interval (Fig. 1 and 
Kröger et al. 1995). The steady-state time interval is often nearly zero, since in real 
articulation behavior, no steady states can be found although perception suggests a 
succession of (steady state) phones.  Steady states are used in our control model 
mainly for defining the time interval of full-closure or near-closure time intervals for 
plosives, nasals, and fricatives. During action onset time intervals, the goal-directed 
movement of the articulator towards the action target (e.g. a labial, apical, or dorsal 
full- or near-closure, a wide velopharyngeal or glottal opening, a specific vocalic 
vocal tract shape) is performed, while during offset the articulator-dependent rest 

346 
D. Bauer, J. Kannampuzha, and B.J. Kröger 
position is approached if no further action unit is using that particular articulator at the 
same time. Thus the time interval of action onset represents the time interval from  
the start of activation of an action until the time point at which the target or goal of 
the action is reached. The articulatory targets of actions were estimated for a speaker 
of Standard German by adaptation of MRI data (Birkholz and Kröger 2006). 
 
 
Fig. 1. Action score of the German word “Mappe” (see also Fig. 3 and Fig. 4). Onset, steady 
state, and offset time intervals are marked for the first consonantal closing gesture. The time 
points ‘begin of onset’ and ‘end of onset’ (BON and EON) and ‘begin-’ and ‘end of offset’ 
(BOF and EOF) are labeled. Amplitude (in bold black lines) indicates the degree of realization 
of each action. The transcription of the word is given below the action score. Note that the 
second vocalic action in “Mappe” for realizing the schwa-sound is a default action (Kröger and 
Birkholz 2007) and thus not explicitly shown in the action score. 
3   Re-synthesis Method  
Our re-synthesis approach is basically an analysis by synthesis method. The natural 
acoustic signal of each speech item produced by our reference speaker of Standard 
German is collected in an acoustic database (Fig. 2). Each item is transcribed 
manually first. A phonological action score consisting of all relevant vocal tract action 
units is generated next. At this level no exact temporal alignment is provided. After a 
first re-synthesis the resulting (synthetic) acoustic wave file is compared with the 
natural wave file and a temporal alignment of action units can now be done (see 
below). Temporal alignment is mainly done for matching discrete landmarks in the 
acoustic signal between natural and synthetic wave file (i.e. onset and release time 
points of consonantal full-closures or near-closures and begin or end of voicing). It is 
not possible to match the formants of the natural wave file exactly with that of the 
synthetic wave file, but the general tendencies of formant movements are tried to be 

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
347 
matched as well. In this way a manual fine tuning with respect to acoustic land marks 
and with respect to formant trajectories is done. After this tuning, the current action 
score is stored in the action score database (Fig. 2). The re-synthesis is done by using 
the articulatory speech synthesizer described by Birkholz (2005) and Birkholz et al. 
(2007). Note that in our re-synthesis method the pitch trajectories are copied from the 
original utterance to ensure that the perception of the synthetic signal is not affected 
by intonation artifacts. Although our action-based control model in principle is 
capable of generating intonation patterns by rule, the natural intonation contour is 
copied in this study in order to concentrate on supralaryngeal articulation.  
 
 
Fig. 2. Schematic view of the re-synthesis workflow 
The fitting of natural and synthetic acoustic wave files is performed with the 
software tool “SAGA”. This software comprises the possibility of synchronous 
display of oscillogram and spectrogram of natural and synthetic signal. Furthermore 
the software allows the synchronous display of the vocal tract action score for 
performing action specific temporal alignments (Fig 3). This temporal alignment of 
onset, steady state, and offset time interval of action units can be done manually in 
order to match the acoustic landmarks in the acoustic speech signal. In addition, the 
program is able to show and to overlay synchronously intensity, pitch, and first three 
formant trajectories for both signals (natural and synthetic).  
4   Acoustic Data Corpus 
Re-synthesis is done in this study for words, pseudo-words and syllables. The main 
problem with the collection of an acoustic data corpus is that either a set of (phonotactic 
correct) pseudo-words with a systematic variation of their syllable structure or a set of  
 

348 
D. Bauer, J. Kannampuzha, and B.J. Kröger 
 
Fig. 3. Screenshot of SAGA showing the spectrograms of synthetic and natural acoustic signals 
and of the appropriate action score of the German word “Mappe” (see also Fig. 1 and Fig. 4). 
The lower spectrogram shows the synthetic, the upper the natural signal. The action score in the 
lower part contains the different tiers for action units: vocalic, consonantal, velum, glottal and 
subglottal pressure. The temporal alignment labels are shown for onset and offset of the first 
consonantal closing action (vertical lines). Rules for temporal alignment in the case of this 
action see section 5.1. 
words which really exist in Standard German can be chosen. In the latter case, gaps for 
certain phone combinations often occur. For this reason we decided to have two corpora 
with different focus: A pseudo-word corpus and a real-word corpus. 

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
349 
Table 1. Corpus of mono-syllabic words with CVC structure in Standard German. The vowel 
can be a German tense or lax vowel or a diphthong. Rows indicate different initial consonants. 
Columns indicate different final consonants.  

Zo\
Zs\
Zj\
Zl\
Zm\
ZM\
Zk\
Za\
Zaço\
Zat9o\
ZaDs\
Zan9s\
ZaNj\
Zan9j\
ZaAUm\
ZaA9n\

ZaAIl\
Za@k\
Zc\
Zch9o\

ZcIk\
ZcNj\
Zc@l\
Zc@m\
ZcHM\
ZcNk\
Zf\
Zf@9o\
ZfHo\
Zfçs\
Zft9s\



Zf@M\
ZfNM\
Zf@Tk\
Zo\
ZoNo\
Zo@s\
Zo`j\



Zo@Tk\
Zon9k\
Zs\
Zs@To\
ZsHo\
Zs@9s\
Zsn9s\
Zs@9j\
Zs@Hj\

Zsn9m\
Zs@M\
Zs@9k\
Zs@Hk\
ZsNk\
Zj\
Zj@o\
ZjHs\
Zjn9s\

Zj@l\
ZjHm\

Zj@9k\
Zjn9k\
Zj@Hk\
Zjx9k\
Zl\
ZlNo\
Zl@9s\
Zl@9j\
ZlTl\
Zl@m\
Zln9m\

Zl@Tk\
Zld9k\
ZlNk\
ZlXk\
Zm\
ZmDo\
Zm@9s\
Zm@Hs\
ZmDs\
Zmn9s\

ZmHl\
Zm@Hm\
ZmNHm\

Zmh9k\
ZmTk\
Zk\
Zk@To\
Zk@Ho\
Zk@Ts\
Zk@Hs\
Zkh9s\
Zkn9s\
ZkDj\
ZkNj\
Zk@9j\
Zk@9l\
Zk@l\
Zkd9l\
Zk@Hl\
Zkn9m\
Zk@M\
Zk@k\
 
The pseudo-word corpus contains CV syllables with all voiced and voiceless 
plosives, nasals and a lateral (/b, d, g/, /p, t, k/, /m, n/, /l/) combined with all 5 long 
vowels in Standard German (/i/, /e/, /a/, /o/, /u/) (Æ 45 items) and CCV syllables with 
plosives as the first consonant and the lateral as the second consonant (/bl/, /gl/, /pl/, 
/kl/) (Æ 10 items). The CCV syllables match the phonotactic constraints of Standard 
German. For this reason the corpus does not contain syllables like */dli/ or */tla/. 
These 55 items were recorded four times by a speaker of Standard German.  
The real-word corpus comprises 85 mono-syllabic words of Standard German with 
CVC structure (Tab. 1) and currently 21 bi-syllabic words of Standard German with 
syllable structure CV-CV with a variation of the first consonant (Kanne, Panne, Tanne) 
and with a variation of the second consonant (Macke, Mappe, Matte). In this 

350 
D. Bauer, J. Kannampuzha, and B.J. Kröger 
constellation the second consonant is ambisyllabic when the first vowel is lax (Tab. 2). 
In addition four words with CVm-pV structure were integrated in order to re-
synthesize nasal-plosive successions (Tab. 2). All words were recorded in the 
environment of the carrier sentence “Ich habe xxx gesagt” (Where ‘xxx’ is replaced by 
the actual word). In this sentence position the words are always stressed. Most of the 
recorded words showed a strong coarticulation effect from the velar constriction of the 
following word “gesagt”. Thus this action unit (cldo) was included during the re-
synthesis even if it is not an integral part of the target word. 
Table 2. Corpus of bi-syllabic words with CV-CV or CVC-CV structure in Standard German. 
The first vowel is always a German lax vowel, the second vowel is always schwa. Rows 
indicate different initial consonants. Columns indicate different medial consonants and 
consonant clusters.  

Zo\
Zs\
Zj\
Zl\
Zm\
m`r*oknr
Zo\
Zo@o?\
 
Zo@j?\
 
Zo@m?\
ZoTlo?\
Zo@lo?\
Zs\
Zs@o?\
ZsHs?\
 
 
Zs@m?\
ZsNm?\
Zs`ms?\
Zj\
Zj@o?\
 
Zj@j?\
ZjNl?\
Zj@m?\
Zj`ms?\
Zl\
Zl@o?\
Zl@s?\
Zl@j?\
Zlxj?\
 
 

Zm\

ZmDs?\

 
ZmNm?\

5   General Alignment Rules 
Three different types of temporal alignment rules or temporal coordination rules exist. 
i) Alignment of onset or offset time points of an action with specific acoustic 
landmarks like begin or release of consonantal closure or begin of voicing (acoustic-
to-action alignment, circles in Fig. 4); ii) Alignment of time points of onset or offset 
of an action with respect to time points of onset or offset of an other gesture (inter-
action alignment, diamonds in Fig. 4); iii) Alignment of duration of onset or offset of 
an action (intra-action alignment, squares in Fig. 4). These different types of align-
ment occur for all types of action units, i.e. consonantal, vocalic, glottal, velic, and 
subglottal pressure action units (see below and see Fig. 4). In addition vocalic targets 
can be adjusted with respect to formant trajectory matching.    
5.1   Alignment of Consonantal Action Units 
In our approach, the manual temporal alignment of action scores starts with the 
alignment of consonantal action units realizing the syllable initial and syllable final 
consonants. The durations of onset and offset intervals of consonantal full-closing and 
near-closing actions are taken from articulatory data (Tab. 3). That leads to an intra-
action alignment for these action units (see time interval labels 1 to 4 in Fig. 4). From 

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
351 
the acoustic signal consonantal full-closure or near-closure intervals can be easily 
detected in most cases for plosives, nasals, fricatives, and laterals. This leads to a 
temporal alignment of EON and BOF for the appropriate consonantal closing vocal 
tract action units (see time point label 5 to 8 in Fig. 4 for example word “Mappe”). 
Since the temporal alignment of the acoustic landmarks for begin and end of 
consonantal closure or consonantal constriction have to be slightly before EON and 
slightly after BOF (see Fig. 4 and Fig. 1, rule: acoustic begin of closure or 
constriction coincides with time point at 3/4 of onset interval; acoustic end of closure 
or constriction coincides with time point of 1/4 of offset interval) these time intervals 
must be set before the temporal alignment of EON and BOF can be done for 
consonantal full-closing or near-closing actions. 
 
 
Fig. 4. Action score of the German word “Mappe” (see also Fig. 1 and Fig. 3). The time point 
labels indicated by circles indicate time points which are aligned with respect to the natural 
acoustic data (acoustic-action alignment). The time point labels indicated by diamonds indicate 
time points which are aligned with respect to other actions (inter-action alignment). The time 
interval labels indicated by squares indicate duration of onset or offset time intervals of actions 
which are set with respect to articulatory data (intra-action alignment).   
Onset and offset durations of consonantal action units are shorter than those of 
vocalic action units. The onset duration ranges between 75 and 90 ms, the offset 
duration between 95 and 120 ms for consonants (Tab. 3). Plosives have slightly 
longer offsets than fricatives and nasals. The temporal coordination of these already 
aligned consonantal action units with glottal action units and velic action units in the 
case of the production of voiceless plosives, fricatives and nasals is given below 
(section 5.3 and 5.4).   
 

352 
D. Bauer, J. Kannampuzha, and B.J. Kröger 
Table 3. Onset and offset durations of consonantal closing actions and reference 
action unit  
onset 
offset 
reference data  
clla (plosive) 
90 
110-120 
Löfqvist 2005, Fig. 10 
  
 
 
Löfqvist and Gracco 1997, Fig. 3 
clla (nasal)
75-85 
95-120 
Löfqvist 2005, Fig. 11 
clap 
90-120 
100-120 
Wrench 1999, Fig 2 
  
100 
100 
Adams et al. 1993, Fig. 6 
cldo 
90-120 
100-140 
Wrench 1999, Fig 2 
5.2   Alignment of Vocalic Action Units 
The preparation of the vocal tract to produce a vowel starts long before the vowel 
becomes audible. With an acoustic based alignment method it is not possible to 
determine the starting time exactly because it is covered by the preceding consonants. 
The EMA data acquired by Wrench (1999) show that the preparation is done during 
the constriction phase of the preceding consonants and it can still go on after the 
release of the closure. Thus the temporal coordination of vocalic actions can be done 
with respect to the already aligned preceding and following consonantal closing 
actions. Begin of vocalic onset starts in the middle of the onset interval of the 
preceding consonantal gesture (time point label 9 in Fig. 4). End of vocalic onset 
coincides with the middle of the onset interval of the following consonantal gesture 
(time point label 10, Fig. 4). The offset interval of vocalic gesture is synchronous to 
the onset interval of the following vocalic gesture, i.e. the offset starts in the middle of 
the onset of the following consonant and ends in the middle of the next consonant 
onset interval (not illustrated in Fig. 4).  
5.3   Alignment of Velic Action Units 
EMA analyses of the velum movement in nasals indicate that the velopharyngeal 
opening is at its maximum at the end of onset interval of the appropriate consonantal 
closing gesture (Wrench 1999). Thus the end of onset interval of a velic action 
coincides with the end of onset of the appropriate consonantal closing action (time 
point label 11 in Fig. 4). The onset movement is relatively slow and starts already at 
the beginning of the preceding vowel. The offset movement of the velic action unit 
often begins during the appropriate consonantal closing action unit. The same finding 
was reported by Moll and Daniloff (1971). The length of onset and offset interval is 
about 200 ms (Horiguchi and Bell-Berti 1987) and can be used for specifying the 
begin of onset time interval and the end of offset time interval for the velic action 
(time interval label 12 and 13 in Fig. 4). When the nasal is followed by a plosive, the 
velum raises much faster in order to prevent a pressure loss during the constriction 
phase of the following plosive. That leads to a much shorter offset interval for the 
velic gesture. This is in accordance also with the EMA data given by Wrench (1999). 
The duration of onset and offset time interval of a velic action unit during the 
production of nasals ranges between 140 ms and 250 ms (see above), but can be 
shortened up to 100 ms when a plosive follows (not shown in Fig. 4).  

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
353 
5.4   Alignment of Glottal Action Units 
Glottal opening action units (opgl) occur in voiceless fricatives and voiceless plosives. 
In voiceless plosives the glottal opening reaches its maximum in the middle of the 
offset of the consonantal full-closing gesture in order to ensure a strong noise burst 
(aspiration) at the release of the consonantal constriction (Löfqvist and Yoshioka 1980, 
and see time point label 14 in Fig. 4 and see Fig. 5 left side). For fricatives the glottal 
opening reaches its maximum in the middle of the steady state portion of the 
appropriate consonantal near-closing action (ibid., and see Fig. 5 right side). The 
duration of onset and offset time interval vary between 90 ms and 120 ms for onset and 
between 100 ms to 130 ms for offset (ibid.). Thus the beginning of onset and end of 
offset interval of glottal opening actions is determined by these durations (time interval 
label 15 and 16 in Fig. 4). In addition the exact duration of onset and offset in the case 
of plosives is limited by the fact that begin of onset coincides with the begin of the 
steady state portion of the appropriate consonantal full-closing gesture in order to 
prevent pre-aspiration and the middle of offset time interval coincides with the begin of 
phonation for the following vowel (time point label 17 in Fig. 4). 
 
 
Fig. 5. Temporal coordination of glottal opening actions with respect to the appropriate 
consonantal closing action for plosives (left side) and for fricatives (right side) 
5.5   Alignment of Subglottal Action Units 
Each utterance is produced on the basis of one single subglottal pressure action. A fast 
pressure built-up and pressure fall is assumed (i.e. short pressure onset and offset 
intervals (around 30 ms; time interval labels 18 and 19 in Fig. 4). The goal of 
pulmonic actions is that the subglottal pressure is roughly constant over the complete 
utterance (Draper et al. 1959).  
It can be assumed that the constriction of the first consonant of an utterance 
coincides with the middle of onset interval for subglottal pressure built up (time point 
label 20 in Fig. 4). That rule ensures a correct production of the first consonant 
without an unwanted insertion of prevocalic schwa which would occur, if subglottal 
pressure onset starts earlier. If subglottal pressure onset starts later than defined by 
this rule, the intra-oral pressure built-up during the consonantal closure of obstruents 
is not strong enough or the voicing of sonorants would start too late.  
The temporal coordination of the offset of the subglottal pressure action coincides 
with the offset interval of the consonantal gesture if the utterance ends with a 
consonant. The offset interval of the subglottal pressure action coincides with the 
offset of the vocalic gestures if the utterance ends with a vowel. In the case of our 
corpus, the offset of the subglottal pressure action is always temporally coordinated 

354 
D. Bauer, J. Kannampuzha, and B.J. Kröger 
with the offset of the consonantal closing gesture of the /g/-realization, which is part 
of the first syllable of the last part of the carrier sentence “gesagt” (see section 4). 
This closing action is not shown in Fig. 1, 3, and 4 in order not to achieve simple and 
understandable figures.  
6   Results and Discussion  
Re-synthesis trials were done for 22 out of 85 items of the mono-syllabic real-word 
corpus and for 11 out of 21 items of the bi-syllabic real-word corpus (see section 4). 
The re-synthesis procedure indicates that the control model including the acoustic-to-
action alignment rules and the inter-action and intra-action alignment rules on the one 
hand leaves still enough flexibility for fitting the natural speech signals of our speaker 
of Standard German but on the other hand delivers enough constraints for specifying 
all action parameters for the complete action unit score (i.e. location of onset, steady 
state, and offset for all vocal tract action units of the score).  
A preliminary perceptual evaluation of the synthesized utterances indicates a high 
degree of naturalness and intelligibility. Especially the smooth transitions of the 
articulation movements resulting from the temporally overlapping vocal tract action 
units as result from our control model lead to promising results. The action-based 
control concept is capable to handle all major phonetic effects for producing a high 
quality acoustic signal.  
The method described in this paper is just a preliminary approach to generate TTS-
applicable control information for articulatory speech synthesis. But the procedure 
can also be used for basic phonetic research on articulatory processes and is also the 
basis for constructing an action score corpus for frequent syllables in Standard 
German. It is planned to complete the set of re-synthesized action scores for the whole 
corpus outlined in section 4 and to define rules for an automatic generation of action 
scores for infrequent syllables. 
Acknowledgments. This work was supported in part by the German Research 
Council DFG grant Kr 1439/13-1 and grant Kr 1439/15-1.  
References 
Adams, S.G., Weismer, G., Kent, R.D.: Speaking Rate and Speech Movement Velocity Profiles. 
Journal of Speech and Hearing Research 36, 41–54 (1993) 
Badin, P., Bailly, G., Revéret, L., Baciu, M., Segebarth, C., Savariaux, C.: Three-Dimensional 
Linear Articulatory Modeling of Tongue, Lips and Face, Based on MRI and Video Images. 
Journal of Phonetics 30, 533–553 (2002) 
Birkholz, P.: 3D Artikulatorische Sprachsynthese. Ph.D Thesis, Rostock (2005) 
Birkholz, P., Kröger, B.J.: Vocal Tract Model Adaptation Using Magnetic Resonance Imaging. 
In: Proceedings of the 7th International Seminar on Speech Production, Belo Horizonte, 
Brazil, pp. 493–500 (2006) 
Birkholz, P., Jackel, D., Kröger, B.J.: Simulation of losses due to turbulence in the time-varying 
vocal system. IEEE Transactions on Audio, Speech, and Language Processing 15, 1218–
1225 (2007) 

 
Articulatory Speech Re-synthesis: Profiting from Natural Acoustic Speech Data 
355 
Birkholz, P., Jackèl, D., Kröger, B.J.: Construction and Control of a Three-Dimensional Vocal 
Tract Model. In: Proceedings of the International Conference on Acoustics, Speech, and 
Signal Processing (ICASSP 2006), Toulouse, France, pp. 873–876 (2006) 
Birkholz, P., Steiner, I., Breuer, S.: Control Concepts for Articulatory Speech Synthesis. In: 
Sixth ISCA Workshop on Speech Synthesis, Bonn, Germany, pp. 5–10 (2007) 
Dang, J., Honda, K.: Estimation of vocal tract shapes from speech sounds with a physiological 
articulatory model. Journal of Phonetics 30, 511–532 (2002) 
Deterding, D., Nolan, F.: Aspiration and Voicing of Chinese and English Plosives. In: 
Proceedings of the ICPhS XVI, Saarbrücken, pp. 385–388 (2007) 
Draper, M.H., Ladefoged, P., Whiteridge, D.: Respiratory Muscles in Speech. Journal of 
Speech and Hearing Research 2, 16–27 (1959) 
Engwall, O.: Articulatory Synthesis Using Corpus-Based Estimation of Line Spectrum Pairs. 
In: Proceedings of Interspeech, Lisbon, Portugal (2005) 
Horiguchi, S., Bell-Berti, F.: The Velotrace: A Device for Monitoring Velar Position. Cleft 
Palate Journal 24(2), 104–111 (1987) 
Kröger, B.J.: A gestural production model and its application to reduction in German. 
Phonetica 50, 213–233 (1993) 
Kröger, B.J., Birkholz, P.: A Gesture-Based Concept for Speech Movement Control in 
Articulatory Speech Synthesis. In: Esposito, A., Faundez-Zanuy, M., Keller, E., Marinaro, 
M. (eds.) COST Action 2102. LNCS (LNAI), vol. 4775, pp. 174–189. Springer, Heidelberg 
(2007) 
Kröger, B.J., Schröder, G., Opgen-Rhein, C.: A gesture-based dynamic mo¬del describing 
articulatory movement data. Journal of the Acoustical Society of America 98, 1878–1889 
(1995) 
Levelt, W.J.M., Roelofs, A., Meyer, A.S.: A Theory of Lexical Access in Speech Production. 
Behav. Brain Sci.  22, 1–38 (1999) 
Levelt, W.J.M., Wheeldon, L.: Do Speakers Have Access to a Mental Syllabary? Cognition 50, 
239–269 (1994) 
Löfqvist, A.: Lip Kinematics in Long and Short Stop and Fricative Consonants. J. Acoust. Soc. 
A. 117(2), 858–878 (2005) 
Löfqvist, A., Gracco, V.L.: Lip and Jaw Kinematics in Bilabial Stop Consonant Production. 
Journal of Speech, Language, and Hearing Research 40, 877–893 (1997) 
Löfqvist, A., Yoshioka, H.: Laryngeal Activity in Swedish Obstruent Clusters. J. Acoust. Soc. 
Am. 68(3), 792–801 (1980) 
Moll, K.L., Daniloff, R.G.: Investigation of the Timinig of Velar Movements during Speech. 
JASA 50(2), 678–684 (1971) 
Wrench, A.: An Investigation of Sagittal Velar Movements and its Correlation with Lip, 
Tongue and Jaw Movement. In: Proceedings of the ICPhS, San Francisco, pp. 435–438 
(1999) 
Yoshioka, H., Löfqvist, A., Hirose, H.: Laryngeal adjustments in the production of consonant 
clusters and geminates in American English. J. Acoust. Soc. Am. 70(6), 1615–1623 (1981) 

A Blind Source Separation Based Approach for
Speech Enhancement in Noisy and Reverberant
Environment
Alessio Pignotti, Daniele Marcozzi, Simone Cifani, Stefano Squartini,
and Francesco Piazza
3MediaLabs, DIBET, Universit`a Politecnica delle Marche
Via Brecce Bianche 31, 60131 Ancona, Italy
s.cifani@univpm.it
Abstract. Several eﬀorts have been put by the international scientiﬁc
community on the Speech Enhancement (SE) research ﬁeld, specially for
the several applications it may have (like human-machine dialogue sys-
tems and speaker identiﬁcation/veriﬁcation). An innovative SE scheme
is presented in this work: it integrates the spatial method (Blind Source
Separation, BSS) with the temporal method (Adaptive Noise Canceller,
ANC) and a ﬁnal stage composed of a Multichannel Signal Detection
and a Post Filter (MSD+PF) to enhance vocal signals in noisy and re-
verberant environment. We used a broadband blind source separation
(BSS) algorithm to separate target and interference signals in real rever-
berant scenarios and the two post-processing stages ANC and MSD+PF,
in cascade with the ﬁrst one, to improve the separation yielded by the
BSS. In particular the former one allows to further reduce the residual
interference signal still presents in the desired target signal after separa-
tion, by using as reference the other output of the BSS stage. Computer
real-time simulations show progressive improvements across the diﬀer-
ent processing stages in terms of the chosen quality parameter, i.e. the
coherence between the two output channels.
1
Introduction
The remarkable success and the increasing demand for hands-free communi-
cation devices and speech-based human-machine interfaces has started a new
controversial trend. On the one hand people claim for high quality and on the
other they need to cope with hostile environments. Many applications, such
as teleconferencing, automatic speech recognition (ASR), hearing aids, severely
degrade their performance when the desired speech signal tends to mix with
interfering signals (interfering speakers, noise bursts, background noise). One
of the hardest situation to handle is called ”cocktail party” and indicates the
extraction of a desired speech signal from mixtures (or multiple mixed signals)
picked up by microphones placed inside an enclosure. Microphone arrays allow
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 356–367, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

A Blind Source Separation Based Approach for Speech Enhancement
357
to detect target signals in noisy environments using spatial information and gen-
erally provide particularly good performance. A well-known method to perform
spatial ﬁltering (beamforming) is to make use of a microphone array with ar-
bitrary geometry to form a beam towards the target signal [1], [2]. In adaptive
beamforming, a structured array of sensors is used to steer the overall gain pat-
tern of the array sensors to form a spatial ﬁlter which can extract signal from a
speciﬁc direction and reduce signals from other directions. Unfortunately, many
beamforming methods require previous knowledge about the acoustic environ-
ment and the sources involved or a large number of sensors is required to obtain
a good performance. On the other hand, blind source separation (BSS) is an
approach for estimating source signals using only the information on mixed sig-
nals observed at each input channel. Since it requires few prior knowledge and
only needs a small number of microphones, BSS seems to be a good alternative
to beamforming. However, for acoustic applications, the multipath environment
convolutively mixes the sources and a matrix of ﬁlters is needed to approxi-
mately unmix the mixtures. The conventional approach to this problem is to
carry out the algorithms in the frequency domain, but at the expenses of scaling
and permutation ambiguities [3].
Two important aspects have to be esteemed in practical applications: ﬁrstly,
microphones are in a limited number causing possible undetermined situations
(number of sensors less than the number of sources); secondly, sources are often
placed in noisy environments. Low et al. [4] proposed a novel two-microphone
speech enhancement structure, where a ﬁrst BSS stage (spatial processing) was
followed by an adaptive noise canceller (ANC) (temporal processing). The struc-
ture aims to apply BSS to enhance a single desired speaker in various noisy con-
ditions and improves the overall output by means of a second precessing step,
the ANC, to fully exploit spectral diversity.
The present work extends the aforementioned scheme and proposes a novel
structure, where the initial BSS stage has been replaced by a more recent and
more performant BSS algorithm [5], based on second-order statistics (SOS).
The ANC is left unaltered and is followed by an additional stage consisting of a
Multichannel Signal Detection (MSD) and a multichannel Post-Filter (PF) [6].
The selected BSS algorithm, with respect to traditional BSS schemes, exploits
the non-stationarity and the non-uniformity spectrum property simultaneously,
avoiding permutation problems and circular convolution eﬀects which charac-
terize the traditional narrowband separation techniques, without needing any
geometrical information about the sensors position. The primary and reference
signals, coming out from the BSS, are then passed to the ANC, inspired from
[7] and especially devised for speech applications. Finally, the outgoing signal
is further processed by the third stage (MSD+PF) to yield a cleaner overall
output, particularly for what concern noise transients.
Here is the paper outline: Section II contains a detailed description of the
proposed structure, including an overview of the complete system. Experimen-
tal simulations and their discussions are presented in Section III. Section IV
concludes the ﬁndings.

358
A. Pignotti et al.
2
Proposed Structure
2.1
Overview
An overall scheme of the proposed system is showed in Fig.1. Initially, the target
signal is separated from the interference by the BSS alghorithm. This process
yields two outputs, one consisting of target signal plus an incorrelated residual
interference and the other containing the interference. The former is then served
as the primary signal for the ANC ﬁlter, while the latter represents the reference
signal. The ANC output is ﬁnally passed to the MSD+PF stage which completes
the enhancement process.
This study is intended to accomplish an alternative scheme to that already
presented in [6], [8]. Looking at the ﬁrst two stages (BSS+ANC), one may notice
that they eﬀectively accomplish the same role of the Generalized Sidelobe Can-
celler (GSC) used in [6], [8]. As aforementioned, the BSS based scheme allows
operating in more general conditions w.r.t. beamforming counterpart: this rep-
resents the main diﬀerence between the two approaches and, at the same time,
the most relevant motivation for the present work.
2.2
Blind Source Separation – BSS
In this stage the speech sources, available under the form of convolutive mixtures
at the microphones, are recovered by applying a suitable BSS algorithm. From
an analytical point of view, the problem can be formulated as follows:
xp (n) =
P

q=1
M−1

k=0
hpq,ksq (n −k) + vp (n)
(1)
where hpq,k with k = 0, 1, ..., M −1 are the coeﬃcients of the M-tap FIR ﬁlter
from the q−th source to the p−th sensor. The algorithm goal consists in ﬁnding
out a multichannel demixing system yielding the following output signals:
yp (n) =
P

q=1
L−1

k=0
wpq,kxq (n −k)
(2)
where wpq,k with k = 0, 1, ..., M −1 are the coeﬃcients of the M-tap FIR ﬁl-
ter from the q −th sensor to the p −th output channel. The main assump-
tion to make the problem tractable, shared by most of the BSS techniques
Fig. 1. Proposed two-mics speech enhancement scheme

A Blind Source Separation Based Approach for Speech Enhancement
359
available in the literature, is considering that sources in (1) are statistically
independent. Separation is achieved by imposing the output signals to be sta-
tistically decoupled till moments of a certain order. Several narrow-band BSS
approaches in the frequency domain have been proposed so far, based on these
concepts [9]: however they present some indeterminacy problems resulting in
a diﬃcult reconstruction of the original signals. That is why a new class of
separation algorithms have been proposed [5], [10], [11], namely broad-band,
where the frequency bins are not treated independently. They are typically
based on second-order statistics (SOS) and, as aforementioned, exploit the non-
stationary and non-uniformity properties of the signal spectrum simultaneously,
inherently avoiding the narrow-band approach limitations. Moreover, although
the current state-of-the-art real-time implementations are based on narrowband
frequency-domain algorithms, a robust real-time implementation of the selected
broadband algorithm is also possible, partially increasing the computational
complexity, as shown in [5], which has been selected as suitable trade-oﬀsolu-
tion in terms of achievable separation performances and on-line implementation
capabilities for the present work purposes.
Fig. 2. Systems setup which need of causal demixing ﬁlter (a) and of non-causal demix-
ing ﬁlter (b)

360
A. Pignotti et al.
In [5] two diﬀerent real-time implementations have been presented: one using
causal demixing ﬁlters, robust and suitable for the great majority of applications,
and the other using noncasual demixing ﬁlters, more general and suitable for
every kind of sources and microphones conﬁguration. Diﬀerent initializations
are required, depending on the chosen version (see 2). In this work we adopt
the second version which uses non-causal demixing ﬁlter and it is adapted in
non-symmetrical placed sources (see 2b).
The algorithm under consideration presents an eﬃcient combination of online
and oﬄine elaboration, named block-online adaptation and a dynamic step-size
control, allowing to increase the convergence speed and the robustness to vari-
ability environment condition in noisy and reverberation terms. Moreover, it also
tackles the problem of noise inﬂuence: indeed, considering that the cost-function
to be minimized by the broadband adaptive algorithm is nothing but a reformula-
tion of output generalized coherence, it can be easily proved that achieving source
separation means partially removing correlated noise components too [10], [12].
2.3
Adaptive Noise Canceller – ANC
Adaptive noise cancelling is an eﬃcient method to recover a signal corrupted by
additive noise. This is achieved by making use of a reference input, assumed to
only contain samples belonging to the noise ﬁeld, which is ﬁltered and subtracted
from a primary input containing both signal and noise. As a result the primary
noise, called jammer, is attenuated or eliminated by cancellation. Jammer is
assumed to be correlated with the reference input and uncorrelated with the
target signal. In ideal condition, the resulting output contains the undistorted
target signal plus an error consisting of a minimum residual jammer. A block
diagram of the ANC is shown in the second stage of the scheme in Fig. 1.
In typical implementation of ANC the ﬁltering is achieved using a FIR ﬁlter
whose coeﬃcients are adjusted using a gradient search algorithm such as the least
mean square (LMS), because of its simplicity, implementation aptitude and low
computational complexity.
Using the LMS algorithms, the update weights equation of the adaptive noise
canceller is,
h(n + 1) = h(n) + μe(n)y(n)
(3)
where n is the time index, boldface characters represent vectors, h(n) is the
vector of adaptive ﬁlter weights, L is the length of the adaptive ﬁlter, μ is a step-
size parameter with units of inverse power, y(n) is the data vector containing
the L samples of the reference signal (coming from the preceding BSS stage),
y(n), that are present in the adaptive ﬁlters tapped delay line at time n, and
e(n) is the system output given by
e(n) = p(n) −hT (n)y(n)
(4)
where p(n) represents the primary signal and T indicates the transpose operation.

A Blind Source Separation Based Approach for Speech Enhancement
361
A disadvantage of the ANC using the LMS algorithm is that the steady-state
excess mean-square error (MSE), which is due to the ongoing adjustment of the
adaptive weights, linearly increases with the target signal power. This leads to
poor performance in the presence of strong target signals but it is not a problem
when recovering a weak target signal corrupted by a stronger jammer. However,
it can signiﬁcantly degrade performance in applications where there are inter-
vals when the target signal is strong relative to the noise signal. This commonly
occurs when the target signal is speech, and is a known problem in many speech
processing applications. To solve this problem, Greenberg [7] developed an up-
date strategy which enables to a more sophisticated control of the steps in the
adaptive weights than the all-or-nothing approach of the conventional LMS up-
date. This control can be obtained replacing the μ in (3) with a time-varying
quantity, f(n), resulting in a class of modiﬁed LMS algorithms described by
h(n + 1) = h(n) + f(n)e(n)y(n)
(5)
where f(n) (units of inverse power) may depend on any time-varying quantity
available to the adaptive processor. In [7], three practical method for adjusting
the step-size function are proposed and we report them in the following:
ftrad(n) =
αtrad
Lˆσ2y(n)
Conventional LMS
fwtsum(n) =
αwtsum
L[ˆσ2e(n) + αwtsumˆσ2y(n)] Weighted Sum Method
fsum(n) =
αsum
L[ˆσ2e(n) + ˆσ2y(n)]
Sum Method
where ˆσ2
e is the time-varying estimate of the output signal power, and ˆσ2
y is
the time-varying estimate of the reference signal power. A ﬁrst order recursive
smoothing ﬁlter is used to estimate ˆσ2
e and ˆσ2
y, given by
ˆσ2
r(n) = βˆσ2
r(n) + (1 −β)ˆσ2
r(n −1)
(6)
where β is a convenient smoothing parameter.
The selection of the best method to adopt in the ANC stage has been carried
out by means of preliminary experiments. As expected, either the weighted sum
or sum method is preferable to the traditional method. Although the weighted
sum method was derived from an optimal approach, the numerous approxima-
tions required to obtain a practical algorithm eliminated any potential advantage
that that approach might have oﬀered over the sum method. Consequently, the
sum method was chosen.
2.4
Multichannel Signal Detection and OM-LSA Postﬁlter
Since the BSS algorithm exploits nonwhiteness and nonstationarity of the sources
and the ANC exploits the correlation between jammer and reference signal, a

362
A. Pignotti et al.
signiﬁcant performance degradation is expected in non-stationary noise environ-
ment. When this occurs, conventional multichannel post-ﬁltering techniques are
not able to properly deal with highly nonstationary noise components and a suit-
able MSD procedure is needed to distinguish between useful signal and noise spec-
tral transients. As stated in [6], the Hypothesis Testing [8] is able to accomplish
this task by means of the local non-stationarity measure, deﬁned as the ratio be-
tween the total and pseudo-stationary spectral power. When a transient is de-
tected simultaneously at the primary and at the reference signal, its origin can be
determined by assuming that desired signal components are relatively strong at
the primary, whereas interfering components are relatively strong at the reference
signal. Once desired components are detected, the a priori signal absence proba-
bility is estimated and taken into account by the subsequent postﬁlter. Indeed,
conventional algorithms, like the one based on log-spectral amplitude (LSA) es-
timation, often fail under quickly time-varying conditions, due to the sparsity of
the spectral coeﬃcients of the speech signal. Therefore reliable detectors of speech
activity and noise transients are needed. An improvement of the LSA estimator
taking speech presence probability into account, namely optimally-modiﬁed LSA
(OM-LSA), has been eﬀectively employed on purpose in [8].
Note that both the Hypothesis Testing and the OM-LSA need a suitable esti-
mate of the noise power spectral density: the improved minima controlled recur-
sive averaging (IMCRA) algorithm [13] works well in non-stationary noise envi-
ronments and then selected here on purpose.
3
The NU-Tech Framework
NU-Tech [14] is a a software framework through which the user might develop
his own real-time (audio oriented) DSP applications and easily checking their
performances by just using the audio hardware resources of a common PC. It
oﬀers a low-latency interface to the sound card and a plug-in architecture. The
plug-in, called NUTS (NU-Tech Satellite), is a user-deﬁned C/C++ object to
be plugged-in within the Board. It follows that the user can program his own
algorithms and consider them as the operating blocks of the overall DSP applica-
tion designed on the Board, as shown in Fig. 3. Compared with Matlab or other
famous development platforms, NU-Tech is especially oriented towards audio
applications and thus oﬀers advanced capabilities of interfacing to most of the
commercial USB/FireWire Audio Interfaces. Therefore, the various algorithms
used in the system have been implemented in diﬀerent NUTSs and conveniently
linked to form the desired scheme. In this way, one may tests the system in terms
of quality and computational performance. Simulations, described in the next
section, have been carried out at a sampling rate of 16kHz on a general purpose
PC. The frame size and the length of the BSS ﬁlters have been ﬁxed at 2048
and 1024 samples, respectively. With these settings, real-time performance are
achievable at almost full CPU utilization. However, the computational overload
due to the MSD+PF stage is no more than 5-6% of the total cost.

A Blind Source Separation Based Approach for Speech Enhancement
363
Fig. 3. The NU-Tech board conﬁgured to realize the proposed system
4
Simulation Results
In this section we apply the proposed postﬁltering algorithm to the speech en-
hancement system and evaluate its performance. We assess either the initial
scheme [4], modiﬁed with the aforementioned broadband BSS algorithm, or the
proposed scheme integrating the third stage (MSD+PF). Moreover, each up-
date method for the ANC has been evaluated in both schemes and diﬀerent test
scenarios have been considered.
4.1
BSS – ANC Scheme Experiments
The ﬁrst simulations have been carried out considering the scheme of Fig. 4 and
two diﬀerent scenarios.
Fig. 4. BSS – ANC scheme
Scenario 1: Two speakers mixtures recorded in a reverberant room
The experiments for this scenario have been conducted using the same data of
[5] (available at http://www.lnt.de/~aichner/), containing sentences spoken
by a male and female speaker sampled at 16 kHz and recorded in a room having
a reverberation time of 50ms with a two-elements microphone array with an
inter a-element spacing of 20cm. Since the source signals are unknown and only
the mixtures are available, conventional performance measures, i.e. Segmental
SNR or Itakura-Saito [15], cannot be computed. However, an objective measure

364
A. Pignotti et al.
Fig. 5. MSC for Scenario 1
is the inverse of the magnitude squared coherence (MSC) [16], which, as afore-
mentioned, is strictly related to the cost function to be minimized by the BSS
algorithm. MSC is deﬁned as
|γxy(ω)|2 =
|Φxy(ω)|2
Φxx(ω)Φyy(ω)
(7)
The lower the MSC is, the better is the achieved separation degree of the outputs.
In this case, as shown in Fig. 5, the broadband BSS algorithm exhibits very good
performance, thus enabling the ANC to show its beneﬁcial eﬀects. Actually, the
improvement brought by the ANC seems to be little, due to the accuracy of the
BSS in this context. However, it is noticeable that the traditional LMS update
provides the poorest result (even worst than the BSS alone), while the other
methods slightly improve the overall output.
Scenario 2: Two speakers mixtures recorded in a car environment
As in the previous scenario, data provided by the authors of [5] have been used.
In this case, the two-element array was mounted at the rear mirror of a Skoda
Felicia car and recorded two speakers, one male and one female, sitting at the
Fig. 6. MSC for Scenario 2

A Blind Source Separation Based Approach for Speech Enhancement
365
driver and co-driver positions. The reverberation time was 50ms. Car noise,
recorded while driving through a suburban area at a speed of 60km/h, and
spatiotemporally uncorrelated white noise was used. The speech mixtures were
additively mixed with each noise type at an SNR of 0 dB. Figure 6 shows the
simulation results. Since the car noise exhibits diﬀuse sound ﬁeld characteristics,
Fig. 7. MSC for the proposed scheme using traditional LMS update
Fig. 8. MSC for the proposed scheme using the Sum method

366
A. Pignotti et al.
i.e., it is spatially correlated for low frequencies but uncorrelated for higher fre-
quencies, the beneﬁcial eﬀects of the ANC are more appreciable.
4.2
Proposed Scheme Experiments
For the sake of comparison and because they have the lower SNR, the proposed
scheme has been assessed using the same data of Scenario . From Fig. 7 and 8, the
improvements brought by the third stage (MSD+PF) come up. Moreover, the
objective results are conﬁrmed by informal listening test. The overall output is
slightly more distorted w.r.t. the BSS-ANC previous scheme but the background
noise is completely removed.
5
Conclusions
In this paper a novel speech enhancement scheme has been presented. It is a cas-
cade of diﬀerent algorithmic blocks: a BSS algorithm [5] as entry stage, followed
by a multichannel noise reduction system and then by an advanced post-ﬁlter
technique, the same developed in [8], [6]. As pointed out in [4], the separation
algorithm allows extracting single speech sources in very general acoustic con-
ditions even using few microphones, that is not really feasible by using common
beamforming techniques: in this case a more robust and performing BSS ap-
proach has been used w.r.t. the one in [4], as stated by related literature, and
also eﬃcient in terms of computational complexity, as conﬁrmed by accomplished
real-time implementation. The employment of the Multichannel Signal Detec-
tion an Post-ﬁltering algorithms represents another signiﬁcant innovation issue
of the present work, since it allows to further improve the achievable speech
enhancement at a low increment of needed computational resources. Several
computer simulations with real-world audio data have been performed: related
results, evaluated in terms of output coherence, have proved that the source sep-
aration degree and the noise impact decreases as we move along the architectural
cascade, conﬁrming the eﬀectiveness of the idea. Future developments could be
targeted to study an optimized joint parametrization of the diﬀerent block and
to test alternative options for each stage in order to augment the sound quality
of the outputs and likely reduce the computational burden: for instance, psy-
choacoustic concepts could be inserted within the overall scheme on purpose, as
done in [6]. Moreover, some work is actually on-going to employ the proposed ap-
proach as speech enhancement front-end in Human-Machine interaction systems
for realistic application scenarios.
References
1. Van Veen, B.D., Buckley, K.M.: Beamforming: A versatile approach to spatial
ﬁltering. IEEE Acoust., Speech and Signal Process. Magazine 5, 4–24 (1988)
2. Kellermann, W.: A self-steering digital microphone array. In: IEEE Int. Conf. on
Acust., and Signal Process, vol. 5, pp. 3581–3584 (1991)

A Blind Source Separation Based Approach for Speech Enhancement
367
3. Smaragdis, P.: Eﬃcient blind separation of convolved sound mixtures. IEEE Apps.
Signal Process. Audio Acoust., 19–22 (1997)
4. Low, S.Y., Nordholm, S., Togneri, S.: Convolutive Blind Signal Separation with
Post-Processing. IEEE Trans. on Speech and Audio Proc. 12, 539–548 (2004)
5. Aichner, R., Buchner, H., Yan, F., Kellermann, W.: A real-time blind source sepa-
ration scheme and its application to reverberant and noisy acoustic environments.
In: Aichner, R., Buchner, H., Yan, F., Kellermann, W. (eds.) IEEE Int. Conf. on
Acust., and Signal Process, Multimedia Communications and Signal Processing,
University of Erlangen-Nuremberg (Available online October 21, 2005)
6. Cifani, S., Principi, E., Rocchi, C., Squartini, S., Piazza, F.: A Multichannel Noise
Reduction Front-End based on Psychoacoustics for robust speech recognition in
highly noisy environment. In: Proc. of HSCMA 2008, Trento, Italy, May 6-8, pp.
172–176 (2008)
7. Greenberg, J.E.: Modiﬁed LMS Algorithms for Speech Processing with an Adaptive
Noise Canceller. IEEE Trans. on Speech and Audio Process. 6(4), 338–350 (1998)
8. Gannot, S., Cohen, I.: Speech enhancement based on the general transfer function
GSC and postﬁltering. IEEE Trans. on Speech and Audio Proc. 12(6) (2004)
9. Cichocki, A., Amari, S.: Adaptive Blind Signal and Image Processing: Learning
Algorithms and Applications. John Wiley & Sons, Inc., Chichester (2002)
10. Fancourt, C.L., Parra, L.: The coherence function in blind source separation of
convolutive mixtures of non-stationary signals. In: Proc. NNSP, pp. 303–312 (2001)
11. Buchner, H., Aichner, R., Kellermann, W.: A generalization of blind source sepa-
ration algorithms for convolutive mixtures based on secondorder statistics. IEEE
Trans. Speech Audio Process. 13(1), 120–134 (2005)
12. Buchner, H., Aichner, R., Kellermann, W.: A generalization of a class of blind
source separation algorithms for convolutive mixtures. In: Proceedings of the In-
ternational Symposium of Independent Component Analysis and Blind Signal Sep-
aration (ICA), Nara, Japan, April 2003, pp. 945–950 (2003)
13. Cohen, I.: Noise spectrum estimation in adverse environments: improved minima
controlled recursive averaging. IEEE Trans. on Speech and Audio Proc. 11(5)
(2003)
14. Squartini, S., Ciavattini, E., Lattanzi, A., Zallocco, D., Bettarelli, F., Piazza, F.:
NU-Tech: implementing DSP Algorithms in a plug-in based software platform for
Real Time Audio applications. Presented at the 118th AES Convention, Barcelona,
Spain, May 28-31 (2005)
15. Hansen, J.H.L., Pellom, B.L.: An eﬀective quality evaluation protocol for speech
enhancement algorithms. In: ICSLP 1998, paper 0917, Sydney, Australia, Novem-
ber 30 - December 4 (1998)
16. Carter, G.C., Knapp, C.H., Nuttall, A.H.: Estimation of the magnitude squared
coherence function via overlapped fast Fourier transform processing. IEEE Trans.
on Audio and Electroacoustics 21(4), 337–344 (1973)

Quantitative Analysis of the Relative
Local Speech Rate
Jan Janda
Czech Technical University in Prague, Faculty of Electrical Engineering,
Technicka 2, Prague 6, Czech Republic
jandaj2@fel.cvut.cz
Abstract. This paper deals with the immediate relative speech rate
analysis. It represents a design of the algorithm of its determination based
on the dynamic time warping (DTW) method. It also shows a practical
application of the relative speech rate determination in the case of some
pathological discourses. From the point of view of the speech rate there
are examined the discourses of the Parkinson’s disease’s patients, persons
suﬀering from stammering and patients after cochlear implantation.
1
Introduction
The speech rate (SR) is construed with the overall speed of the discourse pronun-
ciation. Global speech rate is often expressed by a number of words or syllables
per a minute [7]. It is stated [3] that in a common conversation speech a man
pronounces about 120 words per a minute. It appears that as a speech unit,
carrying the information on duration, it is a syllable [3]. Syllable modiﬁcation
within the SR change, however, is not a linear one. Syllable duration change
inﬂuences much more the duration of vowel segments that consonant ones.
In a number of cases from phoniatric and logopaedic practice, however, in
principle it is suitable to analyse the discourse speed relatively to another –
reference discourse [2]. This method could improve clinical diagnostic of some
dysarthric related diseases.
2
Algorithm for the Instant Relative Speech Rate
Determination
As the algorithm input there are any two diﬀerent discourses of the approxi-
mately same linguistic content (small deviations such as stutter are possible).
As the output there will be a curve of the immediate relative speech rate course.
Figure 1 illustrates the whole analysis procedure.
2.1
Pre-processing and Parameterization of Signals
After centralization and scalling of signals in the case of both the discourses we
perform a segmentation with the 25 ms hamming window with the 10 ms overlap.
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 368–376, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Quantitative Analysis of the Relative Local Speech Rate
369
Reference discourse
Analyzed discourse
Pre-processing and
segmentation
Pre-processing and
segmentation
Parameterization
Parameterization
Matrix of distances
in the parameter
space
Dynamic time warping
Derivation
Filtration
Speech rate curve
Fig. 1. Speech rate analysis ﬂow diagram
For the respective segments we will perform a parametrization either with ten
cepstral linear predictive coding (CLPC) coeﬃcients and also ten RASTA-PLP
(relative spectral transform - perceptual linear prediction)[1] coeﬃcients.

370
J. Janda
2.2
Distance Matrix in the Parameter Space
In this analysis phase a mutual similarity of the respective speech segments of
the measured a reference discourse will be quantiﬁed.
The mentioned-above speech signal parameterizations were designed in such
a way as to eliminate at best the diﬀerences between the speakers. In the case of
CLPC [3] coeﬃcients there is parameterized only the signal component modiﬁed
with the articulatory apparatus. In the case of RASTA-PLP [1],[6] with the help
of knowledge on the speech perception by a person we can focus even more on
the signal component carrying the linguistic information.
The Euclidean distance is a common measure of similarity. It’s used to quan-
tify the similarity between a segment of the analyzed discourse and a segment
of the reference discourse represented as parameter vectors of RASTA-PLP co-
eﬃcients. In the parameters space the Euclidean distances of all segment pairs
are computed and arranged in the distance matrix.
2.3
Dynamic Warping of the Time Axis (DTW)
Both discourses have the identical linguistic content, but a diﬀerent timing.
Thus if we have the well demarcated examined discourses, we know that the
ﬁrst and the last segments of the discourses will correspond one to another. We
can assign the other segments of the both discourses to one another by means
of the DTW function [3],[5], which we will ﬁnd as the optimum way within the
distance matrix.
As the optimum way we understand the one, minimizing the weighed sum of
values, through which it passes, and the route may be created only in such a
manner, so as each of the indexes of the visited point would be bigger at the most
by one value than the appropriate index of the preceding point. The diagonal
procedure is counted with a double weight.
2.4
Relative Speech Rate
The DTW curve has a character of the integral of the speed ratio analyzed and
reference discourses. Thus we are able to deﬁne the relative speech rate of two
linguistically identical discourses as the ﬁrst derivation of their DTW curve.
However, for performing the derivation it is necessary the explicit expression of
the curve. Otherwise the DTW curve diﬀerence would assume only values of 0,1 and
∞. Thus for DTW we make approximation of the explicit function in such a manner
that for every multiple-valued assignment of the analyzed discourse segment to the
reference discourse segment we assign the median of the segment indexes.
After performing the derivation the curve was smoothed using the moving av-
erage (kernel smoothing). As this algorithm output it is a curve expressing the
instant relative speed of the analyzed discourse as compared to the reference dis-
course.
Now we can test the algorithm in the discourse, which gradually loses a reg-
ular rhythm and accelerates. As the reference signal it is utilized the accurately
rhythmized discourse of a speech therapist. The DTW curve was computed both

Quantitative Analysis of the Relative Local Speech Rate
371
0
500
1000
0
500
1000
1500
DTW
500
1000
−1
−0.5
0
0.5
1
Analyzed discourse [segments]
x(n)
−1
0
1
500
1000
1500
x(n)
Reference discourse [segments]
CLPC
RASTA-PLP
Fig. 2. Comparison of the reference discourse (upper left) and the analyzed discourse
(lower right) using DTW curve (upper right)
for CLPC, and for RASTA-PLP parametrization (Fig. 2). Here is also indicated
the example of the mutually corresponding segments.
After a more detailed examination of the correspondence segments it’s obvi-
ous that in the case of the RASTA-PLP parameterization there less comes to
incorrect procedure of the DTW curve. As well as in the case of the other ana-
lyzed discourses stated in this work the RASTA-PLP provided the best results
from all the tested parameterizations. Thus we will further work exclusively with
the RASTA-PLP parameterization.
In Figure 3 we can already see the algorithm output (smoothed ﬁrst derivation
of the DTW curve). We interpret the result in such a manner that the ﬁrst
half of the discourse was spoken in an approximately identical rhythm as the
reference discourse (SR→1). Then in the second half of it the analyzed discourse
accelerates and its end was already spoken with 2.5 times speed as compared to
the reference discourse speed.
Horizontal line segment indicates the mean value of the speech rate
SRavr=1.29. As expected this value of the average speed corresponds to the
duration ratio of both the discourses.

372
J. Janda
1
2
3
4
5
6
7
x 10
4
0
0.5
1
1.5
2
2.5
3
3.5
−→n
Relative speech-rate
Speech rate of accelerating discourse
Fig. 3. Relative speech rate (n in samples)
3
Practical Utilization of the Relative Speech Rate
3.1
Analysis of the Dysarthric Speech of Parkinson’s Disease’s
Patients
Apart from a series of various examinations the patients having the diagnosti-
cated Parkinson’s disease undergo the dysarthric test 3F – dysarthric proﬁle.
In this test it is considered the patient’s ability to keep the rhythm within the
rhythmical text. Firstly the speech therapist reads for the patient the following
verse with the accurate rhythmization (without pauses between rows):
ˇZe to pivo nevypiju?
ˇZe to pivo vypiju!
ˇZe ten dˇzb´anek o ten tr´amek,
ˇze ho taky rozbiju.
Patient is asked to identify the rhythm and repeat the verse. Applying the
relative speech rate algorithm we are able to determine, how accurately the
patient identiﬁed the rhythm and if he/she was able to keep this rhythm for
the whole discourse duration. A slow dysarthric speech and tachyphenia
(precipitate speech) belongs to the most frequent motor symptoms in the early
stage of Parkinson’s disease.
The speech rate measurement was performed within the discourses of 16 pa-
tients. The control group was created by 14 persons of a similar age represen-
tation. In Figure 4 there are shown the examples of the immediate speech rate
courses for dysarthric and non-pathologic discourses. The Parkinson’s disease’s
patient’s discourses showed a higher average speed SRavr = 1.42. The speed sig-
niﬁcantly oscillated and its spread was SRvar = 0.148. On the contrary a ﬂuent

Quantitative Analysis of the Relative Local Speech Rate
373
1
2
3
4
5
6
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
1
2
3
4
5
6
7
8
9
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
Fig. 4. Speech rate of dysarthric (upper) and non-pathologic (bottom) discourses
discourse of a healthy man had SRavr = 1.08, the spread of SRvar = 0.004 and
very resembled with its rhythm to the model discourse of the speech therapist.
In the graphs there were indicated the divides between the respective verses
with vertical lines. In the case of a series of patients we can observe just within
these divides a drop of SR. To this situation there comes to in the cases, when the
patient deviates from a regular rhythm and he/she inserts the pauses between
the respective verses.
1
1.2
1.4
1.6
1.8
−0.05
0
0.05
0.1
0.15
Speechrate variance
Speech rate average
Dysarthric
Control group
Fig. 5. Summarize of the results of dysarthric test

374
J. Janda
Figure 5 summarizes the speech rate analysis results of the dysarthric dis-
courses of the Parkinson’s disease’s patients. It appears that the dysarthric
speech is rather faster and has irregular speech rate. The evaluation of numeral
characteristics of SR for the respective patients corresponded to the subjective
evaluation of the speech therapist.
Latency 0 ms
1
2
3
4
5
6
7
8
9
10
11
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
SRavr=0.93, SRvar=0.15
Latency 10 ms
2
4
6
8
10
12
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
SRavr=0.86, SRvar=0.19
Latency 40 ms
1
2
3
4
5
6
7
8
9
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
SRavr=1.2 SRvar=0.004
Fig. 6. Impact of delayed auditory feedback on SR

Quantitative Analysis of the Relative Local Speech Rate
375
3.2
Speech Rate Analysis of Stammer
We can also apply the SR algorithm for the analysis of impact of the Lee eﬀect
on persons suﬀering from stammering. The Lee eﬀect [8] consists in the fact that
the speaker hears his/her own voice with a certain delay. This eﬀect has often a
favourable impact on the stammering people’s speech.
We will try to ﬁnd the optimum delay value by means of the SR analysis. In the
case of the 110 ms delay the discourse is slow and balanced, we can determine it
as to be reference one. Relative SR for the selected delays are shown in Figure 6.
Signiﬁcant drops of SR for the delays 0 and 10 ms are just caused by stam-
mering. These drops start to lose from the 40 ms delay and then the discourses
are already ﬂuent and slower. As the optimum delay there appears the acoustic
feedback delay of 40 ms, which is uncommonly short delay [8].
From Figure 6 it is obvious that in the case of the stammering disappearance
the SR variance will substantially drop.
3.3
SR with the Persons After Cochlear Implantation
The cochlear implantation usually results in the speech change, too. The change
character may be dependant on many circumstances. Apart from other things,
for example, on the fact, what time period after the implantation we monitor
the patient’s speech, for what time period before the implantation the patient
was deaf, how did he/she perceive his/her voice before that operation and how
does he/she perceive it now.
If we have at our disposal the patient’s record before the implantation and
after it, we are able to follow the changes in the speech speed and ﬂuency by
means of the relative SR. Figure 7 shows the example of a 32 year old woman,
with whom after the loss of hearing due to meningitis there was performed a
cochlear implantation. Before the implantation the woman was deaf for one year
and now she is able to understand the speech without lipreading. SR shows that
now her speech is approximately 1.3 times faster and ﬂuent on the whole.
2
4
6
8
10
12
x 10
4
0
0.5
1
1.5
2
2.5
−→n
SR
Fig. 7. SR modiﬁcation after cochlear implantation

376
J. Janda
4
Conclusions
There was introduced a method of determination of the immediate relative
speech speed based on the DTW algorithm. The method enables a comparison
with the model discourse and may assist within the diagnosis of the Parkinson’s
disease in its early stage. Furthermore it is possible to utilize the method within
determination of the optimum value of the acoustic feedback delay within the
stammering persons’ therapy. The method can also serve for monitoring of the
speed change, for example, after the cochlear implantation.
Within the analysis of stammering speakers the algorithm functionality testi-
ﬁes a robust nature of this method. The calculation demands, however, limit its
utilization only for the discourses containing several sentences (larger discourses
can be splitted). In a number of cases the absence of the reference discourse may
be the obstacle for the application, but there is possible to create a reference
discourse post hoc, either by a professional speaker or even by speech synthesis.
In future this method will be counted for the examination of dependence of the
speech rate on the children’s age.
References
[1] Hermansky, H.: Perceptual Linear Predictive (PLP) Analysis of Speech. J. Acoust.
Soc. Am. 87(4) (1990)
[2] Ohno, S., et al.: Quantitative analysis of the local speech rate and its application
to speech synthesis. Dep. of Applied Electronics, Science University of Tokyo, 278,
Japan (1995)
[3] Psutka, J., et al.: Mluv´ıme s poˇc´ıtaˇcem ˇcesky. In: Spoken communication with
computer, Czech. Academia, Prague (2006)
[4] Uhl´ıˇr, J., Sovka, P.: ˇC´ıslicov´e zpracov´an´ı sign´al˚u. In: Digital signal processing. CTU
publishing, Prague (2002)
[5] Stan, S., Philip, C.: FastDTW: Toward Accurate Dynamic Time Warping in Linear
Time and Space. Intelligent Data Analysis 11(5), 561–580 (2007)
[6] Hermansky, H., Morgan, N.: RASTA processing of speech. IEEE Transactions on
Speech and Audio Processing 2(4), 578–589 (1984)
[7] Zellner, B.: Fast and Slow Speech Rate: a Characterisation for French. In: ICSLP,
5th International Conference on Spoken Language Processing, Sydney, Australia,
December 1998, vol. 7, pp. 3159–3163 (1998)
[8] Sark, S., Kalinowski, J., Stuart, A., Armson, J.: Stuttering amelioration at vari-
ous auditory feedback delays and speech rates. European Journal of Disorders of
Communication 31, 259–269 (1996)

Czech Spontaneous Speech Collection and
Annotation: The Database of Technical Lectures
Josef Rajnoha and Petr Poll´ak
Dept. of Circuit Theory, Czech Technical University, Prague
{rajnoj1,pollak}@fel.cvut.cz
Abstract. Applying speech recognition into real working systems, spon-
taneous speech recognition has increasing importance. For the devel-
opment purposes of such applications, the need of spontaneous speech
database is evident both for general design or training and testing of such
systems. This paper describes the collection of Czech spontaneous data
recorded within technical lectures. It is supposed to be used as a mate-
rial for the analysis of particular phenomena which appear within sponta-
neous speech but also as an extension material for training of spontaneous
speech recognizers. Mainly the presence of spontaneous speech phenom-
ena such as higher rate of non-speech events, changes in pronunciation,
or sentence irregularities, should be the most important contribution of
the collected database for the training purposes in comparison to the
usage of available read speech databases only. Speech signals are cap-
tured in two diﬀerent channels with slightly diﬀerent quality and about
14 hours of speech from 15 diﬀerent speakers are currently collected and
annotated. The ﬁrst analyses of spontaneous speech related eﬀects in the
collected data have been performed and the comparison with read speech
databases is presented.
1
Introduction
Current Automatic Speech Recognition (ASR) systems are used much more fre-
quently in the communication between a human and a machine. We can meet e.g.
voice controlled device operation, dictation machines, or any general recognition
of spoken speech for purposes of transcription of records, on-line TV subtitles,
etc. The speech at the input of such systems becomes more and more natural
and the ASR must deal with the eﬀects of spontaneous talk. Consequently, it
makes the recognition much more diﬃcult [1].
The ﬁrst issue which is usually met appears during the training of speech
recognizers. On the acoustic level, read speech databases are usually used for
the training of acoustic HMM models. As these databases are collected mainly
with this special eﬀort, they often contain phonetically balanced material. But
the level of speech disﬂuencies and other non-speech events typical for sponta-
neous utterances is rather small within these databases. Moreover, additional
non-verbal information comes with spontaneous speaking style, such as changes
in intonation or gestures. The need of the presence of spontaneous utterances
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 377–385, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

378
J. Rajnoha and P. Poll´ak
in the training databases is evident and the collection of spontaneous speech
corpora is necessary for this purpose [2, 3]. It was also proved by our ﬁrst ex-
periments with small vocabulary digit recognizer that modelling of non-verbal
events improved recognition accuracy [4]. To generalize this experiment for large
vocabulary spontaneous speech recognition in the future, it is another purpose
of this spontaneous speech collection.
As rather small amount of spontaneous speech data is currently available for
Czech language from publicly available sources and as we have the opportunity
of quite eﬃcient collection of spontaneous speech data with similar topics and
spontaneous speaking style, we have started collecting of Czech spontaneous
speech database of technical lectures which is described in this paper. Strong
eﬀort is paid to the annotation of the collected data, especially from the point
of view of precise description of non-speech events and other disturbing issues
typical for spontaneous speech appearing in collected utterances.
2
DSP Lecture Collection
Our current collection consists of the recordings captured within Digital Sig-
nal Processing (DSP) lectures at our department, containing periodic doctoral
reports and selected lectures of DSP-specialized courses. Each session concerns
about 20–30 minutes of speech on signal processing theme, which is usually pre-
pared in advance. The speech is then more ﬂuent and better pronounced while
the speaking style is still very spontaneous. The database involves single speaker
utterances rarely disturbed by the question from the audience or by an expected
answer to posed question.
Similar topics of the speech are very important for further testing of sponta-
neous speech recognition systems. This collection could extend currently avail-
able training databases which are important by involving phonetically balanced
read utterances on the other hand. Currently, we have the ﬁrst set of recordings
which consists of 3 lectures and 32 reports. It gives about 14 hours of spontaneous
speech from 15 diﬀerent male speakers.
3
Recording Facility
Commercial wireless recording system was chosen to capture speech (see Figure 1).
It gives the speaker a freedom of movement even in our case of two-channel record-
ing. The system is designed to provide a high quality signal with respect to re-
quested portability but also with the intention of possible connection to standard
PC system. Remote control center provides necessary monitoring and easy adjust-
ing the signal intensity in the case of possible saturation or low signal-level.
3.1
Hardware
Two-channelrecording is performed by wireless transmission sets from Sennheiser.
Each set consists of body pack transmitter, rack mountable receiver and omnidi-
rectional lapel microphone (EW112 G2 set) or super-cardioid headset microphone

Czech Spontaneous Speech Collection and Annotation
379
Fig. 1. Recording platform scheme with EW112 G2 and EW152 G2 wireless transmis-
sion system from Sennheiser
(EW152 G2). The lapel microphone is usually attached to the speaker’s shirt about
10 cm from mouth. It captures higher level of background noise due to its omnidi-
rectional receiving characteristics. The distance between the microphone and the
speaker’s mouth is also variable as the speaker turns his head to the table or presen-
tation slides and to the audience. It causes audible ﬂuctuations of signal intensity
in the ﬁnal record. The headset microphone is intended to capture high-quality
speech signal with maximum movement and close proximity signal reproduction.
Received signal is digitized in dual-input USB sound card E-MU 0404 which
enables direct sound-level manipulation for each channel and low-latency head-
phone monitoring. USB port provides possible connection to any standard PC
for data storage and signal intensity monitoring without using additional tools.
We store the signal with maximum quality. The recording set enables to cap-
ture two-channel signal with 48 kHz sampling rate and with 16 bits precision
per sample. This sampling frequency is suitable for integer-ratio resampling to
the most frequently used 16 kHz sampling rate. The setting of sound-level was
optimized within the ﬁrst records to have balanced signal intensity with low
appearance of saturation for loud events and audible signal for low speech level.
3.2
Software
The recording is performed using freely available software WaveLab Lite included
in sound card package. It enables full dual channel recording support, i.e. data
input monitoring and mastering (e.g. adjusting important points in the wave
during recording) with minimum other superﬂuous functionalities.
On the other hand, no support is available from the point of view of speech
database organization. Each further information, including signal segmentation
and transcription is done later independently of the recording.

380
J. Rajnoha and P. Poll´ak
4
Signal Segmentation and Annotation
The presentations are recorded always at once and these several minutes long
waveforms need to be segmented to shorter parts and annotated. The ﬁrst record-
ings have been segmented and annotated fully manually but we suppose to use
some automated or semi-automated procedure for the next collection for saving
of hard manual work.
Freely distributed speech segmentation, labeling, and transcription software
Transcriber (distributed by LDC [5]) is used for signal segmentation and manual
dataset annotation. It gives full support for the ﬁrst steps in speech corpus
creation. The required steps of signal processing and annotation are described
in the following sections.
4.1
Segmentation
Each utterance is divided into sentences, which are the most suitable form for
the next processing and for training of ASR system. As the transcription is not
known, the start- and end-points of the sentences are found manually.
As shown in [1] the start-point and end-point of the spontaneous sentences can
be very diﬀerent from ﬂuent read speech. False starts, repairs, and repetitions
can appear and the sentence can be also very long. Moreover, the speech can be
silent at the end of the sentence. Important parts of speech can be “hidden”in the
environmental noise, but it is important to keep this information in the correct
segment. Regarding the mentioned eﬀects, the signal is segmented with special
eﬀort to correct placement of the sentence boundaries. Short part of the signal
with non-speech information is kept at the beginning of each sentence if it is
possible. It could be useful in further noise compensating algorithms.
Longer segments without speech activity between two separate blocks of
speech are cut and marked as pause segments. The original long recordings are
kept in the database for the purposes of further research.
4.2
Orthographic Transcription
The speech content is transcribed in the form of orthographic annotation. As in
other database projects ([3], [6]), standardized rules are used for the annotation.
The transcription procedure is divided into several steps.
Only speech content is transcribed in this ﬁrst annotation step and other eﬀects
are omitted. Punctuation is not noted as it is not important for training a phoneme
model-based speech recogniser. A lower-case form is used for all characters.
The speech is rewritten in the form it is exactly spoken, even in the case
of colloquial language or mathematical expressions, which are typically present
in technical speech. Special transcription is used for spelling, mispronunciations
and foreign words (see Table 1).
Small changes in pronunciation are not transcribed as they are supposed to be
covered by the variability of the HMM modelling of elementary phonetic elements.
The content of collected utterances is not known in advance and as it can be
quite unusual many times, some words may be often diﬃcult to be recognized.

Czech Spontaneous Speech Collection and Annotation
381
Table 1. Typical eﬀects in spontaneous speech and their annotation
Event
Transcription
spelled sounds
‘$’ preﬁx and correct pronunciation variant
for given sound
mispronunciations,
‘*’ preﬁx character
small mistakes
strong mispronunciation
‘**’ mark
foreign words
‘∼’ preﬁx character
The annotation is therefore checked by another annotator more than once to
guarantee the correct transcription as much as possible.
4.3
Phonetic Transcription
Phonetic transcription is not part of the annotation. It can be automatically
generated by tool transc [7] in the next annotation phase. This tool generates the
phonetic transcription from orthographic transcription on the basis of grapheme-
to-phoneme conversion rules.
Exceptions in pronunciation are covered by special dictionary or by the an-
notation of special pronunciation as noted within orthographic transcription.
More speciﬁc pronunciation irregularities can be also marked in the form (word/
pronunciation) which deﬁnes correct pronunciation variant for the given word.
4.4
Non-speech Event Annotation
Spontaneous speech diﬀers strongly from read speech mainly by the fact that
speakers often need to think about succeeding words. It causes much more silent
pauses, lengthenings, or ﬁlled pauses in such speech. These eﬀects are marked
together with other environmental events in the next annotation step. As the
receiving characteristics of particular microphones are diﬀerent, also the tran-
scription of non-speech events can diﬀer slightly for particular channels.
Non-speech events are divided into several classes according to Table 2.
Table 2. Description of annotated non-speech events
Mark
Description
Speaker-generated events
[mlask]
lip smack
[dech]
breath
[fil]
ﬁlled pause
[smich]
laugh
[ehm]
throat clear
[kasel]
cough
Mark
Description
Other speaker distortions
[cockt]
cocktail-party eﬀect
[other]
other speaker
Background noise
[sta]
stationary noise
[int]
non-stationary interruption

382
J. Rajnoha and P. Poll´ak
– Speaker-generated non-speech events – As they are produced by the speaker
the same way as speech, speaker-generated non-speech events always occur
between words. They can be therefore annotated as another word, using a key
word in square brackets (e.g. ‘word1 [fil] word2’). Used speaker-generated
events which appear typically in spontaneous speech are listed in Table 2.
– Background noise events –
Even the speech is recorded in quiet environ-
ment, it can still contain disturbing noise which must be annotated. But
environmental distortion can overlap particular words so special rules are
used for better description of noise disturbance within the speech. If the
noise appears only in the pause between words, it is marked similarly to
speaker-generated events. When the following speech is aﬀected, starting
and ending mark is used, e.g. “word1 [int−] word2 word3 [−int] word4”.
This convention corresponds to rules used in transcription tool Transcriber.
– When “[sta]” mark is used without beginning and ending mark, it should
be placed at the beginning of the utterance transcription and it means the
presence of stationary noise in whole signal.
– Other speaker in background –
As the speech is recorded within lectures,
the audience present in the room can inﬂuence the speech and distortion
from other speaker can be present. We resolve two diﬀerent situations (see
Table 2), either the distortion appears within speech pause ([other]) or more
speakers are talking simultaneously ([cockt]).
5
Dataset Analysis
The ﬁnal structure of the database was deﬁned within the processing of the ﬁrst
data. It involves long recordings, cut segments with particular sentences, ortho-
graphic transcription of speech content with possible irregular pronunciation and
non-speech event description. This section provides general comparison of col-
lected spontaneous database with other available read speech databases. Even
the amount of currently available spontaneous data is rather small, it describes
main attributes of the collection and overall character of collected speech.
Currently transcribed part of the database contains 7830 diﬀerent words from
total amount of 63000 words. The spontaneous character of the speech is evident
from the occurrence of colloquial and slang words in comparison to standard writ-
ten corpora (4.6 % of colloquial words against e.g. 0.08 % of these words in Czech
LC-Star2 [8]). The ﬁnal set of words contains about 21.2 % of topic-related words.
5.1
The Speech Intelligibility
Table 3 presents the comparison of the amount of correct, mispronounced and
unintelligible words in diﬀerent speech corpora (the percentage is related to the
amount of all words in the database). ’SPEECON‘ and ’CZKCC‘ are read speech
databases, ’Lectures‘ is our spontaneous speech collection.
The occurrence rate of words with small mispronunciation in the spontaneous
database is comparable to large read speech collections. But the rate of mis-
pronounced words is higher. It is caused mainly by the eﬀect of repetitions and

Czech Spontaneous Speech Collection and Annotation
383
Table 3. Word distribution in particular databases
database
words
mispronunciations
unintelligible/incomplete words
SPEECON
561 716
1157
(0.21 %)
1768
(0.31 %)
CZKCC
1 067 412
1689
(0.16 %)
1902
(0.18 %)
Lectures
63 000
85
(0.14 %)
445
(0.71 %)
repairs which interrupt speech in the middle of a word. On the other hand, de-
spite the spontaneous character of the utterances in our new database, the rate
of mispronunciations is still rather small and collected speech seems to be good
material for training purposes.
5.2
Occurrence of Non-speech Events
The presence of non-speech events in training databases is important for robust
ASR systems. Cleared read speech databases were compared in terms of non-
speech event occurrence. Due to signiﬁcant diﬀerences in speech content, the
part of SPEECON database which contains spontaneous utterances was analysed
separately from the read speech subset for this purpose.
Tables 4 and 5 show the amount of non-speech events marked by the human
annotator (percentage is again related to the amount of words in the fragment
of particular database). We use more precise description of non-speech events in
our currently created database, but other databases use simpler categorization.
Simpliﬁed two classes of speaker non-speech events (ﬁlled pause, other event)
were therefore analyzed.
Table 4. Occurrence of ﬁlled pauses in inspected databases
database
words
ﬁlled pauses
SPEECON read
146537
344
(0.23 %)
SPEECON spont.
34954
1512
(4.33 %)
CZKCC
244044
153
(0.06 %)
Lectures
54314
1449
(2.67 %)
It can be seen in Table 4 that spontaneous collections contain signiﬁcantly
higher rate of ﬁlled pauses than read utterances as it is typical for spontaneous
speech [9]. On the other hand, spontaneous speech is more ﬂuent and without
longer pauses present during recordings of separated read utterances. They are
frequently followed by lip smack and audible breath. The occurrence of other
events is therefore lower for spontaneous speech (see Table 5).
The inﬂuence of recording conditions, mainly chosen microphones and their
position, or further background environment also yield to diﬀerent rates of non-
speech events in compared databases. Moreover, this diﬀerence can be aﬀected
slightly by the annotation inconsistency [10]. Finally, it causes signiﬁcant diﬀer-
ence also between both read speech databases.

384
J. Rajnoha and P. Poll´ak
Table 5. Occurrence of other speaker-generated events in inspected databases before
and after forced-alignment
database
words
other non-speech events
annotated
aligned
SPEECON
181517
33125
(18.25 %)
29934
(16.49 %)
CZKCC
244044
15728
(6.44 %)
9941
(4.07 %)
Lectures
54314
203
(0.37 %)
134
(0.25 %)
Due to the fact mentioned above, it is reasonable to mark only signiﬁcant non-
speech events for modelling purposes. Forced-alignment commonly with reached
acoustic score analysis was used to reduce the occurrence of inexpressive non-
speech events. Table 5 shows the number of retained events for all 3 databases.
Such corrected data are then supposed to represent better material for further
training.
6
Conclusions
The paper presented the collection of Czech spontaneous database. The most
important contributions are summarized in following points.
– Recording scenarios and recording platform for creation of Czech sponta-
neous database of lectures were deﬁned and the ﬁrst sessions were collected.
Currently, the collection contains about 14 hours of spontaneous speech and
the recording continues. The ﬁnal structure of the database involves whole
long recordings, segmented signal cut to particular sentences, commonly with
orthographic transcription with precise annotation of non-speech events.
– Annotation conventions for orthographic transcription of spontaneous speech
were designed and the ﬁrst data were annotated. Extended set of non-speech
events was deﬁned to describe speaker-generated and environmental non-
speech events more precisely.
– According to our assumption, the ﬁrst analyses showed higher rate of slang
and colloquial words in comparison to standard written corpora. We have
observed approximately 21.2 % of topic-related and 4.3 % of colloquial words
in the presented spontaneous collection. Also the rate of interrupted or un-
intelligible words is slightly higher in comparison to standard read speech
collections. Nevertheless, the speech ﬂuency is still high and the data are
suitable for further usage.
– The re-alignment procedure decreased the amount of inexpressive non-speech
events in the collected data. Consequently, more precise modelling of
non-speech events is supposed to be achieved when these data are used for
training purposes.
– Our preliminary experiments on ﬁlled-pause recognition showed signiﬁcant
contribution of proposed spontaneous database in non-speech event recogni-
tion task. Using our database for training, the insertion error rate decreased

Czech Spontaneous Speech Collection and Annotation
385
by approx. 80% against the case with read speech training data. Having
spontaneous speech data from presented database, the full application of
non-speech event modelling into spontaneous speech recognizer can be now
the next step of our activities.
Acknowledgements
The research was supported by grants GAˇCR 102/08/0707 “Speech Recogni-
tion under Real-World Conditions”, GAˇCR 102/08/H008 “Analysis and mod-
elling biomedical and speech signals”, and by research activity MSM 6840770014
“Perspective Informative and Communications Technicalities Research”.
References
[1] Shriberg, E.: Spontaneous speech: How people really talk, and why engineers
should care. In: Proc. Eurospeech 2005, Lisbon, Portugal, pp. 1781–1784 (2005)
[2] Trancoso, I., Nunes, R., Neves, L., Viana, C., Moniz, H., Caseiro, D., Mata, A.I.:
Recognition of classroom lectures in european Portuguese. In: Proc. Interspeech
2006, Pittsburgh, USA (2006)
[3] Psutka, J., Radov´a, V., M¨uller, L., Matouˇsek, J., Ircing, P., Graﬀ, D.: Large
broadcast news and read speech corpora of spoken Czech. In: Proc. Eurpospeech
2001, ˚Alborg, Denmark, pp. 2067–2070 (2001)
[4] Rajnoha, J., Poll´ak, P.: Modelling of speaker non-speech events in robust speech
recognition. In: Proceedings of the 16th Czech-German Workshop on Speech Pro-
cessing, Academy of Sciences of the Czech Republic, Institute of Radioengineering
and Electronics, Prague, pp. 149–155 (2006)
[5] Barras, C., Geoﬀrois, E., Wu, Z., Liberman, M.: Transcriber: A free tool for seg-
menting, labeling and transcribing speech. In: Proc. of the First international con-
ference on language resources & evaluation (LREC), Granada, Spain, pp. 1373–
1376 (1998)
[6] Poll´ak, P., ˇCernock´y, J.: Czech SPEECON adult database (November 2003),
http://www.speechdat.org/speecon
[7] Poll´ak, P., Hanˇzl, V.: Tool for Czech pronunciation generation combining ﬁxed
rules with pronunciation lexicon and lexicon management tool. In: Proc. of LREC
2002, Third International Conference on Language Resources and Evaluation, Las
Palmas, Spain (May 2002)
[8] LC-STAR II project site, http://www.lc-star.org/
[9] Gaji´c, B., Markhus, V., Pettersen, S.G., Johnsen, M.H.: Automatic recognition of
spontaneously dictated medical records for Norwegian. In: COST 278 and ISCA
Tutorial and Research Workshop - ROBUST 2004 (2004)
[10] Rajnoha, J.: Speaker non-speech event recognition with standard speech datasets.
Acta Polytechnica 47(4-5), 107–111 (2008)

BSSGUI – A Package for Interactive Control of
Blind Source Separation Algorithms in
MATLAB
Jakub Petkov and Zbynˇek Koldovsk´y
Faculty of Mechatronics, Informatics and Interdisciplinary Studies,
Technical University of Liberec, Studentsk´a 2,
461 17 Liberec, Czech Republic
Abstract. This paper introduces a Matlab graphical user interface
(GUI) that provides an easy operation of several Blind Source Sepa-
ration (BSS) algorithms together with adjustment of their parameters.
BSSGUI enables working with input and output data, multiple signal
plots, and saving of output variables to the base Matlab workspace or to
a ﬁle. The Monte Carlo Analysis allows for the validation of particular
features of BSS algorithms integrated into the package. The BSSGUI
package is available for free at http://bssgui.wz.cz.
1
Introduction
Blind Source Separation (BSS) has become a very popular branch of signal pro-
cessing in last two decades. It deals with the signal separation. The term blind
says that the goal is to restore unknown original sources mixed in an unknown
mixing system. The only available data are the mixed signals measured by sev-
eral sensors. The blind separation can thus be very useful in various applications
where multidimensional signals are observed, namely, in audio (speech) separa-
tion with multi-microphone systems, hands-free applications, biomedical signal
analysis, etc.
Since many real-world signals coming from diﬀerent sources can be assumed
to be independent, Independent Component Analysis (ICA) is a popular method
for doing the blind separation [1,2,3]. The underlying model considered in ICA
is the instantaneous mixing model described by
X = AS,
(1)
where A is a d×d regular mixing matrix, and X and S are d×N matrices whose
rows contains samples of mixed and original signals, respectively. Algorithms
designed for separation of instantaneous mixtures are usually used as a hearth
of methods for separating convolutive mixtures that are met in many real-world
applications such as blind separation of audio signals [4].
In this paper, we present a software package that we developed for convenient
usage of several ICA methods implemented in Matlab. The main motivation for
completing the package can be summarized in two basic aspects:
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 386–398, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

BSSGUI – A Package for Interactive Control of BSS Algorithms
387
1. We found useful to create a collection of several eﬃcient algorithms for
ICA/BSS using diﬀerent models of signals.
2. Many algorithms are provided on the internet in Matlab codes. According
to the need for comfortable use of the algorithms, we developed an intuitive
and user friendly graphical user interface for their control.
An overview of the methods that were integrated into the package is given in
the following section. Then, Section 3 contains important details about the user
interface of the application. In Section 4, we describe a Monte-Carlo procedure
that allows testing of the algorithms. Some examples are given in Section 5.
2
BSS Algorithms
There are three basic classes of ICA methods considered in the literature that
diﬀer in modeling of the original signals, which also means the diﬀerence in
separation principles. The methods usually rely either on non-Gaussianity, non-
stationarity, or spectral diversity of signals, but, recently, combined models have
been considered also. In the following, we will shortly describe the integrated
algorithms into BSSGUI. Since the algorithms are mathematically quite com-
plex, we refer the reader to corresponding papers. The integrated algorithms
are, respectively, known under acronyms EFICA, WASOBI, BGL, Block EFICA,
BARBI, MULTI-COMBI, COMBI, and FCOMBI.
In BSSGUI, the EFICA algorithm [5] represents methods that assume non-
Gaussianity of the original sources. In fact, EFICA is a modiﬁcation of the
popular FastICA algorithm [6]. Here, the separation consists in ﬁnding uncorre-
lated signals that have minimum entropy under unit variance constraint, which
is equivalent with the maximum likelihood estimation or minimization of the
mutual information of signals. Minimization of the entropy requires evaluation
of higher-order statistics, in FastICA, represented by a nonlinear function. How-
ever, the best possible accuracy of the separation can be achieved if only the
nonlinear function corresponds with true score function(s) of the original sig-
nals. It must be thus known in advance or should be consistently estimated, at
least. In EFICA, therefore, an adaptive choice of the nonlinearity is done.
The second class of ICA methods model original signals as weak-sense sta-
tionary Gaussian processes. Separation is based on approximate joint diagonal-
ization (AJD) of several cross-correlation matrices at selected time lags. These
methods are in BSSGUI represented by the WASOBI algorithm [7], which is a
weight-adjusted version of the earlier SOBI algorithm [8]. The implementation
of WASOBI in BSSGUI comes from [9].
The nonstationarity-based methods are represented by the BGL algorithm
(Block Gaussian Likelihood) [10] that models each original signal as a sequence
of independent Gaussian variables having diﬀerent variance in diﬀerent time-
blocks. The separation consists in dividing the mixed signals into a number of
non-overlapping segments, computing signal covariance matrices on each seg-
ment, and doing AJD of these matrices. In BSSGUI, we use the fast BGL im-
plementation from [11].

388
J. Petkov and Z. Koldovsk´y
An appealing feature of the above methods is that they are asymptotically
eﬃcient within their models under respective circumstances. Hence, the methods
usually achieve a more precise separation then other competitive algorithms.
The same property possess further algorithms included in BSSGUI, namely, the
Block EFICA algorithm [12], and the BARBI algorithm [13]. The former method
eﬀectively combines the non-Gaussianity and the nonstationarity, while the latter
one combines the nonstationarity with the spectral diversity. Next, theoretical
performances of the methods mentioned up to now in terms of the interference-
to-signal ratio are known and could be estimated after the separation using
statistics of separated signals.
Finally, BSSGUI involves three algorithms that take into account both the
non-Gaussianity and the spectral diversity of signals. The methods, COMBI
[14], MULTI-COMBI [15], and FCOMBI [16], are, in fact, diﬀerent decision-
driven procedures combining separation via EFICA and WASOBI (using the
aforementioned post-estimates of the achieved interference-to-signal ratio). The
methods are not claimed to be optimum within the combined model of signals,
nevertheless, they achieve good performance thanks to eﬃciency of theit build-
ing blocks: EFICA and WASOBI. Compared to COMBI, MULTI-COMBI uses
a multi-component approach allowing to separate clusters of signals that are not
separable either by WASOBI or EFICA. Then the clusters of signals are sepa-
rated by the second method. FCOMBI is a faster alternative of MULTI-COMBI
at an aﬀordable loss of accuracy.
3
Graphical User Interface
BSSGUI is provided in a package containing M-ﬁles of all controlled BSS algo-
rithms and supporting ﬁles. The GUI was designed in/for MATLAB 7.1 Release
R14SP3. A main window of the application (see Figure 1) can be loaded by
executing the command bssgui.
Fig. 1. Main window of the application

BSSGUI – A Package for Interactive Control of BSS Algorithms
389
3.1
Input and Output Data
The BSSGUI package works with input signals of the same length organized into
rows of a matrix that is saved in the base workspace of Matlab. It can be loaded
by pressing the Load data button.
There are two main outputs of the separation. The ﬁrst one is the de-mixing
matrix W which separates the input mixed data X, i.e., S = WX, where S is the
matrix of the separated signals. The second output is the d×d ISR matrix whose
ijth element is an estimate of the residual Interference-to-Signal ratio between
the ith and the jth separated signal. The estimate is based on theoretical analysis
of the selected algorithm as mentioned in Introduction. Accuracy of this estimate
can be validated by the Monte-Carlo Analysis presented by Section 4.
All results can be optionally saved as variables to the base workspace of Mat-
lab or to a user-selected ﬁle. For better notion and objective approach of a sepa-
ration, BSSGUI provides a graphical display (multiple signal plots) of the input
or output data matrices.
3.2
Settings of Algorithms
Each algorithm of the BSSGUI can be adjusted by several parameters whose
values can be selected in the BSS characteristics panel. This section gives a brief
overview of the most important ones.
EFICA. The most detailed settings are available for the EFICA algorithm,
which works with non-Gaussian signals. The possible settings correspond with
the fact that the method runs in three consecutive steps.
In the ﬁrst step, an initial estimate of the de-mixing matrix is done via the
Symmetric FastICA algorithm [6]. The algorithm is initialized by the matrix
given by the parameter Starting point, and it iterates until a stopping cri-
terion becomes lower than the Epsilon parameter or the maximum number of
iterations given by MaxIt is reached. A nonlinear function that approximates
true score functions of the original signals is selected through the parameter
Nonlinearity. Note that the popular hyperbolic tangent function can be re-
placed by a rational function whose evaluation is usually faster [20].
The ﬁrst step of EFICA is followed by the optional Test of saddle points
that signiﬁcantly improves global convergence of the algorithm at an aﬀordable
computational burden. The test can be allowed by the SaddleTest parameter.
If a local or saddle-point convergence is detected by the test, further iterations
of appropriately initialized Symmetric FastICA are done, whose maximum value
is given by MaxItAfterST.
The last step of EFICA does ﬁne-tunings and a reﬁnement to improve the
accuracy of the ﬁnal separation as much as possible. The FineEpsilon and
FinetuneMaxIt parameters have analogous purpose as Epsilon and MaxIt in
the ﬁrst step of EFICA, respectively, because each ﬁne-tuning is done via one-
unit FastICA [6,5] iterations with an adaptively selected nonlinear function.
The most important parameters of EFICA can be adjusted in the BSS char-
acteristics panel, while the others can be selected by pressing the Advanced...
button; see Table 1 for an overview of the parameters.

390
J. Petkov and Z. Koldovsk´y
Table 1. EFICA parameters
Parameter
Purpose: options
SaddleTest
enable/disable
the
Test
of
saddle
points:
true/false
Nonlinearity
nonlinear function in Symmetric FastICA: rat1,
rat2 - rational functions [20], tanh - hyperbolic
tangential, gaus - gaussian function
Starting point initialization of Symmetric FastICA: randn(dim)
- N(0, 1)-random matrix, eye(dim) - the identity
matrix
Epsilon
a stopping criterion of Symmetric FastICA
FineEpsilon
a stopping criterion in ﬁne-tunings
MaxIt
maximum number of Symmetric FastICA itera-
tions
MaxItAfterST
maximum number of iterations after a saddle
point was indicated
FinetuneMaxIt
maximum number of ﬁne-tuning iterations (for
each separated source)
WASOBI. The WASOBI algorithm allows to select two parameters. The ﬁrst
one is called ARorder. It represents an order of the auto-regressive model of
the original signals. This value is equivalent to the number of cross-correlation
matrices for the approximate joint diagonalization.
The second parameter, named Stabilization, is due to an elimination of
a possible instability of the AR processes. It therefore corresponds to the max-
imum absolute value of the poles of the AR models. Its value can be taken
from (0, 1⟩, where the value one turns the stabilization oﬀ. An eﬀective choice is
generally a value that is close to one.
COMBI, MULTI-COMBI, FCOMBI.
As mentioned above, COMBI,
MULTI-COMBI and FCOMBI are methods combining EFICA and WASOBI.
Therefore, they use the deﬁned settings of the algorithms.
BGL, Block EFICA, BARBI. The common parameter of these algorithms
is the number of blocks, called Segments, as they consider the nonstationarity
of signals. While the BGL algorithm does not need any additional parameters
since it utilizes the nonstationarity only, the BARBI algorithm has also the
parameters as WASOBI due to AR modeling of each block of a signal. Similarly,
the Block EFICA algorithm inherits the parameters of EFICA, because it takes
the non-Gaussianity of signals into account.
3.3
ISR Matrix
Let W be the separating matrix obtained by an algorithm. The separated signals
are then
S = WX = WAS.

BSSGUI – A Package for Interactive Control of BSS Algorithms
391
Hence, the ijth element of WA, i ̸= j, correspond to residual interference be-
tween the ith and jth separated signal. The ijth element of the ISR matrix is
therefore deﬁned as
ISRij = WA2
ij
WA2
ii
,
i ̸= j.
(2)
To make the reading of ISR matrices more understandable, BSSGUI displays its
values in logarithmic scale (in decibels; see Figure 2). Diagonal elements of the
ISR matrix do not carry any information and are displayed in white color.
Fig. 2. An example of ISR matrix of four separated signals
4
Monte Carlo Analysis
This function of the BSSGUI provides a tool for evaluating performance of algo-
rithms when signals of a given probabilistic-model structure are separated. The
signals are generated according to a parametrized model (described in an extra
subsection below), mixed, and separated in a selected number of independent tri-
als. Finally, the resulting empirical ISR matrix of the separated signals averaged
over the trials is shown, which provides evaluations of the separation results.
Simultaneously, a theoretical estimate of the ISR matrix is shown. This esti-
mate is computed using a formula corresponding to the selected algorithm, which
was previously derived through a theoretical performance analysis. In evaluation,
statistics of the separated signals are used. Comparison of the theoretical esti-
mate of the ISR matrix with the true one gives evaluation of correctness of the
estimate. The estimate should be “good” if the signals obey the model considered
by the selected algorithm. On the other hand, one can consider its usefulness if
the data have diﬀerent structure. The theoretical estimate is not available for
COMBI and MULTI-COMBI algorithms.
Pressing the MC-Analysis... button, the GUI executes a dialog window contain-
ing settings of parameters of the Monte Carlo Analysis that considers the selected
algorithm in the main window. The main parameters are Number of components,
Data length, which sets the length of generated signals, and Number of trials.
Note that the resulting empirical ISR matrix is less inﬂuenced by random errors
when a suﬃcient number of trials (usually about 100) is selected. All the param-
eters mentioned below and above are optional. If they are not set by the user,
BSSGUI takes automatically their default values.

392
J. Petkov and Z. Koldovsk´y
Fig. 3. MC-Analysis window
The Run button starts the trials of the Monte Carlo Analysis. Each trial
proceeds in the following steps:
1. Signals generating - the matrix of original independent signals S is gen-
erated as will be described in the subsection below.
2. Mixing process - a random mixing matrix A is generated, and the original
signals S are mixed according to (1), i.e., X = AS.
The Matlab code is as follows:
A=randn(dim);
X=A*S;
where dim is the selected number of the original signals, i.e., the Number of
components.
3. Separation - the mixed signals X are separated by the selected algorithm,
which yields the estimated de-mixing matrix W and the theoretical estimate
of the ISR matrix.
4. Permutation correction - rows of the de-mixing matrix W are permuted
so as the matrix WA is as much close to diagonal as possible. This is done
using the procedure from [21].
5. Evaluation - the true (empirical) ISR matrix is evaluated according to (2).
The Matlab code is as follows:
G=W*A;
ISR=(G-diag(diag(G))).^2./...
(diag(G).^2*ones(1,dim));
After repeating the process Number of trials-times, the empirical and the-
oretical ISR matrices, averaged over the trials, are displayed.

BSSGUI – A Package for Interactive Control of BSS Algorithms
393
4.1
Signal Generator
The built-in generator of BSSGUI is able to generate signals embodying both
the nonGaussinity, nonstationarity and temporal structure. This is achieved by
1. splitting each signal into selected number of blocks, and
2. generating each block of signal as an ARMA stationary process feeded by an
i.i.d. sequence having Generalized Gaussian Distribution (GGD) with shape-
parameter α (for GGD description see, e.g., Appendix B in [5]) and variance
σ2.
Namely, a block of a signal is generated by Matlab command
sigma*filter(B,A,gengau2(alpha,1,N));
where gengau2 is the generator of GGD with unit variance from [22], alpha is
the shape parameter of GGD whose values are, in practice, from (0, 10], N is
the length of the block in samples, and B and A are vectors of coeﬃcients of the
ARMA process. The parameter sigma corresponds with σ.
Pressing the Advanced... button enables to set characteristics of each gen-
erated signal given by the parameters alpha, B, A, and sigma (see Figure 4).
While alpha is a scalar, the latter three parameters are vectors whose lengths are
Fig. 4. Advanced data settings window
0
200
400
600
800
1000
−10
−5
0
5
10
Fig. 5.
An
example
of
generated
signal
by
Matlab
command
s=filter([1
1],1,gengau2(1,1,1000)), which corresponds with selection of parameters alpha=1,
B=[1 1], A=1, and the variance parameter is 1

394
J. Petkov and Z. Koldovsk´y
selected by the user. An example of selection of the sigma parameter is as follows:
When setting in the variance-textbox values 1 2 3 4, the corresponding signal
will be divided into four blocks of the same length, where the value of sigma in
each block is 1,
√
2,
√
3, 2, respectively. See further examples in Figure 5 and 6.
0
200
400
600
800
1000
−40
−20
0
20
40
Fig. 6. A signal generated with the same parameters as in Figure 5, but with the
variance parameter [30 1 10]
5
Examples
5.1
Demo Signals
BSSGUI allows to validate the separation with demo signals. Pressing the Use
demo button in the Load data window generates a matrix of mixed signals con-
sisting of the three following components of length 500 samples. See Figure 7.
0
100
200
300
400
500
−1
−0.5
0
0.5
1
(a) Sinewave
0
100
200
300
400
500
−1
−0.5
0
0.5
1
(b) Rectangle
0
100
200
300
400
500
−2
−1
0
1
2
(c) Sawtooth
Fig. 7. The original demo signals

BSSGUI – A Package for Interactive Control of BSS Algorithms
395
Generated signals are stored in a matrix S
S(1,:)=sinewave;
S(2,:)=rectangle;
S(3,:)=sawtooth;
and mixed by a random 3 × 3 mixing matrix A representing the mixing system.
This procedure creates three mixed signals X (see Figure 8) that can be separated
using BSSGUI.
0
100
200
300
400
500
−2
−1
0
1
2
0
100
200
300
400
500
−2
−1
0
1
2
0
100
200
300
400
500
−4
−2
0
2
4
Fig. 8. Signals mixed after pressing the Use demo button
5.2
Monte Carlo Analysis of EFICA
As mentioned earlier in the paper, the EFICA algorithm assumes non-Gaussian
signals. Using the MC-Analysis tool in BSSGUI, we considered three signals of
length 1000 samples in 10 trials. The signals were generated according to GGD
law with alpha equal, respectively, to 1, 2 and 2, then ﬁltered with coeﬃcients A
and B as given below. Note that the second and the third signal have Gaussian
distribution (alpha=2), which means that they cannot be separated one from
the other by EFICA.
The generation process runs following commands
S(1,:)=filter(1,[1 0.4],gengau2(1,1,1000));
S(2,:)=filter(1,[1 0.7],gengau2(2,1,1000));
S(3,:)=filter(1,[1 -0.4],gengau2(2,1,1000));
The results of the analysis are displayed in Figure 9. As can be seen, black ﬁelds
in both ISR matrices reveal the non-separability of the second and third signal.
However, both the signals are well separated from the ﬁrst non-Gaussian signal.
Finally, the results show that the theoretical ISR matrix is a correct estimate of
the empirical ISR.

396
J. Petkov and Z. Koldovsk´y
Fig. 9. Theoretical and empirical ISR matrices of EFICA (ISR empirical obtained in
10 trials)
5.3
Monte Carlo Analysis of WASOBI
As mentioned above, the working assumption of WASOBI is the spectral diver-
sity of source signals. We check this in the same way as in the previous example.
We consider three components of the same length 1000 samples in 10 trials. The
ﬁlter coeﬃcients A and B are are the same for the ﬁrst and the second signal,
which makes them undistinguishable by WASOBI.
Signals are generated by following commands
S(1,:)=filter(1,[1 -0.7],gengau2(1,1,1000));
S(2,:)=filter(1,[1 -0.7],gengau2(1,1,1000));
S(3,:)=filter(1,[1 0.4],gengau2(1,1,1000));
Figure 10 shows that the ﬁrst and the second signal are not separated one
from the other. Again, both are well separated from the third signal since its
spectrum is diﬀerent.
Fig. 10. Theoretical and empirical ISR matrices of WASOBI (empirical ISR obtained
after 10 trials)
6
Conclusions
Blind Source Separation in a Graphical User Interface package (BSSGUI) was
introduced in this paper. The package was designed to provide an easy and com-
fortable tool that controls several BSS algorithms (EFICA, WASOBI, COMBI,
MULTI-COMBI, FCOMBI, BGL, Block EFICA, and BARBI) under Matlab.
A Monte Carlo Analysis function was implemented to allow verifying of basic
features of the build-in BSS methods.

BSSGUI – A Package for Interactive Control of BSS Algorithms
397
Acknowledgement
We would like to thank Dr. P. Tichavsk´y for providing us with Matlab codes of
his methods and for his creative suggestions. This work was supported through
the grant 102/07/P384 of the Grant Agency of the Czech Republic.
References
1. Cichocki, A., Amari, S.-I.: Adaptive Signal and Image Processing: Learning Algo-
rithms and Applications. Wiley, New York (2002)
2. Hyv¨arinen, A., Karhunen, J., Oja, E.: Independent Component Analysis. Wiley
Interscience, New York (2001)
3. Comon, P.: Independent Component Analysis, a new concept? Signal Process-
ing 36(3), 287–314 (1994)
4. Parra, L., Spence, C.: Convolutive Blind Separation of Non-Stationary Sources.
IEEE Trans. on Speech and Audio Processing 8(3), 320–327 (2000)
5. Koldovsk´y, Z., Tichavsk´y, P., Oja, E.: Eﬃcient Variant of Algorithm FastICA for
Independent Component Analysis Attaining the Cram´er-Rao Lower Bound. IEEE
Trans. on Neural Networks 17(5) (September 2006)
6. Hyv¨arinen, A.: Fast and Robust Fixed-Point Algorithms for Independent Compo-
nent Analysis. IEEE Transactions on Neural Networks 10(3), 626–634 (1999)
7. Yeredor, A.: Blind separation of Gaussian sources via second-order statistics with
asymptotically optimal weighting. IEEE Signal Processing Letters 7, 197–200
(2000)
8. Belouchrani, A., Abed-Meraim, K., Cardoso, J.-F., Moulines, E.: A blind source
separation technique using second-order statistics. IEEE Trans. Signal Process-
ing 45, 434–444 (1997)
9. Tichavsk´y, P., Doron, E., Yeredor, A., Nielsen, J.: A Computationally Aﬀordable
Implementation of An Asymptotically Optimal BSS Algorithm for AR Sources. In:
Proc. EUSIPCO 2006, Florence, Italy (September 2006)
10. Pham, D.-T., Cardoso, J.-F.: Blind separation of instantaneous mixtures of non
stationary sources. IEEE Trans. Signal Processing 49(9), 1837–1848 (2001)
11. Tichavsk´y, P., Yeredor, A.: Fast Approximate Joint Diagonalization Incorporating
Weight Matrices. IEEE Transactions of Signal Processing 57(3), 878–891 (2009)
12. Koldovsk´y, Z., M´alek, J., Tichavsk´y, P., Deville, Y., Hosseini, S.: Extension of
EFICA Algorithm for Blind Separation of Piecewise Stationary non Gaussian
Sources. In: Proc. ICASSP 2008, Las Vegas, Nevada, USA, April 2008, pp. 1913–
1916 (2008)
13. Tichavsk´y, P., Yeredor, A., Koldovsk´y, Z.: A Fast Asymptotically Eﬃcient Algo-
rithm for Blind Separation of a Linear Mixture of Block-Wise Stationary Autore-
gressive Processes. To be Presented on ICASSP 2009 (April 2009)
14. Tichavsk´y, P., Koldovsk´y, Z., Doron, E., Yeredor, A., Herrero, G.G.: Blind sig-
nal separation by combining two ICA algorithms: HOS-based EFICA and time
structure-based WASOBI. In: Proc. of EUSIPCO 2006, Florence (September 2006)
15. Tichavsk´y, P., Koldovsk´y, Z., Yeredor, A., Herrero, G.G., Doron, E.: A Hybrid
Technique for Blind Non-Gaussian and Time-Correlated Sources Using a Multi-
component Approach. IEEE Trans. on Neural Networks 19(3), 421–430 (2008)

398
J. Petkov and Z. Koldovsk´y
16. Herrero, G.G., Koldovsk´y, Z., Tichavsk´y, P., Egiazarian, K.: A Fast Algorithm for
Blind Separation of Non-Gaussian and Time-Correlated Signals. In: Proceedings of
15th European Signal Processing Conference (EUSIPCO 2007), September 2007,
pp. 1731–1735 (2007)
17. Cardoso, J.-F., Souloumiac, A.: Blind Beamforming from non-Gaussian Signals.
IEE Proc.-F 140(6), 362–370 (1993)
18. Koldovsk´y, Z., Tichavsk´y, P.: Time-domain blind audio source separation using
advanced component clustering and reconstruction. In: Proc. of HSCMA 2008,
Trento, Italy, May 2008, pp. 216–219 (2008)
19. Lee, T.-W., Girolami, M., Sejnowski, T.J.: Independent Component Analysis us-
ing an Extended Infomax Algorithm for Mixed Sub-Gaussian and Super-Gaussian
Sources. Neural Computation 11(2), 417–441 (1999)
20. Tichavsk´y, P., Koldovsk´y, Z., Oja, E.: Speed and Accuracy Enhancement of Linear
ICA Techniques Using Rational Nonlinear Functions. In: Davies, M.E., James, C.J.,
Abdallah, S.A., Plumbley, M.D. (eds.) ICA 2007. LNCS, vol. 4666, pp. 285–292.
Springer, Heidelberg (2007)
21. Tichavsk´y, P., Koldovsk´y, Z.: Optimal Pairing of Signal Components Separated by
Blind Techniques. IEEE Signal Processing Letters 11(2), 119–122 (2004)
22. Publically available MatlabT M codes of Petr Tichavsk´y,
http://si.utia.cas.cz/downloadPT.htm

Accuracy Analysis of Generalized Pronunciation
Variant Selection in ASR Systems
V´aclav Hanˇzl and Petr Poll´ak
Dept. of Circuit Theory, Czech Technical University, Prague
{hanzl,pollak}@fel.cvut.cz
Abstract. Automated speech recognition systems work typically with
pronunciation dictionary for generating expected phonetic content of par-
ticular words in recognized utterance. But the pronunciation can vary
in many situations. Besides the cases with more possible pronunciation
variants speciﬁed manually in the dictionary there are typically many
other possible changes in the pronunciation depending on word context
or speaking style, very typical for our case of Czech language. In this
paper we have studied the accuracy of proper selection of automatically
predicted pronunciation variants in Czech HMM ASR based systems.
We have analyzed correctness of pronunciation variant selection in forced
alignment of known utterances used as an ASR training data. Using the
proper pronunciation variant, more exact transcriptions of utterances
were created for further purposes, mainly for the more accurate training
of acoustic HMM models. Finally, as the target and the most important
application are LVCSR systems, the accuracy of LVCSR results using
diﬀerent levels of automated pronunciation generation were tested.
1
Introduction
Development in the ﬁeld of speech technology during several recent years, to-
gether with increasing power of computers, has allowed the application of Large
Vocabulary Continuous Speech Recognition (LVCSR). It represents one of the
most challenging application of speech technology today. Current LVCSR sys-
tems can reach high accuracy, especially for English. Also for Czech the accept-
able results of LVCSR are available [1] or [2] and this is also our current main
task of our research activities.
LVCSR represents very complex system composed of several principal modules
and for all of them very high accuracy is required to be able to achieve acceptable
accuracy, or Word Error Rate (WER) of such system. One of the key inputs for
recognizing of the utterance is the pronunciation of particular words [1] or [2].
The basic form of this information comes standardly from pronunciation lexicon
but real pronunciation may vary in many situations. Consequently, if we do not
have the proper variant in our pronunciation dictionary the ﬁnal accuracy of
LVCSR is worse. Pronunciation variability is known and natural phenomena in
speech communication and for Czech we can express the following reasons why
these changes can appear:
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 399–408, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

400
V. Hanˇzl and P. Poll´ak
– general context dependency,
– speed of uttered speech - and speaking style in general,
– emotions, as a particular aspect of the speaking style,
– dialect,
– diﬀerent meanings of the word resolved by the pronunciation.
All these problems are not unique for Czech only but they appear in particular
languages [3] on diﬀerent level and we would like to summarize it for Czech
in this work and present the study of application of automatically generated
pronunciation variants both during the training phase and during the application
within ASR system testing. We will describe several typical changes which can
be met in Czech language commonly with the analysis of the accuracy of proper
pronunciation variant choice which is very important. It is supposed that it can
bring the increasing accuracy at the level of trained acoustic HMM models of
particular speech elements (phones or triphones), same as more precise decoding
of recognized speech at the output of LVCSR system. Within the experimental
part of this work we present automated tests together with several manual checks
of proper pronunciation variant selection.
2
State of the Art
The basic and standard source of discussed phonetic content of words required
by ASR is pronunciation lexicon. This approach is standardly used mainly for
English as the language with the most developed level of speech technology ap-
plications. For Czech this approach is also adopted but it is more diﬃcult due
to increasing size of such lexicon due to higher number of inﬂected word forms.
Moreover, Czech is the language with relatively strong rule between regular pho-
netic contents (orthoepic form) and written (orthographic) form of speech. The
application of rule based conversion between orthographic and orthoepic word
form can be an alternative to lexicon based selection of word pronunciations.
Both above mentioned approaches can have some advantages and disadvan-
tages from the point of view of possible pronunciation variants. Within the lexi-
con approach we can generate automatically possible variant based on systematic
replacement of particular phones by other ones. The disadvantage is mainly in
the fact that we work with words independently without any context and con-
sequently all possible pronunciation variants must be included in the lexicon in
this case. Rule based system may implement well interword context of the pro-
nunciation, on the other hand the stronger irregularities cannot be implemented
in this system.
Experiments with pronunciation variants extension and selection based on
real acoustic data were reported for the German language [4] [5].
The research in this ﬁeld is logically part of our activities dealing with applica-
tions of ASR recognition. We have already created basic support for ASR in the
form of pronunciation lexica which were created within diﬀerent database collec-
tion, as the last one the lexicon of LC-StarII has been created. As the basis for
pronunciation lexica generation we use our tool transc implementing conversion

Accuracy Analysis of Generalized Pronunciation Variant Selection
401
rules for generation of pronunciation from orthographic form of the word (sen-
tence) [6]. We have also realized experiments with automated re-annotation of
irregularly appearing glottal stop within available databases [7] and our current
work follows and generalizes these activities.
Main targets of this work could be split to the solution of following problems.
– Summary of possible changes in pronunciation - We would like to analyze
precisely all possible changes in pronunciation due to diﬀerent reasons as
context, ﬂuent speech style (spontaneous speech), emotional speech, etc.
– Extension of the lexica - Having the list of possible changes we would like
to extend our lexica by these pronunciation variants of particular lexical
entries.
– Database re-annotation - HMM forced alignment should be applied on data-
bases to choose proper pronunciation variants and to obtain more precise
transcription of available speech utterances.
– Experimental part - Within our experiments we analyzed the accuracy of
pronunciation variant selection automatically by retraining of HMM models
with more precise pronunciation and check of LVCSR WER. The rates of
pronunciation changes were also studied. Finally, the proper selection of
pronunciation variants was checked manually on small amount of data.
3
Variability of Czech Pronunciation
3.1
Phonetic Inventory
As this paper deals with the language which can be unknown for some readers,
we would like to mention some brief introduction about phonetic inventory of
Czech. The basic information can be available from SAMPA Alphabet WEB-
page where standard set of Czech phonemes is available [8]. We work with 46
phoneme set containing 10 vowels, 3 diphtongs, 29 consonants (8 plosives, 4
aﬀricates, 11 fricatives, 2 liquids, 4 nasals) completed by 2 special allophones,
glottal stop, and schwa. We do not use three additional syllabic consonants as
they are from the acoustic point of view the same as the non-syllabic versions of
these phonemes.
3.2
Basic Pronunciation
The principle theoretical description of Czech phonetics and phonology is avail-
able in [9]. On the basis of this background we have created the basic form of the
tool transc for conversion between orthographic and orthoepic form of the utter-
ance [6]. This tool is continuously updated and more precise and its usage is pos-
sible for Czech words with regular pronunciation. It is used as the pronunciation
predictor for the words which are not included in the pronunciation lexicon.
Basic and standard source of phonetic contents of the word are pronuncia-
tion lexica. We have created or we have participated in several projects within
which large pronunciation lexica have been created. We have started with lexica

402
V. Hanˇzl and P. Poll´ak
of collected speech databases as SpeechDat, SPEECON, or Car speech collec-
tion. Within our last project we have created Czech version of LC-Star lexicon
containing more than 132 000 entries where approx. 84 000 of them represent
general common words.
The pronunciations in these lexica were obtained by our rule based tool fol-
lowed by manual correction of irregularities. Some of them have been created
during annotation of speech DBs so they are based on real pronunciations by
particular speakers. Other lexica are available also in Czech broadcast news
databases from LDC. These lexica are sources for our further generalization of
pronunciation exceptions.
3.3
Studied Changes in Pronunciation
Glottal-stop prediction
The results of this study were published in [10]. As our current work extends this
study and as it uses similar methodology we are presenting the basic summary
of these experiments.
The following rules for glottal-stop prediction were used: Firstly, the glottal
stop was inserted at the beginning of each word starting by a vowel. Secondly,
the glottal stop in inner word position was placed
– after word preﬁxes (“do-, na-, vy-, pod-, nad-, ... ”) followed by vowels,
– in word composites starting with words (“pseudo-, spolu-, samo-, ... ”) again
followed by vowels,
– in numeral composites (“jedno-, dvoj-, ... ”) also followed by vowels.
When the lexicon was extended by these variants of words with glottal stop,
forced alignment was performed for whole Czech SPEECON database. It can
be presented as recognition of present glottal stop and achieved results were
analyzed. Basically following conclusions of this experiment were stated:
– presented glottal stop was usually localized very precisely,
– higher error rate was in missing glottal stop recognition,,
– i.e. the presence of glottal stop was slightly preferred by our models.
General changes in pronunciation
Studied regular changes in pronunciation are listed bellow. They represent the
most important changes which can appear regularly or irregularly in diﬀerent
speaking styles of Czech language.
1. Changes of voicing character of ending or starting consonant - It represents
context dependent change very frequent in Czech. Our grapheme to phoneme
conversion rule works also with this context dependency. When lexicon is
used both variants must be contained.
Ex: “nad pec´ı ” vs. “nad botn´ıkem ” :
“n a t
p e t s i:” vs. “n a d
b o t J i: k e m”

Accuracy Analysis of Generalized Pronunciation Variant Selection
403
2. Back propagation of soft characters - It is already in-word change.
Ex: “botn´ık ” : “b o t J i: k” vs. “b o c J i: k”
Some Czech databases include this type of assimilation variants in a second
extended version of a lexicon [11].
3. Pronunciation of diphthongs “e u” (“a u”) - This represent very diﬃcult
problem as the boundary between pronunciation of “e u” and “e u” is rather
soft. Moreover, however there are exactly deﬁned rules for pronouncing “e u”
or “e u”, people exchange irregularly these variants many times to both sides.
Ex: “neumˇel ” : “n e u m J e l” vs. “n e u m J e l”
4. Manually given pronunciation variants - Such strongly diﬀerent pronuncia-
tions appear especially in words of foreign origin without stabilized pronun-
ciation or in Czech words with diﬀerent meanings. These variants must be
included in pronunciation lexicon only manually.
Ex: “email ” : “i: m e j l” vs. “e m a j l ”
“panick´y ” : “p a n i t s k i:” vs. “p a J i t s k i: ”
4
Experiments
We have tested the inﬂuence of accuracy of ASR on proper pronunciation vari-
ant selection. The main target of realized experiments was to analyze possible
improvement of target ASR accuracy and particular pronunciation variants se-
lection. We did not want to perform too many manual checks so we tried to
substitute them by following automated analysis.
4.1
Results with ASR
Basic setup
Firstly, basic speech recognition was performed using the model trained after
precision of the pronunciation by forced alignment. The basic setup of our rec-
ognizer was as follows:
– The experiments were realized with speech sampled at 16 kHz. The features
were composed of 16 MFCC coeﬃcient plus signal energy, commonly with
Δ and ΔΔ parameters, i.e. MFCC E D A in HTK notation.
– Acoustic HMM models are based on 3-state monophones (i.e. 3 emitting
states) with 1 mixture.
– Acoustic HMM models were trained on 170 hours of speech from database
Czech SPEECON [12], including rather clean utterances from oﬃce, enter-
tainment, and car environment with low background noise level.
– Test setup. Recognizer was constructed on the grammar without any lan-
guage modelling. The loop of equally probable 11782 words without any
out-of-vocabulary word.
– This very simple ASR without any further support was used as a tool for the
most clear analysis of pronunciation variant selection which should give the
information about the contribution of proper pronunciation variant selection.

404
V. Hanˇzl and P. Poll´ak
Multiple alignment cycles
The alignment of trained acoustic models on training data with the purpose
of selection proper pronunciation variant when more than one is available is
standardly used procedure. Within our experiment we realized:
1. the application of this procedure to above mentioned general changes of
pronunciation which was previously automatically generated,
2. the alignment was applied iteratively more than once.
In the table 1 or ﬁgure 1 we can ﬁnd the results of above mentioned experi-
ments with iteratively applied forced alignment. Recognition accuracy (Acc) is
deﬁned as the ratio of the sum of substituted, inserted, and deleted words to the
number of whole words in recognized text. Basically we can summarize these
experiments in the following points:
– It is possible to obtain Acc=15.36% using baseline system with random se-
lection of variant (using 1st variant is unusable, we might get no samples for
“G” in “abych byl ” etc.)
– After the ﬁrst alignment and 3 retraining cycles (Align 1) the accuracy has
been increased to Acc=17.74%. This is standardly used procedure during
the training of ASR.
– When forced alignment is applied iteratively always after 3 retraining cycles,
the accuracy Acc=27.37% can be achieved after the re-alignment and re-
training at the 7-th step. It represents 78% relative improvement of the
accuracy with respect to baseline system, and still 54% relative improvement
above the single one re-alignment and re-training.
– Interesting comparison can be done with more retrainings after single one re-
alignment. In this situation the accuracy is saturated at the value Acc=23%
Table 1. Achieved accuracy of pure recognition without language model
Training step
Baseline
Align 1
Align 2
Align 3
Acc [%]
15.36
17.74
21.58
23.29
Training step
Align 4
Align 5
Align 6
Align 7
Acc [%]
24.84
25.67
26.40
27.37
0
1
2
3
4
5
6
7
14
16
18
20
22
24
26
28
 
 
7 step realignment and retrainings
baseline
1 realignment and more retrainings
Fig. 1. Trends of increasing accuracy of pure recognition without language model

Accuracy Analysis of Generalized Pronunciation Variant Selection
405
(dash-dotted red line in the ﬁgure 1). The best result achieved with 7 step
re-alignment and re-training gives still approx. 20% relative improvement
with respect to this value.
– For the comparison, when triphone multimixture models were used in the
baseline system, 70-75% word accuracy was reached in this test.
4.2
Analysis of Phone Change Rates
Together with the above mentioned analysis of recognition accuracy we have also
analyzed the amount of exchanges between particular phones in word pronunci-
ations after each forced alignment. Within this experiment the rates of changes
among 3 million phones in training data were computed. It should give the an-
swer if this iterative re-alignment converges to more stable solution. The results
are in the following table 2 or ﬁgure 2.
Table 2. Changes after re-alignments among 3 million phones
1-2
2-3
3-4
4-5
5-6
6-7
d →t
3776
3239
3066
2606 2119 1641
t →d
1099
1025
614
470
445
673
z d →s t
2774
876
346
220
182
119
z →s
1524
564
311
184
132
77
S →t S
1247
507
219
126
66
64
i: →i
901
424
160
48
32
28
G →x
592
363
253
201
179
140
g →k
547
268
173
124
107
120
e u →e u
436
137
62
24
13
7
a: →a
436
121
25
7
5
0
s →z
227
212
104
55
29
32
k →g
281
172
108
49
28
36
d Z →t S
125
147
177
126
48
23
x →G
254
123
65
67
36
24
d z d →c t
137
117
73
66
48
56
J/ →d
245
68
48
38
47
54
f →v
161
80
48
36
34
37
1
2
3
4
5
6
0
1000
2000
3000
4000
 
 
t −> d
d −> t
1
2
3
4
5
6
0
500
1000
1500
2000
2500
3000
 
 
z d −> s t
z −> s
S −> t_S
i: −> i
G −> x
g −> k
e u −> e_u
a: −> a
1
2
3
4
5
6
0
50
100
150
200
250
300
 
 
s −> z
k −> g
d_Z −> t_S
x −> G
d_z d −> c − t
J/ −> d
f −> v
Fig. 2. Trends of changes after re-alignments among 3 million phones

406
V. Hanˇzl and P. Poll´ak
This table is a result of rather huge computational eﬀort, far exceeding usual
pronunciation variant selection phase in LVCSR systems construction. Usual
one phase alignment would correspond to the ﬁrst column only. We can how-
ever see that the complex interaction between pronunciation variant selection
and acoustic HMM training is far from settled even in the rightmost column.
Selected changes presented in the table give some idea about the nature of this
convergence in the complex search space.
4.3
Results of Manual Checks
Finally, also manual check was performed on small amount of data. We have
selected the manual analysis of the most frequent changes appearing above in
the table 2 or in the ﬁgure 2, i.e. choice between the pronunciation of “d” vs “t”
at the and of the word.
For this purpose we have checked 226 randomly selected sentences with pos-
sible “d” vs “t” exchange. It means approximately 20 minutes of speech. The
following most important observations have been done:
1. Only 45% human/computer agreement was observed after ﬁrst alignment
and 53% after 7th re-alignment. It means rather small correlation but the
improvement after iterative re-alignment was reasonable.
2. 98% of above mentioned mismatch was in human preference of “t” vs. com-
puter preference of “d”. On repeated closer examination of data we con-
cluded that not only the computer decision contained errors but also the
human decision was quite often wrong, as described below.
3. Our mind does not like cases like “p j e d h o d i n” – with voiced as-
similated pronunciation – when orthography suggests otherwise: “pˇet hodin”.
This type of errors have to be expected in all transcriptions made by anno-
tators with mostly technical education and only marginal background in
phonetics. (This is in sharp contrast with the well known opposite case of
voiced ﬁnal phone assimilated to voiceless.)
4. Automatic choice based on acoustic data sometimes strongly prefers variants
which are theoretically impossible or at least plain wrong, like “o p j e d
s l a v i: t r i u m f” for “opˇet slav´ı triumf ”. On closer examination,
some strange cases like this really happen but quite often we found yet
more complex assimilation to yet other phone sequences. Here the automatic
procedure had to choose between the two variants but neither of them was
the real pronunciation.
5
Conclusions
The paper presented detailed analysis of importance of proper pronunciation
variant selection for accurate speech recognition. The most important contribu-
tions can be summarized in following points.
– We have summarized the most important changes in pronunciation due to
context dependency and diﬀerent speaking styles which appear frequently in
pronounced Czech speech.

Accuracy Analysis of Generalized Pronunciation Variant Selection
407
– It was proved that already very small percentage of wrong pronunciation
variants in the training material severely degrades ASR performance so at
least one forced alignment procedure is necessary. This represents standard
training procedure.
– It was found that more than one forced alignment phase followed by several
retraining cycles can bring further improvement (compared to one variant-
selection alignment described in classical tutorials [13]). It was possible to
observe reasonable increasing of the accuracy of our testing ASR.
– The decreasing number of changes between particular iterative alignments
proved that this iterative re-alignment yields to a stable solution, however
the correlation between human and automated variant selection was not too
high.
– Systematic variant bias needs many iterations to be eliminated.
– Relative improvement of WER: 12% in early stages.
– As a future work we plan mainly to analyze the inﬂuence of multi-step re-
alignment on models with full complexity, i.e. using triphones and multi-
mixture structure. We suppose 70-75% accuracy reached after 1st alignment
should be improved by realignment steps 2 - 7.
– Altogether, we found that it probably makes sense to devote an order of
magnitude more computational eﬀort to good automatic selection of pronun-
ciation variants than is usual in preparation of LVCSR systems. Moreover,
using hand-labelled bootstrap data may be no good substitute for these ex-
pansive iterative procedures based on automatic processing of the acoustic
data: We found human bias to be far too strong to allow human annotators
to serve as a reliable etalon.
– These eﬀects were studied on somewhat artiﬁcial LVCSR system strongly
adapted to the purpose of intended experiments with acoustic properties of
phones. Inﬂuence of higher levels (dictionary, syntax etc.) was minimized as
much as possible to get clearer picture of phone changes. There is however
no doubt that for the purpose of LVCSR itself, similar more complex study
including all these higher level interactions would be valuable, though it
might result in changes in data which will be rather hard to attribute to the
pronunciation selection only.
Acknowledgements
The research was supported by grants GAˇCR 102/08/0707 “Speech Recogni-
tion under Real-World Conditions” and by research activity MSM 6840770014
“Perspective Informative and Communications Technicalities Research”.
References
[1] Psutka, J., Ircing, P., Psutka, J.V., Hajiˇc, J., Byrne, W.J., M´ırovsk´y, J.: Au-
tomatic transcription of Czech, Russian, and Slovak spontaneous speech in the
MALACH project. In: Proc. Interspeech 2005, Lisbon, Portugal, pp. 1349–1352
(2005)

408
V. Hanˇzl and P. Poll´ak
[2] Nouza, J., ˇZd´ansk´y, J., David, P., ˇCerva, P., Kolorenˇc, J., Nejedlov´a, D.: Fully au-
tomated system for Czech spoken broadcast transcription with very large (300K+)
lexicon. In: Proc. Interspeech 2005, Lisbon, Portugal, pp. 1681–1684 (2005)
[3] Dupont, S., Ris, C., Couvreur, L., Boite, J.-M.: A study if implicit and explicit
modeling of coarticulation and pronunciation variation. In: Proc. Interspeech
2005, Lisbon, Portugal, pp. 1353–1356 (2005)
[4] Wolﬀ, M.: On representation and training of pronunciation dictionaries. In: 8th
Czech-German Workshop ’Speech Processing’, Prague, Czech Republic (1998)
[5] Wolﬀ, M., Eichner, M., Hoﬀmann, R.: Evaluation of automatically trained pronun-
ciation dictionaries. In: Proc. Czech-German WS on Speech Processing, Prague,
Czech Republic (2002)
[6] Poll´ak, P., Hanˇzl, V.: Tool for Czech pronunciation generation combining ﬁxed
rules with pronunciation lexicon and lexicon management tool. In: Proc. of LREC
2002, Third International Conference on Language Resources and Evaluation, Las
Palmas, Spain (May 2002)
[7] Poll´ak, P., Vol´ı, J., Skarnitzl, R.: Inﬂuence of hmm’s parameters on the accuracy
of phone segmentation - evaluation baseline. In: ESSP 2005, Electronic Speech
Signal Processing, Prague (September 2005)
[8] Wells, J.C., et al.: Czech SAMPA home page (2003),
http://www.phon.ucl.ac.uk/home/sampa/czech-uni.htm
[9] Palkov´a, Z.: Czech phonetics and phonology. In: Czech language - Fonetika a
fonologie ˇceˇstiny, Charles University. Karolinum (1994)
[10] Poll´ak, P., Vol´ı, J., Skarnitzl, R.: Analysis of glottal stop presence in large speech
corpus and inﬂuence of its modelling on segmentation accuracy. In: 16th Czech-
German Workshop on Speech Processing, Prague (September 2006)
[11] Psutka, J., M¨uller, L., Matouˇsek, J., Radov´a, V.: Mluv´ıme s poˇc´ıtaˇcem ˇcesky
(Talking to the Computer in Czech). Academia, Prague (2006)
[12] Poll´ak, P., ˇCernock´y, J.: Czech SPEECON adult database (November 2003),
http://www.speechdat.org/speecon
[13] Young, S., et al.: The HTK Book, Version 3.3, Cambridge (2005)

A. Esposito and R. Vích (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 409–422, 2009. 
© Springer-Verlag Berlin Heidelberg 2009 
Analysis of the Possibilities to Adapt the Foreign 
Language Speech Recognition Engines for the Lithuanian 
Spoken Commands Recognition 
Rytis Maskeliunas1, Algimantas Rudzionis1, and Vytautas Rudzionis2 
1 Kaunas University of Technology, Kaunas, Lithuania 
2 Vilnius University Kaunas faculty, Kaunas, Lithuania 
rytis.maskeliunas@ktu.lt 
Abstract. This paper presents our activities trying to adapt the foreign language 
based speech recognition engine for the recognition of the Lithuanian speech 
commands. The speakers of less popular languages (such as the Lithuanian) 
have several choices: to develop own speech recognition engines or to try 
adapting the speech recognition models developed and trained for the foreign 
languages to the task of recognition of their native spoken language. The first 
approach is expensive in time, financial and human resources sense. The second 
approach can lead to the faster implementation of the Lithuanian speech 
recognition modules into some practical tasks but the proper adaptation and 
optimization procedures should be found and investigated. This paper presents 
some of our efforts trying to adapt the foreign language oriented speech 
recognition engines for the recognition of the Lithuanian speech commands for 
the speaker-independent applications. The experimental investigation shows 
that the promising results could be achieved with relatively modest investments.  
Keywords: Lithuanian speech recognition, transcriptions. 
1   Introduction 
From the advent of the speech recognition research and the appearance of the first 
commercial applications the main efforts were devoted to the recognition of widely 
used languages, particularly the English language. The reason of such behavior is 
very clear – popular widely used languages have a bigger market potential for the 
practical applications. So looking at the general trend in the development of the 
commercial speech recognition applications and tools for the development of speech 
recognition, such sequence could be observed: first a version of the speech 
recognition engine oriented to the recognition of English (in particularly the US 
English) is released, then that system is supplemented with the engines for the other 
widely used languages (most often Spanish, French, German, etc.) and sometimes but 
not necessarily with recognition modules of some other relatively widely used 
languages (for example Dutch, Italian, Polish, etc.). Many other less widely used 
languages remain out of the scope of interest for the major speech recognition 
solution providers. 

410 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
Businesses and state institutions, in the countries were such less popular languages 
are used as a main source of the spoken language communication, face a challenge of 
the development of own speech recognition tools. The two major ways for the 
solution are as follows: 
 
- 
to develop own speech recognition engine from scratch; 
- 
to adapt the foreign language based engine for the recognition of your native 
language. 
 
The first approach has the potentially higher capabilities to exploit the peculiarities 
of the selected language and hence to achieve a higher recognition accuracy. But the 
drawback of such approach is that the providers of the major speech technologies 
avoid the implementation of such languages in their products – high costs in the 
general sense of this word.  
The second approach has the potential to achieve some practically acceptable 
results faster than developing the entirely new speech recognition engine. Another 
advantage of this approach is the potential to achieve faster compatibility with the 
existing technological platforms. Such advantage is often important for the business 
customers, since they need to follow various technical specifications in order to 
guarantee the consistent functioning of the enterprise. The idea behind this approach 
is to transfer the existing source acoustic models from a source language to the target 
language without using the speech corpora in that language and without full retraining 
of the speech recognition system [1], [3 - 7]. 
There are no Lithuanian speech engines provided by the major speech recognition 
solutions providers. Same is true for the national companies engaged in the 
information technologies. So the cross-lingual adaptation of the foreign language 
based speech recognition engines could be a desirable solution in the case of such 
languages as the Lithuanian. 
2   Overview of the other Language Adaptation Methods 
2.1   Expert – Driven and Data-Driven Approaches 
The similarity measures used to transfer the source acoustic models to a target 
language can be divided into the two major groups [8]: the expert-driven methods and 
the data-driven cross-lingual speech recognition approaches. 
In expert-driven methods mapping from one language to another is performed 
using a human knowledge and is typically based on some acoustic-phonetic 
characteristics. One of the most frequently used methods is the use of so called IPA 
scheme. Expert knowledge of all the included languages is needed. Such approach 
could be very difficult if many different languages were included in the system or 
must be used for the optimization. It was also observed that some subjective expert 
influence from the mapping can be expected. 
According to the IPA scheme [7], for each phoneme in a target language an 
equivalent phoneme in the source language was searched for. As an equivalent the 
 

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
411 
phoneme with the same IPA symbol is often selected. The ratio of the equivalent 
phonemes depends on the acoustic similarity of the languages and on the number  
of phonemes in all the involved languages. In the case when the IPA equivalent is 
non-existent in the target language the most similar phoneme according to the IPA 
scheme is looked for. The search for the most similar candidate can be performed in a 
horizontal or a vertical direction through the IPA scheme. The main advantage of the 
described scheme is that it can be applied without any speech material in the target 
language. The disadvantage of such approach is that the expert knowledge should be 
obtained somehow and this knowledge also has subjective influence introduced by the 
expert. The reason for the subjectivity is that the same IPA symbol could be 
pronounced slightly different between languages or in the different contexts of the 
languages under consideration.  
The data-driven cross-lingual speech recognition approaches are based on the data-
driven similarity measures. In these methods the similarity measure is applied during 
the mapping. The similarity measure itself is obtained from some data applying some 
algorithm. One frequently used method is based on the phoneme confusion matrix. 
Almost none expert knowledge is needed. The disadvantage of this method is that 
some amount of speech material in the target language is still needed to determine the 
similarity between the source and target language acoustic models. But this amount of 
speech data is much smaller than a complete speech database (less than 10%). 
The data-driven approach with the phoneme confusion matrix will be described 
below in more details. The idea behind this method is that the similar phonemes are 
confused during the speech recognition by a phoneme recognizer. The basic 
characteristic of such recognizer is that it recognizes the phoneme sequences instead 
of words from a vocabulary. For generating the cross-lingual confusion matrix, the 
acoustic models of one of the source languages were applied on the speech utterances 
of the target language. The recognized sequence of the source phonemes was then 
aligned to the reference sequence of the target phonemes. The output of this 
alignment was the cross-lingual phoneme confusion matrix M. At this stage for each 
of the target phoneme ftrg the best corresponding source phoneme fsrc should be looked 
for. As a similarity measure, the number of the phoneme confusions c(ftrg, fsrc) is often 
selected. Then the target phoneme is often selected as that phoneme which meets the 
condition: 
ftrg=max c(ftrg, fsrc) 
(1) 
So for each target phoneme ftrg the source phoneme fsrc with the highest number of 
confusions c is selected in this schema. If two or more source phonemes have the 
same highest number of confusions it was proposed to leave the decision for the 
expert which one of the source phonemes should represent the target phoneme ftrg. 
The same procedure could be applied if no confusions between the source and target 
phonemes were observed. 
The advantage of the described data-driven approach based on a confusion matrix 
is that it is a fully data-driven method and theoretically no subjective expert 
knowledge is required (in practice expert knowledge is still necessary to solve a 
situation when same or similar confusions were observed). 

412 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
2.2   Overview of the Similar Experimental Cross-Lingual Speech Recognition 
Studies 
The two studies were selected for the comparison purposes. One study was carried 
under the COST278 MASPER initiative [7] and another was carried on by the Vilnius 
university group [3] and used the Lithuanian language as a target language. In both 
studies the expert-driven and the data-driven approaches were compared.  
During the MASPER study German, Spanish, Hungarian and Slovak databases 
(1000 speakers each) were present as source languages. The Slovenian language was 
applied as a target language in their experiments. As a speech recognizer they used a 
refrec0.96 [2] recognizer. This recognizer was used to build the source acoustic 
models for the cross-lingual experiments and to evaluate the pure monolingual 
Slovenian recognizer. All speech recognizers were evaluated on the six different test 
scenarios: application words, words yes/no, isolated digits, connected digits, 
phonetically balanced words. The speech recognition experiments showed that in the 
case of monolingual source acoustic models the expert-driven method outperformed 
the data-driven method. Another observation from the experimental study followed 
that the language similarity has a significant influence on the speech recognition 
accuracy. 
In the study of Kasparaitis [3] the possibilities to exploit the English speech 
recognizer for the recognition of the Lithuanian voice commands has been 
considered. The phonological systems of English and Lithuanian were compared on 
the basis of the knowledge of phonology and the relations between certain English 
and Lithuanian phonemes were established. Situations in which the correspondences 
between the phonemes were to be established experimentally and the English 
phonemes that best matched the Lithuania sounds or their combinations were 
identified. Both the expert-driven and the data-driven approaches were compared. The 
expert-driven method was used to establish the obvious relations between the 
phonemes while the data-driven approach was used to establish the relations that were 
not so obvious. For example, in the data-driven approach several English phonemes 
were left unused since they don’t have similar counterparts in the Lithuanian speech 
(such as th). The data-driven alignment resulted in some relations between the 
Lithuanian diphthongs and the English phonemes that were unexpected. The results 
were used to create the transcriptions of the Lithuanian surnames that were used in the 
recognition experiments. The recognition accuracy on a small and medium size of the 
vocabulary (10, 50, 100 and 500 surnames) was investigated. The achieved 
recognition accuracy was about 85% for the 500 word pairs.  
Those studies could lead to several conclusions. First of all, it is obvious that quite 
encouraging recognition results were achieved in those studies as well as in some of 
our previous experiments [4]. Both studies suggested for us several ideas that should 
be evaluated in our experiments. Between them is the necessity to evaluate the 
efficiency of the several foreign language speech recognition engines in order to find 
the better suited one. Another idea is to try to implement several transcriptions for the 
recognition, seeking to find transcription which will be better suited for the particular 
case and the particular speaker.  

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
413 
3   Proposed Adaptation Approach 
Previously mentioned considerations had a significant impact on the selection of the 
foreign language speech recognition platform used as a basis for the adaptation. Our 
choice has fallen to the Microsoft Speech Recognition platforms [9] (Microsoft SAPI 
or Microsoft Speech Server based). First of all the Microsoft speech platforms possess 
the well established technical standards, the requirements and consequently 
guarantees a high level of compatibility with other software and technical products.  
Another important factor influencing decision from the technical point of view is 
the relatively good documentation available for the Microsoft SAPI and Microsoft 
Speech Server and the potential to exchange experience with other users or 
developers. As was mentioned earlier, authors in study [3, 4] also used the Microsoft 
SAPI engine for the Lithuanian commands and proper names recognition. So those 
results could serve as some indicative measure for the comparison purposes.  
Next idea applied in our experiments is the usage of more than one phonemic 
transcription per word. The phoneme mapping from one language to another 
inevitably causes some confusions and there is no single mapping for each user and 
each word: a mapping which can be better for one speaker or for some phonetic 
contexts may not be so good for another speaker or another phonemic contexts. One 
of the possible methods to partially avoid such confusions is to try to use several 
phonemic transcriptions for a single word or command. It means that in some 
situation one transcription could provide the better recognition while in the other 
situation another transcription will be used. It is assumed intrinsically that only one 
transcription will be used in each situation while others will be considered in the same 
manner as the transcriptions for other words or commands in the vocabulary (same  
as other words). In principle the number of different transcriptions could be big, but 
we can expect that some optimal number exists. This number depends on the variety 
of factors: the number of words in the vocabulary, the similarity of words, the number 
of speakers, etc. We think that there are no exact algorithms to select the number of 
transcriptions and this job should be performed experimentally. The possibilities to 
use the multi-transcriptional approach successfully will depend essentially from the 
content of a vocabulary but for many practical applications such approach can be 
appropriate and implemented. 
4   Experimental Background and Conditions 
4.1   Scope and Methodology of the Experiments 
This paper will deal with the task of adapting the Microsoft speech recognizer for the 
Lithuanian speech recognition using the two different vocabularies: 
a. 
The 100 Lithuanian names (each utterance composed of first and family 
name): it is expected that this vocabulary is less complicated since the names 
are longer and the task is related with the choice of possibilities; 
b. The 10 Lithuanian digit names: it is expected that this task is more difficult 
since the digit names are shorter, it is necessary to recognize exactly each 
digit in a string and this task is related with the imposed limitations. 

414 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
Also a selection of the vocabularies was determined by the practical potential of 
applications that could be developed from this vocabulary. If we try to compare this 
study with the [3] study it should be mentioned that the newer Microsoft speech 
recognition engines were used here. Another and more important difference is that the 
multiple transcriptions were used. The number of the transcriptions used per word or 
command wasn’t constant and was the case of some rough optimization (optimization 
was done by single speaker and developer trying to find some optimal performance 
level for that person and later those transcriptions were used for the other speakers as 
well). And one more difference from that study was that the bigger number of 
speakers and utterances used in the experiments. 
Technically the comparison was performed using proprietary modifications of 
TalkBalk speech recognition application example from the technical documentation 
provided by Microsoft [9]. 
Evaluating the recognizer performance, the number of correct recognitions as well 
as the number of the substitutions, the indeterminacies and the omissions was 
measured trying to investigate the adaptation procedure more thorough and to get the 
possibilities to do the right conclusions. 
4.2   Speech Database 
Several speech databases were collected for this study. For the Lithuanian first and 
family names database 33 different speakers pronounced each name once. The total 
number of combinations first name + family name was equal to 100 in this case (3300 
utterances total).  
For the experiments with the Lithuanian digits names, a speech corpus was 
collected which contain the utterances from the other 30 different speakers. In this 
case each speaker pronounced 10 digit names 20 times (6000 utterances total). 
And the third corpus contains some Lithuanian syllables. This database was 
collected to evaluate the possibilities to recognize the Lithuanian digit names by 
syllabication – pronouncing digit name by the pronunciation of syllables which forms 
this digit’s name. So a syllable database contains such syllables as “Ke, Tu, Ri” which 
form Lithuanian the digit name “Keturi” (four in English). In this database 35 
speakers pronounced each syllable 100 times (about 70000 utterances). 
It should be noted that not in all of our experiments the full dataset collected in the 
databases has been used. Detailed information about a material used in the particular 
group of experiments will be provided in the description of that group of experiments. 
5   100 Lithuanian Names Recognition 
The first group of the experiments has been carried out using the 100 Lithuanian 
names data. In these experiments we used the transcription formation methodology 
proposed in [3]. The Microsoft Speech Recognition engines were used as a basis for 
the adaptation. Two speech databases were applied in these experiments: the initial 
corpora and the corrected corpora. The corrected database was freed from various 
inadequacies and mistakes that were present in the initial database. The pronunciation 
errors were between the inadequacies and mistakes. Most of such pronunciation errors 

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
415 
were situations when a speaker used the other phoneme than the phoneme present in 
the original family name, but still getting grammatically correct and often really 
existing name (for example, speaker said Dalinkevicius instead of Danilkevicius or 
Cepotis instead of Ceponis). Other errors were related with such problems as a 
stammer near the family name or some technical spoilage such as a truncated part of 
the word (most often at the end of the name).  
5.1   100 Lithuanian Names Recognition Experiments 
The experiments were performed for the 17 male and 16 female speakers separately 
and also for the entire speakers together (table 1). Last row in the table presents the 
results obtained using corrected speech corpora. As a substitution error we have 
treated the situation, when the recognition system accepted wrongly recognized word 
as the correct one. As an indeterminacy error we have treated the situation, when the 
recognition system produced some phonetic unit at the output of the system, which 
resulted in the name that wasn’t present in the list of names (typical recognition 
system output in this situation was “I don’t understand you”). As an omission error 
we have treated the situation, when the recognizer missed the input utterance. 
 
Table 1. The recognition accuracy of the 100 Lithuanian names using the adapted 
transcriptions for the Microsoft Speech Recognition engine 
Speakers 
Correct, % 
Substitutions, % 
Indetermi-
nacies, % 
Omissions, % 
16 females
89.8 
6.5 
3.5 
0.2 
17 males 
92.5 
3.9 
3.4 
0.2 
33 
both 
genders 
91.1 
5.2 
3.3 
0.4 
corrected 
91.4 
5.6 
2.9 
0.1 
 
 
Looking at the results we may observe that the male speaker’s accuracy slightly 
outperformed the females’ speakers but the difference isn’t big. Similar trend was also 
observed in the other studies. One of the most interesting observations was that the 
recognition accuracy for the initial and the corrected databases was almost equal 
which show, that a quite robust recognition system for the small pronunciation errors 
may be developed (rather probable in applications, where we need to recognize 
people names). Detailed analysis showed that one speaker made even 19 errors when 
reading names and confusing at least one phoneme in the first name or family name, 
but all the utterances were recognized correctly. Such result wasn’t obvious since the 
list of names contains a series of the phonetically similar names. Looking at the types 
of recognition errors we see that the omissions were the least often observed type of 
the errors. Despite the fact, that the substitutions were seen more often than the 
indeterminacies (when recognizer was unable to select one name from several that 
produced similar recognition results) the difference between occurrences isn’t big.  
Another group of the experiments using 100 Lithuanian names was carried out 
using the clean and the noisy or clipped speech. The aim was to evaluate the 
robustness of a recognizer for the speech signal distortions. Only 5 speakers 

416 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
participated in this experiment and several SNR levels and clipping coefficients α 
were used.  
A clipping level sc for the signal si, where i = 1, I, has been obtained using the 2nd 
formula: 
max
cs
s
α
=
⋅
 
(2) 
Where the maximum signal model value (smax) is: 
max
1,
max(
( ))
i
i
N
s
abs s
∈
=
 
(3) 
The clipped signal 
c
is  has been determined using the following equations: 
,
,
c
i
c
c
i
c
i
c
s
if
s
s
s
s
if
s
s
>
⎧
= ⎨−
<
⎩
 
(4) 
 
Table 2 shows the results obtained in those experiments. 
Table 2. The recognition accuracy of the 100 Lithuanian names for the different quality of 
speech signals (5 speakers) 
Distortion 
Correct, % 
Substitutions, % 
Indetermi-
nacies,  % 
Omissions, % 
Clean 
96.6 
1.2 
2.2 
0.2 
SNR 40dB 
96.2 
1.6 
2.2 
0.0 
SNR 30 dB 
91.6 
1.4 
7.0 
0.0 
SNR 20 dB 
43.8 
10.0 
29.2 
17.0 
Clipped 0.3 
93.0 
4.8 
2.2 
0.0 
Clipped 0.1 
82.0 
13.6 
4.0 
0.4 
 
 
Looking at the table we see that the performance of the recognizer began to 
deteriorate significantly when the SNR level dropped below 30 dB and was in 
principle unacceptable at the SNR 20 dB. So the performance can’t be treated as 
robust one looking from the SNR variations point of view. Using the clipping 
coefficient 0.3 the recognizer performance dropped relatively insignificantly while the 
clipping coefficient 0.1 resulted in the much bigger loss in the accuracy. The general 
conclusion was that the recognizer doesn’t possess enough robustness to cope with 
the situations that may occur in the practical applications. 
5.2   Analysis of the Errors 
We’ve performed some recognition errors analysis trying to find the ways to improve 
the recognizer performance and find the ways to optimize the adaptation procedure. 
There were 290 substitution and indeterminacy errors (120 substitutions and 170 
indeterminacies) in our first group of experiments and it was natural to expect that not  
 

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
417 
Table 3. Five names that result in the largest number of the recognition errors in the 100 
Lithuanian names recognition experiment 
Indeterminacy errors 
Substitution errors 
Name 
Number of errors 
Name 
Number of errors 
Gudas Audrius 
17 
Biaigo Sandra 
16 
Baublys Algis 
12 
Dolgij Andrej 
16 
Biaigo Sandra 
6 
Grigonis Audrius 
11 
Balcius Ernestas 
6 
Baublys Algis 
10 
Gailiunas Rytis 
6 
Giedra Nerijus 
8 
 
all names will produce the equal number of errors. Table 3 shows the 5 names that 
produced the largest number of errors in these experiments. 
Looking at those results we can see that the 5 most confusing names produced 
almost 40% of all substitution errors and slightly more than 35% of all indeterminacy 
errors. So the “concentration” of errors is big and more attention to the names that 
resulted in larger amounts of errors is necessary. 
Detailed view at the most confusing names in these experiments shows that most of 
those names don’t have the particularly difficult phonetical structure (the family name 
Biaigo may be treated as the more difficult case). The bigger number of errors obtained 
by the name Gailiunas Rytis may be explained by the presence of the name Gailiunas 
Vytautas in the same list. But most of the errors can’t be explained straightforwardly. 
For example, the name Gudas was often confused with the name Butkus. In our 
opinion such situation shows, that there’s still plenty of room for the optimization and 
further investigation and the better recognition accuracy is obtainable. 
6   Syllable Recognition Analysis 
One of the ideas that we’ve tried to implement in our adaptation procedures was the idea 
to use multiple transcriptions for one Lithuanian word. The idea here was that for different 
speakers, the different transcriptions will be better suited and will allow achieving the 
higher overall recognition accuracy. This assumption is based on the belief that there are 
no single mapping from a phonemic units space in one language to the phonemic units 
space in another language, that’ll be equally efficient for all speakers. 
To check this assumption we’ve performed several experiments. The idea of them 
was to generate the multiple transcriptions of the same word and to check which 
transcriptions will be recognized more often for the different speakers. Random 
generation of the transcriptions may not be the best solution and we’ve used the 
transcription system for the Lithuanian words and phonemes proposed by Kasparaitis 
which allows generating the transcriptions in some systematic and ranked manner. 
In this experiment the two family names from the list of 100 Lithuanian names – 
Beliukeviciute and Varanauskas – were selected. These family names are longer and 
have more phonemes in their phonemic structure and allow generating more phonetical 
transcriptions that will be sensible and may produce valuable result during the 
recognition. In the case of the family name Beliukeviciute 1152 transcriptions were  
 

418 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
Table 4. The four most frequently recognized transcriptions, for the two speakers and for the 
two Lithuanian names 
Name 
Beliukeviciute 
Varanauskas 
1st. speaker 
2nd. speaker 
1st. speaker 
2nd. speaker 
transcr. 
occur. 
transcr. 
occur. 
transcr. 
occur. 
transcr. 
occur. 
111 
23 
505 
18 
67 
24 
19 
10 
99 
20 
121 
12 
130 
11 
166 
8 
15 
13 
504 
11 
4 
8 
144 
8 
3 
10 
507 
11 
70 
8 
6 
8 
Overall 27 
transcriptions 
Overall 21 
transcriptions 
Overall 28 
transcriptions 
Overall 27 
transcriptions 
 
generated for this experiment and for the family name Varanauskas 188 transcriptions 
were obtained (using every possible syllable combinations). Then the two professional 
speakers pronounced each of the family names 100 times (in the recording studio, using 
the different speech style and rate) and the recognition system was coded to select 
which of the transcription is the most likely for that speaker and that name. Table 4 
shows the four most popular transcriptions for each of the speakers. 
One of the observations that could be seen in the Table 4 is that a big number of the 
transcriptions were recognized as the most similar ones for each of the speaker. For the 
name Beliukeviciute 27 transcriptions were recognized for the first speaker and 21 
transcriptions were recognized for the second speaker. It is worth noting that in both 
cases the first speaker tended to activate more different transcriptions than the second.  
Even more important observation is that the most often recognized (as the most 
similar) transcriptions aren’t the same for the different speakers. For example, for the 
first speaker the most often used transcriptions for the word Beliukeviciute were 
transcription variations 111th, 99th, 15th and 3d. At the same time for the second speaker 
the most often used transcriptions were the transcription variations 505th, 121st, 504th 
and 507th. Similar situation has been observed for the name Varanauskas (Table 4). 
These results allowed us to conclude that the use of multiple transcriptions in the 
adaptation of the foreign language speech recognition engine to the Lithuanian 
language is a reasonable step and it is worth further investigation and implementation 
in the practical applications. Another conclusion is that the proposed phonetic 
transcriptions generation schema is worth to be investigated and could serve as a good 
basis for a further development of the Lithuanian foreign language based speech 
recognizer adaptation system. 
Another group of the experiments was carried out using the isolated syllables and 
trying to investigate which syllables will be easier and which will be more difficult to 
recognize. We’ve collected the recordings of the several Lithuanian syllables that are 
used in the names of Lithuanian digits. This collection was performed in the 
anticipation that some digit names will be more difficult to recognize and more 
thorough analysis will be needed. For example we’ve used the syllables Ke, Tu, Ri 
which are used in the Lithuanian digit name Keturi (meaning Four). We’ve expected  
 
 

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
419 
Table 5. Percentage of the recognized, the rejected and the omitted syllables Ke, Tu, Ri 
Type of the recognizer decision 
Syllable 
Type of the 
experiment 
Correct, % 
Indeterminacy, %
Omissions, % 
7C-16V 
98.1 
0.3 
1.6 
Ke 
24C-16V 
97.8 
0.4 
1.8 
7C-16V 
94.3 
1.2 
4.5 
Tu 
24C-16V 
91.1 
1.8 
7.1 
7C-16V 
66.3 
16.0 
17.7 
Ri 
24C-16V 
59.1 
21.3 
19.6 
 
 
that this digit name will form the most complicated case when trying to adapt the 
foreign language speech recognition engine for the Lithuanian language. 
The two different recognition experiments were performed. In the first experiment 
we’ve used the transcriptions of syllables formed from the 7 consonants (p, t, k, b, d, 
g, g, r) and 16 vowels. The use of 16 vowels means implementation of all available 
vowels in the Lithuanian speech. The selection of the consonants was based on a 
similarity to the consonants present in the original syllables Ke, Tu and Ri. This 
experiment is denoted as 7C-16V in the Table 5. Another experiment is called 24C-
16V in the Table 5 and means that in this case syllables were formed from all 24 
consonants and all 16 vowels. In the first experiment 112 syllables were used as a 
recognition opportunity and in the second total 384 syllable transcriptions were used. 
First of all we’ve measured the ratio of the recognitions (system recognized one of  
the 3 tested syllable utterances as a syllable from the list of the available syllables), 
the indeterminacies (SR engine rejected to recognize utterance as a syllable) and the 
omissions (recognizer missed this syllable utterance (treated it as silence, etc.). 
The results showed that the speech recognition engine was able to recognize the 
syllables Ke and Tu as some syllable with a high degree. That is particularly true for 
the Ke when in both experiments the number of recognitions was about 98% and in 
the first experiment when the number of the possible options was smaller even higher 
percentage of the recognitions occurred. Another observation is that for the syllable 
Tu higher number of omissions than indeterminacies was seen. 
The situation is different with the syllable Ri. In this case the number of the 
recognitions was significantly lower than for the first two syllables. In the first 
experiment the ratio of recognitions was only 66.3% while in the second experiment, 
it has dropped down to the 59.1%. Looking at the ratio of the indeterminacies and the 
omissions we can’t see the clearly expressed trend and should treat that both the 
indeterminacies and the omissions occurred with a roughly equal frequency. 
Table 6 shows the three syllables that were recognized most often in this 
experiment. In this table columns Rec1, Rec2, Rec3 mean the first most frequent, the 
second most frequent and the third most frequent transcription (UPS symbols), which 
was recognized for each of the tested syllables (for example, during the first 
experiment 53.5 percent of syllables Ke in the speech corpora were recognized as a 
syllable which is described by the phonetic transcription G ae, etc.). 

420 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
Table 6. The three syllables that were recognized most often for test syllables Ke, Tu, Ri 
Three most popular recognitions 
Syllable 
Type of the 
experiment 
Rec.1 
Corr., % 
Rec. 2 
Corr., % 
Rec. 3 
Corr., % 
7C-16V 
G ae 
53.5 
K ae 
21.8 
G eh 
10.2 
Ke 
24C-16V 
Jh ae 
28.9 
G ae 
28.4 
Jh eh 
8.1 
7C-16V 
D ow 
24.1 
T ow 
18.8 
D ao 
13.7 
Tu 
24C-16V 
D ow 
15.3 
S ow 
12.4 
Th ow 
9.2 
7C-16V 
D ax 
8.5 
G ey 
7.1 
D eh 
7.0 
Ri 
24C-16V 
F ih 
8.1 
D ax 
4.6 
Jh ey 
3.7 
 
Looking at these results we can see that in the 7C-16V experiment the recognition 
accuracy was higher than in the 24C-16V experiment. This result may be expected. 
Another observation is that generally the syllable Ke was recognized better than the 
other two syllables, but recognition of the syllable Tu was also relatively good in the 
sense that the recognized transcriptions could enable the correct recognition of a 
whole word if we will try to compose it from the separate syllables. The worst 
situation was with the syllable Ri. In this case we saw a high degree of the distribution 
between the possible recognitions combined with a low degree of recognitions 
showed in the Table 5. Anyway we don’t see that these results show that the 
recognition via syllabication isn’t feasible. If we need to recognize the digit names 
and some names we can expect that the dictionary restrictions may enable the 
limitation of the syllable transcriptions space in such a manner that those limits will 
make recognition possible. From another point of view the results provided guidelines 
for the further optimization procedures. 
7   Lithuanian Digit Names Recognition 
The next group of the experiments was carried out to investigate the adaptation 
possibilities of the foreign language speech recognizer for the 10 Lithuanian digit 
names and the words yes and no (in Lithuanian taip and ne). In these experiments 
we’ve used the templates generated using the Kasparaitis [3] methodology and our 
own methodology derived from the syllable recognition experiments. It must be noted 
that our methodology was applied to the words 0, 2, 3, 4. Table 7 shows the results of 
this experiment. Row PK shows results obtained using the Kasparaitis methodology 
while the row AR shows results obtained using our transcriptions. 
Table 7. The recognition accuracy of the Lithuanian digit names and the words yes/no 
Word 
Templates 
0 
2 
3 
4 
All (12  
words) 
PK 
41.5 % 
73.5 % 
62.0 % 
32.5 % 
75.35 % 
AR 
81.0 % 
86.2 % 
97.5 % 
85.5 % 
83.33 % 

 
Analysis of the Possibilities to Adapt the Foreign Language Speech Recognition 
421 
Table 8. The frequency of the appearance for the several non-trivial phonetic transcriptions for 
some Lithuanian digit names 
Digit name 
Total 
recognitions 
Transcription 
Transcription 
occurrence 
Trys (three) 
195 
T iy s 
127 
Keturi (four) 
171 
G eh t ow g ey 
42 
Nulis (zero) 
159 
M uw z ey s 
45 
Du  (two) 
173 
D ow 
85 
 
 
These results show that the implementation of the additional transcriptions 
generated by using the empirically suggested transcriptions led to the significant 
increase in the recognition accuracy of those digits whose transcriptions were 
constructed suggested by the syllable recognition experiments. A particularly 
significant increase was seen on the words 0, 3 and 4. Increase in the overall 
recognition accuracy will be probably higher if the recognition system will be 
supplemented by the empirically obtained transcriptions for all ten digits, e.g., the 
recognition accuracy of the digit names 5 (75.5%) and 9 (63.5%) are still below 
possibilities. The main obstacle at this stage was the lack of appropriate syllable 
utterances in the corpora to carry out the syllable recognition experiments. 
The importance of the additional transcriptions is illustrated in the Table 8. This 
table shows several popular phonetic transcriptions which are not obvious but were 
useful recognizing several digit names, i.e. the digit trys (three) phonetic transcription 
“t iy s” was even more usefull than the transcriptions obtained using methodic – “t r 
iy s” and “t r ih s”. Such transcriptions as “g eh t ow g ey” for the word “keturi” or 
“m u uw z ey s” for the word “nulis” could be obtained only from the empirical 
evaluations since they are too different from the original phonetic transcription. 
8   Conclusions 
1. Adapting the speech recognition models developed and trained for the foreign 
languages to the task of recognition of their native spoken language may lead to a 
faster implementation of the Lithuanian speech recognition into some practical 
tasks, but the proper adaptation and optimization procedures should be found and 
investigated.  
2. For the 100 Lithuanian names the recognition accuracy of more than 90% was 
achieved. These results show that the implementation of the longer commands and 
the transcription generation methodic proposed in study [3] were confirmed. 
3. In the experiments we’ve used the multiple transcriptions for the single Lithuanian 
command with the aim to improve the recognition accuracy. The additional 
transcriptions were obtained via syllable recognition experiments. The application 
of the multiple transcriptions for the recognition of the 10 Lithuanian digit names 
allowed achieving the recognition accuracy close to the 90% which is significantly 
better than using only the phonetically obvious transcriptions. The improvements 
were particularly visible for those digit names, where the recordings of syllables 
(for optimization) were available.  

422 
R. Maskeliunas, A. Rudzionis, and V. Rudzionis 
References 
1. Byrne, W., et al.: Towards language independent acoustic modeling. In: Proc. of ICASSP 
2000, Acoustics, Speech, and Signal Processing, Istanbul, Turkey, vol. 2, pp. II1029–II1032 
(2000) 
2. Lindberg, B., et al.: Noise Robust Multilingual Reference recognizer Based on Speech Dat. 
In: Proc. of ICSLP 2000, Beijing, vol. 3, pp. 370–373 (2000) 
3. Kasparaitis, P.: Lithuanian Speech Recognition Using the English Recognizer. 
INFORMATICA 19(4), 505–516 (2008) 
4. Maskeliunas, R.: Modeling Aspects of Multimodal Lithuanian Human - Machine Interface. 
In: Esposito, A., et al. (eds.) Multimodal Signals. LNCS (LNAI), vol. 5398, pp. 75–82. 
Springer, Heidelberg (2009) 
5. Schultz, T., Waibel, A.: Language-independent and language-adaptive acoustic modeling 
for speech recognition. Speech Communication 35(1-2), 31–51 (2001) 
6. Villasenor-Pineda, L., et al.: Toward Acoustic Models for Languages with Limited 
Linguistic Resources. In: Gelbukh, A. (ed.) CICLing 2005. LNCS, vol. 3406, pp. 433–436. 
Springer, Heidelberg (2005) 
7. Zgank, A., et al.: The COST278 MASPER initiative – crosslingual speech recognition with 
large telephone databases. In: Proc. of 4th International Conference on Language Resources 
and Evaluation LREC 2004, Lisbon, pp. 2107–2110 (2004) 
8. Zgank, A.: Data driven method for the transfer of source multilingual acoustic models to a 
new language. Ph.D. thesis, University of Maribor (2003) 
9. Microsoft Speech API (SAPI) 5.3, http://msdn.microsoft.com/en-us/ 
library/ms723627(VS.85).aspx (retrieved December 19, 2008) 

MLLR Transforms Based Speaker Recognition
in Broadcast Streams
Jan Silovsky, Petr Cerva, and Jindrich Zdansky
SpeechLab, Technical University of Liberec,
Studentska 2, 461 17 Liberec, Czechia
{jan.silovsky,petr.cerva,jindrich.zdansky}@tul.cz
Abstract. This paper deals with utilization of maximum likelihood lin-
ear regression (MLLR) adaptation transforms for speaker recognition
in broadcast news streams. This task is speciﬁc particularly for widely
varying acoustic conditions, microphones, transmission channels, back-
ground noise and short duration of recordings (usually in the range from
5 to 15 seconds). MLLR transforms based features are modeled using
support vector machines (SVM). Obtained results are compared with
a GMM based system with traditional MFCC features. The paper also
deals with inter-session variability compensation techniques suitable for
both systems and emphases the importance of feature vector scaling for
SVM based system.
Keywords: speaker recognition, broadcast news, MLLR, NAP.
1
Introduction
State-of-the-art speaker recognition systems usually employ a combination of sev-
eral sub-systems based on diﬀerent modeling techniques with various features.
However the core component is mostly based on modeling of short-time cepstral
features (extracted over a few tens of milliseconds) using either Gaussian mix-
ture models (GMMs) [1,2] or support vector machines (SVMs) [3]. These systems
are experienced to yield the best results compared to the others based on high-
level features extracted over longer spans of time (e.g. prosodic features). But the
problem of short-term cepstral features is that their distribution is not depending
only on a speaker characteristic, but also on many other factors, particularly, en-
vironment properties, channel characteristics and the choice of words spoken. A
straightforward approach how to cope with the last mentioned problem is utiliza-
tion of phone-constrained [4] or word-constrained [5] cepstral GMMs. However,
there are two signiﬁcant drawbacks of these techniques. First, accurate speech
recognition is required for consistent segmentation of speech. Second, the frag-
mentation of data makes diﬃcult training of well trained models.
Another approach proposed in [6] exploits the adaptation techniques employed
by current speech recognition systems to turn the speaker-independent recogni-
tion models into more accurate speaker-dependent models. The maximum like-
lihood linear regression (MLLR) represents an adaptation technique based on
A. Esposito and R. V´ıch (Eds.): Cross-Modal Analysis, LNAI 5641, pp. 423–431, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

424
J. Silovsky, P. Cerva, and J. Zdansky
an aﬃne transform of the Gaussian means or variances in the acoustic mod-
els. The MLLR transforms based speaker recognition is based on modeling the
diﬀerence between the speaker-dependent and speaker-independent models in-
stead of modeling the cepstral observations directly. This diﬀerence is embodied
in the coeﬃcients of the aﬃne transform and modeled as speaker features using
SVMs. This approach was proven to provide satisfactory results in NIST speaker
recognition evaluations (SREs) in 2006 and 2008.
In this paper we analyze the deployment of the MLLR transforms based
speaker recognition in the broadcast news (BN) domain. The most signiﬁcant
diﬀerence compared to NIST SREs [7] consists in the amount of data available
for speaker enrolment and for speaker recognition. While the NIST SRE core
condition provides about 2.5 minutes of speech for speaker enrolment and the
same amount for speaker recognition, the BN streams are split into speaker
homogeneous segments with duration ranging from several seconds to at most
several tens of seconds. Also the amount of available enrolment data for speakers
diﬀers in much larger scale, ranging from about several tens of seconds to more
than one thousand. Although there is a data limited evaluation condition in
NIST SRE providing about 30 seconds of speech for enrolment and recognition,
to our knowledge, no results for MLLR transforms based speaker recognition
were published for this task.
The reported experiments were performed on our database of Czech broad-
cast news streams. Results are compared with a baseline system based on Gaus-
sian mixture models with short-term cepstral features. As the main problem
in speaker recognition with acoustic features is a diversity of recording condi-
tions and channels between sessions, utilization of inter-session variability com-
pensation techniques was examined for both systems. Eigenchannel adaptation
[8,9] was applied for the GMM-based system and nuisance attribute projection
(NAP) [10] was employed for SVM-based system. Other normalization technique,
rank-normalization (Rnorm), was employed for SVM-based system and yielded
signiﬁcant performance improvement.
2
Method
2.1
Scheme of the System
The scheme of our baseline system is depicted in Fig.1. First, classic Mel-
frequency cepstral coeﬃcient (MFCC) features are extracted. 13 MFCCs (in-
cluding c0) are extracted from signal and augmented with the ﬁrst and second
derivatives forming a 39-dimensional feature vector. Next, speaker identiﬁcation
module provides a hypothesized identity. As the number of speakers occurring
in TV and radio programs is very large, we often encounter the situation that
the speaker is none of enrolled speakers. Therefore the identity claim (and its
score) is passed to the speaker veriﬁcation module which decides whether the
voice of a speaker in the recording belongs to the hypothesized speaker based on
the log-likelihood ratio test.

MLLR Transforms Based Speaker Recognition in Broadcast Streams
425
Fig. 1. Scheme of the baseline system
Figure 2 depicts the scheme of the MLLR transforms based system. The
MLLR adaptation module requires the knowledge of the transcription. Hence a
speech recognition module has to precede it. From the perspective of the speaker
recognition system this module causes relatively high increase of computational
cost compared to the baseline system. However as this step is anyway performed
within two-stage speech recognition there is no impact on the overall system per-
formance. Further, our experience [11] shows that it is not necessary to employ
full vocabulary (our full vocabulary contains over 300k words). Utilization of
reduced vocabulary speeds up the ﬁrst automatic speech recognition stage with
no signiﬁcant harm of recognition accuracy at the second stage.
Fig. 2. Scheme of the MLLR transforms based system

426
J. Silovsky, P. Cerva, and J. Zdansky
Another modiﬁcation in the recognition scheme is related to the speaker
identiﬁcation module. MLLR transform adaptation parameters form one large-
dimensional vector per recording and thus support vector machines (SVM) are
used for classiﬁcation instead of Gaussian mixture models. This subsequently
aﬀects the speaker veriﬁcation module. As the SVM itself normalizes the output
score within a set of background speakers, no ratio is computed and the raw
score is compared with a detection threshold.
2.2
Eigen-Channel Adaptation
For the baseline system, experiments with the eigen-channel adaptation tech-
nique [8,9] were carried out. This technique copes with inter-session variability
mismatch by adapting the GMM trained under one channel condition towards
the diﬀerent channel condition of a test recording. The GMM mean vector μ is
adapted to
μa = μ + V x,
(1)
where V is eigen-channel space matrix and x is a weight vector, which is consid-
ered to be normally distributed and is obtained by maximizing probability
p (data|μ + V x) p (x) .
(2)
2.3
Maximum Likelihood Linear Regression
In maximum likelihood linear regression (MLLR) [12], an aﬃne transform is
applied to the Gaussian mean vectors of speech recognition models to map from
speaker-independent μ to speaker-dependent means μ′ as
μ′ = Aμ + b,
(3)
where the adaptation parameters A and b are estimated so as to maximize the
likelihood of the recognized speech under a preliminary recognition hypothesis.
Only single transformation (A, b) may be used for all Gaussians of all phone
models or, in a more detailed adaptation scenario, Gaussians might be clustered
by similarity (data-driven approach) or partitioned based on the aﬃliation with
a particular set of phones (e.g. obstruents, sonorants) into multiple classes, and
a diﬀerent transformation (Ai, bi) is applied to each of these classes. Multi-class
MLLR adaptation provides more freedom in adapting, since all the means are
not constrained to move the same way. However, it might happen that there is
not enough adaptation data to obtain a good transform for a given class.
2.4
Support Vector Machine
Support vector machine (SVM) [13] is a two-class linear classiﬁer constructed
from sums of a kernel function K (., .),
f (x) =
L

i=1
γitiK (x, xi) + ξ,
(4)

MLLR Transforms Based Speaker Recognition in Broadcast Streams
427
where the ti are the ideal outputs (-1 or 1), L
i=1 γiti = 0 , and γi > 0 . The
vectors xi are support vectors obtained from the training set by an optimiza-
tion process [14]. For classiﬁcation, a class decision is based upon whether the
value f (x) is above or below a decision threshold. Since the SVM is a two-class
classiﬁer, we have to handle the speaker identiﬁcation as a veriﬁcation problem.
The common method is one vs. all strategy when target speaker model is trained
using positive samples represented by the speaker’s data and negative samples
are drawn from all other speakers enrolled in the system. Classiﬁcation is done
in a winner takes all strategy, in which the classiﬁer with the highest output
function assigns the class (identity).
2.5
MLLR-SVM Modelling
The coeﬃcients from one or more adaptation transforms are concatenated into a
single feature vector. Since our acoustic feature vectors are 39-dimensional, the
dimension of SVM feature vectors will equal the number of transforms × 39 ×
40. For MLLR-SVM system the technique called nuisance attribute projection
(NAP) [10] was employed to cope with inter-session variability mismatch. The
basic idea of NAP is to remove dimensions that are irrelevant to the speaker
recognition problem. The feature vectors are transformed before they are passed
to SVM training using the equation
v′ = v −E

ET v

,
(5)
where E is low-rank matrix deﬁning NAP-subspace. NAP and eigen-channel
adaptation techniques are in many ways very similar. Matrices E and V are
estimated by the same means and depending on the form of eigen-channel adap-
tation the matrices might be equal.
Because the SVM kernel is sensitive to the magnitude of the feature values,
components must be scaled to avoid attributes in greater numeric ranges domi-
nate those in smaller numeric ranges. We found that normalization of the feature
vector components by rank normalization (Rnorm) yields good performance for
us. Rnorm replaces each feature value by its rank (normalized to the interval
[0, 1]) in the background distribution. The side eﬀect is that the original distri-
bution is warped to approximately uniform distribution.
3
Experiments and Results
3.1
Datasets and Evaluation Task Deﬁnition
Experiments were performed using our database of Czech BN streams. It con-
tains streams collected during more than ﬁve years. The whole captured streams
were split into speaker homogeneous segments.
Although whole system frameworks were described in section 2.1, we were
interested particularly in the comparison of discrimination abilities of both pre-
sented approaches. As the problem of speaker veriﬁcation induces also issues

428
J. Silovsky, P. Cerva, and J. Zdansky
related to proper system calibration we decided to carry out our experiments as
closed-set speaker identiﬁcation. Thus we do not perform tests on the recording -
claimant speaker pair basis, where a preliminary identity claim is required. Since
we are dealing with closed-set identiﬁcation task, the speaker in a test recording
must be one of the enrolled speakers and we do not have to deal with speaker
veriﬁcation anymore. 228 enrolled speakers were chosen based on the amount of
data available in the database, models were trained for speakers with at least
30 seconds of speech data available for training and another 30 seconds for test-
ing (regardless the number of segments). The data set preparation was done so
that the training and testing data for a particular speaker were from disjunct
sessions (streams). After forming training and testing data set, remaining data
from known speakers with insuﬃcient amount of data were set apart to form
a data set used for training of background models or inter-session variability
compensation methods. Test data set contained 4245 recordings with duration
ranging mostly from 5 to 15 seconds.
3.2
Baseline System
The universal background model (UBM) with 256 Gaussians was trained on
over 11 hours of speech. Target speaker models were derived from the UBM
by maximum a posteriori (MAP) adaptation method with relevance factor set
to 19. To speed up the recognition process the 10 top scoring components in
the UBM were identiﬁed and stored for each frame of a recording and only
these components were used while likelihood computation for GMMs of enrolled
speakers [2]. Table 1 shows results obtained for both systems with and without
eigen-channel adaptation.
Table 1. Baseline system results
System
Rate [%]
MFCC-GMM
88.15
MFCC-GMM + eigen-channel adaptation
91.26
3.3
MLLR-SVM System
We tested two basic variants of MLLR-SVM system distinct in number of regres-
sion classes. The ﬁrst variant used single global transformation for all Gaussians
in the HMM. The second variant employed more detailed adaptation scenario
with three regression classes. HMM Gaussians were clustered into these classes
by similarity (data-driven approach). In both cases the non-speech units in the
recording were left out of adaptation process since they are not expected to help
in speaker recognition. The size of the SVM feature vector for the ﬁrst system
is 1560 and for the second system 4680. For both systems a linear inner-product
kernel function was used.
Table 2 summarizes achieved results for both systems and also shows results
obtained after application of NAP, Rnorm and both of them. Rnorm yields

MLLR Transforms Based Speaker Recognition in Broadcast Streams
429
Table 2. MLLR-SVM system results
System
Rate [%]
1 transform MLLR-SVM
61.69
1 transform MLLR-SVM + NAP
63.43
1 transform MLLR-SVM + Rnorm
78.18
1 transform MLLR-SVM + NAP + Rnorm
78.26
3 transforms MLLR-SVM
50.15
3 transforms MLLR-SVM + NAP
51.12
3 transforms MLLR-SVM + Rnorm
69.92
3 transforms MLLR-SVM + NAP + Rnorm
69.70
signiﬁcation improvement of speaker recognition rate. Utilization of NAP in
conjunction with Rnorm is of no use.
Throughout results summarized in table 2, feature vectors used for SVM mod-
elling correspond to the MLLR adaptation parameters of gender dependent (GD)
HMMs. The appropriate HMM was chosen based on the gender information
known from our database. However, in real situations this information is not
available. One of the simplest ways how to handle this problem is to use only
one gender independent HMM. Another way would be to identify speaker’s gen-
der, which can be done with high accuracy (i.e. with GMMs), and use existing
GD HMMs. Drawback of such approach is high performance drop if speaker’s
gender in a test recording is misclassiﬁed and hence diﬀerent HMMs are used
within training and classiﬁcation. We decided for yet another approach [15] which
provides several beneﬁts. It makes use of existing GD speech recognition models,
no gender identiﬁcation is required and ﬁnally it is supposed to improve recogni-
tion rate. Instead of using one GD model, transforms for both male and female
HMMs are computed. SVM feature vector is than formed by concatenation of
both transforms. Since gender-dependent HMMs are not just linear transforms
of each other, we can expect the two sets of MLLR features to provide two dif-
ferent, not entirely redundant views of the observation space. Obtained results
are summarized in table 3.
Results in both tables show that more detailed adaptation framework with 3 re-
gression classes performs worse than the system with one global regression
Table 3. Results for MLLR-SVM system with transforms for both GD models
System
Rate [%]
1 transform (M+F) MLLR-SVM
68.63
1 transform (M+F) MLLR-SVM + NAP
70.27
1 transform (M+F) MLLR-SVM + Rnorm
82.51
1 transform (M+F) MLLR-SVM + NAP + Rnorm
82.75
3 transforms (M+F) MLLR-SVM
59.00
3 transforms (M+F) MLLR-SVM + NAP
59.87
3 transforms (M+F) MLLR-SVM + Rnorm
74.43
3 transforms (M+F) MLLR-SVM + NAP + Rnorm
74.73

430
J. Silovsky, P. Cerva, and J. Zdansky
class. This is probably caused by the short duration of records, when there is in-
suﬃcient amount of adaptation data to obtain a good transforms for more classes.
4
Conclusions
In this paper we examined the use of MLLR transforms based speaker recognition
in BN streams. Our results show that very important step for the SVM-based sys-
tem is scaling of feature vectors, rank normalization worked well for us yielding
signiﬁcant performance gain. Rank normalization even suppresses the impact of
nuisance attribute projection method, an inter-session compensation technique,
often used in connection with SVM-based systems in order to cope with
mismatched acoustic conditions within sessions. In contrast to some recently pub-
lished studies, our MLLR-SVM based system is not performing as well as the base-
line GMM-based system with cepstral features. We believe that this is caused by
the diﬀerent nature of evaluation data and particularly by the short duration of
test recordings. Best results for MLLR-SVM system were achieved with only sin-
gle global regression class yielding recognition rate of 82.75 % compared to 91.26 %
obtained for the baseline system based on GMMs with cepstral features.
Acknowledgement
This work was supported by the Ministry of the Interior of the Czech Republic
(project no. VD20072010B160).
References
1. Reynolds, D.: Speaker identiﬁcation and veriﬁcation using Gaussian mixture
speaker models. Speech Communications 17, 91–108 (1995)
2. Reynolds, D.A., Quatieri, T.F., Dunn, R.: Speaker veriﬁcation using adapted Gaus-
sian mixture models. Digital Signal Processing 10(1-3), 19–41 (2000)
3. Campbell, W.M., Sturim, D.E., Reynolds, D.A., Solomonoﬀ, A.: SVM-based
speaker veriﬁcation using a GMM supervector kernel and NAP variability com-
pensation. In: Proc. ICASSP, pp. 97–100 (2006)
4. Park, A., Hazen, T.J.: ASR dependent techniques for speaker identiﬁcation. In:
Hansen, J.H.L., Pellom, B. (eds.) Proc. ICSLP, Denver, September 2002, pp. 1337–
1340 (2002)
5. Sturim, D.E., Reynolds, D.A., Dunn, R.B., Quatieri, T.F.: Speaker Veriﬁcation
Using Text-Constrained Gaussian Mixture Models. In: Proc. International Confer-
ence on Acoustics, Speech, and Signal Processing in Orlando, Florida, May 13-17,
pp. I:677–680. IEEE, Los Alamitos (2002)
6. Stolcke, A., Ferrer, L., Kajarekar, S., Shriberg, E., Venkataraman, A.: MLLR trans-
forms as features in speaker recognition. In: Proc. Interspeech, Lisbon, September
2005, pp. 2425–2428 (2005)
7. The NIST Year 2008 Speaker Recognition Evaluation Plan (2008),
http://www.nist.gov/speech/tests/sre/2008/sre08_evalplan_release4.pdf

MLLR Transforms Based Speaker Recognition in Broadcast Streams
431
8. Kenny, P., Dumouchel, P.: Disentangling speaker and channel eﬀects in speaker
veriﬁcation. In: Proc. ICASSP, Montreal, Canada, May 2004, vol. 1, pp. 37–40
(2004)
9. Vogt, R., Sridharan, S.: Experiments in session variability modelling for speaker
veriﬁcation. In: Proc. ICASSP, Toulouse, France, May 2006, vol. 1, pp. 897–900
(2006)
10. Solomonoﬀ, A., Campbell, W., BoardmanCampbell, I.: Advances in channel com-
pensation for SVM speaker recognition. In: Proc. ICASSP, Philadelphia, PA, USA,
March 2005, vol. I, pp. 629–632 (2005)
11. Cerva, P., Zdansky, J., Silovsky, J., Nouza, J.: Study on Speaker Adaptation Meth-
ods in the Broadcast News Transcription Task. In: Sojka, P., Hor´ak, A., Kopeˇcek,
I., Pala, K. (eds.) TSD 2008. LNCS (LNAI), vol. 5246, pp. 277–284. Springer,
Heidelberg (2008)
12. Gales, M.J.F.: Maximum likelihood linear transformations for HMM-based speech
recognition. Computer Speech and Language 12(2), 75–98 (1998)
13. Burges, C.: A tutorial on support vector machines for pattern recognition. Data
Mining and Knowledge Discovery 2(2), 121–167 (1998)
14. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines (2001),
http://www.csie.ntu.edu.tw/~cjlin/libsvm
15. Stolcke, A., Ferrer, L., Kajarekar, S.: Improvements in MLLR-Transform-based
Speaker Recognition. In: Proc. IEEE Odyssey 2006 Speaker and Language Recog-
nition Workshop, San Juan, Puerto Rico, June 2006, pp. 1–6 (2006)

Author Index
Abel, Andrew
331
Aristodemou, Elena
248
Bauer, Dominik
344
Beetz, Michael
315
Beˇnuˇs, ˇStefan
18
Bortoluzzi, Maria
50
Cerva, Petr
423
Chaloupka, Josef
324
Chaloupka, Zdenek
324
Cifani, Simone
331, 356
Costen, Nicholas
291
Dubˇeda, Tom´aˇs
126
Esposito, Anna
1, 133
Fang, Hui
291
Faundez-Zanuy, Marcos
306
Fortunati, Leopoldina
1, 5
Grillon, Helena
201
Grinberg, Maurice
63
Hanˇzl, V´aclav
399
Hofs, Dennis
276
Holub, Jan
126
Hondorp, Hendri
276
Hristova, Evgenia
63
Hussain, Amir
331
Janda, Jan
368
Jarmolowicz-Nowikow, Ewa
239
Jorschick, Annett B.
182
Kannampuzha, Jim
344
Karpi´nski, Maciej
227
Koldovsk´y, Zbynˇek
386
Kr¨oger, Bernd J.
344
Lalev, Emilian
63
Laouris, Yiannis
248
Laukkanen, Anne-Maria
90
Machaˇc, Pavel
173
Ma¨ım, Jonathan
201
Majewski, Wojciech
42
Makris, Pantelis
248
Marcozzi, Daniele
356
Maskeliunas, Rytis
409
Mayer, Christoph
315
Murphy, Peter J.
90
Nagy, Katalin
266
Nijholt, Anton
276
op den Akker, Harm
276
op den Akker, Rieks
276
Palkov´a, Zdena
149
Papa, Filomena
76
Petkov, Jakub
386
Piazza, Francesco
331, 356
Pignotti, Alessio
356
Poll´ak, Petr
377, 399
Pˇribil, Jiˇr´ı
106
Pˇribilov´a, Anna
106
Radig, Bernd
315
Rajnoha, Josef
377
Riaz, Zahid
315
Rossini, Nicla
214
Rudzionis, Algimantas
409
Rudzionis, Vytautas
409
Rusko, Milan
18
Sapio, Bartolomeo
76
Silovsky, Jan
423
Skarnitzl, Radek
162
Squartini, Stefano
331, 356
Staroniewicz, Piotr
42
Sztah´o, D´avid
266
Thalmann, Daniel
201
Tuˇckov´a, Jana
126

434
Author Index
Veroˇnkov´a, Jitka
149
V´ıch, Robert
98
Vicsi, Kl´ara
266
Vincent, Jane
1, 28
Vol´ın, Jan
190
Vondra, Martin
98
Yersin, Barbara
201
Zdansky, Jindrich
423
Zuta, Vivien
116
Zwiers, Job
276


Mobility of Visually 
Impaired People
Edwige Pissaloux
Ramiro Velázquez   Editors
Fundamentals and ICT Assistive 
Technologies

Mobility of Visually Impaired People

Edwige Pissaloux
• Ramiro Velázquez
Editors
Mobility of Visually
Impaired People
Fundamentals and ICT Assistive Technologies
123

Editors
Edwige Pissaloux
Université de Rouen Normandie
Rouen
France
Ramiro Velázquez
Universidad Panamericana
Aguascalientes
Mexico
ISBN 978-3-319-54444-1
ISBN 978-3-319-54446-5
(eBook)
DOI 10.1007/978-3-319-54446-5
Library of Congress Control Number: 2017935012
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Contents
Part I
Space for Mobility and Its Conscious Perception
Living in Space. A Phenomenological Account . . . . . . . . . . . . . . . . . . . . .
3
Gunnar Declerck and Charles Lenay
Technologies to Access Space Without Vision. Some
Empirical Facts and Guiding Theoretical Principles . . . . . . . . . . . . . . . .
53
Charles Lenay and Gunnar Declerck
Mobility Technologies for Visually Impaired People
Through the Prism of Classic Theories of Perception . . . . . . . . . . . . . . .
77
Marion Chottin
Part II
Neuro-cognitive Basis of Space Perception
for Mobility
The Multisensory Blind Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
Vanessa Harrar, Sébrina Aubin, Daniel-Robert Chebat,
Ron Kupers and Maurice Ptito
On Spatial Cognition and Mobility Strategies. . . . . . . . . . . . . . . . . . . . . .
137
Edwige Pissaloux and Ramiro Velázquez
Sensory Substitution and the Neural Correlates of Navigation
in Blindness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
Daniel-Robert Chebat, Vanessa Harrar, Ron Kupers,
Shachar Maidenbaum, Amir Amedi and Maurice Ptito
Visuo-Vestibular and Somesthetic Contributions to Spatial
Navigation in Children and Adults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Irini Giannopulu
v

Part III
Mobility of the Visually Impaired
Orientation and Mobility Training to People with Visual
Impairments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Mira Goldschmidt
Spatial Orientation in Children: A Tyﬂological Approach. . . . . . . . . . . .
263
Krystyna Nawrocka-Łabuś
Scene Representation for Mobility of the Visually Impaired . . . . . . . . . .
283
Guillaume Tatur
Model of Cognitive Mobility for Visually Impaired
and its Experimental Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
Edwige Pissaloux and Ramiro Velázquez
Solid: A Model to Analyse the Accessibility of Transport
Systems for Visually Impaired People . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
Gérard Uzan and Peter Wagstaff
Part IV
ICT Technologies and Mobility
Mobility Technologies for Blind, Partially Sighted
and Deafblind People: Design Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
M.A. Hersh
Co-designing together with Persons with Visual Impairments. . . . . . . . .
411
Charlotte Magnusson, Per-Olof Hedvall and Héctor Caltenco
Different Approaches to Aiding Blind Persons in Mobility
and Navigation in the “Naviton” and “Sound of Vision” Projects . . . . .
435
P. Strumillo, M. Bujacz, P. Baranski, P. Skulimowski, P. Korbel,
M. Owczarek, K. Tomalczyk, A. Moldoveanu and R. Unnthorsson
Overview of Smart White Canes: Connected Smart Cane
from Front End to Back End. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
469
Gianmario Motta, Tianyi Ma, Kaixu Liu, Edwige Pissaloux,
Muhammad Yusro, Kalamullah Ramli, Jean Connier, Philippe Vaslin,
Jian-jin Li, Christophe de Vaulx, Hongling Shi, Xunxing Diao
and Kun-Mean Hou
Accessible Interactive Maps for Visually Impaired Users . . . . . . . . . . . .
537
Julie Ducasse, Anke M. Brock and Christophe Jouffrais
Smart Multisensor Strategies for Indoor Localization . . . . . . . . . . . . . . .
585
Bruno Andò, Salvatore Baglio, Cristian O. Lombardo
and Vincenzo Marletta
vi
Contents

Constructing Tactile Languages for Situational Awareness
Assistance of Visually Impaired People . . . . . . . . . . . . . . . . . . . . . . . . . . .
597
Ramiro Velázquez and Edwige Pissaloux
Vision Restoration with Implants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
617
Akos Kusnyerik, Miklos Resch, Huba J. Kiss and Janos Nemeth
Mobility, Inclusion and Exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
631
M.A. Hersh
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
649
Contents
vii

About the Editors
Edwige Pissaloux is Full Professor at the University of
Rouen, Physics Department, and closely collaborates
with the ISIR (Institute for Intelligent Systems and
Robotics) at Paris-Sorbonne University and the MIT
(France-MIT program). She has authored more than
250
journals
and
conferences
papers.
Professor
Pissaloux main research interests are the modeling and
design of vision systems, cognitive perception systems,
and cognitive mobility systems.
Her research has been supported by European funds
(EU FP7 Research and Innovation Funding Program),
national funds (CNRS, CEA), and international projects
(France-UK-Mexico). Professor Pissaloux is a member
of several international advisory boards of universities
and research institutes, and teaches in several universi-
ties (Australia, Hong-Kong, Canada). She frequently
acts as an international expert for several research bodies
and international institutions such as the European
Commission, NSF/USA, Canada, UK, Australia, China,
Switzerland, etc.
Professor Pissaloux initiated a series of national
conferences such as “Space and its perception: applica-
tion to design of assistive technologies” or “Eye, gaze
and
interaction”.
She
has
participated
and/or
co-animated European Excellence networks.
In her free time, Prof. Pissaloux teaches violin for
visually impaired children.
ix

Ramiro Velázquez is Full Professor at the Faculty of
Engineering
of
Universidad
Panamericana
(Aguascalientes, Mexico). He is past Dean of the
Faculty of Engineering and currently serves as
Vice-President for Research and member of the
Board of Governors of this university.
He
obtained
the
Ph.D.
in
Robotics
from
UPMC-Paris 6 Sorbonne University in 2006. He has
authored more than 100 journals and conferences
papers. Professor Velázquez main research interests are
assistive technologies, haptic and tactile devices,
mechatronic systems, and human–computer interaction.
His research projects in assistive devices for visually
impaired and blind people have been featured in IEEE
Spectrum, CNN, and BBC Horizons.
Professor Velázquez frequently serves as an expert
evaluator for the European Commission, CONACYT
(Mexico), and COLCIENCIAS (Colombia). He is a
member
of
the
Mexican
National
Systems
of
Researchers (SNI-Level I).
x
About the Editors

Introduction
From the Antiquity to the Age of Light, men regard disability as the expression of a
divine will, sometimes positive and protective, often harmful and responsible for
making men atone for their sins. It was not until the end of the Middle Ages, with
the emergence of medicine in Europe, that a more scientiﬁc approach to man and its
body occurred. However, impaired people remain nowadays assimilated to the
broad group of the indigent who experience exclusion from society, often conﬁned
in hospitals and asylums. Only recently, social progresses (and their associated
legal framework) grant people considered as impaired (with the former name:
handicapped people) the right to work, to attend school, to access the information
available to others, to participate and to be considered as full citizens.
However, several breaks prevent the real implementation of these social and
legal progresses. The access to work, information, the socio-urban structures, and
more general to the space are examples of few, still very strong limitations. In the
case of visually impaired people the access to the space is a fundamental problem.
How to grab a cup of coffee? How to move in the main hall of an unknown
administrative building? How to reach a pharmacy nearby? How to perceive the
social sign expressed by the state of a face? How to get on a train?
This book attempts to provide some answers to the above questions and others;
more generally, the scientiﬁc questions which underlie the problems of human–
space interactions for impaired human beings. Throughout the book, the elements
that answer questions such as: what is the space? how does our brain learn, become
aware and master the space in order to allow us to be mobile? what is mobility?
how do the visually impaired practically face the space? how does technology
provide the support for space perception? for mobility? what should be the most
appropriate (thus future) concepts and technologies for true independent and
autonomous mobility? will be provided.
The proposed approach is an holistic one, which encompasses, so far, two
considered models of disability: medical and social. Indeed, the book aims to
provide the unchangeable/immutable bases for the design of new mobility assistive
ICT (information and communication technologies) devices for the visually
xi

impaired (blind, seniors, ﬁreﬁghters, people whose sight is involved in other
complex tasks, etc.), but also for our (humanoid) robots and “intelligent systems”.
The holistic framework for mobility is the concept of mobility we have learnt
unconsciously in our childhood, which should be rediscovered or discovered as the
new technology take us to new spaces (cyber, virtual reality, and stars).
The holistic approach to the design of new (mobility assistive ICT) technology
results from exchanges we had during the international workshops on NoE (net-
work of excellence) such as EUCOG (for the Advances of Cognitive Systems,
Interaction and Robotics) or CVHI (Conferences on Assistive Technologies for
Vision and Hearing Impairments), both supported by the European Commission
and, national events (co-)organized in France by the CNRS/GDR STIC-Santé, by
the IFRATH (Institut Fédératif de Recherche sur les Aides Techniques pour
Personnes Handicapées) and by the PRIMOH (Pôle de Recherche et d’Innovation
en MObilité et Handicap).
This book also results from the patience and continuous support of Springer
Editorial staff: Zoe Kennedy and James Mary. We would like to thank them for
their enthusiasm and advice during the long gestation of this book.
We hope that this book will allow a better understanding of human mobility for
healthy and visually impaired people, and the interactions with the space. We hope
that the research and practical elements (provided by the mobility instructors in this
book) will contribute to new research tracks, to obtain the complete model of
human mobility, to generate new mobility paradigms which govern our interaction
with the space, to design and implement new mobility curricula for a better social
integration of the visually impaired. Furthermore, we hope that this book will be a
basis for the design of new academic curricula and research projects dedicated to
the design and realization of new efﬁcient mobility aids, which effectively and
efﬁciently assist the visually impaired.
Finally, we take the opportunity to thank all the contributing authors who did a
great job.
We dedicate this work to our families.
Who May Beneﬁt from Reading This Book?
The book is designed to inform a wide range of current and future professionals in
all domains considered by the book:
Researchers in philosophy, neurosciences, engineering sciences, and locomotion
sciences will ﬁnd several useful and pertinent elements for their knowledge
openings to complex domains which may help them for a deeper understanding
of their own domains; the book provides the current state-of-the-art in these
domains.
To students, the book proposes a scientiﬁcally grounded insight into new
domains: sciences of mobility and sciences of visually impaired people. Indeed, the
xii
Introduction

book provides examples of technological solutions for the design of new interaction
modes and interactive devices, not only for mobility. The outlined solution may
stimulate their imagination for designing new innovative technology of high social
impact.
For university academics, the book may be an inspiration for adaptation of
existing biomedical engineering curricula with a conceptual approach (based on a
philosophical framework) to society developments and needs: the book may be a
base for such curricula building and even used as a textbook. The locomotion
classes offered today only in speciﬁc institutions should be transformed in regular
classes on space perception. Indeed, several space sicknesses exist: they have been
discovered through recent clinical progresses on the sense of balance and with the
development of virtual reality technologies. These sicknesses are today rehabilitated
with invasive technologies (such as FES, Functional Stimulation System for
hemi-spatial
neglects,
or
with
TDU,
Tongue
Device
Unit
developed
by
Bach-y-Rita’s team for example). The time is to offer noninvasive technologies
which reinforces all human capabilities (“augment the man”) by the reinforcement
of natural sensor-motor perception cognitive loops; such new class of assistive
device will adapt the technologies to human needs allowing visually impaired to
concentrate on its task (and not on understanding the data provided by the device
which is usually the case of existing assistive devices, which require the human
adaptation to the technology).
To future technology designers, the book explains which are the philosophical
concepts, computational models, and sensor-motor perception cognitive loops that
should be investigated in order to design new systems for interaction (for both
impaired and healthy people).
To foundations and public organizations (such as the European Commission or
FIRAH in France, National Science Foundation, the National Institutes of Health),
the book will show the most promising approaches in order to deeper the existing
scientiﬁc knowledge and new research tracks worthy to explore.
To institutions working with the visually impaired, the book offers a worldwide
tribune to establish and exchange the knowledge taught and elaborated by them.
Mobility learning approaches and the newest technology are presented. We hope
that the book will initiate the dialogue between these professionals and scientiﬁc
engineers and researchers, and lead to new practices for mobility teaching. Indeed,
the new technology will be advantageous with mobility teachers but will never
replace them.
For visually impaired associations (AFB in the USA, RNIB in UK, AVH in
France, ONCE in Spain, and Blind Associations over the world), the book will
provide the latest ﬁndings in assistive technologies for mobility and give them an
opportunity to evaluate new solutions and contribute to knowledge of the mobility.
We hope that the book will stimulate the dialogue and the involvement of the target
population in participative design the technologies for them.
Introduction
xiii

An Overview of the Book
The book is organized in four parts, followed by a short note on “Future trends”.
They are
– Part I, “Space for Mobility and Its Conscious Perception”,
– Part II, “Neuro-cognitive Basis of Space Perception for Mobility”,
– Part III, “Mobility of the Visually Impaired”,
– Part IV, “ICT Technologies and Mobility”.
Each part contains several independent chapters.
Part I, “Space for Mobility and Its Conscious Perception”, includes three
chapters.
Chapter “Living in Space. A Phenomenological Account” by Gunnar Declerck
and Charles Lenay highlights the main phenomenological features of lived space.
A lived space is the space experienced by the subject through various intentional
modes, perceptual or not, frequently based on memory. An overview of the most
important phenomenological accounts made in the literature is recalled, with a
focus on the following topics: the relationships between motricity and lived space;
the relationships between body materiality and experience of space; the role of the
anticipation of possibilities in the enacting and organization of lived space; the role
of sociality and the impact of one’s body ‘visibility’. The objective of this overview
in the context of this book is to get a better understanding of the experience of space
in visually impaired people. Based on this phenomenological account, this chapter
will, as a result, offer a series of reﬂections about the peculiarities of the space blind
people live in.
Chapter “Technologies to Access Space Without Vision. Some Empirical Facts
and Guiding Theoretical Principles” by Charles Lenay and Gunnar Declerck
identiﬁes principles on which the prosthetic perceptual devices works, the condi-
tions of their appropriation, and the general perspectives they open concerning the
role of technical objects and systems in the constitution of human experience. As
technical device for assisting perception are supposed to compensate a sensory
deﬁcit by mobilizing other subsisting sensory modalities, it is necessary to under-
stand the relationship between perceptual activity and sensory input. Finally, the
technical devices which assist perceptual activity can in turn serve as tools for
experimental scientiﬁc research on the mechanisms of perception in general. This
chapter investigates these complementary and synergetic approaches.
Chapter “Mobility Technologies for Visually Impaired People Through the
Prism of Classic Theories of Perception” by Marion Chotin proposes a return on the
theories of the perception of the Classical Age (seventeenth–eighteenth centuries)
and asks if these theories are likely to illuminate contemporary techniques of
mobility for the visually impaired. The inquiry begins with the Cartesian theory of
perception and establishes that Descartes conceived the cane of the blind as an
instrument allowing access to new perceptions, under the assumption that a per-
ceptive learning eliminates the divine laws of the union of the soul and the body.
xiv
Introduction

Thus, the Cartesian theory makes it possible to account for the functioning of more
recent devices, such as the TVSS of Bach-y-Rita. On the other hand, the Cartesian
theory of perception does not describe how one learns how to use the cane.
Therefore, the chapter mobilizes the Condillac theory of perception, which, for its
part, endeavors to describe how a blind man learns to perceive objects by means of
his cane, namely by gradually exteriorizing his sensations. Finally, Condillac’s
theory is confronted with that of Merleau-Ponty, more frequently mobilized by
researchers today, with the aim of evaluating their theoretical and practical
contributions.
Part II, “Neuro-cognitive Basis of Space Perception for Mobility”, includes four
chapters.
Chapter “The Multisensory Blind Brain”, by Vanessa Harrar, Sébrina Aubin,
Daniel-Robert Chebat, Ron Kupers, and Maurice Ptito, addresses the topic of brain
multisensory plasticity and of brain reorganization across the usual sensory
boundaries. Through different experiences, this chapter evidences that instead of
being sensory speciﬁc, cortical regions of brain appear to be functionally speciﬁc.
Such ﬁndings allow the redeﬁnition of current rehabilitation schemes and can be
used to improve assistive technologies built for the blind people (as the occipital
cortex seems to be amodal).
Chapter “On Spatial Cognition and Mobility Strategies”, by Edwige Pissaloux
and Ramiro Velázquez, discusses the current state of the art of sensory data which
seems to be involved in space perception emergence, in movement perception and
in the spatial navigation concept. It outlines also some neural evidences of spatial
cognition and some ﬁndings on spatial cognition in animals. Finally it recalls some
of the most popular models for spatial knowledge structuring and identiﬁes some
open questions of spatial knowledge of VIP (visually impaired people).
Chapter “Sensory Substitution and the Neural Correlates of Navigation in
Blindness”, by Daniel-Robert Chebat, Vanessa Harrar, Ron Kupers, Shachar
Maidenbaum, Amir Amedi, and Maurice Ptito, presents the most recent advances in
the theory of SS (sensory substitution) and evidences the neural correlates of
navigation in CB (congenital blind) people. The analysis of recent theories on the
phenomenological properties of sensory substitution and of recent literature on
spatial abilities of participants using SS devices (SSD)s show that the sensory
information is treated in brain in an amodal fashion. Such results allow CB indi-
viduals to navigate in real and virtual environments however and even perform
better in a variety of sensory and cognitive tasks. Therefore, the relevant training
with appropriate devices may exploit brain’s plasticity and create perception and
other sensation such as qualia.
Chapter “Visuo-Vestibular and Somesthetic Contribution to Spatial Navigation
in Children and Adults”, by Irini Giannopulu, discusses all known human param-
eters involved in the development of spatial navigation skills and their neural
correlates. The current state of the art of the inﬂuence of multimodal visuomotor,
vestibular, and somesthetic abilities for ego motion acquisition and performance, in
conjunction with attention and memory processes in real and virtual environments
is surveyed.
Introduction
xv

Part III, “Mobility of the Visually Impaired”, includes ﬁve chapters.
Chapter “Orientation and Mobility Training to People with Visual Impairments”,
by Mira Goldschmidt, presents the speciﬁc mobility challenges of people with
visual impairment as well as the strategies and techniques used to improve their
orientation and mobility. The chapter also describes the use of mobility devices
helping persons with visual impairment to travel safely.
Chapter “Spatial Orientation in Children: A Tyﬂological Approach”, by
Krystyna Nawrocka-Łabus, addresses the concept of spatial orientation and its
shaping in children, gives some methodological topics on the work with visually
impaired children when teaching them how to acquire the spatial cognition, and
proposes some open questions of spatial knowledge of visually impaired people
(VIP). This chapter discusses several simple exercises which may be assisted by
mobility aids and several games for spatial knowledge acquisition (which may
become serious game); moreover, it suggests that the tactile space of children and
adult may be different, and touch stimulation representation of the space may
require different approaches for adults and children.
Chapter “Scene Representation for Mobility of the Visually Impaired”, by
Guillaume Tatur, presents different representations of space proposed in sensory
substitution, sensory supplementation, and visual neuroprostheses technical solu-
tions in order to assist the mobility of visually impaired people. The chapter pro-
vides also some general recommendations related to scene representation design
and discuss the beneﬁts of providing specialized information dedicated to mobility
as opposed to a general-purpose representation.
Chapter
“Model
of
Cognitive
Mobility
for
Visually
Impaired
and
its
Experimental Validation”, by Edwige Pissaloux and Ramiro Velázquez, con-
tributes to the emergence of the theoretical framework for the design of mobility
aids for visually impaired people (VIP). This framework is based on the under-
standing and interaction (through a touch stimulating device, the underlying sen-
sory motor loops) with space through our senses and proposes a new model of
mobility. The chapter brieﬂy discusses different theories for emergence of space
understanding from our senses, and recalls some models of human mobility before
proposing a new holistic four-component new model of human cognitive mobility.
The latter is based on the concept of tactile gist, which reinforces the emergence of
sensorimotor loops from our perceptions, for understanding of the organization of a
physical space. Some reported original experiments related to the execution of basis
mobility tasks conﬁrm the relevance of the tactile gist to represent a space. These
latter can be used as a benchmark for evaluations and comparisons of mobility
devices, and as a revealer of potential improvements of mobility aids.
Chapter “Solid: A Model to Analyse the Accessibility of Transport Systems for
Visually Impaired People”, by Gérard Uzan and Peter Wagstaff, proposes a new
model developed to analyze the basic requirements and tasks involved to ensure
that optimum solutions to guarantee accessibility to public spaces for all types of
user in any situation. The model, developed originally for applications for the blind
and visually impaired persons (VIPs) in public transport, is generalized here and
can be applied to analyze accessibility in many other situations. In the context of
xvi
Introduction

transportation systems, it enables to identify the essential tasks, requirements, and
information needed at each stage of the chain of displacements. SOLID is an
acronym for the initial letters of the ﬁve essential elements of the model, which are
Safety, Orientation, Localization, Information and Displacement.
Part IV, “ICT Technologies and Mobility”, includes nine chapters.
Chapter “Mobility Technologies for Blind, Partially Sighted and Deafblind
People: Design Issues”, by M.A Hersh, presents an overview of the most popular
travail aids for assistance of mobility of visually impaired. The overview shows
clearly the importance of the active involvement of blind, partially sighted and
deafblind end-users in the design process of a new assistance; we talk about the
participative design. An understanding of how blind people travel, including the
ways they use information from all their senses and travel aids, is indeed an
essential prerequisite for good design. The chapter provides also a brief presentation
of the different phases in the development of travel aides, a summary of the different
ways of categorizing them and some principles of good practice for developing new
travel aids. Speciﬁc features, such as the sensors and interface and features,
including privacy management and contextual features, are also considered.
Chapter “Co-designing together with Persons with Visual Impairments”, by
Charlotte Magnusson, Per-Olof Hedvall, and Héctor Caltenco, proposes a general
background
on
the
design
process,
and
follows
it
by
a
discussion
of
human-centered design and co-design. Six concrete examples of design activities
illustrate how useful it can be to engage in co-design, and as practical inspiration for
future design activities. The examples are followed by a discussion of activities and
materials, inclusive design and accessibility and the importance of the context of
use. The chapter shows that there is no need to feel daunted by organizing design
activities together with persons with visual impairments.
Chapter “Different Approaches to Aiding Blind Persons in Mobility and
Navigation in the “Naviton” and “Sound of Vision” Projects”, by P. Strumillo, M.
Bujacz, P. Baranski, P. Skulimowski, P. Korbel, M. Owczarek, K. Tomalczyk, A.
Moldoveanu, and R. Unnthorsson, summarizes several years of research efforts
aimed at building ICT (Information and Communications Technologies) based
systems for helping the blind in travel and navigation at the Lodz University of
Technology, mainly as part of the “Naviton” project. The following prototype
solutions are outlined: (1) soniﬁed stereovision system for obstacle avoidance and
environment imaging, (2) radio beacons for local navigation, (3) remote assistance
system, (4) mobile navigation applications, (5) real-time tracking of public transport
vehicles, (6) haptic imaging. These technologies are shortly presented and feedback
from end-users, after their evaluation, is reported. The ongoing European project
of the Horizon 2020 framework program and entitled “Sound of Vision: natural
sense of vision through acoustics and haptics” is also presented.
Chapter “Overview of Smart White Canes: Connected Smart Cane from Front
End to Back End”, by Gianmario Motta, Tianyi Ma, Kaixu Liu, Edwige Pissaloux,
Muhammad Yusro, Kalamullah Ramli, Jean Connier, Philippe Vaslin, Jian-jin Li,
Christophe de Vaulx, Hongling Shi, Xunxing Diao, and Kun-Mean Hou, presents
the state-of-the-art of ETAs (Electronic Travel Aids) and focuses on their
Introduction
xvii

functionalities, hardware architectures and integration of ICT (Information and
Communication Technologies) such as cloud computing, IoT (Internet of Things)
and smartphone. Connected Multi-Input Multi-Output ETA—called MIMO eETA
—will improve safety of the blind by providing more relevant environment per-
ception. Therefore, MIMO eETA will signiﬁcantly improve VIPs’ well-being by
easing their mobility and quality of life. Four sections of this chapter introduce
(1) general knowledge about VIPs, their mobility, and a classiﬁcation of electronic
devices used to help them in their mobility; (2) a description of a new concept of
such tool, the eETA, with the description of its desired functionalities; (3) the
current state of the implementation of an eETA, its architecture and related
experimentations and (4) the back-end tier is analyzed (its speciﬁcations, functions
and implementation).
Chapter “Accessible Interactive Maps for Visually Impaired Users”, by Julie
Ducasse, Anke M. Brock, and Christophe Jouffrais, introduces a novel concept,
interactive map for space understanding by visually impaired; its originality comes
from the interactivity offered to users which helps them in their understanding of the
space geometry, of space geography and in the acquisition of new spatial knowl-
edge. A classiﬁcation of interactive accessible maps is proposed with two main
subclasses: digital interactive maps and hybrid interactive maps. All of them are
analyzed and compared. Ongoing and future research in interactive tactile graphics
is shortly discussed as well.
Chapter “Smart Multisensor Strategies for Indoor Localization”, by Bruno Andò,
Salvatore Baglio, Cristian O. Lombardo, and Vincenzo Marletta, focuses on indoor
localization implemented by traditional and advanced enabling technologies,
measurement strategies, and applications. In particular, the discussion is oriented to
perform a benchmark between different solutions with a speciﬁc focus in the
framework of Active and Assisted Living. A case of study of an Assistive Systems
supporting blind people during the exploitation of indoor environments is also
presented.
Chapter “Constructing Tactile Languages for Situational Awareness Assistance
of Visually Impaired People”, by Ramiro Velázquez and Edwige Pissaloux, reports
on the ﬁndings from an experiment on human performance in tactile language
learning and tactile memory. A set of four vibrotactile patterns representing verbal
words was presented to a group of 20 voluntary subjects. Upon learning, subjects
were capable of recognizing the patterns with high accuracy. Patterns were then
combined with the aim of constructing sentences that gradually represent more
complex ideas. Recognition rates remained satisfactory for sentences involving two,
three, and four tactile words. A novel approach of tactile stimuli was explored:
podotactile stimulation. For this study, a prototype of wearable electronic tactile
display that stimulates the foot sole with vibrations was used. Results obtained
suggest that it is possible to construct tactile languages that could enhance situa-
tional awareness feedback provided by assistive devices.
xviii
Introduction

Chapter “Vision Restoration with Implants”, by Akos Kusnyerik, Miklos Resch,
Huba J. Kiss and Janos Nemeth, overviews a prosthetic vision solutions, i.e.
implants with different operating principles and in various stages of progress. They
are presented in detail, with the characteristics of the development highlighted. The
continuous evolvement of microelectronics and engineering achieved the devel-
opment of devices becoming highly similar to the light-sensitive retinal structures.
Such an intra-retinal implantable device replaces the photoreceptors and generates
biological signals induced by the incoming light. The aim of this chapter is to
introduce the reader these techniques and ﬁeld, and to compare the simultaneous
developments of technical novelties and clinical studies.
Chapter “Mobility, Inclusion and Exclusion”, by M.A. Hersh, discusses several
topics related to the relationships between visual impairments and society. The
importance of mobility for participation in society is discussed ﬁrst through the
accessibility of public transports and private cars. Then, the societal attitudes to
blindness are investigated; stigma and stereotypes, acceptance and conﬁdence
stigma and use of mobility technology are considered. Finally, the accessible
environments are analyzed and some of the potential solutions are discussed.
Introduction
xix

Part I
Space for Mobility and Its Conscious
Perception

Living in Space. A Phenomenological
Account
Gunnar Declerck and Charles Lenay
1
Introduction
Accounts of space, its nature and how one’s mind can access it through perception
or thinking, abound in the history of Philosophy, but broadly two opposite views
can be distinguished: realist and constructivist accounts. While realism treats space
as an intrinsic feature of the reality with which the perceiving agent must deal,
constructivism sees space as an original achievement of the mind, something the
mind builds to organize its understanding of reality. The one who opened the way
to constructivist approaches to space is undoubtedly Immanuel Kant. Kant rejects
the traditional realist view of space, and considers instead space as an a priori form
of outer intuition, i.e. what could be described, in more modern terms, as a rep-
resentation format that is used when processing sensory data. One must make use of
this format for the sensory data (e.g. visual or haptic data) to be related to objects
outside of us, i.e. to acquire what contemporary philosophers of mind call a rep-
resentational content, present the world as being such and such.1 For Kant, space is
G. Declerck  C. Lenay (&)
Sorbonne Universités, Université de Technologie de Compiègne,
EA 2223 Costech (Connaissance, Organisation et Systèmes Techniques),
Centre Pierre Guillaumat - CS 60 319-60 203, Compiègne, France
e-mail: charles.lenay@utc.fr
G. Declerck
e-mail: gunnar.declerck@utc.fr
1As Allais [1: 384] explains, for Kant “it is a condition of the possibility of being perceptually
presented with empirical particulars that we represent the world spatially […].” More precisely,
“Kant claims that the representation of space is necessary: to represent things as distinct from and
outside me; to represent things as in different places/as spatially located; to represent things as
spatially related; and to represent things as distinct/different from each other. […] Having, through
sensory affection, a presentation of a particular involves representing a thing that is outside of me,
is distinct from other things, is located, and is spatially related to other things.” [1: 410].
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_1
3

a property of the objects that are spontaneously presented to one’s mind when being
sensory stimulated, that is, a property of the phenomena, not a property of the
mind-independent world that is—putatively—the cause of these stimulations. Why
our mind makes use of spatial representations is not Kant’s concern, but a possible
answer (which can be found in neo-Kantian such as Friedrich Nietzsche, Jean
Piaget, and Henri Poincaré) is that this representation format has an important
practical—and adaptive—value: it enables the subject to ﬁnd its way in the world in
a safe and efﬁcient manner, keep track of targets or remember where things are.2
Our aim in this chapter is not to discuss Kantian approaches to space, nor will we
defend a Kantian account in the strict sense. Our concern will be to address the
experience of space from a theoretical perspective having its roots in Kant, namely
phenomenology. The objective of phenomenology is not to explain why things are
the way they are (identify some conditions Cx to which a phenomenon u can be
causally or ontologically reduced3), but it is to describe how things spontaneously
appear to our subjectivity, that is, to analyze from a ﬁrst person perspective the
phenomena of consciousness, and generally as they occur in ordinary life. As
Heidegger puts it, the task of phenomenology is to describe our being in the world
“as it is primarily and usually—in its average everydayness” [60: 37, 38 [17]].
Accordingly, our primary objective in this chapter will be to analyze from a ﬁrst
person perspective the content of our pre-scientiﬁc (or naïve) experience of space
and the intentional mechanisms subtending this experience. We will consequently
leave aside the theoretical understanding of space (its nature and properties) that can
be achieved based on scientiﬁc modes of explanation and inquiry. Note, though,
that we will occasionally mention some empirical data, especially from psychology,
generally to corroborate the phenomenological descriptions with behavioural ele-
ments. And we will also draw on the way we speak of space in everyday language
to complement the standard phenomenological approach. We follow J.L. Austin’s
assumption that the analysis of the way we talk ordinarily, what terms we use to say
2The attentive reader will certainly have noticed that all these formulae are circular: to describe the
advantage of representing the—not ‘yet’ spatial in the strict sense—reality as a spatial world, one
cannot do without using spatial concepts.
3The canonical form of the scientiﬁc explanation is the reduction: to explain an observable phe-
nomenon u (say, the type of feeling experienced when hungry) amounts to identify a set of
conditions Cx, generally belonging to a different ontological domain (e.g. physicochemical or
neural informational processes), upon which the apparition of u is contingent. As a result, u is
considered explained because causally, or even ontologically reducible to Cx (that is, we have
Cx ! u and ¬ Cx ! ¬ u); in the latter case, it will be claimed that u is nothing but Cx, which
means adopting a form of ontological reductionism. See Searle [102], chap. 5, especially 112–113.
“The basic intuition that underlies the concept of reductionism seems to be the idea that certain
things might be shown to be nothing but certain other sorts of things.” [102: 112].
4
G. Declerck and C. Lenay

what (what Austin calls the “what we would say when”), carries information about
the way we perceive or conceive our world.4
Phenomenology is a broad philosophical ﬁeld, where many accounts of space
have been proposed, with sometimes deep disagreements between authors. Our
analysis will mainly focus on the following topics: the relation between bodily
skills, and more generally motricity (i.e. the capacity to move), and lived space; the
impact of one’s body materiality on one’s experience of space: how possessing a
physical body with material properties such as impenetrability and heaviness affect
one’s experience of space; the role the anticipation of possibilities plays in the
enacting and organization of lived space; and, ﬁnally, the role of sociality and the
impact of one’s body ‘visibility’, i.e. the fact that one can be seen (or more gen-
erally perceived) by others, in one’s experience of space. An objective behind the
selection of these topics is to get a better understanding of the experience of space
in visually impaired people. Based on this phenomenological overview, we will, as
a result, offer a series of reﬂections about the peculiarities of the space blind people
live in (Sect. 6).5
Note, before proceeding, that, from a phenomenological standpoint, lived space
can be accessed through different types of intentional (i.e. mental) acts. Two broad
categories of intentional acts can be distinguished, presentational and representa-
tional acts.
Perception belongs to the ﬁrst category. Perception provides a somewhat direct
access to space: one directly sees how objects are arranged or one directly hears
where sounds come from. In more technical terms, perception is a presentational
mode of awareness: being perceptually aware of something implies that there is
something currently present, something there, that one is aware of.6
But space is also something we access though various representational modes of
awareness, such as memory and imagination. When I engage in an explicit imagery
activity about some place I am acquainted with, e.g. think about the spatial
4“When we examine what we should say when, what words we should use in what situations, we
are looking […] not merely at words (or ‘meanings’, whatever they may be) but also at the realities
we use the words to talk about: we are using a sharpened awareness of words to sharpen our
perception of, though not as the ﬁnal arbiter of, the phenomena.” [5: 182] This is why Austin calls
his method ‘linguistic phenomenology’.
5Note that when speaking of ‘space’ in this chapter, we generally refer to the continuum that is
coextensive with the objective world (which should not be confused with what is sometimes called
the physical reality: objectivity, here, is considered in a Kantian sense, as something that is
somehow public, i.e. applies for a community of subjects, but which is relative to these subjects).
Space is that in which we are with our body and where any concrete being must be located to exist
(see Sect. 2.3). ‘Space’ is, in this regard, another term for ‘world’: this is the world –this world
extending ever further, endlessly—considered in abstraction from all it ‘contains’ and to the extent
that it can contain everything. As a result, we will sometimes make use of the term ‘world-space’,
which can be found in Husserl and Heidegger.
6As Crane and French [26] explain: “Perceptual experience, in its character, involves the pre-
sentation (as) of ordinary mind-independent objects to a subject, and such objects are experienced
as present or there such that the character of experience is immediately responsive to the character
of its objects.”
Living in Space. A Phenomenological Account
5

arrangement of the shelves of the supermarket where I shop, the space that is made
‘present’ through this mental episode is precisely not present, i.e. is not there now
(that is, in my immediate proximity). Certainly, the supermarket currently exists,
and when I am imagining its spatial arrangement, this is an existing object that I am
intending (I can, on the other hand, imagine the spatial arrangement of non-existing
or no more existing places, such as the Ancient Library of Alexandria, based on a
series of engravings). Though the supermarket is too far to be seen, I know I just
have to walk to access it perceptually and ‘be there’. Its ‘presence’ is somewhat
founded on its perceptual accessibility [87, 89]. But the arrangement of the shelves I
imagine is not there before me now, and the aspects under which I imagine the
place are not imposed by my present body orientation, nor immediately responsive
to my movements, as it is the case when I perceive it visually. Perceptual pre-
sentation makes the object actually there ‘in person’, while non-presentational (i.e.
representational) modes of awareness intend their object without making it present
(i.e. ‘there’) in the strict sense.7
Note that there is another sense in which space is ‘represented’, that applies both
to presentational and representational modes of awareness, namely the fact that any
awareness of a place implies some—more or less explicit—belief about, or
acquaintance with, the spatial characteristics of that place, how it is organized, what
you can ﬁnd in that place and where, or where that place is with respect to some
other places, how one can reach it. This points to the everyday sense of the term
‘representation’: how one ﬁgures (i.e. presumes or reckons or imagines) that
something is, what features one attributes to this something. How one conceives a
place undoubtedly plays a decisive role in how one perceives a place, that is, in the
so-called ‘representational’8 content of one’s perceptual experience: what one sees
7“The object stands in perception as there in the ﬂesh, it stands, to speak still more precisely, as
actually present, as self-given there in the current now. In phantasy, the object does not stand there
as in the ﬂesh, actual, currently present. It indeed does stand before our eyes, but not as something
currently given now; it may be possible to be thought of as now, or as simultaneous with the
current now, but this now is a thought one, and is not that now which pertains to presence in the
ﬂesh, perceptual presence. The phantasized is merely ‘represented’ (vorgestellt), it merely places
before us (stellt vor) or presents (stellt dar), but it ‘does not give itself’ as itself, actual and now.”
[68: §4, 12].
8As Peacocke [92] explains, “the representational content of a perceptual experience has to be
given by a proposition, or set of propositions, which speciﬁes the way the experience represents
the world to be.” In other terms, a perceptual experience has a representational content insofar as it
presents objects, properties, facts, state of affairs, and so on, that are supposed to be present or be
the case in the environment. Note that the concept of representation we use in this chapter has a
purely phenomenological scope, that is, it aims at describing the content of one’s experience, and
should not be taken with an epistemological nor ontological meaning. Especially, this phe-
nomenological concept does not entail a commitment to representationalism, in the classical sense
of assuming the existence of neural states that stand in for state of affairs in the external world (see
e.g. [24, 31]). The only claim we make is that perceptual awareness, understood as an intentional
state, is something that presents the world as being this or that way. Or, to say it a bit differently, in
order to describe the perceptual experience of a subject, you need to take into account that this
experience presents the world as being this or that way. This is a purely descriptive constraint.
6
G. Declerck and C. Lenay

(the spatial arrangement one is aware of) when accessing visually to a place
depends to a certain degree on what one already knows about that place (or the
places of this kind). One’s prior knowledge about that place informs one’s per-
ceptual content, ﬁrst of all by shaping the anticipations or expectations one spon-
taneously have with respect to one’s subsequent experience of that place, what one
can see of it, what one will ﬁnd, if one executes this or that exploratory movement
[67: §8 and §25]; [84]. This is what we are going to see with the analysis of a ﬁrst
important feature of lived space, namely its reticular organization.
2
Lived Space as an Interconnected
Set (Network) of Places
2.1
The Reticular Organization of Lived Space
A ﬁrst phenomenological feature of lived space that is worth noting is that it is
essentially experienced as a system of interconnected places, which can somehow
be equated to a network.9 Each place is a node that is directly connected to some
other places (the neighbouring places) and is only indirectly connected to all the
others. Places have a distinct identity (home, ofﬁce, shop, park, etc.) and are, as it
were, compartmentalized (when you get ‘into the shop’ you are no more ‘in the
street’), but they communicate in a certain way. Places are parts of a single network,
and their situation in this network contributes to deﬁne what place they are.
Different levels of granularity can also be considered: each dwelling is an auton-
omous network of places: the kitchen, the living room, the bedrooms, the toilets, the
garage, the garden, etc. But it is also a node in the bigger network the city consists
in. And the cities are themselves nodes in a bigger network: the district or the
country.
How the network is organized depends primarily on how one makes use of it,
what places one frequents, and how one gets from one place to another, how much
time it needs, and if one must transit through other places. Emphasizing the fun-
damental reticularity of lived space amounts, in that regard, to putting forward that
the structure of space is determined by how one ‘operates’ in space and navigates
one’s way across places—thus, as we shall see below (Sect. 3), the essential
reciprocity between lived space and one’s bodily capacity for action. What places
are immediately connected, i.e. constitute adjacent nodes in the network, depends
on whether there is an available possibility of transiting ‘directly’, i.e. without
passing by other intermediary places, from one place to another. The cellar and the
9The systematic account of the reticular organization of lived space that we propose in this section
is, to a large extent, a personal reconstruction, but one can ﬁnd elements supporting this view in
several phenomenologists, e.g. Husserl [68], Heidegger [60, see especially §22] and Campbell
[16].
Living in Space. A Phenomenological Account
7

living room are adjacent nodes, for one can directly reach one from the other: they
communicate. But the bedroom upstairs and the cellar are not adjacent, for one must
transit through the living room to reach the latter from the former. This reticular
organization explains that geographically distant places can be brought closer
through modern means of transport. As noted by Erwin Straus, “for the European of
the twentieth century, America is much closer than for the sixteenth century nav-
igator. The man whose car was parked at the door of his house is closer to the post
ofﬁce than the pedestrian.” (Straus [107: 455], our translation) But this is not only a
question of time needed for the travel; what also matters is the number of places one
must get through before reaching our destination. Paris Charles de Gaulle airport is
‘directly connected’ to New York John F. Kennedy airport, for one can (almost)
directly reach one from the other (the only intermediate place one must get through
is the inside of the plane). Both constitute adjacent nodes, and both are ‘close’ in
that respect.
It is thus included in our ordinary understanding of space (our ‘concept’ of
space) than any place communicates with all the others, in the sense that any place
is accessible from any other. Wherever one is, it is possible to access any other
place, by passing through the intermediate places. This possibility is a de jure
possibility, a possibility in principle, not necessarily an actual concrete—a de facto
—possibility. As a matter of fact, accessing these places may not be possible, e.g.
blocked or forbidden, or may require means of transport that do not exist (Pluto’s
surface is accessible in principle, step by step, from my living room, but factually it
is not).10 Some places may also have existed in the past but no longer exist today. In
this case, they have ceased to be accessible in principle. The part of world-space
that these places occupied is now occupied by something else. This is where the
train station was located early in the century; today it is a trafﬁc circle.
The reticular organization of lived space shapes the content of one’s perception
and is always part of one’s awareness of the situation, ﬁrst of all one’s implicit
knowledge of ‘where one is’. Whatever the situation, one is always, to some degree
or another, aware of where one is, one knows what other places one ﬁnds nearby,
that is, the place where one ﬁnds oneself has been assigned a position in the general
network of places one is acquainted with. Certainly, it may occasionally happen that
one in fact does not know where one is—one may be lost or disoriented–, but when
this happens one does not cease to believe that there are neighbouring places and
that going through these places will ﬁnally lead one to a familiar place. Any place
adjoins other places: this is a necessary principle. It may seem evident, but the
10Note that the difference between de jure and de facto possibilities, which is a key element to
understand our ordinary experience of space, can be formalized using David Lewis’ and Robert
Stalnaker’s possible worlds semantics: a de facto possibility is a possibility that can be actualized
in the actual world; a de jure possibility is either a de facto possibility or a possibility that can be
actualized in (at least some) other possible worlds. In some possible worlds, means of transport
exist that can be used to reach Pluto’s surface. How close (i.e. similar) to the actual world the
possible worlds must be for these possibilities to be considered de jure possibilities in the actual
world is a matter of debate.
8
G. Declerck and C. Lenay

pervasiveness of this principle in our spatial cognition is also what explains that in
whatever room, you always know (i.e. you live in the implicit certainty) that you
can get out.
Our knowledge of (or acquaintance with) the organization of the spatial network
also plays a somewhat more direct role in our perceptual experience of places. What
one knows about the situation of the place where one ﬁnds oneself in the network,
what bigger system this place is a part of and what places communicate with it,
informs one’s perceptual experience of that place. It is a fact that the limited portion
one sees of a place at a given instant always extends beyond the limits of what
strictly speaking one currently ‘sees’. What one sees is always seen as a part of
something bigger.11 My current visual access to the kitchen is limited to a single
part, on which I have in addition a determinate perspective, going with determinate
occlusion phenomena. Yet, there are many things (in fact, a whole world) that I do
not see but that I expect or know to be there. And the parts that I do not see but
extends beyond the portion I see play a pivotal role in determining the identity of
what I currently see. I identify what I see as ‘the kitchen of my house’ for I
apprehend the part of space falling under my view as having these or those spatial
relations with other places, ﬁrst of all the other rooms of the house, but more
generally all the places I know that extend beyond.
A possible description of this phenomenon is that when accessing perceptually
to a place, one’s perceptual content acquires its spatial content12 (i.e. it becomes the
perception of a given place, accessed from a given orientation) through a ‘broad-
ening’ process, through which the limited portion of space in view is ‘extended’
through its inscription in a kind of overall ‘map’ (see 38: 163, for a similar
hypothesis). Before this operation does this job, what one sees is literally nowhere,
has no spatial connectedness with any other place, and is consequently not oriented.
The phenomenon of ‘boundary extension’ [52, 71], namely the fact that “people
remember seeing more of a scene than was present in the physical input, because
they extrapolate beyond the borders of the original stimulus” [22: 2068], is an
indirect evidence in favour of this claim: it demonstrates that when one sees a place
(or a photography of a place), one in fact always perceives more than what one
properly ‘sees’, for one anticipates that there is more beyond. This latter issue points
towards a problem known in philosophy of mind as the ‘problem of perceptual
presence’, namely the fact that many aspects of the world that do not currently fall
under our perception, are not currently accessed, are nevertheless present, partici-
pate in what there is for us now [87, 89].
11As Chadwick et al. [22: 2067] explain: “what we see is always embedded within a wider context.
As such, we never perceive what is in front of our eyes in complete isolation, but instead an object
is perceived as part of a visual scene, and each scene as one of an inﬁnite set of related scenes that
somehow form a continuous sense of space and place.”
12We consider the spatial content of perceptual experience as a type or subcomponent of its
representational content, namely the features of the representational content that are properly
spatial.
Living in Space. A Phenomenological Account
9

A ﬁnal point that must be made is that the reticular organization of lived space is
what enables the particular place where one ﬁnds oneself to fulﬁll an exhibiting
function (i.e. be the manifestation of) with respect to space considered as a whole.
Whatever the place where one stands, this place is part of the general network
world-space consists in. Thus, when one perceives a particular place, this is
world-space itself that one perceives [68: §59]. We might object that one perceives
the room, the amphitheater, the kitchen, not ‘space’ per se. But these places are not
cut off from the rest, they communicate with the adjacent places, and they occupy a
speciﬁc position in the general network of places: they are themselves ‘somewhere’.
As Heidegger explains, “the classroom [is] in the university building, the building
in the city of Marburg, Marburg in Hessen, in Germany, in Europe, on Earth, in a
solar system, in worldspace, in the world.” [59: 157–158].
2.2
What Places Are Is What They Are for
If places are experienced as nodes in a network, with particular relations of prox-
imity with other places, what primarily deﬁnes the identity of a given place (i.e.
what type of place it is) is generally an allotted social function. And a social
function goes with social practices, which make use of allotted equipment.
Heidegger is generally considered the phenomenologist who has systematized this
analysis of space [60: §22, 142–143 [109]].
The identity of a place is deﬁned by the kind of things one generally does in that
place, i.e. by a “this is where one does this or that” ascription. This is where one
sleeps. This is where one cooks. This is where one shops. This is where one takes
the train. And the use of each place is subjected to certain social norms, things have
to be done a certain way, there are protocols to follow and things that should not be
done. How one makes use of space in each place is also prescribed by such norms:
stand in line at the store, do not block the passage in the street. In the spatial
network, some places distinguish themselves by their merely transitional function:
they are places one must go through to reach B from A, but one does nothing in
them expect transiting, they are just corridors. Streets are typically that kind of
places. Now, the same place can certainly have different functions depending on
what people use them, how and when. The main street can be a mere transitory
pathway for people going to work in the morning. But it can be a place where one
lives and speaks and smokes for teenagers messing around.
Together with their social function, places are deﬁned by the kind of things one
expects to ﬁnd in them, and where, in these places, one expects to ﬁnd these things:
in a kitchen one generally ﬁnds a fridge, dry food and tins in cupboards, tables and
chairs to sit, kitchenware. Each place presents itself as a system where things have
allotted places that depend on what they are for [60: §15 and §22]. Where one ﬁnds
this or that thing in a given place is prescribed by (and suitable for) the use that is
made of that place. Think of a kitchen, of a train station, of the toilets, of the street.
And it is not only that the majority of places where one can go, including natural
10
G. Declerck and C. Lenay

areas (forest, river, mountain, etc.), have been designed by and for human beings;
this is how one’s immediate understanding of space works: wherever one goes one
expects things to be ‘in their place’.
This system of allotted places is what makes it possible for things to be ‘in the
wrong place’—which essentially means that one cannot ﬁnd them where one
expects them to be. Note, importantly, that for Heidegger this system of allotted
places for equipment is a public fact, accessible for all, this is not a mental rep-
resentation that would be projected on a meaningless spatial world. How things are
arranged spatially depends on what one does with these things, which is related to
social norms, not to some private individual facts (on this issue, see especially [81]:
85–86).
To sum-up: places are individuated (i.e. their identity is determined) by (a) their
topological situation in the global network and (b) an allotted social function; and
(c) the spatial organization peculiar to each place depends on the kind of things one
ﬁnds in it to do what one is supposed to do in it, the ‘equipment’ available in this
place, considering these things have allotted places that are prescribed by what they
are used for.
This analysis of what places are has direct consequences on what it means to
experience oneself as being in a place. Being in a given place does not only mean
occupying this particular position in the network of places, i.e. ﬁnding these or
those other places in the immediate proximity. Being in a given place (the living
room, the kitchen, the street, the park) means to have the equipmental system this
place consists of at one’s immediate disposal: the places and objects are immedi-
ately available for use, they stand ‘at hand’, as Heidegger says. In the kitchen I can
make some coffee, cook something and do the dishes. At the park, I can sit on the
bench, enjoy the lake, no longer hear the noise. We will come back to this point in
Sect. 4.3 when analyzing the experience of distance.
2.3
Space in the Singular and Space in the Plural
It is important to note that though lived space is fragmented in places, it is nev-
ertheless a whole. Each particular place belongs to world-space, it is a part of it.
There is only one space and it is the same for all: space is deﬁned by its uniqueness.
In this respect, we must distinguish between world-space (or ‘space’, in the sin-
gular), which is unique, and ‘spaces’ (i.e. space in the plural), that are many and do
not necessarily belong to (or are connected to) world-space in the strict sense. One
speaks, for example, of ‘green spaces’ or ‘urban spaces’, of ‘advertising spaces’, of
‘political spaces’, of ‘public spaces’ or ‘private spaces’; one also speaks of ‘digital
spaces’ and of ‘mathematical spaces’. The commercial area, the stadium, the
apartment are also ‘spaces’ in this sense.
Living in Space. A Phenomenological Account
11

What relationship between the two? Given that spaces (in the plural) are of
different types, two cases have to be distinguished:
1. Either ‘spaces’ are parts of ‘space’ (in the singular) and ‘spaces’ is then another
term for ‘places’: this is for example the case of green spaces, of the commercial
area, of the stadium or the apartment.
2. Or ‘spaces’ have an abstract nature (in the sense of ‘abstract’ indicated in the
next paragraph) and, in this case, either (a) one can access them through
interface devices that are themselves located in ‘space’ (in the singular); a good
illustration is so-called virtual environments, e.g. the space where one moves
one’s avatar when playing a video game; strictly speaking, the virtual space of
World of Warcraft is nowhere in world-space, it is not a part of it (that is, it is
not a node or systems of nodes that would be inserted in the general spatial
network); the proof is that you can access this virtual space from (almost)
anyplace on earth; however, in order to access this virtual space, you need some
hardware equipment that is, like any other material object, located in space; or
(b) these ‘spaces’ are radically abstract in the sense that they have no special
relationship with ‘space’ (in the singular), no point of contact or privileged entry
point in it (e.g. a mathematical space, an oneiric space, etc.).
Note that the dichotomy concrete versus abstract as it is traditionally used in
philosophy only works if one already refers to ‘space’ (in the singular). Indeed,
concreteness is generally deﬁned as the property of that which is located in space
and time, i.e. is somewhere and (only) exists over a portion of time (from t1 to t2). In
this sense, we cannot say that ‘space’ (in the singular) is concrete, for it would
amount to saying that space is located in itself. Conversely, abstractness is generally
deﬁned as the property of that which is not located in space and time (e.g. ‘free-
dom’, ‘hate’, ‘redness’). When we say that a space is abstract, it is thus to say that
this space is not located in space (in the singular), that is, is not a part of it.
The previous remarks show, in addition, why space, in a strict sense, cannot be
equated to a concept. While many chairs can be found in the world, so that we can
treat these particulars as different instances of a same concept that includes any real
and possible chair, there is only one exemplar of space, which is itself. Space is
unique. Surely, as previously explained, one speaks of multiple ‘spaces’; however,
when doing so, this is either to refer to parts of ‘space’ or to something that, in any
case, cannot be equated to an instance of ‘space’, for, in order to be an instance of
‘space’, it would necessitate that the entity be itself located in ‘space’, and this is
precisely not the case (abstract spaces are, by deﬁnition, abstract). Space is, to that
extent, much alike an individual and speaking of ‘space’ is like using a proper noun.
Note that this is not true for other notions that are closely related to space, such as
the notion of place: we can speak of a concept of ‘place’ because there are many
places in the world, each particular place exemplifying the abstract concept of
place. As a result, ‘space’ (in the singular) is not something one comes to know
12
G. Declerck and C. Lenay

through an abstraction process, going from concrete particulars to general abstract
features corresponding to the invariants properties shared by these particulars.
‘Space’ is rather known through an explicitation and extrapolation process, going
from parts (the particular places one comes to meet) to the whole that encompasses
all of them. This whole is, however, not a ﬁnite being one could grasp through a
bird-eye-view. It is an open-totality: there are always other places beyond the ones
one can reach or think of. There is always more.
3
Lived Space As Shaped By One’s Body
The analysis of space as a network of places has already shown that how one
‘operates’ in space and navigates one’s way across places, thus one’s bodily skills,
is an essential parameter in one’s awareness of space. Space is experienced as a
network of places to the extent that one can move from one place to another. But,
more radically, the organization of this network reﬂects some properties of one’s
being-in-space-with-a-body, such as the fact that: (a) one cannot be here and there
at the same time, i.e. one is bound to the position occupied by one’s body; (b) one
cannot immediately transfer oneself from one place to another (i.e. ‘relocate’), but
must move, which takes time and energy, and sometimes money and (c) one cannot
pass through walls, so that only certain paths can be followed to access one place
from another. And of course, the social practices from which the places’ identity
derives are rooted in bodily skills: ‘what one does’ in this or that place is, to one
degree or another, something one does with one’s body.
The claim that one’s body plays a constitutive role in one’s experience of space
is among the chief claims phenomenologists have put forward to account for lived
space. The thesis of most phenomenologists is that space as one ordinarily expe-
riences it is shaped by one’s bodily skills, i.e. by what one can do with one’s body,
and that motricity (the ability to move oneself) is a fundamental ingredient of space
perception. One experiences space because one can move, and the type of spatial
organization one is aware of at each moment is patterned by the nature and range of
one’s motor skills. As [38: 157] for instance explains: “the spatial information
embodied in […] perception is speciﬁable only in a vocabulary whose terms derive
their meaning partly from being linked with bodily actions.” That is, the critical
parameter to address the embodied nature of one’s experience of space is the body
as an action organ, i.e. the body as that through which one acts. This claim has been
developed in Husserl, especially through the idea of kinesthetic constitution of
spatial perception, but it has been given its most systematized form in
Merleau-Ponty [82] and subsequent authors having built on his work and belonging
Living in Space. A Phenomenological Account
13

to what is sometimes called the embodied or enactive approach to cognition (e.g.
[36, 43, 70, 86, 87, 89, 90, 106, 116, 122]).13
In the following, we will not only insist on the role of what one can do in the
shaping of lived space, we will in addition insist on the role of what one cannot do
because of one’s bodily nature, which is something much less considered in the
phenomenological literature. The constitutive role of embodiment in one’s expe-
rience of space can be addressed at multiple levels. We will examine two: (a) the
egocentric mode of manifestation of lived space; (b) the impact of one’s body
materiality on lived space.
3.1
The Egocentric Mode of Manifestation of Lived Space.
Where Things Look to Be Depends on Where One Is
Asking where is an obvious way to cope with space in ordinary language; but
‘where’ does not have one single meaning. What one means when asking or when
telling where something is depends on contextual parameters, such as the kind of
object that is considered (where Alpha Centauri is is not deﬁned at the same scale as
where the bathroom is, or where the butter is, or where one’s mind is), or why (i.e.
for what purpose) one wants to know where this something is. Sometimes one
wants to know where something is just to talk. Typically, I meet my new colleague
John for the ﬁrst time, and to break ice, I ask him where he lives. He could tell me:
in the United States or in Germany, or he could answer that he lives in the quartier
de Montmartre in Paris or even tell the exact address (for a similar distinction, see
[19: 118–119]. If one wants to know where something is to access it and use it, the
description of its location will be considered acceptable from the moment it pro-
vides enough indication to get one’s hands on it. I am phoning John with who I
must go to a meeting. We said yesterday that we will join somewhere then go to the
meeting together (John knows how to reach the university from the centre, I don’t).
‘Where are you?’, John asks me on the phone. What John wants to know when
asking this question is where I am so that we can join. What he wants is a series of
indications that will help him to ﬁnd me. I could tell him: ‘I am just in front of the
13To what logical relation the relation of constitutive dependence between embodiment—bodily
action or motor skills—and space perception amounts to is an epistemological challenge in itself
and is beyond the scope of this chapter. The relation of constitution is, basically, a conditional
relation: saying that A is constitutive of B (or that B constitutively depends on A) amounts to
saying that B is conditional on A: A must be for B to be (or B cannot be if A is not). But the
proponents of the embodied approach seem to intend something stronger than (or differing
qualitatively from) a merely causal relation, typically a type of mereological relation. De Jaegher
et al. [32] e.g. explain: “P is a constitutive element if P is part of the processes that produce X.”
This view could however be challenged for one can think of cases where B constitutively depends
on A without B being a whole of whom A is a part. One reason is that B can constitutively depend
on A even if A and B are of different ontological kinds, while the part of relation seems to apply
only to entities of the same ontological kind.
14
G. Declerck and C. Lenay

big shoes shop just beside the postofﬁce and there is also an orange juice seller’. Or:
‘I am near a big red building, which looks like a train station’. If I see where he is I
could also tell him where I am with merely egocentric indications, e.g.: ‘I am
approximately ﬁfty meters on your left’. Or, if we both have a GPS, I could tell him:
‘I am at Latitude 48.856614 and Longitude 2.352222’.
These ordinary situations illustrate the deeply context-dependent character of the
locating process. Locating something (including oneself), i.e. answering the ques-
tion ‘where is X?’, always starts by choosing a frame of reference that makes sense
in the context in which the question has been raised. Where something is depends
on why (for what purpose) one wants to know.14
Now, the thesis of most phenomenologists is that, in perception, such frame of
reference is primarily given by one’s body. The spatial features one accesses
through perception include a (so-called ‘indexical’) reference to oneself as a centre
of reference. Any something that appears to be ‘there’ is ‘there’ relative to one’s
‘here’ [66: §41, 166 [158–159]]. In other terms, the reference frame in which
objects presented perceptually get spatial coordinates has a fundamentally ego-
centric character.
Surely, space is also experienced and conceived in an ‘objective’ and allocentric
way.15 While in an egocentric reference frame the object’s location is deﬁned by
reference to the subject’s own position (‘the door over there at ten meters on the
left’), in an allocentric reference frame it is deﬁned independently of the subject’s
position (or positions of parts of her body), by reference to another reference frame
(allocentric means ﬁrstly non-egocentric). This can be a geocentric reference frame
[91]: the Leaning Tower of Pisa is said to lean by reference to the gravity vertical
axis; or an object-base (i.e. environmental) reference frame, i.e. a “spatial frame of
reference allowing to locate objects with respect to each other” [75: 688]: the
bathroom is near the bedroom, the bread is on the table, John is in the garden. In
short, allocentric coordinates do not depend on one’s own location in space, while
egocentric coordinates do [125: 70]. As a result, the characteristic of an allocentric
description is that it remains true whatever the location of the subject.
14This feature has especially been stressed by John Locke. “This modiﬁcation of distance that we
call place was made by us for our own use, and we ﬁt it to our convenience. When men speak of
the ‘place’ of a thing, they do it by reference to those adjacent things that best serve their present
purpose, ignoring other things that might be better determinants of place for another purpose.
When we are playing chess, it wouldn’t suit our purpose to locate the pieces in relation to anything
except the squares on the board; but quite different standards apply when the chess-men are stored
in a bag and someone asks ‘Where is the black king?’ and the right answer is ‘In the captain’s
cabin’. Another example: when someone asks in what place certain verses are, he doesn’t want an
answer that names a town or a library or a shelf; he wants an answer such as: ‘They are at about the
middle of the ninth book of Virgil’s Aeneid’, which remains true however often the book has been
moved.” [78, Book II, chap. 13: §9].
15Though it is now widespread in the phenomenological literature, the distinction between ego-
centric and allocentric spatial representations comes from psychology where it foremost refers to
the kind of reference frame that is used by an individual to represent the location of an object in
space [12, 25, 75].
Living in Space. A Phenomenological Account
15

Allocentric space is, however, the result of a process of objectiﬁcation that builds
on egocentric space. Phenomenologists, and especially Husserl, have described the
different subjective operations that are involved in this process. Among these
operations, the objectiﬁcation of one’s own body seems to play a pivotal role: one
must apprehend oneself as an object in space among other objects in order to
apprehend space as an objective system whose features are not bound to one’s
current point of view on it. As Zahavi explains: “we only experience space as
objective when its coordinates are no longer being experienced as being dependent
upon [our] indexical ‘here’. But it is only by objectifying the body, only by viewing
it as an object among objects, that its indexicality can be surmounted or suspended.”
[126: 105; see also [38]: 163] Zahavi notes that “this already occurs when one speaks
about moving through space” (Ibid.). Indeed, considering that one’s position in
space changes implies to see one’s own body as an object. To that extent, in order to
understand how one comes to perceive and conceive space as objective, it is nec-
essary to identify what mechanisms make it possible to experience one’s own body
as an object. Among the key parameters involved in the latter process are the
possibility of experiencing one’s own body in reﬂexive touch [26, 65‚ 79‚ 80‚ 82]16
and the possibility of seeing oneself through the eyes of other subjects, that is,
adopting an intersubjective perspective on one’s body [54, 65, 126].
Conversely, it is worth noting that the ‘self-relatedness’ that deﬁnes the ego-
centric system of reference cannot be thought of as a reference to the body con-
ceived as an object that is itself located in space, in which case this would amount
to an allocentric representation [75, pp. 688–689]. As Campbell [16: 74] explains,
“when the subject is identifying places egocentrically, it cannot be thought of as
doing so by ﬁrst identifying a physical thing—itself—through a body image, and
then identifying places by their relation to its body […] the egocentric identiﬁcation
of places does not depend upon a prior [objectifying] identiﬁcation of a body”. The
question is: what is the nature of this body on which the egocentric reference frame
is based if this is not an object in the strict sense?
Merleau-Ponty is probably the ﬁrst to propose a systematic account of this
question. This account consists, in short, in equating the body to an action organ,
that is, a system of action possibilities we spontaneously rely on as embodied
agents. When I look at an object oriented upside down, e.g. the face of somebody
lying down on a bed from behind, what I see looks inverted (i.e. not correctly
oriented) because I anticipate that I could see it upright if I had another position. “I
feel that I could, if I wanted, walk round the bed, and I seem to see through the eyes
of a spectator standing at the foot of the bed.” [82: 252] My bodily ability to move
there and see the object from a different perspective is what gives sense to what I
see from here, it is what the invertedness of the object refers to. The same is true for
16Husserl e.g. explains: “As perceptively active, I experience (or can experience) all of Nature,
including my own animate organism, which therefore in the process is reﬂexively related to itself.
That becomes possible because I ‘can’ perceive one hand ‘by means of’ the other, an eye by means
of a hand, and so forth a procedure in which the functioning organ must become an Object and the
Object a functioning organ.” [65: §44, 97 [128]].
16
G. Declerck and C. Lenay

the upright orientation of objects. “Each object has its ‘top’ and its ‘bottom’ which
indicate […] its ‘natural’ position, the one which it ‘should’ occupy”, because one’s
gaze is used to “[meet] it at a certain angle, and otherwise fails to recognize it.” [82:
252] The ‘normal’ orientation of objects is determined by what Merleau-Ponty
calls, following Husserl, one’s optimal perceptual grip on it.
Certainly, the vertical orientation of the perceptual ﬁeld has something to do
with the gravity axis, i.e. with the geocentric reference frame. But the vertical
gravity is only involved because we have a body that must stand and walk. Gravity
determines the orientation of the perceptual ﬁeld to the extent that it is something
we must deal with when going about our activities in the world: to live, we must
move, and to move we must stand and keep balance.17 As a result, Merleau-Ponty
emphasizes, “What counts for the orientation of the spectacle is not my body as it in
fact is, as a thing in objective space, but as a system of possible actions […]. The
possession of a body implies the ability to change levels and to ‘understand’ space,
just as the possession of a voice implies the ability to change key.” [82: 249–251]
For Merleau-Ponty the same holds for the other basic principles on which perceived
space is organized, especially the distance (near-far) dimension [see 82: 254 sqq.].
It is important to see that Merleau-Ponty’s account only partially solves the
problem described above. Assuming that the space one perceives is oriented
because one apprehends this space as a space in which one can act, two major
issues must be clariﬁed: (a) what does ‘acting’ mean, here? In particular, does the
term exclusively refer to perceptual actions, that is, actions whose purpose is to
modify one’s perceptual relation with the environment? Or does it also include
‘behavioural performances’ (for this distinction, see Sect. 4.2)? An obvious prob-
lem is that one can apparently perceive space as oriented or as organized in depth
even in case one cannot in fact perform any action in it (the things located in space
may be inaccessible or one may be paralysed). This leads to a second difﬁculty:
(b) in what sense of ‘can’ does the spatial organization of the perceptual ﬁeld refers
to what one can do? Is it something one can effectively do right now, an immediate
practical possibility? Or is it something one can do in principle (or could have done)
or something others can do?18 Merleau-Ponty was, to a certain extent, aware of
17As Charles Taylor explains: “The up-down directionality of my ﬁeld is a feature which only
makes sense in relation to my action. It is a correlative of my capacity to stand and act in
equilibrium. Because my ﬁeld is structured in a way which only makes sense in relation to this
capacity, I can say that the world as I perceive it is structured by it; or that I see the world through
this capacity. But a ﬁeld of this structure can only be experienced by an embodied agent.” [110:
24].
18We do not develop this complicated issue in the present chapter. But see Declerck [28].
Living in Space. A Phenomenological Account
17

these difﬁculties,19 but we can doubt he provided an analysis of the role body skills
play in spatial awareness capable of addressing them in an appropriate manner.
Note ﬁnally, and this will make the transition to the next point, that the ego-
centric organization of perceptual space is related to a fact that can be easily
overlooked because of its obviousness, namely that we are ourselves in space when
we perceive it. Space is always experienced from the inside: we are in it, and not
before it and outside of it, as when looking at a painted landscape at the museum.
This feature is presupposed by the structure of egocentric space: the things we
perceive appear to be there relative to us and at this distance from us, because we
are ourselves located in space, i.e. we share the same world. Conversely, we cannot
speak of distance for visual objects that do not belong in the same world as us, e.g.
objects represented on a painting or seen through a stereoscope. For these objects to
be somewhere with respect to us, it would require us to be with them on the other
side of the painting, in the pictural space, somewhat like Lewis Carroll’s Alice.
3.2
Living with a Body in Space. Space as that in Which
One Must Find a Place
Another critical sense in which lived space is shaped by one’s body is related to its
materiality, more precisely its impenetrability to other bodies, the fact that it will
oppose resistance if other bodies try to penetrate its substance.
Being somewhere generally20 implies being somewhere with (or through) one’s
physical body. This sounds like a truism, but this is a key point many phenome-
nologists have emphasized when analyzing one’s perceptual experience of space:
through one’s body one has a direct grip on the things around, one can achieve a
sensorimotor contact with them and one can make use of them, i.e. exploit their
affordances. This is what Dreyfus [35] calls one’s skillful everyday coping with
things. But—and this is now something phenomenologists, including Husserl, do
not generally pay attention to—being somewhere with one’s body also means being
subjected to a series of constraints one must deal with, to have some ‘duties’ to
fulﬁll. It means ﬁrst of all being encumbered with a body one must place some-
where, i.e. to occupy—literally—some space [27, 29]. To that extent, the key point
to understand the embodied character of one’s perceptual experience of space is not
19“How can I perceive objects as manipulatable when I can no longer manipulate them? The
manipulatable must have ceased to be what I am now manipulating, and become what one can
manipulate; it must have ceased to be a thing manipulatable for me and become a thing manip-
ulatable in itself. Correspondingly, my body must be apprehended not only in an experience which
is instantaneous, peculiar to itself and complete in itself, but also in some general aspect and in the
light of an impersonal being.” [82: 82].
20This is not always the case, for one can be located in virtual spaces without being there with
one’s physical body. See below, Sect. 3.3.
18
G. Declerck and C. Lenay

only what one can do with one’s body, it is also, symmetrically, what one cannot
do because of one’s being-a-body.
If one did not have to move, the fact that one takes up space would not cause
difﬁculties. One could simply keep one’s place. The problem is that one cannot (and
does not want to) stay where one is. One must move to live. One must change one’s
place. And to change one’s place, one must ﬁnd a place, i.e. one must ﬁt one’s body
where some room is still available. And this is not an easy task. Most often, the
places where one lives are literally saturated with bodies (whether animate or
inanimate). Think of the subway or train station at rush hour. Think of a super-
market. Think of your own dwelling. And it is only because one knows how to deal
with this cluttering that one can make use of places, do what one generally does in
these places using the available equipment (see Sect. 2.2). In the park, before I can
make use of this bench to relax, I must access it. To get there, I have to go along the
fence, pass the gate, give way to this old man, walk around the tree, avoid pigeons.
I cannot reach the bench in a snap and I cannot get there in a straight line. And of
course, I run the risk that another body takes the place before me. Any displacement
in space is an obstacle course: one must maneuver, work around, push and shove
the bodies that bar the way or impede the passage.
Our contention in this chapter is that this day-to-day challenge, namely ﬁnding a
place, exerts a signiﬁcant pressure on the organization of lived space. The format in
which space presents itself in perception is imprinted by the (implicit and practical)
awareness that one takes up space and must, each time one moves, ﬁnd a place. The
space one perceives is—so to speak—designed and, in a sense, optimized, for this
inescapable and pressing task.
The most obvious expression of this principle—namely, that lived space is
shaped by the awareness of encumbering space with one’s body—is one’s imme-
diate ability to discriminate between empty space and occupied space, or, more
generally, the role this distinction plays in one’s understanding of space. Empty
spaces, e.g. holes, are obviously an important ingredient of one’s naïve ontology,
i.e. one’s ordinary representations of the kind of entities one ﬁnds in the world
[17–19]. The hole in the cheese is there, it does not have less existence than the
cheese itself. You see this hole, you can put something in it, you can even cut it in
half; holes are so real that not having enough holes can be a reason for not buying a
cheese in the supermarket.
A fundamental way in which one deals with holes or openings, or more broadly
empty spaces, in ordinary life, is by taking them as potential places for one’s body
or for objects. Empty spaces are spaces where one could put things, where one
could be, through which one could pass. (Some empty spaces are also openings
through which one could be seen; see Sect. 5.) Emptiness is essentially perceived as
the availability of a place. “Even when we talk about ‘naked’ (empty) regions of
space—regions that are not occupied by any macroscopic object and where nothing
noticeable seems to be going on—we typically do so because we are planning to
move things around, or because we are thinking that certain actions or events did or
should take place in certain sites as opposed to others. The sofa should go right
here; the aircraft crashed right there.” [18: 73].
Living in Space. A Phenomenological Account
19

The ability to perceive empty spaces is among the chief abilities that are exer-
cised in the task of moving oneself in space or, more generally, coping with space.
I see that there is enough room to pass. I see that there is a place where I can seat.
I see that I can walk through this aperture. Several studies from the ecological
psychology literature and the action-speciﬁc approach to perception have furnished
empirical data supporting this claim [see, e.g. 41, 104, 118, 120]. This is also true
for the objects one manipulates. I immediately see the empty space between the
dishcloths and the bottles of cleaning products under the kitchen sinks, and I
immediately see that there is enough space for storing the bucket. How much space
is available for placing objects is immediately visible. I do not have to measure
anything. And I do not have to think. One reason is, as Gibson ﬁrst showed, that the
presence of empty spaces such as holes and passages is speciﬁed by typical patterns
of optical information that the visual system can easily pick up [47, 49].
A possible way to express the shaping relation between the material properties of
one’s body and perceived space is to make use of the concept of ‘effectivity’ that
was proposed in the ecological psychology theoretical framework to account for
what affordances are offered to a given agent by a given environment or object
[103, 114].21 The impenetrability of one’s body—the fact that it will oppose
resistance to any other body coming into contact with its surface—is part of the
effectivities deﬁning the impassability of a region of space (e.g. a wall, a dense
vegetation, a compact crowd), its property of not-affording-passage, or what Gibson
calls the bump-into-ability of physical structures, i.e. their character of obstacle.
Keep in mind, however, that for a region of space to acquire the character of being
impassable, one must in addition have the capacity to move (or, at least, to be
moved), and typically to walk: one’s body impenetrability makes a region of space
impassable only to the extent that one could try to move through this region. This is
in fact a general rule when considering the shaping of lived space by one’s body.
The properties of one’s body determine the organization and phenomenological
properties of lived space, which, as a result, somehow ‘reﬂect’ the properties of
one’s body. But these properties shape lived space only to the extent that they shape
one’s actions, i.e. determine what one can do and how it can be done.
How people perceive and ﬁnd their way in crowded places is particularly
informative about the relation between, on the one hand, the constraints the material
properties of one’s body put on one’s possibilities of occupying space, and, on the
other hand, how space appears to us in perception. When trying to ﬁnd one’s way in
the subway at rush hour, or in the station when one has to hurry up not to miss one’s
train, one tends to perceive the surrounding space as a mere ﬁeld of obstacles: bodies
block the way and must be circumvented, some openings can be passed through,
some objects can be moved, others must be walked around. The spatial layout
of bodies tends to be exclusively seen from the perspective of practicability.
21“An effectivity of an animal (or human) is a speciﬁc combination of the functions of its tissues
and organs taken with reference to an environment. The notion of effectivity may be schematized
as follows: An animal Z can effect action Y on an environmental situation or event X on occasion
O if certain relevant mutual compatibility relations between X and Z obtain.” [103: 197].
20
G. Declerck and C. Lenay

People themselves are perceived as mere masses hindering one’s progress. The
bodies deﬁne the limits where one can move and stand, and this is only to that extent
that one perceives them: as constraints on one’s ﬁeld of possible displacements.
The passability or impassability of a region of space also depends on
socionormative parameters. I have the right to go through a passage or penetrate a
—otherwise forbidden—region of space if I am empowered to do so by my
administrative function or social status. At the Customs Airport I cannot pass
beyond the line. At the pub, I cannot go on the other side of the bar. Basically, I
cannot enter private properties; or I cannot enter the intimate space of someone if I
am not a relative [56]. Note that the term ‘cannot’ has, however, a slightly different
sense in that case, for impassability does not absolutely exclude the possibility of
passage: I can violate the rule of not using the passage. Here, ‘cannot’ does not
mean that I am not able to or that it is impossible to; ‘cannot’ means that I am not
allowed to. In addition, socionormative impassability is always a matter of degrees.
The space may be more or less impenetrable, depending especially on what one
incurs (what risk one takes) if penetrating it. The situation is, however, still more
complex for very often the interdiction of penetrating these spaces is embodied by a
physical device that makes this penetration really impossible: a wall, a barricade, a
fence, a counter. Think of the Berlin wall. The merit of this type of device is not
only to ensure the respect of norms (transgression becomes impossible), it also
enables a certain attention and cognitive sparing. People do not have to constantly
check that they respect the norm.
Once again, the notion of affordance is a valuable resource, for we can apply it to
social behaviours, that is, we can include social norms (rules) and social statuses
(one’s own social habilitations) in the set of effectivities deciding of what can or
cannot be done with this or that structure, object or place. We generally think of the
role of the social dimension in affordance perception merely in terms of ‘social
affordances’, that is, opportunities for social interactions: perceiving that this
interaction with others is potentialized by the social environment, e.g. that one can
answer the smile of that girl, or that one should better not look into the eyes of this
angry guy.22 But social norms not only frame opportunities for interacting with
other persons, they also determine what can and cannot be done with objects
(understand: not persons), e.g. whether one can enter a region or touch or grasp
something, or how this shall be done. That is, many actions that are not ‘social’ in
the strict sense are patterned by social norms.
Keep in mind that the distinction must, however, be maintained between what
can be done (depending on one’s body properties and abilities) and what shall be
done (depending on one’s social status and social norms). What shall be done is
always something that can be done. Conversely, one can choose not to comply with
22This meaning has especially been promoted by Rietveld. “Social affordances are a subcategory
of affordances, namely possibilities for social interaction offered by an environment: a friend’s sad
face invites comforting behaviour, a person waiting for a coffee machine or smiling can afford a
conversation, and an extended hand affords a handshake.” [98: 208].
Living in Space. A Phenomenological Account
21

what shall be done: social norms can be violated. One can enter a forbidden space.
But one cannot enter a space that does not afford penetrability, e.g. a wall.
3.3
When One’s Where Is not Determined by the Position
of One’s Physical Body. Immersion in Virtual Spaces
We have seen in Sect. 3.1 that an essential phenomenological feature of space is
that it is always from the inside that one perceives it: one is oneself located in space
when one accesses space through perception. This point could be radicalized, by
saying that this space within which one is is, in addition, something from which one
cannot escape. One cannot get out and one cannot cease to be somewhere. As
Eugen Fink explains: “We are born in the world and we depart the world in the
world” Fink [39: 230, our translation]. And this is, in a sense, another phe-
nomenological feature of lived space: when one perceives space, one has the
implicit belief (or conviction) that one is, say, locked in space: one knows that,
whatever one does, one cannot leave, and this knowledge also impacts how one
perceives space, especially for what regards the feeling of ‘presence’ (compare with
your experience of being-in a virtual space: if something goes wrong in it, you
know you can get out).
This being-(locked)-in-space derives in a sense from one’s being a body. In a
way, one perceives space from the inside because one is a body that is itself located
in space. But, from the phenomenological perspective, this is not so simple.
Especially, this being-in-space-as-a-body should not be considered an objective
fact. Remember that phenomenology does not build on objectivist assumptions: you
can see yourself as an object; but being an object (say, a physical system where
physiological processes take place) is never taken by phenomenologists as
accounting for what one sees and how one sees it. To experience oneself as a body
located in space is itself the result of an intentional process, namely the process of
objectiﬁcation that we have mentioned in Sect. 3.1.
As a matter of fact, phenomenologically speaking, being located in space
through one’s physical body is not a necessary condition to be somewhere. Most
often this is your physical body that sets your location: you are where your body is.
Your body is in this amphitheater, sitting on this uncomfortable stool; you are in
this amphitheater, sitting on this uncomfortable stool. There are, however, many
situations when one’s physical body is not what determines one’s position in space.
When controlling a physical or a virtual avatar, say a ﬂying drone or a cursor on the
computer screen, this is the avatar that decides of where one is. One is on the
screen, moving from a window to another. This is reﬂected in ordinary language: I
am playing a war planes video game with John; we are both slaloming though
enemy ﬁres, the screen is a chaos with dozens of enemy planes. ‘Where are you?’
John tells me. ‘I am here, just near the big one’. ‘Oh I thought you had stayed
behind, I almost shot you’. In these situations, one has ceased to be where one’s
22
G. Declerck and C. Lenay

physical body is, the ‘there’ of the avatar has become one’s here. This is what is
generally called immersion.23
An immediate reply we can think of is that in these situations, one is not really
there with the avatar, one is still here with one’s body controlling the avatar;
claiming that one is there is a kind of metaphorical way of speaking or an abuse of
language.
Though this reply undoubtedly makes something right about the kind of funda-
mental anchoring in space that is achieved by one’s body (in a way or another one is
somewhere because one has/is a body and one is always where one’s physical body
is24), from a strict phenomenological standpoint this claim is not acceptable. The
metaphorical view neglects at least two important facts about the intentional
mechanisms that subtend one’s lived spatial location (i.e. ‘where one feels to be’):
(i) ﬁrst, the fact that “my body is wherever there is something to be done” [82: 250],
that is, the critical role of practical concern and current activities for deﬁning one’s
location; (ii) second, it leads to not seeing that one’s physical body generally sets
one’s location because it itselffulﬁlls certain functional requirements; in other terms,
having the experience of being located where one’s physical body is is itself the
result of a subjective process, where critical parameters can be indicated.
This can be understood through the following thought experiment. Sam suffers
from classical state locked-in syndrome [9]: he is almost totally paralyzed, the only
motor capacity he retains is vertical eye movements and blinking. But he has learned
to move a cursor on a screen through a brain-computer interface. The avatar he
controls through the device has become his only window on the world: everything he
does that has a sense for him happens in or through the virtual space where his avatar
moves. Sam can surf the Web, he has a FaceBook page with many friends and a
Blog, reads e-books, and through the interface he can also control many things in his
room: he has a voice synthesizer he uses to speak with the medical staff or his
relatives, and he can control the tilt of his bed. As a result, where Sam is most of the
time, is inside the virtual space where the avatar is moving, there ‘on the screen’, not
in his hospital bed. He ‘gets back’ in his hospital bed when something goes wrong
with the interface, when something hurts in his physical body, or when the social
environment addresses him as his body, e.g. when the nurse or his relatives ask him,
while looking into his eyes, how he feels or what he has ‘done’ today.
A critical parameter for something one controls to become one’s ‘here’ is direct
responsiveness to one’s practical intentions [20, 113]. In normal conditions, the
23In the ﬁve stages scale proposed by Auvray [8] to analyze the process of immersion, the feeling
of being there with the avatar corresponds to the fourth stage (‘Distal localization’). A description
in English of the immersion model of Auvray [8] can be found in Auvray et al. [6] and Gapenne
[45].
24Another argument we could think of is that each time one is immersed in a virtual space, sooner
or later, one gets back. Virtual spaces are spaces in which one is immersed only intermittently.
This is not a necessary feature, however, for we could imagine virtual spaces from which one
never gets out. Science-ﬁction has often played with this possibility. Think of Christopher Nolan’s
Inception, for example.
Living in Space. A Phenomenological Account
23

organic body is directly responsive in that speciﬁc sense: immediately one’s
intention to move and do something (grasp this glass, go there, type a sentence on
the computer screen, speak that word) embodies in the appropriate action [82: 145–
146], there is no delay, not even the experience of ‘controlling’ something, no
separation between what one intends to do and what one does. Thanks to this
responsiveness, as Descartes puts it, “I am not merely present in my body as a sailor
is in a ship, but […] I am very closely joined to it and, as it were, intermingled with
it, so that I and the body form a unit” [33: 56 [81]] Only when there is a loss of
responsiveness (e.g. one’s arm is paralyzed because one has slept on it) does one
come to experience one’s body as an organ that does not work anymore, and
symmetrically to perceive oneself as what controls that reluctant ‘thing’ that does
not respond as it should, a mind locked in a body. But as long as it works, one’s
body is something one confounds with, something one is, not something one has
(i.e. possesses and controls). All this implies that being one’s body, and being here
where one’s body is, is the result of a process of embodiment—a process of
identifying oneself to one’s body [34]. One becomes one’s body because one’s
body works correctly, because there is a kind of immediacy between one’s practical
intentions (what one wants and expects to do) and one’s movements (what one
does).
The responsiveness to intention is not the only variable for body appropriation.
The visual location of ‘body sensations’, i.e. haptic and proprioceptive sensations,
is another critical parameter. It is now well-known that one can make someone feel
some external thing accessed visually to be ‘her body’ (i.e. control her feeling of
body ownership) provided one generates tactile and proprioceptive sensations that
are consistent (i.e. occur in synchrony) with the visual input [2, 14, 37].
Note, ﬁnally, that the possibility of experiencing oneself out there with an avatar
does not challenge the assumption that in order to be somewhere one must be
somewhere with a body. In the case of virtual environments, this body will be one’s
avatar.
4
Lived Space and Possibilities. What is and What Can be
Another pivotal claim that has been defended by phenomenologists is that one’s
experience of space constitutively involves a form of anticipation of possibilities.
This is something we have already glimpsed in the previous section: the critical
parameter when considering the shaping of lived space by one’s body is what one
can (or cannot) do with this body, i.e. the action possibilities one anticipates or
takes for granted when accessing perceptually to the surrounding. Remember
Merleau-Ponty’s analysis of spatial orientation: the possibility (which is for the
moment—we must now insist on this—just a possibility: it is not realized yet, and
nothing ensures that it can in fact be realized) of seeing the—now inverted—face
upright if I had another position is what gives sense to its invertedness: the way I
24
G. Declerck and C. Lenay

see what I see constitutively refers to the possibility of having other perspectives on
what I see. But, as we will show, practical possibilities are only one type of
possibilities playing a critical role in one’s experience of space. What can be done is
one thing, what can be is another. And what one believes one can do is always
something one believes to be possible: what can be done is a part of what can be.
4.1
To See Space Is to Foresee Possibilities
The claim that one’s experience of space is based on (or structured by) an inten-
tional relation to possibilities (the exact nature of this relation is, as we will see, an
issue in itself) has been put forward by many authors, the ﬁrst to systematize this
position being probably George Berkeley in his Essay towards a new theory of
vision (1709), and is nowadays sometimes referred to as the berkeleyian account of
space, which many authors have followed since. The most prominent advocates of
this position include, by chronological order: Bergson [10], Poincaré [93], Husserl
[65, 67, 68], Heidegger [60], Scheler [101], Straus [107], Sartre [100],
Merleau-Ponty [82], Evans [38], Taylor [110] and Grush [55]. In the ﬁeld of
psychology, this claim has especially been defended by Gibson [46, 49] in his
ecological theory of perception,25 and by Profﬁtt [94, 95] in the context of the
action-speciﬁc (or ‘economy of action’) approach to perception.
In short, the claim is that being perceptually aware of the spatial properties of
something (and, basically, being aware that something is in space, is a spatial entity)
cannot be separated from, is conditional upon, amounts to or is reducible to (for
authors defending a strong reductionist position), being aware that certain possi-
bilities can be actualized provided certain actualizing conditions are fulﬁlled.26 This
is because one anticipates what may (or may not) be (done), that one experiences
space; one could not experience space if one’s perceptual apparatus would just take
a photograph of what is the case in the environment at time t. Perceiving space
implies to foresee (and presupposes a commitment to) what can be the case in the
future or, more radically, is conditional upon the apprehension of what could have
been the case now.
This claim has been defended for various spatial properties, but most of the time
this is for egocentric distance perception (sometimes called depth perception) that it
has been emphasized, with, in addition, a focus on motricity, that is, the ability to
25See e.g. Gibson’s analysis of the cliff and locomotion affordance [49: 157–158].
26This must be distinguished from the claim, accepted by most psychologists, that the spatial
content of one’s experience generally triggers or can trigger (provided additional conditions are
fulﬁlled) beliefs or expectations about possibilities realizable with the spatial object. According to
this claim, one can perceive something as a spatial object (i.e. one’s perceptual experience can
have a spatial content) without making these inferences or having these beliefs. For this approach,
the connection between the action possibilities and the experience of space is loose, and action
possibilities cannot be said to be truly constitutive of spatial experience.
Living in Space. A Phenomenological Account
25

move, and especially to walk. Locating something one perceives in the distance,
assigning a place to an object in perception, implies to refer this object to one’s
possibilities to access it. The basic prediction of this claim is that provided a given
objective distance between one and an object, a change in one’s capacities to access
the object will result in a change in its phenomenal distance, i.e. how far it seems
to be.
Note that this claim, formulated in this way, is only postulating a functional or
conditional relation between two intentional abilities, namely the ability to perceive
space and the ability to anticipate (generally action) possibilities. Believing that
some possibilities are realizable with object O is a condition for experiencing O as a
spatial objet; one cannot locate O in space without making these inferences or
having these presumptions. But we will see that some authors go further, and
assume that to perceive space is to be aware of such possibilities: seeing that this
object is there is fore-seeing that this or that can be done or may be the case. This
stronger—reductionist—version of the claim thus holds that these inferences about
what can be done or may happen are precisely what constitute the content of one’s
experience of something as spatial: locating something in space is nothing else, and
nothing more, than to make these inferences. In other terms, the awareness of a
given spatial property F is reducible, in terms of content, to the awareness that some
possibilities are realizable (under appropriate circumstances). For instance, seeing
that the chair is at this distance before me is to believe that this or that possibility is
actualizable with the chair provided this or that condition is fulﬁlled.
Berkeley’s account of visual experience undoubtedly belongs to this second—
reductionist—category. The leading claim of Berkeley can be phrased the following
way: the visual experience can only acquire a spatial content, i.e. represent the
spatial organization of the surrounding world, to the extent that it supports antici-
pations about the behavioural possibilities directed to the objects being presented
visually. Considered in isolation, visual sensations are devoid of spatial content: in
particular, they carry no information about the distance of objects [11, §.2]. What
provides the visual experience with a spatial content is the mapping of visual
sensations with these actions (done many times in the past) that can be executed to
approach the objects so as to interact with them (manipulate them, use them). In
other terms, the visual content presents objects located in space, because visual
sensations work as signs (in Berkeley’s words) for the actions that can be directed
towards these objects, ﬁrst of all ambulatory behaviours, i.e. moving close to the
objects (generally to grasp them, and in any case to make use of them) [11, §.59].
Note that what Berkeley had in mind was the connection between visual sensations
and haptic and proprioceptive sensations (though in a broad sense), not with
behavioural possibilities in the strict sense. The berkeleyian account retains an
empiricist—lockean—approach to perceptual experience (i.e. focused on the notion
of sensation). Henri Poincaré holds a very similar view in that regard, which is well
summarized by his claim that “to localize an object simply means to represent to
oneself the movements that would be necessary to reach it” [93: 47]. What Poincaré
refers to here is the rules governing how the sensory input varies when one
26
G. Declerck and C. Lenay

produces some actions. Poincaré is, as a matter of fact, a precursor of the senso-
rimotor approaches to perception.
Note ﬁnally that Berkeley and Poincaré aside, it is not always clear what version
of the claim is held by the phenomenologists defending this somewhat ‘disposi-
tionalist’ approach to space perception.
4.2
What Kind of Action Possibilities One Anticipates
When Perceiving Space
A noteworthy issue raised by the dispositionalist approach is related to the nature of
these possibilities the anticipation of which putatively plays a constitutive role in
one’s experience of space. As said above, phenomenologists generally refer to
action possibilities, that is, possibilities of doing things with one’s body, typically
the possibility of approaching distant objects. But the actions one can perform are
varied, and not all have the same function; they can also be considered at different
descriptive levels.
The dispositionalist approach has essentially been defended with regard to two
types of actions (or two perspectives on the same actions) by phenomenologists:
(1) actions causing changes in the sensational content of experience, i.e. exploratory
(or information-gathering) actions; (2) actions enabling to make use of objects, i.e.
performatory (or executive) actions.27 According to the ﬁrst view, what matters for
perceiving space is the anticipation of the changes in sensational content that would
result from one’s movements, that is, the functional relation between the sensory
data and one’s ﬁeld of kinesthetic possibilities. For the second view, ordinary space
is structured by the possibilities of use afforded by objects and places, the value of
these objects for achieving one’s practical projects, the anticipation of what can be
done, how, when, and at what cost. What matters in the ﬁrst case is what I can see
27The distinction between exploratory and performatory actions was initially proposed by Gibson
[48, 49] in his description of the haptic system, and has been later clariﬁed by Gibson [50] and
Reed [97]. While exploratory actions aim to extract information from the environment (e.g.
eye-focusing on an object or exploring an object with the hand) and are typically used for the
control of activity [97, pp. 80–81], performatory actions aim to use or manipulate the environment
(i.e. change states of affairs). Performatory actions thus cover the activities that are generally put
under the label of behavior. Another difference between these two types of actions is that they
generally have different energy requirements, “Exploratory activity, as I call the scanning for and
use of information […] involves the adjustment of the head and sensory organs to the ambient
energy ﬁelds. These adjustments are typically embodied in cyclic, low-energy and low-impact
movements of the sense organs or the head. […] Performatory activities are precisely those cases
in which the animal does use signiﬁcant amounts of force to alter the substances and surfaces of its
environment. It is one thing to see or to smell a piece of food, it is quite another thing to obtain it,
masticate it and eat it–and this applies whether one is a dragonﬂy or a mammalian carnivore.”
[97: 80–81].
Living in Space. A Phenomenological Account
27

(or, more generally, ‘sense’) if I move; what matters in the second case it is what I
can do, or better what I can use, when.
These two views are not necessarily exclusive. But the fact is that they have
generally been defended by different authors holding opposite positions. What
mainly distinguish the two approaches is in fact a difference in focus and granu-
larity of analysis. While the authors promoting the ﬁrst view are mainly interested
in the qualitative aspect of phenomena, i.e. the sensational layer and the functional
connection between motor acts and sensory changes, the authors favouring the
second view generally disregard this layer of phenomena as a theoretical construct
that does not reﬂect the way things really appear to us in ordinary perception, and
focus instead on our everyday coping with objects and its social signiﬁcance
(Heidegger is undoubtedly the main representative of this latter view).
Poincaré is a typical example of the ﬁrst account (see above, Sect. 4.1). So is
Husserl. For Husserl, the visual perception of tridimensional objects located in
space is not possible without sensations of movements, i.e. what Husserl calls
kinesthetic sensations or kinestheses [see e.g. Husserl 68: 136]. Rigorously, we
should speak of kinesthetic acts, for in Husserl the kinestheses are hardly identi-
ﬁable to sensations as they have traditionally been understood: they rather corre-
spond to the lived experience of motor acts when being executed, and ‘before’ these
acts produce muscle contractions and bodily sensations; they bear, to that extent, an
essential relationship to will and to what Husserl calls the ‘I can’. For Husserl, the
kinestheses are constitutive of one’s experience of spatial objects by virtue of the
counterfactual structure of perceptual intentionality, its ‘if-then’ organization.28
When identifying something in perception, one registers that which one perceives
under a given sense (Sinn) which enables to anticipate that it will exhibit this or that
familiar appearance in case this or that change in the perceptual circumstances
occurs, especially in one’s kinesthetic situation. This set of expectations is borne by
what Husserl calls the ‘inner horizon’ of objects [67: §8, 32]. The kinestheses
motivate systematic changes in the visual data, and they are articulated with the
latter into a whole functional system; as a result, to locate something in space in
visual perception always means to expect a set of functional connections between
the movements of one’s body, head and eyes, and changes in the visual appearance
of the object. This is especially true for the kinesthetic possibility of moving toward
or away from the object. As Dorion Cairns explains: “the visual object is presen-
tationally given in ‘far’ and ‘near’ appearances. If it is presented as far, there is
indicated a horizonal act in which it would be presented as near. This horizonal act
is indicated as a motivational potentiality, to be realized by realizing a certain
28Thing and Space is probably the book of Husserl where this approach has been the most
developed. See also Krisis, §28. Gallagher & Zahavi propose a good summary of Husserl’s
position on that matter: “The absent proﬁles are linked to an intentional ‘if-then’ connection. If I
move in this way, then this proﬁle will become visually or tactually accessible. The back of the car
which I do not see has the meaning of ‘the back of the same car I am currently perceiving’ because
it can become present through the execution of a quite speciﬁc bodily movement on my part.” [44:
110].
28
G. Declerck and C. Lenay

functionally related sequence of kinaesthesia, the ‘loco-motion’ kinaesthesia.” [15:
153] More radically, for Husserl kinestheses are a sine qua non for the sensational
content to assume a ﬁgurative function, that is, for the perceptual experience to
acquire a representational content, presents the world as being this or that way.29
One’s kinesthetic system and the conﬁdence that one can move (the ‘I can’) thus
play an essential role in the constitution of spatial objects: this is because one is
capable of moving and know (or believe or expect) that one can move that one
perceives space, tridimensional objects (that is, objects with other sides, now
invisible, but visible from other points of space) disposed in a gradient of distance.
Note that most theoretical accounts of perception in enactivist frameworks have
adopted and sometimes developed this same idea. One of the contemporary
defender of an enactive account of this kind is Alva Noë [87, 89]. The main
contention of Noë, which has been systematized through his so-called actionist
theory of perception, is that the objects we are aware of through perception (which
does not mean—considering visual perception—that we see them, in the strict
sense: they can be out of view but nevertheless present) are deﬁned by their sen-
sorimotor accessibility. To perceive thus implies to know or believe or anticipate
that this or that would occur in one’s perceptual experience if this or that movement
were produced [see 88: 533]. Though Noë pretends to defend a direct realist theory
of perception, assuming in addition a disjunctivist position [see especially 89: 23–
24 and 112–113], his position is reminiscent of the husserlian approach, but also of
empiricist and ‘sense-data’ accounts of perception, a position dating back to
Berkeley and John Stuart Mill, and developed in the 20th century by Alfred J. Ayer,
Bertrand Russell, and George Edward Moore, to cite only the prominent ﬁgures.
4.3
Phenomenal Distance as a Measure of Availability
for Use
The second approach to the role of possibilities in lived space is mostly represented
by Bergson,30 Straus and Heidegger, and focuses on a different layer of phenomena,
namely the behavioural possibilities that are made available (i.e. potentialized by)
29For Husserl, this claim applies at least for the visual and haptic presentation of spatial objects
(i.e. ‘material things’). It is an open question whether this claim should more generally apply to
any perceptual modality and any type of objects one can perceive, including e.g. smells and
ﬂavours [on this issue, see 53]. One can hardly defend that the exercise of one’s ability to move or
the anticipation of the sensory changes that would result from our kinesthetic acts play the same
critical role in the perception of smells and ﬂavours, than in the visual and haptic perception of
spatial objects.
30‘Ofﬁcially’, Bergson does not belong to the phenomenological movement, but, at least in his
book Matter and Memory, he promotes a descriptive approach very close to phenomenology.
Living in Space. A Phenomenological Account
29

or impacted by the environment, i.e. in general terms, what can or cannot be ‘done’
in it.31 This analysis of distance consists in subordinating (and maybe equating, the
claim is not totally clear) phenomenal distance with availability for use, i.e. in a
nutshell with practical accessibility (with ‘practical’ understood in a broad sense,
not reducing to motor activity). Generally, realistic assumptions lead theoreticians
of space to see practical accessibility as something derived from, and determined
by, objective distance: that which is close is within reach because it is close; that
which is far is beyond reach because it is far. The—possibly counterintuitive, due to
the widespread character of realistic assumptions—contention of phenomenologists
is that practical accessibility is what comes ﬁrst: that which is close is close because
it is within reach, not the inverse.
This is particularly evident in Bergson. Bergson explicitly equates the distance at
which the objects one perceives appear to a measure of their unavailability: the
more distant an object is, the less readily available it is, the less one can count on it
for achieving one’s immediate practical projects. “This very distance represents,
above all, the measure in which surrounding bodies are insured, in some sort,
against the immediate action of my body. In the degree that my horizon widens, the
images which surround me seem to be painted upon a more uniform background
and become to me more indifferent. The more I narrow this horizon, the more the
objects which it circumscribes space themselves out distinctly according to the
greater or less ease with which my body can touch and move them. They send back,
then, to my body, as would a mirror, its eventual inﬂuence.” Bergson [10: 6–7] To
see at what distance something stands is to foresee how available or unavailable it is
for a possible ‘use’ or, more generally, interaction (for good or ill). Erwin Straus
holds a similar position [see Straus 107: 455–456].
Note that an apparent corollary of this claim is that objects are distant in space
because they are distant in time. Something is remote because it cannot be used
now; some time is needed to make it available. To perceive the distance of an object
—this tree in the distance, the garden table a few meters ahead—is to appreciate
how important is the gap between its present unavailability and its future avail-
ability, for instance how much walking is needed to get it at hand. Seeing some-
thing as being there in the distance amounts, in one word, to anticipate how much
time (how long) it would take to reach it. Erwin Straus e.g. explains: “The there of
remoteness is where I am not yet arrived […]. What I see in the distance, what I
31It is difﬁcult to classify Merleau-Ponty in this second category, for the action possibilities
Merleau-Ponty deals with are quite exclusively sensorimotor activities, what he calls the relation of
‘grip’ one can achieve with the spectacle [see e.g. 82: 261 and 296]. In other terms, the action
possibilities Merleau-Ponty describes as playing an essential role for the spatial organization of
experience are possibilities of seeing more or better, not possibilities of doing, i.e. coping with
things and exploiting the affordances they make available. This is a striking difference with
Bergson or Straus (not to speak of Heidegger): as Cataldi [21: 44] explains, whereas “Straus
deﬁned the remote as that which is removed from my longing or that which is beyond the reach of
my desire”, “Merleau-Ponty […] deﬁned increasing distance as ‘expressing merely that the thing
is beginning to slip away from the grip of our gaze’”.
30
G. Declerck and C. Lenay

perceive as near or far is before me as a goal, more exactly as located in the future.”
(Straus [107: 456], our translation) Two remarks must be made with this claim.
(1) Firstly, this time that apparently decides of spatial distances is not the objective
time measured by clocks. The time it takes to access something, upon which the
distances are scaled, is itself measured—by a sort of equivalence operation—by
the ordinary activities we are familiar with and which take more or less time. If
a place ‘is about a half-hour walk’, our understanding of what represents ‘half
an hour’ is set by the time it takes to do things one can do in ‘half an hour’.32 In
addition, other parameters related to activity obviously come into consideration,
in particular the difﬁculty of access and the effort it would take to reach the
object. Some empirical studies have clearly highlighted this point: for instance,
subjects weighted with a load making their moves more difﬁcult perceive
objects as farther away [96, 123].
(2) Secondly, many ordinary situations seem to go against the claim that two
objects are at equal (perceived) distance from the moment that accessing each
would require an equal amount of time and effort.33 It would take the same
amount of time to access the rocking chair at the other end of the room and the
stool, which is nearer in terms of objective distance, but cannot be accessed
without walking around the coffee table; and reaching the stool would surely
require a bigger effort (at least attentional) than the rocking chair, which can be
accessed in a straight line. Yet, visually speaking, the distance to the stool is
undoubtedly smaller than to the rocking chair. One can see that ‘objectively’
the stool is nearer than the rocking chair. Does this conﬂict with the precedent
account of phenomenal distance? Apparently, the only possible way to over-
come this issue is to consider that the distance at which things appear visually
primarily refers to an idealized reaching action. For judging the distance to
objects, visual perception ignores to some extent the deviations and detours that
would impose an effective displacement to them: perception thinks in straight
line, so to say. As a result, the time needed to access the objects is indeed what
decides of their distance, but this action has an abstract character: it does as if
the path to the objects was always free (but see [85], for qualifying this claim).
32“Remoteness is never understood as measurable distance. […] We say that to go over there is a
good walk, a stone’s throw, as long it takes to smoke a pipe. These measures express the fact that
they not only do not intend to ‘measure’, but that the estimated remoteness belongs to a being
which one approaches in a circumspect, heedful way. But even when we use more exact measures
and say ‘it takes half an hour to get to the house’, this measure must be understood as an
estimation. ‘Half an hour’ is not thirty minutes, but a duration which does not have any ‘length’ in
the sense of a quantitative stretch. This duration is always interpreted in terms of familiar, everyday
‘activities’.” [60, §23: 98 [106]].
33As Straus explains: “The objective distance that separates me from the blotter and the inkwell on
my desk is certainly different for these two objects, but their proximity is equal if I can grasp each
of them with a single hand movement.” (Straus [107: 455], our translation).
Living in Space. A Phenomenological Account
31

Heidegger’s analysis of phenomenal distance is reminiscent of Bergson’s and
Straus’ views, but it is more developed and also more radical in its claims. In
addition, contrary to Bergson and Straus, Heidegger conceives the relation of ‘use’
with the objects without explicitly referring to bodily skills (Heidegger is
well-known for having ‘neglected’ the body in his phenomenology). The basic
point to understand Heidegger’s account of distance is his rejection of the objec-
tivist approach, namely the view that perceiving space equates to building a mental
representation of an objective space with intrinsic metrics (the space of the physical
reality), so that the spatial features one perceives are prescribed and constrained by
a ﬁxed preexisting order. This is what Morris [86: 9 sqq.] calls, following
Merleau-Ponty, the ready-made theory of space. For Heidegger, objective distances
cannot decide about the ‘nearness’ and ‘farness’ with which things appear in our
everyday absorbed coping; at best they are only one parameter in the equation [see
e.g. 60: §.21, 90 [97]]. “In the course of a hike through the woods I come for the
ﬁrst time to Freiburg and ask, upon entering the city, ‘Which is the shortest way to
the cathedral?’ This spatial orientation has nothing to do with geometrical orien-
tation as such. The distance to the cathedral is not a quantitative interval; proximity
and distance are not a ‘how much’; the most convenient and shortest way is also not
something quantitative, not merely extension as such.” [62: §16, 72] What decides
of the nearness or farness of the things one deals with is their availability and
accessibility, which are not an affair of objective distance, but depend on what
Heidegger calls one’s circumspective understanding of things.
This claim cannot be understood in abstraction from Heidegger’s analysis of the
way things appear to us in our everyday coping with the world—what he calls the
‘being-concerned-about’ (Besorgen). Heidegger uses the term ‘availableness’ or
‘readiness-to-hand’ (Zuhandenheit) to refer to the mode of being with which things
present themselves ‘primarily and usually’: when absorbed in one’s everyday
activities, one does not perceive ‘objects’, one is dealing with equipment (Zeug),
and what one ‘sees’ (or foresees) ﬁrst is what they are for, what could be done with
them, how they could help achieving one’s goals, or, on the contrary, how they
could hinder this achievement [61: §.15, 163–164]. In addition, contrary to
‘objects’, the equipment one deals with in everyday coping is never apprehended in
isolation, but “always belongs [to] an equipmental whole, in which it can be this
equipment that it is” [60, p. 68]. Any equipment refers to other equipment, e.g. the
32
G. Declerck and C. Lenay

pen refers to ink, paper, table, furniture, etc. [35: 62]. And this “speciﬁc func-
tionality whole is pre-understood” before any individual piece of equipment we
come to meet [61: §.15, 164]. The heideggerian notion of practical circumspection
(Umsicht, literally ‘for-sight’) refers to one’s non-thematic acquaintance with this
equipmental whole and ability to cope skillfully with the equipment. This kind of
‘sight’ [60: §15, 65 [69]] is what enables one to anticipate that this or that piece of
equipment must be used to achieve this or that action, and to plan action sequences,
i.e. to know what must be done with what and when to achieve a given practical
purpose (e.g. crossing the street, smoking a cigarette, washing the dishes). It is this
kind of ‘knowledge’ that is behind one’s everyday behavioural performances and
the apparent ease with which one spontaneously orients oneself in one’s practical
world.34
The core of Heidegger’s thesis about phenomenal distance is that practical cir-
cumspection is the intentional mode under which the distance of things is primarily
experienced in everyday coping. The more immediately something is usable, the
nearer it is [60: §22, 95 [102]]. What matters given our current practical concerns is
thus what decides of the distances. Heidegger uses the concept of ‘de-distancing’
(Ent-fernung)—literally, cancelling the ‘farness’—to refer to the intentional process
that brings things in the ﬁeld of view of practical circumspection. When I am about
to do the dishes, only a part of the equipmental whole is ‘neared’ by my practical
circumspection: the sink, the dirty items: plates, cutlery, glasses, the sponge, the
faucet, the dish soap, the tea towel. These things have their familiar place. And I am
directed practically towards them as resources to perform the work. These are those
pieces of equipment that are currently nearest to me. As Malpas [81: 76] explains,
this ‘bringing-close’ or ‘nearing’ “is not just an overcoming of a purely objective
spatial distance but also a ‘picking out’ or a ‘bringing into salience’ that overcomes
the distance of inattention or ‘not-seeing’.” Which does not mean, conversely, that
one is explicitly paying attention to them. On the contrary, what has been brought in
the ﬁeld of circumspection is generally only peripherally and implicitly present; one
is in a way or another counting on it or with it, but one is not explicitly paying
attention to it.
As a result, in Heidegger, ‘nearness’ is more or less equivalent to ‘available for
use’. That which is immediately available for use, at hand, is what presents itself as
near. Conversely, the more something is ‘far’ from one’s current practical concerns,
the more ‘distant’ it is. Heidegger thus explains: “What is supposedly ‘nearest’ is by
no means that which has the smallest distance ‘from us.’ What is ‘near’ lies in that
which is in the circle of an average reach, grasp, and look. […] For someone who,
for example, wears spectacles which are distantially so near to him that they are
34“Circumspection uncovers and understands beings primarily as equipment. When we enter here
through the door, we do not apprehend the seats as such, and the same holds for the doorknob.
Nevertheless, they are there in this peculiar way: we go by them circumspectly, avoid them
circumspectly, stumble against them, and the lie. Stairs, corridors, windows, chair and bench,
blackboard, and much more are not given thematically. We say that an equipmental contexture
environs us.” [61: §.15, 163].
Living in Space. A Phenomenological Account
33

‘sitting on his nose,’ this useful thing is further away in the surrounding world than
the picture on the wall across the room. This useful thing has so little nearness that
it is often not even to be found at all initially. […] That is also true, for example, of
the street, the useful thing for walking. When we walk, we feel it with every step
and it seems to be what is nearest and most real about what is generally at hand; it
slides itself, so to speak, along certain parts of our body—the soles of one’s feet.
And yet it is further remote than the acquaintance one meets while walking at the
‘remoteness’ twenty steps away ‘on the street.’ Circumspect heedfulness decides
about the nearness and farness of what is initially at hand in the surrounding world.
Whatever this heedfulness dwells in from the beginning is what is nearest, and
regulates our de-distancing.” [60: §23, 99 [107]].
4.4
What Can Be and What Could Have Been:
The Role of Counterfactual Statements
in One’s Experience of Space
Another important sense in which lived space implies an intentional relation with
possibilities has to do with the role of basic spatial principles—comparable to the
basic principles of logic, i.e. the so-called ‘laws of thought’ [see Russell 99: 72]—in
one’s experience of space, and the operationalization of these principles through
counterfactual statements (i.e. subjunctive conditionals) shaping what one believes
(or conceives) can be the case.
Space as one experiences it is undoubtedly something in which some states of
affairs are presently the case: any conceivable spatial being is currently some-
where, in a given place with a given orientation. In that sense, its spatial properties
are totally actual. The point is, however, that it is presupposed by one’s experience
of something as being there, say, of this table in the living room, that (1) it might
not have been there, that is, the place occupied by the table could have been empty
(and still can); and that, conversely (2), it might have been somewhere else: the
table could have occupied another position (and still can).35 This is a basic
principle of spatial location: things occupy space, but the place they occupy is not
cancelled as a place by their presence, the occupable place remains, ‘behind’ them,
and it would have existed even if the object would not have been there.
35These two statements express two sides of the same coin, considering that the relation which is at
stake in the state of affairs ‘occupying place P for an object O’ can both be described as the
relation: O is-occupying P, or as the symmetric relation: P is-occupied-by O. The claim that
perceiving this state of affairs presupposes the two counterfactual statements indicated below is
another way to say that this relation is, like in any other state of affairs, an external relation [3]. “A
state of affairs exists if and only if a particular has a property, or a relation holds between two or
more particulars. The relations are all external relations, that is, in no case are they dictated by the
nature of their terms. In the jargon of possible worlds, it is not the case that in each world in which
the terms exist, that is, in which the related particulars exist, the relation also holds.” [3: 429].
34
G. Declerck and C. Lenay

An immediate practical consequence of this is that in principle any object can be
removed from the place it occupies so as to free the place. This account is another
way to express the widely acknowledged claim that places should not be con-
ceived as reducible to the things they contain. As Casati and Varzi [19] explain,
we cannot treat “the relation between an entity and its place—the place that it
occupies or where it is located—[…] as mere identity, for we want to make sense
of the possibility that the same place be visited by different things at different
times”. As a matter of fact, any place on earth has already been occupied by other
things than these one currently ﬁnds there. To reuse an example from Sect. 2.1, we
can say, typically: at this place there was a train station early in the century; today
it is a trafﬁc circle. Symmetrically, no object is bound to the place it is currently
occupying; it is included in the sense (i.e. the conditions of identity) of spatial
beings that they could have been elsewhere. This basic principle in one’s
awareness of space can be referred to as the place contingency principle: anything
that is experienced as there is implicitly apprehended as something that could have
been elsewhere while remaining what it is.
This principle means that perceiving space not only presupposes a commitment
to what can be the case in the future (typically, what one can or cannot do:
behavioural possibilities), it also, and more radically, presupposes a commitment to
what could have been the case now. It is a challenge for phenomenology to
understand how this kind of understanding is possible, what subjective mechanisms
are implied in the ordinary belief—which is in fact more akin to certainty, in
Wittgenstein’s sense [see Wittgenstein 124: §.194 and §.208–209]—that any place
could be occupied by something else, that ‘behind’ every occupant, there is an
empty space. Some authors (typically Piaget) tend to explain this kind of under-
standing by equating it with (or deriving it from) a behavioural possibility we
spontaneously rely on. This is turning things upside down in our opinion: if one
expects that one can move something, it is because one believes that this something
can be moved, not the opposite. I expect that I can move this chair because I have
identiﬁed what I see as a chair, that is, something which, as any solid object, can
change its place. The practical possibilities one relies on in perception are carved
out into the possibilities delimited by the identity the perceptual objects have been
ascribed. Another reason to reject the precedent thesis is that it is not because one
cannot (effectively) move an object that this object cannot be moved (in principle).
Note that what comes ﬁrst in the developmental history of the subject is not the
point here. Maybe the child learns ﬁrst that she can move objects, and only later
comes to apprehend objects as beings that are not bound to the position they
currently occupy—i.e. acquires “a concept of bodies that can change place” [121].
In fact, it is more than likely. But what she has knowledge of, once this develop-
ment achieved, is a logical order, an order of reason; and her understanding of this
order cannot be reduced to a behavioural possibility, something she can do. Quite
the contrary in fact: this logical order is now framing her knowledge of what she
can do.
As a ﬁrst approximation, this kind of cognitive ability or performance can be
equated to a kind of pre-propositional form of counterfactual reasoning: to perceive
Living in Space. A Phenomenological Account
35

that there is something there is to be aware (to conceive) that it could have been
otherwise.36 In that respect, only a type of mind for which what there is does not
amount to what is currently the case can have the experience of something like a
spatial world (but to use the term ‘world’ is already to think of something spatial).
A living system that would merely be aware of what is currently the case, i.e. which
would access mere facts, without putting them in perspective with what can or
could have been the case, cannot be aware of something like a space. As a matter of
fact, we can even doubt it can be aware of anything.
Other noteworthy principles can be mentioned playing a similar fundamental
role in one’s general understanding of spatial beings. What is generally referred to
as the principle of permanence (or object permanence) in psychology, namely the
fact that objects do not disappear when one cannot see them, undoubtedly belongs
to this category. If I drop my fork under the table, it is necessarily somewhere, even
if at ﬁrst glance I do not see it. It cannot have just vanished. The same applies if you
cannot ﬁnd your shoes, your keys, your car, or your house. The belief in that
principle, namely that material objects cannot just disappear (at least without a good
reason), explains that one can spend a long time (perhaps one’s whole life, if it is
something other than a fork) searching for something. This principle is someway
part of one’s ‘naïve physics’ (or what could be called in that speciﬁc case one’s
naïve metaphysics), namely the pre-scientiﬁc knowledge one has of the laws
governing the behaviour of bodies and physical structures at one’s scale [57].
What is remarkable is the compulsory character of this principle in our repre-
sentation of reality, the irresistible role it plays in what Wittgenstein calls one’s
‘picture of the world’ [see Wittgenstein 124: §93–95]. Material things cannot cease
to be somewhere, they cannot disappear and then reappear, and in general they keep
their place (see below). The point is that this principle (with other principles of the
same kind) that regulates our intelligence of situations, works by shaping—and in a
sense, limiting—the possibilities, it formats what one believes to be possible or not.
Not everything can be. And what one believes to be the case in this or that situation,
e.g. the possible scenarios one considers to explain to oneself what happened to an
object, say one’s keys, is always framed within the bounds of what one believes can
be the case with this object. This is another decisive sense in which one’s expe-
rience of space entails a commitment to—and is shaped by—possibilities.
A connected principle is the principle of place permanence, namely the fact that
most material objects keep their place, i.e. stay where they have been left and seen;
though they can certainly be moved, they do not move by themselves; and this
principle
precisely
applies
only
provided
things
haven’t
been
displaced.
36Note that this can be accounted for in merely dispositional terms: to be aware (or conceive) that
an object could have been elsewhere means to be disposed to behave or ‘think’ (e.g. form beliefs
or intentions) in agreement with this statement. See Steiner [105] for developing such an account
of what it means to possess or master a concept.
36
G. Declerck and C. Lenay

A noteworthy consequence of this principle is that place fulﬁlls to a certain extent
an identiﬁcation function [see 19: 133]. What occupies a given place at instant t2 is
the same entity that was occupying this place at time t1. This principle, like any
other principle of this kind, is so deeply rooted in how one’s mind makes its world
intelligible, that its action can be easily overlooked. But this principle is behind any
of our ordinary identiﬁcation of objects. This cup on the table is the same cup that
was on the table one second ago (it is not another cup), and it will remain the same
cup it is in the next second. In addition, this same principle is what makes possible
to apprehend that something occupying a given place is the same object despite
sometimes substantial changes in its aspect or properties. Think of a piece of sugar
disaggregating in a glass of water: what makes it appear as the same piece of sugar
during the entire duration of the disaggregation process is the place it occupies.
Remember Descartes’ piece of wax.
The same holds considering one’s practical understanding when coping with
things. The chair before me in the kitchen is the same chair I saw and sat on in the
kitchen this morning. The tree in the garden is the same tree that was at this place
one month ago. This old white car spitting out grey smoke parked before my house
is the same that was parked at this same place yesterday spitting out the same grey
smoke. Of course, additional parameters come into the equation. For identifying the
car as the same, some plausibility constraints must in particular be fulﬁlled. If I
come back in twenty years and see a white car of the same model at the same place,
I will probably not identify it as the same car, except if it presents a particularly
discriminating element. The rules one uses to decide of the sameness of the objects
one perceives are partly the result of a cultural process of knowledge acquisition.
We can easily observe that before a certain age, children fail to identify certain
objects as being the same, typically the moon in the sky, when seeing it at a few
minutes interval. They do not yet know that on earth we only have one moon.
Similarly, the scale that applies when place works as an identity principle is
variable and context-sensitive. This white mug is the same mug I use to drink my
coffee every morning since several months for it is in my house. But it can be
anywhere in it. If it happens that I do not ﬁnd it where I thought I had left it but in
another place, I will not apprehend it as another mug that looks exactly similar to
the mug I know. For I know my memory is fallible and there are other people living
in the house that could have changed its place. Conversely, I could see a mug with
exactly the same aspect in a pawn shop and yet not identify it as the mug I know,
for it is in a different place and I do not have at hand a plausible scenario explaining
how it could have moved from my house to the pawn shop. If I suspect my wife to
have sold our kitchen stuff, the situation is of course different.
This is an important feature to keep in mind when addressing the situation of
blind people, for an essential difﬁculty blind people must deal with is the failure of
the place-identity principle, which is due to the fact that objects fail to stay where
they have been left because other people can displace them (see Sect. 6.1).
Living in Space. A Phenomenological Account
37

5
Lived Space and the Possibility of Being Perceived
Another important layer of lived space derives from a feature of one’s being-a-body
that has often been overlooked by phenomenologists, namely the fact that one can
be seen or, more generally, perceived by others. One’s body not only shapes lived
space as an organ of action, it also shapes it by making one irrevocably visible to
other embodied subjects.
When seeing others, we see if they look at us and we see if they can see us. I see
that this man who is turning his back to me does not see me and cannot see me.37 I
see that I can no longer be seen by the people on the other side of the street for a bus
is now blocking the view. And when in the kitchen I know (i.e. live in the implicit
certitude) that my wife in the upstairs bedroom cannot see me. Lived space is
organized along areas of visibility and invisibility, which are continuously rene-
gotiated depending on where we are and where others are. And how we behave
chieﬂy depends on whether we think we can be seen and are currently seen from
where we are.
Many social places exploit this feature. The town square is an open-space where
everybody and everything is visible to anyone; you cannot hide. The actors on the
scene of the theatre are visible from any point of the auditorium, but the prompter is
invisible, except for the actors. In the ﬁtting room, the voting booth or the con-
fessional, nobody can see you (except maybe the One who sees everything). In
many shops you can see what’s inside from the outside, which offers to make
visible what you can buy. Conversely, some places are equipped with tinted win-
dows, so that you can see the outside from the inside without being seen. Black
glasses offer similar possibilities: the others cannot see where you’re looking at.
The principle of the panopticon, designed by Jeremy Bentham in the 18th century
for the jails, consists in making visually accessible each cell from the central room,
so that the guardians can keep an eye on everyone without having to move, but in
making conversely invisible from the cells the central room. As a result, the pris-
oners are always visible for the guardians—they cannot lose their visibility–, but
they cannot see if they are currently observed. This spatial organization preﬁgures
the modern open spaces of ofﬁce workers. The only difference is that there are
(generally) no guards: everybody is in charge of watching the others. Joking aside,
this last case illustrates an important spatial principle, namely that the conﬁguration
of space in itself forces the emergence of particular behaviours and is thus a way to
act on these behaviours. As Foucault [40] explains, “the major effect of the
Panopticon [is] to induce in the inmate a state of conscious and permanent visibility
that assures the automatic functioning of power”. In the case of open spaces also,
37Claiming that being actually or possibly seen is something we see—and not infer, for instance—
could be debated. This is however not the point here. To avoid any controversy, the former claim
could be rephrased in terms of beliefs formed on the basis of visual experience: in many situations,
when we see other people, we immediately form the belief that they see us or can see us.
38
G. Declerck and C. Lenay

it is quite obvious that the behaviours are directly impacted by the possibility of
being continuously seen (thus spied) by others.
All those places and devices build on the fact that lived space is something we
inhabit collectively—i.e. on its radically intersubjective character—and in which
we are located as a visible body. The principles on which this layer of space –which
we propose to call the ‘mutual perceptibility space’—is built can be described,
though, to the extent of our knowledge, this task hasn’t been undertaken yet.
Whether others can perceive us from where they are primarily depends on how
we are located in space with respect to each other, and on the arrangement of
physical structures between us. In order to see if someone can see me from where
she is (assuming she looks in my direction), I must see if her gaze can reach me,
which depends, above all, on whether there are opaque surfaces, i.e. visual
obstacles, between us. (Note that being seen does not necessarily imply being stared
at: one can be seen peripherally.)
A principle we frequently use to determine if we can be seen is the principle of
reciprocity, which can be expressed as follows: if I can see you, you can see me. To
some extent, this principle is a consequence of the laws governing the propagation
of light. This principle does not apply in every type of environment, however.
Typically, it does not apply if I am located behind a one-way mirror or look at
someone through a video monitoring device; in this case, I can see you, but you
can’t see me. Similarly, if I am looking at someone when in a crowd (e.g. an actor
on the theatre scene from a crowded audience), I will spontaneously hold that she
cannot see me. For sure, she can see me in the sense that I am visible for her, but it
is unlikely that she will look at me rather than at somebody else. I am, as we say,
hidden by the crowd. Our respective visual exposure is dissymmetric.
The same can be held for the possibility of being heard. I can hear that somebody
else does not hear me if she does not reply to something I said (and if I expect that
she should have if she had heard me). Checking if others hear us is quite common,
e.g. when talking to old persons going deaf, or to younger ones wearing head-
phones, or to somebody in another room, in the distance or in a noisy environment.
Similarly, when someone sleeps in the next room, one tries not to make noise only
because one believes that one can be heard.
Audition and vision are not completely the same, however, for if we can quite
easily see if others can see us, it is far more difﬁcult to hear if others can (pay
attention to the term) hear us. In order to hear whether one can be heard (from
where one is), one must access aurally to something (a physical structure or a
conﬁguration of the environment) that affords not being heard. I can hear if you
answer me when I talk, or I can hear that the sounds coming from this room stop or
are mufﬂed now that I have closed the door. But I do not access aurally to a
conﬁguration of the environment that could make me anticipate that you can or
cannot hear me from where you are before any conversation. I see that there is a
window between us (I could also feel it with my hands); and I hear that I do not hear
any sound coming from behind this window; as a result, I form the belief that you
cannot hear me from where you are (the same applies if you are too far for me to be
Living in Space. A Phenomenological Account
39

heard). Keep in mind that this belief is about what you can hear, i.e. your perceptual
possibilities, not about what you are currently hearing.
Note that in principle, it might be possible to extract this kind of information
through audition alone, when the sense is used with an echolocation function [51,
108, 109, 119]. With enough training, one can perceive through echolocation that
there is a structure ahead, e.g. a wall, a door or a window, that blocks the sounds
and enables not being heard. One can move back and forth along this structure (or
open and close the door) so as to make appear and disappear some sounds coming
from behind. Assuming a reciprocity principle, one can infer that when one is
hidden behind the structure one cannot be heard from the points of space from
which these sounds come. Echolocation is however limited compared to vision, in
this respect: a complete auditory occlusion is as a matter of fact difﬁcult to create,
because of the mode of propagation of sounds, which differ from that of light rays
which follow a linear trajectory and can be easily blocked. In addition, echolocation
can barely provide access to multiples structures at different distances and
occluding each other (there is no real depth in hearing), which can be easily
perceived with vision. We will come back to this point in Sect. 6.2.
These perceptual laws are an important ingredient in our ordinary understanding
of space. We have some naïve knowledge of the perceptual (e.g. visual and
acoustic) laws that condition perceptual accessibility. And we have some models of
the range of the perceptual apparatus of others, i.e. of the areas of space that can be
accessed visually or aurally by them from where they are. In addition, we are
sensitive to the many perceptual cues suggesting that we are currently perceived by
other agents. How their sensory organs (e.g. their eyes) are oriented relative to us is
a key parameter. But there are many others. In general, the behaviour of others, in
the sense of their body attitude, will be perceived as the sign that they perceive us, if
it is connected in a reciprocal way to one’s own behaviour, that is, if there is a
contingent interaction: how the other behave is contingent upon how I behave, and
vice versa [7, 76]. Eye contact is a paradigmatic case.
Note, however, and this is a key point, that lived space is already structured as a
medium conditioning mutual perceptibility before any other subject comes into
40
G. Declerck and C. Lenay

play and even if there is nobody around. Mutual perceptibility space deals with
mere possibilities: it is a ‘model’ of whether one can be seen or heard from where
one is, but it does not include knowledge about whether one is currently seen or
heard by this or that particular other. The distinction is important, because per-
ceiving whether one is currently perceived results from special mechanisms, which
most of the time can only work based on the mutual perceptibility space. I see that
my current location can be accessed visually (only) from this or that area, that is,
someone that would be there could see me. But additional conditions must be
fulﬁlled when someone is effectively occupying these areas for me to perceive that
she is currently seeing me: I must see that she is oriented to me, that her eyes are
open, that she’s looking at me, etc. Many of these perceptual parameters are
related to body orientation and attitudes. These elements are not included in
mutual perceptibility space, which only delimits the regions of space from which
one could be seen.
The mastering of mutual perceptibility space presupposes, in addition, that we
have an adequate ‘model’ of our body dimensions, something like what is referred
to as the ‘body schema’ in psychology [42, 58]. It is quite obvious that children
under a certain age lack such model or still have an inappropriate one, as
demonstrated by their attempt, when playing hide and seek, to hide behind struc-
tures too small for their body. The hiding affordance offered by the object is not yet
scaled on their body dimensions. More generally, for a long time children lack an
understanding of the spatial conditions that must be fulﬁlled for other persons to
perceive what they perceive. Children under the age of four or ﬁve will for instance
ask your opinion about things (typically, toys they are playing with) you cannot
currently see because you are in another room or on the phone. The same holds for
being heard: in a noisy environment, they can talk to you because they see you’re
there, without understanding that you cannot hear them from where you are.
Modern technologies, such as video surveillance and satellite scanning systems,
or, considering the aural channel, microphones, are also structuring our mutual
perceptibility space. The basic point with these devices is that they break, to a
certain extent, the spatial constraints that are ‘normally’ associated with mutual
perceptibility: where the other perceiving agent and the surveillance device are
located relative to the person who is observed does not matter, they could be at
several meters or on mars, it does not change anything. The spatial constraint is not
erased, however, but it is shifted on the spatial relation between the person who is
observed and the capture device, whose range is more or less important. And it is
this spatial constraint that structures the mutual perceptibility space. The thief sees
that the camera cannot see this part of the store. James Bond puts some loud music
on before starting to speak to Felix Leiter for he suspects his room is full of
microphones.
Living in Space. A Phenomenological Account
41

6
The Space of the Blind. Do Visually Impaired
People Live in Another Space?
In this last section, we will draw on the phenomenological elements analyzed in this
chapter to address the question of how blind people experience space. To put it in a
nutshell, we have seen that:
(i)
space is experienced as a set of interconnected places (a kind of network)
deﬁned by social practices and means of accessibility (Sect. 2);
(ii)
lived space is shaped by one’s bodily skills; the spatial arrangement and
spatial features one is aware of at each moment constitutively depends on
one’s being-a-body and one’s capacity to act through this body (Sect. 3);
(iii)
one’s experience of space in perception is essentially determined by an
anticipation of, and commitment to, possibilities: one perceives space, pla-
ces, distances, because one foresees what can and cannot be; this can be
rephrased the following way: the content of one’s spatial perception must
partly be speciﬁed by appeal to counterfactual statements about one’s
practical possibilities and the possibilities describing how the things in space
could be otherwise located (Sect. 4);
(iv)
lived space is shaped by one’s consciousness of being visible (or more
generally perceptible) by others; that is, the many possibilities shaping one’s
experience of space include the possibility of being perceived by others
(Sect. 5).
Considering the various parameters playing a critical role in the shaping of lived
space, it is reasonable to expect that sighted and blind people should experience
space in a very similar way. You do not cease to have a body that can move because
you cannot see, you do not cease to intend objects as being more or less accessible
depending on where they are, and you do not cease to experience space as orga-
nized as a network of interconnected places with allotted functions, offering dif-
ferent possibilities, with different social status and atmospheres. The same can
certainly be held for what regards the possession of most spatial concepts and for
spatial reasoning. As Landau [74] explains—and this is now widely agreed
upon38–, “certain principles of spatial knowledge arise independently of the par-
ticular avenue of experience. That is, the blind and sighted develop identical
principles rather early in life, despite their very different encounters with the world.”
So, where are the differences? Is there any difference except that space is not seen
by blind people? As a working hypothesis, the following proposal can be made,
which is related to a particular interpretation of the speciﬁcity of vision against
38Note that this claim only applies to the development of spatial concepts, and does not mean that
there are not differences in spatial cognition between the blind and the sighted [112]. For instance,
it is now widely acknowledged that blind people extensively rely on egocentric coding to orient
themselves in space and have difﬁculties with allocentric spatial representations [see e.g. 13, 115].
See however Lewis et al. [77] for qualifying this claim.
42
G. Declerck and C. Lenay

other modalities: compared to the space of sighted people, (i) the space of the blind
is poorer in terms of currently available possibilities and is more internalized (6.1);
(ii) the space of the blind is lacking some principles of organization (or their role is
largely diminished), some of which play a critical role in social relations, such as
the principle of occlusion (6.2).
6.1
The Memory-Dependence of the Space of the Blind
A ﬁrst claim we can put forward to try to catch the speciﬁcity of the kind of space
blind people live in is that it relies to a substantial extent on memory and other
representational abilities (see the Introduction section for this notion). In other
terms, the space blind people live in is much more ‘internalized’ than the space of
sighted people. Surely, the spatial environment is also present for the blind person:
the space around is not something she is thinking about (i.e. something that is
represented, as when recalling a place where one lived), it is there now, and she
experiences herself as located within it, just as sighted people; but to deal efﬁciently
with her environment the blind person must constantly make use of her memory to
keep track of what is there and where things are.
Vision enables what could be described as an externalized awareness of the
varieties of possibilities made available by the environment, the spatial layout and
the objects. When tapping on the computer, the room with its familiar equipment
and related opportunities are visually present in the background. I see what the
environment affords and where the objects are. Or, to put it a bit differently, I know
what the environment affords and where the objects are because it is explicitly
visible, i.e. the information is available and can be accessed if needed [89]. I do not
have to memorize the position of my cup of coffee on the table, for I just have to
look to know this position. The visual space itself works as an external memory
[90].
Because they cannot rely on this immediately available external memory, blind
people have no choice but to keep track of everything, that is, the spatial
arrangement of the environment and the places of objects must be known: this
availability through knowledge can be based on explicit place memory (remem-
bering that the pen is on the top of the shelf), but also on a more implicit way, e.g.
the practical expectation that a given object is where it has always been (the kitchen
table at this location in the room), that is, on automatic body habits. In other terms,
blind people must replicate ‘inside’ the spatial arrangement of the ‘outside’: they
constantly need a mental map.
It is obvious that the only way to manage this task with reasonable performances
is to limit (drastically) the number of things in the environment and be very
organized: everything must have a place and remain in its place. The strategy some
blind people use to clean the ﬂoor is a good illustration. How to achieve this task
without seeing what parts have already been cleaned and what parts haven’t?
When you see, you just have to look. When you don’t, you must memorize.
Living in Space. A Phenomenological Account
43

But your memory is limited and subject to interferences. Most blind people achieve
this task by moving in a grid type pattern: this hyper-rational strategy not only
ensures that no part of the ﬂoor will be forgotten or cleaned twice, it alleviates the
tracking task and make it more robust to interferences. This can probably be taken
as a general rule: how blind people arrange their lifeplace follows from the
requirement that the places allotted to objects can be easily and reliably remem-
bered.39 They have to continuously tidy up their environment, both in a concrete
and a representational sense.
Sighted people generally follow the opposite principle: they rely as much as
possible on the external memory their visual access provides to keep track of where
things are. That is, they only store in their ‘internal’ memory elements that cannot
be directly ‘stored’ in the environment’s external memory or they will store indi-
cations (e.g. memo) that are necessary to make use of the environment as an
external memory [23]. Why should I remember where I put my glass considering I
can immediately see where it is? The lifeplace of many sighted people is a mess
because disorder is something they can deal with, considering their cognitive
equipment.
An additional objective of the lifeplace arrangement of blind people—this is
another hypothesis we propose—is to secure the principle of place permanence
according to which ‘things are still where they have been left’ (see above,
Sect. 4.4). Of course, this requirement only applies to movable objects, not to
objects ﬁxed on an immovable support, e.g. a wall switch, a door handle, a wall
cabinet. But most of the things we make use of are displaceable, and a condition to
use them is to get hold of them. In that respect, living in collectivity can be a real
challenge for blind people, the presence of others in the lifeplace equating to more
entropy, which puts additional perturbation on the principle of place permanence
they must ﬁght to preserve. Keeping track of where objects are through memory is
useless if the objects do not stay where they have been left. This is true for the key
you have put on the table (not on the wall key hook) and the butter you have stored
in the kitchen cupboard (not in the fridge), but also for the chair you have put that
particular way under the table to avoid any future collision.
One of the most difﬁcult (and irritating) tasks for blind people is to search for an
object that is no more where it is supposed to be. One blind women (L.) we have
interviewed40 explained that she always puts the fruit yoghurts on one side of the
fridge and the plain ones on the opposite side (yoghurts boxes have generally the
same shape and can barely be discriminated on the sole basis of touch). But this
simple rule is useless if it is not applied by the other users of the fridge.
39Note that the ﬁeld of situated cognition offers important theoretical resources to deal with these
questions, especially the notion of ‘complementary strategy’ [69, 73].
40We thank her for having helped us toward a better understanding of the experience of blind
people.
44
G. Declerck and C. Lenay

6.2
Can Blind People Hide? The Principle of Perceptual
Occlusion in the Blind
Another peculiarity of the spatial cognition of blind people that can be diagnosed is
that they apparently show difﬁculties in conceptualizing the principle of occlusion
(or inter-position), namely the possibility for something to be hidden or masked
because of the presence of something else.
The principle of occlusion fulﬁlls a pivotal role in the construction of the mutual
perceptibility space, for being hidden by something (typically, a wall) is a basic way
to escape the eyes of others (see Sect. 5). We can thus expect the mutual percep-
tibility space of the blind to show distortions compared to the sighted.41 This claim
must however be taken with caution, for it obviously does not apply equally to
congenitally blind, early blind and late blind persons, and inter-individual differ-
ences are probably important. In addition, though occlusion undoubtedly plays a
central role in the organization of visual experience, it can hardly be reduced to the
visual universe. The existence of non-visual forms of occlusion is for instance
accredited by everyday language. One can mask a bad odour with perfume. One can
mask a sound with a louder one. One can mask one’s emotions or desires. And one
can mask the truth. Moreover, congenitally blind people have obviously a sense of
what part of themselves can be seen or what part cannot. They also exhibit some
complex understanding of the meaning of sentences with terms related to vision
(e.g. ‘see’, ‘look’), which indicate that they understand the spatial constraints on the
visual access to something, typically that you cannot see their back unless they turn
their back to you [see 74: 360 sqq.]. Some data suggest, in addition, that blind
people can understand visual occlusion when exploring tactile images [72]. This is
also true for all that is related to the domain of privacy or intimacy. And—though it
is not a spatial fact in the strict sense—blind people undeniably know their mental
states are not something ‘visible’ to others. (It is worth noting, however, that this
kind of knowledge can have peculiarities compared to the knowledge sighted
people generally have. Some blind people may for instance feel surprised when
they come to learn that most emotions are visible on the face.42) As far as we know,
41The fact that some blind subjects have impairments in representing the perceptual point of view
of others, i.e. what they perceive from where they are, supports this view. Miletic [83] has for
instance observed that congenitally blind children show severe difﬁculties in a perspective taking
task. Hollins and Kelley [64] ﬁnd similar results in blind adults. But see Heller and Kennedy [63]
for contrastive data.
42Ville-Gilon [117], who has performed a series of semi-directed interviews with blind people to
investigate how they understand and deal with their visibility, for instance explains: “During a
sculpture session L. was asked to touch the face of statues: a face contorted in pain or expressing
joy, or laughing, or crying, etc. She told me that she was suddenly aware that emotions were ‘seen
from the outside’ and that she believed until then that for an emotion to be visible on the face of
someone it had to be ‘deliberate’. She thought that a face as such was ‘frozen’, and that it was
consequently useless. She said that now she was ‘careful not to show everything’.” (Ville-Gilon
[117], our translation) Note that L. is approximately 45 years old.
Living in Space. A Phenomenological Account
45

data about the capacity of blind people to conceptualize their own perceptibility and
the possibility of being hidden from others is however lacking, and this is this
speciﬁc issue that we wish to examine in the following. The point is that if
occlusion can surely be conceived through other perceptual modalities than vision,
these modalities probably cannot substitute for the kind of ‘visibility’ that is
enabled by visual perception: in short, vision makes one visible in a way that other
modalities cannot replace.
Is it possible to develop an understanding of the principle of occlusion without
vision? Note ﬁrst that being occluded is not the same as being out of view (i.e. too
distant to be perceived) or absent. What is occluded is there now and visible but it
cannot be seen because of the presence of something else, which ‘blocks’ the
access. Occlusion essentially involves a counterfactual statement about the per-
ceptual accessibility in other conditions of what is currently hidden. In order to
apprehend that something A is occluded by something else B, one must believe or
expect that if B was not there, one would perceive A from where one is (i.e. without
changing one’s position).43 But one must in addition conceive of the occlusion
relation as the result of a spatial arrangement, literally an inter-position: for B to
mask A, in a way or another B must be located before A. In order to perceive
occlusion it is thus necessary to access perceptually to something like a depth, the
simultaneous presence of a series of positions arranged in a before-behind order.
This possibility is obviously lacking in touch which is subordinated (by deﬁnition)
to contact relations (you only touch what you are in contact with). You cannot ﬁnd
it in olfaction either, which obeys similar principles as touch (this is a contact
sense). Is it possible to develop an understanding of the principle of occlusion
through hearing? Hearing seems in a better position to fulﬁll the above prerequi-
sites. Hearing is like vision a sense of distance that enables to access something like
a depth: by hearing you can access simultaneously multiple sources of sound at
different distances, and you can certainly conceive that a sound masks another one,
i.e. makes it inaudible. The problem remains, however, of the spatial character of
this masking relation. When a sound is masking another, this is not primarily
because of the respective positions of the sound sources, but rather their respective
intensities at the position one occupies. As a matter of fact, a sound source can be
more remote than another and mask the latter just because it is louder. I cannot hear
what John, who is right beside me, is saying because the speakers over there are
playing too loud. However, audition makes it possible to conceive a real occlusion
relation with non sound-emitting structures. One can prevent a sound from reaching
one’s ears by blocking its diffusion with a physical obstacle, e.g. by closing a door.
And, as we have seen in Sect. 5, the so-called ‘obstacle sense’ (i.e. echolocation
ability) most blind individuals rely on to locate, based on auditory input, distant
objects [see e.g. 4, 109, 111], could in principle be used to perceive that the
43It does not mean that occlusion is something you cannot perceive, but must conceive or imagine.
As seen in Sect. 4, counterfactual commitments are constitutive of perceptual experience: to
perceive is always to expect that if this change occurred in the perceptual circumstances, that
change would follow in the perceptual content.
46
G. Declerck and C. Lenay

presence of a physical structure prevents the auditory access to what’s behind. In a
similar way, some experiences of causality (more precisely, of the dependence of
perception upon a causal chain going from the source of stimulation to the sensory
organ) seems able to provide access to the principle of occlusion. We can stop the
causal action of the environment on us—noise, heat caused by sunlight, rain—by
interposing a blocking structure or ‘hiding’ behind something—under a roof, an
umbrella, behind a wall, or by covering our ears. These experiences are deﬁnitely
accessible to the blind.
Note, however, that an apparently irreducible gap between these different types
of perceptual occlusion and visual occlusion is that they can only incidentally imply
one’s own ‘visibility’ to other subjects (to make use of a term from the visual
domain, which has assuredly a paradigmatic role here). In the different situations
addressed above, what is blocked is one’s perceptual access to an object; but the
symmetrical situation, namely the situation where oneself is the object to be per-
ceived (or what exerts a causal inﬂuence on someone else), is more difﬁcult to
conceive in non-visual modalities. In other terms, realizing that we can hide from
the others behind occluding structures seems to be, ﬁrst and foremost, a visual
achievement. Once again, this claim must be qualiﬁed, for auditory perception
seems able to fulﬁll this role to some extent. If I make noise I can prevent being
heard by others by interposing a physical structure between us, typically by closing
a door. A difference that seems irreducible, however, is that we do not control, or
only marginally, our visibility (we cannot cease to be visible—though we can cease
to be seen), whereas in the vast majority of cases, we control the sounds emanating
from our body. If our body was constantly emitting a sound—which was, in
addition, characteristic of oneself, as is our visual aspect—, the situation of auditory
occlusion would offer similar perceptual possibilities than the situation of visual
occlusion. We cannot depart from this visibility, and we are, as a result, constantly
exposed to the perception of others: even when there is nobody to see us, we could
be seen. We cannot hide, and this impossibility deeply affects the space built by the
sighted.
References
1. Allais L (2009) Kant, non-conceptual content and the representation of space. J Hist Philos
47(3):383–413
2. Armel KC, Ramachandran VS (2003) Projecting sensations to external objects: evidence
from skin conductance response. Proc R Soc London B: Biol Sci 270(1523):1499–1506
3. Armstrong DM (1993) A world of states of affairs. Philos Perspect 7: 429-440
4. Ashmead DH, Hill EW, Talor CR (1989) Obstacle perception by congenitally blind children.
Percept Psychophysics 46(5):425–433
5. Austin JL (1956–57) A plea for excuses. Proceedings of the Aristotelian Society, 56.
Reedited in Urmson JO, Warnock GJ (eds) Philosophical Papers (pp. 175–204). Clarendon
Press, Oxford, 1979, 3rd edition
Living in Space. A Phenomenological Account
47

6. Auvray M, Hanneton S, Lenay C, O’Regan JK (2005) There is something out there: distal
attribution in sensory substitution, twenty years later. J Integr Neurosci 4(04):505–521
7. Auvray M, Lenay C, Stewart J (2009) Perceptual interactions in a minimalist virtual
environment. New Ideas Psychol 27(1):32–47
8. Auvray M (2004) Immersion et perception spatiale: l’exemple des dispositifs de substitution
sensorielle. Doctoral dissertation, Atelier national de Reproduction des Thèses
9. Bauer G‚ Gerstenbrand F‚ Rumpl E (1979) Varieties of the locked-in syndrome. J Neurol
221(2):77–91
10. Bergson H (1896) Matter and memory (trans: Paul NM & Palmer WS). George Allen and
Unwin, London, 1911
11. Berkeley G (1709) An essay towards a new theory of vision. In: Ayers MR (ed)
Philosophical works. Everyman, London, 1998
12. Berthoz A (1991) Reference frames for the perception and control of movement. In:
Paillard J (ed) Brain and space. Oxford University Press, New York, pp 81–111
13. Bigelow AE (1996) Blind and sighted children’s spatial knowledge of their home
environments. Int J Behav Dev 19(4):797–816
14. Botvinick M, Cohen J (1998) Rubber hands ‘feel’ touch that eyes see. Nature 391(6669):
756–756
15. Cairns D (2013) The philosophy of Edmund Husserl. Phaenomenologica, vol. 207. Springer
Science & Business Media, New York
16. Campbell J (1993) The role of physical objects in spatial thinking. In: Eilan N,
McCarthy RA, Brewer B (eds) Spatial representation: problems in philosophy and
psychology. Blackwell Publishing, Malden, pp 65–95
17. Casati R, Varzi AC (1994) Holes and other superﬁcialities. MIT Press, Cambridge
18. Casati R, Varzi AC (1997) Spatial entities. In: Stock O (ed) Spatial and temporal reasoning.
Springer, Netherlands, pp 73–96
19. Casati R, Varzi AC (1999) Parts and places: the structures of spatial representation. MIT
Press, Cambridge
20. Caspar EA, Cleeremans A, Haggard P (2015) The relationship between human agency and
embodiment. Conscious Cogn 33:226–236
21. Cataldi SL (1993) Emotion, depth, and ﬂesh. A study of sensitive space. Reﬂections on
Merleau-Ponty’s philosophy of embodiment. State University of New York Press,
New York
22. Chadwick MJ, Mullally SL, Maguire EA (2013) The hippocampus extrapolates beyond the
view in scenes: an fMRI study of boundary extension. Cortex 49(8):2067–2079
23. Clark A, Chalmers D (1998) The extended mind. Analysis 58(1):7–19
24. Clark A, Toribio J (1994) Doing without representing? Synthese 101(3):401–431
25. Committeri G, Galati G, Paradis AL, Pizzamiglio L, Berthoz A, LeBihan D (2004)
Reference frames for spatial cognition: different brain areas are involved in viewer-, object-,
and landmark-centered judgments about object location. J Cogn Neurosci 16(9):1517–1535
26. Crane T, French C (2016) The problem of perception. The Stanford Encyclopedia of
Philosophy, Spring 2016 Edition, E.N. Zalta (ed.)
27. Declerck G (2014) Résistance et tangibilité. Essai sur l’origine phénoménologique des corps,
Les Éditions du Cercle Herméneutique
28. Declerck G (2016) What could have been done (but wasn’t). On the counterfactual status of
action in Alva Noë’s theory of perception. Phenomenology and the Cognitive Sciences.
doi:10.1007/s11097-016-9474-y
29. Declerck G (2014b) Des conséquences parfois pénibles de prendre de la place. In
Diaconu M, Copoeru I (eds) Studia phaenomenologica, vol. XIV, « Place, Environment,
Atmosphere » , pp 73–99
30. de Condillac EB (1754) Treatise on the sensations. Translated by (trans: Carr G). The Favil
press, London, 1930
31. Degenaar J, Myin E (2014) Representation-hunger reconsidered. Synthese 191(15):
3639–3648
48
G. Declerck and C. Lenay

32. De Jaegher H, Di Paolo E, Gallagher S (2010) Can social interaction constitute social
cognition? Trends Cogn Sci 14(10):441–447
33. Descartes R (1641) Meditations on ﬁrst philosophy: with selections from the objections and
replies. Trad. J. Cottingham. Cambridge University Press, 2013
34. De Vignemont F (2011) Embodiment, ownership and disownership. Conscious Cogn 20(1):
82–93
35. Dreyfus HL (1991) Being-in-the-world. A commentary on Heidegger’s being and time,
division I. MIT Press, Cambridge
36. Dreyfus HL (1992) What computers still can’t do. A critique of artiﬁcial reason, 1st edn.
MIT Press, Cambridge, 1972
37. Ehrsson HH, Spence C, Passingham RE (2004) That’s my hand! Activity in premotor cortex
reﬂects feeling of ownership of a limb. Science 305(5685):875–877
38. Evans G (1982) The varieties of reference. Clarendon Press‚ Oxford
39. Fink E (1976) Proximité et distance, Essais et conférences phénoménologiques. Trad.
J. Kessler.
Grenoble:
Jérôme
Millon,
1994.
German
edition:
Nähe
und
Distanz:
phänomenologische Vorträge und Aufsätze, Freiburg/München: Alber
40. Foucault M (1975) Discipline and punish: the birth of the prison. Random House/Vintage
Books, Translated by A. Sheridan. New York, p 1977
41. Franchak JM, Celano EC, Adolph KE (2012) Perception of passage through openings
depends on the size of the body in motion. Exp Brain Res 223(2):301–310
42. Gallagher S (1986) Body image and body schema: a conceptual clariﬁcation. J Mind Behav
7(4):541–554
43. Gallagher S (2005) How the body shapes the mind. Clarendon Press, Oxford
44. Gallagher S, Zahavi D (2008) The phenomenological mind. An introduction to philosophy
of mind and cognitive science. Routledge, New York
45. Gapenne O (2010) Kinesthesia and the construction of perceptual objects. In Stewart J,
Gapenne O, Di Paolo E (eds) Enaction. Toward a new paradigm for cognitive science. MIT
Press, Cambridge, pp 183–218
46. Gibson JJ (1950) The perception of the visual world. The Riverside Press, Cambridge
47. Gibson JJ (1958) Visually controlled locomotion and visual orientation in animals. Br J
Psychol 49:182–194
48. Gibson JJ (1962) Observations on active touch. Psychol Rev 69(6):477
49. Gibson JJ (1979) The ecological approach to visual perception. Lawrence Erlbaum
Associates, Hillsadle, New Jersey, p 1986
50. Gibson EJ (1988) Exploratory behavior in the development of perceiving, acting, and the
acquiring of knowledge. Annu Rev Psychol 39(1):1–42
51. Gordon MS, Rosenblum LD (2004) Perception of sound-obstructing surfaces using
body-scaled judgments. Ecol Psychol 16(2):87–113
52. Gottesman CV, Intraub H (1999) Wide-angle memories of close-up scenes: a demonstration
of boundary extension. Behav Res Methods Instrum Comput 31(1):86–93
53. Gray R, Tanesini A (2010) Perception and action: the taste test. Philos Q 60(241):718–734
54. Grush R (2001) Self, world and space: on the meaning and mechanisms of egocentric and
allocentric spatial representation. Brain Mind 1(1):59–92
55. Grush R (2004) The emulation theory of representation: motor control, imagery, and
perception. Behav Brain Sci 27(3):377–396
56. Hall ET (1966) The hidden dimension. Doubleday & Co, New York
57. Hayes PJ (1978) The naive physics manifesto. In: Michie D (ed) Expert systems in the
micro-electronic age. Edinburgh University Press, Edinburgh, pp 242–270
58. Head H, Holmes G (1911) Sensory disturbances from cerebral lesions. Brain 34:102–254
59. Heidegger M (1925) History of the concept of time: prolegomena (GA 20). Indiana
University Press, Translated by T. Kisiel. Bloomington, p 1992
60. Heidegger M (1927) Being and time. Harper & Row, Translated by J. Macquarrie & E.
Robinson. New York, p 1962
Living in Space. A Phenomenological Account
49

61. Heidegger M (1975) The basic problems of phenomenology. Revised edition. (trans:
Hofstadter A). Indiana University Press, 1988
62. Heidegger M (1987) Towards the deﬁnition of philosophy. Freiburg lecture-courses 1919.
Translated by T. Sadler. The Athlone Press, London and New Brunswick, NJ, 2000
63. Heller MA, Kennedy JM (1990) Perspective taking, pictures, and the blind. Percept
Psychophysics 48(5):459–466
64. Hollins M, Kelley EK (1988) Spatial updating in blind and sighted people. Percept
Psychophysics 43(4):380–388
65. Husserl E (1950) Cartesian meditations: an introduction to phenomenology. Martinus
Nijhoff Publishers, Translated by D. Cairns. The Hague/Boston/London, p 1960
66. Husserl E (1952) Ideas pertaining to a pure phenomenology and to a phenomenological
philosophy. Second Book: Studies in the Phenomenology of Constitution. Translated by R.
Rojcewicz & A. Schuwer. Kluwer Academic Publisher, Dordrecht/Boston/London, 1989
67. Husserl E (1954) Experience and judgment: investigations in a genealogy of logic. Revised
and Edited by L. Landgrebe. Translated by J.S. Churchill and K. Ameriks. Northwestern
University Press, 1973
68. Husserl E (1973) Thing and space. Lectures of 1907. Collected Works, vol. 7. (trans:
Rojcewicz R). Kluwer Academic Publishers, Dordrecht, 1997
69. Hutchins E (1995) Cognition in the wild. MIT Press, Cambridge
70. Hutto DD, Myin E (2013) Radicalizing enactivism. Basic minds without content. MIT Press,
Cambridge
71. Intraub H, Richardson M (1989) Wide-angle memories of close-up scenes. J Exp Psychol
Learn Mem Cogn 15(2):179
72. Kennedy JM (1980) Blind people recognizing and making haptic pictures. In: Hagen MA
(ed) The perception of pictures. Dürer’s devices: beyond the projective model of pictures,
vol 2. Academic Press, pp 263–303
73. Kirsh D (1995) The intelligent use of space. Artif Intell 73(1–2):31–68
74. Landau B (1988) The construction and use of spatial knowledge in blind and sighted
children. In: Stiles J, Kritchevsky M, Bellugi U (eds) Spatial cognition: brain bases and
development. Lawrence Erlbaum Associates, Hillsdale, N.J./Hove and London, pp 343–371
75. Legrand D, Brozzoli C, Rossetti Y, Farne A (2007) Close to me: multisensory space
representations
for
action
and
pre-reﬂexive
consciousness
of
oneself-in-the-world.
Conscious Cogn 16(3):687–699
76. Lenay C, Stewart J (2012) Minimalist approach to perceptual interactions. Front Hum
Neurosci 6:258–276
77. Lewis V, Collis G, Shadlock R, Potts M, Norgate S (2002) New methods for studying blind
children’s understanding of familiar space. Br J Vis Impairment 20(1):17–23
78. Locke J (1690) An essay concerning human understanding. Book II. Jonathan Bennett
electronic edition. http://www.earlymoderntexts.com/
79. Maine de Biran FGP (1802) The inﬂuence of habit on the faculty of thinking. Williams &
Wilkins, Baltimore 1929
80. Maine de Biran FGP (1812) Essai sur les fondements de la psychologie. Vrin, Paris 2001
81. Malpas J (2008) Heidegger’s topology: being, place, world. MIT Press, Cambridge,
Massachusetts, London, England
82. Merleau-Ponty M (1945) Phenomenology of perception (trans: Smith C). Routledge &
Kegan Paul, London, 1962
83. Miletic G (1995) Perspective taking: knowledge of Level 1 and Level 2 rules by congenitally
blind, low vision, and sighted children. J Vis Impairment Blindness 89(6):514–523
84. Minsky L (1975) A framework for representing knowledge. In: Winston PH (ed) The
psychology of computer vision. McGraw-Hill, New York, pp 211–277
85. Morgado N, Gentaz É, Guinet É, Osiurak F, Palluel-Germain R (2013) Within reach but not
so reachable: Obstacles matter in visual perception of distances. Psychon Bull Rev 20(3):
462–467
86. Morris D (2004) The sense of space. State University of New York Press
50
G. Declerck and C. Lenay

87. Noë A (2004) Action in perception. MIT Press, Cambridge
88. Noë A (2007) Understanding action in perception: replies to hickerson and keijzer. Philos
Psychol 20(4):531–538
89. Noë A (2012) Varieties of presence. Harvard University Press, Cambridge
90. O’Regan JK, Noë A (2001) A sensorimotor account of vision and visual consciousness.
Behav Brain Sci 24(5):939–973
91. Paillard J (1971) The motor determinants of spatial organization. Cahiers de Psychol
14:261–316
92. Peacocke C (1983) Sense and content: experience, thought, and their relations. Oxford
University Press, Oxford
93. Poincaré H (1905) La Valeur de la science. Paris: Flammarion. Translated by G.R. Halsted,
The value of science. Dover, New York, 1958
94. Profﬁtt DR (2006) Distance perception. Curr Dir Psychol Sci 15(3):131–135
95. Profﬁtt DR (2006) Embodied perception and the economy of action. Perspect Psychol Sci 1(2):
110–122
96. Profﬁtt DR, Stefanucci J, Banton T, Epstein W (2003) The role of effort in perceiving
distance. Psychol Sci 14(2):106–112
97. Reed ES (1996) Encountering the world: toward an ecological psychology. Oxford
University Press, New York, Oxford
98. Rietveld E (2012) Bodily intentionality and social affordances in context. In Paglieri F
(ed) Consciousness in interaction. The role of the natural and social context in shaping
consciousness. J. Benjamins, Amsterdam, pp 207–226
99. Russell B (1912) The problems of philosophy. Oxford University Press, New York, 1997
100. Sartre JP (1943) Being and nothingness: an essay on phenomenological ontology (trans:
Barnes HE). Washington Square Press, New York, 1993
101. Scheler M (1927) Idealismus-Realismus, Philosophischer Anzeiger, II. Verlag, Bonn
Friedrich Cohen (trans:Lachterman DR) in selected philosophical essays. Northwerstern
University Press, Envanston, 1973
102. Searle JR (1992) The rediscovery of the mind. MIT Press, Cambridge
103. Shaw R, Turvey MT, Mace W (1982) Ecological psychology: the consequence of a
commitment to realism. Cogn Symbolic Process 2:159–226
104. Stefanucci JK, Geuss MN (2009) Big people, little world: the body inﬂuences size
perception. Perception 38(12):1782–1795
105. Steiner P (2013) The delocalized mind. Judgements, vehicles, and persons. Phenomenol
Cogn Sci, 1–24
106. Stewart JR, Gapenne O, Di Paolo EA (2010) Enaction: toward a new paradigm for cognitive
science. MIT Press, Cambridge
107. Straus EW (1935) The primary world of senses: a vindication of sensory experience. New
York: Free Press of Glencoe, 1963
108. Stroffregen TA, Pittenger JB (1995) Human echolocation as a basic form of perception and
action. Ecol Psychol 7(3):181–216
109. Supa M, Cotzin M, Dallenbach KM (1944) ‘Facial vision’: the perception of obstacles by the
blind. Am J Psychol 57(2):133–183
110. Taylor C (1995) The validity of transcendantal arguments. In Philosophical arguments
(chap. 2, pp 20–33). Harvard University Press, Cambridge
111. Thaler L (2013) Echolocation may have real-life advantages for blind people: an analysis of
survey data. Front physiol 4:98
112. Thinus-Blanc C, Gaunet F (1997) Representation of space in blind persons: vision as a
spatial sense? Psychol Bull 121(1):20
113. Tsakiris M, Schütz-Bosbach S, Gallagher S (2007) On agency and body-ownership:
phenomenological and neurocognitive reﬂections. Conscious Cogn 16(3):645–660
114. Turvey MT (1992) Affordances and prospective control: an outline of the ontology. Ecol
Psychol 4:173–187
Living in Space. A Phenomenological Account
51

115. Ungar S, Blades M, Spencer C (1996) The construction of cognitive maps by children
with visual impairments. In: The construction of cognitive maps. Springer, Netherlands,
pp 247–273
116. Varela FJ, Thompson E, Rosch E (1991) The embodied mind. Cognitive science and human
experience. MIT Press, Cambridge
117. Ville-Gilon F (2014) Participation à deux projets de recherche sur le visage, MD dissertation.
Université de Technologie de Compiègne, COSTECH
118. Wagman JB, Taylor KR (2005) Perceiving affordances for aperture crossing for the
person-plus-object system. Ecol Psychol 17(2):105–130
119. Wallmeier L, Geßele N, Wiegrebe L (2013) Echolocation versus echo suppression in
humans. Proc Roy Soc Lond B Biol Sci 280(1769):20131428
120. Warren WH, Whang S (1987) Visual guidance of walking through apertures: body-scaled
information for affordances. J Exp Psychol Hum Percept Perform 13(3):371
121. Westphal KR (2009) Kant’s transcendental proof of realism. Cambridge University Press,
Cambridge
122. Wheeler M (2005) Reconstructing the cognitive world: the next step. MIT Press, Cambridge
123. Witt JK, Profﬁtt DR, Epstein W (2005) Tool use affects perceived distance, but only when
you intend to use it. J Exp Psychol Hum Percept Perform 31(5):880
124. Wittgenstein L (1969) On certainty (trans: Paul D, Anscombe GEM). Basil Blackwell, New
York and Evanston
125. Zahavi D (1994) Husserl’s Phenomenology of the Body. Études Phénoménologiques
19:63–84
126. Zahavi
D
(1999)
Self-awareness
and
alterity:
a
phenomenological
investigation.
Northwestern University Press, USA
52
G. Declerck and C. Lenay

Technologies to Access Space Without
Vision. Some Empirical Facts and Guiding
Theoretical Principles
Charles Lenay and Gunnar Declerck
A large number of technical devices attempt to help blind persons improve their
spatial perception and facilitate their mobility. We wish to present here the prin-
ciples on which these prosthetic perceptual devices function, the conditions of their
appropriation, and the general perspectives they open concerning the role of
technical objects and systems in the constitution of human experience. A technical
device for assisting perception has to compensate for a sensory deﬁcit by mobi-
lizing other subsisting sensory modalities which remain available. We therefore
have to understand the relationship between perceptual activity and sensory input.
In addition, we shall see that the technical devices which assist perceptual activity
can in return serves as tools for experimental scientiﬁc research on the mechanisms
of perception in general.
The analyses presented in the preceding chapter seem to show that perceptual
experience of the space which surrounds us is the experience of the availability of a
system of actions, in other words an organized set of possible journeys and oper-
ations that can be performed in the world. Whatever the sensory modalities that a
subject has at her disposition (be they visual, auditory or tactile), we can suppose
that her perception of a space which surrounds her corresponds for her to the
presence of a ﬁeld of possibilities of this sort, which enables her to localize objects,
to choose gestures and movements, to decide which actions to perform, etc. The
tools and prosthetic devices she can use aim at modifying and enriching this ﬁeld of
possibilities, which will give access to new actions and perceptions. Our aim in this
chapter is to propose several hypotheses to explain the mechanisms by which
C. Lenay (&)  G. Declerck
Sorbonne Universités, Université de Technologie de Compiègne,
EA 2223 Costech (Connaissance, Organisation et Systèmes Techniques),
Centre Pierre Guillaumat - CS 60 319-60 203, Compiègne, France
e-mail: charles.lenay@utc.fr
G. Declerck
e-mail: gunnar.declerck@utc.fr
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_2
53

perception can be transformed according to the tools and prosthetic devices
employed, and to show that in each case this transformation amounts, ﬁnally, to a
reconﬁguration of a ﬁeld of possibilities. To do this, we shall start from the extreme
situation of the so-called ‘sensory substitution’ systems; we will then decline
several variations so as to better understand in each case the way in which the
technical mediation transforms the relations between sensation and perception, and
thereby modiﬁes the ﬁeld of accessible possibilities.
1
Sensory Substitution
The so-called ‘sensory substitution’ devices correspond to the ambition, which may
seem naïve, to replace a deﬁcient sensory input by another modality which is still
functional. A more detailed presentation of these devices in all their diversity will be
given later (Chapter “Scene Representation for Mobility of theVisually Impaired”).
Here, we will focus on the most emblematic and most radical device of perceptual
aid to compensate for the absence of vision: the Tactile Vision Substitution System
(TVSS) proposed by Paul Bach y Rita at the end of the 1960’s [4, 5]. The TVSS
consists of a square matrix of 400 tactile stimulators connected to a digital camera.
The image captured by the camera is simpliﬁed and converted into black and white
pixels (without intermediate grey levels), and then used to control the activation of a
‘tactile image’ of 20  20 pixels, i.e. 400 tactile stimulators which are raised or not
according to whether the corresponding element of the image is black or white. This
tactile matrix is applied to the skin, either on the back (the ﬁrst version), or on the
chest or the forehead [13], and more recently on the tongue [7, 9] (Fig. 1).The ﬁrst
trials with this sort of device provided three fundamental results.
(i)
First of all, the presentation of shapes to an immobile camera procured only a
very limited discrimination of the tactile stimuli, which were perceived as
being on the surface of the skin. Thus, the simple substitution of a sensory
input through the optic nerve by a tactile entry does not, in itself, give access
to a spatial perception.
(ii)
However, if the user was active (moving the camera by translations and
rotations, and zooming), he developed spectacular capacities for recognizing
shapes. After 15 h of practice, he discriminated increasingly complex
familiar objects, to the point of being able to recognize faces.
(iii)
Moreover, this capacity to recognize shapes was accompanied by an external
projection of the percepts. The user no longer felt tactile stimuli on the skin,
and instead perceived stable objects ‘out there’ in front of him in a
three-dimensional space [5]. The perception of a stable object ‘out there’ is
quite distinct from the succession of variable sensory stimuli that the subject
receives as he constantly moves the camera. The blind person begins by
learning how variations in his sensations are related to his actions: when he
54
C. Lenay and G. Declerck

moves the camera from left to right, the stimuli on his skin move from right
to left; when he zooms forward, the stimuli move apart, etc. He also dis-
covers perceptual concepts that are new to him such as parallax, shadows,
occlusion, etc. Certain classical visual illusions are spontaneously repro-
duced [6, 24]. This sort of experiment can be performed not only by a blind
person but just as well by a sighted person who is blindfolded.
Through these initial results, we see that a whole set of very fundamental
questions are posed concerning perceptual learning, the localization of objects in a
distal space and the recognition of shapes. It therefore seems to us that study of this
very particular extreme situation of sensory substitution could be useful for shed-
ding light on the general problems involved in the appropriation of perceptual aid
systems. Indeed, this sort of device makes it possible to follow the genesis of a
novel prosthetic perceptual modality; and in particular, to follow the constitution of
a space of perception in which objects can be perceived as being external [1, 38].
One can then carry out in parallel an objective analysis in a third-person perspective
of the resolution of perceptual tasks, coupled with the description in a ﬁrst-person
perspective of the corresponding lived experience.
We will ﬁrst of all examine the question of spatial localization and the perception
of the three-dimensional space which surrounds us; we will then study the
Fig. 1 The tactile vision
substitution System. Here the
matrix of 20  20 tactile
stimulators is placed on the
chest and the camera is placed
on the frame of a pair of
spectacles
Technologies to Access Space Without Vision …
55

perception of shapes and arrangements in the two-dimensional space of writing and
reading; before concluding in a more general way on technical mediation of
perception.
2
Spatial Localization
2.1
The General Problem
In the course of learning, the TVSS user ﬁrst of all feels the successive stimulations
on her skin. However with the progressive use of the device, she ends up forgetting
these tactile sensory inputs and comes to perceive stable objects at a distance, out
there in front of her. The very ﬁrst device transmitted the tactile stimuli to a matrix
placed on the back. Bach y Rita thereupon remarked: ‘When asked to identify static
forms with the camera ﬁxed, subjects have a very difﬁcult time; but when they are
free to turn the camera to explore the ﬁgures, the discrimination is quickly estab-
lished. With ﬁxed camera, subjects report experiences in terms of feelings on their
backs, but when they move the camera over the displays, they give reports in terms
of externally localized objects in front of them.’ [49: 25] Thus, according to the
witness accounts of the users, the proximal irritations provoked by the tactile plate
are quite different from perception as such [5, 42, 48]. The device and the tactile
sensations it procures are no longer perceived as such, when the device procures a
perception of objects in a distal space.
What we want to understand, then, is this appropriation which occurs by a sort of
‘switch’ in perceptual experience, which started out as a series of proximal events,
i.e. the tactile stimulations, and which at some point becomes centred on objects
which are separate from the body and which are situated in a distal space. One of
the keys to this transition seems to be the activity of the perceiving subject, the
movements of the receptor system (the camera). But what exactly is the role of
these movements? How does this activity participate in the emergence of a distal
perception?
Two main approaches are possible.
(i) First there is a representationalist approach, according to which one considers
that the actions of moving the captor have the sole function of acquiring
relevant information (in this case, mainly information about the relative
positions of the body and external objects) in order to construct an internal
representation [10]. According to this view, the actions are not properly
constitutive of the perception (if equivalent information could be obtained by
means other than action, there could quite well be perception without action).
(ii) Alternatively there is an enactivist approach, where one considers that the
actions are indeed constitutive and therefore absolutely necessary in the very
course of the perception of the distance of the objects. On this view, the
56
C. Lenay and G. Declerck

perceptual experience results directly from the sensory-motor dynamics,
without any need for recourse to the intermediary step of representations.
For the ﬁrst, representationalist approach, the sequences of activation of the
matrix of tactile stimulations makes it possible to progressively infer the distal
spatial coordinates of the perceived objects. The question of the proximal-distal
switch is then understood as a problem of ‘distal attribution’ [15, 33], namely ‘the
ability to attribute the cause of our proximal sensory stimulation to an exterior and
distinct object’ [1: 506]. Distal attribution would be a solution to a causal inference
problem, because the most likely environmental cause of incoming vibro-tactile or
electro-tactile stimulation—contrary to the default haptic interpretation—is a distant
scene. This position belongs to what is traditionally called the ‘representationalist-
inferentialist’ approach to perception: perceiving is equivalent to constructing (or
inferring), on the basis of the available sensory data, a representation of the objects
which are supposedly the cause of these sensations [16, 21, 22]. ‘Perception’ is a
process of elaborating hypotheses; its functioning is basically heuristic and prob-
abilistic.1 We may note in passing that this conception is not incompatible with
certain sensori-motor approaches to perception (see for example [40]).
However, this way of considering the phenomenon of the proximal-distal switch
is debatable, in the sense that the problem here is not to attribute the incoming
tactile data to external causes. Right from the start, there is a form of distal attri-
bution, since the novice user of the TVSS is quite conscious that the pressure on her
skin is produced by something external to her body, i.e. the matrix of tactile
stimulators. The tactile sensations are not brute data which have not yet achieved a
‘representational’ function (or a ‘ﬁgurative’ function as Husserl would say). Via the
‘tactile data’ it is already an objective environment which is perceived by the user: a
set of pressures exerted on her skin. The point is rather than these pressures cannot
yet be deciphered in terms of objects located in the environmental space (for
example ‘a chair over there’) or a spatial conﬁguration of rooms (for example ‘a
corridor’, ‘a door’, ‘a wall’). It is like when one looks at a ﬁgurative picture without
yet having succeeded in seeing what it represents, and one only perceives in the
beginning a system of shapes without any ‘meaning’. Auvray and Myin [3] quite
rightly compare sensory substitution to reading: before they have appropriated the
device, sensory data are like the words of a language that one does not speak.2
1“The content of the perceptual state formed in response to a particular pattern of stimulation—the
brain’s operative ‘hypothesis’ about the structure of the impinging environment—is the cause to
which the highest probability is assigned given all the available endogenous and exogenous
evidence. In the case of vision, this will normally be one of indeﬁnitely many possible
three-dimensional scenes.” (Briscoe, forthcoming: [10:6]).
2But, as Heidegger insists: “It requires a very artiﬁcial and complicated frame of mind to 'hear' a
'pure noise. […] In the explicit hearing of the discourse of the other, too, we initially understand
what is said […]. Even when speaking is unclear or the language is foreign, we initially hear
unintelligible words, and not a multiplicity of tone data.” (Heidegger 1927, §.34, p. 153 [p. 164]).
Technologies to Access Space Without Vision …
57

For the second sort of approach, that may be termed enactivist, the activity is an
indispensable component, not only for learning the prosthetic device, but also for
achieving the perceptual experience as such. Spatial perception is constituted as
much by the actions performed as by the variations in sensory input that they
occasion. From this point of view, the phenomenon of the perceptual switch is not a
problem of ‘distal attribution’, but corresponds to an alteration in the sensori-motor
dynamics, the passage from a dynamics of the constitution of a perception of
contact to a speciﬁc dynamics of the constitution of objects at a distance in space.
In order to discuss the relevance of these two approaches, and thus to clarify the
status of action and the way in which the use of a tool can lead to the perception of
objects in a distal space, we have employed a minimalist method.
2.2
A Minimalist Method
Our minimalist method consists of using the simplest possible perceptual device, in
which the repertoires of action and the sensory feedbacks are drastically reduced to
a bare minimum. The ﬁrst point was to verify that the phenomenon of distal
perception still occurs in these impoverished circumstances. This makes it possible
to control quite precisely what are the objects that can be constituted in each case,
and what are the operations that are necessary for this constitution. We have thus
reduced the system of Bach y Rita to a single photo-electric cell connected to a
single all-or-nothing tactile stimulator. When the total luminosity in the incident
light ﬁeld (a cone of about 20°) is greater than a certain threshold, the tactile
stimulus is triggered (Fig. 2).
At each moment in time, the subject (who is blind or blindfolded) thus receives
only minimal information, 1 bit corresponding to the presence or absence of the
tactile stimulus. We have been able to show that even with such a simple device, the
spatial location of luminous targets was still possible [28]. Initially, the subject only
perceives a succession of tactile stimuli which accompany her movements. But
quite soon, as she becomes familiar with the device and starts to master it, she no
longer notices these sensations which are replaced by the perception of a target at a
certain distance in front of her. Here, it is quite clear that perception cannot be
index finder
photo-sensitive cell 
tactile stimulator
target
Fig. 2 The minimalist experimental device for spatial localization. The photo-sensitive cell is
ﬁxed on the index ﬁnger. When the amount of light received is above a certain threshold, the cell
activates a tactile stimulator (that can be held in the other free hand)
58
C. Lenay and G. Declerck

grounded merely on an internal analysis of the input data (namely, the tactile input);
the latter is simply a temporal sequence of all-or-nothing 1’s and 0’s which has
nothing intrinsically spatial about it (contrary to the two-dimensional organization
of the stimulators in the TVSS matrix).
It is quite understandable that it is possible to locate the target, even if the
movements of the subject are simpliﬁed, and reduced to movements of the arm
around the shoulder articulation, and movements of the hand around the wrist
articulation. In Fig. 3, we consider only movements in a horizontal plane
(three-dimensional space can be recovered by integrating up-and-down movements
in the vertical plane). The situation is represented in (x, y) coordinates, with the
subject place at the origin (0,0). The target is a point source, S, situated at a distance
L from the subject with coordinates (0, L). The point P designates the wrist of the
subject; its coordinates are (b.cosa, b.sina), where b is the length of the arm, and the
angle a = (Ox, OP) indicates the orientation of the arm. The angle at the wrist,
between the arm and the hand, is designated by b = (PO, PS).
We may suppose that the subject is oriented with her chest facing the target, and
ﬁnding a tactile stimulation with the arm point straight forwards and the ﬁnger
aligned with the arm (a = 90°, b = 180°). From a strictly mathematical point of
view, a single pair of additional values (a, b) is then sufﬁcient to determine the
distance L. As shown in Fig. 3, L is given by a simple trigonometrical formula, if
we consider that b, the length of the arm, is known.
In order to account for the perception of the position of the target, it therefore
seems plausible to suppose that the subject relies essentially on the proprioceptive
data concerning the positions of her limbs to construct an internal representation of
the relation between the position of the captor at the end of her ﬁnger and the
position of the target (an operation of triangulation [12]). The actions would serve
only to establish this relation by allowing for an exploration of the scene (move-
ments which in addition facilitate a more precise proprioception). In this case, the
Fig. 3 The arm (forearm
included) has a length b. The
distance to the target L (0S) is
given by the trigonometric
formula: L = b (sin a −cos a
tan(a + b)) (Eq. 1)
Technologies to Access Space Without Vision …
59

calculation of the position of the target would only require pointing at it a few
times, several pairs (a, b) to inscribe the object in a space of internal representation.
However, observation shows that in order to maintain the perception of a target
placed in front of her, the subject must act continually, moving the photo-electric cell
so as to aim at the target in different ways. This can easily be understood if we look at
things from the phenomenological point of view of the subject. As soon as the
movements stop, the perception disappears. If she is immobile, there are only two
possibilities: either she receives a continuous stimulus, or she does not. If she is
pointing away from the target, she has only the memory of a perception which fades
away. If she is pointing at the target, she receives a continuous sensory stimulation—
but this does not give rise to the perception of an external object. On the contrary,
consciousness is now ﬁlled by the presence of the tactile sensation as such.
If we wish to explain the necessity for this incessant activity in a representa-
tionalist approach, one might try to explain it by the need to constantly reactivate
the sensitivity of the sensory cells which otherwise (by virtue of rapid adaptation)
might cease to send signals to the central nervous system. However, the phe-
nomenological description shows that if perception ceases when the concrete
activity of moving the hand stops, it is not because the sensory data evaporate, but
on the contrary because the tactile stimulation as such becomes only too present,
thrusting the perception back on the place where it is directly felt. Here, there is not
distal perception without action because spatial perception requires a constant
synthesis of a temporal succession of actions and sensations.
If one admits this essential role played by action in the emergence of perception,
then it must be admitted that what is perceived, recognized, is not really the
invariants extracted from the sensations, by rather the invariants of sensori-motor
loops related to the activity of the subject. This involves abandoning the passive
conception of perception for which the system would receive as input certain
information and then carry out a calculation to identify the objects and events
before going on the produce representations in an internal space. On the contrary,
perception is accomplished by mastery of sensori-motor regularities. The theoretical
framework of active perception has been variously developed in the ecological
approach to perception [19, 20, 46], and in sensori-motor or enactive approaches
[11, 36, 37, 39, 41, 47]. By her action the subject seeks and masters the constant
rules which relate action and sensation. The perception of an object consists in the
discovery of regularities in the relation between variations of action (mobility of the
organ of perception) and variations in sensations (produced by these actions), it is
what Kevin O’Regan calls a ‘law of sensori-motor contingency’ [37]. The richness
of perception should thus depend at least as much on the capacities for action
(mobility, rapidity, zoom, etc.) as on the variety of sensory inputs (width of the
spectrum, number of sensors, etc.).
As we have seen, in our minimalist experiment at least, there is no perception
without action. One observes that the subjects perform regular oscillations around
the target: generally small oscillations of the hand (b), accompanied by larger
movements of the arm (a) which cause progressive changes in the position of the
60
C. Lenay and G. Declerck

wrist. It is as though the subjects seek to identify and verify the functional rela-
tionship between a and b which must be respected in order to obtain a sensory
feedback. There is perception of the position of the target when the subject masters
the sensori-motor rule allowing him to aim at the target from different positions of
the captor in space. The spatial exteriority of the target is constituted by the pos-
sibility of freely and reversibly coming and going around it, alternately leaving and
reﬁnding contact with it. The target is localized in direction and depth when the rule
governing pointing towards it is mastered. This is a good illustration of a ‘law of
sensori-motor contingency’ [37]. Any given position of the target corresponds to a
particular sensori-motor invariant, i.e. a rule relating sensory feedback to the
actions performed; this rule itself is stable over and above the constantly varying
actions and sensations. The whole interest of the minimalist approach is then to
permit a precise characterization of this rule (on condition of simplifying the space
of action to reduce it to two rotations, here the rotation of the arm around the
shoulder and the hand around the wrist) (Fig. 4).
With this in place, the different conceptions of prosthetic perception (and indeed
of perception in general) will depend on the way that these rules (the laws of
sensori-motor contingency) are envisaged:
– either they are internal (inscribed in the central nervous system) and abstract
(represented in symbolic fashion)—this is the representationalist view;
– or else they are external (determined by the objective structure of the layout) and
embodied (directly related to proprioceptive information)—this is the enactivist
view.
This question is particularly important for understanding the processes of the
appropriation of devices for perceptual assistance.
3
Sensori-Motor Rules
In the ﬁrst sort of approach, which is internalist, one might think that these rules
correspond to an internal know-how, inscribed in the nervous system, constructed
by association between the input sensations and information about the actions
performed (this latter information could be based on proprioceptive signals and/or
efferent copies of motor commands). However, a repetition of our experiments by
Siegle and Warren [43] spreads fresh light on this question.
On the occasion of the same task of spatial localization, they separated the
blindfolded subjects into two groups. In the ﬁrst group, labelled ‘Proximal attention’,
participants were explicitly instructed to attend to the location of the arm when the
vibrating motor was active, and to consciously triangulate the location of the target
by imagining their ﬁnger extending out into space. By contrast, in the second group
labelled ‘Distal attention’, participants were explicitly told to not attend to their arms
during the experiment, but to get an intuitive sense of the target’s location and report
how far away it felt. It was then observed that the instructions to attend to distal
Technologies to Access Space Without Vision …
61

properties during learning resulted in improved performance and more precise
judgments of target distance, whereas participants instructed to attend to proximal
variables showed no improvement. In addition, Siegle and Warren [43] observed
that the improved distance judgments were signiﬁcantly correlated with increased
perception of a solid object; that is, the less participants were paying attention to their
arms, the lower the error in distance judgments, the more they felt a concrete solid
object was really present before them in space. This supports the claim ‘that
improved distance judgments reﬂect a distal perceptual awareness rather than an
explicit cognitive strategy’ [43: 220]. Moreover this improvement is conserved even
if one turns the seat on which the subject is placed through 90°, or one switches the
photodiode from the dominant to the nondominant hand. Even if the actions to be
performed, and thus the proprioceptive data are quite different, the subjects still
appeared able to mobilize the results of their learning to localize the target. This is
reminiscent of an observation already made by Paul Bach y Rita: subjects who were
well trained with the TVSS held in the hand and a matrix of tactile stimulators placed
on the back were quite able, without any additional training, to use miniature camera
place on a pair of spectacles and a matrix of tactile stimulators placed on their chest
[8].
These results militate both against an internalist representationalist account
which supposes that distal spatial perceptions are deduced from proprioceptive data,
and against an associative internalist account consisting of directly extracting
regularities from the correlations between sensations and the movements per-
formed. As Siegle and Warren [43] rightly note: ‘transfer to the opposite arm
changed not only the joint angles but the joints and muscles involved […]. These
data clearly indicate that the emergence of distal awareness [cannot] depend on a
particular set of arm conﬁgurations at a muscle- and joint-speciﬁc level’ and so
‘undermine a muscle- or joint-speciﬁc version of the sensori-motor hypothesis, and
indeed any other hypothesis deﬁned at the level of limb conﬁgurations’.
Apparently, the only possible way to account for this data in the frame of the
Fig. 4 Curves representing
the relation between the angle
of the hand with respect to the
wrist (b), and the angle of the
arm with respect to the
shoulder (a), when the target
is at different distances from
the subject. These distance
(L) are expressed in units of
the length of the arm (b)
62
C. Lenay and G. Declerck

sensori-motor account is to postulate ‘a higher level sensori-motor contingency that
is not joint-speciﬁc’.
In view of this, the question rebounds. What is the general structure of spatial
perception that makes it possible to deduce in each situation the sensori-motor rule
that applies? In the externalist perspective adopted by Siegle and Warren, which is
close to the ecological theory of perception, it is posited that the gestures of
pointing are determined in an allocentric reference frame, let us say the structure of
the physical space of the layout. However, it is necessary to account for the
transformations of lived experience that occur during the appropriation of technical
devices for assisted perception. The sensori-motor rules depend as much on our
possibilities for acting and feeling as on the concrete situation in which one is
engaged [29]. It would rather seem, then, that perceptual learning leads to the
discovery of the general organisation of the coupling between the subject and her
environment, an organisation which makes it possible in each instance to master the
rules of pointing towards objects.3
It therefore seems appropriate to adopt an externalist approach that we may call
’enactive’ in the sense that spatial perception is constituted in the sensori-motor
relationship itself: it neither precedes nor follows this relation [31]. In order for the
perceptual experience at time t to have a spatial content (i.e. for it to present objects
located in an external space), the subject must indeed be actively engaged in this
relation. The laws of sensori-motor contingency (relating the movement of the
captor to the sensory input) are thus deﬁned by the action capacities of the organism
and the way in which these actions, depending on the structure of the environment,
determine the sensory input. The lived body, transformed by the technical device
which is associated with it, brings forth a speciﬁc domain of coupling with the
environment. The laws of sensori-motor contingency are deﬁned by the coupling
device, the position of the target in the environment, and the action strategies of the
subject. In order to give rise to a perception, a prosthetic device must be an
instrument of coupling which modiﬁes the lived body by deﬁning new repertoires
of action and sensation. The interest of this approach is that the organism itself does
not need to have any explicit knowledge of the coupling device with which it is
associated (the position and mode of action of the captors and sensory stimulators).
It is sufﬁcient for the subject to engage in a relation with the world, to progressively
acquire a perceptual mastery [37]. This conception of active perception can be
schematically illustrated as follows: (Fig. 5).
On this view, perception is not an internal representation, but the result of
dynamic coupling between the organism and its environment. This is why we
situate perception at the heart of the coupling, and not unilaterally within the
3Incidentally, the concept of “pointing” used by Siegle and Warren actually presupposes already a
spatial framework (if the subject thinks of her action in terms of the gesture of pointing in this or
that direction, this means that she already has the experience of a space). This being so, the process
whereby this framework is set up is precisely what we are trying to understand here: what is the
process of setting up this framework which subsequently makes it possible to interpret gestures as
gestures of pointing?
Technologies to Access Space Without Vision …
63

organism. It is easy to understand that in this conception of perception, there is an
important distinction to be made between ‘sensory input’ and ‘perception’. The
‘sensory input’ delivered to the organism is quite different from the full ‘percep-
tion’, which is based on the law deﬁning the sensory feedback for a full range of
performed actions.
4
The Space of Possibilities
If we come back to the general conception of space itself as a system of positions
and possible movements, we see that the use of the device has transformed this
system of possibilities. Of course blindfolded subjects, like persons who are blind
from birth or through injury, do already have knowledge of the space which sur-
rounds them and the actions which they can perform, even if it is only through the
world of sound or the space of bodily action with tactile and kinaesthetic feedback.
Nevertheless, with the radically novel mediation provided by the technical device a
new space of possibilities is opened up, with a new form of perceptual presence of
objects, this time in exteriority at a distance in a distal space.4
The preceding analysis now makes it possible to specify the way in which the
space of possibilities is reconﬁgured on the occasion of this proximal-distal switch.
sensory feedback 
Perception
Environment  
Causality
Lived body
Strategies
AcƟons 
Fig. 5 Scheme of sensori-motor coupling. The system of prosthetic perception is a ‘coupling
device’ which modiﬁes the lived body by deﬁning the repertoires of actions and sensations which
are available to the subject. Via the environment, the actions ‘a’ give rise to sensory feedback ‘s’:
s = g(a); concomitantly, the organism implements a strategy for generating its actions and
modulating them as a function of its sensory feedback: a = f(s)
4Epstein et al. (1986) have studied, in very controlled conditions, the question of the awareness of
the existence of an external space through the use of a sensory substitution device—a question we
considered again in Auvray et al. [1].
64
C. Lenay and G. Declerck

In the beginning, for the subject, blind or blindfolded, the ﬁeld of possible
perception (i.e. everything that the subject can apprehend by anticipation—even-
tually implicitly—as possibly being perceived by him) corresponds to the set of
possible movements of the captor (here placed at the end of the ﬁnger) which
explores the environment. In certain positions, the subject receives a tactile feed-
back that he can try to ﬁnd again (this is not very difﬁcult because the environment
is immobile). But once the device is mastered, this ﬁeld of possibilities is trans-
formed and becomes the space of possible distal positions of the object. As for
visual perception, the point of view, the position from which the object is perceived,
is then spatially distinct from the distal position of the object.
The result of the instruction of distal attention in the experiment of Siegle and
Warren seems to show that distal perception only occurs if the particular local
actions are forgotten, relegated to oblivion, in favour of mastering the law of
pointing towards the object. When attention is focused on the perception of an
object, the stimulations delivered by the coupling device (be it natural or artiﬁcial)
disappear from consciousness. Similarly, in natural vision, when we perceive a
stable object at a certain distance in front of us, using our eyes and their movements,
we have absolutely no consciousness either of the saccadic movements of our eyes,
or of the variable sensory stimulations at the level of the retina [14]. What we are
perceptually aware of is where the object is relative to us and where we are, as a
point of view, relative to the object. Similarly here, there is only distal perception
when one perceives the position of the object without paying attention either to the
tactile stimulations, or to the variations in viewpoint which make it possible to
determine this position.
In the case of proximal perception (we are considering here the case of tactile
stimulation), there is spatial coincidence between the position of the perceived
stimulation and the bodily position where the stimulation is received: for an ‘object’
occupying a given position, there is only one possible position of the captor, i.e. the
position where the ‘contact’ occurs. By contrast, in the case of distal perception (in
natural conditions of a visual or auditory type) there are an inﬁnite number of
possible positions of the captor for each position of the object (an object occupying
a certain spatial position can be perceived from an inﬁnite number of positions and
orientations of the captor).
The nature of the switch from proximal to distal can thus be understood as a
transformation of the sort of rule which relates the action to the sensory feedback.
Whereas for proximal perception, the rule is that of identity between the position of
the captor and that of the object, for distal perception this rule associates an inﬁnite
number of positions of the captor to each position of the object.5 The duality
between the particular fact and the general rule corresponds to the duality between
5The notion of “distal” implies at the same time the idea of “aspect” and of “perspective” on the
object: the distal perception of an object, precisely because of the possibility of having access to
the latter from an inﬁnity of possible positions, is the perception of the object “under a given
aspect”: the aspect that the object presents as “seen from here”.
Technologies to Access Space Without Vision …
65

the position of the point of view and the position of the object. This duality is
constitutive of distality.
The space of possible positions in play in distal perception is thus radically
different from the corresponding space in the case of proximal perception. In the
space of proximal perception, the relative positions of the perceived object and of
the captor are of the same nature, and can be deﬁned in the bodily framework of the
perceiving subject. By contrast, in distal perception the relative positions of the
perceived object and the captor are speciﬁed by rules of pointing (rules which
indicate, for a given position of the captor, in which direction the latter should be
oriented in order to establish a ‘distal contact’ with the object), and are thus of a
different nature than the particular bodily positions which can satisfy these rules.
They take the status of possible ‘points of view’, i.e. the site from which the object
is perceived. Each position of the object in the distal space is speciﬁed by the rule
which speciﬁes the set of all possible viewpoints on the object. But at the same
time, each of the positions of the viewpoint, rather than being deﬁned within the
bodily system, corresponds itself to a position in this distal space, and in principle
could thus be speciﬁed by the rule giving the set of all possible viewpoints on it.
The space of distal perception thus becomes a space of possible viewpoints.
5
Shape Recognition
Beyond simple spatial localization, we have seen that a device such as the TVSS
can also give rise to the recognition of shapes of varying degrees of complexity.
Here again, it is possible to adopt a minimalist approach in order to analyze with
precision the mechanisms which make this sort of performance possible. We have
developed another system with the aim of providing blind persons with access to
digital forms present on a computer screen. The ‘Tactos’ system [26] consists
essentially of a device for controlling tactile stimulators (Braille cells which elec-
tronically generate the movements of small pegs) as a function of the movements of
a cursor on a computer screen (Fig. 6).
Typically, the cursor is a 4  4 matrix of 16 receptor ﬁelds (two pixels wide),
corresponding to a surface of 64 pixels on the screen. When one of the receptor
ﬁelds encounters at least one black pixel, this triggers the all-or-none activation of
the corresponding peg on the Braille cell. The subject is blindfolded, and moves the
cursor by means of an effector (mouse, stylus on a graphic tablet, touchpad….). The
tactile stimulation is delivered to the other, free hand—but, as we will see, this does
not hamper the perception of the forms. This device of perceptual supplementation
thus allows for the exploration of a virtual tactile image.
For practical applications, it is possible to increase further the number of receptor
ﬁelds and the corresponding tactile stimulators; but from the point of view of
fundamental research it is actually more interesting to reduce the sensory infor-
mation to the limiting case of a single stimulator corresponding to a single receptor
ﬁeld [50]. Even in this minimal version, we observe that subjects are able to
66
C. Lenay and G. Declerck

perceive forms [26]. These forms are not given to the sensory system as a complete
two-dimensional pattern applied to the skin at each instant of time. When there is
only a single receptor ﬁeld, and thus a single sensation at each instant, there is again
no intrinsic spatiality at the level of the input signal. If the subjects succeed in
recognizing shapes in space—and they do—this can only be by virtue of an active
exploration in the course of which they integrate their movements and the corre-
sponding sensory feedbacks over time. Thus, by limiting the sensory input to just a
single bit of information at each instant, we oblige the subjects to deploy their
perceptual activity in space and time; and in this virtual reality situation it is then a
simple matter to record and to analyze this activity. This is what we have called
‘perceptual trajectories’ (see Fig. 7) [30].
Here again and internalist perspective, in terms of the construction of a mental
image by proprioceptive integration of the points of contact with the form, is quite
impossible. The proprioceptive perception and memory of absolute position is too
imprecise for the subject to be able to plot the positions of the hand which holds the
effector (mouse, stylus…) in egocentric X-Y coordinates. It is thus quite impossible
for the subject to scan the whole ﬁeld of the screen, and to integrate the points of
stimulation in order to construct a mental image of the form. In fact, if the subject
inadvertently leaves the contour of the form she is immediately ‘lost’, and cannot
even proprioceptively return to the last point of contact with the form.
Observation of the perceptual activity reveals some behavioural regularities. The
subject starts out with large-scale exploratory movements, but as soon as she
obtains a contact with a line, she converges to a micro-sweeping movement of small
amplitude around the source of stimulation. This can be understood as essentially
Fig. 6 Tactos Device. The pen on the tablet controls the movements of the cursor on the screen.
The cursor corresponds here to a 3  3 matrix of receptor ﬁelds. When a receptor ﬁeld covers a
black pixel on the screen, the corresponding peg of an electronic Braille cell is activated. The
picture on the right shows the two piezo-electric Braille cells combined to form a 4  4 matrix of
16 pegs
Technologies to Access Space Without Vision …
67

an operation of localization: the position of an immobile spatial singularity is
constituted by a stable anticipation of the tactile stimulus according to the move-
ments of the receptor ﬁeld. The fact is that the imprecision and drift of the pro-
prioceptive data do not allow their employment for a representation as an internal
reconstruction of the form being explored (on the contrary, the proprioceptive
system must be continually recalibrated by interactions with the environment). To
the extent that there is no direct localization by proprioception in the absolute space
of the graphic tablet, we have to admit that the subject’s knowledge of her own
position is indirect. The micro-sweeping movement enables the subject to identify
her own position, not in absolute coordinates but relative to the form that she is
exploring and perceiving. The subject situates herself in the allocentric reference
frame of the ﬁgure she is actively exploring and perceiving: ‘I am just a little to the
right of the shape, that I just crossed, now I have come back to the left and I am
pursuing it’, etc. There is thus concomitantly localization of the singularity by
oscillating movements, and localization of the movements of the receptor ﬁeld
relative to this singularity. At each instant, the subject situates herself relative to the
shape that she is in the course of constituting. The ‘viewpoint’, i.e. the site from
which the object is perceived, is not the ﬁnger under which the tactile stimulations
are delivered, but the receptor ﬁeld, because it is on the basis of this site that I deﬁne
my actions. The place of perception (the cursor) corresponds to the point of action
which is situated in the same space as the shape that is perceived.
At a higher level of organisation, the micro-sweeping movements around an
initial point of contact are combined with a tangential displacement, following the
local direction of the segment of the shape. This contour following is the realization
of a second-order anticipation which bets on the stability of a temporal frequency of
the sensations. However, if this strategy makes it possible to recognize straight or
curved segments, it is not yet the recognition of more complex shapes such as
letters. The latter only seems to be achieved when the subject is able to combine the
gesture of micro-sweeping with a dynamic sequence of different segments which
taken together reproduce the shape of the whole. When this is achieved, the receptor
ﬁeld traces the whole shape, with small oscillations and never losing contact.
Fig. 7 Two examples of perceptual trajectories in Tactos
68
C. Lenay and G. Declerck

We can appreciate here that perception is not the reception (and then the rep-
resentation) of a shape, but rather its active construction. The trajectory is at one
and the same time a recognition and a constitution of the shape. The categorization
of perceptual data by integrating them into a known shape is realized by a gesture of
synthesis. This gesture is like a scheme of construction of the shape, whereby the
categories of the understanding apply to the data of sensorial intuition [27, 41].
Here the scheme of ‘assimilation’ corresponds to a concrete activity, deployed in
the space of the movements of the subject. It is achieved by a ‘gestural strategy’
which produces, via the exteroceptive and proprioceptive sensory returns, a set of
chained movements which make it possible at one and the same time to write the
shape and to grasp it as a whole, in a single gesture of anticipation. Here, we can
truly say that ‘reading is writing’. This is indeed exactly what we do when we ask
the subject to validate the perceived shape by drawing it free hand; in other words,
to reproduce the gesture which directed her exploration. By studying the dynamics
of the perceptual trajectories, we thus observe the concrete activity of the consti-
tution of a shape in perception. We could hardly ask for a more telling example of
perception as enaction—the bringing-forth of the perceived object—and not as
‘representation’.
The perceptual modality instantiated by the Tactos system is indeed ‘tactile’—
for the reason that there is spatial coincidence between the receptor ﬁelds and the
perceived shape. It is to be noted that this is independent of the fact that the sensory
modality that is used is also that of touch. Indeed, in the case where there is only a
single receptor ﬁeld, it is quite possible to substitute the sensory return by a sound,
or even by a ﬂash of light. The perceptual activity would be exactly the same [18].
Thus, the perspective of perceptual supplementation leads us to put into question
the classical deﬁnitions whereby the various perceptual modalities are deﬁned
solely by the sensory organ involved.
By reducing the sensory input to a single receptor ﬁeld (just one bit of infor-
mation at each instant), we have forced a spatial and temporal deployment of the
perceptual activity, which has the advantage of facilitating its analysis. The tool
functions here as a system for extracting operations which are habitually realized in
the intimacy of the organism. Of course for practical applications there are
advantages to restoring a degree of parallelism (with Tactos we routinely use a
4  4 matrix of 16 receptor ﬁelds, and in the initial version of the TVSS there were
400 receptor ﬁelds). When this is done, we observe an internalization of the per-
ceptual activity: the economy of movement and memory allows for perception
which is more rapid and more precise. What is to be noted, however, is that the
parallelism of the captors is formally equivalent to a movement already performed
between the diverse positions of the receptor ﬁelds [17, 44, 45]. This is similar to
the situation of binocular vision: mobilizing two eyes and extracting the distance of
the perceived object by their convergence is formally equivalent to using just one
eye and a slight displacement of the head. Indeed this is what we do spontaneously
to evaluate the respective positions of two objects by parallax, and this operation is
equivalent to the triangulation that we studied in the previous section.
Technologies to Access Space Without Vision …
69

6
Technical Mediations of Perceptual Activity
We consider that the concepts developed above in the case of devices that are
deliberately limited and simpliﬁed to the extreme can be generalized to the most
varied systems of perceptual aids, in particular for better understanding the
mechanisms of their appropriation.
6.1
Sensory Modalities and Perceptual Modalities
Following the analyses we have just proposed concerning the transformation of
perceptual activity using technical devices, it appears that it is important to dis-
tinguish on one hand the ‘sensation’ (i.e. the sensory input delivered to the
organism), and on the other hand the ‘perception’ which is speciﬁed by the rule
which deﬁnes, for a given perceptual content, the sensory returns as a function of
the actions performed. In completely analogous fashion, in the case of the use of a
device for perceptual supplementation, it is important to distinguish on one hand the
‘sensory modality’ being used, which corresponds to the type of sensory input to
the central nervous system; and on the other hand, the ‘perceptual modality’ which
is deﬁned by the sort of sensori-motor contingency law that the device gives access
to. For example, for ‘The Voice’ [34] and the TVSS the sensory modalities that are
mobilized are different, respectively auditory and tactile. Nevertheless, for The
Voice as for the TVSS, one can say that the perceptual modality is basically of a
visual type, since both these systems give access to the position and the shape of
distant objects (by exploratory actions of translation and rotation). As noted by
Grice [23], four criteria are generally used to distinguish between perceptual
modalities: sensory organ, nature of the physical stimuli, properties being accessed,
qualitative experience (see especially Auvray and Myin [3] for a discussion of the
nature of the sensory-substituted experience in light of Grice’s analysis). O’Regan
and Noë [37] add the criteria of sensori-motor equivalence, which refers to the type
of sensory changes a given type of action produces. To this list, we propose to add
the criterion of proximal-distal organization, which refers to the type of spatial and
functional relations between the perceptual ‘point of view’ and the perceived object.
The criterion of proximal-distal organization is of the same kind as the criteria of
sensori-motor equivalence, but is focused on a different functional level and aims to
account for phenomenological differences, more precisely differences in the spatial
content of perceptual experience: how one situates oneself in space relative to the
object being accessed perceptually, where one ‘feels’ oneself to be. Its leading
principle concerns the functional difference between proximal and distal perceptual
awareness: while proximal perception is characterized by the spatial coincidence
between one’s ‘point of view’ and the perceived object (in order to perceive the
object, one cannot do without being in contact with it), distal perception is char-
acterized by a spatial noncoincidence between one’s ‘point of view’ and the
70
C. Lenay and G. Declerck

perceived object: in order to perceive the object, one cannot avoid occupying a
different position than the object itself. One noteworthy point is that whereas per-
ceptual modalities can generally be classiﬁed as pertaining to one type or the other
(distal or proximal), the sensory organ they normally make use of (eyes for vision,
ears for audition, skin for touch, etc.) cannot: as demonstrated in an exemplary way
by the TVSS case, the skin can be used to enact a distal-type perceptual awareness;
conversely, one can make use of the eyes and ears to enact a proximal-type per-
ceptual awareness.
The criterion of proximal-distal organization is especially useful to account for
the technical diversity of prosthetic devices. For example, a perceptual modality of
the visual type would be deﬁned in purely functional terms as a situation where the
point of perception (the point of view) is separate and at a distance from the
perceived object. This would involve captors which specify inﬁnite receptor ﬁelds,
such that their movements are rotations and translations in the three dimensions of
space. By contrast, ‘touch’ could be deﬁned by the spatial coincidence of the site of
perception (the receptor ﬁelds) and the perceived object. This would involve
receptor ﬁelds with a ﬁnite dimension, and whose movements would be translation
with respect to the object.
6.2
The Tool ‘in Hand’
Whatever the system of perceptual aid, it is—like any tool which can be taken up in
the hand—a device for artiﬁcial coupling between the organism and the environ-
ment to which it gives access. The new link which it creates, between the actions
and the sensory returns delivered to the user, gives rise to the constitution of
speciﬁc percepts. We ﬁnd a common principle of functioning with the TVSS, the
use of a computer mouse, games in virtual digital spaces, systems of tele-presence
or virtual reality… In the case of the TVSS, the coupling between the actions
(movements of the camera) and sensations (tactile stimuli) passes through the
physical environment. By contrast, in the case of a computer mouse, the coupling
between the actions (movements of the mouse) and sensory feedback (movements
of the cursor on the computer screen) passes by a digital calculation. But in both
cases, once the tool has been grasped and mastered, the tool itself disappears from
consciousness in favour of the space of perception and action that it gives access to.
The tactile stimuli on the skin and the camera in the hand are both forgotten in
favour of the perception of an object ‘out there’ in a distal space; the computer
screen and the movements of the mouse are forgotten in favour of the perception of
the cursor and the operations that it makes it possible to perform in the digital space.
In these two examples, the technical mediation highlights features that are
actually quite general in the use of tools of all sorts. When I grasp a stick in order to
explore the surface of the ground, it is not the stick that I perceive as an object, but
the bumps on the ground at the end of the stick. This has been well described by
phenomenology: ‘The stick of the blind person has ceased to be an object for him, it
Technologies to Access Space Without Vision …
71

is no longer perceived as such, the end of the stick has been transformed into a
sensitive zone, it augments the range and the scope of action of touch, it has become
analogous to vision’ [35]. In a similar vein, when I drive a car, I forget for the
moment the vibrations of the steering-wheel and the seat, and instead I have the
impression that I feel the gravel or the edge of the pavement under ‘my wheels’.
These examples can be generalized to all the technical ‘appendices’ which trans-
form our power of action.
The successful appropriation of a technical device occurs when the user ‘be-
comes as one’ with it. The device becomes invisible in the same way that our own
body is invisible for use: we see neither our eyes, nor our optic nerves, nor our
spectacles (if they are not too dirty!). One observes the same sort of result with
other devices of sensory substitution such as the visual-to-auditory substitution
systems the vOICe system [2, 3, 34] or the VIBE [25]. A coupling device which is
properly appropriated becomes invisible precisely because it enables the subject to
see. The invisibility of the lived body and of a tool that is grasped ‘in hand’ is
explained by their constitutive role in perception. Participating in the constitution of
the perceived object, they are no longer themselves the object of conscious expe-
rience. However, like all tools, they can also be ‘put down’, separated from the
body and hence become again objects of perception. This reversible passage
between the ‘in hand’ mode and the ‘put-down’ mode can of course be more or less
convenient and rapid according to the device in question, from an implant to a pair
of spectacles. In the case of the TVSS this passage can be rapid, the device being
alternately considered as a perceived object (the blind person pays attention to the
irritation produced by the tactile matrix on his skin) or invisible (when the blind
person pays attention to the distal objects that the device enables him to constitute).
Indeed it is often in this very interplay structured by the reversibility that a technical
object takes on its meaning as such [32].
7
Conclusion: Perceptual Supplementation
In the light of these considerations, it seems that the denomination of systems of
‘sensory substitution’ to designate devices such as the TVSS of Paul Bach y Rita or
the Voice of Peter Meijer is awkward and lacks generality. On one hand, we have
seen that the conception and design of a system of perceptual aid should not only
accomplish a transfer between different sensory modalities, but should also take
into account the modes of action that it permits. If there is a ‘substitution’, it would
be better to talk of ‘sensori-motor’ and not only ‘sensory’, because the relevant
lived experience is not restricted to an analysis of the received sensation, but is
rather produced by the complete dynamics of the sensori-motor coupling.
But on the other hand, is it even appropriate to talk of ‘substitution’? Whatever
the outcome of the debates about the nature of the perceptual experience procured
by these devices, it must be recognized and admitted that it does not replace that of
the absent modality; rather, it offers an original perceptual experience, speciﬁc to
72
C. Lenay and G. Declerck

the repertoires of action and sensory return that are proposed by the device. In order
to designate the whole set of devices which modify, enrich or transform perceptual
activity, we propose to speak of Perceptual supplementation rather than ‘sensory
substitution’. Indeed, the term supplementation has the merit of expressing at one
and the same time the act of compensating for a deﬁciency, and the act of positively
expanding or increasing a capacity.
References
1. Auvray M, Hanneton S, Lenay C, Kevin O’Regan (2005) There is something out there: distal
attribution in sensory substitution, twenty years later. J Integr Neurosci 4(04):505–521
2. Auvray M, Hanneton S, O’Regan JK (2007) Learning to perceive with a visuo-auditory
substitution system: localisation and object recognition with ‘the voice’. Perception 36
(3):416–430
3. Auvray M, Myin E (2009) Perception with compensatory devices: from sensory substitution
to sensorimotor extension. Cogn Sci 33(6):1036–1058
4. Bach-y Rita P, Collins CC, Saunders F, White B, Scadden L (1969) Vision substitution by
tactile image projection. Nature 221:963–964
5. Bach-y-Rita P (1972) Brain mechanisms in sensory substitution. Academic Press, New York
6. Bach-y-Rita P, Kercel SW (2003) Sensory substitution and the human-machine interface.
Trends Cogn Neurosci 7(12):541–546
7. Bach-y-Rita P, Kaczmarek KA, Tyler ME (2003) A tongue-based tactile display for portrayal
of environmental characteristics. Virtual and adaptive environments, 169–186
8. Bach-y-Rita P (1984) The relationship between motor processes and cognition in tactile vision
substitution. In: Cognition and motor processes. Springer, pp 149–160
9. Bach-y-Rita P (2004) Tactile sensory substitution studies. Ann New York Acad Sci
1013:83–91
10. Briscoe, R (2017) Bodily action and distal attribution in sensory substitution. In:
Macpherson F (ed) Sensory substitution and augmentation. Proceedings of the British
Academy (forthcoming) Consulté le 15 mars 2016. http://philpapers.org/rec/BRIBAA
11. Brooks R (1999) Cambrian intelligence. MIT Press, The Early History of the New AI
12. Cabe PA, Wright CD, Wright MA (2002) Descartes’s blind man revisited: bimanual
triangulation of distance using static hand-held rods. Am J Psychol 116(1):71-98
13. Collins CC, Bach-y-Rita P (1973) Transmission of pictorial information through the skin.
Adv Biol Med Phys 14(1973):285–315
14. Ditchburn RW (1973) Eye-movements and visual perception. Clarendon, Oxford
15. Epstein W, Hughes B, Schneider S, Bach-y-Rita P (1986) Is there anything out there?: a study
of distal attribution in response to vibrotactile stimulation. Perception 15(3):275–84
16. Fodor JA, Pylyshyn ZW (1981) How direct is visual perception? Some reﬂections on
Gibson’s ‘ecological approach’. Cognition 9(2):139–196
17. Gapenne O, Lenay C, Stewart J, Bériot H, Meidine D (2001) Prosthetic device and 2D form
perception: the role of increasing degrees of parallelism. In: Proceedings of the conference on
assistive technology for vision and hearing impairment (CVHI’2001), pp 113–18
18. Gapenne O, Rovira K, Lenay C, Stewart J, Auvray M (2005) Is form perception necessary
tied to speciﬁc sensory feedback?. In: Proceedings, 16. Monterey, CA
19. Gibson JJ (1966) The senses considered as perceptual systems. Houghton Mifﬂin, Oxford,
p 1966
20. Gibson JJ (2014) The ecological approach to visual perception: classic edition. Psychology
Press
Technologies to Access Space Without Vision …
73

21. Gregory RL (1970) The intelligent eye. Weidenfeld and Nicolson, London
22. Gregory RL (1980) Perceptions as hypotheses. Philos Trans Roy Soc Lond B Biol Sci 290
(1038):181–197
23. Grice HP (1962) Some remarks about the senses. In: Butler RJ (ed) Analytical philosophy
(ﬁrst series). Basil Blackwell, Oxford, pp 248–268
24. Guarniero G (1977) Tactile vision: a personal view. J Vis Impairment Blindness
71(3):125–130
25. Hanneton S, Auvray M, Durette B (2010) The vibe: a versatile vision-to-audition sensory
substitution device. Appl Bion Biomech 7(4):269–276
26. Hanneton S, Gapenne O, Genouel C, Lenay C, Marque C (1999) Dynamics of shape
recognition through a minimal visuo-tactile sensory substitution interface. In: Third
international conference on cognitive and neural systems
27. Kant I (1781) Critique of pure reason. Translated by P. Guyer & A.W. Wood. Cambridge
University Press, Cambridge, 1998
28. Lenay C, Canu S, Villon P (1997) Technology and perception: the contribution of sensory
substitution systems. Cognitive technology, Los Alamitos, CA. IEEE Computer Society,
USA, pp 44–53
29. Lenay C, Gapenne O, Hanneton S, Marque C, Genouëlle C (2003) Sensory substitution:
limits and perspectives. In: Hatwell Y, Streri A, Gentaz E (eds) Touching for knowing,
cognitive psychology of haptic manual perception. John Benjamins Publishing Company,
Amsterdam/Philadelphia, pp 275–92
30. Lenay C, Thouvenin I, Guénand A, Gapenne O, Stewart J, Maillet B (2007) Designing the
ground for pleasurable experience. In: Proceedings of the 2007 conference on Designing
pleasurable products and interfaces. ACM, New York, pp 35–58
31. Lenay C, Steiner P (2010) Beyond the internalism/externalism debate: the constitution of the
space of perception. Conscious Cogn 19(4):938–952
32. Lenay C (2012) Separability and technical constitution. Foundations of science (FOS) Special
issue opening up the in-between: interdisciplinary reﬂections on science, technology and
social change, vol 4, pp 379–84. doi:10.1007/s10699-011-9245-8
33. Loomis JM (1992) Distal attribution and presence. Presence Teleoperators Virtual Environ
1(1):113–119
34. Meijer PB (1992) An experimental system for auditory image representations. Biomedical
Eng IEEE Trans 39(2):112–121
35. Merleau-Ponty M (1945) Phénoménologie de la perception. Paris: Gallimard, Phenomenology
of Perception trans. by Colin Smith (New York: Humanities Press, and London: Routledge &
Kegan Paul, 1962); trans. revised by Forrest Williams (1981; reprinted, 2002); new trans. by
Donald A. Landes. Routledge, New York, 2012
36. Noë A (2004) Action in perception. MIT press, Cambridge
37. O’Regan JK, Noë A (2001) A sensorimotor account of vision and visual consciousness.
Behav Brain Sci 24(05):939–973
38. Pacherie E (1997) Du problème de Molyneux au problème de Bach-y-Rita. Perception et
intermodalite, approches actuelles du probleme de molyneux, pp 255–93
39. Paillard J (1971) Les déterminants moteurs de l’organisation de l’espace. Cahiers de
Psychologie 14(4):261–316
40. Philipona D, O’Regan JK, Nadal JP (2003) Is there something out there? Inferring space from
sensorimotor dependencies. Neural Comput 15(9):2029–2049
41. Piaget J (1936) The origins of intelligence in children. Traduit par Margaret Cook et W.
W. Norton. vol 8. 5. International Universities Press, New York
42. Segond H, Weiss D, Sampaio E (2005) Human spatial navigation via a visuo-tactile sensory
substitution system. Perception 34(10):1231–1249
43. Siegle JH, Warren WH (2010) Distal attribution and distance perception in sensory
substitution. Perception 39(2):208–223
44. Sribunruangrit N, Marque C, Lenay C, Gapenne O, Vanhoutte C (2002) Braille box: analysis
of the parallelism concept to access graphic information for blind people. In: Engineering in
74
C. Lenay and G. Declerck

medicine and biology, 2002. 24th annual conference and the annual fall meeting of the
biomedical engineering society EMBS/BMES conference, 2002. Proceedings of the second
joint, vol 3, pp 2424–25
45. Sribunruangrit N, Marque CK, Lenay C, Hanneton S, Gapenne O, Vanhoutte C (2004)
Speed-accuracy tradeoff during performance of a tracking task without visual feedback.
Neural Syst Rehabilitation Eng IEEE Trans on 12(1):131–139
46. Turvey MT, Shaw RE (1979) The primacy of perceiving: an ecological reformulation of
perception for understanding memory. In: Wilsson LG (ed) Perspectives on memory research.
Lawrence Erlbaum Associates, Hillsdale, N.J., pp 167–222
47. Varela FJ (1979) Principles of biological autonomy. The North Holland series in general
systems research 2. Elsevier North-Holland, Inc., New York, 1979
48. Wall SA, Brewster S (2006) Sensory substitution using tactile pin arrays: human factors,
technology and applications. Sig Process 86(12):3674–95
49. White BW, Saunders FA, Scadden L, Bach-Y-Rita P, Collins CC (1970) Seeing with the skin.
Percept Psychophys 7(1):23–27
50. Ziat M, Lenay C, Gapenne O, Stewart J, Ammar AA, Aubert D (2007) Perceptive
supplementation for an access to graphical interfaces. In: Universal access in human computer
interaction. Coping with diversity. Springer, New York, pp 841–50
Technologies to Access Space Without Vision …
75

Mobility Technologies for Visually
Impaired People Through the Prism
of Classic Theories of Perception
Marion Chottin
Where does the blind man’s self begin? At the tip of the stick?
At the handle of the stick? Or at some point halfway up the
stick?
Gregory Bateson
1
Introduction
What, if anything, do the classic philosophies of perception have to tell us about
technical objects, which, from the cane to more contemporary devices, aim to
facilitate the mobility of the visually impaired? Is it possible that we should just
leave these philosophies where we most often class them, that is, in a past syn-
onymous with passé?
Far from the idea that philosophy in practice is removed from its history, here we
propose a return to the thought that, in the seventeenth and eighteenth centuries, had
taken perception as the focus of its investigation. We will forge the hypothesis that,
for this reason, they can be of use in instructing the operation and use of mobility
technologies—which are also the technologies of perception. Among these
philosophies, the Cartesian theory of perception takes its place as seemingly the ﬁrst
to assert its presence in this domain. Descartes was, in effect, the ﬁrst philosopher to
have taken seriously an object that is still today that which the visually impaired
turn to most frequently to assist their movement, and is an ancestor of today’s
Information and Communication Technologies (ICT): the cane, or what Descartes
calls the “stick” of the blind.
This chapter has been translated from French into English by Collette Alexander (ENS Lyon),
whom we thank warmly.
M. Chottin (&)
IHRIM, ENS and CNRS, Lyon, France
e-mail: marion.chottin@ens-lyon.fr
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_3
77

Certainly, Descartes carried out less theory on the cane itself than he did employ it
to theorize (paradoxically) on visual perception. But this use is based on a con-
ception of “stick” that here we propose to examine. For us, it will thus not be a
question of examining his theory of vision through the prism of his ﬁgure of “the
blind man with the stick”1 , but to explain the manner in which he conceived of this
ﬁgure, essentially independently from what he has written on vision. To do so, we
will mobilize the corpus of Cartesian texts, which, although not required for the
understanding of the theses on vision in the Optics, will enable the intelligibility of
the “Cartesian theory of the cane”. With the help of these reﬂections, we will show
that this also allows us to perform theory on the operation of more recent devices (I).
If Cartesian theory drafts a theory of their use, we will, however, state that on
this point it falls short, and in no way nears completion. However, the problems
encountered today by robotics engineers are not so much concerned with the
operation (they know how to construct the devices they conceive of and design)
than with the use of mobility technologies (how to construct tools that are relevant
for the visually impaired). It is, therefore, necessary to mobilize one or two other
theories. We will do so by appealing to empirical philosophies of perception, and,
amongst them, that which seems to be the most consistent, and also the most
relevant to consider that which Descartes has not theorized (i.e. the genesis of
perception): Condillac’s ideas as presented in his Treatise on the Sensations (II).
Lastly, we will attempt to show where the interest lies in turning to Condillacian
philosophy, since, in recent years, other theories have been mobilized and shown
their fruitfulness in the understanding of mobility technologies—this is the case, for
example, in phenomenology, and in particular, the philosophy of Merleau-Ponty.2
Our question will thus be: what unique point of view does a Condillacian theory of
perception enable for mobility technologies (III)?
2
Mobility Technologies Through the Prism of Descartes
2.1
A Theory of the Cane of the Blind
Man in Descartes’ Optics
Conceived of as application of the Discourse on the Method, this 1637 work
contains a theory of vision, of its errors and means to correct them, in the aim of
perfecting optical instruments (in particular, the telescope, invented shortly before),
and mainly to avoid leaving their fabrication dependent on the chance of circum-
stances. From the very opening of the work, Descartes indicates that he will not
1On this point, see Marion [7, 28] and Le Ru [26]. On the topic of the Cartesian use of the ﬁgure of
the blind man with the stick for thinking more generally on the transmission of movement, see Le
Ru [27].
2See notably Lenay et al. [25].
78
M. Chottin

seek to determinate the nature of light, but will mobilize “two or three comparisons”
(p. 152) to render the way in which it causes vision intelligible. The ﬁgure of the
blind man with the stick constitutes one of these comparisons. And when Descartes
notes, about those born blind that employ the use of a cane, “that one might almost
say that they see with their hands (…)” (p. 153), what does this mean to say?
For Descartes this signiﬁes that the blind man’s cane is not the equivalent of a
tactile organ, an extension of the arm and ordinary sense of touch, characterized by
direct contact with the perceived object. It is thus not an extended arm, but a
medium. It is not any more, however, the equivalent of the organ of sight. In other
words, the blind man who perceives by means of his stick does not see any object.
For Descartes, the cane is not a substitution for miracles, or for scientiﬁc processes:
it does not give vision to the visually impaired. Just as he indicated in Discourse on
the Method, that we can, through knowledge, “make ourselves, as it were, the lords
and masters of nature (p. 142–143, our emphasis), to point out that we will never be
as God is, here he writes about blind men with sticks “that one might almost say
that they see with their hands”, to signify that it is a question of something other
than vision. For Descartes there exist “qualities” belonging properly to the sense of
sight (light and colours), that is to say, sensible qualities that, without determining
any essence, only belong to that sense, and to which the cane, evidently, does not
provide access. Based on a clear distinction of sight and touch, such a refusal to say
that the cane allows one to see, or to make of the cane an equivalent of the eye, is
essential for knowledge on blindness. On the contrary, reducing the differences
between the senses can lead to minimizing the loss that it causes.
If the cane is neither the equivalent of the hands, nor the eyes, it enables an
“almost vision” in the sense that, at the same time similar to, and different from,
sight, it allows a perception in which objects are apprehended not only externally,
but above all, at a certain distance from the body. In short, the cane opens onto a
perceptual mode that blends the properties of touch (tactile sensations experienced
via the stick) with at least one essential feature of sight (perception of distance).
A detour by the critique that Merleau-Ponty employs to counter the Cartesian
theory of perception will aid in our precision on this point.
Merleau-Ponty, the author of Eye and Mind, recalls that, for Descartes, “the blind
(…) ‘see with their hands’” (p. 131). If he does not take into account the “almost”, it
is not to falsely attribute to Descartes the thesis according to which the cane would
substitute for the absence of eyes, but on the contrary, it is to reproach him for having
conceived of vision on the model of touch: Merleau-Ponty’s assessment is that if
Descartes attributes a “vision” to the blind man with the stick (a vision that is, in fact,
reduced to touch), it is that he had already conceived of vision as a tactile organ3 . In
other words, for Merleau-Ponty, if Descartes’ cane elicits an “almost vision” it is for
the sole reason that vision itself is “almost touch”. However, this reading does not
seem to us to be precisely faithful to the text of the Optics.
3Merleau–Ponty writes: “The Cartesian model of vision is modelled after the sense of touch”
(ibid.). This reading is also that of M. Serres, but rejected by Cavaillé. Cf. [7, 34].
Mobility Technologies for Visually Impaired People …
79

Certainly, Descartes estimates that vision is realized through the rays of light that
hit the retina, like the stick in the hand of the blind man. But he knows perfectly
well that these rays, unlike the stick, are not themselves felt, and makes recourse
speciﬁcally to this comparison to make perceptible, and thereby intelligible, the
trajectory of light to the eye. Let us not forget that “comparaison n’est pas raison”.
In addition, for Descartes, the identiﬁcation of the role that rays of light play in the
production of vision is a refutation of the classic theory that material images detach
from things to form an impression on the retina—a theory which, however, con-
ceives of sight as touch. A reading of Merleau-Ponty conﬁrms, that by its very
limits it cannot be a question of reducing the perceptions elicited by the stick of the
Cartesian blind man to ordinary tactile perceptions. In this regard, what follows in
the Optics would be lacking in ambiguity: according to Descartes, “one might
almost say (…) that their stick is the organ of some sixth sense given to them in
place of sight” (p. 153, our emphasis). This statement, which seems to be decisive,
must also be read in light of its “almost”. Firstly, a stick is not an organ. If, for
Descartes, “so all that is artiﬁcial is withal natural [21, p. 289], in the sense that the
laws of mechanics are the same as those of physics, the ontological difference
between human productions and divine works could not be denied. Furthermore, no
external will has governed the use of the cane by the blind and the visually
impaired. No divinity has offered them sticks to compensate them for their loss of
sight, as Zeus has given the gift of divination to Tiresias to compensate for the
blindness inﬂicted on him by Hera. However, because the perceptions that the cane
enables are not, strictly speaking, visual, nor reducible to that of ordinary touch, it is
legitimate to attribute them to “some sixth sense”, which, without invoking a divine
gift, is indeed present in the blind.
But what Merleau-Ponty has especially in his sights, and what he refutes, is the
idea that these perceptions would result from a judgement based on the (ﬁrst)
sensations provided by the stick. The “almost vision” occasioned by the blind
man’s cane, as elsewhere in common perception, and which serves as a model in
the Optics, would be nothing but an intellection4—a thesis which would come to
lack in the essential dimension of sense perception. This is why Merleau-Ponty
judges that, according to Descartes, the stick of the blind man is in no way an organ,
and can be reduced to a simple instrument: in the Optics, the blind man manipulates
his stick as he already manipulates his bodily machine.
However, when he employs this ﬁgure of the blind man with the stick further on,
Descartes insists precisely on the fact that he perceives without “in any way
knowing” or even “thinking” of the position of his hands:
(…) when the blind man (…) turns is hand A towards E, or again his hand C towards E, the
nerves embedded in that hand cause a certain change in his brain, and through this change
his soul can know not only the place A or C but also all the other places located on the
straight line AE or CE; in this way his soul can turn its attention to the objects B and D, and
4Cf. Merleau–Ponty, ibid., p. 136 : “(…) the vision upon which I reﬂect ; I cannot think it except
as thought, the mind’s inspection, judgment (…)”.
80
M. Chottin

determine the places they occupy without in any way knowing or thinking of those which
his hands occupy. (p. 169, our emphasis)
Figure in the Optics, p. 135 
Thus, a blind man perceives, through the means of one or two sticks, the relative
placement of objects, because the nerves of his body transmit the information to the
brain—and not because it performs a calculation (or even a reasoning) from the
sensations that he feels in his hands.5 According to Descartes, the blind man has the
ability to perceive not only the end of the stick that he holds in his hand but yet, via
an act of attention, the objects in contact with the opposite part, without the latter
perception presupposing the ﬁrst as a conscious condition of possibility. In sum, the
stick (or cane) is not uniquely, for the Cartesian blind man, what he perceives, but
before all that with which he perceives—that is to say, it constitutes the equivalent
of an organ.6 Therefore, when Merleau-Ponty writes that “our instruments (…) are
added-on organs” (ibid., p. 138), he says nothing other than what his predecessor
has already stated.
What is thus realized, through Descartes’ pen, is an upheaval of the traditional
division of sense perception into ﬁve types, based, paradoxically, on a traditional
conception of the distinction of ﬁve senses7 : The site of a veritable sixth sense, the
cane places the blind man in a new perceptual universe. This is an important point,
and reinforces the idea of Charles Lenay and François-David Sebbah that con-
temporary devices should not be called sensory “substitution” but “supplement”
because they achieve “the opening of a new space of coupling of a man with the
world” [our translation, 25, p. 58]. Thus, the cane can justly be classiﬁed as the
5On this point, we thus join Alquié [1, p. 429], for whom the “natural geometry” evoked by
Descartes about the blind man with the stick is neither a conscious nor unconscious calculation,
but a “mysterious effect by which the bodily dispositions give sense-experience and knowledge to
the soul what it must sense and know” (our translation). Quoted in Cavaillé [7, p. 117].
6This is also how J.-P. Cavaillé assesses these pages of the Optics [7, pp. 62–65].
7Cf. the Description on the human body, in which Descartes treats the operation of the ﬁve senses
one after the other, and explains the differences in sensations they occasion by the speciﬁcity of
nerves that constitute them.
Mobility Technologies for Visually Impaired People …
81

ancestor of such devices.8 We will now show how, far from a refutation of, these
new devices are conceivable within the Cartesian theory.
2.2
Information and Communication Technologies
Through the Prism of Descartes
It is sometimes expressed in the literature that these apparatuses come to refute the
Cartesian theory of perception. In fact, ICT could indeed help to reveal two things.
Firstly, that perception arises from learning (as Descartes would support an in-
stantaneist conception of perception); and secondly, that perception involves the
movement of the body itself (where Descartes would reduce it to an act of the
mind).
Indeed, what we generally consider to be the ﬁrst mobility ICT, i.e. the “Tactile
Vision Substitution System”, developed by Bach-y-Rita in the 1960s, shows that
time and perception are two necessary conditions for the occurrence of perceptions
that it enables. Remember that the TVSS is a device comprised of a camera that
records the images of the exterior world and an instrument that allows it to trans-
form these images into electric data, retransmitted afterwards onto a part of the skin
of the subject (that of the back or chest, and today, the tongue). The operation of the
apparatus involves the action of the body: to perceive a particular object, the
individuals endowed with this device must move the camera from right to left, up
and down, and also zoom in and zoom out.9 The experimental phase of the device
also shows that these individuals must repeat these gestures multiple times, to
ﬁnally reach the identiﬁcation and localization of what is an obstacle, a man, etc.10
Conversely, the Cartesian theory would not tie in perception, temporality or
bodily activity. And indeed, it has never placed these parameters at the heart of the
perceptual process. According to this theory, perceptions arise from the transfor-
mation of the movement of nerves occasioned by these objects into sensations of
the soul, by virtue of laws instituted by God, whose efﬁciency rests on the prin-
ciples of geometry, through an almost instantaneous process. In the ﬁrst instance,
light, for Descartes (although we are now aware that he was mistaken), spends no
time in passing from the luminous object to our eyes:
In the ﬁrst place this [sc. the comparison to the blind man with the stick] will prevent you
from ﬁnding it strange that this light can extend its rays instantaneously from the sun to us.
For you know that the action by which we move one end of a stick must pass instanta-
neously to the other end, and that the action of light would have to pass from the heavens to
8The fact that state-of-the-art canes exist today gives conﬁrmation on this point, Cf. Pissaloux and
Velazquez [32].
9Cf. Bach-y-Rita [3], Declerck et al. [14, p. 253].
10Ibid.
82
M. Chottin

the earth in the same way, even though the distance in this case is much greater than that
between the ends of a stick. (Optics, p. 153)
The blind man apprehends objects as soon as his stick touches them: also he may
be certain, if not of their essence, at least of their existence—in contrast to those
possessing normal sight who, shortly after Descartes’ death, were aware that it was
possible to see dead stars and that all visual perception only accessed the past. Did
visually impaired people live in another temporality than those who privilege sight
over the other senses? But let us push forward. Secondly, the impression (instan-
taneously) left by the object on the sense organ is yet, according to Descartes,
transmitted instantly to the site in the brain where it is transformed into sensa-
tion11—and this, because the nerves connecting the eye to common sense form a
continuum that may not be moved at one point without doing so at all of the others.
The last phase of this process is trickier to interpret: are cerebral movements
transformed instantly into sensations? Coming back to the text of the Optics as cited
above, we can state that, for Descartes, visual perception of the position of objects
rests, at the very least, on the movement of the eyes. In other words, and as we have
shown elsewhere,12 the laws instituted by God work upon a body in movement and
are thereby inscribed in time.
However, to recognize that the body, in Descartes, is involved in the emergence
of perception is not to make “the blind person with TVSS” a Cartesian ﬁgure: for
the author of the Optics, perception requires no reiteration, no repeat of the expe-
rience. As soon as the body moves, the soul is aroused, invoking the efﬁciency of
divine laws, which exempts the subject from mobilizing allegedly innate geometric
ideas. This is why we have elsewhere classiﬁed the Cartesian device as a “ﬁxisme
sensori-perceptif”,13 despite the concepts of time and action that it implicitly
mobilizes. Can we say, in this case, that ICT contradicts the Cartesian theory?
From a Cartesian point of view, these devices do not arise out of the divinely
instituted one that occasions perception: there are no natural laws by virtue of which
tactile stimuli (such as electric stimulations that are apprehended on the skin) are
transformed into objects perceived at a distance from the body itself. In this
framework, the time required for the visually impaired to perceive through TVSS is
a sign of the artiﬁcial character of these perceptions, and not a proof of an essential
temporality in perceptive phenomena. However, if he maintains that perception,
such as it results from divine laws, does not reveal any genesis, Descartes never
maintained that it do not when it is the site of new perceptual modes—such as that
which, according to him, the blind man experiences with his stick. Much to the
contrary, the philosopher estimates that the blind man must learn to perceive with
his stick to, by virtue of it, apprehend objects:
11Cf. Rules for the Direction of the Mind, Rule XII.
12Cf. [11], the chapter entitled “Le surgissement temporel de la sensation”.
13Ibid.
Mobility Technologies for Visually Impaired People …
83

It is true that this kind of sensation is somewhat confused and obscure in those who do not
have long practice with it. But consider it in those born blind, who have made use of it all
their lives: with them, you will ﬁnd, it is so perfect and so exact that one might almost say
that they see with their hands (…) (Optics, p. 153).
In this case of exceeding natural sensibilities (which, according to Descartes, is
an innate sensibility), the repetition of experience is necessary for perception,
because it is required for the becoming-organ of the stick: the practice of manip-
ulating a cane does not at ﬁrst present gains in skilful and precise movement, but
before all it incorporates an artifact, augments the body itself. Thus, Descartes
conceives that there exist perceptual modes that are beyond those that were tradi-
tionally identiﬁed, and assessed that these modes are but the object of a genesis
based on bodily movement: time and action ﬁnd themselves as necessary conditions
of perception. It is in this sense that, according to us, ICT does not contradict the
Cartesian theory of perception, but are conceivable within its framework.
Moreover, Descartes provides the condition of possibility and the principle of
this circumscribed perceptual learning—which, in this sense, is not only conceiv-
able but conceived of by Cartesian theory. To understand this point, let us ﬁrst turn
to Treatise on Light, and in particular this passage:
(…) if words, which signify nothing except by human convention, sufﬁce to make us think
of things to which they bear no resemblance, then why could nature not also have estab-
lished some sign which would make us have the sensation of light, even if the sign
contained nothing in itself which is similar to this sensation? Is it not thus that nature has
established laughter and tears, to make us read joy and sadness on the faces of men? (p. 81)
These lines teach us that there is nothing unintelligible in the transformation of
bodily movements into sensations of the soul, since we conceive of this passage,
evidently based on non-resemblance, on the model of human language: if men were
able to establish signs that refer to things without in any way resembling them, then
God was a fortiori able to make it so that an alteration of the material signiﬁes an
alteration of the immaterial. The possibility of this institution is supported by the
sure existence of another divine language: that of the passions, which we speak
naturally. Similarly, there should be a language of perceptions, operating innately
within us. However, if here we substantiated in the idea that divine laws of per-
ception operate “in us” and “without us”14 , are we not here diametrically opposed
to perceptual learning?
Not if we consider this thesis on the institution of nature as at the same time
susceptible to give reﬂection on its own surpassing:
It is useful to note here (…), that although nature seems to have joined every movement of
the gland to certain of our thoughts from the beginning of our life, yet we may join them to
others through habit. Experience shows this in the case of language. Words produce in the
gland movements which are ordained by nature to represent to the soul only the sounds of
their syllables when they are spoken or the shape of their letters when they are written, but
nevertheless, through the habit we have acquired of thinking of what they mean when we
14But not “despite us” as found in other natural judgements in Malebranche.
84
M. Chottin

hear the sounds or see the letters, these movements usually make us conceive this meaning
rather than the shape of the letters or the sound of the syllables (The Passions of the Soul,
p. 348)
Whereas the Treatise on Light makes human language a reason a fortiori in
favour of the existence of a divine institution of perception, these lines mobilize it in
an almost-opposite direction, to support the possibility offered to man to bypass the
laws of God. Thus, human language is not only an institution analogous to that of
perception; it rests on a sensory transformation of it: for words to signify things,
bodily movements must, by nature, signify such or such sound, and need to refer
back, beyond these sensations, to the things that men have decided to attach to these
sounds—without which words for us would be reduced into their simple material
sonority, as they do, for Descartes, in animals. Thus, human language to him
testiﬁes on its own the possibility of an alteration of divinely instituted laws. In an
act that, in a way, thumbs his nose at God, he alters the union of the soul and the
body, which, as the occasional cause of a solitary thought, has become a condition
of intersubjectivity. This is what informs the apprehension of artiﬁcial perceptions.
The blind man can perceive objects with his stick because it is possible for a man
to alter in time the laws of the union—in this case, to arrange it so that the
movements imprinted on his hands by the cane become signs, not only of certain
manual perception but yet new perceptions of sensible objects. This phenomenon of
bypassing, or at least a new “coding”15 of the laws of the union, sheds light on the
operation of TVSS. It is also quite remarkable that Bach-y-Rita reappropriates the
linguistic conception inherited from Descartes, based on the (anti-Keplerian) thesis
that there is no homogeneity between the strictly optical phase of vision (the
impression of luminous rays on the bottom of the eyes), and the following, purely
mechanical one (from the optic nerve to the brain):
In normal sight, the optical image does not get beyond the retina. From the retina to the
central perceptual structures, the image, now transformed into nerves pulses, is carried over
nerves ﬁbers. It is in the central nervous system that pulse coded-information is transformed
into the subjective visual experience. (2002, p. 497)
We will thereby not be surprised to read, a bit further on (p. 511), from the pen
of Bach-y-Rita, the very expression “bypass”, mobilized to qualify the cerebral
effect of the use of sign language by deaf people—language perfectly analogous,
according to the author, to his TVSS. However, what follows is as if he were
drawing a non-Cartesian conclusion from Descartes’ own theory. Since they are
movements of the nerves, and not the retinal image itself, which are transmitted to
the brain, it seems possible, according to Bach-y-Rita, “for the same subjective
experience that is produced by a visual image on the retina to be produced by an
optical image captured by an artiﬁcial eye (a TV camera), when a way is found to
deliver the image from the camera to a sensory system that can carry it to the brain”
(p. 497). In short, the exclusively mechanical dimension of the second phase of the
15According to the now adopted formula from Marion [28, p. 254].
Mobility Technologies for Visually Impaired People …
85

process makes the retinal image superﬂuous to vision and opens up the possibility
of substitution for any other image, provided that it can agitate certain nerves and
convey movements to the brain. Thus, for Bach-y-Rita, the TVSS is a genuinely
substitute, and not just supplementary, sensorial system: ideally, if not in reality,
blind people who are so equipped are likely to see16. By all appearances, it is a
matter, according to him, of a direct implication of perceptual semiotism.
We have nevertheless shown that, according to Descartes, the blind man with the
stick does not, strictly speaking, see, but accesses a new perceptual mode. While it
is true that under the Cartesian theory of natural perception, any individual whose
nervous system is not compromised is capable of seeing (what recent experiences
seem to have conﬁrmed17): a blind person in whom we successfully stimulate the
nervous movements that operate in vision would be, literally, a blind (ab-oculis:
“without eyes”) sighted person—and this, even if they had no retina (which science,
however, has not yet shown), because, for Descartes (as for Bach-y-Rita), it is not
the retinal image that constitutes the proximate cause of vision, but the nervous
impulses in the brain. The author of the Optics even highlights that this possibility
in fact partially realized in “madmen” and “those who are asleep” (Optics, p. 172).
However, the perceptions evoked by the cane do not obey, for Descartes, the same
explanatory schema: if he attributes no veritable vision to the blind man with the
stick, but only an “almost vision”, it is because he assesses, logically, that the
nerves agitated by the movements of his hand are those that evoke, in the brain,
tactile perceptions, and not visual ones. It would undoubtedly be the same for the
blind person with TVSS, insofar as Descartes sees it, each sense corresponds to a
series of nerves and where the organ solicited in this case is that of touch (the skin
of the back, chest, abdomen or tongue). For Bach-y-Rita, on the contrary, any
device by which an image is “coded” into nerve impulses is equally able, by virtue
of brain plasticity, to evoke visual perceptions. In short, the latter considers that a
kind of perception does not depend on the organ invoked but on the type of stimuli
caused. Descartes would most likely consider that Bach-y-Rita remains a prisoner
of the traditional signiﬁcance given to the image: for the philosopher, it is not only
the retinal image that is superﬂuous to vision, but any physical image, whatever it
may be. However, according to him, we would never produce visual images by
activating any nerves other than those that God had allocated to vision. Man, for
Descartes, is able to modify the ways of God and produce new perceptual modes,
but cannot reproduce those of divine laws without respecting their modus operandi.
If one wants to give sight to the blind, it is thus useless to conceive of supple-
mentary (or substitute) sensory systems. Only retinal prostheses are capable of
doing so. But if one intends to increase their mobility, and open them up to, as well
16The researcher thus estimates that a kind of sensations is not determined by the organ they issue
from, nor by the subjective experience of the subject that receives them but by the type of stimulus
that occasions them.
17Cf. Reich et al. [33].
86
M. Chottin

as the clear-sighted, new perceptual ﬁelds, then, indeed, these systems offer the
most interest in this regard.
Thus, Cartesian philosophy teaches us that it is not necessary to conceive of
perception as an effect of the body and of time to consider how assistive tech-
nologies operate. It sufﬁces to have a theory of the production of natural perception,
which lends itself to thinking, differentially, about that of artiﬁcial perception, and
what can be transposed onto these devices: this is the case of Descartes’ theory,
which makes perception as exercised via the ﬁve senses an effect of the divine laws
of the union of the soul and the body, and, consequently, of mediated perception the
effect of a bypassing, or temporal reconﬁguration of these laws—and not, as one
might think at ﬁrst, the result of an augmentation of the bodily machine by adding a
second mechanism, and the soul’s reasoning over the combined data from this new
artifact. For Descartes, let us not forget “that I am not only present in my body as a
sailor is present in a ship, but that I am very closely joined and, as it were,
intermingled with it, so that I and the body form a unit” (1641, p. 56).
The Cartesian theory also provides the principle of the use of ICT: it is a matter,
for the individuals ﬁtted with these units to indicate to internal movements within
their bodies other perceptions than those that these movements naturally signify,
and, thus, to access new perceptual worlds. However, nowhere does Descartes
describe the way in such learning would effectively be realized—which is, how-
ever, a necessity for one who wishes to philosophize on these technologies. This is
the reason for which we must now turn towards another theory.
3
An Alternative Theory: Condillac’s Treatise
on the Sensations
3.1
From a Causalist Theory to an Empiricist
Theory of Perception
Far from simply “adding” Condillac’s theory to Descartes’, we will somewhat
substitute the Condillacian theory of perception, as presented in the Treatise on the
Sensations (1754), for the Cartesian theory of the Optics and of the Passions—and
we will do so because thought on perceptual learning assumes the transition to a
new theoretical framework.
Does this transition, as Merleau-Ponty believes, involve a departure from the
external point of view (or the status of the exterior observer) that Descartes would
adopt on perception, in favour of an internal point of view, that of the perceiving
philosopher? For the author of Eye and Mind, Descartes places himself to the
exterior of the perceptual act, and thus condemns himself to employing a “thought
of seeing” instead of seizing the “vision in act” (p. 136). Descartes examines at the
eye as it sees, the blind man as he touches with his stick, and from there
Mobility Technologies for Visually Impaired People …
87

reconstitutes the perceptual act. According to Merleau-Ponty, Descartes does not
perceive, but thinks of perception, and reduces it to a “mind’s inspection” (p. 136).
Descartes does, in fact, come to adopt, in the Optics, such a perceptual exter-
nalism—as the only engraving in the work attests:
Figure in the Optics, p. 139 
However, the external point of view on perception is condemned in effect to miss
all, or at least part of, the genesis of perception: if in accordance with the scientiﬁc
position on nature, such a point of view makes the explanation of vision possible,
i.e. the demonstration of its causes (from the object to the movements that affect the
brain), as far they are—as the body they affect—of a material nature, and thus able
to be seized from the outside (in third person), this perspective does not allow the
apprehension of perception in itself, that is to say, as a mode of thought. This is why
Bach-y-Rita, in describing the way in which his subjects learn to perceive with
TVSS, is forced to rely on their testimony, and, without it, could not know that, for
example, “exploring the face of one’s loved-one can be very disappointing, since
the emotional messages (…) have not been perceived with our TVSS” [3], p. 500).
Nevertheless, if it is true that perceptual learning, as perception itself, cannot be
truly apprehended from the exterior, it is not the case that Descartes set forth a
purely externalist theory. First of all, he is well positioned to say that thought is
experienced in the ﬁrst person,18 and to make of perception a mode of thought.19
How, under this framework, could he judge that perception is accessible from the
exterior? We should not confuse the input of causes, which, for Descartes, can and
must pass, at least in part by observation (he takes the example of the eye of a bull)
and the apprehension of perception in itself, or of the perceptual act, impervious for
its part to external scrutiny, whether of the senses or of the intellect. This deﬁnition
of thought through the consciousness that we have of our objects of thought is
elsewhere at the very foundations of Meditations on First Philosophy, in which, as
we are well aware, Descartes expresses himself in the ﬁrst person and analyzes the
18Descartes [21, p. 113]: “Thought. I use this term to include everything that is within us in such a
way that we are immediately aware of it”.
19Descartes [23, art. XVII].
88
M. Chottin

content of his mind. Without a doubt, if he takes his own perceptions as an object in
this text, it is just as much to place the sensory world aside, and to come to back to
it at the end as a world that we deduce, with little or nothing to say about the world
that we perceive. Between the two, the subject is no longer, or does not believe
himself to be anymore than a thinking (meditating) subject. It was as if Descartes
assumed the point of view of the perceiving subject for the sole purpose, which he
deems necessary for the foundation of knowledge, of disqualifying sense
perception.
In the Optics, on the other hand, Descartes happens not only to adopt such an
internal point of view on sense perception, but judges it fertile for knowledge. Here
indeed is what he writes, just before introducing the ﬁgure of the blind man with the
stick:
No doubt you have had the experience of walking at night over rough ground without a
light, and ﬁnding it necessary to use a stick in order to guide yourself. You may then been
able to notice that by means of this stick you could feel the various objects situated around
you, and that you could even tell whether they were trees or stones or sand or water or grass
or mud or any other such thing. (p. 153)
Here, Descartes indeed recalls an experience, in the ﬁrst person, which should
have been, in his time, relatively common, and implicitly asks each reader to
reproduce it in thought. And Descartes thus describes the effects of this “sixth
sense”, much more highly developed in those blind from birth than in the sighted.
However, the transition from the experience of darkness to that of blindness does
not constitute, along these lines, a change in the point of view adopted on per-
ception. If, in fact, Descartes then goes on to observe a blind man who manipulates
his stick, it is because he has started to adopt, in thought, the point of view of this
blind man. If this was not the case, how could he write that the blind man perceives
differences “between trees, rocks, water and similar things by means of his stick”
(p. 153)? From the exterior, all that is possible to say is that an individual moves
around, and touches objects with his stick. This ﬁgure of the blind man thus does
not constitute as much the object of a sensory observation, than the effect of a
thought experiment. Which is to say that Descartes had the good angle to describe
how perceptions occasioned by the cane are learnt. We must, therefore, look
elsewhere for the reasons why he did not.
If he said nothing about the way in which the blind man learnt how to perceive
with the stick it is, ﬁrstly, that such a discourse would have weakened his theory on
natural perception, based on the idea of divine institution. A genetic analysis of the
manipulation of the stick would have given the impression that ordinary perception
was in itself a genesis, and encourage doubt over the effectiveness of divine laws.
According to Descartes, only the experienced blind man comes to instruct a theory
of vision, not the one that learns to see.20
But if the author of the Optics does not theorize on learning how to manipulate
the cane, it is mainly that he was not in a position to do it more—not due to the
20Cf. Chottin [10].
Mobility Technologies for Visually Impaired People …
89

point of view that he adopts on perception, but in the use that he makes of this point
of view. We know that Descartes considers the ﬁgure of the blind man with the
stick as a comparison through which it is possible to represent the mechanical
causality of natural perception: far from being provoked by ﬂeeting images, vision
requires nothing more than a series of movements that take place one by one.
However, these movements are not sensed by the perceiving subject—this is also
why Descartes makes recourse to the comparison with the blind, as we have stated
above. However, we have shown that perceptual learning, according to the author
of Passions, consists of a reconﬁguration of divine laws that associate, like in
language, certain movements with certain sensations of the soul. But how is one to
establish a language from movements of which we have neither knowledge, nor
consciousness? While the Cartesian conception of the cane relative to its
becoming-organ, as well as his theory of natural perception, seem to evade
Merleau-Pontian objections, that of its use reveals itself to be more permeable to his
critique: if we understand that divine institution may happen, or rather permits the
subject to dispense with paying attention to the movements of his brain, it is much
harder to seize how man could give meaning to these movements as original
perceptions without knowing their nature, or even, in most cases, their existence.
However, such involvement seems required: how can we think about an institution
without the subject that institutes it21 ? We will ﬁnally highlight a last difﬁculty
inherent to the Cartesian semiotism: if perception is analogous to language insti-
tuted by men, is it not the mental or imaginative representations—and not the
sensory—that the perceiving subject must attain, as he does for ideas, through
words? Do not these theoretical difﬁculties signify that we need to step beyond
perceptive semiotism? The theory of the use of the cane, but also of TVSS, evi-
dently assumes employing the point of view that Descartes took on perception
differently—to interpret it as the means of attaining not the unperceived causes of
perception, but the very phenomenon itself.
Lastly, Descartes’ error did not consist of taking a wrong position on perception,
but of attributing to it an inadequate ﬁnality. This is at least what numerous
empiricists of the eighteenth century believe, who for the most part acclaim
Descartes as a philosopher of subjective analysis but reproach him as having gone
beyond experience and supported arbitrary theses. It is a question for those such as
Locke, Berkeley, Diderot, Condillac, Hume, etc. of not extracting from experience
truths over perceptual causality, and of relying on the facts that the subject
apprehends of his consciousness.
Before being practiced by phenomenology, suspending consideration of the
production of perceptions had thus been the case for a certain number of Descartes’
successors. Among them, Condillac presented the theory that, to us, seems to have
the most to add to help us make sense of mobility technologies. Indeed, from his
21The problem seems to come from the fact that, according to the Cartesian theory of perceptual
learning, these are not natural perceptions, entirely conscious, which allow the attainment of
artiﬁcial perceptions, but only the cerebral movements, which are unperceived.
90
M. Chottin

Essay on the origin of human knowledge (1746), the genesis of perception is taken
as the very object of his philosophical enquiry. As for Descartes before him, for
Condillac it is a matter of analyzing the mind—but not by way of methodical doubt,
deemed ineffective—and the subsequent elimination of the sensed world. Instead, it
is a question of maintaining perceptual consciousness and reﬂecting on perception
through the abstraction made over physical movements that can (or not) produce
it22 . We thus discover its series of transformations and the ideas its generates:
precisely what we were lacking in Descartes. It is, however, on Treatise on the
Sensations that we will focus, because in this text Condillac mobilizes the ﬁgure of
the blind man with the stick and reimagines it, at the heart of a radicalized theory of
the genesis of perception.
Far from describing his perceptions, Condillac imagines, in this new work “a
statue internally organized like ourselves, and animated by a mind deprived of any
kind of idea. (…) Its marble exterior did not allow it the use of any of its senses, and
we reserved for ourselves the freedom to open them at will to the different
impressions they are susceptible of” (p. 170). Why such a device? If it is, for him, a
matter of taking perception (and not its causes) as his object, does not the abbé then
fall into the trap of the method attributed (in part falsely) to Descartes, which
involves taking an external point of view on perception, and, moreover, a totally
ﬁctive one?
Rather, his approach consists of somewhat of an extension of what we saw at
work in the Optics: since it is true that, throughout the work, Condillac observes the
statue from the exterior, it is not that he had arbitrarily placed certain perceptions in
its mind, but that he had ﬁrst put himself in its place,23 to then re-transcribe his
experiences through a thought experiment fuelled by the knowledge of his time. But
one question remains: why, on one hand, does he start by denying the statue the
faculties (attention, judgement, reﬂection, etc.) that characterize the human mind,
and, on the other, attribute the senses to it one after the other, while human per-
ception presents itself from the outset as multisensory? In short, how could such a
device instruct the use of mobility technologies that are evidently not designed for
statues without ideas?
For Condillac, to depart from the perception of man “normally” constituted, as
he does in his Essay, amounts to distorting the genesis of perception, that is, to tend
towards to conferring on one sense that which arises out of another—primarily, to
attribute to sight the perception of the external world, when according to him, it
arises from touch. Although it requires analysis, the perception of space without
sight is not problematic in itself, opposite to the perception of space through sight;
due to the fact the eyes alone give no idea about the object external to the mind.
Regarding the ﬁrst abstraction he makes of the faculties, Condillac justiﬁes it, like
in the Essay, by his plan to produce the genesis. This device allows him to establish
22Ibid., p. 30.
231754, p. 155: “(…) I forewarn the reader that it is very important to put himself exactly in the
place of the statue we are going to observe”.
Mobility Technologies for Visually Impaired People …
91

two principle theses: (1) the perception of objects is the product of a veritable
learning process, and (2) each sense allows the deployment of all of the faculties,
but all of the senses do not participate in the same way to the genesis of perception:
it is only touch that gives us access to exteriority.
This is to show—notably at odds with his predecessor, that sight by itself does
not provide vision of any object and needs to be nurtured—that Condillac takes up
the comparison of the blind man’s stick and describes what Descartes inevitably
passed over without mentioning: the genesis of its manipulation, or, more specif-
ically, of its becoming-organ.
3.2
Mobility Technologies Through the Prism of Condillac
– The Condillacian theory of the cane
If he attributes to the ﬁgure of the blind man with the cane all of the relevance
that Descartes bestowed upon it,24 and maintains as he did that it can inform vision,
Condillac interprets this relevance in an anti-Cartesian sense: according to him, the
necessity of learning how to manipulate the cane reveals, through analogy, the
historical character of all perception—since we presuppose nothing about its mode
of production, nor begin by maintaining that divine laws rule over it. As in the ﬁrst
part of this study, we will now consider the ﬁgure of the blind man with the stick for
itself, independently of the considerations on vision that it can support.
Removed from Descartes and his semiotic model, Condillac presents a “poetic”
theory of perception. According to him, the perceptual effects occasioned by the
cane do not rest on any code of mysterious operation, but arise from an elaboration,
in ﬁrst person, of the perceived objet—or from an exteriorization of facts of con-
sciousness, analogous to the process of production, or even of creation25: by means
of conscious sensed data, the blind man, or rather the statue deprived of vision,
constructs for itself the perceived object. The chapter in question can thus be read as
a critical re-writing of the pages of the Optics that we have been discussing,
especially when we note that Condillac follows the exact order of visible qualities
that Descartes adopted, and describes in succession the manner in which the statue
learns to perceive distance and position, then the magnitude of what will become
for it a given object. Yet it is ﬁrst necessary to determine what a blind person
perceives (assumed to be an experiential virgin) when discovering their cane: “The
ﬁrst time that the statue grasps a stick, it knows only the part that it is holding: and it
attributes to that part all the sensations that the stick causes” (p. 253). It sufﬁces for
24Condillac [12, p. 253].
25Ibid., p. 279: “It is the hand that, guiding vision successively over the different parts of a shape,
etches all these parts in memory; it is the hand that, so to say, guides the engraving tool when the
eyes begin to attribute to the exterior the light and colours that they have ﬁrst experienced as in
themselves”. Our emphasis.
92
M. Chottin

our own persuasion to be transported by thought into the mind of this statue, and to
speculate on the way in which perception takes place. On this point, Descartes
would likely have agreed with Condillac: under the laws unifying the soul and the
body, the movements produced by the stick signify, in the soul, the perceptions
experienced in the hand. But the abbé continues: “The statue (…) does not know
that the stick has extension; and as a result it cannot judge the distances of bodies
that it touches with it” (ibid.). If the philosopher deprives the statue of all knowl-
edge of the stick, is because it is necessary to think of it as such to uncover, by
sedimented experience, the conditions of its perceptions: can we appreciate, with a
cane, the distance of objects, without knowing the length of said cane? A question
that would be incidental, if the very essence of perception were not in play: is it the
knowledge of the cane that occasions the perceived object, or vice versa? Do I
elaborate on my perception, or is it that the object that imprints itself, via my senses,
on my mind? For Condillac, the answer seems clear: any mediated perception of an
object assumes knowledge of said medium. The anti-Cartesian armament is at the
ready.
In the Optics, Descartes indeed wrote: “Our blind man holding the two sticks AE
and CE (whose length I assume he does not know) (…) can tell (…) where the point
E is” (p. 170, our emphasis).
Figure in the Optics, p. 136 
According to Descartes, it is the very opposite which happens: a blind man has
access to the perception of distances without (the necessity of) knowing the length
of his stick, because the reconﬁgured divine laws, mobilizing the “geometrizable”
data in his body, sufﬁce to make him perceive. It is such a conception of perception
—based on the passivity of the subject—that is the focus of Condillac’s contention.
Logically, his statue cannot know the position of objects either (p. 253). Condillac
speciﬁes the conditions of these perceptions:
To judge distances by means of a stick, the statue much have touched it throughout its
length, and to judge position by the impression that it received from the stick, the statue
must, while holding the stick in one hand, study the direction of it with the other. (ibid.)
In the blind spot of the Cartesian theory, Condillac reveals perceptual learning
by the subject itself. The incorporation, or the becoming-organ of the cane, requires
Mobility Technologies for Visually Impaired People …
93

its active exploration by touch.26 At the same time, a new mode of perception that
Descartes ﬁrst managed to describe arises:
The statue no longer attributes the sensations caused by the stick at the end that is in its
hand, it feels at the contrary that the hardness or softness of objects on which it places the
stick are associated with the opposite end; and this habit will lead it to distinguish sen-
sations that it did not distinguish previously. (p. 254, our emphasis)
However, Condillac conceived the novelty of this perceptual mode somewhat
differently to Descartes: he does not evoke the occurrence of a sixth sense, but
considers that these new perceptions still fall within the sense of touch. According
to him, in fact, all of the senses know a genesis whose principle consists of an
externalization of sensations: smelling, tasting, hearing, seeing are at ﬁrst internal
(in the statue or in the child), without any perception of the object. It is only under
the auspices of touch that the olfactory, gustatory, auditory and visual senses
externalize, and are thus distinguished, not only from each other, but between each
other: for example, to touch matter allows a distinction between colours, that,
initially, were fused together for (and in) consciousness.27 Touch, for its part, does
not know, in ordinary perception, the same process of externalization: as soon as it
moves, the statue discovers its body, in material and sensorial extension, which it
does not consider as a set of subjective impressions, nor as an object exterior to its
being. In the experience of its body, the statue does not externalize the sensations
ﬁrst experienced subjectively, but more so perceives an extension that it knows as
“I”.28 On the contrary, the manipulation of the cane produces a projection of tactile
sensations equivalent to those of other senses—because the stick constitutes a
medium analogous to luminous rays for vision, sound waves for hearing, etc. which
does not respond when we touch it, nor do the objects it strikes. Like the exter-
nalization of the other senses, that which is occasioned by the cane produces a
distinction between sensations experienced at ﬁrst in the same organ, in this case the
hand—sensations, which, for Condillac, were sensed, but not known. This is why,
according to him, the stick gives rise to perceptions, which, all the while having a
tactile nature, are well and truly original. By externalizing itself in perception,
consciousness manages on its own, and distinguishes its sensations (which thus
delineate objects), feelings and passions (which remain internal).
Thus, everything happens as if semiotism paradoxically prevented thinking of
perceptual learning29: while the Cartesian theory of bypassing divine laws makes it
difﬁcult, almost impossible to conceive of the genesis of perception in the blind
man with the cane, Condillac’s “poetic” conception, which takes its origin in the
26Here we are far from the alleged passivity of the body, but also of the mind, to which classic
empiricism would be condemned.
27This principle also works for sounds, and the other sensible qualities.
28Condillac [12, p. 234].
29On the relationships between semiotic conceptions and geometric conceptions of perception, see
[8].
94
M. Chottin

statue’s sensations themselves, and shows how the perceived object is always
constructed with the hands, makes it conceivable, and even probable.
Such a conception deviates even more from Descartes’ in that it refuses to
condition the perception of geometric laws. Indeed, here is what Condillac writes
on learning about magnitude: “In order to establish the distance between the ends of
two crossed sticks, a geometer need only establish the magnitudes of the angles and
the sides” [12, p. 254]. But, according to him, it is not in this way that the statue
operates:
The statue cannot pursue a method with so much precision. But it knows roughly the size of
the sticks, how much they are inclined, the point at which they are crossed; and it judges
that the ends which touch various objects converge or diverge in the same proportion as the
ends that it as in hand. Thus, we readily imagine how as a result of various trials the statue
will formulate a kind of geometry and will judge the size of objects with the aid of two
sticks. (p. 254–255, our emphasis)
Thus, the statue does not apply any trigonometrical rule to what it perceives:
now it has learned to project its sensations away from itself, it feels at the same time
the gap between its hands30 and that of its sticks, and, by the movements that they
imprint, discover their different relationships. It could thus not only perceive the
magnitude of the bodies it encounters, but also express this magnitude in any unit of
measurement, relative to its body parts. The perception of magnitude by means of
the cane thus does not come from calculation, but from the acquisition of senso-
rimotor schemas.31
Of course, geometry is involved: these schemas are perceptions of relationships.
But it is a matter of the geometry inherent to bodily experience, deprived of signs,
as formalized calculations:
Thus, knowledge of the principles of geometry would be totally useless for our statue (…).
It would be thus quite superﬂuous to conjecture that the statue has innate ideas about size
and position: it needs only hands. (p. 255)
This last proposition is not in contradiction with the Cartesian theory: for
Descartes, the device of divine institution precisely enables the explanation of
perception without placing an innate geometry in the spirit of the subject. The
divide between the two theories must not, however, be underestimated. If the
Cartesian blind man only perceived “as if by a natural geometry” [18, 19, p. 170,
our emphasis], because he does not calculate any more than his Condillacian
cousin, his perceptions are the effect of laws that geometrize in his body. Instead,
Condillac’s blind statue is not the plaything of any theorem. The fact that it must
learn to perceive and make its own geometry “blindly groping along” does not
attest to the actualization of a disruption of the divine laws, but the powerlessness of
the laws of optics to generate perception. The blind man with the cane is himself the
author of what he perceives.
30From here it is a matter of proprioceptive sensations.
31On this notion of sensorimotor schemas, see Lenay et al., op. cit.
Mobility Technologies for Visually Impaired People …
95

– ICT through the prism of Condillac
Following Descartes, Condillac teaches us that mobility technologies do not
restore sight to the blind.32 For the author of Treatise on the Sensations, they give
them access to perceptions that have the distinction of taking place away from the
body, or moreover, by augmenting the body. By the phenomenon of objectiﬁcation
that results, these perceptions show that touch can operate like the other senses, and
project its sensations. Yet, this is exactly what Bach-y-Rita describes: “After suf-
ﬁcient training with the TVSS, our subjects reported experiencing the image in
space, instead of in the skin” (op. cit., p. 500). The researcher elsewhere situates his
invention in continuation with the blind man’s stick:
Similarly, a blind person using a long cane does not perceive the resulting stimulation as
being in the hand, but correctly locates it in the ground being swept with the cane, and a
person writing with a pen does not perceive the contact as being on the ﬁngers, but rather
locates it subjectively on the page. (ibid.)
In addition, Bach-y-Rita highlights what Condillac shows as a necessity, namely
the manual and corporal exploration of the perceptual medium (p. 497). Through
the prism of the Condillacian theory of perception, mobility technologies are thus
conceivable, but not as devices through which (continued) creation alters the union
of the soul and the body, but as instruments analogous to the organs of sense,
through which the human creation of the perceived world takes place. The break
with Descartes is complete. What the author of the Optics could only give the
principle of, namely the use of ICT, Condillac produces in thought, through a
radically revamped method.
But if Bach-y-Rita’s observations already express all the above, to what end
could we employ a Condillacian theory? To assert that, from the eighteenth century,
a philosopher had established that the body in movement was full of new per-
ceptions? The interest is not only historical. As we judge it, Condillac’s thought
enables the avoidance of two errors. Firstly, where Bach-y-Rita writes that the
subjects endowed with TVSS must know of the mechanism, it could not be con-
sidered that a knowledge of the internal operation of the device is required for its
usage: if it is necessary for its incorporation, to explore the medium with which one
perceives, it is not to know its machinery—it is not a mechanism that is known, but
the sensorimotor schemas that one must acquire. As Charles Lenay and
François-David Sebbah (after, by their own acknowledgement, Merleau-Ponty)
have expressed:
(…) we abandon the conception that the subject would receive information then perform
calculations on them to identity objects and produce representations in an internal space. To
the contrary, it is through their action that the subject seeks and constructs rules of con-
sistent relations between sensation and action (our translation, op. cit., p. 57)
32In the debate that sees contemporary researchers in opposition on this subject, our philosophers
would thus agree with Degenaar [15], Warren et al. [35], and ﬁnd Bach-y-Rita [3] and [31]
erroneous.
96
M. Chottin

Secondly, when Bach-y-Rita shows, regarding a TVSS user who is tasked with
batting a rolling ball, that he needs, to achieve this, to “calculate the time it would
take to reach the edge of the table, calculate the position of the table (…), identify
the location in his “visual” ﬁeld of the bat, and correctly time his movement of the
bat in order to bat the ball” (op. cit., p. 501), it could not be the case to believe that
such a series of perceptions and actions constitute, strictly speaking, the effect of a
calculation. In effect, Condillac taught us to not mistake the applicability of
mathematics to perceptual phenomena for the development, in the facts of con-
sciousness, of perception itself: it is not because a phenomenon can be expressed
mathematically that it is the effect of a causality, and even more so of a mathe-
matical knowledge. A geometer would not any better catch said rolling ball than
any other individual. Yet, it is not any more a matter of eliminating the geometry of
perception: during the exercise with the ball, as in the assessment of magnitude, the
perception of relationships gradually emerges from the movement of the body itself.
For Condillac, these are perceptual relationships, issued from an “empirical
geometry” which, through abstraction, becomes geometric theorems. Far from
being the cause of perceptions, mathematics constitute its effects.
Thus, because it is drawn from the difﬁculties of Cartesian philosophy, the
Condillacian theory of perception sheds light on mobility technologies more than
Descartes would have done: cleared of perceptual semiotism, which certainly had
merit to explain their operation, but makes their use difﬁcult to conceive, Condillac’s
theory, based on the suspension of causal considerations and the objectiﬁcation of
sensations, can account for the genesis of perception and the active role of the
perceiving subject. Far from thinking of perception in terms of a mental “tableau”, or
painting, imprinted in the mind by presupposed objects, Condillac develops, in
1754, a dynamic and poetic conception of perceptual phenomena, and makes a
paradigm of the blind man with the stick. However, there will be those who ask what
interest there is in mobilizing empiricism, when phenomenology no longer needs to
prove its fertility. This is why in last place we will weigh Condillac up against the
most pivotal of contemporary philosophers on perception: Maurice Merleau-Ponty,
to examine the theoretical and practical beneﬁts of Condillacian empiricism.
4
Condillac Confronted with Merleau-Ponty
Phenomenology in general, and that of Merleau-Ponty in particular, are regularly
invoked when it comes to distinguishing the space of science from that, irreducible
to it, of perception,33 and notably when insisting on the dynamic and incarnate
character of the perceptual act, which is not conceptualizable in the classic terms of
“representation” or of “tableau”. For what concerns us here more speciﬁcally,
phenomenology comes to serve the understanding of ICT and of their use, which, in
33For example, see [13].
Mobility Technologies for Visually Impaired People …
97

turn, comes to conﬁrm his assertions: TVSS or newer devices support the
Merleau-Pontian thesis of the ability of technical objects to reconﬁgure “the actu-
alization, the implementation, of the constituent power residing in one’s own body”
(our translation, Lenay et al. op. cit., p. 81). In order to appreciate what these
devices have to tell us, what is different, or what they add when we consider them
through the prism of the Treatise on the Sensations, we will start by examining
how, for his part, Merleau-Ponty conceptualizes the cane of the blind man, dif-
ferentiating it from what he assesses to be a wrong conception—that is to say,
Descartes’, but which is in reality closer to Condillac’s. We will, however, see that
Merleau-Ponty’s objections do not pull down the sensualist conception of per-
ception, and will eventually show how mobility technologies can be conceived of as
Condillacian instruments.
4.1
The Merleau-Pontian Theory of the Cane
In Chap. 4 of the ﬁrst part of his Phenomenology of Perception, Merleau-Ponty
undertakes the task of illuminating the concept of the “synthesis of one’s own
body” through the analysis of habit, and makes the following observation:
Learning to ﬁnd one’s way among things with a stick, which we gave a little earlier as an
example of motor habit, is equally an example of perceptual habit. Once the stick has
become a familiar instrument, the world of feelable things recedes and now begins, not at
the outer skin of the hand, but at the end of the stick. (p. 152)
For Merleau-Ponty, as for Condillac before him, the stick of the blind man does
not allow him to see, but to extend the perceptions of touch beyond his earthly
body, and thus of the body itself beyond its natural organs. And, like Condillac, but
also Descartes, Merleau-Ponty considers that the perceptions mediated by the stick
are not of the same kind as those that the hand enables, and instead come from an
new universe: perceptual habit is, for him, in general, synonymous with “possession
of a world” (p. 153)—it is precisely this that the stick reveals. But before formu-
lating this thesis, Merleau-Ponty is careful to indicate, in order to challenge, the
interpretation that we are tempted to make on the emergence of this new world:
One is tempted to say that through the sensations produced by the pressure of the stick on
the hand, the blind man builds up the stick along with its various positions, and that the
latter then mediate a second order object, the external thing. (p. 152)
However, this double construction, that of the stick, then the object it reaches,
strictly corresponds to the Condillacian theory of the cane—according to which, we
will remember, the perception of the object is conditioned by that of the stick, and
thus takes place in two stages. But, as Merleau-Ponty has previously indicated:
In the exploration of things, the length of the stick does not enter expressly as a middle
term: the blind man is rather aware of it through the position of objects than of the position
of objects through it. (p. 143)
98
M. Chottin

The length of the stick inevitably intervenes in the apprehension of objects
(indissociable from the stick itself, the length conditions the possibility of perception)
but does not act “expressly”, or subjectively, as mediate perception. The perception
of the stick is rather, according to Merleau-Ponty, the effect of its manipulation and of
objective encounters—of the kind that—where his view diverges from Condillac’s
—perceptual habit (which is, at the same time, always motor) modiﬁes the originary
spatiality of the body itself. It is for this reason that, for Merleau-Ponty, technical
objects reconﬁgure the constituent power of the body itself: in mediated perception,
emerges, by habit, a new constituent power, giving new objects, which at the same
time form its realization. According to Merleau-Ponty, the blind man’s cane thus
constitutes the object of a becoming-organ, which is not the instrument of bypassing
divine laws (Descartes), or, as we will see, that of the objectiﬁcation of sensations
(Condillac), but the site of a reciprocal constitution of an augmented body and its
new objects. The cane reveals, by the same token, the reciprocal constitution of the
subject and the object, at work, in the same way, in ordinary perception.
Continuing his description of the interpretation (tempting but faulty) of the
genesis of perception, Merleau-Ponty then writes “It would appear in this case that
perception is always a reading off from the same sensory data, but constantly
accelerated (…)” (p. 152). Yet, when Condillac describes how his statue gradually
learns to perceive a single object, instead of the two objects that his crossed sticks
make its ﬁrstly (and falsely) apprehend, it is exactly this acquisition of speed
through the repetition of experience that he describes. Deprived of habit, the statue
“will often believe that it has touched two objects when in fact it has only touched
one” (Treatise on the Sensations, p. 253–254)34 . For Condillac, the correction of
this belief rests on the acquisition of a judgement, hidden from consciousness by
habit, that shortens its duration, and in some way, speeds up the time taken:
This judgement will be at ﬁrst only the result of quite slow reasoning. The statue will say to
itself as it were: these sticks can only cross if the extremity of the one in my left hand is on
my right. As a result, the objects that they touch are in the opposite positions from those of
my hands (…). Subsequently, this line of reasoning will become so familiar and will be
executed so rapidly that the statue will judge the positions of objects without appearing to
give the least attention to the positions of its hands. (p. 254)
The “without appearing to” is decisive: as the premise of the reasoning that the
statue continues to progress in this position, the perception of the sticks by his
hands would not be eliminated in favour of the objects perceived at the other end,
nor become unconscious—since his Essay on the origin of human knowledge,
Condillac rejects the Leibnizian thesis of “small perceptions” in the name of his
own method, based on the primacy of perception on consciousness in the devel-
opment of knowledge. Thus, the conception that Merleau-Ponty challenges in these
34On his side, Descartes only indicated, on the topic of his blind tailor and manual exploration, that
he “does not judge a body to be double although he touches it with his two hands” (18, 19, p. 170)
—without describing the process which according to him arises out of bypassing the laws of the
union.
Mobility Technologies for Visually Impaired People …
99

pages of the Phenomenology is much closer to Condillac’s than to Descartes’,
which, as we have seen, rests within the laws of the union—and not in the con-
sciousness of the subject—the principle of perceptual efﬁcacy.
For Merleau-Ponty, the construction of a perceptual world by means of a stick
does not come out “of any quick estimate or any comparison between the objective
length of the stick and the objective distance away of the goal to be reached”
(p. 143)35. Such an explanation of mediated perception (and, hence, ordinary
perception) amounts to, according to him, giving oneself that which we claim to
realize, that is, the world: that objects are perceived from some initial data pre-
supposes the position, that is to say, in the end, the perception of these objects. This
circle of the causal (or causalist) explanation of perception, that he implicitly
mobilizes here, constitutes the principle objection that Merleau-Ponty addresses to
the scientiﬁc (more precisely, physicist) point of view, to which he contrasts
understanding, and the immediate apprehension of meaning at work in the per-
ceptual act.36
However, if it so seems, at ﬁrst, that he has the Condillacian (or at the least the
empiricist37) theory of the blind man’s cane in his sights, Merleau-Ponty moreso
targets,
in
these
pages,
intellectualism,
and
more
speciﬁcally,
perceptual
Cartesianism. Here indeed is what he writes, quickly indicating the interpretation
that he judges faulty: “But habit does not consist in interpreting the pressures of the
stick on the hand as indications of certain positions of the stick, and these as signs
of an external objects, since it relieves us of the necessity of doing so” (p. 152).
According to him, the experienced blind man does not perceive two objects, the
stick and that which it touches, but, thanks to the repetition of experience, the latter
to the exclusion of the ﬁrst. The individual who, inversely, needs to perceive their
arm in order to apprehend an object, suffers from a perceptual disturbance (for
example, psychic blindness), whose nature consists precisely of producing a
sequencing, and requiring an analysis of the perceptual act, of which the healthy
individual has no need. In so-called “normal” perception, under which touch
mediated by a stick can be categorized, there are never two stages that succeed one
another: if the blind man starts by perceiving simultaneously his cane and the object
it strikes, according to a reciprocal constitution of one by the other, the habit has the
effect of erasing the perception of the cane in favour of that which it touches, and
which, in turn, modiﬁes the boundaries of myself and not-myself: the object
recedes, while the subject, or lived body, augments to the becoming-organ of the
stick. However, it is important for us here to note that through the eyes of
Merleau-Ponty “the pressures of the stick on the hand”, which, according to the
faulty interpretation, persist in ﬁnal perception, constituting “signs” that it is a
35We specify here that this is not exactly what Condillac maintained. The reasoning that his statue
performs does not carry over pre-existing objective data—we will see that he does not fall into the
circle that Merleau–Ponty implicitly denounces here.
36On the idea that such a “meaning” can be interpreted in rationalist terms, and thus take part in the
contemporary quarrel over conceptual contents of perception, see Bimbenet [6], in particular p. 22.
37It would seem that Merleau–Ponty has never read Condillac.
100
M. Chottin

matter of “interpreting”. Yet, according to him, all semiotism is an intellectualism,
and reciprocally:
Intellectualism cannot conceive any passage from the perspective to the thing itself, or from
sign to signiﬁcance otherwise than as an interpretation, an apperception, a cognitive
intention. (p. 152)
And we are aware that as such it must be rejected. Moreover, according to
Merleau-Ponty, empiricism features nothing of a theory that makes a construction
(whether sensory or intellectual) of the perceived world: the “empiricist con-
sciousness”, according to him, “constitutes nothing at all” (p. 28), but merely
passively receives “impressions” or “sensations”, which, combined together, form
objects. Under this framework, perception is conceived of as “purely a matter of
knowledge, a progressive noting down of qualities and of their most habitual dis-
tribution, and the perceiving subject approaches the world as the scientist approa-
ches his experiments” (p. 24). This Merleau-Pontian reading, now widespread, is
perhaps issued from the generalization, on empiricism in general, of the principle
traits of Humean philosophy, via the works of Kant, who, for posterity, made of
Hume the principal representative of the empiricist philosophy. Insofar as Condillac
is manifestly situated diametrically opposite to this reading, should we say that
Merleau-Ponty’s texts reveal, conversely, the intellectualism of the Treatise on the
Sensations?
Certainly, Condillac conditions certain mediated perceptions as the actualization
of a “reasoning”. Moreover, if this conditioning seems restricted to perceptions
mediated by two sticks, and does not apply to the manipulation of the ordinary cane,
the author of Treatise on the Sensations writes elsewhere that “judgments are mixed
in with our sensations whatever the sensory organ by which they are transmitted to
the mind” (p. 289, our emphasis). For example, to acquire by sight the idea of the
triangle, it is necessary to bring a whole series of judgements:
It [sc. the eye] grasps the whole of the simplest shape only when it has analyzed it, that is,
when it has observed all its parts in succession. It must make a judgement about each
particular part, and another judgement that combines them. It must say to itself: here is one
side, here is a second, and here is a third; here is a space bounded by these three sides, and
from all that, this triangle results. (p. 217)
Likewise, the blind man that manipulates his stick objectiﬁes his ﬁrst sensations
and transforms them into perceptions (of objects) by performing a series of
judgements. This conception is the one and the same that Merleau-Ponty classiﬁes
as “intellectualist” and which tasks himself with challenging.
38 According to him, the principle thesis that we need to dismiss is precisely that
which makes judgement the operator of the transformation of sensation into per-
ception. If, undeniably, Condillac makes it his, and consequently considers that the
objectiﬁcation of sensations, or the act that we have classiﬁed as “poetic”, is per-
formed under the auspices of judgement, as he sees it, intellectualism is more of as
38On the Merleau-Pontian critique of intellectualism, see L. Angelino [2]
Mobility Technologies for Visually Impaired People …
101

aside to the reduction of perception into sense impressions: according to him, to
afﬁrm that to perceive consists in receiving, via the ﬁve senses, objects already
formed, accounts to an explanation of perception by a device susceptible to pro-
ducing shapes in the mind, i.e. by laws, mathematical but hidden from con-
sciousness, of the union of the soul and the body—that is, at the end, and, as
Merleau-Ponty will say, under the presupposition that there is something to explain.
Since he conditions the perceptions mediated by the stick as the actualization of a
reasoning (or a series of judgements), Condillac avoids this circle: it is not a matter
of, for his statue, comparing the series of objective data, provided in advance, but,
through attention, to generate what Descartes presupposed, that is, sensed objects.
Reciprocally, the position of a series of judgements at the heart of perception is
not, for Condillac, anything but that intellectualist. Let us now turn back to the
concept of “empirical geometry”, remembering that it operates without signs or
symbols, by the acquisition of sensorimotor schemas: it has come time to build
from it the conditions of possibilities. In other words, how can one geometrize
without reasoning, or even have consciousness of judging? For the author of
Treatise on the Sensations—and therein lies his principal tour de force—an
empirical geometry can inform our perceptions, because the judgements, like
reﬂection, desires, and passions, are reducible to “transformed sensations”.39 In
effect, for Condillac, once we receive two sensations simultaneously, we sense that
one is not the other: we compare them. Yet to compare is none other than to judge:
“A judgement is (…) only the perception of a relation between two ideas that are
compared” (p. 180). In other words, a judgement is none other than an establishing
of relations between sensations.
Far from making an exception to such sensualism, the blind man with the stick
makes a contribution to its exposition, and attests that the manipulation of the cane
elicits the acquisition of new perceptions of relationships, which are purely sensory:
the individual that learns to perceive with a stick is not conscious of reasoning,
because his judgements are solely the sequence of sensations. Restored back to
sensation, the act of judging passes necessarily unnoticed by a consciousness that
seeks within itself an intellectual act, conceived as a way of understanding. Thus
envisaged, perception has nothing of a “mind’s inspection”, understood as a syn-
thesis of the soul actualized on the affected body. To the extent that judgements
come from what is sensed, perception seems moreso the site of the reciprocal
formation, via the body in movement, of the perceiving subject and the object
perceived. Thus, far from revealing the intellectualist dimension of Condillac’s
thought, the Merleau-Pontian analyses highlight its originality and the impossibility
to reduce the classic theories to what the Phenomenology of Perception says.
However, if he seems to disregard the Condillacian sensualism, Merleau-Ponty has
nonetheless grasped, in empiricism, a dimension closely corresponding to the theses
39Cf. ‘The Plan of This Work’, p. 171: “Judgement, reﬂection, desires, passions and so forth are
only sensation itself differently transformed”.
102
M. Chottin

of Condillac, and whose examination will also contribute to appreciating their
theoretical and practical fertility.
4.2
Theoretical and Practical Contributions
of the Condillacian Theory of Perception
If Merleau-Ponty is mistaken when he classiﬁes as “intellectualist” any conception
that places judgements at the heart of perception, and also when he reduces
empiricism to a theory of perceptual passivity, he does, however, have a good grasp
on the primacy that it accords to the concept of sensation. A touch coarser than
intellectualism, in his opinion empiricism holds several of its own ﬁctions, with
sensation ﬁguring ﬁrst and foremost—of which the ﬁrst chapters of the
Phenomenology intend to show nothing less than its inexistence.
By no means constituent, but instead constituted by the (empiricist, but also
Kantian) subject who, a posteriori, separates matter and form of the sensory, sen-
sation is, as such, a pure ﬁction. In other words, all consciousness, according to
Merleau-Ponty, is necessarily consciousness of objects. It sufﬁces to think of per-
ception to notice this: never do we have sensation of hot, of cold, of yellow or of
red, that appear to us other than under the features of one or multiple objects, even
if they be poorly determined. Thus, for Merleau-Ponty, perception is not derived
from sensation, and does not consist of an objectiﬁcation of the subjective. In
contrast to Condillac, who maintains that all consciousness is, before all else, a
consciousness of the self, and only becomes consciousness of an object through
touch and movements of the body, Merleau-Ponty writes:
More generally, it is the very notion of the immediate which is transformed: henceforth the
immediate is no longer the impression, the object which is one with the subject, but the
meaning, the structure, the spontaneous arrangement of parts. (p. 58)
If there is a perceptual learning period in Phenomenology of Perception, it
consists of a circulation of less determined objects and more determined objects,
and nowhere a transition from (subjective) sensation to (objective) perception.
Certainly, as [9] has highlighted, the phenomenological critique of sensation as a
ﬁction does not quite reach Condillacian sensualism, insofar as Condillac himself,
from the Essay onwards, maintains that as adults, we no longer have sensations—
but only perceptions, or ideas of objects. However, Merleau-Ponty generalises
somewhat this empiricist thesis and afﬁrms that we have never done anything other
but
perceive—what
Condillac
sets
himself
precisely
to
challenging.
The
Merleau-Pontian refusal to keep sensation as origin derives equally from his
rejection of any genetic conception of the mind: according to him “all idea of a
genesis of mind is a hybrid idea, because it puts back into time the mind for which
time exists” (p. 45, note). We have here, it seems, a new obstacle of Kantian origin
for the appreciation of continental empiricism.
Mobility Technologies for Visually Impaired People …
103

If, for Merleau-Ponty as for the abbé, the cane allows the tactile word to retreat,
and enables, far from all semiotism, a “projective” conception of ordinary per-
ception40 , his conception thus has nothing “poetic” about it, in the sense that we
encountered previously. The Merleau-Pontian blind man with the stick does not
spread out into the world, does not construct objects through the means of his
interiority, but encounters otherness and circulates in a world of sensory objects, to
which his body itself belongs. Yet, there is a ﬁrst advantage, on the theoretical level,
to do the inverse and base the genesis of perception on the level of sensation: unlike
Merleau-Ponty, Condillac theorizes the apparition of objects for consciousness and
is not satisﬁed with the phenomenological “there is” (“il y a”). If, through epoché
the former intends to ﬁnd the emergence of the object, the latter goes further back,
by virtue of the ﬁctive nature of the statue, to a state prior to objective perception,
and therein conceives of its formation. In addition to the theoretical gains for
empiricism on this point, we can also highlight the practical beneﬁts. Not only, as
we have seen, does the Condillacian conception of the blind man’s cane provide,
based on the objectiﬁcation of sensation, a frame of thought for the examination of
mobility technologies, but presents the advantage, over phenomenology, of joining
up with Bach-y-Rita’s descriptions, which, support it back in turn41: everything that
Merleau-Ponty rejects as faulty interpretation of the blind man’s cane—the con-
stitution of the object perceived by that of the medium used, the presence of
judgements in the genesis of perception, the transition from sensation to perception,
and that we ﬁnd, on the other hand, word for word in Treatise on the Sensations, is
also found verbatim from the pen of Bach-y-Rita. We will not quote here the
passages in their entirety, but only these lines, in which Bach-y-Rita implicitly
distinguishes sensation and perception, and insists on the ability of the subject to
move from one to the other:
Subjects using the TVSS learn to treat the information arriving at the skin in its proper
context. Thus, at one moment the information arriving at the skin has been gathered by the
TV camera, but at another it relates to the usual cutaneous information (pressure, tickle,
wetness, etc.). The subject is not confused; when he/she scratches his/her back under the
matrix nothing is “seen.” Even during task performance with the sensory system, the
subject can perceive purely tactile sensations when asked to concentrate on these sensa-
tions. (op. cit., p. 501)
The sensations felt on the back do not constitute the simple “objective” condi-
tions necessary for distance perception—as does the length of the stick for
Merleau-Ponty, who refuses however, that its knowledge intervenes in the genesis
of perception. Not only do these sensations constitute the subjective conditions of
the perceptions of objects, but as evidenced by the fact that it is possible at any time
to return to them, they do not end up eliminating themselves to their own beneﬁt.
40“In the gaze we have at our disposal a natural instrument analogous to the blind man’s stick” [29,
p. 153]. “I have only to see something to know how to reach it and deal with it” [30, p. 124].
41On the relationships between ICT and philosophical theories, see, for example, Declerck, op. cit.,
p. 207.
104
M. Chottin

As a result, attention is not, as Merleau-Ponty believes, reducible to a creative
faculty42: in accordance with what Condillac says, it is moreso what is given that
creates it, or at the very least elicits it in experience—tickling, for example.
We will highlight a ﬁnal theoretical beneﬁt of Condillacian empiricism: its
ability to conceptualize, under the term “transformed sensations”, what intellectu-
alism failed to consider—namely, the “mystery” of a perception steeped in
judgements, which, however, elude consciousness. We have already come across
this: once we show that to judge is to sense, the problem is resolved. Ignorant of this
solution, Merleau-Ponty attributes to the body the act of synthesis of which,
according to him, intellectualism made of the soul the only subject: “The cultivation
of habit is indeed the grasping of a signiﬁcance, but it is the motor grasping of a
motor signiﬁcance” (p. 143). After having taken the example of dance and high-
lighted, rightly, that its learning does not consist of an analysis of the formula of
movement and its application to the body-object, he adds: “As has often been said,
it is the body which ‘catches’ and ‘comprehends’ movement” (ibid.). Similarly, he
continues a few lines further on:
If I want to get used to a stick, I try it by touching a few things with it, and eventually I have
it ‘well in hand’, I can see what things are ‘within reach’ or out of reach of my stick. (ibid.)
Merleau-Ponty can thus conclude that “habit has its abode neither in thought nor
in the objective body, but in the body as mediator of a world” (p. 145). But how are
we to conceive of this “knowledge in the hands” (p. 144)? If, for Condillac no more
than for Merleau-Ponty, objects perceived by virtue of the cane are not “as objective
positions in relation to the objective position occupied by our body; they mark, in
our vicinity, the varying range of our aims and our gestures” (p. 143), the Treatise
on the Sensations does not situate the principle of habit or of the genesis of per-
ception in the body to the exclusion of thought: his concept of “transformed sen-
sation”—in this case, sensory judgement—precisely overcomes the alternative, and
in effect the dualism, to which Merleau-Ponty remains partly prisoner43: once we
understand that the mind is but itself a collection of transformed sensations, we can
recognize in it a capability for synthesis, without placing it in a position of control
from above in regard to its body and to objects, but above all, we will be able to say
what it is that constitutes this “knowledge in the hands” (of perceptions of rela-
tionships), and guide the learning of mobility technologies from there. Instead, the
Merleau-Pontian discourse condemns itself, on this point, to resort to quotation
marks, to mobilize expressions of opinion—largely remaining within intellection—
and, conversely, going beyond the intelligible to take on the aspect of a poetics of
perception, in the narrow sense of the term.
42P. 28: “(…) the two doctrines, then, have this idea in common that attention creates nothing,
since a world of impressions in itself or a universe of determining thought are equally independent
of the action of mind”.
43On the persistence of a certain dualism in Merleau–Ponty, see Barbaras [5, pp. 94–95].
Mobility Technologies for Visually Impaired People …
105

5
Conclusion
Among the classic theories of perception, we have selected two, for the prime
position they occupy to with regards to each other on the topic on the blind man’s
cane. We have ﬁrstly examined the manner in which Cartesian philosophy con-
ceived of this instrument of mobility and of perception, and we have shown that,
beyond the comparison that he establishes in the Optics, between the way in which
a blind man perceives by means of a stick, and the process of engendering vision,
Descartes informs our knowledge of the cane for itself. On the contrary to what a
quick reading of Cartesianism may suggest, the author of the Optics does not reduce
it to a mechanical appendage of a bodily machine, on which the soul would learn to
measure, calculate and ﬁnally construct objects which can be reduced to portions of
space or extension. He much more envisages the cane as analogous to a new organ,
in that it bestows upon the blind man a sixth sense and opens up to him a new
universe, composed of perceptions, in light of our own natural sensibility, are
neither entirely visual, nor entirely tactile, but well and truly “sensible”. These
perceptions do not arise from a subject’s calculations on ﬁrst sensations, but result
from a reconﬁguration, through habit, of the laws uniting the soul and the body,
which, for their own part, continue to follow geometrical theorems. The Cartesian
conception of natural perception allows us, all things considered, to bypass the
latter through the institution of a new code, and at the same time, make sense of any
device that, like TVSS, creates perceptions. In the terms of contemporary neuro-
science, we would say that neuroplasticity is such that it enables ordinary corporal
impulses to signify extra-ordinary perceptions.
If Descartes could deploy this conception of the cane only by means of a thought
experiment, if he had at hand a good “point of view” for thinking of the way in
which a blind person learns to step into this new universe; his use of the blind man’s
stick, as much as it remains subordinate to a causalist and semiotic theory of vision,
had nevertheless restricted him to not being able to offer an account of this process
of learning: under this framework, we do not understand how the perceiving subject
can associate new meanings to cerebral movements of which he has neither
knowledge, nor even consciousness—which is, however, required from the moment
we make recourse to the concept of “institution”. Perceptual semiotism, at least
conceived of as such, seems destined to fail.
This is why we have considered a second theory, that of Condillac’s Treatise on
the Sensations, based on a rejection of the causalist approach and the idea of the
union of the soul and the body governed by a set of pre-instituted laws. For the
abbé, the blind man’s cane is the model of a poetic conception of perception, in
which perceived objects constitute the projections of sensations, constructed by the
subject himself through the movements of his body, driven by the pursuit of
pleasure. In this framework, the cane is not an instrument of a mysterious recon-
ﬁguration of the laws of the union, but an objectiﬁcation of sensations analogous to
those that are produced for the four other senses during the ﬁrst few months of our
life. This objectiﬁcation is realized through judgements, realized by the subject
106
M. Chottin

himself, which are not pre-existent to sensations, but emerge from them, and are
nothing but the sensory apprehension of their relationships. The cane appears thus
as the means, pre-constituted by the movements of the body by which the perceived
object and the subject constitute one another. By itself, the Condillacian exploration
of the cane thus sufﬁces to rectify the conception that we commonly make about
empiricism.
We have lastly moved closer the Condillacian theory of the stick with that which
Merleau-Ponty rejects in the Phenomenology of Perception, and introduced his
defence, in which the blind man’s cane constitutes one of the organ-objects which,
along with the car, the hat, and the typewriter, allow us to think afresh about the
body and its objects: far from being an object of thought for a disembodied mind,
the body constitutes, in his view, a “lived body”, which ﬁnds itself with the per-
ceived object in a reciprocal relationship. An instrument that is susceptible to, by
habit, augmenting the body itself, the cane, in the same exact manner, constitutes its
objects at the same time that it is constituted by them. Having become analogous to
an organ, it at once seizes its objects, without the spirit reﬂecting on any of its
(alleged) sensations. For Merleau-Ponty, perceptual development does not consist
of a transition from sensation to perception, but an encounter better determined in
relation to otherness. In the end, we have emphasized that, on this point, ICT—at
least the TVSS of Bach-y-Rita—corresponds more to Condillacian descriptions,
which, in turn, illuminates them better than the phenomenology of Merleau-Ponty.
First and foremost, Condillac’s philosophy merits putting the concept of “sensa-
tion” in prime position, which mobility technologies do not seem able to do
without. It thus reveals that mediated perception enables not only the emergence of
new worlds, but also new judgements, and participates in the genesis of the mind44.
It should now be a matter of appreciating the fruitfulness of sensualism in the light
of newer technologies, and to examine how they can in turn actualize and nourish it.
References
1. Alquié F (1963) édition des Œuvres philosophiques de Descartes, vol. 3 . Classiques Garnier,
Paris
2. Angelino L (2007–2008) Merleau-Ponty et la critique des ‘intellectualismes’. Philonsorbonne
2:11–30
3. Bach-y-Rita P (2002) Sensory substitution and qualia. In: Noë A, Thompson E (eds) Vision
and mind. Selected readings in the philosophy of perception. The MIT Press, Cambridge,
pp 497–514
4. Barbaras R (2001) Merleau-Ponty et la psychologie de la forme. Les Études philosophiques 2
(57):151–163
5. Barbaras R (2010) Renaud Barbaras répond aux questions de F.-D. Sebbah. Rue Descartes 4
(70):88–105
44On the reversal of the form/consciousness relationship in later Merleau-Ponty, see Barbaras 4,
p. 162.
Mobility Technologies for Visually Impaired People …
107

6. Bimbenet E (2010) Merleau-Ponty et la querelle des contenus conceptuels de la Perception.
Rue Descartes 4(70):4–23
7. Cavaillé J-P (1991) Descartes. La fable du monde. Vrin, Paris
8. Charrak A (2000) Géométrie et métaphysique dans la Lettre sur les aveugles de Diderot.
Recherches sur Diderot et l’ Encyclopédie 28:43–53
9. Charrak A (2009) Empirisme et théorie de la connaissance. Réﬂexion et fondement des
sciences au XVIIIe siècle. Vrin, Paris
10. Chottin M (2009) L’aveugle aux bâtons face à l’aveugle de Molyneux: le rationalisme à
l’épreuve de l’empirisme. In: Chottin M (ed) L’aveugle et le philosophe, ou comment la cécité
donne à penser. Publications de la Sorbonne, Paris, pp 83–106
11. Chottin M (2014) Le partage de l’empirisme. Une histoire du problème de Molyneux aux
XVIIe et XVIIIe siècles. Honoré Champion, Paris
12. Condillac EB (de) [1754] (1982) Treatise on the Sensations, in Philosophical Writings of
Etienne Bonnot, Abbé de Condillac, translated by F. Philip, with the collaboration of H. Lane,
Lawrence Erlbaum Associates, Hillsdale, London, vol. I
13. Declerck G (2011) Physique de l’espace et phénoménologie de l’espace. Philosophia Scientae
15(3):197–219
14. Declerck G, Lenay C, Khatchatourov A (2009) Rendre tangible le visible. Ingénierie et
Recherche biomédicale 30(5–6):252–257
15. Degenaar M [1992] (1996) Molyneux’s problem: three centuries of discussion. Kluwer
Academic Publishing, Dordrecht
16. Descartes R (1985) The philosophical writings of Descartes, translated by J. Cottingham, R.
Stoothoff and D. Murdoch, Cambridge University Press, Cambridge
17. Descartes R [1628] Rules for the direction of the mind. vol. I
18. Descartes R [1637] Discourse on the method. vol. I
19. Descartes R [1637] Optics. vol. I
20. Descartes R [1641] Meditations on ﬁrst philosophy. vol. II
21. Descartes R [1647] Principles of philosophy. vol. I
22. Descartes R [1648] Description of the human body. vol. I
23. Descartes R [1649] The passions of the soul. vol. I
24. Descartes R [1664] Treatise on light. vol. I
25. Lenay C, Sebbah F-D (2001) La constitution de la perception spatiale. Approches
phénoménologique et expérimentale. Intellectica, 1, 32:45–85
26. Le Ru V (2000) La Lettre sur les aveugles et le bâton de la raison. Recherches sur Diderot et
sur l’ Encyclopédie 28:25–37
27. Le Ru V (2009) L’aveugle et son bâton ou comment Descartes résout l’énigme de la
communication de l’action ou de la force mouvante. In: Chottin M (ed) L’aveugle et le
philosophe, ou comment la cécité donne à penser. Publications de la Sorbonne, Paris, pp 15–26
28. Marion J-L [1981] (1991) Sur la théologie blanche de Descartes. Analogie, création des
vérités éternelles et fondement. PUF, Paris
29. Merleau-Ponty M [1945] (1962) Phenomenology of perception. London, Routledge and K.
Paul, Humanities Press, New York
30. Merleau-Ponty M [1960] (1993) Eye and mind. In: Johnson GA, Smith MB (eds) The
Merleau-Ponty aesthetics reader: philosophy and painting. Northwestern University Press, St
Evanston
31. Morgan MJ (1977) Molyneux’s question: vision, touch and the philosophy of perception.
Cambridge University Press, Cambridge
32. Pissaloux E, Velazquez R, Hersh M, Uzan G (2016) Towards a cognitive model of human
mobility: an investigation of tactile perception for use in mobility devices. J Navig 1–17
33. Reich L, Maidenbaum S, Amedi A (2012) The brain as a ﬂexible task machine: implications for
visual rehabilitation using noninvasive vs. invasive approaches. Curr Opin Neurol 25:86–95
34. Serres M (1968) L’évidence, la vision et le tact. Etud Philos 2:191–195
35. Warren DH, Strelow ER (1984) Learning spatial dimensions with a visual sensory aid:
Molyneux revisited. Perception 13:331–350
108
M. Chottin

Part II
Neuro-cognitive Basis of Space
Perception for Mobility

The Multisensory Blind Brain
Vanessa Harrar, Sébrina Aubin, Daniel-Robert Chebat, Ron Kupers
and Maurice Ptito
1
Introduction
The brain is a fascinating organ. It has the incredible ability to turn electrical,
mechanical, and chemical energy into multisensory knowledge about the world.
Neuroscientists are taught that the brain is divided into regions—each responsible
for interpreting information from a single sense, except a few integrative multi-
sensory areas whose role is to combine information from the different senses
(Fig. 1). For example, the posterior part of the brain, the occipital cortex, interprets
visual information; the area above the ears, the temporal cortex, interprets auditory
information; above from that towards the middle of the head is the somatosensory
sulcus, which interprets tactile stimulation and generates motor actions. This idea
that the mind is composed of distinct faculties, with separate seats in the brain, took
origin from same kind of categorical location-based ideology as sixteenth century
V. Harrar (&)  S. Aubin  M. Ptito
Chaire de recherche Harland Sanders en Sciences de la Vision,
École d’Optométrie, Université de Montréal, Montréal, Canada
e-mail: vanessa.harrar@umontreal.ca
D.-R. Chebat
Visual and Cognitive Neuroscience (VCN Lab), Department of Behavioral
Sciences & Psychology, Ariel University, Ariel, Israel
e-mail: dchebat00@hotmail.com
M. Ptito
Laboratory of Neuropsychiatry and Psychiatric Center Copenhagen,
University of Copenhagen, Copenhagen, Denmark
e-mail: ptito.maurice@gmail.com
R. Kupers
Brain Research and Integrative Neuroscience Laboratory, Danish Center for Sleep Medicine,
Department of Clinical Neurophysiology, Rigshospitalet, Glostrup, Denmark
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_4
111

phrenology. Yet, we now know that this speciﬁcity hypothesis is largely incomplete
and a fallacy. Studies into the plasticity in the blind brain have revealed that the
occipital cortex is not solely reserved for visual functions. In fact, blind people
trained in spatial tasks, such as braille reading, or on sensory substitution devices,
need the occipital cortex to interpret these non-visual stimulations.
In the second and third sections of this chapter, we will review the literature on
the traditional multisensory brain areas. We argue that multisensory integration is a
core aspect of human survival. In the fourth and ﬁfth sections, we will review the
literature on multisensory integration in areas of the brain that were classically
considered to be modality-speciﬁc, and demonstrate that these areas are also active
in the integration of information from multiple modal sources. We will debunk the
myth that the visual areas of the brain are strictly visual. In the ﬁnal section of this
chapter, we argue in favour of the idea that the brain is divided according to
function rather than modality. Information is, therefore, represented in an amodal
manner, i.e. in a way that it is abstracted from its modal source. We further argue
that cognitive processes of memory and even mental imagery can be amodal in
nature.
The impressive multisensory plasticity of the blind brain can, therefore, be
capitalised on in order to improve high-tech rehabilitation devices for people with
decreased visual abilities. Further, this evidence for cross-modal and cross-cortical
plasticity suggest possibilities of rehabilitating patients with brain trauma (e.g.
stroke) by accessing the latent multisensory pathways within the brain.
Fig. 1 Lateral view of the human brain indicating the various lobes and their primary functions.
Modiﬁed from Anatomy of the Human Body, Gray H, Fig. 728 (1918)
112
V. Harrar et al.

2
What Is Multisensory?
There are a few areas of the brain that have been labelled as multisensory areas, and
appear to be particularly important for integrating the information from multiple
senses. Multisensory areas are composed of multisensory neurons—meaning neu-
rons that respond to stimuli from multiple modalities. For example, neurons in the
deep layers of the cat’s superior colliculus (SC) respond to visual, auditory, and
somatosensory stimuli [92, 134], and with overlapping receptive ﬁelds [131]. Thus,
a stimulus located on the bottom left of the cat’s sensory ﬁeld could activate the
same neuron in the SC whether the stimulus is visual, auditory, or a brush of the
whisker. What makes these neurons particularly interesting is that they ﬁre more
frequently when stimulation come from two modalities, as compared to only one
modality [132, 135]. These superadditive responses exceed the strongest unisensory
response, and often exceed even the sum of the unisensory responses.
Investigations into superadditive neurons have revealed the principles of mul-
tisensory integration. The conditions that lead to increased neuronal activity also
lead to behavioural improvements; an enhanced signal would, thus, improve
detection (and lower threshold), allowing for faster responses with less variability—
especially for eye movements [30, 31, 37, 133]. The conditions under which
superadditivity and thus multisensory integration are likely to occur include: spatial
and temporal proximity [44], and inverse effectiveness [93]. That is, for stimuli to
be combined into a single event, they must occur in the same location (spatial
proximity), at the same time (temporal proximity), and should be relatively weak
(inverse effectiveness) in order to enable a substantial superadditive enhancement
effect. Of course, without boundaries (space and time) all stimuli would collapse
together. The question then becomes, how do multisensory neurons learn which
stimuli go together?
Multisensory integration is thought to have developed to maximise our chance of
survival by minimising the unisensory limitations of each sensory system. For
example, the eye transduces light at a rather slow rate and has a coarse perception of
time. In contrast, the auditory system has a mechanical transduction, which is
relatively fast and temporally precise, but is limited in the spatial plane. Thus, our
ability to spatially locate sounds is much less precise than our ability to locate visual
stimuli. When we are faced with danger, the survival predicament dictates that we
need to immediately detect the origin of the growling animal and either run away or
prepare to ﬁght. Many studies have demonstrated that the multisensory integration
within the SC allows us to orient our eyes to a sound. The theory is that the sound is
used to detect a rough location of the threat, and vision is used to determine what is
threatening. In less dangerous conditions, tested in laboratory settings, it has been
established that multisensory integration can sometimes be “optimal”—our per-
ception is a weighted sum of the total information available, with more weight
afforded to the sense with the least variability [1, 39]. Multisensory integration,
thus, optimises all four f’s necessary for our survival: ﬁght, ﬂight, feeding, and
fertility.
The Multisensory Blind Brain
113

Eating is an important multisensory activity [7]. Chefs have known this intu-
itively by making food that not only taste good but is also visually and texturally
appealing. The inﬂuence of vision on ﬂavour has been empirically tested; for
example the effect of colour—be it the colour of the food itself [130], or the colour
of the plate and tableware [61, 62] is known to greatly inﬂuence the feeding
experience (for a review see [129]). The sound a food makes in the mouth is also
important in our overall liking [147]. We will return to this idea of multisensory
experience in nourishment when we discuss taste and smell perception in blind
individuals (Sect. 4.4). It is already clear, however, that multisensory integration is
a crucial aspect of the human experience and is closely linked to many survival
responses.
3
Multisensory Areas in the Brain
In this section we review all the classical multisensory areas of the brain. We argue
that these multisensory areas are crucial for the development of body reference
frames and spatial representations, for interacting with objects, accomplishing
complex motor movements, and for determining one’s self-representation within
the world. These brain areas are capable of integrating information from multiple
modalities, rather than only a speciﬁc sensory modality. We argue that these areas
are organised according to functional speciﬁcity and combine multiple input
sources to create an amodal reference frame.
The posterior parietal cortex (PPC) has been identiﬁed as an important mul-
tisensory area within the brain, converging visual, auditory, and tactile information
for planning and executing movements. The PPC appears to be important for
reference frame transformations between the senses [19]. For example, when we
hear something, the perceived sound is encoded according to head-centred coor-
dinates (because our ears are ﬁxed on the head). In order to look at the source of the
sound, its location needs to be coded in an eye-centred frame of reference (relative
to the current eye position). Lewald and colleagues suggested that the PPC relates
azimuth angles of sounds to body coordinates, in order to convert the information to
different modality-speciﬁc reference frames [83]. To make a hand movement to the
seen/heard object, it would then need to be coded in a hand-centred reference frame.
Rather than undergoing arduous computations each time we move towards a sensed
object, the PPC appears to maintain a common eye-centred reference frame,
modulated by eye-, head-, body-, or limb position signals [26]. Indeed, even a touch
on the arm appears to be coded in the common eye-centred reference frame [59, 60,
109]. Further evidence of that the PPC is involved in integration comes from studies
using brain stimulation. Transcranial magnetic stimulation (TMS) over this region
disrupts multisensory visual–tactile integration [101], while transcranial direct
current stimulation (tDCS) over this area enhances multisensory spatial orienting
[14]. A common multisensory reference frame would facilitate communication
between the modalities (with regards to spatial location) and the PPC is, therefore,
114
V. Harrar et al.

considered an important multisensory area in the brain—particularly in spatial
coding, though it may also play a role in temporal perception [152].
The anterior intraparietal sulcus (aIPS), from the postcentral to the superior
parietal sulcus, is another known multisensory brain area with a particular role in
the integration of visual and haptic signals (for a review, see [64]). For example,
multisensory location information is needed to reach towards and pick up an object
that is seen on the table. Current understanding is that the proprioceptive knowledge
of the hand converges with the visual information for hand position in this area of
the cortex. While the posterior IPS and some visual areas (discussion to follow)
represent the hand in a predominantly visual manner, the aIPS converges multi-
sensory information (vision and an important proprioceptive contribution) to build
hand position representations for peripersonal hand space [50, 89].
The superior temporal polysensory (STP) region, in the posterior bank of the
superior temporal sulcus (STS), has been a labelled as and “association” area
because it receives auditory, visual, and somatosensory stimulation and is com-
posed of unimodal, bimodal, and even trimodal neurons [18]. Its receptive ﬁelds are
generally large, including both visual ﬁelds and bilateral somesthetic and auditory
receptive ﬁelds [65]. In particular, peripheral vision and motion perception appear
to be supported by projections from the STP and STS to the primary visual cortex
[41]. These parietal areas project to primary visual cortex (V1 and V2) [117]. This
same region, but a little higher at the temporoparietal junction (TPJ), has been
demonstrated in humans to be responsive to visual–tactile stimulation [68]. Once
again, this multisensory cortex is associated with spatial perception—more
speciﬁcally the feeling of being localised at a position in space, from a ﬁrst-person
perspective [13].
The anterior Ectosylvian Sulcus (AEC) in cats is an important anatomical
area because of its wealth of cortico-cortico as well as cortico-subcortical projec-
tions. In many cases, neurons in the AEC respond to multisensory stimuli [96, 113].
In one of the ﬁrst studies to demonstrate trisensory neuronal response (Fig. 2), Jiang
and colleagues reported that of the cells in the AEC, recorded with the single-unit
technique, *60% were unimodal, *30% were bimodal, and *10% were
trimodal [71].
Depending on how multisensory neurons are deﬁned, or tested, some studies
report as much as 66% of neurons in the AEC to be multisensory [35] with pro-
jections to the SC, which likely underlies the latter’s multisensory nature [70, 71].
While neurons in the SC always demonstrate maximum enhancement when their
unimodal discharges overlap [143], the relationship in the AEC appears to be more
complex. While some neurons in the AEC prefer auditory stimulation to precede
the visual stimulation, other neurons have peak responses when the auditory
stimulus trails the visual by as much as 200 ms [70]. Each neuron appears to have a
preferred temporal relationship between stimuli (see also [11]). Moreover,
depending on the temporal interval between the modalities, the same neuron can be
excitatory or inhibitory [70].
Given the already multisensory nature of this region, cross-modal plasticity
following sensory deprivation might be expected, where visually speciﬁc neurons
The Multisensory Blind Brain
115

may become responsive to stimuli in other modalities after a lack of visual stim-
ulation. Indeed, a signiﬁcant cross-modal plasticity of the AEC was observed in cats
that were visually deprived at birth [114]. Areas that were normally visual (the
ventral bank and fundus of the AES) underwent a cross-modal intrusion, becoming
primarily responsive to auditory and/or somatosensory stimulation.
These results in the cat provide a model for the intermodal compensatory
plasticity that is observed following sensory loss; an area that is primarily visual,
but demonstrates some measure of integration of auditory and somatosensory
information with vision, will undergo considerable plasticity when vision is not
available, becoming more responsive to auditory and tactile stimuli. Multisensory
areas therefore allow for cross-modal plasticity following loss of a particular sen-
sory modality. This transition from bimodal (or trimodal) into unimodal (or
bimodal) speciﬁcity is likely mediated by GABAergic neurons [36], with a corre-
sponding expansion of the remaining unisensory receptive ﬁelds into the area
previously occupied by the modality that has been eliminated. Thus, the
cross-modal abilities following sensory loss are maintained and strengthened, rather
than completely novel. Following up on this point, the subsequent sections of this
chapter will discuss cross-modal abilities in cortical areas that are traditionally
considered unisensory.
Fig. 2 Responses of a
trimodal unit to a visual (a),
an auditory (b), and a
somatosensory (c) stimulus.
Visual stimulus: a light bar
(3°  10°) moving upwards
through the RFs at a speed of
300°/s; somatosensory
stimulus: an air puff (100 ms);
auditory stimulus: a burst of
white noise (100 ms, 70 dB).
Black bars, visual stimuli;
thin line with ﬁlled bars,
auditory stimuli; “up bars”,
air puff stimuli. Binwidth
10 ms. Insets show positions
of RFs. The black dot in the
schematic coronal section of
the cat’s brain represents
approximate position of the
cell within anterior
ectosylvian cortex. Adapted
from [70], Copyright (1994)
116
V. Harrar et al.

4
Primary Visual Cortex
Visual information from the retina is projected from the eye through the optic tract
(including the optic chiasm) to the Lateral Geniculate Nucleus (LGN) before
arriving to the occipital cortex. Neurons in this part of the brain, combined with
other neurons ‘devoted’ to vision throughout the cortex, are said to take up about
30% of the cortex [54]. In comparison, approximately 8% of the cortex is estimated
to be devoted to tactile information processing, and 3% for auditory processing.
What, then, happens to these neurons when vision is restricted or completely
eliminated? Over the last ten years, neuroimaging studies have demonstrated a
considerable amount of neural plasticity in the “visual cortex” of blind people. This
plasticity has been used to explain the superior abilities of blind people in auditory,
tactile, proprioceptive and motion tasks (for a review, see [82]). We propose that
this plasticity potentially arises from the presence of multisensory neurons within
the primary visual cortex. These neurons would remain latent and largely immature
in the normal “visual” brain, but would become active in the blind brain as a result
of the absence of visual input.
4.1
Auditory Activation of Visual Areas
Auditory stimuli have repeatedly been demonstrated to activate the visual cortex in
early blind (EB) individuals, but not necessarily in sighted controls. The initial
ﬁndings that EBs have neural activity within the occipital cortex was a novel insight
and an explanation for anecdotal evidence of heightened auditory abilities in blind
people [33, 144]. Several studies have demonstrated superior performance of EBs
in auditory tasks as compared to sighted controls (SC), which has been attributed to
an increased metabolic activity in the visual cortex of CB during the performance of
these auditory tasks [6, 122]. Sensory substitution devices (SSD), such as the PSVA
(prosthesis substituting vision with audition), have also been used to demonstrate
the fact that auditory stimulation can activate the occipital cortex [34]. The activity
in the “visual cortex” is critical for interpreting the auditory information from
PSVA devices [28], for localising simple sounds [27], and for interpreting auditory
motion [106]. Moreover, the activation of the visual cortex from auditory stimu-
lation led to the functional specialisation hypothesis [29], which suggests that the
occipital cortex is specialised for spatial processing—rather than being strictly
associated to visual processing (this will be further discussed in Sect. 6). While this
plasticity in blind people is remarkable, and can be used to explain their often
superior auditory abilities, activation of the occipital cortex by auditory stimuli is
not always dependent on visual deprivation.
Auditory stimuli can activate the visual cortex, even in a brain that developed
“normally” (see review in [52]; Fig. 3). Half a century ago, studies in the cat
demonstrated that as many as 41% of “visual neurons” also responded to auditory
The Multisensory Blind Brain
117

stimuli [97]. In support of the functional specialisation hypothesis, these multi-
sensory neurons in the visual cortex were thought to be spatially speciﬁc [97]. With
spatially and temporally precise neuroimaging techniques, a recent study has
conﬁrmed that auditory stimuli evoke spatial-speciﬁc activity in the visual cortex of
normally developed sighted individuals [17], even in the absence of simultaneous
visual stimuli. The activation of the visual cortex by auditory stimuli is likely to
originate from early projections from the auditory cortex and the superior temporal
sulcus (STS) [117].
Taken together, we can conclude that a pathway for auditory stimuli to the
occipital cortex is present even in brains that have developed with visual input.
While these pathways are not completely pruned over the course of normal
development, they remain largely undeveloped. These multisensory connections
appear signiﬁcantly strengthened and have expanded cortical representation when
visual input is unavailable, for example in the blind brain, and become apparent
following training and experiences with non-visual stimuli [29].
4.2
Tactile and Proprioceptive Activation of Visual Areas
In the same vein as auditory stimuli, tactile devices that provide a complex touch
stimulus also activate the visual cortex in blind people, but not necessarily in
sighted controls. For example, in addition to somatosensory cortex response, blind
people’s occipital cortex is activated during braille reading but not for simple tactile
detection tasks [124]. This activation appears to be critical for interpreting and
understanding the tactile Braille stimulus, since Braille reading is impaired when
the occipital cortex is damaged [57] or disrupted with TMS [25]. While reading was
impaired, simple tactile detection was not when TMS was applied to the occipital
cortex [25], but tactile detection was disrupted when TMS was applied to the
parietal cortex [58]. Therefore, the visual cortex is actively involved in the repre-
sentation of Braille letters, but not their detection. It could, thus, be argued that the
visual cortex is actively involved in an amodal representation of text. The fact that
text is represented in such an amodal fashion could explain why the visual word
form area (VWFA), located within the visual cortex (in the ventral visual stream), is
activated when blind individuals read a braille text (tactile) and when sighted
individuals read a visual text [116].
Early blind people who are trained with the tongue display unit (TDU) also
activate the occipital cortex when determining the orientation of a letter presented
with the device [112] (Fig. 4a). Moreover, activation of the visual cortex, using
TMS stimulation, induces phosphenes in sighed controls but refereed sensations on
the tongue of practised TDU users [79], and tactile sensations in the ﬁngers of
practised Braille readers [110] (Fig. 4b). The activation of the occipital cortex
during both Braille reading and TDU tasks in blind people is thought to be a result
of the fact that these tasks demand a high degree of spatial acuity—they are both
spatial perception tasks. The spatial layout of the neurons in the occipital cortex
118
V. Harrar et al.

enables the development of a retinotopic map, which provides vision with a high
degree of spatial acuity. The particular characteristics of the neurons in this region
appear to be an important factor for its recruitment in non-visual spatial tasks during
blindness.1
Under some conditions, early visual impairment appears to lead to better tactile
performance in blind people as compared to sighted controls, as discussed in [82].
For example, in the “crossed hands paradigm”, where people cross their arms and
Fig. 3 The more modern scheme of the cortical anatomy of multisensory areas. Coloured areas
represent regions where there have been anatomical and/or electrophysiological data demonstrat-
ing multisensory interactions. In primary and secondary visual areas (V1 and V2), the multisensory
interactions seem to be restricted to the representation of the peripheral visual ﬁeld. Dashed gray
outlines represent opened sulci. Reprinted from [52], Copyright (2006), with permission from
Elsevier
1Further discussions on cross-modal plasticity elicited by the stimulation of the tongue (TDU), and
amodal representation of space, can be found in Chap. 6 by Chebat et al.
The Multisensory Blind Brain
119

then have to determine which hand (left or right) was touched ﬁrst, early blinds
outperform sighted people [120]. Sighted people typically have difﬁculty deter-
mining which hand was touched ﬁrst but blinds can report the temporal order of
touches equally well with their hands in any posture. These results support the
hypothesis that when vision develops normally, touch on the body is coded in a
visual reference frame [59], both for perception and to guide actions [60]. The
visual reference frame is an externally deﬁned coordinate system that is automat-
ically used by sighted controls. Blind people, on the other hand, only use an
externally deﬁned coordinate system when speciﬁcally instructed [94, 95]. Thus,
guiding actions towards external objects is based on an external reference frames in
sighted, but is based on an internal reference frame in blind people [118]. Much like
the visual system, online corrections for hand orientation are made with proprio-
ceptive inputs in the blind [55]. In addition to the increased neural plasticity in the
blind brain, the ability of blind people to use either reference frame can also
partially explain certain behavioural advantages compared to sighted controls.
The cortical networks associated with movement control are fairly similar for
sighted and blind people, leading researchers to conclude that a multisensory net-
work develops with a sensorimotor feedback system, rather than a visual feedback
system [42]. While visual feedback primarily supports the system in adults with
normal visual development, they hypothesise that such a system originates from a
Fig. 4 TMS of the visual cortex in congenitally blind subjects can induce tactile sensations.
a Somatotopically organised tactile sensations in the tongue induced by TMS over occipital cortex
in four blind subjects who were trained to use their tongue to perform a motion discrimination task.
The ﬁgure shows the areas of the tongue where tactile sensations were felt (indicated in black) after
TMS stimulation of the occipital cortex. The numbers on the scales refer to the distance (in cm)
from the inion. b TMS-induced tactile sensations referred to the ﬁngertips in two congenitally
blind proﬁcient Braille readers. The number of visual cortex sites from which paresthesiae could
be induced in a particular ﬁnger is colour-coded (see colour map), with red indicating the highest
number of cortical sites that induced paresthesiae in a particular ﬁnger and purple the lowest
number. Reprinted from [82], Copyright (2014) with permission from Elsevier
120
V. Harrar et al.

multisensory framework. Thus, this same network can be used regardless of the
modality feeding the system in people with sensory deﬁcits. Similarly, several
perceptual tasks have demonstrated the role of the occipital cortex in decrypting
highly spatial, non-visual information. While the visual cortex is shown to be active
when blind people, but not sighted controls, are highly trained in tactile tasks [112],
there are now a handful of studies that demonstrate exceptions to this rule, even for
certain simple tactile stimuli.
Despite popular belief, the occipital cortex can be recruited for tactile spatial
tasks even in people with intact visual systems [148]. In this study they demon-
strated that applying TMS to the occipital cortex disrupts tactile orientation dis-
crimination (a spatial-based task) but not the ability to detect the stimulus, or to
perceive texture (detection or discrimination tasks). Further, a subregion of the
lateral occipital cortex, known as the lateral occipital tactile–visual area (LOtv) is
activated by tactile stimuli (see discussion in [64]), for both 3D haptic perception
[5], and less complex haptic stimuli [74, 108]. It remains a controversial point
whether the activity in the LOtv represents amodal shape perception [5], or is
attributed to mental imagery [149]—which can also be amodal (see Sect. 6.2). As
was suggested for auditory-based studies, the multisensory cortical connections in
the occipital lobe likely underpin the activity in the occipital cortex of blind people
elicited by tactile stimuli; these stimuli likely become transduced into amodal
representations in the multisensory blind brain.
4.3
Pain Activation of Visual Areas
The relationship between vision and pain is well known, even intuitive [63]. We
almost always look at the site of injury, and what we see will gauge our response. If
the skin appears unharmed there is a “visual analgesic” effect and we feel less pain.
If the skin is broken and bleeding, we will have an increase in pain perception.
Experimentally, the effect of short-term visual deprivation (one week), causes
otherwise normal people to experience increased tactile and thermal acuity [153].
This early empirical data also demonstrated that blindfolding causes a drop in heat
pain thresholds, indicating that lack of vision is related to hypersensitivity to pain.
In our pioneering study of pain and temperature perception in a blind population,
we demonstrated that blind subjects had signiﬁcantly lower heat pain and cold pain
thresholds than matched controls [127]. We demonstrated that hypersensitivity to
pain is speciﬁc to noxious thermal stimulation, rather than to thermal stimulation in
general, and the effect is not culturally based. Taken together with ﬁndings of
augmented responses to threatening auditory stimuli in blind subjects [75], these
data provide compelling evidence that early blindness might cause an increased
attentiveness to external threats.
In order to determine if the hypersensitivity to threatening stimuli arises from a
compensatory neural plasticity that is rooted in the critical period of development,
we compared the pain sensitivity in early and late blind subjects. While early blind
The Multisensory Blind Brain
121

individuals were quite different, data from late blind subjects was very similar to
that of sighted individuals, including both responses to painful heat stimuli and
questionnaires assessing awareness and anxiety towards pain [126]. This suggests
that visual deprivation, per se, does not alone determine the development of pain
hypersensitivity—the time at which the visual system is deprived is equally
important.
Two competing hypotheses have been proposed to explain pain hypersensitivity
in congenital blindness [126, 127]. According to the ﬁrst hypothesis, pain hyper-
sensitivity reﬂects cross-modal plasticity of brain circuits as a result of the lack of
visual input. The absence of inhibitory effects of vision on pain perception in the
visually deprived blind brain (i.e. a lack of inhibitory feedback from the visual
analgesia effect) might leave the neural circuitry associated with nociceptive inputs
particularly sensitive [86, 87, 153]. Alternatively, a second hypothesis suggests that
the pain hypersensitivity results from a hypervigilance to threatening stimuli in
early blind individuals. Vision, when present, can signal potential threats to the
body (e.g., a red hot stove) so that individuals can remain carefree until they see a
dangerous stimulant. In the absence of the visual warning function, blind individ-
uals might instead adopt a chronic state of hypervigilance as a way to avoid tissue
damage. This more integrative interpretation of the pain hypersensitivity, com-
bining both the psychological and biological aspects of pain, can also account for
the observations that early blind individuals show increased responses to auditory
threats [75], and are better at identifying body odours with a negative emotional
valence [69]. Indeed, as shown in recent brain imaging studies, salient visual and
noxious stimuli activate a partly overlapping cortical network [98], supporting the
hypothesis that there is an intricate integration of vision and pain processing.
4.4
Chemical Senses
The relationship between vision and taste is often automatic. Chefs will naturally
devote signiﬁcant amounts of time making their food look good, in addition to
making it taste good. There is also empirical evidence that colour affects ﬂavour
perception [61, 62, 130].
Behavioural evidence has demonstrated that EB is associated with increased
odour awareness [9] and increased ability to correctly name odours from everyday
life [32, 99, 123]. We have replicated this enhanced detection of odours when blind
participants sniffed the odorants (orthonasal route), but when they smelled them
retronasally (through the mouth) the sighted controls tended to outperform the blind
participants. Similarly, blind people appear to have a reduced taste sensitivity [47].
There are a few studies that have investigated the neural plasticity and beha-
vioural differences associated with smell and taste in blindness (see review in [48]).
BOLD responses recently demonstrated the important difference between taste and
smell in blindness. While odours activate the visual cortex in blind participants, but
not
sighted
participants
[78],
taste
processing
does
not demonstrate
this
122
V. Harrar et al.

multisensory plasticity [49], see discussion in [81]. These studies have suggested
that experience with food is an important predictor of performance in chemosensory
experiments.
In the absence of vision, blind people face several obstacles when searching for
food, buying food in impervious packaging, preparing food, and eating in restau-
rants [12], which might limit the diet of many blind people. While their diet may be
limited, their exposure to odours is not, since most odours are not food-related. The
limited variety in a blind person’s diet would provide them with fewer ﬂavour
experiences causing poorer sensitivity to stimuli presented through the mouth, but
normal (or heightened) sensitivity when stimuli are sensed orthonasally. Thus, the
difference of experience between taste and odour appears to underlie the neural
activation and behavioural performance differences reported between blind and
sighted controls [48].
5
Primary Auditory and Somatosensory Cortex
In addition to the mounting evidence presented above of other sensory modalities
activating the primary visual cortex, here we present evidence of the reverse—
visual, and other types of stimuli, activating the primary auditory and somatosen-
sory cortices in sensory deprived and normally developed brains.
In the auditory cortex, despite it being primarily an auditory processing centre,
single electrode recordings have demonstrated the presence of trisensory neurons,
responding to auditory, visual, and somatosensory stimuli [45]. This multisensory
facilitation in the auditory cortex appears to be particularly responsive to voices and
faces [22, 105, 139], i.e. multisensory integration for communication [52]. These
multisensory neurons for communication are more likely to be enhanced when
audio-visual delays are short, whereas longer delays between stimuli are associated
with response suppression [51]. These neurons, therefore, support the hypothesis
that temporal proximity is an important modulator of the activity in multisensory
neurons. In addition to communication, multisensory neurons in the auditory cortex
also play an important role in eye positioning [46, 146], and somatosensory pro-
cessing [72]. Further, these multisensory pathways within the primary auditory
cortex are present even after normal auditory development. However, as was the
case for blind processing, these multisensory pathways appear to be particularly
well developed in deaf individuals, as evidenced by cortical plasticity, and may
underpin the selectively enhanced visual abilities of the deaf [8, 85].
Similarly, the primary somatosensory cortex is also responsive to non-tactile
stimulation, in particular auditory and visual stimulation. Multisensory convergence
on a single neuron in the somatosensory cortex has been demonstrated through
single-unit recordings [150, 151], and tracer studies [23]. These kinds of multi-
sensory neurons would likely support the cortical plasticity associated with early
somatosensory deprivation [53].
The Multisensory Blind Brain
123

In somewhat of a resistance to accept multisensory activity in primary cortices,
this was thought to occur only after the stimuli had been processed in multisensory
regions (presumably after an initial processing in the unisensory areas). That is,
evidence for multisensory convergence in the “unisensory” cortices was thought to
be the result of “top-down” feedback from multisensory areas to the primary sen-
sory areas [21, 38, 88, 102]. However, tracer studies have shown direct projections
of auditory neurons to V1 and V2 [117]. In addition, damage to higher order
“multisensory regions” does not necessarily hinder multisensory integration abili-
ties (for a review see Ettlinger and Wilson [40]). Further, the timing of some
integration activity (occurring 40 and 50 ms after stimulus) are too early to arise
from feedback pathways [43]. Thus, multisensory activity in the primary sensory
cortices results from a combination of feedback, feedforward, and lateral connec-
tions [43].
6
The Amodal Cortex
It is now becoming particularly clear that the view of a modality-speciﬁc divided
brain is inappropriate, or at least incomplete. Although this might mostly hold true
of primary cortices, where only about 10% of neurons respond to “inappropriate
sensory” stimuli, even the boundaries of these divisions are unclear (i.e. multi-
sensory [142]). Instead of sensory delineations, sensory deprivation studies have
suggested a functional delineation for certain cortices [3, 136]. For example, the
motion sensitive middle temporal cortex (hMT+) responds to any kind of motion,
be it visual, auditory, or tactile in origin, and this is true for both sighted and blind
individuals [91]. A common hypothesis suggests that the occipital cortex is, thus,
spatially arranged, providing spatial information regardless of the modality (for a
review see [125], see also Chebat et al., Chap. 6).
While vision might be used to localise objects for sighted individuals, auditory
and proprioceptive localisation cues are utilised by the blind [145], and as such,
both might rely on the same brain areas to interpret spatial information. In partic-
ular, the retinotopic arrangement of the visual cortex seen in sighted individuals is
maintained in blind individuals when this cortical region is recruited for tactile
tasks, conserving a topographic representation of space [79, 110], see Fig. 4. Visual
experience is, therefore, not a mandatory prerequisite for the topographically
organised, functionally related, representations in the extrastriate visual cortex—
these appear to be supramodal neural response patterns in the human brain [80]. In
the same vein, the dorsal stream appears to be shaped by non-visual spatial
information during early development [42].
Theoretically it is easy to fathom that the representation of spatial information in
the brain is amodal: the structures supporting mental and spatial representations in
the blind and sighted are often the same and the cortex maintains its functional
organisation despite the absence of vision from birth. Demonstrating such a thing is
very difﬁcult, however. For example, auditory activation of the “visual cortex” can
124
V. Harrar et al.

be interpreted as an attentional recruitment by auditory stimuli rather than
attributing the activity to the auditory stimulus (e.g. [24, 84]). Similarly, in the case
where a sound of a dog barking activates the visual cortex, the classic criticism has
been that the sound invokes imagery, and the imagery is what then activates the
visual cortex. Since all modalities can initiate mental imagery, might imagery be
amodal?
6.1
Amodal Imagery
While imagery is a highly visual construct in normal-sighted individuals, mental
imagery also expands to the other sensory modalities [76]. In this more broad use of
the term, mental imagery refers to the construct or representation of a
quasi-perceptual experience in the absence of perceptual sensory input. It is also
commonly referred to as “seeing with the mind’s eye”. Thus, imagery refers to a
particular aspect of memory in which a mental “image” of a stimulus is maintained.
Various neuroimaging studies have demonstrated that mental imagery activates
similar neuronal patterns as processes related to sensory perception. For example,
visual imagery will activate the visual cortex, particularly the extrastriate and
associate visual cortices; while auditory imagery will be associated with the acti-
vation of secondary auditory cortices [76].
Mental imagery in the blind appears to be a construct of the remaining sensory
modalities. While blind people have limited mental imagery capabilities restricted
to non-visual modalities, they often outperform sighted individuals in mental
imagery tasks. For example, Paivio and Okovita [100] found that congenitally blind
people were better than sighted individuals at recalling item pairs with a
high-auditory imagery component, whereas the inverse was found for items with a
high-visual imagery component [100]. Moreover, as imagery has been shown to
facilitate encoding, learning and memory recall, Marchant and Malloy [90]
demonstrated that congenitally blind and deaf individuals were able to recall as
many paired words as sighted control individual, when these words contained
multi-modal imagery (e.g. a train) [90]. More speciﬁcally, recall was only impaired
for congenitally blind individuals when items possessed unimodal visual imagery
(e.g. a rainbow). As such, the mental imagery of blind individuals is limited to, but
also compensated by, non-visual sensory modalities. Moreover, some particularities
of object representation that, in sighted individuals, are strongly visual-based, such
as shape, contours and textures, are acquired in the blind through the other
modalities, such as touch. Vision is, therefore, neither necessary nor sufﬁcient for
mental imagery, and as such can be equally experienced even by early blind
individuals.
Mental imagery in the blind may also be limited to sequential processing, as
simultaneous processing is thought to be purely visual [141]. This recent revelation
might partially explain the impaired performance (i.e. slower response times) of
blind individuals in some, but not all, visuospatial imagery tasks [2, 73, 141].
The Multisensory Blind Brain
125

Therefore, although mental imagery is possible, even when reared in the absence of
visual input, it is limited to non-visual sequential imagery.
In support of the amodal mental imagery hypothesis, the visual cortical areas are
activated when blind people perform mental imagery tasks. For example, the
ventral stream is known to play an essential role in object representation [56, 138]
and, consequently, is an important locus for imagery. In sighted individuals, the
occipitotemporal (OCT) cortex of the ventral stream seems to be equally active for
visual and haptic shape representation [4, 5, 77, 104]. Although sighted individuals
can resort to visual characteristics of the stimulus for mental shape representation,
blind individuals can also generate a mental image of an object’s shape through
tactile sensory information. A recent study by Peelen et al. [104] demonstrated that,
in the blind, the OCT cortex, and particularly the object-selective cortex (OSC) is
active during tactile shape mental imagery, prompted by having blind people think
about and compare the shape of different objects [104]. Object representation was
prompted by a verbal (auditory) stimulus, by which the name of a common object
was given. The activation of the OCT in both sighted and blind individuals, for
visual and tactile shape processing, supports the notion that this cortical area is
associated with amodal shape representation [111].
Likewise, the dorsal stream, associated with spatial localisation, also plays a role
in imagery, particularly in imagery of visuospatial information. As revealed through
studies in the blind, this cortical stream is associated with amodal spatial pro-
cessing; spatial imagery prompted from verbal instructions has been shown to
activate parieto-occipital areas of the dorsal stream in both blind and sighted
individuals [16, 140]. Similarly, in an experiment where participants discriminated
and compared the size of the angle between a clock’s hands, blind and sighted
participants both activated the posterior parietal cortical areas [16]. Thus, both the
dorsal and ventral streams (see Fig. 5) normally associated with visual imagery also
have a supramodal functional nature and are recruited for tactile and auditory
mental imagery.
In summary, ﬁndings suggest that the multisensory plasticity following blind-
ness extends beyond sensory perception, to processes of mental imagery. While
there are some limitations in the imagery capabilities of the blind, namely the
absence of certain, purely visual, characteristics (e.g. colour), and their inability to
perform simultaneous processing of mentally generated images, blind people often
outperform visual controls in non-visual mental imagery tasks. These multisensory
processes suggest that it is possible that different forms of mental imagery can be
represented simultaneously in the brain.
6.2
Amodal Memory
Although mental imagery refers to a particular aspect of memory, memory
encompasses various other dimensions, including declarative, semantic, episodic,
and procedural memory. Early blindness seems to result in an advantage for many
126
V. Harrar et al.

of these aspects of memory leading several researchers to conclude that the primary
visual cortex is recruited for higher order cognitive processes, such as verbal
memory. In blind individuals, while the anterior region of the occipital cortex is
associated with Braille reading, the posterior region is active during verbal memory
and verb generation tasks [6]. It has been suggested that, in the absence of visual
input from birth, the typical visual system hierarchy is inverted: in the blind,
extrastriate areas become associated with amodal (or multisensory) processing,
while the primary “visual” cortex is recruited for processing higher order cognitive
functions, such as verbal memory [3, 6].
Verbal memory tasks yield different activation patterns for blind and sighted
controls. Comparative BOLD fMRI measurements have revealed that, while the
primary visual cortex is active during verbal memory and verb generation tasks in
blind individuals, this activation is absent in sighted controls [6]. Furthermore, the
magnitude of V1 activation is strongly correlated with verbal memory performance
in blind participants [6]. Similarly, disrupting activity in the occipital pole by
application of rTMS induces greater errors—particularly semantic errors—in an
associative verb generation task in blind but not sighted individuals [3]. These
results, thus, further support the role of the visual cortex in verbal processes of early
blind individuals (see also auditory verb generation [20], speech comprehension
[121], and language processing [10]). The visual cortex is not reserved for low-level
visual processing, it can also be critical for processing higher order cognitive
functions. Furthermore, in a follow-up study, long-term recall performance was
assessed by recalling blind participants one year after initial testing, and retesting
them with the words used during the initial verbal memory tasks [115]. Raz and
Fig. 5 The dorsal stream (green) and ventral stream (purple) are shown. They originate from a
common source in the primary visual cortex. Modiﬁed by Selket from Anatomy of the Human
Body, Gray H, Fig. 728 (1918)
The Multisensory Blind Brain
127

colleagues reiterated the correlation between activation in the occipital cortex and
performance in verbal memory. The occipital cortex also plays an active role in
episodic and semantic memory processes in early blind people.
More generally, memory capacity and ﬁdelity appear to be enhanced with early
visual deprivation. Blind individuals, particularly early blind people, are often
reported to have superior memory [6, 67, 103, 107, 119, 128, 137]. Moreover,
superior memory abilities associated with blindness depend on both the degree and
timing of the visual impairment [67]. While memory appears to not be different
between the late blinds and sighted individuals, it is considerably better in early
blind populations [103]. Early blind individuals were found to, not only report more
correct words (words from the original list), but also report fewer “false memories”
(words that were semantically similar to, but not part of, the original word list) and
fewer unrelated words [103]. This enhancement in memory suggests that blind
individuals may rely more heavily on working and episodic memory in the absence
of visual input.
7
Conclusions
While it was originally thought that the brain was parcelled into several speciﬁc
subregions, each responsible for interpreting the information from a given sensory
modality, along with a few “multisensory” areas that work to combine information
across the senses, this idea has been largely debunked. A more recent view of the
brain suggests that, instead of being sensory speciﬁc, cortical regions are func-
tionally speciﬁc, likely based on their neuronal arrangement (e.g. striate cortex is
best for interpreting spatial information, while the LOtv interprets shapes). Much of
the evidence for non-visual activation of the “visual” cortex has come from blind
individuals who demonstrate considerable multisensory neural plasticity. Blinds
trained in tactile and auditory tasks or on high-tech devices recruit their visual
cortex to interpret the complex material. While this now classic example of neural
plasticity was surprising and confusing at ﬁrst, it ﬁts with the evidence from several
papers demonstrating that, even under normal visual development, activity in the
occipital cortex can be elicited from tactile, auditory, olfactory, pain, and propri-
oceptive stimulation. Thus, the multisensory circuitry to the visual cortex is fas-
tidious but certainly present, and becomes particularly robust when visual
development is irregular.
Perhaps, vision plays a pivotal role in establishing multisensory functions during
ontogeny, which would explain why visual pathways extend across to nearly all
areas of the brain [66]. Vision, during development, might calibrate the senses to
each other (so that size can be understood visually or haptically; so that speed can
be understood visually and auditorally, enabling reference frame conversion
between the modalities, etc.). In conditions of poor vision, the senses would be
calibrated by another modality. Thus, while multisensory cortical areas have the
primary role of integrating information, the evidence of multisensory activity in
128
V. Harrar et al.

primary sensory cortices might be primarily related to the role of vision in onto-
geny. After normal development of the senses, vision would be less relevant for
primary cortical functions, and should eventually be pruned.
Understanding the multisensory nature of the brain enables us to develop better
rehabilitation schemes and technologies. Rehabilitation schemes following brain
damage (e.g. stroke related) have already beneﬁtted from the knowledge of auto-
matic audio–visual interactions when performing visual search tasks [15].
Moreover, understanding the role of vision within the brain, and the extensive
plasticity in the blind brain, can improve technologies built for the blind. Sensory
substitution devices are developed with the knowledge that objects can be repre-
sented in multiple senses. These same devices can be further improved with a better
understanding of the multisensory blind brain. Rather than devices that represent
vision, we must remember that vision is in itself a representation of the outside
world. The kinds of information that could enrich blind people’s environments do
not necessarily have a strictly “visual” component. Instead, thinking about the
amodal functions of the occipital cortex might provide insight into the technological
developments that should be pursued in the future.
References
1. Alais D, Burr D (2004) The ventriloquist effect results from near-optimal bimodal
integration. Curr Biol 14(3):257–262. doi:10.1016/j.cub.2004.01.029
2. Aleman A, van Lee L, Mantione MH, Verkoijen IG, de Haan EH (2001) Visual imagery
without visual experience: evidence from congenitally totally blind people. NeuroReport 12
(11):2601–2604
3. Amedi A, Floel A, Knecht S, Zohary E, Cohen LG (2004) Transcranial magnetic stimulation
of the occipital pole interferes with verbal processing in blind subjects. Nat Neurosci 7
(11):1266–1270
4. Amedi A, Jacobson G, Hendler T, Malach R, Zohary E (2002) Convergence of visual and
tactile shape processing in the human lateral occipital complex. Cereb Cortex 12(11):
1202–1212
5. Amedi A, Malach R, Hendler T, Peled S, Zohary E (2001) Visuo-haptic object-related
activation in the ventral visual pathway. Nat Neurosci 4(3):324–330
6. Amedi A, Raz N, Pianka P, Malach R, Zohary E (2003) Early ‘visual’cortex activation
correlates with superior verbal memory performance in the blind. Nat Neurosci 6(7):
758–766
7. Auvray M, Spence C (2008) The multisensory perception of ﬂavor. Conscious Cogn 17
(3):1016–1031
8. Bavelier D, Dye MW, Hauser PC (2006) Do deaf individuals see better? Trends Cogn Sci 10
(11):512–518
9. Beaulieu-Lefebvre M, Schneider FC, Kupers R, Ptito M (2011) Odor perception and odor
awareness in congenital blindness. Brain Res Bull 84(3):206–209
10. Bedny M, Pascual-Leone A, Dodell-Feder D, Fedorenko E, Saxe R (2011) Language
processing in the occipital cortex of congenitally blind adults. Proc Natl Acad Sci 108
(11):4429–4434
11. Berman AL (1961) Interaction of cortical responses to somatic and auditory stimuli in
anterior ectosylvian gyrus of cat. J Neurophysiol 24(6):608–620
The Multisensory Blind Brain
129

12. Bilyk MC, Sontrop JM, Chapman GE, Barr SI, Mamer L (2009) Food experiences and
eating patterns of visually impaired and blind people. Can J Diet pract Res 70(1):13–18
13. Blanke O (2012) Multisensory brain mechanisms of bodily self-consciousness. Nat Rev
Neurosci 13(8):556–571
14. Bolognini N, Olgiati E, Rossetti A, Maravita A (2010) Enhancing multisensory spatial
orienting by brain polarization of the parietal cortex. Eur J Neurosci 31(10):1800–1806
15. Bolognini N, Rasi F, Coccia M, Làdavas E (2005) Visual search improvement in hemianopic
patients after audio-visual stimulation. Brain 128(12):2830–2842
16. Bonino D, Ricciardi E, Bernardi G, Sani L, Gentili C, Vecchi T, Pietrini P (2015) Spatial
imagery relies on a sensory independent, though sensory sensitive, functional organization
within the parietal cortex: a fMRI study of angle discrimination in sighted and congenitally
blind individuals. Neuropsychologia 68:59–70
17. Brang D, Towle VL, Suzuki S, Hillyard SA, Di Tusa S, Dai Z, Tao J, Wu S, Grabowecky M
(2015) Peripheral sounds rapidly activate visual cortex: evidence from electrocorticography.
J Neurophysiol 114(5):3023–3028 (jn. 00728.02015)
18. Bruce C, Desimone R, Gross CG (1981) Visual properties of neurons in a polysensory area
in superior temporal sulcus of the macaque. J Neurophysiol 46(2):369–384
19. Buneo CA, Jarvis MR, Batista AP, Andersen RA (2002) Direct visuomotor transformations
for reaching. Nature 416(6881):632–636
20. Burton H, Snyder A, Diamond J, Raichle M (2002) Adaptive changes in early and late blind:
a FMRI study of verb generation to heard nouns. J Neurophysiol 88(6):3359–3371
21. Calvert GA, Brammer MJ, Bullmore ET, Campbell R, Iversen SD, David AS (1999)
Response ampliﬁcation in sensory-speciﬁc cortices during crossmodal binding. NeuroReport
10(12):2619–2623
22. Calvert GA, Bullmore ET, Brammer MJ, Campbell R, Williams SC, McGuire PK,
Woodruff PW, Iversen SD, David AS (1997) Activation of auditory cortex during silent
lipreading. Science 276(5312):593–596
23. Cappe C, Barone P (2005) Heteromodal connections supporting multisensory integration at
low levels of cortical processing in the monkey. Eur J Neurosci 22(11):2886–2902
24. Cate AD, Herron TJ, Yund EW, Stecker GC, Rinne T, Kang X, Petkov CI, Disbrow EA,
Woods DL (2009) Auditory attention activates peripheral visual cortex. PLoS ONE 4(2):
e4645
25. Cohen LG, Celnik P, Pascual-Leone A, Corwell B, Faiz L, Dambrosia J, Honda M,
Sadato N, Gerloff C, Catala MD (1997) Functional relevance of cross-modal plasticity in
blind humans. Nature 389(6647):180–183
26. Cohen YE, Andersen RA (2002) A common reference frame for movement plans in the
posterior parietal cortex. Nat Rev Neurosci 3(7):553–562
27. Collignon O, Davare M, De Volder AG, Poirier C, Olivier E, Veraart C (2008) Time-course
of posterior parietal and occipital cortex contribution to sound localization. J Cogn Neurosci
20(8):1454–1463
28. Collignon O, Lassonde M, Lepore F, Bastien D, Veraart C (2007) Functional cerebral
reorganization for auditory spatial processing and auditory substitution of vision in early
blind subjects. Cereb Cortex 17(2):457–465
29. Collignon O, Vandewalle G, Voss P, Albouy G, Charbonneau G, Lassonde M, Lepore F
(2011) Functional specialization for auditory–spatial processing in the occipital cortex of
congenitally blind humans. Proc Natl Acad Sci 108(11):4435–4440
30. Colonius H, Arndt P (2001) A two-stage model for visual-auditory interaction in saccadic
latencies. Percept Psychophys 63(1):126–147
31. Corneil B, Van Wanrooij M, Munoz D, Van Opstal A (2002) Auditory-visual interactions
subserving goal-directed saccades in a complex scene. J Neurophysiol 88(1):438–454
32. Cuevas I, Plaza P, Rombaux P, De Volder AG, Renier L (2009) Odour discrimination and
identiﬁcation are improved in early blindness. Neuropsychologia 47(14):3079–3083
130
V. Harrar et al.

33. De Volder AG, Bol A, Blin J, Robert A, Arno P, Grandin C, Michel C, Veraart C (1997)
Brain energy metabolism in early blind subjects: neural activity in the visual cortex. Brain
Res 750(1):235–244
34. De Volder AG, Catalan-Ahumada M, Robert A, Bol A, Labar D, Coppens A, Michel C,
Veraart C (1999) Changes in occipital cortex activity in early blind humans using a sensory
substitution device. Brain Res 826(1):128–134
35. Dehner LR, Keniston LP, Clemo HR, Meredith MA (2004) Cross-modal circuitry between
auditory and somatosensory areas of the cat anterior ectosylvian sulcal cortex: a ‘new’
inhibitory form of multisensory convergence. Cereb Cortex 14(4):387–403
36. Desgent S, Ptito M (2012) Cortical GABAergic interneurons in cross-modal plasticity
following early blindness. Neural Plast 2012:590725. doi:10.1155/2012/590725
37. Diederich A, Colonius H (2004) Bimodal and trimodal multisensory enhancement: effects of
stimulus onset and intensity on reaction time. Percept Psychophys 66(8):1388–1404
38. Driver J, Spence C (2000) Multisensory perception: beyond modularity and convergence.
Curr Biol 10(20):R731–R735
39. Ernst MO, Banks MS (2002) Humans integrate visual and haptic information in a
statistically optimal fashion. Nature 415(6870):429–433. doi:10.1038/415429a
40. Ettlinger G, Wilson W (1990) Cross-modal performance: behavioural processes, phyloge-
netic considerations and neural mechanisms. Behav Brain Res 40(3):169–192
41. Falchier A, Clavagnier S, Barone P, Kennedy H (2002) Anatomical evidence of multimodal
integration in primate striate cortex. J Neurosci 22(13):5749–5759
42. Fiehler K, Rösler F (2009) Plasticity of multisensory dorsal stream functions: evidence from
congenitally blind and sighted adults. Restor Neurol Neurosci 28(2):193–205
43. Foxe JJ, Schroeder CE (2005) The case for feedforward multisensory convergence during
early cortical processing. NeuroReport 16(5):419–423
44. Frens MA, Van Opstal AJ, Van der Willigen RF (1995) Spatial and temporal factors
determine auditory-visual interactions in human saccadic eye movements. Percept
Psychophys 57(6):802–816
45. Fu K-MG, Johnston TA, Shah AS, Arnold L, Smiley J, Hackett TA, Garraghty PE,
Schroeder CE (2003) Auditory cortical neurons respond to somatosensory stimulation.
J Neurosci 23(20):7510–7515
46. Fu K-MG, Shah AS, O’Connell MN, McGinnis T, Eckholdt H, Lakatos P, Smiley J,
Schroeder CE (2004) Timing and laminar proﬁle of eye-position effects on auditory
responses in primate auditory cortex. J Neurophysiol 92(6):3522–3531
47. Gagnon L, Kupers R, Ptito M (2013) Reduced taste sensitivity in congenital blindness.
Chem Senses 38(6):509–517
48. Gagnon L, Kupers R, Ptito M (2014) Making sense of the chemical senses. Multisensory
Res 27(5–6):399–419
49. Gagnon L, Kupers R, Ptito M (2015) Neural correlates of taste perception in congenital
blindness. Neuropsychologia 70:227–234
50. Gentile G, Petkova VI, Ehrsson HH (2011) Integration of visual and tactile signals from the
hand in the human brain: an FMRI study. J Neurophysiol 105(2):910–922
51. Ghazanfar AA, Maier JX, Hoffman KL, Logothetis NK (2005) Multisensory integration of
dynamic faces and voices in rhesus monkey auditory cortex. J Neurosci 25(20):5004–5012
52. Ghazanfar AA, Schroeder CE (2006) Is neocortex essentially multisensory? Trends Cogn
Sci 10(6):278–285. doi:10.1016/j.tics.2006.04.008
53. Ghoshal A, Tomarken A, Ebner F (2011) Cross-sensory modulation of primary sensory
cortex is developmentally regulated by early sensory experience. J Neurosci 31(7):
2526–2536
54. Gilbert SJ, Walsh V (2004) Vision: the versatile ‘visual’ cortex. Curr Biol 14(24):
R1056–R1057
55. Gosselin-Kessiby N, Kalaska JF, Messier J (2009) Evidence for a proprioception-based rapid
on-line error correction mechanism for hand orientation during reaching movements in blind
subjects. J Neurosci 29(11):3485–3496
The Multisensory Blind Brain
131

56. Grill-Spector K (2003) The neural basis of object perception. Curr Opin Neurobiol 13
(2):159–166
57. Hamilton R, Keenan JP, Catala M, Pascual-Leone A (2000) Alexia for Braille following
bilateral occipital stroke in an early blind woman. NeuroReport 11(2):237–240
58. Hamilton RH, Pascual-Leone A (1998) Cortical plasticity associated with Braille learning.
Trends Cogn Sci 2(5):168–174
59. Harrar V, Harris LR (2009) Eye position affects the perceived location of touch. Exp Brain
Res 198(2–3):403–410. doi:10.1007/s00221-009-1884-4
60. Harrar V, Harris LR (2010) Touch used to guide action is partially coded in a visual
reference frame. Exp Brain Res 203(3):615–620. doi:10.1007/s00221-010-2252-0
61. Harrar V, Piqueras-Fiszman B, Spence C (2011) There’s more to taste in a coloured bowl.
Percept Lond 40(7):880
62. Harrar V, Spence C (2013) The taste of cutlery: how the taste of food is affected by the
weight, size, shape, and colour of the cutlery used to eat it. Flavour 2(1):1–13
63. Harrar V, Vandenbroucke S, Slimani H, Ptito M, Kupers R (2016) Out of sight but not out of
mind: the role of vision in pain perception. In: Garcia-Larrea LJ (ed) Pain & consciousness,
IASP Press, Washington DC, pp 183–195
64. Helbig HB, Ernst MO, Ricciardi E, Pietrini P, Thielscher A, Mayer KM, Schultz J,
Noppeney U (2012) The neural mechanisms of reliability weighted integration of shape
information from vision and touch. Neuroimage 60(2):1063–1072
65. Hikosaka K, Iwai E, Saito H, Tanaka K (1988) Polysensory properties of neurons in the
anterior bank of the caudal superior temporal sulcus of the macaque monkey. J Neurophysiol
60(5):1615–1637
66. Hötting K, Röder B (2009) Auditory and auditory-tactile processing in congenitally blind
humans. Hear Res 258(1):165–174
67. Hull T, Mason H (1995) Performance of blind children on digit-span tests. J Visual
Impairment Blindness 89:166
68. Ionta S, Heydrich L, Lenggenhager B, Mouthon M, Fornari E, Chapuis D, Gassert R,
Blanke O (2011) Multisensory mechanisms in temporo-parietal cortex support self-location
and ﬁrst-person perspective. Neuron 70(2):363–374
69. Iversen KD, Ptito M, Moller P, Kupers R (2015) Enhanced chemosensory detection of
negative emotions in congenital blindness. Neural Plast 2015:469750. doi:10.1155/2015/
469750
70. Jiang H, Lepore F, Ptito M, Guillemot J-P (1994) Sensory interactions in the anterior
ectosylvian cortex of cats. Exp Brain Res 101(3):385–396
71. Jiang H, Lepore F, Ptito M, Guillemot J-P (1994) Sensory modality distribution in the
anterior ectosylvian cortex (AEC) of cats. Exp Brain Res 97(3):404–414
72. Kayser C, Petkov CI, Augath M, Logothetis NK (2005) Integration of touch and sound in
auditory cortex. Neuron 48(2):373–384
73. Kerr NH (1983) The role of vision in “visual imagery” experiments: evidence from the
congenitally blind. J Exp Psychol Gen 112(2):265
74. Kim S, James TW (2010) Enhanced effectiveness in visuo-haptic object-selective brain
regions with increasing stimulus salience. Hum Brain Mapp 31(5):678–693
75. Klinge C, Röder B, Buchel C (2010) Increased amygdala activation to emotional auditory
stimuli in the blind. Brain 133(Pt 6):1729–1736. doi:10.1093/brain/awq102
76. Kosslyn SM, Ganis G, Thompson WL (2001) Neural foundations of imagery. Nat Rev
Neurosci 2(9):635–642
77. Kourtzi Z, Kanwisher N (2001) Representation of perceived object shape by the human
lateral occipital complex. Science 293(5534):1506–1509
78. Kupers R, Beaulieu-Lefebvre M, Schneider FC, Kassuba T, Paulson OB, Siebner HR,
Ptito M (2011) Neural correlates of olfactory processing in congenital blindness.
Neuropsychologia 49(7):2037–2044. doi:10.1016/j.neuropsychologia.2011.03.033
132
V. Harrar et al.

79. Kupers R, Fumal A, de Noordhout AM, Gjedde A, Schoenen J, Ptito M (2006) Transcranial
magnetic stimulation of the visual cortex induces somatotopically organized qualia in blind
subjects. Proc Natl Acad Sci USA 103(35):13256–13260. doi:10.1073/pnas.0602925103
80. Kupers R, Pietrini P, Ricciardi E, Ptito M (2011) The nature of consciousness in the visually
deprived brain. Front Psychol 2:19. doi:10.3389/fpsyg.2011.00019
81. Kupers R, Ptito M (2011) Insights from darkness: what the study of blindness has taught us
about brain structure and function. Prog Brain Res 192:17–31. doi:10.1016/B978-0-444-
53355-5.00002-6
82. Kupers R, Ptito M (2014) Compensatory plasticity and cross-modal reorganization following
early visual deprivation. Neurosci Biobehav Rev 41:36–52. doi:10.1016/j.neubiorev.2013.
08.001
83. Lewald J, Foltys H, Töpper R (2002) Role of the posterior parietal cortex in spatial hearing.
J Neurosci Ofﬁcial J Soc Neurosci 22(3):RC207–RC207
84. Liotti M, Ryder K, Woldorff MG (1998) Auditory attention in the congenitally blind: where,
when and what gets reorganized? NeuroReport 9(6):1007–1012
85. Lomber SG, Meredith MA, Kral A (2010) Cross-modal plasticity in speciﬁc auditory
cortices underlies visual compensations in the deaf. Nat Neurosci 13(11):1421–1427
86. Longo MR, Betti V, Aglioti SM, Haggard P (2009) Visually induced analgesia: seeing the
body reduces pain. J Neurosci 29(39):12125–12130. doi:10.1523/JNEUROSCI.3072-09.
2009
87. Longo MR, Iannetti GD, Mancini F, Driver J, Haggard P (2012) Linking pain and the body:
neural correlates of visually induced analgesia. J Neurosci 32(8):2601–2607. doi:10.1523/
JNEUROSCI.4031-11.2012
88. Macaluso E, Frith CD, Driver J (2000) Modulation of human visual cortex by crossmodal
spatial attention. Science 289(5482):1206–1208
89. Makin TR, Holmes NP, Zohary E (2007) Is that near my hand? Multisensory representation
of peripersonal space in human intraparietal sulcus. J Neurosci 27(4):731–740
90. Marchant B, Malloy TE (1984) Auditory, tactile, and visual imagery in PA learning by
congenitally blind, deaf, and normal adults. J Ment Imagery 8(2):19–32
91. Matteau I, Kupers R, Ricciardi E, Pietrini P, Ptito M (2010) Beyond visual, aural and haptic
movement perception: hMT+ is activated by electrotactile motion stimulation of the tongue
in sighted and in congenitally blind individuals. Brain Res Bull 82(5–6):264–270. doi:10.
1016/j.brainresbull.2010.05.001
92. Meredith MA, Stein BE (1983) Interactions among converging sensory inputs in the superior
colliculus. Science 221(4608):389–391
93. Meredith MA, Stein BE (1986) Visual, auditory, and somatosensory convergence on cells in
superior colliculus results in multisensory integration. J Neurophysiol 56(3):640–662
94. Millar S (1994) Understanding and representing space: theory and evidence from studies
with blind and sighted children. Clarendon Press/Oxford University Press, UK
95. Millar S, Al-Attar Z (2002) The Muller-Lyer illusion in touch and vision: implications for
multisensory processes. Percept Psychophys 64(3):353–365
96. Minciacchi D, Tassinari G, Antonini A (1987) Visual and somatosensory integration in the
anterior ectosylvian cortex of the cat. Brain Res 410(1):21–31
97. Morrell F (1972) Visual system’s view of acoustic space. Nature 238:44–46
98. Mouraux A, Diukova A, Lee MC, Wise RG, Iannetti GD (2011) A multisensory
investigation of the functional signiﬁcance of the “pain matrix”. Neuroimage 54(3):
2237–2249. doi:10.1016/j.neuroimage.2010.09.084
99. Murphy C, Cain WS (1986) Odor identiﬁcation: the blind are better. Physiol Behav 37
(1):177–180
100. Paivio A, Okovita HW (1971) Word imagery modalities and associative learning in blind
and sighted subjects. J Verbal Learn Verbal Behav 10(5):506–510
101. Pasalar S, Ro T, Beauchamp MS (2010) TMS of posterior parietal cortex disrupts visual
tactile multisensory integration. Eur J Neurosci 31(10):1783–1790
The Multisensory Blind Brain
133

102. Pascual-Leone A, Hamilton R (2001) The metamodal organization of the brain. Prog Brain
Res 134:427–446
103. Pasqualotto A, Lam JS, Proulx MJ (2013) Congenital blindness improves semantic and
episodic memory. Behav Brain Res 244:162–165
104. Peelen MV, He C, Han Z, Caramazza A, Bi Y (2014) Nonvisual and visual object shape
representations in occipitotemporal cortex: evidence from congenitally blind and sighted
adults. J Neurosci 34(1):163–170
105. Pekkola J, Ojanen V, Autti T, Jääskeläinen IP, Möttönen R, Tarkiainen A, Sams M (2005)
Primary auditory cortex activation by visual speech: an fMRI study at 3 T. NeuroReport 16
(2):125–128
106. Poirier C, Collignon O, Scheiber C, Renier L, Vanlierde A, Tranduy D, Veraart C, De
Volder A (2006) Auditory motion perception activates visual motion areas in early blind
subjects. Neuroimage 31(1):279–285
107. Požár L (1982) Effect of long-term sensory deprivation on recall of verbal material. Stud
Psychol (Bratisl)
108. Prather S, Votaw JR, Sathian K (2004) Task-speciﬁc recruitment of dorsal and ventral visual
areas during tactile perception. Neuropsychologia 42(8):1079–1087
109. Pritchett LM, Harris LR (2011) Perceived touch location is coded using a gaze signal. Exp
Brain Res 213(2–3):229–234. doi:10.1007/s00221-011-2713-0
110. Ptito M, Fumal A, de Noordhout AM, Schoenen J, Gjedde A, Kupers R (2008) TMS of the
occipital cortex induces tactile sensations in the ﬁngers of blind Braille readers. Exp Brain
Res 184(2):193–200. doi:10.1007/s00221-007-1091-0
111. Ptito M, Matteau I, Zhi Wang A, Paulson OB, Siebner HR, Kupers R (2012) Crossmodal
recruitment of the ventral visual stream in congenital blindness. Neural Plast 2012:1–9.
doi:10.1155/2012/304045
112. Ptito M, Moesgaard SM, Gjedde A, Kupers R (2005) Cross-modal plasticity revealed by
electrotactile stimulation of the tongue in the congenitally blind. Brain 128(Pt 3):606–614.
doi:10.1093/brain/awh380
113. Ptito M, Tassinari G, Antonini A (1987) Electrophysiological evidence for interhemispheric
connections in the anterior ectosylvian sulcus in the cat. Exp Brain Res 66(1):90–98
114. Rauschecker JP, Korte M (1993) Auditory compensation for early blindness in cat cerebral
cortex. J Neurosci 13(10):4538–4548
115. Raz N, Amedi A, Zohary E (2005) V1 activation in congenitally blind humans is associated
with episodic retrieval. Cereb Cortex 15(9):1459–1468
116. Reich L, Szwed M, Cohen L, Amedi A (2011) A ventral visual stream reading center
independent of visual experience. Curr Biol 21(5):363–368
117. Rockland KS, Ojima H (2003) Multisensory convergence in calcarine visual areas in
macaque monkey. Int J Psychophysiol 50(1–2):19–26
118. Röder B, Kusmierek A, Spence C, Schicke T (2007) Developmental vision determines the
reference frame for the multisensory control of action. Proc Natl Acad Sci 104(11):
4753–4758
119. Röder B, Rösler F, Neville HJ (2001) Auditory memory in congenitally blind adults: a
behavioral-electrophysiological investigation. Cogn Brain Res 11(2):289–303
120. Röder B, Rösler F, Spence C (2004) Early vision impairs tactile perception in the blind. Curr
Biol 14(2):121–124
121. Röder B, Stock O, Bien S, Neville H, Rösler F (2002) Speech processing activates visual
cortex in congenitally blind humans. Eur J Neurosci 16(5):930–936
122. Röder B, Teder-Sälejärvi W, Sterr A, Rösler F, Hillyard SA, Neville HJ (1999) Improved
auditory spatial tuning in blind humans. Nature 400(6740):162–166
123. Rosenbluth R, Grossman E, Kaitz M (2000) Performance of early-blind and sighted children
on olfactory tasks. Perception 29(1):101–110
124. Sadato N, Pascual-Leone A, Grafman J, Ibanez V, Deiber MP, Dold G, Hallett M (1996)
Activation of the primary visual cortex by Braille reading in blind subjects. Nature 380
(6574):526–528. doi:10.1038/380526a0
134
V. Harrar et al.

125. Schinazi VR, Thrash T, Chebat DR (2016) Spatial navigation by congenitally blind
individuals. Wiley Interdiscip Rev Cogn Sci 7(1):37–58
126. Slimani H, Danti S, Ptito M, Kupers R (2014) Pain perception is increased in congenital but
not late onset blindness. PLoS ONE 9(9):e107281. doi:10.1371/journal.pone.0107281
127. Slimani H, Danti S, Ricciardi E, Pietrini P, Ptito M, Kupers R (2013) Hypersensitivity to
pain in congenital blindness. Pain 154(10):1973–1978. doi:10.1016/j.pain.2013.05.036
128. Smits B, Mommers M (1976) Differences between blind and sighted children on WISC
verbal subtests. New Outlook Blind 70(6):240–246
129. Spence C, Harrar V, Piqueras-Fiszman B (2012) Assessing the impact of the tableware and
other contextual variables on multisensory ﬂavour perception. Flavour 1(1):7
130. Spence C, Levitan CA, Shankar MU, Zampini M (2010) Does food color inﬂuence taste and
ﬂavor perception in humans? Chemosens Percept 3(1):68–84
131. Stein BE, Magalhaes-Castro B, Kruger L (1975) Superior colliculus: visuotopic-somatotopic
overlap. Science 189(4198):224–226
132. Stein BE, Meredith MA (1993) The merging of the senses. Cognitive neuroscience series.
MIT Press, Cambridge
133. Stein BE, Meredith MA, Huneycutt WS, McDade L (1989) Behavioral indices of
multisensory integration: orientation to visual cues is affected by auditory stimuli. J Cogn
Neurosci 1(1):12–24. doi:10.1162/jocn.1989.1.1.12
134. Stein BE, Meredith MA, Wallace MT (1993) The visually responsive neuron and beyond—
multisensory integration in cat and monkey. Prog Brain Res 95:79–90. doi:10.1016/S0079-
6123(08)60359-3
135. Stein BE, Stanford TR (2008) Multisensory integration: current issues from the perspective
of the single neuron. Nat Rev Neurosci 9(4):255–266. doi:10.1038/nrn2331
136. Théoret H, Merabet L, Pascual-Leone A (2004) Behavioral and neuroplastic changes in the
blind: evidence for functionally relevant cross-modal interactions. J Physiol Paris 98(1):
221–233
137. Tillman MH, Bashaw WL (1968) Multivariate analysis of the WISC scales for blind and
sighted children. Psychol Rep 23(2):523–526
138. Ungerleider LG, Mishkin M (1982) Two cortical visual systems. In: Ingle DJ, Goodale MA,
Mansﬁeld RJW (eds) Analysis of visual behavior. MIT Press, Cambridge, pp 549–586
139. Van Atteveldt N, Formisano E, Goebel R, Blomert L (2004) Integration of letters and speech
sounds in the human brain. Neuron 43(2):271–282
140. Vanlierde A, De Volder AG, Wanet-Defalque MC, Veraart C (2003) Occipito-parietal cortex
activation during visuo-spatial imagery in early blind humans. Neuroimage 19(3):698–709
141. Vecchi T, Tinti C, Cornoldi C (2004) Spatial memory and integration processes in
congenital blindness. NeuroReport 15(18):2787–2790
142. Wallace MT, Ramachandran R, Stein BE (2004) A revised view of sensory cortical
parcellation. Proc Natl Acad Sci USA 101(7):2167–2172
143. Wallace MT, Wilkinson LK, Stein BE (1996) Representation and integration of multiple
sensory inputs in primate superior colliculus. J Neurophysiol 76(2):1246–1266
144. Wanet-Defalque M-C, Veraart C, De Volder A, Metz R, Michel C, Dooms G, Gofﬁnet A
(1988) High metabolic activity in the visual cortex of early blind human subjects. Brain Res
446(2):369–373
145. Warren DH, Pick HL (1970) Intermodality relations in localization in blind and sighted
people. Percept Psychophys 8(6):430–432
146. Werner-Reiss U, Kelly KA, Trause AS, Underhill AM, Groh JM (2003) Eye position affects
activity in primary auditory cortex of primates. Curr Biol 13(7):554–562
147. Zampini M, Spence C (2004) The role of auditory cues in modulating the perceived
crispness and staleness of potato chips. J Sens Stud 19(5):347–363
148. Zangaladze A, Epstein CM, Grafton ST, Sathian K (1999) Involvement of visual cortex in
tactile discrimination of orientation. Nature 401(6753):587–590
The Multisensory Blind Brain
135

149. Zhang M, Weisser VD, Stilla R, Prather S, Sathian K (2004) Multisensory cortical
processing of object shape and its relation to mental imagery. Cogn Affect Behav Neurosci 4
(2):251–259
150. Zhou Y-D, Fuster JM (2000) Visuo-tactile cross-modal associations in cortical somatosen-
sory cells. Proc Nat Acad Sci 97(17):9777–9782
151. Zhou Y-D, JnM Fuster (2004) Somatosensory cell response to an auditory cue in a haptic
memory task. Behav Brain Res 153(2):573–578
152. Zmigrod S, Zmigrod L (2015) Zapping the gap: Reducing the multisensory temporal binding
window by means of transcranial direct current stimulation (tDCS). Conscious Cogn
35:143–149
153. Zubek JP, Flye J, Aftanas M (1964) Cutaneous sensitivity after prolonged visual deprivation.
Science 144(3626):1591–1593
136
V. Harrar et al.

On Spatial Cognition and Mobility
Strategies
Edwige Pissaloux and Ramiro Velázquez
1
Concept of the Spatial Cognition
Spatial cognition is a fundamental and vital property of human knowledge (and
behavior) which provides means to interact with space once it has been perceived
and its structure has been understood.
Since the seminal works of Hein and Held [1], it is known that spatial cognition
is acquired pro-actively, i.e., by the physical experience of space interaction,
notably by mobility in the space.
Sometimes, instead of “mobility,” the terms “navigation” or “way ﬁnding” are
used by abuse of the language.
Initially, the term “navigation” was used to express someone’s capability to ﬁnd
his/her own way while navigating on the sea between two points located far away
from each other.
By extension, some authors [2, 3] use the word “navigation” as the capability to
ﬁnd the way on the earth and in the air while intentionally keeping the planned
trajectory from point A to B.
However, the mobility is a complex cognitive task, and vital for all; it encom-
passes navigation, but also requires to keep the posture, to orient oneself in space, to
know the socio-urban organization, to update the current path because of the
unscheduled and unexpected events, etc. For example, moving from your own
home to work, includes more elementary tasks such as (1) the constant orientation
toward the target location (with respect to your current spatial position), (2) the
constant judgment of the traveled distances, (3) the constant check if you are
E. Pissaloux (&)
Université de Rouen Normandie, Rouen, France
e-mail: pissaloux@sfr.fr
R. Velázquez
Universidad Panamericana, Aguascalientes, Mexico
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_5
137

following the correct path, (4) the constant avoidance of obstacles, (5) the mem-
orization of all this information, so the return trip is possible, (6) the veriﬁcation if
reached and expected targets correspond to each other, etc.
The spatial knowledge is a key element for the quality of life.
This knowledge may emerge from the possible interactions between speciﬁc
types of sensory data and the interactions of the cognitive strategies involved in the
spatial mobility inside small, medium and large-scale environments [4–6] (personal,
near, and far spaces).
The rest of the chapter is organized as follows. Section 2 brieﬂy discusses the
current state of art on sensory data which seems to be involved in space perception
emergence, while the Sect. 3 discusses the movement perception and spatial nav-
igation. Section 4 outlines some neural evidences of spatial cognition. Section 5
addresses some ﬁndings of spatial cognition in animals. Section 6 recalls some of
the most popular models for spatial knowledge structuring. The concluding Sect. 7
encompasses some open questions of spatial knowledge of visually impaired people
(VIP).
2
Sensory Information and Perception of the Movement
Several heterogeneous sources of sensory data lead to the perception of the space
(the environment) and to the experience of travel. These data come from the
navigable space (such as vision data) and from body’s motion ego-perception (such
as proprioceptive and vestibular data) which occur during the interaction with space
via the navigation.
2.1
Vision Data
The investigations on the interplay between visual data and mobility are usually
spread in two categories: optic ﬂow and visual cues/clues/landmarks.
2.1.1
Optic Flow
The human retina is stimulated by a reﬂected light generated by a given location in
the space. During the navigator’s motion in space, the image of the location on the
retina moves as well. The dynamic relationship between the navigator and the
visual motion of the object is called “optic ﬂow” [6, 7]. For Lee [8], the visual
perception starts with optic ﬂow.
138
E. Pissaloux and R. Velázquez

Optic ﬂow plays several functions during the mobility:
(1) it allows the estimation of distances from the viewed objects;
(2) it allows 3-D object recognition (frequently from their shape) [9], without any
additional information from the vestibule [10];
(3) it allows to implement the simple navigation strategies such as path integration
[11, 12];
(4) it controls the posture during mobility [13, 14], and the posture adaptation
during the obstacle avoidance or body-motion alteration [15–17]. In experi-
mental conditions, where the optic ﬂow is the sole environmental data available
to the navigator, the modiﬁcation of the optic ﬂow can modulate the mobility
behavior without destabilization of the navigator’s movement [18].
(5) it enhances the perception of the body motion [19, 20] and of the position of the
head during the movement [21].
However, the knowledge of the optic ﬂow alone does not guarantee the precise
estimation of distances and orientation during the mobility [22]. The traveled dis-
tance underestimation is a basic error is such case [23].
The optic ﬂow data in combination with other visual indices (visual cues/clues/
landmarks) can lead to accurate ego-motion perception and correct estimation of the
ego-distances, which are fundamental for safe mobility [24]. The combination of
optic ﬂow and distal vision cues or landmarks increases the reliability of
ego-motion cues estimation (self-rotation and heading direction toward distal
goals), but does not reduce the error of the distance underestimation [21].
2.1.2
Importance of Visual Cues/Clues/Landmarks
Near and far static speciﬁc objects, which may be perceived with sight, are named,
respectively, visual cues, visual clues, and landmarks. Visual clues are the mobile
visual cues, while landmarks are visual objects which may be perceived from
almost any place of a large space. In the remainder of this chapter, and par abuse the
language, the word “cues” will designed three categories of objects; cues, clues, and
landmarks.
All cues, once occurred in the visual ﬁeld can be used as references while
traveling in the space. Despite of the fact that optic ﬂow is a fundamental source of
the data for guidance and motion anticipation, the available visual cues have several
important roles during the mobility [25].
The main results on the importance of visual cues are recalled hereafter.
First of all, visual cues signiﬁcantly increase the stability and control of balance
[26] whenever visual cues are perceived in central or peripheral retinal ﬁeld [27].
For example, Azulay et al. [28] reported that the Parkinsonian subjects improve
signiﬁcantly their ability to move steadily and continuously if the visual strips were
integrated on a featureless ﬂoor because their vision has more data to provide to the
balance system. Philbeck et al. [29] found that even very short (instant) perception
of cues may signiﬁcantly improve the locomotion and the reaching of the target.
On Spatial Cognition and Mobility Strategies
139

Second, the visual cues are useful for object location in the mobility space.
Indeed, they are useful for obstacles avoidance and for gait adaptation [30], and for
veriﬁcations
between
planned
and
effectively
executed
path
(via
(conscious/unconscious) matching of expected and effectively perceived cues).
Third, the distance to an object is estimated at the beginning of the trip (in its
starting point). Consequently, there is no continuous update of this distance during
the displacement. The distance estimation is independent of the 3-D motion per-
ception and of the optic ﬂow [31].
Fourth, precision of distance and angle estimation may be increased when a cue
is seen by the navigator [32].
The reliability of visual cues, i.e., the coherence of the information attached to
them in the global traveled space, is a key point for spatial mobility. Jacobs [33]
claims that reliable cues play a dynamic role in the visual and spatial learning.
The importance of visual information has been shown through several experi-
ences. Breuneval [34] noticed that a drift from the planned straightforward path
occurs when congenitally blind subjects move in large open spaces (such as the
university hall). Cornell and Bourassa [35] observed that the blindfolded subjects
underestimate or overestimate angles while traveling on, respectively, more curved
or less curved paths. The tactile version of “homing” experience [36] conﬁrms these
latter results. During a turnaround of an obstacle more than 60% of blindfolded
participants did more or less one turn, but they were convinced that they did one
whole single turn (360°).
This conﬁrms that nonvisual information alone is insufﬁcient for accurately
encoding the angular value of the travels [35, 37].
2.1.3
Putative Explanations of the Dominant Role
of Vision During the Mobility
Several studies conﬁrm the dominant role of vision in space perception and
understanding during navigation [38, 39]. Other senses, such as vestibule or
kinesthetic data, contribute only to a rough estimation of external space parameters
(distances, angles, directions, etc.) [40, 41].
Several plausible explanations of the dominant role of vision exist.
Berthoz [42] links vision dominance to the (huge) size of cerebral network
involved
in
visual
data
acquisition
and
processing
(from
low-cognitive
(sub-cortical) level to high cognitive (cortical) level with interactions with atten-
tional and conscious processes). Waller and Greenauer [43] explain the visual
dominance over proprioceptive and vestibular data by source of the processed
information, respectively, external (external-world-dependent data) and internal
(ego-dependent data). As for us, the vision dominance seems to be ﬁnally related to
the size (volume) of the data which must be processed.
Ohmi [20] claims that the visual data is necessary and sufﬁcient for ego-motion
(speed and acceleration) perception, while Li et al. [44] showed that these data are
fundamental for controlling heading direction and locomotion.
140
E. Pissaloux and R. Velázquez

Finally, the vision seems to structure proprioceptive and vestibular data, espe-
cially the ego-distance estimation [45].
When reproducing, during the mobility in illumination conditions, a passive
rotation previously performed in darkness, the oculomotor activities indicate how
the vision system involves the way vestibular data are memorized with respect to
different patterns of space arrangements.
Moreover, the vestibular activity is linked to different navigational strategies.
According to Siegler et al. [46] experience, participants rotated in complete dark-
ness with large shifts in beating ﬁelds used an allocentric navigation strategy when
they perform a rotation task, while other participants used an egocentric navigation
strategy for the same task.
More recently Israel et al. [47], showed experimentally in the “without vision”
condition that for short distances (2–4 m) the distance reproduction is better than
for the longest ones (usually undershot) and the error is smaller without vision at
short distances; therefore, vision is not necessary for linear short distance
navigation.
2.2
Proprioception
The proprioception is the information acquired by the body of relative positions of
its subparts while moving [42]. This concept well known to musicians, corresponds
to the construction of a body internal presentation which encodes all muscular,
articular, and tendon sensors data in order to calculate the precise movements of
articulations with respect one to another, their velocity and the strength.
The proprioception is of fundamental importance to keep one’s orientation in
space, to allow one to walk in complete darkness (and keeping his/her balance), to
allow a violinist to play a concert with closed eyes. The proprioception reinforces
the perception and learning of visual inputs while moving and allows the update of
the body internal presentation necessary for successful locomotion [48, 49].
According to [50], the appropriate integration of the proprioceptive stimuli
improves the execution of spatial tasks such as path integration. In the case of
failure of the proprioceptive system (for some pathological or emotional reasons),
subjects cannot move normally their body in the space and must relearn the con-
venient movements by visually controlling the body movements [51].
In healthy subjects, the proprioceptive information is insufﬁcient for a precise
estimation of the ego-motion during the mobility. This limitation may be due to the
interaction between vision and proprioception [52]. However, in some groups with
very well trained proprioception (such as musicians, dancers, circus people, skiers),
the locomotion seems to rely mainly on proprioceptive data without any vision
coordination [13].
The above ﬁndings tend to conﬁrm a certain degree of plasticity and of adaptive
ﬂexibility of the human locomotion system [53].
On Spatial Cognition and Mobility Strategies
141

2.3
Vestibular Data
The vestibular system seems to be the principle control system of balance. It is
localized in the vestibule (inner ear) and is composed of two anatomical structures:
– the three semi-structural canals (stimulated during head angular motion as they
are sensitive to rotations and to the perception of angular displacements, [54,
55]), and
– the otoliths (stimulated during head linear translation).
Therefore, the vestibular system provides data on perception of velocity (linear
and angular) during the locomotion and assists the orientation in mobility space.
Moreover, the vestibular system keeps a stable external image on retina during
the locomotion. Indeed, the vestibule-ocular reﬂex (VOR), neural links between
vision and the vestibular system, is in charge of providing a clear vision during the
head and body movements. During locomotion, the head movements must be
rapidly compensated in order to generate the clear view of the traveled space
(otherwise, the image of this space will be a sequence of individual separated
images). The visual coherence of a 3-D space is achieved with signals projected
from the semi-circular canals to eye muscles via three neurons (three-neuron arc).
Indeed, this arc transmits signal of VOR mechanism, an automatic eye movement
compensation mechanism generated from head movements.
The VOR is a mechanism independent of any other sensory systems. When
during the travel, the mobility target is modiﬁed, the locomotion mechanism
plasticity can involve the VOR reorganization only even in the absence of vision
[56].
VOR contributes to the emergence of the percept of position and of ego-motion
in the space [57]. However, the VOR alone is insufﬁcient to maintain the stable
image on retina during the motion [58]. Indeed, the vestibular system usually acts in
conjunction with other sensory mechanisms (combination of heterogeneous data).
The combination of
(a) vestibular and vision data contributes to the better perception of the body
motion and the execution of high level cognitive tasks sub tending the mobility
[59];
(b) vestibular and proprioceptive data contributes to the static representation of
traveled path, in both real and virtual navigation [60];
(c) vestibular and podokinetic data contributes to accurate perception of body
orientation in space [61];
(d) vestibular and somesthetic data contributes to the estimation of the traveled
distance and some dynamic characteristics of the motion such as speed and
duration.
All these known aforementioned functions are fundamental for human mobility
reproduction [62] and prove the role of locomotion in human evolution.
142
E. Pissaloux and R. Velázquez

2.4
Multisensory Data Integration and Its Brain Support
Multisensory integration is the process which allows the coherent perception of an
experience (e.g., the mobility) thanks to the combined information originated from
different sensory and human body sources. For example, optic ﬂow and proprio-
ceptive data are integrated during gait and locomotion [63]; proprioceptive,
vestibular, and (peripheral) vision data are combined in body tilt perception [64];
the constant velocity ego-motion percept emerges from a combination of vestibular
and vision data [65].
Sensory integration relies on speciﬁc brain networks, however, the precise
integration process is unknown and prior knowledge is often required for inter-
preting the sensory signals [66].
The sensory data (visual and auditory) start to be processed in structures such as
mid-brain and brainstem.
Superior colliculus (a part of the mid-brain) contains multisensory neurons and
are involved in the (saccadic) eye movements, multisensory and visuo–motor
integration, and eye-head coordination [67].
The superior temporal gyrus, linked to the superior colliculus, participates in
integration of visual and auditory stimuli.
The medial parieto-occipital cortex is activated by visual stimuli, while the
parieto-insular vestibular cortex is not activated. This implies that a cortical
visual-vestibular inhibitory mechanism exists and it is activated during the
ego-motion. The existence of such mechanism may be related to the minimization
of the potential multisensory (visuo-vestibular) incoherencies in cases such an
unexpected head acceleration during the locomotion; in such conditions, only
reliable and dominant sensory information could be used for ego-motion perception
[65].
More recently, a “perceptive binding” mechanism has been discovered [68, 69].
This, not yet well known temporal-perceptive, process seems to be based on parallel
but independent processing of different sensory modalities in different brain areas,
which results are later combined using temporal information (temporal synchro-
nization; [70]).
The sensory integration may lead to sensory conﬂicts which may occur when the
mismatch occurs between acquired and the expected sensory data during a speciﬁc
spatial tasks [71]. Several theoretical frameworks and computational models have
been proposed for such situations. Zacharias and Young [72] and Borah et al. [73]
model the interaction of visual and vestibular data during the motion. In 1992
Glasauer [74] proposed his model of the interaction of visual and vestibular data
during the motion delivering the similar predictions to Borah’s model. Merfeld and
Zupan [75] extended Borah’s model to interactions of sensory and VOR infor-
mation during spatial orientation tasks.
The information generated during mobility is usually redundant [76], as data
from different modalities may be combined differently through different brain
networks. For example, in the case of mobility (walking), the brain receives
On Spatial Cognition and Mobility Strategies
143

information from vision, vestibular, and proprioceptive systems. The vision system,
through optic ﬂow and visual cues, contributes, between other functions, to the
detection of objects, to distance estimation, to path planning, to effective path
recognition, etc.; the vestibular system contributes, between other functions, to the
estimation of the velocity modiﬁcations (acceleration) and rotations’ angles; the
proprioceptive signals give feedback of posture and the course of displacement.
Recent works have shown [37], that one sensory modality alone does not pro-
vide sufﬁcient data for emergence of a precise concept from perceived stimulations,
as their values can be noisy or insufﬁcient [36]. Therefore, as some authors suggest
[77, 78], the availability of various sensor stimuli may not improve the emergence
of a speciﬁc concept but may deﬁne the area to which it belongs.
3
Movement Perception and Spatial Navigation
The sensory information related to spatial navigation is processed by dynamic
mechanisms called cognitive strategies. The three main human mobility strategies
are: path integration (Sect. 3.1), landmark-based strategy (Sect. 3.2), and
geometry-based strategy (Sect. 3.3) [79, 80].
3.1
Path Integration
Path integration (PI) is a continuous cognitive process which updates the current
position (location) of the navigator with respect to a given reference point, usually
path starting point. This update uses signals generated by the locomotor system
from performed translations and rotations [2, 81, 82].
Two subclasses of PI can be considered: simple and complex.
In the cognitively simple spatial tasks, the human navigator exhibits elementary
behavior through paradigms such as distance estimation, following a linear segment
or triangle (homing) task.
The classic triangle homing task involves naïve blindfolded participants. They
learn to navigate and to estimate the distance between two line segments: AB and
BC; next they navigate from A to C directly (not via B). The ﬁnal goal of this task is
the distance estimation after the locomotion based on solely proprioceptive and
vestibular signals.
Proprioceptive cues, i.e., kinaesthetic data generated by the motor system, are
fundamental for traveled distance estimation. If the pace rhythm varies from usual,
the traveler overestimates (in the case of higher rhythm) or underestimates (in the
case of lower rhythm) the distance unless some vision information is available [83].
Vestibular data are linked directly to the performed rotations and accelerations.
In order to return to his/her starting point following a predeﬁned path, the traveler
144
E. Pissaloux and R. Velázquez

needs a global orientation toward the starting point. Physical rotations of the body
modify the status of vestibular data; they allow the orientation estimation.
Although proprioceptive and vestibular data are necessary for PI, the vision data,
even brieﬂy presented to the navigator during the mobility, signiﬁcantly improve
the PI and the homing is successfully achieved [36, 84].
For a simple travel path in the homing task, with few rotations and few linear
segments, the PI may produce the spatial representation of the traveled path [85]. For
more complex travel patterns, the PI performance reduces drastically, and systematic
errors in heading direction estimations and in distance estimations occur. Wang and
Spelke [79] suggest that if the presence of a cognitive presentation of the traveled
path would occur during the PI, errors in distance and orientation estimation would
be independent of the complexity of the traveled path and no systematic relation
between them would be noticed. Fukita et al. [85] and Foo et al. [86] argue that the PI
cannot lead to a precise cognitive presentation of the traveled path, without other
navigational cues such as landmarks. Experiments by Rieke et al. [87] suggest that
the visual PI, without any vestibular or kinesthetic cues, can be sufﬁcient for ele-
mentary navigation tasks such as rotations, translations, and triangle completion.
However, the precise contribution of the PI to spatial cognition is still an open
question.
3.2
Landmark-Based Mobility Strategy
A landmark is an object which offers unique speciﬁc perceptual characteristics
which make it distinctive from other objects located in its vicinity. Landmarks are
used during way ﬁnding and spatial learning [88, 89]. These perceptual charac-
teristics are of different nature; shape, color, texture, form, etc. They are mainly
vision characteristics while smell, temperature, or environmental noise are char-
acteristics which may be perceived with other senses.
Landmarks may be global or local for a considered navigable space. A global
landmark can be perceived from any point while a local landmark is perceived only
locally (in a short distance). Both, global and local landmarks may provide direc-
tional and positional information, however, a global landmark will provide such
data regardless the position of the navigator in this space, while the local landmark
may provide them in a short distance to the landmark. Some authors [86, 88, 90]
suggest that the navigator learns landmarks in decision points and navigates using a
graph of memorized landmarks. Visually impaired people learn landmarks also in
points which allow them to conﬁrm the progress of their travel [34, 91, 92].
Familiar landmarks play a deﬁnite role in remembering the decision points during
mobility [93].
The use of either global or local landmarks varies with personal preferences [94].
The local landmarks are more important in active (real) than in virtual navigation
[86], while the global landmarks contribute to the distance estimation in large-scale
environments [95].
On Spatial Cognition and Mobility Strategies
145

Landmark-based navigation strategy is an episodic process of spatial learning [2]
and is based on “episodes” bound to the considered landmarks in such a way that all
episodes lead to the reconstruction of the trajectory of the path taken.
The use of landmarks varies with age. Children (of 30 months old) use land-
marks in order to ﬁnd a hidden object in a room [96]. Children and pre-adolescents
exploit (different) landmarks for distance estimation during navigation [97]. Adults
use landmarks in conjunction with path integration (PI) strategy as landmarks
increase the accuracy of navigation [86].
Landmark navigation strategy contributes to the updates of space knowledge and
to the recognition of familiar view-dependent scenes [98].
Landmark-based spatial learning activates speciﬁc brain networks such as para-
hippocampus and hippocampus cortex [99–102].
3.3
Geometry-Based Navigation Strategy
Geometry-based navigation relies essentially on the knowledge of the geometry (a
global conﬁguration) of the navigable space, i.e., on objects of the navigable space
and their relative spatial positions (metric–geometric relationships).
Usually, the geometry data is a visual information, but it can be also tactile,
kinesthetic, and auditory information.
During navigation this information is updated with an episodic cognitive
process.
The effectively used strategy when performing various tasks is a function of at
least two parameters: size of the space and form (shape) of the space, and it varies
with age.
During the reorientation tasks in a small room (4  6 ft) (with a goal reaching,
i.e., ﬁnding a hidden toy), children (until the age of 3 years old) use mainly the
geometry properties of a room [79, 103, 104]; after disorientation in a familiar room
children rely on symmetrical geometrical properties of the room. In larger rooms
(8  12 ft) with furniture, children rely also on landmarks and reorient themselves
to ﬁnd the hidden toy [96, 104]. In adults, the orientation tasks are mainly based of
landmarks of a (small or large) space.
The space form is another important parameter for navigation strategy.
Experimental studies with children conﬁrm that rectangular-shaped spaces
provide a limited geometric data as there is no distinctive information (as all angles
are of 90°); other shapes (isosceles triangles, kite-like shapes, trapeziums, rhombus)
are more informative [105–107]. Furthermore, the same studies evidence that
landmark and geometric information in the enclosed spaces are unique and undi-
vided knowledge globally and simultaneously encoded by children.
Experimental studies with adults show that for large-scale environments the
geometry inﬂuences the estimation and reproduction of the traveled distance.
Ruddle and Peruch [95] report that when the perimeter of the environment was
oblique, the navigator follows a longer segment than when the perimeter had an
146
E. Pissaloux and R. Velázquez

orthogonal shape. Buckley et al. [108] report that in virtual navigation, humans
encode a global representation of the overall shape of the environments in, or
around, they navigate.
Based on animal studies, Gallistel [2] proposed the “long axis” theory of nav-
igation. This theory hypotheses that the space geometric information is encoded
using main axis of the navigable space (and not global geometric conﬁguration) as
the entire space is structured around the main axes. Consequently, the environ-
ment’s detailed structure has little impact on navigation as the navigator mainly
focuses on the axes and not on the perimeter of the environment.
Valiquette et al. [109] experimental works support Gallistel’s theory. An array of
objects placed inside orthogonal spaces were memorized and then recalled by
adults; the collected results show that the objects were learned by respecting the
axes of the orthogonal layout and were recalled as if positioned in rows and col-
umns with respect to these axes.
It should be noted that the GPS-based way ﬁnding implements Gallistel’s theory.
Indeed, the “long axis” theory requires the small cognitive load and it is the sim-
plest for explaining, memorizing, and implementing during the navigation [36].
3.4
Reference Frames
Reference frames are means to ﬁnd data in the space to which they are attached.
Cognitive reference frames allow to understand which data are encoded by brain
mechanisms.
Two main reference frames are considered: the egocentric and allocentric.
Egocentric (or ﬁrst person) frame encodes spatial data (e.g., locations) with respect
of the observer’s body perspective and position. Allocentric (or third person) frame
encodes spatial data using an external—to the observer- point which is independent
from the observer.
The human brain uses two reference frames for spatial data encoding.
The allocentric reference frame allows to encode relative positions of several
ﬁxed objects simultaneously, without interfering with the observer’s position and
view point. It is possible to speak about “map-like” (or mapping) encoding obtained
with “survey learning” process. This process seems to be implemented during
mental (virtual) navigation in a space, if the user mentally visualizes the map and
moves in it [110, 111].
The egocentric reference frame seems to be used in the process where spatial
learning is implemented during the route learning via active navigation of the
observer in the space.
As both reference frames are important for mobility assistance, both should be
supported by the technologies when necessary.
On Spatial Cognition and Mobility Strategies
147

4
Neural Evidences of Spatial Cognition
This section proposes a state of art on the neural supports for spatial cognition and
spatial learning (cf. [112–114] for detailed overview of the question). This section
may lead to new ideas on the design of control systems for intelligent mobility
assistances for visually impaired people, for humanoid robotic systems, and more
general, for autonomous mobile intelligent system.
4.1
Hippocampus and Spatial Information
Scoville and Milner’s studies [115] on spatial memory established the contributions
of the hippocampus and some parts of parietal cortex; epileptic subjects with
excision in hippocampus showed sever spatial amnesia.
Berthoz [42] recalls several studies which deep Milner’s results to parahip-
pocampal area and includes the processing of multisensory spatial information
(vision, smell, auditory).
Spatial navigation capability and hippocampus activities are strongly correlated
[110, 116, 117]. The hippocampus is activated on the view of places during the
navigation [100, 116], on encoding [118] and on retrieving of topographic data
[119, 120]. The hippocampus role in allocentric topographic data encoding is
unclear [100, 119 versus. 103, 121].
Several studies conﬁrm the role of hippocampus in route planning toward a
speciﬁc target inside large-scale familiar spaces [122].
Other studies suggest that the hippocampus and other structures of the
medial-temporal lobe are fundamental for spatial information encoding in the
long-term memory, but they do not participate in recalling of spatial data encoded in
the remote past [123].
The hippocampus plays an important role in a target-oriented navigation as it
retrieves information from a cognitive map stored as allocentric representation of
the known space in order to perform path planning tasks [122].
The activation of both hemispheres of hippocampus differs. The patients with
lesions in the right hemisphere are unable to perform spatial allocentric tasks [124]
such as survey learning [111], while those with lesions in left hemisphere perform
similarly to the healthy subjects.
4.2
Parahippocampus and Spatial Encoding
The activation of parahippocampus occurs during the egocentric spatial tasks such
as navigation [116, 125], especially during landmark recognition and spatial views
[99, 100].
148
E. Pissaloux and R. Velázquez

Parahippocampus activation may be bilateral or unilateral.
Bilateral activation was observed when encoding data of large-scale environ-
ments or buildings. Unilateral activation, limited to the right hemisphere was
observed during this information retrieval from a long-term memory [126] or during
the recalls of familiar landmarks located in a speciﬁc spatial context [99].
Burgess studies showed the activation of the right parahippocampus and right
posterior parietal and posteriodorsal medial parietal cortex during the retrieving of
spatial context [127].
Deﬁcit of the allocentric presentation and of visuospatial encoding and recalling
was found with spatial disorientation tasks performed by subjects with lesion on the
right medio-temporo-occipital lobe (parahippocampal sulcus included) [128].
4.3
Hippocampus, Parahippocampus, and Spatial Memory
Hippocampus and parahippocampus work in synergy when the brain spatial pre-
sentation is generated [129]. Dynamic role of both organs is claimed by Aguirre
et al. [121] during topographic encoding and recalling.
Bilateral activation of hippocampus (when encoding allocentric data) and of
parahippocampal sulcus (when encoding the egocentric spatial data) was conﬁrmed
by several studies [127, 128, 130].
When retrieving encoded spatial information, the activation limited to right
hemisphere was observed by some authors [127, 128, 130], while others observed it
in the left hemisphere [102, 120].
Lambrey et al. [131] provide some elements for these contradictory results on
hippocampal activation. They designed an experiment for a navigation task in a
virtual reality maze and tested it with three groups of subjects: healthy (control),
right medial-temporal lobe (RMTL) patients, and left medial-temporal lobe
(LMTL) patients. The maze contained distinct objects (landmarks) in every road
intersection; the path to follow was designed by green walls, while red walls
indicated wrong directions. The navigation test in the maze has been performed ﬁve
times, and each navigation ended with a drawing of the traveled path (with closed
eyes). Participants named landmarks in the order they thought that they had
appeared during their navigation. The collected results show that both (left and
right) median-temporal lobes are involved in spatial memory but with different
contributions: right median-temporal lobe is implied in survey (allocentric) spatial
encoding, while the left median-temporal lobe is mainly implied in egocentric tasks
(associative learning, route memory). This laterization could explain the contra-
dictory results of the previous works on hippocampal activation.
The above results show clearly that the precise role of hippocampus and of
parahippocampus is still an open question.
On Spatial Cognition and Mobility Strategies
149

4.4
Parietal Cortex and Spatial Processing
Parietal cortex plays an important role in egocentric spatial processing [132].
During route learning (i.e., egocentric spatial encoding) an important activity in
posterior superior and inferior parietal areas (the precuneus included) was evi-
denced [121, 133, 134]. Wolbers et al. [133] suggested the involvement of parietal
cortex in temporal processing during learning tasks.
Works by others [99, 111, 121] show that parietal cortex and precuneus is
involved in allocentric spatial tasks such as way ﬁnding and changing of the view
point (tasks which are based on cognitive maps).
Studies with hemi-neglect subjects showed the importance of parietal structures
in attention processes. In most of the cases these subjects have a lesion in the right
parietal lobe, and present attentional deﬁcit for the visual stimuli in the opposite
(left) visual ﬁeld.
Therefore, it is possible to say that parietal cortex plays an important role in
spatial learning and recalling for either egocentric or allocentric reference frames of
space presentations. It seems also that the parietal cortex is a part of a larger
network involved in the spatial encoding and learning processes.
4.5
Frontal Cortex and Spatial Processing
The activation of the frontal cortex was evidenced during both encoding and
learning processes, independently of the used reference frame.
Superior and medial frontal gyri are activated during the encoding of routes and
of spatial environments [133], during the recalling in way ﬁnding tasks and in
environment recalling [99, 111].
Prefrontal cortex is activated when performing some process-speciﬁc functions
such encoding, recognition and recalling [135, 136], and this is independent of the
context-speciﬁc functions (spatial vs. non spatial memory).
This speciﬁc role of the prefrontal cortex was evidenced by neuroimagery
studies (PET, fMRI) during which the activation of the dorsolateral prefrontal
cortex was registered during the way ﬁnding tasks [120].
According to the above results it seems that the frontal cortex is a coordinator of
memory and way ﬁnding processes.
4.6
Wide Neural Network and Spatial Processing
The aforementioned brain structure works in cooperation and redundancy with
other structures in the implementation of high level cognitive spatial tasks [137] via
wide brain networks.
150
E. Pissaloux and R. Velázquez

Committeri et al. [130] showed that:
– retrosplenial and ventromedial occipito-temporal cortex, and ventrolateral
occipito-temporal cortex are involved in allocentric spatial tasks;
– dorsal stream (connected to frontal regions) are mainly involved in egocentric
spatial presentation (ﬁrst person);
– distinct neural substrates are activated for ego and allocentric tasks.
In complex environments the synergistic activity of different cortical areas was
stated. The egocentric tasks activate parietal and frontal premotor areas, while
allocentric tasks activate the parietal-frontal, ventromedial occipito-temporal and
retrosplenial regions.
In topographical and spatial orientation tasks (such as activation of cognitive
maps) retrosplenial and anterior hippocampal cortex work in parallel and in synergy
[138, 139]. Especially, the retrosplenial cortex plays a signiﬁcant role in the
coordination of the spatial information retrieved during the mobility by integrating
new information, updating the already existing spatial presentation of the cognitive
map related to a speciﬁc environment. This allows the update of the ongoing route
planning [122].
The above-related studies indicate that several distinct brain areas are involved in
the spatial behavior processing; however, the precise role of each area is still unclear.
Possible explanations of such situation may be the complexity of the cognitive
processes underlying the mobility and the inadequate experimental instruments and
procedures, huge number of parameters which underpin the simplest mobility
function and, ﬁnally, large interpersonal variability of cognitive behavior during the
mobility tasks.
5
Spatial Cognition of Animals
Scientiﬁc knowledge of animal spatial navigation and spatial encoding allowed
progresses on human spatial cognition thanks to the phylogenetic theory which
assumes that brain structures evolved according the species’ evolution in an
ascending way, from low-level species to above species until humans.
This section recalls the main ﬁndings on cognitive mobility strategies in animals
which are similar to human main strategies: path integration, geometry-based and
landmark-based strategies.
5.1
Path Integration and Homing
PI is a navigational strategy common in all animal species, from insects to mam-
mals, activated mainly outdoor in large environments [140].
On Spatial Cognition and Mobility Strategies
151

PI in rodents (including hamsters, rats, and mice) is generated by internal signals
produced during the locomotion such as proprioceptive and vestibular cues [141].
Self-motion cues allow constant estimation of the animal’s position with respect
to its nest as the reference (initial) point. The PI alone during foraging does not
allow an accurate homing process. In animals like ants and hamster, PI vector
undershoots the traveled distance; the longer the journey, the greater the underes-
timation is. In order to compensate this distance underestimation and return home
effectively, animals rely on the view of vision cues (landmarks). These landmarks
conﬁrm the PI vector’s estimation and correct it, if necessary [139, 141]. Collet and
Graham [142] distinguish two types of vision cues based on whether vision cues are
interaction with it or not.
Insects, ants and bees can use either landmarks-based navigation in familiar
environments or PI (vector navigation) in unfamiliar environments. The precise role
of landmarks (visual, olfactory, etc.) is still an open question [142]: does the view
of landmarks reset the estimate of the PI vector? Does the view of a landmark is
used for conﬁrmation of the vector estimation toward the nest? In their inbound
travel ants use a simple approximation of the PI vector calculation; this vector is
subject to small systematic errors; therefore, this vector does not allow to reach the
nest itself but only the nest periphery [140]. For this purpose, ants rely on familiar
landmarks positioned around and in proximity to the nest [143]. More recent studies
[144, 145] claim, that “for [ants (rats)] robust scene recognition what matters is the
spatial arrangement of the objects across the scene, and not the identiﬁcation of
speciﬁc individual objects” [145].
Bees need to rely on visual or other cues (olfactory) even in order to perform PI;
it seems that PI in bees is considered as a complex behavior [146, 147].
Studies in rodents contradict these results. The role of landmarks in rodents’
navigation is unclear. Some experiments shows that PI vector can be redirected
when viewing familiar cues [148–150], therefore visual cues provide spatial
information, computed along PI, and useful for the update of homing vector.
Studies by others [150] show that a visual cue resets the PI and allows direct return
to the nest based of the visual information. Recent works suggest that landmarks
coordinate the activation and encoding mechanisms like PI [151].
PI is a common behavior in the animal kingdom, as mobility is a fundamental
activity for their survival (e.g., food collection, safe homing, etc.). However, the
exact mechanism of PI and the way how external (visual, olfactory, etc.) data
impact the PI is still an open question.
5.2
Geometric Based Navigation Strategy
In indoor space, where vision landmarks are available for reorientation, the non-
human species rely mainly on geometric properties of the navigable space for
navigation subtasks (orientation, recognition [152]).
152
E. Pissaloux and R. Velázquez

Two problems have been investigated so far: (1) which geometric properties are
exploited during the spatial navigation (and their encoding); (2) relationship
between these geometric properties and other spatial properties such as landmarks.
The ‘disorientation task’ is a classic paradigm for search of the geometric
properties. Several studies show that many animal species rely essentially on
geometric cues (e.g., the global conﬁguration of the navigable space) in order to
reorient themselves inside limited environments. Rodents [148, 153], pigeons,
rhesus monkeys [154] reorient themselves using the global shape of the near closed
environments, regardless of the type of the shape (rectangular, square, etc.) and its
size [155]. Humans use not only cue salience but also cognitive complexity of the
environment in which human is supposed to reorient to [156].
All the above studies suggest that geometric properties of the navigated envi-
ronment are modular and may be supported by a “brain geometric module” acti-
vated during spatial tasks [152]. Moreover, animals encode geometric data
independently of the presence (or absence) of other environmental cues.
This means that independently of other sensory cues experiences by animal
during the navigation, the geometric learning of the environment is a compulsory
process of its spatial behavior [152, 157, 158].
Principal axes of the navigable environment seem to be mainly geometric data
encoded [144, 145, 152, 159]. These results support Gallistel’s ‘long axis’ theory
for mammals’ spatial navigation [2]. Consequently, the learning process during
animal navigation seems to be concentrating on spatial data near by the axes (cf.
§3.3. for the human learning process [160]).
5.3
Landmark-Based Navigation Strategy
Generally, animals rely on visual landmarks in spatial tasks. Honeybees use
extensively landmarks when approaching to the target [161, 162], when they need
to redirect PI vector. Ants associate the view of landmarks nearby their nest with
local PIs; during inward journey these vectors are activated only in the presence of
the known landmarks. The encoding mechanisms can be activated independently
but in parallel to the PI mechanism [163].
Vision landmarks seem to dominate other types of information. Adult rats,
contrary to young rats, privilege vision cues over olfactory cues [164]. Mountain
chickadees seem to ignore the geometric features of the environment when other
salient visual landmarks are available [165].
There is no scientiﬁcally proven explanation of such dominance. Experiences
with rhesus monkeys [154] indicate that they tend to rely on voluminous landmarks
to distinguish symmetrical corners inside test-rooms and tend to ignore small-sized
landmarks. This suggests that local and small-scale environments do not sufﬁce to
dominate or overshadow the geometric encoding.
Landmark encoding and geometric encoding of spatial knowledge in rats are
probably due to distinct mechanisms, possibly independent, but usually activated in
On Spatial Cognition and Mobility Strategies
153

parallel. In some circumstances the landmark-based encoding may overshadow
(without cancelling) the encoding of the geometric features [166]. However, no
competition is observed between landmark and geometry encoding in the case
when the geometry-based learning uses global shape and not local geometric
properties [167]. In goal searching tasks, if geometry and vision cues are coherent
neither competition nor overshadowing was stated.
When two stimuli are not coherent, vision stimulus dominates spatial encoding
[168]. Finally, McGregor et al. [169] claim that spatial learning based on local
landmarks and global landmarks are two mechanisms which may work indepen-
dently and in parallel, one complementing the other.
6
Toward a Structure of Spatial Knowledge
Different concepts have been proposed to model the spatial knowledge. In this
section we present the most popular model: cognitive map (Sect. 6.1); its physio-
logical evidences (Sect. 6.2), and two models, sequential and parallel, for spatial
knowledge acquisition (Sects. 6.3 and 6.4, respectively).
6.1
Concept of the Cognitive Map
Gallistel [2] deﬁnes a brain representation as a functional isomorphism in the
mathematical sense of the term, that is, a bijective correspondence between two
systems. Thus, in a representation of the space it is possible to ﬁnd the information
giving a coherence to the environment in which one evolves.
The term “cognitive map” has been introduced by Tolman [170] as a brain
presentation of navigated spatial location, while the cognitive mapping designs a
process of spatial knowledge acquisition and this knowledge coherent structuring in
a robust presentation which may control the motor system during the navigation.
The content of the cognitive map is established from all sensory and kinesthetic
data which occur during the mobility. Indeed, the mapping is activated by pro-
cessing of sensory data, including perceptual properties of the physical space,
subject internal state including the memories produced by previous experiences
with space interactions (physical properties of the environment, socioeconomic
data, expected data on the current environment, etc.) [171, 172]. The space pre-
sentation includes both, allocentric, and egocentric forms of spatial knowledge
[172].
Topological and metric data of environment experiences are fundamental ele-
ments of cognitive maps [2]. That is why two types of internal presentations of
space are sometimes considered: topological maps and cognitive maps.
According to [173], the spatial organization of sensory data is reﬂected in the
spatial conﬁguration of some of the data processed in the brain. Recent research has
154
E. Pissaloux and R. Velázquez

made possible to locate certain brain structures in which these topological maps are
observable. For example, in the parietal cortex different regions representing the
stimuli, according to retinotopic or somesthetic organizations [174].
However, this type of spatial organization of space only allows to reference the
objects in the space linked to the sensor. To construct allocentric representations, it
is necessary to create representations in a more global reference frame.
According to Denis [175], cognitive maps allow:
– to know where we are at any time in our environment,
– to identify the position of objects without direct perception,
– to choose the path to take,
– to communicate to others our spatial knowledge.
Some authors [176, 177] are interested in the mathematical properties, and
mainly in the metric of these spatial representations in order to propose models
reﬂecting the imperfections of our perception.
Cognitive maps are not identically produced in all humans, as their content is
related to personal experience. Landmarks contribute signiﬁcantly in the emergence
of coherent cognitive maps after physical navigation, and they probably have a role
of “anchor points” which structure the whole spatial information during cognitive
mapping [178].
6.2
Physiological Evidences for Cognitive Mapping
Many studies attempt to understand how representations of space are formed at the
level of our brain. Experiments initiated by O’Keefe and Dostrovsky [179, 180] in
rats revealed the existence of specialized cells for this task in the hippocampus.
Indeed, “place cells” discharge selectively, i.e., when the rat is in a well-determined
position of space. If the rat changes its place or if its space changes the content, the
mapping process resets all place cells and activates new place cells in relation to the
new space. The activity of these cells is directed by the information coming from
the different sensory modalities [181]. Mathematical models of the functioning of
these place cells have been proposed [182].
The place cells do not reﬂect the structure of space itself: they inform about our
position in the latter.
Another type of neural structures, head direction cells, provides a sense of
direction in some species (rodents and rats [183]). The cells also discharges
selectively and independently, the animal’s position in the environment, when the
animal’s head is oriented toward a speciﬁc direction. Place cells are localized in
anterior dorsal thalamic nucleus (in Papez circuit), and they continuously process
information of heading direction during locomotion [184].
Studies on human cognitive mapping show the involvement of hippocampus
which seems to process metric information and seems to contribute to the
On Spatial Cognition and Mobility Strategies
155

generation of allocentric cognitive maps [185]. Recent neuroimagery studies
showed that acquisition of cognitive maps in humans relies on hippocampal
function complemented by retrosplenial activity [138].
6.3
Hierarchical Models of Spatial Knowledge Acquisition
The ﬁrst models of the spatial knowledge acquisition process suggested that it is
acquired sequentially and by combination of different sources of spatial information
[176, 186].
Shemyakin [187] claims that “the acquisition of spatial knowledge progresses
consecutively from landmark recognition to path deﬁnition and the understanding
of general relational characteristics of areas”. This means that from more perceived
data you can generate more precise cognitive maps.
Hart and Moore [188] and Siegel and White [189] proposed multilevel models of
spatial knowledge acquisition. At the ﬁrst level the landmark knowledge is
acquired; at the second level, route knowledge occurs between landmarks. Both
elements are organized into clusters with metric relationships at third level. At the
ﬁnal level clusters are linked in schematic and systematic structures and they form
the survey knowledge.
These hierarchical models correspond to the way in which we organize the
information in order to compute it on our computer system, and their effective
existence in the brain is still an open question.
6.4
Models of Parallel Acquisition of Spatial Knowledge
These models hypotheses that different sensory data interact between them on a
quantitative (and not qualitative) basis [190]. Montello model [191] deﬁnes ﬁve
principles allowing to acquire the spatial knowledge; they are: (1) parallel acqui-
sition of all different types of spatial knowledge; (2) parallel increase of the volume
of acquired sensory data; (3) integration of elementary places into more complex
hierarchically organized knowledge; (4) inter-individual differences may play an
important role in the spatial knowledge integration; (5) linguistics systems con-
tribute to structure the already existing spatial knowledge.
Montello’s model was tuned by others.
Jackobs and Schenk [192] proposed the “parallel map theory” (PTM) based on
anatomical, functional and behavioral data collected from animals (reptiles, rodents,
primates). Two parallel cognitive maps work: the “bearing map” and the “sketch
map.”
The bearing map encompasses directional and idiothetic cues, and produces
coarse-grained presentation of space useful for long-distance navigation.
156
E. Pissaloux and R. Velázquez

The sketch map encodes all position cues (landmarks) respecting topographic
relationship between them. This is a ﬁne-grained map of a local space and suitable
for local navigation only.
Finally, a third-integrated cognitive map is built from the two above maps; it
allows short and long path navigations.
The PTM model has its own update process which allows the spatial information
recovery in the case of encoded data missing.
Roche et al. “neurocognitive map” [172] combines PTM based on animals
studies with studies on humans. The model proposes a presentation of human brain
structures that correspond to each cognitive aspect of the model. The acquired
spatial data belong to one of three classes: functional presentation, egocentric
presentation, and allocentric presentation.
The functional presentation corresponds to information acquired from path
integration during locomotion.
Egocentric representation encodes information of places visited during naviga-
tion, and is linked with route-spatial knowledge.
Allocentric representation is supplied by an external sources of spatial knowl-
edge such as a survey map.
The combination of these three mental constructions produces a neurocognitive
map (which is supposed to exist in cortical and sub-cortical areas).
The PI (or functional presentation) process is subserved by the vestibular and
proprioceptive systems, the parietal somatosensory regions, and the dorsal visual
system. Egocentric representation draws from the fusiform gyrus, inferior temporal
gyri, and superior parietal cortex. Allocentric representation involves the parahip-
pocampal gyrus and/or the lingual gyrus. This brain support of the neurocognitive
map needs more experimental studies.
The Mou et al. model [193, 194] is based on human behavioral data. This model
claims that human behavior is induced by the two subsystems, the egocentric, and
the environmental. The egocentric subsystem encodes the data received during the
locomotion and includes relationships “self-to-spatial” (e.g., obstacle avoidance
while walking); this information gets activated only during the navigation. The
environmental subsystem encodes the static and enduring elements of the navigated
environments and represents them in “intrinsic reference frame”. During spatial
navigation, the spatial encoding occurs by updating the intrinsic reference frame.
Recent work by Finaly et al. [195] provides support to this model.
7
Conclusion
The proposed overview shows that animals and humans spatial behavior display
several similarities. The same navigation strategies (PI, landmarks, and geometric
features) are used for the spatial learning and encoding the navigated space.
The contribution of PI in human navigation is demonstrated only in simple
spatial tasks such as homing. Future studies of PI’s participation in more complex
On Spatial Cognition and Mobility Strategies
157

spatial tasks are necessary. It is expected that these tasks may lead to a deep
comprehension of its role in human navigation and spatial cognition.
Ontological, neurocognitive and neuroimaging considerations evidence that
landmark-based strategy is a substantial mechanism for spatial cognition, providing
accurate encoding of spatial experiences.
Geometry-based encoding plays an active role in spatial cognition. However, its
precise role and interaction with other strategies of spatial learning (especially with
landmark-based strategy) is unclear.
Many theoretical, cognitive, and computational models try to understand the
cognitive and brain mechanisms involved in cognitive mapping. Some of these
models are based on biological studies in order to be biologically plausible and
reliable.
As far as visually impaired people (VIP) mobility is concerned, the main
question is how to adapt these models and the existing navigation strategies and
allow VIPs to understand and explore alone the space with their own perceptual
capabilities.
References
1. Held R, Hein A (1963) Movement-produced stimulation in the development of visually
guided behaviour. J Comp Physiol Psychol 56(5):872–876
2. Gallistel CR (1990) The organization of learning. MIT Press, Cambridge
3. Berthoz A (2000) The brain’s sense of movement. Harvard University Press, Cambridge
4. Tversky B (1993) Cognitive maps, cognitive collages, and spatial mental models. In:
Frank AU, Campari I (eds) Spatial information theory: a theoretical basis for GIS,
Proceedings of COSIT’93. Lecture notes in computer science, vol 716. Springer, Berlin,
pp 14–24
5. Tversky B (2001) Spatial schemas in depictions. In: Gattis M (ed) Spatial schemas and
abstract thought, MIT Press, Cambridge
6. Tversky B (2005) Functional signiﬁcance of visuospatial representations. In: Shah P,
Miyake A (eds) Handbook of higher-level visuospatial thinking. Cambridge University
Press, Cambridge, pp 1–34
7. Gibson JJ (1950) The perception if visual surfaces. Am J Psychol 63:367–384
8. Lee DN (1980) The optic ﬂow ﬁeld: the foundation of vision. Philos Trans Roy Soc Lond B
Biol Sci 290:169–179
9. Johansson G (1977) Studies of visual perception on locomotion. Perception 6:365–376
10. Landy MS, Dosher BA, Sperling G, Perkins ME (1991) The kinetic depth effect and
optic-ﬂow—II. First- and second-order motion. Vis Res 31:859–876
11. Kearns MJ, Warren WH, Duchon AP, Tarr MJ (2002) Path integration from optic ﬂow and
body senses in a homing task. Perception 31:348–374
12. Kirschen MP, Kahana MJ, Sekuler R, Burack B (2000) Optic ﬂow helps humans learn to
navigate through synthetic environments. Perception 29:801–818
13. Golomer E, Crémieux J, Dupui P, Isableu B, Ohlmann T (1999) Visual contribution to
self-induced body sway frequencies and visual perception of male professional dancers.
Neurosci Lett 267:189–192
158
E. Pissaloux and R. Velázquez

14. Israël I, Fetter M, Koenig E (1993) Vestibular perception of passive whole-body rotation
about horizontal and vertical axes in humans: goal-directed vestibule-ocular reﬂex and
vestibular memory-contingent saccades. Exp Brain Res 96:335–346
15. Lejeune L, Anderson DI, Campos JJ, Witherington DC, Uchiyama I, Barbu-Roth M (2006)
Responsiveness to terrestrial optic ﬂow in infancy: does locomotor experience play a role?
Hum Mov Sci 25:4–17
16. Nomura Y, Mulavara A, Richards J, Brady R, Bloomberg J (2005) Optic ﬂow dominates
visual scene polarity in causing adaptive modiﬁcation of locomotor trajectory. Cogn Brain
Res 25:624–631
17. Warren WH Jr, Kay BA, Zosh WD, Duchon AP, Sahuc S (2001) Opic ﬂow is used to control
human walking. Nat Neurosci 4:213–216
18. Konczak J (1994) Effects of optic ﬂow on the kinematics of human gait: a comparison of
young and older adults. J Mot Behav 26:225–236
19. Bertin RJ, Israel I (2005) Optic-ﬂow-based perception of two-dimensional trajectories and
the effects of a single landmarks. Perception 34:453–475
20. Ohmi M (1996) Egocentric perception through interaction among many sensory systems.
Brain Res Cogn Brain Res 5:87–96
21. van den Berg AV, Brenner E (1994) Humans combine the optic ﬂow with static depth cues
for robust perception of heading. Vis Res 34:2153–2167
22. Bakker NH, Werkhoven PJ, Passenier PO (1999) The effects of proprioceptive and visual
feedback on geographical orientation in virtual environments. Presence Teleoperators
Virtual Environ 8:36–53
23. Frenz H, Lappe M (2005) Absolute travel distance from otic ﬂow. Vis Res 45:1679–1692
24. Kellman PJ, Kaiser MK (1995) Extracting object motion during observe motion: combining
constraints from optic ﬂow and binocular disparity. J Opt Soc Am A Opt Image Sci Vis
12:623–625
25. Prevost P, Ivanennko Y, Grasso R, Berthoz A (2002) Spatial invariance in anticipatory
orienting behavior during human navigation. Neurosci Lett 339:243–247
26. Macuga KL, Loomis JK, Beall AC, Kelly JW (2006) Perception of heading without retinal
optic ﬂow. Percept Psychophys 68:872–878
27. Assaiante C, Marchand AR, Amblard B (1989) Discrte visual samples may control
locomotor equilibrium and foot positioning in man. J Mot Behav 21:72–91
28. Azulay JP, Mesure S, Blin O (2006) Inﬂuence of visual cues on gait in Parkinson’s disease:
contribution to attention or sensory dependence? J Neurol Sci 248:192–195
29. Philbeck JW, Klatzky RL, Behrmann M, Loomis JM, Goodridge J (2001) Active control of
locomotion facilitates nonvisual navigation. J Exp Psychol Hum Percept Performance
27:141–153
30. Rietdyk S, Rhea CK (2006) Control of adaptive locomotion: effect of visual obstruction and
visual cues in the environment. Exp Brain Res 169:272–278
31. Harris JM, Drga VF (2005) Using visual direction in three-dimensional motion perception.
Nat Neurosci 8:229–233
32. Spiers HJ, Burgess N, Hartley T, Vargha-Khadem F, O’Keef J (2001) Bilateral hippocampal
pathology impairs topographical and episodic memory but non visual pattern matching.
Hippocampus 11:715–725
33. Jacobs RA (2002) What determines visual cues reliability? Trends Cogn Sci 6:345–350
34. Breuneval A (2016) Canne blanche: outil pour les déplacements d’un déﬁcient visuel,
Rapport, U. de Rouen Normandie, France (congenitally blind researcher)
35. Cornell EH, Bourassa CM (2007) Human non-visual discrimination of gradual turning is
poor. Psychol Res 71:314–312
36. Pissaloux E, Velázquez R, Hersh M, Uzan G (2016) Towards a cognitive model of human
mobility: an investigation of tactile perception for use in mobility devices. J Navig 6:1–17
37. Maingreaud F, Pissaloux E, Gelin R, Leroux Ch (2005) Towards the understanding of the
obstacle perception by visually handicapped: a visuo-tactile approach. ASME Int J Adv
Model C 65(7/8):1–12
On Spatial Cognition and Mobility Strategies
159

38. Berthoz A, Pavard B, Young L (1975) Perception of linear horizontal self motion induced by
peripheral vision (linear vection). Exp Brain Res 23:471–489
39. Loomis JM, Klatzky RL, Golledge RG, Cicinelli JG, Pellegrino JW, Fry PA (1993)
Nonvisual navigation by blind and sighted: assessment of path integration ability. J Exp
Psychol Gen 122(1):73–91
40. Marlinsky VV (1999) Vestibular and vestibule-perceptive perception of motion in the
horizontal plane in blindfolded man—III. Route inference. Neuroscience 90:403–411
41. Pissaloux E, Chen Y, Velazquez R (2010) Image matching optimisation via vision and
inertial data fusion. Int J Image Graph 10(4):545–555
42. Berthoz A (2000) The brain’s sense of movement. Harward University Press, Cambridge
43. Waller D, Greenauer N (2007) The role of body-based sensory information in the acquisition
of enduring spatial representation. Psychol Res 71:322–332
44. Li L, Sweet BT, Stone LS (2006) Humans can perceive heading without visual path
information. J Vis 6:874–881
45. Rieser JJ, Ashmead DH, Talor CR, Youngquist GA (1990) Visual perception and the
guidance of locomotion without vision to previously seen targets. Perception 19(5):675–689
46. Siegler I, Israel I, Berthoz A (1998) Shift of the beating ﬁeld of vestibular nystagmus: an
orientation strategy? Neurosci Lett 254:93–96
47. Israel I, Capelli A, Priot A-E, Giannopulu I (2013) Spatial linear navigation: is vision
necessary? Neurosci Lett 25(554):34–38
48. Lackner JR (1988) Some proprioceptive inﬂuences on the perceptual representation of body
shape and orientation. Brain 111:281–297
49. Sun HJ, Campos JL, Chan GS (2004) Multisensory integration in the estimation of relative
path length. Exp Brain Res 154:246–254
50. Bakker NH, Werkhoven PJ, Passenier PO (1999) The effects of proprioceptive and visual
feedbacks on geographical orientation in virtual environments. Presence 8:36–53
51. Berthoz A (1997) Le sens du mouvement. Odile Jacob, Paris
52. Gentilucci M, Jeannerod M, Tadary B, Decety J (1994) Dissociating visual and kinesthetic
coordinates during pointing movements. Exp Brain Res 102:359–366
53. Gordon CF, Fletcher WA, Melvill Jones G, Block EW (1995) Adaptive plasticity in the
control of locomotor trajectory. Exp Brain Res 102:540–545
54. Ivanenko YP, Grasso R, Israel I, Berthoz A (1997) The contribution of otoliths and
semicircular canals to the perception of two-dimensional passive whole-body motion in
humans. J Pysiol 502(Pt 1):223–233
55. Ivanenko YP, Grasso R, Israel I, Berthoz A (1997) Spatial orientation in humans: perception
of angular whole-body displacements in two-dimensional trajectories. Exp Brain Res
117:419–427
56. Melvill Jones G, Berthoz A, Segal B (1984) Adaptive modiﬁcation of the vestibule-ocular
reﬂex by mental effort in darkness. Exp Brain Res 56:149–153
57. Berthoz A (ed) (1993) Multisensory control of movement. Oxford University Press, Oxford
58. Collewijn H (1989) The vestibule-ocular reﬂex: an outdated concept? Prog Brain Res 80:209
59. Probst
T,
Straube
A,
Bles
W
(1985)
Differential
effects
of
ambivalent
visual-vestibular-somatosensory stimulation on the perception self-motion. Behav Brain
Res 16:71–79
60. Israel I, Grasso R, Georges-François P, Tsuzuku T, Berthoz A (1997) Spatial memory and
path integration studies by self-driven passive linear displacement. I. Basic properties.
J Neurophysiol 77:3180–3192
61. Melvill Jones G, Galiana HL, Weber KD, Fletcher WA, Block EW (2000) Complex
podokinetic (PK) response to post-rotational vestibular stimulation. Archives Italiennes de
Biologies 138:99–105
62. Berthoz A, Israel I, Georges-François P, Grasso R, Tsuzuku T (1995) Spatial memory of
body linear displacement: what is being stored? Science 269:95–98
160
E. Pissaloux and R. Velázquez

63. Varraine E, Bonnard M, Pailhous J (2002) The top down and bottom up mechanisms
involved in sudden awareness of low level sensorimotor behavior. Brain Res Cogn Brain
Res 13:357–361
64. Luyat M, Mobarek S, Leconte C, Gentaz E (2005) The plasticity of gravitational reference
frame and the subjective vertical: peripheral visual information affects the oblique effect.
Neurosci Lett 385:215–219
65. Brandt T, Bartenstein P, Janek A, Dieterich M (1998) Reciprocol inhibitory visual-vestibular
interaction. Visual motion stimulation deactivates the parieto-insular vestibular cortex. Brain
121:1749–1758
66. Ernst MO, Bülthoff HH (2004) Merging the senses into a robust percept. Trends Cogn Sci
8:162–169
67. Tardif E, Delacuisine B, Probst A, Clarke S (2005) Intrisic connectivity of human superior
colliculus. Exp Brain Res 166:316–324
68. Singer W (2001) Consciousness and the binding problem. Ann NY Acad Sci 929:123–146
69. Zmigrod S, Hommel B (2010) Temporal dynamics of unimodal and multimodal feature
binding. Attention Percept Physchophys 72(1):142–152
70. King A (2005) Multisensor integration: strategies for synchronization. Curr Biol 15(9):
R339–R341
71. Oman CM (1982) A heuristic mathematical model for the dynamics of sensory conﬂict and
motion sickness. Acta Otorhinolaryngol (Suppl 392):1–44
72. Zacharias GL, Young LR (1981) Inﬂuence of combined visual and vestibular cues on human
perception and control of horizontal rotation. Exp Brain Res 41:159–171
73. Borah J, Yung LR, Curry RE (1998) Optimal estimator model for human spatial orientation.
Ann NY Acad Sci 545:51–73
74. Glasauer S (1991) Interaction of semicircular canals and otoliths in the processing structure
of the subjective zenith. Ann NY Acad Sci 656:849
75. Merﬂed DM, Zupan LH (2002) Neural processing of gravitoinertial cues in humans. III.
Modeling tilt and translation responses. J Neurophysiol 87:819–933
76. Gottfried JA, Dolan R (2003) The nose smells what the eye sees: crossmodal visual
facilitation on human olfactory perception. Neuron 39:375–386
77. Droulez J, Darlot C (1989) The geometric and dynamic implications of the coherence
constrains in three dimensional sensorimotor coordinates. In Jeannerod M (ed) Attention and
performance XIII, Laurence Erlbaum Associates, Hillsdale, N.J., pp 495–526
78. Cornilleau-Peres V, Droulez J (1993) Stereo-motion cooperation and the use of motion
disparity in the visual perception of 3-D structure. Percept Psychophys 54:223–239
79. Wang RF, Spelke ES (2002) Human spatial representation: insights from animals. Trends
Cogn Sci 6:376–382
80. Maguire EA, Mummery CJ, Buchel C (2000) Patterns of hippocampal-cortical interaction
dissociate temporal lobe memory subsystems. Hippocampus 10:475–482
81. Siegrest C, Etienne AS, Boulens V, Maurer R, Rowe T (2003) Homing by path integration in
a new environment. Anim Behav 65:185–194
82. McNaughton BL, Battaglia FP, Jensen O, Moser EI, Moser M-B (2006) Path integration and
the neural basis of the ‘cognitive map’. Nat Rev Neurosci 7:663–678
83. Mittelstaedt M, Mittelstaedt H (2001) Idiothetic navigation in humans: estimation of path
length. Exp Brain Res 139:318–332
84. Foo P, Duchon A, Warren WH, Tarr MJ (2007) Humans do not switch between path
knowledge and landmarks when learning a new environment. Psychol Res 71:240–251
85. Fukita N, Klatzky RL, Loomis JM, Golledge RG (1993) The encoding-error model of
pathway completion without vision. Geogr Anal 22:326–225
86. Foo P, Duchon A, Warren WH, Tarr MJ (2005) Do humans integrate routes into a cognitive
map? Map versus landmark-based navigation of novel shortcuts. J Exp Psychol Learn Mem
Cogn 31:195–215
On Spatial Cognition and Mobility Strategies
161

87. Rieke BE, van Veen HA, Bulthoff HH (2002) Visual homing is possible without landmarks:
a path integration study in virtual reality. Presence-Teleoperators Virtual Environ 11:
443–473
88. Zhao M, Warren WH (2010) Are path integration and visual landmarks optimally combined
in spatial navigation? Spatial Cognition, Mt. Hood, Oregon
89. Daniel M-P, Denis M (1998) Spatial descriptions as navigation aids: a cognitive analysis of
route directions. Kognitionwissenschaft 7:45–52
90. Uzan G et al (2008) Besoins en sécurité, localisation et orientation des déﬁcients visuels en
milieu urbain: analyse de la situation et pistes d’évolution. Proc Handicap, Paris, pp 37–42
(late blind researcher)
91. Magnusson CH, Hedvall P-O, Caltenco H (2018) Co-designing together with persons with
visual impairments. In: Pissaloux E, Velázquez R (eds) Mobility of visually impaired
people—fundamentals and ICT assistive technologies, Springer
92. Pissaloux E (2013) Visually impaired mobility and ICT supports. IEEE signal processing:
algorithms, architectures, arrangements, and applications (SPA), ISSN: 2326-0262
93. Janzen G, van Turennout M (2004) Selective neural representation of objects relevant for
navigation. Nat Sci 7:673–677
94. Steck SD, Mallot HA (2000) The role of global and local landmarks in virtual environment
navigation. Presence 9:69–83
95. Ruddle R, Peruch P (2004) Effects of proprioceptive feedback and environmental
characteristics on spatial learning in virtual environments. Int J Hum Comput Stud
60:299–326
96. Learmonth AE, Newcombe NS, Huttenlocher J (2001) Toddler’s use of metric information
and landmarks to reorient. J Exp Child Psychol 80:225–244
97. Allen GL, Kirasic KC, Siegel AW, Herman JF (1979) Developmental issues in cognitive
mapping: the selection and utilization of environmental landmarks. Child Dev 50:
1062–1070
98. Riecke BE, Cunningham D, Bulthoff HH (2006) Satial updating in virtual reality: the
sufﬁciency of visual information. Psychol Res 71:298–313
99. Rosenbaum RS, Ziegler M, Wincus G, Grady CL, Moscovitch M (2004) “I have often
walked down this street before”: fMRI studies on the hippocampus and other structures
during mental navigation of an old environment. Hippocampus 14:826–835
100. Ekstrom AD, Kahana MJ, Caplan JB, Fields TA, Isham EA, Newman EL (2003) Cellular
networks underlying human navigation. Nature 392:598–601
101. Esptein R, Kanwisher N (1998) A cortical representation of the local visual environment.
Nature 392:598–601
102. Maguire EA, Gadian DG, Johnsrude IS, Good CD, Ashburner J, Frackowiak RS, Frith CD
(2000)Navigation-related structural change in the hippocampi of taxi drivers. PNAS 97
(8):4398–4403
103. Cheng K (1986) A purely geometric module in rat’s spatial representation. Cognition
23:148–178
104. Hermer L, Spelke ES (1994) A geometric process of spatial reorientation in young children.
Nature 370:19–20
105. Lee SA, Sovrano VA, Spelke ES (2012) Navigation as a source of geometric knowledge:
young children’s use of length, angle, distance, and direction in a reorientation task.
Cognition 123(1):144–161
106. Huttenlocher J, Vasilyeva M (2003) How toddlers represent enclosed spaces. Cogn Sci
27:749–766
107. Lourenco SF, Huttenlocher J (2007) Using geometry to specify location: implication for
spatial coding in children and nonhumans animals. Psychol Res 71:252–264
108. Buckley MG, Smith AD, Haselgrove M (2016) Thinking outside of the box: transfer of
shape-based reorientation across the boundary of an arena. Cogn Psychol 87:53–87
109. Valiquette CM, MCNamara TP, Labrecque JS (2007) Biased representations of the spatial
structure of navigable environments. Psychol Res 71:288–297
162
E. Pissaloux and R. Velázquez

110. Ghaem O, Mellet E, Crivello F, Tzourio N, Mazoyer B, Berthoz A (1997) Mental navigation
along memorized routes activates the hippocampus, precuneus and insula. NeuroReport
8:739–744
111. Mellet E, Briscogne S, Tzourio-Mazoyer N, Ghaem O, Petit L, Zago L, Etard O, Berthoz A,
Mazoyer B, Denis M (2000) Neural correlates of topographic mental exploration: the impact
of route versus survey perspective learning. NeuroImage 12:588–600
112. Burgess N (2008) Spatial cognition and the brain. Ann NY Acad Sci 1124:77–97
113. Panagiotaki P, Lambrey S, Berthoz A (2006) Neural bases and cognitive mechanism of
human spatial memory. In: Vecchi T, Bottini G (eds) Imagery & spatial cognition: methods,
models and cognitive assessment, John Benjamin, Amsterdam
114. Denis M, Loomis JM (2007) Perspectives on human spatial cognition: memory, navigation,
and environmental learning. Psychol Res 71:235–239
115. Scoville WB, Milner B (1957) Loss of recent memory after bilateral hippocampal lesions.
J Neurol Neurosurg Psychiatry 20(1):11–21
116. Hartley T, Maguire EA, Spiers HJ, Burgess N (2003) The well-worn route and the path less
traveled: distinct neural bases of route following and wayﬁnding in humans. Neuron 37:
877–888
117. Maguire EA, Burgess N, Donnet JG, Frackowiak RS, Frith CD, O’Keefe J (1998) Knowing
where and getting there: a human navigation network. Science 280:921–924
118. Barrash J, Damasio H, Adolphs R, Tranel D (2000) The neuroanatomical correlates of route
learning impairment. Neuropsychologia 38:820–836
119. Gron G, Wunderlich AP, Spitzer M, Tomczak R, Riepe MW (2000) Brain activation during
human navigation: gender-different neural networks as substrate of performance. Nat
Neurosci 3:404–408
120. Pine DS, Grun J, Maguire EA, Burgess N, Zrahn E, Koda V, Fryer A, Szeszko PR,
Bilder RM (2002) Neurodevelopmental aspects of spatial navigation: a virtual reality fMRI
study. NeuroImage 15:396–406
121. Aguirre GK, Detre JA, D’Esposito M (1996) The parahippocampus subserves topographical
learning in man. Cereb Cortex 6:823–829
122. Spiers HJ, Maguire EA (2004) A ‘landmark’ study on the neural basis of navigation. Nat
Neurosci 7:572–574
123. Rosenbaum RS, Gao F, Richards B, Black SE, Moscovitch M (2005) “Where to?” remote
memory for spatila relations and landmark identity in former taxi drivers with Alzheimer’s
disease and encephalitis. J Cogn Neurosci 17:446–462
124. Parslow DM, Morris RG, Fleminger S, Rahman Q, Abrahams S, Recce M (2005)
Allocentric
spatial
memory
in
humans
with
hippocampal
lesions.
Acta
Physiol
(Oxf) 118:123–147
125. Maguire EA, Frackowiak RS, Frith CD (1997) Recalling routes around London: activation
of the right hippocampus in taxi drivers. J Neurosci 17:7103–7110
126. Maguire EA, Frith CD, Cipolotti L (2001) Distinct neural systems for the encoding and
recognition of topography and faces. Neuroimage 13:743–770
127. Burgess N, Becker S, King JA, O’Keefen J (2001) Memory for events and their spatial
context: models and experiments. Philos Trans Roy Soc B Lond Biol Sci 29:1493–1503
128. Nyffeler T, Gutbrod K, Pﬂugshaupt T, von Wartburg R, Hess CW, Muri RM (2005)
Allocentric and egocentric spatial impairments in a case of topographical disorientation.
Cortex 41:133–143
129. Redish AD, Touretzky DS (1997) Cognitive map beyond the hippocampus. Hippocampus
7:15–35
130. Committeri G, Galati G, Paradis A-L, Pizzamiglio L, Berthoz A, LeBihan D (2004)
Reference frames for spatial cognition: different brain areas are involved in viewer-, object-
and landmark-centered judgments about object location. J Cogn Neurosci 16:1517–1535
131. Lambrey S, Samson S, Dupont S, Baulac M, Berthoz A (2003) Reference frames and
cognitive strategies during navigation: is the left hippocampal formation involved in the
sequential aspects of route memory? Int Congr Ser 1250:261–274
On Spatial Cognition and Mobility Strategies
163

132. Heilman KM, Watson RT, Valenstein E (2002) Spatial neglect. In: Karnath H-O, Milner D,
Vallar G (eds) The cognitive and neural bases of spatial neglect, Oxford University Press,
UK, pp 3–30
133. Wolbers T, Weiller C, Buchel C (2004) Neural foundations of emerging route knowledge in
complex spatial environments. Cogn Brain Res 21:401–411
134. Iaria G, Petrides M, Dagher A, Pike B, Bohbot VD (2003) Cognitive strategies dependent of
the hippocampus and caudate nucleus in human navigation: variability and change with
practice. J Neurosci 23:5945–5952
135. Leung HC, Gore JC, Goldman-Rakic PS (2005) Differential anterior prefrontal activation
during the recognition stage of a spatial working memory task. Cereb Cortex 15:1742–1749
136. Makino Y, Okosawa K, Takeda Y, Kumada T (2004) Visual search and memory search
engage extensive overlapping cerebral cortices: an fMRI study. NeuroImage 23:525–533
137. Kesner RP, Rogers J (2004) An analysis of independence and interactions of brain substrates
that subserve multiple attributes, memory systems and underlying processes. Neurobiol
Learn Mem 82:199–215
138. Iaria G, Chen JK, Guariglia C, Ptito A, Petrides M (2007) Retrospelnial and hippocampal
brain regions in human navigation: complementary functional contributions to the formation
and use of cognitive maps. Eur J Neurosci 25:890–899
139. Ekstrom AD, Arnold AEGF, Iaria G (2014) A critical review of the allocentric spatial
representation and its neural underpinnings: toward a network-based perspective. Front Hum
Neurosci 8:803
140. Wehner R, Boyer M, Loertscher F, Sommer S, Menzi U (2006) Ant navigation: one-way
routes rather than map. Curr Biol 16:75–79
141. Etienne AS, Maurer R, Berli J, Reverdin B, Rowe T, Georgakopoulos J, Séguinot V (1998)
Navigation through vector addition. Nature 396:161–164
142. Collet TS, Graham P (2004) Animal navigation: path integration, visual landmarks and
cognitive maps. Curr Biol 14:R474–R477
143. Macquart D, Garnier L, Combe M, Beugnon G (2006) Ant navigation en route to the goal:
signature routes facilitate way-ﬁnding of gigantiops destructor. J Comp Physiol A
Neuroethology Sens Neural Behav Physiol 192:221–234
144. Wystrach A et al (2011) Landmarks or panorama: what do navigation ants attend to for
guidance? Front Zool 8(21)
145. Wystrach A et al (2012) What can we learn from studies of insect navigation? Anim Behav
84(1):13–20
146. Chittka L, Williams N, Rasmussen H, Thomson J (1999) Navigation without vision:
bumblebee orientation in complete darkness. Proc Roy Acad Lond 266:45–50
147. Srinivassan M (2015) Where paths meet and cross: navigation by path integration in the
desert ant and the honeybee. J Comp Physiol A 201(6):533–546
148. Touretzky DS, Redish AD (1996) Theory of rodent navigation based on interacting
representations of space. Hippocampus 6:247–270
149. Etienne AS, Boulens V, Maurer R, Rowe T, Siegrist C (2000) A brief view of known
landmarks reorientates path integration in hamsters. Naturwissenschaften 87:494–498
150. Etienne AS, Maurer R, Boulens V, Levy A, Rowe T (2004) Resetting the path integrator: a
condition for route-based navigation. J Exp Biol 207:1491–1508
151. Collett TS, Collett M (2006) Memory use in insect visual navigation. Nat Rev Neurosci
3:542–552
152. Cheng K, Newcombe NS (2005) Is there a geometric module for spatial orientation?
Squaring theory and evidence. Psychon Bull Rev 12:1–23
153. Margules J, Gallistel CR (1988) Heading in the rat: determination by environmental shape.
Anim Learn Behav 16:404–410
154. Goûteux S, Thinus-Blanc C, Vauclair J (2001) Thesus monkeys use geometric and
nongeometric information during a reorientation task. J Exp Psychol Gen 130:505–529
155. Tommasi L, Thinus-Blanc C (2004) Generalisation in place learning and geometry
knowledge in rats. Learn Mem 11:153–161
164
E. Pissaloux and R. Velázquez

156. Mou W, Nankoo JF, Zhou R, Spetch ML (2014) Use of geometric properties of landmark
arrays for reorientation relative to remote cities and local objects. J Exp Psychol Learn Mem
Cogn 40(2):476–491
157. Troffa R (2010) Visibility and wayﬁnding: a VR study on emergency strategies. Report
series of the Transregional Collaborative Research Center SFB/TR 8 Spatial Cognition
Universität Bremen, Universität Freiburg
158. Ben-Yehoshua D, Yaski O, Eilam D (2011) Spatial behavior: the impact of global and local
geometry. Anim Cogn 14(3):341–350
159. Cheng K (2005) Reﬂexions on geometry and navigation. Connection Sci 17:5–21
160. Lew AR, Usherwood B, Fragkioudaki F, Koukoumi V, Smith SP, Austen JM, McGregor A
(2014) Transfer of spatial search between environments in human adults and young children
(Homo sapiens) implications for representation of local geometry by spatial systems. Dev
Psychobiol 56(3):421–434
161. Fry SN, Wehner R (2005) Look and turn: landmark-based goal navigation in honey bees.
J Exp Biol 208:3945–3955
162. Dittmar L, Egelhaaf M, Stürzl W, Boeddeker N (2011) The behavioral relevance of
landmark texture for honeybee homing. Front Behav Neurosci 5:20
163. Bishc-Knaden S, Wehner R (2003) Local vectors in desert ants: context-dependent landmark
learning during outbound and homebound runs. J Comp Physiol A Neuroethology Sens
Neural Behav Physiol 189:181–187
164. Grandchamp N, Schenk F (2006) Adaptive changes in a radial maze task: efﬁcient selection
of baited arms with reduces foraging in senescent hooded rats. Behav Brain Res 168:
161–166
165. Gray ER, Bloomﬁeld LL, Ferrey A, Spetch ML, Sturdy CB (2005) Spatial encoding in
mountain chickadees: feature overshadow geometry. Biol Lett 1:314–317
166. Pearce JM, Ward-Robinson J, Good M, Fussell C, Aydin A (2001) Inﬂuence of a beacon on
spatial learning based on the shape of the test environment. J Exp Psychol Anim Behav
Processes 27:329–344
167. Hayward A, McGregor A, Good MA, Pearce JM (2003) Absence of overshadowing and
blocking between landmarks and the geometric cues provided by the shape of a test arena.
Q J Exp Psychol B 56:114–126
168. Wall PL, Botly LC, Black CK, Shettleworth SJ (2004) The geometric module in the rat:
independence of shape and feature learning in a food ﬁnding task. Learn Behav 32:289–298
169. McGregor A, Good MA, Pearce JM (2004) Absence of an interaction between navigational
strategies based on local and distal landmarks. J Exp Psychol Anim Behav Process 30:34–44
170. Tolman EC (1948) Cognitive maps in rats and men. Psychol Rev 55(4):189–208
171. Golledge RG, Stimpson RJ (1997) Spatial behavior: a geographic perspective. The Guilford
Press, New York
172. Roche RA, Mangaoang MA, Commis S, O’Mara SM (2005) Hippocampal contributions to
neurocognitive mapping in humans: a new model. Hippocampus 15:622–641
173. Andersen R, Snyder H, Bradley D, Xing J (1997) Multimodal representation of space in the
posterior parietal cortex and its use in planning movements. Annu Rev Neurosci 20:303–330
174. Hubbard EM, Ramachandran VS (2005) Neurocognitive mechanisms of synesthesia.
Neuron 48:509–520
175. Denis M, Borst G (2006) Variations on the image scanning paradigm: what do they
contribute to our knowledge of mental imagery? In: Vecchi T, Bottini G (eds) Imagery &
spatial cognition: methods, models and cognitive assessment, John Benjamin, Amsterdam
176. Golledge
RG
(1999)
Human
wayﬁnding
and
cognitive
maps.
In:
Golledge
RG
(ed) Wayﬁnding behavior, Johns Hopkins University Press, Baltimore, pp 5–45
177. Koenderink JJ (2002) Pappus in optical space. Percept Psychophys 64(3):380–391
178. Couclelis H, Golledge RG, Gale N, Tobler W (1987) Exploring the anchor-point hypothesis
of spatial cognition. J Environ Psychol 5:99–122
179. O’Keefe J, Dostrovsky J (1971) The hippocampus as a spatial map. Preliminary evidence
from unit activity in the freely-moving rat. Brain Res 34:171–175
On Spatial Cognition and Mobility Strategies
165

180. O’Keefe J, Burgess N (1996) Geometric determinants of place ﬁelds of hippocampal
neurons. Nature 381:425–428
181. Save E, Cressant A, Thinus-Blanc C, Poucet B (1998) Saptial ﬁring of hippocampal place
cells in blind rats. J Neurosci 18(5):1818–1826
182. O’Keefe J, Nadel L (1978) The hippocampus as a cognitive map. Oxford University Press,
UK
183. Etienne AS, Joris-Lambert S, Dahn-Hurni C, Reverdin B (1995) Optimizing visual
landmarks: two and three-dimensional minimal landscapes. Anim Behav 49:165–179
184. Stackman RW, Taube JS (2002) Firing properties of head direction cells in the rat anterior
thalamic nucleus: dependence on vestibular input. J Neurosci 17:4349–4358
185. Kessels R, de Haan E, Kappelle L, Postma A (2001) Varieties of human spatial memory: a
meta-analysis on the effects of hippocampal lesions. Brain Res Rev 35:295–303
186. Golledge RG, Smith TR, Pellegrino JW, Doherty SE, Marshall SP (1985) A conceptual
model and empirical analysis of children’s acquisition of spatial knowledge. J Environ
Psychol 5:125–152
187. Shemyakin FN (1962) General problems of orientation is space and space representations.
In: Anan’yev et al (eds) Psychological science in the USSR, Ofﬁce of Technical Services,
Washington, pp 184–225
188. Hart RA, Moore GT (1973) The development of spatial cognition. In: Downs RM, Stea D
(eds) Image and environment, Adline Publishing, Chicago, pp 246–288
189. Siegel AW, White SH (1975) The development of spatial representations of large-scale
environments. Adv Child Dev Behav 10:9–55
190. Kuipers BJ, Levitt TS (1988) Navigation and mapping in large-scale space. AI Mag 9:25–46
191. Montello DR (1998) A new framework for understanding the acquisition of spatial
knowledge in large-scale environments. In: Egenhofer MJ, Glledge RG (eds) Spatial and
temporal reasoning in geographic information systems, Oxford University Press, UK,
pp 143–154
192. Jacobs LF, Schenk F (2003) Unpacking in cognitive map: the parallel map theory of
hippocampal function. Psychol Rev 110:285–315
193. Mou W, McNamara TP, Valiquette CM, Rump B (2004) Allocentric and egocentric
updating of spatial memory. J Exp Psychol Learn Mem Cogn 30:142–157
194. Mou W, Fan Y, McNamara TP, Owen CB (2008) Intrisinc frames of reference and
egocentric viewpoints in scene recognition. Cognition 106:750–769
195. Finaly CA, Motes MA, Kozhevnikov M (2007) Updating representations of learned scenes.
Psychol Res 71:265–276
166
E. Pissaloux and R. Velázquez

Sensory Substitution and the Neural
Correlates of Navigation in Blindness
Daniel-Robert Chebat, Vanessa Harrar, Ron Kupers,
Shachar Maidenbaum, Amir Amedi and Maurice Ptito
1
Introduction
Sensory substitution devices (SSDs) are capable of efﬁciently transmitting visual
information in real time via touch or sound. This chapter includes several sections
reviewing the different elements associated with the use of SSDs by blind or
visually impaired individuals. We also develop the idea that an active reinterpre-
tation of sensory information through mechanisms of training-induced plastic
changes enables CB to use SSDs, as if they had access to speciﬁc properties of real
vision. These properties can include, for example, the shape of an object perceived
at a distance, the distance of an object to the observer, or the color of the object. We
also review the phenomenological properties of sensory substitution concerning the
user’s perceptual sensation when experiencing nonvisual stimulation. Despite wide
use of SSDs for research over many decades, their use is not widespread in the blind
population. We suggest guidelines for creating better devices. In addition, we
suggest that the activations of task-speciﬁc specialized brain areas demonstrate the
D.-R. Chebat (&)
Visual and Cognitive Neuroscience Laboratory (VCN Laboratory), Department of Behavioral
Sciences and Psychology, Ariel University, Ariel, Israel
e-mail: danielc@ariel.ac.il
V. Harrar  M. Ptito
Chaire de recherche Harland Sanders en Sciences de la Vision, École d’Optométrie,
Université de Montréal, Montréal, Canada
M. Ptito
Laboratory of Neuropsychiatry, Psychiatric Centre Copenhagen, University of Copenhagen,
Copenhagen, Denmark
R. Kupers  M. Ptito
Brain Research and Integrative Neuroscience Laboratory, Danish Center for Sleep Medicine,
Department of Clinical Neurophysiology, Rigshospitalet, Glostrup, Denmark
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_6
167

amodality and task speciﬁcity of SSD processing. Finally, we examine the literature
on the neural correlates of navigation in sighted and blind people. The abilities of
blind individuals trained with SSDs in spatial navigation, combined with anatom-
ical and functional changes, illustrate the ability of the brain to rewire itself during
perceptual learning and provide novel interpretation of nonvisual sensory
information.
2
Sensory Substitution Devices (SSDs)
Sensory substitution is rooted in the idea that it is possible to replace an impaired or
lost sense by the novel interpretation of another sense. Paul Bach-y-Rita, the father
of sensory substitution as a formulated ﬁeld, aimed his work at restoring visual
functions in blind people [8]. Today, most SSDs attempt to restore visual functions,
but not exclusively [16, 181, 219]. In this chapter, we concentrate on SSDs that
translate visual information into tactile or auditory information. SSDs usually
consist of these three components: a sensor, a processing unit that converts the
sensory information using a code or algorithm, and a way to transmit this infor-
mation to the user.
Visual SSDs differ from one another in terms of their respective approaches for
capturing, transforming, and sending information. For example, SSDs differ in
terms of the sensors used to capture the visual information. Some use image-based
transformations from a camera (Fig. 1a, c) [8, 139, 145], sonic [103] ultrasonic [13,
90], or infrared sensors (Fig. 1D) [51, 131, 194], while others use magnetic sensors
instead [101].
SSDs also differ from one another in terms of the algorithm used to translate the
visual information into another modality. In the case of the tongue display unit
(TDU) (Fig. 1a), the image from a camera is translated into electrotactile pulses on
a grid of electrodes applied to the tongue (Fig. 1b) (see [171]). In the case of the
vOICe (Fig. 1c), visual information is transduced into an auditory soundscape (see
[145]). In the case of the EyeCane, the handheld device sends infrared distance
S. Maidenbaum  A. Amedi
Department of Medical Neurobiology, the Institute for Medical Research Israel Canada,
Faculty of Medicine, The Hebrew University of Jerusalem, 91220 Jerusalem, Israel
S. Maidenbaum  A. Amedi
The Edmond and Lily Safra Centre for Brain Sciences (ELSC),
The Hebrew University of Jerusalem, 91220 Jerusalem, Israel
A. Amedi
Cognitive Sciences Program, The Hebrew University of Jerusalem,
91220 Jerusalem, Israel
168
D.-R. Chebat et al.

Fig. 1 Sensory substitution
devices. Examples of the
experimental setup for several
different sensory substitution
devices. a The BrainPort
(WICAB) with a camera
mounted on a pair of
darkened glasses. The box on
the chest controls the intensity
of the electrotactile
stimulation. b The tongue
grid. Applied to the tongue, it
delivers a tingling sensation
through the electrodes. c The
EyeMusic experimental setup
with headphones and camera.
d A participant holding the
EyeCane that delivers
vibrations and sounds to
indicate the distance to an
object. e The sensors of the
EyeCane and device
Sensory Substitution and the Neural Correlates …
169

information in the form of vibrations and sounds (Fig. 1d, e). Despite their dif-
ferences, for all these SSDs the sensations must be actively reinterpreted, from a
meaningless tactile or auditory sensation, into a “visual” percept. The resulting
perceptual experience after training is in many ways comparable to real vision
[8, 111, 171, 220, 221].
Another factor that differentiates between SSDs is the modality used to transfer
visual information, usually either tactile or auditory. Thus, the process of learning to
use SSDs usually involves the active reinterpretation of tactile [8, 131, 166] or
auditory information [103, 145] to signify visual information through sensorimotor
feedback (Fig. 2).
3
Active Perceptual Learning of SSDs
The process of attributing visual properties to tactile or auditory stimulation can be
described as an active sensorimotor feedback loop (Fig. 2), respecting a strict set of
contingencies (Sensori-Motor Contingencies: SMCs) [102, 158, 160]. This idea
suggests that when using SSDs (or sensory augmentation devices), motor actions
linked to sensory stimulations are regulated by SMCs. SMCs are a set of rules or
regularities that help relate movement of the user in relation to the object. SMCs
must be learned, in effect the user is learning the algorithm that transforms the
image into tactile or auditory stimulation.
Fig. 2 Sensory substitution motor loop. This loop enables the embodiment of perceptions. 1 The
sensor is pointed in a given direction. 2 A computer or microchip analyzes the image and
transforms it into tactile or auditory stimulation. 3 The user receives this tactile or auditory
stimulation and a percept is generated. The user tests his percept and receives feedback (e.g., by
touching the object that was in the camera’s path). 4 Learning. The user adjusts his understanding
of the code in order to match perceptions with the sensory feedback he receives. 1 The user
reinitiates the loop again by moving the device
170
D.-R. Chebat et al.

The SSD learning process can be broken down into several steps (Fig. 2). First, a
sensor captures the visual information that is processed via a computer, or smart-
phone microchip using a code (a specialized algorithm) substituting the visual
information into an alternative tactile [8, 224], auditory [145], or both modalities
simultaneously [12, 131, 147]. The user gets a perceptual stimulation (auditory or
tactile) that represents the image that is captured by the SSD. The user then
accomplishes exploratory movements that modify the sensory information. This
relation between movement and image helps the user learn the SMC code that
governs the image transformation into touch or sound. In order to learn this code,
the sensorimotor loop must be rehearsed many times. The user then attempts to
interpret the stimulation and casts a hypothesis about his perception. For example, if
the sensory stimulation represents a vertical line, moving the camera to the right
should result in shifting the stimulus to the right as well. The user then makes the
required motor adjustments to test the hypothesis and the loop starts anew.
Completion of the sensorimotor loop and exploration through trial and error with
SSDs help create an illusion similar to vision. After a training period that can
sometimes be very intensive (anywhere from just a few minutes to several weeks of
training) and cognitively taxing, touch or sound acquire perceptual properties
similar to “vision.”
Once the code has been learned, it requires less attention and the user begins to
automatically perceive the stimulation as external. This externalization of the
stimulation has been described previously as distal attribution (for a recent review,
[83]). This distal attribution has been linked to a number of tasks usually considered
visual. Nonvisual stimuli that contain information on shape [5, 6, 171, 198] and
motion [170, 198] are interpreted as “visual” by CB. The same is true for obstacles
[32] and spatial relations [30, 120]. Bach-y-Rita [10] argued that it is correct to use
the term “vision” to describe the sensations experienced by CB and late blind
(LB) participants via SSDs because of the “visual” phenomenological percept that
they report. The creation of a novel “visual” percept through an already existing
modality provides an opportunity to study the impact of SSD training on cortical
processing [184].
4
Phenomenological Properties of Sensory Substitution
Participants often report that the sensation provided by the SSD takes on different
properties after training. What are then the phenomenological properties of sensory
substitution?
Sensory Substitution and the Neural Correlates …
171

4.1
Self-reports
I took part in a scientiﬁc study which enables blind people to be able to visualize
objects by stimulation of their tongue. After being introduced to the program, I was
seated facing a wall 3 feet in front of me. I was allowed to feel the shape of what
turned out to be a very large letter E. After a few minutes of adjusting my head up
and down, and from left to right, in carefully contrived movements, I was able to
visualize the letter E. This was exhilarating and I could hear my heart beating quite
loudly. This was the ﬁrst time I could visit the world I thought had disappeared
when I lost my sight…
A.D., late-blind participant
Anecdotal self-reports from blind people using SSDs indicate that they perceive
the information as similar to what they imagine vision to be like, or in the case of
LB individuals, to what they remember from their past visual experience. For
example, blind participants report many emotions and perceptions, such as the
world opening up around them [136], or being able to grasp objects with better
precision [120]. They often use terms such as “visualize,” or report seeing objects
or movement during experiments. It is even possible to measure visual-acuity via
SSDs [31, 196, 198, 199] and the substituted information allows them to sense
stimuli and do speciﬁc tasks and sensing stimuli that are normally only available
visually. These include for example: walking and avoiding obstacles [32, 131],
ﬁnger maze learning [66], recognizing routes [110], navigating a maze [30],
identifying the direction of motion [142, 198], recognizing body postures [195],
reading words [196] and numbers [1].
Participants often respond to sensory substitution very enthusiastically and
emotionally [8, 9, 102] (but see also Sect. 5.3 Why is SSD use not widespread).
For example, when a ﬂame was presented for the ﬁrst time via the TDU to one of
our CB participants, the tingling sensation on the tongue provided by the elec-
trodes lead him to exclaim “oh! That is what ﬁre looks like!.”1 Participants
become so emotionally attached to their new sense (visuotactile in this case) that
removing access to it results in a feeling of loss. The resulting embodiment of
SSD stimulation is perceived as an extension of the senses, not an outside
apparatus [215].
Using an auditory SSD, several LB participants liken their sensations, after using
this device for several years, to that of vision. Several authors refer to this
experience as an “acquired synesthesia” [220]. Synesthesia is the phenomenological
merging of one or more attributes from one sensory modality to another; for
example, many synesthetes report sensing speciﬁc colors linked to speciﬁc musical
1The reader is referred to a ﬁlm produced by Discovery channel on the various abilities developed
by CB and LB participants using the TDU. « The Plastic Fantastic Brain » https://www.youtube.
com/watch?v=IzmZArOryGk&t=5s.
172
D.-R. Chebat et al.

tones [63, 96, 140, 209, 229], or colors being associated with the imagined per-
ception of a geometric form [33]. The experience of SSD stimulation has been
linked to synesthesia because of the phenomenological attribution of visual prop-
erties to tactile or auditory stimulation [168, 221]. There are certain differences,
however, between synesthesia and SSD perception. Synesthesia is an innate auto-
matic process where the individual cannot help but perceive a merging of the
senses, whereas the SSD perceptual process is an active process at ﬁrst. The user
must ﬁrst learn to interpret the tactile or auditory sensory signal from the SSD as a
new sense, and after training this process becomes automatic.
One could argue that learning to see is not an automatic process either; it is
active. Just as one must learn to use an SSD, we do not automatically know
how to interpret photons of light hitting the retina, vision is also an active
learning process achieved through trial and error. Thus, it is possible that
synesthesia is also an acquired process resulting from visual training.
We argue that the information provided by SSDs is an amodal task-dependent
perception. The information extracted from SSDs through its active use is
abstracted from the modal source (auditory or tactile) to form an amodal
representation.
5
Sensory Substitution and Navigational Abilities
of the Blind
Recently, there has been an increase in the advent of technological aids (for review
see [89]) and SSDs (for review see: [133]) to help blind or visually impaired people
navigate. Research in laboratory settings has demonstrated the potential level of
spatial abilities that blind people can display when using SSDs [30, 51, 82, 110,
187, 190, 194]. For example, early research with the tactile visual sensory substi-
tution (TVSS) demonstrated the “visuo”-motor abilities of the blind in tasks like
batting a ball, identifying the source of light and recognizing shapes [9]. Blind
people can have rapid and precise reaching movements using modern SSD’s [120],
and recognize various geometric forms [173, 171], complex images [6, 143, 111],
and even the direction of moving stimuli [142, 170, 198]. Blind people can even
complete complex tasks such as recognizing body postures [4] or facial expressions
[18, 109].
It is noteworthy that vision conveys a vast amount of information instantly
whereas SSDs cannot. Currently, the lack of temporal and spatial resolution of
SSDs does not allow for the transfer of the complexity and richness of visual
information with the same speed and accuracy [40, 191]. Instead of trying to
Sensory Substitution and the Neural Correlates …
173

convey the entire complexity of the visual information, some research with SSDs
has focused instead on understanding and improving the way speciﬁc aspects of
vision can help with navigation [30, 32, 110, 187, 190]. These speciﬁc aspects can
be divided into those improving locomotion and those assisting in wayﬁnding.
Navigation is composed of both wayﬁnding and locomotion tasks [150].
Locomotion involves the ability to negotiate a path around obstacles, while
wayﬁnding involves the more complex navigation of large environments [125].
Whiles both are made easier by visual input [141, 163, 162], locomotion and
wayﬁnding involve different components of decision-making, and demand different
skills [150]. Furthermore, they require different aspects of visual information. For
example in locomotion tasks, vision is used to update distance information with the
obstacle [163, 162]. In wayﬁnding tasks, vision helps by spotting landmarks that
help the navigator situate its relative position in an environment [53, 141]. In order
to be useful navigational aids, SSDs should be geared to answer the speciﬁc
demands of both locomotion and wayﬁnding, to convey the speciﬁc information
needed for both tasks.
5.1
Sensory Substitution Devices (SSD) and Locomotion
In locomotion tasks, the important aspects of the visual information are those that
enable us to detect the presence, size, position, and distance to an obstacle that is in
the path, as well as the basic geometry of our environment. Several SSDs are
capable of sending this type of information. With a visuotactile SSD (the TDU), CB
subjects outperform sighted blindfolded participants in a high contrast, life size
obstacle course [32]. All participants had more difﬁculty stepping over an obstacle,
than going around it. Using a depth to audio SSD (the EyeCane), Buchs et al. [17]
tested obstacle negotiation and reported in their subjects the same difﬁculty in
stepping over obstacles. Blindfolded participants using the EyeCane virtually per-
formed better than another blindfolded group using a white cane, demonstrating
that SSDs can be more efﬁcient and can convey a wider variety and more complex
information than a white cane alone. These studies show it is possible for blind
people to learn how to use SSDs for locomotion. Now that this demonstration has
been made, an effort should be made to demonstrate their usability in real world
settings. Furthermore, the appropriate comparison for a device that attempts to
mimic vision resides in the use of a visual control group, which is lacking in the
studies mentioned. Another interesting comparison that should be made systemat-
ically when testing SSDs is with vs without the device. Many people who are blind
have developed effective ways (using a guide dog or a white cane) to navigate. New
devices should be systematically tested against the preferred means of locomotion
of participants. Future research with SSDs for locomotion needs to be weary of
ecological validity, perhaps trying them in a simulated street environment, and
compare results with the appropriate control groups, using vision.
174
D.-R. Chebat et al.

5.2
Sensory Substitution for Wayﬁnding
Lost in a foreign city, the ability to see from afar a building passed along a route can
greatly enhance your chances of being able to retrace your steps. In this example, the
building can constitute a landmark, an easily remembered and sometimes visible cue
to help you ﬁnd your way. In wayﬁnding tasks, the most important aspects of vision
are those that help convey one’s position in the environment. Landmarks are an
important source of positioning information [79]. Images of landmarks at decision
points along a route are better remembered, and elicit a greater BOLD signal in the
retrosplenial cortex (RSC) than those not at decision points along the route [185].
SSDs can transfer landmark or positioning information: for example, the FeelSpace
or NavBelt) indicates the magnetic North from a compass via vibrotactile stimulations
on a belt that are updated with the wearer’s movements [101]. Researchers found that
after several weeks of training, late blind participants improved on a pointing task,
maintaining a sense of direction over long distances, and ﬁnding shortcuts in familiar
environments [101]. After participants have learned the SSD code, the acquired new
perception is “embodied,” which not only helps them select more efﬁcient routes (i.e.,
shortcuts) during navigation, but also creates the feeling of a “new” sense [102].
Blindfolded participants in a maze can even remotely navigate a robot using
tactile cues as landmarks [187]. In a virtual reality task, CB participants not only
learned to navigate different routes but also recognize them [110]. In another study
using virtual mazes, participants using the EyeCane outperformed both the
no-device and white cane groups [137]. Testing the EyeCane in a real-life size
Hebb–Williams maze demonstrated that CB, LB and sighted blindfolded controls
(SBC) could learn to navigate with a similar performance to the sighted visual
control (SVC) group [30]. Thus, many different types of SSDs can provide infor-
mation that can help recognize landmarks and position the user in the environment.
Why then, despite many years of research and such positive results, has SSD use
not become widespread in people who are blind and visually impaired?
5.3
Why Is SSD Use not Widespread?
Participants were asked if they would ever consider using SSDs regularly.
Here are some examples of their answers:
Well, you know, I think it might be useful but I already have my dog for that! I don’t
think anything could ever replace my dog!
Participant OB, M, 47-years-old congenitally blind trained to use the TDU
(2006).
Participant RL, M, 36-years-old congenitally blind trained to use the TDU
(2007).
Sensory Substitution and the Neural Correlates …
175

This device is using up all of my concentration, it would be difﬁcult to use it and also
keep track of where I am going!
This device is way too expensive and would require extensive hours of training.
I can already get around just ﬁne with my cane! …Maybe if people started training
from a very young age… but me personally I would not buy it, not at that price.
Participant BD, M, 34-years-old trained to use the TDU, (2008).
I would love to be able to use this device to get around the University! It would help
me ﬁnd doors for my classes from further away… I wouldn’t have to worry about
hitting people in the legs
Participant DS, F, 27-years-old trained to use the EyeCane (2014).
In spite of ambitious aspirations, impressive achievements, and the fact that
some devices have been available for several decades, very few devices have found
their way into the hands of blind people for everyday use, and none have become
widespread [55, 124, 133, 191]. There is a high rate of abandonment of assistive
technology by users [164], and even the most used SSD, the vOICe, only has a few
hundred regular users around the world.
Why have decades of research using sensory substitution devices not achieved
the main goal of improving the quality of life of people who are visually impaired
and blind [122]? We identify several problems with the current forms of sensory
substitution (Box 1) and promising approaches that are attempting to circumvent
these problems. In order for devices to be generally accepted they must be made
available at a very low cost (or even free of charge), and attempt to meet the speciﬁc
needs and preferences of blind people when using SSDs [81].
The majority of SSDs for navigation present several problems [4]. In box 1, we
discuss results from the four main SSDs used in navigation tasks: The tactile visual
sensory substitution (TVSS) system [187], The TDU [32, 110], The EyeCane [133,
135, 17, 30] and the NavBelt [101].
Box 1
The Learning Problem: Most, but not all SSDs require many hours of
practice and training [32, 110, 143, 171, 197]. SSD training can sometimes
involve elements that are very different from the ones people learn in ori-
entation and mobility training. For example, if the sensor is placed on a pair
of eyeglasses, the user must make many head movements to adjust the angle
of the ﬁeld of view of the camera. By contrast, orientation and mobility (O &
M) training encourages keeping the head straight [89]. Learning skills that are
in contradiction could impair previously acquired mobility skills and dis-
courage new potential users from using SSDs. The fact that people who may
be interested in using SSDs for navigation need to acquire new skills to
navigate, when many of them have already learned to move around efﬁciently
176
D.-R. Chebat et al.

using a guide dog or a cane, may make it difﬁcult for them to see the
advantages of using SSDs.
The Standardization of Training Problem: There seem to be as many
tasks involving SSDs as there are publications in the ﬁeld, each article
developing a new task to ﬁt its methodological needs. There are no clear
guidelines for training paradigms with SSDs. The lack of standardization of
the tasks makes it hard to compare the devices in terms of efﬁciency.
Recently, a standardized version [154] of the obstacle course we used to test
locomotion in the blind [32] can help resolve some of these issues.
Standardization is the establishment of a set of controlled parameters against
which we can test the efﬁciency of SSDs. For example, a strict set of con-
trolled obstacles should be used to test these devices. Nau et al. [154] aimed
to establish this set of contingencies by regulating the size and shape of
obstacles in an obstacle course for all SSDs so that we may compare them to
one another. Using standardized testing procedures in obstacle avoidance,
i.e., locomotion [32, 154], or navigation, (wayﬁnding like the Hebb–Williams
mazes [30]) will help us understand how to develop better devices geared
toward speciﬁc tasks. Most SSD training programs do not take into account
the motor aspect (see Sect. 2). This aspect is crucial and must be taken into
consideration when devising a standardized training protocol for SSDs. The
vOICe (https://www.seeingwithsound.com) (see also: [177]) and EyeMusic
(http://brain.huji.ac.il/launch/StepByStepHeb/see) do have certain training
exercises on their websites to help users learn to recognize objects, but this
does not compare to the extensive training people receive when learning how
to use the long cane in O & M, for example. Researchers must direct their
attention to optimizing the learning process for SSDs perceptual training in
order to help guide potential users through the steps needed to interpret the
information from the device.
The Temporal Problem: In order for an SSD to be useful in navigation,
the image in the user’s surroundings needs to be presented and interpreted in
real time. Many forms of auditory-based SSDs translate the image into
sounds using the temporal aspect as the left-right translation of the image.
This creates a delay in the presentation of the image to the user (though see
[220] for how this can be solved with enough experience, similar to foveal
vision). Other forms of tactile based SSDs such as the TDU transmit the
image in real time, but because of the complexity of the information, the
interpretation of the stimulation is often delayed.
The Dissemination Problem: Many blind or visually impaired people are
not informed of the SSD technologies available today. More effort should be
made to disseminate the results of SSD scientiﬁc research to the ﬁrst ones
concerned by such forms of research. Furthermore, a lot of the scientiﬁc
journals (where this type of research is published) are not always easily
accessible for the blind, including the presentation of graphs and ﬁgures.
Sensory Substitution and the Neural Correlates …
177

Peter Meijer, a scientist based in the Netherlands, and the inventor of the
vOICe system has made noticeable and praiseworthy efforts in this direction
by regrouping the most recent literature on SSDs, brain plasticity and reha-
bilitation (https://www.seeingwithsound.com/), and making his website easily
accessible for blind and visually impaired individuals.
The Cognitive Load Problem: The cognitive load problem concerns the
complexity of the algorithm used to generate the stimuli, which ultimately
needs to be learned by the user. The more complex the interpretation of SSD
information, the more difﬁcult the completion of the sensorimotor loop
described in Fig. 2. This makes it cognitively taxing to simultaneously
interpret the SSD’s information and accomplish a task that demands attention.
Most forms of SSDs require great concentration, even in the case of trained
users [122]. Reducing SSD information (from whole scene information to
more precise forms of information) could help improve this issue.
The Orientation Problem: This problem concerns localizing objects in
space accurately using SSDs. The direction of the SSD information is often
confusing and although participants can sense that objects are in the ﬁeld of
view of the camera or sensor, they often report being unable to tell where
exactly the sensor is pointing in the environment. For example, in the case of
head-mounted camera systems (such as the TDU, or vOICe), it becomes quite
difﬁcult to understand the direction the camera is pointing to, since it is not
ﬁxed (i.e., it can be pointed up or down). The sensorimotor loop comprises
therefore an additional error that makes learning the code more difﬁcult. In
order to localize an object in space accurately, the ﬁeld of view of the camera
or sensor of the SSD must be constant. Proper training in remapping must be
optimal to achieve the appropriate distal attribution of the moving stimulus.
The Depth Problem: Older generation SSDs do not code for depth. The
lack of depth information makes it difﬁcult to detect ground level obstacles
and step over them [17, 32]. The newer devices do incorporate some aspects
of depth information. The EyeCane, for example, is entirely based on the
translating depth information into vibrations and sounds [131] (see Fig. 1d,
e). Another device that uses laser projection patterns allows 90% accuracy in
detecting potholes [174].
The Contrast Problem: Many SSD’s, such as the TDU, only work under
optimal contrast conditions. Under any other (naturalistic) settings, or dimly
lit situations these devices will not work very well.
The Resolution Problem: Most SSDs down sample the resolution of the
image in order to send it through another modality. This down sampling of
the image results sometimes in very low resolution. The best visual-acuity
available with a 10  10 grid of the TDU is still inferior to the standard of
vision used in rehabilitation. The vOICe has a better resolution than most
SSDs, but takes longer to interpret. This loss in resolution of the image makes
it difﬁcult to recognize details of a scene. Most SSDs, however do enable a
zoom in factor (both the TDU and vOICe have this option). A recent study
178
D.-R. Chebat et al.

explored this question by testing the zoom in factor in the EyeMusic SSD,
and found that participants are able to recognize facial features of cartooned
identities [18].
The Cost Problem: Although the cost to make most SSDs is minimal, the
companies that make and distribute them usually inﬂate the sell prices. This is
probably due to the long research and development phases most SSDs go
through. Some companies or laboratories that develop SSDs try to keep their
cost down by basing their programming on already existing devices, such as
exploiting the hardware on smartphones, instead of creating dedicated
hardware.
Some attempts have been made to overcome these problems. For example, the
EyeCane is easy to use and require little training but has a very low resolution. In
sharp contrast, devices that offer a higher resolution comprise very complex coding
that make them more difﬁcult to use and require therefore many hours of training
(for example, the vOICe [145]; and the EyeMusic; [1]). The EyeMusic has
implemented many of the solutions suggested here: it is available for free, and is
downloadable as an application for smart phones. The EyeCane is still in devel-
opment but researchers have already demonstrated its use in judging distances in
the real [131, 137], and virtual world [132, 133], detecting and avoiding obstacles
[17], and navigating through a real-life-sized maze [30]. It uses the same strategies
as a white cane, making sweeping movements from side to side to scan the envi-
ronment. Blind people trained with the white cane can build on existing training to
use the EyeCane [135]. We predict that these devices may have a huge impact on
the lives of blind people in the next few years, but in order for this to happen they
must meet the criteria described above and the particular needs of blind people
when using SSDs.
6
Neural Structures Supporting Navigation
Navigation involves the coordination of various neural structures (for review in
humans see: [11, 228]; for review in animals: [144]), supporting perceptual [116],
memory [22], proprioceptive [227], and motor systems (for review see: [205]). We
describe these structures in the next few sections, ﬁrst in animals and then in
humans.
Sensory Substitution and the Neural Correlates …
179

6.1
Animal Studies
In animals, the posterior parietal cortex comprises maps for places in terms of their
order on a route [156, 155]. Other structures also play a supporting role in navi-
gation, such as the caudate nucleus that integrates spatial behavior with motor
behavior [222], and the parahippocampal gyrus that possesses view-responsive
neurons to spatial scenes [179]. The hippocampus and entorhinal cortex have
several different classes of neurons [20, 19] that deal with a slightly different aspect
of navigation, such as place cells [91, 159], head-direction cells [206, 225], grid
cells [78]; for review: [151] and boundary/border (or vector cells) [20, 119]. There
is evidence in bats that their ability to ﬂy enables a multiscale 3D acquisition of
spatial information cells [67]. Multiscale maps are composed of three-dimensional
head-direction coding cells in the bat brain [61]. Lesions in hippocampal, frontal or
parietal cortices impair the ability of rats to learn and retain a spatial layout [39].
Only lesions to the hippocampus speciﬁcally impair spatial navigation [72].
Furthermore, goal locations are encoded via place cells as vectors to local targets in
a maze [65]. These studies highlight the speciﬁc role of cortical and subcortical
areas in navigation.
This complex navigation system is mature very early on in development. The
place cell network of the hippocampus displays adult-like properties as soon
as exploratory behavior emerges [152], and head-direction cells are functional even
before eye opening in the rat [203]. These results suggest that environment-based
cell networks develop independently of vision. What are the brain regions that
mediate navigation in Humans?
6.2
Human Studies
Spatial processing functions underlying navigation in humans are believed to be
generally similar to those encoding spatial memory in animals [46]. Using, a slow
event-related fMRI paradigm, Spiers, and Maguire [193] broke down the different
steps involved in navigation in humans, clarifying the role of the different regions
initially identiﬁed in animals. They found a complex choreography of areas and
cellular populations involved in both transient and sustained activity during navi-
gation [193]. In this section, we review the speciﬁc contribution of these distinct
brain regions involved in different forms of navigation strategies.
Traditionally, spatial memory and navigation have been associated with the
hippocampus [126, 127]. In the sighted, the link between hippocampal volume-and
navigation-related tasks are well established. In a classic MRI volumetric study,
Maguire and colleagues [129] investigated volumetric differences in the hip-
pocampus of humans with extensive navigational training. They measured the
different segments of the hippocampus (anterior, posterior or body) and showed that
the training-induced plastic changes in highly trained navigators resulted in a
180
D.-R. Chebat et al.

volumetric augmentation of its posterior segment [129]. Conversely, hippocampal
volume reductions are associated with impaired spatial memory [76]. Several dif-
ferent kinds of neurons supporting spatial behavior have also been found in humans
[84]; places cells: [54]; grid cells: [46, 97]; for review on place and boundary cells:
[14].
Recently, it has been shown that the anterior part of the hippocampus shows
increased activation bilaterally at the start position and near to the exit of a maze
[214], whereas the posterior segment is sensitive to borders or boundaries, like
being near a wall [47]. The different segments of the hippocampus (Anterior,
posterior and body) may actually complement each other by competitive
activation/deactivation during navigation [50]. For example, the anterior hip-
pocampus is sensitive to encoding of spatial information, while the posterior is
active during retrieval [75, 94].
Two main spatial strategies have been identiﬁed [107, 108]: egocentric (a rep-
resentation that is relative to the observer), and allocentric (a representation that is
based on the relative position objects, landmarks or places have in relation to one
another). Egocentric strategies comprise acquiring route knowledge, and categorical
knowledge. Route knowledge is when spatial knowledge is acquired from navi-
gating a route, and categorical knowledge is when spatial relations are learned
between locations [159]. Allocentric strategies are composed of survey knowledge,
and coordinate spatial learning [223]. Distinct brain regions mediate egocentric and
allocentric navigation strategies [38], and different neural structures play different
roles for learning an environment [23]. The posterior parietal lobe has a role in
egocentric spatial processing (for review see [19]). The medial part of the posterior
parietal lobe seems to be involved in the processing of movement through imme-
diate space with the lateral part involved in processing movement through
non-immediate surroundings [192]. The striatum, in particular the caudate nucleus,
is critical for memory of places [15]. The retrosplenial cortex bilaterally [94] is
believed to update reference frames when navigating [95], and shows increased
activity during spontaneous route planning, which necessitates updating topo-
graphic information [192]. The parahippocampus is involved in encoding the local
scene and generating a cognitive map ([2, 57–59, 128] (for review on RSC and PPA
see [56, 60]). Additionally, the visual cortex may also play a direct role in navi-
gation. Recently, Dilks and collaborators [45] described an area around the trans-
verse occipital sulcus (coined the Occipital Place Area, OPA) that plays a role in
representing boundaries (i.e. edges, walls, and other delimitations in the environ-
ment) [99], and the local elements of a scene [100].
These studies show that navigation is composed of many different elements that
rely on the interaction of several distinct brain regions. The question remains,
however, on the contribution of these brain areas when vision is impaired such as in
congenital or acquired blindness.
Sensory Substitution and the Neural Correlates …
181

7
Theories of Spatial Representation
in People Who Are Blind
Despite many demonstrations of navigational abilities in blind people, some the-
orists believe that they have deﬁciencies in some aspects of their spatial repre-
sentations. Although certain changes are apparent when considering the spatial
sense of blind people, we argue that those differences are task speciﬁc, due to the
limited access to spatial information, rather than due to deﬁciencies in spatial
representations. That is, the deﬁciencies exhibited by blind people during naviga-
tion are due to the lack of access to spatial information, not to an inherent inability
to represent space. There is no apparent consensus about how blind people repre-
sent space or their speciﬁc navigational strategies [186]. Studies report differences
between congenitally blind and sighted individuals concerning their ability to
represent space, some reporting deﬁciencies [24, 25, 73, 104, 213], others
supra-normal abilities [37, 48, 64, 74, 121, 178, 216]. Can these apparent contra-
dictions be reconciled?
The deﬁciency theory argues that the lack of visual experience creates a
misalignment of auditory spatial maps and, consequently, a deﬁciency in spatial
representations [73]. In this view, vision is needed to calibrate space for the other
senses. Since vision plays an important role in auditory spatial map development
[105], this theory argues that vision is required to construct and calibrate auditory
spatial maps. This argument is supported by ﬁndings that early blind individuals are
impaired in the encoding of audio motion in the vertical axis [62], and of auditory
spatial relationships between sound sources [73]. Early blind children also show a
deﬁcit on a bisection task between sound sources [213], and on simple auditory
localization tasks [24]. Auditory and proprioceptive skills of both early blind
children and adults are also compromised [25].
Other studies ﬁnd, in contrast, that congenitally blind people are better than their
blindfolded counterparts at locating a sound source, and have better use of
monaural cues [118, 216]. Congenitally blind people have also been found to have
superior pitch discrimination [74]. Using an SSD (the TDU), CB individuals have a
ﬁner visuotactile acuity [31, 199], are better at recognizing routes [110], and can
negotiate and recognize obstacles better [32] than their blindfolded counterparts.
The supranormal abilities of CB are attributed to the recruitment of the occipital
cortex [6, 7, 36, 37, 171, 218] and other visual structures for motion [182].
This contradiction between both sets of results (deﬁcient versus supranormal
spatial representations) can be somewhat reconciled. There appears to be a trade-off
between localization in the horizontal and the vertical plane; superior abilities for
monaural sound localization in the horizontal plane are associated with a deﬁcit for
localizing a sound source in the vertical plane in CB [217]. Furthermore, the tasks
used by Gori et al. [73] are complex and require the subjects to estimate the relative
position of objects to one another. It, therefore, seems that these differences might
be explained by the differences in the nature and complexity of the tasks used. In a
recent review, it was argued that differences in spatial representations between
182
D.-R. Chebat et al.

sighted and blind individuals, are either convergent, cumulative, or persistent [186].
Convergent differences in spatial representations mean that although it may take
longer for blind people to acquire spatial information, the difference between the
blind and the sighted decreases as the blind gain knowledge about the environment.
The cumulative model of differences in spatial representations between the blind
and sighted, however argues that the discrepancies in spatial knowledge will
increase with spatial experience. In other words, the blind will never be able to gain
the amount of spatial knowledge held by the sighted because the differences
between these groups only increases as they gain experience of their environment.
The persistent model proposes that spatial knowledge differences remain constant
between the blind and sighted. Deﬁciency theorists argue that the differences
between blind and sighted individuals must be either persistent or cumulative. We
propose that this difference is convergent and with the substitution of the right
aspect of spatial information (such as with SSDs), blind individuals can learn an
environment as efﬁciently as the sighted people, and perform as well as the sighted
in certain spatial tasks. This is possible because spatial representations are amodal
in nature and because of the plastic adaptation capabilities of the brain.
7.1
Amodal Representation of Space
We suggest that the successful performance of blind individuals in many spatial and
cognitive tasks indicates that the representation of space can be amodal. Spatial
information is acquired from both visual and nonvisual sources that combine to
form a multimodal representation of space [70, 113, 207]. Spatial information can
be gathered from other senses besides vision, even if vision is the best-adapted
sense to acquire spatial information. Spatial representations can also be formed
using extended-touch: poking a probe, a 1–2 m long pole, to gather information
about an object that is beyond arm’s reach [69].
A blind person might be able to acquire the same information about the envi-
ronment using a probe, but will probably take longer than a sighted person acquiring
the same spatial information visually. That is because vision collects the spatial
information instantaneously. It is possible to experimentally match the information a
blind person gets through the tactile or auditory modalities to those gained through
vision by either manipulating the environment (deteriorating the quality of visual
information, for example), or increasing the amount of knowledge the tactile or
auditory senses can gain (with SSDs for example). When spatial information is
matched through different modalities, CB can learn spatial representations as well
[30], and in some instances even better than the sighted [32, 122, 226]. It is the lack
of access to spatial information through the use of visual cues that accounts for blind
people’s difﬁculties navigating [28, 64, 123, 148, 161, 208, 212].
Sensory Substitution and the Neural Correlates …
183

Studies in perceptual learning [169], and mental scanning of pathways [92] led
to the conclusion that the representation of space does not require vision [88, 93].
For example, blind people can learn spatial maps through the sense of touch [66,
208, 210, 211], or from virtual reality training [136, 201], and generate accurate
representations of space, which can later be used to navigate [115, 136, 146, 202].
Both blind and sighted participants can access amodal, functionally equivalent
representations of space, although they are based on different sensory inputs [122].
Further evidence for the amodal representation of space comes from studies
exploring the neural correlates of navigation in the blind with or without SSDs. We
review that literature in the next section.
7.2
Neural Processing of SSD Information
One of the most fascinating aspects of users trained with SSDs is their recruitment
of cortical “visual areas” [6, 171, 184]. This training-induced recruitment has been
linked to increased tactile [3, 52] and auditory [44, 80, 118] sensitivity in response
to sensory input and environmental demands.
Before training (Fig. 1), the SSD stimulation is incomprehensible to the user,
and will recruit modality speciﬁc and prefrontal areas in the brain [171]. After
training however, this stimulation has taken on visual properties and users feel a
sense of embodiment concerning the SSD signal. In CB participants this shift in
interpretation of the signal is correlated with the recruitment of cortical areas typ-
ically associated with vision [6, 171]. This recruitment of the visual cortices
depends on an active interpretation of the SSD stimulation [153], as described in
Sect. 3. In other words, our understanding of the sensory information during
training shapes our cortical processing of it.
The impact of training-induced plasticity in the blind seems to be twofold. First,
as mentioned above, SSD tasks recruit the occipital lobe and prefrontal areas
demonstrating that these areas are multimodal. Second, there is a task-speciﬁc
activation of similar regions in blind and sighted participants with completely
different sensory inputs. It has been shown consistently that the brain treats the
information from SSDs as “vision” in a task-speciﬁc way, sending the information
to highly specialized cortical treatment centers. The “visual cortex” of congenitally
blind people is functionally active and can receive information from different senses
[26, 165, 171, 183]. The SSD information is transmitted to the appropriate nodes of
the brain according to the task.
Visual processing is generally segregated into two separate and parallel path-
ways in humans [85, 189]. These anatomical pathways are separated into a dorsal
stream that analyzes visuospatial information and a ventral stream that is respon-
sible for processing form [71, 149].
These separate visual streams are composed of highly specialized nodes that
process speciﬁc aspects of visual information. For example, motion is processed in
cortical area hMT+, objects, and faces in the inferior temporal cortex [41, 204].
184
D.-R. Chebat et al.

There are also distinct cortical nodes for body shapes [49] and even words [34].
Research with SSDs has consistently demonstrated the recruitment of specialized
nodes in the congenitally blind that are task-dependent [142, 170, 173]; for review:
[111]. Furthermore, using a visual to auditory SSD, [198] and colleagues showed
that the selectivity for shape and localization in the two streams could develop in
the total absence of vision after just a few hours of training.
Studies with SSDs have used specialized tasks to recruit these nodes presenting
words, body shapes, and motion through touch or sounds. In CB participants,
task-speciﬁc nodes that have been thus far identiﬁed include: a Visual Word Form
Area (VWFA) [175], an area for recognizing body postures [195], an area for
number forms [1], an area for motion [142, 170, 198], an area for shapes [173,
198], and one for navigation [110]. These areas are well known in the sighted to be
involved in these speciﬁc tasks, and appear to be recruited by the congenitally blind
when performing the same tasks using nonvisual modalities.
Taken together, these results suggest that not only is the occipital cortex of the
blind recruited when using SSDs by an active reinterpretation of the signal, but also
that the brain treats the afferent information in an amodal, task-dependent way (for
review see: [176]). The functional connectivity of the visual cortex in the blind
develops according to retinotopic organization, despite the total absence of vision
[200]. This had led many researchers to posit that the human brain has evolved so
that certain types of information are treated in speciﬁc areas, regardless of the
modality, and to view the brain as an amodal task-speciﬁc machine [5]; for review
see: [87]. This hypothesis implies that the brain is not particularly interested in the
modality from which the information originates, but rather depends on the task it is
trying to perform with the sensory input the cortex receives (for review on the
integration of multisensory information in the brain see Harrar et al., Chap. 4)2.
7.3
Neural Correlates of Navigation in the Case
of Blindness and Evidence for the Amodal Processing
of Spatial Information
Although visual cues greatly facilitate acquiring spatial information [53], the neural
circuits associated with spatial processing include multiple sources and are not tied
to a speciﬁc modality [114, 68, 207]. The parahippocampal place area (PPA) and
retrosplenial cortex (RSC) are both recruited for spatial processing in blind and
sighted individuals. Research on the neural correlates of navigation in the blind has
been limited, until now, by the difﬁculty of creating virtual environments for the
blind as these environments, and especially the in-scanner versions, are mainly
based on optic ﬂow. A ﬁrst attempt to try and understand the impact of congenital
2For more on this, please see the chapter, Harrar et al., The Multisensory Brain, Chap. 4 in this
special edition).
Sensory Substitution and the Neural Correlates …
185

blindness on the structures commonly associated with navigation was made using
volumetric studies. For the sake of clarity, we have divided the information on brain
correlates into volumetric and functional MRI studies. In the ﬁrst set of results, the
volume of a certain area can be related to its function. In the second set of results,
activations of a speciﬁc area indicate its function because activations are linked to a
speciﬁc task.
7.4
Volumetric Studies in the Blind’s Brain
The brain of congenitally blind people undergoes massive structural and volumetric
changes in all of the visual structures [111, 117, 138, 172, 188]. These volumetric
changes in the blind depend upon the duration of blindness [138], and hypertrophy
of nonvisual frontal and cerebellar areas suggest compensatory adaptations in
blindness [117].
While the caudate nucleus is not volumetrically different between the blind and
sighted, and these differences were not linked to a speciﬁc navigation task [217], the
hippocampus of the CB undergoes signiﬁcant changes in both the posterior [29] and
anterior end ([64]; but see also [138]). While the posterior portion of the hip-
pocampus is reduced in the blind [29], the volumetric enlargement of the anterior
portion is associated with superior navigational abilities [64]. This ﬁnding was
explained by the lack of visual projections from the atrophied visual cortex (for
review: [157]), and the possibility that the blind may rely more heavily on other
structures besides the posterior hippocampus (such as the posterior parietal cortex)
for navigation. Since the anterior end of the hippocampus is not usually associated
with spatial processing but rather with verbal memory [77]; but see also: [130]),
congenitally blind participants may rely more on verbal representations of space
(i.e., egocentric strategies), and increased demands on memory systems.
Two out of the three studies that investigated volume in the blind did not
correlate volume with performance [29, 117], and the third aggregated blind and
sighted populations to obtain their results [64]. Furthermore, the methods used to
segment the hippocampus are arbitrary and not based on any cytoarchitectonic
differences in cellular populations. The anterior, posterior, and body segmentations
of the hippocampus actually comprise several different, distinct cellular populations
(Fig. 3).
For this reason, we feel that the link between volume and performance in the
blind has not been fully established [186]. Furthermore, the segmentations of the
hippocampus that were used are based on arbitrary limits: There is no cellular basis
for segmenting the hippocampi into posterior, anterior and body, rather each one of
these segments is composed of distinct cellular populations (Fig. 3).
186
D.-R. Chebat et al.

7.5
MRI and EEG Studies on Spatial
Representations in the Blind
Research has explored the neural correlates of navigation in blindness in an attempt
to disentangle the volumetric results mentioned above. We previously proposed a
task whereby blind people had to learn how to recognize routes using a visuotactile
sensory substitution device that translates a visual image into electrotactile stimu-
lation applied to the tongue (i.e., The TDU: see Sect. 3 on sensory substitution)
[110]. Participants had to learn to navigate these routes in virtual environments, and
then recognize those routes while in a 3-T scanner. These results (Fig. 4) enabled
the identiﬁcation of a neural network involved in route recognition in the blind
Fig. 3 Hippocampus (FD Fascia dentata in red, SUB subiculum in green and CA cornu amonis in
dark blue), hippocampal area (HATA hippocampal amygdaloid transition area, EC ectosylvian
cortex in light blue)
Sensory Substitution and the Neural Correlates …
187

(Fig. 4a), for the ﬁrst time [110]. Two control groups were used for this study:
Sighted blindfolded participants (Fig. 4b) who used the TDU to do the task, and
sighted participants who were allowed to see the routes (Fig. 4c), thereby using
vision to perform the same task. The structures activated in the congenitally blind
brain (Fig. 4) during this route recognition task were the same as those activated in
the sighted participants doing the same task visually: the parahippocampal gyrus,
posterior parietal cortex, and the visual cortex (Fig. 4a, c). In contrast, the sighted
blindfolded participants recruited a different network of brain regions that did not
include the parahippocampal gyrus or the visual cortex [110] (Fig. 4b). The con-
genitally blind brain does recruit visual areas for tactile [21, 35, 171, 180] and
auditory tasks [6, 167], whereas a brain that is wired for vision (blindfolded con-
trols) does not recruit primary visual areas for nonvisual tasks. This functional
rewiring enables the recruitment of the same cortical network used for spatial
navigation tasks in CB as in sighted subjects.
A study using tactile mazes (accomplished by placing the ﬁnger in a hand maze)
found that active navigation could recruit the hippocampal formation in congeni-
tally blind subjects [66]. In addition, parietal and occipital areas were also activated
during this tactile maze solving. It is possible that the lack of activations in a
passive route recognition task [110], compared to the hippocampal recruitment in
the active ﬁnger maze navigation [66] reﬂects the fact that the hippocampus strictly
processes active navigation in the CB, and not passive recognition of previously
learned routes.
The recruitment of the hippocampal formation was also found in congenitally
blind individuals making spatial distance judgments with an audio-tactile device
[27]. Participants were asked to judge the distance of an object that they could
detect using a device translating distance into tactile and auditory pulses. In addition
to the hippocampus, the inferior parietal cortex and parts of the right occipital
cortex were also activated in the congenitally blind for this task [27]. Chan and
c
Fig. 4 Neural correlates of route recognition in blind and sighted subjects. Brain activation
patterns during route recognition using the TDU compared to a visual control paradigm. Red and
yellow voxels represent clusters of signiﬁcant BOLD signal increases during the route recognition
compared with random noise presentation, superimposed on cortical ﬂatmaps. (A) Results of blind
participants, showing activation of occipital and posterior parietal cortices, precuneus, fusiform
gyrus, and right parahippocampus during route recognition with the TDU. (B) Blindfolded sighted
control subjects did not activate the parahippocampus or occipital cortex, but they activated the
posterior parietal cortex and the precuneus. (C) Sighted control subjects, performing the route
recognition task visually, showed strong bilateral BOLD increases in the occipital and superior
parietal cortices, the precuneus, fusiform gyrus, and right parahippocampus. Cu cuneus; FG
fusiform gyrus; Ins insula; IFG inferior frontal gyrus; IPL inferior parietal lobule; ITG inferior
temporal gyrus; LG lingual gyrus; LO lateral occipital; MFG middle frontal gyrus; MTG middle
temporal gyrus; Orb orbital gyrus; Paracent paracentral gyrus; Pericalc pericalcarine sulcus;
Precent precentral gyrus; Precun precuneus; Postcent postcentral gyrus; SFG superior frontal
gyrus; STG superior temporal gyrus; SPL superior parietal lobule; SMG supramarginal gyrus
188
D.-R. Chebat et al.

colleagues explain that integration and binding of auditory features to distances are
responsible for the hippocampal activations. Sighted blindfolded participants did
not activate this network when doing the same task, rather they only recruited
parietal areas. These studies conclude that the blind use the same networks to
navigate as the visually sighted, while the sighted blindfolded population recruits a
different network. They also suggest that mechanisms of brain plasticity can recruit
the visual cortex in the blind to accomplish navigation -related tasks.
Sensory Substitution and the Neural Correlates …
189

During imagined locomotion, congenitally blind participants recruited multi-
sensory vestibular areas in the posterior insula and superior temporal gyrus, but did
not show activations in parahippocampal and fusiform regions [42]. In a follow-up
study by the same group, imagined locomotion and stance (standing) did activate
the hippocampus [43]. Further, during imagined locomotion, widespread activity
was found in the right parahippocampal gyrus in both sighted and blind individuals,
although compared to sighted controls, blind subjects showed less activity in the
right dorsal parahippocampal region [98]. These authors concluded that the
parahippocampal and fusiform gyri, which are connected to visual cortical areas,
are important for visually guided locomotion and landmark recognition during
navigation.
Sighted blindfolded participants recruited both PPA and RSC for visual and
haptic exploration of a 3D spatial scene [226]. These results suggest that these areas
are amodal, meaning that they are more concerned with the nature of the task at
hand (navigation) and less with the modality (visual, auditory or somatosensory).
Additionally the RSC, transverse occipital cortex and PPA were more strongly
activated by landmarks (large non-manipulable objects) for both congenitally blind
and sighted participants (blindfolded and visual group) in an auditory size judgment
experiment, [86]. In this task, participants had to judge the size of an object (small
tools, large manipulable objects and animals) based on the auditory cues. The
authors concluded that these regions did not need visual experience to develop a
preference for large landmarks (i.e., a stronger activation in the landmark condition
than the other conditions). Furthermore, using EEG, Kober et al. [106] showed a
signiﬁcant occipital activation during a motor imagery task in mostly late blind
participants, which is not the case with the sighted. The authors conﬁrmed that the
blind recruit the visual cortex to navigate [110].
8
Conclusions
In this chapter, we have reported and discussed the most recent advances in sensory
substitution and the neural correlates of navigation in the case of visual impairment
or congenital blindness. We have suggested that the sensorimotor loop involving
SSDs is the basis for training-induced plastic changes in the brain. Indeed, phe-
nomenological reports of late blind people using SSDs liken it to what they
remember of vision. Furthermore, the brain of CB seems to treat this information in
amodal, task-speciﬁc highly specialized brain nodes. This cortical plasticity enables
the learning of an SSD code and the embodiment of the resulting perceptions. This
embodiment, in turn, enables participants to accomplish many behavioral tasks such
as navigation. The neural structures that support navigation and SSD use in the CB
and LB brains indicate that spatial information is amodal and that the brain treats
spatial information in a task-speciﬁc way. Future studies should place an emphasis
on testing SSDs in naturalistic settings and investigating the neural correlates of
active navigation in the blind.
190
D.-R. Chebat et al.

References
1. Abboud S, Maidenbaum S, Dehaene S, Amedi A (2015). A number-form area in the blind.
Nat Commun 6
2. Aguirre GK, Detre JA, Alsop DC, D’Esposito M (1996) The parahippocampus subserves
topographical learning in man. Cereb Cortex 6(6):823–829
3. Alary F, Goldstein R, Duquette M, Chapman CE, Voss P, Lepore F (2008) Tactile acuity in
the blind: a psychophysical study using a two-dimensional angle discrimination task. Exp
Brain Res 187(4):587–594
4. Amedi A, Chebat DR, Levy-Tzedek S, Buchs G, Maidenbaum S (2014) Returning sensory
substitution to practical visual rehabilitation. Invest Ophthalmol Vis Sci 55(13):4146
5. Amedi A, Merabet LB, Camprodon J, Bermpohl F, Fox S, Ronen I, Kim DS, Pascual-Leone
A (2008) Neural and behavioral correlates of drawing in an early blind painter: a case study.
Brain Res 1242:252–262
6. Amedi A, Stern WM, Camprodon JA, Bermpohl F, Merabet L, Rotman S, Hermond C,
Meijer P, Pascual-Leone A (2007) Shape conveyed by visual-to-auditory sensory
substitution activates the lateral occipital complex. Nat Neurosci 10(6):687–689
7. Arnott SR, Thaler L, Milne JL, Kish D, Goodale MA (2013) Shape-speciﬁc activation of
occipital cortex in an early blind echolocation expert. Neuropsychologia 51(5):938–949
8. Bach-y-Rita P, Collins CC, Saunders FA, White B, Scadden L (1969) Vision substitution by
tactile image projection. Nature 221:963–964
9. Bach-y-Rita P (Ed.) (1972) Brain mechanisms in sensory substitution. Academic Press,
New York
10. Bach-y-Rita P (1971) A tactile vision substitution system based on sensory plasticity.
In: Visual prosthesis. Academic Press, New York
11. Baumann O, Mattingley JB (2013) Dissociable roles of the hippocampus and parietal cortex
in processing of coordinate and categorical spatial information. Front Human Neurosci 8:73
12. Beaudry-Richard A, Harrar V, Auvray M, Spence C, Kupers R, Ptito M (2015) The
multisensory substitution device: replacing vision with multisensory perception (poster).
In: 16th Multisensory Research Forum (IMRF), Pisa, Italy, 13–16 June
13. Bhatlawande SS, Mukhopadhyay J, Mahadevappa M (2012) Ultrasonic spectacles and
waist-belt for visually impaired and blind person. In: National conference on communica-
tions (NCC). IEEE, pp 1–4
14. Bird CM, Bisby JA, Burgess N (2012) The hippocampus and spatial constraints on mental
imagery. Front Human Neurosci 6:142
15. Bohbot VD, Iaria G, Petrides M (2004) Hippocampal function and spatial memory: evidence
from functional neuroimaging in healthy participants and performance of patients with
medial temporal lobe resections. Neuropsychology 18(3):418
16. Borisoff JF, Elliott SL, Hocaloski S, Birch GE (2010) The development of a sensory
substitution system for the sexual rehabilitation of men with chronic spinal cord injury. J Sex
Med 7(11):3647–3658
17. Buchs G, Maidenbaum S, Amedi A (2014) Obstacle identiﬁcation and avoidance using the
‘EyeCane’. EuroHaptics, LNCS 8619:13–18
18. Buchs G, Maidenbaum S, Amedi A, Levy-Tzedek S (2015) Virtually zooming-in with
sensory substitution for blind users. In: International conference on Virtual rehabilitation
proceedings (ICVR). IEEE, pp 133–134
19. Burgess N, Maguire EA, O’Keefe J (2002) The human hippocampus and spatial and
episodic memory. Neuron 35(4):625–641
20. Burgess N, Jackson A, Hartley T, O’keefe J (2000) Predictions derived from modelling the
hippocampal role in navigation. Biol Cybern 83(3):301–312
21. Burton H, McLaren DG, Sinclair RJ (2006) Reading embossed capital letters: an fMRI study
in blind and sighted individuals. Hum Brain Mapp 27(4):325–339
Sensory Substitution and the Neural Correlates …
191

22. Buzsáki G, Moser EI (2013) Memory, navigation and theta rhythm in the hippocampal-
entorhinal system. Nat Neurosci 16(2):130–138
23. Byrne P, Becker S, Burgess N (2007) Remembering the past and imagining the future: a
neural model of spatial memory and imagery. Psychol Rev 114(2):340
24. Cappagli G, Gori M (2016) Auditory spatial localization: developmental delay in children
with visual impairments. Res Dev Disabil 53:391–398
25. Cappagli G, Cocchi E, Gori M. (2015) Auditory and proprioceptive spatial impairments in
blind children and adults. Develop Sci
26. Cattaneo Z, Vecchi T (2008) Supramodality effects in visual and haptic spatial processes.
J Exp Psychol Learn Mem Cogn 34:631–642
27. Chan CC, Wong AW, Ting KH, Whitﬁeld-Gabrieli S, He J, Lee T (2012) Cross
auditory-spatial learning in early-blind individuals. Hum Brain Mapp 33(11):2714–2727
28. Chandler E, Worsfold J (2013) Understanding the requirements of geographical data for
blind and partially sighted people to make journeys more independently. Appl Ergon 44(6):
919–928
29. Chebat DR, Chen JK, Schneider F, Ptito A, Kupers R, Ptito M (2007) Alterations in right
posterior hippocampus in early blind individuals. NeuroReport 18(4):329–333
30. Chebat DR, Maidenbaum S, Amedi A (2015) Navigation using sensory substitution in real
and virtual mazes. PLoS ONE 10(6):e0126307
31. Chebat DR, Rainville C, Kupers R, Ptito M (2007) Tactile—‘visual’ acuity of the tongue in
early blind individuals. NeuroReport 18(18):1901–1904
32. Chebat DR, Schneider FC, Kupers R, Ptito M (2011) Navigation with a sensory substitution
device in congenitally blind individuals. NeuroReport 22(7):342–347
33. Chiou R, Stelter M, Rich AN (2013) Beyond colour perception: auditory–visual synaesthesia
induces experiences of geometric objects in speciﬁc locations. Cortex 49(6):1750–1763
34. Cohen L, Dehaene S, Naccache L, Lehéricy S, Dehaene-Lambertz G, Hénaff MA, Michel F
(2000) The visual word form area. Brain 123(2):291–307
35. Cohen LG, Celnik P, Pascual-Leone A, Corwell B, Faiz L, Dambrosia J, Hallett M (1997)
Functional relevance of cross-modal plasticity in blind humans. Nature 389(6647), 180–183
36. Collignon O, Lassonde M, Lepore F, Bastien D, Veraart C (2007) Functional cerebral
reorganization for auditory spatial processing and auditory substitution of vision in early
blind subjects. Cereb Cortex 17(2):457–465
37. Collignon O, Renier L, Bruyer R, Tranduy D, Veraart C (2006) Improved selective and
divided spatial attention in early blind subjects. Brain Res 1075(1):175–182
38. Committeri G, Galati G, Paradis AL, Pizzamiglio L, Berthoz A, LeBihan D (2004)
Reference frames for spatial cognition: different brain areas are involved in viewer-, object-,
and landmark-centered judgments about object location. J Cogn Neurosci 16(9):1517–1535
39. Compton DM, Grifﬁth HR, McDaniel WF, Foster RA, Davis BK (1997) The ﬂexible use of
multiple cue relationships in spatial navigation: a comparison of water maze performance
following hippocampal, medial septal, prefrontal cortex, or posterior parietal cortex lesions.
Neurobiol Learn Mem 68(2):117–132
40. Deroy O, Auvray M (2014) A crossmodal perspective on sensory substitution. Percep
Modalities 327–349
41. Desimone R (1991) Face-selective cells in the temporal cortex of monkeys. J Cogn Neurosci
3(1):1–8
42. Deutschländer A, Stephan T, Hüfner K, Wagner J, Wiesmann M, Strupp M, Brandt T,
Jahn K (2009) Imagined locomotion in the blind: an fMRI study. Neuroimage 45(1):
122–128
43. Deutschländer A, Stephan T, Hüfner K, Wagner J, Wiesmann M, Strupp M, Brandt T,
Jahn K (2009) Vestibular cortex activation during locomotor imagery in the blind. Ann NY
Acad Sci 1164(1):350–352
44. Dietrich S, Hertrich I, Ackermann H (2013) Training of ultra-fast speech comprehension
induces functional reorganization of the central-visual system in late-blind humans. Front
Hum Neurosci 7(701):10–3389
192
D.-R. Chebat et al.

45. Dilks DD, Julian JB, Paunov AM, Kanwisher N (2013) The occipital place area is causally
and selectively involved in scene perception. J Neurosci 33(4):1331–1336
46. Doeller CF, Barry C, Burgess N (2010) Evidence for grid cells in a human memory network.
Nature 463(7281):657–661
47. Doeller CF, King JA, Burgess N (2008) Parallel striatal and hippocampal systems for
landmarks and boundaries in spatial memory. Proc Natl Acad Sci 105(15):5915–5920
48. Doucet ME, Guillemot JP, Lassonde M, Gagné JP, Leclerc C, Lepore F (2005) Blind
subjects process auditory spectral cues more efﬁciently than sighted individuals. Exp Brain
Res 160(2):194–202
49. Downing PE, Jiang Y, Shuman M, Kanwisher N (2001) A cortical area selective for visual
processing of the human body. Science 293(5539):2470–2473
50. Duarte IC, Ferreira C, Marques J, Castelo-Branco M (2014) Anterior/posterior competitive
deactivation/activation dichotomy in the human hippocampus as revealed by a 3D
navigation task. PLoS ONE 9(1):e86213
51. Dunai L, Peris-Fajarnés G, Lluna E, Defez B (2013) Sensory navigation device for blind
people. J Navig 66(03):349–362
52. D’Angiulli AMEDEO, Waraich P (2002) Enhanced tactile encoding and memory
recognition in congenital blindness. Int J Rehabil Res 25(2):143–145
53. Ekstrom AD (2015) Why vision is important to how we navigate. Hippocampus 25(6):
731–735
54. Ekstrom AD, Kahana MJ, Caplan JB, Fields TA, Isham EA, Newman EL, Fried I (2003)
Cellular networks underlying human spatial navigation. Nature 425(6954):184–188
55. Elli GV, Benetti S, Collignon O (2014) Is there a future for sensory substitution outside
academic laboratories? Multisensory Res 27(5–6):271–291
56. Epstein RA (2008) Parahippocampal and retrosplenial contributions to human spatial
navigation. Trends Cogn Sci 12(10):388–396
57. Epstein R, Harris A, Stanley D, Kanwisher N (1999) The parahippocampal place area:
recognition, navigation, or encoding? Neuron 23(1):115–125
58. Epstein R, Kanwisher N (1998) A cortical representation of the local visual environment.
Nature 392(6676):598–601
59. Epstein RA, Parker WE, Feiler AM (2007) Where am I now? Distinct roles for
parahippocampal and retrosplenial cortices in place recognition. J Neurosci 27(23):6141–6149
60. Epstein RA, Vass LK (2014) Neural systems for landmark-based wayﬁnding in humans.
Philos Trans R Soc Lond B Biol Sci 369(1635):20120533
61. Finkelstein A, Derdikman D, Rubin A, Foerster JN, Las L, Ulanovsky N (2015)
Three-dimensional head-direction coding in the bat brain. Nature 517(7533):159–164
62. Finocchietti S, Cappagli G, Gori M (2015) Encoding audio motion: spatial impairment in
early blind individuals. Front Psychol 6
63. Fornazzari L, Fischer CE, Ringer L, Schweizer TA (2012) “Blue is music to my ears”:
multimodal synesthesias after a thalamic stroke. Neurocase 18(4):318–322
64. Fortin M, Voss P, Lord C, Lassonde M, Pruessner J, Saint-Amour D, Rainville C, Lepore F
(2008) Wayﬁnding in the blind: larger hippocampal volume and supranormal spatial
navigation. Brain 131(11):2995–3005
65. Foster DJ, Knierim JJ (2012) Sequence learning and the role of the hippocampus in rodent
navigation. Curr Opin Neurobiol 22(2):294–300
66. Gagnon L, Schneider FC, Siebner HR, Paulson OB, Kupers R, Ptito M (2012) Activation of
the hippocampal complex during tactile maze solving in congenitally blind subjects.
Neuropsychologia 50(7):1663–1671
67. Geva-Sagiv M, Las L, Yovel Y, Ulanovsky N (2015) Spatial cognition in bats and rats: from
sensory acquisition to multiscale maps and navigation. Nat Rev Neurosci 16(2):94–108
68. Giudice NA, Betty MR, Loomis JM (2011) Functional equivalence of spatial images from
touch and vision: Evidence from spatial updating in blind and sighted individuals. J Exp
Psychol Learn Mem Cogn 37(3):621
Sensory Substitution and the Neural Correlates …
193

69. Giudice NA, Klatzky RL, Bennett CR, Loomis JM (2013) Perception of 3-D location based
on vision, touch, and extended touch. Exp Brain Res 224(1):141–153
70. Giudice NA, Tietz JD (2008) Learning with virtual verbal displays: effects of interface
ﬁdelity on cognitive map development. In: International conference on spatial cognition.
Springer, Berlin, pp 121–137
71. Goodale MA, Milner AD (1992) Separate visual pathways for perception and action. Trends
Neurosci 15(1):20–25
72. Goodrich-Hunsaker NJ, Hunsaker MR, Kesner RP (2005) Effects of hippocampus
sub-regional lesions for metric and topological spatial information processing. Soc
Neurosci, Abstr
73. Gori M, Sandini G, Martinoli C, Burr DC (2014) Impairment of auditory spatial localization
in congenitally blind human subjects. Brain 137(1):288–293
74. Gougoux F, Lepore F, Lassonde M, Voss P, Zatorre RJ, Belin P (2004) Neuropsychology:
pitch discrimination in the early blind. Nature 430(6997):309
75. Greicius MD, Krasnow B, Boyett-Anderson JM, Eliez S, Schatzberg AF, Reiss AL,
Menon V (2003) Regional analysis of hippocampal activation during memory encoding and
retrieval: fMRI study. Hippocampus 13(1):164–174
76. Guderian S, Dzieciol AM, Gadian DG, Jentschke S, Doeller CF, Burgess N, Mishkin M,
Vargha-Khadem F (2015) Hippocampal volume reduction in humans predicts impaired
allocentric spatial memory in virtual-reality navigation. J Neurosci 35(42):14123–14131
77. Hackert VH, den Heijer T, Oudkerk M, Koudstaal PJ, Hofman A, Breteler MMB (2002)
Hippocampal head size associated with verbal memory performance in nondemented elderly.
Neuroimage 17(3):1365–1372
78. Hafting T, Fyhn M, Molden S, Moser MB, Moser EI (2005) Microstructure of a spatial map
in the entorhinal cortex. Nature 436(7052):801–806
79. Hamid SN, Stankiewicz B, Hayhoe M (2010) Gaze patterns in navigation: encoding
information in large-scale environments. J Vision 10(12):28
80. Hamilton RH, Pascual-Leone A, Schlaug G (2004) Absolute pitch in blind musicians.
NeuroReport 15(5):803–806
81. Hamilton-Fletcher G, Obrist M, Watten P, Mengucci M, Ward J (2016) I always wanted to
see the night sky: blind user preferences for Sensory Substitution Devices
82. Hartcher-O’Brien J, Auvray M (2014) The process of distal attribution illuminated through
studies of sensory substitution. Multisensory Res 27(5–6):421–441
83. Hartcher-O’Brien J, Auvray M, Hayward V (2015) Perception of distance-to-obstacle
through time-delayed tactile feedback. In World Haptics Conference (WHC), 2015 IEEE
(pp 7–12). IEEE
84. Hassabis D, Chu C, Rees G, Weiskopf N, Molyneux PD, Maguire EA (2009) Decoding
neuronal ensembles in the human hippocampus. Curr Biol 19(7):546–554
85. Haxby JV, Grady CL, Horwitz B, Ungerleider LG, Mishkin M, Carson RE, Herscovitch P,
Schapiro MB, Rapoport SI (1991) Dissociation of object and spatial visual processing
pathways in human extrastriate cortex. Proc Nat Acad Sci 88(5):1621–1625
86. He C, Peelen MV, Han Z, Lin N, Caramazza A, Bi Y (2013) Selectivity for large
nonmanipulable objects in scene-selective visual cortex does not require visual experience.
Neuroimage 79:1–9
87. Heimler B, Striem-Amit E, Amedi A (2015) Origins of task-speciﬁc sensory-independent
organization in the visual and auditory brain: neuroscience evidence, open questions and
clinical implications. Curr Opin Neurobiol 35:169–177
88. Herman JF, Herman TG, Chatman SP (1983) Constructing cognitive maps from partial
information: a demonstration study with congenitally blind subjects. J Vis Impair Blindness
89. Hersh MA, Johnson MA (2010) A robotic guide for blind people. Part 1. A multi-national
survey of the attitudes, requirements and preferences of potential end-users. Appl Bion
Biomech 7(4):277–288
90. Hill J, Black J (2003) The miniguide: a new electronic travel device. J Vis Impair Blindness
97(10):1–6
194
D.-R. Chebat et al.

91. Huxter J, Burgess N, O’Keefe J (2003) Independent rate and temporal coding in
hippocampal pyramidal cells. Nature 425(6960):828–832
92. Iachini T, Ruggiero G (2010) The role of visual experience in mental scanning of actual
pathways: evidence from blind and sighted people. Perception 39(7):953–969
93. Iachini T, Ruggiero G, Ruotolo F (2014) Does blindness affect egocentric and allocentric
frames of reference in small and large scale spaces? Behav Brain Res 273:73–81
94. Iaria G, Chen JK, Guariglia C, Ptito A, Petrides M (2007) Retrosplenial and hippocampal
brain regions in human navigation: complementary functional contributions to the formation
and use of cognitive maps. Eur J Neurosci 25(3):890–899
95. Iaria G, Petrides M, Dagher A, Pike B, Bohbot VD (2003) Cognitive strategies dependent on
the hippocampus and caudate nucleus in human navigation: variability and change with
practice. J Neurosci 23(13):5945–5952
96. Ione A, Tyler C (2004) Neuroscience, history and the arts synesthesia: is F-sharp colored
violet? J Hist Neurosci 13(1):58–65
97. Jacobs J, Weidemann CT, Miller JF, Solway A, Burke JF, Wei X, Kahana MJ (2013) Direct
recordings of grid-like neuronal activity in human spatial navigation. Nat Neurosci 16(9):
1188–1190
98. Jahn K, Wagner J, Deutschländer A, Kalla R, Hüfner K, Stephan T, Brandt T (2009) Human
hippocampal activation during stance and locomotion. Ann NY Acad Sci 1164(1):229–235
99. Julian JB, Ryan J, Hamilton RH, Epstein RA (2016) The occipital place area is causally
involved in representing environmental boundaries during navigation. Curr Biol 26(8):1104–
1109
100. Kamps FS, Julian JB, Kubilius J, Kanwisher N, Dilks DD (2016) The occipital place area
represents the local elements of scenes. NeuroImage 132:417–424
101. Karcher S, Fenzlaff S, Hartmann D, Nagel S, Konig P (2012) Sensory augmentation for the
blind. Front Hum Neurosci 1(6):37. doi:10.3389/fnhum.2012.00037
102. Kaspar K, König S, Schwandt J, König P (2014) The experience of new sensorimotor
contingencies by sensory augmentation. Conscious Cogn 28:47–63
103. Kay L (1974) A sonar aid to enhance spatial perception of the blind: engineering design and
evaluation. Radio Electron Eng 44(11):605–627
104. King AJ (2014) What happens to your hearing if you are born blind? Brain 137(1):6–8
105. King AJ (2009) Visual inﬂuences on auditory spatial learning. Philos Trans R Soc London B
Biol Sci 364(1515):331–339
106. Kober SE, Wood G, Kampl C, Neuper C, Ischebeck A (2014) Electrophysiological
correlates of mental navigation in blind and sighted people. Behav Brain Res 273:106–115
107. Kosslyn SM, Chabris CF, Marsolek CJ, Koenig O (1992) Categorical versus coordinate
spatial relations: computational analyses and computer simulations. J Exp Psychol Hum
Percept Perform 18(2):562
108. Kosslyn SM, Koenig O, Barrett A, Cave CB, Tang J, Gabrieli JD (1989) Evidence for two
types of spatial representations: hemispheric specialization for categorical and coordinate
relations. J Exp Psychol Hum Percept Perform 15(4):723
109. Krishna S, Bala S, McDaniel T, McGuire S, Panchanathan S (2010) VibroGlove: an assistive
technology aid for conveying facial expressions. In: Proceedings of the 28th of the
international conference extended abstracts on human factors in computing systems. ACM,
Atlanta, Georgia, USA, pp 3637–3642. doi:10.1145/1753846.1754031
110. Kupers R, Chebat DR, Madsen KH, Paulson OB, Ptito M (2010) Neural correlates of virtual
route recognition in congenital blindness. Proc Natl Acad Sci 107(28):12716–12721
111. Kupers R, Ptito M (2014) Compensatory plasticity and cross-modal reorganization following
early visual deprivation. Neurosci Biobehav Rev 41:36–52
112. Kupers R, Pietrini P, Ricciardi E, Ptito M (2011) The nature of consciousness in the visually
deprived brain. Front Psychol 2(4)
113. Lacey S, Sathian K (2014) Visuo-haptic multisensory object recognition, categorization, and
representation. Front Psychol 5:730
Sensory Substitution and the Neural Correlates …
195

114. Lacey S, Stilla R, Sreenivasan K, Deshpande G, Sathian K (2014) Spatial imagery in haptic
shape perception. Neuropsychologia 60:144–158
115. Lahav O (2006) Using virtual environment to improve spatial perception by people who are
blind. Cyberpsychology Behav 9(2):174–177
116. Law CT, Gold JI (2008) Neural correlates of perceptual learning in a sensory-motor, but not
a sensory, cortical area. Nat Neurosci 11(4):505–513
117. Leporé N, Shi Y, Lepore F, Fortin M, Voss P, Chou YY, Lord C, Lassonde M, Dinov ID,
Toga AW, Thompson PM (2009) Pattern of hippocampal shape and volume differences in
blind subjects. Neuroimage 46(4):949–957
118. Lessard N, Pare M, Lepore F, Lassonde M (1998) Early-blind human subjects localize sound
sources better than sighted subjects. Nature 395(6699):278–280
119. Lever C, Burton S, Jeewajee A, O’Keefe J, Burgess N (2009) Boundary vector cells in the
subiculum of the hippocampal formation. J Neurosci 29(31):9771–9777
120. Levy-Tzedek S, Hanassy S, Abboud S, Maidenbaum S, Amedi A (2012) Fast, accurate
reaching movements with a visual-to-auditory sensory substitution device. Restorative
Neurol Neurosci 30(4):313–323
121. Lewald J (2013) Exceptional ability of blind humans to hear sound motion: implications for
the emergence of auditory space. Neuropsychologia 51(1):181–186
122. Loomis JM, Klatzky RL, Giudice NA (2012) Sensory substitution of vision: importance of
perceptual and cognitive processing. CRC Press, Boca Raton, pp 162–191
123. Loomis JM, Klatzky RL, Golledge RG, Cicinelli JG, Pellegrino JW, Fry PA (1993)
Nonvisual navigation by blind and sighted: assessment of path integration ability. J Exp
Psychol Gen 122(1):73
124. Loomis JM, Wiener WR, Welsh RL, Blasch BB (2010) Sensory substitution for orientation
and mobility: what progress are we making? In: Guth DA, Rieser JJ, Ashmead DH
(eds) Perceiving to move and moving to perceive: control of locomotion by students with
vision loss. Foundations of Orientation and Mobility (History and Theory), pp 7–10
125. Lynch K (1960) The image of the city, vol 11. MIT Press, Cambridge
126. Maguire EA, Frackowiak RSJ, Frith CD (1996) Learning to ﬁnd your way: a role for the
human hippocampal formation. Proc R Soc Lond B Biol Sci 263(1377):1745–1750
127. Maguire EA, Frackowiak RS, Frith CD (1997) Recalling routes around London: activation
of the right hippocampus in taxi drivers. J Neurosci 17(18):7103–7110
128. Maguire EA, Frith CD, Burgess N, Donnett JG, O’Keefe J (1998) Knowing where things
are: parahippocampal involvement in encoding object locations in virtual large-scale space.
Cogn Neurosci J 10(1):61–76
129. Maguire EA, Gadian DG, Johnsrude IS, Good CD, Ashburner J, Frackowiak RS, Frith CD
(2000) Navigation-related structural change in the hippocampi of taxi drivers. Proc Natl
Acad Sci 97(8):4398–4403
130. Maguire EA, Woollett K, Spiers HJ (2006) London taxi drivers and bus drivers: a structural
MRI and neuropsychological analysis. Hippocampus 16(12):1091–1101
131. Maidenbaum S, Abboud S, Amedi A (2014) Sensory substitution: closing the gap between
basic research and widespread practical visual rehabilitation. Neurosci Biobehav Rev 41:3–15
132. Maidenbaum S, Abboud S, Buchs G, Amedi A (2015) Blind in a virtual world: using
sensory substitution for generically increasing the accessibility of graphical virtual
environments. In Virtual Reality (VR), 2015 IEEE (pp 233–234). IEEE
133. Maidenbaum S, Arbel R, Abboud S, Chebat D, Levy-Tzedek S, Amedi A (2012) Virtual 3D
shape and orientation discrimination using point distance information. In Proceedings of the
9th international conference disability, virtual reality & associated technologies, pp 471–474
134. Maidenbaum S, Buchs G, Abboud S, Lavi-Rotbain O, Amedi A (2016) Perception of
graphical virtual environments by blind users via sensory substitution. PLoS ONE 11(2):
e0147501
135. Maidenbaum S, Hanassy S, Abboud S, Buchs G, Chebat DR, Levy-Tzedek S, Amedi A
(2014) The “EyeCane”, a new electronic travel aid for the blind: Technology, behavior &
swift learning. Restorative Neurol Neurosci 32(6):813–824
196
D.-R. Chebat et al.

136. Maidenbaum S, Levy-Tzedek S, Chebat DR, Amedi A (2013) Increasing accessibility to the
blind of virtual environments, using a virtual mobility aid based on the “EyeCane”:
feasibility study. PLoS ONE 8(8):e72555
137. Maidenbaum S, Levy-Tzedek S, Chebat DR, Namer-Furstenberg R, Amedi A (2014) The
effect of extended sensory range via the EyeCane sensory substitution device on the
characteristics of visionless virtual navigation. Multisensory Res 27(5–6):379–397
138. Maller JJ, Thomson RH, Ng A, Mann C, Eager M, Ackland H, Fitzgerald PB, Egan G,
Rosenfeld JV (2016) Brain morphometry in blind and sighted subjects. J Clin Neurosci
139. Mann S, Huang J, Janzen R, Raymond L, Rampersad V, Chen A, Doha T (2011) Blind
navigation with a wearable range camera and vibrotactile helmet. In: Proceedings of the 19th
ACM international conference on multimedia, pp 1325–1328
140. Marks LE (1975) On colored-hearing synesthesia: cross-modal translations of sensory
dimensions. Psychol Bull 82(3):303
141. Marston JR, Church RL (2005) A relative access measure to identify barriers to efﬁcient
transit use by persons with visual impairments. Disabil Rehabil 27(13):769–779
142. Matteau I, Kupers R, Ricciardi E, Pietrini P, Ptito M (2010) Beyond visual, aural and haptic
movement perception: hMT+ is activated by electrotactile motion stimulation of the tongue
in sighted and in congenitally blind individuals. Brain Res Bull 82(5):264–270
143. Matteau I, Kupers R, Ptito M (2008) Tactile shape recognition through the tongue in the
congenitally blind. In: FENS. Abstracts, vol 4, pp 153–158
144. McClelland JL, McNaughton BL, O’Reilly RC (1995) Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures
of connectionist models of learning and memory. Psychol Rev 102(3):419
145. Meijer PB (1992) An experimental system for auditory image representations. IEEE Trans
Biomed Eng 39:112–121
146. Merabet LB, Connors EC, Halko MA, Sánchez J (2012) Teaching the blind to ﬁnd their way
by playing video games. PLoS ONE 7(9):e44958
147. Merabet LB, Sánchez J (2016) Development of an audio-haptic virtual interface for
navigation of large-scale environments for people who are blind. In: International conference
on universal access in human-computer interaction. Springer International Publishing,
Berlin, pp 595–606
148. Millar S (1988) Models of sensory deprivation: the nature/nurture dichotomy and spatial
representation in the blind. Int J Behav Dev 11(1):69–87
149. Milner AD, Goodale MA (2008) Two visual systems re-viewed. Neuropsychologia 46(3):
774–785
150. Montello DR, Sas C (2006) Human factors of wayﬁnding in navigation
151. Moser EI, Kropff E, Moser MB (2008) Place cells, grid cells, and the brain’s spatial
representation system. Neuroscience 31(1):69
152. Muessig L, Hauser J, Wills TJ, Cacucci F (2016) Place cell networks in pre-weanling rats
show associative memory properties from the onset of exploratory behavior. Cereb Cortex
174
153. Murphy MC, Fisher C, Kim SG, Schuman JS, Nau AC, Chan KC (2014) Top down
inﬂuence on the visual cortex of the blind during auditory sensory substitution. In:
Proceedings of International Society for Magnetic Resonance in Medicine, vol 22, p 579
154. Nau AC, Pintar C, Fisher C, Jeong JH, Jeong K (2014) A standardized obstacle course for
assessment of visual function in ultra low vision and artiﬁcial vision. J Visualized Exp
(JoVE) 84:e51205–e51205
155. Nitz DA (2006) Tracking route progression in the posterior parietal cortex. Neuron 49(5):
747–756
156. Nitz D (2009) Parietal cortex, navigation, and the construction of arbitrary reference frames
for spatial information. Neurobiol Learn Mem 91(2):179–185
157. Noppeney U (2007) The effects of visual deprivation on functional and structural
organization of the human brain. Neurosci Biobehav Rev 31(8):1169–1180
Sensory Substitution and the Neural Correlates …
197

158. Noë A (2010) Vision without representation. In: Perception, action, and consciousness:
sensorimotor dynamics and two visual systems, pp 245–256
159. O’Keefe J, Nadel L (1978) The hippocampus as a cognitive map, vol 3. Clarendon Press,
Oxford, pp 483–484
160. O’Regan JK, Noë A (2001) A sensorimotor account of vision and visual consciousness.
Behav Brain Sci 24(05):939–973
161. Passini R, Proulx G, Rainville C (1990) The spatio-cognitive abilities of the visually
impaired population. Environ Behav 22(1):91–118
162. Patla AE, Prentice SD, Gobbi LT (1996) Visual control of obstacle avoidance during
locomotion: strategies in young children, young and older adults. Adv Psychol 114:257–277
163. Patla AE, Vickers JN (1997) Where and when do we look as we approach and step over an
obstacle in the travel path? NeuroReport 8(17):3661–3665
164. Phillips B, Zhao H (1993) Predictors of assistive technology abandonment. Assistive
Technol 5(1):36–45
165. Pietrini P, Ptito M, Kupers R (2009) Blindness and consciousness: new light from the dark. In:
The neurology of consciousness: Cognitive neuroscience and neuropathology, pp 360–374
166. Pissaloux E, Velazquez R, Hersh M, Uzan G (2016) Towards a cognitive model of human
mobility: an investigation of tactile perception for use in mobility devices. J Navig 1–17
167. Poirier C, Collignon O, DeVolder AG, Renier L, Vanlierde A, Tranduy D, Scheiber C
(2005) Speciﬁc activation of the V5 brain area by auditory motion processing: an fMRI
study. Cogn Brain Res 25(3):650–658
168. Proulx MJ (2010) Synthetic synaesthesia and sensory substitution. Conscious Cogn 19
(1):501–503
169. Proulx MJ, Pasqualotto A, Taya S (2012) The role of visual experience for spatial numerical
associations. Seeing Perceiving 25:222-222
170. Ptito M, Matteau I, Gjedde A, Kupers R (2009) Recruitment of the middle temporal area by
tactile motion in congenital blindness. NeuroReport 20(6):543–547
171. Ptito M, Moesgaard SM, Gjedde A, Kupers R (2005) Cross-modal plasticity revealed by
electrotactile stimulation of the tongue in the congenitally blind. Brain 128(3):606–614
172. Ptito M, Schneider FC, Paulson OB, Kupers R (2008) Alterations of the visual pathways in
congenital blindness. Exp Brain Res 187(1):41–49
173. Ptito M, Matteau I, Zhi Wang A, Paulson OB, Siebner HR, Kupers R (2012) Crossmodal
recruitment of the ventral visual stream in congenital blindness. Neural Plast
174. Rao AS, Gubbi J, Palaniswami M, Wong E (2016, May). A vision-based system to detect
potholes and uneven surfaces for assisting blind people. In 2016 IEEE International
Conference on Communications (ICC)(pp 1–6). IEEE
175. Reich L, Szwed M, Cohen L, Amedi A (2011) A ventral visual stream reading center
independent of visual experience. Curr Biol 21(5):363–368
176. Reich L, Maidenbaum S, Amedi A (2012) The brain as a ﬂexible task machine: implications
for visual rehabilitation using noninvasive vs. invasive approaches. Curr Opin Neurol 25
(1):86–95
177. Reynolds Z, Glenney B (2009) Interactive training for sensory substitution devices. In:
Proceedings of AP-CAP, pp 131–134
178. Roder B, Teder-SaÈlejaÈrvi W, Sterr A, RoÈsler F, Hillyard SA, Neville HJ (1999)
Improved auditory spatial tuning in blind humans. Nature 400(6740):162–166
179. Rolls ET, O’Mara SM (1995) View-responsive neurons in the primate hippocampal
complex. Hippocampus 5(5):409–424
180. Sadato N, Pascual-Leone A, Grafman J, Deiber MP, Ibanez V, Hallett M (1998) Neural
networks for Braille reading by the blind. Brain 121(7):1213–1229
181. Sadeghi SG, Minor LB, Cullen KE (2012) Neural correlates of sensory substitution in
vestibular pathways following complete vestibular loss. J Neurosci 32(42):14685–14695
182. Saenz M, Lewis LB, Huth AG, Fine I, Koch C (2008) Visual motion area MT+/V5 responds
to auditory motion in human sight-recovery subjects. J Neurosci 28(20):5141–5148
198
D.-R. Chebat et al.

183. Sathian K, Zangaladze A (2002) Feeling with the mind’s eye: contribution of visual cortex to
tactile perception. Behav Brain Res 135(1):127–132
184. Sathian K, Lacey S (2008) Visual cortical involvement during tactile perception in blind and
sighted individuals. Blindness and brain plasticity in navigation and object perception.
Erlbaum, Mahwah, pp 113–125
185. Schinazi VR, Epstein RA (2010) Neural correlates of real-world route learning. Neuroimage
53(2):725–735
186. Schinazi VR, Thrash T, Chebat DR (2016) Spatial navigation by congenitally blind
individuals. Wiley Interdisc Rev Cogn Sci 7(1):37–58
187. Segond H, Weiss D, Sampaio E (2005) Human spatial navigation via a visuo-tactile sensory
substitution system. Perception 34(10):1231–1249
188. Shimony JS, Burton H, Epstein AA, McLaren DG, Sun SW, Snyder AZ (2006) Diffusion
tensor imaging reveals white matter reorganization in early blind humans. Cereb Cortex
16(11):1653–1661
189. Shmuelof L, Zohary E (2005) Dissociation between ventral and dorsal fMRI activation
during object and action recognition. Neuron 47(3):457–470
190. Shoval S, Borenstein J, Koren Y (1998) Auditory guidance with the navbelt-a computerized
travel aid for the blind. IEEE Trans Syst Man Cybern Appl Rev 28(3):459–467
191. Spence C (2014) The skin as a medium for sensory substitution. Multisensory Res 27(5–6):
293–312
192. Spiers HJ, Maguire EA (2006) Thoughts, behaviour, and brain dynamics during navigation
in the real world. Neuroimage 31(4):1826–1840
193. Spiers HJ, Maguire EA (2007) Decoding human brain activity during real-world
experiences. Trends Cogn Sci 11(8):356–365
194. Stoll C, Palluel-Germain R, Fristot V, Pellerin D, Alleysson D, Graff C (2015) Navigating
from a depth image converted into sound. Appl Bionics Biomech 2015
195. Striem-Amit E, Amedi A (2014) Visual cortex extrastriate body-selective area activation in
congenitally blind people “seeing” by using sounds. Curr Biol 24(6):687–692
196. Striem-Amit E, Cohen L, Dehaene S, Amedi A (2012a) Reading with sounds: sensory
substitution selectively activates the visual word form area in the blind. Neuron 76(3):
640–652
197. Striem-Amit E, Dakwar O, Hertz U, Meijer P, Stern W, Pascual-Leone A, Amedi A (2011)
The neural network of sensory-substitution object shape recognition. Funct Neurol Rehab
Ergon 1(2):271
198. Striem-Amit E, Dakwar O, Reich L, Amedi A (2012b) The large-scale organization of
“visual” streams emerges without visual experience. Cereb Cortex 22(7):1698–1709
199. Striem-Amit E, Guendelman M, Amedi A (2012c) ‘Visual’acuity of the congenitally blind
using visual-to-auditory sensory substitution. PLoS ONE 7(3):e33136
200. Striem-Amit E, Ovadia-Caro S, Caramazza A, Margulies DS, Villringer A, Amedi A (2015)
Functional connectivity of visual cortex in the blind follows retinotopic organization
principles. Brain 138(6):1679–1695
201. Sánchez J, Lumbreras M (1999) Virtual environment interaction through 3D audio by blind
children. CyberPsychology Behav 2(2):101–111
202. Sánchez J, de la Torre N (2010) Autonomous navigation through the city for the blind. In:
Proceedings of the 12th international ACM SIGACCESS conference on computers and
accessibility (ASSETS’10). ACM Press, New York, p 195
203. Tan HM, Bassett JP, O’Keefe J, Cacucci F, Wills TJ (2015) The development of the head
direction system before eye opening in the rat. Curr Biol 25(4):479–483
204. Tanaka K (1997) Mechanisms of visual object recognition: monkey and human studies. Curr
Opin Neurobiol 7(4):523–529
205. Taube JS (2007) The head direction signal: origins and sensory-motor integration. Annu Rev
Neurosci 30:181–207
Sensory Substitution and the Neural Correlates …
199

206. Taube JS, Muller RU, Ranck JB (1990) Head-direction cells recorded from the
postsubiculum
in freely
moving
rats. II.
Effects
of
environmental
manipulations.
J Neurosci 10(2):436–447
207. Tcheang L, Bülthoff HH, Burgess N (2011) Visual inﬂuence on path integration in darkness
indicatesamultimodalrepresentationoflarge-scalespace.ProcNatlAcadSci108(3):1152–1157
208. Tinti C, Adenzato M, Tamietto M, Cornoldi C (2006) Visual experience is not necessary for
efﬁcient survey spatial cognition: evidence from blindness. Q J Exp Psychol 59(7):1306–1328
209. Tyler CW (2005) Varieties of synesthetic experience. In: Robertson LC, Sagiv N (eds)
210. Ungar S, Blades M, Spencer C (1993) The role of tactile maps in mobility training. Brit J Vis
Impair 11(2):59–61
211. Ungar S, Blades M, Spencer C, Morsley K (1996) The ability of visually impaired children
to locate themselves on a tactile map. J Vis Impair Blindness 90:526–535
212. Ungar S (2000) Cognitive mapping without vision. Cogn Map Past Present Future 4:221
213. Vercillo T, Burr D, Gori M (2016) Early visual deprivation severely compromises the
auditory sense of space in congenitally blind children. Dev Psychol 52(6):847
214. Viard A, Doeller CF, Hartley T, Bird CM, Burgess N (2011) Anterior hippocampus and
goal-directed spatial decision making. J Neurosci 31(12):4613–4621
215. Visell Y (2009) Tactile sensory substitution: Models for enaction in HCI. Interact Comput
21(1–2):38–53
216. Voss P, Lassonde M, Gougoux F, Fortin M, Guillemot JP, Lepore F (2004) Early-and
late-onset blind individuals show supra-normal auditory abilities in far-space. Curr Biol 14
(19):1734–1738
217. Voss P, Tabry V, Zatorre RJ (2015) Trade-off in the sound localization abilities of early
blind individuals between the horizontal and vertical planes. J Neurosci 35(15):6051–6056
218. Voss P, Zatorre RJ (2011) Occipital cortical thickness predicts performance on pitch and
musical tasks in blind individuals. Cereb Cortex bhr311
219. Vuillerme N, Hlavackova P, Franco C, Diot B, Demongeot J, Payan Y (2011) Can an
electro-tactile vestibular substitution system improve balance in patients with unilateral
vestibular loss under altered somatosensory conditions from the foot and ankle?. In: 2011
annual international conference of the IEEE Engineering in Medicine and Biology Society.
IEEE, pp 1323–1326
220. Ward J, Meijer P (2010) Visual experiences in the blind induced by an auditory sensory
substitution device. Conscious Cogn 19(1):492–500
221. Ward J, Wright T (2014) Sensory substitution as an artiﬁcially acquired synaesthesia.
Neurosci Biobehav Rev 41:26–35
222. Whishaw IQ, Mittleman G, Bunch ST, Dunnett SB (1987) Impairments in the acquisition,
retention and selection of spatial navigation strategies after medial caudate-putamen lesions
in rats. Behav Brain Res 24(2):125–138
223. White NM, McDonald RJ (2002) Multiple parallel memory systems in the brain of the rat.
Neurobiol Learn Mem 77(2):125–184
224. White BW, Saunders FA, Scadden L, Bach-Y-Rita P, Collins CC (1970) Seeing with the
skin. Percept Psychophys 7(1):23–27
225. Wiener, S. I., & Taube, J. S. (2005). Head direction cells and the neural mechanisms of
spatial orientation (bradford books). The MIT Press
226. Wolbers T, Klatzky RL, Loomis JM, Wutte MG, Giudice NA (2011) Modality-independent
coding of spatial layout in the human brain. Curr Biol 21(11):984–989
227. Wolbers T, Wiener JM, Mallot HA, Büchel C (2007) Differential recruitment of the
hippocampus, medial prefrontal cortex, and the human motion complex during path
integration in humans. J Neurosci 27(35):9408–9416
228. Wolbers T, Hegarty M (2010) What determines our navigational abilities? Trends Cogn Sci
14:138–146
229. Zamm A, Schlaug G, Eagleman DM, Loui P (2013) Pathways to seeing music: Enhanced
structural connectivity in colored-music synesthesia. NeuroImage 74:359–366
200
D.-R. Chebat et al.

Visuo-Vestibular and Somesthetic
Contributions to Spatial Navigation
in Children and Adults
Irini Giannopulu
1
Introduction
Infants come in the world with a neural system to see and perceive what is around
them, but their visual structure is poor, the most primary functions are immature.
Their visual development is characterised by critical periods in many notable visual
functions, and by extensive learning from experience and increasing control over
the visual mechanisms. Ventral and dorsal systems authorise them to transform
visual information into cognitive representations associated with object’s recogni-
tion and location in the egocentric space all along static and dynamic motion
activities. Visual areas serving dynamic motion process allow infants to harmonise
visual information with vestibular information to discern their own body position
and movement in space. To analyse rotational and translational movements, the
vestibular system is structurally composed by the semicircular canal system, which
displays angular accelerations; and the otoliths, which display linear accelerations.
The vestibular system sends signals primarily to the neural structures that command
eye movements, i.e., vestibulo–ocular reﬂex, which is required for clear vision, and
to the muscles, i.e., proprioceptive system, that commands posture, which is nec-
essary to keep infants upright. Together with the visual information, bilateral
vestibular organisation not only contribue to the equilibrioception in three axis
(sagittal, vertical and lateral) at cortical and subcortical levels, but it is directly and
indirectly involved in internal representation of the body, navigation, memory, and
orientation at cortical level.
The internal representation of the body is related to the internal representation of
the verticality (e.g., [76]). Behavioral ﬁndings seem to be coherent with neuro-
physiological data showing that vestibular, somesthetic, graviceptive afferents and
I. Giannopulu (&)
Faculties of Humanities and Social Sciences, Bond University, Gold Coast, Australia
e-mail: igiannop@bond.edu.au
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_7
201

the Bayesian probability theory established that the perception of ego motion
direction relies on integration of multiple sensory cues. All these afferents which are
interconnected in cortical and subcortical levels are transformed into representations
that contribute to the internal representation of the body. This latter can be thought
as an egocentric reference providing spatial navigation and orientation. In active or
passive locomotion a large range of sensory information can potentially provide
feedback
including
visual,
vestibular
and
somesthetic
(proprioceptive
and
exteroceptive-haptic) inputs. Balance and locomotion are highly interdependent:
balance authorises children to walk safely, locomotion improves the control of
balance (e.g., [130]). Balance and locomotion provide the ground of spatial navi-
gation towards object and locations. Visuomotor, vestibular, and somesthetic
abilities give children and adults the possibility to explore actively or passively the
space. Cognitive factors potentially inﬂuence ego motion in children and in adults.
Linear sagittal ego motion seems to be determined by cognitive and vestibular
components. More particular, the intra-individual variability of vestibular sensi-
tivity between sagittal and vertical axes would explain the intra-individual vari-
ability of the ego motion sensitivity between both axes (e.g., [47]). The
inter-individual variability of vestibular sensitivity along the vertical axis would
explain the inter-individual variability of the ego motion sensitivity along this axis.
Exclusively cognitive variables, such as the plausibility or implausibility of ego
motion is capable of modulating the ego motion sensitivity. Visual attentional
factors and memory process specially those associated with visuospatial working
memory can be considered as decisive elements of ego motion in real as well as in
virtual environments. Visuo-vestibular and somesthetic interactions together with
cognitive factors strongly contribute to the ego motion. Both bottom-up and
top-down ego processes act together in a synergic way to harmoniously perform
spatial navigation. It seems that the brain consistently weights multiple information
that is on the basis of spatial navigation. Under passive and active navigation, the
perception of ego motion is assisted by covariant signals coming from visual,
vestibular, and somesthetic systems.
Starting with the visual development, we will continue with the analysis of
vestibular system and its relationship with the visual, somesthetic (proprioceptive,
exteroceptive-haptic) systems. Then the relationship between the integration of
information and internal representation of the body and verticality will be given.
Considering the internal representation of the body as an egocentric reference
providing spatial navigation and orientation, the ego motion in children and adults
in space will be analysed. Ego motion in children and adults will be examined via
visuo-vestibular and cognitive inﬂuences. Naturalistic prerecorded and virtual
environments will be presented in studies where visually induced ego motion is
discussed in relation with attentional and visuospatial memory processes and spatial
intuition. The effect of visual information in spatial navigation in blind and
blindfolded sighted persons is also reported.
202
I. Giannopulu

2
From the Eye to the Perception of Visual Movement
The nervous system uses visual information of the environment in order to trans-
form it to perception. A big agreement is known regarding how this process takes
place within individual visual pathway. Visual development and perception do not
follow an established plan. Irregularities in the typical development progression can
have permanent repercussions on the ﬁnal degree of visual functioning. Studies
realised in humans (e.g., [6]) have explained that visual development and percep-
tion rely upon the visual inputs very early in life. The argument here is to give the
development sequences of visual system and perception.
The eye is the peripheral organ of the visual system. Its relevant parts are optical:
scleral conjectiva, iris, cornea, pupil, lens (Fig. 1a). This external architecture
fashions an adaptable optical complex to transport images at the back of the eye: the
retina (Fig. 1b). The visual system of the human infant is immature at birth. Studies
argue that this immaturity starts at the retina and more particular in the fovea (e.g.,
[1]). In other words, young infants are inept at different performances such as eye
movements (e.g., [7]), accommodation and convergence (e.g., [9]).
Iris
Pupil
Scleral Conjunctiva
Cornea
Lens
Retina
Vitreous Humor
Fovea
Optic Nerve
(a) external anatomy
(b) internal anatomy
Fig. 1 Anatomy of human eye (a and b)
Visuo-Vestibular and Somesthetic Contributions …
203

Studies have reported that the ability to focus an object in order to see its image
depends on brain maturity. Under one month, young infants have a very bad
accommodation. At four months they became good accommodators and they can
correctly see objects at various distances (e.g., [9]). Functional vision also depends
on how the eyes are jointly placed. Hainline and Riddell [61] put forward an inter
depended relation between accommodation and vergence. Young infants who do
not show evidence of varying accommodation to targets at various distances, they
do not show any evidence of convergence. It has also been reported that around
four months, the convergence of young infants is almost good (e.g., [84]). For both
accommodation and convergence, binocular fusion is indispensable (e.g., [63]) as
depends on having the ability to place (point) the two eyes accordingly when
looking at objects. Following the hypothesis of Hainline [63], such ability develops
early on in such a manner that as ﬁne acuity matures, the eyes maintain to be in
alignment with the extent of precision compatible with the level of spatial vision.
Spatial vision itself cannot be considered without the oculomotor development.
It is of evidence that functional vision is inﬂuenced of eye movement: saccades
and ﬁxations. The most rapid eye movements, i.e., saccades, serve to point the
fovea at objects that must be examined. As fovea of young infants is immature,
saccadic behavior is immature as well. Surprising evidence exists that the maximum
velocity and the amplitude of a saccade are very similar for infants and adults (e.g.,
[60]). Even if a saccade depends on object’s size and nature, i.e., natural or virtual,
the saccadic system appears quite mature and ready to function at early age.
A saccade is commonly accompanied by a ﬁxation. During ﬁxation the eyes are
grasped more or less immobile in order to examine the object. Infant ﬁxation time
seems to be related to the quantity of available visual details. Many details necessitate
briefer ﬁxations that few details. The maintenance of stable ﬁxation is difﬁcult for
young infants, as if the visual feedback about a stimulus moving rapidly across the
retina leads to a reﬁxation (e.g., [62]). Small objects are better reﬁxed by young
children than big objects. Given the above, it seems that infants possess sufﬁcient
oculomotor activity and control to provide careful investigation of static visual scenes.
The retina is placed in the back of the moving eye that is located in a moving
head. To keep an image stable on the retinal fovea, the eye must rotate smoothly.
Smooth pursuit is the speciﬁc ability that stabilises the moving image of a given
object. Immature fovea showed poorly developed smooth pursuit (e.g., [120]).
Studies made evident a smooth pursuit for salient targets at slow speeds (e.g.,
[132]). With the speed increases, infants use adaptive saccades in addition with
smooth pursuit (e.g., [62]). The oculomotor system linked to the vestibular system,
optokinetic nystagmus (OKN) and vestibulo–ocular reﬂex (VOR) are participating
to compensate the movements of the organism thought the world and some of the
associated movements of the retinal image (e.g., [34]). OKN is produced when a
large portion of the visual ﬁeld’s retinal image shifts cruise the retina. The two eyes
move as a unit following two alternative phases: slow phase and fast phase. In that
case, the slipping of the image in the retina is controlled. Even if oculomotor
asymmetries are presented when OKN is triggered from one eye at the time, infants
show well-developed OKN from birth. OKN is quite important in its complex
204
I. Giannopulu

relationship with vestibular stabilisation. VOR is that system that maintains the
eye’s gaze direction in spite of head rotation. VOR and OKN compensate for head
changes: the VOR gives the initial ocular stabilisation, the OKN adds any necessary
duration components. Both systems compensate better image sliding generated by
head rotation or body movement in space. The VOR is also important in the
maintenance of posture during body movement. The VOR achieves adult level over
the ﬁrst 5–6 years.
It is rather evident that the visual mechanism is a complex neural network. The
visual system intervening on the perception of objects, i.e., ventral system, is
considered to be separate from the visual system intervening on the location and
action directed at the perceived objects, i.e., dorsal system. Both systems come into
existence with information contained in the pattern of activity in the cells of the
retina caused by the light that occurs in that part of the eye. The ventral perceptual
system projects from V1 through a set of cortico-cortical projections such as ventral
temporal and occipital structures to anterior temporal cortex, i.e., the inferotemporal
cortex. This system affords detailed representations of the world needed for cog-
nitive operations like recognition and identiﬁcation. The dorsal action system
projects from V1 to posterior parietal structures affords an adaptable control of
subcortical visuomotor areas. The dorsal system provides representations of the
localised objects in the visual space (Fig. 2). Even if we put greater emphasis on
differences of the ventral perceptual and dorsal action systems, both systems pro-
cess information about object characteristics, such as size, orientation, shape, and
shadow and both dispose information of spatial location. Transformations accom-
plished in the ventral system authorise the formation of perceptual and cognitive
representations that enclose the continuing ingredients of objects and their spatial
relations with each other. Transformations achieved in the dorsal system utilise
instantaneous object characteristics that are constructed within egocentric format of
reference to mediate representations related to the control of goal-directed actions.
The developmental trajectory of the two systems provides developmental erratic
results. Studies comparing directly the developmental stages of both visual systems
have revealed that functions mediated by the ventral system mature earlier than
those mediated by the dorsal system (e.g., [100]). Young infants appear capable to
perform any actions that are regulated by perceptual information (e.g., [22]). The
developmental stage at which the specialisation of both systems becomes similar to
adult have given uncertain ﬁndings. Some studies claimed that even if the two
different neural systems are developing within the ﬁrst year of life (e.g., [80]), both
systems become specialised to treat different aspects of the same object as early as
5 years of age even at the very basic level of visual coding (e.g., [8]).
Indeed, many visual capabilities such as perception of colors, depth, and per-
ception of motion start to develop soon after birth and continue to mature as infants
experience the world around them. Using functional magnetic resonance imaging
(fMRI) to record brain activity in awake 7-week-old, Biagi et al. [20] revealed early
maturation of the cortical areas including MT+, V6 as well as the vestibular cortical
area PIVC/PIC for motion process. The comparison with adult’s brain activity gave
Visuo-Vestibular and Somesthetic Contributions …
205

several similarities of MT+ areas both in the left and the right hemispheres and
differences in V1 area as well as in PIVC/PIC area.
Brieﬂy, infants come in the world with a neural system to see and perceive what
is around them, but their visual structure is poor, the most primary functions are
immature. Their visual development is characterised by critical periods in many
notable visual functions, and by extensive learning from experience and increasing
control over the visual mechanisms. Ventral and dorsal systems authorise infants to
transform visual information into cognitive representations associated with object’s
recognition and location in the egocentric space all along static and dynamic motion
activities. Visual areas serving dynamic motion process allow infants to harmonise
visual motion with vestibular information to discern their own body position and
movement in space.
3
Peripheral and Central Vestibular Organisation
The vestibular system, or labyrinth, is one of the most ancient sensory system. It
consists of sense organs in the cavities of the inner ear. It allows vertebrates identify
and adjust the position of their body and their movement in space.
Using scanning electron microscopy vestibular receptors were well identiﬁed. At
8 weeks all receptors were individualised and differentiated. Conversions in cristae
shapes take place in rapid steps with a pause in the maturation process between the
Retina
 Lateral Geniculate Nucleus
dorsal
system
ventral
system
Infero-Temporal Cortex
Parietal Cortex
movement
color and/or form
MT/v5
v4
v2 Secondary visual cortex
v3
v1 Secondary visual cortex
Fig. 2 From retina to dorsal and ventral systems
206
I. Giannopulu

9th and 12th weeks. The utricles develop earlier than the cristae. Indeed at
14 weeks, all the receptors had an adult proﬁle with the exception of cristea, which
have only approached 55% of their adult size (e.g., [35]). The peripheral vestibular
apparatus develops earlier than the central nervous apparatus.
The peripheral apparatus is composed by the semicircular canals and the otoliths
organs. Both are completely similar to all the vertebrate phyla. Many studies have
differentiated between static or head position receptors, i.e., the otoliths organs and
dynamic or head movement receptors, i.e., the semicircular canals. The semicircular
canals report the magnitude and direction of angular motion, however it is pro-
duced, the otoliths report the magnitude and the direction of linear motion (e.g.,
[106]). Both organs are therefore dynamic and static receptors. It is generally
accepted that angular acceleration will have some effect on the utricles and linear
acceleration will have some effect in angular acceleration. There are anatomical
arrangements of the semicircular canals and cochlea that are suspended in the
perilymph. Inside, the organs share a common cavity ﬁlled with the endolymph.
Labyrinthine semicircular exteroceptors share a common cavity in the
utricule-canal system with the saccule and the saccule with the cochlea (e.g., [70]).
There are three types of canals: the horizontal, the anterior vertical, and posterior
vertical (Fig. 3a). When a person holds the head in a natural posture, the horizontal
canals are almost parallel to the earth-horizontal plane. When the head is in a
natural position the posterior canal lies in an almost vertical plane which forms an
angle of 45° to the frontal plane of the head and the anterior canal lies in a plane
inclined at an angle of about 15° to the vertical which also forms an angle of 45° to
the frontal plane. Each of the three canals has a contralateral pair that forms a
functional unity, a synergistic pair. In that context, the two horizontal canals, the
left anterior and the right posterior canals and the left posterior and the right anterior
canals form synergistic pairs. Following the hydrodynamic theory of the canals, the
movements of the endolymph are important in order to analyse the angular motion.
The microstructure of the sensory epithelium of the cristae comprises two types of
sensory hair cell: type I and type II. Type I cells are enclosed in a goblet-shaped
nerve chalice innervated predominantly by the larger diameter ﬁbers in the cristal
nerve. They are excited by ipsilateral acceleration. Type II cells are cylindrical and
innervated by small diameter nerve ﬁbers. They are excited by contralateral
acceleration. Both types of cells are also innervated by efferent neurones. Each
sensory cell is composed by sterocilia and one kinocilium. The increment of the
resting discharge is produced during depolarisation, i.e., the inclination of the
sterocilia is in the direction of the kinocilium, the decrement during hyperpolari-
sation, i.e., the inclination of the sterocilia is in the opposite direction of the
kinocilium (Fig. 3b).
Labyrinthine otolithes exteroceptors, i.e., utricule and saccule are cavities at the
junction of the three semicircular canals. Each cavity is ﬁlled with endolymph and
contains a sensory epithelium, the macula, which responds to the magnitude and
direction of linear acceleration. Their mode of functioning is remarkably constant.
The utricular macula is relatively in the same plane as the horizontal canal even
through it twists upwards at its anterior end. The saccular macula lies on the medal
Visuo-Vestibular and Somesthetic Contributions …
207

wall and is tilted inwards from the vertical by at least 20°. The outer part of each
macula is concave. The cellular structure of the macula is composed by 33,000 hair
cells in the utricule and 19,000 in the saccule. Each cell has about 70 stereocilia and
one kinocilium. The membrane potential is maximally depolarised when a force
shares the stereocilia in the direction of kinocilium, i.e., increases the resting dis-
charge rate and is hyperpolarised when a force acts in the opposite direction, i.e.,
decreases the resting discharge. The geometry of the otolith organs is complex.
Indeed, utricular macula and saccular macula are a multidirectional detector with
several outputs, each of which increases or decreases according to the linear vector
along its polarisation axis.
The inﬂuence of angular and linear accelerations expands all over the central
nervous
system
to
involve
brain
functions
related
to
visual
perception,
somatosensation, movement, and memory. A possible consequence of that is the
(a) Otoliths and semicircular canals
       Type I Hair Cells
Type II Hair Cells
   - depolarisation                                                                -hyperpolarisation
(b) Hydrodynamic theory of the canals
Utricle
Saccule
Utricle
Saccule
Horizontal canal
Horizontal canal
Anterior canal
Anterior canal
Posterior canal
Posterior canal
Fig. 3 Functions of vestibular organisation (a and b)
208
I. Giannopulu

fact that the physiological activity of vestibular apparatus inﬂuences the activity of
central as well as peripheral nervous system (e.g., [70]). Indeed, the peripheral
vestibular system provides the central vestibular system with information con-
cerning the movement of the body and its position in space.
The central system uses this information along with visual and somesthetic
information to create the associated representations. The central vestibular system
path involves the vestibular nerve projections from the peripheral vestibular organs
to the vestibular nucleus in the brainstem but also the projections from the brain-
stem to cerebellum and ﬁnally the cord spinal and thalamic nuclei and from the
thalamus to the cerebral cortex (e.g., [137]). A lot of neuroimaging studies specially
those used functional magnetic resonance imagery (fMRI) were utilised to analyse
vestibular processing in humans. Using speciﬁc head postures, it has been revealed
that peripheral vestibular stimulation causes activations within the thalamus via the
vestibular nuclei (e.g., [82]). There is some evidence that vestibulothalamic circuits
would form specialised tracks that incorporate vestibular signals with other
modality-speciﬁc information within the thalamus (e.g., [95]). Ventrobasal nuclei
receive vestibular projections from bilateral superior vestibular nucleus but also
contralateral medial vestibular nucleus via the medial longitudinal fascicles (e.g.,
[98]). Ventral posteromedial neurons simulated linear and circular movements (e.g.,
[97]). All these nuclei project to vestibular cortical regions as anterior suprasylvian
cortex, and the intraparietal sulcus (e.g., [26]). In addition, Deecke et al. [37] have
reported evidence that some posterior thalamic nuclei are also activated during
somesthetic cutaneous stimulation. They project directly to primary somesthetic
cortex, to secondary association somesthetic cortex (e.g., [36]) as well as to pos-
terior parietal cortex (e.g., [99]). Moreover, the ventral anterior and lateral nuclei
project to primary motor and premotor cortices, i.e., 4 and 6 Brodmann’s areas
respectively (e.g., [137]). A meta-analysis on 16 studies has reported that different
cortical areas such as Sylvian ﬁssure, insula, retroinsular cortex, frontoparietal
operculum, superior temporal gyrus as well as the cingulate cortex are associated
with the vestibular cortex (e.g., [96]).
Associated with cortical and subcortical functioning, the anatomical organisation
of the vestibular system is rather bilateral than unilateral (e.g., [25]). The central
vestibular system joins together ipsilateral and contralateral sensory tracks from the
vestibular nuclei to the midbrain tegmentum, the thalamus, and the cortex. Another
part of the system is the hippocampus and the parahippocampus where vestibular
information is processed.
In sum, the vestibular system matures early. It is represented by the labyrinth of
the inner ear bilaterally in most mammals. To analyse rotational and translational
movements, the vestibular system is structurally composed by the semicircular
canal system, which displays angular accelerations; and the otoliths, which display
linear accelerations. The vestibular system sends signals primarily to the neural
structures that command eye movements, i.e., vestibulo–ocular reﬂex, which is
required for clear vision, and to the muscles, i.e., proprioceptive system, that
command posture, which is necessary to keep infants upright. Together with the
visual and somesthetic information, bilateral vestibular organisation not only
Visuo-Vestibular and Somesthetic Contributions …
209

contributes to the equilibrioception in three axis (sagittal, vertical, and lateral) at
subcortical level, but it is directly and indirectly involved in internal representation
of the body, navigation, memory and orientation at cortical level.
4
Internal Representation of the Body
To perceive our own orientation in space, the brain needs to compute and con-
tinuously update spatial information which is the base of the internal representation
of the body. This internal representation is a dynamic process, enabling conscious
and unconscious perception. Dynamic in nature, the vestibular system is crucial
(e.g., [4]). The existence of two interdependent somesthetic graviceptive systems in
humans has been established, aside from the otoliths of the vestibular system:
the vascular graviceptive system, and the kidneys (truncal graviceptive system)
(e.g., [101]).
The internal representation and orientation of the body in space have been amply
studied in humans. Experiments have been carried out through apparent postural
vertical or apparent upright (e.g., [13]) on the roll or sagittal X-axis. Postural
vertical was most often studied with this roll or sagittal X-axis, although some
experiments were also reported about postural vertical with the pitch or vertical Y-
axis (e.g., [21]). Postural horizontal has less often been examined with the sagittal
X-axis (e.g., [59]) and with the Y-axis (e.g., [79]). Only one study has compared
the internal representations of the body via a task of self-controlled whole-
body orienting about roll (X-axis) and pitch (Y-axis) rotation axes (e.g., [50, 76])
(Fig. 4).
In that study, participants were seated on a rotating chair. They were required to
position themselves horizontally (=90°) from a different initial tilt of body, in
complete darkness, and thereafter vertically (=0°). The accuracy of body orientation
was measured and compared between both directions for X- and Y-axes. It was
expected that the internal representation of body orientation would be different
between these two axes, mainly because of the seated posture (often no experience
of seated roll rotations) of the participants. It has been shown that the internal
representation of body orientation is different according to these two rotation axes.
It seems that there was a systematic and symmetrical difference between the prone
and supine sides on the Y-axis. More precisely, the participants turned too much on
the prone side (ca. 11°), and not enough on the supine side (also about 11°), when
trying to align along the earth-horizontal on this pitch-axis. In the contrary, there
was no side effect on the X-axis (Fig. 5). This seems to conﬁrm a basic difference
between both axes, which relies most probably on the internal representation of
body orientation.
Guedry [59] described a strong effect of reduced somesthetic cues on subjective
postural horizontal and vertical. Indeed, with studies during water immersion, it had
been suggested that the utricles do not provide good static indication of orientation
(e.g., [79]). It was also noted that 10° up or down from horizontal seems to be a
210
I. Giannopulu

range within which the otoliths are relatively insensitive in signaling head orien-
tation relative to the gravitational force vector (e.g., [86]). When participants
attempted to set themselves to the upright through X-axis when seated on a chair, all
reported using pressure cues (e.g., [65]). A dominance of proprioceptive input over
vestibular input has been shown through differential rotation of the feet and/or the
trunk about the vertical Z-axis (e.g., [67]). The authors concluded that, perceptually,
Z-axis
Y-axis
pitch
yaw
back
up
down
left
right
roll
front
Fig. 4 Three axes of the body (X-sagittal, Z-vertical, Y-lateral)
Fig. 5 Subjective horizontal and vertical on the X-axis and Y-axis (from [76])
Visuo-Vestibular and Somesthetic Contributions …
211

the vestibular information is linked to the foot support body representation through
proprioception. They further suggested that body position perception is built
bottom-up with the feet related to space coordinates (vestibularly derived), the trunk
related to the feet and the head to the trunk. Perception would use vestibular input to
control for stationarity of external references and if true, would rely on these instead
of the vestibular derived space reference. It had been also suggested that sensory
information from vestibular system and from somesthetic afferents in feet and
ankles play an important role in determining the availability of postural strategies
(e.g., [86]). The inﬂuence of touch and pressure cues on apparent orientation during
“barbecue” rotation has clearly been demonstrated (e.g., [129]).
In the study presented above, (e.g., [76]) participants were ﬁrmly embedded in
deﬂated pillows, so that somesthetic cues, i.e., touch and pressure cues, were
reduced. The forward tilted posture that has been observed represents probably a
kind of “default” reaction to keep balance in critical situations. It is probably due to
the
proprioceptive
backward-tilted
referential
of
verticality
(e.g.,
[10]).
Furthermore, Barra et al. [12] revealed the existence of a synthesis of vestibular and
somaesthetic graviception for which the posterolateral thalamus plays a major role,
corresponding to a primary property of internal models and yielding the neural
bases of the Aubert effect. They concluded that humans construct and update
internal models of verticality in which somesthetic information plays an important
role. The participants behaved as if they could “see the vertical”. Subjective pos-
tural horizontal and vertical rely on mental imagery and on the internal computation
of an head-foot line, both involving an internal representational system (e.g., [109]),
i.e., an internal body representation providing a basis for conscious and uncon-
scious perception.
With the Y-axis, subjective horizontal was about 11° too high or too low.
Consider a subject with 80 cm ear-hip and 50 cm knee-feet length, and with 30 cm
hip-knee. The angle of the ear-feet line would then be arctan (30/130) = 13°, rather
close from the 11° found. This suggests that participants adjusted the orientation not
of their trunk, but of a “virtual line” joining head and feet. This “virtual line”
materialises the internal representation of the body. It is based on vestibular,
somesthetic, and vascular graviceptive afferents. When reaching subjective vertical,
whereas this head-foot line assumption does not strictly hold anymore, participants
still seemed to be attracted toward it on the forward direction and front side. The
trunk axis weight is probably increased due to its real weight, now parallel to the
gravity vector.
In other words, the internal representation of the body is related to the internal
representation of the verticality. The participants behaved as if they could “see the
vertical”, from supine, and therefore they stopped rotating right on it. Rotation not
only “reorients” the canal and otolith information relative to gravity but also
transforms somesthetic graviception. Both receptors seem to be interdependent in
order to lead to a robust judgement of vertical postural sense. These behavioral
212
I. Giannopulu

ﬁndings seem to be coherent with neurophysiological data showing that vestibular,
somesthetic graviceptive afferents and the Bayesian probability theory established
that the perception of ego motion direction relies on integration of multiple sensory
cues. All these afferents which are interconnected in cortical and subcortical levels
[26] are transformed into representations that contribute to the internal represen-
tation of the body. The internal representation of the body can be thought as an
egocentric reference providing spatial representation and orientation.
5
Spatial Representation
As explained previously, in the brain not only visual but also multiple vestibular
and somesthetic information is crucial for the performance of navigation. One of the
indicators of spatial behavior is the capability to pursue a moving spot with hand
and eye movements. Such an ability supports visual and vestibular anticipation, at
least. It appears early in development. It seems that infants aged 6 and 14 weeks are
able to discriminate between motion directions and pursue small visual objects
(e.g., [113]). Even better, neonates as young as 3-days old seem to be able to
display visual, vestibular, and proprioceptive responses through head movements
when exposed to visual stimulation (e.g., [123]). Infants seem also to be able to
continuously produce smooth pursuit eye movement at 2 months old (e.g., [135])
and discriminate between different kind of visual motion, i.e., real or virtual, at 3
and 5 months old (e.g., [53]). Jonsson and von Hofsten [81] have demonstrated that
at 6 months of age healthy infants can produce predictive head and eye movements
and anticipate the ﬁnal position of a visual object.
Using EEG in 8-month-old infants and adults, studies have reported that these is
a difference in brain activities related to the processing of optokinetic information.
Infants have shown longer latencies than adults for random visual motion (e.g.,
[130]). No walking infants aged 4–5 and 8–10 months detect visual motion less
efﬁciently compared to adults (e.g., [131]). Older infants seem have a more
developed neurobiological system that permits them to use the vision motion
information like adults. It is well documented that the cortical and subcortical areas
related to visual motion perception continue to develop through infancy to adult-
hood (e.g., [54]). The more efﬁcient processing of motion is related to the old than
to the young infants. Locomotion experience inﬂuences changes in brain activity
and vice versa (e.g., [92]). Agyei et al. [2] investigated the effect of visual motion
perception in prelocomotor infants aged 3–4 months and locomotor infants aged
11–12 months. Only locomotor infants have rapid and compatible reactions with
forward visual motion information. Furthermore, crawling experience is relevant
(e.g., [3]).
Anticipatory and compensatory postural activities have been found to be asso-
ciated to visual mobile information in infants between 10 and 17 months (e.g.,
Visuo-Vestibular and Somesthetic Contributions …
213

[88]). Such activity is progressively developed with age as infants learn to stand and
walk (e.g., [118]). It helps them to keep balance during locomotion. Cignetti et al.
[33] have reported that the acquisition of locomotion enhances sensorimotor con-
trol of posture. Peripheral visual information is more effective and goes hand
in hand with balance and self-locomotor experiences in typical children
(e.g., [130]).
The balance structure authorises children to keep up equilibrium by incorpo-
rating internal and external information during static and dynamic situations.
Balance development starts with the maintenance of balance. Being able to remain
upright allows children for touching, grasping, and reaching. The method
of “swinging room” developed by [91] has been widely used to investigate the
stability responses of infants and children. Follow the principle, children stand in a
platform surrounding by a room. The walls and the ceiling of the room move back
and forth independently of the ﬂoor that is ﬁxed. This movement provokes children
to sway with the direction of the room. Forward movements of the room cause
backward movement to the children that is thought to be a way to compensate the
loss of balance.
Sway responses seem to be present very early in life. At 5 months of age, infants
sway in synchrony with the room when sitting (e.g., [19]). The degree of sway is
high for infants with crawling or self-locomotion experience only (e.g., [64]). New
locomotors can bumble or fall over. The same behavior is also observed on children
aged 3–4 years old (e.g., [134]). The inﬂuence of visual information to balance
decreases between 4 and 6 years old (e.g., [55]). It seems that the gain of sway
when submitted to a visual information produced by a swinging room is higher in
children aged 7–14 years than in adults (e.g., [125]). Children body sway reaches
adult level by 10 years of age (e.g., [112]). When vestibular and proprioceptive
information are considered only, signiﬁcant sway responses are observed in healthy
children older than 4 years. Haptic (e.g., [11]) as well as proprioceptive sources of
information are sufﬁcient to induce sway (e.g., [124]). It seems that children aged
4–6 years old are more destabilised than adults during sensory conﬂict: proprio-
ceptive system signiﬁes no movement but vision or vestibular system signiﬁes
movement. In other words, the aforementioned studies suggest an early reliance on
visual information. It appears that the end of this period and transition emerges
around 5 years (e.g., [107]).
In sum, in active or passive locomotion a large range of sensory information can
potentially provide feedback including visual, vestibular, and somesthetic (propri-
oceptive and haptic) inputs. Balance and locomotion are highly interdependent:
balance authorises children to walk safely, locomotion improves the control of
balance. Balance and locomotion provide the ground of spatial navigation toward
objects and locations. Visuomotor, vestibular, and somesthetic abilities give chil-
dren and adults the possibility to explore actively or passively the space. Cognitive
factors inﬂuence ego motion in children and in adults.
214
I. Giannopulu

6
Linear Sagittal Ego-Motion Perception
Ego motion is described as an embodied sensation of movement in space than can
be experienced by an observer when s/he is submitted to appropriate visual inputs
(e.g., [51]). Similar movements can be induced by other sensory inputs such as
auditory, tactile information, and muscular proprioception (e.g., [85]). Ego motion
can be global, i.e., involving the whole body, or segmentary, i.e., involving some
parts of the body (e.g., [127]). Global ego motion is described with respect to the
three mutually orthogonal axes of an upright observer: the sagittal X-axis, the lateral
Y-axis, and the vertical Z-axis. Ego motion rotation around one of these axes is
named roll (around the X-axis), pitch (around the Y-axis), and yaw (around the Z-
axis). Accordingly, ego motion translation along one of these axes are labeled
sagittal, lateral, and vertical (along the X-axis, Y-axis, and Z-axis respectively).
Studies on ego motion have been conducted from various standpoints. However,
during the past two decades, a growing tendency has emerged for such motion to be
studied less and less as a basic sensory process and more and more as a complex
integrated perception (e.g., [114]). Only marginal attention has been paid to the
possible involvement of cognition in ego motion. Various data allow one to con-
sider contradictory hypotheses.
From one hand it is suggested that ego motion is independent of cognition.
Several examples support this position. Stationary observers bilaterally exposed to a
backward sagittal visual scene, experimental or naturalistic, experience ego motion
even if they are facing a physical obstacle, i.e., a wall. This means that the
observer’s knowledge of a physical obstacle to a real forward body displacement
does not prevent the onset of a forward ego motion (e.g., [23]). Analogously, the
segmentary extension of lesion or the forearm either vibratory (e.g., [56]) or
visually induced (e.g., [127]), has been felt even when the observer know that their
forearm is restrained or physically immobilised by an arm holder. The above data
suggest that ego motion is independent of cognition.
From the other hand some data lead us to hypothesise that ego motion might be
subject to cognitive modulation. In some studies it appears that the same infor-
mation that can induce ego motion may be modulated or differentially interpreted
according to the context. Young and Shelhamer [143] exposed adults to a dome
rotating around the X axis of the head. In one condition, called “free-ﬂoating”, the
observer was only restrained by a custom-ﬁt bite board. In the other, named
“tactile”, the observer restrained by the bite board was also held on the ﬂoor surface
by a stretcher elastic from a shoulder harness. The observers have reported much
stronger ego motion in the “free-ﬂoating” than in the “tactile” condition. The
interpretation of such a situation is strongly attached to the existence of cognitive
factors associated to a speciﬁc knowledge of body orientation. In Baumberger’s
study [15], children aged 8, 10, and 12 years old and adults had to aim at the
memorised location of a stationary target in either a mobile or an immobile visual
environment. In all population aiming was much more imprecise in the visually
mobile environment than in immobile one. Moreover, when aiming was under the
Visuo-Vestibular and Somesthetic Contributions …
215

visually mobile condition, the aiming errors were more important in children than in
adults and more important in 8 years than in 12 years of age. The errors were
attributed to the ego motion in all the groups. All the errors were quite remarkable
as all the participants have aimed as if they have really changed their body position
in space along the sagittal axis, i.e., X axis.
Direct cognitive effects on linear forward sagittal ego motion in children have
been investigated when they were exposed to a visual experimental scene [89].
Children aged 7 and 11 years old were in the same physical position, i.e., seated in
a stationary armchair, but in two contrasted cognitive conditions. In one condition,
the children were informed that the stationary armchair could move, in the other
that it could not move. Even if the probability to obtaining ego motion it was not
affected by the knowledge about the possibility of a physical displacement, the
results have shown that ego motion was more rapidly obtained when the dis-
placement was known to be possible than when it was know to be impossible
(Fig. 6). Everything happened as if the cognitive preparation has changed the
observer’s response criterion. It seems that the cognitive preparation have had some
top-down effect on the sensory integration sites, where visual information is pro-
cessed as specifying body movement. The absence of effects on ego motion
occurrence and the presence of an effect on timing for reported ego motion can
theoretically be accounted for in the context of visuo-vestibular interaction.
Based on the above study, Shirai et al. [122] have studied the development of
linear ego motion by comparing children aged 9 and 14 years old with adults aged
21 and 22 years old. Considering the sensory level only, their results are consistent
with the fact that children are more sensitive to ego motion than adults suggesting
that there is a difference in the level of perceptual engagement between both groups.
0
3
5
8
10
13
7-year-old
11-year-old
possible
impossible
onset time (sec) 
Fig. 6 Median self-motion onset time for each cognitive condition within each age group
216
I. Giannopulu

Similar ﬁndings have observed in children aged 7 years old (e.g., [14]). In that
context, linear forward ego motion behavior is thought to be associated to chil-
dren’s immature capability to use vestibular and/or somesthetic inputs, i.e., non-
visual, to treat ego motion (e.g., [121]).
Linear forward sagittal ego motion has been also analysed in healthy adults aged
26 years old via visual strategies. The aim of the study was to compare visuomotor
and attentional strategies of individuals when exposed to two naturalistic visual
environments: a real trafﬁc urban scenario prerecorded on video; and a virtual
scenario, i.e., the 3D simulation of the same scene that represents a district of the
center of Paris, between the Louvre and the Opera (e.g., [49]) (Fig. 7). Participants’
eye movements (amplitude and latency) and ﬁxations (number and time) were
recorded using a binocular eye tracking system (Eyelink II). The number of ﬁxa-
tions was higher in the prerecorded than in 3D simulated environment, which would
mirror participants’ need to renew their perception. This latter behavior is a classic
one in real situations (e.g., [52]). Analogously, the ﬁxation time was higher in the
prerecorded environment than in virtual one explaining a more attentional
engagement in the former environment than in the recent one. Saccades’ amplitude
and latency were more important in the prerecorded than in the virtual environment.
It is well known that eye movements are strongly linked to visual attention: the
more eye movement the more the attention (e.g., [66]). As the prerecorded envi-
ronment is richer in visuospatial information than the virtual one, making more
ﬁxations and saccades implies higher neurocognitive functions. In other words, the
exposition of participants to a prerecorded environment would necessitate more
cerebral activity than to the virtual environment. This is also corroborated with the
declaration of participants “prerecorded environment are more rich than virtual
environments.” Information recall is a genuine memory process, which constrains
retention in visuospatial working memory. Different explanations, such as inte-
gration in saccade memory buffer or the fact that perception is renewed with each
Fig. 7 Naturalistic prerecorded (a) and virtual (b) environments: Louvre-Rivoli base (from [50])
Visuo-Vestibular and Somesthetic Contributions …
217

ﬁxation and saccades are coherent with that. Neuroimaging data also suggest the
presence of common cortical areas for attention and memory processes (e.g., [117]).
A more recent study has investigated the cognitive and physiological processes
associated with forward sagittal ego motion and spatial intuition in healthy adults
20 years old (e.g., [45]). An experimental group and a control group have been
considered. All the participants of the experimental group were immersed in the
virtual naturalist environment after having been immersed in a real one containing
visual and olfactory information correlated with the experimental task: name a
preselected place in the naturalistic environment. All the participants of the control
group have been immersed in the virtual environment after having been immersed
in a neutral real environment. Somesthetic electrodermal activity and verbal dec-
laration have been recorded. The median electrodermal activity as well as the
coefﬁcient of variation of the electrodermal activity were more important for the
experimental group than for the control group. In addition, the comparable con-
ﬁguration of results is observed for the verbal declaration between both groups.
A continuity between somesthetic and cognitive factors would be on the basis of
spatial intuition in ego motion. In other words, in real and virtual environments
although vision is the most characteristic, other information are signiﬁcant:
vestibular, and somesthetic. In the following part, the visuo-vestibular and
somesthetic contribution to ego motion will be analysed in sagittal as well as in
vertical ego motion condition.
7
Linear Sagittal and Vertical Ego Motion Perception
Linear and circular ego motion perception is a visuo-vestibular phenomenon
basically (e.g., [136]). The visually induced ego motion is characterised by three
main perceptual phases. At the beginning of the visual motion (peripheral or cen-
tral), an observer perceives her/himself as stationary and the visual environment as
mobile. In a second phase, the observer begins to perceive the mobile visual
environment as moving more slowly and her/himself as gradually moving in the
opposite direction. Finally, in a third phase, the participant receives ego motion
only and the visual environment looks stationary (e.g., [51]). The transition from a
perceived self-stationarity to a perceived ego motion is theoretically framed as
resulting from a conﬂictual interaction between the visual and the vestibular system
(e.g., [40]). Naturally, for an observer exposed to a mobile scene, the vestibular
inputs indicate self-stationarity while visual inputs indicate ego motion. In their
model, Zacharias and Young [144] posed that the conﬂict is the difference between
two signals: one vestibular and one visual. The vestibular signal indicates
self-stationarity on the basis of the resting activity of the vestibular system (e.g.,
[18]). The other is a visually generated reference signal. This latter is assumed to be
the vestibular signal, which would be expected if the visual movement was due to a
veridical movement of the observer in space. When suddenly exposed to a constant
velocity visual scene, a stationary observer is in the middle of a conﬂict at the
218
I. Giannopulu

movement start and this conﬂict disappears later on as the constant-speed
visual mobile scene continues. Since the reference signal is supposed to correctly
represent the vestibular dynamics associated to Zacharias and Young’s model,
different characteristics of ego motion should be related to vestibular sensitivity
(e.g., [68]).
Converging neuroanatomical studies carried out that various cortical areas are
specialised in visuo-vestibular afferents in humans associating frontal, temporal,
parietal, and prefrontal cortex (e.g., [24]). Some others have shown that the tem-
poroparietal cortex would selectively be involved in the treatment of visual and
vestibular interaction. Vestibular projection area is rather associated with the neural
activity of the occipito-temporo-parietal cortex (e.g., [16]) and is involved in the
complex representations of egocentric perception of the body in space in both static
and dynamic conditions (e.g., [47]).
Several characteristics of ego motion have been found to depend on the mag-
nitude of conﬂict between visual and vestibular inputs. It has been shown that pitch
ego motion is generally accompanied by a limited tilt of the body in
vestibular-healthy observers on earth (e.g., [71]). The body tilting is thought to be
due to the conﬂict between visual information, vestibular otolith information and
somesthetic receptors (e.g., [41]). It increases markedly when the vestibular affer-
ents are reduced by lateral head tilting in healthy observers on earth (e.g., [142]) or
suppression of graviceptive information in weightlessness (e.g., [30]). Patients with
bilateral labyrinthine defects may experience not limited tilt but complete body
rotation (e.g., [31]). Similarly, the feeling of ego motion is asymmetrical in patients
with unilateral Manière’s disease while it is symmetrical in healthy observers (e.g.,
[139]). In addition, in vestibular-healthy participants, ego motion is more important
in weightlessness than on earth (e.g., [143]). The aforementioned data suggest that
the provoked decrease in vestibular inputs in weightlessness or the invoked
decrease in vestibular sensitivity in vestibular patients would both have facilitating
effects on ego motion because of the reduction in the visual–vestibular interaction
(e.g., [47]). In other words, ego motion seems to be facilitated by the reduction of
visuo-vestibular conﬂict.
Physiological data suggest that typical human vestibular sensitivity varies
between sagittal and vertical axes. As analysed previously (cf. peripheral and
central vestibular systems), the otolithic system is functionally adapted to transduce
the magnitude and direction of linear acceleration (e.g., [17]). It comprises the
utricule and saccule that transduce horizontal accelerations and vertical accelera-
tions respectively. Studies have shown that utricule has a greater sensitivity than the
saccule in normally positioned head (e.g., [128]). With that in mind, it is possible
that the visuo-–vestibular interaction differs between sagittal and vertical axes.
Sagittal ego motion is less easier to be induced than the vertical ego motion in
healthy observer (e.g., [47]).
The results suggest that the normal intra-individual variation of the human
vestibular sensitivity between head axes could account for the normal variation of the
ego motion between head axes [e.g., 47]. This would signify that the normal variation
of human vestibular sensitivity in the terrestrial environment would modulate the
Visuo-Vestibular and Somesthetic Contributions …
219

magnitude of the visual–vestibular interaction and ego motion. A research has
attemptedto expand this rationale to the inter-individual level.Both vestibular and ego
motion sensitivities are known to be inter-individually variable among healthy par-
ticipants. In that context, vestibular and ego motion sensitivities are negatively cor-
related: the lower the vestibular sensitivity, the higher the ego motion sensitivity. This
correlation may have two meanings. On the one hand, it may be that ego motion and
vestibular sensitivities are intrinsically related. This would suggest that ego motion
sensitivity would be only partly determined by the reference signal as originally
conceived by Zacharias and Young [144]. On the other hand, this would suggest that,
althoughego motion and vestibular sensitivities are intrinsically relatedalso depended
on other kinds of information such as somesthetic afferents (e.g., [90]).
En masse, linear sagittal ego motion of the body seems to be determined by
cognitive and vestibular components. The intra-individual variability of vestibular
sensitivity between sagittal and vertical axes would explain the intra-individual
variability of the ego motion sensitivity between both axes. The inter-individual
variability of vestibular sensitivity along the vertical axis would explain the
inter-individual variability of the ego motion sensitivity along this axis. In addition,
exclusively cognitive variables such as the representation of the plausibility or
implausibility of the ego motion is capable of modulating the ego motion sensi-
tivity. Visual attentional factors and memory process specially those associated with
visuospatial working memory can be considered as decisive elements of ego motion
of the body in real and virtual environments. The following point will analyse the
relationship between ego motion and the associated somesthetic cardiovascular
activity.
8
Somesthetic Inputs to Linear Vertical Ego
Motion Perception
Vertical ego motion perception, i.e., elevator movement, can occur in naturalistic
situation when, for example, we take a glass-walled elevator to move upward or
downward. Though a rapid waterfall is seeable (frequently present in international
airports, centers and buildings), we can have the impression that our elevator is
moving at the opposite direction (downward or upward). This visually induced ego
motion perception is a complex phenomenon which necessitates bimodal
visuo-vestibular interaction at least (e.g., [48]). Studies suggest that there is a
differential sensitivity between utricular maculae (transduces horizontal accelera-
tions) and saccular maculae (transduces vertical accelerations) in the otolithic
system of healthy observers (e.g., [69]). Moreover, when saccular maculae are
maximally aligned with gravitational force, electrophysiological and psychophys-
ical ﬁndings have reported a differential sensitivity to opposite accelerations within
the vertical axis (e.g., [46]). Downward ego motion is more easily induced than
upward movement especially because of the same direction with gravitational
acceleration (e.g., [51]).
220
I. Giannopulu

It has been shown that in healthy participants with erect head aligned in the
vertical axis, the otolithic saccular acceleration regulates the heart rate thanks to the
activation of the autonomic nervous system [e.g., 140]. Precisely, otolithic stimu-
lation leads to lower cardiovascular responses (i.e., low heart rate) in patients with
bilateral labyrinthine disease than in healthy persons (e.g., [111]). Cardiovascular
responses would be “vestibuloform” by the fact that they are produced only during
activation of otolith receptors (saccular and utricular maculaes). In that context, the
intensity of the visual–vestibular interaction that modiﬁes ego motion perception
would also modify the functioning of the autonomic cardiovascular responses: the
lesser the vestibular sensitivity, the lower the cardiovascular reaction. This
hypothesis has been tested by the measuring of the variation of heart rate in healthy
people when experienced vertical ego motion perception. Due to the fact that the
otolithic saccular sensitivity for upward ego motion is higher than for downward
motion (e.g., [38]), the cardiovascular reaction, i.e., heart rate would be lower for
downward than for upward ego motion perception (Fig. 8).
Expressly, under normal terrestrial conditions, it was not only demonstrated that
the autonomic cardiovascular response (i.e., the heart rate) is lower for the down-
ward than for the upward ego motion, but that there is also a signiﬁcant difference
in cardiovascular activity between the rest condition and the upward condition. The
cardiovascular activity is higher in upward ego motion condition than in rest
condition. Namely, these results are consistent with the hypothesis claimed that the
cardiovascular responses would be “vestibuloform” as they can be inﬂuenced by the
otolithic saccular receptors. Experiments on animals and humans participants have
demonstrated that inputs from vestibular otolith organs contribute to the control of
heart rate during movement (e.g., [111]). It follows that the intensity of the
cardiovascular activity (bpm)
65
75
85
95
105
Conditions
Rest
Upward
Downward
Fig. 8 Heart rate in vertical ego motion in space (from [51])
Visuo-Vestibular and Somesthetic Contributions …
221

visuo-vestibular interaction along the vertical axis would also reshape cardiovas-
cular activation depending on the directional condition (upward vs. downward). It is
lower for downward ego motion that for upward ego motion. In other words, these
data are clearly consistent with studies that have demonstrated that any reduction
(because of the differential vestibular sensitivity) or loss of vestibular information,
as it is the case in patients with unilateral or bilateral labyrinthine disease, provides
an attenuation of the cardiovascular activation (e.g., [140] ). Similarly, it has been
established that the development of motion sickness that is characterised by
increased ego motion perception leads to the disintegration of autonomic nervous
system balance. And this is because, it is accompanied by an increase in sympa-
thetic activity (and a decreased in parasympathetic activity). Analogously, the
adaptation to motion sickness is accompanied by the recovery of autonomic ner-
vous system balance (e.g., [72]). Signiﬁcant changes in cardiorespiratory parame-
ters occur during visually mediated illusory tilt and vertical ego motion. Such
responses are consistent with the hypothesis that visuo-vestibular input contributes
to the initial cardiovascular adjustment to a change in posture in humans (e.g., [73]).
Even if, it is not possible to determine, with the present studies, the precise
mechanism that occurs but rather can only speculate that it plays an obligatory role
in producing changes in cardiovascular autonomic activation during otolithic
engagement in healthy adults. These observations can be enriched by recent neu-
roimaging data underlying that the neural correlates of vertical ego motion per-
ception (as linked to the gravitational ﬁeld) activate a key cortex area receiving
vestibular inputs: the PIVC parieto-insular vestibular cortex (e.g., [39]). This cor-
tical area, which is directly and indirectly associated with the neural activity of the
occipito-temporo-parietal cortex (e.g., [16]) and with the frontal, temporal, parietal,
and prefrontal cortex (e.g., [24]) would also be associated with an internal model of
gravity, i.e., an egocentric representation of the body in space that necessitates, as
explained previously, the integration of visual, vestibular, and somesthetic infor-
mation at least.
In sum, visuo-vestibular and somesthetic interaction together with cognitive
factors strongly contributes to the ego motion. Both bottom-up and top-down
processes act together in a synergic way to harmoniously perform spatial
navigation.
9
Spatial Navigation in the Presence and Absence
of Visual Afferent
In spatial navigation, humans can adopt two distinct strategies in order to take
information about their position and orientation during travel: (1) navigation using
landmarks from the environment and (2) path integration. It is this second ability
that we use when we walk or are transported with our eyes closed (e.g., [105]). This
kind of navigation involves two main parameters: perception of the distance and
222
I. Giannopulu

direction of movement. Updating these parameters is based on idiothetic infor-
mation (e.g., [104]), arising from several sensory organs including the visual sys-
tem, the vestibular system, the proprioceptive system (e.g., [102]). All information
is integrated into an internal representation (e.g., [93]), which enables humans to
navigate efﬁciently.
Through navigation based on idiothetic (i.e. internal) cues such as vestibular,
proprioception, and visual (e.g., [103]), any human should continuously know
his/her position with respect to the starting point (e.g., [105]). Accurate navigation
from a start location to a goal depends on a constantly updated perception of one’s
location and direction of linear navigation, as well as the appropriate trajectory to
reach the goal (e.g., [58]). Broadly, the perception of spatial linear navigation is
multimodal, the experimental conditions but also the individual preferences such as
cognitive mode seem to inﬂuence the selection of the relevant cue (visual,
vestibular, proprioceptive) (e.g., [74]).
Continuous visual and vestibular signals interact in the brain. Visual sensitive
neurons in the medial superior temporal area (e.g., [42]) and in the ventral intra-
parietal area (e.g., [27]) have visual receptive ﬁelds and are selective for visual
patterns similar to those seen during ego motion (e.g., [87]). The neurons of both
areas are also selective for motion in darkness, suggesting that they receive
vestibular signals (e.g., [57]). The medial parietal cortex is assumed to provide
spatial information tied to a speciﬁc view (e.g., [78]). This cortex is widely inter-
connected with higher cortical areas (including lateral PPC) and subcortical
structures (e.g., [29]). A recent fMRI-study of spatial navigation found that the
medial parietal cortex is the area in which activation increases during visually
signaled forward navigation (e.g., [138]). The frontal lobes are special in that they
have been associated with working visuospatial memory and planning, which all
depend on the recognition and integration of a vast network of signals directly
linked with spatial linear navigation (e.g., 116). Neuroimaging studies have also
shown that somesthetic afferent signals reach the frontal and ventral prefrontal
cortex also contribute to spatial navigation (e.g., [5]).
In the frame of spatial linear navigation, it has been revealed that during
self-driven reproduction of passively traveled distances, the reproduced distances
were longer in light than in darkness (e.g., [110]). Consistent with that are exper-
imental observations following which the efﬁciency and accuracy of spatial navi-
gation depends on the degree of lightness: both increase when lightness decreases
(e.g., [28]). More speciﬁcally, visual information seems to cause an underpercep-
tion of passive self linear navigation which leads to an overshoot of this navigation.
This visual information effect seems to persevere during active movement (e.g.,
walking). Sun et al. [126] found that the availability of visual information during
walking led to an underperception of movement (leading to overshoot) relative to
conditions in which visual information was absent.
While the presence or the absence of visual information is analysed in the
reproduction of passive/active movement, the importance of exteroceptive and
proprioceptive somesthetic information on the reproduction of such movement is
less documented.
Visuo-Vestibular and Somesthetic Contributions …
223

Researchers have studied conditions in which visual information is removed and
only proprioceptive (and vestibular) cues are available during active movement.
Some results have clearly showed that participants were able to reproduce the
imposed distance by walking without vision [94]. Some others have proved that
individuals are able to estimate distance information when learning and responding
through blindfolded walking (e.g., [44]). Without visual information, the effec-
tiveness of vestibular information seems to be limited during larger scale navigation
(e.g., [133]), when exteroceptive information such as vibrations is available (e.g.,
[119]). The mechanisms through which exteroceptive as well as proprioceptive
information can be used to perform self-navigation have recently been studied. It
has been suggested that humans use “step integration” to correctly realise
self-navigation tasks (e.g., [43]). Step integration, i.e., path integration, might be
reliable for short traveled distances (e.g., [32]). The aforementioned studies seem to
inform us that at constant velocity, the availability of visual, vibratory and pro-
prioceptive information inﬂuences the estimation and reproduction of spatial linear
navigation.
Focalised on the way visual information could inﬂuence spatial linear naviga-
tion, a recent study had used a task of distance reproduction. Four distances (2, 4, 6,
and 8 m) were used. The results of the study can be summarised as follows: (1) with
visual information the reproduction of all distances was underperceived, i.e., par-
ticipants overshot the distances to be reproduced; (2) without visual information the
participants undershot the longer distances (Fig. 9). While with vision the repro-
duction seems more accurate than without vision, this was not always signiﬁcant;
therefore vision was not necessary but useful.
In the same vein, Nico et al. [108] have demonstrated that when participants had
to close a triangle of which they were passively submitted to the ﬁrst two sides, they
overshot the distance they had to compute. Both results suggest that an overshoot of
distances can occur regardless the experimental task: estimation, production,
reproduction, or computation.
Moreover, when healthy participants were asked to drive without vision (in
darkness) repeatedly a distance of 2 m, the produced distance was 1.55 m ± 22.5%
at the ﬁrst trial and 1.90 m ± 5% at the end of the trials (e.g., [77]). Therefore,
without visual information the production of distance is always overperceived (i.e.,
undershot). Similarly, the reproduction of 2 m distance via active movement
(walking) without vision, is characterised by overperception, i.e., the participants
undershot that distance (e.g., [83]).
Altogether, these data are compatible with neurophysiological ﬁndings which
show that a vast network of signals including visual, vestibular and somesthetic
ones reaches the medial superior temporal area (e.g., [27]), the ventral intraparietal
area (e.g., [115]), the frontal [5], and prefrontal cortex during passive/active forward
navigation (Angelaki et al. 2006), cortices which are important for spatial linear
navigation.
In the previous study, because of constant velocity the vestibular afferents were
minimised. The somesthetic information is characterised by precise somatotopy
which leads to an appropriate body representation in space. When visual
224
I. Giannopulu

information is combined with somesthetic information (“with vision” condition),
the visual available information predominates (and dampens the other signals) the
elaboration of body representation in space. As a consequence, during visual
navigation the perception of somesthetic information is impoverished which leads
to overshoot. When only somesthetic information is available (“without vision”
condition), the participants undershoot the long distances. In other words, path
integration is not successfully achieved for long distances, where external cues are
certainly useful (e.g., [75]).
The data also reﬂect a consistent combined effect of visual and somesthetic
information with an overall higher inﬂuence of visual cues during spatial linear
navigation. With such results, it seems that working visuospatial memory and
number of responses
0
10
20
30
40
reproduction of distance (m)
1-1,5
2,5-3
4-4,5
5,5-6
7-7,5
8,5-9
10-10,5
11,5-12
2 meters 
4 meters 
6 meters
8 meters
number of responses
0
10
20
30
40
reproduction of distance (m)
1-1,5
2,5-3
4-4,5
5,5-6
7-7,5
8,5-9
10-10,5
11,5-12
2 meters
4 meters
6 meters
8 meters
(a)
(b)
Fig. 9 Distribution of the responses a “with vision” and b “without vision”, following the four
traveled distances (from [75])
Visuo-Vestibular and Somesthetic Contributions …
225

planning requiring multimodal processes in brain contribute to the navigation of
distances (e.g., [115]).
In sum, it seems that the brain consistently weights internal representation of
body information that is on the base of spatial navigation. Under passive and active
navigation, the perception of ego motion is assisted by covariant signals coming
from visual, vestibular and somesthetic proprioceptive systems.
10
Conclusion
Children’s and adults’ common daily activities necessitate passive and/or active
movement within the environment, which are possible because of the development
of the internal representation of the body. Accurate linear and circular ego motion
perception of the body is critical. The reported studies have provided evidence that
passive and/or active ego motion is possible not only because of the integration of
various information, visual, vestibular and somesthetic (exteroceptive, proprio-
ceptive, interoceptive) but also because of cognitive, attentional, and memory
processes.
Of particular importance is the visual information available during ego motion.
Spatial navigation in the absence of vision, in blind persons, has speciﬁc inﬂuence
in their cortical activity and behavior. Indeed, blind persons have a greater ability
than blindfolded sighted persons in utilising auditory information for ego motion.
During ego motion imagery in sighted individuals, a functional magnetic resonance
imaging (fMRI) study has showed deactivations of vestibular areas, in the posterior
insult and in temporal cortex. Structures such as somesthetic cortex, parahip-
pocampal and fusiform gyri are activated. As opposed to sighted persons, totally
blind individuals did not show activations in parahippocampal and fusiform areas.
In addition with the aforementioned areas, blind persons showed activations in the
superior temporal gyrus, somesthetic and primary motor cortical areas with during
the same imagery task (e.g., [141]). It seems that blind individuals rely more on
vestibular and somesthetic feedback for ego motion control than do sighted sub-
jects. Taken together, these ﬁndings underlie once more the signiﬁcant and synergic
effect of visual inputs in the vestibular and somesthetic contributions to ego motion
in space.
Associated with visuo-vestibular and somesthetic interactions, the above data
relative to sighted and blind persons can potentially contribute to the spatial nav-
igation of humanoid robots. It is well known that during ego motion robots gen-
erally deviate from their trajectory due to uncertainty in both equilibrium and
position. From neuro-anatomical view point, visuo-vestibular and somesthetic
afferences are directly connected with the cerebellum which in turn is intercon-
nected with both subcortical and cortical areas. This vestibulo-cerebellum-cortex
pathway is critically involved in adaptive changes in gravitational environment.
Changes in position can provide noticeable challenges to the conservation of bal-
ance and cardiovascular activity as the vestibular system is partly responsible for
226
I. Giannopulu

the regulation of blood pressure. The vestibulo-sympathetic responses integrate
various sensory signals reﬂecting body position in space, somesthetic and visual
information. Even if the integration of the aforementioned signals along the three
body axis is crucial for self-position and balance, visuo-vestibular and sympathetic
responses along the sagittal and vertical body axes could be considered in order to
make potential adjustments to robot autonomous navigation. A top-down organi-
sation associated to visuo-vestibular and somesthetic pathway along these two axes
contributed to body consciousness, i.e., ego-position and equilibrium, could afford a
minimalistic neuromorphic basis in order to improve safe and reliable navigation of
humanoid robots.
References
1. Abramov J, Gordon A, Henrdickson L, Hainline V (1982) Dobson, and I. LaBossière, The
retina of the newborn human infant, Science 217:265–267
2. Agyei SB, Holth M, van der Weel FR, van der Meer ALH (2015) Longitudinal study of
perception of structure optic ﬂow and random visual motion in infants using high-density
EEG. Dev Sci. 18:436–451
3. Agyei SB, van der Weel FR, van der Meer ALH (2016) Development of visual motion
perception for prospective control: brain and behavioral studies in infants. Front Psychol
7:1–14
4. Angelaki DE, Cullen KE (2008) Vestibular system: the many facets of a multimodal sense.
Annu Rev Neurosci 31:125–150
5. Angelaki DE, Gu Y, DeAngelis GC (2011) Visual and vestibular cue integration for heading
perception in extrastriate visual cortex. J Physiol 589:825–833
6. Archer SM (1993) Detection and treatment of congenital esotropia. In: Simons K (ed) Early
visual development, normal and abnormal. Oxford University Press, New York
7. Aslin RN (1993) Perception of visual direction in human infants. In: Granrud CE (ed) Visual
perception and cognition in infancy. Erlbaum, Hillsdale
8. Atkinson J, Braddick O (2007) Visual and visuocognitive development in children born very
prematurely. Prog Brain Res 164:123–149
9. Banks MS (1980) The development of visual accommodation during early infancy. Child
Dev 51:646–666
10. Barbieri G, Gissot AS, Fouque F, Casillas JM, Pozzo T, Perennou D (2008) Does
proprioception contribute to the sense of verticality? Exp Brain Res 185:545–552
11. Barela JA, Jeka JJ, Clark JE (2003) Postural control in children: coupling to dynamic
somatosensory information. Exp Brain Res 150:434–442
12. Barra J, Marquer A, Joassin R, Reymond C, Metge L, Chauvineau V, Pérennou D (2010)
Humans use internal models to construct and update a sense of verticality. Brain 133:3552–
3563
13. Bauermeister M (1964) Effect of body tilt on apparent verticality, apparent body position,
and their relation. J Exp Psychol 67:142–147
14. Baumberger B, Isableu B, Flückiger M (2004) The visual control of stability in children and
adults: Postural readjustments in a ground optical ﬂow. Exp Brain Res 159:33–46
15. Baumberger B (1993) La localisation spatiale dans un environnement visuel mobile Thèse
de Doctorat N° 194, Faculté de Psychologie et des Sciences de l’Education, Université de
Genève Genève, Suisse
Visuo-Vestibular and Somesthetic Contributions …
227

16. Becker-Bense S, Buchholz HG, zu Eulenburg P, Best C, Bartenstein P, Schreckenberger M,
Dieterich M (2012) Ventral and dorsal streams processing visual motion perception
(FDG-PET study). BMC Neurosci 16:13–81
17. Benson AJ (1989) The vestibular sensory system. In: Barlow HB, Mollon JD (eds) The
senses. Cambridge University Press, Cambridge, pp 333–368
18. Benson AJ (1990) Sensory functions and limitations of the vestibular system. In: Warren R,
Wertheim AH (eds) Perception and control of self-motion. Erlbaum, Hillsdale, pp 145–170
19. Bertenthal BI, Rose JL, Bai DL (1997) Perception-action coupling in the development of
visual control of posture. J Exp Psychol Hum Percept Perform 23:1631–1643
20. Biagi L, Crespi SA, Tosetti M, Morrone MC (2015) BOLD response selective to ﬂow-motion
in very young iInfants. PLoS Biol 29:13(9):e1002260. doi: 10.1371/journal.pbio.1002260
21. Bisdorff AR, Wolsley CJ, Anastasopoulos D, Bronstein A, Gresty MA (1996) The
perception of body verticality (subjective postural vertical) in peripheral and central
vestibular disorders. Brain 119:1523–1534
22. Bloch H, Carchon I (1992) On the onset of eye-hand coordination in infants. Behav Brain
Res 49:85–90
23. Bonnet C (1987) La perception visuelle du mouvement. Le Courrier du CNRS 69–70:19–22
24. Bottini G, Sterzi R, Paulesu E, Vallar G, Cappa SF, Ermiono F, Passingham RE, Frith CD,
Frackowiak RSJ (1994) Identiﬁcation of the central vestibular projections in man: a positron
emission tomography activation study. Exp Brain Res 99:164–169
25. Brand T, Dietrich M (2015) The bilateral central vestibular system: its pathways, functions,
and disorders. Ann NY Acad Sci 1343:10–26
26. Brandt T, Dieterich M (1999) Does the vestibulo-ocular reﬂex use the same pathways for
functions in roll and pitch planes? Electroencephalogr Clin Neurophysiol Suppl 50:221–225
27. Bremmer F, Duhamel JR, Ben Hamed S, Graf W (2002) Heading encoding in the macaque
ventral intraparietal area (VIP). Eur J Neurosci 16:1554–1568
28. Cavallo V, Colomb M, Dore J (2001) Distance perception of vehicle rear lights in fog. Hum
Factors 43:442–451
29. Cavanna AE, Trimble MR (2006) The precuneus: a review of its functional anatomy and
behavioural correlates. Brain 129:564–583
30. Cheung BSK, Howard IP, Money KE (1990) Visually-induced tilt during parabolic ﬂights.
Exp Brain Res 81:391–397
31. Cheung BSK, Howard IP, Nedzelski JM, Landolt JP (1989) Circular vection about
horizontal axes in bilateral labyrinthine-defective subjects. Acta Oto-laryngologica (Stockh)
108:336–344
32. Cheung A, Zhang S, Stricker C, Srinivasan MV (2007) Animal navigation: the difﬁculty of
moving in a straight line. Biol Cybern 97:47–61
33. Cignetti F, Zedka M, Vaugoyeau M, Assaiante C (2013) Independent walking as a major
skill for the development of anticipatory postural control: evidence from adjustments to
predictable perturbations. PLoS ONE 8:e56313–e56313
34. Cohen B, Henn V, Raphan T and D. Dennett, Velocity storage, nystagmus, and
visuo-vestibular interaction in humans, in Vestibular and oculomotor physiology, ed., B.
Cohen (New York: New York Academy of Science, 1981)
35. Dechesne CJ, Sans A (1985) Development of vestibular receptor surfaces in human fetuses.
Am J Otolaryngol 6:378–387
36. Deecke L, Schwarz DW, Fredrickson JM (1974) Nucleus ventroposterior inferior (VPI) as
the ventibular thalamic relay in the rhesus monkey. I. Field potential investigation. Exp
Brain Res 20:88–100
37. Deecke L, Schwarz DW, Fredrickson JM (1977) Vestibular responses in the rhesus monkey
ventroposterior thalamus. Vestibulo-proprioceptive convergence at thalamic neurons. Exp
Brain Res 30:219–232
38. De Saedeleer C, Vidal M, Lipshits M, Bengoetxea A, Cebolla AM, Berthoz A et al (2013)
Weightlessness alters up/down asymmetries in the perception of self-motion. Exp Brain Res
226:95–106
228
I. Giannopulu

39. Deutschländer A, Bense S, Stephan T, Schwaiger M, Dieterich M, Brandt T (2004)
Rollvection versus linearvection: comparison of brain activations in PET. Human Brain
Marring 21:143–153
40. Dichgans J, Brandt T (1978) Visual-vestibular interaction: Effects on self motion perception
and postural control. In: Held R, Leibowitz HW, Teuber HL (eds) Handbook of sensory
physiology, vol VIII. Springer, New York, pp 755–804
41. Dichgans J, Held R, Young LR, Brandt T (1972) Moving visual scenes inﬂuence the
apparent direction of gravity. Science 178:1217–1219
42. Duffy CJ, Wurtz RH (1991) Sensitivity of MST neurons to optic ﬂow stimuli. A continuum
of response selectivity to largeﬁeld stimuli. J Neurophysiol 65:1329–1345
43. Durgin FH, Akagi M, Gallistel CR, Haiken W (2009) The precision of locomotor odometry
in humans. Exp Brain Res 193:429–436
44. Ellard CG, Shaughnessy SC (2003) A comparison of visual and nonvisual sensory inputs to
walked distance in a blindwalking task. Perception 32:567–578
45. Eskinazi, M, Giannopulu I (2016) Fluid environment: a continuity in intuition? (in
preparation)
46. Fluur E (1970) The interaction between the utricle and the saccule. Acta Otolaryng 69:17–24
47. Giannopulu I, Lepecq JC (1998) Linear vection chronometry along spinal and sagittal
body-motion. Perception 27:363–449
48. Giannopulu I, Leboucher P, Rautureau G, Israël I, Jouvent R (2015) Spatial vertical
navigation in healthy adults. In: New trends in medical and service robots, pp 101–112
49. Giannopulu I, Bertin RJV, Brémond R, Kapoula Z, Espié S (2008) Visual strategies in
virtual and pre-recording environments. Adv Transport Stud Int J Sect B 14
50. Giannopulu I (2011) Contribution à la compréhension des représentations multimodales chez
l’homme sein et chez des patients avec atteinte neuropsychologique: une approche life span.
Habilitation à Diriger des Recherches, Université Pierre et Marie Curie-Paris VI
51. Giannopulu I, Leboucher P, Rautureau G, Israël I, Jouvent R (2016) Spatial vertical
navigation in healthy adults. In: New trends in medical and service robots, pp 101–112
52. Gibson JJ (1979) The ecological approach to visual perception. Lawrence Erlbaum
Associates, Hillsdale
53. Gilmore RO, Baker TJ, Grobman KH (2004) Stability in young infants’ discrimination of
optic ﬂow. Dev Psychol 40:259–270
54. Gilmore RO, Hou C, Pettet MW, Norcia AM (2007) Development of cortical responses to
optical ﬂow. Vis Neurosci 24:845–856
55. Godoi D, Barela JA (2008) Body sway and sensory motor coupling adaptation in children:
effects of distance manipulation. Dev Psychobiology 50:77–87
56. Goodwin GM, McCloskey DI, Matthews PBC (1972) Proprioceptive illusions induced by
muscle vibration: contribution to perception by muscle spindles. Science 175:1382–1384
57. Gu Y, Angelaki DE, DeAngelis GC (2008) Neural correlates of multisensory cue integration
in macaque MSTd. Nat Neurosci 11:1201–1210
58. Gu Y, Deangelis GC, Angelaki DE (2012) Causal links between dorsal medial superior
temporal area neurons and multisensory heading perception. J Neurosci: Ofﬁcial J Soc
Neurosci 32:2299–2313
59. Guedry FE (1974) Psychophysics of vestibular sensation. In: Kornhuber HH (ed) Handbook
of sensory physiology, vol VI/2. Springer, New York, pp 3–154
60. Hainline L, Abramov I (1985) Saccades and small-ﬁeld optokinetic nystagmus in infants.
J Am Optometric Assoc 56:620–626
61. Hainline L, Riddell PM (1996) Eye alignment and convergence in young infants. In:
Vital-Durant F, Atkinson J, Braddick O (eds) Infant vision. Oxford University Press, Oxford
62. Hainline L (1993) Conjugate eye movements of infants. In: Simons K (ed) Early visual
development, normal and abnormal. Oxford University Press, New York
63. Hainline L (1998) The development of basic visual abilities. In: Slater A (ed) Perceptual
development. Psychological Press, Hove, pp 5–42
Visuo-Vestibular and Somesthetic Contributions …
229

64. Higgins CI, Campos JJ, Kermoian R (1996) Effect of self-produced locomotion on infant
postural compensation to optical ﬂow. Dev Psychol 32:836–841
65. Hlavacka F, Mergner T, Krizkova M (1996) Control of the body vertical by vestibular and
proprioceptive inputs. Brain Res Bull 40:431–434
66. Hollingworth JM, Henderson A (1998) Eye movements during scene viewing: An overview.
In: Underwood G (ed) Eye guidance in reading and scene perception. Elsevier, Oxford,
pp 269–293
67. Horak FB, Nashner LM, Diener HC (1990) Postural strategies associated with somatosen-
sory and vestibular loss. Exp Brain Res 81:167–177
68. Howard IP, Howard A (1994) Vection: the contributions of absolute and relative visual
motion. Perception 23:745–751
69. Howard IP (1986) The vestibular system. In: Boff KR, Kaufman L, Thomas JP
(eds) Handbook of Perception and Human Performance Vol. I. New York: Wiley and
Sons pp 11–3 to 11–26
70. Howard IP (1982) Human Visual Orientation. John Wiley and Sons
71. Howard IP, Cheung BSK, Landolt J (1988) Inﬂuence of vection axis and body posture on
visually-induced self rotation. Advisory Group Aerosp Res Develop 433:15-1–15-8
72. Hu S, Grant WF, Stern RM, Koch KL (1991) Motion sickness severity and physiological
correlates during repeated expo-sures to a rotating optokinetic drum. Aviat Space Environ
Med 62:308–314
73. Indovina I, Maffei V, Pauwels K, Macaluso E, Orban GA, Lacquaniti F (2013) Simulated
self-motion in a visual gravity ﬁeld: sensitivity to vertical and horizontal heading in the
human brain. Neuroimage 71:114–124
74. Isableu B, Ohlmann T, Cremieux J, Vuillerme N, Amblard B, Gresty MA (2010) Individual
differences in the ability to identify, select and use appropriate frames of reference for
perceptuo-motor control. Neuroscience 169:1199–1215
75. Israël I, Capelli A, Priot AE, Giannopulu I (2013) Spatial linear navigation: is vision
necessary? Neurosci Lett 554:34–38
76. Israël I, Giannopulu I (2012) Subjective posture in tridimensional space. J Vestibul Res.
22(4):173–180
77. Israël I, Grasso R, Georges-François P, Tsuzuku T, Berthoz A (1997) Spatial memory and
path integration studied by selfdriven passive linear displacement. 1. Basic properties.
J Neurophysiol 77:3180–3192
78. Jahn G, Wendt J, Lotze M, Papenmeier F, Huff M (2012) Brain activation during spatial
updating and attentive tracking of moving targets. Brain Cogn 78:105–113
79. Jarchow T, Mast FW (1999) The effect of water immersion on postural and visual
orientation. Aviat Space Environ Med 70:879–886
80. Johnson M (1990) Cortical maturation and the development of visual attention in early
infancy. J Cogn Neurosci 2:81–95
81. Jonsson B, von Hofsten C (2003) Infants’ ability to track and reach for temporarily occluded
objects. Dev Sci 6:86–99
82. Kirsch V, Keeser D, Hergenroeder T, Erat O, Ertl-Wagner B, Brandt T et al (2015)
Structural and functional connectivity mapping of the vestibular circuitry from human
brainstem to cortex. Brain Struct Funct. doi:10.1007/s00429-014-0971-x
83. Klatzky RL, Loomis JM, Golledge RG, Cicinelli JG, Doherty S, Pellegrino JW (1990)
Acquisition of route and survey knowledge in the absence of vision. J Mot Behav 22:4–19
84. Kwon MK, Setoodehnia M, Baek J, Luck SJ, Oakes LM (2016) The development of visual
search in infancy: attention to faces versus salience. Dev Psychol 52(4):537–555
85. Lackner JR (1977) Induction of illusory self-rotation and nystagmus by a rotating sound
ﬁeld. Aviat Space Environ Med 48:129–131
86. Lackner JR, Graybiel A (1978) Some infuences of touch and pressure cues on human spatial
orientation. Aviat Space Environ Med 49:798–804
87. Lappe M, Bremmer F, Pekel M, Thiele A, Hoffmann KP (1996) Optic ﬂow processing in
monkey STS: a theoretical and experimental approach. J Neurosci 16:6265–6285
230
I. Giannopulu

88. Lejeune L, Anderson DI, Campos J, Witherington DC, Uchiyama I, Barbu-Roth M (2006)
Responsiveness to terrestrial optic ﬂow in infancy: does locomotor experience play a role?
Hum Mov Sci 25:4–17
89. Lepecq JC, Giannopulu I, Baudonnière PM (1995) Cognitive effects on visually induced
body-motion in children. Perception 24:435–449
90. Lepecq JC, Giannopulu I, Mertz S, Baudonnière PM (1999) Vestibular sensivity and linear
vection chronometry along spinal axis in erect man. Perception 28:63–72
91. Lishman JR, Lee DN (1973) The autonomie of visual kinesthesis. Perception 2:287–294
92. Loenneker T, Klaver P, Bucher K, Lichtensteiger J, Imfeld A, Martin E (2011)
Microstructural development: organisational differences of the ﬁber architecture between
children and adults in dorsal and ventral visual streams. Hum. Brain. Maps. 32:935–946
93. Loomis JM, Klatzky RL, Golledge RG, Cicinelli JG, Pellegrino JW, Fry PA (1993)
Nonvisual navigation by blind and sighted: assessment of path integration ability. J Exp
Psychol Gen 122:73–91
94. Loomis JM, Da Silva JA, Fujita N, Fukusima SS (1992) Visual space perception and
visually directed action. J Exp Psychol Hum Percept Perform 18:906–921
95. Lopez C, Blanke O (2011) The thalamocortical vestibular system in animals and humans.
Brain Res Rev 67(1–2):119–146
96. Lopez C, Blanke O, Mast FW (2012) The human vestibular cortex revealed by
coordinate-based activation likelihood estimation meta-analysis. Neuroscience 14:159–179
97. Marlinski V, Mccrea RA (2008) Activity of ventroposterior thalamus neurons during
rotation and translation in the horizontal plane in the alert squirrel monkey. J Neurophysiol
99:2533–2545
98. Matesz C, Bácskai T, Nagy E, Halasi G (2002) A, Kulik, Efferent connections of the
vestibular nuclei in the rat: a neuromorphological study using PHA-L. Brain Res Bull
57:313–315
99. Matsuzaki M, Honkura NG, Ellis-Davies C, Kasai H (2004) Structural basis of long-term
potentiation in single dendritic spines. Nature 429:761–766
100. Milner AD, Goodale MA (2008) The two systems re-viewed. Neuropsychoelogia 46:
774–785
101. Mittelstaedt H (1998) Origin and processing of postural information. Neurosci Biobehav
Rev 22:473–478
102. Mittelstaedt H (2000) Triple-loop model of path control by head direction and place cells.
Biol Cybern 83:261–270
103. Mittelstaedt H, Mittelstaedt ML (1973) Mechanismen der Orientierung ohne richtende
Aubenreize. Fortschr Zool 21:46–58
104. Mittelstaedt ML, Mittelstaedt H (1980) Homing by path integration in a mammal. Naturwiss.
67:566–567
105. Mittelstaedt ML, Mittelstaedt H (2001) Idiothetic navigation in humans: estimation of path
length. Exp Brain Res 139:318–332
106. Moore ST, Hirasaki E, Cohen B, Raphan T (2001) The human vestibulo-ocular reﬂex during
linear locomotion. Ann NY Acad Sci 942:139–147
107. Nardini M, Cowie D (2012) The development of multisensory balance, locomotion,
orientation and navigation. In: Bremner A, Lewkowicz D, Spence C (eds) Multisensory
development. Oxford University Press, Oxford, pp 137–158
108. Nico D, Israël I, Berthoz A (2002) Interaction of visual and idiothetic information in a path
completion task. Exp Brain Res 146:379–382
109. Paillard J (1987) Cognitive versus sensorimotor encoding of spatial information. In: Ellen P,
Thinus-Blanc C (eds) Cognitive processes and spatial orientation in animal and man.
Martinus Nijhoff Publishers BV, Dordrecht, pp 43–77
110. Priot AE, Israël I (2004) Visual and non visual informations in perception of passive linear
displacement. In: XXIIIth meeting of the Barany Society, Paris, France, July 7–9, 2004
111. Ray CA, Monahan KD (2002) Aging attenuates the vestibulosympathetic reﬂex in humans.
Circulation 26:956–961
Visuo-Vestibular and Somesthetic Contributions …
231

112. Rinaldi NM, Polastri PF, Barela JA (2009) Age-related changes in postural control sensory
reweighting. Neurosci Lett 467:225–229
113. Rohlﬁng KJ, Longo MR, Bertenthal BI (2012) Dynamic pointing triggers shifts of visual
attention in young infants. Dev Sci 15(3):426–35
114. Roll JP, Roll R, Velay JP (1991) Proprioception as a link between body space and
extra-personal space. In: Paillard J (ed) Brain and space. Oxford University Press, Oxford,
pp 112–132
115. Romanski LM (2012a) Convergence of auditory, visual, and somatosensory information in
ventral prefrontal cortex. In: Murray MM, Wallace MT (2012) The neural bases of
multisensory processes. CRC Press, Boca Raton
116. Romanski LM (2012b) Integration of faces and vocalizations in ventral prefrontal cortex:
implications for the evolution of audiovisual speech. Proc Natl Acad Sci U S A 109 Suppl
1:10717–10724
117. Ruff CC, Kristjánsson A, Driver J (2007) Readout from iconic memory and selective spatial
attention involve similar neural processes. Psychol Sci 18:901–909
118. Santos MJ, Kanekar N, Aruin AS (2010) The role of anticipatory postural adjustments in
compensatory control of posture: electromyographic analysis. J Electromyogr Kinesiol
20:388–397
119. Seidman SH (2008) Translational motion perception and vestiboocular responses in the
absence of non-inertial cues. Exp Brain Res 184(1):13–29
120. Shea SL, Alsin RN (1984) Development of horizontal and vertical pursuit in human infants.
Invest Ophth Vis Sci 19:1092–1400
121. Shirai N, Imura T, Tamura R, Seno T (2014) Stronger vection in junior high school children
than in adults. Front Psychol 5:1–5
122. Shirai N, Seno T, Morohashi S (2012) More rapid and stronger vection in elementary school
children compared with adults. Perception 41:1399–1402
123. Shirai N, Yamaguchi MK (2010) How do infants utilize radial optic ﬂow for their motor
actions? A review of behavioral and neural studies. Jpn Psychol Res 52:78–90
124. Shumway-Cook A, Woollacott M (1985) The growth of stability: Postural control from a
developmental perspective. J Motor Behav 17:131–147
125. Sparto PJ, Redfern MS, Jasko JG, Casselbrant ML, Mandel EM, Furman JM (2006) The
inﬂuence of dynamic visual cues for postural control in children aged 7–12 years. Exp Brain
Res 168:505–516
126. Sun HJ, Campos JL, Young M, Chan GS, Ellard CG (2004) The contributions of static
visual cues, nonvisual cues, and optic ﬂow in distance estimation. Perception 33:49–65
127. Tardy-Gervet MF, Gilhodes JC, Roll JP (1984) Perceptual and motor effects eliced by a
moving visuel stimulus below the forearm: an example of segmentary vection. Behav Brain
Res 11:171–184
128. Tomko DL, Peterka RJ, Schor RH (1981) Responses to head tilt in cat eighth nerve afferents.
Exp Brain Res 41:216–221
129. Trousselard M, Barraud PA, Nougier V, Raphel C, Cian C (2004) Contribution of tactile and
interoceptive cues to the perception of the direction of gravity. Cogn Brain Res 20:355–362
130. van der Meer ALH, Fallet G, van der Weel FR (2008) Perception of structured optic ﬂow
and random visual motion in infants and adults: a high-density EEG study. Exp Brain Res
186:493–502
131. Vilhelmsen K, van der Weel FR, van der Meer ALH (2015) Development of optic ﬂow
perception in infants: a high-density EEG study of speed direction. In: Wesat-Knapp JA,
Malone ML, Abney DH (eds) Studies in perception and action XIII. Psychological Press,
New York, pp 157–160
132. Von Hofsten C, Rosander K (1996) The development of gaze control and predictive tracking
in young infants. Vision Res 36:81–96
133. Waller D, Greenauer N (2007) The role of body-based sensory information in the acquisition
of enduring spatial representations. Psychol Res 71(3):322–32
232
I. Giannopulu

134. Wann JP, Mon-Williams M, Rushton K (1998) Postural control and coordination disorders:
the swinging room revised. Hum Move Sci 17:491–513
135. Wattam-Bell J, Birtles D, Nyström P, von Hofsten C, Rosander K, Anker S, Atkinson J,
Braddick O (2010) Reorganization of global form and motion processing during human
visual development. Curr Biol 20(5):411–415
136. Wertheim AH (1994) Motion perception during self-motion: The direct versus inferential
controversy revisited. Behav Brain Sci 17:293–355
137. Wijesinghe R, Protti DA, Camp AJ (2015) Vestibular interactions in the thalamus. Front
Neural Circuits. doi:10.3389/fncir.2015.00079
138. Wolbers T, Hegarty M, Buchel C, Loomis JM (2008) Spatial updating: how the brain keeps
track of changing object locations during observer motion. Nat Neurosci 11:1223–1230
139. Wong SCP, Frost BJ (1981) The effect of visual-vestibular conﬂict on the latency of
steady-state visually induced subjective rotation. Percept Psychophys 30(3):228–236
140. Yates BJ, Bolton PS, Maceﬁeld VG (2004) Vestibulo-sympathetic responses. Compr.
Physiol. 4(2):851–887
141. Yates BJ, Aoki M, Burchill P, Bronstein AM, Gresty MA (1999) Cardiovascular responses
elicited by linear acceleration in humans. Exp Brain Res 125:476–484
142. Young LR, Oman CM, Dichgans JM (1975) Inﬂuence of head orientation on visually
induced pitch and roll sensation. Aviat Space Environ Med 46:264–268
143. Young LR, Shelhamer M (1990) Weightlessness enhances the relative contribution of
visually-induced self motion. In: Warren R, Wertheim AH (eds) Perception and control of
self-motion. Erlbaum, Hillsdale, pp 523–538
144. Zacharias GL, Young LR (1981) Inﬂuence of combined visual and vestibular cues on human
perception and control of horizontal rotation. Exp Brain Res 41:159–171
Visuo-Vestibular and Somesthetic Contributions …
233

Part III
Mobility of the Visually Impaired

Orientation and Mobility Training
to People with Visual Impairments
Mira Goldschmidt
1
Introduction
There is consensus in the specialized literature that mobility is an important part of
everyday life, and that impairment to mobility affects the quality of life [4, 12, 24,
36]. Independent travel is a well-known challenge for visually impaired persons
(that is, those who are blind or have low vision). People with visual impairment
experience difﬁculties moving around dynamic environments and in unfamiliar
areas. The goal of orientation and mobility training is to prepare the person with
visual impairment to travel independently in any environment, familiar or
unfamiliar.
Training in orientation and mobility refers to the skills and techniques required
for independent and safe travel by persons who are visually impaired [18].
Mobility has been deﬁned as the act of moving through space in a safe and efﬁcient
manner [13]. Orientation has been deﬁned as “knowledge of one’s distance and
direction relative to things observed or remembered in the surroundings and keeping
track of these spatial relationships as they can change during locomotion” [4].
Training in orientation and mobility aims to increase the skills of moving in the
environment in a safe and efﬁcient way. Orientation and mobility specialists, a
speciﬁc profession to blindness and low vision, provide instruction to people with
visual impairment helping them to develop or relearn the skills and concepts they
need in order to travel safely and independently.
This chapter begins by presenting the orientation and mobility training to people
with visual impairment. The chapter describes the speciﬁc mobility challenges of
M. Goldschmidt (&)
Department of Ophthalmology, University of Lausanne, Jules Gonin Eye Hospital,
Low Vision Departement, Fondation Asile des Aveugles, Lausanne, Switzerland
e-mail: mira.goldschmidt@fa2.ch
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_8
237

people with visual impairment as well as the strategies and techniques used to
improve orientation and mobility. The chapter also describes the use of mobility
devices helping persons with visual impairment to travel safely. More information
about researches and experience of practitioners in the area of orientation and
mobility can be found in Wiener et al. [42].
2
Population
According to the World Health Organization, 285 million people are estimated to
be visually impaired worldwide. The visually impaired population refers to persons
with low vision or with blindness. The population of people with low vision is
much larger than the population of people who are totally blind. 39 million people
are blind and 246 million have low vision.
According to the International Classiﬁcation of Diseases—10 (Update and
Revision 2006), there are four levels of visual function
1. Normal vision
2. Moderate visual impairment
3. Severe visual impairment
4. Blindness.
Moderate visual impairment, combined with severe visual impairments is
grouped under the term “low vision”. Approximately 90% of visually impaired
people live in developing countries.
According to the WHO, the major causes of visual impairment worldwide are
• Uncorrected refractive errors (myopia, hyperopia or astigmatism), 43%
• Unoperated cataracts, 33%
• Glaucoma, 2%.
About 65% of all people with visual impairment are aged 50 and older. As the
population ages, the number of people with visual impairment continues to grow.
According to the national federation of the blind, there are 5.5 million seniors in
the United States who are blind or visually impaired. Only 1% of the blind pop-
ulation was born blind. The majority of visually impaired people in the United
States lose their vision later in life because of age-related macular degeneration,
glaucoma and diabetes. Age-related macular degeneration (AMD) is a leading
cause of vision loss in older adults [17].
In macular degeneration the central vision deteriorates, resulting in blurred
vision. Glaucoma causes damage to the optic nerve through pressure, compro-
mising ﬁrst peripheral vision. Diabetic retinopathy occurs when diabetes damages
the tiny blood vessels inside the retina (Figs. 1, 2 and 3).
238
M. Goldschmidt

Fig. 1 Simulation of macular degeneration: a blurred image with a dark spot in the centre
Fig. 2 Simulation of glaucoma: a blurred image with a sharply focused area in the centre
Orientation and Mobility Training to People …
239

People with vision loss may have hearing loss or physical, health or cognitive
impairments. The special challenges to orientation and mobility specialists when
teaching people with visual impairment and other disabilities will not be discussed
in this chapter, but the use of new technologies in these situations might be very
important.
3
Orientation and Mobility Instruction
The goal of orientation and mobility training is to prepare the person to travel in a
variety of environments, both familiar and unfamiliar, and to increase the skills of
moving in the environment in a safe and efﬁcient way. Training in orientation and
mobility develops the ability to understand one’s position in space in relation to the
external world and improve the ability to travel independently. Orientation and
mobility training began after World War II, when techniques were developed to
help veterans who had lost their sight [42].
Nowadays, training in orientation and mobility is given to people of all ages:
infants, children, grown-ups and elderly. The orientation and mobility training
program has to be tailored to the student’s capabilities and level of functioning, and
individualized programs are developed to meet the needs of each student.
Successful orientation and mobility training involves a lot more than the use of a
Fig. 3 Simulation of diabetic retinopathy: a blurred image with dark spots throughout
240
M. Goldschmidt

long cane or a guide dog. Training to improve the travel skills and strategies of
individuals who are visually impaired will be explained in the next pages. After
describing the orientation and mobility instruction for people with blindness, we
will describe the speciﬁc orientation and mobility training program for people with
low vision.
3.1
Orientation and Mobility Training for People
with Blindness
People with normal vision can see about 180° of their environment at a glance.
There is no other sense that can collect and process the same volume and richness
of information as quickly as sight can. The primary differences between sighted and
blind travel are the speed, volume, detail and distance from which auditory and
tactile information are obtained and processed [9]. Estimates suggest that as much
as 80–90% of ones’ information is obtained through vision. Vision provides more
useful spatial information than nonvisual spatial information.
The totally blind person has major problems in orientation and mobility and is at
a disadvantage compared to the sighted pedestrian. He develops strategies and
techniques in order to travel safely in the environment and uses many types of
nonvisual information in place of visual information, but requires additional time in
walking an unfamiliar route and crossing streets.
According to the specialized literature comparing the blind and the sighted [37–
39], the long-term blind has difﬁculties in spatial, perceptual and conceptual abil-
ities. The blind incorporates fewer external references and uses more egocentric
information in mobility.
In a survey [25], it was found that the most frequent travel difﬁculty reported by
blind and partially sighted people included a lack of conﬁdence in going out alone,
going to unfamiliar places and meeting obstacles in the environment, and fears
about busy trafﬁc.
Instruction in orientation and mobility starts with an assessment phase. The
instructor must consider psychosocial, sensory, motor, perceptual, cognitive and
environmental factors in the choice of instructional techniques and in the devel-
opment of the training program. After setting the goals of the orientation and
mobility training, pre-cane skills and techniques are taught prior to instructions on
the use of the cane. Generally, the lessons progress from the most simple to the
more complex. First, indoors environment, which is easier to control, followed by
outdoors environment. Orientation and mobility training is usually given on a
one-on-one basis, so that it can be adapted to the student’s personal abilities.
A systematic curriculum of orientation and mobility training can be found in the
literature [13, 15, 18].
Orientation and Mobility Training to People …
241

3.2
Pre-cane Skills and Techniques
3.2.1
Sighted Guide Techniques
The instruction of totally blind adults starts usually with the sighted guide tech-
nique. Using this technique, the student walks one half step behind the guide and
gets kinaesthetic and proprioceptive information while moving with the sighted
guide. The guide takes the responsibility for orientation and for protection from
obstacles, while the student gets used to moving safely through the environment.
The student gets information from the guide about the sensory clues, spatial and
environmental concepts which are necessary for orientation. While following the
sighted guide, the student also receives information about the characteristics,
identity and position of objects in the environment. He also learns how to judge
distances and to interpret context and landmarks.
The student follows the guide movements and learns the concepts’ prerequisites
for establishing orientation and planning routes to travel. Some examples of spatial
and environmental concepts which are necessary for orientation are: left, right,
front, back, straight-line concept, parallel, perpendicular, right angle, etc. The
student learns how to interpret the information he gets from the guide’s movements.
He learns how to change direction and how to negotiate stairs. By using the sighted
guide technique, the student can develop skills and get preparation for independent
travel.
3.2.2
Self-protection Technique
Self-protection is a technique used by people with visual impairment, to allow them
to travel efﬁciently and independently in familiar indoor environments, affording
them maximum protection without the use of a mobility aid. The forearm, when
held in the correct position in front of the body, acts as a shock absorber and
reduces the risk of injury. The self-protection technique enables the person to detect
vertical objects at the upper region of the body and at the waist level.
3.2.3
Trailing Technique
The purpose of the trailing is to facilitate the student’s maintenance of a straight line
of travel (for example in hallways and corridors) and to locate a speciﬁc object. The
student follows with his ﬁngers a line of travel (for example the wall), so that he can
keep constant contact with the environment and can gain advance information about
an unfamiliar area (location of stairs, level changes, etc.).
242
M. Goldschmidt

3.2.4
Accurate Turns
The student learns how to make accurate turns and how to travel in open doorways
and to establish a straight line of travel so that he will be able to familiarize himself
systematically with a particular environment.
3.3
Mobility
Without mobility aid, the blind person’s movements will be severely restricted. The
most prevalent devices that are used by people with visual impairment are the long
cane and the guide dog. Long cane is the primary tool utilized by the visually
impaired people in orientation and mobility. The long cane was introduced by
Richard Hoover in the late 1940s, replacing the traditional short wooden cane.
Richard Hoover introduced the technique of moving the cane from side to side with
the tip touching the ground in front of the trailing foot. Systematic training of guide
dogs began in the eighteenth century, but guide dogs became widespread only after
the First World War [40].
3.3.1
Long Cane Techniques
The cane skills enable the person to travel safely, efﬁciently and independently, in a
familiar or unfamiliar environment. Strategies for using the cane in a systematic
way are essential. The function of the long cane is to preview the immediate
environment for objects on the path of travel, and to preview changes of the surface
of travel. The student can perceive the material, slope, elevation of an upcoming
walking surface, the location and dimension of the obstacles. The long cane pro-
vides detection of obstacles and information about the travel path in advance, as it
allows detection of obstacles within a 3-ft (about 90 cm) range. To clear the path
ahead, the cane is swept from left to right, synchronized with the steps of the user. If
an obstacle is detected the cane user has to react quickly to avoid collision. When
the cane contacts the ground, it can also provide information concerning objects on
the side of the path through the acoustical reﬂections that result from tapping the
ground with the cane tip. The trained pedestrian can get haptic and auditory inputs
from the cane which will help him map the environment. A systematic curriculum
of instructions is used by the orientation and mobility specialists.
Different cane techniques are used to guide the locomotion.
For example, the diagonal technique (used mostly in a familiar indoor envi-
ronment), the touch technique (used for example to detect drop-offs and objects in
the vertical plane in familiar or unfamiliar environments), the touch and slide
technique (used to detect textural changes, subtle drop-offs and blended areas
perpendicular to the line of travel), the touch and drag technique (used for example
for locating bus poles or for street crossing alignment), the three point touch
Orientation and Mobility Training to People …
243

technique (used for example to follow the curb of a street when no sidewalk exists),
and the ascending and descending steps technique.
With the long cane techniques, the student gets protection from obstacles and
gains environmental information. The cane serves also as an indicator to drivers and
sighted pedestrians that care should be taken in the presence of such a device.
The limitations of the long cane are such that small objects can be missed by the
sweep of the cane, and the overhanging obstacles cannot be detected by the cane.
The body from the waist up is vulnerable to physical contact with objects or people
[32].
3.3.2
Guide Dogs
The next most common mobility aid after the cane is the guide dog. Completion of
a mobility training course is a prerequisite for using a guide dog. Guide dogs are
trained to avoid all obstacles (also overhanging obstacles) and the blind pedestrian
needs to assess the dog’s behaviour and interpret it to remain safe and oriented.
Using a guide dog facilitates travel in open areas, in off-road conditions, in snow
and parks. At street crossings, it is easier to maintain the line of direction with a
guide dog. As guide dogs are trained to avoid obstacles, they provide minimal
tactile contact with the environment and might deprive the blind pedestrian from
information that could be helpful for his orientation.
Guide dogs are not appropriate as mobility aids for all blind people. The training
process is physically vigorous, and the walking speed with a guide dog is much
faster than that with a cane. The blind person needs to have good coordination,
strength and balance. The training in orientation and mobility is also useful to the
guide dog user. For example, with orientation skills, ability to align with trafﬁc
sound or the ability to recover when disoriented.
With the long cane techniques or with a guide dog, the student can assume
responsibility for travelling safely, but he also needs sensory training. Sensory
training includes developing the student’s ability to utilize all of his available senses
for orientation. Efﬁcient use of tactile and auditory information does not always
develop automatically and the student with visual impairment needs some exercises
to increase the sensitivity and the application of his remaining senses.
3.3.3
Street Crossings
Street crossing is associated with increased risk of injury. The most important
aspect of street crossing is safety and it is an important part of the student’s
orientation and mobility training. Generally, the lessons on making trafﬁc judg-
ments progress from the simplest streets crossings to the more complex streets
crossings.
The student is trained to cross streets in a safe, efﬁcient and systematic manner.
He learns how to locate the street and how to analyze the intersection (for example,
244
M. Goldschmidt

the location of the crosswalk, the direction of the opposite corner, the number of
intersecting streets, the width of the street, etc.). The student has to learn how to line
up and make a straight crossing. He learns how to prevent veering while crossing
the street and how to recover from veering and reorient himself to the desired
position. He also needs to learn how to analyze irregular intersections and to judge
and determine the good moment for safe crossing. The use of technologies like
detectable warning surfaces and accessible pedestrian signals helps the blind
traveller with street crossings. The visually impaired pedestrian has to deal with
complex environments like roundabouts and unusual geometric shapes of inter-
section and orientation and mobility training in these situations is essential. Quiet
cars are presenting challenges to pedestrians with blindness and adapted strategies
are necessary.
3.3.4
Residential and Commercial Areas
The student is trained to travel in residential and commercial areas. He learns how
to use escalators, elevators and revolving doors. The student exercises travelling
day and night in speciﬁed conditions like congested areas and subways.
3.3.5
Transportation System
The student is introduced to the use of public transportation. He learns how to use
the various types of transportation systems and develops the needed strategies to
plan and to travel independently. The student learns how to deal with unexpected
events (for example, when he gets off at the wrong station) and emergency situa-
tions. The use of new technology, like audible arrival and departure information
systems and online trip information improved access to public transportation.
3.4
Sensory training
The development of orientation skills enables the blind pedestrian to get familiar
with unfamiliar environments. The student learns how to get a line course from an
object or sound to facilitate travelling in a straight direction toward an objective. He
also learns how to reorient himself to a desired position (recovery) after drifting
away from the desired line of travel.
Orientation is maintained by the use of a number of skills and information
gained from the environment through other senses. According to Long and Giudice
[19] there are four fundamental aspects of spatial orientation: information collect-
ing, the use of strategies for following familiar routes, the use of cognitive maps,
and the application of strategies to solve problems.
Orientation and Mobility Training to People …
245

Persons with visual impairments are able to gain orientation and to maintain this
orientation while moving from one part of the environment to the other by using a
combination of the remaining sensory information. They use landmarks and clues
to establish and maintain orientation. The landmarks and clues are used as reference
points to locate speciﬁc objectives and for perpendicular and parallel alignment
needed for a straight line of travel.
A landmark can be any familiar object, sound, odour, temperature or tactual clue
which is constant, easily recognized and with a permanent location in the envi-
ronment. A clue is any visual, auditory, tactile, olfactory or kinaesthetic information
that can help the blind pedestrian to obtain direction and to determine his position in
the environment. As he needs to learn to negotiate his surroundings, the student has
to develop his capacities of utilizing the remaining senses in establishing his
position and relationship to all other signiﬁcant objects in his environment. The
sensory training he gets can help him to utilize the remaining senses to the opti-
mum. For example, at home familiar sounds can help him locate objects, or outdoor
trafﬁc noise can help him to orient himself. The changes in sound as he moves
around can also be used for orientation. He can also continually visualize his
environment and his location within that environment.
The principal sensory cues for spatial location are available to the blind through
touch for near space and through sound for far space.
Touch is used to identify objects encountered during travel and to recognize
familiar landmarks. Tactile exploration and the ability to use tactile and haptic
information accurately and efﬁciently have to be developed. Touch is used to
identify objects in the environment during travel and to recognize landmarks.
As sound localization and sound discrimination are important auditory skills in
orientation and mobility, an auditory training program is proposed so that the
student can learn how to interpret the sound and determine, for example, the
approximate size of a room and the location of the doorway. The student can
perceive the direction of a sound source and localize the distance of sound sources
and use it, for example, for street crossing. He can also track movements of vehicles
by the sounds they produce and locate signs and poles by the sound they reﬂect.
Many blind persons develop echolocation, the awareness of the presence and
locations of objects in the environment without vision. Echolocation can help the
pedestrian with visual impairment to avoid large obstacles and locate openings
along walls before making physical contact with them. Lower frequency sound cues
are particularly important in echolocation. Self-produced sounds enhance the use of
reﬂected sound for mobility, and persons with visual impairment also develop the
capacity to produce sounds that are optimal for perception.
When visual, auditory, or tactile information is not completely accessible,
pedestrians with visual impairment can use proprioceptive and kinaesthetic cues for
mobility. The kinaesthetic senses, based on proprioception, can provide nonvisual
information that can help the student to remain oriented as he walks through
familiar surroundings [21]. Training the kinaesthetic sense might help the student to
246
M. Goldschmidt

maintain his line of travel and improves measurement of turns and measurement of
walking distances.
Training the compensatory use of environmental olfactory cues is also useful in
orientation, as in some circumstances the perception and discrimination of odours
can help identifying places like restaurants or others shops. The use of cardinal
directions in mobility is important for independent travel. Tactile or audible com-
passes are used in orientation and mobility training to establish a line of travel, since
compass’ directions add stability to the environment and are constant. Tactile
compasses have a raised dial on which an arrow represents north, while east, south,
and west are represented by Braille letters. Electronic compasses provide the
pedestrian with spoken announcements. The use of electronic orientation aids is
discussed later in this chapter.
Cognitive maps, which are mental images of the information about routes, are
essential for independent travel of people with visual impairments. Orientation and
mobility specialists use orientation aids like tactile maps and large-print maps,
models and route descriptions for the development of spatial concepts and cognitive
maps. Maps for visually impaired people have to include only essential information.
Training in the use of tactile maps and models can enhance understanding of the
spatial and environmental concepts and can improve travelling in novel areas and
estimating travel time. Tactile maps and models are good alternatives to visual
preview but the ability to use them effectively and efﬁciently has to be developed.
Orientation and mobility specialists teach strategies for map reading. Concepts like
linear continuity and directionality, symbolic representation, scale and shape are
necessary for map reading. Scanning systematically, identifying symbols, tracing
line symbols and recognizing shapes are also some of the skills that orientation and
mobility instructors teach to make optimal use of maps. More information about
teaching the use of maps in orientation and mobility can be found in Bentzen and
Martin [2].
Congenitally blind people do not have the same rehabilitation needs as people
with acquired blindness. Congenitally blind children might have gait and posture
problems if they do not get early intervention. They need to develop basic concepts
and orientation skills. Millar [23] suggested that congenitally blind people have less
precise sources of spatial information because they base their spatial knowledge
more on proprioceptive and kinaesthetic information. People who are congenitally
blind have a different mental representation of the spatial layout compared to people
who become blind. A person who was born blind will put locations in relation to
each other without having a concrete image of it, while a person who is a late blind
will form an image based on images that he has seen before. According to
Thinus-Blanc and Gauner [34], the age of onset of visual impairment has no effect
on egocentric spatial tasks, but when tasks require a more allocentric frame of
reference, there are differences in performances between groups of people who lost
vision early in life and those who lost vision later in life.
Orientation and Mobility Training to People …
247

3.5
Orientation and Mobility Training
for People with Low Vision
Mobility training program for a low vision person has to be preceded by assessment
of the student’s visual functioning in a variety of conditions and in a variety of
settings. The functional evaluation with the remaining vision is necessary in order
to adapt the orientation and mobility training to the person with low vision, so that
he will be able to use the remaining vision for navigation to its fullest potential.
3.5.1
The Speciﬁc Mobility Needs of People With Low Vision
The majority of people who have a visual impairment have some residual vision
[8]. WHO [41] deﬁne low vision as the inability, even with corrective lenses, to
clearly see at a distance of 6 m (20 ft) what individuals with normal vision can
clearly see at a distance of 18 m (60 ft).
Most of the people with visual impairment have some usable vision for navi-
gation. The ﬁeld of low vision mobility is about ﬁve decades old. The idea that
people with low vision have the same rehabilitation needs as the blind people has
been changed within the last decades. Orientation and mobility training for people
with low vision is different from the trainings designed for people who are totally
blind. Although many of the trained techniques and skills are useful for both
populations, persons with low vision need a special training program designed at
increasing their visual functioning in orientation and mobility.
Reduced visual acuity is more common than reduced visual ﬁeld. Table 1
describes the common pathologies and their functional problems.
The mobility problems associated with reduced visual acuity might be different
from the mobility difﬁculties associated with reduced visual ﬁelds. Some students
with reduced visual acuity do not have the ability to read or to recognize faces and
cannot detect stairs and curbs. Students with a restricted visual ﬁeld may see
obstacles but lose perspective and might not be able to avoid them. Glare and depth
perception are the most frequent functional difﬁculties. Some students do not have
the ability to judge speed and direction of trafﬁc. In most of the pathologies causing
low vision, people suffer from glare, loss of depth perception and ﬂuctuating vision
and need orientation and mobility intervention.
Turano et al. [35] had studied the distribution of perceived ability for inde-
pendent mobility in people at various stages of Retinitis Pigmentosa (RP). They
found that four of the six most difﬁcult mobility situations were related to: lighting
conditions, walking at night, adjusting to lighting changes, walking in dimly lit
indoor areas, and walking in high-glare areas. They also found that in the situations
such as moving about in the home and walking in familiar areas, the RP subjects
reported little or no difﬁculty. According to the authors, this ﬁnding implies that
248
M. Goldschmidt

vision may not be as critical when people already have a mental representation of
their surroundings and when the objects in those surroundings remain stationary.
Smith et al. [29] had studied the perceptions of the most difﬁcult mobility
problems in persons with low vision and in orientation and mobility specialists. The
individuals with low vision’s perceptions of their ﬁve most difﬁcult mobility
problems were: drop-offs (negotiating steps, curbs, and ramps), lighting conditions
and adapting to changes in lighting, street crossing (inability to distinguish the
colour of trafﬁc lights and crossing without trafﬁc lights), changes in terrain (broken
or uneven sidewalks, street, and surfaces), and obstacles (low-laying objects,
head-high and low-hanging objects). When orientation and mobility specialists
were asked to list the ﬁve more difﬁcult mobility problems of the “low vision
population in general”, they mentioned the same top ﬁve problems. They differed
only in the order of the problem; with “lighting conditions” exchanging place with
“drop-offs” to become the most commonly cited problems.
These ﬁndings suggest that the long cane or other electronic devices could be
used to detect curbs and ramps and help in negotiating steps and this is the case also
for people with low vision.
Table 1 Common pathologies and their functional problems
Pathology
Glare
Visual
ﬁeld
loss
Scotoma
Night
blindness
Light
adaptation
Fluctuating
vision
Depth
perception
Achromatopsia
x
x
Albinism
x
x
Aniridia
x
x
Aphakia
x
x
Cataracts
x
x
x
x
Coloboma
x
x
Diabetes
x
x
x
x
x
x
Glaucoma
x
x
x
x
x
x
Hemianopsia
x
High myopia
x
x
x
Keratoconus
x
x
x
Macular
degeneration
x
x
x
x
Optic atrophy
x
x
x
x
x
Retinal
detachment
x
x
x
Retinopathy of
prematurity
x
x
x
x
x
Retinitis
pigmentosa
x
x
x
x
x
x
Source Geruschat and Smith [10, pp. 63–83]
Orientation and Mobility Training to People …
249

Long et al. [20] had also described the difﬁculties caused by lighting conditions,
adaptations to different conditions of illumination, and the negative effect of
reduced lighting on mobility. These results also point to the need of devices to help
in the travel difﬁculties cause by varying conditions of illumination.
Most people with low vision are elderly, sometimes with other impairments, and
do not make effective adaptation to blindness as do younger persons. According to
Campbell et al. [5] older people with visual impairment have an increased risk of
falling. Other studies [6, 33, 31] showed that AMD has been associated with falls.
Orientation and mobility instructors assess the home environment and provide
environmental modiﬁcations to prevent falling.
Sengupta et al. [28] showed that patients with low vision due to macular
degeneration do not engage in physical activity nor do they leave their homes as
much as normally sighted individuals.
According to Glen et al. [11], patients with low vision due to glaucoma reported
problems relating to mobility (walking and driving), navigating around obstacles
(including steps and uneven ground) and their general interaction with people and
places.
Khan [16] showed that persons with low vision due to diabetic retinopathy often
experience difﬁculties with activities such as identifying faces, reading bus numbers
from a distance, reading small and low contrast print, intolerance to light, and
difﬁculty in moving outdoors.
According to Geruschat and Smith [9], most of the students with low vision
experience ﬂuctuation in the quality and clarity of what they see. Vision provides
more useful spatial information than other senses and pedestrians with low vision
get impoverished visual information and sometimes confusing information.
These results also point to the need of training in different situations of travel
difﬁculties. The pedestrian with low vision has to decide when the visual infor-
mation is reliable and sufﬁcient or when the other sensory information is more
reliable for travelling in a safe manner. When the pedestrian with low vision is
looking down most of the time instead of using his vision to get visual information
from the environment, it affects his security in navigation, and the use of a long
cane is proposed. Sometimes a combined use of low vision and the long cane is
needed. The cane helps to improve vision efﬁciency for orientation and can aug-
ment assurance and independent mobility. Some people with low vision use the
cane only for travel in heavy glare, or for night conditions, or in crowded places and
unfamiliar areas. The long cane can be an important tool for enhancing visual
efﬁciency. Geruschat and Smith [9] explain that one of the most important aspects
of instruction in low vision mobility is teaching the student the critical variables to
assess if the use of the long cane is necessary. People with low vision also use a
short cane as an indicator to sighted persons that attention should be given in the
presence of a person using such a device.
250
M. Goldschmidt

3.5.2
Training for People with Low Vision
The instruction in orientation and mobility starts often indoors in a controlled
environment and is then trained outdoors. Barraga and Morris [1] described a
program to develop efﬁciency in visual functioning and showed that people with
low vision can learn how to better use their remaining vision. Orientation and
mobility specialists work on enhancing visual experiences by working in a variety
of environmental situations.
Some people with low vision experience difﬁculties performing visual tasks and
need visual training to maximize visual efﬁciency and to develop the use of vision
for mobility. As the visual localization and visual identiﬁcation of objects and
people in movement can be challenging in changing surroundings, it is often
necessary to teach the low vision pedestrian to scan, to follow moving objects and
to interpret confusing visual information. Tracing, scanning and tracking are some
of the visual motor skills that the student with low vision has to develop in order to
maximize the use of his functional vision.
The student with low vision also has to develop techniques and strategies to
maximize safety and efﬁciency of movement. Instruction in low vision mobility
includes strategy like monocular depth and perception of visual cues that could help
him judging changes in elevation. People with central visual ﬁeld loss (for example
in age-related macular degeneration) have to develop eccentric viewing. Some
students automatically develop eccentric ﬁxation and some students need instruc-
tion in locating and using eccentric viewing. The training of the use of eccentric
viewing is necessary for a clearer view at trafﬁc lights, recognizing or reading street
signs, building’s and bus’ numbers. Students with peripheral visual ﬁeld loss (for
example in Retinitis Pigmentosa, Glaucoma or Hemianopia) have to be trained at
the scanning of objects.
Most of the students with low vision have to develop strategies for increasing
light and decreasing glare in order to maximize their remaining vision.
The use of optical and non-optical devices can improve orientation and mobility.
The use of sun glasses as well as a sun hat or a visor is proposed by orientation and
mobility specialists to improve vision under conditions of glare. To improve vision
under conditions of low light or no light, orientation and mobility instructors
propose the use of wide and bright beam lights’ sources. Night training in orien-
tation and mobility is very important to people with night blindness (for example in
pathologies like Retinitis Pigmentosa and Glaucoma). Although contrast sensitivity
and visual acuity are related, people with relatively good visual acuity might have
difﬁculties at nightfall and need some training in low contrast situations. Some
people with low vision travel in night time more safely than in the daytime and get a
short night training in orientation and mobility.
Telescopes are often used in orientation and mobility, as they can improve
distance visual acuity and can be used in orientation and mobility for recognizing
trafﬁc lights, reading street signs, numbers on buildings, and bus numbers. Using a
telescope improves visual acuity but reduces visual ﬁeld and reduces the amount of
light entering the eye. Orientation and mobility instructors have to train the use of
Orientation and Mobility Training to People …
251

optical devices and teach how to reduce blurred images, how to attain maximum
acuity and how to deal with restricted visual ﬁeld of the telescope. The student
learns how to localize stationary objects with the telescope and how to focus on a
moving object at a constant distance. He also learns how to follow an object in
movement and refocus continuously. Finally, he learns how to look for a detail on
moving objects. Fresnel and Pelli prisms can also assist students with restricted
visual ﬁeld and good visual acuity to increase visual scanning. Reversed telescopes
can help students with restricted visual ﬁelds and good visual acuity to widen the
viewing area.
The use of near vision optical and electronic aids can be helpful for navigation.
During orientation and mobility training, the use of near vision optical and elec-
tronic aids is trained also for reading maps, reading time tables or preparing lessons.
Many students with low vision need training in using visual and nonvisual
information together. They have to learn how to trust the nonvisual information and
how to integrate the use of both types of information.
3.6
Environmental Modiﬁcations
Environmental modiﬁcations indoors are proposed to facilitate orientation and to
enable safe travel. Some of the modiﬁcations include: reducing glare, improving
lighting, painting the edge of steps with a contrasting colour, avoiding environ-
mental hazards, etc.
Orientation and mobility specialists are also involved in environmental modiﬁ-
cations in complex pedestrian areas to facilitate orientation and to prevent accidents.
Some of the environmental modiﬁcations are the use of colour and contrast cues or
the use of consistent sign style and placement, the use of tactile signs, elimination of
wall-mounted objects and pole-mounted objects, making stairs safe and easy to
negotiate, etc. The use of technological solutions in pedestrian areas to enhance
mobility for blind and partially sighted people is very useful. For example, the
provision of an audible signal or the uses of linear surfaces to indicate crosswalk
locations are useful cues for ﬁnding a crossing in an unfamiliar area.
4
Electronic Travel Aids and Electronic Orientation Aids
The most prevalent devices that are used by people with visual impairment are the
long cane and the guide dog. As new electronic aids emerge, the cane and guide
dog are becoming part of a larger collection of potential tools adapted to the needs
of persons with visual impairments. Electronic travel aids and electronic orientation
252
M. Goldschmidt

aids can be used with a cane or a guide dog. The electronic travel aids and elec-
tronic orientation aids were developed so they can provide solutions to some of the
limitations of the traditional aids in orientation and mobility.
The choice of electronic travel aids and electronic orientation aids are often
determined by the users with the guidance of orientation and mobility instructors. In
order to use the electronic travel aids and electronic orientation aids, it is very
important for the student to have a strong foundation in orientation and mobility
skills.
Orientation and mobility instructors have to evaluate the travel needs and the
characteristics of candidates for using electronic travel aids and electronic orien-
tation aids. Smith and Penrod [30] present some characteristics of candidates to be
considered for using electronic travel aids and electronic orientation aids. For
example, the travel history of the candidate, his level of competence, his conﬁdence
with the cane or the guide dog, the amount of remaining vision of the candidate, the
physical ability of the candidate to utilize the device safely. Other considerations
are the geographical area in which the candidate travels, cosmetic acceptability,
subtlety of the signal output, attitudes and reaction of family and public. According
to the authors, “the obvious factors to be considered in matching a person to a
device are auditory discrimination ability, tactile discrimination, visual acuity,
motivation, and cost beneﬁt”. Orientation and mobility specialists need to be
involved in the design of electronic travel aids and electronic orientation aids and
make sure that the travel aids promote independent travel in any situation.
Electronic travel aids and electronic orientation aids may provide additional
safety, ﬂuidity of travel and awareness of landmarks. Penrod et al. [26] explain that
it is not easy to show the improvement in mobility performance by using an
electronic travel aid. Smith and Penrod [30] stated that electronic travel aids and
electronic orientation aids can offer advantages to many people with visual
impairment, but cost, requirement for training, lack of trained orientation and
mobility specialists, and the effectiveness of other devices have all limited the
number of users. According to Smith and Penrod [30] the ideal length of training
the use of electronic travel aids depends on several factors and it ranges from 20 to
120 h, depending on the student, the device and the travel environment.
The use of electronic aids in orientation and mobility can be useful for people
with visual and additional impairments.
4.1
Electronic Travel Aids
Electronic travel aids have the purpose of enhancing mobility for the blind
pedestrian. Blasch et al. [3] deﬁned electronic travel aids as “devices that transform
information about the environment that would normally be relayed through vision,
Orientation and Mobility Training to People …
253

into a form that can be conveyed through another sensory modality”. The electronic
travel aids send energy out into the environment and receive the reﬂected energy for
interpretation by the pedestrian.
According to the National Research Council (1986) as cited by Smith and
Penrod [30], the most essential information needs of the pedestrian are
1. Detection of obstacles from ground level to head height for the full body width.
2. Information about the travel surface
3. Detection of objects bordering the travel path
4. Distant object and cardinal direction information
5. Landmark location and identiﬁcation information
6. Information enabling mental mapping of an environment
Electronic travel aids, by giving audible or tactile outputs or both can provide
obstacle detection and can provide information about the travel path in advance.
Some electronic travel aids detect obstacles and landmarks that the cane might miss
and some electronic travel aids replace the function of the cane. As was described
before, the information that the long cane provides is transmitted at the moment of
contact with the object and not before. The use of electronic travel aids enhances
the preview provided by the long cane and might reduce anxiety and embarrass-
ment related to unwanted personal contact with objects or with people.
Furthermore, the electronic travel aids may provide information that would never
be available otherwise, for example the appreciation of the height and size of
buildings and trees. They can extend the student’s knowledge of self-to-object and
object-to-object relationships. The electronic travel aids are very useful for students
who are learning concepts (for example, congenitally blind children).
Farmer [7] describes two categories of electronic travel aids: primary and sec-
ondary. If the device could be used to replace a mobility system, for example a
cane, guide dog or human guide, it was considered a primary device (provides
surface preview and obstacle preview) and if it must be used in combination with a
cane or guide dog to ensure the detection of stairs, curbs and drop off, it was
considered a secondary device.
In order to use the electronic travel aids efﬁciently, it is very important for the
student to have a strong foundation in orientation and mobility skills. When the
student starts using the electronic travel aids, he usually gives his attention to the
device and might ignore other sensory information. Using electronic travel aids can
be tiring at the beginning and sometimes students might be frustrated and confused
if the training is too fast.
Penrod et al. [27] described a curriculum with a basic model of lessons for
teaching the use of electronic travel aids. The authors propose the following indoor
lessons’ sequence to teach the use of electronic aids by orientation and mobility
specialists:
– familiarization with the aid;
– development of general proprioceptive awareness of the distance setting of the
electronic aid;
254
M. Goldschmidt

– learning how to judge distance with the aid using the variability of pulse or
frequency information;
– development of accuracy in relating pulse of frequency to distance;
– using the electronic aid to align perpendicularly with a wall; using the electronic
aid in combination with the cane or a guide dog;
– learning how to detect and avoid overhanging objects;
– practice in travel situations the interpretation of multiple signals from the device;
using the electronic aid to locate landmarks;
– detecting doorways with the electronic aid;
– trailing a wall with the electronic aid (without using the cane or the ﬁngers);
– detecting intersection hallways with the electronic aid.
After the indoor lessons, the student gets outdoor lessons in more complex
environments. The student is trained in using the electronic aid to trail outside wall,
tree line or fence and to locate particular landmarks. Then the student learns how to
follow speciﬁc routes, identifying salient features that may be used as landmarks,
identifying and avoiding overhead and forward standing obstacles, detecting curbs
and steps; identifying and avoiding other pedestrians; and at the same time main-
taining safe travel techniques.
The authors explain the limits and the possibilities of electronic travel aids
according to the pedestrians needs. For example, electronic travel aids which use
tactile-haptic vibrations may be more suitable for a student who has a hearing loss
but does not have any problems with neuropathy, and electronic travel aids which
use audible pulses may be more adequate for student with neuropathy problems.
4.2
Electronic Orientation Aids
Electronic orientation aids are devices that give new possibilities for locating
destinations for persons who are visually impaired. Electronic orientation aids are
devices that locate the pedestrian within a global or local coordinate system. They
use the Global Positioning System (GPS) technology and can be very useful for
mobility, as it can identify and give directions to a speciﬁc address and can assist
the pedestrian with visual impairment in orientation. Such devices receive a GPS
signal and use it to compute the position of the pedestrian. The GPS is a supplement
to the long cane or to the guiding dogs and is not intended to provide detailed
information about the most immediate environment. Electronic orientation aids
provide a complement of orientation to other mobility devices. With the electronic
orientation aids, people with visual impairment can better plan their travel and
identify their location along their travel path. They can also travel independently in
various unfamiliar areas. A GPS can improve travel planning and reduce travel
anxiety. Guide dog users and cane users can combine the use of the GPS with the
guide dog and get more information about the environment.
Orientation and Mobility Training to People …
255

GPS accuracy can help the student arrive in close proximity to his ﬁnal objective
but cannot always announce destinations with extreme precision. The development
of GPS with increased precision is essential to aid people with visual impairment.
Many students ﬁnd the GPS useful also when travelling with a sighted guide or
with public transportation. The use of GPS requires practice and instruction and
students need to learn to use the system efﬁciently. They need to learn when to
listen to the GPS, what to listen for and how to use the information they get from
the GPS.
Many spatial concepts and skills can be trained using a GPS, and orientation and
mobility specialists use it to facilitate understanding processes. One has to mention
that although GPS is very useful in orientation, it cannot replace training in ori-
entation and mobility.
According to the curriculum for teaching the use of GPS presented by Penrod
et al. [27], the following lessons are proposed by the orientation and mobility
specialists:
– information about the GPS network and GPS receiver technology;
– familiarization with the GPS components;
– learning how to turn the GPS on and off;
– learning to shift the volume and control the speech rate;
– learning the names and positions of all keys;
– learning to wear and adjust the GPS;
– learning to establish and check the GPS signal status;
– learning to determine compass heading for the direction of travel;
– learning about point of interest and how to create personal points of interest;
– learning about travel (pedestrian and motorized) modes and digital maps;
– learning to locate personal points of interest;
– learning about the virtual mode of the GPS for planning and for learning new
areas;
– learning to identify trafﬁc intersections;
– learning to use a dialog box and to navigate menus;
– learning to navigate and change information in the dialog box;
– learning to edit points of interests;
– learning to create pedestrian point-to-point routes;
– learning to walk point-to-point routes;
– learning how to use the rerouting feature;
– learning how to create quick routes;
– learning how to change the GPS system’s settings;
– learning to maintain and adapt the GPS system.
The GPS does not work inside buildings because it cannot establish a connection
to the GPS satellites and it must be supplemented by other systems, like Wi-Fi or
Bluetooth, to locate the visually impaired person.
256
M. Goldschmidt

5
Electronic Visual Aids with Augmented Vision Systems
Electronic travel aids that enhance the use of vision can be very helpful in orien-
tation and mobility for people who have some residual vision.
Electronic visual aids were proposed for a variety of vision impairments. Luo
and Peli [22] developed a head-mounted displaying device for people with
restricted peripheral visual ﬁelds that was shown to signiﬁcantly improve user’s
search performance. Different augmented vision systems provided mostly magni-
ﬁcation and video contrast control. Other prototype devices provided image
enhancement and light ampliﬁcation for night vision.
These products are usually not recommended for use while walking because of
the limited ﬁeld of view of most devices and the difﬁculties adapting to high
magniﬁcation vision for constant use in mobility. Google Glass could be used
during mobility. Hwang and Peli [14] explored the possibility of using the Google
Glass as a visual aid for people with impaired vision, by providing augmented edge
enhancement. More research and development is needed in this area.
Tablets are accessible for people with low vision and are useful for gathering
information in orientation and mobility as the large screen makes text and images
larger. Using smart phones with cameras, GPS and compass apps are also helpful in
orientation and mobility.
Many applications (apps) were developed speciﬁcally for people with visual
impairments and can be divided into three categories: navigation, identiﬁcation and
reading. For example, some apps provide additional audible announcements and are
useful for cane or guide dogs users. These apps can provide information such as the
names of and distances to upcoming streets and intersections, business names and
addresses as you pass them, along with the distance remaining and the direction to
your destination. Colour identiﬁer and light sensor apps can help to get more
information about the environment. Some apps can be used to identify and describe
objects in the closed environments for people with ultra-low vision or blindness.
Head-mounted display devices are used after retinal implantation in people with
blindness. There are some commercially available products that provide visual
perceptions to blind persons. People with a retinal implant are trained by orientation
and mobility specialists indoors and outdoors to use their visual perception for
navigation. For example, they are trained to detect ceiling lights in a hallway for
improved walking, straight down the hallway, and to detect lights from windows
and open doors for better orientation in a room, and also to detect the line of a
crosswalk for preventing veering while crossing the street.
As mentioned earlier, the use of new technologies with people who have vision
loss and other disabilities like hearing loss, or physical and health impairments, or
cognitive impairments, might be very helpful. For example, the use of electronic
travel aid devices by people with visual impairment who use wheelchairs or
Orientation and Mobility Training to People …
257

scooters might provide obstacle detection and avoid hazards and drop-offs. Another
example is the use of electronic orientation aid devices with tactile output by people
with hearing loss.
6
Conclusion
The chapter describes the training of visually impaired persons in orientation and
mobility. The goal of orientation and mobility training is to prepare people with
visual impairment to travel in a variety of environments, both familiar and unfa-
miliar, and to increase the skills of moving in the environment in a safe and efﬁcient
way.
The chapter explains the speciﬁc mobility challenges of people with visual
impairment and the strategies and techniques used to improve orientation and
mobility. The use of mobility devices helping persons with visual impairment to
travel safely is also described in this chapter.
The most essential information needs of the pedestrian with visual impairment
are: detection of obstacles from ground level to head height for the full body width,
information about the travel surface, detection of objects bordering the travel path,
location and identiﬁcation of landmarks, and information enabling mental mapping
of an environment.
Although the cane and the guide dog are reliable mobility aids, they have some
limitations. The long cane cannot detect small objects or holes and overhanging
obstacles even when proper techniques are being used. Guide dogs provide minimal
tactile contact with the environment and might deprive the blind pedestrian from
information that could be helpful for his orientation. Electronic travel aids and
electronic orientation aids offer complementary aid to the traditional mobility aids.
By giving audible or tactile outputs or both, electronic aids can provide advance
obstacle detection, awareness of landmarks and information about the travel path,
and might give ﬂuidity of travel and reduce anxiety and embarrassment related to
unwanted personal contact with people or objects. Furthermore, the electronic travel
aids may provide information that would never be available otherwise, for example
the appreciation of the height and size of buildings and trees, and can be used by
orientation and mobility specialist in concept development.
The use of new technologies is an integral part of training in orientation and
mobility. New technologies have revolutionized the ﬁeld of orientation and
mobility by giving more information than ever before to the pedestrian with visual
impairment. Some of the devices are designed to help visually impaired pedestrians
to travel safely and quickly among obstacles and other hazards in complex and
unfamiliar environments.
As the majority of people who have visual impairment have some residual
vision, the development of electronic travel aids that enhance the use of vision in
orientation and mobility can be very helpful. Some devices were developed to adapt
image enhancement technologies to people with low vision and to help them
258
M. Goldschmidt

maximize the use of their vision in different situations and conditions. The most
difﬁcult mobility problems of people with low vision are the negotiation of steps,
curbs, ramps and lighting conditions and adapting to changes in lighting, distin-
guishing the colour of trafﬁc lights and crossing without trafﬁc lights, adapting to
the changes in terrain (broken or uneven sidewalks, street, and surfaces), and
obstacles (low-laying objects, head-high and low-hanging objects). As ﬂuctuating
vision is also a common functional problem of people with low vision, devices with
the capacity to adapt to ﬂuctuating vision and to different mobility situations could
be very useful. Electronic travel aids and electronic orientation aids may promote
independent travel in any situation.
In order to use new technologies as travel aids it is very important for the student
to have a strong foundation in orientation and mobility skills.
References
1. Barraga NC, Morris JE (1980) Program to develop efﬁciency in visual functioning.
Sourcebook on low vision. American Printing House, Louisville
2. Bentzen BL, Martin JR (2010) Teaching the use of orientation aids for orientation and
mobility. In: Foundations for orientation and mobility. AFB, New York
3. Blasch BB, Long RG, Grifﬁn-Shirly N (1989) Results of a national survey of electronic travel
aid use. J Vis Impair Blindness 83
4. Blasch BB, Wiener WR, Welshe RL (1997) Foundations of orientation and mobility, 2nd edn.
AFB Press, New York
5. Campbell AJ, Roberston MC, LaGrow SJ et al (2005) Randomized controlled trial of
prevention of falls in people aged >75 with severe visual impairment: the VIP trial. BMJ
331:817–820
6. Cruess A, Zlateva G, Xu X, Rochon S (2007): Burden of illness of neovascular age-related
macular degeneration in Canada. Can J Ophthalmol 42
7. Farmer LW (1980) Mobility devices. In: Welsh RL, Blasch BB (eds) Foundations of
orientation and mobility. American Foundation for the Blind, New York, pp 357–412
8. Genensky SM (1976) Acuity measurements—do they indicate how well a partially sighted
person functions or could function? Am J Optom Physiol Opt 53
9. Geruschat DR, Smith AJ (2010) Improving the use of low vision for orientation and mobility.
In: Foundation of orientation and mobility. AFB II, New York
10. Geruschat DR, Smith AJ (2010) Low vision for orientation and mobility. In: Foundation of
orientation and mobility. AFB I, New York
11. Glen FC, Crabb DP (2015) Living with glaucoma: a qualitative study of functional
implications and patients’ coping behaviours”. BMC Ophthalmol 15:128
12. Hersh MA, Johnson MA (2008) Assistive technology for visually impaired and blind people,
1st edn. Springer, Glasgow
13. Hill E, Ponder P (1976) Orientation and mobility: techniques. American Foundation for the
Blind, New York Press, New York
14. Hwang AD, Peli E (2014) Augmented edge enhancement for vision impairment using google
glass. SID Symp Digest Tech Pap 45:305–307. doi:10.1002/j.2168-0159.2014.tb00082.x
15. Jacobson WH (1993) The art and science of teaching orientation and mobility to persons with
visual impairements. AFB Press, New York
16. Khan SA (2007) Low vision rehabilitation and diabetic retinopathy. Saudi J Ophthalmol 21
17. Klein R, Chou C-F, Klein BEK, Zhang X, Meuer SM, Saaddine JB (2011) Prevalence of
age-related macular degeneration in the US population. Arch Ophthalmol 129
Orientation and Mobility Training to People …
259

18. LaGrow SJ, Weessies M (1994) Orientation and mobility: techniques for independence.
Dunmore Press, Palmerston North
19. Long RG, Giudice NA (2010) Establishing and maintaining orientation for mobility. In:
Foundations for orientation and mobility. AFB, New York
20. Long RG, Reiser JJ, Hill EW (1990) Mobility in individuals with moderated visual
impairment. J Vis Impair Blindness 84(3)
21. Loomis JM, Klatzky RI, Golledge RG, Philbeck JW (1999) Human navigation by path
integration. In: Golledge R (ed) wayﬁnding behavior: cognitive mapping and other spatial
processes. John Hopkins University Press, Baltimore
22. Luo G, Peli E (2006) Use of an augmented-vision device for visual search by patients with
tunnel vision. Invest Ophthalmol Vis Sci 47(9)
23. Millar S (1994) Understanding and representing space. Theory and evidence from studies
with blind and sighted children. Clarendon Press, Oxford
24. Montarzino A, Robertson B, Aspinall P, Ambrecht A, Findlay C, Hine J, Dhillon B (2007)
The impact of mobility and public transport on the independence of vision impaired people.
Vis Impair Res 9
25. Pavey S, Dodgson A, Douglas G, Clements B (2009) Travel, transport, and mobility of people
who are blind and partially sighted in the UK. Royal National Institute of Blind People
26. Penrod W, Bauder D, Simmons T, Brostek LD (2009) A comparison of selected secondary
electronic travel aids with a primary mobility system. Int J Orient Mob 2(1)
27. Penrod WM, Smith DL, Haneline R, Corbett MP (2010) Teaching the use of electronic travel
aids and electronic orientation aids. In: Foundations for orientation and mobility, vol II. AFB,
New York, pp 470–475
28. Sengupta S et al (2015) Evaluation of real-world mobility in age-related macular
degeneration, BMC Ophthalmol 15:9 http://www.biomedcentral.com/1471-2415/15/9
29. Smith AJ, De l’Aune W, Geruschat DR (1992) Low vision mobility problems: perceptions of
O&M specialists and persons with low vision. J Vis Impair Blindness 86(1)
30. Smith DL, Penrod WM (2010) Adaptive technology for orientation and mobility. In:
Foundations for orientation and mobility, vol I. AFB, New York, pp 241–276
31. Soubrane G, Cruess A, Lotery A, Pauleikhoff D, Monès J, Xu X, Zlateva G, Buggage R,
Conlon
J,
Goss
TF
(2007):
Burden
and
health
care
resource
utilization
in
neovascularage-related macular degeneration: ﬁndings of a multicountry study. Arch
Ophthalmol 125:1249–1254
32. Suterko S (1967) Long cane training: its advantages and problems. In: Proceedings of the
conference for mobility trainers and technologists, Massachusetts Institute of Technology,
Cambridge
33. Szabo SM, Janssen PA, Khan K, Lord SR, Potter MJ (2010): Neovascular AMD: an
overlooked risk factor for injurious falls. Osteoporos Int 21
34. Thinus-Blanc C, Gaunet F (1997) Representation of space in blind persons: vision as a spatial
sense? Psychol Bull 121(1):20–42
35. Turano KA, Geruschat DR, Massof RW, Stahl JW (1999) Perceived visual ability for
independent mobility in persons with retinitis pigmentosa. Invest Ophthalmol Vis Sci 40
(5):865–877
36. Turner A (1998) Mobility Skills. In: Turner A, Foster M, Johnson SE (eds) Occupational
therapy and physical dysfunction: principles, skills and practices, 4th edn. Churchill
Livingstone, London, pp 225–282
37. Warren D (1977) blindness and early childhood development. Am Foundation for the Blind,
New York
38. Warren D, Anooshian L, Bullinger J (1973) Early versus late blindness: the role of early
vision in spatial behavior. Am Found Blind Recherche Bull 28:191–218
39. Warren D, Kocon JA (1974) Factors in the successful mobility of the blind. Am Found Blind
Recherche Bull 28:191–218
40. Welsh RL, Blasch BB (1987) Foundation of orientation and mobility. American Foundation
for the Blind, New York
260
M. Goldschmidt

41. WHO (1992) International statistical classiﬁcation of diseases, injuries and causes of death:
tenth revision. WHO, Geneva
42. Wiener WR, Siffermann E (2010) The history and progression of the profession of orientation
and mobility. In: Foundation of orientation and mobility. AFB, New York. http://www.who.
int/mediacentre/factsheets/fs282/fr/
Orientation and Mobility Training to People …
261

Spatial Orientation in Children:
A Tyﬂological Approach
Krystyna Nawrocka-Łabuś
This chapter explains the cognitive and psychological basis of teaching visually
impaired children, especially when teaching them spatial mobility and spatial ori-
entation. The tyﬂological approach is proposed.
The whole chapter is organized as follows.
Section 1 addresses the concept of spatial orientation and its shaping in children.
Secttion 2 brieﬂy presents some methodological topics on the work with visually
impaired children when teaching them how to acquire the spatial cognition.
Section 3 encompasses some open questions of spatial knowledge of visually
impaired people (VIP).
1
Shaping Children’s Spatial Orientation
1.1
Spatial Orientation
Orientation is the process of using our senses to determine our own position and the
relationships between all major objects in the environment; orientation is to be
aware of where we are.
The term spatial orientation in physical space encompasses the control of the
body in relation to the environment, in relation to the location of objects, of people,
being aware of sizes, shapes, and objects in space.
Tyﬂology is the science of mobility; Tyﬂos is the Greek word for blind.
K. Nawrocka-Łabuś (&)
Ośrodek Szkolno-Wychowawczy dla Dzieci Słabowidzących i Niewidomych,
Dabrowa Gornicza, Poland
e-mail: k.nawrocka.labus@gmail.com
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_9
263

The term orientation deﬁnes the ability to recognize the space, primarily the
surrounding space. Being oriented in space means to be able determine the size and
shape of its components. Spatial orientation is the practical use of spatial
imagination.
The above deﬁnitions are similar and refer to the orientation of all people. The
differences that occur in the orientation of sighted and blind people rely on the fact
that sighted orientation involves primarily vision, while visually impaired people
(VIP) use other senses (hearing, touch, smell, kinesthetic sense and balance), as
well as the sense speciﬁc to blind people and deﬁned as the sense of mass.
Spatial orientation of VIP is a unique specialty. Mobility and orientation spe-
cialists are locomotion or mobility teachers (named also tyﬂologues after Greek
word “tyﬂos” meaning the blind person). Teaching the self-movement and the daily
activities is to not only teach self-reliance as a way to personal independence, but in
the same time the way to achieve a better quality of life.
Restrictions on the free and independent movement of blind people and cog-
nitive limitations are considered the largest and most visible barrier in human life.
Sightless navigation in space relies on:
– spatial orientation as the capability of exploring the surroundings and the
understanding of the space-time relations; a crucial role is played by cognitive
processes, concepts already known, knowledge of the body schema, spatial
imagination, knowledge about the environment and how to manage relationship
between time and distances;
– locomotion as the ability to move from place to place depending on the motor
features: agility, strength, speed, endurance, balance, and a number of skills
such as correct walking, running, maintaining direction, homing, and space
layouts understanding.
The practice requires learning of orientation and locomotion, which are mutually
dependent. The student personality plays an important role in learning orientation and
independent mobility. Not only intelligence is important, but also self-conﬁdence,
motivation, decision-making skills, mental strength, and past experience.
Achievements in spatial orientation allow:
– to move independently (without external help) which in returns helps devel-
oping spatial knowledge, the new emotions (acquire new qualias), and enables
to observe new phenomena, to realize live new experiences, to learn and to
exploit new experiences;
– to
experience
the
satisfaction
of
overcoming
difﬁculties,
to
develop
self-conﬁdence, perseverance in goal achievement, and to increase his/her
self-esteem;
– to become more efﬁcient and physically ﬁt, to gain better coordination, and
overall appearance, attitude, and his/her own way of functioning;
– to be independent, and thus having a chance to achieve better balance between
their private and professional life.
264
K. Nawrocka-Łabuś

The overriding long-term objective, which is to help VIP achieve their optimal
development, giving them and their relatives the greatest possible independence and
psychical comfort, is induced by several factors.
The number of factors and hierarchy of meaning will be different and personal.
Although the earliest possible introduction of classes aimed at teaching spatial
orientation and independent movement and the continuous stimulation, the devel-
opment of mental, motor, and physical of a visually impaired children is the
undeniably priority.
It is necessary to emphasize that there are no publications or studies which
evidence how, in the subsequent years, children understand and master orientation.
There are no scientiﬁc studies clearly stating which competencies in spatial ori-
entation children between three and seven year old possess.
As early as 1939, J. Piaget stated that in his/her ﬁrst years of life, the child is not
able to take the perspective of another person: they view the word through a
self-centered perspective. Everything that happens is because of them, the world
begins with them. They are at a stage when they are only able to conceive the world
through their own point of view (egocentric perception). The ability of children to
take the point of view of another person shapes slowly.
In the second stage of their development, children can deal with reality, taking
other points of view, and especially adopting the point of view of another person.
This means that the child can determine the position of objects in relation to any
reference point (an allocentric perception).
In the third stage of their development, the child can take the point of view of
things.
This process of social maturation is called decentralization by Piaget.
The development of spatial orientation abilities of visually impaired children is
probably linked to this process of decentralization, which allows objectiﬁcation of
the examined phenomena. The issue is the basis of separation of children’s spatial
orientation, and requires a methodology to shape the orientation from the point of
view of the child, and not from an adult. This difference should be taken into
account when designing orientation-teaching methodology for VIP children: one
should adapt the way orientation information is provided to the self-centered point
of view of VIP children.
Children’s world view is different from adults’. The meaning they attach to
space-related concepts can be different, e.g., a child distinguishes left from right,
but this is not enough to communicate. From experience, (s)he knows that it is
enough to rotate by 180° and everything is different. To cope with this situation, the
child reasons on the level of concrete operations. (S)He must realize that the
position of the object changes with the reference frame.
Children’s spatial orientation has two reference frames: egocentric and allo-
centric (or geographic).
In the egocentric frame, the child is in the center, focused on him/herself,
everything being centered around his/her own body. This corresponds to the pattern
of self-world. On average, children function with an egocentric orientation until the
age of nine. Egocentric orientation requires training at the micro level, and is used
Spatial Orientation in Children: A Tyﬂological Approach
265

in a small space—tactile space. Training has to be speciﬁc, related to a particular
situation (e.g., the action of drinking). We allow the child a great deal of inde-
pendence and to elaborate of his/her own strategies. Warren [1] claimed that to
“help your child is to convince him/her that (s)he does not need your help.”
Another guideline should be to avoid the child getting used to being helped for
he will start expecting assistance from his/her teacher in every difﬁcult situation. It
is thus necessary to show the way to the child, and not to do everything in his/her
stead. The teacher’s task is to help the child direct his/her attention to the external
world.
If the child sets his/her attention upon the teacher, it is necessary to adopt the
attitude of the child adopts: the teacher is the focus, and not the environment.
Independence is favored by short, precise instructions and by explicitly instructing
them to act independently.
Allocentric orientation aims to teach gradually the far space within the loco-
motion classes, and is associated with movement. This includes training in between
daily activities. This is the second level of training, training at macro-level, tran-
sition from eating activity to fun. This macro-level is implemented with activities
carried out at various positions within a common larger space (a large room for
example).
The spatial implementation of activities is correlated to three basic questions of
orientation:
1. Where am I?
2. What is my goal?
3. How can I achieve it?
Sighted persons, once they determine their spatial position, will pave the route
and will not have any issue with the execution of the navigation. A blind person
determines where (s)he is located, shall specify the purpose, imagine an itinerary to
reach the goal, but still has the difﬁculty of performing the task. Indeed, a VIP needs
spatial imagination; (s)he must be able to mentally represent the planned journey.
Indeed, the mental representation is a key element in the development of spatial
imagination.
“Imagination is more important than knowledge, because while knowledge
points to what it is, imagination points to what will be” (Albert Einstein).
Imagination emerges naturally from ties between sensory-motor perception, emo-
tions and memory.
It is possible to observe how the process unwinds by watching the reactions of
children listening someone reading them a book:
1. The child is focused on listening: (s)he connects internal images and emotions
with understanding;
2. When the adult ﬁnishes reading, the child will ask for him to read it again and
again: (s)he needs consistency and stability to integrate images and to develop
their imagination;
266
K. Nawrocka-Łabuś

3. Then the child will tell the same story; his/her speech embodies the story in
motion and sensation;
4. Finally, the child plays a story theatrically. The physical acting allows the child
to understand the story with his own sensory perceptions.
The development of spatial imagination can be encouraged by asking children to
imagine and narrate his/her own stories.
In the therapy, children are a special group, because of the possibility to initiate
the development potential. Relevant stimuli cause the biggest changes in devel-
opment and have the strongest impact in childhood; later, their importance greatly
reduces, but the process of acquiring individual experience is still there.
The therapeutic practice contributes to the design of new technique level and
tests to evaluate the child’s development. However, even the best methods will not
yield positive results, if they do not meet the basic psychological needs of the child,
such as the need for familiar links, security, and for continuous presence of an adult.
Satisﬁed psychological needs create facilitating conditions for the development of a
structured sense of “me.”
If the needs are not satisﬁed, it will result in accumulation of unpleasant expe-
riences potentially leading to tension, guilt, a feeling of injustice, a lack of
acceptance, expanding over defenses, and a tendency to develop a feeling of losing
control over the environment, but above all, it will inhibit the development of the
child’s cognitive activity, which will limit the exploration capabilities and curiosity
of the child thus potentially starting a pathological process. Lack of experience can
cause a deformation of the person’s self-image, leading to the formed self-image of
a person helpless or to the inhibited development of the autonomy; this latter has a
close relationship with the child’s cognitive development. Therapeutic work is a
speciﬁc emotional and intellectual relationship between an adult and a child.
The child, if treated as a subject, will ﬁnd the conditions for proper development,
including the development of the structure of “I.” This is an important premise that
affects the processes of learning and socialization.
Every child is different and requires different interactions. There is a difference
between teaching a congenitally blind child and late blind one who was able to
move independently for a time, and can refer to this experience and the memories it
produced. The congenitally blind children will have more difﬁculty understanding
the (spatial) concepts and may have different idea of space. Learning spatial ori-
entation dependents also on the degree of blindness and of the occurrence of visual
impairment.
An important task for the therapist is to prepare rehabilitation program adapted
to the child’s, with the ability to record the changes taking place because of the
regular classes. It is important that members of the therapeutic team closely
cooperate in the exchange of experiences, to continuously consolidate the acquired
skills in a various situations, including the family environment of the child.
An individualized therapy plan should include:
• information about the child and pedagogical records,
• analysis of medical, psychological and pedagogical records,
Spatial Orientation in Children: A Tyﬂological Approach
267

• a collection of information from parents and guardians,
• analysis of the child’s life, including the assessment of his living situation (e.g.
type of parents’ attitudes toward the child).
It is possible to design your own research tools, e.g.,
1. Daily clock—parents register the child’s daily activities and their temporal
occurrence; for older child it is possible to register data through the child’s
dream games.
2. The weekly “bank activities”—this register includes the child’s planned and also
effectively carried on activities during the week. An analysis carried out with
parents’ participation can allow for more insightful conclusions (e.g., the child’s
life is too poor, monotonous, or vice versa, the child’s life overﬂows excess of
classes, running between lessons music, languages, swimming pool, and others
at the expense of leisure and welfare).
3. Develop a sheet of skills mastered and of these which should be taught to the
child.
The efﬁciency of the used tool should be assessed with the following criteria:
• the child’s interactions with his/her parents, with adults and with other children;
• the degree of physical involvement and environmental curiosity, and the extent
of autonomy of actions;
• the posture, the transitions actions (sitting down and standing up), the move-
ments, the coordination, the precision of movements;
• the orientation in a familiar environment, e.g., the child’s room;
• identiﬁed habits and preferences of the child (what (s)he likes, what he/she does
not);
• the objectives for child’s needs and their priorities;
• the easiness to establish a contact with the child and to build emotional ties;
• the use of various methods of work, with a relaxed approach of learning.
Identiﬁed tasks and skills to be learnt must be clearly identiﬁed, and the child
should focus on them and reach the goal. Skills development needs to be based on
the already acquired knowledge. It is important to shape not only the verbal
communication but also facial expressions, gestures, and changes in activity.
It is necessary to elaborate an approach which allows the child to know how to
express what (s)he understands. The work style must be uniﬁed for everyone
working with the child, parents included. A multisensory learning environment is
fundamental, as well as the safety of the physical and mental health of the child.
The family environment is a key element of the success; in order to favor the
acceptance of the disability by child, psychological help and family therapy may be
useful.
268
K. Nawrocka-Łabuś

1.2
Range of Corrective and Compensatory Actions
A blind child should master a number of skills and knowledges, which are the basis
for learning spatial orientation skills, e.g., independent and protected movement,
assistance of a guide, or usage of a white cane.
The skills which should be developed are the following:
1. knowledge of the capacities of their own body,
2. sensory and kinesthetic learning,
3. sensory memory,
4. stability of the object,
5. sense of distance,
6. efﬁcient detection of obstacles.
Knowing one’s own body leads to the formation of self-identity, self-isolation
from the environment. A man (women) must feel and know his (her) own body, its
components and capabilities. This knowledge emerges around the age of three.
Main stages of this knowledge’s development are the following: perception of your
own body as a whole, naming of body parts and knowledge of their role and
possibilities during interactions, and consciousness of the possibility to control our
body, its movements, and, as a result, our own behavior.
Kinesthetic and sensory trainings are designed to teach the correct understanding
and interpretation of sensory information extracted from the environment and one’s
own body. This is, once again, an opportunity to analyze thoughts, emotions, and
mentally pictured space when moving. Neurosciences provide a set of practical
exercises for kinesthetic and sensory trainings.
The main component of the experience is the sensory system, or more precisely,
the information acquired through it. The stimulations are generated by the envi-
ronment (and perceived by our eyes, ears, taste buds, touch mechanoreceptors) and
by the inside of the body.
Our senses begin to develop in the ﬁrst two months of fetal life. When we think
of our sensory organs, we consider the ﬁve main senses: hearing, smell, taste, touch,
vision; all of them contribute to the development of balance and to the perception of
gravity, all of them allow us to develop our perception of the world.
Later, in order to obtain a stable and structured perception of our environment,
data from all senses are combined in complex sensory images. As babies touch
everything in their environment, they learn the dimensions, area. Much later,
around the age of eight months, a true visual image appears. Touch is very
important for vision.
Psychologists compare touch actions of VIPs with visual actions of sighted
people; they highlight the signiﬁcant differences in the efﬁciency of one and the
other way of learning. However, a synthesis of the touch data, which are analytical
and synthetic, gives “a picture” that can be related to visual observations.
The ﬁrst sensory system that fully develops during the ﬁfth month of fetal life,
the vestibular system, controls the sense of movement and balance. This system is
Spatial Orientation in Children: A Tyﬂological Approach
269

in charge of maintaining static balance, which refers to information to knowing and
keeping the body’s equilibrium in absence of movement and is also responsible for
dynamic equilibrium, i.e., maintaining the body’s position in response to sudden
movements (acceleration, release, upheaval). From birth to the ﬁfteenth month of
life, the vestibular system is very active. It stimulates every movement of the child.
The movement activates the relationship between the vestibular and brain cortex.
This is very important for the learning process. If we do not move, the vestibular
system is inactive, we do not receive information from the environment. Starting
with the knee-jerk reactions at the birth, the baby during the ﬁrst year of his life
learns a lot (standing, walking, running, etc.) taking into account gravity. Later, (s)
he learns a range of activities like any sighted child: walking up the stairs, over-
coming obstacles in the playground, etc., activities that require a good sense of
balance.
For visually impaired children, it is advisable to avoid restrictions (e.g., riding a
scooter or a tricycles, skateboarding). Such activities allow for contact with the
ground, provide joy, show that (s)he is the same as sighted peers. Kinesthetic
training is of great importance for the orientation. The term “kinesthetic sense” is
understood as the efﬁciency of the control of muscle and limb movement in relation
to the body, and other points of reference. VIPs have mobility issues of various
types (uncertain walking, lack of coordination, reduced efﬁciency of movements,
etc.). Furthermore, the plotting of airborne ﬁgures, on a horizontal or vertical plane,
maintaining their hand in the same position, or rotating it by 90° are of major
difﬁculties for VIPs. The aim of the training program is to teach the most appro-
priate, time effective and precise movements. The exercises may consist in carrying
out controlled movements, in any settings, and performing simple manual move-
ments, relevant to a given situation.
The implemented exercises could consist in writing in a plane or performing
speciﬁc movements in the air, e.g., following the shape of geometric ﬁgures. Such
exercises improve the efﬁciency of maintaining the width, height, and arches while
using the white cane. This would allow for independent rotations of 90° and 180°,
and also help maintaining the same stride length when walking, in order to obtain a
smooth gait and a relaxed posture. Exercises of equal length step can be implement
using tiles distributed in a room, or by going up and down using the stairs.
1.3
Touch Sense
The skin is the biggest organ of the body. It is ﬁlled with various mechanoreceptors
reacting to variations in pressure, temperature, pain, etc. These information will
allow the brain to decide whether or not to change the body’s position.
The feeling to be touched increases the activity of the nervous system.
The lack of the touch may reduce the human motor capability and may affect
mental health. Touch immediately after birth stimulates the sensory nerves’ ends. If
the nerves ends are not activated, the stimulation of the cortex decreases, leading to
270
K. Nawrocka-Łabuś

impaired muscle development, impaired sensory perception, emotional distur-
bances, and learning difﬁculties. The insufﬁcient development of touch can inhibit
the growth of nerves to such extent that it can lead to impairment of the vital
functions of the body. The touch, even negative one (e.g., beating or ﬂip), is
preferred over the lack of the touch. Touch contact is a natural and integral part of
life. The touch training starts at birth.
Training of touch can be based on the performance of the passive exercises and
active babies from the ﬁrst month of life. Different exercises are proposed in
“Training Baby” by Edward Franus and Barbara Franus-Urbanczyk. These are
passive exercises intended for the infant from the ﬁrst months of life, and they
prepare and develop physical skills in the following months. The exercises are
natural, simple, and attractive to the babies. They are very well adapted to the
various phases of child development and progresses of motor development, and
include the psychic contact with the child.
The medical approach to spatial orientation via touch includes differentiation of
textures, search for touch similarities and dissimilarities. The training uses everyday
objects. A direct touch is predominant; it is advisable to build the ranks according to
the speciﬁed criterion, e.g. roughness, hardness, etc.
The next stage includes exercises of object recognition and identiﬁcation from
tactile perception. The perimeter touch technique is the usual approach: it consists
in exploring objects with ﬁngertips, and includes several exercises to prepare for
learning Braille.
The second technique, named “cartographic grid” is used situationally, e.g.,
when searching for dropped objects, when exploring unknown spaces (rooms,
apartment, etc.). Gradually, object recognition techniques involving all body parts
via touch are introduced; e.g., recognition of the ground’s texture with the feet,
comparing his/her height with another person, measuring the length of a feet,
measuring the ulna length, etc.
In the next stage we start “training the indirect touch.” These techniques allow to
recognize (and thus to acquire the knowledge of) an object using another object
held in the hand (such as cane for children). Children touch objects, analyze their
experience, and recognize the object they touched indirectly. These exercises pre-
pare to properly use the white cane and train them to efﬁciently receive tactile
stimuli through an “extended hand.”
1.4
Hearing Sense
The fetus moves spontaneously from the twelfth week. It is surrounded by the ﬁrst
models of sounds, which are the mother’s heartbeat, her breathing and her voice. In
the ﬁfth month, (s)he reacts to the phonemes of the language spoken by the mother.
After birth, the sense of hearing of a newborn is very accurate and important, as it is
his/her ﬁrst line of defense. The child will react to the sound of an unknown source,
especially if it is loud and violent, by screaming in order to call for
Spatial Orientation in Children: A Tyﬂological Approach
271

help. Throughout our life, hearing maintains vigilance of our brain to sort incoming
content.
Hearing carries information important for safety. The interpretation of envi-
ronmental stimuli by the VIP is extremely important in the orientation of the
surrounding world. Auditory training includes exercises to locate sound sources,
differentiation of sounds’ intensity and interpretation of sounds’ content. The ability
to deﬁne and differentiate sounds is most useful for the purpose of orientation. It is
important to distinguish the sound signals generated by the ground and transmitted
by the white cane. Exercises are made using natural sources of sound and recorded
sound situations.
1.5
Smell Sense
The sense of smell is excellent at birth. Sixth weeks old babies can distinguish
between the smell of the mother’s breast with the scent of another.
Smell is associated with memory and plays an important role in learning in
infancy and throughout life.
Smell is also used to warn against dangers.
1.6
Sense of Taste
Taste receptors are located in the taste buds on the tongue. The range of taste
sensations is between bitter, salty, sour, and sweet. Mouthfeel arise after irritation
stimuli (e.g., with food).
It is necessary to be careful with children, especially at a time when they put
various items in their mouths, which has the largest collection of touch receptors, in
order to recognize them. Indeed, objects may have adverse effects (e.g., chemicals,
drugs) or may cause injury to the child’s mouth and to surrounding areas.
Tasting allows you to explore foods, fruits. Training the taste is important while
learning activities of the everyday life.
1.7
Some General Comments
1.7.1
Memory of Senses
Our senses are largely dependent on memory.
The basic memory patterns are formed when we use our senses and the more
efﬁciently we learn to use them, the more we are able discriminate very close
stimuli values. Well trained senses allow us to experience our surroundings better
272
K. Nawrocka-Łabuś

and faster. The memory is a free system which facilitates the retrieval of data from
all the areas of the brain. Therefore, in order to remember something as precisely as
possible, it is best to combine physical sensations, sensory and emotional stimu-
lations. The term “sensory memory” is a theoretical construct, but useful and used
to determine the efﬁciency of memorizing speciﬁc sensory information, which acts
as clues and landmarks for spatial knowledge.
Based on these data, the VIP will be able to maintain or adjust his/her orientation
in the environment. (S)He associates the audio, tactile, olfactory, and kinesthetic
information with speciﬁc situations, places, conditions, etc., which act as orienta-
tion cues. Training sensory memory will include a range of speciﬁc messages to
master, e.g., the memorizing of speciﬁc sound signals generated by the environ-
ment, identifying known places, localizing objects in the environment (thus
allowing for their recognition by touch) and will allow to identify the differences
between two places. The auditory, olfactory and tactile information, combined with
the knowledge of the position of ﬁxed points where information is generated (or is
available), assist the understanding and interpretation of spatial differences in dif-
ferent temporal conditions (such as weather conditions).
1.7.2
Stability of Objects
Object stability is linked to conceptual learning. This means that the general rules
remain unchanged, regardless of the changes in the environment. If a VIP, who
recognized a chair with his/her ﬁngertips when the chair was located in the corner
of the room, cannot identify the chair with the cane, if the same chair is located by
the window, this means that the VIP has perceived the latter information as dif-
ferent, and this situation needs correction. Formed ideas are wrong, the received
data induced a one-dimensional image based on the sensor data from one analyzer
(one sense).
We must teach VIP to perceive the invariability of the object independently of
their spatial context and changing spatial relationships. This knowledge is related to
multisensory perception of the subject, i.e., object recognition through multiple
senses. Data provided by different senses are heterogeneous. After the training, just
one information is necessary to identify the object, but also a change of one of the
characteristics will attest to the fact that we are dealing with another object. The VIP
is taught to recognize one object thanks the heterogeneous data it provides and to
categorize and to classify the cognizable objects.
Regardless of its position, the VIP using multimodal knowledge of this subject,
should allow its recognition. For young children, the situation is even more difﬁcult,
e.g., young children did not yet developed the concept of mass. If we use the same
quantity of plasticine to make a ball and a cylinder, and if we ask a child: where is
more plasticine, the child will answer according to the length of the object (i.e., (s)
he will say “cylinder”). It is only after their seven-eight years, that child can answer
this question correctly.
Spatial Orientation in Children: A Tyﬂological Approach
273

1.7.3
Sense of Distance and the Notion of Ego-Motion
The concept of distance refers to the capacity to perceive spatial relationships
between objects. Having a sense of distance means that the object is located in the
nearby space, and some distance separates us from it.
A distance can be measured in usual metric units, or a number of steps between
us and the object, if the object is not directly within our reach.
The concept of movement refers to our ability of noticing changes in body
position in relation to any point in space. This requires to realize that either
something is moving toward me or I am moving with respect to a speciﬁed ﬁxed
object.
The teaching of ego-motion perception starts from a situation where the VIP is
close to an object, e.g., a wall. Next, a set of various items, located either close or
far away, yet still within, are introduced to be directly grasped by the VIP. Through
touch, the VIP estimates directly his/her distance from the object.
Exercises for distance evaluation using hearing cues start in a closed room.
The VIP compares the distance separating him/her from the source of sound thus
manipulating the concepts of “closer” and “farer.” In the open space, the VIP uses
natural sounds, e.g., the noise of the street trafﬁc or the noise of the streets inter-
section in order to localize and orient in the space.
1.7.4
Sense of Obstacles (Mass)
Literature proposes many theories on how to deal with obstacles. Dolański [2],
suggests that the VIP receives acoustic waves reﬂected from obstacles in the form
of touch. Other authors emphasize that the sense of obstacles may be acquired
through learning using appropriate techniques.
In practice, several exercises are implemented using movable obstacles and the
trial and error methodology. Exercises are executed in both indoor and outdoor
spaces. The VIP is told that his/her speciﬁc route, on which (s)he practice, may
have obstacles. The VIP repeatedly wanders through the route, and stops when an
obstacle is detected. The obstacles’ layout changes several times during the training
session.
2
Selected Methodological Topics: Work
with a Visually Impaired Child
2.1
Games and Fun
“In the early years of child development fun is almost identical with real life.
Having fun is in second most important need after the need to be fed, protected and
274
K. Nawrocka-Łabuś

loved. It is also an essential component of physical, intellectual, social and emo-
tional development” [3, 4].
While playing, a child learns the world and how it is organized. Between the age
of 2 and 5 children are at an important stage of their cognitive development,
because (s)he learns to process information and to expand it in the work. Interactive
communication and fun with his/her peers accelerates this development.
It should be noted that nowadays, children do not have the time or opportunity to
play regularly without the necessary supervision and mediation of adults. The
variety of games and their appeal should provide children a lot of joy and pleasure.
Games should be selected according to child’s needs, age, and level of ﬁtness.
Visually impaired children in their movements, games and making noise should not
be unduly constrained by adults. Toys for blind children should be safe, attractive,
and should emit sounds. Visually impaired children must learn more than sighted
children.
The activity must be at the center of each game. It is necessary to show that
behind a game there may be something better, more interesting. The variety of
games, their nature, whether they are played solo or in a group, applies to all
children. Fun child must be geared toward achieving the objective. Perception and
coordination are developed by fun and physical activities, often combined with
music, relaxation and controlled breathing.
It is important that the child follows the rhythm of the music, knows and exe-
cutes the recommendations suggested by the songs. It is often better to give up the
CD recordings and sing along with the children. Manipulating objects, building,
playing with plasticine, and any other didactic play that will teach them to be
self-sufﬁcient when performing an action must be simulating and (auto-)correcting.
Experience in working with children shows that excellent course of action is lying
board games with your child. Each game is fun, and not every fun is a game.
Children’s behaviors reﬂect the fun they are having during the playground and
thematic plays. Thematic and building games should be implemented in parallel.
Agreement in force in these pastimes refers to speciﬁc situations that the child is
trying to play. A game can be played alone or in a group with other children. In
group, one child plays a dominant role, and the others must respect his/her
authority, otherwise a conﬂict may rise. In the case of games, all partners have the
same chance to win and all must respect the predeﬁned rules. A game is a situation
of equal chance for all, a chance for competition, which requires effort and constant
desire to achieve a goal. Success is built on the game’s understanding and on
anticipation. While playing, the child may learn more self-control, develop reﬂexes,
orientation, speech, thinking memory, and interpersonal skills.
Bought board games are not recommended for blind children, because their
instructions are too complicated and their charts illegible. Methodology for the
construction of board games is tailored to the cognitive abilities of children from
ﬁve to nine. Analysis of the rules of games with a child ensures his involvement and
the games’ adequation to his/her skills.
The construction of a game has several stages and each is accompanied by
emotions.
Spatial Orientation in Children: A Tyﬂological Approach
275

The ﬁrst stage (for the children of 5-9 year old) brings meaning to the game type
“race.” The game is dominated by an adult, but a child cannot loose. Each game
should propose only one activity.
The second step shows how to build a story in game.
The third stage involves the construction of a variety of games; powerful
mathematical and computer science tools may be helpful. The beneﬁce of games is
to improve the precision and efﬁciency of manual and intellectual skills resulting
from the content of the game.
2.2
Relaxation Techniques
Relaxation is the elimination of negative emotions. The goal of relaxation for
children is to relax muscles, to improve peripheral circulation, to abolish internal
tensions, and to give the child a sense of calm. Relaxation technique by Jakobson
[5] (in its modiﬁed version by B. Kaja) aims to carry out activities via loosening and
tightening the muscles while having fun, which the author calls the “weak-man and
strong-man” game.
Example: a child identiﬁes him/herself with a small ant, relaxes muscles, then (s)
he identiﬁes with the big elephant, heavily straining muscles. Exercise should
include all parts of the body: arms, legs, trunk, neck, and head. It is better if the
relaxation of the children is in the supine position, which increases the feeling of
safety.
Relaxation training based on autogenous Schultz approach [6] can be adapted to
small children (cf. [7]). Children relax through different approaches, e.g., the lis-
tening of the story of tired Little Red Riding Hood, they identify themselves with
the heroes of fairy tales, and make suggestions to the therapist. The story may be
accompanied by a soft music. Children should be comfortable in a room devoid of
additional incentives (such as a noise).
2.3
The Method of Sound Symbols (a Music Therapy)
The method is based on music and movement; it is suggested for children from 5 to
6 years old. Suggested activities are divided into different exercises ranging from
mobility, providing awaking of the whole body, with voluntary movements, con-
trolled movements and manual movements. Children move according to the rhythm
of music, then they activate (via dancing or via running), then they stop suddenly in
order to control the emerging setting.
Training of cognitive processes develops thought processes. Those exercises will
help the child to notice the characteristics of a sound (low/high, short/long,
quiet/loud), and link them to some symbols (abstract knowledge).
276
K. Nawrocka-Łabuś

Those exercises can also help to recover from emotional tension; this training is
a combination of speech, music, and movement, and may use poetry too. It is also
possible to use fairy tales illustrated musically, and apply them later to staging
exercises or pantomime.
2.4
Method Based on Painting
The method of painting with ﬁngers has therapeutic values, and relies upon the
tendency of children to dip their ﬁngers in substances with the consistency of mud.
The substance (paint) should be in cups adjusted to the size of a child’s hand. (S)He
paints on sheets spread on easels or tables with access from each side. Children can
paint on any subject, and inform the adults once (s)he completed his/her work, then
talk about its content. An additional advantage is to sensitize ﬁngertips, essential for
the exercise of touch of a blind child.
2.5
Drawing
Drawing and painting are human natural means of expression and communication,
in a similar way as talking, singing or gestures. Blind child can develop these means
of expression if they are offered the relevant technical possibilities to draw and to
control their work by touch feedback. Drawing is a common way to communicate
information, it is a language in itself.
For a visually impaired child the apprehension of the shapes of objects and the
identiﬁcation of relationships between linking these objects directly, from the touch
feedback, are very difﬁcult tasks. Therefore, it is necessary to perform exercises
exploiting the concept of a spatial plane. This includes not only drawing and
analysis of drawings, but also, and perhaps above all, exercise aiming the appre-
hension of orientation in a small space. Learning and consolidating concepts of
spatial relationships in a small space is a prerequisite for proper understanding of
the concepts and spatial relationships in a large space.
2.6
Maps in Relief
Relief maps facilitate the movement of VIP and are an important working tool for
professionals in spatial orientation. They provide knowledge about the environ-
ment, and allow to learn concepts, spatial relationships and spatial conﬁguration of
speciﬁc areas. Maps, plans, diagrams, and sketches are useful at every step, in every
area of life and every level of education.
Spatial Orientation in Children: A Tyﬂological Approach
277

Visually impaired people have a hard time accessing and learning how to pro-
cess information from maps and charts used in everyday life. In Poland, the tyﬂo
carthographic versions of maps were in beta-tests for many years in order to assess
their quality; teachers themselves usually made copies of maps in relief for their
own purpose. Often teachers beneﬁted from the relief maps produced in Germany.
In 1987 the Polish Association of the Blind initiated the production of relief maps
using the vacuum technique (rather expensive), leading to maps with a static
content. The next step of mapping technology was based on a matrix in order to
obtain the protrusions needed to touch stimulations. In 2003 the Centre for Blind
Children in Owińska implemented a tyﬂographic program (professors Alina
Talugder and Marek Jakubowski) for accommodating the educational needs of
blind people. Electronic matrix for maps display was realized; its originality comes
from the possibility to modify the map content dynamically.
This new technological approach allowed to elaborate maps of various spatial
precision (possibility of “zooming”). Therefore, tactile maps can be adapted to
various needs and to the touch stimulation perception of different end users; it is
also possible to adjust the contents of the map, i.e., increase or decrease the amount
of details.
The advantage of the above solution is the easiness of creating such map and the
possibility to exchange those maps in electronic form between the institutions
involved in teaching VIPs.
The spatial orientation classes include the preparation of outlines, plans, maps,
hand-made foils, relief or Braille maps of a particular route. Often this is done with
the student setting his/her own landmarks and information. In order to start to learn
spatial organization, children can create a topographical map of a room or a
building.
It is also possible to use dedicated heating devices to highlight any content,
drawings and maps or to use the “inﬂating paper” (paper capsule, manufactured in
Norway, Fig. 1); under high temperature, the black parts of the sheets (colored with
a special pencil) will increase their in volume, swelling and forming bulges.
2.7
The Method of Developing Movement
Movement is a developmental factor but also a mean to treat developmental
disorders.
The method presented here was deﬁned by Sherborne [8] and is derived from the
concept of Laban [9], who believed that knowledge of one’s own body is of a great
importance in building relationships between people, gives a sense of security, and
enables the recognition of space, where the child is located.
Sherborn’s method is based on the natural needs of the child to establish close
physical and emotional contact with parents (called romping). The basic aims of
this method is to develop body awareness through movements and improving
278
K. Nawrocka-Łabuś

mobility, and spatial awareness, especially to detect if other people are present
inside the same space in order to communicate with them. More than just motion,
this method helps developing other important elements such as the analysis of
kinesthetic sensations, the feeling of balance and provides also tactile sensations to
the VIPs.
When the child meets other persons while exploring, the known space becomes
more familiar, safer, allowing the child be more active and creative. Different
categories of movements can be developed: the movements leading to the knowl-
edge of your own body, movements to help shaping the relationship with the
physical environment, movements leading to the formation of interactions and
connections with another human being, movements leading to teamwork, and the
creative movements.
2.8
Brain Gym
The “brain gym” is a study of blood’s circulation. Masters of applied kinesiology
Alexander, Rudolf Laban, and Milton Traeger proposed this method [10].
Brain Gym is a system of mind–body integration which uses brain gymnastics to
strengthen bilateral functions and skills in the middle of the brain, where both
hemispheres work together.
Student assimilates the experience of both hemispheres. Assumptions of edu-
cational kinesiology are the following:
Fig. 1 Map used during the
orientation class (adapted
from TyﬂowySwiat [11])
Spatial Orientation in Children: A Tyﬂological Approach
279

• the whole body is ready to learn
• physical exercise stimulates brain functions
• stress inhibits learning.
When under stress, the mind-body’s activity is focused on preparing the ﬁght or
a response during the ﬁght (aggression or anxiety). As a result, the memory is
locked and abstract thinking and reasoning are signiﬁcantly inhibited.
Kinesiology exercises are done easily due to their naturalness; with a little effort,
exercises give a feeling of satisfaction. Exercises are divided into four groups:
• movements, which allow to cross the center line (they stimulate both large and
small motor skills);
• muscles stretching exercises (they tell the brain that the person is relaxed, calm
and ready to go);
• energizing exercises (they provide the necessary speed and intensity to the
neural processes);
• exercises to train your attitude (they lead to a deep positive attitude or affect the
emotional limbic system of the brain which interacts with the centers of per-
ception of ourself).
2.9
Kinetic Therapy—Treatment of Movement
Movements and exercises, as natural stimuli, can affect the body, contributing
positively to the activities of all organs and can thus increase or restore physical
ﬁtness. Therapeutic exercises are conducted by specialists during physical therapy
while also teaching techniques to caregivers and parents, are very important con-
sidering their privileged relationship to the children.
Exercises are divided into:
• local exercises (e.g., impacting on speciﬁc groups of muscles or increasing a
given range of motions);
• exercises improving overall ﬁtness (they aim to improve the overall physical
performance or relaxation);
• passive exercises (performed by the therapist);
• active exercises (performed under the direction of, or with the help of a
therapist);
• speciﬁc exercises (e.g., breathing, coordination, and relaxing exercises);
• compensatory gymnastics (a resource of exercises that compensate for quanti-
tative and qualitative impairments of movement);
• corrective exercises which are intended to improve what has been damaged,
(e.g., improving posture).
280
K. Nawrocka-Łabuś

3
Conclusion
The system of education in the ﬁeld of spatial orientation is based on the use of the
possibilities inherent to human body. You have to learn the skills of learning and
the joy of learning.
The purpose of corrective and compensatory training is for the VIP to explore
the world the world and function in it without vision but through touch. This new
mechanism is very different than how sighted people explore the world (which is
90% based on sight), but certainly not worse.
The teachers of spatial orientation create and organize the learning process, and
their personality; they play a very important role in the success of the training
program. The training can be fully individualized, and will involve alongside the
child’s development to match his abilities and needs. The training program is based
on daily life and speciﬁc situations. A progressive inclusion of well selected con-
cepts will facilitate the acquisition of information our various senses. Recalls must
be accompanied by the ﬁxing of meaning and an experience of emotions. Children
need move, play and exercise a lot; letting them experience freely, stimulating their
curiosity, and directing their attention to the surroundings is paramount.
It is necessary to gradually guide children and replace the conventional tech-
niques of sighted people by techniques of spatial orientation adapted to their dis-
ability, e.g., accompanying the child (traveling with a guide), protecting them with
speciﬁc equipment when they travel autonomously, and providing them with
techniques to look for dropped items.
Since the beginning of classes of spatial orientation it is mandatory to associate
the “concept of cane” and the cane. The introduction to the cane techniques is
dependent on the physical and mental readiness of the child (individual decision
made when they are around 10 years old). Operating the white cane as a natural
extension “of touch in space ahead” provides the ability to collect information,
safely and to produce efﬁcient self-movements.
Learning Braille is very important as one of the factors important for the
independency of VIP. The system, developed in 1825 by a blind Frenchman Louis
Braille, is now adopted universally and has been applied to all the languages of the
world.
It is necessary to emphasize the needs to learn a personal signature - skills
acquired on the basis of muscle memory.
The training of a guide-dog is based on the spatial orientation of a VIP. The VIP
thinks and decides, and must have imagined the itinerary in his mind. The purpose
of a guide-dog is to identify and to avoid obstacles. It is important that a VIP
moving with the dog completed the spatial orientation training and has always with
him a white folding cane, which is used situationally, e.g., when boarding a train.
The guide-dog can be obtained at the age of sixteen.
These learnt skills can be used in any situation. In addition to the traditional
models of space learning, modern techniques of education, innovation and new
technologies are introduced, e.g., a computer network, internet, and software
Spatial Orientation in Children: A Tyﬂological Approach
281

support. Educating the VIP to always stay up to date on the new technologies
developed to compensate for their handicap will increase their learning opportu-
nities, and, in adulthood, their job opportunities in real companies.
The quantity and quality of assistance provided to the VIP will greatly facilitate
their everyday activities. Various assistive systems are developed to support space
learning; some of them are obstacle detectors or verbal guidance systems (e.g.,
about the passed-near-by the building). Several experiments with robots as guide
for VIP are also conducted.
Taking the responsibility to teach VIP children requires high qualiﬁcations and
adequate preparation. Methods evolve continuously and adapt to evolution in our
understanding of how we process space.
Acknowledgements This chapter is dedicated to my outstanding student and friend Miss Hanna
Pasterny. Hanna Pasterny is a graduate of Romance Philology and postgraduate in speech therapy.
She works as a consultant for persons with disabilities in the association “Civil Initiative
Development Centre (CRIS)” in Rybnik (Poland). She is a social assistant for the European deputy
Prof. Jerzy Buzek, and a volunteer at the Association for Welfare of Deafblind. In 2009, Hanna
received Lady D award for her involvement in social integration of the VIP, and in 2011 she was
awarded the distinction “Man without barriers.” Hanna has authored three books “How I con-
quered Belgium with the white cane” “Tandem and tartan” and “My travels in the darkness” (all
written in Polish).
References
1. Warren DH (1994) Blindness and children: an individual differences approach. Cambridge
University Press, UK
2. Dolański W (1954) Czy istnieje “zmysł przeszkód” u niewidomych?. PWN, Warszawa (in
Polish)
3. Uy Ch, “Seeing” sounds: echolocation by blind humans. Harvard Undergraduate Society for
Neurosciences, USA. http://harvard.edu/*husn/BRAIN/vol1/echo/html
4. Montague A (1971) Touching: the human signiﬁcance of the skin. Harper & Row, USA
5. Jacobson E (1962) You must relax. Mc Graw-Hill Book Company, New York
6. Schultz JH (1958) Le training autogène. PUF, Paris (in French)
7. Polender A (2016) Pojedynek z nerwica. http://darmowe-ebooki.com.pl/13980-polender_
anna_-_pojedynek_z_nerwic%C4%85.html (free of charge book)
8. Sherborne V (2011) Developmental movement for children. Worth Publishing, New York
9. Laban R (2003) La danse moderne éducative. Éditions Complexe et Centre National de la
Danse, France. ISBN 2-87027-936-1 (in French)
10. Gibbs KL (2007) Study regarding the effects of brain gym on student learning. Education and
Human Development, College at Brockport (Master’s Theses)
11. TyﬂoSwiat (2009) N° 3(5). www.tyﬂoswiat.pl (in Polish)
282
K. Nawrocka-Łabuś

Scene Representation for Mobility
of the Visually Impaired
Guillaume Tatur
1
Introduction
As it is signiﬁcantly correlated with autonomy and quality of life, a vast majority of
the developed approaches or devices dedicated to visually impaired individuals
attempt to augment or support orientation and mobility abilities and maximize
independence, safety and efﬁciency of movement [1]. Experience gained from
sensory supplementation studies, visual neuro-prostheses experiments and technical
aids development provide evidences regarding some general guidelines for scene
representation design at a functional level, whether it address the problematic of
mobility or another task:
– User’s impairment: should the user suffer from congenital blindness, late
blindness or low vision, the scene representation should be adapted,
– User’s functional vision (i.e., efﬁciency of use of the remaining visual infor-
mation which, among other things, depends on the visual strategies learned or
developed) and abilities,
– The device itself (e.g., tactile stimulation device, retinal implant, etc.), through
which the representation is provided,
– The complexity or richness of the information that can be conveyed through this
support,
– The sensory modality (or modalities) that will receive the information,
– The deﬁned objective(s) of the representation.
For this last point, we can distinguish two approaches regarding the category of
information that a representation may provide. The ﬁrst approach would be the
attempt of representing a scene using general information (e.g., raw input from a
G. Tatur (&)
Institut de la Vision, Aging in Vision and Action Laboratory, CNRS—INSERM—University
Pierre and Marie Curie, 17, rue Moreau, 75012 Paris, France
e-mail: guillaume.tatur@inserm.fr
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_10
283

camera), expecting that the user will extract the relevant cues for the current task
performed from the pattern of luminosity values as represented by the image pixels
gray level for instance. This approach is highly dependent on the complexity of the
represented information and on the user capability to decipher this ambiguous
representation through the selected sensory modality. Another approach would be
to optimize a representation to provide speciﬁc information for a particular task like
mobility.
Optimizing representation regarding a speciﬁc task or activity is interesting for
several reasons:
– It takes into account the limited capabilities or resolution of the device,
– Information can be ﬁltered or simpliﬁed and enhanced relatively to the task
performed,
– As it is designed for a speciﬁc task and as it uses specialized information, this
representation should be easier to understand.
Additionally, a representation that requires too much time and effort even after
training may obviously be detrimental to a safe and efﬁcient navigation. Mobility is
a time-critical activity where one should be able to understand the environment
(e.g., spatial layout of the surrounding environment) as well as to anticipate and
adapt to its dynamic.
The complexity of designing such representation may be partly attributed to the
fact that one needs to deal with some generalized–specialized balance. Indeed, as
stated above, designing an enhanced representation dedicated to a complex task
such as mobility requires that the system provide a balanced amount of specialized
information as well as general information of the scene. As an example of spe-
cialized information, a representation may give access to the distances to obstacles
in the scene through a simple coding of these distances into artiﬁcial stimuli
intensity. This simpliﬁed representation may be easier to understand and to deci-
pher. Additionally it will certainly be very pertinent for a safe perambulation and
allow the user to safely and efﬁciently move through an environment and to avoid
obstacles. On the contrary, a representation based on general information may for
instance give access to luminosity information. This representation may be
ambiguous and unstable regarding several factors (e.g., lighting condition, view-
point, textures), it might thus require more time and cognitive effort to be processed.
However, it may allow the user to infer the location of distant and visually salient
scene features (e.g., ceiling lights, large contrasted walls) and use them as land-
marks for navigation.
In the following sections we will present an overview of currently proposed
scene representations for the visually impaired. As stated above, we can distinguish
different kind of visual impairments, from congenital blindness to low vision, and
thus different needs and abilities. For each, several approaches and technical
solutions have been proposed or are still in development.
One can note that a same technical aid can be used in several type of visual
impairment as for instance an electronic cane or a BrainPort, presented in the
284
G. Tatur

following section of this chapter, may be used either by blind persons or by indi-
viduals suffering from low vision. However, some scene representations and
technical solutions are intrinsically dependent on the visual impairment they are
dealing with and the chosen sensory modality for information presentation. In the
following sections we will thus review these approaches and present the related
methods for scene representation.
2
Technical Aid and Sensory Supplementations Systems
In this section we will present non-invasive technological solutions in order to give
some information to the user about its surrounding and mobility related information
based on sensory supplementations systems.
In these systems, information usually acquired from one sensory modality are
captured through an artiﬁcial sensor (e.g., video camera) and are transformed in
order to create stimuli adapted to another modality. We can distinguish two main
categories of these devices in order to supplement the sense of vision: visuo-tactile
and visuo-auditory systems.
2.1
Visuo-Tactile Sensory Supplementation
The objective of these devices is to convert visual information into tactile stimuli.
A large literature exists on this approach and conﬁrms its efﬁciency in order to give
the user, after adapted training, the ability to recognize simple geometric shapes [2,
3], to localize entities [4] as well as the ability to estimate other vision related
information such as distance and perspective [5].
As stated in the introduction, this technique is dependent on the stimulation
method as several limitations will apply (e.g., maximum signal frequency, required
stimulation signal intensity) as discussed for instance in [6]. The principal stimu-
lation methods are electrotactile and vibrotactile. Electrotactile stimulations are
based on a direct electrical stimulation applied on the skin whereas vibrotactile
stimulation uses localized vibrations to elicit sensations. Additionally, it is note-
worthy that the location of the stimulation on the skin will constrain the maximum
allowed spatial resolution that can be perceived although some authors argue that
the poor tactile (two-point) resolution of the skin may not be the limiting factor in
perceiving complex images for instance as the brain extracts information based on
the stimulation patterns [7].
Various systems have been proposed so far and we will present some of the most
representative.
The Bach-y-Rita’s TVSS (tactile vision substitution system [7]) is one of the early
developed and most well-known visual to tactile sensory supplementation devices.
The system converts an image captured by a video camera in order to generate tactile
Scene Representation for Mobility of the Visually Impaired
285

stimuli through a stimulation device placed on some part of the skin such as the
lower back, the abdomen or the ﬁngertip in the ﬁrst version of the system.
The proposed scene representation was a point-for-point projection of scene
images, captured by a ﬁxed, hand-held or head-worn camera (Fig. 1). Images were
transformed into black and white images and reduced spatially to accommodate for
device resolution onto a two dimensional tactile stimulation matrix (between 100
and 1032 stimulation points).
In [8], extending the work initiated with Bach-y-Rita, Collins et al. have
developed a 1024 wearable electrotactile system placed on the abdomen and con-
verting images acquired through a 90° ﬁeld of view camera to a point-to-point
electrical stimulation pattern. As a result, subjects equipped with such device
(Fig. 2) have been able to safely avoid and navigate around large and contrasted
objects such as tables and chairs. However, these results are only valid in simple
indoor environment as subjects, even trained, failed to use the system in real
outdoor environment (see also [4]).
This setup, which gives the user the possibility to experience and analyze the
dynamical scene, was later modiﬁed into a tongue display unit (TDU, Fig. 3), also
called BrainPort [9] that allows reduced required electrical current for the stimu-
lation and was more suitable for mobility purpose.
In [10], the authors was evaluating if the use of a BrainPort inﬂuenced subjects
ability to navigate a standardized indoor course composed of highly contrasted
Fig. 1 Young subject reproducing hand gesture perceived through a head-worn camera and a
6  24 vibrotactile array. The LED monitor in the foreground show the active pattern of the tactile
display. From Bach-y-Rita et al
286
G. Tatur

obstacles. The experimental device was constituted of a tongue electrotactile
stimulator (20  20 array of electrodes) and a spectacle frame mounted camera
with a ﬁeld of view of 73°. They found that the use of a BrainPort help subjects to
avoid obstacles in such environment and this ability were also observed in other
studies as in [11].
These devices are based on the general information approach presented and
discussed in the introduction. Validity of this approach in the context of sensory
supplementation systems and real world tasks have been discussed by some authors
[4, 12].
Some research teams have developed a “specialized” representation approach in
order to present relevant and adapted information for mobility.
In [13], the authors propose a novel scene representation method dedicated to
mobility. In their approach, the 3D scene captured by a stereoscopic pair of
Fig. 2 A 1024 wearable electrotactile system. Images are captured from a head-worn miniature
camera and are reduced to match the device resolution. Corresponding gray levels of this reduced
image will drive the electrical stimulation amplitude on the abdomen. From Collins et al
Scene Representation for Mobility of the Visually Impaired
287

cameras, is projected vertically (i.e., ground projected) on a 2D plan. Through the
proposed processing, obstacles are represented as non-accessible areas on the plan,
whether they are overhanging or lying on the ground. The obtained simpliﬁed map
of the environment is then presented to the user through a hand-held touch stim-
ulating Braille-like device [14] (Fig. 4) and is updated in real-time. In the proposed
representation, processing of the 3D data allows a binary partitioning of the space
into two zones: free path and obstacles. The system is aimed to provide a dynamical
tactile representation of the scene where raised taxels (i.e., individual tactile stim-
ulators that compose the device) principally show the limits of the obstacle-free
space. Furthermore, information are represented on the tactile surface in a
user-centered Euclidian reference frame which observation point is indicated by a
notch on one of the border of the tactile surface. Preliminary navigation experi-
ments in standardized test environments indicate that the system can be used to
perceive the spatial layout of the dynamical environment through this binary tactile
representation.
As another approach to mobility, Chekhchoukh et al. propose a guidance
technique using also a TDU (Fig. 5). In [15], the authors mainly propose to simplify
the scene by only showing, in a perspective view, a 3D path composed of dots that
must be followed by the user for navigating from one place to another. Stimulus
intensity associated to each dots represent the distance of this 3D dot from the user.
The authors observed that a symbolic representation appears to be more easily
understood.
Fig. 3 A tongue display unit. Electrotactile stimuli are delivered to the tongue via ﬂexible
electrode arrays placed in the mouth. From Bach-y-Rita et al
288
G. Tatur

2.2
Visuo-auditory Sensory Supplementation
Taking advantage of the human auditory system performances [16], visual to au-
ditory sensory supplementation systems may be categorized into two approaches:
– The auditory coding of the position and distance of the obstacles through the use
of an ultrasonic or laser rangeﬁnder device for instance.
– A visual to sound online translation based on the coding of visual pattern
extracted from captured images (e.g., from a head-mounted camera).
For the ﬁrst category, the auditory feedback in such devices gives the user clues
about the texture, the distance, the size, and the position of the scene entities. In
these devices, distance is mainly represented by the sound intensity and the position
around the user is coded through binaural disparity. A non-exhaustive but repre-
sentative list of these devices is presented below.
– The C-5 laser cane [17] is a triangulation-based device, embedded in a white
cane, that informs the user on the presence of the obstacles in mainly three
zones:
• UP (at head height to detect overhanging obstacles),
• FORWARD (in front of the user as an extension of the cane tip),
• DOWN (to detect obstacles on the ground, stairs and holes).
Fig. 4 Illustration of the developed scene representation (bottom row) and device (upper row).
Image of the scene, acquired from a stereoscopic pair of cameras, are processed in order to create a
2D tactile map (right image). This simpliﬁed representation shows the accessible and
non-accessible parts of the environment delimited by raised taxels. The center of the Euclidian
reference frame is materialized by a notch in one of the device borders. The stimulation device is a
Braille matrix of 8 by 8 shape memory alloys based taxels. From Pissaloux et al
Scene Representation for Mobility of the Visually Impaired
289

Obstacle detection output provides information regarding the presence of an
obstacle in each zone by the generation of a speciﬁc tone.
– The binaural sonic aid [18] is a wide (55°) beam ultrasonic rangeﬁnder device
mounted on a pair of glasses. In this system two receivers are placed on each
side of the emitter. The signal from the receivers is adapted and presented to
each ear separately, which allows large objects localization. Frequency of the
generated sound signal is relative to the obstacle distance.
– The NavBelt is a computerized electronic travel aid using an array of ultrasonic
rangeﬁnders. This device has been presented in [19]. It uses a binaural feedback
to guide the user toward an obstacle free direction of travel. An alternative
“image” mode exists and provides the user with a 180° acoustic image of the
environment based on instantaneous polar obstacle density measurements.
Using a temporal sweeping technique [20] to present this acoustic image, the
scene in front of the user is automatically scanned horizontally (left to right) and
sound amplitude at a time t is driven by obstacle density in the corresponding
direction.
Fig. 5 Illustration of the proposed representation method from Chekhchoukh et al. Left
illustration of the active electrodes on a 12  12 electrode array. Shades of gray represent
stimulation intensity. Right perspective view of the 3D positions of two spatial locations.
Navigation between places (Right image) can be achieved by following a 3D path. This path is
composed of dots (Left image) perceived through the TDU as localized stimulation in a perspective
viewpoint. Stimulation intensity depends of the distance of the 3D dots from the user and act as a
guide to the destination location
290
G. Tatur

– The Mowat Sensor, as described in [21], is a hand-held device equipped with an
ultrasonic range ﬁnder. The distance information is transmitted by tactile
vibration and vibration intensity is proportional to the proximity of the obstacle.
For the second category based on image to sound conversion, we will present the
most representative approaches.
– The vOICe was developed by Meijer [22] and it proposes an image to sound
mapping based on the conversion of the video frame pixel arrays into successive
sound signals. Each signal is generated according to the pixel position and
brightness: pitch is proportional to pixel height (vertical position), amplitude
(loudness) to brightness. To give information on the horizontal position of the
stimulus, a left to right scanning technique have been implemented (“synchro-
nization” sound click indicate the start of a frame scan). The working resolution
of the system composed of an audio headset and a head-mounted camera
(Fig. 6) was of 64  64 pixels with 16 gray levels available per pixel and an
image-to-sound conversion time (i.e., duration of one frame presentation) of
about 1 s.
– Capelle et al. [23] propose a different approach: the PSVA system (Prosthesis
Substituting Vision by Audition). Consisting of an audio headset and a
head-mounted camera (Fig. 7), this device is an experimental prototype that
uses a simpliﬁed model of the retina. Indeed, the system uses multiple resolution
of an input image to process information in a similar way than the retina by
distinguishing central and peripheral ﬁeld of view. Information contained in
Fig. 6 The vOICe system from Meijer et al. The system is composed of a stereo headset and a
head worn camera. Right individual using a portable version of the device while walking on a
street
Scene Representation for Mobility of the Visually Impaired
291

each of these resolutions will be represented using different sets of tones.
Finally, an inverse model of audition with an image-to-sound mapping based on
a pixel-frequency association and binaural intensity balance is used to create the
output audio signal.
– The See ColOr (Seeing Color with an Orchestra) Mobility aid was introduced by
Bologna et al. [24]. They postulate that color and depth are important infor-
mation for mobility as color allows detecting potential landmarks and depth can
be used to evaluate the distance to objects in the scene. The authors approach
was thus based on the soniﬁcation of the color and depth information. Color was
represented by the Hue, Saturation and Brightness values parameters of the HSL
color system and was associated respectively to an instrument timbre, sound
pitch and sound mixing using double bass (for low luminosity levels) or a
singing voice (for high luminosity levels). Color information of the video image
is computed in real-time on a row of 25 pixels around the image center. Depth
information coding is based on sound duration (e.g., 254 ms duration for dis-
tances between 1 and 2 m). As an experiment they have shown that both blind
and blindfolded subjects successfully followed a colored path on the ground
(Fig. 8). Additionally a well-trained subject was notably able to successfully
perform simple tasks such as walking on a corridor and ﬁnd a colored object or
detect an open door.
Fig. 7 PSVA system from Capelle et al.
292
G. Tatur

3
Using Residual Vision: Augmented and Virtual Reality
Devices
Several visual disorders such as retinitis pigmentosa (RP), age related macular
degeneration (AMD) or glaucoma may induce severe vision loss. Indeed, persons
suffering from low vision may experience signiﬁcant visual ﬁeld reduction (like in
RP patients, Fig. 9) or central vision loss in the case of AMD for instance, visual
defects (e.g., scotoma) and other visual impairments (e.g., reduced contrast sensi-
tivity, impaired color vision).
Augmented and virtual reality devices make possible, to a certain extent, the
compensation of these visual impairment by:
– Providing additional information in the remaining ﬁeld of view. Without such
technical aid, these information may not be directly accessible. For instance, in
the case visual ﬁeld reduction, laterally located static or moving obstacles may
not be seen directly. The idea is thus to transmit alert signals or additional
information in the residual ﬁeld of view in order to facilitate acquisition of static
and dynamic information of the environment.
– Enhancing the residual vision, by displaying reinforced contrasts and colors for
instance.
Fig. 8 A blindfolded subject equipped with the See ColOr device composed of a stereoscopic pair
of cameras and a stereo headset. Information from the cameras are processed by a laptop which
delivers the sound signal allowing the subject to follow a colored (red) path. From Bologna et al.
Scene Representation for Mobility of the Visually Impaired
293

These devices (Fig. 10) are composed of one or two displays, and may be
non-immersive for augmented reality devices or immersive in the case of virtual
reality devices.
Designing such technical aid for low vision persons is not an easy task as, for
instance, there is a large interindividual variability in visually impaired residual
vision and furthermore in their functional vision.
One of the most well-known methods based on augmented reality technique are
from Eli Peli’s Works. In [25], the author presents his approach on “vision mul-
tiplexing”. Peli’s approach is based on visual ﬁeld restitution as visual ﬁeld is one
of the key components for mobility. The importance of visual ﬁeld extent related to
Fig. 9 Illustration of the residual visual ﬁeld of an RP subject, characterized by a peripheral ﬁeld
loss (dark gray areas)
Fig. 10 Immersive and non-immersive devices. Left Lumus (Lumus Ltd) augmented realty device
with see-through display screens. Right Vuzix (Vuzix Ltd) virtual reality device with opaque
screens
294
G. Tatur

the quality of life and mobility of visually impaired individual is conﬁrmed by other
studies such as [26, 27].
The principle of vision multiplexing, in the case of persons with a severely
restricted visual ﬁeld of view [28], can mainly be described as a spatial multiplexing
technique: using a wide- angle head-worn camera and a see-through head-mounted
display, the method is based on the presentation of captured images contours
(through real-time edge detection) displayed as white lines and superimposed on the
natural view (Fig. 11). As the camera possess a wide ﬁeld of view (from 75° to
100°), the contours are miniﬁed to match the smaller ﬁeld of view of the user and
thus provide additional information that would not or barely be accessible other-
wise. It is noteworthy that without embedded eye tracking system, the user must
keep his gaze at the same central position in order to maximize the visibility of the
miniﬁed images.
In their experiments [29], the authors evaluated whether the device improved
subjects performances in a visual search task. One can note that this ability is highly
correlated with mobility performance [28]. Subjects, with a ﬁeld of view extent
ranging from 7° to 16°, have also been tested on an obstacle collision risk esti-
mation task in a virtual environment. For the ﬁrst experiment, using the device
Fig. 11 Illustration of the spatial multiplexing principle dedicated to low vision patients with
severe visual ﬁeld reduction, equipped with a head-mounted display and a wide ﬁeld of view
camera. As it is a see-through device, the natural scene can be seen directly (background image).
Edge maps are calculated in real-time from the captured camera images and are miniﬁed and
displayed as white lines. Although the patient has a tunnel vision (illustrated as a circular
highlighted aperture in the image center), he now has access to information contained in a wider
part of the environment. From Peli et al.
Scene Representation for Mobility of the Visually Impaired
295

subjects better performed than without in the visual search task. In the second
experiment, no signiﬁcant effect of the presentation of a miniﬁed edge map was
found in obstacle collision risk judgment.
In [30], the author examined the impact of non-immersive head-mounted dis-
plays on the user’s visual ﬁeld and concluded that while the open design of such
devices provides the necessary ﬁeld of view required for mobility, the created
relative scotoma by the device screen and the divided attention between the envi-
ronment and the display may be detrimental to a safe navigation. The authors
recommend to take care of the screen display positioning to control the location of
the created scotoma. Training of the wearer is also recommended to use more safely
and efﬁciently such devices.
One can note that low vision users, with reduced functional vision (e.g., reduced
contrast sensitivity, impaired color vision, glare disabilities and impaired luminosity
adaptation [31]), may experience further difﬁculties while wearing see-through
HMD depending on the opacity of the device and environmental conditions.
Indeed, some vision disorders like RP may additionally imply difﬁculties to see and
to adapt to low light as well as bright light conditions. In these situations, their
ability to observe simultaneously the displayed information and the environment
may be severely impaired (e.g., displayed information not perceived in outdoor
luminosity condition or environment visibility masked in indoor situation).
AUREVI [32] is a research project dedicated to the development of a technical
aid for low vision individuals (mainly persons with RP) based on an immersive
approach. The currently proposed device concept is composed of head-mounted
cameras and a binocular display with screens that prevent the external light from
reaching the user eyes. The later point is important, as the chosen approach is to
have a complete control over the presented luminosity and visual aspect of the
scene representation displayed on the screens. Controlling luminosity in real-time
should for instance provide optimal display physical output regarding the user’s
dynamic light adaptation capability (e.g., smoothed light transition between indoor
to outdoor scenes). In addition to light adaptation, other system parameters such as
color and contrast are driven by the user’s residual vision screening. Interestingly
these parameters may be updated to account for user disease progression. Scene
representation tends to be optimized for mobility tasks and optimal visual ﬁeld
extent and presentation, obstacle avoidance signals representation as well as ded-
icated image processing are currently being investigated.
4
Visual Neuro-prosthesis
Visual neuro-prostheses are an invasive technique to restore some rough form of
vision in individuals suffering from late blindness due to degenerative diseases of
the retina.
These devices are based on the stimulation of still functional neurons in some
part of the visual pathway. By means of this stimulation, mainly electrical, it is
296
G. Tatur

possible to elicit visual perceptions in the implant recipient visual ﬁeld. These
perceptions are called phosphenes and are often described as localized blob of light
approximatively circular with a Gaussian luminosity proﬁle [33]. The generated
phosphenes may have various size and apparent luminosity depending of the
stimulation intensity [34]. Finally, a phosphene position in the visual ﬁeld is
determined by the electrode location on the retina or in the visual cortex for
instance, depending of the visual neuro-prosthesis category used.
A visual neuro-prosthesis combine three main systems:
– One or more sensors providing information about the environment. Mainly, it
consists of a head-worn camera as in [35] or computer-generated images [36].
– A processing unit that process and analyzes information from the sensors.
Output of this system is the scene representation that will be sent to the stim-
ulation subsystem.
– Finally, the stimulation subsystem uses the information provided by the pro-
cessing stage to generate a proper stimulation at the biological level.
Phosphene generation can be achieved using various stimulation techniques:
Mechanically, magnetically or electrically. Devices based on electrical stimulation
are the most studied and used, we will therefore focus our presentation on these
devices.
In order to better apprehend the underlying problematic of scene representation,
or information transmission, through a visual neuro-prosthesis we’ll give a brief
overview of the main existing electrical implant categories.
4.1
Visual Neuro-prosthesis Implants
4.1.1
The Cortical Implant
This device, mainly developed by Dobelle [37], directly stimulates the primary
visual cortex. It has thus the advantages to override any disorder affecting for
instance the retina or the optic nerve. However, it implies a high risk surgery with
possibly major postoperative complications like an epileptic seizure [38].
The relationship between localized electrical stimuli (and thus electrodes
arrangement) and phosphenes location is not trivial as it is shown in Fig. 12.
4.1.2
Epiretinal Prosthesis
An epiretinal implant is an electrodes array implanted on the epiretinal side of the
retina (Fig. 13). These devices are the most studied so far and several clinical trials
have been performed which give information on the elicited perception character-
istics and the actual performance of patients using the implant. Current Argus II
Scene Representation for Mobility of the Visually Impaired
297

Fig. 12 Upper Row (From left to Right) X-ray image of the implant on the right occipital lobe and
illustration of the information acquisition and stimulation systems of the device. Bottom row: On
the left Map of the positions of the perceived phosphenes, numbers indicating which electrodes are
active. Right schematic of the electrode array
Fig. 13 Argus I epiretinal prosthesis. Left Perception location in the implantee visual ﬁeld, Right
Argus 1 electrode array on the retina. From Humayun et al.
298
G. Tatur

(Second Sight Medical Products, Inc.) implant is composed of 60 electrodes
(6  10 electrodes array). Interested readers may refer to [39].
4.1.3
Optic Nerve and Subretinal Implants
Two other methods exist for visual pathway stimulation: Optic nerve stimulation
and subretinal implant. The former is the direct stimulation of the optic nerve
through 4 ﬁxed electrodes. Depending on the stimulation parameters, various
positions and appearance (size, shape, and brightness) of phosphenes elicited in the
implantee visual ﬁeld can be achieved. Optic nerve stimulation devices are studied
by several research groups and are notably described in [40].
For all the above presented methods, the stimulation signal is generated based on
external sensors information. Therefore, in the current setup, eye movements will
not allow natural exploration of the visual scene and the perceived stimuli will stay
unchanged. In subretinal implants, however, the objective is to stimulate the
greatest part of the remaining functional visual pathway and to use as much as
possible the “natural” processing. To achieve this, nonfunctional photoreceptor of
the retina are replaced by an electrodes array coupled with a photodiodes array
which transform incoming external lights to electrical signal. Even if it requires
ampliﬁcation [41] in order to correctly stimulate the retinal cells, this technique has
proven to be successfully implanted and used by patients with RP. The fading effect
as evoked in [42], which corresponds to the disappearance of the visual perceptions
due to a constant stimulation pattern, can be avoided using subretinal prosthesis
because of the natural eye movements; whereas all other visual prosthesis using
information sources external to the eye must use artiﬁcial methods to avoid such
effect (e.g., adding noise to each electrode output signal). Interested readers can
refer to [43].
4.2
Prosthetic Vision for Mobility
We have seen that these devices are able to generate perceptions in a form of an
array of elicited phosphenes with various brightness depending on the stimulation
level. Works on prosthetic vision correspond to the development of an optimal
scene representation with the technical and biological constraints of a visual neuro-
prosthesis.
One can note that a visual neuro-prosthesis does not allow the implantee to “see”
like we commonly experience vision. Furthermore, besides the interest of exploiting
the remaining functional part of the visual pathways, this device can be seen as a
sensory supplementation device using the visual modality and phosphenes to
transmit information. Therefore, the same problematic on how to create an optimal
scene representation remains.
Scene Representation for Mobility of the Visually Impaired
299

Most of the works on prosthetic vision have been done using simulation with
well-sighted subjects. This allows researchers to more easily experiment various
implant characteristics number and spatial layout of the stimulation electrodes,
applied image processing). These simulation studies involve displaying, on a screen
or a virtual reality display, images that simulate perceptions of implanted individ-
uals (Fig. 14).
In the literature, a majority of authors propose to use captured gray level images
as input for the prosthesis. Various approaches about phosphene simulation (e.g.,
phosphenes simulated appearance) and, more interestingly, methods on how to
transform image pixels information into scene representation suitable for the
stimulation device have thus been proposed. For instance Cha et al. [35], use
impulse sampling of the captured camera image through a matrix layout, which
represent the position in the ﬁeld of view of the phosphenes. Thus, the gray value
sampled at each phosphene center position will drive the related electrical stimu-
lation amplitude in the case of actual implant or phosphene brightness for simu-
lation experiment.
If we consider that a phosphene represent an image region (also called receptor
ﬁeld) and thus few degrees of the camera ﬁeld of view, some authors have proposed
different techniques to estimate the corresponding phosphene value: in [44] the
authors suggest to use a Gaussian ﬁltering technique to compute the mean value of
the represented pixels gray levels. Boyle et al. [45] propose to divide the image
matrix into blocs of pixels. Each phosphene intensity value is then computed as the
mean of the pixels gray levels of each blocs.
This averaging technique (Fig. 15), which is commonly used in prosthetic
vision, can also be described as an image reduction as it simply average luminosity
Fig. 14 Phosphenes
simulation. Phosphenes are
positioned following a matrix
layout which only occupy a
speciﬁc part of the subject
visual ﬁeld, the remaining
visual ﬁeld is represented here
as the black uniform
background (cropped here for
demonstration purpose). From
Cha et al.
300
G. Tatur

information in each image region to obtain a lower resolution image adapted to the
dimension of an implant electrodes array.
In the receptor ﬁeld approach, there is a tradeoff between receptor ﬁeld size and
representation ambiguity. Indeed, when increasing the size of the receptor ﬁelds,
more pixels regions which probably account for different objects in the scene are
represented by the same phosphene which certainly makes the representation harder
to decipher. Chen et al. [46] have shown that using overlapping Gaussian receptor
ﬁelds seems to enhance object localization performances, at least in an artiﬁcial and
highly contrasted experimental setup.
Simulation studies from Cha et al. [35] of navigation (Fig. 16) and acuity per-
formance measures, are often presented as reference work. In their tests, they
evaluated two parameters: the number of phosphenes and the presented ﬁeld of
view. One can note that Marron and Bailey [47] observed that visual ﬁeld extent
and contrast sensitivity are the two parameters that predict the most adequately
performances in orientation and mobility.
They conclude that optimal performances, based on time of travel and number of
contacts with the obstacles, are obtained for a ﬁeld of view of 30° and a matrix of
25  25 regularly spaced phosphenes. The environment used for testing exhibit a
highly contrasted appearance: white walls and ground, and black obstacles.
Similar results were reported by Sommerhalder et al. [48]. The author concludes
that 500 stimulation points should be enough in order to perform the majority of
mobility tasks. They reported that using up to 1000 stimulation points only con-
tributes to the feeling of safety.
In [49], the authors have tested mobility performances of subjects in a real and a
virtual environment. For the real environment, subjects were equipped with an
Fig. 15 Illustration of the averaging technique output with a 9  9 simulated electrodes array.
Image are captured (left image) and processed using the mean calculation method to create the
luminosity-based representation (right image) where each phosphene value corresponds to the
mean gray level of the pixels in the corresponding image region. From Tatur et al.
Scene Representation for Mobility of the Visually Impaired
301

HMD and a head-worn camera whereas in the virtual environment, simulated
phosphenes representations of computer generated-virtual view were displayed on a
screen. Here again the method used for scene representation is based on the
computation of the mean gray value of pixels in each image region associated to a
phosphene. They concluded that as few as 60 electrodes may be enough to ensure
autonomy in mobility with the requirement of intensive training. At this time, the
authors were also testing the second generation of epiretinal implant, ARGUS 2.
In [50] the authors propose an interesting idea: adding electrodes, and thus
phosphenes, in the periphery of the main electrodes array. In their simulation setup,
height additional phosphenes are available around a 6  10 central array, each one
representing a direction in space. The subject could ask verbally to localize various
objects in the scene which will be then visually indicated by means of a blinking
phosphene in the object relative direction. Additional phosphenes are here used for
guidance purpose. Tatur et al. [51] proposed to use this peripheral area for obstacles
proximity and motion detection in a wide ﬁeld of view.
Works cited above were based on the more common general information
approach. As previously discussed, in the case of visual prostheses this approach
mainly propose a scene representation based on luminosity information, which
imply that an electrode stimulation intensity or phosphene appearance (i.e., mainly
brightness) is driven by the mean gray value of pixels in the corresponding captured
image region. We will now present representative studies of the specialized
approach.
Fig. 16 Subject equipped
with a simulation apparatus.
A camera and a screen are
mounted in a ski goggles.
From Cha et al.
302
G. Tatur

In [52], the authors presented a method for scene representation dedicated to
mobility. They propose to apply an image processing technique to the captured
image frame which consists of the segmentation of this image in homogenous gray
level regions and in the tracking of their expansion. The idea is to create an alert
signal indicating the proximity or the motion of objects in some part of the camera
ﬁeld of view. While this idea is interesting, the lack of robustness regarding lighting
condition makes this technique not operational.
In [51, 53], Tatur et al. introduced a novel approach dedicated to mobility.
According to the authors, developing a method based on specialized information is
a necessary step toward an efﬁcient and functional scene representation dedicated to
a speciﬁc task, like mobility in this study. The authors establish the basis of an
original use of distance information as an input for scene representation in visual
neuro-prostheses. They postulate that as distance-based contrasts should provide
more invariant as well as less ambiguous information than mean calculation of pixel
gray levels, using distance information as a substitute for luminosity information
should provide an efﬁcient representation for scene geometrical layout under-
standing and a safe perambulation.
Through the use of a head-worn depth sensor (i.e., stereoscopic pair of cameras)
to acquire distance information of the scene, they have proposed a method to
generate a depth-based representation (DBR), where the brightness of each phos-
phene is deﬁned by the distance to the surrounding objects, its intensity increases as
distance decreases. The information obtained from this representation is indepen-
dent of the texture, reﬂectivity of entities and of lighting conditions. Additionally
this representation directly provides ego-centered distance information of observed
entities.
To create this representation, they propose to convert distance information into
intensity levels, which in turn can be used to generate simulated phosphenes images
for simulation studies or be use as an input for the stimulation stage of an actual
prosthesis. The developed transfer function can be linear or nonlinear (Fig. 17).
Fig. 17 Illustration of linear and non-linear transfer function. In order to represent the depth
information of an observed scene (left image) using a depth to brightness transfer function, one can
use a linear (center image) or non-linear methods. Non-linear method allows close distances to be
represented with more intensity values and thus depth layers at close range are more contrasted.
From Tatur et al.
Scene Representation for Mobility of the Visually Impaired
303

The main advantage of nonlinear conversion is that it allows enhanced contrast
between closer objects by using more brightness levels to represent near distances,
which seems more suitable for mobility purpose.
Similar to the mean calculation in luminosity-based representation (LBR), they
provide comparison of three methods in order to obtain representative distance
values at each phosphene receptor ﬁeld location: arithmetic mean, the median, or
the minimum of the distance values. Whereas the minimum calculation method
gives an overestimate of the size of the obstacle (Fig. 18), the median value, among
the other two presented methods, provide better obstacle visibility and a more
accurate objects segmentation from the background and from other superimposed
objects at different depth layers. Additionally, using the median calculation should
mitigate the inﬂuence of local distance values that would result from depth esti-
mation errors.
A similar approach using this depth-based representation has been tested in [54]
in a real (as opposed to virtual) navigation through a simple maze with overhanging
obstacles. Results indicate that the depth-based representation was more efﬁcient
than the luminosity-based one. However, the simulated electrode array dimension
(30  30 phosphenes) was far from current epiretinal prostheses capability.
As stated by Tatur et al., a representation based on depth information may better
perform than the luminosity-based representation in low-resolution prostheses since
the representation directly provides the distance values and also because several
processing may be applied to simplify the representation. Therefore, only relevant
information can be represented. Following processing has been proposed by the
authors:
– Information ﬁltering based on distance: whether it is applied to the LBR or
DBR, the objective is to remove information above a speciﬁc distance threshold
and thus represented information will only be contained in a restricted volume
around the user. This technique should be particularly useful in cluttered
environment.
Fig. 18 Comparison of calculation methods for depth representation by phosphenes. From left to
right Mean, minimum and median. The median method allows for more accurate object
segmentation results as for instance the black vertical object on the foreground. From Tatur et al.
304
G. Tatur

– Ground removal or obstacles detection: using ground segmentation algorithm
like V-Disparity [55, 56] it is possible to selectively present information about
obstacles or obstacle-free path (Fig. 19).
Despite the advantages of depth-based representation, some difﬁculties remains:
as only discrete intensity levels can be perceived by the implantee [57] and thus
only a discrete number of different phosphene brightness values can be generated,
such representation will have to cope with a trade-off between the range of pre-
sented distances and the resolution at which it will be displayed (i.e., number of
brightness levels representing a range of distance values). Furthermore, despite this
representation seems efﬁcient for a safe perambulation, a navigation task requires
also to be able to orient oneself. For this purpose the luminosity information can be
used to acquire visual cues such as light sources (e.g., windows, ceiling lights…) as
well as contrasted areas (e.g., marked pedestrian crossing). The authors propose a
method to create a composite representation that combines both types of infor-
mation in a unique representation based on temporal scanning of the depth layers
(Fig. 20).
In this representation, the initially displayed phosphenes pattern correspond to
the luminosity-based representation. Then, depending of their distance to the user, a
successive highlight of the objects occurs until a previously deﬁned maximum
scanning distance have been reached. Implementation details and considerations are
presented in [53]. This representation has additional advantages (see Fig. 21):
– This
temporal
scanning
method
should
provide
a
better
depth
layer
discrimination,
– Clues on environment geometrical layout may be inferred by the user,
Fig. 19 Illustration of an additional information processing for simpliﬁcation of the represen-
tation. Left image Image of a large room with obstacles. Right image Obstacle segmentation binary
output (white obstacle, black ground or no 3D data available) using the V-Disparity algorithm.
From Tatur et al.
Scene Representation for Mobility of the Visually Impaired
305

– Progressive highlights of the depth layer should lower the ambiguity of the
luminosity-based representation by giving local depth information on contrasted
areas which could be misinterpreted otherwise (e.g., a dark wall could be
understood as a passage).
5
Conclusion
We have seen through this chapter that various technical solutions have been
proposed and recent advances seem very promising. Unfortunately most proposed
solutions are for research purpose only and have not reached the market for now
and may never be commercially available. This can be explained by the exploratory
nature and the required multidisciplinary approach of these researches, in order to
be efﬁciently used and accepted by visually impaired individuals. As an example of
recent multidisciplinary methods, we have seen interesting approaches that take into
account user’s residual and functional vision as in the case of augmented reality
devices which may be a ﬁrst step toward a functional and adapted technical aid for
low vision individuals.
Fig. 20 Illustration of the composite representation as proposed by Tatur et al. While observing a
scene using the LBR (a), the proposed method allows to activate a temporal scanning technique
which progressively highlight objects contained in the successive depth layers of the scene (from
near distances (b) at a time t0 to a maximum distance (e) at t1 > t0). From Tatur et al.
306
G. Tatur

Whether it applies to mobility or any other tasks, researches still need to be
conducted to deﬁne optimal scene representations adapted to the chosen category of
technical aid, stimulation method, and processing strategy.
As it has been discussed through this chapter, there is still a debate whether the
information should be processed or not regarding the performed task for instance, or
simply “converted” in an attempt to imitate the natural visual sense input (e.g.,
luminosity to stimuli intensity) even if the recent literature tends to support the
former approach.
Fig. 21 Representation of a corridor-like scene. Top row Full resolution image (white lines
represent phosphenes receptor ﬁeld areas) of a large room with panels that create a corridor.
Bottom row Composite representation for the time t0 to t3. Depth information indicates the
presence of lateral obstacles that border what seems to be a corridor. A dark region on the right
side of the image could have been interpreted as a passage with only the LBR. However, it appears
to be part of the wall and could then be correctly interpreted. Vanishing point may also be detected
by observing the convergence of the successive depth layers highlight
Scene Representation for Mobility of the Visually Impaired
307

References
1. Wiener WR, Welsh RL, Blasch BB (2010) Foundations of orientation and mobility, 3rd edn.
vols 1–2. AFB Press, New York
2. Epstein W, Hughes B, Schneider SL, Bach-y-Rita P (1989) Perceptual learning of
spatiotemporal events: evidence from an unfamiliar modality. J Exp Psychol Hum Percept
Perform 15(1):28–44
3. Kaczmarek KA, Haase SJ (2003) Pattern identiﬁcation and perceived stimulus quality as a
function of stimulation current on a ﬁngertip-scanned electrotactile display. IEEE Trans
Neural Syst Rehabil Eng 11:9–16
4. Collins CC (1985) On mobility aids for the blind. Electronic spatial sensing for the blind. In:
Warren DH, Strelow ER (eds) Mobility aids for the blind. Matinus Nijhoff, Dordrecht,
Netherlands, pp 35–64
5. Bach-y-Rita P et al (1969) Vision substitution by tactile image projection. Nature 221:
963–964
6. Bach-y-Rita P, Kaczmarek KA, Tyler ME, Garcia-Lara J (1998) Form perception with a
49-point electrotactile stimulus array on the tongue: a technical note. J Rehabil Res Dev 35
(4):427–430
7. Bach-y-Rita P, Kercel SW (2003) Sensory substitution and the human-machine interface.
Trends Cogn Sci 7(12):541–546
8. Collins CC (1977) Electrotactile visual prosthesis. In: Hambrecht FT, Reswick JB
(eds) Functional electrical stimulation. Marcel Dekker, New York, pp 289–301
9. Bach-y-Rita P, Kaczmarek KA, Meier K (1998) The tongue as a man-machine interface: a
wireless communication system. In: Proceeding of the 1998 international symposium on
information theory and its applications, pp 79–81
10. Friberg TR, Nau AC, Pintar C, Fisher CN, Chen WS (2011) “Seeing” with your tongue:
sensory substitution using a simple alternative to the retinal chip. Paper presented at meeting
of the association for research in vision and ophthalmology, Fort Lauderdale
11. Chebat DR, Schneider FC, Kupers R, Ptito M (2011) Navigation with a sensory substitution
device in congenitally blind individuals. NeuroReport 22:342–347
12. Upson S (2007) Tongue vision. IEEE Spectr 44(1):44–45. doi:10.1109/MSPEC.2007.273043
13. Pissaloux E, Velázquez R, Maingreaud F (2009. In: Hahn H, Ko H, Lee S (eds) Multisensor
fusion and integration for intelligent systems: an edition of the selected papers from the IEEE
international conference on multisensor fusion and integration for intelligent systems 2008.
Springer, Berlin, pp 349–357. doi:10.1007/978-3-540-89859-7_24
14. Velazquez R, Pissaloux E, Hafez M, Szewczyk J (2008) Tactile rendering with shape memory
alloy pin-matrix. IEEE Trans Instrum Meas 57(5):1051–1057
15. Chekhchoukh A, Goumid M, Vuillerme N, Payan Y, Glade N (2013) Electrotactile vision
substitution for 3D trajectory following. Conference proceedings: annual international
conference of the ieee engineering in medicine and biology society. pp 6413–6416.
doi:10.1109/EMBC.2013.6611022
16. Hirsh IJ (1988) Auditory perception and speech. In: Atkinson RC, Hernstein RJ, Lindzey G,
Luce RD (eds) Handbook of experimental psychology (2nd edn., vol 1). Wiley, New York,
pp 377–408
17. Benjamin MJ, Ali NA, Schepis AF (1973) A laser cane for blinds. In: Proceedings of the San
Diego biomedical symposium, vol 12, pp 53–57
18. Kay L (1974) A sonar aid to enhance spatial perception of the blind: engineering design and
evaluation. Radio Electron Eng 44:605–627
19. Borenstein J (1990) The NavBelt—a computerized multi-sensor travel aid for active guidance
of the blind. In: Proceedings of the 5th annual conference on technology and persons with
visual disabilities, Los Angeles, USA, pp 107–116
20. Fish RM (1976) Audio display for the blind. IEEE Trans Biomed Eng BME-23(2):144–154
21. Pressey N (1977) Mowat sensor. Focus 11(3):35–39
308
G. Tatur

22. Meijer PBL (1992) An experimental system for auditory image representations. IEEE Trans
Biomed Eng 39:112–121
23. Capelle C, Trullemans C, Arno P, Veraart C (1998) A real-time experimental prototype for
enhancement of vision rehabilitation using auditory substitution. IEEE Trans Biom Eng
45:1279–1293
24. Bologna G, Deville B, Diego Gomez J, Pun T (2011) Toward local and global perception
modules for vision substitution. Neurocomputing 74(8):1182–1190
25. Peli E (2001) Vision multiplexing: an engineering approach to vision rehabilitation device
development. Optom Vis Sci 78(5):304–315
26. Lovie-Kitchin JE, Soong GP, Hassan SE, Woods RL (2010). Visual ﬁeld size criteria for
mobility rehabilitation referral. Optom Vision Sci 87:948–957
27. Kuyk T, Elliott JL, Fuhr PS (1998) Visual correlates of obstacle avoidance in adults with low
vision. Optom Vision Sci: Ofﬁcial Publ Am Acad Optom 75:174-82. PMID 9547798.
doi:10.1097/00006324-199803000-00022
28. Peli E (1999) Augmented vision for patients with tunnel vision. Optom Vis Sci 76(Suppl):102
29. Luo G, Peli E (2011) Development and evaluation of vision rehabilitation devices. In:
Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and
Biology Society EMBS, pp 5228–5231
30. Woods RL, Fetchenheuer I, Vargas-Martin F, Peli E (2002) 33.4: The impact of
non-immersive HMDs on the visual ﬁeld. In: SID symposium digest of technical papers,
vol 33, pp 998–1001. doi:10.1889/1.1830952
31. Gawande AA, Donovan WJ, Ginsburg AP, Marmor MF (1989) Photoaversion in retinitis
pigmentosa. Br J Ophthalmol 73(2):115–120
32. Maalej A, Tatur G, Lorenzini M-C, Dupeyron G, Dumas M, Marc I (2014) High dynamic
range imaging system for the visually impaired. ACVR2014 Second workshop on assistive
computer vision and robotics
33. Humayun MS, Weiland JD, Greenberg G, Williamson R, Little J, Mech B, Cimmarusti V,
Van Boemel G, Dagnelie G, De Juan E (2003) Visual perception in a blind subject with a
chronic microelectronic retinal prosthesis. Vision Res 43(24):2573–2581
34. Rizzo JF, Jensen RJ, Loewenstein J, Wyatt J (2003). Unexpectedly small percepts evoked by
epi-retinal electrical stimulation in blind humans. Invest Ophthalmol Visual Sci 44
35. Cha K, Horch KW, Normann RA (1992) Mobility performance with a pixelized vision
system. Vision Res 32:1367–1372
36. Hallum LE, Taubman D, Suaning GJ, Morley J, Lovell NH (2003). A ﬁltering approach to
artiﬁcial vision: a phosphene visual tracking task. In: IFMBE proceedings, world congress on
medical physics and biomedical engineering, 24–29 Aug 2003, Sydney
37. Dobelle WH (2000) Artiﬁcial vision for the blind by connecting a television camera to the
visual cortex. ASAIO J 46:3–9
38. Normann RA, Maynard EM, Rousche PJ et al (1999) A neural interface for a cortical vision
prosthesis. Vision Res 39:2577–2587
39. Ho C, Humayun MS, Dorn JD, Da Cruz L, Dagnelie G, Handa J,. Barale PO, Sahel JA,
Stanga PE, Hafezi F, Safran AB, Salzmann J, Santos A, Birch D, Spencer R, Cideciyan AV,
De Juan E, Duncan JL, Eliott D, Fawzi A, Olmos L, De Koo C, Brown GC, Haller JA,
Regillo CD,. Del Priore LV, Arditi A, Geruschat DR, Greenberg RJ (2015) Long-term results
from an epiretinal prosthesis to restore sight to the blind. Ophthalmology 122(8):1547–1554
40. Veraart C, Raftopoulos C, Mortimer JT, Delbeke J, Pins D, Michaux G (1998) Visual
sensations produced by optic nerve stimulation using an implanted self-sizing spiral cuff
electrode. Brain Res 813:181–186
41. Besch D, Sachs H, Szurman P (2008) Extraocular surgery for implantation of an active
subretinal visual prosthesis with external connections: feasibility and outcome in seven
patients. Br J Ophthalmol 2008(92):1361–1368
42. Zrenner E, Benav H, Bruckmann A, Greppmaier U, Kusnyerik A, Stett A, Stingl K, Wilke R
(2010) Electronic implants provide continuous stable percepts in blind volunteers only if the
image receiver is directly linked to eye movement, ARVO 2010. Fort Lauderdale, Florida
Scene Representation for Mobility of the Visually Impaired
309

43. Stingl K, Bartz-Schmidt KU, Besch D, Chee CK, Cottriall CL, Gekeler F, Groppe M,
Jackson TL, MacLaren RE, Koitschev A, Kusnyerik A, Neffendorf, Nemeth J, Naeem MAN,
Peters T, Ramsden JD, Sachs H, Simpson A, Singh MS, Wilhelm B, Wong D, Zrenner E
(2015) Subretinal visual implant alpha IMS—clinical trial interim report. Vision Res
111:149–160
44. Yanai D, Weiland J, Mahadevappa M, Greenberg R, Fine I, Humayun MS (2007) Visual
performance using a retinal prosthesis in three subjects with retinitis pigmentosa. Am J
Ophthalmol 143(5):820–827
45. Boyle J, Maeder A, Boles W (2003) Scene speciﬁc imaging for bionic vision Implants. In:
The 3rd international symposium in image and signal processing and analysis, ISPA
46. Chen SC, Suaning GJ, Morley JW, Lovell NH (2009) Simulating prosthetic vision: I. Visual
models of phosphenes. Vision Res 49:1493–1506
47. Marron JA, Bailey IL (1982) Visual factors and orientation-mobility performance. Am J
Optom PhysioZ Opt 59:413
48. Sommerhalder JR, Perez Fornos A, Chanderli K, Colin L, Schaer X, Mauler F, Safran AB,
Pelizzone M (2006) Minimum requirements for mobility in unpredictible environments.
Invest Ophthalmol Visual Sci 47
49. Dagnelie G, Keane P, Narla V, Yang L, Weiland J, Humayun M (2007) Real and virtual
mobility performance in simulated prosthetic vision. J Neural Eng 4(1):92–101
50. Parikh N, Humayun MS, Weiland JD (2010) Mobility experiments with simulated vision and
peripheral cues. ARVO 2010, Fort Lauderdale, Florida
51. Tatur G, Marc I, Dumas M, Dupeyron G (2010) Conception of a phosphene based visual
system—a functional approach. In: Proceedings of the third international conference on
bio-inspired systems and signal processing (BIOSIGNALS 2010), pp 339–344
52. Dowling J, Maeder A, Boles W (2005) A PDA based artiﬁcial human vision simulator. In:
APRS workshop on digital image computing, Brisbane, pp 109–114
53. Tatur G, Marc I, Dupeyron G, Dumas M (2014) Orientation and mobility with prosthetic
vision—combination of luminosity and depth information for scene representation. In:
Proceedings of the international conference on bio-inspired systems and signal processing
(BIOSTEC 2014), ISBN 978-989-758-011-6, pp 122–126. doi:10.5220/0004732101220126
54. Lieby P, Barnes N, McCarthy C, Liu N, Dennett H, Walker JG, Botea V, Scott AF (2012)
Substituting depth for intensity and real-time phosphene rendering: visual navigation under
low vision conditions, ARVO 2012. Fort Lauderdale, Florida
55. Zhao J, Katupitiya J, Ward J (2007) Global correlation based ground plane estimation using
v-disparity image. In: IEEE international conference on robotics and automation, Roma, Italy,
pp 10–14
56. De Cubber G, Doroftei D, Nalpantidis L, Sirakoulis GC, Gasteratos A (2009) Stereo-based
terrain traversability analysis for robot navigation. In: IARP/EURON workshop on robotics
for risky interventions and environmental surveillance, Belgium
57. Humayun MS (2009) Preliminary results from Argus II feasibility study: a 60 electrode
epiretinal prosthesis. Invest Ophthalmol Visual Sci 50(13):4744 (e-abstract)
310
G. Tatur

Model of Cognitive Mobility for Visually
Impaired and its Experimental Validation
Edwige Pissaloux and Ramiro Velázquez
1
Introduction
Human mobility is one of the most important cognitive tasks. Indeed, independent
and secure mobility in a real physical space has direct impact on the quality of life,
on well-being, and on integration in the numeric society. When one of our senses is
faulty (e.g., visual impairment, human laterality, balance disorders, etc.) the per-
ception of space, which usually precedes the execution of a physical movement,
becomes extremely complex; the artiﬁcial simulacra of human mobility imple-
mented in humanoid robots attest to the difﬁculty of understanding the cognitive
function of mobility.
Indeed, the concept of a space emerges from fusion of multimodal perceptions
and identiﬁcation of characteristics of space relevant for mobility. However, these
cognitive processes are far from being identiﬁed and well understood.
Furthermore, there is limited literature on the relationship and on the interplay
between the cognitive processes which underlay human mobility, the structure
(organization) of the physical space, and the design of mobility assistances.
This chapter aims to contribute to the emergence of the theoretical framework
for the design of mobility aids for visually impaired people (VIP). This framework
is based on the understanding and interaction (through a touch-stimulating device)
with space through our senses and a new model of mobility, a holistic model.
The rest of the chapter is organized as follows. Section 2 brieﬂy discusses dif-
ferent theories for emergence of space understanding from our senses, while the
Sect. 3 recalls the main models of human mobility. Section 4 proposes our holistic
E. Pissaloux (&)
Université de Rouen Normandie, Rouen, France
e-mail: pissaloux@sfr.fr
R. Velázquez
Universidad Panamericana, Aguascalientes, Mexico
e-mail: rvelazquez@up.edu.mx
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_11
311

new model of human cognitive mobility. The latter is based on the concept of tactile
gist which reinforces the emergence of sensorimotor loops from our perceptions,
the base for understanding the organization of the physical space. Section 5
addresses the validation of the tactile gist concept through several experiments. This
validation relies on a purposely designed “perception-movement” experimental
platform, and uses the purposely designed touch stimulation device (named
TactiPad), a potential new aid for mobility assistance of visually impaired. Finally,
the Sect. 6 concludes the presented research.
2
Theories for Emergence of Space Understanding
The theory of emergence of space understanding from our senses stimulations is
still an open question.
Here only some main steps in the space consciousness useful for design of new
mobility aids are considered; for a universal approach to the space concept, please
refer to the Chap. “Living in Space. A Phenomenological Account” of this book.
For Kant [1], the structure of physical space emerges from the human capability
to perceive the same feeling after a series of movements, and to conﬁrm the same
feeling from the same position. This series of movements establishes the relation
between an external object and a feeling, i.e., the external object becomes an
invariant of physical space (e.g., an obstacle for mobility).
Müller [2] deﬁnes a neurophysiological approach to perception and suggests that
the perception of the space may raise from sensations transmitted by speciﬁc nerve
pathways that project to particular cerebral regions. However, he does not answer
how the multimodal data are fused in one concept.
For Poincaré [3], the physical space structure is not an innate faculty and does
not emerge from our feelings taken separately, but from laws structuring the suc-
cession of feelings induced by space invariants (i.e., laws of simultaneous per-
ception of all obstacles in near space).
The most recent theory—theory of sensorimotor contingencies [4–11]—extends
Poincaré’s position. The sensorimotor theory (SMT) states that the perception is
inherently active phenomenon based on an interaction between the living (e.g.,
higher mammals or human being) and its environment. The SMT suggests that our
brain can be considered as a system which exchanges with external world via senses
(sight, touch, etc.) and effectors (hands, legs, etc.). The brain analyzes the results of
movements, which it orders, using perceptions which rise from these movements.
This analysis provides access to the structure and properties of the physical space
and, more generally, to all sensory concepts which our brains can attach. Therefore,
the perception (visual, tactile, auditory, etc.) is a capability to actively modify the
sensory data using movement and attention; this modiﬁcation exploits the
312
E. Pissaloux and R. Velázquez

knowledge acquired on experienced sensorimotor transformations which link action
and perception.
Consequently, the SMT hypothesizes that perceptual capacities are independent
of sensors and processes performed on acquired data. This leads to a sensory
substitution approach to perception such as “seeing with touch” [12].
This approach has been mathematically modeled [10] by a perception space of a
very high dimension (each dimension of this space corresponds to one nerve ﬁber!)
which is embedded in a variety of low dimension with topology which is home-
omorphic with a physical space. Therefore, the space perception learning is learning
of a variety of the perception space. To achieve this learning, the nervous system
uses the offsetability of sensory changes it perceives and speciﬁcally detection of
compensable motion: a sensory change induced by the environment can be com-
pensated by movement, or vice versa, the system then recovers its initial sensation
(status). Therefore, the SMT considers that the compensability of our sensory
perception is a base for the space perception. It should be stressed that this type of
treatment is likely to work in the central nervous system as the cerebellum was
identiﬁed for its role in sensory prediction of voluntary movements.
Furthermore, several facts from the psychology of perception conﬁrm experi-
mentally the relevance of the SMT as the framework for the design of assistive
device for mobility.
Held and Hein’s [13] “kitten carousel” experience clearly shows that the per-
ception and ego-motion are inherently linked and their simultaneous occurrence
allows to correctly interact with the environment. This result is conﬁrmed with
visuotactile
substitution
systems
by
Bach-y-Rita,
TVSS
(Tactile-Vision
Substitution System) and TDU (Tong Device Unit) [12, 14] and with Pissaloux’s
visuotactile supplementation system named “Intelligent Glasses” [15].
Initially discussed in nineteenth century, the recent research on the phenomenon
of change blindness [14, 16] showed that signiﬁcant changes in a scene illumination
or in a scene content can be not consciously perceived by observers if they cannot
automatically capture attention, i.e., they intervene in a roundabout way (e.g.,
during a saccade or is concealed by a distractor). Similarly, the incremental changes
can also go undetected because our perception systems (visual, auditory, tactile,
etc.) do not carry out a perfect and faithful reconstruction of our environment.
As conclusion, it is possible to say the sensorimotor theory demonstrates that the
space structure understanding emerges from the invariants of feelings (induced by
perceptions), and from the invariants of sensorimotor loops, loops induced by the
subject’s activity. To perceive a scene does not mean only to represent it but to
interact with.
As a corollary to the above statements, it is possible to claim that:
– the physical space is primary and universal, and its structure [the locations of its
components (objects)] may be established with respect to a reference frame;
– the simultaneous perception and action (thus the interaction with space) are
necessary to understand the space structure; however, it is not yet evidences if
this interplay is sufﬁcient for understanding;
Model of Cognitive Mobility for Visually Impaired …
313

– the invariants of sensorimotor loops can be learnt and exploited during the
action (e.g., during a displacement);
– the assistive technologies should reinforce the natural sensorimotor loops.
It should be noticed that Kant’s theory is a framework for path integration (or
dead reckoning) paradigm, a navigation strategy taught worldwide to VIP by
locomotion teachers. The ‘perception-motion’ loop is a basis for building of
understanding of a space structure from its partial snapshots. We name this building
process as “the space integration” paradigm.
This space integration paradigm is a framework for orientation in space and for
understanding of the space structure. Consequently, with an appropriate techno-
logical aid, it would be possible to augment this understanding, to reinforce the
natural mobility skills and to improve the learning process, even in the case of
sensory deﬁcit (such as vision, for example).
Therefore, as far as the design of most suitable mobility aids for VIP is con-
sidered, the following questions may be asked: (1) What is the computational
model, which underlies mobility skills? (2) Which objects contribute to the space
organization emergence and must be and/or may be “extracted” from the envi-
ronment by the VIP during their mobility (3) When should this (these) extraction(s)
be done? (4) Which sensorimotor loops are relevant for such objects perception?
(5) How these sensorimotor loops can be reinforced and learnt? (6) Which pertinent
feedback/presentation of these objects is useful for mobility? (7) How can we
support the learning and acquisition of new mobility skills? (8) What digital
technologies may be used or should be designed for such computational model
implementation?
The following sections try to answer to these questions or bring some elements
for such answers elaboration.
3
Models of Human Mobility
An assistance of the travel process of VIP involves two synergetic elements: a
model of travel (mobility) (Sect. 3.1) and a technological assistance (Sect. 3.2)
which transform the cognitive model into computational model which should be
integrated in assistive devices. The state of the art of these two points are brieﬂy
discussed in this section.
3.1
Models of Mobility
A model of human mobility may be deﬁned as a set of functions which underlay the
physical mobility, e.g., all cognitive functions which allow interactions with the
space.
314
E. Pissaloux and R. Velázquez

In the previous section it was stressed that proposed so far models try to rep-
resent the physical space, and use the mobility framework known as Tolman’s
cognitive map [17]. This framework assumes that sensory data, extracted from the
environment and ﬁltered by the mobility task, are used for implementation of the
different basic cognitive functions necessary for moving [18]. Indeed, these models
assume that (1) a kind of internal map—which corresponds to the external space
internal representation—is built in our brain, and (2) this static map supervises the
motor function.
These models are interesting as they take into account the scene content in order
to build the map. However, they differ by the data extracted from the environment
and by the physical size of the near or far investigated space, [19, 20]; the size of
the physical space is important as the cognitive processes underlying the mobility
are different in the two spaces. The near space can be surveyed with a glance of the
eye or by other means for the visually impaired (probing with a cane or other
device), while the far space (or locomotor space) is too large to be seen at once, and
the appropriate assistance is necessary.
Lynch’s travel model [21] may be suitable for the far space. It is mainly based on
trajectory geometry patterns (paths) in an (mentally) “imageable” city. This abstract
model is a two-level presentation of a city for mobility purposes, as it considers that
a cognitive map of a city (brain imaged city) is built by processing two kinds of
elements of the environment: (1) local (e.g., paths and edges), and (2) global (e.g.,
city transportation’s nodes and landmarks). This model does not offer enough
speciﬁc data for mobility of VIP in near space (e.g., the conﬁrmation of the journey
progress). Furthermore, geometric elements of the environment (space invariants,
obstacles) are supposed to be visually extracted and are related to a structured space
only (a city).
Appleyard’s travel model [22] may also suit the far space only, but his travel
model is an abstraction of actually practiced walking patterns in a real city.
Consequently, this two-level model takes into account the city global geometric
layout (level 1) and its walking speciﬁcities (level 2). It is difﬁcult to generalize
these data, especially theses of level 2, for any city. As the previous model, this
model does not provide enough basic data for mobility of VIP in near space;
furthermore, the process of extraction of cues for mobility from the structured
environment is unknown.
Loomis et al.’s [23] model of mobility is a ﬁve-component model which applies
to both near and far spaces. It requires: (1) information acquisition about
self-motion and landmarks; (2) path integration (memorization the sequence of
path’s adjacent segments and their relative spatial orientation); (3) generation of a
map-like space presentation; (4) trajectory to target computing, (5) execution this
trajectory. Based on experience with congenitally blind people, this model has been
evaluated only on short paths without positional cues. Recent experimental works
with visually impaired people [24] seem show that the VIP are not always con-
scious about the generation of map-like representation of the space when they walk
and when they implement “automatically” well-known to them the path integration
strategy.
Model of Cognitive Mobility for Visually Impaired …
315

Millar’s model of mobility [25] can be used in both spaces, near and far. Based
on the mobility of congenitally blind people, it links two reference frames usually
used during the mobility: (1) ego-centric (or body-centerd), based on the axes of
anatomy of the human body, used for execution of displacement (with constantly
updated views
of the space
and its representation), and (2) allo-centric
(or exo-centric), map-like presentation of the space used for planning of the dis-
placement. However, the process underlying the reference frame changes and the
map update are unknown.
Tversky’s spatial mobility framework [26] can be used for near and far spaces.
In the near space, a usual three-dimensional model of the physical space is con-
sidered, while in the far space a two-dimensional (a map) model is considered.
Tversky’s model clearly states that a mobility path is a mean for “linking separate
views” of the space, views which depend on a reference frame attached to the space
and of the metric used (usually category and not digital metric). As in Lynch’s
model, only geometric elements of the environment (obstacles) are supposed to be
visually extracted.
Brambring two-level model [27] can be used for near and far spaces. At the ﬁrst
level, the locomotion process is deﬁned as two independent tasks: object perception
and orientation. Object perception is subdivided in two subtasks: obstacle detection
(i.e., ahead of time detection of hazardous objects so they can be avoided), and
identiﬁcation of landmarks (which are very different and more various than these of
sighted). Orientation is also subdivided in two subtasks: spatial orientation (con-
sidered as ability to know your current position during the walking in near space)
and geographic orientation (i.e., trajectory to move from one to another point in
space). In this model the concept of obstacle is not well established and only
geometric elements of the environment are supposed to be considered [28]. The
model does not clearly make a link with a sensorimotor loop [29].
Harper and Green’s [29] model, “ﬂow of travel,” deﬁnes a (aim/aimless) travel
framework as a sequential process with seven ordered steps: (1) preplan journey,
(2) decision on start and end points, (3) journey, (4) track keeping, (5) in-route
guidance, (6) movement to the next point, and (7) next point achievement. Similar,
to Brambring’s model, it extends it by adding two embedded loops: (6) to (4) and
(7) to (3). The model does not consider the locomotion space preview despite of the
fact that it suggests its addressing when creating new travel aid.
Maingreaud’s et al. model of locomotion [30] considers a space preview by
providing “obstacles–obstacles-free” presentation of the time-variant near space.
The perception-motor loop is implemented through spatiotemporal modiﬁcation of
the preview during the displacement.
Hersh’s three-level models [28] is mainly related to spatial knowledge acqui-
sition (in near and far space). It encompasses the following steps: (1) travel process
(data acquisition included), (2) cognitive map building or update, and (3) map
representation. It proposes a framework for how to learn routes. However, the
integration of this model into a supportive travel technology is not suggested.
The above presented mobility models—sometime named also de facto models
[28]—have been frequently established from spatial description of sighted or
316
E. Pissaloux and R. Velázquez

partially sighted participants with discrete data; however, the space perception of
sighted persons is very different from this of VIP [31]. Furthermore, these models
are based on single journey, and lack empirical validation with a larger number of
participants.
Moreover, these models are of a very high cognitive level and it is unclear how
the Tolman’s map are updated and how they can be translated into sensorimotor
environmental data exploitable by the VIP. Indeed, only the physical obstacles are
considered as mobility cues; neither navigational nor orientation cues are explicitly
considered. Other multimodal cues are missing; the sensorimotor loops invariants,
thus dynamic and temporal variations of space presentation are not explicitly taken
into account. Therefore, the underlying cognitive processes (e.g., mobility skills
generation, learning and their usage during the travel) are insufﬁciently addressed
and cannot be integrated in the design of new assistive devices.
The brief overview of the most popular mobility assistances proposed in the next
subsection conﬁrms the above analysis.
3.2
Some Assistive Devices for VIP Mobility
Several technological solutions have been proposed in order to assist the mobility of
VIP [32–34]. These aids may be classiﬁed in SSUD (Sensory SUpplementation
Devices) or SSD (Sensory Substitution Devices).
SSUDs aim to provide to the end user a complementary information which
reinforces the user’s personal locomotion capacities and skills. Another sense
(s)/modality(ies) are usually put to the contribution. This allows to provide subtle,
user expected, cues which ensure that the user’s main attention is on the (podo-)
tactile and auditory cues of the real world, and not on the locomotion cues of the
device. Such cues may be exploited during a task execution.
SSDs aim to provide to the end user the information via another (substituting)
sense; they offer a new language (new code) which should be learnt independently
of the initial (natural) role of this sense: a speciﬁc stimulus (a code) is associated
with each object supposed to be perceived with a new sense.
This chapter concentrates on SSUD with a tactile supplementation (and not
auditory as natural environmental audio cues are used by VIP when they move); for
a review of SSDs, a reader may refer to [32, 33, 35].
The existing assistances of VIP mobility usually support the most popular
cognitive tasks identiﬁed by the models presented in the previous section. They are:
detection of some obstacles (Sect. 3.2.1), data useful for orientation or navigation
and for wayﬁnding (Sect. 3.2.2) or for space awareness (Sect. 3.2.3).
Model of Cognitive Mobility for Visually Impaired …
317

3.2.1
Aids for Obstacle Detection
The white cane is the most popular assistance used since centuries. It provides a
pointwise space (virtual) touch in a certain solid angle, which subtends the mobility,
with the user as its apex. A cane provides a feedback to VIPs when a static obstacle
is located on the walking surface at the distance of around 1 m. Once an obstacle is
detected, the VIP should mentally elaborate an appropriated avoidance procedure
by the cognitive synthesis of the discrete tactile (and/or audio) feedbacks (which
includes the estimation of distance to the obstacle, its VIP-centered direction, its
partial and approximate shape, etc.). The initial concept of white stick has been
transformed in tools more appropriate for secure and independent mobility, espe-
cially in well structured environment such as our cities, via two generations of
electronic canes.
The electronic canes of the ﬁrst generation (1970–2010), transformed the white
stick into a detector and localizer of obstacles in longer distances (C-5 Laser Cane,
smart cane [36]), or into a detector and localizer of more complex architectural
structures such as stairs going down (robotized smart cane [37]), overhanging
obstacles for chest protection (C-5 Laser Cane, intelligent canes [38, 39] or into an
assistant for obstacles avoidance and for change of walking directions [37] (Fig. 1).
The smart cane of the University of Rouen, of the TRL 3, offers also the drift
correction for a VIP [24].
These improvements usually require additional technologies such as active
sensors (e.g., ultrasonic or infrared) for environmental data acquisition, and
vibrotactile actuators for data feedback to the user. The frequencies and intensities
of vibrations provide the information on the distance to the obstacle (a code to be
learnt).
However, the distance estimated with such sensors and the imprecision of the
provided information lead to a limited solid angle of space which can be investi-
gated (frequently no more than 1 m ahead of the user). Consequently, the tech-
nological limits do not allow these canes to detect cognitive cues used by the VIP
(e.g., cues for conﬁrmation of the correct progress of the journey such as a wall to
his right, or a recess in the wall). These sensors do not enable the detection of some
Fig. 1 From left to right Robotised smart cane, smart cane (K-Sonar), intelligent canes
(UltraCane, TomPouce)
318
E. Pissaloux and R. Velázquez

important obstacles such as small objects (e.g., stairs of high less than 25 cm or
signs). Being guided by a cane during the obstacle avoidance may lead to spatial
disorientation with more adverse consequence if the system suddenly breaks down
[40]. Furthermore, being guided implies to concentrate on the locomotor capabil-
ities of the device and not on the environmental cues which can help to understand
the space and to orient in the space.
Only two intelligent canes (UltraCane, TomPouce) are currently commercial-
ized, however, their success is limited as they only very partially use the principle
of sensorimotor loop during the locomotion and the precise location of remote
overhanging obstacles is very difﬁcult.
Some of these canes are very complex to use and require concentrated attention
for their operation which limits that availability for the locomotion process and
environment understanding. The second generation of cane tries to overcome these
limits.
3.2.2
Orientation (Navigation and Wayﬁnding) Aids
Orientation, a cognitive task, has not clear deﬁnition. Brabyn [41] proposed its
deﬁnition as knowledge where one is in absolute terms of reference.
As such global knowledge continuously changes with the travel, we propose to
deﬁne orientation as a relative knowledge of one current position in the space with
respect to the journey already accomplished and the target (future) position of the
journey.
However, a cognitive mobility of VIPs requires more than just this precise
localization information. The existing orientation aids or orientation prototypes
prove it clearly.
So far designed orientation aids can be divided into two classes: (1) a locator of a
speciﬁc beacon, and (2) user’s locator and tracking system.
This ﬁrst class of aids is mainly based on the concept of an active beacon [42], or
passive [32, 41] with audio feedbacks. In the active devices, a dedicated voice
message is sent to the user (about his/her current position, for example), after the
beacon has been automatically activated by a hand held device. In the case of
passive devices, the user sends a signal to a beacon in order to obtain the infor-
mation about his/her current place. The ﬁrst version of “Talking Signs” (today
named
as
RIAS—Remote
Infrared
Audible
Signage)
developed
at
the
Smith-Kettlewell Institute [41], provided the voice output to make accessible
dedicated indoor locations in large buildings (such as room number).
The inconveniences of the “beacon” approach is due to the fact that it is nec-
essary to install a network of beacons, to have a dedicated pointer, to be in the range
(in the cone of transmission) of the beacon’s sensor and to ﬁnd it; moreover, the
provided information may be known to others. However, improved method is
frequently applied in large public space (e.g., railway stations, airports).
The other class of orientation aids includes mainly GPS based systems (e.g.,
Drishti, Trekker Breeze, Captain, Casblip system/European FP6 project, Haptimap
Model of Cognitive Mobility for Visually Impaired …
319

system/European FP7 project, Sound-of-Vision/H2020 FP project). They are usu-
ally stand-alone system or a speciﬁc unit of an electronic cane of second generation
(2011–) (e.g., the SEES cane [43], Visio cane [44]) and they integrate a Wayﬁnder
software (e.g., Sendero’s Seeing Eye software). The latter is frequently also the
person locator and tracker.
While beacon systems can be used in indoors and outdoors, the GNSS-based
systems can be used only in outdoors. Both systems support the path integration
strategy of navigation. However, none of the systems reinforces the user’s own
locomotion capacities and skills. Indeed, such systems do not provide any means to
learn and to perceive the space, therefore to allow the end user to select an alter-
native path to reach the target or to change the path due to change of initial target.
Moreover, the density of beacons may be insufﬁcient to obtain the conﬁrmation of
the current path.
Space perception aids try to overcome these limits.
3.2.3
Space Perception (Awareness) Aids
The space perception aids aim to overcome drawbacks of detection and wayﬁnding
devices. The tactile maps dedicated to help the VIP are tools for supporting the
understanding of far space organization. At relevant scale, speciﬁc mobility maps
can assist the navigation [45, 46]. The thermoformed maps are usually used for
understanding city structure (Fig. 2 left), while the city concrete map (Fig. 2 right)
is of very limited help for them.
Very few prototypes from academia exist (e.g., SEES cane [43]), and there is no
commercially available devices [44].
Fig. 2 Mobility map technologies: thermoformed map (Paris) and concrete map (Hamburg)
320
E. Pissaloux and R. Velázquez

The SEES cane (Fig. 3) is a multimodal (tactile, voice) academic prototype
(TRL 3). The SEES system provides some global services such as wayﬁnding
assistance (via its global remote web server, iSEE), and local services such as data
on the status of trafﬁc light (via an embedded local server built in a commercial SEE
phone). Data on local obstacles are provided in the same way as electronic canes of
ﬁrst generation do (via its smart SEE stick). The SEE phone is always connected to
SEE stick through Wi-Fi.
The Netherlands regional project of I-Cane, partially ﬁnanced by the European
Commission, and the Visio cane project, France [44], extend the SEES cane’s
services by providing remote access related to data on commercial shops in a mall
(opening hours, types of goods sold, etc.).
All prototypes of canes make a large usage of available digital technologies such as
new sensors (e.g., camera, wheel encoder, accelerometer, and compass); the locating
and tracking systems use also the Global Navigation Satellite System (GNSS).
Being based on the white cane principle which provides the pointwise infor-
mation on the space, the sensorimotor loops, a base for space structure under-
standing and learning, are not efﬁciently supported. This is partly due to the fact that
the target locomotion model which should support the design of mobility aids not
explicitly addressed.
The next section proposes a holistic framework of locomotion and two syner-
getic elements which allow its integration in new mobility aids: the four-component
model of mobility and the concept of tactile gist.
Fig. 3 SEES cane principle [43]
Model of Cognitive Mobility for Visually Impaired …
321

4
Holistic New Model of Human Cognitive Mobility
This section introduces a novel model of mobility of VIP (Sect. 4.2) and its pre-
liminary hardware support, the TactiPad (Sect. 4.3). As the model is based on the
concept of tactile gist, this section addresses this concept ﬁrst (Sect. 4.1). The model
aims to support natural mobility cues.
4.1
Space Presentation by a Tactile Gist
The concept of tactile gist (for mobility) ﬁnds its roots in the mobility model of ants
(or rats) [47–49] and in its vision’s analogue, the visual gist [50]. Indeed, “for [ants
(rats)] robust scene recognition the key matter is the spatial arrangement of the
objects across the scene, and not the identiﬁcation of the speciﬁc individual objects”
[49]; visual gist provides a rapid visual overview of a scene, i.e., an “expeditious
process of seeing allowing understanding everything at once” but not in detail
[50–52].
The observed 3D scene’s tactile gist is a simpliﬁed presentation of its geometry
(or architectural/urban) layout, orthogonally projected on a 2D surface and gener-
ated in three procedures: (1) targeted task selection (mobility in our case), (2) a 2D
convex hull of 3D objects of the scene, and (3) ego-perception of the convex hull.
From computation point of view, as suggested in [53], a 3D scene tactile gist is a
(simpliﬁed) 2D representation of the observed scene by a set of the nearest edges of
the nearest obstacles seen by the observer, with respect to their depth in the scene,
from his/her ego-centric point of view (Fig. 4). Note the notch on the 2D plan “user
in the scene” which is a key point of scene presentation with a tactile gist (and
originality of the proposed approach).
This presentation preserves the spatial relationships between scene’s objects
representation and induces a partition (or a dual representation) of the observed
Fig. 4 From visual a to tactile b gist
322
E. Pissaloux and R. Velázquez

scene in two zones: obstacles and obstacles-free. This partition changes with the
observer’s movements in the space.
The tactile gist computations are performed in the Euclidian reference frame
(anchored on the subject) and using Euclidean metric.
The tactile gist scene presentation for mobility has been successfully validated
with original experiments (cf. infra).
The tactile gist is a base for the computation of the refreshable tangible images
which may be displayed on a touch stimulation device, such as the TactiPad,
presented in the Sect. 4.3. The TactiPad was used to the experimental validation of
the proposed four-component model of cognitive mobility addressed in the
upcoming subsection.
4.2
Four-Element Holistic Framework
of Cognitive Mobility in a Small Space
There is not yet a universal deﬁnition of the mobility, and proposed hereafter model
reﬁnes and extends the model of Loomis et al. [23].
By cognitive model we understand the set of all cognitive functions which allow
one to move in a space under the hypothesis that the subject is able to manage
correctly his/her physical posture (motor functions).
The mobility is a complex cognitive function, and may be analyzed as a
four-dimensional problem—the mobility function subclasses—such as walking,
orientation, space awareness and navigation. Each of these fundamental functions is
deﬁned by its space domain and the human consciousness level attached to it [23,
34]. There is no cognitive hierarchy between these functions. These functions are
interrelated, but their precise inter dependency in still open research question.
Walking can be deﬁned as a low conscious (cognitive) aimless displacement in
the near space. Walking encompasses more elementary functions such as obstacle
(ego-centered) detection and localization (e.g., distance and direction estimation
between subject and obstacles), (approximate) estimation of the obstacle form and
its height, and (allo-centered) distance estimation of between obstacles [54, 55].
Walking can include a trace function, i.e., memorization of the set of spatially
adjacent segments and their relative orientations with respect to one’s (ego-centric)
position, if the path integration is the used navigation strategy.
The orientation (usually designed by a limitative term “wayﬁnding”) can be
deﬁned as a set of continuous processes which allow to know one’s current position
in space, to update his/her the target position, and to estimate the direction to take
from one’s current position in order to reach one’s target, the target recognition, etc.
Orientation implies the capability of planning a speciﬁc route to reach the target
location from the current point [8], which implies being aware of the space
structure/organization.
The space awareness (or space knowledge) is especially linked to the near space
and requires the high consciousness level. It can be deﬁned by several cognitive
Model of Cognitive Mobility for Visually Impaired …
323

parameters such as (1) the forming and making use of the space representation
where the travel takes place, (2) the forming and making use of urban and social
data (e.g., name of the street, library, bakery, etc.), (3) acquisition and combination
of spatiotemporal sensory stimuli such as landmarks, clues, cues, trafﬁc light status,
road works, smells, (podo-)tactile stimuli, temperature), (4) the acquisition of
unpredictable events, (5) anticipation of events. Space awareness acquisition may
include a map function, i.e., the space presentation with a (geographic or not)
map. The space awareness requires a good memory (and its efﬁcient support), and
the fast and reliable matching capability between memorized and real-time per-
ceived (tactile gist of the) scene.
Navigation, a high level cognitive task, is performed in all spaces and results
from implementation of all listed above functions while traveling. From a com-
putational point of view, the navigation is an iterative process, the “perception-
orientation-space_awarness” loop which achieves once the target location is
reached.
Navigation develops the anticipation skills in order to update the navigation
strategy to real locomotion needs.
A successful locomotion requires cooperation of several elements, some of them
performed simultaneously other sequentially. Examples of simultaneous events are:
(1) path integration and space integration [33] and drift correction; (2) data on
landmark and obstacles acquisition; (3) orientation and path trace building; (4) so-
ciourban data perception and space integration.
Sensorimotor loops execution, location updates, and the drift correction are
inherently sequential.
The proposed new model is a holistic framework for mobility. It includes all
interactions functions between subject and space, as all these interactions are
necessary to understand and act in the space. Moreover, the identiﬁed interactions.
All elementary functions of the proposed model can be efﬁciently implemented
with existing digital technologies. The next paragraph presents the TactiPad, an
academic numeric prototype which may be a new type of mobility aid.
4.3
TactiPad, a Hardware Support for a Near Space
Refreshable Tangible Presentation with Tactile Gist
The TactiPad (Fig. 5) is an academic prototype of a tangible refreshable multimodal
interface. In general, the TactiPad is a support for refreshable tangible images (e.g.,
geographic map, tactile presentation of faces, tactile presentation of images, etc.).
The interface is similar to a Braille matrix of 8  8 taxels (tactile elements); a
8-dot Braille cell is a TactiPad of 2  4 taxels (dots). However, contrary to the
Braille cells, TactiPad is a Braille surface realized with shape memory alloy
technology [40, 54, 56]. The whole system is a cube of 8 cm of edge length and
weights 200 g; it is a wearable embedded device.
324
E. Pissaloux and R. Velázquez

Taxels’ interspacing is this of Braille cell, i.e., 2.6 mm; the TactiPad’s display
refresh rate is 1.5 Hz (which is suitable with the human cognitive ability to perceive
a tactile stimulation). Each taxel is individually controlled, i.e., programmable.
The TactiPad is used for space presentation with a tactile gist display. From
computational point of view, a TactiPad is a binary matrix where an obstacle (or a
part of it) is coded “1” (raised taxel) and obstacles-free zones are coded “0” (taxel in
a lower position) (cf. Fig. 6). The origin of the 3D reference frame attached to the
represented space corresponds to the subject’s position in the represented scene and
is materialized with a notch on the board of the TactiPad (Fig. 6). This reference
frame is necessary for metric data supply for mobility.
The TactiPad provides simultaneously ego- and allo-centric displays of a scene
represented by a tactile gist. TactiPad surface can be manually explored actively or
passively. The active strategies involve unconstrained hand movements and dif-
ferent exploration strategies can be used for its surface scanning (e.g., lateral
motion, i.e., moving the ﬁngers back and forth across a texture or feature), contour
following (i.e., tracing an edge within the image) and whole-hand exploration of
global shape [53]. The passive strategies use usually the whole hand superposed
over the touch stimulation surface.
Fig. 5 TactiPad, a touch-stimulating refreshable tangible multimodal interface
Fig. 6 TactiPad: a 2D matrix of tactile individually programmable elements with its reference
frame
Model of Cognitive Mobility for Visually Impaired …
325

This ﬁrst prototype of TactiPad (TRL 4) is mainly the device for obstacle detection
and space representation (with tactile gist). However, it already offers the hardware
support for space awareness and for orientation. A prolonged push on a raised taxel
provides simple voice synthesized information selected with a (voice) menu.
The second prototype of the TactiPad, under design (with 32  32 taxels),
includes all functions for orientation (implemented with technologies similar to
these used in the SEES cane, internet/Cloud technologies included). The hardware
and software of this new prototype will provide a navigational assistance through
continuous background communication with available networks and clouds.
5
Experimental Validation of Tactile Gist as a Support
for Sensorimotor Loops
The experiments validating the tactile gist as a support for sensorimotor loops
learning and recalling, have been implemented on dedicated “Perception-
Movement Platform” (PMP) (Sect. 5.1) which allow the participants (Sect. 5.2)
to perform a set of cognitive tasks such as association of cognitive information with
perceived tactile stimuli or basic mobility tasks (Sect. 5.3) using TactiPad.
5.1
Perception-Movement Platform
The built “Perception-Movement Platform” (PMP) is presented on Fig. 7.
The PMP is a dedicated 5 m  7 m room, containing a set of static, but
moveable, obstacles. The platform global image is acquired with a camera over-
hanging (6 m) the platform [56]. The navigating subject wears a bicolor hat; colors
order indicates the subject’s face (gaze) orientation. This hat is tracked by the vision
system overlooking the platform. The vision system transforms the platform’s
image in front of the subject (ego- or ﬁrst person image) into the relevant tactile gist
presentation of the scene and displays it on the TactiPad carried by the subject.
5.2
Participants
The presented experiments involved twenty blindfolded healthy volunteers, who
had never used a tactile surface before and ﬁve congenitally blind subjects.
Seventeen subjects were men and eight women, aged from 19 to 59 years. Three
participants were left-handed.
An initial questionnaire assessed participants preferred hand, their ability to use
touch-stimulating interfaces (touch screens) and to play video games.
326
E. Pissaloux and R. Velázquez

During the navigation on the platform, the environmental audio cues were
rendered inaudible by broadcasting a pink noise in the participants’ earphones. All
subjects participated in all tasks.
5.3
Proposed Experiments for Validation of the Tactile Gist
as a Support for Sensorimotor Loop
Two classes of experiments have been implemented: perception of tactile stimuli
(Sect. 5.3.1) and elementary mobility tasks (Sect. 5.3.2). Only one participant was
performing a task at any one time.
5.3.1
Perception of Tactile Stimuli
The working hypothesis assumes that the perception of tactile stimulus is successful
if the participant is able to associate a cognitive information with it, such as a
recognized object or identiﬁed moving direction.
Experimental Conditions
These experiments duration was about 30 min. Participants were seated in front of
the TactiPad (Fig. 8). In order to recognize the displayed tactile data, subjects
explored the tactile surface with a hand. This manual exploration was not con-
strained; the used hand, the number of hands, the exploration strategy and the
perception time were selected by the participant.
Subjects’ answers have been recorded by the experimenter. At the end of the
experiment, the assistance provided in the tactile stimuli recognition and overall
feelings of participants about the device were recorded.
Two types of stimuli have been evaluated: static (Sect. Tactile Perception of
Static Shapes) and dynamic (Sect. Recognition of Patterns’ Motion Direction).
Fig. 7 PMP,
“perception-movement”
platform
Model of Cognitive Mobility for Visually Impaired …
327

Tactile Perception of Static Shapes
The main goal of this task (T1) was to identify which geometric parameters of
tactile shapes are relevant for tactile presentation of scene.
The working hypotheses of this task are the following:
H1.1
a line segment is easiest to recognize;
H1.2
the size of the patterns impacts the quality and speed of recognition; the
biggest tactile patterns are easier to recognize that the pattern is of medium
or small size;
H1.3
surface patterns are more difﬁcult to recognize than the framed patterns.
Protocol
A sequence of 8 static shapes (Fig. 9) were displayed on the TactiPad. A displayed
sequence was supposed to produce a lot of changes in ﬁngertips’ tactile stimulation
(i.e., generate as much as possible the tactile gradients of amplitudes as big as
possible) [57].
The three following parameters (independent variables) of a displayed pattern
have been considered: shape (square, triangle, circle, or line segment), size (small
4  4 taxels, medium 6  6 taxels, and large 8  8 taxels), and type (surface (or
full) or framed).
Results
Table 1 summarizes the collected results; they correspond to the average value for
the whole tested population. The average was used because of very inhomogeneous
characteristics of participants.
The collected results show that:
– line segment is the easiest pattern to recognize; independently of its size, the
recognition rate is around 87%;
– the average recognition rate of a triangle is 62.3%;
Fig. 8 Experimental setup
for tactile stimuli cognitive
perception
328
E. Pissaloux and R. Velázquez

– squares and circles (full/surface patterns) are the most difﬁcult to recognize; their
respective recognition rate, independently of size and type, is 47.8 and 28.8%.
Therefore, the H1.1 is conﬁrmed by the obtained results.
As far as the geometric ﬁgure size is considered (regardless ﬁgure shape and
type), it can be observed that the average recognition rate of big patterns is 56.7%
(with the average recognition time of 25 s), this for medium pattern is 53.7% (with
the average recognition time of 28.6 s) and this for small patterns is 46.7% (and the
average recognition time is 32.6 s). The big line segment was recognized faster than
medium and small size segment (21 s vs. 27 s and 27 s). For other geometric
ﬁgures, their type (full/surface or framed) has the impact on the recognition time.
Therefore, the H1.2 is conﬁrmed by the obtained results.
Table 1 Geometric ﬁgure recognition
Geometric
ﬁgure
Type
Recognition
rate (%)
Mean time of
recognition (s)
Most frequently
confused with
Big size shapes (8  8 taxels)
Square
Surface
34
21
A triangle, a line, a
circle
Square
Framed
40
19
A circle
Circle
Surface
5
7
A square
Circle
Framed
89
32
A square
Triangle
Surface
57
23
A square, a circle
Triangle
Framed
79
52
A square
Line
segment
93
21
A square
Medium size shapes (6  6 taxels)
Square
Surface
40
25
A circle
Square
Framed
63
42
A triangle, a line
Circle
Surface
25
24
A triangle
Circle
Framed
20
41
A triangle
Triangle
Surface
75
10
A circle, a square, a
line
Triangle
Framed
63
31
A circle
Line
segment
90
27
A triangle
Small size shapes (4  4 taxels)
Square
Surface
50
42
A triangle, a circle
Square
Framed
60
43
A triangle,
Circle
Surface
20
33
A triangle, a circle
Circle
Framed
20
47
A triangle, a square
Triangle
Surface
25
16
A circle, a line
Triangle
Framed
75
20
A line
Line
segment
77
27
A triangle, a circle, a
square
Model of Cognitive Mobility for Visually Impaired …
329

As far as the geometric ﬁgure type is considered (full/surface or framed), the line
segment is excludes from the analysis as its type is surface and framed at the same
time.
The average recognition rate of big surface patterns is 32% (within 17 s), while
big framed patterns is 69% (within 34 s).The average recognition rate of medium
surface patterns is 47% (within 20 s), while medium framed patterns is 48% (within
29 s).The average recognition rate of small surface patterns is 32% (within 30 s),
while small framed patterns is 52% (within 37 s). Therefore, the recognition rate of
framed shapes is better (and faster) than this of surface shapes.
The analysis of the impact of geometric ﬁgure form and its tactile type pre-
sentation leads to the following results: the recognition rate of the surface square is
43% versus 53% for framed shapes (regardless of size); these for circle are,
respectively, 17 and 43%; and these for triangle are 52 and 72%. Regardless of
geometric ﬁgure form and its size, the framed shapes are better recognized than
their surface counterparts.
Therefore, the H1.3 is conﬁrmed by the obtained results.
Almost all subjects have selected the active exploration of the tactile surface
through lateral whole-hand exploration of global shape (movements over the global
shape), complemented sometime by contour following (i.e., tracing an edge within
the image). The stimulation of the touch with a ﬁxed raised taxels was appreciated
as it was possible to make several explorations of the same pattern with movements
of any direction.
Results interpretation
The tactile pattern recognition rate is based on the tactile stimulation of the
mechanoreceptors of ﬁngertips. Strongest mechanical stimulation (lower the
physiologically acceptable) induces biggest tactile gradient and better and faster
perception of the stimuli. However, in the related experiments, all taxels have been
raised on the same height in order to form cognitively coherent 2D geometric
ﬁgures.
The recognition speed is not really pertinent, as it is shown in subsequent
experiments, the learning process plays an important role in recognition time and it
is put in motion since the beginning of experiments.
The main results are related to the type of ﬁgure presentation: the framed type
seems to be more appropriate than its surface counterpart.
The better recognition of framed shapes can be explained by the more frequent
stimulation of ﬁngertip mechanoreceptors compared to the surface shapes. Indeed,
the most frequently observed strategy for TactiPad surface exploration was lateral
whole-hand exploration of global shape, complemented sometime by contour fol-
lowing. This means that the ﬁnger spatial parallelism plays a role in recognition;
this point was always commented by the participants. Moreover, the changes of
mechanoreceptor stimulation (tactile gradient) occur more frequently on framed
presentations than in surface presentations. These results are similar to the results by
Hein and Held, the theory of human receptors [13].
330
E. Pissaloux and R. Velázquez

Contours following were mainly used for conﬁrmation of the shape discovered
with the whole hand with its mental image of the geometric ﬁgure (memorized
visually).
Therefore, it is possible to claim that the recognition of static framed tactile
patterns with a ﬁxed display is possible under the condition of the pattern appro-
priate size and type (framed).
The TactiPad ﬁxed (and not vibrating) display of patterns allows the end user to
better perceive them and does not generate any physiological and psychological
discomfort. The good appropriation of the ﬁxed tactile display may be expected.
The TactiPad resolution (8 * 8 taxels, 2.6 mm intertaxel distance) could explain
a confusion in perception of different patterns. This means that the better (and
faster?) cognitive integration of sparse tactile information may be obtained with a
smaller intertaxel distance (i.e., with a touch stimulation device of higher resolu-
tion). The weak recognition rate of the small patterns (such as a line segment of 4
taxels) could conﬁrm this hypothesis.
The tactile shapes’ recognition may support the obstacles’ recognition in the
mobility tasks.
Recognition of Patterns’ Motion Direction
Goal
The purpose of this task (T2) was to determine if the tactile moving pattern can
allow the recognition of motion direction.
Tested hypotheses
We wanted to check if a tactile arrow, being a tactile representation of a visual
pointer of direction, could play the same role as its visual equivalent, i.e., be an
indicator of the moving pattern direction.
The working hypotheses of this task are the following:
H 2.1
The moving framed shapes (framed arrow, line segment) allows better
recognition of the direction motion.
H2.2
The framed medium size arrow is the best support for the motion direction
recognition.
H2.3
Directions North and South are easier to recognize than East and West.
Protocol
Simple patterns moving in a predeﬁned direction have been displayed on the
TactiPad. The pattern motion has been implemented by shifting the display of the
same pattern on adjacent taxels at different, temporally adjacent, time slots (every
300 ms); therefore a shape needs 2.4 s in order to sweep the entire tactile surface.
Model of Cognitive Mobility for Visually Impaired …
331

The two consecutive directions were randomly selected but in such a way that
they offered the maximal stimulation of ﬁngertips.
The three independent parameters of experiment were: the shape, the shape size
and the motion direction. The possible values of shape were: surface arrow
(triangle), framed arrow, and line segment (cf. Fig. 10); the possible sizes were big
(8 taxels) or medium (6 taxels) for triangle/arrow base; the size of line segment was
8 taxels; the possible motion directions were orthogonal (North, East, West and
South or NEWS).
Participants should recognize the motion direction (one from expected NEWS
directions) but not the displayed form. If the wrong answer is obtained the same
shape is displayed second time (but no more).
The TactiPad surface exploration methods were free.
Collected results
Table 2 summarizes the results of NEWS directions recognition for two moving
shapes (line segment and arrows) with three representations of an arrow (surface,
big framed and medium framed). These results are averaged by the size of the tested
population. The average was used because of very inhomogeneous characteristics
of participants.
The average recognition rate of the movement direction with framed patterns is
85% and signiﬁcantly better than with surface patterns (57%). The best recognition
rate of the moving direction is with the framed arrow (medium arrow: 88%).
Therefore, moving framed shapes are generally better recognized that surface
shapes what conﬁrms the H2.1.
The average recognition time of a direction is 20 s (with 16 s for north and 17 s
for south direction).
The moving direction recognition rate with a line segment is 73%, this with a
framed arrow is 82% and this with a surface arrow (triangle) is 57% only.
Fig. 9 Evaluated tactile patterns (white/black pixels: active/inactive taxels)
332
E. Pissaloux and R. Velázquez

Therefore, the framed medium size arrow is the best support for the motion
direction recognition. Therefore, the hypothesis H2.2 is validated.
The average rate of direction recognition is of 75%. The north and south
directions recognition with a medium framed triangle was better and faster than the
recognition of east and west directions. The north directions recognition with a line
segment is of 83%, while the south direction recognition is only 27%.
Therefore, the hypothesis H2.3 is validated.
Moreover, the direction recognition with a surface arrow is close to 57%. These
results conﬁrm those of triangle recognition.
All subjects have selected a passive exploration of the tactile surface, i.e., they
put their ﬁngertips on the TactiPad and tried to understand the motion direction.
The participants have pointed out the confusion in recognition: the big triangle
has been frequently mixed with a line segment moving in a diagonal direction.
The movement recognition is better for framed patterns than for full ones.
Results interpretation
A shape moving to the North (N) or to the South (S) direction follows the
ﬁngers’ “natural” direction; this could be an explanation of the best recognition of
these directions. This result suggests also that the loss of physical continuity
between adjacent ﬁngers during TactiPad passive exploration, when information
Table 2 Collected results for motion direction recognition
Moving shape
Type
Size
Recognition rate
Mean time of recognition
NORTH direction
Arrow
Framed
Big
88
13
Arrow
Framed
Medium
100
15
Arrow
Surface
Medium
28
17
Line segment
Big
83
20
EAST direction
Arrow
Framed
Big
79
16
Arrow
Framed
Medium
85
38
Arrow
Surface
Medium
80
14
Line segment
Big
89
16
WEST direction
Arrow
Framed
Big
81
13
Arrow
Framed
Medium
76
47
Arrow
Surface
Medium
58
28
Line segment
Big
91
16
SOUTH direction
Arrow
Framed
Big
78
11
Arrow
Framed
Medium
90
26
Arrow
Surface
Medium
60
15
Line segment
Big
27
15
Model of Cognitive Mobility for Visually Impaired …
333

moves in a direction perpendicular to “ﬁngers’ directions,” increases the cognitive
load for east and west directions recognition.
The tactile arrow pattern is relevant for healthy subjects who know the meaning
of a visual arrow for their spatial orientation.
On the average, the recognition of directions seems a less complex cognitive task
that the pattern recognition because of the quantity of data which should be
analyzed.
The direction recognition (and the recognition time) are important for
exploitation of data provided by dynamic maps represented by the tactile gist.
Remarks on Tactile Pattern Recognition
For the two experiments presented above, standard deviation shows a very large
intersubject variability. This variability allows to establish the very preliminary
rules for tactile pattern perception and recognition. This variability explains why the
average values of collected results have been presented here. Therefore, evalua-
tions, with large number of participants, are necessary.
The average success rates of tactile pattern recognition tasks with the TactiPad
are very promising. Consequently it seems that the tactile gist will allow to track the
evolution of object frontiers during the mobility. Finally, the collected results show
that the TactiPad it is possible support for the “tactile gist.”
5.3.2
Elementary Mobility Tasks
The considered mobility tasks (T3–T6) are the following: obstacles awareness
(Sect. Obstacle Awareness, T3), homing (Sect. Homing and Obstacle Avoidance,
T4), passage localization and passage size estimation (Sect. Passage Localization
and Passage Size Estimation, T5) and large space understanding from local views
(Sect. Large Space Understanding: Space Integration, T6).These tasks have been
selected as they are baselines for other more complex mobility tasks.
These tasks provide a support for sensorimotor loops as the participants explore
the space with a TactiPad and perform movement according the their understanding
of space presented on the TactiPad by a tactile gist and by the tactile feedback
provided by the TactiPad once they performed the movement.
All tasks have been implemented on dedicated “Perception-Movement Platform”
(PMP) were six rectangular obstacles were randomly placed; ﬁve of them were near
the platform edges and the sixth was located randomly inside the platform
(cf. Fig. 7).
334
E. Pissaloux and R. Velázquez

Obstacle Awareness
Goal
This task T3 overall goal was to check whether or not the tactile gist can provide
efﬁcient assistance for independent mobility by providing data on obstacles in the
space near by the participant.
Tested hypothesis
Tested hypothesis was the following:
H3: Tactile interface can provide the necessary and sufﬁcient information to
assist obstacle avoidance (obstacle awareness).
Protocol
The task has three stages: oral explanation, learning, and execution (navigation).
During the explanation, a participant learnt about the relationships between
tactile displays provided by the TactiPad and obstacles localized on the PMP
platform. The notch on the TactiPad surface represents participant in the space and
the obstacles localized on the platform in front of the him/her are represented by a
tactile patterns. The TactiPad notch allows to estimate the ego-centered distance to
obstacles. The tactile patterns and their place on the TactiPad change with partic-
ipants movements.
During the second stage the participant, with the assistance, learnt how to use the
TactiPad when moving and how to match the real obstacles with their represen-
tations and positions on the TactiPad.
During the navigation, once the participant has the bicolor cap and the blinding
mask, (s)he was positioned at a random location on the platform and walked freely
in it using the TactiPad and trying to avoid the obstacles localized on the platform
(Fig. 11). This was the only instruction given to the participant.
During the navigation, the PMP registered participants trajectories, the corre-
sponding direction of the gaze (approximated with the bicolor hat) and the time and
location of each obstacle contact.
At the end of the experiment still blindfolded participant was accompanied to a
room for ﬁnal discussion and ﬁlling a semi-structured questionnaire.
Fig. 10 Arrows (surface and framed) and line segment “moving” in the East direction
Model of Cognitive Mobility for Visually Impaired …
335

Collected results
Values of three parameters were registered:
– the number of physical contacts (bumps) during the considered slot time;
– the effectively explored physical space;
– the space perceived.
The average number (for all participants) of contacts with obstacles during
10 min free walking on the PMP platform was 5. Figure 12 summarizes the results
on frequency of visiting different places of the PMP platform during the free
walking stage; the left image shows the orthogonal view of the PMP platform
(navigation space) while the right image shows the frequency of participant pres-
ence in this place of the platform (the average value for all participants). It should
be observed that all participants visited 24 same places on the platform (Fig. 12
right, white pixels); the most frequently observed gaze orientation, shown with a
white arrow, conﬁrms that participants perceived well the obstacles with the
TactiPad.
The effectively explored physical space is deﬁned as the surface physically
explored by subject during the walking in the considered slot time; it is quantiﬁed
by as a surface of all rectangles attached to subject’s trajectory (a rectangle being
deﬁned by the subject shoulders’ width and subject’s one-step forward distance).
Figure 13a image shows an example of a participant’s explored space (partici-
pant’s trajectory, and Fig. 13b) image perceived space (in the sense of the above
deﬁnition) associated to the explored space. The collected results show that over a
time slot of 10 min, on the average 72% of the available area was explored.
The space perceived via TactiPad is the surface of physical space displayed on
the TactiPad during the walking in the considered slot time (Fig. 13b). The
Fig. 11 Participant wearing
bi color hat, headset (for the
pink noise) and carrying the
TactiPad (at the height of his
belt)
336
E. Pissaloux and R. Velázquez

collected results show that over a time slot of 10 min, on the average 96% of
available area was perceived.
Results interpretation
From the obtained results (Fig. 12b) it is possible to conclude that subjects had a
good awareness of obstacles’ presence as the number of contacts is low (5) with
respect to the large part of the explored space (96%).
The few contacts are often due to misjudgment of distance. Indeed, often it was
noted that the participant was aware of the presence of obstacle (he turns around, he
very gently approached it), but ﬁnally he overestimated the distance to it. He did
one step too to the obstacle and touched it with his foot.
This result is consistent with results concerning pattern recognition on the
interface: poor spatial stimuli resolution is certainly the cause. Indeed, participants
“feel” an obstacle to reach them when they are approaching, but they have a poor
deﬁnition of its spatial position and, therefore, the relationship between their own
speed and approach speed of obstacle on the interface.
As far as the timing of these contacts is considered, it is not possible to establish
a rule. For the contacts which occur at the beginning of free navigation we can
assume that the lack of participant familiarity with the interface and tactile navi-
gation causes their occurrence; for those of the end of the experiment, which were
less frequent that at the beginning, we can assume they are due to participant
fatigue. However, the increased familiarity with the interface and the lower number
of bumps in the end of experiments conﬁrm that TactiPad provides an assistance for
sensorimotor loops (tactile perception and mobility).
The obtained results indicate that the TactiPad allowed participants to determine
the presence of obstacles and their approximate locations. Therefore, the hypothesis
H3 is validated.
Fig. 12 PMP navigational space (right) and frequency of different places of the PMP platform
Model of Cognitive Mobility for Visually Impaired …
337

Homing and Obstacle Avoidance
Goal
Task T4, “Homing”, is a “sensorimotor” version of the classic triangle (homing
vector) task [23]). Knowing that there is an obstacle on the PMP platform, the
subject was asked to turn around it and return to his/her starting point.
T4 investigates possibility the pertinence of the TactiPad, a refreshable tangible
to localize an object, to correctly estimate the distance between subject and
obstacle, to approximate shape necessary to walk around and to assist the orien-
tation in space (assist the return to the starting point).
Tested hypotheses were the following:
H4.1
A tactile device allows to localize an obstacle.
H4.2
The tactile gist allows to estimate the shape of an obstacle in order to walk
around it.
H4.3
The sole tactile gist does not assist the return to the starting point.
Experimental protocol
Each participant was explained that “(s)he will be placed at the entrance point of
a PMP platform which is limited by a continuous wall; one obstacle is randomly
placed on the platform. (S)He is asked to ﬁnd this obstacle, take a walk around it
and return to the platform entrance.”
(S)He was also told that one taxel of the TactiPad matches à real surface of about
30 cm  30 cm. There was no indication on the shape of the obstacle.
The expected outcomes were: (1) subject’s itinerary which was ﬁlmed by the
camera overhanging the platform, and (2) the sentence “I have ﬁnished”.
Once the subject gave authorization to proceed, (s)he put the bi color hat on his/her
head and put the TactiPad on (Fig. 11), then was conducted to platform entrance.
Collected results
Figure 14 shows ﬁve examples of the ﬁlmed trajectories. It can be observed that
all participants have correctly localized the obstacle and none of them have touched
it. In the whole tested population of 20 participants, 19 subjects did at least one entire
tour around the obstacle without touching it; so task success rate is 95%. Therefore,
the hypothesis H4.1 is validated (this result conﬁrms these obtained in T3).
Figure 14 shows that four over ﬁve subjects made the least one entire tour
around the obstacle. In the whole tested population of 20 participants, 17 subjects
did at least one entire tour around the obstacle, i.e., task success rate is 85%.
Therefore, the hypothesis H4.2 is validated, i.e., the tactile gist allows to roughly
estimate the shape of an obstacle (and avoid it).
As far as ‘the return to the starting point” is considered, it can be concluded from
Fig. 14 that only two subjects (B and C) returned to their starting point; some subjects
are convinced to return to their starting point while in reality they did ¾ or 1.5 or of the
338
E. Pissaloux and R. Velázquez

cycles. In the whole tested population of 20 participants, 7 subjects returned to their
starting point; most frequently, participants did between ¾ and 2.5 of the cycles.
Therefore, the task success rate is 35% and the hypothesis H4.3 is validated: the sole
tactile gist does not assist efﬁciently the return to the starting point.
Different subjects used different strategies for walking around the obstacle. From
Fig. 14, it can be observed that the subject C (a congenitally blind subject) took the
obstacle as it rotation center; others (e.g., the subject A, late blind) walked around
the obstacle touching it virtually (through the TactiPad), from time to time, in order
to know whether or not the obstacle is behind him.
Interpretation of the results
The mistakes made by users are particularly interesting as they seem conﬁdent of
being returned to their starting point. Also, even if they do not appear to totally trust
to the gist presentation, they use it and rely on it to locate in the global environment.
It should be observed that the users use information which changes with their
displacement; moreover, they foresee it as “inaccurate”. Therefore, their behavior
observed through mobility trajectory can be explained in two ways:
– participants lacked of conﬁdence in the vestibular information or did not pay
attention to their ego-motion, therefore they absolutely seek to use another—
exogenous—information,
– the participants seek to merge the (variable) gist information with their own
vestibular information (with different success).
Concerning the validity of tactile gist for the homing task, it is necessary to ﬁnd
others distinctive features (visual or not) in the environment which would stabilize
the purely geometric data of regularly (especially symmetrically) shaped obstacles.
They can be iconic (poster, color, other object, etc.) or based on the geometry of the
whole scene itself [58].
In this task, despite of the necessary orientation data, only the geometrical
indications (cues) have been extractable due to the proposed tactile gist presentation
of a scene. This point of the design of the TactiPad should be improved; operations
(a)
(b)
Fig. 13 a Effectively explored space, b effectively perceived space
Model of Cognitive Mobility for Visually Impaired …
339

such as zoom-in/out or TactiPad better resolution which will allow to obtain ﬁne
distinctive details of the obstacle shape should be considered.
From semi-structured questionnaires, participants pretend the more successful
use of strategies in which the focus is on the “homing.” They used the interface to
avoid the obstacle without using the displayed geometry as a salient index of the
environment, which could serve as a reference when performing the task.
This experiment showed that tactile gist and a refreshable tangible device seem
pertinent for obstacle localization while walking. However, the usage of purely
geometrical local representation of an obstacle may be insufﬁcient in some navigation
tasks. Other distinctive features and/or the geometry of the whole environment should
be considered. Finally, it seems that the TactiPad reinforces the natural mobility skills
of the participants as the obstacle was never touched by any of the 20 participants.
The post-experimental discussions highlighted the fact that subjects who per-
formed better in this task implemented a strategy where returning to the starting
point was their main goal. They used the TactiPad only for obstacle avoidance and
not for using the platform’s geometry as a salient feature of the environment. This
conﬁrms that the classic mobility strategies learnt in our youth are local, and that the
possibility to have access to a representation of the larger space is not used by these
strategies. This opportunity for building new mobility strategies and for better
spatial understanding should be evaluated in depth.
Finally, participants appreciated the continuous support of the TactiPad as they
walked around the platform. This shows that the TactiPad provides a good support
for memorization (and recall) of the sensorimotor loops.
The impact of gender and congenital blindness for the execution of this task was
not investigated due to the small number (20) of participants.
Passage Localization and Passage Size Estimation
Goal
The goal of the T5, “Size of passage estimation” (affordance) which constitutes a
baseline for the T6, is localization of a passage (e.g., a doorway, at the center, to the
right, to the left) and estimation of its size from TactiPad display before deciding to
pass through or not. Moreover, it investigates the pertinence of the TactiPad for the
rough estimation of a metric distance.
Therefore, this task investigated how well the untrained participants can locate a
passage and estimate its size.
Tested hypotheses were the following:
H5.1
it is possible to locate/recognize the passage
H5.2
it is possible estimate the size of passage from its tactile presentation and to
learn metric distance using a touch stimulation unit.
340
E. Pissaloux and R. Velázquez

Protocol
He was seated on a chair in front of a table placed in the middle of the PMP
platform. The TactiPad was put on the table in front of the user. Each participant
wore an eye mask (if necessary).
This task has two independent parameters (cf. Fig. 15):
– passage location with respect to the subject position (no passage, located cen-
trally, to the left, to the right);
– sizes of the passage small (2-taxel), medium (3-taxel) and large (4-taxel).
A passage location were randomly displayed on the TactiPad; the size of the
displayed passage was also randomly selected (by a dedicated software).
The participant was told that “(s)he will be placed in a virtual space where a wall
may have a passage. His/her task is to determine whether or not there is a passage in
this wall and, if the passage is present, (s)he should estimate if its size allows
him/her to pass through.”
The participant was also told that one TactiPad’s taxel matches à real surface of
about 30 cm  30 cm, and that the average width between the human’ shoulders is
around 60 cm. Expected answers were: “no passage,” “no” if he cannot pass,
and “left” , “right” or “central”.
Once the subject gave the authorization to proceed, a wall was displayed on the
TactiPad. Once (s)he gave an answer, a new wall was displayed.
The task was repeated twice in order to evaluate the learning effect (the ﬁrst ﬁve
attempts were considered as learning).
Collected results
Table 3 shows the evolution of the passage location recognition rate as a
function of its location in the wall. Learning increases, in a homogeneous way, the
correct recognition rate by roughly 50%.
Fig. 14 Trajectories of ﬁve subjects during the homing task
Table 3 Statistics for passage localization
Passage localization
Centrally located (%)
Left located (%)
Right located (%)
Correct recognition without learning
52
42
63
Correct recognition with learning
93
65
95
Model of Cognitive Mobility for Visually Impaired …
341

Therefore, the hypothesis H5.1 is conﬁrmed.
Table 4 shows the evolution of the recognition rate as a function of the passage
size. As previously, learning increases the recognition rate, especially for the small
passage (by 100%).
Table 5 shows the evolution of the response time with and without the learning.
Globally, the average response time of participants is low.
The results summarized in Table 5 conﬁrm the hypothesis H5.2.
Results interpretation
This task has twofold meaning: affordance (possibility to perceive) and object
recognition (passage location). The results obtained clearly show that learning
signiﬁcantly improves the success rate of passage size estimation.
Several factors can inﬂuence such results: the ability of the participant to project
him(her)self in the tactile gist representation of the passage provided by the
TactiPad, the experience (learning of the representation), or the repetition of the
stimulations generated by the TactiPad (which probably increases the subject’s
sensitivity).
The recognition rate of the left passage is far less than other, which may be
explained by the subjects laterality; indeed, as all subjects used their right hand for
TactiPad exploration, the left part of the tactile display was considered as the central
part.
Tactile gist seems to be an efﬁcient way for stationary representation of the
scene, and the Tactipad is an efﬁcient support and allows fast and easy execution of
the task and learning; moreover, it seems that the TactiPad reinforces the natural
perception skills of the participants.
This task shows clearly that it is possible to learn the metric relationships
between distances displayed on the TactiPad and physical space (once the scaling
factor—here 1 taxel = 30 cm  30 cm—is known).
The effects of gender and congenital blindness on this task execution were again
not investigated due to the small number (25) of participants. However, the sub-
ject’s laterality may inﬂuence this task recognition rate.
Table 4 Statistics for passage size estimation
Percentage (%) of the recognized passage
Small (%)
Medium (%)
Large (%)
Correct recognition without learning
25
64
69
Correct recognition with learning
66
89
95
Table 5 Recognition time of a passage
The recognition time
Average (in s)
Minimal (in s)
Maximal (in s)
Average time without learning
18.9
6.8
41.5
Average time with learning
12.2
7.5
20.9
342
E. Pissaloux and R. Velázquez

Large Space Understanding: Space Integration
The task T6, “Apartment geometry (layout) reconstruction from walking” is a
cognitively complex task as the participant should collate isolated tactile repre-
sentations of each room in the global architecture of the apartment.
Therefore, this task investigated how well the untrained participants are able to
recognize the architectural layout of the whole apartment from isolated tactile
layouts of its rooms investigated sequentially during the virtual navigation in this
apartment.
This task investigates also if the tactile gist and TactiPad contribute to the
emergence of an efﬁcient representation of a more global space (the apartment)
from its local representations (individual rooms). This task is named space
integration.
Tested hypothesis is the following:
H6
it is possible to reconstructing the geometry of a larger space from tactile local
snapshots of different subparts of this space.
Protocol
Each participant was seated on a chair in front of a table placed in the middle of
the PMP platform. The TactiPad was put on the table in front of the user.
(S)he was told “you will virtually navigate in a 4-room apartment, from room 1
to room 4, using the TactiPad.” The TactiPad will provide you a layout of a room
seen from its entry door (there is one entrance to the room and one exit). You can
explore its tactile presentation as long as you wish. Once you declare that you fully
understand the current display, you will be asked to report its layout as a drawing.
If you want to move to the next room you simply say: “next room please,” and
the next room will be displayed on the TactiPad and you repeat the process until
you ﬁnish room 4 (end); there is no obstacle on your itinerary when you navigate
from one room to the next.
At the end, your four drawings will be concatenated in order to get the total
explored space.”
Figure 16a gives the layout of the investigated 4-room apartment, while Fig. 16b
shows the 4 snapshots of each room and their tactile coding on the TactiPad. The
exploration time of each drawing has been registered.
Collected results
This task is composed of two interrelated subtasks: the recognition of layouts of
each room from its tactile (gist) presentation, and (mental) integration of the whole
apartment layout from rooms’ layouts. Figure 17 shows an example of drawings
made by 4 blindfolded subjects (from top to bottom) in the ﬁrst sub-task. These
drawings are similar to the space representation displayed on the tactile device
Model of Cognitive Mobility for Visually Impaired …
343

(Fig. 16b), especially those done by subject 4. Data collected from all 20 sighted
participants show that roughly 70% of them (14 participants) have correctly rec-
ognized the layout of each room.
During the implementation of the second subtask, each participant was asked to
select the layout between four supposed to match the layout of the explored
apartment. During the selection, sighted subjects used their own drawings, while
the congenitally blind subjects used the tactile (ﬁxed) drawing made by volunteer
undergraduate students using their textual descriptions of each room layout.
Figure 18 shows the proposed layouts, where B is the correct layout .
In response to question after the experiments, almost all the participants stated
that they proceeded by elimination of the less probable layouts. For example, in
room (1), subject 1 had translated both entrance parallel-to-him/her lines into
perpendicular lines. He did not detect the triangle in room (2) but had acceptably
represented rooms (3) and (4). When reconstructing the whole layout, based on his
ﬁrst drawing, subject 1 ﬁrst has discarded layouts A and B. From his second
Fig. 15 Passage size estimation task: four investigated cases
Fig. 16 a 4 room apartment and the virtual navigation path (from 1 to 4); a cone in each point 1–4
approximates participant’s ﬁeld of view; b tactile snapshots of each room displayed on the
TactiPad from the entrance point to each room (1–4). The black point on the tactile display border
represents the participant’s position in the observed scene
344
E. Pissaloux and R. Velázquez

drawing, he pointed to the layout D but his third drawing suggested him that he
missed the triangle in the second drawing, so he pointed then to C. The door’s
position in the fourth drawing had indicated that there is an error in C and subject-1
reconsidered his B choice. From C and B, subject-1 was more persuaded by the ﬁrst
drawing and ﬁnally he chose the layout C.
From the obtained results it is possible to claim that the hypothesis H6 is
conﬁrmed.
Figure 19 shows the space exploration and recognition time per subject. The
recognition time for ﬁve subjects, except number 2, has decreased with the
exploration time, and the average recognition time is around 3 min for totally
inexperienced subjects.
Fig. 17 Drawings of four blindfolded subjects 1–4 from top to bottom (lower line) of their
understanding of the tactily explored rooms (1–4)
Model of Cognitive Mobility for Visually Impaired …
345

It was observed that the fastest subjects tried to obtain a global idea of the space
explored (by rapid scanning of the whole tactile surface), while the slower subjects
have tempted to get the details of the scene elements (and have used the sequential
follow during the representation acquisition).
Fig. 18 Four proposed layouts of the apartment for space (whole apartment layout) integration
Fig. 19 Exploration times
per subject in the apartment
layout recognition task from
tactile gist and participant’s
drawings
346
E. Pissaloux and R. Velázquez

Results interpretation
An analysis of drawing of each room conﬁrms that the tactile gist is suitable for
geometric layouts presentation and that the spatial information displayed on a touch
stimulation device, such as the TactiPad, may be is pertinent to support of the tactile
gist, and to support the emergence of the high level concepts.
The obtained results show that the apartment overview was more effective and
accurate for all participants.
A methodological lesson has been learnt: the ﬁrst attempt to reconstruct the
apartment did not involve any drawings; however it was realized that this space
integration task was too complex as the subjects were capable of remembering only
the tactile layout of the last explored room and just for a short period of time (in the
average 20 s). Consequently, this experiment methodology was modiﬁed to the
presented above.
The post-experiment discussions with participants conﬁrm the pertinence of the
TactiPad as a tactile gist display unit. The decrease of the room’s exploration time
shows that the learning of tactile gist presentations is fast. This suggests a
learning/habituation to the task, a pertinence of the tactile gist concept for layout
presentations, and a potential fast appropriation of the TactiPad as a mobility aid.
The importance of gender and the effects of blindness on task execution were not
investigated.
5.4
Summary of Main Results
The obtained results can be summarized as follows: it is possible using tactile gist
displayed on a touch stimulation surface such as TactiPad
1. To recognize basic geometric ﬁgures, especially these represented by frames
(T1).
2. To determine the direction of motion of a (medium) arrow with the best
recognition of North and South movement (T2).
3. To detect and avoid obstacles after a very short learning period (T3).
4. To detect a passage and to estimate its size (T5).
5. To establish metric relationship between data displayed on the TactiPad and in
real scene (T5).
6. To integrated large space from its subparts’ snapshots tactily displayed (T6).
7. To navigate while avoiding obstacles and represent and create obstacle layouts,
using egocentric and allocentric representation of the space (T6).
The sole tactile gist representation of a scene local geometry is insufﬁcient to
orient in the space, and especially to return to the starting point (T4). A holistic
approach is necessary.
All these results conﬁrm the pertinence of the TactiPad as a refreshable tangible
device for tactile gist display and space representation, and conﬁrm the importance
Model of Cognitive Mobility for Visually Impaired …
347

of sensorimotor loops in the space integration paradigm (through the reconstruction
of the apartment’s whole layout).
It should be stressed that the age and education level impacted the tactile gist
understanding. All six senior participants, with very heterogeneous level of edu-
cation, needed more time to understand the code; young subjects, all graduates with
bachelor level at minimum, tried to use the device without understanding its
working principle and ﬁnally, after few minutes of the usage of the TactiPad, they
declared they understood the basis of its use. However, these two different learning
strategies—one based on a deep understanding of the tool and the other based on a
‘trial and error’ approach, did not inﬂuence the success rate in task execution. These
results conﬁrm similar ﬁndings already obtained [56].
Finally, the proposed experiments can be considered as ﬁrst benchmark for
evaluation and comparison of mobility assistive devices.
6
Conclusion
This chapter summarizes the results related to the mobility of the visually impaired
(VI) and the design of assistive technology. It stresses that the mobility is a complex
high-level cognitive task and requires the holistic approach.
The limitations of existing models of mobility are overcome with the aid of
the proposed new holistic four-component model of mobility, which includes
(1) obstacle detection, (2) space orientation (3) awareness, and (4) motor
displacement.
The signiﬁcant characteristics of this model are that (1) the mobility process is
considered as a sensorimotor loop which contributes to continuous updates of the
cognitive map, for interaction with the space when moving and for understanding
the space, (2) tactile gist, an appropriate tactile space representation (or coding) for
mobility, which is easy to understand and exploit when moving.
Implemented experiments validate the proposed model with voluntary, con-
genitally blind and blindfolded, subjects. They investigated some basic (elemen-
tary) cognitive functions which subtend the mobility such as perception of tactile
geometric ﬁgure (static and dynamic), obstacle detection and egocentric localiza-
tion, passage and space metric data estimation, homing tasks and space integration.
The experiments were implemented on a purpose built “perception-motor”
platform (PMP), and used the TactiPad, a digital tangible unit for tactile gist display.
The collected results conﬁrm the pertinence of the proposed mobility framework
as a support for natural mobility skills and sensorimotor loops.
The proposed model also constitutes a framework for the design of new mobility
aids. Indeed, the TactiPad is a refreshable tangible unit, and its preliminary for-
mative evaluation through the proposed experiments shows that it supplements or
improves a person’s natural ability to navigate.
The TactiPad may support different mobility strategies used by VI such
perimeter, gridline, reference point and cyclic searches [33], which may be useful
348
E. Pissaloux and R. Velázquez

for the implementation of other cognitive tasks. Therefore, the TactiPad allows not
only seamless navigation (as a non-focal activity), but also other interactive tasks in
2D and 3D spaces.
Future research should continue in several directions. It is mandatory to identify
all the elementary functions of mobility and evaluate them experimentally; all
stakeholders (including locomotion teachers) must be involved.
The extensive evaluation of all the above-mentioned tasks should include a large
number of subjects with various degrees of visual impairment (and ARMD subjects
included) in order to obtain statistically valid results. This summative evaluation
must be done in virtual and real physical environments.
The simple rules for automatic generation of tactile gist presentation of the near
mobility space are necessary.
The bigger size and higher resolution of the TactiPad (with zoom in/out dis-
plays) is necessary for a better mediation between space and VI. The pure geometric
representation of the space with a tactile gist should be complete with other
modalities for orientation purposes.
A new assistive device based on the TactiPad should offer several services
involving new digital technologies such as access to the Internet, to the Cloud, to
Wi-Fi, to the current status of the environment (e.g., status of the trafﬁc lights,
knowledge of road works).
All these improvements should allow the elaboration of new mobility strategies.
Finally, the importance of gender, blindness, age, and education levels should be
investigated with a larger cohort of participants. This will allow us to adapt the
TactiPad capability to subjects having different cognitive proﬁles and transform the
TactiPad into a more efﬁcient tool for access to space by visually impaired.
Acknowledgements This research has been ﬁnancially supported by several institutions: the
CNRS (ROBEA program), the French Ministry of Education and the Mexican CONACYT
Research Agency, the CEA (Commissariat à l’Energy Atomique et aux energies renouvellables),
and the European Commission (AsTERICS FP7 project). We thank all of them for their support.
We would like to thank our undergraduate and master students and the MIT students (trainees in
our research group) who participated in implementation of the experiments presented in this paper.
References
1. Kant E (1771) Critique of pure reason. PUF, Paris
2. Mûller GE (1896) Zur Psychophysik des Gesichtsempﬁndungen (Concerning the psy-
chophysics of visual sensations). Zeitschrift für Psychologie 10:1–82
3. Poincaré H (1911) La valeur de la science. Flammarion, Paris
4. Piaget J (1937) La construction du réel chez l’enfant. Delachaux and Niestlé, France
5. Gibson JJ (2015) The ecological approach to visual perception. Taylor & Francis, UK
6. Bruce V (1993) La perception visuelle, physiologie, psychologie et écologie. P.U.G, France
7. Berthoz A (1997) Le sens du mouvement. Odile Jacob, Paris
8. Golledge RG (1999) Human wayﬁnding and cognitive maps. In: Golledge RG (ed) Way
ﬁnding behaviour. John Hopkins University Press, USA, pp 5–45
Model of Cognitive Mobility for Visually Impaired …
349

9. O’Regan JK, Noë A (2001) A sensorimotor account of vision and visual consciousness.
Behav Brain Sci 24(5):883–917
10. Philipona D, O’Regan JK, Nadal J-P (2003) Is there something out there? Inferring space
from sensorimotor dependencies. Neural Comput 15(9):2029–2049
11. Frolov AA (2011) Physiological basis of 3-D external space perception: approach of Henri
Poincaré. In: History of the neurosciences in France and Russia. Hermann, Histoire des
Sciences
12. Bach-y-Rita P, Collins C, Saunders FA, White B, Scanned L (1969) Vision substitution by
tactile image projection. Nature 221(5184):963–964
13. Held R, Hein A (1963) Movement-produced stimulation in the development of visually
guided behaviour. J Comp Physiol Psychol 56(5):872–876
14. Auvray M, O’Regan JK (2003) L’inﬂuence des facteurs sémantiques sur la cécité aux
changements progressifs dans les scènes visuelles. L’année psychologique 103:9–32
15. Velázquez R, Maingreaud F, Pissaloux EE (2003) The intelligent glasses concept: A new
man-machine interface concept integrating computer vision and human tactile perception,
Eurohaptics, Dublin, pp 456–460, 6–10 July 2003
16. Noë A, O’Regan JK (2000) Perception, attention and the grand illusion, Special issue on
Inattentional Blindness. Psyche 6(15)
17. Tolman EC (1948) Cognitive maps in rats and men. Psychol Rev 55(4):189–208
18. O’Keefe J, Nadel L (1978) The hippocampus as a cognitive map. Oxford University Press,
UK
19. Tversky B (1993) Cognitive maps, cognitive collages, and spatial mental models. In:
Frank AU, Campari I (eds) Spatial information theory: a theoretical basis for GIS.
Proceedings of COSIT ’93. Lecture notes in computer science, vol 716. Springer, Berlin,
pp 14–24
20. Tversky B (2005) Functional signiﬁcance of visuospatial representations. In Shah P,
Miyake A (eds) Handbook of higher-level visuospatial thinking. Cambridge University Press,
UK, pp 1–34
21. Lynch K (1960) The image of the city. MIT Press, Cambridge, Mass
22. Appleyard DC (1970) Styles and methods of structuring a city. Environ Behav 2:100–116
23. Loomis JM, Klatzky RL, Golledge RG, Cicinelli JG, Pellegrino JW, Fry PA (1993) Nonvisual
navigation by blind and sighted: assessment of path integration ability. J Exp Psychol, Gen
122(1):73–91
24. Breuneval A (2016) Conception d’une aide à la mobilité d’un déﬁcient visual, LITIS,
University of Rouen, technical memorandum, 8 July 2016 (congenitally blind early
researcher)
25. Millar S (1994) Understanding and representing space: theory and evidence from studies with
blind and sighted children. Clarendon Press/Oxford University Press, Oxford
26. Tversky B (2001) Spatial schemas in depictions. In: M Gattis (ed) Spatial schemas and
abstract thought. MIT Press, USA
27. Brambring M (1985) Mobility and orientation processes of the blind. In: Warren DH,
Strelow ER (eds) Electronic spatial sensing for the blind, vol. 99, NATO ASI series. Springer,
NL, pp 493–508
28. Hersh MA (2016) Travel and information processing by blind people: a new three-component
model. Glasgow University, UK, http://web.eng.gla.ac.uk/assistive/media/publications/travel_
model.pdf
29. Harper S, Green P (2000) A travel ﬂow and mobility for visually impaired travellers. In:
Proceedings of ICCHP, pp 289–296
30. Maingreaud F, Pissaloux E, Gelin R, Leroux Ch (2004) «Autour de la notion d’obstacles pour
les déﬁcients visuels: déﬁnition d’une interface visuo-tactile». In: Proceedings of handicap,
Paris, France, 17–18 juin 2004
31. Thinus-Blanc C, Gaunet F (1997) Representation of space in blind persons: vision as a spatial
sense? Psychol Bull 121:20–42
350
E. Pissaloux and R. Velázquez

32. Hersh M, Johnson M (eds) (2008) Assistive technology for visually impaired and blind
people. Springer, Berlin
33. Schinazi VR, Thrash Tyler, Chebat DR (2016) Spatial navigation by congenitally blind
individuals. Cogn Sci 7(1):37–58
34. Pissaloux EE (2013) Visually impaired mobility and ICT supports. In: IEEE signal
processing: algorithms, architectures, arrangements, and applications (SPA). ISSN: 2326–
0262
35. Dakopoulos D, Bourbakis NG (2010) Wearable obstacle avoidance electronic travel aids for
blind: a survey. IEEE Trans SMC, part C 40(1):25–35
36. Terlau T et al (2008) ‘K’ sonar curriculum handbook. American Printing House for the Blind,
Inc., USA
37. Borenstein J, Ulrich I (2001) The GuideCane—applying mobile robot technologies to assist
the visually impaired. IEEE Trans SMC, Part A: Syst Humans 31(2):131–136
38. Hoyle B, Dodds S (2006) The UltraCane® mobility aid at work training programmes to case
studies. CVHI, Kufstein, Austria
39. Farcy R (2006) Electronic travel aids and electronic orientation aids for blind people:
technical, rehabilitation and everyday life points of vie. In: CVHI 2006, Kufstein, Austria
2006
40. Velázquez R, Pissaloux EE, Hafez M, Szewczyk J (2008) Tactile Rendering with shape
memory alloy pin-matrix. IEEE Trans Instrum Measur 57(5):1051–1057
41. Brabyn JA (1982) New developments in mobility and orientation aids for the blind. IEEE
Trans Biomed Eng, BME-29, N° 4:285–289
42. Brabyn J, Crandall W, Gerrey W (1993) Talking signs: a remote signage, solution for the
blind, visually impaired and reading disabled. In: Proceedings of the 15th annual international
conference of the IEEE EMBS, pp 1309–1310
43. Yusro M, Hou KM, Pissaloux E, Shi HL, Ramli K, Sudiana D (2013) SEES: concept and
design of a smart environment explorer stick. In: IEEE HSI 2013
44. Grallet G, La canne blanche connectée qui veut changer la vie de malvoyants. http://www.
lepoint.fr/technologie/la-canne-blanche-connectee-qui-veut-changer-la-vie-des-malvoyants-
07–02-2015-1903108_58.php
45. Koch O, Teller S (2008) A vision-based navigation assistant. In: ECCV workshop on
computer vision applications for the visually impaired, Marseille, France
46. Kammoun S, Dramas F, Oriolaand B, Jouffrais C (2010) Route selection algorithm for Blind
pedestrian. In: International conference on control automation and systems (ICCAS),
pp 2223–2228
47. Zeil J, Hoffman MI, Chahl JS (2003) Catchment areas of panoramic snapshots in outdoor
scenes. J Opt Soc Am A: Opt Image Sci Vision 20:450–469
48. Banhamou S, Poucet B (1998) Landmark use by navigating rats (Rattus norvegicus):
contrasting geometric and featural information. J Comp Psychol 112:317–322
49. Wystrach A et al (2012) Landmarks or panorama: what do navigation ants attend to for
guidance? Front Zool 8(21)
50. Oliva A (2005) Gist of the scene. In: Itti L, Rees G, Tsotsos JK (eds) Encyclopedia of
neurobiology of attention. Elsevier, The Netherlands, pp 251–256
51. Friedman A (1979) Framing pictures: the role of knowledge in automatized encoding and
memory for gist. J Exp Psychol: Gen 108(3):316–355
52. Potter MC (1976) Short-term conceptual memory for pictures. J Exp Psychol: Human Learn
Mem 2(5):509–522
53. O’Modhrain S, Giudice NA, Gardner JA, Legge GE (2015) Designing media for
visually-impaired users of refreshable touch displays: possibilities and pitfalls. IEEE Trans
Haptics 8(3):248–257
54. Pissaloux E, Velazquez R, Maingreaud F (2009) Intelligent glasses: a multimodal interface for
data communication to the visually impaired. In: Lee S, Ko H, Hahn H (eds) Multisensor
fusion and integration for intelligent systems, Lecture notes in electrical engineering, vol 35,
Springer, Berlin, pp 349–357.
Model of Cognitive Mobility for Visually Impaired …
351

55. Uzan G et al (2008) Besoins en sécurité, localisation et orientation des déﬁcients visuels en
milieu urbain: analyse de la situation et pistes d’évolution. In: Proceedings of handicap, Paris,
pp 37–42 (late blind researcher)
56. Pissaloux E, Velázquez R, Hersh M, Uzan G (2016) Towards a cognitive model of Huan
mobility: an investigation of tactile perception for use in mobility devices, J Navig, 1–17.
doi:10.1017/SO373463316000461
57. Bicchi A, Dente D, Scilingo EP (2003) Haptic illusions induces by tactile ﬂow. In:
Proceedings Eurohaptics, pp 2412–2417
58. Hartleya T, Trinklera I, Burgess N (2004) Geometric determinants of human spatial memory.
Cognition 94:39–75
59. Millar S (1988) Models of sensory deprivation: the nature nurture dichotomy and spatial
representation in the blind. Int J Behav Dev 11(1):69–87
60. Palani H, Giudice U, Giudice NA (2016) Evaluation of non-visual zooming operations on
touchscreen devices. In: Antona M, Stephanidis C (eds) Proceedings of the 10th international
conference of universal access in human-computer interaction (UAHCI), part of HCI
international 2016. Toronto, CA. Springer International Publishing, Switzerland, pp 162–174,
17–22 July 2016
61. Helal AS, Moore SE, Ramachandran B (2001) Drishti: an integrated navigation system for
visually impaired and disabled. In: 5th international symposium on wearable computers
(ISWC 2001), 8–9 Oct 2001, Zurich, Switzerland. doi:10.1109/ISWC.2001.962119
62. Harper S, Green P (2000) A travel ﬂow and mobility framework for visually impaired
travellers. In: International conference on computers helping people with special needs,
Germany, pp 289–296
63. Velázquez R, Pissaloux E, Lay-Ekuakille A, Tactile-foot stimulation can assist the navigation
of people with visual impairment. Appl Bionics Biomech 2015(2015), Article ID 798748.
http://dx.doi.org/10.1155/2015/798748
64. Auvray M, Hanneton S, Lenay C, O’Regan JK (2005) There is something out there; distal
attribution in sensory substitution, twenty years later. J Integr Neurosci 4(4):505–521
352
E. Pissaloux and R. Velázquez

Solid: A Model to Analyse the Accessibility
of Transport Systems for Visually
Impaired People
Gérard Uzan and Peter Wagstaff
1
Introduction
In order to maintain a fulﬁlling and active lifestyle it is essential to remain mobile
and have access to public buildings, transport and sources of information as well as
to have a full social and professional life and interact with other members of
society. This is particularly true for the disabled and elderly and over the last few
years there is increasing interest in responding to their needs and introducing
practical solutions to guarantee mobility and accessibility. More than 80 million
people in the EU have some kind of disability and this ﬁgure is expected to increase
to around 120 million by 2020, due to the effects of the ageing population, so the
challenges faced to ensure full accessibility for all are enormous.
Beyond the economic and political considerations, one of the primary elements
in the change of attitudes leading to progress is the increased understanding of what
constitutes disability through the establishment of the international classiﬁcation of
disability by the WHO [1]. Applying the model of Philip Wood, the International
Classiﬁcation of Disability of the WHO deﬁned the situation of disability in 1980 as
an individual characteristic, independent of situations and resulting from the
interaction between a disability, incapacity and handicap. In 2001 the International
Classiﬁcation of Functioning, Disability and Health (ICF) by the WHO [2] adopted
a model based on an analysis of actions, interactions and activities of daily living
deﬁning disability no longer as the personal difﬁculty of an individual, but the result
of the combination for one or several individuals of the context of an activity and
the tasks to be performed.
G. Uzan
University of Paris 8, THIM ChART 2344, Saint-Denis, France
e-mail: guzan@univ-paris8.fr
P. Wagstaff (&)
22b Rue Laugier, 75017 Paris, France
e-mail: peterwag@gmail.com
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_12
353

To reduce or eliminate the situation of disability it is important to act on the
context and the conditions necessary to carry out the tasks. The initiative should not
only be left to individuals acting alone, charitable organisations or NGO’s, but
should be an aim of society and concern all those involved in designing or
organising the framework of our daily living and social life. The universal avail-
ability of accessible equipment or devices, accessible transport and accessible
private or public premises and infrastructure are central to these objectives. This
may be attained through the creation of new norms and obligations through new
regulations, the development and implementation of solutions for the accessibility
of public places and essential equipment and the political will to apply the concepts
of universal design. The right to accessibility is now enshrined in the United
Nations Convention for the Rights of Persons with Disabilities (UNCRPD), which
was ratiﬁed by the EU in 2011 and has since been enacted in most member
countries.
2
Accessibility
The principle of accessibility applies to two essential spheres of daily life: digital
space (documents and applications on computers, smart phones, websites, inter-
active terminals and various types of machines such as ATMs) and also to physical
spaces (vehicles, buildings, infrastructure and spaces open to the public in the
framework of personal, professional or health requirements as well as highways and
public transport). Work on accessibility standards, in addition to its territorial
implications (national, continental or international) is the result of compromises
between organisations, companies and local and national governments, whose
cultural characteristics and efﬁciency in the deployment of solutions reﬂect the
position of each actor. We should note that standardisation has even more impact on
policies and decision-making, since it is inscribed in national regulations protecting
human rights and normally associated with strict delays for implementation and
sanctions for failure. This integration of standards in law presupposes that it is
feasible to implement practical solutions for all users in each application in tech-
nical and economic terms. Recently we have seen that the economic crisis in
Europe has resulted in some failures in completing the goals announced in this area
for 2015. We therefore distinguish normative and regulatory or contractual acces-
sibility integrating effective accessibility on the one hand from the constraints of
feasibility and the reality of access resulting from the measures taken. For architects
and engineers, normative accessibility as translated into standards and regulations is
not sufﬁcient. Effective accessibility requires that more speciﬁc questions be raised
to validate the feasible solutions, which should ensure:
(a) Real access for people in difﬁculty based on the criterion of effectiveness.
(b) Completion of tasks in a reasonable time in sociocultural terms.
354
G. Uzan and P. Wagstaff

(c) Non-discriminatory and acceptable solutions to ﬁt normative criteria.
(d) Potential accessibility for all;
(e) Technically feasible solutions given the state of the art and modern techniques.
Technical and economic feasibility can be deﬁned as a currently available
ambient device useable by anyone, an ambient device that can be used with the help
of an available technical aid or a personal technical aid. These questions help to
avoid overlooking solutions that are based on universal design or claiming that full
accessibility is unfeasible and settling for simply applying the norms and standard
regulations.
In the context of this document we limit our considerations to the problems
linked to the accessibility of transport systems for the visually impaired, which are
the principal subject of our studies, but the principles involved may be extended
directly to the accessibility of any public spaces or buildings.
2.1
Accessibility to Transport and Public Spaces
All European countries have a legislative framework and regulations to ensure
minimum standards of accessibility for public spaces, new construction and
transport infrastructure, but new initiatives have been launched over the last few
years to improve standards in line with advances in technology and current political
thinking. The law on accessibility which was introduced in France on the 11th of
February 2005, aimed to achieve full accessibility for everybody by 2015, but
despite important advances in many areas, not all the aims were met by the
deadline, mainly due to the economic and practical difﬁculties involved and a
revised programme has now been put into place.
Amongst the practical difﬁculties to overcome is the fact that improvements are
particularly difﬁcult to achieve for existing infrastructure and transport systems in
older towns and cities where the installations are often more than a century old and
there are problems in ﬁnding the space or the money to make the modiﬁcations
required. In such cases it may be impossible to ensure complete accessibility for all
and solutions such as alternative transport services may be necessary for some types
of disability. Nevertheless, changes to improve accessibility in the existing
infrastructure are achievable and would reduce the demands and expenses for
alternative services. Any attempt to achieve progress in this area demands a min-
imum level of comprehension of traveller’s real needs and a corresponding system
of veriﬁcation with centres of expertise in order to ensure that the situation can be
improved. With the introduction of new regulations and political initiatives, many
regional or local authorities, transport, highway, civil engineering and building
organisations have implemented public consultation procedures to diagnose the
most critical problems, carry out experiments to deﬁne solutions and initiate work
programmes based on the results. As participants in some of these investigations
into public transport systems as presented for example in Pretorius et al. [3] and
Solid: A Model to Analyse the Accessibility of Transport …
355

Uzan et al. [4] we tried to formalise some of the problems faced by the traveller and
identify some models that facilitate the understanding of the requirements to
improve accessibility, particularly for the visually impaired. The results of these
reﬂections have been formalised in the development of a model of accessibility
called SOLID. This model breaks down all the principal elements, tasks actions and
phases involved in entering and completing a journey in a public transport system
and permits the analysis of problems and solutions or validates the accessibility for
the disabled user [5, 6]. The essential elements of the model are brieﬂy summarised
in the following section and a detailed analysis and discussion of the practical
aspects to identify problems and solutions in the context of the visually impaired
user are treated later.
3
The Model SOLID
The name SOLID is an acronym of the ﬁrst letters of the ﬁve essential elements
involved in ensuring that any user has optimum parameters for accessibility to the
transport system, enabling him to arrive safely at his destination. These elements are
Safety, Orientation, Localisation, Information and Displacements.
Safety
Ensuring the Safety and security for all the users in all circumstances and at all
phases of the journey is of course the primary responsibility of any transport
system. The design of the systems and infrastructure should ensure that travellers
with any disability are catered for, both in terms of the man machine interfaces and
providing solutions to resolve any problems of guidance, localisation, information
and displacements to complete the journey. Special safety procedures put into place
to any eventual mechanical, electrical or electronic failures should also be designed
to cater for the disabled.
Orientation
The correct orientation of the traveller during the journey requires providing timely
indications of the direction he should take at each stage in order to allow him to
follow his route. The issue of orientation is closely linked to that of localisation for
visually impaired travellers, since standard signs or display panels indicating the
direction to follow to reach the appropriate train or platform are not adapted for
VIPs.
Localisation
Localisation refers to the need for the user to know where he is and the direction of
local points of interest at any given moment. Conﬁrmation of progress by an
indication of his arrival at appropriate “milestones” or intermediate destinations of
the journey (ego-localisation) and the relative position of points of interest with
356
G. Uzan and P. Wagstaff

respect to his current position (allo-localisation) should be provided using methods
adapted to those with disabilities. For public transport systems, ego-localisation
conﬁrms that the traveller has reached the required entrance, intersection, ticket
barrier, bus stop, station or platform. Allo-localisation indicates the relative position
of other strategic points of interest from his current position, such as ticket ofﬁces,
information displays, maps, stairs, escalators, lifts, toilets, entrances and exits and
commercial installations.
Information
The information for the location and orientation of the traveller as well as the
direction of points of interest is generally provided by standard signs and infor-
mation panels. For the visually impaired the means of communicating information
is necessarily limited to audible messages. For VIPs who also have difﬁculties
walking or climbing stairs, information on the presence of alternative routes,
enabling him to use an escalator or lift will also facilitate their journey. Information
provided by these means of communication could include directions to more
accessible routes, but will include train destinations, arrival times and perturbations
or delays as well as commercial or tourist information.
Displacements
The displacements undertaken by the traveller will be actions linked to the essential
tasks of entering and proceeding through the station, ﬁnding the correct platform
and entering the train, riding to the destination and the process of leaving and the
procedures to follow in case of an emergency. The displacements requiring the
greatest physical effort will be the pedestrian phases and those requiring the least
effort the phases of riding on an escalator or in a lift and riding in the train.
These ﬁve elements deﬁne the fundamental characteristics, tasks and information
required to ensure a successful and safe journey in a transport system, but not how
to facilitate the accessibility and ensure the completion of these tasks for a person
with disabilities. In order to analyse the problems of accessibility faced by VIPs
each task may be broken down into a sequence of sub-tasks to identify difﬁculties
or requirements to complete each phase successfully. This is illustrated in the next
section in the context of using the model to identify problems of accessibility
encountered in existing transport systems.
4
General Principles and Problems for Accessible
Public Transport
The traditional and often elegant entrances and information panels in older metro
and underground systems such as the Parisian metro entrances shown in Fig. 1
illustrate the problems faced by VIPs when using existing systems. They need to be
modiﬁed to meet modern accessibility standards and in particular, the presence of
Solid: A Model to Analyse the Accessibility of Transport …
357

steps should be indicated and other forms of guidance should be available. Details
of the lines destinations and network should be available using appropriate modes
of communicating information.
In the next section we describe the basic elements of accessibility and explore
the problems and general requirements of ensuring that physical constraints, human
behaviour and different technical solutions may be taken into account. This con-
stitutes a more complete approach to analyse, evaluate and validate practical
solutions for the visually impaired in their context. Further discussions illustrate the
impact of such an approach to characterise the inﬂuence of technological choices
and methods of assisting this type of user.
The model of accessibility is constructed from a superposition of the four ele-
ments or phases that apply to each stage of the sequence of operations that are
involved in deﬁning a successful accessible design. The ﬁrst two are,
(a) “Entering” a zone, building or public space, entering a lift, escalator, train
carriage, bus, tram or any other form of transport
(b) “Actions” such as buying or using a ticket at a barrier, using a piece of
equipment such as a ticket machine, ﬁnding the right platform, entering and
ﬁnding a place in a carriage, lift or escalator and travelling to an intermediate
destination etc…
These ﬁrst two phases may be summarised as “entering” and subsequently
completing an “action”,
Two other phases or elements remain which can be summarised as “exiting” or
leaving after normal completion of the “action” and a ﬁnal phase of providing a safe
route for “evacuating” to a safe location in cases of emergency. The sequence of the
different
phases
required
to
complete
a
given
process
is
illustrated
in
Fig. 2. Experience shows that it is relatively easy to ensure that the processes of
ensuring accessibility for entering and evacuation in an emergency are catered for,
Fig. 1 A traditional Parisian metro entrance and network information display for travellers
358
G. Uzan and P. Wagstaff

but the processes of completing one or several “actions” and “exiting” or “evac-
uating” without difﬁculty are often neglected or more difﬁcult to assure for those
with disabilities. The concept of providing interfaces to enable these “actions” to be
completed without difﬁculties obliges architects, engineers and designers to adopt a
common systematic approach to try to meet all the requirements of universal design
to adapt the interfaces for all types of user.
The major differences between ensuring accessibility to electronic or digital
interfaces on the one hand and mechanical, structural and physical spaces on the
other, is the costs of updates, and the ease of implementing modiﬁcations that might
be necessary to improve accessibility. A website or digital application is “live”
because it is renewed frequently in the course of time, often several times a year, a
month or a week and improvements can be made rapidly to counteract any prob-
lems encountered by users. Physical sites, such as buildings, equipment, transport
infrastructure and rolling stock are difﬁcult and expensive to update and are gen-
erally the object of relatively long term programmes of scheduled improvements or
modiﬁcations over a period of 10, 20, 30 or even 40 years. The constraints asso-
ciated with the preservation and conservation of physical assets, structures,
machines and their renewal are also very different. The concept of physical
accessibility, which is the basis for our model, has been developed after multiple
studies and analysis of the needs and problems of disabled passengers using public
transport and particularly those of the visually impaired. In this context the con-
straints of the site and the difﬁculties of carrying out improvements in a restricted
space are major problems for management. The design and exploitation of systems
of assistance and the validation of personal or general systems of aid and infor-
mation for the voyager are crucial.
In a broader context the functional speciﬁcations and development of devices
with Human Machine Interfaces adapted to providing assistance for localisation and
guidance for visually impaired pedestrians in urban areas have been tested in many
countries including France. As participants in these studies our analysis of the
requirements for guidance, information, localisation of points of interest and
transportation data (destinations, schedules, disruptions) resulted in a categorisation
of requirements adapted to our local and national criteria. Different projects dealing
with the different modes of transport and their associated areas included, bus stops in
city streets, design of roads and shared spaces, trains and stations, metro and
underground stations and systems of information for voyagers and tramway systems.
In these studies, different technologies of localisation were compared (GPS,
wireless footprints, inertial navigation, visual landmarks and RF tags) and various
associated interfaces (voice, visual) on personal devices (smartphones, remote
Fig. 2 The different phases involved in completing a task in an accessible environment
Solid: A Model to Analyse the Accessibility of Transport …
359

controls) or ambient systems such visual displays or public address systems. The
use of such devices does not however permit the basic concepts of providing an
accessible environment to be neglected or ignored, so the description of the model
of requirements provided by SOLID is a valuable tool in the process of analysing,
designing and validating the functionality of solutions to improve accessibility.
It should be noted that people walking in the accessible areas in a public transport
system often disregard signs or social codes and do not follow the rules such as
keeping to the right or walking in the wrong direction even if they can see the signs.
Travellers walking on the wrong side of the passageways, stopping without warning
or slowing down and pushing past others are particularly hazardous for the visually
impaired. For them, the level of concentration required to memorise and follow their
route and detect potential clues on their environment is a signiﬁcant effort under
normal conditions and any antisocial behaviour of others can lead to the possibility
of collisions with other travellers or falls, or at the very least a lapse in concentration
and subsequent disorientation. It is therefore essential to remind users of the rules
frequently and provide the means for the visually impaired to verify their position
and other information. It is also particularly important that the name of the line, the
current and next train or metro as well as the destination should be communicated by
the public address system as well as visual displays to persons waiting on the
platform or travelling in the train. Often, even if there is a vocal message there is no
display for persons with problems of hearing and vice versa. Often the intelligibility
of the public address system is inadequate, due to background noise, poor acoustics,
poor message structure, or excessive speed of delivery. These kinds of problems are
obviously being remedied as new rolling stock, infrastructure and communication
systems are introduced to update existing lines.
4.1
The Chain of Mobility
The displacements of a person using public transport are a succession of phases
where the user is either walking on or in stationary infrastructure or being trans-
ported and almost immobile in a moving vehicle, a lift or escalator. This can be
represented by the chain of mobility, part of which is illustrated in Fig. 3
below. The transitions between the phases where the traveller is being transported
are of course the most critical for those with mobility problems and this situation
applies for any kind of disability but is more acute for wheelchair users or visually
impaired, persons. Access to older transport networks including metro or under-
ground systems is difﬁcult for the former because of the physical barriers and lack
of lifts and also for those who are visually impaired because of their need for
guidance and information. These circumstances have a direct impact on vigilance,
attention, susceptibility to stress and the priority of the need for information for a
person who is visually impaired.
The traveller moves on a journey deﬁned by alternating stages where he is
walking and using all his faculties and concentration to ﬁnd his way or travelling in
360
G. Uzan and P. Wagstaff

a vehicle where he can relax until he arrives at the next milestone on his journey.
The journey represents the contents of a representational and semantic memory of
the subject (a mental image of a map or itinerary) or the recollection of an external
image (a map, network or neighbourhood, line etc.) before he lost his sight. He
follows the route marked out by successive landmarks, each one in turn consisting
of a goal or milestone (or an intermediate destination) and a point of exchange for
new information (new beginning). Each milestone is a stage on the itinerary and
thus a “mental switch” for the collection of information. It is a point of arrival and
also a point for a new beginning. The mental attitude (cognitive and possibly
affective) is different in terms of the treatment (e.g. veriﬁcation of the landmark and
then switching to the identiﬁcation of a new segment of the route) with an effect on
the emotions connected with the discovery or recognition of each new landmark on
the itinerary. Waiting for a vehicle is not considered here as “immobility”, but is
part of the pedestrian’s route which could be used for other integrated activities.
Immobility is considered as a period of total inactivity where no further planning or
preparation is involved such as the end of the journey.
4.2
The Motives of the Journey
The motives for undertaking a journey have a great inﬂuence on the strategy,
planning and information requirements of any visually impaired person. Any
journey involves going out into the street, walking and/or travelling to one or
several destinations, eventually using public transport or other means of transport.
Journeys that are regularly repeated are quickly memorised, are the easiest to learn
and become routine without any stress. Others for pleasure often require planning,
but unfamiliarity can lead to problems and the likelihood of errors or delays. These
motives can be divided into ﬁve main categories, linked to the reasons, regularity
and purpose of the journey:
(a) Going to work, school, university or shopping; a regular, often repeated route.
(b) Physical Activity, for example jogging, walking, walking the dog; frequently
repeated routes
(c) Social relations with others, visiting family or friends, museums, theatres, art
galleries, meeting out for coffee, lunch or tea; Irregular or rarely repeated
variable routes
(d) To visit a stately home or gardens, a landmark or village in the countryside.
One off visits
Fig. 3 Part of the chain of mobility when using the metro, train or bus systems
Solid: A Model to Analyse the Accessibility of Transport …
361

(e) To reach a place for vacations or meet with a person. Even if the destination is
ﬁxed, such as an apartment or hotel other persons are potentially mobile and
could move during the journey, or be delayed, necessitating a change of the
meeting point (a sporting event, the tour de France or a rally for example).
4.3
The Zones of Displacement
Articulated around the treatment of the information and tasks of the pedestrians, the
zoning of the model does not exactly follow that derived from the functional
organization of the station with its lines, shopping areas, or access zones to ticket
machines, passageways, escalators, stairs, platforms and trains, but remains con-
sistent with it:
(a) The zone at street level or main transit area is characterised by the diversity of
potential conﬁgurations with multiple activities including commercial, cultural
and residential activities one of which is transport so it is multifunctional. The
pedestrians in this zone are not necessarily using the transport system.
(b) The access zone is where the main activity of the persons present is to move
from the surface zone to the boarding zone or, conversely, from a train to the
surface or street. This zone may or may not be dedicated speciﬁcally to
transport. It can be crossed by a signiﬁcant number of people shopping or
travelling during the day and particularly by commuters hurrying to their
destinations during rush hours.
(c) The transfer zone includes the platform in a station or tram stop, the curb or bus
shelter at a bus stop and the entry and exit zones at the interior of vehicles,
which are close to the entry and exit doors. It is destined for transfers during the
arrival and the departure of the vehicle and the transfer of passengers from the
platform to the seating zone of the vehicle and vice versa. This is an area where
there is limited time to enter and leave because the bus or train has to close the
doors and continue the journey. This can cause stress for persons with problems
of mobility and is a zone where the need for security and guidance is a priority.
(d) The zone of “transportation” is inside the vehicle. In this area the traveller is
relieved of the responsibility of walking and searching his way or exercising
physical movements and delegates this task to the transport system.
Cognitively, he is in a situation similar to that of waiting at a bus or tram stop,
but is not expecting the arrival of a vehicle, simply the arrival of his vehicle at
his destination.
362
G. Uzan and P. Wagstaff

5
Integration of Requirements Using the Model
of Accessibility SOLID
The model takes into account each factor necessary to complete the journey suc-
cessfully. In addition to the primary factors of safety, orientation, localisation, the
acquisition of information and the means of displacement it is also necessary to
analyse the chain of mobility, the motives for travelling and identify the phases of
the journey which are difﬁcult for the user with impaired mobility. In the case of
persons who are visually impaired, the means available to enable him to guide
himself in security are a fundamental part of the equation.
5.1
Design and Aids for Safety
As previously mentioned, the ﬁrst and most important requirement for any public
transport organisation is to ensure the safety of all travellers and members of staff at
all times.
This can be ensured by designing the public areas to avoid any possibilities of
accidents, presuming that we exclude any accidents due to failures of mechanical,
electrical or electronic systems of the rolling stock and infrastructure. This requires
optimal accessibility with ergonomic and logical design, minimising distances to be
covered, direction changes and variations in level for those with any kind of dis-
ability. For those with disabilities, reliable systems to warn them of obstacles,
inaccessible areas or potentially dangerous situations where they could trip up or
fall are required.
These include stairways, escalators, lifts, corners or changes in direction or ﬂoor
level. This is particularly important in congested platforms and passageways where
collisions with other travellers may occur. This is of course vitally important for
those who are visually impaired, since they will have difﬁculties choosing a clear
pathway to avoid bumping into other travellers coming in the opposite direction as
well as identifying any temporary or permanent obstacles on their route.
5.1.1
The White Cane
The standard tool used by the visually impaired for guidance and ensuring safety is
the white cane, which is helpful for avoiding obstacles, but also indicates to fellow
travellers that they should try to clear a path and avoid contact. The idea of a white
cane originates from 1921 when James Biggs of Bristol, England decided to paint
his walking stick white in order to make it easier for him to be identiﬁed by drivers
when crossing the road. Campaigns to introduce the white cane as a symbol for the
blind led to their adoption in France, England and the USA from 1931. After World
War 2 Dr. Richard Hoover introduced the concept of the long white cane for
Solid: A Model to Analyse the Accessibility of Transport …
363

American soldiers who had lost their sight during the war. Up to this point VIPs
held a white cane in front of them to warn others of their disability and to detect
obstacles directly in their path. The new longer device enabled them to sweep the
cane in front to detect obstacles, barriers, edges of walkways and changes in the
surface at a greater distance. The greater length improved their visibility to others. It
has since become the primary mobility tool for the visually impaired and is well
adapted to most situations including travelling in public transport systems The main
functions of the long cane may be summarised as follows;
(a) Identiﬁcation to others of their disability
(b) To detect and explore around obstacles and identify their form and nature
(c) To detect boundaries and follow a path, passageway, corridor or a building or
boundary wall
(d) To detect changes in level, gaps and negotiate stairways, doors and entrances
(e) To detect changes in surfaces, gravel, grass, holes, material and tactile paving
characteristics
(f) An assistance to maintain equilibrium
A white cane provides a guide of the relative position of walls, stairways and
obstacles, but it is not always spotted by other travellers, particularly those walking
immediately in front of the user in the same direction or in spaces where there is
cross trafﬁc and the cane may contact their feet or legs. This kind of problem is
generally well accepted by the passenger affected, but can sometimes cause difﬁ-
culties. A hard prolonged contact may also permanently distort the cane, particu-
larly if it is made of aluminium or a metallic alloy. For this reason, carbon ﬁbre
canes are preferable to the metallic versions, despite their increased weight, since
they are not distorted in this kind of incident. Another alternative to the white cane
is of course the guide dog, which performs a valuable role in ensuring safety for
VIPs, but not enough persons are able to have one.
5.1.2
Tactile Surfaces
Tactile pavement surfaces were ﬁrst introduced for pedestrian crossings in streets in
1965 by the Japanese engineer Seiichi Miyake. A regular pattern of ﬂattened domes,
cones or blisters are introduced in the paving at the edge of the footpath in order to
help visually impaired pedestrians identify the boundary between the footpath and
the road. The kerbs at pedestrian crossing areas are lowered to simplify passage for
wheelchairs, push chairs and prams and the transition from the footpath to the road
surface could then be detected by the visually impaired since this pattern could be
detected when walking on it or with the help of a cane. The same principle was
extended to warn of danger on the edge of platforms in train stations in Japan in the
1980s and later in Australia and other countries in Europe and elsewhere. Tactile
paving surfaces are used to indicate any potential obstacles or other hazards
wherever possible as well as for guidance and information, but standards vary from
364
G. Uzan and P. Wagstaff

one country to another. Tactile paving is used at the edge of platforms and can be
employed at the entrance to stairs, escalators and lifts and to warn of any changes in
level. This tactile paving gives a visually impaired person a warning that they are
entering a zone which could be dangerous or which warns of an impending change
in level and can also be used as a guide to avoid obstacles. The lack of an inter-
national standard for tactile paving has led to inconsistencies between the appli-
cations and formats used in different countries.
Difﬁculties with identifying obstacles or changes in level may also be encountered
by partially sighted persons who have problems such as macular degeneration,
glaucoma, a restricted ﬁeld of view or any other defect. They may still be able to
visually identify zones of danger if these are indicated clearly. These persons will not
necessarily use a long white cane to identify the surface in front of them, so may not
detect the tactile paving before stepping directly on it, unless it is in a contrasting
colour. The use of different colours and contrasting shades for tactile paving as well as
their pattern depending on their function is a way of improving safety in such cases.
Their use has been codiﬁed and adopted in national standards in Japan, Australia,
Europe and many other countries. The use of white or yellow for tactile paving to
contrast with grey bitumen or the lighter shades of tiled surfaces used in some station
platforms, but other combinations of contrasting colours have also been adopted.
On the left of Fig. 4 below is a photograph of an overhead line metro station in Paris
originally designed over a hundred years ago. This is typical of the design of stations
of that period, which were implemented in many train, metro and underground sys-
tems in Europe and elsewhere. Following the introduction of tactile surfaces in France
the edge of the platform is now indicated by a strip of white paint preceded by a grey
tactile pavement warning surface with the standard pattern of raised domes. In the
more recent metro system at Washington DC on the right of Fig. 4 the tactile paving is
also grey, but red ﬂashing lights are added as an extra warning.
The stations shown in Fig. 4 have relatively narrow platforms and machines,
seats and other obstacles close to the inside wall of the platform. This could con-
stitute a problem for visually impaired persons, who would normally try to stay
close to the wall and as far as possible from the edge as they walk along the
Fig. 4 A traditional overhead metro station (left) and a metro station in Washington DC
Solid: A Model to Analyse the Accessibility of Transport …
365

platform. It can be noticed that the station on the left of the ﬁgure is on a line using
pneumatic tyres rather than standard steel railway wheels. These have the advantage
of providing a quieter and more comfortable ride with improved braking and
acceleration and improve the audibility of station announcements when a train is
arriving or leaving the station, so the use of tyres is a distinct advantage, but
unfortunately the operating costs are higher. The access is a stairway at the far end
of the platform, but many of these stations have now been equipped with an up
escalator and/or a lift for those with mobility problems.
5.2
Other Risk Factors and Solutions
Partially sighted users may also suffer from other conditions such as impaired
perception, vestibular imbalance, deﬁcits of attention or other health problems. In
this same context, poor lighting, visual illusions, unidentiﬁed sounds or their
reﬂections, missing warning notices for temporary maintenance work, staircase and
escalator maintenance, irregular surfaces and a lack of handrails on stairways can all
complicate the problem of orientation and necessitate deviations and more physical
effort with the consequent increased risk of falling.
A fall is of course far more dangerous than simply bumping into someone in a
crowd, particularly if it occurs down steps or escalators, or even worse, from the
platform onto the rails, but for the elderly or others who also have problems of
equilibrium, any slight contact with another person or object could potentially result
in a fall. Obstacles which have a mass, shape or material which might affect
physical integrity such as walls, partitions, seats, machines for distributing drinks or
snacks, posts, poles and pipes can also cause similar problems for the visually
impaired as can items left unattended such as bags or luggage.
The effects of collisions with objects or obstacles differ in an important respect in
that they do not constitute a conﬂict of interests between persons, since only the
disabled person and the object or obstacle are involved. Avoiding the risk of
collision with other persons may sometimes imply the resolution of conﬂicts
between users and the risks related to security or the perceived risk of theft or
assault. This is also the case for other emergency situations and any conﬂicts that
might arise associated with the ability of the disabled user to evacuate rapidly to
exits or safe areas in case of an incident, accident, ﬁre or other emergency.
On the left side of Fig. 5 below, the ﬁrst author, who is blind, is shown using a
long cane in his left hand to negotiate a set of stairs at the entrance to a public
building. For a partially sighted person a contrast in colour or shade between the
risers and the horizontal ﬂoor and steps as you enter the building can help to
identify the presence of stairs, but only the aluminium proﬁle between the top of the
riser and the step above is visible when exiting.
As an example of the use of tactile surfaces in this context, the photograph on the
right of Fig. 5 shows a stairway in a metro station with a white tactile warning
surface at the top of the steps to warn VIPs of the impending change in level.
366
G. Uzan and P. Wagstaff

A tactile surface warning strip is also present at the beginning of the second
ﬂight of steps. In other countries such as the UK the choice has been to use warning
strips composed of parallel strips or a “corduroy pattern” at the top and bottom of
stairways. The railings surrounding the stairwell permit the entrance to the stairwell
to be easily identiﬁed with the aid of the cane and the handrails guide and help to
support the user when climbing or descending the stairs. As previously mentioned,
the user of the cane in Fig. 5 is left-handed which may also be a positive advantage
when negotiating steps in metro systems where the convention is to keep to the
right. The right hand may then be used to hold on to the handrail and the cane used
to locate the position of the next step up or down. The cane is also used to identify
the position of the train doors and any change in level or gap between the platform
and the ﬂoor of the train. One of the primary dangers, that of falling from the
platform onto the tracks, is gradually being dealt with in Paris as some older metro
lines are being equipped with barriers between the platform and track with auto-
matic synchronised doors that only open when the train is present and in some cases
automatic driverless operation (see left of Fig. 6).
Fig. 5 Using a white cane (left) and a tactile warning surface (right) to identify the top of
stairways
Fig. 6 French metro station with a new safety barrier (left) and guidance path in Spain
Solid: A Model to Analyse the Accessibility of Transport …
367

The tactile pavement and white paint strip at the platform edge have been
removed with the installation of the barrier since there is no longer any risk of
falling onto the track and the barrier constitutes a safe reference for the traveller to
orientate his progress along the platform.
5.3
Orientation Requirements and Practical Solutions
Orientation is the second element of the model and refers to the ability of the
traveller to be able to determine the direction he should take to reach his next
intermediate destination or milestone. This entails following the route to each
intermediate point, in the direction of the next milestone on the journey, without
deviating. Orientation is of course only relevant once the traveller has been able to
plan his route and verify his location at each phase and is the ﬁrst information
required at the beginning of the journey.
The problem of orientation is closely linked to the physical and mental capacities
of the person making the journey particularly if he is unable to read signs and verify
the direction he should take on the next phase of the journey. If the correct ori-
entation is chosen at the beginning it is preferable to walk straight to the next
milestone, however this is a difﬁcult task for VIPs, since they cannot walk in a
straight line without visual veriﬁcation of their progress. VIPs negotiating a plat-
form, a passageway or larger space usually use their white cane to locate the rear
wall, sidewall or perimeter and follow this until the next intersection is encountered.
As we have seen above, these walls are used for seating, vending machines, waste
baskets or occupied by other travellers, which could lead to difﬁculties and conﬂict
and it would be safer to walk in a straight line in the middle of the platform. In fact
tests show that VIPs or blindfolded subjects unable to verify their direction of travel
cannot walk in a straight path and the normal tendency is to walk in an approximate
circle of greater or lesser magnitude [7]. Slight discrepancies in the symmetry of the
human body have been suggested as possible causes, but this research showed that
the direction of deviations or veering is closely correlated with the variations of the
positions of the centres of pressure on the soles of the feet and that only 40% of the
persons tested veered always in the same direction and none could walk consis-
tently in a straight line. The conclusion of the research was that deviations from a
straight line were triggered by vestibular inconsistencies, which were variable in
time, rather than postural discrepancies.
The solution available to deal with this problem is the use of a different type of
tactile paving to guide the traveller on the correct path. The Japanese introduction of
tactile paving with a pattern of ﬂattened domes or blisters was later followed by the
development of another pattern for guidance using rectangular motifs to indicate the
preferred path to follow [8]. This pattern is used in most public transport facilities in
Japan, New Zealand, Australia the UK and other countries, but has not been uni-
versally accepted. The pattern used for this type of tactile paving is rectangular in
form with the longer axis of the rectangles indicating the line to be followed. On the
368
G. Uzan and P. Wagstaff

right of Fig. 6 is an example of the implementation of this system in Spain where
path from the entrance is deﬁned using the rectangular pattern paving on the right
and the intersection with the path to the ticket ofﬁce at the top of the photo and the
path to the ticket barrier on the left is signalled using the warning dome pattern. The
main objection this system in France is that the destination or direction of the
guidance path is not indicated, so the user may be travelling the wrong way [9].
It does however permit improvements in maintaining the correct orientation and
following a straighter path for blind and partially sighted users providing that
information on orientation is available at each intersection.
The problem of indicating the correct direction to follow in station platforms in
Japan by orienting the guiding surface obliquely from the train exit door in the
direction of the exit before it joins the main guiding surface in the middle of the
platform. This ensures that the visually impaired traveller can move along the
platform without the risk of entering into contact with obstacles such as seats,
vending machines and other obstructions, which will normally be located along the
wall furthest from the rails.
One solution is the integration of RFID chips in the tactile paving such as that
proposed by Kurachi et al. in Japan [10]. A cane equipped with sensors is used to
read the information and indicate the direction followed as well as the location.
Other systems of guidance and communication are being developed using wiﬁ
beacons, bluetooth location, light data transmission or other means in combination
with tactile guidance paths as mentioned in [11]. The norms for tactile surfaces
change from one country to another [12, 13], but blister heights (5 mm +), sepa-
ration (50–70 mm) and dimensions 25–35 mm diameter are similar.
5.4
Route Planning and Learning
For VIPs it is necessary to commit all this information to memory before the start of
the journey and/or ask for help along the way as required. If he wishes to learn how
to use a new route which he will use regularly in the future to go to work, school or
other destination, a guide is necessary to train him. Courses of “travel training”
exist in most countries, alternatively friends or colleagues can help during this
learning process.
During the course of a journey it is also necessary to purchase a ticket, pass
through the ticket barrier, proceed to and ﬁnd the correct platform and catch the
correct train. Where trains on the same line have two or more alternative destina-
tions it is important to be able to verify that the train is going in the direction you
require. This information is normally shown on an illuminated display and
broadcast on the public announcement system. For VIPs the only source of the
information available is the public announcements, which can sometimes give
incomplete information or be difﬁcult to understand. Therefore advanced guidance
systems, which could relay messages as well as position at the relevant time would
be an extremely useful asset.
Solid: A Model to Analyse the Accessibility of Transport …
369

The use of a white cane can be discarded if a guide dog is available, but not all
dogs can guide a VIP in the metro except on a regular journey and orientation and
planning the journey is necessary in other cases. This is only a partial solution, but
the other problem is the memorisation of the different phases of the journey, since
knowing where to turn and taking the correct direction at every section of the
journey requires a good memory or other source of information. The indication of
where to turn can be provided by a personal guidance system at the intersections
and retain details of milestones or points where his progress can be veriﬁed. He
should also be able to ﬁnd or choose an alternative route in case of need.
5.5
Information Delivery and Classiﬁcation of Needs
Information is provided in response to needs. We can classify them into different
categories for security, localisation and orientation, transport information and
payment as well as information services related to physical displacements within
the station. They may also be classiﬁed according to the requirement of being
synchronised or not to the displacements. There is therefore:
(a) Structural information and synchronised updates on services lines, platforms
and destinations;
(b) Cyclical information: linked to temporary situations such as maintenance work,
holidays, and closure.
(c) Event information which is of short duration, a temporary interruption of trafﬁc,
a suspicious package or the arrival of a train.
The problems of deﬁning the optimum timing and method of delivering infor-
mation were initially deﬁned for vocal interfaces to aid in the localisation and
orientation of visually impaired travellers in bus interchanges or subway stations.
Three key instants have been identiﬁed for the dissemination of information, at the
entry of the traveller in a key zone, when there is a change of situation, such as the
imminent arrival of a train and when there is an emergency or a change of platform
or schedule The efﬁciency of the delivery of each type of information depends on
the mode of communication used by the system and the physical and cognitive
capacities of the traveller.
Sources of information such as access zones to transport, lines, destinations,
routes, schedules, and disruptions of services have to be available to VIPs.
Information on the environment and peripheral activities such as tourist attractions,
landmarks, shopping facilities, entertainment and cultural centres would also be
included. This could be provided by ambient installations, which give visual
information and additional vocal information to those are affected by visual
impairment. Often some of this information may be considered excessive or
redundant for the everyday traveller, but may be essential for those with disabilities.
In order to adopt a systematic approach to evaluating the different needs and types
370
G. Uzan and P. Wagstaff

of information required to ensure optimum safety and guidance for all classes of
user we have developed a model of the basic requirements and tasks involved when
using public transport.
5.6
Displacements
Displacements constitute the ﬁnal factor of the model, where the mobility and mode
of progression of the traveller is linked to physical or cognitive difﬁculties he has in
making his journey. Shared or large spaces constitute a difﬁcult challenge [14–16]
and tactile guidance paths and zone delineation provide a solution if used with
modern localisation devices. The installations should be designed to avoid any
difﬁculties or the obligation to cross barriers or paths with difﬁcult accessibility.
Means of avoiding problems should be provided by giving indications of alternative
routes where the traveller has reduced mobility or has problems due to age, physical
or cognitive impairments. Clear indications of the location of any elevators or
escalators should be provided and the needs of travellers with other disabilities
should also be taken into account [17, 18].
6
Conclusions
The increasing use of personal devices and ambient sources of information for
orientation, localisation, and general information in combination with better design
and the use of tactile surfaces has led to improved accessibility and greater facility
for persons with problems of mobility to use public transport. The model SOLID
helps to deﬁne the requirements to successfully deﬁne potential improvements or
weaknesses in metro or train services and infrastructure. and analyse the different
aspects causing particular problems Optimum solutions for accessibility can be
investigated using this model in advance at the design stage or when speciﬁc
changes are proposed to existing infrastructure. Since trains and buses are designed
to be used for many years, it will inevitably take a long time to completely replace
older stock, which will not be to the latest norms. It is also difﬁcult or too expensive
to ﬁnd space to guarantee step free access in all the older metro or underground
stations and therefore some lines will only have this feature at a limited number of
stations which should be identiﬁed for travellers who need it.
In circumstances where existing outdated transport systems are being updated by
transport authorities to comply with the new regulations, alternative transport
solutions such as on-demand electric vehicles could be proposed to circumvent
some of the problems, but much can still be done to improve accessibility in
existing systems. The advantages of using this model are illustrated by the analysis
of requirements for ensuring optimum mobility and accessibility in metro and train
systems for the visually impaired, but the same principles can be applied to analyse
Solid: A Model to Analyse the Accessibility of Transport …
371

the needs of persons with other kinds of impairment. Investigations are continuing
in these areas, including the simulation of different sites to validate accessibility in
wheelchairs, improving portable guidance and communication devices to take into
account the speciﬁc characteristics of the user and testing and optimisation of vocal
and visual communication and haptic interfaces.
Acknowledgements The authors gratefully acknowledge the support of the RATP, the SNCF,
TCL and numerous partners from other institutions that have collaborated in the projects, which
have contributed to the development and validation of the ideas presented in this paper.
References
1. International Classiﬁcation of Impairments, Disabilities and Handicaps (ICIDH), based on the
work of Philip Green. Geneva WHO 1980
2. International Classiﬁcation of Functioning, Disability and Health (ICF) WHO 2001
3. Pretorius S, Baudoin G, Venard O (2010) “Real time information for visual and auditory
impaired passengers using public transport—technical aspects of the Infomoville project”
Handicap 2010. IFRATH, Paris
4. Uzan G, Wagstaff PR (2011) A model and methods to solve problems of accessibility and
information for the visually impaired. STHESCA, Krakow
5. Uzan G, Seck M, Wagstaff PR, Dejeammes M (2011) Solid: A model of the principles,
processes and information required to ensure mobility for all in public transport systems. In:
Invited presentation 18th international conference on intelligent transport systems, Orlando
6. Uzan G, Hanse P-C, Seck M, Wagstaff P (2015) Solid: a model of the principles, processes
and information required to ensure mobility for all in public transport systems. In:
Proceedings 19th triennial congress of the IEA, Melbourne, 9–14 Aug 2015
7. Bestaven E, Guillaud E, Cazalets J-R (2012) Is “Circling” behavior in humans related to
postural asymmetry? PLoS ONE 7(9):e43861. doi:10.1371/journal.pone.0043861
8. National Standards Japan JIS T 9251:2001 Dimensions and patterns of raised of parts of
tactile ground surface indicators for blind persons
9. Dejeammes M, Piriou C-N, Leclère D (2005) Usage des surfaces podotactiles par les
personnes aveugles ou malvoyantes, Publication of the CERTU. ISBN: 2–11-095322-5,
Lyon, France
10. Kurachi K, Kitikazi S, Fujishima Y, Watanabe N, Kamato M (2005) Integrated pedestrian
guidance system using mobile device. In: 12th world congress on ITS, 6–10 Nov 2005
11. Marin-Lamellet C, Aymond P (2008) Combining verbal information and a tactile guidance
surface: the most efﬁcient way to guide people with visual impairment in transport stations.
Br J Visual Impairment 26(1):63–81
12. DfT (Department for Transport) (2007) Guidance on the use of tactile paving surfaces. DfT,
London, UK. See http://www.dft.gov.uk/publications/guidance-on-the-use-oftactile-paving-
surfaces
13. Bentzen BL, Barlow JM, Tabor LS (2000) Detectable warnings: synthesis of U.S. and
international practice. U.S. Access Board, Washington, DC 20004-1111, (http://www.access-
board.gov) Work performed under contract by: Accessible Design for the Blind, Berlin,
Massachusetts 01503 USA, 12 May 2000
372
G. Uzan and P. Wagstaff

14. Childs CR, Thomas C, Sharp S, Tyler NA (2010) Can shared surfaces be safely negotiated by
blind and partially sighted people? The 12th international conference on mobility and
transport for elderly and disabled persons (TRANSED), Hong Kong, China
15. Norgate SH (2012) Accessibility of urban spaces for visually impaired pedestrians. In:
Proceedings of the institution of civil engineers, vol 165, issue ME4, Municipal Engineer,
pp 231–237
16. Hamilton-Baillie B (2008) Towards shared space. Urban Design Int 13(2):130–138
17. Jenness JW, Singer J (2008) Visibility and conspicuity of detectable warnings for pedestrians
with visual impairments. Transp Res Rec 2073(104–113):2008
18. Johnson V, Petrie H (1998) Travelling safely: the problems and concerns of blind pedestrians.
Br J Visual Impairment 16(1):27–31
Solid: A Model to Analyse the Accessibility of Transport …
373

Part IV
ICT Technologies and Mobility

Mobility Technologies for Blind,
Partially Sighted and Deafblind People:
Design Issues
M.A. Hersh
1
Introduction
There are an estimated 285 million visually impaired people globally, with 39
million of them blind [75]. Blind and many partial-sighted people require travel
aids and/or personal assistance to travel safely and reach their destinations, though
the design requirements of partially sighted people may be different from those of
blind people. Despite nearly 120 years of the development of electronic travel
aids, resulting in a large number of prototypes and a much smaller number of
commercialised devices, the most commonly used travel aids are still the long
cane followed by the guide dog. Nearly 10% of the estimated 1.1 million blind
people in the USA use a long cane and just under 1% a guide dog [83]. Since
these are the two most commonly used mobility devices, this indicates that the
majority of blind people in the USA and probably the rest of the world do not use
any mobility devices. This is probably not due to lack of need and research
indicates that there is still signiﬁcant stigma associated with blindness and long
cane use and that this may delay or prevent decisions to use a long cane [36, 37].
Two-thirds of cane users, but only one third of visually impaired people are
below 65 years of age [21]. This indicates both the greater difﬁculties older
people have in adapting to blindness and learning to use a cane and the need to
take into account age and, in particular, the needs of older blind people when
designing mobility devices. Surveys indicate that a signiﬁcant minority of blind
people do not leave the home (on their own) and those who travel on their own
tend to travel locally and/or stick to known routes [14, 19, 31, 47]. While these
surveys are dated and there may have been some improvement, this does indicate
the barriers experienced by many blind people and the need for accessible
environments and well-designed mobility aids to overcome them.
M.A. Hersh (&)
Biomedical Engineering, University of Glasgow, Glasgow G12 8LT, Scotland, UK
e-mail: marion.hersh@glasgow.ac.uk
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_13
377

There are a number of different reasons for the low usage of mobility devices
other than the long cane and guide dog including [45, 79] high costs, limited (ad-
ditional) beneﬁts relative to the long cane, lack of easily available, low cost training
and the difﬁculties involved in learning to use these devices, weight, unattractive and
obtrusive appearance and lack of information about what is available. This can be
summarised as insufﬁcient additional beneﬁts relative to the long cane to outweigh
the additional costs and training required. Some of these devices are also affected by
weather and other conditions, leading to inconsistent feedback, may produce dis-
orientation from excessive sound information, involve cumbersome hardware,
which is not easily portable and neglect social aspects of device use [9]. It has also
been suggested [9] that devices should present an integrated, multifunctional,
transparent and extensible solution. Although good design cannot be guaranteed to
ensure the success of a new device, paying attention to design will lead to a better
device that is more likely to be used by a signiﬁcant number of people.
In the remainder of this chapter, the term ‘blind people’ will be used to refer to
anyone with some degree of visual impairment, particularly, if they experience
barriers to mobility as a result. Where it is necessary to make distinctions additional
terms, such as partially sighted or deafblind will be used. It will be clear from the
context whether the term ‘blind’ is being used generically or to refer to people who
are legally blind, have light perception or are totally blind.
2
Preliminary Considerations
2.1
The Long Cane
Before the more general discussion it is useful to examine the main features of the
long cane and the factors that have led to its relative success. Despite its limitations,
which will be discussed below, the long cane is able to provide safe mobility to
blind people who have had appropriate training, though additional aids and/or
training will generally be required for travel in unfamiliar areas. It can be divided
into the grip, a rigid or non-rigid shaft and the tip [93]. It is available in different
materials, including aluminium, ﬁbreglass and carbon ﬁbre, with the most appro-
priate material often depending on the user and the type of journey [82]. The cane is
light, though extended use can lead to arm fatigue. The simple mechanical design
means that there are no moving parts or electronic components to malfunction.
Although damage in use is possible, for instance through contact with vehicles,
most modern canes are both ﬂexible and robust and can survive a range of mishaps.
The tip and the ties holding the sections together can be easily replaced if damaged,
thereby extending the cane’s life.
The cane can also to some extent be customised to the user, with the option of
different tips, folding, telescopic or rigid shaft, different handle design and various
materials, with different weights and properties. There are also versions designed to
378
M.A. Hersh

be used in the mountains and shorter versions intended mainly to indicate that the
user is blind. The cost is low enough to enable many users to have more than one
cane, for instance to choose different canes in different contexts, and for a spare to
always be available in the case of damage to the main cane. The cane takes up little
space and can be left in a corner or folded up and put in a drawer or backpack when
not in use. In addition to its function as a mobility aid for detecting obstacles, the
long cane also acts as an indicator that the user is blind. This is advantageous to
many users, enables them to obtain assistance and indicates to other people the need
to give the approaching blind pedestrian extra space. However, the distinctiveness
of the cane and its symbolic association with blindness are also seen as stigmatising
by many potential users. Further drawbacks include incomplete protection against
drop-offs, lack information about objects at a height and insufﬁcient reach to pro-
vide much distance information or object and foot placement preview [83]. The
cane may also get caught or stuck in uneven or irregular ground and has no way of
detecting objects approaching at speed, such as balls or vehicles [64], though the
latter are likely to be heard. Using a long cane safely and effectively is not easy and
generally requires an extended period of training.
In summary the long cane is able to support blind people to travel safely, is low
cost and robust. However, it has limitations, particularly, the need for extensive
training to use it safely and the lack of information on high-up obstacles and beyond
the reach of the cane.
2.2
Understanding How Blind People Travel
In order to design effective mobility devices it is useful to have some understanding
of how blind people travel. The information presented here provides a brief over-
view rather than an indepth consideration of all the important issues. Mobility
requires the analysis of sensory information to determine a direction of travel, avoid
obstacles and move in a (relatively) straight line rather than veering. The travel may
be to or without a destination, for instance, when exploring an area or walking for
pleasure, though blind people generally travel to a speciﬁc destination rather than
walk for pleasure. Unlike sighted people, who rely largely on vision for spatial
information, blind people use information from all their senses with auditory and
tactile information generally the most important. However, partially sighted people
mainly use vision [35]. The different properties of the different senses (see Table 1)
[44, 79] mean that blind people generally lack preview and overview information
when travelling. Consequently, they need more frequent landmarks than sighted
people to check that they are on the correct route.
Blind people generally use some combination of auditory, tactile, olefactory,
proprioceptive and kinaesthetic information, with auditory and tactile information
generally the most important. Objects need to be relatively close to the person to be
perceived by touch, whether by contact with the cane, feet or hands or a combi-
nation of them. This means they act as either landmarks or obstacles or both
Mobility Technologies for Blind, Partially Sighted …
379

simultaneously and may change roles during a journey [35]. The lack of overview
and preview information available to blind people [35, 44, 78] makes route learning
particularly important. This enables them to use remembered knowledge about the
route in an analogous way to the use of preview and overview information by
sighted people. However, this memorisation involves a heavy cognitive load and
the signiﬁcant cognitive demands associated with concentrating on non-visual
information may act to limit the amount of information it is feasible to remember. In
addition, these factors in combination can mean that travel is very tiring for blind
people and involves a lot of concentration.
Blind people use shore lines, such as kerbs, the edge of grass verges and walls,
which are detected by the (long) cane in order to walk in a straight line. There is
often a relationship between the particular mobility aid used and the available
information. For instance, a blind person using a long cane will obtain obstacle and
shore line information from the cane and can use this information as landmarks,
while also needing to avoid the obstacles. Guide dog users will probably be una-
ware of obstacles which the dog guides them round and shore lines, since the dog
may walk in the middle of the pavement. They will therefore need to use other types
of landmarks. Consideration should be given to the information provided and its
likely impact on travel strategies when designing travel aids.
2.3
Categorisation of Travel Aids
There are a number of different categorisations of travel aids, including the fol-
lowing [45]:
1. Into primary and secondary aids: Primary aids are used on their own to deliver
safe mobility and largely involve detection of low-level obstacles on the path in
front of the user, for instance the long cane. Secondary aids are unable to
Table 1 Comparison of information from the different senses (adapted from [44])
Property
Vision
Touch
Hearing
Smell
Landmark
info
Do not vary with time of day
or season
Varies with time of day or season
Focus
Sharp
Sharp
Less sharp
Less sharp
Spatial ﬁeld
Large
Small
Large
Large
Object
location
Precise
Precise
within small
ﬁeld
Less precise than
vision
Less precise than
vision
Overview
information
Yes, many
signals at
once
No, ﬁeld is
too small
ﬁeld
No, signals
interfere with
each other
No, signals may
interfere with each
other
Object
identiﬁcation
Good
Less precise
than vision
Less easy than
vision
Very imprecise
380
M.A. Hersh

provide safe mobility on their own and are generally used to supplement pri-
mary aids, generally by detecting obstacles, for instance at a height or a greater
distance than available with the long cane.
2. Device functionality, including:
a. Mobility devices for obstacle avoidance and/or detecting objects at a height,
distance preview and/or detecting drop-offs
b. Orientation and navigation devices that provide information on landmarks
and support route ﬁnding
c. Environmental access assistive technology
d. Object location devices
e. Apps, including for contextual information, real time timetable and other
travel information and locating bus stops and other facilities.
3. The main technology used to obtain the environmental information, including
ultrasonic, infrared, camera, satellites (global positioning system GPS), mobile
phone technology and a combination of different sensors.
4. The way information is provided to the user:
a. Tactile: vibration; Braille; other
b. Speech
c. Non-speech sounds: musical tones, sounds of varying loudness and pitch;
virtual sound.
5. How the device is worn or carried
a. Hand-held: cane; other hand-held device
b. Carried: In a pocket or backpack
c. Worn: on head; on body.
3
Overview of Travel Aid Development
3.1
Phase 1: Obstacle Avoidance Devices
The development of electronic travel aids has taken place in a number of phases
[79], though there is some overlap between them and ﬁrst and second phase
developments are still continuing. The ﬁrst phase focused on resolving the limi-
tations of the long cane by detecting obstacles at a height and/or a greater distance
to allow preview, frequently by either a small box clipped to the cane or a long cane
with integrated additional features. From the design perspective there are probably
advantages in the detachable box, as long as it is securely fastened, rather than the
modiﬁed cane approach. If the additional components malfunction the box can be
sent for repair, leaving the user with a basic long cane. If damage occurs to the
mechanical cane, repair (or replacement) of a standard long cane is likely to be
Mobility Technologies for Blind, Partially Sighted …
381

considerably cheaper and quicker than that of a modiﬁed cane and the user can
transfer the box to another cane. In addition, the detachable box approach gives
users greater ﬂexibility by enabling them to use the additional features with the long
cane of their choice or different long canes in different conditions. Examples
include the laser cane [45], the smart cane [90], the ultracane [49] and the Tom
Pouce and Télétact [24]. An overview of some of the obstacle detection devices is
given in Table 2.
These canes all use infrared, ultrasonic and/or laser sensors to obtain environ-
mental information. The different types of sensors are discussed in slightly more
detail in Sect. 5.3. Most of the cane extension devices have used tactile or auditory
interfaces to provide information to users, with tactile information generally
Table 2 Overview of obstacle detection devices [45]
Device
name
Approx.
year
Technology
Body position
References
and notes
Current
availability
Sonicguide
ca 1965
Ultrasonic
Head worn
[52, 59, 60]
Discontinued
Bat K sonar
cane
ca 1965
Ultrasonic
Clips onto long
cane
[90]
Commercially
available
Pathsounder
ca 1966
Ultrasonic
Neck worn
[85]
Not available
Mims
device
ca 1970
Infrared light
Head worn
[68]
Prototype only
Mowat
sensor
ca 1970
Ultrasonic
Hand-held,
torch-like
See [25]
Not known
Laser
CaneTM
ca 1970
Infrared system
Hand-held cane
[7, 8, 70]
Commercially
available
Polaron
ca 1980
Ultrasonic and
ranging (sonar)
system
Hand-held torch
format or worn
on chest
[70]
Commercially
available
Sensory 6
ca 1980
Ultrasonic and
ranging (sonar)
system
Head worn
[25]
Custom orders
Wheelchair
Pathﬁnder
ca 1980
Ultrasonic and
Infrared
systems
Wheelchair
mounted
[70]
Commercially
available
Sonic
Pathﬁnder
ca 1990
Ultrasonic
Head worn
[25, 46]
Not known
Ultra
CaneTM
ca 1990
Ultrasonic
Hand-held cane
[50, 91]
Commercially
available
Tom Pouce
ca 1990
Infrared system
Cane mounted
[24]
Commercially
available
Teletact
ca 1990
Infrared system
Hand-held/Cane
mounted
[24]
Commercially
available
Miniguide
ca 1998
Ultrasonic
Hand-held
[26, 27]
Commercially
available
382
M.A. Hersh

vibratory and auditory information involving possibly musical sounds of different
frequencies. However, a number of these devices have paid insufﬁcient attention to
the difﬁculties related to the cognitive processing of complex information and the
importance of not impeding access to environmental sounds, which are vital for the
successful mobility of blind people. Successful use of devices presenting highly
complex information will frequently require an extensive training period, but in
many cases training has not been available, with the exceptions including the Tom
Pouce and Télétact [24].
A more intuitive interface is provided by robotic devices, such as the GuideCane
[92], which has not gone beyond the prototype stage. They contain a (small) motor
and power source, enabling them to move to avoid obstacles. The user feels this
movement through the handle and automatically moves to follow it. This gives an
intuitive and easy to use obstacle avoidance system which does not require
extensive training. This could enable some people who only go out accompanied to
travel on their own at, least on some routes. However, the disadvantage of this
intuitive interface is that users do not receive any information about landmarks or
obstacles. They may therefore also require a basic wayﬁnding or navigation device,
either integrated into the robotic cane or stand alone, to make up for the lack of
landmarks, enable them to have some idea of their location and verify they are on
the correct route. Drawbacks of the GuideCane, though not necessarily other
robotic devices, are its unattractive appearance, difﬁculties in using it on stairs and
the unsuitability due to the cumbersome design for use as a long cane in the case of
malfunction of the robotic features. Another cane extension device which has links
to the third phase (see Sect. 3.3) involves an attachment to the cane for ultrasonic
sensors audio and haptic output via an android app on a smartphone [64].
3.2
Phase 2: Navigation and Wayﬁnding Devices
Navigation and wayﬁnding devices were the focus of the second phase and require
the location of either the user or a particular point in the environment. This led to
the development of two distinct approaches with overlapping functionality [41]:
• Global navigation satellite systems (GNSS), currently most commonly using
global positioning systems (GPS), which locate the user.
• Environmental information beacons, which locate a point in space.
Both approaches can be used for outdoor navigation, but GNSS/GPS cannot be
used indoors. It requires a line of sight signal which can be disrupted or completely
obscured by tall buildings and dense foliage [30]. Since GNSS/GPS uses existing
satellite systems the end-user needs a receiver, but no transmitters embedded in the
environment or other environmental modiﬁcations are required. Beacons require a
transmitter and possibly other hardware to be installed in the environment. This is a
disadvantage, as it means that the user cannot control system availability and is
Mobility Technologies for Blind, Partially Sighted …
383

dependent on the installation of transmitters and possibly other hardware. The
provision of comprehensible information by GPS systems is dependent on the
availability of maps of the area, for instance to enable information to be linked to
particular streets and feasible turns to be indicated. Geographical information
systems (GIS) can provide useful environmental information in navigation aids
[58], but need to provide additional information for blind people, including on
streets, pavements, junctions and marked paths across the road [28]. GPS systems
for blind people include both stand-alone devices, such as the Trekker Breeze,
Trekker GPS, Navigator and Kapten and software, such as Wayﬁnder, that can be
used on a mobile phone or other mobile device. Both types of systems may provide
additional functions. GPS systems generally provide information on points of
interest and the location of facilities, for instance all restaurants within a certain
distance of the user. The system can then guide the user to the chosen restaurant or
other facility. Some environmental information beacons provide information about
buildings or send signals to request vehicle doors are opened or the presence of an
assistant. It has been suggested that guidance should combine orientation and
localisation and provide warnings and information about the location of destina-
tions, junctions and paths across roads, as well as orientation, progression and
route-ending instructions, with a high degree of precision in all cases [28].
Environmental information beacons and most satellite navigation systems use
speech to provide information, though some GPS systems, such as BrailleNote GPS,
use Braille, generally as well as speech. The widest range of options is available in
English, but examples in other languages include the Polish Navigator and the Czech
system of environmental information beacons. The Talking Signs system [10] uses
an infrared signal to transmit a repeating, directionally selective voice message from
the sign to a hand-held receiver some distance away. The message becomes louder as
the user approaches, helping them locate the facility indicated by the sign. The
beacon approach could provide information about several different facilities located
within a certain area of predetermined size around the user [53].
There
are
also
a
few
aids
which
combine
obstacle
avoidance
and
wayﬁnding/navigation functionality. The Stick for Environment ExplorationS (SEES
system, [96], Fig. 4) is a long cane-based device which combines satellite-based GPS
navigation with obstacle avoidance information from a camera. It can also provide
contextual information, such as the location of the nearest trafﬁc lights.
3.3
Phase Three: Apps for Smart Mobile Devices
The third or current phase of development involves apps for smart mobile devices
[56]. There seems to have been a natural progression from mainly hardware (phase
one), through a combination of hardware and software (phase two) to purely
software (phase three) with the hardware provided by an existing mobile device.
Developments in information and communication technology (ICT), which have
signiﬁcantly increased the computing power that can be contained on small mobile
384
M.A. Hersh

devices, have enabled the move to apps. Currently, mobility related apps are used
for very speciﬁc applications or provide very speciﬁc contextual information.
A number of different apps have been developed, but the potential to provide a wide
ranging navigation and information system has not yet been met. Design issues for
mobile apps will be discussed in Sect. 4.4.
4
General Design Issues
4.1
Evidence from Device Acceptance and Rejection
In addition to travel aids designed speciﬁcally for blind people, there are also
devices for the general population which could be used by blind people if appro-
priately designed. An example is the increasingly popular travel apps on smart
phones and other mobile devices. Travel aids for blind people therefore have fea-
tures in common with both other types of assistive technology and general purpose
(consumer) products. Therefore, good design practice for the design of general
purpose (consumer) products is also relevant to mobility devices.
Several studies have been carried out of end-user preferences for assistive
technology features. The factors found to be the most important included
effectiveness/functionality, costs of purchase (maintenance and repair), ease of use,
durability/reliability, simplicity, appearance, safety features, ease of repair, comfort,
portability and good instructions [6, 13, 94]. It should be noted that these factors are
not speciﬁc to the design of assistive devices and can be considered to be general
good design practice. There are also several studies of device use and abandonment,
but most of them have focused on the devices used by physically disabled people.
The results suggest that devices are most likely to be abandoned either in the ﬁrst
year or after ﬁve years. Abandonment in the ﬁrst year probably implies that users
either never really used the device in the ﬁrst place or were only requiring or
intending to use it for a short period to manage a short-term impairment or period of
ill health, whereas abandonment after ﬁve years probably indicates a change in the
user’s condition or circumstances after an extended period of use.
The main factors relating to device abandonment for a sample of mainly
physically disabled people were found to be user opinions not being taken into
account in device selection, easy device procurement and poor device performance,
reliability, ease of use, safety, durability and comfort, as well as changes in user
lifestyle, activities, conditions, needs or priorities, including lifestyle and activity
changes and changes in users’ conditions. However, lack of information and sup-
port may have contributed to poor ease of use [77]. Like other assistive devices,
mobility aids can be considered positively as an aid to independent living and
increased functionality or negatively as a symbol of lost functionality [29] and
disability status. This is borne out in a recent study of cane use by older people [37].
While it is important to challenge negative stereotypes of disability and the
marginalisation of disabled people, this is a much wider issue. However, designers
Mobility Technologies for Blind, Partially Sighted …
385

can design assistive devices to be as attractive as possible and to look like a device
suitable for and attractive to the general population rather than something that only
disabled people who have no choice about the matter will use.
A number of factors have been found to affect cane use by older blind people and
are summarised in Table 3 [37]. In practice most of them will also affect cane use
by younger people and the use of mobility devices in general by users of all ages.
4.2
Design Considerations
When designing mobility (and other) devices, designers should take into account
the particular end-users they are designing for and the skills they will need to use
the device effectively. This includes both general orientation and mobility skills and
the speciﬁc skills required to use the device. An associated issue is the need for
training [83]. While ease of use with a minimal need for training and minimal
mobility skills has a number of advantages, in some cases there may be tradeoffs
between device features and the skills and training required to use it effectively and
safely. On the one hand, mobility devices have an important role in enabling travel
by blind people, potentially including those with poor mobility skills who rarely if
at all go out on their own. On the other, this raises the ethical and practical issues of
the safety of blind people with limited mobility skills in the event of device mal-
function or batteries running down. It is still an open question as to whether it is
feasible to develop mobility devices which are sufﬁciently reliable and have
appropriate functionality and performance to allow blind people to depend on them
in the same way as on a sighted guide.
4.3
Principles of Good Design Practice
A number of principles of good design practice are listed below [3, 42] and several
of them will be discussed in more detail in later sections.
End-user factors
1. User-centred design: involvement of end-users from the very start and
throughout the design and development (and distribution) process [23].
2. Target end-users: awareness of the characteristics of the end-user groups the
device is being aimed at and essential and desirable end-user skills in order to
use the device safety and effectively.
3. Ease and intuitiveness of use: preferably without requiring extensive docu-
mentation and training and high levels of existing end-user skills. There may be
tradeoffs between ease and intuitiveness of use and the features provided or
more advanced features may require additional documentation and training and
higher levels of user skills.
386
M.A. Hersh

Table 3 Factors that affect cane use [37]
Category
Encourage cane use
Discourage cane use
Internal
Self image
Positive self-image
Negative self-image in general
and/or as a blind or disabled
person
Successful transition of
self-perception from sighted to
blind
Still has a self-perception as
sighted
Impacts of
stigma
Not worried about what other
people might think
Concerns about what other
people might think, particularly
those who knew them when
they were sighted
Reduced shame and
embarrassment due to
familiarity
Feelings of shame and
embarrassment
Equation of
cane and
blindness
Belief that cane use is only for
totally blind people
Attitudes to
(in)
dependence
Desire for independence
Acceptance of dependence and
overprotection
Safety
Concerns about their own
and/or their children’s safety,
possibly following accidents or
their close avoidance
External
Societal
attitudes to
blindness and
disability
Negative attitudes to blind and
other disabled people
Equation of blindness and cane
use as a symbol of blindness
Attitudes and
behaviour of
friends and
family
Lack of acceptance and/or
embarrassment of friends and
relatives
Support and encouragement
from friends, family to use a
cane and/or do things
themselves
(Over)protection by other
people and discouragement
from going out on their own
and/or doing things themselves
Lack of availability of other
people to accompany them and
do things for them
Other people do everything for
them
Safety
Experience of being targeted for
abuse due to the visible symbol
of blindness
Facilities
Lack of facilities in villages
acting as a disincentive to going
out
Training
Availability of O&M instructors
or experienced blind people to
teach or help and support them
Mobility Technologies for Blind, Partially Sighted …
387

4. Where feasible devices should be designed to minimise the required skills to
enable them to be used by as wide a range of users as possible. In practice, this
is unlikely to be feasible for more complex devices. It is also desirable that all
device users have some level of mobility skills so they can manage in the case of
device malfunction.
Functionality and features:
5. Functionality: speciﬁc clearly identiﬁed function(s) which are of value to users
and ﬁt into their lifestyles.
6. Appearance and portability: attractive, unobtrusive and/or similar to commonly
used devices, light weight, easily portable. In considering the maximum weight
the fact that the user may need to carry other items, such as shopping or tools,
books and papers for their work or studies, should be considered.
7. Hands free use: hands free use is desirable to free the hands for other activities,
such as using a cane and carrying baggage. If this is not possible use should
require at most one hand.
8. Good performance: the device performs well under a range of conditions.
9. Reliability, durability, comfort and safety: blind users require devices to travel
safely to their destinations. Device malfunction can leave users lost in an
unfamiliar location. In the case of devices which provide obstacle avoidance
functions, the option to use the device as a long cane in the event of mal-
function is desirable.
10. Low cost: this should cover purchase, maintenance, repair and upgrade to make
the device affordable. While small cost increases with increasing functionality
may be acceptable, low cost design is important to avoid unnecessary barriers
to device use.
11. Privacy management system: the importance of privacy and privacy manage-
ment is discussed in Sect. 5.4.
12. Language: text and speech interfaces should be available in a variety of
languages.
Design approach
13. Iterative, multi-criteria approaches: consideration of diverse factors including
function, form, attractiveness to all the senses, pleasure in use, usability,
accessibility, performance, reliability, safety and environmental factors. The
frameworks for taking into account important design factors include the
Promise Project’s six As: awareness, accessibility, availability, appropriateness
(usefulness), affordability and acceptability [16].
14. Design for ease of upgrading, repair and maintenance: robust design to reduce
the likelihood of faults occurring. This beneﬁts end-users and the environment
and could reduce costs.
15. A modular software architecture: this facilitates later developments, e.g. the
addition of other functions and reduces the impact of problems in one com-
ponent on other parts of the design.
388
M.A. Hersh

16. Compliance with relevant standards or other regulations: good design practice
generally goes beyond minimal compliance. Proactive rather than reactive
compliance generally reduces costs.
17. Compatibility with the long cane (if the device does not provide similar
functionality).
Information and training
18. Necessary skills and training: clear communication of the essential and desir-
able skills and required training to use devices safely and effectively and their
relationship to existing skills, so that potential end-users can make appropriate
choices about devices.
19. Documentation: the provision of appropriate information using clear language
and layout and availability in a range of alternative formats, with at least
electronic versions available in different languages.
20. Training: availability of suitable training locally for users with a wide range of
orientation and mobility skills, preferably free or at low cost.
21. Follow-up: the availability of follow-up information, support, maintenance and
repair facilities to end-users.
4.4
App Design
Mobility apps are equally relevant to disabled and non-disabled people, particularly
in unfamiliar, complex and unstructured environments and when using public
transport. Many apps, such as Find my bus or Find my bustop, provide information
of relevance to both blind and sighted people. If designed to be compatible with a
range of different interface modalities they can be used by both groups. Some
mobility support devices developed for non-disabled people may have particular
applications for disabled people. For instance, a smart travel alarm [69], which
provides an alert when the user reaches a particular location, could be set to indicate
when a blind user was approaching their bus or tram stop or railway station, with
the time in advance chosen to allow them sufﬁcient time to get off without haste.
However, not all features are of interest to both disabled and non-disabled people.
Facilities likely to be of interest to both groups include (i) personalised and con-
textual travel advice and information; (ii) route planning; (iii) navigation and
wayﬁnding; and (iv) facility location; whereas (v) obstacle avoidance is particularly
relevant to blind and some other groups of disabled travellers.
Blind people frequently also require both different types of information and more
speciﬁc information than sighted people, for instance to turn left at a particular
auditory or tactile landmark after walking two blocks rather than after the distance
of 500 m without any other information [18]. They may also require additional
information to walk these two blocks, for instance about the ground surface, the
best shore line to use and where to cross the intervening roads. When going to the
Mobility Technologies for Blind, Partially Sighted …
389

theatre information about the nearest bus stop and bus times for a particular facility
is of interest to both blind and sighted people, though sighted people are more likely
to have their own transport. However, blind people may also require detailed
information about the route from the bus stop to the theatre entrance, including
information about landmarks/obstacles, crossing points and the exact location of the
entrance, whether it is approached by steps or a ramp or is level with the ground.
They may also be interested in a live guidance/navigation feature and will want to
know whether and, if so, for what performances audio description is available. They
will also probably require text or audio descriptions of the theatre and an
in-building guidance feature or otherwise require assistance in ﬁnding their way
around the theatre. All these features could be provided by apps.
Apps have a number of beneﬁts and considerable potential for (blind) end-users,
both on a stand-alone basis and in conjunction with appropriate hardware. They use
an existing device, with frequently minimal additional costs for downloading the
app software. Many apps are open source or otherwise cost free, whereas others are
commercially available for a relatively small fee. A single mobile device is able to
provide a wide range of different functions through the use of different apps,
therefore reducing the number of devices the user needs to carry around with them
to obtain this functionality. Apps have the further very signiﬁcant advantage for
blind people of using a mobile device, such as a smart phone, which is used by the
non-disabled population. Consequently, unlike some other travel aids, app use is
not stigmatising. Apps are able to provide a range of contextual functions which
take account of location awareness [71], as well as to allow information sharing
between users. Context awareness is discussed brieﬂy in Sect. 5.5. This means that
app design should facilitate information sharing and the incorporation of the
information received in contextual functions. However, as discussed below,
improvements to the user interface will be required.
Apps on mobile devices could be used, probably together with appropriate
hardware, to provide improved obstacle avoidance systems in combination with
additional functions. A suitable user interface could be provided on the mobile
device, but it is still open to question whether the incorporated sensors would be
able to provide sufﬁcient and appropriate information or whether additional hard-
ware would be required. Such a device would be able to provide environmental and
contextual information. However, as in the case of cane extension devices, it would
probably need to be used in conjunction with the long cane. While having the
advantages of additional ﬂexibility and functionality, this would reduce the
advantage of the approach of only requiring a mobile device and not using a
potentially stigmatising additional ‘assistive’ travel aid.
By deﬁnition mobility apps are intended to be used while the user is mobile.
However, the small size and multi-functionality that make mobile devices attractive
by making them easily portable and enabling them to be used at any time and place
make them more difﬁcult to use, particularly, when the user is moving. Mobility
and small size lead to reduced memory and processing speed, and the need for
batteries to be regularly recharged [62], as well as small, relatively difﬁcult to see or
feel screens, keyboards and pointers. Other drawbacks which complicate mobile
390
M.A. Hersh

use and may reduce usability include limited, slow and unreliable connectivity, the
need for some proﬁciency in data entry and limited input (and output) modalities
[34, 97]. Small screens with low resolution can cause particular problems for
partially sighted people.
Using mobile devices while walking can add to cognitive load [34]. This may be
a particular issue for blind people due to the heavy cognitive load involved in their
travel, as discussed in Sect. 2.2, as well as the need to memorise the location of
controls on the touch screen interface. While accessibility features such as reading
aloud the labels of buttons and other items when the user taps them are useful, they
do not fully resolve this problem. The user still needs to be able to memorise the
approximate screen layout in order to reduce time searching. The ability to use a
keyboard or switch interface with mobile devices, offered by android and iOS
platforms, is probably more useful. However, there is still a need for the design of
mobile devices which have a range of input and output modalities without or with
only minimal increase in size. The extra complexity and cognitive load associated
with device use by blind people makes effective app design to facilitate ease of use
particularly important. Apps need to be easy to both learn how to use and remember
how to use when not used for a period.
A number of mobile apps provide navigation and wayﬁnding functions and
consequently require the localisation of either the user or a point in space [41].
Mobile devices generally localise the user. This raises ethical issues related to the
user’s privacy and security and, in particular, who has access to this information
and the measures required to prevent unauthorised access. This will be discussed
further in Sect. 5.4. Options for localising the user include the use of a camera,
Bluetooth and near-ﬁeld communication or radio frequency identiﬁcation (RFID)
with environmental markers [57]. Camera-based approaches have the signiﬁcant
advantage of not requiring environmental markers, which may not be available, but
the signiﬁcant disadvantage of requiring the camera to be focused in order to take
photos. This is not necessarily easy for blind people.
Mobile apps have been classiﬁed according to whether they run on the device’s
operating system (native), use a browser on a mobile device (web based) or use a
common code base for native like apps on a wide range of platforms (hybrid) [54],
Rudolph, undated). The choice of native or hybrid platforms is an important one,
which generally depends on the complexity and type of application. Native apps
may have advantages when development costs are not an issue and better user
experience or device speciﬁc features are required. Hybrid apps have the advan-
tages of allowing developers to use existing web skills and some device and
operating system features. They have reduced development time and costs, a single
code base for multiple platforms and easy design for various form factors, including
tablets. Many developers prefer to develop native apps, which allow them to use
device features, such as camera and sensors, and avoid the problems of poor device
connectivity, since the app is not web based. A number of developers have been
found to prefer open source platforms, whereas others are concerned about the lack
of adherence to standards that can result [54].
Mobility Technologies for Blind, Partially Sighted …
391

5
Design Features
5.1
Device Functionality
One of the important choices that inﬂuence many other aspects of the design is what
functions the device provides. The overview of the history of the development of
mobility devices in Sect. 3 shows a progression from obstacle avoidance devices,
through navigation and wayﬁnding devices, possibly with additional features, to
apps able to provide a range of possibly contextual and smart functions. Navigation
devices based on GPS are able to provide information about points of interest and
other location speciﬁc information. Wayﬁnding devices based on environmental
information beacons are able to guide users to a particular facility and provide
information about it. They may also offer options for requesting assistance, opening
the doors of public transport vehicles and turning on audio pedestrian crossing
indicators. Different approaches are frequently taken to navigation indoors and
outdoors. In particular, satellite systems do not function indoors and indoor systems
are of the environmental information beacon type based on the RFID or infrared
tags. The nature of indoor environments also means that they may require a rela-
tively high density of tags, e.g. one every few doors.
Other functions of interest include reading street names and numbers and
information boards, informing the user of their current location, ﬁnding pedestrian
crossings, bus stops and other facilities, identifying the building or feature in front
of the person and providing information about it, locating objects in various con-
texts, including goods in shops, indicating the user’s public transport stop or sta-
tion, and providing audio information about the status of trafﬁc lights, i.e. whether
the lights are indicating it is safe to cross the road [40]. Information sharing between
users may become increasingly important for blind travellers. GPS/satellite navi-
gation systems already allow the sharing of points of interest and information
sharing apps are being developed to allow information sharing about best routes
and other features. If apps are designed to facilitate this, information sharing could
be a way to overcome the lack of the very detailed information, such as the location
of pedestrian crossings and bus stops required by blind people, in GIS and digital
maps. Giving blind end-users control of the information available to them is clearly
desirable. There are also arguments that this could reduce the onus on designers and
developers to produce fully accessible systems. However, the discussion is beyond
the scope of this chapter.
5.2
Interface Design
The system interface consists of input(s) and output(s). End-users use the input(s) to
‘communicate’ with the device, operate it and in some cases give it instructions.
The output(s) are the means by which the device conveys information to the user.
392
M.A. Hersh

In the case of simple mechanical devices such as the long cane to a large extent the
handle serves the function of input, though the concept is less relevant than for
more complex devices. For the cane it is again the handle which acts as the output
and through which the user feels and can explore objects and the ground surface.
However, good input and output design is likely to be more difﬁcult in the case of
more complex devices.
Since mobility devices are intended to be used while the user is moving the
interface should be designed to be easy as possible to use while moving and
preferably not require users to slow down or stop in order to use it. In addition both
the input and output need to be accessible to blind people. The graphical user
interfaces used in many mobile devices are both inaccessible to blind users and
inappropriate to sighted users. In particular, they generally require too much of the
user’s attention and cannot be used in hands free mode, whereas the user needs to
pay attention to avoiding danger, real world tasks and engaging with other people
and needs their hands free for other activities [48], including for holding a cane in
the case of blind people.
Accessible input includes speech, a keypad or keyboard a joystick or switch and
press buttons. A multinational survey of blind people on attitudes to and prefer-
ences for different features of a robotic guide found that just over two thirds wanted
speech input, just over two ﬁfths a keypad and about a quarter a joystick or switch,
with much smaller numbers interested in buttons [39]. Unfortunately, speech
recognition rates in noisy environments are still low, leading to the possibility of
error, and the voice recognition system generally needs to be trained. In addition,
the ability of devices to distinguish between speech and non-speech sounds is
frequently poor, leading to action easily being triggered by background sounds.
Speech input is most likely to be effective in mobility devices when used for a
relatively small set of commands, with the expressions used very clearly distinct
from each other. In addition, a means of indicating the start and end of each
command, either through the use of a distinct sound or pressing a button on the
device would be useful. In many, though not all, cases the restriction to a set of
commands will not be a strong limitation. However, the need to indicate the start
and end of each command could be irritating and will sometimes be forgotten.
Another option could be to conﬁrm command in response to a speech prompt.
However, both these approaches could make speech input slightly less easy to use
and therefore less attractive.
Other options suggested by survey respondents include the use of buttons and a
USB or wireless connection for route planning, conﬁguration and software updates
[39]. Buttons are particularly suited to devices which only require a small number of
controls. They should be appropriately located and tactilely distinct to make it easy
for blind people to distinguish them from each other, including when the user is
moving. Prototype 2D arrays of mini vibrators have been used to represent 3D
information. Positioning them on the chest or back has the advantages of keeping the
hands free, giving a direct relationship between 2D positions in space and on the
display and allowing them to be hidden under clothes [9], thereby avoiding stig-
matisation or other unwelcome attention. However, the back and chest are not
Mobility Technologies for Blind, Partially Sighted …
393

particularly sensitive areas, so tactile displays using them may not provide sufﬁcient
resolution for complex spatial data. 3D virtual audio displays transforms audio sig-
nals into separate signals to the two ears that the user listens to through stereo
headphones using different audio ﬁlters in the left and right ear paths. More complex
version uses a constructed 3D scene model for each selected frame [48, 88, 89].
Multinational survey respondents have been found to have a very strong pref-
erence for speech output with only a small percentage interested in Braille [39].
Several respondents noted that Braille output would not be particularly useful due
to the small percentage of blind people who know Braille and that the time involved
in reading Braille would slow the user down. Braille output mobility devices, such
as the BrailleNote GPS, generally also provide speech output. The BrailleNote GPS
also provides a choice of Braille and qwerty key input. It can be worn with a
shoulder strap, and could therefore possibly be used while walking slowly by guide
dog users. However, safety considerations make it preferable for users to stop to use
the Braille outputs and keyboard input.
Speech output has a number of advantages. In particular, it can convey both
simple and relatively complex information in a format that most potential users ﬁnd
easy to understand. However, it can be difﬁcult to understand or even incompre-
hensible in noisy environments. It will generally provide little comprehensible
information to people with more than mild or possibly moderate hearing impair-
ments, with understanding decreasing more rapidly than for people without hearing
impairments with increasing noise. Other output options include musical sounds or
other audio output, tactile indications and text messages. For instance, speciﬁc types
of sounds could be used to indicate particular obstacles and vibration of the handle
to signal approaching obstacles.
Other possibilities include a combination of speech and vibro-tactile signals,
choice of synthetic voice, Braille and other options, a text message on a mobile
phone or palmtop and different types of feedback to indicate different obstacle
directions. Clock face information may be particularly suitable for given directions
[65] and seems to be preferred by blind people. However, information can only be
given in this format on one obstacle at a time. It is generally the object closest to the
user within a certain arc in front of them that is most critical. Since the object
position changes as the user moves, it may be more appropriate to use audio beepers
[65] or other virtual sound located in the environment to indicate the direction.
Where a device has more than one message to convey to users, e.g. about obstacle
locations or safe path directions, the order of message delivery needs to be decided
with information critical for the user’s safety given priority over other information
[65]. All audio information needs to be transmitted carefully to ensure that it does
not block access to environmental sounds which are very important for blind
people. This includes the consideration of the design of head phone or ear pieces
used to convey the sound and the frequency and type of sound.
(Vibro-)tactile output has the advantage of being suitable for deafblind people,
including those with signiﬁcant hearing impairments. It can also be applied to
different parts of the body, e.g. right or left wrist or shoulder to indicate the
approximate direction of an obstacle or the direction in which the user should turn.
394
M.A. Hersh

The distance to obstacles or other objects and obstacles/objects at different distances
could be indicated by either different types of tactile input or sound, with for
instance the frequency of vibration, repetition rate or loudness of the sound
increasing as the object approaches. However, there is more scope to do this with
sounds. In both cases users will generally require a relatively long period of training
or familiarisation before they can easily understand the output.
The part of the body stimulated by (vibro-)tactile output is also important. The
most sensitive parts of the body are the hands, lips and tongue. However, devices
with vibro-tactile output to the lips and tongue are likely to be socially unacceptable
and would interfere with speech. This would affect both the social interactions of
blind people while travelling and their ability to ask for directions or assistance.
Since this is an important component of independent travel for blind people [41]
obstructing the tongue and/or lips would make travel more difﬁcult for blind people.
Suggestions of vibro-tactile output to the forehead via a hat [33] are also unsuitable.
Stimulation of the head should be avoided due to the possibility of headache or
even dizziness or disorientation and some users are likely to be concerned about the
effect of the hat on their appearance. There are also issues of the appropriateness of
hat wearing to the climate and its possible social and cultural implications.
Another tactile output option involves ﬁnger Braille with vibratory stimulation
of three ﬁngers on both hands [2]. This is less bulky and cheaper than other Braille
displays and could leave the hands free for other activities, but does not overcome
the problem of lack of Braille knowledge by most blind people. The robustness of
the design would need to be improved, including through using stronger materials
and either becoming totally wireless or wires being hidden. Another option involves
using an existing mobile device, such as a mobile or smart phone, as the interface.
This would have the advantages of familiarity, thereby possibly reducing the time
involved
in
training,
reducing
costs
and
the
use
of
a
general
purpose
non-stigmatising device. In this case, a platform independent design would ensure
the widest usage, though it should be noted that 82.8% of mobile devices had
android operating systems in 2015 followed by iOS with 13.8% [51].
There are also still open questions as to what types of and how much information
users can comprehend easily while travelling, the contextual and other factors that
affect this, and how interface designcan facilitate information processing and reduce
cognitive load. The answers to these questions are very important for device design.
There will also be tradeoffs between the amount of information that can be rela-
tively easily processed and whether the user is able to continue walking or needs to
stop to use the device.
In summary, there is evidence that blind people have strong preferences for
speech input and output, at least for more complex devices. However, the perfor-
mance of speech input systems still needs improvement, speech output systems are
unsuitable in noisy environments and for many deafblind people and need to be
appropriately designed to avoid blocking access to environmental information. The
above discussion also indicates that interfaces for blind people should meet the
following conditions:
Mobility Technologies for Blind, Partially Sighted …
395

1. High quality
2. Low cost
3. Easy to use while the user is moving and preferably do not require users to slow
down or stop in order to use it
4. Non-interference of any audio output with environmental sounds
5. A choice of (vibro-)tactile and audio output, with audio output options including
speech, other than for simple mechanical devices.
5.3
Sensors
In simple terms mobile travel aids function by obtaining information from sensors,
processing this information and transmitting it to the user through actuators of the
user interface. Thus, sensors have a crucial role in mobility devices, with system
performance dependent on the quality, quantity and type of information they pro-
vide. The types of sensors required and their prerequisite properties depend on the
speciﬁc application. There may be tradeoffs between different desirable properties,
such as low cost and high accuracy and precision.
Sensors which can be used in obstacle avoidance devicesto determine the
position of objects include infrared, ultrasonic, laser range ﬁnders and (video)
camera. Devices that use sonar can function in the dark, rain and snow, making
them very versatile for outdoor operation. However, sonar echoes become distorted
in crowded or conﬁned places, making the information transmitted unreliable [30].
Laser range ﬁnders are generally more accurate than ultrasonic and infrared sensors,
but they are expensive [39]. Camera-based devices work well in a range of indoor
and outdoor conditions, but may have problems stabilising images when the user is
moving and there can be considerable variation in illumination both within and
between scenes [30]. More recent developments (e.g. [4, 63]) are able to provide
additional environmental information through the use of cameras, particularly
RGB-D, ranging or depth cameras with signal processing algorithms.
The most commonly used sensors in navigation and wayﬁnding systems are
satellites, and infrared and RFID systems for satellite navigation systems and en-
vironmental information beacons respectively. As discussed in Sect. 3.2, there are
signiﬁcant differences in the types of information available in the two different
approaches.
Inertial sensors comprising a combination of accelerometer, gyroscope and
magnetometer, and optoelectronic sensors could be used in navigation and
wayﬁnding systems to determine the user’s orientation [61]. This is particularly
important for blind people and can also be useful for sighted people, for instance
when leaving a poorly signposted (large) unfamiliar railway station. Inertial sensors
have the advantage of relatively small size and therefore being relatively incon-
spicuous and have been found to generally give satisfactory performance in terms
of errors. Inertial and optoelectronic sensors have been found to give comparable
396
M.A. Hersh

results [61]. However, care needs to be taken when using sensors which include
magnetometers to avoid interference from ferromagnetic materials. This may be a
particular issue when the user is using additional assistive technology containing
ferromagnetic materials which move independently relative to the magnetometer.
Interference from ferromagnetic objects to which the sensors are connected, so that
the motion is not independent, can be determined and its impact designed out.
However, this is not possible when the motion is independent [61].
5.4
Privacy
As discussed in the previous section, most electronic travel aids use information
from sensors. This potentially raises security and privacy issues related to pro-
tecting access to this information, determining who, if anyone, in addition to the
user can access it and what information they are permitted to access. This may be
dependent on the nature of the information, what inferences it allows to be made
about the user and their attitude to privacy. Mobility aids that locate the user and/or
obtain contextual information about them and/or the context of use raise particular
issues, which should be, but are rarely considered in aid design. Some of the
technologies involved such as RFID tags are particularly insecure and can be easily
read by unauthorised people [81]. This could lead to the targeting of blind people
who may be seen as vulnerable by ill-intentioned individuals, Security schemes for
managing access to RFID information are being developed, e.g. [74], including for
mobile RFID devices [73, 81]. However, in practice they are not generally used.
The lack of discussion in the literature of privacy issues associated with assistive
technology implies this is rarely considered in assistive device design. Therefore,
designers rarely provide information to end-users about privacy issues, making it
highly unlikely that end-users are aware of them. It is quite probable both that
different users will have different attitudes to privacy and that many blind end-users
would be willing to accept a reduction in privacy for increased device functionality,
particularly if this leads to increased independence. However, this should not be
assumed and developers need to make information available to suppliers and
end-users in clear language that can be easily understood by people without a
technical background. They should also consult end-users about their privacy
preferences and tradeoffs.
However, it is preferable to avoid users having to make choices between
improved mobility (device functionality) and privacy. While, it may not be possible
to totally eliminate privacy and security concerns, the use of privacy management
and security systems can signiﬁcantly reduce them and enable users to enjoy good
device functionality with no or only a minimal reduction in privacy. They can be
used to allow users to determine privacy settings and give them control over
authorised access to information. They can also be used to signiﬁcantly reduce the
risk of unauthorised access, though it is very difﬁcult to eliminate this entirely.
Mobility Technologies for Blind, Partially Sighted …
397

Privacy protection has been divided into eight requirements which need to be
addressed in order to reduce the likelihood of the collection of data that can identify
the user. Privacy protection approaches can be divided into the two categories of
security requirements engineering which involves consideration of security and
privacy issues in the early stages of system development and technological solu-
tions in system implementation [55]. Security requirements engineering has the
drawback of not linking the identiﬁed requirements to implementation solutions.
Consequently, it does not take account of the important relationship between user
needs and software capabilities. It is also possible that blind people may have
privacy concerns relating to the functions they use the mobility device to carry out.
This implies that protocols to ensure the anonymity of the user carrying out par-
ticular functions may be required or pseudoanonymity if full anonymity is not
possible.
5.5
Context Awareness
Context awareness involves adapting or personalising information for the indi-
vidual user [11]. Context is multidimensional. It includes physical, cognitive,
temporal, social and application factors [11] and any information that is relevant to
the interaction between the user and an application [1]. This allows devices to act in
the absence of explicit user instructions in order to modify the system. The aim is to
make interaction more natural and personalised and to reduce the need for end-user
interaction, as this can often be inferred. This would have the advantage of freeing
end-users to, for instance, concentrate on route learning, talking to a friend or
enjoying the walk rather than interaction with the device. However, the extent and
type of automatic interaction would need to be under end-user control to eliminate
the possibility of the initiation of actions users disagreed with or the device con-
trolling activities users want to retain under their own control.
An understanding of context can be used to identify the dimensions necessary to
infer a user’s activities and intentions and provide more relevant context-speciﬁc
feedback. Contextual information can be obtained directly from sensors, through a
middleware infrastructure which introduces a layered architecture with low level
sensing or via a context server which gives multiple clients access to a remote data
server [17]. Mobile devices generally have built in sensors and access data directly
from them. However, they may also want access to remote data sources. Sensors
can be used to obtain, for instance, data about the local environment and the user’s
state, and contextual resources, e.g. diaries to obtain information about the user’s
meetings and other activities. In addition, communications to external services, for
instance via Bluetooth, internet protocols or messaging services can be used to
obtain contextual information from them [80]. The different options for representing
the context include the use of context widgets, as typiﬁed by the Contextual
Toolbox [22, 86] the use of the blackboard metaphor [5, 95] and an
398
M.A. Hersh

infrastructure-centred distributed service model based on the client-server dialogue.
The Contextual Toolkit provides a framework for using context widgets and other
software components to represent the context and enable applications to access
contextual information from their operating environment, combine it in various
ways and increase its level of abstraction or otherwise process it. The blackboard
metaphor uses a data centric approach. It is less efﬁcient than a widget architecture,
but more robust.
Context aware systems should also allow extension of data types to enable
additional sensor-based and inferred contextual variables to be added to the system
[80]. Contextual systems can also support and facilitate information sharing
between users. Blind people already share verbal route descriptions and travel
experiences [28] and points of interest on GPS systems. Context awareapps are able
to do this in a much more sophisticated way, e.g. [20]. One of the problems in using
data from unknown sources is its reliability. However, the volume of the data, even
in the case of the considerably smaller blind than sighted population and the fact
that correct data will be repeated, whereas erroneous data is likely to be erroneous
in different ways, make it highly likely that the ﬁnal result will be correct.
As discussed in Sect. 5.4 the collection, analysis and storage of user and con-
textual information raises privacy and security issues which need to be appropri-
ately managed. Different users may be interested in different types of contextual
information, want to use it in different ways and be able to process different
amounts and types of information without this having a negative impact on travel
safety. There is therefore a need for options for personalisation of the ways in which
contextual information is collected, used and protected. User proﬁles could be used
to store user preferences and dislikes in this area, as well as other aspects of device
use, such as the preferred type of interface. Personalisation options also raise design
issues of what, if any, design featuresshould be the default option and whether the
selection of contextual features should be based on opting in or opting out or a
combination of the two.
6
The Design Process and End-User Involvement
6.1
Stages of the Design Process
Design consists of a multi-stage iterative process with both frequent backtracking
and overlap between the different stages. As already discussed, best practice
requires user-centred design, which has been shown to lead to more usable systems
and to save time in large projects [11]. This is particularly important for mobility
(and other assistive) devices for blind (and other disabled) people as few designers
of mobility (or other assistive) devices for blind people are themselves blind (or
otherwise disabled). An example of the problems that can occur otherwise is
illustrated by an article on future mobility technologies from the 1970s which
suggested that the blind person of the future would be like a robot covered in
Mobility Technologies for Blind, Partially Sighted …
399

technology and clanking and clicking as they walked. This shows a total lack of
understanding of the social aspects of technology design and the fact that blind
people are just as concerned about their appearance as sighted people. It also shows
a lack of awareness of the importance of environmental sounds to blind people,
since these sounds would presumably be drowned out by all the clanking and
clicking. In particular this illustrates the problems that can result from designers
using design approaches based on their stereotypes and misconceptions about the
end-user group(s). Another common design error is design based on the charac-
teristics, needs and preferences of the designer(s), even when they are very different
from those of the end-users they are supposed to be designing for.
Most of the diverse approaches to user-centred design have the following four
main steps [15, 32], which could be considered a model of user-centred design:
1. Learning about the users and their activities as early as possible, preferably right
at the start.
2. Applying this knowledge about the users to inform the design.
3. Repeatedly presenting the users with early prototypes for evaluation.
4. An iterative cycle of design, (end-user) testing, measurement and redesign,
which is repeated as often as necessary, to identify and take account of any
issues identiﬁed in end-user testing and resolve the associated problems.
6.2
End-User Involvement
The design of effective mobility devices which are attractive to users requires an
understanding of the target user groups and their goals, requirements, preferences
[87], lifestyle, language, cultural and other contexts, which may be different from
those of the dominant culture of the country or region they are in [72]. As indicated
above, this is particularly important in the case of technologies for blind or other
groups of disabled people. Only (potential) blind end-users fully understand their
own requirements for mobility devices, can test what does and does not work in
practice and determine whether the proposed device is useful to them and appro-
priate for their lifestyle. Appropriate functionality and good technical performance
are clearly very important for mobility devices. However, as discussed in Sect. 4, a
number of other issues including appearance and cost are equally important.
Decisions on functionality and mode of use are generally made early on, with
changes at a later date likely to be both less effective and more expensive. Asking
end-users about an existing design rather than consulting them and using the results
to draw up a design could lead to a less than optimal result. There is also the risk of
bias with designers being tempted to try to obtain information which supports their
existing design [15] rather than investigating end-user opinions, requirements and
desires. This further supports the importance of end-user involvement right from the
start. The primary users of mobility devices are blind people. There are also sec-
ondary users, including orientation and mobility instructors, some family members
400
M.A. Hersh

and various other professionals. Their role in the use of these technologies is
subordinate to that of blind end-users, who should be given the greatest consid-
eration in the design process. However, there may be some value in consultation
with O&M instructors, particularly with regard to any requirements for training to
use the devices effectively and safely and information on how this training ﬁts into
the wider context of O&M training.
(Potential) funding organisations, including state/public healthcare and social
services departments and a range of non-governmental organisations, are also
stakeholders. Their interests and perspectives are not necessarily the same as those
of blind end-users. Therefore, they should not be involved in the design process to
ensure independence and the highest design standards [42]. However, designers of
new mobility technologies should be aware that potential blind end-users of these
technologies may require full or partial ﬁnancial assistive in order to purchase them,
though this should not lead to design compromises in an attempt to please potential
funders. In addition, users are more likely to become aware of and therefore use
devices that are available through social and/or health services, even if they have to
pay all or part of the costs themselves. O&M instructors may also have a role in
recommending particular devices to blind people, but are not involved in funding
them. While, it can be useful for designers to develop relationships to inform
instructors, social and health services and other potential suppliers and funders of
available devices and their features, these relationships should not be allowed to
inﬂuence the design process. It should also be noted that the availability of after
sales information, support, maintenance and repairs is both useful to end-users and
can positively affect decisions on supplying and promoting a device.
The involvement of end-users [42] can be broadly divided into the three cate-
gories of participation, consultation and research involvement. In participative
approaches end-users generally represent one or more organisations, are members
of research teams and participate in decision making and other activities in the
research and development project. Their role is therefore analogous to that to other
researchers and developers participating in the project, with a particular focus on
their end-user expertise. In consultation end-users act as consultants and provide
advice at different stages in the project, including initially, and are involved in
testing and evaluating prototypes and ﬁnished devices. Research participants, who
used to be referred to as research subjects, participate in questionnaires, interviews
and focus groups, as well as experiments of different types, in addition to partici-
pating in similar activities to consultants. The basic differences are the treatment of
consultants as experts and research participant as general members of the popula-
tion of interest and generally asking for personal data from participants, but not
consultants, for statistical correlation purposes and to check the whole population of
interest is covered. Ethical approval is required for the involvement of research
participants, but not of consultants.
Involvement of blind end-users in mobility (or other) device research and
development requires consideration of the diversity of the population of blind
people on factors including age, gender, race/ethnicity, education, employment,
O&M training and skills, other impairments, extent of visual impairment and the
Mobility Technologies for Blind, Partially Sighted …
401

age of onset of visual impairment. These and other factors may affect end-user
requirements, preferences and the ways they use mobility devices. Doing this
effectively may require consideration of cultural factors in the design of ques-
tionnaires and interviews and translation of information and questions into users’
preferred languages. Care is required when translating questionnaires and inter-
views in cross and multicultural research to ensure that the meaning is not distorted
and is equivalent in the different versions and cultural and national differences,
including attitudes and institutions, are taken account of [43, 67]. This may include,
for instance, differences in the school system with some countries providing sec-
ondary education in one school and others dividing it into upper and lower sec-
ondary schools. Even in this relatively simple case the use of inappropriate
terminology could lead to misunderstandings, incorrect answers or even the with-
drawal of potential participants.
Many blind and partially sighted people ﬁnd printed documents inaccessible.
Therefore, all documents should be made available in a range of different formats to
take account of the accessibility requirements of the end-users’ being consulted. This
may include electronic formats which are compatible with screen readers and screen
magniﬁers, large print with the font size and colours suited to the particular end-users
requirements and Braille, though only a minority of blind people are ﬂuent Braille
readers. It should be noted that PDF documents are not necessarily accessible [38].
Decisions on different design factors may involve the presentation of mock-ups
of the device to end-users. The mock-ups need to be designed in a way that enables
their main features to be investigated by blind people. Some care will be required
with this, since drawings or cardboard cut-outs which are investigated visually are
generally not appropriate. Further issues related to end-user involvement and some
of the approaches are discussed in Hersh [42].
6.3
Device Testing
There are numerous reports in the literature of device testing by blindfolded sighted
people, presumably because the researchers have found it easier to ﬁnd sighted than
blind volunteers. While this may be acceptable for the earliest testing stages, the
travel strategies and spatial knowledge and understanding of blindfolded sighted
and blind people are not identical. In addition, factors such as age, age of onset and
history of visual impairment, experience of early exploration and experience of
O&M training and the presence of additional impairments, will affect blind people’s
travel experiences and performance. Therefore testing needs to involve blind people
with a variety of personal characteristics to obtain a real understanding of device
performance, acceptability and attractiveness to potential end-users. There may be
value in combining end-user ﬁeld tests with usability measurement studies by
measurement professionals [71], with particular beneﬁts from the involvement of
blind and partially sighted professionals. However this may require more blind and
partially sighted measurement professionals to be trained.
402
M.A. Hersh

Device evaluation often involves measurement of the time required to complete
tasks, but this is insufﬁcient on its own. In addition, conﬁdence in the device, the
provision of accurate information, successful task completion, ease of use and
safety may all be very important to potential end-users. Therefore, full evaluation
will generally require consideration of a number of different factors and should
usually include both quantitative data on user performance with the device and
qualitative data on user subjective opinions of it. It is also useful to record and
analyse different types of errors to determine which ones were device related and
which a result of, for instance, misunderstanding the instructions. It is also
important to obtain users’ opinions of the device and subjective impressions of task
difﬁculty both before and after the experiment [66]. Various factors related to how
performance evaluation is carried will affect the results. For instance, the length of
time allowed and whether and when participants are allowed to ask for assistance
can also affect performance.
The amount of training in device use before tests are carried out will frequently
affect both performance and user safety. There are also issues of the duration and
type of training and experience using the device required for meaningful results can
be obtained, particularly in the case of more complex devices. There are clearly
advantages in the development of devices whose use is intuitively obvious and at
most requires a simple list of instructions. However, the complex information
provided by some electronic cane (extension) devices requires an extensive period
of familiarisation before participants can use the device safely and effectively and
obtain maximum beneﬁt from it. This then raises the question of the point in the
training/familiarisation cycle at which testing should be carried out so as to obtain
appropriate trade-offs between participants’ time commitment to a device that may
not be developed and the likelihood of obtaining useful results. It may also be
useful to obtain estimates of the average length of trainingrequired to obtain par-
ticular levels of performance.
Quantitative measurements commonly used for obstacle avoidance devices
include the number of contacts with obstacles and the percentage of preferred
walking speed (PPWS) with the device, which has largely replaced measurement of
time or speed [76]. Preferred walking speed can be obtained by measurement of
walking speed on a straight course without obstacles when the person is using their
standard mobility aid or walking with a sighted guide. PPWS is obtained by
measuring walking speed when using the device in different circumstances and
calculating the ratio of this to preferred walking speed. It has the advantage of
allowing participants to act as their own controls and giving an automatic com-
parison of performance with and without the device (Patel et al. 76). However, in
some cases the use of mobility devices will lead to a reduction in PPWS due to the
time and cognitive effort associated with device use, but will have other beneﬁts
such as enabling users to travel more widely on their own. In addition, PPWS with
Mobility Technologies for Blind, Partially Sighted …
403

the device may increase with training and device use. In the case of wayﬁnding
devices other useful measures could include the number of times the user has to
backtrack and the ratio of the length of the route followed compared to the mini-
mum length route that meets the user speciﬁcations.
Safety is a very important issue when blind people test mobility devices. Initial
end-user testing of prototypes and devices can take place on constructed courses
with carefully placed obstacles and no moving trafﬁc, but end-user testing in real
environments will also be required to fully understand the features and performance
of the device and user reactions to it. Even unfamiliar constructed courses have
some risk, due to the presence of obstacles and the use of a new device with which
the user is not yet fully familiar. Therefore, safety monitoring is essential at least in
the initial and intermediate stages of device testing until the safety of the particular
user with the device has been established through extensive testing. This should
preferably be carried out by an O&M instructor and otherwise by a person with
appropriate expertise and experience and/or training. In most cases the safety
monitor should only monitor one blind person at a time and not concurrently be
involved in observation or other aspects of the research. They should remain alert
and be ready to intervene swiftly to remove the blind person from potential harm if
necessary. The safety of participants should be of paramount importance at all times
and override any other considerations. It is therefore prudent to design end-user
tests in such a way that they can be easily restarted after any necessary interventions
to remove participants from potential danger.
7
Conclusions
Design is an complex and multi-dimensional topic. Therefore, even in a relatively
long chapter it is not possible to give more than an overview. The chapter has
stressed the importance of the involvement of blind, partially sighted and deafblind
end-users from the start and throughout the research, development and design
process. An understanding of how blind people travel, including the ways they use
information from all their senses and travel aids, is a further essential prerequisite
for good design. While an overview has been provided, indepth understanding can
best be obtained by interaction with blind people.
The chapter has also provided a brief presentation of the different phases in the
development of travel aids, a summary of the different ways of categorising them
and some principles of good practice for developing new travel aids. Speciﬁc
features, such as the sensors, interface, privacy management and contextual fea-
tures, have also been considered.
404
M.A. Hersh

References
1. Abowd
GD,
Dey
AK
(2000)
Towards
a
better
understanding
of
context
and
context-awareness. In: Proceedings of CHI workshop on the what, who, where, when and
how of context-awareness, The Hague, Netherlands
2. Amemiya T, Yamashita J, Hirota K, Hirose M (2004). Virtual leading blocks for the
deaf-blind: a real-time way-ﬁnder by verbal-nonverbal hybrid interface and high-density
RFID tag space. In: Virtual Reality, 2004. Proceedings. IEEE, pp 165–287
3. Andò B (2003) Electronic sensory systems for the visually impaired. Instrum Meas Mag IEEE
6(2):62–67
4. Arditi A, Tian Y (2013) User interface preferences in the design of a camera-based navigation
and wayﬁnding aid. J Visual Impair Blind 107(2):18–129
5. Baldauf, Dustdar MS, Rosenberg F (2007) A survey on context-aware systems. Int J Ad Hoc
and Ubiquit Comput 7(4):263–277
6. Batavia AI, Hammer GS (1990) Toward the development of consumer-based criteria for the
evaluation of assistive devices. J Rehabil Res Dev 27(4):425–436
7. Benjamin JM, Ali NA, Schepis AF (1973) A laser cane for the blind. Proc San Diego Biomed
Symp 12:53–57
8. Blasch BB, Long RG, Grifﬁn-Shirley N (1989) National evaluation of electronic travel aids
for blind and visually impaired individuals: implications for design, RESNA 12th annual
conference. Louisiana, New Orleans, pp 133–134
9. Bourbakis N (2008) Sensing surrounding 3-D space for navigation of the blind. Eng Med Biol
Mag IEEE 27(1):49–55
10. Brabyn J, Crandall W, Gerrey W (1993) Talking signs: a remote signage, solution for the
blind, visually impaired and reading disabled. In: Annual international conference of the IEEE
engineering in medicine and biology society, pp 1309–1310
11. Bradley N, Dunlop M (2008) Navigation AT: context-aware computing. In: Hersh MA,
Johnson MA (eds) Assistive technology for visually impaired and blind people, Springer,
Berlin, pp 231–260. ISBN 978-1-84628-866-1
12. BrailleNote GPS (undated) http://store.humanware.com/hus/braillenote-gps-software-and-
receiver-package.html. Accessed 7 June 2016
13. Brooks NA, Hoyer, EA (1989) Consumer evaluation of assistive devices. In: Proceedings of
the 12th annual conference of the rehabilitation engineering society of North America,
pp 358–359
14. Bruce I, McKennell AC, Walker EC (1991) Blind and partially sighted adults in Britain: the
RNIB survey, Vol. 1. HM Stationery Ofﬁce, UK, p 120
15. Buurman RD (1997) User-centred design of smart products. Ergonomics 40(10):1159–1169
16. CEN (2003) [internet] CEN workshop agreement CWA 14661. Guidelines to standardisers of
ICT products and services in the CEN ICT domain. [cited 2010 Aug 12]. Available from:
ftp://cenftp1.cenorm.be/PUBLIC/CWAs/e-Europe/DFA/cwa14661-00-2003-Feb.pdf
17. Chen H (2004) An intelligent broker architecture for pervasive context-aware systems. PhD
Thesis, University of Maryland, Baltimore
18. Chen HE, Lin YY, Chen CH, Wang I (2015) BlindNavi: a navigation app for the visually
impaired smartphone user. In: Proceedings of the 33rd annual ACM conference extended
abstracts on human factors in computing systems, pp 19–24
19. Clark-Carter DD, Heyes AD, Howarth CI (1986) The efﬁciency and walking speed of visually
impaired people. Ergonomics 29(6):779–789
20. De Choudhury M, Feldman M, Amer-Yahia S, Golbandi N, Lempel R, Yu C (2010)
Automatic construction of travel itineraries using social breadcrumbs. In: Proceedings of the
21st ACM conference on hypertext and hypermedia, ACM Press, New York, pp 35–44
21. Dem. UP (1994) Demographics update. J Vis Impairment Blindness, 88(part 2(1)):4–5
22. Dey AK, Abowd GD, Salber D (2001) A conceptual framework and a toolkit for supporting
the rapid prototyping of context-aware applications. Hum Comput Interact 16(2):97–166
Mobility Technologies for Blind, Partially Sighted …
405

23. Dvir D, Raz T, Shenhar AJ (2003) An empirical analysis of the relationship between project
planning and project success. Int J Project Manage 21:82–95
24. Farcy R (2006) Electronic travel aids and electronic orientation aids for blind people:
technical, rehabilitation and everyday life points of vie. In: CVHI 2006. Kufstein, Austria
25. Farmer LW, Smith DL (1997) Adaptive technology. In: Blasch BB, Wiener WR, Welsh RL
(eds) Foundations of orientation and mobility, 2nd Edn. pp 231–259
26. GD (2006) www.guidedogs.com.au, website for Guide Dogs New South Wales, Australia
27. GDP (2006) www.gdp-research.com.au
28. Gaunet F, Briffault X (2005) Exploring the functional speciﬁcations of a localized wayﬁnding
verbal aid for blind pedestrians: Simple and structured urban areas. Hum Comput Interact
20(3):267–314
29. Gitlin LN (1995) Why older people accept or reject assistive technology. Generations
19(1):41–46
30. Giudice NA, Legge GE (2008) Blind navigation and the role of technology. In: Engineering
handbook of smart technology for aging, disability, and independence, pp 479–500
31. Golledge RG (1993) Geography and the disabled: a survey with special reference to vision
impaired and blind populations. In: Transactions of the institute of British geographers,
pp 63–85
32. Gould JD, Lewis C (1985) Designing for usability: key principles and what designers think.
Commun ACM 28:300–311
33. Gurkan GK, Akan A (2013) VibroCap: a mobility supporting hat for blind. In: Electrical and
electronics engineering (ELECO), 2013 8th international conference on, pp 367–370
34. Harrison R, Flood D, Duce D (2013) Usability of mobile applications: literature review and
rationale for a new usability model. J Interact Sci 1(1):1–16
35. Hersh MA (2009) Designing assistive technology to support independent travel for blind and
visually impaired people CVHI’09. Wrocław, Poland
36. Hersh MA (2014) Deafblind people stigma and the use of communication and mobility
assistive devices. Technol Disabil 25(4):245–261
37. Hersh MA (2015) Cane use and late onset visual impairment. Technol Disabil 27(3):103–116
38. Hersh MA, Johnson MA (2006) Accessibility of PDF documents. AXMEDIS, Leeds
39. Hersh MA, Johnson MA (2010) A robotic guide for blind people part 1: a multi-national
survey of the attitudes, requirements and preferences of potential end-users. Appl Bion
Biomech 7(4):277–288
40. Hersh MA, Johnson MA (2012) A robotic guide for blind people part 2. Appl Bion Biomech
9:29–43
41. Hersh MA (2009b) The application of information and other technologies to improve the
mobility of blind, visually impaired and deafblind people, travel health informatics and
telehealth. In: Mihalaş G et. al. (eds) Selected papers from EFMI special topic conference,
Antalya, Turkey, Victor Babes University Publishing House, pp 11–24
42. Hersh MA (2010) The design and evaluation of assistive technology products and devices
part 1: design. Int Encycl Rehabil. Available online: http://cirrie.buffalo.edu/encyclopedia/en/
article/309/
43. Hersh MA (2011) Participative research with diverse end-user groups: multi-language,
multi-country blind and visually impaired people. In: 18th IFAC congress, Milan, Italy
44. Hersh MA (2016) Travel and information processing by blind people: a new three-component
model. Biomed Eng, University of Glasgow Report, http://web.eng.gla.ac.uk/assistive/pages/
publications.php
45. Hersh MA, Johnson MA (eds) (2008) Mobility: an overview. In: Assistive technology for
visually impaired and blind people, Springer, Berlin
46. Heyes A (1993) Sonic pathﬁnder training manual. Royal Guide Dog Association of Australia,
Kew, Australia
406
M.A. Hersh

47. Hill EW, Rieser JJ, Hill MM, Hill M (1993) How persons with visual impairments explore
novel spaces: strategies of good and poor performers. J Vis Impair Blind 87(8)
48. Holland S, Morse DR, Gedenryd H (2002) AudioGPS: spatial audio navigation with a
minimal attention interface. Pers Ubiquit Comput 6(4):253–259
49. Hoyle B, Dodds S (2006) The ultracane® mobility aid at work training programmes to case
studies. CVHI, Kufstein, Austria
50. Hoyle B, Waters D (2008) Mobility at: the batcane (ultracane). In: Hersh MA, Johnson MA
(eds) Assistive technology for visually impaired and blind people, Springer, London,
pp 209–229
51. IDC (2015) Smartphone
OS market
share,
2015 Q2. http://www.idc.com/prodserv/
smartphone-os-market-share.jsp, Accessed 7.6.2–16
52. Jacobson W (1979) Complementary travel aids for the blind person: the sonic guide used with
a dog guide. J Vis Impair Blind 73(1):10–12
53. Jacobson RD, Kitchin R (1997) GIS and people with visual impairments or blindness:
Exploring the potential for education, orientation, and navigation. Trans GIS 2(4):315–332
54. Joorabchi ME, Mesbah A, Kruchten P (2013) Real challenges in mobile app development. In:
Empirical software engineering and measurement, 2013 ACM/IEEE international symposium
on, pp 15–24
55. Kalloniatis C, Kavakli E, Gritzalis S (2008) Addressing privacy requirements in system
design: the PriS method. Requirements Eng 13(3):241–255
56. Kane SK, Jayant C, Wobbrock JO, Ladner RE (2009). Freedom to roam: a study of mobile
device adoption and accessibility for people with visual and motor disabilities. In:
Proceedings of the 11th international ACM SIGACCESS conference on computers and
accessibility, pp 115–122
57. Kannan B, Kothari N, Gnegy C, Gedaway H, Dias MF, Dias MB (2014) Localization, route
planning, and smartphone interface for indoor navigation. In: Cooperative robots and sensor
networks, Springer, Berlin, pp 39–59
58. Katz BF, Kammoun S, Parseihian G, Gutierrez O, Brilhault A, Auvray M, Jouffrais C (2012)
NAVIG: augmented reality guidance system for the visually impaired. Virtual Reality 16
(4):253–269
59. Kay L (1974) A sonar aid to enhance spatial perception of the blind: Engineering design and
evaluation. Radio Electron Eng 44(11):605–627
60. Kay L (1980) The sonic guide, long cane and dog guide: their compatibility. J Vis Impair
Blind 75(7):277–280
61. Kendell C, Lemaire ED (2009) Effect of mobility devices on orientation sensors that contain
magnetometers. J Rehabil Res Dev 46(7):957–962
62. Kukulska-Hulme A (2007) Mobile usability in educational contexts: what have we learnt! Int
Rev Res Open Distance Learn 8(2)
63. Kumar A, Patra R, Manjunatha M, Mukhopadhyay J, Majumdar AK (2011) An electronic
travel aid for navigation of visually impaired persons. In: Communication systems and
networks (COMSNETS), 2011 third international conference on, pp 1–5
64. Leduc-Mills B, Proﬁta H, Bharadwaj S, Cromer P (2013) ioCane: a smart-phone and
sensor-augmented mobility aid for the blind
65. Lin Q, Han Y (2014) A context-aware-based audio guidance system for blind people using a
multimodal proﬁle model. Sensors 14(10):18670–18700
66. Marston J, Barlow J, Bentzen B, Brabyn J, Gilden D, Miele J, Myer L, Scott A, Simon H
(2009) Considerations in designing research to evaluate wayﬁnding. In: CVHI 2009,
Wroclaw, Poland
67. McGorry S (2000) Measurement in a cross-cultural environment: survey translation issues.
Qual Mark Res Int J 3(2):74–81
68. Mims III FM (1972) Eyeglass mounted mobility aid. J Am Optom Asso 673–676
Mobility Technologies for Blind, Partially Sighted …
407

69. Mishra A, Jain A, Pamecha N (2014) Smart travel alarm–an iOS app on sencha 2.2. Int J Eng
Manage Sci 1(7)
70. NR (2006) www.nurion.net, website of Nurion-Raycal, Station Square Building 2, Suite B,
Paoli, PA 1930I, USA
71. Nayebi F, Desharnais JM, Abran A (2012) The state of the art of mobile application usability
evaluation. In: CCECE, pp 1–4
72. Parette HP, Huer MB, Scherer M (2004) Effects of acculturation on assistive technology
service delivery. J Spec Ed Tech 19(2):31–41
73. Park N (2011) Implementation of terminal middleware platform for mobile RFID computing.
Int J Ad Hoc Ubiquitous Comput 8(4):205–219
74. Park N (2010) Security scheme for managing a large quantity of individual information in
RFID environment. In: Information computing and applications. Springer, Berlin, pp 72–79
75. Pascolini D, Mariotti SP (2011) Global estimates of visual impairment: 2010. Br J
Ophthalmol: bjophthalmol-2011
76. Patel I, Turano KA, Broman AT, Bandeen-Roche K, Munoz B, West SK (2006) Measures of
visual function and percentage of preferred walking speed in older adults: the salisbury eye
evaluation project. Invest Ophthalmol Vis Sci 47(1):65–71
77. Phillips B, Zhao H (1993) Predictors of assistive technology abandonment. Assistive Technol
5(1):36–45
78. Pissaloux E (2013) Visually impaired mobility and ICT supports. In: IEEE signal processing:
algorithms, architectures, arrangements, and applications (SPA). ISSN 2326–0262
79. Pissaloux E, Velazquez R, Hersh M, Uzan G (2016) Towards a cognitive model of human
mobility: an investigation of tactile perception for use in mobility devices. J Navig (in press)
80. Raento M, Oulasvirta A, Petit R, Toivonen H (2005) ContextPhone: a prototyping platform
for context-aware mobile applications. Pervasive Comput IEEE 4(2):51–59
81. Rieback MR, Crispo B, Tanenbaum AS (2005). RFID guardian: a battery-powered mobile
device for RFID privacy management. In: Information security and privacy, Springer, Berlin,
pp 184–194
82. Rodgers M, Emerson R (2005) Materials testing in long cane design: sensitivity, ﬂexibility,
and transmission of vibration. J Vis Impair Blind 99(11):696–706
83. Roentgen UR, Gelderblom GJ, Soede M, de Witte LP (2008) Inventory of electronic mobility
aids for persons with visual impairments: a literature review. J Vis Impair Blind 102(11):702
84. Rudolph (undated) Hybrid mobile apps: providing a native experience with web technologies.
https://www.smashingmagazine.com/2014/10/providing-a-native-experience-with-web-
technologies/, Accessed 19 May 2016
85. Russell L (1966) Travel Pathsounder and evaluation, In Dutton R (Ed) In: Proceedings
conference on the evaluation of sensory devices for the blind, St. Dunstan’s, London, pp 293–
297
86. Salber D, Dey AK, Abowd GD (1999) The context toolkit: aiding the development of
context-enabled applications In: Proceedings of CHI’99, ACM Press, New York
87. Scherer MJ (2002) Editorial, the change in emphasis from people to person. Introduction to
the special issue on assistive technology. Disabil Rehabil 24:1–4
88. Skulimowski P, Strumillo P (2007) Obstacle localization in 3D scenes from stereoscopic
sequences. In: signal processing conference, 2007 15th European, pp 2095–2099
89. Strumillo P (2010) Electronic interfaces aiding the visually impaired in environmental access,
mobility and navigation. In: Human system interactions (HSI), 2010 3rd conference on,
pp 17–24
90. Terlau T (2008) ‘K’ sonar curriculum handbook. American Printing House for the Blind, Inc.,
USA
91. UC (2006) UltraCaneTM. www.soundforesight.co.uk/, Sound Foresight Ltd, UK
408
M.A. Hersh

92. Ulrich I, Borenstein J (2001) The GuideCane-applying mobile robot technologies to assist the
visually impaired. IEEE Trans Syst Man Cybern Part A 31(2):131–136
93. Walraven J (1986) Ergonomics, mechanics and functional aspects of the long cane. In:
Development of electronic aids for the visually impaired, Springer, Netherlands, pp 275–286
94. Ward C (1990) Design for all: consumer needs assessment project year 2. Results of the
second year of a ﬁve year study, 45p
95. Winograd T (2001) Architectures for context. Hum Comput Interact J 16(2):401–409
96. Yusro M, et al. (2013) SEES: concept and design of a smart environment explorer stick. In:
IEEE HSI 2013
97. Zhang D, Adipat B (2005) Challenges, methodologies, and issues in the usability testing of
mobile applications. Int J Hum Comput Interact 18(3):293–308
Mobility Technologies for Blind, Partially Sighted …
409

Co-designing together with Persons
with Visual Impairments
Charlotte Magnusson, Per-Olof Hedvall and Héctor Caltenco
1
Introduction
The importance of involving the persons intended to use a design, already in the
design process leading up to the ﬁnal product or service, is increasingly
acknowledged. This chapter is intended to provide both inspiration and practical
suggestions for anyone interested in designing for and with persons with visual
impairments. The text focuses on co-design, but many of the adaptations and
materials presented can also be used in more traditional design activities, such as
usability testing. The chapter rests on an inclusive mindset. In other words, we
focus on how to expand and enhance existing methods regarding who is involved,
and how to provide means for participation to wider target groups, rather than how
to create “special” methods for “special” users with “special” needs.
The chapter starts with a general background on the design process. This
background is followed by a discussion of human-centred design and co-design,
and why the visual ability of the persons involved in the process matters. We then
present six concrete examples of design activities from our own work. These
examples have been selected to be inherently different, and are intended to serve
both as illustrations of how useful it can be to engage in co-design, and as practical
inspiration for future design activities. We hope to show that there is no need to feel
daunted by organizing design activities together with persons with visual impair-
ments—in fact there is a wide range of possible activities you can engage in
together with your future users. The examples are followed by a discussion of
activities and materials, inclusive design and accessibility and the importance of the
context of use. The chapter ends with a conclusion/take away message.
C. Magnusson (&)  P.-O. Hedvall  H. Caltenco
Lund University, Lund, Sweden
e-mail: charlotte.magnusson@certec.lth.se
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_14
411

2
The Design Process
A typical feature of any design process is that much information about “the
problem” (the persons involved, the way things are used, the surrounding context,
etc.) is missing at the start of the process. And since the way the persons involved
behave, the way tasks are handled and the context typically changes as artefacts
change and new artefacts are introduced, the design process aims at a moving
target. To get to the goal the design work needs to start before all relevant
knowledge is available—the design process itself will be a tool for gathering the
necessary knowledge. In the words of Donald Schön [33–35] one needs to start a
reﬂective conversation with the materials:
There is no direct path between the designer’s intention and the outcome. As you work a
problem, you are continually in the process of developing a path into it, forming new
appreciations and understandings as you make new moves. [34]
In a sense this can be termed “doing for the sake of knowing”. Actions are not
just actions to produce a certain result, but rather acts to inquire into the current
design problem. This type of actions can be used to explore or to experiment—to
physically test your ideas in the world [9]. An important point in Gedenryd’s thesis
is that design is not a purely intramental activity—instead the interaction with
materials (in a broad sense) forms an integral part of the cognitive process.
The designer and the design team is progressively moving along, making
judgements about different responses from the medium—and sometimes discov-
ering completely unexpected things. Schön uses the term backtalk for this type of
surprising discoveries—the materials talk back at you telling you things you did not
know [34].
Donald Schön introduces the concepts “reﬂection in action” and “reﬂection on
action” in his classic book The Reﬂective Practitioner [33]. Reﬂection in action is
different from just knowing how to act, how to do. Reﬂection in action is closely
linked to an element of surprise. A person responds to an unexpected outcome
(good or bad) by thinking about what he or she is doing, in a way that inﬂuences
further doing. Examples given are a group of jazz musicians who note and respond
to surprises introduced by other players, or a designer conducting a set of experi-
ments through drawing a series of sketches and responding for example to unex-
pected constraints posed by the environment.
In some situations, the person instead responds to the surprise by stopping to
think about what happened. In this case it is a matter of reﬂection on action.
A member of the design team may pause to think back over what has been done in a
project and exploring the understandings that were brought into the process and
framing new theories [34]. Of course one may also reﬂect on practice. This is a
higher level type of reﬂection which involves patterns of behaviour.
If the ﬁnal product is to be usable, then the persons who are expected to use it, as
well as the usage have to be part of the described processes of reﬂection and action.
412
C. Magnusson et al.

Furthermore, the cost of implementing changes becomes larger, the closer to the
ﬁnished product we are getting in the design process. Thus, we would like to
involve the right persons and contexts in the process right from the very start.
3
Human-Centred Design
In the traditional waterfall model for engineering design [31], the basic idea is that
you ﬁrst ﬁnd the requirements for the future system, then you design and build it,
and ﬁnally you evaluate it to see that it fulﬁls the initially speciﬁed requirements.
Unfortunately, user requirements are typically hard to ﬁnd out. There is usually
much more to understanding what is wanted, needed and desired, than to ask people
what they want. Confronted with the question “what do you want,” most people
will actually answer with a question of their own: “what can I get?”. It is also very
difﬁcult to know surely exactly what one wants in a speciﬁc situation without being
able to try it out. In addition, the situation of use will be changed in the presence of
new artefacts. Thus the persons who will be the future users of the product or
service, as well as the context of use, need to be continuously involved. It is against
this background the user centred or human-centred design has been developed.
Human-centred design acknowledges that design is iterative, and that the require-
ments need to be updated as the design process progresses towards a ﬁnal product.
Human-centred design is currently well established. There is even an ISO
standard (ISO 9241-210:2010 Human-centred design for interactive systems). The
terms user centred and human-centred are often used interchangeably, but since
“user” limits the role of a person to that of a user we suggest “human” is more
appropriate—most persons are much more than just a “user”. Technology use is
normally only a part of an activity and we cannot generally expect that the goal of
the activity is to use technology. We should be designing for people, not just for
users. Despite this we will (for brevity) in the following continue to use the term
“user” when referring to the person who is expected to make use of some current or
future technology.
As deﬁned in ISO 9241-210, Human-centred design is a process where the
components “understand and specify the context of use”, “specify the user
requirements”, “produce design solutions to meet user requirements” and “evaluate
the designs against the requirements” are iterated until there is a good solution that
passes the evaluation. Two key concepts are also deﬁned:
Usability: the extent to which a system, product or service can be used by
speciﬁed users to achieve speciﬁed goals with effectiveness, efﬁciency and satis-
faction in a speciﬁed context of use.
User experience: a person’s perceptions and responses resulting from the use
and/or anticipated use of a product, system or service.
There are many ways to involve users and their needs, wishes and dreams in a
design process [29]. The approaches and techniques for user involvement are so
numerous that [17] explicitly states that user involvement is a vague concept. In
Co-designing together with Persons …
413

some approaches users take active roles in different design activities, while in others
they are providers of information, commentators or objects for observations. Users
can also be involved directly in person, or indirectly through guidelines, checklists,
personas, scenarios, simulations or other tools/techniques that capture indirect
information about the user and the context of use. Thus, user involvement can be
both direct and indirect, while the level of user involvement can vary from infor-
mative, through consultative to participative.
As is stated above, the importance of involving users in the design process is
well established. And not only users—the context of use is just as important.
According already to Lueg and Pfeifer [19] human cognition should be considered
to be emergent from the interaction of the human with the environment, i.e. the
current situation the human is involved in. Thus, to obtain working designs one
needs not only to involve users in the design process, but also involve real usage
situations as much as possible. Laboratory tests may provide information about how
a user can use a device or a system, but one can never be sure about how valid this
information is for real contexts of use.
4
Participatory Design and Co-Design
Participatory design and co-design is a subset of human-centred design where the
design is done with the users, rather than for the users. The mindset in
co-design/participatory design is that users are not just sources of information—
they are active participants in the design process, contributing knowledge and
perspectives that would not be reached otherwise. This mindset is crucial, since it
will inﬂuence not only the way you think of your users, but also the way the whole
design process is carried out: the way you design different design activities and how
you work together with the users. The overall mindset of co-design is thus a
necessary component for this type of design projects.
An often cited early participatory design project was UTOPIA [5] which in the
early 80s set out to design a graphic workstation for a newspaper together with the
newspaper graphic workers. In this project the graphic workers were seen as
co-designers, and mutual learning between the researchers and the workers was an
explicit activity in the project. The participatory design toolbox has been extended
since UTOPIA, but central to this approach is low technology prototyping and
design sessions or workshops together with the intended users of the future tech-
nology. While participatory design has strong political roots, and was originally
developed for design within a workplace environment, co-design can be used as a
more general term for design activities involving future users as co-designers [32].
A
common
type
of
design
activity
in
co-design
is
the
design
workshop. Workshops are hands-on sessions where small groups of persons (end
users, professionals, etc.) work creatively together. The key is the group work,
allowing for a creative interaction between group members that involves elements
of brainstorming (expressed ideas lead to new ideas). At a workshop one can work
414
C. Magnusson et al.

with concepts, technologies and practical examples. Normally a workshop consists
of activities ranging from discussion and idea generation to creation and imple-
mentation of simple prototypes [24].
It should be noted that co-design is not the same as design done by the users. The
observations “Users are not Designers”, as well as the counterpart “Designers are
not Users” [28], are still relevant. What we strive for is for the users to be active and
creative contributors in the design process together with the specialists in the design
team. Contributions may take the form of actual designs or creative input for
design. Co-design activities typically also provide an improved understanding of
the current design “problem”.
5
Co-designing Together with Persons with Visual
Impairments
Looking at existing methods for co-design and participatory design, it is obvious
that there is nothing that per se makes these less suited for use with persons with
visual impairments. The difﬁculty lies in the fact that many standard design tools
like Post-it notes, drawings, cards, videos and lo-ﬁpaper prototypes are visual tools.
Thus, anyone working together with persons with visual impairments typically
needs to put in extra work in creating or adapting activities in order to make them
suitable for non-visual use. In many cases adaptation can be straight forward like
using tactile prototyping materials instead of pen and paper or using NFC or RFID
tags to sonify objects. Sometimes the translation may seem simple but can turn out
to require more work than initially expected—an example is lists of words relating
to experience, e.g. happy, sad, interesting, boring. Such lists often contain words
like light, bright or dark that refer to visual impressions, and a proper adaptation
should include making sure words relating also to sound and touch experiences are
included.
To make co-design practices more concrete, in the following we present six
practical examples of co-design activities that have been used together with persons
with visual impairments. These examples are taken from our own work. The
examples have been selected to show a wide range of activities and materials. The
ﬁrst example is a single lo-ﬁdesign activity using LEGO bricks, the second
example is a more elaborate study involving focus groups, a diary and a hands-on
prototyping workshop. The third study is a more implicit design activity where
participants design their system by asking questions to a person impersonating a
mobile navigation system. The fourth example is again a combined study aimed at
understanding sound and touch preferences among visually impaired children. This
example is longer, and is divided into the subsections questionnaires, sound and
touch workshops and sound story workshop. The ﬁfth and sixth examples are a
game creation game creation workshop and a gesture creation workshop carried out
together with visually impaired children.
Co-designing together with Persons …
415

Example 1: VR Prototyping Using Functional Prototypes and LEGO Bricks
This example comes from a master thesis project at our department [37] aimed at
exploring different ways to navigate a haptic VR environment non-visually through
touch and sound. Virtual Reality (VR) is usually visual. By using interactive haptics
like the Geomagic Touch (formerly the PHANToM), it becomes possible to make
VR environments that can be used also through touch.
It is, however, generally difﬁcult for end users to contribute ideas to the
development if they have no experience of the technology to be used. While sighted
persons have usually seen VR (or at least images or videos of it), and thus have a
general idea of what it is and how it works, the situation can be quite different for a
blind person. When dealing with novel technology unfamiliar to the user, one
generally has to show working examples in order to provide the persons with an
idea of what the technology is and how it works. At the same time such examples
risk inﬂuencing/guiding the users and cannot generally be used as the sole basis for
the open-ended mindset needed for innovative designs. In this example, the solution
used for this dilemma was to combine working examples of VR environments with
lo-ﬁexploration using LEGO bricks, in order to have lo-ﬁdesigns that could be
accessed through touch.
At a workshop together with persons with severe visual impairments, partici-
pants were ﬁrst allowed to explore some existing PHANToM environments. After
this a simple room environment built using LEGO was presented, and a LEGO doll
with a cane representing the avatar (Fig. 1) could be manipulated in order to explore
different navigation alternatives. This way, the workshop participants could discuss
how it should be possible to affect the avatar in the game world to accomplish
movement, change of direction and interaction with objects.
Through the LEGO model (with the initial technology exploration as a basis),
the participants were able to provide suggestions that could be incorporated in the
ﬁrst functional prototype.
Fig. 1 Lo-ﬁVR environment
with an avatar using a cane
416
C. Magnusson et al.

Example 2: Hands-on Design Workshop
This example comes from the HaptiMap EU project (www.haptimap.org), and was
published in [23]. The study was part of the early work in the project which was
aimed at eliciting user requirements from different user groups for navigation and
map software. The study consisted of three parts: a focus group/test, a diary study
and a design workshop where the users envisioned new kinds of interaction with
mobile navigation systems by building and demonstrating low-ﬁprototypes.
The initial focus group consisted of two parts—one where the participants sat in
a conference room and discussed navigation (strategies, problems, etc.), and one
where the participants discussed while on the move, testing an existing navigation
system. The difference in the discussion between the indoor stationary setting and
the outdoor mobile context was considerable—indoors the discussion revolved
around more general and high level observations on navigation while the outdoor
mobile context triggered details and comments in a way the indoor stationary
context did not.
After the initial focus groups, the participants were asked to keep a simple diary
answering a few predetermined questions each day during one week (visually
impaired participants got the diary electronically). Although the diaries provided
some information, their most important function turned out to be priming: to get the
participants to start thinking actively about navigation before the design workshop.
Since the purpose of the workshop was to create a mobile navigation system, the
props were selected and also pre-designed to simulate common devices. The par-
ticipants had access to models of mobile devices glued together from rubber
material, as well as suitable materials for mimicking earphones, wristbands, cables,
etc. (Fig. 2). When the groups had constructed their prototypes, they presented
them to the researchers and then conducted a “simulation walk” with the device.
The simulation walk was made indoors, but the users were instructed to envision
outdoor use. The simulation walk was made to demonstrate the functionalities of
the prototype device in a live setting.
The sequence design used was found to work well. The inclusion of existing
technology in the focus group discussions provided technological reference [15] for
the users, while the scenario walk provided the necessary context (during the walk
comments were made that were not triggered in the preceding discussion). The
Fig. 2 Lo-ﬁmobile devices and materials
Co-designing together with Persons …
417

diaries provided some information and also prepared the users for the ﬁnal work-
shop, although it was suggested by some users that the diary should have been kept
over longer time. The workshop ﬁnally, showed that also for non-visual interaction
design, lo-ﬁworkshops can be a useful tool for involving users in the design
process. The workshop and discussion turned out to be a fruitful arena to get into
detailed questions about the functionality of the system.
During the simulated walk, both users and researchers were in the context of a
way-ﬁnding task, however artiﬁcial, which triggered questions and also made more
plausible that both parties were talking about the same thing.
A caveat based on observations at other similar workshops, is that it matters how
lo-ﬁworkshops are presented to the participants. Unless the activity is framed
properly, it may be considered “childish” or a “kindergarten” kind of activity.
Another caveat is that moderation may be needed if some persons in a group
becomes too dominant (just like in a focus group). Thus, it is recommended that
several persons from the design team are present.
Example 3: The Mobile Oracle
The Wizard of Oz (WOZ) type of prototyping, where a human fakes the func-
tionality of the—not yet developed—technology, makes it easy to test auditory
interface prototypes and can thus be useful when developing technology aimed at
persons with visual impairments. In the “Mobile Oracle” [20] WOZ is combined
with the information on demand technique [30] (Fig. 3).
The user was instructed to request information from a person impersonating a
mobile service. The person was instructed to ask for the information when he/she
Fig. 3 Pictures from the test environment (a mall)
418
C. Magnusson et al.

felt it was needed in order to perform a navigational test task. To further strengthen
the idea of asking for information, the person impersonating the service was called
an “oracle”, since oracles are persons who you come to with questions. The test was
carried out in November, and was set inside a shopping mall. The test task was to
shop three speciﬁed items in different stores.
While using a mall for the test was useful since it made it possible to run the test
even if the weather outside was bad, a mall as a testing environment has its own sets
of problems. Firstly, there were restrictions on how the test could be documented—
video ﬁlming was not allowed and we were not allowed to take photos of the
ceiling. Sound recordings were fortunately allowed. At the pilot test an observer
shadowed the test pair (the oracle and the test participant) taking notes, but this
attracted too much attention and the note-taking was left out during the real test.
During the real test a mobile phone held by the “oracle” recorded the dialogue. As a
precaution we visited all shops beforehand to explain to them what it was we were
doing.
The main outcome of this study was the questions asked by the test participants.
These turned out to be very informative and provided rich input for the design
process. The questions were also grouped, and the resulting overall categories
“Content
overview”,
“Spatial
layout”,
“Direction/route”,
“Distance”,
“Notiﬁcation/prompts”,
“Conﬁrmation”,
“Content”,
“Recommendation”,
“Memory”, “Time” and “Capability of the device” agree in general with what was
found in earlier studies on navigation conﬁrming the validity of the “Mobile
Oracle” approach.
Both persons with visual impairments and sighted persons participated in the
test. Many of the questions asked were quite similar, but there was a clear tendency
for participants with visual impairments to ask for more details (e.g. distances in
metres).
Example 4: Sound and Touch Preferences
This study [21] was done within the framework of the ABBI project. This project is
aimed at developing a wearable device (a bracelet) intended to support sensory
motor rehabilitation of children with visual impairments. The selected example is
an exploratory study of aesthetic/hedonistic preferences for sounds and touch
experiences among visually impaired children. Mixed methods were used: ques-
tionnaires, workshops (Fig. 4) and ﬁeld trial using a mobile location based app for
story creation.
The reason for the mix was that preferences may vary depending on context (e.g. a
scary sound may be acceptable in a movie, but disliked when you hear it in real life).
This led us to use methods combining questionnaires and rating of sounds (“is this
sound pleasant or unpleasant”) with more creative activities involving creating
interesting “toys” and location-based sound stories. The rating of sounds and the
creation of interesting “toys” were performed at three workshops at the Chiossone
Institute in Italy (one workshop with habilitation personnel and two workshops with
children with visual impairments). The creation of location-based sound stories took
place at aninvited activity at asummer camp for visually impaired children in Sweden.
Co-designing together with Persons …
419

This method mix included both activities intended to capture direct preferences
—your immediate reaction in terms of pleasure/dislike upon hearing a sound—but
also more activity related preferences: how do you react to sounds and tactile
experiences when these are assigned to a toy, and what kind of sounds would you
like when playing and telling stories?
In the following we describe the different methods in more detail.
5.1
Questionnaires
To get initial information on sound preferences—but also to get the children to start
thinking about their sound preferences—a short questionnaire was distributed and
digitally and answered before the three workshops in Italy. The questionnaire
contained the following questions:
• Please describe shortly a sound you think is pleasant to listen to—and if possible
add a comment on why you like it.
Fig. 4 Sound and touch workshop design exercise
420
C. Magnusson et al.

• Please describe shortly a sound you think is unpleasant to listen to—and if
possible add a comment on why you don’t like it.
• Please describe shortly a material you think feels pleasant to touch—and if
possible add a comment on why you like it.
• Please describe shortly a material you think feels unpleasant to touch—and if
possible add a comment on why you don’t like it.
5.2
Sound and Touch Workshops
At the workshops we explored what kind of sounds and materials the children
thought were pleasant or unpleasant. The workshops were designed with two
exercises:
1. Exercise 1: a focus group type exercise where 71 different sounds (32 natural
and 39 synthetic) were played and the children could raise their hands if they
liked/didn’t like them (or do nothing if they felt neutral). Hand raising was
selected since it allowed the children to answer simultaneously without
inﬂuencing each other too much (since the children had visual impairments,
their ability to see what the others were doing was limited).
2. Exercise 2: a design exercise where the children were asked to combine a tactile
object and a sound to make a nice, cool or interesting “toy”. Children could
associate one of the 20 sounds stored in an NFC tag and play it back by touching
the material with a smart phone, or by vocalizing their own sounds.
After a pilot workshop it was decided to change the order of the sounds in
exercise 1 so that the natural (potentially more interesting) sounds were played ﬁrst.
5.3
Sound Story Workshop
At this workshop we used a location-based app to allow visually impaired persons
to create and experience outdoor story trails. We had been invited by the persons
arranging a summer camp for children with visual impairments to arrange a fun
location-based activity, and decided this could also be a good opportunity to gather
more information about sound preferences in a more realistic situation. In order to
do so, the natural sounds from the sound and touch workshops were added to the
app, so that for each story GPS point location one pre-recorded natural sound could
be assigned/played alone or together with a voice recording. A few synthetic sounds
were also added: a long square and sine tones, a low synth, white noise, piano and
high and low ping sounds. The selection focused on timbre, and included both
clean/bright sounds and mufﬂed/noisy ones. The app was an android app, and had
only a single trail.
Co-designing together with Persons …
421

The activity was designed as group work in a series of one hour slots where 8
children divided into 3–4 groups of 2–3 persons would ﬁrst get a joint short
introduction and try an example trail. The groups in the ﬁrst time slot tried an
example created by the researcher while following groups tried a part of one of the
trails created by the groups in the previous time slot. After this they created their
own trail, tested it, and if there was time swapped phones with another group and
tried theirs. The researcher together with persons working at the camp were
available as support in case of technical problems.
Due to the nature of the activity (a summer camp) the activity could not be
tightly controlled and we were unable to ﬁlm/record or gather personal data. The
only material saved was the created trails, which were copied and stored after each
slot.
The results from these activities showed that preferences vary, indicating a need
for ﬂexibility and personalization. Even so, sine or square tones were less liked,
while sounds with more harmonics (wider frequency content) were in general better
appreciated. Moreover, long continuous sounds were in general considered more
unpleasant than short or repetitive sounds. Birds and water sounds were generally
popular, while sharp, sudden and loud sounds were disliked—but the creative
exercise and the story telling showed that also the unpleasant or drastic has a role to
play. The sound story creation exercise furthermore allowed us to identify several
roles for the sounds in storytelling and potentially during play, that needs to be
considered: facilitate creation, sound effects, story elements, ambience and
inspiration.
For tactile materials, soft, furry and hard materials were appreciated, while sharp,
rough, hard, sticky, rubbery and runny materials were less well liked. Thus,
materials common in toys and play materials like ﬁnger paint, play dough, balloons
and rubber toys may not always be appreciated by children with visual
impairments.
The methods used generally worked quite well. The few questions in the
questionnaire generated a lot of useful results, as did the sound story exercise and
the creative workshops. The tangible representation of the sounds allowed partic-
ipants to physically manipulate and select different sounds (although moderator
support was provided to keep the apps on the phones working). With tangible
objects it was possible to sort sounds in piles and also to revisit and reﬁne selec-
tions. Additionally, having tangible sound objects allowed the children to physi-
cally associate a sound with a material—a design which appeared well suited to the
participating children. The activity that yielded the least interesting results was the
hand raising exercise. It took quite a long time to go through all the sounds, and
given that the overall result was basically “preferences differ” and that we got the
essentially same information from the initial questionnaires, it is questionable if this
was time well spent.
Example 5: Game design workshop
This workshop, which just as the previous example, was performed as part of the
ABBI EU project [25] illustrates an activity which served both as a test of the
422
C. Magnusson et al.

technology and as input for how to use the technology. Before this workshop was
organized, the ABBI sound bracelet had been tested in ad hoc individual rehabil-
itation, but since this type of device also holds potential for more social and playful
activities we wanted to explore how the ABBI bracelet could be used in social
games and play. Three workshops were organized: two together with a group of
children with visual impairments at the Chiossone Institute in Italy and one together
with children with visually impaired children together with their sighted siblings
and families in Sweden.
The workshops in Italy started with an exercise where all participants had an
ABBI bracelet. As a ﬁrst “priming” exercise the bracelets were assigned sounds in
pairs, and the task was to ﬁnd the person who had a bracelet making the same sound
as your own bracelet. After this the children were divided into groups and asked to
design games of their own using the ABBIs (Fig. 5).
At the workshop in Sweden the initial task also included playing the game
“statues”, created by one of the groups in Italy, where the goal was to walk as far as
you could without the ABBI making a sound. After this introduction, children and
family members were again divided into groups and asked to design games that
involved the ABBI bracelet. All workshops ended with the different groups
demonstrating their games.
The workshops resulted in a range of games which have later been used in the
rehabilitation training, and clearly showed how useful it is to involve children in
creative exercises—several new types of games were created that the design team
had not previously considered. This exempliﬁes that co-creation activities work
well not only with sighted children [4], but also with visually impaired children. At
the same time the activity provided an important usability test of the technology,
and key usability problems were identiﬁed: LED lights on the device were working
in a non-standard way, a mute button and a simpler way to change the sound of the
device was needed. Important points to consider when organizing this kind of
activity with prototype technology:
• Make sure there is a backup if the technology fails. In our case we had made an
app that allowed a phone to work as a replacement ABBI.
Fig. 5 ABBI bracelet
together with a mobile device
used to control the device
settings. By default the ABBI
is quiet when it is stationary,
and only makes sounds when
it is moved
Co-designing together with Persons …
423

• Think about how you present tasks to allow everyone to access the information,
but also about how participants can share what they do while working together
or when demonstrating their work.
The selection of sounds again conﬁrmed the results from the initial studies [21],
which showed the importance not only on nice and pleasurable sounds, but also of
unpleasant or drastic ones. Running around “exploding” at every step, sounding
like an elephant, like a mosquito or making rhythmical musical sounds is quite a
different experience compared to walking around making a “ping” at each step. At
the same time, we saw that it may occasionally get noisy, and in the post activity
interviews at Furuboda a common suggestion was to provide a mute button (input
for the usability evaluation).
Moving on to the game creation, as expected we saw that many of the created
games were of a “ﬁnd the sound” type. Complementing this was the “avoid the
sound”. Both these appear in the game “hide and seek”: the chaser tries to ﬁnd the
sound source, whilst the person being chased tries to avoid the sound of the chaser
(this game was designed and played by the two blind participants in Fig. 6). An
interesting use of the ABBI was to rely on the fact that it was silent when it wasn’t
moved. This way one can create “sneaking” games, where the goal is to avoid being
noticed. These kind of games had not been discussed in the ABBI design team prior
to the workshop, and it can be noted that “non-activation” does not feature in the
identiﬁed related work [1, 2, 16, 38].
Example 6: Gesture Design Workshop
This ﬁnal example also comes from the ABBI EU project. Since the ABBI device is
the size of a matchbox and lacks a screen, a smartphone app is used to control the
device. The app allows the user to change volume, sound parameters, etc. The
device has an accelerometer which is used to control when sound is played (to save
battery the ABBI is silent when it isn’t moved). A shaking gesture is used to wake
the device up when it has entered sleep mode, but apart from this gestures are
currently not used to interact with the ABBI. It has been suggested that gestures
Fig. 6 Left Initial ﬁnding exercise. Blindfolds were used to put everyone on an equal footing.
Right Two blind children, probably for the ﬁrst time in their lives, playing tag/hide and seek
424
C. Magnusson et al.

designed by end users are easier to remember [27], and thus we decided to organize
gesture design workshops together with our visually impaired end users at the
Chiossone institute in Italy. Two workshops were organized, one with blind chil-
dren (5 girls aged 12, 12, 14, 15, 17) and one with children who had low vision (3
boys aged 8, 11, 15 and 3 girls aged 10, 13, 15).
At the workshop the participants had access to ABBI devices. One of these was a
new version of the ABBI which also had a small button. This ABBI was sent
around the table so that all the children could feel it. The children were told that
would like their suggestions for gestures for the different functions we would be
presenting. Additionally, we explained that a “gesture” could be anything from
pressing the button on the device, shaking the device, tapping on the device, waving
it about or manipulating it in some other way. We then explained the functions one
by one and asked the participants for ideas for each presented function. The
functions investigated were mute/unmute, change the volume, change the sound,
change the sensitivity and change between pre-programmed settings. The new
ABBI ﬁrmware allows four different sets to be saved, and we wanted to investigate
how one could interact with this functionality. The workshops were video recorded,
transcribed and translated to English.
The children at both workshops came up with several suggestions for the
mute/unmute, change volume and sound functions. The sensitivity—how quickly
the ABBI responds to movement—and the saved settings turned out to be more
abstract and the children found these more difﬁcult.
In general, the children preferred making gestures on the device to making
gestures with the device. Gestures shown on the device were mainly tapping and
sliding. This was a conscious decision in the ﬁrst group where one child commented
that bigger gestures could cause accidental activation: if the movement is too big it
will be difﬁcult to be used in everyday life as it could make it sound when we don’t
want it and that control gestures are different from the movements you make during
rehabilitation: but then it should be a movement that we don’t do all the time. In the
second group there was no discussion about this—in fact the discussion turned
almost immediately to hardware buttons/wheels. Still, also this group suggested
tapping and sliding gestures for sound changes and there was even one tentative
suggestion put forward that shaking could be used, although the discussion
immediately moved on from there and the suggestion wasn’t elaborated on.
There is a clear difference between the groups, where the ﬁrst group provide
more gesture ideas while the second group quickly zooms in on adding additional
hardware. Our interpretation is that this has less to do with the visual ability of the
children involved, and more is due to age and mobile phone experience (some of
the younger children did not use a smartphone). In fact one of the children explicitly
refer to the smartphone when discussing rotational gestures: “we could use the
rotatory movement as in the smartphone”.
An observation made during the test was that while it was quite easy to discuss
and demonstrate gestures, it turned out to be quite hard to share them with the
whole group. When needed the test leader would demonstrate a gesture to all
children in the group, showing it to one child at a time. The original workshop
Co-designing together with Persons …
425

design had involved the children working in pairs where this would have been less
of an issue, but it was decided for practical reasons at the last minute to have the
workshop more as a joint brain storm/focus group. The children were still quite
good at explaining what they meant, but it is possible that this also skewed the
discussion in favour of gestures that were easy to explain. Given that the gestures
implemented should be both easy to explain as well as easy to remember, this is not
necessarily something negative—but it is a bias one needs to be aware of.
This workshop provided the design team with useful design suggestions, and is
an example of the involvement of end users also in more concrete design work later
in the design process.
6
The Overall Challenge Lies in the Activities
and Materials
The above examples were selected to range over a variety of different activity
designs and contexts. All involve some element of creative work—in the mobile
oracle study the participants implicitly designed their navigation system through the
questions they asked, while the other examples involve explicit design activities.
While it is easy to ﬁnd papers on co-design with children [3, 11, 14, 40] and adults
[32, 41]—the cited works are but few of all the studies that exist, and there is even a
journal devoted to co-design/co-creation: CoDesign: International Journal of
CoCreation in Design and the Arts—co-design work that involves persons with
visual impairments are fewer. In addition to the above mentioned work, another
example is [26] where the authors worked together with children with visual
impairments to design games. The work involved hands-on prototyping of toys as
well as monsters and for the lo-ﬁprototyping clay, ﬁxed design elements (e.g.
buttons and dials) and different fabrics/ﬁllings were used.
The examples in this text provide ample illustration of the fact that there is
nothing per se, that prevents the involvement of persons with visual impairments in
co-creative design activities. That having been said, there are some practicalities
which anyone working together with persons from this group need to consider:
• Consider how you can support the sharing of information between participants
during the exercise.
• Also consider how information before, during and after the activity is made
accessible to the participants.
• When asking people to be creative it often helps if they can think about the
“problem” beforehand. This is true for all users, but when dealing with persons
with visual impairments you need to make sure also the priming information or
activity is accessible.
• Provide multiple means for people to engage with and participate in the design
activities, for instance by working with several modalities.
426
C. Magnusson et al.

• Make sure you have a plan B for failing technology. Again you need to think the
plan through so that it works for your intended users.
• A speciﬁc consideration if you organize activities at your lab is the question of
transportation. As a sighted person it can sometimes be hard to describe how to
get to different places in a way that suits a person who has a visual impairment.
• If you need people to sign papers, think about how this is done. It is always
recommended to send information beforehand to participants, but this becomes
even more important if participants are not able to read printed text. By sending
information electronically beforehand the person will be able to access it using a
screen reader. If your participants can read Braille (this is not always the case) it
can be a good idea to also have some braille printed information.
As we see from this list, the design of the activity, and the materials need special
consideration. For lo-ﬁprototyping different types of tangible materials (LEGO,
rubber, cardboard, pipe cleaners, clay, fabric, string, etc.) can be used. Having a
human faking functionality (Wizard of Oz) is often well suited also for non-visual
interaction. Depending on the task it is commonly useful to prepare some
“ready-made” materials—if the design should involve buttons, sliders or dials these
design elements can be provided, or if the participants are to design for some
speciﬁc technology models or examples of this technology should be made
available.
Although our examples deal with co-design, the above materials can also be
used when asking users to test design concepts generated by the design team. They
can also be used to allow users to provide feedback. For example, LEGO has been
used for evaluation of maps. In [13] an auditory map was tested by visually
impaired persons. When testing how a person understands and remembers a map, it
is quite common to ask the person to draw the map from memory after testing the
system. If the user is visually impaired this no longer works, and one solution to this
problem was to allow the person to use LEGO blocks instead to replicate the map.
Since, as we say in the title of this section, the challenge lies in the activities and
the materials, there is really nothing that in principle prevents the involvement of
persons with visual impairment in the design of products and/or services aimed for
the general public. If care is taken when designing the activities and accessible
materials are used, co-design can be made inclusive. In fact, two of the examples
above were explicitly made in an inclusive setting and participants involved also
persons who were sighted and elderly: Hands-on design workshop and The Mobile
Oracle. This leads to our next discussion topic: inclusive design and accessibility.
7
Inclusive Design and Accessibility
When it comes to design intended to be used by persons with disabilities, there are
basically three approaches. You can aim for an inclusive design that can be used by
“everyone”, you can design inclusively for a situation where an assistive device is
Co-designing together with Persons …
427

used, or you can custom make a design (or an assistive device) for a speciﬁc user
group. While custom-made designs are speciﬁcally designed to ﬁt the requirements
of a speciﬁed user group, an inclusive approach aims to generate solutions that can
be used by a wider range of users.
Inclusive design, universal design and design for all are terms that are often used
interchangeably, although the terms have different origins and are also used more
frequently in different parts of the world. Inclusive design comes from the UK and
is deﬁned as “products, services and environments that include the needs of the
widest number of consumers”. Thus inclusive design is, as the name says, quite
inclusive and goes beyond older and/or persons with disabilities to include also
other marginalized or excluded groups. Universal design, on the other hand,
originated in the USA and has traditionally a strong focus on the built environment
and on persons with disabilities. Design for all ﬁnally, is closely related to both the
previous terms. Although it started with a focus on persons with disabilities, the EC
describes it as ensuring that environments, products, services and interfaces work
for people of all ages and abilities in different situations and under various
circumstances.
An important advantage with an inclusive design philosophy is of course that
you get technology and environments that does not exclude or stigmatize persons or
groups of persons. On the other hand, the wide scope makes it unclear “who the
user is”. Since it could be anyone, deciding who to involve and how it is done
becomes an issue. Thus, human-centred design in this context can be quite chal-
lenging. In her thesis [8] discusses how to design inclusively and identiﬁes 13
elements of inclusive design:
• Based on a user (human) centred process
• Organizational commitment towards and knowledge about inclusive design is
necessary
• The approach must be holistic, interdisciplinary and context-driven
• Focus on a variety of users and usage contexts
• Involve diverse users early on and throughout the development process
• Facilitate communication between user and developer
• The process must be able to handle different stakeholders, conﬂicting require-
ments and changes in requirements
• Ensure conformance to accessibility standards and interoperability with AT
• Focus on a combination of usability and accessibility in context
• Method triangulation: Use more than one method in user research
• Consider using multimodality
• Identity management mechanisms (security, passwords, etc.) need to be
inclusive
• Personalization and adaptation may be used to achieve ﬂexibility
When designing activities and materials for user involvement one needs to take
common assistive devices into account. Persons with severe visual impairment or
blindness typically use the cane (and/or a guide dog) to be able to navigate and
428
C. Magnusson et al.

avoid obstacles. To access computers, tablets and mobile phones, a screen reader is
used. The screen reader can provide output either through synthetic speech or a
braille display. Different types of marking systems are also used—e.g. braille
printed stickers or more advanced RFID-based tags and reading pens. Persons with
low vision may use different types of zooming or enlarging technology.
It is important to remember that not all persons with visual impairment/blindness
are skilled at reading braille, and that the majority of the persons with visual
impairments are in fact not blind, but actually have use of their vision. Depending
on the braille reading skills of a person with visual impairment, sending information
and informed consents via email, allowing the person to access the information via
the screen reader, may be more accessible than providing printed braille text.
Although one can never be certain of which kinds of assistive technology potential
users will have, designs aimed at persons with visual impairments need to be
designed to work with standard assistive devices. In fact, the ISO 26800 deﬁnition
of accessibility explicitly includes assistive technologies:
The extent to which products, systems, services, environments and facilities can be used by
people from a population with the widest range of characteristics and capabilities to achieve
a speciﬁed goal in a speciﬁed context of use
NOTE 1. Context of use includes direct use or use supported by assistive technologies.
As we see in this deﬁnition, context of use is a part of accessibility, as well as of
inclusive design, usability and the general user experience. In the following section
we will discuss this more in detail.
8
Context of Use
What a person can or cannot do is dependent on the human and artefactual envi-
ronment and the interplay between the these. Thus the human and artefactual
context plays a key role in determining the success of a design. Simply put, it
matters where, when and with whom activities for user involvement take place. The
location dependence was seen quite clearly in the hands-on design workshop
example, where focus group interviews sitting down indoors and walking outside
yielded quite different results. We do not suggest that all design activities need to
take place in a realistic context, but it is imperative that at least some do.
It should be noted that the ofﬁce environment, where most of the development
and much of the discussion and the design decisions take place, often becomes an
implicit and unarticulated reference context for the design. Ofﬁce spaces are typi-
cally fairly quiet, organized and predictable in a way many other environments are
not. Using the ofﬁce environment as a reference is no real problem if the context
designed for is similar to an ofﬁce, but as soon as the intended context of use differs
signiﬁcantly from the ofﬁce type environment this becomes problematic.
Another implicit design reference is the development situation itself. While
much design work takes place during workshops, brain storms and other design
Co-designing together with Persons …
429

activities, as is described earlier in this text, important design decisions are usually
also taken by a developer sitting at a desk writing code or constructing hardware. In
this situation the developer focuses fully on the artefact, and may be tempted to
favour solutions that work well when the user similarly is able to focus fully on the
product, but which may work poorly (or not at all) in a context where the user focus
needs to be on what they are doing and/or what happens in the environment.
Thus, it becomes important to articulate and reﬂect on the context of use and to
consider how one can bring it into the design process. A thought structure that can
be helpful in this process is the “Activity Diamond” introduced by Hedvall [12].
The Activity Diamond can be used to highlight the cultural mediation of (dis)
abilities and based on Cultural–Historical Activity Theory (CHAT).
CHAT deals with “the individual in society” and “society in the individual” and
how the two co-evolve over time, with contradictions and tensions as the main
sources of development.
It has its origins in Russia and the psychologist L.S. Vygotsky’s works.
According to CHAT, humans and their actions cannot be understood without their
mediating cultural tools [7, 18, 39]. Engeström [6] introduced the activity system
model, which enables simultaneous analysis of the individual perspective and the
collective perspective, portrayed as several simultaneously present activity systems
[6]. Targeting the disability ﬁeld, [12] has further developed Engeström’s activity
systems model into The Activity Diamond (Fig. 7). The Activity Diamond is a
conceptual model aimed at capturing and describing how humans, artefacts and
nature mediate and thus inﬂuence, i.e. propel or impede, what a person (subject) can
do (object). The desired outcome directs the activity. The system is situated [36] in
time and place, i.e. in the context of use.
– The subject in the model is often an acting individual. On another level, it can
be a group of people, such as a family or a community.
Fig. 7 The Activity
Diamond [12]
430
C. Magnusson et al.

– The object of an activity is related to the motives and needs of the subject, such
as getting better grades, learning to read or producing a new car. Thus, the
object often consists of or is related to tasks to fulﬁl or desired goals to reach.
– The artefactual and natural environment consists of material and immaterial
artefacts, and their respective affordances and resistances. Some examples are
computers, language, legislation, air temperature, snowstorms and sunshine.
– The human environment is made up of the people or groups of people
inﬂuencing the activity at hand, such as family, work colleagues or larger
portions of society that are involved in or otherwise affect the activity regarding
attitudes, norms and expectations.
Hedvall [12] stresses the individual’s lived perspective on accessibility. The
Activity Diamond analyses how human, artefactual and natural factors impact an
individual’s possibilities to act in concrete situations that are part of a systemic
whole. In design processes the model can be used for discussions regarding who the
user is, what the user want to do, and how humans and technology might support
this in achieving this desired motive.
Making sure to also involve design activities set in the intended context of use
solves part of the problem. In addition, one can consider how this context can be
made part of activities that take place in a more lab/ofﬁce type environment. While
personas [10] have been suggested as a tool for user involvement also in activities
where no users are present, Context cards [22] have been put forward as one way of
allowing varied contexts to be part of activities such as brain storms or focus
groups.
9
Conclusion
In this chapter we have discussed human-centred design, and in particular co-design
together with persons with visual impairments. We have provided examples
intended to serve as inspiration, as well as suggestions for tools and materials that
can be used. As is pointed out already by Gedenryd [9] we do not believe that there
is such a thing as a design method, in the sense of a recipe that you can follow from
the start to the end to ensure good design. What one can (and should) do, is increase
the probability of ending up with a successful ﬁnal product. We suggest that
adopting human-centred design is one way of doing this, but we additionally put
forward the mindset of co-creation something which will allow the design team to
not only design for the users but also with the users. In our experience it is the
overall mindset that is the key to success—details of tools, methods and activities
tend to fall in place once the design team has a co-creative mindset. Through
co-design you gain several advantages:
• by involving future users as an active part of the design team, and by including
them on a more equal footing to the rest of the team you open up for the
Co-designing together with Persons …
431

unexpected—when the design team controls what information to gather, only
input you expect will be gathered and you miss unexpected, but potentially
crucial input.
• you get creative input for your design in the form of inspirational ideas or actual
designs.
• you gain an improved understanding of the users, the usage and the context of
use.
Our hope is that we in this chapter have provided a reader unfamiliar with
co-design with motivation to adopt co-creation as a mindset, and that we have
provided inspiration and practical information useful for anyone interested in
designing for and with persons with visual impairments.
Acknowledgements The authors want to that the EU for supporting the projects HaptiMap and
ABBI. We also want to thank VINNOVA for additional support during HaptiMap. We gratefully
acknowledge everyone involved in the work presented in the examples.
References
1. Amores J, Benavides X, Boldu R, Maes P (2015) Exploring the design of a wearable device to
turn everyday objects into playful experiences. In: Extended abstrcts on human factors in
computing systems (ACM CHI’15), vol 2, pp 2145–2150
2. Chung K, Shilman M, Merrill C, Ishii H (2010) OnObject: gestural play with tagged everyday
objects. In: Adjunct proceedings of the 23nd annual ACM symposium on user interface
software and technology, pp 379–380
3. Druin A (1999) Cooperative inquiry: developing new technologies for children with children.
Hum Fact Comput Syst 592–599
4. Druin A (2002) The role of children in the design of new technology. Behav Inf Technol
21:1–25
5. Ehn P (1988) Work-oriented design of computer artifacts. Umeå University
6. Engeström Y (1987) Learning by expanding: an activity—theoretical approach to develop-
mental research. Orienta-Konsultit Oy, Helsinki
7. Engeström Y (2001) Expansive learning at work: toward an activity theoretical reconcep-
tualization. J Educ Work 14:133–156
8. Fuglerud KS (2014) Inclusive design of ICT: the challenge of diversity. Diss, Degree PhD
9. Gedenryd H (1998) How designers work—making sense of authentic cognitive activities.
Lund University
10. Grudin J, Pruitt J (2002) Personas, participatory design and product development: an
infrastructure for engagement. Design 2002:144–161
11. Guha ML, Druin A, Chipman G, Fails JA, Simms S, Farber A (2005) Working with young
children as technology design partners. Commun ACM 48:39–42
12. Hedvall P-O (2009) The activity diamond—modeling an enhanced accessibility. Lund
University
13. Heuten W (2008) Non-visual support for navigation in urban environments
14. Hourcade JP, Bederson BB, Druin A (2004) Building KidPad: an application for children’s
collaborative storytelling. Softw Pract, Exp
15. Jönsson B, Magnusson C, Eftring H (1995) Capturing better data. User Talk 6:4–5
16. Konkel M, Leung V, Ullmer B, Hu C (2004) Tagaboo: a collaborative children’s game based
upon wearable RFID technology. Pers Ubiquitous Comput 8:382–384
432
C. Magnusson et al.

17. Kujala S (2003) User involvement: a review of the beneﬁts and challenges. Behav Inf Technol
22:1–16
18. Leontiev AN (1981) Problems of the development of the mind. Progress
19. Lueg C, Pfeifer R (1997) Cognition, situatedness, and situated design. In: Proceedings of
second international conference on cognitive technology humanizing the information age,
pp 124–135
20. Magnusson C, Anastassova M, Tollmar K, Pielot M, Rassmus-Gröhn K, Roselier S (2009a)
The mobile Oracle: a tool for early user involvement. In: Proceedings of the 11th international
conference
on
human-computer
interaction
with
mobile
devices
and
services
(MobileHCI’09), p 1
21. Magnusson C, Caltenco H, Finocchietti S, Cappagli G, Wilson G, Gori M (2015a) What do
you like? Early design explorations of sound and haptic preferences. In: Proceedings of 17th
international conference human-computer interaction with mobile devices and services
adjunct (HCI’15), pp 766–773
22. Magnusson C, Larsson A, Warell A, Eftring H, Hedvall P (2012) Bringing the mobile context
into industrial design and development. In: NordiCHI12, p 4
23. Magnusson C, Rassmus-Gröhn K, Tollmar K, Stigmar H (2009b) Exploring user
requirements for non-visual mobile navigation systems. In: Lecture notes in computer
science (including subseries lecture notes in artiﬁcial intelligence and lecture notes in
bioinformatics), pp 754–757
24. Magnusson C, Rassmus-Gröhn K, Tollmar K, Deaner E (2009c) User study guidelines
25. Magnusson C, Rydeman B, Finocchietti S, Cappalgi G, Porquis LB, Baud-Bovy G, Gori M
(2015) Co-located games created by children with visual impairments. In: MobileHCI’15
proceedings of the 17th international conference on human-computer interaction with mobile
devices and services adjunct, pp 1157–1162
26. McElligott J, Van Leeuwen L (2004) Designing sound tools and toys for blind and visually
impaired children. In: Proceedings of 2004 conference on interaction design and children:
building a community, pp 65–72
27. Nacenta MA, Kamber Y, Qiang Y, Kristensson PO (2013) Memorability of pre-designed &
user-deﬁned gesture sets. In: CHI’13 Proceedings of the SIGCHI conference on human
factors in computing systems, pp 1099–1108
28. Nielsen J (1993) Usability engineering
29. Preece J, Rogers Y, Sharp H (2002) Interaction design: beyond human-computer interaction.
Design 18:68
30. Rimoldi HJA (1963) Processus de décision et fonctions mentales complexes. Eur Rev Appl
Psychol 13:65–81
31. Royce W (1970) Managing the development of large software systems. Proc IEEE WESCON
26:1–9
32. Sanders EB-N, Stappers PJ (2008) Co-creation and the new landscapes of design. CoDesign
4:5–18
33. Schön D (1983) The reﬂective practitioner. Pediatrics 116:1546–1552
34. Schön D, Bennett J (1996) Reﬂective conversation with materials. In: Bringing design to
software, pp 171–185
35. Schön D (1987) Educating the reﬂective practitioner: toward a new design for teaching and
learning in the professions, San Francisco
36. Suchman LA (2007) Human and machine reconﬁgurations: plans and situated actions, 2nd
edn. Cambridge University Press, Cambridge
37. Szymkiewicz K (2008) Navigating with ears in hand. Lund University
38. Ueoka R, Kobayashi H, Hirose M (2009) SoundTag: RFID based wearable computer play
tool for children. In: Lecture notes in computer science (including subseries lecture notes in
artiﬁcial intelligence and lecture notes in bioinformatics), pp 36–47
39. Vygotsky LS (1978) Mind in society: development of higher psychological processes.
Harvard University Press, Cambridge
Co-designing together with Persons …
433

40. Walsh G, Druin A, Guha M, Foss E, Golub E, Hatley L, Bonsignore E, Franckel S (2010)
Layered elaboration: a new technique for co-design with children. In: Conference on human
factors in computing systems, pp 1237–1240
41. Xie B, Druin A, Fails J, Massey S, Golub E, Franckel S, Schneider K (2012) Connecting
generations: developing co-design methods for older adults and children. Behav Inf Technol
31:413–423
434
C. Magnusson et al.

Different Approaches to Aiding Blind
Persons in Mobility and Navigation
in the “Naviton” and “Sound of Vision”
Projects
P. Strumillo, M. Bujacz, P. Baranski, P. Skulimowski, P. Korbel,
M. Owczarek, K. Tomalczyk, A. Moldoveanu and R. Unnthorsson
1
Introduction
Loss of vision deprives the human of a key sensory modality used by humans in
spatial orientation and mobility (O&M). The primary travel aids (the white cane or
a guide dog) can help the blind user only in the so-called micro-navigation tasks
like obstacle detection and avoidance or solving local navigation tasks like rec-
ognizing of landmarks or shorelines. However, in new unfamiliar environment, the
visually impaired are unable to solve independently the so-called macro-navigation
challenges, i.e., the tasks of identifying the route and ﬁnding a safe way from an
arbitrary current location A to a destination point B.
Building an electronic travel aid
(ETA), whether dedicated to micro- or
macro-navigation tasks, has turned out to be a difﬁcult interdisciplinary challenge.
The research efforts to build an out-of-laboratory ETA device date back to 1889
when the ﬁrst attempts were undertaken by a Polish scientist Kazimierz
Noiszewski. He built the so-called “Electroftalm” (see Fig. 1), a device that utilized
the photoelectric properties of Selenium cells and converted the visual signal to
auditory stimulations [62].
The device was heavy and inconvenient to use, therefore did not ﬁnd any
practical application. Unfortunately, one can conclude that more than 100 years
P. Strumillo (&)  M. Bujacz  P. Baranski  P. Skulimowski  P. Korbel 
M. Owczarek  K. Tomalczyk
Institute of Electronics, Lodz University of Technology, Lodz, Poland
e-mail: pawel.strumillo@p.lodz.pl
A. Moldoveanu
Polytechnic University of Bucharest, Bucharest, Romania
R. Unnthorsson
University of Iceland, Reykjavík, Iceland
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_15
435

after Noiszewski’s Electroftalm and more than 40 years after the attempts made by
Bach-y-Rita in the 1970s [1], there is no single ETA device that is widely accepted
and used by the community of the visually impaired users [17]. However, the
research efforts continue with many, yet partial, successes in the ﬁeld—see an
excellent and comprehensive review of the last decade assistive technologies for the
visually and blind people edited by Hersh and Johnson [29].
Most of the earlier and modern ETAs use the concept of sensory substitution
[42]. A sensory substitution system can be deﬁned as a system that converts the
visual modality into acoustic and/or haptic representation. As importantly pointed
out in Maidenbaum et al. [42], the neurophysiological mechanism underlying the
operation of the sensory substitution systems is that the brain regions can restore
their function in vision, even if the stimuli come from an auditory or haptic senses.
This is a fundamental observation since more than 90% of visual impairments is not
caused by malfunction of the brain regions (i.e., visual cortex), but by defects (e.g.,
due to illness or an accident) of the visual pathway [68].
A general scheme of a sensory substitution system for aiding the visual impaired
in mobility and travel is shown in Fig. 2. The environment sensing module (irre-
spective of the employed technology) acquires information about the environment
that is converted into nonvisual representation for the blind user. If the system aids
the blind traveler in micro-navigation, the sensing module provides information on
the geometric structure of the local surrounding of the user. If the system plays the
role of macro-navigation assistive device, the sensing module provides both the
local and global data about the environment, e.g., by means of ground-based (e.g., a
network of radio beacons built into environment infrastructure) or satellite based
radio-navigation systems (e.g., the GPS system). Those systems are capable of
informing the visually impaired user about his/her current geographic position and
guide along the preset path to the destination.
The general scheme of the sensory substitution systems applies to the three
major types of the ETAs for the visually impaired
Fig. 1 Putting the
electroftalm on a patient’s
head [45]
436
P. Strumillo et al.

1. Obstacle detectors—handheld small devices or devices attached to the white
cane [15]; it was postulated by Loomis [40] that proximal stimulation, e.g.,
vibration of a white cane can induce perceptual impression of objects in external
space, this phenomenon was termed distal attribution and has been investigated
in later studies [58],
2. Environment imagers—imaging systems employing computer vision technolo-
gies to convert scene images into other modalities [10],
3. Navigation systems—different ICT-based technologies using, e.g., embedded
infrastructures, GPS navigation systems, urban travel aids, or teleassistance
systems [5, 36].
The ﬁrst two classes of aids are personal (wearable) devices aiding the
micro-navigation tasks within personal and near spaces. The third group of aids
includes systems that offer macro-navigation support for the blind user.
In this chapter, we review the prototypes of sensory substitution systems for the
blind, which have been made in the Institute of Electronics of the Lodz University
of Technology. Many of the solutions were tested and evaluated by blind indi-
viduals. The assistive devices that were developed are reviewed in this chapter
according to the introduced ETA categories. The environment imagers are
• Soniﬁed stereovision system for local navigation—a wearable device incorpo-
rating stereovision module and for detecting and warning of the obstacles
employing stereovision;
• Haptic interface—a device allowing to build haptic maps of the environment
intended to be used as a training aid for the visually impaired, which can be
deﬁned as environmental imagers using employing imaging techniques for 3D
scene reconstruction and auditory or haptic nonvisual presentation techniques;
and the navigational systems adding the visually impaired are:
• A network of radio beacons mounted in urban infrastructure—the network aids
the visually impaired in more precise positioning in indoor areas and improves
GPS positioning precision outdoors, e.g., locating public transport stops,
Fig. 2 A block diagram of electronic travel aid (ETA) system employing the sensory substitution
concept (in simpler ETA solutions the processing block denoted by a dashed line is optional)
Different Approaches to Aiding Blind Persons in Mobility …
437

• Public transport information module—employing a server storing online
information about location of public transport vehicles;
• Dedicated applications for smart phones equipped with speech synthesizer—
serving as a navigation aid capable of informing the user of nearest points of
interest (POIs) and routing pedestrian paths to indicated destinations;
• Teleassistance systems—a solution enabling a person located at a remote ter-
minal (stationary or mobile) to guide visually impaired individuals by moni-
toring transmitted life images of the path in front of the blind traveler, which in
turn can be classiﬁed as navigational systems aiding the visually impaired.
2
Soniﬁed Stereovision System for Local Navigation
An ETA wearable solution that we proposed for local navigation and obstacle
avoidance utilizes stereovision input and an auditory output. It was intended to be a
midpoint between environmental imagers and obstacle detectors, i.e., simpliﬁed
scene imaging and reliable obstacle detection. Figure 3 explains the key concept of
this approach. Stereovision images are processed and analyzed to build a 3D model
of the environment. The detected obstacles (within the radius of 4 m) are coded by
a set of sonifying parameters characterizing the shape, size, and also the orientation
and location of the obstacle versus the observer.
Fig. 3 The “Naviton” ETA concept [10] for local navigation and obstacle avoidance: stereovision
is used as the environment sensing module, its two output images are used to compute a simpliﬁed
3D model of the observed scene, which is then soniﬁed using spatial audio
438
P. Strumillo et al.

2.1
Environment Sensing Subsystems
From the possible methods of scene acquisition, stereovision was chosen as the best
choice that could offer both portability and high resolution. Stereovision is a passive
method for environment depth sensing which does not use any internal light source
and works properly in both outdoor and indoor environments.
A wearable stereovision system as shown in Fig. 4 was designed and built as an
environment sensing module for the electronic travel aid (ETA) for the blind [53].
The module, which is housed in a case resembling large glasses comprises a pair of
RGB
cameras,
a
six
DoF
(Degrees
of
Freedom)
inertial
sensor
and
a
custom-designed two-channel sound card.
The geometry of the optics of the stereovision camera is shown in Fig. 5. An
important condition for a proper reconstruction of the scene depth is a correct cali-
bration of the stereovision system so that the so-called canonical setup of the system is
achieved, i.e., the corresponding rows of pixels in image sensors in the left and right
camera are collinear. Moreover, geometric distortions of both cameras must be
identiﬁed and corrected for each of the captured images of the environment [41].
Fig. 4 Electronic module comprising a stereovision system and a sound card; the distance
between the optical axis of the cameras (camera baseline) is B = 60 mm
Fig. 5 Stereoscopic system in a canonical setup
Different Approaches to Aiding Blind Persons in Mobility …
439

To calculate a dense depth map, which gives information about the Z-coordinate
(depth) of each scene point seen by stereovision cameras, the disparity for each
point in the image must be calculated. Disparity d = xL −xR can be deﬁned as the
difference in the position xL and xR of the same point P visible in both images and
can be efﬁciently calculated using the Block Matching (BM) technique [60]. The
relation between the disparity d and the depth Z which is the distance along the Z-
axis of the camera is given by a hyperbolic dependence Z = Bf/d where f is the focal
length of the camera and B is the baseline. Figure 6 shows a pair of stereovision
images of the scene and the depth map shown in pseudo-colors. Depth map shows
the Z-coordinate of each point visible in the reference image captured by the camera
in the camera coordinate system. This distance is coded using greyscale or
pseudo-color values.
A 3D geometric scene model is built form the obtained depth map. The proposed
scene model consists of the two categories of elements: surfaces (especially the
surface of the ground) and obstacles. Surfaces tend to be very important sources of
information for the visually impaired and blind users and can be described using the
plane equation. In order to determine the surface descriptions, we proposed an
iterative algorithm which is based on the comparison of normal vectors found in
regular triangle mesh in the depth image [59]. The result will be a scene which is
divided into two types of regions: planes and other objects. The objects assume 3D
coordinates that do not ﬁt any of the detected planes—thus, they are interpreted as
the obstacles (see Fig. 7a). On this basis the geometric scene model is rendered
from the OpenGL 3D modeling software as shown in Fig. 7b. Note, that only the
obstacles within a distance of three meters from the blind user are reconstructed and
detected by the system.
The detected planes are described by plane equations. Having calculated the
normal vectors of the planes it is possible to identify ground surface and wall
surfaces. The detected objects, on the other hand, are described by a set of geo-
metric parameters deﬁning location, size and spatial orientation of a cuboid cir-
cumscribing the object—see Fig. 7b [59].
Fig. 6 Three-dimensional reconstruction of the “corridor” scene from stereovision: a left
(reference) image, b right image, c depth map is pseudo-colors (the color palette codes the depth of
the scene objects; warm colors represent objects in close proximity whereas cool colors represent
distant objects)
440
P. Strumillo et al.

From the generated models we have also built the so-called occupancy-grid
representation of the 3D scene (see Fig. 8). This visualization technique allows to
deﬁne the scene areas devoid of obstacles. This information is of key importance for
the soniﬁcation algorithms used for auditory display of the environment for the
blind user of the system.
2.2
Auditory Display
Simple monosonic alerts were present in the early ETAs designs [20, 33]. The
sounds later developed to two-channel stereo [34] for basic 2D acoustic imaging of
blind person’s visual surrounding [14]. Human’s hearing is however far more
advanced than a simple left–right pair of microphones. The soniﬁed output of the
Fig. 7 a Model composed of the detected planes and other objects (obstacles), b the OpenGL 3D
render of the scene
Fig. 8 An occupancy grid of a three-dimensional scene calculated at ﬂoor level
Different Approaches to Aiding Blind Persons in Mobility …
441

proposed “Naviton” ETA [10] was spatial audio, so extensive research in this
direction was made. Sound source location is identiﬁed by distance and direction.
Distance perception depends mainly on the sound pressure level—the louder a
source is, the closer it appears to the listener. Determination of direction is more
complex.
The most frequently listed phenomena and factors contributing to the sense of
direction are [50]
• Interaural time differences (ITD),
• Interaural level differences (ILD),
• The ﬁltering action of the human body (i.e., outer ears, head, shoulder, and
torso), which makes ITD and ILD a function of direction and frequency,
• The ratio of the reverberant versus directly received sound energy,
• Familiarity of the listener with the sound source,
• Changes in sound due to head movement.
The importance of interaural effects is clearly seen in the superior accuracy of
horizontal (i.e., left to right) over vertical (i.e., top to bottom) source localization
[16, 19].
When considering a spatial audio based wearable ETAs, the basic idea is to use a
pair of stereophonic headphones and artiﬁcial ITD/ILD synthesis, which should
allow to create at least horizontal illusion of sound directionality. This method
however cancels the natural sound propagation effects introduced by the nearby
parts of the human body (outer ears in particular), which may deteriorate the
localization accuracy or, in case of vertical location sensing, even render it
impossible. A more advanced technique that simulates the human body ﬁltering
effects is based on the application of Head-Related Transfer Functions (HRTFs),
determined for different sound source directions and describing the dependence of
ILD and ITD on the direction and frequency of a sound [13, 15]. Determination of
the HRTFs is however a complex process as they are signiﬁcantly dependent on the
individual anatomical features of a given listener. Different approaches are in use
• Full individualization—direct HRTF measurements for every listener [12, 40, 42],
• Empirical HRTF selection from an existing database [24],
• Analytical HRTF calculations based on generic head/torso model [42],
• Modeling based on generic HRTFs and anthropometric data [44, 46–48],
• Modeling based on 3D ﬁnite element method simulations [12, 42],
• No individualization—application of generic (dummy head) HRTFs.
Individualized HRTFs provide better localization performance, the measurement
procedure is however difﬁcult, time-consuming and demands specialized equipment
(see Fig. 9). Localization errors such as “in-the-head” perception, front-back con-
fusions or elevation shift can still be present in listening tests, but techniques such
as headphone equalization, head tracking, and reﬂection rendering can signiﬁcantly
improve the spatial audio experience [10, 11, 16, 18, 21].
442
P. Strumillo et al.

2.2.1
Headphones
Another factor to consider when designing auditory output for a device to be used
by blind persons are the headphones. Even open-type headphones signiﬁcantly
attenuate external environment sounds. While the “Naviton” project used
high-quality reference headphones for all the listening tests, including pilot trials of
the prototype, part of the “Sound of Vision” project [61] is to choose, or design, a
suitable headphone that does not impair normal hearing of the wearer, but still
allows the spatial audio perception.
In the Sound of Vision project, selected headphones were compared in order to
determine which would deliver the best spatial audio. The compared headphones
included: full size reference open-air supra-aural headphones as a high-quality
control group, bone-conduction headphones that press against the temples and do
not cover the ears, in-ear “secret service” type headphones that utilize small rubber
tubes that push the sound into the ear canal without blocking it, and ﬁnally a custom
prototype with four proximaural speakers (see Fig. 10). The headphones were
tested with 10 volunteers in standard listening trials with static virtual sources and
various spatial audio solutions—generic HRTFs from the MIT KEMAR database
[24], individualized HRTFs measured at TUL [16], constant energy stereo panning
and “quadrophonic” panning for the multi-speaker prototype [66].
Fig. 9 Equipment for fast
measurement of individual
HRTFs
Different Approaches to Aiding Blind Persons in Mobility …
443

In terms of average azimuth errors, all headphone types produced similar results,
ranging from 16° to 18°, with the reference headphones and the custom prototype
providing the most reliably accurate localization. The custom headphones had a
very good localization accuracy only for “low-resolution” tests where sources were
60° apart. The average elevation errors (cf. Fig. 11) were very high regardless of the
headphone type and spatialization method used, and ranged between 19° and 39°,
while the expected error for random guessing would be 48°. The high-quality
reference headphones gave the best results, with individually measured HRTFs
providing only a marginal improvement over the generic ones. While the in-ear air
tube headphones tended to be the worst, the bone-conduction headphones showed
the most variability—for some users they were the best type overall, for some the
worst. The custom headphones performed on par with the reference headphones,
well enough to warrant implementation in the planned “Sound of Vision” prototype
[61] and further research on the subject.
2.2.2
Soniﬁcation Methods
Soniﬁcation, deﬁned as the delivery of information through a nonspeech audio, is
widely used in ETAs for visually impaired people. The ETA applications range
from basic obstacle detectors to complex environmental imaging systems. The
Fig. 10 Prototype proximaural quadrophonic headphones for generating spatial audio without
blocking environmental sounds
444
P. Strumillo et al.

existing and proposed ETA solutions differ in how they collect information about
the surrounding environment and how they present it to the user by means of
soniﬁcation. Input for an ETA may come from:
• “Simple” range sensors, e.g., active (emitter + receiver) ultrasound, infrared,
laser [19, 20, 30, 33, 34, 42, 49],
• Multiple combined range sensors [52, 57],
• 2D video camera, which may be supplemented with an additional range ﬁnder
or GPS/GIS information [21, 38, 47, 56, 69],
• Stereovision cameras [2, 10, 18, 26, 27, 47].
The soniﬁcation aims at transforming the input environmental information to a
possibly intuitive aural impression. Known soniﬁcation techniques include
• Direct or inverse distance to pitch and/or loudness transform [20, 33, 42], with
optional binaural amplitude variation adding directional information [34, 57],
• Instead of continuous sound of varying pitch, binaural clicks of varying repe-
tition frequency [52] or discrete musical tones and/or rhythmic patterns [19, 20,
26, 30, 38, 49] can be implemented,
• Mono- or stereo-inverse spectrogram, where rows and columns of an analyzed
2D or stereovision image are coded with sound frequency components [2, 38,
47, 56],
• Spatialized (HRTF) virtual sound sources (Gonzales-Mora [18, 27], additional
reverb adds distance information [21],
• Stereophonic auditory icons with loudness coding distance [69],
• Forward traveling scanning surface emits discrete sounds as it hits the obstacles
and varying pitch and decaying loudness along continuous surfaces [10].
Simple ETAs based on discrete sensors have their functionality limited to sig-
naling the user of an obstacle they are pointing at. On the other hand, the quantity of
information delivered by sophisticated environmental imaging systems requires
substantially more focus and training from the user and often results in tiredness in
case of prolonged usage [25].
Fig. 11 Average absolute
elevation and azimuth errors
for the custom quadrophonic
headphones prototype in a
virtual sound source
localization task (5  7 grid,
every 30°)
Different Approaches to Aiding Blind Persons in Mobility …
445

2.2.3
Soniﬁcation Methods in the “Naviton” and “Sound of Vision”
Projects
The very ﬁrst soniﬁcations tested in the “Naviton” project resembled those found in
the ETAs utilizing simple range sensors, with range represented by inversely
proportional musical tones, i.e., the smaller the distance the higher the note. The
environment was probed or “scanned” with these “musical range ﬁnders” producing
sequences of notes (see Fig. 12).
Progress with the 3D reconstruction and segmentation leads to another possible
soniﬁcation approach—assigning sounds to segmented scene objects. These sounds
would convey various parameters of each object, such as direction and distance to
observer, category (e.g., wall, stairs, car) or size. The object sounds could be
spatially ﬁltered to appear to originate from appropriate positions in the scene and
could be presented in various order (e.g., of proximity) or be independently looped.
These soniﬁcation approaches were tested as computer simulations with two
groups of 10 volunteers each, the ﬁrst normally sighted, the second blind. The
surveys conducted during the tests led to several conclusions that were utilized in
later designs [10]. Among them were
• Availability of several soniﬁcation modes for a user to choose from, e.g., more
narrow obstacle detection for travel, wide scanning for scene orientation,
• Preference towards discrete musical sounds, over continuous frequency
changes,
• Preference for familiar musical instrument sounds over artiﬁcially sounding
tones, such as a synthesized glottal tone,
• Pauses between delivery of consecutive sound sequences,
• Markers indicating start/end of a sequence of sounds.
Fig. 12 First trials of the “musical range ﬁnder”—the environment was scanned with various
patterns and a sound corresponding to the nearest obstacle in each cone was played
446
P. Strumillo et al.

The “Naviton” project ended with a pilot study of a portable prototype that
utilized a stereovision camera, scene reconstruction, segmentation that divided
scene objects into walls and generic obstacles, and HRTF ﬁltered output. The
soniﬁcation scheme used in the prototype, inspired by sonar and dubbed “depth
scanning” (see Fig. 13), was as follows:
• The observed scene was segmented into walls and generic obstacles of
approximated height and width,
• A virtual scanning plane parallel to the camera image plane moved away from
the observer along the camera axis (the default settings was that the plane moved
4 m in 2 s cycles),
• The movement of the scanning plane was signiﬁed by four progressively quieter
percussive ticks, every 1 m,
• Whenever the scanning plane intersected an object, a virtual sound source
corresponding to that object was played back and tracked in real time,
• Each sound source was spatially ﬁltered with individualized HRTFs to appear to
originate from the corresponding object’s center of mass,
• The sound’s pitch and loudness were inversely proportional to the distance and
duration was proportional to the object’s width,
• Walls were assigned a separate category of sounds with lower pitch and different
timbre, their sounds were also usually longer as they lasted as long as the
scanning plane intersected them.
The “Sound of Vision” project inherits a lot of concepts from the “Naviton”
project, such as the general concept of segmenting the 3D scene into objects,
starting by detecting large surfaces and generic obstacles, with the possibility of
more speciﬁc categorizations (e.g., detecting stairs, cars, humans) and assigning
sounds to said objects.
So far four soniﬁcation models have been proposed and tested by the “Sound of
Vision” consortium. Two models are inspired by the “Naviton” soniﬁcation—one is
the depth scanning model, the second is a variation of it, except that the scanning
surface sweeps left to right from −45° to 45° azimuth. Two are completely new.
Fig. 13 The concept of “depth scanning”—sound sources corresponding to scene objects are
released in order of their proximity and spatialized to appear to originate from the obstacles
Different Approaches to Aiding Blind Persons in Mobility …
447

The ﬁrst one assigns repeated impact sounds (virtual sound sources) to each object.
The sources are ordered left to right in azimuth, but the number and intensity of
repetitions correspond to the distance to an object, while the width is factored into
the frequency of the sound. The second divides the observed scene into three
regions: left, center, and right third, each 30° wide. And quickly plays back three
sounds corresponding to these regions. If a region is empty, a quiet “heartbeat”
sound is played, if there is at least one object in range, the closest object is
responsible for the sound properties. The sound is lower for wider objects, and the
loudness signiﬁes distance.
2.2.4
Tests and Results
The Naviton soniﬁcation was tested by ﬁve blind and ﬁve blindfolded volunteers,
each group consisting of two women and three men, in a controlled environment,
i.e., with cardboard box obstacles color coded to aid segmentation. The volunteers
tested the efﬁciency of obstacle detection, avoidance, and orientation in a safe
environment. Trial times, collisions, and verbal scene descriptions served as
objective results.
The tests were designed to provide simultaneous training and thus progressed in
complexity and difﬁculty. After introductory 5 min. instructions the participants
learned through the tests themselves, completing ten tasks in each test. The ﬁrst trial
consisted of identifying the location of a single soniﬁed obstacle, pointing to it with
a white cane, then walking over to touch it. The second trial was similar, but for two
obstacles. The third and fourth trials consisted of walking towards a speaker 5 m
away from the observer, while avoiding two and four obstacles in the respective
trials. The results are presented in Tables 1 and 2.
Travel speeds are considered a poor measure of an ETA’s efﬁciency, as their use
frequently increases safety at a cost of travel time [29], but the gathered data
allowed for some observations and conclusions. The blind participants also pro-
vided valuable subjective feedback. The participants needed to hear on average two
scan repetitions (2  2 s.) to determine the position of a single obstacle and three
2 s. repetitions for three obstacles. As they explained, the additional “looks” were
needed to verify if the object sounds correctly corresponded to their initial
impression. Although the prototype trials utilized personalized HRTFs, four of ten
participants stated that they perceived little to none spatial audio externalization;
however, as they progressed through the trials they began to subconsciously
associate the sounds with external sources. The volunteers also moved very slowly,
it took on average 30 s to reach a target that was ﬁve meters away. This was likely
due to the instructions to minimize the use of the white cane (holding it vertically)
and clearly announce any contact with an obstacle beforehand (otherwise it was
counted as a minor collision) (Fig. 14).
448
P. Strumillo et al.

Table 1 Task completion times and error rates in “Naviton” prototype trials
Trial
no.
Task
Static localization
Dynamic travel
Average
task time
[s]
Standard
deviation
[s]
Direction
error rate
(%)
Distance
error rate
(%)
Direction error
rate (collision)
Distance error
rate (collision)
Path choice
error
(collision)
1
Localize a single
obstacle, walk to it
4.1
1.2
4
9
4% (3%)
8% (4%)
–
2
Localize two
obstacles, walk to the
second
5.4
1.3
4
4
6% (0%)
6% (4%)
–
3
Navigate 5 m path
between two obstacles
24.5
7.0
–
–
3% (3%)
4% (4%)
2% (0%)
4
Navigate 5 m path
between four obstacles
33.5
13.7
–
–
0% (0%)
4% (4%)
11% (6%)
Different Approaches to Aiding Blind Persons in Mobility …
449

3
Haptic Display Techniques
It was shown in many studies that touch, apart from hearing, is another modality
that can be used to substitute vision in spatial perception. The early prototypes by
Noiszewski [62], Bach-y-Rita [1] and later by Bourbakis [9] showed potential
usability of tactile vision substitution systems. Recently this line of research has
been actively developed by Pissalox and coworkers [43, 54, 65].
A seminal study on using haptics for exploration of the environment was ini-
tiated also in the “Naviton” project. A haptic space presentation system was built in
order to enable the blind persons a touching interaction with 3D real objects created
in virtual reality. The prototype (Fig. 15) consists of the “Mesa Imaging SR3000”
ToF (Time of Flight) camera, a laptop and a “Falcon Novint” haptic interface.
The ToF camera computes the distance from obstacles in a scene by measuring
the time of ﬂight of the emitted and the light reﬂected back. A depth map (where
depth values are represented as colors, cf. Fig. 16a) is built at the output. The depth
map is segmented in order to detect the obstacles and create a virtual scene of the
environment (Fig. 16). “Falcon Novint” haptic game controller is used for a tactile
exploration of the virtual scene. Using the sense of touch the blind user accesses
Table 2 Error types in “Naviton” prototype trials
Navigation and travel errors (out of 100)
Trial 1
Trial 2
Trial 3
Trial 4
Incorrectly chosen path
–
–
2
11
Lost orientation
–
–
1
0
Minor collision due to the chosen path
–
–
0
6
Minor collisions due to misjudged direction
3
0
3
0
Minor collisions due to misjudged distance
3
4
4
4
Missed target due to misjudged direction
3
6
–
–
Missed target due to misjudged distance
5
2
–
–
Fig. 14 “Naviton” prototype trials in a controlled environment. Blind (a) and blindfolded
(b) participants ﬁrst localized, then navigated between cardboard obstacles to reach a sound source
(indicated by a white arrow in a photo on the right)
450
P. Strumillo et al.

information about the content of the observed scenes. The system usability was
examined by eight blind participants [51]. The tested scene was rendered by a
haptic representation in a scale reducing of the scene size by a factor of 38. The
achieved precision (maximum error) of the rendering (related to real scene scale)
was 15 cm. The blind participants were able to successfully count the obstacles and
describe their spatial distribution. With an increasing number of the rendered
objects, however, the time required for haptic exploration tended to increase sub-
stantially and reached 4 min with an average time of 3 min for four rendered
objects. A possible application concept of a personal haptic interface is shown in
Fig. 17.
Fig. 15 A haptic interface bench. From left to right: model cardboard scene, 3D computer model
of the scene, haptic interface programmed to simulate the virtual scene
Fig. 16 The virtual scene modeling process: a 2.5D depth map of the scene, b the segmented
scene, c the obstacles are replaced by cubes [51]
Different Approaches to Aiding Blind Persons in Mobility …
451

4
Teleassistance Systems
The concept of telenavigation is relatively new and provides many advantages and
new research directions. The main idea behind the teleassistance relies on trans-
mitting video and audio stream from a blind person’s terminal to a sighted remote
operator (the guide). Basing on the video data, the guide can navigate the blind
traveler by spoken instructions that are sent back through an audio channel. In most
cases, the visually impaired require help on just a few occasions and then can
proceed on their own. Hence, the solution affords more privacy and autonomy in
comparison to a traditional seeing guide.
4.1
Review of Teleassistance Systems
One of the ﬁrst researches on teleassistance systems was reported in [23]. The
researchers conducted a study where a blind user wore a UHF (Ultra High
Frequency) camera, GSM phone for voice communication and GSM modem for
relaying GPS coordinates. A PC terminal displayed on the map the traveler’s
location and a video stream. The trials described successful guidance within the
university campus. The UHF video transmission was restrained to several dozens of
meters.
Subsequent research [32] employed the GSM technology to transmit all nec-
essary data, thus providing assistance almost from any place.
As the GSM bandwidth is not suitable for sending online video streams of
adequate frame rate, the researchers in Garaj et al. [22] undertook a study on the
effectiveness of teleassistance under a limited frame rate. The results are somewhat
counterintuitive. The researchers claimed, that the effectiveness of navigation was
independent of the frame rate within a range of 2–25 frames/s. Lower frame rates
Fig. 17 The concept of haptic display of the environment using the ToF camera and a personal
haptic interface
452
P. Strumillo et al.

caused, however, the remote operator to get fatigued, as the navigation required
more mental effort to anticipate situations.
Study [37] proves effectiveness of teleassistance in shopping. Other attempts to
build telenavigation systems for the blind are reported, e.g., in [4].
4.2
Proposed Solution
The system developed at the Lodz University of Technology is presented in
Fig. 18a. It is composed of a mobile terminal worn by a blind person and remote
terminal (e.g., a PC computer) operated by a sighted guide.
The mobile terminal is a small one-box device housing a camera, IR LED for
night mode, headset, GPS receiver, HSPA GSM modem, keyboard with big convex
keys and processing unit. The photo of the terminal is shown in Fig. 18b. The
device is operated through speech-synthesized menu. The camera lens is of wide
ﬁeld of view (ca. 90° horizontally), as compared to the ﬁeld of view of mobile
phones being around 20° The video, GPS coordinates and the battery status are sent
through the GSM Internet, which introduces transmission delays. The voice can be
transmitted either through dedicated GSM audio channel, thus not experiencing
delays, or Internet data channel, which is usually cheaper.
The remote guide is equipped with a dedicated PC application which allows to
observe the video stream from the blind person’s terminal, see the geographical
Fig. 18 Proposed telenavigation system: a simpliﬁed diagram, b photo of the mobile terminal
Different Approaches to Aiding Blind Persons in Mobility …
453

location and walking direction of the traveler, monitor the battery level of the
mobile terminal and what is important watch the video transmission delay. The
application is shown in Fig. 19a. The remote guide can in run-time change the
resolution of the camera between 160  120, 320  240, and 640  480, whereby
lower resolutions provide faster frame rate and higher facilitate reading details like
textual information. The guide can also assist blind itinerants using a mobile phone
or tablet running on Android operating system—see Fig. 19b. More details are
available in Baranski and Strumillo [6].
4.3
Results
The system effectiveness was validated through the emphatic trials, where seven
sighted individuals (aged 26–55, ﬁve men and two women) took the role of blind
itinerants. The participants were not blindfolded, however were told to follow
literally the guide’s instructions. The tests were carried out at the university campus
and based on traveling from one place to another. Each participant had to cover
three unknown paths, all ca. 370 m in length. An example path is shown in Fig. 20.
During the walk, the following events were noted down: walking duration, mis-
steps, minor collisions, lost ways, lost connections. The results are summarized in
Table 3. The average walking speed with the remote guide equaled 2.44 km/h and
was compromised mainly by lost connections. By comparison an average walking
speed in a straight line for a sighted individual is 4.5 km/h.
Fig. 19 Proposed telenavigation system: a PC application of the remote guide, b the application
running on an Android mobile phone
454
P. Strumillo et al.

The conducted trials corroborated the effectiveness of the teleassistance concept,
both in macro- and micro-navigation. Telenavigation systems can be helpful in
learning new paths. In many cases, only short navigation sessions are sufﬁcient to
help a blind person to travel on their own (e.g., crossing a street, ﬁnding a button
activating green-light for pedestrians, getting back on the track when lost). The
users of the system should be aware of the transmission parameters, determining the
transmission delay and quality of the video. The new, dynamically developing 4G
LTE (Long-Term Evolution) technology should provide better quality of service,
especially in urban areas.
Teleassistance improves on GPS navigation in many aspects. In urban areas,
GPS readouts tend to be inaccurate—up to several dozen of meters [5]. The remote
guide can on the base of the video ascertain the traveler location and navigate him
or her precisely to the destination.
Fig. 20 An example path of
371 m length. Red solid line
denotes the path followed;
blue dots show GPS readouts
Different Approaches to Aiding Blind Persons in Mobility …
455

5
Navigational Aids
Majority of contemporary systems use global navigation satellite systems (GNSS)
receivers to estimate actual user location [55]. Due to multipath propagation and
strong attenuation of radio signals in the urban environment, the positioning errors
may reach 100 m [64]. One of the ways to enhance the positioning accuracy in
urban areas is to deploy in the environment a network of embedded low power,
short-range radio beacons that can be used to indicate the proximity of the user to a
given point of interest (POI), i.e., a place of a special signiﬁcance to the user.
Exemplary beaconing systems such as PAVIP [8], Step-Hear [63] and URNA [7]
use infrared or radio transmitters to indicate various classes of POIs.
5.1
Naviton Radio Beacons
The framework of the guidance system developed within the “Naviton” project
consists of dedicated applications, database servers, a network of radio beacons, and
user terminals. The general architecture of the proposed solution is shown in
Fig. 21.
The application servers store data required for given services, i.e., a database of
POIs or timetables of public transport vehicles, and manage the communication
between system components.
A network of short-range radio beacons located on bus stops, public transport
vehicles, entrances to public buildings, etc., provides high accuracy location
information at places of possible interest to the user. The transmitters periodically
send unique identiﬁers that can be decoded by the application server and used to
provide the user with additional context-related information.
The core structure of the systems can support a variety of user terminals ranging
from off-the-shelf mobile devices, e.g., smartphones or tablets, to dedicated elec-
tronic travel aids tailored to the speciﬁc needs of visually impaired users.
The prototype guidance system comprised a network of battery operated radio
beacons operating in 868 MHz unlicensed frequency band (Fig. 22). The core
element of the device is Texas Instruments CC430 family ultra-low power
Table 3 Aggregated data for three paths
TP
TP1
TP2
TP3
TP4
TP5
TP6
TP7
Duration (min)
29.3
24.8
27.4
28.5
26.2
21.5
36.1
Avg. speed
(km/h)
2.26
2.66
2.41
2.31
2.52
3.07
1.83
Missteps
1
1
1
Minor collisions
Parked car 1
Parked car 1
Connection lost
2
1
4
The total traversed distance was ca. 1.1 km for each participant. TP stands for trial participant
456
P. Strumillo et al.

microcontroller managing integrated radio transceiver core. Depending on the POI
category, the beacons send unique messages with transmitting power levels ranging
from −30 to +12 dBm. The adjustment of the transmitting power is used to meet
different requirements regarding the expected coverage of the beacon. For example,
at the tram stop the beacon coverage is limited only to the zone where the user can
safely wait for the vehicle. On the other hand, transmitting range of the beacon
indicating the vehicle approaching the stop is set to higher level to give the user
enough time to prepare for getting on. The beacons can also send messages with
multiple, sequentially changed power levels.
Fig. 21 Architecture of the distributed system for guiding the visually impaired
Fig. 22 Radio beacon
prototype built for system
tests
Different Approaches to Aiding Blind Persons in Mobility …
457

5.2
“POI Explorer”
One of the usage scenarios of the guidance system assumes the use of standard
off-the-shelf smartphones as user terminals. The contemporary mobile devices
equipped with GNSS receivers, motion sensors, and multiple wireless communi-
cation interfaces provide a good basis to serve as terminals for visually impaired
users [14]. To verify the suitability of modern smartphones to the needs of guidance
of visually impaired users, a “POI Explorer” application was developed [35]. Also a
dedicated remote controller was proposed to facilitate the control of key functions
of the mobile device.
“POI Explorer” is an application for Android operating system aiding the
visually impaired in independent travel. Its functionality has been worked out with
the target group of users. It uses standard Android components, which allow to
cooperate with available screen readers, e.g., “Google TalkBack” [28]. Some
advanced navigation modules are prepared using graphical mode with the use of
Text-To-Speech libraries. Such solution allows to easily navigate through the
menus, while advanced modes for communication with the user can use the
vibrations, voice commands, and sounds. “POI Explorer” allows to navigate the
blind users in different modes. In the ﬁrst mode user can be navigated along
previously prepared paths or to the selected point, like shop, bus stop, etc. The
distance to the waypoint or to the endpoint is calculated based on readings from the
GPS receiver. Direction to the point is presented in the per hour mode. Orientation
is determined based on a compass sensor and based on the history of the GPS
values.
The look around mode is especially dedicated if users move around unfamiliar
areas. You can get a list of POIs from the selected categories within a deﬁned
radius. For each of the points the distance and the direction are estimated. If user
selects a destination point, our application will automatically switch to the navi-
gation to the speciﬁc point mode.
To simplify the use of the application special soniﬁed mode was proposed. User
can search for POIs in his neighborhood. A virtual circle is moving away from the
user’s location. When it crosses the POIs location, a short sound assigned to the
given category is played. User can stop scanning to hear detailed description of the
point. Application navigation in this mode is based on the 3D gesture detection.
Gesture detection module uses gyroscope, accelerometer, and gravity sensor.
The remote controller visible in Fig. 23 is used to control the “POI Explorer”
mobile application. One can navigate through the application menu and choose the
designated option without the direct use of the phone; it can be hidden in the user’s
pocket. Connection between the device and the mobile phone uses the Bluetooth
interface.
The second task of the remote control is detecting and reading data from the
radio beacons scattered in the environment. The messages read from the beacons
can then be further processed by the “POI Explorer” application to offer additional
location-based- and context-aware services.
458
P. Strumillo et al.

The system developed within the framework of the “Naviton” project has been
applied to support visually impaired users in using public transport services [36].
Having an online access to servers processing information on public transport
routes and timetables, as well as actual locations of trams and buses enables a
variety of applications of the proposed distributed guidance system.
To make the best use of the data from the passenger information system, several
extensions to the “POI Explorer” application were developed. The additional
functionalities include route planning and providing the user with actual location
aware information on public transport vehicles approaching the bus/tram stop. The
latter functionality makes use of identiﬁers broadcasted by radio beacons located on
vehicles and bus/tram stops.
5.3
Results
The positioning accuracy of a network of radio beacons was evaluated in one of the
university campus buildings [67]. The layout of the test premises along with beacon
installation points is presented in Fig. 24. The radio beacons were set up to transmit
packets with output power ranging from −30 dBm in a proximity detection mode to
+10 dBm in a coarse localization mode. When the beacons transmit signals with
low power, the position of the user terminal can be approximated by the known
location of the transmitter. This approach is called proximity detection and can be
used to inform the user of reaching given POI. When beacons transmit signals with
higher output power, the signals can be used to roughly estimate the user position.
During the system evaluation, we used the so-called received signal strength
indicator (RSSI), a ﬁngerprinting technique which relies on assessment of the
similarity of received signal strength indicator (RSSI) measurements reported by the
Fig. 23 “POI Explorer” in a
soniﬁed mode working on
Galaxy Note 3 Android
phone. Each dot visible in the
screen marks point of interest
from the address category.
The small device on the left is
used for controlling (via a
Bluetooth link) the “POI
Explorer” application
Different Approaches to Aiding Blind Persons in Mobility …
459

terminal to the some predeﬁned reference data recorded in predeﬁned locations and
stored in the system database. Table 4 summarizes the results of coarse localization
with the use of the database correlation technique.
6
“Sound of Vision” Project: Natural Sense of Vision
Through Acoustics and Haptics—Concept and Current
Status
“Sound of Vision: natural sense of vision through acoustics and haptics” [61] is
research project funded by the European Commission under the Horizon 2020
framework programme [31]. In this section, we present the project concepts and
discuss the key aspects that distinguish it from other attempts to create assistive
solutions for the visually impaired.
Radio 
beacons
User terminal
with POI Explorer 
application
Seminar 
Room
321A
Laboratory
320A
Lab.
320
321
320B
310
311
312
313
314
315
316
317A
317B
309
308
17,5 m
41 m
Fig. 24 Proximity detection tests—test site with radio beacons (black dots) indicating entrances
to ofﬁce rooms
Table 4 Summary of coarse
localization results
Criterion
Result
Percentage of correct entrance detections
29.8%
Mean positioning error
4.47 m
Median positioning error
2.38 m
460
P. Strumillo et al.

6.1
Concept and Functionality
“Sound of Vision” aims to design, implement, and validate a noninvasive, wearable
solution (hardware and software) that will assist visually impaired people by cre-
ating and conveying an auditory and haptic representation of the surrounding
environment. The main processing steps and its basic components of the system are
represented in Figs. 25 and 26.
The input of the system is represented by a head-mounted hybrid stereo + ToF
3D acquisition system, including also inertial measurement unit (IMU). As in the
“Naviton” project, the raw 3D scene data is to be segmented into distinct objects,
corresponding to the entities from the environment. Furthermore, a few classes of
objects that are extremely relevant for perception and navigation are identiﬁed
separately, such as the ground, walls, stairs, doors and ground-level obstacles, and
holes. Based on this analysis and classiﬁcation of the 3D environment, a synthetic
model of the environment is created.
The synthetic model will be conveyed to the user using a parallel audio and
haptic encoding (soniﬁcation and haptiﬁcation). The audio rendering will use
spatialized audio and the haptic rendering will use a vest with a matrix of vibrating
motors and possibly vibrating bracelets. Additionally, the “Sound of Vision” sys-
tem will take a pragmatic approach to attempt to detect possible dangerous situa-
tions, such as predictable collisions or falls, in which case the user will be alerted.
The alerts will be different from the normal rendering. Another feature of the system
will be scanning for recognizable text and if possible, reading it through text to
speech. In addition to the device itself that will be created by the project, a set of
comprehensive accompanying training programs will be also created. Furthermore,
intensive user evaluation tests are part of the project in order to validate the design.
Fig. 25 Basic processing stages of the “Sound of Vision” system
Different Approaches to Aiding Blind Persons in Mobility …
461

During some of the tests techniques such as psychophysics, EEG and more will be
used to help identifying functional adaptations of users resulted from the usage of
the system.
6.2
Key Prospects
Some key aspects that distinguish “Sound of Vision” from most of the previous
approaches are
• Artiﬁcial sense: the focus on creating a new, artiﬁcial, cross-modal sense,
allowing the user to “feel” the environment, rather than providing just naviga-
tion support and obstacle detection.
• Dual correlated rendering, both audio and haptic.
Fig. 26 Basic components of the “Sound of Vision” system [61]
462
P. Strumillo et al.

• Strong navigational support, both through detection of dangerous situations and
texts.
• Naturalistic encoding: signiﬁcant effort will be dedicated to make the developed
artiﬁcial sense as similar as possible to the natural sense of vision, and at the
same adapted to the speciﬁcs and limitations of the substituting modalities
(hearing and tactile).
• Unprecedented focus on training: in order to help the user achieve performance
in understanding the provided audio-haptic representation, “Sound of Vision”
will develop a set of training procedures based on real and virtual training
environments. In particular, the virtual training will make extensive use of
gamiﬁcation, speciﬁcally, serious games, with the goal to provide high degrees
of immersion, motivation, and fun for the visually impaired trainees. This will
stimulate them to train for signiﬁcant amounts of time, with good focus, and
thus achieve high performance. It should be noted that the very few experiments
[38, 39] which attempted training with sensory substitution solutions, even basic
ones, with less or no naturalness, achieved very promising results. Initial
experiments performed by the “Sound of Vision” project [3] proved that even
simple forms of training, without gamiﬁcation elements and complex scenarios,
and even for short duration, can help trainees develop abilities to understand and
use sound based encodings of environments.
• Interdisciplinary team: the consortium includes high caliber specialists in as-
sistive technologies, computer vision, acoustics, wearable devices, electronics,
neurosciences, eHealth and, most importantly, organizations representing the
end users and caretakers.
6.3
Current Status
The project started at the beginning of 2015 and already has completed its ﬁrst two
stages, out of ﬁve.
The ﬁrst stage focused strongly on obtaining information (through interviews,
questionnaires, and reviews) from the end users to elicit, reﬁne, and optimize the
requirements. An assessment of the current stage in the base technologies needed
for the prototype (such as stereo and ToF cameras, haptic actuators and drivers,
audio headsets, and mobile processing units) was performed. Starting from these,
the system architectural design was created.
The work in the second stage was concentrated on implementing and testing
alternative techniques and components in order to be able to make selection for the
ﬁrst prototype. The work included 3D scene modeling and reconstruction (ﬁltering,
reconstruction, segmentation, features extraction, classiﬁcation), testing and com-
paring different off-shelf headphones for delivering spatial sounds, creating and
testing custom headphones for delivering spatial sounds, creation and testing of
soniﬁcation models and clearing out the most of the technological risks. A detailed
Different Approaches to Aiding Blind Persons in Mobility …
463

perspective on the training was provided, through the design of a complex serious
game concept.
Furthermore, as both the development of the device itself and the accompanying
neuroscientiﬁc studies rely heavily upon testing, dedicated testing environment,
including both real and virtual environments, as well as customized data collection
tools, were designed and created.
7
Summary and Conclusions
In this chapter, we have focused on reporting our different research approaches to
building electronic assistive devices for supporting the visually impaired in orien-
tation and mobility. Our key efforts were directed towards building human–machine
interfaces (wearable or embedded into the urban infrastructure) to help the persons
with visual disability in such tasks as space perception, obstacle avoidance, and
navigation in unfamiliar environments. They reported
• Wearable soniﬁed stereovision system enabled auditory presentation of 3D
scenes so that the blind users could maneuver and omit obstacles in laboratory
setting,
• Haptic display system proved useful in helping the blind users to build a cog-
nitive map of the environment with possible future wearable implementation,
• Teleassistance system, that relays video from the location of the blind user to a
remote guide, underwent ﬁrst successful trials and shows, in our mind, the most
promising solution assisting the visually impaired both in local and global
navigation,
• Network of radio beacons embedded into the urban environment and public
transport vehicles proved efﬁcient in playing the role of navigation buoys
informing the user about his/her whereabouts that outperforms the GPS in
localization precision,
• “POI Explorer” mobile application, can communicate both with the beacons
playing the role of POIs, identify its locations on a map and perform user
auditory interface.
Reporting on the above technological successes on the ETA prototypes we are
aware that these achievements are humble successes, which are still far from being
ready solutions for guiding the visually impaired in real-world indoor or outdoor
environments. We see these technologies rather as bases for virtual reality or
real-world training environments. From our rich interactions with the visually
impaired we conclude that that developing an adequate, user-centered scheme for
training in efﬁcient use of the ETA is of paramount importance. The requirement for
user training is strongly highlighted in the “Sound of Vision” EU project.
464
P. Strumillo et al.

Finally, we believe that our work is yet another path strengthening multinational
and multidisciplinary efforts in overcoming many barriers the visually impaired
people face in their social and professional life.
Acknowledgements This work has been supported by the National Centre for Research and
Development of Poland grant no. NR02–0083–10 in years 2010–2013 and received funding from
the European Union’s Horizon 2020 research and innovation programme under grant agreement
No 643636 “Sound of Vision”.
References
1. Bach-y-Rita P (1972) Brain mechanisms in sensory substitution. Academic Press, New York
2. Balakrishnan G, Sainarayanan G, Nagarajan R et al (2006) A stereo image processing system
for visually impaired. Int J Signal Process 2(3):136–145
3. Balan O, Moldoveanu A, Moldoveanu F (2015) Navigational audio games: an effective
approach toward improving spatial contextual learning for blind people. Int J Disabil Hum
Develop 14(2):109–118
4. Baranski P, Polanczyk M, Strumillo P (2010) A remote guidance system for the blind. In:
Proceedings of the 12th IEEE international conference on e-health networking, application &
services (Healthcom2010), Lyon, France
5. Baranski P, Strumillo P (2012) Enhancing positioning accuracy in urban terrain by fusing data
from a GPS receiver, inertial sensors, stereo-camera and digital maps for pedestrian
navigation. Sensors 12(6):6764–6801
6. Baranski P, Strumillo P (2015) Emphatic Trials of a Teleassistance System for the Visually
Impaired. Journal of Medical Imaging and Health Informatics 5(1–12):1640–1651
7. Bohonos S, Lee A, Malik A, et al. (2007) Universal real-time navigational assistance
(URNA): an urban bluetooth beacon for the blind. In: Proceedings of 1st ACM SIGMOBILE
international workshop on systems and networking support for healthcare and assisted living
environment, New York, pp 83–88
8. Bones Inc. (2014) What is PAVIP? http://bones.ch/pavip.php. Accessed 14 Jan 2016
9. Bourbakis N (2008) Sensing surrounding 3-D space for navigation of the blind. IEEE Eng
Med Biol Mag 49–55
10. Bujacz M, Skulimowski P, Strumillo P (2012) Naviton—a prototype mobility aid for auditory
presentation of three-dimensional scenes to the visually impaired. J Audio Eng Soc 60
(9):696–708
11. Bujacz M, Skulimowski P, Wroblewski G et al (2009) A proposed method for soniﬁcation of
3D environments using scene segmentation and personalized spatial audio. In: Conference
and workshop on assistive technology for people with vision and hearing impairments
(CVHI2009), Wroclaw, Poland, pp 1–6
12. Bujacz M, Strumillo P (2008) Synthetizing a 3D auditory scene for use in an electronic travel
aid for the blind. In: Signal processing symposium, proceedings of the SPIE (6937),
Jachranka, Poland, 693737–693737–8
13. Capp M, Picton P (2000) The optophone: an electronic blind aid. Eng Sci Educ J 9(3):37–143
14. Csapo A, Wersenyi G, Nagy H, Stockman T (2015) A survey of assistive technologies and
applications for blind users on mobile platforms—a review and foundation for research.
J Multimodal User Inter 9(3)
15. Dakopoulos D, Bourbakis NG (2010) Wearable obstacle avoidance electronic travel aids for
blind: a survey. IEEE Trans Syst Man Cybern Part C Appl Rev 40(1):25–35
Different Approaches to Aiding Blind Persons in Mobility …
465

16. Dobrucki A, Plaskota P, Pruchnicki P et al (2010) Measurement system 33for personalized
head–related transfer functions and its veriﬁcation by virtual source localization trials with
visually impaired and sighted individual. J Audio Eng Soc 58(9):724–738
17. Elli GV, Benetti S, Collington O (2014) Is there a future for sensory substitution outside
academic laboratories? Multisensory Res 27:271–291
18. Fajarnes GP, Dunai L, Praderas VS et al (2010) CASBLiP—a new cognitive object detection
and orientation system for impaired people. In: 4th international conference on cognitive
systems, Zurich, Switzerland
19. Farcy R, Bellik Y (2002) Locomotion assistance for the blind. In: Keates S, Langdom P,
Clarkson P, Robinson P (eds) Universal access and assistive technology. Springer, Berlin,
pp 277–284
20. Farmer L (1978) Mobility devices. In: Foundation of orientation and mobility. American
Foundation for the Blind Inc., New York
21. Fontana F, Fusiello A, Gobbi M et al (2002) A cross-modal electronic travel aid device.
Human computer interaction with mobile devices. Lecture notes in computer science 2411,
pp 393–397
22. Garaj V, Hunaiti Z, Balachandran W (2010) Using remote vision: the effects of video image
frame rate on visual object recognition performance. In: IEEE Trans Syst Man Cybern Part A
Syst Hum 40(4):689–707
23. Garaj V, Jirawimut R, Ptasinski P et al. (2003) A system for remote sighted guidance of
visually impaired pedestrians. Br J Vis Impair 21(55)
24. Gardner B, Martin K (1994) HRTF Measurements of a KEMAR dummy-head microphone.
http://sound.media.mit.edu/resources/KEMAR.html. Accessed 14 Jan 2016
25. Gärling T, Book A, Lindberg E, Nilsson T (1981) Memory for the spatial layout of the
everyday physical environment: Factors affecting rate of acquisition. J Environ Psychol
1:263–277
26. Gomez Valencia J. (2014) A computer-vision based sensory substitution device for the
visually impaired (See ColOr), PhD thesis, University of Geneva
27. González–Mora J, Rodríguez–Hernández A, Rodríguez–Ramos L et al (1999) Development
of a new space perception system for blind people, based on the creation of a virtual acoustic
space. In: Engineering applications of bio–inspired artiﬁcial neural networks, pp 321–330,
Springer, Berlin
28. Google Inc. (2015) Google TalkBack https://play.google.com/store/apps/details?id=com.
google.android.marvin.talkback. Accessed 14 Jan 2016
29. Hersh M, Johnson M (2008) Assistive technology for visually impaired and blind people.
Springer, London
30. Heyes D (1984) The Sonic Pathﬁnder: a new electronic travel aid. J Vis Impair Blindness
77:200–202
31. Horizon 2020 (2014) The EU framework programme for research and innovation https://ec.
europa.eu/programmes/horizon2020/. Accessed 14 Jan 2016
32. Hunaiti Z, Garaj V, Balachandran W (2009) An assessment of a mobile communication link
for a system to navigate visually impaired people. IEEE Trans Instrument Measure 58
(9):3263–3268
33. Kay L (1964) An ultrasonic sensing probe as a mobility aid for the blind. Ultrasonics 2(2):53–
56
34. Kay L (1974) A sonar aid to enhance spatial perception of the blind: engineering design and
evaluation. Radio Electron Eng 44:605–627
35. Korbel P, Skulimowski P, Wasilewski P (2013) A radio network for guidance and public
transport assistance of the visually impaired. In: The 6th international conference on human
system interaction (HSI), pp 485–488, 6–8 June 2013. doi:10.1109/HSI.2013.6577869
36. Korbel P, Skulimowski P, Wasilewski P et al. (2013) Mobile applications aiding the visually
impaired in travelling with public transport. In: Federated conference on computer science and
information systems (FedCSIS), 2013, pp 825–828, 8–11 Sept 2013
466
P. Strumillo et al.

37. Kutiyanawala A, Kulyukin V, Nicholson J (2010) Teleassistance in accessible shopping for
the blind. In: Proceedings of the international conference on internet computing, Las Vegas,
USA
38. Levy-Tzedek S, Hanassy S, Abboud S et al (2012) Fast, accurate reaching movements with a
visual–to–auditory sensory substitution device. Restor Neurol Neurosci 30:313–323
39. Levy-Tzedek S, Novick I, Arbel R et al (2012) Cross-sensory transfer of sensory-motor
information: visuomotor learning affects performance on an audiomotor task, using
sensory-substitution. Scientiﬁc Reports
40. Loomis JM (1992) Distal attribution and presence. Forum Spotlight on: The Concept of
Telepresence 1:113–119
41. Ma Y, Soatto S, Sastry S (2004) An Invitation to 3-D Vision. Springer, Berlin
42. Maidenbaum S, Abboud S, Amedi A (2014) Sensory substitution: closing the gap between
basic research and widespread practical visual rehabilitation. Neurosci Biobehav Rev 14:3–15
43. Maingreaud F, Pissaloux E, Gelin R, Leroux Ch (2005) Towards the understanding of the
obstacle perception by visually handicapped: a visuo-tactile approach. ASME Int J Adv
Model C 65(7/8):1–12
44. Malvern B, Nazir A (1973) An improved laser cane for the blind. In: Developments in laser
technology II, SPIE Proceedings 0041
45. Marcinkowski T (1991) Doktoraty HC: Prof. Witold Starkiewicz. Medyk-Czasopismo lekarzy
i studentów 10(545):12
46. McGookin DK, Brewster SA (2004) Understanding concurrent earcons: applying auditory
scene analysis principles to concurrent earcon recognition. ACM Trans Appl Percept 1
(2):130–155
47. Meijer P (1992) An experimental system for auditory image representations. IEEE Trans
Biomed Eng 39:112–121
48. Merabet L, Battelli L, Obretenova S et al (2009) Functional recruitment of visual cortex for
sound encoded object identiﬁcation in the blind. NeuroReport 20(2):132–138
49. Milios E, Kapralos B, Kopinska A et al (2003) Soniﬁcation of range information for 3-D
space perception. IEEE Trans Neural Syst Rehabil Eng 11(4):416–421
50. Moldoveanu A, Balan O (2014) Training system for improving spatial sound localization. In:
Proceedings of the 10th international scientiﬁc conference eLearning and software for
education, Bucharest, pp 79–84, 24–25 April 2014
51. Morański M, Materka A (2012) A haptic presentation of 3D objects in virtual reality for the
visually disabled. In: Sharkey PM, Klinger E (Eds.) Proceedings of 9th international
conference on disability, virtual reality and associated technologies with art abilitation, Laval,
France, pp 103–109, 10–12 Sept 2012
52. Orlowski R (1976) Ultrasonic echo reinforcement for the blind, Ph.D. thesis University of
Nottingham
53. Pelczynski P, Ostrowski B (2013) Automatic calibration of stereoscopic cameras in an
electronic travel aid for the blind. Metrol Measure Syst 2:229–238
54. Pissaloux E, Maingreaud F, Velazquez R (2007) Concept of the walking cognitive assistance:
experimental validation, ASME Int J Adv Model C 67:75–86
55. Ruizhi C, Guiness RE (2014) Geospatial computing in mobile devices. Artech House, Boston
56. Sainarayanan G, Nagarajan R, Yaacob S (2007) Fuzzy image processing scheme for
autonomous navigation of human blind. Appl Soft Comput 7(1):257–264
57. Shoval S, Borenstein J, Koren Y (1998) Auditory guidance with the navbelt—a computerized
travel aid for the blind. IEEE Trans Syst Man Cybern 28(3):459–467
58. Siegle JH, Warren WH (2010) Distal attribution and distance perception in sensory
substitution. Perception 39(2):208–223
59. Skulimowski P, Strumillo P (2007) Obstacle localization in 3D scenes from stereoscopic
sequences. In: 15th European signal processing conference (EUSIPCO 2007), Poznań,
Poland, ISBN: 978-83-921340-2-2
60. Skulimowski P, Strumillo P (2015) Veriﬁcation of visual odometry algorithms with an
OpenGL-based software tool. J Electron Imaging 24(3):033003
Different Approaches to Aiding Blind Persons in Mobility …
467

61. Sound of Vision (2015) “Sound of Vision” project website http://www.soundofvision.net/.
Accessed 14 Jan 2016
62. Starkiewicz W, Kuliszewski T (1965) Progress report on the elektroftalm mobility aid. In:
Proceedings of the Rotterdam mobility research conference. American Foundation for the
Blind, New York, pp 27–38
63. Step-Hear Ltd. (2008) How does Step-Hear® work? http://www.step-hear.com/sh-how.htm.
Accessed 14 Jan 2016
64. van Diggelen F (2009) A-GPS: assisted GPS, GNSS, and SBAS. Artech House, Boston
65. Velázquez R, Pissaloux EE (2008) Tactile displays in human-machine interaction: four case
studies. Int J Virt Real 7(2):51–58
66. Vercoe B (2008) The canonical csound reference manual http://www.csounds.com/manual/
html/index.html. Accessed 14 Jan 2016
67. Wawrzyniak P, Korbel P (2013) Wireless indoor positioning system for the visually impaired.
In: Federated conference on computer science and information systems (FedCSIS), pp 927–
930, 8–11 Sept 2013
68. World Health Organization (2014) Visual impairment and blindness. Fact Sheet N°282 http://
www.who.int/mediacentre/factsheets/fs282/en/. Accessed 14 Jan 2016
69. Zhigang F, Ting L (2010) Audiﬁcation-based electronic travel aid system. In: IEEE
international conference on computer design and applications (ICCDA 2010), Qinhuangdao,
China, pp. 137–141
468
P. Strumillo et al.

Overview of Smart White Canes:
Connected Smart Cane from Front End
to Back End
Gianmario Motta, Tianyi Ma, Kaixu Liu, Edwige Pissaloux,
Muhammad Yusro, Kalamullah Ramli, Jean Connier,
Philippe Vaslin, Jian-jin Li, Christophe de Vaulx, Hongling Shi,
Xunxing Diao and Kun-Mean Hou
1
Introduction
There are 285 million visually impaired people (VIP) worldwide, among whom 39
million are blind [1]. Numerous Electronic Travel Aids (ETAs) have been devel-
oped, but most VIPs still use a white cane for their displacements and as a symbol
indicating that they are blind or have low vision. The white cane cost, ease of use,
and safety are among the reasons of its popularity. However, this assistive tool has
several limitations and its concept should be revisited, especially in the presence of
new digital technologies.
In this chapter is presented the state-of-the-art of ETAs by focusing on their
functionalities,
hardware
architectures,
and
integration
of
Information
and
Communication Technologies (ICT), such as Cloud computing, Internet of Things
(IoT), and smartphone. Connected Multi-Input Multi-Output ETA—called MIMO
eETA—will improve the blind safety by providing more relevant environment
perception. Therefore, MIMO eETA will signiﬁcantly improve VIPs’ well-being by
easing their mobility and quality of life.
This chapter is divided into four sections: ﬁrst, is introduced general knowledge
about VIPs, their mobility, and a classiﬁcation of electronic devices used to help
J. Connier  P. Vaslin  J. Li  C. de Vaulx  H. Shi  X. Diao  K.-M. Hou
Université Clermont Auvergne, Aubière, France
e-mail: kunmean.hou@gmail.com
G. Motta (&)  T. Ma  K. Liu
University in Pavia, Pavia, Italy
e-mail: motta05@unipv.it
M. Yusro  K. Ramli
Universitas Indonesia, Depok, Indonesia
E. Pissaloux
Université de Rouen Normandie, Rouen, France
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_16
469

them in their mobility. The second section is dedicated to the description of a new
concept of such tool, the eETA, with the description of its desired functionalities:
obstacle detection, navigation and guidance, and support to environment percep-
tion. The third section exposes the current state of the implementation of an eETA,
its architecture, and related experimentations. Finally, the back-end tier is analyzed:
its speciﬁcations, functions, and implementation are described in the fourth section.
1.1
Main Features of VIP Mobility
Mobility and orientation are the biggest challenges for VIPs: according to
Manduchi and Kurniawan, 6% of blind people never go outside of familiar routes,
34% once a month or more, 45% once a week or more, and 15% once per day or
more, since walking without sight brings forth the risk of loss, falls, and colli-
sions [2]. One way to improve VIPs’ quality of life is to develop assistive tech-
nologies to ease indoor and outdoor environment explorations and interactions.
Mobility encompasses at least two synergetic tasks1: walking and orientation.
The walking skill is the ability to safely move from one place to another. It means
being able to walk without hitting obstacles, without tripping or falling, to cross
roads and to use transportation means, in particular public ones. The orientation
skill is one’s ability to self-localize in space and to assess different places with
respect to his/her current position [3]. Orientation may imply the usage of a mental
representation of a spatial map.
VIPs have a limited capacity to walk safely, since they hardly sense obstacles on
their way without assistance. Their ability to know where they are and how to reach
a target location is also limited, since they can only use a limited amount of
information to identify and recognize places. Therefore, they are only able to build
limited representations of their environments with respect to the representation a
sighted person is able to build [4].
ETAs are supposed to provide minimal but necessary information for mobility.
Lévesque (2008) presented a good survey on the ETAs assisting obstacle detection
and allowing the blind to ﬁnd a safe and clear path [5]. Existing classiﬁcations are
only based on the output information of ETAs: devices providing complex or basic
information. To meet the evolution of ICT, a new classiﬁcation of ETAs should be
elaborated by considering their inputs, outputs, connectivity capabilities and their
integration with IoT, cloud computing and mobile networks. These new tech-
nologies offer not only opportunities for new services such as near real-time VIP
navigation and assistance, but also a perception of the world closer to that of sighted
people.
1Cf. Part 3 of this book on mobility cognitive models.
470
G. Motta et al.

1.2
Environment Perception by VIPs
The knowledge of the environment is essential for human mobility and orientation.
Before taking decisions relating to their mobility and orientation, sighted people
establish a mental mapping of spaces mainly through the visual channel [6]. They
tend to consider a representation of space as a visual experience. Vision appears to
be the best sensory channel for the acquisition of spatial information because it
provides a relatively simultaneous perception of large spatial ﬁelds [4]. Other
sensations and actions such as hearing, smelling, touching, and moving can also
provide perceptual information about the environment.
VIPs—especially congenitally blind people—have enhanced auditory acuity and
very sensitive haptic and olfactory senses. Integrating multiple perceptual experi-
ences, they can progressively construct a mental map of spaces through the
acquisition of spatial patterns [7].
However, there are differences between the perception of space by sighted
people and VIPs. In the latter, remoteness or closeness is not measured by distance
(in meter or km), but by step count (as a pedometer). German philosopher and
physician Ernst Platner (1744–1818) even concludes that, for congenitally blind
people, distance means a longer or shorter trip time to reach the objective, as
distance means a longer trip time to reach the objective. Furthermore, spatial rep-
resentation may be built with different cognitive patterns: VIPs usually constitute
their space representation from a “path structure”, while sighted people frequently
use “spatially related landmarks” [4, 8].
Blindness onset has also impact on space representation. Studies carried out on
early blindness and perceptual developments do not produce unanimous result [4,
9–13]. However, they demonstrated the ability to form mental spatial models in
congenital, early and late blind people. Most of these studies come to the conclu-
sion that “early blind people have lower performance in spatial tasks than late ones,
especially in displacement representation and spatial inference” [4, 10, 11, 13].
Indeed, the sensorimotor intermodal coordination established during the ﬁrst years
of life is beneﬁcial for late blind people [14]. This coordination allows their spatial
performance to be closer to that of sighted people. However, both early and late
blind people prefer local, path-based information for spatial representation and
strategies, and use the exploration movement-based coding mode for localization
inference. Such behavior can be explained with the type of information obtained
under blindness, which is different from that obtained with vision. Path-based
representation is better suited and more efﬁcient for blind people, as it is easier to
exploit without global vision. The existing ETAs support the path-based approach
to the mobility.
Overview of Smart White Canes: Connected Smart Cane …
471

1.3
Classiﬁcation of ETAs
Nowadays, to assist blind people in their everyday life, different tools are available,
such as white canes, guide dogs, and ETAs. The estimated average lifetime of a
guide dog is 5–8 years and its average cost is around 30 k€ for acquisition and 5 k€
for annual maintenance over the animal’s working life [15].2 In USA, only 0.5% of
legal blinds own a guide dog [16]. For VIPs, low cost, robust, and easy to use ETAs
appear to be a more appropriate general solution.
Several classiﬁcations of ETAs already exist,3 all only based on the type of
output information provided to the end user (e.g., voice, sound and vibration).
Each ETA belongs to a class ranging from I to IV.
Class I devices have a single point-wise output for object preview (vibration or
sound to indicate the presence of an obstacle).
Class II devices have multiple outputs for object preview (point-wise information
on object is provided from multiple directions); it is a secondary aid or primary tool.
Class III devices have object preview, plus environmental information by giving
text rather than headlines.
Class IV devices provide previews of objects, plus artiﬁcial intelligence [16].
This classiﬁcation does not highlight ETAs’ hardware and software architecture
features such as robustness (e.g., space and time redundancy) and overall real-time
performance. Moreover, it does not consider the services that may be provided by
ITS (Intelligent Transportation System: trafﬁc light informs the blind of its status),
IoT (smart object informs its location and use), and Cloud (e.g., itinerary and hints
when the blind is lost). A new generation of ETAs (eETA-connected ETA) can
provide more accurate and reliable environmental information by exploiting mul-
tiple information sources and communication channels. We therefore propose a new
classiﬁcation of ETAs that takes into account the number of both local and remote
(e.g., cloud server) input and output information channels in order to provide an
overview of the complexity of each ETA in term of hardware and software orga-
nizations. As a result, this new classiﬁcation enables, for instance, to estimate the
robustness of obstacle detection (e.g., multiple sensors are used to detect obstacle)
and ETA connectivity for orientation efﬁcient assistance (connectable or not to a
network). For each connectivity class, three sub-classes of ETAs have then been
identiﬁed according to their input–output capabilities: Single-Input-Single-Output
(SISO), Multiple-Input-Single-Output (MISO), and Multiple-input-Multiple-Output
(MIMO). Each of these classes is brieﬂy characterized hereafter.
2http://www.ncbi.nlm.nih.gov/pubmed/18432492.
3See also Part 4 of this book.
472
G. Motta et al.

1.3.1
Single-Input-Single-Output (SISO) ETAs
The ﬁrst generation of unconnected SISO ETAs mainly focused on the obstacle
detection using 1D range sensors: ultrasonic, infrared (IR), and laser sensors.4
Most of the available ETAs are based on ultrasonic sensors; this choice was
made due to the simplicity of their use, low cost, and acceptable range detection
(e.g., 15.2–600 cm). The principle of operation is to use ultrasonic echolocation to
detect objects at varying distances. Ultrasonic waves are elastic waves typically
ranging between 40 and 180 kHz that propagate in solids, liquids, and gases [17].
The distance of an obstacle may be estimated by measuring the Time of Flight
(ToF) from a transmitter to the obstacle and then, after reﬂection, back to the
receiver located near the transmitter.
Equation (1) provides means to estimate the distance d (m) to a point of an
obstacle located in front of the end user and the speed v (m/s) of ultrasonic wave
propagation in the air in function of its temperature T:
d ðmÞ ¼ v  ToF
2
; v is the velocity of ultrasound
v ðm=sÞ ¼ 331:4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ TðCÞ
273:15
r
where T is air ambiance temperature:
ð1Þ
The ﬁrst handheld ETAs based on ultrasounds were developed by Russel [18]
and Kay [19]; Russel’s device indicated whether the path was clear or not, while
Kay’s device directly communicated distances to obstacles as sound pulses. A more
recent handheld ultrasonic ETA is the Miniguide with a push button to change the
mode of operation. It detects obstacles at ﬁve different distances, referred to as
operating modes [20]. This device vibrates to notify the user of obstacles in its
immediate environment. The vibration frequency increases when the distance of the
object decreases. The Mowat Sensor is another handheld ultrasonic-based device
that informs the user of the distance to detected objects by means of tactile
vibrations [21]. However, as materials such as cloth, cotton and wool absorb
ultrasounds, obstacle detection using ultrasonic waves is not completely reliable.
Infrared sensors overcome some limits of ultrasonic sensors; they are widely
used as proximity sensors and for obstacle avoidance in robotics [22]. Figure 1
outlines the principle of IR scanning of the environment. An IR receiver captures
the reﬂected light and the voltage is measured based on the intensity of received
light, which enables to estimate the distance of the object.
4Although “laser” and “infrared” refer to different, independent aspects of light, and as such cannot
perfectly discriminate sensors, we use the terms “infrared sensor” and “laser sensor” in their
common sense: an infrared sensor uses an incoherent beam of infrared light, whereas a laser sensor
uses a single coherent beam of light, visible or not. Furthermore, we call “laser sensor” only the
sensors measuring the distance on a single point; we refer to higher dimensionality laser sensors
(2D, 3D) as “LIDARs”.
Overview of Smart White Canes: Connected Smart Cane …
473

“Tom Pouce” is a handheld ETA detachable device that can be attached to a
cane. It is based on several near IR beams (950 nm wavelength) generated by
collimated LEDs, in different directions and at different emission powers, in order
to cover a “protected” area [23]. When the received photoelectric signal is greater
than a ﬁxed threshold, the device vibrates to alert the blind to the presence of an
obstacle. Tom Pouce has three running modes for three detection distances: 50 cm,
1.5, and 3 m. Notice that IR sensors are sensible to the sunlight and, as with
ultrasonic sensors, the intensity of the reﬂected waves depends on the obstacle
properties (surfaces have different scattering, reﬂection, and absorption properties).
The typical response time of IR is 2 ms. The distance measurement has globally
20% of uncertainty (around the length of one step).
Like ultrasonic distance sensors, laser distance sensors measure the time-of-ﬂight
to estimate the distance between the emitter, the obstacle, and the receiver. Teletact 1
is a handheld laser telemeter that has been developed at the University Paris 11. It
has two output alert channels: tactile vibrations and sound [23]. According to the
obstacle distance, a signal is emitted: a sound from 6 to 15 m, a light vibration on a
ﬁnger from 3 to 6 m, a strong vibration from 1.5 to 3 m, and a strong vibration on
another ﬁnger below 1.5 m. The accuracy of the estimated distance of the obstacle is
about 1% for distances ranging from 10 cm to 10 m.
The point-wise environment scanning obtained with laser, ultrasounds, and IR
requires the VIP to remain strongly focused in order to discover the whole shape of
the detected obstacle and to elaborate the avoidance procedure. This complex space
integration task can be facilitated by object detection and recognition using image
technologies. The basic steps of object recognition, once it is localized, are the
Fig. 1 Principle of IR
scanning of the environment
474
G. Motta et al.

following: image capture, features extraction, features grouping, feature matching,
or classiﬁcation.
Thanks to the tremendous advances of smartphones in terms of cost and com-
putation power, a lot of research has been carried out using the smartphone camera
as an ETA input sensor for obstacle detection and recognition [24–26].
According to the ﬁeld test results published by Foerster et al. [24] and Tapu et al.
[26], cameras are not accurate enough to be used as SISO ETAs, for any type of
obstacle. However, identiﬁcation of a known speciﬁc object (e.g., trafﬁc light
signaling, door) using a camera gives acceptable results. For example, Yusro et al.
[27] used a smartphone camera to detect the status of trafﬁc lights and inform the
VIP through audio output. Figure 2 illustrates the block diagram of the system, and
Fig. 3 its ﬂowchart: image acquisition and processing on a smartphone (color ﬁl-
tering and thresholding) and audio data generation for a VIP.
Lin et al. [28] showed that doors, door handles, and door plates may be detected
and recognized (Figs. 4 and 5) with an ORB feature detector running on a
Raspberry Pi board (Rev. B); the processing time was about 40 ms for each frame.
Considering their widespread use and powerful abilities, it is likely that the role
of smartphones as an input device of ETA for obstacle detection, human–machine
interface (HMI), router, etc. will be more and more important in the coming years.
In the case of eETA, more precision may be obtained by sending raw images to
friends or to the remote cloud server to get help, because CMOS camera are
sensitive to changes of environmental parameters such as light intensity and the
color of the obstacles compared with the ground one.
Fig. 2 Block diagram of the system for trafﬁc light status communication to a VIP
Fig. 3 Main step of the image processing on the smartphone for trafﬁc light status discovery
Overview of Smart White Canes: Connected Smart Cane …
475

1.3.2
Multiple-Input-Single-Output (MISO) ETAs
As already said, SISO ETAs are not able to deliver reliable information concerning
the presence or not of an obstacle and require an important cognitive effort in order
to extract all characteristics relevant to obstacle avoidance. An alternative would be
to use several—heterogeneous or homogenous—sensors in an ETA. MISO ETAs
use several sensors to enhance the efﬁciency and accuracy of obstacle detection and
navigation. Input sensors are usually a mix of the sensors used in SISO. Based on
this approach, several systems have been designed.
Like Teletact 1, Teletact 2 is based on a laser telemeter; however, it adds an
infrared proximity sensor to provide more reliable results. In both systems, the
human–machine interfaces are either tactile or sound-based [23].
Fig. 4 Door handle detection
with image processing
running on a Raspberry Pi B
board
Fig. 5 Doorplate detection
using the ORB feature
detector running on a
Raspberry Pi B board
476
G. Motta et al.

The NavBelt is based on a belt equipped with eight ultrasonic sensors sub-
tending a 120° solid angle of the space, the end user being the apex of this angle.
The NavBelt is able to detect the presence of obstacles, and stereophonic head-
phones inform the VIP about the status of the environment. After 10–20 h of
self-training, users have been able to travel at 0.6–0.8 m/s while avoiding obstacles
as small as 10 cm in diameter [29].
The GuideCane is a motorized wheeled cane (Fig. 6). A steering servomotor,
operating under the control of the GuideCane’s built-in computer, can steer the
guide wheels left and right, relatively to the cane. Attached to each guide wheel is
an incremental encoder, and an array of ultrasonic sensors is mounted in a
semi-circular fashion above the guide wheels. A digitally controlled ﬂuxgate
compass is also mounted above the guide wheels. The embedded computer uses the
data from encoders and from the ﬂuxgate compass to compute the relative motion
of the traveler—by means of odometry, as well as the instantaneous travel speed.
A miniature joystick that can be operated with the thumb allows the user to indicate
a desired direction of motion [30]. The GuideCane may be equipped with a GPS
receiver to provide global positioning information within 20 m accuracy. Haptic
Fig. 6 GuideCane:
operational space [30]
Overview of Smart White Canes: Connected Smart Cane …
477

feedback indicates the path: the devices turns and brakes to avoid obstacles, which
the user can feel through the handle.
Stereo rigs provide depth estimation measures; they are intensively used in
robotics and smart cars to perceive environment, mainly for obstacle detection and
avoidance, and for object recognition [31–33]. Lazaros et al. [34] presented a good
survey on the stereo vision algorithms, taking software and hardware into account.
However, in general, the authors do not consider the energy consumption of stereo
vision for object detection and recognition, which is a key constraint for wearable
and portable systems. Another problem is its dependence on environmental char-
acteristics: scene illumination, scene surface characteristics, scene object density,
etc. Reliable real-time stereo vision for obstacle recognition and avoidance is still an
open research issue.
According to the current state-of-the-art, the performance of stereo vision is not
reliable enough to be used as a single sensor of a MI ETA, but it may occasionally
be used as a part of a MISO ETA combined with other sensors for object recog-
nition and obstacle avoidance to minimize energy consumption and increase the
accuracy of object detection. In the case of connected MISO ETAs, stereo images
could be sent to the cloud server for more complex processing implementation (e.g.,
object recognition, 3D scene reconstruction).
1.3.3
Multiple-Input-Multiple-Output (MIMO) ETA
In order to make an ETA reliable, ﬂexible and easy to use, having multiple inputs
and outputs is a key issue. Several MISO ETA presented above may further evolve
as MIMO ETAs; for instance, the NavBelt becomes a MIMO ETA once it offers
both tactile and stereophonic outputs to inform the VIP about the status of the
environment.
Notice that output interfaces of ETAs such as coin vibration motors, sound,
speech synthesis, and tactile braille will be more and more affordable in terms of
cost, energy consumption, and space. Therefore, it is possible to integrate them in
new ETAs.
1.3.4
Summary
Nowadays, available unconnected ETAs (SISO, MISO, and MIMO) do not
appropriately fulﬁll VIPs’ needs, particularly those of blind people, in terms of
obstacle detection, recognition and avoidance, navigation, and assistance.
The next sections will present an example of eETA, a connected smart cane,
which can signiﬁcantly improve VIPs’ perception of the environment. Moreover,
VIPs may be guided remotely in near real time everywhere worldwide, and they can
get help when needed.
478
G. Motta et al.

1.4
Connected ETA (eETA): Concept and Proposed
Architecture
As we pointed out previously, most blind people worldwide still use a simple white
cane for their displacements because the current available unconnected ETAs do
not provide expected functionalities and dependability. This section introduces the
concept of a MIMO eETA and its implementation into a connected smart stick.
1.4.1
Concept of a MIMO eETA
A MIMO eETA is an electronic device dedicated to assisting VIPs with their
mobility and able to communicate with the surrounding world—other devices
included—via a set of sensors and information outputs.
Sensors, which often can be seen as electronic ersatz of physiological sensitive
organs, provide data describing the external world according to their perceptual
capability. Cameras (in visible or IR spectrum), ultrasonic range sensors, and GPS
chips are example of frequently used sensors (MIMO eETA inputs). Speech syn-
thesizers, touch-stimulating surfaces (such as Braille keyboards), and vibro-tactile
stimulators are the most popular examples of information outputs. They provide
data that can be used either by humans or other electronic devices. MIMO eETA
sensors and information outputs are able to communicate both between each other
and, through the network, with the external world.
1.4.2
Proposed Architecture of a MIMO eETA
A MIMO eETA can be implemented in a distributed manner, in three components
(Fig. 7):
• The front end is a white cane equipped with sensors allowing it to perceive
indoor and outdoor environments (also named a Local MIMO Smart Cane or
“LMSC”);
• The smartphone is a kind of local server, as it provides ofﬂine server-like
services, e.g., maps repository. Furthermore, it plays the role of router and of
human–machine interface;
• The back end is a dedicated cloud computing platform for VIP guiding and
assistance in near real-time worldwide.
It should be stressed that the smartphone can be enhanced by a device designed
to communicate with VIPs, such as a touch-stimulating surface.
Overview of Smart White Canes: Connected Smart Cane …
479

2
Design of a MIMO eETA Based on a White Cane
This section presents in details the concept of a MIMO eETA. It starts with an
overview of an academic prototype named 2SEES, and then addresses an overview
of the available technologies for the design and implementation of a future MIMO
eETA.
2.1
2SEES: An Academic Prototype
of a Connected Smart Stick
2SEES (pronounce “to SEE”) is under joint development by a consortium com-
posed of LIMOS (CNRS/Clermont-Auvergne University, France), University of
Rouen (France), Pavia University (Italy), and University of Indonesia (Indonesia) in
collaboration
with
charity
associations
FAF
Auvergne
GAIPAR
and
Normandie-Lorraine. The 2SEES research project is an extension of the Smart
Environment Explorer Stick (SEES), which was the ﬁrst MIMO eETA prototype
(Fig. 8) jointly developed by the LIMOS (Blaise Pascal University, France),
University of Rouen (France), and University of Indonesia (2011–2014).
The SEES system contains three main components: a global remote server
(iSEE), an embedded local server (SEE-phone), and a smart stick (SEE-stick). iSEE
is a global server providing web services for VIP such as remote real-time hint and
help and remote monitoring (e.g., tracking VIP’s location). The SEE-phone is a
commercial smartphone that is used as an embedded local server and provides local
services for the SEE-stick, such as itinerary and Internet access to iSEE. Speciﬁc
functions have been developed to meet VIPs’ requirements (e.g., trafﬁc lights
detection). Functions available on SEE-phone, such as voice recognition, will be
used to enhance the VIP user-friendly interface. The SEE-phone is always
Fig. 7 A white-cane-based MIMO eETA
480
G. Motta et al.

connected to the SEE-stick through Wi-Fi. The SEE-stick collects data for walking,
while SEE-phone is the key device for orientation; indeed, the SEE-phone com-
municates with the GPS, through the web server accessing to the map database and
with other mobile devices.
Using the SEES system, a VIP can know his/her current location. Furthermore,
other persons (e.g., friends and family members) can also monitor the VIP’s journey
remotely. Therefore, the SEES can help VIP to move safely and easily in any place
and anywhere (indoors or outdoors).
The main innovation of the SEES is its Internet connection, which enables VIPs
to get help and be monitored from anywhere worldwide. Therefore, the SEES is an
ubiquitous smart stick [35].
The 2SEES, which will be the successor of the SEES, will have the three
following components: the LMSC, a smartphone, and a cloud back end, which
functionalities are presented in details in the following sections. The active
multi-sensor context-awareness concept is adopted to be implemented in the LMSC
to allow a high enough accuracy for obstacle detection and localization while
maintaining a high battery lifetime and offering robustness capabilities.
Fig. 8 Architecture of the SEES platform
Overview of Smart White Canes: Connected Smart Cane …
481

2.2
Functionalities of a MIMO eETA
The functionalities of an eETA should allow VIPs to freely travel and interact with
unknown environments. Most of these functionalities can be organized into three
categories: obstacle detection, navigation, and environment perception.
2.2.1
Obstacle Detection
In the context of VIP mobility, an obstacle is a difﬁculty that impedes the move-
ment of a human. Obstacles may have different temporal and spatial properties, and
they play different roles in the mobility. Figure 9 presents the objects to be con-
sidered as obstacles by 2SEES.
Fixed obstacles (e.g., walls of buildings, curbs, trash cans) keep a constant
position on maps over the time. They are space invariants and can be used as spatial
cues and landmarks for journey progress monitoring.
Passive mobile obstacles (e.g., table of a café, movable trash cans, signboards,
louvers, parked vehicles, open shutters, car mirrors) can change their spatial
position but may be considered as static objects with respect to the human motion at
a given moment. They provide information on traveled space, social, and urban
organization, and might be used as mobility clues; they can conﬁrm the journey
progress.
Moving obstacles (bicycles, cars, animals, humans, etc.) usually change their
spatial position faster than VIP’s motion. They provide/conﬁrm information on
traveled space, social and urban organization; they can conﬁrm the journey
progress.
The means to detect obstacles depend on both obstacle properties and obstacle
role during the travel. To detect ﬁxed or movable obstacles, a combination of
several information sources—commonly called “data fusion”—is used. Scalar
distance sensors (e.g., ultrasonic transducers, infrared sensors, and laser telemeters)
can detect the presence and estimate the distance of obstacles located in front of the
VIP, whereas vector or matrix distance sensors (e.g., LIDARs and Time-of-Flight
cameras) can be used to detect protruding shapes located in front of the VIP [36].
Images from 2D cameras can also be processed to get an insight on the presence of
obstacles. For some objects, it is possible to match a map with very precise
localization information provided by a GPS; sometimes, this may be complemented
with stereo rig images.
As the user must be able to detect an obstacle before she/he hits it, the detection
functionality has a strong real-time constraint: latency. Assuming a detection dis-
tance of 2 m by the sensor, a walking speed of 1 m/s, and a reaction time of 1 s, the
smart stick should be able to signal the user that there is an obstacle 1 m in front of
him/her in less than 1 s to keep a safety distance of roughly one step length.
Depending on the detection technique used, it may be more or less easy to guar-
antee such a latency, or even impossible.
482
G. Motta et al.

Aguerrevere et al. [37] used a sonar range sensing system composed of six
ultrasonic transducers to build an approximate representation of the space around
the user. The presence of obstacles in six directions around the head of the user was
communicated using 3-D space soniﬁcation [38].
As described before, the Teletact developed by Farcy et al. [23] is a handheld
laser telemeter that can be clipped on a white cane. It has been commercialized and
thoroughly tested by users. Teletact can be associated with the “Geotact” that uses a
GPS to provide audio navigation assistance in areas where GPS signal is available
and precision not critical.
Filipe et al. [39] proposed an obstacle detection system based on the Kinect
stereoscopic camera and neural networks. The Kinect camera estimates depth
images that contain the distance between the camera and the object along with
traditional RGB pictures. By moving the camera in 3D directions, the user scans the
environment in front of him. The obtained images are then classiﬁed with the help
of a neural network in four classes of obstacles: free path, obstacle, upstairs, and
downstairs. This method has a very high accuracy: the paper reports approximately
99% of correct classiﬁcations—it is also suitable for both indoor and outdoor
environments. However, the Kinect requires a rather high power and, more
importantly, a high computational capability. Those are two hard constraints in the
context of embedded systems. The three previous devices allow a good detection of
obstacles and an efﬁcient feedback to the user, but they do not allow indoor
navigation.
The detection of moving obstacles implies latency constraints, such that the
system end user has time to take actions to avoid them. Usually, sighted subjects
detect a moving object (e.g., vehicle, bicycle, horse) using visual pattern
Fig. 9 Obstacles detected by the eETA
Overview of Smart White Canes: Connected Smart Cane …
483

recognition, while sightless people mainly detect it from the sound it produces. This
difference in used sense explains why any solution using hearing for detection
process is not suitable for VIPs.
Intelligent Transportation Systems may share information about the position of
vehicles [40]. Because the detection of a vehicle alone is insufﬁcient to predict if a
collision is probable, the system must be able to estimate the trajectory of both the
user and the vehicle.
Detection of moving obstacles is very challenging with respect to computation
time, latency, and accuracy. The accuracy must be very high since on one hand,
false negative predicted (i.e., unpredicted) collisions may lead to serious injuries,
and on the other hand, given the number of situations of proximity between vehicles
and pedestrians without accidents, false positive collision alarms can disturb the
user with a lot of useless signaling.
Sound processing can reveal the presence of vehicles and provide information
about some types of vehicles (trucks, cars, and motorcycles) [41]. Using the
eigenfaces method, it is possible to model the distribution of power in the frequency
domain of sounds. In that form, the sound of vehicles can be recognized by
comparison to a database of already classiﬁed sounds.
Computer vision can also be used for vehicle detection: the smartphone-based
obstacle detection system of Tapu et al. [26] is able to detect moving objects and
evaluate the risk they present. For that purpose, they extract and track points of
interest from the video stream using a multiscale Lucas–Kanade algorithm; the
algorithm then makes regional clusters in the images based on the apparent motion
in areas. Object features are extracted using Histograms of Oriented Gradients
(HOG), which are then used to build Bag-of-Words representations. These repre-
sentations are ﬁnally fed into an SVM classiﬁer so that the system is able to
recognize objects.
2.2.2
Mobility: Localization and Tracking
As explained before, VIP mobility consists of two tasks: walking and orientation.
Whereas the purpose of the obstacle detection functionality is to support the
walking task, the purpose of the mobility and tracking functionality is to allow users
to reach their desired destinations, i.e., support the orientation task. This means that
the mobility-assistive device should constantly localize the VIP and match his/her
current path with a trajectory leading to the target location (Fig. 10).
Localization and Tracking Principles
Localization is the knowledge of the position and the orientation of a body in any
terrestrial reference frame. Tracking is the continuous localization of that body.
VIPs’ tracking is necessary for at least two reasons:
484
G. Motta et al.

• The system should provide obstacles information and direction instructions to
the user in real time, regularly and upon request;
• The system should provide socio-urban, contextual information in real time
(upon request).
The localization function uses kinematic parameters allowing the description of
a free body’s movement in space (body’s tracking): they include its position, its
orientation, and its linear and angular velocities. To compute the position and the
orientation of a body, it is required to know the three Cartesian coordinates of one
of its points and the three orientation angles of the body’s own reference frame axes
with respect to those of the terrestrial reference frame (REarth). These six parameters
—or degrees of freedom (i.e., three translations and three rotations)—can be
directly measured with different sensors.
There are many ways to compute the position and the orientation of a body, but
it should be noted that it is a difﬁcult problem, both indoors and outdoors [42]. In
outdoors, the availability of absolute positioning systems (e.g., GPS, Galileo,
BeiDou) allows easier development of localization technologies than indoors. On
the other side, systems designed to work indoors are easier to adapt to outdoor
environments.
Furthermore, in order to be widely adopted by VIPs, a smart stick should not
rely on a costly, dedicated infrastructure, such as a set of ﬁxed external sensors for
tracking the moving object within the volume deﬁned by a 3-D array of sensors.
Two solutions remain:
• Usage of so-called “proprioceptive sensors”, i.e., embedded sensors whose
outputs contain information about various kinematic parameters (e.g., linear and
angular velocities and accelerations), allowing the estimation of the absolute
position of a moving object in the terrestrial reference frame (REarth), indoors or
outdoors equally;
• Usage of “exteroceptive sensors” measuring environmental parameters, e.g.,
embedded cameras, LIDARs, and optical encoders. These sensors may or may
not have afﬁnities with either indoor or outdoor environments (e.g., satellite
signal blocked by roofs, indoors wall structure easier to recognize with a
LIDAR).
Localization and Tracking: Sensors and Approaches
Fixed external sensors involved in 3-D motion analysis systems are usually com-
posed of heterogeneous sensors (e.g., video, magnetic) that must be conveniently
set in the environment in order to record from different points of view the suc-
cessive positions of the free body of which movements are studied. As the sensors
are ﬁxed in the reference frame, the analysis volume is limited to the intersection of
the sensors perception ﬁelds, which is convenient for studying numerous human
Overview of Smart White Canes: Connected Smart Cane …
485

movements, but does not allow the study or the following of large displacements
like indoor or outdoor human-free walking.
For that purpose, most approaches use sensors embedded in a portable device.
These embedded sensors usually involved for movement data acquisition are wheel
encoders, accelerometers, pedometers, gyroscopes, rate gyroscopes, inertial mea-
surement units (IMU) or inertial navigation systems (INS), and cameras.
A wheel encoder can report the distance traveled by one wheel provided that the
wheel rolls without slipping on the ground. However, this distance is not repre-
sentative of the trajectory followed by the contact point between the wheel and the
ground, which can be a 3-D curve of any shape. To get the 2-D trajectory of a body
or a vehicle rolling on a ﬂat (horizontal or not) surface, two wheel encoders ﬁxed on
two separate wheels are necessary.
The ego-centered positions and distances in REarth of known spatial points which
may constitute mobility cues, clues, and landmarks, can be acquired with range
sensors, such as ultrasonic transducers, infrared range sensors, laser telemeters,
LIDARs, and time-of-ﬂight cameras or a (stereo) camera.
If radio signal is available, it is possible to use it in two ways: triangulation or
ﬁngerprinting. If the locations of the radio sources are known, we can have crude
information about the position of the receiver by measuring the transmitted powers.
Another possibility is to map the ﬁngerprints of the signal intensities of the sources:
the localization will then only be the reverse operation, i.e., ﬁnding the position
from the ﬁngerprints [42].
Outdoors, where it is possible, global positioning systems such as GPS,
GLONASS, Galileo or BeiDou, can be used [43].
Wi-Fi ﬁngerprinting [44] is a commonly implemented navigation technique that
relies on surrounding Wi-Fi access points to deduce the position of a receiver. This
technique allows to create a radio map of the environment and then to compare the
Wi-Fi measurements of an object to this map in order to get its position. To improve
the robustness of this technique with regard to the difference of characteristics
between the devices used during cartography from those used during localization,
advanced error modeling methods are required. This method has the advantages of
constant precision with respect to time (like the GPS) and somewhat low compu-
tational requirements. However, it has at least two drawbacks: it needs a precise
radio mapping of the environment and a good Wi-Fi infrastructure. It can be applied
both indoors and outdoors.
Li et al. [45] proposed an indoor localization system that exploits the LED
lighting infrastructure to transmit information, using their ability to quickly switch
on and off. Every lamp of the lighting infrastructure emits a different signal con-
taining its position, which is detected by a camera carried by the user. A smartphone
camera can be used for this purpose. The camera outputs both the position of the
lamps it sees and their signal strength (i.e., how much light is received). From this
information, an algorithm is able to estimate the position of the camera, and thus
that of the user. The LED bulbs use binary frequency shift keying (BFSK) mod-
ulation to encode their data; as they all share the same medium (visible light),
collision avoidance techniques are implemented. Channel hopping is used: basic
486
G. Motta et al.

time periods are divided into slots, and every bulb randomly chooses one of these
slots. As long as the number of slots is high enough with respect to the number of
bulbs, collisions have a statistically negligible importance. This method is very
accurate (about 40 cm accuracy is reported) and makes use of a necessary infras-
tructure (lighting) instead of adding another one. This infrastructure still has non-
trivial constraints: lighting must be done by LED bulbs that must be paired to
special circuits to generate the broadcast signals.
Riehle et al. [46] proposed a VIP indoor tracking system based on the magnetic
signature of a building. Modern buildings, due to their steel, reinforced concrete
structure, and power systems, generate distortions in the magnetic ﬁeld. These
distortions present enough temporal stability and space variability to be used in
location systems. However, most of these solutions rely on accurate mapping of the
magnetic ﬁeld, which takes time and effort. The system developed by Riehle et al.
[46] surveys the building magnetic signatures (a sighted “leader”) only on routes of
interest, and signals points of interest along these routes. The blind traveler fol-
lowing these routes is warned by a sound if she or he diverges from it. This system
reduces the need of professional surveying and drastically reduces the cost. An
implementation was realized using a smartphone and two IMU chips, each con-
taining three-axis gyroscopes, accelerometers, and magnetometers. The strong
points of this solution are the elimination of the need of a speciﬁc infrastructure and
of precise building surveying, as well as the low cost of the solution. However, it
still requires upstream work of a leader and provides only a restricted,
point-to-point navigation solution: if the user deviates from the pre-existing paths,
his location will not be determined. Suitable conditions for this system might also
be restrictive: not all buildings possess a steel structure and electrical power systems
(e.g., ancient buildings).
Another class of approaches uses accelerometer, an inertial sensor designed for
measuring one or several components of the linear acceleration of a free body. As a
ﬁrst approach, let us consider a one-axis accelerometer made with a box containing
a sensitive cell composed of a small mass ﬁxed to a spring and located at point
M. The mass (m) of M and the stiffness (k) of the spring are known from con-
struction. A reference frame RA (origin A) is linked to the box and the axis
(x) deﬁned by vector AM represents the measurement direction of the accelerometer
(Fig. 11a).
According to Newton’s Second Law, when the box is submitted to an acceler-
ation along its measurement direction (x), the mass exerts on the spring a force
F (Fig. 11b) that is directly proportional to the submitted acceleration a(M/RA),
which is then computed as follows Eq. (2):
~F ¼ m~a M=RA
ð
Þ
thus:~a M=RA
ð
Þ ¼
~F
m :
ð2Þ
Accounting to the miniaturization of the sensitive cell components, the spring
length and the mass movements within RA are assumed to be negligible. These
Overview of Smart White Canes: Connected Smart Cane …
487

assumptions lead to consider that both points M and A are superimposed and thus
the acceleration measured by the accelerometer is that of the origin A of RA.
In the reality of a free body’s movement, it is not possible to know a priori in
which direction of the terrestrial reference frame (REarth) it will move. It is also quite
impossible to precisely ﬁx an accelerometer along one axis of the body (e.g., shaft
axis of a white cane): it is thus relevant to use a 3-D accelerometer and to ﬁx it in
any position on a known point of the body.
The use of a single 3-D accelerometer only enables to study the translations of a
free body moving on a ﬂat horizontal or inclined ﬂoor. Moreover, these limited
conditions do not allow computing the distance traveled by the free body along a
straight line on a horizontal ﬂoor with a convenient precision because of the
integration drift of the acceleration and of the unevenness of the ﬂoor [47–49]. They
also do not allow rebuilding the trajectory followed by the free body during any
displacement. Indeed, if the body is moving with both translations and rotations in
REarth, the acceleration measured by the accelerometer is the driving acceleration,
which is the vectorial sum of several terms (e.g., tangential and centripetal com-
ponents) that cannot be computed without knowing the instantaneous angular
velocity of RA with respect to REarth. As a consequence, to get the other kinematic
parameters of a free body’s displacement, it is necessary to quantify its rotations,
which can only be done with the help of other inertial sensors, like gyroscopes and
rate gyroscopes.
The name “gyroscope” was given by Léon Foucault to the device he designed in
1852 for the ﬁrst inertial experiments that exhibited the Earth’s rotation. Indeed, this
device allowed viewing the rotation of its box with respect to a ﬁxed direction in
Fig. 10 Overview of the localization technologies for the eETA
488
G. Motta et al.

REarth. Originally, this device contained a rotor—or spinning top—rotating at such a
high speed that its rotation axis kept a ﬁxed direction with respect to REarth,
whatever the movement of its box or of the body on which it is ﬁxed. This
phenomenon derives from the principle of angular momentum conservation [50–
52], which requires that at the initial instant the gyroscope rotates around the
spinning axis, and that the vectorial sum of the moments of external forces applied
on its center of mass G is null.
The behavior of gyroscopes is the reason of their use as navigation instruments
on ships (1865: gyrocompass), and then on planes (1914: artiﬁcial horizon), for
determining a ﬁxed direction with respect to REarth, which is assumed to be
Galilean. Gyroscopes embedded in these instruments generally have two axes, and
the spinning axis can be oriented either horizontally (gyrocompass) or vertically
(artiﬁcial horizon): in the ﬁrst case, the gyroscope gives a measure of the free
body’s orientation with respect to the geographic North (yaw angle), whereas in the
second case it indicates its roll and pitch angles.
As gyroscopes allow to measure rotation angles of a free body, they were
logically used for building the ﬁrst instruments designed for measuring angular
velocities: rate gyroscopes. A rate gyroscope is a one-axis gyroscope that measures
the component of the instantaneous rotation velocity of its box along yaw, roll, or
pitch axis. Thanks to the development and miniaturization electronic components,
new types of rate gyroscopes without spinning top—simpler, more compact, more
reliable, and less expensive—have appeared in the last decades: laser, whirlpool, or
tuning fork rate gyroscopes [52]. The latter ones get the rotation velocity from the
measurement of the Coriolis’s inertial force acting on a vibrating elastic blade. The
small dimensions of these devices make them particularly convenient to be
embedded in mini Inertial Measurement Units (IMU).
An IMU (and INS, inertial navigation system) is a composite sensor integrating
three accelerometers, which measure the components of the linear acceleration of
the free body along the three orthogonal axes of its own reference frame (RA), and
three rate gyroscopes, which measure the components of the instantaneous rotation
velocity of the free body around the three axes of RA. Some IMU/INS also includes
magnetometers that measure the three components of their orientation with respect
to the earth magnetic ﬁeld. Finally, an integrated processor directly gives the three
orientation angles (a, b, c) of the IMU/INS reference frame (RA) with respect to the
axes of REarth.
Inertial sensors work along mechanical principles that allow them to measure the
driving acceleration (accelerometers), the orientation (gyroscopes), or the instan-
taneous angular velocity (rate gyroscopes) of the free body on which they are ﬁxed.
The improvements and the miniaturization of electronics components during the
last decades have allowed to integrate these devices within compact and light
measurement systems that can be now used in many applications where their low
mass and low bulk are criteria of main importance [47, 53–63].
When compared to 3-D movement analysis systems based on video, inertial
sensors are “blind” as they do not take information in the nearby environment.
Indeed, their name comes from their ability to measure some kinematic values of
Overview of Smart White Canes: Connected Smart Cane …
489

their own movement, assuming that they move within a Galilean—or “inertial”—
reference frame. From these measurements and the knowledge of the movement
initial conditions, it is possible to compute their three orientation angles and the
three components of their linear velocity in REarth. It is also possible to compute
their trajectory provided their absolute position and orientation are periodically
updated during the movement. For these reasons, they have been used for a long
time in sea and air navigation for computing the orientation and the direction
followed by ships and aircraft without earth benchmarks or when visibility is low or
null (e.g., fog, cloud, night).
Moreover, inertial sensors have the following advantages:
• They can be directly ﬁxed on the body (e.g., vehicle, boat, air plane, human) of
which movement is studied (“strap-down” systems);
• They are not submitted to the use limits of 3-D movement analysis systems (e.g.,
restricted analysis volume, temporally hidden, or lost markers);
• Some kinematic values of the body’s movement can be computed in real time by
an embedded processor, which reduces the processing time;
• Their cost is relatively low with respect to 3-D movement analysis systems
composed of several high-speed video cameras, for instance.
Conversely, the use of inertial sensors requires some caution because the
kinematic parameters (i.e., linear velocities, orientations, and positions) computed
from their measurements are subjected to different error sources like integration
drift [47, 48, 57, 60, 62]. It is thus necessary to regularly measure their position and
their orientation within the Earth magnetic ﬁeld using magnetometers. But, in turn,
magnetic sensors have a drawback: they are sensitive to the environment magnetic
disturbances due to the proximity of magnetic sources or metallic objects [63].
The reasons for using inertial sensors for VIPs’ navigation can be diverse, as
well as the kinematic parameters to be computed from these measurements.
Naqvib et al. [64] developed a pedometer based on the accelerometer embedded
in most smartphones. The user placed the smartphone at the waist level, typically in
a pocket, and a program used the accelerometer signal for detecting and counting
the user’s steps. The steps count is then multiplied by the average step length to
compute the distance traveled by the user, which is a very important information in
a navigation system. This system is interesting given the low material constraints (a
smartphone in a pocket) and the use of an accelerometer; furthermore, it can equally
be used indoors or outdoors. However, it needs a good estimate of the average step
length, and it might be affected by an irregular gait. It thus quickly accumulates
error so that it cannot be conﬁdently used alone for tracking the displacement of a
VIP.
Zheng et al. [65] used inertial sensors in a shoe-mounted conﬁguration in an
indoor pedestrian navigation system. This system uses zero-velocity updates to
correct the drift due to time integration of inertial sensors data. It also uses an
extended Kalman ﬁlter to estimate and to correct the error parameters of the sensors.
The ﬁlter uses the stance phase—when the foot is lying on the ground—to get the
490
G. Motta et al.

error parameters of the IMU, which are then used to correct the computed position
and heading. This technique provides the self-reliance of inertial sensors while
allowing a good accuracy that is less than 1% of the total traveled distance.
However, the foot-mounted position of the IMU can limit the usability of this
technique for VIPs’ displacements.
2.2.3
Collaborative Elaboration of the Journey Path
Guiding the user to his or her destination requires a path to be computed. The
previously seen methods for localization and tracking will then be applied to know
the position of the user with respect to this path and guide her or him accordingly.
The quality of the paths users will have to follow is very important for them. In
order to globally enhance this quality, to build computation rules of a good path,
and to help the user to choose the path that suits him or her, some form of com-
munication between the user and the path computation system is required.
The goal of the path computation system is to ﬁnd the shortest and most secured
path that should fulﬁll several criteria:
• Dangers should be avoided as much as possible;
• The clutter level of the path (density of obstacles on the path);
• The presence of assistive devices (e.g., tactile surface paving) or guiding ele-
ments along the path (e.g., straight walls, curbs).
The route should also take into account the precision of localization sensors: if
information relative to their performance or to the required processing power along
the path is known, it should be used in the optimization process. In particular, paths
where the system is likely to lose track of its localization, as well as those without
wireless network coverage, should be avoided.
The collaboration will take place at two levels:
• Between the user and the system in general;
• Between the local server (e.g., smartphone) and the cloud back end.
Short- and long-term components are included in both contexts. In the ﬁrst one,
the short-term collaboration is the proposition/revision/acceptance mechanism: that
way, users can reject paths that they know they will not like, and it increases the
Fig. 11 Operating principle of a one-axis accelerometer
Overview of Smart White Canes: Connected Smart Cane …
491

conﬁdence they have in the results given by the smart stick. On a longer term, the
system is able to learn the user’s speciﬁc habits and tastes to directly propose higher
quality solutions. Regarding collaboration between the local server (smartphone)
and the global cloud back end, the short-term component is the communication of
map, trafﬁc, accidents, and other contextual information related to the traveled
locations. The long-term component is the compilation of individual preferences
from local servers to infer global preferences of the users.
Man–Machine Collaboration
During trips, communication between the user and the system is essential for the
latter to guide the former. These communications can be done through vocal
synthesis on one hand, and vocal recognition on the other hand, as it avoids the cost
and complexity of a keyboard and Braille display.
At start the user must inform the system about his destination and let the system
localize his or her initial point. During the displacement, the system provides
directives that are important for reaching the target place at strategic mobility
spatio-temporal points (e.g., changing of direction or drift warnings or conﬁrmation
of the progress). Indeed, the VIP’s feeling of safety requires that the system informs
him or her regularly, but sparingly, about the journey progress.
Essentially, four constraints apply on the guidance instructions:
• They must be steadily output;
• They must be clear (messages should not contain any explicit or implicit
ambiguity).
• Instructions will be generated in terms the VIP has learnt in mobility classes
(terms and units);
• System must inform the VIP when she or he has reached the target.
Gaunet and Briffault [66] have worked on the design of rules suited to guidance
of visually impaired and blind people in outdoor environments. They have analyzed
route descriptions given by blind pedestrians to infer guidance rules. They found
that a limited set of functions can be used to guide blind people, for instance,
describing the location and orientation of the user, describing an intersection,
crosswalks, etc. However, this work should be extended with locomotion teachers.
Man–Man Collaboration
In some circumstances, navigation algorithms can be too limited to ensure the
safety and satisfaction of a VIP. It can happen because the estimated position and
orientation of the smart cane has drifted and reached large-scale errors in guiding. It
can also happen because the system detects by itself that it cannot ensure sufﬁcient
guiding accuracy in an area that will be traversed, because of the lack of meaningful
492
G. Motta et al.

cues to extract from the environment, or simply because the system battery is
discharged. Therefore, the smart cane might not work correctly, or be inefﬁcient for
a speciﬁc task. In these cases, direct assistance of a human operator should be
provided.
The video feed from the smart cane’s camera must thus be transmitted to the
operator, who will guide the user according to what she or he sees—as if the
operator were giving directions while being near the user. With good enough image
quality and latency, an operator should be able to give correct instructions. Besides,
it could be reassuring for users to be guided by a human being instead of by a
machine. Furthermore, knowing that it is possible to talk with someone could help
breaking potential loneliness or anguish.
People giving directions can be family members, friends, volunteers, or pro-
fessionals. Because being remotely guided means that the VIP must have a very
high conﬁdence in the guide, friends, and family should be solicited ﬁrst; then,
volunteers, and, ﬁnally, professionals. To build up trust, successful associations
between a user and a guide should be maintained as much as possible. The tool used
by operators would preferably be a mobile application displaying a map together
with the video. That way, volunteers could give assistance in various situations,
without being forced to dedicate themselves to this task.
2.2.4
Environment Perception
The eETA should provide information on the surrounding environment so that it
will help VIPs to enhance their ability to perceive it (Fig. 12). Going out for VIP
should become a new social and leisure activity.
Fig. 12 Overview of the environmental perception functionalities of the eETA
Overview of Smart White Canes: Connected Smart Cane …
493

Street Crossing and Assistance
Street crossing is one of the major problems for VIPs for several reasons: estimation
of vehicles’ (cars, bicycles, etc.) motion with respect to their own motion and fast
real-time ego-perception capabilities; localization of pedestrian crossing; and
mobility cues changes. There is a possibility that a driver does not see a person
crossing and does not stop, even if the trafﬁc light is red. Sighted people can spot
such a vehicle from a distance; however, it is difﬁcult or even impossible for VIPs
to do the same. Moreover, sighted people can estimate the time before a vehicle
arrives at the location they want to cross, and decide whether to cross or not based
on this information; VIP can hardly do it. Therefore, it is much more dangerous for
the latter to cross roads lonely, and even if there is no danger, it is likely that they
will not feel at ease crossing. In order to get enough conﬁdence to cross a road, a
VIP may spend a lot of time analyzing the trafﬁc and waiting for the absence of
vehicles. In some areas, with slow but continuous trafﬁc, it even may be impossible
for VIP to cross [67]. Crossing a road may induce several problems:
• Pedestrian crossings may be difﬁcult to locate, especially when there are no
tactile guidance paths. Therefore, VIPs use environmental clues, like the noise
of streets intersection. Indeed, such noise is speciﬁc, learnt during the mobility
classes and recognized by VIPs;
• Crossing a street outside pedestrian crossings is extremely dangerous for VIPs.
Indeed, they can hardly ﬁnd suitable locations for crossing: for instance, how
may a blind person know that the road is turning 15 m after the place where she
or he is crossing? Similarly, it may be difﬁcult or impossible to know the speed
limit on a road;
• Crossing a road may induce a loss of orientation. In some conﬁgurations, the
physical properties of buildings surfaces change before and after crossing. This
is an extremely confusing situation, as many VIPs with speciﬁc ear air pressure
capability (named “mass perception”) use walls as a cue to follow;
• It is also possible that the direction of the VIP changes while crossing a road,
especially in complex conﬁgurations (e.g., with an island in the middle).
Hence, crossing assistance should be provided in three ways:
• Localization of pedestrian crossings;
• Identiﬁcation of the best moment to cross safely;
• Helping the user to keep a straight direction.
Systems to help VIP in these three aspects have been developed. Wang and Tian
[68] used an RGBD camera to detect both stairs and pedestrian crosswalks. They
use RGB images to indiscriminately recognize them (as they both appear as hori-
zontal lines), and then categorize them as up-going stairs, down-going stairs, or
crosswalk according to the depth information of the camera, with the help of an
SVM classiﬁer.
494
G. Motta et al.

Ivanchenko et al. [69] developed a system that can detect “zebra” pedestrian
crossings and another one that helps the user keeping the right direction along
two-stripe crosswalks [70]. Both functionalities run on a smartphone, processing
images from the onboard camera. Zebra crosswalk detection uses multiple cues to
recognize the parallel bands of crosswalks with a good accuracy; it signals users
with an audio tone when they are aligned with the crosswalk. The two-line
crosswalk alignment system makes use of the smartphone accelerometer to detect
the horizon and only select the lines on the ground (i.e., below the horizon), and
then detects the presence of strong parallel lines. When a single line is detected, the
smartphone emits a low pitch noise, and when two lines are detected, the smart-
phone emits a high pitch noise: this way, the user can ﬁnd the direction of a
crosswalk.
Speciﬁc Signage Detection
Signage is a prime example of information embedded in the environment. Indeed,
the purpose of signage is to communicate information to a speciﬁc population.
Therefore, it can be of high utility to communicate the contents of signage elements
to VIPs who cannot or hardly see them.
Which particular signage must be detected and described should be selected on
the basis of the context at that time. This context has several facets: it includes what
is generally useful for users, which can be statically deﬁned or statistically inferred.
It also includes personal preferences of users, which they can input into the system,
or which can once again be statistically inferred. More importantly maybe, it
includes the current goal and desires of the users (i.e., what they are looking for),
which can also be input in the system. Speciﬁc signage could be elaborated in order
to provide data useful for VIPs’ journey progress, which may be already existing
signage or objects (useful for sighted), but also new objects, which are helpful to
VIPs only.
Wang and Tian [71] worked on a framework to recognize signage and doors. To
spare computation time, they ﬁrst computed a saliency map to ﬁnd areas having the
highest probabilities to feature the searched elements. These areas are then scanned
with a sliding window, in which a bipartite graph matching algorithm is used to
compare the content of the window to the signage of interest. In their experiments,
these authors have tested their software to detect open/close door elevator buttons,
men/women/disabled bathroom pictograms, and direction arrows.
Detection of Speciﬁc Objects in the Vicinity of VIPs
The identiﬁcation or recognition of some objects can be useful; the information can
either directly be communicated to the user, or be used in the system to compute or
reﬁne information. Indoors, doors are very important for users, as they are the main
Overview of Smart White Canes: Connected Smart Cane …
495

way of circulating in a building. Other objects (e.g., furniture, appliances, electronic
devices) can also be of interest, depending on what the VIP wants to do. Detection
of any object can be interesting: a broader detection range implies a broader range
of situations that can be attended; hence, more freedom. Outdoors, apart from
signage and obstacles, elements of street furniture (e.g., benches, post boxes, bus or
tramway stations, bins) can be useful. Both indoors and outdoors, in an unknown
context—where an assistance device is the most useful—it is likely that the main
goal is the navigation to a speciﬁc location in the building (e.g., airport, mall,
public, or private buildings).
For indirect use (i.e., when the information is not communicated to the user but
used in an internal process) several types of objects are interesting. First, all objects
referenced on the map can be used to compute or correct the localization, acting as
references. This usage is strongly dependent on the richness of the map. All objects
cannot be referenced on a map: most of them are very likely to move or disappear.
Only ﬁxed objects should be considered; indoors, this should particularly be the
case with doors, and outdoors, the ﬁxed street furniture. Second, the function of the
location where the VIP is can be inferred from the presence of objects in a room: for
instance, is it an ofﬁce, a bathroom, a living room…
Several sensing mechanisms may allow detection and recognition of objects
around the user. Pattern recognition is the main method used by sighted people to
recognize objects. However, this approach is scientiﬁcally difﬁcult to reproduce and
of high computational complexity. Another method is to use wireless network
messages to identify objects. Without a heavy added infrastructure (i.e., putting an
identiﬁcation chip on every object), it is only possible for “smart” objects, of which
number should grow steadily in the next coming years [72]. The previous paragraph
brieﬂy exposed how objects referenced on the map can give an insight on the
position and orientation of the smart stick when they are detected. The opposite is
also possible, that is, using map data about locations to detect objects. In particular,
this can be used in information fusion, to get more conﬁdence in the results of
image or radio detection.
Tian et al. [73] have developed a method implemented in a program that can
recognize doors. For that purpose, they used the following model: doors are
composed of four corners and four lines of which two are parallel, and they are
recessing inside walls (in opposition to furniture that is protruding from walls). As it
is unlikely that a VIP is able to fully frame a door with a camera, since he or she is
not able to see it, Tian et al. took the partial visibility of doors into account.
Yi et al. [74] proposed to use both a network of ﬁxed cameras distributed in the
rooms of a home and a camera held by the user. The user informs by speech the
system that she or he is looking for a speciﬁc item, whose appearance has previ-
ously been learned. Then, all ﬁxed cameras search for that item, and, when one
ﬁnds it, its location (e.g., “in the kitchen”) is reported. The user can thereafter go
there to search more precisely the object with the handheld camera.
496
G. Motta et al.

Transports Schedule Communication
Even for sighted people, using public transportation can sometimes be difﬁcult; so,
for VIPs, it is often nearly impossible. To be able to use a transport, there are
several steps:
• Users must locate a bus, tramway, or subway station;
• They must reach the platform;
• They must know which stations are included in the line;
• They must know the timetable of the transport;
• They must have a travel ticket (in most countries);
• They must be physically able to board the vehicle.
The ﬁrst and second steps are mainly managed by the localization functionality.
The complexity of the second step greatly varies, both in nature and in magnitude:
discriminating the two sides of a tramway station may be difﬁcult—in particular if
localization relies on GPS—and guiding a blind user in the corridors of a convo-
luted multi-ﬂoor subway hub may also be challenging. The fourth step has two
levels: the ﬁrst one is the knowledge of the time until the next transport vehicle
arrives in the station; this information is often provided to travelers on a screen, and
is thus not accessible to most VIPs. The second level is the knowledge of the whole
timetable, which is useful in planning phases. The order of these steps is not ﬁxed
and depends on the context: someone may want to know the timetables and location
to make plans before traveling, but someone else may decide things on spot.
As said before, the two ﬁrst steps are mostly within the purview of the navi-
gation functionality. The third and fourth steps are about communication of line
information to the user. Essentially, two ways are available for the smart cane to
access transport line information: by the Internet or directly from the station. In the
second case, two methods exist: if possible, the smart stick will connect wirelessly
to the station and request the timetable. Hopefully, in the current context of
development of intelligent transport systems, it is likely that both vehicles and
stations will be more and more capable. In particular, it could mean that the stations
would provide wireless local services to users, through IEEE 802.15.4 or Bluetooth
Low Energy for instance. If wireless connection to the station were not possible, the
smart stick could use image processing to extract information from the visual
displays in the station. This solution is much more complex, and would certainly
require other information to be efﬁcient. A ﬁrst problem would be to localize the
display panel; for that, a model of the station could be used together with a precise
localization of the stick and of the station, to help the user pointing the camera
toward the panel. In a second time, an a priori model of the timetable or line
description displays could be used to efﬁciently extract data from them.
Overview of Smart White Canes: Connected Smart Cane …
497

Text Reading
Textual information is usually ubiquitous as it is displayed everywhere in modern
environments, be it on signage, on informative panels (e.g., in restaurants), or on
objects. More often than not, it is not possible to obtain this information through
another mean than vision. And even when the information is also written in braille,
or can be automatically read by a device, VIPs do not necessarily have the
knowledge that information is available, and they need to move their camera close
to the text to read it. Functionality designed to replace the user’s ability to read texts
would thus be welcomed.
The only sensor able to retrieve all textual information is a camera. Fortunately,
a lot of work has been done in this area, since it is useful in many ﬁelds (e.g.,
document scanning, license plate recognition) [75]. However, there are additional
difﬁculties in the case of assistance tools for VIPs:
• The environment is uncontrolled (i.e., the text can be present on any support or
background);
• The position and orientation of the camera are uncontrolled (i.e., the user does
not know where to orient the camera and will probably not keep a steady
orientation);
• The structure of texts is uncontrolled (i.e., how to know if blocks of text are
related, and how to communicate efﬁciently the contents of several related or
independent blocks of text).
Yi and Tian [76] have developed a novel text recognition method in the context of
assistance to VIP and blind people. This method works in two phases: ﬁrst, text
regions in the pictures are detected by segmenting images into regions with unique
colors, and then searching for horizontal alignment inside the regions—as it appears
that most texts are aligned and have constant colors. The second phase is the recog-
nition of characters in the text. To do it, Yi and Tian use the Harris, MSER, dense, and
random key point extractors and histograms of oriented gradients (HOG) to create
feature vectors; these vectors are then aggregated using a Bag-of-Words
(BOW) model and a Gaussian Mixture Model (GMM) to create character descrip-
tors. The system has been implemented as a smartphone application and was able to
recognize texts in testing datasets with a better accuracy rate than most other solutions.
Locational Context Information
VIPs would greatly beneﬁt from general, contextual knowledge about the places
they visit. This contextual knowledge may be of several natures:
• The domain of the location: public or private;
• The nature/function of the location: public square, kitchen, corridor, subway
station, etc.;
• More general knowledge about the location: history, trivia.
498
G. Motta et al.

Such knowledge would have several beneﬁts. First, it gives information to the
users about what they can expect around them, limiting surprises; in some cases, it
can reduce the probability of dangers (e.g., walking on a road without curbs).
Second, it provides spatial feedback, indicating to VIPs whether they are on the
right path, by communicating mobility cues that are speciﬁc to locations and that
VIPs can recognize. Third, it may help people remembering locations by attaching a
meaning to them, which is normally an easier and more efﬁcient memory process
than learning by heart.
Locational context information can be stored in various geographic databases or
inferred from other data. These two methods—storing locational information or
inferring it—have different complexities. The ﬁrst method is simpler in the front
end—accessing a database—than in the back end—the database either needs to be
ﬁlled by hand or information extracted from public, open data sources that can
reasonably be assumed to be trustworthy. The complexity of the second method
mainly lies in the front end. Indeed, in that case, the working principle is akin to that
of an expert system: rules are deﬁned, evidences are collected from sensor data and
known information, and, using these, an inference engine can ﬁnd new information
—in our case, about a location. The rules would be of the form “If there is a fridge
nearby, then the room is a kitchen with a conﬁdence of 60%” [77]. The choice
between methods is not binary, and the smart cane could take advantage of both, as
they have different qualities and application domains. With sufﬁcient information, it
should be possible for the system to try to guess the function of a location and
communicate to a user.
Human Detection
Because humans are social beings, interactions, and communication with other
individuals are of prime importance. People tend to change their behavior in the
presence of others. Therefore, knowledge about the number and general attitude of
people can be beneﬁcial for VIPs.
Furthermore, communication between people comprises not only words, but also
nonverbal cues, like facial expressions, body posture, gestures, and gaze.
Unfortunately, for VIPs, a very signiﬁcant part of these nonverbal cues are normally
acquired through vision. In a focused context (e.g., a conversation), it would be
useful for the smart cane to communicate to its user some of the visual nonverbal
cues of the interlocutor.
Another problem to be addressed is the identiﬁcation of speciﬁc characteristics
of people. In particular, some details may allow recognizing people whose task is to
help others. It is common to seek assistance in unfamiliar locations; for blind and
VIP, it is twice as important, since they have much more difﬁculties adapting to
unfamiliar settings. As, usually, such people can only be discriminated from the
Overview of Smart White Canes: Connected Smart Cane …
499

others from visual cues (e.g., badge or uniform), VIPs often cannot independently
locate them.
Image processing is the method with the highest potential in this domain. It has
been extensively used for facial expression recognition [78] and for gaze tracking
[79]. It has also been used for clothing recognition [80]. Other sensors can be used
in the required estimations: a microphone and signal processing can give insights on
the number of people around the user [81]; a depth camera system can provide
meaningful information about gestures and body posture [82].
2.3
System Constraints
System constraints globally apply to all the smart stick functionalities, from hard-
ware to software.
2.3.1
Usability
The purpose of the smart cane is to allow VIP to move around like sighted people
do. The cane is, essentially, a tool that provides freedom (including safety) and
warns others about the fragility of its owner. Hence, the eETA should not make its
user feel constrained, which would cancel its beneﬁts. This means that this device
should be light enough, have a long lifetime, and be dependable.
The physical efforts of the user should be limited to the minimum:
• Carried weight should be minimal;
• Required motions should be limited;
• Carried weight should be placed where it stresses the least (e.g., close to the
center of mass of the user).
The eETA lifetime should be long enough to allow continuous use in difﬁcult
contexts. The device should be able to run for at least 12 h, approximately a
working day, without being charged. In order to reach such a long running lifetime,
energetically costly components and operations should only be switched xxxON
when needed (sustainable design).
Other characteristics linked to cane management (e.g., pliability, maintenance)
should be established with end users.
2.3.2
Dependability
Smart cane users will only trust it, if it is dependable. Dependability comprises six
different attributes [83]:
500
G. Motta et al.

• Availability;
• Reliability;
• Safety;
• Conﬁdentiality;
• Integrity; and
• Maintainability.
Availability designates the readiness for correct service of the system. It means
that in spite of the environmental difﬁculties, the cane must be able to start up and
provide reliable obstacle detection functionality. More generally, it should be able
to run a set of tests, to determine how much functionality can be provided to the
user with acceptable conﬁdence, and to detect faulty components in the system.
In our case, the system has three physical components (smart cane front end,
smartphone, cloud back end), which should be able to start up, initialize, and be
ready for use quickly when the user wants to use the eETA. Due to the complexity
of this device, it is likely that running in a fully functional mode will not be possible
all the time; for instance, the system may be used in locations without cellular
network coverage. In these places, the smart cane will not be able to connect to the
cloud back end, to download maps or to stream video to a remote operator.
Basically, the only possible functionality of the smart cane will be obstacle
detection.
Reliability means continuity of correct service. As well as self-testing at startup
time, the system should be able to adapt to new conditions especially if they
worsen. For instance, it should be able to handle transitions from areas covered by
GPS to areas without coverage. Essentially, the functionalities should be able to
provide correct accuracies as far as allowed by the context. In particular, the esti-
mated localization of the user should remain correct and should not gradually drift
over time.
Ensuring the safety of the user is a very tricky task. Indeed, the range of contexts
that can be encountered is not limited; and any of these contexts can present threats.
Even for able-bodied people, it is not possible to provide complete safety in a
controlled or uncontrolled environment. But dangers induced by the device should
be avoided at all costs. These dangers have roots in three sources. First, the usage of
a smart cane can lead—in an ideal world—to trips to more unfamiliar locations,
inducing new dangers (e.g., traveling in a foreign country where driving rules are
different). Second, it can increase the frequency of trips outside of safe zones, which
mechanically increases the probability of having an accident. Third, if the smart
stick appears to work well and reliably, the user may rely on the device to an
important extent. In such a case, it is possible that the user lessens its caution level
—putting him in danger in cases where the cane does not detect the hazard. In order
to increase safety, these three sources of dangers should be mitigated, while giving
the smart cane capacities to avoid the “normal” hazardous situations.
Conﬁdentiality is not a trivial problem. On one hand, the smart cane is working
with personal information (user proﬁle): trips of the users, images of their envi-
ronments, preferences, etc.; on the other hand, by its very nature, the smart cane is
Overview of Smart White Canes: Connected Smart Cane …
501

connected to external networks, allowing attacks. Furthermore, there are several
gates to the system, since it spans over several subsystems, both physically and
logically. Information may leak in the cloud back end, from the smartphone, or in
the network between them. Unauthorized disclosures could also take place when
the device communicates with smart objects or intelligent transport systems (ITS).
Both physical and software integrity are necessary, at a high level: if the smart
cane breaks easily (as an object), the user will not be able to use it anymore and will
probably regard it as useless. Likewise, if the software blocks durably, the assistive
functions cannot be recovered by rebooting.
The last attribute that makes up dependability is maintainability. The smart cane
system should be able to keep in a good state without direct human intervention for
as long as possible. Outside hardware modiﬁcations, it should be possible to per-
form all maintenance operations remotely and seamlessly, since it cannot be ex-
pected that all VIPs can work with software.
2.4
Sustainable Design and Development
Objectives of such design are twofold:
(a) human: a design that matches
• user expectations (weight, maneuverability, adaptation to different assis-
tance levels (environment, age), wearability, aesthetics);
• required level of conﬁdentially (see above).
(b) technical: minimize the energy consumption, use of low-cost components,
activation/deactivation of components, seamless update of software (ubiquity).
3
Local MIMO Smart Cane Prototype: Software
and Hardware
3.1
Hardware Architecture
The Local MIMO Smart Cane (LMSC), whose features have been presented earlier
in this section, is a connected embedded multi-sensor system (or embedded sensor
node or sensor node or node). These nodes are used in Embedded Wireless Sensor
Networks (EWSNs), Internet of Things (IoT), and Cyber Physical Systems
(CPS) research domains. A node is connected to sensors that collect information
about its environment and it communicates with neighbor nodes or with a sink node
over wireless network. Sensor nodes must meet many constraints: they must be low
502
G. Motta et al.

cost, consume little energy (e.g., more than 8 hours of battery life for the LMSC),
and be dependable and reconﬁgurable.
Currently, most of existing nodes are connected to few sensors and are built on
single-core architecture, which is made up of four basic units (Fig. 13): a sensing
unit, a processing unit, a transceiver unit, and a power unit [84].
The sensing unit collects environmental analog signal and converts it to digital
data. The processing unit executes the operating system and the application. The
transceiver unit connects the node to the wireless network. The power unit supplies
power to the other units and its features deﬁne the lifetime of the node.
Single-core architectures will, however, soon be outdated. Indeed, since their
processing capabilities are exceeded [85, 86], emerging WSN applications use more
and more sensors, including multimedia sensor like a camera. Nodes have
increasingly heterogeneous data to process and to transmit in real time.
The LMSC belongs to this new generation of WSN applications. It embeds
many heterogeneous sensors for giving information about their environment to
VIPs: obstacles, Points Of Interest (POI), direction to move, etc. It must store,
process, and fuse in real time all the data originating from its heterogeneous sen-
sors. It must also be fault tolerant.
In order to satisfy the needs of emerging WSN applications, researchers have
recently begun to work on multi-core architectures for sensor nodes. Notice that if
the number of cores present in the processor is high, we speak of many-core
architecture. If the cores are physically separated, we speak of multi-CPU
architecture.
A multi-core architecture (Fig. 14) is very similar to the single-core architecture.
If we look at the multi-core architecture introduced by Munir et al. [87], we can see
that the main difference is the use of a multi-core processor instead of a single-core
processor.
The main interest of multi-core architecture is that multi-core nodes can run
applications requiring a high computational power while achieving energy savings
[88–90]. Some of their most interesting new characteristics are the following:
• As a multi-core node has more computing power, it can reduce transmissions by
processing locally some parts of the sensed data;
• Several low-powered cores working in parallel consume less energy than an
equivalent single-core [91];
• It is possible to completely switch off a core when it is not used to minimize
energy consumption;
• A multi-core sensor node is more robust than a single-core sensor node: if one of
the cores is broken down, the others can continue to run in degraded mode.
For our LMSC prototype we have chosen to use an asymmetric ON/OFF
multi-CPU architecture (Fig. 15).
Overview of Smart White Canes: Connected Smart Cane …
503

Fig. 13 Architecture of a single-core node
Fig. 14 Standard node multi-core architecture
504
G. Motta et al.

The LMSC prototype is composed of three sensing units connected through
Ethernet and ZigBee. Most of the time only one of these sensing units is active, but
when the system needs more computing power or more memory, the three units can
work in parallel.
A sensing unit is composed of a Raspberry Pi [92], an iLive module [93], and a
cluster of sensors (including accelerometers, camera, compass, gyroscopes, tem-
perature, infrared, laser, light and ultrasonic sensors, and wheel encoder). Some
possible technologies to be integrated in any cluster of sensors of an eETA and their
key features are listed in Table 1.
The Raspberry Pi is connected to the camera and to the earphones. It is in charge
of tasks that need lots of computations, as, for instance, complex fusion algorithms
for decision-making, image processing, speech processing, Wi-Fi, or Bluetooth
communication with the smartphone.
The iLive module is a low-power consumption sensor node developed by the
SMIR group of LIMOS UMR 6158 CNRS [90].
We use a cluster of sensors to increase the robustness of results given by our
LMSC. One sensor is not enough to provide good results because no sensor is
100% reliable (see Table 1). For obstacles detection, for instance, we mainly use
four sensors: camera, infrared, laser, and ultrasonic sensors. Temperature and light
sensors are used in addition to conﬁrm the results given by these sensors.
Fig. 15 LMSC prototype and its asymmetric multi-CPU architecture
Overview of Smart White Canes: Connected Smart Cane …
505

Table 1 Key features of the different technologies that may be used to implement an eETA
Key components
Functionality
Response
time
Cost
($)
Form
factor
Energy
consumption
Accuracy
Range
US
Obst. detection
*50 ms
*25
★★☆
★★★
★
15 cm–6 m
IR
Obst. detection
*25 ms
*10
★★
★
☆
1–5.5 m
Laser
Obst. detection
*30 ms
*350
☆
☆
★★☆
0–50 m
High-speed
laser
Obst. detection
*30 µs
*450
★
☆
★★☆
0–50 m
Laser range ﬁnder
Obst. detection
*200 ms
*100
★
☆
★★☆
15 cm–2 m
Camera module
Obst. detection and recognition
*250 ms
*30
★★☆
☆
★☆
*10 m
USB camera
Obst. detection and recognition
*250 ms
*80
★★
☆
★☆
*10 m
Stereo camera
Obst. detection and recognition
*250 ms
*160
★★
☆
★★
*10 m
GPS
Loc., travel distance and speed
100 ms
*30
★★☆
★
☆
Everywhere
Magnetic rotary encoder
Travel distance
*96 µs
*10
★★☆
★
★★★
Everywhere
Gyroscope
Travel distance, navigation
*5 ms
*2
★★★
★★★
★
Everywhere
Accelerometer
Motion, speed, distance
*5 ms
*2
★★★
★★★
★
Everywhere
Compass
Navigation
*5 ms
*2
★★★
★★★
★
Everywhere
Light
Env. context-awareness
*100 ms
*3
★★★
★★★
★★★
Everywhere
Air temperature
Env. context-awareness
*1 s
*2
★★★
★★★
★★★
Everywhere
Humidity
Env. context-awareness
*1 s
*2
★★★
★★★
★★★
Everywhere
Air pressure
Env. context-awareness, location
(e.g., change of ﬂoor)
*300 ms
*2
★★★
★★★
★★★
Everywhere
Buzzer
Danger signaling
<1 ms
*2
★★★
★★★
★★★
Everywhere
Vibrator
Event signaling
<5 ms
*4
★★★
★
★★★
Everywhere
Low-power Wi-Fi
Network connection, localisation
*100 ms
*15
★★
★★★
☆
Indoor 10–20 m,
Outdoors 100 m
Bluetooth ZigBee and
6LoWPAN
Network connection, localisation
*100 ms
*10
★★★
★★★
☆
Indoor 10–20 m,
Outdoors 300 m
Each attribute of a component is roughly rated by a star number: the higher the number of stars, the more appropriate the component is to be used in an ETA device. Notice that
black stars (★) are better than white stars (☆)
506
G. Motta et al.

3.2
Software Architecture
The software architecture of sensor nodes is made up of three basic components: the
operating system (OS), the protocol stack, and the application, which are described
below.
3.2.1
Operating System
The operating system serves as a link between the hardware layer and the application
layer of a node. It provides the drivers of the hardware components and manages the
memory and the tasks. It can provide an API or middleware to facilitate the
development of applications. As already stated, sensor nodes are low resources
devices: they have a limited computing power, low energy reserves, and in general
are not real-time constrained. Therefore, the eETA cannot use classical operating
systems like Windows, Linux, or Mac OS X. Speciﬁc operating systems are needed.
Currently, there are three types of WSN operating systems: multithreaded,
event-driven, and hybrid-embedded systems. Most of WSN OS are multithreaded
or event-driven. Multithreading allows better exploitation of the available hardware
parallelism.
Multithreaded OS are real time and have a large memory footprint. Event-driven
systems are not real time but have a small memory footprint [94]. Some examples
of multithreaded OS are the following: MantisOS [95], DREAM [96], and
SDREAM [97]. Some examples of event-driven systems are the following: TinyOS
[98], Contiki [99], and SOS [100]. To try to enjoy the best of both worlds, few
hybrid OS have been developed. TinyOS with TOSTThread [101] and Contiki with
multithreading [99] can execute a real-time scheduler on the top of the event-driven
scheduler. MIROS [102] can execute a real-time scheduler in parallel of an event
scheduler.
All these operating systems have been introduced for mono-core architecture
sensor nodes. Today, no operating system is well adapted to multi-core architec-
tures: their schedulers are not designed for parallel programming and they are not
adapted to the context-aware mechanism. Only DREAM and MIROS offer some
mechanisms to develop fault-tolerant applications.
In conclusion, no current system exactly corresponds to the needs of the LMSC,
which explains why our prototype uses a hybrid system composed of a Xenomai
[103], a real-time LINUX (on Raspberry Pi), and a MIROS [102] real-time OS on
the iLive board [90].
3.2.2
Protocol Stack
The protocol stacks used on desktop computers (Fig. 16) are not well adapted to
nodes used in wireless sensor networks. Their footprints are too large and they
Overview of Smart White Canes: Connected Smart Cane …
507

consume too much energy for data transmission. In the last few years, many new
protocols and architectures have been developed to try to solve these problems:
6LoWPAN [104, 105], RPL [106], COAP [107], MQTT [108], and REST [109].
Our prototype uses a tiny Ethernet/Wi-Fi desktop computer stack in each
Raspberry Pi and a ZigBee sensor node stack in each iLive module.
3.2.3
Application to the Design of the Local
MIMO Smart Cane Prototype
The main role of the application running on the LMSC is to fetch data from the
sensors, the networks and the smartphone, and to process them to output the
expected guidance, obstacle or dangers alerts, and informational messages to the
VIP. When performing these tasks, the application has to satisfy real time,
dependability, and energy constraints. The chosen solution requires to carefully
design the application to control heterogeneous redundant and parallel hardware
according to the context. Context can be used to control the hardware in several
situations:
• If the application “has a doubt” about one result, it can compute it again using
other components. For instance, if a distance measured by an ultrasonic sensor is
not coherent with the measurement of the same distance by an infrared or laser
sensor, the software can start up the camera to process the image to know
whether there is an obstacle or not. Otherwise, the camera would normally be off
in order to save battery life.
• In situations where the system needs to perform heavy computations, it can
wake up one or several processors. Extrapolating the previous example, images
from the camera have to be processed in order to extract information of obstacle
presence. If it were executed on a very low-power microcontroller, the machine
vision algorithms would deteriorate the assistive device performances; the
Fig. 16 Typical sensor node protocol stack
508
G. Motta et al.

application should thus start one or more stronger processors, like those of the
Raspberry Pi (ARM).
• Finally, the application may need to wake up components in cases of failures.
All important vulnerable components being redundant, if one comes to fail, the
system can wake up another identical component and use it instead. One dif-
ﬁculty is the detection of failures; for instance, processors can regularly send
“heartbeats” (dummy messages) to each other to indicate that they are running
and not blocked. But other solutions may also be investigated.
Most of the tasks performed by the processors will be data or information fusion,
in one form or another. Data from the sensors are of different nature and quality;
they sometimes can be redundant or conﬂicting; in these cases, fusion can be used
to enhance accuracy or to obtain a better conﬁdence in the results. Data or infor-
mation fusion may provide additional value in three ways [110]:
• Complementarily: the different sensors or information sources describe different
parts of an identical quantity;
• Competitively: the sensors or information sources describe the exact same
quantity. This mode of fusion gives more accuracy;
• Cooperatively: the sensors or information sources are used to build more
complex information than what they describe individually.
The sensors embedded in the LMSC offer the ability to cover the three ﬁelds.
However, they inherently provide imperfect data (e.g., inaccuracy of measurement,
integration drift, missing data) related to their physical characteristics and appli-
cation environment. Among the existing information fusion methods, the most
popular for multi-sensor applications are probabilistic methods, evidence theory,
and fuzzy set theory, as they allow representation of imperfect information.
Probabilistic methods are generally based on Bayes’ rule and time series.
Bayesian inference characterizes likely reasoning under uncertainty. It uses earlier
observation to predict or rectify later observations, and models to reﬁne the estimate
as new data arrive. Kalman ﬁlters, Sequential Monte Carlo methods (SMC), and
Markov Chain Monte Carlo methods (MCMC) all rely on Bayesian inference.
Evidence theory, or Dempster-Schafer theory [111], is suitable for working on
uncertain data, as it represents all the possible “hesitations” between possible dis-
crete states of a system (e.g., an object can be a “cup”, a “glass”, or a “cup or
glass”). Rules are used to compute belief and plausibility measures for every
possible proposition; the interval between the two is the uncertainty over the
conﬁdence that the system has in the proposition. Rules exist to fuse belief and
plausibility of a proposition according to the information coming from different
sources.
These methods are not trivial to implement: Bayesian methods and Monte Carlo
methods in particular have high computing power requirements; this stands in
opposition with the battery life requirement of the LSMC. Active context-
awareness is a solution to that problem: when more power is required, the system
Overview of Smart White Canes: Connected Smart Cane …
509

can start up one or several powerful processors, and stop them when the result has
been obtained.
Dempster-Shafer is not directly applicable either: indeed, it works on the power
set of discrete possible states; consequently, computations have an exponential (O
(n)) complexity, lim
n!1 2n ¼ lim
n!1 n, with respect to the number of states of the
observed system (n). On the other side, as data from the sensors tend to be con-
tinuous, a compression of their information is required.
3.3
Case Studies
The static obstacle detection functionality has been implemented in the prototype of
SEES [35]. Six ultrasonic transducers, oriented in a semi-circle manner, were user
to detect obstacles in two ranges: 0–1 and 1–2 m. When an obstacle is detected, a
vibration coin motor alerts the user, with a different vibration frequency depending
on the sensed distance to the obstacle (“close” of “far”). Detection of obstacles has
been successfully tested from different distances (0.5, 1.5 m) and heights (0.4, 1,
1.8 m).
Localization experiments have been conducted with the prototype of SEES.
During early tests, only the wheel encoder and the gyroscope (which was ﬁrst
calibrated with one accelerometer) were used. Trajectories were computed by
integrating the distance measured by the wheel encoder and projected on the dis-
placement direction computed from the integrated angular velocity measured by the
rate gyroscope. This method proved to be little robust, because of drift error
appearing and growing quickly over time. In further tests, a particle ﬁlter was used
to fuse the data from these two sensors, plus those of ultrasonic sensors, as follows:
the predicted motions of particles were Gaussian samples around the trajectory
steps computed from the angular speed—measured by the rate gyroscope—and the
displacement—measured by the wheel encoder. Then the particles likelihoods were
computed with a simple model of ultrasonic propagation that relied on map
information. This method gave a higher accuracy than the former one; however, it
was demanding in resources and sensible to the quality of ultrasonic range mea-
surements [28].
A door detection functionality has also been developed for the SEES prototype
[28]. Hue–saturation–value (HSV) thresholds were used together with edge
detection to recognize doors in the laboratory building. This technique, combined
with the scaling down of images, allowed an acceptable detection accuracy while
requiring low computing resources: on a Raspberry Pi B, the processing of an
image took about 40 ms.
Linked to the door detection, a door handle and doorplate detection functionality
was implemented on the prototype. They both used the Oriented FAST and
Rotated BRIEF (ORB) feature detector. Because of the power required to run
detection using ORB, this method was only performed when a door was
510
G. Motta et al.

recognized. Furthermore, a full resolution picture of the door was used, instead of
the scaled-down version used to detect doors [28].
Experiments involving connected objects have also been conducted. iLive nodes
[90] were used to mimic smart objects, regularly broadcasting an identiﬁer with a
IEEE802.15.4 radio. The messages are sensed by the smart stick, which measures
the Received Signal Strength Indicator (RSSI) of the messages. Because of the
properties of electromagnetic waves, the RSSI decreases with the distance traveled
by the signal and with the traversed obstacles. Thus, it can be used as an indicator of
both distance and presence of walls between the transmitter and the receiver.
A simple inside/outside the room model, based on an empirical threshold, was used
to detect which smart objects were present in the room where the smart cane is.
4
Back End
4.1
Functional Speciﬁcation of a Global Architecture
Back end is a set of functions which gather, store, and structure information which
is used by the front-end system. Back end and front end communicate through
information services. Back-end information services can be triggered by the
mobility process monitored by the front-end sensors (e.g., next positioning while
walking indoor) or by an inquiry made by the user (e.g., destination). Hence, back
end is the source of stored information for the front end. Such information includes
the following:
• public data (e.g., street map),
• proprietary information (e.g., map of buildings), and
• personal information (e.g., the path followed by a speciﬁc user).
The above-mentioned back-end functions support both outdoor and indoor
mobilities. However, indoor and outdoor differ, and, therefore, they require speciﬁc
systems (Table 2). In general, the back-end system provides positioning and path
planning information to front end. The front-end system can therefore set the
position and guide with the highest precision the VIP user.
In the subsequent sections we illustrate architecture and algorithms of back-end
services, and provide foundations for their design and implementation. Speciﬁcally,
Sect. 4.2 addresses outdoor mobility, and Sect. 4.3 considers indoor mobility.
4.2
Back-End Functions for Outdoor Mobility
The back-end services provide event-awareness and accessible itinerary planning to
VIPs. To ensure safety, suggested itineraries skip harmful events (e.g., accidents,
Overview of Smart White Canes: Connected Smart Cane …
511

Table 2 Back-end typical technologies and main functions
Outdoor
Indoor
Comments
Dimensions
2D
3D
Buildings have ﬂoors. A key information at
which ﬂoor the user is
Positioning
technologies
GPS
Various technologies: magnetic
ﬁelds and/or wi-ﬁand/or computer
vision
GPS cannot be used
Hence, positioning should be based on the
relative location against a known point in
the building map
Various technologies can be used
individually or integrated
End-to-end
path
planning
service
Across roads, A* algorithm can be used for
walking. However, typical path planning
addresses an optimal combination of
available/sustainable transport routes (with
mobility speciﬁcities of the VIP)
Across ﬂoors path planning should
be based on optimized wayﬁnding
method as Ant Colony
Optimization (ACO)
Back-end system fetches obstacles on the
path and Front-end system (Smart Stick)
detects them in real time and dialogs with
the VIP user
Transport
management
service
Public/Private/shared/walking
Walking/elevators/spatial vehicles
(e.g., AGV)
Outdoor map stores routes of public/shared
transports.
Indoor map stores elevator position and
other key POI (stairs)
Event
management
service
Road/transport disruptions
Social events
Floor/elevator disruptions
Social events
Events have a space and time extension
512
G. Motta et al.

trafﬁc jam, and parade) and roads that are blocked. Finally, back-end services
provide analysis for municipalities and transit agencies (Fig. 17).
In order to support such services, the following data are integrated:
• Events extracted from
– Social network
– Web
– Citizen-sourcing systems
• Point of Interest (POI)
• Data on accessible mobility:
– Accessible map
– Accessibility of POIs
• Public transit data:
– General Transit Feed Speciﬁcation (GTFS): public transit information which
includes timetables, routes, stops, etc.
– GTFS-real-time feeds, e.g., delays and service alerts.
• Sensor data:
– GPS.
– Obstacle detection sensors.
4.2.1
Outdoor Map
Outdoor map is made of several layers, all implemented on OSM (Fig. 18); each
layer provides different information on the same space:
• basic map, i.e., the default local street map;
• event map, which displays location and duration of events extracted from
multiple data sources;
Fig. 17 Overview of back end on outdoor mobility
Overview of Smart White Canes: Connected Smart Cane …
513

• accessible map, which displays the accessibility of roads, and replaces the basic
map when an accessible itinerary planning request is received;
• POI map, showing POI information for VIP, such as accessibility.
Since maps cannot be seen by VIP, APIs (OpenTripPlanner) deliver RESTful
web services to front end.
4.2.2
Itinerary Planning
Itinerary planning includes two main services, i.e., (1) event-aware, which in turn
includes two modules, namely (1a) event extraction and (1b) trip planner; and
(2) accessible itinerary planning services. Let us shortly illustrate them:
Event Extraction Module
It mines events from multiple sources, as (1) online news or public information
services about trafﬁc, (2) social networks as tweets from Twitter, and 3)
picture-and-text feeds submitted from city issue management systems. The event
extraction process is shown in Fig. 19. Here below we illustrate the steps of data
collection, sematic analysis, and extraction.
Step 1: Data Collection
Data collection methods reﬂect the diversity of sources. Public trafﬁc news, which
describe events in terms of location, date, and duration, are crawled from relevant
websites. Tweets in real time, which include text, timestamp, user proﬁle and,
optionally, geo-location, are obtained by APIs. Finally, events from city issue
management systems are obtained by web services.
Fig. 18 Event aware itinerary planning
514
G. Motta et al.

Step 2: Data Pre-processing
In pre-processing relevant event news are identiﬁed in tweets (and other sources)
through the following steps:
• Tokenization;
• Elimination of punctuation, emoji expression, URL link;
• Change of text to lower case; and
• Part-of-Speech (POS) tagging by N-gram tagger;
• Word stemming by semantic analysis through NLTK (Natural Language Tool
Kit5) or alike methods.
Step 3: Event Classiﬁcation
Pre-processed texts are grouped. Trafﬁc related keywords are classiﬁed by TF–IDF
(Term Frequency and Inverse Document Frequency).
Step 4: Time and Location Extraction
First, named entities are recognized by semantic analysis module before extracting
attributes of events. For example, the software recognizes which part of the text
relates to time and/or location. AI rules and address dictionary, as well as proba-
bility estimation are used. Finally, Geo-Encoding API of OSM to associate names
and geographic coordinates.
Step 5: Event Normalization
It avoids duplication of events by comparing their attributes of time, geo-location,
text, and type. Finally, normalized events are to be used by Trip Planning module.
Data Collection 
from Crawler/API
Data Pre-
processing
Event 
Classification
Location & Time
Acquisition
Event Integration
Fig. 19 Event extraction process
5Natural Language Toolkit: http://www.nltk.org/.
Overview of Smart White Canes: Connected Smart Cane …
515

Trip Planning Module
It generates multi-modal itineraries that are aware of events and avoid obstacles. It
can be implemented on OpenTripPlanner (OTP6) or alike platforms. An enhanced
A* algorithm considers the impact of events; therefore, the generated itineraries
avoid harmful events that may affect itineraries. The overall process is shown in
Fig. 20.
The module receives trip time, departure, destination, etc. and it computes the
shortest path by pathﬁnding algorithm and by evaluating the impact of nearby
events (Fig. 20); of course the module supports accessible itineraries, which skip
steps or obstacles (Fig. 21) by collecting related data as negative POIs interest.
Steps include the following:
Fig. 20 Event-awareness itinerary planning
6OpenTripPlanner: http://www.opentripplanner.org/.
516
G. Motta et al.

• Find the shortest path from start to end;
• Find out the events along the path;
• If events are going on, check the geographic impact;
• If the impact is relevant, ﬁnd an itinerary route or re-compute, until the system
ﬁnds a feasible itinerary, which may be even null.
Figure 22 shows an event-aware and accessible itinerary, which is displayed by
front-end applications and web services.
4.2.3
POI Management
POIs are a key issue for VIPs. Indeed, a POI may be (a) accessible, i.e., entrance
and rooms without steps; (b) partially accessible, i.e., one step on entrance with a
height of 7 cm or less, and most of rooms are without steps; (c) not accessible, i.e.,
Fig. 21 Accessible trip planning
Fig. 22 Example of event-awareness and accessible trip planning
Overview of Smart White Canes: Connected Smart Cane …
517

entrance step and rooms not accessible; and (d) unknown. Wheelmap7 is a POI
information
source
for
wheelchair
users,
which
contains
information
on
steps/barriers networks. Figure 23 shows accessibility information on POIs.
POI information can be extracted also from generic social networks, as
Facebook and Foursquare, and be merged and normalized by clustering (by map-
ping datasets through a predeﬁned category-based decision tree), matching (by
calculating the Levenshtein distances between two POIs), and ranking (by check-in
counts information from Facebook and Foursquare).
4.3
Back-End Functions for Indoor Mobility
Back-end functions provide seamless support indoor/outdoor to any user. In par-
ticular, a real-time pedestrian navigation in buildings with multiple ﬂoors suggests
path and POIs. The architecture includes the following (see Fig. 24):
• Indoor Map: a proprietary information of the estate owner organization, which
provides static data for positioning and POIs (Maps may be on two/three
dimension and are obtained from blueprint or various data models).
• Indoor Positioning: positions the user in a ﬂoor within a building, and informs
the front-end services; given the constrains of indoor spaces, magnetic ﬁelds are
used.
Fig. 23 Accessibility of POIs
7Wheelmap: http://wheelmap.org.
518
G. Motta et al.

• Indoor Path Planning: calculates an optimal route for indoor navigation based on
a speciﬁc algorithm (Ant Colony Optimization) and forward directional
prompts, such as go straight, turn left, or right.
• POI management: gathers and stores information about POIs in a building, and
can link any kind of attachment, like speech clips, text clips, audio clips, etc.
4.3.1
Indoor Map Module
SVG (Scalable Vector Graphics) is used for the indoor map format and Open Street
Map (OSM) is used for the base map. SVG ﬁts very well since indoor environment
can be updated selectively. Figure 25 shows the indoor map creation process.
Finally, the integrated map is uploaded and stored. Metadata group has SVG ele-
ments in several layers (Fig. 26). Indoor Map module includes the following:
Fig. 24 Overall architecture of indoor navigation
Overview of Smart White Canes: Connected Smart Cane …
519

• Export OSM base map: determines the target building, sets latitude and longi-
tude, and extracts the OSM map of the building contour, which is exported by
JOSM [112];
• Convert OSM into base SVG by Maperitive [113];
• Convert CAD ﬂoor map into SVG: the DWG/DXF ﬂoor plan is converted into
SVG [114];
• Combine SVG base map and SVG ﬂoor map: following the predeﬁned SVG
indoor map metadata, the SVG base map and ﬂoor map are imported into
Inkscape [115] as one ﬁle. By integrating the SVG base map and SVG ﬂoor
plan, an organized SVG indoor map is created.
4.3.2
Indoor Positioning
Indoor positioning is based on Magnetic matching, which includes ofﬂine training
and online positioning. In ofﬂine training, collection and calibration of magnetic
data on landmarks and ﬂoor plans is the ﬁrst step. Such calibration removes
Fig. 25 SVG indoor map creation process
520
G. Motta et al.

deterministic sensor errors. Then, the server stores magnetic ﬁelds and positions
them on the map. Online positioning matches magnetic ﬁnger prints gathered by the
sensor against the stored map. The sensor transmits a sequence of measurements,
and the server replies the best estimated position (cf. Fig. 27).
Fig. 26 SVG indoor map metadata deﬁnition
Fig. 27 Schematic diagram of geomagnetic positioning
Overview of Smart White Canes: Connected Smart Cane …
521

4.3.3
Path Planning
Path planning supports the navigation across ﬂoors; it is based on an optimized ant
colony algorithm [116]. Ants crawl to avoid obstacles, in turn crawling is inﬂu-
enced by the intensity of the pheromone trails by preceding ants. In a given period,
the shorter the distance, the more pheromone left, the higher probability of the
shortest way. However, in the basic ant colony algorithm, ants go around and
around to ﬁnd the way. In our ant colony optimization (ACO), we connect by a
straight line the current point and the farthest point that does not touch a wall. When
we ﬁnd the farthest point that does not touch a wall, we continue to search within
next 200 points, until we ﬁnd only points that touch walls (cf. Fig. 28: black line is
the basic ant colony path, blue line is based on initial idea, and red line is further
optimized).
The solution has been tested on a building of University of Pavia. Figure 29
shows the example of the non-optimized trail (Fig. 29a) and optimized trail
(Fig. 29b).
In indoor spaces, ACO looks better than the classic A*. Some results are shown
in Table 3: the distance with ACO is shorter than with AC, with an average
improvement of 69.9%. The time cost of ACO is slightly higher but still acceptable.
We tested 10,000 times a trail and ACO always found the optimal path.
Fig. 28 Path comparison
Fig. 29 An example trail on indoor map: left (a), right (b)
522
G. Motta et al.

Table 3 Testing results of basic ant colony (AC) model and optimized (ACO) algorithms
No.
Start point
(x, y)
End point
(x, y)
Time duration of
AC (ms)
Time duration of
ACO (ms)
AC distance of
trail (pixels)
ACO distance of
trail (pixels)
Distance reduce
rate (%)
Time increase
rate (%)
A-1
(89, 98)
(268, 289)
327
429
1384
307
77.8
31.2
A-2
(241, 37)
(360, 170)
851
1020
1086
328
69.8
19.9
B-3
(172, 78)
(269, 224)
432
552
754
234
69.0
27.8
B-4
(141, 77)
(258, 278)
369
492
1189
337
71.7
33.3
C-5
(154, 82)
(275, 135)
281
333
684
231
66.2
18.5
C-6
(247, 214)
(326, 195)
337
381
835
269
67.8
13.0
D-7
(155, 84)
(274, 266)
477
568
777
291
62.5
19.0
D-8
(59, 577)
(419, 361)
436
653
1800
566
68.6
49.7
Aver.
NA
NA
438
553
1064
320
69.9
26.2
Overview of Smart White Canes: Connected Smart Cane …
523

4.3.4
Indoor POI Management
The overall architecture of Indoor POI management is shown in Fig. 30; it includes
the following:
• Data layer: its stores indoor POI information for business services.
• Business layer: business services run on a Web Server, to handle request,
operate on data bases, and provide results to users;
• Presentation layer where user can actively use the system.
In order to achieve a low cohesion, layers are integrated by data ﬂows. Figure 31
shows the screenshots of POI management system.
Fig. 30 Overall architecture of indoor POI management system
Fig. 31 A POI management screenshot
524
G. Motta et al.

4.4
Implementation and Deployment
Before discussing the deployment, let us summarize in very simple terms the
hardware conﬁguration of the front-end/back-end system. The overall system is
made of several interconnected elements, which play front end, back end or both
roles: cane, smartphone, communication network, back-end servers.
The cane provides a set of short range sensing functions and communicates with
the user as speciﬁed in Sect. 1.4.2. The cane, which is the key front-end element, is
interconnected with smartphone.
The smartphone enhances the cane functions. The smartphone provides
long-range sensing and support functions, and can play the twofold role of front end
and back end. As a back end, it can play server functions as path planning. Of
course, server functions are bounded by the inherent storage and computation
limits. However, those server functions are critical when network and cloud server
are not available. The wide range of roles played by the smartphone relies on the
very rich sensors of smartphone as magnetometer, accelerometer, compass, camera,
etc.
Communication network by different technologies interconnects smartphone and
servers; the network is critical in terms of availability, reliability, and scalability.
Actually, network context may include Open Outdoor (at least 4 satellites avail-
able), Challenged Outdoor (GNSS-hostile outdoor environment as urban canyon),
Light Indoor (availability of signal), and Deep Indoor (no satellite in view) [117]. In
Deep Indoor context, only WI-FI can be available. And in some case even WI-FI
will not be available. So the ability of smartphone to provide services anyway is a
key requirement; the system must work even in a downgraded way without net-
work. The scalability becomes an issue if the network is overloaded. Also in this
case, the ability of smartphone to provide services is a key.
Back-end servers, in cloud or in premises, store master data as maps, magnetic
ﬁngerprints, POIs, etc. They also provide related services to the smartphone.
In this section, we describe how front-end and back-end systems are deployed in
order to perform above-mentioned functions. At the end of this section, we describe
our practice in deploying services on cloud environment.
4.4.1
Implementation of Mobility Functions
Indoor and outdoor navigation APPs are deployed on smartphones and tablets.
Table 4 lists the implemented mobility functions and corresponding back-end
services, and involved data. The key features in implementation are also described.
The above-mentioned functions for indoor and outdoor mobility rely on a wide
range of data. To support the lifecycle of data collection, integration, processing,
analysis, and sharing, an event-driven SOA (Service Oriented Architecture) is
implemented. The architecture assembles heterogeneous data and open-source
techniques. The data are integrated as layers and can be shared by services.
Overview of Smart White Canes: Connected Smart Cane …
525

Table 4 The implemented front-end functions and their supported back-end services
Mobility
functions
Back-end services
Data
Key features in implementation
Indoor map
management
Map services
Raw indoor map
Map services support the following indoor map conversion functions:
1. OSM/CAD map to SVG 2D map
2. SVG 2D map to 3D map. (details in Sect. 4.3.1)
A web portal for map administrators manages the indoor maps. The indoor maps can be downloaded
ofﬂine from the local server into smart devices when no internet connection is available
Indoor
positioning
Positioning services,
indoor services
Indoor map (ﬂoors),
magnetic ﬁngerprints
Positioning services support the online position matching between ﬂoor plan and magnetic ﬁngerprints
(see Sect. 4.3.2). In ofﬂine positioning, a local positioning server in smart devices will synchronize the
data from remote servers. The magnetic ﬁngerprints are collected by magnetometer sensors and uploaded
to IndoorAtlas and dedicated servers
Indoor path
planning
Path planning
services
Indoor map (ﬂoor)
An Ant Colony Optimization (ACO) algorithm is implemented for indoor path planning (Sect. 4.3.3).
ACO doesn’t require the magnetic ﬁngerprints because it detects edges (walls) and accessible path on
map images. ACO can also run ofﬂine on smartphone when no internet connection is available
POI
management
POI services
POIs
POI services collect POIs from social networks through APIs (for outdoor POIs, see Sect. 4.2.3; for indoor
POIs, see Sect. 4.3.4). POI services provide a web portal to map administrators to create and edit POIs
AR guide
Path planning
services
Magnetic
ﬁngerprints,
accelerator values
Both outdoor and indoor navigation uses Vuforia AR SDK [118] to create AR guide functions, which
contains two main modules:
1. Guide pointer: the guide pointer shows the orientation during the trip. It changes its orientation when
values of geomagnetic ﬁngerprints and accelerometer change. The guide pointer and orientation is
calculated in smart devices
2. Object detection: objects en-route (e.g., buildings, rooms, shops, bus stops) can be detected and
highlighted in the camera view. The software also popups the information on highlighted objects. The
images and corresponding information of these objects are to be uploaded to the cloud server of Vuforia
using Vuforia AR SDK
Event
detection
Event services
Tweets, trafﬁc feeds
Event services are implemented by natural language processing and machine learning techniques to
detect trafﬁc events for outdoor navigation (see details in Section “Event extraction module”)
Outdoor path
planning
Path planning
services, mobility
tracking and
monitoring services
GPS data, GTFS,
Outdoor maps, trafﬁc
events
The outdoor path planning is implemented by A* algorithm and considers trafﬁc events to provide event
awareness path plans. The path plans are tracking by speciﬁc services and dynamically update en-route
when new events are detected (see Sect. 4.2.2)
526
G. Motta et al.

Figure 32 shows the data layers for outdoor and indoor mobilities that can be used
either individually or together with other data layers.
Figure 33 shows the implementation of the whole architecture and its technol-
ogy stack. The architecture manages back-end activities by events, which are
triggered by requests and the change of data. For example, in back end of outdoor
mobility, a detected event/an expired event will trigger the en-route trip
rescheduling, which involves multiple layers of data (i.e., map data, accessibility
data, event data, etc.) and multiple services (i.e., indoor/outdoor path planning
service, mobility analysis service).
Let us illustrate some key practices in optimizing the performance of the above
mobility functions.
1. Online and ofﬂine modes—the double servers: Table 4 shows that most of
mobility functions may run ofﬂine, without Internet connection. To enable
ofﬂine availability, a local server is implemented inside the mobile APP, which
contains the same basic functions of remote servers and synchronizes data with
them. The local server can also reduce the latency, e.g., geomagnetic posi-
tioning, object detection by camera. However, due to the limited local storage,
the local server cannot cache all the area, and data synchronization should be
complete before the user starts the trip. Finally, the load can be balanced on
double server. For example, with a low-performance device, the local server will
shift the computation to remote servers (if connected to the Internet); when the
load of remote server is heavy or the latency of Internet connection is high (i.e.,
larger than 100 ms, due to the minimal number of geomagnetic ﬁngerprints
required in one second is 10), the computation will shift to the local server
(ofﬂine mode) until new data have to be synchronized or uploaded.
2. Sensor data streams: sensor data streams are generated by assistant devices
(e.g., connected smart sticks, intelligent canes) and sensors on smart devices
(e.g., Camera, GPS, Bluetooth, Magnetometer), which require a high resampling
rate. In our case, the resampling rate of geomagnetic ﬁeld sensor is 100 Hz. To
reduce data transmission between front end and back end, a batch processing is
Fig. 32 Data layers for outdoor indoor mobility
Overview of Smart White Canes: Connected Smart Cane …
527

Fig. 33 The implementation of event-driven SOA for back end of outdoor/indoor mobility and its
technology stack
528
G. Motta et al.

performed to generate ﬁngerprints in a certain time interval with a relatively low
rate. However, still, at least 10 ﬁngerprints are generated and sent to the back
end per device per second for a smooth positioning. In order to process such
data streams with low latency, a Publisher/Subscriber model is to be imple-
mented in back end for Event Stream Processing (ESP). This model decouples
data producing and data consuming with Distributed Messaging System (DMS),
a middleware between data and data processing (e.g., RocketMQ [119]). Thanks
to Distributed Stream-based Processing Engine (DSPE, e.g., Apache Storm
[120]), the sensor data streams can be distributed processed in real time/near real
time (e.g., less than 10 ms). In our test, one 8-core 8G RAM server can process
up to 2000 sensor records (4 KB each record) per second, i.e., about 200 clients
are online at the same time.
4.4.2
Deployment of Back-end Servers
To support a scalable, consistent development and deployment for such architec-
ture, container technology (e.g., Docker [121]) is to be used and deployed on cloud
environment. The container technology reduces the burden of distributed deploy-
ment by automating the conﬁguration of processes, and providing a kind of con-
ﬁguration management system [121]. Of course, the cloud environment can scale
up the system performance for the distributed computing on sensor data streams in
an easy way, a rather essential feature in outdoor and indoor mobility systems.
References
1. WHO (2014) Visual impairment and blindess. http://www.who.int/mediacentre/factsheets/
fs282/en/. Accessed 13 April 2017
2. Manduchi R, Kurniawan S (2011) Mobility-related accidents experienced by people with
visual impairment. AER J Res Pract Vis Impairment and Blindness 4(2):44–54
3. Wiener W, Welsch R, Blasch B (eds) (2010) Foundation of orientation and mobility, volume
I: history and theory. American Foundation for the Blind, New York
4. Thinus-Blanc C, Gaunet F (1997) Representation of space in blind persons: vision as a
spatial sense? Psychol Bull 121:20–42. doi:10.1037/0033-2909.121.1.20
5. Lévesque (2008) Blindness, Technology and Haptics. Survey, Haptics Laboratory, Centre
for Intelligent Machines, McGill University, Montreal, Québec, Canada
6. Lahav O, Mioduser D (2003) A blind person’s cognitive mapping of new spaces using a
haptic virtual environment. J Res Spec Educ Needs 3:172–177. doi:10.1111/1471-3802.
00012
7. Briscoe R, Grush R (2015) Action-based theories of perception. In: Stanford encyclopedia
philosophy, pp 1–66
8. Foulke E (1982) Perception, cognition and the mobility of blind pedestrians. In: Potegal M
(ed) Spatial Abilities: Development and physiological foundations, Academic Press, San
Diego, CA, USA, pp 55-76
Overview of Smart White Canes: Connected Smart Cane …
529

9. Loomis JM, Klatzky RL, Golledge RG, Cicinelli JG, Pellegrino JW, Fry PA (1993)
Nonvisual navigation by blind and sighted: assessment of path integration ability. J Exp
Psychol Gen 122:73–91. doi:10.1037/0096-3445.122.1.73
10. Noordzij ML, Zuidhoek S, Postma A (2006) The inﬂuence of visual experience on the
ability to form spatial mental models based on route and survey descriptions. Cognition
100:321–342. doi:10.1016/j.cognition.2005.05.006
11. Rieser JJ, Lockman JJ, Pick HL (1980) The role of visual experience in knowledge of spatial
layout. Percept Psychophys 28:185–190. doi:10.3758/BF03204374
12. Gaunet F, Gentaz E (2009) Effect of visual experience on haptic estimations of spatial
locations. L’année psychologique 109:237–252
13. Rieser JJ, Guth DA, Hill EW (1986) Sensitivity to perspective structure while walking
without vision. Perception 15:173–188
14. Hatwell Y (2003) Le développement perceptivo-moteur de l’enfant aveugle. Enfance 55:
88–94
15. Wirth KE, Rein DB (2008) The economic costs and beneﬁts of dog guides for the blind.
Ophthalmic Epidemiol 15:92–98. doi:10.1080/09286580801939353
16. Li K (2015) Electronic travel aids for blind guidance. Presentation, University of California
Berkeley
17. Murata (2008) Ultrasonic sensor application guide. 17
18. Russell L (1965) Travel path sounder. In: Proceedings of the rotterdam mobility research
conference, American Foundation for the Blind, New York
19. Kay L (1964) An ultrasonic sensing probe as a mobility aid for the blind. Ultrasonics
2(2):53–59
20. GDP Research (2005) The miniguide mobility aid. http://www.gdp-research.com.au/minig_
1.htm. Accessed 13 Sept 2016
21. Pressey N (1977) Mowat sensor. Focus 11(3):35–39
22. Benet G, Blanes F, Simó JE, Pérez P (2002) Using infrared sensors for distance
measurement in mobile robots. Robot Auton Syst 40:255–266. doi:10.1016/S0921-8890(02)
00271-3
23. Farcy R, Leroux R, Jucha A, Damaschini R, Grégoire C, Zogaghi A (2006) Electronic travel
aids and electronic orientation aids for blind people: technical, rehabilitation and everyday
life points of view. In: Hersh MA (ed) Conference & workshop on assistive technologies for
people with vision & hearing impairments, Kufstein, Austria, 17–21 July 2006
24. Foerster K-T, Gross A, Hail N, Uitto J, Wattenhofer R (2014) SpareEye: enhancing the
safety of inattentionally blind smartphone users. In: Proceedings of the 13th international
conference on mobile and ubiquitous multimedia, CM, New York, NY, USA, pp 68–72
25. Karthick M, Suguna R (2015) Obstacle detection for visually impaired people using smart
phones. Int J Emerg Technol Comput Sci Electron 13(1):530–535
26. Tapu R, Mocanu B, Bursuc A, Zaharia T (2013) A smartphone-based obstacle detection and
classiﬁcation system for assisting visually impaired people. In: 2013 IEEE international
conference on computer vision workshop, pp 444–451
27. Yusro M, Hou KM, Pissaloux E, Shi HL, Ramli K, Sudiana D (2013) SEES: concept and
design of a smart environment explorer stick. In: 2013 6th international conference on
human system interaction HSI, pp 70–77
28. Lin XM, Connier J, Vaslin P, Guo CC, Zang TY, Li JJ, De Vaulx C, Hou KM (2015) Indoor
navigation with the Smart Environment Explorer Stick (SEES). Paper presented at the 2015
new information communication sciences and technology workshop (NICST 2015), IMS,
Bordeaux, France, 8–9 Sept 2015
29. Shoval S, Borenstein J, Koren Y (1993) The navbelt—a computerized travel aid for the
blind. In: Proceedings of the RESNA ’93 conference, Las Vegas, Nevada, 12–17 June 1993
30. Ulrich I, Borenstein J (2001) The GuideCane–applying mobile robot technologies to assist
the visually impaired. IEEE Trans Syst Man Cybern Part Syst Hum 31:131–136. doi:10.
1109/3468.911370
530
G. Motta et al.

31. Mhajeri N, Raste R, Daneshvar S (2011) An obstacle detection system for blind People. In:
Proceedings of the world congress on engineering 2011 vol II, London, U.K., 6–8 July 2011
32. Molton N, Se S, Brady JM, Lee D, Probert P (1998) A stereo vision-based aid for the
visually impaired. Image Vis Comput 16:251–263. doi:10.1016/S0262-8856(97)00087-5
33. Sharma PS, Chitaliya NG (2015) Obstacle avoidance using stereo vision: a survey. Int J
Innovative Res Comput Commun Eng 3(1):24–29
34. Lazaros N, Sirakoulis GC, Gasteratos A (2008) Review of stereo vision algorithms: from
software to hardware. Int J Optomechatronics 2:435–462. doi:10.1080/15599610802438680
35. Yusro M, Hou K-M, Pissaloux E, Ramli K, Sudiana D, Zhang L-Z, Shi H-L (2014) Concept
and design of SEES (Smart Environment Explorer Stick) for visually impaired person
mobility assistance. Hum Comput Syst Interact Backgrounds Appl 300(3):245–259
36. LIDAR: https://en.wikipedia.org/wiki/Lidar. Accessed 8 Aug 2016
37. Aguerrevere D, Choudhury M, Barreto A (2004) Portable 3D sound/soar navigation system
for blind individuals. In: Second LACCEI international Latin American and Caribbean
conference for engineering and technology
38. Parseihian G, Brilhault A, Dramas F (2010) Navig: an object localization system for the
blind. In: Workshop pervasive 2010: multimodal location based techniques for extreme
navigation, Helsinki
39. Filipe V, Fernandes F, Fernandes H, Sousa A, Paredes H, Barroso J (2012) Blind navigation
support system based on microsoft kinect. Procedia Comput Sci 14:94–101. doi:10.1016/j.
procs.2012.10.011
40. European Commission (2010) Intelligent transport systems. Directorate-General for
Research, Publications Ofﬁce of the European Union, Luxembourg, 28 p
41. Wu H, Siegel M, Khosla P (1998) Vehicle sound signature recognition by frequency vector
principal component analysis. In: Proceedings of IEEE instrumentation and measurement
technology conference, 1998. IMTC 98, vol 1. pp 429–434
42. Lymberopoulos D, Liu J, Yang X, Choudhury RR, Handziski V, Sen S (2015) A realistic
evaluation and comparison of indoor location technologies: experiences and lessons learned.
In: Proceedings of the 14th international conference on information processing in sensor
networks. ACM, Seattle, Washington, pp 178–189
43. Terrier P, Turner V, Schutz Y (2005) GPS analysis of human locomotion: further evidence
for long-range correlations in stride-to-stride ﬂuctuations of gait parameters. Hum Mov Sci
24:97–115. doi:10.1016/j.humov.2005.03.002
44. Beder C, Klepal M (2012) Fingerprinting based localisation revisited: a rigorous approach
for comparing RSSI measurements coping with missed access points and differing antenna
attenuations. In: 2012 international conference on indoor positioning and indoor navigation
(IPIN), pp 1–7
45. Li L, Hu P, Peng C, Shen G, Zhao F (2014) Epsilon: a visible light based positioning system.
In: Proceedings of 11th USENIX conference on networked systems design and implemen-
tation, USENIX Association, Seattle, WA, pp 331–343
46. Riehle TH, Anderson SM, Lichter PA, Condon JP, Sheikh SI, Hedin DS (2011) Indoor
waypoint navigation via magnetic anomalies. In: Annual international conference of the
IEEE engineering in medicine and biology society EMBC 2011, pp 5315–5318
47. Sabatini AM (2005) Quaternion-based strap-down integration method for applications of
inertial sensing to gait analysis. Med Biol Eng Comput 43:94–101
48. de Saint Rémy N, Vaslin P, Dabonneville M, Roux D, Hou KM, Cid M (2003) Can the
distance run by a wheelchair be calculated from the measurements of a 3-D accelerometer?
Actes du XXVIIIème Congrès de la Société de Biomécanique, Poitiers, 11–12 Sept, Arch
Physiol Biochem 111(suppl.):99
49. Vaslin P, Dabonneville M (2000) Use of a 3D accelerometer for kinetic analysis of
wheelchair propulsion. In: Proceedings of the 12th conference of the european society of
biomechanics, Trinity College, Dublin (Ireland), 28–30 Aug, p 345
50. Faroux JP, Renault J (1996) Mécanique 1: point et systèmes de points (1ère année MPSI –
PCSI). Dunod [Collection J’intègre – Prépas scientiﬁques], 4ème édition, Paris, 372 p
Overview of Smart White Canes: Connected Smart Cane …
531

51. Grossetête C, Olive P (1999) Mécanique des systèmes et du solide – Cours et exercices
corrigés (2ème année MP – MP* – PT – PT*), Ellipses/Édition Marketing S.A. [Collection
Taupe-Niveau – Classes préparatoires aux Grandes Écoles Scientiﬁques], Paris, 312 p
52. Radix JC (1978) Gyroscopes et gyromètres. Sup’ Aéro: École Nationale Supérieure de
l’Aéronautique et de l’Espace, Toulouse: Cépaduès Éditions, 390 p
53. Animazoo Uk Ltd (2006) Quayside ofﬁces. Basin Road South, Brighton, East Sussex, BN41
1WF. http://www.animazoo.com/contact/. Accessed 15 Sept 2016
54. Cutti AG, Giovanardi A (2008) Ambulatory measurement of shoulder and elbow kinematics
through inertial and magnetic sensors. Med Biol Eng Comput 46:169–178
55. Cutti AG, Ferrari A (2010) ‘Outwalk’: a protocol for clinical gait analysis based on inertial
and magnetic sensors. Med Biol Eng Comput 48:17–25
56. Ferrari A, Cutti AG, Garofalo P, Raggi M, Heijboer M, Cappello A, Davalli A (2010) First
in vivo assessment of ‘‘Outwalk’’: a novel protocol for clinical gait analysis based on inertial
and magnetic sensors. Med Biol Eng Comput 48:1–15
57. Luinge HJ, Veltink PH (2005) Measuring orientation of human body segments using
miniature gyroscopes and accelerometers. Med Biol Eng Comput 43(2):273–282
58. Luinge HJ, Veltink PH, Baten CTM (2007) Ambulatory measurement of arm orientation.
J Biomech 40(1):78–85
59. Picerno P, Cereatti A, Cappozzo A (2008) Joint kinematics estimate using wearable inertial
and magnetic sensing modules. Gait Posture 28(4):588–595
60. Picerno P, Cereatti A, Cappozzo A (2011) A spot check for assessing static orientation
consistency of inertial and magnetic sensing units. Gait Posture 33:373–378
61. deVries WHK, Veeger HEJ, Cutti AG, Baten C, Van der Helm FCT (2010) Functionally
interpretable local coordinate systems for the upper extremity using inertial & magnetic
measurement systems. J Biomech 43:1983–1988
62. Williamson R, Andrews BJ (2001) Detecting absolute human knee angle and angular
velocity using accelerometers and rate gyroscopes. Med Biol Eng Compu 39:1–9
63. Xsens Technologies (2009) Pros and cons of inertial sensing for human motion analysis.
2 Nov 2009. Webinar Xsens. http://download.xsens.com/XsensProsCons20091105.wmv.
PowerPoint
presentation.
http://download.xsens.com/XsensProsConsInertialSensing2009
1105.pdf
64. Naqvib NZ, Kumar A, Chauhan A, Sahni K (2012) Step counting using smartphone-based
accelerometer. Int J Comput Sci Eng 4:675
65. Zheng X, Yang H, Tang W, Pu S, Zheng L, Zheng H, Liao B, Wang J (2014) Indoor
pedestrian navigation with shoe-mounted inertial sensors. In: Park JJJH, Chen S-C, Gil J-M,
Yen YN (eds) Multimedia and ubiquitous engineering, Springer, Berlin, pp 67–73
66. Gaunet F, Briffault X (2005) Exploring the functional speciﬁcations of a localized
wayﬁnding verbal aid for blind pedestrians: simple and structured urban areas. Hum Comput
Interact 20:267–314. doi:10.1207/s15327051hci2003_2
67. Matthews B, Hibberd D, Carsten O (2014) Road and street crossings for blind and partially
sighted people: the importance of being certain. Institute for Transport Studies, University of
Leeds
68. Wang S, Tian Y (2012) Detecting stairs and pedestrian crosswalks for the blind by RGBD
camera. In: 2012 IEEE international conference on bioinformatics and biomedicine
workshops (BIBMW), pp 732–739
69. Ivanchenko V, Coughlan J, Shen H (2008) Detecting and locating crosswalks using a camera
phone. In: Proceedings of CVPR IEEE computer society conference on computer vision and
pattern recognition 2008:4563143. doi:10.1109/CVPRW.2008.4563143
70. Ivanchenko V, Coughlan J, Shen H (2009) Staying in the crosswalk: a system for guiding
visually impaired pedestrians at trafﬁc intersections. Assist Technol Res Ser 25:69–73.
doi:10.3233/978-1-60750-042-1-69
71. Wang S, Tian Y (2011) Indoor signage detection based on saliency map and bipartite graph
matching. In: 2011 IEEE international conference on bioinformatics and biomedicine
workshops (BIBMW), pp 518–525
532
G. Motta et al.

72. Gubbi J, Buyya R, Marusic S, Palaniswami M (2013) Internet of Things (IoT): a vision,
architectural elements, and future directions. Future Gener Comput Syst 29(7):1645–1660
73. Tian Y, Yang X, Arditi A (2010) Computer vision-based door detection for accessibility of
unfamiliar environments to blind persons. In: Miesenberger K, Klaus J, Zagler W,
Karshmer A (eds) Computers helping people with special needs. Springer, Berlin,
pp 263–270
74. Yi C, Flores RW, Chincha R, Tian Y (2013) Finding objects for assisting blind people. Netw
Model Anal Health Inform Bioinforma 2:71–79. doi:10.1007/s13721-013-0026-x
75. Jung K, In Kim K, Jain KA (2004) Text information extraction in images and video: a
survey. Pattern Recognit 37:977–997. doi:10.1016/j.patcog.2003.10.012
76. Yi C, Tian Y (2014) Scene text recognition in mobile applications by character descriptor
and structure conﬁguration. IEEE Trans Image Process 23:2972–2982. doi:10.1109/TIP.
2014.2317980
77. Buckley JJ, Siler W, Tucker D (1986) A fuzzy expert system. Fuzzy Sets Syst 20:1–16.
doi:10.1016/S0165-0114(86)80027-6
78. Fasel B, Luettin J (2003) Automatic facial expression analysis: a survey. Pattern Recognit
36:259–275. doi:10.1016/S0031-3203(02)00052-3
79. Martinez F, Carbone A, Pissaloux E (2013) Combining ﬁrst-person and third‐person gaze for
attention recognition. In: 10th IEEE conference on automatic face and gesture recognition,
Shanghai
80. Yang M, Yu K (2011) Real-time clothing recognition in surveillance videos. In: 2011 18th
IEEE international conference on image processing ICIP, pp 2937–2940
81. Kotus J, Lopatka K, Czyzewski A (2012) Detection and localization of selected acoustic
events in acoustic ﬁeld for smart surveillance applications. Multimed Tools Appl 68:5–21.
doi:10.1007/s11042-012-1183-0
82. Schwarz LA, Mkhitaryan A, Mateus D, Navab N (2011) Estimating human 3D pose from
time-of-ﬂight images based on geodesic distances and optical ﬂow. In: 2011 IEEE
international conference on automatic face & gesture recognition, Workshop FG 2011,
pp 700–706
83. Randell B, Avizienis A (2001) Fundamental concepts of dependability. LAAS-CNRS
84. Akyildiz IF, Su W, Sankarasubramaniam Y, Cayirci E (2002) Wireless sensor networks: a
survey. Comput Netw 38:393–422. doi:10.1016/S1389-1286(01)00302-4
85. Akyildiz IF, Melodia T, Chowdhury KR (2008) Wireless multimedia sensor networks:
applications and testbeds. Proc IEEE 96:1588–1605. doi:10.1109/JPROC.2008.928756
86. Hamdi M, Boudriga N, Obaidat MS (2008) Bandwidth-effective design of a satellite-based
hybrid wireless sensor network for mobile target detection and tracking. IEEE Syst J 2:74–
82. doi:10.1109/JSYST.2007.916049
87. Munir A, Gordon-Ross A, Ranka S (2015) Modeling and optimization of parallel and
distributed embedded systems. Wiley, USA
88. Munir A, Gordon-Ross A, Ranka S (2014) Multi-core embedded wireless sensor networks:
architecture and applications. IEEE Trans Parallel Distrib Syst 25:1553–1562. doi:10.1109/
TPDS.2013.219
89. Shi HL, Hou KM, Zhou HY, Liu X (2011) Energy efﬁcient and fault tolerant multicore
wireless sensor network: E2MWSN. In: 2011 7th international conference on wireless
communications, networking and mobile computing WiCOM, pp 1–4
90. Shi H-L, Hou K-M, Diao X, Xing L, De Vaulx C (2013) A robust multi-core multi-support
and modular wireless multimedia sensor network: MiLive. ECOTECHS’2013, Montoldre,
France, 9–10 Oct 2013
91. Kleihorst R, Schueler B, Danilin A, Heijligers M (2006) Smart camera mote with high
performance vision system. In: Workshop on distributed smart cameras (DSC), Boulder,
Colorado, Oct 2006
92. Rasberry Pi Foundation (2016) Raspberry Pi 1 model B. http://www.raspberry.org/products/
model-b/. Accessed 18 Mar 2016
Overview of Smart White Canes: Connected Smart Cane …
533

93. SMIR Group LIMOS UMR 6158 CNRS Blaise Pascal University (2016) iLive platform
introduction. http://edss.isima.fr/auth/home.php, 2016. Accessed 18 Mar 2016
94. Duffy C, Roedig U, Herbert J, Sreenan C (2008) A comprehensive experimental comparison
of event driven and multi-threaded sensor node operating systems. J Netw. doi:10.4304/jnw.
3.3.57-70
95. Bhatti S, Carlson J, Dai H, Deng J, Rose J, Sheth A, Shucker B, Gruenwald C, Torgerson A,
Han R (2005) MANTIS OS: an embedded multithreaded operating system for wireless
micro sensor platforms. Mob Netw Appl 10:563–579
96. De Vaulx C, Hou K-M (2002) DREAM: un micro noyau temps réel orienté pour la tolérance
aux fautes. Informatique et Santé 13:63–69
97. Zhou H, Hou KM, De Vaulx C (2006) SDREAM : a super-small distributed real-time
microkernel dedicated to wireless sensors. JPCC Spec Issue Key Technol Appl Wirel Sens
Body-Area Netw 12 p
98. Hill J, Szewczyk R, Woo A, Hollar S, Culler D, Pister K (2000) System architecture
directions for networked sensors. In: Proceedings of ninth international conference on
architectural support for programming languages and operating systems, ACM, New York,
NY, USA, pp 93–104
99. Dunkels A, Gronvall B, Voigt T (2004) Contiki—a lightweight and ﬂexible operating
system for tiny networked sensors. In: 29th annual IEEE international conference on local
computer networks 2004, pp 455–462
100. Han C, Kumar R, Shea R, Kohler E, Srivastava M (2005) A dynamic operating system for
sensor nodes. In: Proceedings of the mobile systems, applications, and services (MobiSys),
ACM, p 117–124
101. Klues K, Liang CJM, Paek J, Musaloiu-Elefteri R, Levis P, Terzis A, Govindan R (2009)
TOSThreads: Thread-safe and non-invasive preemption in TinyOS. In: Proceedings of the
7th ACM conference on embedded networked sensor systems, ACM, p 127–140
102. Liu X, Hou KM, de Vaulx C, Shi H, Gholami KE (2014) MIROS: a hybrid real-time
energy-efﬁcient operating system for the resource-constrained wireless sensor nodes. Sensors
14:17621–17654. doi:10.3390/s140917621
103. Xenomai (2016) About Xenomai. https://xenomai.org/about-xenomai/. Accessed 20 Mar
2016
104. Kushalnagar N, Montenegro G, Schumacher C (2007) IPv6 over Low-Power Wireless
Personal Area Networks (6LoWPANs): overview, assumptions, problem statement, and
goals. https://tools.ietf.org/html/rfc4919, Accessed 18 Mar 2016
105. Montenegro G, Kushalnagar N, Hui J, Culler D (2007) Transmission of IPv6 packets over
IEEE 802.15.4 networks. https://tools.ietf.org/html/rfc4944. Accessed 18 Mar 2016
106. Winter T, Thubert P, RPL Author Team (2012) RPL: IPv6 routing protocol for low power
and lossy networks. https://tools.ietf.org/html/rfc6550. Accessed 18 Mar 2016
107. Shelby Z, Hartke K, Bormann C (2014) The Constrained Application Protocol (CoAP).
https://tools.ietf.org/html/rfc7252. Accessed 18 Mar 2016
108. OASIS (2014) MQTT version 3.1.1. http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.
1.1-os.html. Accessed 18 Mar 2016
109. Fielding RT (2000) Architectural styles and the design of network-based software
architectures. Ph.D. dissertation, Irvin University
110. Durrant-Whyte HF (1988) Sensor models and multisensor integration. Int J Robot Res 7:97–
113. doi:10.1177/027836498800700608
111. Wu H (2004) Sensor data fusion for context-aware computing using dempster-shafer theory.
Carnegie Mellon University, Pittsburgh
112. W School Josm. http://josm.openstreetmap.de
113. Maperitive. http://maperitive.net/
114. Easy CAD to SVG converter. http://dwg2svg.en.softonic.com/
115. Inkscape. https://inkscape.org/en/
116. Dorigo M, Stützle T (2010) Ant colony optimization: overview and recent advances. In:
Gendreau M, Potvin J-Y (eds) Handbook of metaheuristics, Springer US, pp 227–263
534
G. Motta et al.

117. Chang Q, Wang W, Li Q, Towards automatic context-sense for seamless navigation and
localization using smart phone sensors. Sensors (to be published)
118. Vuforia. http://www.vuforia.com/
119. RocketMQ. https://github.com/alibaba/RocketMQ
120. Apache Storm. http://storm.apache.org/
121. Docker. https://www.docker.com/
Overview of Smart White Canes: Connected Smart Cane …
535

Accessible Interactive Maps for Visually
Impaired Users
Julie Ducasse, Anke M. Brock and Christophe Jouffrais
1
Introduction
1.1
Context
Mobility and orientation are among the greatest challenges for visually impaired
people. In France, 56% of the visually impaired population state that they face
difﬁculties when walking outside and 29% are not able to navigate on their own
[12]. One reason that explains these issues is that visually impaired people usually
exchange verbal descriptions of itineraries, which may help them ﬁnd their way, but
does not provide them with any clue about the spatial layout of the targeted
environment. GPS-based systems, although facilitating navigation, raise the same
issue. Sighted people usually acquire information about a spatial environment
through visual perception or using geographical maps. However, maps are essen-
tially visual and thus inherently inaccessible for visually impaired people. Limited
access to maps has drastic consequences on mobility and education, but more
generally on personal and professional life, and can lead to social exclusion [84].
Julie Ducasse and Anke Brock are both ﬁrst co-authors and contributed equally to this chapter.
J. Ducasse  C. Jouffrais (&)
IRIT, CNRS, Toulouse, France
e-mail: Christophe.Jouffrais@irit.fr
J. Ducasse  C. Jouffrais
IRIT, University Paul Sabatier, Toulouse, France
A.M. Brock
Potioc, Inria Bordeaux, Talence, France
A.M. Brock
LaBRI, University Bordeaux, Talence, France
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_17
537

Beyond orientation and mobility purposes, maps are very useful tools to explore
and analyze geographical data and to acquire general knowledge about many
subjects such as demography, geopolitics, history, etc. They are also often used in
the classroom for this purpose. As stated by O’Modhrain et al. [80], “there is an
immediate need for research and development of new technologies to provide
non-visual access to graphical material. While the importance of this access is
obvious in many educational, vocational, and social contexts for visually impaired
people, the diversity of the user group, range of available technologies, and breadth
of tasks to be supported complicate the research and development process.”
In specialized education centers for visually impaired people, tactile maps are
commonly used to give visually impaired students access to geographical repre-
sentations. However, these materials are rarely used or available outside of schools,
because their production is a costly and time-consuming process [97]. To create a
tactile map, one of the most common methods is to print it on a special paper, called
“swell paper” (synonyms are “microcapsule paper” or “heat sensitive paper”),
which contains microcapsules of alcohol in its coating. When the paper is heated,
the microcapsules expand and create relief over black lines (Fig. 1a). The resulting
maps, called raised-line maps, can be perceived by touch. But they are also visual
maps, making it possible to share information between blind and (partially) sighted
people. Raised-line maps are usually prepared with a computer, which makes it
possible to print and fuse several copies of the same map.
Another technique, called vacuum-forming or thermoforming, consists in placing a
sheet ofplastic overamastermadeofavarietyoftexturedmaterials.Whenitisheatedin
a vacuum, the sheet is permanently deformed according to the master. Hand-crafted
techniques can also be used to produce maps and other graphics. For example, for
orientation and mobility lessons, teachers and students construct itineraries or local
maps by progressively placing magnets over a magnetic board (Fig. 1b). Students are
sometimes asked to replicate the construction, so that the teacher can check that the
itinerary has been memorized. Small-scale models made out of wood also exist,
alongside graphics made out of paper, cardboard, ropes, etc. (see Fig. 1c). Edman [23]
presented a comprehensive summary of production techniques for accessible maps.
Fig. 1 a Production of a raised-line map. The map is printed on swell paper that is passed into a
fuser. The relief appears over black lines. b A simple local map is being built with small magnets.
c Example of a wooden small-scale model used during orientation and mobility lessons
538
J. Ducasse et al.

Although tactile maps are efﬁcient for acquisition of spatial knowledge, they pre-
sent several limitations and issues. As stated by Rice et al. [97] the production of tactile
maps is time consuming and expensive. In addition, tactile maps must be produced by
a tactile graphics specialist who knows how to present information so that it can be
perceived by touch [114]. Other critiques concern the number of elements that can be
displayed and the accuracy of the map content. Indeed, because of the perceptual limits
of the tactile modality, less details can be represented on a tactile map than on a visual
map. Furthermore, once a tactile map is printed, its content is static and cannot be
updated dynamically. Tactile maps are then quickly getting outdated [133]. In addi-
tion, the use of braille labels is an issue. Only a small percentage of visually impaired
people read braille (National Federation of the Blind [79]); and braille is not so
convenient when compared to printed text which can be written with different font
sizes and styles. It can be rotated to ﬁt in open spaces. Upper and lower cases, as well
as color, can be used to highlight important items. In contrast, braille lacks all of these
possibilities and needs a lot of space because it is ﬁxed in size, inter-cell spacing, and
orientation [114]. Due to the lack of space, braille abbreviations are commonly used on
tactile maps, which are then explained in a legend accompanying the map. Because the
reader must frequently move hands between the map and the legend, it disrupts the
reading process [43]. Finally, because they are not interactive, tactile maps cannot
provide advanced functionalities such as panning, zooming, annotating, computing
itineraries or distances, and using ﬁlters to facilitate the exploration.
Several research projects have been led during the past three decades in order to
improve access to maps using interactive technologies. In this chapter, we ﬁrst present
an exhaustive review of interactive map prototypes. We classiﬁed existing interactive
maps into two categories: Digital Interactive Maps (DIMs) that are displayed on a ﬂat
surface such as a screen; and Hybrid Interactive Maps (HIMs) that include both a
digital and a physical representation. In each family, we identiﬁed several subcate-
gories depending on the technology being used. We compared the categories and
subcategories according to cost, usability, and technological limitations. Then,
because evaluation is essential in order to verify that devices are usable, we reviewed a
number of studies showing that they can support spatial learning for visually impaired
users. In the following section, we discussed the similarities and differences between
maps and other types of graphics (such as bar charts, organigrams, etc.) Finally, we
identiﬁed new technologies and methods that could improve the accessibility of
graphics for visually impaired users (e.g., shape-changing interfaces and 3-D printing).
1.2
Important Notions
1.2.1
About Accessible Interactive Maps
Deﬁnition and characteristics of accessible interactive maps
In this chapter, we use the term “map” as suggested by Montello [77]. Montello
deﬁnes four psychological spaces (i.e., space as perceived by a human) that depend
Accessible Interactive Maps for Visually Impaired Users
539

on “the projected size of the space relative to the human body.” Figural space is
“projectively smaller than the body” and can be apprehended without moving.
Within ﬁgural space, pictorial spaces are small ﬂat spaces (i.e., maps) and object
spaces are 3-D spaces (i.e., small-scale models). Vista space is “as large as or larger
than the body” and can be apprehended without moving (e.g., town squares).
Environmental space is “projectively larger than the body” and requires locomotion
to be apprehended (e.g. cities). Geographical space is “much larger than the body”
and cannot be apprehended through locomotion (e.g. countries). Maps are then
pictorial spaces that represent environmental or geographical spaces.
Maps can have different purposes. Reference maps support orientation and
mobility, by providing for example the possibilities to explore unknown areas,
acquire an overview of the surrounding of a landmark, localize speciﬁc landmarks,
or prepare a journey. You-are-here (YAH) maps are reference maps showing fea-
tures in the immediate environment of the map reader [78]. They normally include a
symbol representing location, and possibly orientation of a person reading the map.
Other map types include topological maps showing selected features without
necessarily respecting distance, scale and orientation, as well as thematic maps
giving qualitative or quantitative information on a speciﬁc topic [23]. Choropleth
maps are a subset of thematic maps in which the shading of areas indicates the value
of a speciﬁc variable [140]. Map representations are not perfect representations of
the environment. Deformations occur when the cartographer transforms spatial data
into a map representation. The different map types can be used for teaching
geography, but also politics, economy, history, etc.
These observations apply to maps made for both sighted and visually impaired
users. However, even though purposes and motivations for using maps are similar
for both groups, the map representations are different. They are essentially visual
when designed for sighted users, whereas they are based on auditory and tactile
cues when designed for visually impaired users.
Map data versus map representation
We distinguish between map data and map representation. The map data is usually
stored in a database and cannot be explored as it is: it consists in a set of locations
(cities, landmarks, streets, etc.), each being determined in absolute terms—latitude,
longitude—or in relative terms—in comparison to a reference point [67]. The map data
needs to be rendered in a perceptible way, with visual, haptic, tactile, or auditory cues.
When the map data is represented in the form of objects that one can explore by
touch, we consider that the map representation is physical (e.g., raised-line maps,
3-D printed maps, maps built using magnets, etc.). If such a physical representation
is also associated with a digital representation, we consider that the map repre-
sentation is hybrid (e.g., an interactive tactile map). When there is no physical
representation associated with the map, we considered it as digital (e.g. an auditory
map explored with the ﬁnger).
Accessible Interactive Maps
In this chapter, the term “interactive maps” refers to all the devices that have been
designed to display pictorial spaces, as deﬁned by Montello [77]. The interaction
540
J. Ducasse et al.

relies on technologies that provide feedback in response to user’s input. Accessible
Interactive Maps are designed for visually impaired users. The family of
“touch-enabled” devices has been widely used to design Accessible Interactive
Maps. They refer to different technologies that directly sense the user’s touch input.
They have also been called touchscreens or touch-sensitive devices. Some of these
devices are mono-touch (e.g. digitizer tablets, older touchscreen models), but most
of the current ones are multi-touch (e.g. tablets and smartphones). Some devices
react to bare ﬁngers, whereas others require a pen for input (e.g., digitizer tablets).
These devices vary in scale, from smartwatches to large tabletops.
1.2.2
About Touch
Sense of touch
The terms “touch,” “haptic,” “tactile,” etc., are used in many scientiﬁc ﬁelds
including human–computer interaction and psychology, but with slightly different
meanings. Here, we more likely refer to the literature in psychology for their
deﬁnition.
The sense of touch relies on the somatosensory system, which includes ther-
moreceptors, mechanoreceptors, chemoreceptors, and nociceptors. Tactile (or
sometimes touch) perception concerns the contact between the skin of any part of
the body and other objects. Because of the presence of different receptors in the
skin, muscles, and bones, the sense of touch provides cues on temperature, pressure,
movement, and pain.
Cutaneous perception refers to the perception of an object applied onto the skin
in absence of any movement. It includes temperature, pressure, and pain.
Kinesthetic perception (perception of movement) is based on the deformation of
mechanoreceptors in muscles, tendons and joints, such as in the hand-arm-shoulder
system [29]. It can also rely on the efferent copy of the movement that is being
performed. The kinesthetic perception provides feedback on the current position,
and movement of body parts. Kinesthetic feedback can also be referred to as
proprioceptive feedback.
Haptic perception has been deﬁned as the combination of cutaneous and
kinesthetic perceptions in a complex manner across space and time [29, 40]. It is a
dynamic process that combines the cutaneous perception with movement, for
instance when actively exploring an object or a pictorial space. Cutaneous per-
ception is referred to as “passive,” and haptic perception as “active” [63]. In the
ﬁeld of human–computer interaction, “haptics” has recently been used to describe
any form of interaction involving touch.
Point(s) of contact
In this chapter, a “point of contact” refers to the map location that is currently being
explored, either indirectly, by the means of a pointing device, or directly, by the
Accessible Interactive Maps for Visually Impaired Users
541

user’s hands or ﬁngertips. The number of points of contact that can be simulta-
neously tracked depends on the technology being used. For example, when
exploring a Digital Map with a regular mouse, users rely on a single point of
contact. On the contrary, several points of contact are simultaneously available
when users directly explore a Physical Map with both hands.
Tactile exploration
Many cognitive processes are demanding during tactile exploration of a map. For
instance, vision allows the immediate estimation of the relative position of two
objects (distance and orientation between them) on a map. This is not the case
during tactile exploration of a map. Here the same estimation is based on three
serial processes including the exploration of the whole map, the localization of the
items displayed on the maps, and the comparison of their relative position. These
processes are based on successive one-handed or simultaneous two-handed
movements. For instance, it has been observed that visually impaired people use
one ﬁnger as a ﬁxed reference point when exploring adjacent parts of a tactile map
[129]. Obviously, when constraining tactile exploration to a single point of contact,
spatial integration is even slower and more complicated, and leads to more sig-
niﬁcant cognitive issues [41, 70].
Tactile reference frames
Obviously a map has its own reference frame, but the user exploring the map with
the hands will build own mental reference frames. In psychology, a dissociation has
been accepted between “egocentred” and “allocentred” frames of reference [see,
e.g. 63]. In the egocentred reference frame, coordinates of items, and hence dis-
tances and directions, are speciﬁed relative to the person who is exploring the
map. In the allocentred reference frame, coordinates are attached to an external
landmark within the display.
Human vision provides both egocentred and allocentred coordinates simulta-
neously because objects are perceived in relation to the retina but also in relation to
each other. This process is automatic and immediate. It is harder to construct both
egocentred and allocentred reference frames by touch. Touch is by essence ego-
centred, and primarily attached to the part of the body that is in contact with the
map (usually the hand). Then the origin of the egocentred reference frame is the
user itself, and it is more reliable if the user does not move (i.e., change of location
relative to the map). In order to build an allocentred mental representation of the
map, it is mandatory to explore many items successively. The origin of an allo-
centred reference frame must be a static tactile landmark on the setup. Finally,
because touch is a serial process that relies on direct contact with the various
elements of the map, spatial integration by touch, regardless whether it is ego-
centred or allocentred, needs time.
542
J. Ducasse et al.

1.3
Motivation and Method for the Classiﬁcation
Since the ﬁrst accessible interactive map prototype has been proposed by Parkes
[83], many researchers have designed interactive maps using a variety of tech-
nologies and interaction techniques. The current book chapter is an attempt of
structuring the work that has been conducted on accessible interactive maps until
today. Of course, we acknowledge earlier surveys of interactive maps, which fed
the current work. For instance, Zeng and Weber [138] classiﬁed interactive maps
for visually impaired people in four groups: (1) “virtual acoustic maps” are entirely
based on verbal and nonverbal audio output (for instance, the user interacts by
tapping on a tablet which then produces audio feedback); (2) “virtual tactile maps”
make use of haptic devices (e.g., a force-feedback joystick or mouse); (3) “braille
tactile maps” are based on the use of dedicated raised pin displays; and ﬁnally,
(4) “augmented paper-based tactile maps” use a raised-line map as overlay over a
touch-sensitive display combined with audio output. More recently, Kaklanis et al.
[54, 55] presented an overview of accessible interactive prototypes. This survey
contained map prototypes as well as other accessible graphic tools. In the current
survey, we propose a classiﬁcation limited to interactive maps but including more
recent map types, for instance Tangible Maps for the visually impaired, and cov-
ering a higher number of references to research projects.
Except in the above mentioned references [138], we did not observe any attempt
to more precisely deﬁne a terminology to refer to the different types of accessible
interactive maps. Various names have been chosen in different publications, and
there is rarely an explanation about the choices that have been made. Several
authors used the term “audio-tactile maps” [47, 74, 81, 82, 125]. In general, but not
systematically, this term refers to maps that are based on touch-enabled devices
with audio output. Frequently, but not systematically, “audio-tactile maps” are
based on raised-line map overlays augmented with audio output. The term “virtual
tactile maps” has also been used to refer to the same kind of prototypes (1999).
Some authors referred to different prototypes, based on haptic devices and audio
output, as “haptic soundscapes” [33, 61]. In fact, there is no overall agreement
about the nomenclature of accessible interactive maps.
In this book chapter, we present a comprehensive summary of accessible in-
teractive maps for visually impaired people. We cover a much larger design space
than in previous work, and propose a classiﬁcation and terminology. We performed
an exhaustive search with the aim of covering as many relevant publications as
possible using different scientiﬁc databases (ACM Digital Library, SpringerLink,
IEEE Explorer, and Google Scholar). Then, we applied different criteria: ﬁrst, we
only considered interactive maps that were designed speciﬁcally for visually
impaired
people.
Second,
we
only
included
publications
in
journals
or
peer-reviewed conferences. Third, publications that proposed concepts without any
implementation were discarded. Fourth, if several papers were published on the
same prototype, we only considered one publication for each. Exceptions were
made if the map prototype has had major changes between successive papers.
Accessible Interactive Maps for Visually Impaired Users
543

2
Classiﬁcation of Interactive Accessible Maps
for Visually Impaired Users
In the current classiﬁcation, we distinguish “Digital Interactive Maps” whose rep-
resentation is purely digital (i.e., none of the elements of the maps is embedded into
a physical object) from “Hybrid Interactive Maps” whose representations are both
digital and physical. In this chapter we did not include maps that are purely
physical, i.e., that do not make use of any interactive technology (such as raised-line
maps). In the ﬁrst section we present Digital Interactive Map prototypes according
to the artifact used to explore the map (ﬁnger, joystick, etc.) In the second section
we present prototypes of Hybrid Interactive Maps and classify them into three
subcategories: Interactive Tactile Maps, Tangible Maps, and Refreshable Tactile
Maps.
2.1
Digital Interactive Maps (DIMs)
DIMs can be displayed on a screen or projected onto a surface. They can be
explored with one or many points of contacts, which are either direct (e.g. ﬁn-
gertips) or indirect (e.g. the cursor of a mouse or keyboard). DIMs provide auditory
feedback and/or tactile feedback (texture, relief, pressure and/or vibration),
according to the ﬁnger or cursor position. Most of the regular input devices, such as
keyboards or joysticks, do not provide additional tactile feedback. In that case, only
audio feedback is provided. More recent input devices can provide additional tactile
feedback (e.g. a mouse with braille cell or a force-feedback joystick).
2.1.1
Regular 2-D Pointing Devices
Keyboard
The keyboard is a standard device for both sighted and visually impaired people. It
has been used in several DIM prototypes [1, 82, 107, 127, 141].
iSonic [141] was a tool for the exploration of georeferenced statistical data
(thematical maps). The map was divided into a 3  3 grid. Each cell was mapped
to one key of the numerical keypad. By pressing one of the nine keys, users could
retrieve data associated with the corresponding region. The arrow keys enabled
users to navigate within the map. The keyboard could also be used for zooming.
The authors conducted a user study showing that most of the participants used the
3  3 keys to navigate the map, while the arrow keys were used to answer speciﬁc
questions such as ﬁnding the adjacent regions. Some participants managed to
understand the overall layout of the map using the 3  3 navigation keys. This map
prototype also worked with a touchpad.
544
J. Ducasse et al.

Delogu et al. [20] compared two techniques to navigate a soniﬁed map: a
keyboard and a tablet. Users were asked to move the cursor across regions, and
listen to auditory information about the current location. Then they had to identify
the map that they have previously explored among a set of different maps. The
results did not show any signiﬁcant difference in the map recognition task
depending on the type of input device used for map exploration (keyboard or
tablet). Both input devices enabled users to build an effective cognitive
map. However, the results showed that tablet users were more exhaustive than
keyboard users, i.e., they explored a higher number of regions. In addition, tablet
users changed the direction of exploration more often and were faster. Delogu et al.
[20] concluded that the absence of a reliable haptic reference frame and the step by
step displacement when using the keyboard demanded a greater cognitive load for
integrating the map conﬁguration.
In many other digital maps, keyboard use was limited to additional functions
(i.e., command selection) rather than spatial exploration. For instance, it has been
used to change the map heading [107] or to enter commands such as zooming or
scrolling [1].
Joystick
Picinali et al. [87] implemented a device that used a regular joystick for navigating a
virtual environment. The virtual environment represented a corridor leading to a
few rooms as well as various objects (doors, windows, elevators) and 3-D virtual
sounds (music, voices, etc.) Footstep noises were played every 50 cm, and ﬁnger
snapping noises could be triggered by the user at any time to determine the position
of objects by listening to the echoes. The navigation speed depended on the
pressure applied to the joystick. Results of their user study showed that participants
were able to build correct mental representations of the environment, and that these
mental representations were similar to those acquired through actual navigation in
the real environment. Similarly to the keyboard, regular joysticks do not provide
any feedback other than visual regarding the position of the cursor. In addition, the
cursor movement is relative to the last cursor position, which is hard to track
without vision. Although the results by Picinali et al. [87] were encouraging,
joysticks have rarely been used in accessible digital maps.
Tangible pointing devices
In this subcategory, we refer to the use of objects to move a cursor over the
map. We do not refer to maps whose representations are embedded into several
tangible objects (see “Tangible Maps”). In the speciﬁc context of this chapter, we
consider the computer mouse as a tangible pointing device.
Only few projects were based on a regular mouse. Earth+1 is a project developed
by the NASA where the user moved the mouse within the map. The visual image
was transcribed into auditory feedback, with different colors corresponding to
different notes. These notes were then played corresponding to the current cursor
1http://prime.jsc.nasa.gov/earthplus/ [last accessed September 29th 2016].
Accessible Interactive Maps for Visually Impaired Users
545

position. To our knowledge the device has not been further developed since 2009,
and remained in beta version. In addition it has not been evaluated with visually
impaired users. In fact, it appears that mouse is rarely used by visually impaired
people because the feedback concerning the cursor position is primarily visual. In
addition, when using a mouse, distortions appear between perceived and real dis-
tances [52]. Finally, the mouse can be lifted up and moved elsewhere while keeping
the pointer position stationary [61], which generates disorientation when using it in
absence of vision [33, 91].
Milne et al. [76] used a pen-based digitizer tablet with a stylus that enabled users
to control the position of the cursor within the map. Orientation was controlled by
rotating the stylus (a tactile cue on the stylus indicated the forward direction).
Daunys and Lauruska [18] used a similar approach. The users explored the map by
moving a pen over a digitizer tablet, and non-speech sounds were played accord-
ingly. Brittell et al. [6] also used a similar apparatus to present a choropleth map to
visually impaired users, including basic spatial analysis tools. Depending on the
position of the stylus, different queries were sent to a spatial database to provide the
user with up-to-date content. Different sounds were played to inform the users about
the population density as well as to indicate when they were near a border or
outside the map. Pielot et al. [88] used a toy as a “virtual listener” that a user could
move over a map. A camera placed above the map tracked the object’s position and
orientation, and auditory output was given according to its position.
2.1.2
Pointing Devices with Additional Somatosensory Feedback
Visually impaired people are used to both auditory and tactile cues. The afore-
mentioned prototypes provided only auditory feedback, which may limit their
ability to convey information. Many DIM prototypes have used pointing devices
with additional force or cutaneous feedback.
Pointing devices with force-feedback
Force-feedback devices mechanically produce a force that is perceived as a
kinesthetic feedback by the user [24]. Various force-feedback devices have been
used for the exploration of DIMs by visually impaired people including computer
mice, devices with handles, gamepads and joysticks.
As mentioned before, regular computer mice are difﬁcult to use in absence of
vision. Force-feedback mice provide additional tactile feedback that may be helpful
[14, 61, 82, 97, 118]. For instance, in the map prototype by Rice et al. [97] a haptic
grid overlay and a haptic frame were rendered by a force-feedback mouse. Moving
the mouse over the grid produced force-feedback, and allowed users to keep a sense
of distance, scale, and direction. The haptic frame around the map served as a
barrier to present the map outline. Rice et al. [97] reported that the frame was very
helpful for the users as it worked as a reference frame (see Sect. 1.2.2). However,
546
J. Ducasse et al.

Lawrence et al. [61] observed that users encountered problems regarding spatial
orientation with such a device even if a grid was provided.
Other prototypes used gamepads [82, 98] or joysticks [60, 83] with
force-feedback. Both are affordable and available as mainstream products. They
generally possess a small number of degrees of freedom and moderate output forces
[24]. In the BATS prototype [82], the input device provided slight or large bumps
when the cursor moved across boundaries, as well as vibrations when the cursor
moved over a city. Schmitz and Ertl [98] used the vibrations of the gamepad to
indicate when the cursor was in proximity of a street. Users could navigate the map
by moving the analog sticks of the gamepad. In the prototype of Lahav et al. [60],
users could navigate a virtual environment using a force-feedback joystick. The
environment used in their study represented a room with several elements such as
doors, windows, and pieces of furniture. Footsteps noises were played during the
displacement as well as other sounds (tapping, bumping, names, etc.). Results
showed that participants were able to understand the spatial conﬁguration of the
room, but also to rely on the build mental map to successfully perform Orientation
and Mobility tasks in the real environment.
Other haptic devices rely on a handle that can be moved and eventually rotated
in space and that allows interaction in three dimensions. Both the Geomagic Touch
X2© (formerly the Sensable Phantom Desktop) and the Novint Falcon3© are ten-
sioned cable systems, i.e., the handle is moved in different directions by several
actuated cables [24]. The user grasping the handle can sense the force that is
produced by the device. The Geomagic Touch X provides six degrees of freedom
(DoF), i.e., the possibility to vary position and orientation along the three spatial
axes, as well as 3-D force-feedback. The Novint Falcon allows only three DoF [24].
Several DIMs have been implemented with these devices. The prototype by De
Felice et al. [19] allowed the exploration of indoor environments as well as complex
geographical areas. Each element of the map (doors or rivers for example) was
associated with a speciﬁc haptic feedback. Users could select the content that they
wanted to display as well as change the scale and level of details of the
map.
HaptiRiaMaps
[53] was
an
open-source
web
application
based
on
OpenStreetMap4 (OSM). It allowed visually impaired users to search for a speciﬁc
address, and explore the haptic and audio map around the selected address.
VAVETaM (Verbally Assisting Virtual Environment Tactile Maps; see Lohmann
et al. [69]) provided visually impaired users with the name of the elements that they
were currently exploring and with an additional description of the local spatial
layout (for example, “this is the intersection between road A and road B”).
SeaTouch [107] allowed blind sailors to prepare a journey by using a DIM that
provided haptic feedback, text-to-speech output (TTS) and ambient sounds.
2http://geomagic.com/en/products/phantom-desktop/overview [last accessed September 29th 2016].
3http://www.novint.com/index.php/novintfalcon [last accessed September 29th 2016].
4https://www.openstreetmap.org [last accessed September 29th 2016].
Accessible Interactive Maps for Visually Impaired Users
547

Iglesias et al. [45] worked with the “GRAB” interface. This device creates 3-D
force-feedback, but in contrast to the previously mentioned devices it has two
distinct handles. Two ﬁngers, either of the same or different hands, are placed in
two thimbles onto which two independent force-feedbacks are applied. Several
applications were developed, one of which being a city map explorer application.
Observations conﬁrmed that using a second ﬁnger “can be vital as an “anchor” or
reference point” (see Points of contact in 1.2.2).
Pointing devices with cutaneous feedback
Input devices can also be augmented with cutaneous feedback. The more common
devices include mice with an array of pins, in which the pins move up and down
according to the cursor location. For instance, the VTPlayer by VirTouch was such a
tactile mouse with two 4  4 arrays of pins. These arrays were located under the index
and middle ﬁngers, and were actuated according to the cursor location. We identiﬁed
one project [51] that made use of a mouse with an array of pins that were actuated
according to outlines and textures. Jansson et al. evaluated this device with 60 sighted
blindfolded participants comparing different map representations. They recommended
that shapes should be represented without any ﬁlling when using such a device. Tixier
et al. [117] designed Tactos, a system made of two devices: one pointing device
moving over the map, and two ﬁxed braille cells placed on another device on the side.
Moving the pointing device determined which information was displayed on the
braille cells. Different pointing devices could be used, such as a stylus on a digitizer
tablet. To our knowledge, Tactos was not evaluated with visually impaired users.
Latero-tactile displays have been designed quite recently and provide different
cutaneous feedback. A latero-tactile display deforms the skin of the ﬁngertip with
an array of laterally moving pins actuated by miniature motors [66, 86]. It can
evoke perceptions like dots, grating (waves) and vibrations. In a ﬁrst study [86], the
latero-tactile device was used to explore a map with two levels of information: one
for the continents and the other for the location of ﬁve civilizations. Users could
switch between these two levels by pressing a key. In this study, the device was
mounted on the Pantograph [15], a haptic device that allows 2-D movement over a
limited surface. In another study, Levesque et al. [66] observed nine blind users
exploring a concert hall seat plan with a latero-tactile display mounted on a mov-
able carrier. The carrier measured the absolute location of the device. These studies
showed that visually impaired people as well as blindfolded sighted people could
successfully explore the maps with the device. However, because this device is a
research prototype, only few projects made use of it.
2.1.3
Finger-Based Exploration
Finger-based exploration, which we also call “direct exploration”, has been used in
many research projects. In that case the input device is one of the user’s ﬁngers,
onto which the position of the cursor is directly mapped. Feedback can be auditory
548
J. Ducasse et al.

or tactile (vibrations). The location of the user’s ﬁngers can be tracked using a
touch-enabled device (e.g., digitizer tablets, smartphones, tablet or tabletops) or a
camera.
Finger tracking on touch-enabled devices
Because smartphones, tablets and digitizer tablets are mainstream consumer devi-
ces, many accessible map projects have used this kind of device. Smartphones and
tablets moreover present the advantage of being usable in mobile situations. Large
tabletops are more expensive and have been used less frequently. Some projects did
not specify the type or size of touch device and might therefore function with
different hardware [1, 83].
Several projects relied on touchpads (i.e., single input devices that are commonly
used by graphic designers and that enable interaction using a ﬁnger or a pen
(Fig. 2)). Jacobson’s prototype [47] allowed visually impaired people to explore
auditory maps by pressing speciﬁc areas on a touchpad (north, west, south, east, and
zoom buttons). Verbal descriptions, ambient sounds (such as trafﬁc noise), and
auditory icons were played during exploration. Five visually impaired and ﬁve
blind people evaluated this device. All of them were able to use it and found it
simple, satisfying and fun. The iSonic project [140] also relied on a touchpad but
with an additional keyboard for exploring choropleth maps. When tapping on a
region, one of ﬁve violin pitches was played according to the choropleth’s value
within that region. Using iSonic, seven participants were able to solve complex
tasks. Participants found the prototype easy to use and helpful.
Fig. 2 Example of a Digital Interactive Map: the map content is displayed on a touch-enabled
device. During exploration the device produces audio output
Accessible Interactive Maps for Visually Impaired Users
549

Recently, smartphones have been used. TimbreMap [109] was a soniﬁcation
interface for exploring ﬂoor-plans of buildings. Stereo auditory feedback indicated
the corrective action needed to follow a path on the screen, or the direction that the
ﬁnger was drifting towards. Six blind participants managed to identify simple
shapes with 80% accuracy. One participant performed a supplementary task in
which he was able to ﬁnd all points of interest on two maps of different com-
plexities. TouchOverMap [93] provided a basic overview of a map layout, by
giving vibrational and vocal feedback when the ﬁnger passed over a map element
(e.g., streets or buildings). The evaluation, performed with eight blindfolded sighted
users, showed that all participants acquired a basic understanding of the
“zoomed-out” and “zoomed-in” version of a map, even though they found the
“zoomed-out” condition difﬁcult.
Several projects relied on the use of tablets that are larger than a smartphone but
smaller than a computer screen. Simonnet et al. [106] designed a tablet application
with auditory and vibrational feedback. Carroll et al. [16] described the design of a
weather map of the USA for visually impaired users. When the user moved his/her
ﬁnger over the tablet, a TTS engine pronounced the name of the states and different
musical notes were played to convey temperature values. When the user was
touching a state containing a minimal or maximal temperature, the device vibrated.
Double-tap was used for zooming. The same design was presented by Lazar et al.
[62]. Another project called Open Touch/Sound Maps [54] provided visually
impaired users with access to OpenStreetMap data using TTS synthesis, vibration
feedback and soniﬁcation. Vibration and TTS description were executed when the
user’s ﬁnger touched a point of interest. Besides, 3-D auditory cues were provided
to indicate the distance between the current ﬁnger location and the next crossroad.
In the framework of the BATS project, Parente and Bishop [82] developed two
prototypes to enable a blind student to explore a map of Great Britain. The user
could explore the map either by moving the on-screen cursor with a mouse or a
trackball or by using a touchscreen directly. TTS and spatialized auditory icons
were provided. An informal evaluation with one user revealed that he preferred
using a trackball to the touchpad. However, an informal evaluation with one user
only is not sufﬁcient to draw conclusions.
Few projects have been developed on tabletops. Kane et al. [57] designed three
interaction techniques for maps displayed on large touchscreens. In a ﬁrst bimanual
technique called “Edge Projection”, locations of map elements were projected onto
the x- and y-axes on the left and lower edges of the screen. Users could thus browse
the axes to ﬁnd the names of on-screen targets. When they identiﬁed a target, they
could drag both ﬁngers from the edges to the interior of the screen in order to locate
it. The second technique, called “Neighborhood Browsing”, expanded the size of
map elements by using the empty area around them. Touching the neighborhood of
a map element launched audio feedback pronouncing the element’s name. The third
technique, called “Touch-and-Speak”, combined touching the screen with speech
recognition, so that a user could, for example, ask for the list of on-screen targets.
Both “Neighborhood browsing” and “Touch-and-Speak” allowed to ﬁnd a speciﬁc
target based on spoken instructions. Fourteen blind users compared these three
550
J. Ducasse et al.

interaction techniques to a regular implementation of Apple’s VoiceOver. The
results revealed that “Touch-and-Speak” was the fastest technique, followed by
“Edge Projection”. Furthermore, there were signiﬁcantly more incorrect answers
with VoiceOver than with the other techniques. Users also ranked “Edge
Projection” and “Touch-and-Speak” signiﬁcantly better than VoiceOver. Another
tabletop based prototype was designed by Yairi et al. [132] to enable users to
follow a line on a map. A line was divided into eight equidistant segments. When
the ﬁnger followed the line, the successive notes of an octave (‘do re mi fa sol la si
do’) were played according to the segment being touched. The authors asked four
blind people to explore a map with streets, and then ﬁnd a route without assistance.
Using this technique, users were able to know the length of each route segment
being touched. Interestingly, they were able to anticipate the next crossing when a
route was made of many lines. Finally, all participants achieved the goal of ﬁnding a
route, even though they sometimes made wrong turns or felt lost.
In the Tikisi project [1] the touchscreen type and size was not speciﬁed. Users
could touch a map and hear the name of the element under their ﬁnger (country,
road, etc.). They could also zoom in and out, scroll, or select a location. Additional
input techniques (keystrokes and speech recognition) were used, and TTS was
provided as output. Twelve visually impaired people used the application and
reported surprise, enjoyment and interest. However, the study did not report any
result about success.
Camera-based ﬁnger tracking
Cameras (including webcams, embedded smartphone cameras, stereo cameras or
motion capture systems) can be used to detect one or multiple ﬁnger(s). In contrast
to touch-enabled devices, they may allow to identify each ﬁnger. On the other hand,
these systems are subject to technical challenges such as lighting conditions and
occlusion.
The “KnowWhere” system [59] was composed of a camera mounted above an
illuminated table. A sound was played each time the user’s ﬁngertip passed over an
element. Users could also enlarge a speciﬁc area in order to access a more detailed
map. Another prototype developed by Schneider and Strothotte [100] allowed the
exploration of an urban area as well as learning a route. The feedback was based on
both speech and sound. Seisenbacher et al. [102] used a similar set up, but with a
color marker attached to one ﬁnger and tracked by a camera mounted above a
tabletop. When the ﬁnger entered an interactive zone, the corresponding sound was
played. AccessLens [56] enabled visually impaired users to interact with paper
documents including elements with labels. The camera placed above the table was
used to decode the labels and recognize the user’s gestures. Users were able to
retrieve the names of the elements, but also to use a gesture or voice command
menu. During the evaluation, ﬁve blind users were asked to explore a diagram, a
map of the USA, or a table presenting poll results. Qualitative results showed that
the participants were highly satisﬁed with the system and the interaction modes,
even if they faced some interaction issues (when placing both hands on the doc-
ument for example).
Accessible Interactive Maps for Visually Impaired Users
551

Bardot et al. [2] used a motion tracking system to track a smartwatch during map
exploration (Fig. 3). Motion tracking allowed both 2-D (exploration) and mid-air
gestures (ﬁltering). The smartwatch provided sound and vibrational feedback, in
addition to different ﬁltering commands. A two-step guidance function based on
vibrations helped users to ﬁnd speciﬁc targets. In a follow-up study, Bardot et al. [3]
used a similar set up to track the user’s dominant ﬁnger. The maps were composed
of several areas, each area being associated with a name and quantitative data.
Three exploration techniques were designed: the “Plain” exploration technique
simply provided auditory feedback when the ﬁnger was entering an area; the
“Filter” technique relied on ﬁlter selection on the smartwatch and enabled the user
to select which data to display; the “Grid-Filter” technique combined the “Filter”
with the use of a virtual 3  3 grid that the user could explore using mid-air
gestures. The evaluation, including 12 visually impaired participants, compared the
exploration of a regular raised-line map to the exploration of a digital map with
these three different techniques. It showed that the exploration of a digital map,
without any tactile cues, is possible. It also showed that the “Grid-Filter” technique
is efﬁcient for data selection or comparison tasks.
Fig. 3 Example of a Digital Interactive Map: one ﬁnger is tracked with a motion-capture system.
During exploration the user’s smartwatch produces audio output and vibrates
552
J. Ducasse et al.

2.1.4
Summary and Conclusions
In this category, we discussed different prototypes that provide visually impaired
people with access to digital maps, i.e., maps whose representation does not rely on
any physical object as elements of the map. The term “virtual”, which is sometimes
used, is appropriate, but should not be used in opposition to visual maps. In
comparison to “visual” maps, these maps could speciﬁcally be called “auditory” or
“somatosensory” maps. We chose to use the term “digital” in order to incorporate in
the category all the prototypes with auditory and/or somatosensory feedback (in
many prototypes both feedback are used simultaneously), but also to stress the
opposition with hybrid maps that combine a physical representation (e.g., raised
lines maps, small-scale models, or refreshable displays) with a digital one.
As we showed, only few accessible interactive map prototypes made use of a
keyboard. A keyboard is very well adapted to text entry, but is less adapted to 2-D
pointing and hence to exploration of spatial data. In the context of accessible maps,
keyboards have mainly been used as a complementary text and command input device.
Joysticks are based on the relative displacement of a cursor, which works well
with visual feedback. In absence of vision, the location of the cursor must be
perceived through speciﬁc auditory or somatosensory feedback. The localization of
the cursor then causes an additional cognitive task that must be performed by the
user. In the prototype of Picinali et al. [87], participants managed to explore a
virtual environment with a joystick. But the environment was rather simple (a
corridor and a few objects), and it is uncertain if a joystick could be used to explore
more complex environments.
In contrast to keyboard and joystick, a tangible pointing device allows the user to
point at speciﬁc locations on a digital map. The location of a tangible pointing device
is perceivable by touch and is trackable with a camera or a touchscreen device. Then
no dedicated hardware is required, and if the object does not provide additional
feedback (e.g., vibrations) it can be very cheap. Indeed, affordable mainstream
devices such as mice, pens and toys can be used with a cheap camera [88].
Many tangible pointing devices provide additional cutaneous or haptic feedback.
It has been shown that such feedback may increase usability. For instance, the
augmentation of a computer mouse with force-feedback that indicates map
boundaries and edges of a grid helped users to maintain spatial orientation [97].
Other pointing devices with tactile displays (e.g., braille cells or latero-tactile dis-
play) also enhance map exploration, for example by enabling users to switch
between different levels of information [66]. However, these devices are lab pro-
totypes, and hence are not available for visually impaired users. Speciﬁc haptic
devices, such as the Geomagic Touch X, are on the market, and also provide
sensory cues that enhance accessibility. However, they are more expensive than
regular pointing devices.
Touchscreens, tablets and smartphones have been frequently used for non-visual
map exploration because they are widespread and affordable. They provide a sur-
face on which the map is displayed, which helps the user to select a reference frame
(e.g., one corner of the touchscreen), and quickly perceive the spatial extent of the
Accessible Interactive Maps for Visually Impaired Users
553

map (it usually corresponds to the size of the touchscreen). In addition, tablets and
smartphones can be used in mobile situations, which may be very important for
visually impaired users. However, touchscreens do not provide cutaneous feedback
associated to the elements that are displayed on the map. They are per se input
devices with visual display [11]. In the absence of tactile cues, it is difﬁcult to
understand the spatial layout and estimate relationships between elements of the
map. It is then necessary to provide additional non-visual feedback for visually
impaired users.
Camera-based digital maps provide the same advantages and drawbacks as maps
based on touch-enabled devices. However, they highly depend on lighting condi-
tions and occlusions, which make them less usable, especially when they are used
in mobility. Finally, maps based on motion capture devices offer enhanced
opportunities because they are adapted to the capture of 3-D gestures. They can
provide mid-air [2, 3] or complex gesture recognition, but they are much more
expensive and need accurate calibration as well as a ﬁxed setting. It is interesting to
note that even though some force-feedback devices (e.g., Geomagic Touch X) can
detect 3-D gestures, no prototype took advantage of this capability.
In conclusion, digital maps can be explored using a variety of input devices, as
well as with touch-and-gesture interaction. Digital maps present many advantages.
Depending of the input device that is used, a digital map prototype can be very
cheap [e.g., 109] and used in various contexts including home, the workplace, or
school. In addition, digital maps can be dynamically updated, and therefore support
operations such as panning and zooming. However, the absence of tactile cues
during non-visual exploration is an important drawback that must be compensated.
2.2
Hybrid Interactive Maps (HIMs)
As we previously mentioned, Hybrid Interactive Maps (HIMs) are made of a digital
and a physical representation. There have been many prototypes in the literature
relying on different physical displays. We classiﬁed HIMs into three categories
according to the type of physical display that was used.
2.2.1
Interactive Tactile Maps (ITMs)
With the term “Interactive Tactile Maps” we refer to physical representations that
correspond to tactile maps (see 1.1 Context). In contrast with tangible or refreshable
maps, which we will introduce hereafter, tactile maps are static. The content cannot be
dynamically updated. The only way to change the map view (zoom or pan for
instance) is to change, or erase and redraw the physical representation. In the category
of tactile maps, raised-line maps have been extensively used, but more recently 3-D
printed maps have emerged. Two different technologies have then been used to track
the user’s ﬁnger(s) over the tactile map: touch-enabled devices or cameras.
554
J. Ducasse et al.

ITMs with ﬁnger tracking on touch-enabled devices
Up to now, this category only includes Interactive Raised-line Maps but other types
of overlays could be considered, such as 3D printe overlays. Raised-line maps have
proved beneﬁcial for spatial learning by visually impaired people for many years,
and are thus a familiar tool for most of them [121]. Several prototypes relied on this
observation, and aimed at making raised-line maps interactive. In those maps, a
raised-line overlay is placed over a touchscreen device that allows the detection of
touch inputs through the overlay. Users perform taps or double-taps on any inter-
active element of the overlay, which produces speech, sound, or vibrational feed-
backs (Fig. 4).
Parkes [83] was the ﬁrst to design an interactive raised-line map with an overlay
placed over a touchscreen. Even though the technical aspects of his NOMAD
prototype were not precisely described, Parkes envisioned that gestural interactions
(for example “touching two points for direction and distance interaction”) combined
with appropriate auditory feedback could greatly enhance the use of tactile maps.
Since then, several research projects have been developed along this direction. The
prototype described by Weir et al. [127] presented a weather forecast map of the
USA. When the user selected a state by tapping on it, a sound was played whose
pitch encoded temperature (the higher the pitch, the higher the temperature). The
prototype of Senette et al. [103] provided the names of the streets using a TTS
engine. It was enriched with non-speech sounds and vibrations that were activated
when the user touched pedestrian zones. Hamid and Edwards [38] presented an
interactive raised-line map placed on a turntable. When rotating the turntable, users
Fig. 4 Example of an Interactive Tactile Map: a raised-line map overlay is placed on top of a
touch-enabled device. During exploration the device produces audio output
Accessible Interactive Maps for Visually Impaired Users
555

could explore a route from an egocentered point of view (i.e., from the traveler’s
perspective). In addition to the satisfaction provided by the map rotation, users
reported that sounds attributed to ground textures were especially useful.
As described above, Parkes envisioned using gestural interaction in interactive
raised-line maps. To our knowledge, only Brock et al. [7] implemented gestural
interaction in such a map type in order to access distances between two points or to
get additional information about map elements.
Mappie [9] was an extension of this previous prototype. Mappie’s overlay used
different colors in order to be accessible not only for blind, but also for low-vision
and sighted people, which enabled them to collaborate. In addition, a menu bar was
included to enable users to choose between different types of spatial content. Then,
Brulé et al. [9] developped the MapSense prototype that relied on the same inter-
active raised-line maps, but provided 14 additional conductive tangibles. Some
tangibles could be ﬁlled with scents, such as olive oil, smashed raisins, or honey,
thus creating a real multisensory experience including olfactory cues.
Interactive Raised-Line Maps are used in the wild. ABAplans5 is a project
initiated by the Engineering School of Geneva (Ecole d’Ingénieurs de Genève). The
device is based on a raised-line map overlay placed over a mono-touch-sensitive
surface. It includes a speciﬁc map editor, and allows users to explore a city map, to
ﬁnd a speciﬁc place, to prepare journeys, and to learn about public transportation. It
is currently distributed by the AudioTactile association. The company ViewPlus
commercializes a similar device called IVEO.6 It includes a mono-touch-sensitive
surface and an editor for sketching raised-line maps and drawings. It is possible to
purchase additional software that allows creating raised-line images from PDF or
scanned documents using optical character recognition. In both cases, users must
own the mandatory equipment to print raised-line maps.
Weir et al. [127] compared the usability of an Interactive Raised-Line Map to the
usability of a Digital Interactive Map based on a touchscreen or a keyboard. The
evaluation showed that the participants preferred the Interactive Raised-Line
Map. A few studies also compared the usability of Interactive Raised-Line Maps to
regular (non interactive) raised-line maps (see, e.g., [125]). Six blind users raised
positive comments on the clarity of information that was provided. They found that
the device was easy to use and helpful for pedestrian navigation. In a follow-up
study, Wang et al. [126] compared an Interactive Raised-Line Map with a tactile
map without any textual information. They observed that users preferred the
interactive map. Furthermore, they observed that the interactive map was quicker in
64% of all cases for identifying start and end points of a route, but not for route
exploration. However this comparison should be considered with precaution
because users spent most of the time listening to the audio output in the interactive
condition, whereas the tactile map did not contain any textual information. Brock
et al. [8] compared the usability of a regular raised-line map to an Interactive
5http://www.audiotactile.ch/ [last accessed June 3rd 2016].
6https://viewplus.com/product/iveo-hands-on-learning-system/ [last accessed June 3rd 2016].
556
J. Ducasse et al.

Raised-Line Map that displayed comparable content. The evaluation included 24
blind participants that were required to explore an unknown neighborhood. The
study assessed the time needed for exploration, the accuracy of the spatial learning,
and the satisfaction of the users. The results showed that interactivity signiﬁcantly
shortened exploration time and increased user satisfaction.
While the previous studies have been done in a laboratory setting, other studies
have been performed in a classroom scenario. Brulé et al. [9] conducted a formative
study with the Mappie prototype over several months in a specialized education
center. The results were promising and provided insights in the design of accessible
interactive maps using other sensory modalities (i.e., olfaction and taste) to foster
learning. They recommended using additional interactive objects to support sto-
rytelling. The MapSense prototype has also been used during two classes made by a
locomotion trainer and a specialized teacher. The authors observed that the device
triggered strong positive emotions and stimulated learning as well as creativity.
ITMs with a camera-based ﬁnger tracking
Interactive Raised-Line Maps usually combine a touch-enabled device with a
raised-line overlay. Technically this works because touch inputs can be detected
through the overlay (Fig. 5). Another way of tracking the user’s ﬁngers is to use a
camera. With the Tactile Graphic Helper [28], a visually impaired user can place a
tactile map on a regular table and then interact with it. The camera placed above the
Fig. 5 Example of camera-based ﬁnger tracking combined with a raised-line map. During
exploration the device produces audio output
Accessible Interactive Maps for Visually Impaired Users
557

tactile drawing recognizes the layout and tracks the user’s ﬁngers. The user can
point at elements and ask for information. The Tactile Graphic Helper aimed at
allowing visually impaired students to discriminate tactile symbols (texture, Braille
labels, etc.) without requiring the help of a sighted person.
Sullivan and Picinali [110] used another technology, the Leap Motion©, to detect
pointing movements toward elements of a tactile map placed over an inclined table.
The user had to perform a distinct pointing gesture to select a speciﬁc element. Two
types of auditory feedback were given, either to provide users with detailed infor-
mation about speciﬁc elements, or to guide them along a route. Spatial sounds were
also played, as, for example, sounds of ﬂowing water when a river was selected.
Götzelmann and Pavkovic [34] designed interactive 3-D printed maps. The
production of 3-D maps was automatic so that visually impaired users could make
them without assistance. Once printed, the ﬁngers were tracked with a smartphone
held above the 3-D map. The application identiﬁed the map with a printed barcode,
and, in addition, helped the user to correctly hold the smartphone over the
map. With the other hand, the user was free to explore the map, and received
auditory feedback when pointing at elements.
Erasable Tactile Map
Recently, an interesting prototype based on 3-D printing has been designed by
Swaminathan et al. [111]. LineSpace was a platform that included a movable 3-D
printer head mounted over a drafting table. The system could print and erase spatial
content based on vocal commands and deictic gestures. The gestures were detected
by a camera tracking the markers afﬁxed to one of the user’s ﬁngers. It provided
visually impaired users with the possibility to dynamically draw and explore spatial
content. Among the different applications described in the paper, one allowed users
to search for real estates within a city map. Once the map was printed, the user
could retrieve additional information about a particular estate. For rescaling the map
or exploring a new part, the system printed a new map on a blank part of the
drawing table, which enabled the user to switch back and forth between two dif-
ferent views. The system could also remove elements that have been previously
printed using a “scraper.”
2.2.2
Tangible Maps
Tangible user interfaces combine physical objects with digital data, and thus enable
interaction with the digital world through the use of physical artifacts [120]. As
already mentioned, we make a distinction between digital maps that are based on a
tangible pointing device (see 2.1.1) and tangible maps per se. Tangible maps are
made of several physical objects that represent map elements and allow bimanual
exploration (Figs. 6 and 7). Users can also manipulate the tangible objects in order
to (re)construct or edit the map. On the contrary, tangible pointing devices (such as
558
J. Ducasse et al.

Fig. 6 Tangible Reels are composed of a sucker pad and a retractable reel and are used to
construct tangible maps. The user is guided step by step to correctly place each Tangible Reel until
the whole map is reconstructed. During exploration, audio feedback is provided whenever the user
points at one element with one ﬁnger
Fig. 7 MapSense combines an interactive raised-line map with additional conductive tangibles.
Some tangibles can be ﬁlled with scents, such as olive oil, smashed raisins or honey, thus creating
a multi-sensory experience including olfactory and gustatory cues. The Photograph is licensed
under Creative Commons BY-NC-ND
Accessible Interactive Maps for Visually Impaired Users
559

computer mice, pens or toys) do not represent any element of the map but only
serve as a pointing device.
Schneider and Strothotte [101] designed a prototype that enabled visually
impaired students to independently construct an itinerary using building blocks of
various lengths. The system indicated the length and orientation of the next building
block that had to be placed. The user’s dominant ﬁnger was tracked during
exploration of the virtual map. More recently, Ducasse et al. [22] designed a novel
type of physical icons, called Tangible Reels, which were used to render map points
(cities, bus stops, etc.) and lines (streets, rivers, boundaries, etc.) tangible. The
Tangible Reels were made of stable objects combined with a retractable reel. The
user was guided by audio instructions to correctly place and link the objects on a
tabletop, until the whole map was constructed. The user could then retrieve the
name of the elements with a pointing gesture above the objects and the links. The
evaluation, conducted with eight visually impaired users, showed that the interface
enabled the construction and exploration of maps of various complexities (up to
twelve objects), in a short amount of time (24 seconds on average to place and link
an object), and with very few errors (283 out of 288 objects were correctly placed
during the whole evaluation).
2.2.3
Refreshable Tactile Maps
Refreshable Tactile Maps refer to maps that are physically rendered (using a matrix
of pins for example), and that the system can dynamically update. Such devices
may intuitively provide blind users with access to any content, at any time. They
can represent a complete drawing with different heights for the relief (also called
2.5D7). The display can be refreshed dynamically, and hence allows any update, as
well as zooming and panning.
Raised-pin displays rely on pins that are raised mechanically either by electro-
magnetic technologies, piezoelectric crystals, shape memory alloys, pneumatic
systems, or heat pump systems [24]. Vidal-Verdú and Hafez [123] referred to this
type of devices as static refreshable displays: they are equivalent to a screen “where
pixels are replaced by taxels, i.e., touch stimulation units.” Visually impaired
people are accustomed to this type of display because they frequently use dynamic
braille displays that are based on a similar principle [5]. Braille displays are used to
display textual information. They are made of one or two lines of 40–80 cells, each
with 6 or 8 movable pins that represent the dots of a braille cell.
Four map prototypes have been designed using a large display composed of
actuated pins. These displays present information in relief. The user moves the hand
across the display to explore the content. Obviously, it is possible to dynamically
72.5D has been deﬁned in opposition to 2D (ﬂat and without relief) and 3D (object). 2.1D is
sometimes used to refer to relief with just one height. Braille cells are 2.1D devices because the
raised pins can reach one position only.
560
J. Ducasse et al.

change the content or to highlight elements by raising or recessing speciﬁc pins.
Zeng and Weber [137] used the BrailleDis 9000 tablet which was composed of
7200 pins arranged in a 60  120 matrix, and actuated by piezoelectric cells.
Touch sensors allowed the user to provide input to the system (tap or double-tap for
instance). Zeng and Weber designed a tactile symbol set for displaying different
types of information such as bus stops or buildings. In a second publication, they
improved the tactile symbol set as well as the prototype, and introduced the ATMap
prototype that enabled users to pan and zoom but also to share annotations [139]. In
their study, Schmitz and Ertl [99] used the HyperBraille display8, a commercial
version of the BrailleDis 9000, to display two types of maps: detailed maps rep-
resenting buildings or small outdoor areas, and overview maps retrieved from
OpenStreetMap. The prototype also provided panning and zooming functionalities.
When the user typed in an address or the name of a point of interest, the map was
refreshed. Ivanchev et al. [46] also used the BrailleDis 9000 to present routes to
visually impaired users. They compared several patterns to help the user visualize
the routes by varying the thickness of the lines or making the pins blink for
example. An observation made on one subject indicated that the blinking mode was
efﬁcient. Panning, zooming, layering and searching functionalities were also
implemented. Shimada et al. [105] constructed a display with 3072 raised pins. In
addition, this system included a scroll bar that indicated which part of the image
was inside the displayed area. This device also contained touch sensors in order to
track user inputs.
2.2.4
Summary and Conclusions
In this large category of Hybrid Maps, we presented map prototypes that include an
interactive physical representation of the map. The most studied prototypes are
undoubtedly the Interactive Raised-line Maps. They have been evaluated in many
studies, and altogether, these studies show that maps combining touch devices with
raised-line overlay and audio output are highly usable for acquiring spatial
knowledge in absence of vision. In addition, they are now on the market and are
being used in many situations like at home or at school. The downside of these
devices lies in the necessity of tactile overlays, whose production is time consuming
and relies on tactile document makers. In commercial offers, they are sold as
packages for a region or a country. Tactile overlays also limit the usage of the
device in mobility, because of physical bulkiness and weight [58]. Furthermore,
zooming or panning operations are not possible without placing another tactile
overlay, which may cause cognitive issues.
It is therefore interesting that some projects aimed at automatically producing
tactile overlays either by retrieving spatial data from a Geographic Information
Systems, or based on image recognition and segmentation. Campin et al. [14]
8http://www.hyperbraille.de/?lang=en [last accessed August 21st 2013].
Accessible Interactive Maps for Visually Impaired Users
561

described a prototype that allowed the user to select a SVG map on a dedicated web
site, and then to print it at home with a thermal enhancer. In the “Talking TMAP”
project [74], the maps were automatically generated and could be ordered online.
Wang et al. [125] implemented a system that automatically created interactive
tactile maps from map images.
In the same subcategory, we placed 3-D printed maps that are interactive thanks
to a camera that tracks the ﬁngers. 3-D printers are affordable, and can be used to
print small neighborhoods, but also small-scale models of buildings, objects, etc.
They already spread over specialized education centers, but to our knowledge, no
interactive device has already been used on the wild. Talking small-scale models
are now used in many public places. They are now designed by a few companies
(see, e.g., Talking Tactile Campus, Touch Graphics, Inc., USA). They rely on
touch-sensitive zones (not on camera-based ﬁnger tracking) and proved useful for
wayﬁnding. However, they are quite expensive and cannot be updated.
At the intersection between Interactive Tactile Maps and Refreshable Tactile
Maps, LineSpace [111] is a research prototype that could be used to dynamically
print and erase interactive maps on a drawing board. However, the time required to
print a map was not indicated by the authors. It is therefore unclear whether the map
could be readily updated or not. Obviously, LineSpace could be used in speciﬁc
places such as specialized education centers but not in mobility.
The second subcategory of HIMs called “Tangible Maps” is promising, but there
is surprisingly little work on that topic. This may be explained by the fact that
Tangible Maps, although they can be relatively cheap (a webcam, a PC, and a table
are enough), still require a large tabletop and several objects, which is cumbersome.
Commercial interactive tabletops that identify and track tangible objects also exist
(e.g., the Reactable9) but are expensive. One of the main advantages of Tangible
Maps is that they support the autonomous construction of map, which may help the
users to understand and memorize the spatial layout of the map [22]. Besides,
because the map representation can be easily manipulated by the user, it is possible
to “refresh” the map by moving the objects or adding and removing objects. It is
therefore technically possible to rescale or reposition the map by moving the
objects. In conclusion, tangible interfaces are not yet mainstream devices, and
cannot be used in mobility. However, we guess that in the near future they should
become more frequent in public spaces such as museums, schools, etc.
The last subcategory was called Refreshable Tactile Maps. It includes maps
whose physical representation can be automatically updated, which is an out-
standing advantage. However, the more advanced devices, namely raised-pin dis-
plays, are still very expensive. In 2007, Vidal-Verdu and Hafez [123] estimated that
a display large enough to be explored with two hands would require more than
75,000 pins, and would approximatively cost 270,000 €. In 2012, the HyperBraille
display with 60  120 pins was sold 50,000 €. Hence, current raised-pin displays
9http://reactable.com/.
562
J. Ducasse et al.

are generally small, with a low resolution [137]. However, they can be used in
mobility [140], which is very important for visually impaired people.
3
Discussion
3.1
Comparison of Interactive Map Devices
As shown above, there is a large variety of devices allowing visually impaired users
to explore maps. Each type of devices that we described in the previous sections
presents advantages, drawbacks, and limitations. To our knowledge, the different
types of devices that we presented in this chapter have not been systematically
compared. We believe that doing so could help designers and researchers better
identify the advantages and drawbacks of each device depending on the tasks that
they intend to support, the context in which the prototype will be used, the sensory
capacities of the users, etc.
At the end of the two previous sections (Summary and conclusions), we already
discussed the advantages and limitations of the devices used in terms of cost,
availability, and technological limitations. We also mentioned whether the proto-
types could be used in mobility, and if they allow readily updating the map rep-
resentation. In the following sections, we discuss the different categories in terms of
content, factors impacting map comprehension, and interactivity.
3.1.1
Map Content
Maps are used to display spatial content that is made of points, lines and areas, with
accompanying text. All the different types of Accessible Interactive Maps that we
previously described do not provide the same opportunities to render these different
elements. DIMs that are explored with a tangible artifact mainly rely on auditory
feedback. It is therefore possible to render maps that are made of several landmarks
or separated areas such as states, but the type, number and resolution of rendered
elements is obviously limited. Even though 3-D sounds may increase the quantity
and quality of auditory feedback, maps from this category are quite simple in
general. In fact, most of the prototypes based on 2-D pointing devices were used to
display choropleth maps, i.e., maps with regions only [6, 20, 141], or to display a
limited number of landmarks [76, 90].
These limitations are very similar when the DIM is directly explored with the
ﬁngers. Finger-based exploration prevents from having reference frames issues
(when the visually impaired user does not exactly know where the device is
pointing) but cannot provide more complex maps. In particular, it has been
observed that the simple exploration of lines is problematic [58]. For example,
Poppinga et al. [93] reported that participants were not able to know whether two
Accessible Interactive Maps for Visually Impaired Users
563

roads were close to each other or were crossing each other. They also had difﬁ-
culties to identify the direction of short roads, and were less accurate when
redrawing a “zoom-out” map than a “zoom-in” map, i.e., when more roads were
displayed on the screen. Additional feedback can be provided with the vibrations of
the devices, but the vibrations are not spatialized (i.e., the whole device vibrates),
and hence cannot provide accurate cutaneous feedback. Consequently, most of the
existing Digital Interactive Maps based on ﬁnger exploration are quite simple, and
only display a very limited number of elements [57, 89, 106, 109, 131].
DIMs that are based on less conventional tactile feedback (force-feedback,
latero-tactile displays, Braille mice, etc.) can be more complex or detailed. For
instance campus maps, building plans, country maps with various cities and/or
areas, street maps, etc., have been designed. Indeed, a number of cutaneous and
haptic feedbacks can be used to render various elements such as boundaries, tex-
tures, points of interest, etc. [see 33]. Furthermore, haptic devices can help to
recognize geometric properties of objects [5].
Hybrid Interactive Maps are very useful for visually impaired users because the
relief or the objects used to display the map represent useful tactile cues. As we
already mentioned, raised-line maps have been criticized for their low resolution
when compared to visual maps but they remain the best way for displaying complex
content, using different patterns of points (e.g., triangles and circles), lines (e.g.,
dotted lines and plain lines), and areas (ﬁlled and half-ﬁlled). When making them
interactive, Braille labels can be removed, and new elements can be added [8].
Besides, the fact that they support two-handed exploration is highly beneﬁcial [128].
When compared to Interactive Tactile Maps, Tangible Maps are more limited by
the number and type of elements that they can render. Indeed, physical icons are
generally relatively large, which limits the number of points of interest that can be
simultaneously represented [see, e.g., 22]. Only two prototypes make it possible to
represent lines using either physical bricks [101] or retractable strings [22].
Although the former prototype allowed the construction of routes only, the latter
allowed the construction of complex representations. There are many advantages to
physical lines: users can easily locate and identify them, but also interact with them,
which can be used to provide useful pieces of information on borders, rivers,
transportation routes, etc. Yet, there is still a challenge on rendering areas with
Tangible Maps.
Finally, raised-pin displays are really adapted for rendering various patterns of
symbols [139] and lines [46], but their resolution is drastically limited by the size of
the device and the number of pins. Then, even though it is possible to display
various points of interests, lines and areas, the current prototypes cannot be used to
display complex maps. Besides, several pins are needed to distinguish different
symbols [139], which requires a lot of space, and hence impacts map resolution. In
fact, they cannot reach the complexity of raised-line maps.
564
J. Ducasse et al.

3.1.2
Factors Impacting Map Comprehension
Number of points of contact
As we already mentioned, constraining the exploration of a map to a single point of
contact makes the map exploration cognitively more challenging. Indeed, in order
to build a mental representation of the map being explored, users must perform a
complex cognitive integration along space and time. They must integrate the path of
their own movement and of the movement of the cursor with a transfer function if a
pointing device is used. In addition, users must integrate the tactile cues under their
ﬁngers, which are related to each position over time [58]. This complex integration
process is cognitively demanding, and may impact user’s performances. For
example, Loomis et al. [70] showed that exploring a raised-line drawing with one
ﬁnger only (i.e., one point of contact) is similar to exploring a visual drawing with a
narrow ﬁeld of view (the size of a ﬁngertip). Not surprisingly, two-handed
exploration of tactile images proved to be more efﬁcient than exploration with one
hand only [128].
Among DIM prototypes, only the GRAB [45] prototype, which relied on a
force-feedback device with two handles, provided more than one point of contact.
The evaluation showed that using a second point of contact helped users “orientate
themselves in space, more readily understand object’s relationships (distribution
and distance) and make re-ﬁnding objects easier.”
Because they rely on a physical representation of the map, HIMs allow
two-handed exploration. In addition, most of them provide multiple interactive
points of contacts. Among Interactive Tactile Maps, earlier prototypes (see Abaplan
for instance) relied on a single interactive contact point. More recent Interactive
Tactile Maps [see, e.g., 8] provide many interactive contact points, which support
multiple ﬁngers and gesture command menus. Interestingly, Brock et al. [8] ob-
served that visually impaired users may prefer deactivating audio output when
exploring a map for the ﬁrst time. Finger tracking in Tangible Maps can either rely
on cameras [101] or infrared frames [21]. Even though in the prototype of
Schneider et al. only one ﬁnger was tracked, it is now possible to track multiple
ﬁngers using a camera. Thus, it would be interesting to design gestural interactions
and evaluate their usability. As for raised-pin displays, they can detect touches and
therefore offer a wide range of possibilities to interact with the map.
Haptic frame of reference
Another important aspect is the possibility offered to the user to build a reliable
mental reference frame during tactile exploration. Millar et al. [75] showed that
using an external (allocentred) reference frame to encode spatial information
resulted in better performances.
DIMs based on keyboards and joysticks do not provide a reliable external haptic
reference frame. Indeed, the displacement of the cursor is relative to the previous
position, and its current position within the map cannot be inferred by touching the
pointing device. Both Zhao et al. [141] and Delogu et al. [20] reported that using a
keyboard made it difﬁcult for the user to know the cursor position within the
Accessible Interactive Maps for Visually Impaired Users
565

map. DIMs based on mice and handles do provide a better feedback to build a
haptic reference frame, but can still generate confusions if the movement of the
cursor is not directly mapped to the movement of the pointing device. DIMs based
on direct exploration provide a more reliable reference frame because the mapping
between the hand and the map is ﬁxed. In addition the outline of the touch-enabled
device can be perceived, which represents efﬁcient tactile landmarks. When using a
camera to track the hand instead of a touch-enabled device, these tactile cues are
missing. Therefore, some prototypes included a rigid frame that delimited the
exploration area [see 3].
Most of force-feedback devices also provide an external reference frame because
the displacement of the device is in general constrained. The device cannot move
over physical limits in space. Rice et al. [97] reported that such a frame was very
helpful. However, the physical limits of the device can be used as a reference frame
if they correspond to a static view of the map. If the map view is displaced when the
user pushes toward one side (sometimes called “inertial displacement”), then, in
absence of efﬁcient feedback, the reference frame is lost.
Obviously, HIMs provide a very stable and reliable haptic reference frame.
Indeed, any static point of the tactile display (relief, identiﬁed item, edge of the
display, etc.) can serve as a reference point, and anchor the reference frame. The
reference frame can then be fed by all the other static tactile elements displayed in
the map.
3.1.3
Map Interactivity
Updating map content, zooming and panning
All the accessible interactive map prototypes that we described share a basic fea-
ture: the user can select a map element to retrieve its name. However, very few
prototypes provided additional interactions with the map representation (e.g.,
panning, zooming, ﬁltering, highlighting) or eventually the map content (e.g.,
annotation, edition).
DIMs can be dynamically updated and are not limited by physical constraints. In
that sense, they are similar to visual maps that sighted users can access online, and
share the potential of providing visually impaired users with access to a large
quantity of geospatial data. However, performing zooming and panning operations
on DIMs, without any tactile cue, leads to sensory and cognitive challenges that
have not yet been addressed in any experimental study.
Unlike DIMs, Hybrid Interactive Maps (HIMs) are shaped and constrained by
their physical representation. Interactive Tactile Maps are constrained by the tactile
overlay, which cannot be dynamically altered. Different map contents, but also
different views or different scales, must be rendered with different tactile overlays.
Of course, it is possible to pre-print these different overlays, and dynamically call
the corresponding digital content when one overlay is being used. However, when
566
J. Ducasse et al.

zooming or panning, users must interrupt the ongoing exploration in order to
replace the overlay, and start a new exploration process after the corresponding
digital content has been called. In order to link the mental representations corre-
sponding to both maps, they have to ﬁnd reference points that were on both
overlays. This procedure clearly leads to cognitive challenges again. Furthermore,
users cannot select a scale or a view that has not been previously prepared. At the
intersection between Interactive Tactile Maps and Refreshable Maps, LineSpace
(Erasable Tactile Map) did not provide regular panning and zooming operations.
Instead, a new map with a different view or scale was printed over a blank space
around the map being explored. However, the cognitive issues that we already
mentioned (interruption of the current exploration and ﬁnding common reference
points) also apply in that case.
The dynamicity of Tangible Maps is also constrained. Moving, adding or
removing tangible objects is possible. Obviously, rescaling and repositioning take
some time, but it renders the representation more ﬂexible than regular tactile maps
such as raised-line maps or 3-D small-scale models. Nevertheless, further studies
are required to investigate whether Tangible Maps raise perceptual and cognitive
issues. In the GeoSpace tangible and visual map [119] users could rotate and rescale
the map by moving two objects, but only a few elements of the maps were rendered
using physical objects. Shaer et al. [104] refer to this problem as a problem of
scalability: when one user needs to zoom in or zoom out, all the objects need to be
repositioned, i.e., the whole map needs to be reconstructed.
Refreshable Tactile Maps are the most dynamic interactive maps. It is possible to
update the content instantly, but also to provide advanced interactive functions such
as zooming and panning operations [105] or annotation [140].
Adapted exploration functions
Additional functions that help visually impaired users to explore maps are not
speciﬁc to DIMs or HIMs. They can provide verbal descriptions and guidance,
spatial and semantic ﬁltering, but also speciﬁc computations (e.g., distance between
two points). Some functions have been implemented and evaluated.
As we previously mentioned, it is a challenge to ﬁnd and relate speciﬁc points
when the exploration is tactile, especially when it is performed with only one
contact point. For instance, Kane et al. [57] developed three interaction techniques
to help the user locate, relocate and relate points of interest on a map. The
Talking TMAP prototype [74] provided assistance to ﬁnd a location, and calculate
distances, but also provided a menu for modifying the settings (sensitivity, unity of
measure, and speech rate).
With the Interactive Raised-Line Map called Mappie [9], children could also rely
on audio instructions to locate speciﬁc points of interests. In addition, they were
able to choose among different types of spatial content (city, countries, etc.) for the
same overlay, which is very useful for teachers. Indeed, the same tactile overlay
may be used in conjunction with different digital contents. It may then serve suc-
cessive lessons of the same discipline, but also different disciplines. One tactile
overlay representing a country can for instance be used for Geography lessons and
Accessible Interactive Maps for Visually Impaired Users
567

the digital content can be updated as the lesson goes on. The same overlay can also
be used for a lesson in Economics, with a different digital content.
Other prototypes provided access to complex functionalities. For instance,
iSonic [141] included a “gist” of the map, which was a sequence of sounds that
provided users with an “overview” of the regions. A subpart of the map could then
be selected. The iSonic prototype provided additional functions such as:
details-on-demand, adjust information level, situate (give the current status of the
interface), select (only the regions selected triggered audio feedback and are played
for the gist), brush (between the table and the map views), ﬁlter and search. Bardot
et al. [2, 3] also implemented several interaction techniques for spatial or semantic
ﬁltering of the map content. A ﬁrst one allowed the user to get a spatial overview of
the map before being guided towards a speciﬁc point of interest. A second one
enabled users to select the content to be displayed, and therefore avoided the
cumbersome exploration of undesired items.
3.2
Geographic Maps for Visually Impaired People
and Spatial Cognition
Maps serve a concrete purpose: acquiring spatial knowledge about an environment.
In the following sections, we present studies on acquisition of spatial knowledge
without sight that have been done using tactile maps or interactive maps.
3.2.1
Tactile Maps and Spatial Cognition
Some studies speciﬁcally investigated the beneﬁts of tactile maps for spatial
learning for visually impaired adults. Jacobson [48] compared sketch maps drawn
before and after reading tactile maps. He observed that the sketch maps drawn after
map reading included far more description and details than the initial sketch maps.
In another study [50], visually impaired adults failed in estimating relative distances
between towns, but succeeded in locating towns on a partially complete tactile
map. Furthermore they successfully determined which map was correct out of a set
of three rotated maps. In a follow-up study [47], sketch maps proved to be more
accurate for a group of participants which had explored an audio-tactile map
compared to subjects having walked the same route with a mobility instructor.
Espinosa et al. [26] observed that participants who learned a route from a combi-
nation of direct experience and tactile map reading had a better spatial knowledge
of the environment than those learning the route from direct experience only or
verbal descriptions. In a second experiment, participants performed just as well
after exploring a tactile map as after direct experience in the environment. Similarly,
Caddeo et al. [13] observed that participants who had access to a tactile map
showed better performance in walking time and a reduced deviation from the route
568
J. Ducasse et al.

as compared to participants directly experiencing the environment with or without
verbal descriptions. Similar results have been found, when studying tactile maps
with visually impaired children [122].
Different hypothesis explain why tactile maps are well adapted for acquiring
spatial knowledge in the absence of vision. Tactile maps preserve relations between
landmarks in space but present those relationships within one or two hand-spans
[121]. Thinus-Blanc and Gaunet [116] argue therefore that the exploration of a
tactile map demands a smaller working load than the exploration of a real envi-
ronment. Also they outline that during tactile exploration of space, it is possible to
keep a ﬁxed reference point, whereas during the exploration of space via loco-
motion, the participant is moving and so is the reference system (the own body).
Besides, the tactile map is simpliﬁed in content and therefore free from the per-
turbations that can be present in the environment [121].
3.2.2
Interactive Maps and Spatial Cognition
Several studies investigated if interactive maps can be used for acquiring spatial
knowledge by visually impaired people. Very often these studies compared dif-
ferent groups of users including early and late blind, but also people with residual
visual perceptions, and blindfolded sighted.
First we report the studies that have been done with blindfolded sighted people,
and which may present limited validity due to the perceptual and cognitive dif-
ferences between visually impaired and sighted people. Poppinga et al. [93] asked
eight sighted users to explore a smartphone application with vibration and audio
output and to draw a map of the perceived environment. They compared two zoom
levels and observed that the “zoom in” condition resulted in a more accurate
drawing than “zoom out” condition. Participants correctly perceived basic infor-
mation concerning the map, but also relations between map elements. However, the
authors suggested that the task was cognitively demanding as participants needed
up to 15 min for sketching a rather small map. In another study [68], 24 blindfolded
sighted subjects compared two conditions of a DIM prototype with speech output.
In the ﬁrst condition only names of landmarks and routes were indicated. In the
second condition, additional information about the relationships between geo-
graphic items was provided. Participants acquired more precise spatial knowledge
in the second condition. However, the result also depended on the type of spatial
knowledge. Scores related to landmarks showed a larger difference between the two
conditions than scores related to routes. Pielot et al. [88] evaluated a DIM with a
tangible pointing device with eight blindfolded sighted participants. The results
showed that the tangible pointing device, called a virtual listener, allowed detecting
small deviations from the real orientation. Milne et al. [76] compared two proto-
types of a DIM with ﬁve blindfolded sighted participants. One device was based on
a stylus while the other was based on body rotation. They observed issues related to
the shifting of reference frames between egocentred and allocentred perspectives.
Accessible Interactive Maps for Visually Impaired Users
569

Other studies were done with blind participants and therefore present a higher
validity. Among these studies, a few evaluated DIMs are based on touchscreen
devices and audio output. Jacobson [47] asked ﬁve visually impaired and ﬁve blind
people to evaluate a map prototype based on a touchpad with auditory feedback. He
analyzed verbal descriptions, map drawings, and qualitative feedback. He observed
that all users successfully created mental representations following the DIM
exploration. Besides, the users found the interface simple, satisfying and fun. In the
study of Heuten et al. [42], eleven blind users explored a DIM based on a touchpad
and a stylus, with the instruction of understanding spatial relationships between
map elements. Users found the exploration easy, except for the identiﬁcation of the
shapes of the elements. The authors also observed confusions when two similar
objects were close to each other. Yairi et al. [132] asked four blind people to
explore a DIM with musical feedback, and then walk the route unaided. All par-
ticipants reached the goal, even if one was unsure about it. Even if people made
wrong turns at cross points or felt lost, they were able to correct their route.
Simonnet et al. [107] observed two blind sailors learning a maritime environment
with a haptic device and then navigate at sea. Their study revealed that using the
map was beneﬁcial because the users had to mentally coordinate egocentred and
allocentred maps. In a more recent study, Picinali et al. [87] compared the results of
ﬁve blind participants who walked along a corridor versus ﬁve blind participants
who explored the DIM with a joystick. The results showed that all participants were
able to build correct mental representations that were similar to the reference
map. All these studies show that DIMs with audio output can effectively be used for
creating mental maps.
As we mentioned, DIMs can be augmented with vibrations. In the study of
Simonnet et al. [106], one blind user explored a DIM based on a tablet with
auditory and vibratory feedback. After exploration, the user had to draw a map of
the explored environment. They observed that the drawing was relatively precise. In
the study of Yatani et al. [133], ten blind and two low-vision participants explored a
DIM based on a smartphone with audio and vibratory output, but in two different
conditions. In the ﬁrst condition, the subjects used the smartphone with auditory
output only. They received additional vibratory feedback through nine motors in the
second condition. They were then asked to draw the maps. The drawings were more
accurate with additional tactile feedback.
A few studies investigated the acquisition of spatial knowledge with HIMs.
Jacobson [49] compared route learning with a mobility instructor versus with an
Interactive Tactile Map (touchscreen with tactile overlay). The author used many
methods including verbal description, map drawing, distances by ratio-scaling,
tactile scanning assessment, and talk aloud protocol. The results showed that both
groups were able to complete the route and verbally describe it. All sketches
showed a high degree of completion and correctness but the group who had
explored the interactive map was more accurate. Participants also provided positive
qualitative feedback. More recently, in a study with 24 blind participants, Brock
et al. [8] compared the usability of an Interactive Tactile Map versus a regular
raised-line map with braille legend. They measured the time required to explore the
570
J. Ducasse et al.

maps, the correctness of the spatial knowledge acquired during exploration, and the
satisfaction. The results showed that Interactive Tactile Map was signiﬁcantly more
efﬁcient, and preferred among users.
Finally, Zeng and Weber [136] evaluated the acquisition of spatial knowledge
using a Refreshable Map with ten blind users. They compared it to two other
conditions including a regular raised-line map, and a DIM displayed on a tablet.
They measured the subjects’ performances in reading street names and preparing a
journey. The results showed that participants were able to perform these tasks using
the raised-pin device or the raised-line map, but failed to perform them on the
tablet.
In conclusion, it appears that DIMs can be used to acquire spatial knowledge but,
except in the study of Yairi et al. [132], spatial learning was limited to simple
topological features (relative positions of items within the map).
In some of these studies, but also in the literature in psychology, there have been
contradictory results showing that early blind, late blind, visually impaired, and
sighted subjects outperform each other [17]. Other factors, including expertise in
tactile reading, education level, but also the type of drawing being read, might have
been confounded with the visual status. In any case, we suggest that a greater access
to interactive maps is mandatory for visually impaired users and will decrease the
differences between the different groups of subjects.
3.3
Maps and Other Graphics
The challenges related to the accessibility of maps for visually impaired people are
similar to those observed when designing accessible interfaces for other types of
interactive graphics. By graphics, we refer to a variety of materials whose layout is
used to provide spatial content to the reader including diagrams, ﬁgures, drawings,
as well as maps [115]. In this section, we present a non-exhaustive list of Interactive
Graphics prototypes. They illustrate how interactive devices providing non-visual
access to maps and graphics are similar. As for maps, it is easy to classify the
Interactive Graphics prototypes in two categories (Digital and Hybrid Interactive
Graphics, which we call DIGs and HIGs) and their subcategories.
3.3.1
Examples of Interactive Graphics
A number of DIG prototypes were based on touch-sensitive surfaces or motion
capture. For example, the AudioFunctions prototype combined a novel soniﬁcation
technique with touch interaction to enable a visually impaired person to explore a
mathematical function [113]. Users performed better using the prototype than using
a raised-line diagram. Other prototypes [30, 134] enabled visually impaired users to
identify simple shapes displayed on a touchscreen device. TouchMelody [95] aug-
mented raised-line diagrams with spatial non-speech sounds. Both the index of the
Accessible Interactive Maps for Visually Impaired Users
571

non-dominant hand (used as a reference point) and the index of the dominant hand
(used to explore the diagram) were tracked by a motion capture system, and a sound
was played according to the position of one index as compared to the other one.
Force-feedback devices have also been used. The prototype designed by Yu and
Brewster [135] could be used with either a Geomagic Touch X device or a Logitech
WingMan Force-Feedback mouse, and enabled visually impaired users to explore
line graphs or bar charts. McGookin and Brewster [72] designed a prototype that
enabled users to drag the bar of a bar chart. Bernareggi et al. [4] developed a system
that enabled users to insert, delete, connect or disconnect nodes.
Giudice et al. [32] combined gestural input with audio and vibratory feedbacks.
When users moved a ﬁnger over the tablet, vibratory patterns indicated whether
they were touching edges or vertices. This prototype proved efﬁcient for the
exploration and understanding of bar charts as well as for letter recognition tasks
and orientation discrimination tasks.
Pointers with additional tactile feedback have been used in other prototypes.
Wall and Brewster [124] used a stylus for pointing combined with a mice with an
array of pins (VTPlayer). The user pointed to different zones of the graphs, and
received tactile cues according to the section of the pie chart being explored.
Pietrzak et al. [92] used a similar device to provide directional cues that guided the
users during the exploration of geometrical shapes. Levesque et al. [65] showed that
simple and small shapes could be rendered using the STRESS latero-tactile display
and three primitive drawings (dots, vibration and gratings).
Examples of HIGs include the tangible prototype for the non-visual exploration
of graphs by McGookin et al. [73]. This system combined a ﬁxed grid and movable
objects that represented the top of a bar or the turning point of a linear function.
When moving a slider along the x-axis, the user was able to retrieve the corre-
sponding y-values, which were soniﬁed. Similarly, TIMMs [71] were objects that
provided multimodal feedback and enabled blind persons to create and modify
graphs and diagrams.
3.3.2
Maps and Graphics: A Comparison
As previously mentioned, raised-line maps are bulky, not interactive, but their
accessibility can be improved by making them interactive and dynamic. The same
applies to any type of raised-line graphics. So far, most research projects have either
focused on graphics or on maps. Obviously, researchers could beneﬁt from taking
into account both of these application areas. Yet, it remains to be explored to what
extent the issues raised by interactive maps are similar to those raised by interactive
graphics.
One essential point is that all graphics, whether they are drawings, maps, bar
charts, diagrams, etc., are solely made of four primitives: dots, lines, areas, and
labels. Moreover, colors and textures are often used in order to improve readability.
Graf [36] distinguished the propositional representation (verbal annotations) from
the spatial representation (the map itself that represents the topology of the
572
J. Ducasse et al.

environment). Considering multiple dimensions (including layout, complexity,
dynamicity and usages), we did not ﬁnd any signiﬁcant difference between the
properties of these two representations related either to diagrams or maps. Therefore
no matter the ﬁeld of application, ﬁndings concerning the legibility of the symbols
and representations with a particular content or representation could apply to any
other content or representation. For example, we cited a few articles that aimed at
understanding how simple geometrical drawings that are displayed on a touch-
screen could be identiﬁed using audio and vibrational feedbacks [e.g., 32, 65].
These ﬁndings could inform the design of both interactive maps and graphics
prototypes. Obviously, we would need to perform comparative experiments in order
to ensure that, except slight differences in complexity, there are no speciﬁc needs
according to the type of graphics that is rendered.
3.4
Ongoing and Future Research in Interactive Tactile
Graphics
There are a few topics that we wanted to address which represent current challenges
for research in accessible interactive graphics.
3.4.1
Rich Open and Volunteered Data
In this chapter we have mainly discussed the design of accessible interactive map
prototypes. Yet, for the usability of maps, the availability and reliability of geo-
graphic content is as important as the design of the devices. Indeed map prototypes
will not be used outside of research laboratories if adapted geographic data is not
available. In Sect. 2.2.4, we presented a few projects that aim at automating the
production of adapted content or at facilitating the collection of volunteered geo-
graphic data. We suggest that OpenStreetMap is particularly relevant as speciﬁc
accessibility tags such as tactile paving can be added, [25]. Then, visually impaired
users, but also sighted users that want to participate, can create annotations when
they are traveling or exploring the map [44, 96]. Sighted users can also volunteer to
add details based on street view images [39] or on their own knowledge of the places.
3.4.2
Authoring Tools and Content Sharing
Rich and adapted open data is not the only challenge. Up to now, the automatic
production of maps is still difﬁcult and the intervention of professionals is required
(see 1.1). Besides, authoring tools are not common and that they are mainly cir-
cumscribed to research projects. Researchers should closely work with tactile
graphic specialists in order to better understand how the production of adapted
Accessible Interactive Maps for Visually Impaired Users
573

content could be further automated. Authoring tools may then include adaptation
functions that help experts but also non experts to create accessible content that is
adapted to be displayed on DIMs or HIMs. Such tools may encourage professionals
to create and share accessible map content. Successful projects have been devel-
oped [34, 53, 54] and should now be tested in the ﬁeld, to evaluate their long-term
impact upon the accessibility of graphical data for visually impaired users.
3.4.3
Shape-Changing Interfaces
Interactive Tactile Maps present a high usability because they provide reliable
tactile cues and ahaptic reference frame, as well as interactivity. However, they are
constrained by the physical overlay, which prevents dynamic zooming and panning.
In contrast, refreshable displays, which include both physical and digital repre-
sentations, offer remarkable possibilities for dynamic interaction with maps and
graphics (including, zooming, panning, annotation, ﬁltering, etc.). In addition, they
enable users to explore the maps with both hands, which is more efﬁcient. However,
those devices are currently very expensive, which prevents a large adoption by
visually impaired users and professionals. Moreover, current prototypes only pro-
vide a small surface size.
Low-cost and large refreshable displays are still at infancy, but a number of
approaches are promising. Shape memory alloy actuators change shape when they
are heated. Voice-coil motors and piezoelectric actuators may also provide solutions
for larger displays [see 80, 123 for reviews]. Wilhelm et al. [130] developed a
prototype based on microﬂuidic phase change actuators. The actuators are ﬁlled
with a phase change material that can be heated. When pressure is applied, the
membrane laid over the display bulges. Taher et al. [112] investigated new inter-
actions with physical bar charts. Other research works have to be mentioned, such
as Relief [64], inFORM [27], and Lumen [94]. These physical visualization devices
could increase the accessibility of graphics for visually impaired users in the future.
3.4.4
3-D Printing with Embedded Interactivity
There are several limitations to 3-D printing such as the relatively high cost for
acquiring a printer, time for printing, limited size of the printed object, and lack of
interactivity once printed. However, it is an emerging tool for the production of
maps, graphics, and books for visually impaired users. We already described the
work of Götzelmann and Winkler [35] who automated the production of interactive
3-D maps, which in contrast to raised-line maps can provide multiple levels of
relief. Many other recent studies show the importance of 3-D printing for visually
impaired users.
Buehler et al. [10] reported that the creation of educational materials is one of the
three primary functions of 3-D printing in special education. With 3-D printing, it is
possible to print customized objects that can be used to explore or annotate maps.
574
J. Ducasse et al.

For example, users can explore a virtual map by moving and rotating a toy above a
table [88]. Such a toy could be personalized using 3-D printing in order to get the
students more engaged with the exploration task. Brulé et al. [9] used 3-D printing
to create tangible objects that could be used in association with an Interactive
Tactile Map device. Such objects could be placed on a tactile map to augment the
information that is displayed or to highlight important elements. Giraud and
Jouffrais [31] showed that 3-D printing with low-cost prototyping resources
empowers specialized teachers to create their own teaching material. Swaminathan
et al. [111] proposed a sense-making platform for blind people using dynamic 3-D
printing. Finally, Gual et al. [37] compared the use of 3-D printed symbols versus
2-D tactile symbols in a tactile map. They found that 3-D symbols were easier to
memorize than 2-D symbols.
Recent work has also shown that 3-D printed objects are not limited to non
interactive plastic structures. Current 3-D printers can print soft and interactive
objects that embed conductive ﬁlaments [85]. Objects produced with those printers
could greatly enhance the interactivity of Tactile Maps and Graphics as they could
potentially vibrate, move, emit sounds, or detect how they are grabbed.
4
Conclusion
In this chapter we presented an overview of accessible interactive maps for visually
impaired people. We identiﬁed two families of Interactive Maps that differ
according to the presence or absence of a physical representation of the map, which
is useful for visually impaired users because it is perceivable by touch. The ﬁrst
family was called Digital Interactive Maps (DIMs) and relies on a digital repre-
sentation of the map only. The second family was called Hybrid interactive Maps
(HIMs) and relies on both digital and physical representations of the map. We
deﬁned subcategories in each family that, hopefully, may help to structure the
research ﬁeld.
In each family, we have observed a large variety of prototypes based on various
input and output interaction devices. They have leveraged the design of non-visual
interaction
techniques
allowing
visually
impaired
users
to
explore
a
map. Additional functions have been designed too, allowing zooming, panning,
annotating, sharing, visualizing over time, etc.
However, these different devices come with advantages and shortcomings. DIMs
based on pointing devices such as touch-enabled screens or video tracking are
available at low prices and thus affordable for everyone. They can easily be used in
many situations (home, school, mobility, etc.) but they miss tactile cues that are
useful for non-visual exploration because they facilitate haptic integration, and also
provide a reliable haptic reference frame. Hence they must be designed for speciﬁc
conditions such as mobility, and for compensating the absence of tactile cues. In
addition, spatial content should not be too complex.
Accessible Interactive Maps for Visually Impaired Users
575

Based on a physical representation, HIMs are more adapted to non-visual
exploration. We observed that one type of HIM—Interactive Raised-Line Maps—
has been largely addressed in the literature (design and evaluation). Because they
have a high usability, they are now spreading in the wild, and devices are being
used in specialized education centers. Of course, they still suffer from the necessity
of printing raised-line maps in advance, which is manageable but costs time and
money. Refreshable displays supposedly provide an alternative; however, they are
not yet available with a sufﬁcient resolution at affordable cost. Tangible Maps have
not been extensively studied so far. With the spreading of TUIs, but also 3-D
printers and low-cost prototyping technologies, we predict that, during the
upcoming years, these devices will get further addressed as a research question, but
also more used in the wild. Indeed, although spatial resolution will always be
limited by the size of the physical icons, they provide an appropriate setup to
explore and manipulate spatial data without vision, especially in collaboration with
other sighted or non-sighted users.
We have to mention that, nowadays, it is still difﬁcult for visually impaired
people to access geographic information. Yet, non-visual access to geographic
information is crucial for education, as well as mobility and orientation. It has
signiﬁcant consequences on personal and professional occupations, and on social
participation. Therefore, it is very important that future research work on maps
focuses on the design and evaluation of novel interaction techniques, but also on
devices that are readily usable. As an agenda for short- and mid-term research, we
suggest topics that should be addressed: improving the accessibility of Digital Maps
with wearable technologies; facilitating the autonomous (without the intervention of
a tactile document maker) making of Interactive Maps; and designing interaction
techniques that provide visually impaired people with more interactive functions
such as zooming, panning, annotating, and collaborating. Obviously, researchers
and designers should always keep in mind that maps serve a purpose: the acqui-
sition of spatial knowledge. In order to validate that the devices effectively serve
this purpose it is necessary to conduct user studies with visually impaired users.
When possible, new displays should be studied in situ, i.e., outside the lab, to better
understand how and why interactive maps and graphics are used.
References
1. Bahram
S
(2013)
Multimodal
eyes-free
exploration
of
maps:
TIKISI
for
maps.
ACM SIGACCESS Accessibility Comput 106:3–11. doi:10.1145/2505401.2505402
2. Bardot S, Brock A, Serrano M, Jouffrais C (2014) Quick-glance and in-depth exploration of
a tabletop map for visually impaired people. In: Proceedings of the 26th conference on
interaction homme-machine—IHM ‘14. ACM Press, New York, pp 165–170. doi:10.1145/
2670444.2670465
3. Bardot S, Serrano M, Jouffrais C (2016) From tactile to virtual: using a smartwatch to
improve spatial map exploration for visually impaired users. In: Proceedings of the 18th
international conference on human-computer interaction with mobile devices and services—
MobileHCI ’16. ACM Press, New York, pp 100–111. doi:10.1145/2935334.2935342
576
J. Ducasse et al.

4. Bernareggi C, Comaschi C, Marcante A, Mussio P, Parasiliti Provenza L, Vanzi S (2008) A
multimodal interactive system to create and explore graph structures. In: Proceeding of the
twenty-sixth annual CHI conference extended abstracts on human factors in computing
systems—CHI ’08 . ACM Press, New York, p. 2697. doi:10.1145/1358628.1358747
5. Brewster S, Brown LM (2004) Tactons: structured tactile messages for non-visual
information display. In: AUIC’04 Proceedings of the ﬁfth conference on Australasian user
interface. Australian Computer Society, Inc, pp 15–23
6. Brittell M, Young M, Lobben A (2013) The MGIS: a minimal geographic information
system accessible to users who are blind. In Proceedings of the 21st ACM SIGSPATIAL
international conference on advances in geographic information systems—SIGSPATIAL’13
. ACM Press, New York, pp 554–557. doi:10.1145/2525314.2525329
7. Brock AM, Truillet P, Oriola B, Jouffrais C (2014) Making gestural interaction accessible to
visually impaired people. In: EuroHaptics, LNCS 8619, pp. 41–48. Retrieved from http://
link.springer.com/chapter/10.1007/978-3-662-44196-1_6
8. Brock AM, Truillet P, Oriola B, Picard D, Jouffrais C (2015) Interactivity improves usability
of geographic maps for visually impaired people. Hum Comput Interact 30:156–194
9. Brule E, Bailly G, Brock A, Valentin F, Denis G, Jouffrais C (2016) MapSense:
multi-sensory interactive maps for children living with visual impairments. In: International
conference for human-computer interaction (CHI 2016). ACM, San Jose, pp 445–457
10. Buehler E, Kane SK, Hurst A (2014) ABC and 3D: opportunities and obstacles to 3D
printing in special education environments. In: Proceedings of the 16th international
ACM SIGACCESS conference on computers & accessibility—ASSETS ’14. ACM Press,
New York, pp 107–114. doi:10.1145/2661334.2661365
11. Buxton W (2007) Multi-touch systems that i have known and loved. Retrieved from http://
www.billbuxton.com/multitouchOverview.html
12. C2RP (2005) Déﬁcience Visuelle—Etudes et Résultats. Lille, France
13. Caddeo P, Fornara F, Nenci AM, Piroddi A (2006) Wayﬁnding tasks in visually impaired
people: the role of tactile maps. Cogn Process 7:168–169. doi:10.1007/s10339-006-0128-9
14. Campin B, McCurdy W, Brunet L, Siekierska E (2003) SVG maps for people with visual
impairment. In: SVG open conference. Vancouver, Canada. Retrieved from http://www.
svgopen.org/2003/papers/svgmappingforpeoplewithvisualimpairments/
15. Campion G (2005) The pantograph MK-II: a haptic instrument. In: The synthesis of three
dimensional haptic textures: geometry, control, and psychophysics. Springer, New York,
pp 45–58
16. Carroll D, Chakraborty S, Lazar J (2013) Designing accessible visualizations : the case of
designing a weather map for blind users. In: Universal access in human-computer
interaction. Design methods, tools, and interaction techniques for inclusion. Springer, Berlin,
pp 436–445
17. Cattaneo Z, Vecchi T (2011) Blind vision: the neuroscience of visual impairment.
Rehabilitation. MIT Press, Cambridge
18. Daunys G, Lauruska V (2009) Soniﬁcation system of maps for blind—alternative view. In:
Stephanidis C (ed) Universal access in human-computer interaction. Intelligent and
ubiquitous interaction environments, vol 5615. Springer, Berlin, pp 503–508. doi:10.1007/
978-3-642-02710-9
19. De Felice F, Renna F, Attolico G, Distante A (2007) A haptic/acoustic application to allow
blind the access to spatial information. In: Second joint eurohaptics conference and
symposium on haptic interfaces for virtual environment and teleoperator systems (WHC’07),
pp 310–315. IEEE. doi:10.1109/WHC.2007.6
20. Delogu F, Palmiero M, Federici S, Plaisant C, Zhao H, Belardinelli O (2010) Non-visual
exploration of geographic maps: does soniﬁcation help? Disabil Rehabil Assistive Technol 5
(3):164–174. doi:10.3109/17483100903100277
21. Ducasse J, Macé M, Jouffrais C (2015) From open geographical data to tangible maps:
improving the accessibility of maps for visually impaired people. Int Arch Photogrammetry
Accessible Interactive Maps for Visually Impaired Users
577

Remote Sens Spat Inf Sci XL(3): 517–523. doi:http://dx.doi.org/10.5194/isprsarchives-XL-
3-W3-517-2015
22. Ducasse J, Macé M, Serrano M, Jouffrais C (2016) Tangible reels: construction and
exploration of tangible maps by visually impaired users. In: Proceedings of the 2016 CHI
conference on human factors in computing systems, pp 2186–2197
23. Edman P (1992) Tactile graphics. AFB press, New York
24. El Saddik A, Orozco M, Eid M, Cha J (2011) Haptics technologies—bringing touch to
multimedia. Springer, Berlin
25. El-Safty A, Schmitz B, Ertl T (2014) An OpenStreetMap editing interface for visually
impaired users based on geo-semantic information. In: Proceedings of ICCHP 2014, LNCS,
vol 8548, pp 116–119
26. Espinosa MA, Ungar S, Ochaita E, Blades M, Spencer C (1998) Comparing methods for
introducing blind and visually impaired people to unfamiliar urban environments. J Environ
Psychol 18(3):277–287
27. Follmer S, Leithinger D, Olwal A, Hogge A, Ishii H (2013) inFORM: dynamic physical
affordances and constraints through shape and object actuation. In: Proceedings of the 26th
annual ACM symposium on user interface software and technology—UIST ’13. ACM Press,
New York, pp 417–426. doi:10.1145/2501988.2502032
28. Fusco G, Morash VS (2015) The tactile graphics helper: providing audio clariﬁcation for
tactile
graphics
using
machine
vision.
In:
Proceedings
of
the
17th
international
ACM SIGACCESS conference on computers and accessibility. ACM, New York, pp 97–
106. doi:10.1145/2700648.2809868
29. Gentaz É (2003) General characteristics of the anatomical and functional organization of
cuntaneous and haptic perceptions. In Hatwell Y, Streri A, Gentaz E (eds) Touching for
knowing: cognitive psychology of haptic manual perception. John Benjamins Publishing,
Amsterdam/Philadelphia, pp 17–31
30. Gerino A, Picinali L, Bernareggi C, Alabastro N, Mascetti S (2015) Towards large scale
evaluation of novel soniﬁcation techniques for non visual shape exploration. In: ASSETS
’15 the 17th international ACM SIGACCESS conference on computers and accessibility.
ACM, New York, pp 13–21. doi:10.1145/2700648.2809848
31. Giraud S, Jouffrais C (2016) Empowering low-vision rehabilitation professionals with
“do-it-yourself” methods. Springer International Publishing, New York, pp 61–68. doi:10.
1007/978-3-319-41267-2_9
32. Giudice NA, Palani HP, Brenner E, Kramer KM (2012) Learning non-visual graphical
information using a touch-based vibro-audio interface. In: Proceedings of the 14th
international ACM SIGACCESS conference on computers and accessibility—ASSETS
’12. ACM Press, New York, pp 103–110. doi:10.1145/2384916.2384935
33. Golledge RG, Rice M, Jacobson RD (2005) A commentary on the use of touch for accessing
on-screen spatial representations: the process of experiencing haptic maps and graphics. Prof
Geogr 57(3):339–349
34. Götzelmann T, Pavkovic A (2014) Towards automatically generated tactile detail maps by
3D printers for blind persons. In: Proceedings of ICCHP 2014, LNCS, vol 8548, pp 1–7
35. Götzelmann T, Winkler K (2015) SmartTactMaps: a smartphone-based approach to support
blind persons in exploring tactile maps. In: Proceedings of the 8th ACM international
conference on pervasive technologies related to assistive environments—PETRA ’15 . ACM
Press, New York, pp 1–8. doi:10.1145/2769493.2769497
36. Graf C (2010) Verbally annotated tactile maps—challenges and approaches. In: Hölscher C,
Shipley TF, Olivetti Belardinelli M, Bateman JA, Newcombe NS (eds) Spatial cognition VII,
LNCS Volume 6222, vol 6222. Springer, Berlin, pp 303–318. doi:10.1007/978-3-642-14749-4
37. Gual J, Puyuelo M, Lloveras J (2014) Three-dimensional tactile symbols produced by 3D
Printing: Improving the process of memorizing a tactile map key. Br J Vis Impairment 32
(3):263–278. doi:10.1177/0264619614540291
38. Hamid NNA, Edwards ADN (2013) Facilitating route learning using interactive audio-tactile
maps for blind and visually impaired people. In: CHI EA ’13 extended abstracts on human
578
J. Ducasse et al.

factors in computing systems. ACM Press, New York, pp 37–42. doi:10.1145/2468356.
2468364
39. Hara K, Froehlich JE, Azenkot S, Campbell M, Bennett CL, Le V, Ng RH (2013) Improving
public transit accessibility for blind riders by crowdsourcing bus stop landmark locations
with Google street view. In: Proceedings of the 15th international ACM SIGACCESS
conference on computers and accessibility—ASSETS ’13, pp 1–8. doi:10.1145/2513383.
2513448
40. Hatwell Y, Streri A, Gentaz É (2003) Touching for Knowing: cognitive psychology of
haptic manual perception. In: Hatwell Y, Streri A, Gentaz E (eds) John Benjamins
Publishing Company
41. Heller MA (1989) Picture and pattern perception in the sighted and the blind: the advantage
of the late blind. Perception 18(3):379–389
42. Heuten W, Henze N, Boll S (2007) Interactive exploration of city maps with auditory
torches. In: CHI EA ’07 extended abstracts on human factors in computing systems. ACM,
New York, pp 1959–1964
43. Hinton RAL (1993) Tactile and audio-tactile images as vehicles for learning. In: Non-visual
human-computer interactions: prospects for the visually handicapped : Proceedings of the
INSERM-SETAA conference, vol 228. John Libbey Eurotext, Paris, pp 169–179
44. Holone H, Misund G (2007) Users are doing it for themselves : Pedestrian navigation with
user generated content. In: International conference on next generation mobile applications,
services and technologies, (Ngmast), pp 91–99
45. Iglesias R, Casado S, Gutierrez T, Barbero JI, Avizzano CA, Marcheschi S, Bergamasco M
(2004) Computer graphics access for blind people through a haptic and audio virtual
environment. In: Haptic, audio and visual environments and their applications, 2004. HAVE
2004. IEEE Press, pp 13–18
46. Ivanchev M, Zinke F, Lucke U (2014) Pre-journey visualization of travel routes for the blind
on refreshable interactive tactile displays. In: Miesenberger K, Fels D, Archambault D,
Peňáz P, Zagler W (eds) Proceedings of ICCHP 2014, LNCS Vol 8548, vol 8548. Springer
International Publishing, Paris. doi:10.1007/978-3-319-08599-9
47. Jacobson R (1998) Navigating maps with little or no sight: an audio-tactile approach. In:
Proceedings of content visualization and intermedia representations. Montréal, Québec,
Canada, pp 95–102
48. Jacobson RD (1992) Spatial cognition through tactile mapping. Swansea Geogr 29:79–88
49. Jacobson RD (1996) Talking tactile maps and environmental audio beacons: An orientation
and mobility development tool for visually impaired people. In: ICA Comission on maps and
diagrams for blind and visually impaired people: needs, solutions and developments.
Ljubjiana, Slovenia, pp 1–22
50. Jacobson RD, Kitchin RM (1995) Assessing the conﬁgurational knowledge of people with
visual impairments or blindness. Swansea Geogr 32:14–24
51. Jansson G, Juhasz I, Cammilton A (2006) Reading virtual maps with a haptic mouse: effects
of some modiﬁcations of the tactile and audio-tactile information. Br J Vis Impairment 24
(2):60–66. doi:10.1177/0264619606064206
52. Jetter H-C, Leifert S, Gerken J, Schubert S, Reiterer H (2012) Does (multi-) touch aid users’
spatial memory and navigation in “panning” and in “zooming & panning” UIs? In :
Proceedings of the international working conference on advanced visual interfaces—AVI
’12. ACM Press, New York, pp 83–90. doi:10.1145/2254556.2254575
53. Kaklanis N, Votis K, Moschonas P, Tzovaras D (2011) HapticRiaMaps: towards interactive
exploration of web world maps for the visually impaired. In: Proceedings of the international
cross-disciplinary conference on web accessibility—W4A ’11 (p. 20). ACM Press, New
York
54. Kaklanis N, Votis K, Tzovaras D (2013a) A mobile interactive maps application for a
visually impaired audience. In: Proceedings of the 10th international cross-disciplinary
conference on web accessibility. ACM, New York, pp 23:1–23:2. doi:10.1145/2461121.
2461152
Accessible Interactive Maps for Visually Impaired Users
579

55. Kaklanis N, Votis K, Tzovaras D (2013) Open touch/sound maps: a system to convey street
data through haptic and auditory feedback. Comput Geosci 57:59–67. doi:10.1016/j.cageo.
2013.03.005
56. Kane SK, Frey B, Wobbrock JO (2013) Access lens: a gesture-based screen reader for
real-world documents. In: Proceedings of the SIGCHI conference on human factors in
computing systems—CHI ’13 (p. 347). ACM Press, New York. doi:10.1145/2470654.
2470704
57. Kane SK, Morris MR, Perkins AZ, Wigdor D, Ladner RE, Wobbrock JO (2011) Access
overlays: improving non-visual access to large touch screens for blind users. In: Proceedings
of the 24th annual ACM symposium on user interface software and technology—UIST ’11.
ACM Press, New York, pp 273–282. doi:10.1145/2047196.2047232
58. Klatzky RL, Giudice NA, Bennett CR, Loomis JM (2014) Touch-screen technology for the
dynamic display of 2D spatial information without vision: promise and progress.
Multisensory Res 27(5–6):359–378. doi:10.1163/22134808-00002447
59. Krueger MW, Gilden D (1997) KnowWhereTM: an audio/spatial interface for blind people.
In: Proceedings of the fourth international conference on auditory display (ICAD)’97. Palo
Alto, CA
60. Lahav O, Mioduser D (2008) Construction of cognitive maps of unknown spaces using a
multi-sensory virtual environment for people who are blind. Comput Hum Behav 24
(3):1139–1155. doi:10.1016/j.chb.2007.04.003
61. Lawrence MM, Martinelli N, Nehmer R (2009) A haptic soundscape map of the university
of oregon. J Maps 5(1):19–29. doi:10.4113/jom.2009.1028
62. Lazar J, Chakraborty S, Carroll D, Weir R, Sizemore B, Henderson H (2013) Development
and evaluation of two prototypes for providing weather map data to blind users through
soniﬁcation. J Usability Stud 8(4):93–110
63. Lederman SJ, Klatzky RL (2009) Haptic perception: a tutorial. Atten Percept Psychophys 71
(7):1439–1459. doi:10.3758/APP.71.7.1439
64. Leithinger D, Ishii H (2010) Relief: a scalable actuated shape display. In: Proceedings of the
fourth international conference on Tangible, embedded, and embodied interaction—TEI ’10.
ACM Press, New York, p 221. doi:10.1145/1709886.1709928
65. Lévesque V, Hayward V (2008) Tactile graphics rendering using three laterotactile drawing
primitives. In: 2008 Symposium on haptic interfaces for virtual environment and teleoperator
systems. IEEE, pp 429–436. doi:10.1109/HAPTICS.2008.4479989
66. Levesque V, Petit G, Dufresne A, Hayward V (2012) Adaptive level of detail in dynamic,
refreshable tactile graphics. IEEE Haptics Symp (HAPTICS), 1–5. doi:10.1109/HAPTIC.
2012.6183752
67. Lloyd R (2000) Understanding and learning maps. In: Kitchin R, Freundschuh S
(eds) Cognitive mapping: past present and future (Routledge). Taylor & Francis, New
York, pp 84–107
68. Lohmann K, Habel C (2012) Extended verbal assistance facilitates knowledge acquisition of
virtual tactile maps. In: Stachniss C, Schill K, Uttal D (eds) Spatial cognition VIII, LNCS
7463, vol 7463, pp 299–318. Springer, Berlin. doi:10.1007/978-3-642-32732-2
69. Lohmann K, Kerzel M, Habel C (2010) Generating verbal assistance for tactile-map
explorations. In: van der Sluis I, Bergmann K, van Hooijdonk C, Theune M (eds) 3rd
Workshop on multimodal output generation (MOG 2010). Dublin, Ireland, pp 27–35
70. Loomis JM, Klatzky RL, Lederman SJ (1991) Similarity of tactual and visual picture
recognition with limited ﬁeld of view. Perception 20(2):167–177
71. Manshad MS, Pontelli E, Manshad SJ (2012) Trackable interactive multimodal manipu-
latives: towards a tangible user environment for the blind. In: Miesenberger K, Karshmer A,
Peňáz P, Zagler W (eds) ICCHP 2012, vol 7383. Springer, Berlin, pp 664–671. doi:10.1007/
978-3-642-14097-6
72. McGookin DK, Brewster SA (2007) Graph builder: constructing non-visual visualizations.
People Comput XX—Engage. Springer, London, pp 263–278
580
J. Ducasse et al.

73. McGookin D, Robertson E, Brewster S (2010) Clutching at straws: using tangible interaction
to provide non-visual access to graphs. In: Proceedings of the 28th international conference
on human factors in computing systems—CHI ’10. ACM Press, New York, pp 1715–1724.
doi:10.1145/1753326.1753583
74. Miele JA, Landau S, Gilden D (2006) Talking TMAP: automated generation of audio-tactile
maps using Smith-Kettlewell’s TMAP software. Br J Vis Impairment 24(2):93–100
75. Millar S, Al-Attar Z (2004) External and body-centered frames of reference in spatial
memory: evidence from touch. Percept Psychophys 66(1):51–59. doi:10.3758/BF03194860
76. Milne AP, Antle AN, Riecke BE (2011) Tangible and body-based interaction with auditory
maps. In: CHI EA ’11 extended abstracts on human factors in computing systems. ACM
Press, New York, p 2329. doi:10.1145/1979742.1979874
77. Montello DR (1993) Scale and multiple psychologies of space. In: Frank AU, Campari I
(eds) Spatial information theory: a theoretical basis for GIS. Springer, Berlin, pp 312–321
78. Montello DR (2010) You are where? The function and frustration of you-are-here
(YAH) maps. Spat Cogn Computation 10(2–3):94–104. doi:10.1080/13875860903585323
79. National Federation of the Blind (2009) The Braille literacy crisis in America: facing the
Truth. Reversing the Trend, Empowering the Blind
80. O’Modhrain S, Giudice NA, Gardner JA, Legge GE (2015) Designing media for
visually-impaired users of refreshable touch displays: possibilities and pitfalls. IEEE Trans
Haptics 8(3):248–57. doi:10.1109/TOH.2015.2466231
81. Paladugu DA, Wang Z, Li B (2010) On presenting audio-tactile maps to visually impaired
users for getting directions. In: CHI EA ’10. ACM Press, Atlanta, pp 3955–3960. doi:10.
1145/1753846.1754085
82. Parente P, Bishop G (2003) BATS : the blind audio tactile mapping system. In: Proceedings
of ACM south eastern conference. ACM Press, Savannah
83. Parkes D (1988) “NOMAD”: an audio-tactile tool for the acquisition, use and management
of spatially distributed information by partially sighted and blind persons. In: Tatham A,
Dodds A (eds) Proceedings of second international conference on maps and graphics for
visually disabled people. Nottingham, United Kingdom, pp 24–29
84. Passini R, Proulx G (1988) Wayﬁnding without vision: an experiment with congenitally,
totally blind people. Environ Behav 20(2):227–252. doi:10.1177/0013916588202006
85. Peng H, Mankoff J, Hudson SE, McCann J (2015) A layered fabric 3D printer for soft
interactive objects. In: Proceedings of the 33rd annual acm conference on human factors in
computing systems—CHI ’15. ACM Press, New York, pp 1789–1798. doi:10.1145/
2702123.2702327
86. Petit G, Dufresne A, Levesque V, Hayward V, Trudeau N (2008) Refreshable tactile
graphics applied to schoolbook illustrations for students with visual impairment. In: Assets
’08 proceedings of the 10th international ACM SIGACCESS conference on computers and
accessibility. ACM Press, New York, pp 89–96. doi:10.1145/1414471.1414489
87. Picinali L, Afonso A, Denis M, Katz BFG (2014) Exploration of architectural spaces by
blind people using auditory virtual reality for the construction of spatial knowledge. Int J
Hum Comput Stud 72(4):393–407. doi:10.1016/j.ijhcs.2013.12.008
88. Pielot M, Henze N, Heuten W, Boll S (2007) Tangible user interface for the exploration of
auditory city map. In: Oakley I, Brewster S (eds) Haptic and audio interaction design, LNCS
4813 (LNCS, vol 4813). Springer, Berlin, pp 86–97. doi:10.1007/978-3-540-76702-2
89. Pielot M, Poppinga B, Boll S (2010) PocketNavigator: vibro-tactile waypoint navigation for
everyday mobile devices. In: MobileHCI 2010, pp 423–426
90. Pielot M, Poppinga B, Heuten W, Boll S (2011) A tactile compass for eyes-free pedestrian
navigation. In Campos P, Graham N, Jorge J, Nunes N, Palanque P, Winckler M
(eds) Human-computer interaction - interact 2011, LNCS 6947. Springer, Lisbon, pp 640–
656
91. Pietrzak T, Crossan A, Brewster SA, Martin B, Pecci I (2009) Creating usable pin array
tactons for non-visual information. IEEE Trans Haptics 2(2):61–72
Accessible Interactive Maps for Visually Impaired Users
581

92. Pietrzak, T., Martin, B., Pecci, I., Saarinen, R., Raisamo, R., & Järvi, J. (2007). The micole
architecture : multimodal support for inclusion of visually impaired children. In Proceedings
of the ninth international conference on Multimodal interfaces - ICMI ’07 (p. 193). New
York, New York, USA: ACM Press. doi:10.1145/1322192.1322227
93. Poppinga B, Magnusson C, Pielot M, Rassmus-Gröhn K (2011) TouchOver map:
audio-tactile exploration of interactive maps. In: Proceedings of the 13th international
conference on human computer interaction with mobile devices and services—MobileHCI
’11. ACM Press, New York, pp 545–550. doi:10.1145/2037373.2037458
94. Poupyrev I, Nashida T, Maruyama S, Rekimoto J, Yamaji Y (2004) Lumen: interactive
visual and shape display for calm computing. In: ACM SIGGRAPH 2004 emerging
technologies on—SIGGRAPH ’04 (p. 17). ACM Press, New York. doi:10.1145/1186155.
1186173
95. Ramloll R, Brewster S (2002) A generic approach for augmenting tactile diagrams with
spatial non-speech sounds. In: CHI ’02 extended abstracts on human factors in computing
systems—CHI ‘02 (p. 770). ACM Press, New York. doi:10.1145/506443.506589
96. Rice MT, Jacobson RD, Caldwell DR, McDermott SD, Paez FI, Aburizaiza AO, Qin H
(2013) Crowdsourcing techniques for augmenting traditional accessibility maps with
transitory obstacle information. Cartography Geographic Inf Sci 1–10. doi:10.1080/
15230406.2013.799737
97. Rice MT, Jacobson RD, Golledge RG, Jones D (2005) Cartographic data and design
considerations for haptic and auditory map interfaces. Cartography Geogr Inf Sci 32(4):381–
391
98. Schmitz
B,
Ertl
T
(2010)
Making
digital
maps
accessible
using
vibrations.
In
Miesenberger K, Klaus J, Zagler W, Karshmer A (eds) ICCHP 2010, Part I. LNCS, vol
6179. Springer, Heidelberg, pp 100–107
99. Schmitz B, Ertl T (2012) Interactively displaying maps on a tactile graphics display. In:
SKALID 2012–spatial knowledge acquisition with limited information displays (2012),
pp 13–18
100. Schneider J, Strothotte T (1999) Virtual tactile maps. In: Bullinger H.-J, Ziegler J
(eds) Proceedings of HCI international. L. Erlbaum Associates Inc., Munich, Germany,
pp 531–535
101. Schneider J, Strothotte T (2000) Constructive exploration of spatial information by blind
users. In: Proceedings of the fourth international ACM conference on assistive technologies
—assets ’00. ACM Press, New York, pp 188–192. doi:10.1145/354324.354375
102. Seisenbacher G, Mayer P, Panek P, Zagler WL (2005) 3D-ﬁnger—system for auditory
support of haptic exploration in the education of blind and visually impaired students—idea
and feasibility study. 8th European conference for the advancement of assistive technology
in europe—AAATE. IOS Press, Lille, France, pp 73–77
103. Senette C, Buzzi MCM, Buzzi MCM, Leporini B, Martusciello L (2013) Enriching graphic
maps to enable multimodal interaction by blind people. In: Stephanidis C, Antona M
(eds) Proceedings of UAHCI 2013, vol 8009. Springer, Berlin, pp 576–583. doi:10.1007/
978-3-642-39188-0
104. Shaer O, Hornecker E (2009) Tangible user interfaces: past, present, and future directions.
Foundations Trends® Hum Comput Interact 3(1–2):1–137. doi:10.1561/1100000026
105. Shimada S, Murase H, Yamamoto S, Uchida Y, Shimojo M, Shimizu Y (2010)
Development of directly manipulable tactile graphic system with audio support function.
In: Miesenberger K, Klaus J, Zagler W, Karshmer A (eds) ICCHP 2010, Part II. LNCS, vol
6180. Springer, Vienna, pp 451–458
106. Simonnet M, Bothorel C, Maximiano LF, Thepaut A (2012) GeoTablet, une application
cartographique pour les personnes déﬁcientes visuelles. In: Handicap 2012, vol 7, pp 8–13
107. Simonnet M, Jacobson D, Vieilledent S, Tisseau J (2009) SeaTouch: a haptic and auditory
maritime environment for non visual cognitive mapping of blind sailors. In: Hornsby KS
(ed) COSIT 2009, LNCS 5756. Springer-Verlag, Aber Wrac’h, France, pp 212–226. doi:10.
1007/978-3-642-03832-7_13
582
J. Ducasse et al.

108. Simonnet M, Vieilledent S, Jacobson RD, Tisseau J (2011) Comparing tactile maps and
haptic digital representations of a maritime environment. J Vis Impairment Blindness 105
(April):222–234
109. Su J, Rosenzweig A, Goel A, de Lara E, Truong KN (2010) Timbremap: enabling the
visually-impaired to use maps on touch-enabled devices. Proceedings of the 12th
international conference on human computer interaction with mobile devices and services
—MobileHCI ’10. ACM Press, New York, pp 17–26
110. Sullivan LO, Picinali L, Cawthorne D (2014) A prototype interactive tactile display with
auditory feedback. In: Irish HCI Conference 2014 (p. 4). Dublin City University, Dublin,
Ireland
111. Swaminathan S, Roumen T, Kovacs R, Stangl D, Mueller S, Baudisch P (2016) Linespace: a
sensemaking platform for the blind. In: Proceedings of the 2016 CHI conference on human
factors in computing systems—CHI ’16. ACM Press, New York, pp 2175–2185. doi:10.
1145/2858036.2858245
112. Taher F, Hardy J, Karnik A, Weichel C, Jansen Y, Hornbæk K, Alexander J (2015)
Exploring interactions with physically dynamic bar charts. In: Proceedings of the 33rd
annual ACM conference on human factors in computing systems—CHI ’15, pp 3237–3246.
doi:10.1145/2702123.2702604
113. Taibbi M, Bernareggi C, Gerino A, Ahmetovic D, Mascetti S (2014) AudioFunctions:
eyes-free exploration of mathematical functions on tablets. In Miesenberger K, Fels D,
Archambault D, Peňáz P, Zagler W (eds) Computers helping people with special needs: 14th
international conference, ICCHP 2014, Paris, France, July 9–11, 2014, Proceedings, Part I.
Springer International Publishing, Cham, pp 537–544. doi:10.1007/978-3-319-08596-8_84
114. Tatham AF (1991) The design of tactile maps: theoretical and practical considerations. In:
Rybaczak M, Blakemore K (eds) Proceedings of international cartographic association:
mapping the nations. ICA, London, UK, pp 157–166
115. The Braille Authority of North America (2010) Guidelines and standards for tactile graphics.
Retrieved from http://brailleauthority.org/tg/web-manual/
116. Thinus-Blanc C, Gaunet F (1997) Representation of space in blind persons: vision as a
spatial sense? Psychol Bull 121(1):20–42. doi:10.1037/0033-2909.121.1.20
117. Tixier M, Lenay C, Le Bihan G, Gapenne O, Aubert D (2013) Designing interactive content
with blind users for a perceptual supplementation system. In: Proceedings of the 7th
international conference on tangible, embedded and embodied interaction—TEI ’13. ACM
Press, New York, p. 229. doi:10.1145/2460625.2460663
118. Tornil B, Baptiste-Jessel N (2004) Use of force feedback pointing devices for blind users. In:
Stary C, Stephanidis C (eds) 8th ERCIM workshop on user interfaces for all, user-centered
interaction paradigms for universal access in the information society, LNCS Vol 3196, vol
3196. Springer, Vienna, pp 479–485. doi:10.1007/b95185
119. Ullmer B, Ishii H (1997) The metaDESK : models and prototypes for tangible user
interfaces. In: Proceedings of the 10th annual ACM symposium on user interface software
and technology—UIST ’97. ACM Press, New York, pp 223–232. doi:10.1145/263407.
263551
120. Ullmer B, Ishii H (2000) Emerging frameworks for tangible user interfaces. IBM Syst J 39
(3.4):915–931. doi:10.1147/sj.393.0915
121. Ungar S (2000) Cognitive mapping without visual experience. In: Kitchin R, Freundschuh S
(eds) Cognitive mapping: past present and future. Routledge, Oxon, UK, pp 221–248
122. Ungar S, Blades M, Spencer C, Morsley K (1994) Can the visually impaired children use
tactile maps to estimate directions? J Vis Impairment Blindness 88(3):221–233
123. Vidal-Verdú F, Hafez M (2007) Graphical tactile displays for visually-impaired people.
Neural Syst Rehabil Eng IEEE Trans 15(1):119–130
124. Wall SA, Brewster SA (2006) Tac-tiles : multimodal pie charts for visually impaired users,
(October), 14–18
125. Wang Z, Li B, Hedgpeth T, Haven T (2009) Instant tactile-audio map: enabling access to
digital maps for people with visual impairment. In: Assets ’09 proceedings of the 11th
Accessible Interactive Maps for Visually Impaired Users
583

international ACM SIGACCESS conference on computers and accessibility. ACM Press,
New York, pp 43–50. doi:10.1145/1639642.1639652
126. Wang Z, Li N, Li B (2012) Fast and independent access to map directions for people who are
blind. Interact Comput 24(2):91–106. doi:10.1016/j.intcom.2012.02.002
127. Weir R, Sizemore B, Henderson H, Chakraborty S, Lazar J (2012) Development and
evaluation of soniﬁed weather maps for blind users. In: Keates S, Clarkson PJ, Langdon P,
Robinson P (eds) Proceedings of CWUAAT. Springer, Cambridge, pp 75–84
128. Wijntjes MWA, van Lienen T, Verstijnen IM, Kappers AML (2008) Look what I have felt:
unidentiﬁed haptic line drawings are identiﬁed after sketching. Acta Physiol (Oxf) 128
(2):255–263
129. Wijntjes MWA, van Lienen T, Verstijnen IM, Kappers AML (2008) The inﬂuence of picture
size on recognition and exploratory behaviour in raised-line drawings. Perception 37
(4):602–614. doi:10.1068/p5714
130. Wilhelm E, Schwarz T, Jaworek G, Voigt A, Rapp BE (2014) Towards displaying graphics
on a cheap, large-scale braille display. In: Miesenberger K, Fels D, Archambault D, Pe\vnáz
P, Zagler W (eds) Computers helping people with special needs: 14th international
conference, ICCHP 2014, Paris, France, July 9–11, 2014, Proceedings, Part I. Springer
International Publishing, Cham, pp 662–669. doi:10.1007/978-3-319-08596-8_102
131. Yairi IE, Azuma Y, Takano M (2009) The one octave scale interface for graphical
representation for visually impaired people. In: ASSETS ’09. ACM, Pittsburgh, pp 255–256.
doi:10.1145/1639642.1639702
132. Yairi IE, Takano M, Shino M, Kamata M (2008) Expression of paths and buildings for
universal designed interactive map with due consideration for visually impaired people. In:
2008 IEEE international conference on systems, man and cybernetics, pp 524–529. IEEE.
doi:10.1109/ICSMC.2008.4811330
133. Yatani K, Banovic N, Truong K (2012) SpaceSense: representing geographical information
to visually impaired people using spatial tactile feedback. In: Proceedings of the 2012 ACM
annual conference on human factors in computing systems—CHI ’12. ACM Press, New
York, pp 415–424. doi:10.1145/2207676.2207734
134. Yoshida T, Kitani KM, Koike H, Belongie S, Schlei K (2011) EdgeSonic: image feature
soniﬁcation for the visually impaired. In: Proceedings of the 2nd augmented human
international conference on—AH ’11, pp 1–4. doi:10.1145/1959826.1959837
135. Yu W, Brewster S (2003) Evaluation of multimodal graphs for blind people. Univ Access Inf
Soc 2(2):105–124. doi:10.1007/s10209-002-0042-6
136. Zeng L, Miao M, Weber G (2014) Interactive audio-haptic map explorer on a tactile display.
Interact Comput, iwu006–. doi:10.1093/iwc/iwu006
137. Zeng L, Weber G (2010) Audio-haptic browser for a geographical information system. In:
Miesenberger K, Klaus J, Zagler W, Karshmer A (eds) ICCHP 2010. LNCS, vol. 6180, vol
6180/2010. Springer, Heidelberg, pp 466–473. doi:10.1007/978-3-642-14100-3_70
138. Zeng L, Weber G (2011) Accessible maps for the visually impaired. In: Proceedings of
IFIP INTERACT 2011 workshop on ADDW. Lisbon, Portugal, pp 54–60
139. Zeng L, Weber G (2012a) ATMap: annotated tactile maps for the visually impaired. In:
Esposito A, Esposito AM, Vinciarelli A, Hoffmann R, Müller VC (eds) COST 2102
international training school, cognitive behavioural systems, LNCS Volume 7403, 2012.
Springer, Berlin, pp 290–298. doi:10.1007/978-3-642-34584-5
140. Zeng L, Weber G (2012b) Building augmented you-are-here maps through collaborative
annotations for the visually impaired. In: SKALID 2012–spatial knowledge acquisition with
limited information displays. Kloster Seeon, Germany, pp 7–12
141. Zhao H, Plaisant C, Shneiderman B, Lazar J (2008) Data soniﬁcation for users with visual
impairment. ACM Trans Comput-Hum Interaction 15(1):1–28. doi:10.1145/1352782.
1352786
584
J. Ducasse et al.

Smart Multisensor Strategies for Indoor
Localization
Bruno Andò, Salvatore Baglio, Cristian O. Lombardo
and Vincenzo Marletta
1
Introduction
On the tail of the continuous growing of enabling Information and Communication
Technologies
(ICT),
including
sensors,
electronics
and
signal
processing,
researchers are strongly involved in the development of assistive systems to
improve the life quality of frail people, thus facing the “Society for All” challenge
[1–7]. This is really becoming a societal challenge which involves efforts from the
industries, research centers and the European Commission.
The World Health Organization (WHO) is focusing on the problem of disability.
Blindness is one of the most invalidating form of disability which involves hun-
dreds of millions of people all over the world. Although blind people can express a
high level of autonomy in performing both indoor and outdoor activities, very often
unfamiliar environments can cause discouragement, loss of self-conﬁdence as well
as can compromise their social inclusion and job opportunities. Structural, psy-
chological and cultural barriers make things even more complicate.
Thinking to the Society for All, including people with impairments, means
providing efforts for bringing technologies to all, making services available and
exploitable to all, making environments comfortable for all. ICT technologies can
dramatically help such development, with the aim to adapt the society to frail
people needs, rather than asking them to ﬁt the society requirements.
Exploiting technologies already available and distributed in the environment, as
well as enriching everyday environments with new sensing capability, could be the
right solution to promote such form of technological driven inclusion.
B. Andò (&)  S. Baglio  C.O. Lombardo  V. Marletta
D.I.E.E.I, University of Catania, Catania, Italy
e-mail: bruno.ando@unict.it
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_18
585

Assistive solutions must be “User Centered Designed (UCD)” to improve and to
encourage the user acceptability and conﬁdence in the use of such devices.
The UCD concept involves many different aspects, such as reliability, the form,
quantity and meaning of the information these devices are able to provide the user
with, needs of demanding training, cosmetics, costs. These aspects are well covered
in the literature and will not be addressed through this work.
Among primary needs of blind people, indoor and outdoor navigation remains a
critical task, both from a technological and a social point of view. Independent
travel is an important goal sought by most visually impaired and blind people.
It is interesting to observe that studies have demonstrated that the most important
traveling aids for the visually impaired person are still the white cane and the guide dog.
Theyaremultifunctional, reliableand also tellothers thatthepersonis visually impaired.
Such thought can lead to the consideration that the main target of electronic aids
should be not replacing primary traveling aids, but to develop complementary
systems (to be used along with the cane or the dog) which are reliable, efﬁcient,
easy to use and to wear and providing an optimized redundancy of information.
In the following Section a review of assistive technologies aimed to improve
efﬁciency in performing mobility tasks in indoor environment is presented.
Section 3 is dedicated to discuss, as a case of study, an active and assistive system
aimed to provide blind users with useful information for a safe and an efﬁcient
exploitation of indoor environments.
2
Related Works
Extensive reviews of assistive solutions for people with sensory disability, such as
blind, are available in the literature, which highlight the strategic role of enabling
technologies in improving their functionalities [1–7]. In the framework of systems
assisting users during their mobility, two main classes of devices can be identiﬁed:
Obstacle Detectors and Location Bases Services (LBS).
Several technologies have been exploited to develop obstacle detectors,
including ultrasonic, infrared, laser and cameras. Some of these devices use audi-
tory and/or tactile technology to provide the user with a useful piece of information
[8, 9], as well as electromagnetic and optical technologies [10, 11]. Approaches
based on vision systems, which convey the achieved information to the user by
sound or tactile interfaces, are presented in [12, 13].
Navigation systems based on Location Based Services for indoor environments
should be able to build awareness of the user/environment interaction, in order to
provide the user itself with strategic information for a safe and efﬁcient exploitation
of that area. To such aim very accurate localization systems are required.
Localization systems using digital tags, active badges, infrared or ultrasound
devices and photodiodes to transmit some form of remote signal once the user gets
into the range of the device, suffer from the discontinuous information provided to
586
B. Andò et al.

the user as well as installation costs and complexity. On the other side, solutions
based on inertial sensors suffer from cumulative position error and drift [4–6].
More efﬁcient strategies adopt real time and continuous localization systems,
based on Vision, InfraRed (IR), UltraSounds (US), WLAN, Wi-Fi and Bluetooth
enabling technologies. In the following main approaches are brieﬂy presented, with
reference to outstanding reviews in the ﬁeld [14, 15].
In [14] the state of the art in methodologies for indoor localization is presented,
with particular regards to Angle of Arrival (AoA), Time of Arrival (ToA) and
Fingerprint techniques, highlighting advantages and drawbacks of each approach.
A benchmark between different technologies and localization systems is pro-
vided in [15], mainly in terms of accuracy, costs and complexity.
Vision based systems are based on the matching between previously recorded
images with the captured one or on real time target tracking [16, 17]. Interesting
solutions exploiting IR technology can be found in [18–21]. US based indoor
localization systems, exploiting trilateration paradigms, are presented in [4, 15].
Performances of WLAN based localization systems relying on the use of RSS,
Triangulation and Fingerprint are addressed in [22, 23], respectively.
Above statements allow to afﬁrm that indoor localization systems can be clas-
siﬁed on the basis of different key factors, at different levels: the measurement
strategy (TOA, AOA, RSS), the adopted enabling technology (vision, US, IR,
Wi-Fi), the localization algorithm (trilateration, triangulation) and applications.
Being above mentioned approaches just some examples among solutions available
in the wide panorama of indoor localization strategies.
Drawbacks of localization systems can arise from high/medium costs of the
adopted equipment (vision and IR), multipath issue (IR and sometimes US),
Non-Line of Sight (NLOS) (vision, IR, US), installation requirements (IR, US), low
accuracy (Wi-Fi), power consumption (vision, Wi-Fi).
Table 1 provides some examples of Enabling Technologies for indoor local-
ization, along with some speciﬁcations and main drawbacks.
Table 1 Benchmark between Enabling Technologies for indoor localization
Technology
Advantages
Drawbacks
Accuracy
(m)
Operating
range (m)
Vision and image
processing
High accuracy
High cost
NLOS
10−3–10−1
1–10
US and trilateration
High accuracy
Low cost
Installation
requirements
NLOS
10−2
1–5
IR and triangulation
High accuracy
Low cost
Installation
requirements
NLOS
10−2
1–10
WLAN (RSS,
ﬁngerprint)
Low cost
No installation
requirements
LOS not required
Low accuracy
1–20
10–50
Bluetooth
Low cost
LOS not required
Low accuracy
High latency
2–3
1–30
Smart Multisensor Strategies for Indoor Localization
587

As evidenced by the above analysis of the state of the art, the choice of the
suitable localization methodology is strictly dependent on requirements coming
from the speciﬁc application.
Many application contexts may require advanced indoor localization strategies,
from logistics, environmental monitoring, structural monitoring, healthcare as well
as Active and Assisted Living (AAL) for frail people.
In particular, in case the active assistance of frail people is pursued, the need for
very accurate indoor positioning solutions emerges, fulﬁlling also user acceptability
requirement.
In the following, a case study of a solution developed to assist blind people
during their mobility in indoor environments is presented. The system uses a
multi-trilateration ultrasound based systems and smart signal processing to improve
the accuracy of the localization task [4].
3
A Case of Study: An Active System to Assist Blind
People in Indoor Environment
As mentioned the implementation of a reliable and efﬁcient solution to assist frail
people in indoor environments requires an accurate localization system. Among the
enabling technologies, ultrasound based localization systems assume a strategic role
for the very high accuracy which can be reached, at the expense of a sensor network
which must be installed in the environment.
In the following, the implementation of a ultrasound based localization archi-
tecture is addressed, with a speciﬁc focus on the case of study of an active system
adopted to provide blind users with a continuous and reliable form of information
for a safe exploitation of indoor environments [4].
The system exploits emerging technologies such as Wireless Sensor Network
and advanced signal processing, to get awareness of the user position inside the
environment, the user and environment status, as well as the User Environment
Interaction (UEI) and the User-Environment-Contextualization (UEC).
Adopted technologies and sensing methodologies allow for providing the user
with a real time and continuous assistance with a high spatial resolution.
This feature, which represents a dramatic advance with respect to the state of the
art, is mainly due to the performances of UEI/UEC tool and the indoor localization
strategy, which shows an accuracy better than 4 cm.
In the following, the system architecture is presented along with smart para-
digms adopted for the sake of user localization.
588
B. Andò et al.

3.1
The Assistive System Architecture
The architecture of the active system developed to assist blind people while per-
forming daily activities in indoor environments is shown in Fig. 1. The system
exploits multi-sensor nodes (environment nodes and the user node) in a Wireless
Network conﬁguration. Environment nodes are equipped with sensors (for Liquid
Propane Gas, Smoke and Temperature detection) and ultrasound sensors to
implement the user localization task. Users are required to wear a “user node”,
which embeds inertial sensors for his/her status monitoring and the ultrasound
sensor. The latter is coupled to the ultrasound receiver embedded in the environ-
ment nodes to implement the user localization functionality by a smart trilateration
approach.
Fig. 1 Schematization of the indoor localization and assistive system (© [2014] IEEE)
Smart Multisensor Strategies for Indoor Localization
589

Data coming from the sensor network are collected by a PC running dedicated
signal processing algorithms, with speciﬁc regards to the UEI and UEC function-
alities. It must be observed that the PC can be replaced by extending the compu-
tational functionality in the sensor network nodes.
The UEI is in charge of detecting events (such as collisions with obstacles or
availability of services) by exploiting information on the user position and the
presence of obstacle or services within the environment. As a consequence of such
ﬁndings candidate messages for the user will be generated by the UEI tool.
The UEC tool combines the awareness of the user inertial status (e.g. posture) as
respect to the environment status (ﬁres, gas leakage, smoke), to provide useful
information to the system supervisor and to generate candidate messages for the
user. Messages generated by these tools have a well deﬁned degree of priority
which will be managed by the Decision Support System (DSS) service, in order to
properly deliver messages to the user with an optimized degree of information. The
message center is in charge of delivering notiﬁcations by the user interface,
implemented through a Bluetooth audio feedback device. The supervisor can get
awareness of the user interaction and contextualization within the environment by
exploiting the information provided by the DSS, in order to manage alert situations.
3.2
The Smart Trilateration Algorithm
The ultrasound localization system is based on the continuous measurement of
distances between the user and the environment nodes, di, which are successively
processed by a smart trilateration algorithm. This approach allows for a real time
and very accurate estimation of the user’s position within the environment. In order
the perform the estimation of di distance, the ToA approach, shown in Fig. 2, is
used. The aim of the Anti-Bouncing Filter (ABF) is the reduction of bouncing
effects in the estimation of ToAs, by measuring and processing 4 successive
ultrasound impulses. The resulting ToAs are then converted in distances and
conveyed to the Multi-Trilateration Algorithm (MTA) for the sake of user local-
ization. The algorithm estimates the user coordinates taking into account and
manipulating all possible combinations of di distances, as schematized in Fig. 3 [4,
24]. In order to produce all candidate estimations of the user position, each com-
bination of two user/node distances is considered. Such estimations are then sta-
tistically analyzed to extract the optimal user position within the environment.
Although the MTA is higher time-demanding as respect to the traditional
Trilateration Algorithm, the tradeoff between localization performances and pro-
cessing time, deﬁnitively encourages its use in case extremely accurate indoor
localization tasks are required [24].
Figure 4 shows an example of a path reconstruction performed by using the
ultrasound/MTA localization strategy above described. As already mentioned, the
localization accuracy is in the order of 4 cm. The system performances have been
590
B. Andò et al.

estimated by computing the index, J, weighting the residual between the estimated
path (WDEstim) travelled by the user and the nominal one (WDNom),over a total
length of 110 m.
J% ¼ WDEstim  WDNom
j
j
WDNom
 100
ð1Þ
3.3
Compensating for Uncertainty in WSN Node Position
Sometimes, coordinates of the WSN nodes cannot be properly known due to the
absence of absolute reference, irregular ﬂoor or roof height. This lack of infor-
mation can increase the overall uncertainty in the estimation of the user’s position.
In the following, a method to compensate for node mispositioning is described, in
few notes [4].
The proposed approach exploits a Nelder–Mead nonlinear simplex paradigm for
the optimal estimation of the node coordinates. In particular, the task of the para-
digm is to ﬁnd the sensor nodes coordinates which minimize the distances, hi,
between N nominal user positions and their estimations performed by the trilater-
ation algorithm. The minimization procedure uses a rough estimation of the node
position as initial conditions and the following performance index:
Fig. 2
Schematization of Time of Flight estimation methodology (© [2014] IEEE)
Smart Multisensor Strategies for Indoor Localization
591

JED ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
hi
ð Þ2
s
N
ð2Þ
During the minimization process, constraints have been imposed to limit the
nodes’ positions within a tolerance of ±5 cm as respect to rough measured values.
In order to assess performances of the proposed compensation method, tests
have been performed in an environment of 8.60 m by 7.10 m, with seven nodes
roughly positioned at a height of about 3.0 m.
For the experiment under consideration, 45 reference positions have been
deﬁned inside the environment under test, in order generate the data set to be
adopted by the minimization paradigm [4]. The distance between grid points is
(X1,Y1)
(X2,Y2)
(X3,Y3)
(X4,Y4)
(X5,Y5)
(X6,Y6)
(X7,Y7)
(X8,Y8)
(X9,Y9)
(X10,Y10)
(X12,Y12)
(X11,Y11)
d1
d2
d3
d4
d5
d6
d7
d8
d9
d10
d11
d12
MTA
Filter
d1
d2
dN
(x1,y1)
(x2,y2)
(xM,yM)
(x,y)
…
…
ENHANCED TRILATERATION ALGORITHM
(
) (
) (
)
[
]
2
i
2
j
2
i
2
j
2
j
2
i
i
j
i
j
j,i
j,i
Y
Y
X
X
d
d
Y
Y
X
X
2
y
x
−
+
−
+
−
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
−
−
=
⎥
⎦
⎤
⎢
⎣
⎡
xi,j, yi,j
Fig. 3
Schematization of environment to implement the trilateration paradigm. Coordinates of
the user in the plane, (xi,j, yi,j) are estimated by each pair of environment sensor nodes (Xi, Yi) and
(Xj, Yj) (© [2011] IEEE)
592
B. Andò et al.

0.50 m. Values of JED, before and after applying the minimization algorithm, are
6.1 and 3.9 cm, respectively, thus conﬁrming the suitability of the proposed
strategy.
4
Conclusions
A deep analysis of the state of the art conﬁrms the strong interest in developing
efﬁcient solutions in the framework of Active and Ambient Assisted Living.
This chapter aims to provide some points of thought on active and assistive
systems, with speciﬁc regards to blind people. In particular, the need for a very
accurate indoor localization system is discussed against the application context to
be addressed.
A benchmark between different localization strategies is presented in terms of
sensing methodologies, enabling technologies and applications.
In order to properly contextualize the subject in the ﬁeld of assistive systems for
the visually impaired, a case of study of an active system developed to assist users
performing daily activities in indoor environments is presented. The system is a
valuable example of the synergic cooperation between hardware architectures and
smart algorithms. The latter are mandatory to get awareness of the user status in the
environment, as well as the User Environment Interaction and Contextualization.
To such aim an accurate indoor localization system is developed, which exploits a
ultrasound sensing strategy and MTA. The system is able to provide the user with a
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
9
X [m]
Y [m]
Measured Path
Nominal Path
WSN Nodes
%
46
.2
100
%
=
⋅
−
=
Nom
Nom
Estim
WD
WD
WD
J
1
2
3
4
5
7
6
Fig. 4 Results of the
tracking test by the MTA
algorithm for a close walking
path of 110 m (© [2014]
IEEE)
Smart Multisensor Strategies for Indoor Localization
593

real time and continuous form of assistance mainly due to the real time and high
spatial resolution features of the localization tool.
Finally, a method for compensating the mispositioning of environment nodes
has been also discussed.
References
1. Hersh M, Johnson MA (eds) (2008) Assistive technology for visually impaired and blind
people. Springer, London, GB
2. Bujacz M, Baranski P, Moranski M, Strumillo P, Materka A (2008) Remote mobility and
navigation aid for the visually disabled. In: Sharkey PM, Lopes-dos-Santos P, Weiss PL,
Brooks AL (eds) Proceedings of 7th international conference on disability, virtual reality and
associate technologies with art abilitation, Maia, Portugal, 8–11 Sept 2008, pp 263–270
3. Andò B, Ascia A (2007) Navigation aids for the visually impaired: from artiﬁcial codiﬁcation
to natural sensing. IEEE Mag Instrum Meas 10(3):44–51
4. Andò B, Baglio S, Lombardo CO (2014) RESIMA: an assitive paradigm to support weak
people in indoor environment. IEEE Trans Instrum Meas 63(11):2522–2528
5. Andò B, Baglio S, La Malfa S, Marletta V (2011) Innovative smart sensing solutions for the
visually impaired. In: Handbook of research on personal autonomy technologies and disability
informatics, Medical inforMation science reference, IGI Global, Hershey, pp 60–74
6. Andò B (2006) Sensors that provide security for people with depressed receptors. IEEE Mag
Instrum Meas 9(2):58–63
7. Velázquez R (2010) Wearable assistive devices for the blind. In: Lay-Ekuakille A,
Mukhopadhyay SC (eds) Wearable and autonomous biomedical devices and systems for
smart environment: issues and characterization, LNEE 75, Springer, Berlin, Chap. 17,
pp 331–349
8. Andò B, Graziani S (2009) Multisensor strategies to assist blind people: a clear-path indicator.
IEEE Trans Instrum Meas 58(8):2488–2494
9. Andò B (2008) A smart multisensor approach to assist blind people in speciﬁc urban
navigation tasks. IEEE Trans Neural Syst Rehab Eng 16(6):592–594
10. Villanueva J, Farcy R (2012) Optical device indicating a safe free path to blind people. IEEE
Trans Instrum Meas 61(1):170–177
11. Scalise L, Primiani VM, Russo P, Shahu D, Di Mattia V, De Leo A, Cerri G (2012)
Experimental investigation of electromagnetic obstacle detection for visually impaired users:
a comparison with ultrasonic sensing. IEEE Trans Instrum Meas 61(11):3047–3057
12. Meijer P (1992) An experimental system for auditory image representations. IEEE Trans
Biomed Eng 39(2):112–121
13. Velazquez R, Fontaine E, Pissaloux E (2006) Coding the environment in tactile maps for
real-time guidance of the visually impaired. In: Proceedings of IEEE international symposium
on micro-nanomechatronics and human science, Nagoya, Japan, 5–8 Nov 2006, pp 1–6
14. Nicholaus R. Mrindoko, Lusajo M. Minga A (2016) Comparison review of indoor positioning
techniques. Int J Comput (IJC) 21(1):42–49
15. Mainetti L, Patrono L, Sergi I (2014) A survey on indoor positioning systems. In: Software,
telecommunications and computer networks (SoftCOM), 2014 22nd International Conference
on, Split, pp 111–120
16. Liu JJ, Philips C, Daniilidis K (2010) Video-based localization without 3D mapping for the
visually impaired. In: IEEE computer society conference on computer vision and pattern
recognition workshops (CVPRW), San Francisco, CA, 13–18 June 2010, pp 23–30
594
B. Andò et al.

17. Zhongna Z, Xi C, Yu-Chia C, Zhihai H, Han TX, Keller JM (2009) Video-based activity
monitoring for indoor environments. In: International symposium on circuits and systems,
Taipei, 24–27 May 2009, pp 1449–1452
18. Pierlot V, Van Droogenbroeck M (2014) BeAMS: a beacon-based angle measurement sensor
for mobile robot positioning. IEEE Trans Robot 30(3)
19. Zhang D, Xia F, Yang Z, Yao L, Zhao L (2010) Localization technologies for indoor human
tracking. IEEE Commun Surv Tutorials 11:1–6
20. Want Roy, Hopper Andy, Falcao Veronica, Gibbons Jonathan (1992) The active badge
location system. ACM Trans Inform Syst (TOIS) 10(1):92–102
21. Fireﬂy Motion Tracking System User’s guide (1999) http://www.gesturecentral.com/ﬁreﬂy/
FireﬂyUserGuide.pdf
22. Bahl P, Padmanabhan V (2000) RADAR: An in-building RF based user location and tracking
system. IEEE INFOCOM 2:775–784
23. King T, Kopf S, Haenselmann T, Lubberger C, Effelsberg W (2006) COMPASS: a
probabilistic indoor positioning system based on 802.11 and digital compasses. ACM
WiNTECH, Los Angeles, USA, pp 34–40
24. Andò B, Baglio S, La Malfa S, Marletta V (2011) A sensing architecture for mutual
user-environment awareness case of study: a mobility aid for the visually impaired. IEEE
Sens J 11(3):634–640
Smart Multisensor Strategies for Indoor Localization
595

Constructing Tactile Languages
for Situational Awareness Assistance
of Visually Impaired People
Ramiro Velázquez and Edwige Pissaloux
1
Introduction
Auditory and tactile channels are often used to convey information to people with
visual impairments. Considering that hearing becomes the primary sense for this
population and that it is generally loaded with plenty of stimuli from the envi-
ronment, the use of touch has been long studied to transmit information that unloads
audition and that could assist in daily tasks such as reading, computer access, and
mobility [1–3].
Tactile information displayed by assistive devices normally consists of very
simple tactile cues to alert of events (for example, obstacles in the immediate
location or to point users to a navigational direction). However, situational
awareness assistance cannot be provided as simple event alerts. In fact, providing
feedback on environmental elements with respect to time or space and the update of
their status after some variable has changed deﬁnitively needs a more complex
communication structure such as language. It still remains a challenge for assistive
devices to provide situational awareness information that could be easily and fast
understood through touch.
Language learning is the process by which humans acquire the capacity to per-
ceive and use words to understand and communicate [4]. For every ﬁeld of knowl-
edge addressing human language and communication, the language learning process
R. Velázquez (&)
Universidad Panamericana, Aguascalientes, Mexico
e-mail: rvelazquez@up.edu.mx
E. Pissaloux
Université de Rouen Normandie, Rouen, France
e-mail: edwige.pissaloux@univrouen.fr
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_19
597

is very different whether it is the ﬁrst or second language. While the ﬁrst refers to an
infant’s acquisition of the native language, the second deals with the process of
learning an additional language when a native one has been already learned.
Learning a second language is a complex process extensively studied in neu-
roscience [5], applied linguistics [6], sociolinguistics [7], psychology [8], and
education [9]. With no intention of further reviewing this process, we shall limit the
discussion to state that humans learn a second language by making relations with
their own native language, by memorizing, and by practicing. Think of a
Spanish-speaking native learning Italian; as both are Latin-based languages, rela-
tions can be easily established. However, for a Spanish-speaking native learning
Chinese, memorizing new vocabulary and grammar rules as well as constant
practice seem the only way.
Language learning does not refer exclusively to spoken languages. Natural
languages such as semiotics, sign language, gestural/body language, and tactile
patterns are equally used to communicate ideas without conveying any sound.
Regardless of their complexity, natural languages also require certain amount of
time, effort, and practice in order to master their usage.
In particular, tactile patterns have received growing attention in several domains
of human–computer interaction such as virtual reality, sensory augmentation,
sensory substitution, robotics, mobile and wearable devices, game and entertain-
ment, among many others. Short structured tactile patterns called tactile icons or
tactons [10] have been used to code verbal language and convey information to
touch especially in applications where sight and hearing are restricted or
overloaded.
Tactons have been already studied to encode simple information such as ﬂight
data for pilots [11, 12], warning signals for car drivers [13] and clinicians [14],
instructions for improving physical performance [15, 16], navigational assistance
for visually impaired and blind people [17, 18], emotions [19, 20], and verbal words
[19, 21]. In these studies, different sets of tactons were proposed to test subjects and
satisfactory recognition rates were reported. However, only one tacton was rec-
ognized at a time.
In this study, we present several tactons to a group of 20 voluntary subjects and
we combine them to construct sentences that represent gradually more complex
information. We seek to evaluate human performance in tactile language learning
and tactile memory and to determine how far we can cognitively handle tactons for
more ambitious applications in human–computer interaction, wearable/mobile
computing, and assistive devices. For this evaluation, we propose a novel approach:
podotactile stimulation. Tactile-foot stimulation has shown interesting results and
great potential in our previous studies [17, 19, 22].
The rest of the paper is organized as follows: Sect. 2 presents a brief review of
relevant prior related work. Section 3 presents a technical overview of the apparatus
used in this study. Section 4 evaluates human performance in tactile learning, tactile
memory, and tactile language usage with the proposed device while Sect. 5 shows
an example of situational awareness assistance with tactile language. Finally,
Sect. 6 concludes with main remarks and future work perspectives.
598
R. Velázquez and E. Pissaloux

2
Related Work
Let us start by deﬁning the three main concepts addressed in this paper: tactile
learning, tactile language, and tactile memory.
Tactile learning is the process of acquiring new information through tactile
exploration. It is a process that does not happen all at once but is built upon
practice.
Tactile language is a set of tactile information that can be used to construct a
communication system. As oral languages, tactile ones contain a set of rules that
govern how tactile information is used to form sequences meaning phrases.
Tactile memory refers to the persistence of learning in a state that can be
revealed at a later occasion. It can be either long-term- or short-term memory.
While tactile information stored in the long-term memory affects our world per-
ception and inﬂuences our interaction with the environment, tactile information in
the short-term memory is held in mind in small amount for a short period in an
active readily available state [23].
Several studies evaluating these three concepts using more than a simple set of
few tactons can be found on the literature.
F. Geldard conducted in 1957 one of the earliest attempts to evaluate tactile
memory. He proposed to encode symbols of the alphabet with vibratory patterns.
Guided by this reasoning, Geldard design the Vibratese language, which was
composed of 45 basic elements; the tactile equivalent of numbers and letters [24].
About 12 h of practice were required to learn the language. Subjects were able to
recognize single letters but could not interpret continuous sequences of patterns
correctly.
During the 70s, experimental research on understanding tactile sequences pro-
vided the ﬁrst interesting insights into the capabilities of tactile memory: Gilson and
Baddeley [25] and Sullivan and Turvey [26] concluded that tactile memory works
best for stimuli lasting 5–10 s and that people quickly forget tactile stimuli. Watkins
and Watkins [27] and later, Mahrer and Miles [28] evidenced the importance of
training for memorizing tactile sequences. Handel and Buffardi [29] experimentally
observed that it is not only possible to learn and understand tactile sequences but to
encode also statistical regularities to predict patterns within the sequences.
Conway and Christiansen [30] conducted experiments to compare the ability of
learning sequences with hearing and touch. Ten different sequences combining
from three to ﬁve elements each were displayed to both senses. Results showed that
the auditory modality displays a signiﬁcant learning advantage compared to touch.
Evreinova et al. proposed in [31], a memory game destined to strengthen the
short-term tactile memory of hearing impaired adults. Ten participants explored 27
different vibrotactile patterns using the Logitech tactile feedback mouse. Results
reported that after a signiﬁcant learning time, subjects could reasonably manipulate
the set of tactons. No concatenation of tactons was reported.
Wang et al. presented in [32], a computer implementation of the classic memory
card game using the STreSS tactile display. Twelve tactile memory cards had to be
Constructing Tactile Languages for Situational Awareness …
599

matched with their visual counterparts. After a short training period, the cards could
be distinguished from one another using tactile stimuli alone. Vision shortened the
learning process.
Kuber et al. described in [33], a multimodal memory game for the blind.
Combining speech, nonspeech audio, and tactons, both sighted and blind users
achieved to replicate complex sequences of information. Concatenation of nonvi-
sual information was reported with good results; however, multimodality eased the
task. Similarly, Raisamo et al. proposed a tactile memory game using visual, audio,
and tactile feedback [34]. The game got a positive response from a group of seven
visually impaired children.
Oliveira and Maciel presented in [35] the design and assessment of tactile
vocabularies to support navigation in 3D environments. Two approaches were
explored: preﬁxation and tactile sequences. Using an eight tactor belt, vibrating
patterns encoding obstacle, destination, course, warning, and itinerary information
were conveyed to the user to enhance the visual navigation of virtual scenarios.
Preﬁxed patterns were easier to learn and memorize than tactile sequences.
Barber et al. conducted in [36] a study involving three categories of tactons:
static, dynamic, and directional. While static tactons consisted of constant patterns,
dynamic and directional tactons consisted of a set of sequential subpatterns that
transmit the sensation of motion. Static and dynamic tactons represent words while
the directional ones describe some type of navigational context. Their results
showed that through the pairing of dynamic and directional tactons, users were able
to interpret two-tacton sentences with an accuracy of 92%.
Finally, Riddle and Chapman proposed in [37] a ﬁve step methodology for
building tactile languages. The ﬁrst step is to deﬁne the message set. Deﬁning the
message set consists of identifying the concepts to be communicated either for
many different tasks or situations or for speciﬁc uses. The second step seeks to
determine the physical characteristics of vibrations such as vibrating frequency,
pulse duration, and sequence of activation. The third step is to deﬁne
application-speciﬁc design rules so that tactile parameters have implied meaning. If
these meanings can be linked to the inherent meaning of messages, patterns would
be intuitive and easy to learn. The fourth step is the creation of the tactons while
step 5 evaluates the tactons to validate their design and performance.
Note that most of the applications cited in this section were developed in the
context of human–computer interaction, virtual reality, and serious games.
However, much of their basic research comes from the ﬁelds of psychology,
cognition, and neurolinguistics.
3
Tactile Display for the Foot
Much of what is found in the literature about tactile feedback concerns tactile
stimulation of the ﬁngers and hands. However, the acuity of other body areas has
been explored as well: the wrist/forearm [14], abdomen [38], chest [39], tongue
600
R. Velázquez and E. Pissaloux

[40], ears [41], head [42], and even the backside [43] have been studied to transmit
information to a user. The devices are as diverse as the technology used and the
location on the body.
Since 2008, we have been studying the role of one of the less explored body
locations in tactile perception: the human foot. We seek to understand how people
perceive information through their feet and to evaluate whether this perception level
can be exploited in different human–computer interaction tasks.
For this purpose, we have developed several prototypes of electronic tactile
displays for the foot. In particular, the newest design (Fig. 1a) consists of four
vibrating actuators that stimulate the medial and lateral plantar areas of the foot sole
which concentrate most of the mechanoreceptors sensitive to vibrotactile stimula-
tion [44, 45].
In this prototype, vibrators are arranged in a diamond-like shape with 35 mm
side-length (Fig. 1b). All four actuators are integrated in a commercial inexpensive
foam shoe-insole. They provide axial forces up to 13 mN and vibrating frequencies
between 10 and 55 Hz. Each vibrator is independently controlled with a speciﬁc
vibrating frequency command [19].
This version is intended to be used on the left foot and is fully wearable
(Fig. 1c): it includes an RF (radio-frequency) transmission module which allows
simple and reliable point-to-point communication with a computer within a range of
100 m. It also includes the electronic drive to power the vibrating actuators and an
on-board power supply that ensures 6 h of autonomy. Figure 1c inset details the
electronic module that the user carries comfortably attached to the ankle.
Experimental perceptual studies have been already conducted with our proto-
types in sighted and blind users. We have tested navigational direction recognition,
shape identiﬁcation, pattern and emotion recognition, vocabulary learning, and
real-time navigation in space [17, 19, 22]. Our results indicate that people actually
understand information displayed to the plantar surface of the foot. However, this
information must not be complex as the foot is not capable of precise discrimina-
tion. Information displayed to the feet must be simple and preferably, encoded as
short structured vibrating patterns (tactons).
One of the most challenging applications for this device is perhaps the assistance
of visually impaired people: this mechatronic shoe-insole could be used for pro-
viding diverse information such as directions for independent navigation and sit-
uational awareness assistance.
An attractive feature of this device is that it can be further inserted into a shoe
making it an inconspicuous and visually unnoticeable assistive device. Unlike other
portable/wearable assistive devices, an on-shoe device does not heighten the
handicapped image that affects the user’s self-esteem.
Constructing Tactile Languages for Situational Awareness …
601

4
Evaluation and Results
Perceptual experiments were carried out to evaluate human performance in tactile
language learning and tactile memory using this prototype.
We reported in the past an experiment involving vocabulary learning [19]. For
that test, we proposed ﬁve vibrotactile patterns or tactons to represent arbitrary ﬁve
different words. Tactons were presented to test subjects, which were requested to
memorize them in a short period of time. Subjects were then asked to identify the
tactons. Results were very encouraging: recognition rates indicated that subjects
(a)
(b)
(c)
Fig. 1 Tactile display for the
foot: a Design concept. Inset:
target stimulation area
enclosed in square.
b Prototype. c Fully wearable
device with wireless
connection. Inset electronic
module
602
R. Velázquez and E. Pissaloux

could reasonably manipulate the set of tactons. We wondered then whether tactons
could be combined to represent more complex information and subject performance
in this case.
4.1
Study Participants and Experimental Procedure
Twenty undergraduate students (16 men and 4 women) at Panamericana University
participated voluntarily in the experiments. All gave their consent in agreement with
the university ethics guidelines. No special criteria were used to select them but
availability. None of the participants reported problems in tactile sensory or cog-
nitive functions. Their ages ranged from 18 to 24 years old with an average age of
20.8. None of them had tried any of our tactile display prototypes for the foot before.
To avoid any possible distraction, experiments were conducted in a restricted
access laboratory where it just remained the test subject and the researcher. During
the experiments, the subjects were seated wearing the tactile display on the left foot
and a headphone set. Audio cues generated by the vibrating motors were discarded
with a pink noise provided by the headphones. For hygiene, all subjects were
requested to use socks. Before each session, they were totally naive about all
aspects of the test and were given general instructions concerning the task. A short
familiarization time with the device was granted prior to the tests. Each subject was
asked to perform four experiments and to ﬁll out four answer forms. All of this took
on average 25–30 min.
For statistical analysis, subjects were randomly divided into two groups of 10.
The v2 distribution was used to evaluate difference in proportions across samples of
a same group while the z-test to give a conﬁdence interval for the true difference in
proportions between groups. The level of signiﬁcance to reject the null hypothesis
(a) was set to 0.05 in all cases.
4.2
Experiment I: Vocabulary Learning
The purpose of the ﬁrst test is to present the set of tactons to the subjects and to
evaluate whether they could quickly learn and retain them in memory.
4.2.1
Method
Four tactons were chosen for this test: boy, play, ball, and big. The vibrotactile
patterns in Fig. 2 were arbitrarily chosen to represent these words. For example,
‘boy’ in tactile language is represented by a long vibration followed by two short
ones while ‘play’ by a long vibration followed by a short one, and again a long
vibration.
Constructing Tactile Languages for Situational Awareness …
603

Subjects were asked to match what they felt tactually with one of these words.
Before the test, all four tactons were displayed to the subjects so that they could
make a mental representation of them. Upon request, they could have the tacton
refreshed on the display. When ready, the tester made a 1 min small talk on purpose
to distract their minds from the test. The test consisted of a single trial. Each tacton
was randomly displayed twice. Subjects had no time restriction to provide their
answers and they were allowed to modify them if they felt they had made a mistake.
4.2.2
Results
Figure 3 shows the results obtained. For group 1 the average recognition rates were
80, 90, 90, and 90% for boy, play, ball, and big, respectively. For group 2, these
were 70, 75, 85, and 90%, respectively. Recognition rates suggest that the proposed
tactons were easy to learn and to remember.
Subjects in both groups exhibited a uniform performance across the test (group
1: v2 = 1.37, p = 0.71, group 2: v2 = 3.12, p = 0.37). There was no statistically
signiﬁcant difference in the performances of the two groups (p > 0.05).
4.3
Experiment II: Constructing Sentences with Two Tactons
The purpose of this test is to evaluate subject performance when combining two
tactons. This test would require a higher level of concentration and is a ﬁrst step
toward constructing sentences that describe more complex situations.
4.3.1
Method
Tactons were combined in pairs to form four sentences: big-boy, boy-play, big-ball,
and playball. Sentences were displayed as in verbal communication: one tacton
ﬁrst, then a short pause, then the second tacton.
Boy
Play
Ball
Big
time
Fig. 2 Tactons representing
arbitrarily four words. Set
times for short and long
vibrations are 0.5 and 3 s,
respectively. Set time for ‘big’
(the longest vibration) is 5 s.
For all tactons, all four
vibrating motors in the
display are actuated
simultaneously at a vibrating
frequency of 55 Hz
604
R. Velázquez and E. Pissaloux

Subjects were asked to feel the entire sentence before reporting the words per-
ceived. They were not aware about the sentences so they could report any possible
combination. The test consisted of a single trial. Each sentence was randomly
displayed twice. Subjects had no time restriction to provide their answers and they
could have the tactile sentence refreshed on the display upon request.
4.3.2
Results
Figure 4a shows the results obtained. For group 1, the average recognition rates
were 65, 80, 80, and 75% for big-boy, boy-play, big-ball, and playball, respectively.
For group 2, these were 85, 75, 90, and 65%, respectively. Even though this task
was more complicated, recognition rates did not decrease substantially.
As in the previous test, subjects in both groups exhibited a uniform performance
across the test (group 1: v2 = 1.6, p = 0.65, group 2: v2 = 4.4, p = 0.22). Again,
there was no statistically signiﬁcant difference in the performances of the two
groups (p > 0.05).
Scores presented in Fig. 4a refer to totally correct sentences, that is when sub-
jects recognized both tactons. However, it is interesting to appreciate in detail
subject performance. Figure 4b shows the average distribution of both correct and
wrong answers. For example, for sentence big-boy, the 65% of the answers pro-
vided by subjects in group 1 indicate that they recognized successfully both tactons
while the remaining 35% indicate that they recognized one tacton but failed to do so
Boy
100
90
80
70
60
50
40
30
20
10
0
Correct answers (%)
Play
Ball
Tacton
Group 1
Group 2
Big
Fig. 3 Performance of the 20
subjects at learning and
memorizing the set of tactons
(p > 0.05). The standard error
is shown as an error bar
Constructing Tactile Languages for Situational Awareness …
605

for the other. Similarly, the 85% of the answers provided by subjects in group 2
indicate that they recognized both tactons, 5% only one, and 10% that they did not
recognized any tacton. Note that for most incorrect sentences, subjects tend to
recognize at least one tacton.
(a)
(b)
Fig. 4 a Performance of the 20 subjects at recognizing tactile sentences with two tactons
(p > 0.05). b Average distribution of both correct and wrong answers
606
R. Velázquez and E. Pissaloux

4.4
Experiment III: Constructing Sentences
with Three Tactons
The third test proceeds to evaluate subject performance with sentences containing
three tactons.
4.4.1
Method
Tactons were combined in triples to form three sentences: big-boy-play,
boy-playball, and play-big-ball. Again, tactile sentences were displayed as in verbal
communication: one tacton ﬁrst, a short pause, then the second tacton, a short
pause, and ﬁnally the third tacton.
As in the previous tests, subjects were asked to feel the entire sentence before
reporting the three words perceived. They were not aware about the sentences so
they could report any possible combination. The test consisted of a single trial. Each
sentence was randomly displayed twice. Subjects had no time restriction to provide
their answers and they could have the tactile sentence refreshed on the display upon
request.
4.4.2
Results
Figure 5a shows the results obtained. For group 1, the average recognition rates
were 70, 85, and 75% for big-boy-play, boy-playball, and play-big-ball, respec-
tively. For group 2, these were 70, 80, and 80%, respectively. Note that subjects
practically obtained the same recognition rates as with sentences with two tactons.
It was observed that subjects requested more often to refresh the sentence on the
display, but only at ﬁrst; subjects quickly arrived to get concentrated and manage
the three tactons.
Subjects in both groups exhibited again a uniform performance across the test
(group 1: v2 = 0.74, p = 0.68, group 2: v2 = 1.3, p = 0.52) and there was no
statistically signiﬁcant difference in the performances of the two groups (p > 0.05).
Figure 5b presents the average distribution of answers. Note that for most
incorrect answers, subjects did recognize one or two tactons. Answers with all three
tactons incorrect are the less.
4.5
Experiment IV: Constructing Sentences
with Four Tactons
The ﬁnal test combines all four tactons to form the longest sentences that will be
presented to the subjects.
Constructing Tactile Languages for Situational Awareness …
607

(a)
(b)
Fig. 5 a Performance of the 20 subjects at recognizing tactile sentences with three tactons
(p > 0.05). b Average distribution of answers
608
R. Velázquez and E. Pissaloux

4.5.1
Method
Tactons were combined in quads to form two sentences: big-boy-playball and
boy-play-big-ball. Again, tactile sentences were displayed as in verbal communi-
cation: an alternating sequence between tacton and pause.
The same protocol was followed: subjects were asked to feel the entire sentence
before reporting the sequence of words perceived. They were not aware about the
sentences so they could report any possible combination. The test consisted of a
single trial. Each sentence was randomly displayed twice. Subjects had no time
restriction to provide their answers and they could have the tactile sentence refre-
shed on the display upon request.
4.5.2
Results
Figure 6a shows the results obtained. For group 1, the average recognition rates were
65 and 55% for big-boy-playball and boy-play-big-ball, respectively. For group 2,
these were 70 and 75%, respectively. Note that subjects in group 1 dropped their
performance while subjects in group 2 maintained it reasonably. The results for
group 1, however, are heavily skewed by the low performance of three subjects.
For this last test, subjects in both groups exhibited again a uniform performance
across the test (group 1: v2 = 0.42, p = 0.5, group 2: v2 = 0.12, p = 0.72) and there
was no statistically signiﬁcant difference in the performances of the two groups
(p > 0.05).
Figure 6b shows the average distribution of answers. Note that for most incor-
rect answers, subjects did recognize half of the sentence. Answers reporting no
correct word were rarely observed.
4.6
Discussion
All four tests show that tactons can be quickly learned and retained in memory.
Furthermore, these experiments show that tactons can be combined into sentences
that represent more complex ideas and that tactile sentences containing up to four
tactons can be cognitively handled with high accuracy.
At each experiment, subjects in both groups observed a uniform performance
across the test and no statistically signiﬁcant difference in the performances between
groups was found. However, performance of subjects in group 1 was statistically
signiﬁcant different across the four experiments (v2 = 11.66, p = 0.008).
Figure 7 shows the evolution of the mean of correct answers across the four
experiments for both groups. Note that performance of group 1 progressively
decreases while that of group 2 remains reasonably constant. Also note that for both
groups, performance is practically the same for sentences with two and three
tactons.
Constructing Tactile Languages for Situational Awareness …
609

(a)
(b)
Fig. 6 a Performance of the 20 subjects at recognizing tactile sentences with four tactons
(p > 0.05). b Average distribution of answers
610
R. Velázquez and E. Pissaloux

5
An Example of Situational Awareness Assistance
During Navigation with Tactile Language
Independent and secure navigation in a real environment is one of the most chal-
lenging daily tasks people with visual impairments face having a direct impact on
quality of life, on well-being, and on social integration.
As aforementioned, the main aim of the on-shoe tactile display is to assist the
navigation of visually impaired and blind individuals. The purpose of this test is to
evaluate if it possible to manage directional information and tactile language rep-
resenting situational awareness at the same time.
5.1
Directional Information
We previously reported in [19], a tactile rendering approach for directional infor-
mation that has achieved high recognition rates.
This approach consists on setting a navigational direction to each one of the four
contact pins: forward F, backward B, right R, and left L. A navigational direction is
encoded in ﬁve sequences (t1–t5) as follows: three consecutive short vibrations in
the corresponding contact pin, then a short vibration in the opposite contact pin, and
again a short vibration in the correct contact pin.
Figure 8 shows for example, the codiﬁcation for going forward. Note that the
contact pin F vibrates three times, then B once, and again F. Average recognition
rates obtained from a group of 20 voluntary subjects were 91.65, 91.25, 78.75, and
91.65% for F, B, L, and R, respectively [19]. These ﬁgures suggest that people
easily and intuitively associate the tactile patterns to navigational directions.
Fig. 7 Evolution of
performance across the four
tests. Subjects in group 1
exhibit a statistically
signiﬁcant different
performance (v2 = 11.66,
p = 0.008) while subjects in
group 2 do not (v2 = 0.95,
p = 0.8)
Constructing Tactile Languages for Situational Awareness …
611

5.2
Navigation
5.2.1
Method
A camera-based tracking platform was set for this experiment. It consisted of a
camera placed 4 m above the ground surface that recorded RGB video. The
acquired video was later processed in a PC for subject tracking.
To have an idea of a typical performance that is not skewed by good or poor
performances, one of the 20 voluntary subjects was chosen for the experiment: a
female exhibiting the median average performance in understanding tactile
sentences.
The tactons shown in Fig. 8 were used for pointing her to a navigational
direction. A ﬁfth pattern consisting of two consecutive short vibrations, then a
pause, and then two consecutive short vibrations (the typical pattern for SMS alerts
in mobile phones) was used for indicating to stop. Patterns describing ‘ball’ and
‘big’ (Fig. 2) were redeﬁned to ‘chair’ and ‘obstacle’, respectively. Subject was
trained in learning the seven tactons as described in Sect. 4.2.
Tactons were provided by a computer located outside the navigation environ-
ment. During the test, the subject was blindfolded so that no cue from sight could be
obtained. A navigational environment (Fig. 9) was proposed to the subject who was
totally naive about their structure prior to and during the test.
Subject was requested to move according to the pattern felt and to sit down on a
chair located inside the environment when indicated. She had no time restriction to
complete the test and, upon request, she could have the tactons refreshed on the
interface.
Fig. 8 Schedule of activation of the vibrating motors for the navigational direction rendering
(example for going forward)
612
R. Velázquez and E. Pissaloux

5.2.2
Results
Figure 9a shows the results obtained in this test. Subject successfully followed 11
navigational instructions to reach point A (forward-stop-turn right-forward-stop-
turn left-forward-stop-turn left-forward-stop). At point A, a four-tacton sentence
displays: obstacle left, chair right (Fig. 9a). Subject acts accordingly (Fig. 9b).
Note that the obstacle/chair preﬁx changed the meaning from actually going to
just pointing directions. Semantics relations like this one can be easily established
by subjects.
6
Conclusion
This paper presented the results of user studies showing the ability to learn,
memorize, and use tactile words.
Vibrotactile patterns or tactons abstractly representing verbal language can be
understood, quickly learned, and retained in memory. Furthermore, sentences
involving two, three, or four tactons can be constructed and recognized with high
(a)
(b)
Fig. 9 Situational awareness
assistance during navigation:
a Navigation in a structured
environment. The broken
yellow line represents the
trajectory followed by the test
subject. b Action upon
assistance provided by a
tactile sentence
Constructing Tactile Languages for Situational Awareness …
613

accuracy, which broadens the possibilities for describing complex situations that
could improve interaction with mobile and wearable computers and in particular,
situational awareness feedback provided by assistive devices.
An interesting observation from these tests is that tactons are retained in
short-time memory. A follow-up study revealed that after one week without any
practice; most of the test subjects could only recall the pattern associated to ‘big’.
After a month and without any further practice, no tacton could be identiﬁed by any
of the 20 subjects. Nevertheless, with constant practice (case of visually impaired
and blind individuals), tactons could work in long-time memory.
Results obtained from the tests seem very promising for podotactile stimulation.
Tactons applied to the foot can be understood and their predeﬁned meaning can be
easily associated. Future work will evaluate the perceptual load of tactons and
tactile sentences. This work will seek to determine for how long and under which
circumstances a user manages to be fully concentrated in tactons before getting tired
or distracted. Several high perceptual load conditions will be tested such as noisy
environments and crowded spaces at several tacton presentation rates: rare, con-
stant, and high.
References
1. Pawluk D, Adams R, Kitada R (2015) Designing haptic assistive technology for individuals
who are blind or visually impaired. IEEE Trans Haptics 8(3):258–278
2. Velazquez R, Hernandez H, Preza E (2012) A portable piezoelectric tactile terminal for
Braille readers. Appl Bion Biomech 9(1):45–60
3. Flores G, Kurniawan S, Manduchi R, Martinson E, Morales L, Sisbot E (2015) Vibrotactile
guidance for wayﬁnding of blind walkers. IEEE Trans Haptics 8(3):306–317
4. Tomasello M (2008) Origins of human communication. MIT Press, Cambridge
5. Birdsong D (2006) Age and second language acquisition and processing: a selective
overview. In: Gullberg M and Indefrey P (eds) The cognitive neuroscience of second
language acquisition, Wiley, New York, pp 9–49
6. Plonsky L (2011) The effectiveness of second language strategy instruction: a meta-analysis.
Lang Learn 61(4):993–1038
7. Bayley R (2005) Second language acquisition and sociolinguistic variation. Intercultural
Commun Stud 14(2):1–15
8. Dornyei Z (2005) Psychology of the language learner: individual differences in second
language acquisition. Lawrence Erlbaum, New Jersey
9. Douglas Brown H (2006) Principles of language learning and teaching. Pearson, UK
10. Brewster S, Brown L (2004) Tactons: structured tactile messages for non-visual information
display. In: Proceedings of Australasian user interface conference, pp 18–22
11. Jennings S, Craig G, Cheung B, Rupert A, Schultz K (2004) Flight-test of a tactile situational
awareness system in a land-based deck landing task. In: Proceedings of human factors and
ergonomics society annual meeting, pp 142–146
12. Sklar A, Sarter N (1999) Good vibrations: tactile feedback in support of attention allocation
and human-automation coordination. Hum Factors 41(4):543–552
13. Ho C, Tan H, Spence C (2005) Using spatial vibrotactile cues to direct visual attention in
driving scenes. Trans Res Part F: Trafﬁc Psychol Behav 8(6):397–412
614
R. Velázquez and E. Pissaloux

14. Ng G, Barralon P, Dumont G, Schwarz S, Ansermino J (2007) Optimizing the tactile display
of physiological information: vibro-tactile vs. electro-tactile stimulation, and forearm or wrist
location. In: Proceedings of annual international conference of the IEEE engineering in
medicine and biology society, pp 4202–4205
15. Nakamura A, Tabata S, Ueda T, Kiyofuji S, Kuno Y (2005) Dance training system with active
vibro-devices and a mobile image display. In: Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp 3075–3080
16. Van Erp J, Saturday I, Jansen C (2006) Application of tactile displays in sports: where to, how
and when to move. In: Proceedings of EuroHaptics Conference, pp 90–95
17. Velazquez R, Bazan O (2010) Preliminary evaluation of podotactile feedback in sighted and
blind users. In: Proceedings of annual international conference of the IEEE engineering in
medicine and biology society, pp 2103–2106
18. Tsukada K, Yasumrua M (2004) ActiveBelt: belt-type wearable tactile display for directional
navigation. In: Proceedings of UbiComp 2004, Springer LNCS3205, pp 384–399
19. Velazquez R, Bazan O, Alonso C, Delgado-Mata C (2011) Vibrating insoles for tactile
communication with the feet. In: Proceedings of international conference on advanced
robotics, pp 118–123
20. Benali-Khoudja M, Hafez M, Sautour A, Jumpertz S (2005) Towards a new tactile language
to communicate emotions. In: Proceedings of IEEE international conference on mechatronics
and automation, pp 286–291
21. Jones LA, Kunkel J Torres E (2007) Tactile vocabulary for tactile displays. In: Proceedings of
IEEE world haptics conference, pp 574–575
22. Velazquez R, Bazan O, Magaña M (2009) A shoe-integrated tactile display for directional
navigation. In: Proceedings of IEEE/RSJ international conference on intelligent robots and
systems, pp 1235–1240
23. Nicholas J (2010) From active touch to tactile communication: What’s tactile cognition got to
do with it? Danish Resource Centre on Congenital Deafblindness
24. Geldard F (1957) Adventures in tactile literacy. Am Psychol 12(3):115–124
25. Gilson E, Baddeley A (1969) Tactile short-term memory. Q J Exp Psychol 21(2):180–184
26. Sullivan E, Turvey M (1972) Short-term retention of tactile stimulation. Q J Exp Psychol
24:253–261
27. Watkins M, Watkins O (1974) A tactile sufﬁx effect. Mem Cogn 2:176–180
28. Mahrer P, Miles C (1999) Memorial and strategic determinants of tactile recency. J Exp
Psychol Learn Mem Cogn 25:630–643
29. Handel S, Buffardi L (1969) Using several modalities to perceive one temporal pattern. Q J
Exp Psychol 21(3):256–266
30. Conway C, Christiansen M (2005) Modality-constrained statistical learning of tactile, visual,
and auditory sequences. J Exp Psychol Learn Mem Cogn 31(1):24–39
31. Evreinova T, Evreinov G, Raisamo R (2006) An alternative approach to strengthening tactile
memory for sensory disabled people. Univ Access Inf Soc 5(2):189–198
32. Wang Q, Levesque V, Pasquero J, Hayward V (2006) A haptic memory game using the
STReSS tactile display. In: Proceedings of ACM conference on human factors in computing
systems, pp 271–274
33. R. Kuber, M. Tretter, and E. Murphy, “Developing and evaluating a non-visual memory
game”, Proc. of INTERACT 2011, LNCS 6947, pp 541–553, 2011
34. Raisamo R, Patomaki S, Hasu M, Pasto V (2007) Design and evaluation of a tactile memory
game for visually impaired children. Interact Comput 9(2):196–205
35. V. Oliveira and A. Maciel, “Assessment of tactile languages as navigation aid in 3D
environments”. In M. Auvray and C. Duriez (eds.), Haptics: Neuroscience, Devices,
Modeling, and Applications, LNCS 8619, Springer, pp 104–111, 2014
36. Barber D, Reinerman L, Matthews G (2015) Toward a tactile language for human-robot
interaction: two studies of tacton learning and performance. Hum Factors 57(3):471–490
37. Riddle D, Chapman R (2012) Tactile language design. In: Proceedings of human factors and
ergonomics society annual meeting, pp 478–482
Constructing Tactile Languages for Situational Awareness …
615

38. Tsukada K, Yasumrua M (2004) ActiveBelt: belt-type wearable tactile display for directional
navigation. In: Proceedings of UbiComp 2004, Springer LNCS 3205, pp 384–399
39. Jones L, Lockyer B, Piateski E (2006) Tactile display and vibrotactile pattern recognition on
the torso. Adv Robot 20:1359–1374
40. Ptito M, Moesgaard S, Gjedde A, Kupers R (2005) Cross-modal plasticity revealed by
electrotactile stimulation of the tongue in the congenitally blind. Brain 128:606–614
41. Gemperle F, Ota N, Siewiorek D (2001) Design of a wearable tactile display. In: Proceedings
of international symposium on wearable computers, pp 5–12
42. Kalb J, Amrein B, Myles K (2008) Instrumentation and tactor considerations for a
head-mounted tactile display. In: Technical Report ARL-MR-705
43. Hogema J, de Vries S, van Erp J, Kiefer R (2009) A tactile seat for direction coding in car
driving: ﬁeld evaluation. IEEE Trans Haptics 2(4):181–188
44. Kennedy P, Inglis T (2002) Distribution and behavior of glabrous cutaneous receptors in the
human foot sole. J Physiol 538(3):995–1002
45. Kafa N (2015) Foot sensation, balance and proprioception. In: Kaya D (ed) Proprioception:
the forgotten sixth sense, OMICS Group International, pp 1–13
616
R. Velázquez and E. Pissaloux

Vision Restoration with Implants
Akos Kusnyerik, Miklos Resch, Huba J. Kiss and Janos Nemeth
1
Introduction
Up until now there has been no available treatment for diseases causing the per-
manent impairment of retinal photoreceptors. Currently, the development of the
retinal prostheses is the earliest to promise a result that can be implemented in the
clinical treatment of these patients. To date two different types of retinal prostheses
are commercially available, the Retina Implant approved the CE marking in 2013,
and Argus II implant achieved FDA approval in the same year. Besides the diseases
of the retinal photoreceptors, the permanent impairment of other retinal cells or
diseases and injuries of the optic nerve or optic pathway are also causing severe and
untreatable conditions.
Implants with different operating principles and in various stages of progress are
presented in details, highlighting the characteristics of the development.
Despite of the considerable development of ophthalmologic microsurgery
techniques and medical treatment modalities many eye diseases remained untreat-
able. According to the WHO survey the number of blind people is approximately
39 million [1–3]. The “Vision 2020” program that was started in 2000 by the World
Health Organization aims to extinguish the causes of avoidable blindness and to
reach a signiﬁcant decrease in the number of the visually impaired people [4, 5, 6].
The WHO “Universal Eye Health, a Global Action Plan 2014–2019” has initiated a
new program in 2014 to reduce the visual impairments worldwide by 25% till the
end of 2019. To fulﬁll the goals of the programs is difﬁcult, since the increase in
both the number and the average age of the human population makes a higher and
higher incidence of blindness [3].
A. Kusnyerik  M. Resch  H.J. Kiss  J. Nemeth (&)
Department of Ophthalmology, Semmelweis University,
Maria utca 39, 1085 Budapest, Hungary
e-mail: nemeth.janos@med.semmelweis-univ.hu
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_20
617

In developed countries degenerative diseases of the retina play a leading role in
the causes of the blindness [7]. Destruction of the photoreceptors is common in both
retinitis pigmentosa and age-related macular degeneration. Currently at the
end-stage of degenerative retinal diseases, improvement of the vision may be
achieved only by the different retinal implants.
Vision restoration implants are devices that induce visual experience by the
stimulation of the remaining and functional cells of the retina, optic nerve, visual
pathway, or visual cortex. Retinal implants are devices that induce visual experi-
ence by the stimulation of the remaining and functional cells of the retina. The
implantable retinal implant is called vision restoration implant, and it includes a
retinal chip, as well. Elementary visual light experiences, the so-called phosphenes
have been achieved in the 50s by the help of electrodes implanted directly into the
brain [8]. Because of the numerous technical difﬁculties these experiments were
suspended, and research in this ﬁeld was restarted only recently [9–11].
The continuous evolvement of microelectronics and engineering achieved the
development of devices becoming highly similar to the light-sensitive retinal
structures. Such an intra-retinally implantable device replaces the photoreceptors,
and generates biological signals induced by the incoming light. Our aim in this
chapter was to introduce the reader to these techniques and ﬁeld, and to compare the
simultaneous developments of technical novelties and clinical studies.
2
Early Historic Developments
The “silicone retinal chipsets” was the ﬁrst attempt in the imitation of the function
of the human retina by integrated circuits. However, the lack of the knowledge of
retinal modeling, these devices remained as engineering specialties, being able to
mimic only external retinal effects [12].
Intense intraocular implant research began at the early 90s. The most obvious
solution seemed to be the use of subretinal implants, since subretinal implants
“only” need to replace the generation of the electronic signals as a response to the
visible light. The main difﬁculties of this technique are the proper design of the
metal
electrode,
its
biocompatibility
and
the
elimination
of
the
charge
accumulation.
The next possibility is the epiretinal chip. Here a serious preprocessing is
required to replace the function of the retina by mimicry. The “Bionic Eye”
architecture serves as the core of this design [13]. The pioneer of the epiretinal
chipsets is Marc Humayun, the ophthalmologist researcher of the University of
Southern California. His fellow researcher, professor Wen-Tai Liu of the University
of California at Santa Cruz created the ﬁrst chipsets with a resolution of 4 by 4 than
8 by 8. These were implanted into humans with moderate success [14].
The more detailed information discovered on the structure of the mammalian
retina lately [15] made a new situation in the ﬁeld. Since the output of the retina is
not only a single image but a summary of different frames, and since the attachment
618
A. Kusnyerik et al.

surface may not be too small, channels could be short-circuited easily. Due to these
technical difﬁculties the exact modeling of the retinal channels by the cellular retina
chip is rather impossible [16]. This discovery made certain research groups shift to
the development of subretinal implants [17].
In the following, we are going to summarize the main properties of the different
retinal implants and to detail the indications of their implantation.
3
Types of the Implants
1. Intraocular implants
Since the energy of the visible light reaching the retina currently is not enough to
operate the implant, independently of their location, all implants have an external
energy source. Implants can be grouped by their localization to the retina. Implants
attached to the vitreal surface of the retina are the epiretinal, while implants attached
to the pigment epithelium under the retina are the subretinal implants. Figure 1
summarizes the possible locations of the chipset related to the retina. The energy for
the ampliﬁcation of the light reaching the chipset, the data processing, and analysis
of the data is coming from the external unit. Depending on the operational mech-
anisms the epiretinal chipsets (Fig. 1a) could be connected not only to external
energy sources, but as well as cameras integrated into the frames of glasses. At
subretinal chipsets (Fig. 1b) the implant is directly stimulated by the visible light
and thus requires only an energy input.
1:a. Epiretinal implant
The epiretinal implant is located on the vitreal surface of the retina. It has multiple
segments located at the place of the crystalline lens, or integrated in the frame of the
glasses, as well as in the external unit. The intraocular chipset has a direct connection
to the retina and it is attached to the surface by a specially designed rivet. This design
allows the chipset to stimulate all axons of the remaining ganglion cells by the
information recorded by the camera in the frame of the glasses directed by head
movements.
1:b. Subretinal implant
The characteristic property of the subretinal prostheses is that they are localized
between the retinal pigment epithelium and the layer of the destroyed photore-
ceptors. Their other speciﬁcs may differ. The intraocular unit of the most advanced
subretinal implant contains 1.500 microelectrodes. The wire starting from the
chipset runs between the pigment epithelium and the neuroretina, perforates the
sclera and goes into the orbital cavity. The design is highly similar to the cochlear
implants designed for people of hearing loss [17, 18].
Vision Restoration with Implants
619

Fig. 1 Figure Illustration of the intraocular implants. a The design of the epiretinal implant. b The
design of the subretinal implant. a The design of the epiretinal implant. 1 The object in the visual
ﬁeld of the camera integrated into the glasses; 2 the camera integrated into the glasses, recording
motion pictures; 3 the unit processing the recorded movie with the power unit; 4 glasses with
camera and transceiver; 5 intraocular coil designed for wireless signal forwarding; 6 postprocess-
ing unit between the recording unit and the implant; 7 intraocular wire; 8 epiretinal implant, b the
design of the subretinal implant. 1 The object in the visual ﬁeld of the chip; 2 the subretinal
implant; 3 wire under the retina; 4 ﬁxation plate attached to the sclera; 5 power unit implanted
subcutaneously behind the ear; 6 outer power and processing unit
2. Extraocular implants
Extraocular prostheses outside of the eyeball exist, as well. They are located either
intraorbitally or intracranially. On this ﬁeld researchers are faced by numerous
difﬁculties as biocompatibility, convenient electrode design or the stimulation
patterns of the electrodes.
2:a. Optic nerve device
There are current experiments using an extraocular but intraorbital implant having a
device ﬁxed ring-wise on the optic nerve (Fig. 2) [19].
2:b. Thalamus device
An appropriate place of microstimulation would also be the lateral geniculate body
of the thalamus, as it is an important part of the visual pathway. Pezaris et al.
conﬁrmed this approach lately in successful monkey experiments [20, 21].
620
A. Kusnyerik et al.

Fig. 2 Figure illustration of the ring-shaped optic nerve implant (C-sight project)
2:c. Cortical visual implant
An additional approach is that of cortical devices and electrodes which are placed
directly to the visual cortex. First Brindley than Dobelle et al. had experiments of
phosphen responses evoked by such electrodes [22–24].
The cortical device is using an external camera with an integrated image ana-
lyzer to transfer the processed signal to the central computer unit [24]. This unit is
stimulating the remaining cells and structures according to the requested pat-
terns [25]. Limitation of this approach is the complete development of an
image-processing system to convert an electronic image captured by a camera into a
real-time data stream [24].
However, human trials of the next generation of cortical visual implants are
imminent [24, 26]. Recently, Second Sight announced the ﬁrst successful
implantation of Orion I Visual Cortex Prosthesis.
4
Indications
1. Indications of intraocular implants
The retinal implants are most useful in bilateral cases when both eyes are damaged
and the patient is blind. The retinal implants are able to work only in case of those
retinal diseases which did not involve the whole retina and the ganglion cells and so
Vision Restoration with Implants
621

the optic nerve and visual pathway are well functioning and they are able to transfer
the information and electric signals coming from the retinal implant to the visual
cortex. These implants replace the functions of the damaged retinal cells, like the
photoreceptors light sensitivity (in case of the subretinal implants) or the pho-
toreceptors and bipolar cells functions (like the epiretinal implants). There are some
other important inclusion criteria, like earlier period of appropriate visual functions
at least 12 years, adequately perfused retinal vessels, age, no retinal detachment or
edema, good inner retinal function, etc. [27]
Degenerative diseases of the retina develop a varied appearance. Among the her-
itable types of degenerative retinal diseases retinitis pigmentosa occurs the most
often and with the most homogenous clinical appearance. Progression of this dis-
ease depends on many factors; among many others the diverse genetic background
inﬂuences strongly the outcome. Retinitis pigmentosa is a common name for those
diseases, in which photoreceptors of the retina decay progressively making the light
unable to evoke responses on the retina. The word retinitis in the name of the
disease preserved the formerly believed inﬂammatory etiology. Previously, the
name tapetoretinal degeneration was also used [28, 29], but now the degeneratio
pigmentosa retinae and the retinitis pigmentosa are names used. Typically, the
visual impairment begins at adolescent age, ﬁrst the vision loss occurs at twilight
and/or at the peripheral vision. Later, the disease may lead to a complete loss of
light sensitivity by the young adult age [30]. In this phase, all other elements of the
Fig. 3 Localisation of the subretinal implant in the right eye of one of the patients with end stage
retinitis pigmentosa, undergone surgery in Semmelweis University, Budapest, Hungary. a Fundus
photograph showing optic nerve, narrow retinal arterioles and venules, atrophic macula and
pigment clumps in the peripheral retina. b Infrared fundus image of the same ye, green line
represents the site of the scan shown on Fig C. c Optical coherence tomographic (OCT) image
characteristic for retinitis pigmentosa, demonstrating atrophic choroid and retinal pigmentepithe-
lium. Neuroretinal thickness is low, foveolar morphology is almost normal. d Funduscopic image
of the same eye 3 months after implantation. Note the subretinal electrode of the implant under the
macular region. e Infrared image. Note the retinal vessels in front of the implant. f OCT cross
sectional image of the implant (arrow), and the edematous neuroretina above (*), for comparison
intact retinal structure corresponding to retinitis pigmentosa on the left side of the image (#)
622
A. Kusnyerik et al.

retina except the photoreceptors are at least partially functional [30, 31]. This
explains why retinal implants may be useful in these types of diseases: the neural
cells of the retina others than the photoreceptors and the visual pathway must be
functional to be able to forward the information. That part of the implant, which is
attached to the retina, replaces the photoreceptors and directly stimulates the
remaining cells with electric stimuli. This is why the target population at the
beginning of the testing of these devices was the group of patients affected by this
disease. This situation has unchanged. The current major indication of retinal chip
implantation is the population of retinitis pigmentosa patients, who are at the
end-stage of the disease and whose vision is damaged in such severe way that they
are not able to travel alone or to sense the light [32].
If an appropriate improvement of vision becomes achievable among retinitis
pigmentosa patients, the use of the implant emerges in other degenerative retinal
diseases, such as in age-related macular degeneration, one of the leading causes of
blindness in developed countries. Other indications might be cone rod dystrophies
and choroideremia.
2. Indications of extraocular implants
In case of implants with electric stimulation of the thalamus or visual cortex, the
indication is far wider. Theoretically, they are able to restore useful vision for all
persons with acquired blindness due to any eye disease, including the total
destruction of the retina (like in diabetic retinopathy, old retinal detachment), or the
eyeball (like in severe cancers and injuries) or in case of the total destruction of the
optic nerve (like in glaucoma). In these cases, the electronic devise replaces all the
functions of the retina (image capture and primary analysis) and that of the optic
nerve (information transfer to the brain).
5
Preclinical Studies
Several preclinical trials of the ﬁeld were initiated in the last decade and they
indicated a strong cooperation between different institutes and workgroups.
One of the latest examples of such cooperation is an Australian project. The
participants of this consortium are the Sydney University, the University of
Melbourne, the Centre for Eye Research Australia, the University of New South
Wales, the NICTA, the Bionic Ear Institute, the Australian National University, and
the University of Western Sydney. The project is called the Bionic Vision Australia
and is led by Professor Anthony Burkitt. Participants of this project had a signiﬁ-
cant role in the development of the cochlear implant formerly. They aimed to create
the prototype of the ﬁrst human implant and to implant it into patients suffering of
vision loss in 2012. Starting of this prototype they plan to develop the second
generation of the implants, which would allow face-recognition, reading and a
Vision Restoration with Implants
623

visual acuity of 20/80. Stimulation of this device would be done by 1.000 electrodes
and a two-way high-speed communication would be possible between the units.
A camera attached to the glasses would record the images, and the processed
information would have forwarded to the suprachoroidally implanted chipset
wirelessly [33, 34]. Later, they started to develop three different bionic eye devices
with different number of electrodes (44 electrodes, 98 and 256 electrodes,
respectively).
The “C-Sight Project” of China is led by Professor Qiushi Ren. In this project,
needle-like electrodes would be attached in a ring-like fashion to the optic nerve
penetrating between the nerve ﬁbers (Fig. 2). The Universities of Beijing,
Shanghai, and other Chinese Universities participate in this consortium. The animal
experiments already started but the following schedule is unknown yet [19, 35, 36].
In the United States several trials were running simultaneously, many of these
was expected to reach the clinical phase.
The Boston Retinal Implant Project (BRIP) is a joint work between the Harvard
University (Professor Joseph Rizzo), the MIT Electronic Laboratories (Professor
J. Wyatt), and the Cornell laboratory (CNF). First, they interest concerned epiretinal
implants, but recently they expanded their efforts to subretinal implants, as well.
Their research proﬁle includes the special electric circuit defendant system, the
improvement of the surgical implantation technique and the complete covering of
the device to protect the chipset, the wires, and the electrode surfaces [37]. The
device has been tested both in vitro and in two Yucatan minipigs [38].
The trial of the Stanford University workgroup, led by Professor Daniel Palanker
is both surprising and promising. They develop a ﬂexible, easily implantable
prosthesis, which uses the energy of the light, the “High Resolution Photovoltaic
Retinal Prosthetic System.” According to the original description the processed
images from video camera are displayed on a special video goggles, and projected
through the eye optics directly onto the retina using a pulsed near-infrared (880 nm)
light. These experiments successfully tested in ex vivo and in vivo; the ﬁrst human
trial recruited participants in collaboration with Pixium Vision under supervision of
Prof. Jose Sahel [39–42]. The unit called PRIMA planned to be implanted in Paris,
at the Quinze-Vingts National Eye Hospital.
The working group of Korea led by Professor Kim started its work named as the
“Korean Artiﬁcial Retina Project” in 2000 at The University of Seoul. Their
chipset, attached on a ﬂexible polyimide plate is implanted suprachoroidial. The
prosthesis is tested only in animal experiments yet [43, 44]. Their high-density
electrode arrays were under investigation focusing on stimulation parameters that
were optimized by ex vivo experiments.
In Japan, simultaneously several centers (Gamagori, Tokyo, and Sendai)
focusing on the development of retinal implants. The Japan Retina Implant Group
started by the late Professor Yasou Tano and supported by the Nidek Vision
Institute develops suprachoroidial prostheses since 2000. They planned to improve
the resolution of the implant by the increase of the electrodes [45–49].
There are several articles and reviews presenting and comparing the latest results
[24, 25, 47].
624
A. Kusnyerik et al.

6
Clinical Trials and Results
6.1
Argus II Retinal Prosthesis, United States of America
The Argus type second-generation epiretinal chipset is currently available to
implant in humans by a group led by Mark Humayun. The clinical trial is coor-
dinated by the ﬁrm, Second Sight of California. The resolution of this
second-generation prosthesis is 60 pixels, compared to the previous generation
16-electrode Argus I implant. A central processing unit transforms the images
recorded by the camera glasses to the 60 electrodes located on the retina. The
prosthesis used in this trial was developed by the Lawrence Livermore National
Laboratory.
The device is composed of three units: the camera integrated into the glasses,
connected to a signal forwarding system. The signal is sent to the central computer
unit, that transcodes the signal into electric impulses. The third unit of the prosthesis
is the implantable unit containing a thin net of tiny electrodes and a signal receiver.
The prosthesis has been implanted into 36+ participants (Argus I: 2002–2004: 6;
Argus II: 2005-: 30+). Patients could often—but not always—sense and identify the
light and moving objects. One of the patients could identify and follow an
inch-wide line painted on the ﬂoor. According to the data published in
peer-reviewed, different kind of visual function assessments were performed:
(1) square/object localization, (2) direction of motion and (3) discrimination of
oriented gratings (a.k.a. visual acuity). In the trial real-world orientation and
mobility (O&M) tests were also explored: a test where patients located a door and a
test, where patients were asked to follow a line on the ﬂoor.
Adverse events experienced by Argus II recipients occurred in the early postop-
erative period. The detailed data on the adverse events are awaiting publication [26].
In two cases a severe ophthalmic inﬂammation, endophthalmitis occurred [50, 51].
6.2
The Epi-Ret3 Trial, Germany
In the IMI-IRIS study of a German consortium led by Professor Gisbert Richard an
epiretinal implant is used in an approach similar to that of the American researchers.
In the ﬁrst phase of these studies, the chipset was implanted for the duration of less
than an hour. Recently, the third phase is in progress that was started in 2007. In
their latest papers they published details of the implantation method, the following
removal and the achieved results [52, 53, 54]. The prosthesis was implanted into six
patients in Essen and Aachen. Thanks to the compact design, the implantable unit
has two subunits. The extraocular unit includes the computer and the signal for-
warding system. The intraocular unit includes a receiver, a signal processor, and the
25 hexagonally distributed, iridium oxide covered electrodes having a diameter of
100 lm each. By induction, the device is controlled remotely, wirelessly.
Vision Restoration with Implants
625

The tests were performed in 4 weeks following the surgery. All the patients
could identify the stimulations of the electrodes as ﬂashes of light. During the
stimulations they discovered, that the evoked responses have a stronger correlation
with the width of the stimulus as its intensity [54].
In this project implantations are not performed recently, and they cannot accept
new applicants.
6.3
Retina Implant Alpha IMS, Germany
The most impressive surgeries and clinical trials with subretinal implants were run
by the working group of Professor Eberhart Zrenner, Tubingen, Germany. The
Retina Implant AG coordinates these trials. Many German clinics participate in this
program, e.g., those in Regensburg, Dresden, and Kiel.
During the ﬁrst phase of the trial running from 2005 to 2009 ﬁrst generation
implants were implanted into 11 patients. The second phase of the trial—with the
participation of the Department of Ophthalmology Semmelweis University
Budapest—provided evidence, that theoretically, the second-generation implants
may remain in the human body for an indeﬁnite time, because they are separated
completely from the outside environment. Based on these results the second-
generation subretinal prostheses achieved CE marking. The resolution of these
devices is unique having as much as 1.500 pixels (Fig. 3).
The implantation of the chipset is done transchoroideally by the help of a special,
supporting ﬁlm. The exact position of the implant is speciﬁcally determined by
previous considerations giving the promise of the best possible functional results
(Fig. 3). The planning before the surgery includes the determination of the required
place and the design of the wire with individual length [26]. Many preceding
measurements including imaging processes and data analysis are needed for the
precise chip and chip-location design [55]. At the operations so far no complica-
tions or side effects developed that would have required a further intervention.
Subjects tolerated the surgery well and remained complaintless [56].
In several cases, patients having a successful implantation surgery told as if they
had seen a shiny window frame. By the help of the prosthesis larger shapes became
recognizable and changes in light intensity became distinguishable. The idea, that
by the help of the subretinal implant a visual experience is achievable, was con-
ﬁrmed by the study of Tubingen in the most robust way of all. One of the patients
had such a vision improvement that from a 60 cm distance became able to deter-
mine the direction of the Landolt C that is equal to a visual acuity of 20/1000 (log
MAR = 1.69) [56]. A larger number of patients were successful in orientation tests;
patients could recognize common subjects, like a plate, cutlery, a banana, etc [57].
The Alpha IMS has CE approval and currently commercially available in several
European sites, at the moment testing of the updated model Alpha AMS is under
investigation.
626
A. Kusnyerik et al.

7
Orion I Visual Cortical Prosthesis (Orion I)
In October 2016, the Second Sight announced the ﬁrst successful implantation of a
wireless visual cortical stimulator in a human subject. The implantation was per-
formed in the UCLA. The ﬁrst implantation did not yet include a camera, but after
the activation of the prosthesis, the young adult patient was able to perceive and
localize individual phosphenes or spots of light. No any signiﬁcant adverse side
effect was observed.
8
Conclusions
Many preclinical and clinical trials of retinal, cortical and other vision restoration
implants were started years ago and many are still in progress. The cortical, the
subretinal, and the epiretinal prostheses have all reached the clinical phase. The
outcome after the surgery depends on numerous factors. A comparison between the
different types of implants and a summary of their differences would be useful.
If the electrodes are located the most physiological way, they may have an
optimal use of the remaining neural cells able to process visual information starting
at the ﬁrst transmission step in the retina like in healthy functions. The intraretinal
data processing has a great signiﬁcance in the ﬁnal visual experience. One of the
most important advantages of the subretinal implant is the best possible use of most
remaining functional cells and synapses. Since the subretinal implant is strongly
attached to the retina the energy needed for the stimulation is lower, and the harmful
heating effect is less. The retinotopic stimulation is beneﬁcial too, because it is
highly similar to the physiological signal transmission.
Moreover, the subretinal implant is fastened by the help of the intraocular
pressure and does not require a rivet, like the epiretinal implant. As the subretinal
implant does not touch the vitreoretinal surface, the proliferative-vitreoretinopathy,
a severe complication is less likely to develop. Because the light sensors are on the
surface of the Tubingen type chipset, with this approach exactly those retinal areas
become stimulated, that the light falls on. This has two advantages: the exercises
requiring coordination are easier to perform, and the ﬁne eye movements are
enough to position the subject in the focus not requiring large movements of the
whole head to achieve this goal. Though further tests are needed to ascertain this
statement, but from the experience gathered so far it seems that the micro-saccade
movements of the eye play an important role in sensation of the sharp images and
the contrasts. Therefore, this type of subretinal implant is able to refresh the
neighboring pixels without further manipulation or programming.
Mobilization of the patients with visual loss is one of the most important goals of
all the programs, and all the working groups handle the movement and coordination
exercises carefully. As there are diverse tests for these functions, and no consensus
Vision Restoration with Implants
627

exists among the groups, currently it is difﬁcult to compare their results. Intentions
emerge to develop a uniﬁed testing method enabling the direct comparison of the
data of different working groups with different devices.
9
Future Trends
Retinal implants provide a promising chance for people having a vision loss with an
intact visual pathway able to forward the generated signals of the chipset.
Developments in microelectronics in recent years made it possible and proved to
be feasible to replace the degenerated elements in the retina with electrical stimu-
lation. Multiple comparable approaches are running simultaneously. Two types of
these implants are directly stimulating the remaining living cells in the retina.
Hitherto the ﬁnest resolution has been achieved with the subretinal implants.
Although the epiretinal implant offer lower resolution, but requires shorter surgery
for implantation. Retinal implants in certain retinal diseases are proved to be cap-
able of generating vision-like experiences. A number of types of retinal implants
are already available in clinical practice.
References
1. Pascolini D, Mariotti SP (2012) Global estimates of visual impairment: 2010. Br J
Ophthalmol 96:614–618
2. Schulze Schwering M (2007) Global blindness. Ophthalmologe 104:845–848
3. Stevens GA, White RA, Flaxman SR, Price H, Jonas JB, Keeffe J, Leasher J, Naidoo K,
Pesudovs K, Resnikoff S, Taylor H, Bourne RRA, on behalf of the Vision Loss Expert Group
(2013) Global prevalence of vision impairment and blindness. Magnitude and temporal
trends, 1990e2010. Ophthalmology 120:2377–2384
4. Lewis PM, Lauren NA, Robyn H (2016) Advances in implantable bionic devices for
blindness: a review. ANZ J Surg 86:654–659
5. Nayar A (2010) World gets 2020 vision for conservation. Nature 468:14
6. Németh J, Süveges I (2001) Vision 2020-Worldwide program for the elimination of avoidable
blindness. Szemészet 138:115–117
7. Gehrs KM, Anderson DH, Johnson LV et al (2006) Age-related macular degeneration–
emerging pathogenetic and therapeutic concepts. Ann Med 38:450–471
8. Tassicker GE (1956) Preliminary report on a retinal stimulator. Br J Physiol Opt 13:102–105
9. Normann RA, Greger B, House P et al (2009) Toward the development of a cortically based
visual neuroprosthesis. J Neural Eng 6:035001
10. Bhandari R, Negi S, Rieth L et al (2008) A novel method of fabricating convoluted shaped
electrode arrays for neural and retinal prostheses. Sens Actuators, A Phys 145–146:123–130
11. Warren DJ, Normann RA (2005) Functional reorganization of primary visual cortex induced
by electrical stimulation in the cat. Vision Res 45:551–565
12. Mahowald MA, Mead C (1991) The silicon retina. Sci Am 264:76–82
13. Werblin F, Roska T, Chua LO (1995) The analogic cellular neural network as a bionic eye.
Int J Circ Theor Appl 23:541–69
628
A. Kusnyerik et al.

14. Humayun MS, Juan De, Jr E, Dagnelie G et al (1996) Visual perception elicited by electrical
stimulation of retina in blind humans. Arch Ophthalmol 114:40–46
15. Roska B, Werblin F (2001) Vertical interactions across ten parallel, stacked representations in
the mammalian retina. Nature 410:583–587
16. Balya D, Roska B, Roska T, Werblin F (2002) A CNN framework for modeling parallel
processing in a mammalian retina. Int J Circ Theor Appl 30:363–93
17. Chen J, Shah HA, Herbert C et al (2009) Extraction of a chronically implanted,
microfabricated, subretinal electrode array. Ophthalmic Res 42:128–137
18. Gekeler F, Kopp A, Sachs H et al (2010) Visualisation of active subretinal implants with
external connections by high-resolution CT. Br J Ophthalmol 94:843–847
19. Li L, Cao P, Sun M et al (2009) Intraorbital optic nerve stimulation with penetrating
electrodes: in vivo electrophysiology study in rabbits. Graefes Arch Clin Exp Ophthalmol
247:349–361
20. Pezaris JS, Reid RC (2007) Demonstration of artiﬁcial visual percepts generated through
thalamic microstimulation. Proc Natl Acad Sci USA 104:7670–7675
21. Pezaris JS, Reid RC (2009) Simulations of electrode placement for a thalamic visual
prosthesis. IEEE Trans Biomed Eng 56:172–178
22. Brindley GS, Lewin WS (1968) The sensations produced by electrical stimulation of the
visual cortex. J Physiol 196:479–493
23. Dobelle WH, Mladejovsky MG (1974) Phosphenes produced by electrical stimulation of
human occipital cortex, and their application to the development of a prosthesis for the blind.
J Physiol 243:553–576
24. Lewis PM, Ackland HM, Lowery AJ (2015) Restoration of vision in blind individuals using
bionic devices: a review with a focus on cortical visual prostheses. Brain Res 1595:51–73
25. Dobelle WH, Mladejovsky MG, Evans JR et al (1976) “Braille” reading by a blind volunteer
by visual cortex stimulation. Nature 259:111–112
26. Humayun M, de Juan E, Jr Dagnelie G (2016) The bionic eye: a quarter century of retinal
prosthesis research and development. Ophthalmology 123:S89–S97
27. Kusnyerik A, Greppmaier U, Wilke R (2012) Positioning of electronic subretinal implants in
blind retinitis pigmentosa patients through multimodal assessment of retinal structures. Invest
Ophthalmol Vis Sci 53:3748–3755
28. Kahán
Á,
Sipos
M
(1951)
Die
Feinstruktur
der
Maculagegend
im
Lichte
der
Funktionsprüfung von zentralen Tapetoretinal-Degenerationsfällen. Albrecht v Graefes
Arch Ophthal 151:476–499
29. Francois J (1977) A diffúz chorioretális heredodystrophiák, vagy perifériás tapetoretinalis
dystrophiák osztályozása. Újabb eredmények a szemészetben 1977; Országos Szemészeti
Intézet 2:7–13
30. Sahel J, Bonnel S, Mrejen S, Paques M (2010) Retinitis pigmentosa and other dystrophies.
Dev Ophthalmol 47:160–167
31. Janáky M (1989) Combined electroretinography and visual evoked potential for the
differential diagnosis of retinitis pigmentosa. Szemészet 126:203–208
32. Stingl K, Zrenner E (2013) Electronic approaches to restitute vision in patients with
neurodegenerative diseases of the retina. Ophthalmic Res 50:215–220
33. Tsai D, Morley JW, Suaning GJ, Lovell NH (2009) Direct activation of retinal ganglion cells
with subretinal stimulation. In: Proceedings of IEEE engineering in medicine and biology
society, pp 618–621
34. Shivdasani MN, Luu CD, Cicione R et al (2010) Evaluation of stimulus parameters and
electrode geometry for an effective suprachoroidal retinal prosthesis. J Neural Eng 7:036008
35. Chai X, Li L, Wu K et al (2008) C-sight visual prostheses for the blind. IEEE Eng Med Biol
Mag 27:20–28
36. Cai C, Li L, Li X et al (2009) Response properties of electrically evoked potential elicited by
multi-channel penetrative optic nerve stimulation in rabbits. Doc Ophthalmol 118:191–204
37. Shire DB, Kelly SK, Chen J et al (2009) Development and implantation of a minimally
invasive wireless subretinal neurostimulator. IEEE Trans Biomed Eng 56:2502–2511
Vision Restoration with Implants
629

38. Rizzo JF 3rd1, Shire DB, Kelly SK, Troyk P et al (2011) Development of the boston retinal
prosthesis. In: Conference proceedings of IEEE engineering in medicine and biology society,
3135–3138.
39. Kelly SK, Shire DB, Chen J et al (2009) Realization of a 15-channel, hermetically-encased
wireless subretinal prosthesis for the blind. In: Proceedings of IEEE engineering in medicine
and biology society, pp 200–203
40. Palanker D, Vankov A, Huie P, Baccus S (2005) Design of a high-resolution optoelectronic
retinal prosthesis. J Neural Eng 2:S105–S120
41. Lorach H, Goetz G, Smith R (2015) Photovoltaic restoration of sight with high visual acuity.
Nature Med 21:476–482
42. Butterwick A, Huie P, Jones BW et al (2009) Effect of shape and coating of a subretinal
prosthesis on its integration with the retina. Exp Eye Res 88:22–29
43. Lee SW, Seo JM, Ha S et al (2009) Development of microelectrode arrays for artiﬁcial retinal
implants using liquid crystal polymers. Invest Ophthalmol Vis Sci 50:5859–5866
44. Ryu SB, Ye JH, Lee JS et al (2009) Characterization of retinal ganglion cell activities evoked
by temporally patterned electrical stimulation for the development of stimulus encoding
strategies for retinal implants. Brain Res 1275:33–42
45. Nakauchi K, Fujikado T, Kanda H et al (2005) Transretinal electrical stimulation by an
intrascleral multichannel electrode array in rabbit eyes. Graefes Arch Clin Exp Ophthalmol
243:169–174
46. Nishida K, Kamei M, Kondo M et al (2010) Efﬁcacy of suprachoroidal-transretinal
stimulation in a rabbit model of retinal degeneration. Invest Ophthalmol Vis Sci 51:2263–
2268
47. Lin TC, Chang HM, Hsu CC (2015) Retinal prostheses in degenerative retinal diseases.
J Chin Med Assoc 78:501–505
48. Fujikado
T,
Kamei
M,
Sakaguchi
H
(2016)
One-year
outcome
of
49-channel
suprachoroidal-transretinal stimulation prosthesis in patients with advanced retinitis pigmen-
tosa. Invest Ophthalmol Vis Sci 57:6147–6157
49. Fujikado T et al (2011) Testing of semichronically implanted retinal prosthesis by
suprachoroidal-transretinal
stimulation
in
patients
with
retinitis
pigmentosa.
Invest
Ophthalmol Vis Sci 52:4726–4733
50. Ahuja AK, Dorn JD, Caspi A et al (2010) Blind subjects implanted with the Argus II retinal
prosthesis are able to improve performance in a spatial-motor task. Br J Ophthalmol 95:
539–543
51. Humayun MS, Dorn JD, Ahuja AK et al (2009) Preliminary 6 month results from the Argus II
epiretinal prosthesis feasibility study. In: Proceedings of IEEE engineering in medicine and
biology society, pp 4566–4568
52. Roessler G, Laube T, Brockmann C et al (2009) Implantation and explantation of a wireless
epiretinal retina implant device: observations during the EPIRET3 prospective clinical trial.
Invest Ophthalmol Vis Sci 50:3003–3008
53. Kelly SH, Shire D, Chen J et al (2011) A hermetic wireless subretinal neurostimulator for
vision prostheses. IEEE Trans Biomed Eng 11:3197–3205
54. Klauke S, Goertz M, Rein S et al (2010) Stimulation with a wireless intraocular epiretinal
implant elicits visual percepts in blind humans: results from stimulation tests during the
EPIRET3 prospective clinical trial. Invest Ophthalmol Vis Sci 52:449–455
55. Kusnyerik Á, Resch M, Csákány B et al (2010) Ultrasound reproducibility measurements in
deﬁning the dimensions of the human eyeball’s equatorial and axial length. Szemészet 147:5–12
56. Zrenner E, Bartz-Schmidt KU, Benav H et al (2011) Subretinal electronic chips allow blind
patients to read letters and combine them to words. Proc Biol Sci 1711:1489–1497
57. Stingl K, Bartz-Schmidt KU, Besch D et al (2013) Artiﬁcial vision with wirelessly powered
subretinal electronic implant alpha-IMS. Proc Biol Sci 280:1757
630
A. Kusnyerik et al.

Mobility, Inclusion and Exclusion
M.A. Hersh
1
Introduction: Travel by Blind People
There are an estimated 285 million visually impaired people globally, with 39
million of them blind [1]. Nearly 10% of the estimated 1.1 million blind people in
the USA use a long cane and just under 1% a guide dog [2]. Since these are the two
most commonly used mobility devices, this implies that most blind people in the
USA and probably the rest of the world do not use any mobility devices. A number
of surveys have found that a signiﬁcant minority of blind people do not leave the
home (on their own) and that sole travel by blind people is generally local or
restricted to known routes [3–6]. While these surveys are dated and some
improvement may have occurred subsequently, there is still an urgent need for
accessible environments and well-designed mobility aids to reduce the barriers to
independent travel by blind people.
Mobility requires the analysis of sensory information to determine a direction of
travel, avoid obstacles and walk in a (relatively) straight line. Unlike sighted people,
who rely largely on vision for spatial information, blind people use information
from all their senses with auditory and tactile information generally the most
important. However, many partially sighted people are largely dependent on vision
and people with progressive visual impairments may experience difﬁculties in
adapting from vision based mobility to mobility based on all their senses [7]. Blind
people use shore lines, such as kerbs, the edge of grass verges and walls, which are
detected by the (long) cane rather than vision, to walk in a straight line. They
generally require much greater concentration than sighted people when travelling
M.A. Hersh (&)
Biomedical Engineering, University of Glasgow, Glasgow G12 8LT, Scotland, UK
e-mail: marion.hersh@glasgow.ac.uk
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5_21
631

and need to memorise route information to make up for the lack of overview and
preview information available to them [7–9]. The need for memorisation and high
levels of concentration impose a heavy cognitive load, making travel tiring for blind
people. Additional information about the travel processes of blind people is pro-
vided in [10].
2
The Importance of Mobility for Participation in Society
In most industrialised/western countries car ownership and use has become the
main form of transport and travel patterns have changed. For instance, people in the
UK travel on average 10 times as far as they did 50 years ago, but it is average
journey length, rather than the number of trips and time involved that has increased
[11]. Increased car use has enabled the average person in many countries to carry
out signiﬁcantly more activities in a day and travel far greater distances than
50 years ago, but at a very high social and environmental price. It has led to
signiﬁcant disadvantage to people without car access and contributed to the further
exclusion of already disadvantaged individuals and groups [12]. This is due to the
changes in planning policy based on dispersed car-oriented development and an
associated reduction in the availability, affordability and acceptability of public
transport, as well as changes in the location of education, employment, healthcare,
leisure and shopping facilities. There has also been a signiﬁcant reduction in the
quality of local environments [12–14].
Transport and land use policies in the UK have been found to systemically create
and reinforce social exclusion [15] and the same is highly likely to be true else-
where. In particular, not having a car can be a signiﬁcant barrier to obtaining
employment and restrict job choice by making it more difﬁcult to travel to inter-
views and the workplace. It also makes it more difﬁcult to participate in post-16
education and training, attend hospital appointments, get to leisure facilities and
reasonably priced food shops and visit family and friends [14]. Therefore, many
non-car owners have reduced access to services, facilities and social networks [14].
This is particularly problematical for blind people who are rarely car drivers and
generally require public transport to go anywhere they cannot reach on foot. In rural
areas the lack of public transport can make access to most facilities, which are
generally located in the nearest town, very difﬁcult [16]. This combination of a lack
of transport and things to do locally can act as a disincentive to blind people in rural
areas going out on their own. As one of my research participants said ‘I do not go
about on my own. … It is a little village… there’s nothing there, not like a town
with shops’ [17]. Demographic changes with an ageing population also increase the
need for public transport to avoid further exclusion. Many people cease driving as
they get older. In addition there is a cohort of particularly older women who have
never driven and are likely to outlive any male partners who may be drivers.
Public transport is frequently (relatively) good in large cities, leading to reduced
car ownership. However, it is not necessarily fully accessible or easy to use by blind
632
M.A. Hersh

and other disabled people. Travel information is rarely fully accessible. For
instance, arrivals and departures boards in stations are frequently located too high
up (above eye level) to be read by partially sighted people. They do not have an
audio equivalent, for instance an audio announcement when a button on a pole in an
easily identiﬁable location is pressed. Although the use of audio bus stop
announcements is increasing, they are not universally available and not always
clear. Accessible information on the numbers and destinations of arriving buses is
rarely available. However, an audio announcement system operated by remote
control has been in use in the Czech Republic for a number of years [18]. Buses
frequently arrive in groups and do not stop long enough for blind people to ask the
driver or another passenger the bus number or destination [19]. Public transport
needs to be both accessible and available when people want to travel [20]. Even
when public transport availability is relative good during the day it is generally
much poorer at night, making it more difﬁcult for blind and other public transport
users to visit family and friends and participate in other social activities or work or
study at night.
In addition to the factors already discussed, including availability, service
location and cost, concerns about safety and security when travelling and unwill-
ingness or lack of conﬁdence to travel any distance from home can act as barriers to
the use of public transport [15]. Blind people experience various barriers to trav-
elling on their own and in some cases may ﬁnd it difﬁcult or lack the conﬁdence to
travel even relatively short distances. In addition to normal safety concerns, there is
also some risk of hate crimes or other forms of violence. My research shows some
evidence of targeting of cane users [17] and particular safety concerns by blind
women [19].
2.1
Approaches to Improving Public Transport
Measures are required to improve public transport, both in general and for speciﬁc
groups, such as blind people. This could include prescribed minimum accessibility
levels, such as the percentage of people in an area who live within a 10 minute walk
of an hourly or better bus service [21]. However, although this could lead to
improvements, some people would still remain excluded, including people for
whom a 10 minute walk is a barrier. In addition, minimum levels easily become
maxima. Accessibility planning has some potential, but should be accompanied by
measures to restrain and signiﬁcantly reduce exponential growth in car travel [12].
Changes in the way in which public transport is ﬁnanced and paid for can also
have an impact. I would suggest that it should be treated as a public service ﬁnanced
totally from taxation and free at the point of use rather than as a commodity to be
paid for by users. This would have a number of advantages. In particular, it would
remove the barrier of frequently high cost to the use of public transport and allow
more equal or at least considerably improved provision, particularly in rural areas,
where subsidies would otherwise be required to allow fare reductions. Free public
Mobility, Inclusion and Exclusion
633

transport could also have a positive environmental impact, by encouraging a modal
shift from private car use to public transport. However, it may need to be combined
with a public relations campaign to improve the image of public transport.
Improvements in quality and comfort and, in some cases, punctuality and inte-
gration between different types of public transport will also be required.
There have been a number of initiatives to improve public transport, particularly
for members of marginalised groups, and most of them seem to have had a positive
impact on users by increasing access to services to some extent. However, they
have generally had local rather than national coverage and not always included
sufﬁcient features [22]. They have included personal training schemes, dial-a-ride
and community transport schemes for disabled and older people of rural areas,
improved public transport and walking and cycling routes.
It can be useful to have a framework for the improvement of public transport for
blind and other groups of disabled people. One possible approach is based on eight
different concepts that have been identiﬁed in the literature on geographical
equality, justice and fairness and which can be divided into the two groups of
justice, equity and equality concepts and deserts, rights and needs [23]. In the case
of access to transport and facilities for blind and other groups of disabled people,
rights, needs and ‘substantive equality’ of outcomes are probably the most useful
concepts. A framework based on rights, needs and outcomes is particularly useful,
as it implies that blind people are entitled to be able to access public transport and
other facilities that meet their needs and that this will require speciﬁc consideration
of what these needs are and what is required to ensure they are met. It also avoids
marginalisation by providing a context in which blind people are one of a number
of groups who require speciﬁc provisions for their needs to be appropriately met
and implies that a tick box approach is not sufﬁcient.
2.2
The Private Car—Glamour and Exclusion
Various factors inﬂuence the value of a particular possession, including its use-
fulness, the associated enjoyment, its symbolisation of social relationships and its
role in expressing and/or reinforcing identity and self-expression [24]. Several
studies show that that people buy and drive cars as a source of pleasure rather than
purely in response to need [25]. Car use is evaluated positively on instrumental,
symbolic and affective features, whereas public transport is judged much less
positively [26, 27] and even survey respondents who rarely used their car have been
found to prefer it to public transport on most features [27]. The level of car use and,
in particular, car commuting has been found to be generally determined by sym-
bolic and affective rather than instrumental motives [28], with the car being con-
sidered a status symbol and means of expression by many people [29]. Frequent
drivers have been found to evaluate the symbolic, but not the affective function of
car use more favourably than infrequent drivers [28].
634
M.A. Hersh

However, large scale car use also causes serious problems. The impact on
development planning and reducing the availability and quality of public transport
has already been discussed. Globally it is a major energy consumer and contributes
to global warning whereas locally it causes air pollution and health problems.
Noise, trafﬁc accidents and land encroachment have reduced the quality of life in
urban areas [29]. Land encroachment, due to, for instance a car requiring 24 times
as much road space per person as a bus [30], has led to a reduction in the
historical/cultural, aesthetic and restorative qualities of urban areas, leading to a
reduced quality of life. The associated congestion has reduced accessibility to
various destinations [29] for non-car users as well as car users. The prevalence of
affective and symbolic over instrumental motives in car use shows both that car use
is by no means essential and the likely opposition and difﬁculties in reducing it.
In the case of blind people with progressive visual impairments giving up
driving is a marker of increasing visual impairment which is strongly resisted.
According to one of my research participants ‘I had normal vision for a long time, I
drove a car. I received a diagnosis, but you resist it,… you continue driving, though
with difﬁculty…. I had a small accident and then I stopped driving’. For blind
people the issue is probably as much the symbolic and emotional signiﬁcance of
being able to drive as its practical usefulness. Thus giving up driving when changes
in vision make it no longer safe is not easy, since it requires acceptance of disability
and change of status to a person who is no longer able to drive.
3
Attitudes to Blind People
There has been some work on the development of attitudes to and social respon-
sibility about blindness scales, e.g. [31–33]. However, even the existence of such
scales is an indicator of a potential problem or the fact that blind people are a
minority group that requires particular consideration. There do not seem to be any
attitudes to or social responsibility to sightedness scales, probably because sight-
edness is seen as the ‘norm’, making consideration of attitudes to it apparently
unnecessary. Positive attitudes to oneself and one’s identity groups are generally
beneﬁcial and can have a positive impact on learning, whereas negative attitudes
can have the opposite effect. This is a particular issue in the case of blind people,
where stigma about blindness and the (long) cane can affect cane use [17].
Studies have found that blind people have more positive attitudes to blindness
than sighted people and women have slightly more positive attitudes than men,
though the gender difference may not be large enough to be meaningful [33, 34]. In
addition, previous contact with blind people has generally been found to improve
attitudes [34]. Thus, personal knowledge may be sufﬁcient to overcome stereotypes
and misconceptions. However, this effect has been found to be slightly reduced for
people with a blind person in their family, possibly as a result of contact with
relatives who became blind later in life, but did not become independent as a blind
person [34], but this would need to be investigated.
Mobility, Inclusion and Exclusion
635

3.1
Stigma and Stereotypes
The term stigma may originate in the abusive practice of the ancient Greeks of
cutting, burning or branding slaves, traitors and offenders [35]. However, there is
still not a generally accepted deﬁnition. Work by Goffman [35] was inﬂuential, but
has been extensively criticised for its outdated and inappropriate concepts which are
incompatible with multicultural and pluralist societies [36]. A fairly comprehensive
recent deﬁnition [37] involves distinguishing and labelling salient differences, e.g.
gender and disability which lead to negative attributes or stereotypes, the separation
of dominant ingroups (‘us’) and stigmatised outgroups (‘them’) and consequent
status loss and discrimination. Both individual and group discrimination occur [38].
Stigma can act at the level of large groups (social), structural systems and
individuals (internalised) [38]. The endorsement of stigma or stereotypes by large
groups can lead to the use of stigmatising labels, ostracism, discrimination and
violence. This stigma can also affect the friends, family and close associates of
stigmatised individuals, in this case called courtesy stigma [35]. Structural stigma
results from the rules, policies or procedures of organisations which result in dis-
crimination and disadvantage for stigmatised groups, e.g. institutionalised sexism,
racism and ablism. Internalised stigma can result in reduced self-esteem,
self-efﬁcacy, empowerment and morale and a lower quality of life [39]. It can lead
to individuals restricting their own opportunities and life chances by not, for
instance, applying for a particular job due to stereotypical beliefs about their (lack
of) abilities as members of a particular group. As will be discussed later, it can also
lead to rejection of assistive devices, such as the (long) cane [38], with a resulting
reduction in travel safety and/or opportunities. Differences in power are an
important, but neglected, feature of stigma, with stigma generally affecting only
relatively powerless groups [38], such as blind people. Stigma can also increase
existing inequalities resulting from factors such as gender and income [40]. It is a
consequence of group membership, but individual members of stigmatised groups
may have very different experiences and use very different strategies to deal with it.
Like many social phenomena, stigma is a construct rather than something innate.
Consequently, different societies frequently have different attitudes to stigmatised
groups, including disabled people, and assistive devices. However, cross-cultural
comparisons have generally focused on the attitudes of non-disabled people [41–
43] or the experiences, including of courtesy stigma, of the parents of disabled
children [44, 45] rather than those of disabled people.
The identiﬁcation of patterns and the use of categorisations [46] can be a very
helpful mental tactic for organising information, recognising similarities and
reducing cognitive load. Unfortunately, these approaches can also lead to stereo-
types [47], which may act as self-fulﬁlling prophecies. Other causes of stereotyping
are discussed in [48]. Even apparently positive stereotypes, related for instance to
the stereotypical talents of a particular group, can be damaging and should be
avoided. They are based on false assumptions of greater homogeneity of minority
than majority groups and can reduce the opportunities available to members of
636
M.A. Hersh

these groups, for instance in employment through stereotypical assumptions about
their abilities and what jobs are suitable for them. In addition, stereotyping easily
leads to minority groups being perceived as ‘other’ and acts as a barrier to inclusion
[47]. There are probably individuals, both blind and sighted, who meet particular
stereotypes. However, this is not the issue. The problem is rather assuming that
particular stereotypes, whether positive or negative, are valid for the (overwhelm-
ing) majority of blind people.
Stereotype threat involves concerns about negative perceptions resulting from
group membership. It has been found to lead to reductions in short-term perfor-
mance. Long-term stereotype threat can lead to reduced feelings of self-integrity
and adaptive adequacy and may result in reduced motivation, performance and
well-being [49]. Fear of stereotyping or stereotype threat can lead to avoidance of
situations, such as job applications or learning to use a long cane, in which
stereotyping is likely to occur in order to preserve a self-image of adaptive ade-
quacy. This has been found to reduce well-being, employment and challenge
seeking and increase global stress. However, there is also some evidence that value
afﬁrmation can reduce the impacts of stereotype threat [49].
Older surveys e.g. [50] indicate three main stereotypes of blind people, namely
that they deserve sympathy and pity, are either totally incapable or have extraor-
dinary non-visual senses and all share similar interests, as well as indications of the
effectiveness of educational interventions in changing these stereotypes. However,
further research will be required to investigate whether these stereotypes are still
current and any changes in the types and prevalence of stereotypes over time.
The representation of blind and other disabled people in literature and the arts is
indicative of attitudes to them. Unfortunately stereotypes of dependency, help-
lessness and infantalisation and the appropriation of the blind gaze to maintain the
dominant position of sighted people are common, for instance in the representa-
tionsof blind women in the cinema [51]. Some English language literature has
apparently positive stereotypes based on the concept of ‘beneﬁcial blindness’.
However, this can be just as damaging as more obviously negative stereotypes.
Stereotypes of ‘beneﬁcial blindness’ diminish the achievements and talents of blind
people [52] and trivialise the often very serious barriers they have to overcome.
‘Positive’ stereotypes of blindness include musical genius, ‘effortless success’
and possessing extraordinarily sensitive non-visual senses [52]. The stereotype of
effortless success is based on the assumption of an increased ability to concentrate
due to the lack of (visual) distractions. This implies that vision is the only important
sense and downgrades the value and interest of information from the other senses.
While independent travel (with a cane and other travel aids) does require very high
levels of concentration, there is no evidence that blind people have a greater ability
to concentrate than sighted people in general. Research indicates that the senses of
blind and sighted people are comparable [53] and that blind people who use their
other senses more effectively do so as the result of practice rather than automatically
[54]. Thus, beliefs that blind people have extraordinary senses are purely stereo-
types with no basis in reality. There are also a number of negative stereotypes in
ﬁction, including a confusion of dreaming and waking and a lack of knowledge
Mobility, Inclusion and Exclusion
637

about the world about them [52]. These stereotypes are in direct contradiction to the
‘positive’ stereotypes of a superior ability to concentrate and non-visual senses.
This further illustrates the total lack of logic and connection to reality of
stereotypes.
3.2
Acceptance and Conﬁdence
Calls have been made, e.g. [55] for the acceptance of diverse ways of doing things
in public spaces. This would be relevant to ethnic and other minority groups, as
well as disabled people, since it would cover the use of different languages, dialects,
greetings and other rituals, as well as types of dress and food. It would also lead to
recognition that there are different types of mobility and lead to equal indifference
to or acceptance of, for instance, long cane and GPS use and mobility scooters and
electric cars. In practice, we are a long way from this situation of equal acceptance
of mobility with and without assistive devices and indifference to whether or not
people use assistive devices to support travel. This would require a very signiﬁcant
change in attitudes to disability and disabled people.
The potential additional accomplishments of disabled people if they did not have
to cope with negative attitudes, inaccessible environments and feeling that they are
‘on approval’ have been noted [55]. In particular, for blind people the ability to use
mobility devices without having to worry about other people’s attitudes and people
‘staring’ [17] could be both liberating and signiﬁcantly increase their freedom of
movement and social participation. However, this would not remove the need for
design to take into account device appearance. Attractive device appearance,
compact size and weight and easy portability are important design features in
themselves and not just a way to reduce the stigma associated with device use.
A lack of conﬁdence has already been indicated as one of the barriers to travel
using public transport. Some of my research participants lacked the conﬁdence to
travel on their own with a cane and preferred to use a sighted guide. Travelling on
their own was a particular issue for deafblind people who face additional barriers
due to communication difﬁculties [19]. There may also be a gender dimension to
the lack of conﬁdence. While by no means the most frequent occurrence, my Italian
respondents included a few blind and/or partially sighted couples where the blind or
partially sighted woman did not use a cane, but her male partner did and served as
her guide. This included partially sighted women with blind partners, in which case
cane use resulted in the male partners having a much greater degree of independent
mobility despite their greater visual impairments [56].
The role of cane use in increasing independence and opportunities together with
the desire for independence and participation can act as important motivating
factors to overcome the barriers to (effective) cane use. The limited availability of
sighted guides may also force people to accept the cane, though the comments of a
number of my research participants indicate that they found being accompanied less
638
M.A. Hersh

stigmatising. Support, including from other blind people and instructors, was found
to be very helpful in overcoming the barriers to cane use [17].
4
Stigma and the Use of Mobility Technology
Since the long cane is the most commonly used mobility device it will be the focus
of this section. The cane is commonly used as a symbolic representation of
blindness and blind people [57]. Since blindness is unfortunately still a stigmatised
identity this makes the cane a ‘visible stigma’ of blindness [58]. This identiﬁcation
can become a reason not to use the cane, as it forces users to confront and come to
terms with the fact that they are blind or partially sighted [38], and consequently
people who do not (fully) accept their visual impairment may delay or never even
start using a cane. As stated by one of my respondents who became blind in later
life ‘the cane is a symbol that I am blind and cannot hide it any more’. This
symbolic identiﬁcation and ‘visual stigma’ may be largely responsible for the
self-consciousness, worries about being stared at and consequent aversion to cane
use reported by many of my interviewees [17].
The symbolic identiﬁcation of the cane with blindness means that the cane may
be seen as only for totally blind people, leading to some partially sighted people,
newly blind people and people with progressive conditions ﬁnding it difﬁcult to
accept or delaying cane use [58]. For instance, one of my respondents ‘delayed a lot
in using a cane as, as long as I had some vision, … I could get about normally’ [17].
One of my research participants who used several assistive devices considered the
long cane the most difﬁcult to accept. ‘It’s difﬁcult to accept because it has to work
with the invisible signs of disability … You actually have to accept the new visual
images of yourself. You have to accept your mirror image and integrate it into
yourself’ [38].
The long cane is intended to be easily visible and to attract attention in order to
alert pedestrians and road users to the fact that the user is blind or partially sighted
and encourage them to be particularly careful around the user, as well as making it
easier for the user to obtain assistance when required. However, this visibility
frustrates the desires of many potential users to be inconspicuous and not draw
attention to their visual impairment. Despite recognition in the disability literature
of this value of interdependence, e.g. [59] and the role of autonomy or making
choices and having control over one’s life [60, 61], the need to be independent in
the sense of doing everything oneself is still predominant in western industrialised
societies. In practice, almost everyone in modern industrialised societies is
dependent on other people and very few, if any, people are totally self-sufﬁcient.
However, the persistence of the value judgement can make it difﬁcult to accept the
need for assistance. In the words of one of my interviewees ‘It’s difﬁcult, possibly
because [using a cane] makes it apparent to other people that you need help’ [17].
Mobility, Inclusion and Exclusion
639

Shame, embarrassment and not wanting to ‘stand out’ and ‘be marked as the
blind person’ act as particular barriers to starting cane use [17, 62]. Research
participants with progressive visual impairments or who became blind later in life
were often concerned about what people who knew them before they started using a
cane would think or found that other people experienced difﬁculties in accepting
that they really were blind [17]. In some cases they even travelled away from home
to learn to use the cane or found it easier to use it in areas where they were not
known. Delays in starting cane use as a result of these concerns were relatively
common. In a number of cases it took one or more accidents to convince blind and
partially sighted people of the need to use a cane for their own safety [17].
Feelings of shame and embarrassment may also lead to attempts to use a cane
while trying to hide it, for instance by having the cane folded or using a short cane,
making cane use either less effective or totally ineffective [17]. Some blind people,
for instance those with night blindness, do not need a cane all the time. However,
rather than taking a folded cane with them so they could use it when necessary, they
frequently did not use it at all, even when it was deﬁnitely necessary. In some cases
it is friends and relatives who are ashamed of the cane [17], probably due to the
negative views of disability and disabled people that still have not been eradicated
[63] and possibly due to fears of courtesy stigma [35]. Consequently some regular
cane users do not use a cane when visiting these friends and relatives [17].
However, support and encouragement from family and friends can be important in
overcoming the barriers to cane use and supporting blind people through the
extended training and practice required to learn to use a (long) cane effectively and
safely. On the other hand, active discouragement and disapproval can have the
opposite effect and negative attitudes, lack of acceptance and stereotypes generally
reduce conﬁdence, self-image and self-esteem [38].
It is only relatively recently that attention has been given to the appearance of
assistive devices [64], though this is important to end-users, who want devices to be
elegant and attractive [65, 66]. In some cases end-users may have been expected to
be grateful for the availability of any devices regardless of whether they are stig-
matising [64]. The fact that blind people sometimes need sighted friends to point
out that devices are not attractive [62] is not a valid reason for unattractive device
design. Functionality and appearance need to be appropriately combined [67] to
produce attractive, effective and high performance mobility (and other) devices with
functions that meet users’ needs. Unsurprisingly blind people have been found to
frequently share the general preference for small easily portable devices, such as
mobile and smart phones, rather than larger, more awkward and less easily portable
devices. Thus, mobility apps on mobile and smart phones are popular with many
blind and partially sighted people and have the advantages of looking like devices
that everyone uses and therefore being non-stigmatising, in addition to easy
portability. Device design should enable similar items or features to be easily
distinguished to improve usability [68]. However, designing a mobility app which
is able to provide basic obstacle avoidance features may be difﬁcult.
640
M.A. Hersh

5
Accessible Environments
Appropriate design and the provision of navigation and information systems can
considerably improve the accessibility of urban environments, generally with
considerable beneﬁts for both non-disabled people and blind and other disabled
people. Designers, planners and architects should be educated in the principles of
accessible environmental design [69]. They should also regularly consult with
(organisations of) blind, partially sighted, deafblind and other disabled people and
involve them in any proposals for environmental modiﬁcations.
Geographical data is very helpful in supporting mobility. Geographical infor-
mation systems (GIS) are generally accepted to be an important tool for storing,
manipulating, representing and analysing spatial information. GIS software has
enabled sighted people to view spatial data and made it easier to understand [70].
However, the visual formats frequently used to represent this data make it inac-
cessible to blind people, giving a need for accessible versions.
The following principles of accessible design are based on [69].
Lighting and surfaces
1. Good illumination of all public and pedestrian areas, generally using diffuse
rather than direct lighting and taking care to avoid shadows. As far as possible, a
choice of lighting types and the facility to regulate lighting levels should be
provided in all rooms in public buildings.
2. Matte surfaces to reduce glare.
Signage and information systems
3. Well-designed signage systems, covering both directional information, e.g. to
the nearest station or toilets and location information, e.g. a sign on the door of a
particular room or public building. Whenever feasible, the information should
be given in tactile, visual and audio form.
4. Tactile paving to warn of hazards and direct people to facilities.
5. Colour contrasts, including colour contrasting strips, to provide information and
make it easier for blind and visually impaired people to distinguish, for instance,
door handles and furniture.
Safety
6. Wide pathways which can accommodate wheelchair users and blind people
with guide dogs, a sighted guide or a guide-communicator in the case of
deafblind people.
7. Good layout design so that facilities, such as benches, litter bins and furniture,
do not become potentially hazardous obstacles.
8. Regular maintenance programmes to prevent avoidable hazards such as holes in
the road, due to the poor state of building and streetscape repair.
9. Two supports for all signs on the pavement to facilitate detection with a cane
and reduce the risk of accidents to (deaf)blind people. Care should be taken to
Mobility, Inclusion and Exclusion
641

ensure that signs do not block the pavement and temporary signs should
generally be put at the side of the road rather than on the pavement.
10. Appropriate location of bus stops, preferably widening the pavement if nec-
essary, to provide sufﬁcient space for wheelchair users and (deaf)blind people
with guide dogs, a sighted guide or a guide-communicator.
11. Frequent and appropriately located cycle parking to avoid bikes being chained
to lamp posts or the poles of accessible crossing indicators/trafﬁc lights and
becoming a potential hazard.
Public awareness
12. Publicity campaigns to increasing awareness of the needs of disabled pedes-
trians, including to reduce street furniture, particularly on (relatively) narrow
pavements, and their ability and right to travel on their own.
5.1
Public Transport
(Deaf)blind people require accessible and widely available public transport to travel
outside the areas they can reach on foot [38]. Public transport therefore needs to
become more widely available in regards to both times and location and also more
accessible. This will require increasing the frequency of all forms of public trans-
port, extending coverage to villages and areas of towns with only limited services
and extending the hours of service into the night and early hours of the morning.
Access to information is an important feature of accessibility. This should include
both audio announcements and tactile information of the approaching bus stop and
the number and destination of approaching buses.
Audio announcements should be clear, pleasantly modulated and an appropriate
volume. They need to be sufﬁciently loud to be easily heard, but not excessively
loud to avoid causing irritation. In the case of stations with a two word name both
need to be pronounced clearly rather than the ﬁrst word clearly enunciated and the
second muttered. However, there is still the question as to whether the announce-
ments should be always on or operated by a mobile or smart phone app or remote
control. Constantly on announcements would be (potentially) available to all
travellers, but could irritate the driver and autistic and some other travellers.
Announcements turned on by an app or remote control would avoid noise distur-
bance and the driver turning the announcements off, but would require users to have
the app or remote control.
Existing systems, for instance in the Czech Republic, for obtaining an audio
announcement of the bus number and destination through the use of a remote
control should become generally available. However, some improvements in voice
quality and volume control will be required, as well as the option for receiving the
information via an ear piece. In addition, technological developments mean that
apps on smart and mobile phones are now more appropriate for many, though not
642
M.A. Hersh

all potential users. Other strategies which should become better known and gen-
erally accepted include the use of plastic cards with the bus number as an indication
for the bus driver to stop, with colour coding to indicate whether the person is blind
or deafblind. Requiring all buses to stop brieﬂy at the head of the queue, particu-
larly when several arrive together, would give (deaf)blind people time to obtain
information to identify the number and enable all slower moving users to get to the
bus and get on in time.
Another important issue is training. All transport personnel should have dis-
ability equality training (organised and provided by disabled people) as well as
training in interacting with and meeting the needs of particular groups of disabled
people. This training should also be updated on a regular basis.
5.2
Accessible Pedestrian Crossings
Crossing the road safely is an important issue for all pedestrians and raises par-
ticular issues for several groups of pedestrians, including (deaf)blind and some
other disabled people, people who move slowly and children. The widespread use
of accessible pedestrian crossings could resolve many of the crossing problems
experienced by (deaf)blind people. Major roads can act as a barrier to unaccom-
panied travel by blind people and to an even greater extent to deafblind people: the
strategies commonly used by blind people of using their hearing to decide when to
cross, asking for assistance and crossing with other people may not be feasible for
deafblind people The availability of accessible crossing indicators is essential to
overcome these barriers and support unaccompanied travel by (deaf)blind people. It
also has the further advantage of enabling (deaf)blind people to cross without
requiring assistance, which is of value both in itself and because there may not be
other people available to provide it.
Deafblind people require vibro-tactile crossing indicators, whereas blind people
can use either audio or vibro-tactile indicators, but tend to prefer audio signals.
Trafﬁc crossing indicators (trafﬁc lights) with audio signals are more common than
those with tactile signals, though neither is particularly widely available. There is
considerable variation in the design of audio signals. This includes the types of
audio signals used for the crossing and waiting phases, whether the signal is
continuous or initiated or made louder by a remote control and whether it is located
on poles on one or both sides of the road or on a small loudspeaker on the remote
control. In the vibro-tactile case, vibration or other movement is used to indicate the
crossing phase of the signal, but there are considerable differences in the design and
location of the vibrating or other moving component. The differences n audio signal
used could result in real danger to users, particularly as the same signal may be used
to indicate the waiting and crossing phases on different designs.
In the case of deafblind people research [19] indicates the need of an interna-
tional standard for accessible crossing points. In addition to a standard design of
vibro-tactile crossing indicators, they should also include tactile arrows to facilitate
Mobility, Inclusion and Exclusion
643

alignment with the crossing direction and tactile tiles to indicate the best straight
path across the road. In the case of blind people further research is required to
develop a standard for accessible audio signals and incorporate it into the accessible
crossing standard. Analogous considerations as in the case of bus stop announce-
ments hold with regard to whether the standard should involve constantly on signals
or signals operated or made louder by an app or remote control. The fact that
different crossing and waiting audio signal are already in use in different places will
complicate the decision on a standard and its acceptance and incorporation into all
crossings. Building a considerable number of additional crossings to this standard
and upgrading existing crossings on a phased basis would improve safety for deaf
(blind) people and other groups, including children, and facilitate travel to other
cities and countries by (deaf)blind people.
Roundabouts should as far as possible be replaced by road junctions with
crossing points with standardised accessible pedestrian signals. Changing regula-
tions, where necessary, to forbid turning on the crossing phase of crossing indi-
cators would increase safety for both blind and many groups of disabled people and
people from countries where this is not allowed [19].
6
Discussion and Conclusions
Everyone, whether disabled or not, should have an equal right to participate in and
contribute to society in all its aspects. Development is increasingly based on the
needs of car users rather than pedestrians and public transport users. This has led to
increasingly dispersed developments and frequently the need to travel longer dis-
tances in order to access employment, education, leisure and other facilities, while
in many areas public transport has reduced. Even where public transport is rela-
tively good, it is often not fully accessible to blind and other groups of disabled
people. In the case of blind people one of the main barriers is the lack of fully
accessible information. Public transport availability is particularly limited in rural
areas, but most facilities are located in towns which may therefore be difﬁcult to
impossible to reach without a private car. The solutions need to cover both
increased availability and quality of public transport with regard to routes and times
covered and accessible information. Some of the potential solutions have been
discussed in the chapter.
Apps on mobile and smart phones have particular potential to provide accessible
information to blind and partially sighted people and to some extent also to deaf-
blind people. Mobility support apps are equally relevant to disabled and
non-disabled people, particularly in unfamiliar, complex and unstructured envi-
ronments. However, obstacle avoidance functions are particularly useful to blind,
partially sighted and some other groups of disabled travellers, whereas personalised
and contextual travel advice and information, route planning, navigation and
wayﬁnding, locating facilities and travel timetables are equally relevant to all
travellers. Blind people frequently require more detailed and additional information
644
M.A. Hersh

compared to non-disabled travellers to enable them to travel safely and effectively.
This could usefully include the location of bus stops and pedestrian crossings, as
well as time sensitive and other contextual features such as roadworks and when
cafes are open and likely to have tables on the pavement.
Mobility devices both have an important role in supporting mobility and are a
potential source of stigma and attention to the user as a disabled person. Particular
issues are raised by the (long) cane which, as a symbol of blindness confronts the
user with their visual impairment. This forces them to either ‘integrate it into
yourself’ in the words of one of my research participants [38] or leads to rejection
and non-use, at least initially. Many blind and partially sighted people prefer to
travel accompanied than use a long cane, due to varying combinations of shame and
embarrassment about cane use and concerns about the difﬁculties and danger
involved in unaccompanied travel. However, the downside of this is a degree of
dependence on other people and some restrictions on mobility, since another person
is not always available to accompany them. In other cases accidents force blind and
partially sighted people trying to travel on their own without a cane to accept it in
the interests of safety.
Improved device design to be attractive, unobtrusive and resemble devices used
by non-disabled people can resolve some of the problems. However, the long cane
in particular has the supplementary function of marking the user as a blind person,
thereby facilitating them obtaining Assistance and encouraging other people to take
particular care round them. A totally unobtrusive device would be unable to fulﬁl
this function. There is thus a need to challenge and overcome stereotypes and
negative attitudes to blind people both because of the value of doing this in itself
and to overcome barriers to using mobility devices which can be ‘positively lib-
erating’ due to the removal of dependence and the need to be accompanied
everywhere [71].
References
1. Pascolini D, Mariotti SP (2011) Global estimates of visual impairment: 2010. Br J
Ophthalmol: bjophthalmol-2011
2. Roentgen UR, Gelderblom GJ, Soede M, de Witte LP (2008) Inventory of electronic mobility
aids for persons with visual impairments: a literature review. J Visual Impairment Blindness
102(11):702
3. Bruce I, McKennell AC, Walker EC (1991) Blind and partially sighted adults in Britain: the
RNIB survey (vol 1, p 120). HM Stationery Ofﬁce, UK
4. Clark-Carter DD, Heyes AD, Howarth CI (1986) The efﬁciency and walking speed of visually
impaired people. Ergonomics 29(6):779–789
5. Golledge RG (1993). Geography and the disabled: a survey with special reference to vision
impaired and blind populations. Transac Inst Br Geogr, 63–85
6. Hill EW, Rieser JJ, Hill MM, Hill M (1993) How persons with visual impairments explore
novel spaces: strategies of good and poor performers. J Visual Impairment Blindness
7. Hersh MA (2009) Designing assistive technology to support independent travel for blind and
visually impaired people CVHI ’09. Wrocław, Poland
Mobility, Inclusion and Exclusion
645

8. Hersh
MA
(2016a)
Travel
and
information
processing
by
blind
people:
a
new
three-component model, Biomedical Engineering, University of Glasgow Report. http://
web.eng.gla.ac.uk/assistive/pages/publications.php
9. Pissaloux E (2013) Visually impaired mobility and ICT supports. In: IEEE signal processing:
algorithms, architectures, arrangements, and applications (SPA). ISSN: 2326–0262
10. Hersh MA (2016) Improving deafblind travelers’ experiences: an international survey.
J Travel Res 55(3):380–394
11. Foster C (2002) Motoring towards 2050. RAC Foundation, UK
12. Lucas K (2006) Providing transport for social inclusion within a framework for environmental
justice in the UK. Transp Res Part A: Policy Pract 40(10):801–809
13. DETR (2000) Social exclusion and the provision and availability of public transport. DETR,
London
14. Kenyon S, Lyons G, Rafferty J (2002) Transport and social exclusion: investigating the
possibility of promoting inclusion through virtual mobility. J Transp Geogr 10(3):207–219
15. SEU (2003) Making the connections: transport and social exclusion. Social Exclusion Unit,
The Stationery Ofﬁce, London
16. CA/YF
(2002)
Bentham
moving
on—An
action
plan
for
Bentham.
Countryside
Agency/Yorkshire Forward
17. Hersh MA (2015) Cane use and late onset visual impairment. Technol Disabil 27(3):103–116
18. Hersh MA (2009b) The application of information and other technologies to improve the
mobility of blind, visually impaired and deaf blind people. In: Antalya T, Mihalaş G et. al
(eds.) Travel health informatics and telehealth, selected papers from efmi special topic
conference. Victor Babes University Publishing House, Romania, pp 11–24
19. Hersh MA (2016b) Mobility technologies for blind, partially sighted and deafblind people:
design issues
20. Kwan MP (2013) Beyond space (as we knew it): toward temporally integrated geographies of
segregation, health, and accessibility: space–time integration in geography and GIScience.
Ann Assoc Am Geogr 103(5):1078–1086
21. Farrington J, Farrington C (2005) Rural accessibility, social inclusion and social justice:
towards conceptualisation. J Transp Geogr 13(1):1–12
22. Kilby K, Smith N (2012) Accessibility planning policy: evaluation and future directions. Final
Report, DfT
23. Hay AM (1995) Concepts of equity, fairness and justice in geographical studies. Transac Inst
Br Geogr, 500–508
24. Richins ML (1994) Valuing things: the public and private meanings of possessions. J Consum
Res, 504–521
25. Sandqvist K, Kriström S (2001) Getting along without a family car: the role of an automobile
in adolescents’ experiences and attitudes. Inner city Stockholm, Lärarhögskolan
26. Jensen M (1999) Passion and heart in transport—a sociological analysis on transport
behaviour. Transp Policy 6(1):19–33
27. Steg L (2003) Can public transport compete with the private car? Iatss Res 27(2):27–35
28. Steg L (2005) Car use: lust and must. Instrumental, symbolic and affective motives for car
use. Transport Res Part A: Policy Pract 39(2):147–162
29. Gärling T, Schuitema G (2007) Travel demand management targeting reduced private car use:
effectiveness, public acceptability and political feasibility. J Social Issues 63(1):139–153
30. Servant L (1996) L’automobile dans la ville. Cahiers du IAURIF, (114)
31. Bell E, Silverman A (2011) Psychometric investigation of the social responsibility about
blindness scale (SRBS). J Blindness, Innov Res 1(2)
32. Cowen EL, Underberg RP, Verrillo RT (1958) The development and testing of an attitude to
blindness scale. J Social Psychol 48(2):297–304
33. Rowland MP, Bell EC (2012) Measuring the attitudes of sighted college students toward
blindness. J Blindness Innov Res 2(2)
34. Seo WS, Chen RK (2009) Attitudes of college students toward people with disabilities. J Appl
Rehabil Couns 40(4):3
646
M.A. Hersh

35. Goffman E (1963) Stigma, notes on the management of spoiled identity. Simon and Schuster,
New York
36. Weiss MG, Ramakrishna J, Somma D (2006) Health-related stigma: rethinking concepts and
interventions. Psychol, Health Med 11(3):277–287
37. Link BC, Phelan JC (2001) Conceptualizing stigma. Annu Rev Socio 27:363–385
38. Hersh MA (2014) Deafblind People, Stigma and the use of communication and mobility
assistive devices. Technol Disabil 25(4):245–261
39. Brohan E, Elgie R, Sartorius N, Thornicroft G (2010) Schizophr Res 122:232–238
40. De Choudhury M, Feldman M, Amer-Yahia S, Golbandi N, Lempel R, Yu C (2010, Crocker J
(1999). Social stigma and self-esteem: situation construction of self-worth. J Exp Social Psych
35:89–107
41. Jaques M, Linkowski D, Sieka F (1970) Cultural attitudes toward disability: Denmark, Greece
and the US. Int J Social Psychiatry 15:54–62
42. Saetermoe CL, Scattone D, Kim DH (2001) Ethnicity and the stigma of disabilities. Psychol
Health 16:699–713
43. Westbrook MT, Legge V, Pennay M (1993) Attitudes towards disabilities in a multicultural
society. Soc Sci Med 36(5):615–623
44. Florian V, Katz S (1983) The impact of cultural, ethnic and national variables on attitudes
towards the disabled in Israel: a review. Int J Intercult Relat 7:167–179
45. Mardiros M (1989) Conception of childhood disabilities among Mexican-American parents.
Med Anthropol 12:55–68
46. Shermer M (2003) How we believe: science, scepticism and the search for god. Owl Books,
New York
47. Maynard DC, Ferdman BM, Holmes TR (2010) Mobility and inclusion. In: The psychology
of global mobility. Springer, New York, pp 211–233
48. Hersh MA (2013) Barriers to ethical behaviour and stability: stereotyping and scapegoating as
pretexts for avoiding responsibility. Annu Rev Control 37(2):365–381
49. Silverman AM, Cohen GL (2014) Stereotypes as stumbling-blocks how coping with
stereotype threat affects life outcomes for people with physical disabilities. Pers Soc Psychol
Bull 40(10):1330–1340
50. Marsh V, Friedman R (1972) Changing public attitudes toward blindness. Except Children
38(5):426–428
51. Cheu J (2009) Seeing blindness on screen: The cinematic gaze of blind female protagonists.
J Popular Cult 42(3):480–496
52. Bolt D (2006) Beneﬁcial blindness: literary representation and the so-called positive
stereotyping of people with impaired vision. N Z J Disabil Stud 12:80–100
53. Lowenfeld B (1981) The case for the exceptional. Berthold Lowenfeld on blindness and blind
people: Selected papers by Berthold Lowenfeld. American Foundation or the Blind (Original
work published 1946), New York, pp. 205–209
54. Kirtley DD (1975) Blindness in the arts. Psychol Blindness, 49–92
55. Hansen N, Philo C (2007) The normality of doing things differently: bodies, spaces and
disability geography. Tijdschrift voor economische en sociale geograﬁe 98(4):493–506
56. Hersh MA (2016d) Unpublished interviews
57. Wong EY, Guymer RH, Hassell JB, Keeffe JE (2004) The experience of age-related macular
degeneration. J Visual Impairment Blindness 98(10):629–640
58. Barland M (2007) Incorporating user experience when developing assistive technology: the
case of the I-Cane. University of Oslo, Norway/Universiteit Maastricht, The Netherlands
59. Reindal SM (1999) Independence, dependence, interdependence: some reﬂections on the
subject and personal autonomy. Disabil Soc 14(3):353–367
60. Brisenden S (1986) Independent living and the medical model of disability. Disabil, Handicap
Soc 1(2):173–178. doi:10.1080/02674648666780171
61. Wood WM, Fowler CH, Uphold N, Test DW (2005) A review of self-determination
interventions with individuals with severe disabilities. Res Pract Persons Severe Disabil
30(3):121–146. doi:10.2511/rpsd.30.3.121
Mobility, Inclusion and Exclusion
647

62. Shinohara K, Wobbrock JO (2011) In the shadow of misperception: assistive technology use
and social interactions. In: Proceedings of the SIGCHI conference on human factors in
computing systems. ACM, New York, pp 705–714
63. Wood L (2006) Media representation of disabled people: a critical analysis. http://www.
disabilityplanet.co.uk/critical-analysis.html. Accessed 8 May 2012
64. Parette P, Scherer M (2004) Assistive technology use and stigma. Educ Training Dev Disabil
39(3):217–226
65. Hersh MA, Johnson MA (2010) A robotic guide for blind people part 1: a multi-national
survey of the attitudes, requirements and preferences of potential end-users. Appl Bion
Biomech 7(4):277–288
66. Hersh MA, Johnson MA (2012) A robotic guide for blind people part 2. Appl Bion Biomech
9:29–43
67. Bichard J-A, Coleman R, Langdon P (2007) Does my stigma look big in this? Considering
acceptability and desirability in the inclusive design of technology products. In: Stephanidis C
(ed) Universal Access in HCI, Part I, HCII, pp 622–631
68. Shinohara K, Tenenberg J (2009) A blind person’s interactions with technology.
Commun ACM 52(8):58–66
69. Hersh MA, Johnson MA (2008) Mobility: an overview. In: Hersh MA, Johnson MA
(eds) Assistive technology for visually impaired and blind people. Springer, Verlag
70. Fernandes H, Conceição N, Paredes H, Pereira A, Araújo P, Barroso J (2012) Providing
accessibility to blind people using GIS. Univ Access Inf Soc 11(4):399–407
71. Hayeems RZ, Geller G, Finkelstein D, Faden RR (2005) How patients experience progressive
loss of visual function: a model of adjustment using qualitative methods. Br J Ophthalmol
89(5):615–620
648
M.A. Hersh

Index
A
Acceptance of disability, 635
Accessibility, 411, 427–429, 431
Accessibility levels, 633
Accessible design principles, 641
Accessible environments, 631, 641
Accessible input, 393
Accessible pedestrian crossings, 643
Action, 7, 13, 16, 17, 19, 21, 24, 26, 27, 30, 31,
33, 37, 38, 47
Active and assisted living, 585, 588
Active multisensory context-awareness, 481,
509
Activity diamond, The, 430, 431
Additional information needs, 384, 389
Adult, 103, 120, 146, 147, 153, 180, 182, 202,
204, 205, 207, 213–218, 222, 226, 238,
242, 265–268, 275–277, 426, 568, 599,
622, 627
Allocentric reference frame, 265, 266
Amodal, 112, 114, 118, 121, 124–127, 129
Amodal spatial representations, 183
Anterior Ectosylvian Sulcus (AEC), 115, 116
App design, 389–391
Appearance of assistive devices, 640
Assistive devices, 597, 598, 601, 614
Assistive technology, 436, 463, 551, 567
Asymmetry ON/OFF multi-core architecture,
507
Attention, 18, 21, 31, 33, 39, 62, 65, 72, 80, 81,
90, 99, 102, 105, 150, 171, 177, 178, 202,
215, 220, 226, 250, 254, 266, 281, 296,
312, 319, 339, 366, 378, 393, 419, 598,
639, 645
Attitudes to blind people, 635, 645
Audio announcements, 633, 642
Auditory display, 441
Augmented reality, 294, 306
Autonomy, 283, 302
B
Bach-y-Rita, 82, 85, 86, 88, 96, 97, 104, 107
Barriers to employment, 632
Benchmark for mobility aids, 348
Blind, 538, 539, 547, 549, 550, 556, 569, 571,
575
Blind end-users, 392, 397, 400, 401, 404
Blindness, 435, 439, 443, 448, 450, 453
Blind people, 353
Body, 5, 13, 17, 22, 41, 56, 81, 87, 94, 99, 141,
173, 202, 210, 216, 226, 258, 269, 281,
395, 488, 540, 620, 626
Brain gym, 279
Brain imagery (EEG, MRI and fMRI), 5, 112,
121, 125, 126, 190, 209, 212, 226
C
Cane-based devices, 384
Car use and exclusion, 634
CHAT, 430
Child, 201, 202, 204, 214–216, 226
Children’s spatial orientation, 265
Clinical studies, 618
Co-design, 411, 414, 415, 426, 427, 431, 432
Cognitive load, 380, 391, 395, 598
Cognitive map, 148, 150, 151, 154–156
Computer assisted navigation, 598, 601, 611
Condillac, 78, 87, 90–103, 105
Congenital blindness, 111, 120, 122, 167, 186
Context aware, 390, 398, 399
Context of use, 411, 413, 429–432
Counterfactuals, 28, 34, 35, 42, 46
Crossmodal, 112, 115, 116, 122
Cross sensory, 105, 116
D
Depth perception, 25, 248
Descartes, 77–100, 102, 106
Design approach, 388, 400
© Springer International Publishing AG 2018
E. Pissaloux and R. Velázquez (eds.), Mobility of Visually Impaired People,
DOI 10.1007/978-3-319-54446-5
649

Design features, 392, 399
Design process, 411–415, 418, 419, 426, 430
Device abandonment, 385
Device acceptance, 385
Device evaluation, 403
Displacement, 353, 356, 357, 360, 362, 370,
371
E
Educational tools, 574
eETA, 469, 472, 478–480, 482, 488, 493, 501,
502, 506
Ego-centric frame, 265
Ego-motion, 274
Electronic orientation aids, 247, 252, 253, 255,
258, 259
Electronic travel aids, 252–255, 257–259, 435,
437, 439, 456
Embodiment, 14, 24
Enabling technologies, 585–588, 593
Enactive approach, 60, 63
End-user factors, 386
End-user involvement, 399, 400, 402
En-route assistance, 526, 527
Environmental information beacons, 383, 392,
396
ETA, 469–473, 475, 476, 478, 506
Event-awareness, 513, 516, 517
Extraocular implant, 620, 623
Eye diseases, 617, 623
Eye pathologies, 248, 249, 251
F
Factors that affect cane use, 387
Focus group, 415, 417, 418, 421, 426, 429, 431
G
Geometry-based navigation strategy, 146
Global navigation satellite systems, 383
Good design practice, 385, 386
Guide objects, 105, 242, 384, 452
H
Haptic imaging, 435
Hearing sense, 271
Heidegger, 4, 10, 11, 28, 32, 34
Hippocampus, 167, 180, 181, 186–188, 190
Human-centred design, 413, 414, 428, 431
Human-computer interaction, 598, 600, 601
Husserl, 7, 10, 13, 15–17, 25, 28, 29
Hypervigilance, 122
I
Inclusive design, 411, 427–429
Indoor localization, 585, 587–590, 593
Indoor mobility, 511, 518, 527–529
Indoor navigation, 483, 519, 526
Information, 353, 356–363, 368–371
Information and communication technologies,
77, 82
Information systems, 641
Intelligent cane, 318, 319, 527
Intelligent glasses, 313
Intentional mode, 3, 33
Interactive drawings, 556, 570
Interactive maps, 539, 540, 543, 557, 568, 569,
572, 575, 576
Interface design, 392, 395
Intraocular implant, 618–621
K
Kinesthetic training, 270
Kinetic therapy, 280
L
Lack of car access, 632
Landmark-based mobility strategy, 145
Language learning, 597, 598, 602
Lighting, 641
Lived space, 3, 5, 7, 8, 10, 11, 13, 14, 18–20,
24, 29, 34, 38, 39, 42
Localisation, 353, 356, 359, 363, 370, 371
Location Based Services (LBS), 586
Locomotion, 174, 177, 190
Lo-ﬁ, 415–418, 426, 427
Long cane features, 378, 382
Low usage of mobility devices, 378, 403
Low vision, 283, 284, 293–296, 306
M
Maps in relief, 277, 278
Memory, 5, 37, 43, 44, 60, 67, 69, 112,
125–129, 148–150, 179–181, 186, 201,
210, 226, 272, 361, 574, 599, 602, 614
Mental imagery, 112, 121, 125, 126
Merleau-Ponty, 78–81, 87, 88, 96–105, 107
Mindset, 411, 414, 416, 431
Minimalist method, 58
Mobile apps, 385, 391
Mobility, 53, 60, 283–288, 292, 294–296, 299,
301–304, 307
Mobility aids, 244, 258
Mobility and participation, 632
Mobility apps, 389, 390
650
Index

Mobility challenges of visually impaired
people, 237, 258
Mobility technologies, 77, 78, 90–92, 96–98,
104, 105, 107
Model of cognitive mobility, 323
Motricity, 3, 5, 13, 25
Movement perception, 138, 144
Multi-data streaming, 484
Multimodality, 428, 600
Multisensory cortex, 115
Multisensory data integration, 143
Multi-Trilateration Algorithm (MTA), 590, 593
N
Navigation, 167, 174–177, 179–182, 185, 186,
188–190
Navigation aids, 438
Needs, 353, 355, 359, 370–372
Neural correlates of navigation, 167, 168, 184,
185, 187, 190
Neural evidences of spatial cognition, 138, 148
Non-visual interaction, 575
Number of blind people, 377, 631
O
Obstacle avoidance devices, 381, 392, 396, 403
Ophthalmologic microsurgery, 617
Optic ﬂow, 138–140, 143, 144
Orientation, 353, 356, 357, 363, 366, 368–371
Orientation and mobility instruction, 240, 241
Outdoor mobility, 511, 513, 525
P
Participatory design, 414, 415
Path integration, 139, 141, 144, 146, 151, 157
Path planning, 511, 522, 525–527
Pedestrian, 357, 359, 361, 362, 364
Perception, 77–107
Perception-movement platform, 326, 334
Perceptual activity, 53, 67, 69, 70, 73
Perceptual devices, 53, 54
Perceptual learning, 168, 170, 184
Perceptual modality, 55, 69–71
Phenomenological properties of sensory
substitution, 167, 171
Phenomenology, 4, 22, 32, 35
Planning, 632, 635, 644
Plasticity, 111, 112, 115–117, 120–123, 126,
128, 129
Portable device, 486
Positioning, 477, 485, 486, 511, 512, 518, 520,
521, 527, 529
‘Positive’ stereotypes, 637
Posterior Parietal Cortex (PPC), 114, 180, 186,
188
Privacy, 388, 391, 397–399, 404
Proprioception, 141
Prosthetic vision, 299, 300
Prototyping, 414–416, 418, 426, 427
Public transport, 632–635, 638, 642, 644
R
Real/virtual environments, 201, 202, 217, 218,
220
Reference frames, 147, 150
Representation, 3, 6, 32, 57, 114, 145, 157,
182, 205, 210, 247, 284, 288, 296, 303,
422, 470, 484, 540, 548, 570, 575, 604, 639
Representations of blind people, 637
Retinal photoreceptors, 617
Retinal prosthesis, 625
Robotic devices, 383
S
Safety, 353, 356, 363–365, 367, 371, 385–388,
394, 399, 403, 404, 633, 636, 640, 641,
644, 645
Scene representation, 283, 284, 286, 287, 289,
296, 297, 299, 300, 302, 303, 307
SEES, 481, 482, 510
Sense of distance, 269, 274
Sense of obstacles, 274
Sense of taste, 272
Senses of touch, 270
Sensorimotor contingencies, 312
Sensori-motor rules, 61, 63
Sensory cues, 246
Sensory data fusion, 482
Sensory information, 138, 143, 144, 379, 631
Sensory modalities, 53, 70, 72
Sensory substitution, 54, 55, 57, 72, 73, 81,
436, 463
Sensory substitution devices, 112, 117, 129,
167–169, 174, 176
Sensory supplement, 81
Sensory supplementation, 283, 285, 287, 289,
299
Sensory training, 244–246, 269
Shore lines, 380
Signage, 641
Situational awareness assistance, 598, 601,
611, 613
Skill development, 242, 245
Smart cane, 318
Smart stick, 479, 480, 482, 485, 492, 496, 497,
501, 511, 512, 527
Index
651

Smell sense, 272
Somesthetic, 201, 202, 209, 210, 212–214,
217–220, 222–227
Space awareness, 317, 323, 324, 326
Space integration, 314, 324, 343, 347, 348
Space perception, 13, 27, 53, 55, 56, 66, 71,
313, 317, 320
Spatial cognition, 568, 569
Spatial cognition in blind persons, 45
Spatial cognition of animals, 151
Spatial encoding, 148–151, 154, 157
Spatial knowledge, 138, 153, 154–157
Spatial learning, 181
Spatial localization, 55, 56, 61, 66
Spatial navigation, 138, 144, 148, 151, 153,
157
Spatial orientation, 263–265, 267, 269, 271,
277, 278, 281
Spatial processing, 150
Specialized information, 283, 284, 303
Speech input, 393, 395
Speech output, 394, 395
Stability of objects, 273
Stereotypes, 635–637, 640, 645
Stigma, 635, 636, 639, 640
Substitution devices, 317
Superadditive neurons, 113
Superior colliculus, 113
Superior Temporal Polysensory (STP), 115
Supplementation devices, 317
Symbolic meaning of cane, 639
T
Tactile display, 597, 599, 600, 602, 603, 611
Tactile-foot stimulation, 598
Tactile gist, 312, 321–327, 334, 335, 338–340,
342, 343, 346–349
Tactile language, 597–600, 602, 603, 611
Tactile memory, 597–600, 602
Tactile patterns recognition, 328, 330, 334
Tactile perception, 601
Tactile stimulation device, 283
Tactile words, 597, 613
TactiPad, 312, 322–328, 330–344, 347–349
Tangible interaction, 558, 566
Technical aid, 283, 284, 293, 296, 306
Touch, 79, 80, 86, 89, 91, 92, 94–96, 98–100,
103
Touch stimulation device, 312, 323, 331, 347
Training, 378, 379, 383, 386, 387, 389, 395,
401, 403, 404
Training-induced brain plasticity, 167
Training techniques, 237, 248
Transit rider, 7
Transport and land use policies, 632
Travel aid categorisation, 380
Travel aid development, 377, 381, 404
Tyﬂology, 263
U
Ultrasound system, 585, 586, 588, 590
Uncertainty compensation, 591
Universal design, 428
User-centred design, 386, 399, 400
User-Environment Contextualization (UEC),
585, 588, 590, 593
User-Environment Interaction (UEI), 585, 588,
590, 593
V
Vestibular, 201, 202, 204–206, 208–210,
212–214, 216–224, 226
Vestibular data, 138, 140–154
Vibro-tactile output, 395
Vibrotactile patterns (tactons), 599, 602, 603,
613
Virtual reality, 175, 184, 293, 294, 300
Vision, 78–80, 82, 86, 87, 90, 92, 94, 106
Visual, 201–205, 208, 209, 213–226
Visual cortex, 618, 621–623
Visual cues/clues/landmarks, 138, 139
Visual impairment, 3, 5, 283, 284, 293, 436,
537, 539–544, 546–555, 558, 560, 561,
563–566, 568–570, 572, 574–576
Visually impaired, 353, 355–361, 363–366,
369–371
Visually Impaired People (VIP), 83, 469–471,
477–480, 482, 484, 490–496, 498, 499,
502, 503, 506, 508, 512, 514, 517
Visual neuro-prosthesis, 283, 296, 297, 299,
303
Visuo-auditory, 285, 289
Visuo-tactile, 285
Vocabulary, 598, 601–603
W
Wayﬁnding, 174, 175, 177, 317
Wearable computers, 614
White cane, 318, 321
Wireless sensor network, 588
Wizard of OZ, 418, 427
Workshop, 414–427, 429
652
Index


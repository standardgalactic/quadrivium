
Designing Distributed Systems
SECOND EDITION
Patterns and Paradigms for Scalable, Reliable Systems Using Kubernetes
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take advantage
of these technologies long before the official release of these titles.
Brendan Burns

Designing Distributed Systems
by Brendan Burns
Copyright © 2024 Brendan Burns. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North,
Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales
promotional use. Online editions are also available for most titles
(http://oreilly.com). For more information, contact our
corporate/institutional sales department: 800-998-9938 or
corporate@oreilly.com .
Aquisitions Editor: Louise Corrigan
Development Editor: Jill Leonard
Production Editor: Elizabeth Faerm
Copyeditor: TO COME
Proofreader: TO COME
Indexer: TO COME

Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
February 2025: Second Edition
Revision History for the Early Release
2023-11-02: First release
2024-04-05: Second release
See http://oreilly.com/catalog/errata.csp?isbn=9781098156350 for release
details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc.
Designing Distributed Systems, the cover image, and related trade dress are
trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the author and do not
represent the publisher’s views. While the publisher and the author have
used good faith efforts to ensure that the information and instructions
contained in this work are accurate, the publisher and the author disclaim all
responsibility for errors or omissions, including without limitation
responsibility for damages resulting from the use of or reliance on this

work. Use of the information and instructions contained in this work is at
your own risk. If any code samples or other technology this work contains
or describes is subject to open source licenses or the intellectual property
rights of others, it is your responsibility to ensure that your use thereof
complies with such licenses and/or rights.
978-1-098-15629-9
[LSI]

Chapter 1. Introduction
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take advantage
of these technologies long before the official release of these titles.
This will be the 1st chapter of the final book. The GitHub repository is
available at https://github.com/brendandburns/designing-distributed-
systems.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this chapter,
please reach out to the editor at jleonard@oreilly.com.
Today’s world of always-on applications and APIs have availability and
reliability requirements that would have been required of only a handful of
mission critical services around the globe only a few decades ago.
Likewise, the potential for rapid, viral growth of a service means that every
application has to be built to scale nearly instantly in response to user
demand. These constraints and requirements mean that almost every
application that is built—whether it is a consumer mobile app or a backend
payments application—needs to be a distributed system.

But building distributed systems is challenging. Often, they are one-off
bespoke solutions. In this way, distributed system development bears a
striking resemblance to the world of software development prior to the
development of modern object-oriented programming languages.
Fortunately, as with the development of object-oriented languages, there
have been technological advances that have dramatically reduced the
challenges of building distributed systems. In this case, it is the rising
popularity of containers and container orchestrators. As with the concept of
objects within object-oriented programming, these containerized building
blocks are the basis for the development of reusable components and
patterns that dramatically simplify and make accessible the practices of
building reliable distributed systems. In the following introduction, we give
a brief history of the developments that have led to where we are today.
A Brief History of Systems Development
In the beginning, there were machines built for specific purposes, such as
calculating artillery tables or the tides, breaking codes, or other precise,
complicated but rote mathematical applications. Eventually these purpose-
built machines evolved into general-purpose programmable machines. And
eventually they evolved from running one program at a time to running
multiple programs on a single machine via time-sharing operating systems,
but these machines were still disjoint from each other.

Gradually, machines came to be networked together, and client-server
architectures were born so that a relatively low-powered machine on
someone’s desk could be used to harness the greater power of a mainframe
in another room or building. While this sort of client-server programming
was somewhat more complicated than writing a program for a single
machine, it was still fairly straightforward to understand. The client(s) made
requests; the server(s) serviced those requests.
In the early 2000s, the rise of the internet and large-scale datacenters
consisting of thousands of relatively low-cost commodity computers
networked together gave rise to the widespread development of distributed
systems. Unlike client-server architectures, distributed system applications
are made up of multiple different applications running on different
machines, or many replicas running across different machines, all
communicating together to implement a system like web-search or a retail
sales platform.
Because of their distributed nature, when structured properly, distributed
systems are inherently more reliable. And when architected correctly, they
can lead to much more scalable organizational models for the teams of
software engineers that built these systems. Unfortunately, these advantages
come at a cost. These distributed systems can be significantly more
complicated to design, build, and debug correctly. The engineering skills
needed to build a reliable distributed system are significantly higher than
those needed to build single-machine applications like mobile or web

frontends. Regardless, the need for reliable distributed systems only
continues to grow. Thus there is a corresponding need for the tools,
patterns, and practices for building them.
Fortunately, technology has also increased the ease with which you can
build distributed systems. Containers, container images, and container
orchestrators have all become popular in recent years because they are the
foundation and building blocks for reliable distributed systems. Using
containers and container orchestration as a foundation, we can establish a
collection of patterns and reusable components. These patterns and
components are a toolkit that we can use to build our systems more reliably
and efficiently.
A Brief History of Patterns in Software
Development
This is not the first time such a transformation has occurred in the software
industry. For a better context on how patterns, practices, and reusable
components have previously reshaped systems development, it is helpful to
look at past moments when similar transformations have taken place.
Formalization of Algorithmic Programming

Though people had been programming for more than a decade before its
publication in 1962, Donald Knuth’s collection, The Art of Computer
Programming (Addison-Wesley Professional), marks an important chapter
in the development of computer science. In particular, the books contain
algorithms not designed for any specific computer, but rather to educate the
reader on the algorithms themselves. These algorithms then could be
adapted to the specific architecture of the machine being used or the
specific problem that the reader was solving. This formalization was
important because it provided users with a shared toolkit for building their
programs, but also because it showed that there was a general-purpose
concept that programmers should learn and then subsequently apply in a
variety of different contexts. The algorithms themselves, independent of
any specific problem to solve, were worth understanding for their own sake.
Patterns for Object-Oriented Programming
Knuth’s books represent an important landmark in the thinking about
computer programming, and algorithms represent an important component
in the development of computer programming. However, as the complexity
of programs grew, and the number of people writing a single program grew
from the single digits to the double digits and eventually to the thousands, it
became clear that procedural programming languages and algorithms were
insufficient for the tasks of modern-day programming. These changes in
computer programming led to the development of object-oriented

programming languages, which elevated data, reusability, and extensibility
to peers of the algorithm in the development of computer programs.
In response to these changes to computer programming, there were changes
to the patterns and practices for programming as well. Throughout the early
to mid-1990s, there was an explosion of books on patterns for object-
oriented programming. The most famous of these is the “gang of four”
book, Design Patterns: Elements of Reusable Object-Oriented
Programming by Erich Gamma et al. (Addison-Wesley Professional).
Design Patterns gave a common language and framework to the task of
programming. It described a series of interface-based patterns that could be
reused in a variety of contexts. Because of advances in object-oriented
programming and specifically interfaces, these patterns could also be
implemented as generic reusable libraries. These libraries could be written
once by a community of developers and reused repeatedly, saving time and
improving reliability.
The Rise of Open Source Software
Though the concept of developers sharing source code has been around
nearly since the beginning of computing, and formal free software
organizations have been in existence since the mid-1980s, the very late
1990s and the 2000s saw a dramatic increase in the development and
distribution of open source software. Though open source is only
tangentially related to the development of patterns for distributed systems, it

is important in the sense that it was through the open source communities
that it became increasingly clear that software development in general and
distributed systems development in particular are community endeavors. It
is important to note that all of the container technology that forms the
foundation of the patterns described in this book has been developed and
released as open source software. The value of patterns for both describing
and improving the practice of distributed development is especially clear
when you look at it from this community perspective.
NOTE
What is a pattern for a distributed system? There are plenty of instructions out there that will tell you
how to install specific distributed systems (such as a NoSQL database). Likewise, there are recipes
for a specific collection of systems (like a MEAN stack). But when I speak of patterns, I’m referring
to general blueprints for organizing distributed systems, without mandating any specific technology
or application choices. The purpose of a pattern is to provide general advice or structure to guide
your design. The hope is that such patterns will guide your thinking and also be generally applicable
to a wide variety of applications and environments.
The Value of Patterns, Practices, and
Components
Before spending any of your valuable time reading about a series of
patterns that I claim will improve your development practices, teach you
new skills, and—let’s face it—change your life, it’s reasonable to ask:

“Why?” What is it about the design patterns and practices that can change
the way that we design and build software? In this section, I’ll lay out the
reasons I think this is an important topic, and hopefully convince you to
stick with me for the rest of the book.
Standing on the Shoulders of Giants
As a starting point, the value that patterns for distributed systems offer is
the opportunity to figuratively stand on the shoulders of giants. It’s rarely
the case that the problems we solve or the systems we build are truly
unique. Ultimately, the combination of pieces that we put together and the
overall business model that the software enables may be something that the
world has never seen before. But the way the system is built and the
problems it encounters as it aspires to be reliable, agile, and scalable are not
new.
This, then, is the first value of patterns: they allow us to learn from the
mistakes of others. Perhaps you have never built a distributed system
before, or perhaps you have never built this type of distributed system.
Rather than hoping that a colleague has some experience in this area or
learning by making the same mistakes that others have already made, you
can turn to patterns as your guide. Learning about patterns for distributed
system development is the same as learning about any other best practice in
computer programming. It accelerates your ability to build software without

requiring that you have direct experience with the systems, mistakes, and
firsthand learning that led to the codification of the pattern in the first place.
A Shared Language for Discussing Our Practice
Learning about and accelerating our understanding of distributed systems is
only the first value of having a shared set of patterns. Patterns have value
even for experienced distributed system developers who already understand
them well. Patterns provide a shared vocabulary that enables us to
understand each other quickly. This understanding forms the basis for
knowledge sharing and further learning.
To better understand this, imagine that we both are using the same object to
build our house. I call that object a “Foo” while you call that object a “Bar.”
How long will we spend arguing about the value of a Foo versus that of a
Bar, or trying to explain the differing properties of Foo and Bar until we
figure out that we’re speaking about the same object? Only once we
determine that Foo and Bar are the same can we truly start learning from
each other’s experience.
Without a common vocabulary, we waste time in arguments of “violent
agreement” or in explaining concepts that others understand but know by
another name. Consequently, another significant value of patterns is to
provide a common set of names and definitions so that we don’t waste time

worrying about naming, and instead get right down to discussing the details
and implementation of the core concepts.
I have seen this happen in my short time working on containers. Along the
way, the notion of a sidecar container (described in Chapter 2 of this book)
took hold within the container community. Because of this, we no longer
have to spend time defining what it means to be a sidecar and can instead
jump immediately to how the concept can be used to solve a particular
problem. “If we just use a sidecar” …  “Yeah, and I know just the container
we can use for that.” This example leads to the third value of patterns: the
construction of reusable components.
Shared Components for Easy Reuse
Beyond enabling people to learn from others and providing a shared
vocabulary for discussing the art of building systems, patterns provide
another important tool for computer programming: the ability to identify
common components that can be implemented once.
If we had to create all of the code that our programs use ourselves, we
would never get done. Indeed, we would barely get started. Today, every
system ever written stands on the shoulders of thousands if not hundreds of
thousands of years of human effort. Code for operating systems, printer
drivers, distributed databases, container runtimes, and container

orchestrators—indeed, the entirety of applications that we build today are
built with reusable shared libraries and components.
Patterns are the basis for the definition and development of such reusable
components. The formalization of algorithms led to reusable
implementations of sorting and other canonical algorithms. The
identification of interface-based patterns gave rise to a collection of generic,
object-oriented libraries implementing those patterns.
Identifying core patterns for distributed systems enables us to to build
shared common components. Implementing these patterns as container
images with HTTP-based interfaces means they can be reused across many
different programming languages. And, of course, building reusable
components improves the quality of each component because the shared
code base gets sufficient usage to identify bugs and weaknesses, and
sufficient attention to ensure that they get fixed.
More recently, a series of software supply chain attacks has made
dependencies and dependency management a critical part of securing our
applications. In the context of a secure software supply chain, these shared
components taken on even more importance. Every library or application
that we use brings in more dependencies and consequently more risk.
Relying on a single shared implementation of a core idea reduces the total
amount of software that the world needs to depend on and by focusing
attention on a few dependencies, significantly improves the chances that

they are properly maintained and protected from software supply chain
attacks.
Summary
Distributed systems are required to implement the level of reliability,
agility, and scale expected of modern computer programs. Distributed
system design continues to be more of a black art practiced by wizards than
a science applied by laypeople. The identification of common patterns and
practices has regularized and improved the practice of algorithmic
development and object-oriented programming. It is this book’s goal to do
the same for distributed systems. Enjoy!

Part I. Single-Node Patterns

Chapter 2. The Sidecar Pattern
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take advantage
of these technologies long before the official release of these titles.
This will be the 3rd chapter of the final book. The GitHub repository is
available at https://github.com/brendandburns/designing-distributed-
systems.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this chapter,
please reach out to the editor at jleonard@oreilly.com.
The first single-node pattern is the sidecar pattern. The sidecar pattern is a
single-node pattern made up of two containers. The first is the application
container. It contains the core logic for the application. Without this
container, the application would not exist. In addition to the application
container, there is a sidecar container. The role of the sidecar is to augment
and improve the application container, often without the application
container’s knowledge. In its simplest form, a sidecar container can be used
to add functionality to a container that might otherwise be difficult to

improve. Sidecar containers are coscheduled onto the same machine via an
atomic container group, such as the pod  API object in Kubernetes. In
addition to being scheduled on the same machine, the application container
and sidecar container share a number of resources, including parts of the
filesystem, hostname and network, and many other namespaces. A generic
image of this sidecar pattern is shown in Figure 2-1.
Figure 2-1. The generic sidecar pattern
An Example Sidecar: Adding HTTPS to
a Legacy Service
Consider, for example, a legacy web service. Years ago, when it was built,
internal network security was not as high a priority for the company, and
thus, the application only services requests over unencrypted HTTP, not

HTTPS. Due to recent security incidents, the company has mandated the
use of HTTPS for all company websites. To compound the misery of the
team sent to update this particular web service, the source code for this
application was built with an old version of the company’s build system,
which no longer functions. Containerizing this HTTP application is simple
enough: the binary can run in a container with a version of an old Linux
distribution on top of a more modern kernel being run by the team’s
container orchestrator. However, the task of adding HTTPS to this
application is significantly more challenging. The team is trying to decide
between resurrecting the old build system versus porting the application’s
source code to the new build system, when one of the team members
suggests that they use the sidecar pattern to resolve the situation more
easily.
The application of the sidecar pattern to this situation is straightforward.
The legacy web service can be configured to serve exclusively on localhost
(127.0.0.1), which means that only services that share the local network
with the server will be able to access the service. Normally, this wouldn’t be
a practical choice because it would mean that no one could access the web
service. However, using the sidecar pattern, in addition to the legacy
container, we will add an nginx sidecar container. This nginx container lives
in the same network namespace as the legacy web application, so it can
access the service that is running on localhost. At the same time, this nginx
service can terminate HTTPS traffic on the external IP address of the pod
and proxy that traffic to the legacy web application (see Figure 2-2). Since

this unencrypted traffic is only sent via the local loopback adapter inside the
container group, the network security team is satisfied that the data is safe.
Likewise, by using the sidecar pattern, the team has modernized a legacy
application without having to figure out how to rebuild a new application to
serve HTTPS. A similar form of this pattern can also be used to add
automatic certificate rotation, or even authentication and authorization to
legacy web applications that may not be easy to modify.
Figure 2-2. The HTTPS sidecar
Dynamic Configuration with Sidecars
Simply proxying traffic into an existing application is not the only use for a
sidecar. Another common example is configuration synchronization. Many
older applications use a configuration file for parameterizing the
application; this may be a raw text file or something more structured like
TOML, XML, JSON, or YAML. Many pre-existing applications were
written to assume that this file was present on the filesystem and read their

configuration from there. However, in a cloud-native environment it is often
quite useful to use an API for updating configuration stored elsewhere. This
allows you to do a dynamic push of configuration information via an API
instead of manually logging in to every server and updating the
configuration file using imperative commands. The desire for such an API
is driven both by ease of use as well as the ability to add automation like
rollback, which makes configuring (and reconfiguring) safer and easier.
Kubernetes supports ConfigMap  resources for exactly this purpose.
Similar to the case of HTTPS, new applications can be written with the
expectation that configuration is a dynamic property that should be obtained
using a cloud API, but adapting and updating an existing application can be
significantly more challenging. Fortunately, the sidecar pattern again can be
used to provide new functionality that augments a legacy application
without changing the existing application. For the sidecar pattern shown in
Figure 2-3, there again are two containers: the container that is the serving
application and the container that is the configuration manager. The two
containers are grouped together into a pod where they share a directory
between themselves. This shared directory is where the configuration file is
maintained.
When the legacy application starts, it loads its configuration from the
filesystem, as expected. This configuration is placed into the filesystem
using ConfigMap  volume which places the current contents of a
ConfigMap  resource at a particular location in the file system. When the

configuration manager starts, it examines the configuration API and looks
for differences between the local filesystem and the configuration stored in
the API. If there are differences, the configuration manager downloads the
new configuration to the local filesystem and signals to the legacy
application that it should reconfigure itself with this new configuration. The
actual mechanism for this notification varies by application. Some
applications actually watch the configuration file for changes, while others
respond to a SIGHUP signal. In extreme cases, the configuration manager
may send a SIGKILL signal to abort the legacy application. Once aborted,
the container orchestration system will restart the legacy application, at
which point it will load its new configuration. As with adding HTTPS to an
existing application, this pattern illustrates how the sidecar pattern can help
adapt pre-existing applications to more cloud-native scenarios.

Figure 2-3. A sidecar example of managing a dynamic configuration
Modular Application Containers

At this point, you might be forgiven if you thought that the sole reason for
the sidecar pattern to exist was to adapt legacy applications where you no
longer wanted to make modifications to the original source code. While this
is a common use case for the pattern, there are many other motivations for
designing things using sidecars. One of the other main advantages of using
the sidecar pattern is modularity and reuse of the components used as
sidecars. In deploying any real-world, reliable application, there is
functionality that you need for debugging or other management of the
application, such as giving a readout of all of the different processes using
resources in the container, similar to the top  command line tool.
One approach to providing this introspection is to require that each
developer implement an HTTP /topz  interface that provides a readout of
resource usage. To make this easier, you might implement this webhook as
a language-specific plugin that the developer could simply link into their
application. But even if done this way, the developer would be forced to
choose to link it in and your organization would be forced to implement the
interface for every language it wants to support. Unless done with extreme
rigor, this approach is bound to lead to variations among languages as well
as a lack of support for the functionality when using new languages.
Instead, this topz  functionality can be deployed as a sidecar container
that shares the process-id (PID) namespace with the application container.
This topz  container can introspect all running processes and provide a
consistent user interface. Moreover, you can use the orchestration system to
automatically add this container to all applications deployed via the

orchestration system to ensure that there is a consistent set of tools available
for all applications running in your infrastructure.
Of course, with any technical choice, there are trade-offs between this
modular container-based pattern and rolling your own code into your
application. The library-based approach is always going to be somewhat
less tailored to the specifics of your application. This means that it may be
less efficient in terms of size of performance, or that the API may require
some adaptation to fit into your environment. I would compare these trade-
offs to the difference between buying off-the-rack clothing versus bespoke
fashion. The bespoke fashion will always fit you better, but it will take
longer to arrive and cost more to acquire. As with clothes, for most of us it
makes sense to buy the more general-purpose solution when it comes to
coding. Of course, if your application demands extremes in terms of
performance, you can always choose the handwritten solution.
Hands On: Deploying the topz Container
To see the topz  sidecar in action, you first need to deploy some other
container to act as the application container. Choose an existing application
that you are running and deploy it using Docker:
$ docker run -d <my-app-image> 
<container-hash-value>

After you run that image, you will receive the identifier for that specific
container. It will look something like: cccf82b85000…  If you don’t
have it, you can always look it up using the docker ps  command,
which will show all currently running containers. Assuming you have
stashed that value in an environment variable named APP_ID , you can
then run the topz  container in the same PID namespace using:
$ docker run --pid=container:${APP_ID} \
    -p 8080:8080 \
    brendanburns/topz:db0fa58 \
    /server --addr=0.0.0.0:8080
This will launch the topz  sidecar in the same PID namespace as the
application container. Note that you may need to change the port that the
sidecar uses for serving if your application container is also running on port
8080 . Once the sidecar is running, you can visit
http://localhost:8080/topz to get a complete readout of the processes that
are running in the application container and their resource usage.
You can mix this sidecar with any other existing container to easily get a
view into how the container is using its resources via a web interface.
Building a Simple PaaS with Sidecars

The sidecar pattern can be used for more than adaptation and monitoring. It
can also be used as a means to implement the complete logic for your
application in a simplified, modular manner. As an example, imagine
building a simple platform as a service (PaaS) built around the git
workflow. Once you deploy this PaaS, simply pushing new code up to a Git
repository results in that code being deployed to the running servers. We’ll
see how the sidecar pattern makes building this PaaS remarkably
straightforward.
As previously stated, in the sidecar pattern there are two containers: the
main application container and the sidecar. In our simple PaaS application,
the main container is a Node.js server that implements a web server. The
Node.js server is instrumented so that it automatically reloads the server
when new files are updated. This is accomplished with the nodemon  tool.
The sidecar container shares a filesystem with the main application
container and runs a simple loop that synchronizes the filesystem with an
existing Git repository:
#!/bin/bash 
 
while true; do 
  git pull 
  sleep 10 
done

Obviously this script could be more complex, pulling from a specific
branch instead of simply from HEAD. It is left purposefully simple to
improve the readability of this example.
The Node.js application and Git synchronization sidecar are coscheduled
and deployed together to implement our simple PaaS (Figure 2-4). Once
deployed, every time new code is pushed to a Git repository, the code is
automatically updated by the sidecar and reloaded by the server.

Figure 2-4. A simple sidecar-based PaaS

Designing Sidecars for Modularity and
Reusability
In all of the examples of sidecars that we have detailed throughout this
chapter, one of the most important themes is that every one was a modular,
reusable artifact. To be successful, the sidecar should be reusable across a
wide variety of applications and deployments. By achieving modular reuse,
sidecars can dramatically speed up the building of your application.
However, this modularity and reusability, just like achieving modularity in
high-quality software development requires focus and discipline. In
particular, you need to focus on developing three areas:
Parameterizing your containers
Creating the API surface of your container
Documenting the operation of your container
Parameterized Containers
Parameterizing your containers is the most important thing you can do to
make your containers modular and reusable regardless of whether they are
sidecars or not, though sidecars and other add-on containers are especially
important to parameterize.

What do I mean when I say “parameterize”? Consider your container as a
function in your program. How many parameters does it have? Each
parameter represents an input that can customize a generic container to a
specific situation. Consider, for example, the SSL add-on sidecar deployed
previously. To be generally useful, it likely needs at least two parameters:
the first is the name of the certificate being used to provide SSL, and the
other is the port of the “legacy” application server running on localhost.
Without these parameters, it is hard to imagine this sidecar container being
usable for a broad array of applications. Similar parameters exist for all of
the other sidecars described in this chapter.
Now that we know the parameters we want to expose, how do we actually
expose them to users, and how do we consume them inside the container.
There are two ways in which such parameters can be passed to your
container: through environment variables or the command line. Though
either is feasible, I have a general preference for passing parameters via
environment variables. An example of passing such parameters to a sidecar
container is:
docker run -e=PORT=<port> -d <image>
Of course, delivering values into the container is only part of the battle. The
other part is actually using these variables inside the container. Typically, to
do that, a simple shell script is used that loads the environment variables

supplied with the sidecar container and either adjusts the configuration files
or parameterizes the underlying application.
For example, you might pass in the certificate path and port as environment
variables:
In your container, you would use those variables to configure an
nginx.conf  file that points the web server to the correct file and proxy
location.
Define Each Container’s API
Given that you are parameterizing your containers, it is obvious that your
containers are defining a “function” that is called whenever the container is
executed. This function is clearly a part of the API that is defined by your
container, but there are other parts to this API as well, including calls that
the container will make to other services as well as traditional HTTP or
other APIs that the container provides.
As you think about defining modular, reusable containers, it is important to
realize that all aspects of how your container interacts with its world are
part of the API defined by that reusable container. As in the world of
microservices, these micro-containers rely on APIs to ensure that there is a
docker run -e=PROXY_PORT=8080 -e=CERTIFICATE_PATH

clean separation between the main application container and the sidecar.
Additionally the API exists to ensure that all consumers of the sidecar will
continue to work correctly as subsequent versions are released. Likewise,
having a clean API for a sidecar enables the sidecar developer to move
more quickly since they have a clear definition (and hopefully unit tests) for
the services they provide as a part of the sidecar.
To see a concrete example of why this API surface area is important,
consider the configuration management sidecar we discussed earlier. A
useful configuration for this sidecar might be UPDATE_FREQUENCY ,
which indicates how often the configuration should be synchronized with
the filesystem. It is clear that if, at some later time, the name of the
parameter is changed to UPDATE_PERIOD , this change would be a
violation of the sidecar’s API and clearly would break it for some users.
While that example is obvious, even more subtle changes can break the
sidecar API. Imagine, for example, that UPDATE_FREQUENCY  originally
took a number in seconds. Over time and with feedback from users, the
sidecar developer determined that specifying seconds for large time values
(e.g., minutes) was annoying users and changed the parameter to accept
strings (10 m, 5 s, etc.). Because old parameter values (e.g., 10, for 10
seconds) won’t parse in this new scheme, this is a breaking API change.
Suppose still that the developer anticipated this but made values without
units parse to milliseconds where they had previously parsed to seconds.
Even this change, despite not leading to an error, is a breaking API change

for the sidecar since it will lead to significantly more frequent configuration
checks and correspondingly more load on the cloud configuration server.
I hope this discussion has shown you that for true modularity, you need to
be very conscious of the API that your sidecar provides, and that “breaking”
changes to that API may not always be as obvious as changing the name of
a parameter.
Documenting Your Containers
By now, you’ve seen how you can parameterize your sidecar containers to
make them modular and reuseable. You’ve learned about the importance of
maintaining a stable API to ensure that you don’t break sidecars for your
users. But there’s one final step to building modular, reusable containers:
ensuring that people can use them in the first place.
As with software libraries, the key to building something truly useful is
explaining how to use it. There is little good in building a flexible, reliable
modular container if no one can figure out how to use it. Sadly, there are
few formal tools available for documenting container images, but there are
some best practices you can use to accomplish this.
For every container image, the most obvious place to look for
documentation is the Dockerfile  from which the container was built.
There are some parts of the Dockerfile  that already document how the

container works. One example of this is the EXPOSE  directive, which
indicates the ports that the image listens on. Even though EXPOSE  is not
necessary, it is a good practice to include it in your Dockerfile  and
also to add a comment that explains what exactly is listening on that port.
For example:
... 
 
# Main web server runs on port 8080
EXPOSE 8080 
...
Additionally, if you use environment variables to parameterize your
container, you can use the ENV  directive to set default values for those
parameters as well as document their usage:
Finally, you should always use the LABEL  directive to add metadata for
your image; for example, the maintainer’s email address, web page, and
... 
 
# The PROXY_PORT parameter indicates the port on 
# traffic to.
ENV PROXY_PORT 8000 
...

version of the image:
The names for these labels are drawn from the schema established by the
Open Containers Initiative The Open Containers Intiative (OCI) is the
specification body which maintains the definition of what it means to be a
container image. A part of that specification is the definition of a shared set
of annotations/labels. By using a common taxonomy of image labels,
multiple different tools can rely on the same meta information in order to
visualize, monitor, and correctly use an application. By adopting shared
terms, you can use the set of tools developed in the community without
modifying your image. Of course, you can also add whatever additional
labels make sense in the context of your image.
Summary
Over the course of this chapter, we’ve introduced the sidecar pattern for
combining containers on a single machine. In the sidecar pattern, a sidecar
... 
 
LABEL "org.opencontainers.image.vendor"="name@com
LABEL "org.opencontainers.image.url"="http://imag
LABEL "org.opencontainers.image.version"="1.0.3" 
...

container augments and extends an application container to add
functionality. Sidecars can be used to update existing legacy applications
when changing the application is too costly. Likewise, they can be used to
create modular utility containers that standardize implementations of
common functionality. These utility containers can be reused in a large
number of applications, increasing consistency and reducing the cost of
developing each application. Subsequent chapters introduce other single-
node patterns that demonstrate other uses for modular, reusable containers.

Part II. Serving Patterns

Chapter 3. Sharded Services
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take advantage
of these technologies long before the official release of these titles.
This will be the 7th chapter of the final book. The GitHub repository is
available at https://github.com/brendandburns/designing-distributed-
systems.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this chapter,
please reach out to the editor at jleonard@oreilly.com.
In the previous chapter, we saw the value of replicating stateless services
for reliability, redundancy, and scaling. This chapter considers sharded
services. With the replicated services that we introduced in the preceding
chapter, each replica was entirely homogeneous and capable of serving
every request. In contrast to replicated services, with sharded services, each
replica, or shard, is only capable of serving a subset of all requests. A load-
balancing node, or root, is responsible for examining each request and
distributing each request to the appropriate shard or shards for processing.

The contrast between replicated and sharded services is represented in
Figure 3-1.
Figure 3-1. Replicated service versus sharded service
Replicated services are generally used for building stateless services,
whereas sharded services are generally used for building stateful services.
The primary reason for sharding the data is because the size of the state is
too large to be served by a single machine. Sharding enables you to scale a
service in response to the size of the state that needs to be served.
Sharding is not exclusively for stateful services though. Sometimes
sharding can also be useful for stateless services for the purpose of
isolation. Failures in distributed systems can sometimes come from the
input requests to the system (“poison requests”) in a replicated system, if

the poison requests come in a large enough volume they can take down all
of the replicas. In such systems, sharding can form an isolation boundary
that protects the majority of the requests from the poison ones. Though the
poison requests may take out the entire shard, that will only be a fraction of
the requests that the system processes.
Sharded Caching
To completely illustrate the design of a sharded system, this section
provides a deep dive into the design of a sharded caching system. A
sharded cache is a cache that sits between the user requests and the actually
frontend implementation. A high-level diagram of the system is shown in
Figure 3-2.

Figure 3-2. A sharded cache
In Chapter 4, we discussed how an ambassador could be used to distribute
data to a sharded service. This section discusses how to build that service.
When designing a sharded cache, there are a number of design aspects to
consider:

Why you might need a sharded cache
The role of the cache in your architecture
Replicated, sharded caches
The sharding function
Why You Might Need a Sharded Cache
Reminder: the primary reason for sharding any service is to increase the
size of the data being stored in the service. To understand how this helps a
caching system, imagine the following system: Each cache has 10 GB of
RAM available to store results, and can serve 100 requests per second
(RPS). Suppose then that our service has a total of 200 GB possible results
that could be returned, and an expected 1,000 RPS. Clearly, we need 10
replicas of the cache in order to satisfy 1,000 RPS (10 replicas × 100
requests per second per replica). The simplest way to deploy this service
would be as a replicated service, as described in the previous chapter. But
deployed this way, the distributed cache can only hold a maximum of 5%
(10 GB/200 GB) of the total data set that we are serving. This is because
each cache replica is independent, and thus each cache replica stores
roughly the exact same data in the cache. This is great for redundancy, but
pretty terrible for maximizing memory utilization. If instead, we deploy a
10-way sharded cache, we can still serve the appropriate number of RPS
(10 × 100 is still 1,000), but because each cache serves a completely unique
set of data, we are able to store 50% (10 × 10 GB/200 GB) of the total data
set. This tenfold increase in cache storage means that the memory for the

cache is much better utilized, since each key exists only in a single cache.
The more data we store in the cache, the faster the overall system will be
able to operate since more requests can be served from the cache.
The Role of the Cache in System Performance
In Chapter 6 we discussed how caches can be used to optimize end-user
performance and latency, but one thing that wasn’t covered was the
criticality of the cache to your application’s performance, reliability, and
stability.
Put simply, the important question for you to consider is: If the cache were
to fail, what would the impact be for your users and your service?
When we discussed the replicated cache, this question was less relevant
because the cache itself was horizontally scalable, and failures of specific
replicas would only lead to transient failures. Likewise, the cache could be
horizontally scaled in response to increased load without impacting the end
user.
This changes when you consider sharded caches. Because a specific user or
request is always mapped to the same shard, if that shard fails, that user or
request will always miss the cache until the shard is restored. Given the
nature of a cache as transient data, this miss is not inherently a problem, and
your system must know how to recalculate the data. However, this

recalculation is inherently slower than using the cache directly, and thus it
has performance implications for your end users.
The performance of your cache is defined in terms of its hit rate. The hit
rate is the percentage of the time that your cache contains the data for a user
request. Ultimately, the hit rate determines the overall capacity of your
distributed system and affects the overall capacity and performance of your
system.
Imagine, if you will, that you have a request-serving layer that can handle
1,000 RPS. After 1,000 RPS, the system starts to return HTTP 500  errors
to users. If you place a cache with a 50% hit rate in front of this request-
serving layer, adding this cache increases your maximum RPS from 1,000
RPS to 2,000 RPS. To understand why this is true, you can see that of the
2,000 inbound requests, 1,000 (50%) can be serviced by the cache, leaving
1,000 requests to be serviced by your serving layer. In this instance, the
cache is fairly critical to your service, because if the cache fails, then the
serving layer will be overloaded and half of all your user requests will fail.
Given this, it likely makes sense to rate your service at a maximum of 1,500
RPS rather than the full 2,000 RPS. If you do this, then you can sustain a
failure of half of your cache replicas and still keep your service stable.
But the performance of your system isn’t just defined in terms of the
number of requests that it can process. Your system’s end-user performance
is defined in terms of the latency of requests as well. A result from a cache

is generally significantly faster than calculating that result from scratch.
Consequently, a cache can improve the speed of requests as well as the total
number of requests processed. To see why this is true, imagine that your
system can serve a request from a user in 100 milliseconds. You add a cache
with a 25% hit rate that can return a result in 10 milliseconds. Thus, the
average latency for a request in your system is now 77.5 milliseconds.
Unlike maximum requests per second, the cache simply makes your
requests faster, so there is somewhat less need to worry about the fact that
requests will slow down if the cache fails or is being upgraded. However, in
some cases, the performance impact can cause too many user requests to
pile up in request queues and ultimately time out. It’s always recommended
that you load test your system both with and without caches to understand
the impact of the cache on the overall performance of your system.
Finally, it isn’t just failures that you need to think about. If you need to
upgrade or redeploy a sharded cache, you can not just deploy a new replica
and assume it will take immediately take the existing load. A newly created
cache shard doesn’t have any data stored in memory. Responses are cached
as they are received in response to user requests. A newly deployed cache
needs to be “warmed up” or filled with responses before it can provide
maximal benifit. Deploying a new version of a sharded cache will generally
result in temporarily losing some capacity. Another, more advanced option
is to replicate your shards.

Replicated, Sharded Caches
Sometimes your system is so dependent on a cache for latency or load that
it is not acceptable to lose an entire cache shard if there is a failure or you
are doing a rollout. Alternatively, you may have so much load on a
particular cache shard that you need to scale it to handle the load. For these
reasons, you may choose to deploy a sharded, replicated service. A sharded,
replicated service combines the replicated service pattern described in the
previous chapter with the sharded pattern described in previous sections. In
a nutshell, rather than having a single server implement each shard in the
cache, a replicated service is used to implement each cache shard.
This design is obviously more complicated to implement and deploy, but it
has several advantages over a simple sharded service. Most importantly, by
replacing a single server with a replicated service, each cache shard is
resilient to failures and is always present during failures. Rather than
designing your system to be tolerant to performance degradation resulting
from cache shard failures, you can rely on the performance improvements
that the cache provides. Assuming that you are willing to over-provision
shard capacity, this means that it is safe for you to do a cache rollout during
peak traffic, rather than waiting for a quiet period for your service.
Additionally, because each replicated cache shard is an independent
replicated service, you can scale each cache shard in response to its load;
this sort of “hot sharding” is discussed at the end of this chapter.

Hands On: Deploying an Ambassador and
Memcache for a Sharded Cache
In Chapter 4 we saw how to deploy a sharded Redis service. Deploying a
sharded memcache is similar.
First, we will deploy memcache as a Kubernetes StatefulSet :
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sharded-memcache
spec:
  serviceName: "memcache"
  replicas: 3
  template:
    metadata:
      labels:
        app: memcache
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: memcache
        image: memcached
        ports:
        - containerPort: 11211
          name: memcache

Save this to a file named memcached-shards.yaml and you can deploy this
with kubectl create -f memcached-shards.yaml . This will
create three containers running memcached.
As with the sharded Redis example, we also need to create a Kubernetes
Service  that will create DNS names for the replicas we have created.
The service looks like this:
apiVersion: v1
kind: Service
metadata:
  name: memcache
  labels:
    app: memcache
spec:
  ports:
  - port: 11211
    name: memcache
  clusterIP: None
  selector:
    app: memcache
Save this to a file named memcached-service.yaml and deploy it with
kubectl create -f memcached-service.yaml . You should
now have DNS entries for memcache-0.memcache , memcache-

1.memcache , etc. As with Redis, we can use these names to configure
twemproxy .
memcache:
  listen: 127.0.0.1:11211
  hash: fnv1a_64
  distribution: ketama
  auto_eject_hosts: true
  timeout: 400
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - memcache-0.memcache:11211:1
   - memcache-1.memcache:11211:1
   - memcache-2.memcache:11211:1
In this config, you can see that we are serving the memcache protocol on
localhost:11211  so that the application container can access the
ambassador. We will deploy this into our ambassador pod using a
Kubernetes ConfigMap  object that we can create with: kubectl
create configmap --from-file=nutcracker.yaml twem-
config .
Finally, all of the preparations are done, and we can deploy our ambassador
example. We define a pod that looks like this:

apiVersion: v1
kind: Pod
metadata:
  name: sharded-memcache-ambassador
spec:
  containers:
    # This is where the application container wou
    # - name: nginx
    #   image: nginx
    # This is the ambassador container
    - name: twemproxy
      image: ganomede/twemproxy
      command:
      - nutcracker
      - -c
      - /etc/config/nutcracker.yaml
      - -v
      - 7
      - -s
      - 6222
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: twem-config

You can save this to a file named memcached-ambassador-pod.yaml, and
then deploy it with:
Of course, we don’t have to use the ambassador pattern if we don’t want to.
An alternative is to deploy a replicated shard router service. There are
trade-offs between using an ambassador versus using a shard routing
service. The value of the service is a reduction of complexity. You don’t
have to deploy the ambassador with every pod that wants to access the
sharded memcache service, it can be accessed via a named and load-
balanced service. The downside of a shared service is twofold. First,
because it is a shared service, you will have to scale it larger as demand
load increases. Second, using the shared service introduces an extra
network hop that will add some latency to requests and contribute network
bandwith to the overall distributed system.
To deploy a shared routing service, you need to change the twemproxy
configuration slightly so that it listens on all interfaces, not just localhost:
memcache:
  listen: 0.0.0.0:11211
  hash: fnv1a_64
kubectl create -f memcached-ambassador-pod.yaml

  distribution: ketama
  auto_eject_hosts: true
  timeout: 400
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - memcache-0.memcache:11211:1
   - memcache-1.memcache:11211:1
   - memcache-2.memcache:11211:1
You can save this to a file named shared-nutcracker.yaml, and then create a
corresponding ConfigMap  using kubectl :
Then you can turn up the replicated shard routing service as a
Deployment :
apiVersion: extensions/v1
kind: Deployment
metadata:
  name: shared-twemproxy
spec:
  replicas: 3
  template:
    metadata:
kubectl create configmap --from-file=shared-nutcr

      labels:
        app: shared-twemproxy
    spec:
      containers:
      - name: twemproxy
        image: ganomede/twemproxy
        command:
        - nutcracker
        - -c
        - /etc/config/shared-nutcracker.yaml
        - -v
        - 7
        - -s
        - 6222
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config
      volumes:
      - name: config-volume
        configMap:
          name: shared-twem-config
If you save this to shared-twemproxy-deploy.yaml, you can create the
replicated shard router using kubectl :
kubectl create -f shared-twemproxy-deploy.yaml

To complete the shard router, we have to declare a load balancer to process
requests:
kind: Service
apiVersion: v1
metadata:
  name: shard-router-service
spec:
  selector:
    app: shared-twemproxy
  ports:
    - protocol: TCP
      port: 11211
      targetPort: 11211
This load balancer can be created using kubectl create -f
shard-router-service.yaml .
An Examination of Sharding Functions
So far we’ve discussed the design and deployment of both simple sharded
and replicated sharded caches, but we haven’t spent very much time
considering how traffic is routed to different shards. Consider a sharded
service where you have 10 independent shards. Given some specific user
request Req, how do you determine which shard S in the range from zero to

nine should be used for the request? This mapping is the responsibility of
the sharding function. A sharding function is very similar to a hashing
function, which you may have encountered when learning about hashtable
data structures. Indeed, a bucket-based hashtable could be considered an
example of a sharded service. Given both Req and Shard, then the role of
the sharding function is to relate them together, specifically:
Shard = ShardingFunction(Req)
Commonly, the sharding function is defined using a hashing function and
the modulo (%) operator. Hashing functions are functions that transform an
arbitrary object into an integer hash. The hash function has two important
characteristics for our sharding:
Determinism
The output should always be the same for a unique input.
Uniformity
The distribution of outputs across the output space should be equal.
For our sharded service, determinism and uniformity are the most important
characteristics. Determinism is important because it ensures that a particular
request R always goes to the same shard in the service. Uniformity is
important because it ensures that load is evenly spread between the different
shards.

Fortunately for us, modern programming languages include a wide variety
of high-quality hash functions. However, the outputs of these hash functions
are often significantly larger than the number of shards in a sharded service.
Consequently, we use the modulo operator (%) to reduce a hash function to
the appropriate range. Returning to our sharded service with 10 shards, we
can see that we can define our sharding function as:
Shard = hash(Req) % 10
If the output of the hash function has the appropriate properties in terms of
determinism and uniformity, those properties will be preserved by the
modulo operator.
Selecting a Key
Given this sharding function, it might be tempting to simply use the hashing
function that is built into the programming language, hash the entire object,
and call it a day. The result of this, however, will not be a very good
sharding function.
To understand this, consider a simple HTTP request that contains three
things:
The time of the request
The source IP address from the client
The HTTP request path (e.g., /some/page.html)

If we use a simple object-based hashing function, shard ( request ),
then it is clear that {12:00, 1.2.3.4, /some/file.html}  has a
different shard value than {12:01, 5.6.7.8,
/some/file.html} . The output of the sharding function is different
because the client’s IP address and the time of the request are different
between the two requests. But of course, in most cases, the IP address of the
client and the time of the request don’t impact the response to the HTTP
request. Consequently, instead of hashing the entire request object, a much
better sharding function would be shard ( request.path ). When we
use request.path  as the shard key, then we map both requests to the
same shard, and thus the response to one request can be served out of the
cache to service the other.
Of course, sometimes client IP is important to the response that is returned
from the frontend. For example, client IP may be used to look up the
geographic region that the user is located in, and different content (e.g.,
different languages) may be returned to different IP addresses. In such
cases, the previous sharding function shard ( request.path ) will
actually result in errors, since a cache request from a French IP address may
be served a result page from the cache in English. In such cases, the cache
function is too general, as it groups together requests that do not have
identical responses.
Given this problem, it would be tempting then to define our sharding
function as shard ( request.ip, request.path ), but this

sharding function has problems as well. It will cause two different French
IP addresses to map to different shards, thus resulting in inefficient
sharding. This shard function is too specific, as it fails to group together
requests that are identical. A better sharding function for this situation
would be:
shard ( country ( request.ip ), request.path )
This first determines the country from the IP address, and then uses that
country as part of the key for the sharding function. Thus multiple requests
from France will be routed to one shard, while requests from the United
States will be routed to a different shard.
Determining the appropriate key for your sharding function is vital to the
optimal design of a sharded system. Determining the correct shard key
requires an understanding of the requests that you expect to see.
Consistent Hashing Functions
Setting up the initial shards for a new service is relatively straightforward:
you set up the appropriate shards and the roots to perform the sharding, and
you are off to the races. However, what happens when you need to change
the number of shards in your sharded service? Such “re-sharding” is often a
complicated process.

To understand why this is true, consider the sharded cache previously
examined. Certainly, scaling the cache from 10 to 11 replicas is
straightforward to do with a container orchestrator, but consider the effect
of changing the scaling function from hash(Req) % 10 to hash(Req) % 11.
When you deploy this new scaling function, a large number of requests are
going to be mapped to a different shard than the one they were previously
mapped to. In a sharded cache, this is going to temporarily cause a dramatic
increase in your miss rate until the cache is repopulated with responses for
the new requests that have been mapped to that cache shard by the new
sharding function. In the worst case, rolling out a new sharding function for
your sharded cache will be equivalent to a complete cache failure.
To resolve these kinds of problems, many sharding functions use consistent
hashing functions. Consistent hashing functions are special hash functions
that are guaranteed to only remap # keys / # shards, when being resized to #
shards. For example, if we use a consistent hashing function for our sharded
cache, moving from 10 to 11 shards will only result in remapping < 10% (K
/ 11) keys. This is dramatically better than losing the entire sharded service.
Hands On: Building a Consistent HTTP Sharding
Proxy
To shard HTTP requests, the first question to answer is what to use as the
key for the sharding function. Though there are several options, a good
general-purpose key is the request path as well as the fragment and query

parameters (i.e., everything that makes the request unique). Note that this
does not include cookies from the user or the language/location (e.g.,
EN_US). If your service provides extensive customization to users or their
location, you will need to include them in the hash key as well.
We can use the versatile nginx HTTP server for our sharding proxy.
worker_processes  5;
error_log  error.log;
pid        nginx.pid;
worker_rlimit_nofile 8192; 
 
events {
  worker_connections  1024;
} 
 
http {
    # define a named 'backend' that we can use in
    # below.
    upstream backend {
        # Has the full URI of the request and use
        hash $request_uri consistent
        server web-shard-1.web;
        server web-shard-2.web;
        server web-shard-3.web;
    } 
 
server {

Note that we chose to use the full request URI as the key for the hash and
use the key word consistent  to indicate that we want to use a
consistent hashing function.
Sharded, Replicated Serving
Most of the examples in this chapter so far have described sharding in terms
of cache serving. But, of course, caches are not the only kinds of services
that can benefit from sharding. Sharding is useful when considering any
sort of service where there is more data than can fit on a single machine. In
contrast to previous examples, the key and sharding function are not a part
of the HTTP request, but rather some context for the user.
For example, consider implementing a large-scale multi-player game. Such
a game world is likely to be far too large to fit on a single machine.
However, players who are distant from each other in this virtual world are
unlikely to interact. Consequently, the world of the game can be sharded
    server {
        listen localhost:80;
        location / {
            proxy_pass http://backend;
        }
    }
}

across many different machines. The sharding function is keyed off of the
player’s location so that all players in a particular location land on the same
set of servers.
Hot Sharding Systems
Ideally the load on a sharded cache will be perfectly even, but in many
cases this isn’t true and “hot shards” appear because organic load patterns
drive more traffic to one particular shard.
As an example of this, consider a sharded cache for a user’s photos; when a
particular photo goes viral and suddenly receives a disproportionate amount
of traffic, the cache shard containing that photo will become “hot.” When
this happens, with a replicated, sharded cache, you can scale the cache
shard to respond to the increased load. Indeed, if you set up autoscaling for
each cache shard, you can dynamically grow and shrink each replicated
shard as the organic traffic to your service shifts around. An illustration of
this process is shown in Figure 3-3. Initially the sharded service receives
equal traffic to all three shards. Then the traffic shifts so that Shard A is
receiving four times as much traffic as Shard B and Shard C. The hot
sharding system moves Shard B to the same machine as Shard C, and
replicates Shard A to a second machine. Traffic is now, once again, equally
shared between replicas.


Figure 3-3. An example of a hot sharded system: initially the shards are evenly distributed, but when
extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on
a single machine
Summary
This chapter introduced the concept of sharding which directs particular
requests to particular groups of machines instead of load-balancing all
traffic to all replicas of a service. An example was given of how sharding
can significantly improve the efficiency and performance of a caching
system attached to a serving API. Sharding is not just useful for caching,
however, systems like large-scale open world game servers, data bases can
scale beyond the size limits of a single machine by sharding their data
across multiple different shard servers. Sharding also helps with isolation,
limiting the blast radius of poison requests to a subset of all servers. Setting
up a sharded system requires a careful understanding and design of both the
shard key and the key hashing function. Identifying the right way to
partition your data can often make the difference between a scalable system
and one which runs into bottlenecks as it scales. Understanding the details
of sharding is critical to building larger and more complex distributed
systems.

About the Author
Brendan Burns is a distinguished engineer at Microsoft and a cofounder of
the Kubernetes open source project. At Microsoft he works on Azure,
focusing on Containers and DevOps. Prior to Microsoft, he worked at
Google in the Google Cloud Platform, where he helped build APIs like
Deployment Manager and Cloud DNS. Before working on cloud
computing, he worked on Google’s web-search infrastructure, with a focus
on low-latency indexing. He has a PhD in computer science from the
University of Massachusetts Amherst with a specialty in robotics. He lives
in Seattle with his wife, Robin Sanders, their two children, and a cat, Mrs.
Paws, who rules over their household with an iron paw.


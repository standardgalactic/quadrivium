OPTIMIZATION THEORY AND 
METHODS
Nonlinear Programming

Springer Optimization and Its Applications 
VOLUME 1 
Managing Editor 
Panos M. Pardalos (University of Florida)  
Editor—Combinatorial Optimization 
Ding-Zhu Du (University of Texas at Dallas) 
Advisory Board 
J. Birge (University of Chicago) 
C.A. Floudas (Princeton University) 
F. Giannessi (University of Pisa) 
H.D. Sherali (Virginia Polytechnic and State University) 
T. Terlaky (McMaster University) 
Y. Ye (Stanford University) 
Aims and Scope 
Optimization has been expanding in all directions at an astonishing rate 
during the last few decades. New algorithmic and theoretical techniques have 
been developed, the diffusion into other disciplines has proceeded at a rapid 
pace, and our knowledge of all aspects of the field has grown even more 
profound. At the same time, one of the most striking trends in optimization is 
the constantly increasing emphasis on the interdisciplinary nature of the field. 
Optimization has been a basic tool in all areas of applied mathematics, 
engineering, medicine, economics and other sciences. 
The series Springer Optimization and Its Applications publishes 
undergraduate and graduate textbooks, monographs and state-of-the-art 
expository works that focus on algorithms for solving optimization problems 
and also study applications involving such problems. Some of the topics 
covered include nonlinear optimization (convex and nonconvex), network 
flow 
problems, 
stochastic 
optimization, 
optimal 
control, 
discrete 
optimization, 
multi-objective 
programming, 
description 
of 
software 
packages, approximation techniques and heuristic approaches.

OPTIMIZATION THEORY AND 
METHODS
By 
WENYU SUN 
Nanjing Normal University, Nanjing, China 
YA-XIANG YUAN 
Chinese Academy of Science, Beijing, China 
1 3
Nonlinear Programming

Library of Congress Control Number: 2005042696 
Printed on acid-free paper. 
O 2006 Springer Science+Business Media, LLC 
All rights reserved. This work may not be translated or copied in whole or in part without the written 
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 
10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in 
connection with any form of information storage and retrieval, electronic adaptation, computer software, 
or by similar or dissimilar methodology now known or hereafter developed is forbidden. 
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are 
not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject 
to proprietary rights. 
Printed in the United States of America. 

Contents
Preface
xi
1
Introduction
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Mathematics Foundations . . . . . . . . . . . . . . . . . . . .
2
1.2.1
Norm
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2.2
Inverse and Generalized Inverse of a Matrix . . . . . .
9
1.2.3
Properties of Eigenvalues
. . . . . . . . . . . . . . . .
12
1.2.4
Rank-One Update
. . . . . . . . . . . . . . . . . . . .
17
1.2.5
Function and Diﬀerential
. . . . . . . . . . . . . . . .
22
1.3
Convex Sets and Convex Functions . . . . . . . . . . . . . . .
31
1.3.1
Convex Sets . . . . . . . . . . . . . . . . . . . . . . . .
32
1.3.2
Convex Functions
. . . . . . . . . . . . . . . . . . . .
36
1.3.3
Separation and Support of Convex Sets
. . . . . . . .
50
1.4
Optimality Conditions for Unconstrained Case
. . . . . . . .
57
1.5
Structure of Optimization Methods . . . . . . . . . . . . . . .
63
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2
Line Search
71
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.2
Convergence Theory for Exact Line Search
. . . . . . . . . .
74
2.3
Section Methods
. . . . . . . . . . . . . . . . . . . . . . . . .
84
2.3.1
The Golden Section Method . . . . . . . . . . . . . . .
84
2.3.2
The Fibonacci Method . . . . . . . . . . . . . . . . . .
87
2.4
Interpolation Method . . . . . . . . . . . . . . . . . . . . . . .
89
2.4.1
Quadratic Interpolation Methods . . . . . . . . . . . .
89
2.4.2
Cubic Interpolation Method . . . . . . . . . . . . . . .
98
2.5
Inexact Line Search Techniques . . . . . . . . . . . . . . . . .
102

vi
CONTENTS
2.5.1
Armijo and Goldstein Rule
. . . . . . . . . . . . . . .
103
2.5.2
Wolfe-Powell Rule
. . . . . . . . . . . . . . . . . . . .
104
2.5.3
Goldstein Algorithm and Wolfe-Powell Algorithm
. .
106
2.5.4
Backtracking Line Search . . . . . . . . . . . . . . . .
108
2.5.5
Convergence Theorems of Inexact Line Search
. . . .
109
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
3
Newton’s Methods
119
3.1
The Steepest Descent Method . . . . . . . . . . . . . . . . . .
119
3.1.1
The Steepest Descent Method . . . . . . . . . . . . . .
119
3.1.2
Convergence of the Steepest Descent Method . . . . .
120
3.1.3
Barzilai and Borwein Gradient Method
. . . . . . . .
126
3.1.4
Appendix: Kantorovich Inequality . . . . . . . . . . .
129
3.2
Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.3
Modiﬁed Newton’s Method
. . . . . . . . . . . . . . . . . . .
135
3.4
Finite-Diﬀerence Newton’s Method . . . . . . . . . . . . . . .
140
3.5
Negative Curvature Direction Method . . . . . . . . . . . . .
147
3.5.1
Gill-Murray Stable Newton’s Method
. . . . . . . . .
148
3.5.2
Fiacco-McCormick Method . . . . . . . . . . . . . . .
151
3.5.3
Fletcher-Freeman Method . . . . . . . . . . . . . . . .
152
3.5.4
Second-Order Step Rules
. . . . . . . . . . . . . . . .
155
3.6
Inexact Newton’s Method . . . . . . . . . . . . . . . . . . . .
163
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
4
Conjugate Gradient Method
175
4.1
Conjugate Direction Methods . . . . . . . . . . . . . . . . . .
175
4.2
Conjugate Gradient Method . . . . . . . . . . . . . . . . . . .
178
4.2.1
Conjugate Gradient Method . . . . . . . . . . . . . . .
178
4.2.2
Beale’s Three-Term Conjugate Gradient Method . . .
185
4.2.3
Preconditioned Conjugate Gradient Method . . . . . .
188
4.3
Convergence of Conjugate Gradient Methods
. . . . . . . . .
191
4.3.1
Global Convergence of Conjugate Gradient Methods .
191
4.3.2
Convergence Rate of Conjugate Gradient Methods . .
198
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
5
Quasi-Newton Methods
203
5.1
Quasi-Newton Methods
. . . . . . . . . . . . . . . . . . . . .
203
5.1.1
Quasi-Newton Equation . . . . . . . . . . . . . . . . .
204

CONTENTS
vii
5.1.2
Symmetric Rank-One (SR1) Update . . . . . . . . . .
207
5.1.3
DFP Update
. . . . . . . . . . . . . . . . . . . . . . .
210
5.1.4
BFGS Update and PSB Update
. . . . . . . . . . . .
217
5.1.5
The Least Change Secant Update . . . . . . . . . . . .
223
5.2
The Broyden Class . . . . . . . . . . . . . . . . . . . . . . . .
225
5.3
Global Convergence of Quasi-Newton Methods
. . . . . . . .
231
5.3.1
Global Convergence under Exact Line Search . . . . .
232
5.3.2
Global Convergence under Inexact Line Search . . . .
238
5.4
Local Convergence of Quasi-Newton Methods . . . . . . . . .
240
5.4.1
Superlinear Convergence of General Quasi-Newton Meth-
ods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
5.4.2
Linear Convergence of General Quasi-Newton Methods 250
5.4.3
Local Convergence of Broyden’s Rank-One Update . .
255
5.4.4
Local and Linear Convergence of DFP Method . . . .
258
5.4.5
Superlinear Convergence of BFGS Method . . . . . . .
261
5.4.6
Superlinear Convergence of DFP Method
. . . . . . .
265
5.4.7
Local Convergence of Broyden’s Class Methods . . . .
271
5.5
Self-Scaling Variable Metric (SSVM) Methods . . . . . . . . .
273
5.5.1
Motivation to SSVM Method . . . . . . . . . . . . . .
273
5.5.2
Self-Scaling Variable Metric (SSVM) Method . . . . .
277
5.5.3
Choices of the Scaling Factor . . . . . . . . . . . . . .
279
5.6
Sparse Quasi-Newton Methods
. . . . . . . . . . . . . . . . .
282
5.7
Limited Memory BFGS Method . . . . . . . . . . . . . . . . .
292
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
6
Trust-Region and Conic Model Methods
303
6.1
Trust-Region Methods . . . . . . . . . . . . . . . . . . . . . .
303
6.1.1
Trust-Region Methods . . . . . . . . . . . . . . . . . .
303
6.1.2
Convergence of Trust-Region Methods . . . . . . . . .
308
6.1.3
Solving A Trust-Region Subproblem . . . . . . . . . .
316
6.2
Conic Model and Collinear Scaling Algorithm . . . . . . . . .
324
6.2.1
Conic Model
. . . . . . . . . . . . . . . . . . . . . . .
324
6.2.2
Generalized Quasi-Newton Equation . . . . . . . . . .
326
6.2.3
Updates that Preserve Past Information . . . . . . . .
330
6.2.4
Collinear Scaling BFGS Algorithm . . . . . . . . . . .
334
6.3
Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . .
337
6.3.1
Tensor Method for Nonlinear Equations . . . . . . . .
337
6.3.2
Tensor Methods for Unconstrained Optimization . . .
341

viii
CONTENTS
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
7
Nonlinear Least-Squares Problems
353
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
7.2
Gauss-Newton Method . . . . . . . . . . . . . . . . . . . . . .
355
7.3
Levenberg-Marquardt Method . . . . . . . . . . . . . . . . . .
362
7.3.1
Motivation and Properties . . . . . . . . . . . . . . . .
362
7.3.2
Convergence of Levenberg-Marquardt Method . . . . .
367
7.4
Implementation of L-M Method . . . . . . . . . . . . . . . . .
372
7.5
Quasi-Newton Method . . . . . . . . . . . . . . . . . . . . . .
379
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
8
Theory of Constrained Optimization
385
8.1
Constrained Optimization Problems
. . . . . . . . . . . . . .
385
8.2
First-Order Optimality Conditions . . . . . . . . . . . . . . .
388
8.3
Second-Order Optimality Conditions . . . . . . . . . . . . . .
401
8.4
Duality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
9
Quadratic Programming
411
9.1
Optimality for Quadratic Programming
. . . . . . . . . . . .
411
9.2
Duality for Quadratic Programming
. . . . . . . . . . . . . .
413
9.3
Equality-Constrained Quadratic Programming
. . . . . . . .
419
9.4
Active Set Methods
. . . . . . . . . . . . . . . . . . . . . . .
427
9.5
Dual Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
435
9.6
Interior Ellipsoid Method
. . . . . . . . . . . . . . . . . . . .
441
9.7
Primal-Dual Interior-Point Methods
. . . . . . . . . . . . . .
445
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
451
10 Penalty Function Methods
455
10.1 Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . .
455
10.2 The Simple Penalty Function Method
. . . . . . . . . . . . .
461
10.3 Interior Point Penalty Functions
. . . . . . . . . . . . . . . .
466
10.4 Augmented Lagrangian Method . . . . . . . . . . . . . . . . .
474
10.5 Smooth Exact Penalty Functions . . . . . . . . . . . . . . . .
480
10.6 Nonsmooth Exact Penalty Functions . . . . . . . . . . . . . .
482
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490

CONTENTS
ix
11 Feasible Direction Methods
493
11.1 Feasible Point Methods
. . . . . . . . . . . . . . . . . . . . .
493
11.2 Generalized Elimination . . . . . . . . . . . . . . . . . . . . .
502
11.3 Generalized Reduced Gradient Method . . . . . . . . . . . . .
509
11.4 Projected Gradient Method . . . . . . . . . . . . . . . . . . .
512
11.5 Linearly Constrained Problems . . . . . . . . . . . . . . . . .
515
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
12 Sequential Quadratic Programming
523
12.1 Lagrange-Newton Method . . . . . . . . . . . . . . . . . . . .
523
12.2 Wilson-Han-Powell Method . . . . . . . . . . . . . . . . . . .
530
12.3 Superlinear Convergence of SQP Step
. . . . . . . . . . . . .
537
12.4 Maratos Eﬀect
. . . . . . . . . . . . . . . . . . . . . . . . . .
541
12.5 Watchdog Technique . . . . . . . . . . . . . . . . . . . . . . .
543
12.6 Second-Order Correction Step . . . . . . . . . . . . . . . . . .
545
12.7 Smooth Exact Penalty Functions . . . . . . . . . . . . . . . .
550
12.8 Reduced Hessian Matrix Method . . . . . . . . . . . . . . . .
554
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
558
13 TR Methods for Constrained Problems
561
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561
13.2 Linear Constraints . . . . . . . . . . . . . . . . . . . . . . . .
563
13.3 Trust-Region Subproblems . . . . . . . . . . . . . . . . . . . .
568
13.4 Null Space Method . . . . . . . . . . . . . . . . . . . . . . . .
571
13.5 CDT Subproblem . . . . . . . . . . . . . . . . . . . . . . . . .
580
13.6 Powell-Yuan Algorithm
. . . . . . . . . . . . . . . . . . . . .
585
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
594
14 Nonsmooth Optimization
597
14.1 Generalized Gradients . . . . . . . . . . . . . . . . . . . . . .
597
14.2 Nonsmooth Optimization Problem . . . . . . . . . . . . . . .
607
14.3 The Subgradient Method
. . . . . . . . . . . . . . . . . . . .
609
14.4 Cutting Plane Method . . . . . . . . . . . . . . . . . . . . . .
615
14.5 The Bundle Methods . . . . . . . . . . . . . . . . . . . . . . .
617
14.6 Composite Nonsmooth Function
. . . . . . . . . . . . . . . .
620
14.7 Trust Region Method for Composite Problems
. . . . . . . .
623
14.8 Nonsmooth Newton’s Method . . . . . . . . . . . . . . . . . .
628
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
634

x
CONTENTS
Appendix: Test Functions
637
§1. Test Functions for Unconstrained Optimization Problems
637
§2. Test Functions for Constrained Optimization Problems
.
638
Bibliography
649
Index
682

Preface
Optimization is a subject that is widely and increasingly used in science,
engineering, economics, management, industry, and other areas.
It deals
with selecting the best of many possible decisions in real-life environment,
constructing computational methods to ﬁnd optimal solutions, exploring the
theoretical properties, and studying the computational performance of nu-
merical algorithms implemented based on computational methods.
Along with the rapid development of high-performance computers and
progress of computational methods, more and more large-scale optimization
problems have been studied and solved. As pointed out by Professor Yuqi He
of Harvard University, a member of the US National Academy of Engineering,
optimization is a cornerstone for the development of civilization.
This book systematically introduces optimization theory and methods,
discusses in detail optimality conditions, and develops computational meth-
ods for unconstrained, constrained, and nonsmooth optimization.
Due to
limited space, we do not cover all important topics in optimization.
We
omit some important topics, such as linear programming, conic convex pro-
gramming, mathematical programming with equilibrium constraints, semi-
inﬁnite programming, and global optimization. Interested readers can refer
to Dantzig [78], Walsch [347], Shu-Cheng Fang and S. Puthenpura [121], Luo,
Pang, and Ralph [202], Wright [358], Wolkowitz, Saigal, and Vandenberghe
[355].
The book contains a lot of recent research results on nonlinear program-
ming including those of the authors, for example, results on trust region
methods, inexact Newton method, self-scaling variable metric method, conic
model method, non-quasi-Newton method, sequential quadratic program-
ming, and nonsmooth optimization, etc.. We have tried to make the book

xii
PREFACE
self-contained, systematic in theory and algorithms, and easy to read. For
most methods, we motivate the idea, study the derivation, establish the global
and local convergence, and indicate the eﬃciency and reliability of the nu-
merical performance. The book also contains an extensive, not complete,
bibliography which is an important part of the book, and the authors hope
that it will be useful to readers for their further studies.
This book is a result of our teaching experience in various universities
and institutes in China and Brazil in the past ten years. It can be used as a
textbook for an optimization course for graduates and senior undergraduates
in mathematics, computational and applied mathematics, computer science,
operations research, science and engineering. It can also be used as a reference
book for researchers and engineers.
We are indebted to the following colleagues for their encouragement, help,
and suggestions during the preparation of the manuscript: Professors Kang
Feng, Xuchu He, Yuda Hu, Liqun Qi, M.J.D. Powell, Raimundo J.B. Sam-
paio, Zhongci Shi, E. Spedicato, J. Stoer, T. Terlaky, and Chengxian Xu.
Special thanks should be given to many of our former students who read
early versions of the book and helped us in improving it.
We are grate-
ful to Edwin F. Beschler and several anonymous referees for many valuable
comments and suggestions. We would like to express our gratitude to the
National Natural Science Foundation of China for the continuous support to
our research. Finally, we are very grateful to Editors John Martindale, An-
gela Quilici Burke, and Robert Saley of Springer for their careful and patient
work.
Wenyu Sun,
Nanjing Normal University
Yaxiang Yuan,
Chinese Academy of Science
April 2005

Chapter 1
Introduction
1.1
Introduction
Optimization Theory and Methods is a young subject in applied mathemat-
ics, computational mathematics and operations research which has wide ap-
plications in science, engineering, business management, military and space
technology. The subject is involved in optimal solution of problems which are
deﬁned mathematically, i.e., given a practical problem, the “best” solution to
the problem can be found from lots of schemes by means of scientiﬁc methods
and tools. It involves the study of optimality conditions of the problems, the
construction of model problems, the determination of algorithmic method
of solution, the establishment of convergence theory of the algorithms, and
numerical experiments with typical problems and real life problems. Though
optimization might date back to the very old extreme-value problems, it did
not become an independent subject until the late 1940s, when G.B. Dantzig
presented the well-known simplex algorithm for linear programming.
Af-
ter the 1950s, when conjugate gradient methods and quasi-Newton methods
were presented, the nonlinear programming developed greatly. Now various
modern optimization methods can solve diﬃcult and large scale optimization
problems, and become an indispensable tool for solving problems in diverse
ﬁelds.
The general form of optimization problems is
min
f(x)
s.t.
x ∈X,
(1.1.1)
where x ∈Rn is a decision variable, f(x) an objective function, X ⊂Rn

2
CHAPTER 1. INTRODUCTION
a constraint set or feasible region. Particularly, if the constraint set X =
Rn, the optimization problem (1.1.1) is called an unconstrained optimization
problem:
min
x∈Rn f(x).
(1.1.2)
The constrained optimization problem can be written as follows:
minx∈Rn
f(x)
s.t.
ci(x) = 0, i ∈E,
(1.1.3)
ci(x) ≥0, i ∈I,
where E and I are, respectively, the index set of equality constraints and
inequality constraints, ci(x), (i = 1, · · · , m ∈E ∪I) are constraint functions.
When both objective function and constraint functions are linear functions,
the problem is called linear programming. Otherwise, the problem is called
nonlinear programming.
This book mainly studies solving unconstrained optimization problem
(1.1.2) and constrained optimization problem (1.1.3) from the view points of
both theory and numerical methods. Chapters 2 to 7 deal with unconstrained
optimization. Chapters 8 to 13 discuss constrained optimization. Finally, in
Chapter 14, we give a simple and comprehensive introduction to nonsmooth
optimization.
1.2
Mathematics Foundations
In this section, we shall review a number of results from linear algebra and
analysis which are useful in optimization theory and methods.
Throughout this book, Rn will denote the real n-dimensional linear space
of column vector x with components x1, · · · , xn, and Cn the corresponding
space of complex column vectors. For x ∈Rn, xT denotes the transpose of
x, while, for x ∈Cn, xH is the conjugate transpose. A real m × n matrix
A = (aij) deﬁnes a linear mapping from Rn to Rm and will be written as
A ∈Rm×n or A ∈L(Rn, Rm) to denote either the matrix or the linear
operator. Similarly, a complex m × n matrix A will be written as A ∈Cm×n
or A ∈L(Cn, Cm).

1.2. MATHEMATICS FOUNDATIONS
3
1.2.1
Norm
Deﬁnition 1.2.1 A mapping ∥· ∥is called a norm if and only if it satisﬁes
the following properties:
(i) ∥x∥≥0, ∀x ∈Rn; ∥x∥= 0 if and only if x = 0;
(ii) ∥αx∥= |α|∥x∥, ∀α ∈R, x ∈Rn;
(iii) ∥x + y∥≤∥x∥+ ∥y∥, ∀x, y ∈Rn.
Well-known examples of vector norm are as follows:
∥x∥∞= max
1≤i≤n |xi|,
(l∞-norm)
(1.2.1)
∥x∥1 =
n

i=1
|xi|,
(l1-norm)
(1.2.2)
∥x∥2 =
 n

i=1
|xi|2
1/2
,
(l2-norm).
(1.2.3)
The above examples are particular cases of lp-norm which is deﬁned as
∥x∥p =
 n

i=1
|xi|p
1/p
, (lp-norm).
(1.2.4)
Another vector norm frequently used is the ellipsoid norm which is deﬁned
as
∥x∥A = (xT Ax)1/2,
(1.2.5)
where A ∈Rn×n is a symmetric and positive deﬁnite matrix.
Similarly, we can deﬁne a matrix norm.
Deﬁnition 1.2.2 Let A, B ∈Rm×n. A mapping ∥· ∥: Rm×n →R is said to
be a matrix norm if it satisﬁes the properties
(i) ∥A∥≥0, ∀A ∈Rm×n; ∥A∥= 0 if and only if A = 0;
(ii) ∥αA∥= |α|∥A∥, ∀α ∈R, A ∈Rm×n;
(iii) ∥A + B∥≤∥A∥+ ∥B∥, ∀A, B ∈Rm×n.

4
CHAPTER 1. INTRODUCTION
Corresponding to the above vector lp-norm, we have the matrix lp-norm:
∥A∥p = sup
x̸=0
∥Ax∥p
∥x∥p
= max
∥x∥p=1 ∥Ax∥p
(1.2.6)
which is said to be induced by, or subordinate to, the vector lp-norm. In
particular,
∥A∥1 = max
1≤j≤n
m

i=1
|aij|,
(maximum column norm)
(1.2.7)
∥A∥∞= max
1≤i≤m
n

j=1
|aij|,
(maximum row norm)
(1.2.8)
∥A∥2 =

λmax(AT A)
1/2 ,
(spectral norm.)
(1.2.9)
Obviously, we have
∥A−1∥p =
1
minx̸=0
∥Ax∥p
∥x∥p
.
For an induced matrix norm, we always have ∥I∥= 1, where I is an n × n
identity matrix. More generally, for any vector norm ∥· ∥α on Rn and ∥· ∥β
on Rm, the matrix norm is deﬁned by
∥A∥α,β = sup
x̸=0
∥Ax∥β
∥x∥α
.
(1.2.10)
The most frequently used matrix norms also include the Frobenius norm
∥A∥F =
⎛
⎝
m

i=1
n

j=1
|aij|2
⎞
⎠
1/2
= [tr(AT A)]1/2,
(1.2.11)
where tr(·) denotes the trace of a square matrix with tr(A) = 
n
i=1 aii. The
trace satisﬁes
1. tr(αA + βB) = αtr(A) + βtr(B);
2. tr(AT ) = tr(A);
3. tr(AB) = tr(BA);
4. tr(A) = 
n
i=1 λi if the eigenvalues of A are denoted by λ1, · · · , λn.

1.2. MATHEMATICS FOUNDATIONS
5
The weighted Frobenius norm and weighted l2-norm are deﬁned, respec-
tively, as
∥A∥M,F = ∥MAM∥F , ∥A∥M,2 = ∥MAM∥2,
(1.2.12)
where M is an n × n symmetric and positive deﬁnite matrix.
Further, let A ∈Rn×n; if we deﬁne ∥x∥′ = ∥Px∥for all x ∈Rn and P an
arbitrary nonsingular matrix, then
∥A∥′ = ∥PAP −1∥.
(1.2.13)
The orthogonally invariant matrix norm is a class of important norms
which satisﬁes, for A ∈Rm×n and U an m × m orthogonal matrix, the
identity
∥UA∥= ∥A∥.
(1.2.14)
Clearly, the l2-norm and the Frobenius norm are orthogonally invariant ma-
trix norms.
A vector norm ∥· ∥and a matrix norm ∥· ∥′ are said to be consistent if,
for every A ∈Rm×n and x ∈Rn,
∥Ax∥≤∥A∥′∥x∥.
(1.2.15)
Obviously, the lp-norm has this property, i.e.,
∥Ax∥p ≤∥A∥p∥x∥p.
(1.2.16)
More generally, for any vector norm ∥· ∥α on Rn and ∥· ∥β on Rm we have
∥Ax∥β ≤∥A∥α,β∥x∥α,
(1.2.17)
where ∥A∥α,β is deﬁned by
∥A∥α,β = sup
x̸=0
∥Ax∥β
∥x∥α
(1.2.18)
which is subordinate to the vector norm ∥· ∥α and ∥· ∥β.
Likewise, if a norm ∥· ∥satisﬁes
∥AB∥≤∥A∥∥B∥,
(1.2.19)
we say that the matrix norm satisﬁes the consistency condition (or submulti-
plicative property). It is easy to see that the Frobenius norm and the induced
matrix norms satisfy the consistency condition, and we have
∥AB∥F ≤min{∥A∥2∥B∥F , ∥A∥F ∥B∥2}.
(1.2.20)

6
CHAPTER 1. INTRODUCTION
Next, about the equivalence of norms, we have
Deﬁnition 1.2.3 Let ∥·∥α and ∥·∥β be two arbitrary norms on Rn. If there
exist µ1, µ2 > 0, such that
µ1∥x∥α ≤∥x∥β ≤µ2∥x∥α, ∀x ∈Rn,
(1.2.21)
we say that the norms ∥· ∥α and ∥· ∥β are equivalent.
In particular, we have
∥x∥2 ≤∥x∥1 ≤√n∥x∥2,
(1.2.22)
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
(1.2.23)
∥x∥∞≤∥x∥1 ≤n∥x∥∞,
(1.2.24)
∥x∥∞≤∥x∥2 ≤∥x∥1,
(1.2.25)
√
λ∥x∥2 ≤∥x∥A ≤
√
Λ∥x∥2,
(1.2.26)
where λ and Λ are the smallest and the largest eigenvalues of A respectively.
For A ∈Rm×n, we have
∥A∥2 ≤∥A∥F ≤√n∥A∥2,
(1.2.27)
max
i,j |aij| ≤∥A∥2 ≤√mn max
i,j |aij|,
(1.2.28)
1
√n∥A∥∞≤∥A∥2 ≤√m∥A∥∞,
(1.2.29)
1
√m∥A∥1 ≤∥A∥2 ≤√n∥A∥1.
(1.2.30)
By use of norms, it is immediate to introduce the notation of distance.
Let x, y ∈Rn, the distance between two points x and y is deﬁned by ∥x−y∥.
In particular, in the 2-norm, if x = (x1, · · · , xn)T , y = (y1, · · · , yn)T , then
∥x −y∥2 =

(x1 −y1)2 + · · · + (xn −yn)2
which is just a direct generalization of distance in analytical geometry.
Obviously, by Deﬁnition 1.2.1, we have the following properties of dis-
tance:
1. ∥x −y∥≥0, ∥x −y∥= 0 if and only if x = y.

1.2. MATHEMATICS FOUNDATIONS
7
2. ∥x −z∥≤∥x −y∥+ ∥y −z∥.
3. ∥x −y∥= ∥y −x∥.
A vector sequence {xk} is said to be convergent to x∗if
lim
k→∞∥xk −x∗∥= 0.
(1.2.31)
A matrix sequence {Ak} is said to be convergent to A if
lim
k→∞∥Ak −A∥= 0.
(1.2.32)
Choice of norms is irrelevant since all norms in ﬁnite dimension space are
equivalent.
Deﬁnition 1.2.4 A sequence {xk} ⊂Rn is said to be a Cauchy sequence if
lim
m,l→∞∥xm −xl∥= 0;
(1.2.33)
i.e., given ϵ > 0, there is an integer N such that ∥xm −xl∥< ϵ for all
m, l > N.
In Rn, a sequence {xk} converges if and only if the sequence {xk} is a
Cauchy sequence . However, in a normed space, a Cauchy sequence may not
be convergent.
We conclude this subsection with several inequalities on norms.
(1) Cauchy-Schwarz inequality :
|xT y| ≤∥x∥2∥y∥2,
(1.2.34)
the equality holds if and only if x and y are linearly dependent.
(2) Let A be an n × n symmetric and positive deﬁnite matrix, then the
inequality
|xT Ay| ≤∥x∥A∥y∥A
(1.2.35)
holds; the equality holds if and only if x and y are linearly dependent.
(3) Let A be an n × n symmetric and positive deﬁnite matrix, then the
inequality
|xT y| ≤∥x∥A∥y∥A−1
(1.2.36)
holds; the equality holds if and only if x and A−1y are linearly dependent.

8
CHAPTER 1. INTRODUCTION
(4) Young inequality: Assume that real numbers p and q are each larger
than 1, and 1
p + 1
q = 1. If x and y are also real numbers, then
xy ≤xp
p + yq
q ,
(1.2.37)
and equality holds if and only if xp = yq.
Proof.
Set s = xp and t = yq. From the arithmetic-geometry inequality,
we immediately have
xy = s1/pt1/q ≤s
p + t
q = xp
p + yq
q .
Further, the equality holds if and only if s = t, i.e., xp = yq.
2
(5) H¨older inequality:
|xT y| ≤∥x∥p∥y∥q =
 n

i=1
|xi|p
1/p  n

i=1
|yi|q
1/q
,
(1.2.38)
where p and q are real numbers larger than 1 and satisfy 1
p + 1
q = 1.
Proof.
If x = 0 or y = 0, the result is trivial. Now we assume that both
x and y are not zero. From Young inequality, we have
|xiyi|
∥x∥p∥y∥q
≤1
p
|xi|p
∥x∥p
p
+ 1
q
|yi|q
∥y∥q
q
, i = 1, · · · , n.
Taking the sum over i on both sides of the above inequality yields
1
∥x∥p∥y∥q
n

i=1
|xiyi|
≤
1
p∥x∥p
p
n

i=1
|xi|p +
1
q∥y∥q
q
n

i=1
|yi|q
=
1
p + 1
q
=
1.
2
Multiplying ∥x∥p∥y∥q on both sides gives our result.
(6) Minkowski inequality:
∥x + y∥p ≤∥x∥p + ∥y∥p,
(1.2.39)

1.2. MATHEMATICS FOUNDATIONS
9
i.e.,
 n

i=1
|xi + yi|p
1/p
≤
 n

i=1
|xi|p
1/p
+
 n

i=1
|yi|p
1/p
,
(1.2.40)
where p ≥1.
The proof of this inequality will be given in §1.3.2 as an
application of the convexity of a function.
1.2.2
Inverse and Generalized Inverse of a Matrix
In this subsection we collect some basic results of inverse and generalized
inverse.
Theorem 1.2.5 (Von-Neumann Lemma) Let ∥· ∥be a consistent matrix
norm with ∥I∥= 1. Let E ∈Rn×n. If ∥E∥< 1, then I −E is nonsingular,
and
(I −E)−1 =
∞

k=0
Ek,
(1.2.41)
∥(I −E)−1∥≤
1
1 −∥E∥.
(1.2.42)
If A ∈Rn×n is nonsingular and ∥A−1(B −A)∥< 1, then B is nonsingular
and satisﬁes
B−1 =
∞

k=0
(I −A−1B)kA−1,
(1.2.43)
and
∥B−1∥≤
∥A−1∥
1 −∥A−1(B −A)∥.
(1.2.44)
Proof.
Since ∥E∥< 1, then
Sk
∆= I + E + E2 + · · · + Ek
deﬁnes a Cauchy sequence , and hence Sk is convergent. So,
∞

k=0
Ek = lim
k→∞Sk = (I −E)−1
which proves (1.2.41)-(1.2.42).

10
CHAPTER 1. INTRODUCTION
Since A is nonsingular and ∥A−1(B −A)∥= ∥−(I −A−1B)∥< 1, by
setting E = I −A−1B and using (1.2.41) and (1.2.42), we obtain immediately
(1.2.43) and (1.2.44).
2
This theorem indicates that the matrix B is invertible if B is suﬃciently
approximate to an invertible matrix A.
The above theorem also can be
written in the following form which sometimes is said to be the perturbation
theorem:
Theorem 1.2.6 Let A, B ∈Rn×n. Assume that A is invertible with ∥A−1∥≤
α. If ∥A −B∥≤β and αβ < 1, then B is also invertible, and
∥B−1∥≤
α
1 −αβ .
(1.2.45)
Let L and M be subspaces of Rn. The sum of two subspaces L and M is
deﬁned as
L + M = {x = y + z | y ∈L, z ∈M}.
(1.2.46)
The intersection of two subspaces L and M is deﬁned as
L ∩M = {x | x ∈L and x ∈M}.
(1.2.47)
Two subspaces L and M are orthogonal, denoted by L ⊥M, if
< y, z >= 0,
∀y ∈L, ∀z ∈M.
Rn is said to be a direct sum of L and M, denoted by
Rn = L ⊕M,
if and only if Rn = L + M and L ∩M = {0}.
Let Rn = L ⊕M. If a linear operator P : Rn →Rn satisﬁes
Py = y, ∀y ∈L; Pz = 0, ∀z ∈M,
then P is called a projector of Rn onto the subspace L along the subspace
M. Such a projector is denoted by PL,M or P. If M ⊥L, then the above
projector is called an orthogonal projector, denoted by PL or P.
Normally, Cm×n denotes a set of all complex m × n matrices, Cm×n
r
denotes a set of all complex m × n matrices with rank r. A∗denotes the
conjugate transpose of a matrix A. For a real matrix, Rm×n and Rm×n
r
have

1.2. MATHEMATICS FOUNDATIONS
11
similar meaning. Now we present some deﬁnitions and representations of the
generalized inverse of a matrix A.
Let A ∈Cm×n. Then A+ ∈Cn×m is a Moore-Penrose generalized inverse
of A if
AA+A = A, A+AA+ = A+, (AA+)∗= AA+, (A+A)∗= A+A,
(1.2.48)
or equivalently,
AA+ = PR(A), A+A = PR(A+),
(1.2.49)
where PR(A) and PR(A+) are the orthogonal projectors on range R(A) and
R(A+) respectively.
If A ∈Cm×n
r
and A has the orthogonal decomposition
A = Q∗RP,
(1.2.50)
where Q and P are m×m and n×n unitary matrices respectively, R ∈Cm×n,
R =

R11
0
0
0

,
where R11 is the r × r nonsingular upper triangular matrix, then
A+ = P ∗R+Q,
(1.2.51)
where
R+ =

R−1
11
0
0
0

.
Similarly, if A ∈Cm×n
r
has the singular value decomposition (SVD)
A = UDV ∗,
(1.2.52)
where U and V are m × m and n × n unitary matrices respectively,
D =

Σ
0
0
0

∈Cm×n,
where Σ = diag(σ1, · · · , σr), σi > 0 (i = 1, · · · , r) are the nonzero singular
values of A, then
A+ = V D+U∗,
(1.2.53)

12
CHAPTER 1. INTRODUCTION
where
D+ =

Σ−1
0
0
0

.
An important role of the generalized inverse is that it oﬀers the solution
of general linear equations (including singular, rectangular, or inconsistent
case). In the following we state this theorem and prove it by the singular
value decomposition.
Theorem 1.2.7 Let A ∈Cm×n, b ∈Cm.
Then ¯x = A+b is the unique
solution of Ax = b, i.e.,
∥¯x∥≤∥x∥, ∀x ∈{x | ∥Ax −b∥≤∥Az −b∥, ∀z ∈Cn}.
(1.2.54)
Such an ¯x is called the minimal least-squares solution of Ax = b.
Proof.
From the singular value decomposition (1.2.52), (1.2.54) is equiv-
alent to
min
x∈Rn{∥V ∗x∥| ∥DV ∗x −U∗b∥≤∥DV ∗z −U∗b∥, ∀z ∈Rn}
i.e., for y = V ∗x,
min
y∈Rn{∥y∥| ∥Dy −U∗b∥≤∥Dˆz −U∗b∥, ∀ˆz ∈Rn}.
(1.2.55)
Since
∥Dy −U ∗b∥2 =
r

i=1
(σiyi −(U∗b)i)2 +
m

i=r+1
((U∗b)i)2
which is minimized by any y with yi = (U∗b)i/σi, (i = 1, · · · , r) and ∥y∥
is minimized by setting yi = 0 (i = r + 1, · · · , m), then y = D+U∗b is the
minimal least-squares solution of (1.2.55). Therefore ¯x = V D+U∗b = A+b is
the minimal least-squares solution of Ax = b.
2
1.2.3
Properties of Eigenvalues
In this subsection we state, in brief, some properties of eigenvalues and eigen-
vectors that we will use in the text. We also summarize the deﬁnitions of pos-
itive deﬁnite, negative deﬁnite and indeﬁnite symmetric matrices and their
characterizations in terms of eigenvalues.

1.2. MATHEMATICS FOUNDATIONS
13
The eigenvalue problem of a matrix A is that
Ax = λx, A ∈Rn×n, x ̸= 0, x ∈Rn,
(1.2.56)
where λ is called an eigenvalue of A, x an eigenvector of A corresponding to
λ, (λ, x) an eigen-pair of A.
The spectral radius of A is deﬁned as
ρ(A) = max
1≤i≤n |λi|.
Let A ∈Rm×n have singular values σ1 ≥σ2 ≥· · · ≥σn, then
∥A∥2 = σ1,
∥A∥2
F = σ2
1 + · · · + σ2
n.
In particular, if A ∈Rn×n is symmetric with eigenvalues λ1, · · · , λn, then
∥A∥2 = max
1≤i≤n |λi|.
Then we immediately have that if A is nonsingular, the condition number of
A is
κ(A) = σ1
σn
;
in addition, if A is symmetric, then
κ(A) = maxi |λi|
mini |λi| .
Let A ∈Rn×n with eigenvalues λ1, · · · , λn. We have the following conclu-
sions about the eigenvalues.
1. The eigenvectors corresponding to the distinct eigenvalues of A are
independent.
2. A is diagonalizable if and only if, for each eigenvalue of A, its geometric
multiplicity is equal to the algebraic multiplicity, i.e., the dimension of
its corresponding eigenvectors is equal to the multiplicity of the eigen-
value.
3. Let f(A) be a polynomial of A. If (λ, x) is an eigen-pair of A, then
(f(λ), x) is the eigen-pair of f(A).

14
CHAPTER 1. INTRODUCTION
4. Let B = PAP −1, where P ∈Rn×n is a nonsingular transformation
matrix. If (λ, x) is an eigen-pair of A, then (λ, Px) is the eigen-pair
of B. This means that the similar transformation does not change the
eigenvalues of a matrix.
Deﬁnition 1.2.8 Let A ∈Rn×n be symmetric.
A is said to be positive
deﬁnite if vT Av > 0, ∀v ∈Rn, v ̸= 0. A is said to be positive semideﬁnite if
vT Av ≥0, ∀v ∈Rn. A is said to be negative deﬁnite or negative semideﬁnite
if −A is positive deﬁnite or positive semideﬁnite. A is said to be indeﬁnite
if it is neither positive semideﬁnite nor negative semideﬁnite.
The main properties of a symmetric matrix are as follows. Let A ∈Rn×n
be symmetric. Then
(1) All eigenvalues of A are real.
(2) The eigenvectors corresponding to the distinct eigenvalues of A are or-
thogonal.
(3) A is orthogonally similar to a diagonal matrix, i.e., there exists an n × n
orthogonal matrix Q such that
Q−1AQ = QT AQ =
⎡
⎢⎣
λ1
...
λn
⎤
⎥⎦,
where λ1, · · · , λn are the eigenvalues of A. This means a symmetric
matrix has an orthonormal eigenvector system.
The following properties are about symmetric positive deﬁnite, symmetric
positive semideﬁnite, and so on.
Let A ∈Rn×n be symmetric. Then A is positive deﬁnite if and only if
all its eigenvalues are positive. A is positive semideﬁnite if and only if all its
eigenvalues are nonnegative. A is negative deﬁnite or negative semideﬁnite
if and only if all its eigenvalues are negative or nonpositive. A is indeﬁnite
if and only if it has both positive and negative eigenvalues. Furthermore,
A is positive deﬁnite if and only if A has a unique Cholesky factorization
A = LDLT with all positive diagonal elements of D.
The following is the deﬁnition of the Rayleigh quotient of a matrix and
its properties.

1.2. MATHEMATICS FOUNDATIONS
15
Deﬁnition 1.2.9 Let A be an n × n Hermitian matrix and u ∈Cn. Then
the Rayleigh quotient of A is deﬁned by
Rλ(u) = u∗Au
u∗u , u ̸= 0.
(1.2.57)
Theorem 1.2.10 Let A be an n × n Hermitian matrix and u ∈Cn. Then
the Rayleigh quotient deﬁned by (1.2.57) has the following basic properties:
(i) Homogeneous Property:
Rλ(αu) = Rλ(u), α ̸= 0.
(1.2.58)
(ii) Extreme Property:
λ1
=
max
∥u∥2=1 u∗Au = max
u̸=0
u∗Au
u∗u ,
(1.2.59)
λn
=
min
∥u∥2=1 u∗Au = min
u̸=0
u∗Au
u∗u ,
(1.2.60)
which show that the Rayleigh quotient has bounded property:
λn ≤Rλ(u) ≤λ1.
(1.2.61)
(iii) Minimal Residual Property: for any u ∈Cn,
∥(A −Rλ(u)I)u∥≤∥(A −µI)u∥, ∀real number µ.
(1.2.62)
Proof.
Property (i) is immediate from Deﬁnition 1.2.9. Now we consider
Property (ii). By Property (i), we can consider the Rayleigh quotient on a
unit sphere, i.e.,
Rλ(u) = u∗Au, ∥u∥2 = 1.
Let T be a unitary matrix such that T ∗AT = Λ, where Λ is a diagonal matrix.
Also let u = Ty, then
u∗Au = y∗Λy =
n

i=1
λi|yi|2

≥λn

n
i=1 |yi|2,
≤λ1

n
i=1 |yi|2.
Note that ∥u∥2 = ∥y∥2 = 1, hence the boundedness follows. Furthermore,
when y1 = 1 and yi = 0, i ̸= 1, λ1 is the maximum; when yn = 1 and
yi = 0, i ̸= n, λn is the minimum. This proves Property (ii).

16
CHAPTER 1. INTRODUCTION
To establish Property (iii), we deﬁne
s(u) = Au −Rλ(u)u, u ̸= 0,
(1.2.63)
which implies that
Au = Rλ(u)u + s(u).
(1.2.64)
By Deﬁnition 1.2.9, we have ⟨s(u), u⟩= ⟨Au −Rλ(u)u, u⟩= 0 which means
that the decomposition (1.2.64) is an orthogonal decomposition. Thus Rλ(u)u
is an orthogonal projection of Au on L = {u}, which shows that the residual
deﬁned by (1.2.63) has the minimal residual Property (iii).
2
Next, we state some concepts of reducible and irreducible matrices which
are useful in discussing invertibility and positive deﬁniteness of a matrix.
Deﬁnition 1.2.11 Let A ∈Rn×n. A is said to be reducible if there is a
permutation matrix P such that
PAP T =

B11
B12
0
B22

,
where B11 and B22 are square matrices; A is irreducible if it is not reducible.
Equivalently, A is reducible if and only if there is a nonempty subset of indices
J ⊂{1, · · · , n} such that
akj = 0, ∀k ∈J, j /∈J.
Deﬁnition 1.2.12 Let A ∈Rn×n. A is said to be diagonally dominant if
n

j=1,j̸=i
|aij| ≤|aii|, i = 1, · · · , n.
(1.2.65)
A is said to be strictly diagonally dominant if strict inequality holds in (1.2.65)
for all i. A is said to be irreducibly diagonally dominant if it is irreducible,
diagonally dominant, and strict inequality holds in (1.2.65) for at least one
i.
The above concepts give an important theorem which is called the Diag-
onal Dominant Theorem.
Theorem 1.2.13 (Diagonal Dominant Theorem) Let A ∈Rn×n be either
strictly or irreducibly diagonal dominant. Then A is invertible.

1.2. MATHEMATICS FOUNDATIONS
17
As a corollary of the above theorem, we state the Gerschgorin circle The-
orem which gives an isolation property of eigenvalues.
Theorem 1.2.14 Let A ∈Cn×n. Deﬁne the i-th circle as
Di = {λ | |λ −aii| ≤
n

j=1,j̸=i
|aij|}, i = 1, · · · , n.
Then each eigenvalue of A lies in the union S = ∪n
i=1Di. This also means
that
min
i
λi ≥min
i {aii −
n

j=1,j̸=i
|aij|}
and
max
i
λi ≤max
i {aii +
n

j=1,j̸=i
|aij|}.
1.2.4
Rank-One Update
The rank-one update of matrices is often used in optimization. In this sub-
section we introduce inverse of rank-one update, determinant of rank-one
update, chain of the eigenvalues of rank-one update, and updating matrix
factorizations. Detailed proofs can be found in books on linear algebra or
numerical linear algebra.
The following theorem due to Sherman and Morrison is wellknown.
Theorem 1.2.15 Let A ∈Rn×n be nonsingular and u, v ∈Rn be arbitrary.
If
1 + vT A−1u ̸= 0,
(1.2.66)
then the rank-one update A + uvT of A is nonsingular, and its inverse is
represented by
(A + uvT )−1 = A−1 −A−1uvT A−1
1 + vT A−1u.
(1.2.67)
An interesting generalization of the above theorem is
Theorem 1.2.16 (Sherman-Morrison-Woodburg Theorem)
Let A be an n×n nonsingular matrix, U, V n×m matrices. If I+V ∗A−1U
is invertible, then A + UV ∗is invertible, and
(A + UV ∗)−1 = A−1 −A−1U(I + V ∗A−1U)−1V ∗A−1.
(1.2.68)

18
CHAPTER 1. INTRODUCTION
Consider the determinant of a rank-one update; we have
det(I + uvT ) = 1 + uT v.
(1.2.69)
In fact, assuming u ̸= 0, we have that the eigenvectors of I + uvT are either
orthogonal to v or parallel to u. If they are orthogonal to v, the corresponding
eigenvalues are 1; otherwise the corresponding eigenvalue is 1 + uT v. Hence
(1.2.69) follows.
Furthermore, for the determinant of rank-two update, we have the fol-
lowing result:
det(I + u1uT
2 + u3uT
4 )
=
(1 + uT
1 u2)(1 + uT
3 u4) −(uT
1 u4)(uT
2 u3).
(1.2.70)
In fact, as long as we note that
I + u1uT
2 + u3uT
4 = (I + u1uT
2 )[I + (I + u1uT
2 )−1u3uT
4 ],
it follows from (1.2.69) and (1.2.67) that
det(I + u1uT
2 + u3uT
4 )
=
(1 + uT
1 u2)[1 + uT
4 (I + u1uT
2 )−1u3]
=
(1 + uT
1 u2)

1 + uT
4

I −
u1uT
2
1 + uT
1 u2

u3

=
(1 + uT
1 u2)(1 + uT
3 u4) −(uT
1 u4)(uT
2 u3).
By ∥A∥2
F = tr(AT A), where tr(·) denotes the trace of a matrix, it follows
that the Frobenius norm of rank-one update A + xyT is
∥A + xyT ∥2
F = ∥A||2
F + 2yT AT x + ∥x∥2∥y∥2.
(1.2.71)
About the chain of the eigenvalues of rank-one update, we have the fol-
lowing theorem.
Theorem 1.2.17 Let A be an n×n symmetric matrix with eigenvalues λ1 ≥
λ2 ≥· · · ≥λn. Also let ¯A = A + σuuT with eigenvalues ¯λ1 ≥¯λ2 ≥· · · ≥¯λn,
where u ∈Rn. Then we have the conclusions:
(i) if σ > 0, then
¯λ1 ≥λ1 ≥¯λ2 ≥λ2 ≥· · · ≥¯λn ≥λn.
(ii) if σ < 0, then
λ1 ≥¯λ1 ≥λ2 ≥¯λ2 ≥· · · ≥λn ≥¯λn.

1.2. MATHEMATICS FOUNDATIONS
19
Next, we discuss updating matrix factorizations which conclude updates
of Cholesky factorization and orthogonal decomposition.
Let B and ¯B be n × n symmetric and positive deﬁnite matrices,
¯B = B + αyyT , B = LDLT .
(1.2.72)
We can ﬁnd the Cholesky factorization ¯B = ¯L ¯D¯LT as follows:
¯B
=
B + αyyT
=
L(D + αppT )LT ,
(1.2.73)
where p solves Lp = y. Note that since D+αppT is a positive deﬁnite matrix
with the Cholesky factorization D + αppT = ˆL ˆDˆLT , we have
¯B = LˆL ˆDˆLT LT = ¯L ¯D¯LT ,
(1.2.74)
where ¯L = LˆL, ¯D = ˆD. The following algorithm gives the steps for computing
¯L and ¯D.
Algorithm 1.2.18 (Cholesky Factorization of Rank-One Update)
1. Set α1 = α, w(1) = y.
2. For j = 1, 2, · · · , n, compute
pj = w(j)
j ,
¯dj = dj + αjp2
j,
βj = pjαj/ ¯dj,
αj+1 = djαj/ ¯dj,
w(j+1)
r
= w(j)
r
−pjlrj, r = j + 1, · · · , n,
¯lrj = lrj + βjw(j+1)
r
, r = j + 1, · · · , n. 2
Similarly, for the negative rank-one update of Cholesky factorization , we
have
¯B
=
B −yyT = L(D −ppT )LT
=
LˆL ˆDˆLT LT = ¯L ¯D¯LT .
(1.2.75)
Since, in this case, it is possible that the elements of ¯D become zero or
negative due to round-oﬀerror, this phenomenon must be taken into consid-
eration. The following algorithm keeps all ¯dj (j = 1, · · · , n) positive.

20
CHAPTER 1. INTRODUCTION
Algorithm 1.2.19 (Cholesky Factorization of Negative Rank-One Update)
1. Solve Lp = y for p. Set tn+1 = 1 −pT D−1p. If tn+1 < ϵM,
set tn+1 = ϵM, where ϵM is the relative precision of the
computer.
2. For j = n, n −1, · · · , 1, compute
tj = tj+1 + p2
j/dj,
¯dj = djtj+1/tj,
βj = −pj/(djtj+1),
w(j)
j
= pj,
¯lrj = lrj + βjw(j+1)
r
, r = j + 1, · · · , n.,
w(j)
r
= w(j+1)
r
+ pjlrj, r = j + 1, · · · , n. 2
Furthermore, Algorithm 1.2.18 and Algorithm 1.2.19 about Cholesky fac-
torization of rank-one update can be used to compute the Cholesky factor-
ization of rank-two update. Consider
¯B = B + vwT + wvT .
(1.2.76)
Setting
x = (v + w)/
√
2, y = (v −w)/
√
2
(1.2.77)
yields
¯B = B + xxT −yyT ,
(1.2.78)
so, we can use Algorithm 1.2.18 and Algorithm 1.2.19 to get the Cholesky
factorization of ¯B.
Below, we consider the special cases of rank-two update. Let B be an n×n
symmetric positive deﬁnite matrix with Cholesky factorization B = LDLT .
Consider the case adding one row and one column to B:
¯B =

B
b
bT
θ

,
(1.2.79)
where b ∈Rn and θ is a number. If we set
ˆB =

B
0
0
θ

,
(1.2.80)

1.2. MATHEMATICS FOUNDATIONS
21
then we have
¯B = ˆB + en+1

b
0
T
+

b
0

eT
n+1.
(1.2.81)
So, we can use the above algorithm to compute Cholesky factors ¯L and ¯D of
¯B. In addition, we can show that ¯L and ¯D have the following forms:
¯L =

L
0
lT
1

, ¯D =

D
0
0
d

.
(1.2.82)
In fact, it is enough to consider

L
0
lT
1
 
D
0
0
d
 
LT
l
0
1

=

B
b
bT
θ

(1.2.83)
and solve the equations obtained
LDl = b,
d = θ −lT Dl
(1.2.84)
for l and d. Then we get ¯L and ¯D from (1.2.82).
Now we consider the case deleting the j-th row and j-th column from B.
Let B = LDLT with the form
B =
⎡
⎢⎢⎣
B1
...
B2
· · ·
·
· · ·
BT
2
...
B3
⎤
⎥⎥⎦←j-th row.
(1.2.85)
Deﬁne
¯B =

B1
B2
BT
2
B3

} n −1columns.
(1.2.86)
The algebraic operations give
¯B = ˆLDˆLT ,
(1.2.87)
which is our desired result, where ˆL is an (n −1) × n matrix obtained by
deleting the j-th row from L.
In the above, we discussed the Cholesky factorization of rank-one up-
date. Next, we handle the QR factorization of rank-one update. Let A, ¯A ∈
Rn×n, u, v ∈Rn,
A = QR, ¯A = A + uvT .
(1.2.88)

22
CHAPTER 1. INTRODUCTION
Then we have
¯A = QR + uvT = Q(R + wvT ),
(1.2.89)
where w = QT u. Forming QR decomposition
R + wvT = ˜Q ˜R,
we have
¯A = Q ˜Q ˜R ∆= ¯Q ¯R,
(1.2.90)
where ¯Q = Q ˜Q, ¯R = ˜R.
Similarly, if m × n matrix A (m < n) has an orthogonal decomposition
A = [L 0]Q,
(1.2.91)
where L is an m×m unit lower triangular matrix and Q is an n×n orthogonal
matrix with QT Q = I, then we can obtain the LQ decomposition of
¯A = A + xyT
(1.2.92)
as follows.
¯A
=
A + xyT
=
[L 0]Q + xyT
=
([L 0] + xwT )Q
(where w = Qy)
=
([L 0] + xwT )P T PQ
(where P T P = I)
=
([H 0] + αxeT
1 )PQ
(where Pw = αe1, H = LP T )
=
[ ¯H 0]PQ
=
[ ¯H 0] ¯P ¯P T PQ
(where ¯P ¯P T = I)
=
[¯L 0] ¯P T PQ
(where [ ¯H 0] ¯P = [¯L 0])
=
[¯L 0] ¯Q
(where ¯Q = ¯P T PQ).
(1.2.93)
1.2.5
Function and Diﬀerential
This subsection presents some materials of set theory and multivariable cal-
culus background.
Give a point x ∈Rn and a δ > 0. The δ-neighborhood of x is deﬁned as
Nδ(x) = {y ∈Rn | ∥y −x∥< δ}.

1.2. MATHEMATICS FOUNDATIONS
23
Let D ⊂Rn and x ∈D. The point x is said to be an interior point of
D if there exists a δ-neighborhood of x such that Nδ(x) ⊂D. The set of all
such points is called the interior of D and is denoted by int(D). Obviously,
int(D) ⊂D. Furthermore, if int(D) = D, i.e., every point of D is the interior
point of D, then D is an open set.
x ∈D ⊂Rn is said to be an accumulation point if for each δ > 0, D ∩
Nδ(x) ̸= φ, where φ is an empty set. It means that there exists a subsequence
{xnk} ⊂D, such that xnk →x. The set of all such points is called the closure
of D and is denoted by ¯D. Obviously, D ⊂¯D. Furthermore, if D = ¯D, i.e.,
every accumulation point of D is contained in D, then D is said to be closed.
It is also clear that a set D ⊂Rn is closed if and only if its complement is
open.
A set D ⊂Rn is said to be compact if it is bounded and closed. For every
sequence {xk} in a compact set D, there exists a convergent subsequence
with a limit in D.
A function f : Rn →R is said to be continuous at ¯x ∈Rn if, for any given
ϵ > 0, there exists δ > 0 such that ∥x −¯x∥< δ implies |f(x) −f(¯x)| < ϵ. It
can also be written as follows: ∀ϵ > 0, ∃δ > 0, such that ∀x ∈Nδ(¯x), we have
f(x) ∈Nϵ(f(¯x)). If f is continuous at every point in an open set D ⊂Rn,
then f is said to be continuous on D.
A continuous function f : Rn →R is said to be continuously diﬀerentiable
at x ∈Rn, if

∂f
∂xi

(x) exists and is continuous, i = 1, · · · , n. The gradient
of f at x is deﬁned as
∇f(x) =
 ∂f
∂x1
(x), · · · , ∂f
∂xn
(x)
T
.
(1.2.94)
If f is continuously diﬀerentiable at every point of an open set D ⊂Rn, then
f is said to be continuously diﬀerentiable on D and denoted by f ∈C1(D).
A continuously diﬀerentiable function f : Rn →R is called twice con-
tinuously diﬀerentiable at x ∈Rn if
∂2f
∂xi∂xj (x) exists and is continuous,
i = 1, · · · , n. The Hessian of f is deﬁned as the n × n symmetric matrix
with elements
[∇2f(x)]ij =
∂2f
∂xi∂xj
(x), 1 ≤i, j ≤n.
If f is twice continuously diﬀerentiable at every point in an open set D ⊂Rn,
then f is said to be twice continuously diﬀerentiable on D and denoted by
f ∈C(2)(D).

24
CHAPTER 1. INTRODUCTION
Let f : Rn →R be continuously diﬀerentiable on an open set D ⊂Rn.
Then for x ∈D and d ∈Rn, the directional derivative of f at x in the
direction d is deﬁned as
f′(x; d)
def
= lim
θ→0
f(x + θd) −f(x)
θ
= ∇f(x)T d,
(1.2.95)
where ∇f(x) is the gradient of f at x, an n × 1 vector.
For any x, x + d ∈D, if f ∈C1(D), then
f(x + d)
=
f(x) +
 1
0
∇f(x + td)T ddt
=
f(x) +
 x+d
x
∇f(ξ)dξ.
(1.2.96)
Thus,
f(x + d) = f(x) + ∇f(ξ)T d, ξ ∈(x, x + d).
(1.2.97)
Similarly, for all x, y ∈D, we have
f(y) = f(x) + ∇f(x + t(y −x))T (y −x), t ∈(0, 1),
(1.2.98)
or
f(y) = f(x) + ∇f(x)T (y −x) + o(∥y −x∥).
(1.2.99)
It follows from (1.2.98) that
|f(y) −f(x)| ≤∥y −x∥
sup
ξ∈L(x,y)
||f′(ξ)∥,
(1.2.100)
where L(x, y) denotes the line segment with endpoints x and y.
Let f ∈C(2)(D). For any x ∈D, d ∈Rn, the second directional derivative
of f at x in the direction d is deﬁned as
f′′(x; d) = lim
θ→0
f′(x + θd; d) −f′(x; d)
θ
,
(1.2.101)
which equals dT ∇2f(x)d, where ∇2f(x) denotes the Hessian of f at x. For
any x, x + d ∈D, there exists ξ ∈(x, x + d) such that
f(x + d) = f(x) + ∇f(x)T d + 1
2dT ∇2f(ξ)d,
(1.2.102)
or
f(x + d) = f(x) + ∇f(x)T d + 1
2dT ∇2f(x)d + o(∥d∥2).
(1.2.103)

1.2. MATHEMATICS FOUNDATIONS
25
Let h : Rn →R, g : Rm →R, f : Rn →Rm.
Let f ∈C1, g ∈
C1, h(x0) = g(f(x0)). Then the chain rule is
h′(x0) = g′(f(x0))f′(x0),
(1.2.104)
where f′(x0) =
 ∂fi(x0)
∂xj

m×n is an m × n matrix. Also
h′′(x0) = ∇f(x0)T ∇2g[f(x0)]∇f(x0) +
m

i=1
∂g[f(x0)]
∂fi
[fi(x0)]′′.
(1.2.105)
Next, we discuss the calculus of vector-valued functions.
A continuous function F : Rn →Rm is continuously diﬀerentiable at x ∈
Rn if each component function fi(i = 1, · · · , m) is continuously diﬀerentiable
at x. The derivative F ′(x) ∈Rm×n of F at x is called the Jacobian matrix
of F at x,
F ′(x) = J(x)
with components
[F ′(x)]ij = [J(x)]ij = ∂fi
∂xj (x), i = 1, · · · , m; j = 1, · · · , n.
If F : Rn →Rm is continuously diﬀerentiable in an open convex set D ⊂Rn,
then for any x, x + d ∈D, we have
F(x + d) −F(x) =
 1
0
J(x + td)ddt =
 x+d
x
F ′(ξ)dξ.
(1.2.106)
In many of our considerations, we shall wish to single out diﬀerent types
of continuities.
Deﬁnition 1.2.20 F : D ⊂Rn →Rm is H¨older continuous on D if there
exist constants γ ≥0 and p ∈(0, 1] so that for all x, y ∈D,
∥F(y) −F(x)∥≤γ∥y −x∥p.
(1.2.107)
If p = 1, then F is called Lipschitz continuous on D and γ is a Lipschitz
constant.
F : D ⊂Rn →Rm is H¨older continuous at x ∈D if (1.2.107) holds for
any y in the neighborhood of x.

26
CHAPTER 1. INTRODUCTION
Deﬁnition 1.2.21 F : D ⊂Rn →Rm is hemi-continuous at x ∈D if, for
any d ∈Rn and ϵ > 0, there is a δ = δ(ϵ, d) so that whenever |t| < δ and
x + td ∈D,
∥F(x + td) −F(x)∥< ϵ.
(1.2.108)
We also can deﬁne the upper hemi-continuous and lower hemi-continuous
at x ∈D if, instead of (1.2.108), we use, respectively, F(x + td) < F(x) + ϵ
and F(x + td) > F(x) −ϵ for suﬃciently small t.
The following two theorems establish the bounds of errors within which
some standard models approximate the objective functions. For F : Rn →
Rm, Theorem 1.2.22 gives a bound of the error in linear model F(x)+F ′(x)d
as an approximation to F(x + d).
Similarly, for f : Rn →R, Theorem
1.2.23 gives a bound of errors with a quadratic model as an approximation
to f(x + d).
Theorem 1.2.22 Let F : Rn →Rm be continuously diﬀerentiable in the
open convex set D ⊂Rn. Let F ′ be Lipschitz continuous at x ∈D. Then for
any x + d ∈D, we have
∥F(x + d) −F(x) −F ′(x)d∥≤γ
2∥d∥2.
(1.2.109)
Proof.
F(x + d) −F(x) −F ′(x)d
=
 1
0
F ′(x + αd)ddα −F ′(x)d
=
 1
0
[F ′(x + αd) −F ′(x)]ddα.
Hence
∥F(x + d) −F(x) −F ′(x)d∥
≤
 1
0
∥F ′(x + αd) −F ′(x)∥∥d∥dα
≤
 1
0
γ∥αd∥∥d∥dα
=
γ∥d∥2
 1
0
αdα
=
γ
2∥d∥2.
2

1.2. MATHEMATICS FOUNDATIONS
27
Theorem 1.2.23 Let f : Rn →R be twice continuously diﬀerentiable in the
open convex set D ⊂Rn. Let ∇2f(x) be Lipschitz continuous at x ∈D with
Lipschitz constant γ. Then for any x + d ∈D, we have
f(x + d) −[f(x) + ∇f(x)T d + 1
2dT ∇2f(x)d]
 ≤γ
6∥d∥3.
(1.2.110)
The proof of this theorem is left to readers as an exercise.
As a generalization of Theorem 1.2.22, we obtain
Theorem 1.2.24 Let F : Rn →Rm be continuously diﬀerentiable in the
open convex set D ⊂Rn. Then for any u, v, x ∈D, we have
∥F(u) −F(v) −F ′(x)(u −v)∥
≤

sup
0≤t≤1
∥F ′(v + t(u −v)) −F ′(x)∥

∥u −v∥.
(1.2.111)
Furthermore, assume that F ′ is Lipschitz continuous in D, then
∥F(u) −F(v) −F ′(x)(u −v)∥≤γσ(u, v)∥u −v∥
(1.2.112)
and
∥F(u) −F(v) −F ′(x)(u −v)∥≤γ ∥u −x∥+ ∥v −x∥
2
∥u −v∥,
(1.2.113)
where σ(u, v) = max{∥u −x∥, ∥v −x∥}.
Proof.
By (1.2.106) and the mean-value theorem of integration, we have
∥F(u) −F(v) −F ′(x)(u −v)∥
=

 1
0
[F ′(v + t(u −v)) −F ′(x)](u −v)dt

≤
 1
0
∥F ′(v + t(u −v)) −F ′(x)∥∥u −v∥dt
≤

sup
0≤t≤1
∥F ′(v + t(u −v)) −F ′(x)∥

∥u −v∥
which is (1.2.111). Also since F ′ is Lipschitz continuous in D, we proceed
with the above inequality and get
∥F(u) −F(v) −F ′(x)(u −v)∥

28
CHAPTER 1. INTRODUCTION
≤
γ
 1
0
∥v + t(u −v) −x∥∥u −v∥dt
≤
γ sup
0≤t≤1
∥v + t(u −v) −x∥∥u −v∥
=
γσ(u, v)∥u −v∥
which is (1.2.112).
Similarly, we can derive (1.2.113) which is left as an
exercise.
2
The following theorem is useful, giving a relation between ∥F(u) −F(v)∥
and ∥u −v∥.
Theorem 1.2.25 Let F and F ′ satisfy the conditions of Theorem 1.2.24.
Assume that [F ′(x)]−1 exists. Then there exist ϵ > 0 and β > α > 0 such
that for all u, v ∈D, when max{∥u −x∥, ∥v −x∥} ≤ϵ, we have
α∥u −v∥≤∥F(u) −F(v)∥≤β∥u −v∥.
(1.2.114)
Proof.
By the triangle inequality and (1.2.112),
∥F(u) −F(v)∥
≤
∥F ′(x)(u −v)∥+ ∥F(u) −F(v) −F ′(x)(u −v)∥
≤
(∥F ′(x)∥+ γσ(u, v))∥u −v∥
≤
∥F ′(x)∥+ γϵ
 ∥u −v∥.
Set β = ∥F ′(x)∥+ γϵ, we obtain the right inequality of (1.2.114). Similarly,
∥F(u) −F(v)∥
≥
∥F ′(x)(u −v)∥−∥F(u) −F(v) −F ′(x)(u −v)∥
≥

1/∥[F ′(x)]−1∥−γσ(u, v)

∥u −v∥
≥
[1/∥[F ′(x)]−1∥−γϵ]∥u −v∥.
Hence, if
1
∥[F ′(x)]−1∥γ > ϵ, the left inequality of (1.2.114) also holds with
α =
1
∥[F ′(x)]−1∥−γϵ > 0.
2
Corollary 1.2.26 Let F and F ′ satisfy the conditions of Theorem 1.2.22.
When u and v are suﬃciently close to x, we have
lim sup
ω→0
∥u −x∥
∥v −x∥≤C lim sup
ω→0
∥F(u) −F(x)∥
∥F(v) −F(x)∥,
(1.2.115)
where C = ∥F ′(x)∥∥F ′(x)−1∥is a constant and ω = max{∥u −x∥, ∥v −x∥}.

1.2. MATHEMATICS FOUNDATIONS
29
Proof.
By using Theorem 1.2.22, we have
∥F(v) −F(x)∥
≤
∥F ′(x)(v −x)∥+ ∥F(v) −F(x) −F ′(x)(v −x)∥
≤
∥F ′(x)∥∥v −x∥+ O(∥v −x∥2)
and
∥F(u) −F(x)∥
≥
∥F ′(x)(u −x)∥−∥F(u) −F(x) −F ′(x)(u −x)∥
≥
∥u −x∥/∥[F ′(x)]−1∥+ O(∥u −x∥2).
Then
∥F(u) −F(x)∥
∥F(v) −F(x)∥≥∥u −x∥/∥F ′(x)−1∥+ O(∥u −x∥2)
∥F ′(x)∥∥v −x∥+ O(∥v −x∥2)
.
Setting C = ∥F ′(x)∥∥F ′(x)−1∥and taking limit give
C lim sup
ω→0
∥F(u) −F(x)∥
∥F(v) −F(x)∥≥lim sup
ω→0
∥u −x∥
∥v −x∥,
where ω = max{∥u −x∥, ∥v −x∥}.
2
We conclude this subsection with some remarks about diﬀerentiation of
the vector-valued functions.
About the calculus of vector-valued functions, we would like to review
Gateaux and Fr´echet derivatives.
Deﬁnition 1.2.27 Let D ⊂Rn be an open set.
The function F : D ⊂
Rn →Rm is Gateaux- (or G-) diﬀerentiable at x ∈D if there exists a linear
operator A ∈L(Rn, Rm) such that for any d ∈Rn,
lim
α→0
1
α∥F(x + αd) −F(x) −αAd∥= 0.
(1.2.116)
The linear operator A is denoted by F ′(x) and is called the G-derivative of
F at x.
Deﬁnition 1.2.28 Let D ⊂Rn be an open set. The function F : Rn →Rm
is Fr´echet- (or F-) diﬀerentiable at x ∈D if there is a linear operator A ∈
L(Rn, Rm) such that for any d ∈Rn,
lim
d→0
∥F(x + d) −F(x) −Ad∥
∥d∥
= 0.
(1.2.117)

30
CHAPTER 1. INTRODUCTION
The linear operator A is again denoted by F ′(x), and is called the F-derivative
of F at x.
The F-diﬀerentiability can also be written as
F(x + d) −F(x) = F ′(x)d + o(∥d∥).
Furthermore, if for any u, v ∈Rn,
lim
∥u−v∥→0
∥F(u) −F(v) −F ′(x)(u −v)∥
∥u −v∥
= 0,
(1.2.118)
then F is called strongly F-diﬀerentiable at x ∈D and F ′(x) is called a strong
F-derivative.
From the above two deﬁnitions, we know the following facts.
1. If F : Rn →Rm is continuous at x ∈Rn, then F is hemi-continuous at
x.
2. If F : Rn →Rm is G-diﬀerentiable at x ∈D, then F is hemi-continuous
at x.
3. If F : Rn →Rm is F-diﬀerentiable at x ∈D, then F is continuous at
x.
4. If F is F-diﬀerentiable at x ∈D, then it is G-diﬀerentiable at x; how-
ever, the reverse is not true.
5. If F is G-diﬀerentiable and its G-derivative F ′ is continuous, then F is
F-diﬀerentiable and the F-derivative is continuous. In this case, we say
that F is continuously diﬀerentiable.
6. The G-derivative and F-derivative of F, if they exist, are equal and
given by the Jacobian matrix
F ′(x) =
⎡
⎢⎣
∂f1
∂x1 (x)
∂f1
∂x2 (x)
· · ·
∂f1
∂xn (x)
· · ·
· · ·
· · ·
· · ·
∂fm
∂x1 (x)
∂fm
∂x2 (x)
· · ·
∂fm
∂xn (x)
⎤
⎥⎦,
where f1, f2, · · · , fm are components of F.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
31
7. The mean-value theorem: Let F : Rn →Rm be G-diﬀerentiable in the
open convex set D ⊂Rn. Then we have the following forms of the
mean-value theorem:
(a) For any x, y, z ∈D, there exist t1, t2, · · · , tm ∈[0, 1] such that
F(y) −F(x) =
⎛
⎜
⎜
⎜
⎜
⎝
f′
1(x + t1(y −x))
f′
2(x + t2(y −x))
...
f′
m(x + tm(y −x))
⎞
⎟
⎟
⎟
⎟
⎠
(1.2.119)
and
∥F(y) −F(x)∥≤sup
0≤t≤1
∥F ′(x + t(y −x))∥∥y −x∥.
(b) For any x, y, z ∈D,
∥F(y)−F(z)−F ′(x)(y−z)∥≤sup
0≤t≤1
∥F ′(z+t(y−z))−F ′(x)∥∥y−z∥.
(1.2.120)
(c) Furthermore, if the G-derivative F ′ is hemi-continuous on D, then
for any x, y ∈D,
F(y) −F(x) =
 1
0
F ′(x + t(y −x))(y −x)dt.
(1.2.121)
(d) If assume also that F ′(x) is H¨older continuous on D, then for all
x, y ∈D,
∥F(y) −F(x) −F ′(x)(y −x)∥≤
γ
p + 1∥y −x∥p+1.
(1.2.122)
If p = 1, it is just (1.2.109).
1.3
Convex Sets and Convex Functions
Convex sets and convex functions play an important role in the study of
optimization. In this section, we introduce the fundamental concepts and
results of convex sets and convex functions.

32
CHAPTER 1. INTRODUCTION
1.3.1
Convex Sets
Deﬁnition 1.3.1 Let the set S ⊂Rn. If, for any x1, x2 ∈S, we have
αx1 + (1 −α)x2 ∈S, ∀α ∈[0, 1],
(1.3.1)
then S is said to be a convex set.
This deﬁnition indicates, in geometry, that for any two points x1, x2 ∈S,
the line segment joining x1 and x2 is entirely contained in S. It also states
that S is path-connected, i.e., two arbitrary points in S can be linked by a
continuous path.
It can be shown by induction that the set S ⊂Rn is convex if and only
if for any x1, x2, · · · , xm ∈S,
m

i=1
αixi ∈S,
(1.3.2)
where 
m
i=1 αi = 1, αi ≥0, i = 1, · · · , m.
Figure 1.3.1 Convex set and nonconvex set
In (1.3.1), x = αx1+(1−α)x2, where α ∈[0, 1], is called a convex combination
of x1 and x2. In (1.3.2), x = 
m
i=1 αixi is called a convex combination of
x1, · · · , xm, where 
m
i=1 αi = 1, αi ≥0, i = 1, · · · , m.
Example 1.3.2 The hyperplane H = {x ∈Rn | pT x = α} is a convex set,
where p ∈Rn is a nonzero vector referred to as the normal vector to the
hyperplane, and α is a scalar.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
33
In fact, for any x1, x2 ∈H and each θ ∈[0, 1],
pT [θx1 + (1 −θ)x2] = α,
then θx1 + (1 −θ)x2 ∈H.
In the hyperplane H = {x ∈Rn | pT x = α}, if α = 0, it can be reduced
to a subspace of vectors that are orthogonal to p.
Similarly, the closed half space H−= {x ∈Rn | pT x ≤β} and H+ =
{x ∈Rn | pT x ≥β} are closed convex sets. The open half space (
◦
H)−=
{x ∈Rn | pT x < β} and (
◦
H)+ = {x ∈Rn | pT x > β} are open convex sets .
Example 1.3.3 The ray S = {x ∈Rn | x = x0 + λd, λ ≥0} is a convex set,
where d ∈Rn is a nonzero vector, and x0 ∈Rn is a ﬁxed point.
In fact, for any x1, x2 ∈S and each λ ∈[0, 1], we have
x1 = x0 + λ1d, x2 = x0 + λ2d,
where λ1, λ2 ∈[0, 1]. Hence
λx1 + (1 −λ)x2
=
λ(x0 + λ1d) + (1 −λ)(x0 + λ2d)
=
x0 + [λλ1 + (1 −λ)λ2]d.
Since λλ1 + (1 −λ)λ2 ≥0, then λx1 + (1 −λ)x2 ∈S.
The ﬁnite intersection of closed half spaces
S = {x ∈Rn | pT
i x ≤βi, i = 1, · · · , m},
is called a polyhedral set, where pi is a nonzero vector, βi a scalar. The
polyhedral is a convex set .
Since an equality can be represented by two inequalities, the following
sets are examples of polyhedral sets:
S = {x ∈Rn | Ax = b, x ≥0},
S = {x ∈Rn | Ax ≥0, x ≥0}.
The theorems below state the algebraic properties and topological prop-
erties. That is the intersection of two convex sets is convex, the algebraic
sum of two convex sets is convex, the interior of a convex set is convex, and
the closure of a convex set is convex.

34
CHAPTER 1. INTRODUCTION
Theorem 1.3.4 Let S1 and S2 be convex sets in Rn. Then
1. S1 ∩S2 is convex;
2. S1 ± S2 = {x1 ± x2 | x1 ∈S1, x2 ∈S2} is convex.
Proof.
The proof is immediate from the deﬁnition of convex set and left
to readers as an exercise.
2
From this theorem, we know that the feasible regions in linear program-
ming and quadratic programming are convex sets, because they are the in-
tersection of a hyperplane and a half space.
Theorem 1.3.5 Let S ⊂Rn be a convex set. Then
1. the interior intS of S is a convex set;
2. the closure ¯S of S is a convex set.
Proof.
1) Let x and x′ be in intS, and x′′ = αx + (1 −α)x′, α ∈(0, 1).
Choose δ > 0 such that B(x′, δ) ⊂S, where B(x′, δ) is the δ-neighborhood
of x′.
It is easy to see that ∥x′′ −x∥/∥x′ −x∥= 1 −α.
We know that
B(x′′, (1 −α)δ) is just the set αx + (1 −α)B(x′, δ) which is in S. Therefore
B(x′′, (1 −α)δ) ⊂S which shows that x′′ ∈int S.
2) Take x, x′ ∈¯S. Select in S two sequences {xk} and {x′
k} converging
to x and x′ respectively. Then, for α ∈[0, 1], we have
∥[αxk + (1 −α)x′
k] −[αx + (1 −α)x′]∥
=
∥α(xk −x) + (1 −α)(x′
k −x′)∥
≤
α∥xk −x∥+ (1 −α)∥x′
k −x′∥.
Taking the limit yields
lim
k→∞∥[αxk + (1 −α)x′
k] −[αx + (1 −α)x′]∥= 0,
which shows αx + (1 −α)x′ ∈¯S.
2
Now we state some concepts related to convex sets.
Let S ⊂Rn be a nonempty set. We deﬁne the convex hull conv(S) as the
intersection of all convex sets containing S, which is described as the set of

1.3. CONVEX SETS AND CONVEX FUNCTIONS
35
all convex combinations of the elements of S:
conv(S)
∆=
∩{C | C is convex and contains S}
=
{x ∈Rn | x =
m

i=1
αixi, xi ∈S,
m

i=1
αi = 1,
αi ≥0, i = 1, · · · , m}.
(1.3.3)
We can see that conv(S) is the smallest convex set containing S.
A nonempty set C ⊂Rn is called a cone if it is closed under positive scalar
multiplication, i.e., if x ∈C implies that λx ∈C for all λ > 0. If, in addition,
C is convex, then C is called a convex cone. C ⊂Rn is a convex cone if and
only if it is closed under addition and positive scalar multiplication. The
smallest convex cone containing convex S is
C = {λx | λ > 0, x ∈S}.
The following are examples of convex cones. For example, the nonnegative
orthant of Rn
{x = (ξ1, · · · , ξn) | ξ1 ≥0, · · · , ξn ≥0},
positive orthant of Rn
{x = (ξ1, · · · , ξn) | ξ1 > 0, · · · , ξn > 0}
and the intersection of m half-spaces
{x ∈Rn | xT bi ≤0, bi ∈Rn, i = 1, · · · , m}
are convex cones .
A specially important class of convex cones is polar cone.
Let S be
a nonempty set in Rn.
The polar cone of S, denoted by S∗, is given by
{p | pT x ≤0 for all x ∈S }. It is easy to see from the above deﬁnition that
the polar cone S∗of a nonempty set S has the following properties:
1. S∗is a closed convex cone.
2. S ⊂S∗∗, where S∗∗is the polar cone of S∗. If S is a nonempty closed
convex set, then S∗∗= S.
3. If S1, S2 are nonempty sets, then S1 ⊂S2 implies S∗
2 ⊂S∗
1.

36
CHAPTER 1. INTRODUCTION
The normal and tangent cones play a special role in constrained optimiza-
tion. Here we give their deﬁnitions below. Let S be a closed convex set. The
normal cone of S at ¯x is deﬁned as
N(¯x) = {y ∈Rn | ⟨y, x −¯x⟩≤0, ∀x ∈S}.
(1.3.4)
The tangent cone of S at ¯x ∈S is the polar of the normal cone at ¯x, that is
T(¯x) = (N(¯x))∗
=
cl{λ(x −¯x) | λ ≥0, x ∈S}
(1.3.5)
=
{d | d = lim
x→¯x λ(x −¯x), λ ≥0, x ∈S},
where cl{S} denotes the closure of S.
1.3.2
Convex Functions
Deﬁnition 1.3.6 Let S ⊂Rn be a nonempty convex set. Let f : S ⊂Rn →
R. If, for any x1, x2 ∈S and all α ∈(0, 1), we have
f(αx1 + (1 −α)x2) ≤αf(x1) + (1 −α)f(x2),
(1.3.6)
then f is said to be convex on S. If the above inequality is true as a strict
inequality for all x1 ̸= x2, i.e.,
f(αx1 + (1 −α)x2) < αf(x1) + (1 −α)f(x2),
(1.3.7)
then f is called a strict convex function on S. If there is a constant c > 0
such that for any x1, x2 ∈S,
f(αx1 +(1−α)x2) ≤αf(x1)+(1−α)f(x2)−1
2cα(1−α)∥x1 −x2∥2, (1.3.8)
then f is called a uniformly (or strongly) convex function on S.
If −f is a convex (strictly convex, uniformly convex) function on S, then
f is said to be a concave (strictly concave, uniformly concave) function.
Figure 1.3.2 Convex function and concave function

1.3. CONVEX SETS AND CONVEX FUNCTIONS
37
Figure 1.3.2 gives examples of convex, concave, and neither convex nor
concave functions. The geometrical interpretation of a convex function says
that the function values are below the corresponding chord, that is, the values
of a convex function at points on the line segment αx1 + (1 −α)x2 are less
than or equal to the height of the chord joining the points (x1, f(x1)) and
(x2, f(x2)). It is obvious from the deﬁnition of convex function that a linear
function f(x) = aT x+β is both a convex and concave function on Rn, where
a, x ∈Rn, β ∈R.
The other basic and important examples of convex functions are indicator
function, support function, norm and distance function.
Let S ⊂Rn be a nonempty subset; the indicator function IS : Rn →
R ∪{+∞} is deﬁned by
IS(x) :=

0,
if x ∈S,
+∞,
otherwise.
(1.3.9)
Clearly, IS is convex if and only if S is convex.
Let S ⊂Rn be a nonempty subset. The support function of S is deﬁned
by
σS(s) := sup{⟨s, x⟩| x ∈S}.
(1.3.10)
This is a convex function.
It is easy to see that a norm on Rn is a convex function. If we deﬁne the
distance function as
dS(x) := inf{∥y −x∥| y ∈S},
where S ⊂Rn is a nonempty convex set and ∥· ∥is any norm on Rn, then
dS is a convex function.
A convex function can also be described by an epigraph. Now we ﬁrst
give the deﬁnition of the epigraph of f, and then show that f is convex if
and only if its epigraph is a convex set.
Let S ⊂Rn be a nonempty set.
A set {(x, f(x)) : x ∈S} ⊂Rn+1
describing the function f is said to be the graph of the function f. Related
to the graph of f, there are the epigraph, which consists of points above the
graph of f, and the hypograph, which consists of points below the graph of
f.

38
CHAPTER 1. INTRODUCTION
Deﬁnition 1.3.7 Let S ⊂Rn be a nonempty set. Let f : S ⊂Rn →R. The
epigraph of f, denoted by epif, is a subset of Rn+1 deﬁned by
epif = {(x, α) | f(x) ≤α, x ∈S, α ∈R}.
(1.3.11)
The hypograph of f, denoted by hypf, is a subset of Rn+1 deﬁned by
hypf = {(x, α) | f(x) ≥α, x ∈S, α ∈R}.
(1.3.12)
Figure 1.3.3 Epigraph and hypograph
The following theorem indicates the relation between convex function and
convexity of epif.
Theorem 1.3.8 Let S ⊂Rn be a nonempty convex set. Let f : S ⊂Rn →
R. Then f is convex if and only if epif is a convex set.
Proof.
Assume that f is convex. Let x1, x2 ∈S and (x1, α1), (x2, α2) be
in epif. Then, it follows from Deﬁnition 1.3.6 and Deﬁnition 1.3.7 that
f(λx1 + (1 −λ)x2) ≤λf(x1) + (1 −λ)f(x2) ≤λα1 + (1 −λ)α2
for any λ ∈(0, 1). Since S is a convex set, λx1 + (1 −λ)x2 ∈S. Hence
(λx1 + (1 −λ)x2, λα1 + (1 −λ)α2) ∈epi f, which means epif is convex.
Conversely, assume that epif is convex, and let x1, x2 ∈S and (x1, f(x1)),
(x2, f(x2)) ∈epif. Then we have from the convexity of epif that
(λx1 + (1 −λ)x2, λf(x1) + (1 −λ)f(x2)) ∈epif, for λ ∈(0, 1).

1.3. CONVEX SETS AND CONVEX FUNCTIONS
39
This means
f(λx1 + (1 −λ)x2) ≤λf(x1) + (1 −λ)f(x2)
for each λ ∈(0, 1). Hence f is convex.
2
The epigraph epif of a function f is an important concept and it is used
often in convex programming. Here we would like to mention its properties.
The epif has a closed relation with the lower semi-continuity (l.s.c.) of f
which is also very important, because, for a function to have a minimum, a
very basic requirement is lower semi-continuity. We may recall that a function
f is lower semi-continuous if, for each x ∈Rn,
lim inf
y→x f(y) ≥f(x).
(1.3.13)
The following theorem gives an equivalent property between epif and l.s.c.
Theorem 1.3.9 For f : Rn →R ∪{+∞}, the following three statements
are equivalent:
1. f is lower semi-continuous on Rn;
2. epif is a closed set in Rn × R;
3. the level sets Lr(f) = {x ∈Rn | f(x) ≤r, r ∈R} are closed for all
r ∈R.
Proof.
(1) ⇒(2): Let {(yk, rk)} be a sequence of epif converging to (x, r)
for k →∞. Since f(yk) ≤rk for all k, the (1.3.13) gives
r = lim
k→∞rk ≥lim inf
yk→x f(yk) ≥f(x),
which indicates that (x, r) ∈epif.
(2) ⇒(3): Construct the level set Lr(f) which is the intersection of two
closed sets epif and (Rn × {r}). Obviously the intersection is closed.
(3) ⇒(1): Suppose that f is not lower semi-continuous at some x, which
means there exists a sequence {yk} converging to x such that {f(yk)} con-
verges to ρ < f(x) ≤+∞. Take r ∈(ρ, f(x)). When k tends large enough,
we have f(yk) ≤r < f(x) which means that Lr(f) does not contain its limit
x. Hence Lr(f) is not closed.
2
Using Theorem 1.3.9, we can give a deﬁnition of closed function.

40
CHAPTER 1. INTRODUCTION
Deﬁnition 1.3.10 A function f : Rn →R ∪{+∞} is said to be closed if
it is lower semi-continuous everywhere, or if its epigraph is closed, or if its
level sets are closed.
Obviously, the indicator function IS is closed if and only if S is closed.
Also, epiIS = S × R+. The support function σS is closed too.
Next, we give some properties of convex functions.
Theorem 1.3.11
1. Let f be a convex function on a convex set S ⊂Rn
and real number α ≥0, then αf is also a convex function on S.
2. Let f1, f2 be convex functions on a convex set S, then f1 + f2 is also a
convex function on S.
3. Let f1, f2, · · · , fm be convex functions on a convex set S and real num-
bers α1, α2, · · · , αm ≥0, then 
m
i=1 αifi is also a convex function on
S.
Proof.
We only prove the second statement. The others are similar.
Let x1, x2 ∈S and 0 < α < 1, then
f1(αx1 + (1 −α)x2) + f2(αx1 + (1 −α)x2)
≤
α[f1(x1) + f2(x1)] + (1 −α)[f1(x2) + f2(x2)]. 2
Continuity is an important property of a convex function. However, it
is not sure that a convex function whose domain is not open is continuous.
The following theorem shows that a convex function is continuous on an open
convex set or the interior of its domain.
Theorem 1.3.12 Let S ⊂D be an open convex set. Let f : D ⊂Rn →R
be convex. Then f is continuous on S.
Proof.
Let x0 be an arbitrary point in S. Since S is an open convex set,
we can ﬁnd n+1 points x1, · · · , xn+1 ∈S such that the interior of the convex
hull
C = {x | x =
n+1

i=1
αixi, αi ≥0,
n+1

i=1
αi = 1}
is not empty and x0 ∈intC.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
41
Now let α = max1≤i≤n+1 f(xi), then
f(x) = f
n+1

i=1
αixi

≤
n+1

i=1
αif(xi) ≤α, ∀x ∈C,
(1.3.14)
so that f is bounded over C. Also, since x0 ∈int C, there is a δ > 0 such
that B(x0, δ) ⊂C, where B(x0, δ) = {x| ∥x −x0∥≤δ}. Hence for arbitrary
h ∈B(0, δ) and λ ∈[0, 1], we have
x0 =
1
1 + λ(x0 + λh) +
λ
1 + λ(x0 −h).
(1.3.15)
Since f is convex on C, then
f(x0) ≤
1
1 + λf(x0 + λh) +
λ
1 + λf(x0 −h).
(1.3.16)
By (1.3.16) and (1.3.14), we have
f(x0 + λh) −f(x0) ≥λ(f(x0) −f(x0 −h)) ≥−λ(α −f(x0)).
(1.3.17)
On the other hand,
f(x0 + λh) = f(λ(x0 + h) + (1 −λ)x0) ≤λf(x0 + h) + (1 −λ)f(x0),
which is
f(x0 + λh) −f(x0) ≤λ(f(x0 + h) −f(x0)) ≤λ(α −f(x0)).
(1.3.18)
Therefore, (1.3.17) and (1.3.18) give
|f(x0 + λh) −f(x0)| ≤λ|f(x0) −α|.
(1.3.19)
Now, for given ϵ > 0, choose δ′ ≤δ so that δ′|f(x0) −α| ≤ϵδ. Set d = λh
with ∥h∥= δ, then d ∈B(0, δ) and
|f(x0 + d) −f(x0)| ≤ϵ.
2
If a convex function is diﬀerentiable, we can describe the characterization
of diﬀerential convex functions. The following theorem gives the ﬁrst order
characterization of diﬀerential convex functions.

42
CHAPTER 1. INTRODUCTION
Theorem 1.3.13 Let S ⊂Rn be a nonempty open convex set and let f :
S ⊂Rn →R be a diﬀerentiable function. Then f is convex if and only if
f(y) ≥f(x) + ∇f(x)T (y −x), ∀x, y ∈S.
(1.3.20)
Similarly, f is strictly convex on S if and only if
f(y) > f(x) + ∇f(x)T (y −x), ∀x, y ∈S, y ̸= x.
(1.3.21)
Furthermore, f is strongly (or uniformly) convex if and only if
f(y) ≥f(x) + ∇f(x)T (y −x) + 1
2c∥y −x∥2, ∀x, y ∈S,
(1.3.22)
where c > 0 is a constant.
Proof.
Necessity: Let f(x) be a convex function, then for all α with
0 < α < 1,
f(αy + (1 −α)x) ≤αf(y) + (1 −α)f(x).
Hence,
f(x + α(y −x)) −f(x)
α
≤f(y) −f(x).
Setting α →0 yields
∇f(x)T (y −x) ≤f(y) −f(x).
Suﬃciency: Assume that (1.3.20) holds.
Pick any x1, x2 ∈S and set
x = αx1 + (1 −α)x2, 0 < α < 1. Then
f(x1)
≥
f(x) + ∇f(x)T (x1 −x),
f(x2)
≥
f(x) + ∇f(x)T (x2 −x).
Hence
αf(x1) + (1 −α)f(x2)
≥
f(x) + ∇f(x)T (αx1 + (1 −α)x2 −x)
=
f(αx1 + (1 −α)x2),
which indicates that f(x) is a convex function.
Similarly, we can prove (1.3.21) and (1.3.22) by use of (1.3.20).
For
example, from the deﬁnition of the strictly convex, we have
f(x + α(y −x)) −f(x) < α(f(y) −f(x)).

1.3. CONVEX SETS AND CONVEX FUNCTIONS
43
Then, using (1.3.20) and the above inequality, we have
⟨∇f(x), α(y −x)⟩≤f(x + α(y −x)) −f(x) < α(f(y) −f(x))
which is the required (1.3.21).
To obtain (1.3.22), it is enough to apply (1.3.20) to the function f−1
2c∥·∥2.
2
Deﬁnition 1.3.6 of convex function indicates that the function value is
below the chord, which means that the linear interpolation of the function
values at two points is larger than the function value at the interpolation
point. This theorem represents that the linear approximation based on a
local derivative is a lower estimate, i.e., the convex function always lies above
its tangent at any point. Such a tangent is called a supporting hyperplane of
the convex function.
Figure 1.3.4 The ﬁrst order characteristic of a convex function
Below, we consider the second order characteristic of a twice continuously
diﬀerentiable convex function.
Theorem 1.3.14 Let S ⊂Rn be a nonempty open convex set, and let f :
S ⊂Rn →R be twice continuously diﬀerentiable. Then
1. f is convex if and only if its Hessian matrix is positive semideﬁnite at
each point in S.
2. f is strictly convex if its Hessian matrix is positive deﬁnite at each point
in S.
3. f is uniformly convex if and only if its Hessian matrix is uniformly
positive deﬁnite at each point in S, i.e., there exists a constant m > 0
such that
m∥u∥2 ≤uT ∇2f(x)u, ∀x ∈S, u ∈Rn.

44
CHAPTER 1. INTRODUCTION
Proof.
We only prove the ﬁrst case. The other two cases are analogous.
Suﬃciency. Suppose that the Hessian matrix ∇2f(x) is positive semidef-
inite at each point x ∈S. Consider x, ¯x ∈S. By the mean-value theorem,
we have
f(x) = f(¯x) + ∇f(¯x)T (x −¯x) + 1
2(x −¯x)T ∇2f(ˆx)(x −¯x),
where ˆx = ¯x + θ(x −¯x), θ ∈(0, 1). Noting that ˆx ∈S, it follows from the
assumption that
f(x) ≥f(¯x) + ∇f(¯x)T (x −¯x).
Hence f is a convex function by Theorem 1.3.13.
Necessity. Suppose that f is a convex function and let ¯x ∈S. We need
to prove pT ∇2f(¯x)p ≥0, ∀p ∈Rn. Since S is open, then there exists δ > 0
such that when |λ| < δ, ¯x + λp ∈S. By Theorem 1.3.13,
f(¯x + λp) ≥f(¯x) + λ∇f(¯x)T p.
(1.3.23)
Also since f(x) is twice diﬀerentiable at ¯x, then
f(¯x + λp) = f(¯x) + λ∇f(¯x)T p + λ2
2 pT ∇2f(¯x)p + o(∥λp∥2).
(1.3.24)
Substituting (1.3.24) into (1.3.23) yields
1
2λ2pT ∇2f(¯x)p + o(∥λp∥2) ≥0.
Dividing by λ2 and letting λ →0, it follows that
pT ∇2f(¯x)p ≥0.2
Next, we would like to characterize a convex function with monotonicity
which is very useful.
We ﬁrst introduce a deﬁnition of monotone mapping.
Deﬁnition 1.3.15 A mapping F : D ⊂Rn →Rn is monotone on D0 ⊂D
if
⟨F(x) −F(y), x −y⟩≥0, ∀x, y ∈D0;
(1.3.25)
F is strictly monotone on D0 if
⟨F(x) −F(y), x −y⟩> 0, ∀x, y ∈D0, x ̸= y;
(1.3.26)

1.3. CONVEX SETS AND CONVEX FUNCTIONS
45
F is uniformly ( or strongly ) monotone if there is a constant c > 0 so that
⟨F(x) −F(y), x −y⟩≥c∥x −y∥2, ∀x, y ∈D0.
(1.3.27)
If we let F = ∇f in the above deﬁnition, we can get the following theo-
rem which says that, for convex function f, its gradient ∇f is a monotone
mapping.
Theorem 1.3.16 Suppose that f : D ⊂Rn →R is diﬀerentiable on an open
set D, and that S ⊂D is a convex subset. Then f is convex on S if and only
if its gradient ∇f is monotone, i.e.,
⟨∇f(x) −∇f(y), x −y⟩≥0, ∀x, y ∈S;
(1.3.28)
and f is strictly convex on S if and only if its gradient ∇f is strictly mono-
tone, i.e.,
⟨∇f(x) −∇f(y), x −y⟩> 0, ∀x, y ∈S, x ̸= y;
(1.3.29)
ﬁnally, f is uniformly (or strongly ) convex on S if and only if its gradient
∇f is uniformly monotone, i.e.,
⟨∇f(x) −∇f(y), x −y⟩≥c∥x −y∥2,
(1.3.30)
where c > 0 is the constant of (1.3.8).
Proof.
Let f be uniformly convex on S, then, by Theorem 1.3.13, for any
x, y ∈S, we have
f(y)
≥
f(x) + ⟨∇f(x), y −x⟩+ 1
2c∥y −x∥2,
(1.3.31)
f(x)
≥
f(y) + ⟨∇f(y), x −y⟩+ 1
2c∥x −y∥2,
(1.3.32)
and addition of these two inequalities shows that (1.3.30) holds.
Similarly, if f is convex, (1.3.31) and (1.3.32) hold with c = 0, and hence
(1.3.28) holds. Moreover, if f is strictly convex, then (1.3.31) and (1.3.32)
hold with c = 0 but with strict inequality for x ̸= y. Hence the addition
establishes (1.3.29).
Conversely, suppose that ∇f is monotone. For any ﬁxed x, y ∈S, the
mean-value theorem (1.2.97) gives
f(y) −f(x) = ⟨∇f(ξ), y −x⟩,
(1.3.33)

46
CHAPTER 1. INTRODUCTION
where ξ = x + t(y −x), t ∈(0, 1). Then, it follows from (1.3.28) that
⟨∇f(ξ) −∇f(x), y −x⟩= 1
t [∇f(ξ) −∇f(x)]T (ξ −x) ≥0,
(1.3.34)
which, together with (1.3.33), gives
f(y) −f(x)
=
⟨∇f(ξ) −∇f(x), y −x⟩+ ⟨∇f(x), y −x⟩
≥
⟨∇f(x), y −x⟩.
(1.3.35)
The above inequality shows, by Theorem 1.3.13, that f is convex.
Similarly, if (1.3.29) holds, the same will be true in (1.3.35) with strict
inequality and x ̸= y, and thus f is strictly convex.
Finally, for uniform convexity, suppose (1.3.30) holds. Let φ(t) = f(x +
t(y −x)) = f(u), where u = x + t(y −x), t ∈(0, 1). Noting that φ′(t) =
⟨∇f(u), y −x⟩and φ′(0) = ⟨∇f(x), y −x⟩, then (1.3.30) means
φ′(t) −φ′(0)
=
⟨∇f(u) −∇f(x), y −x⟩= 1
t ⟨∇f(u) −∇f(x), u −x⟩
≥
1
t c∥u −x∥2 = tc∥y −x∥2.
Hence,
φ(1) −φ(0) −φ′(0) =
 1
0
[φ′(t) −φ′(0)]dt ≥1
2c∥y −x∥2,
which, by the deﬁnition of φ, shows
f(y) ≥f(x) + ⟨∇f(x), y −x⟩+ 1
2c∥y −x∥2.
Therefore, we complete the proof.
2
Combining Theorem 1.3.14 and 1.3.16, we immediately obtain the follow-
ing theorem.
Theorem 1.3.17 Let S ⊂Rn be a nonempty open convex set and f be a
twice continuously diﬀerentiable function on S. Then
1. ∇f is monotone on S if and only if ∇2f(x) is positive semideﬁnite for
all x ∈S.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
47
2. If ∇2f(x) is positive deﬁnite for all x ∈S, then ∇f is strictly monotone
on S.
3. ∇f is uniformly ( or strongly) monotone on S if and only if ∇2f(x) is
uniformly positive deﬁnite, i.e., there exists a number c > 0 so that
dT ∇2f(x)d ≥c∥d∥2, ∀x ∈S, d ∈Rn.
In the following, we are concerned with the level set which is closely
related to a convex function and important to the minimization algorithm.
The following theorem shows that the level set Lα corresponding to a convex
function is convex.
Theorem 1.3.18 Let S ⊂Rn be a nonempty convex set, f a convex function
deﬁned on S, α a real number. Then the level set Lα = {x | x ∈S, f(x) ≤α}
is a convex set.
Proof.
Let x1, x2 ∈Lα, then x1, x2 ∈S, f(x1) ≤α, f(x2) ≤α.
Let
λ ∈(0, 1) and x = λx1 + (1 −λ)x2. Then from the convexity of S, we have
x ∈S. Also since f is convex,
f(x) ≤λf(x1) + (1 −λ)f(x2) ≤λα + (1 −λ)α = α.
Hence x ∈Lα, which implies that Lα is a convex set.
2
From Theorem 1.3.18 and Theorem 1.3.9, we know immediately that if
f is a continuously convex function, then the level set Lα is a closed convex
set. Furthermore, we also have
Theorem 1.3.19 Let f(x) be twice continuously diﬀerentiable on S ⊂Rn,
where S is a nonempty convex set. Suppose that there exists a number m > 0
such that
uT ∇2f(x)u ≥m∥u∥2, ∀x ∈L(x0), u ∈Rn.
(1.3.36)
Then the level set L(x0) = {x ∈S | f(x) ≤f(x0)} is a bounded closed convex
set.
Proof.
By using Theorem 1.3.14, (1.3.36) implies that f is convex on
L(x0), and then it follows from Theorem 1.3.18 that L(x0) is convex. Note
that f(x) is continuous, then L(x0) is a closed convex set for all x0 ∈Rn.

48
CHAPTER 1. INTRODUCTION
Now we prove the boundedness of L(x0). Using (1.3.36) and the fact that
L(x0) is convex, we have for any x, y ∈L(x0),
m∥y −x∥2 ≤(y −x)T ∇2f(x + α(y −x))(y −x).
Also by twice diﬀerentiability and the above inequality, we have
f(y)
=
f(x) + ∇f(x)T (y −x)
+
 1
0
 t
0
(y −x)T ∇2f(x + α(y −x))(y −x)dαd t
≥
f(x) + ∇f(x)T (y −x) + 1
2m∥y −x∥2,
where m is independent of x and y. Therefore for arbitrary y ∈L(x0) and
y ̸= x0,
f(y) −f(x0)
≥
∇f(x0)T (y −x0) + 1
2m∥y −x0∥2
≥
−∥∇f(x0)∥∥y −x0∥+ 1
2m∥y −x0∥2.
Noting that f(y) ≤f(x0), the above inequality implies
∥y −x0∥≤2
m∥∇f(x0)∥,
which shows that the level set L(x0) is bounded.
2
To conclude this subsection, we give a proof of Minkowski inequality
which is an application of convexity of function.
Minkowski inequality:
∥x + y∥p ≤∥x∥p + ∥y∥p,
(1.3.37)
i.e.,
 n

i=1
|xi + yi|p
1/p
≤
 n

i=1
|xi|p
1/p
+
 n

i=1
|yi|p
1/p
,
(1.3.38)
where p ≥1.
Proof.
If x or y is the zero vector, the result is obvious. Now suppose
that x ̸= 0 and y ̸= 0.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
49
If p = 1, since |xi + yi| ≤|xi| + |yi|, i = 1, · · · , n, then summing over i
gives the result.
Now let p > 1 and consider the function
φ(t) = tp, t > 0.
Since
φ′′(t) = p(p −1)tp−2 > 0,
then φ(t) is strictly convex. Note that
∥x∥p
∥x∥p + ∥y∥p
+
∥y∥p
∥x∥p + ∥y∥p
= 1,
it follows from the deﬁnition of convex function that

∥x∥p
∥x∥p + ∥y∥p
|xi|
∥x∥p
+
∥y∥p
∥x∥p + ∥y∥p
|yi|
∥y∥p
p
≤
∥x∥p
∥x∥p + ∥y∥p

|xi|
∥x∥p
p
+
∥y∥p
∥x∥p + ∥y∥p

|yi|
∥y∥p
p
.
(1.3.39)
Hence, using (1.3.39), we get
n

i=1

|xi + yi|
∥x∥p + ∥y∥p
p
≤
n

i=1

|xi| + |yi|
∥x∥p + ∥y∥p
p
=
n

i=1

∥x∥p
∥x∥p + ∥y∥p
|xi|
∥x∥p
+
∥y∥p
∥x∥p + ∥y∥p
|yi|
∥y∥p
p
≤
n

i=1

∥x∥p
∥x∥p + ∥y∥p

|xi|
∥x∥p
p
+
∥y∥p
∥x∥p + ∥y∥p

|yi|
∥y∥p
p
≤
∥x∥p
∥x∥p + ∥y∥p
n

i=1

|xi|
∥x∥p
p
+
∥y∥p
∥x∥p + ∥y∥p
n

i=1

|yi|
∥y∥p
p
=
∥x∥p
∥x∥p + ∥y∥p
∥x∥p
p
∥x∥p
p
+
∥y∥p
∥x∥p + ∥y∥p
∥y∥p
p
∥y∥p
p
=
1,

50
CHAPTER 1. INTRODUCTION
which implies that
n

i=1
|xi + yi|p ≤(∥x∥p + ∥y∥p)p.
Taking the p-th root gives our result.
2
1.3.3
Separation and Support of Convex Sets
The separation and support of convex sets are important tools for research
of optimality conditions. We ﬁrst discuss the projection theorem which char-
acterizes the projection and describes the suﬃcient and necessary condition
for the distance between a closed convex set and a point not in the set to be
minimal.
Theorem 1.3.20 (Projection Theorem)
Let S ⊂Rn be a nonempty closed convex set and y /∈S, then there exists
a unique point ¯x ∈S with minimal distance from y, i.e.,
∥y −¯x∥= inf
x∈S ∥y −x∥.
(1.3.40)
Furthermore, ¯x is the minimal point of (1.3.40) if and only if
⟨y −¯x, x −¯x⟩≤0, ∀x ∈S,
(1.3.41)
or say that ¯x is the projection PS(y) of y on S if and only if (1.3.41) holds.
Proof.
Let
inf{∥y −x∥| x ∈S} = γ > 0.
(1.3.42)
There is a sequence {xk} ⊂S so that ∥y −xk∥→γ. In the following, we
prove {xk} is a Cauchy sequence and hence there exists a limit ¯x ∈S.
By the parallelogram law, we have
∥xk −xm∥2
=
2∥xk −y∥2 + 2∥xm −y∥2 −∥xk + xm −2y∥2
=
2∥xk −y∥2 + 2∥xm −y∥2 −4

xk + xm
2
−y

2
.(1.3.43)
Note that (xk + xm)/2 ∈S, we have, from the deﬁnition of γ,

xk + xm
2
−y

2
≥γ2.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
51
Therefore,
∥xk −xm∥2 ≤2∥xk −y∥2 + 2∥xm −y∥2 −4γ2.
Taking k and m suﬃciently large yields
∥xk −xm∥→0
which indicates that {xk} is a Cauchy sequence with limit ¯x.
Since S is
closed, then ¯x ∈S. This shows there exists ¯x such that ∥y −¯x∥= γ.
Next, we prove the uniqueness. Suppose that ¯x, ¯x′ ∈S and satisfy
∥y −¯x∥= ∥y −¯x′∥= γ.
(1.3.44)
Since S is convex, (¯x + ¯x′)/2 ∈S. Then
y −¯x + ¯x′
2
 ≤1
2∥y −¯x∥+ 1
2∥y −¯x′∥= γ.
(1.3.45)
If the strict inequality holds, we get a contradiction to (1.3.42). Then the
equality holds in (1.3.45) and we have
y −¯x = λ(y −¯x′), for some λ.
So, it follows from (1.3.44) that |λ| = 1. If λ = −1, we have y = (¯x+¯x′)/2 ∈S
which contradicts y /∈S. Therefore, λ = 1, that means ¯x = ¯x′.
Finally, we prove that the distance between ¯x ∈S and y /∈S is minimal
if and only if (1.3.41) holds.
Take x arbitrary in S and suppose (1.3.41) holds. Since
∥y −x∥2
=
∥y −¯x + ¯x −x∥2
=
∥y −¯x∥2 + ∥¯x −x∥2 + 2(¯x −x)T (y −¯x),
then ∥y −x∥2 ≥∥y −¯x∥2 which is the desired suﬃciency.
Conversely, let ∥y −x∥2 ≥∥y −¯x∥2, ∀x ∈S. Since ¯x + λ(x −¯x) ∈S with
λ ∈(0, 1), then we have
∥y −¯x −λ(x −¯x)∥2 ≥∥y −¯x∥2.
Developing the square gives
∥y −¯x −λ(x −¯x)∥2 = ∥y −¯x∥2 + λ2∥x −¯x∥2 + 2λ(x −¯x)T (¯x −y).
Then we get
λ2∥x −¯x∥2 + 2λ(x −¯x)T (¯x −y) ≥0.
Dividing by λ and letting λ ↓0, we obtain the result.
2

52
CHAPTER 1. INTRODUCTION
Figure 1.3.5 The angle-characterization of a projection
In fact, if we note that ¯x = PS(y) is the solution of
min
x∈S φ(x) ∆= 1
2∥x −y∥2,
then, for the above minimization problem, one concludes from the optimality
condition that
⟨φ′(¯x), x −¯x⟩≥0, ∀x ∈S.
Since φ′(x) = x −y, we have
⟨¯x −y, x −¯x⟩≥0, ∀x ∈S
which is just (1.3.41).
Remark: If S is an aﬃne manifold (for example, a subspace), then
¯x −x ∈S whenever x −¯x ∈S. Therefore, (1.3.41) implies
⟨y −¯x, x −¯x⟩= 0, ∀x ∈S,
(1.3.46)
which is (y −¯x) ⊥S.
Now we can present the most fundamental separation theorem which
separates a closed convex set and a point not in the set. This theorem is
based on the above projection theorem.
Theorem 1.3.21 Let S ⊂Rn be a nonempty closed convex set and y /∈S.
Then there exist a nonzero vector p and a real number α such that
pT y > α and pT x ≤α, ∀x ∈S,
(1.3.47)
i.e.,
pT y > sup{pT x, ∀x ∈S}
(1.3.48)
which says there exists a hyperplane H = {x | pT x = α} that strictly separates
y and S.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
53
Proof.
Since S is a nonempty closed convex set and y /∈S, then, by
Theorem 1.3.20, there exists a unique point ¯x ∈S such that
(x −¯x)T (y −¯x) ≤0, ∀x ∈S.
Set p = y −¯x ̸= 0, then
0
≥
(y −¯x)T (y −¯x + x −y)
=
pT x −pT y + ∥p∥2.
Hence
pT y ≥pT x + ∥p∥2, ∀x ∈S.
Set α = sup{pT x | x ∈S}, and we get our result.
2
As a consequence of Theorem 1.3.21, we immediately obtain Farkas’
Lemma which has been used extensively in the derivation of optimality con-
ditions.
Theorem 1.3.22 (Farkas’ Lemma) Let A ∈Rm×n and c ∈Rn. Then ex-
actly one of the following two systems has a solution:
System 1
Ax ≤0, cT x > 0,
(1.3.49)
System 2
AT y = c, y ≥0.
(1.3.50)
Proof.
Suppose that there is a solution for (1.3.50), that is, there exists
y ≥0 such that AT y = c. Let x satisfy Ax ≤0, it follows from y ≥0 that
cT x = yT Ax ≤0,
which shows that (1.3.49) has no solution.
Now suppose (1.3.50) has no solution. Let
S = {x | x = AT y, y ≥0},
which is a polyhedral set, and hence it is a nonempty closed convex set and
c /∈S. By Theorem 1.3.21, there exist p ∈Rn and α ∈R such that
pT c > α and pT x ≤α, ∀x ∈S.
Since 0 ∈S, α ≥pT 0 = 0. Then pT c > 0. Also note that
α ≥pT x = pT AT y = yT Ap, ∀y ≥0

54
CHAPTER 1. INTRODUCTION
and that y could be made arbitrarily large, thus it follows that Ap ≤0. So,
there is a vector p ∈Rn which is a solution of (1.3.49), and the proof is
complete.
2
In order to discuss the separation between two convex sets, we need the
following deﬁnition and theorem of a supporting hyperplane.
Deﬁnition 1.3.23 Let S ⊂Rn be a nonempty set, p ∈Rn, and ¯x ∈∂S,
where ∂S denotes the boundary of S. If either
S ⊂H+ = {x ∈S | pT (x −¯x) ≥0}
(1.3.51)
or
S ⊂H−= {x ∈S | pT (x −¯x) ≤0},
(1.3.52)
then the hyperplane H = {x ∈S | pT (x −¯x) = 0} is called a supporting
hyperplane of S at ¯x. If, in addition, S ̸⊂H, then H is called a proper
supporting hyperplane of S at ¯x.
The following theorem shows that a convex set has a supporting hyper-
plane at each boundary point (see Figure 1.3.6).
Figure 1.3.6 Supporting hyperplane
Theorem 1.3.24 Let S ⊂Rn be a nonempty convex set and ¯x ∈∂S. Then,
there exists a hyperplane supporting S at ¯x; that is, there exists a nonzero
vector p such that
pT (x −¯x) ≤0, ∀x ∈¯S,
(1.3.53)
where ¯S denotes the closure of S.

1.3. CONVEX SETS AND CONVEX FUNCTIONS
55
Proof.
Since ¯x ∈∂S, there exists a sequence {yk} ̸⊂¯S so that yk →
¯x, (k →∞).
By Theorem 1.3.21, corresponding to each yk, there exists
pk ∈Rn with ∥pk∥= 1, such that
pT
k yk > pT
k x, ∀x ∈¯S.
(1.3.54)
Since {pk} is bounded, there is a convergent subsequence {pk}K with limit p
and ∥p∥= 1. For this subsequence, (1.3.54) holds, that is
pT
kjykj > pT
kjx, ∀x ∈¯S.
Fix x ∈¯S and take the limit as k ∈K and k →∞, we have pT ¯x ≥pT x, ∀x ∈
¯S, which is our desired result.
2
By use of Theorem 1.3.21 and Theorem 1.3.24, the following corollary is
obvious.
Corollary 1.3.25 Let S ⊂Rn be a nonempty convex set and ¯x /∈S. Then
there exists a nonzero vector p such that
pT (x −¯x) ≤0, ∀x ∈¯S.
(1.3.55)
Proof.
Let ¯x /∈S; there are two cases.
If ¯x /∈¯S, the conclusion is
immediate from Theorem 1.3.21. If ¯x ∈∂S, the corollary reduces to Theorem
1.3.24.
2
Now, we are going to discuss the separation theorems of two convex sets
which include separation theorem, strict separation theorem and strong sep-
aration theorem.
Deﬁnition 1.3.26 Let S1, S2 ⊂Rn be nonempty convex sets. If
pT x ≥α, ∀x ∈S1 and pT x ≤α, ∀x ∈S2,
(1.3.56)
then the hyperplane H = {x | pT x = α} is said to separate S1 and S2. If, in
addition, S1 ∪S2 ̸⊂H, then H is said to properly separate S1 and S2. If
pT x > α, ∀x ∈S1 and pT x < α, ∀x ∈S2,
(1.3.57)
then H is said to strictly separate S1 and S2. If
pT x ≥α + ϵ, ∀x ∈S1 and pT x ≤α, ∀x ∈S2,
(1.3.58)
then H is said to strongly separate S1 and S2, where ϵ > 0.

56
CHAPTER 1. INTRODUCTION
Theorem 1.3.27 (Separation Theorem)
Let S1, S2 ⊂Rn be nonempty convex sets. If S1 ∩S2 = φ, then there
exists a hyperplane separating S1 and S2, that is, there exists a nonzero vector
p ∈Rn such that
pT x1 ≤pT x2, ∀x1 ∈¯S1, x2 ∈¯S2.
(1.3.59)
Proof.
Let
S = S1 −S2 = {x1 −x2 | x1 ∈S1, x2 ∈S2}.
Note that S is a nonempty convex set and that 0 /∈S (otherwise, if 0 ∈S,
then we have x1 −x2 = 0 and x1 = x2 ∈S1 ∩S2 which implies S1 ∩S2 ̸= φ).
Hence, by Corollary 1.3.25, there exists a nonzero vector p such that
pT x ≤pT 0 = 0, ∀x ∈¯S,
which implies that
pT x1 ≤pT x2, ∀x1 ∈¯S1, x2 ∈¯S2.
Then we complete the proof.
2
Note that (1.3.59) also can be written as
sup{pT x | x ∈S1} ≤inf{pT x | x ∈S2}.
(1.3.60)
Theorem 1.3.28 (Strong Separation Theorem)
Let S1 and S2 be two closed convex sets on Rn, and suppose that S2 is
bounded. If S1∩S2 = φ, then there exists a hyperplane that strongly separates
S1 and S2, that is, there exist a nonzero vector p and ϵ > 0 such that
inf{pT x | x ∈S2} ≥sup{pT x | x ∈S1} + ϵ.
(1.3.61)
Proof.
Let S = S1 −S2. Note that S is convex and 0 /∈S. We ﬁrst
prove that S is closed.
Let {xk} ⊂S, xk →x.
By the deﬁnition of S,
xk = yk −zk, yk ∈S1, zk ∈S2. Since S2 is compact, there exists a convergent
subsequence {zk}K, zk →z, z ∈S2, k ∈K. Since
yk −zk →x, zk →z, ∀k ∈K,

1.4. OPTIMALITY CONDITIONS FOR UNCONSTRAINED CASE
57
then yk →y. Also since S1 is closed, y ∈S1. Therefore,
x = y −z, y ∈S1, z ∈S2,
which means x ∈S and S is closed.
Now we have that S is a closed convex set and 0 /∈S. Then, by Theorem
1.3.21, there exist nonzero vector p and real number α, such that
pT x ≤α, ∀x ∈S and pT 0 > α.
Hence, α < 0. Using the deﬁnition of S yields
pT z ≥pT y −α, ∀y ∈S1, z ∈S2,
which, by setting ϵ = −α > 0, is
inf{pT z | z ∈S2} ≥sup{pT y | y ∈S1} + ϵ.
2
Similarly, we can obtain the following strict separation theorem.
Theorem 1.3.29 (Strict Separation Theorem)
Let S1 and S2 be two closed convex sets on Rn, and suppose that S2 is
bounded. If S1 ∩S2 = φ, there exists a nonzero vector p such that
inf{pT x | x ∈S2} > sup{pT x | x ∈S1}.
(1.3.62)
Proof.
The result (1.3.62) is immediate from (1.3.61).
2
1.4
Optimality Conditions for Unconstrained Op-
timization
In this section we consider the unconstrained optimization problem
min f(x), x ∈Rn
(1.4.1)
and present its optimality conditions which include ﬁrst-order conditions and
second-order conditions.
In general, we have two types of minimizers: local minimizer and global
minimizer. In the following, we give their exact deﬁnitions.

58
CHAPTER 1. INTRODUCTION
Deﬁnition 1.4.1 A point x∗is called a local minimizer if there exists δ > 0
such that f(x∗) ≤f(x) for all x ∈Rn satisfying ∥x −x∗∥< δ.
A point x∗is called a strict local minimizer if there exists δ > 0 such that
f(x∗) < f(x) for all x ∈Rn with x ̸= x∗and ∥x −x∗∥< δ.
Deﬁnition 1.4.2 A point x∗is called a global minimizer if f(x∗) ≤f(x) for
all x ∈Rn. A point x∗is called a strict global minimizer if f(x∗) < f(x) for
all x ∈Rn with x ̸= x∗.
Figure 1.4.1 Types of minimal points
Note that, in practice, most algorithms are able to ﬁnd only a local mini-
mizer that is not a global minimizer. Normally, ﬁnding a global minimizer is
a diﬃcult task. In many practical applications, we are content with getting
a local minimizer. In addition, many global optimization algorithms proceed
by solving a sequence of local optimization algorithms. Hence, in this book,
our focus is on the model, property, convergence and computation of local
optimization algorithms. Usually, in the book, the minimizer refers to the
local minimizer. However, if the problem is a convex programming problem,
all local minimizers are also global minimizers.
The descent direction given in the following deﬁnition is an important
concept.
Deﬁnition 1.4.3 Let f : Rn →R be diﬀerentiable at x ∈Rn. If there exists
a vector d ∈Rn such that
⟨∇f(x), d⟩< 0,
(1.4.2)
then d is called a descent direction of f at x.

1.4. OPTIMALITY CONDITIONS FOR UNCONSTRAINED CASE
59
By means of Taylor’s expansion,
f(xk + td) = f(xk) + t∇f(xk)T d + o(t),
then it is easy to see that
∃δ > 0 such that f(xk + td) < f(xk), ∀t ∈(0, δ)
if and only if d is a descent direction of f at xk.
Now we discuss the ﬁrst-order optimality condition.
Theorem 1.4.4 (First-Order Necessary Condition)
Let f : D ⊂Rn →R be continuously diﬀerentiable on an open set D. If
x∗∈D is a local minimizer of (1.4.1), then
∇f(x∗) = 0.
(1.4.3)
Proof.
[proof I] Let x∗be a local minimizer. Consider the sequence
xk = x∗−αk∇f(x∗), αk > 0.
By Taylor’s expansion, for k suﬃciently large, we have
0 ≤f(xk) −f(x∗) = −αk∇f(ηk)T ∇f(x∗),
where ηk is a convex combination of xk and x∗. Dividing by αk and taking
the limit, it follows from f ∈C1 that
0 ≤−∥∇f(x∗)∥2
which means ∇f(x∗) = 0.
2
[proof II] (By contradiction). Suppose that ∇f(x∗) ̸= 0. Taking d =
−∇f(x∗) yields
dT ∇f(x∗) = −∥∇f(x∗)∥2 < 0.
So, d is a descent direction and there exists δ > 0 such that
f(x∗+ αd) < f(x∗), ∀α ∈(0, δ)
which contradicts the assumption that x∗is a local minimizer.
2

60
CHAPTER 1. INTRODUCTION
[proof III] Let x∗be a local minimizer, then there exists δ > 0 so that
f(x) ≥f(x∗) for any x with ∥x −x∗∥< δ. By Taylor’s expansion,
f(x) = f(x∗) + ∇f(x∗)T (x −x∗) + o(∥x −x∗∥) ≥f(x∗).
Dividing ∥x −x∗∥and letting x →x∗yield
∇f(x∗)T (x −x∗)
∥x −x∗∥≥0.
Setting s = (x −x∗)/∥x −x∗∥, the above inequality is
∇f(x∗)T s ≥0, ∀s with ∥s∥= 1.
Choosing s = ±ei, (i = 1, · · · , n), we obtain ∇f(x∗) = 0.
2
Theorem 1.4.4 says that if x∗is a local minimizer, f has a zero slope at
x∗. The following theorem indicates that if x∗is a local minimizer, f has
nonnegative curvature at x∗besides zero slope.
Theorem 1.4.5 (Second-Order Necessary Condition)
Let f : D ⊂Rn →R be twice continuously diﬀerentiable on an open set
D. If x∗is a local minimizer of (1.4.1), then ∇f(x∗) = 0 and ∇2f(x∗) is
positive semideﬁnite.
Proof.
[proof I] We have known from Theorem 1.4.4 that ∇f(x∗) = 0,
hence we only need to prove that ∇2f(x∗) is positive semideﬁnite. Consider
the sequence
xk = x∗+ αkd, αk > 0,
where d is arbitrary. Since f ∈C2 and ∇f(x∗) = 0, then by Taylor’s expan-
sion, we have for k suﬃciently large that
0 ≤f(xk) −f(x∗) = 1
2α2
kdT ∇2f(ηk)d,
where ηk is a convex combination of xk and x∗. Dividing by 1
2α2
k and taking
the limit, we get
dT ∇2f(x∗)d ≥0, ∀d ∈Rn.
Hence we complete the proof.
2
[proof II] (By contradiction). Suppose that ∇2f(x∗) is not positive semidef-
inite, then we can choose d ∈Rn such that dT ∇2f(x∗)d < 0. Since f ∈C2,

1.4. OPTIMALITY CONDITIONS FOR UNCONSTRAINED CASE
61
there exists δ > 0 and we can choose ϵ > 0 such that x∗+ ϵd ∈B(x∗, δ) and
dT ∇2f(x∗+ ϵd)d < 0.
By use of ∇f(x∗) = 0, it follows that
f(x∗+ ϵd) = f(x∗) + 1
2ϵ2dT ∇2f(x∗+ θϵd)d,
where 0 ≤θ ≤1.
Therefore f(x∗+ ϵd) < f(x∗).
This contradicts the
assumption that x∗is a local minimizer.
2
Next, we describe the second-order suﬃcient condition.
Theorem 1.4.6 (Second-Order Suﬃcient Condition)
Let f : D ⊂Rn →R be twice continuously diﬀerentiable on an open set
D. If ∇f(x∗) = 0 and ∇2f(x∗) is positive deﬁnite, then x∗∈D is a strict
local minimizer.
Proof.
[proof I] Assume that ∇f(x∗) = 0 and ∇2f(x∗) is positive deﬁnite.
By Taylor’s expansion, for any vector d ∈Rn such that x∗+ d lies in a
neighborhood of x∗in which ∇2f(x∗+ d) is positive deﬁnite, we have
f(x∗+ d) = f(x∗) + 1
2dT ∇2f(x∗+ θd)d,
where θ ∈(0, 1). Then we can choose δ > 0 such that x∗+ d ∈B(x∗, δ) and
dT ∇2f(x∗+ θd)d > 0. Therefore,
f(x∗+ d) > f(x∗)
which shows our result.
2
[proof II] (By contradiction). Assume that x∗is not a strict local min-
imizer, then there exists a sequence {xk} ⊂D with xk ̸= x∗, ∀k, such that
f(xk) ≤f(x∗) for k suﬃciently large. By use of Taylor’s expansion,
0
≥
f(xk) −f(x∗)
=
∇f(x∗)T (xk −x∗) + 1
2(xk −x∗)T ∇2f(ηk)(xk −x∗),
where ηk is a convex combination of xk and x∗. Using ∇f(x∗) = 0, dividing
by 1
2∥xk −x∗∥2 and taking the limit, we have
0 ≥¯eT ∇2f(x∗)¯e,
(1.4.4)

62
CHAPTER 1. INTRODUCTION
where ¯e is the accumulation point of the uniformly bounded sequence {(xk −
x∗)/∥xk −x∗∥} and ∥¯e∥= 1.
Obviously, (1.4.4) contradicts the positive
deﬁniteness of ∇2f(x∗).
2
Deﬁnition 1.4.7 A point x∗∈Rn is said to be a stationary (or critical)
point for the diﬀerentiable f if ∇f(x∗) = 0.
Theorem 1.4.4 tells us that if x∗is a local minimizer, then it is a station-
ary point. However, the converse is not true. If x∗is a stationary point, it
is possible for x∗to be a local minimizer or maximizer, it is also possible for
x∗to not be an extreme point. If a stationary point x∗is neither minimizer
nor maximizer, it is called a saddle point. Therefore, a stationary point need
not be a local minimizer. But if the objective function which is diﬀeren-
tiable is convex, its stationary points are the local minimizers and the global
minimizers.
Theorem 1.4.8 below says, for a convex function, that its local minimizer
is also a global minimizer. Theorem 1.4.9 says, for a diﬀerentiable convex
function, that its stationary point is also a global minimizer.
Theorem 1.4.8 Let S ⊂Rn be a nonempty convex set and f : S ⊂Rn →R.
Let x∗∈S be a local minimizer for minx∈S f(x).
1. If f is convex, then x∗is also a global minimizer.
2. If f is strictly convex, then x∗is a unique global minimizer.
Proof.
(1) Let f be convex and x∗be a local minimizer, then there exists
a δ-neighborhood B(x∗, δ) such that
f(x) ≥f(x∗), ∀x ∈S ∩B(x∗, δ).
(1.4.5)
By contradiction, suppose that x∗is not a global minimizer. Then we can
ﬁnd some ˆx ∈S such that f(ˆx) < f(x∗). By convexity of f, we have for
α ∈(0, 1),
f(αˆx + (1 −α)x∗)
≤
αf(ˆx) + (1 −α)f(x∗)
<
αf(x∗) + (1 −α)f(x∗)
=
f(x∗).
(1.4.6)

1.5. STRUCTURE OF OPTIMIZATION METHODS
63
But for suﬃciently small α > 0, αˆx + (1 −α)x∗∈S ∩B(x∗, δ). Therefore,
(1.4.6) contradicts (1.4.5). This contradiction proves the ﬁrst conclusion.
(2) From part (1) we have that x∗is a global minimizer because strict
convexity means convexity. Therefore, it is enough to prove the uniqueness.
By contradiction, suppose that x∗is not the unique global minimizer, so
that we can ﬁnd x ∈S, x ̸= x∗, such that f(x) = f(x∗).
By strict convexity of f,
f(1
2x + 1
2x∗) < 1
2f(x) + 1
2f(x∗) = f(x∗).
(1.4.7)
Note from the convexity of S that 1
2x+ 1
2x∗∈S. Therefore, (1.4.7) contradicts
the fact that x∗is a global minimizer.
2
Theorem 1.4.9 Let f : Rn →R be a diﬀerentiable convex function. Then
x∗is a global minimizer if and only if ∇f(x∗) = 0.
Proof.
Suﬃciency. Let f be a diﬀerentiable convex function in Rn and
∇f(x∗) = 0, then
f(x) ≥f(x∗) + ∇f(x∗)(x −x∗) = f(x∗), ∀x ∈Rn
which indicates that x∗is a global minimizer of f.
Necessity. It is obvious because the global minimizer is also a local mini-
mizer, and is also a stationary point.
2
The optimality conditions of constrained optimization will be discussed
in Chapter 8.
1.5
Structure of Optimization Methods
Usually, the optimization method is an iterative one for ﬁnding the minimizer
of an optimization problem. The basic idea is that, given an initial point
x0 ∈Rn, one generates an iterate sequence {xk} by means of some iterative
rule, such that when {xk} is a ﬁnite sequence, the last point is the optimal
solution of the model problem; when {xk} is inﬁnite, it has a limit point
which is the optimal solution of the model problem. A typical behavior of
an algorithm which is regarded as acceptable is that the iterates xk move
steadily towards the neighborhood of a local minimizer x∗, and then rapidly
converge to the point x∗. When a given convergence rule is satisﬁed, the

64
CHAPTER 1. INTRODUCTION
iteration will be terminated. In general, the most natural stopping criterion
is
∥∇f(xk)∥≤δ,
(1.5.1)
where δ is a prescribed tolerance. If (1.5.1) is satisﬁed, it implies that the
gradient vector ∇f(xk) tends to zero and the iterate sequence {xk} converges
to a stationary point.
Let xk be the k-th iterate, dk k-th direction, αk k-th steplength factor.
Then the k-th iteration is
xk+1 = xk + αkdk.
(1.5.2)
We can see from (1.5.2) that the diﬀerent stepsize αk and diﬀerent direction
dk form diﬀerent methods. In Chapter 2 we will discuss several methods to
determine αk. In Chapter 3 we will present various methods to ﬁnd search
directions dk. Most optimization methods are so-called descent methods in
the sense that f satisﬁes at each iteration
f(xk+1) = f(xk + αkdk) < f(xk),
in which dk is a descent direction deﬁned by Deﬁnition 1.4.3.
The basic scheme of optimization methods is as follows.
Algorithm 1.5.1 (Basic Scheme)
Step 0. (Initial step) Given initial point x0 ∈Rn and the tolerance
ϵ > 0.
Step 1. (Termination criterion) If ∥∇f(xk)∥≤ϵ, stop.
Step 2. (Finding the direction) According to some iterative scheme,
ﬁnd dk which is a descent direction.
Step 3. (Line search) Determine the stepsize αk such that the objec-
tive function value decreases, i.e.,
f(xk + αkdk) < f(xk).
Step 4. (Loop) Set xk+1 = xk + αkdk, k := k + 1, and loop to Step
1.
2

1.5. STRUCTURE OF OPTIMIZATION METHODS
65
Convergence rate, which is a local characterization of an algorithm, can
measure the eﬀectiveness of an optimization method. We now give a brief
description associated with diﬀerent types of convergence rate. More details
can be found in Ortega and Rheinboldt (1970).
Let the iterate sequence {xk} generated by an algorithm converge to x∗
in some norm, i.e.,
lim
k→∞∥xk −x∗∥= 0.
(1.5.3)
If there are real number α ≥1 and a positive constant β which is independent
of the iterative number k, such that
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥α = β,
(1.5.4)
we say that {xk} has α-order of Q-convergence rate, where Q-convergence
rate means Quotient-convergence rate. In particular,
1. when α = 1 and β ∈(0, 1), the sequence {xk} is said to converge
Q-linearly;
2. when α = 1 and β = 0, or 1 < α < 2 and β > 0, the sequence {xk} is
said to converge Q-superlinearly;
3. when α = 2, we say that {xk} has Q-quadratic convergence rate.
The primary motivation for introducing the Q-convergence rate is to com-
pare the speed of convergence of diﬀerent iterations. It is not diﬃcult to see
that the convergence rate depends on α and (more weakly) on β. Suppose
that there are two sequences {xk} and {x′
k} and that their Q-order and Q-
factor are respectively {α, β} and {α′, β′}. If α > α′, then the sequence with
Q-α order converges faster than the sequence with Q-α′ order. For exam-
ple, a quadratically convergent sequence will eventually converge faster than
linearly and superlinearly convergent sequences. When α = α′, i.e., their
Q-order of convergence rate is the same, if β < β′, then the sequence {xk} is
faster than {x′
k}.
Mainly, we are concerned with Q-linear, Q-superlinear and Q-quadratic
convergence. Usually, if the convergence rate of an algorithm is Q-superlinear
or Q-quadratic, we say that it has rapid convergence rate. For example, quasi-
Newton methods converge Q-superlinearly, and Newton’s method converges
Q-quadratically.

66
CHAPTER 1. INTRODUCTION
Another measure of the convergence rate which is weaker than Q-convergence
rate is R-convergence rate which means Root-convergence rate.
Let {xk} ⊂Rn be any sequence that converges to x∗. Let
Rp =

lim supk→∞∥xk −x∗∥1/k,
if p = 1;
lim supk→∞∥xk −x∗∥1/pk,
if p > 1.
If R1 = 0, {xk} is said to be R-superlinearly convergent to x∗.
If 0 < R1 < 1, {xk} is said to be R-linearly convergent to x∗.
If R1 = 1, {xk} is said to be R-sublinearly convergent to x∗.
Similarly, if R2 = 0, 0 < R2 < 1, R2 ≥1 respectively, then {xk} is said to
be R-superquadratically, R-quadratically, and R-subquadratically convergent
to x∗respectively.
The above R-rate of convergence can also be stated as follows:
The sequence {xk} is said to be R-linearly convergent if there is a sequence
of nonnegative scalars {qk} such that
∥xk −x∗∥≤qk
for all k, and {qk} converges Q-linearly to zero.
Similarly, the sequence {xk} is said to be R-superlinearly convergent if {qk}
converges Q-superlinearly to zero; the sequence {xk} is said to be R-quadratically
convergent if {qk} converges Q-quadratically to zero.
Similar to Q-rate of convergence, R-rate of convergence also depends on
R-order p and R-factor Rp. The higher the R-order is, the faster the corre-
sponding sequence converges. When the R-order is the same, the smaller the
R-factor is, the faster the corresponding sequence converges.
Throughout this book we mainly discuss Q-convergence rate. Hence, if
there is not speciﬁc indication, the convergence rate refers to Q-convergence
rate.
As indicated above, usually an algorithm with superlinear or quadratic
rate is said to be desirable. However, it must be appreciated that the theo-
retical results of the convergence and convergence rate are not a guarantee of
good performance. Not only do these results themselves fall short of guaran-
tee of good behavior, but also they neglect computer round-oﬀerrors which
may be crucial. In addition, these results often impose certain restrictions
on f(x) which may not be easy to verify, and in some cases (for example,
in the convex case), these conditions may not be satisﬁed in practice. Thus,

1.5. STRUCTURE OF OPTIMIZATION METHODS
67
the development of an optimization method also relies on numerical experi-
mentation. The ideal is a good selection of experimental testing backed up
by the proofs of convergence and convergence rate.
We have known from the above discussion that the convergence rate mea-
sures the local behavior of an algorithm and is used in local analysis. The
theorem below gives a characterization of superlinear convergence which is
useful for constructing termination criteria.
Theorem 1.5.2 If the sequence {xk} converges Q-superlinearly to x∗, then
lim
k→∞
∥xk+1 −xk∥
∥xk −x∗∥
= 1.
(1.5.5)
However, in general, the converse is not true.
Proof.
For a given integer k ≥0,
∥xk+1 −x∗∥
∥xk −x∗∥
=
∥(xk+1 −xk) + (xk −x∗)∥
∥xk −x∗∥
≥

∥xk+1 −xk∥
∥xk −x∗∥
−∥xk −x∗∥
∥xk −x∗∥
 .
It follows from the deﬁnition of Q-superlinear convergence that
lim
k→∞
∥xk+1 −xk∥
∥xk −x∗∥
= 1.
To show that the converse is not true, we give a counter-example. In
normed space {R, | · |}, deﬁne a sequence {xk} as follows:
x2i−1 = 1
i!
(i = 1, 2, · · ·),
x2i = 2x2i−1
(i = 1, 2 · · ·).
Obviously, x∗= 0. We have
|xk+1 −xk|
|xk −x∗|
=

1,
k = 2i −1, i ≥1,
1 −
1
2(i+1),
k = 2i, i ≥1.
So, {xk} satisﬁes (1.5.5) but does not converge Q-superlinearly to x∗.
2

68
CHAPTER 1. INTRODUCTION
This theorem shows that if an algorithm is convergent Q-superlinearly,
instead of ∥xk −x∗∥, we can use ∥xk+1 −xk∥to give a termination criterion,
and the estimation will be improved as k increases.
Finally, we discuss some termination criteria which are used frequently
in practice. In order to guarantee convergence of an algorithm, we require
|f(xk) −f(x∗)| ≤ϵ or ∥xk −x∗∥≤ϵ,
where the parameter ϵ is user-supplied. Unfortunately, these are not practi-
cable since they need the information of the solution x∗.
Instead, we often use the following termination criteria:
∥∇f(xk)∥≤ϵ3,
(1.5.6)
∥xk+1 −xk∥≤ϵ1,
(1.5.7)
f(xk) −f(xk+1) ≤ϵ1.
(1.5.8)
Normally, when an algorithm can be expected to converge rapidly, it
is suggested to use (1.5.7) or (1.5.8).
When an algorithm has ﬁrst-order
derivative information and can be expected to converge less rapidly, a test
based on (1.5.6) may be appropriate.
Himmeblau [174] suggested that it is suitable to use (1.5.7) together with
(1.5.8) as follows:
When ∥xk∥> ϵ2 and |f(xk)| > ϵ2, use
∥xk+1 −xk∥
∥xk∥
≤ϵ1, |f(xk) −f(xk+1)|
|f(xk)|
≤ϵ1;
(1.5.9)
otherwise, use
∥xk+1 −xk∥≤ϵ1, |f(xk) −f(xk+1)| ≤ϵ1.
(1.5.10)
He also suggested using (1.5.9)-(1.5.10) together with (1.5.6).
In general, take ϵ1 = ϵ2 = 10−5, ϵ3 = 10−4.
Exercises
1. Let A be an n×n nonsingular matrix. Prove that ∥Ax∥≥∥x∥/∥A−1∥.
2. Prove the equivalence (1.2.22)-(1.2.26) of vector norms.

1.5. STRUCTURE OF OPTIMIZATION METHODS
69
3. Prove Cauchy-Schwarz inequality (1.2.34). Further, prove inequality
(1.2.35).
4. Prove (1.2.45).
5.
Let A = UDV ∗be the singular value decomposition.
Prove that
A+ = V D+U∗, where D+ is deﬁned in (1.2.54).
6.
Prove Sherman-Morrison formula (1.2.67) and Sherman-Morrison-
Woodburg formula (1.2.68).
7. Show Theorem 1.2.6 (Von-Neumann Lemma).
8. Prove (1.2.69) and (1.2.70).
9. Prove that a function that is Fr´echet diﬀerentiable must be Gateaux
diﬀerentiable, but the converse is not true.
10. Prove Theorem 1.2.23.
11. Show that the intersection of ﬁnitely many convex sets is a convex set.
12. Show by induction that the set S ⊂Rn is convex if and only if for
any x1, x2, · · · , xm ∈S,
m

i=1
αixi ∈S,
where 
m
i=1 αi = 1, αi ≥0, i = 1, · · · , m. That means a convex combination
of arbitrarily ﬁnitely many points of a convex set still belongs to the convex
set.
13. Let A ∈Rm×n, b ∈Rm. Show, by deﬁnition, that
S = {x ∈Rn | Ax = b, x ≥0}
is a convex set.
14. Let
D1 = {x| x1 + x2 ≤1,
x1 ≥0},
D2 = {x| x1 −x2 ≥0,
x1 ≤0}.

70
CHAPTER 1. INTRODUCTION
Set D = D1∪D2. Show that D is not necessarily convex though both D1 and
D2 are convex. This means that the union of convex sets is not necessarily a
convex set.
15. Write the convex hull of the set S = {(0, 0)T , (1, 0)T , (0, 1)T }.
16. Let S ⊆Rn. Prove that the following two statements are equivalent.
(1) The convex hull is the set of all convex combinations of arbitrarily ﬁnitely
many elements of S as deﬁned in (1.3.3).
(2) The convex hull conv(S) is the intersection of all convex sets containing S.
17. Let fi(x), i = 1, 2, · · · , m, be convex functions deﬁned on convex set
D ⊂Rn. Show that the function
g(x) =
m

i=1
αifi(x)
is also a convex function on D, where 
m
i=1 αi = 1, αi ≥0, i = 1, 2, · · · , m.
This means that the convex combination of convex functions is a convex
function.
18. Discriminate convexity of the following functions:
(1) f(x1, x2) = x1e−(x1+x2);
(2) f(x1, x2, x3) = x2
1 + 3x2
2 + 9x2
3 −2x1x2 + 6x2x3 + 2x3x1.
19. Prove Theorem 1.3.11.
20. State the ﬁrst-order and second-order optimality conditions for un-
constrained optimization and outline their proofs.

Chapter 2
Line Search
2.1
Introduction
Line search, also called one-dimensional search, refers to an optimization pro-
cedure for univariable functions. It is the base of multivariable optimization.
As stated before, in multivariable optimization algorithms, for given xk, the
iterative scheme is
xk+1 = xk + αkdk.
(2.1.1)
The key is to ﬁnd the direction vector dk and a suitable step size αk. Let
φ(α) = f(xk + αdk).
(2.1.2)
So, the problem that departs from xk and ﬁnds a step size in the direction
dk such that
φ(αk) < φ(0)
is just line search about α.
If we ﬁnd αk such that the objective function in the direction dk is mini-
mized, i.e.,
f(xk + αkdk) = min
α>0 f(xk + αdk),
or
φ(αk) = min
α>0 φ(α),
such a line search is called exact line search or optimal line search, and αk is
called optimal step size. If we choose αk such that the objective function has
acceptable descent amount, i.e., such that the descent f(xk)−f(xk+αkdk) >

72
CHAPTER 2. LINE SEARCH
0 is acceptable by users, such a line search is called inexact line search, or
approximate line search, or acceptable line search.
Since, in practical computation, theoretically exact optimal step size gen-
erally cannot be found, and it is also expensive to ﬁnd almost exact step
size, therefore the inexact line search with less computation load is highly
popular.
The framework of line search is as follows. First, determine or give an
initial search interval which contains the minimizer; then employ some section
techniques or interpolations to reduce the interval iteratively until the length
of the interval is less than some given tolerance.
Next, we give a notation about the search interval and a simple method
to determine the initial search interval.
Deﬁnition 2.1.1 Let φ : R →R, α∗∈[0, +∞), and
φ(α∗) = min
α≥0 φ(α).
If there exists a closed interval [a, b] ⊂[0, +∞) such that α∗∈[a, b], then
[a, b] is called a search interval for one-dimensional minimization problem
minα≥0 φ(α). Since the exact location of the minimum of φ over [a, b] is not
known, this interval is also called the interval of uncertainty.
A simple method to determine an initial interval is called the forward-
backward method. The basic idea of this method is as follows. Given an
initial point and an initial steplength, we attempt to determine three points
at which their function values show “high–low–high” geometry. If it is not
successful to go forward, we will go backward. Concretely, given an initial
point α0 and a steplength h0 > 0. If
φ(α0 + h0) < φ(α0),
then, next step, depart from α0+h0 and continue going forward with a larger
step until the objective function increases. If
φ(α0 + h0) > φ(α0),
then, next step, depart from α0 and go backward until the objective function
increases. So, we will obtain an initial interval which contains the minimum
α∗.

2.1. INTRODUCTION
73
Algorithm 2.1.2 (Forward-Backward Method)
Step 1. Given α0 ∈[0, ∞), h0 > 0, the multiple coeﬃcient t > 1
(Usually t = 2). Evaluate φ(α0), k := 0.
Step 2. Compare the objective function values. Set αk+1 = αk + hk
and evaluate φk+1 = φ(αk+1). If φk+1 < φk, go to Step 3;
otherwise, go to Step 4.
Step 3. Forward step. Set hk+1 := thk, α := αk, αk := αk+1, φk :=
φk+1, k := k + 1, go to Step 2.
Step 4. Backward step. If k = 0, invert the search direction. Set
hk := −hk, αk := αk+1, go to Step 2; otherwise, set
a = min{α, αk+1}, b = max{α, αk+1},
output [a, b] and stop.
2
The methods of line search presented in this chapter use the unimodality
of the function and interval. The following deﬁnitions and theorem introduce
their concepts and properties.
Deﬁnition 2.1.3 Let φ : R →R, [a, b] ⊂R. If there is α∗∈[a, b] such that
φ(α) is strictly decreasing on [a, α∗] and strictly increasing on [α∗, b], then
φ(α) is called a unimodal function on [a, b]. Such an interval [a, b] is called
a unimodal interval related to φ(α).
The unimodal function can also be deﬁned as follows.
Deﬁnition 2.1.4 If there exists a unique α∗∈[a, b], such that for any
α1, α2 ∈[a, b], α1 < α2, the following statements hold:
if α2 < α∗, then φ(α1) > φ(α2);
if α1 > α∗, then φ(α1) < φ(α2);
then φ(α) is the unimodal function on [a, b].
Note that, ﬁrst, the unimodal function does not require the continuity and
diﬀerentiability of the function; second, using the property of the unimodal
function, we can exclude portions of the interval of uncertainty that do not

74
CHAPTER 2. LINE SEARCH
contain the minimum, such that the interval of uncertainty is reduced. The
following theorem shows that if the function φ is unimodal on [a, b], then the
interval of uncertainy could be reduced by comparing the function values of
φ at two points within the interval.
Theorem 2.1.5 Let φ : R →R be unimodal on [a, b]. Let α1, α2 ∈[a, b],
and α1 < α2. Then
1. if φ(α1) ≤φ(α2), then [a, α2] is a unimodal interval related to φ;
2. if φ(α1) ≥φ(α2), then [α1, b] is a unimodal interval related to φ.
Proof.
From the Deﬁnition 2.1.3, there exists α∗∈[a, b] such that φ(α)
is strictly decreasing over [a, α∗] and strictly increasing over [α∗, b]. Since
φ(α1) ≤φ(α2), then α∗∈[a, α2] (see Figure 2.1.1). Since φ(α) is unimodal
on [a, b], it is also unimodal on [a, α2]. Therefore [a, α2] is a unimodal interval
related to φ(α) and the proof of the ﬁrst part is complete.
The second part of the theorem can be proved similarly.
2
This theorem indicates that, for reducing the interval of uncertainty, we
must at least select two observations, evaluate and compare their function
values.
Figure 2.1.1 Properties of unimodal interval and unimodal function
2.2
Convergence Theory for Exact Line Search
The general form of an unconstrained optimization algorithm is as follows.
Algorithm 2.2.1 (General Form of Unconstrained Optimization)
Initial Step: Given x0 ∈Rn, 0 ≤ϵ ≪1.

2.2. CONVERGENCE THEORY FOR EXACT LINE SEARCH
75
k-th Step: Compute the descent direction dk;
Compute the step size αk, such that
f(xk + αkdk) = min
α≥0 f(xk + αdk);
(2.2.1)
Set
xk+1 = xk + αkdk;
(2.2.2)
If ∥∇f(xk+1)∥≤ϵ, stop; otherwise, repeat the above steps.
2
Set
φ(α) = f(xk + αdk),
(2.2.3)
obviously we have from the algorithm that
φ(0) = f(xk), φ(α) ≤φ(0).
In fact, (2.2.1) is to ﬁnd the global minimizer of φ(α) which is rather diﬃcult.
Instead, we look for the ﬁrst stationary point, i.e., take αk such that
αk = min{α ≥0 | ∇f(xk + αdk)T dk = 0}.
(2.2.4)
Since, by (2.2.1) and (2.2.4), we ﬁnd the exact minimizer and the stationary
point of φ(α) respectively, we say that (2.2.1) and (2.2.4) are exact line
searches.
Let ⟨dk, −∇f(xk)⟩denote the angle between dk and −∇f(xk), we have
cos⟨dk, −∇f(xk)⟩= −
dT
k ∇f(xk)
∥dk∥∥∇f(xk)∥.
(2.2.5)
The following theorem gives a bound of descent in function values for each
iteration in exact line search.
Theorem 2.2.2 Let αk > 0 be the solution of (2.2.1).
Let ∥∇2f(xk +
αdk)∥≤M ∀α > 0, where M is some positive number. Then
f(xk) −f(xk + αkdk) ≥
1
2M ∥∇f(xk)∥2 cos2⟨dk, −∇f(xk)⟩.
(2.2.6)

76
CHAPTER 2. LINE SEARCH
Proof.
From the assumptions we have that
f(xk + αdk) ≤f(xk) + αdT
k ∇f(xk) + α2
2 M∥dk∥2, ∀α > 0.
(2.2.7)
Set ¯α = −dT
k ∇f(xk)/(M∥dk∥2); it follows from the assumptions, (2.2.7) and
(2.2.5) that
f(xk) −f(xk + αkdk)
≥
f(xk) −f(xk + ¯αdk)
≥
−¯αdT
k ∇f(xk) −¯α2
2 M∥dk∥2
=
1
2
(dT
k ∇f(xk))2
M∥dk∥2
=
1
2M ∥∇f(xk)∥2
(dT
k ∇f(xk))2
∥dk∥2∥∇f(xk)∥2
=
1
2M ∥∇f(xk)∥2 cos2⟨dk, −∇f(xk)⟩.2
Now we are in position to state the convergence property of general un-
constrained optimization algorithms with exact line search. The following
two theorems state the convergence by diﬀerent forms.
Theorem 2.2.3 Let f(x) be a continuously diﬀerentiable function on an
open set D ⊂Rn, assume that the sequence from Algorithm 2.2.1 satisﬁes
f(xk+1) ≤f(xk) ∀k and ∇f(xk)T dk ≤0. Let ¯x ∈D be an accumulation
point of {xk} and K1 be an index set with K1 = {k | limk→∞xk = ¯x}. Also
assume that there exists M > 0 such that ∥dk∥< M, ∀k ∈K1. Then, if ¯d is
any accumulation point of {dk}, we have
∇f(¯x)T ¯d = 0.
(2.2.8)
Furthermore, if f(x) is twice continuously diﬀerentiable on D, then
¯dT ∇2f(¯x) ¯d ≥0.
(2.2.9)
Proof.
It is enough to prove (2.2.8) because the proof of (2.2.9) is similar.
Let K2 ⊂K1 be an index set with ¯d = limk∈K2 dk. If ¯d = 0, (2.2.8) is
trivial. Otherwise, we consider the following two cases.

2.2. CONVERGENCE THEORY FOR EXACT LINE SEARCH
77
(i) There exists an index set K3 ⊂K2 such that limk∈K3 αk = 0. Since
αk is an exact step size, then ∇f(xk +αkdk)T dk = 0. Since ∥dk∥is uniformly
bounded above and αk →0, taking the limit yields
∇f(¯x)T ¯d = 0.
(ii) Case of lim infk∈K2 αk = ¯α > 0. Let K4 ⊂K2 be an index set of k
with αk ≥¯α/2, ∀k ∈K4. Now assume that the conclusion (2.2.8) is not true,
then we have
∇f(¯x)T ¯d < −δ < 0.
So, there exist a neighborhood N(¯x) of ¯x and an index set K5 ⊂K4 such
that when x ∈N(¯x) and k ∈K5,
∇f(x)T dk ≤−δ/2 < 0.
Let ˆα be a suﬃciently small positive number, such that for all 0 ≤α ≤ˆα
and all k ∈K5, xk + αdk ∈N(¯x).
Take α∗= min(¯α/2, ˆα), then from
the non-increasing property of the algorithm, exact line search and Taylor’s
expansion, we have
f(¯x) −f(x0)
=
∞

k=0
[f(xk+1) −f(xk)]
≤

k∈K5
[f(xk+1) −f(xk)]
≤

k∈K5
[f(xk + α∗dk) −f(xk)]
(2.2.10)
=

k∈K5
∇f(xk + τkdk)T α∗dk
(2.2.11)
≤

k∈K5
−
 δ
2
!
α∗
=
−∞,
where 0 ≤τk ≤α∗. The above contradiction shows that (2.2.8) also holds
for case (ii).
The proof of (2.2.9) is similar. It is enough to note using the second-order
form of the Taylor expansion instead of the ﬁrst-order form in (2.2.11). In
fact, from (2.2.10) we have
f(¯x) −f(x0)

78
CHAPTER 2. LINE SEARCH
≤

k∈K5
[f(xk + α∗dk) −f(xk)]
=

k∈K5

∇f(xk)T (α∗dk) + (α∗)2
2
dT
k ∇2f(xk + τkdk)dk

for 0 ≤τk ≤α∗
≤

k∈K5
(α∗)2
2
dT
k ∇2f(xk + τkdk)dk for 0 ≤τk ≤α∗
≤

k∈K5

−1
2
 δ
2
!
(α∗)2

=
−∞.
(2.2.12)
We also get a contradiction which proves (2.2.9).
2
Theorem 2.2.4 Let ∇f(x) be uniformly continuous on the level set L =
{x ∈Rn | f(x) ≤f(x0)}. Let also the angle θk between −∇f(xk) and the
direction dk generated by Algorithm 2.2.1 is uniformly bounded away from
90◦, i.e., satisﬁes
θk ≤π
2 −µ, for some µ > 0.
(2.2.13)
Then ∇f(xk) = 0 for some k; or f(xk) →−∞; or ∇f(xk) →0.
Proof.
Assume that, for all k, ∇f(xk) ̸= 0 and f(xk) is bounded below.
Since {f(xk)} is monotonic descent, its limit exists. Therefore
f(xk) −f(xk+1) →0.
(2.2.14)
Assume, by contradiction, that ∇f(xk) →0 does not hold. Then there
exists ϵ > 0 and a subset K, such that ∥∇f(xk)∥≥ϵ∀k ∈K. Therefore
−∇f(xk)T dk/∥dk∥= ∥∇f(xk)∥cos θk ≥ϵ sin µ ∆= ϵ1.
(2.2.15)
Note that
f(xk + αdk)
=
f(xk) + α∇f(ξk)T dk
=
f(xk) + α∇f(xk)T dk + α[∇f(ξk) −∇f(xk)]T dk
≤
f(xk) + α∥dk∥

∇f(xk)T dk
∥dk∥
+ ∥∇f(ξk) −∇f(xk)∥

, (2.2.16)

2.2. CONVERGENCE THEORY FOR EXACT LINE SEARCH
79
where ξk lies between xk and xk + αdk. Since ∇f(x) is uniformly continuous
on the level set L, there exists ¯α such that when 0 ≤α∥dk∥≤¯α, we have
∥∇f(ξk) −∇f(xk)∥≤1
2ϵ1.
(2.2.17)
By (2.2.15)–(2.2.17), we get
f
 
xk + ¯α dk
∥dk∥
!
≤
f(xk) + ¯α

∇f(xk)T dk
∥dk∥
+ 1
2ϵ1

≤
f(xk) −1
2 ¯αϵ1.
Therefore
f(xk+1) ≤f
 
xk + ¯α dk
∥dk∥
!
≤f(xk) −1
2 ¯αϵ1,
which contradicts (2.2.14). The contradiction shows that ∇f(xk) →0. We
complete this proof.
2
In the remainder of this section, we discuss the convergence rate of min-
imization algorithms with exact line search. For convenience of the proof of
the theorem, we ﬁrst give some lemmas.
Lemma 2.2.5 Let φ(α) be twice continuously diﬀerentiable on the closed
interval [0, b] and φ′(0) < 0. If the minimizer α∗∈(0, b) of φ(α) on [0, b],
then
α∗≥˜α = −φ′(0)/M,
(2.2.18)
where M is a positive number such that φ′′(α) ≤M, ∀α ∈[0, b].
Proof.
Construct the auxiliary function
ψ(α) = φ′(0) + Mα,
which has the unique zero
˜α = −φ′(0)/M.
Noting that φ′′(α) ≤M, it follows that
φ′(α) = φ′(0) +
 α
0
φ′′(α)d α ≤φ′(0) +
 α
0
Md α = ψ(α).

80
CHAPTER 2. LINE SEARCH
Setting α = α∗in the above inequality and noting that φ′(α∗) = 0, we obtain
0 ≤ψ(α∗) = φ′(0) + Mα∗
which is (2.2.18).
2
Lemma 2.2.6 Let f(x) be twice continuously diﬀerentiable on Rn. Then for
any vector x, d ∈Rn and any number α, the equality
f(x + αd) = f(x) + α∇f(x)T d + α2
 1
0
(1 −t)[dT ∇2f(x + tαd)d]dt (2.2.19)
holds.
Proof.
From calculus, we have
f(x + αd) −f(x)
=
 1
0
df(x + tαd)
=
−
 1
0
[α∇f(x + tαd)T d]d(1 −t)
=
−[(1 −t)α∇f(x + tαd)T d]1
0 +
 1
0
(1 −t)d[α∇f(x + tαd)T d]
=
α∇f(x)T d + α2
 1
0
[(1 −t)dT ∇2f(x + tαd)d]dt.
2
Lemma 2.2.7 Let f(x) be twice continuously diﬀerentiable in the neighbor-
hood of the minimizer x∗. Assume that there exist ϵ > 0 and M > m > 0,
such that
m∥y∥2 ≤yT ∇2f(x)y ≤M∥y∥2, ∀y ∈Rn
(2.2.20)
holds when ∥x −x∗∥< ϵ. Then we have
1
2m∥x −x∗∥2 ≤f(x) −f(x∗) ≤1
2M∥x −x∗∥2
(2.2.21)
and
∥∇f(x)∥≥m∥x −x∗∥.
(2.2.22)

2.2. CONVERGENCE THEORY FOR EXACT LINE SEARCH
81
Proof.
From Lemma 2.2.6 we have
f(x) −f(x∗)
=
∇f(x∗)T (x −x∗) +
 1
0
(1 −t)(x −x∗)T ∇2f(tx + (1 −t)x∗)(x −x∗)dt
=
 1
0
(1 −t)(x −x∗)T ∇2f(tx + (1 −t)x∗)(x −x∗)dt.
(2.2.23)
Note that (2.2.20) and the integral mean-value theorem give
m∥x −x∗∥2
 1
0
(1 −t)dt
≤
 1
0
(1 −t)(x −x∗)T ∇2f(tx + (1 −t)x∗)(x −x∗)dt
≤
M∥x −x∗∥2
 1
0
(1 −t)dt.
(2.2.24)
Then combining (2.2.23) and (2.2.24) yields (2.2.21).
Also, using Taylor expansion gives
∇f(x) = ∇f(x) −∇f(x∗) =
 1
0
∇2f(tx + (1 −t)x∗)(x −x∗)dt.
Then
∥∇f(x)∥∥x −x∗∥
≥
(x −x∗)T ∇f(x)
=
 1
0
(x −x∗)T ∇2f(tx + (1 −t)x∗)(x −x∗)dt
≥
m∥x −x∗∥2
which proves (2.2.22).
2
Now we are in the position to give the theorem about convergence rate
which shows that the local convergence rate of Algorithm 2.2.1 with exact
line search is at least linear.
Theorem 2.2.8 Let the sequence {xk} generated by Algorithm 2.2.1 con-
verge to the minimizer x∗of f(x). Let f(x) be twice continuously diﬀeren-
tiable in a neighborhood of x∗. If there exist ϵ > 0 and M > m > 0 such that
when ∥x −x∗∥< ϵ,
m∥y∥2 ≤yT ∇2f(x)y ≤M∥y∥2, ∀y ∈Rn
(2.2.25)
holds, then the sequence {xk}, at least, converges linearly to x∗.

82
CHAPTER 2. LINE SEARCH
Proof.
Let limk→∞xk = x∗. We may assume that ∥xk −x∗∥≤ϵ for k
suﬃciently large. Since ∥xk+1 −x∗∥< ϵ, there exists δ > 0 such that
∥xk + (αk + δ)dk −x∗∥= ∥xk+1 −x∗+ δdk∥< ϵ.
(2.2.26)
Note that φ(α) = f(xk +αdk), φ′(α) = ∇f(xk +αdk)T dk, φ′(0) = ∇f(xk)T dk
and |φ′(0)| ≤∥∇f(xk)∥∥dk∥. We have φ′(0) < 0,
ρ∥∇f(xk)∥∥dk∥≤−φ′(0) ≤∥∇f(xk)∥∥dk∥, for some ρ ∈(0, 1)
(2.2.27)
and
φ′′(α) = dT
k ∇2f(xk + αdk)dk ≤M∥dk∥2.
Then, by Lemma 2.2.5, we know that the minimizer αk of φ(α) on [0, αk + δ]
satisﬁes
αk ≥˜αk = −φ′(0)
M∥dk∥2 ≥ρ∥∇f(xk)∥
M∥dk∥
∆= ¯αk.
(2.2.28)
Set ¯xk = xk + ¯αkdk. Obviously, it follows from (2.2.26) that ∥¯xk −x∗∥< ϵ.
Therefore,
f(xk + αkdk) −f(xk)
≤
f(xk + ¯αkdk) −f(xk)
=
¯αk∇f(xk)T dk + ¯α2
k
 1
0
(1 −t)dT
k ∇2f(xk + t¯αkdk)dkdt (from Lemma 2.2.6)
≤
¯αk(−ρ)∥∇f(xk)∥∥dk∥+ 1
2M ¯α2
k∥dk∥2 (from (2.2.25) and (2.2.27))
≤
−ρ2
2M ∥∇f(xk)∥2 (from (2.2.28))
≤
−ρ2
2M m2∥xk −x∗∥2 (from (2.2.22))
≤
−
 ρm
M
!2
[f(xk) −f(x∗)] (from (2.2.21)).
The above inequalities give
f(xk+1) −f(x∗)
=
[f(xk+1) −f(xk)] + [f(xk) −f(x∗)]
≤

1 −
 ρm
M
!2
[f(xk) −f(x∗)].
(2.2.29)

2.2. CONVERGENCE THEORY FOR EXACT LINE SEARCH
83
Set
θ =

1 −
 ρm
M
!2 1
2
.
(2.2.30)
Obviously θ ∈(0, 1). Therefore (2.2.29) can be written as
f(xk) −f(x∗)
≤
θ2[f(xk−1) −f(x∗)]
≤
· · ·
≤
θ2k[f(x0) −f(x∗)].
(2.2.31)
Furthermore, by (2.2.21), we have
∥xk −x∗∥2
≤
2
m[f(xk) −f(x∗)]
≤
2
mθ2[f(xk−1) −f(x∗)]
≤
2
mθ2 M
2 ∥xk−1 −x∗∥2
which implies that
∥xk −x∗∥≤
"
M
m θ∥xk−1 −x∗∥
(2.2.32)
and that the sequence {xk}, at least, converges linearly to x∗.
2
In the end of this section, we give a theorem which describes a descent
bound of the function value after each exact line search.
Theorem 2.2.9 Let αk be an exact step size. Assume that f(x) satisﬁes
(x −z)T [∇f(x) −∇f(z)] ≥η∥x −z∥2.
(2.2.33)
Then
f(xk) −f(xk + αkdk) ≥1
2η∥αkdk∥2.
(2.2.34)
Proof.
Since αk is an exact step size, then
∇f(xk + αkdk)T dk = 0.
(2.2.35)

84
CHAPTER 2. LINE SEARCH
Therefore, it follows from the mean-value theorem, (2.2.35) and (2.2.33) that
f(xk) −f(xk + αkdk)
=
 αk
0
−dT
k ∇f(xk + tdk)dt
=
 αk
0
dT
k [∇f(xk + αkdk) −∇f(xk + tdk)]dt
≥
 αk
0
η(αk −t)dt∥dk∥2
=
1
2η∥αkdk∥2.
(2.2.36)
This completes the proof.
2
2.3
The Golden Section Method and the Fibonacci
Method
The golden section method and the Fibonacci method are section methods.
Their basic idea for minimizing a unimodal function over [a, b] is iteratively
reducing the interval of uncertainty by comparing the function values of the
observations. When the length of the interval of uncertainty is reduced to
some desired degree, the points on the interval can be regarded as approxi-
mations of the minimizer. Such a class of methods only needs to evaluate the
functions and has wide applications, especially it is suitable to nonsmooth
problems and those problems with complicated derivative expressions.
2.3.1
The Golden Section Method
Let
φ(α) = f(x + αd)
be a unimodal function on the interval [a, b]. At the iteration k of the golden
section method, let the interval of uncertainty be [ak, bk]. Take two observa-
tions λk, µk ∈[ak, bk] and λk < µk. Evaluate φ(λk) and φ(µk). By Theorem
2.1.5, we have
Case 1 if φ(λk) ≤φ(µk), then set ak+1 = ak, bk+1 = µk;
Case 2 if φ(λk) > φ(µk), then set ak+1 = λk, bk+1 = bk.
How to choose the observations λk and µk? We require that λk and µk
satisfy the following conditions:

2.3. SECTION METHODS
85
1. The distances from λk and µk to the end points of the interval [ak, bk]
are equivalent, that is,
bk −λk = µk −ak.
(2.3.1)
2. The reduction rate of the intervals of uncertainty for each iteration is
the same, that is
bk+1 −ak+1 = τ(bk −ak), τ ∈(0, 1).
(2.3.2)
3. Only one extra observation is needed for each new iteration.
Now we consider Case 1. Substituting the values of Case 1 into (2.3.2) and
combining (2.3.1) yield
µk −ak
=
τ(bk −ak),
bk −λk
=
µk −ak.
Arranging the above equations gives
λk
=
ak + (1 −τ)(bk −ak),
(2.3.3)
µk
=
ak + τ(bk −ak).
(2.3.4)
Note that, in this case, the new interval is [ak+1, bk+1] = [ak, µk]. For fur-
ther reducing the interval of uncertainty, the observations λk+1 and µk+1 are
selected. By (2.3.4),
µk+1
=
ak+1 + τ(bk+1 −ak+1)
=
ak + τ(µk −ak)
=
ak + τ(ak + τ(bk −ak) −ak)
=
ak + τ 2(bk −ak).
(2.3.5)
If we set
τ 2 = 1 −τ,
(2.3.6)
then
µk+1 = ak + (1 −τ)(bk −ak) = λk.
(2.3.7)
It means that the new observation µk+1 does not need to compute, because
µk+1 coincides with λk.

86
CHAPTER 2. LINE SEARCH
Similarly, if we consider Case 2, the new observation λk+1 coincides with
µk. Therefore, for each new iteration, only one extra observation is needed,
which is just required by the third condition.
What is the reduction rate of the interval of uncertainty for each iteration?
By solving the equation (2.3.6), we immediately obtain
τ = −1 ±
√
5
2
.
Since τ > 0, then take
τ = bk+1 −ak+1
bk −ak
=
√
5 −1
2
≈0.618.
(2.3.8)
Then the formula (2.3.3)–(2.3.4) can be written as
λk
=
ak + 0.382(bk −ak),
(2.3.9)
µk
=
ak + 0.618(bk −ak).
(2.3.10)
Therefore, the golden section method is also called the 0.618 method.
Obviously, comparing with the Fibonacci method below, the golden sec-
tion method is more simple in performance and we need not know the number
of observations in advance.
Since, for each iteration, the reduction rate of the interval of uncertainty
is τ = 0.618, then if the initial interval is [a1, b1], the length of the interval
after n-th iteration is τ n−1(b1 −a1). Therefore the convergence rate of the
golden section method is linear.
Algorithm 2.3.1 (The Golden Section Method)
Step 1. Initial step. Determine the initial interval [a1, b1] and give
the precision δ > 0. Compute initial observations λ1 and
µ1:
λ1
=
a1 + 0.382(b1 −a1),
µ1
=
a1 + 0.618(b1 −a1),
evaluate φ(λ1) and φ(µ1), set k = 1.
Step 2. Compare the function values. If φ(λk) > φ(µk), go to Step
3; if φ(λk) ≤φ(µk), go to Step 4.

2.3. SECTION METHODS
87
Step 3. (Case 2) If bk −λk ≤δ, stop and output µk; otherwise, set
ak+1 := λk, bk+1 := bk, λk+1 := µk,
φ(λk+1) := φ(µk), µk+1 := ak+1 + 0.618(bk+1 −ak+1).
Evaluate φ(µk+1) and go to Step5.
Step 4. (Case 1) If µk −ak ≤δ, stop and output λk; otherwise set
ak+1 := ak, bk+1 := µk, µk+1 := λk,
φ(µk+1) := φ(λk), λk+1 := ak+1 + 0.382(bk+1 −ak+1).
Evaluate φ(λk+1) and go to Step 5.
Step 5. k := k + 1, go to Step 2.
2
2.3.2
The Fibonacci Method
Another section method which is similar to the golden section method is the
Fibonacci method.
Their main diﬀerence is in that the reduction rate of
the interval of uncertainty for the Fibonacci method does not use the golden
section number τ ≈0.618, but uses the Fibonacci number. Therefore the
reduction of the interval of uncertainty varies from one iteration to another.
The Fibonacci sequence {Fk} is deﬁned as follows:
F0 = F1 = 1,
(2.3.11)
Fk+1 = Fk + Fk−1, k = 1, 2, · · · .
(2.3.12)
If we use Fn−k/Fn−k+1 instead of τ in (2.3.3)–(2.3.4), we immediately obtain
the formula
λk
=
ak +
 
1 −
Fn−k
Fn−k+1
!
(bk −ak)
(2.3.13)
=
ak + Fn−k−1
Fn−k+1
(bk −ak), k = 1, · · · , n −1,
µk
=
ak +
Fn−k
Fn−k+1
(bk −ak), k = 1, · · · , n −1,
(2.3.14)
which is called the Fibonacci formula.

88
CHAPTER 2. LINE SEARCH
As stated in the last section, in Case 1, if φ(λk) ≤φ(µk), the new interval
of uncertainty is [ak+1, bk+1] = [ak, µk]. So, by using (2.3.14), we get
bk+1 −ak+1 =
Fn−k
Fn−k+1
(bk −ak)
(2.3.15)
which gives a reduction in each iteration. This equality is also true for Case
2.
Assume that we ask for the length of the ﬁnal interval no more than δ,
i.e.,
bn −an ≤δ.
Since
bn −an
=
F1
F2
(bn−1 −an−1)
=
F1
F2
F2
F3
· · · Fn−1
Fn
(b1 −a1)
=
1
Fn
(b1 −a1),
(2.3.16)
then
Fn ≥b1 −a1
δ
.
(2.3.17)
Therefore, given initial interval [a1, b1] and the upper bound δ of the length
of the ﬁnal interval, we can ﬁnd the Fibonacci number Fn and further n from
(2.3.17).
Our search proceeds until the n-th observation.
The procedure
of the Fibonacci method is similar to Algorithm 2.3.1. We leave it as an
exercise.
Letting Fk = rk and substituting in (2.3.11)-(2.3.12), we get
r2 −r −1 = 0.
(2.3.18)
Solving (2.3.18) gives
r1 = 1 +
√
5
2
, r2 = 1 −
√
5
2
.
(2.3.19)
Then, the general solution of the diﬀerence equation Fk+1 = Fk + Fk−1 is
Fk = Ark
1 + Brk
2.
(2.3.20)

2.4. INTERPOLATION METHOD
89
Using the initial condition F0 = F1 = 1, we get
A = r1
√
5, B = −r2
√
5.
Substituting in (2.3.20) gives
Fk =
1
√
5
⎧
⎨
⎩

1 +
√
5
2
k+1
−

1 −
√
5
2
k+1⎫
⎬
⎭.
(2.3.21)
Hence
lim
k→∞
Fk−1
Fk
=
√
5 −1
2
= τ.
(2.3.22)
This shows that, when k →∞, the Fibonacci method and the golden section
method have the same reduction rate of the interval of uncertainty. There-
fore the Fibonacci method converges with convergence ratio τ. It is worth
mentioning that the Fibonacci method is the optimal sectioning method for
one-dimensional optimization and it requires the smallest observations for a
given ﬁnal length δ, and that the golden section method is approximately
optimal. However, since the procedure of the golden section method is very
simple, it is more popular.
2.4
Interpolation Method
Interpolation Methods are the other approach of line search. This class of
methods approximates φ(α) = f(x + αd) by ﬁtting a quadratic or cubic
polynomial in α to known data, and choosing a new α-value which mini-
mizes the polynomial. Then we reduce the bracketing interval by comparing
the new α-value and the known points. In general, when the function has
good analytical properties, for example, it is easy to get the derivatives, the
interpolation methods are superior to the golden section method and the
Fibonacci method discussed in the last subsection.
2.4.1
Quadratic Interpolation Methods
1. Quadratic Interpolation Method with Two Points (I).
Given two points α1, α2, and their function values φ(α1) and φ(α2), and the
derivative φ′(α1) (or φ′(α2)). Construct the quadratic interpolation function

90
CHAPTER 2. LINE SEARCH
q(α) = aα2 + bα + c with the interpolation conditions:
q(α1)
=
aα2
1 + bα1 + c = φ(α1),
q(α2)
=
aα2
2 + bα2 + c = φ(α2),
(2.4.1)
q′(α1)
=
2aα1 + b = φ′(α1).
Write φ1 = φ(α1), φ2 = φ(α2), φ′
1 = φ′(α1), and φ′
2 = φ′(α2). Solving (2.4.1)
gives
a
=
φ1 −φ2 −φ′
1(α1 −α2)
−(α1 −α2)2
,
b
=
φ′
1 + 2φ1 −φ2 −φ′
1(α1 −α2)
(α1 −α2)2
α1.
Hence
¯α
=
−b
2a
=
α1 + 1
2
φ′
1(α1 −α2)2
α1 −α2 −φ′
1(α1 −α2)
=
α1 −1
2
(α1 −α2)φ′
1
φ′
1 −φ1−φ2
α1−α2
.
(2.4.2)
Then we get the following iteration formula:
αk+1 = αk −1
2
(αk −αk−1)φ′
k
φ′
k −φk−φk−1
αk−αk−1
.
(2.4.3)
where φk = φ(αk), φk−1 = φ(αk−1), and φ′
k = φ′(αk).
After ﬁnding the new αk+1, we compare αk+1 with αk and αk−1, and
reduce the bracketing interval. The procedure will continue until the length
of the interval is less than a prescribed tolerance.
2. Quadratic Interpolation Method with Two Points (II).
Given two points α1, α2, and one function value φ(α1) (or φ(α2) ), and two
derivative values φ′(α1) and φ′(α2). Construct the quadratic interpolation
function with the following conditions:
q(α1)
=
aα2
1 + bα1 + c = φ(α1),
q′(α1)
=
2aα1 + b = φ′(α1),
(2.4.4)
q′(α2)
=
2aα2 + b = φ′(α2).

2.4. INTERPOLATION METHOD
91
Precisely, with the same discussion as above we obtain
¯α = −b
2a = α1 −α1 −α2
φ′
1 −φ′
2
φ′
1.
(2.4.5)
Therefore the iteration scheme is
αk+1 = αk −αk −αk−1
φ′
k −φ′
k−1
φ′
k
(2.4.6)
which is also called the secant formula. The formula (2.4.5) can also be got
by setting L(α) = 0 where L(α) is the Lagrange interpolation formula
L(α) = (α −α1)φ′
2 −(α −α2)φ′
1
α2 −α1
(2.4.7)
which interpolates the derivative values of φ′(α) at two points α1 and α2.
In the following discussion, for convenience, we call the quadratic inter-
polating method (I) the quadratic interpolation formula, and the quadratic
interpolation method (II) the secant formula. Next, we turn to the conver-
gence of the quadratic interpolating method with two points.
Theorem 2.4.1 Let φ : R →R be three times continuously diﬀerentiable.
Let α∗be such that φ′(α∗) = 0 and φ′′(α∗) ̸= 0. Then the sequence {αk}
generated from (2.4.6) converges to α∗with the order 1+
√
5
2
≈1.618 of con-
vergence rate.
Proof.
By the representation of residual term of the Lagrange interpola-
tion formula
R2(α) = φ′(α) −L(α) = 1
2φ′′′(ξ)(α −αk)(α −αk−1), ξ ∈(α, αk−1, αk).
(2.4.8)
Setting α = αk+1 and noting that L(αk+1) = 0, we have
φ′(αk+1) = 1
2φ′′′(ξ)(αk+1 −αk)(αk+1 −αk−1), ξ ∈(αk−1, αk, αk+1), (2.4.9)
Substituting (2.4.6) into (2.4.9) yields
φ′(αk+1) = 1
2φ′′′(ξ)φ′
kφ′
k−1
(αk −αk−1)2
(φ′
k −φ′
k−1)2 , ξ ∈(αk−1, αk, αk+1).
(2.4.10)

92
CHAPTER 2. LINE SEARCH
We know from the mean-value theorem of diﬀerentiation that
φ′
k −φ′
k−1
αk −αk−1
= φ′′(ξ0), ξ0 ∈(αk−1, αk),
(2.4.11)
φ′
i = φ′
i −φ′(α∗) = (αi −α∗)φ′′(ξi),
(2.4.12)
where ξi ∈(αi, α∗), i = k −1, k, k + 1. Therefore it follows from (2.4.10)-
(2.4.12) that
αk+1 −α∗= 1
2
φ′′′(ξ)φ′′(ξk)φ′′(ξk−1)
φ′′(ξk+1)[φ′′(ξ0)]2
(αk −α∗)(αk−1 −α∗).
(2.4.13)
Let ei = |αi −α∗|, (i = k −1, k, k + 1). In the intervals considered, let
0 < m2 ≤|φ′′′(α)| ≤M2, 0 < m1 ≤|φ′′(α)| ≤M1,
K1 = m2m2
1/(2M3
1 ), K = M2M2
1 /(2m3
1).
Then
K1|αk −α∗||αk−1 −α∗| ≤|αk+1 −α∗| ≤K|αk −α∗||αk−1 −α∗|.
(2.4.14)
Noting that φ′′ and φ′′′ are continuous at α∗, we get
αk+1 −α∗
(αk −α∗)(αk−1 −α∗) →1
2
φ′′′(α∗)
φ′′(α∗) .
(2.4.15)
Therefore
ek+1 =

φ′′′(η1)
2φ′′(η2)
 ekek−1
∆= Mekek−1,
(2.4.16)
where η1 ∈(αk−1, αk, α∗), η2 ∈(αk−1, αk), M = |φ′′′(η1)/2φ′′(η2)|. The above
relations indicate that there exists δ > 0 such that, when the initial points
α0, α1 ∈(α∗−δ, α∗+ δ) and α0 ̸= α1, the sequence {αk} →α∗.
Next, we consider the convergence rate.
Set ϵi = Mei, yi = ln ϵi, i =
k −1, k, k + 1, then
ϵk+1 = ϵkϵk−1,
(2.4.17)
yk+1 = yk + yk−1.
(2.4.18)
Obviously, (2.4.18) is the equation that the Fibonacci sequence satisﬁes, and
its characteristic equation is
t2 −t −1 = 0
(2.4.19)

2.4. INTERPOLATION METHOD
93
whose solutions are
t1 = 1 +
√
5
2
, t2 = 1 −
√
5
2
.
(2.4.20)
Therefore the Fibonacci sequence {yk} can be written as
yk = Atk
1 + Btk
2, k = 0, 1, · · · ,
(2.4.21)
where A and B are coeﬃcients to be determined. Obviously, when k →∞,
ln ϵk = yk ≈Atk
1.
(2.4.22)
Since
ϵk+1
ϵt1
k
≈exp(Atk+1
1
)
[exp(Atk
1)]t1 = 1,
then
ek+1
et1
k
≈Mt1−1
(2.4.23)
which implies that the convergence rate is t1 = 1+
√
5
2
≈1.618.
2
This theorem tells us that the secant method has superlinear convergence.
3. Quadratic Interpolation Method with Three Points.
Given three distinct points α1, α2 and α3, and their function values. The
required interpolation conditions are
q(αi) = aα2
i + bαi + c = φ(αi), i = 1, 2, 3.
(2.4.24)
By solving the above equations, we obtain
a = −(α2 −α3)φ1 + (α3 −α1)φ2 + (α1 −α2)φ3
(α1 −α2)(α2 −α3)(α3 −α1)
,
b = (α2
2 −α2
3)φ1 + (α2
3 −α2
1)φ2 + (α2
1 −α2
2)φ3
(α1 −α2)(α2 −α3)(α3 −α1)
.
Then
¯α = −b
2a
=
1
2
(α2
2 −α2
3)φ1 + (α2
3 −α2
1)φ2 + (α2
1 −α2
2)φ3
(α2 −α3)φ1 + (α3 −α1)φ2 + (α1 −α2)φ3
(2.4.25)
=
1
2(α1 + α2) + 1
2
(φ1 −φ2)(α2 −α3)(α3 −α1)
(α2 −α3)φ1 + (α3 −α1)φ2 + (α1 −α2)φ3
.(2.4.26)

94
CHAPTER 2. LINE SEARCH
Equations (2.4.25) and (2.4.26) are called the quadratic interpolation formula
with three points. The above formula can also be obtained from considering
the Lagrange interpolation formula
L(α) =
(α −α2)(α −α3)
(α1 −α2)(α1 −α3)φ1+ (α −α1)(α −α3)
(α2 −α1)(α2 −α3)φ2+ (α −α1)(α −α2)
(α3 −α1)(α3 −α2)φ3,
(2.4.27)
and setting L′(α) = 0.
Algorithm 2.4.2 (Line Search Employing Quadratic Interpolation with Three
Points)
Step 0. Given tolerance ϵ. Find an initial bracket {α1, α2, α3} con-
taining α∗; Compute φ(αi), i = 1, 2, 3.
Step 1. Use the formula (2.4.25) to produce ¯α;
Step 2. If (¯α −α1)(¯α −α3) ≥0 go to Step 3; otherwise go to Step 4;
Step 3. Construct new bracket {α1, α2, α3} from α1, α2, α3 and ¯α.
Go to Step 1.
Step 4. If |¯α −α2| < ε, stop; otherwise go to Step 3.
2
Figure 2.4.1 is a diagram for the quadratic interpolation line search with
three points.
The following theorem shows that the above algorithm has convergence
rate with order 1.32.
Theorem 2.4.3 Let φ(α) have continuous fourth-order derivatives. Let α∗
satisfy φ′(α∗) = 0 and φ′′(α∗) ̸= 0. Then the sequence {αk} generated from
the formula (2.4.25) has convergence rate with order 1.32.
Proof.
By Lagrange interpolation formula (2.4.27), we have
φ(α) = L(α) + R3(α),
(2.4.28)
where
R3(α) = 1
6φ′′′(ξ(α))(α −α1)(α −α2)(α −α3).
(2.4.29)

2.4. INTERPOLATION METHOD
95
Since 0 = φ′(α∗) = L′(α∗) + R′
3(α∗), we get
φ1
2α∗−(α2 + α3)
(α1 −α2)(α1 −α3) + φ2
2α∗−(α3 + α1)
(α2 −α3)(α2 −α1)
+φ3
2α∗−(α1 + α2)
(α3 −α1)(α3 −α2) + R′
3(α∗) = 0.
(2.4.30)
Noting that (2.4.25) can be rewritten as
α4 = 1
2
φ1(α2+α3)
(α1−α2)(α1−α3) +
φ2(α3+α1)
(α2−α3)(α2−α1) +
φ3(α1+α2)
(α3−α1)(α3−α2)
φ1
(α1−α2)(α1−α3) +
φ2
(α2−α3)(α2−α1) +
φ3
(α3−α1)(α3−α2)
,
(2.4.31)
it follows from (2.4.30) and (2.4.31) that
α∗−α4 = 1
2
R′
3(α∗)
φ1
(α1−α2)(α1−α3) +
φ2
(α2−α3)(α2−α1) +
φ3
(α3−α1)(α3−α2)
.
(2.4.32)
Let ei = α∗−αi, i = 1, 2, 3, 4. It follows from (2.4.32) that
e4[−φ1(e2 −e3) −φ2(e3 −e1) −φ3(e1 −e2)]
=
−1
2R′
3(α∗)(e1 −e2)(e2 −e3)(e3 −e1).
(2.4.33)
Noting that φ′(α∗) = 0, it follows from Taylor expansion that
φi = φ(α∗) + 1
2e2
i φ′′(α∗) + O(e3
i ).
(2.4.34)
Neglecting the third-order term and substituting (2.4.34) into (2.4.33) give
e4 =
1
φ′′(α∗)R′
3(α∗).
(2.4.35)
Also, by the Lagrange interpolation formula, we have
R′
3(α) =
1
6φ′′′(ξ(α))[(α −α2)(α −α3) + (α −α1)(α −α3)
+(α −α1)(α −α2)] + 1
24φ(4)(η)(α −α1)(α −α2)(α −α3),
which implies
R′
3(α∗) = 1
6φ′′′(ξ(α∗))(e1e2 + e2e3 + e3e1) + 1
24φ(4)(η)e1e2e3.
(2.4.36)

96
CHAPTER 2. LINE SEARCH
Neglecting the fourth-order derivative term, it follows from (2.4.35) and
(2.4.36) that
e4 = φ′′′(ξ(α∗))
6φ′′(α∗) (e1e2 + e2e3 + e3e1) = M(e1e2 + s2e3 + e3e1),
where M is some constant. In general, we have
ek+2 = M(ek−1ek + ekek+1 + ek+1ek−1).
(2.4.37)
Since ek+1 = O(ek) = O(ek−1) when ek →0, there exists ¯
M > 0 such that
|ek+2| ≤¯
M|ek−1||ek|,
i.e.,
¯
M|ek+2| ≤¯
M|ek−1| ¯
M|ek|.
When |ei|, (i = 1, 2, 3) are suﬃciently small such that
δ = max{ ¯
M|e1|, ¯
M|e2|, ¯
M|e3|} < 1,
one has
¯
M|e4| ≤¯
M|e1| ¯
M|e2| ≤δ2.
Set
¯
M|ek| ≤δqk,
(2.4.38)
then
¯
M|ek+2| ≤¯
M|ek| ¯
M|ek−1| ≤δqkδqk−1 ∆= δqk+2,
hence
qk+2 = qk + qk−1, (k ≥2)
(2.4.39)
where q1 = q2 = q3 = 1. Obviously, the characteristic equation of (2.4.39) is
t3 −t −1 = 0
(2.4.40)
with one root t1 ≈1.32 and other two conjugate complex roots, |t2| = |t3| < 1.
The general solution of (2.4.39) has form
qk = Atk
1 + Btk
2 + Ctk
3,
(2.4.41)
where A, B and C are coeﬃcients to be determined. Clearly, when k →∞,
qk+1 −t1qk = Btk
2(t2 −t1) + Ctk
3(t3 −t1) →0.

2.4. INTERPOLATION METHOD
97
Figure 2.4.1 Flow chart for quadratic interpolation method
with three points

98
CHAPTER 2. LINE SEARCH
So, when k is suﬃciently large, we have
qk+1 −t1qk ≥−0.1.
(2.4.42)
Note from (2.4.38) that |ek| ≤(1/ ¯
M)δqk ∆= Bk, (k ≥1). Then, by (2.4.42),
when k is suﬃciently large,
Bk+1
Bk
=
δqk+1/ ¯
M
δt1qk/( ¯
M)t1 = ¯
Mt1−1δqk+1−t1qk ≤δ−0.1 ¯
Mt1−1,
which indicates that the convergence order t1 ≈1.32.
2
2.4.2
Cubic Interpolation Method
The cubic interpolation method approximates the objective function φ(α) by
a cubic polynomial. To construct the cubic polynomial p(α), four interpo-
lation conditions are required. For example, we may use function values at
four points, or function values at three points and a derivative value at one
point, or function values and derivative values at two points. Note that, in
general, the cubic interpolation has better convergence than the quadratic
interpolation, but that it needs computing of derivatives and more expensive
computation. Hence it is often used for smooth functions. In the following,
we discuss the cubic interpolation method with two points.
We are given two points a and b, the function values φ(a) and φ(b), and
the derivative values φ′(a) and φ′(b) to construct a cubic polynomial of the
form
p(α) = c1(α −a)3 + c2(α −a)2 + c3(α −a) + c4
(2.4.43)
where ci are the coeﬃcients of the polynomial which are chosen such that
p(a) = c4 = φ(a),
p′(a) = c3 = φ′(a),
p(b) = c1(b −a)3 + c2(b −a)2 + c3(b −a) + c4 = φ(b),
p′(b) = 3c1(b −a)2 + 2c2(b −a) + c3 = φ′(b).
(2.4.44)
From the suﬃcient condition of the minimizer, we have
p′(α) = 3c1(α −a)2 + 2c2(α −a) + c3 = 0
(2.4.45)
and
p′′(α) = 6c1(α −a) + 2c2 > 0.
(2.4.46)

2.4. INTERPOLATION METHOD
99
Solving (2.4.45) yields
α
=
a +
−c2 ±

c2
2 −3c1c3
3c1
, if c1 ̸= 0,
(2.4.47)
α
=
a −c3
2c2
, if c1 = 0.
(2.4.48)
In order to guarantee the condition (2.4.46) holding, we only take the
positive in (2.4.47). So we combine (2.4.47) with (2.4.48), and get
α −a =
−c2 +

c2
2 −3c1c3
3c1
=
−c3
c2 +

c2
2 −3c1c3
.
(2.4.49)
When c1 = 0, (2.4.49) is just (2.4.48). Then the minimizer of p(α) is
¯α = a −
c3
c2 +

c2
2 −3c1c3
.
(2.4.50)
The minimizer in (2.4.50) is represented by c1, c2 and c3. We hope to
represent ¯α by φ(a), φ(b), φ′(a) and φ′(b) directly.
Let
s = 3φ(b) −φ(a)
b −a
, z = s −φ′(a) −φ′(b),
w2 = z2 −φ′(a)φ′(b).
(2.4.51)
By use of (2.4.44), we have
s
=
3φ(b) −φ(a)
b −a
= 3[c1(b −a)2 + c2(b −a) + c3],
z
=
s −φ′(a) −φ′(b) = c2(b −a) + c3,
w2
=
z2 −φ′(a)φ′(b) = (b −a)2(c2
2 −3c1c3).
Then
(b −a)c2 = z −c3,

c2
2 −3c1c3 =
w
b −a,
and so
c2 +

c2
2 −3c1c3 = z + w −c3
b −a
.
(2.4.52)

100
CHAPTER 2. LINE SEARCH
Using c3 = φ′(a) and substituting (2.4.52) into (2.4.50), we get
¯α −a = −(b −a)φ′(a)
z + w −φ′(a),
(2.4.53)
which is
¯α −a
=
−(b −a)φ′(a)φ′(b)
(z + w −φ′(a))φ′(b) =
−(b −a)(z2 −w2)
φ′(b)(z + w) −(z2 −w2)
=
(b −a)(w −z)
φ′(b) −z + w .
(2.4.54)
Unfortunately, the formula (2.4.54) is not adequate for calculating ¯α, because
its denominator is possibly zero or merely very small. Fortunately, it can be
overcome by use of (2.4.53) and (2.4.54), and we have
¯α −a
=
−(b −a)φ′(a)
z + w −φ′(a) = (b −a)(w −z)
φ′(b) −z + w
=
(b −a)(−φ′(a) + w −z)
φ′(b) −φ′(a) + 2w
=
(b −a)
 
1 −
φ′(b) + z + w
φ′(b) −φ′(a) + 2w
!
,
(2.4.55)
or
¯α = a + (b −a)
w −φ′(a) −z
φ′(b) −φ′(a) + 2w.
(2.4.56)
In (2.4.55) and (2.4.56), the denominator φ′(b) −φ′(a) + 2w ̸= 0. In fact,
since φ′(a) < 0 and φ′(b) > 0, then w2 = z2 −φ′(a)φ′(b) > 0. Taking w > 0,
it follows that the denominator φ′(b) −φ′(a) + 2w > 0.
In the same way as we did in the last subsection, we can discuss the
convergence rate of the cubic interpolation method. Similar to (2.4.16), we
can obtain
ek+1 = M(eke2
k−1 + e2
kek−1),
where M is some constant. We can show that the characteristic equation is
t2 −t −2 = 0,
which solution is t = 2. Therefore the cubic interpolation method with two
points has convergence rate with order 2.
Finally, we give a ﬂow diagram of the method in Figure 2.4.2.

2.4. INTERPOLATION METHOD
101

102
CHAPTER 2. LINE SEARCH
Figure 2.4.2 Flow chart for cubic interpolation method with two points
2.5
Inexact Line Search Techniques
Line search is a basic part of optimization methods. In the last sections we
have discussed some exact line search techniques which ﬁnd αk such that
f(xk + αkdk) = min
α≥0 f(xk + αdk),
or
αk = min{α| ∇f(xk + αdk)T dk = 0, α ≥0}.
However, commonly, the exact line search is expensive. Especially, when an
iterate is far from the solution of the problem, it is not eﬀective to solve
exactly a one-dimension subproblem. Also, in practice, for many optimiza-
tion methods, for example, Newton method and quasi-Newton method, their
convergence rate does not depend on the exact line search. Therefore, as
long as there is an acceptable steplength rule which ensures that the objec-
tive function has suﬃcient descent, the exact line search can be avoided and
the computing eﬀorts will be decreased greatly. In the following, we deﬁne
gk = ∇f(xk) without special indication.

2.5. INEXACT LINE SEARCH TECHNIQUES
103
2.5.1
Armijo and Goldstein Rule
Armijo rule [4] is as follows: Given β ∈(0, 1), ρ ∈(0, 1
2), τ > 0, there exists
the least nonnegative integer mk such that
f(xk) −f(xk + βmτdk) ≥−ρβmτgT
k dk.
(2.5.1)
Goldstein (1965) [157] presented the following rule. Let
J = {α > 0 | f(xk + αdk) < f(xk)}
(2.5.2)
be an interval. In Figure 2.5.1 J = (0, a). In order to guarantee the objective
function decreases suﬃciently, we want to choose α such that it is away from
the two end points of the interval J. The two reasonable conditions are
f(xk + αdk) ≤f(xk) + ραgT
k dk
(2.5.3)
and
f(xk + αdk) ≥f(xk) + (1 −ρ)αgT
k dk,
(2.5.4)
which exclude those points near the right end-point and the left end-point,
where 0 < ρ <
1
2, All α satisfying (2.5.3)-(2.5.4) constitute the interval
J2 = [b, c]. We call (2.5.3)-(2.5.4) Goldstein inexact line search rule, in brief,
Goldstein rule.
When a step-length factor α satisﬁes (2.5.3)-(2.5.4), it is
called an acceptable step-length factor, and the obtained interval J2 = [b, c]
is called an acceptable interval.
Figure 2.5.1 Inexact line search
As before, let φ(α) = f(xk + αdk).
Then (2.5.3) and (2.5.4) can be
rewritten respectively
φ(αk)
≤
φ(0) + ραkφ′(0),
(2.5.5)
φ(αk)
≥
φ(0) + (1 −ρ)αkφ′(0).
(2.5.6)

104
CHAPTER 2. LINE SEARCH
Note that the restriction ρ < 1
2 is necessary. In fact, if φ(α) is a quadratic
function satisfying φ′(0) < 0 and φ′′(0) > 0, then the global minimizer α∗of
φ satisﬁes
φ(α∗) = φ(0) + 1
2α∗φ′(0).
Hence α∗satisﬁes (2.5.5) if and only if ρ < 1
2. The restriction ρ < 1
2 will
also ﬁnally permit α = 1 for Newton method and quasi-Newton method.
Therefore, without the restriction ρ < 1
2, the superlinear convergence of the
methods will not be guaranteed.
2.5.2
Wolfe-Powell Rule
As shown in Figure 2.5.1, it is possible that the rule (2.5.4) excludes the
minimizing value of α outside the acceptable interval. Instead, the Wolfe-
Powell rule gives another rule to replace (2.5.4):
gT
k+1dk ≥σgT
k dk, σ ∈(ρ, 1),
(2.5.7)
which implies that
φ′(αk)
=
[∇f(xk + αkdk)]T dk ≥σ∇f(xk)T dk
=
σφ′(0) > φ′(0).
(2.5.8)
It shows that the geometric interpretation of (2.5.7) is that the slope φ′(αk)
at the acceptable point must be greater than or equal to some multiple σ ∈
(0, 1) of the initial slope. The rule (2.5.3) and (2.5.7) is called the Wolfe-
Powell inexact line search rule, in brief, the Wolfe-Powell rule, which gives
the acceptable interval J3 = [e, c] that includes the minimizing values of α.
In fact, the rule (2.5.7) can be obtained from the mean-value theorem
and (2.5.4). Let αk satisfy (2.5.4). Then
αk[∇f(xk + θkαkdk)]T dk
=
f(xk + αkdk) −f(xk)
≥
(1 −ρ)αk∇f(xk)T dk
which shows (2.5.7). Now we show the existence of αk satisfying (2.5.3) and
(2.5.7). Let ˆαk satisfy the equality in (2.5.3). By the mean-value theorem
and (2.5.3), we have
ˆαk[∇f(xk + θk ˆαkdk)]T dk
=
f(xk + ˆαkdk) −f(xk)
=
ρˆαk∇f(xk)T dk,

2.5. INEXACT LINE SEARCH TECHNIQUES
105
where θk ∈(0, 1). Let ρ < σ < 1, and note that ∇f(xk)T dk < 0, we have
[∇f(xk + θk ˆαkdk)]T dk = ρ∇f(xk)T dk > σ∇f(xk)T dk
which is just (2.5.7) if we set αk = θk ˆαk. The discussion above also shows
that the requirement ρ < σ < 1 is necessary, such that there exists steplength
factor αk satisfying the Wolfe-Powell rule.
It should point out that the inequality requirement (2.5.7) is an approxi-
mation of the orthogonal condition
gT
k+1dk = 0
which is satisﬁed by exact line search. However, unfortunately, one possible
disadvantage of (2.5.7) is that it does not reduce to an exact line search in
the limit σ →0. In addition, a steplength may satisfy the Wolfe-Powell rule
(2.5.3) and (2.5.7) without being close to a minimizer of φ. Luckily, if we
replace (2.5.7) by using the rule
|gT
k+1dk| ≤−σgT
k dk,
(2.5.9)
the exact line search is obtained in the limit σ →0, and the points that are
far from a stationary point of φ will be excluded. Therefore the rule (2.5.3)
and (2.5.9) is also a successful pair of inexact line search rules which is called
the strong Wolfe-Powell rule. Furthermore, we often employ the following
form of the strong Wolfe-Powell rule:
|gT
k+1dk| ≤σ|gT
k dk|
(2.5.10)
or
|φ′(αk)| ≤σ|φ′(0)|.
(2.5.11)
In general, the smaller the value σ, the more exact the line search. Nor-
mally, taking σ = 0.1 gives a fairly accurate line search, whereas the value
σ = 0.9 gives a weak line search. However, taking too small σ may be unwise,
because the smaller the value σ, the more expensive the computing eﬀort.
Usually, ρ = 0.1 and σ = 0.4 are suitable, and it depends on the speciﬁc
problem.

106
CHAPTER 2. LINE SEARCH
2.5.3
Goldstein Algorithm and Wolfe-Powell Algorithm
Although it is possible that the minimizing value of α may be excluded by the
rule (2.5.4), it seldom occurs in practice. Therefore, Goldstein rule (2.5.3)-
(2.5.4) is a frequently used rule in practice. The overall structure is illustrated
in Figure 2.5.2 and the details of the algorithm are described in Algorithm
2.5.1.
Figure 2.5.2 Flow chart for Goldstein inexact line search
Algorithm 2.5.1 (Inexact Line Search with Goldstein Rule)
Step 1. Choose initial data.
Take initial point α0 in [0, +∞) (or
[0, αmax]). Compute φ(0), φ′(0). Given ρ ∈(0, 1
2), t > 1. Set
a0 := 0, b0 := +∞(or αmax), k := 0.
Step 2. Check the rule (2.5.3). Compute φ(αk). If
φ(αk) ≤φ(0) + ραkφ′(0),

2.5. INEXACT LINE SEARCH TECHNIQUES
107
go to Step 3; otherwise, set ak+1 := ak, bk+1 := αk, go to
Step 4.
Step 3. Check the rule (2.5.4). If
φ(αk) ≥φ(0) + (1 −ρ)αkφ′(0),
stop, and output αk; otherwise, set ak+1 := αk, bk+1 := bk.
If bk+1 < +∞, go to Step 4; otherwise set αk+1 := tαk, k :=
k + 1, go to Step 2.
Step 4. Choose a new point. Set
αk+1 := ak+1 + bk+1
2
,
and k := k + 1, go to Step 2.
2
Similarly, we give in Figure 2.5.3 the diagram of the Wolfe-Powell algo-
rithm.

108
CHAPTER 2. LINE SEARCH
Figure 2.5.3 Flow chart for Wolfe-Powell inexact line search
2.5.4
Backtracking Line Search
In practice, frequently, we also use only the condition (2.5.3) if we choose
an appropriate α which is not too small. This method is called backtracking
line search. The idea of backtracking is, at the beginning, to set α = 1. If
xk + αdk is not acceptable, we reduce α until xk + αdk satisﬁes (2.5.3).
Algorithm 2.5.2
Step 1. Given ρ ∈(0, 1
2), 0 < l < u < 1, set α = 1.
Step 2. Test
f(xk + αdk) ≤f(xk) + ραgT
k dk;
Step 3. If (2.5.3) is not satisﬁed, set α := ωα, ω ∈[l, u], and go to
Step 2; otherwise, set αk := α and xk+1 := xk + αkdk.
2
In Step 3 of the above algorithm, the quadratic interpolation can be used
to reduce α. Let
φ(α) = f(xk + αdk).
(2.5.12)
At the beginning, we have
φ(0) = f(xk), φ′(0) = ∇f(xk)T dk.
(2.5.13)
After computing f(xk + dk), we have
φ(1) = f(xk + dk).
(2.5.14)
If f(xk + dk) does not satisfy (2.5.3), the following quadratic model can be
used to approximate φ(α):
m(α) = [φ(1) −φ(0) −φ′(0)]α2 + φ′(0)α + φ(0),
(2.5.15)
which obeys the three conditions in (2.5.13)-(2.5.14). Setting m′(α) = 0 gives
ˆα = −
φ′(0)
2[φ(1) −φ(0) −φ′(0)],
(2.5.16)

2.5. INEXACT LINE SEARCH TECHNIQUES
109
which can be taken as the next value of α.
In order to prevent α from being too small and not terminating, some
safeguards are needed. For example, given the least step minstep, if (2.5.3)
is not satisﬁed but ∥αdk∥< minstep, the line search stops.
In summary, in this section we introduced three kind of inexact line search
rules:
1. Goldstein rule: (2.5.3)-(2.5.4).
2. Wolfe-Powell rule: (2.5.3) and (2.5.7); Strong Wolfe-Powell rule: (2.5.3)
and (2.5.9).
3. Backtracking rule (also called Armijo rule): (2.5.3) or (2.5.1).
The above three inexact line search rules are frequently used in optimization
methods below.
2.5.5
Convergence Theorems of Inexact Line Search
In the ﬁnal subsection we establish convergence theorems of inexact line
search methods. To prove the descent property of the methods, we try to
avoid the case in which the search directions sk = αkdk are nearly orthogonal
to the negative gradient −gk, that is, the angle θk between sk and −gk is
uniformly bounded away from 90o,
θk ≤π
2 −µ, ∀k
(2.5.17)
where µ > 0, θk ∈[0, π
2 ] is deﬁned by
cos θk = −gT
k sk/(∥gk∥∥sk∥),
(2.5.18)
because, otherwise, gT
k sk will approach zero and so sk is almost not a descent
direction.
A general descent algorithm with inexact line search is as follows:
Algorithm 2.5.3
Step 1. Given x0 ∈Rn, 0 ≤ε < 1, k := 0.
Step 2 If ∥∇f(xk)∥≤ε, stop; otherwise, ﬁnd a descent direction
dk such that dT
k ∇f(xk) < 0.

110
CHAPTER 2. LINE SEARCH
Step 3 Find the steplength factor αk by use of Goldstein rule (2.5.3)-
(2.5.4) or Wolfe-Powell rule (2.5.3) and (2.5.7).
Step 4 Set xk+1 = xk + αkdk; k := k + 1, go to Step 2.
2
In Algorithm 2.5.3, dk is a general descent direction provided it satisﬁes
dT
k ∇f(xk) < 0, and αk is a general inexact line-search factor provided some
inexact line search rule is satisﬁed.
So, this algorithm is a very general
algorithm, that is, it contains a great class of methods.
Now, we establish the global convergence of the general descent algorithm
with inexact line search.
Theorem 2.5.4 Let αk in Algorithm 2.5.3 be deﬁned by Goldstein rule (2.5.3)-
(2.5.4) or Wolfe-Powell rule (2.5.3) and (2.5.7). Let also sk satisfy (2.5.17).
If ∇f exists and is uniformly continuous on the level set {x| f(x) ≤f(x0)},
then either ∇f(xk) = 0 for some k, or f(xk) →−∞,or ∇f(xk) →0.
Proof.
Let αk be deﬁned by (2.5.3)-(2.5.4). Assume that, for all k, gk =
∇f(xk) ̸= 0 (whence sk = αkdk ̸= 0) and f(xk) is bounded below, it follows
that f(xk) −f(xk+1) →0, hence −gT
k sk →0 from (2.5.3).
Now assume that gk →0 does not hold. Then there exist ε > 0 and a
subsequence such that ∥gk∥≥ε and ∥sk∥→0. Since θk ≤π
2 −µ, we get
cos θk ≥cos(π
2 −µ) = sin µ,
hence
−gT
k sk ≥sin µ∥gk∥∥sk∥≥ε sin µ∥sk∥.
But the Taylor series gives
f(xk+1) = f(xk) + g(ξk)T sk,
where ξk is on the line segment (xk, xk+1). By uniform continuity, we have
g(ξk) →gk when sk →0. So
f(xk+1) = f(xk) + gT
k sk + o(∥sk∥).
Therefore we obtain
f(xk) −f(xk+1)
−gT
k sk
→1,

2.5. INEXACT LINE SEARCH TECHNIQUES
111
which contradicts (2.5.4). Hence, gk →0, and the proof is complete.
Similarly, instead of (2.5.4), if we use (2.5.7), we can get global conver-
gence of the Wolfe-Powell algorithm. The proof is essentially the same as
above. We need only to note that, by uniform continuity of g(x), it follows
that
gT
k+1sk = gT
k sk + o(∥sk∥),
such that
gT
k+1sk
gT
k sk
→1.
This contradicts gT
k+1sk/gT
k sk ≤σ < 1 given by (2.5.7).
Hence gk →0.
Therefore, the global convergence theorem also holds when αk is deﬁned by
Wolfe-Powell rule (2.5.3) and (2.5.7).
2
Next, we give the convergence theorems with the Wolfe-Powell rule.
Theorem 2.5.5 Let f : Rn →R be continuously diﬀerentiable and bounded
below, and let ∇f be uniformly continuous on the level set Ω= {x | f(x) ≤
f(x0)}. Assume that αk is deﬁned by Wolfe-Powell rule (2.5.3) and (2.5.7).
Then the sequence generated by Algorithm 2.5.3 satisﬁes
lim
k→+∞
∇f(xk)T sk
∥sk∥
= 0,
(2.5.19)
which means
∥∇f(xk)∥cos θk →0.
(2.5.20)
Proof.
Since ∇f(xk)T sk < 0 and f is bounded below, then the sequence
{xk} is well-deﬁned and {xk} ⊂Ω. Also, since {f(xk)} is a descent sequence,
hence it is convergent.
We now prove (2.5.19) by contradiction. Assume that (2.5.19) does not
hold. Then there exist ε > 0 and a subsequence with index set K, such that
−∇f(xk)T sk
∥sk∥
≥ε, k ∈K.
From (2.5.3), one has
f(xk) −f(xk+1) ≥ρ∥sk∥

−∇f(xk)T sk
∥sk∥

≥ρ∥sk∥ε.

112
CHAPTER 2. LINE SEARCH
Since also {f(xk)} is a convergent sequence, then {sk : k ∈K} converges to
zero. Also by (2.5.7), we have
(1 −σ)(−∇f(xk)T sk) ≤(∇f(xk + sk) −∇f(xk))T sk, k ≥0.
Therefore
ε ≤−∇f(xk)T sk
∥sk∥
≤
1
1 −σ∥∇f(xk + sk) −∇f(xk)∥, k ∈K.
(2.5.21)
However, since we have proved {sk | k ∈K} →0, then the right-hand side
of (2.5.21) goes to zero by the uniform continuity of ∇f on the level set Ω.
Hence there is a contradiction which completes the proof.
2
Note that (2.5.19) implies
∥∇f(xk)∥cos θk →0,
which is called the Zoutendijk condition, where θk is the angle between
−∇f(xk) and sk. If cos θk ≥δ > 0, we have limk→∞∥∇f(xk)∥= 0. Also,
if the assumption of uniform continuity is replaced by Lipschitz continuity,
the theorem is also true. In the theorem below, we prove this case. We ﬁrst
prove a lemma which gives a bound of descent for a single step.
Lemma 2.5.6 Let f : D ⊂Rn →R be continuously diﬀerentiable, also let
∇f(x) satisfy Lipschitz condition
∥∇f(y) −∇f(z)∥≤M∥y −z∥,
where M > 0 is a constant. If f(xk + αdk) is bounded below and α > 0, then
for all αk > 0 satisfying (2.5.3) and (2.5.7), we have
f(xk) −f(xk + αkdk) ≥β∥∇f(xk)∥2 cos2⟨dk, −∇f(xk)⟩,
(2.5.22)
where β > 0 is a constant.
Proof.
From Lipschitz condition of ∇f and (2.5.7) we have
αkM∥dk∥2 ≥dT
k [∇f(xk + αkdk) −∇f(xk)] ≥−(1 −σ)dT
k ∇f(xk),
that is
αk∥dk∥
≥
1 −σ
M∥dk∥∥dk∥∥∇f(xk)∥cos⟨dk, −∇f(xk)⟩
=
1 −σ
M
∥∇f(xk)∥cos⟨dk, −∇f(xk)⟩.

2.5. INEXACT LINE SEARCH TECHNIQUES
113
Using (2.5.3) yields
f(xk) −f(xk + αkdk) ≥−αkρdT
k ∇f(xk)
=
αkρ∥dk∥∥∇f(xk)∥cos⟨dk, −∇f(xk)⟩
≥
ρ∥∇f(xk)∥cos⟨dk, −∇f(xk)⟩1 −σ
M
∥∇f(xk)∥cos⟨dk, −∇f(xk)⟩
=
ρ(1 −σ)
M
∥∇f(xk)∥2 cos2⟨dk, −∇f(xk)⟩,
which is (2.5.22) in which β = ρ(1 −σ)/M.
2
Theorem 2.5.7 Let f(x) be continuously diﬀerentiable on Rn, and let ∇f(x)
satisfy Lipschitz condition
∥∇f(x) −∇f(y)∥≤M∥x −y∥.
(2.5.23)
Also let αk in Algorithm 2.5.3 be deﬁned by Wolfe-Powell rule (2.5.3) and
(2.5.7).
If the condition (2.5.17) is satisﬁed, then, for the sequence {xk}
generated by Algorithm 2.5.3, either ∇f(xk) = 0 for some k, or f(xk) →
−∞, or ∇f(xk) →0.
Proof.
Assume that ∇f(xk) ̸= 0, ∀k. By Lemma 2.5.6, we have
f(xk) −f(xk+1) ≥β cos2 θk∥∇f(xk)∥2,
(2.5.24)
where β = ρ(1 −σ)/M is a positive constant being independent of k. Then,
for all k > 0, we have
f(x0) −f(xk)
=
k−1

i=0
[f(xi) −f(xi+1)]
≥
β min
0≤i≤k ∥∇f(xi)∥2
k−1

i=0
cos2 θi.
(2.5.25)
Since θk satisﬁes (2.5.17), this means that
∞

k=0
cos2 θk = +∞.
(2.5.26)
Then it follows from (2.5.25) that either ∇f(xk) →0 or f(xk) →−∞. This
completes the proof.
2

114
CHAPTER 2. LINE SEARCH
In fact, Theorem 2.5.7 is a direct result coming from (2.5.20) and the
angle condition (2.5.17).
Finally, we derive an estimate of descent amount of f(x) under inexact
line search.
Theorem 2.5.8 Let αk satisfy (2.5.3). If f(x) is a uniformly convex func-
tion, i.e., there exists a constant η > 0 such that
(y −z)T [∇f(y) −∇f(z)] ≥η∥y −z∥2,
(2.5.27)
or there exist positive constants m and M (m < M), such that
m∥y∥2 ≤yT ∇2f(x)y ≤M∥y∥2.
(2.5.28)
Then
f(xk) −f(xk + αkdk) ≥
ρη
1 +
)
M/m∥αkdk∥2,
(2.5.29)
where ρ is deﬁned in (2.5.3).
Proof.
We divide into two cases.
First, assume that dT
k ∇f(xk + αkdk) ≤0. In this case we have
f(xk) −f(xk + αkdk)
=
 αk
0
−dT
k ∇f(xk + tdk)dt
=
 αk
0
dT
k [∇f(xk + αkdk) −∇f(xk + tdk)]dt
≥
 αk
0
η(αk −t)dt∥dk∥2
=
1
2η∥αkdk∥2
≥
ρη
1 +
)
M/m∥αkdk∥2.
(2.5.30)
Second, assume that dT
k ∇f(xk + αkdk) > 0. Then there exists 0 < α∗<
αk, such that dT
k ∇f(xk + α∗dk) = 0. So, it follows from (2.5.28) that
f(xk) −f(xk + α∗dk) ≤1
2M∥α∗dk∥2,
(2.5.31)
and
f(xk + αkdk) −f(xk + α∗dk) ≥1
2m∥(αk −α∗)dk∥2.
(2.5.32)

2.5. INEXACT LINE SEARCH TECHNIQUES
115
Since f(xk + αkdk) < f(xk), it follows from (2.5.31) and (2.5.32) that
αk ≤
⎛
⎝1 +
"
M
m
⎞
⎠α∗.
(2.5.33)
Hence
f(xk) −f(xk + αkdk)
≥
−αkρdT
k ∇f(xk)
≥
αkρdT
k [∇f(xk + α∗dk) −∇f(xk)]
≥
ηραkα∗∥dk∥2
≥
ρη
1 +
)
M/m∥αkdk∥2.
(2.5.34)
Hence (2.5.29) holds in both cases. This completes the proof.
2
In this chapter we have discussed exact and inexact line search techniques
which guarantee monotonic decrease of the objective function. On the other
hand it is found that enforcing monotonicity of the function values may con-
siderably slow the rate of convergence, especially in the presence of narrow
curved valleys. Therefore, it is reasonable to present a nonmonotonic line
search technique for optimization which allows an increase in function value
at each step, while retaining global convergence. Grippo etc. [164] general-
ized the Armijo rule to the nonmonotone case and relaxed the condition of
monotonic decrease. Several papers also deal with these techniques. Here we
only state the basic result of nonmonotonic line search as follows.
Theorem 2.5.9 Let {xk} be a sequence deﬁned by
xk+1 = xk + αkdk, dk ̸= 0.
Let τ > 0, σ ∈(0, 1), γ ∈(0, 1) and let M be a nonnegative integer. Assume
that
(i) the level set Ω= {x | f(x) ≤f(x0)} is compact;
(ii) there exist positive numbers c1, c2 such that
∇f(xk)T dk ≤−c1∥∇f(xk)∥2,
(2.5.35)
∥dk∥≤c2∥∇f(xk)∥;
(2.5.36)
(iii) αk = σhkτ, where hk is the ﬁrst nonnegative integer h, such that
f(xk + σhτdk) ≤
max
0≤j≤m(k)[f(xk−j)] + γσhτ∇f(xk)T dk,
(2.5.37)

116
CHAPTER 2. LINE SEARCH
where m(0) = 0 and 0 ≤m(k) ≤min[m(k −1) + 1, M], k ≥1.
Then the sequence {xk} remains in Ωand every accumulation point ¯x
satisﬁes ∇f(¯x) = 0.
Proof.
See Grippo etc. [164].
2
Exercises
1. Let f(x) = (sin x)6 tan(1 −x)e30x. Find the maximum of f(x) in [0, 1]
by use of the 0.618 method, quadratic interpolation method, and Goldstein
line search, respectively.
2. Write the Fibonacci algorithm and its program in MATLAB (or FOR-
TRAN, C).
3. Let φ(t) = e−t + et. Let the initial interval be [−1, 1].
(1) Minimize φ(t) by 0.618 method.
(2) Minimize φ(t) by Fibonacci method.
(3) Minimize φ(t) by Armijo line search.
4. Let φ(t) = 1 −te−t2. Let the initial interval be [0, 1]. Try to minimize
φ(t) by quadratic interpolation method.
5. Let φ(t) = −2t3 + 21t2 −60t + 50.
(1) Minimize φ(t) by Armijo rule if t0 = 0.5 and ρ = 0.1.
(2) Minimize φ(t) by Goldstein rule if t0 = 0.5 and ρ = 0.1.
(3) Minimize φ(t) by Wolfe rule if t0 = 0.5, ρ = 0.1, and σ = 0.8.
6.
Let f(x) = x4
1 + x2
1 + x2
2.
Given current point xk = (1, 1)T and
dk = (−3, −1)T . Let ρ = 0.1, σ = 0.5.
(1) Try using the Wolfe rule to ﬁnd a new point xk+1.
(2) Set α = 1, α = 0.5, α = 0.1 respectively, describe that for which α
satisﬁes the Wolfe rule and for which α does not satisfy the Wolfe rule.
7. Show that if 0 < σ < ρ < 1, then there may be no steplengths that
satisfy the Wolfe rule.
8. Describe the outline of Theorem 2.5.4.

2.5. INEXACT LINE SEARCH TECHNIQUES
117
9. Prove the other form of Theorem 2.5.5: Let f : Rn →R be continu-
ously diﬀerentiable and bounded below, and let ∇f be Lipschitz continuous
on the level set Ω= {x | f(x) ≤f(x0)}. Assume that αk is deﬁned by Wolfe-
Powell rule (2.5.3) and (2.5.7). Then the sequence generated by Algorithm
2.5.3 satisﬁes
lim
k→+∞
∇f(xk)T sk
∥sk∥
= 0,
which means
∥∇f(xk)∥cos θk →0.

Chapter 3
Newton’s Methods
3.1
The Steepest Descent Method
3.1.1
The Steepest Descent Method
The steepest descent method is one of the simplest and the most fundamental
minimization methods for unconstrained optimization. Since it uses the neg-
ative gradient as its descent direction, it is also called the gradient method.
Suppose that f(x) is continuously diﬀerentiable near xk, and the gradient
gk
def
= ∇f(xk) ̸= 0. From the Taylor expansion
f(x) = f(xk) + (x −xk)T gk + o(∥x −xk∥),
(3.1.1)
we know that, if we write x −xk = αdk, then the direction dk satisfying
dT
k gk < 0 is called a descent direction that is such that f(x) < f(xk). Fixing
α, it follows that the smaller the value dT
k gk (i.e., the larger the value |dT
k gk|)
is, the faster the function value decreases. By the Cauchy-Schwartz inequality
|dT
k gk| ≤∥dk∥∥gk∥,
(3.1.2)
we have that the value dT
k gk is the smallest if and only if dk = −gk. Therefore
−gk is the steepest descent direction.
The iterative scheme of the steepest descent method is
xk+1 = xk −αkgk.
(3.1.3)
In the following we give the algorithm.

120
CHAPTER 3. NEWTON’S METHODS
Algorithm 3.1.1 (The Steepest Descent Method)
Step 0. Let 0 < ε ≪1 be the termination tolerance. Given an initial
point x0 ∈Rn. Set k = 0.
Step 1. If ∥gk∥≤ε, stop ; otherwise let dk = −gk.
Step 2. Find the steplength factor αk, such that
f(xk + αkdk) = min
α≥0 f(xk + αdk);
Step 3. Compute xk+1 = xk + αkdk.
Step 4. k := k + 1, return to Step 1.
2
3.1.2
Convergence of the Steepest Descent Method
The steepest descent method is of importance in the area of optimization
from the theoretical point of view. The importance of its convergence theory
is not only in itself but also in other optimization methods. In the following,
we discuss the global convergence and local convergence rate of the steepest
descent method.
Theorem 3.1.2 (Global convergence theorem of the steepest descent method)
Let f ∈C1. Then each accumulation point of the iterative sequence {xk}
generated by the steepest descent Algorithm 3.1.1 with exact line search is a
stationary point.
Proof.
Let ¯x be any accumulation point of {xk} and K an inﬁnite index
set such that limk∈K xk = ¯x. Set dk = −∇f(xk). Since f ∈C1, the sequence
{dk | k ∈K} is uniformly bounded and ∥dk∥= ∥∇f(xk)∥. Since the as-
sumptions of Theorem 2.2.3 are satisﬁed, it follows that ∥∇f(¯x)∥2 = 0, i.e.,
∇f(¯x) = 0.
2
Theorem 3.1.3 (Global convergence theorem of the steepest descent method)
Let f(x) be twice continuously diﬀerentiable in Rn and ∥∇2f(x)∥≤M for
a positive constant M. Given any initial x0 and ε > 0. Then the sequence
generated from Algorithm 3.1.1 terminates in ﬁnitely many iterations, or
limk→∞f(xk) = −∞, or limk→∞∇f(xk) = 0.

3.1. THE STEEPEST DESCENT METHOD
121
Proof.
Consider the inﬁnite case. From Algorithm 3.1.1 and Theorem
2.2.2, we have
f(xk) −f(xk+1) ≥
1
2M ∥∇f(xk)∥2.
Then
f(x0) −f(xk) =
k−1

i=0
[f(xi) −f(xi+1)] ≥
1
2M
k−1

i=0
∥∇f(xi)∥2.
Taking limits yields either limk→∞f(xk) = −∞or limk→∞∥∇f(xk)∥= 0.
The result then follows.
2
Instead of the exact line search in Step 2 of Algorithm 3.1.1, the steepest
descent method can also use inexact line search technique. For this case, the
global convergence is given below.
Theorem 3.1.4 (Convergence theorem of the steepest descent method with
inexact line search)
Let f ∈C1. Consider the steepest descent method with inexact line search.
Then each accumulation point of the sequence {xk} is a stationary point.
Proof.
It follows directly from Theorem 2.5.4.
2
Unfortunately, the global convergence does not guarantee that the steep-
est descent method is an eﬀective method. We can clearly ﬁnd this problem
from the following analysis and the local convergence rate theorem.
In fact, the steepest descent direction is only a local property of the
algorithm. For many problems, the steepest descent method is not the actual
“steepest”, but is very slow. Although the method usually works well in the
early steps, as a stationary point is approached, it descends very slowly with
zigzagging phenomena. This zigzagging phenomena is illustrated in Figure
3.1.1 for the problem
min(x1 −2)4 + (x1 −2x2)2,
in which zigzagging occurs along the valley shown by the dotted lines.

122
CHAPTER 3. NEWTON’S METHODS
Figure 3.1.1 Zigzagging in the steepest descent method
In fact, the zigzagging of the steepest descent method can be explained
by the following facts. Since, from exact line search, one has
gT
k+1dk = 0,
then
gT
k+1gk = dT
k+1dk = 0.
(3.1.4)
This shows that two gradients are orthogonal to each other on the successive
iterates, and thus two successive directions are also orthogonal, which leads
to the zigzagging. When the stationary point is approached, ∥gk∥will be
very small. By means of the expression
f(xk + αd) = f(xk) + αgT
k d + o(∥αd∥),
(3.1.5)
it is easy to see that the ﬁrst order term αgT
k d = −α∥gk∥2 is of a very small
order of magnitude. Hence the descent of f is very small.
Next, we discuss the convergence rate of the steepest descent method,
ﬁrst for the case of a quadratic function and then for the case of a general
function.
When the objective function is quadratic, the convergence rate of the
steepest descent method depends on the ratio of the longest axis and the

3.1. THE STEEPEST DESCENT METHOD
123
shortest axis of the ellipsoid which corresponds to the contour of the objec-
tive function. The bigger the ratio is, the slower the descent is. The following
theorem indicates this fact and says that the steepest descent method con-
verges linearly.
Theorem 3.1.5 (The convergence rate theorem of the steepest descent method
for the case of a quadratic function)
Consider the unconstrained minimization problem
min
x∈Rn f(x) = 1
2xT Gx,
(3.1.6)
where G is an n × n symmetric and positive deﬁnite matrix. Let λ1 and
λn be the largest and the smallest eigenvalues of G respectively. Let x∗be
the solution of the problem (3.1.6). Then the sequence {xk} generated by
the steepest descent method converges to x∗, the convergence rate is at least
linear, and the following bounds hold:
f(xk+1) −f(x∗)
f(xk) −f(x∗)
≤
(κ −1)2
(κ + 1)2 = (λ1 −λn)2
(λ1 + λn)2 ,
(3.1.7)
∥xk+1 −x∗∥G
∥xk −x∗∥G
≤
κ −1
κ + 1 =
 λ1 −λn
λ1 + λn
!
,
(3.1.8)
∥xk+1 −x∗∥
∥xk −x∗∥
≤
√κκ −1
κ + 1 =
"
λ1
λn
 λ1 −λn
λ1 + λn
!
,
(3.1.9)
where κ = λ1/λn.
Proof.
Consider the minimization of (3.1.6); we have
xk+1 = xk −αkgk,
(3.1.10)
with
αk = gT
k gk
gT
k Ggk
(3.1.11)
and gk = Gxk.
f(xk) −f(xk+1)
f(xk)
=
1
2xT
k Gxk −1
2(xk −αkgk)T G(xk −αkgk)
1
2xT
k Gxk
=
αkgT
k Gxk −1
2α2
kgT
k Ggk
1
2xT
k Gxk

124
CHAPTER 3. NEWTON’S METHODS
=
(gT
k gk)2
gT
k Ggk −1
2
(gT
k gk)2
gT
k Ggk
1
2gT
k G−1gk
=
(gT
k gk)2
(gT
k Ggk)(gT
k G−1gk).
(3.1.12)
By using Kantorovich inequality (3.1.33), we have immediately that
f(xk+1)
f(xk)
=

1 −
(gT
k gk)2
(gT
k Ggk)(gT
k G−1gk)

(3.1.13)
≤

1 −
4λ1λn
(λ1 + λn)2

=
 λ1 −λn
λ1 + λn
!2
,
(3.1.14)
which is just (3.1.7).
By using (3.1.13), it is not diﬃcult to get (3.1.8) and (3.1.9). In fact, let
ek = xk −x∗, ∀k ≥0. Noting that G is symmetric and positive deﬁnite, we
have
λneT
k ek ≤eT
k Gek ≤λ1eT
k ek.
(3.1.15)
Since x∗= 0, we have
∥xk −x∗∥2
G = eT
k Gek = xT
k Gxk = 2f(xk).
(3.1.16)
So, it follows from (3.1.15) that
λn∥xk −x∗∥2 ≤2f(xk) ≤λ1∥xk −x∗∥2, ∀k ≥0.
(3.1.17)
From (3.1.13), (3.1.16) and (3.1.17), we get
λn∥xk+1 −x∗∥2
λ1∥xk −x∗∥2
≤∥xk+1 −x∗∥2
G
∥xk −x∗∥2
G
≤
 λ1 −λn
λ1 + λn
!2
,
(3.1.18)
which gives (3.1.8) and (3.1.9).
2
If we consider, more generally, the objective function with the form
f(x) = 1
2xT Gx −bT x,
(3.1.19)
where G is an n×n symmetric positive deﬁnite matrix and b ∈Rn, the above
theorem is also true.
If the objective function is generalized to the non-quadratic case, we also
can get the linear convergence rate of the steepest descent method.

3.1. THE STEEPEST DESCENT METHOD
125
Theorem 3.1.6 Let f(x) satisfy the assumptions of Theorem 2.2.8. If the
sequence {xk} generated from the steepest descent method converges to x∗,
then the convergence rate is at least linear.
Proof.
It is a direct result from Theorem 2.2.8.
2
The above convergence rate theorem of the steepest descent method for
a general function can also be described as follows.
Theorem 3.1.7 Let f(x) be twice continuously diﬀerentiable near x∗with
∇f(x∗) = 0 and ∇2f(x∗) positive deﬁnite. Let the sequence {xk} generated
by the steepest descent method converge to x∗. Let
f(xk+1) −f(x∗)
f(xk) −f(x∗)
= βk.
(3.1.20)
Then βk < 1, ∀k and
lim sup
k→+∞
βk ≤M −m
M
< 1,
(3.1.21)
where M and m satisfy
0 < m ≤λn ≤λ1 ≤M,
(3.1.22)
and λn and λ1 are respectively the smallest and the largest eigenvalues of
∇2f(x).
Proof.
From Theorem 2.2.2, we have
[f(xk) −f(x∗)] −[f(xk+1) −f(x∗)]
=
f(xk) −f(xk+1)
≥
1
2M ∥∇f(xk)∥2,
(3.1.23)
which is, by the deﬁnition of βk, that
(1 −βk)[f(xk) −f(x∗)] ≥
1
2M ∥∇f(xk)∥2.
Hence, by the assumption of f, we get
βk ≤1 −
∥∇f(xk)∥2
2M[f(xk) −f(x∗)] < 1.
(3.1.24)

126
CHAPTER 3. NEWTON’S METHODS
Now suppose that (xk −x∗)/∥xk −x∗∥→¯d. It is obvious that
∥∇f(xk)∥2 = ∥xk −x∗∥2(∥∇2f(x∗) ¯d∥2 + o(1))
and
f(xk) −f(x∗) = 1
2∥xk −x∗∥2( ¯dT ∇2f(x∗) ¯d + o(1)).
Using the above equalities and (3.1.22) yields
lim
k→∞
∥∇f(xk)∥2
f(xk) −f(x∗) = 2∥∇2f(x∗) ¯d∥2
¯dT ∇2f(x∗) ¯d ≥2m.
(3.1.25)
Hence, it follows from (3.1.24) and (3.1.25) that
lim sup
k→∞
βk
≤
1 −lim inf
k→∞
∥∇f(xk)∥2
2M[f(xk) −f(x∗)]
≤
1 −m
M < 1.
We complete the proof.
2
3.1.3
Barzilai and Borwein Gradient Method
From the above discussions we know that the classical steepest descent method
performs poorly, converges linearly, and is badly aﬀected by ill-conditioning.
Barzilai and Borwein [8] presented a two-point step size gradient method,
which is called usually the Barzilai-Borwein (or BB) gradient method. In the
method, the step size is derived from a two-point approximation to the secant
equation underlying quasi-Newton methods (see Chapter 5).
Consider the gradient iteration form
xk+1 = xk −αkgk
(3.1.26)
which can be written as
xk+1 = xk −Dkgk,
(3.1.27)
where Dk = αkI. In order to make the matrix Dk have quasi-Newton prop-
erty, we compute αk such that
min
∥sk−1 −Dkyk−1∥.
(3.1.28)

3.1. THE STEEPEST DESCENT METHOD
127
This yields that
αk = sT
k−1yk−1
yT
k−1yk−1
,
(3.1.29)
where sk−1 = xk −xk−1, yk−1 = gk −gk−1.
By symmetry, we may minimize ∥D−1
k sk−1 −yk−1∥with respect to αk
and get
αk = sT
k−1sk−1
sT
k−1yk−1
.
(3.1.30)
The above description produces the following algorithm.
Algorithm 3.1.8 (The Barzilai-Borwein gradient method)
Step 0. Given x0 ∈Rn, 0 < ε ≪1. Set k = 0.
Step 1. If ∥gk∥≤ε, stop ; otherwise let dk = −gk.
Step 2. If k = 0, ﬁnd α0 by line search; otherwise compute αk by
(3.1.29) or (3.1.30).
Step 3. Set xk+1 = xk + αkdk.
Step 4. k := k + 1, return to Step 1.
2
It is easy to see that in this method no matrix computations and no line
searches (except k = 0) are required. The Barzilai-Borwein method is, in
fact, a gradient method, but requires less computational work, and greatly
speeds up the convergence of the gradient method. Barzilai and Borwein
[8] proved that the above algorithm is R-superlinearly convergent for the
quadratic case.
In the general non-quadratic case, a globalization strategy based on non-
monotone line search is suitable to Barzilai-Borwein gradient method. In
addition, in general non-quadratic case, αk computed by (3.1.29) or (3.1.30)
can be unacceptably large or small.
Therefore, we must assume that αk
satisﬁes the condition
0 < α(l) ≤αk ≤α(u),
for all k,
where α(l) and α(u) are previously determined numbers.

128
CHAPTER 3. NEWTON’S METHODS
If we employ the iteration
xk+1 = xk −1
αk
gk = xk −λkgk
(3.1.31)
with
αk = sT
k−1yk−1
sT
k−1sk−1
,
λk = 1
αk
,
(3.1.32)
note that sk = −1
αk gk = −λkgk, then we have
αk+1 = sT
k yk
sT
k sk
= −λkgT
k yk
λ2
kgT
k gk
= −gT
k yk
λkgT
k gk
.
Now we give the following Barzilai-Borwein gradient algorithm with non-
monotone globalization.
Algorithm 3.1.9 (The Barzilai-Borwein gradient algorithm with nonmono-
tone linesearch)
Step 0. Given x0 ∈Rn, 0 < ε ≪1, an integer M ≥0, ρ ∈(0, 1), δ >
0, 0 < σ1 < σ2 < 1, α(l), α(u). Set k = 0.
Step 1. If ∥gk∥≤ε, stop.
Step 2. If αk ≤α(l) or αk ≥α(u) then set αk = δ.
Step 3. Set λ = 1/αk.
Step 4. (nonmonotone line search) If
f(xk −λgk) ≤
max
0≤j≤min(k,M) f(xk−j) −ρλgT
k gk,
then set
λk = λ,
xk+1 = xk −λkgk,
and go to Step 6.
Step 5. Choose σ ∈[σ1, σ2], set λ = σλ, and go to Step 4.
Step 6. Set αk+1 = −(gT
k yk)/(λkgT
k gk), k := k + 1, return to Step 1.
2
Obviously, the above algorithm is globally convergent.

3.1. THE STEEPEST DESCENT METHOD
129
3.1.4
Appendix: Kantorovich Inequality
We conclude this section with a famous Kantorovich Inequality which is used
in the proof of Theorem 3.1.5.
Theorem 3.1.10 (Kantorovich Inequality) Let G be an n × n symmetric
positive deﬁnite matrix with eigenvalues λ1 ≥· · · ≥λn. Then, for any x ∈
Rn, the following inequality holds:
(xT x)2
(xT Gx)(xT G−1x) ≥
4λ1λn
(λ1 + λn)2 .
(3.1.33)
Proof.
Let the spectral decomposition of G be
G = UΛU.
Set x = Uy, then
(xT x)2
(xT Gx)(xT G−1x)
=
(yT y)2
(yT Λy)(yT Λ−1y)
=
(
n
i=1 y2
i )2
(
n
i=1 λiy2
i )(
n
i=1 y2
i /λi).
(3.1.34)
Let
ξi =
y2
i

n
i=1 y2
i
, φ(λ) = 1
λ,
(3.1.35)
then (3.1.34) becomes
(xT x)2
(xT Gx)(xT G−1x) =
1
(
n
i=1 λiξi)(
n
i=1 φ(λi)ξi).
(3.1.36)
Below we use the convexity of φ to estimate the lower bound of the right-
hand side of (3.1.36). Let
λ =
n

i=1
λiξi, λφ =
n

i=1
φ(λi)ξi.
(3.1.37)
Since ξi ≥0 (i = 1, · · · , n) and 
n
i=1 ξi = 1, we have λn ≤λ ≤λ1. Then each
λi can be represented as a convex combination of λ1 and λn:
λi = λ1 −λi
λ1 −λn
λn + λi −λn
λ1 −λn
λ1.

130
CHAPTER 3. NEWTON’S METHODS
From the convexity of φ, we have obviously
φ(λi) ≤λ1 −λi
λ1 −λn
φ(λn) + λi −λn
λ1 −λn
φ(λ1).
(3.1.38)
Then, it follows from (3.1.37), (3.1.38) and (3.1.35) that
λφ
≤
n

i=1
 λ1 −λi
λ1 −λn
φ(λn) + λi −λn
λ1 −λn
φ(λ1)

ξi
=
n

i=1
λ1 + λn −λi
λ1λn
ξi
=
λ1 + λn −λ
λ1λn
.
(3.1.39)
Therefore, by (3.1.36), (3.1.37) and (3.1.39) we obtain
(xT x)2
(xT Gx)(xT G−1x)
=
1
λλφ
≥
λ1λn
λ(λ1 + λn −λ)
≥
λ1λn
maxλ∈[λn,λ1] λ(λ1 + λn −λ) =
4λ1λn
(λ1 + λn)2 ,
which is our result.
2
3.2
Newton’s Method
The basic idea of Newton’s method for unconstrained optimization is to it-
eratively use the quadratic approximation q(k) to the objective function f at
the current iterate xk and to minimize the approximation q(k).
Let f : Rn →R be twice continuously diﬀerentiable, xk ∈Rn, and the
Hessian ∇2f(xk) positive deﬁnite. We model f at the current point xk by
the quadratic approximation q(k),
f(xk + s) ≈q(k)(s) = f(xk) + ∇f(xk)T s + 1
2sT ∇2f(xk)s,
(3.2.1)
where s = x −xk. Minimizing q(k)(s) yields
xk+1 = xk −[∇2f(xk)]−1∇f(xk)
(3.2.2)
which is Newton’s formula. Set
Gk = ∇2f(xk), gk = ∇f(xk).
(3.2.3)

3.2. NEWTON’S METHOD
131
Then we write (3.2.2) as
xk+1 = xk −G−1
k gk,
(3.2.4)
where sk = xk+1 −xk = −G−1
k gk is a Newton’s direction. Clearly, the New-
ton’s direction is a descent direction because it satisﬁes gT
k sk = −gT
k G−1
k gk <
0 if Gk is positive deﬁnite. Please note, in the remainder of this book, the
ﬁrst and the second derivatives of f will be denoted by
g(x) ∆= ∇f(x), G(x) ∆= ∇2f(x)
(3.2.5)
for convenience, if they exist.
The corresponding algorithm is stated as follows.
Algorithm 3.2.1 (Newton’s Method)
Step 1. Given x0 ∈Rn, ϵ > 0, k := 0;
Step 2. If ∥gk∥≤ϵ, stop;
Step 3. Solve Gks = −gk for sk;
Step 4. Set xk+1 = xk + sk;
Step 5. k := k + 1, go to Step 2.
2
Obviously, Newton’s method can be regarded as a steepest descent method
under the ellipsoid norm ∥· ∥Gk. In fact, for f(xk + s) ≈f(xk) + gT
k s, we
regard sk as the solution of the minimization problem
min
s∈Rn
gT
k s
∥s∥.
(3.2.6)
The solution of (3.2.6) depends on the norm. If we employ l2 norm, then we
get sk = −gk and the resultant method is the steepest descent method. If
we employ the ellipsoid norm ∥· ∥Gk, then we get sk = −G−1
k gk which is just
the Newton’s method. In fact, in this case, (3.2.6) is equivalent to
mins∈Rn
gT
k s
s.t.
∥s∥Gk ≤1.

132
CHAPTER 3. NEWTON’S METHODS
Note that, by (1.2.36), we have that
(gT
k s)2 ≤(gT
k G−1
k gk)(sT Gks)
and that gT
k s will be the smallest when s = −G−1
k gk. The above discussion
gives us a clear explanation.
For the positive deﬁnite quadratic function, Newton’s method can reach
the minimizer with one iteration. However, for a general non-quadratic func-
tion, it is not sure that Newton’s method can reach the minimizer with
ﬁnite iterations. Fortunately, since the objective function is approximate to
a quadratic function near the minimizer, then if the starting point is close
to the minimizer the Newton’s method will converge rapidly. The following
theorem shows the local convergence and the quadratic convergence rate of
Newton’s method.
Theorem 3.2.2 (Convergence Theorem of Newton’s Method) Let f ∈C2
and xk be close enough to the solution x∗of the minimization problem with
g(x∗) = 0. If the Hessian G(x∗) is positive deﬁnite and G(x) satisﬁes Lips-
chitz condition
|Gij(x) −Gij(y)| ≤β∥x −y∥, for some β, for all i, j
(3.2.7)
where Gij(x) is the (i, j)-element of G(x), then for all k, Newton’s iteration
(3.2.4) is well-deﬁned; the generated sequence {xk} converges to x∗with a
quadratic rate.
Proof.
Let hk = xk −x∗. From Taylor’s formula, it follows that
0 = g(x∗) = gk −Gkhk + O(∥hk∥2).
Since f ∈C2, xk is close enough to x∗, and G(x∗) is positive deﬁnite, it is rea-
sonable to assume that xk is in the neighborhood of x∗, Gk positive deﬁnite,
G−1
k
upper bounded. Hence the k-th Newton’s iteration exists. Multiplying
through by G−1
k
yields
0
=
G−1
k gk −hk + O(∥hk∥2)
=
−sk −hk + O(∥hk∥2)
=
−hk+1 + O(∥hk∥2).
By deﬁnition of O(·), there is a constant C such that
∥hk+1∥≤C∥hk∥2.
(3.2.8)

3.2. NEWTON’S METHOD
133
If xk ∈Ω= {x | ∥h∥≤γ/C, h = x −x∗, γ ∈(0, 1)}, then
∥hk+1∥≤γ∥hk∥≤γ2/C < γ/C.
(3.2.9)
Hence xk+1 ∈Ω. By induction on k, Newton’s iteration is well-deﬁned for all
k, and ∥hk∥→0 as k →∞. Therefore the iteration converges. Also, (3.2.8)
shows that the convergence rate of the iteration sequence is quadratic.
2
Note that Newton’s method is a local method. When the starting point
is far away from the solution, it is not sure that Gk is positive deﬁnite and
Newton’s direction dk is a descent direction. Hence the convergence is not
guaranteed. Since, as we know, the line search is a global strategy, we can
employ Newton’s method with line search to guarantee the global conver-
gence. However it should be noted that only when the step size sequence
{αk} converges to 1, Newton’s method is convergent with the quadratic rate.
Newton’s iteration with line search is as follows:
dk
=
−G−1
k gk,
(3.2.10)
xk+1
=
xk + αkdk,
(3.2.11)
where αk is a step size. The formula (3.2.10)–(3.2.11) corresponds to the
following algorithm.
Algorithm 3.2.3 (Newton’s Method with Line Search)
Step 1. Initial step: given x0 ∈Rn, ϵ > 0, set k := 0.
Step 2. Compute gk. If ∥gk∥≤ϵ, stop and output xk; otherwise go
to Step 3.
Step 3. Solve Gkd = −gk for dk.
Step 4. Line search step: ﬁnd αk such that
f(xk + αkdk) = min
α≥0 f(xk + αdk).
Step 5. Set xk+1 = xk + αkdk, k := k + 1, go to Step 2.
2
Next, we prove the above Algorithm 3.2.3 is globally convergent.

134
CHAPTER 3. NEWTON’S METHODS
Theorem 3.2.4 Let f : Rn →R be twice continuously diﬀerentiable on
open convex set D ⊂Rn. Assume that for any x0 ∈D there exists a constant
m > 0 such that f(x) satisﬁes
uT ∇2f(x)u ≥m∥u∥2, ∀u ∈Rn, x ∈L(x0),
(3.2.12)
where L(x0) = {x | f(x) ≤f(x0)} is the corresponding level set. Then the
sequence {xk} generated by Algorithm 3.2.3 satisﬁes
1. when {xk} is a ﬁnite sequence, gk = 0 for some k;
2. when {xk} is an inﬁnite sequence, {xk} converges to the unique mini-
mizer x∗of f.
Proof.
First, from (3.2.12), we know that f(x) is a strictly convex function
on Rn, and hence its stationary point is the unique global minimizer.
Also, from the assumption, it follows that the level set L(x0) is a bounded
closed convex set. Since {f(xk)} is monotonic descent, then {xk} ⊂L(x0)
and {xk} is bounded. Therefore there exists a limit point ¯x ∈L(x0) with
xk →¯x, and further f(xk) →f(¯x). Also since f ∈C2(D), by Theorem 2.2.4,
we have gk →g(¯x) = 0. Finally, note that the stationary point is unique,
then the whole sequence {xk} converges to ¯x which is the unique minimizer.
2
Similarly, if we employ inexact line search rule (2.5.3) and (2.5.7), it
follows from (2.5.22) that
f(xk) −f(xk + αkdk) ≥¯η∥gk∥2 cos2⟨dk, −gk⟩,
(3.2.13)
where ¯η is some constant independent of k. In this case the global convergence
still holds.
Theorem 3.2.5 Let f : Rn →R be twice continuously diﬀerentiable on an
open convex set D ⊂Rn. Assume that for any x0 ∈Rn, there exists m > 0
such that f(x) satisﬁes (3.2.12) on the level set L(x0). If the line search
employed satisﬁes (3.2.13), then the sequence {xk} generated from Newton’s
algorithm satisﬁes
lim
k→∞∥gk∥= 0,
(3.2.14)
and {xk} converges to the unique minimizer of f(x).

3.3. MODIFIED NEWTON’S METHOD
135
Proof.
Since f(x) satisﬁes (3.2.12), we see that f(x) is uniformly convex
on L(x0). Also, from (3.2.13), it follows that f(x) is strictly monotonically
descending and further that {xk} is bounded. Therefore there exists a con-
stant M > 0 such that
∥Gk|| ≤M ∀k.
(3.2.15)
From (3.2.10), (3.2.12) and (3.2.15), it follows that
cos⟨dk, −gk⟩
=
−dT
k gk
∥dk∥∥gk∥=
gT
k G−1
k gk
∥G−1
k gk∥∥gk∥
=
dT
k Gkdk
∥dk∥∥Gkdk∥≥m
M .
(3.2.16)
Hence, by (3.2.13) and (3.2.16), we have
∞>
∞

k=0
[f(xk) −f(xk+1)] ≥
∞

k=0
¯η m2
M2 ∥gk∥2,
(3.2.17)
which shows (3.2.14). Note that f(x) is uniformly convex, then f(x) has
only one stationary point, and (3.2.14) indicates that {xk} converges to the
unique minimizer x∗of f.
2
3.3
Modiﬁed Newton’s Method
The main diﬃculty faced by Newton’s method is that the Hessian Gk is not
positive deﬁnite.
In this case, it is not sure that the model function has
minimizers. When Gk is indeﬁnite, the model function will be unbounded.
To overcome these diﬃculties, there are several modiﬁed schemes.
Goldstein-Price Method
Goldstein and Price [159] presented a modiﬁed method: when Gk is not
positive deﬁnite, the steepest descent direction −gk is used. If we combine
this strategy with the angle rule
θ ≤π
2 −µ, for some µ > 0,
where θ is the angle between −gk and dk, we can determine the direction dk
as follows:
dk =

−G−1
k gk,
if cos θ ≥η,
−gk,
otherwise,
(3.3.1)

136
CHAPTER 3. NEWTON’S METHODS
where η > 0 is a given constant. Then the consultant direction dk satisﬁes
cos θ ≥η and the angle rule is satisﬁed, and thus the corresponding algorithm
is convergent.
Goldfeld et al. Method
Goldfeld et al. [156] presented another modiﬁed Newton’s method. Their
method does not substitute the steepest descent method for Newton’s method,
but makes the Newton’s direction −G−1
k gk turn to the steepest descent direc-
tion −gk. More precisely, when Gk is not positive deﬁnite, one changes the
model Hessian Gk to Gk + νkI, where νk > 0 such that Gk + νkI is positive
deﬁnite and well-conditioned. Ideally, νk is not much larger than the smallest
ν that makes Gk + νI positive deﬁnite and well-conditioned. The framework
of the algorithm is as follows.
Algorithm 3.3.1 (Modiﬁed Newton’s Method)
Initial step: Given an initial point x0 ∈Rn.
k-th step:
(1) Set ¯Gk = Gk + νkI, where
νk = 0, if Gk is positive deﬁnite;
νk > 0, otherwise.
(2) Solve ¯Gkd = −gk for dk.
(3) Set xk+1 = xk + dk.
2
In the above algorithm, the smallest possible νk is slightly larger than the
magnitude of the most negative eigenvalue of Gk. We suggest applying the
Gill-Murray’s modiﬁed Cholesky factorization to Gk to determine νk, which
results in
Gk + E = LDLT ,
(3.3.2)
where E is a diagonal matrix with nonnegative diagonal elements (see Gill,
Murray and Wright [152]). If E = 0, set νk = 0; if E ̸= 0, we can use the
Gerschgorin Circle Theorem 1.2.14 to compute an upper bound b1 of νk:
b1 =

min
1≤i≤n
⎧
⎨
⎩(Gk)ii −

j̸=i
|(Gk)ij|
⎫
⎬
⎭

≥
min
i
λi
 .
(3.3.3)

3.3. MODIFIED NEWTON’S METHOD
137
In addition, note that
b2 = max
i {eii}
(3.3.4)
is also an upper bound of νk, where eii is the i-th diagonal element of E.
Then we set
νk = min{b1, b2},
(3.3.5)
and get the positive deﬁnite matrix ¯Gk and its Cholesky factorization.
In the remainder of this section, we would like to introduce another nu-
merically stable modiﬁed Cholesky factorization due to Gill and Murray [149].
It is well-known that the Cholesky factorization Gk = LDLT of a positive
deﬁnite matrix Gk can be described as follows:
djj
=
gjj −
j−1

s=1
dssl2
js,
(3.3.6)
lij
=
1
djj
⎛
⎝gij −
j−1

s=1
dssljslis
⎞
⎠, i ≥j + 1,
(3.3.7)
where gij denote the elements of Gk, djj the diagonal elements of D. Now we
ask the Cholesky factors L and D to satisfy the following two requirements:
one is that all the diagonal elements of D are positive; the other is that the
elements of the factors are uniformly bounded. That is,
dkk > δ > 0, ∀k and |rik| ≤β, i > k,
(3.3.8)
where rik = lik
√dkk, β is a given positive number and δ is a small positive
number.
Below we will describe the j-th step of this factorization. Suppose that
the ﬁrst j −1 columns of the factors have been computed, that is, for k =
1, · · · , j−1, dkk and lik (i = 1, · · · , n) have been computed and satisfy (3.3.8).
Now we compute
γj = |ξj −
j−1

s=1
dssl2
js|,
(3.3.9)
where ξj takes gjj and the test value ¯d takes
¯d = max{γj, δ}.
(3.3.10)

138
CHAPTER 3. NEWTON’S METHODS
In order to judge whether to accept ¯d as the j-th element of D, we check if
rij = lij
√¯d satisﬁes (3.3.8). If yes, set djj = ¯d and form the j-th column of
L by use of lij = rij/
)djj; otherwise, set
djj =

ξj −
j−1

s=1
dssl2
js

,
(3.3.11)
where we take ξj = gjj + ejj in which ejj is chosen such that max |rij| = β,
and also form the j-th column of L as above.
When the above procedure is complete, we obtain a Cholesky factoriza-
tion of ¯Gk,
¯Gk = LDLT = Gk + E,
(3.3.12)
where E is a diagonal matrix with nonnegative diagonal elements ejj. For
given Gk, the nonnegative diagonal matrix E depends on the given β. Gill
and Murray (1974) prove that if n > 1, then
∥E(β)∥∞≤
 ξ
β + (n −1)β
!2
+ 2(γ + (n −1)β2) + δ,
(3.3.13)
where ξ and γ are respectively the maximum modules of non-diagonal ele-
ments and diagonal elements of Gk. Since, when β2 = ξ/
√
n2 −1, the above
bound is minimized, then we take β satisfying
β2 = max{γ, ξ/
)
n2 −1, ϵM}
(3.3.14)
where ϵM denotes the machine precision. Also, note that adding the term
ϵM in (3.3.14) is to prevent the case in which ∥Gk∥is too small.
Now we are in a position to state the modiﬁed Cholesky factorization al-
gorithm in which cis = lisdss(s = 1, · · · , j; i = j, · · · , n) are auxiliary variables
saved in Gk and we need not increase the storage.
Algorithm 3.3.2 (Modiﬁed Cholesky Factorization due to Gill and Murray
(1974))
Step 1. Compute β by (3.3.14). Given δ. Set j := 1, cii = gii for
i = 1, · · · , n.
Step 2. Find the smallest index q such that |cqq| = maxj≤i≤n |cii|,
exchange the q-th and the i-th rows, the q-th and the i-th
columns.

3.3. MODIFIED NEWTON’S METHOD
139
Step 3. Compute the j-th row of L and ﬁnd the maximum module of
lijdjj.
Set ljs = cjs/dss, s = 1, · · · , j −1;
Compute cij = gij −
j−1
s=1 ljscis, i = j + 1, · · · , n;
Set θj = maxj+1≤i≤n |cij| (if j = n, θj = 0).
Step 4. Compute the j-th diagonal element of D:
djj = max{δ, |cjj|, θ2
j/β2};
Update the element ejj: ejj = djj −cjj. If j = n, stop.
Step 5. Update cii = cii −c2
ij/djj, i = j + 1, · · · , n;
Set j := j + 1, go to Step 2.
2
The modiﬁed Cholesky factorization above needs about 1
6n3 arithmetic
operations which are almost the same as the normal Cholesky factorization.
Example 3.3.3 Consider
Gk =
⎛
⎜
⎝
1
1
2
1
1 + 10−20
3
2
3
1
⎞
⎟
⎠.
(3.3.15)
By the above Algorithm 3.3.2, we can get β2 = 1.061,
L =
⎛
⎜
⎝
1
0
0
0.2652
1
0
0.5303
0.4295
1
⎞
⎟
⎠, D =
⎛
⎜
⎝
3.771
0
0
0
5.750
0
0
0
1.121
⎞
⎟
⎠,
E =
⎛
⎜
⎝
2.771
0
0
0
5.016
0
0
0
2.243
⎞
⎟
⎠.
The diﬀerence ∥¯Gk −Gk∥F = ∥E∥F ≈6.154. Since djj ≥δ in the modiﬁed
factorization, it is guaranteed that ¯Gk = Gk + Ek is positive deﬁnite and the
condition number is uniformly bounded, i.e.,
∥¯Gk∥∥¯G−1
k ∥≤κ, κ ≥0.
So, we have
−∇f(xk)T sk
∥sk∥
≥1
κ∥∇f(xk)∥.
(3.3.16)

140
CHAPTER 3. NEWTON’S METHODS
Thus, it follows from the inexact line search, (2.5.19) and (3.3.16) that
{∇f(xk)} converges to zero.
Theorem 3.3.4 Let f : D ⊂Rn →R be twice continuously diﬀerentiable
on an open set D. Let the level set Ω= {x | f(x) ≤f(x0)} be compact. If
the sequence {xk} is generated by the modiﬁed Newton’s method, then
lim
k→∞∇f(xk) = 0.
(3.3.17)
3.4
Finite-Diﬀerence Newton’s Method
The ﬁnite-diﬀerence Newton’s method is to use the ﬁnite-diﬀerence as an
approximation of derivatives in Newton’s method.
We ﬁrst review the ﬁnite-diﬀerence derivative approximations.
Let F : Rn →Rm. The (i, j)-component of the Jacobian J(x) of F(x)
can be approximated by
ai,j = fi(x + hej) −fi(x)
h
,
(3.4.1)
where fi(x) denotes the i-th component of F(x), ej the j-th unit vector, h a
small perturbation of x. Equivalently, if A.j denotes the j-th column of A,
we have
A.j = F(x + hej) −F(x)
h
.
(3.4.2)
Theorem 3.4.1 Let F : Rn →Rm satisfy the conditions of Theorem 1.2.22.
Let the norm ∥· ∥satisfy ∥ej∥= 1, j = 1, · · · , n. Then
∥A.j −J(x).j∥≤γ
2|h|.
(3.4.3)
If the norm used is l1 norm, then
∥A −J(x)∥1 ≤γ
2|h|.
(3.4.4)
Proof.
By setting d = hej in (1.2.109), we obtain
∥F(x + hej) −F(x) −J(x)hej∥≤γ
2∥hej∥2 = γ
2|h|2.
Dividing by h gives (3.4.3). Noting from (1.2.7) that the l1 norm of a matrix
is the maximum of the l1 norm of a vector, we immediately get (3.4.4).
2

3.4. FINITE-DIFFERENCE NEWTON’S METHOD
141
Now, let f : Rn →R. An approximation to the gradient ∇f(x) can be
obtained by the forward-diﬀerence approximation, deﬁned as
∂f
∂xi
(x) ≈f(x + hei) −f(x)
h
.
(3.4.5)
This process requires evaluation of f at n + 1 points: x and x + hei, i =
1, · · · , n. Obviously, it follows from (1.2.109) that
∂f
∂xi
(x) = f(x + hei) −f(x)
h
+ δh,
(3.4.6)
where
|δh| ≤γ
2h.
(3.4.7)
It means there is O(h) error in the forward-diﬀerence formula.
A more accurate approximation to the derivative can be obtained by using
the central-diﬀerence formula, deﬁned as
∂f
∂xi
(x) ≈f(x + hei) −f(x −hei)
2h
.
The two theorems below give respectively approximations to the gradient
and the Hessian of f, and describe the error bounds of these approximations.
Theorem 3.4.2 Let f : D ⊂Rn →R satisfy the conditions of Theorem
1.2.23. Let the norm used satisfy ∥ei∥= 1, i = 1, · · · , n. Assume that x +
hei, x −hei ∈D, i = 1, · · · , n. Also let the vector a ∈Rn with components ai,
be deﬁned as
ai = f(x + hei) −f(x −hei)
2h
.
(3.4.8)
Then
|ai −|∇f(x)|i| ≤γ
6h2.
(3.4.9)
If the norm used is the l∞norm, then
∥a −∇f(x)∥∞≤γ
6h2.
(3.4.10)
Proof.
Deﬁne α and β respectively as
α = f(x + hei) −f(x) −h[∇f(x)]i −1
2h2[∇2f(x)]ii
(3.4.11)

142
CHAPTER 3. NEWTON’S METHODS
and
β = f(x −hei) −f(x) + h[∇f(x)]i −1
2h2[∇2f(x)]ii.
(3.4.12)
By using (1.2.110) and setting d = ±hei, we have
|α| ≤γ
6h3, |β| ≤γ
6h3.
Then using the triangle inequality gives
|α −β| ≤γ
3h3.
Also, from (3.4.11)-(3.4.12) and (3.4.8), we get
α −β = 2h(ai −[∇f(x)]i),
which gives (3.4.9).
Finally, by using the deﬁnition of l∞norm, we get
(3.4.10) immediately from (3.4.9).
2
Theorem 3.4.3 Let f satisfy the conditions of Theorem 3.4.2. Assume that
x, x + hei, x + hej, x + hei + hej ∈D, 1 ≤i, j ≤n. Also let A ∈Rn×n with
components aij deﬁned as
aij = f(x + hei + hej) −f(x + hei) −f(x + hej) + f(x)
h2
.
(3.4.13)
Then
|aij −[∇2f(x)]ij| ≤5
3γh.
(3.4.14)
If the matrix norm is l1, l∞, or Frobenius norm, then
∥A −∇2f(x)∥≤5
3γhn.
(3.4.15)
Proof.
The proof is similar to the proof in Theorem 3.4.2. Set
α
=
f(x + hei + hej) −f(x) −(hei + hej)T ∇f(x)
−1
2(hei + hej)T ∇2f(x)(hei + hej),
β
=
f(x + hei) −f(x) −(hei)T ∇f(x) −1
2(hei)T ∇2f(x)(hei),
η
=
f(x + hej) −f(x) −(hej)T ∇f(x) −1
2(hej)T ∇2f(x)(hej),

3.4. FINITE-DIFFERENCE NEWTON’S METHOD
143
respectively. Then
α −β −η = h2(aij −[∇2f(x)]ij).
(3.4.16)
Also, we have
|α −β −η|
≤
|α| + |β| + |η|
≤
γ
6∥hei + hej∥3 + γ
6∥hei∥3 + γ
6∥hej∥3
≤
5
3γh3.
This inequality together with (3.4.16) gives the result (3.4.14). The inequality
(3.4.15) is a consequence of (3.4.14) and deﬁnitions of norms.
2
Now we are in a position to discuss the ﬁnite-diﬀerence Newton’s method
for nonlinear equations
F(x) = 0,
(3.4.17)
where F : Rn →Rn is continuously diﬀerentiable.
The Newton’s method for (3.4.17) is as follows:
Solve J(xk)d = −F(xk) for dk;
Set xk+1 = xk + αkdk;
where J(xk) is the Jacobian matrix of F at xk. When J(x) is not available,
we can use ﬁnite-diﬀerence derivative approximation and get the following
ﬁnite-diﬀerence Newton’s method for (3.4.17):
(Ak).j
=
F(xk + hkej) −F(xk)
hk
, j = 1, · · · , n,
(3.4.18)
xk+1
:=
xk −A−1
k F(xk), k = 0, 1, · · · .
(3.4.19)
Theorem 3.4.4 Let F : Rn →Rn be continuously diﬀerentiable on an open
convex set D ⊂Rn.
Assume there exist x∗∈Rn and r, β > 0, so that
N(x∗, r) ⊂D, F(x∗) = 0, J(x∗)−1 exists and satisﬁes ∥J(x∗)−1∥≤β, where
J is Lipschitz continuous in the neighborhood N(x∗, r) = {x ∈Rn| ∥x−x∗∥<
r}. Then there exist ϵ, h > 0, such that if x0 ∈N(x∗, ϵ) and {hk} is a real
sequence with 0 < |hk| ≤h, then the sequence {xk} generated from (3.4.18)-
(3.4.19) is well-deﬁned and converges to x∗linearly. If
lim
k→∞hk = 0,

144
CHAPTER 3. NEWTON’S METHODS
the convergence is superlinear. Furthermore, if there exists a constant c1,
such that
|hk| ≤c1∥xk −x∗∥,
(3.4.20)
or equivalently, there exists a constant c2, such that
|hk| ≤c2∥F(xk)∥,
(3.4.21)
then the convergence rate is quadratic.
Proof.
Choose ϵ and h such that, for xk ∈N(x∗, ϵ), Ak is nonsingular and
|hk| < h. Let ϵ ≤r and
ϵ + h ≤
1
2βγ .
(3.4.22)
Now we prove, by induction, that
∥xk+1 −x∗∥≤1
2∥xk −x∗∥,
(3.4.23)
so
xk+1 ∈N(x∗, ϵ).
(3.4.24)
For k = 0, we ﬁrst prove A0 is nonsingular. By assumptions and Theorem
3.4.1, we have ∥A(x) −J(x)∥≤γh
2 , and then
∥J(x∗)−1[A0 −J(x∗)]∥
≤
∥J(x∗)−1∥∥[A0 −J(x0)] + [J(x0) −J(x∗)]∥
≤
β
 γh
2 + γϵ
!
≤1
2.
(3.4.25)
From Von-Neumann Theorem 1.2.5 we know that A0 is nonsingular and that
∥A−1
0 ∥≤2β.
(3.4.26)
Hence x1 is well-deﬁned and
x1 −x∗
=
−A−1
0 F(x0) + x0 −x∗
=
A−1
0 {[F(x∗) −F(x0) −J(x0)(x∗−x0)]
+[(J(x0) −A0)(x∗−x0)]}.
(3.4.27)

3.4. FINITE-DIFFERENCE NEWTON’S METHOD
145
Then from (3.4.26), (1.2.109) and (3.4.22), we get
∥x1 −x∗∥
≤
∥A−1
0 ∥{∥F(x∗) −F(x0) −J(x0)(x∗−x0)∥
+∥A0 −J(x0)∥∥x∗−x0∥}
(3.4.28)
≤
2β
*γ
2∥x∗−x0∥2 + γ
2h∥x0 −x∗∥
+
(3.4.29)
≤
βγ(ϵ + h)∥x∗−x0∥
≤
1
2∥x0 −x∗∥.
(3.4.30)
Assume that the conclusion holds for k = j, in the same way as k = 0,
we can prove that the conclusion is also true for k = j + 1.
Therefore,
(3.4.23)-(3.4.24) hold. They also show the linear convergence of the iterative
sequence.
The key for superlinear and quadratic convergence requires an improved
bound on ∥A0−J(x0)∥. When limk→∞hk = 0, the second term in the bracket
of (3.4.29) approaches zero, and hence
∥xk+1 −x∗∥
∥xk −x∗∥
→0, when k →∞,
which implies that the method converges superlinearly.
Similarly, when
(3.4.20) is satisﬁed, it follows from (3.4.29) that the method converges to
x∗quadratically. Finally, the equivalence of (3.4.20) and (3.4.21) is just a
consequence of Theorem 1.2.25.
2
For unconstrained optimization problem
min
x∈Rn f(x),
(3.4.31)
when the gradient ∇f(x) is available, we can obtain the Hessian approxima-
tion by using the forward-diﬀerence or central-diﬀerence of the gradient. In
this case, the iteration scheme for the k-th step is as follows:
(A).j
=
∇f(xk + hjej) −∇f(xk)
hj
, j = 1, · · · , n,
(3.4.32)
Ak
=
A + AT
2
,
(3.4.33)
xk+1
=
xk −A−1
k ∇f(xk),
(3.4.34)
where
hj = √η max{|xj|, ˜xj}sign(xj),
(3.4.35)

146
CHAPTER 3. NEWTON’S METHODS
˜xj is a typical estimation given by users, and η is a small number more than
the machine accuracy.
If the standard assumptions of Theorem 3.4.4 hold, and if hj satisﬁes
hj = O(∥xk −x∗∥),
this ﬁnite-diﬀerence Newton’s method (3.4.34) maintains the quadratic con-
vergence rate.
Sometimes, some algorithms require us to supply the Hessian matrix-
vector product ∇2f(x)d, where d is a given vector. Instead of (3.4.32), we
can use
∇2f(xk)d ≈∇f(xk + hd) −∇f(xk)
h
,
(3.4.36)
which also has O(h) approximation error. For obtaining this approximation,
the cost is only evaluation of a single gradient at xk + hd. However, the cost
of (3.4.32) is evaluation of the gradient at n + 1 points xk and xk + hjej, j =
1, · · · , n.
In the case that the gradient ∇f(x) is not available, we can only use the
function values to approximate the Hessian. The expression (3.4.13) gives
the Hessian approximation as follows:
(Ak)ij =
[f(xk + hiei + hjej) −f(xk + hiei)] −[f(xk + hjej) −f(xk)]
hihj
,
where
hj =
3√η max{|xj|, ˜xj}sign(xj)
or
hj = (˜ϵ)1/3xj,
where ˜ϵ is a machine accuracy.
Using the forward-diﬀerence and central-
diﬀerence, the gradient approximations are respectively
(ˆgk)j = f(xk + hjej) −f(xk)
hj
, j = 1, · · · , n
(3.4.37)
and
(ˆgk)j = f(xk + hjej) −f(xk −hjej)
2hj
, j = 1, · · · , n.
(3.4.38)

3.5. NEGATIVE CURVATURE DIRECTION METHOD
147
Their approximation errors are O(hj) and O(h2
j) respectively. In this case,
the ﬁnite-diﬀerence Newton’s iteration is
xk+1 = xk −A−1
k ˆgk,
(3.4.39)
where Ak and ˆgk are ﬁnite-diﬀerence approximations of ∇2f(xk) and ∇f(xk)
respectively.
Under the standard assumptions of Theorem 3.4.4, we have
similarly
∥xk+1 −x∗∥
≤
∥A−1
k ∥(v
2∥xk −x∗∥2 + ∥Ak −∇2f(xk)∥∥xk −x∗∥
+
∥ˆgk −∇f(xk)∥).
(3.4.40)
Note that there is an additional term ∥ˆgk−∇f(xk)∥than (3.4.28). If we want
to get the quadratic convergence rate, it is obvious to require ∥ˆgk−∇f(xk)∥=
O(∥xk −x∗∥2) which implies hj = O(∥xk −x∗∥2). Therefore, it tells us that,
when using the central-diﬀerence, the iteration (3.4.39) possesses quadratic
rate. If we use the forward-diﬀerence, the iteration has quadratic rate only
when hj = O(∥xk −x∗∥2).
In general, the forward-diﬀerence scheme is practical. Although the error
of the central-diﬀerence scheme is O(h2
j), as compared to the O(hj) error in
forward-diﬀerence, the cost is about twice as much as that of the forward-
diﬀerence. Hence, we use the central-diﬀerence scheme only for those prob-
lems which need higher accuracy.
Stewart [323] gave a switch rule from
forward diﬀerence to central diﬀerence. Finally, it should be mentioned that,
if the gradient is available, it is better to make the best use of it.
3.5
Negative Curvature Direction Method
Another strategy for modifying Newton’s method, the negative curvature
direction method, is presented, because the modiﬁed Newton’s methods de-
scribed above are not adequate for the case in which the Hessian ∇2f(xk) is
indeﬁnite and xk is close to a saddle point.
Now, we ﬁrst put forward the deﬁnition below.
Deﬁnition 3.5.1 Let f : Rn →R be twice continuously diﬀerentiable on an
open set D ⊂Rn.
(i) If ∇2f(x) has at least a negative eigenvalue, then x is said to be an
indeﬁnite point.

148
CHAPTER 3. NEWTON’S METHODS
(ii) If x is an indeﬁnite point and d satisﬁes dT ∇2f(x)d < 0, then d is said
to be a negative curvature direction of f(x) at x.
(iii) If
sT ∇f(x) ≤0, dT ∇f(x) ≤0, dT ∇2f(x)d < 0,
then the vector pair (s, d) is said to be a descent pair at the indeﬁnite
point x. If x is not an indeﬁnite point and satisﬁes
sT ∇f(x) < 0, dT ∇f(x) ≤0, dT ∇2f(x)d = 0,
then the vector pair (s, d) is said to be a descent pair at x.
As an example of a descent pair, we can choose
s
=
−∇f(x),
d
=

0,
if ∇2f(x) ≥0,
−sign (uT ∇f(x))u,
otherwise,
where u is a unit eigenvector corresponding to a negative eigenvalue of
∇2f(x).
Obviously, there no longer exists the descent pair if and only if ∇f(x) = 0
and ∇2f(x) is positive semi-deﬁnite.
From the deﬁnition above, at the stationary point, the negative curvature
direction must be a descent direction. At a general point, if the negative
curvature direction satisﬁes dT ∇f(x) = 0, then both d and −d are descent
directions. If dT ∇f(x) ≤0, d is a descent direction, and if dT ∇f(x) ≥0, −d
is a descent direction.
In this section, we ﬁrst give the Gill-Murray stable Newton’s method
which uses negative curvature direction. Then we discuss two negative cur-
vature direction methods: Fiacco-McCormick method and Fletcher-Freeman
method. Finally, we consider the second order Armijo step rules and the
second order Wolfe-Powell step rules.
3.5.1
Gill-Murray Stable Newton’s Method
The basic idea of Gill-Murray stable Newton’s method is: when the Hessian
Gk is indeﬁnite, one uses the modiﬁed Cholesky factorization to force the
matrix Gk to be positive deﬁnite; when xk approaches to a stationary point,
use the negative curvature direction to decrease the objective function.

3.5. NEGATIVE CURVATURE DIRECTION METHOD
149
Let the modiﬁed Cholesky factorization be
¯Gk = Gk + Ek = LkDkLT
k ,
where
Dk = diag(d11, · · · , dnn), Ek = diag(e11, · · · , enn).
When ∥gk∥≤ϵ and ∇2f(xk) is not positive semi-deﬁnite, we use the following
negative curvature direction algorithm.
Algorithm 3.5.2
Step 1. Set ψj = djj −ejj, j = 1, · · · , n.
Step 2. Find the subscript t, such that ψt = min{ψj | j = 1, · · · , n}.
Step 3. If ψt ≥0, stop; otherwise, solve
LT
k d = et
(3.5.1)
for dk, where et is a unit vector with the t-th component of
et being 1.
2
Theorem 3.5.3 Let Gk be the Hessian of f(x) at xk and
¯Gk = Gk + Ek = LkDkLT
k .
If the direction dk is obtained by Algorithm 3.5.2, then dk is a negative cur-
vature direction at xk, and at least one in dk and −dk is descent direction at
xk.
Proof.
Since Lk is a unit lower triangular matrix, the solution dk of (3.5.1)
has the form
dk = (ρ1, · · · , ρt−1, 1, 0, · · · , 0)T .
Then
dT
k Gkdk
=
dT
k ¯Gkdk −dT
k Ekdk
=
dT
k LkDkLT
k dk −dT
k Ekdk
=
eT
t Dket −
t−1

r=1
ρ2
rerr + ett

=
dtt −ett −
t−1

r=1
ρ2
rerr
=
ψt −
t−1

r=1
ρ2
rerr.

150
CHAPTER 3. NEWTON’S METHODS
By the modiﬁed Cholesky factorization Algorithm 3.3.2, we have
ejj
=
¯gjj −gjj = djj +
j−1

r=1
l2
jrdr −gjj
=
djj −cjj ≥0,
which indicates that 
t−1
r=1 ρ2
rerr ≥0. Also, since ψt < 0, we obtain dT
k Gkdk <
0, which means dk is a negative curvature direction, and −dk too. If gT
k dk ≤0,
then dk is a descent direction; otherwise, −dk is a descent direction.
2
The algorithm below is the Gill-Murray numerically stable Newton’s
method.
Algorithm 3.5.4
Step 1. Given a starting point x0, ϵ > 0. Set k := 1.
Step 2. Compute gk and Gk.
Step 3. Compute modiﬁed Cholesky factorization by using Algorithm
3.3.2
Gk + Ek = LkDkLT
k .
Step 4. If ∥gk∥> ϵ, solve LkDkLT
k dk = −gk for dk, and go to Step
6; otherwise, go to Step 5.
Step 5 Perform Algorithm 3.5.2. If it cannot produce dk (i.e., ψt ≥
0), stop; otherwise, ﬁnd dk and set
dk =

−dk,
if gT
k dk > 0,
dk,
otherwise.
Step 6. Compute line search factor αk, and set xk+1 = xk + αkdk.
Step 7. If f(xk+1) ≥f(xk), stop; otherwise, set k = k + 1, and go
to Step 2.
2
About the convergence of the algorithm above, we have the following
theorem.

3.5. NEGATIVE CURVATURE DIRECTION METHOD
151
Theorem 3.5.5 Let f : Rn →R be twice continuously diﬀerentiable on an
open set D. Assume there exists ¯x ∈D ⊂Rn such that the level set
L(¯x) = {x | f(x) ≤f(¯x)}
is a bounded closed convex set. Assume that we pick ϵ = 0 in Algorithm
3.5.4, and the starting point x0 ∈L(¯x). Then the sequence {xk} generated
from Algorithm 3.5.4 satisﬁes
(i) when {xk} is a ﬁnite sequence, its last element must be the stationary
point of f(x);
(ii) when {xk} is an inﬁnite sequence, it must have accumulation points, and
all accumulation points are the stationary points of f(x).
The proof is omitted. We refer the interested reader to the original paper
Gill and Murray [147].
3.5.2
Fiacco-McCormick Method
The idea of the negative curvature direction method was ﬁrst presented by
Fiacco and McCormick [122] who dealt with the case that the Hessian Gk has
negative eigenvalues and employed the exact line search. The idea is simply
to go forward along a negative curvature direction and decrease the objective
function.
When
dT
k gk ≤0 and dT
k Gkdk < 0,
(3.5.2)
f(xk + dk) ≈f(xk) + dT
k gk + 1
2dT
k Gkdk
will be descending. Since Gk is indeﬁnite, the Fiacco-McCormick method
uses the decomposition
Gk = LDLT ,
(3.5.3)
where L is a unit lower triangular, and D is a diagonal matrix. If Gk is
positive deﬁnite, the dk generated from this decomposition (3.5.3) is a descent
direction. However, if there exists a negative dii, then solve
LT t = a,
(3.5.4)

152
CHAPTER 3. NEWTON’S METHODS
where the components ai of the vector a is deﬁned as
ai =

1,
dii ≤0,
0,
dii > 0.
(3.5.5)
It is easy to show that
dk =

t,
gT
k t ≤0,
−t,
gT
k t > 0,
(3.5.6)
is a negative curvature direction satisfying (3.5.2).
Unfortunately, the decomposition (3.5.3) may be potentially unstable,
amplify the rounding errors, and even do not exist.
Hence, Fletcher and
Freeman [135] employ a stable symmetric indeﬁnite factorization.
3.5.3
Fletcher-Freeman Method
Fletcher and Freeman [135], instead, employ a stable symmetric indeﬁnite
factorization due to Bunch and Parlett [33]. For any symmetric matrix Gk,
there exists a permutation matrix, such that
P T GkP = LDLT ,
(3.5.7)
where L is unit lower triangular, D is a block diagonal matrix with blocks
of dimension 1 or 2. The aim to use the permutation matrix is to maintain
the symmetricity and numerical stability. Contrasting with the factorization
(3.5.3), the factorization (3.5.7) always exists and can be computed by a
numerically stable process. Now, for 1 × 1 pivot case, let A be an n × n
matrix
A = A(0) =

a11
⃗aT
21
⃗a21
A22

,
(3.5.8)
where ⃗a21 is (n−1)×1 vector, A22 is an (n−1)×(n−1) matrix. Eliminating
one row and one column yields a reduced matrix A(1):
A(1) = A(0) −d11l1lT
1 =

0
0T
0
A22 −⃗a21⃗aT
21/d11

,
(3.5.9)
where
d11 = a11, l1 =
1
d11

a11
⃗a21

=

1
⃗a21/d11

.
(3.5.10)

3.5. NEGATIVE CURVATURE DIRECTION METHOD
153
For 2 × 2 pivot case, let
A(0) =

A11
AT
21
A21
A22

,
(3.5.11)
where A11 is a 2 × 2 block matrix, A21 is an (n −2) × 2 matrix, and A22 is
an (n −2) × (n −2) matrix. Eliminating two rows and two columns yields a
reduced matrix A(2):
A(2)
=
A(0) −L1D1LT
1 = A(0) −

I
L21

D1[I LT
21]
=

0
0
0
A22 −A21D−1
1 AT
21

,
(3.5.12)
where
D1 = A11, L1 =

A11
A21

D−1
1
=

I
A21A−1
11

∆=

I
L21

.
(3.5.13)
Next step, we will apply the same process to the remaining matrix A22 −
⃗a21⃗aT
21/d11 or A22 −A21D−1
1 AT
21 with dimension (n−1)×(n−1) or (n−2)×
(n −2) respectively. Finally, this recursive procedure gives (3.5.7).
In all the iterations, the algorithm has to identify the pivot block between
two pivoting forms. A natural problem is how to identify 1 × 1 submatrix
a11 or 2×2 block submatrix A11 as a pivot block. Now we describe a criteria
as follows. First, compute the largest-magnitude diagonal and the largest-
magnitude oﬀ-diagonal elements, denoting their respective magnitude by ξdia
and ξoff. If the growth ratio ξdia/ξoff is acceptable, we choose the diagonal
element with largest-magnitude as a pivot and perform row-column exchange
such that a11 is just the element.
Otherwise, we choose the oﬀ-diagonal
element, say aij, whose magnitude is ξoff, and choose the corresponding
2 × 2 block

aii
aij
aij
ajj

as a pivot block. Then we perform row-column exchange such that A11 is
this 2 × 2 block.
This decomposition needs n3/6 + O(n2) multiplications. Maybe the ex-
pensive computation is a disadvantage of this method. A more economical
improvement is presented by Bunch and Kaufman [32]. The interested reader

154
CHAPTER 3. NEWTON’S METHODS
may consult that paper. The forms of L and D produced by the decompo-
sition are a block lower triangular matrix and a block diagonal matrix, for
example,
D =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, L =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
1
∗
∗
1
∗
∗
0
1
∗
∗
∗
∗
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The decomposition above is said to be Bunch-Parlett factorization, in brief,
B-P factorization which can generate negative curvature direction.
Let Gk have symmetric indeﬁnite factorization
Gk = LDLT .
(3.5.14)
We solve the triangular system of equations
LT t = a,
(3.5.15)
where, in the case of 1 × 1 pivot, the components of a are
ai =

1,
dii ≤0,
0,
dii > 0;
(3.5.16)
in the case of 2 × 2 pivot,

ai
ai+1

is the unit eigenvector corresponding to
the negative eigenvalue of

dii
di,i+1
di+1,i
di+1,i+1

. Set
dk =

t,
when gT
k t ≤0,
−t,
when gT
k t > 0,
(3.5.17)
then dk is the negative curvature direction satisfying (3.5.2). In fact, we have
dT
k Gkdk = dT
k LDLT dk = aT Da =

i:λi<0
λi < 0,
(3.5.18)
and
dT
k gk ≤0.
(3.5.19)

3.5. NEGATIVE CURVATURE DIRECTION METHOD
155
In addition, when D has negative eigenvalues, the direction dk can also
be computed by
dk = −L−T ˜D+L−1gk,
(3.5.20)
where ˜D is the positive part of D, i.e.,
˜Di =

dii,
when dii > 0,
0,
otherwise,
and ˜D+ is the generalized inverse of ˜D.
When D contains at least one zero eigenvalue, the direction dk can be
computed by
Gkdk = LDLT dk = 0, gT
k dk < 0.
(3.5.21)
When all the eigenvalues of D are positive, all blocks of D are 1 × 1
elements.
In this case, B-P decomposition is reduced to usual Cholesky
factorization, and the direction produced is usual Newton’s direction
dk = −L−T D−1L−1gk.
It is not diﬃcult to see that the negative curvature descent direction de-
termined by (3.5.17) is limited in some subspace; the direction from (3.5.20)
is a Newton’s direction limited in the subspace of positive curvature direc-
tion. Although the idea of using negative curvature directions is in some ways
attractive, Fletcher and Freeman [135] ﬁnd that it is not satisfactory to use
such directions on successive iterations and that if we alternate positive cur-
vature and negative curvature search, i.e., alternate (3.5.17) and (3.5.20), we
can get better results. Similarly, if one continuously meets zero eigenvalue,
alternating (3.5.20) and (3.5.21) will give better results.
3.5.4
Second-Order Step Rules
Second-Order Armijo Step Rule – McCormick Method
In §2.5 we have discussed Armijo line search rule. Consider
min f(x), x ∈D ⊂Rn,
(3.5.22)
where f : Rn →R is a continuously diﬀerentiable function in the open set
D.

156
CHAPTER 3. NEWTON’S METHODS
Given β ∈(0, 1) and ρ ∈(0, 1), mk is the least nonnegative integer m
such that
f(xk + βmτdk) ≤f(xk) + ρβmτgT
k dk,
(3.5.23)
where τ > 0, or require α to satisfy
f(xk + αdk) ≤f(xk) + ραgT
k dk.
(3.5.24)
For the steepest descent method
xk+1 = xk −2−igk,
(3.5.25)
the Armijo rule is
f(xk+1) ≤f(xk) −ρ2−i∥gk∥2, ρ ∈(0, 1).
(3.5.26)
Instead of using only one descent direction and searching in a line deter-
mined by that direction, we search along a curve of the form
x(α) = xk + φ1(α)sk + φ2(α)dk,
(3.5.27)
where (sk, dk) is a descent pair at xk deﬁned in Deﬁnition 3.5.1, φ1(α) and
φ2(α) are polynomials with φ1(0) = φ2(0) = 0.
If we set Φ(α) = f(x(α)) and assume that ρ ∈(0, 1), there is an ¯α > 0
such that
Φ(α) ≤Φ(0) + ρ[Φ′(0)α + 1
2Φ′′(0)α2]
(3.5.28)
for all α ∈[0, ¯α] provided that either Φ′(0) < 0 or Φ′(0) = 0 and Φ′′(0) < 0.
Normally, in (3.5.27) we choose φ1(α) and φ2(α) as lower-order polyno-
mials. The simplest functions of this type are
φ1(α) = α2, φ2(α) = α,
which lead to the iteration
x(α) = xk + α2sk + αdk.
(3.5.29)
If we set α = γi, γ ∈(0, 1), (3.5.29) becomes
xk(i) = xk + γ2isk + γidk ∈D.
(3.5.30)

3.5. NEGATIVE CURVATURE DIRECTION METHOD
157
The second-order Armijo rule requires us to ﬁnd i(k) which is the smallest
nonnegative integer i such that
f(xk(i)) ≤f(xk) + ργ2i[gT
k sk + 1
2dT
k Gkdk],
(3.5.31)
where ρ ∈(0, 1), and set xk+1 = xk(i(k)).
Typically, McCormick [203]
chooses γ2 = 1
2 in (3.5.30).
There exists a ﬁnite i(k) satisfying (3.5.31) provided that
sT
k gk < 0, whenever gk ̸= 0
(3.5.32)
and
dT
k Gkdk < 0, whenever gk = 0.
(3.5.33)
Only if xk is a point satisfying the second-order optimal condition, there
does not exist the descent pair satisfying (3.5.32)-(3.5.33), and the algorithm
terminates. The following is the convergence theorem of the second-order
Armijo rule.
Theorem 3.5.6 Let f : Rn →R be twice continuously diﬀerentiable on the
open set D, and assume that for some x0 ∈D, the level set
L(x0) = {x ∈D : f(x) ≤f(x0)}
is compact. Suppose that {∥sk∥} and {∥dk∥} are bounded. If {xk} satisﬁes
(3.5.30) and (3.5.31), then
lim
k→∞gT
k sk = 0
(3.5.34)
and
lim
k→∞dT
k Gkdk = 0.
(3.5.35)
Proof.
The sequence {f(xk)} is decreasing and bounded below due to
the continuity of f and the compactness of L(x0). Thus {f(xk) −f(xk+1)}
converges to zero. Let i(k) be the smallest nonnegative integer such that
(3.5.30)-(3.5.31) hold, then there are two cases to consider.
Case 1. Suppose the integer sequence {i(k)} is bounded above by β ≥0.
Then
f(xk) −f(xk+1) ≥−ργ2β[gT
k sk + 1
2dT
k Gkdk].
(3.5.36)

158
CHAPTER 3. NEWTON’S METHODS
Since −gT
k sk ≥0 and −dT
k Gkdk ≥0, the conclusion follows.
Case 2. Suppose that the integer {i(k)} is not bounded above. So, with-
out loss of generality, we can assume that limk→+∞i(k) = +∞. By (3.5.30)-
(3.5.31),
f(x(i(k) −1)) −f(xk) > ργ2[i(k)−1][gT
k sk + 1
2dT
k Gkdk].
(3.5.37)
For convenience, let
pk = γ2[i(k)−1]sk + γi(k)−1dk.
By using Taylor’s theorem and noting that ∇2f(x) is continuous, we have
f(x(i(k) −1)) −f(xk) = pT
k gk + 1
2pT
k Gkpk + o(γ2[i(k)−1]).
(3.5.38)
Combining (3.5.37) and (3.5.38) gives
o(γ2[i(k)−1]) > (1 −ρ)γ2[i(k)−1][−gT
k sk −1
2dT
k Gkdk].
(3.5.39)
Dividing by (1 −ρ)γ2[i(k)−1] and taking limits yields
gT
k sk →0 and dT
k Gkdk →0.
2
Furthermore, we have the following result.
Theorem 3.5.7 Assume that the conditions in Theorem 3.5.6 hold. In ad-
dition, suppose there exist positive constants c1, c2, c3, such that
∥sk∥≥c3∥gk∥,
(3.5.40)
dT
k Gkdk ≤c2λGk,
(3.5.41)
−sT
k gk ≥c1∥sk∥∥gk∥,
(3.5.42)
where λGk is the most negative eigenvalue of Gk. Then the accumulation
point x∗of {xk} satisﬁes ∇f(x∗) = 0, and ∇2f(x∗) is positive semi-deﬁnite
with at least one zero eigenvalue.
Proof.
From Theorem 3.5.6, we have
gT
k sk →0 and dT
k Gkdk →0.

3.5. NEGATIVE CURVATURE DIRECTION METHOD
159
Using (3.5.42) and (3.5.40) gives
−gT
k sk ≥c1c3∥gk∥2.
Thus we get ∥gk∥→0. Also, it follows from (3.5.41) that ¯dT ∇2f(x∗) ¯d = 0
with ¯d a limit eigenvector of a subsequence of {dk}. Therefore, ∇2f(x∗) is
positive semi-deﬁnite with at least one zero eigenvalue.
2
Second-Order Armijo Step Rule — Goldfarb Method
Goldfarb [154] thinks that the iteration
xk(α) = xk + α2sk + αdk
(3.5.43)
is not ideal. The form (3.5.43) may be good in the neighborhood of a saddle
point. However, far from a saddle point, it is not a good approach. Then
Goldfarb [154] put forward a similar second-order Armijo rule based on the
iteration of the form
xk(α) = xk + αsk + α2dk,
(3.5.44)
and gives the following algorithm:
For given γ and ρ, where 0 < γ, ρ < 1, and an initial point x0, determine
xk+1, for k = 0, 1, · · · , as follows:
Choose a descent pair (sk, dk) at xk. If none exists, stop. Otherwise, let
i(k) + 1 be the smallest nonnegative integer such that
f(xk(γi)) −f(xk) ≤ρ[γisT
k gk + 1
2γ4idT
k Gkdk]
(3.5.45)
and set
xk+1 = xk(γi(k)+1).
(3.5.46)
In very much the same manner as Theorem 3.5.6 and Theorem 3.5.7, we
have the convergence theorems. So we give them as follows without proof.
Theorem 3.5.8 Let f : Rn →R have two continuous derivatives on the
open set D and let the level set S = {x | f(x) ≤f(x0)} be a compact subset
of D for a given x0 ∈D. Suppose that an admissible sequence of descent
pairs {(sk, dk)} is used in the above algorithm, and that
−sT
k gk
≥
c1∥sk∥2,
(3.5.47)
sT
k Gksk
≤
c2∥sk∥2,
(3.5.48)
where 0 < c1, c2 < ∞. Then gk →0, sk →0, λk →0, and dk →0.

160
CHAPTER 3. NEWTON’S METHODS
Theorem 3.5.9 In addition to the assumptions of Theorem 3.5.8, assume
that the set of stationary points of f(x) in the level set L is ﬁnite. Then, if
{xk} is the sequence obtained by the second-order Armijo steplength algorithm
(3.5.45) and (3.5.46), we have
lim
k→∞xk = x∗, g(x∗) = 0, G(x∗) ≥0.
(3.5.49)
Moreover, if inﬁnitely many Gk ̸≥0, then G(x∗) has at least one eigenvalue
equal to zero.
Second-Order Wolfe-Powell Step Rule — Mor´e-Sorensen Rule
Consider the iteration of the form
x(α) = xk + α2sk + αdk,
(3.5.50)
where (sk, dk) is a descent pair at xk. Replacing Wolfe-Powell step rule (2.5.3)
and (2.5.7), we ask α to satisfy
f(x(α)) ≤f(x) + ρα2[∇f(x)T s + 1
2dT ∇2f(x)d],
(3.5.51)
∇f(x(α))T x′(α) ≥σ[∇f(x)T d + 2α∇f(x)T s + αdT ∇2f(x)d],
(3.5.52)
where 0 < ρ ≤σ < 1.
When d = 0, these conditions reduce to those
of (2.5.3) and (2.5.7). The conditions (3.5.51) and (3.5.52) are said to be
the second-order Wolfe-Powell step rule which is contributed by Mor´e and
Sorensen [221].
If (sk, dk) is a descent pair at xk and we set
Φk(α) = f(xk + α2sk + αdk),
(3.5.53)
then (3.5.51) and (3.5.52) are equivalent to
Φk(αk)
≤
Φk(0) + 1
2ρΦ′′
k(0)α2
k,
(3.5.54)
Φ′
k(αk)
≥
σ[Φ′
k(0) + Φ′′
k(0)αk].
(3.5.55)
The second order Wolfe-Powell step rule has a geometric interpretation as
shown in Figure 3.5.1.

3.5. NEGATIVE CURVATURE DIRECTION METHOD
161
Figure 3.5.1 Second-order Wolfe-Powell step rule
Similar to the preceding discussion, we now give the following convergence
results.
Theorem 3.5.10 Let f : Rn →R have twice continuous derivatives on the
open set D, and assume that for some x0 ∈D, the level set
L(x0) = {x ∈D | f(x) ≤f(x0)}
is a compact subset of D. If {xk} satisﬁes (3.5.50)-(3.5.52), then
lim
k→∞gT
k sk = 0 and lim
k→∞dT
k Gkdk = 0.
(3.5.56)
Proof.
From (3.5.53) we have Φ′
k(0) = gT
k dk and
Φ′′
k(0) = 2gT
k sk + dT
k Gkdk.
Since (sk, dk) is a descent pair, Φ′
k(0) ≤0 and Φ′′
k(0) < 0. Thus (3.5.51)
implies that {xk} ⊂L(x0). By the continuity of f and compactness of L(x0)
we have that {fk −fk+1} converges to zero. Since
fk −fk+1 ≥−1
2ρΦ′′
k(0)α2
k ≥0,
it follows that
lim
k→∞α2
kgT
k sk = 0
(3.5.57)
and
lim
k→∞α2
kdT
k Gkdk = 0.
(3.5.58)

162
CHAPTER 3. NEWTON’S METHODS
From (3.5.55) we have
Φ′
k(αk) −Φ′
k(0) −αkΦ′′
k(0) ≥−(1 −σ)[Φ′
k(0) + Φ′′
k(0)αk],
and hence
Φ′
k(αk) −Φ′
k(0) −αkΦ′′
k(0) ≥−(1 −σ)Φ′′
k(0)αk.
An application of the mean-value theorem yields that for some θk ∈(0, αk),
Φ′′
k(θk) −Φ′′
k(0) ≥−(1 −σ)Φ′′
k(0).
(3.5.59)
In the following, we prove (3.5.56) by contradiction. Suppose either the
ﬁrst equality or the second equality does not hold, then there is a subsequence
{ki} and η > 0 such that
−Φ′′
ki(0) ≥η > 0.
(3.5.60)
Hence (3.5.59) implies that {αki} does not converge to zero.
However, if
{αki} does not converge to zero and (3.5.60) holds, then (3.5.57) and (3.5.58)
cannot be satisﬁed. This contradiction establishes the theorem.
2
Furthermore, we have
Theorem 3.5.11 Let f : Rn →R have twice continuous derivatives on
the open set D, and assume that, for some x0 ∈D, the level set L(x0) =
{x ∈D | f(x) ≤f(x0)} is compact. In addition, assume that f has a ﬁnite
number of critical points in L(x0). Then, if {xk} is a sequence obtained by
the second-order step rule (3.5.50)-(3.5.52), we have
lim
k→∞xk = x∗, g(x∗) = 0, G(x∗) ≥0.
(3.5.61)
Moreover, if inﬁnitely many Gk ̸≥0, then G(x∗) has at least one eigenvalue
equal to zero.
Proof.
It is similar to the proof of Theorem 3.5.7.
2
Determine Descent Pair (sk, dk)
Finally, we mention a way to obtain the descent pair (sk, dk) which satisﬁes all
of the requirements of Theorem 3.5.10 and 3.5.11. First, consider computing
sk. Assume that
Gk = LkDkLT
k

3.6. INEXACT NEWTON’S METHOD
163
is the Bunch-Parlett symmetric indeﬁnite factorization where we omit the
permutations, Lk is a unit lower triangular matrix, Dk a block diagonal
matrix with 1 × 1 or 2 × 2 diagonal block. Let
Dk = UkΛkUT
k
be the spectral decomposition of Dk. Set
¯λ(k)
j
= max
*
|λ(k)
j |, ϵn max
1≤i≤n |λ(k)
i
|, ϵ
+
,
j = 1, · · · , n,
¯Λk = diag(¯λ(k)
1 , · · · , ¯λ(k)
n ),
where ϵ is the relative machine precision. Set
¯Dk = Uk ¯ΛkUT
k .
We obtain sk as the solution of
Lk ¯DkLT
k s = −gk.
Next, the negative curvature direction dk is obtained as the solution of
LT
k dk = ±| min{λ(Dk), 0}|
1
2 zk,
where λ(Dk) is the smallest eigenvalue of Dk and zk the corresponding unit
eigenvector of Dk. The other way to obtain a negative curvature direction
dk is to solve
LT
k dk = ±

λj(Dk)≤0
zj.
3.6
Inexact Newton’s Method
As mentioned before, the pure Newton’s method is expensive in each iter-
ation, especially when the dimension n is large. Also, the quadratic model
used to derive the Newton equation may not provide a good prediction of the
behavior of the function, especially when the iterate xk is remote from the
solution x∗. In this section, we consider a class of inexact Newton’s methods
in which we only approximately solve the Newton equation. In the following,
we discuss this class of methods for solving nonlinear equations F(x) = 0. It

164
CHAPTER 3. NEWTON’S METHODS
is not diﬃcult for readers to deal with unconstrained optimization problems
by using this way.
Consider solving the nonlinear equations
F(x) = 0,
(3.6.1)
where F : Rn →Rn is assumed to have the following properties:
A1 There exists x∗such that F(x∗) = 0.
A2 F is continuously diﬀerentiable in the neighborhood of x∗.
A3 F ′(x∗) is nonsingular.
Recall that the basic Newton’s step is obtained by solving
F ′(xk)sk = −F(xk)
(3.6.2)
and setting
xk+1 = xk + sk.
(3.6.3)
Now, we consider inexact Newton’s method: solve
F ′(xk)sk = −F(xk) + rk,
(3.6.4)
where
∥rk∥≤ηk∥F(xk)∥.
(3.6.5)
Set
xk+1 = xk + sk.
(3.6.6)
Here, rk = F ′(xk)sk+F(xk) denotes the residual, and {ηk} (with 0 < ηk < 1)
is a forcing sequence which controls the inexactness.
Next, we study the local convergence of inexact Newton’s methods.
Lemma 3.6.1 Let F : D ⊂Rn →Rn be continuously diﬀerentiable in a
neighborhood of x∗∈D, and let F ′(x∗) be nonsingular. Then there exist
δ > 0, ξ > 0, and ϵ > 0, such that when ∥y −x∗∥< δ and y ∈D, F ′(y) is
nonsingular and
∥F ′(y)−1∥≤ξ.
(3.6.7)
Also, F ′(y)−1 is continuous at x∗, that is
∥F ′(y)−1 −F ′(x∗)−1∥< ϵ.
(3.6.8)

3.6. INEXACT NEWTON’S METHOD
165
Proof.
Set α = ∥F ′(x∗)−1∥. For a given β < α−1, choose δ such that
when ∥y −x∗∥< δ with y ∈D,
∥F ′(x∗) −F ′(y)∥≤β.
It follows from Von-Neumann Theorem 1.2.5 that F ′(y) is invertible, and
(3.6.7) holds with ξ = α/(1 −βα). Thus,
∥F ′(x∗)−1 −F ′(y)−1∥
=
∥F ′(x∗)−1(F ′(y) −F ′(x∗))F ′(y)−1∥
≤
αξ∥F ′(x∗) −F ′(y)∥
≤
αβξ
∆=
ϵ,
which says that the continuity of F ′ guarantees the continuity of (F ′)−1.
2
In the following, we establish the linear convergence in Theorem 3.6.2 and
superlinear convergence in Theorem 3.6.4.
Theorem 3.6.2 Let F : Rn →Rn satisfy the properties (A1)–(A3). Assume
that the sequence {ηk} satisﬁes 0 ≤ηk ≤η < t < 1. Then, for some ϵ > 0, if
the starting point x0 is suﬃciently near x∗, the sequence {xk} generated by
inexact Newton’s method (3.6.4)–(3.6.6) converges to x∗, and the convergence
rate is linear, i.e.,
∥xk+1 −x∗∥∗≤t∥xk −x∗∥∗,
(3.6.9)
where ∥y∥∗= ∥F ′(x∗)y∥.
Proof.
Since F ′(x∗) is nonsingular, for y ∈Rn, we have
1
µ∥y∥≤∥y∥∗≤µ∥y∥,
(3.6.10)
where
µ = max{∥F ′(x∗)∥, ∥F ′(x∗)−1∥}.
(3.6.11)
Since η < t, there exists suﬃciently small γ > 0, such that
(1 + γµ)[η(1 + µγ) + 2µγ] ≤t.
(3.6.12)
Now choose ϵ > 0 suﬃciently small, such that if ∥y −x∗∥≤µ2ϵ, we have
∥F ′(y) −F ′(x∗)∥≤γ,
(3.6.13)
∥F ′(y)−1 −F ′(x∗)−1∥≤γ,
(3.6.14)
∥F(y) −F(x∗) −F ′(x∗)(y −x∗)∥≤γ∥y −x∗∥.
(3.6.15)

166
CHAPTER 3. NEWTON’S METHODS
Let ∥x0 −x∗∥≤ϵ. We now prove (3.6.9) by induction. By using (3.6.10)–
(3.6.11) and assumption of the induction, we have
∥xk −x∗∥
≤
µ∥xk −x∗∥∗≤µtk∥x0 −x∗∥∗
≤
µ2∥x0 −x∗∥≤µ2ϵ.
Then, when y = xk, (3.6.13)–(3.6.15) hold. Since
F ′(x∗)(xk+1 −x∗)
=
F ′(x∗)(xk −x∗−F ′(xk)−1F(xk) + F ′(xk)−1rk)
=
F ′(x∗)F ′(xk)−1[F ′(xk)(xk −x∗) −F(xk) + rk]
=
[I + F ′(x∗)(F ′(xk)−1 −F ′(x∗)−1)][rk + (F ′(xk) −F ′(x∗))(xk −x∗)
−(F(xk) −F(x∗) −F ′(x∗)(xk −x∗))],
(3.6.16)
by taking norms and using (3.6.11), (3.6.14), (3.6.5), (3.6.13) and (3.6.15),
we obtain
∥xk+1 −xk∥∗
≤
[1 + ∥F ′(x∗)∥∥F ′(xk)−1 −F ′(x∗)−1∥][∥rk∥+
∥F ′(xk) −F ′(x∗)∥∥xk −x∗∥+ ∥F(xk) −F(x∗) −F ′(x∗)(xk −x∗)∥]
≤
(1 + µγ)[ηk∥F(xk)∥+ γ∥xk −x∗∥+ γ∥xk −x∗∥].
(3.6.17)
Note that
F(xk) = [F ′(x∗)(xk −x∗)] + [F(xk) −F(x∗) −F ′(x∗)(xk −x∗)],
taking the norm gives
∥F(xk)∥≤∥xk −x∗∥∗+ γ∥xk −x∗∥.
(3.6.18)
Substituting (3.6.18) into (3.6.17) and using (3.6.10) and (3.6.12) yield
∥xk+1 −x∗∥∗
≤
(1 + µγ)[ηk(∥xk −x∗∥∗+ γ∥xk −x∗∥) + 2γ∥xk −x∗∥]
≤
(1 + µγ)[η(1 + µγ) + 2µγ]∥xk −x∗∥∗
≤
t∥xk −x∗∥∗.
2
Below, we discuss the superlinear convergence rate of the inexact New-
ton’s methods. We ﬁrst give a lemma.

3.6. INEXACT NEWTON’S METHOD
167
Lemma 3.6.3 Let
α = max{∥F ′(x∗)∥+ 1
2β , 2β},
where β = ∥F ′(x∗)−1∥. Then, for ∥y −x∗∥suﬃciently small, the inequality
1
α∥y −x∗∥≤∥F(y)∥≤α∥y −x∗∥
(3.6.19)
holds.
Proof.
From the continuous diﬀerentiability of F, we know that there
exists a suﬃciently small δ > 0, such that when ∥y −x∗∥< δ,
∥F(y) −F(x∗) −F ′(x∗)(y −x∗)∥≤1
2β ∥y −x∗∥
(3.6.20)
holds. Note that
F(y) = [F ′(x∗)(y −x∗)] + [F(y) −F(x∗) −F ′(x∗)(y −x∗)],
and take norms, then we have
∥F(y)∥
≤
∥F ′(x∗)∥∥y −x∗∥+ ∥F(y) −F(x∗) −F ′(x∗)(y −x∗)∥
≤
 
∥F ′(x∗)∥+ 1
2β
!
∥y −x∗∥
(3.6.21)
and
∥F(y)∥
≥
∥F ′(x∗)−1∥−1∥y −x∗∥−∥F(y) −F(x∗) −F ′(x∗)(y −x∗)∥
≥
 
∥F ′(x∗)−1∥−1 −1
2β
!
∥y −x∗∥
=
1
2β ∥y −x∗∥.
(3.6.22)
Combining (3.6.21) and (3.6.22) gives (3.6.19).
2
Theorem 3.6.4 Let the assumptions of Theorem 3.6.2 be satisﬁed. Assume
that the sequence {xk} generated by the inexact Newton’s method converges
to x∗, then, if and only if
∥rk∥= o(∥F(xk)∥), k →∞,
(3.6.23)
{xk} converges to x∗superlinearly.

168
CHAPTER 3. NEWTON’S METHODS
Proof.
Assume that {xk} converges to x∗superlinearly. Since
rk
=
F(xk) + F ′(xk)(xk+1 −xk)
=
[F(xk) −F(x∗) −F ′(x∗)(xk −x∗)] −[F ′(xk) −F ′(x∗)](xk −x∗)
+[F ′(x∗) + (F ′(xk) −F ′(x∗))](xk+1 −x∗),
taking norms and using property (A1)-(A3) and the superlinear convergence
property of {xk} yield
∥rk∥
≤
∥F(xk) −F(x∗) −F ′(x∗)(xk −x∗)∥+ ∥F ′(xk) −F ′(x∗)∥∥xk −x∗∥
+[∥F ′(x∗)∥+ ∥F ′(xk) −F ′(x∗)∥]∥xk+1 −x∗∥
=
o(∥xk −x∗∥) + o(1)∥xk −x∗∥
+[∥F ′(x∗)∥+ o(1)]o(∥xk −x∗∥).
(3.6.24)
Thus, by use of Lemma 3.6.3, we have, when k →∞, that
∥rk∥= o(∥xk −x∗∥) = o(∥F(xk)∥).
(3.6.25)
Conversely, assume that ∥rk∥= o(∥F(xk)∥). From (3.6.16), it follows
that
∥xk+1 −x∗∥
≤
(∥F ′(x∗)−1∥+ ∥F ′(xk)−1 −F ′(x∗)−1∥)(∥rk∥
+∥F ′(xk) −F ′(x∗)∥∥xk −x∗∥+ ∥F(xk) −F(x∗) −F ′(x∗)(xk −x∗)∥)
=
(∥F ′(x∗)−1∥+ o(1))(o(∥F(xk)∥) + o(1)∥xk −x∗∥+ o(∥xk −x∗∥)).
Therefore, we get from Lemma 3.6.3 that
∥xk+1 −x∗∥
=
o(∥F(xk)∥) + o(∥xk −x∗∥)
=
o(∥xk −x∗∥),
which shows the superlinear convergence of sequence {xk}.
2
The following corollary indicates that when {ηk} →0, the sequence {xk}
converges to x∗superlinearly.
Corollary 3.6.5 Assume that the sequence {xk} generated by inexact New-
ton’s method converges to x∗. Then, if sequence {ηk} converges to zero, the
sequence {xk} converges to x∗superlinearly.

3.6. INEXACT NEWTON’S METHOD
169
Proof.
If limk→∞ηk = 0, then
lim sup
k→∞
∥rk∥
∥F(xk)∥= 0,
which means that ∥rk∥= o(∥F(xk)∥). Then the conclusion is obtained from
Theorem 3.6.4.
2
There are several proofs of local convergence for the inexact Newton’s
method. Below, we give outlines of other proofs.
The outline of the second proof is as follows.
From (3.6.4)–(3.6.6), we have
xk+1 −x∗
=
xk −x∗−F ′(xk)−1F(xk) + F ′(xk)−1rk
=
F ′(xk)−1[F ′(xk)(xk −x∗) −F(xk) + F(x∗) + rk].
(3.6.26)
Taking norms, and using (3.6.7), (3.6.15) and Lipschitzian continuity of F(x),
i.e., ∥F(xk)∥= ∥F(xk) −F(x∗)∥≤L∥xk −x∗∥, we obtain
∥xk+1 −x∗∥
≤
ξ[γ∥xk −x∗∥+ ηkL∥xk −x∗∥]
≤
ξ(γ + ηkL)∥xk −x∗∥.
(3.6.27)
If we choose γ and ηk such that ξ(γ + ηkL) < 1, then {xk} converges to
x∗linearly. If we choose ηk →0 and note that γ is suﬃciently small, then
ξ(γ + ηkL) →0, and thus the sequence {xk} converges to x∗superlinearly.
The third proof is as follows.
Theorem 3.6.6 Let F : Rn →Rn satisfy the properties (A1)–(A3). Assume
that the sequence {ηk} satisﬁes 0 ≤ηk ≤η < 1. Then, for some ϵ > 0, if
the starting point x0 is suﬃciently near x∗, the sequence {xk} generated by
inexact Newton’s method (3.6.4)–(3.6.6) converges to x∗, and the convergence
rate is linear, i.e., for all k suﬃciently large,
∥xk+1 −x∗∥≤c∥xk −x∗∥
(3.6.28)
for some constant 0 < c < 1.
Furthermore, if ηk →0, then the sequence {xk} converges to x∗superlin-
early. If ηk = O(∥F(xk)∥), then the sequence converges to x∗quadratically.

170
CHAPTER 3. NEWTON’S METHODS
Proof.
From (3.6.4),
sk = F ′(xk)−1[−F(xk) + rk].
Taking norms and using (3.6.7) and (3.6.5), we obtain
∥sk∥≤ξ(∥F(xk)∥+ ∥rk∥) ≤ξ(1 + η)∥F(xk)∥≤2ξ∥F(xk)∥.
(3.6.29)
By using Taylor’s theorem, (3.6.4) and the above expression, we have
F(xk+1)
=
F(xk) + F ′(xk)sk + O(∥sk∥2)
=
rk + O(∥F(xk)∥2).
(3.6.30)
By taking norms and using (3.6.5), we get
∥F(xk+1)∥≤ηk∥F(xk)∥+ O(∥F(xk)∥2).
(3.6.31)
Dividing both sides by ∥F(xk)∥, passing to the lim sup, k →∞, noting that
ηk ≤η < 1, we deduce
lim sup
k→∞
∥F(xk+1)∥
∥F(xk)∥
≤η < 1.
(3.6.32)
By using Corollary 1.2.26 (or Lemma 3.6.3) we immediately obtain
lim sup
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
≤C lim sup
k→∞
∥F(xk+1)∥
∥F(xk)∥
(3.6.33)
for some constant C. When {xk} is suﬃciently close to x∗and Cη < 1, the
sequence {xk} converges to x∗locally and linearly.
Furthermore, if ηk →0, then
lim sup
k→∞
∥rk∥
∥F(xk)∥= 0,
i.e., ∥rk∥= o(∥F(xk)∥). By using (3.6.30) and taking norms, we have
lim sup
k→∞
∥F(xk+1)∥
∥F(xk)∥
= 0
(3.6.34)
which indicates the superlinear convergence in the function value sequence
{F(xk)}. It is easy to see that
lim sup
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0
(3.6.35)

3.6. INEXACT NEWTON’S METHOD
171
by use of Corollary 1.2.26 (or (3.6.19)).
If
ηk = O(∥F(xk)∥),
(3.6.36)
then there exists some constant c1 such that ηk ≤c1∥F(xk)∥.
By using
(3.6.5) we get that
lim sup
k→∞
∥rk∥
∥F(xk)∥2 ≤c1,
(3.6.37)
which shows that
rk = O(∥F ′(xk)∥2).
(3.6.38)
We have immediately from (3.6.30) that
lim sup
k→∞
∥F(xk+1)∥
∥F(xk)∥2 = c
(3.6.39)
for some constant c, which means quadratic convergence of {F(xk)}. And
therefore we have that
lim sup
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥2 = c.
2
(3.6.40)
It is easy to apply the above result to unconstrained optimization problem
minx∈Rn f(x). In fact, instead of (3.6.4)–(3.6.5), we use
∇2f(xk)sk = −∇f(xk) + rk,
(3.6.41)
where
∥rk∥≤ηk∥∇f(xk)∥,
(3.6.42)
and then we can get the same results for unconstrained optimization prob-
lems. Similar to the above discussion we have the following theorem.
Theorem 3.6.7 Suppose that ∇f(x) is continuously diﬀerentiable in a neigh-
borhood of a minimizer x∗, and assume that ∇2f(x∗) is positive deﬁnite.
Consider the iteration xk+1 = xk + sk, where sk is an inexact Newton step
satisfying (3.6.41) and (3.6.42).
Assume that the sequence {ηk} satisﬁes
0 ≤ηk ≤η < 1. Then, if the starting point x0 is suﬃciently near x∗, the
sequence {xk} converges to x∗linearly, i.e., for all k suﬃciently large,
∥xk+1 −x∗∥≤c∥xk −x∗∥
(3.6.43)
for some constant 0 < c < 1.
The sequence {xk} converges to x∗superlinearly if ∥rk∥= o(∥∇f(xk)∥).
The sequence {xk} converges to x∗quadratically if ∥rk∥= O(∥∇f(xk)∥2).

172
CHAPTER 3. NEWTON’S METHODS
About the implementation of the inexact Newton’s method, we can gen-
erate the search direction by applying the conjugate gradient method to the
Newton’s equation ∇2f(xk)sk = −∇f(xk), and then ask that the termination
test (3.6.42) be satisﬁed.
Inexact Newton’s method is an eﬃcient method, especially for large scale
nonlinear equations and optimization problems. Inexact Newton’s method
was due to Dembo, Eisenstat and Steihaug [83]. The other important works
about this method can be found in Steihaug [321], Dennis and Walker [99],
Ypma [365], and Nash [229].
Exercises
1. Let f(x) = 3
2x2
1+ 1
2x2
2−x1x2−2x1. Let the initial point x(0) = (−2, 4)T .
Minimize f(x) by use of the steepest descent method and Newton’s method,
respectively.
2. Let
(1) f(x) = 1
2(x2
1 + 9x2
2);
(2) f(x) = 1
2(x2
1 + 104x2
2).
Discuss the convergence rate of the steepest descent method.
3. Let f(x) = 1
2xT x + 1
4σ(xT Ax)2, where
A =
⎡
⎢⎢⎢⎣
5
1
0
1
2
1
4
1
2
0
0
1
2
3
0
1
2
0
0
2
⎤
⎥⎥⎥⎦.
Let (1) x(0) = (cos 70◦, sin 70◦, cos 70◦, sin 70◦)T ;
(2) x(0) = (cos 50◦, sin 50◦, cos 50◦, sin 50◦)T .
In the case of σ = 1 and σ = 104, discuss the numerical results and behavior
of convergence rate of pure Newton’s method and Newton’s method with line
search respectively.
4. Minimize the Rosenbrock function f(x) = 100(x2 −x2
1)2 + (1 −x1)2
by the steepest descent method and Newton’s method respectively, where
x(0) = (−1.2, 1)T , x∗= (1, 1)T , f(x∗) = 0.
5. By your opinion, state the reasons that the steepest descent method
converges slowly.

3.6. INEXACT NEWTON’S METHOD
173
6. Prove the convergence of the inexact Newton methods.

Chapter 4
Conjugate Gradient Method
In the preceding chapter we have discussed the steepest descent method and
the Newton method. In this chapter we introduce the conjugate gradient
method which is one between the steepest descent method and the Newton
method. The conjugate gradient method deﬂects the direction of the steepest
descent method by adding to it a positive multiple of the direction used in the
last step. This method only requires the ﬁrst-order derivatives but overcomes
the steepest descent method’s shortcoming of slow convergence. At the same
time, the method need not save and compute the second-order derivatives
which are needed by Newton method. In particular, since it does not require
the Hessian matrix or its approximation, it is widely used to solve large scale
optimization problems.
In this chapter, we will discuss the derivation, the properties, the al-
gorithm and numerical experiments, and the convergence of the conjugate
gradient method. Note that the restarting and preconditioning are very im-
portant to improve the conjugate gradient method. As a beginning, we ﬁrst
introduce the concept of conjugate directions and the conjugate direction
method.
4.1
Conjugate Direction Methods
One of the main properties of the conjugate gradient method is that its
directions are conjugate. Now, we ﬁrst introduce conjugate directions and
conjugate direction methods.

176
CHAPTER 4. CONJUGATE GRADIENT METHOD
Deﬁnition 4.1.1 Let G be an n × n symmetric and positive deﬁnite matrix,
d1, d2, · · · , dm ∈Rn be non-zero vectors, m ≤n. If
dT
i Gdj = 0, ∀i ̸= j,
(4.1.1)
the vectors d1, d2, · · · , dm are called G-conjugate or simply conjugate.
Obviously, if vectors d1, · · · , dm are G-conjugate, then they are linearly
independent. If G = I, the conjugacy is equivalent to the usual orthogonality.
A general conjugate direction method has the following steps:
Algorithm 4.1.2 (General Conjugate Direction Method)
Step 1. Given an initial point x0, ϵ > 0, k := 0. Compute g0 = g(x0);
Compute d0 such that dT
0 g0 < 0.
Step 2. If ∥gk∥≤ϵ, stop.
Step 3. Compute αk such that
f(xk + αkdk) = min
α≥0 f(xk + αdk).
Set xk+1 = xk + αkdk.
Step 4. Compute dk+1 by some conjugate direction method, such that
dT
k+1Gdj = 0, j = 0, 1, · · · , k.
Step 5. Set k := k + 1, go to Step 2.
2
The conjugate direction method is an important class of optimization
methods. The following theorem shows that, under exact line search, the
conjugate direction methods have quadratic termination property, which
means that the method terminates in at most n steps when it is applied
to a quadratic function with positive deﬁnite Hessian.
Theorem 4.1.3 (Principal Theorem of Conjugate Direction Method) For a
quadratic function with positive deﬁnite Hessian G, the conjugate direction
method terminates in at most n exact line searches. Each xi+1 is the min-
imizer in the subspace generated by x0 and the directions d0, · · · , di, that is
{x | x = x0 + 
i
j=0 αjdj}.

4.1. CONJUGATE DIRECTION METHODS
177
Proof.
Since G is positive deﬁnite and the conjugate directions d0, d1, · · ·
are linearly independent, it is enough to prove for all i ≤n −1 that
gT
i+1dj = 0, j = 0, · · · , i.
(4.1.2)
(Note that if (4.1.2) holds, we immediately have gT
n dj = 0, j = 0, · · · , n −1
and gn = 0, therefore xn is a minimizer.)
To prove (4.1.2), we consider two cases j < i and j = i. Keep in mind
that
yk
Def
= gk+1 −gk = G(xk+1 −xk) = αkGdk.
(4.1.3)
When j < i, by use of exact line search and the conjugacy, we have
gT
i+1dj
=
gT
j+1dj +
i

k=j+1
yT
k dj
=
gT
j+1dj +
i

k=j+1
αkdT
k Gdj
(4.1.4)
=
0.
When j = i, (4.1.2) is a direct result from the exact line search. Thus (4.1.2)
holds and we complete the proof.
2
This theorem is simple but important. All conjugate direction methods
rely on this theorem.
We reemphasize that, under exact line search, all
conjugate direction methods satisfy (4.1.2), and have quadratic termination
property. This shows that conjugacy plus exact line search implies quadratic
termination.
Figure 4.1.1 The gradient of conjugate direction method satisﬁes (4.1.2)

178
CHAPTER 4. CONJUGATE GRADIENT METHOD
4.2
Conjugate Gradient Method
4.2.1
Conjugate Gradient Method
In the conjugate direction method described in §4.1, there is not an explicit
procedure for generating a conjugate system of vectors d1, d2, · · ·.
In this
section we describe a method for generating mutually conjugate direction
vectors, which is theoretically appealing and computationally eﬀective. This
method is called the conjugate gradient method.
In conjugate direction methods, the conjugate gradient method is of par-
ticular importance.
Now it is widely used to solve large scale optimiza-
tion problems. The conjugate gradient method was originally proposed by
Hestenes and Stiefel [173] in the 1950s to solve linear systems. Since solv-
ing a linear system is equivalent to minimizing a positive deﬁnite quadratic
function, Fletcher and Reeves [138] in the 1960s modiﬁed it and developed
a conjugate gradient method for unconstrained minimization. By means of
conjugacy, the conjugate gradient method makes the steepest descent direc-
tion have conjugacy, and thus increases the eﬃciency and reliability of the
algorithm.
Now we derive the conjugate gradient method for the quadratic case.
Let
f(x) = 1
2xT Gx + bT x + c,
(4.2.1)
where G is an n × n symmetric positive deﬁnite matrix, b ∈Rn and c is a
real number. Obviously, the gradient of f(x) is
g(x) = Gx + b.
(4.2.2)
Set
d0 = −g0,
(4.2.3)
then we have
x1 = x0 + α0d0,
(4.2.4)
where α0 is generated by an exact line search. Then we have
gT
1 d0 = 0.
(4.2.5)
Set
d1 = −g1 + β0d0,
(4.2.6)

4.2. CONJUGATE GRADIENT METHOD
179
and choose β0 such that
dT
1 Gd0 = 0.
(4.2.7)
It follows from multiplying (4.2.6) by dT
0 G that
β0 = gT
1 Gd0
dT
0 Gd0
= gT
1 (g1 −g0)
dT
0 (g1 −g0) = gT
1 g1
gT
0 g0
.
(4.2.8)
In general, in the k-th iteration, set
dk = −gk +
k−1

i=0
βidi.
(4.2.9)
Choosing βi such that dT
k Gdi = 0, i = 0, 1, · · · , k −1, and noticing from
Theorem 4.1.3 that
gT
k di = 0, gT
k gi = 0, i = 0, 1, · · · , k −1,
(4.2.10)
it follows from multiplying (4.2.9) by dT
j G, (j = 0, 1, · · · , k −1) that
βj = gT
k Gdj
dT
j Gdj
= gT
k (gj+1 −gj)
dT
j (gj+1 −gj), j = 0, 1, · · · , k −1.
(4.2.11)
Then
βj
=
0, j = 0, 1, · · · , k −2,
(4.2.12)
βk−1
=
gT
k (gk −gk−1)
dT
k−1(gk −gk−1) =
gT
k gk
gT
k−1gk−1
.
(4.2.13)
The above derivation establishes the iterative scheme of the conjugate
gradient method:
xk+1
=
xk + αkdk,
(4.2.14)
dk
=
−gk + βk−1dk−1,
(4.2.15)
where
βk−1 =
gT
k gk
gT
k−1gk−1
, (F-R Formula)
(4.2.16)
and αk is an exact step size, in particular, for the quadratic case,
αk = −gT
k dk
dT
k Gdk
.
(4.2.17)

180
CHAPTER 4. CONJUGATE GRADIENT METHOD
The other famous formulas of βk are as follows:
βk−1
=
gT
k (gk −gk−1)
dT
k−1(gk −gk−1), (H-S or C-W Formula)
(4.2.18)
βk−1
=
gT
k (gk −gk−1)
gT
k−1gk−1
, (PRP Formula)
(4.2.19)
βk−1
=
−
gT
k gk
dT
k−1gk−1
, (Dixon Formula)
(4.2.20)
βk−1
=
gT
k gk
dT
k−1(gk −gk−1), (D-Y Formula)
(4.2.21)
where F-R, H-S (or C-W), PRP, Dixon and D-Y formula refer respectively
Fletcher-Reeves formula, Hestenes-Stiefel (or Crowder-Wolfe) formula, Polak-
Ribi`ere-Polyak formula, Dixon formula and Dai-Yuan Formula. It is easy to
see that these formulas are equivalent in the sense that all yield the same
search directions when used in minimizing a quadratic function with positive
deﬁnite Hessian matrix. However, for a general nonlinear function with inex-
act line search, their behavior is markedly diﬀerent. Some descriptions will
be given later in this subsection.
From (4.2.14)-(4.2.16), we can see that the conjugate gradient method
is only a little more complex than the steepest descent method, but it has
quadratic termination property and need not compute the Hessian or its
approximation matrix. Besides, we will learn below that the conjugate gra-
dient method has global convergence and n-step local quadratic convergence.
Hence this method is very attractive especially for large scale optimization
problems.
The following theorem includes the main properties of a conjugate gradi-
ent method.
Theorem 4.2.1 (Property theorem of conjugate gradient method) For posi-
tive deﬁnite quadratic function (4.2.1), the conjugate gradient method (4.2.14)-
(4.2.16) with exact line searches terminates after m ≤n steps, and the fol-
lowing properties hold for all i, (0 ≤i ≤m),
dT
i Gdj = 0, j = 0, 1, · · · , i −1,
(4.2.22)
gT
i gj = 0, j = 0, 1, · · · , i −1,
(4.2.23)
dT
i gi = −gT
i gi,
(4.2.24)

4.2. CONJUGATE GRADIENT METHOD
181
[g0, g1, · · · , gi] = [g0, Gg0, · · · , Gig0],
(4.2.25)
[d0, d1, · · · , di] = [g0, Gg0, · · · , Gig0].
(4.2.26)
where m is the number of distinct eigenvalues of G.
Proof.
We prove (4.2.22)–(4.2.24) by induction. For i = 1, it is trivial.
Suppose (4.2.22)–(4.2.24) hold for some i < m. We show that they also hold
for i + 1.
For quadratic function (4.2.1), we have obviously
gi+1 = gi + G(xi+1 −xi) = gi + αiGdi.
(4.2.27)
From (4.2.17), αi can be written as
αi =
gT
i gi
dT
i Gdi
̸= 0.
(4.2.28)
Using (4.2.27) and (4.2.15) gives
gT
i+1gj
=
gT
i gj + αidT
i Ggj
=
gT
i gj −αidT
i G(dj −βj−1dj−1).
(4.2.29)
When j = i, (4.2.29) becomes
gT
i+1gi = gT
i gi −gT
i gi
dT
i Gdi
dT
i Gdi = 0.
When j < i, (4.2.29) is zero directly by induction hypothesis. So, (4.2.23)
follows.
Now, from (4.2.15) and (4.2.27), it follows that
dT
i+1Gdj
=
−gT
i+1Gdj + βidT
i Gdj
=
gT
i+1(gj −gj+1)/αj + βidT
i Gdj.
(4.2.30)
When j = i, it follows from (4.2.30), (4.2.23), (4.2.28) and (4.2.16) that
dT
i+1Gdi = −gT
i+1gi+1
gT
i gi
dT
i Gdi + gT
i+1gi+1
gT
i gi
dT
i Gdi = 0.
When j < i, (4.2.30) is also zero from induction hypothesis. Then (4.2.22)
follows.

182
CHAPTER 4. CONJUGATE GRADIENT METHOD
Also, from (4.2.15) and the exact line search, we have
dT
i+1gi+1
=
−gT
i+1gi+1 + βidT
i gi+1
=
−gT
i+1gi+1,
which shows (4.2.24) holds for i + 1.
Finally, we show (4.2.25) and (4.2.26) by induction. It is trivial for i = 0.
Now suppose they hold for some i, and we prove that they hold also for i+1.
From the induction hypothesis, both gi and Gdi belong to
[g0, Gg0, · · · , Gig0, Gi+1g0].
Then it follows from (4.2.27) that gi+1 ∈[g0, Gg0, · · · , Gi+1g0]. Furthermore,
we need to show
gi+1 /∈[g0, Gg0, · · · , Gig0] = [d0, · · · , di].
In fact, since vectors d0, · · · , di are conjugate, it follows from Theorem 4.1.3
that gi+1 ⊥[d0, · · · , di]. If gi+1 ∈[g0, Gg0, · · · , Gig0] = [d0, · · · , di], then it
results in gi+1 = 0. This is a contradiction. Therefore (4.2.25) follows.
Similarly, by (4.2.15) and induction hypothesis, we can get (4.2.26).
2
In this theorem, (4.2.22)–(4.2.24) represent respectively conjugacy of di-
rections, orthogonality of gradients, and descent condition. (4.2.25)–(4.2.26)
give some relations between direction vectors and gradients. Usually, The
subspace [g0, Gg0, · · · , Gig0] is called the Krylov subspace.
Recall please the convergence rate (3.1.7), (3.1.8), and (3.1.9) of the steep-
est descent method for quadratic functions in Theorem 3.1.5. Similarly, for
quadratic functions, we can also obtain the following facts for the conjugate
gradient method:
Fact 1:
∥xk −x∗∥G
∥x0 −x∗∥G
≤
√κ −1
√κ + 1
k
,
(4.2.31)
where κ is the spectral condition number of G.
Fact 2:
starting from x1, the iterate xk+2 of the conjugate gradient
method after k + 1 iterations satisﬁes
E(xk+2) ≤
 λk+1 −λn
λk+1 + λn
!2
E(x1) =
 1 −λn/λk+1
1 + λn/λk+1
!2
E(x1),
(4.2.32)

4.2. CONJUGATE GRADIENT METHOD
183
where E(x) is deﬁned by
E(x) = 1
2(x −x∗)T G(x −x∗),
and the eigenvalues λi of G satisfy
λ1 ≥λ2 ≥· · · ≥λk ≥λk+1 ≥· · · ≥λn > 0.
Clearly, after the ﬁrst iteration (k = 0), the obtained iterate x2 satisﬁes
E(x2) ≤
 λ1 −λn
λ1 + λn
!2
E(x1)
which is the same as convergence rate (3.1.8) of the steepest descent method,
this is because, at the ﬁrst iteration, the direction of the conjugate gradient
method just is the steepest descent direction.
However, after the second
iteration (k = 1), we have
E(x3) ≤
 λ2 −λn
λ2 + λn
!2
E(x1).
At this time, the inﬂuence of the largest eigenvalue λ1 has been removed. The
formula (4.2.32) indicates that after each additional iteration of the conjugate
gradient method, the inﬂuence of one bigger eigenvalue will be removed.
Next, we would like to discuss restart strategy. Since the direction dk
after n steps is no longer conjugate for general non-quadratic functions, it is
suitable to reset periodically dk to the steepest descent direction, i.e., set
dcn = −gcn, c = 1, 2, · · · .
This strategy is called restart. With this strategy, the resultant xn−1 is nearer
to x∗than x0. Especially, when the iterate enters from an area in which
non-quadratic behavior is strong into a neighborhood in which a quadratic
model function approximates f(x) well, the restart method is able to con-
verge rapidly. For large scale problems, restart strategy will be used more
frequently, for example, every k iterations restart, where k < n, even k ≪n.
Notice that the restart conjugate gradient method permits inexact line
search. However, some control measures are needed so that the resultant
direction is descending. In fact, we have
gT
k dk = −gT
k gk + βk−1gT
k dk−1.
(4.2.33)

184
CHAPTER 4. CONJUGATE GRADIENT METHOD
If exact line search was used in the previous iteration, then gT
k dk−1 = 0, and
hence gT
k dk = −gT
k gk < 0 which guarantees that dk is a descent direction.
However, if inexact line search was used in the previous iteration, the quantity
βk−1gT
k dk−1 may be positive and larger than −gT
k gk, consequently −gT
k gk +
βk−1gT
k dk−1 is possibly larger than zero. In this case dk will not be a descent
direction. A typical remedy for such an eventuality is to restart the algorithm
with dk as the steepest descent direction −gk. However, frequently setting dk
to the steepest descent direction will lessen the eﬃciency of the algorithm,
and make the behavior of the algorithm incline to a steepest descent method.
This situation requires care. The following control measure can be used to
overcome this diﬃculty.
Let ¯gk+1, ¯dk+1 and ¯βk denote the computed values of gk+1, dk+1 and βk
at xk + αjdk respectively, where {αj} is a test step size sequence generated
from a step size algorithm. If
−¯gT
k+1 ¯dk+1 ≥σ∥¯gk+1∥2∥¯dk+1∥2,
(4.2.34)
where σ is a small positive number, then αj is accepted as αk. If (4.2.34) is
not satisﬁed at any trial points, we will use exact line search to produce αk.
The following algorithm is a restart conjugate gradient method with exact
line search.
Algorithm 4.2.2 (Restart F-R Conjugate Gradient Method)
Step 0. Given x0, ϵ > 0.
Step 1. Set k = 0. Compute g0 = g(x0).
Step 2. If ∥g0∥≤ϵ, stop; otherwise, set d0 = −g0.
Step 3. Compute step size αk, such that
f(xk + αkdk) = min
α≥0{f(xk + αdk)}.
Step 4. Set xk+1 = xk + αkdk, k := k + 1.
Step 5. Compute gk = g(xk). If ∥gk∥≤ϵ, stop; otherwise go to Step
6.
Step 6. If k = n, set x0 = xk, and go to Step 1; otherwise, go to
Step 7.

4.2. CONJUGATE GRADIENT METHOD
185
Step 7. Compute β = gT
k gk/gT
k−1gk−1, dk = −gk + βdk−1.
Step 8. If dT
k gk > 0, set x0 = xk, and go to Step 1; otherwise go to
Step 3.
2
(4.2.18)–(4.2.20) are common formulas of the conjugate gradient method.
The Fletcher-Reeves formula (4.2.16) is the ﬁrst presented in 1964 for solving
optimization problems and now is the most widely used in practice. However,
in general, this formula does not have the descent property and is often used
in conjunction with exact line search. Dixon’s formula (4.2.20) has descent
property. If we employ inexact line search
|gT
k+1dk| ≤−σgT
k dk, 0 < σ < 1,
Dixon’s formula satisﬁes
dT
k gk < 0, if gk ̸= 0.
The Polak-Ribiere-Polyak (PRP) formula (4.2.19) has a characteristic
that it can restart automatically. When the algorithm goes slowly and gk+1 ≈
gk, PRP formula will produce βk ≈0 and thus dk+1 ≈−gk+1. This indicates
that the algorithm has a tendency of restarting automatically, so that it
can overcome some shortcomings of going forward slowly. Various numerical
experiments show that PRP formula is more robust and eﬃcient than other
existing formulas for solving optimization problems.
4.2.2
Beale’s Three-Term Conjugate Gradient Method
Beale [10] considered the three-term conjugate gradient method. The idea is
as follows. When frequently periodic restarts with the steepest descent direc-
tion are used, the reduction at the restart iteration is often poor compared
with the reduction that would have occurred without restarting. However, if
the restart direction is taken as an arbitrary vector, the required conjugacy
relations may not hold. Now we consider restarting at xt, and take the di-
rection dt generated by the algorithm as the restarting direction to begin the
new cycle, and require the constructed sequence of directions to satisfy the
conjugacy.
Set
dt+1
=
−gt+1 + βtdt,
(4.2.35)
dk
=
−gk + γk−1dt + βt+1dt+1 + · · · + βk−1dk−1,
(4.2.36)

186
CHAPTER 4. CONJUGATE GRADIENT METHOD
where n + t −1 ≥k ≥t + 2. Similar to the derivation of the traditional
conjugate gradient method, by means of conjugacy between dt+1 and dt, dk
and dt, dt+1, · · · , dk−1, we can get the following relation:
βk−1 =
gT
k Gdk−1
dT
k−1Gdk−1
, γk−1 = gT
k Gdt
dT
t Gdt
,
βj = 0, j = t + 1, · · · , k −2.
Then (4.2.36) can be reduced as
dk = −gk + βk−1dk−1 + γk−1dt,
(4.2.37)
where
βk−1
=
gT
k (gk −gk−1)
dT
k−1(gk −gk−1),
(4.2.38)
γk−1
=
⎧
⎨
⎩
0,
if k = t + 1;
gT
k (gt+1−gt)
dT
t (gt+1−gt),
if k > t + 1.
(4.2.39)
Note that βk−1 in (4.2.38) can be represented as any formula in (4.2.18)-
(4.2.21), for example,
βk−1 =
gT
k gk
gT
k−1gk−1
which is F-R formula.
Note also that in Beale’s three-term formula (4.2.37), dk may not be a
descent direction, even if exact line searches are made. In order to make dk
be suﬃcient downhill and make two consecutive gradients not be far from
orthogonal, we may impose some control measures as follows,
−gT
k dk ≥σ∥gk∥∥dk∥,
(4.2.40)
where σ is a small positive number, and
|gT
k−1gk| < 0.2∥gk∥2.
(4.2.41)
Since the iterate xk generated from (4.2.37)-(4.2.39) is a minimizer of the
linear manifold
Bk−1
=
xt + [dt, dt+1, · · · , dk−1]
=
xt + [dt, gt+1, · · · , gk−1],

4.2. CONJUGATE GRADIENT METHOD
187
and hence
gk ⊥[dt, dt+1, · · · , dk−1]
and
gk ⊥[dt, gt+1, · · · , gk−1].
Below, we give Beale’s three-term conjugate gradient algorithm.
Algorithm 4.2.3 (Beale’s three-term CG method)
Step 1. Given x0, set k = 0, t = 0, evaluate g0 = g(x0). If ∥g0∥≤ϵ,
stop; otherwise set d0 = −g0.
Step 2. Compute αk by exact line search.
Step 3. Set xk+1 = xk + αkdk, set k := k + 1, evaluate gk = g(xk).
Step 4. If ∥gk∥≤ϵ, stop; otherwise go to Step 5.
Step 5. If both conditions
|gT
k−1gk| ≥0.2∥gk∥2
and
k −t ≥n −1
do not hold, go to Step 7; otherwise go to Step 6.
Step 6. Set t = k −1.
Step 7. Compute dk by (4.2.37)-(4.2.39).
Step 8. If k > t + 1, go to Step 9; otherwise go to Step 2.
Step 9 If
−1.2∥gk∥2 ≤dT
k gk ≤−0.8∥gk∥2,
go to Step 2; otherwise go to Step 6.
2

188
CHAPTER 4. CONJUGATE GRADIENT METHOD
4.2.3
Preconditioned Conjugate Gradient Method
In the discussion above we have known that if the conjugate gradient method
is applied to minimize the quadratic function
f(x) = 1
2xT Gx + bT x + c,
(4.2.42)
where G is symmetric and positive deﬁnite, it computes the solution of the
system
Gx = −b.
(4.2.43)
In this case, the algorithm is called the linear conjugate gradient method,
and the notation r is used for the gradient vector Gxk + b, which, in fact, is
the residual of the system (4.2.43).
The linear conjugate gradient method is as follows: given x0 and r0 =
Gx0 + b, β−1 = 0, d−1 = 0, and each iteration includes the following steps for
k = 0, 1, · · · :
dk = −rk + βk−1dk−1,
αk =
rT
k rk
dT
k Gdk
,
xk+1 = xk + αkdk,
(4.2.44)
rk+1 = rk + αkGdk,
βk = rT
k+1rk+1
rT
k rk
.
If exact arithmetic is used, the convergence of the linear conjugate gradient
method will be achieved in m(≤n) iterations, where m is the number of
distinct eigenvalues of G. If the eigenvalues of G are clustered into groups of
approximately equal value, the method may converge very quickly. However,
for general eigenvalue structure, due to rounding errors, considerably more
than n iterations may be required. Hence, the convergence rate depends on
the structure of eigenvalues of G and the condition number of G.
If the
original system is replaced by an equivalent system in which the conditioning
of G is improved, then the convergence rate can be improved. This technique
is called preconditioning.
Consider the transformation
x = C−1z,

4.2. CONJUGATE GRADIENT METHOD
189
where C is a nonsingular matrix. The solution of Gx = −b is equivalent to
solving the linear system
C−T GC−1z = −C−T b.
If we adequately choose C such that the condition number of C−T GC−1 is
as small as possible, the convergence rate of the algorithm will be improved.
Since C−T GC−1 is similar to W −1G, where W = CT C, it means that we
should choose W such that the condition number of W −1G is as small as
possible.
The preconditioned conjugate gradient method is as follows: given x0, set
g0 = Gx0 + b, and let v0 = W −1g0 and d0 = −v0. For k = 0, 1, · · ·,
αk =
gT
k vk
dT
k Gdk
,
(4.2.45)
xk+1 = xk + αkdk,
(4.2.46)
gk+1 = gk + αkGdk,
(4.2.47)
vk+1 = W −1gk+1,
(4.2.48)
βk = gT
k+1vk+1
gT
k vk
,
(4.2.49)
dk+1 = −vk+1 + βkdk.
(4.2.50)
The preconditioning matrix W can be deﬁned in several ways. The sim-
plest strategy is to choose W as the diagonal of G. In this case, the condition
number of W −1G is bounded by (1 + δ)/(1 −δ), where δ ≪1. The popular
strategy for preconditioning is use of incomplete Cholesky factorization. The
basic idea is as follows. Instead of computing the exact Cholesky factor L
which satisﬁes G = LLT , we compute an approximate factor ˜L which is more
sparse than L, such that G ≈˜L˜LT , and then choose C = ˜LT , and hence
W = ˜L˜LT ,
C−T GC−1 = ˜L−1G˜L−T ≈I
and
W −1G = (˜L˜LT )−1G ≈I.
In this procedure, any ﬁll-in during the sparse Cholesky factorization is dis-
carded.
The other preconditioning matrix can be obtained by performing a limited-
memory quasi-Newton method. From the quasi-Newton method (see Chap-
ter 5) the limited memory matrix M satisﬁes the quasi-Newton condition for

190
CHAPTER 4. CONJUGATE GRADIENT METHOD
r (r ≪n) pairs of vectors {sj, yj},
sj = Myj, j = 1, · · · , r,
where sj = xj+1 −xj, yj = gj+1 −gj. Since Gsj = yj, we have
sj = MGsj,
and the matrix MG has r unit eigenvalues with eigenvectors {sj}. Therefore,
M can be used as W −1.
For the minimization of a non-quadratic function, the preconditioning
matrix W is varied from iteration to iteration. In this case, we consider
x = C−1z,
(4.2.51)
and the objective function is transformed as
f(x) = f(C−1z) = ˜f(z).
(4.2.52)
Set
zk = Cxk, ˜gk = ∇˜f(zk) = C−T ∇f(xk) = C−T gk,
then
˜dk = Cdk, ˜sk = Csk, ˜yk = C−T yk.
So, application of conjugate gradient method, for example (4.2.18), to ˜f(z)
yields the direction
˜dk+1
=
−˜gk+1 + ˜gT
k+1(˜gk+1 −˜gk)
˜dT
k (˜gk+1 −˜gk)
˜dk
=
−

I −
˜dk˜yT
k
˜dT
k ˜yk

˜gk+1,
(4.2.53)
and hence
dk+1
=
−

I −dkyT
k
dT
k yk

W −1gk+1,
∆=
−Pk+1gk+1
(4.2.54)
which is the formula of the preconditioned conjugate gradient method, where
W = CT C. Similarly, we can obtain
dk+1 = −

I −
1
yT
k sk
(yksT
k + skyT
k ) +

1 + yT
k yk
yT
k sk

sksT
k
yT
k sk

W −1gk+1 (4.2.55)

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
191
which is the preconditioned conjugate gradient method in BFGS formula
without memory.
In general, the preconditioning matrix is varied with diﬀerent problems.
There is not a general-purpose formula for preconditioners.
4.3
Convergence of Conjugate Gradient Methods
As for the convergence results of the conjugate gradient method for mini-
mizing a general non-quadratic function, there have been various results. In
this section, we introduce global convergence results of conjugate gradient
methods due to Zoutendijk [385], Polyak [255] and Al-Baali [2] etc., and also
give in brief the outline of local convergence rates obtained by Cohen [61],
and McCormick and Ritter [205].
4.3.1
Global Convergence of Conjugate Gradient Methods
This subsection is divided into two parts. The ﬁrst part discusses the global
convergence of conjugate gradient methods with exact line search, and con-
sists of three theorems which state respectively global convergence of Fletcher-
Reeves (F-R) conjugate gradient method, Crowder-Wolfe (C-W) conjugate
gradient method, and Polak-Ribi`ere-Polyak (PRP) conjugate gradient method.
The second part discusses the global convergence of F-R conjugate gradient
method with inexact line search.
Now, we start the discussion by proving the global convergence result of
F-R method in the case of exact line search.
Theorem 4.3.1 ( Global convergence of F-R conjugate gradient method )
Suppose that f : Rn →R is continuously diﬀerentiable on a bounded level
set L = {x ∈Rn | f(x) ≤f(x0)}, and that F-R conjugate gradient method is
implemented with exact line search. Then the produced sequence {xk} has at
least one accumulation point which is a stationary point, i.e.,
(1) when {xk} is a ﬁnite sequence, then the ﬁnal point x∗is a stationary
point of f;
(2) when {xk} is an inﬁnite sequence, it has limit point, and any limit
point is a stationary point.
Proof.
(1) When {xk} is ﬁnite, from the termination condition, it fol-
lows that the ﬁnal point x∗satisﬁes ∇f(x∗) = 0, and hence x∗is a stationary
point of f.

192
CHAPTER 4. CONJUGATE GRADIENT METHOD
(2) When {xk} is inﬁnite, we have ∇f(xk) ̸= 0, ∀k. Noting that dk =
−gk + βk−1dk−1 and gT
k dk−1 = 0 by exact line search, we have
gT
k dk = −∥gk∥2 + βk−1gT
k dk−1 = −∥gk∥2 < 0,
(4.3.1)
which means that dk is a descent direction, {f(xk)} is a monotone descent
sequence, and thus {xk} ⊂L. Therefore {xk} is a bounded sequence and
must have a limit point.
Let x∗be a limit point of {xk}. Then there is a subsequence {xk}K1
converging to x∗, where K1 is an index set of a subsequence of {xk}. Since
{xk}K1 ⊂{xk}, {f(xk)}K1 ⊂{f(xk)}. It follows from the continuity of f
that for k ∈K1,
f(x∗) = f( lim
k→∞xk) = lim
k→∞f(xk) = f∗.
(4.3.2)
Similarly, {xk+1} is also a bounded sequence. Hence there exists a subse-
quence {xk+1}K2 converging to ¯x∗, where K2 is an index set of a subsequence
of {xk+1}. In this case,
f(¯x∗) = f( lim
k→∞xk+1) = lim
k→∞f(xk+1) = f∗.
(4.3.3)
Then
f(¯x∗) = f(x∗) = f∗.
(4.3.4)
Now we prove ∇f(x∗) = 0 by contradiction. Suppose that ∇f(x∗) ̸= 0,
then, for α suﬃciently small, we have
f(x∗+ αd∗) < f(x∗).
(4.3.5)
Since
f(xk+1) = f(xk + αkdk) ≤f(xk + αdk), ∀α > 0,
then for k ∈K2, passing to limit k →∞and using (4.3.5), we get
f(¯x∗) ≤f(x∗+ αd∗) < f(x∗),
(4.3.6)
which contradicts (4.3.4). This proves ∇f(x∗) = 0, i.e., x∗is a stationary
point of f.
2
Similarly, we can state the global convergence of Crowder-Wolfe (C-W)
restart conjugate gradient method with exact line search as follows.

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
193
Theorem 4.3.2 (Global convergence of Crowder-Wolfe conjugate gradient
method) Suppose that the level set L = {x ∈Rn | f(x) ≤f(x0)} is bounded,
and that ∇f(x) is Lipschitz continuous. Assume that Crowder-Wolfe conju-
gate gradient method is implemented with exact line search and restart strat-
egy. Then the produced sequence {xk} has at least one accumulation point
which is a stationary point.
Proof.
See Polyak [255].
2
As mentioned before, PRP method is more eﬃcient than F-R method.
We naturally hope PRP method has also the above property for a general
non-quadratic function. Unfortunately, the above Theorem 4.3.1 is not true
for PRP method (see Powell [270]). However, with stronger condition that f
is uniformly convex, the PRP method is globally convergent. The following
theorem states this result.
Theorem 4.3.3 Let f(x) be twice continuously diﬀerentiable and the level
set L = {x ∈Rn | f(x) ≤f(x0)} be bounded. Suppose that there is a constant
m > 0 such that for x ∈L,
m∥y∥2 ≤yT ∇2f(x)y, ∀y ∈Rn.
(4.3.7)
Then the sequence {xk} generated by PRP method with exact line search
converges to the unique minimizer x∗of f.
Proof.
From Theorem 2.2.4, we know that it is enough to prove that
(2.2.13) holds, that is, there exists a constant ρ > 0 such that
−gT
k dk ≥ρ∥gk∥∥dk∥,
(4.3.8)
which means
cos θk ≥ρ > 0.
Then, from Theorem 2.2.4, we have gk →0 and g(x∗) = 0. From (4.3.7), it
follows that {xk} →x∗which is a unique minimizer.
By using gT
k dk−1 = 0 and (4.2.15), we have
gT
k dk = −∥gk∥2.
Then (4.3.8) is equivalent to
∥gk∥
∥dk∥≥ρ.
(4.3.9)

194
CHAPTER 4. CONJUGATE GRADIENT METHOD
From (4.2.17) and (4.2.15), it follows that
αk−1 = −
gT
k−1dk−1
dT
k−1Gk−1dk−1
=
∥gk−1∥2
dT
k−1Gk−1dk−1
,
(4.3.10)
where
Gk−1 =
 1
0
G(xk−1 + tαk−1dk−1)dt.
(4.3.11)
By (4.3.11), the integral form of the mean-value theorem is
gk −gk−1 = g(xk−1 + αk−1dk−1) −g(xk−1) = αk−1Gk−1dk−1.
(4.3.12)
Then, by (4.3.11) and (4.3.10), (4.2.19) becomes
βk−1
=
gT
k (gk −gk−1)
gT
k−1gk−1
= αk−1
gT
k Gk−1dk−1
∥gk−1∥2
=
gT
k Gk−1dk−1
dT
k−1Gk−1dk−1
.
(4.3.13)
Since the level set L is bounded, there is a constant M > 0, such that
yT G(x)y ≤M∥y∥2, x ∈L, ∀y ∈Rn.
(4.3.14)
Then, by (4.3.13), (4.3.14) and (4.3.7), we have
|βk−1| ≤∥gk∥∥Gk−1dk−1∥
m∥dk−1∥2
≤M
m
∥gk∥
∥dk−1∥.
(4.3.15)
Therefore
∥dk∥
≤
∥gk∥+ |βk−1|∥dk−1∥
≤
∥gk∥+ M
m ∥gk∥
=
 
1 + M
m
!
∥gk∥,
(4.3.16)
which gives
∥gk∥
∥dk∥≥
 
1 + M
m
!−1
.
(4.3.17)
The above inequality shows that (4.3.9) holds. We complete the proof.
2

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
195
Next, we discuss the case of inexact line search. Al-Baali [2] studied the
F-R conjugate gradient method with strong Wolfe-Powell rule (2.5.3) and
(2.5.9), and proved the global convergence. The following theorem indicates
that, in the inexact case, the search direction dk satisﬁes descent property:
gT
k dk < 0.
Theorem 4.3.4 If, for all k, αk are determined by strong Wolfe-Powell rule
(2.5.3) and (2.5.9), then for F-R-CG method, the inequality
−
k

j=0
σj ≤gT
k dk
∥gk∥2 ≤−2 +
k

j=0
σj
(4.3.18)
holds for all k, and hence the descent property
gT
k dk < 0, ∀k
(4.3.19)
holds, as long as gk ̸= 0.
Proof.
The proof is by induction. For k = 0, d0 = −g0, σ0 = 1, hence
(4.3.18) and (4.3.19) hold for k = 0.
Now we suppose that (4.3.18) and (4.3.19) hold for any k ≥0. By (4.2.15)
and (4.2.16), we have
gT
k+1dk+1
∥gk+1∥2 = −1 + gT
k+1dk
∥gk∥2 .
(4.3.20)
Using (2.5.9) and induction assumption (4.3.19) yields
−1 + σ gT
k dk
∥gk∥2 ≤gT
k+1dk+1
∥gk+1∥2 ≤−1 −σ gT
k dk
∥gk∥2 .
(4.3.21)
Also, by induction assumption (4.3.18), we have
−
k+1

j=0
σj
=
−1 −σ
k

j=0
σj ≤gT
k+1dk+1
∥gk+1∥2
≤
−1 + σ
k

j=0
σj = −2 +
k+1

j=0
σj.
Then, (4.3.18) holds for k + 1.

196
CHAPTER 4. CONJUGATE GRADIENT METHOD
Since
gT
k+1dk+1
∥gk+1∥2 ≤−2 +
k+1

j=0
σj
(4.3.22)
and
k+1

j=0
σj <
∞

j=0
σj =
1
1 −σ,
(4.3.23)
where σ ∈(0, 1), it follows from 1 −σ > 1
2 that −2 + 
k+1
j=0 σj < 0. Hence,
from (4.3.22), we obtain gT
k+1dk+1 < 0. We complete the proof by induction.
2
Now, we are in a position to prove the global convergence of F-R-CG
algorithm with inexact line search.
Theorem 4.3.5 Let f be twice continuously diﬀerentiable, and the level set
L = {x ∈Rn | f(x) ≤f(x0)} be bounded. Suppose that the steplength αk
is determined by strong Wolfe-Powell rule (2.5.3) and (2.5.9), where 0 <
ρ < σ < 1
2. Then the sequence {xk} generated by F-R-CG method is globally
convergent, i.e.,
lim inf
k→∞∥gk∥= 0.
(4.3.24)
Proof.
By (2.5.9), (4.3.18) and (4.3.23), we have
|gT
k dk−1| ≤−σgT
k−1dk−1 ≤σ
k−1

j=0
σj∥gk−1∥2 ≤
σ
1 −σ∥gk−1∥2.
(4.3.25)
Also, by (4.2.15), (4.3.25) and (4.2.16), we obtain
∥dk∥2
=
∥gk∥2 −2βk−1gT
k dk−1 + β2
k−1∥dk−1∥2
≤
∥gk∥2 +
2σ
1 −σ∥gk∥2 + β2
k−1∥dk−1∥2
=
 1 + σ
1 −σ
!
∥gk∥2 + β2
k−1∥dk−1∥2.
(4.3.26)
By applying this relation repeatedly, it follows that
∥dk∥2 ≤
 1 + σ
1 −σ
!
∥gk∥4
⎛
⎝
k

j=0
∥gj∥−2
⎞
⎠,
(4.3.27)

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
197
where we used the facts that
β2
kβ2
k−1 · · · β2
k−i =
∥gk∥2
∥gk−i−1∥2 .
Now we prove (4.3.24) by contradiction. It assumes that (4.3.24) does
not hold, then there exists a constant ϵ > 0 such that
∥gk∥≥ϵ > 0
(4.3.28)
holds for all k suﬃciently large. Since gk is bounded above on the level set
L, it follows from (4.3.27) that
∥dk∥2 ≤c1k,
(4.3.29)
where c1 is a positive constant. From (4.3.18) and (4.3.23), we have
cos θk
=
−
gT
k dk
∥gk∥∥dk∥≥
⎛
⎝2 −
k

j=0
σj
⎞
⎠∥gk∥
∥dk∥
≥
 1 −2σ
1 −σ
! ∥gk∥
∥dk∥.
(4.3.30)
Since σ < 1
2, substituting (4.3.29) and (4.3.28) into (4.3.30) gives

k
cos2 θk ≥
 1 −2σ
1 −σ
!2 
k
∥gk∥2
∥dk∥2 ≥c2

k
1
k,
(4.3.31)
where c2 is a positive constant. Therefore, the series 
k cos2 θk is divergent.
Let M be an upper bound of ∥G(x)∥on the level set L, then
gT
k+1dk = (gk + αkG(xk)dk)T dk ≤gT
k dk + αkM∥dk∥2.
By using (2.5.9), i.e., σgT
k dk ≤gT
k+1dk ≤−σgT
k dk, we obtain
αk ≥−1 −σ
M∥dk∥2 gT
k dk.
(4.3.32)
Substituting αk of (4.3.32) into (2.5.3) gives
fk+1
≤
fk −(1 −σ)ρ
M

gT
k dk
∥dk∥
2
=
fk −c3∥gk∥2 cos2 θk,

198
CHAPTER 4. CONJUGATE GRADIENT METHOD
where c3 = (1−σ)ρ
M
> 0. Since f(x) is bounded below, 
k ∥gk∥2 cos2 θk con-
verges, which indicates that 
k cos2 θk converges by use of (4.3.28). This
fact contradicts (4.3.31). We complete the proof.
2
In the above theorem, the conclusion is also true if, instead of f being
twice continuously diﬀerentiable, the assumptions on f are changed: let f be
continuously diﬀerentiable and bounded below, and ∇f be Lipschitz contin-
uous.
To conclude the subsection, we give the global convergence of D-Y con-
jugate gradient method with Wolfe-Powell rule.
Theorem 4.3.6 Let x1 be a starting point, f(x) be continuously diﬀeren-
tiable and bounded below on the level set L, ∇f(x) satisfy the Lipschitz con-
dition on L. Let αk satisfy Wolfe-Powell rule (2.5.3) and (2.5.7). Then, for
all k,
gT
k dk < 0,
and further
lim inf
k→∞∥gk∥= 0.
Proof.
See Dai and Yuan [75].
2
4.3.2
Convergence Rate of Conjugate Gradient Methods
We have already seen that the conjugate gradient method has quadratic
termination, that is, for a convex quadratic function, the conjugate gradient
method with exact line search terminates after n iterations.
In (4.2.31) and (4.2.32), we give two formulas for convergence rate of
conjugate gradient method, from which we have seen that, for a quadratic
function, the rate of convergence of conjugate gradient methods is not worse
than that of the steepest descent method; that is, it is not worse than linear.
Furthermore, we can also have the following demonstration. For convenience,
we assume
f(x) = 1
2xT Gx,
(4.3.33)
where G is an n × n positive deﬁnite matrix. Clearly, the explicit expression
of steplength is
αk = −dT
k Gxk
dT
k Gdk
= −dT
k gk
dT
k Gdk
.
(4.3.34)

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
199
So, we can obtain
f(xk+1)
=
1
2xT
k+1Gxk+1
=
1
2(xk + αkdk)T G(xk + αkdk)
=
1
2xT
k Gxk −1
2
(gT
k dk)2
dT
k Gdk
.
(4.3.35)
In the case of the steepest descent (SD) method we have dk = −gk and thus
f(xk+1
SD ) = 1
2xT
k Gxk −1
2
∥gk∥4
gT
k Ggk
.
(4.3.36)
Whereas in the case of the conjugate gradient (CG) method, we have dk =
−gk + βk−1dk−1 and thus
f(xk+1
CG )
=
1
2xT
k Gxk −1
2
∥gk∥4
dT
k Gdk
(4.3.37)
=
f(xk) −1
2
∥gk∥4
dT
k Gdk
.
(4.3.38)
Since
dT
k Gdk
=
(−gk + βk−1dk−1)T G(−gk + βk−1dk−1)
=
gT
k Ggk + β2
k−1dk−1Gdk−1
≤
gT
k Ggk,
it follows that
f(xk+1
CG ) ≤f(xk+1
SD ).
(4.3.39)
The above discussion indicates again that the conjugate gradient method
reduces the value of f at least as much as the steepest descent method. Since
the steepest descent method has a linear convergence rate, we conclude that
conjugate gradient methods have convergence rates that are no worse than
the linear rate.
From (4.3.38) we also know that, for conjugate gradient
methods, the objective value is strictly decreasing. Similarly, the result is
true for the preconditioned conjugate gradient method and we have
f(xk+1
CG ) = f(xk) −1
2
(gT
k vk)2
dT
k Gdk
,
(4.3.40)

200
CHAPTER 4. CONJUGATE GRADIENT METHOD
where vk = W −1gk.
Note that the conjugate gradient method with exact line search can ﬁnd
the minimizer of a convex quadratic function in at most n iterations, which
corresponds to one step of Newton method.
Hence we can say that if n
iterations of the conjugate gradient method are regarded as a big iteration,
the conjugate gradient method should have a similar convergence rate as
Newton method. Cohen [61], Burmeister [39], and McCormick and Ritter
[205] studied the n-step quadratic convergence rate. We now state this result
without proof in the following theorem.
Assume that
(A1) f : Rn →R is three times continuously diﬀerentiable;
(A2) there exist constants M > m > 0 such that
m∥y∥2 ≤yT ∇2f(x)y ≤M∥y∥2, ∀y ∈Rn, x ∈L,
(4.3.41)
where L is a bounded level set.
Theorem 4.3.7 Assume that the conditions (A1) and (A2) are satisﬁed,
then the sequence {xk} generated by PRP-CG and F-R-CG restart methods
have n-step quadratic convergence rate, that is, there exists a constant c > 0,
such that
lim sup
k→∞
∥xkr+n −x∗∥
∥xkr −x∗∥2 ≤c < ∞,
(4.3.42)
where r means that the methods restart per r iterations.
Further, Ritter [287] shows that the convergence rate is n-step super-
quadratic, that is,
∥xk+n −x∗∥= o(∥xk −x∗∥2).
(4.3.43)
The other results on convergence rate of conjugate gradient methods can
consult Stoer [325].
Exercises
1. Let G be an n ×n symmetric positive deﬁnite matrix, p1, p2, · · · , pn be
n linearly independent vectors. Deﬁne
d1 = p1,
dk+1 = pk+1 −
k

i=1
pT
k+1Gdi
dT
i Gdi
di,
k = 1, 2, · · · , n −1.

4.3. CONVERGENCE OF CONJUGATE GRADIENT METHODS
201
Prove that {dk} are G-conjugate.
2. Using F-R conjugate gradient method minimize the following func-
tions:
(1) f(x) = x2
1 + 2x2
2 −2x1x2 + 2x2 + 2, the initial point x(0) = (0, 0)T .
(2) f(x) = (x1 −1)4 + (x1 −x2)2, the initial point x(0) = (0, 0)T .
3. Using respectively F-R conjugate gradient method and PRP conjugate
gradient method minimize the Rosenbrock function in Appendix 1.1 and Ex-
tended Rosenbrock function in Appendix 1.2.
4. Try to prove respectively that {dk} generated by PRP-CG method
and Dixon-CG method are conjugate.
5.
Derive the Beale three-term conjugate gradient formula (4.2.38)–
(4.2.39).
6. Let f(x) = 1
2xT Ax −bT x, where A is an n × n symmetric positive
deﬁnite matrix. Setting xk+1 = xk + αkdk and dk = −rk + βkdk−1, prove
(1) the exact step size αk = −rT
k dk
dT
k Adk ,
(2) βk =
rT
k Adk−1
dT
k−1Adk−1 .
7. Using the linear conjugate gradient method minimize function f(x) =
1
2xT Ax −bT x, where A is a Hilbert matrix A =

1
i+j−1

, b = (1, 1, · · · , 1)T ,
the initial point x(0) = 0. Try considering the cases of n = 5, 10, 20.

Chapter 5
Quasi-Newton Methods
5.1
Quasi-Newton Methods
We have seen that Newton’s method xk+1 = xk −G−1
k gk is successful because
it uses the Hessian which oﬀers the useful curvature information. However,
for various practical problems, the computing eﬀorts of the Hessian matrices
are very expensive, or the evaluation of the Hessian is diﬃcult, even the
Hessian is not available analytically. These lead to a class of methods that
only uses the function values and the gradients of the objective function and
that is closely related to Newton’s method. Quasi-Newton method is such a
class of methods which need not compute the Hessian, but generates a series
of Hessian approximations, and at the same time maintains a fast rate of
convergence.
Recall that in Chapter 3 the n-dimensional Newton’s method xk+1 =
xk −G−1
k gk comes from the one-dimensional Newton’s method. Can we get
any inspiration to the n-dimensional quasi-Newton method from the one-
dimensional method? The answer is positive.
In Chapter 2, for quadratic interpolation with two points (2.4.6), we use
interpolation condition (2.4.4) and obtain
αk+1 = αk −αk −αk−1
φ′
k −φ′
k−1
φ′
k.
(5.1.1)
If we set
bk = φ′
k −φ′
k−1
αk −αk−1
,
(5.1.2)

204
CHAPTER 5. QUASI-NEWTON METHODS
then (5.1.1) can be written as
αk+1 = αk −b−1
k φ′
k
(5.1.3)
which is also called the secant method.
Comparing with Newton’s form
αk+1 = αk −[φ′′
k]−1φ′
k indicates that here bk is used to approach φ′′
k without
computing φ′′
k. Also, the convergence rate of the secant method is 1+
√
5
2
≈
1.618 (see Theorem 2.4.1) which is fast.
Now we apply this idea to the
n-dimensional quasi-Newton method.
5.1.1
Quasi-Newton Equation
Instead of computing the Hessian Gk, we would like to construct Hessian
approximation, for example, Bk in the quasi-Newton method. We hope that
the sequence {Bk} possesses positive deﬁniteness, has the direction dk =
−B−1
k gk down, and behaves like Newton’s method. In addition, it is also
required that its computation is convenient. What conditions does such a
sequence {Bk} satisfy? How to form {Bk}? In this subsection, we ﬁrst reply
the ﬁrst question, and in the subsequent subsections we shall discuss the
formations of Bk.
Let f : Rn →R be twice continuously diﬀerentiable on an open set
D ⊂Rn. Let the quadratic approximation of f at xk+1 be
f(x) ≈f(xk+1) + gT
k+1(x −xk+1) + 1
2(x −xk+1)T Gk+1(x −xk+1),
(5.1.4)
where gk+1
∆= ∇f(xk+1) and Gk+1
∆= ∇2f(xk+1).
Finding the derivative
yields
g(x) ≈gk+1 + Gk+1(x −xk+1).
(5.1.5)
Setting x = xk, sk = xk+1 −xk and yk = gk+1 −gk, we get
G−1
k+1yk ≈sk.
(5.1.6)
Clearly, it is true that (5.1.6) holds exactly with equality for quadratic func-
tion f with the Hessian G, i.e.,
sk = G−1yk, or yk = Gsk.
(5.1.7)
Now we ask the produced inverse Hessian approximations Hk+1 in the quasi-
Newton method to satisfy this relation, i.e.,
Hk+1yk = sk,
(5.1.8)

5.1. QUASI-NEWTON METHODS
205
which is called the quasi-Newton equation or quasi-Newton condition, where
sk = xk+1 −xk, yk = gk+1 −gk.
(5.1.9)
In fact, if we consider the model function at xk+1,
mk+1(x) = f(xk+1)+gT
k+1(x−xk+1)+ 1
2(x−xk+1)T Bk+1(x−xk+1) (5.1.10)
which satisﬁes the interpolation conditions
mk+1(xk+1) = f(xk+1), ∇mk+1(xk+1) = gk+1,
(5.1.11)
where Bk+1 = H−1
k+1 is an approximation to the Hessian Gk+1. Instead of the
interpolation condition ∇2mk+1(xk+1) = Gk+1 in Newton’s method, we ask
the model (5.1.10) to satisfy
∇mk+1(xk) = gk,
(5.1.12)
that is
gk = gk+1 + Bk+1(xk −xk+1).
So we have
Bk+1(xk+1 −xk) = gk+1 −gk
or
Bk+1sk = yk
(5.1.13)
which is also the quasi-Newton equation expressed in Hessian approximation
form.
Premultiplying (5.1.13) by sT
k gives
sT
k Bk+1sk = sT
k yk.
It means that if
sT
k yk > 0,
(5.1.14)
the matrix Bk+1 is positive deﬁnite. Usually, (5.1.14) is called the curvature
condition.
The above discussion tells us that the key point of the quasi-Newton
method is to produce Hk+1 (or Bk+1) by use of some convenient methods
such that the quasi-Newton equation (5.1.8) (or (5.1.13)) holds. In general,
such an Hk+1 will be produced by updating Hk into Hk+1, which is our
topic in the subsequent subsections. Now we state a general quasi-Newton
algorithm below.

206
CHAPTER 5. QUASI-NEWTON METHODS
Algorithm 5.1.1 (A general quasi-Newton algorithm)
Step 1. Given x0 ∈Rn, H0 ∈Rn×n, 0 ≤ϵ < 1, k := 0.
Step 2. If ∥gk∥≤ϵ, stop.
Step 3. Compute
dk = −Hkgk.
(5.1.15)
Step 4. Find a step size αk > 0 by line search, and set xk+1 =
xk + αkdk.
Step 5. Update Hk into Hk+1 such that the quasi-Newton equation
(5.1.8) holds.
Step 6. k := k + 1 and go to Step 2.
2
In the above algorithm, it is common to start the algorithm with H0 = I,
an identity matrix or set H0 to be a ﬁnite-diﬀerence approximation to the
inverse Hessian G−1
0 . If H0 = I, the ﬁrst iteration is just a steepest descent
iteration. Sometimes, quasi-Newton method takes the form of Hessian ap-
proximation Bk. In this case, the Step 3 and Step 5 in Algorithm 5.1.1 have
the following forms respectively.
Step 3*. Solve
Bkd = −gk for dk.
(5.1.16)
Step 5*. Update Bk into Bk+1 so that quasi-Newton equation (5.1.13) holds.
Next, we give some comparisons with Newton’s method, which indicate
that the quasi-Newton method is advantageous.
Comparison of quasi-Newton method vs Newton’s method
quasi-Newton method
Newton’s method
Only need the function values
Need the function values,
and gradients
gradients and Hessians
{Hk} maintains positive deﬁnite
{Gk} is not sure to be
for several updates
positive deﬁnite
Need O(n2) multiplications
Need O(n3) multiplications
in each iteration
in each iteration

5.1. QUASI-NEWTON METHODS
207
As Newton’s method is a steepest descent method under the norm ∥·∥Gk,
the quasi-Newton method is a steepest descent method under the norm ∥·∥Bk,
where Bk is the approximation of the Hessian Gk. In fact, dk now is the
solution of the minimization problem
min
gT
k d
s.t.
∥d∥Bk ≤1.
(5.1.17)
From the inequality
(gT
k d)2 ≤(gT
k B−1
k gk)(dT Bkd),
it follows that when
dk = −B−1
k gk = −Hkgk,
gT
k dk is the smallest.
By the way, since the metric matrices Bk are positive deﬁnite and always
changed from iteration to iteration, the method is also called the variable
metric method.
5.1.2
Symmetric Rank-One (SR1) Update
As we have seen, the key point of the quasi-Newton method is to generate
Hk+1 (or Bk+1) by means of the quasi-Newton equation. This subsection
and the subsequent two subsections will discuss some typical and popular
quasi-Newton updates. In this subsection we introduce a simple rank-one
update that satisﬁes the quasi-Newton equation.
Let Hk be the inverse Hessian approximation of the k-th iteration. We
try updating Hk into Hk+1, i.e.,
Hk+1 = Hk + Ek,
(5.1.18)
where, usually, Ek is a matrix with lower rank. In the case of rank-one, we
have
Hk+1 = Hk + uvT ,
(5.1.19)
where u, v ∈Rn. By quasi-Newton equation (5.1.8), we obtain
Hk+1yk = (Hk + uvT )yk = sk,
that is
(vT yk)u = sk −Hkyk.
(5.1.20)

208
CHAPTER 5. QUASI-NEWTON METHODS
This indicates that u must be in the direction of sk −Hkyk. Assume that
sk −Hkyk ̸= 0 (otherwise, Hk has satisﬁed the quasi-Newton equation) and
that the vector v satisﬁes vT yk ̸= 0, then it follows from (5.1.19) and (5.1.20)
that
Hk+1 = Hk +
1
vT yk
(sk −Hkyk)vT .
(5.1.21)
Since the inverse Hessian approximation Hk is required to be symmetric, we
can set simply v = sk −Hkyk and get
Hk+1 = Hk + (sk −Hkyk)(sk −Hkyk)T
(sk −Hkyk)T yk
(5.1.22)
which is called the symmetric rank-one update (SR1 update).
By the way, (5.1.21) is a general Broyden rank-one update in which, par-
ticularly, if v = yk, (5.1.21) is called the Broyden rank-one update presented
by Broyden (1965) for solving systems of nonlinear equations.
The distinct property of SR1 update is its natural quadratic termination,
that is, for a quadratic function, it need not to do line search, but can be
terminated within n steps, i.e., Hn = G−1, where G is the Hessian of the
quadratic function. This fact is proved by Theorem 5.1.2 below.
Theorem 5.1.2 (Property Theorem of SR1 Update) Let s0, s1, · · · , sn−1 be
linearly independent. Then, for a quadratic function with a positive deﬁnite
Hessian, SR1 method terminates at n + 1 steps, that is, Hn = G−1.
Proof.
Let the Hessian G be positive deﬁnite. We can use
yk = Gsk, k = 0, 1, · · · , n −1,
(5.1.23)
that is shared by all proofs on quadratic termination.
First, by induction, we prove the hereditary property
Hiyj = sj, j = 0, 1, · · · , i −1.
(5.1.24)
For i = 1, it is trivial from (5.1.22). Now suppose it is true for i ≥1; we will
prove it holds for i + 1.
From (5.1.22), we have
Hi+1yj = Hiyj + (si −Hiyi)(si −Hiyi)T yj
(si −Hiyi)T yi
.
(5.1.25)

5.1. QUASI-NEWTON METHODS
209
When j < i, from the induction assumption and (5.1.23), we have
(si −Hiyi)T yj
=
sT
i yj −yT
i Hiyj
=
sT
i yj −yT
i sj
=
sT
i Gsj −sT
i Gsj
=
0.
Then
Hi+1yj = Hiyj = sj, j < i.
When j = i, it is a direct consequence from (5.1.22) that
Hi+1yi = si.
Therefore, (5.1.24) follows.
Furthermore, since
sj = Hnyj = HnGsj, j = 0, 1, · · · , n −1
and sj (j = 0, 1, · · · , n −1) are linearly independent, then HnG = I, that is
Hn = G−1.
2
It is not diﬃcult to ﬁnd that SR1 update has the following characteristics.
1. SR1 update possesses natural quadratic termination.
2. SR1 update satisﬁes the hereditary property: Hiyj = sj, j < i.
3. SR1 update does not retain the positive deﬁniteness of Hk.
If and
only if (sk −Hkyk)T yk > 0, SR1 update retains positive deﬁniteness.
However, this condition is diﬃcult to guarantee. The remedy is that
SR1 update can be used in the trust region framework since the trust
region method does not require positive deﬁniteness of the Hessian
approximations (see Chapter 6).
4. Sometimes, the denominator (sk−Hkyk)T yk is very small or zero, which
results in serious numerical diﬃculty or even the algorithm is broken.
This disadvantage restricts its applications. So, it is a topic deserving
research how to modify SR1 update such that it possesses not only
natural quadratic termination but also positive deﬁniteness. A special

210
CHAPTER 5. QUASI-NEWTON METHODS
skipping strategy to prevent the SR1 update from breaking down is as
follows. We use (5.1.22) only if
|(si −Hiyi)T yi| ≥r∥si −Hiyi∥∥yi∥,
(5.1.26)
where r ∈(0, 1); otherwise we set Hi+1 = Hi.
5. The SR1 update has a good behavior that it continues to generate good
Hessian approximations, which is stated in the following theorem.
Theorem 5.1.3 Let f be twice continuously diﬀerentiable, and its Hes-
sian be bounded and Lipschitz continuous in a neighborhood of a point
x∗. Let {xk} be a sequence of iterates with xk →x∗. Suppose that the
skipping rule (5.1.26) holds for all k, and the steps sk are uniformly
linearly independent. Then the matrix sequence {Bk} generated by SR1
update satisﬁes
lim
i→∞∥Hi −[∇2f(x∗)]−1∥= 0.
(5.1.27)
5.1.3
DFP Update
DFP update is another typical update which is a rank-two update, i.e., Hk+1
is formed by adding to Hk two symmetric matrices, each of rank one. Let us
consider the symmetric rank-two update
Hk+1 = Hk + auuT + bvvT ,
(5.1.28)
where u, v ∈Rn, a and b are scalars to be determined. By the quasi-Newton
equation (5.1.8),
Hkyk + auuT yk + bvvT yk = sk.
(5.1.29)
Clearly, u and v are not uniquely determined, but their obvious choices are
u = sk, v = Hkyk.
Then, from (5.1.29), we have
a = 1/uT yk = 1/sT
k yk, b = −1/vT yk = −1/yT
k Hkyk.
Therefore
Hk+1 = Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
.
(5.1.30)

5.1. QUASI-NEWTON METHODS
211
The formula (5.1.30) is the ﬁrst quasi-Newton update proposed originally by
Davidon [79] and developed later by Fletcher and Powell [137]. Hence it is
called DFP update.
Now we state the quasi-Newton algorithm with DFP update (in brief,
DFP method) as follows.
Algorithm 5.1.4 (DFP method)
Initial Step: Given x0 ∈Rn an initial point, H0 ∈Rn×n a symmetric and
positive deﬁnite matrix, ϵ > 0 a termination scalar, k := 0.
k-th Step: For k = 0, 1, · · ·,
1. If ∥gk∥≤ϵ, stop.
2. Compute dk = −Hkgk.
3. Compute the step size αk.
4. Set sk = αkdk, xk+1 = xk + sk, yk = gk+1 −gk, and
Hk+1 = Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
.
5. k := k + 1, go to Step 1.
2
DFP method has the following important properties:
1. For a quadratic function (under exact line search)
(1) DFP update has quadratic termination, i.e., Hn = G−1.
(2) DFP update has hereditary property, i.e., Hiyj = sj, j < i.
(3) DFP method generates conjugate directions; when H0 = I, the
method generates conjugate gradients.
2. For a general function
(1) DFP update maintains positive deﬁniteness.
(2) Each iteration requires 3n2 + O(n) multiplications.
(3) DFP method is superlinearly convergent.
(4) For a strictly convex function, under exact line search, DFP method
is globally convergent.

212
CHAPTER 5. QUASI-NEWTON METHODS
The convergence properties of DFP method will be established in §5.3
and §5.4.
In the remainder of this subsection we shall discuss the other
two important properties: positive deﬁniteness of the update and quadratic
termination of the method.
The fact that quasi-Newton update retains positive deﬁniteness is of im-
portance in eﬃciency, numerical stability and global convergence. If the Hes-
sian G(x∗) is positive deﬁnite, the stationary point x∗is a strong minimizer.
Hence, we hope Hessian approximation {Bk} (or inverse Hessian approxima-
tion {Hk}) is positive deﬁnite. In addition, if {Bk} (or {Hk}) is positive
deﬁnite, the local quadratic model of f has a unique local minimizer, and
the direction dk from (5.1.15) or (5.1.16) is a descent direction. Usually, the
update retaining positive deﬁniteness means that if Hk (or Bk) is positive
deﬁnite, then Hk+1 (or Bk+1) is also positive deﬁnite. Such an update is also
called positive deﬁnite update. Next, we discuss the positive deﬁniteness of
DFP update.
Theorem 5.1.5 (Positive Deﬁniteness of DFP Update)
DFP update (5.1.30) retains positive deﬁniteness if and only if sT
k yk > 0.
Proof.
For the proof, we give two methods.
Proof (I) Suﬃciency. We will prove
zT Hkz > 0, ∀z ̸= 0
(5.1.31)
by induction.
Obviously, H0 is symmetric and positive deﬁnite. We now suppose that
(5.1.31) holds for some k ≥0 and set Hk = LLT as the Cholesky factorization
of Hk. Let
a = LT z, b = LT yk.
(5.1.32)
Then by DFP update (5.1.30) we have
zT Hk+1z
=
zT

Hk −HkykyT
k Hk
yT
k Hkyk

z + zT sksT
k
sT
k yk
z
=

aT a −(aT b)2
bT b

+ (zT sk)2
sT
k yk
.
(5.1.33)
It is obvious from Cauchy-Schwartz inequality that
aT a −(aT b)2
bT b
≥0.
(5.1.34)

5.1. QUASI-NEWTON METHODS
213
In addition, the second term in (5.1.33) is also nonnegative because of sT
k yk >
0. Therefore we obtain that
zT Hk+1z ≥0.
Below, we must prove that at least one term in (5.1.33) is strictly larger
than zero.
Since z ̸= 0, the equality holds in (5.1.34) if and only if a is
parallel to b, equivalently, if and only if z is parallel to yk. If z is parallel to
yk, we have z = βyk, where β ̸= 0, and
(zT sk)2
sT
k yk
= β2sT
k yk > 0,
which indicates that if z is parallel to yk, i.e., if the ﬁrst term in (5.1.33)
equals zero, the second term must be strictly larger than zero. Thus, for any
z ̸= 0, we always have zT Hk+1z > 0. The suﬃciency follows.
In analogy, the necessity can be shown.
2
Proof (II). Let Hk = LLT , ¯y = LT yk, ¯s = L−1sk.
Then DFP update
(5.1.30) can be written as
Hk+1 = LWLT ,
(5.1.35)
where
W = I −¯y¯yT
¯yT ¯y + ¯s¯sT
¯sT ¯y.
(5.1.36)
By the determinant relation (1.2.70) of update,
det(W) = ¯sT ¯y
¯yT ¯y =
sT
k yk
yT
k Hkyk
,
which, together with (5.1.35), gives
det(Hk+1) = det(Hk) sT
k yk
yT
k Hkyk
.
(5.1.37)
This implies that if Hk is positive deﬁnite, then det(Hk+1) > 0 if and only if
sT
k yk > 0.
Let
Hk+1 = Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
= ¯H −HkykyT
k Hk
yT
k Hkyk
,

214
CHAPTER 5. QUASI-NEWTON METHODS
where ¯H = Hk + sksT
k
sT
k yk .
Since Hk is positive deﬁnite, we know by use of
Theorem 1.2.17 that sT
k yk > 0 implies all eigenvalues of ¯H are positive, i.e.,
¯H is positive deﬁnite. Using Theorem 1.2.17 again indicates that, at most,
the smallest eigenvalue of Hk+1 is not positive. Hence, det(Hk+1) and the
smallest eigenvalue of Hk+1 have the same sign, which shows that Hk+1 is
positive deﬁnite if and only if det(Hk+1) > 0. Therefore we have
sT
k yk > 0 ⇔det(Hk+1) > 0 ⇔Hk+1 is positive deﬁnite.
2
This theorem gives a suﬃcient and necessary condition of positive deﬁnite
DFP update. By diﬀerent deﬁnitions of positive deﬁniteness and diﬀerent
algebraic tricks, we can establish this theorem. The interested readers may
try diﬀerent methods to give the proofs. The curvature condition sT
k yk > 0 for
preserving positive deﬁniteness is moderate, practical, and can be satisﬁed.
For a quadratic positive deﬁnite function, obviously,
sT
k yk = sT
k Gsk > 0.
For a strong convex function, the average Hessian
¯Gk =
 1
0
∇2f(xk + τsk)dτ
(5.1.38)
is positive deﬁnite. So, from Taylor’s formula
yk = ∇f(xk + sk) −∇f(xk) =
 1
0
∇2f(xk + τsk)skdτ = ¯Gksk,
we have that
yT
k sk = sT
k ¯Gksk > 0.
For a general function, we have
sT
k yk = gT
k+1sk −gT
k sk.
Note that gT
k sk < 0 is due to sk being a descent direction. Using exact line
search with gT
k+1sk = 0, we have sT
k yk > 0. When we use inexact line search,
for example, if the rule (2.5.7) is satisﬁed, the condition sT
k yk > 0 can also
be satisﬁed. In general, as long as we increase the precision of line search,
we can make gT
k+1sk small enough in magnitude to the desired degree.
From this theorem and the above discussion, it is obvious that, for Algo-
rithm 5.1.4 with exact or inexact line search, the condition sT
k yk > 0 holds
and therefore each update matrix Hk in DFP algorithm is positive deﬁnite.
So, we have the following corollary.

5.1. QUASI-NEWTON METHODS
215
Corollary 5.1.6 Each matrix Hk generated by DFP Algorithm 5.1.4 is pos-
itive deﬁnite, and the directions dk = −Hkgk are descent directions.
Finally, we give a theorem on quadratic termination of DFP method. This
theorem shows that, for a quadratic function with positive deﬁnite Hessian
G, the directions generated from DFP method are conjugate, and the method
terminates at n steps, that is Hn = G−1.
Theorem 5.1.7 (Quadratic Termination Theorem of DFP Method)
Let f(x) be a quadratic function with positive deﬁnite Hessian G. Then,
if exact line search is used, the sequence {sj} generated from DFP method
satisﬁes hereditary property, conjugate property and quadratic termination,
that is, for i = 0, 1, · · · , m, where m ≤n −1,
1. Hi+1yj = sj, j = 0, 1, · · · , i; (hereditary property)
2. sT
i Gsj = 0, j = 0, 1, · · · , i −1; (conjugate direction property)
3. The method terminates at m + 1 ≤n steps. If m = n −1, then Hn =
G−1.
Proof.
We prove part (1) and (2) by induction. Clearly, when i = 0,
it is trivial. Now suppose that part (1) and (2) hold for some i. We show
that they also hold for i + 1. Since gi+1 ̸= 0, by exact line search, the fact
that yk = gk+1 −gk = G(xk+1 −xk) = Gsk, (1 ≤k ≤i) and the induction
hypothesis, we have, for j ≤i,
gT
i+1sj
=
gT
j+1sj +
i

k=j+1
(gk+1 −gk)T sj
=
gT
j+1sj +
i

k=j+1
yT
k sj
=
0 +
i

k=j+1
sT
k Gsj
=
0.
(5.1.39)
Hence, by use of si+1 = −αi+1Hi+1gi+1, induction hypothesis in part (1) and
(5.1.39), it follows that
sT
i+1Gsj
=
−αi+1gT
i+1Hi+1yj

216
CHAPTER 5. QUASI-NEWTON METHODS
=
−αi+1gT
i+1sj
=
0,
(5.1.40)
which proves part (2) holds for i + 1.
Next, we prove that part (1) holds for i + 1, i.e.,
Hi+2yj = sj, j = 0, 1, · · · , i + 1.
(5.1.41)
When j = i + 1, part (1) is immediate from DFP update (5.1.30), that is
Hi+2yi+1 = si+1.
(5.1.42)
When j ≤i, it follows from (5.1.40) and the induction hypothesis in part (1)
that
sT
i+1yj = sT
i+1Gsj = 0,
yT
i+1Hi+1yj = yT
i+1sj = sT
i+1Gsj = 0.
Then
Hi+2yj
=
Hi+1yj + si+1sT
i+1yj
sT
i+1yi+1
−Hi+1yi+1yT
i+1Hi+1yj
yT
i+1Hi+1yi+1
=
Hi+1yj
=
sj.
(5.1.43)
This, together with (5.1.42), shows (5.1.41). Therefore part (1) follows.
Finally, since si (i = 0, 1, · · · , m) are conjugate, the method is a conjugate
direction method. Based on Theorem 4.1.3 of the conjugate direction method,
the method terminates after m(≤n) steps. When m = n −1, since si (i =
0, 1, · · · , n −1) are linearly independent, then part (1) means
HnGsj = Hnyj = sj, j = 0, 1, · · · , n −1
which implies Hn = G−1.
2
From this theorem we see that DFP method is a conjugate direction
method. If the initial approximation H0 = I, the method becomes a con-
jugate gradient method. By the hereditary property, we have Hi+1Gsj =
sj, j = 0, 1, · · · , i, which also indicates that these sj are eigenvectors of ma-
trix Hi+1G(j = 0, 1, · · · , i) corresponding to the eigenvalue 1.
DFP method is a seminal quasi-Newton method and has been widely
used in many computer codes. It has played an important role in theoretical

5.1. QUASI-NEWTON METHODS
217
analysis and numerical computing. However, further studies indicate that
DFP method is numerically unstable, and sometimes produces numerically
singular Hessian approximations. The other famous quasi-Newton update —
BFGS update introduced in the next subsection will overcome these draw-
backs and perform better than DFP update.
5.1.4
BFGS Update and PSB Update
In §5.1.1 we have seen that
Hk+1yk = sk and Bk+1sk = yk
(5.1.44)
are the quasi-Newton equations with respect to inverse Hessian approxima-
tion and Hessian approximation respectively.
Note that any approxima-
tion in (5.1.44) can be obtained from the other by means of exchanging
Hk+1 ↔Bk+1 and sk ↔yk. In analogy to the derivation of DFP update
(5.1.30) about Hk, we can get
B(BFGS)
k+1
= Bk + ykyT
k
yT
k sk
−BksksT
k Bk
sT
k Bksk
,
(5.1.45)
which is called BFGS update discovered independently by Broyden [27],
Fletcher [125], Goldfarb [153] and Shanno [304]. In fact, if one makes di-
rectly simple exchanges Hk ↔Bk and sk ↔yk, BFGS update (5.1.45) is just
obtained from DFP update (5.1.30). Thus, BFGS update is also said to be
a complement DFP update. Since Bksk = −αkgk and Bkdk = −gk, (5.1.45)
can also be written as
B(BFGS)
k+1
= Bk + gkgT
k
gT
k dk
+
ykyT
k
αkyT
k dk
.
(5.1.46)
By using twice the Sherman-Morrison formula (1.2.67), (5.1.45) will be-
come as follows:
H(BFGS)
k+1
=
Hk +

1 + yT
k Hkyk
sT
k yk

sksT
k
sT
k yk
−skyT
k Hk + HkyksT
k
sT
k yk
(5.1.47)
=
Hk + (sk −Hkyk)sT
k + sk(sk −Hkyk)T
sT
k yk

218
CHAPTER 5. QUASI-NEWTON METHODS
−(sk −Hkyk)T yk
(sT
k yk)2
sksT
k
(5.1.48)
=

I −skyT
k
sT
k yk

Hk

I −yksT
k
sT
k yk

+ sksT
k
sT
k yk
.
(5.1.49)
(5.1.47)–(5.1.49) are the three forms of BFGS update about Hk. Further-
more, by making exchanges Hk ↔Bk and sk ↔yk in (5.1.47)–(5.1.49), we
can get three corresponding forms of DFP update about Bk:
B(DFP)
k+1
=
Bk +

1 + sT
k Bksk
yT
k sk

ykyT
k
yT
k sk
−yksT
k Bk + BkskyT
k
yT
k sk
(5.1.50)
=
Bk + (yk −Bksk)yT
k + yk(yk −Bksk)T
yT
k sk
−(yk −Bksk)T sk
(yT
k sk)2
ykyT
k
(5.1.51)
=

I −yksT
k
yT
k sk

Bk

I −skyT
k
yT
k sk

+ ykyT
k
yT
k sk
.
(5.1.52)
The above discussions describe a method for ﬁnding its dual update from
a given update. Given a quasi-Newton update Hk+1 about H-form, by ex-
changing Hk ↔Bk and sk ↔yk, we can get its dual update B(D)
k+1 about
B-form.
Then, applying the Sherman-Morrison formula to B(D)
k+1, we will
produce the dual update H(D)
k+1 of Hk+1 about the H-form. Similarly, if we
employ the same operations to the dual update H(D)
k+1, the original update
Hk+1 will be restored. Notice that, for an H-form, the dual update of Hk+1 is
H(D)
k+1. In addition, the dual operation maintains the quasi-Newton equation.
The following ﬁgure represents the dual relation.

5.1. QUASI-NEWTON METHODS
219
Figure 5.1.1 Duality of H(DFP)
k+1
and H(BFGS)
k+1
For SR1 update
H(SR1)
k+1
= Hk + (sk −Hkyk)(sk −Hkyk)T
(sk −Hkyk)T yk
,
(5.1.53)
exchanging Hk ↔Bk and sk ↔yk gives
B(D)
k+1 = Bk + (yk −Bksk)(yk −Bksk)T
(yk −Bksk)T sk
.
(5.1.54)
Then applying the Sherman-Morrison formula to (5.1.54), we see that the
resultant H(D)
k+1 is still the H(SR1)
k+1
, i.e., H(D)
k+1 = H(SR1)
k+1
. Thus SR1 update
is self-dual. As we pointed out in §5.1.2, SR1 update does not retain the
positive deﬁniteness of the update. A self-dual update retaining the positive
deﬁniteness is called Hoshino update which will be given in (5.2.6) of §5.2.
The BFGS update is presently considered to be the best one of all quasi-
Newton updates, which has all good properties of DFP update. In addition,
when inexact line search (2.5.3) and (2.5.7) are used, BFGS method is glob-
ally convergent. Note that it is still an open problem whether DFP update
has this property. The numerical performance of BFGS update is superior
to that of DFP update. In particular, BFGS update can often work well in
conjunction with some line searches with lower accuracy.
The next topic in this subsection is PSB update which is formally known
as the Powell-symmetric-Broyden update due to Powell [260].
Let B ∈Rn×n be a symmetric matrix. Consider the general Broyden
rank-one update
C1 = B + (y −Bs)cT
cT s
,

220
CHAPTER 5. QUASI-NEWTON METHODS
where c ∈Rn, cT s ̸= 0. In general, C1 is not symmetric. So, we consider a
symmetrization:
C2 = (C1 + CT
1 )/2.
Now C2 is symmetric but, in general, does not obey the quasi-Newton equa-
tion. Then we might continue the above process and generate the sequence
{Ck}:
C2k+1
=
C2k + (y −C2ks)cT
cT s
,
C2k+2
=
(C2k+1 + CT
2k+1)/2, k = 0, 1, · · ·
(5.1.55)
where C0 = B. Here each C2k+1 is the closest matrix in Q(y, s) to C2k, and
each C2k+2 is the closest symmetric matrix to C2k+1, where Q(y, s) = {C ∈
Rn×n | Cs = y} is a matrix set satisfying the quasi-Newton equation. The
Figure 5.1.2 illustrates the symmetrization process, where S denotes the set
of symmetric matrices.
Figure 5.1.2 Production of the sequence Ck
Below, we show the limit of matrix sequence {Ck} is
¯B = B + (y −Bs)cT + c(y −Bs)T
cT s
−(y −Bs)T s
(cT s)2
ccT
(5.1.56)
which satisﬁes symmetricity and the quasi-Newton equation.
Theorem 5.1.8 Let B ∈Rn×n be symmetric, c, s, y ∈Rn and cT s ̸= 0. Let
the sequence {Ck} be deﬁned by (5.1.55), and C0 = B. Then the sequence
{Ck} converges to ¯B in (5.1.56).

5.1. QUASI-NEWTON METHODS
221
Proof.
We only need to prove that the sequence {C2k} converges. Let
Gk = C2k. From (5.1.55), we have
Gk+1 = Gk + 1
2
wkcT + cwT
k
cT s
,
(5.1.57)
where wk = y −Gks. Note that
wk+1
=
y −Gk+1s
=
y −Gks −1
2
wkcT s + cwT
k s
cT s
=
1
2

I −csT
cT s

wk,
that is
wk+1 = Pwk, where P = 1
2

I −csT
cT s

.
(5.1.58)
Then it follows from Sherman-Morrison formula (1.2.67) that
∞

k=0
wk
=
∞

k=0
P k(y −G0s) =
∞

k=0
P k(y −Bs)
=
(I −P)−1(y −Bs) = 2

I −1
2
csT
cT s

(y −Bs)
=
2(y −Bs) −csT
cT s(y −Bs).
(5.1.59)
Since
lim
k→∞Gk = B +
∞

k=0
(Gk+1 −Gk),
(5.1.60)
and by (5.1.57) and (5.1.59), we get that the sequence {Gk} is convergent.
Note that
∞

k=0
(Gk+1 −Gk) = 1
2
∞

k=0
wkcT + cwT
k
cT s
=
1
cT s

(y −Bs)cT −1
2
sT (y −Bs)
cT s
ccT + c(y −Bs)T −1
2
(y −Bs)T s
cT s
ccT

=
1
cT s[(y −Bs)cT + c(y −Bs)T ] −(y −Bs)T s
(cT s)2
ccT ,
(5.1.61)

222
CHAPTER 5. QUASI-NEWTON METHODS
hence the conclusion (5.1.56) follows by (5.1.60) and (5.1.61).
2
(5.1.56) gives a class of rank-two update which is derived by a symmetriza-
tion process. If we add the subscripts, it can be written as
Bk+1
=
Bk + (yk −Bksk)cT
k + ck(yk −Bksk)T
cT
k sk
−(yk −Bksk)T sk
(cT
k sk)2
ckcT
k ,
(5.1.62)
which is called the general PSB update. In particular,
If ck = yk −Bksk, (5.1.62) is SR1 update (5.1.54).
If ck = yk, (5.1.62) is DFP update (5.1.51).
If ck =
1
wk+1yk +
wk
wk+1Bksk, where wk = (yT
k sk/sT
k Bksk)
1
2 , (5.1.62) is BFGS
update (5.1.46).
If ck = sk, (5.1.62) is PSB update:
Bk+1
=
Bk + (yk −Bksk)sT
k + sk(yk −Bksk)T
sT
k sk
−(yk −Bksk)T sk
(sT
k sk)2
sksT
k .
(5.1.63)
Its dual update in H-form is
Hk+1
=
Hk + (sk −Hkyk)yT
k + yk(sk −Hkyk)T
yT
k yk
−(sk −Hkyk)T yk
(yT
k yk)2
ykyT
k
(5.1.64)
which is called Greenstadt update (see Greenstadt [163]).
PSB update (5.1.63) is important in theoretical research and practical
computing. However, the drawback that PSB update does not retain the
positive deﬁniteness of updates hurts its performance in computing. Fortu-
nately, the drawback can be avoided if we employ the trust region framework
with PSB update.

5.1. QUASI-NEWTON METHODS
223
5.1.5
The Least Change Secant Update
Various quasi-Newton updates obey the least change property which refers
to the Hk+1 (or Bk+1) being the minimum change to Hk (or Bk) consistent
with the quasi-Newton equation if the change Hk+1 −Hk (or Bk+1 −Bk)
is measured under some norm. This property is helpful to maintain some
information of the last iteration. By the way, by use of the property, we also
can derive quasi-Newton update.
Theorem 5.1.9 Let B ∈Rn×n, s, y ∈Rn and s ̸= 0. Then Broyden rank-
one update
¯B = B + (y −Bs)sT
sT s
(5.1.65)
is a unique solution of the minimization problem
min{∥ˆB −B∥F : ˆBs = y}.
(5.1.66)
Proof.
[proof I] Since y = ˆBs, then
∥¯B −B∥
=

(y −Bs)sT
sT s

F
=
( ˆB −B)ssT
sT s

F
≤
∥ˆB −B∥F .
(5.1.67)
Also, since the Frobenius norm is strictly convex and the set of matrix ˆB
satisfying the quasi-Newton equation is convex, then the solution of (5.1.66)
is unique.
[proof II] Deﬁne C = ˆB−B and let cT
i be the i-th row of C. Then (5.1.66)
can be represented as
min
n

i=1
∥cT
i ∥2
2
s.t.
cT
i s = (y −Bs)i, i = 1, · · · , n
(5.1.68)
where (y −Bs)i denotes the i-th component of y −Bs. Obviously, (5.1.68)
can be divided into n subproblems
min
∥cT
i ∥2
2
s.t.
cT
i s = (y −Bs)i.
(5.1.69)

224
CHAPTER 5. QUASI-NEWTON METHODS
Solving (5.1.69) is equivalent to ﬁnding the Moore-Penrose inverse s+ of s.
Therefore
cT
i = (y −Bs)is+ = (y −Bs)isT
sT s
which indicates that (5.1.65) is the unique solution of (5.1.66).
2
This theorem shows that Broyden’s rank-one update
Bk+1 = Bk + (yk −Bksk)sT
k
sT
k sk
(5.1.70)
is the unique solution of the minimization problem
min{∥ˆB −Bk∥F : ˆBsk = yk}.
(5.1.71)
Similarly,
Hk+1 = Hk + (sk −Hkyk)yT
k
yT
k yk
(5.1.72)
is the unique solution of the minimization problem
min{∥ˆH −Hk∥F :
ˆHyk = sk}.
(5.1.73)
Next, we discuss the least change property of general symmetric rank-two
update.
Theorem 5.1.10 Let B ∈Rn×n be symmetric, c, s, y ∈Rn, and cT s > 0.
Assume that M ∈Rn×n is a symmetric and nonsingular matrix satisfying
Mc = M−1s.
(5.1.74)
Then the general PSB update
¯B = B + (y −Bs)cT + c(y −Bs)T
cT s
−(y −Bs)T s
(cT s)2
ccT
(5.1.75)
is the unique solution of the minimization problem
min{∥ˆB −B∥M,F : ˆBs = y, ˆBT = ˆB},
(5.1.76)
where ∥B∥M,F = ∥MBM∥F .

5.2. THE BROYDEN CLASS
225
Proof.
Let ˆB be a symmetric matrix obeying y = ˆBs. Let also Mc =
M−1s = z, E = M( ˆB −B)M, ¯E = M( ¯B −B)M. Left- and right-multiplying
(5.1.75) by M yields
¯E = EzzT + zzT E
zT z
−zT Ez
(zT z)2 zzT .
Clearly, ∥¯Ez∥2 = ∥Ez∥2, and if v ⊥z, then ∥¯Ev∥2 ≤∥Ev∥2. Therefore
∥¯E∥F ≤∥E∥F .
Also, note that the weighted Frobenius norm ∥· ∥M,F is
strictly convex and the matrix set { ˆB | ˆBs = y, ˆBT = ˆB} is convex, thus the
general PSB update (5.1.75) is the unique solution of the problem (5.1.76).
2
In particular, some diﬀerent choices of c in (5.1.75) give diﬀerent conclu-
sions.
Choosing c = s (in this case, M = I), we get PSB update (5.1.63). Hence,
Theorem 5.1.10 implies that ¯BPSB is the unique solution to the problem
min
ˆB∈Rn×n{∥ˆB −B∥F | ˆBs = y, ˆBT = ˆB}.
(5.1.77)
Choosing c = y (in this case, M satisﬁes M−2s = y), we get DFP update
(5.1.50). Hence Theorem 5.1.10 implies that ¯BDFP is the unique solution to
the problem
min{∥ˆB −B∥M,F | ˆBs = y, ˆBT = ˆB}.
(5.1.78)
Similarly, by the dual technique, ¯HBFGS in (5.1.47) is the unique solution
to the problem
min{∥ˆH −H∥M−1,F | ˆHy = s, ˆHT = ˆH}.
(5.1.79)
As an exercise, it is not diﬃcult to discuss the least change property of
dual general PSB update.
5.2
The Broyden Class
From the last section we have seen that both DFP and BFGS updates are
symmetric and positive deﬁnite rank-two updates consisting of Hkyk and sk.
It is natural to discuss their weighted (or convex) combinations which have
the same type, and consider their behaviors.

226
CHAPTER 5. QUASI-NEWTON METHODS
Consider the update class
Hφ
k+1 = (1 −φ)HDFP
k+1 + φHBFGS
k+1
,
(5.2.1)
where φ is a parameter. (5.2.1) is called the Broyden class of update. If
φ ∈[0, 1], (5.2.1) is called the Broyden convex class of update. Obviously,
Broyden class (5.2.1) satisﬁes quasi-Newton equation (5.1.8). We can also
write (5.2.1) in the following forms:
Hφ
k+1
=
HDFP
k+1 + φvkvT
k
(5.2.2)
=
HBFGS
k+1
+ (φ −1)vkvT
k
(5.2.3)
=
Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
+ φvkvT
k ,
(5.2.4)
where
vk = (yT
k Hkyk)
1
2

sk
sT
k yk
−
Hkyk
yT
k Hkyk

.
(5.2.5)
In particular, in (5.2.4),
set φ = 0, we get DFP update (5.1.30);
set φ = 1, we get BFGS update (5.1.47);
set φ =
sT
k yk
(sk−Hkyk)T yk , we get SR1 update (5.1.22);
set
φ =
1
1 ∓(yT
k Hkyk/sT
k yk),
(5.2.6)
we get Hoshino update.
Broyden class (5.2.2)–(5.2.4) can be derived directly by the quasi-Newton
equation. Consider a general rank-two update consisting of sk and Hkyk:
Hk+1 = Hk + asksT
k + b(HkyksT
k + skyT
k Hk) + cHkykyT
k Hk,
(5.2.7)
where a, b, c are scalars to be determined. Using the quasi-Newton equation
yields
1
=
asT
k yk + byT
k Hkyk,
0
=
1 + bsT
k yk + cyT
k Hkyk.
(5.2.8)

5.2. THE BROYDEN CLASS
227
Here are two equations with three unknowns and one free degree. Set
b = −φ/sT
k yk,
(5.2.9)
where φ is a parameter.
Solving (5.2.8) and substituting the result into
(5.2.7), we have
Hφ
k+1 = Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
+ φvkvT
k = HDFP
k+1 + φvkvT
k ,
where vk is deﬁned by (5.2.5). The above expression is just (5.2.2) and (5.2.4).
By a slight arrangement, Broyden class has the following matrix form:
Hφ
k+1 = Hk + [sk, Hkyk]
⎡
⎣
1+φyT
k Hkyk/sT
k yk
sT
k yk
−
φ
sT
k yk
−
φ
sT
k yk
φ−1
yT
k Hkyk
⎤
⎦[sk, Hkyk]T . (5.2.10)
Correspondingly, it is easy to produce Broyden class in B-form:
Bθ
k+1
=
θBDFP
k+1 + (1 −θ)BBFGS
k+1
(5.2.11)
=
BBFGS
k+1
+ θwkwT
k
(5.2.12)
=
BDFP
k+1 + (θ −1)wkwT
k
(5.2.13)
=
Bk + ykyT
k
sT
k yk
−BksksT
k Bk
sT
k Bksk
+ θwkwT
k ,
(5.2.14)
where
wk = (sT
k Bksk)1/2

yk
sT
k yk
−
Bksk
sT
k Bksk

.
(5.2.15)
Note that the relation between θ and φ is
θ = (φ −1)/(φ −1 −φµ),
(5.2.16)
where
µ = yT
k HkyksT
k Bksk
(sT
k yk)2
.
(5.2.17)
Since vT
k yk = 0 and wT
k sk = 0, then (5.2.1)-(5.2.2) and (5.2.11)-(5.2.14)
satisfy respectively the quasi-Newton equation (5.1.8) and (5.1.13) for any
parameter φ and θ. In analogous to Theorem 5.1.2 and Theorem 5.1.5, we
can show the quadratic termination property and positive deﬁnite property
of Broyden class.

228
CHAPTER 5. QUASI-NEWTON METHODS
Theorem 5.2.1 (Quadratic Termination Theorem of Broyden Class) Let
f(x) be a quadratic function with positive deﬁnite Hessian G. Then, when
exact line search is used, the Broyden class of update has hereditary property
and conjugate direction property, that is, for i = 0, 1, · · · , m, (m ≤n −1),
Hereditary property:
Hi+1yj = sj, j = 0, 1, · · · , i.
(5.2.18)
Conjugate direction:
sT
i Gsj = 0, j = 0, 1, · · · , i −1.
(5.2.19)
The method terminates at m steps. If m = n −1, then Hn = G−1.
Proof.
It is similar to the proof of Theorem 5.1.5.
2
Theorem 5.2.2 (Positive Deﬁniteness of Broyden Class of Update) Let φ ≥
0. If and only if sT
k yk > 0, Broyden class of update (5.2.2) retains the positive
deﬁniteness.
Proof.
From Theorem 5.1.2, if and only if sT
k yk > 0, DFP update retains
positive deﬁniteness. Since φ ≥0, it follows from (5.2.3) and Theorem 1.2.17
that the smallest eigenvalue of Hφ
k+1 is not less than the smallest one of
HDFP
k+1 . Hence Hφ
k+1 is positive deﬁnite.
2
This theorem shows that not all members of Broyden class retain the
positive deﬁniteness. Clearly, when φ ≥0, Hφ
k+1 maintains its positive deﬁ-
niteness; when φ < 0, it is possible that the update becomes singular. The
following Theorem 5.2.3 gives a value ¯φ, and says that as long as φ > ¯φ, Hφ
k+1
will maintain positive deﬁniteness. Such a value ¯φ is called the degenerate
value of Broyden class, which makes H
¯φ
k+1 singular.
Theorem 5.2.3 The degenerate value of Broyden class of update is
¯φ =
1
1 −µ =
1
1 −yT
k HkyksT
k Bksk/(sT
k yk)2 .
(5.2.20)
Proof.
Let dk = −Hkgk, sk = αkdk.
When we use exact line search,
gT
k+1dk = 0 = gT
k+1sk. Notice also that gk+1 = yk + gk, vT
k gk = 0, and using
(5.2.5), we have
dφ
k+1
=
−Hφ
k+1gk+1
=
−

Hk + sksT
k
sT
k yk
−HkykyT
k Hk
yT
k Hkyk
+ φvkvT
k

gk+1

5.2. THE BROYDEN CLASS
229
=
−Hkgk −Hkyk + yT
k Hk(gk + yk)
yT
k Hkyk
Hkyk −φvT
k gkvk
=
−Hkgk + yT
k Hkgk
yT
k Hkyk
Hkyk −φvT
k gkvk
=
dk −
yT
k dk
yT
k Hkyk
Hkyk −φvT
k gkvk
=
dT
k yk
(yT
k Hkyk)1/2

(yT
k Hkyk)1/2

dk
dT
k yk
−
Hkyk
yT
k Hkyk

−φvT
k gkvk
=

dT
k yk
(yT
k Hkyk)1/2 −φvT
k gk

vk.
(5.2.21)
This shows that when exact line search is used, (5.2.21) holds. When gk+1 ̸=
0, if dφ
k+1 = −Hφ
k+1gk+1 = 0, then φ is called the degenerate value of Hφ
k+1.
By using dφ
k+1 = 0 and (5.2.5), we obtain
φ
=
yT
k dk
(yT
k Hkyk)1/2vT
k gk
=
yT
k dk
−gT
k Hkyk + (sT
k gk)(yT
k Hkyk)/sT
k yk
=
1
1 −(sT
k Bksk)(yT
k Hkyk)
(sT
k yk)2
=
1
1 −µ.
2
(5.2.21) indicates that the parameter φ of Broyden class does not change
the search direction, but only the length. Hence, we could expect that: any
method of Broyden class is, in some degree, independent from the parameter
φ. Dixon [107] proves: under exact line search, all updates of Broyden class
(φk > ¯φ) generate the identical points, although for non-quadratic functions.
Theorem 5.2.4 Let f : Rn →R be continuously diﬀerentiable, the level set
L(x0) = {x | f(x) ≤f(x0)} be bounded, and H0 ∈Rn×n be symmetric and
positive deﬁnite. Let {Hφ
k } be a sequence generated by Broyden class, where
φk > ¯φ and ¯φ is the degenerate value of Broyden class. Assume that HBFGS
k+1
is an update obtained by applying BFGS update to Hφ
k . Then, under exact

230
CHAPTER 5. QUASI-NEWTON METHODS
line search, Broyden class of update has the property: for all k ≥0, xk+1 and
HBFGS
k+1
are independent from parameters φ0, φ1, · · · , φk−1.
Proof.
We show this result by induction. For k = 0, it is trivially true.
Now suppose it is true for k ≥0, i.e., xk+1 and HBFGS
k+1
are independent from
φ0, φ1, · · · , φk−1. We shall show it is also true for k + 1.
From (5.2.21), the direction dk+1 generated by Broyden class does not
depend on φk.
Since dk+1 ∝−HBFGS
k+1
gk+1, by the induction hypothesis,
the direction dk+1 does not also depend on φ0, φ1, · · · , φk−1. Then, by exact
line search, xk+2 = xk+1 +αk+1dk+1 does not depend on φ0, φ1, · · · , φk−1, φk.
Now, from the assumption,
HBFGS
k+2
=

I −sk+1yT
k+1
sT
k+1yk+1

Hφ
k+1

I −yk+1sT
k+1
sT
k+1yk+1

+ sk+1sT
k+1
sT
k+1yk+1
.
(5.2.22)
Note
Hφ
k+1 = HBFGS
k+1
+ (φk −1)vkvT
k .
(5.2.23)
Since

I −sk+1yT
k+1
sT
k+1yk+1

sk+1 = 0,
it follows from (5.2.21) that

I −sk+1yT
k+1
sT
k+1yk+1

vk = 0.
(5.2.24)
Then, substituting (5.2.23) into (5.2.22) and using (5.2.24) yield that HBFGS
k+2
can be deﬁned by use of HBFGS
k+1
, sk+1, and yk+1. So, by induction hypothesis,
HBFGS
k+2
is independent from φ0, φ1, · · · , φk. We complete the proof.
2
To conclude this section, we give a brief introduction to Huang class of
updates. Huang [180] presented a wider class of updates than Broyden class.
In Broyden class, the update matrix sequence {Hk} satisﬁes symmetricity
and quasi-Newton equation, i.e.,
HT
k = Hk and Hk+1yk = sk.
(5.2.25)
However, in Huang class, the symmetricity condition is removed, and the
update matrix {Hk} is required to obey
Hk+1yk = ρsk,
(5.2.26)

5.3. GLOBAL CONVERGENCE OF QUASI-NEWTON METHODS
231
which is said to be a generalized quasi-Newton equation or a generalized
quasi-Newton condition, where ρ is a parameter.
Huang class of updates can be described as follows:
Hk+1 = Hk + skuT
k + HkykvT
k ,
(5.2.27)
where uk and vk satisfy
uk = a11sk + a12HT
k yk,
(5.2.28)
vk = a21sk + a22HT
k yk,
(5.2.29)
uT
k yk = ρ,
(5.2.30)
vT
k yk = −1.
(5.2.31)
There are ﬁve parameters a11, a12, a21, a22 and ρ, in which three parameters
are free. Hence, in fact, Huang class of update depends on three parameters.
In particular, if requiring {Hk} symmetric and setting ρ = 1, then Huang
class is just Broyden class. This means that Broyden class is a subclass of
Huang class.
The main properties of Huang class of update are as follows:
• For positive deﬁnite and quadratic functions, Huang class generates
conjugate directions and has quadratic termination property. All meth-
ods of Huang class generate the identical points.
• For general functions, the sequence generated by Huang class only de-
pends on the parameter ρ.
Based on our experience, the generalized quasi-Newton equation (5.2.26)
is important to present a good quasi-Newton method. The parameter ρ will
play a big role on the iterative sequence and the properties of algorithms.
5.3
Global Convergence of Quasi-Newton Methods
In this section we discuss the global convergence for quasi-Newton methods.
The global properties of quasi-Newton methods were established by Pow-
ell [262] and Powell [265]. These results have been extended to restricted
Broyden’s class by Byrd, Nocedal and Yuan [47]. We will study the global
convergence of quasi-Newton methods under exact line search and inexact
line search respectively in §5.3.1 and §5.3.2.
In the discussion of this section, we need the following assumptions:

232
CHAPTER 5. QUASI-NEWTON METHODS
Assumption 5.3.1 (a) f : Rn →R is twice continuously diﬀerentiable on
convex set D.
(b) f(x) is uniformly convex, i.e., there exist positive constants m and M
such that for all x ∈L(x) = {x|f(x) ≤f(x0)}, which is convex, we
have
m∥u∥2 ≤uT ∇2f(x)u ≤M∥u∥2, ∀u ∈Rn.
(5.3.1)
The assumption (b) implies that ∇2f(x) is positive deﬁnite on L(x), and
that f has a unique minimizer x∗in L(x).
5.3.1
Global Convergence under Exact Line Search
We begin the discussion in case of exact line search.
Let
¯G =
 1
0
∇2f(xk + τsk)dτ,
(5.3.2)
then we have from Taylor’s theorem that
yk = ¯Gsk.
(5.3.3)
Immediately, we have
m ≤yT
k sk
∥sk∥2 = sT
k ¯Gksk
∥sk∥2
≤M
(5.3.4)
and
1
M ≤∥sk∥2
yT
k sk
≤1
m.
(5.3.5)
Since also
∥yk∥2
sT
k yk
= sT
k ¯G2
ksk
sT
k ¯Gksk
,
if we let zk = ¯G
1
2
k sk, then
∥yk∥2
sT
k yk
= zT
k ¯Gkzk
zT
k zk
≤M.
(5.3.6)
In addition, we have
∥yk∥≤∥¯G∥∥sk∥,
∥sk∥≤∥¯G−1
k ∥∥yk∥

5.3. GLOBAL CONVERGENCE OF QUASI-NEWTON METHODS
233
which give
∥yk∥
∥sk∥≤M
(5.3.7)
and
∥sk∥
∥yk∥≤1
m.
(5.3.8)
Therefore, from the above discussion, we have
Lemma 5.3.2 Let f : Rn →R satisfy Assumption 5.3.1. Then
∥sk∥
∥yk∥, ∥yk∥
∥sk∥, sT
k yk
∥sk∥2 , sT
k yk
∥yk∥2 , ∥yk∥2
sT
k yk
are bounded.
Lemma 5.3.3 Under exact line search, 
 ∥sk∥2 and 
 ∥yk∥2 are conver-
gent.
Proof.
Let ψ(τ) = f(xk+1 −τsk). From (5.3.1), it follows that ψ′′(τ) ≥
m∥sk∥2. Note that the exact line search gives ψ′(0) = 0. Then we have
ψ(τ) ≥ψ(0) + 1
2m∥sk∥2τ 2.
Taking τ = 1, we deduces
f(xk) −f(xk+1) ≥1
2m∥sk∥2.
By summing this expression we have
∞

k=0
∥sk∥2 ≤2{f(x0) −f(x∗)}/m,
which implies 
 ∥sk∥2 is convergent, where f(x∗) is the minimum of f(x).
By Lemma 5.3.2, we also obtain that 
 ∥yk∥2 is convergent.
2
Lemma 5.3.4 For all vectors x, the inequality
∥g(x)∥2 ≥m[f(x) −f(x∗)]
(5.3.9)
holds, where f(x∗) is the minimum of f(x).

234
CHAPTER 5. QUASI-NEWTON METHODS
Proof.
Since the function
ψ(τ) = f(x + τ(x∗−x)), (0 ≤τ ≤1)
is a convex function, then
f(x + τ(x∗−x)) ≥f(x) + τ(x∗−x)T g(x).
In particular, set τ = 1, then we have
f(x) −f(x∗) ≤−(x∗−x)T g(x) ≤∥g(x)∥∥x∗−x∥.
(5.3.10)
By (5.3.5) and Cauchy-Schwartz inequality, we deduce
∥x∗−x∥2
≤
(x∗−x)T (g(x∗) −g(x))/m
(5.3.11)
≤
∥x∗−x∥∥g(x∗) −g(x)∥/m,
which gives
∥x∗−x∥≤∥g(x∗) −g(x)∥/m = ∥g(x)∥/m.
(5.3.12)
Substituting (5.3.12) into (5.3.10) establishes (5.3.9).
2
Theorem 5.3.5 Suppose that f(x) satisﬁes Assumption 5.3.1. Then, under
exact line search, the sequence {xk} generated by DFP method converges to
the minimizer x∗of f.
Proof.
Consider DFP formula of inverse Hessian approximation
Hk+1 = Hk −HkykyT
k Hk
yT
k Hkyk
+ sksT
k
sT
k yk
(5.3.13)
and DFP formula of Hessian approximation
Bk+1 =

I −yksT
k
sT
k yk

Bk

I −skyT
k
sT
k yk

+ ykyT
k
sT
k yk
.
(5.3.14)
Obviously, Bk+1Hk+1 = I. By computing the trace of (5.3.14), we have
Tr(Bk+1) = Tr(Bk) −2sT
k Bkyk
sT
k yk
+ (sT
k Bksk)(yT
k yk)
(sT
k yk)2
+ yT
k yk
sT
k yk
.
(5.3.15)

5.3. GLOBAL CONVERGENCE OF QUASI-NEWTON METHODS
235
The middle two terms can be written as
−2sT
k Bkyk
sT
k yk
+ (sT
k Bksk)(yT
k yk)
(sT
k yk)2
=
αk

2gT
k yk
sT
k yk
+ (−gT
k sk)(yT
k yk)
(sT
k yk)2

=
αk
2gT
k yk + yT
k yk
sT
k yk
=
∥gk+1∥2 −∥gk∥2
gT
k Hkgk
.
(5.3.16)
Since gT
k+1sk = 0, then
gT
k+1Hk+1gk+1
=
gT
k+1

Hk −HkykyT
k Hk
yT
k Hkyk

gk+1
=
gT
k

Hk −HkykyT
k Hk
yT
k Hkyk

gk
=
gT
k

Hk −HkgkgT
k Hk
yT
k Hkyk

gk
=
(gT
k Hkgk)(gT
k+1Hkgk+1)
gT
k Hkgk + gT
k+1Hkgk+1
.
By ﬁnding the inverse number of the above expression, we get
1
gT
k+1Hk+1gk+1
=
1
gT
k+1Hkgk+1
+
1
gT
k Hkgk
.
(5.3.17)
Using (5.3.16) and (5.3.17), then (5.3.15) becomes
Tr(Bk+1)
=
Tr(Bk) +
∥gk+1∥2
gT
k+1Hk+1gk+1
−
∥gk∥2
gT
k Hkgk
−
∥gk+1∥2
gT
k+1Hkgk+1
+ ∥yk∥2
sT
k yk
.
(5.3.18)
By recurrence, we obtain
Tr(Bk+1)
=
Tr(B0) +
∥gk+1∥2
gT
k+1Hk+1gk+1
−
∥g0∥2
gT
0 H0g0
−
k

j=0
∥gj+1∥2
gT
j+1Hjgj+1
+
k

j=0
∥yj∥2
sT
j yj
.
(5.3.19)

236
CHAPTER 5. QUASI-NEWTON METHODS
Therefore, by Lemma 5.3.2, there exists a positive number M which is inde-
pendent of k, such that
Tr(Bk+1) ≤
∥gk+1∥2
gT
k+1Hk+1gk+1
−
k

j=0
∥gj+1∥2
gT
j+1Hjgj+1
+ Mk.
(5.3.20)
In the left part, we will prove that if the theorem does not hold, then the
sum of the last two terms in (5.3.20) is negative.
Now consider the trace of Hk+1. From (5.3.13), we have
Tr(Hk+1) = Tr(H0) −
k

j=0
∥Hjyj∥2
yT
j Hjyj
+
k

j=0
∥sj∥2
sT
j yj
.
(5.3.21)
Since Hk+1 is positive deﬁnite, the right-hand side of (5.3.21) is positive. By
Lemma 5.3.2, there exists m > 0 which is independent of k, such that
k

j=0
∥Hjyj∥2
yT
j Hjyj
< k
m.
(5.3.22)
Note that
(yT
j Hjyj)2 ≤∥Hjyj∥2∥yj∥2
(5.3.23)
and
yT
j Hjyj
=
gT
j+1Hjgj+1 + gT
j Hjgj + 2gT
j+1dj
>
gT
j+1Hjgj+1
(5.3.24)
by the positive deﬁniteness of Hj and exact line search, then by using (5.3.24),
(5.3.23) and (5.3.22) in turn, we obtain
k

j=0
gT
j+1Hjgj+1
∥yj∥2
≤
k

j=0
yT
j Hjyj
∥yj∥2
≤
k

j=0
∥Hjyj∥2
yT
j Hjyj
≤k
m.
(5.3.25)
By using Cauchy-Schwartz inequality and (5.3.25)
k

j=0
∥gj+1∥2
gT
j+1Hjgj+1
≥
⎛
⎝
k

j=0
∥gj+1∥
∥yj∥
⎞
⎠
2 , k

j=0
gT
j+1Hjgj+1
∥yj∥2
≥
m
k
⎛
⎝
k

j=0
∥gj+1∥
∥yj∥
⎞
⎠
2
.
(5.3.26)

5.3. GLOBAL CONVERGENCE OF QUASI-NEWTON METHODS
237
Now suppose that the theorem is not true, that is, there exists δ > 0 such
that for all suﬃciently large k,
∥gk∥≥δ.
(5.3.27)
Also, by (5.3.11) and Theorem 2.2.9, there exists a constant η > 0 such that
f(xk) −f(xk+1) ≥1
2η∥sk∥2,
which gives ∥sk∥→0 and further ∥yk∥→0. Then, by (5.3.26) and (5.3.27),
we deduce, for k suﬃciently large, that
k

j=0
∥gj+1∥2
gT
j+1Hjgj+1
> Mk.
(5.3.28)
The above inequality implies that the sum of the last two terms in (5.3.20)
is negative.
By (5.3.28) and (5.3.20), we immediately obtain
Tr(Bk+1) <
∥gk+1∥2
gT
k+1Hk+1gk+1
.
(5.3.29)
Note that, for a symmetric and positive deﬁnite matrix, the inverse of trace
is the lower bound of the least eigenvalue of inverse of the matrix. Then, it
follows from (5.3.29) that
gT
k+1Hk+1gk+1
∥gk+1∥2
< µ,
(5.3.30)
where µ is the lower bound of the least eigenvalue of Hk+1. However, from
Theorem 1.2.10 on the property of Rayleigh quotient, we have
gT
k+1Hk+1gk+1
∥gk+1∥2
> µ,
(5.3.31)
which contradicts (5.3.30). This contradiction proves that {xk} converges to
x∗and that our theorem holds.
2

238
CHAPTER 5. QUASI-NEWTON METHODS
5.3.2
Global Convergence under Inexact Line Search
Now, we turn to study the global convergence of BFGS method under inexact
line search.
Let us rewrite BFGS method as follows:
xk+1 = xk + sk = xk + αkdk = xk −αkB−1
k gk,
(5.3.32)
Bk+1 = Bk −BksksT
k Bk
sT
k Bksk
+ ykyT
k
sT
k yk
.
(5.3.33)
Theorem 5.3.6 Let x0 and B0 be a starting point and a symmetric positive
deﬁnite initial matrix, respectively. Suppose that f(x) satisﬁes Assumption
5.3.1. Then, under Wolfe-Powell inexact line search (2.5.3) and (2.5.7), the
sequence {xk} generated by BFGS method converges to the minimizer x∗of
f.
Proof.
By computing the trace and determinant of BFGS formula (5.3.33),
we obtain that
Tr(Bk+1) = Tr(Bk) −∥Bksk∥
sT
k Bksk
+ ∥yk∥2
yT
k sk
(5.3.34)
and
det(Bk+1) = det(Bk) yT
k sk
sT
k Bksk
.
(5.3.35)
Let us deﬁne
mk = yT
k sk
sT
k sk
, Mk = yT
k yk
yT
k sk
.
(5.3.36)
It follows from (5.3.4) and (5.3.6) that
m ≤mk ≤M, m ≤Mk ≤M.
(5.3.37)
Let us also deﬁne
cos θk =
sT
k Bksk
∥sk∥∥Bksk∥, qk = sT
k Bksk
sT
k sk
.
(5.3.38)
We then obtain that
∥Bksk∥2
sT
k Bksk
= ∥Bksk∥2∥sk∥2
(sT
k Bksk)2
sT
k Bksk
∥sk∥2
=
qk
cos2 θk
.
(5.3.39)

5.3. GLOBAL CONVERGENCE OF QUASI-NEWTON METHODS
239
In addition, we have from (5.3.36) that
det(Bk+1) = det(Bk)yT
k sk
sT
k sk
sT
k sk
sT
k Bksk
= det(Bk)mk
qk
.
(5.3.40)
Now we introduce the following function of a positive deﬁnite matrix Bk:
ψ(Bk) = Tr(Bk) −ln(det(Bk)),
(5.3.41)
where ln(·) denotes the natural logarithm. It is not diﬃcult to show that
ψ(Bk) > 0. By using (5.3.34)–(5.3.41), we have that
ψ(Bk+1)
=
Tr(Bk) + Mk −
qk
cos2 θk
−ln(det(Bk)) −ln mk + ln qk
=
ψ(Bk) + (Mk −ln mk −1)
+

1 −
qk
cos2 θk
+ ln
qk
cos2 θk

+ ln cos2 θk.
(5.3.42)
Note that the function h(t) = 1 −t + ln t ≤0 for all t > 0. Hence the term
inside the square brackets is nonpositive, and thus by summing both sides of
(5.3.42), we have
0 < ψ(Bk+1) ≤ψ(B1) + ck +
k

j=1
ln cos2 θj,
(5.3.43)
where the constant c = M −ln m −1 is assumed to be positive without loss
of generality.
From Theorem 2.5.5, we have
lim
k→∞∥gk∥cos θk = 0.
(5.3.44)
If θk is bounded away from 90◦, there is a positive constant δ such that
cos θk > δ > 0, for k suﬃciently large,
and thus we have our result.
Now assume, by contradiction, that cos θk →0. Then there exists k1 > 0
such that for all j > k1, we have
ln cos2 θj < −2c,

240
CHAPTER 5. QUASI-NEWTON METHODS
where c is the constant deﬁned above.
By using (5.3.43), we deduce, for all k > k1, that
0
<
ψ(B1) + ck +
k1

j=1
ln cos2 θj +
k

j=k1+1
(−2c)
=
ψ(B1) +
k1

j=1
ln cos2 θj + 2ck1 −ck
<
0,
which gives a contradiction. Therefore the assumption cos θj →0 is not true,
and there exists a subsequence {jk} such that
{cos θjk} ≥δ > 0,
which means
lim inf ∥∇f(xk)∥= 0.
(5.3.45)
Since the problem is strong convex, then (5.3.45) implies xk →x∗.
2
5.4
Local Convergence of Quasi-Newton Methods
In this section, we discuss local convergence of quasi-Newton methods. The
convergence analysis in this section mainly makes use of Broyden, Dennis, and
Mor´e [29], Dennis and Mor´e [91], Dennis and Mor´e [92], Nocedal and Wright
[233] and others. In §5.4.1 we ﬁrst consider solving F(x) = 0. The necessary
and suﬃcient condition of superlinear convergence for solving F(x) = 0 is
given in Theorem 5.4.3, which is basic and the most important in conver-
gence analysis for quasi-Newton methods. Theorem 5.4.4 is a corollary of
Theorem 5.4.3, and Lemma 5.4.5 gives the geometry of superlinear conver-
gence for quasi-Newton methods. Then, we generalize the above results to
minimization problems. We give superlinear convergence results in the case
of basic iteration, exact line search and inexact line search respectively in
Theorem 5.4.6, Theorem 5.4.7 and Theorem 5.4.8. In §5.4.2, we give linear
convergence of general quasi-Newton methods by means of the bounded de-
terioration principle. In §5.4.3 the linear and superlinear convergence of SR1
method is established. In §5.5.4, we discuss the linear convergence of DFP
method. In §5.4.5 and 5.4.6. we give the superlinear convergence results
of BFGS and DFP methods respectively by diﬀerent techniques. Finally, in
§5.4.7, the local convergence of Broyden’s class methods is discussed.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
241
5.4.1
Superlinear Convergence of General Quasi-Newton Meth-
ods
First, we consider
F(x) = 0,
(5.4.1)
where F : Rn →Rn is a mapping. In convergence analysis, we often need
the following assumption.
Assumption 5.4.1 (a) F : Rn →Rn is continuously diﬀerentiable on an
open convex set D ⊂Rn.
(b) There is x∗∈D with F(x∗) = 0 and F ′(x∗) nonsingular.
(c) F ′ is Lipschitzian at x∗, i.e., there is a constant γ such that
∥F ′(x) −F ′(x∗)∥≤γ∥x −x∗∥, x ∈D.
Second, we consider the minimization problem
min
x∈Rn f(x).
(5.4.2)
If in Assumption 5.4.1, we replace F(x) and F ′(x) by g(x) and ∇2f(x) re-
spectively, we get the following assumption for optimization problem (5.4.2):
Assumption 5.4.2 (a) f : Rn →R is twice continuously diﬀerentiable on
an open convex set D ⊂Rn.
(b) There is a strong local minimizer x∗∈D with ∇2f(x∗) symmetric
and positive deﬁnite.
(c) There is a neighborhood N(x∗, ε) of x∗such that
∥∇2f(¯x) −∇2f(x)∥≤γ∥¯x −x∥, ∀x, ¯x ∈N(x∗, ε).
Superlinear Convergence: Nonlinear System
We begin our discussion on a basic necessary and suﬃcient condition of
superlinear convergence for a nonlinear system.
Theorem 5.4.3 Let F : Rn →Rn satisfy (a) and (b) in Assumption 5.4.1.
Let {Bk} be a sequence of nonsingular matrices. Suppose, for x0 ∈D, that
the iterates generated by
xk+1 = xk −B−1
k F(xk)
(5.4.3)

242
CHAPTER 5. QUASI-NEWTON METHODS
remain in D. xk ̸= x∗(∀k ≥0). Suppose also that {xk} converges to x∗.
Then {xk} converges to x∗at a superlinear rate if and only if
lim
k→+∞
∥[Bk −F ′(x∗)](xk+1 −xk)∥
∥xk+1 −xk∥
= 0.
(5.4.4)
Proof.
Our idea is to prove the following equivalence:
lim
k→∞
∥[Bk −F ′(x∗)]sk∥
∥sk∥
= 0
⇔
lim
k→∞
∥F(xk+1)∥
∥sk∥
= 0
⇔
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0,
(5.4.5)
where sk = xk+1 −xk.
First, suppose (5.4.4) holds. By (5.4.3), we have
[Bk −F ′(x∗)](xk+1 −xk)
=
−F(xk) −F ′(x∗)(xk+1 −xk)
=
[F(xk+1) −F(xk) −F ′(x∗)(xk+1 −xk)] −F(xk+1).
(5.4.6)
By taking the norm, dividing by ∥sk∥, and using Theorem 1.2.24, we obtain
∥F(xk+1)∥
∥sk∥
≤∥(Bk −F ′(x∗))sk∥
∥sk∥
+ ∥F(xk+1) −F(xk) −F ′(x∗)sk∥
∥sk∥
≤∥(Bk −F ′(x∗))sk∥
∥sk∥
+ γ
2(∥xk −x∗∥+ ∥xk+1 −x∗∥).(5.4.7)
Since limk→∞xk = x∗, it follows from (5.4.4) that
lim
k→∞
∥F(xk+1)∥
∥sk∥
= 0.
(5.4.8)
Since also limk→∞∥sk∥= 0, we have
F(x∗) = lim
k→∞F(xk) = 0.
Noting that F ′(x∗) is nonsingular, it follows from Theorem 1.2.25 that there
is a β > 0 and k0 ≥0 such that ∀k ≥k0, we have
∥F(xk+1)∥= ∥F(xk+1) −F(x∗)∥≥β∥xk+1 −x∗∥.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
243
Thus
∥F(xk+1)∥
∥xk+1 −xk∥≥
β∥xk+1 −x∗∥
∥xk+1 −x∗∥+ ∥xk −x∗∥= β
rk
1 + rk
,
(5.4.9)
where
rk = ∥xk+1 −x∗∥
∥xk −x∗∥.
Combining (5.4.8) and (5.4.9) implies that
rk
1 + rk
→0
which gives
lim
k→∞rk = 0,
(5.4.10)
i.e., the sequence {xk} is convergent to x∗superlinearly.
Conversely, assume that {xk} converges superlinearly to x∗and F(x∗) =
0. By Theorem 1.2.25, there exist ¯β > 0 and k0 ≥0, such that ∀k ≥k0, we
have
∥F(xk+1)∥≤¯β∥xk+1 −x∗∥.
Since {xk} is convergent superlinearly, we have
0
=
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
≥lim
k→∞
∥F(xk+1)∥
¯β∥xk −x∗∥
=
lim
k→∞
1
¯β
∥F(xk+1)∥
∥xk+1 −xk∥
∥xk+1 −xk∥
∥xk −x∗∥.
By use of Theorem 1.5.2 giving limk→∞∥xk+1−xk∥/∥xk−x∗∥= 1, we obtain
lim
k→∞
∥F(xk+1)∥
∥xk+1 −xk∥= 0,
which gives (5.4.4) by means of (5.4.6).
2
Theorem 5.4.3 indicates that if Bk converges to F ′(x∗) along the direction
sk, then quasi-Newton methods converge superlinearly. This theorem is very
important in analysis of quasi-Newton methods. Equation (5.4.4) is called
the Dennis-Mor´e characterization of superlinear convergence. The following
theorem shows, for the iteration (5.4.11), that the method is convergent su-
perlinearly if and only if the sequence of steplength factors {αk} converges
to 1. The proof of Theorem 5.4.4 is completed by use of Theorem 5.4.3.

244
CHAPTER 5. QUASI-NEWTON METHODS
Theorem 5.4.4 Let F : Rn →Rn satisfy the assumptions of Theorem 5.4.3.
Let {Bk} be a sequence of nonsingular matrices. Suppose, for x0 ∈D, that
the iteration
xk+1 = xk −αkB−1
k F(xk)
(5.4.11)
remains in D and {xk} converges to x∗. If (5.4.4) holds, then {xk} converges
to x∗superlinearly and F(x∗) = 0 if and only if {αk} converges to 1.
Proof.
Necessity. Suppose that {xk} converges to x∗superlinearly and
F(x∗) = 0. By Theorem 5.4.3, we have
lim
k→∞
∥[α−1
k Bk −F ′(x∗)](xk+1 −xk)∥
∥xk+1 −xk∥
= 0.
(5.4.12)
So, (5.4.4) implies that
lim
k→∞∥(α−1
k
−1)Bk(xk+1 −xk)∥/∥xk+1 −xk∥= 0.
Since Bk(xk+1 −xk) = −αkF(xk), the above equality can be written as
lim
k→∞∥(αk −1)F(xk)∥/∥xk+1 −xk∥= 0.
(5.4.13)
Noting that F ′(x∗) is nonsingular, it follows from Theorem 1.2.25 that there
exists β > 0 such that ∥F(xk)∥≥β∥xk −x∗∥. Then, from (5.4.13), we obtain
lim
k→∞|αk −1| β∥xk −x∗∥
∥xk+1 −xk∥= 0.
(5.4.14)
Since also {xk} is convergent superlinearly, i.e., limk→∞∥xk+1 −xk∥/∥xk −
x∗∥= 1, we obtain immediately from (5.4.14) that {αk} →1.
Suﬃciency. Suppose that {αk} →1. It follows from (5.4.4) that (5.4.12)
holds. Therefore, from Theorem 5.4.3, we obtain that {xk} converges to x∗
superlinearly and F(x∗) = 0.
2
This theorem suggests that when a method is required to be superlinearly
convergent, we should ask for αk →1 as k →∞.
Next, we interpret the geometry of superlinear convergence of quasi-
Newton methods, which is an equivalent and geometric representation of
(5.4.4).
Let sk = xk+1−xk. Let also Newton’s iteration be sN
k = −F ′(xk)−1F(xk).
Since F(xk) = −Bksk, then
sk −sN
k = sk + F ′(xk)−1F(xk) = F ′(xk)−1[F ′(xk) −Bk]sk.
(5.4.15)

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
245
By use of Assumption 5.4.1, we have that ∥F ′(xk)−1∥is bounded above for
xk suﬃciently close to x∗. Thus,
F ′(xk)−1[F ′(xk) −Bk]sk = O(∥[F ′(xk) −Bk]sk∥) = o(∥sk∥),
where we have used (5.4.4). Therefore (5.4.15) is equivalent to
lim
k→∞
∥sk −sN
k ∥
∥sk∥
= 0.
(5.4.16)
The above (5.4.16) indicates that when {xk} converges superlinearly, the
relative error of sk should tend to zero.
It is not diﬃcult to prove that
(5.4.16) is equivalent to the fact that sk tends to sN
k in both direction and
length. For this, we introduce the following lemma.
Lemma 5.4.5 Let u, v ∈Rn, u, v ̸= 0, and α ∈(0, 1). If ∥u −v∥≤α∥u∥,
then ⟨u, v⟩is positive and
1 −∥v∥
∥u∥
 ≤α, 1 −
 ⟨u, v⟩
∥u∥∥v∥
!2
≤α2.
(5.4.17)
Conversely, if ⟨u, v⟩is positive and (5.4.17) holds, then
∥u −v∥≤3α∥u∥.
(5.4.18)
Proof.
First, assume that ∥u −v∥≤α∥u∥. Then

∥u∥−∥v∥
∥u∥
 ≤∥u −v∥
∥u∥
≤α,
which implies that the ﬁrst inequality in (5.4.17) holds.
Let ω = ⟨u, v⟩/(∥u∥∥v∥). Since
∥v∥2 −2⟨u, v⟩+ ⟨u, v⟩2
∥v∥2
=

∥v∥−⟨u, v⟩
∥v∥
2
≥0,
then
∥v∥2 −2⟨u, v⟩≥−⟨u, v⟩2
∥v∥2 .

246
CHAPTER 5. QUASI-NEWTON METHODS
So,
∥u −v∥2
=
∥u∥2 −2∥u∥∥v∥ω + ∥v∥2
(5.4.19)
≥
∥u∥2 −⟨u, v⟩2
∥v∥2
=
∥u∥2(1 −ω2).
(5.4.20)
Therefore,
1 −ω2 ≤∥u −v∥2
∥u∥2
≤α2
giving the second inequality of (5.4.17). In addition, if ω ≤0, it follows from
(5.4.19) that ∥u −v∥≥∥u∥, and therefore α ≥1. Hence, if α < 1, we have
that ⟨u, v⟩is positive.
Conversely, if ⟨u, v⟩is positive and (5.4.17) holds, then by using (5.4.17)
and some manipulations, we obtain
∥u −v∥2
=
(∥u∥−∥v∥)2 + 2(1 −ω)∥u∥∥v∥
≤
α2∥u∥2[1 + 2(1 + α)],
which gives (5.4.18) since α < 1.
2
If (5.4.16) holds, we have, for given ε ∈(0, 1), that
∥sk −sN
k ∥≤ε∥sk∥
when k ≥k0. So, by Lemma 5.4.5, it follows that if ⟨sk, sN
k ⟩> 0 and k ≥k0,
we have
1 −∥sN
k ∥
∥sk∥
 ≤ε
and
1 −

⟨sk, sN
k ⟩
∥sk∥∥sN
k ∥
2
≤ε2.
They show that (5.4.16) is equivalent to
lim
k→∞
∥sN
k ∥
∥sk∥= lim
k→∞
-
sk
∥sk∥, sN
k
∥sN
k ∥
.
= 1.
(5.4.21)
Therefore we have a conclusion: the necessary and suﬃcient condition of
superlinear convergence of quasi-Newton method is that sk approaches sN
k in
both length and direction.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
247
Superlinear Convergence: Minimization Problem
Next, we consider minimization problem (5.4.2) and discuss the superlin-
ear convergence in the case of basic iteration, exact line search, and inexact
line search.
Completely similar to Theorem 5.4.3, for minimization problem (5.4.2),
we have
Theorem 5.4.6 Let f : Rn →R satisfy the assumptions (a) and (b) in
Assumption 5.4.2. Consider iteration sequence
xk+1 = xk −B−1
k gk,
(5.4.22)
where {Bk} is a sequence of symmetric and positive deﬁnite matrices. As-
sume that {xk} converges to x∗. Then {xk} converges superlinearly to x∗if
and only if
lim
k→∞
∥[Bk −∇2f(x∗)]sk∥
∥sk∥
= 0.
(5.4.23)
Proof.
The proof is the same as for Theorem 5.4.3.
2
The following Theorem 5.4.7 shows the superlinear convergence of quasi-
Newton method in the case of exact line search.
Theorem 5.4.7 Let f : Rn →R satisfy conditions (a) and (b) in Assump-
tion 5.4.2. Suppose {Bk} is a sequence of symmetric and positive deﬁnite
matrices. Consider, for a given x0 ∈D, the iteration
xk+1 = xk −αkB−1
k gk,
(5.4.24)
where αk is determined by exact line search. If the sequence {xk} provided
by (5.4.24) remains in D and xk ̸= x∗(∀k ≥0), and if xk →x∗, then when
lim
k→∞
∥[Bk −∇2f(x∗)]sk∥
∥sk∥
= 0,
(5.4.25)
we have αk →1 and g(x∗) = 0, hence {xk} converges to x∗superlinearly.
Proof.
It is enough to prove αk →1 when (5.4.25) holds. Other conclu-
sions can be obtained direct from Theorem 5.4.4.
Since ∇2f(x∗) is positive deﬁnite, there exists m > 0 such that
sT
k ∇2f(x∗)sk ≥m∥sk∥2.

248
CHAPTER 5. QUASI-NEWTON METHODS
Therefore we only need to prove
(αk −1)sT
k ∇2f(x∗)sk = o(∥sk∥2).
(5.4.26)
From (1.2.111), we have
∥gk+1 −gk −∇2f(x∗)sk∥≤max
0≤t≤1 ∥∇2f(xk + tsk) −∇2f(x∗)∥∥sk∥.
Then from xk →x∗and the continuity of ∇2f(x), we obtain
∥gk+1 −gk −∇2f(x∗)sk∥= o(∥sk∥)
which implies
gT
k+1sk −gT
k sk −sT
k ∇2f(x∗)sk = o(∥sk∥2).
(5.4.27)
Since αk is a steplength from exact line search, gT
k+1sk = 0. Also, noting
that Bksk = αkBkdk = −αkgk, we may write (5.4.27) as
sT
k ∇2f(x∗)sk
=
−gT
k sk + o(∥sk∥2)
=
1
αk
sT
k Bksk + o(∥sk∥2).
(5.4.28)
From (5.4.25), we have
sT
k [Bk −∇2f(x∗)]sk = o(∥sk∥2).
(5.4.29)
So, combining (5.4.28) and (5.4.29) gives
(αk −1)sT
k ∇2f(x∗)sk
=
sT
k [Bk −∇2f(x∗)]sk + o(∥sk∥2)
=
o(∥sk∥2)
which proves (5.4.26).
2
About inexact line search, we consider Wolfe-Powell rule (2.5.3) and
(2.5.7). By use of dk = −Bkgk, we employ the following rule: if
f(xk −B−1
k gk)
≤
f(xk) −ρgT
k B−1
k gk,
(5.4.30)
g(xk −B−1
k gk)T B−1
k gk
≤
σgT
k B−1
k gk
(5.4.31)
hold, take αk = 1; otherwise, take αk > 0 such that
f(xk −αkB−1
k gk) ≤f(xk) −ραkgT
k B−1
k gk,
(5.4.32)
g(xk −αkB−1
k gk)T B−1
k gk ≤σgT
k B−1
k gk,
(5.4.33)
where g(·) = ∇f(·).

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
249
Theorem 5.4.8 Let f : Rn →R satisfy conditions (a) and (b) in Assump-
tion 5.4.2. Suppose that {Bk} is a sequence of symmetric and positive def-
inite matrices. For given x0 ∈D, consider the iteration (5.4.24), where αk
is determined by Wolfe-Powell rule (5.4.30)–(5.4.33). If the sequence {xk}
produced by (5.4.24) remains in D and xk ̸= x∗(∀k ≥0) and if xk →x∗, then
when (5.4.25) holds, αk →1 and hence {xk} converges to x∗superlinearly.
Proof.
Now we only need to prove that for suﬃciently large k, (5.4.30)–
(5.4.33) hold, and thus αk = 1. The remainder is obtained from Theorem
5.4.4.
Since Bksk = −αkgk, it follows from (5.4.25) that
0
=
lim
k→∞
∥[Bk −∇2f(x∗)]sk∥
∥sk∥
=
lim
k→∞
∥gk −∇2f(x∗)B−1
k gk∥
∥B−1
k gk∥
.
Then
gT
k B−1
k gk −(B−1
k gk)T ∇2f(x∗)(B−1
k gk)
=
(gk −∇2f(x∗)B−1
k gk)T (B−1
k gk)
=
o(∥B−1
k gk∥2),
that is
gT
k B−1
k gk = (B−1
k gk)T ∇2f(x∗)(B−1
k gk) + o(∥B−1
k gk∥2).
(5.4.34)
Since ∇2f(x∗) is positive deﬁnite, there exists η > 0 such that for suﬃciently
large k,
gT
k B−1
k gk ≥η∥B−1
k gk∥2.
(5.4.35)
Then, from Taylor’s expansion (1.2.103) and (5.4.34), we have
f(xk −B−1
k gk) −f(xk)
=
−gT
k B−1
k gk + 1
2gT
k B−1
k gk + o(∥B−1
k gk∥2)
=
−1
2gT
k B−1
k gk + o(∥B−1
k gk∥2)
≤
−ρgT
k B−1
k gk.
(5.4.36)

250
CHAPTER 5. QUASI-NEWTON METHODS
Also, by (1.2.111) and a proof similar to (5.4.27), we get
g(xk −B−1
k gk)T B−1
k gk −gT
k B−1
k gk + (B−1
k gk)T ∇2f(x∗)(B−1
k gk)
=
o(∥B−1
k gk∥2),
which, together with (5.4.34), gives
g(xk −B−1
k gk)T B−1
k gk = o(∥B−1
k gk∥2) ≤σgT
k B−1
k gk.
(5.4.37)
It follows from (5.4.36) and (5.4.37) that (5.4.30)-(5.4.31) hold, and thus
αk = 1 for k suﬃciently large.
2
5.4.2
Linear Convergence of General Quasi-Newton Methods
In this subsection, our goal is to discuss the local and linear convergence
results of general quasi-Newton methods. Let the iterative scheme of general
quasi-Newton methods be
xk+1 = xk −B−1
k F(xk),
(5.4.38)
Bk+1 ∈U(xk, Bk),
(5.4.39)
where U(xk, Bk) denotes a nonempty set of updates, (xk, Bk) ∈domU, domU
denotes the domain of U.
Theorem 5.4.9 Let F : Rn →Rn satisfy the assumptions (a), (b) and (c)
in Assumption 5.4.1, U an update function, such that for all (xk, Bk) ∈domU
and Bk+1 ∈U(xk, Bk), we have that
∥Bk+1 −F ′(x∗)∥≤∥Bk −F ′(x∗)∥+ γ
2(∥xk+1 −x∗∥+ ∥xk −x∗∥), (5.4.40)
where γ is some constant, or that
∥Bk+1−F ′(x∗)∥≤[1+α1σ(xk, xk+1)]∥Bk−F ′(x∗)∥+α2σ(xk, xk+1), (5.4.41)
where α1 and α2 are some constants, and
σ(xk, xk+1) = max{∥xk −x∗∥, ∥xk+1 −x∗∥}.
(5.4.42)
Then, there exist constants ε and δ, such that, for ∥x0 −x∗∥< ε and
∥B0 −F ′(x∗)|| < δ, the iteration (5.4.38)–(5.4.39) is well-deﬁned, and {xk}
converges to x∗linearly.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
251
Proof.
First, we prove the conclusion for the given condition (5.4.40).
Assume ∥F ′(x∗)−1∥≤β and choose ε and δ such that
6βδ < 1,
(5.4.43)
3γε ≤2δ.
(5.4.44)
To prove the local and linear convergence, we prove, by induction, that
∥Bk −F ′(x∗)∥≤(2 −2−k)δ,
(5.4.45)
∥xk+1 −x∗∥≤1
2∥xk −x∗∥.
(5.4.46)
For k = 0, (5.4.45) is obvious. Since the proof of (5.4.46) for k = 0 is the
same as that in the following general case, we omit it here.
Now, suppose that (5.4.45) and (5.4.46) hold for k = 0, 1, · · · , i −1. For
k = i, by assumption of induction and (5.4.40), we have
∥Bi −F ′(x∗)∥
≤
∥Bi−1 −F ′(x∗)∥+ γ
2(∥xi −x∗∥+ ∥xi−1 −x∗∥)
≤
(2 −2−(i−1))δ + 3
4γ∥xi−1 −x∗∥.
(5.4.47)
From (5.4.46) and ∥x0 −x∗∥< ε, we have
∥xi−1 −x∗∥≤2−(i−1)∥x0 −x∗∥≤2−(i−1)ε.
(5.4.48)
Substituting (5.4.48) into (5.4.47) and using (5.4.44) yield
∥Bi −F ′(x∗)∥
≤
(2 −2−(i−1))δ + 3
4γ · 2−(i−1)ε
≤
(2 −2−(i−1) + 2−i)δ = (2 −2−i)δ,
(5.4.49)
which proves (5.4.45).
To prove (5.4.46), we ﬁrst show that Bi is invertible.
In fact, since
∥F ′(x∗)−1∥≤β, it follows from (5.4.45) and (5.4.43) that
∥F ′(x∗)−1[Bi −F ′(x∗)]∥
≤
∥F ′(x∗)−1∥∥Bi −F ′(x∗)∥
≤
β(2 −2−i)δ ≤2βδ ≤1
3.

252
CHAPTER 5. QUASI-NEWTON METHODS
Then, by Von-Neumann Theorem 1.2.5, we know that Bi is invertible, and
∥B−1
i
∥
≤
∥F ′(x∗)−1∥
1 −∥F ′(x∗)−1(Bi −F ′(x∗))∥
≤
β
1 −1
3
= 3β
2 .
(5.4.50)
Thus, xi+1 is well-deﬁned. Also,
Bi(xi+1 −x∗)
=
Bi(xi −x∗) −F(xi) + F(x∗)
=
[−F(xi) + F(x∗) + F ′(x∗)(xi −x∗)]
+[Bi −F ′(x∗)](xi −x∗),
(5.4.51)
which gives
∥xi+1 −x∗∥
≤
∥B−1
i
∥[∥−F(xi) + F(x∗) + F ′(x∗)(xi −x∗)∥
+∥Bi −F ′(x∗)∥∥xi −x∗∥].
(5.4.52)
By use of Theorem 1.2.22,
∥−F(xi) + F(x∗) + F ′(x∗)(xi −x∗)∥≤γ
2∥xi −x∗∥2.
(5.4.53)
So, (5.4.52), (5.4.50), (5.4.53) and (5.4.49) give
∥xi+1 −x∗∥≤3
2β
γ
2∥xi −x∗∥+ (2 −2−i)δ

∥xi −x∗∥.
(5.4.54)
Also, using (5.4.48) and (5.4.44), we have
γ
2∥xi −x∗∥≤2−(i+1)γε ≤2−i
3 δ.
Substituting the above inequality into (5.4.54), we obtain
∥xi+1 −x∗∥
≤
3
2β
1
32−i + 2 −2−i

δ∥xi −x∗∥
≤
3βδ∥xi −x∗∥
≤
1
2∥xi −x∗∥.
Therefore, the desired result (5.4.46) is proved.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
253
Similarly, for the given condition (5.4.41), we also can prove our conclu-
sion.
In fact, let ∥F ′(x∗)∥≤β and r ∈(0, 1). Choose ε(r) = ε and δ(r) = δ,
such that
(2α1δ + α2)
ε
1 −r ≤δ,
(5.4.55)
β(1 + r)(γε + 2δ) ≤r.
(5.4.56)
To prove the local and linear convergence, we still prove
∥Bk −F ′(x∗)∥≤2δ,
(5.4.57)
∥xk+1 −x∗∥≤r∥xk −x∗∥
(5.4.58)
by induction.
Obviously, for k = 0, the conclusion holds. Suppose that the conclusion
holds for k = 0, 1, · · · , i −1. By (5.4.41), we have
∥Bk+1 −F ′(x∗)∥−∥Bk −F ′(x∗)∥≤2α1δεrk + α2εrk.
Summing for k = 0 to i −1 yields
∥Bi −F ′(x∗)∥≤∥B0 −F ′(x∗)∥+ (2α1δ + α2)
ε
1 −r.
So, using (5.4.55) and ∥B0 −F ′(x∗)∥≤δ, we obtain
∥Bi −F ′(x∗)∥≤2δ,
(5.4.59)
which proves (5.4.57).
To prove (5.4.58), ﬁrst note that ∥B−1
i
∥≤(1 + r)β from (5.4.59) and
Theorem 1.2.5. Then, by Theorem 1.2.24, we have
∥xi+1 −x∗∥
≤
∥B−1
i
∥[∥F(xi) −F(x∗) −F ′(x∗)(xi −x∗)∥
+∥Bi −F ′(x∗)∥∥xi −x∗∥]
≤
β(1 + r)(γε + 2δ)∥xi −x∗∥.
By using (5.4.56) we immediately obtain
∥xi+1 −x∗∥≤r∥xi −x∗∥.
So, (5.4.58) is proved. We complete the proof by induction.
2
Similarly, we have the following local and linear convergence theorem for
update of the inverse Hessian approximation.

254
CHAPTER 5. QUASI-NEWTON METHODS
Theorem 5.4.10 Let F : Rn →Rn satisfy the conditions (a), (b), and
(c) in Assumption 5.4.1.
Let U be an update function, such that for all
(xk, Hk) ∈domU and Hk+1 ∈U(xk, Hk), we have that
∥Hk+1−F ′(x∗)−1∥≤∥Hk−F ′(x∗)−1∥+ γ
2(∥xk+1−x∗∥+∥xk−x∗∥), (5.4.60)
where γ is some constant, or that
∥Hk+1 −F ′(x∗)−1∥≤[1 + α1σ(xk, xk+1)]∥Hk −F ′(x∗)−1∥+ α2σ(xk, xk+1),
(5.4.61)
where α1 and α2 are some constants, and
σ(xk, xk+1) = max{∥xk −x∗∥, ∥xk+1 −x∗∥}.
Then, there exist constants ε and δ, such that, for ∥x0 −x∗∥< ε and ∥H0 −
F ′(x∗)−1∥< δ, the iteration
xk+1 = xk −HkF(xk), Hk+1 ∈U(xk, Hk)
(5.4.62)
is well-deﬁned and {xk} converges to x∗linearly.
As a consequence of the above two theorems, we give the following corol-
laries on superlinear convergence for general iterations.
Corollary 5.4.11 Suppose that the assumptions of Theorem 5.4.9 hold. If
some subsequence of {∥Bk −F ′(x∗)∥} converges to zero, then {xk} converges
to x∗superlinearly.
Proof.
We hope to prove
lim
k→+∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0.
Let r ∈(0, 1). It follows from Theorem 5.4.9 that there exist ε(r) and δ(r)
such that ∥B0−F ′(x∗)∥< δ(r) and ∥x0−x∗∥< ε(r) imply that ∥xk+1−x∗∥≤
r∥xk −x∗∥, ∀k ≥0.
From the assumption, we can choose m > 0 such
that ∥Bm −F ′(x∗)∥< δ(r) and ∥xm −x∗∥< ε(r). Hence ∥xk+1 −x∗∥≤
r∥xk −x∗∥, ∀k ≥m. Since r ∈(0, 1) is arbitrary, the conclusion is shown.
2
Similarly, we have
Corollary 5.4.12 Suppose that the conditions of Theorem 5.4.10 hold. If
some subsequence of {∥Hk −F ′(x∗)−1∥} converges to zero, then {xk} con-
verges to x∗superlinearly.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
255
5.4.3
Local Convergence of Broyden’s Rank-One Update
In this section, we prove the linear convergence and superlinear convergence
of Broyden’s rank-one update
xk+1 = xk −B−1
k F(xk),
(5.4.63)
Bk+1 = Bk + (yk −Bksk)sT
k
sT
k sk
.
(5.4.64)
Theorem 5.4.13 Let F : Rn →Rn satisfy the conditions (a), (b), and (c)
in Assumption 5.4.1.
Assume that there exist positive constants ε and δ
such that ∥x0 −x∗∥< ε and ∥B0 −F ′(x∗)∥< δ. Then the sequence {xk}
generated by Broyden’s rank-one update (5.4.63)–(5.4.64) is well-deﬁned and
convergent to x∗superlinearly.
Proof.
It is enough to prove, under the conditions of the theorem, that
(5.4.40) and (5.4.4) are satisﬁed respectively.
First, we prove that Bk+1 generated by Broyden’s rank-one update sat-
isﬁes (5.4.40).
By (5.4.63)–(5.4.64), we have
Bk+1 −F ′(x∗) = Bk −F ′(x∗) + (yk −Bksk)sT
k
sT
k sk
=
Bk −F ′(x∗) + (F ′(x∗)sk −Bksk)sT
k
sT
k sk
+ (yk −F ′(x∗)sk)sT
k
sT
k sk
=
(Bk −F ′(x∗))

I −sksT
k
sT
k sk

+ (yk −F ′(x∗)sk)sT
k
sT
k sk
.
(5.4.65)
Taking norms gives
∥Bk+1 −F ′(x∗)∥≤∥Bk −F ′(x∗)∥
I −sksT
k
sT
k sk
 + ∥yk −F ′(x∗)sk∥
∥sk∥
. (5.4.66)
Note that
I −sksT
k
sT
k sk
 = 1
(5.4.67)
and
∥yk −F ′(x∗)sk∥
=
∥F(xk+1) −F(xk) −F ′(x∗)sk∥
≤
γ
2(∥xk+1 −x∗∥+ ∥xk −x∗∥)∥sk∥
(5.4.68)

256
CHAPTER 5. QUASI-NEWTON METHODS
by Theorem 1.2.24, we obtain immediately that
∥Bk+1 −F ′(x∗)∥≤∥Bk −F ′(x∗)∥+ γ
2(∥xk+1 −x∗∥+ ∥xk −x∗∥),
which is (5.4.40). The linear convergence is proved.
Next, we prove the superlinear convergence of Broyden’s rank-one update
by use of Theorem 5.4.3, that is, we want to prove that (5.4.4) holds.
Let Ek = Bk −F ′(x∗). From (5.4.65),
∥Ek+1∥F ≤
Ek

I −sksT
k
sT
k sk

F
+ ∥(yk −F ′(x∗)sk)sT
k ∥F
sT
k sk
.
(5.4.69)
Since
Ek
sksT
k
sT
k sk

2
F
=
tr

(Eksk)T (Eksk)
(sT
k sk)2
sksT
k

=
∥Eksk∥2
∥sk∥4 ∥sk∥2 = ∥Eksk∥2
∥sk∥2 ,
we get
∥Ek∥2
F
=
Ek
sksT
k
sT
k sk

2
F
+
Ek

I −sksT
k
sT
k sk

2
F
=
∥Eksk∥2
∥sk∥2
+
Ek

I −sksT
k
sT
k sk

2
F
.
Hence
Ek

I −sksT
k
sT
k sk

F
=

∥Ek∥2
F −∥Eksk∥2
∥sk∥2
 1
2
.
(5.4.70)
Since (α2 −β2)
1
2 ≤α −β2/(2α) for any α ≥|β| ≥0, (5.4.70) implies that
Ek

I −sksT
k
sT
k sk

F
≤∥Ek∥F −
1
2∥Ek∥F
 ∥Eksk∥
∥sk∥
!2
.
(5.4.71)
Also, by means of Theorem 1.2.24,
∥yk −F ′(x∗)sk∥F ≤γ
2(∥xk+1 −x∗∥+ ∥xk −x∗∥)∥sk∥.
(5.4.72)

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
257
So, by using (5.4.71), (5.4.72), and (5.4.46), we can write (5.4.69) as
∥Ek+1∥F ≤∥Ek∥F −
∥Eksk∥2
2∥Ek∥F ∥sk∥2 + 3
4γ∥xk −x∗∥,
which is
∥Eksk∥2
∥sk∥2
≤2∥Ek∥F

∥Ek∥F −∥Ek+1∥F + 3
4γ∥xk −x∗∥

.
(5.4.73)
Recalling (5.4.45) and (5.4.46), we have that
∥Ek∥F ≤2δ, ∀k ≥0
and
∞

k=0
∥xk −x∗∥≤2ε.
Thus, (5.4.73) can be written as
∥Eksk∥2
∥sk∥2
≤4δ

∥Ek∥F −∥Ek+1∥F + 3
4γ∥xk −x∗∥

.
(5.4.74)
By summing both sides, we obtain
i

k=0
∥Eksk∥2
∥sk∥2
≤
4δ

∥E0∥F −∥Ei+1∥F + 3
4γ
i

k=0
∥xk −x∗∥

≤
4δ

∥E0∥F + 3
2γε

≤
4δ

δ + 3
2γε

,
(5.4.75)
which holds for any i ≥0. Therefore
∞

k=0
∥Eksk∥2
∥sk∥2
is ﬁnite and further
lim
k→∞
∥Eksk∥
∥sk∥
= 0,
(5.4.76)
which is (5.4.4). Then we have proved the superlinear convergence of Broy-
den’s rank-one update by use of Theorem 5.4.3.
2

258
CHAPTER 5. QUASI-NEWTON METHODS
Similarly, for the following form of Broyden’s rank-one update in inverse
Hessian approximation:
xk+1
=
xk −HkF(xk),
(5.4.77)
Hk+1
=
Hk + (sk −Hkyk)yT
k
yT
k yk
,
(5.4.78)
we have the following theorem.
Theorem 5.4.14 Let F : Rn →Rn satisfy the conditions (a), (b), and
(c) in Assumption 5.4.1. Assume that there exist ε and δ such that ∥x0 −
x∗∥< ε and ∥H0 −F ′(x∗)−1∥< δ. Then the sequence {xk} generated by
Broyden’s rank-one update (5.4.77)–(5.4.78) is well-deﬁned and convergent
to x∗superlinearly.
5.4.4
Local and Linear Convergence of DFP Method
In this subsection and subsequent subsections, we discuss the local conver-
gence of rank-two methods, which includes the linear and superlinear conver-
gence, and local convergence under line search. Note that we introduce two
diﬀerent techniques to prove the superlinear convergence of BFGS and DFP
methods respectively.
The DFP iteration we consider is
xk+1
=
xk −B−1
k ∇f(xk),
(5.4.79)
Bk+1
=
Bk + (yk −Bksk)yT
k + yk(yk −Bksk)T
yT
k sk
−(yk −Bksk)T sk
(yT
k sk)2
ykyT
k .
(5.4.80)
To study the local convergence of DFP method, it is required to estimate
∥Bk+1 −∇2f(x∗)∥. As shown in the following theorem, there is a matrix
P = I −skyT
k
sT
k yk in Bk+1 −∇2f(x∗). Since
∥P∥2 = ∥sk∥∥yk∥
sT
k yk
,
(5.4.81)
it is a secant of the angle between yk and sk.
In general, yk and sk is
not parallel, so ∥P∥2 may be quite big, and it is not suitable to estimate

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
259
∥Bk+1 −∇2f(x∗)∥by means of l2 norm. However, near x∗, f(x) closes a
quadratic function, and hence A−1
2 yk and A
1
2 sk are approximately parallel,
where A = ∇2f(x∗). It motivates us to use some weighted norm to estimate
∥Bk+1 −∇2f(x∗)∥. Then we deﬁne
∥E∥DFP = ∥E∥A−1
2 ,F = ∥A−1
2 EA−1
2 ∥F .
(5.4.82)
Below, we ﬁrst develop the linear convergence of DFP method.
Theorem 5.4.15 Let f : Rn →R satisfy Assumption 5.4.2. Also let
µγσ(xk, xk+1) ≤1
3
(5.4.83)
in a neighborhood of x∗, where µ = ∥∇2f(x∗)−1∥, σ(xk, xk+1) = max{∥xk −
x∗∥, ∥xk+1−x∗∥}. Then, there exist ε > 0 and δ > 0 such that for ∥x0−x∗∥<
ε and ∥B0−∇2f(x∗)∥DFP < δ, the iteration (5.4.79)–(5.4.80) of DFP method
is well-deﬁned, and the produced sequence {xk} converges to x∗linearly.
Proof.
Based on Theorem 5.4.9, to prove the linear convergence of DFP
method, it is enough to prove
∥Bk+1 −∇2f(x∗)∥DFP
<
[1 + α1σ(xk, xk+1)]∥Bk −∇2f(x∗)∥DFP
+α2σ(xk, xk+1),
(5.4.84)
where α1 and α2 are positive constants independent of xk and xk+1, σ(xk, xk+1) =
max{∥xk −x∗∥, ∥xk+1 −x∗∥}.
Let A = ∇2f(x∗). From (5.4.79)–(5.4.80), it follows that
Bk+1 −A = P T (Bk −A)P + (yk −Ask)yT
k + yk(yk −Ask)T P
yT
k sk
,
(5.4.85)
where
P = I −skyT
k
sT
k yk
.
(5.4.86)
Note that ∥P∥2 = ∥sk∥∥yk∥/sT
k yk, hence
∥P T (Bk −A)P∥DFP
≤
∥A
1
2 PA−1
2 ∥2
2∥Bk −A∥DFP
≤
1
ω2 ∥Bk −A∥DFP ,
(5.4.87)

260
CHAPTER 5. QUASI-NEWTON METHODS

yk(yk −Ask)T P
yT
k sk

DFP
≤
1
ω2
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
,
(5.4.88)

(yk −Ask)yT
k
yT
k sk

DFP
≤
1
ω
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
,
(5.4.89)
where
ω =
yT
k sk
∥A−1
2 yk∥∥A
1
2 sk∥
= ⟨A−1
2 yk, A
1
2 sk⟩
∥A−1
2 yk∥∥A
1
2 sk∥
.
(5.4.90)
Now, we estimate ∥Bk+1−A∥DFP by using (5.4.87), (5.4.88) and (5.4.89),
and have
∥Bk+1 −A∥DFP
≤
1
ω2 ∥Bk −A∥DFP
+ 2
ω2
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
.
(5.4.91)
Note from Theorem 1.2.24 that
∥A−1
2 yk −A−1
2 sk∥
∥A
1
2 sk∥
≤
∥A−1
2 ∥∥yk −Ask∥
∥sk∥/∥A−1
2 ∥
=
µ∥yk −Ask∥
∥sk∥
≤
µγσ(xk, xk+1) ≤1
3.
(5.4.92)
Also, by Lemma 5.4.5, we have
1 −ω2 ≤

µ∥yk −Ask∥
∥sk∥
2
≤[µγσ(xk, xk+1)]2.
Then, if xk and xk+1 are in the neighborhood of x∗, then
1 −ω2 ≤[µγσ(xk, xk+1)]2 < 1
2,
which is
ω2 > 1
2 > µγσ(xk, xk+1).

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
261
Hence
1
ω2
=
1 + 1 −ω2
ω2
< 1 + [µγσ(xk, xk+1)]2
µγσ(xk, xk+1)
=
1 + µγσ(xk, xk+1).
So, the two terms in (5.4.91) satisfy respectively
1
ω2 ∥Bk −A∥DFP < (1 + µγσ(xk, xk+1))∥Bk −A∥DFP
(5.4.93)
and
2
ω2
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
<
2[1 + µγσ(xk, xk+1)]µγσ(xk, xk+1)]µγσ(xk, xk+1)
<
3µγσ(xk, xk+1).
(5.4.94)
Substituting (5.4.93) and (5.4.94) into (5.4.112) yields (5.4.113), where α1 =
µγ, α2 = 3µγ. So, we complete the proof.
2
5.4.5
Superlinear Convergence of BFGS Method
In this subsection, we discuss the superlinear convergence of BFGS method.
Let
˜sk = G
1
2∗sk, ˜yk = G
−1
2
∗
yk, ˜Bk = G
−1
2
∗
BkG
−1
2
∗
,
(5.4.95)
where G∗= G(x∗) = ∇2f(x∗). Deﬁne
cos ˜θk =
˜sT
k ˜Bk˜sk
∥˜sk∥∥˜Bk˜sk∥, ˜qk = ˜sT
k ˜Bk˜sk
∥˜sk∥2 ,
(5.4.96)
and deﬁne
˜
Mk = ∥˜yk∥2
˜yT
k ˜sk
,
˜mk = ˜yT
k ˜sk
˜sT
k ˜sk
.
(5.4.97)
By pre- and postmultiplying the BFGS update (5.1.45) by G
−1
2
∗
, we obtain
˜Bk+1 = ˜Bk −
˜Bk˜sk˜sT
k ˜Bk
˜sT
k ˜Bk˜sk
+ ˜yk˜yT
k
˜yT
k ˜sk
.
(5.4.98)

262
CHAPTER 5. QUASI-NEWTON METHODS
Since this expression has precisely the same form as the BFGS formula, it
follows from the argument leading to (5.3.42) that
ψ( ˜Bk+1)
=
ψ( ˜Bk) + ( ˜
Mk −ln ˜mk −1)
=

1 −
˜qk
cos2 ˜θk
+ ln
˜qk
cos2 ˜θk

+ ln cos2 ˜θk.
(5.4.99)
Noting that
yk −G∗sk = ( ¯Gk −G∗)sk,
where
¯Gk =
 1
0
∇2f(xk + τsk)dτ,
we obtain
˜yk −˜sk = G
−1
2
∗
( ¯G∗−Gk)G
−1
2
∗
˜sk.
Assuming that the Hessian matrix G is Lipschitz at x∗, then we have
∥˜yk −˜sk∥≤∥G
−1
2
∗
∥2∥˜sk∥∥¯Gk −G∗∥≤∥G
−1
2
∗
∥2∥˜sk∥Lϵk,
which gives
∥˜yk −˜sk∥
∥˜sk∥
≤¯cϵk
(5.4.100)
for some positive constant ¯c, where
ϵk = max{∥xk+1 −x∗∥, ∥xk −x∗∥}.
(5.4.101)
Now we are in a position to prove the superlinear convergence theorem.
Theorem 5.4.16 Let f be twice continuously diﬀerentiable and the Hessian
matrix G be Lipschitz continuous at x∗. Suppose that the sequence generated
by the BFGS algorithm converges to a minimizer x∗and that the condition
∞

k=1
∥xk −x∗∥< ∞
(5.4.102)
holds. Then {xk} converges to x∗at a superlinear rate.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
263
Proof.
By (5.4.100), we have
∥˜yk∥−∥˜sk∥≤¯cϵk∥˜sk∥, ∥˜sk∥−∥˜yk∥≤¯cϵk∥˜sk∥,
which give
(1 −¯cϵk)∥˜sk∥≤∥˜yk∥≤(1 + ¯cϵk)∥˜sk∥.
(5.4.103)
By squaring (5.4.100) and using (5.4.103), we obtain
(1 −¯cϵk)2∥˜sk∥2 −2˜yT
k ˜sk + ∥˜sk∥2 ≤∥˜yk∥2 −2˜yT
k ˜sk + ∥˜sk∥2 ≤¯c2ϵ2
k∥˜sk∥2,
and therefore
2˜yT
k ˜sk ≥(1 −2¯cϵk + ¯c2ϵ2
k + 1 −¯c2ϵ2
k)∥˜sk∥2 = 2(1 −¯cϵk)∥˜sk∥2.
It follows from the deﬁnition of ˜mk that
˜mk = ˜yT
k ˜sk
∥˜sk∥2 ≥1 −¯cϵk.
(5.4.104)
Combining (5.4.103) and (5.4.104) gives also that
˜
Mk = ∥˜yk∥2
˜yT
k ˜sk
≤1 + ¯cϵk
1 −¯cϵk
.
(5.4.105)
Since xk →x∗, we have that ϵk →0. Thus by (5.4.105) there exists a positive
constant c > ¯c such that the following inequalities hold for all suﬃcient large
k:
˜
Mk ≤1 +
2¯c
1 −¯cϵk
ϵk ≤1 + cϵk.
(5.4.106)
Making use of the nonpositiveness of the function h(t) = 1 −t + ln t gives
−x
1 −x −ln(1 −x) = h
 
1
1 −x
!
≤0.
Now for k large enough we can assume that ¯cϵk < 1
2, and by using the above
inequality we have
ln(1 −¯cϵk) ≥
−¯cϵk
1 −¯cϵk
≥−2¯cϵk.
This relation and (5.4.104) imply that for suﬃciently large k, we have
ln ˜mk ≥ln(1 −¯cϵk) ≥−2¯cϵk > −2cϵk.
(5.4.107)

264
CHAPTER 5. QUASI-NEWTON METHODS
We can now deduce from (5.4.99), (5.4.106), and (5.4.107) that
0 < ψ( ˜Bk+1) ≤ψ( ˜Bk) + 3cϵk + ln cos2 ˜θk +

1 −
˜qk
cos2 ˜θk
+ ln
˜qk
cos2 ˜θk

.
(5.4.108)
By summing this expression and making use of (5.4.102) we have that
∞

j=0

ln
1
cos2 ˜θj
−

1 −
˜qj
cos2 ˜θj
+ ln
˜qj
cos2 ˜θj

≤
ψ( ˜B0) + 3c
∞

j=0
ϵj < +∞.
Since the term in the square brackets is nonpositive, and since ln(1/ cos2 ˜θj) ≥
0 for all j, we obtain
lim
j→∞ln
1
cos2 ˜θj
= 0, lim
j→∞

1 −
˜qj
cos2 ˜θj
+ ln
˜qj
cos2 ˜θj

= 0,
which imply that
lim
j→∞cos ˜θj = 1,
lim
j→∞˜qj = 1.
(5.4.109)
By use of these limits we can obtain that
∥G
−1
2
∗
(Bk −G∗)sk∥2
∥G
1
2∗sk∥2
=
∥( ˜Bk −I)˜sk∥2
∥˜sk∥2
=
∥˜Bk˜sk∥2 −2˜sT
k ˜Bk˜sk + ˜sT
k ˜sk
˜sT
k ˜sk
=
˜q2
k
cos2 ˜θk
−2˜qk + 1
→
0.
(5.4.110)
Then we conclude that
lim
k→∞
∥(Bk −G∗)sk∥
∥sk∥
= 0
(5.4.111)
which shows that the rate of convergence is superlinear.
2

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
265
5.4.6
Superlinear Convergence of DFP Method
We ﬁrst give the following three lemmas.
Lemma 5.4.17 Let M ∈Rn×n be a nonsingular symmetric matrix. If, for
β ∈[0, 1/3], the inequality
∥Myk −M−1sk∥≤β∥M −1sk∥
(5.4.112)
holds, then for any nonzero matrix E ∈Rn×n, we have
(a)
(1 −β)∥M −1sk∥2 ≤yT
k sk ≤(1 + β)∥M−1sk∥2,
(5.4.113)
(b)
E

I −(M−1sk)(M−1sk)T
yT
k sk

F
≤
)
1 −αθ2∥E∥F ,
(5.4.114)
(c)
E

I −M−1sk(Myk)T
yT
k sk

F
≤
)
1 −αθ2 + (1 −β)−1 ∥Myk −M−1sk∥
∥M−1sk∥

∥E∥F ,
(5.4.115)
where
α = 1 −2β
1 −β2 ∈
3
8, 1

, θ =
∥EM −1sk∥
∥E∥F ∥M−1sk∥∈[0, 1].
(5.4.116)
Proof.
Note that
yT
k sk = (Myk)T (M−1sk) = (Myk −M−1sk)T M−1sk +∥M−1sk∥2. (5.4.117)
Also, it follows from Cauchy-Schwartz inequality and (5.4.112) that
|(Myk −M−1sk)T M−1sk| ≤β∥M −1sk∥2.
(5.4.118)
Then, combining (5.4.117) and (5.4.118) gives the ﬁrst conclusion (a).
Now, we prove (b). By using the property (1.2.71) of Frobenius norm of
a rank-one update, we have
∥E(I −uvT )∥2
F = ∥E∥2
F −2vT ET Eu + ∥Eu∥2∥v∥2.

266
CHAPTER 5. QUASI-NEWTON METHODS
In particular,
E

I −(M−1sk)(M−1sk)T
yT
k sk

2
F
=
∥E∥2
F + (−2yT
k sk + ∥M−1sk∥2)∥EM −1sk∥
(yT
k sk)2
.
Using (a) and (5.4.116) yields
E

I −(M−1sk)(M−1sk)T
yT
k sk

2
F
≤
∥E∥2
F −
 1 −2β
1 −β
! ∥EM −1sk∥2
yT
k sk
≤
∥E∥2
F −α

∥EM −1sk∥
∥M−1sk∥
2
=
∥E∥2
F (1 −αθ2),
which shows (b).
Finally, we prove (c) by means of (b). It is enough to prove that
E M −1sk(M−1sk −Myk)T
yT
k sk

F
≤
(1 −β)−1

∥Myk −M−1sk∥
∥M−1sk∥

∥E∥F .
(5.4.119)
Since

M−1sk(M−1sk −Myk)T
yT
k sk

F
≤∥M−1sk∥∥M−1sk −Myk∥
yT
k sk
,
then we obtain (5.4.119) by using (a).
2
Lemma 5.4.18 Let {φk} and {δk} be sequences of nonnegative numbers sat-
isfying
φk+1 ≤(1 + δk)φk + δk
(5.4.120)
and
∞

k=1
δk < +∞,
(5.4.121)
then {φk} converges.

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
267
Proof.
We ﬁrst prove that {φk} is bounded above. Let
µk =
k−1
/
j=1
(1 + δj).
Obviously, µk ≥1. Inequality (5.4.121) indicates that there exists a constant
µ such that µk ≤µ. By using (5.4.120), we have
φk+1
µk+1
≤φk
µk
+
δk
µk+1
≤φk
µk
+ δk.
Hence
φm+1
µm+1
≤φ1
µ1
+
m

k=1
δk.
From (5.4.121) and the boundedness of {µk}, we obtain that {φk} is bounded.
Since {φk} is bounded, then there is at least a limit point. Suppose that
there are two subsequences {φkn} and {φkm}, which converge to φ′ and φ′′
respectively. We can show that φ′ ≤φ′′, and that φ′′ ≤φ′ by symmetry.
Thus φ′ = φ′′ and {φk} is convergent.
In fact, let φ be a bound of {φk}. Let also, for example, kn ≥km. From
(5.4.120), we have
φkn −φkm ≤(1 + φ)
kn−1

j=km
δj.
By the selection of kn, we have
φ′ −φkm ≤(1 + φ)
∞

j=km
δj.
By the selection of km, we have
φ′ −φ′′ ≤0.
Therefore φ′ ≤φ′′. Similarly, by symmetry, we obtain φ′′ ≤φ′. We complete
the proof.
2
We have known that if f : Rn →R satisﬁes Assumption 5.4.2, then
(5.4.84) holds. Let ∥Bk−A∥DFP = φk and max{α1σ(xk, xk+1), α2σ(xk, xk+1)} =
δk. Then (5.4.121) holds. Thus, it follows from Lemma 5.4.18 that the limit
lim
k→+∞∥Bk −A∥DFP
(5.4.122)
exists.

268
CHAPTER 5. QUASI-NEWTON METHODS
Lemma 5.4.19 Under the assumptions of Theorem 5.4.15, there exist pos-
itive constants β1, β2, and β3, such that ∀xk, xk+1 ∈N(x∗, ε), we have
∥Bk+1 −∇2f(x∗)∥DFP
≤

1 −β1θ2
k + β2σ(xk, xk+1)

∥Bk −∇2f(x∗)∥DFP
+β3σ(xk, xk+1),
(5.4.123)
where
σ(xk, xk+1) = max{∥xk −x∗∥, ∥xk+1 −x∗∥},
(5.4.124)
θk =
∥∇2f(x∗)−1
2 [Bk −∇2f(x∗)
1
2 ]sk∥
∥Bk −∇2f(x∗)∥DFP ∥∇2f(x∗)
1
2 sk∥
.
(5.4.125)
Proof.
Write A = ∇2f(x∗). From (5.4.85), we have
∥Bk+1 −A∥DFP
≤
∥P T (Bk −A)P∥DFP +

(yk −Ask)yT
k
yT
k sk

DFP
+

yk(yk −Ask)T P
yT
k sk

DFP
.
(5.4.126)
Let
Q = I −A
1
2 skyT
k A−1
2
yT
k sk
, Ek = A−1
2 (Bk −A)A−1
2 .
(5.4.127)
Then
∥P T (Bk −A)P∥DFP
=
∥(A−1
2 P T A
1
2 )(A−1
2 (Bk −A)A−1
2 )(A
1
2 PA−1
2 )∥F
=
∥QT EQ∥F .
Similar to the proof of Theorem 5.4.15, we know that there exist α3 and
α4 > 0 such that

(yk −Ask)yT
k
yT
k sk

DFP
≤1
ω
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
≤α3σ(xk, xk+1),

yk(yk −Ask)T P
yT
k sk

DFP
≤1
ω2
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
≤α4σ(xk, xk+1).
If we let β3 = α3 + α4, then (5.4.126) becomes
∥Bk+1 −A∥DFP ≤∥QT EQ∥F + β3σ(xk, xk+1).
(5.4.128)

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
269
Since
∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥
≤µγσ(xk, xk+1) ≤1
3,
then, by use of Lemma 5.4.17, we obtain
∥QT EQ∥F ≤

1 + (1 −β)−1 ∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥

∥QT E∥F .
Note that ∥QT E∥F = ∥ET Q∥F = ∥EQ∥F , thus, by using Lemma 5.4.17 once
more, we obtain
∥EQ∥F ≤

1 −αθ2
k + (1 −β)−1 ∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥

∥E∥F ,
where θk is deﬁned by (5.4.125). Then
∥QT EQ∥F
≤

1 −αθ2
k + 5
2(1 −β)−1 ∥A−1
2 yk −A
1
2 sk∥
∥A
1
2 sk∥

∥E∥F
≤
[

1 −β1θ2
k + β2σ(xk, xk+1)]∥E∥F ,
(5.4.129)
where β1 = α, β2 = 5
2(1 −β)−1µγ. Substituting (5.4.129) into (5.4.128), we
deduce the desired result (5.4.123). The proof is complete.
2
Using the above three lemmas, we can establish the following superlinear
convergence theorem of DFP method.
Theorem 5.4.20 Under the assumptions of Theorem 5.4.15, DFP method
deﬁned by (5.4.79)–(5.4.80) is convergent superlinearly.
Proof.
Since (1 −β1θ2
k)
1
2 ≤1 −(β1/2)θ2
k, then (5.4.123) can be written as
(β1θ2
k/2)∥Bk −A∥DFP
≤
∥Bk −A∥DFP −∥Bk+1 −A∥DFP
+[β2∥Bk −A∥DFP + β3]σ(xk, xk+1).
Summing both sides yields
1
2β1
∞

k=1
θ2
k∥Bk −A∥DFP
≤
∥B1 −A∥DFP + β2
∞

k=1
σ(xk, xk+1)∥Bk −A∥DFP
+β3
∞

k=1
σ(xk, xk+1).

270
CHAPTER 5. QUASI-NEWTON METHODS
Since, from Theorem 5.4.15, {xk} is linearly convergent, then 
∞
k=1 σ(xk, xk+1) <
∞. Also, since {∥Bk −A∥DFP } is bounded, then
β1
2
∞

k=1
θ2
k∥Bk −A∥DFP < ∞.
By (5.4.56), the limit limk→∞∥Bk −A∥DFP exists. Hence, if some subse-
quence of {∥Bk −A∥DFP } converges to zero, the whole sequence converges
to zero. Therefore
lim
k→∞
∥(Bk −A)sk∥
∥sk∥
= 0,
and the conclusion holds. Otherwise, if ∥Bk −A∥DFP ≥ω > 0, ∀k ≥k0, then
θk →0. Note that
∥(Bk −A)sk∥
∥sk∥
≤
∥A
1
2 ∥∥A−1
2 (Bk −A)sk∥
∥A
1
2 ∥−1∥A
1
2 sk∥
=
∥A∥∥Bk −A∥DFP
∥A−1
2 (Bk −A)sk∥
∥Bk −A∥DFP ∥A
1
2 sk∥
=
∥A∥∥Bk −A∥DFP θk,
then, by using θk →0, we immediately obtain
lim
k→∞
∥(Bk −A)sk∥
∥sk∥
= 0.
Hence {xk} is convergent superlinearly. We complete the proof.
2
Similarly, we can state the superlinear convergence theorem for BFGS
method.
Theorem 5.4.21 Under the assumptions of Theorem 5.4.15, the sequence
{xk} generated by BFGS method (5.4.79) and (5.1.45) is convergent to x∗
superlinearly.
It is not diﬃcult to describe the above theorems in inverse Hessian ap-
proximations, which proofs are left to interested readers as an exercise.
We consider BFGS update in inverse Hessian approximation (5.1.48), now
written again as
xk+1
=
xk −Hkgk,
(5.4.130)

5.4. LOCAL CONVERGENCE OF QUASI-NEWTON METHODS
271
Hk+1
=
Hk + (sk −Hkyk)sT
k + sk(sk −Hkyk)T
sT
k yk
−(sk −Hkyk)T yk
(sT
k yk)2
sksT
k .
(5.4.131)
We employ the weighted norm
∥E∥BFGS = ∥E∥A1/2,F = ∥A1/2EA1/2∥F ,
(5.4.132)
where A = ∇2f(x∗).
Theorem 5.4.22 Let f : Rn →R satisfy Assumption 5.4.2. Also let
µγσ(xk, xk+1) ≤1
3
(5.4.133)
in a neighborhood of x∗, where µ = ∥∇2f(x∗)−1∥and σ(xk, xk+1) = max{∥xk−
x∗∥, ∥xk+1−x∗∥}. Then, there exist ε > 0 and δ > 0 such that for ∥x0−x∗∥<
ε and ∥H0−∇2f(x∗)−1∥BFGS < δ, BFGS method (5.4.130)–(5.4.131) is well-
deﬁned, and the produced sequence {xk} converges to x∗linearly. Further, if

∞
k=0 ∥xk −x∗∥< +∞, then the sequence {xk} converges to x∗superlinearly.
5.4.7
Local Convergence of Broyden’s Class Methods
Finally, in this section, we discuss local convergence of Broyden’s class meth-
ods.
Byrd, Nocedal and Yuan [47] proved the superlinear convergence of Broy-
den’s class method. We state the theorem without proof.
Theorem 5.4.23 Suppose that f : Rn →R is twice continuously diﬀeren-
tiable on a convex set D and that f(x) is uniformly convex, i.e., there exists
m > 0 such that for any x ∈Rn and u ∈Rn,
uT ∇2f(x)u ≥m∥u∥2.
Suppose also that there is a neighborhood N(x∗, ε) of x∗, such that
∥∇2f(¯x) −∇2f(x)∥≤γ∥¯x −x∥, ∀x, ¯x ∈N(x∗, ε).
Then, for any positive deﬁnite matrix B0, when line search satisﬁes Wolfe-
Powell rule (5.4.30)–(5.4.33), the sequence {xk} generated by the restricted
Broyden’s class (θ ∈(0, 1)) converges to x∗superlinearly.

272
CHAPTER 5. QUASI-NEWTON METHODS
For Broyden’s class with exact line search, we have
Theorem 5.4.24 Suppose that the assumptions of Theorem 5.4.23 hold.
When the exact line search is employed, the sequence {xk} generated by Broy-
den’s class method converges to x∗superlinearly.
Byrd, Liu, and Nocedal [43] established the following superlinear char-
acterization in which the superlinear characterization (5.4.25) is replaced by
(5.4.135) and (5.4.136).
Theorem 5.4.25 Let iterates generated by
xk+1 = xk −αkB−1
k gk
converge to x∗with ∇f(x∗) = 0 and ∇2f(x∗) positive deﬁnite. Then
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0
(5.4.134)
if and only if
lim
k→∞cos2⟨B−1
k gk, −∇2f(x∗)−1gk⟩= 1
(5.4.135)
and
lim
k→∞
sT
k Bksk
αksT
k yk
= 1.
(5.4.136)
Proof.
Suppose that (5.4.134) holds, then we have
lim
k→∞cos2⟨B−1
k gk, xk −x∗⟩= 1.
(5.4.137)
Note also that
lim
k→∞cos2⟨xk −x∗, −∇2f(x∗)−1gk⟩= 1.
(5.4.138)
Hence (5.4.135) holds.
By (5.4.134) and the positive deﬁniteness of ∇2f(x∗), we have
lim
k→∞
∥gk + yk∥
∥gk∥
= 0,
which implies
lim
k→∞
sT
k gk + sT
k yk
∥sk∥∥gk∥
= 0.

5.5. SELF-SCALING VARIABLE METRIC (SSVM) METHODS
273
Therefore
lim
k→∞
−sT
k gk
sT
k yk
= 1,
(5.4.139)
which means (5.4.136).
Conversely, assume that (5.4.135) and (5.4.136) hold. By (5.4.135) and
(5.4.138) we deduce that (5.4.137) holds. Also, (5.4.136) means (5.4.139).
Then, we obtain
lim
k→∞
sT
k gk + sT
k ∇2f(x∗)sk
sT
k yk
= 0,
which is
lim
k→∞
sT
k ∇2f(x∗)[sk + ∇2f(x∗)−1gk]
sT
k ∇2f(x∗)sk
= 0.
(5.4.140)
Then, (5.4.140) and (5.4.135) gives
lim
k→∞
∥sk + ∇2f(x∗)−1gk∥
∥sk∥
= 0,
(5.4.141)
which is equivalent to (5.4.134). We complete the proof.
2
5.5
Self-Scaling Variable Metric (SSVM) Methods
5.5.1
Motivation to SSVM Method
We have seen that DFP method is a typical rank-two quasi-Newton method.
However, numerical experiments show that its implementation is not ideal.
Why? Below, we would like to give some analysis.
First, we clarify that the single-step convergence Theorem 3.1.5 of the
steepest descent method is also true for various Newton-like methods. Let
f(x) = 1
2xT Gx −bT x,
(5.5.1)
where G is an n×n symmetric and positive deﬁnite matrix. Let the Newton-
like method be deﬁned by
xk+1 = xk −αkHkgk,
(5.5.2)
where
gk = Gxk −b,
(5.5.3)
αk = gT
k Hkgk/gT
k HkGHkgk,
(5.5.4)

274
CHAPTER 5. QUASI-NEWTON METHODS
then we have the following theorem.
Theorem 5.5.1 Let x∗be a minimizer of the quadratic function (5.5.1),
and let Newton-like methods be deﬁned by (5.5.2).
Then, the single-step
convergence rate satisﬁes the following bound:
f(xk+1) −f(x∗)
f(xk) −f(x∗)
≤(λ1 −λn)2
(λ1 + λn)2 ,
(5.5.5)
E(xk+1) ≤(λ1 −λn)2
(λ1 + λn)2 E(xk),
(5.5.6)
where E(xk) = 1
2(xk −x∗)T G(xk −x∗), λ1 and λn are the largest and the
smallest eigenvalues of matrix HkG respectively.
Proof.
Since
x∗= xk −G−1gk
(5.5.7)
and
f(xk) −f(x∗) = 1
2gT
k G−1gk,
(5.5.8)
and since the exact line search factor αk is represented by (5.5.4), we have
f(xk+1) = f(xk) −1
2α2
kgT
k HkGHkgk
and
f(xk+1) −f(x∗) = 1
2gT
k G−1gk −1
2α2
kgT
k HkGHkgk.
Hence
f(xk+1) −f(x∗)
f(xk) −f(x∗)
=
1 −
(gT
k Hkgk)2
(gT
k G−1gk)(gT
k HkGHkgk)
=
1 −
(zT
k zk)2
(zT
k (H
−1
2
k
)T G−1H
−1
2
k
zk)(zT
k H
1
2
k G(H
1
2
k )T zk)
,
(5.5.9)
where zk = H
1
2
k gk. Then the conclusion (5.5.5) is obtained by using Kan-
torovich Theorem 3.1.10.

5.5. SELF-SCALING VARIABLE METRIC (SSVM) METHODS
275
Similarly, we have
E(xk) −E(xk+1)
E(xk)
=
(zT
k zk)2
(zT
k Tkzk)(zT
k T −1
k zk),
where Tk = H
1
2
k GH
1
2
k . By using Kantorovich Theorem 3.1.10 and noting that
HkG and Tk are similar, we immediately obtain the conclusion (5.5.6).
2
From Theorem 5.5.1, we may see that if the condition number κ(Tk) is
very large, the single-step convergence rate will be very slow. In order to
obtain a rapid rate in every iteration, we should make
 λ1 −λn
λ1 + λn
!2
or
κ(Tk) −1
κ(Tk) + 1
2
(5.5.10)
as small as possible, where κ(Tk) = λ1/λn.
Second, let us observe carefully the DFP method. It is not diﬃcult to
see the fact that, usually, the eigenvalues of H0G are greater than 1, and
that DFP method and Broyden class method make one eigenvalue to being
1 in essence in each iteration. Hence, in the iterative procedure, a non-ideal
eigen-ratio of {HkG} is produced. Also since HkG and Tk are similar, the
eigen-ratio of {Tk} is also non-ideal.
In fact, if we let
Rk = G
1
2 HkG
1
2 , rk = G
1
2 sk,
(5.5.11)
then Rk is similar to HkG, and further to Tk. By using yk = G
1
2 sk, the DFP
formula (5.1.30) is equivalent to
Rk+1 = Rk −RkrkrT
k Rk
rT
k Rkrk
+ rkrT
k
rT
k rk
.
(5.5.12)
Let the eigenvalues of Rk satisfy λ1 ≥λ2 ≥· · · ≥λn > 0. Let
P = Rk −RkrkrT
k Rk
rT
k Rkrk
(5.5.13)
with eigenvalues µ1 ≥µ2 ≥· · · ≥µn. Obviously, Prk = 0. Then we have
λ1 ≥µ1 ≥λ2 ≥µ2 ≥· · · ≥λn ≥µn = 0.
(5.5.14)

276
CHAPTER 5. QUASI-NEWTON METHODS
From (5.5.12), it follows that
Rk+1 = P + rkrT
k
rT
k rk
(5.5.15)
and
Rk+1rk = rk.
(5.5.16)
Since rk is the eigenvector of P, and since P is symmetric, then all other
eigenvectors of P are orthogonal to rk. So, the unique diﬀerent eigenvalue
between Rk+1 and P is the eigenvalue associated to rk, which is 1. This
shows that DFP method moves one eigenvalue of Rk to 1 in each iteration.
Note that Rk is similar to HkG, thus, it implies that if all eigenvalues of H0G
are greater than 1, then the eigen-ratio of HkG will worsen.
However, if 1 ∈[λn, λ1], then, it follows from the above discussion that
the eigenvalues µ1, µ2, · · · , µn−1 of Rk+1 and 1 will be contained in [λn, λ1].
Hence, in this case, the eigen-ratio of HkG will not worsen. This conclusion
is true for updates of Broyden class with 0 ≤φ ≤1.
Theorem 5.5.2 Let λ1, λ2, · · · , λn be eigenvalues of HkG with λ1 ≥λ2 ≥
· · · ≥λn > 0. Suppose that 1 ∈[λn, λ1]. Then, for any φ with 0 ≤φ ≤1, the
eigenvalues of Hφ
k+1G are contained in [λn, λ1], where Hφ
k+1 is the Broyden
class update deﬁned by (5.2.4).
Proof.
The case φ = 0 has been proved as before.
Now we consider the case φ = 1 (BFGS update). The BFGS formula
(5.1.45) can be written as
H−1
k+1 = H−1
k
+ ykyT
k
sT
k yk
−H−1
k sksT
k H−1
k
sT
k H−1
k sk
,
which is equivalent to
R−1
k+1 = R−1
k
−R−1
k rkrT
k R−1
k
rT
k R−1
k rk
+ rkrT
k
rT
k rk
.
(5.5.17)
Since the eigenvalues of R−1
k
satisfy
1
λ1
≤1
λ2
≤· · · ≤1
λn
,

5.5. SELF-SCALING VARIABLE METRIC (SSVM) METHODS
277
then we have 1 ∈[1/λ1, 1/λn]. Similar to the above discussion, we know
that if the eigenvalues of R−1
k+1 satisfy 1/µ1 ≤1/µ2 ≤· · · ≤1/µn, then these
eigenvalues are contained in [1/λ1, 1/λn]. Hence, we have that 1/λ1 ≤1/µ1
and 1/λn ≥1/µn, i.e., µn ≥λn and µ1 ≤λ1. This shows that all eigenvalues
of Rk+1 are contained in [λn, λ1]. Therefore, the conclusion holds for φ = 1.
Finally, we know that Broyden class updating formula (5.2.4) is equivalent
to
Rφ
k+1 = Rk −RkrkrT
k Rk
rT
k Rkrk
+ rkrT
k
rT
k rk
+ φukuT
k ,
(5.5.18)
where
uk = G
1
2 vk = (rT
k Rkrk)
1
2

rk
rT
k rk
−
Rkrk
rT
k Rkrk

.
(5.5.19)
Clearly, the eigenvalues of Rφ
k+1 are increasing monotonically as k in-
creases. Since, for φ = 0 and φ = 1, the eigenvalues of Rφ
k+1 are contained
in [λn, λ1], then, for 0 ≤φ ≤1, the eigenvalues of Rφ
k+1 are also contained
in [λn, λ1]. Thus, from the fact that Rφ
k+1 and Hφ
k+1G are similar, we obtain
the conclusion.
2
The above theorem says that if we scale the matrix Hk such that the
eigenvalues of HkG satisfy 1 ∈[λn, λ1], the eigenvalue structure of Rφ
k+1 will
be improved.
Obviously, for a quadratic function, it is enough to scale only the initial
matrix H0. However, in general, it is useful to scale each Hk.
5.5.2
Self-Scaling Variable Metric (SSVM) Method
In this section we describe SSVM method due to Oren [237]. Multiplying Hk
by γk and then replacing Hk by γkHk in (5.2.2) yield
H(φ,γk)
k+1
=

Hk −HkykyT
k Hk
yT
k Hkyk
+ φvkvT
k

γk + sksT
k
sT
k yk
,
(5.5.20)
where
vk = (yT
k Hkyk)
1
2 [sk/sT
k yk −Hkyk/yT
k Hkyk],
where φ is a parameter of Broyden class and γk a self-scaling parameter.
The formula (5.5.20) is referred to as the self-scaling variable metric (SSVM)
formula. When γk = 1, it is reduced to Broyden class update.

278
CHAPTER 5. QUASI-NEWTON METHODS
Algorithm 5.5.3 (SSVM Algorithm)
Step 0. Given an initial matrix H0 and a starting point x0.
Set
k = 0.
Step 1. Set dk = −Hkgk.
Step 2. Find stepsize αk, and set xk+1 = xk + αkdk, compute gk+1
and set yk = gk+1 −gk.
Step 3. Choose Broyden’s class parameter φ ≥0 and self-scaling
parameter γk > 0, and compute H(φ,γk)
k+1
by (5.5.20).
Step 4. k := k + 1, go to Step 1.
2
Similar to the discussion of DFP method in §5.1, we can prove that the
SSVM method has the following properties. The proof is omitted.
Theorem 5.5.4 (Properties of SSVM Method)
1. If Hk is positive deﬁnite and sT
k yk > 0, then when φ ≥0 and γk > 0,
the matrix H(φ,γk)
k+1
produced by (5.5.20) is positive deﬁnite.
2. If f(x) is a quadratic function with Hessian G, the vectors s0, s1, · · · , sn−1
produced by SSVM method are G-conjugate, i.e., satisfy
sT
i Gsj = 0, i ̸= j; i, j = 0, 1, · · · , n −1,
(5.5.21)
and for each k, s0, s1, · · · , sk are the eigenvalues of H(φ,γk)
k+1
G, i.e., sat-
isfy
H(φ,γk)
k+1
Gsi = ¯γi,ksi, 0 < i < k,
(5.5.22)
where ¯γi,k = 0k
j=i+1 γj, ¯γii = 1.
This theorem shows that although the property H(φ,γn−1)
n
= G−1 is not
retained for quadratic functions by SSVM method, the property of conjugate
directions is still retained. Therefore, for quadratic functions, the sequence
generated from SSVM method converges to a minimizer in at most n steps.

5.5. SELF-SCALING VARIABLE METRIC (SSVM) METHODS
279
5.5.3
Choices of the Scaling Factor
Now, the problem is how to choose a suitable scaling factor. Let λ1 ≥λ2 ≥
· · · ≥λn > 0 be eigenvalues of HkG. Clearly, they are also the eigenvalues
of Rk. We hope to choose a suitable scaling factor which is used to multiply
Hk, such that 1 is contained among the new eigenvalues and thus the eigen-
structure is improved. Therefore we get κ(Rφ
k+1) ≤κ(Rk). The following
theorem is a consequence of Theorem 5.5.2.
Theorem 5.5.5 Let φ ∈[0, 1] and γk > 0. Let Rk and Rφ
k+1 be deﬁned
respectively by (5.5.11) and (5.5.24). Let λ1 ≥λ2 ≥· · · ≥λn and µφ
1 ≥µφ
2 ≥
· · · ≥µφ
n be eigenvalues of Rk and Rφ
k+1 respectively. Then the following
statements hold.
1. If γkλn ≥1, then µφ
n = 1 and 1 ≤γkλi+1 ≤µφ
i ≤γkλi, i = 1, 2, · · · , n −
1.
2. If γkλ1 ≤1, then µφ
1 = 1 and γkλi ≤µφ
i ≤γkλi−1 ≤1, i = 2, 3, · · · , n.
3. If γkλn ≤1 ≤γkλ1 and i0 is an index with γkλi0+1 ≤1 ≤γkλi0, then
γkλ1 ≥µφ
1 ≥γkλ2
≥
µφ
2 ≥· · · ≥γkλi0 ≥µi0 ≥1 ≥µi0+1
≥
γkλi0+1 ≥· · · ≥γkλn,
(5.5.23)
and there is at least one eigenvalue in µφ
i0 and µφ
i0+1 which equals 1.
Proof.
This theorem is a direct consequence of Theorem 5.5.2.
Since
SSVM method is equivalent to
Rφ
k+1 =

Rk −RkrkrT
k Rk
rT
k Rkrk
+ φukuT
k

γk + rkrT
k
rT
k rk
,
(5.5.24)
where rk and uk are deﬁned respectively by (5.5.11) and (5.5.19), the above
expression is just obtained by replacing Rk by γkRk in (5.5.18). Therefore,
from Theorem 5.5.2, replacing λ1, λ2, · · · , λn by use of γkλ1, · · · , γkλn gives
our conclusion.
2
Corollary 5.5.6 Let φ ∈[0, 1] and γk = 1. Then
|µφ
k −1| ≤|λk −1|.
(5.5.25)

280
CHAPTER 5. QUASI-NEWTON METHODS
Proof.
From Theorem 5.5.5, for γk = 1, one of the following cases will
hold:
(a) λi ≥µφ
i ≥1;
(b) λi ≤µφ
i ≤1.
Hence the conclusion (5.5.25) is obtained.
2
Obviously, if we choose γk such that
λn ≤1
γk
≤λ1,
(5.5.26)
we have
γkλn ≤1 ≤γkλ1,
(5.5.27)
which says that 1 is included in the interval of scaled eigenvalues. In addition,
we have
Corollary 5.5.7 Let φ ∈[0, 1] and γk > 0. Let κ(·) denote the condition
number. If λn ≤
1
γk ≤λ1, then, for (5.5.24), we have
κ(Rφ
k+1) ≤κ(Rk).
(5.5.28)
Proof.
From Theorem 5.5.5 (3), it follows that
γkλ1 ≥µφ
1 ≥1 ≥µφ
n ≥γkλn,
(5.5.29)
which gives
µφ
1
µφ
n
≤λ1
λn
.
Thus, we complete the proof.
2
In the above discussion about the condition of γk, we always restrict the
Broyden class parameter φ ∈[0, 1]. In fact, this restriction is suﬃcient and
also necessary for the statement that if λn ≤
1
γk ≤λ1, then κ(Rφ
k+1) ≤κ(Rk)
and H(φ,γk)
k+1
is positive deﬁnite.
Corollary 5.5.7 says that λn ≤
1
γk ≤λ1 is a suitable requirement to choose
a scaling factor. Note that
rT
k Rkrk
rT
k rk
= yT
k Hkyk
sT
k yk

5.5. SELF-SCALING VARIABLE METRIC (SSVM) METHODS
281
and
λn ≤rT
k Rkrk
rT
k rk
≤λ1,
it follows that
γk =
sT
k yk
yT
k Hkyk
(5.5.30)
is a suitable scaling factor. Similarly, since
rT
k R−1
k rk
rT
k rk
= sT
k H−1
k sk
sT
k yk
and
1
λ1
≤rT
k R−1
k rk
rT
k rk
≤1
λn
,
we have that
γk = sT
k H−1
k sk
sT
k yk
= −αksT
k gk
sT
k yk
=
sT
k gk
gT
k Hkyk
(5.5.31)
is also a suitable scaling factor. Noting that when αk is an optimal stepsize,
we have that sT
k yk = −sT
k gk, and thus
γk = αk.
(5.5.32)
The above (5.5.32) shows an interesting fact, that we may choose directly an
optimal stepsize as a scaling factor.
For any ω ∈[0, 1],
γk = (1 −ω) sT
k yk
yT
k Hkyk
+ ωsT
k H−1
k sk
sT
k yk
(5.5.33)
is a convex combination of (5.5.30) and (5.5.31). Hence (5.5.33) gives a convex
class of suitable scaling factors. For this convex class, Oren [239] presented
the following switch rule of parameters φ and ω.
If
sT
k yk
yT
k Hkyk > 1, choose φ = 1 and ω = 0,
(i.e., φ = 1, γk = sT
k yk/yT
k Hkyk).
If sT
k H−1
k
sk
sT
k yk
< 1, choose φ = 0, ω = 1.
(i.e., φ = 0, γk = sT
k H−1
k
sk
sT
k yk
).

282
CHAPTER 5. QUASI-NEWTON METHODS
If
sT
k yk
yT
k Hkyk ≤1 ≤sT
k H−1
k
sk
sT
k yk
, choose
ω = φ =
sT
k yk(yT
k Hkyk−sT
k yk)
(sT
k H−1
k
sk)(yT
k Hkyk)−(sT
k yk)2 , (i.e., γk = 1).
Another technique is an initial scaling method presented by Shanno and
Phua [306]. At the beginning, set H0 = I, and the stepsize α0 is determined
by some line search, such that the objective function descends suﬃciently.
Before computing H1, instead of H0, we use
ˆH0 = α0H0,
(5.5.34)
and compute H1 from ˆH0, where α0 is a stepsize or determined by
α0 := γ0 =
sT
0 y0
yT
0 H0y0
.
(5.5.35)
The diﬀerence between the initial scaling and SSVM is that SSVM does
process scaling in each iteration, but the initial scaling method does only at
the beginning. Numerical experiments show that the initial scaling is simple
and eﬀective for a lot of problems in which the curvature changes smoothly.
By the way, a special self-scaling BFGS formula
Bk+1 =
sT
k yk
sT
k Bksk

Bk −BksksT
k Bk
sT
k Bksk

+ ykyT
k
sT
k yk
(5.5.36)
is used widely in practice.
5.6
Sparse Quasi-Newton Methods
Schubert [303] ﬁrst extended quasi-Newton update to an unsymmetric sparse
matrix and proposed a sparse quasi-Newton method for solving nonlinear
equations. Powell and Toint [276], Toint [341] derived sparse quasi-Newton
update respectively, and Steihaug [321] presented a sparse quasi-Newton
method with preconditioning and established the convergence.
The sparse quasi-Newton method requires generating sparse quasi-Newton
updates which have the same (or similar) sparsity pattern as the true Hes-
sian. It means that the current Hessian approximation Bk reﬂects the nonzero
structure of the true Hessian, i.e.,
(Bk)ij = 0 for (i, j) ∈I,
(5.6.1)

5.6. SPARSE QUASI-NEWTON METHODS
283
where
I ∆= {(i, j) | [∇2f(x)]ij = 0}
(5.6.2)
is a set of integer pairs. We also deﬁne
J ∆= {(i, j) | [∇2f(x)]ij ̸= 0}.
(5.6.3)
It says that J, a set of integer pairs, is a complement of I. So, we demand
that Bk+1 satisﬁes the quasi-Newton condition
Bk+1sk = yk,
(5.6.4)
and keeps symmetry and sparsity. Neglecting the subscript, we would like to
ﬁnd ¯B, such that
¯B = B + E,
(5.6.5)
where E satisﬁes
Es = y −Bs,
(5.6.6)
E = ET ,
(5.6.7)
Eij = 0, (i, j) ∈I,
(5.6.8)
where Eij are elements of the matrix E. If we determined E, we can get
¯B from (5.6.5). However, (5.6.6)–(5.6.8) cannot determine completely the
matrix E. So, to this end, we require that ¯B is as close as possible to B in
Frobenius norm. Therefore, we consider the following minimization problem:
min
1
2∥E∥2
F
(5.6.9)
s.t.
Es = r,
(5.6.10)
E = ET ,
(5.6.11)
Eij = 0, (i, j) ∈I,
(5.6.12)
where r is assumed to be
r = y −Bs.
(5.6.13)
In the left part of the section, we denote the j-th component of the vector
s by sj, and deﬁne the component of vector s(i) as
s(i)j =

sj,
(i, j) ∈J
0,
(i, j) ∈I.
(5.6.14)

284
CHAPTER 5. QUASI-NEWTON METHODS
Then the condition (5.6.10) can be written as
n

j=1
Eijs(i)j = ri, i = 1, · · · , n.
(5.6.15)
In order to let E be symmetric, take
E = 1
2(A + AT ).
(5.6.16)
Then the problem (5.6.9)-(5.6.12) becomes the following problem: ﬁnding a
matrix A, such that
min
1
8∥A + AT ∥F
(5.6.17)
s.t.
n

j=1
(Aij + Aji)s(i)j = 2ri, i = 1, · · · , n,
(5.6.18)
where Aij denote the elements of A.
Now, we discuss solving the problem (5.6.17)–(5.6.18). The Lagrangian
function is
Φ(A, λ)
=
1
8
n

i=1
n

j=1
(A2
ij + A2
ji + 2AijAji)
−
n

i=1
λi
⎡
⎣
n

j=1
(Aij + Aji)s(i)j −2ri
⎤
⎦.
(5.6.19)
Setting the derivative with respect to Aij to be zero, we have
∂Φ(A, λ)
∂Aij
=
1
2(Aij + Aji) −λis(i)j −λjs(j)i = 0,
i, j = 1, · · · , n.
(5.6.20)
By using (5.6.16), the above expression is just
Eij = λis(i)j + λjs(j)i, i, j = 1, · · · , n.
(5.6.21)
In place of (5.6.18), we employ (5.6.15). Substituting (5.6.21) into (5.6.15),
we obtain
n

j=1
[λis(i)j + λjs(j)i]s(i)j = ri, i = 1, · · · , n,
(5.6.22)

5.6. SPARSE QUASI-NEWTON METHODS
285
which is
λi
n

j=1
[s(i)j]2 +
n

j=1
λjs(j)is(i)j = ri, i = 1, · · · , n.
(5.6.23)
Thus, we derive the update formula
¯B = B + E,
(5.6.24)
which is, from (5.6.21), that
¯B = B +
n

i=1
λi[eis(i)T + s(i)eT
i ],
(5.6.25)
where ei is the i-th unit vector and λ is a Lagrange multiplier vector satisfying
Qλ = r,
(5.6.26)
where
Q =
n

i=1
(s(i)T sei + eT
i ss(i))eT
i .
(5.6.27)
In fact, as long as we notice that
Qλ
=
r = Es =
n

i=1
λi[eis(i)T s + s(i)eT
i s]
=
n

i=1
[s(i)T sei + eT
i ss(i)]eT
i λ,
we can immediately obtain (5.6.27).
The matrix Q deﬁned above satisﬁes symmetry, sparsity and positive
deﬁniteness. The properties of symmetry and sparsity can be seen direct
from (5.6.27).
As to the positive deﬁniteness of Q, we give the following
theorem.
Theorem 5.6.1 If all vectors s(i) (i = 1, · · · , n) are nonzero, then the matrix
Q is positive deﬁnite, that is
zT Qz > 0, ∀z ∈Rn, z ̸= 0.
(5.6.28)

286
CHAPTER 5. QUASI-NEWTON METHODS
Proof.
Take z ̸= 0, z ∈Rn. Let zi denote the components of vector z.
From (5.6.27),
zT Qz
=
n

i=1
n

j=1
zT
i Qijzj
=
n

i=1
n

i=1
zis(i)js(j)izj +
n

i=1
n

j=1
[s(i)j]2z2
i
=

(i,j)∈J
[zisisjzj + z2
i s2
j]
=
1
2

(i,j)∈J
[sizj + sjzi]2
=
2
n

i=1
z2
i s2
i + 1
2

(i,j)∈J
i̸=j
(zisj + zjsi)2
≥
0.
(5.6.29)
Suppose that zT Qz = 0; since z ̸= 0, there exists a component of z, for
example, zk ̸= 0, such that by (5.6.29) we have
zksk = 0,
(5.6.30)
zksj + zjsk = 0, (k, j) ∈J, j ̸= k.
(5.6.31)
Thus, sk = 0. Furthermore, sj = 0, j ̸= k, (k, j) ∈J. This is equivalent to
s(k) = 0, which contradicts the assumption. We complete the proof.
2
Since Q is positive deﬁnite, it follows from (5.6.21) and (5.6.26) that
Eij = (Q−1r)is(i)j + (Q−1r)js(j)i,
(5.6.32)
which can be written as
Eij =

0,
(i, j) ∈I,
λisj + λjsi,
(i, j) ∈J.
(5.6.33)
The above discussion gives the derivation of general sparse quasi-Newton
update.
Now, we turn to the sparse PSB update.
Let F : Rn →Rn. For solving sparse nonlinear equations F(x) = 0,
Schuburt [303] ﬁrst suggested that Broyden’s rank-one update
¯B = B + (y −Bs)sT
sT s
(5.6.34)

5.6. SPARSE QUASI-NEWTON METHODS
287
can be written in the following form
¯B = B +
n

i=1
eieT
i
(y −Bs)sT
sT s
,
(5.6.35)
which is an update by row, where ei is the i-th unit vector. By use of notation
s(i), one knows that
¯B = B +
n

i=1
eieT
i
(y −Bs)s(i)T
s(i)T s
(5.6.36)
satisﬁes the quasi-Newton condition ¯Bs = y, and has the sparsity pattern
desired.
The general form of Schuburt sparse update is
¯B = B +
n

i=1
αieiz(i)T ,
(5.6.37)
where
αi = eT
i (y −Bs)
s(i)T s
, z(i)j =

zj,
(i, j) ∈J,
0,
(i, j) ∈I.
(5.6.38)
Now we employ symmetrization to (5.6.37) and deduce that
¯B = B +
n

i=1
αi(eiz(i)T + z(i)eT
i ).
(5.6.39)
Let us choose αi, such that ¯B satisﬁes the quasi-Newton condition. Obvi-
ously, ¯B is symmetric and satisﬁes sparsity.
Similar to the discussion before, we can obtain that α satisﬁes
Tα = r,
(5.6.40)
where
T =
n

i=1
[z(i)T sei + eT
i sz(i)]eT
i .
(5.6.41)
In particular, setting z(i) = s(i), we immediately get (5.6.25)–(5.6.27), which
is sparse PSB update.
Next, let us proceed to the sparse BFGS update.

288
CHAPTER 5. QUASI-NEWTON METHODS
For clarity, we repeat the BFGS update given in (5.1.45):
¯B = B + yyT
sT y −BssT B
sT Bs ,
(5.6.42)
where B is assumed to have some sparsity pattern. Since the ¯B deﬁned by
the above formula has not such a structure, we modify it and make it have
this kind of sparsity structure. Deﬁne
ˆB = ¯B + E.
(5.6.43)
We demand that ˆB satisﬁes the following conditions:
(i) ˆB satisﬁes the quasi-Newton condition.
(ii) ˆB is symmetric.
(iii) ˆB is the closest to ¯B in Frobenius norm.
So, we consider the following minimization problem:
min
∥E∥F = 1
2Tr(ET E)
(5.6.44)
s.t.
Es = 0,
(5.6.45)
Eij = −¯Bij, (i, j) ∈I,
(5.6.46)
E = ET .
(5.6.47)
To solve (5.6.44)–(5.6.47), we deﬁne the Lagrange function Φ as follows:
Φ(E, µ, Λ, λ)
=
1
2Tr(ET E) −Tr(EsµT ) −Tr(Λ(E −ET ))
−

(i,j)∈I
λijTr(E + ¯B)ejeT
i
=
1
2Tr(ET E) −Tr(EsµT ) −Tr(Λ(E −ET ))
−Tr(ΛT (E + ¯B)),
(5.6.48)
where µ is the multiplier vector, Λ and ∆are multiplier matrices, and λij
are the elements of the matrix ∆. When (i, j) ∈J, λij = 0.
Diﬀerentiating (5.6.48) and setting ∂Φ
∂E = 0, we have
∂Φ
∂E = E −sµT −ΛT + Λ −∆= 0,
(5.6.49)

5.6. SPARSE QUASI-NEWTON METHODS
289
which gives
E = sµT + ∆−Λ + ΛT
(5.6.50)
and
ET = µsT + ∆T −ΛT + Λ.
(5.6.51)
By using (5.6.47), we get
E −ET = sµT + ∆−∆T + 2(ΛT −Λ) = 0,
(5.6.52)
that is
Λ −ΛT = 1
2(sµT −µsT + ∆−∆T ).
(5.6.53)
By use of (5.6.50) and (5.6.53), we have
E = 1
2(sµT + µsT + ∆+ ∆T ),
(5.6.54)
which gives, by (5.6.46), that
eT
i Eej = 1
2(eT
i µsT ej + eT
i sµT ej + λij + λji) = −¯Bij,
that is
λij + λji = −2 ¯Bij −eT
i µsT ej −eT
i sµT ej, (i, j) ∈I.
(5.6.55)
The above expression can be written in matrix form:
∆+ ∆T = −2 ¯B(I)
ij −
n

i=1
eieT
i (µˆs(i)T + sˆµ(i)T ),
(5.6.56)
where
¯B(I)
ij =
 ¯Bij,
(i, j) ∈I,
0,
(i, j) ∈J,
(5.6.57)
ˆs(i)j =

sj,
(i, j) ∈I,
0,
(i, j) ∈J,
ˆµ(i)j =

µj,
(i, j) ∈I,
0,
(i, j) ∈J.
(5.6.58)
By (5.6.54) and (5.6.56), we deduce that
E
=
1
2
 n

i=1
eieT
i (µsT + sµT ) −2 ¯B(I) −
n

i=1
eieT
i (µˆs(i)T + sˆµ(i)T )

=
1
2
 n

i=1
eieT
i (µs(i)T + sµ(i)T ) −2 ¯B(I)

,
(5.6.59)

290
CHAPTER 5. QUASI-NEWTON METHODS
where
µ(i)j =

µi,
(i, j) ∈J,
0,
(i, j) ∈I.
(5.6.60)
Also, by (5.6.45), we have
Es = 1
2
 n

i=1
eieT
i (µs(i)T + sµ(i)T ) −2 ¯B(I)

s = 0,
(5.6.61)
which is
n

i=1
eieT
i (µs(i)T s + sµ(i)T s) = 2 ¯B(I)s.
(5.6.62)
Note that
n

i=1
eieT
i sµ(i)T =
n

i=1
µis(i)eT
i ,
and we can rewrite (5.6.62) as
n

i=1
µi(eis(i)T + s(i)eT
i )s = t,
(5.6.63)
where t = 2 ¯B(I)s.
Then, provided that we solve (5.6.63) for µi and substitute µi into (5.6.59),
we can deduce that
E = 1
2
n

i=1
µi(eis(i)T + s(i)eT
i ) −¯B(I).
(5.6.64)
Thus,
ˆB
=
¯B + E
=
¯B + 1
2
n

i=1
µi(eis(i)T + s(i)eT
i ) −¯B(I)
=
¯B(J) + 1
2
n

i=1
µi(eis(i)T + s(i)eT
i )
(5.6.65)
where
¯B(J)
ij
=
 ¯Bij,
(i, j) ∈J,
0,
(i, j) ∈I.
(5.6.66)

5.6. SPARSE QUASI-NEWTON METHODS
291
The formula (5.6.65) is said to be sparse BFGS update. Similarly, we can
derive the sparse update for other quasi-Newton updates.
Note that the above formula (5.6.65) is obtained by minimization of prob-
lem (5.6.44)–(5.6.47) in Frobenius norm. Instead, we consider this minimiza-
tion problem in the weighted Frobenius norm, i.e., consider the problem
min
∥E∥W,F = 1
2Tr(WET WE)
(5.6.67)
s.t.
Es = 0,
(5.6.68)
Eij = −¯Bij, (i, j) ∈I,
(5.6.69)
E = ET .
(5.6.70)
Then, corresponding to (5.6.54), we have
E = 1
2[z(sT M) + (Ms)zT + M(∆+ ∆T )M],
(5.6.71)
where M = W −1, z = Mµ.
Set p = Ms. We can obtain that if and only if M(∆+∆T )M and ∆+∆T
have the same sparsity pattern, the solution of (5.6.67)–(5.6.70) is
ˆB = ¯B(I) +
n

i=1
zi(eip(i)T + p(i)eT
i ),
(5.6.72)
where
p(i)j =

pj,
(i, j) ∈J,
0,
(i, j) ∈I,
(5.6.73)
zi is the solution of the equations
n

i=1
zi(eip(i)T + p(i)eT
i )s = 2 ¯B(I)s.
(5.6.74)
Clearly, if W is a positive deﬁnite and diagonal matrix, M(∆+ ∆T )M and
∆+ ∆T have the same sparsity structure.
Toint [341] considered sparse quasi-Newton update in the case that the
weighted matrix is a non-diagonal matrix. For solving eﬃciently the sparse
equations about µi, Steihaug [321] presented a preconditioned conjugate gra-
dient method to solve the linear equations.
An alternative approach is to relax the quasi-Newton equation, making
sure that it is approximately satisﬁed along the last few steps rather than

292
CHAPTER 5. QUASI-NEWTON METHODS
requiring it to hold strictly on the latest step. Deﬁne the n × m matrices Sk
and Yk by
Sk = [sk−m, · · · , sk−1], Yk = [yk−m, · · · , yk−1].
(5.6.75)
We ask Bk+1 to be a solution of
min
∥¯BSk −Yk∥2
F
(5.6.76)
s.t.
¯B = ¯BT ,
(5.6.77)
¯Bij = 0, (i, j) ∈I.
(5.6.78)
In general, sparse quasi-Newton methods lost some advantages of dense
quasi-Newton methods.
(1) Because of the complexity of the sparse pattern, the modiﬁed matrix E
is a rank-n matrix, rather than a rank-two matrix.
(2) To compute the matrix E, we must solve a sparse linear equation about
µi.
(3) The positive deﬁniteness of the update matrix {Bk} cannot be guaran-
teed.
(4) So far, the numerical performance is not ideal.
We think that it is still a challenging topic to solve large-scale optimization
problems by studying sparse quasi-Newton methods.
5.7
Limited Memory BFGS Method
Limited memory quasi-Newton methods are useful for solving large-scale op-
timization problems. For large-scale problems, the methods save only a few
n-dimensional vectors, instead of storing and computing fully dense n × n
approximations of the Hessian.
Since BFGS method is the most eﬃcient
method for solving unconstrained optimization problems, in this section we
consider the limited memory BFGS method, known as L-BFGS, which is
based on BFGS method.
As we know that the BFGS formula for inverse Hessian approximation
Hk is
Hk+1 =

I −skyT
k
sT
k yk

Hk

I −yksT
k
sT
k yk

+ sksT
k
sT
k yk
.
(5.7.1)

5.7. LIMITED MEMORY BFGS METHOD
293
Set
ρk =
1
sT
k yk
,
Vk = I −ρyksT
k ,
(5.7.2)
then
Hk+1 = V T
k HkVk + ρksksT
k .
(5.7.3)
The above equation says that the matrix Hk+1 is obtained by updating Hk
using the pair {sk, yk}. In L-BFGS method we save implicitly a modiﬁed
version of Hk by storing m pairs {si, yi}(i = k −m, k −m + 1, · · · , k −1).
In the following, we describe the expression of the updating matrix Hk of
the k-th iteration in L-BFGS method.
Choose some initial Hessian approximation H(0)
k
for the k-th iteration.
We apply the formula (5.7.3) m times repeatedly, i.e.,
H(j+1)
k
= V T
k−m+jH(j)
k Vk−m+j + ρk−m+jsk−m+jsT
k−m+j, j = 0, 1, · · · , m −1,
(5.7.4)
and obtain
Hk
=
(V T
k−1 · · · V T
k−m)H(0)
k (Vk−mVk−m+1 · · · Vk−1)
+ρk−m(V T
k−1 · · · V T
k−m+1)sk−msT
k−m(Vk−m+1 · · · Vk−1)
+ρk−m+1(V T
k−1 · · · V T
k−m+2)sk−m+1sT
k−m+1(Vk−m+2 · · · Vk−1)
+ · · ·
+ρk−1sk−1sT
k−1.
(5.7.5)
It follows from the above expression that if we know pairs {si, yi}(i = k −
m, k −m + 1, · · · , k −1), we can compute Hk. In fact, we need not compute
and save Hk explicitly, instead, we only save the pairs {si, yi} and compute
Hkgk, where gk is the gradient of f at xk. So, we have
Hkgk
=
(V T
k−1 · · · V T
k−m)H(0)
k (Vk−mVk−m+1 · · · Vk−1)gk
+ρk−m(V T
k−1 · · · V T
k−m+1)sk−msT
k−m(Vk−m+1 · · · Vk−1)gk
+ρk−m+1(V T
k−1 · · · V T
k−m+2)sk−m+1sT
k−m+1(Vk−m+2 · · · Vk−1)gk
+ · · ·
+ρk−1sk−1sT
k−1gk.
(5.7.6)
Since
Vigk = (I −ρiyisT
i )gk, i = k −1, k −2, · · · , k −m,
we have the following algorithm to compute Hkgk.

294
CHAPTER 5. QUASI-NEWTON METHODS
Algorithm 5.7.1 (L-BFGS two-loop recursion for Hkgk)
Step 1. q := gk;
Step 2. for i = k −1, k −2, · · · , k −m
αi := ρisT
i q;
q := q −αiyi;
end (for)
Step 3. r := H(0)
k q;
Step 4. for i = k −m, k −m + 1, · · · , k −1
β := ρiyT
i r;
r := r + si(αi −β)
end (for)
2
By use of the above algorithm, we obtain r = Hkgk. A choice of H(0)
k
is
H(0)
k
= sT
k yk
∥yk∥2 I.
(5.7.7)
The limited memory BFGS algorithm can be stated as follows.
Algorithm 5.7.2 (L-BFGS Method)
Step 1. Given a starting point x0 ∈Rn, an initial symmetric and
positive deﬁnite matrix H0 ∈Rn×n, a nonnegative integer
m ≥0, an error tolerance ϵ > 0, k := 0.
Step 2. Compute gk = ∇f(xk). If ∥gk∥≤ϵ, we take x∗= xk, stop;
otherwise, compute dk = −Hkgk from Algorithm 5.7.1.
Step 3. Find a step size αk > 0 by using Wolfe rule.
Step 4. Set xk+1 = xk + αkdk.
Step 5. If k > m, discard the vector pairs {sk−m, yk−m} from stor-
age;
Set sk = xk+1 −xk, yk = gk+1 −gk;
Take H(0)
k
= sT
k yk
∥yk∥2 I.
Step 6. k := k + 1 and go to Step 2.
2

5.7. LIMITED MEMORY BFGS METHOD
295
The above L-BFGS algorithm is equivalent to the usual BFGS algorithm
if the initial matrix H0 is the same in both algorithms, and if H(0)
k
= H0
at each iteration. Normally, for large-scale problems, we take m ≪n. In
practice, the choice of m is dependent on the dimension of the problem and
the storage of employed computer. Usually, we take 3 ≤m ≤30.
In the following, we establish the convergence and convergence rate of
L-BFGS method.
Lemma 5.7.3 Let f(x) be a twice continuously diﬀerentiable and uniformly
convex function, i.e., there exist 0 < m ≤M such that
m∥u∥2 ≤uT G(x)u ≤M∥u∥2, ∀x ∈L(x0), u ∈Rn,
(5.7.8)
where G(x) = ∇2f(x) and L(x0) = {x | f(x) ≤f(x0)}. Then
∥y∥
∥sk∥≤M,
∥sk∥2
sT
k yk
≤1
m,
∥yk∥2
sT
k yk
≤M.
(5.7.9)
Proof.
1) Let ¯G =
1 1
0 G(xk + τsk)dτ. Then
yk = gk+1 −gk =
 1
0
G(xk + τsk)skdτ = ¯Gsk.
(5.7.10)
Taking the norm, then we obtain
∥yk∥≤∥sk∥
 1
0
∥G(xk + τsk)∥dτ.
(5.7.11)
From the assumptions, it follows that L(x0) is a bounded, closed and convex
set, then xk + τsk ∈L(x0). Then ∥G(xk + τsk)∥≤M. Thus we have that
∥yk∥≤M∥sk∥which is the ﬁrst conclusion in (5.7.9).
2) By use of (5.7.10), we have that
sT
k yk =
 1
0
sT
k G(xk + τsk)skdτ ≥m∥sk∥2,
(5.7.12)
which means
∥sk∥2
sT
k yk
≤1
m.

296
CHAPTER 5. QUASI-NEWTON METHODS
3) Since yk = ¯Gsk, then
∥yk∥2
sT
k yk
=
yT
k yk
sT
k yk
= sT
k ¯G
1
2 ¯G ¯G
1
2 sk
sT
k ¯G
1
2 ¯G
1
2 sk
=
γT
k ¯Gγk
γT
k γk
≤M,
(5.7.13)
where γk = ¯G
1
2 sk.
2
Theorem 5.7.4 Let f(x) be a twice continuously diﬀerentiable and uni-
formly convex function.
Then the iterative sequence {xk} generated from
L-BFGS Algorithm 5.7.2 converges to the unique minimizer x∗of f(x).
Proof.
From Lemma 5.7.3, we have
∥sk∥2
sT
k yk
≤M, ∥yk∥2
sT
k yk
≤M.
(5.7.14)
Then
∥Vk∥≤1 + M.
(5.7.15)
Let ¯m = min{k, m}. Without loss of generality, we assume that ∥H(0)
k ∥≤
M. Then by (5.7.5) and (5.7.14)-(5.7.15) we get
∥Hk∥
≤
M(1 + M)2 ¯m +
¯m

j=1
M(1 + M)2( ¯m−j)
≤
M(1 + M)2 ¯m( ¯m + 1).
(5.7.16)
On the other hand, write B(0)
k
= (H(0)
k )−1. From (5.7.4) we have
B(j+1)
k
= B(j)
k −
B(j)
k sk−¯m+jsT
k−¯m+j(B(j)
k )T
sT
k−¯m+jB(j)
k sk−¯m+j
+
yk−¯m+jyT
k−¯m+j
yT
k−¯m+jsk−¯m+j
, j = 0, 1, ¯m−1.
Then
B( ¯m)
k
= Bk = H−1
k .
Since Tr(xyT ) = xT y for x, y ∈Rn and Tr(A + B) =Tr(A)+Tr(B) for
n × n matrices A and B, then it follows from (5.7.14) that
Tr(B(j+1)
k
)
= Tr(B(j)
k ) −
∥B(j)
k sk−¯m+j∥2
sT
k−¯m+jB(j)
k sk−¯m+j
+
∥yk−¯m+j∥2
yT
k−¯m+jsk−¯m+j
≤Tr(B(j)
k ) + M.
(5.7.17)

5.7. LIMITED MEMORY BFGS METHOD
297
Repeatedly applying (5.7.17) ¯m times, and using (5.7.7) and (5.7.14), we
obtain that
Tr(Bk)
=
Tr(B( ¯m)
k
) ≤Tr(B(0)
k ) + ¯mM
=
Tr((H(0)
k )−1) + ¯mM
≤
(n + ¯m)M.
(5.7.18)
Let the eigenvalues of Bk be 0 < λ1 ≤· · · ≤λn, then the eigenvalues of
Hk are
0 < 1
λn
≤
1
λn−1
≤· · · ≤1
λ1
.
By use of the property of the Rayleigh quotient and Tr(Bk) = 
n
j=1 λj, we
obtain
cos θk
=
−dT
k gk
∥dk∥∥gk∥=
gT
k Hkgk
∥Hkgk∥∥gk∥
≥∥gk∥2/λn
∥Hk∥∥gk∥2 =
1
λn∥Hk∥
≥
1
Tr(Bk)∥Hk∥.
(5.7.19)
Then, it follows from (5.7.16) and (5.7.18) that there is a ρ > 0 such that
cos θk ≥ρ
(5.7.20)
holds for all k. This implies that there is ¯µ > 0 such that
θk ≤π
2 −¯µ, ∀k.
(5.7.21)
The assumptions of the theorem and Theorem 1.3.19 indicate that the
level L(x0) is bounded, closed and convex. Then, the continuous function
∇f(x) exists and is uniformly continuous on L(x0). Noting that αk is deter-
mined by Wolfe rule, then we obtain, by Theorem 2.5.5 and (5.7.21), that
the sequence {xk} converges to the unique minimizer x∗of f(x).
2
Next, we establish the convergence rate of L-BFGS method.
Lemma 5.7.5 Let f(x) be a twice continuously diﬀerentiable and uniformly
convex function. Then
f(x) −f(x∗) ≤1
m∥g(x)∥2.
(5.7.22)

298
CHAPTER 5. QUASI-NEWTON METHODS
Proof.
Since f(x) is a convex function, for any x ∈Rn we have
f(x) −f(x∗) ≤g(x)T (x −x∗) ≤∥g(x)∥∥x −x∗∥.
(5.7.23)
Note that
g(x) = g(x) −g(x∗) =
 1
0
G(x∗+ τ(x −x∗))(x −x∗)dτ.
(5.7.24)
Writing ¯G =
1 1
0 G(x∗+ τ(x −x∗))dτ, then we have
g(x) = ¯G(x −x∗).
(5.7.25)
By use of (5.7.8), we get
m∥x −x∗∥2
≤
(x −x∗)T ¯G(x −x∗) ≤(x −x∗)T g(x)
≤
∥x −x∗∥∥g(x)∥,
(5.7.26)
that is
∥x −x∗∥≤∥g(x)∥/m.
(5.7.27)
Substituting (5.7.27) into (5.7.23) gives (5.7.22).
2
Lemma 5.7.6 Let f(x) be a twice continuously diﬀerentiable and uniformly
convex function. Let xk+1 = xk + αkdk, where αk is determined by Wolfe
rule. Then
c1∥gk∥cos θk ≤∥sk∥≤c2∥gk∥cos θk
(5.7.28)
and
f(xk+1) −f(x∗) ≤(1 −ρmc1 cos2 θk)[f(xk) −f(x∗)],
(5.7.29)
where c1 = (1 −σ)/M, c2 = 2(1 −ρ)/m, ρ and σ are deﬁned by Wolfe rule,
θk is an angle between dk and −gk.
Proof.
From (5.7.9) we have that
yT
k sk
∥sk∥2 ≤∥yk∥∥sk∥
∥sk∥2
= ∥yk∥
∥sk∥≤M.
(5.7.30)
By using Wolfe rule, we have that
yT
k sk = gT
k+1sk −gT
k sk ≥−(1 −σ)gT
k sk.
(5.7.31)

5.7. LIMITED MEMORY BFGS METHOD
299
Then the above expressions give
∥sk∥2 ≥yT
k sk
M
≥−1 −σ
M
gT
k sk = 1 −σ
M
∥gk∥∥sk∥cos θk,
that is
∥sk∥≥1 −σ
M
∥gk∥cos θk.
(5.7.32)
We obtain the left-hand side inequality of (5.7.28).
By Taylor expression and Wolfe rule, we have that
gT
k sk + 1
2sT
k G(ξk)sk = f(xk+1) −f(xk) ≤ρgT
k sk,
(5.7.33)
where ξk lies between xk and xk+1. Then
sT
k G(ξk)sk ≤−2(1 −ρ)gT
k sk.
(5.7.34)
Since L(x0) is a bounded, closed and convex set, ξk ∈L(x0). Then we have
that
m∥sk∥2 ≤sT
k G(ξk)sk.
(5.7.35)
The inequalities (5.7.34) and (5.7.35) yield
∥sk∥≤2(1 −ρ)
M
∥gk∥cos θk,
which is the right-hand side of (5.7.28).
Finally, we prove (5.7.29). By using Wolfe rule and (5.7.28), we have that
f(xk+1) −f(xk)
≤
ρgT
k sk = −ρ∥gk∥∥sk∥cos θk
≤
−ρc1∥gk∥2 cos2 θk.
(5.7.36)
From Lemma 5.7.5, we have
∥gk∥2 ≥m[f(xk) −f(x∗)].
(5.7.37)
So, we can substitute (5.7.37) into (5.7.36) to obtain that
f(xk+1) −f(xk) ≤−ρmc1 cos2 θk[f(xk) −f(x∗)],
(5.7.38)
which gives result (5.7.29) by subtracting f(x∗) from both sides.
2

300
CHAPTER 5. QUASI-NEWTON METHODS
Theorem 5.7.7 Let f(x) be a twice continuously diﬀerentiable and uni-
formly convex function. Assume that the iterative sequence {xk} generated
by L-BFGS Algorithm 5.7.2 converges to the unique minimizer x∗of f(x).
Then the rate of convergence is at least R-linear.
Proof.
From (5.7.29) we have
f(xk+1) −f(x∗) ≤δ(f(xk) −f(x∗)),
where δ ∈(0, 1). Also, since f(x) is a uniformly convex function, there are
0 < m1 ≤M1 such that
m1∥u∥2 ≤uT G(x)u ≤M1∥u∥2, ∀x ∈L(x0), u ∈Rn.
(5.7.39)
By using Taylor expression of f(xk) at x∗and (5.7.39), we obtain that
f(xk) −f(x∗) ≥m1
2 ∥xk −x∗∥2.
(5.7.40)
Hence
∥xk −x∗∥
≤
"
2
m1
(f(xk) −f(x∗))
1
2
≤
"
2
m1
δ
1
2 (f(xk−1) −f(x∗))
1
2
≤
· · ·
≤
"
2
m1
(δ
1
2 )k(f(x0) −f(x∗))
1
2 .
(5.7.41)
The above inequality shows that the sequence {xk} is R-linearly convergent.
2
This theorem indicates that L-BFGS method often converges slowly, which
leads to a relatively large number of function evaluations. Also, it is inef-
ﬁcient on highly ill-conditioned optimization problems.
Though there are
some weaknesses, L-BFGS method is a main choice for large-scale problems
in which the true Hessian is not sparse, because, in this case, it may outper-
form other rival algorithms. For further details of L-BFGS method, please
consult Liu and Nocedal [200] and Nash and Nocedal [228].

5.7. LIMITED MEMORY BFGS METHOD
301
At the end of this section, we mension a memoryless BFGS formula. For
BFGS formula (5.7.1) and (5.7.3), if we set Hk = I at each iteration, we have
Hk+1
=
V T
k Vk + ρksksT
k
(5.7.42)
=

I −skyT
k
sT
k yk
 
I −yksT
k
sT
k yk

+ sksT
k
sT
k yk
.
(5.7.43)
The above formula satisﬁes quasi-Newton condition and positive deﬁnite-
ness, and is called the memoryless BFGS formula.
Obviously, if m = 1
and H(0)
k
= I, ∀k, the limited memory BFGS method is just the memoryless
BFGS method.
Exercises
1. Using DFP method minimize the Rosenbrock function in Appendix
1.1 and the Extended Rosenbrock function in Appendix 1.2.
2. Using BFGS method minimize the Extended Rosenbrock function in
Appendix 1.2 and the Powell singular function in Appendix 1.4.
3. State the properties of DFP and BFGS formulas and their relations.
4. Prove that if f is strong convex, yT
k sk > 0 holds.
5. Prove that HBFGS
k+1
given by (5.1.49) is the unique solution of problem
(5.1.79).
6. Prove Theorem 5.2.1.
7. State the properties of Broyden class and Huang class, and their rela-
tions.
8. Prove Theorem 5.4.3.
9. Describe the motivation of self-scaling strategy in variable metric meth-
ods by observing DFP method.
10. Do programming of L-BFGS algorithm in §5.9 in MATLAB or FOR-
TRAN.

Chapter 6
Trust-Region Methods and
Conic Model Methods
6.1
Trust-Region Methods
6.1.1
Trust-Region Methods
The basic idea of Newton’s method is to approximate the objective function
f(x) around xk by choosing a quadratic model of the form
q(k)(s) = f(xk) + gT
k s + 1
2sT Gks,
where gk = ∇f(xk) and Gk = ∇2f(xk), and use the minimizer sk of q(k)(s) to
modify xk,
xk+1 = xk + sk.
However, this method can only guarantee the local convergence, i.e., when
s is small enough, the method is convergent locally. In Chapter 2, we have
introduced line search approaches which guarantee the method is convergent
globally. Line search approaches use the quadratic model to generate a search
direction and then ﬁnd a suitable stepsize α along the direction. Although
it is successful at most time, it does not use suﬃciently the n-dimensional
quadratic model. The other disadvantage is that the Newton’s method can-
not be used if the Hessian matrices are not positive deﬁnite.
In this section the other class of global approaches is introduced, which
is called the trust-region method.
It not only replaces line search to get

304
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
the global convergence, but also circumvents the diﬃculty caused by non-
positive deﬁnite Hessian matrices in line search. Besides, it produces more
signiﬁcant reduction in objective value f than line search approaches. In the
trust-region method, we ﬁrst deﬁne a region around the current iterate
Ωk = {x : ∥x −xk∥≤∆k},
where ∆k is the radius of Ωk, in which the model is trusted to be adequate
to the objective function. And then we choose a step to be the approximate
minimizer of the quadratic model in the trust-region, i.e., such that xk + sk
is the approximately best point on the generalized sphere
{xk + s | ∥s∥≤∆k}
with center xk and radius ∆k. If the step is not acceptable, we reduce the size
of the trust-region and ﬁnd a new minimizer. This method retains the rapid
local convergence rate of Newton’s method and quasi-Newton method, but
also has ideal global convergence. Since the step is restricted by the trust-
region, it is also called the restricted step method. The model subproblem
of the trust-region method is
min
q(k)(s) = f(xk) + gT
k s + 1
2sT Bks
s.t.
∥s∥≤∆k,
(6.1.1)
where ∆k > 0 is the trust-region radius, Bk is symmetric and approximate to
the Hessian Gk. Normally, we use l2 norm ∥·∥2 so that sk is the minimizer of
q(k)(s) in the ball of radius ∆k. Other norms can also be used, however, the
diﬀerent norms deﬁne the diﬀerent shapes of the trust-region. In (6.1.1), if we
set Bk = Gk, the method is said to be a Newton-type trust-region method.
How to choose ∆k at each iteration?
In general, when there is good
agreement between the model q(k)(s) and the objective function value f(xk +
s), one should select ∆k as large as possible. Let
Aredk = f(xk) −f(xk + sk)
(6.1.2)
which is called the actual reduction, and let
Predk = q(k)(0) −q(k)(sk)
(6.1.3)

6.1. TRUST-REGION METHODS
305
which is called the predicted reduction. Deﬁne the ratio
rk = Aredk
Predk
,
(6.1.4)
which measures the agreement between the model function q(k) and the ob-
jective function f. This ratio rk plays an important role in selecting new
iterate xk+1 and updating the trust-region radius ∆k. If rk is close to 1, it
means there is good agreement, and we can expand the trust-region for the
next iteration; if rk is close to zero or negative, we shrink the trust-region;
otherwise, we do not alter the trust-region. The following is the trust-region
algorithm.
Algorithm 6.1.1 (Trust-Region Algorithm)
Step 1. Given initial point x0, ¯∆, ∆0 ∈(0, ¯∆), ϵ ≥0, 0 < η1 ≤η2 < 1
and 0 < γ1 < 1 < γ2, k := 0.
Step 2. If ∥gk∥≤ϵ, stop.
Step 3. Approximately solve the subproblem (6.1.1) for sk.
Step 4. Compute f(xk + sk) and rk. Set
xk+1 =

xk + sk,
if rk ≥η1,
xk,
otherwise.
Step 5. If rk < η1, then ∆k+1 ∈(0, γ1∆k];
If rk ∈[η1, η2), then ∆k+1 ∈[γ1∆k, ∆k];
If rk ≥η2 and ∥sk∥= ∆k, then ∆k+1 ∈[∆k, min{γ2∆k, ¯∆}].
Step 6. Generate Bk+1, update q(k), set k := k + 1, go to Step 2.
2
In the above algorithm, ¯∆is an overall bound for all ∆k. The iterations
for which rk ≥η2 and thus for which ∆k+1 ≥∆k, are said to be very
successful iterations; the iterations for which rk ≥η1 and thus for which
xk+1 = xk + sk, are said to be successful iterations; otherwise the iterations
for which rk < η1 and thus for which xk+1 = xk, are said to be unsuccessful
iterations. Sometimes, the iterations in the ﬁrst two cases are said to be
successful iterations.

306
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
We like to point out some choices of the parameters, for instance, η1 =
0.01, η2 = 0.75, γ1 = 0.5, γ2 = 2, ∆0 = 1 or ∆0 =
1
10∥g0∥.
However, the
algorithm is insensitive to their change. In addition, ∆k+1 can be selected by
polynomial interpolation. For example, if rk < 0.01, then ∆k+1 can be chosen
in an interval (0.01, 0.5)∥sk∥on the basis of a polynomial interpolation. Also,
if we use quadratic interpolation, we have
λ =
−gT
k sk
2[f(xk + sk) −f(xk) −gT
k sk],
(6.1.5)
and we set
∆k+1 = λ∥sk∥.
(6.1.6)
Finally, to conclude this subsection, we give the characterization of the
solution of subproblem (6.1.1). For convenience, we drop the subscripts in
the following theorem.
Theorem 6.1.2 The vector s∗is the solution of the subproblem
min
f + gT s + 1
2sT Bs
(6.1.7)
s.t.
∥s∥2 ≤∆,
(6.1.8)
if and only if there is a scalar λ∗≥0 such that
(B + λ∗I)s∗= −g,
(6.1.9)
∥s∗∥2 ≤∆,
(6.1.10)
λ∗(∆−∥s∗∥2) = 0,
(6.1.11)
and (B + λ∗I) is positive semideﬁnite.
Proof.
Let s∗be the solution of subproblem (6.1.7)-(6.1.8). From the
optimality condition of constrained optimization (see Chapter 8), there exists
a multiplier λ∗≥0 such that (6.1.9)-(6.1.11) hold. We now need to prove
that the matrix (B + λ∗I) is positive semideﬁnite.
If ∥s∗∥2 < ∆, then λ∗= 0 and s∗is an unconstrained minimizer of q,
and thus B is positive semideﬁnite and furthermore (B + λ∗I) is positive
semideﬁnite.
If ∥s∗∥2 = ∆, it follows from the second-order necessary condition (see
§8.3) that
sT (B + λ∗I)s ≥0
(6.1.12)

6.1. TRUST-REGION METHODS
307
for all s satisfying sT s∗= 0.
If sT s∗̸= 0, take t = −2sT s∗/∥s∥2
2, then
∥s∗+ ts∥2 = ∆. By the deﬁnition of s∗, we have
q(s∗+ ts) + 1
2λ∗∥s∗+ ts∥2
2 ≥q(s∗) + 1
2λ∗∥s∗∥2
2.
(6.1.13)
Developing q(·) yields
1
2t2sT (B + λ∗I)s ≥−t(sT [g + (B + λ∗I)s∗]).
(6.1.14)
By using (6.1.9) we get that the right-hand side of (6.1.14) is equal to zero.
Then the above inequality indicates that
sT (B + λ∗I)s ≥0
for all s with sT s∗̸= 0. Therefore, B + λ∗I is positive semideﬁnite.
Conversely, assume that there is λ∗≥0 such that s∗satisﬁes (6.1.9)-
(6.1.11) and that B + λ∗I is positive semideﬁnite. Then, for all s satisfying
∥s∥2 ≤∆, we have
q(s)
=
f(x) + gT s + 1
2sT (B + λ∗I)s −1
2λ∗∥s∥2
2
≥
f(x) + gT s∗+ 1
2(s∗)T (B + λ∗I)s∗−1
2λ∗∥s∥2
2
=
q(s∗) + 1
2λ∗[∥s∗∥2
2 −∥s∥2
2].
By use of (6.1.11), we have that λ∗(∆2 −(s∗)T s∗) = 0. So, the above in-
equality becomes
q(s)
≥
q(s∗) + 1
2λ∗[(∥s∗∥2
2 −∆2) + (∆2 −∥s∥2
2)]
=
q(s∗) + 1
2λ∗[∆2 −∥s∥2].
Thus, from λ∗≥0 and ∥s∥2 ≤∆, we immediately have
q(s) ≥q(s∗),
which implies s∗is the solution of (6.1.7)-(6.1.8).
2
If (B + λ∗I) is singular, we refer to this case as the hard case. In this
case, d∗has the form
d∗= −(B + λ∗I)+g + v,
(6.1.15)

308
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
where (B + λ∗I)+ denotes the generalized inverse of (B + λ∗I), and v is a
vector in null space of (B + λ∗I).
Assume that (B + λ∗I) is positive deﬁnite, then d∗can be obtained by
solving
λ[∆−∥(B + λI)−1g∥2] = 0,
(6.1.16)
∥(B + λI)−1g∥2 ≤∆,
λ ≥0
(6.1.17)
for λ∗and then setting
d∗= −(B + λ∗I)−1g.
If B is positive deﬁnite and ∥B−1g∥2 < ∆, then d∗= −B−1g simply. Other-
wise, λ∗> 0. We need to solve
ψ(λ) =
1
∥(B + λI)−1g∥2
−1
∆= 0.
(6.1.18)
We consider solving ψ(λ) = 0, instead of solving ∆−∥(B + λI)−1g∥2 = 0,
because ψ(λ) is nearly linear in the considered range. By direct computation,
we have
ψ′(λ) = gT H(λ)−3g
∥H(λ)−1g∥3
2
,
(6.1.19)
ψ′′(λ) = −3gT H(λ)−4g
∥H(λ)−1g∥3
2
[1 −cos2(⟨H(λ)−1g, H(λ)−2g⟩)],
(6.1.20)
where H(λ) = B + λI. Therefore, for the most negative eigenvalue λ1 < 0, if
λ > −λ1, ψ(λ) is strictly increasing and concave. So, Newton’s method can
be used to solve (6.1.18), that is,
λ+
= λ −ψ(λ)
ψ′(λ)
= λ −
1
gT (B+λI)−3g
∥(B+λI)−1g∥3
2

1
∥(B + λI)−1g∥2
−1
∆

.
(6.1.21)
6.1.2
Convergence of Trust-Region Methods
In order to discuss the convergence of trust-region methods, we ﬁrst give
some assumptions and technical lemmas.

6.1. TRUST-REGION METHODS
309
We assume that the approximate Hessians Bk are uniformly bounded in
norm, and that the level set
{x | f(x) ≤f(x0)}
(6.1.22)
is bounded, on which the function f : Rn →R is continuously diﬀerentiable.
For generality, we also allow the length of the approximate solution sk of the
subproblem (6.1.1) to exceed the trust-region bound, provided that it stays
within a ﬁxed multiple of the bound, that is
∥sk∥≤˜η∆k,
(6.1.23)
where ˜η is a positive constant. The above assumptions are said to be As-
sumption (A0).
For trust-region algorithm, in general, we do not seek an accurate solution
of subproblem (6.1.1) but we are content with a nearly optimal solution of
(6.1.1). Strong theoretical and numerical results can be obtained if the step
sk produced by Algorithm 6.1.1 satisﬁes
qk(0) −qk(sk) ≥β1∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
,
(6.1.24)
where β1 ∈(0, 1]. Below, we show that the Cauchy point sc
k satisﬁes (6.1.24)
with β1 = 1
2 and that the exact solution sk of the subproblem (6.1.1) satisﬁes
(6.1.24) with β1 = 1
2. If sk is an approximate solution of the subproblem
(6.1.1) with q(k)(0)−q(k)(sk) ≥β2(q(k)(0)−q(k)(sc
k)), then it satisﬁes (6.1.24)
with β1 = 1
2β2.
Lemma 6.1.3 Let sk be the solution of (6.1.1), let ∥· ∥= ∥· ∥2, then
Predk
=
q(k)(0) −q(k)(sk)
≥
1
2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
(6.1.25)
Proof.
By the deﬁnition of sk, for all α ∈[0, 1], we have
q(k)(0) −q(k)(sk)
≥
q(k)(0) −q(k)(−α ∆k
∥gk∥2
gk)
=
α∆k∥gk∥2 −1
2α2∆2
kgT
k Bkgk/∥gk∥2
2
≥
α∆k∥gk∥2 −1
2α2∆2
k∥Bk∥2.
(6.1.26)

310
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Therefore we must have
Predk
≥
max
0≤α≤1[α∆k∥gk∥2 −1
2α2∆2
k∥Bk∥2]
≥
1
2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
2
(6.1.27)
The Cauchy point of the subproblem (6.1.1) can be deﬁned by
q(k)(sc
k) = min{q(k)(s) | s = τsG
k , ∥s∥≤∆k},
(6.1.28)
where sG
k solves a linear version of subproblem (6.1.1):
min
f(xk) + gT
k s
s.t.
∥s∥≤∆k.
(6.1.29)
Obviously, the solution of (6.1.29) is
sG
k = −∆k
∥gk∥2
gk.
Therefore, the Cauchy point of the subproblem (6.1.1) can be expressed as
sc
k = τksG
k = −τk
∆k
∥gk∥2
gk,
(6.1.30)
where
τk =

1
if gT
k Bkgk ≤0;
min{∥gk∥3
2/(∆kgT
k Bkgk), 1}
otherwise.
(6.1.31)
In fact, if gT
k Bkgk ≤0, the function q(k)(sc
k) = q(k)(τsG
k ) decreases monotoni-
cally with τ when gk ̸= 0. Therefore, we can take τ as large as possible within
∥τsG
k ∥≤∆k. In this case, by use of (6.1.30) and ∥τsG
k ∥≤∆k, we have that
τk = 1. If gT
k Bkgk > 0, q(k)(τsG
k ) is a convex and quadratic function in τ.
Then, by minimizing q(k)(τsG
k ), we obtain that τk equals ∥gk∥3
2/(∆kgT
k Bkgk),
or the boundary value 1.
Lemma 6.1.4 The Cauchy point sc
k satisﬁes
q(k)(0) −q(k)(sc
k) ≥1
2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
(6.1.32)

6.1. TRUST-REGION METHODS
311
Proof.
Consider ﬁrst the case of gT
k Bkgk ≤0. In this case, it follows from
(6.1.31) that τk = 1, and we have
q(k)(0) −q(k)(sc
k)
=
−q(k)
 
−∆k
∥gk∥2
gk
!
=
∆k∥gk∥2 −1
2∆2
kgT
k Bkgk/∥gk∥2
2
≥
∆k∥gk∥2
≥
∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
(6.1.33)
Consider the case of gT
k Bkgk > 0 and
∥gk∥3
∆kgT
k Bkgk
≤1.
(6.1.34)
In this case, τk = ∥gk∥3/(∆kgT
k Bkgk), and we have
q(k)(0) −q(k)(sc
k)
=
∥gk∥4
2
gT
k Bkgk
−1
2gT
k Bkgk
∥gk∥4
2
(gT
k Bkgk)2
=
1
2
∥gk∥4
2
gT
k Bkgk
≥
1
2
∥gk∥2
2
∥Bk∥2
≥
1
2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
(6.1.35)
Consider the case of gT
k Bkgk > 0 and
∥gk∥3
2
∆kgT
k Bkgk
> 1.
(6.1.36)
In this case, τk = 1, and by use of (6.1.36) we have
q(k)(0) −q(k)(sc
k)
=
∆k∥gk∥2 −1
2∆2
kgT
k Bkgk/∥gk∥2
2
≥
∆k∥gk∥2 −1
2
∆2
k
∥gk∥2
2
∥gk∥3
2
∆k
=
1
2∆k∥gk∥2
≥
1
2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
.
(6.1.37)

312
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
The above discussion of three cases gives the result (6.1.32).
2
Usually, we assume that sk is an approximate solution of the subproblem
(6.1.1) and satisﬁes
q(k)(0) −q(k)(sk) ≥β2(q(k)(0) −q(k)(se
k)),
(6.1.38)
where se
k is an exact solution of subproblem (6.1.1) and β2 ∈(0, 1] is a
constant. Since q(k)(se
k) ≤q(k)(sc
k), we immediately have
q(k)(0) −q(k)(sk) ≥β2(q(k)(0) −q(k)(sc
k)),
(6.1.39)
where sc
k = −τk
∆k
∥gk∥2 gk with 0 ≤τk ≤1 is a Cauchy point. So, we immedi-
ately have
Lemma 6.1.5 Let sk be an approximate solution of (6.1.1) and satisfy (6.1.38)
or (6.1.39). Then
Predk
=
q(k)(0) −q(k)(sk)
≥
1
2β2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
,
(6.1.40)
where β2 ∈(0, 1].
Next, in order to prove the global convergence theorem, we give some
technical lemmas.
Lemma 6.1.6 Let Assumption (A0) hold. We have
|f(xk + sk) −q(k)(sk)| ≤1
2M∥sk∥2 + C(∥sk∥)∥sk∥,
(6.1.41)
where C(∥sk∥) is arbitrarily small by restricting the size of sk.
Proof.
By Taylor’s theorem,
f(xk + sk) = f(xk) + gT
k sk +
 1
0
[∇f(xk + tsk) −∇f(xk)]T skdt.
Also,
q(k)(sk) = f(xk) + gT
k sk + 1
2sT
k Bksk.
Then
|f(xk + sk) −q(k)(sk)|
=
|1
2sT
k Bksk −
 1
0
[∇f(xk + tsk) −∇f(xk)]T skdt|
≤
1
2M∥sk∥2 + C(∥sk∥)∥sk∥.
2

6.1. TRUST-REGION METHODS
313
Lemma 6.1.7 Assume that Assumption (A0) holds. Suppose that ∥gk∥2 ≥
ϵ > 0 and that ∆k is smaller than some threshold ˜∆. Then the k-th iteration
is a very successful iteration which satisﬁes ∆k+1 ≥∆k.
Proof.
By Lemma 6.1.5 and the assumptions,
Predk
=
q(k)(0) −q(k)(sk)
≥
1
2β2∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
≥
1
2β2ϵ min
*
∆k, ϵ
M
+
.
(6.1.42)
From Algorithm 6.1.1, by use of (6.1.41), (6.1.42) and (6.1.23), we have
|rk −1|
=

(f(xk) −f(xk + sk)) −(q(k)(0) −q(k)(sk))
q(k)(0) −q(k)(sk)

=

f(xk + sk) −q(k)(sk)
q(k)(0) −q(k)(sk)

≤
1
2M∥sk∥2 + C(∥sk∥)∥sk∥
1
2β2ϵ min{∆k, ϵ/M}
≤
˜η∆k(M ˜η∆k + 2C(∥sk∥))
β2ϵ min{∆k, ϵ/M}
.
(6.1.43)
Since ∆k is smaller than some threshold ˜∆, we may choose ˜∆to be small
enough such that
∆k ≤˜∆≤ϵ/M, M ˜η∆k + 2C(∥sk∥) ≤(1 −η2)β2ϵ/˜η,
so we have rk ≥η2. It follows from Algorithm 6.1.1 that ∆k+1 ≥∆k.
2
This lemma indicates that if the current iterate is not a ﬁrst-order sta-
tionary point and the trust-region radius ∆k is small enough, then we always
have ∆k+1 ≥∆k and the iteration is very successful. Now we are in a position
to give the global convergence theorem.
First, we consider the case when there are only ﬁnitely many successful
iterations.
Theorem 6.1.8 Under Assumption (A0), if Algorithm 6.1.1 has ﬁnitely many
successful iterations, then the algorithm converges to the ﬁrst-order stationary
point.

314
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Proof.
Since the algorithm has only ﬁnitely many successful iterations,
then, for suﬃciently large k, the iteration is unsuccessful. Thus, the sequence
{∆k} from the algorithm converges to zero.
Suppose that k0 is the index of the last successful iteration. If ∥gk0+1∥> 0,
it follows from Lemma 6.1.7 that there must be a very successful iteration of
index larger than k0, which satisﬁes ∆k0+1+1 > ∆k0+1. This is a contradiction
to the assumption. The contradiction proves our theorem.
2
Next, we only need to restrict our attention to the case where there are
inﬁnitely many successful iterations.
Theorem 6.1.9 Let Assumption (A0) hold. If Algorithm 6.1.1 has inﬁnitely
many successful iterations, then the sequence of Algorithm 6.1.1 satisﬁes
lim inf
k→∞∥gk∥= 0.
(6.1.44)
Proof.
Assume, by contradiction, that there is ϵ > 0 and a positive index
K such that
∥gk∥≥ϵ for all k ≥K.
From Algorithm 6.1.1 and Lemma 6.1.5, it follows for successful iterations
that
f(xk) −f(xk+1)
≥
η1[q(k)(0) −q(k)(sk)]
≥
1
2η1β2∥gk∥2 min

∆k, ∥gk∥2
∥Bk∥2

≥
1
2η1β2ϵ min

∆k, ϵ
β

,
(6.1.45)
where β = max{1+∥Bk∥2} is an upper bound of the Hessian approximation.
So,
f(x0) −f(xk+1)
=

j=0,j∈S
[f(xj) −f(xj+1)]
≥
1
2σkη1β2ϵ min

∆k, ϵ
β

,
where σk is a number of successful iterations till the k-th iteration with
lim
k→∞σk = +∞,
and S is an index set of successful iterations.

6.1. TRUST-REGION METHODS
315
Since f is bounded below, it follows from the above inequality that
lim
k→∞∆k = 0,
(6.1.46)
contradicting the conclusion of Lemma 6.1.7.
2
Now we give a stronger result on the convergence which is for all limit
points.
Theorem 6.1.10 Suppose that Assumption (A0) holds. Then
lim
k→∞gk = 0.
(6.1.47)
Proof.
Assume, by contradiction, that the conclusion does not hold, then
there is a subsequence of successful iterations such that
∥gti∥≥2ϵ > 0
(6.1.48)
for some ϵ > 0 and for all i.
Theorem 6.1.9 guarantees that, for each i, there exists a ﬁrst successful
iteration l(ti) > ti such that ∥gl(ti)∥< ϵ. We denote li
∆= l(ti). Thus, there
exists another subsequence {li} such that
∥gk∥≥ϵ
for
ti ≤k < li
and
∥gli∥< ϵ.
(6.1.49)
Since
f(xk) −f(xk+1)
≥
η1[q(k)(0) −q(k)(sk)]
≥
1
2η1β2ϵ min[∆k, ϵ/β],
(6.1.50)
it follows from the monotonically decreasing and the bounded below of the
sequence {f(xk)} that
lim
k→∞∆k = 0.
(6.1.51)
Then
∆k ≤
2
η1β2ϵ[f(xk) −f(xk+1)]
(6.1.52)
which implies that for i suﬃciently large,
∥xti −xli∥
≤
li−1

j=ti
∥xj −xj+1∥≤
li−1

j=ti
∆j
≤
2
η1β2ϵ[f(xti) −f(xli)].
(6.1.53)

316
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
From the fact that the right-hand side converges to zero, we get
∥xti −xli∥→0, when
i →∞,
which deduces from continuity of gradient that
∥gti −gli∥→0,
which contradicts (6.1.49), because (6.1.49) implies that ∥gti −gli∥≥ϵ. The
contradiction proves our conclusion.
2
6.1.3
Solving A Trust-Region Subproblem
The Dogleg Method and The Double Dogleg Method
An eﬃcient implementation to solve the trust-region subproblem is the
so-called dogleg method which was presented by Powell [260]. To ﬁnd an
approximate solution of the subproblem (6.1.1), i.e., to ﬁnd xk+1 = xk + sk
such that ∥sk∥= ∆k, Powell used a path consisting of two line segments to
approximate s. The ﬁrst line segment runs from the origin to the Cauchy
point (a minimizer C.P. generated by the steepest descent method); the sec-
ond line segment runs from the Cauchy point C.P. to the Newton point (the
minimizer xN
k+1 generated by Newton method or quasi-Newton method). Let
xk+1 be the intersection point of the path and the trust-region boundary. Ob-
viously, ∥xk+1 −xk∥= ∆k. When the Newton step sN
k satisﬁes ∥sN
k ∥≤∆k,
the new iterate xk+1 is just the Newton point, xk+1 = xN
k+1 = xk −B−1
k gk.
Dennis and Mei [90] found that if the point generated by trust-region
iteration is biased towards the Newton direction, the behavior of the algo-
rithm will be further improved. Then we choose a point ˆN on the Newton
direction, and connect the Cauchy point C.P. to ˆN. The intersection point
of the connection line and the trust-region boundary is taken as the new it-
erate xk+1 (see x(2)
k+1 in Figure 6.1.1). Comparatively, x(2)
k+1 is more biased to
the Newton direction than x(1)
k+1. We say xk →C.P. →xN
k+1 as dogleg, and
xk →C.P. →ˆN →xN
k+1 as double dogleg.

6.1. TRUST-REGION METHODS
317
Figure 6.1.1 Dogleg method and double dogleg method
For quadratic model
q(k)(xk −αgk) = f(xk) −α∥gk∥2
2 + 1
2α2gT
k Bkgk,
the exact line search factor αk has the obvious representation
αk =
∥gk∥2
2
gT
k Bkgk
.
Then the step along the steepest descent direction is
sc
k = −αkgk = −gT
k gk
gT
k Bkgk
gk.
(6.1.54)
If ∥sc
k∥2 = ∥αkgk∥2 ≥∆k, we take
sk = −∆k
∥gk∥2
gk
(6.1.55)
and
xk+1 = xk −
∆k
∥gk∥2
gk
(6.1.56)
which lies at the intersection of the negative gradient and the trust-region
boundary. If ∥sc
k∥2 < ∆k and ∥sN
k ∥2 > ∆k, we take
sk(λ) = sc
k + λ(sN
k −sc
k), 0 ≤λ ≤1,
and thus
xk+1 = xk + sk(λ) = xk + sc
k + λ(sN
k −sc
k), 0 ≤λ ≤1,
(6.1.57)

318
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
where the value λ is obtained by solving the equation
∥sc
k + λ(sN
k −sc
k)∥2 = ∆k.
Otherwise, take
sk = sN
k = −B−1
k gk.
(6.1.58)
Combining (6.1.56), (6.1.57) and (6.1.58) yields
xk+1 =
⎧
⎪
⎨
⎪
⎩
xk −
∆k
∥gk∥2 gk,
when ∥sc
k∥2 ≥∆k,
xk + sc
k + λ(sN
k −sc
k),
when ∥sc
k∥2 < ∆k and ∥sN
k ∥2 > ∆k,
xk −B−1
k gk,
when ∥sc
k∥2 < ∆k and ∥sN
k ∥2 ≤∆k,
(6.1.59)
where 0 ≤λ ≤1.
The following theorem demonstrates the property possessed by the dogleg
method and the double dogleg method. In the following, as an example, we
only consider the double dogleg method.
Theorem 6.1.11 In the double dogleg method,
1. The distance from xk to C.P., to ˆN, is increasing monotonically.
2. The model value q(k)(xk +s) is decreasing monotonically when the point
moves from xk to C.P., to ˆN, and to xN
k+1.
Proof.
(1) Since
∥sc
k∥
=
∥−αkgk∥= ∥gk∥3
2/gT
k Bkgk
≤
∥gk∥3
2
gT
k Bkgk
∥gk∥2∥B−1
k gk∥2
gT
k B−1
k gk
=
∥gk∥4
2
(gT
k Bkgk)(gT
k B−1
k gk)∥sN
k ∥2
∆=
γ∥sN
k ∥2,
(6.1.60)
it follows from Kantorovich inequality (3.1.33) that γ ≤1 and then
∥sc
k∥2 ≤γ∥sN
k ∥2 ≤∥sN
k ∥2.
(6.1.61)
Take ˆN being
x
ˆ
N = xk −ηB−1
k gk = xk + ηsN
k ,
(6.1.62)

6.1. TRUST-REGION METHODS
319
where
γ ≤η ≤1.
(6.1.63)
Thus
∥xc −xk∥2 ≤∥x
ˆ
N −xk∥2 ≤∥xN
k+1 −xk∥2
(6.1.64)
which shows the property (1) holds.
(2) It is enough to prove that q(k)(xk + s) decreases monotonically when
the point moves from the point C.P. to the point ˆN. In fact,
xk+1(λ) = xk + sc
k + λ(ηsN
k −sc
k), 0 ≤λ ≤1.
(6.1.65)
The direction derivative of q(k) at xk+1(λ) is
∇q(k)(xk+1(λ))T (ηsN
k −sc
k)
=
(gk + Bksc
k)T (ηsN
k −sc
k) + λ(ηsN
k −sc
k)T Bk(ηsN
k −sc
k). (6.1.66)
When Bk is positive deﬁnite, the right-hand side of (6.1.66) is a monotone
increasing function of λ.
Therefore, in order to make the above equality
negative when 0 ≤λ ≤1, it is enough to ask the above equality to be
negative when λ = 1, i.e.,
(gk + Bksc
k)T (ηsN
k −sc
k) + λ(ηsN
k −sc
k)Bk(ηksN
k −sc
k) < 0.
Developing and using BksN
k = −gk, the above inequality is equivalent to
0 > (1 −η)(gT
k (ηsN
k −sc
k)) = (1 −η)(γ −η)(gT
k B−1
k gk).
(6.1.67)
Obviously, it is satisﬁed when γ < η < 1. Therefore the second property
holds.
2
In summary, the double dogleg method chooses the point ˆN which is
deﬁned by
xk+1 = xk + ηsN
k , η ∈[γ, 1].
(6.1.68)
When η = 1, the point ˆN is just the Newton point xN
k+1 and the double
dogleg step is just the dogleg step. Generally, we take η = 0.8γ + 0.2.
After generating the points C.P. and ˆN, we ﬁnd xk+1(λ) by (6.1.65), such
that
∥sc
k + λ(ηsN
k −sc
k)∥2
2 = ∆2
k,
(6.1.69)

320
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
which is a one-dimensional root-ﬁnding problem and can be solved by New-
ton’s method. If xk+1(λ) obtained satisﬁes the descent requirement
f(xk+1(λ)) ≤f(xk) + ρgT
k (xk+1(λ) −xk), ρ ∈(0, 1
2),
(6.1.70)
xk+1(λ) will be accepted as new iterate xk+1, and the trust-region will be
updated by Step 4 in Algorithm 6.1.1; if xk+1(λ) does not satisfy (6.1.70),
then set xk+1 := xk.
Steihaug-CG Method
The methods for solving the trust-region subproblem described above
require the solution of a linear system.
When the problem is large, the
operation may be quite costly. Steihaug [322] proposed a technique based
on a preconditioned and truncated conjugate gradient method and trust-
region method, which solves the trust-region subproblem approximately. This
method is usually called Steihaug-CG method. Since it was independently
proposed by Toint [341], it is also called the Steihaug-Toint method.
Consider a scaled trust-region subproblem
min
q(s) = gT s + 1
2sT Bs
(6.1.71)
s.t.
∥s∥W ≤∆,
(6.1.72)
(we drop the subscripts here for simplicity) where W is a symmetric and
positive deﬁnite matrix. Steihaug applied the preconditioned conjugate gra-
dient method (PCG) to subproblem (6.1.71)–(6.1.72), and considered three
possible termination rules. Firstly, if dT
k Bdk > 0, the method corresponds to
the convex interior solution. Secondly, if dT
k Bdk ≤0, we meet a direction of
negative curvature. In this case, we move to the trust-region boundary along
the line sk +τdk with τ > 0 so that ∥sk +τdk∥W = ∆. Finally, if the solution
lies outside the trust-region, we ask that the new point be on the boundary.
The Steihaug-CG algorithm for trust-region subproblem is as follows.
Algorithm 6.1.12 (Steihaug-CG Algorithm for TR Subproblem)
Step 0. Given ε > 0. Let s0 = 0, g0 = g, v0 = W −1g0, d0 = −v0.
If ∥g0∥< ε, set s = s0, stop;
For j = 0, 1, · · · , perform the following steps:

6.1. TRUST-REGION METHODS
321
Step 1. If dT
j Bdj ≤0, compute τ > 0 so that ∥sj + τdj∥W = ∆,
set s = sj + τdj,
stop;
End if
Step 2. Set αj = gT
j vj/dT
j Bdj;
Set sj+1 = sj + αjdj;
If ∥sj+1∥W ≥∆, compute τ > 0 so that ∥sj + τdj∥W = ∆,
set s = sj + τdj,
stop;
End if
Step 3. Set gj+1 = gj + αjBdj;
If ∥gj+1∥W < ε∥g0∥W , set s = sj+1, stop;
End if
Step 4. Set
vj+1 = W −1gj+1,
βj = gT
j+1vj+1/gT
j vj,
dj+1 = −vj+1 + βjdj.
This method has some properties similar to the dogleg method. Next,
we state these properties. In the proof of the property theorem, we need the
following lemma which is easy.
Lemma 6.1.13 Assume dT
i Bdi ̸= 0, then we have
gT
i dj = −gT
j vj, 0 ≤i ≤j,
(6.1.73)
dT
i Wdj = gT
j vj
gT
i vi
dT
i Wdi, 0 ≤i ≤j,
(6.1.74)
q(si+1) = q(si) −1
2
(gT
i vi)2
dT
i Bdi
.
(6.1.75)
Proof.
We can use the explicit formula for the steplength α and iterative
scheme of PCG to get the results. In fact, by gT
j dj−1 = 0 and the conjugacy
of dj and dj−1, we have
gT
j vj
=
gT
j (−dj + βj−1dj−1)
=
−gT
j dj

322
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
=
−(gj−1 + αj−1Bdj−1)T dj
=
−gT
j−1dj
=
· · ·
=
−gT
i dj, i ≤j
which shows (6.1.73).
Next, we prove (6.1.74).
dT
j Wdi
=
(−vj + βj−1dj−1)T Wdi
=
−vT
j Wdi + βj−1dT
j−1Wdi
=
−gT
j di + βj−1dT
j−1Wdi
=
βj−1dT
j−1Wdi.
By recurrence, we have
dT
j Wdi
=
βj−1βj−2 · · · βidT
i Wdi
=
gT
j vj
gT
i vi
dT
i Wdi
that shows (6.1.74).
For (6.1.75), it is a direct consequence of (4.3.40).
2
Now we are in a position to state the properties of the Steihaug-CG
algorithm.
Theorem 6.1.14 Let ∥sj∥be the iterates generated by PCG Algorithm 6.1.12.
Then q(sj) in (6.1.71) is strictly decreasing, i.e.,
q(sj+1) < q(sj).
(6.1.76)
Further, ∥sj∥W is strictly increasing:
0 = ∥s0∥W < · · · < ∥sj∥W < ∥sj+1∥W < · · · < ∥s∥W ≤∆.
(6.1.77)
Proof.
We ﬁrst prove (6.1.76). From (6.1.75), q(sj) is strictly decreasing.
Consider the last iterate s. If s = sj+1, then the result follows directly.
From (6.1.73) we have that
gT
j dj = −gT
j vj = −(Bsj + g)T W −1(Bsj + g) < 0,

6.1. TRUST-REGION METHODS
323
hence dj is a descent direction for q(sj). If dT
j Bdj > 0, then
q(sj) ≥q(sj + τdj) ≥q(sj+1), for 0 < τ ≤αj.
Since τ ≤αj, we have the desired result.
For dT
j Bdj ≤0, then the quadratic term is non-positive, and we have
q(sj) ≥q(sj + τdj), for τ ≥0,
and the result follows.
Now we show that ∥sj∥W is strictly increasing and that (6.1.77) holds.
From Algorithm 6.1.12, we have
sj = s0 +
j−1

k=0
αkdk =
j−1

k=0
αkdk
(6.1.78)
and
αk > 0, k = 0, 1, · · · , j −1.
(6.1.79)
Hence, by (6.1.78) and (6.1.74), we have
sT
j Wdj =
j−1

k=0
αkdT
k Wdj > 0.
(6.1.80)
Using (6.1.80) and (6.1.79) gives
sT
j+1Wsj+1 = sT
j Wsj + 2αjsT
j Wdj + α2
jdT
j Cdj ≥sT
j Wsj
(6.1.81)
which shows ∥sj∥W is strictly increasing.
If s = sj+1, then (6.1.77) follows directly. If the algorithm stops because
dT
j Bdj ≤0 or ∥sj+1∥W ≥∆, then the ﬁnal iterate s is chosen on the bound-
ary, i.e., ∥s∥W = ∆, which is the largest possible length any iterate can have.
Therefore (6.1.77) is satisﬁed.
2
Steihaug-CG method is used in Step 3 in Algorithm 6.1.1 for solving
the trust-region subproblem.
The trust-region method with Steihaug-CG
technique is very useful for large-scale optimization problems.
About other techniques of solving subproblems, please consult Gay [144],
Mor´e and Sorensen [222], and Rendl and Wolkowicz [286].

324
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
6.2
Conic Model and Collinear Scaling Algorithm
6.2.1
Conic Model
The well-known quadratic model usually considered is
q(d) = f(xk) + gT
k d + 1
2dT Bkd,
(6.2.1)
where gk = ∇f(xk) and Bk is a symmetric matrix that is intended to ap-
proximate the Hessian matrix. The model (6.2.1) satisﬁes
q(0) = f(xk), ∇q(0) = ∇f(xk).
(6.2.2)
In quasi-Newton method, the updates satisfy the quasi-Newton condition
Bk(xk −xk−1) = ∇f(xk) −∇f(xk−1),
(6.2.3)
which is just the interpolation condition
∇q(−d) = ∇f(xk−1).
(6.2.4)
Therefore, a secant method based on a quadratic model satisﬁes the three
interpolation conditions in (6.2.2) and (6.2.4). However, a quadratic function
simply does not possess enough degrees of freedom to incorporate all of the
information in the iterative procedure. It often leads to poor prediction of
minimizer by these methods based on a quadratic model, especially for those
functions with strong non-quadratic behavior or severely changed curvature.
Davidon [82] proposed a new class of algorithm which is able to interpolate
richer information on functions and gradients. Such a model function is more
general than the quadratic model. This new model is called a conic model.
The new algorithm is called a conic model algorithm or a collinear scaling
algorithm.
A smooth function is said to be conic if and only if it is a ratio of a
quadratic function to the square of an aﬃne function.
Now, we consider the conic model function
c(d) = f(xk) +
gT
k d
1 + bT d + 1
2
dT Akd
(1 + bT d)2 .
(6.2.5)

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
325
Its gradient is
∇c(d)
=
(1 + bT d)gk −gT
k db
(1 + bT d)2
+ (1 + bT d)2Akd −(1 + bT d)dT Akdb
(1 + bT d)4
=
(1 + bT d)I −bdT
1 + bT d
· (1 + bT d)gk + Akd
(1 + bT d)2
=
1
1 + bT d

I −
bdT
1 + bT d
 
gk +
Akd
1 + bT d

.
(6.2.6)
This gradient vanishes, ∇c(d) = 0, if and only if
gk +
Akd
1 + bT d = 0.
(6.2.7)
In this time, the conic model c(d) has minimizer which is by (6.2.7) that
d =
−A−1
k gk
1 + bT A−1
k gk
.
(6.2.8)
Hence, if 1 + bT A−1
k gk ̸= 0, then the desired minimizer is
xk+1 = xk −
A−1
k gk
1 + bT A−1
k gk
.
(6.2.9)
In fact, an essential ingredient of a conic model is to construct a collinear
scaling
x(d) −x ∆= ˜d =
d
1 + bT d
(6.2.10)
or
d =
˜d
1 −bT ˜d
.
(6.2.11)
In new variable ˜d-space, the conic model (6.2.5) becomes a quadratic model
c( ˜d) = f(xk) + gT
k ˜d + 1
2
˜dT Ak ˜d.
(6.2.12)

326
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
6.2.2
Generalized Quasi-Newton Equation
By means of collinear scaling, Sorensen [315] derived the generalized quasi-
Newton equations that the conic model method satisﬁes.
Let collinear scaling be
x(w) = x +
Jw
1 + hT w,
(6.2.13)
where J ∈Rn×m, h ∈Rm, and w ∈Rm. The local quadratic model to the
scaled objective function φ(w) = f(x(w)) has the form
ψ(w) = φ(0) + φ′(0)w + 1
2wT Bw.
(6.2.14)
Obviously,
φ′(w) = f′(x(w))x′(w)
(6.2.15)
with
x′(w) =
1
1 + hT wJ

I −
whT
1 + hT w

.
(6.2.16)
In terms of the objective function f and the matrix J in the collinear scaling,
the quadratic model has the form
ψ(w) = f(x) + f′(x)Jw + 1
2wT Bw.
(6.2.17)
If B is positive deﬁnite, then the step v that solves
vT B = −f′(x)J
(6.2.18)
is a predicted minimizer of the scaled function φ(w). The step s from x to ¯x
is
s =
Jv
1 + hT v,
(6.2.19)
so that ¯x = x(v) in (6.2.13).
Next, we develop the generalized quasi-Newton equations that the conic
model satisﬁes. Let ¯x be not an acceptable approximation to a local min-
imizer of f.
Then we wish to update the collinear scaling and also the
quadratic model of the new scaled function
¯φ(w) = f(¯x(w)).
(6.2.20)

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
327
Here
¯x(w) = ¯x +
¯Jw
1 + ¯hT w
(6.2.21)
is a collinear scaling with barred quantities ¯J, ¯h, ¯x replacing J, h, x in (6.2.13).
The corresponding new quadratic model of the new scaled function is
¯ψ(w) = ¯φ(0) + ¯φ′(0)w + 1
2wT ¯Bw.
(6.2.22)
For convenience of discussion, we write the derivatives as follows:
¯x′(w) = (1 + ¯hT w) ¯J −¯Jw¯hT
(1 + ¯hT w)2
,
(6.2.23)
¯x′(0) = ¯J,
(6.2.24)
¯x′(−v) = (1 −¯hT v) ¯J + ¯Jv¯hT
(1 −¯hT v)2
,
(6.2.25)
¯φ′(w) = f′(¯x(w))¯x′(w),
(6.2.26)
¯φ′(0) = f′(¯x(0))¯x′(0) = f′(¯x) ¯J,
(6.2.27)
¯φ′(−v) = f′(¯x(−v))¯x′(−v) = f′(x)( ¯J + s¯hT )/γ,
(6.2.28)
¯ψ′(w) = ¯φ′(0) + wT ¯B,
(6.2.29)
¯ψ′(0) = ¯φ(0) = f′(¯x) ¯J,
(6.2.30)
¯ψ′(−v) = ¯φ′(0) −vT ¯B = f′(¯x) ¯J −vT ¯B,
(6.2.31)
where
γ = 1 + ¯hT (−v) = 1 −¯hT v.
(6.2.32)
Then, (6.2.22) can be written as
¯ψ(w)
=
f(¯x) +
∇f(¯x)T ¯s
1 −¯hT ¯J−1¯s + 1
2
¯sT ¯J−T ¯B ¯J−1¯s
(1 −¯hT ¯J−1¯s)2
(6.2.33)
=
f(¯x) + f′(¯x) ¯Jw + 1
2wT ¯Bw,
(6.2.34)
where
¯s
=
¯Jw/(1 + ¯hT w),
(6.2.35)
w
=
¯J−1¯s/(1 −¯hT ¯J−1¯s).
(6.2.36)
To update J, h, B to ¯J, ¯h, ¯B, we introduce the consistency condition
¯x(0) = ¯x, ¯x(−v) = x
(6.2.37)

328
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
and the interpolation conditions:
¯ψ(0) = ¯φ(0), ¯ψ′(0) = ¯φ′(0),
(6.2.38)
¯ψ(−v) = ¯φ(−v), ¯ψ′(−v) = ¯φ′(−v).
(6.2.39)
From the consistency condition ¯x(−v) = x, we have
x = ¯x(−v) = ¯x −¯Jv/γ,
(6.2.40)
that is
¯Jv = γs,
(6.2.41)
where s = ¯x −x. Obviously, conditions (6.2.38) are immediately met by
the quadratic model (6.2.22).
Also, consider the interpolation conditions
(6.2.39); since
¯ψ(−v)
=
¯φ(0) −¯φ′(0)v + 1
2vT ¯Bv
=
f(¯x) −f′(¯x) ¯Jv + 1
2vT ¯Bv
=
f(¯x) −γf′(¯x)s + 1
2vT ¯Bv
and
¯φ(−v) = f(x),
then the ﬁrst equation of (6.2.39) becomes
f(x) = f(¯x) −γf′(¯x)s + 1
2vT ¯Bv.
(6.2.42)
Similarly, it follows from (6.2.31) and (6.2.28) that the second equation of
(6.2.39) becomes
f′(x)( ¯J + s¯hT )/γ = f′(¯x) ¯J −vT ¯B,
(6.2.43)
that can be written as
¯Bv = r,
(6.2.44)
where
rT = ¯φ′(0) −¯φ′(−v) = f′(¯x) ¯J −f′(x)( ¯J + s¯hT )/γ,
(6.2.45)
which is the gradient diﬀerence of the scaled function.

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
329
Then, we obtain a generalized quasi-Newton equation
¯Bv = r, ¯Jv = γs, ¯hT v = 1 −γ,
(6.2.46)
where r is deﬁned by (6.2.45). In particular, when ¯J = I, ¯h = 0, γ = 1,
the generalized quasi-Newton equations reduce to the usual quasi-Newton
equation
¯Bv = r.
(6.2.47)
At this time, v = s = ¯x −x and r = f′(¯x) −f′(x).
It remains to determine the choices of γ. By the second and the third
equations of (6.2.46), we have
( ¯J + s¯hT )v = s,
(6.2.48)
so that
vT ¯Bv = rT v = (γf′(¯x) −f′(x)/γ)s ∆= yT s,
(6.2.49)
where
y = γf′(¯x)T −f′(x)T /γ.
(6.2.50)
Substituting the above into (6.2.42) gives
γ2f′(¯x)s + 2γ[f(x) −f(¯x)] + f′(x)s = 0.
(6.2.51)
To make γ real, we must require
ρ2 ∆= (f(¯x) −f(x))2 −(f′(¯x)s)(f′(x)s) ≥0.
(6.2.52)
If ¯B is to be positive deﬁnite, then we obtain
vT ¯Bv = 2ρ
(6.2.53)
from (6.2.49) by taking
γ
=
−f′(x)s
f(x) −f(¯x) + ρ
(6.2.54)
=
f(x) −f(¯x) + ρ
−f′(¯x)s
(6.2.55)
as the positive root of (6.2.51).
For the one-dimensional case, the corresponding conic model iteration is
as follows.

330
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Algorithm 6.2.1 (Conic Model Algorithm for One-dimensional Case)
Step 0. Given x1, s1, evaluate f1, f′
1 at x1;
Step k. for k = 1, 2, · · ·
Step k.1
set xk+1 = xk + sk;
Step k.2
evaluate fk+1, f′
k+1;
Step k.3
set ρk = ((fk −fk+1)2 −(f′
ksk)(f′
k+1sk))
1
2 ;
γk = −f′
ksk/(fk −fk+1 + ρk);
Step k.4 sk+1 = sk/[(1/γ3
k)(f′
k/f′
k+1) −1].
2
6.2.3
Updates that Preserve Past Information
Based on the generalized quasi-Newton equations and other criteria, we can
obtain some updates about J, h, and B.
Let W be the linear span of previous scaled search directions and let
¯
W = span{W, v}. Then a natural requirement is that
¯φ(w −v) = φ(w), ∀w ∈N0 ⊂W,
(6.2.56)
where N0 = {w ∈W : 1 + hT w > 0}. Condition (6.2.56) immediately leads
to the requirement
¯x(w −v) = x(w), ∀w ∈N0 ⊂W.
(6.2.57)
Since ¯x(−v) = x and ¯x(0) = ¯x, it follows that
¯x(w −v)
=
¯x(0) +
¯J(w −v)
¯hT (w −v) + 1
=
x +
¯Jv
γ +
¯J(w −v)
¯hT w + γ
(by (6.2.46)(iii))
=
x +
¯Jv(¯hT w/γ) + ¯Jw
¯hT w + γ
=
x + ( ¯J + s¯hT )w
¯hT w + γ
(by (6.2.46)(ii)).
(6.2.58)

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
331
By (6.2.57) and (6.2.58), we have
x + ( ¯J + s¯hT )w
¯hT w + γ
= x(w) = x +
Jw
hT w + 1.
(6.2.59)
Set w = αp, p ∈N0 ⊂W, α ∈[0, 1]. Matching the coeﬃcients of α on both
sides of (6.2.59) yields
( ¯J + s¯hT )p = γJp, ¯hT p = γhT p
for every p ∈N0. Then we obtain
( ¯J + s¯hT )w = γJw
(6.2.60)
and
¯hT w = γhT w, ∀w ∈W.
(6.2.61)
Since
s =
Jv
hT v + 1,
then (6.2.60) becomes
( ¯J + γshT )w = γJw, ∀w ∈W,
that is
¯J = γ(J −shT )
(6.2.62)
satisfying ¯Jv = γs as well as (6.2.60). The equation (6.2.62) is an update
about J.
Next, we discuss the update about h. Note that ¯h satisﬁes
¯hT w = γhT w, ¯hT v = 1 −γ.
(6.2.63)
Now let Q be an orthogonal projector on W and P = I −Q. Let
¯h = Qc + Pd,
(6.2.64)
where c and d are arbitrary vectors. Multiplying (6.2.64) by wT gives
γhT w = ¯hT w = wT Qc = cT w,
then we take c = γh. Further, multiplying (6.2.64) by vT yields
1 −γ = ¯hT v = γvT Qh + vT Pd.
(6.2.65)

332
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Then
¯hT v = 1 −γ = γvT Qh + 1 −γ −γvT Qh
vT Pd
vT Pd.
(6.2.66)
Hence, we take
¯h = γQh + Pd
(6.2.67)
or
¯h = γQh + 1 −γ −γvT Qh
vT Pd
Pd
(6.2.68)
as long as vT Pd ̸= 0. So, we obtain the updates about h.
By use of (6.2.27) and (6.2.28), it follows from (6.2.60) that
¯φ′(−v)w
=
f′(x)
γ
( ¯J + s¯hT )w
=
f′(x)
γ
· γJw
=
f′(x)Jw
=
φ′(0)w.
(6.2.69)
To update the Hessian of the quadratic model of a scaled function, the
following requirements are imposed:
¯ψ(w −v) = ψ(w),
(6.2.70)
¯ψ′(w −v)q = ψ′(w)q,
(6.2.71)
for all w, q ∈W. Condition (6.2.70) implies that
¯φ(0) + ¯φ′(0)(w −v) + 1
2(w −v)T ¯B(w −v) = φ(0) + φ′(0)w + 1
2wT Bw
for all w ∈W. Arranging it gives

¯φ(0) −¯φ′(0)v + 1
2vT ¯Bv −φ(0)

+ [¯φ′(0) −φ′(0) −vT ¯B]w
+1
2wT ( ¯B −B)w = 0, ∀w ∈W.
The ﬁrst term vanishes identically due to (6.2.42), and the second term van-
ishes due to (6.2.60) and (6.2.43). Therefore, we have
wT ( ¯B −B)w = 0, ∀w ∈W.
(6.2.72)

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
333
Similarly, condition (6.2.71) implies
[¯φ′(0) + (w −v)T ¯B]q = [φ′(0) + wT B]q,
that is
[¯φ′(0) −φ′(0) −vT ¯B]q + wT ( ¯B −B)q = 0, ∀w, q ∈W.
The ﬁrst bracket in the left-hand side of the above equation vanishes due to
(6.2.60) and (6.2.43). Then we also get
wT ( ¯B −B)q = 0, ∀w, q ∈W.
(6.2.73)
Hence, the above discussion shows that if and only if
wT ( ¯B −B)q = 0, ∀w, q ∈W,
(6.2.74)
both (6.2.70) and (6.2.71) are satisﬁed.
Consequently, the required update satisﬁes
¯Bv = r, wT ( ¯B −B)q = 0, ∀w, q ∈W.
(6.2.75)
The above can be written as
¯B
=
UQ(B, v, r)
=

¯B

¯Bv = r, QT ( ¯B −B)Q = 0, ¯B symmetric,
Q is an orthogonal projector in W.
3
. (6.2.76)
Here the update class coming from additional requirements (6.2.70)-(6.2.71)
is bigger than the update class due to Schnabel [299]:
4 ¯B | ¯Bv = r, ( ¯B −B)w = 0, ∀w ∈W, ¯B symmetric
5 .
Also, the update (6.2.75) includes the optimal conditioning update due to
Davidon [80].
From the above discussion, we have obtained a class of updates:
¯J = γ(J −shT ),
(6.2.77)
¯h = γQh + 1 −γ −γvT Qh
vT Pd
Pd,
(6.2.78)
wT ( ¯B −B)q = 0, ∀w, q ∈W.
(6.2.79)

334
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
6.2.4
Collinear Scaling BFGS Algorithm
Sorensen [315] developed a collinear scaling BFGS algorithm without projec-
tions. That is, in (6.2.77)–(6.2.79) we take P = I and Q = 0. Further, we
take
d = ¯JT g, s = ¯x −x, y = γ¯g −g/γ.
Then we obtain the following updating formulas:
¯J = γ(J −shT ),
(6.2.80)
¯h =
 1 −γ
γgT s
!
¯JT g,
(6.2.81)
¯H = H + v(v −Hr)T
vT r
+ (v −Hr)vT
vT r
−rT (v −Hr)
(vT r)2
vvT ,(6.2.82)
which is called a collinear scaling BFGS formula for updating the inverse
Hessian approximation H, where H = B−1 and ¯H = ¯B−1.
Further, denote
C = JHJT , ¯C = ¯J ¯H ¯JT .
(6.2.83)
Using (6.2.80), (6.2.81) and (6.2.45), we have
r
=
¯JT ¯g −1
γ ( ¯JT + ¯hsT )g = ¯JT ¯g −1
γ ( ¯JT + 1 −γ
γ
¯JT )g
=
¯JT ¯g −1
γ2 ¯JT g = (J −shT )T y.
(6.2.84)
Thus
¯J

I −vrT
vT r

= γ(J −shT ) −γs(JT y −hsT y)T
sT y
= γ

I −syT
sT y

J. (6.2.85)
Equation (6.2.85) allows us to obtain
¯C = γ2

I −syT
sT y

C

I −ysT
sT y

+ ssT
sT y

.
(6.2.86)
So, instead of updating J and H, we only need to update C.
By (6.2.21), the scaled direction is
sk+1
=
1
1 + hT
k+1vk+1
Jk+1vk+1 (note vk+1 = −Hk+1JT
k+1gk+1)

6.2. CONIC MODEL AND COLLINEAR SCALING ALGORITHM
335
=
−Jk+1Hk+1JT
k+1gk+1
1 −(1 −γk)gT
k Jk+1Hk+1JT
k+1gk+1/(γkgT
k sk)
∆=
−Jk+1Hk+1JT
k+1gk+1
1 + δk+1
∆=
−θk+1Ck+1gk+1.
(6.2.87)
Hence, we obtain the iterative scheme
xk+1 = xk −θkCkgk,
(6.2.88)
where
θk = 1/(1 + δk),
δk = −(1 −γk−1)gT
k−1Ckgk/(γk−1gT
k−1sk−1),
Ck = U(Ck−1, sk−1, yk−1).
In the following, we give a description of the algorithm.
Algorithm 6.2.2 (Collinear Scaling BFGS Algorithm)
Step 1. Initialize C0 positive deﬁnite, x0, δ0, αmax > 0.
Compute
f0, g0. Set k = 0.
Step 2. If δk < 0, set ¯α = min(αmax, −1/δk), else ¯α := αmax.
Do line search for the function
φ(α) ∆= f
 
xk −
α
1 + αδk
Ckgk
!
and ﬁnd αk ∈(0, ¯α). Set
sk = −
αk
1 + αkδk
Ckgk,
xk+1 = xk + sk,
fk+1 = f(xk+1), gT
k+1 = f′(xk+1),
ρ2 = (fk −fk+1)2 −(gT
k+1sk)(gT
k sk),
such that ρ2 > 0 and fk+1 < fk.
Step 3. If “convergence” then stop.

336
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Step 4. Compute
γk = −gT
k sk/(fk −fk+1 + ρ), yk = γkgk+1 −gk/γk,
Ck+1 = γ2
k[(I −skyT
k /sT
k yk)Ck(I −yksT
k /sT
k yk) + sksT
k /sT
k yk],
δk+1 = −(1 −γk)gT
k Ck+1gk+1/γkgT
k sk.
Set k := k + 1, go to Step 2.
2
Following the Broyden-Dennis-Mor´e convergence theory about quasi-Newton
methods, we can establish Q-superlinear convergence of the collinear scaling
BFGS algorithm, i.e.,
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0.
Furthermore, Di and Sun [101] propose a conic trust-region method for
unconstrained optimization.
Let x denote the current approximation of the minimizer and let
f = f(x), g = g(x) = ∇f(x).
(6.2.89)
Then the conic trust-region model of f(x + s) is
min
ψ(s) = f +
gT s
1 −aT s + 1
2
sT As
(1 −aT s)2 ,
(6.2.90)
s.t.
∥Ds∥≤∆,
(6.2.91)
where A ∈Rn×n is the Hessian approximation at x, a ∈Rn is a horizontal
vector such that 1 −aT s > 0, D is a scaling matrix and ∆a trust-region
radius. The above subproblem can be written as
min
f + gT Jw + 1
2wT Bw
(6.2.92)
s.t.
s = Jw/(1 + hT w), ∥Ds∥≤∆.
(6.2.93)
Di and Sun [101] discussed the necessary and suﬃcient condition of the so-
lution for the conic trust-region subproblem, presented an algorithm and
established the global and superlinear convergence. Besides, Zhu etc. [384]
discussed a quasi-Newton type trust-region method based on a conic model
for solving unconstrained optimization.
Sun and Yuan [337], Sun, Yuan,
and Yuan [338] studied a conic trust-region algorithm for linear and nonlin-
ear constrained optimization respectively. About the topic of conic model
method, readers are referred also to Grandinetti [162], Ariyawansa [3], Sun
[333], Sheng [308], Han, Sun et al. [168].

6.3. TENSOR METHODS
337
6.3
Tensor Methods
The tensor method is also a generalization of the quadratic model method. In
fact, the tensor method is based on the third- or fourth-order model for opti-
mization problems, and intends to improve upon the eﬃciency and reliability
of standard methods on problem where ∇2f(x∗) is singular.
The tensor method was introduced by Schnabel and Frank [302] for solv-
ing systems of nonlinear equations and by Schnabel and Chow [301] for un-
constrained optimization, respectively. In this section, we will describe the
tensor methods for nonlinear equations and for unconstrained optimization.
6.3.1
Tensor Method for Nonlinear Equations
Let F : Rn →Rn. Consider solving nonlinear equations
F(x) = 0,
(6.3.1)
that is to ﬁnd x∗∈Rn so that F(x∗) = 0. Newton’s method for (6.3.1) is
deﬁned as
x+ = xc −F ′(xc)−1F(xc),
(6.3.2)
when F ′(xc) is nonsingular, where xc and x+ denote the current and the next
iterate respectively. Newton’s method is based on the linear model at xc,
M(xc + d) = F(xc) + F ′(xc)d.
(6.3.3)
As we know, the outstanding advantage of Newton’s method is its rapid
convergence, that is if F ′(xc) is Lipschitz continuous in the neighborhood of
x∗and F ′(x∗) is nonsingular, then the sequence produced by (6.3.2) locally
and quadratically converges to x∗. This implies that there are δ > 0 and
c ≥0, such that when ∥x0 −x∗∥≤δ, the iterative sequence {xk} satisﬁes
∥xk+1 −x∗∥≤c∥xk −x∗∥2.
(6.3.4)
However, if F ′(x∗) is singular, then the iterative sequence does not converge
rapidly. The tensor method described in this section will overcome the short-
coming, and we can see that the tensor method still has rapid convergence
when F ′(x∗) is singular.
Consider the second-order model
MT (xc + d) = F(xc) + F ′(xc)d + 1
2Tcdd,
(6.3.5)

338
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
where Tc ∈Rn×n×n is a three-dimensional tensor.
Usually, the (6.3.5) is
said to be a tensor model, and the corresponding method is called a tensor
method. To discuss the tensor method, we ﬁrst give a deﬁnition concerning
these tensors.
Deﬁnition 6.3.1 Let T ∈Rn×n×n. Then T consists of n horizontal faces
Hi ∈Rn×n, i = 1, · · · , n, where Hi[j, k] = T[i, j, k]. For v, w ∈Rn, we have
Tvw ∈Rn with the i-th component
Tvw[i] = vT Hiw =
n

j=1
n

k=1
T[i, j, k]v[j]w[k].
(6.3.6)
Hence, the tensor model given in (6.3.5) is, in fact, an n-dimensional
vector in which each component is a quadratic model of the component of
F(x), i.e.,
(MT (xc + d))[i] = fi + gT
i d + 1
2dT Hid, i = 1, · · · , n,
(6.3.7)
where fi = F(xc)[i], gT
i is the i-th row of F ′(xc), and Hi the Hessian matrix
of the i-th component of F(x).
An obvious choice of Tc in (6.3.5) is F ′′(xc). However, the computational
amount is prohibitive, since, in each iteration, it needs to compute n3 second-
order partial derivatives of F ′′(xc), store over n3/2 elements, and solve n
quadratic equations in n unknowns. To overcome these drawbacks, the tensor
method constructs Tc in low-rank by using available information of function
values and ﬁrst derivatives. So, the additional eﬀorts are small related to the
standard method.
To construct Tc, we select p not necessarily consecutive past iterates
x−1, · · · , x−p and ask the model (6.3.5) to interpolate F(x) at these points,
i.e.,
F(x−k) = F(xc) + F ′(xc)sk + 1
2Tcsksk, k = 1, · · · , p,
(6.3.8)
where
sk = x−k −xc, k = 1, · · · , p.
(6.3.9)
The selected directions {sk} are required to be strongly independent, i.e.,
make the angle between each direction sk and the subspace spanned by other
directions have at least θ degree. Values of θ between 20 and 40 degrees
have proven to be best in practice. This procedure is easily implemented by

6.3. TENSOR METHODS
339
using a modiﬁed Gram-Schmidt algorithm. Since directions {sk} are linearly
independent, then p ≤n. In practice, one takes
p ≤√n.
Now we write (6.3.8) as
Tcsksk = zk, k = 1, · · · , p,
(6.3.10)
where
zk = 2(F(x−k) −F(xc) −F ′(xc)sk).
(6.3.11)
The (6.3.10) is a set of np ≤n3/2 linear equations in n3 unknowns Tc[i, j, k],
1 ≤i, j, k ≤n.
We choose the smallest symmetric Tc, in the Frobenius
norm, which satisﬁes the equations (6.3.10). Below, we choose Tc following
the technique for a secant update with the smallest change in quasi-Newton
methods (see Chapter 5).
First, we deﬁne the three-dimensional rank-one tensor.
Deﬁnition 6.3.2 Let u, v, w ∈Rn. The tensor T ∈Rn×n×n, for which
T[i, j, k] = u[i] · v[j] · w[k], (1 ≤i, j, k ≤n),
(6.3.12)
is called a third-order rank-one tensor of T ∈Rn×n×n and is denoted by
T = u ⊗v ⊗w.
(6.3.13)
Obviously, the i-th horizontal face of the rank-one tensor u ⊗v ⊗w is a
rank-one matrix u[i](vwT ).
Theorem 6.3.3 Let p ≤n. Let sk ∈Rn, k = 1, · · · , p with {sk} linearly
independent, and let zk ∈Rn, k = 1, · · · , p. Deﬁne M ∈Rp×p by M[i, j] =
(sT
i sj)2, 1 ≤i, j ≤p, and deﬁne Z ∈Rn×p with zk the k-th column, k =
1, · · · , p. Then M is positive deﬁnite, and the solution to
minTc∈Rn×n×n
∥Tc∥F
(6.3.14)
s.t.
Tcsksk = zk, k = 1, · · · p
(6.3.15)
is
Tc =
p

k=1
(ak ⊗sk ⊗sk),
(6.3.16)
where ak is the k-th column of A ∈Rn×p and A = M−1Z.

340
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Proof.
Since the objective function and constraints can be decomposed
into n separate objective functions and constraints, then (6.3.14)-(6.3.15) are
equivalent to the following separate minimization problem
minHi∈Rn×n
∥Hi∥F
(6.3.17)
s.t.
sT
k Hisk = zk[i], k = 1, · · · , p,
(6.3.18)
where Hi are the horizontal faces of Tc, i = 1, · · · , n. Note that the problem
(6.3.17)-(6.3.18) is a sub-determined set of p equations in n2 unknowns.
Let hi ∈Rn2,
hi
=
(Hi[1, 1], Hi[1, 2], · · · , Hi[1, n], Hi[2, 1], · · · ,
Hi[2, n], · · · , Hi[n, 1], · · · , Hi[n, n])T .
(6.3.19)
Let ¯S ∈Rp×n2, the k-th row of ¯S is
¯sk = (sk[1]sT
k , sk[2]sT
k , · · · , sk[n]sT
k ).
(6.3.20)
Let also the i-th row of Z ∈Rn×p be ¯zi,
¯zi ∈Rp, ¯zi[k] = zk[i], 1 ≤i ≤n, 1 ≤k ≤p.
Then (6.3.17) is equivalent to
minhi∈Rn2
∥hi∥2
(6.3.21)
s.t.
¯Shi = ¯zT
i .
(6.3.22)
Note that the {sk} are linearly independent, then ¯S is full row rank, and
hence the solution to (6.3.21)-(6.3.22) is
hi = ¯ST ( ¯S ¯ST )−1¯zT
i .
(6.3.23)
Since M = ¯S ¯ST , then M is positive deﬁnite. Also,
⎡
⎢⎣
¯a1
...
¯an
⎤
⎥⎦= A = M−1Z = ( ¯S ¯ST )−1
⎡
⎢⎣
¯z1
...
¯zn
⎤
⎥⎦.
(6.3.24)
Hence the i-th row of A is
¯ai = ( ¯S ¯ST
i )−1¯zi.
(6.3.25)

6.3. TENSOR METHODS
341
Therefore (6.3.23) means
hi = ¯ST ¯aT
i .
(6.3.26)
Note that here ¯ai is the i-th row of A and ak is the k-th column of A, then
¯ai[k] = ak[i], 1 ≤i ≤n, 1 ≤k ≤p.
Then it follows from (6.3.26) that
hi =
p

k=1
¯ai[k]¯sT
k =
p

k=1
ak[i]¯sT
k ,
(6.3.27)
where ¯sk is deﬁned by (6.3.20) and the k-th row of ¯S, the ¯sT
k denotes a
transpose of ¯sk and a column vector with n2 elements.
Returning to (6.3.27) in the terms of Hi and sk, and using (6.3.19) and
(6.3.20) give
Hi =
p

k=1
ak[i]sksT
k .
(6.3.28)
Finally, combining n matrices Hi gives the desired Tc in (6.3.16).
Substituting (6.3.16) into (6.3.5), the tensor model has the form
MT (xc + d) = F(xc) + F ′(xc)d + 1
2
p

k=1
ak(dT sk)2.
(6.3.29)
In the above model, the simple form of the second-order term is a key to
eﬃciently ﬁnd a minimizer of this model. In the tensor method, the additional
4pn storage are required to save {ak}, {sk}, {x−k} and {F(x−k)}. Additional
cost is n2p+O(np2) operations for computing A = M−1Z. Since p ≤√n, this
is a very small additional cost, more than the cost of the standard quadratic
model method.
6.3.2
Tensor Methods for Unconstrained Optimization
In this subsection, we extend the tensor method to solving unconstrained
optimization problem
min
x∈Rn f(x), f : Rn →R.
(6.3.30)
Note that the standard quadratic model methods do not converge quickly if
the Hessian ∇2f(x∗) is singular. In this case, the convergence rate is linear

342
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
at best. Furthermore, the third derivatives do not supply information in the
direction where the second derivative matrix is lacking. Thus, adding an ap-
proximation to ∇3f(xc) alone will not lead to better-than-linear convergence.
Therefore, we consider employing the following fourth order tensor model
mT (xc+d) = f(xc)+∇f(xc)·d+ 1
2∇2f(xc)·d2+ 1
6Tc·d3+ 1
24Vc·d4, (6.3.31)
where Tc ∈Rn×n×n, a three-dimensional tensor and Vc ∈Rn×n×n×n, a four-
dimensional tensor, are symmetric. Equation (6.3.31) is called a tensor model
for unconstrained optimization; the methods based on (6.3.31) are referred
to tensor methods.
How to Choose Tc and Vc?
To select Tc and Vc, we select p not necessarily consecutive past iterates
x−1, · · · , x−p, and ask that the model (6.3.31) interpolate f(x) and ∇f(x) at
these points, i.e.,
f(x−k)
=
f(xc) + ∇f(xc) · sk + 1
2∇2f(xc) · s2
k + 1
6Tc · s3
k
+ 1
24Vc · s4
k,
(6.3.32)
∇f(x−k)
=
∇f(xc) + ∇2f(xc) · sk + 1
2Tc · s2
k
+1
6Vc · s3
k,
(6.3.33)
where sk = x−k −xc, k = 1, · · · , p. As in the previous subsection, the direc-
tions {sk} are strongly linearly independent. We also set p ≤n1/3.
Multiplying (6.3.33) by sk gives
∇f(x−k) · sk = ∇f(xc) · sk + ∇2f(xc) · s2
k + 1
2Tc · s3
k + 1
6Vc · s4
k.
(6.3.34)
Deﬁne α, β ∈Rp respectively by
α[k] = Tc · s3
k,
(6.3.35)
β[k] = Vc · s4
k,
(6.3.36)
where k = 1, · · · , p.
Then (6.3.34) and (6.3.32) have the following form
respectively:
1
2α[k] + 1
6β[k] = q1[k],
(6.3.37)

6.3. TENSOR METHODS
343
1
6α[k] + 1
24β[k] = q2[k],
(6.3.38)
where
q1[k] = ∇f(x−k) · sk −∇f(xc) · sk −∇2f(xc) · s2
k,
(6.3.39)
q2[k] = f(x−k) −f(xc) −∇f(xc) · sk −1
2∇2f(xc) · s2
k, (6.3.40)
for k = 1, 2, · · · , p. The system (6.3.37)-(6.3.38) is nonsingular, so each α[k]
and β[k] are uniquely determined. Thus, we can determine Vc by the mini-
mization problem
minVc∈Rn×n×n×n
∥Vc∥F
s.t.
Vc · s4
k = β[k], k = 1, · · · , p.
(6.3.41)
Vc symmetric.
We then substitute the obtained value of Vc into (6.3.33), obtaining
Tc · s2
k = ak, k = 1, · · · , p,
(6.3.42)
where
ak = 2
 
∇f(x−k) −∇f(xc) −∇2f(xc) · sk −1
6V · s3
k
!
.
This is a set of np ≤n4/3 linear equations in n3 unknowns Tc[i, j, k], 1 ≤
i, j, k ≤n. Then we determine Tc by the minimization problem
minTc∈Rn×n×n
∥Tc∥F
s.t.
Tc · s2
i = ai, i = 1, · · · , p
(6.3.43)
Tc symmetric.
The following two theorems give the solutions of problems (6.3.41) and
(6.3.43).
Theorem 6.3.4 Let p ≤n. Let sk ∈Rn, k = 1, · · · , p with {sk} linearly
independent, and let β ∈Rp. Deﬁne M ∈Rp×p by M[i, j] = (sT
i sj)4, 1 ≤
i, j ≤p. Deﬁne γ ∈Rp by γ = M−1β. Then the solution to (6.3.41) is
Vc =
p

k=1
γ[k](sk ⊗sk ⊗sk ⊗sk).
(6.3.44)

344
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Proof.
Deﬁne ˆv ∈Rn4 by
ˆvT =
(Vc[1, 1, 1, 1], Vc[1, 1, 1, 2], · · · , Vc[1, 1, 1, n],
Vc[1, 1, 2, 1], · · · , Vc[1, 1, 2, n], · · · , Vc[n, n, n, n]).
Let the matrix ˆS ∈Rp×n4 with the k-th row as
(sk[1])4, (sk[1])3(sk[2]), · · · , (sk[1])3(sk[n]), · · · , (sk[n])4.
Then, (6.3.41) is equivalent to
minˆv
∥ˆv∥2
(6.3.45)
s.t.
ˆSˆv = β, Vc symmetric,
(6.3.46)
where Vc is the original form of ˆv. Since {sk} are linearly independent, ˆS has
full row rank. Hence, the solution to
minˆv
∥ˆv∥2
(6.3.47)
s.t.
ˆSˆv = β
(6.3.48)
is
ˆv = ˆST ( ˆS ˆST )−1β = ˆST M−1β = ˆST γ,
(6.3.49)
where M = ˆS ˆST .
By reversing the transformation from ˆv to Vc, we get
(6.3.44). Since Vc is symmetric, it is the solution of (6.3.41).
2
Theorem 6.3.5 Let p ≤n. Let sk ∈Rn, k = 1, · · · , p with {sk} linearly
independent, and let ak ∈Rn, k = 1, · · · , p. Then the solution to problem
(6.3.43) is
Tc =
p

k=1
(bk ⊗sk ⊗sk + sk ⊗bk ⊗sk + sk ⊗sk ⊗bk),
(6.3.50)
where bk ∈Rn, k = 1, · · · , p, and {bk} is the unique set of vectors for which
(6.3.50) satisﬁes
Tcs2
i = ai, i = 1, · · · , p.
Proof.
First, we show that the constraint set in (6.3.43) is feasible. Let
ti ∈Rn, i = 1, · · · , p satisfy
tT
i sj =

1,
i = j,
0,
i ̸= j,
for j = 1, · · · , p.

6.3. TENSOR METHODS
345
Since {si} are linearly independent, such vectors ti can be obtained via a QR
factorization. Then
T =
p

i=1
(ti ⊗ti ⊗ai + ti ⊗ai ⊗ti + ai ⊗ti ⊗ti −2(aT
i si)(ti ⊗ti ⊗ti))
is a feasible solution to (6.3.43).
Dennis and Schnabel [93] assume that the set of tensors Tj ∈Rn×n×n is
generated by the following procedure: T0 = 0 and for j = 0, 1, · · · , T2j+1 is
the solution of
min
∥T2j+1 −T2j∥F
(6.3.51)
s.t.
T2j+1 · s2
i = ai, i = 1, · · · , p,
(6.3.52)
and T2j+2 is the solution of
min
∥T2j+2 −T2j+1∥F
(6.3.53)
s.t.
T2j+2 symmetric.
(6.3.54)
Then the sequence {Tj} has a limit which is the unique solution to (6.3.43).
(see the derivation of Powell symmetric Broyden update in §5.1).
Next, we show that this limit has form (6.3.50) for some set of vectors
{bk}, by showing that each T2j has this form.
Trivially, it is true for T0. Assume it is true for some j, i.e.,
T2j =
p

k=1
(uk ⊗sk ⊗sk + sk ⊗uk ⊗sk + sk ⊗sk ⊗uk)
(6.3.55)
for some set of vectors uk. Then from Theorem 6.3.3, the solution to (6.3.51)-
(6.3.52) is
T2j+1 = T2j +
p

k=1
(vk ⊗sk ⊗sk)
for some set of vectors {vk}. Thus
T2j+2
=
T2j + 1
3
p

k=1
(vk ⊗sk ⊗sk + sk ⊗vk ⊗sk + sk ⊗sk ⊗vk)
=
p

k=1
  
uk + vk
3
!
⊗sk ⊗sk + sk ⊗
 
uk + vk
3
!
⊗sk
+sk ⊗sk ⊗
 
uk + vk
3
!!
,
(6.3.56)

346
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
which again has the form (6.3.55). Thus, by induction, the solution Tc must
have the form (6.3.50) for some set of vectors {bk}.
Finally, we show that the set of vectors {bk}, for which Tc given by (6.3.50)
satisﬁes
Tcs2
i = ai, i = 1, · · · , p,
(6.3.57)
is unique. This will mean that equations (6.3.50) and (6.3.57) uniquely deter-
mine the solution to (6.3.43). In fact, substituting (6.3.50) into (6.3.57) gives
a system of np linear equations in np unknowns, where the matrix is a func-
tion of {sk}, the unknowns are the elements of the {bk}, and the right-hand
side consists of the elements of the {ak}.
Since we have showed above that (6.3.43) is feasible for any {ak}, the
above derivation and the theory of Dennis-Schnabel [93] imply that for any
set {sk}, this linear system has at least one solution for any right-hand side.
Therefore, the linear system must be nonsingular and have a unique solution.
This means that the set of vectors {bk} is uniquely determined.
2
Solving the Tensor Model
Substituting the values of Tc and Vc in (6.3.50) and (6.3.44) into the
tensor model (6.3.31) gives
mT (xc + d)
=
f(xc) + ∇f(xc) · d + 1
2∇2f(xc) · d2
1
2
p

k=1
(bT
k d)(sT
k d)2 + 1
24
p

k=1
γ[k](sT
k d)4
=
f(xc) + gT d + 1
2dT Hd
+1
2
p

k=1
(bT
k d)(sT
k d)2 + 1
24
p

k=1
γ[k](sT
k d)4,
(6.3.58)
where g = ∇f(xc), H = ∇2f(xc).
Let S ∈Rn×p with k-th column sk. Let Z ∈Rn×(n−p) and W ∈Rn×p
have full column rank and satisfy ZT S = 0 and W T S = I, respectively. The
Z and W can be calculated through the QR factorization of S.
Write
d = Wu + Zt,
(6.3.59)
where u ∈Rp and t ∈Rn−p. Substituting (6.3.59) into (6.3.58) gives
mT (xc + Wu + Zt) = f(xc) + gT Wu + gT Zt + 1
2uT W T HWu

6.3. TENSOR METHODS
347
+uT W T HZt + 1
2tT ZT HZt + 1
2
p

k=1
u[k]2(bT
k Wu + bT
k Zt)
+ 1
24
p

k=1
γ[k]u[k]4,
(6.3.60)
which is a quadratic with respect to t. Therefore, for the tensor model to
have a minimizer, ZT HZ must be positive deﬁnite and the derivative of the
model with respect to t must be 0, i.e.,
ZT g + ZT HZt + ZT HWu + 1
2ZT
p

i=1
biu[i]2 = 0.
(6.3.61)
Therefore
t = −(ZT HZ)−1ZT

g + HWu + 1
2
p

i=1
biu[i]2

.
(6.3.62)
Substituting (6.3.62) into (6.3.60) reduces the problem of minimizing the
tensor model to ﬁnding a minimizer of
ˆmT (u)
=
f + gT Wu + 1
2uT W T HWu + 1
2
p

i=1
u[i]2(bT
i Wu)
+ 1
24
p

i=1
γ[i]u[i]4 −1
2

g + HWu + 1
2
p

i=1
biu[i]2
T
·Z(ZT HZ)−1ZT

g + HWu + 1
2
p

i=1
biu[i]2

,
(6.3.63)
which is a fourth-degree polynomial in u-variable. If (6.3.63) has a minimizer
u∗, then the minimizer of the original tensor model (6.3.58) is given by
d∗= Wu∗+ Zt∗,
(6.3.64)
where t∗is determined by setting u = u∗in (6.3.60).
In implementation we may employ line search or trust-region strategy. If
the obtained direction d∗is a descent direction, but xc +d∗is not acceptable,
we set x+ = xc + λd∗where λ is a steplength factor.
If (6.3.63) has no
minimizer, or d∗is not in a descent direction, we ﬁnd the next iterate by
using a line search algorithm based on the standard quadratic model.

348
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
Similarly, we can also use trust-region technique studied in §6.1. The
trust-region tensor model is
mind∈Rn
mT (xc + d)
(6.3.65)
s.t.
∥d∥2 ≤∆c,
(6.3.66)
where ∆c ∈R is the trust-region radius.
The tensor algorithm for unconstrained optimization is as follows.
Algorithm 6.3.6 (Tensor Method) Given xc, f(xc), ∆c.
Step 1. Calculate ∇f(xc), and decide whether to stop. If not, go to
Step 2.
Step 2. Calculate ∇2f(xc).
Step 3. Select p past points from among the n1/3 most recent past
points.
Step 4. Calculate Tc and Vc.
Step 5. Find a potential acceptable next iterate xc +dT and a poten-
tial new trust-region radius ∆T by using the tensor model
and a trust-region technique.
Step 6. Find a potential acceptable next iterate xc +dN and a poten-
tial new trust-region radius ∆N by using the quadratic model
and a trust-region technique.
Step 7. If f(xc + dT ) ≤f(xc + dN), then set
x+ = xc + dT , ∆+ = ∆T ;
else set
x+ = xc + dN, ∆+ = ∆N.
Step 8. Set xc = x+, f(xc) = f(x+), ∆c = ∆+, and go to Step 1.
2
Note that in the tensor method, the Hessian can be replaced by ﬁnite
diﬀerence Hessian approximation or secant updates, because the cost of com-
puting a Hessian is large. Also, we would like to point out that the tensor

6.3. TENSOR METHODS
349
method is a generalization of the standard quadratic model method. How-
ever, there are still various problems waiting for us to solve. For example, the
strategy of computing both tensor model and quadratic model at each itera-
tion is not ideal; how to choose a suitable p, how to use the tensor method in
constrained problems and so on. This kind of method is worth doing further
study.
Exercises
1. Let f(x) = x4
1 +x2
1 +x2
2. Let the current iterate x(k) = (1, 1)T , ∆k = 1
2.
Try using double-dogleg method to ﬁnd x(k+1).
2. Let f(x) = 1
2x2
1 + x2
2. Let the starting point x(0) = (1, 1)T . For ∆0 = 1
and ∆0 = 5
4,
(1) Use dogleg method to ﬁnd x(1).
(2) Use double-dogleg method to ﬁnd x(2).
3. Let sk be an approximate solution of subproblem (6.1.1). Show that
sk satisﬁes
q(k)(0) −q(k)(sk) ≥β∥gk∥2 min
*
∆k, ∥gk∥2
∥Bk∥2
+
,
where β ∈(0, 1].
4. What is the attractive point of the trust-region method?
5. Use trust-region Newton method to minimize the Rosenbrock function
(see Appendix: Problem 1.1).
6.
Use trust-region quasi-Newton method to minimize the extended
Rosenbrock function (see Appendix: Problem 1.2).
7.
Consider using dogleg method to construct path s(τ).
Show that
∥s(τ)∥increases monotonically along this path.
8. Derive expression (6.1.30)–(6.1.31) of the Cauchy point.

350
CHAPTER 6. TRUST-REGION AND CONIC MODEL METHODS
9. (1) Let sG
k solve
min
f(xk) + gT
k s
s.t.
∥Ds∥≤∆k.
Show that
sG
k = −
∆k
∥D−1gk∥2
D−2gk.
(2) The generalized Cauchy point can be deﬁned by
q(k)(sc
k) = min{q(k)(s) | s = τsG
k , ∥Ds∥≤∆k},
where sG
k is deﬁned by (1). Therefore, the generalized Cauchy point can be
expressed as
sc
k = τksG
k = −τk
∆k
∥D−1gk∥2
D−2gk,
(6.3.67)
where
τk = arg min
τ>0 q(k)(τsG
k )
s.t. ∥τDsG
k ∥≤∆k.
Show:
τk =

1
if gT
k D−2BkD−2gk ≤0;
min{∥D−1gk∥3
2/(∆kgT
k D−2BkD−2gk), 1}
otherwise.
(6.3.68)
10. Mimic Theorem 6.1.2, state and prove the necessary and suﬃcient
condition that s∗is the solution of subproblem
mins
f + gT s + 1
2sT Bs
s.t.
∥Ds∥2 ≤∆.
11. Write out the program of Steihaug-CG Algorithm 6.1.12.
12. Try to state the relations among quadratic model, conic model, tensor
model and collinear scaling.

6.3. TENSOR METHODS
351
13. Starting from collinear scaling s =
w
1+hT w, derive a generalized quasi-
Newton equation.
14. Derive the collinear scaling BFGS formula. Try to derive other for-
mulas of collinear scaling.

Chapter 7
Solving Nonlinear
Least-Squares Problems
7.1
Introduction
This chapter is devoted to solving the following nonlinear least-squares prob-
lems:
min
x∈Rn f(x) = 1
2r(x)T r(x) = 1
2
m

i=1
[ri(x)]2, m ≥n
(7.1.1)
where r : Rn →Rm is a nonlinear function of x. If r(x) is a linear function,
the problem (7.1.1) is the linear least-squares problem.
Nonlinear least-squares problem (7.1.1) can be regarded as a special case
for unconstrained minimization with a special structure. This problem can
also be interpreted as solving the system of m nonlinear equations
ri(x) = 0, i = 1, 2, · · · , m,
(7.1.2)
where ri(x) is called the residual function. When m > n, the system is called
over-determined, and when m = n the system is well-determined.
Nonlinear least-squares problems have wide applications in data ﬁtting,
parameter estimation, function approximation, and others.
For example,
suppose we are given the data (t1, y1), (t2, y2), · · · , (tm, ym) and want to ﬁt
a function φ(t, x) which is a nonlinear function of x. We want to choose x
such that the function φ(t, x) ﬁts the data as well as possible in the sense of

354
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
minimizing the sum of the squares of the residual,
min
m

i=1
[ri(x)]2
(7.1.3)
where
ri(x) = φ(ti, x) −yi, i = 1, · · · , m
(7.1.4)
are the residual. Usually, m ≫n. So, we obtain the problem (7.1.1). For
solving nonlinear least-squares problem, we usually use Newton’s method to
solve the resulting system of the normal equations. However, it is expensive,
and the normal equations tend easily to be ill-conditioned. Note that the
problem (7.1.1) has special structure which inspires some special methods.
In this chapter, we shall give some eﬀective and special methods for solving
nonlinear least-squares problem (7.1.1).
Let J(x) be the Jacobian of r(x),
J(x) =
⎡
⎢⎢⎢⎣
∂r1
∂x1 (x)
∂r1
∂x2 (x)
· · ·
∂r1
∂xn (x)
∂r2
∂x1 (x)
∂r2
∂x2 (x)
· · ·
∂r2
∂xn (x)
· · ·
· · ·
· · ·
· · ·
∂rm
∂x1 (x)
∂rm
∂x2 (x)
· · ·
∂rm
∂xn (x)
⎤
⎥⎥⎥⎦.
(7.1.5)
Then the gradient of f(x) is
g(x) =
m

i=1
ri(x)∇ri(x) = J(x)T r(x)
(7.1.6)
and the Hessian is
G(x)
=
m

i=1
(∇ri(x)∇ri(x)T + ri(x)∇2ri(x))
=
J(x)T J(x) + S(x),
(7.1.7)
where
S(x) =
m

i=1
ri(x)∇2ri(x).
(7.1.8)
Therefore, the quadratic model of the objective function f(x) is
q(k)(x)
=
f(xk) + g(xk)T (x −xk) + 1
2(x −xk)T G(xk)(x −xk)
=
1
2r(xk)T r(xk) + (J(xk)T r(xk))T (x −xk)
+1
2(x −xk)T (J(xk)T J(xk) + S(xk))(x −xk).
(7.1.9)

7.2. GAUSS-NEWTON METHOD
355
Then we have Newton’s method for (7.1.1),
xk+1 = xk −(J(xk)T J(xk) + S(xk))−1J(xk)T r(xk).
(7.1.10)
We have seen in Chapter 3 that, under standard assumptions, the iter-
ation (7.1.10) is locally quadratically convergent. However, the main disad-
vantage of the above Newton’s method is that the second-order term S(x)
in the Hessian G(x) is diﬃcult or expensive to compute. It is also not suit-
able to use a secant approximation of the whole of G(x), because J(x) and
furthermore the ﬁrst-order term J(x)T J(x) in G(x) are available when we
compute the gradient g(x). Hence, for reducing the computation, it may be
reasonable and eﬀective to either neglect S(x) or use ﬁrst-order derivative
information to approximate S(x). Notice from (7.1.8) that when ri(x) ap-
proaches zero or closes to a linear function, in which case ∇2ri(x) approaches
zero, S(x) is small and can be neglected. We call this case a small residual
problem, otherwise, a large residual problem.
7.2
Gauss-Newton Method
In this section, we discuss the Gauss-Newton method, which is obtained by
neglecting the second-order term S(x) of G(x) in the quadratic model (7.1.9).
So, (7.1.9) becomes
¯q(k)(x)
=
1
2r(xk)T r(xk) + (J(xk)T r(xk))T (x −xk)
+1
2(x −xk)T (J(xk)T J(xk))(x −xk).
(7.2.1)
Hence (7.1.10) becomes
xk+1 = xk + sk = xk −(J(xk)T J(xk))−1J(xk)T r(xk).
(7.2.2)
To make the iteration well-deﬁned, it is required that Jacobian matrix J(xk)
has full column rank. The following is the Gauss-Newton algorithm.
Algorithm 7.2.1 (Gauss-Newton Method)
Step 0. Given x0, ϵ > 0, k := 0.
Step 1. If ∥gk∥≤ϵ, stop.

356
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
Step 2 Solve
J(xk)T J(xk)s = −J(xk)T r(xk)
for sk.
(7.2.3)
Step 3. Set xk+1 = xk + sk, k := k + 1. Go to Step 1.
2
Obviously, whenever J(xk) has full rank and the gradient g(xk) is nonzero,
the search direction sk is a descent direction for f, because
sT
k ∇f(xk) = sT
k J(xk)T r(xk) = −sT
k J(xk)T J(xk)sk ≤0.
The ﬁnal inequality is strict unless J(xk)T sk = 0, which is equivalent to
J(xk)T r(xk) = g(xk) = 0.
Equation (7.2.3) is said to be the Gauss-Newton equation. Obviously, by
comparing (7.2.3) and (7.1.10), we ﬁnd that the diﬀerence between Gauss-
Newton method and Newton method is that the ﬁrst-order term J(xk)T J(xk)
is used to replace the Hessian G(xk).
Note that Step 2 in Algorithm 7.2.1 is just analogous to the normal equa-
tions of linear least-squares problem. Besides, the model (7.2.1) is equivalent
to considering the aﬃne model of r(x) near xk,
¯
Mk = r(xk) + J(xk)(x −xk),
(7.2.4)
and solve the linear least-squares problem
min 1
2∥¯
Mk(x)∥2.
(7.2.5)
These two observations expose that Gauss-Newton method, in fact, is a lin-
earization method for nonlinear least-squares problem. From (7.2.2), we see
that Gauss-Newton method has some advantages in that it only requires
the ﬁrst-order derivative information of the residual function r(x), and that
J(x)T J(x) is at least positive semi-deﬁnite.
Since Newton’s method, under the standard assumptions, is locally and
quadratically convergent, the success of Gauss-Newton method will depend
on the importance of the neglected second-order term S(x) in G(x). The
following theorem shows:
1. if S(x∗) = 0, the Gauss-Newton method is quadratically convergent;
2. if S(x∗) is small relative to J(x∗)T J(x∗), the Gauss-Newton method is
locally Q-linearly convergent;

7.2. GAUSS-NEWTON METHOD
357
3. if S(x∗) is too large, the Gauss-Newton method will not be convergent.
The proofs of the following theorem are similar to that of Theorem 3.2.2
for Newton’s method. The diﬀerent proofs given in Theorem 7.2.2 and The-
orem 7.2.3 are helpful to study and understand the convergence theorems of
various iterative methods.
Theorem 7.2.2 Let f : Rn →R and f ∈C2. Assume that x∗is the local
minimizer of the nonlinear least-squares problem (7.1.1) and J(x∗)T J(x∗)
is positive deﬁnite. Assume also that the sequence {xk} generated by Algo-
rithm 7.2.1 converges to x∗. Then, if G(x) and (J(x)T J(x))−1 are Lipschitz
continuous in the neighborhood of x∗, we have
∥xk+1 −x∗∥≤∥(J(x∗)T J(x∗))−1∥∥S(x∗)∥∥xk −x∗∥+O(∥xk −x∗∥2). (7.2.6)
Proof.
Since G(x) is Lipschitz continuous, J(x)T J(x) and S(x) are also
Lipschitz continuous. Hence, there exist α, β, γ > 0, such that for any x, y in
the neighborhood of x∗, we have
∥J(x)T J(x) −J(y)T J(y)∥≤α∥x −y∥,
(7.2.7)
∥S(x) −S(y)∥≤β∥x −y∥,
(7.2.8)
∥(J(x)T J(x))−1 −(J(y)T J(y))−1∥≤γ∥x −y∥,
(7.2.9)
(see Exercise).
Since f ∈C2 and G(x) is Lipschitz continuous, then we have
g(xk + s) = g(xk) + G(xk)s + O(∥s∥2).
(7.2.10)
Let hk = xk −x∗and s = −hk. We can deduce that
0 = g(x∗) = g(xk) −G(xk)hk + O(∥hk∥2).
(7.2.11)
Substituting (7.1.6) and (7.1.7) into (7.2.11) gives
J(xk)T r(xk) −(J(xk)T J(xk) + S(xk))hk + O(∥hk∥2) = 0.
(7.2.12)
Assume that xk is in a neighborhood of x∗.
From Theorem 1.2.5, it
follows that for k suﬃciently large, J(xk)T J(xk) is positive deﬁnite, and
hence (J(xk)T J(xk))−1 is bounded above and
∥(J(xk)T J(xk))−1∥≤2∥(J(x∗)T J(x∗))−1∥.
(7.2.13)

358
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
Then, multiplying (7.2.12) by (J(xk)T J(xk))−1 yields that
−sk −hk −(J(xk)T J(xk))−1S(xk)hk + O(∥hk∥2) = 0.
(7.2.14)
Note that sk + hk = xk+1 −x∗= hk+1, the above equality can be written as
−hk+1 −(J(x∗)T J(x∗))−1S(x∗) −(J(xk)T J(xk))−1(S(xk) −S(x∗))hk
−[(J(xk)T J(xk))−1 −(J(x∗)T J(x∗))−1]S(x∗)hk + O(∥hk∥2)
= 0.
(7.2.15)
Taking the norm and using (7.2.8)–(7.2.9) and (7.2.13) give the result (7.2.6).
2
Theorem 7.2.3 Let f : D ⊂Rn →R and f ∈C2(D), where D is an open
convex set. Let J(x) be Lipschitz continuous on D, i.e.,
∥J(x) −J(y)∥2 ≤γ∥x −y∥2, ∀x, y ∈D,
(7.2.16)
and ∥J(x)∥2 ≤α, ∀x ∈D. Assume that there exist x∗∈D and λ, σ ≥0 such
that J(x∗)T r(x∗) = 0, λ is the smallest eigenvalue of J(x∗)T J(x∗), and
∥(J(x) −J(x∗))T r(x∗)∥2 ≤σ∥x −x∗∥2, ∀x ∈D.
(7.2.17)
If σ < λ, then, for any c ∈(1, λ/σ), there exists ϵ > 0 such that for all
x0 ∈N(x∗, ϵ), the sequence generated by Gauss-Newton Algorithm 7.2.1 is
well-deﬁned, converges to x∗, and satisﬁes
∥xk+1 −x∗∥2 ≤cσ
λ ∥xk −x∗∥2 + cαγ
2λ ∥xk −x∗∥2
2
(7.2.18)
and
∥xk+1 −x∗∥2 ≤cσ + λ
2λ
∥xk −x∗∥2 < ∥xk −x∗∥2.
(7.2.19)
Proof.
By induction. For convenience, let J0, r0, r∗denote J(x0), r(x0)
and r(x∗). From Theorem 1.2.5, it follows that there exists ϵ1 > 0 such that
JT
0 J0 is nonsingular and satisﬁes
∥(JT
0 J0)−1∥≤c/λ, for x0 ∈N(x∗, ϵ1).
(7.2.20)
Let
ϵ = min
*
ϵ1, λ −cσ
cαγ
+
,
(7.2.21)

7.2. GAUSS-NEWTON METHOD
359
where γ is the Lipschitz constant deﬁned in (7.2.16). Then, x1 is well-deﬁned
at the ﬁrst iteration, and we have
x1 −x∗
=
x0 −x∗−(JT
0 J0)−1JT
0 r0
=
−(JT
0 J0)−1[JT
0 r0 + JT
0 J0(x∗−x0)]
=
−(JT
0 J0)−1[JT
0 r∗−JT
0 (r∗−r0 −J0(x∗−x0))]. (7.2.22)
By Theorem 1.2.22, we have
∥r∗−r0 −J0(x∗−x0)∥≤γ
2∥x0 −x∗∥2.
(7.2.23)
Noting that J(x∗)T r(x∗) = 0 and using (7.2.17), we get
∥JT
0 r∗∥= ∥(J0 −J(x∗))T r∗∥≤σ∥x −x∗∥.
(7.2.24)
By using (7.2.20), (7.2.24), (7.2.23) and ∥J0∥≤α, it follows from (7.2.22)
that
∥x1 −x∗∥
≤
∥(JT
0 J0)−1∥(∥JT
0 r∗∥+ ∥J0∥∥r∗−r0 −J0(x∗−x0)∥)
≤
c
λ
 
σ∥x0 −x∗∥+ αγ
2 ∥x0 −x∗∥2
!
.
(7.2.25)
This proves that (7.2.18) holds at k = 0.
Furthermore, from (7.2.25) and (7.2.21), we deduce that
∥x1 −x∗∥
≤
∥x0 −x∗∥
 cσ
λ + cαγ
2λ ∥x0 −x∗∥
!
≤
∥x0 −x∗∥
 cσ
λ + λ −cσ
2λ
!
=
cσ + λ
2λ
∥x0 −x∗∥
<
∥x0 −x∗∥,
(7.2.26)
which shows that (7.2.19) holds at k = 0.
For the general case of k, the proof is the same completely as the above.
Hence, we complete the proof by induction.
2
Theorem 7.2.4 Assume that the assumptions of Theorem 7.2.2 or Theorem
7.2.3 are satisﬁed. If r(x∗) = 0, then there exists ϵ > 0 such that for any x0 ∈
N(x∗, ϵ), the sequence {xk} generated by Gauss-Newton method converges to
x∗with quadratic convergence rate.

360
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
Proof.
For Theorem 7.2.2, if r(x∗) = 0, then S(x∗) = 0. So, the quadratic
convergence rate is obtained immediately from (7.2.6).
For Theorem 7.2.3, if r(x∗) = 0, then the σ in (7.2.17) can be taken as
σ = 0. Hence, it follows from (7.2.19) that {xk} converges to x∗, and from
(7.2.18) that the rate is quadratic.
2
Gauss-Newton method now is the most basic method for solving nonlinear
least-squares problems. The following example demonstrates that it works
well with small residual problems.
Example 7.2.5 Let r1(x) = x + 1, r2(x) = λx2 + x −1. Consider
min f(x) =
2

i=1
ri(x)2 = (x + 1)2 + (λx2 + x −1)2,
where n = 1, m = 2, and x∗= 0. For λ = 0.1, the Gauss-Newton iteration
has the following result:
k
1
2
3
4
5
6
xk
1.000000
0.131148
0.013635
0.001369
0.000137
0.000014
You can see that, when λ = 0.1, the degree of nonlinearity in r(x) is
small, and the Gauss-Newton method works well. In this case, from (7.2.2),
the Gauss-Newton iteration is
xk+1 = 2λ2x3
k + λx2
k + 2λxk
1 + (2λxk + 1)2
.
When λ = 0, in which case r(x) is linear, then x1 = 0 = x∗. This indicates
that Gauss-Newton method gets its minimizer in one iteration. When λ ̸= 0,
we have
xk+1 = λxk + O(∥xk∥2).
When λ is small enough, the convergence rate is linear. When |λ| > 1, the
Gauss-Newton method fails to converge. This example shows that Gauss-
Newton method is valuable only when both x0 closes to x∗and the matrix
S(x∗) is small.
Remark: In practice, we usually use Gauss-Newton method with line
search
xk+1 = xk −αk(J(xk)T J(xk))−1J(xk)T r(xk),
(7.2.27)

7.2. GAUSS-NEWTON METHOD
361
which is called the damped Gauss-Newton method, where αk is a step size.
As we have seen, this method guarantees the descent of the objective function
in each step and therefore global convergence.
To conclude this section, we mention some numerical aspects on Gauss-
Newton method.
It should be pointed out that for the problem to solve
Gauss-Newton equations
J(xk)T J(xk)s = −J(xk)T r(xk),
(7.2.28)
usually, we employ matrix factorization instead of solving (7.2.28) directly.
Then the solution is found by back-substitution technique. So, we can sub-
stantially improve the numerical precision. To see this, it follows from the
error analysis that
∥δs∥
∥s∥≤κ(J(xk)T J(xk))
∥E∥
∥J(xk)T J(xk)∥,
(7.2.29)
where
κ(J(xk)T J(xk)) = σ2
1/σ2
n,
(7.2.30)
δs and E denote the errors of s and J(xk)T J(xk) respectively, and σ1 and
σn are the largest and smallest singular values of J(xk) respectively.
If we employ QR factorization for the augmented matrix, we have
[J(xk)
rk] = Q[R
QT rk],
(7.2.31)
where Q is an orthogonal matrix,
R =
 R1
0

,
and R1 is a nonsingular upper triangular matrix. Then, we obtain
J(xk)T J(xk) = RT R = RT
1 R1.
(7.2.32)
The solution of (7.2.28) can be found by solving
R1s = −(QT rk)n,
(7.2.33)
where (·)n denotes the ﬁrst n element partition.
It can be shown that
κ(R1) = σ1
σn
(7.2.34)
and therefore the errors generated by the orthogonal factorization approach
are magniﬁed much less than that directly solve (7.2.28).

362
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
7.3
Levenberg-Marquardt Method
7.3.1
Motivation and Properties
Usually, Gauss-Newton method with line search is employed in practice.
However, if J(x) is rank-deﬁcient, then either the Gauss-Newton method
cannot work well, or the algorithm converges to a non-stationary point.
To overcome the diﬃculty, we consider employing trust-region technique
(for details, see §6.1). In fact, we have seen that, in Gauss-Newton method,
a linearized model (7.2.4) is used to replace nonlinear function r(x), and
that a linear least-squares problem (7.2.5) is obtained. Unfortunately, this
linearization is not eﬀective for all (x −xk). Therefore, we put a constraint
of trust-region on it, and consider the following trust-region model:
min
1
2∥J(xk)(x −xk) + r(xk)∥2
2
(7.3.1)
s.t.
∥x −xk∥2 ≤∆k,
(7.3.2)
which is a constrained linear least-squares problem. Model (7.3.1)-(7.3.2) can
be written as
min
qk(x) = 1
2∥rk∥2 + rT
k J(xk)(x −xk) + 1
2(x −xk)T J(xk)T J(xk)(x −xk)
s.t.
∥x −xk∥2 ≤∆k.
(7.3.3)
Set s = x−xk. The solution of the subproblem (7.3.1)-(7.3.2) is characterized
by solving the system
(J(xk)T J(xk) + µkI)s = −J(xk)T r(xk).
(7.3.4)
Hence,
xk+1 = xk −(J(xk)T J(xk) + µkI)−1J(xk)T r(xk).
(7.3.5)
When ∥(J(xk)T J(xk))−1J(xk)T r(xk)∥≤∆k, then µk = 0 and the sub-
problem is solved by sk. Otherwise, there exists µk > 0 such that the solution
sk satisfying ∥sk∥= ∆k and
(J(xk)T J(xk) + µkI)sk = −J(xk)T r(xk).
(7.3.6)
Since (J(xk)T J(xk) + µkI) is positive deﬁnite, the direction s produced by
(7.3.4) is a descent direction. This method is called the Levenberg-Marquardt
method, since it was proposed by Levenberg [199] and Marquardt [210]. The

7.3. LEVENBERG-MARQUARDT METHOD
363
above discussion exposes that the Levenberg-Marquardt method is just a
Gauss-Newton method by replacing the line search with a trust region strat-
egy.
Another perspective about Levenberg-Marquardt method is as follows.
This method is just a switch rule between Gauss-Newton method and the
steepest descent method. This implies that this method allows choosing any
direction between these two directions to be a search direction. When µk = 0,
it reduces to the Gauss-Newton direction. While µk is very large, (7.3.4) is
approximate to
µkIs = −J(xk)T r(xk).
(7.3.7)
The produced direction is close to the steepest descent direction.
Furthermore, if, instead of I, we employ some positive deﬁnite and diag-
onal matrix Dk, then (7.3.4) becomes
(J(xk)T J(xk) + µkDk)s = −J(xk)T r(xk).
(7.3.8)
In this case, the produced direction is a combination of Gauss-Newton di-
rection and the steepest descent direction with respect to a metric matrix
Dk.
Next, we will describe some properties of Levenberg-Marquardt method.
Let s = s(µ) be a solution of
(JT J + µI)s = −JT r,
(7.3.9)
where, for convenience, J = J(x), r = r(x), and g = g(x) = JT r.
Theorem 7.3.1 When µ increases monotonically from zero, ∥s(µ)∥will de-
crease strictly monotonically.
Proof.
d
dµ∥s∥= d
dµ(sT s)
1
2 =
sT ds
dµ
∥s∥.
(7.3.10)
Diﬀerentiating (7.3.9) gives
(JT J + µI) ds
dµ = −s.
(7.3.11)
It follows from (7.3.11) and (7.3.9) that
ds
dµ = (JT J + µI)−2g.
(7.3.12)

364
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
By substituting (7.3.12) into (7.3.10) and using (7.3.9), we obtain
d
dµ∥s∥= −gT (JT J + µI)−3g
∥s∥
.
(7.3.13)
When µ ≥0, JT J + µI is positive deﬁnite. Therefore (7.3.13) shows that
∥s(µ)∥decreases strictly monotonically.
2
Theorem 7.3.2 The angle ψ between s and −g does not increase monoton-
ically as µ increases.
Proof.
Since
cos ψ = −
gT s
∥g∥∥s∥,
(7.3.14)
then it is enough to prove
d
dµ cos ψ ≥0.
By using (7.3.9)-(7.3.13), we deduce
d
dµ(cos ψ)
=
−gT ds
dµ
∥g∥∥s∥+
gT s
∥g∥∥s∥
d∥s∥
dµ
∥s∥
=
1
∥g∥∥s∥3 {−(gT (JT J + µI)−2g)2
+(gT (JT J + µI)−1g)(gT (JT J + µI)−3g)}. (7.3.15)
So, it is enough to prove that the part in braces is greater than or equal to
zero.
Note that JT J is symmetric, then there is an orthogonal matrix Q such
that
JT J = QT DQ,
where D = diag(λ1, · · · , λn). Set v = Qg, then the part in braces on the
right-hand-side of (7.3.15) can be written as
n

j=1
n

k=1

−
v2
j v2
k
(λj + µ)2(λk + µ)2 +
v2
j v2
k
(λj + µ)(λk + µ)3
3
.
(7.3.16)

7.3. LEVENBERG-MARQUARDT METHOD
365
If let
a =
 
v1
(λ1 + µ)3/2 , · · · ,
vn
(λn + µ)3/2
!T
,
b =
 
v1
(λ1 + µ)1/2 , · · · ,
vn
(λn + µ)1/2
!T
,
then (7.3.16) becomes
∥a∥2∥b∥2 −⟨a, b⟩2,
and it follows from Schwartz inequality that the above expression is greater
than or equal to zero. Therefore ψ is not increasing. We complete the proof.
2
Theorem 7.3.3 Let µk > 0 and sk be a solution of (7.3.4). Then sk is a
global solution of the subproblem
min
q(k)(s) = 1
2∥Jks + rk∥2
2
(7.3.17)
s.t.
∥s∥≤∥sk∥.
(7.3.18)
Proof.
Since sk is a solution of (7.3.4), then
q(k)(sk)
=
1
2rT
k rk + rT
k Jksk + 1
2sT
k JT
k Jksk
=
1
2rT
k rk −sT
k (JT
k Jk + µkI)sk + 1
2sT
k JT
k Jksk
=
1
2rT
k rk −µksT
k sk −1
2sT
k JT
k Jksk.
(7.3.19)
On the other hand, for any s, we have
q(k)(s)
=
1
2rT
k rk + sT JT
k rk + 1
2sT JT
k Jks
=
1
2rT
k rk −sT (JT
k Jk + µkI)sk + 1
2sT JT
k Jks
=
1
2rT
k rk −µksT sk −sT JT
k Jksk + 1
2sT JT
k Jks.
(7.3.20)
Then, for any s satisfying ∥s∥≤∥sk∥, we deduce that
q(k)(s) −q(k)(sk)
=
1
2(sk −s)T JT
k Jk(sk −s) + µk(sT
k sk −sT sk)
≥
1
2(sk −s)T JT
k Jk(sk −s) + µk∥sk∥(∥sk∥−∥s∥)
≥
0,
(7.3.21)

366
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
which shows that sk is a global optimal solution of problem (7.3.17)-(7.3.18).
2
Theorem 7.3.4 The vector sk is a solution of problem (7.3.1)-(7.3.2), i.e.,
min
1
2∥Jks + rk∥2
2
(7.3.22)
s.t.
∥s∥≤∆k
(7.3.23)
for some ∆k > 0 if and only if there exists µ ≥0 such that
(JT
k Jk + µI)sk = −JT
k rk,
(7.3.24)
µ(∆k −∥sk∥) = 0,
(7.3.25)
∥sk∥≤∆k.
(7.3.26)
Proof.
It is obtained directly from Theorem 6.1.2.
2
Usually, Levenberg-Marquardt method is characterized by the equation
(J(xk)T J(xk) + µkD(xk))s = −J(xk)T r(xk),
(7.3.27)
where D(xk) is a diagonal and positive deﬁnite matrix. The steplength factor
αk satisﬁes Armijio rule (2.5.3):
f(xk + αksk) ≤f(xk) + σαkgT
k sk,
σ ∈
 
0, 1
2
!
.
(7.3.28)
Theorem 7.3.5 For (7.3.27), the condition number of J(x)T J(x) + µD(x)
is a non-increasing function of µ.
Proof.
Let β1 and βn be the largest and smallest eigenvalues of D(x) re-
spectively. Let λ1 and λn be the largest and smallest eigenvalues of J(x)T J(x)+
µD(x) respectively. Let also µ1 > µ2 ≥0. Since the range of a normal matrix
is a convex hull of its spectrum, we have
λ1(µ1)
λn(µ1)
≤
λ1(µ2) + (µ1 −µ2)β1
λn(µ2) + (µ1 −µ2)βn
≤
λ1(µ2) + (µ1 −µ2)(1 + µ2)−1λ1(µ2)
λn(µ2) + (µ1 −µ2)(1 + µ2)−1λn(µ2)
=
λ1(µ2)
λn(µ2).
Therefore, the conclusion is obtained.
2
This property indicates that the Levenberg-Marquardt method improves
the condition of the equations solved.

7.3. LEVENBERG-MARQUARDT METHOD
367
7.3.2
Convergence of Levenberg-Marquardt Method
In this subsection we establish convergence of the Levenberg-Marquardt method.
Theorem 7.3.6 Let {xk} be a sequence produced by Levenberg-Marquardt
method (7.3.27). Suppose that the step lengths αk are determined by Armijo
rule (7.3.28). If there is a subsequence {xki} that converges to x∗, and if
the corresponding subsequence {JT
kiJki + µkiDki} converges to some positive
deﬁnite matrix P, where Jki = J(xki) and Dki = D(xki) denoting a diagonal
positive deﬁnite matrix, then g(x∗) = 0.
Proof.
(By contradiction) Suppose that g(x∗) ̸= 0. Let
ski = −(JT
kiJki + µkiDki)−1JT
kirki,
s∗= lim ski = −P −1J(x∗)T r(x∗),
where rki = r(xki). Obviously, g(x∗)T s∗< 0. Let β ∈(0, 1), σ ∈(0, 1
2). Let
also m∗be the least non-negative integer m such that
f(x∗+ βms∗) < f(x∗) + σβmg(x∗)T s(x∗).
By continuity, for k suﬃciently large, we have
f(xki + βm∗ski) ≤f(xki) + σβm∗g(xki)T ski.
Hence
f(xki+1) = f(xki + βmkiski) ≤f(xki) + σβm∗g(xki)T ski.
(7.3.29)
By the monotone descent of the method, we have
lim f(xki+1) = lim f(xki) = f(x∗).
Therefore, taking limits on both sides of (7.3.29) yields
f(x∗) ≤f(x∗) + σβm∗g(x∗)T s∗< 0.
This is impossible because σβm∗g(x∗)T s∗< 0. So we complete the proof.
2
The above theorem states the convergence of a subsequence. Below, we
give convergence of the whole sequence.

368
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
Theorem 7.3.7 Suppose that the following assumptions hold:
(a) the level set
L(¯x) = {x | f(x) ≤f(¯x)}
is bounded and closed for any ¯x ∈Rn;
(b) the number of stationary points at which the function values of f(x) are
the same is ﬁnite;
(c) J(x)T J(x) is positive deﬁnite ∀x;
(d) µk ≤M < ∞, ∀k, that is, M is an upper bound of µk.
Then for any initial point x0, the sequence {xk} generated from Levenberg-
Marquardt method converges to a stationary point of f(x).
Proof.
From (a) and the monotone property of iterative function, we know
that the sequence {xk} is in compact set L(¯x). This shows that {xk} must
have accumulation points. To prove the theorem, we only need to prove the
accumulation points are unique.
By (c), (d) and Theorem 7.3.6, we have that each accumulation point of
{xk} is unique. Since {f(x)} is a monotone descent sequence, f(x) has the
same values at accumulation points of {xk}. Also, from (b), it follows that
the number of stationary points of f on L(¯x) are ﬁnite. Therefore, there are
only ﬁnitely many accumulation points.
Notice that, for some subsequence {xki}, we have xki →ˆxk and
lim
k→∞g(xki) = g(ˆxk) = 0.
Notice also that
s(µki) = −(J(xki)T J(xki) + µkiD(xki))−1g(xki).
Then it follows from (c) and (d) that s(µki) →0. Therefore, for sequence
{s(µk)}, we have s(µk) →0.
Assume for the moment that there are more than one accumulation point
of {xk}. Let ϵ∗be the smallest distance between any two accumulation points.
Since {xk} is in a compact set, there exists a positive integer N, such that
for all k ≥N, xk is contained in a closed ball with some accumulation point

7.3. LEVENBERG-MARQUARDT METHOD
369
as center and ϵ∗/4 as radius. On the other hand, there is an integer N′ ≥N
such that
∥s(µk)∥< ϵ∗/4, ∀k ≥N ′.
Therefore, when k ≥N ′, all xk are in the closed ball mentioned above with
that accumulation point as center and ϵ∗/4 as radius. Then we have a con-
tradiction which proves the theorem.
2
The above theorem establishes global convergence of the Levenberg-Marqu-
ardt method.
In the following, similar to Theorem 7.2.2, we discuss the
convergence rate of the Levenberg-Marquardt method.
Theorem 7.3.8 Suppose that the iterates xk generated by Levenberg-Marquardt
method converge to a stationary point x∗. Let l be the smallest eigenvalue of
J(x∗)T J(x∗), M the maximum of absolute values of eigenvalues of S(x∗) =

m
i=1 ri(x∗)∇2ri(x∗). If
τ = M/l < 1, 0 < β < (1 −τ)/2, µk →0,
(7.3.30)
then, for all k suﬃciently large, the stepsize αk = 1,
lim sup ∥xk+1 −x∗∥
∥xk −x∗∥
≤τ,
(7.3.31)
and x∗is a strict local minimizer of f(x).
Proof.
We ﬁrst prove αk = 1 for k large enough. Notice that
f(xk + sk) −f(xk) = gT
k sk + 1
2sT
k G(xk + θsk)sk,
(7.3.32)
where θ ∈(0, 1). By means of Armijo rule (7.3.28), to prove αk = 1 for k
large enough, we need to show
βgT
k sk −[f(xk + sk) −f(xk)] ≥0.
(7.3.33)
By use of gk = −(JT
k Jk + µkDk)sk and (7.3.32), the left-hand side of (7.3.33)
can be written as
(1 −β)sT
k (JT
k Jk + µkDk)sk −1
2sT
k G(xk + θsk)sk
=
sT
k

(1 −β)JT
k Jk −1
2G(xk) + (1 −β)µkDk
−1
2(G(xk + θsk) −G(xk))

sk
=
sT
k
 1
2 −β
!
JT
k Jk −1
2S(xk) + Vk

sk,

370
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
where Vk = (1 −β)µkDk −1
2(G(xk + θsk) −G(xk)), S(xk) is deﬁned by
(7.1.7). Since Vk →0, to prove (7.3.33) holds for k large enough, we show
(1
2 −β)JT
k Jk −1
2S(xk) converges to a positive deﬁnite matrix. Note that the
smallest eigenvalue of
 1
2 −β
!
J(x∗)T J(x∗) −1
2S(x∗)
is bounded below and that the lower bound is
 1
2 −β
!
l −1
2M = l
1
2 −β −1
2τ

> 0,
which holds because the second inequality in (7.3.30) is met for β. So we
obtain αk = 1 for suﬃciently large k.
Second, we prove (7.3.31). By (7.3.27) and (7.1.7), we have
xk+1 −x∗
=
xk −x∗−(JT
k Jk + µkDk)−1gk
=
xk −x∗−(JT
k Jk + µkDk)−1[Gk(xk −x∗)
+gk + Gk(x∗−xk)]
=
−(JT
k Jk + µkDk)−1[S(xk)(xk −x∗)
−µkDk(xk −x∗) + gk + Gk(x∗−xk)].
(7.3.34)
Taking norm gives
∥xk+1 −x∗∥
≤
∥(JT
k Jk)−1∥(∥S(xk)∥∥xk −x∗∥
+µk∥Dk∥∥xk −x∗∥+ ∥gk + Gk(x∗−xk)∥).(7.3.35)
Since
∥gk + Gk(x∗−xk)∥
=
∥gk −g(x∗) −Gk(xk −x∗)∥
≤
ϵk∥xk −x∗∥,
(7.3.36)
where ϵk →0, then dividing the both sides of (7.3.35) by ∥xk −x∗∥deduces
∥xk+1 −x∗∥
∥xk −x∗∥
≤∥(JT
k Jk)−1∥(∥S(xk)∥+ µk∥Dk∥+ ϵk).
(7.3.37)
Note that µk →0 and that ϵk →0, and it follows immediately that
lim sup ∥xk+1 −x∗∥
∥xk −x∗∥
≤M
l = τ,

7.3. LEVENBERG-MARQUARDT METHOD
371
which proves (7.3.31).
Finally, since g(x∗) = 0 and G(x∗) = J(x∗)T J(x∗)+S(x∗) with the lower
bound l −M > 0 of the smallest eigenvalue, then G(x∗) is positive deﬁnite.
Hence x∗is a strict local minimizer of f(x).
2
As mentioned above, the Levenberg-Marquardt method can be described
and analyzed by use of the framework of trust region method (7.3.1)–(7.3.2)
or (7.3.3). So, following the discussions of Section 6.1, we immediately have
the following algorithm and theorem which are straightforward consequences
of Algorithm 6.1.1 and Theorem 6.1.9, respectively.
Algorithm 7.3.9 (Trust-Region Type Levenberg-Marquardt Algorithm)
Step 1. Given initial point x0, ¯∆, ∆0 ∈(0, ¯∆), ϵ ≥0, 0 < η1 ≤η2 < 1
and 0 < γ1 < 1 < γ2, k := 0.
Step 2. If ∥gk∥= ∥JT
k rk∥≤ϵ, stop.
Step 3. Approximately solve the subproblem (7.3.1)–(7.3.2) for sk.
Step 4. Compute
Predk = f(xk) −qk(sk),
Aredk = f(xk) −f(xk + sk),
rk = Aredk
Predk
.
Step 5. If rk < η1, set ∆k = γ1∆k and go to Step 3.
Step 6. Set xk+1 = xk + sk. Set
∆k+1 =

min{γ2∆k, ¯∆},
if rk ≥η2 and ∥sk∥= ∆k,
∆k,
otherwise.
Step 7. Set k := k + 1, go to Step 2.
2
From Step 3 of the above algorithm, sk is the approximate solution of
subproblem (7.3.1)–(7.3.2). It follows from Lemma 6.1.3 that
qk(0) −qk(sk) ≥c1∥JT
k rk∥min

∆k, ∥JT
k rk∥
∥JT
k Jk∥

(7.3.38)

372
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
for some constant c1 > 0.
Now we can state the convergence result which is a straightforward con-
sequence of Theorem 6.1.9.
Theorem 7.3.10 Suppose that the function f(x) = 1
2

m
i=1[ri(x)]2 is twice
continuously diﬀerentiable, that the level set
L(x0) = {x | f(x) ≤f(x0)}
is bounded, and that there are constants M1 > 0, M2 > 0 such that
∥∇2f(x)∥≤M1,
∀x ∈L(x0),
∥J(x)T J(x)∥≤M2,
∀x ∈L(x0).
Then we have that
lim
k→∞∇f(xk) = lim
k→∞JT
k rk = 0.
(7.3.39)
7.4
Implementation of L-M Method
There are various implementations of the Levenberg-Marquardt method.
Mor´e [218] gave an eﬃcient and reliable implementation, which is contained
in the MINPACK software package.
The Levenberg-Marquardt method Mor´e [218] considered is to ﬁnd s by
means of solving equations
s(µ) = −(JT
k Jk + µkDT
k Dk)−1JT
k rk,
(7.4.1)
which correspond to a trust region subproblem (or a constrained linear least-
squares problem)
min ∥rk + Jks∥s.t. ∥Dks∥≤∆k,
(7.4.2)
where ∆k > 0 is the trust-region radius. If Jk is singular and µk = 0, the
solution of (7.4.2) can be deﬁned by a limit
Dks(0) =
lim
µk→0+ Dks(µk) = −(JkD−1
k )+rk.
(7.4.3)
There are two possibilities: either µk = 0 and ∥Dks(0)∥≤∆k, in which case
s(0) is the solution of (7.4.2); or µk > 0 and ∥Dks(µk)∥= ∆k, in which case
s(µk) is a unique solution of (7.4.2). Hence we can describe the following
algorithm.

7.4. IMPLEMENTATION OF L-M METHOD
373
Algorithm 7.4.1 (Levenberg-Marquardt Algorithm)
(a) Given ∆k > 0, ﬁnd µk ≥0 such that
(JT
k Jk + µkDk)sk = −JT
k rk.
Then either µk = 0 and ∥Dksk∥≤∆k, or µk > 0 and
∥Dksk∥= ∆k.
(b) If ∥r(xk + sk)∥≤∥r(xk)∥, set xk+1 = xk + sk and compute
Jk+1; otherwise set xk+1 = xk and Jk+1 = Jk.
(c) Choose ∆k+1 and Dk+1.
2
In the following, we discuss how to perform the above algorithm eﬃciently
and reliably.
(1) How to solve the trust-region subproblem (i.e., constrained linear
least-squares problem).
For equations
(JT
k Jk + µkDT
k Dk)s = −JT
k rk,
(7.4.4)
the simplest way is using Cholesky factorization. However, because of the
special structure of the coeﬃcient matrix in (7.4.4), it is easy to use QR
factorization.
Note that (7.4.4) are just the normal equations for linear least-squares
problem

Jk
µ1/2
k
Dk

s ∼= −

r
0

.
(7.4.5)
For the structure of (7.4.5), instead of computing JT
k Jk and DT
k Dk and their
Cholesky factorization, we can use column pivoting QR factorization.
Now we describe the two-step QR factorization to ﬁnd the solution of the
linear least-squares problem (7.4.5).
First Step: Calculate the QR factorization of Jk and obtain
QJkπ =

T
W
0
0

,
(7.4.6)
where Q is orthogonal, T is a nonsingular upper triangular matrix with
rank(T) =rank(Jk), and π is a permutation matrix. If µk = 0, then the

374
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
solution of (7.4.5) is
s = π

T −1
0
0
0

Qrk ≡J−
k rk,
(7.4.7)
where J−
k denotes {1, 3}-inverse satisfying
JkJ−
k Jk = Jk, JkJ−
k = (JkJ−
k )T
(see X.He and W. Sun [172], Ben-Israel and Greville [12]). If µk > 0, since
(7.4.6) becomes

Q
0
0
πT
 
Jk
µ1/2
k
Dk

π =
⎡
⎢⎣
R
0
Dµ
⎤
⎥⎦,
(7.4.8)
where Dµ = µ1/2
k
πT Dkπ and R is an upper trapezoid matrix, it follows from
(7.4.8) that (7.4.5) becomes
⎡
⎢⎣
R
0
Dµ
⎤
⎥⎦πT s = −

Qr
0

.
(7.4.9)
Second Step: It is easy to eliminate Dµ in (7.4.9) by a sequence of n(n +
1)/2 Givens rotations and obtain
W
⎡
⎢⎣
R
0
Dµ
⎤
⎥⎦=
⎡
⎢⎣
Rµ
0
0
⎤
⎥⎦,
(7.4.10)
where Rµ is a nonsingular upper triangular matrix and W a product of a
sequence of rotations. Then (7.4.9) becomes

Rµ
0

πT s = −W

Qr
0

∆=

u
v

,
(7.4.11)
and we obtain
s = πR−1
µ u.
(7.4.12)
(2) How to update the trust-region radius ∆k.

7.4. IMPLEMENTATION OF L-M METHOD
375
As described in §6.1, the choice of ∆k depends on the ratio between actual
reduction and predicted reduction of the objective function. In the nonlinear
least-squares case, the ratio is
ρ =
∥r(xk)∥2 −∥r(xk + sk)∥2
∥r(xk)∥2 −∥r(xk) + J(xk)sk∥2 ,
(7.4.13)
which measures the agreement between the linearized model and the nonlin-
ear function. For example, if r(x) is linear, then ρ = 1. If J(xk)T r(xk) ̸= 0,
then ρ →1 when ∥sk∥→0. If ∥r(xk + sk)∥≥∥r(xk)∥, then ρ ≤0.
Because of roundoﬀerror, there may be overﬂow when we compute ρ by
(7.4.13). So we write (7.4.13) in a safeguard form. Multiplying both sides of
(7.4.4) by 2sT yields
−2rT
k JT
k s = 2sT JT
k Jks + 2µksDT
k Dks,
which is
rT
k rk −rT
k rk −2rT
k JT
k s −sT JT
k Jks = sT JT
k Jks + 2µksT DT
k Dks.
Then we obtain
∥rk∥2 −∥rk + Jks∥2 = ∥Jks∥2 + 2µk∥Dks∥2.
(7.4.14)
Substituting the above into (7.4.13) gives
ρ =
1 −
∥r(xk + sk)∥
∥r(xk)∥
2
 ∥Jks∥
∥r(xk)∥
2
+ 2

µ1/2
k
∥Dks∥
∥r(xk)∥
2 .
(7.4.15)
It is easy to see from (7.4.14) that
∥Jks∥≤∥r(xk)∥, µ1/2
k
∥Dks∥≤∥r(xk)∥.
Hence the computation in (7.4.15) will not lead to overﬂow. Also, regardless
of roundoﬀerror, the denominator in (7.4.15) is always nonnegative. It should
be mentioned that when ∥r(xk + sk)∥≫∥r(xk)∥, the numerator in (7.4.15)
may be overﬂown.
However, since we are only interested in ρ ≥0, then
when ∥r(xk + sk)∥> ∥r(xk)∥, we set ρ = 0 without needing to compute ρ by
(7.4.15).

376
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
(3) How to ﬁnd a Levenberg-Marquardt parameter.
In the Mor´e algorithm, if
|φ(µ)| ≤σ∆, σ ∈(0, 1),
(7.4.16)
where
φ(µ) = ∥D(JT J + µDT D)−1JT r∥−∆,
(7.4.17)
then µ > 0 is accepted as a Levenberg-Marquardt parameter, where σ indi-
cates the related error in ∥Dks(µ)∥. If φ(0) ≤0, then µ = 0 is a required
parameter. Therefore we only need to discuss the case of φ(0) > 0. Since φ is
a continuous and strictly descending function, then, when µ →∞, we have
φ(µ) →−∆. Thus, there exists a unique µ∗> 0 such that φ(µ∗) = 0. To
determine the Levenberg-Marquardt parameter, we start from µ0 > 0 and
generate a sequence {µk} →µ∗.
From (7.4.17), we have
φ(µ) = ∥( ˜JT ˜J + µI)−1 ˜JT r∥−∆,
(7.4.18)
where ˜J = JD−1. Let ˜J = UΣV T be the singular value decomposition of ˜J,
then
φ(µ) =
n

i=1

σ2
i z2
i
(σ2
i + µ)2
1/2
−∆,
(7.4.19)
where z = UT r, σ1, · · · , σn are singular values of ˜J. Hence we assume
φ(µ) ·=
a
b + µ ≡˜φ(µ)
(7.4.20)
and choose a and b such that ˜φ(µk) = φ(µk), ˜φ′(µk) = φ′(µk). Then ˜φ(µk+1) =
0 if
µk+1 = µk −
φ(µk) + ∆
∆
  φ(µk)
φ′(µk)

.
(7.4.21)
To make computation of µk+1 safe and reliable, the Mor´e algorithm de-
signs the following technique for computing µk+1.
Let
u0 = ∥(JD−1)T r∥
∆
,
l0 =

−φ(0)/φ′(0),
if J is nonsingular,
0,
otherwise,

7.4. IMPLEMENTATION OF L-M METHOD
377
(a) If µk /∈(lk, uk), set µk = max{0.001uk, (lkuk)1/2}.
(b) Compute φ(µk) and φ′(µk). Update uk:
uk+1 =

µk,
if φ(µk) < 0,
uk,
otherwise.
Update lk:
lk+1 = max
*
lk, µk −φ(µk)
φ′(µk)
+
.
(c) Compute µk+1 by (7.4.21).
The above algorithm gives upper and lower bounds of µk. In (a), it shows
that if µk is not in (lk, uk), a point in (lk, uk) inclining to lk will replace µk.
In (b), the convexity of φ guarantees that the Newton’s iteration can be used
to update lk. The sequence {µk} generated by the algorithm will converge
to µ∗. In fact, if we set σ = 0.1, it takes no more than two steps on average
to satisfy (7.4.16).
As to computing φ′(µ), we have from (7.4.17) that
φ′(µ) = −(DT q(µ))T (JT J + µDT D)−1(DT q(µ))
∥q(µ)∥
,
(7.4.22)
where q(µ) = Ds(µ). By (7.4.8) and (7.4.10) we get
πT (JT J + µDT D)π = RT
µRµ.
Then
φ′(µ) = −∥q(µ)∥
R−T
µ

πT DT q(µ)
∥q(µ)∥

2
.
(7.4.23)
(4) How to update the scaling matrix.
In the Levenberg-Marquardt method, Dk is a diagonal matrix which re-
duces the eﬀects of poor scaling of the problems. In the algorithm, we choose
Dk = diag (d(k)
1 , · · · , d(k)
n ),
(7.4.24)
where
d(0)
i
= ∥∂ir(x0)∥,
d(k)
i
= max{d(k−1)
i
, ∥∂ir(xk)∥}, k ≥1.

378
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
We should point out that the above scaling is invariant under scaling, that
is, if D is a diagonal and positive deﬁnite matrix, then for function r(x)
with starting point x0 and for function ˜r(x) = r(D−1x) with starting point
˜x0 = Dx0, Algorithm 7.4.1 generates the same sequence of iterates.
Finally, we give the Mor´e version of the Levenberg-Marquardt algorithm
and the convergence theorem.
Algorithm 7.4.2 (Mor´e’s Version)
(a) Let σ ∈(0, 1). If ∥DkJ−
k rk∥≤(1 + σ)∆k, set µk = 0 and
sk = −J−
k rk; otherwise, determine µk > 0 such that if

Jk
µ1/2
k
Dk

sk ∼= −

rk
0

(i.e., sk is the solution of the above least-squares problem),
then
(1 −σ)∆k ≤∥Dksk∥≤(1 + σ)∆k.
(b) Compute the ratio ρk between the actual reduction and the
predicted reduction of the objective function.
(c) If ρk ≤0.0001, set xk+1 = xk and Jk+1 = Jk.
If ρk > 0.0001, set xk+1 = xk + sk, and compute Jk+1.
(d) If ρk ≤1
4, set ∆k+1 ∈

1
10∆k, 1
2∆k

. If either ρk ∈

1
4, 1
3

and µk = 0, or ρk ≥3
4, then set ∆k+1 = 2∥Dksk∥.
(e) Update Dk by (7.4.24).
2
For the above algorithm, the convergence theorem is stated as follows
without proof. The interested readers may consult Mor´e [218].
Theorem 7.4.3 Let r : Rn →Rm be continuously diﬀerentiable. Let {xk}
be a sequence generated by Algorithm 7.4.2. Then
lim inf
k→+∞∥(JkD−1
k )T rk∥= 0.
(7.4.25)
This result indicates that the scaled gradient is, at last, suﬃciently small. If
{Jk} is bounded, then (7.4.25) implies
lim inf
k→+∞∥JT
k rk∥= 0.
(7.4.26)

7.5. QUASI-NEWTON METHOD
379
Further, if ∇r(x) is uniformly continuous, then
lim
k→+∞∥JT
k rk∥= 0.
(7.4.27)
7.5
Quasi-Newton Method
We have seen from the above sections that, for large-residual problems (i.e.,
r(x) is large or r(x) is severely nonlinear), the performance of the Gauss-
Newton method and Levenberg-Marquardt method is usually poor.
The
convergence is slow and only linear. This is because we don’t use the second-
order information S(x) in Hessian G(x) = J(x)T J(x) + S(x) which is signif-
icant. As mentioned in §7.1, in fact, computation of S(x) is either diﬃcult
or expensive. It is also not suitable to use the secant approximation of the
whole Hessian G(x). So, it may be a good idea to use a secant approximation
of the second information S(x) = 
m
i=1 ri(x)∇2ri(x) in G(x).
Let Bk be a secant approximation of S(xk), then the iteration (7.1.10)
becomes
(J(xk)T J(xk) + Bk)dk = −J(xk)T r(xk).
(7.5.1)
Since
S(xk+1) =
m

i=1
ri(xk+1)∇2ri(xk+1),
(7.5.2)
then we use
Bk+1 =
m

i=1
ri(xk+1)(Hi)k+1
(7.5.3)
to approximate S(xk+1), where (Hi)k+1 is a secant approximation of ∇2ri(xk+1).
Then we have that
(Hi)k+1(xk+1 −xk) = ∇ri(xk+1) −∇ri(xk).
(7.5.4)
Hence, we get
Bk+1(xk+1 −xk)
=
m

i=1
ri(xk+1)(Hi)k+1(xk+1 −xk)
=
m

i=1
ri(xk+1)(∇ri(xk+1) −∇ri(xk))
=
(J(xk+1) −J(xk))T r(xk+1) ∆= yk
(7.5.5)

380
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
which is a quasi-Newton condition imposed on Bk.
Similarly, if we ask
(J(xk+1)T J(xk+1) + Bk+1)sk = J(xk+1)T r(xk+1) −J(xk)T r(xk)
(7.5.6)
to hold, then Bk+1 should satisfy
Bk+1sk = ˜yk,
(7.5.7)
where
˜yk = J(xk+1)T r(xk+1) −J(xk)T r(xk) −J(xk+1)T J(xk+1)sk.
(7.5.8)
Now, we give an update formula for Bk by weighted Frobenius norm. The
following theorem is a restatement of Theorem 5.1.10 in Chapter 5.
Theorem 7.5.1 Let vT
k sk > 0 and T ∈Rn×n be a symmetric and positive
deﬁnite matrix such that
TT T sk = vk,
(7.5.9)
where
vk
∆=
∇f(xk+1) −∇f(xk)
=
J(xk+1)T r(xk+1) −J(xk)T r(xk).
(7.5.10)
Then the update
Bk+1
=
Bk + (yk −Bksk)vT
k + vk(yk −Bksk)T
sT
k vk
−sT
k (yk −Bksk)
(sT
k vk)2
vkvT
k
(7.5.11)
is a unique solution of the minimization problem
min
∥T −T (Bk+1 −Bk)T −1∥F
s.t.
(Bk+1 −Bk) is symmetric, Bk+1sk = yk.
(7.5.12)
Dennis, Gay and Welsch [88] use the quasi-Newton condition (7.5.5) and
(7.5.11), and present a quasi-Newton algorithm NL2SOL with trust region
strategy. At each step, it is required to solve the subproblem
min
1
2r(xk)T r(xk) + (x −xk)T J(xk)T r(xk)
+1
2(x −xk)T (J(xk)T J(xk) + Bk)(x −xk)
s.t.
∥x −xk∥≤∆k.
(7.5.13)

7.5. QUASI-NEWTON METHOD
381
In this algorithm, a deﬁciency of the update (7.5.11) for Bk is that this matrix
is not guaranteed to vanish when the iterates approach to a zero-residual
solution, so it can interfere with superlinear convergence. This problem can
be avoided by a strategy of scaling Bk, that is, we choose a scaling factor
γk = min

sT
k yk
sT
k Bksk
, 1
3
,
(7.5.14)
multiply Bk by γk, and then use (7.5.11) to update it.
Numerical experiments show that, for large-residual problems, quasi-
Newton algorithm NL2SOL is signiﬁcantly advantageous; for small-residual
problems, the performance of NL2SOL and Mor´e’s Levenberg-Marquardt al-
gorithm is similar; for zero-residual problems we prefer the Gauss-Newton
method. Therefore, the Gauss-Newton method, Levenberg-Marquardt method,
and quasi-Newton method introduced in this chapter are very important to
solve nonlinear least-squares problems. Now, Mor´e’s Levenberg-Marquardt
algorithm and quasi-Newton algorithm NL2SOL are very popular.
Similar to the above discussion, Bartholomew-Biggs [15] uses the quasi-
Newton condition (7.5.5) and the following rank-one updating formula
Bk+1 = Bk + (yk −Bksk)(yk −Bksk)T
(yk −Bksk)T sk
,
(7.5.15)
and gives a quasi-Newton method for nonlinear least-squares problems. In
Bartholomew-Biggs’ algorithm, the scaling factor is
γk = rT
k+1rk+1/rT
k rk.
(7.5.16)
Fletcher and Xu [139] presented a hybrid algorithm which combines Gauss-
Newton method and quasi-Newton method. If the current Gauss-Newton
step reduces the function f by a certain ﬁxed amount, i.e.,
f(xk) −f(xk+1) ≥τf(xk), τ ∈(0, 1),
(7.5.17)
we use the Gauss-Newton step.
Otherwise, we use quasi-Newton update
(for example, BFGS update). In the zero-residual case, the method eventu-
ally takes Gauss-Newton steps and gives quadratic convergence; while in the
nonzero-residual case, the method eventually reduces to BFGS formula. The
theoretical analysis shows that Fletcher-Xu method is superlinearly conver-
gent. Normally, we take τ = 0.2 in (7.5.17).

382
CHAPTER 7. NONLINEAR LEAST-SQUARES PROBLEMS
Exercises
1. Solve the least-squares problem
min f(x) = 1
2[(x2 −x2
1)2 + (1 −x1)2],
x(0) = (0, 0)T
by Gauss-Newton method and Levenberg-Marquardt method.
2. Consider nonlinear least-squares problems:
min
x∈Rn f(x) = 1
2r(x)T r(x) = 1
2
m

i=1
[ri(x)]2, m ≥n
where
r1(x) = x2
1 + x2
2 + x2
3 −1,
r2(x) = x2
1 + x2
2 + (x3 −2)2 −1,
r3(x) = x1 + x2 + x3 −1,
r4(x) = x1 + x2 −x3 + 1,
r5(x) = x3
1 + 3x2
2 + (5x3 −x1 + 1)2 −36.
(1) Compute ∇f(x), J(x)T J(x), and ∇2f(x).
(2) Please answer whether J(x)T J(x) = ∇2f(x) holds for x = (0, 0)T , and
why?
3. Prove (7.2.7)–(7.2.9).
4. Suppose that the function f(x) = 1
2

m
i=1[ri(x)]2 is twice continuously
diﬀerentiable, and that the level set
L(x0) = {x | f(x) ≤f(x0)}
is bounded. Let the sequence {xk} generated by trust-region type Levenberg-
Marquardt Algorithm 7.3.9 converge to x∗with positive deﬁnite ∇2f(x∗) and
S(x∗) =
m

i=1
ri(x∗)∇2ri(x∗) = 0.
Prove that {xk} converges to x∗with quadratic rate.

7.5. QUASI-NEWTON METHOD
383
5. Let r ∈Rm, J ∈Rm×n, µ > 0. Prove that s = −(JT J + µI)−1JT r is
the solution of the least squares problem
min ∥Ws + y∥2,
where
W =

J
µ
1
2 I

,
y =

r
0

.
6. Consider nonlinear least-squares problems
min
x∈Rn f(x) = 1
2r(x)T r(x) = 1
2
m

i=1
[ri(x)]2, m ≥n
where r : Rn →Rm is a nonlinear function of x and its Jacobian matrix
J(x) is full-rank for all x ∈Rn. Denote the Gauss-Newton direction, the
Levenberg-Marquardt direction, and the steepest descent direction respec-
tively by sGN, sLM, and sC:
sGN = −(JT J)−1JT r,
sLM = −(JT J + µI)−1JT r,
sC = −JT r.
Prove that
lim
µ→0 sLM(µ) = sGN,
lim
µ→∞
sLM(µ)
∥sLM(µ)∥=
sC
∥sC∥.

Chapter 8
Theory of Constrained
Optimization
8.1
Constrained Optimization Problems
A general formulation for nonlinear constrained optimization is
min
x∈Rn
f(x)
(8.1.1)
s.t.
ci(x) = 0, i = 1, · · · , me;
(8.1.2)
ci(x) ≥0, i = me + 1, · · · , m,
(8.1.3)
where the objective function f(x) and the constrained functions ci(x), (i =
1, · · · , m) are all smooth, real-valued functions on Rn, and at least one is non-
linear, and me and m are nonnegative integers with 0 ≤me ≤m. Sometimes,
we set
E = {1, · · · , me} and I = {me + 1, · · · , m}
as index sets of equality constraints and inequality constraints, respectively.
If m = 0, the problem (8.1.1)-(8.1.3) is an unconstrained optimization prob-
lem; if me = m ̸= 0, the problem is called an equality constrained opti-
mization problem; if all ci(x)(i = 1, · · · , m) are linear functions, the problem
(8.1.1)-(8.1.3) is called a linearly constrained optimization problem. A lin-
early constrained optimization problem with quadratic objective function
f(x) is said to be a quadratic programming problem which will be discussed
in Chapter 9.

386
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Deﬁnition 8.1.1 The point x ∈Rn is said to be a feasible point if and only
if (8.1.2)-(8.1.3) hold. The set of all feasible points is said to be a feasible
set.
In problem (8.1.1)–(8.1.3), (8.1.2)-(8.1.3) are constrained conditions. From
Deﬁnition 8.1.1, the feasible point is the point satisfying all constraints. We
write the feasible set X as
X =

x

ci(x) = 0,
i = 1, · · · , me;
ci(x) ≥0,
i = me + 1, · · · , m
3
.
(8.1.4)
or
X = {x | ci(x) = 0, i ∈E; ci(x) ≥0, i ∈I}.
(8.1.5)
So, we can rewrite problem (8.1.1)-(8.1.3) as
min
x∈X f(x)
(8.1.6)
which means that solution of constrained optimization problem (8.1.1)-(8.1.3)
is just to ﬁnd a point x on the feasible set X, such that the objective function
f(x) is minimized.
In the following, we give some deﬁnitions about local and global minimiz-
ers.
Deﬁnition 8.1.2 If x∗∈X and if
f(x) ≥f(x∗), ∀x ∈X,
(8.1.7)
then x∗is said to be a global minimizer of the problem (8.1.1)–(8.1.3). If
x∗∈X and if
f(x) > f(x∗), ∀x ∈X, x ̸= x∗,
(8.1.8)
then x∗is said to be a strict global minimizer.
Deﬁnition 8.1.3 If x∗∈X and if there is a neighborhood B(x∗, δ) of x∗
such that
f(x) ≥f(x∗), ∀x ∈X ∩B(x∗, δ),
(8.1.9)
then x∗is said to be a local minimizer of problem (8.1.1)–(8.1.3), where
B(x∗, δ) = {x | ∥x −x∗∥2 ≤δ}
(8.1.10)

8.1. CONSTRAINED OPTIMIZATION PROBLEMS
387
and δ > 0.
If x∗∈X and if there is a neighborhood B(x∗, δ) of x∗such that
f(x) > f(x∗), ∀x ∈X ∩B(x∗, δ), x ̸= x∗,
(8.1.11)
then x∗is said to be a strict local minimizer.
Deﬁnition 8.1.4 If x∗∈X and if there is a neighborhood B(x∗, δ) of x∗
such that x∗is the only local minimizer in X ∩B(x∗, δ), then x∗is an isolated
local minimizer.
Obviously, a global minimizer is also a local minimizer.
Assume that x∗is a local minimizer of problem (8.1.1)–(8.1.3), if there is
an index i0 ∈I = [me + 1, m] such that
ci0(x∗) > 0,
(8.1.12)
then, if we delete the i0-th constraint, x∗is still the local minimizer of the
problem obtained by deleting i0-th constraint. Thus, we say that the i0-th
constraint is inactive at x∗. Now, we give the deﬁnitions of active constraint
and inactive constraint. First, write
I(x) = {i | ci(x) = 0, i ∈I}.
(8.1.13)
Deﬁnition 8.1.5 For any x ∈Rn, the set
A(x) = E ∪I(x)
(8.1.14)
is an index set of active constraints at x, ci(x) (i ∈A(x)) is an active con-
straint at x, ci(x) (i /∈A) is an inactive constraint at x.
Assume that A(x∗) is an index set of the active constraints of problem
(8.1.1)–(8.1.3) at x∗, then, from the observation about inactive constraints,
it is enough for us to solve the constrained optimization problem
min
f(x)
s.t.
ci(x) = 0, i ∈A(x∗).
(8.1.15)
In general, it is easier to solve equality constraint problem (8.1.15) than the
original problem (8.1.1)–(8.1.3).

388
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
8.2
First-Order Optimality Conditions
In this section we discuss the ﬁrst-order optimality conditions.
Since the
feasible directions play a very important role in deriving the optimality con-
ditions, we ﬁrst give some deﬁnitions of several feasible directions.
Deﬁnition 8.2.1 Let x∗∈X, 0 ̸= d ∈Rn. If there exists δ > 0 such that
x∗+ td ∈X, ∀t ∈[0, δ],
then d is said to be a feasible direction of X at x∗. The set of all feasible
directions of X at x∗is
FD(x∗, X) = {d | x∗+ td ∈X, ∀t ∈[0, δ]}.
(8.2.1)
Deﬁnition 8.2.2 Let x∗∈X and d ∈Rn. If
dT ∇ci(x∗) = 0,
i ∈E,
dT ∇ci(x∗) ≥0,
i ∈I(x∗),
then d is said to be a linearized feasible direction of X at x∗. The set of all
linearized feasible directions of X at x∗is
LFD(x∗, X) =

d

dT ∇ci(x∗) = 0,
i ∈E
dT ∇ci(x∗) ≥0,
i ∈I(x∗)
3
.
(8.2.2)
Deﬁnition 8.2.3 Let x∗∈X and d ∈Rn. If there exist sequences dk(k =
1, 2, · · ·) and δk > 0, (k = 1, 2, · · ·) such that x∗+ δkdk ∈X, ∀k and dk →
d, δk →0, then the limiting direction d is called the sequential feasible di-
rection of X at x∗. The set of all sequential feasible directions of X at x∗
is
SFD(x∗, X) =

d

x∗+ δkdk ∈X, ∀k
dk →d, δk →0
3
.
(8.2.3)
In the deﬁnition above, if set xk = x∗+δkdk, then {xk} is a feasible point
sequence that satisﬁes:
(1) xk ̸= x∗, ∀k;
(2) limk→∞xk = x∗;

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
389
(3) xk ∈X for all k suﬃciently large.
If set δk = ∥xk −x∗∥, then we have
dk =
xk −x∗
∥xk −x∗∥→d,
which means that xk = x∗+δkdk is a feasible point sequence with the feasible
direction d.
Note that if SFD(x∗, X) includes the zero vector, it is referred to as the
tangent cone of X at x∗, i.e.,
TX(x∗) = SFD(x∗, X) ∪{0}.
Obviously, by use of the above deﬁnitions of some feasible directions, we
have the following lemma which indicates the relations of the above sets of
feasible directions FD(x∗, X), SFD(x∗, X) and LFD(x∗, X).
Lemma 8.2.4 Let x∗∈X. If all constraint functions are diﬀerentiable at
x∗, then
FD(x∗, X) ⊆SFD(x∗, X) ⊆LFD(x∗, X).
(8.2.4)
Proof.
For any d ∈FD(x∗, X), it follows from Deﬁnition 8.2.1 that there
is a δ > 0 such that (8.2.1) holds. Set dk = d and δk = δ/2k, then (8.2.3)
holds and clearly dk →d and δk →0. Thus d ∈SFD(x∗, X). Since d is
arbitrary, then
FD(x∗, X) ⊆SFD(x∗, X).
(8.2.5)
Next, for any d ∈SFD(x∗, X), if d = 0, then d ∈LFD(x∗, X). Assume
that d ̸= 0. By Deﬁnition 8.2.3, there exist sequences dk (k = 1, 2, · · ·) and
δk > 0 (k = 1, 2, · · ·) such that (8.2.3) holds, and dk →d ̸= 0 and δk →0. By
use of (8.2.3), we see that x∗+ δkdk ∈X, i.e.,
0 = ci(x∗+ δkdk) = δkdT
k ∇ci(x∗) + o(∥δkdk∥), i ∈E;
(8.2.6)
0 ≤ci(x∗+ δkdk) = δkdT
k ∇ci(x∗) + o(∥δkdk∥), i ∈I(x∗).
(8.2.7)
Dividing the above two equations by δk > 0 and setting k →∞, we obtain
(8.2.2). Thus we also have
SFD(x∗, X) ⊆LFD(x∗, X).
(8.2.8)
Both (8.2.5) and (8.2.8) give the result of (8.2.4).
2

390
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
In order to describe clearly necessary conditions for a local solution, it is
convenient to introduce the set
D(x′) = D′ = {d | dT ∇f(x′) < 0},
(8.2.9)
which is called a set of descent direction at x′.
Now we describe the most basic necessary condition – geometry optimality
condition as follows.
Theorem 8.2.5 (Geometry optimality condition)
Let x∗∈X be a local
minimizer of problem (8.1.1)-(8.1.3). If f(x) and ci(x) (i = 1, 2, · · · , m) are
diﬀerentiable at x∗, then
dT ∇f(x∗) ≥0, ∀d ∈SFD(x∗, X),
(8.2.10)
which means
SFD(x∗, X) ∩D(x∗) = φ,
(8.2.11)
where φ is an empty set.
Proof.
For any d ∈SFD(x∗, X), there exist δk > 0 (k = 1, 2, · · ·) and
dk (k = 1, 2, · · ·) such that x∗+ δkdk ∈X with δk →0 and dk →d. Since
x∗+ δkdk →x∗and x∗is a local minimizer, then for k suﬃciently large, we
have
f(x∗) ≤f(x∗+ δkdk) = f(x∗) + δkdT
k ∇f(x∗) + o(δk)
(8.2.12)
which implies
dT ∇f(x∗) ≥0.
(8.2.13)
Since d is arbitrary, we obtain (8.2.10).
Furthermore, (8.2.13) also implies d /∈D(x∗), and hence SFD(x∗, X) ∩
D(x∗) = φ.
2
If we use terminology of the tangent cone to represent (8.2.10), we have
dT ∇f(x∗) ≥0, ∀d ∈TX(x∗),
i.e.,
−∇f(x∗)T d ≤0, ∀d ∈TX(x∗).
(8.2.14)
This implies that
−∇f(x∗) ∈NX(x∗),
(8.2.15)

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
391
where NX(x∗) is the normal cone of X at x∗.
Theorem 8.2.5 shows that there is no sequential feasible direction at a
local minimizer x∗. Unfortunately, it is not possible to proceed further with-
out constraint qualiﬁcation. In the following, by means of Farkas’ Lemma
1.3.22 and the constraint qualiﬁcation, we can get the ﬁrst-order optimality
condition — the famous Karush-Kuhn-Tucker Theorem.
Farkas’ Lemma 1.3.22 gives the following form.
Lemma 8.2.6 The set
S =
⎧
⎪
⎨
⎪
⎩
d

dT ∇f(x∗) < 0,
dT ∇ci(x∗) = 0, i ∈E,
dT ∇ci(x∗) ≥0, i ∈I
⎫
⎪
⎬
⎪
⎭
(8.2.16)
is empty if and only if there exist real numbers λi, i ∈E and nonnegative real
numbers λi ≥0, i ∈I such that
∇f(x∗) =

i∈E
λi∇ci(x∗) +

i∈I
λi∇ci(x∗).
(8.2.17)
In fact, set
d = −x, ∇f(x∗) = c, A =
⎡
⎢⎣
∇cT
1 (x∗)
...
∇cT
m(x∗)
⎤
⎥⎦, λ = y,
we immediately have that (8.2.16) is just (1.3.49), and that (8.2.17) and
λi ≥0, i ∈I are just (1.3.50). This implies that Lemma 8.2.6 is a direct
consequence of Farkas’ Lemma 1.3.22 and also called Farkas’ Lemma.
It is convenient to state the optimality condition by introducing the La-
grangian function
L(x, λ) = f(x) −
m

i=1
λici(x),
(8.2.18)
where λ = (λ1, · · · , λm)T ∈Rm is a Lagrange multiplier vector.
Now, we are in a position to state the ﬁrst-order necessary condition of a
local minimizer by use of Farkas’ Lemma and Theorem 8.2.5.
Theorem 8.2.7 (Karush-Kuhn-Tucker Theorem)

392
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Let x∗be a local minimizer of problem (8.1.1)–(8.1.3). If the constraint
qualiﬁcation (CQ)
SFD(x∗, X) = LFD(x∗, X)
(8.2.19)
holds, then there exist Lagrange multipliers λ∗
i such that the following condi-
tions are satisﬁed at (x∗, λ∗):
∇f(x∗) −
m

i=1
λ∗
i ∇ci(x∗) = 0,
(8.2.20)
ci(x∗) = 0,
∀i ∈E,
(8.2.21)
ci(x∗) ≥0,
∀i ∈I,
(8.2.22)
λ∗
i ≥0,
∀i ∈I,
(8.2.23)
λ∗
i ci(x∗) = 0,
∀i ∈I.
(8.2.24)
Proof.
Since x∗is a local minimizer, x∗is feasible and the conditions
(8.2.21) and (8.2.22) are satisﬁed.
Let d ∈SFD(x∗, X); since x∗is a local minimizer, it follows from Theo-
rem 8.2.5 that dT ∇f(x∗) ≥0. By constraint qualiﬁcation (8.2.19), we have
d ∈LFD(x∗, X). Thus the system
dT ∇ci(x∗) = 0,
i ∈E,
(8.2.25)
dT ∇ci(x∗) ≥0,
i ∈I(x∗),
(8.2.26)
dT ∇f(x∗) < 0
(8.2.27)
has no solution. By Farkas’ Lemma, we immediately obtain that
∇f(x∗) =

i∈E
λ∗
i ∇ci(x∗) +

i∈I(x∗)
λ∗
i ∇ci(x∗),
(8.2.28)
where λ∗
i ∈R (i ∈E) and λ∗
i ≥0 (i ∈I(x∗)). Setting λ∗
i = 0 (i ∈I\I(x∗)), it
follows that
∇f(x∗) =
m

i=1
λ∗
i ∇ci(x∗),
which is (8.2.20). It is obvious that λ∗
i ≥0, ∀i ∈I.
Finally, note that:
when i ∈I(x∗), ci(x∗) = 0 and λ∗
i ≥0, therefore λ∗
i ci(x∗) = 0;
when i ∈I \ I(x∗), ci(x∗) > 0 but λ∗
i = 0, therefore we also have
λ∗
i ci(x∗) = 0.

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
393
Thus we obtain that λ∗
i ci(x∗) = 0, ∀i ∈I.
2
Theorem 8.2.7 was presented by Kuhn and Tucker [193], and is known as
the Kuhn-Tucker Theorem. Since Karush [185] also considered similarly the
optimality condition for constrained optimization, the conditions (8.2.20)–
(8.2.24) are often known as the Karush-Kuhn-Tucker conditions, or KKT
conditions for short. A point that satisﬁes the conditions is referred to as a
KKT point.
In KKT conditions, (8.2.20) is called a stationary point condition, because
it can be rewritten as
∇xL(x∗, λ∗) = ∇f(x∗) −
m

i=1
λ∗
i ∇ci(x∗) = 0.
(8.2.29)
Conditions (8.2.21) and (8.2.22) are called the feasibility conditions, (8.2.23)
is the nonnegativity condition for multipliers, and (8.2.24) is referred to as the
complementarity condition which states that both λ∗
i and ci(x∗) cannot be
nonzero, or equivalently that Lagrange multipliers corresponding to inactive
constraints are zero.
We say that the strict complementarity condition holds if exactly one
of λ∗
i and ci(x∗) is zero for each i ∈I, i.e., we have that λ∗
i > 0 for each
i ∈I ∩A(x∗).
An inequality constraint ci is strongly active if i ∈I ∩A(x∗) and λ∗
i > 0,
i.e., λ∗
i > 0 and ci(x∗) = 0. An inequality constraint ci is weakly active if
i ∈I ∩A(x∗) and λ∗
i = 0, i.e., λ∗
i = ci(x∗) = 0.
The condition (8.2.19) is called the constraint qualiﬁcation (CQ). The
constraint qualiﬁcation is important for KKT conditions.
As an example
given by Fletcher [133], it indicates that if constraint qualiﬁcation (8.2.19)
does not hold, then the local minimizer of problem (8.1.1)–(8.1.3) may not
be a KKT point.
Example:
min
(x1,x2)∈R2
x1
(8.2.30)
s.t.
x3
1 −x2 ≥0,
(8.2.31)
x2 ≥0.
(8.2.32)
It is not diﬃcult to see that x∗= (0, 0)T is the global minimizer of
(8.2.30)–(8.2.32). At x∗, we have
SFD(x∗, X) =

d
 d =

α
0

, α ≥0
3
(8.2.33)

394
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
and
LFD(x∗, X) =

d
 d =

α
0

, α ∈R1
3
.
(8.2.34)
Therefore, (8.2.19) does not hold. By direct computing, we have
∇f(x∗) =

1
0

,
∇c1(x∗) =

0
−1

,
∇c2(x∗) =

0
1

,
(8.2.35)
which show that there does not exist λ∗
1 and λ∗
2 such that
∇f(x∗) = λ∗
1∇c1(x∗) + λ∗
2∇c2(x∗).
(8.2.36)
This simple example indicates the importance of constraint qualiﬁcation.
However, it is not easy to examine whether or not the CQ condition (8.2.19)
holds. In the following, we give some concrete constraint qualiﬁcations which
are easy to examine and frequently used.
The most simple and obvious constraint qualiﬁcation is linear function
constraint qualiﬁcation.
Deﬁnition 8.2.8 If all constraints ci(x∗) (i ∈A(x∗) = E ∪I(x∗)) are linear
functions, we say that linear function constraint qualiﬁcation (LFCQ) holds.
By the deﬁnition, if ci(x∗) (i ∈A(x∗)) are linear functions, then CQ
condition (8.2.19) holds and we have the following corollary.
Corollary 8.2.9 Let x∗be a local minimizer of problem (8.1.1)-(8.1.3). If
the linear function constraint qualiﬁcation holds at x∗, then x∗is a KKT
point.
The most important and frequently used constraint qualiﬁcation is the
following linear independence constraint qualiﬁcation (LICQ).
Deﬁnition 8.2.10 If active constraint gradients ∇ci(x∗), i ∈A(x∗) are lin-
early independent, we say that the linear independence constraint qualiﬁcation
(LICQ) holds.
Theorem 8.2.11 Let x∗be a feasible point and A(x∗) an index set of active
constraints at x∗. If ∇ci(x∗), i ∈A(x∗), are linearly independent, then the
constraint qualiﬁcation (8.2.19) holds.

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
395
Proof.
Since SFD(x∗, X) ⊆LFD(x∗, X), it is enough that we only need
to prove LFD(x∗, X) ⊆SFD(x∗, X). Let d ∈LFD(x∗, X) be arbitrary.
Now let
A(x∗) = E ∪I(x∗) = {1, · · · , l}, me ≤l ≤n.
Since ∇c1(x∗), · · · , ∇cl(x∗) are linearly independent, there are bl+1, · · · , bn
such that ∇c1(x∗), · · · , ∇cl(x∗), bl+1, · · · , bn are linearly independent.
Consider the nonlinear system
r(x, θ) = 0,
(8.2.37)
whose components are deﬁned as
ri(x, θ) = ci(x) −θdT ∇ci(x∗),
i = 1, · · · , l,
(8.2.38)
ri(x, θ) = (x −x∗)T bi −θdT bi,
i = l + 1, · · · , n.
(8.2.39)
When θ = 0, the system (8.2.37) is solved by x∗, and when θ ≥0 is suﬃciently
small, any solution x is also a feasible point in (8.1.1)–(8.1.3).
Let us write
A = [∇c1(x), · · · , ∇cl(x)], B = [bl+1, · · · , bn].
Then the Jacobian matrix J(x, θ) = ∇xrT (x, θ) = [A : B].
Obviously,
J(x∗) = [A(x∗) : B] is nonsingular. Hence by the implicit function theo-
rem there exist open neighborhoods Ωx about x∗and Ωθ about θ = 0 such
that for any θ ∈Ωθ, a unique solution x(θ) ∈Ωx exists, and x(θ) is feasible
and continuously diﬀerentiable with respect to θ. From (8.2.37) and using
the chain rule,
0 = dri
dθ =

j
∂ri
∂xj
dxj
dθ + ∂ri
∂θ , i = 1, · · · , n,
that is
∇ci(x)T dx
dθ −∇ci(x∗)T d = 0, i = 1, · · · , l,
(8.2.40)
bT
i
dx
dθ −bT
i d = 0, i = l + 1, · · · , n.
(8.2.41)
The above system is
JT dx
dθ −J(x∗)T d = 0.

396
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Since x = x∗at θ = 0, we have J = J(x∗) at θ = 0. Thus the above equation
becomes
J(x∗)[dx
dθ |θ=0 −d] = 0.
Since the coeﬃcient matrix is nonsingular, we obtain
dx
dθ = d at θ = 0,
which implies that if θk ↓0 is any sequence, then x(θk) is a feasible sequence
with the feasible direction d, i.e.,
x(θk) −x∗
θk
→d.
This shows that d ∈SFD(x∗, X). Since d ∈LFD(x∗, X) is arbitrary, we get
LFD(x∗, X) ⊆SFD(x∗, X).
2
By the above theorem and Theorem 8.2.7, we immediately obtain the
following theorem.
Theorem 8.2.12 Let x∗be a local minimizer of problem (8.1.1)–(8.1.3). If
LICQ holds, i.e., ∇ci(x∗), i ∈A(x∗) = E ∪I(x∗), are linearly independent,
then there are Lagrange multipliers λ∗
i (i = 1, · · · , m) such that (8.2.20)–
(8.2.24) hold.
We want to mention that sometimes we use the regularity assumption
SFD(x∗, X) ∩D(x∗) = LFD(x∗, X) ∩D(x∗).
(8.2.42)
Since both sides are subsets of SFD(x∗, X) and LFD(x∗, X) respectively,
this assumption is clearly implied by the CQ (8.2.19). However, the converse
does not hold.
With the regularity assumption (8.2.42), the necessary condition (8.2.14)
in Theorem 8.2.5 (no feasible descent directions: SFD(x∗, X) ∩D(x∗) = φ)
becomes
LFD(x∗, X) ∩D(x∗) = φ,
i.e., there are no linearized feasible descent directions. Furthermore, as a
corollary of KKT Theorem 8.2.7, we have
Theorem 8.2.13 Let x∗be a local minimizer of problem (8.1.1)–(8.1.3). If
the regularity assumption (8.2.42) holds, then x∗is a KKT point.

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
397
Next, we discuss the ﬁrst-order suﬃciency condition.
Theorem 8.2.14 Let x∗∈X. Let f(x) and ci(x) (i = 1, · · · , m) be diﬀeren-
tiable at x∗. If
dT ∇f(x∗) > 0, ∀0 ̸= d ∈SFD(x∗, X),
(8.2.43)
then x∗is a strict local minimizer of problem (8.1.1)–(8.1.3).
Proof.
Suppose, by contradiction, that x∗is not a strict local minimizer,
then there exist xk ∈X(k = 1, 2, · · ·) such that
f(xk) ≤f(x∗),
(8.2.44)
and xk →x∗, xk ̸= x∗(k = 1, 2, · · ·). Without loss of generality, we assume
that
xk −x∗
∥xk −x∗∥2
→d.
(8.2.45)
Set dk = (xk −x∗)/∥xk −x∗∥2, δk = ∥xk −x∗∥2. By Deﬁnition 8.2.3, we have
d ∈SFD(x∗, X).
(8.2.46)
By use of (8.2.44), (8.2.45) and f(xk) = f(x∗) + (xk −x∗)T ∇f(x∗) + o(∥xk −
x∗∥2), by dividing ∥xk −x∗∥2 and then taking the limit as k →∞, we obtain
dT ∇f(x∗) ≤0,
(8.2.47)
which, together with (8.2.46), contradicts (8.2.43). This completes the proof.
2
Since SFD(x∗, X) ⊆LFD(x∗, X), we also have the following corollary.
Corollary 8.2.15 Let x∗∈X. Let f(x) and ci(x) (i = 1, · · · , m) be diﬀer-
entiable at x∗. If
dT ∇f(x∗) > 0, ∀0 ̸= d ∈LFD(x∗, X),
(8.2.48)
then x∗is a strict local minimizer of problem (8.1.1)–(8.1.3).
The other important optimality condition, which is credited to Fritz John
[183], is the Fritz John optimality condition, which needs no the constraint
qualiﬁcation.

398
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Theorem 8.2.16 Let f(x) and ci(x) (i = 1, · · · , m) be continuously diﬀer-
entiable on a nonempty open set containing the feasible set X. If x∗is a local
minimizer of problem (8.1.1)–(8.1.3), then there exist a scalar λ∗
0 ≥0 and a
vector λ∗∈Rm such that
λ∗
0∇f(x∗) −
m

i=1
λi∇ci(x∗) = 0,
(8.2.49)
ci(x∗) = 0, i ∈E,
(8.2.50)
ci(x∗) ≥0, i ∈I,
(8.2.51)
λ∗
i ≥0, i ∈I,
(8.2.52)
λ∗
i ci(x∗) = 0, ∀i,
(8.2.53)
m

i=0
(λ∗
i )2 > 0.
(8.2.54)
Proof.
If ∇ci(x∗) (i ∈A(x∗)) are linearly dependent, then there are
λ∗
i (i ∈A(x∗)) not all zero, such that

i∈A(x∗)
λ∗
i ∇ci(x∗) = 0.
(8.2.55)
Set λ∗
0 = 0 and λ∗
i = 0, (i ∈I\I(x∗)), we obtain (8.2.49)–(8.2.54).
If ∇ci(x∗) (i ∈A(x∗)) are linearly independent, we can obtain immedi-
ately (8.2.49)–(8.2.54) with λ0 = 1 by means of Theorem 8.2.12.
2
The point satisfying (8.2.49)–(8.2.54) is said to be the Fritz John point.
The following weighted Lagrangian function
˜L(x, λ0, λ) = λ0f(x) −
m

i=1
λici(x)
(8.2.56)
is said to be the Fritz John function. Obviously, the Fritz John point is the
stationary point of the Fritz John function. Note that λ0 ≥0. If λ0 > 0,
the Fritz John function can be regarded as a λ0 multiple of the Lagrangian
function.
However, if λ0 = 0, the Fritz John function only describes the
constraint functions and is independent of the objective function. In such
a case, Fritz John optimality conditions do not represent actually the op-
timality conditions of the original constrained optimization problem. This
disadvantage makes the Fritz John conditions unfavorable.

8.2. FIRST-ORDER OPTIMALITY CONDITIONS
399
We conclude this section with an optimality condition of convex program-
ming.
As we know, the problem of minimizing a convex function on a convex
set Ωis said to be a convex programming problem. Such a problem has the
form
min
f(x)
s.t.
x ∈Ω,
(8.2.57)
where f(x) is a convex function on a convex set Ω. Typically, in nonlinear
programming
min
f(x)
s.t.
ci(x) = 0, i ∈E,
(8.2.58)
ci(x) ≥0, i ∈I,
if f(x) is convex, ci(x), (i ∈E) are linear functions, and ci(x), (i ∈I) are
concave, then the constrained set Ω= {x | ci(x) = 0, i ∈E; ci(x) ≥0, i ∈I}
is a convex set, and hence the problem (8.2.58) is convex programming.
As Theorem 1.4.9 in the unconstrained case, the following theorem in-
dicates that the local minimizer of convex programming is also its global
minimizer.
Theorem 8.2.17 Each local minimizer of convex programming problem (8.2.57)
is also the global minimizer, and the set S of global minimizers is convex.
Proof.
Suppose, by contradiction, that x∗is a local but not global mini-
mizer. Then there exists x1 ∈Ωsuch that f(x1) < f(x∗). Consider
xθ = (1 −θ)x∗+ θx1, θ ∈[0, 1].
By convexity of Ω, xθ ∈Ω. Also, by convexity of f,
f(xθ)
≤
(1 −θ)f(x∗) + θf(x1)
=
f(x∗) + θ(f(x1) −f(x∗))
<
f(x∗).
For suﬃciently small θ, xθ) ∈N(x∗, ϵ) ∩Ω. So, it follows from assumption
that x∗is a local minimizer that f(xθ) ≥f(x∗). We get a contradiction
which means that local minimizers are global.

400
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Let x0, x1 ∈S. Deﬁne xθ = (1 −θ)x0 + θx1, θ ∈[0, 1]. By the global
property, f(xθ) ≥f(x0) = f(x1).
However, by convexity of f, f(xθ) ≤
(1 −θ)f(x0) + θf(x1) = f(x0) = f(x1). Therefore f(xθ) = f(x0) = f(x1)
and so xθ ∈S, which means that S is convex.
2
Theorem 8.2.18 The KKT point of convex programming must be its mini-
mizer.
Proof.
Let (x∗, λ∗) be any KKT pair of convex programming. Obviously,
the Lagrangian function
L(x, λ∗) = f(x) −

i∈E
λ∗
i ci(x) −

i∈I
λ∗
i ci(x)
(8.2.59)
is convex for x. By use of properties of convex function and KKT conditions,
we have for any feasible x,
L(x, λ∗)
≥
L(x∗, λ∗) + (x −x∗)T ∇xL(x∗, λ∗)
=
L(x∗, λ∗)
=
f(x∗) −
m

i=1
λ∗
i ci(x∗)
=
f(x∗).
(8.2.60)
Note that x is a feasible point and λ∗
i ≥0, i ∈I, so we have
λ∗
i ci(x) = 0, i ∈E; λ∗
i ci(x) ≥0, i ∈I.
Hence
L(x, λ∗) ≤f(x).
(8.2.61)
By (8.2.60) and (8.2.61) we obtain
f(x) ≥f(x∗),
(8.2.62)
that is, KKT point x∗is a minimizer.
2
Theorem 8.2.19 The convex programming with strictly convex objective func-
tion has unique minimizer.
The proof of this theorem is as an exercise.

8.3. SECOND-ORDER OPTIMALITY CONDITIONS
401
8.3
Second-Order Optimality Conditions
We have seen in unconstrained optimization that the second-order derivative
information has signiﬁcant implications in optimality conditions. Let x∗∈X.
If
dT ∇f(x∗) > 0, ∀0 ̸= d ∈SFD(x∗, X),
(8.3.1)
then x∗is a strict local minimizer of problem (8.1.1)–(8.1.3). If
there exists d ∈SFD(x∗, X) such that dT ∇f(x∗) < 0,
(8.3.2)
then from Theorem 8.2.5 it follows that x∗must not be a local minimizer of
problem (8.1.1)–(8.1.3). These results tell us that, provided either (8.3.1) or
(8.3.2) holds, the ﬁrst-order optimality condition can be used to determine
whether x∗is a local minimizer. However, we cannot determine whether x∗
is a local minimizer by the ﬁrst derivative information alone, if both (8.3.1)
and (8.3.2) do not hold, i.e.,
dT ∇f(x∗) ≥0, ∀d ∈SFD(x∗, X);
(8.3.3)
dT ∇f(x∗) = 0, ∃0 ̸= d ∈SFD(x∗, X).
(8.3.4)
In these cases, the second-order derivative information is needed.
Assume that the constraint qualiﬁcation (8.2.19) holds. It follows from
(8.3.3), (8.2.19) and Farkas’ Lemma 8.2.6 that x∗is a KKT point. By (8.3.4)
and the deﬁnition of Lagrange multipliers, there exists 0 ̸= d ∈SFD(x∗, X)
such that
dT ∇f(x∗) =
m

i=1
λ∗
i dT ∇ci(x∗) = 0.
(8.3.5)
Since SFD(x∗, X) ⊆LFD(x∗, X), by use of Deﬁnition 8.2.2, we have that
(8.3.5) is equivalent to
λ∗
i dT ∇ci(x∗) = 0, ∀i ∈I(x∗).
(8.3.6)
So, we give the following deﬁnitions. Let x∗be a KKT point of (8.1.1)–
(8.1.3), and λ∗a corresponding Lagrange multiplier vector. Deﬁne a set of
strong active constraints as
I+(x∗) = {i | i ∈I(x∗) with λ∗
i > 0}.
(8.3.7)
Obviously, I+(x∗) ⊆I(x∗).

402
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Deﬁnition 8.3.1 Let x∗be a KKT point of (8.1.1)–(8.1.3), and λ∗a corre-
sponding Lagrange multiplier vector. If there exist sequences dk (k = 1, 2, · · ·)
and δk > 0 (k = 1, 2, · · ·) such that
x∗+ δkdk ∈X
(8.3.8)
satisfy
ci(xk) = 0,
i ∈E ∪I+(x∗),
(8.3.9)
ci(xk) ≥0,
i ∈I(x∗) \ I+(x∗),
(8.3.10)
and dk →d and δk →0, then d is said to be a sequential null constraint
direction at x∗. The set of all sequential null constraint directions is written
as S(x∗, λ∗),
S(x∗, λ∗) =
⎧
⎪
⎨
⎪
⎩
d

xk = x∗+ δkdk ∈X, δk > 0, δk →0, dk →d,
ci(xk) = 0, i ∈E ∪I+(x∗),
ci(xk) ≥0, i ∈I(x∗) −I+(x∗).
⎫
⎪
⎬
⎪
⎭
.
(8.3.11)
It is easy to see that (8.3.9)–(8.3.10) imply that
m

i=1
λ∗
i ci(x∗+ δkdk) = 0.
(8.3.12)
So, equivalently,
S(x∗, λ∗) =

d

d ∈SFD(x∗, X);

m
i=1 λ∗
i ci(xk) = 0.
3
.
(8.3.13)
Obviously, S(x∗, λ∗) ⊆SFD(x∗, X).
Similar to the linearized feasible direction, we have the following deﬁni-
tion.
Deﬁnition 8.3.2 Let x∗be a KKT point of (8.1.1)–(8.1.3), and λ∗a corre-
sponding Lagrange multiplier vector. If d is a linearized feasible direction at
x∗and (8.3.6) holds, then d is said to be a linearized null constraint direction.
The set of all linearized null constraint directions is written as G(x∗, λ∗),
G(x∗, λ∗) =
⎧
⎪
⎨
⎪
⎩
d

d ̸= 0,
dT ∇ci(x∗) = 0,
i ∈E ∪I+(x∗),
dT ∇ci(x∗) ≥0,
i ∈I(x∗) \ I+(x∗).
⎫
⎪
⎬
⎪
⎭
.
(8.3.14)

8.3. SECOND-ORDER OPTIMALITY CONDITIONS
403
Equivalently,
G(x∗, λ∗) =

d

d ∈LFD(x∗, λ∗);
dT ∇ci(x∗) = 0, i ∈I+(x∗).
3
(8.3.15)
If the Lagrange multiplier at x∗is unique, G(x∗, λ∗) can be denoted by
G(x∗).
By the above deﬁnitions, we have
S(x∗, λ∗)
⊆
SFD(x∗, X),
(8.3.16)
G(x∗, λ∗)
⊆
LFD(x∗, X).
(8.3.17)
Similar to SFD(x∗, X) ⊆LFD(x∗, X), we also can prove
S(x∗, λ∗) ⊆G(x∗, λ∗),
(8.3.18)
which is an exercise left to readers.
Now, we state the main results of this section.
Theorem 8.3.3 (Second-order necessary conditions)
Let x∗be a local minimizer of (8.1.1)–(8.1.3). If the constraint qualiﬁca-
tion (8.2.19) holds, then we have
dT ∇2
xxL(x∗, λ∗)d ≥0, ∀d ∈S(x∗, λ∗),
(8.3.19)
where L(x, λ) is a Lagrangian function.
Furthermore, if
S(x∗, λ∗) = G(x∗, λ∗),
(8.3.20)
then
dT ∇2
xxL(x∗, λ∗)d ≥0, ∀d ∈G(x∗, λ∗).
(8.3.21)
Proof.
For any d ∈S(x∗, λ∗), if d = 0, it is obvious that dT ∇2
xxL(x∗, λ∗)d =
0. Now we consider d ̸= 0. From the deﬁnition of S(x∗, λ∗), there exist {dk}
and {δk} such that (8.3.8)–(8.3.12) hold. Therefore, by (8.3.12) and KKT
conditions, we have
f(x∗+ δkdk)
=
L(x∗+ δkdk, λ∗)
=
L(x∗, λ∗) + 1
2δ2
kdT
k ∇2
xxL(x∗, λ∗)dk + o(δ2
k)
=
f(x∗) + 1
2δ2
kdT
k ∇2
xxL(x∗, λ∗)dk + o(δ2
k).
(8.3.22)

404
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Since x∗is a local minimizer, it follows for all k suﬃciently large that
f(x∗+ δkdk) ≥f(x∗).
(8.3.23)
Using (8.3.22)–(8.3.23) and taking limits give
dT ∇2
xxL(x∗, λ∗)d ≥0.
Since d ∈S(x∗, λ∗) is arbitrary, then (8.3.19) follows.
By (8.3.20), we immediately obtain (8.3.21) from (8.3.19).
2
Theorem 8.3.4 (Second-order suﬃcient conditions)
Let x∗be a KKT point of (8.1.1)–(8.1.3). If
dT ∇2
xxL(x∗, λ∗)d > 0, ∀d ∈G(x∗, λ∗),
(8.3.24)
then x∗is a strict local minimizer.
Proof.
Assume that x∗is not a strict local minimizer, then there exists a
sequence {xk} ⊂X such that
f(xk) ≤f(x∗),
(8.3.25)
with xk →x∗and xk ̸= x∗(k = 1, 2, · · ·). Without loss of generality, we
assume that
xk −x∗
∥xk −x∗∥2
→d.
By a similar argument to (8.2.45)–(8.2.47), we have
dT ∇f(x∗) ≤0
(8.3.26)
and
d ∈SFD(x∗, X) ⊆LFD(x∗, X).
(8.3.27)
It follows by KKT conditions and (8.2.4) that
dT ∇f(x∗) =
m

i=1
λidT ∇ci(x∗) ≥0.
(8.3.28)
Note that (8.3.26) and (8.3.28) give
dT ∇f(x∗) = 0
(8.3.29)

8.3. SECOND-ORDER OPTIMALITY CONDITIONS
405
which implies from (8.3.28) and Deﬁnition 8.2.2 that
λidT ∇ci(x∗) = 0, ∀i ∈I(x∗).
(8.3.30)
So, it follows from (8.3.27), (8.3.30) and Deﬁnition 8.3.2 that
d ∈G(x∗, λ∗).
(8.3.31)
From (8.3.25), we have
L(x∗, λ∗)
≥
L(xk, λ∗)
=
L(x∗, λ∗) + 1
2δ2
kdT
k ∇2
xxL(x∗, λ∗)dk + o(δ2
k).
(8.3.32)
Dividing by δ2
k and taking the limit give
dT ∇2
xxL(x∗, λ∗)d ≤0
(8.3.33)
which contradicts (8.3.24). We complete the proof.
2
Notice that a suﬃcient condition for (8.3.24) is that
dT ∇2
xxL(x∗, λ∗)d > 0
for all d ̸= 0 such that dT ∇ci(x∗) = 0, i ∈A+(x∗, λ∗), where
A+(x∗, λ∗) = E ∪{i | i ∈I(x∗), λ∗
i > 0},
(8.3.34)
which is obtained by deleting indices for which λ∗
i = 0, i ∈I(x∗) from A(x∗).
The A+(x∗, λ∗) is said to be an index set of strong active constraints, which is
a union of the index sets of equality constraints and strongly active inequality
constraints. So, we immediately obtain the following corollary which is also
a second-order suﬃcient condition and more convenient to verify in practice.
Corollary 8.3.5 Let x∗be a KKT point of (8.1.1)–(8.1.3). If
dT ∇2
xxL(x∗, d∗)d > 0
(8.3.35)
for all d satisfying
dT ∇ci(x∗) = 0, ∀i ∈A+(x∗, λ∗),
(8.3.36)
then x∗is a strict local minimizer.

406
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Proof.
It is enough to prove that (8.3.35)–(8.3.36) are the suﬃcient con-
ditions of (8.3.24). In fact, if x∗is a KKT point, then ∀d ∈SFD(x∗, X) ⊆
LFD(x∗, X),
dT ∇ci(x∗) = 0,
i ∈E
(8.3.37)
dT ∇ci(x∗) ≥0,
i ∈I(x∗).
(8.3.38)
So, (8.3.6) holds, which implies d ∈G(x∗, λ∗) from Deﬁnition 8.3.2. There-
fore, by means of Theorem 8.3.4, it follows that (8.3.35)–(8.3.36) implies
(8.3.24).
2
8.4
Duality
We conclude this chapter with a brief discussion of duality.
The concept
of duality occurs widely in the mathematical programming literature. The
aim is to provide an alternative formulation of a mathematical programming
problem which is more convenient computationally or has some theoretical
signiﬁcance.
The original problem is referred to as the primal, and the transformed
problem is referred to as the dual.
In this section, we give an introduction of duality theory which is asso-
ciated with the convex programming problem. We will introduce the La-
grangian dual problem, and prove the duality theorem and the weak duality
theorem. Now we ﬁrst state the duality theorem.
Theorem 8.4.1 Let x∗be a minimizer of convex primal problem (P)
min
x
f(x)
s.t.
ci(x) ≥0, i = 1, · · · , m.
(8.4.1)
If f(x) and ci(x), (i = 1, · · · , m) are continuously diﬀerentiable and the reg-
ularity condition (8.2.42) holds, then x∗and λ∗solve the dual problem
max
x,λ
L(x, λ)
s.t.
∇xL(x, λ) = 0,
(8.4.2)
λ ≥0.
Furthermore, the minimum of the primal and the maximum of the dual
are equal, i.e.,
f(x∗) = L(x∗, λ∗).
(8.4.3)

8.4. DUALITY
407
Proof.
By the assumption and KKT Theorem 8.2.7, there exist Lagrange
multipliers λ∗≥0 such that ∇xL(x∗, λ∗) = 0 and λ∗
i ci(x∗) = 0, i = 1, · · · , m.
Thus, f(x∗) = L(x∗, λ∗).
Let x, λ be dual feasible. Using λ ≥0, convexity of L, and ∇xL(x, λ) = 0
gives
L(x∗, λ∗)
=
f(x∗) ≥f(x∗) −
m

i=1
λici(x∗)
=
L(x∗, λ)
≥
L(x, λ) + (x∗−x)T ∇xL(x, λ)
=
L(x, λ)
(8.4.4)
which means that (x∗, λ∗) solves the dual problem.
2
Usually, (8.4.3) is said to be the strong duality.
Now, we give some
examples of dual problems. Let the primal problem in linear programming
be
min
cT x
s.t.
AT x ≥b.
(8.4.5)
By Theorem 8.4.1, we immediately have the dual:
max
bT λ
s.t.
Aλ = c,
(8.4.6)
λ ≥0.
Normally, linear programs have standard form:
min
cT x
s.t.
Ax = b,
(8.4.7)
x ≥0.
The corresponding dual problem is
max
bT λ
s.t.
AT λ ≤c.
(8.4.8)
It is easy to examine that the optimality conditions of (8.4.7) and (8.4.8) are
identical.

408
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
For convex quadratic programming, the primal problem is
min
x
1
2xT Gx + hT x
s.t.
AT x ≥b,
(8.4.9)
where G is positive deﬁnite. The dual problem is
max
x,λ
1
2xT Gx + hT x −λT (AT x −b)
s.t.
Gx + h −Aλ = 0,
(8.4.10)
λ ≥0.
(8.4.11)
By eliminating x, we obtain the following problem:
max
λ
−1
2λT (AT G−1A)λ + λT (b −AT G−1h) −1
2hT G−1h
s.t.
λ ≥0.
(8.4.12)
This is a quadratic programming problem in λ with bounded-constraints
λ ≥0.
The following theorem, referred to as the weak duality theorem, shows
that the objective value of any feasible point of the primal problem is larger
than or equal to the objective value of any feasible point of the dual problem.
Theorem 8.4.2 Let x′ be any feasible point in primal problem (8.4.1). Let
(x, λ) be any feasible point in dual problem (8.4.2). Then
f(x′) ≥L(x, λ).
(8.4.13)
Proof.
Let x′ be primal feasible and (x, λ) dual feasible. Then by convex-
ity of f, dual feasibility, concavity of ci, and nonnegativity of ci(x′) and λi
in turn, it follows that
f(x′) −f(x)
≥
∇f(x)T (x′ −x)
=
m

i=1
λi∇ci(x)T (x′ −x)
≥
m

i=1
λi(ci(x′) −ci(x))
≥
−
m

i=1
λici(x).

8.4. DUALITY
409
Hence
f(x′) ≥f(x) −
m

i=1
λici(x) = L(x, λ).
2
From the above theorem, we immediately have
inf
x f(x) ≥sup
x,λ
L(x, λ).
(8.4.14)
This implies that if the primal problem is unbounded, it follows that infx f(x) =
supx,λ L(x, λ) = −∞, and this is not possible if (x, λ) is feasible. Therefore,
an unbounded primal implies an inconsistent dual.
Exercises
1. Assume that f(x) is a convex function, ci(x) (1 ≤i ≤me) are linear
functions and ci(x) (me + 1 ≤i ≤m) are concave functions. Show that x∗is
a global minimizer of (8.1.1)–(8.1.3) if it is a local minimizer of (8.1.1)–(8.1.3).
2. Deﬁne the ϵ-active set by
Iϵ(x) = {i | ci(x) ≤ϵ,
i ∈I(x)}.
Prove that, for any given x ∈ℜn,
lim
ϵ→0+ Iϵ(x) = I(x).
3. Prove: if ci(x) (i ∈A(x∗)) are linear functions, then CQ condition
(8.2.19) holds.
4. Prove (8.3.16) and (8.3.17).
5. Prove (8.3.18).
6. Let 0 ̸= c ∈ℜn. Consider the problem
min
cT x
s.t.
∥x∥2
2 ≤1.

410
CHAPTER 8. THEORY OF CONSTRAINED OPTIMIZATION
Prove that x∗= c/∥c∥2 satisﬁes the second-order suﬃcient condition.
7. Form the KKT conditions for
max
(x + 1)2 + (y + 1)2
s.t.
x2 + y2 ≤2,
1 −y ≥0
and then determine the solution.
8. Give an example in which the second-order necessary condition holds
while the second-order suﬃcient condition fails.
9. By solving the KKT equation, ﬁnd the point on the ellipse deﬁned
by the intersection of the surface x + y = 1 and x2 + 2y2 + z2 = 1 which is
nearest to the origin.
10. Show that the dual of problem
min
1
2σx2
1 + 1
2x2
2 + x1
s.t.
x1 ≥0
is a maximization problem in terms of a Largrange multiplier λ. For the case
σ = +1 and σ = −1, investigate whether the local solution of the dual gives
the multiplier λ∗which exists at the local solution to the primal.

Chapter 9
Quadratic Programming
9.1
Optimality Conditions for Quadratic Program-
ming
Quadratic programming is the simplest constrained nonlinear optimization
problem. It is a special class of optimization problem (8.1.1)–(8.1.3) with a
quadratic objective function f(x) and linear constraints ci(x) (i = 1, · · · , m).
The general quadratic programming (QP) has the following form:
min
Q(x) = 1
2xT Gx + gT x
(9.1.1)
s.t.
aT
i x = bi, i ∈E,
(9.1.2)
aT
i x ≥bi, i ∈I,
(9.1.3)
where G is a symmetric n × n matrix, E and I are ﬁnite sets of indices, E =
{1, · · · , me} and I = {me+1, · · · , m}. If the Hessian matrix G is positive semi-
deﬁnite, then (9.1.1)–(9.1.3) is a convex quadratic programming problem
and the local solution x∗is a global solution. If G is positive deﬁnite, then
(9.1.1)–(9.1.3) is a strict convex QP and x∗is a unique global solution. If G
is indeﬁnite, then (9.1.1)–(9.1.3) is a nonconvex QP which is more important
and worth emphasizing.
From Theorem 8.2.7, Theorem 8.3.3 and Theorem 8.3.4, we immediately
get the following theorems:
Theorem 9.1.1 (Necessary conditions)

412
CHAPTER 9. QUADRATIC PROGRAMMING
Let x∗be a local minimizer of quadratic programming problem (9.1.1)–
(9.1.3). Then there exist multipliers λ∗
i (i = 1, · · · , m) such that
g + Gx∗=
m

i=1
λ∗
i ai,
(9.1.4)
aT
i x∗= bi, i ∈E,
(9.1.5)
aT
i x∗≥bi, i ∈I,
(9.1.6)
λ∗
i (aT
i x∗−bi) = 0, i ∈I,
(9.1.7)
λ∗
i ≥0, i ∈I.
(9.1.8)
Furthermore,
dT Gd ≥0, ∀d ∈G(x∗, λ∗),
(9.1.9)
where
G(x∗, λ∗) =
⎧
⎪
⎨
⎪
⎩
d ̸= 0

dT ai = 0, i ∈E
dT ai ≥0, i ∈I(x∗)
dT ai = 0, i ∈I(x∗) and λ∗
i > 0
⎫
⎪
⎬
⎪
⎭
.
(9.1.10)
Theorem 9.1.2 (Suﬃcient conditions)
Let x∗be a KKT point and λ∗a corresponding Lagrange multiplier vector.
If dT Gd > 0 ∀0 ̸= d ∈G(x∗, λ∗), then x∗is a strict local minimizer to
(9.1.1)–(9.1.3).
Next, we give a suﬃcient and necessary optimality condition for (9.1.1)–
(9.1.3).
Theorem 9.1.3 (Necessary and suﬃcient conditions)
Let x∗be a feasible point of quadratic programming problem (9.1.1)–
(9.1.3), then x∗is a local minimizer if and only if (x∗, λ∗) is a KKT pair
such that (9.1.4)–(9.1.8) hold, and
dT Gd ≥0, ∀d ∈G(x∗, λ∗).
(9.1.11)
Proof.
Let x∗be a local minimizer, it follows from Theorem 9.1.1 that
there exists multiplier vector λ∗such that (9.1.4)–(9.1.8) hold. Let 0 ̸= d ∈
G(x∗, λ∗). Obviously, for suﬃciently small t > 0, we have
x∗+ td ∈X.
(9.1.12)

9.2. DUALITY FOR QUADRATIC PROGRAMMING
413
Then, by the deﬁnition of d, for suﬃciently small t > 0, we have
Q(x∗)
≤
Q(x∗+ td) = Q(x∗) + tdT (Gx∗+ g) + 1
2t2dT Gd
=
Q(x∗) + t
m

i=1
λ∗
i aT
i d + 1
2t2dT Gd
=
Q(x∗) + 1
2t2dT Gd,
(9.1.13)
which, together with the arbitrariness of d, means (9.1.11) holds.
Second, we prove the suﬃciency. Suppose, by contradiction, that x∗is
not a local minimizer, so that there exists xk = x∗+ δkdk ∈X such that
Q(xk) = Q(x∗+ δkdk) < Q(x∗),
(9.1.14)
where δk > 0, δk →0, dk →¯d. Completely similar to the proof of Theorem
8.3.4, we know that
¯d ∈G(x∗, λ∗).
(9.1.15)
Thus, it follows from (9.1.14) and KKT conditions that
L(x∗, λ∗)
>
L(xk, λ∗)
=
L(x∗, λ∗) + 1
2δ2
kdT
k Gdk + o(δ2
k).
(9.1.16)
Dividing both sides by δ2
k and taking the limit, we obtain
¯dT G ¯d < 0.
(9.1.17)
Noting that ¯d ∈G(x∗, λ∗), it follows that (9.1.17) contradicts the assumption
(9.1.11). Then we complete the proof.
2
Obviously, ﬁnding the KKT point of a quadratic programming problem
is equivalent to ﬁnding x∗∈Rn, λ∗∈Rm such that (9.1.4)–(9.1.8) hold.
9.2
Duality for Quadratic Programming
In this section we give more detailed discussion on the duality of convex
quadratic programming, because in some classes of practical problems we
can take advantage of the special structure of the dual to solve the problems
more eﬃciently.

414
CHAPTER 9. QUADRATIC PROGRAMMING
Assume that G is a positive deﬁnite matrix. From the results in §9.1, we
have known that solving the quadratic programming problem (9.1.1)–(9.1.3)
is equivalent to solving (9.1.4)–(9.1.8). Write
y = Aλ −g,
(9.2.1)
and
ti = aT
i x −bi, i ∈I,
(9.2.2)
where A = [a1, · · · , am] ∈Rn×m, λ = [λ1, · · · , λm]T ∈Rm. Note that (9.1.4)
is just y = Gx and that (9.1.5)–(9.1.6) become
AT x −b = (0, · · · , 0, tme+1, · · · , tm)T ,
then (9.1.4)–(9.1.8) can be written as

−b
G−1y

=

−AT
I

x + (0, · · · , 0, tme+1, · · · , tm, 0, · · · , 0)T ,(9.2.3)
Aλ −y = g,
(9.2.4)
λi ≥0, i ∈I,
(9.2.5)
tiλi = 0, i ∈I,
(9.2.6)
ti ≥0, i ∈I.
(9.2.7)
By KKT conditions, it follows that (9.2.3)–(9.2.7) are equivalent to solving
the problem
max
λ,y
bT λ −1
2yT G−1y
Def
=
¯Q(λ, y)
(9.2.8)
s.t.
Aλ −y = g,
(9.2.9)
λi ≥0, i ∈I,
(9.2.10)
which is the dual of the primal (9.1.1)–(9.1.3). As an exercise, please prove
that problem (9.2.8)–(9.2.10) just is
maxx,λ
L(x, λ) = 1
2xT Gx + gT x −λT (AT x −b)
(9.2.11)
s.t.
∇xL(x, λ) = 0
(9.2.12)
λi ≥0, i ∈I.
(9.2.13)

9.2. DUALITY FOR QUADRATIC PROGRAMMING
415
Eliminating y in (9.2.9) by use of (9.2.1), we get that (9.2.8)–(9.2.10) can
be reduced to
min
λ∈Rm
−(b + AT G−1g)T λ + 1
2λT (AT G−1A)λ
(9.2.14)
s.t.
λi ≥0, i ∈I.
(9.2.15)
Assume that x and (λ, y) are feasible points of the primal problem (9.1.1)–
(9.1.3) and the dual problem (9.2.8)–(9.2.10) respectively, then we have
Q(x) −¯Q(λ, y)
=
xT (Aλ −y) + 1
2xT Gx
−(λT Ax −

i∈I
λiti −1
2yT G−1y)
=

i∈I
λiti + 1
2(xT Gx + yT G−1y −2xT y), (9.2.16)
where ti is deﬁned in (9.2.2). Then, the positive deﬁniteness of G gives
Q(x) ≥¯Q(λ, y),
(9.2.17)
which is what we showed in Theorem 8.4.2.
It also follows from (9.2.16) that both sides of (9.2.17) are equal if and
only if

i∈I
λi(aT
i x −bi) = 0
(9.2.18)
and
x = G−1y.
(9.2.19)
It is not diﬃcult to see that (9.2.19) and (9.2.18) are equivalent to (9.1.4)
and (9.1.7) respectively. So, with the assumptions of feasibility, we have the
following theorem.
Theorem 9.2.1 Let G be positive deﬁnite. If the primal problem is feasible,
then x∗∈X is a solution of primal problem (9.1.1)–(9.1.3) if and only if
(λ∗, y∗) is the solution of dual problem (9.2.8)–(9.2.10).
In §8.4, we have shown that an unbounded primal implies an infeasible
dual. We would like to know whether or not an infeasible primal implies an
unbounded dual. This guess does not always hold. However, it is true for
linearly constrained problems.

416
CHAPTER 9. QUADRATIC PROGRAMMING
Theorem 9.2.2 Let G be positive deﬁnite. Then the primal problem (9.1.1)–
(9.1.3) is infeasible if and only if the dual (9.2.8)–(9.2.10) is unbounded.
Proof.
From (9.2.17), if the primal problem is feasible, the objective
function of the dual problem on set satisfying constraints (9.2.9)–(9.2.10) is
uniformly bounded above.
Now suppose that the primal problem is infeasible, then the system
(aT
i , bi)˜x = 0, i ∈E,
(9.2.20)
(aT
i , bi)˜x ≥0, i ∈I,
(9.2.21)
(0, · · · , 0, 1)˜x < 0
(9.2.22)
has no solution for ˜x ∈Rn+1. By Corollary 8.2.6 of Farkas’ Lemma, it follows
that there exist ¯λi (i = 1, · · · , m) such that
(0, · · · , 0, 1) =

i∈E
¯λi(aT
i , bi) +

i∈I
¯λi(aT
i , bi),
(9.2.23)
i.e.,
m

i=1
¯λiai = 0,
(9.2.24)
m

i=1
¯λibi = 1,
(9.2.25)
¯λi ≥0, i ∈I.
(9.2.26)
Set λi = t¯λi, then (9.2.24) gives Aλ = tA¯λ = 0. It follows from (9.2.9) that
y = −g. Therefore, when t →+∞, it follows from (9.2.8) and (9.2.25) that
¯Q(λ, y) = t →+∞.
Also, for all t > 0, we have that λ = (t¯λ1, · · · , t¯λm)T and y = −g satisfy
constraints (9.2.9)–(9.2.10) of the dual problem. This shows that the dual
problem is unbounded.
2
There is a closed connection between Lagrangian function
L(x, λ) = Q(x) −
m

i=1
λi(aT
i x −bi)
(9.2.27)

9.2. DUALITY FOR QUADRATIC PROGRAMMING
417
of the primal problem and duality. It is not diﬃcult to see that solving KKT
conditions is equivalent to ﬁnding a stationary point of L(x, λ) on the area
{(x, λ) | λi ≥0, i ∈I}. Since the Hessian matrix of L(x, λ) is
∇2L(x, λ) =

G
−A
−AT
0

,
(9.2.28)
by use of the identity

I
0
AT G−1
I

∇2L(x, λ)

I
G−1A
0
I

=

G
0
0
−AT G−1A

,
(9.2.29)
we know that ∇2L(x, λ) has just n positive eigenvalues, and that the number
of negative eigenvalues equals rank(A). Thus, in general, the stationary point
of L(x, λ) is a saddle point, i.e., there is λ∗∈Λ,
Λ = {λ ∈Rm | λi ≥0, i ∈I},
such that (x∗, λ∗) satisﬁes
L(x∗, λ) ≤L(x∗, λ∗) ≤L(x, λ∗)
(9.2.30)
for all x ∈X and λ ∈Λ.
In fact, for all x ∈X, we have
max
λ∈Λ L(x, λ) = Q(x).
(9.2.31)
For any λ ∈Λ, set
y = Aλ −g,
(9.2.32)
then (λ, y) is a feasible point of dual problem (9.2.8)–(9.2.10). This means
that such a feasible (λ, y) satisﬁes (9.2.9), which is ∇xL(x, λ) = 0, i.e., such
a feasible (λ, y) such that minx∈Rn L(x, λ). Therefore,
min
x∈Rn L(x, λ) = bT λ −1
2yT G−1y = ¯Q(λ, y).
(9.2.33)
Let (x∗, λ∗) be a solution of (9.1.4)–(9.1.8). Let y∗= Aλ∗−g. It follows
that (λ∗, y∗) is a feasible point of (9.2.8)–(9.2.10). Then, for any x ∈X and
any λ ∈Λ, we have
L(x, λ∗)
≥
¯Q(λ∗, y∗)
=
L(x∗, λ∗) = Q(x∗) ≥L(x∗, λ),
(9.2.34)

418
CHAPTER 9. QUADRATIC PROGRAMMING
which means that (x∗, λ∗) is a saddle point of L(x, λ).
Conversely, if
L(x∗, λ) ≤L(x∗, λ∗) ≤L(x, λ∗)
(9.2.35)
holds for all x ∈X and λ ∈Λ, then
Q(x∗) −
m

i=1
λi(aT
i x∗−bi)
≤
Q(x∗) −
m

i=1
λ∗
i (aT
i x∗−bi)
≤
Q(x) −
m

i=1
λ∗
i (aT
i x −bi).
(9.2.36)
Rearranging the ﬁrst inequality gives
m

i=1
(λi −λ∗
i )(aT
i x∗−bi) ≥0,
(9.2.37)
which is

i∈E
(λi −λ∗
i )(aT
i x∗−bi) +

i∈I
(λi −λ∗
i )(aT
i x∗−bi) ≥0.
(9.2.38)
Now we prove
aT
i x∗= bi,
i ∈E
(9.2.39)
aT
i x∗≥bi,
i ∈I
(9.2.40)
by contradiction. Suppose that aT
k x∗> bk for some k ∈E. Set λi = λ∗
i
for i ̸= k and λk = λ∗
k −1, then we get a contradiction from (9.2.38) to the
assumption aT
k x∗> bk. If we suppose aT
k x∗< bk for some k ∈E, we can get
a similar contradiction. Therefore, we have that aT
i x∗= bi, ∀i ∈E.
Now assume, for some k ∈I, that
λk = λ∗
k + 1 and λi = λ∗
i for i ̸= k.
(9.2.41)
Obviously, it follows that
aT
k x∗−bk ≥0, k ∈I.
(9.2.42)
Repeating the process for all k ∈I, we obtain
aT
i x∗−bi ≥0, ∀i ∈I.
(9.2.43)

9.3. EQUALITY-CONSTRAINED QUADRATIC PROGRAMMING
419
Then, x∗is a feasible point to the primal problem.
Set λ = 0, it follows from (9.2.35) that L(x∗, λ∗) ≥L(x∗, 0), which is
m

i=1
λ∗
i (aT
i x∗−bi) ≤0.
(9.2.44)
By use of (9.2.44), (9.2.35) and λ∗∈Λ, we have, for all x ∈X,
Q(x∗)
≤
Q(x∗) −
m

i=1
λ∗
i (aT
i x∗−bi)
=
L(x∗, λ∗)
≤
L(x, λ∗)
≤
L(x, λ∗) +
m

i=1
λ∗
i (aT
i x −bi)
=
Q(x),
(9.2.45)
which shows that x∗is a minimizer of the primal problem.
Therefore, we get the following theorem which is a famous saddle point
theorem on the relationship between the saddle point of a Lagrangian func-
tion and the minimizer of the primal problem.
Theorem 9.2.3 (Saddle point theorem for quadratic programming)
Let G be positive deﬁnite. Then x∗∈X is a minimizer of the primal
problem (9.1.1)–(9.1.3) if and only if there exists λ∗∈Λ such that (x∗, λ∗) is
a saddle point of Lagrangian function L(x, λ), i.e., the saddle point conditions
L(x∗, λ) ≤L(x∗, λ∗) ≤L(x, λ∗)
(9.2.46)
hold for all x ∈X and λ ∈Λ.
9.3
Equality-Constrained Quadratic Programming
The equality-constrained quadratic programming problem can be written as
min
x∈Rn
Q(x) = gT x + 1
2xT Gx
(9.3.1)
s.t.
AT x = b,
(9.3.2)

420
CHAPTER 9. QUADRATIC PROGRAMMING
where g ∈Rn, b ∈Rm, A = [a1, · · · , am] ∈Rn×m, G ∈Rn×n and G is
symmetric. Without loss of generality, we assume that rank (A) = m, i.e., A
has full column rank.
First, we introduce the variable elimination method. Assume that the
partitions are as follows:
x =

xB
xN

, A =

AB
AN

, g =

gB
gN

, G =

GBB
GBN
GNB
GNN

, (9.3.3)
where xB ∈Rm, xN ∈Rn−m, and AB is invertible. By these partitions, the
constraint condition (9.3.2) can be written as
AT
BxB + AT
NxN = b.
(9.3.4)
Since A−1
B exists, then
xB = (A−1
B )T (b −AT
NxN).
(9.3.5)
Substituting it into (9.3.1) gives the following form
min
xN∈Rn−m
1
2xT
N ˆGNxN + ˆgT
NxN + ˆc,
(9.3.6)
which is equivalent to (9.3.1), where
ˆgN
=
gN −ANA−1
B gB + [GNB −ANA−1
B GBB](A−1
B )T b,
(9.3.7)
ˆGN
=
GNN −GNB(A−1
B )T AT
N
−ANA−1
B GBN + ANA−1
B GBB(A−1
B )T AT
N,
(9.3.8)
ˆc
=
1
2bT A−1
B GBBA−T
B b + gT
BA−T
B b.
(9.3.9)
If ˆGN is positive deﬁnite, the solution of (9.3.6) is
x∗
N = −ˆG−1
N ˆgN
(9.3.10)
which is unique. So the solution of problem (9.3.1)–(9.3.2) is
x∗=

x∗
B
x∗
N

=

(A−1
B )T b
0

+

(A−1
B )T AT
N
−I

ˆG−1
N ˆgN.
(9.3.11)
Let λ∗be the Lagrange multiplier vector at x∗, then
g + Gx∗= Aλ∗,
(9.3.12)

9.3. EQUALITY-CONSTRAINED QUADRATIC PROGRAMMING
421
and thus
λ∗= A−1
B (gB + GBBx∗
B + GBNx∗
N).
(9.3.13)
If ˆGN in (9.3.6) is positive semi-deﬁnite, then when
(I −ˆGN ˆG+
N)ˆgN = 0,
(9.3.14)
i.e., ˆgN ∈R( ˆGN), the minimization problem (9.3.6) is bounded, and its
solution is
x∗
N = −ˆG+
N ˆgN + (I −ˆG+
N ˆGN)˜x,
(9.3.15)
where ˜x ∈Rn−m is any vector, ˆG+
N denotes the generalized inverse matrix of
ˆGN. In this case, the solution of problem (9.3.1)–(9.3.2) can be represented
by (9.3.15) and (9.3.5). If (9.3.14) does not hold, the problem (9.3.6) has no
lower bound, and thus the original problem (9.3.1)–(9.3.4) also has no lower
bound, that is, the original problem has no ﬁnite solution.
If ˆGN has negative eigenvalue, it is obvious that the minimization problem
(9.3.6) has not lower bound, and thus the problem (9.3.1)–(9.3.2) has not
ﬁnite solution.
Example 9.3.1
min
Q(x) = x2
1 −x2
2 −x2
3
(9.3.16)
s.t.
x1 + x2 + x3 = 1,
(9.3.17)
x2 −x3 = 1.
(9.3.18)
From (9.3.18), we have
x2 = x3 + 1.
(9.3.19)
Substituting it into (9.3.17) yields
x1 = −2x3.
(9.3.20)
In fact, here xB = (x1, x2)T , xN = x3. By substituting (9.3.19)–(9.3.20) into
(9.3.16), we obtain
min
x3∈R 4x2
3 −(x3 + 1)2 −x2
3.
(9.3.21)
Solving (9.3.21) gives x3 = 1
2. By substituting x3 = 1
2 into (9.3.19)–(9.3.20),
we get
x∗= (−1, 3
2, 1
2)T ,

422
CHAPTER 9. QUADRATIC PROGRAMMING
which is the solution of (9.3.16)–(9.3.18).
By use of g∗= Aλ∗, it follows that
⎛
⎜
⎝
−2
−3
−1
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
1
1
1
−1
⎞
⎟
⎠

λ∗
1
λ∗
2

(9.3.22)
which gives Lagrange multipliers λ∗
1 = −2 and λ∗
2 = −1.
2
The idea of variable elimination method is simple and clear. However,
when AB closes to singular, computing the solution by (9.3.11) will lead to
a numerically instable case.
A direct generalization of the variable elimination method is the gener-
alized elimination method. We partition Rn into two complementary sub-
spaces, i.e., Rn = R(A)⊕N(AT ). Let y1, · · · , ym be a set of linearly indepen-
dent vectors in R(A), the range of A, and let z1, · · · , zn−m be a set of linearly
independent vectors in N(AT ), the null space of AT . Write
Y = [y1, · · · , ym], Z = [z1, · · · , zn−m],
which are n × m and n × (n −m) matrices respectively. Obviously, R(Y ) =
R(A), R(Z) = N(AT ), and [Y : Z] is nonsingular.
In addition, AT Y is
nonsingular and AT Z = 0. Set
x = Y ¯x + Zˆx,
(9.3.23)
where ¯x ∈Rm, ˆx ∈Rn−m, it follows from the constraint condition (9.3.2)
that
b = AT x = AT Y ¯x.
(9.3.24)
Then the feasible point of (9.3.1)–(9.3.2) can be represented as
x = Y (AT Y )−1b + Zˆx.
(9.3.25)
By substituting (9.3.25) into (9.3.1), we obtain
min
ˆx∈Rn−m(g + GY (AT Y )−1b)T Zˆx + 1
2 ˆxT ZT GZˆx,
(9.3.26)
which is an unconstrained minimization problem in Rn−m. Here ZT GZ and
ZT (g + GY (AT Y )−1b) are called reduced Hessian and reduced gradient, re-
spectively.
Suppose that ZT GZ is positive deﬁnite, then it follows from
(9.3.26) that
(ZT GZ)ˆx = −[ZT GY (AT Y )−1b + ZT g]
(9.3.27)

9.3. EQUALITY-CONSTRAINED QUADRATIC PROGRAMMING
423
or
ˆx∗= −(ZT GZ)−1ZT (g + GY (AT Y )−1b).
(9.3.28)
The system (9.3.27) can be solved by means of Cholesky factorization. Thus,
the (9.3.28) and (9.3.25) give the solution of problem (9.3.1)–(9.3.2)
x∗= Y (AT Y )−1b −Z(ZT GZ)−1ZT (g + GY (AT Y )−1b)
=
(I −Z(ZT GZ)−1ZT G)Y (AT Y )−1b −Z(ZT GZ)−1ZT g.(9.3.29)
Furthermore, from the KKT condition
Aλ∗= g + Gx∗,
by left-multiplying Y T and noting that AT Y is nonsingular, we obtain
(Y T A)λ∗= Y T (g + Gx∗)
and
λ∗
=
(AT Y )−T Y T [g + Gx∗]
=
(AT Y )−T Y T [Pg + GP T Y (AT Y )−1b],
(9.3.30)
where
P = I −GZ(ZT GZ)−1ZT
(9.3.31)
is an aﬃne mapping from Rn to R(A). In particular, if we choose Y such
that
AT Y = I,
(9.3.32)
where Y is a left-inverse of AT , then (9.3.25) becomes
x = Y b + Zˆx,
(9.3.33)
where ˆx ∈Rn−m, and further (9.3.29)–(9.3.30) become
x∗
=
Y b −Z(ZT GZ)−1ZT (g + GY b)
(9.3.34)
=
P T Y b −Z(ZT GZ)−1ZT g,
(9.3.35)
and
λ∗
=
Y T (g + Gx∗)
=
Y T (Pg + GP T Y b).
(9.3.36)

424
CHAPTER 9. QUADRATIC PROGRAMMING
From (9.3.25), we know that the feasible area of (9.3.1)–(9.3.2) is a sub-
space parallel to N(AT ).
The generalized elimination method just uses
column-vectors zi (i = 1, · · · , n−m) of Z, which form a base of the null space
of AT , as basis vectors and transforms the quadratic programming prob-
lem (9.3.1)–(9.3.2) into an unconstrained minimization (9.3.26) of quadratic
function in a reduced space. Thus, this kind of method is also said to be
null-space method.
The above discussions tell us that how to choose matrix Z, base matrix
of the null space N(AT ), is a key for this kind of methods. Diﬀerent choices
of Z form diﬀerent null-space methods for solving quadratic programming
problem (9.3.1)–(9.3.2). In the following we give some typical choices.
Clearly, the variable elimination method is a particular case of the gen-
eralized elimination method in which
Y
=

A−1
B
0

,
(9.3.37)
Z
=

−A−T
B AT
N
I

.
(9.3.38)
Another particular case is based on QR decomposition of A. Let
A = Q

R
0

= [Q1 Q2]

R
0

= Q1R,
(9.3.39)
where Q is an n × n orthogonal matrix, R is an m × m nonsingular upper
triangular matrix. Therefore, we have a choice
Y = (A+)T = Q1R−T , Z = Q2.
(9.3.40)
A general scheme for choosing Y and Z is as follows. For any Y and Z
with AT Y = I and AT Z = 0,
AT [Y Z] = [I 0].
(9.3.41)
Since [Y Z] is nonsingular, there exists V ∈Rn×(n−m) such that
[Y Z] =

AT
V T
−1
,
(9.3.42)

9.3. EQUALITY-CONSTRAINED QUADRATIC PROGRAMMING
425
i.e.,
[A V ]−1 =

Y T
ZT

.
(9.3.43)
It means that the diﬀerent choices of V ∈Rn×(n−m) lead to diﬀerent Y and
Z, and diﬀerent elimination methods. For example, if we set
V =

0
In−m

,
we can get the variable elimination method (9.3.11). If we set
V = Q2,
the above orthogonal decomposition choice (9.3.40) is obtained. Normally,
null-space method is very useful, especially for small and medium-sized prob-
lems and when the computation of the null-space matrix Z and the factors
of ZT GZ is not very expensive.
The Lagrange method for solving equality-constrained quadratic pro-
gramming is based on KKT conditions, which are
g + Gx = Aλ,
(9.3.44)
AT x = b.
(9.3.45)
The above system can be written in the matrix form

G
−A
−AT
0
 
x
λ

= −

g
b

.
(9.3.46)
Here

G
−A
−AT
0

(9.3.47)
is a KKT matrix for quadratic programming (9.3.1)–(9.3.2). It is not diﬃcult
to show that if A has full column-rank and ZT GZ is positive deﬁnite, then
KKT matrix (9.3.47) is nonsingular.
Theorem 9.3.2
Let A ∈Rn×m be a full column-rank matrix.
Assume
that the reduced Hessian ZT GZ is positive deﬁnite. Then the KKT matrix
(9.3.47) is nonsingular. Furthermore, there exists a unique KKT pair (x∗, λ∗)
such that equation (9.3.46) is satisﬁed.

426
CHAPTER 9. QUADRATIC PROGRAMMING
Proof.
The proof is by contradiction. Suppose that KKT matrix (9.3.47)
is singular, then there exists nonzero vector (p, v) ̸= 0 such that

G
−A
−AT
0
 
p
v

= 0,
(9.3.48)
where p ∈Rn and v ∈Rm. Clearly, we have AT p = 0. By left-multiplying

p
v
T
on both sides of (9.3.48), we obtain
0 =

p
v
T 
G
−A
−AT
0
 
p
v

= pT Gp.
Since p ∈N(AT ) and Z = [z1, · · · , zn−m] spans N(AT ), we may denote
p = Zu for some u ∈Rn−m and have
0 = pT Gp = uT ZGZu.
The assumption that ZT GZ is positive deﬁnite gives u = 0 and then
p = Zu = 0.
(9.3.49)
So, it follows from (9.3.48) that Av = 0. Notice that A has full column-rank,
then we obtain also v = 0 which together with (9.3.49) contradicts the fact
(p, v) ̸= 0. We complete the proof.
2
Now let KKT matrix be nonsingular.
Then there exist matrices U ∈
Rn×n, W ∈Rn×m and T ∈Rm×m such that

G
−A
−AT
0
−1
=

U
W
W T
T

,
(9.3.50)
and the unique solution of (9.3.46) is
x∗
=
−Ug −Wb,
(9.3.51)
λ∗
=
−W T g −Tb.
(9.3.52)
As long as the KKT matrix (9.3.47) is nonsingular, then (9.3.50) is deter-
mined uniquely, so the stationary point of the Lagrangian function is deter-
mined uniquely by (9.3.51)–(9.3.52). However, since there are many expres-
sions for U, W, and T, and we can derive a diﬀerent computational schemes
of formula (9.3.51)–(9.3.52).

9.4. ACTIVE SET METHODS
427
If G is invertible and A has full column-rank, then (AT G−1A)−1 exists.
It is not diﬃcult to show that the expressions of U, W, and T in (9.3.50) are
U
=
G−1 −G−1A(AT G−1A)−1AT G−1,
(9.3.53)
W
=
−G−1A(AT G−1A)−1,
(9.3.54)
T
=
−(AT G−1A)−1.
(9.3.55)
Then it follows from (9.3.46) that the solution for quadratic programming
with equality constraints is
x∗
=
−G−1g + G−1A(AT G−1A)−1[AT G−1g + b],
(9.3.56)
λ∗
=
(AT G−1A)−1[AT G−1g + b].
(9.3.57)
As we said, if A has full column-rank and ZT GZ is positive deﬁnite, then
KKT matrix is invertible. In this case, if Y and Z are deﬁned by (9.3.42),
the matrices U, W, and T in (9.3.50) can be represented as
U
=
Z(ZT GZ)−1zT ,
(9.3.58)
W
=
−P T Y,
(9.3.59)
T
=
−Y T GP T Y,
(9.3.60)
where P is deﬁned by (9.3.31). Substituting (9.3.58)–(9.3.60) into (9.3.51)–
(9.3.52) yields the formula (9.3.35)–(9.3.36). Hence, the Lagrange method is
equivalent to the generalized elimination method.
9.4
Active Set Methods
Most QP problems involve inequality constraints and so can be expressed
in the form (9.1.1)–(9.1.3). In this section we describe how the methods for
solving equality-constrained QP can be generalized to handle the general QP
problem (9.1.1)–(9.1.3) by means of active set methods, which are, in general,
the most eﬀective methods for small and medium-sized problems. We start
our discussion by considering the convex case, i.e., the matrix G in (9.1.1)–
(9.1.3) is positive semi-deﬁnite. The other case in which G is indeﬁnite will
be simply discussed in the end of the section. Intuitively, inactive inequality
constraints do not play any role near the solution, so they can be dropped;
the active inequality constraints have zero values at solution, and so they can
be replaced by equality constraints. The following lemma is a base for active
set methods.

428
CHAPTER 9. QUADRATIC PROGRAMMING
Lemma 9.4.1 Let x∗be a local minimizer of QP problem (9.1.1)–(9.1.3).
Then x∗is a local minimizer of problem
minx∈Rn
gT x + 1
2xT Gx
(9.4.1)
s.t.
aT
i x = bi, i ∈E ∪I(x∗).
(9.4.2)
Conversely, if x∗is a feasible point of (9.1.1)–(9.1.3) and a KKT point of
(9.4.1)–(9.4.2), and the corresponding Lagrange multiplier vector λ∗satisﬁes
λ∗
i ≥0, i ∈I(x∗),
(9.4.3)
then x∗is also the KKT point of problem (9.1.1)–(9.1.3).
Proof.
Since, near x∗, the feasible point of (9.1.1)–(9.1.3) is also feasible
for problem (9.4.1)–(9.4.2), then, obviously, the local minimizer of (9.1.1)–
(9.1.3) is also the local minimizer of problem (9.4.1)–(9.4.2).
Now let x∗be feasible for (9.1.1)–(9.1.3) and a KKT point for (9.4.1)–
(9.4.2). Let there exist λ∗
i (i ∈E ∪I(x∗)) such that
Gx∗+ g =

i∈I(x∗)∪E
aiλ∗
i ,
(9.4.4)
λ∗
i (aT
i x∗−bi) = 0, λ∗
i ≥0, i ∈I(x∗).
(9.4.5)
Deﬁne
λ∗
i = 0, i ∈I \ I(x∗).
(9.4.6)
Then we immediately have from (9.4.4)–(9.4.6) that
Gx∗+ g =
m

i=1
λ∗
i ai,
(9.4.7)
aT
i x∗= bi, i ∈E,
(9.4.8)
aT
i x∗≥bi, i ∈I,
(9.4.9)
λ∗
i ≥0, i ∈I,
(9.4.10)
λ∗
i (aT
i x∗−bi) = 0, ∀i
(9.4.11)
which means that x∗is a KKT point of problem (9.1.1)–(9.1.3).
2
The active set methods are a feasible point method, that is, all iterates
remain feasible. In each iteration, we solve a quadratic programming sub-
problem with a subset of equality constraints. This subset is said to be a
working set and is denoted by Sk ⊂E ∪I(x∗).

9.4. ACTIVE SET METHODS
429
If the solution of the equality-constrained QP subproblem on Sk is feasible
for original problem (9.1.1)–(9.1.3), we need to examine whether (9.4.3) is
satisﬁed or not. If (9.4.3) is satisﬁed, then stop and we get the solution of the
original problem. Otherwise, the KKT conditions are not satisﬁed, and the
objective function q(·) can be decreased by dropping this constraint. Thus,
we remove the index from the working set Sk and solve a new subproblem.
If the solution of equality-constrained QP subproblem on Sk is not feasible
for problem (9.1.1)–(9.1.3), we need to add a constraint into the working set
Sk and then solve a new subproblem.
At each iteration, a feasible point xk and a working set Sk are known.
Each iteration attempts to locate a solution of an equality-constrained sub-
problem on Sk. Let d be a step from xk. We can express the QP subproblem
in terms of d. Consider the QP subproblem
mind∈Rn
1
2(xk + d)T G(xk + d) + gT (xk + d),
(9.4.12)
s.t.
aT
i d = 0, i ∈Sk
(9.4.13)
which can be written as
mind∈Rn
1
2dT Gd + gT
k d
(9.4.14)
s.t.
aT
i d = 0, i ∈Sk
(9.4.15)
where gk = ∇Q(xk) = Gxk + g. Denote the KKT point of (9.4.12)–(9.4.13)
by dk, the corresponding Lagrange multipliers by λ(k)
i
(i ∈Sk). If dk = 0,
then xk is the KKT point of subproblem
minx∈Rn
1
2xT Gx + gT x
(9.4.16)
s.t.
aT
i x = bi, i ∈Sk.
(9.4.17)
At this time, if λ(k)
i
≥0, ∀i ∈Sk ∩I, then xk is a KKT point of problem
(9.1.1)–(9.1.3), and we terminate the iteration. Otherwise, there exists nega-
tive Lagrange multiplier, for example, λ(k)
ik < 0. In this case, it is possible to
reduce the objective function by dropping the ik-th constraint from current
working set Sk. Then we solve the resulting QP subproblem. Note that if
there are more than one index such that λi < 0, it is usual to choose ik for
which
λik = min
i∈Sk∩I
λ(k)
i
<0
λ(k)
i
(9.4.18)

430
CHAPTER 9. QUADRATIC PROGRAMMING
and set
Sk := Sk \ {ik}.
(9.4.19)
Suppose that the solution dk ̸= 0. If xk + dk is feasible with regard to all
the constraints, then we set
xk+1 = xk + dk.
(9.4.20)
Otherwise, a line search is made along the direction dk and we set
xk+1 = xk + αkdk,
(9.4.21)
where αk is a steplength such that xk + αkdk is the “best” feasible point on
[xk, xk + dk] and the closest to xk + dk, i.e., take αk as large as possible in
the interval [0, 1].
Now we derive the explicit formula for αk. We ask xk+αkdk for satisfying
all constraints. Obviously, if i ∈Sk, then the corresponding constraint will
be certainly feasible. Thus we only need to consider those constraints for
which i /∈Sk. There are two cases we need to consider. If aT
i dk ≥0 for some
i /∈Sk, then we have for all αk ≥0,
aT
i (xk + αkdk) ≥aT
i xk ≥bi, i /∈Sk.
In this case, the constraint is satisﬁed. If aT
i dk < 0 for some i /∈Sk, we have
aT
i (xk + αkdk) ≥bi
only if
αk ≤bi −aT
i xk
aT
i dk
, i /∈Sk.
(9.4.22)
Hence, we should take
αk =
min
i/∈Sk
aT
i dk<0
bi −aT
i xk
aT
i dk
.
(9.4.23)
Since we want αk to be as large as possible in [0, 1] subject to remaining
feasibility, we have the following formula:
αk = min
⎧
⎪
⎨
⎪
⎩
1, min
i/∈Sk
aT
i dk<0
bi −aT
i xk
aT
i dk
⎫
⎪
⎬
⎪
⎭
.
(9.4.24)

9.4. ACTIVE SET METHODS
431
If αk < 1, i.e., (9.4.23) holds, then there exists some j /∈Sk such that
αk = bj −aT
j xk
aT
j dk
.
Thus,
aT
j xk+1 = aT
j xk + αkaT
j dk = bj.
This means that there is a new constraint indexed by j /∈Sk becoming an
active constraint at xk+1. So we put it into the working set, that is, set
Sk+1 = Sk ∪{j}.
If αk = 1, then the working set remains the same, i.e., Sk+1 = Sk.
So, we can continue the next iteration on the new working set Sk+1.
Now, we are in a position to give the algorithm of active set method as
follows.
Algorithm 9.4.2 (Active Set Methods)
Step 1. Given x1, set S1 = E ∪I(x1), k := 1.
Step 2. Find the solution dk for subproblem (9.4.12)–(9.4.13).
If dk ̸= 0, go to Step 3;
Else if dk = 0, compute λ(k)
i
from Gxk + g = 
i∈Sk λ(k)
i
ai.
If λ(k)
i
≥0 ∀i ∈Sk ∩I, stop;
else ﬁnd ik by (9.4.18).
Sk := Sk \ {ik}, xk+1 = xk, go to Step 4.
Step 3. Find αk by (9.4.24);
Set
xk+1 = xk + αkdk.
(9.4.25)
If αk = 1, go to Step 4;
Else ﬁnd j /∈Sk such that
aT
j (xk + αkdk) = bj.
(9.4.26)
Set Sk := Sk ∪{j}.
Step 4. Sk+1 := Sk, k := k + 1, go to Step 2.
2

432
CHAPTER 9. QUADRATIC PROGRAMMING
Now we give analysis to the algorithm.
From Algorithm 9.4.2, we know that all iterates are feasible, i.e.,
xk ∈X, ∀k,
(9.4.27)
and the objective function remains descent, i.e.,
Q(xk+1) ≤Q(xk), ∀k.
(9.4.28)
Further, as long as dk ̸= 0 (i.e., xk is not the KKT point of (9.4.16)–(9.4.17))
and αk > 0, we have
Q(xk+1) < Q(xk).
(9.4.29)
If the algorithm terminates in ﬁnitely many steps, the obtained point is
a KKT point of the original problem (9.1.1)–(9.1.3).
Suppose that the algorithm does not terminate in ﬁnitely many steps;
since there is only a ﬁnite number of constraints, it is impossible that the
number of elements in Sk increases inﬁnitely many times and does not reduce.
So there are inﬁnitely many indices k such that dk = 0. It follows from the
algorithm that there are inﬁnitely many indices k such that xk is a KKT
point of (9.4.16)–(9.4.17). Since the number of constraints is ﬁnite, Sk has
only ﬁnitely many diﬀerent combinations and so the sequence of the objective
values {Q(xk)} has only ﬁnitely many elements. Therefore, there must exist
a suﬃciently large k0 such that
Q(xk+1) = Q(xk), ∀k ≥k0.
(9.4.30)
Then for all k ≥k0, in both
αk = 0
(9.4.31)
and
dk = 0,
(9.4.32)
only one holds. Since there are only ﬁnitely many constraints, it is impossible
that the algorithm only increases the constraint into Sk, nor reduces the
constraint from Sk. Hence, there must be inﬁnitely many indices k such that
dk ̸= 0,
(9.4.33)
and inﬁnitely many indices k such that
dk = 0.
(9.4.34)

9.4. ACTIVE SET METHODS
433
So, there exist k2 > k1 > k0 such that
dk1 = 0, dk2 = 0,
(9.4.35)
dk ̸= 0, k1 < k < k2,
(9.4.36)
and
k2 > k1 + 1.
(9.4.37)
Lemma 9.4.3 Let k0 be an index satisfying (9.4.30). If k2 > k1 > k0 satisfy
(9.4.35)–(9.4.37), then
Sk2 ̸= Sk1.
(9.4.38)
Proof.
By (9.4.35), there exist λ(k1)
i
such that
g + G¯x =

i∈Sk1
aiλ(k1)
i
,
(9.4.39)
where ¯x = xk0. From (9.4.31)–(9.4.32), it follows that xk = ¯x for all k ≥k0.
Since dk1+1 ̸= 0, αk1+1 = 0, there must be
j /∈Sk1+1,
(9.4.40)
such that j ∈Sk1+2,
j ∈I(¯x)
(9.4.41)
and
aT
j dk1+1 < 0.
(9.4.42)
Since dk is a solution for subproblem (9.4.12)–(9.4.13), i.e., dk is a descent
direction of the objective function, then
(g + G¯x)T dk1+1 ≤0.
(9.4.43)
By using (9.4.39), (9.4.43) and Sk1+1 = Sk1 \ {ik1}, we get
λ(k1)
ik1 aT
ik1dk1+1 ≤0,
(9.4.44)
which means that
aT
ik1dk1+1 ≥0
(9.4.45)
by the deﬁnition of {ik}. Comparing (9.4.42)–(9.4.44) gives j ̸= ik1. Hence
it follows from (9.4.40) that j /∈Sk1.
On the other hand, j ∈Sk1+2 ⊆Sk2. Hence we have Sk2 ̸= Sk1. The
proof is complete.
2
Finally, we give the convergence theorem of active set methods.

434
CHAPTER 9. QUADRATIC PROGRAMMING
Theorem 9.4.4 If, for all k, ai (i ∈E ∪I(xk)) are linearly independent,
then either the sequence generated from Algorithm 9.4.2 converges to a KKT
point of problem (9.1.1)–(9.1.3) in ﬁnite iterations, or the original problem
(9.1.1)–(9.1.3) is unbounded below.
Proof.
Assume that the problem (9.1.1)–(9.1.3) is bounded below, then
the sequence {xk} is bounded.
If the solution of subproblem (9.4.12)–(9.4.13) is dk = 0, then xk is a
KKT point of (9.4.16)–(9.4.17) for the current working set Sk.
If λ(k)
i
≥
0, ∀i ∈Sk ∩I, then xk is a KKT point of the original problem (9.1.1)–(9.1.3).
Otherwise, there exists λ(k)
ik < 0 (ik ∈Sk ∩I) for which we can ﬁnd a feasible
descent direction dk such that
aT
j dk = 0,
j ∈Sk, j ̸= ik,
(9.4.46)
aT
ikdk > 0
(9.4.47)
and
gT
k dk = (λ(k))T AT
k dk = (aT
ikdk)(λ(k))T eik = (aT
ikdk)λ(k)
ik < 0.
(9.4.48)
If we substitute (9.4.46) for the constraints in (9.4.13), i.e., set Sk := Sk\{ik},
the resulting QP subproblem will have a feasible descent direction. Since
αk > 0, we have
Q(xk+1) < Q(xk),
and consequently, by ﬁniteness of constraints, the algorithm never returns to
the current working set Sk, and the sequence {xk} is ﬁnite.
If dk ̸= 0 and αk = 1, then Sk+1 = Sk, and the subproblem (9.4.12)–
(9.4.13) is unchanged for xk+1 and so the xk+1 is the solution of (9.4.12)–
(9.4.13).
Only if dk ̸= 0 and αk < 1, xk+1 is not the solution of (9.4.12)–(9.4.13).
At this time, from (9.4.26) in Step 3 of Algorithm 9.4.2, we know that there is
an index j /∈Sk such that the j-th constraint is feasible. So, such a constraint
is added into Sk+1. If this procedure occurs repeatedly, then after at most n
iterations the working set Sk will contains n indices, which correspond to n
linearly independent vectors, then it follows from (9.4.13) that dk = 0. Thus
such a procedure continues at most n times. So there will be a KKT point
xk of (9.4.16)–(9.4.17) at most after n iterations.
Combining the above discussion, in any case, the algorithm will converge
in ﬁnite iterations to the KKT point of problem (9.1.1)–(9.1.3).
2

9.5. DUAL METHOD
435
By modifying the algorithm, the active set method for a convex QP prob-
lem can be adopted to the indeﬁnite case in which the Hessian matrix G has
some negative eigenvalues.
As we know from §9.3 that if G in Sk is indeﬁnite, then the problem
(9.4.13) may be unbounded. We can choose the direction dk such that aT
i dk =
0 (∀i ∈Sk) and either
dT
k Gdk < 0
(9.4.49)
or
∇Q(xk)T dk < 0, dT
k Gdk = 0
(9.4.50)
where ∇Q(xk) = g + Gxk. If, for all i /∈Sk, aT
i dk ≥0, then the original
problem (9.1.1)–(9.1.3) is unbounded below. Otherwise, we can ﬁnd i /∈Sk
and aT
i dk < 0.
Then, when α > 0 is suﬃciently large, xk + αdk is not
a feasible point of (9.1.1)–(9.1.3). In this case we can take αk as large as
possible and make xk + αkdk feasible.
9.5
Dual Method
For the convex QP problem
minx∈Rn
gT x + 1
2xT Gx
(9.5.1)
s.t.
aT
i x = bi, i ∈E,
(9.5.2)
aT
i x ≥bi, i ∈I,
(9.5.3)
where G is symmetric and positive deﬁnite. We know from §9.2 that the dual
problem is
minλ∈Rm
−(b + AG−1g)T λ + 1
2λT (AT G−1A)λ
(9.5.4)
s.t.
λi ≥0, i ∈I.
(9.5.5)
Now we adopt the active-set method to (9.5.4)–(9.5.5).
The equality-
constrained subproblem we solved at each iteration is
minλ∈Rm
−(b + AT G−1g)T λ + 1
2λT (AT G−1A)λ
(9.5.6)
s.t.
λi = 0, i ∈¯Sk,
(9.5.7)

436
CHAPTER 9. QUADRATIC PROGRAMMING
where ¯Sk ⊆I is a working set for dual problem (9.5.4)–(9.5.5). Let λk be a
KKT point of the subproblem (9.5.6)–(9.5.7). Set
xk = −G−1(g −Aλk),
(9.5.8)
then
Gxk + g = Aλk,
(9.5.9)
and from
(b + AT G−1g −AT G−1Aλk)i = 0, ∀i /∈¯Sk,
(9.5.10)
we have
(AT xk −b)i = 0, ∀i /∈¯Sk.
(9.5.11)
Thus, xk is the KKT point of the subproblem
minx∈Rn
gT x + 1
2xT Gx
(9.5.12)
s.t.
aT
i x = bi, i /∈¯Sk.
(9.5.13)
Write Sk = {I ∪E} \ ¯Sk. It is obvious that (9.5.12)–(9.5.13) is the same as
(9.4.16)–(9.4.17). It is not diﬃcult to see that the Lagrange multipliers of
dual problem (9.5.6)-(9.5.7) satisfy
(AT G−1Aλk −b −AT G−1g)i
=
(AT xk −b)i = aT
i xk −bi, i ∈¯Sk.
(9.5.14)
We ask λk to be a feasible point of (9.5.4)–(9.5.5). If the Lagrange multiplier
(9.5.14) of the dual problem (9.5.6)–(9.5.7) is nonnegative, xk is a KKT point
of the original problem (9.5.1)–(9.5.3). Let Ak be a matrix with the columns
ai (i ∈Sk), ¯λk the vector consisting of the components of λk corresponding
to i ∈Sk. It follows from (9.5.10) that
bi + aT
i G−1g −aT
i G−1Ak¯λk = 0, i ∈Sk,
(9.5.15)
i.e.,
b(k) + AT
k G−1g −AT
k G−1Ak¯λk = 0,
(9.5.16)
where b(k) consists of the components of b corresponding to i ∈Sk. Then
(9.5.16) gives
¯λk = (AT
k G−1Ak)−1[b(k) + AT
k G−1g].
(9.5.17)

9.5. DUAL METHOD
437
When Lagrange multipliers in (9.5.10) are not all nonnegative, we should,
by the active-set method, drop an index ik ∈¯Sk, that is, add the index ik into
Sk. For convenience of sign, we write ik as p. Then we have Sk+1 = Sk ∪{p}.
Let
¯λk+1 =
 ¯λk
0

+

δλk
βk

.
(9.5.18)
It follows from (9.5.17) that

AT
k G−1Ak
AT
k G−1ap
aT
p G−1Ak
aT
p G−1ap
 
δλk
βk

=

0
bp −aT
p xk

,
(9.5.19)
which gives
¯λk+1 =
 ¯λk
0

+ βk

−(AT
k G−1Ak)−1AT
k G−1ap
1

.
(9.5.20)
So,
xk+1
=
xk + G−1Ak+1

¯λk+1 −
 ¯λk
0

=
xk + βkG−1(I −Ak(AT
k G−1Ak)−1AT
k G−1)ap.
(9.5.21)
Let
A∗
k
=
(AT
k G−1Ak)−1AT
k G−1,
(9.5.22)
yk
=
A∗
kap.
(9.5.23)
Since ¯λk+1 should satisfy ¯λk+1 ≥0, it follows from (9.5.20) and (9.5.23) that
0 ≤βk ≤
min
j∈Sk
(yk)j>0
(¯λk)j
(yk)j
.
(9.5.24)
If
G−1(I −AkA∗
k)ap = 0
(9.5.25)
and yk ≤0, then
(−yk, 1)T (AT
k+1G−1Ak+1)

−yk
1

= 0
(9.5.26)

438
CHAPTER 9. QUADRATIC PROGRAMMING
and
(−yk, 1)T (b(k+1) + AT
k+1G−1g) = bp −aT
p xk > 0,
(9.5.27)
which indicate that the dual problem (9.5.4)–(9.5.5) is unbounded. Further,
we know by the duality theory that the original problem (9.5.1)–(9.5.3) has
no feasible point.
Now, by use of the analysis above, we describe the dual method due to
Goldfarb and Idnani [155] as follows (we consider the case in which me = 0,
i.e., the problem with only inequality constraints).
Algorithm 9.5.1 (Dual Method)
Step 1. x1 = −G−1g, f1 = 1
2gT x1, S1 = Φ; k := 1, ¯λ1 = Φ, q = 0.
Step 2. Compute ri = bi −aT
i xk, i = 1, · · · , m.
If ri ≤0, stop.
Choose p such that rp = max1≤i≤m ri;
¯λk :=
 ¯λk
0

.
Step 3. dk := ˆGkap = G−1(I −AkA∗
k)ap; yk := A∗
kap.
If {j| (yk)j > 0, j ∈Sk} is nonempty, set
αk =
min
(yk)j>0
j∈Sk
(¯λk)j
(yk)j
= (¯λk)l
(yk)l
;
(9.5.28)
else set αk = ∞.
Step 4. If dk ̸= 0, go to Step 5;
If αk = ∞, stop (the original problem has no feasible point);
Sk := Sk \ {l}; q := q −1;
¯λk := ¯λk + αk

−yk
1

;
Modify A∗
k and ˆGk; turn to Step 3.
Step 5 ˆα := −(bp −aT
p xk)/aT
p dk;
αk := min{αk, ˆα};
xk+1 := xk + αkdk;
fk+1 := fk + αkaT
p dk(1
2αk + (¯λk)q+1);
¯λk+1 := ¯λk + αk

−yk
1

.

9.5. DUAL METHOD
439
Step 6. If αk < ˆα, go to Step 7;
Sk+1 := Sk ∪{p}; q := q + 1;
Compute ˆGk+1 and A∗
k+1, k := k + 1; turn to Step 2.
Step 7. Sk := Sk \ {l}; q := q −1;
Remove the l-th component from ¯λk and obtain a new ¯λk;
Compute ˆGk and A∗
k, turn to Step 3.
2
Next, we give a simple example which employs Algorithm 9.5.1.
Example 9.5.2
min
1
2x2
1 + 1
2x2
2 + 1
2x2
3 −3x2 −x3
(9.5.29)
s.t.
−x1 −x2 −x3 ≥−1,
(9.5.30)
x3 −x2 ≥−1.
(9.5.31)
Solution. This example is a modiﬁcation of the problem (9.3.16)–(9.3.18).
The unique solution is still (−1, 3
2, 1
2)T . By use of Algorithm 9.5.1, we have
x1 = −G−1g =
⎛
⎜
⎝
0
3
1
⎞
⎟
⎠,
r1 = 3 > 0, r2 = 1 > 0.
Then we have p = 1 from Step 2, and
d1 = G−1ap =
⎛
⎜
⎝
−1
−1
−1
⎞
⎟
⎠.
Since S1 is empty, α1 = ∞in Step 3. In Step 5, we get
ˆα = −r1/aT
p d1 = 1,
and obtain
α1 = 1, x2 = x1 + α1dk =
⎛
⎜
⎝
−1
2
0
⎞
⎟
⎠,
¯λ2 = (1), S2 = {1}.

440
CHAPTER 9. QUADRATIC PROGRAMMING
Thus, after one iteration, x2 is the solution of the subproblem
min
1
2x2
1 + 1
2x2
2 + 1
2x2
3 −3x2 −x3
(9.5.32)
s.t.
−x1 −x2 −x3 = −1.
(9.5.33)
In the second iteration, we have
r1 = 0, r2 = 1.
Then p = 2 from Step 2, and
d2 = G−1(I −
⎛
⎜
⎝
1
1
1
⎞
⎟
⎠1
3 (1 1 1))
⎛
⎜
⎝
0
−1
1
⎞
⎟
⎠=
⎛
⎜
⎝
0
−1
1
⎞
⎟
⎠.
Since y2 = aT
2 a1 = 0, we have α2 = ∞in Step 3. In Step 5, we have
ˆα = −r2/aT
2 d2 = 1
2.
Then α2 := ˆα = 1
2,
x3 = x2 + α2d2 =
⎛
⎜
⎝
−1
−1
−1
⎞
⎟
⎠+ 1
2
⎛
⎜
⎝
0
−1
1
⎞
⎟
⎠=
⎛
⎜
⎝
−1
−3
2
1
2
⎞
⎟
⎠,
and ¯λ3 = (1 1
2)T . Hence, x3 is the solution of the original problem and ¯λ3 is
the corresponding Lagrange multiplier.
2
In concrete computation, Goldfarb and Idnani suggested using the Cholesky
factorization of G,
G = LLT ,
and then employing QR decomposition to L−1Ak, that is,
L−1Ak = Qk

Rk
0

.
This approach allows us to get better numerical stability than by using G−1
directly.

9.6. INTERIOR ELLIPSOID METHOD
441
Instead, Powell [274] suggested using
Ak = Qk

Rk
0

= [Q(1)
k
Q(2)
k ]

Rk
0

,
(9.5.34)
and then employing the inverse Cholesky factorization of [Q(2)
k ]T GQ(2)
k , i.e.,
UkUT
k = [Q(2)
k ]T GQ(2)
k ,
(9.5.35)
where Uk is an upper triangular matrix. In the algorithm that Powell [274]
presented, each iteration updates Q(1)
k , Rk, and Uk.
9.6
Interior Ellipsoid Method
Karmarkar [184] introduced a new polynomial-time algorithm for solving lin-
ear programming problems that sparked enormous interest in the mathemat-
ical programming community. Karmarkar’s algorithm generates a sequence
of points in the interior of the feasible region while converging to the opti-
mal solution. This algorithm is eﬀective and competitive with the simplex
method in terms of solution time for linear programming (LP).
Ye and Tse [364] present an extension of Karmarkar’s LP algorithm for
convex quadratic programming. We introduce this algorithm in brief. The
interested readers can consult the original paper for details.
The original version of Karmarkar’s algorithm solves a linear program-
ming of the special form
min
ˆcT ˆx
(9.6.1)
s.t.
ˆAT ˆx = 0, eT ˆx = n + 1, ˆx ≥0,
(9.6.2)
where ˆc ∈Rn+1, ˆx ∈Rn+1, ˆA ∈R(n+1)×(m+1), e = (1, · · · , 1)T ∈Rn+1. Now,
we generalize the Karmarkar’s algorithm to convex quadratic programming.
Consider convex quadratic programming problem
min
gT x + 1
2xT Gx ∆= q(x)
(9.6.3)
s.t.
AT x = b,
(9.6.4)
x ≥0,
(9.6.5)

442
CHAPTER 9. QUADRATIC PROGRAMMING
where A ∈Rn×m. Let xk be an interior point, i.e.,
AT xk = b,
(9.6.6)
xk > 0.
(9.6.7)
Deﬁne
Dk = diag(xk) =
⎡
⎢⎣
(xk)1
0
...
0
(xk)n
⎤
⎥⎦.
(9.6.8)
Let the transformation ˆx = Tkx ∈Rn+1 as follows:
ˆxi = (n + 1)(D−1
k x)i
eT D−1
k x + 1
, i = 1, · · · , n;
(9.6.9)
ˆxn+1 = (n + 1)/[eT D−1
k x + 1].
(9.6.10)
Obviously, the inverse transformation T −1
k
: Rn+1 →Rn is deﬁned by
x = T −1
k
ˆx = Dkˆx[n]
ˆxn+1
,
(9.6.11)
where e = (1, · · · , 1)T ∈Rn+1, ˆx[n] = (ˆx1, · · · , ˆxn)T .
Then, the problem
(9.6.3)–(9.6.5) can be written as
minˆx∈Rn+1
ˆxn+1q(T −1
k
ˆx) ∆= ˆq(ˆx)
(9.6.12)
s.t.
AT Dkˆx[n] −ˆxn+1b = 0,
(9.6.13)
eT ˆx = n + 1,
(9.6.14)
ˆx[n] ≥0, ˆx > 0.
(9.6.15)
By substituting (9.6.11) into (9.6.12), we obtain an equivalent form of (9.6.12)–
(9.6.15):
min
ˆgT
k ˆx[n] + 1
2 ˆx[n]T ˆGkˆx[n]/ˆxn+1
(9.6.16)
s.t.
ˆAT
k ˆx = ˆb,
(9.6.17)
ˆx[n] ≥0, ˆxn+1 > 0,
(9.6.18)
where
ˆGk = DkGDk, ˆgk = Dkg,
(9.6.19)

9.6. INTERIOR ELLIPSOID METHOD
443
ˆAk =
⎡
⎢⎣
DkA
e
−bT
⎤
⎥⎦, ˆb =
⎛
⎜
⎜
⎜
⎜
⎝
0
...
0
n + 1
⎞
⎟
⎟
⎟
⎟
⎠
.
(9.6.20)
Using the interior ellipsoid method, we solve the following subproblem
(9.6.22)–(9.6.24) over an interior ellipsoid centered at ˆxk, instead of solving
the subproblem (9.6.12)–(9.6.15). Note that, for a given iterate xk, ˆxk =
Tkxk = e and ˆq(ˆxk) = ˆq(e) = q(xk). So, the interior ellipsoid happens to be
an interior sphere in the feasible area of problem (9.6.12)–(9.6.15). Thus, the
condition (9.6.18) can be enhanced to
∥ˆx −e∥2 ≤β < 1.
(9.6.21)
Obviously, (9.6.18) will hold provided (9.6.21) holds. Hence, we consider the
subproblem
min
ˆgT
k ˆx[n] + 1
2 ˆx[n] + 1
2 ˆx[n]T ˆGkˆx[n]/ˆxn+1
(9.6.22)
s.t.
ˆAT
k ˆx = ˆb,
(9.6.23)
∥ˆx −e∥2 ≤β < 1,
(9.6.24)
where β < 1 is a constant independent of k.
By the Karush-Kuhn-Tucker Theorem, solving (9.6.22)–(9.6.24) is equiv-
alent to solving the following system:
ˆgk + ˆx−1
n+1 ˆGkˆx[n] = ˆAk[n]λ + µ(ˆx[n] −e[n]),
(9.6.25)
−1
2
1
ˆx2
n+1
ˆx[n]T ˆGkˆx[n] = (ˆa(k)
n+1)T λ + µ(ˆxn+1 −1) = 0,
(9.6.26)
ˆAT
k ˆx = ˆb,
(9.6.27)
∥ˆx −e∥2 ≤β,
(9.6.28)
µ[∥ˆx −e∥2 −β] = 0, µ ≤0,
(9.6.29)
where (9.6.25) and (9.6.26) are the ﬁrst n equations and the last equation of
the stationary point condition in KKT conditions respectively. Here ˆAk[n]
is the matrix of the ﬁrst n rows of matrix ˆAk, ˆa(k)
n+1 is the (n + 1)-th row of
ˆAk, e[n] = (1, · · · , 1)T ∈Rn and λ ∈Rm+1. So, (9.6.25) and (9.6.27) can be
written in the following form
Pk

ˆx[n]
ˆλ

= ˆxn+1¯b + ˜b,
(9.6.30)

444
CHAPTER 9. QUADRATIC PROGRAMMING
where
Pk =
 ˆGk + ˆµI
−ˆA[n]
ˆA[n]T
0

,
(9.6.31)
¯b =
⎡
⎢⎣
−ˆgk
b
−1
⎤
⎥⎦, ˜b =
⎡
⎢⎣
ˆµe
0
n + 1
⎤
⎥⎦,
(9.6.32)
ˆλ = ˆxn+1λ, ˆµ = −ˆxn+1µ.
(9.6.33)
Then, for any given ˆµ ≥0, we can ﬁnd ˆλ and ˆx[n] from (9.6.30). Then we
obtain ˆxn+1 from substituting ˆλ and ˆx[n] into (9.6.26). This indicates that
for any given ˆµ ≥0, we can ﬁnd ˆx(ˆµ). Deﬁne the function
h(ˆµ) = ∥ˆx(ˆµ) −e∥2 −β.
(9.6.34)
If h(0) ≤0, then ˆx(0) is the solution of (9.6.12)–(9.6.15).
In this case,
x = Dkˆx(0)[n]/ˆx(0)n+1 is the solution of the original problem.
If h(0) > 0, since limˆµ→∞h(ˆµ) = −β < 0, we can ﬁnd ˆµk by a bisectioning
method such that h(ˆµk) = 0, and further the solution ˆx(ˆµk) of problem
(9.6.22)–(9.6.24).
By back-substituting ˆx(ˆµk), we obtain the new iterate
xk+1, i.e.,
xk+1 = T −1
k
ˆx(ˆµk) = Dkˆx(ˆµk)[n]
ˆx(ˆµk)n+1
,
(9.6.35)
where ˆx(ˆµk)[n] = (ˆx(ˆµk)1, · · · , ˆx(ˆµk)n)T .
The interior ellipsoid algorithm for solving convex quadratic programming
problems is introduced as follows.
Algorithm 9.6.1 (Interior Ellipsoid Method for Convex QP)
Step 1. Given a strict interior point x1 of (9.6.3)–(9.6.5); k := 1.
Step 2. Solve the subproblem (9.6.22)–(9.6.24) for ˆx(ˆµk); and com-
pute xk+1 by (9.6.35).
Step 3. If xk+1 is a KKT point, stop;
k := k + 1, go to Step 2.
2
The further details of interior ellipsoid methods for convex quadratic pro-
gramming can be found in Ye and Tse (1989).

9.7. PRIMAL-DUAL INTERIOR-POINT METHODS
445
9.7
Primal-Dual Interior-Point Methods
The primal-dual interior-point method for linear programming can be applied
to convex quadratic programming through a simple extension of the method.
Since we have not discussed the topic linear programming in the book, we
ﬁrst outline this method for linear programming.
Consider the linear programming problem in standard form
minx∈Rn
cT x
s.t.
Ax = b,
(9.7.1)
x ≥0,
where c and x are vectors in Rn, b is a vector in Rm, and A is an m × n
matrix. The dual problem for (9.7.1) is
maxλ∈Rm
bT λ
s.t.
AT λ + s = c,
(9.7.2)
s ≥0,
where λ is a vector in Rm and s is a vector in Rn. The primal-dual solution
of (9.7.1) and (9.7.2) are characterized by the Karush-Kuhn-Tucker (KKT)
conditions:
AT λ + s = c,
(9.7.3)
Ax = b,
(9.7.4)
xisi = 0,
i = 1, · · · , n
(9.7.5)
(x, s) ≥0,
(9.7.6)
where vectors λ and s are Lagrange multipliers for the constraints Ax = b
and x ≥0, respectively.
Primal-dual interior-point methods ﬁnd primal-dual solutions (x∗, λ∗, s∗)
of KKT system by applying variants of Newton’s method to the three equality
conditions (9.7.3)–(9.7.5) of this system and modifying the search directions
and steplength so that the inequalities (x, s) ≥0 are satisﬁed strictly at every
iteration.
To derive primal-dual interior-point methods, we restate the KKT con-
ditions (9.7.3)–(9.7.6) in a slightly diﬀerent form by means of a mapping

446
CHAPTER 9. QUADRATIC PROGRAMMING
F : R2n+m →R2n+m:
F(x, λ, s) =
⎡
⎢⎣
AT λ + s −c
Ax −b
XSe
⎤
⎥⎦= 0,
(9.7.7)
(x, s) ≥0,
(9.7.8)
where
X = diag(x1, x2, · · · , xn),
S = diag(s1, s2, · · · , sn),
and e = (1, 1, · · · , 1)T . Note that F is actually linear in its ﬁrst two terms
Ax −b, AT λ + s −c, and only mildly nonlinear in the remaining term XSe.
Primal-dual interior-point methods generate iterates (xk, λk, sk) that sat-
isfy the bound (9.7.8) strictly, that is, xk > 0 and sk > 0. This property is
the origin of the term interior-point. By respecting these bounds, the meth-
ods avoid spurious solutions, which are points that satisfy F(x, λ, s) = 0 but
not (x, s) ≥0.
Newton’s method forms a linear model of F around the current point and
obtains the search direction (∆x, ∆λ, ∆s) by solving the following system of
linear equations:
J(x, λ, s)
⎡
⎢⎣
∆x
∆λ
∆s
⎤
⎥⎦= −F(x, λ, s),
(9.7.9)
where J is the Jacobian of F. If the current point is strictly feasible, the
Newton step equations become
⎡
⎢⎣
0
AT
I
A
0
0
S
0
X
⎤
⎥⎦
⎡
⎢⎣
∆x
∆λ
∆s
⎤
⎥⎦=
⎡
⎢⎣
0
0
−XSe
⎤
⎥⎦.
(9.7.10)
Note that a full step along this direction usually is not permissible, since it
would violate the bound (x, s) ≥0. To avoid this diﬃculty, we perform a
line search along the Newton direction so that the new iterate is
(x, λ, s) + α(∆x, ∆λ, ∆s)
(9.7.11)
for some line search parameter α ∈(0, 1]. Unfortunately, we often can take
only a small step along the direction (α ≪1) before violating the condition
(x, s) > 0. Hence the pure Newton direction (9.7.10) often does not allow us
to make much progress toward a solution.

9.7. PRIMAL-DUAL INTERIOR-POINT METHODS
447
In the following, we give the central path technique which modiﬁes the
basic Newton procedure.
The Central Path
The central path C is an arc of strictly feasible points that plays a vital
role in primal-dual algorithms. It is parametrized by a scalar τ > 0, and each
point (xτ, λτ, sτ) ∈C solves the following system:
AT λ + s = c,
(9.7.12)
Ax = b,
(9.7.13)
xisi = τ,
i = 1, 2, · · · , n,
(9.7.14)
(x, s) > 0.
(9.7.15)
These conditions diﬀer from KKT conditions only in the term τ on the right-
hand side of (9.7.14). Instead of the complementarity condition (9.7.5), we
require that the pairwise product xisi have the same value τ for all indices
i. From (9.7.12)–(9.7.15), we can deﬁne the central path as
C = {(xτ, λτ, sτ) | τ > 0}.
It can be shown that (xτ, λτ, sτ) is deﬁned uniquely for each τ > 0 if and
only if the strictly feasible set Fo deﬁned by
Fo = {(x, λ, s) | Ax = b, AT λ + s = c, (x, s) > 0}
is nonempty. Hence the entire path C is well deﬁned.
Another way of deﬁning C is to use the mapping F deﬁned in (9.7.7) and
write
F(xτ, λτ, sτ) =
⎡
⎢⎣
0
0
τe
⎤
⎥⎦,
(xτ, sτ) > 0.
(9.7.16)
The equations (9.7.12)–(9.7.15) approximate (9.7.3)–(9.7.6) more and more
closely as τ goes to zero. If C converges to anything as τ ↓0, it must con-
verge to a primal-dual solution of the linear program. The central path thus
guides us to a solution along a route that steers clear of spurious solutions by
keeping all the pairwise products xisi strictly positive and decreasing them
to zero at the same rate.
Primal-dual interior-point algorithms take Newton steps toward points on
C for which τ > 0, rather than pure Newton steps for F. Since these steps are

448
CHAPTER 9. QUADRATIC PROGRAMMING
biased toward the interior of the nonnegative orthant deﬁned by (x, s) ≥0,
it usually is possible to take longer steps along them than along the pure
Newton steps for F before violating the positivity condition. To describe the
biased search direction, we introduce a centering parameter σ ∈[0, 1] and a
duality measure µ deﬁned by
µ = 1
n
n

i=1
xisi = xT s
n ,
(9.7.17)
which measures the average value of the pairwise product xisi. By writing
τ = σµ and applying Newton’s method to the system (9.7.16), we obtain
⎡
⎢⎣
0
AT
I
A
0
0
S
0
X
⎤
⎥⎦
⎡
⎢⎣
∆x
∆λ
∆s
⎤
⎥⎦=
⎡
⎢⎣
0
0
−XSe + σµe
⎤
⎥⎦.
(9.7.18)
The step (∆x, ∆λ, ∆s) is a Newton step toward the point (xσµ, λσµ, sσµ) ∈C,
at which the pairwise product xisi are all equal to σµ. In contrast, the step
(9.7.10) aims directly for the point at which the KKT conditions (9.7.3)–
(9.7.6) are satisﬁed.
If σ = 1, the equations (9.7.18) deﬁne a centering direction, a Newton step
toward the point (xµ, λµ, sµ) ∈C. If σ = 0, the (9.7.18) gives the standard
Newton step.
In the following, we deﬁne a general framework of primal-dual interior-
point algorithm.
Algorithm 9.7.1 (A Primal-Dual Interior-Point Framework)
Given (x0, λ0, s0) ∈Fo.
For k = 0, 1, 2, · · · ,
solve
⎡
⎢⎣
0
AT
I
A
0
0
Sk
0
Xk
⎤
⎥⎦
⎡
⎢⎣
∆xk
∆λk
∆sk
⎤
⎥⎦=
⎡
⎢⎣
0
0
−XkSke + σkµke
⎤
⎥⎦,
where σk ∈[0, 1] and µk = (xk)T sk/n;
set
(xk+1, λk+1, sk+1) = (xk, λk, sk) + αk(∆xk, ∆λk, ∆sk),
choosing αk such that (xk+1, sk+1) > 0.

9.7. PRIMAL-DUAL INTERIOR-POINT METHODS
449
end(For).
2
For most problems, however, a strictly feasible starting point (x0, λ0, s0)
is diﬃcult to ﬁnd. Infeasible interior-point methods require only that the
components of x0 and s0 be strictly positive. Therefore, we give a slight
change to the equation (9.7.18). If we deﬁne the residuals for the two linear
equations as
rb = Ax −b,
rc = AT λ + s −c,
the modiﬁed step equation is
⎡
⎢⎣
0
AT
I
A
0
0
S
0
X
⎤
⎥⎦
⎡
⎢⎣
∆x
∆λ
∆s
⎤
⎥⎦=
⎡
⎢⎣
−rc
−rb
−XSe + σµe
⎤
⎥⎦.
(9.7.19)
Primal-Dual Interior-Point Methods for Convex Quadratic Pro-
gramming
Now we return to convex quadratic programming. Let us discuss convex
quadratic programming with inequality constraints:
minx∈Rn
q(x) def
= 1
2xT Gx + xT g
(9.7.20)
s.t.
Ax ≥b,
(9.7.21)
where g ∈Rn, b ∈Rm, A ∈Rm×n and G ∈Rn×n is symmetric and positive
semideﬁnite. The KKT conditions of (9.7.20)–(9.7.21) state as follows: If
x∗is a solution of (9.7.20)–(9.7.21), there is a Lagrange multiplier vector λ∗
such that the following conditions are satisﬁed for (x, λ) = (x∗, λ∗):
Gx −AT λ + g = 0,
(9.7.22)
Ax −b ≥0,
(9.7.23)
(Ax −b)iλi = 0,
i = 1, 2, · · · , m,
(9.7.24)
λ ≥0.
(9.7.25)
By introducing the slack vector y = Ax −b, we have
Gx −AT λ + g = 0,
(9.7.26)
Ax −y −b = 0,
(9.7.27)
yiλi = 0,
i = 1, 2, · · · , m,
(9.7.28)
(y, λ) ≥0.
(9.7.29)

450
CHAPTER 9. QUADRATIC PROGRAMMING
As in the case of linear programming, here the KKT conditions are not
only necessary but also suﬃcient, because problem (9.7.20)–(9.7.21) is convex
programming. Hence we can solve it by ﬁnding solutions of system (9.7.26)–
(9.7.29). As discussed above, we apply modiﬁcations of Newton’s method to
this system. We can deﬁne
F(x, y, λ) =
⎡
⎢⎣
Gx −AT λ + g
Ax −y −b
Y Λe
⎤
⎥⎦,
(y, λ) ≥0,
(9.7.30)
where
Y = diag(y1, y2, · · · , ym),
Λ = diag(λ1, λ2, · · · , λm),
e = (1, 1, · · · , 1)T .
Given a current iterate (x, y, λ) that satisﬁes (y, λ) > 0, we can deﬁne a
duality measure µ by
µ = 1
m
m

i=1
yiλi = yT λ
m .
(9.7.31)
The central path C is the set of points (xτ, yτ, λτ)(τ > 0) satisfying
F(xτ, yτ, λτ) =
⎡
⎢⎣
0
0
τe
⎤
⎥⎦,
(yτ, λτ) > 0.
(9.7.32)
The generic step (∆x, ∆y, ∆λ) is a Newton-type step toward the point (xσµ, yσµ, λσµ) ∈
C. As in (9.7.19), this step satisﬁes the following system:
⎡
⎢⎣
G
−AT
0
A
0
−I
0
Y
Λ
⎤
⎥⎦
⎡
⎢⎣
∆x
∆y
∆λ
⎤
⎥⎦=
⎡
⎢⎣
−rg
−rb
−ΛSe + σµe
⎤
⎥⎦,
(9.7.33)
where
rg = Gx −AT λ + g,
rb = Ax −y −b.
So, we obtain the next iterate
(x+, y+, λ+) = (x, y, λ) + α(∆x, ∆y, ∆λ),
(9.7.34)
where α is chosen so that (y+, λ+) > 0.
For mode details of primal-dual interior-point methods for convex quadratic
programming, please consult Wright [358].

9.7. PRIMAL-DUAL INTERIOR-POINT METHODS
451
Exercises
1. Let H = Diag(h11, h22, ..., hnn) be a positive deﬁnite diagonal matrix.
Find the minimizer of (9.1.1) subject to the condition ∥x∥∞≤1.
2. Prove Theorem 9.1.1.
3. Prove that problem (9.2.8)–(9.2.10) is the dual of problem (9.1.1)–
(9.1.3).
4. Solve the dual of the problem
min
(x2
1 + x2
2)/2 + x1
s.t.
x1 ≥0.
5. If f(x) is a convex function and ci(x)(i = 1, · · · , m) are concave func-
tions, the problem
min
f(x)
s.t.
ci(x) ≥0
is called a convex programming problem. Generalize the dual theory in Sec-
tion 9.2 from convex quadratic programming to general convex programming.
6. Find the smallest circle in the plane that contains the points (1, −4),
(−2, −2), (−4, 1) and (4, 5). Formulate the problem as a convex program-
ming problem, then solve the dual.
7. Assume that B ∈Rn×n is positive deﬁnite, A ∈Rm×n, g ∈Rn and
b ∈Rm. Give the dual problem of the following QP:
min
gT x + 1
2xT Bx
s.t.
Ax = b.
8. Solve the equality constraint QP problem
min

1
−1
T
x + 1
2xT

1
2
2
4

x
s.t.
x1 + x2 = 1.

452
CHAPTER 9. QUADRATIC PROGRAMMING
9. 1) Check that if set V =

0
In−m

in (9.3.42), the choice (9.3.37)–
(9.3.38) can be obtained.
2) Check that if set V = Q2 in (9.3.42), the choice (9.3.40) can be ob-
tained.
10. Show that if A ∈Rn×m has full column-rank and ZT GZ is positive
deﬁnite, then KKT matrix (9.3.47) is nonsingular.
11. Show (9.3.53)–(9.3.55).
12. Show (9.3.58)–(9.3.60).
13. Assume A ∈Rm×n has full row rank. Let Z ∈Rn×(n−m) be any full
column rank matrix such that AZ = 0. Prove that the matrix

B
AT
A
0

(9.7.35)
is nonsingular if and only if ZT BZ is nonsingular.
14. Use the active set method to solve the problem
min
−1000x1 −1000x2 + x2
1 + x2
2
s.t.
3x1 + x2 ≥3,
x1 + 4x2 ≥4,
x1 ≥0,
x2 ≥0.
Illustrate the result by sketching the set of feasible solutions.
15. Program Algorithm 9.4.2 and use it to solve
min
x2
1 + 2x2
2 −2x1 −6x2 −2x1x2
s.t.
1
2x1 + 1
2x2 ≤1,
−x1 + x2 ≤2,
x1, x2 ≥0.

9.7. PRIMAL-DUAL INTERIOR-POINT METHODS
453
16. Try to give the primal-dual interior-point algorithm for convex quadratic
programming (9.7.20)–(9.7.21).

Chapter 10
Penalty Function Methods
10.1
Penalty Function
The penalty function methods are an important class of methods for con-
strained optimization problem
minx∈Rn
f(x)
(10.1.1)
s.t.
ci(x) = 0, i ∈E
Def
= {1, · · · , me},
(10.1.2)
ci(x) ≥0, i ∈I
Def
= {me + 1, · · · , m}.
(10.1.3)
In this class of methods we replace the original constrained problem by a
sequence of unconstrained subproblems that minimizes the penalty functions.
The penalty function is a function with penalty property
P(x) = ¯P(f(x), c(x)),
(10.1.4)
constructed from the objective function f(x) and the constraints c(x). The
so-called “penalty” property requires P(x) = f(x) for all feasible points x ∈
X of (10.1.1)–(10.1.3), and P(x) is much larger than f(x) when the constraint
violations are severe.
To describe the degree of constraint violation, we deﬁne the constraint
violation function c(−)(x) = (c(−)
1
(x), · · · , c(−)
m (x))T as follows:
c(−)
i
(x)
=
ci(x), i ∈E,
(10.1.5)
c(−)
i
(x)
=
min{ci(x), 0}, i ∈I.
(10.1.6)

456
CHAPTER 10. PENALTY FUNCTION METHODS
Deﬁne
C = {ci(x) | ci(x) = 0, i ∈E; ci(x) ≥0, i ∈I}.
(10.1.7)
Obviously, x is a feasible point if and only if c(x) ∈C. Furthermore,
if ci(x) ≥0, i.e., ci(x) ∈C, then c(−)
i
(x) = 0;
if ci(x) < 0, i.e., ci(x) /∈C, then c(−)
i
(x) ̸= 0.
This means, for each constraint, the constraint violation function is nonzero
when the corresponding constraint is violated and zero when the correspond-
ing constraint is feasible.
It is not diﬃcult to see that for any x ∈Rn, we have
∥c(−)(x)∥2 = dist (c(x), C),
(10.1.8)
where dist(·, ·) denotes the distance from a point to a set and is deﬁned as
dist(x, Y ) = min{∥x −y∥2 | ∀y ∈Y }.
(10.1.9)
The penalty function consists of a sum of the original objective function
and a penalty term, i.e.,
P(x) = f(x) + h(c(−)(x)),
(10.1.10)
where the penalty term h(c(−)(x)) is a function deﬁned on Rm and satisﬁes
h(0) = 0,
lim
∥c∥→+∞h(c) = +∞.
(10.1.11)
The earliest penalty function is the Courant penalty function, or called
the quadratic penalty function, deﬁned as
P(x) = f(x) + σ∥c(−)(x)∥2
2,
(10.1.12)
where σ > 0 is a positive constant, which is called the penalty parameter.
We give an example to describe the penalty function.
min
x
s.t.
x −2 ≥0.
(10.1.13)

10.1. PENALTY FUNCTION
457
Then
h(c(−)(x)) = ∥c(−)(x)∥2
2 = [min{0, x −2}]2 =

0
if x ≥2,
(x −2)2
if x < 2.
Note that the minimum of f(x) + σ∥c(−)(x)∥2 occurs at the point 2 −1
σ, and
approaches the minimum point ¯x = 2 of the original problem, as σ approaches
∞.
Obviously, (10.1.12) is a particular case of (10.1.10) in which h(c) =
σ∥c∥2
2. In fact, for any norm on Rm and any α > 0, the function h(c) = σ∥c∥α
satisﬁes (10.1.11). So, a class of penalty functions can be deﬁned as:
P(x) = f(x) + σ∥c(−)(x)∥α,
(10.1.14)
where σ > 0 is a penalty parameter, α > 0, and ∥· ∥is some norm on Rm.
Typically, (10.1.12) is often written as
P(x)
=
f(x) + 1
2σ∥c(−)(x)∥2
(10.1.15)
=
f(x) + 1
2σ
me

i=1
c2
i (x) + 1
2σ
m

i=me+1
[c(−)
i
(x)]2
(10.1.16)
and is called the quadratic penalty function, where σ > 0 and c(−)
i
(x) =
min{0, ci(x)}.
Besides (10.1.12), the common particular forms of (10.1.14) are
P1(x) = f(x) + σ∥c(−)(x)∥1
(10.1.17)
and
P∞(x) = f(x) + σ∥c(−)(x)∥∞,
(10.1.18)
which are called L1 penalty function and L∞penalty function respectively.
If the penalty function takes values approaching +∞as x approaches the
boundary of the feasible region, it is called the interior point penalty function.
The interior point penalty function is suitable only to inequality-constrained
problems, i.e., me = 0. Typically, the two most important interior point
penalty functions are the inverse barrier function
P(x) = f(x) + 1
σ
m

i=1
1
ci(x)
(10.1.19)

458
CHAPTER 10. PENALTY FUNCTION METHODS
and the logarithmic barrier function
P(x) = f(x) −1
σ
m

i=1
log ci(x).
(10.1.20)
If given an initial point in the interior of the feasible region, the whole se-
quence generated by the interior point penalty function method is interior
points. Since these functions set an inﬁnitely high “barrier” on the boundary,
they are also said to be barrier functions.
Let x∗be a KKT point of constrained optimization (10.1.1)–(10.1.3).
Then it follows from (10.1.12) that ∇P(x∗) = ∇f(x∗). In general, x∗is not
a stationary point of the Courant penalty function, and the penalty function
method attempts to create a local minimizer at x∗in the limit σk →∞. To
overcome this shortcoming, we introduce parameters θi (i = 1, · · · , m) with
θi ≥0 (i = me + 1, · · · , m) to change the origin of the penalty term. Write
θ = (θ1, · · · , θm)T . Modifying (10.1.12) gives
P(x)
= f(x) +
m

i=1
σi
2
 
(c(x) −θ)(−)
i
2 −θ2
i
!
= f(x) +
me

i=1

−λici(x) + 1
2σi(ci(x))2

+
m

i=me+1

−λici(x) + 1
2σi(ci(x))2,
if
ci(x) < λi
σi ;
−1
2λ2
i /σi,
otherwise
(10.1.21)
where
λi = σiθi, i = 1, · · · , m.
(10.1.22)
Because the penalty function (10.1.21) can be obtained from Lagrange func-
tion (8.2.18) by adding a penalty term, (10.1.21) is referred to as an aug-
mented Lagrangian function. Alternatively, (10.1.21) can be also obtained
from the penalty function (10.1.12) by adding a multiplier term −λT c, (10.1.21)
is also called a multiplier penalty function.
Let x∗be a KKT point of
the constrained optimization problem, and λ∗
i (i = 1, · · · , m) correspond-
ing Lagrange multipliers.
Then the augmented Lagrangian function with
λ∗
i (i = 1, · · · , m) satisﬁes ∇P(x∗) = 0. In addition, the Lagrange multiplier
λ∗is not known in advance, so the augmented Lagrangian function method
needs to update λi (i = 1, · · · , m) successfully.
For equality-constrained problem (me = m), we deﬁne
λ(x) = (A(x))+g(x),
(10.1.23)

10.1. PENALTY FUNCTION
459
where A(x) = (∇c1(x), · · · , ∇cm(x)) is an n × m matrix, g(x) = ∇f(x), A+
denotes the generalized inverse of A, and the multiplier λ(x) is the minimum
l2 norm solution of the least-squares problem
min
λ∈Rm
∇f(x) −
m

i=1
λi∇ci(x)

2
2
.
(10.1.24)
By using (10.1.23), we can give Fletcher’s smooth exact penalty func-
tion (or Fletcher’s augmented Lagrangian function) for the case where only
equality constraints are present in (10.1.1)–(10.1.3) as follows:
P(x) = f(x) −λ(x)T c(x) + 1
2
m

i=1
σi(ci(x))2,
(10.1.25)
where σi > 0 (i = 1, · · · , m) are penalty parameters.
Let x∗be the solution of the equality-constrained problem, A(x∗) have
full column rank,
∇xP(x∗)
=
g(x∗) −A(x∗)λ∗= 0,
∇2
xxP(x∗)
=
W ∗+ A(x∗)A(x∗)+W ∗
+W ∗A(x∗)A(x∗)+ + A(x∗)DA(x∗)T ,
(10.1.26)
where D = diag(σ1, · · · , σm) and
W ∗= ∇2f(x∗) −
m

i=1
λ∗
i ∇2ci(x∗) = ∇2
xxL(x∗, λ∗).
Lemma 10.1.1 Let H ∈Rn×n be symmetric and A ∈Rn×m. If
dT Hd > 0
(10.1.27)
for any nonzero vector d with AT d = 0, then there exists σ ≥0 such that
H + σAAT
(10.1.28)
is a positive deﬁnite matrix.
Proof.
By assumption, there is δ > 0, such that if ∥AT d∥2 ≤δ and
∥d∥2 = 1, then (10.1.27) holds. Consider
min
∥AT d∥2≥δ
∥d∥2=1
dT Hd
∥AT d∥2
2
.
(10.1.29)

460
CHAPTER 10. PENALTY FUNCTION METHODS
Since the set {d | ∥d∥2 = 1, ∥AT d∥2 ≥δ} is a ﬁnite and closed set, then
the minimum of (10.1.29) is achieved.
Hence there is η such that for all
d ∈{d | ∥d∥2 = 1, ∥AT d∥2 ≥δ} we have
dT Hd
∥AT d∥2
2
> η.
(10.1.30)
Set σ = max{−η, 0}. Therefore, for any d with ∥d∥2 = 1, we have
dT (H + σAAT )d > 0.
(10.1.31)
We complete the proof.
2
If the second-order suﬃcient condition
dT W ∗d > 0, ∀d ̸= 0, (A∗)T d = 0
(10.1.32)
holds, then it follows from the above lemma that there exists ¯σ ≥0 such that
for all σi ≥¯σ, the matrix ∇2
xxP(x∗) deﬁned by (10.1.26) is positive deﬁnite.
Therefore the penalty function (10.1.25) is said to be exact if the solution
x∗of the original problem is also the strict local minimizer of the penalty
function P(x∗).
If Lagrange multipliers λi of the augmented Lagrangian function are the
Lagrange multipliers λ∗
i at the solution x∗of the problem, then under the
second-order suﬃcient condition (10.1.32), x∗is also the local minimizer of
the augmented Lagrangian function (10.1.21) when σ is large enough. Thus,
the augmented Lagrangian function is also an exact penalty function.
For an L1 penalty function, if
σ > ∥λ∗∥∞,
(10.1.33)
then under the second-order suﬃcient condition (10.1.32), the solution x∗of
the original problem is also a strict local minimizer of the L1 penalty function.
Thus, the L1 penalty function is referred to as an L1 exact penalty function.
Similarly, an L∞penalty function is also exact.
Note that the KKT point of the constrained optimization problem is not,
in general, the stationary point of the Courant penalty function. Thus, the
Courant penalty function is not an exact penalty function.
In this chapter, we will describe the simple penalty function method,
interior point penalty function method (i.e., barrier function method), mul-
tiplier penalty function method, smooth exact penalty function method and
non-smooth exact penalty function method.

10.2. THE SIMPLE PENALTY FUNCTION METHOD
461
10.2
The Simple Penalty Function Method
The penalty function method is an approach to minimize a sequence of
penalty functions and obtain the minimizer of the original constrained opti-
mization problem.
Consider the simple penalty function
Pσ(x) = f(x) + σ∥c(−)(x)∥α,
(10.2.1)
where σ > 0 is the penalty parameter, α > 0 a positive constant, and ∥· ∥a
given norm on Rm. Write x(σ) as a solution of problem
min
x∈Rn Pσ(x).
(10.2.2)
Next, we ﬁrst give some lemmas.
Lemma 10.2.1 Let 0 < σ1 < σ2. Then
Pσ1(x(σ1))
≤
Pσ2(x(σ2)),
(10.2.3)
f(x(σ1))
≤
f(x(σ2)),
(10.2.4)
∥c(−)(x(σ1))∥
≥
∥c(−)(x(σ2))∥.
(10.2.5)
Proof.
From the deﬁnition of x(σ), we have
Pσ1(x(σ1)) ≤Pσ1(x(σ2)) ≤Pσ2(x(σ2)) ≤Pσ2(x(σ1)),
(10.2.6)
which shows (10.2.3). By use of (10.2.6) and (10.2.1), we have
0
≤
Pσ1(x(σ2)) −Pσ2(x(σ2)) −[Pσ1(x(σ1)) −Pσ2(x(σ1))]
=
(σ1 −σ2)[∥c(−)(x(σ2))∥α −∥c(−)(x(σ1))∥α],
(10.2.7)
which means that (10.2.5) holds. Using (10.2.6) and (10.2.5) gives
f(x(σ1))
≤
f(x(σ2)) + σ1(∥c(−)(x(σ2))∥α −∥c(−)(x(σ1))∥α)
≤
f(x(σ2)).
(10.2.8)
Hence (10.2.4) holds. We complete the proof.
2
Lemma 10.2.2 Let δ = ∥c(−)(x(σ))∥.
Then x(σ) is also the solution of
problem
minx∈Rn
f(x)
(10.2.9)
s.t.
∥c(−)(x)∥≤δ.
(10.2.10)

462
CHAPTER 10. PENALTY FUNCTION METHODS
Proof.
For any x satisfying (10.2.10), we have
0
≤
σ(∥c(−)(x(σ))∥α −∥c(−)(x)∥α)
=
Pσ(x(σ)) −f(x(σ)) −Pσ(x) + f(x)
=
[Pσ(x(σ)) −Pσ(x)] + f(x) −f(x(σ))
≤
f(x) −f(x(σ)).
(10.2.11)
Hence, for any x satisfying (10.2.10), we have
f(x) ≥f(x(σ)),
(10.2.12)
which shows that x(σ) is the solution of (10.2.9)–(10.2.10).
2
By the deﬁnition of the constraint violation function c(−)(x), the original
problem (10.1.1)–(10.1.3) can be written equivalently as
minx∈Rn
f(x),
(10.2.13)
s.t.
∥c(−)(x)∥= 0.
(10.2.14)
Hence, if δ is suﬃciently small, the problem (10.2.9)–(10.2.10) can be re-
garded as an approximation of (10.2.13)–(10.2.14), and so x(σ) can be re-
garded as the approximate solution of the original problem. In fact, from
Lemma 10.2.2, we know that when c(−)(x(σ)) = 0, x(σ) is just the solution
of problem (10.1.1)–(10.1.3).
The basic idea of the penalty function method is that the penalty param-
eter σ is increased in each iteration until ∥c(−)(x(σ))∥is smaller than a given
tolerance. Below we give a penalty function method with the simple penalty
function.
Algorithm 10.2.3 (Simple Penalty Function Method)
Step 1. Given x1 ∈Rn, σ1 > 0, ϵ ≥0, k := 1.
Step 2. Find a solution x(σk) of
min
x∈Rn Pσk(x),
(10.2.15)
starting at xk.
Step 3. If ∥c(−)(x(σk))∥≤ϵ, stop;
Set xk+1 = x(σk), σk+1 = 10σk;
k := k + 1, turn to Step 2.
2

10.2. THE SIMPLE PENALTY FUNCTION METHOD
463
Note that the parameter {σk} can be chosen ﬂexibly and adoptively. It
means that you can choose σk+1 = 10σk or σk+1 = 2σk, which depends on
the diﬃculty of minimizing the penalty function at iteration k.
Now we discuss the convergence property of Algorithm 10.2.3.
Theorem 10.2.4 Suppose that the tolerance ϵ in Algorithm 10.2.3 satisﬁes
ϵ > min
x∈Rn ∥c(−)(x)∥,
(10.2.16)
then the algorithm must terminate ﬁnitely.
Proof.
Suppose, by contradiction, that the theorem is not true. Then
there must exist σk →+∞and for all k,
∥c(−)(x(σk))∥> ϵ.
(10.2.17)
From (10.2.16), there exists ˆx ∈Rn such that
∥c(−)(ˆx)∥< ϵ.
(10.2.18)
By use of the deﬁnition of x(σ) and (10.2.4), we have
f(ˆx) + σk∥c(−)(ˆx)∥α
≥
f(x(σk)) + σk∥c(−)(x(σk))∥α
≥
f(x(σ1)) + σk∥c(−)(x(σk))∥α. (10.2.19)
By arranging (10.2.19) and taking the limit as σk →+∞, we obtain that
∥c(−)(ˆx)∥α −∥c(−)(x(σk))∥α
≥
1
σk
[f(x(σ1)) −f(ˆx)] →0,
(10.2.20)
which contradicts (10.2.17)–(10.2.18). This completes the proof.
2
Theorem 10.2.5 If Algorithm 10.2.3 does not terminate ﬁnitely, then
min
x∈Rn ∥c(−)(x)∥≥ϵ
(10.2.21)
and
lim
k→∞∥c(−)(x(σk))∥= min
x∈Rn ∥c(−)(x)∥,
(10.2.22)
and any accumulation point x∗of {x(σk)} is the solution of problem
min
x∈Rn f(x),
(10.2.23)
s.t. ∥c(−)(x)∥= min
y∈Rn ∥c(−)(y)∥.
(10.2.24)

464
CHAPTER 10. PENALTY FUNCTION METHODS
Proof.
Suppose that the algorithm does not terminate ﬁnitely. It follows
from Theorem 10.2.4 that (10.2.21) holds. Since σk →+∞and by (10.2.20),
we have that for given ˆx ∈Rn,
lim inf
k→∞

∥c(−)(ˆx)∥α −∥c(−)(x(σk))∥α
≥0
(10.2.25)
which concludes (10.2.22).
Let x∗be any accumulation point of {x(σk)}. By (10.2.22), x∗must be
feasible point of (10.3.25). If x∗is not the solution of (10.2.23)-(10.3.25),
there exists ¯x such that
f(¯x) < f(x∗)
(10.2.26)
and
∥c(−)(¯x)∥= min
y∈Rn ∥c(−)(y)∥.
(10.2.27)
It follows from Lemma 10.2.1 that f(x(σk)) approach to f(x∗). Then, by
(10.2.26), we have that the inequality
f(¯x) < f(x(σk))
(10.2.28)
holds for k suﬃciently large, which, together with (10.2.27), gives
f(¯x) + σk∥c(−)(¯x)∥
<
f(x(σk)) + σk min
y
∥c(−)(y)∥
= f(x(σk)) + σk∥c(−)(x(σk))∥
(10.2.29)
for k suﬃciently large. This is just
Pσk(¯x) < Pσk(x(σk)).
(10.2.30)
This contradicts the deﬁnition of x(σk). The contradiction proves our theo-
rem.
2
The above two theorems establish a direct consequence.
Corollary 10.2.6 Let problem (10.1.1)-(10.1.3) have feasible points. Then
Algorithm 10.2.3 either ﬁnitely terminates at the solution of (10.2.9)-(10.2.10),
or any accumulation points of the generated sequence are the solution of the
original problem.

10.2. THE SIMPLE PENALTY FUNCTION METHOD
465
For the Courant penalty function, i.e., ∥· ∥2 and α = 2 in (10.2.1), we
have
∇f(x(σk)) + 2σk
m

i=1
c(−)
i
(x(σk))∇c(−)
i
(x(σk)) = 0.
(10.2.31)
Suppose that the inﬁnite sequence {xk} from Algorithm 10.2.3 converges to
x∗, we then have
∇f(xk+1) =
m

i=1
λ(k+1)
i
∇ci(xk+1),
(10.2.32)
where
λ(k+1)
i
= −2σkc(−)
i
(xk+1).
(10.2.33)
Hence the multiplier λ(k+1) given in (10.2.33) is an approximation to a La-
grangian multiplier. It is not diﬃcult to see that if xk →x∗, c(−)(x∗) = 0
and ∇ci(x∗) (i ∈E ∪I(x∗)) are linearly independent, then λk →λ∗. Since,
in the general case, ∥λ∗∥2 ̸= 0, it follows from (10.2.33) that
1
σk
= O(∥c(−)(xk+1)∥2) = O(∥xk+1 −x∗∥2).
(10.2.34)
On the other hand, by using (10.2.32), c(−)(x∗) = 0 and ∥λk+1 −λ∗∥=
O(∥xk+1 −x∗∥), we can obtain

W ∗
−A∗
−(A∗)T
0
 
xk+1 −x∗
ˆλ(k+1) −ˆλ∗

=

0
ˆc(xk+1)

+ O(∥xk+1 −x∗∥),
(10.2.35)
where
W ∗= ∇∗
xxL(x∗, λ∗),
(10.2.36)
A∗is a matrix consisting of ∇ci(x∗) (i ∈E or λ∗
i > 0), ˆλ∗is a vector consisting
of those components of λ∗(i ∈E or λ∗
i > 0), and the deﬁnitions of ˆλk+1 and
ˆc(xk+1) are similar to ˆλ∗. By (10.2.33), we have
∥ˆc(xk+1)∥= O
 1
σk
!
.
(10.2.37)
If the second suﬃcient conditions (8.3.35)-(8.3.36) are satisﬁed, then the
matrix

W ∗
−A∗
−(A∗)T
0

(10.2.38)

466
CHAPTER 10. PENALTY FUNCTION METHODS
is nonsingular. By use of (10.2.35) and (10.2.37), we have
∥xk+1 −x∗∥= O
 1
σk
!
.
(10.2.39)
Then, the above equality and (10.2.34) implies that the rate of ∥xk+1−x∗∥→
0 is the same as that of
1
σk →0. This phenomenon can be illustrated by the
following example.
Example 10.2.7 Consider the problem
min(x1,x2)∈R2
x1 + x2,
(10.2.40)
s.t.
x2 −x2
1 = 0.
(10.2.41)
Solution.
For the Courant penalty function, we have
x(σ) =

−1
2
1
4 −1
2σ

= x∗−

0
1
2

1
σ,
(10.2.42)
where x∗= (−1/2, 1/4) is the unique solution of (10.2.40)-(10.2.41). Thus,
the sequence {xk} generated by Algorithm 10.2.3 satisﬁes
xk+1 −x∗=

0
−1
2

1
σk
.
(10.2.43)
Therefore, we need to choose a very large penalty factor σk to solve con-
strained optimization problems. However, this leads to numerical diﬃculties
of ill-conditioning.
2
If L1 or L∞penalty functions are employed, in general, Algorithm 10.2.3
terminates ﬁnitely at an exact solution of the original problem. In fact, let
x∗be a solution of the original constrained problem, and λ∗a corresponding
Lagrange multiplier, then x∗is a minimizer of the L1 exact penalty function
if σ1 > ∥λ∗∥∞. Unfortunately, it is a nonsmooth optimization problem to
minimize a L1 exact penalty function.
The topic about minimization of
nonsmooth exact penalty function will be discussed in detail in §10.6.
10.3
Interior Point Penalty Functions
Similar to the penalty functions discussed in the previous sections, the inte-
rior point penalty functions are also used to transform a constrained problem

10.3. INTERIOR POINT PENALTY FUNCTIONS
467
into an unconstrained problem or into a sequence of unconstrained problems.
These functions set a barrier against leaving the feasible region, i.e., these
functions are characterized by the property of preserving strict constraint
feasibility at all times (i.e., the generated sequence always lies in the interior
of the feasible region), by using an interior point penalty term which is inﬁ-
nite on the constraint boundaries. Such methods based on an interior point
penalty function are referred to as interior point penalty function methods.
Note that the interior point penalty function is also said to be a barrier
function, and the corresponding techniques are known as barrier function
methods.
The interior point function methods are used to deal with the inequality-
constrained optimization problem
minx∈Rn
f(x)
(10.3.1)
s.t.
ci(x) ≥0, i = 1, · · · , m,
(10.3.2)
where
X = {x ∈Rn | ci(x) ≥0, i = 1, 2, · · · , m}
is a feasible region. The strictly feasible region is deﬁned by
int(X) ∆= {x ∈Rn | ci(x) > 0 for all i}.
(10.3.3)
We assume that intX is nonempty.
The interior point penalty function is of a general form
Pσ(x) = f(x) + 1
σ
m

i=1
h(ci(x)),
(10.3.4)
where σ > 0 is a barrier parameter, which controls the iteration. If {σk} →∞
is chosen, the barrier term becomes more and more negligible except close to
the boundary. The h(ci) is a real function deﬁned on (0, +∞) which satisﬁes
that
lim
ci→0+ h(ci) = +∞,
(10.3.5)
which means the value h(ci) approaches ∞as x approaches the boundary of
int(X), and that
h(c1) ≥h(c2), ∀c1 < c2.
(10.3.6)
Some interior point penalty functions satisfy
h(ci) > 0, ∀ci > 0.
(10.3.7)

468
CHAPTER 10. PENALTY FUNCTION METHODS
As we have seen, the inverse barrier function (10.1.19) and the logarithmic
barrier function (10.3.16) are the two most important special cases of (10.3.4).
For the inverse barrier function, (10.3.7) holds.
Let x(σ) be the solution of the problem
min
x∈Rn Pσ(x).
(10.3.8)
Assume that we solve (10.3.8) with a strict interior point as an initial point.
Note that Pσ(x) has value ∞on its boundary, then x(σ) must be an interior
point.
Similar to Lemma 10.2.1 and Lemma 10.2.2, we can prove the following
results.
Lemma 10.3.1 Let σ2 > σ1 > 0, then
f(x(σ2)) ≤f(x(σ1)),
(10.3.9)
m

i=1
h(ci(x(σ2))) ≥
m

i=1
h(ci(x(σ1))).
(10.3.10)
Lemma 10.3.2 Set δ = 
m
i=1 h(ci(x(σ))). Then x(σ) is a solution of prob-
lem
minx∈Rn
f(x)
(10.3.11)
s.t.
m

i=1
h(ci(x)) ≤δ.
(10.3.12)
When δ is suﬃciently large, the problem (10.3.11)-(10.3.12) can be re-
garded as an approximation to
minx∈Rn
f(x)
(10.3.13)
s.t.
m

i=1
h(ci(x)) < +∞.
(10.3.14)
By the deﬁnition of h(ci), (10.3.14) is equivalent to
ci(x) > 0, i = 1, · · · , m.
(10.3.15)
The diﬀerence between (10.3.15) and (10.3.2) is whether the boundary of the
feasible region is feasible points or not. If σ > 0 is very large and δ in Lemma

10.3. INTERIOR POINT PENALTY FUNCTIONS
469
10.3.2 is very large, then x(σ) is close to the boundary of the feasible region
of (10.3.2). Hence, the feasible region of (10.3.12) is also close to that of
the original problem. Therefore x(σ) is close to the solution of the original
problem.
If σ > 0 is very large but δ bounded, it follows from the deﬁnition of
(10.3.4)that the interior point penalty function Pσ(x) is very close to f(x)
near x(σ). In this case, x(σ) is regarded approximately as a local minimizer
of f(x). Based on these analyses, an algorithm can be written as follows. We
assume that h(·) satisﬁes (10.3.7).
Algorithm 10.3.3 (Algorithm based on interior point penalty function)
Step 1. Given x1 satisfying (10.3.15). Let σ1 > 0, ϵ ≥0, k := 1.
Step 2. Starting with xk solve the problem (10.3.8) for x(σk). Set
xk+1 = x(σk).
Step 3. If
1
σk
m

i=1
h(ci(xk+1)) ≤ϵ,
(10.3.16)
stop; otherwise, set σk+1 = 10σk, k := k + 1; go to Step 2.
2
For Algorithm 10.3.3, we will establish the following convergence theorem.
Theorem 10.3.4 Let f(x) be bounded below on the feasible region X. Then
Algorithm 10.3.3 will terminate ﬁnitely at ϵ > 0, and when it does not ter-
minate ﬁnitely, we have that
lim
k→∞
1
σk
h(ci(xk+1)) = 0
(10.3.17)
and
lim
k→∞f(xk) =
inf
x∈int(X)
f(x)
(10.3.18)
hold, where int(X) is deﬁned by (10.3.3). Furthermore, any accumulation
point of {xk} is the solution of problem (10.3.1)-(10.3.2).

470
CHAPTER 10. PENALTY FUNCTION METHODS
Proof.
Obviously, we only need to prove that (10.3.17)-(10.3.18) hold
when the algorithm does not terminate ﬁnitely.
First, for any η > 0, there exists xη ∈int(X) such that
f(xη) <
inf
x∈int(X)
f(x) + η
2.
(10.3.19)
Since the algorithm does not terminate ﬁnitely, there is σk →+∞. Hence
there exists ¯k such that
σk > 2
η
m

i=1
h(ci(xη)), ∀k ≥¯k.
(10.3.20)
Then, by using the deﬁnition of xk+1, and (10.3.19)-(10.3.20), we have
Pσk(xk+1) = Pσk(x(σk)) ≤Pσk(xη),
that is
1
σ
m

i=1
h(ci(xk+1))
≤
f(xη) + 1
σk
m

i=1
h(ci(xη)) −f(xk+1)
≤
inf
x∈int(X)
f(x) + 1
2η + 1
2η −f(xk+1)
≤
η
(10.3.21)
holds for all k ≥¯k. Since η > 0 is arbitrary, it follows from (10.3.21) that
(10.3.17) is true.
From the ﬁrst row in (10.3.21), we also get
f(xk+1)
≤
f(xη) + 1
σk
m

i=1
h(ci(xη))
≤
inf
x∈int(X)
f(x) + η
(10.3.22)
holds for all k ≥¯k. Then (10.3.18) holds.
2
Suppose that the sequence {xk} generated by Algorithm 10.3.3 converges
to x∗. If x∗is a strict interior point, then it follows from
∇f(xk+1) + 1
σk
m

i=1
h′(ci(xk+1))∇ci(xk+1) = 0
(10.3.23)

10.3. INTERIOR POINT PENALTY FUNCTIONS
471
that
∥∇f(xk+1)∥= O
 1
σk
!
.
(10.3.24)
If the second-order suﬃcient condition (i.e., ∇2f(x∗) positive deﬁnite) is
satisﬁed, then (10.3.24) is equivalent to
∥xk+1 −x∗∥= O
 1
σk
!
.
(10.3.25)
The above discussion also tells us that the rate of ∥xk+1 −x∗∥→0 is, in
general, no quicker than 1/σk.
Now, let us consider xk →x∗, where x∗is a boundary point of the
feasible region of (10.3.2), i.e., there exists i such that ci(x∗) = 0.
Let
∇ci(x∗)(i ∈I(x∗)) be linearly independent.
Let λ∗
i denote the Lagrange
multiplier at x∗. From (10.3.23), we have
lim
k→∞−h′(ci(xk+1))/σk = λ∗
i .
(10.3.26)
Write
λ(k+1) = (−h′(c1(xk+1)), · · · , −h′(cm(xk+1)))T /σk.
Deﬁne A∗as a matrix consisting of ∇ci(x∗) (i ∈I(x∗)), ˆλ∗as a vector con-
sisting of those components λ∗
i of λ∗(i ∈I(x∗)), and note that the deﬁnitions
of ˆλ(k+1) and ˆc(xk+1) are similar to ˆλ∗. By (10.3.26), we have
|λ(k+1)
i
| = O
 1
σk
!
, ∀i /∈I(x∗).
(10.3.27)
Since the columns of A∗are linearly independent, the above equality gives
∥ˆλ(k+1) −ˆλ∗∥= O
 
∥xk+1 −x∗∥+ 1
σk
!
.
(10.3.28)
Note that by (10.3.23), we obtain
W ∗(xk+1 −x∗) −A∗(ˆλ(k+1) −ˆλ∗) = o(∥xk+1 −x∗∥) + O
 1
σk
!
.
(10.3.29)
Also,
−(A∗)T (xk+1 −x∗) = −ˆc(xk+1) + o(∥xk+1 −x∗∥).
(10.3.30)

472
CHAPTER 10. PENALTY FUNCTION METHODS
Then the above two equalities give

W ∗
−A∗
−(A∗)T
0
 
xk+1 −x∗
ˆλ(k+1) −ˆλ∗

=

0
−ˆc(xk+1)

+ o(∥xk+1 −x∗∥) + O
 1
σk
!
.
(10.3.31)
Suppose that λ∗
i > 0 (i ∈I(x∗)). Then for an inverse barrier function and
a logarithmic barrier function, by using (10.3.26), we obtain respectively
ci(xk+1) = O

1
√σk

, i ∈I(x∗)
(10.3.32)
and
ci(xk+1) = O
 1
σk
!
, i ∈I(x∗).
(10.3.33)
Hence, provided that the second-order suﬃcient condition is satisﬁed, for the
inverse barrier function and logarithmic barrier function, we have
∥xk+1 −x∗∥= O

1
√σk

(10.3.34)
and
∥xk+1 −x∗∥= O
 1
σk
!
.
(10.3.35)
From (10.3.34)-(10.3.35), it is easy to see that the logarithmic barrier function
converges more quickly than the inverse barrier function does.
Now we consider solving (10.3.8) inexactly by interior point function
methods. Suppose that both f(x) and h(ci(x)) are convex functions of x,
then Pσ(x) is also convex with respect to x. Given starting point xk, then
for problem
min Pσk(x),
(10.3.36)
the Newton step is
dk = −[∇2Pσk(xk)]−1∇Pσk(xk).
(10.3.37)
To avoid solving the subproblem exactly, the xk + dk is regarded as an ap-
proximate solution of (10.3.36). For simplicity, we assume
h(ci) = −log ci(x).
(10.3.38)

10.3. INTERIOR POINT PENALTY FUNCTIONS
473
Set xk+1 = xk + dk. Then we have from (10.3.37) that
∇2Pσk(xk)(xk+1 −xk) = −∇Pσk(xk).
(10.3.39)
Note that
∇xPσk(xk)
= ∇f(xk) −
m

i=1
1
σ
1
ci(xk)∇ci(xk),
∇2
xPσk(xk)
= ∇2f(xk) −
m

i=1
1
σk
1
ci(xk)∇2ci(xk)
+
m

i=1
1
σk
1
(ci(xk))2 ∇c −i(xk)∇ci(xk)T .
Write
λ(k)
i
=
1
σkci(xk).
Then (10.3.39) can be written as

∇2f(xk) −
m

i=1
λ(k)
i
∇2ci(xk) +
m

i=1
λ(k)
i
1
ci(xk)∇ci(xk)∇ci(xk)T

·(xk+1 −xk) = −

∇f(xk) −
m

i=1
λ(k)
i
∇ci(xk)

.
(10.3.40)
If the xk+1 deﬁned above lies in the interior of the feasible region, it is re-
garded as next iterate. Otherwise, there is ¯α > 0 such that the point xk+¯αkdk
lies on the boundary of the feasible region. In such a case, we set
xk+1 = xk + 0.9¯αkdk.
(10.3.41)
So, the xk+1 still is an interior point. Hence, an inexact interior point penalty
function algorithm for subproblem (10.3.36) can be written as follows.
Algorithm 10.3.5 (Inexact Log-Barrier Function Method)
Step 1. Given x1 satisfying (10.3.15), σ1 > 0, ϵ ≥0, k := 1.
Step 2. Compute λ(k)
i
=
1
σkci(xk), i = 1, · · · , m;
dk
= −[∇2f(xk) −
m

i=1
λ(k)
i
∇2ci(xk)

474
CHAPTER 10. PENALTY FUNCTION METHODS
+
m

i=1
λ(k)
i
1
ci(xk)∇ci(xk)∇ci(xk)T ]−1
·(∇f(xk) −
m

i=1
λ(k)
i
∇ci(xk));
(10.3.42)
If dk ̸= 0 go to Step 3;
If ∥∇f(xk)∥≤ϵ, stop;
Otherwise, σk := 10σk; go to step 2.
Step 3. Set αk = 1;
If xk + dk is an interior point, go to Step 4;
Find 1 ≥¯αk > 0 such that xk + ¯αkdk is on the boundary of
the feasible region;
Set αk := 0.9¯αk.
Step 4. Set xk+1 := xk + αkdk;
If
1
σk
m

i=1
log
 
1
ci(xk)
!
≤ϵ,
(10.3.43)
stop; σk+1 := 10σk; k := k + 1; go to Step 2.
2
10.4
Augmented Lagrangian Method
In this section we discuss the augmented Lagrangian method (or the method
of multiplier penalty function).
We know from §10.1 that this method is an extension of the quadratic
penalty function method. It reduces the possibility of ill-conditioning of the
subproblem by introducing Lagrange multiplier estimates. In fact, it is a
combination of the Lagrangian function and the quadratic penalty function.
For the case where only equality constraints are presented (m = me), we
rewrite the augmented Lagrangian function as
P(x, λ, σ) = f(x) −
m

i=1
λici(x) + 1
2
m

i=1
σi(ci(x))2.
(10.4.1)
When we diﬀerentiate with respect to x, we obtain
∇xP(x, λ, σ) = ∇f(x) −
m

i=1
(λi −σici(x))∇ci(x),
(10.4.2)

10.4. AUGMENTED LAGRANGIAN METHOD
475
which suggests the formula
λ(k+1)
i
= λ(k)
i
−σ(k)
i
ci(xk+1).
(10.4.3)
Now we consider the general problem (10.1.1)–(10.1.3) by the augmented
Lagrangian function. We rewrite the augmented Lagrangian function P(x, λ, σ)
(10.1.21) as follows:
P(x, λ, σ)
= f(x) +
me

i=1

−λici(x) + 1
2σic2
i (x)

+
m

i=me+1
 
−λici(x) + 1
2σic2
i (x)

,
if ci(x) < λi
σi ;
−1
2λ2
i /σi,
otherwise
(10.4.4)
where λi (i = 1, · · · , m) are Lagrange multipliers, σi (i = 1, · · · , m) are penalty
parameters.
Consider the k-th iteration, using λ(k)
i
and σ(k)
i
to denote corresponding
components of λ and σ respectively at the k-th iteration. Let xk+1 be the
solution of the subproblem
min
x∈Rn P(x, λ(k), σ(k)).
(10.4.5)
Then we have
∇f(xk+1)
=
me

i=1
[λ(k)
i
−σ(k)
i
ci(xk+1)]∇ci(xk+1)
+
m

i=me+1
max{λ(k)
i
−σ(k)
i
ci(xk+1), 0}∇ci(xk+1).(10.4.6)
Hence we take
λ(k+1)
i
= λ(k)
i
−σ(k)
i
ci(xk+1), i = 1, · · · , me;
(10.4.7)
λ(k+1)
i
= max{λ(k)
i
−σ(k)
i
ci(xk+1), 0}, i = me + 1, · · · , m, (10.4.8)
as next Lagrange multipliers. By (10.4.6)–(10.4.8), we have that
∇f(xk+1) −
m

i=1
λ(k+1)
i
∇ci(xk+1) = 0,
(10.4.9)

476
CHAPTER 10. PENALTY FUNCTION METHODS
which indicates that for any k ≥2, the error of the KKT condition for (xk, λk)
is
∥∇xL(xk, λ(k))∥+ ∥c(−)(xk)∥= ∥c(−)(xk)∥,
(10.4.10)
where L(x, λ) is the Lagrangian function deﬁned in (8.2.18), c(−)(x) is a
constraint violation function deﬁned in (10.1.5)–(10.1.6). Therefore, for k ≥
2, provided that the inequality
|c(−)
i
(xk+1)| ≤1
4|c(−)
i
(xk)|
(10.4.11)
is not satisﬁed, we enlarge the penalty parameters, i.e., set
σ(k+1)
i
= 10σ(k)
i
.
(10.4.12)
Below, we give an algorithm based on the augmented Lagrangian function.
Algorithm 10.4.1 (Augmented Lagrangian Method)
Step 1. Given starting point x1 ∈Rn, λ(1) ∈Rm with λ(1)
i
≥0 (i ∈
I); σ(1)
i
> 0 (i = 1, · · · , m); ϵ ≥0, k := 1.
Step 2. Find approximate minimizer xk+1 to (10.4.5).
If ∥c(−)(xk+1)∥∞≤ϵ, stop.
Step 3. For i = 1, · · · , m, set
σ(k+1)
i
=

σ(k)
i
,
if (10.4.11) holds;
max[10σ(k)
i
, k2],
otherwise.
(10.4.13)
Step 4. Update Lagrange multipliers using (10.4.7)–(10.4.8) to ob-
tain λ(k+1), k := k + 1, go to Step 2.
2
A practical implementation of the above algorithm is given in LANCELOT
due to Conn, Gould, and Toint [68].
Now we establish the ﬁnite termination of Algorithm 10.4.1.
Theorem 10.4.2 Let the feasible region X of problem (10.1.1)–(10.1.3) be
nonempty. Then for some ϵ > 0, Algorithm 10.4.1 is either ﬁnitely termi-
nated, or the sequence {xk} produced by Algorithm 10.4.1 satisﬁes
lim inf
k→∞f(xk) = −∞.
(10.4.14)

10.4. AUGMENTED LAGRANGIAN METHOD
477
Proof.
Suppose, on the contrary, that the theorem is not true, that is,
for some ϵ > 0, Algorithm 10.4.1 does not terminate ﬁnitely, and {f(xk)} is
bounded below. Deﬁne the set J by
J =
*
i | lim
k→∞|c(−)
i
(xk)| = 0, 1 ≤i ≤m
+
.
(10.4.15)
Since the algorithm does not terminate ﬁnitely, then the set
ˆJ = {1, 2, · · · , m}/J
(10.4.16)
is not empty. Thus, by the construction of the algorithm, for any i ∈ˆJ, we
have
lim
k→∞σ(k)
i
= +∞.
(10.4.17)
Deﬁne
µ(k)
i
= λ(k)
i
/

σ(k)
i
.
(10.4.18)
It is not diﬃcult to prove that
∥µ(k+1)∥2
2
≤
m

i=1
[λ(k+1)
i
]2/σ(k)
i
≤
∥µ(k)∥2
2 + 2[P(xk+1, λ(k), σ(k)) −f(xk+1)]
−2[P(¯x, λ(k), σ(k)) −f(¯x)]
≤
∥µ(k)∥2
2 + 2[f(¯x) −f(xk+1)],
(10.4.19)
where ¯x is any feasible point of (10.1.1)–(10.1.3). Since {f(xk)} is bounded
below, then (10.4.19) suggests that there exists δ > 0 such that
∥µ(k)∥2
2 ≤δk
(10.4.20)
holds for all k. Set
˜J = {i | lim
k→∞σ(k)
i
= +∞}.
(10.4.21)
Equation (10.4.17) indicates that ˆJ ⊆˜J. By the deﬁnition of xk+1, we have
f(¯x) +

i>me
1
2σ(k)
i
⎡
⎣

ci(¯x) −λ(k)
i
σ(k)
i
2
−
−

λ(k)
i
σ(k)
i
2⎤
⎦
≥
f(xk+1) +

i≤me
1
2σ(k)
i
⎡
⎣

ci(xk+1) −λ(k)
i
σ(k)
i
2
−

λ(k)
i
σ(k)
i
2⎤
⎦
+

i>me
1
2σ(k)
i
⎡
⎣

ci(xk+1) −λ(k)
i
σ(k)
i
2
−
−

λ(k)
i
σ(k)
i
2⎤
⎦,
(10.4.22)

478
CHAPTER 10. PENALTY FUNCTION METHODS
where ¯x is any feasible point of (10.1.1)–(10.1.3), (α)−denotes min{0, α}.
By use of (10.4.20)–(10.4.22), we can deduce that
f(¯x)
−f(xk+1) ≥O(k)
+

i≤me
i∈˜
J
1
2σ(k)
i
⎡
⎣

ci(xk+1) −λ(k)
i
σ(k)
i
2
−

λ(k)
i
σ(k)
i
2⎤
⎦
+

i>me
i∈˜
J
1
2σ(k)
i
⎡
⎣

ci(xk+1) −λ(k)
i
σ(k)
i
2
−
−

λ(k)
i
σ(k)
i
2⎤
⎦. (10.4.23)
Since the algorithm does not terminate ﬁnitely, then for any k, there exists
¯k > k such that for some i ∈˜J, we have σ(¯k+1)
i
> σ(¯k)
i
and
|ci(x¯k+1)| > ϵ if i ≤me
or
|(ci(x¯k+1))−| > ϵ if i > me.
Then it follows from (10.4.23) that
f(¯x) −f(x¯k+1)
≥
O(¯k) + 1
2σ(¯k)
i
ϵ2 + o(σ(¯k)
i
)
≥
O(¯k) + 1
4
¯k2ϵ
(10.4.24)
≥
1
8
¯k2ϵ.
(10.4.25)
This contradicts the fact that the {f(xk)} is bounded below. The contradic-
tion proves the theorem.
2
Theorem 10.4.3 Let the feasible region X of problem (10.1.1)–(10.1.3) be
nonempty. Then for ϵ = 0, any accumulation point x∗of the sequence {xk}
generated by Algorithm 10.4.1 is feasible. Further, if {λ(k)} is bounded, then
x∗is the solution of the original problem (10.1.1)–(10.1.3).
Proof.
By Algorithm 10.4.1 and Theorem 10.4.2, we have
lim
k→∞∥c−(xk)∥= 0.
(10.4.26)
Hence, any accumulation point of {xk} is a feasible point of (10.1.1)–(10.1.3).

10.4. AUGMENTED LAGRANGIAN METHOD
479
Suppose that {λ(k)} is bounded for all k. Then by (10.4.22) and (10.4.26),
we can deduce that
f(¯x)
≥
f(xk+1) +

i≤me
1
2σ(k)
i
c2
i (xk+1)
+

i>me
1
2σ(k)
i
⎡
⎣

c(xk+1) −λ(k)
i
σ(k)
i
2
−
−

−λ(k)
i
σ(k)
i
2
−
⎤
⎦+ o(1)
≥
f(xk+1) + o(1).
(10.4.27)
Since ¯x ∈X is arbitrary, we have
lim
k→∞f(x) = inf
x∈X f(x).
(10.4.28)
Thus, any accumulation point x∗of {xk} is the solution of the original prob-
lem.
2
Finally, we consider the convergence rate of Algorithm 10.4.1. For sim-
plicity, we consider the case with equality constraints only.
Suppose that xk →x∗. By Theorem 10.4.3, x∗is the solution of (10.1.1)–
(10.1.3). Let A(x∗) have full column rank with rank(A(x∗)) = m. Let A(xk)
have full column rank for all k too. Then the λ(k+1) generated by Algorithm
10.4.1 is equivalent to the λ(xk+1) deﬁned by the following expression
A(xk+1)λ(xk+1) = g(xk+1),
(10.4.29)
where g(x) = ∇f(x). Note that
λ(x) = [A(x)]+g(x).
(10.4.30)
It is not diﬃcult to get
∇λ(x) = [A(x)]+W(x),
(10.4.31)
where
W(x) = ∇2f(x) −
m

i=1
[λ(x)]i∇2ci(x).
(10.4.32)
By (10.4.7), we have
λ(xk+1) + Dkc(xk+1) = λ(xk),
(10.4.33)

480
CHAPTER 10. PENALTY FUNCTION METHODS
where
Dk =
⎡
⎢⎢⎣
σ(k)
1
0
...
0
σ(k)
m
⎤
⎥⎥⎦.
(10.4.34)
Then, it suggests by diﬀerentiation that
[DkA(x∗)T +A(x∗)+W(x∗)](xk+1 −x∗) ≈A(x∗)+W(x∗)(xk −x∗). (10.4.35)
The above expression says that, unless σ(k) →+∞, the sequence {xk} gen-
erated by Algorithm 10.4.1 is, in general, convergent linearly.
A shortcoming of the augmented Lagrangian function is that it is only
once continuously diﬀerentiable. Hence it is possible that there will be some
numerical diﬃculties in solving the subproblem (10.4.5).
10.5
Smooth Exact Penalty Functions
For the equality-constrained problem
minx∈Rn
f(x),
(10.5.1)
s.t.
c(x) = 0,
(10.5.2)
Fletcher [126] ﬁrst presented a smooth exact penalty function
P(x, σ) = f(x) −λ(x)T c(x) + 1
2c(x)T Dc(x),
(10.5.3)
where λ(x) is given by (10.1.23), D = diag(σ1, · · · , σm). From the discussions
in §10.1 and §10.4, we know that, if the second-order suﬃcient condition holds
and σi are suﬃciently large, then the local minimizer of (10.5.1)–(10.5.2) is
a strict local minimizer of the penalty function (10.5.3). Conversely, if ¯x is a
minimizer of (10.5.3) and c(¯x) = 0, then ¯x is also the minimizer of problem
(10.5.1)–(10.5.2).
If we set all σi equal in (10.5.3), then a simple form of Fletcher’s smooth
exact penalty function
P(x, σ) = f(x) −λ(x)T c(x) + 1
2σ∥c(x)∥2
2
(10.5.4)
is obtained. For this penalty function, let x(σ) be a solution of the subprob-
lem
min
x∈Rn P(x, σ).
(10.5.5)

10.5. SMOOTH EXACT PENALTY FUNCTIONS
481
We have, similar to (10.2.5), that
∥c(x(σ2))∥2 ≤∥c(x(σ1))∥2, ∀σ2 ≥σ1 > 0.
(10.5.6)
Comparing with the simple penalty function, by use of (10.5.5) and (10.5.4),
we need not require σ →+∞. Thus it is possible to attempt the solution of
original problem (10.5.1)–(10.5.2) by solving (10.5.5) without needing σ →
+∞. In addition, the exact penalty function (10.5.3) is smooth, and thus
the convergence rate of methods to solve the unconstrained optimization
problem (10.5.5) is rapid. A drawback of this approach is, however, that
computing ∇xP(x, σ) needs computation of ∇λ(x), and further ∇2f(x) and
∇2ci(x), (i = 1, · · · , me). It is expensive.
If, in (10.5.3), we replace D by
2σA+(A+)T ,
(10.5.7)
where A = ∇c(x), we obtain
P(x) = f(x) −π(x)T c(x),
(10.5.8)
where
π(x) = A+(g(x) −σ(A+)T c(x)).
(10.5.9)
It is not diﬃcult to ﬁnd that π(x) is the Lagrange multiplier of the problem
mind∈Rn
1
2σdT d + g(x)T d
(10.5.10)
s.t
A(x)T d + c(x) = 0.
(10.5.11)
For general inequality constrained optimization problem (10.1.1)–(10.1.3),
we can deﬁne π(x) as the Lagrange multiplier of subproblem
mind∈Rn
g(x)T d + 1
2σ∥d∥2
2,
(10.5.12)
s.t.
ci(x) + dT ∇ci(x) = 0, i ∈E,
(10.5.13)
ci(x) + dT ∇ci(x) ≥0, i ∈I,
(10.5.14)
and then construct the penalty function
P(x) = f(x) −π(x)T c(x).
(10.5.15)

482
CHAPTER 10. PENALTY FUNCTION METHODS
The multiplier π(x) can also be obtained by solve the dual problem of (10.5.12)-
(10.5.14)
min
πi≥0
i∈I
1
2
g(x) −
m

i=1
πi∇ci(x)

2
2
+ σπT c(x).
(10.5.16)
As an alternative to the subproblem (10.5.4), we may consider the smooth
exact penalty function
P(x, λ)
=
f(x) −c(x)T λ + 1
2σ∥c(x)∥2
2
+1
2ρ∥M(x)[g(x) −A(x)λ]∥2
2
(10.5.17)
to deal with an equality constrained problem, where M(x) can be A(x)T , A(x)+,
or an identity matrix.
Equation (10.5.17) may be extended to handle an
inequality-constrained problem. We refer the reader to Di Pillo, Grippo and
Lampariell [104] or Fletcher [132].
10.6
Nonsmooth Exact Penalty Functions
Let h(c) be a convex function deﬁned on Rm with h(0) = 0. If there exists a
positive constant δ > 0 such that
h(c) ≥δ∥c∥1
(10.6.1)
holds for all c ∈Rm, then h(c) is called a strong distance function.
For any strong distance function h(c), we say that the penalty function
Pσ,h(x) = f(x) + σh(c(−)(x))
(10.6.2)
is a nonsmooth exact penalty function, where σ > 0 is a penalty parameter
and c(−)(x) is a constraint violation function deﬁned in (10.1.5)–(10.1.6).
For nonsmooth exact penalty function (10.6.2), we give the following
theorem about necessity.
Theorem 10.6.1 Let x∗be a local minimizer of constrained optimization
problem (10.1.1)–(10.1.3) satisfying, together with the corresponding Lagrange
multiplier vector λ∗, the second-order suﬃcient condition
dT ∇2
xxL(x∗, λ∗)d > 0, ∀0 ̸= d ∈LFD (x∗, X).
(10.6.3)

10.6. NONSMOOTH EXACT PENALTY FUNCTIONS
483
Then, if
σδ > ∥λ∗∥∞,
(10.6.4)
the vector x∗is a strict local minimizer of penalty function Pσ,h(x) deﬁned
in (10.6.2).
Proof.
Let (10.6.4) hold. Suppose, to the contrary, that the theorem is
not true. Then there exist xk (k = 1, 2, · · ·) such that xk ̸= x∗, xk →x∗and
Pσ,h(xk) ≤P(x∗), ∀k.
(10.6.5)
The above expression gives
f(xk) + σδ∥c(−)(xk)∥1 ≤f(x∗).
(10.6.6)
Without loss of generality, we assume that
(xk −x∗)/∥xk −x∗∥→d.
(10.6.7)
By (10.6.6) and the deﬁnition of Lagrange multiplier, we obtain
(σδ −∥λ∗∥∞)∥c(−)(xk)∥1
=
(g(x∗) −A(x∗)λ∗)T (xk −x∗) + (σδ −∥λ∗∥∞)∥c(−)(xk)∥1
=
f(xk) −
m

i=1
λ∗
i ci(xk) −f(x∗) −1
2(xk −x∗)T ∇2
xxL(x∗, λ∗)(xk −x∗)
+(σδ −∥λ∗∥∞)∥c(−)(xk)∥1 + o(∥xk −x∗∥2)
=
f(xk) + σδ∥c(−)(xk)∥1 −f(x∗) −
m

i=1
(∥λ∗∥∞|c(−)
i
(xk)| + λ∗
i ci(xk))
−1
2dT ∇2
xxL(x∗, λ∗)d∥xk −x∗∥2
2 + o(∥xk −x∗∥2
2)
≤
−1
2dT ∇2
xxL(x∗, λ∗)d∥xk −x∗∥2
2 + o(∥xk −x∗∥2
2).
(10.6.8)
By using (10.6.8) and (10.6.4), and taking the limit, we yield
lim
k→∞
∥c(−)(xk)∥1
∥xk −x∗∥= 0,
(10.6.9)
which indicates
d ∈LFD(x∗, X).
(10.6.10)

484
CHAPTER 10. PENALTY FUNCTION METHODS
From the second-order suﬃcient condition we have
dT ∇2
xxL(x∗, λ∗)d > 0
(10.6.11)
which shows that the last row in inequality (10.6.8) is negative when k is
suﬃciently large. Then it produces a contradiction. The contradiction proves
the theorem.
2
The common nonsmooth exact penalty functions are the L1 exact penalty
function
P1(x) = f(x) + σ∥c(−)(x)∥1
(10.6.12)
and the L∞exact penalty function
P∞(x) = f(x) + σ∥c(−)(x)∥∞.
(10.6.13)
For nonsmooth exact penalty function (10.6.2), let x(σ) be a minimizer
of the problem
min
x∈Rn Pσ,h(x).
(10.6.14)
Completely similar to Lemma 10.2.1 and Lemma 10.2.2, we have the following
lemmas. The proofs are omitted.
Lemma 10.6.2 Let σ2 > σ1 > 0. Then we have
f(x(σ2)) ≥f(x(σ1)),
(10.6.15)
h(c(−)(x(σ2))) ≤h(c(−)(x(σ1))).
(10.6.16)
Lemma 10.6.3 Let η = h(c(−)(x(σ))), then x(σ) is also the solution of
cconstrained problem
minx∈Rn
f(x)
(10.6.17)
s.t.
h(c(−)(x)) ≤η.
(10.6.18)
It is advantageous for an exact penalty function that it is possible to
attempt exactly the solution of a constrained optimization problem by solving
only a single or ﬁnitely many unconstrained problems.
The nonsmooth exact penalty methods can be written in the following
form:
Algorithm 10.6.4 (Nonsmooth Exact Penalty Method)

10.6. NONSMOOTH EXACT PENALTY FUNCTIONS
485
Step 1. Given x1 ∈Rn, σ1 > 0, k := 1.
Step 2. Solve
min
x∈Rn Pσ,h(x)
(10.6.19)
at xk to obtain x(σ).
Step 3. If c(−)(x(σk)) = 0, stop;
xk+1 := x(σk), σk+1 := 10σk;
k := k + 1; go to Step 2.
2
Note that since Pσ,h(x) is an exact penalty function, we can obtain the
exact solution of the original problem provided that σ is suﬃciently large.
The following is the convergence result of Algorithm 10.6.4.
Theorem 10.6.5 Let f(x), ci(x) (i = 1, · · · , m) be twice continuously diﬀer-
entiable. Let the feasible region of constrained optimization problem (10.1.1)–
(10.1.3) be nonempty.
If second order suﬃcient condition (10.6.3) holds,
then either Algorithm 10.6.4 terminates at a strict local minimizer of prob-
lem (10.1.1)–(10.1.3) in ﬁnitely many iterations, or the generated sequence
satisﬁes ∥xk∥→∞.
Proof.
(1) If the algorithm terminates ﬁnitely at x(σ), then x(σk) must be
a local minimizer of Pσk,h(x). By Lemma 10.6.6, which will be presented be-
low, x(σk) is also a local minimizer of the original problem (10.1.1)–(10.1.3).
Since the second-order suﬃcient condition is satisﬁed at x(σk), then x(σk) is
a strict local minimizer.
(2) Now we prove the second conclusion by contradiction. Suppose that
the theorem is not true. Then, for any k, we have c(−)(xk) ̸= 0, {∥xk∥} has a
bounded subsequence and σk →∞. Let ¯x be any local minimizer of problem
(10.1.1)–(10.1.3). By the deﬁnition of x(σk), we have
f(xk+1) + σkh(c(−)(xk+1)) ≤f(¯x) + σkh(c(−)(¯x)) = f(¯x),
(10.6.20)
which means
σkh(c(−)(xk+1)) ≤f(¯x) −f(xk+1).
Then we have
lim
k→∞h(c(−)(xk+1)) = 0.
(10.6.21)

486
CHAPTER 10. PENALTY FUNCTION METHODS
The above expression and (10.6.1) suggest that
lim
k→∞∥c(−)(xk)∥= 0.
(10.6.22)
Since {∥xk∥} has a bounded subsequence, we may let ˆx be an accumulation
point of {xk} and therefore using (10.6.22) we have
c(−)(ˆx) = 0.
(10.6.23)
Let xkj →ˆx. If ˆx is not a local minimizer of (10.1.1)–(10.1.3), then there
exists ˜x suﬃciently approaching ˆx and we have
f(˜x) < f(ˆx),
(10.6.24)
c(−)(˜x) = 0.
(10.6.25)
From (10.6.24) and that xkj →ˆx, we obtain
f(xkj) > f(˜x)
(10.6.26)
for j suﬃciently large. Hence, we deduce
Pσkj−1,h(xkj) > Pσkj−1,h(˜x),
(10.6.27)
which contradicts the deﬁnition of {xkj}. Therefore, ˆx is a local minimizer
of the original problem (10.1.1)–(10.1.3).
Then it follows from Theorem 10.6.1 that there exist ¯δ and ¯σ such that
P¯σ,h(x) > P¯σ,h(ˆx), ∀∥x −ˆx∥≤δ, x ̸= ˆx.
(10.6.28)
The above expression suggests that
Pσ,h(x) > Pσ,h(ˆx), ∀x ̸= ˆx, ∥x −ˆx∥≤δ, σ > ¯σ.
(10.6.29)
Since σkj →∞, xkj →ˆx and xkj ̸= ˆx, then there exists j such that ∥xkj−ˆx∥<
δ and σkj−1 > ¯σ. Hence
Pσkj−1,h(xkj) > Pσkj−1,h(ˆx),
(10.6.30)
which contradicts the deﬁnition of xkj. The contradiction shows the theorem.
2
If Algorithm 10.6.4 terminates ﬁnitely, it is sure that it terminates at
a local minimizer of the original problem. This is based on the following
lemma.

10.6. NONSMOOTH EXACT PENALTY FUNCTIONS
487
Lemma 10.6.6 For any σ > 0 and ¯x ∈Rn, if h(c(−)(¯x)) = 0 and ¯x is a local
minimizer of the nonsmooth exact penalty function Pσ,h(x), then ¯x is also a
local minimizer of the constrained optimization problem (10.1.1)–(10.1.3).
Proof.
Let ¯x satisfy h(c(−)(¯x)) = 0 and be a local minimizer of Pσ,h(x).
Suppose, to the contrary, that the lemma is not true.
Then there exist
xk, (k = 1, 2, · · ·), such that xk →¯x, xk ̸= ¯x and
f(xk) < f(¯x),
(10.6.31)
c(−)(xk) = 0.
(10.6.32)
Then, we have
Pσ,h(xk) < Pσ(¯x),
(10.6.33)
which contradicts the fact that ¯x is a local minimizer of Pσ,h(x). Then we
complete the proof.
2
We would like to mention that, in a rare case, it is possible that there is
∥xk∥→∞for Algorithm 10.6.4. For example, consider
minx∈R1
100e−x −
1
x2 + 1
(10.6.34)
s.t.
xe−x = 0.
(10.6.35)
Taking h(c) = |c| yields that the penalty function is
Pσ(x) = 100e−x −
1
x2 + 1 + σ
xe−x .
(10.6.36)
For a suﬃciently large σ > 0, the minimizer x(σ) of Pσ(x) satisﬁes the
equation
−100 +
2xex
(x2 + 1)2 = σ(x −1),
(10.6.37)
and x(σ) > 1. Then
lim
σ→∞
2ex(σ)
(x(σ)2 + 1)2σ = 1.
(10.6.38)
Therefore
lim
σ→∞x(σ) = +∞,
(10.6.39)
which says that the sequence generated by Algorithm 10.6.4 satisﬁes xk →
+∞.

488
CHAPTER 10. PENALTY FUNCTION METHODS
We note that when the gradients of the constraint function are linearly
dependent, it is possible that the minimizer of the original problem (10.1.1)–
(10.1.3) is not a stationary point of the exact penalty function (10.6.2). For
example,
minx∈R1
x
(10.6.40)
s.t.
c(x) = x2 = 0.
(10.6.41)
Taking h(c) = c yields that, for any given σ > 0, the solution x∗= 0 of
problem (10.6.40)–(10.6.41) is not the stationary point of the exact penalty
function
Pσ,h(x) = x + σx2.
(10.6.42)
However, if the gradients of the constraint function are linearly indepen-
dent, the minimizer of original problem (10.1.1)–(10.1.3) is also the minimizer
of the exact penalty function.
Theorem 10.6.7 Let x∗be a local minimizer of constrained optimization
problem (10.1.1)–(10.1.3) and λ∗be a corresponding Lagrange multiplier. If
∇ci(x∗), i ∈E ∪I(x∗)
(10.6.43)
are linearly independent, then when (10.6.4) holds, the x∗is also a local
minimizer of the penalty function (10.6.2).
Proof.
If the theorem is not true, then there exist xk (k = 1, 2, · · ·) such
that xk ̸= x∗, xk →x∗and
Pσ,h(xk) < P(x∗), ∀k.
(10.6.44)
Then by (10.6.1) we have
f(xk) + σδ∥c(−)(xk)∥1 < f(x∗).
(10.6.45)
Similar to the proof of Theorem 10.6.1, we may assume that (10.6.7) holds
and use (10.6.8) to get
d ∈LFD (x∗, X).
(10.6.46)
The second-order necessary condition gives
dT ∇2
xxL(x∗, λ∗)d ≥0,
(10.6.47)

10.6. NONSMOOTH EXACT PENALTY FUNCTIONS
489
from which together with (10.6.8), we can deduce that
dT ∇2
xxL(x∗, λ∗)d = 0.
(10.6.48)
Then
∥c(−)(xk)∥1 = o(∥xk −x∗∥2).
(10.6.49)
Since x∗is a local minimizer of the original problem (10.1.1)–(10.1.3), it
follows from (10.6.45) that
∥c(−)(xk)∥1 > 0
(10.6.50)
for all suﬃciently large k. Since the gradients of all active constraints are
linearly independent, then there is some yk such that
c(−)(yk) = 0
(10.6.51)
and
∥yk −xk∥= O(∥c(−)(xk)∥).
(10.6.52)
By use of the optimality of x∗and (10.6.51), we have
f(yk) ≥f(x∗).
(10.6.53)
On the other hand, by the KKT condition, (10.6.4) and (10.6.45), we can
obtain that
f(yk)
=
f(xk) + ∇f(x∗)T (yk −xk) + o(∥yk −xk∥)
=
f(xk) +
m

i=1
λ∗
i (yk −xk)T ∇ci(x∗) + o(∥yk −xk∥)
≤
f(xk) + λ∗∥∞∥c(−)(xk)∥1 + o(∥c(−)(xk)∥1)
<
f(xk) + σδ∥c(−)(xk)∥1
<
f(x∗),
(10.6.54)
which contradicts (10.6.53). The contradiction proves the theorem.
2
It is not diﬃcult to see that the equivalence between the nonsmooth exact
penalty function (10.6.2) and the constrained optimization problem is based
on (10.6.4). In fact, if the inequality (10.6.4) is not satisﬁed, then the local
minimizer of (10.1.1)–(10.1.3) is not necessarily a stationary point of the
penalty function (10.6.2).

490
CHAPTER 10. PENALTY FUNCTION METHODS
Theorem 10.6.8 Let x∗be a local minimizer of constrained optimization
(10.1.1)–(10.1.3) and ∇f(x∗) ̸= 0. Write
T = max
v∈∂h(0) ∥v∥.
(10.6.55)
Then, when
σ∥∇c(−)(x∗)∥< ∥∇f(x∗)∥/T,
(10.6.56)
x∗is not the stationary point of the penalty function (10.6.2).
Proof.
Since the subgradient of the penalty function (10.6.2) at x∗is
∂Pσ,h(x∗) = ∇f(x∗) + σ∇c(−)(x∗)T ∂h(0),
(10.6.57)
then, by (10.6.56), we have
0 /∈∂Pσ,h(x∗).
(10.6.58)
Therefore, x∗is not a stationary point of Pσ,h(x).
2
Exercises
1. Use the Courant penalty function method to solve the problem
min
−2x1 + x2
s.t.
x2 −x2
1 = 0.
2. Apply the inverse penalty function method to solve the problem
min
−x2
1 −x2
2
s.t.
x1 ≤8,
x2 ≤8,
x1 + x2 ≥1
with the initial point (2
2)T .
3. Apply the logarithmic barrier function method to solve the problem
min
x1 −x2 + x2
2
s.t.
x1 ≥0,
x2 ≥0

10.6. NONSMOOTH EXACT PENALTY FUNCTIONS
491
with the initial point (1, 1)T .
4. Apply the Augmented Lagrangian function method to solve the prob-
lem in the previous exercise, using initial multipliers λ1 = 1 and λ2 = 1.
Compare the performances of the two methods ( Logarithmic barrier func-
tion method and the Augmented Lagrangian function method).
5. Let x(σ) be the solution of
min P(x, σ) = f(x) + 1
σ
m

i=1
1
ci(x)
(10.6.59)
in the interior region {x|ci(x) > 0, i = 1, ..., m}, where σ > 0 is a parameter.
Prove that, as σ increases,
(1) P(x(σ), σ) is non-increasing;
(2) 
m
i=1
1
ci(x(σ)) is non-decreasing;
(3) f(x(σ) is non-increasing.
6. Discuss the penalty function (10.1.10) when h(c) = ec.
7. Using the approximation
max{c1, ..., cm} ≈log
 m

i=1
eci

,
we can replace the L∞penalty function by
Pe(x) = f(x) + σ log
 m

i=1
e|c−
i (x)|

.
Study the properties of the above penalty function Pe(x).
8. Introducing the slack variables for the inequality constraints, we can
reformulate (8.1.1)–(8.1.3) as
min
f(x)
s.t.
ci(x) = 0,
i = 1, · · · , me,
ci(x) −yi = 0,
i = me + 1, ..., m,
yi ≥0,
i = me + 1, ..., m.

492
CHAPTER 10. PENALTY FUNCTION METHODS
Compare the augmented Lagrange function for the reformulated problem and
(10.4.4).
9. Prove (10.5.6).
10. Prove Lemma 10.6.2.

Chapter 11
Feasible Direction Methods
11.1
Feasible Point Methods
A feasible point method requires that all iterate points xk generated are
feasible points of the constraints. For general constrained optimization prob-
lems (8.1.1)-(8.1.3), given a current iterate xk ∈X, if we can ﬁnd a descent
direction d which is also a feasible direction at xk, namely
dT ∇f(xk) < 0,
(11.1.1)
d ∈FD (xk, X),
(11.1.2)
there must exist new feasible points in the form of xk +αd with the property
that f(xk + αd) < f(xk). Here FD (xk, X) is deﬁned by Deﬁnition 8.2.1. A
direction d satisfying (11.1.1)-(11.1.2) is called a feasible descent direction at
xk.
Let c1 ∈(0, 1) be a given positive constant, xk be any point in the feasible
set X, and d be a vector that satisﬁes (11.1.1)-(11.1.2). We call α a feasible
point Armijo step along direction d at point xk if α > 0 satisﬁes
f(xk + αd) ≤f(xk) + αc1dT ∇f(xk),
(11.1.3)
and
f(xk + 2αd) > f(xk) + 2αc1dT ∇f(xk)
(11.1.4)
holds when xk + 2αd ∈X.

494
CHAPTER 11. FEASIBLE DIRECTION METHODS
Lemma 11.1.1 Assume that xk ∈X and d satisﬁes (11.1.1)-(11.1.2). Let
α be a feasible point Armijo step along direction d at point xk, then
f(xk + αd) ≤f(xk) −c1(1 −c1)
M

dT ∇f(xk)
∥d∥2
2
(11.1.5)
if xk + 2αd ∈X, and
f(xk + αd) ≤f(xk) + c1
Γ(xk)
2∥d∥2
dT ∇f(xk),
(11.1.6)
if xk + 2αd ̸∈X, where M = max0≤t≤2 ∥∇2f(xk + td)∥2 and Γ(¯x) is the
distance from ¯x to the set of all infeasible points, namely
Γ(¯x) = inf
y̸∈X ∥¯x −y∥.
(11.1.7)
Proof.
First assume that xk + 2αd ∈X. It follows from (11.1.4) and
Taylor expansion that
2αc1dT ∇f(xk)
<
2αdT ∇f(xk) + 1
2(2αd)T ∇2f(xk + ηk2αd)(2αd)
≤
2αdT ∇f(xk) + 2α2M∥d∥2
2,
(11.1.8)
where ηk ∈(0, 1). From the above inequality we can obtain that
α > −(1 −c1)
M∥d∥2
2
dT ∇f(xk).
(11.1.9)
Inequality (11.1.15) follows from the above relation and (11.1.3).
Now we consider the case when xk + 2αd ̸∈X.
It follows from the
deﬁnition of Γ(x) that
2α∥d∥2 > Γ(xk).
(11.1.10)
Thus, α > Γ(xk)
2∥d∥2 . This inequality and condition (11.1.3) imply (11.1.6).
2
The algorithm given below is a simple algorithm for calculating a feasible
point Armijo step. It tries to ﬁnd an acceptable step by repeatedly doubling
or halving the step.
Algorithm 11.1.2

11.1. FEASIBLE POINT METHODS
495
Step 1. Given x ∈X, d ∈DF(x, X) and dT ∇f(x) < 0;
given c1 ∈(0, 1);
let αmax = +∞, α = 1.
Step 2. if
f(x + αd) > f(x) + c1αdT ∇f(x),
or x + αd ̸∈X go to Step 3;
if αmax < +∞then stop;
α := 2α; Go to Step 2.
Step 3. αmax := α; α := α/2; Go to Step 2.
2
It is easy to see that Algorithm 11.1.2 terminates after ﬁnitely many
iterations with a feasible point Armijo step unless x + 2kd ∈X for all k
and f(x + 2kd) →−∞. Instead of simply doubling or halving the trial step,
we can also use quadratic or cubic interpolations in the above algorithm to
accelerate the convergence speed.
For any x ∈X and d ∈FD (x, X), we call the step α∗> 0 that satisﬁes
α∗:
min
α>0
x+αd∈X
f(x + αd)
(11.1.11)
a feasible point exact line search step.
Lemma 11.1.3 Assume that x ∈X, d ∈FD (x, X), and α∗satisﬁes (11.1.11).
It follows that
f(x) −f(x + α∗d) ≥
1
2M

dT ∇f(x)
∥d∥2
2
,
(11.1.12)
or
f(x) −f(x + α∗d) ≥−Γ(x)
2∥d∥2
dT ∇f(x),
(11.1.13)
where M = maxt≥0 ∥∇2f(x + td)∥2 and where Γ(x) is deﬁned by (11.1.7).
Proof.
From Taylor expansion, it follows that
f(x + αd) ≤f(x) + αdT ∇f(x) + M
2 ∥d∥2
2α2 ∆= φ(α).
(11.1.14)
Let α0 = −dT ∇f(x)/(M∥d∥2
2). If x + α0d ∈X, we have that

496
CHAPTER 11. FEASIBLE DIRECTION METHODS
f(x + α∗d)
≤
f(x + α0d) ≤φ(α0)
=
f(x) −
1
2M

dT ∇f(x)
∥d∥2
2
.
(11.1.15)
If x + α0d /∈X, it follows that α0 ≥Γ(x)/∥d∥. From the convexity of φ(α),
we can show that
f(x) −f(x + α∗d)
≥
sup
0<α<Γ(x)/∥d∥2
[f(x) −f(x + αd)]
≥
sup
0<α<Γ(x)/∥d∥2
[f(x) −φ(α)]
=
f(x) −φ[Γ(x)/∥d∥2]
≥
Γ(x)
∥d∥2α0
[f(x) −φ(α0)]
=
−Γ(x)
2∥d∥2
dT ∇f(x).
(11.1.16)
The above two inequalities indicate that the lemma is true.
2
Having the technique of searching along a feasible direction in the feasible
region, we can solve a constrained optimization problem iteratively as long
as we can ﬁnd a feasible descent direction in every iteration. However, it is
not always possible to ﬁnd a feasible descent direction. For example, for the
constraint
c(x, y) = y −x2 = 0,

x
y

∈ℜ2,
(11.1.17)
FD((x, y), X) = Φ at every feasible point.
Therefore no feasible descent
direction exists at any feasible point. Fortunately, when the feasible set X is
convex, at any point x ∈X there exists a feasible descent direction provided
that x is not a KKT point. We write this result in the form of a lemma as
follows.
Lemma 11.1.4 Assume that x ∈X, X is a convex set and f(x) is a convex
function. Then there exists a feasible descent direction at x if and only if x
is not a minimizer of problem (8.1.1)-(8.1.3).
Proof.
It is obvious that there exists no feasible descent direction at x if
x is a minimizer.

11.1. FEASIBLE POINT METHODS
497
Now assume that x is not a minimizer, then there exists an ˆx ∈X such
that
f(ˆx) < f(x).
(11.1.18)
Because f(x) is a convex function, it follows from (11.1.18) that
dT ∇f(x) < 0,
where d = ˆx −x. Because of the convexity of X, d ∈FD (x, X). Therefore
d is a feasible descent direction.
2
A general algorithm that uses feasible descent directions is given as fol-
lows.
Algorithm 11.1.5
Step 1. Given initial point x1 ∈X, k := 1;
Step 2. If no vector d satisﬁes (11.1.1)-(11.1.2) then stop;
ﬁnd dk that satisﬁes (11.1.1)-(11.1.2);
Step 3. Carry out a certain feasible point search, obtaining αk > 0.
Step 4. xk+1 = xk + αkdk; k := k + 1; Go to Step 2.
We can use a feasible point exact line search or a feasible point Armijo
search to obtain αk in Step 3 of the above algorithm.
From example (11.1.17), even if Algorithm 11.1.5 terminates, it may not
stop at a stationary point. However, when the objective function f(x) is
convex and when the feasible set is convex, xk must be the optimal solution
if Algorithm 11.1.5 terminates at iteration k.
An important issue is the choice of dk that satisﬁes (11.1.1)-(11.1.2).
Consider the very special case when X = ℜn.
Let f(x) be a uniformly
convex function deﬁned on ℜn. Assume that dk is the search direction at the
k-th iteration satisfying
dT
k ∇f(xk) < 0.
(11.1.19)
Let θk be the angle between dk and the steepest descent direction −∇f(xk),
namely
cos θk = −
dT
k ∇f(xk)
∥dk∥2∥∇f(xk)∥.
(11.1.20)

498
CHAPTER 11. FEASIBLE DIRECTION METHODS
Lemma 11.1.6 For an unconstrained optimization problem, assume that the
objective function is twice continuously diﬀerentiable and uniformly convex
and that a line search algorithm with xk+1 = xk + αkdk and ∥∇f(xk)∦= 0
for all k, satisﬁes
∞

k=1
cos2 θk < +∞,
(11.1.21)
where cos θk is deﬁned by (11.1.20). Then
lim
k→∞inf ∥∇f(xk)∥> 0.
(11.1.22)
Proof.
Because f(x) is uniformly convex, there exists x∗such that
f(x∗) = min
x∈ℜ
n f(x).
(11.1.23)
It is obvious that (11.1.22) is equivalent to
lim
k→∞f(xk) > f(x∗).
(11.1.24)
Deﬁne X1 = {x|f(x) ≤f(x1)} and
m1
=
min
x∈X1 min
∥d∥2=1 dT ∇2f(x)d,
(11.1.25)
M1
=
max
x∈X1 max
∥d∥2=1 dT ∇2f(x)d.
(11.1.26)
It can be shown that 0 < m1 ≤M1 < +∞because f(x) is uniformly convex.
Therefore,
f(xk) −f(xk+1)
≤
f(xk) −min
t>0 f(xk + tdk)
≤
1
2m1
∥∇f(xk)∥2
2 cos2 θk
≤
cos2 θk
2m1
(M1∥xk −x∗∥2)2
≤
cos2 θk
2
 M1
m1
!2
[f(xk) −f(x∗)].
(11.1.27)
Consequently, it follows that
f(xk+1) −f(x∗) ≥

1 −M2
1
2m2
1
cos2 θk

[f(xk) −f(x∗)],
(11.1.28)

11.1. FEASIBLE POINT METHODS
499
for all k. Assumption (11.1.21) implies the existence of k0 such that
M2
1
2m2
1
cos2 θk < 1,
∀k ≥k0.
(11.1.29)
Because ∥∇f(xk0)∦= 0, we have that f(xk0)−f(x∗) = δ > 0. From (11.1.21)
there exists η > 0 such that
0∞
j=k0

1 −M2
1
2m2
1
cos2 θk

≥η > 0.
(11.1.30)
Thus, it follows from (11.1.28) and (11.1.30) that
f(xk) −f(x∗) ≥ηδ > 0
for all k ≥k0. This implies that (11.1.24).
2
The above lemma tells us that we require
∞

k=1
cos2 θk = +∞,
(11.1.31)
to ensure the algorithm converging to a stationary point.
Similar to the steepest descent direction, we can deﬁne the feasible steep-
est descent direction as follows.
Deﬁnition 11.1.7 Let x ∈X; if a vector d in the closure of FD(x, X) solves
min
d∈FD (x,X)
d̸=0
dT ∇f(x)
∥d∥2
,
(11.1.32)
it is called a feasible steepest descent direction.
Because FD(x, X) may not be a closed set, the minimum of (11.1.32)
can not be reached by any d ∈FD (x, X). Thus, a feasible steepest descent
direction may not belong to FD(x, X). Therefore, it is not easy to generate
the steepest direction directly to constrained optimization by simply making
the steepest descent direction “feasible”.
Consider the inequality constrained optimization problem
min f(x),
(11.1.33)

500
CHAPTER 11. FEASIBLE DIRECTION METHODS
s.t. ci(x) ≥0
i = 1, · · ·, m.
(11.1.34)
Let xk ∈X. It is obvious that I(xk) = {i|ci(xk) = 0}. In order to ﬁnd a
feasible descent direction at the k-th iteration, we consider the approximate
subproblem
min αdT ∇f(xk),
(11.1.35)
s.t. xk + αd ∈X.
(11.1.36)
As the aim for constructing this subproblem is to ﬁnd a search direction,
we can assume that ∥αd∥is very small. When ∥αd∥is suﬃciently small,
(11.1.36) is equivalent to
cj(xk + αd) ≥0,
j ∈I(xk).
(11.1.37)
The above inequalities hold if we require that
αdT ∇cj(xk) −1
2Mα2∥d∥2
2 ≥0,
j ∈I(xk),
(11.1.38)
where M > 0 is an upper bound for
max
x∈X max
j∈I(xk) ∥∇2cj(x)∥2.
(11.1.39)
Replacing αd by d, we can obtain the following subproblem
min dT ∇f(xk),
(11.1.40)
s.t. dT ∇ci(xk) −M
2 ∥d∥2
2 ≥0,
i ∈I(xk).
(11.1.41)
By further replacing d by Md, the above problem can be rewritten as
min dT ∇f(xk),
(11.1.42)
s.t. dT ∇ci(xk) −1
2∥d∥2
2 ≥0,
i ∈I(xk).
(11.1.43)
The dual problem for the above problem is
max
λ
min
d∈ℜ
n
⎡
⎣dT ∇f(xk) −

i∈I(xk)
λi
 
dT ∇ci(xk) −1
2∥d∥2
2
!⎤
⎦
(11.1.44)
s.t. λi ≥0,
i ∈I(xk).
(11.1.45)

11.1. FEASIBLE POINT METHODS
501
The above problem can be written in the equivalent form
minλ
∇f(xk) −
i∈I(xk) λi∇ci(x)

2
2

i∈I(xk) λi
,
(11.1.46)
s.t.
λi ≥0,
i ∈I(xk),

i∈I(xk)
λi > 0.
(11.1.47)
Deﬁne
d(λ) = −
1

i∈I(xk) λi
⎛
⎝∇f(xk) −

i∈I(xk)
λi∇ci(xk)
⎞
⎠.
(11.1.48)
The objective function in (11.1.46) can be written as
φ(λ) =

i∈I(xk)
λi∥d(λ)∥2
2.
(11.1.49)
Direct calculations show that
∇φ(λ) =
⎡
⎢⎣
2d(λ)T ∇ck1(xk) −∥d(λ)∥2
2
...
2d(λ)T ∇ckI(xk) −∥d(λ)∥2
2
⎤
⎥⎦,
(11.1.50)
where {k1, k2, · · ·, kI} are the elements of I(xk). Furthermore, we have that
∇2φ(λ) =
2

i∈I(xk) λi
T(λ)T T(λ),
(11.1.51)
where
T(λ) = (d(λ) −∇ck1(xk), d(λ) −∇ck2(xk), · · ·, d(λ) −∇ckI(xk)). (11.1.52)
Thus, φ(λ) is a convex function. Let λ(k) be a solution of (11.1.46)-(11.1.47),
then d(λ(k)) is a solution of problem (11.1.40)-(11.1.41). In that case, xk is
a KKT point of (11.1.33)-(11.1.34) if d(λ(k)) = 0, and d(λ(k)) is a feasible
descent direction at xk satisfying
d(λ(k))T ∇f(xk) =
∥d(λ(k))∥2
2

i∈I(xk) λ(k)
i
< 0,
(11.1.53)

502
CHAPTER 11. FEASIBLE DIRECTION METHODS
if d(λ(k)) ̸= 0.
It is not diﬃcult to show that subproblem (11.1.40)-(11.1.41) has nonzero
minimum if and only if
dT ∇f(xk)
<
0,
(11.1.54)
dT ∇ci(xk)
>
0,
i ∈I(xk)
(11.1.55)
has a solution. That is to say, when (11.1.54)-(11.1.55) has a solution we
can obtain a feasible descent direction by solving (11.1.40)-(11.1.41). On the
other hand, if (11.1.54)-(11.1.55) has no solution, similar to Lemma 8.2.5 it
can be shown that there exist λ∗
i (i ∈I(xk)) ≥0 and λ∗
0 ≥0 such that
λ∗
0∇f(xk) −

i∈I(xk)
λ∗
i ∇ci(x∗) = 0,
(11.1.56)
and that 
i∈I(xk) λ∗2
i
+ λ∗
0 ̸= 0. Therefore we know that xk is a Fritz John
point of the original optimization problem (11.1.33)-(11.1.34).
Another subproblem for ﬁnding a feasible descent direction is directly
based on (11.1.54)-(11.1.55), having the form:
min σ
(11.1.57)
s.t. dT ∇f(xk)
≤
+σ,
(11.1.58)
dT ∇ci(xk)
≥
−σ,
i ∈I(xk),
(11.1.59)
∥d∥
≤
1.
(11.1.60)
It is easy to see that the minimum of the above subproblem σ∗= 0 if and
only if (11.1.54)-(11.1.55) has no solutions.
11.2
Generalized Elimination
Consider the equality constrained problem
min
f(x)
(11.2.1)
s.t.
c(x) = 0,
(11.2.2)
where c(x) = (c1(x), · · ·, cm(x))T . Assume that we have a certain partition
on the variable x:
x =

xB
xN

,
(11.2.3)

11.2. GENERALIZED ELIMINATION
503
where xB ∈ℜm, xN ∈ℜn−m. Therefore (11.2.2) can be written as
c(xB, xN) = 0.
(11.2.4)
Suppose that we can solve xB from (11.2.4), namely
xB = φ(xN),
(11.2.5)
then (11.2.1)-(11.2.2) is equivalent to
min
xN∈ℜ
n−m f(xB, xN) = f(φ(xN), xN) = ˜f(xN).
(11.2.6)
The vector
˜g(xN) = ∇xN ˜f(xN)
(11.2.7)
is called the reduced gradient. It is easy to verify that
˜g(xN) =
∂
∂xN
f(xB, xN) + ∂xT
B
∂xN
∂
∂xB
f(xB, xN).
(11.2.8)
From (11.2.4) we can see that ∂xT
B
∂xN satisﬁes that
∂xT
B
∂xN
∂
∂xB
c(xB, xN)T +
∂
∂xN
c(xB, xN)T = 0.
(11.2.9)
If ∂cT
∂xB is nonsingular, the above two equations imply that
˜g(xN)
=
∂f(xB, xN)
∂xN
−∂c(xB, xN)T
∂xN

∂c(xB, xN)T
∂xB
−1 ∂f(xB, xN)
∂xB
. (11.2.10)
Therefore, the reduced gradient can be expressed as the gradient of the La-
grangian function at the reduced space:
˜g(xN) =
∂
∂xN
[f(x) −λT c(x)],
(11.2.11)
where λ is a multiplier satisfying
∂f(x)
∂xB
= ∂cT (x)
∂xB
λ.
(11.2.12)

504
CHAPTER 11. FEASIBLE DIRECTION METHODS
In other words, when the Lagrange multiplier λ is chosen as

∂cT (xB)
∂xB
−1 ∂f(x)
∂xB
,
(11.2.13)
we have that
∇xL(x, λ) =

0
˜g(xN)

.
(11.2.14)
Therefore, the reduced gradient can be viewed as the nonzero part of the
gradient of the Lagrangian function.
Using the reduced gradients, we can construct line search directions for
the unconstrained problem (11.2.6). For example, we can use the steepest
descent direction
¯dk = −˜g((xN)k)
(11.2.15)
or the quasi-Newton direction
¯dk = −B−1
k ˜g((xN)k).
(11.2.16)
Here the subscript k indicates the iterate number, Bk is an approximate
Hessian matrix which can be updated from iteration to iteration(for example,
by BFGS formula). It is worth pointing out that carrying out a line search
min
α≥0 f(φ((xN)k + α ¯dk),
(xN)k + α ¯dk)
(11.2.17)
on the unconstrained problem (11.2.6) is equivalent to carrying out a curve
search on the original objective function f(x) along the following curve:
c(xB, (xN)k + α ¯dk) = 0.
(11.2.18)
Because the function φ(x) is not known explicitly, for every trial α we need
to solve (11.2.18) to obtain
xB = φ((xN)k + α ¯dk)
(11.2.19)
when carrying out line searches (11.2.17). This can be done by an approxi-
mate Newton’s method, namely
x(0)
B
=
(xB)k,
(11.2.20)
x(i+1)
B
=
x(i)
B −

∂c(xk)T
∂xB
−1
c(x(i)
B , (xN)k + α ¯dk).
(11.2.21)

11.2. GENERALIZED ELIMINATION
505
Because Newton’s method converges quadratically, usually an acceptable xB
will be obtained after applying (11.2.21) for a few iterations. If x(i)
B does
not converge after some iterations, α should be reduced to continue the line
search procedure.
The following is a general framework of the variable elimination method.
Algorithm 11.2.1 (Variable Elimination Method)
Step 1. Given a feasible point x1 ∈X, ϵ ≥0, k = 1;
Step 2. Compute
∂c(xk)T
∂x
=

AB
AN

,
(11.2.22)
where the partition satisﬁes that AB is nonsingular.
Compute λ by (11.2.12), and ˜gk by (11.2.11).
Step 3. If ∥˜gk∥≤ϵ then Stop;
Generate a feasible descent direction ¯dk satisfying
¯dT
k ˜gk < 0.
(11.2.23)
Step 4. Carry out line search (11.2.17) obtaining αk > 0,
Let xk+1 = (φ((xN)k + αk ¯dk), (xN)k+ αk ¯dk),
k := k + 1; go to Step 2.
It is very easy to see that the above algorithm is in fact a descent method
for the unconstrained optimization problem (11.2.6). The only thing that we
should keep in mind is that the partition (xB, xN) may diﬀer from iteration to
iteration. Using the convergence results of descent methods for unconstrained
optimization, we can easily establish the following result.
Theorem 11.2.2 Assume that f(x) and c(x) are twice continuously diﬀer-
entiable. If [(∇c(x)T )T ∇c(x)T ]−1 is bounded above uniformly on the feasible
set X, Algorithm 11.2.1 with exact line searches and the assumption

cos2⟨¯dk, ˜gk⟩= ∞
(11.2.24)
ensures that
lim inf
k→∞∥(∇f(xk) −∇c(xk)T λk)∥= 0,
(11.2.25)

506
CHAPTER 11. FEASIBLE DIRECTION METHODS
or
lim
k→∞f(xk) = −∞,
(11.2.26)
where λk = [∇c(xk)T ]+∇f(xk).
Let ¯dk = −˜gk, then (11.2.23) and (11.2.24) hold.
In that case, Algo-
rithm 11.2.1 is exactly the steepest descent method in the lower dimensional
space using the variable partition.
Consider any nonsingular matrix S ∈ℜn×n and variable transformation:
x = Sw.
(11.2.27)
We partition the variable w:
w =

wB
wN

,
(11.2.28)
where wB ∈ℜm, wN ∈ℜn−m. Using the constrained condition
c((S)BwB + (S)NwN) = 0
(11.2.29)
to eliminate variable wB, namely
wB = ¯φ(wN).
(11.2.30)
In this way, the optimization problem (11.2.1)-(11.2.2) is equivalent to
min
wN∈ℜ
n−m f(SBwB + SNwN) = ¯f(wN).
(11.2.31)
Provided that ST
B∇C(x)T is nonsingular, direct calculations give that
∇wN ¯f(wN) = ¯g(wN) = ST
N[∇f(x) −∇c(x)T λ],
(11.2.32)
where λ satisﬁes
ST
B[∇f(x) −∇c(x)T λ] = 0.
(11.2.33)
Thus, we have obtained an elimination method based on the variable
transformations at every iterations. This method is called the generalized
elimination method.
Algorithm 11.2.3 (General Elimination Method)

11.2. GENERALIZED ELIMINATION
507
Step 1. Given a feasible point x1 ∈X, ϵ ≥0, k = 1;
Step 2. Construct a nonsingular matrix Sk, and a partition Sk =
[(Sk)B ,
(Sk)N] such that (Sk)T
B∇c(xk)T nonsingular;
Compute λ by (11.2.33) and ¯gk by (11.2.32).
Step 3. If ∥¯gk∥≤ϵ then stop;
Generate a descent direction ¯dk satisfying
¯dT
k ¯gk < 0;
(11.2.34)
Step 4. Carry out line search:
min
α>0 f((Sk)B ¯φ((wk)N + α ¯dk) + (Sk)N[(wk)N + α ¯dk])
(11.2.35)
obtaining αk > 0; let
xk+1 = (Sk)B ¯φ((wk)N + αk ¯dk) + (Sk)N[(wk)N + αk ¯dk];
(11.2.36)
k := k + 1; go to Step 2.
2
In the algorithm, wk is a vector satisfying xk = Skwk. Similar to the
elimination method, for each trial step α > 0, we need to compute
wB = ¯φ((wk)N + α ¯dk),
(11.2.37)
which can be done by applying the approximate Newton’s method to the
nonlinear system
c((Sk)BwB + (Sk)N[(wk)N + α ¯dk]) = 0.
(11.2.38)
That is,
w(i+1)
B
=
w(i)
B −[(∇c(xk)T )T (Sk)B]−1c((Sk)Bw(i)
B
+
(Sk)N[(wk)N + α ¯dk]),
i = 1, 2, · · ·.
(11.2.39)
It is not diﬃcult to see that if Sk is the unit matrix in every iteration, the
generalized elimination method is exactly the original elimination method.
The variable increment xk+1 −xk of the generalized elimination method
in every iteration is actually the sum of two parts:
xk+1 = xk + d(1)
k
+ d(2)
k ,
(11.2.40)

508
CHAPTER 11. FEASIBLE DIRECTION METHODS
where
d(1)
k
=
αk(Sk)N ¯dk,
(11.2.41)
d(2)
k
=
(Sk)B[¯φ((wk)N + αk ¯dk) −(wk)B].
(11.2.42)
The iteration process ﬁrst obtains the step d(1)
k
, then uses the approximate
Newton method along the direction d(2)
k
to ﬁnd a point in the feasible set, as
shown in Figure 11.2.1.
Figure 11.2.1 Iterative procedure of generalized elimination method
Looking at Figure 11.2.1, we see an undesirable property of such a process.
The iteration ﬁrst moves away from the feasible set, and then it comes back,
though the essential idea for feasible point methods is to force all iterate
points inside the feasible region. Except for very special constraints, it is
unavoidable to use the technique of moving away and coming back if we
require that all iterate points are feasible. But, how to make the “moving
away” as small as possible?
An intuitive answer is to choose d(1)
k
to be
a linearized feasible direction at xk. It is reasonable to believe that such
a d(1)
k
would make xk + d(1)
k
closer to the feasible region, consequently the
approximate Newton’s method will bring xk +d(1)
k
back to the feasible region
more quickly. Figure 11.2.2 illustrates the above discussions.

11.3. GENERALIZED REDUCED GRADIENT METHOD
509
Figure 11.2.2
When d(1)
k
is a linearized feasible direction, the method is called a feasible
direction method. It is obvious that if
(Sk)T
N∇c(xk)T = 0
(11.2.43)
holds, d(1)
k
is a linearized feasible direction. Because of this, feasible direction
methods can also be viewed as special generalized elimination methods.
11.3
Generalized Reduced Gradient Method
The generalized reduced gradient method (GRG method) is in fact Algo-
rithm 11.2.1 with ¯dk = −˜gk. It is the steepest descent method in the reduced
space.
In each iteration, the line search can be the Armijo rule, namely reducing
the trial step repeatedly until an acceptable one is obtained. The condition
for accepting the new point can be the simple reduction
f(xk+1) < f(xk).
(11.3.1)
In each iteration, we apply (11.2.21) for at most N times to compute xB,
where N is a given positive number. If the approximate Newton’s method has
not converged after N iterations, we reduce the trial step α and repeat the
iteration. Because the quadratic convergence of Newton’s method, normally
one or two iterations of (11.2.21) will return a suﬃciently accurate feasible
point xk+1. Therefore in practice we can choose N between 3 to 6.
The algorithm is the generalized reduced gradient method with Armijo
line searches requiring simple reductions.
Algorithm 11.3.1 (Generalized Reduced Gradient Method)

510
CHAPTER 11. FEASIBLE DIRECTION METHODS
Step 1. Given a feasible point x1 ∈X, ϵ ≥0, ¯ϵ > 0; positive integer
M; k := 1.
Step 2. Compute
∇c(xk)T =

AB
AN

,
(11.3.2)
where the partition satisﬁes that AB ∈ℜm×m is nonsingular;
Compute λ from (11.2.12) and ˜gk from (11.2.11).
Step 3. If ∥˜gk∥≤ϵ then stop;
let ¯dk = −˜gk; and α = α(0)
k
> 0.
Step 4. xN = (xk)N + α ¯dk;
xB = (xk)B; j := 0.
Step 5. xB = xB −A−T
B c(xB, xN);
compute c(xB, xN);
if ∥c(xB, xN)∥≤¯ϵ then go to Step 7;
j := j + 1; if j < M go to Step 5.
Step 6. α := α/2, go to Step 4.
Step 7. If f(xB, xN) ≥f(xk) then go to Step 6.
xk+1 = (xB, xN), k := k + 1; go to Step 2.
This algorithm is in fact a gradient method. Thus the simple reduction
(11.3.1) on the objective function can not guarantee convergence. In other
words, we can not show that the iterates generated by Algorithm 11.3.1
converges to a KKT point of the original optimization problem (11.2.1)-
(11.2.2). There are two ways to overcome this. The ﬁrst one is to use a
better line search condition. For example, we can replace the simple reduction
condition (11.3.1) by the Wolfe line search condition
˜f((xk)N + αk ¯dk) ≤˜f((xk)N) + βαk ¯dT
k ˜gk,
(11.3.3)
where α is the step length, β ∈(0, 1) is a positive constant, and ˜f(xN) is
deﬁned by (11.2.6). Condition (11.3.3) can be written as
f(xk+1) ≤f(xk) −αkβ∥˜gk∥2
2.
(11.3.4)

11.3. GENERALIZED REDUCED GRADIENT METHOD
511
Thus if we replace the condition f(xB, xN) ≥f(xk) for rejecting a new point
in Step 7 of Algorithm 11.3.1 by
f(xB, xN) > f(xk) −αβ∥˜gk∥2
2,
(11.3.5)
the Wolfe line search condition (11.3.3) is satisﬁed. Another way is to require
that the initial trial step length α(0)
k
in Step 3 of the algorithm satisﬁes that
α(0)
k
∥˜gk∥→0,
(11.3.6)
∞

k=1
α(0)
k
∥˜gk∥= +∞.
(11.3.7)
Similar to convergence analyses for unconstrained optimization methods,
we can prove the following convergence results.
Theorem 11.3.2 Assume that f(x), c(x) are twice continuously diﬀeren-
tiable, that the matrices A−1
B in Step 2 of Algorithm 11.3.1 are bounded above
uniformly, and that α(0)
k
in Step 3 satisﬁes that (α(0)
k )−1 is uniformly bounded.
If the condition f(xB, xN) ≥f(xk) in Step 7 is replaced by (11.3.5), if ϵ = 0
and if Algorithm 11.3.1 does not terminate, it follows that either
lim
k→∞∥˜gk∥= 0
(11.3.8)
or
lim
k→∞f(xk) = −∞.
(11.3.9)
Theorem 11.3.3 Assume that f(x), c(x) are twice continuously diﬀeren-
tiable, that the matrices A−1
B in Step 2 of Algorithm 11.3.1 are bounded above
uniformly, and that α(0)
k
in Step 3 satisﬁes (11.3.6) and (11.3.7). Then if
ϵ = 0 and if Algorithm 11.3.1 does not terminate, either (11.3.8) or (11.3.9)
holds.
One advantage of the generalized reduced gradient method is the dimen-
sion of the problem is reduced due to variable elimination. The method can
also make good use of the special structure of the problem such as sparsity
and constant coeﬃcients so that λ and ˜g can be computed quickly. Simi-
larly, if AB is sparse, sparse linear system solvers can be used when using

512
CHAPTER 11. FEASIBLE DIRECTION METHODS
the approximate Newton’s method to obtain xB. Therefore, for large-scale
nonlinear programming problems there are many linear constraints or with
sparse structures, the generalized reduced gradient method is one of the most
eﬃcient methods.
Because one or more systems of nonlinear equations have to be solved in
the generalized reduced gradient method, its computation cost is very high
if the matrix AB is not sparse and without special structures.
11.4
Projected Gradient Method
From the discussions at the end of Section 11.2, in order to choose d(1)
k
as a
linearized feasible direction in the generalized elimination method, Sk should
satisfy
(Sk)T
N∇c(xk)T = 0.
(11.4.1)
Consider the case that the steepest descent direction is used in the generalized
elimination method, namely
¯dk = −¯gk.
(11.4.2)
From (11.2.41) it follows that
d(1)
k
= −αk(Sk)N(Sk)T
N∇f(xk).
(11.4.3)
Obviously, (Sk)N(Sk)T
N is a linear projection from ℜn to the subspace spanned
by the columns of (Sk)N. Suppose Ak = ∇c(xk)T is full column rank, the
subspace spanned by the columns of (Sk)N is the null space of AT
k . Therefore
the direction deﬁned by (11.4.3) is actually the projection of the negative
gradient of the objective function to the null space of the Jacobi matrix. If
Sk satisﬁes
(Sk)T
N(Sk)N = I,
(11.4.4)
(Sk)N(Sk)T
N is an orthogonal projector, and
Pk
=
(Sk)N(Sk)T
N
=
I −Ak(AT
k Ak)−1AT
k ,
(11.4.5)
when Ak has full column rank. In this case, Pk∇f(xk) is an orthogonal pro-
jection of ∇f(xk) to the null space of AT
k . Therefore, the generalized elimina-
tion method is a projected gradient method. In a practical implementation

11.4. PROJECTED GRADIENT METHOD
513
of the projected gradient method, we can use the QR factorization of Ak:
Ak = [Yk Zk]

Rk
0

.
(11.4.6)
It is easy to see that we can let (Sk)N = Zk. Hence,
¯gk = ZT
k gk
(11.4.7)
is the reduced gradient and
dk = −Zk¯gk = −ZkZT
k gk
(11.4.8)
is a projection of the negative gradient to the null space of AT
k , which is a
descent direction of f(x). Thus, we can choose αk such that
f(xk + αkdk) < f(xk).
(11.4.9)
The point xk + αkdk may be infeasible. A feasible point can be obtained by
the approximate Newton’s method
x(1)
k
=
xk + αkdk,
(11.4.10)
x(i+1)
k
=
x(i)
k −YkR−1
k c(x(i)
k ),
i = 1, 2, . . . .
(11.4.11)
When c(xi+1
k
) is suﬃciently small, we terminate (11.4.11) and set xk+1 =
x(i+1)
k
. The above iteration process is essentially (11.2.39) with (Sk)B = Yk.
If αk is suﬃciently small, we have that
∥xk+1 −(xk + αkdk)∥= O(α2
k),
(11.4.12)
therefore there exists αk > 0 such that
f(xk+1) < f(xk).
(11.4.13)
The algorithm given below is the projected gradient method with Armijo line
searches requiring simple reductions.
Algorithm 11.4.1 (Projected Gradient Method)
Step 1. Given a feasible point x1 ∈X, ϵ ≥0, ¯ϵ > 0, a positive
integer N, k := 1.

514
CHAPTER 11. FEASIBLE DIRECTION METHODS
Step 2. Compute the QR Factorization
∇C(xk)T = [Yk Zk]

Rk
0

;
¯gk = ZT
k ∇f(xk);
if ∥¯gk∥≤ϵ then stop;
dk = −Zk¯gk; set α = α(0)
k
> 0.
Step 3. y := xk + αdk; i := 0.
Step 4. y := y −YkR−1
k c(y);
if ∥C(y)∥≤¯ϵ and f(y) < f(xk) then go to Step 5;
i := i + 1; if i < N then go to Step 4;
α = α/2; go to Step 3.
Step 5. xk+1 := y, k := k + 1; go to Step 2.
Similar to the generalized reduced gradient method, Algorithm 11.4.1
needs to modify its line search conditions or to impose certain conditions on
the initial steplength in order to guarantee convergence.
For inequality constraints, active set technique can be used to obtain
feasible directions. However, one diﬃculty of the active set technique is the
zigzagging phenomenon, which was pointed out by Wolfe [350]. There are
many ways to overcome zigzagging in feasible direction methods. The main
idea for avoiding zigzagging is not to delete constraints from the active set
unless it is absolutely needed.
If the search direction dk = −Zk¯gk in the last line of Step 2 in Algo-
rithm 11.4.1 is replaced by
dk = −Zkzk,
(11.4.14)
where zk ∈ℜn−m is any vector that satisﬁes
zT
k ¯gk < 0,
(11.4.15)
then the algorithm is a general form of the linearized feasible direction method,
often called feasible direction method.
Based on our deﬁnitions, the search directions in a feasible direction
method is only a linearized feasible direction instead of a feasible direction.
An exception is the case when all the constraints are linear functions. In this

11.5. LINEARLY CONSTRAINED PROBLEMS
515
case, the linearized feasible directions are also feasible directions. Because
feasible direction methods were ﬁrst used for linearly constrained problems,
when they are generalized to nonlinear constraints they are still called feasible
direction methods. To be precise, this method, when applied to nonlinearly
constrained problems, should be called linearized feasible direction method
instead of feasible direction method.
For nonlinear constraints, normally linearized feasible directions are not
feasible directions. Therefore searching along a linearized feasible direction
may return an infeasible point. That is why Newton’s method or approximate
Newton’s method should be applied to bring the iterate back to the feasible
region before continuing the next search along a search direction and another
moving back to the feasible region. This procedure leads to the sawtooth
phenomenon, as indicated by Figure 11.4.1
Figure 11.4.1
11.5
Linearly Constrained Problems
Feasible direction methods are very eﬃcient for linearly constrained problems,
for example, for equality constrained problem
minx∈ℜ
n
f(x),
(11.5.1)
s.t.
AT x = b,
(11.5.2)
where b ∈ℜm, A ∈ℜn×m, rank(A) = m, and f(x) is a nonlinear function.
The search direction of a feasible direction method can be expressed by
dk = Z ¯dk,
(11.5.3)

516
CHAPTER 11. FEASIBLE DIRECTION METHODS
where ¯dk ∈ℜn−m, and Z ∈ℜn×(n−m) satisfying
AT Z
=
0,
(11.5.4)
¯dT
k ZT ∇f(xk)
<
0.
(11.5.5)
Speciﬁcally, we can let ¯dk = −ZT ∇f(xk), which leads to the following feasible
direction method based on steepest descent direction.
Algorithm 11.5.1
Step 1. Given a feasible point x1;
Compute Z such that AT Z = 0 and Rank(Z) = n −m;
k = 1, ϵ ≥0.
Step 2. dk = −ZZT ∇f(xk); if ∥dk∥≤ϵ then stop;
Carry out line search along dk obtaining αk > 0;
xk+1 = xk + αkdk; k := k + 1; go to Step 2.
The algorithm is actually a steepest descent method in the feasible region.
Thus, its convergence can be established under certain line search conditions.
When Z satisﬁes ZT Z = I, Algorithm 11.5.1 is a projected gradient method.
Now we discuss a projected gradient method for general linearly con-
strained optimization problems, which was proposed by Calamai and Mor´e
[50].
For a general linearly constrained optimization problem
minx∈ℜ
n
f(x),
(11.5.6)
s.t.
aT
i x = bi,
i ∈E,
(11.5.7)
aT
i x ≥bi,
i ∈I,
(11.5.8)
the feasible set X is
X = {x|aT
i x = bi, i ∈E; aT
i x ≥bi, i ∈I}.
(11.5.9)
Deﬁne the mapping P,
P(x) = arg min{∥z −x∥, z ∈X},
(11.5.10)
where arg min indicates any z ∈X that minimizes ∥z −x∥, ∥· ∥is a norm.
For simplicity, we assume that ∥· ∥is the Euclidean norm ∥· ∥2.

11.5. LINEARLY CONSTRAINED PROBLEMS
517
Consider the steepest descent method. The iterate xk+1 should be a point
on the straight line
¯xk(α) = xk −α∇f(xk).
(11.5.11)
But we need the iterate points on the feasible region, we use the projection
P to project the line (11.5.11) to X, obtaining the piecewise line
xk(α) = P[xk −α∇f(xk)].
(11.5.12)
We search along the piecewise line, namely ﬁnd αk > 0 such that
f(xk(αk)) ≤f(xk) + µ1(xk(αk) −xk)T ∇f(xk),
(11.5.13)
αk ≥γ1
or
αk ≥γ2 ¯αk > 0,
(11.5.14)
where ¯αk satisﬁes
f(xk(¯αk)) > f(xk) + µ2(xk(¯αk) −xk)T ∇f(xk).
(11.5.15)
Here, γ1, γ2, µ1, µ2 are positive constants and µ1, µ2 ∈(0, 1).
The method of Calamai and Mor´e can be stated as follows.
Algorithm 11.5.2
Step 1. Given a feasible point x1, µ ∈(0, 1), γ > 0, α0 = 1, k := 1;
Step 2. αk := max{2αk−1, γ}.
Step 3. if (11.5.13) holds go to Step 4;
αk = αk/4; go to Step 3;
Step 4. xk+1 := xk(αk); k := k + 1; go to Step 2.
It is easy to see that αk computed by Algorithm 11.5.2 satisﬁes (11.5.13)-
(11.5.15) for µ2 = µ1 = µ, γ1 = γ, γ2 = 1/4.
From the deﬁnition of P(x), for any x ∈ℜn it follows that
(x −P(x))T (z −P(x)) ≤0,
∀z ∈X.
(11.5.16)
Let x = xk −αk∇f(xk) and z = xk in the above relation, then we obtain
that
(xk −αk∇f(xk) −xk+1)T (xk −xk+1) ≤0.
(11.5.17)

518
CHAPTER 11. FEASIBLE DIRECTION METHODS
Thus, it follows from (11.5.13) and (11.5.17) that
f(xk) −f(xk+1) ≥µ1
∥xk+1 −xk∥2
2
αk
.
(11.5.18)
First we have the following lemmas.
Lemma 11.5.3 Assume that f(x) is continuously diﬀerentiable and bounded
below on the feasible set X. If ∇f(x) is uniformly continuous on X, the
iterates generated by Algorithm 11.5.2 satisfy
lim
k→∞
∥xk+1 −xk∥
αk
= 0.
(11.5.19)
Proof.
If the lemma is not true, there is an inﬁnite subsequence K0 such
that
∥xk+1 −xk∥
αk
≥δ,
∀k ∈K0
(11.5.20)
where δ > 0 is a positive constant independent of k. It follows from the above
relation and (11.5.18) that for all k ∈K0 we have that
f(xk) −f(xk+1) ≥δµ1∥xk+1 −xk∥≥δ2µ1αk.
(11.5.21)
Because f(x) is bounded below on the feasible set and all xk are feasible, it
follows that
∞

k=1
[f(xk) −f(xk+1)] < +∞.
(11.5.22)
Inequalities (11.5.21) and (11.5.22) imply that
lim
k→∞
k∈K0
∥xk+1 −xk∥= lim
k→∞
k∈K0
αk = 0.
(11.5.23)
Therefore the ﬁrst condition of (11.5.14) does not hold for suﬃciently large
k ∈K0, which shows that
αk ≥γ2¯αk,
(11.5.24)
and that (11.5.15) holds. Using the monotonically non-increasing property
of
Ψ(α) = ∥P(x + αd) −x∥
α
,
α > 0
(11.5.25)

11.5. LINEARLY CONSTRAINED PROBLEMS
519
and relation (11.5.24), we can prove that
∥xk −xk(¯αk)∥
¯αk
≥min
6
1, 1
γ2
7∥xk −xk(αk)∥
αk
.
(11.5.26)
Thus, letting x = xk −¯αk∇f(xk) and z = xk in (11.5.16) gives that
−(xk(¯αk) −xk)T ∇f(xk) ≥∥xk −xk(¯αk)∥2
¯αk
≥min
*
1, 1
γ2
+
δ∥xk −xk(¯αk)∥
(11.5.27)
for all suﬃciently large k ∈K0.
The uniform continuity of ∇f(x) on X
implies that
f(xk(¯αk)) −f(xk) = (xk(¯αk) −xk)T ∇f(xk) + o(∥xk(¯αk) −xk∥). (11.5.28)
It follows from (11.5.15) and (11.5.28) that
−(xk(¯αk) −xk)T ∇f(xk) ≤o(∥xk −xk(¯αk)∥).
(11.5.29)
The above inequality contradicts (11.5.27), which shows that the lemma is
true.
2
Lemma 11.5.4 A point x∗∈X is a KKT point of problem (11.5.6)-(11.5.8)
if and only if there exists ¯δ > 0 such that
P(x∗−α∇f(x∗)) = x∗
(11.5.30)
holds for all α ∈[0, ¯δ].
Proof.
Equation (11.5.30) is equivalent to
∥x∗−¯δ∇f(x∗) −x∗∥2
2 ≤∥x∗−¯δ∇f(x∗) −x∥2
2
(11.5.31)
holds for all x ∈X. Because X is a convex set, (11.5.31) is equivalent to
(x −x∗)∇f(x∗) ≥0
(11.5.32)
holds for all feasible points suﬃciently close to x∗. This means that x∗is the
minimizer of function xT ∇f(x∗) on X, which is equivalent to that x∗is a
KKT point of problem (11.5.6)-(11.5.8).
2
From the above two lemmas, we can easily establish the convergence result
of Algorithm 11.5.2.

520
CHAPTER 11. FEASIBLE DIRECTION METHODS
Theorem 11.5.5 Assume that f(x) is continuously diﬀerentiable on the fea-
sible set X. Then, any accumulation point x∗of {xk} generated by Algo-
rithm 11.5.2 is a KKT point of problem (11.5.6)-(11.5.8).
Proof.
If the theorem is not true, there exist a subsequence of {xk}
satisfying
lim
k∈K0
k→∞
xk = x∗,
(11.5.33)
and
P(x∗−¯δ∇f(x∗)) ̸= x∗,
(11.5.34)
where ¯δ > 0, K0 is a subset of {1, 2, · · ·}.
Because of (11.5.33), we can
assume that xk ∈S (k ∈K0), and S is a bounded closed set. Because ∇f(x)
is continuous on S, it is also uniformly continuous on S.
It follows from
Lemma 11.5.3 that
lim
k∈K0
k→∞
∥xk+1 −xk∥
αk
= 0.
(11.5.35)
From the continuity of ∇f(x) and (11.5.33)-(11.5.34), we can show that
lim
k∈k0
k→∞
∥xk(¯δ) −xk∥
¯δ
= ∥P(x∗−¯δ∇f(x∗)) −x∗∥
¯δ
> 0.
(11.5.36)
Because the function Ψ(α) deﬁned by (11.5.25) is monotonically non-increasing,
it follows from (11.5.35) and (11.5.36) that αk ≥¯δ holds for all suﬃciently
large k ∈K0. Therefore,
f(xk) −f(xk+1)
≥
−µ1(∇f(xk))T (xk(αk) −xk)
≥
−µ1(∇f(xk))T (xk(¯δ) −xk)
≥
µ1
∥xk(¯δ) −xk∥2
¯δ
.
(11.5.37)
Now it follows from (11.5.37) and (11.5.36) that
lim
k∈K0
k→∞
inf[f(xk) −f(xk+1)] > 0.
(11.5.38)
This contradicts the fact that limk→∞f(xk) = f(x∗). Therefore the theorem
is true.
2
Exercises

11.5. LINEARLY CONSTRAINED PROBLEMS
521
1. Assume that X is a convex polyhedral deﬁned by X = {x |
Ax ≥b}.
Show that ﬁnding a direction d satisfying (11.1.1)–(11.1.2) is a convex pro-
gramming problem and give its dual.
2. By direct elimination, ﬁnd the point on the ellipse deﬁned by the in-
tersection of the surface x + y = 1 and x2 + 2y2 + z2 = 1 which is nearest to
the origin.
3. Apply Newton’s method with the generalized elimination to solve the
problem
min
8x4
1 −x4
2
s.t.
x1 + x2 = 1.
4. For the above problem, at the point (3, −2)T , please give the projected
gradient and the projected Hessian. What are the projected gradient and the
projected Hessian at the solution?
5. Give the projected gradient algorithm for the box constrained problem
min
f(x)
s.t.
l ≤x ≤u.
6. Prove Theorem 11.3.2.
7. Assume the symmetric matrix B ∈ℜn×n is invertible and b ∈ℜn.
Prove that the matrix
ˆB =
 B
b
bT
β

(11.5.39)
is invertible if and only if β −bT B−1b ̸= 0.
And prove that, when ˆB is
invertible, there exist µ and u such that
ˆB−1 =
 B−1
0
0
0

+ µuuT .
(11.5.40)

Chapter 12
Sequential Quadratic
Programming
12.1
Lagrange-Newton Method
Consider the equality constrained optimization problem
min
x∈ℜ
n
f(x)
(12.1.1)
s.t.
c(x) = 0,
(12.1.2)
where c(x) = (c1(x), · · · , cm(x))T ∈ℜn. The Lagrangian function is
L(x, λ) = f(x) −λT c(x).
(12.1.3)
A point x is a KKT point of (12.1.1)-(12.1.2) if and only if there exists λ ∈ℜm
such that
∇xL(x, λ) = ∇f(x) −∇c(x)T λ = 0,
(12.1.4)
∇λL(x, λ) = −c(x) = 0.
(12.1.5)
The nonlinear system (12.1.4)-(12.1.5) requires x to be a stationary point of
the Lagrangian function. Therefore any method based on solving (12.1.4)-
(12.1.5) can be called a Lagrange method. For a given iterate point xk ∈ℜn
and an approximate Lagrange multiplier λk ∈ℜm, the Newton-Raphson step
for solving (12.1.4)-(12.1.5) is ((δx)k, (δλ)k), which satisﬁes

W(xk, λk)
−A(xk)
−A(xk)T
0
 
(δx)k
(δλ)k

= −

∇f(xk) −A(xk)λk
−c(xk)

,
(12.1.6)

524
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
where
A(xk)
=
∇c(xk)T ,
(12.1.7)
W(xk, λk)
=
∇2f(xk) −
m

i=1
(λk)i∇2ci(xk).
(12.1.8)
Consider the penalty function
P(x, λ) = ∥∇f(x) −A(x)λ∥2
2 + ∥c(x)∥2
2;
(12.1.9)
it is easy to show that (δx)k and (δλ)k deﬁned by (12.1.6) satisfy that
((δx)T
k , (δλ)T
k )∇P(xk, λk) = −2P(xk, λk) ≤0.
(12.1.10)
Here ∇P is the gradient of P in the space (x, λ). The method given below
is based on (12.1.6), hence it is called the Lagrange-Newton method.
Algorithm 12.1.1 (Lagrange-Newton Method)
Step 1. Given x1 ∈ℜn, λ1 ∈ℜm, β ∈(0, 1), ϵ ≥0, k := 1;
Step 2. Compute P(xk, λk); if P(xk, λk) ≤ϵ then stop;
solving (12.1.6) obtaining (δx)k and (δλ)k;
α = 1;
Step 3. if
P(xk +α(δx)k, λk +α(δλ)k) ≤(1−βα)P(xk, λk), (12.1.11)
then go to Step 4;
α = α/4, go to Step 3;
Step 4. xk+1 = xk + α(δx)k; λk+1 = λk + α(δλ)k;
k := k + 1; go to Step 2.
2
For the above algorithm, we have the following convergence result.
Theorem 12.1.2 Assume that f(x) and c(x) are twice continuously diﬀer-
entiable. If the matrix

W(xk, λk)
−A(xk)
−A(xk)T
0
−1
(12.1.12)
is uniformly bounded, then any accumulation point of {(xk, λk)} generated by
Algorithm 12.1.1 is a root of P(x, λ) = 0.

12.1. LAGRANGE-NEWTON METHOD
525
Proof.
Suppose that the theorem is not true, i.e., suppose (¯x, ¯λ) is an
accumulation point of {(xk, λk)} and
P(¯x, ¯λ) > 0.
(12.1.13)
Then there exists a subset K0 ⊆{1, 2, · · ·} which has inﬁnitely many elements
and satisﬁes that
lim
k∈K0
k→∞
xk = ¯x,
lim
k∈K0
k→∞
λk = ¯λ.
(12.1.14)
From the line search condition (12.1.11), we can see that
P(xk+1, λk+1) ≤(1 −βαk)P(xk, λk).
(12.1.15)
It follows from (12.1.13)-(12.1.15) that
lim
k∈K0
k→∞
αk = 0.
(12.1.16)
Therefore we have that
P(xk + ˆαk(δx)k, λk + ˆαk(δλ)k) > (1 −β ˆαk)P(xk, λk)
(12.1.17)
for all suﬃciently large k ∈K0, where ˆαk = 4αk ∈(0, 1). Let (δx, δλ) be the
solution of

W(¯x, ¯λ)
−A(¯x)
−A(¯x)T
0
 
δx
δλ

= −

∇f(¯x) −∇c(¯x)T ¯λ
c(¯x)

.
(12.1.18)
Because ˆαk →0, we can show that
lim
k∈K0
k→∞
P(¯x + ˆαkδx, ¯λ + ˆαkδλ) −P(¯x, ¯λ)
ˆαk
= −2P(¯x, ¯λ) < −P(¯x, ¯λ). (12.1.19)
From the uniform boundedness of (12.1.12) and the fact that (xk, λk) →
(¯x, ¯λ)(k ∈K0), it follows that ((δx)k, (δλ)k) →(δx, δλ).
Therefore, for
suﬃciently large k ∈K0 we have that
P(xk + ˆαk(δx)k, λk + ˆαk(δλ)k) −P(xk, λk)
ˆαk
≤−P(xk, λk).
(12.1.20)
Because β < 1, (12.1.20) contradicts (12.1.17). This implies that the theorem
is true.
2

526
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Theorem 12.1.3 Assume that f(x) and c(x) are twice continuously diﬀer-
entiable. If the matrix (12.1.12) is uniformly bounded, then any accumulation
point of {xk} generated by Algorithm 12.1.1 is a KKT point of (12.1.1)-
(12.1.2).
Proof.
If the theorem is not true, it follows from the monotonicity of
P(xk, λk) that
lim
k→∞P(xk, λk) > 0.
(12.1.21)
This limit and condition (12.1.11) imply that
∞
/
k=1
(1 −βαk) > 0.
(12.1.22)
The above relation indicates that
∞

k=1
αk < +∞.
(12.1.23)
Because

W(xk, λk)
−A(xk)
−A(xk)T
0
 
(δx)k
λk + (δλ)k

=

−∇f(xk)
c(xk)

,
(12.1.24)
there exists a positive constant γ > 0 such that
∥(δx)k∥+ ∥λk + (δλ)k∥≤γ(∥∇f(xk)∥+ ∥c(xk)∥).
(12.1.25)
Let ¯x be any accumulation point of {xk}. Deﬁne the set
Sδ = {x|∥x −¯x∥≤δ},
(12.1.26)
where δ > 0 is any given positive constant. From (12.1.25) we know that
there exists a constant η > 0 such that
∥(δx)k∥≤η
(12.1.27)
for all xk ∈Sδ. It follows from (12.1.23) that there exists ¯k such that
∞

k=¯k
αk < δ
2η.
(12.1.28)

12.1. LAGRANGE-NEWTON METHOD
527
Because ¯x is an accumulation point of {xk}, there exists ˆk > ¯k such that
∥xˆk −¯x∥< δ
2.
(12.1.29)
From (12.1.27)-(12.1.29) and the fact that ∥xk+1 −xk∥= αk∥(δx)k∥we have
that
xk ∈Sδ,
∀k ≥ˆk.
(12.1.30)
Therefore (12.1.27) holds for all k ≥ˆk. Thus, it follows from (12.1.23) that
lim
k→∞xk = ¯x.
(12.1.31)
This relation and the last theorem imply that there are no accumulation
points of {(xk, λk)}, which shows that
lim
k→∞∥λk∥= ∞.
(12.1.32)
Hence, it follows from (12.1.32) and (12.1.25) that
∥λk+1∥
=
∥λk + αk(δλ)k∥
=
∥(1 −αk)λk + αk(λk + (δλ)k)∥
=
(1 −αk)∥λk∥+ O(αk) < ∥λk∥
(12.1.33)
holds for all suﬃciently large k, which contradicts (12.1.32). This completes
our proof.
2
About the convergence rate of Algorithm 12.1.1, we have the following
result.
Theorem 12.1.4 Assume that the sequence {xk} generated by Algorithm 12.1.1
converges to x∗, if f(x) and c(x) are three times continuously diﬀerentiable
near x∗, A(x∗) is full column rank, and the second-order suﬃcient condition
is satisﬁed at x∗, then λk →λ∗, and


xk+1 −x∗
λk+1 −λ∗
 = O
⎛
⎝


xk −x∗
λk −λ∗

2⎞
⎠.
(12.1.34)
Proof.
Because Algorithm 12.1.1 is the Newton-Raphson method for
(12.1.4)-(12.1.5), and because the second-order suﬃcient condition implies
that the matrix

W(x∗, λ∗)
−A(x∗)
−A(x∗)T
0

(12.1.35)

528
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
is nonsingular, we have that


xk + (δx)k −x∗
λk + (δλ)k −λ∗
 = O
⎛
⎝


xk −x∗
λk −λ∗

2⎞
⎠
(12.1.36)
for all suﬃciently large k. The above relation, and the fact that f(x) and
c(x) are three times continuously diﬀerentiable imply that (12.1.11) holds for
α = 1. Therefore (12.1.34) holds.
2
It should be pointed out that (12.1.34) is not equivalent to the usual
quadratic convergence, which is
∥xk+1 −x∗∥= O(∥xk −x∗∥2).
(12.1.37)
For the analysis of the convergence rate of the iterates {xk}, we need the
following result.
Lemma 12.1.5 Under the assumptions of Theorem 12.1.4, we have that
ϵk+1 = O(∥xk −x∗∥ϵk),
(12.1.38)
where
ϵk = ∥xk −x∗∥+ ∥λk −λ∗∥.
(12.1.39)
Proof.
From the proof of Theorem 12.1.4 we see that αk = 1 for all
suﬃciently large k. Therefore it follows from the deﬁnitions of (δx)k and
(δλ)k that

W(xk, λk)
−A(xk)
−A(xk)T
0
 
xk+1 −x∗
λk+1 −λ∗

=

−∇f(xk) + A(xk)λk
c(xk)

+

W(xk, λk)(xk −x∗) −A(xk)(λk −λ∗)
−A(xk)T (xk −x∗)

=

(A(x∗) −A(xk))(λk −λ∗) + O(∥xk −x∗∥2)
O(∥xk −x∗∥2)

=

O(∥xk −x∗∥[∥xk −x∗∥+ ∥λk −λ∗∥])
O(∥xk −x∗∥2)

= O(∥xk −x∗∥ϵk).
(12.1.40)
The above relation and the nonsingularity of matrix (12.1.35) show that the
lemma holds.
2

12.1. LAGRANGE-NEWTON METHOD
529
Theorem 12.1.6 Under the assumptions of Theorem 12.1.4, the sequence
{xk} converges to x∗superlinearly and
∥xk+1 −x∗∥= o
⎛
⎝∥xk −x∗∥
p
/
j=1
∥xk−j −x∗∥
⎞
⎠
(12.1.41)
holds for any given positive integer p.
Proof.
It follows from (12.1.38) that {xk} converges to x∗superlinearly.
For any given positive integer p, applying (12.1.38) recursively we obtain that
∥xk+1 −x∗∥
=
O(ϵk+1) = O(∥xk −x∗∥ϵk)
=
O(∥xk −x∗∥∥xk−1 −x∗∥ϵk−1)
=
O
⎛
⎝∥xk −x∗∥
p
/
j=1
∥xk−j −x∗∥ϵk−p
⎞
⎠
=
o
⎛
⎝∥xk −x∗∥
p
/
j=1
∥xk−j −x∗∥
⎞
⎠.
(12.1.42)
Therefore the theorem is true.
2
One of the most important contributions of the Lagrange-Newton method
is the development of the sequential quadratic programming method based
on it. Sequential quadratic programming algorithms are the most impor-
tant algorithms for solving medium and small scale nonlinear constrained
optimization problems.
Setting ¯λk = λk + (δλ)k, we can write (12.1.6) in the following equivalent
form
W(xk, λk)(δx)k + ∇f(xk)
=
A(xk)[λk + (δλ)k],
(12.1.43)
c(xk) + A(xk)T (δx)k
=
0,
(12.1.44)
which is just, in matrix form,

W(xk, λk)
−A(xk)
−A(xk)T
0
 
(δx)
¯λ

=

−g(xk)
c(xk)

(12.1.45)
with solution (δx)k and ¯λk. Then xk+1 is given by
xk+1 = xk + (δx)k.
(12.1.46)

530
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
It is easy to show that the above system (12.1.45) is a KKT condition of the
quadratic programming subproblem
min
d∈ℜn
dT ∇f(xk) + 1
2dT W(xk, λk)d,
(12.1.47)
s.t.
c(xk) + A(xk)T d = 0
(12.1.48)
with ((δx)k, ¯λk) being the corresponding KKT pair thereof. Therefore, the
Lagrange-Newton method can be viewed as a method that solves the quadratic
programming subproblem (12.1.47)-(12.1.48) successively.
12.2
Wilson-Han-Powell Method
In this section we present a sequential quadratic programming method, which
was proposed by Han [169]. The method is based on the Lagrange-Newton
method discussed in the previous section.
In each iteration the matrix
W(xk, λk) is replaced by a matrix Bk. Because the Lagrange-Newton method
was ﬁrst considered by Wilson [349], and because Han’s method was modiﬁed
and analyzed by Powell [268], the method presented in this section is often
called the Wilson-Han-Powell method.
Consider nonlinearly constrained optimization problem (8.1.1)-(8.1.3),
Similar to (12.1.47)-(12.1.48), we construct the following subproblem
min
d∈ℜ
n
gT
k d + 1
2dT Bkd,
(12.2.1)
s.t.
ai(xk)T d + ci(xk) = 0, i ∈E,
(12.2.2)
ai(xk)T d + ci(xk) ≥0,
i ∈I,
(12.2.3)
where
A(xk) = [a1(xk), · · ·, am(xk)] = ∇c(xk)T ,
(12.2.4)
gk = g(xk) = ∇f(xk), E = {1, 2, · · ·me}, I = {me+1, · · ·, m}, and Bk ∈ℜn×n
is an approximation to the Hessian matrix of the Lagrangian function. Let
dk be a solution of (12.2.1)-(12.2.3). The vector dk is the search direction in
the k-th iteration by the Wilson-Han-Powell method. Let λk be the corre-
sponding Lagrange multiplier of (12.2.1)-(12.2.3) (just like ¯λk in the previous
section), then it follows that
gk + Bkdk
=
A(xk)λk,
(12.2.5)
(λk)i
≥
0,
i ∈I,
(12.2.6)
(λk)i[ci(xk) + ai(xk)T dk]
=
0,
i ∈I.
(12.2.7)

12.2. WILSON-HAN-POWELL METHOD
531
A very good property of dk is that it is a descent direction of many penalty
functions. For example, considering the L1 exact penalty function, we have
the following result.
Lemma 12.2.1 Let dk be a KKT point of (12.2.1)-(12.2.3) and λk be the
corresponding Lagrange multiplier. Consider the L1 penalty function
P(x, σ) = f(x) + σ∥c(−)(x)∥1,
(12.2.8)
where c(−)(x) is deﬁned by (10.1.2)-(10.1.3). Then we have that
P ′
α(xk + αdk, σ)|α=0 ≤−dT
k Bkdk −σ∥c(−)(xk)∥1 + λT
k c(xk).
(12.2.9)
If dT
k Bkdk > 0 and σ ≥∥λk∥∞, then dk is a descent direction of the penalty
function (12.2.8) at xk.
Proof.
By Taylor expression and using convexity of ∥(c + Ad)(−)∥1, we
have that
P ′
α(xk + αdk, σ)|α=0 = lim
α→0+
P(xk + αdk) −P(xk)
α
= gT
k dk + lim
α→0+ σ∥[c(xk) + αA(xk)T dk](−)∥1 −∥c(−)(xk)∥1
α
≤gT
k dk + σ[∥(c(xk) + A(xk)T dk)(−)∥1 −∥c(−)(xk)∥1]
= gT
k dk −σ∥c(−)(xk)∥1.
(12.2.10)
It follows from (12.2.5) and (12.2.7) that
gT
k dk = −dT
k Bkdk + λT
k c(xk).
(12.2.11)
Therefore (12.2.9) follows from (12.2.10) and (12.2.11).
Because λk satisﬁes (12.2.6), it follows from the deﬁnition of c(−)(x) that
λT
k c(xk) ≤
m

i=1
|(λk)i| |c(−)
i
(xk)|.
(12.2.12)
Substituting the above inequality into (12.2.9), and using the assumptions
that dT
k Bkdk > 0 and σ ≥∥λk∥∞, we have that
P ′
α(xk + αdk, σ)|α=0 ≤−dT
k Bkdk −
m

i=1
(σ −|(λk)i|)|c(−)
i
(xk)| < 0. (12.2.13)

532
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
This shows that the lemma is true.
2
The following algorithm is the sequential quadratic programming algo-
rithm proposed by Han [169].
Algorithm 12.2.2
Step 1. Given x1 ∈ℜn, σ > 0, δ > 0, B1 ∈ℜn×n, ϵ ≥0, k := 1;
Step 2. Solve (12.2.1)-(12.2.3) giving dk;
if ∥dk∥≤ϵ then stop;
ﬁnd αk ∈[0, δ] such that
P(xk + αkdk, σ) ≤min
0≤α≤δ P(xk + αdk, σ) + ϵk.
(12.2.14)
Step 3. xk+1 = xk + αkdk;
Evaluate f(xk+1), gk+1, c(xk+1), Ak+1;
Step 4. Compute λk+1 = −(AT
k+1Ak+1)−1AT
k+1gk+1;
Set sk = αdk, yk = ∇xL(xk+1, λk+1) −∇xL(xk, λk+1);
generate Bk+1 by updating Bk using a quasi-Newton for-
mula;
k := k + 1; go to Step 2.
2
In (12.2.14), the penalty function P(x, σ) is the L1 exact penalty function,
ϵk is a sequence of nonnegative numbers satisfying
∞

k=1
ϵk < +∞.
(12.2.15)
The global convergence result of the above algorithm is as follows.
Theorem 12.2.3 Assume that f(x) and ci(x) are continuously diﬀeren-
tiable, and that there exist constants m, M > 0 such that
m∥d∥2 ≤dT Bkd ≤M∥d∥2
(12.2.16)
holds for all k and d ∈ℜn, if ∥λk∥∞≤σ for all k, then any accumulation
point of {xk} generated by Algorithm 12.2.2 is a KKT point of (8.1.1)-(8.1.3).

12.2. WILSON-HAN-POWELL METHOD
533
Proof.
If the theorem is not true, there exists a subsequence of {xk}
converging to ¯x which is not a KKT point. Therefore there exists a subset
K0 having inﬁnitely many elements such that
lim
k∈K0
k→∞
xk = ¯x.
(12.2.17)
Without loss of generality, we can assume that
lim
k∈K0
k→∞
λk = ¯λ,
lim
k∈K0
k→∞
Bk = ¯B.
(12.2.18)
If
lim
k∈K0
k→∞
∥dk∥= 0,
(12.2.19)
from the relation
gk + Bkdk = A(xk)λk,
(12.2.20)
it follows that
g(¯x) = A(¯x)¯λ.
(12.2.21)
This contradicts the fact that ¯x is not a KKT point. Therefore we can assume
that
∥dk∥≥η > 0,
∀k ∈K0,
(12.2.22)
where η is a constant. The above relation and (12.2.13) imply that
P ′
α(xk + αdk, σ)|α=0 ≤−mη∥dk∥,
(12.2.23)
holds for all k ∈K0. It follows from (12.2.23) and the continuity assumptions
on the functions that there exists a positive constant ¯η such that
min
0≤α≤δ P(xk + αdk, σ) ≤P(xk, σ) −¯η
(12.2.24)
hold for all k ∈K0. Thus,
P(xk+1, σ) ≤P(xk, σ) −¯η + ϵk,
∀k ∈K0.
(12.2.25)
Consequently we can derive the inequality

k∈K0
¯η
≤

k∈K0
[P(xk, σ) −P(xk+1, σ)] +

k∈K0
ϵk
≤
∞

k=1
[P(xk, σ) −P(xk+1, σ)] +
∞

k=1
ϵk.
(12.2.26)

534
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Because limk→∞P(xk, σ) = P(¯x, σ), it follows that

k∈K0
¯η ≤P(x1, σ) −P(¯x, σ) +
∞

k=1
ϵk < +∞.
(12.2.27)
From the above inequality and ¯η > 0 we see that K0 can have only ﬁnitely
many elements, which contradicts our assumption in the beginning of the
proof. This indicates that the theorem is true.
2
The global convergence requires that
σ > ∥λk∥∞
(12.2.28)
for all k. However, in practice it is very diﬃcult to choose such a penalty
parameter.
If σ is too small, condition (12.2.28) may be violated.
If σ
is too large, the step-length αk may tend to be too small to prevent the
fast convergence of the algorithm. Powell [268] suggests using exact penalty
function
P(x, σk) = f(x) +
m

i=1
(σk)i|c(−)
i
(x)|
(12.2.29)
in the k-th iteration, where (σk)i > 0 and these parameters are updated in
the following way.
(σ1)i
=
(λ1)i,
(12.2.30)
(σk)i
=
max
*
|[λk]i|, 1
2[(σk−1)i + |(λk)i|]
+
,
k > 1,
(12.2.31)
for all i = 1, · · · , m. The parameters σk deﬁned above satisfy
(σk)i ≥|(λk)i|,
i = 1, 2, · · ·, m.
(12.2.32)
This very clever update technique allows the penalty parameters to change
from iteration to iteration, and, intuitively, the inequality (12.2.32) oﬀers
a similar property to (12.2.28). But, because (σk)i are not constants, the
conditions of Theorem 12.2.3 do not hold. And Chamberlain [53] gives an
example to show that cycles may happen due to this update technique.
Now we discuss the update of Bk+1, which is usually generated by a cer-
tain quasi-Newton formula. From our analyses in Section 12.1, we hope that
Bk+1 is an approximation to the Hessian matrix of the Lagrangian function.

12.2. WILSON-HAN-POWELL METHOD
535
Similar to unconstrained optimization, we can apply the standard quasi-
Newton updates using
sk
=
xk+1 −xk,
(12.2.33)
yk
=
∇f(xk+1) −∇f(xk)
−
m

i=1
(λk)i[∇ci(xk+1) −∇ci(xk)].
(12.2.34)
A crucial diﬀerence is that line
sT
k yk > 0,
(12.2.35)
which would be always true for unconstrained optimization. Therefore, for
example, we can not directly apply the BFGS update. Powell [268] suggests
that yk be replaced by
¯yk =

yk,
if sT
k yk ≥0.2sT
k Bksk,
θkyk + (1 −θk)Bksk,
otherwise
(12.2.36)
where
θk =
0.8sT
k Bksk
sT
k Bksk −sT
k yk
.
(12.2.37)
The vector ¯yk deﬁned above satisﬁes sT
k ¯yk > 0.
The idea of such a choice of ¯yk is to obtain an update vector using the
convex combination of yk and Bksk. Because Bksk can also be viewed as
an approximation to yk, because it satisﬁes (assuming that Bk is positive
deﬁnite)
sT
k (Bksk) > 0,
(12.2.38)
it is very natural to use the convex combination of yk and Bksk. The geomet-
ric interpretation of Powell’s formula is as follows. Suppose we normalize the
length of the projection of Bksk to direction sk. The rule (12.2.36)-(12.2.37)
is in fact to choose ¯yk from the line segment between yk and Bksk that is
as close to yk as possible and whose projection to sk is at least 0.2. This is
shown in Figure 12.2.1.

536
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Figure 12.2.1
Having computed the vector ¯yk, we can now apply the BFGS formula to
update Bk+1:
Bk+1 = Bk −BksksT
k BT
k
sT
k Bksk
+ ¯yk¯yT
k
sT
k ¯yk
.
(12.2.39)
Another way to modify yk is to use
ˆyk = yk + 2ρ
m

i=1
−ci(xk)∇ci(xk)
(12.2.40)
to replace yk, where ρ > 0 is a parameter. Because
ˆyk ≈[∇2L(xk, λk) + 2ρA(xk)A(xk)T ]sk,
(12.2.41)
updating Bk by using ˆyk can be viewed as making Bk+1 approximate the
Hessian matrix of the augmented Lagrange function. An advantage of this
choice is that
sT
k ˆyk > 0
(12.2.42)
can usually be satisﬁed. If sT
k ˆyk ≤0, we can always make (12.2.42) hold
by increasing ρ, unless ∥A(xk)sk∥= 0. Normally, the Hessian matrix of the
augmented Lagrange function is positive deﬁnite, thus it is very reasonable
to use a positive deﬁnite matrix Bk to approximate it.

12.3. SUPERLINEAR CONVERGENCE OF SQP STEP
537
12.3
Superlinear Convergence of SQP Step
In order to prove the superlinear convergence property of the sequential
quadratic programming method, i.e.
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0,
(12.3.1)
we only need to show that the search direction dk satisﬁes
lim
k→∞
∥xk + dk −x∗∥
∥xk −x∗∥
= 0
(12.3.2)
and that the line search condition will allow αk = 1 for all large k if (12.3.2)
holds. Thus, the important thing is to show that the search direction gen-
erated by the SQP method satisﬁes (12.3.2). A step dk satisfying (12.3.2) is
called a superlinearly convergent step. In this section, we discuss the condi-
tions for ensuring the sequential quadratic programming method to produce
superlinearly convergent steps.
Throughout this section, we make the following assumptions.
Assumption 12.3.1
1) f(x), ci(x) are twice continuously diﬀerentiable;
2) xk →x∗;
3) x∗is a KKT point and
∇ci(x∗),
i ∈E ∪I(x∗)
(12.3.3)
are linearly independent. Let A(x∗) be the n × |E ∪I(x∗)| matrix
consisting of the vectors given in (12.3.3). For all nonzero vectors d
satisfying
A(x∗)T d = 0,
(12.3.4)
we have that
dT W(x∗, λ∗)d ̸= 0,
(12.3.5)
where W(x∗, λ∗) is deﬁned by (12.1.8), and λ∗is the Lagrange mul-
tiplier at x∗.

538
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
The above assumptions are often used for local convergence analyses of
algorithms for constrained optimization. For example, relations (12.3.4) and
(12.3.5) hold if we assume the second-order suﬃcient condition
dT W(x∗, λ∗)d > 0,
∀d ̸= 0, A(x∗)T d = 0.
(12.3.6)
We also make the assumption that the active set at the solution can be
identiﬁed when the iterations are very close to the solution. Therefore, when
k is suﬃciently large, the search direction dk is actually the solution of an
equality constrained quadratic programming subproblem.
Assumption 12.3.2 For suﬃciently large k, dk is a solution of
min
d∈ℜ
n
gT
k d + 1
2dT Bkd
(12.3.7)
s.t.
ci(xk) + dT ∇ci(xk) = 0,
i ∈E ∪I(x∗).
(12.3.8)
Under Assumption 12.3.2, for all large k there exists λk ∈ℜ|E∪I(x∗)| such
that
gk + Bkdk
=
A(xk)λk,
(12.3.9)
A(xk)T dk
=
−ˆc(xk),
(12.3.10)
where ˆc(x) is a vector whose elements are ci(x)(i ∈E ∪I(x∗)).
Theorem 12.3.3 Under the conditions of Assumptions 12.3.1 and 12.3.2,
dk is a superlinearly convergent step, namely
lim
k→∞
∥xk + dk −x∗∥
∥xk −x∗∥
= 0
(12.3.11)
if and only if
lim
k→∞
∥Pk(Bk −W(x∗, λ∗))dk∥
∥dk∥
= 0,
(12.3.12)
where Pk is a projection from ℜn onto the null space of A(xk)T :
Pk = I −A(xk)(A(xk)T A(xk))−1A(xk)T .
(12.3.13)

12.3. SUPERLINEAR CONVERGENCE OF SQP STEP
539
Proof.
From (12.3.9) and the deﬁnition of Pk, we have that
PkBkdk
=
−Pkgk = −Pk[∇f(xk) −A(xk)λ∗]
=
−Pk[∇xL(xk, λ∗) −∇xL(x∗, λ∗)]
=
−PkW(x∗, λ∗)(xk −x∗) + O(∥xk −x∗∥2).
(12.3.14)
Therefore, it follows that
Pk(Bk −W(x∗, λ∗))dk
=
−PkW(x∗, λ∗)[xk + dk −x∗]
+O(∥xk −x∗∥2).
(12.3.15)
Using relation (12.3.10) and
ˆc(xk)
=
ˆc(xk) −ˆc(x∗)
=
A(xk)T (xk −x∗) + O(∥xk −x∗∥2),
(12.3.16)
we can show that
A(xk)T (xk + dk −x∗) = O(∥xk −x∗∥2).
(12.3.17)
Equations (12.3.15) and (12.3.17) can be rewritten in matrix form:

PkW(x∗, λ∗)
A(xk)T

(xk + dk −x∗)
=

−Pk(Bk −W(x∗, λ∗))dk
0

+
O(∥xk −x∗∥2).
(12.3.18)
Deﬁne the matrix
G∗=

P∗W(x∗, λ∗)
A(x∗)T

,
(12.3.19)
where P∗= I −A(x∗)(A(x∗)T A(x∗))−1A(x∗)T . For any d ∈ℜn, if G∗d = 0
we have that
A(x∗)T d
=
0,
(12.3.20)
dT P∗W(x∗, λ∗)d
=
0.
(12.3.21)
From (12.3.20) it follows that P∗d = d. Thus,
dT W(x∗, λ∗)d = 0.
(12.3.22)

540
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
The above relation and Assumption 12.3.1 show that d = 0. Therefore matrix
G∗is a full column rank matrix. Hence, from (12.3.18) and the fact that
xk →x∗we can see that (12.3.11) is equivalent to
lim
k→∞
∥Pk(Bk −W(x∗, λ∗))dk∥
∥xk −x∗∥
= 0.
(12.3.23)
Using the equivalence between (12.3.23) and (12.3.11) and that between
(12.3.11) and
lim
k→∞∥xk −x∗∥
8
∥dk∥= 1,
(12.3.24)
we can show that (12.3.23) is equivalent to (12.3.12). This completes the
proof.
2
Using relation (12.3.9) and λk →λ∗, we have that
W(x∗, λ∗)dk = W(xk, λk)dk + o(∥dk∥)
=
∇f(xk + dk) −A(xk + dk)λk −∇f(xk) + A(xk)λk + o(∥dk∥)
=
∇f(xk + dk) −A(xk + dk)λk + Bkdk + o(∥dk∥).
(12.3.25)
Therefore,
Pk(Bk −W(x∗, λ∗))dk
=
−Pk[∇f(xk + dk) −A(xk + dk)λk]
+o(∥dk∥).
(12.3.26)
From the above relation and Theorem 12.3.3, we can get the following
result.
Corollary 12.3.4 Under the assumptions of Theorem 12.3.3, (12.3.11) is
equivalent to
lim
k→∞
∥Pk[∇f(xk + dk) −A(xk + dk)λk∥
∥dk∥
= 0.
(12.3.27)
From Theorem 12.3.3, we should choose Bk such that (12.3.12) is satis-
ﬁed in order to have superlinear convergence, namely Bk should be a good
approximation to W(x∗, λ∗).

12.4. MARATOS EFFECT
541
12.4
Maratos Eﬀect
For unconstrained optimization, if x∗is a stationary point at which the
second-order suﬃcient condition holds, namely
∇2f(x∗)
positive deﬁnite,
(12.4.1)
if xk →x∗, and if dk is a superlinearly convergent step, then
f(xk + dk) < f(xk)
(12.4.2)
holds for all large k. That is to say, superlinearly convergent steps are ac-
ceptable for unconstrained problems. However, this is not always true for
constrained problems. Such a phenomenon was ﬁrst discovered by Maratos
[209], so it is called the Maratos Eﬀect.
Consider the equality constrained optimization problem
min
x=(u,v)∈ℜ
2
f(x) = 3v2 −2u,
(12.4.3)
s.t.
c(x) = u −v2 = 0.
(12.4.4)
It is easy to see that x∗= (0, 0)T is the unique minimizer and condition 3) of
Assumption 12.3.1 is satisﬁed. In fact, the second-order suﬃcient condition
holds at x∗. Consider any points that are close to the solution x∗and that
have the form
¯x(ϵ) = (u(ϵ), v(ϵ))T = (ϵ2, ϵ)T
(12.4.5)
where ϵ > 0 is a small parameter. Let B = W(x∗, λ∗); the quadratic pro-
gramming subproblem is
min
d∈ℜ
2
dT

−2
6ϵ

+ 1
2dT

0
0
0
2

d,
(12.4.6)
s.t.
dT

1
−2ϵ

= 0.
(12.4.7)
It is easy to see that the solution of (12.4.6)-(12.4.7) is
¯d(ϵ) =

−2ϵ2
−ϵ

.
(12.4.8)

542
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Therefore, we have that
∥¯x(ϵ) + ¯d(ϵ) −x∗∥= O(∥¯x(ϵ) −x∗∥2).
(12.4.9)
Thus, ¯d(ϵ) is a superlinearly convergent step. Direct calculation indicates
that
f(¯x(ϵ) + ¯d(ϵ))
=
2ϵ2,
(12.4.10)
c(¯x(ϵ) + ¯d(ϵ))
=
−ϵ2.
(12.4.11)
Because
f(¯x(ϵ))
=
ϵ2,
(12.4.12)
c(¯x(ϵ))
=
0,
(12.4.13)
we have that
f(¯x(ϵ) + ¯d(ϵ))
>
f(¯x(ϵ)),
(12.4.14)
|c(¯x(ϵ) + ¯d(ϵ))|
>
|c(¯x(ϵ))|.
(12.4.15)
This example shows that even though ¯d(ϵ) is a superlinearly convergent step
(namely ¯x(ϵ) + ¯d(ϵ) is much closer to x∗than ¯x(ϵ)), the point ¯x(ϵ) + ¯d(ϵ) is
“worse” than ¯x(ϵ) from the objective function values and from the constraint
violations.
In fact, for any penalty functions Pσ,h(x) having the form of
(10.6.2), we would have that
Pσ,h(¯x(ϵ) + ¯d(ϵ)) > Pσ,h(¯x(ϵ)).
(12.4.16)
Especially, when the merit function is the L1 exact penalty function,
¯x(ϵ) + ¯d(ϵ) is not acceptable.
The Maratos Eﬀect shows that for many penalty functions a superlin-
early convergent step may not be accepted, which, sometimes, prevents the
algorithm from fast convergence.
There are mainly three ways to overcome the Maratos Eﬀect. The ﬁrst
one is to relax the line search conditions. Roughly speaking, since the search
direction dk is a superlinearly convergent step, we should choose αk = 1 as
often as possible provided that convergence is ensured. The second one is to
use a second-order correction step ˆdk, where ˆdk satisﬁes ∥ˆdk∥= O(∥dk∥2),
and Pσ(xk + dk + ˆdk) < Pσ(xk). In this way, dk + ˆdk is an acceptable step
and it is still a superlinearly convergent step. The third way is to use smooth

12.5. WATCHDOG TECHNIQUE
543
exact penalty functions as merit functions. If the penalty function Pσ(x) is
smooth, we can show that
Pσ(xk + dk) < Pσ(xk)
(12.4.17)
for all large k as long as (12.3.11) holds.
We will discuss these three techniques in the following sections.
12.5
Watchdog Technique
The nature of the Maratos Eﬀect is the inequality
Pσ(xk + dk) > Pσ(xk),
(12.5.1)
makes xk+1 ̸= xk + dk, therefore the superlinearly convergent property is
destroyed. In the Watchdog technique proposed by Chamberlain et. al. [54],
the standard linear search which implies that
Pσ(xk+1) < Pσ(xk),
(12.5.2)
is used in some iterations, but in the other iterations, the line search con-
ditions are relaxed. The relaxed line search can be either simply αk = 1 or
requiring the Lagrange function to be reduced. Assume that the new point
obtained in one iteration yields a suﬃcient reduction on the merit function
Pσ(x); comparing with the best point in the previous iterations, we can use
the relaxed line search in the next iteration.
Deﬁne the function
Pσ(x) = f(x) +
me

i=1
σi|ci(x)| +
m

i=me+1
σi| min[0, ci(x)]|,
(12.5.3)
and the approximate models
P (k)
σ (x)
=
f(xk) + (x −xk)T ∇f(xk) + 1
2(x −xk)T Bk(x −xk)
+
me

i=1
σi|ci(xk) + (x −xk)T ∇ci(xk)|
+
m

i=me+1
σi| min[0, ci(xk) + (x −xk)T ∇ci(xk)]|.
(12.5.4)

544
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Assume that l ≤k is the index in which the best point has been found
up to the k-th iteration, namely
Pσ(xl) = min
1≤i≤k Pσ(xi).
(12.5.5)
Let β ∈

0, 1
2

be a given positive constant. If the iterate point obtained in
the k-th iteration xk+1 = xk + αkdk satisﬁes
Pσ(xk+1) ≤Pσ(xl) −β[Pσ(xl) −P (l)
σ (xl+1)],
(12.5.6)
then we say xk+1 (comparing to xl) yields a “suﬃcient” reduction on the
merit function Pσ(x).
The following is an algorithm with the Watchdog technique.
Algorithm 12.5.1 (Watchdog Method)
Step 1. Given x1 ∈ℜn, a positive constant ¯n.
Set line search type to be standard; k := l := 1;
Step 2. Compute the search direction dk;
Carry out line search using the line search type, obtaining
αk > 0;
xk+1 = xk + αkdk;
Step 3. if (12.5.6) holds, then set the next line search type to be
“relaxed”, otherwise to be standard.
Step 4. if Pσ(xk+1) ≤Pσ(xl), then l := k + 1;
Step 5. if k < l + ¯n, then go to Step 6;
xk+1 := xl; l := k + 1;
Step 6. if convergence criterion is satisﬁed then stop;
k := k + 1; go to Step 2.
2
Actually, if the “relaxed” line search conditions are the same as the stan-
dard condition, the above algorithm is the original method that is based on
the standard line searches. Therefore, the Watchdog method is a generaliza-
tion of the standard method.
Assume the standard line search condition is
Pσ(xk+1) ≤Pσ(xk) −β[Pσ(xk) −P (k)
σ (xk+1)].
(12.5.7)

12.6. SECOND-ORDER CORRECTION STEP
545
From the descriptions of the above algorithm, we know that there exists
k ≤l + ¯n + 1 such that
Pσ(xk+1) ≤Pσ(xl) −β[Pσ(xl) −P (l)
σ (xl+1)].
(12.5.8)
Thus, the watchdog method will reduce the merit function Pσ(x) in every ¯n+1
iterations, even though it can not guarantee the monotonically decreasing of
Pσ(xk). Let l(j) be the j-th value of l; from the discussions above we see
that
l(j) < l(j + 1) ≤l(j) + ¯n + 2.
(12.5.9)
If we assume that the sequence {xk} is bounded, then Pσ(xl(j)) will not tend
to negative inﬁnity. Thus, it follows from the inequality
Pσ(xl(j+1)) ≤Pσ(xl(j)) −β[Pσ(xl(j)) −P (l(j))
σ
(xl(j)+1)]
(12.5.10)
that
∞

j=1
[Pσ(xl(j)) −P (l(j))
σ
(xl(j)+1)] < +∞.
(12.5.11)
The above relation shows that there exists an accumulation point of {xk}
that is a KKT point of the constrained optimization problem.
12.6
Second-Order Correction Step
A second-order correction step is a vector ˆdk such that
∥ˆdk∥= O(∥dk∥2)
(12.6.1)
and
Pσ(xk + dk + ˆdk) < Pσ(xk)
(12.6.2)
for all suﬃciently large k. Consider that ˆdk is deﬁned as a solution of the
following quadratic programming problem:
min
d∈ℜ
n
gT
k (dk + d) + 1
2(dk + d)T Bk(dk + d),
(12.6.3)
s.t.
ci(xk + dk) + ai(xk)T d = 0,
i ∈E,
(12.6.4)
ci(xk + dk) + ai(xk)T d ≥0,
i ∈I,
(12.6.5)
where dk is the solution of (12.2.1)-(12.2.3).

546
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
For simplicity, we assume that all the constraints are equality constraints.
We also assume that the second-order suﬃcient conditions hold at x∗and that
xk →x∗. From the KKT condition there exist λk ∈ℜm and ˆλk ∈ℜm such
that
Bkdk = −gk + A(xk)λk,
(12.6.6)
A(xk)T dk = −c(xk)
(12.6.7)
and that
Bkdk + Bk ˆdk
=
−gk + A(xk)ˆλk,
(12.6.8)
A(xk)T ˆdk
=
−c(xk + dk).
(12.6.9)
From (12.6.6) and (12.6.8) we see that
PkBk ˆdk = 0,
(12.6.10)
where Pk is deﬁned by (12.3.13). We make the following assumptions.
Assumption 12.6.1
1) xk →x∗;
2) A(x∗) is full column rank;
3) there exist positive constants ¯m and ¯
M such that ∥Bk∥≤¯
M and that
dT Bkd ≥¯m∥d∥2
2
(12.6.11)
holds for all d satisfying A(xk)T d = 0 for all k.
From the above assumptions we can show the following lemma.
Lemma 12.6.2 Under the conditions of Assumption 12.6.1, there exists a
positive constant η such that


PkBk
A(xk)T

d

2
≥η∥d∥2
(12.6.12)
holds for all d ∈ℜn and all suﬃciently large k.

12.6. SECOND-ORDER CORRECTION STEP
547
Proof.
Let the QR factorization of A(xk) be
A(xk) = [Yk Zk]

Rk
0

.
(12.6.13)
Because A(x∗) is nonsingular, there exists k0 such that for k ≥k0 we have
∥R−1
k ∥2 ≤ˆη,
(12.6.14)
where ˆη > 0 is a constant. Therefore,
∥A(xk)T d∥2 = ∥RT
k Y T
k d∥2 ≥1
ˆη∥Y T
k d∥2,
(12.6.15)
for k ≥k0. Using the relation YkY T
k + ZkZT
k = I, we can show that
∥PkBkd∥2
=
∥ZkZT
k Bkd∥2
=
∥ZkZT
k BkYkY T
k d + ZkZT
k BkZkZT
k d∥2
≥
∥ZkZT
k BkZkZT
k d∥2 −∥Bk∥2∥Y T
k d∥2
≥
¯m∥ZT
k d∥2 −¯
M∥Y T
k d∥2.
(12.6.16)
Thus, if
∥Y T
k d∥≥
¯m
2 ¯
M ∥ZT
k d∥,
(12.6.17)
it follows from (12.6.15) that
∥A(xk)T d∥2
≥
1
ˆη∥Y T
k d∥2
≥
¯m
2 ¯
M
ˆη
9
1 +

¯m
2 ¯
M
2 ∥d∥2.
(12.6.18)
If (12.6.17) does not hold, it follows from (12.6.16) that
∥PkBkd∥2 ≥1
2 ¯m∥ZT d∥2 ≥
¯
M
9
1 +

2 ¯
M
¯m
2 ∥d∥2.
(12.6.19)
Therefore, when k ≥k0, either of (12.6.18) and (12.6.19) must hold. Let
η = min
61
ˆη, ¯
M
7
1

1 + 4( ¯
M/ ¯m)2 ,
(12.6.20)

548
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
then we see that (12.6.12) holds for all k ≥k0 and all d ∈ℜn.
2
Using (12.6.9)-(12.6.10), we have that

PkBk
A(xk)T

ˆdk =

0
−c(xk + dk)

= O(∥dk∥2
2).
(12.6.21)
Now, from the above relation and Lemma 12.6.2 we can show the following
lemma.
Lemma 12.6.3 Under the conditions of Assumption 12.6.1, there exists a
positive constant ¯η > 0 such that
∥ˆdk∥2 ≤¯η∥dk∥2
2.
(12.6.22)
To this end, we have shown that the step deﬁned by (12.6.3)-(12.6.5) is
indeed a second-order correction step.
In the following we show that the second-order correction step ˆdk will
make the step dk + ˆdk acceptable. First, using (12.6.9) we see that
c(xk + dk + ˆdk)
=
c(xk + dk) + A(xk)T ˆdk + o(∥ˆdk∥)
=
o(∥dk∥2) = o(∥xk −x∗∥2).
(12.6.23)
Deﬁne the vector
¯dk = −(A(xk)T )+c(xk + dk) −Pk(xk + dk −x∗),
(12.6.24)
then it follows that
∥xk + dk + ¯dk
−
x∗∥= ∥(I −Pk)(xk + dk −x∗)
−(A(xk)T )+c(xk + dk)∥
=
∥(I −Pk)(xk + dk −x∗)
−(A(xk)T )+A(xk)T (xk + dk −x∗)∥
+o(∥xk −x∗∥2) = o(∥xk −x∗∥2).
(12.6.25)
Furthermore, it follows from (12.6.24) that
A(xk)T ¯dk = −c(xk + dk).
(12.6.26)
If we assume not only (12.3.12) but also
∥(Bk −W(x∗, λ∗))d∥
∥d∥
→0
(12.6.27)

12.6. SECOND-ORDER CORRECTION STEP
549
holds for d = dk + ˆdk, and d = dk + ¯d, then it follows that
(gk −Akλ∗)T d + 1
2dT Bkd
=
L(xk + d, λ∗) −L(xk, λ∗) + o(∥d∥2) + o(∥xk −x∗∥2)
=
L(xk + d, λ∗) −L(xk, λ∗) + o(∥xk −x∗∥2)
(12.6.28)
holds for d = dk + ˆdk and d = dk + ¯dk. From the deﬁnition of ˆdk, we can
show that
gT
k ˆdk
+
1
2(dk + ˆdk)T Bk(dk + ˆdk)
≤
gT
k ¯dk + 1
2(dk + ¯dk)T Bk(dk + ¯dk).
(12.6.29)
If follows from (12.6.28) and (12.6.29) that
L(xk + dk + ˆdk, λ∗)
≤
L(xk + dk + ¯dk, λ∗) + o(∥xk −x∗∥2)
≤
L(x∗, λ∗) + o(∥xk −x∗∥2).
(12.6.30)
The above inequality and (12.6.23) imply that
f(xk + dk + ˆdk) ≤f(x∗) + o(∥xk −x∗∥2).
(12.6.31)
It follows from the above relation and (12.6.23) that
Pσ(xk + dk + ˆdk) ≤Pσ(x∗) + o(∥xk −x∗∥2).
(12.6.32)
Under the second-order suﬃcient condition, there exists a positive constant
δ > 0 such that
Pσ(xk) ≥Pσ(x∗) + δ∥xk −x∗∥2.
(12.6.33)
Therefore, by the above two inequalities we can deduce that
Pσ(xk + dk + ˆdk) < Pσ(xk).
(12.6.34)
To be more exact, from (12.6.32)-(12.6.33) we can show that
lim
k→∞
Pσ(xk) −Pσ(xk + dk + ˆdk)
Pσ(xk) −Pσ(x∗)
= 1.
(12.6.35)
Therefore, it follows from (12.6.22) and (12.6.34), that
lim
k→∞
∥xk + dk + ˆdk −x∗∥
∥xk −x∗∥
= 0,
(12.6.36)

550
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
namely, dk + ˆdk is a superlinearly convergent step and it is acceptable.
Another way to compute a second-order correction step is to solve the
following subproblem:
min
d∈ℜ
n
˜gT
k d + 1
2dT Bkd,
(12.6.37)
s.t.
ci(xk) + ai(xk)T d = 0,
i ∈E,
(12.6.38)
ci(xk) + ai(xk)T d ≥0,
i ∈I,
(12.6.39)
where
˜gk = gk + 1
2
m

i=1
(λk)i[∇ci(xk) −∇ci(xk + dk)],
(12.6.40)
and where λk is the Lagrange multiplier of the quadratic programming sub-
problem (12.2.1)-(12.2.3). It can be shown that the search direction deﬁned
by (12.6.37)-(12.6.39) is a superlinearly convergent step and is also an ac-
ceptable step. For more detailed discussions, please see Mayne and Polak
[214] and Fukushima [142].
12.7
Smooth Exact Penalty Functions
The reason for the Maratos Eﬀect to happen is because the merit function
used to carry out line search is nonsmooth. If P(x) is a smooth function,
if x∗is its minimizer, and if ∇2P(x∗) is positive deﬁnite, we can easily see
that, for all x suﬃciently close to x∗,
¯
M∥x −x∗∥2 ≥P(x) −P(x∗) ≥¯m∥x −x∗∥2,
(12.7.1)
where ¯
M ≥¯m are two positive constants. Therefore if
∥xk + dk −x∗∥
∥xk −x∗∥
→0,
(12.7.2)
it is easy to show that
P(xk + dk)
≤
P(x∗) + ¯
M∥xk + dk −x∗∥2
<
P(x∗) + ¯m∥xk −x∗∥2 ≤P(xk)
(12.7.3)
holds for suﬃciently large k. Therefore, the Maratos Eﬀect can be avoided
if we use a smooth exact penalty function as the merit function.

12.7. SMOOTH EXACT PENALTY FUNCTIONS
551
Consider the equality constrained optimization problem:
min
x∈ℜ
n
f(x),
(12.7.4)
s.t.
c(x) = 0.
(12.7.5)
We use Fletcher’s smooth exact penalty function (10.5.4) as the merit func-
tion. Because the derivative of function (10.5.4) needs to compute the second-
order derivatives of f(x) and c(x), Powell and Yuan [277] uses an approximate
form of (10.5.4):
Φk,i(αβk,i)
=
f(xk + αβk,idk)
−
[λ(xk) + α(λ(xk + βk,idk) −λ(xk))]T c(xk + αβk,idk)
+
1
2σk,i∥c(xk + αβk,idk)∥2
2,
0 ≤α ≤1,
(12.7.6)
where dk is a solution of the quadratic programming subproblem (12.2.1)-
(12.2.3), βk,i is the (i + 1)-th trial step length in the k-th iteration, and σk,i
is the current penalty parameter which satisﬁes that
Φ′
k,i(0)
≤
−1
2[dT
k Bkdk + σk,i∥c(xk)∥2
2]
≤
−1
4σk,i∥c(xk)∥2
2.
(12.7.7)
The Powell and Yuan’s method can be stated as follows:
Algorithm 12.7.1 (Powell and Yuan’s Method)
Step 1. Given x1 ∈ℜn, β1 ∈(0, 1), β2 ∈(β1, 1), µ ∈(0, 1/2),
σ1,−1 > 0, B1 ∈ℜn×n, ϵ ≥0. k := 1;
Step 2. Solve (12.2.1)-(12.2.3), giving dk;
if ∥dk∥≤ϵ then stop;
let i = 0, βk,0 = 1;
Step 3. Choose σk,i such that (12.7.7) holds; if
Φk,i(βk,i) ≤Φk,i(0) + µβk,iΦ′
k,i(0),
(12.7.8)
then go to Step 4.
i := i + 1, βk,i ∈[β1, β2]βk,i−1; go to Step 3;

552
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Step 4. xk+1 = xk + βk,idk; σk+1,−1 = σk,i; update Bk+1;
k := k + 1; go to Step 2.
For the above algorithm, it can be shown that the following lemma holds.
Lemma 12.7.2 Assume that {xk}, {dk}, {Bk} are bounded.
If A(x) =
∇c(x)T is full column rank for all x ∈ℜn and if there exists a constant
δ > 0 such that
dT Bkd ≥δ∥d∥2
2,
∀A(xk)T d = 0
(12.7.9)
holds for all k, then there exists a positive integer k′ such that
σk,i = σk′,0 = ¯σ > 0
(12.7.10)
for all k ≥k′ and that
lim
k→∞∥dk∥= 0.
(12.7.11)
Using this lemma, we can prove the global convergence result of Algo-
rithm 12.7.1
Theorem 12.7.3 Under the conditions of Lemma 12.7.2, any accumulation
point of {xk} generated by Algorithm 12.7.1 is a KKT point of (12.7.4)-
(12.7.5).
Now we show that when the iterates are close to a solution, any super-
linearly convergent step will be accepted by Algorithm 12.7.1.
Lemma 12.7.4 Suppose that the assumptions of Lemma 12.7.2 are satisﬁed,
and assume that the sequence {xk} generated by Algorithm 12.7.1 converges
to x∗. For any subsequence {ki, i = 1, 2, · · ·}, if
∥xki + dki −x∗∥= o(∥xki −x∗∥),
ki →∞,
(12.7.12)
then we have that
xki+1 = xki + dki
(12.7.13)
for all large i.

12.7. SMOOTH EXACT PENALTY FUNCTIONS
553
Proof.
Without loss of generality, we assume that ki ≥k′. For simplicity
of notation, we substitute ki by j. From the descriptions of the algorithm,
we only need to show that
Φj,0(1) −Φj,0(0) −µΦ′
j,0(0) < 0.
(12.7.14)
It follows from (12.7.10) that
Φj,0(1) = f(xj + dj) −λ(xj + dj)T c(xj + dj) + 1
2 ¯σ∥c(xj + dj)∥2
2. (12.7.15)
Because f(x) is twice continuously diﬀerentiable, we have that
f(xj + dj)
=
f(xj) + 1
2dT
j [gj + g(xj + dj)] + o(∥dj∥2
2)
=
f(xj) + 1
2dT
j [gj + g(x∗)] + o(∥dj∥2
2).
(12.7.16)
Also, we can obtain similar formulae as (12.7.16) for ci(xj +dj). Substituting
all these formulae into (12.7.15), we obtain that
Φj,0(1)
−
Φj,0(0) = 1
2dT
j [gj + g(x∗)]
−λ(xj + dj)T

cj + 1
2AT
j dj + 1
2A(x∗)T dj

−

−λT
j cj + 1
2 ¯σ∥cj∥2
2

+ o(∥dj∥2
2)
=
1
2Φ′
j,0(0) + 1
2dT
j [g(x∗) −A(x∗)λ(xj + dj)]
+o(∥dj)∥2
2) = 1
2Φ′
j,0(0) + o(∥dj∥2
2).
(12.7.17)
It is not diﬃcult to show there exists a positive constant ¯η such that
Φ′
k,i(0) ≤−¯η∥dk∥2
2
(12.7.18)
holds for all k and i. From (12.7.17), (12.7.18) and µ < 1
2, we can see that
(12.7.14) holds for suﬃciently large j = ki. Thus, the lemma is true.
2
A direct corollary of the above result is the superlinear convergence of
the algorithm, which we write as follows.

554
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Theorem 12.7.5 Suppose that the assumptions of Lemma 12.7.2 are satis-
ﬁed, and assume that the sequence {xk} generated by Algorithm 12.7.1 con-
verges to x∗. If
lim
k→∞
∥xk + dk −x∗∥
∥xk −x∗∥
= 0,
(12.7.19)
then we have that xk+1 = xk + dk for all suﬃciently large k, which implies
that {xk} superlinearly converges to x∗.
12.8
Reduced Hessian Matrix Method
The reduced Hessian matrix method was also developed from the Lagrange-
Newton method. A fundamental idea of the reduced Hessian matrix method
is that only part of the Hessian matrix of the Lagrangian function is used so
that the method requires less storage and computing costs in each iteration.
Consider the equality constrained problem (12.1.1) and (12.1.2). Denot-
ing the Lagrange-Newton step by (dk, (δλ)k), it follows from (12.1.6) that

W(xk, λk)
−A(xk)
−A(xk)T
0
 
dk
(δλ)k

= −

∇f(xk) −A(xk)λk
−c(xk)

.
(12.8.1)
Using the notations
Wk
=
W(xk, λk),
(12.8.2)
Ak
=
A(xk) = ∇c(xk)T ,
(12.8.3)
gk
=
∇f(xk),
(12.8.4)
ck
=
c(xk),
(12.8.5)
ˆλk
=
λk + (δλ)k,
(12.8.6)
we can rewrite (12.8.1) as

Wk
−Ak
−AT
k
0
 
dk
ˆλk

=

−gk
ck

.
(12.8.7)
Let the QR factorization of Ak be
Ak = [Yk Zk]

Rk
0

,
(12.8.8)

12.8. REDUCED HESSIAN MATRIX METHOD
555
then linear system (12.8.7) can be written in the following form:
⎡
⎢⎣
Y T
k WkYk
Y T
k WkZk
−Rk
ZT
k WkYk
ZT
k WkZk
0
−RT
k
0
0
⎤
⎥⎦
⎡
⎢⎣
pk
qk
ˆλk
⎤
⎥⎦=
⎡
⎢⎣
−Y T
k gk
−ZT
k gk
ck
⎤
⎥⎦,
(12.8.9)
where
pk
=
Y T
k dk,
(12.8.10)
qk
=
ZT
k dk.
(12.8.11)
It is obvious that pk and qk are the projections of dk to the range space of
AT
k and the null space of AT
k . Because (12.8.9) has a block triangle form, we
can easily solve pk, qk and ˆλk in turns:
RT
k pk
=
−ck,
(12.8.12)
(ZT
k WkZk)qk
=
−ZT
k gk −ZT
k WkYkpk,
(12.8.13)
Rkˆλk
=
Y T
k gk + Y T
k Wk(Ykpk + Zkqk).
(12.8.14)
If we consider only the last two lines in the linear system (12.8.9), we obtain
a linear system independent of λ:

ZT
k WkYk
ZT
k WkZk
−RT
k
0
 
pk
qk

=

−ZT
k gk
ck

,
(12.8.15)
which is essentially

ZT
k Wk
−AT
k

dk =

−ZT
k gk
ck

.
(12.8.16)
Nocedal and Overton [232] suggests that the matrix ZT
k Wk be replaced by
a quasi-Newton matrix Bk, namely at each iteration the line search direction
dk is obtained by solving the linear system

Bk
−AT
k

d =

−ZT
k gk
ck

,
(12.8.17)
where Bk ∈ℜ(n−m)×n is an approximation to ZT
k Wk. We can apply Broy-
den’s nonsymmetric rank-one formula to update Bk, that is
Bk+1 = Bk + (yk −Bksk)sT
k
sT
k sk
,
(12.8.18)

556
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
where
sk
=
xk+1 −xk,
(12.8.19)
yk
=
ZT
k+1gk+1 −ZT
k gk.
(12.8.20)
Because ZT
k Wk is a one-side reduced Hessian matrix, this method is also
called the one-side reduced Hessian matrix method. Under certain conditions,
Nocedal and Overton [232] proved that the one-side reduced Hessian matrix
method using (12.8.18)-(12.8.20) is locally superlinearly convergent.
If we use a symmetric matrix Bk ∈ℜ(n−m)×(n−m) to substitute for
ZT
k WkZk, and a zero matrix to replace ZT
k WkYk, we can see that (12.8.15)
yields that

0
Bk
−RT
k
0
 
pk
qk

=

−ZT
k gk
ck

.
(12.8.21)
One reason for doing so is a fact discovered by Powell [274] that the SQP
method converges 2-step Q-superlinearly
lim
k→∞
∥xk+1 −x∗∥
∥xk−1 −x∗∥= 0
(12.8.22)
provided Y T
k WkZk is bounded.
Another reason is that when all iteration
points are feasible, we have pk = 0, the value of ZT
k WkYk does not alter qk.
For linearly constrained problems, all iteration points xk(k ≥k0) are feasible
if the initial point xk0 is feasible. An advantage of updating ZT
k WkZk instead
of ZT
k Wk is that ZT
k WkZk is a square matrix and it is symmetric positive
deﬁnite near the solution where the second-order suﬃcient conditions hold.
Therefore, we can use positive deﬁnite matrices to approximate it, such as
the BFGS update:
Bk+1 = Bk −BksksT
k Bk
sT
k Bksk
+ ykyT
k
sT
k yk
,
(12.8.23)
where
sk
=
ZT
k (xk+1 −xk),
(12.8.24)
yk
=
ZT
k+1gk+1 −ZT
k gk.
(12.8.25)
We can write (12.8.21) in the equivalent form

BkZT
k
−AT
k

dk =

−ZT
k gk
ck

.
(12.8.26)

12.8. REDUCED HESSIAN MATRIX METHOD
557
Because the matrix Bk tries to approximate ZT
k WkZk, which is a two-side
reduced Hessian matrix, the method using search direction dk deﬁned by
(12.8.26) is called the two-side reduced Hessian matrix method.
Such a
method is two-step superlinearly convergent near the solution.
Theorem 12.8.1 Let dk be deﬁned by (12.8.26). If xk+1 = xk+dk, xk →x∗,
A(x∗) is full column rank, second-order suﬃcient conditions hold at x∗, and
∥B−1
k ∥is bounded uniformly and satisﬁes
lim
k→∞
∥[Bk −Z(x∗)T W(x∗, λ∗)Z(x∗)]ZT
k dk∥
∥dk∥
= 0,
(12.8.27)
then the sequence converges 2-step Q-superlinearly:
lim
k→∞
∥xk+1 −x∗∥
∥xk−1 −x∗∥= 0.
(12.8.28)
Proof.
It follows from (12.8.26) that
BkZT
k dk
=
−ZT
k gk = −ZT
k [gk −Akλ∗]
=
−ZT
k W(x∗, λ∗)(xk −x∗) + O(∥xk −x∗∥2). (12.8.29)
Thus, we have that
[Bk
−
Z(x∗)T W(x∗, λ∗)Z(x∗)]ZT
k dk
=
−ZT
k W(x∗, λ∗)(xk −x∗) −Z(x∗)T W(x∗, λ∗)dk
+O(∥xk −x∗∥2) + O(∥Y (x∗)T dk∥) + o(∥dk∥)
=
−ZT
k W(x∗, λ∗)(xk + dk −x∗)
+O(∥xk −x∗∥2 + ∥Y (x∗)T dk∥) + o(∥dk∥).
(12.8.30)
Therefore, based on the assumption (12.8.27), it follows from the above in-
equality that
ZT
k W(x∗, λ∗)(xk+dk−x∗) = o(∥xk−x∗∥+∥dk∥)+O(∥Y (x∗)T dk∥). (12.8.31)
The deﬁnition of dk implies that
AT
k (xk + dk −x∗) = O(∥xk −x∗∥2).
(12.8.32)
Because A(x∗) is full column rank, we have that
∥Y (x∗)T dk∥= O(∥c(xk)∥) = O(∥dk−1∥2).
(12.8.33)

558
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
Using (12.8.31) and (12.8.32), we see that

ZT
k W(x∗, λ∗)
AT
k

(xk+dk−x∗) = o(∥xk−x∗∥+∥dk∥)+o(∥dk−1∥). (12.8.34)
Observing the assumption that B−1
k
is uniformly bounded, and using (12.8.26),
we can show that
∥dk∥= O(∥xk −x∗∥),
(12.8.35)
which indicates that ∥xk −x∗∥≤∥xk−1 −x∗∥+ ∥dk−1∥= O(∥xk−1 −x∗∥).
Thus, it follows from (12.8.34) that

ZT
k W(x∗, λ∗)
AT
k

(xk + dk −x∗) = o(∥xk−1 −x∗∥).
(12.8.36)
Similar to (12.3.19), we can prove that the matrix

Z(x∗)T W(x∗, λ∗)
A(x∗)T

(12.8.37)
is nonsingular. Therefore, it follows from (12.8.36) that
∥xk + dk −x∗∥= o(∥xk−1 −x∗∥),
(12.8.38)
which shows that the theorem is true.
2
The 2-step superlinearly convergence result of the two-side reduced Hes-
sian matrix method can not be improved. In fact, an example given by Yuan
[370] shows that it is possible to show that
∥x2k+1 −x∗∥∞
=
∥x2k −x∗∥∞,
(12.8.39)
∥x2k+2 −x∗∥∞
=
∥x2k+1 −x∗∥2
∞,
(12.8.40)
which reveals the “one-step fast one-step slow” behaviour of the two-side
reduced Hessian matrix method, and it shows that it is impossible to establish
a one-step Q-superlinearly convergence result. A similar example was also
given by Byrd [40] independently.
Exercises
1. Use the Lagrange-Newton method to solve Rosenbrock’s problem:
min
(1 −x1)2
s.t.
x2 −x2
1 = 0

12.8. REDUCED HESSIAN MATRIX METHOD
559
with initial point (0.8, 0.6)T and λ = 1.0. Give the ﬁrst three iterations.
2. The damped BFGS update (12.2.39) uses ¯yk which is a linear combina-
tion of yk and Bksk. Consider the case generating ¯yk by linear combinations
of yk and sk. What are the advantages and disadvantages of using sk instead
of Bksk?
3. Prove Corollary 12.3.4.
4.
If the QP subproblem in a SQP method is infeasible, one way to
overcome this diﬃculty is to consider the subproblem
mind∈ℜn,θ∈[0,1]
gT
k d + 1
2dT Bkd + σ(1 −θ)2
s.t.
ai(xk)T d + θci(xk) = 0,
i ∈E,
ai(xk)T d + θci(xk) ≥0,
i ∈I.
Let (d(σ), θ(σ) be the solution of the above QP subproblem. Prove that θ(σ)
is non-decreasing as σ increases. Discuss the case when θ(σ) = 0 for all σ > 0.
5. Consider application of the SQP method to the following problem:
min
−x1 + 10(x2
1 + x2
2)
s.t.
x2
1 + x2
2 = 1.
Give the point ¯x = (cos(θ), sin(θ))T and calculate d by solving the QP sub-
problem with B = I. Assume θ is very small and show that
Pσ(¯x) < Pσ(¯x + d)
for any σ ≥0, where Pσ(x) is the L1 exact penalty function. Calculate a
second-order correction step ˆd and verify that
Pσ(¯x) > Pσ(¯x + d + ˆd).
6. Prove that the Watchdog Technique (Algorithm 12.5.1) can overcome
the Maratos Eﬀect.

560
CHAPTER 12. SEQUENTIAL QUADRATIC PROGRAMMING
7. Apply the two-sided reduced Hessian method to the following problem:
min
1
2x2
2 −x1x2 +
1
6(1 −x2)3

−4(x2 −x1)3 −6(x2 −x1)2(x1 −x2
2)
−12(x2 −x1)(x1 −x2
2)2 −17(x1 −x2
2)3 + 3(x1 −x2
2)4
1 −x2

s.t.
x1 +
1
(1 −x2)2 [(x2 −x1)2 + (x2 −x1)(x1 −x2
2) + 2(x1 −x2
2)2] = 0
with initial point (ϵ, ϵ), where ϵ > 0 is a very small positive number. You
will ﬁnd the iterates converge to the solution (0, 0) in the one-fast-one-slow
pattern.

Chapter 13
Trust-Region Methods for
Constrained Problems
13.1
Introduction
Trust-region methods for unconstrained optimization have been discussed in
Chapter 6. In this chapter we consider trust-region methods for constrained
optimization.
The essential of a trust-region method is that the trial step is within
a trust-region. Unlike line search methods where line searches are carried
out along a search direction, trust-region algorithms compute a trial step dk
which satisﬁes
∥dk∥≤∆k,
(13.1.1)
where ∆k > 0 is the trust-region bound at the k-th iteration, and ∥· ∥
is some norm in ℜn. The fundamental belief is that the increment to the
variables should not be too large and that it seems not to be a wise idea to
search along a not-so-good direction (for example when step of one is not
accepted in the search direction).
For unconstrained optimization, a line
search type algorithm normally obtains its search direction by minimizing
an approximation model (for example, a quadratic model in a quasi-Newton
method). Minimizing the same approximation model with the trust-region
constraint:
∥d∥≤∆k
(13.1.2)
would give a trial step in the trust-region. Therefore it is obvious that almost
all line search algorithms for unconstrained optimization can be modiﬁed to

562
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
derive corresponding trust-region algorithms.
Unfortunately, the situation for constrained cases are not the same. First,
it is easy to see that we can not transform a line search algorithm for con-
strained optimization into a trust-region algorithm by simply adding a trust-
region constraint (13.1.2) to the subproblem of a line search algorithm. Be-
cause the subproblems of most line search algorithms for constrained opti-
mization have linear or quadratic constraints, which are approximations to
the original constraints, these linear or quadratic constraints may not be
consistent with the trust-region condition. For example, if the line search al-
gorithm we have in mind is the Wilson-Han-Powell method discussed in Sec-
tion 12.2, the undesirable situation is that the linearized constraints (12.2.2)–
(12.2.3) may have no solutions in the trust-region (13.1.2). To overcome this
infeasibility diﬃculty, some special considerations have to be made. There are
mainly three approaches, which lead to three diﬀerent types of trust-region
subproblems.
The ﬁrst approach is to scale the constraint violations:
θkci(xk) + dT ∇ci(xk)
=
0
i = 1, 2, . . . , me;
(13.1.3)
θkci(xk) + dT ∇ci(xk)
≥
0
i = me + 1, . . . , m
(13.1.4)
where θk ∈(0, 1] is a parameter (see Byrd, Schnabel and Shultz [48] and
Vardi [345]). We can see that a smaller θk would have smaller constraint
violations for the linearized constraints (13.1.3)–(13.1.4), which makes it more
likely that its feasible set has a nonempty intersection with the trust-region
(13.1.2). Geometrically, the parameter θk moves all the feasible points of the
linearized constraints (12.2.2)–(12.2.3) towards the origin with the fraction of
βk. Trial steps of the trust-region algorithms that apply null space techniques
can also be viewed as solutions of (13.1.2)–(13.1.4).
The second approach is replacing all the linearized constraints by a linear
squares constraint. Namely, linear constraints (12.2.2)–(12.2.3) are replaced
by a single constraint:
me

i=1
(ci(xk) + dT ∇ci(xk))2 +
m

i=me+1

min(0, ci(xk) + dT ∇ci(xk))
2 ≤ξk
(13.1.5)
where ξk ≥0 is a parameter.
It can be seen that if ξk = 0, the single
constraint on piece-wise linear squares is equivalent to the original linearized
constraints (12.2.2)–(12.2.3). The parameter ξk should be chosen in such a

13.2. LINEAR CONSTRAINTS
563
way that the constraint (13.1.5) has a nonempty intersection with the trust-
region ball (13.1.2).
The third way to overcome the inconsistency of the linearized constraints
and the trust-region constraint is replacing the linearized constraints by a
penalty term in the subproblem. This approach is essentially applying a trust-
region algorithm for nonsmooth optimization to the corresponding penalty
function.
A giant monograph on trust-region methods was published by Conn,
Gould and Toint [70].
13.2
Linear Constraints
In this section we give a trust-region algorithm for linearly constrained opti-
mization problems. The method uses trust-region conditions to deﬁne trial
steps and forces all iteration points in the feasible set. The method can be
considered as a combination of the feasible point method and a trust-region
technique.
Consider the linearly constrained problem
min
x∈ℜ
n
f(x)
(13.2.1)
s. t.
aT
i x = bi,
i ∈E,
(13.2.2)
aT
i x ≥bi,
i ∈I.
(13.2.3)
Assume that the current iterate point xk at the k-th iteration is feasible. The
trust-region subproblem can be deﬁned by
min
d∈ℜ
n
gT
k d + 1
2dT Bkd ∆= φk(d),
(13.2.4)
s.t.
aT
i d = 0, i ∈E,
(13.2.5)
aT
i (xk + d) ≥bi, i ∈I,
(13.2.6)
∥d∥∞≤∆k.
(13.2.7)
It is easy to see that (13.2.4)–(13.2.7) is a quadratic programming prob-
lem, which can be solved by methods discussed in Chapter 9. Let dk be a
solution of (13.2.4)–(13.2.7). Deﬁne the ratio of actual reduction and pre-
dicted reduction by
rk = f(xk) −f(xk + dk)
φk(0) −φk(dk)
.
(13.2.8)

564
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
From the deﬁnition of dk, we can easily see that dk = 0 if and only if xk is a
KKT point of the original problem (13.2.1)–(13.2.3). Because all constraints
are considered in the subproblem (13.2.4)–(13.2.7), the zigzagging can not
happen. The following is the statement of a trust-region algorithm, assuming
that the initial point x1 is feasible.
Algorithm 13.2.1
Step 1. Given x1 satisfying (13.2.2)–(13.2.3); given B1 ∈ℜn×n,
∆1 > 0, ε ≥0, k := 1.
Step 2. Solve (13.2.4)–(13.2.7) giving dk; if ∥dk∥≤ε then stop;
Compute (13.2.8);
xk+1 =

xk + dk,
if
rk > 0
xk,
Otherwise.
(13.2.9)
Step 3. If rk ≥0.25, go to Step 4;
∆k := ∆k/2, go to Step 5.
Step 4. If rk < 0.75 or ∥dk∥∞< ∆k then go to Step 5;
∆k := 2∆k.
Step 5. ∆k+1 := ∆k; Generate Bk+1;
k := k + 1; go to Step 2.
The matrix Bk+1 can be updated by quasi-Newton formulae.
In the
convergence analyses below, we assume that {Bk} are uniformly bounded.
Namely, there exists a positive constant M such that
∥Bk∥≤M
(13.2.10)
holds for all k.
Theorem 13.2.2 Assume that f(x) is continuously diﬀerentiable on the fea-
sible set and that (13.2.10) holds. If the sequence {xk} generated by Algo-
rithm 13.2.1 has accumulation points, then there exists an accumulation point
which is also a KKT point of the original constrained optimization problem
(13.2.1)–(13.2.3).

13.2. LINEAR CONSTRAINTS
565
Proof.
If the theorem is not true, we can show that
lim
k→∞∆k = 0.
(13.2.11)
If the above relation does not hold, there exists a positive constant δ > 0,
such that
∆k ≥δ
and
rk ≥0.25
(13.2.12)
hold for inﬁnitely many k. Deﬁne by K0 the set of all k such that (13.2.12)
hold. Without loss of generality, we assume that
lim
k∈K0
k→∞
xk = ¯x.
(13.2.13)
From our assumption, ¯x is not a KKT point of (13.2.1)–(13.2.2), thus d = 0
is not a solution of
min
g(¯x)T d + M
2 ∥d∥2
2
(13.2.14)
s.t.
aT
i d = 0, i ∈E,
(13.2.15)
aT
i (¯x + d) ≥0, i ∈I,
(13.2.16)
∥d∥∞≤δ/2.
(13.2.17)
Let ¯d be a solution of (13.2.14)–(13.2.17), then
η = g(¯x)T ¯d + 1
2M∥¯d∥2
2 < 0.
(13.2.18)
Thus, it follows from (13.2.12), (13.2.13) and (13.2.18) that
φk(0) −φk(dk) ≥−1
2η > 0
(13.2.19)
holds for all suﬃciently large k ∈K0. Using (13.2.19) and the second in-
equality of (13.2.12) we can see that
f(xk) −f(xk+1) ≥−1
8η > 0
(13.2.20)
holds for all suﬃciently large k ∈K0.
Because limk→∞f(xk) = f(¯x),
(13.2.20) can not hold for inﬁnitely many k. This contradiction indicates
that (13.2.11) must hold if the theorem is not true.

566
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Now we suppose that the theorem is not true. The above analyses imply
that (13.2.11) holds. There exists a subsequence K1 such that
rk < 0.25, ∀k ∈K1.
(13.2.21)
Assume that
lim
k∈K1
k→∞
xk = ˆx.
(13.2.22)
From our assumption, ˆx is not a KKT point. Let ˆd be a solution of the
subproblem
min
d∈ℜ
n
g(ˆx)T d + 1
2M∥d∥2
2,
(13.2.23)
s.t.
aT
i d = 0, i ∈E,
(13.2.24)
aT
i (ˆx + d) ≥bi, i ∈I,
(13.2.25)
∥d∥∞≤1,
(13.2.26)
then we have that
g(ˆx)T ˆd + M
2 ∥ˆd∥2
2 = ˆη < 0.
(13.2.27)
Thus, because (∆k ˆd) is a feasible point of the problem
mind∈ℜ
n
g(ˆx)T d + 1
2M∥d∥2
2,
(13.2.28)
s.t.
aT
i d = 0, i ∈E,
(13.2.29)
aT
i (ˆx + d) ≥bi, i ∈I,
(13.2.30)
∥d∥∞≤∆k,
(13.2.31)
we can see that
g(ˆx)T ˆdk + 1
2M∥ˆdk∥2
2 < ∆kˆη,
(13.2.32)
provided that ∆k ≤1, where ˆdk is a solution of (13.2.28)–(13.2.31). It follows
from (13.2.22) and (13.2.32) that
φk(0) −φk(dk) ≥−1
2 ˆη∆k
(13.2.33)
holds for all suﬃciently large k ∈K1. From the continuously diﬀerentiable
property of f(x) and the uniform boundedness of {Bk}, we have that
Predk = Aredk + o(∥dk∥).
(13.2.34)

13.2. LINEAR CONSTRAINTS
567
It can be shown from (13.2.33) and (13.2.34) that
lim
k∈K1
k→∞
rk = 1.
(13.2.35)
This contradicts (12.2.21). Therefore the theorem is true.
2
Similar to our analysis of the trust-region method for unconstrained op-
timization, Theorem 13.2.2 is still true if the condition (13.2.10) is replaced
by
∞

k=1
1
1 + max1≤i≤k ∥Bi∥= +∞.
(13.2.36)
From the proof of the above theorem, we can see that it is not necessary to
require the trial step dk to be the exact solution of (13.2.4)–(13.2.7). Deﬁne
the projected gradient of f(x) (with respect to the feasible set X) by
∇Xf(x) = lim
α→0+
P(x −α∇f(x)) −x
α
,
(13.2.37)
where
P(y) = arg min{∥z −y∥, z ∈X}.
It is not diﬃcult to show that x∗is a KKT point of (13.2.1)–(13.2.3) if and
only if
∇Xf(x∗) = 0.
(13.2.38)
From the proof of Theorem 13.2.2, we can see that Algorithm 13.2.1 remains
globally convergent provided that dk is a feasible point of (13.2.5)–(13.2.7)
and satisﬁes
φk(0) −φk(dk) ≥¯δ∥∇Xf(xk)∥min
*
∆k, ∥∇Xf(xk)∥
∥Bk∥
+
.
(13.2.39)
As for local convergence analysis, we assume that xk →x∗and that there
are only equality constraints. We also assume that the second-order suﬃcient
conditions hold at x∗and that the Jacobian matrix A(x∗) = ∇c(x∗)T ∈ℜn×m
has full column rank. Under these conditions, it is not diﬃcult to show that
Algorithm 13.2.1 is superlinearly convergent, namely
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
= 0
(13.2.40)

568
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
if and only if
lim
k→∞
∥Z∗T (Bk −[∇2f(x∗) −
 λ∗
i ∇2ci(x∗)])Z∗Z∗T dk∥
∥dk∥
= 0,
(13.2.41)
where Z∗∈ℜn×(n−m) is a matrix satisfying Z∗T A(x∗) = 0 and Z∗T Z∗= I.
13.3
Trust-Region Subproblems
The key part of a trust-region algorithm is the calculation of the trial step
dk, which is normally a solution of a trust-region subproblem. Therefore the
crucial issue of a trust-region algorithm is the construction of the trust-region
subproblem. Because one of the most successful line search type methods is
the sequential quadratic programming method, it is natural to consider the
combination of quadratic models and trust-region technique. The combined
method is usually called the TR-SQP (Trust-Region – Sequential Quadratic
Programming) method. Since a trust-region constraint has the form
∥d∥≤∆k,
(13.3.1)
directly amalgamating (13.3.1) and the quadratic programming subproblem
(12.2.1)–(12.2.3) of the sequential quadratic programming method gives the
following subproblem:
mind∈ℜ
n
gT
k d + 1
2dT Bkd ∆= φk(d),
(13.3.2)
s.t.
ci(xk) + ai(xk)T d = 0, i ∈E
(13.3.3)
ci(xk) + ai(xk)T d ≥0, i ∈I
(13.3.4)
∥d∥≤∆k.
(13.3.5)
This is not a perfect way, as the constraints (13.3.3)–(13.3.5) might have
no solutions. Therefore, the subproblem (13.3.2)–(13.3.5) has to be modi-
ﬁed in order to derive a reasonable trust-region subproblem for constrained
optimization.
First, we can consider a subproblem of the following type:
mind∈ℜ
n
gT
k d + 1
2dT Bkd ∆= φk(d),
(13.3.6)
s.t.
θkci(xk) + dT ∇ci(xk) = 0, i ∈E,
(13.3.7)
θkci(xk) + dT ∇ci(xk) ≥0, i ∈I,
(13.3.8)
∥d∥≤∆k,
(13.3.9)

13.3. TRUST-REGION SUBPROBLEMS
569
where θk ∈(0, 1] is a parameter. Subproblem (13.3.7)–(13.3.9) usually has
feasible points when θk is suﬃciently small. Geometrically, multiplying ci(xk)
by a factor θk is to pull the feasible points of the linearized constraints towards
the original. In other words, the role of θk is to shift the line corresponding
to the linearized constraints to a parallel line that intersects the trust-region.
This technique of introducing a parameter had already been used in line
search algorithms.
If θk ̸= 1, obviously the trial step dk obtained by solving subproblem
(13.3.6)–(13.3.9) may not be a feasible point of (13.3.3) and (13.3.4).
In
order to force dk to be as feasible in the sense of (13.3.3)–(13.3.4) as possible,
we should choose θk as close to 1 as possible. On the other hand, the larger
the parameter θk, the smaller the feasible set of (13.3.7)–(13.3.9). To allow
certain freedom to the subproblem, we should not choose a too large θk.
The minimum-norm solution of the problem
min
d∈ℜ
n ∥(c(xk) + A(xk)T d)(−)∥2
(13.3.10)
is called the Gauss-Newton step, which is denoted by dGN
k
.
Here c(−) is
deﬁned by (10.1.5)–(10.1.6). From the deﬁnition of dGN
k
, (13.3.7)–(13.3.9) is
feasible if and only if
θk∥dGN
k
∥≤∆k.
(13.3.11)
To avoid unnecessary small θk, it is reasonable to require
θk∥dGN
k
∥≥δ1∆k
(13.3.12)
if θk < 1, where δ1 ∈(0, 1) is a given constant. For example, we can deﬁne
θk by the formula
θk =

1,
if
2∥dGN
k
∥≤∆k,
1
2∆k/∥dGN
k
∥,
otherwise.
(13.3.13)
An indirect way to choose the parameter θk is regarding θ = θk as a
variable. The idea of forcing θ as large as possible is achieved by a penalty
term σ(θ −1)2. The subproblem can be written as
mind∈ℜ
n,θ∈(0,1]
gT
k d + 1
2dT Bkd + σk(θ −1)2,
(13.3.14)
s.t.
θci(xk) + dT ∇ci(xk) = 0, i ∈E,
(13.3.15)
θci(xk) + dT ∇ci(xk) ≥0, i ∈I,
(13.3.16)
∥d∥≤∆k,
(13.3.17)

570
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
where σk > 0 is a penalty parameter.
Another method to overcome the inconsistency of (13.3.3)–(13.3.5) is re-
placing (13.3.3)–(13.3.4) by a single constraint, which requires the sum of the
squares of all linearized constraints being bounded by a certain bound:
∥(ck + AT
k d)(−)∥2
2 ≤ξk,
(13.3.18)
where ck = c(xk) = (c1(xk), · · ·, cm(xk))T , Ak = A(xk) = ∇c(xk)T , and
ξk > 0 is a parameter. Thus, the subproblem has the form
min
d∈ℜ
n
gT
k d + 1
2dT Bkd,
(13.3.19)
s.t.
∥(ck + AT
k d)(−)∥2
2 ≤ξk,
(13.3.20)
∥d∥2
2 ≤∆2
k.
(13.3.21)
It is easy to see that ξk must satisfy
ξk ≥
min
∥d∥2≤∆k
∥(ck + AT
k d)(−)∥2
2,
(13.3.22)
in order to secure the feasibility of (13.3.20)–(13.3.21). Let ¯dk be the negative
gradient direction of the function ∥(ck + AT
k d)(−)∥2
2 at d = 0, namely ¯dk =
−Akc(−)
k
, and let ¯αk > 0 be the solution of problem
min
α>0
∥α ¯
dk∥2≤∆k
∥(ck + AT
k α ¯dk)(−)∥2
2.
(13.3.23)
We call ¯αk ¯dk the Cauchy point or the Cauchy step, which is denoted by dCP
k
.
In the method of Celis, Dennis and Tapia [52],
ξk = ∥(ck + AT
k dCP
k
)(−)∥2
2,
(13.3.24)
while in Powell and Yuan [278], ξk can be any number satisfying
min
∥d∥2≤b1∆k
∥(ck + AT
k d)(−)∥2
2 ≤ξk ≤
min
∥d∥2≤b2∆k
∥(ck + AT
k d)(−)∥2
2,
(13.3.25)
where b1 ≥b2 are two positive constants in (0, 1).
The third type of trust-region subproblem is based on exact penalty func-
tions. For example, based on the exact penalty function
P(x, σ) = f(x) + σ∥c(−)(x)∥,
(13.3.26)

13.4. NULL SPACE METHOD
571
we can construct trust-region subproblem
min
d∈ℜ
n
gT
k d + 1
2dT Bkd + σk∥(ck + AT
k d)(−)∥,
(13.3.27)
s.t.
∥d∥≤∆k.
(13.3.28)
For this kind of subproblems, the norm in (13.3.27) and that in (13.3.28) may
not be necessarily the same. For example, if we take the l1-norm in (13.3.27)
and l∞-norm in (13.3.28), we obtain the subproblem
min
d∈ℜ
n
gT
k d + 1
2dT Bkd + σk

i∈E
|ci(xk) + ∇ci(xk)T d|
+σk

i∈I
|ci(xk) + ∇ci(xk)T d|(−)
(13.3.29)
s.t.
∥d∥∞≤∆k.
(13.3.30)
Essentially, a trust-region algorithm based on subproblem (13.2.27)–(13.2.28)
is the same as a nonsmooth trust-region algorithm for minimizing the exact
penalty function (13.3.26).
13.4
Null Space Method
Consider the equality constrained problem
minx∈ℜ
n
f(x),
(13.4.1)
s.t.
c(x) = 0.
(13.4.2)
The trust-region subproblem (13.3.6)–(13.3.9) can be written as
mind∈ℜ
n
gT
k d + 1
2dT Bkd = φk(d),
(13.4.3)
s.t.
θkck + AT
k d = 0,
(13.4.4)
∥d∥2 ≤∆k.
(13.4.5)
We assume that ck ∈Range (AT
k ), it follows from (13.3.11) that θk should
satisfy
θk∥(AT
k )+ck∥2 ≤∆k.
(13.4.6)

572
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Let dk be a solution of (13.4.3)–(13.4.5). It can be seen that dk is also a
solution of the following problem
mind∈ℜ
n
φk(d),
(13.4.7)
s.t.
AT
k (d −ˆdk) = 0,
(13.4.8)
∥d −ˆdk∥2 ≤¯∆k,
(13.4.9)
where
ˆdk
=
−θk(AT
k )+ck,
(13.4.10)
¯∆k
=

∆2
k −∥ˆdk∥2
2.
(13.4.11)
Notice that ˆdk = θkdGN
k
where dGN
k
is the Gauss-Newton step discussed in
the previous section. Deﬁne variable ¯d = d−ˆdk and let Zk be a matrix whose
columns are an orthonormal base of the null space of AT
k , namely AT
k Zk = 0,
ZT
k Zk = I. We can then write
¯d = Zku, u ∈ℜn−r,
(13.4.12)
where r is the rank of Ak. Using the above relation, we can rewrite subprob-
lem (13.4.7)–(13.4.9) in the following equivalent form:
minu∈ℜ
n−r
¯gT
k u + 1
2uT ¯Bku,
(13.4.13)
s.t.
∥u∥2 ≤¯∆k,
(13.4.14)
where ¯gk = ZT
k (gk +Bk ˆdk), ¯Bk = ZT
k BkZk. This is already in the form of the
trust-region subproblem for unconstrained optimization, which is discussed
in Chapter 6. Techniques given there can be used to solve problem (13.4.13)–
(13.4.14), giving uk. Once uk is computed, the trial step dk can be obtained
by using dk = ˆdk + Zkuk.
We use the L1 exact penalty function
P1(x) = f(x) + σk∥c(x)∥1
(13.4.15)
as the merit function to decide whether the trial step dk should be accepted.
The actual reduction of the exact penalty function is
Aredk = P1(xk) −P1(xk + dk).
(13.4.16)

13.4. NULL SPACE METHOD
573
We deﬁne the predicted reduction by the reduction of the approximate penalty
function φk(d) + σk∥ck + AT
k d∥1, namely,
Predk = φk(0) −φk(dk) + σk[∥ck∥1 −∥ck + AT
k dk∥1].
(13.4.17)
Assume that f(x) and c(x) are twice continuously diﬀerentiable and ∥Bk∥is
bounded, then we have that
Aredk = Predk + O(∥dk∥2
2).
(13.4.18)
From the deﬁnition of ˆdk, it follows that
ˆdk
=
(AT
k )+AT
k dk,
(13.4.19)
dk −ˆdk
=
ZkZT
k dk = (I −(AT
k )+AT
k )dk.
(13.4.20)
The step ˆdk is a vector in the range space of Ak, hence it is called the range
space step. While the step dk −ˆdk is in the null space of AT
k , it is called the
null space step. Geometrically, it is often that the range space step is vertical
and the null space step is horizontal when we sketch an illustrated diagram
(for example, see Figure 13.4.1). Therefore, the range space step and the null
space step are called the vertical step and the horizontal step respectively.
Figure 13.4.1
Using the vertical step and the horizontal step, we can decompose the
predicted reduction into two parts:
V predk
=
φk(0) −φk( ˆdk) + σk(∥ck∥1 −∥ck + AT
k ˆdk∥1), (13.4.21)
Hpredk
=
φk( ˆdk) −φk(dk).
(13.4.22)

574
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
We can assume that θk satisﬁes the “not too small” condition:
θk∥(AT
k )+ck∥2 ≥δ1∆k, if θk < 1.
(13.4.23)
Assume again that we choose a suﬃciently large σk so that
σk ≥
A+
k
 
gk + 1
2Bk ˆdk
!
∞
+ ρ.
(13.4.24)
With all these, we can show that
V predk ≥ρ min[∥ck∥1, δ1∆k/∥(AT
k )+∥2].
(13.4.25)
For the null space, the situation is essentially the same as that of uncon-
strained optimization. Applying Lemma 6.1.3, we have that
Hpredk ≥1
2∥¯gk∥2 min[ ¯∆k, ∥¯gk∥2/∥¯Bk∥2].
(13.4.26)
Thus, we have established that there exist positive constants ρ1, ρ2 such that
Predk
≥
ρ1 min[∥ck∥1, ∆k/∥(AT
k )+∥2]
+ρ2∥¯gk∥2 min[ ¯∆k, ∥¯gk∥2/∥¯Bk∥2].
(13.4.27)
In practice, we can ﬁrst compute ˆdk using the Gauss-Newton step, and then
obtain uk by solving (13.4.13)–(13.4.14) inexactly. The vector dk = ˆdk+Zkuk
satisﬁes the suﬃcient reduction condition (13.4.27).
The following is a trust-region algorithm based on null space technique.
Algorithm 13.4.1
Step 1. Given x1 ∈ℜn, ∆1 > 0, ϵ ≥0.
0 < β3 < β4 < 1 < β1, 0 ≤β0 ≤β2 < 1,
β2 > 0, σ1 > 0, k := 1;
Step 2. If ∥ck∥2 + ∥ZT
k gk∥2 ≤ϵ then stop;
If (13.4.24) is satisﬁed then go to Step 3. Set
σk =
A+
k
 
gk + 1
2Bk ˆdk
! + 2ρ;
Step 3. Compute a trial step dk satisfying (13.4.27).

13.4. NULL SPACE METHOD
575
Step 4. Compute Aredk and Predk by (13.4.16)–(13.4.17);
Set rk = Aredk/Predk;
xk+1 =

xk + dk,
if rk > β0,
xk,
otherwise.
(13.4.28)
Choose ∆k+1 such that
∆k+1 ∈

(β3∥dk∥2, β4∆k),
if rk < β2,
(∆k, β1∆k),
otherwise.
(13.4.29)
Step 5. Generate Bk+1; Set σk+1 := σk and k := k + 1; go to Step
2.
In order to establish the global convergence of the above algorithm, we
need the following lemma.
Lemma 13.4.2 If dk satisﬁes (13.4.27), the inequality
Predk ≥τ min[εk, 1] min[∆k, εk/(1 + ∥Bk∥2)]
(13.4.30)
holds, where τ = min[ρ1/[2 max(1, ∥A+
k ∥2)], ρ2/4] and
εk = ∥ck∥2 + ∥ZT
k gk∥2.
(13.4.31)
Proof.
If
∥ck∥2 >
∆k
2∥A+
k ∥2
,
(13.4.32)
it follows directly from (13.4.27) that
Predk ≥ρ1∆k/2∥A+
k ∥2.
(13.4.33)
Thus we see that (13.4.30) holds.
Therefore, for the rest of the proof we can assume that (13.4.32) is not
true. This implies that
¯∆k =

∆2
k −∥ˆdk∥2
2 ≥

∆2
k −(∥A+
k ∥2∥ck∥2
2)2 ≥1
2∆k.
(13.4.34)
If
(1 + 2∥Bk∥2∥A+
k ∥2)∥ck∥2 ≤∥ZT
k gk∥2,
(13.4.35)

576
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
we can show that
∥¯gk∥2
≥
∥ZT
k gk∥2 −∥Bk∥2∥ˆdk∥2
≥
∥ZT
k gk∥2 −∥Bk∥2∥A+
k ∥2∥ck∥2
≥
1
2(∥ZT
k gk∥2 + ∥ck∥2) = 1
2εk.
(13.4.36)
This inequality, (13.4.34) and (13.4.27) indicate that (13.4.30) holds when
τ = ρ2/4.
Now, we assume that inequality (13.4.35) does not hold, which implies
that
∥ck∥2 ≥
εk
2(1 + ∥Bk∥2∥A+
k ∥2).
(13.4.37)
Consequently, we can use (13.4.27) to show that
Predk
≥
ρ1 min[εk/[2(1 + ∥Bk∥2∥A+
k ∥2)], ∆k/∥A+
k ∥2]
≥
ρ1
2 max[1, ∥A+
k ∥2] min[∆k, εk/(1 + ∥Bk∥2)],
(13.4.38)
which says that (13.4.30) holds when τ = ρ1/(2 max[1, ∥A+
k ∥2]).
2
The following lemma says that the norm of the trial step can not converge
to zero faster than the reciprocal of the norm of quasi-Newton matrices Bk,
if the iteration points are bounded away from KKT points.
Lemma 13.4.3 Assume that f(x) and c(x) are twice continuously diﬀeren-
tiable and that {xk, k = 1, 2, ...} are generated by Algorithm 13.4.1. If ∥Ak∥2
is bounded above uniformly and
∥ck∥2 + ∥ZT
k gk∥2 ≥δ > 0
(13.4.39)
for all k, then there exists a positive constant β5 such that
∥dk∥2 ≥β5/Mk, k = 1, 2, · · ·
(13.4.40)
holds for all k, where
Mk = max
1≤i≤k ∥Bi∥2 + 1.
(13.4.41)
Proof.
If ∥dk∥2 < ∆k, we can easily see that
∥¯dk∥2 < ¯∆k.
(13.4.42)

13.4. NULL SPACE METHOD
577
The deﬁnition of ¯dk and the above inequality imply that
¯gk + ¯Bk ¯dk = 0.
(13.4.43)
Thus, we have
∥¯dk∥2 ≥∥¯gk∥2
∥¯Bk∥2
≥∥ZT
k gk∥2
∥Bk∥2
≥∥ZT
k gk∥2
Mk
.
(13.4.44)
From the deﬁnition of ˆdk, we see that
∥ˆdk∥2 ≥min

δ1∆k, ∥ck∥2
∥Ak∥2

.
(13.4.45)
Relations (13.4.44) and (13.4.45) indicate that either
∥dk∥2 ≥δ1∆k,
(13.4.46)
or
∥dk∥2 ≥
1
2(1 + ∥Ak∥2)
∥ck∥2 + ∥ZT
k gk∥2
Mk
.
(13.4.47)
The boundedness of ∥Ak∥shows that (13.4.40) holds if (13.4.46) fails.
For the rest of the proof, we assume that (13.4.46) holds. If the lemma
is not true there exists a subsequence {ki} such that
∥dki∥2 ≥δ1∆ki
(13.4.48)
and
lim
i→∞∆kiMki = 0.
(13.4.49)
A direct consequence of the above limit is ∆ki →0. Because Mk is monotoni-
cally increasing, we can assume that ∆ki < ∆ki−1 for all i. Denote ¯i = ki −1,
(13.4.29) implies that ∥d¯i∥2 ≥∆ki/β3, which, together with (13.4.49) and
M¯i ≤Mki, shows that
lim
i→∞∥d¯i∥M¯i = 0.
(13.4.50)
This shows that
∥d¯i∥≥δ1∆¯i.
(13.4.51)
This inequality guarantees the existence of a positive number ¯τ such that
Pred¯i ≥¯τ∥d¯i∥2
(13.4.52)

578
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
for suﬃciently large i. This inequality and (13.4.18) show that
lim
i→∞rki−1 = lim
i→∞
Ared¯i
Pred¯i
= 1.
(13.4.53)
Therefore, for suﬃcient large i, we have that
∆ki ≥∆ki−1.
(13.4.54)
This contradicts the assumption that ∆ki−1 < ∆ki. Thus, the lemma is true.
2
The following lemma is due to Powell [264], which is a very powerful tool
for convergence analysis of trust-region algorithms.
Lemma 13.4.4 Suppose that {∆k} and {Mk} are two sequences of positive
numbers. If there exist positive constants τ > 0, β1 > 0, β4 ∈(0, 1), and a
subset I of {1, 2, 3, · · ·} such that
∆k+1 ≤β1∆k,
∀k ∈I;
∆k+1 ≤β4∆k,
∀k ̸∈I;
∆k ≥τ/Mk,
∀k;
(13.4.55)
Mk+1 ≥Mk,
∀k;

k∈I 1/Mk < +∞,
then
∞

k=1
1
Mk
< +∞.
(13.4.56)
Proof.
Let p be a positive integer satisfying
β1 · βp−1
4
< 1.
(13.4.57)
Deﬁne the set
Ik = I ∩{1, 2, · · · , k}.
(13.4.58)
Denote the number of elements of Ik by |Ik|. Deﬁne the set
J := {k | k ≤p|Ik|}.
(13.4.59)
From the monotone property of Mk and the above deﬁnition, we have that

k∈J
1
Mk
≤p

k∈I
1
Mk
< +∞.
(13.4.60)

13.4. NULL SPACE METHOD
579
For k ̸∈J, we have that |Ik| < k/p, which gives |Ik−1| ≤|Ik| ≤(k −1)/p.
Thus,
∆k
≤
β|Ik−1|
1
βk−1−|Ik−1|
4
∆1
≤
(β1βp−1
4
)(k−1)/p∆1
(13.4.61)
holds for all k ̸∈J. Consequently, we have that

k̸∈J
1
Mk
≤
∞

k=1
(β1βp−1
4
)(k−1)/p∆1/τ
=
∆1
τ[1 −(β1βp−1
4
)1/p]
.
(13.4.62)
Now, we can see that (13.4.56) follows from (13.4.60) and (13.4.62).
2
Using the above lemmas, we can prove the global convergence of Algo-
rithm 13.4.1.
Theorem 13.4.5 Assume that f(x) and c(x) are twice continuously diﬀer-
entiable, that all the iteration points {xk} generated by Algorithm 13.4.1 are
in an open set S, and that ∇f(x), ∇2f(x), A(x), ∇A(x) are bounded above
on S. If σk = ¯σ for all suﬃciently large k, P1(xk) is bounded below, and
{∥Ak∥2, ∥A+
k ∥2} are uniformly bounded, and
∞

k=1
1
1 + max1≤i≤k ∥Bi∥2
= ∞,
(13.4.63)
then
lim inf
k→∞[∥ck∥2 + ∥ZT
k gk∥2] = 0.
(13.4.64)
Furthermore, under additional assumptions that ∥Bk∥2 is uniformly bounded
and β0 > 0, we have that
lim
k→∞[∥ck∥2 + ∥ZT
k gk∥2] = 0.
(13.4.65)
Proof.
If the theorem is not true, the sequence {P¯σ(xk) = f(xk) +
¯σ∥c(xk)∥1} is bounded below and there exists a positive constant δ such
that (13.4.39) holds for all k. Deﬁne the set
I = {k|rk ≥β2},
(13.4.66)

580
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
then it follows from Lemmas 13.4.2 and 13.4.3 that
+∞
>
∞

k=1
[P¯σ(xk) −P¯σ(xk+1)] ≥

k∈I
[P¯σ(xk) −P¯σ(xk+1)]
≥

k∈I
β2Predk ≥

k∈I
1
2β2δ min[∆k, δ/Mk]
≥

k∈I
1
2β2δ min[β5, δ]/Mk.
(13.4.67)
This inequality and the previous lemma imply that
∞

k=1
1
Mk
< +∞,
(13.4.68)
which contracts (13.4.63). Therefore the theorem is true.
2
13.5
CDT Subproblem
Consider the subproblem (13.3.19)–(13.3.21) for the case when there are only
equality constraints (me = m). It can be written as
min
d∈ℜ
n
gT d + 1
2dT Bd = φ(d),
(13.5.1)
s.t.
∥AT d + c∥2 ≤ξ,
(13.5.2)
∥d∥2 ≤∆,
(13.5.3)
here we omit the subscript for convenience. Such a subproblem was proposed
by Celis, Dennis and Tapia [52], and is generally called the CDT subproblem.
Obviously, only when
ξ ≥ξmin =
min
∥d∥2≤∆∥AT d + c∥2,
(13.5.4)
there exist feasible points for (13.5.2)–(13.5.3).
First, we consider the case when ξ = ξmin. It is easy to deduce from the
convexity of ∥d||2 that either there is only one feasible solution of (13.5.2)–
(13.5.3) or that
ξ = ξ∗
min = min
d∈ℜ
n ∥AT d + c∥2.
(13.5.5)

13.5. CDT SUBPROBLEM
581
The case when there is only one feasible solution of (13.5.2)–(13.5.3) requires
no further consideration since this feasible point must be the solution of the
CDT subproblem. This feasible point must have the following form:
d = −(AAT + λI)+c,
(13.5.6)
where λ ≥0, and λ = 0 if ∥d∥2 < ∆.
AAT + λI is nonsingular unless
∥d∥2 = ∆. For the case when (13.5.5) holds, we have that
∥ˆd∥2 ≤∆,
(13.5.7)
where ˆd = −(AT )+c is the minimal norm solution (also called the Gauss-
Newton Step). Let Z be a matrix whose columns are a basis of the null space
of AT . By the variable substitution d = ˆd + Zu as given in the previous
section, problem (13.5.1)–(13.5.3) can be transformed as
minu∈ℜ¯n
¯gT u + 1
2uT ¯Bu,
(13.5.8)
s.t.
∥u∥2 ≤¯∆,
(13.5.9)
which is already in the form of the trust-region subproblem for unconstrained
optimization discussed in Chapter 6.
Therefore in this section, we concentrate our attention on the case when
ξ > ξmin.
(13.5.10)
First we have the following necessary result.
Theorem 13.5.1 Let d∗be a global solution of the subproblem (13.5.1)–
(13.5.3). Assume that (13.5.10) holds. Then there exist nonnegative con-
stants λ∗, µ∗such that
(B + λ∗I + µ∗AAT )d∗= −(g + µ∗Ac),
(13.5.11)
where λ∗and µ∗satisfy the complementarity conditions
λ∗[∆−∥d∗∥2]
=
0,
(13.5.12)
µ∗[ξ −∥c + AT d∗∥2]
=
0.
(13.5.13)
Furthermore, the matrix
H(λ∗, µ∗) = B + λ∗I + µ∗AAT
(13.5.14)
has at most one negative eigenvalue if the multipliers λ∗and µ∗are unique.

582
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Proof.
Assumption (13.5.10) implies that the feasible region X of (13.5.2)–
(13.5.3) is convex and has a nonempty interior, and we can easily prove that
LFD(d∗, X) = SFD(d∗, X). From the results in Chapter 8, there exist non-
negative numbers λ∗and µ∗such that (13.5.11)–(13.5.13) hold. To complete
the proof, we only need to prove that the matrix H(λ∗, µ∗) has no more than
one negative eigenvalue if the multipliers λ∗, µ∗are unique.
If at most one of the constraints (13.5.2)–(13.5.3) is active at the solution
d∗, the second-order suﬃcient condition given in Chapter 8 shows that the
matrix H(λ∗, µ∗) has at most one negative eigenvalue.
For the rest of the proof, we assume that both constraints are active.
Deﬁne the vector
y∗= A(c + AT d∗).
(13.5.15)
If d∗and y∗are linearly dependent, there exists η ∈ℜsuch that
y∗= ηd∗.
(13.5.16)
The assumption ξ > ξmin implies η > 0.
It follows from the uniqueness
of λ∗and µ∗that λ∗= µ∗= 0.
Thus, d∗is a stationary point of φ(d).
From (13.5.16) and η > 0 we see that d is a feasible direction provided that
dT d∗< 0. This shows that dT Bd ≥0 holds for all d satisfying dT d∗≤0,
which implies that B is a semi-deﬁnite matrix.
If d∗and y∗are linearly independent, the second-order necessary condi-
tion shows that the matrix H(λ∗, µ∗) is positive semi-deﬁnite in the n−2 di-
mensional subspace orthogonal to d∗and y∗. Assume that H(λ∗, µ∗) has two
negative eigenvalues, then there exist linearly independent vectors z1, z2 ∈ℜn
such that H(λ∗, µ∗) is negative deﬁnite on Span(z1, z2). The intersection of
Span(z1, z2) and the n −2 dimensional subspace mentioned above is empty
except the original. Therefore the matrix

zT
1 d∗
zT
2 d∗
zT
1 y∗
zT
2 y∗

(13.5.17)
is nonsingular. The nonsingularity of the above matrix implies the existence
of a nonzero vector ¯d ∈Span(z1, z2) such that
∥d∗+ ¯d∥2 = ∆, ∥c + AT (d∗+ ¯d)∥2 = ξ.
(13.5.18)
Relation (13.5.18) and the negative deﬁniteness of H(λ∗, µ∗) on Span(z1, z2)
give that φ(d∗+ ¯d) < φ(d∗). This contradicts the optimality of d∗. Hence
the lemma is true.
2
The following is a suﬃcient condition.

13.5. CDT SUBPROBLEM
583
Theorem 13.5.2 Let d∗be a feasible point of (13.5.2)–(13.5.3). If there ex-
ist λ∗≥0 and µ∗≥0 such that (13.5.11)–(13.5.13) hold, and that H(λ∗, µ∗)
is positive semi-deﬁnite, then d∗is a global solution of (13.5.1)–(13.5.13).
Proof.
Let d be any vector satisfying (13.5.2)–(13.5.3). We have that
φ(d)
=
φ(d) + 1
2λ∗∥d∥2
2 + 1
2µ∗∥c + AT d∥2
2
−1
2[λ∗∥d∥2
2 + µ∗∥c + AT d∥2
2]
≥
φ(d∗) + 1
2λ∗∥d∗∥2
2 + 1
2µ∗∥c + AT d∗∥2
2
−1
2[λ∗∥d∥2
2 + µ∗∥c + AT d∥2
2]
=
φ(d∗) + 1
2λ∗[∆2 −∥d∥2
2] + 1
2µ∗[ξ2 −∥c + AT d∥2
2]
≥
φ(d∗).
(13.5.19)
Thus, we can see that d∗is a global solution of (13.5.1)–(13.5.3).
2
A direct consequence of the above theorem is the following.
Corollary 13.5.3 Assume that B is positive semi-deﬁnite. A feasible point
d∗of (13.5.2)–(13.5.3) is a solution of (13.5.1)–(13.5.3) if and only if there
exist λ∗≥0, µ∗≥0 such that (13.5.11)-(13.5.13) hold.
Therefore, when B is positive deﬁnite, the solution of (13.5.1)–(13.5.3)
must have the form
d(λ, µ) = −H(λ, µ)−1[g + µAc].
(13.5.20)
From Corollary 13.5.3 we can easily see that the following lemma holds.
Lemma 13.5.4 Assume that B is positive deﬁnite. Then d(λ, µ) deﬁned by
(13.5.20) is a solution of (13.5.1)–(13.5.3) if and only if it is a feasible point
of (13.5.2)–(13.5.3), and one of the following holds:
1. λ = µ = 0;
2. λ > 0, µ = 0, ∥d(λ, µ)∥2 = ∆;
3. λ = 0, µ > 0, ∥c + AT d(λ, µ)∥2 = ξ;

584
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
4. λ > 0, µ > 0, ∥d(λ, µ)∥2 = ∆, ∥c + AT d(λ, µ)∥2 = ξ.
From the above statements, solving a convex CDT subproblem is equiv-
alent to ﬁnding λ∗, µ∗≥0 such that d(λ∗, µ∗) is feasible and one of the four
possibilities in Lemma 13.5.4 holds.
For the case λ∗= µ∗= 0, the solution is d = −B−1g.
For µ∗= 0 and λ∗> 0, we can solve ¯ψ(λ, 0) = 0 to obtain λ∗, where
¯ψ(λ, µ) =
1
∥d(λ, µ)∥2
−1
∆.
(13.5.21)
The reason for considering ¯ψ(λ, 0) = 0 instead of ∥d(λ, 0)∥2 = ∆is similar to
that in Chapter 6, namely ¯ψ(λ, 0) behaves more like a linear function. ¯ψ(λ, µ)
as a function of λ is concave and increasing, thus we can apply Newton’s
iteration:
λ+ = λ −
¯ψ(λ, 0)
¯ψ
′
λ(λ, 0).
(13.5.22)
It is not diﬃcult to show that iteration process (13.5.22) with any initial
λ ∈[0, λ∗] will generate a monotone increasing sequence converging to λ∗.
When λ∗= 0 and µ∗> 0, we deﬁne
ˆψ(λ, µ) =
1
∥c + AT d(λ, µ)∥2
−1
ξ .
(13.5.23)
Similarly, we can apply Newton’s method to ˆψ(0, µ) = 0, that is,
µ+ = µ −
ˆψ(0, µ)
ˆψ
′
µ(0, µ)
.
(13.5.24)
When λ∗> 0 and µ∗> 0, we need to solve
¯ψ(λ, µ)
=
0,
(13.5.25)
ˆψ(λ, µ)
=
0.
(13.5.26)
The Newton iteration for the above system is

λ+
µ+

=

λ
µ

−J(λ, µ)−1
 ¯ψ(λ, µ)
ˆψ(λ, µ)

,
(13.5.27)
where J(λ, µ) is the Jacobi matrix:
J(λ, µ) =
 ¯ψ
′
λ(λ, µ)
¯ψ
′
µ(λ, µ)
ˆψ
′
λ(λ, µ)
ˆψ
′
µ(λ, µ)

.
(13.5.28)
An algorithm based on the above analyses is given as follows.

13.6. POWELL-YUAN ALGORITHM
585
Algorithm 13.5.5
Step 1. Given g ∈ℜn, B positive deﬁnite, ∆> 0, ξ > ξmin.
Step 2. Compute d(0, 0). If d(0, 0) is feasible then stop;
If ∥d(0, 0)∥≤∆then go to Step 4;
Step 3. Applying (13.5.22) to solve ¯ψ(λ, 0) = 0 giving λ∗;
If d(λ∗, 0) is feasible then stop;
Step 4 Applying (13.5.24) to solve ˆψ(0, µ) = 0 giving µ∗;
If d(0, µ∗) is feasible then stop;
Step 5. Applying (13.5.27) to solve (13.5.25)–(13.5.26) giving λ∗, µ∗;
stop.
The above algorithm is in fact an enumeration of the four cases given in
Lemma 13.5.4. A more direct way is to solve the system
 ¯ψ(λ, µ)
ˆψ(λ, µ)

≥0, (λ, µ)T
 ¯ψ(λ, µ)
ˆψ(λ, µ)

= 0
(13.5.29)
in the nonnegative orthant ℜ2
+ = {λ ≥0, µ ≥0}. Such an approach to
identify the Lagrange multipliers λ∗and µ∗is equivalent to solving the dual
problem of (13.5.1)–(13.5.3). A truncated Newton’s method based on the
dual of (13.5.1)–(13.5.3) is given by Yuan [373], which is basically the Newton-
Raphson method for the nonlinear system (13.5.29). The approach given by
Zhang [381] is to reformulate (13.5.29) as a univariate problem. Basically it
is to solve the problem
ˆψ(λ(µ), µ) = 0
(13.5.30)
where λ(µ) is deﬁned by ¯ψ(λ, µ) = 0.
13.6
Powell-Yuan Algorithm
Consider the constrained optimization problem (13.4.1)–(13.4.2). The trial
step dk is obtained by solving
min
d∈ℜ
n
gT
k d + 1
2dT Bkd = φk(d),
(13.6.1)
s. t.
∥ck + AT
k d∥2 ≤ξk,
(13.6.2)
∥d∥2 ≤∆k,
(13.6.3)

586
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
where ∆k is the trust-region radius, ξk is a parameter satisfying (13.3.25).
The merit function is Fletcher’s diﬀerentiable function:
Pk(x) = f(x) −λ(x)T c(x) + σk∥c(x)∥2
2,
(13.6.4)
where σk > 0 is a penalty parameter, λ(x) is the minimum norm solution of
min
λ∈ℜ
m ∥g(x) −A(x)λ∥2.
(13.6.5)
The actual reduction is
Aredk = Pk(xk) −Pk(xk + dk),
(13.6.6)
and the predicted reduction is deﬁned by
Predk
=
−(gk −Akλk)T dk −1
2dT
k Bk ¯dk
+[λ(xk + dk) −λk]T 
ck + 1
2AT
k dk

+σk(∥ck∥2
2 −∥ck + AT
k dk∥2
2),
(13.6.7)
where ¯dk is the orthogonal projection of dk to the null space of AT
k , namely
¯dk
=
¯Pkdk,
(13.6.8)
¯Pk
=
I −A(xk)A(xk)+.
(13.6.9)
If ∥ck∥2−∥ck+AT
k dk∥2 > 0, from (13.6.7) and by increasing σk (if needed),
we have that
Predk ≥1
2σk(∥ck∥2
2 −∥ck + AT
k dk∥2
2).
(13.6.10)
If ∥ck∥2−∥ck+AT
k dk∥2 = 0, dk is the minimizer of φk(d) in the intersection
of the trust-region and the null space of AT
k and Predk = φk(0) −φk(dk).
Thus, Predk = 0 if and only if gk −Akλk = 0.
The following algorithm is given by Powell and Yuan(1991):
Algorithm 13.6.1
Step 1. Given x1 ∈ℜn, ∆1 > 0, ϵ > 0.
0 < τ3 < τ4 < 1 < τ1, 0 ≤τ0 ≤τ2 < 1, τ2 > 0; k := 1.
Step 2. If ∥ck∥2 + ∥gk −Akλk∥2 ≤ϵ then Stop. Otherwise solve the
problem (13.6.1)-(13.6.3) which gives dk;

13.6. POWELL-YUAN ALGORITHM
587
Step 3. Calculate Predk by formula (13.6.7); If (13.6.10) is satisﬁed
then go to Step 4; Set
σk := 2σk + max
6
0,
−2Predk
∥ck∥2
2 −∥ck + AT
k dk∥2
2
7
.
(13.6.11)
Recalculate Predk by (13.6.7).
Step 4. Calculate the ratio rk = Aredk/Predk. Set the values
xk+1 =

xk + dk,
if rk > 0,
xk,
otherwise;
(13.6.12)
and
∆k+1 =
⎧
⎪
⎨
⎪
⎩
max[4∥dk∥2, ∆k],
if rk > 0.9,
∆k,
0.1 ≤rk ≤0.9,
min[∆k/4, ∥dk∥2/2],
rk < 0.1.
(13.6.13)
Step 5. Generate Bk+1. Set σk+1 := σk. Set k := k + 1 and go to
Step 2.
In order to establish the convergence results of the above algorithm, we
make the following assumptions.
Assumption 13.6.2
1. There exists a bounded convex closed set Ω∈ℜn such that {xk}, {xk +
dk} are all in Ωfor all k.
2. A(x) has full column rank for all x ∈Ω.
3. The matrices {Bk|k = 1, 2, · · ·} are uniformly bounded.
The following two lemmas provide a lower bound on the predicted reduc-
tion Predk.
Lemma 13.6.3 The inequality
∥ck∥2 −∥ck + AT
k dk∥2 ≥min

∥ck∥2, b2∆k
∥A+
k ∥2

(13.6.14)
holds for all k, where b2 is introduced in (13.3.25).

588
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Proof.
If b2∆k ≥∥(AT
k )+ck∥2, we have ξk = 0. Thus,
∥ck∥2 −∥ck + AT
k dk∥2 = ∥ck∥2,
(13.6.15)
which implies (13.6.14).
In the case when b2∆k < ∥(AT
k )+ck∥2, it follows from (13.3.25) and con-
straint condition (13.6.2) that
∥ck∥2
−
∥ck + AT
k dk∥≥∥ck∥2 −ξk
≥
∥ck∥2 −
ck −AT
k

b2∆k
∥(AT
k )+ck∥2

(AT
k )+ck

2
=
∥ck∥2
b2∆k
∥(AT
k )+ck∥2
≥b2∆k
∥A+
k ∥2
.
(13.6.16)
Thus the lemma is true.
2
Lemma 13.6.4 There exists a positive constant δ1 such that the inequality
Predk
−
1
2σk(∥ck∥2
2 −∥ck + AT
k dk∥2
2) + δ1∥dk∥2∥ck∥2
≥
1
4∥¯Pk¯gk∥2 min
 ¯∆k, ∥¯Pk¯gk∥2
2∥Bk∥2

+1
2σk∥ck∥2 min

∥ck∥2, b2∆k
∥A+
k ∥2

(13.6.17)
holds for all k, where we use the notation
¯gk
=
gk + Bk ˆdk,
(13.6.18)
ˆdk
=
dk −¯Pkdk = dk −¯dk,
(13.6.19)
¯∆k
=

∆2
k −∥ˆdk∥2
2.
(13.6.20)
Proof.
The deﬁnition of ˆdk and ∥ck + AT
k dk∥2 ≤∥ck∥2 imply the bound
∥ˆdk∥2
=
∥AkA+
k dk∥2 = ∥(A+
k )T [(ck + AT
k dk) −ck]∥2
≤
2∥A+
k ∥2∥ck∥2.
(13.6.21)
From its deﬁnition, ¯dk is a solution of the subproblem
min
d∈ℜ
n
¯gT
k d + 1
2dT Bkd,
(13.6.22)
s. t.
AT
k d = 0,
(13.6.23)
∥ˆdk + d∥2 ≤∆k.
(13.6.24)

13.6. POWELL-YUAN ALGORITHM
589
It is easy to see that ¯dk also solves the calculation
min
d∈ℜ
n
( ¯Pk¯gk)T d + 1
2( ¯Pkd)T Bk( ¯Pkd),
(13.6.25)
s. t.
∥d∥2 ≤¯∆k.
(13.6.26)
Similar to the proof of Lemma 6.1.3, we can show that
¯gT
k ¯dk ≤−1
2∥¯Pk¯gk∥2 min
 ¯∆k, ∥¯Pk¯gk∥2
2∥Bk∥2

.
(13.6.27)
Hence the deﬁnitions of λk, ¯dk, ¯gk, the fact that expression (13.6.25) increases
monotonically between d = ¯dk and d = 0, and the inequalities (13.6.21) and
(13.6.27) imply the bound
(gk −Akλk)T dk
+
1
2dT
k Bkdk =

gk + 1
2Bkdk
T ¯dk
=
1
2[gT
k ¯dk + ¯dT
k Bk ¯dk + ¯gT
k ¯dk] ≤1
2gT
k ¯dk
≤
1
2¯gT
k ¯dk + 1
2∥Bk ˆdk∥2∥¯dk∥2
≤
−1
4∥¯Pk¯gk∥2 min
 ¯∆k, ∥¯Pk¯gk∥2
2∥Bk∥2

+∥A+
k ∥2∥Bk∥2∥dk∥2∥ck∥2.
(13.6.28)
Moreover, due to the deﬁnition of λ(x) and Assumption 13.6.2, there exists
a positive constant δ2 > 0 such that the condition
∥λ(xk) −λ(xk + dk)∥2 ≤δ2∥dk∥2
(13.6.29)
holds for all k. The convexity of ∥ck + AT
k d∥2 shows that
ck + 1
2AT
k dk

2 ≤1
2(∥ck∥2 + ∥ck + AT
k dk∥2) ≤∥ck∥2.
(13.6.30)
Therefore, the inequality (13.6.17) now follows from (13.6.7) and (13.6.14)
and (13.6.28)-(13.6.30) if we let δ1 = δ2 + supk≥1{∥Bk∥2∥A+
k ∥2}, which is
ﬁnite due to Assumption 13.6.2.
2
A direct corollary of the above lemma is that (13.6.10) is satisﬁed if
∥ck∥2/∆k is suﬃciently small.

590
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Corollary 13.6.5 There exist positive constants δ3 and δ4, such that, on the
iterations that satisfy the condition
∥ck∥2 ≤δ3∆k,
(13.6.31)
we have the inequality
Predk ≥1
2σk[∥ck∥2
2 −∥ck + AT
k dk∥2
2] + δ4∆k.
(13.6.32)
Proof.
From Assumption 13.6.2, there exists ¯
M such that ∆k ≤¯
M. If
δ3 <
ϵ
3 ¯
M , (13.6.31) implies ∥ck∥2 ≤ϵ/3. Unless the algorithm terminates, we
have that
∥gk −Akλk∥2 ≥2ϵ/3.
(13.6.33)
If δ3 < ϵ/(6 ¯
M supk ∥Bk∥2∥A+
k ∥2), (13.6.31) yields
∥ck∥2 ≤
ϵ
6 sup1≤k ∥Bk∥2∥A+
k ∥2
,
(13.6.34)
which implies that
∥gk −Akλk∥2
=
∥¯Pkgk∥2 ≤∥¯Pk¯gk∥2 + ∥¯PkBk ˆdk∥2
≤
∥¯Pk¯gk∥2 + 2∥A+
k ∥2∥Bk∥2∥ck∥2
≤
∥¯Pk¯gk∥2 + ϵ
3.
(13.6.35)
Thus, provided that
δ3 <
ϵ
3 ¯
M min

1,
1
2 sup ∥Bk∥2∥A+
k ∥2

,
(13.6.36)
we have, using (13.6.33) and (13.6.35), that
∥¯Pk¯gk∥2 ≥ϵ
3.
(13.6.37)
Consequently, it follows from Lemma 13.6.4 that
Predk
−
1
2σk[∥ck∥2
2 −∥ck + AT
k dk∥2
2] + δ1∥dk∥2∥ck∥2
≥
ϵ
12 min
 ¯∆k,
ϵ
6∥Bk∥

.
(13.6.38)

13.6. POWELL-YUAN ALGORITHM
591
If δ3 satisﬁes
δ3 ≤0.3/ sup
k
∥A+
k ∥,
(13.6.39)
we have ¯∆k > 0.8∆k from (13.6.31). When
δ3 <
ϵ
24δ1
min
0.8
¯
M ,
ϵ
6 ¯
M2 supk ∥Bk∥2

,
(13.6.40)
it follows from (13.6.31) that
δ1∥ck∥2∥dk∥2 ≤ϵ
24 min

0.8∆k,
ϵ
6∥Bk∥2

.
(13.6.41)
Now, inequalities (13.6.38) and (13.6.41) give that
Predk
−
1
2σk[∥ck∥2
2 −∥ck + AT
k dk∥2
2]
≥
ϵ
24 min

0.8∆k,
ϵ
6∥Bk∥

.
(13.6.42)
The corollary follows from the above inequality, and the assumptions that
{∆k}, {∥Bk∥} are bounded.
2
Now, using the above results, we can easily prove the boundedness of the
sequence {σk}, which is important in establishing the convergence properties
of the algorithm.
Lemma 13.6.6 The sequence {σk|k = 1, 2, · · ·} remains bounded. In other
words, because any increase in σk is by at least a factor of 2, there exists ¯k,
such that
σk = σ¯k,
∀k ≥¯k.
(13.6.43)
Proof.
Corollary 13.6.5 shows that (13.6.10) fails only if ∥ck∥2 > δ3∆k.
In this case, using ∆k ≥∥dk∥2 too, Lemma 13.6.4 provides the bound
Predk
−
1
2σk(∥ck∥2
2 −∥ck + AT
k dk∥2
2)
≥
∥dk∥2∥ck∥2
1
2σk min(δ3, b2/δ5) −δ1

,
(13.6.44)
where δ5 is an upper bound on {∥A+
k ∥2, k = 1, 2, · · ·}.
Hence condition
(13.6.10) holds if σk > 2δ1 max[1/δ3, δ5/b2].
Therefore the number of in-
crease in σk is ﬁnite.
2
We now assume without loss of generality that σk ≡σ for all k. The next
lemma shows that both the trust-region bound and the constraints converges
to zero, if the algorithm does not terminate after ﬁnitely many iterations.

592
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
Lemma 13.6.7 If the algorithm does not terminate, we have the limits
lim
k→∞∆k
=
0,
(13.6.45)
lim
k→∞∥ck∥2
=
0.
(13.6.46)
Proof.
To prove (13.6.45), we assume that the number
η = lim sup
k→∞
∆k
(13.6.47)
is positive and deduce a contradiction. Deﬁne K to be the set of integers k
satisfying
rk ≥0.1,
∆k ≥η/8.
(13.6.48)
The set K contains inﬁnitely many elements because of (13.6.47). Since the
monotonically decreasing sequence {P(xk)} is convergent, we have that
lim
k∈K
k→∞
Predk = 0.
(13.6.49)
Therefore, (13.6.32) does not hold for suﬃciently large k ∈K. It follows
from Corollary 13.6.5 and (13.6.48) that
∥ck∥2 > δ3η/8
(13.6.50)
holds for all suﬃciently large k ∈K. Thus, Lemma 13.6.3 implies that
Predk
≥
1
2σ[∥ck∥2
2 −∥ck + AT
k dk∥2
2]
≥
1
2σ∥ck∥2 min

∥ck∥2, b2∆k
∥A+
k ∥2

.
(13.6.51)
Using the above inequality, relations (13.6.48) and (13.6.49), we can deduce
that
lim
k∈K
k→∞
∥ck∥2 = 0,
(13.6.52)
which contradicts (13.6.50). Therefore (13.6.45) is true.
As for (13.6.46), we deduce a contradiction from the assumption that
¯η = lim sup
k→∞
∥ck∥2 > 0.
(13.6.53)

13.6. POWELL-YUAN ALGORITHM
593
Deﬁne ¯K = {k|∥ck∥2 > ¯η/2}. It follows from (13.6.51) and (13.6.45) that
there exists a constant ¯δ > 0 such that
Predk ≥¯δ∆k,
∀k ∈¯K.
(13.6.54)
The above inequality and (13.6.45) imply that
lim
k∈K
k→∞
rk = 1,
(13.6.55)
which, together with (13.6.54), shows that

k∈¯
K
∆k < +∞.
(13.6.56)
From the deﬁnition of ¯K, inequality (13.6.56) and the continuity of c(x), we
can show that
lim
k→∞∥ck∥2 = ¯η.
(13.6.57)
Thus, k ∈
¯K for all suﬃciently large k.
This observation and relation
(13.6.55) imply that ∆k+1 ≥∆k for all suﬃciently large k.
This contra-
dicts (13.6.45). Therefore, (13.6.46) is true.
2
Having established the above results, we can easily show the global con-
vergence of the algorithm.
Theorem 13.6.8 Under Assumption 13.6.2, Algorithm 13.6.1 will termi-
nate after ﬁnitely many iterations. In other words, if we remove the conver-
gence test from Step 2, then dk = 0 for some k or the limit
lim inf
k→∞[∥ck∥2 + ∥¯Pkgk∥2] = 0
(13.6.58)
is obtained, which ensures that {xk, k = 1, 2, ...} is not bounded away from
stationary points of the problem (13.4.1)-(13.4.2).
Proof.
First we assume that ϵ > 0. If the algorithm does not terminate,
then the inequality
∥ck∥2 + ∥¯Pkgk∥2 ≥ϵ
(13.6.59)
holds for all k. It follows from (13.6.46) that
∥¯Pkgk∥2 ≥ϵ/2
(13.6.60)

594
CHAPTER 13. TR METHODS FOR CONSTRAINED PROBLEMS
holds for all suﬃciently large k.
Using (13.6.60), (13.6.45), (13.6.46) and
(13.6.17) we can show that there exists a positive constant δ such that
Predk ≥δ∆k
(13.6.61)
is true for all suﬃciently large k. The above inequality implies that
lim
k→∞rk = 1,
(13.6.62)
which leads to the inequality ∆k+1 ≥∆k (for all suﬃciently large k). This
contradicts (13.6.45). The contradiction indicates that for any positive ϵ > 0
Algorithm 13.6.1 will terminate after ﬁnitely many iterations.
If ϵ = 0, then the algorithm terminates if and only if dk = 0. If dk =
0, then xk is a KKT point of the optimization problem (13.4.1)–(13.4.2).
Assume that the algorithm dost not terminate, then dk ̸= 0 for all k. Let
η = inf
k [∥ck∥2 + ∥¯Pkgk∥2].
(13.6.63)
If η > 0, we see that the algorithm does not terminate for ϵ = η/2, which
contradicts the proof given above. This shows that we must have η = 0,
which implies (13.6.58).
2
Under second-order suﬃcient conditions and other mild conditions, lo-
cally superlinear convergence of the algorithm can be proved (see, Powell
and Yuan [278]).
Exercises
1. Prove that the trust-region subproblem
min
d∈ℜn gT
k d + 1
2dT Bkd + σk∥(ck + AT
k d)(−)∥∞
subject to
∥d∥∞≤∆k
can be reformulated as a quadratic programming problem.
2. Extend the null space trust-region method for equality constrained
optimization to handle also inequality constraints.

13.6. POWELL-YUAN ALGORITHM
595
3. Consider the CDT subproblem when
g =

2
0

,
B =

−2
0
0
2

,
A = I,
c =

−2
0

,
∆= 2 and ξ = 1. Verify that the Hessian of the Lagrange function can have
one negative eigenvalue even when only one of the constraints are active at
the solution.
4. Construct an example to show that the Hessian of the Lagrange of the
CDT subproblem at the solution may have two negative eigenvalues.
5. Let C, D ∈ℜn×n be two symmetric matrices and let A and B be two
closed sets in ℜn such that A ∪B = ℜn. If we have that
xT Cx ≥0, ∀x ∈A,
xT Dx ≥0, ∀x ∈B,
prove that there exists a t ∈[0, 1] such that the matrix tC + (1 −t)D is
positive semi-deﬁnite.
6. Discuss the local convergence properties of Powell-Yuan’s trust-region
algorithm.

Chapter 14
Nonsmooth Optimization
14.1
Generalized Gradients
In this book, nonsmooth functions are those functions which need not be
diﬀerentiable. Therefore they are also called nondiﬀerentiable functions.
The nonlinear programming problem (8.1.1)–(8.1.3) is said to be a nons-
mooth optimization problem, provided that either the objective function f(x)
or at least one of the constraint functions ci(x), (i = 1, · · · , m) is a nonsmooth
function.
To conclude the book, we would like to give an initial and readable in-
troduction to nonsmooth optimization. To study the optimality condition
of nonsmooth optimization and construct some numerical methods for solv-
ing nonsmooth optimization problems, we ﬁrst introduce the fundamental
conceptions and properties of nonsmooth functions.
Let X be a Banach space with a norm ∥· ∥deﬁned on X. Let Y be a
subset of X. A function f : Y →R is Lipschitz on Y if f(x) satisﬁes
|f(x) −f(y)| ≤K∥x −y∥, ∀x, y ∈Y ⊆X,
(14.1.1)
where K is called the Lipschitz constant.
The inequality (14.1.1) is also
referred to as a Lipschitz condition.
We deﬁne a generalized sphere
B(x, ϵ) = {y | ∥x −y∥≤ϵ}.
(14.1.2)
We say that f is Lipschitz near x if, for some ϵ > 0, f satisﬁes a Lipschitz
condition on B(x, ϵ).

598
CHAPTER 14. NONSMOOTH OPTIMIZATION
It is easy to see that a function having a Lipschitz property near a point
need not be diﬀerentiable there, nor need admit a directional derivative in
the classical sense.
The directional derivative of f at x in the direction d is
f′(x; d) = lim
t↓0
f(x + td) −f(x)
t
.
(14.1.3)
The upper Dini directional derivative of f at x in the direction d is
f(D)(x; d) = lim sup
t↓0
f(x + td) −f(x)
t
.
(14.1.4)
Let f be Lipschitz near a given point x, and let d be any other vector in X.
The generalized directional derivative of f at x in the direction d is deﬁned
as follows:
fo(x; d) = lim sup
y→x
t↓0
f(y + td) −f(y)
t
,
(14.1.5)
where of course y is a vector in X and t is a positive scalar, and t ↓0 denotes
that t tends to zero monotonically and downward.
Since the generalized
directional derivative is due to Clarke [60], it is also referred to as a Clarke
directional derivative.
For a locally Lipschitz function, the directional derivative may not exist
but the Dini and the Clarke directional derivatives always exist. Obviously,
we always have the relation
f(D)(x; d) ≤fo(x; d)
(14.1.6)
for all x and d. If the directional derivative exists, then it is equal to the
upper Dini directional derivative. If f′(x; d) exists at x for all d, then f is
said to be directionally diﬀerentiable at x. If f is directionally diﬀerentiable
at x and
f ′(x; d) = fo(x; d),
(14.1.7)
then f is said to be regular at x. The function f is said to be a regular
function if it is regular everywhere.
Lemma 14.1.1 Let f(x) be Lipschitz near x. Then
1. The function d →f o(x; d) is positive homogeneous and subadditive on
X, and satisﬁes
|fo(x; d)| ≤K∥d∥.
(14.1.8)

14.1. GENERALIZED GRADIENTS
599
2. fo(x; d) is Lipschitz on X as a function of d.
3. fo(x; d) is upper semicontinuous as a function of (x; d).
4. fo(x; −d) = (−f)o(x; d).
Proof.
1) In view of (14.1.1), (14.1.5) and the fact that f(x) is Lipschitz
near x, we immediately have (14.1.8). The fact that
f o(x; λd) = λf o(x; d)
for any λ > 0 is immediate from the deﬁnition (14.1.5). Now we turn to the
subadditivity.
From (14.1.5), we have
fo(x; d1 + d2)
=
lim sup
y→x
t↓0
f(y + t(d1 + d2)) −f(y)
t
≤
lim sup
y→x
t↓0
f(y + td1 + td2) −f(y + td2)
t
+ lim sup
y→x
t↓0
f(y + td2) −f(y)
t
≤
fo(x; d1) + fo(x; d2).
(14.1.9)
2) Let any d1, d2 ∈X be given. We have from the Lipschitz condition
that
f(y + td1) −f(y) ≤f(y + td2) −f(y) + Kt∥d1 −d2∥
(14.1.10)
holds for y near x, t > 0 suﬃciently small. Dividing by t and taking upper
limits as y →x, t ↓0, gives
fo(x; d1) ≤fo(x; d2) + K∥d1 −d2∥.
(14.1.11)
Similarly, we obtain
fo(x; d2) ≤fo(x; d1) + K∥d1 −d2∥.
(14.1.12)
The above two inequalities give
|fo(x; d1) −fo(x; d2)| ≤K∥d1 −d2∥.
(14.1.13)

600
CHAPTER 14. NONSMOOTH OPTIMIZATION
Then we complete 2).
3) Now let {xi} and {di} be arbitrary sequences with xk →x and dk →d
respectively. For each i, by deﬁnition of upper limit, there exist yk ∈X and
tk > 0 such that
∥yk −xk∥+ tk < 1
k,
(14.1.14)
fo(xk; dk) −1
k
≤
f(yk + tdk) −f(yk)
tk
≤
f(yk + tkdk) −f(yk + tkd)
tk
+ f(yk + tkd) −f(yk)
tk
. (14.1.15)
Upon taking upper limits (as k →∞), we derive
lim sup
k→∞
fo(xk; dk) ≤fo(x; d),
(14.1.16)
which establishes the upper semicontinuity.
4) Finally, we calculate
fo(x; −d)
=
lim sup
y→x
t↓0
f(y −td) −f(y)
t
=
lim sup
u→x
t↓0
(−f)(u + td) −(−f)(u)
t
=
(−f)o(x; d),
(14.1.17)
where u = y −td. Hence, we complete the proof.
2
The Hahn-Banach Theorem (for example, see Cryer [72], Theorem 7.4)
asserts that any positive homogeneous and subadditive functional on X ma-
jorizes some linear functional on X. Under the condition of Lemma 14.1.1,
therefore, there is at least one linear functional ξ : X →R such that, for all
d ∈X, one has
fo(x; d) ≥ξ(d).
It follows also that ξ is bounded, and hence belongs to the dual space X∗
of continuous linear functionals on X, for which we adopt the convention of
using ⟨ξ, d⟩or ⟨d, ξ⟩for ξ(d).
We then give the following deﬁnition:

14.1. GENERALIZED GRADIENTS
601
Deﬁnition 14.1.2 Let f(x) be Lipschitz near x. Then we say that the gen-
eralized diﬀerential (or Clarke diﬀerential) of f at x is the set
∂f(x) = {ξ ∈X∗| fo(x; d) ≥⟨ξ, d⟩, ∀d ∈X}.
(14.1.18)
The ξ is said to be the generalized gradient.
The norm ∥ξ∥∗in conjugate space X∗is deﬁned as
∥ξ∥∗= sup{⟨ξ, d⟩: d ∈X, ∥d∥≤1}.
(14.1.19)
The following summarizes some basic properties of the generalized gradi-
ent.
Lemma 14.1.3 Let f(x) be Lipschitz near x. Then
1) ∂f(x) is a nonempty, convex, weak*-compact subset of X∗and ∥ξ∥∗≤
K for every ξ ∈∂f(x).
2) For every d ∈X, one has
fo(x; d) = max
ξ∈∂f(x){⟨ξ, d⟩}.
(14.1.20)
Proof.
Assertion 1) is immediate from the preceding remarks and Lemma
14.1.1. (The weak*-compactness follows from Alaoglu’s Theorem.)
Assertion 2) is simply a restatement of the fact that ∂f(x) is by deﬁnition
the weak*-closed convex set whose support function is fo(x; ·). To see this
independently, suppose that for some d, fo(x; d) exceeded the given maximum
(it cannot be less, by deﬁnition of ∂f(x)). According to a common version of
the Hahn-Banach Theorem there is a linear functional ξ majorized by fo(x, ·)
and agreeing with it at d.
It follows that ξ ∈∂f(x), whence fo(x; d) >
⟨ξ; d⟩= fo(x; d). This contradiction establishes the assertion 2).
2
Note that if f(x) is convex, the conceptions of generalized directional
derivative and generalized gradient coincide with that of directional derivative
and subgradient deﬁned for convex functions due to Rockafellar [288].
As an example, we calculate the generalized diﬀerential of the absolute-
value function in the case of X = R.
Consider the problem
f(x) = |x|.

602
CHAPTER 14. NONSMOOTH OPTIMIZATION
Obviously, f is Lipschitz by the triangle inequality. If x > 0, we calculate
fo(x; d) = lim sup
y→x
t↓0
y + td −y
t
= d,
so that
∂f(x) = {ξ | d ≥ξd, ∀d ∈R}
reduces to the singleton {1}.
Similarly, we have
∂f(x) = {−1} if x < 0.
The remaining case is x = 0. We ﬁnd
fo(0; d) =

d,
if d ≥0,
−d,
if d < 0,
that is
fo(0, d) = |d|.
Thus ∂f(0) consists of those ξ satisfying |d| ≥ξd for all d; that is ∂f(0) =
{−1, 1}. Therefore, we conclude
∂f(x) =
⎧
⎪
⎨
⎪
⎩
{1},
x > 0,
{−1},
x < 0,
{−1, 1},
x = 0.
We introduce an important conception as follows.
The support function of a nonempty subset Ωof X is a function σΩ(ξ) :
X∗→R ∪{+∞} deﬁned by
σΩ(ξ) := sup
x∈Ω
{⟨ξ, x⟩}.
(14.1.21)
It is easy to see that fo(x; ·) is the support function of ∂f(x).
By (14.1.21) and Deﬁnition 14.1.2, the following lemma is obvious.
Lemma 14.1.4 Let f(x) be Lipschitz near x. Then
ξ ∈∂f(x) if and only if fo(x; d) ≥⟨ξ; d⟩∀d ∈X.
(14.1.22)
Furthermore, ∂f(x) has the following properties:

14.1. GENERALIZED GRADIENTS
603
a)
∂f(x) = ∩δ>0 ∪y∈x+B(0,δ) ∂f(y),
(14.1.23)
where
B(0, δ) = {x | ∥x∥≤δ, x ∈X}.
If X is ﬁnite-dimensional, then the ∂f is upper semi-continuous.
b) If fi (i = 1, · · · , m) are ﬁnitely many Lipschitz functions near x, then

m
i=1 fi is also Lipschitz near x and
∂(
m

i=1
fi)(x) ⊂
m

i=1
∂fi(x).
(14.1.24)
c) If f(x) = g(h(x)), where h(x) = (h1(x), · · · , hn(x))T , each hi(x) is
Lipschitz near x, and g(x) is Lipschitz near h(x), then f(x) is Lipschitz near
x and
∂f(x) ⊂co
 n

i=1
αiξi : ξi ∈∂hi(x), α ∈∂g(h)|h=h(x)
3
,
(14.1.25)
where co denotes a weak∗-compact convex hull (see Theorem 2.3.9 in Clarke
[60]).
Below, we turn to the optimal condition for minimization of a Lipschitz
function. By Lemma 14.1.4, we can immediately deduce the ﬁrst-order nec-
essary condition.
Theorem 14.1.5 If f(x) attains a local minimum or maximum at x∗and
f(x) is Lipschitz near x∗, then
0 ∈∂f(x∗).
(14.1.26)
Proof.
If x∗is a local minimizer of f(x), then it follows from the deﬁnition
(14.1.5) that for any d ∈X we have
fo(x∗; d) ≥0.
(14.1.27)
Thus, by Lemma 14.1.4, we have 0 ∈∂f(x∗).
If x∗is a local maximizer of f(x), then x∗is a local minimizer of (−f)(x).
It suggests that 0 ∈∂(−f)(x∗). It is not diﬃcult to show that for any scalar

604
CHAPTER 14. NONSMOOTH OPTIMIZATION
s, one has ∂(sf)(x) = s∂f(x). Therefore 0 ∈∂(−f)(x∗) = −∂f(x∗) which
means 0 ∈∂f(x∗). Hence we complete the proof.
2
A point x∗is called a stationary point of f if f is directionally diﬀeren-
tiable at x∗and for all d,
f′(x∗, d) ≥0.
(14.1.28)
A point x∗is called a Dini stationary point of f if for all d,
f(D)(x∗; d) ≥0.
(14.1.29)
A point x∗is called a Clarke stationary point of f if for all d,
fo(x∗; d) ≥0,
(14.1.30)
i.e.,
0 ∈∂f(x∗).
(14.1.31)
A local minimizer x∗of a local Lipschitzian function f is always a Dini
stationary point of f. If f is directionally diﬀerentiable at x∗, then x∗is also
a stationary point. A Dini stationary point is always a Clarke stationary
point but not vice versa.
Now we state the suﬃcient condition which is based on a lemma below.
Lemma 14.1.6 Let f(x) be convex and Lipschitz near x∗, then the general-
ized diﬀerential ∂f(x) coincides with the subdiﬀerential at x, and the gener-
alized directional derivative fo(x; d) coincides with the directional derivative
f′(x; d) for each d.
Proof.
It is known from convex analysis that f′(x; d) exists for each d
and f′(x; d) is the support function of the subdiﬀerential at x. It suﬃces
therefore to prove that for any d, fo(x; d) = f′(x; d). Note that
fo(x; d) = lim
ϵ↓0
sup
∥x′−x∥<ϵδ
sup
0<t<ϵ
f(x′ + td) −f(x′)
t
,
(14.1.32)
where δ is any ﬁxed positive number. It follows from the deﬁnition of convex
function that the function
t →f(x′ + td) −f(x′)
t

14.1. GENERALIZED GRADIENTS
605
is non-decreasing, whence
fo(x; d) = lim
ϵ↓0
sup
∥x′−x∥<ϵδ
f(x′ + ϵd) −f(x′)
ϵ
.
Now by the Lipschitz condition, for any x′ in x + B(0, ϵδ), one has

f(x′ + ϵd) −f(x′)
ϵ
−f(x + ϵd) −f(x)
ϵ
 ≤2δK,
so that
fo(x; d) ≤lim
ϵ↓0
f(x + ϵd) −f(x)
ϵ
+ 2δK = f′(x; d) + 2δK.
Since δ is arbitrary, we deduce fo(x; d) ≤f′(x; d). Therefore the equality
follows. The proof is complete.
2
We now can state the suﬃcient condition.
Theorem 14.1.7 Let f(x) be convex and Lipschitz near x∗, and
0 ∈∂f(x∗),
(14.1.33)
then x∗is a local minimizer of f(x).
Proof.
For a convex and Lipschitzian function, from Lemma 14.1.6, the
generalized diﬀerential and the subdiﬀerential
{ξ ∈X∗| f(z) −f(x) ≥⟨ξ, z −x⟩, ∀z ∈X}
(14.1.34)
are equivalent. Then, by (14.1.33) and (14.1.34), we have that x∗is a local
minimizer of f(x).
2
Hence, for a convex and Lipschitzian function, (14.1.33) is a suﬃcient
and necessary condition for x∗to be a local minimizer of f(x). This is also
equivalent to
fo(x∗; d) ≥0, ∀d ∈X.
(14.1.35)
For a convex and Lipschitzian function, the generalized directional derivative
fo(x; d) coincides with the directional derivative f′(x; d):
f′(x; d) = lim
t↓0
f(x + td) −f(x)
t
.
(14.1.36)
(We would like to mention that, from convex analysis, convex functions are
Lipschitz except in the pathological case).
Furthermore, we can state a suﬃcient condition for a strict (strong) min-
imizer.

606
CHAPTER 14. NONSMOOTH OPTIMIZATION
Theorem 14.1.8 Let f(x) be convex and Lipschitz near x∗. If
f′(x∗; d) > 0, ∀d ̸= 0, d ∈X,
(14.1.37)
then x∗is a strict (strong) minimizer of f(x), i.e., there exists δ > 0 such
that
f(x) −f(x∗) ≥δ∥x −x∗∥
(14.1.38)
holds for all x suﬃciently close to x∗.
Proof.
Deﬁne a set
S = {d | d ∈X, ∥d∥= 1}.
Obviously, S is compact and closed. By (14.1.37), it follows that f′(x∗, d) is
positive on S. Then, from the continuity of f′(x∗, d) (in fact, f′(x∗; d) is a
positive homogeneous and convex function of d), there exists δ > 0 such that
f′(x∗; d) ≥2δ, ∀d ∈S.
(14.1.39)
Then for any d ∈S, there exists t(d) > 0 such that
f(x∗+ td) −f(x∗) ≥td, ∀t ∈[0, t(d)].
(14.1.40)
By convexity and continuity of f(x), we can show that there is an ϵ > 0 such
that
t(d) ≥ϵ, ∀d ∈S.
(14.1.41)
Hence, for all x with ∥x −x∗∥≤ϵ, we have
f(x) −f(x∗) ≥δ∥x −x∗∥
(14.1.42)
which indicates (14.1.38).
2
However, for a non-convex function, the above suﬃciency result is not
true. In fact, let us consider an example below: for f : R1 →R1,
f(x) =

(−1)k+1 
1
2k+1 −3x

,
x ∈

1
2k−1 , 1
2k

,
0,
x = 0,
(14.1.43)
f(x) = f(−x), ∀x ∈[−1, 0).
(14.1.44)
Clearly, f(x) is Lipschitz on [−1, 1], and
f o(x∗; ±1) = 3 > 0
(14.1.45)
at x∗= 0, which means there are two generalized directional derivatives equal
to 3. But x∗= 0 is not the extreme point.

14.2. NONSMOOTH OPTIMIZATION PROBLEM
607
14.2
Nonsmooth Optimization Problem
Consider unconstrained optimization problem
min
x∈X f(x),
(14.2.1)
where f(x) is a nondiﬀerentiable function deﬁned in Banach space and sat-
isﬁes a Lipschitz condition. From the discussion in §14.1, if x∗is a solution
of (14.2.1), then
0 ∈∂f(x∗),
(14.2.2)
i.e., x∗is a stationary point of (14.2.1).
As to solution for nonsmooth optimization problem (14.2.1), there are two
main diﬃculties if one is using a method suitable for diﬀerentiable problems.
First, it is not easy to give a termination criteria. It is well-known that when
x approaches the minimizer of a continuously diﬀerentiable function f(x),
the ∥∇f(x)∥is very small. Hence the common termination criteria
∥∇f(x)∥≤ϵ
(14.2.3)
is used. However, for a nonsmooth function, there are no similar results. For
example, consider the simple problem that f : R1 →R1 and f(x) = |x|.
Then, for any x that is not a solution, f(x) is diﬀerentiable and
|∂f(x)| = |∇f(x)| = 1.
(14.2.4)
Hence, in this case, we cannot use (14.2.3) as a termination criteria.
Second, as indicated by Wolfe [354], when f(x) is nondiﬀerentiable, if
one uses the steepest descent method with line search to solve (14.2.1), it is
possible to generate a sequence {xk} converging to a non-stationary point.
For example, let f : R2 →R1, x = (u, v)T and
f(x) = max
1
2u2 + (v −1)2, 1
2u2 + (v + 1)2

.
(14.2.5)
Suppose that xk has the form
xk =

2(1 + |ϵk|)
ϵk

,
(14.2.6)
where ϵk ̸= 0. Then we can calculate
∇f(xk) =

2(1 + |ϵk|)
2(1 + |ϵk|)tk

= 2(1 + |ϵk|)

1
tk

,
(14.2.7)

608
CHAPTER 14. NONSMOOTH OPTIMIZATION
where tk = sign(ϵk). If we employ the negative gradient direction −∇f(xk),
then we have
xk+1
=
xk + αk(−∇f(xk)) =

2(1 + |ϵk|/3)
−ϵk/3

=

2(1 + |ϵk+1|)
ϵk+1

,
(14.2.8)
where ϵk+1 = −ϵk/3 ̸= 0. Then we can prove ϵk →0. So, for a given initial
point as (2 + 2|δ|, δ)T , where δ ̸= 0, the sequence generated by the steepest
descent method with exact line search converges to (2, 0)T . It is obvious that
(2, 0)T is not the stationary point.
A nonsmooth constrained optimization problem has the form
min
x∈Y f(x),
(14.2.9)
where Y ⊆X is a set, or a feasible region. Deﬁne a distance function
dist(x, Y ) = min
y∈Y ∥y −x∥.
(14.2.10)
By the theory of penalty function, under suitable conditions, (14.2.9) is equiv-
alent to
min
x∈X f(x) + σdist(x, Y ),
(14.2.11)
where f(x) + σdist(x, Y ) is a non-diﬀerentiable function. Hence, the non-
smooth constrained optimization problem is transformed to an equivalent
nonsmooth unconstrained problem. This interprets why one always is inter-
ested in studying nonsmooth unconstrained optimization problems.
There are many examples of nonsmooth optimization problems, for ex-
ample, the minimax problem
min
x∈X max
1≤i≤m fi(x).
(14.2.12)
In addition, in order to solve nonlinear equations
fi(x) = 0, i = 1, · · · , m,
(14.2.13)
we often ﬁnd the solution of the minimization problem
min
x∈X f(x) = min
x∈X ∥¯f(x)∥
(14.2.14)

14.3. THE SUBGRADIENT METHOD
609
under some norm ∥· ∥, where f(x) = ∥¯f(x)∥, ¯f(x) = (f1(x), · · · , fm(x)) is a
vector function from X to Rn. Clearly, the problem (14.2.14) is a nonsmooth
optimization problem. In particular, if ∥· ∥= ∥· ∥1, it is a L1 minimization
problem; if ∥· ∥= ∥· ∥∞, it is Chebyshev approximation problem.
Note that the exact penalty function (10.6.2) is also a nonsmooth func-
tion.
Therefore, the minimization to the exact penalty function is also a
nonsmooth optimization problem.
14.3
The Subgradient Method
The subgradient method is a direct generalization of the steepest descent
method, which generates a sequence {xk} by use of −gk as a direction, where
gk ∈∂f(xk).
Let f(x) be a convex function on Rn and the minimization problem be
minx∈Rn f(x). We have seen that the convex function is diﬀerentiable almost
everywhere, and
∂f(x) = conv Ω(x),
(14.3.1)
where convΩdenotes the convex hull of Ω,
Ω(x) = {g | g = lim ∇f(xi), xi →x, ∇f(xi) exists}.
(14.3.2)
The subgradient method is described as follows.
Algorithm 14.3.1 (The subgratient method)
Step 1. Given an initial point x1 ∈Rn, k := 1.
Step 2. Compute f(xk), gk ∈∂f(xk).
Step 3. Choose stepsize αk > 0 and set
xk+1 = xk −αkgk/∥gk∥2,
(14.3.3)
k := k + 1, go to Step 2.
2
As shown in the above section, in the subgradient method, the exact line
search may cause convergence to a non-stationary point.
In smooth optimization, inexact line search is to ﬁnd the stepsize αk such
that
f(xk + αkdk) ≤f(xk) + αkc1dT
k ∇f(xk),
(14.3.4)

610
CHAPTER 14. NONSMOOTH OPTIMIZATION
where c1 ∈(0, 1) is a constant. For the steepest descent method, the above
rule becomes
f(xk −αk∇f(xk)) ≤f(xk) −αkc1∥∇f(xk)∥2.
(14.3.5)
However, when f(x) is nonsmooth, then for any c1 ∈(0, 1) and gk ∈∂f(xk),
the inequality
f(xk −αgk) ≤f(xk) −αc1∥gk∥2
(14.3.6)
may not hold for any α > 0.
Therefore, inexact line search is also not
practicable in nonsmooth optimization.
Note that a constant stepsize is unsuitable because the function may be
nondiﬀerentiable at the solution and then {gk} does not necessarily tend to
zero, even if {xk} converges to the optimal point.
Therefore, the rules for determining αk for the subgradient method are
entirely diﬀerent from that for the steepest descent method.
Although the exact and inexact line search for smooth optimization can-
not be simply generalized to the nonsmooth case, the negative subgradient
direction is a “good” direction such that the new iterate is closer to the
solution.
Lemma 14.3.2 Let f(x) be a convex function and the set
S∗= {x | f(x) = f∗= min
x∈Rn f(x)}
(14.3.7)
be nonempty. If xk /∈S∗, then for any x∗∈S∗and gk ∈∂f(xk), there must
exist Tk > 0 such that
xk −α
gk
∥gk∥2
−x∗

2
< ∥xk −x∗∥2
(14.3.8)
holds for all α ∈(0, Tk).
Proof.
For any xk,
xk −α
gk
∥gk∥2
−x∗

2
2
=
∥xk −x∗∥2
2
+2α
 
gk
∥gk∥2
!T
(x∗−xk) + α2. (14.3.9)

14.3. THE SUBGRADIENT METHOD
611
Since gk ∈∂f(xk) and xk /∈S∗, then we have
gT
k (x∗−xk) ≤f(x∗) −f(xk) < 0.
(14.3.10)
Deﬁne
Tk = −2gT
k (x∗−xk)/∥gk∥2 > 0,
(14.3.11)
then (14.3.9) becomes
∥xk −α
gk
∥gk∥2
−x∗∥2
2 = ∥xk −x∗∥2
2 + α(α −Tk).
(14.3.12)
If 0 < α < Tk, then α(α −Tk) < 0 and further (14.3.8) holds.
2
By use of the above property of subgradient direction, we can take a
suﬃciently small step, such that the sequence {xk} is closer and closer to the
solution. From the above lemma we can deduce easily the following result
due to Shor [309].
Theorem 14.3.3 Let f(x) be convex and the set S∗be nonempty. For any
δ > 0 there exists r > 0 such that if the subgradient Algorithm 14.3.1 is used
with αk ≡α ∈(0, r) then we have
lim inf
k→∞f(xk) ≤f∗+ δ.
(14.3.13)
Note that the choice of constant stepsize αk ≡α may cause the algorithm
not to converge. Ermoliev [118] and Polyak [254] suggest choosing αk which
would satisfy
αk > 0, lim
k→∞αk = 0,
(14.3.14)
∞

k=1
αk = ∞,
(14.3.15)
and establish the following convergence theorem.
Theorem 14.3.4 Let f(x) be convex, and the set S∗be nonempty and bounded.
If αk satisﬁes (14.3.14) and (14.3.15), then the sequence {xk} generated by
Algorithm 14.3.1 satisﬁes
lim
k→∞dist(xk, S∗) = 0,
(14.3.16)
where dist(x, S) is deﬁned by (14.2.10).

612
CHAPTER 14. NONSMOOTH OPTIMIZATION
Proof.
Since f(x) is convex, there exists continuous function δ(ϵ) such
that
f(x) ≤f∗+ ϵ
(14.3.17)
holds for any
dist(x, S∗) ≤δ(ϵ),
(14.3.18)
where δ(ϵ) > 0 (∀ϵ > 0). For each k, we deﬁne
ϵk = f(xk) −f∗≥0.
(14.3.19)
If ϵk > 0, then
∥xk+1 −x∗∥2
=
∥xk −x∗∥2 + α2
k −2αk(xk −x∗)T gk/∥gk∥2
=
∥xk −x∗∥2 + α2
k −2δ(ϵk)αk
−2αk

xk −x∗−δ(ϵk)
gk
∥gk∥2
T
gk/∥gk∥2
≤
∥xk −x∗∥2 + α2
k −2δ(ϵk)αk.
(14.3.20)
Hence
[dist(xk+1, S∗)]2 −[dist(xk, S∗)]2 ≤−αk[2δ(ϵk) −αk].
(14.3.21)
Deﬁne δ(0) = 0, then the above expression holds for every k. Summing both
sides of (14.3.21) gives
lim inf
k→∞δ(ϵk) = 0.
(14.3.22)
Thus,
lim inf
k→∞dist(xk, S∗) = 0.
(14.3.23)
Suppose to the contrary that the theorem is not true. Then there exist a
positive constant δ′ > 0 and inﬁnitely many k such that
dist(xk+1, S∗) > dist(xk, S∗)
(14.3.24)
and
ϵk > δ′
(14.3.25)
hold. From (14.3.24) and (14.3.21), we deduce that
2δ(ϵk) < αk
(14.3.26)

14.3. THE SUBGRADIENT METHOD
613
holds for suﬃciently large k. Clearly, (14.3.26) contradicts (14.3.25). This
contradiction shows the theorem.
2
The above theorem indicates that Algorithm 14.3.1 converges if αk satis-
ﬁes (14.3.14)–(14.3.15). However, for such chosen αk, the algorithm does not
converge rapidly. In fact, we have
∥xk −x∗∥+ ∥xk+1 −x∗∥≥∥xk −xk+1∥= αk.
(14.3.27)
Then, by (14.3.27) and (14.3.15), we have immediately that
∞

k=1
∥xk −x∗∥= +∞,
(14.3.28)
which shows that the sequence cannot converge R-linearly.
To make the algorithm converge R-linearly, Shor [310] takes
αk = α0qk, 0 < q < 1.
(14.3.29)
But, such an αk does not satisfy (14.3.15). For any given α0 and q, as long
as
dist(x1, S∗) >
α0
1 −q,
(14.3.30)
the sequence generated from the algorithm is not possible to close S∗.
The convergence result of the algorithm with step rule (14.3.29) is stated
as follows.
Theorem 14.3.5 Let f(x) be convex and let there exist positive constant
δ1 > 0 such that for all x,
(x −x∗)T g ≥δ1∥g∥∥x −x∗∥, ∀g ∈∂f(x),
(14.3.31)
then there must exist constants ¯q ∈(0, 1) and ¯α > 0 such that, provided that
q ∈(¯q, 1), α0 > ¯α,
(14.3.32)
then the sequence {xk} generated by Algorithm 14.3.1 satisﬁes
∥xk −x∗∥≤M(δ, α0)qk,
(14.3.33)
where x∗∈S∗, ¯q and ¯α are constants related to ∥x1−x∗∥and δ1, M(δ1, α0) >
0 is a constant irrelative to k and related to δ1 and α0.

614
CHAPTER 14. NONSMOOTH OPTIMIZATION
However, the rule (14.3.29) to determine stepsize is almost infeasible in
practice, because, in general, it is impossible to know the values of ¯α and ¯q.
If the given α0 is too small, then (14.3.32) is not satisﬁed; if α0 is too big,
then the algorithm converges very slowly.
When f∗is known in advance, let us set
αk = λf(xk) −f∗
∥gk∥
, 0 < λ < 2.
(14.3.34)
The convergence theorem of Algorithm 14.3.1 with stepsize rule (14.3.34) is
due to Polyak [255] and stated as follows.
Theorem 14.3.6 Let f(x) be convex and the set S∗be nonempty. If there
exist positive numbers ¯c and ˆc such that
∥g∥≤¯c, ∀g ∈∂f(x),
(14.3.35)
f(x) −f ∗≥ˆc dist(x, S∗)
(14.3.36)
hold for all x satisfying dist(x, S∗) ≤dist(x1, S∗), then the sequence generated
by Algorithm 14.3.1 with stepsize (14.3.34) converges to some x∗∈S∗, and
there exists a positive constant M such that
∥xk −x∗∥≤Mqk,
(14.3.37)
where q = (1 −λ(2 −λ)ˆc2/¯c2)1/2 < 1.
The above discussion has shown that the improvements only in the step-
size rule cannot, in general, signiﬁcantly accelerate convergence. Indeed, slow
convergence is due to the fact that the gradient is almost perpendicular to
the direction towards the minimum. There is a simple way of changing the
angles between the gradient and the direction towards the minimum. This
can be done by performing a space dilation technique, which, in fact, is a
generalization of the variable metric method.
Now we describe the space dilation method as follows:
Algorithm 14.3.7 (The space dilation method)
Step 1. Given initial point x1, α > 0, H1 = αI; k := 1.
Step 2. Evaluate gk ∈∂f(xk); ﬁnd the stepsize αk > 0; set
xk+1 = xk −αkHkgk/(gT
k Hkgk)1/2.
(14.3.38)

14.4. CUTTING PLANE METHOD
615
Step 3. Choose rk > 0 and βk < 1. Set
Hk+1 = rk

Hk −βk
HkgkgT
k Hk
gT
k Hkgk

.
(14.3.39)
k := k + 1, go to Step 2.
2
It is not diﬃcult to see that the matrix sequence {Hk} generated by
(14.3.39) are positive deﬁnite. There are various ways to choose αk, βk and
rk, for example,
αk =
1
n + 1, βk =
2
n + 2, rk =
n2
n2 −1.
(14.3.40)
Below, we state the convergence of the space dilation method without
proof. The interested reader can consult Shor [311].
Theorem 14.3.8 Let f(x) be convex and the set S∗be nonempty. If
dist(x1, S∗) ≤α,
then the sequence {xk} generated by Algorithm 14.3.7 with (14.3.40) satisﬁes
lim inf
k→∞
f(xk) −f∗
qk
< +∞,
(14.3.41)
where
q =
 
1 −
2
n + 1
! 1
2n
n
√
n2 −1.
(14.3.42)
There are other generalizations to the subgradient method, for example,
ellipsoid algorithm, ﬁnite diﬀerence approximation etc. We refer the readers
to Zowe [386] and Shor [313] for details.
14.4
Cutting Plane Method
The cutting plane method for convex programming was presented indepen-
dently by Kelley [186] and Cheney and Goldstein [58] respectively.
The
underlying idea of the cutting plane method is to ﬁnd the minimum of a
function on a convex polyhedral set in each iteration. After each iteration,
a cutting plane is introduced, and a point, which does not satisfy the new

616
CHAPTER 14. NONSMOOTH OPTIMIZATION
hyperplane, is cut oﬀfrom the feasible region, and hence the polyhedral set
is reduced. At last, the iterates converge to a solution. The procedure is
performed by solving a sequence of approximating linear programming.
For convex function f(x), obviously, we have
f(x) = sup
y
sup
g∈∂(y)
[f(y) + gT (x −y)].
(14.4.1)
Therefore, the minimization of f(x) is equivalent to the following problem
min
v
(14.4.2)
s.t.
v ≥f(y) + gT (x −y), ∀y ∈Rn, g ∈∂f(y).
(14.4.3)
The cutting plane method is just, at each iteration, to solve an approxi-
mation problem to (14.4.2)–(14.4.3). Let xi (i = 1, · · · , k) be existing iterates.
At each iteration, we would like to solve the subproblem
min
v
(14.4.4)
s.t.
v ≥f(xi) + gT
i (x −xi), i = 1, · · · , k.
(14.4.5)
Obviously, the linear programming problem (14.4.4)–(14.4.5) is an approxi-
mation to problem (14.4.2)–(14.4.3).
We can state the cutting plane method as follows.
Algorithm 14.4.1 (Cutting plane method)
Step 1. Given an initial point x1 ∈S, where S is a given polyhedral
set. Set k := 1.
Step 2. Compute gk ∈∂f(xk).
Step 3. Solve the linear program (14.4.4)–(14.4.5) for vk+1 and xk+1.
Set k := k + 1, go to Step 2.
2
As indicated above, at each iteration, the algorithm adds a new con-
straint, which means, in geometry, that a part in S which does not contain
the solution, will be cut oﬀby a hyperplane.
The convergence of the cutting plane method can be stated below.
Theorem 14.4.2 Let f(x) be convex and bounded below. Then the sequences
{xk} and {vk} generated by Algorithm 14.4.1 satisfy
1) v2 ≤v3 ≤· · · ≤vk →f∗.
2) Any accumulation point of {xk} is a minimizer of f(x) in S.

14.5. THE BUNDLE METHODS
617
Suppose that f(x) is diﬀerentiable and the algorithm converges to a so-
lution, then for k suﬃciently large, gk = ∇f(xk) is very small, and hence
the constraint condition (14.4.5) will be ill-conditioned. The other disadvan-
tage of the cutting plane method is that when k is suﬃciently large, there
are too many constraints in problem (14.4.4)–(14.4.5) such that the cost is
prohibitively expensive, since cutting plane constraints are always added to
the existing set of constraints but are never deleted. Because of these disad-
vantages, the cutting plane methods have never been attractive, although it
is one of the earliest methods for general convex programming. Therefore,
some modiﬁed versions of the cutting plane methods are needed.
14.5
The Bundle Methods
The bundle method is a class of methods extended from the conjugate sub-
gradient method. This is a descent method with f(xk+1) ≤f(xk) for each
k.
The conjugate subgradient method was presented by Wolfe [354]. At the
k-th iteration, there is an index set Ik ⊂{1, · · · , k}. The search direction is
determined by
dk = −

i∈Ik
λ(k)
i
gi, gi ∈∂f(xk),
(14.5.1)
where λ(k)
i
(i ∈Ik) are obtained by solving the subproblem
min


i∈Ik
λigi

2
2
(14.5.2)
s.t.

i∈Ik
λi = 1, λi ≥0.
(14.5.3)
When f(x) is a convex quadratic function and Ik = {1, 2, · · · , k}, under exact
line search, the direction generated from (14.5.1)–(14.5.3) is the same as that
of the conjugate gradient method. So, this method is said to be a conjugate
subgradient method. We now state the algorithm as follows.
Algorithm 14.5.1 (Conjugate Subgradient Method)
Step 1. Given initial point x1 ∈Rn, compute g1 ∈∂f(x1). Choose
0 < m2 < m1 < 1
2, 0 < m3 < 1; ϵ > 0, η > 0, k := 1; I1 =
{1}.

618
CHAPTER 14. NONSMOOTH OPTIMIZATION
Step 2. Compute the direction dk by (14.5.1)–(14.5.3).
If ∥dk∥≤η stop.
Step 3. Compute yk = xk + αkdk such that
f(yk) ≤f(xk) −m2αk∥dk∥2
2,
(14.5.4)
or
∥yk −xk∥≤m3ϵ.
(14.5.5)
Step 4. If there is gk+1 ∈∂f(yk) such that
gT
k+1dk ≥−m1∥dk∥2
2,
(14.5.6)
then set xk+1 := yk, otherwise set xk+1 := xk.
Step 5. Set Ik+1 := Ik ∪{k + 1} \ Tk, where Tk is an index set
Tk = {i | ∥xi −xk+1∥> ϵ}.
Step 6. k := k + 1, go to Step 2.
2
The following convergence theorem was given by Wolfe [354].
Theorem 14.5.2 Let f(x) be convex and ∥∂f(x)∥be bounded on some open
set containing the set {x | f(x) ≤f(x1)}. Let the sequence {xk} generated
by Algorithm 14.5.1 make f(xk) bounded below. Then the algorithm must
terminate in ﬁnitely many iterations.
Now we consider an extension of the conjugate subgradient method. Sup-
pose that we have performed several steps of the conjugate subgradient
method. A certain number of points have been generated, at which the value
of f has been computed together with some subgradient. We symbolize this
information by the bundle x1, · · · , xk; f1, · · · , fk; g1, · · · , gk; where fi = f(xi)
and gi ∈∂f(xi).
Suppose that at k-th iteration we have weighted factors t(k)
i
≥0 (i =
1, · · · , k). Consider the following subproblem
min

k

i=1
λigi

(14.5.7)

14.5. THE BUNDLE METHODS
619
s.t.
k

i=1
λi = 1, λi ≥0,
(14.5.8)
k

i+1
λit(k)
i
≤¯ϵ,
(14.5.9)
where ¯ϵ > 0 is a given constant. Write λ(k)
i
as a solution of (14.5.7)–(14.5.9).
Then the search direction of the bundle method is
dk = −
k

i=1
λ(k)
i
gi.
(14.5.10)
It is not diﬃcult to see that if t(k)
i
= 0 (i ∈Ik) and t(k)
i
= +∞(i /∈Ik), then
(14.5.7)–(14.5.9) is equivalent completely to (14.5.2)–(14.5.3).
Algorithm 14.5.3 (Bundle Method)
Step 1. Given initial point x1 ∈Rn, compute g1 ∈∂f(x1). Choose
0 < m2 < m1 < 1
2, 0 < m3 < 1, ϵ > 0, η > 0, k := 1 and
t(1)
1
= 1.
Step 2. Solve (14.5.7)–(14.5.9) for λ(k)
i
.
Compute dk by (14.5.10).
If ∥dk∥≤η stop.
Step 3. Compute yk = xk + αkdk such that (14.5.4) holds or
f(yk) −αkgT
k+1dk ≥f(xk) −ϵ,
(14.5.11)
where gk+1 ∈∂f(yk).
If (14.5.4) does not hold, then go to Step 5.
Step 4. xk+1 := yk, t(k+1)
k+1
= 1,
t(k+1)
j
= t(k)
j
+ f(xk+1) −f(xk) −αkgT
j dk, j = 1, · · · , k.
Set k := k + 1, go to Step 2.
Step 5. xk+1 := xk, t(k+1)
j
= t(k)
j
(j = 1, · · · , k)
t(k+1)
k+1
= f(xk) −f(yk) + αkgT
k+1dk.
Set k := k + 1, go to Step 2.
2

620
CHAPTER 14. NONSMOOTH OPTIMIZATION
The convergence of the bundle method was established by Lemarechal
[196] and stated below.
Theorem 14.5.4 Under the assumptions of Theorem 14.5.2, Algorithm 14.5.3
will terminate in ﬁnitely many iterations, i.e., there exists k ∈IN such that
f(xk) ≤f∗+ ϵ, where IN is an index set of positive integers.
14.6
Basic Property of a Composite Nonsmooth
Function
In the following two sections of the chapter, we will discuss a problem with
the special form
min
x∈Rn h(f(x)),
(14.6.1)
and develop the trust-region method for solving this class of problems. In
(14.6.1), f(x) = (f1(x), · · · , fm(x))T is a continuously diﬀerentiable function,
and h(f) : Rm →R1 is convex but nonsmooth. The objective function in
(14.6.1) is a composite function, and the problem (14.6.1) is referred to as
composite nonsmooth optimization (for brief, composite NSO) or composite
nondiﬀerentiable optimization (for brief, composite NDO).
There are many examples of composite NSO in discrete approximation
and data ﬁtting. The following is a simple example.
Consider linear equations
Ax = b,
(14.6.2)
where A ∈Rm×n and b ∈Rm. If m > n, the equations (14.6.2) in general
have no solution. However, we can take x such that the error between Ax and
b is as small as possible. This means that we need to solve the minimization
problem
min
x∈Rn ∥Ax −b∥,
(14.6.3)
where ∥· ∥is a norm on Rm. Obviously, (14.6.3) is a form of (14.6.1). If we
take ∥· ∥2 in (14.6.3), the problem is just the classical least-squares problem.
In addition, note that a general smooth constrained optimization problem
can be transformed to a composite NSO problem via an L1 exact penalty
function. This is the other reason that the composite NSO attracts us.
A prerequisite for describing algorithms for composite NSO is a study of
optimality conditions for composite NSO, which is a direct use of the result

14.6. COMPOSITE NONSMOOTH FUNCTION
621
in §14.1. For simplicity, we introduce the following conception:
χ(x, d) = h(f(x)) −h(f(x) + A(x)T d),
(14.6.4)
ψt(x) = max
∥d∥≤t χ(x, d),
(14.6.5)
DF(x, d) =
sup
λ∈∂h(f(x))
dT A(x)λ,
(14.6.6)
where ∂h(f(x)) denotes the subgradient of h(·) at f(x), A(x) = ∇f(x)T is
an n × m matrix.
Since h(·) is a convex function, by use of the chain rule of the subgradient
of a composite function, it is not diﬃcult to get the following lemma.
Lemma 14.6.1 For composite function ˜f(x) = h(f(x)), the fact
0 ∈∂˜f(x)
(14.6.7)
is equivalent to
DF(x, d) ≥0, ∀d ∈Rn.
(14.6.8)
Then the stationary point of nonsmooth optimization satisﬁes (14.6.8).
From the convexity of h(f), we can also obtain the following results:
Lemma 14.6.2 Let χ(x, d), ψt(x), DF(x, d) be deﬁned in (14.6.4)–(14.6.6).
Then
1) DF(x, d) exists for all x and d ;
2) χ(x, d) is a concave function with respect to d, its directional derivative
at d∗= 0 in the direction d is −DF(x, d).
3) ψt(x) ≥0, ∀t ≥0; ψ1(x) = 0 if and only if x is a stationary point;
4) ψt(x) is a concave function of t;
5) ψt(x) is a continuous function of x for any given t ≥0.
By the above results, we can show that the following statements are
equivalent:
1) The sequence {xk} has an accumulation point x∗which is a stationary
point.
2)
lim inf
k→∞ψ1(xk) = 0.
(14.6.9)
From the necessity theorem in §14.1, it follows that if x∗is a minimizer
of h(f(x)), then it is a stationary point. For a special form of composite
nonsmooth function, it can be written in the following equivalent form.

622
CHAPTER 14. NONSMOOTH OPTIMIZATION
Theorem 14.6.3 If x∗is a local minimizer of composite NSO problem (14.6.1),
then there exists λ∗∈∂h(f(x∗)) such that
A(x∗)λ∗= 0,
(14.6.10)
where A(x) = ∇f(x)T .
Proof.
It is enough to prove that (14.6.10) and
DF(x∗, d) ≥0, ∀d ∈Rn
(14.6.11)
are equivalent.
If (14.6.10) holds, then it follows from the deﬁnition (14.6.6) that (14.6.11)
holds.
Now let us assume that (14.6.11) holds. Suppose to the contrary that
(14.6.10) does not hold. Then the set
¯S = {A(x∗)λ | λ ∈∂h(f(x∗))}
(14.6.12)
does not contain the origin. Since ∂h(f(x∗)) is a closed convex set, then ¯S
is too. Hence by applying the separation theorem of convex sets, we know
there must exist ¯d ∈Rn such that
¯dT A(x∗)λ < 0, ∀λ ∈∂h(f(x∗)).
(14.6.13)
Since ∂h(f(x∗)) is closed, the above expression (14.6.13) contradicts the
fact that DF(x∗, ¯d) ≥0. This contradiction shows the equivalence between
(14.6.11) and (14.6.10).
2
Although the function ˜f(x) = h(f(x)) may not be convex, we can obtain
the following ﬁrst-order suﬃcient conditions.
Theorem 14.6.4 (First order suﬃcient conditions) If
DF(x∗, d) > 0
(14.6.14)
holds for all nonzero vectors d, then x∗is a strictly local minimizer of h(f(x)).
Proof.
By (14.6.14), there exists δ > 0 such that
DF(x∗, d) ≥δ, ∀∥d∥2 = 1.
(14.6.15)

14.7. TRUST REGION METHOD FOR COMPOSITE PROBLEMS
623
Suppose that the theorem is not true, then there exists xk →x∗with
h(f(xk)) ≤h(f(x∗)). Let us suppose that
xk = x∗+ αkdk, ∥dk∥2 = 1, αk > 0, αk →0+.
Then
h(f(xk)) −h(f(x∗))
=
h(f(x∗) + A(x∗)T (xk −x∗)) −h(f(x∗)) + o(αk)
≥
αkDF(x∗, dk) + o(αk)
≥
αkδ + o(αk),
(14.6.16)
which contradicts the fact that h(f(xk)) ≤h(f(x∗)).
The contradiction
proves the theorem.
2
In fact, it also follows from (14.6.16) that, under assumption (14.6.14),
there exist ¯δ and ¯ϵ such that
h(f(x)) −h(f(x∗)) ≥¯δ∥x −x∗∥
(14.6.17)
holds for all x with ∥x −x∗∥≤¯ϵ.
14.7
Trust Region Method for Composite Nons-
mooth Optimization
For composite nonsmooth optimization (14.6.1), the subproblem of the trust-
region method has the form
mind∈Rn
h(f(xk) + A(xk)T d) + 1
2dT Bkd ∆= φk(d)
(14.7.1)
s.t.
∥d∥≤∆k,
(14.7.2)
where A(x) = ∇f(x)T ∈Rn×m, Bk ∈Rn×n is a symmetric matrix, and ∆k >
0 is a radius of the trust-region which is adjusted adaptively to be as large
as possible subject to adequate agreement between φk(d) and h(f(xk + d))
being maintained. The norm ∥· ∥in (14.7.2) is arbitrary but ∥· ∥2 is used in
this section without special speciﬁcation.
Let dk be a solution of subproblem (14.7.1)–(14.7.2). Similar to Theorem
14.6.3, we can prove that there must exist
λk ∈∂h(f(xk) + A(xk)T dk),
(14.7.3)
µk ∈∂∥dk∥,
(14.7.4)

624
CHAPTER 14. NONSMOOTH OPTIMIZATION
and ¯µk ≥0 such that
A(xk)λk + Bkdk + ¯µkµk = 0,
(14.7.5)
¯µk[∆k −∥dk∥] = 0.
(14.7.6)
The trust-region algorithm for composite nonsmooth optimization due to
Fletcher [129] is as follows.
Algorithm 14.7.1 (Trust-region algorithm for composite NSO)
Step 1. Given x1 ∈Rn, λ0 ∈Rm, ∆1 > 0, ϵ ≥0, k := 1.
Step 2. Compute
Bk =
m

i=1
(λk−1)i∇2fi(xk);
(14.7.7)
Solve the subproblem (14.7.1)–(14.7.2) for dk;
If ∥dk∥≤ϵ, stop.
Step 3. Calculate
rk = h(f(xk)) −h(f(xk + dk))
φk(0) −φk(dk)
.
(14.7.8)
If rk < 0.25 set ∆k+1 := ∥dk∥/4;
if rk > 0.75 and ∥dk∥= ∆k, set ∆k+1 = 2∆k;
otherwise, set ∆k+1 = ∆k.
Step 4. If rk > 0 go to Step 5;
else xk+1 := xk, λk := λk−1, go to Step 6.
Step 5. Set xk+1 := xk + dk, λk is deﬁned by (14.7.5).
Step 6. k := k + 1, go to Step 2.
2
To analyze the convergence of Algorithm 14.7.1, we assume that the se-
quence {xk} from the algorithm is bounded, which is implied if any level set
{x | h(f(x)) ≤h(f(x1))} is bounded. The boundedness of {xk} suggests that
there exists a bounded, closed convex set Ωsuch that
xk ∈Ω, xk + dk ∈Ω, ∀k = 1, 2, · · · .
(14.7.9)

14.7. TRUST REGION METHOD FOR COMPOSITE PROBLEMS
625
Since h(·) is convex and well-deﬁned on all of Rm, then there exists constant
L > 0 such that
|h(f1) −h(f2)| ≤L∥f1 −f2∥
(14.7.10)
holds for all f1, f2 ∈f(Ω) = {v = f(x), x ∈Ω}.
From the continuous
diﬀerentiability of f and the boundedness of Ω, it follows that there is a
constant M > 0 such that
∥A(x)∥≤M
(14.7.11)
holds for all x ∈Ω.
Theorem 14.7.2 Let fi(x) (i = 1, · · · , m) be twice continuously diﬀeren-
tiable, if the sequence {xk} generated by Algorithm 14.7.1 is bounded, then
there exists an accumulation point x∗of Algorithm 14.7.1, which is a station-
ary point of optimization problem (14.6.1).
As to the proof of the theorem, please consult Fletcher (1981). Further,
we have the following corollary.
Corollary 14.7.3 Under the assumption of Theorem 14.7.2, if, instead of
(14.7.7), ∥Bk∥is uniformly bounded, then {xk} has an accumulation point
x∗, which is a stationary point.
Now, the uniform boundedness of ∥Bk∥is relaxed to
∥Bk∥≤c5 + c6
k

i=1
∆i.
(14.7.12)
Also, the adjustment of trust-region radius can be extended to the general
case:
∥dk∥≤∆k+1 ≤min[c1∆k, ¯∆],
if rk ≥c2,
(14.7.13)
c3∥dk∥≤∆k+1 ≤c4∆k,
if rk < c2,
(14.7.14)
where ci(i = 1, · · · , 6) are positive constants and satisfy c1 > 1 > c4 >
c3, c2 < 1; ¯∆is a constant given in advance, an upper bound of the trust-
region radius.
Under the extended conditions, we also can establish the convergence.
We ﬁrst give a lemma.

626
CHAPTER 14. NONSMOOTH OPTIMIZATION
Lemma 14.7.4 Let dk be a solution of (14.7.1)–(14.7.2), then
h(f(xk)) −φk(dk) ≥1
2ψ∆k(xk) min

1, ψ∆k(xk)
∥Bk∥∆2
k

,
(14.7.15)
where ψt(x) is deﬁned by (14.6.4)–(14.6.5).
Proof.
It follows from the deﬁnition of dk that
h(f(xk)) −φk(dk) ≥h(f(xk)) −φk(d)
(14.7.16)
holds for any d with ∥d∥≤∆k. By the deﬁnition (14.6.5) of ψt(x), there
exists ∥¯dk∥≤∆k such that
ψ∆k(xk) = h(f(xk)) −h(f(xk) + A(xk)T ¯dk).
(14.7.17)
Then, by using the convexity of h(·), we obtain that
h(f(xk)) −φk(dk)
≥
h(f(xk)) −φk(α ¯dk)
=
χ(xk, α ¯dk) −1
2α2 ¯dT
k Bk ¯dk
≥
αχ(xk, ¯dk) −1
2α2∥Bk∥∥¯dk∥2
≥
αψ∆k(xk) −1
2α2∥Bk∥∆2
k
(14.7.18)
holds for all α ∈[0, 1]. Therefore
h(f(xk)) −φk(dk)
≥
max
0≤α≤1

αψ∆k(xk) −1
2α2∥Bk∥∆2
k

≥
1
2 min

ψ∆k(xk), [ψ∆k(xk)]2
∥Bk∥∆2
k

.
(14.7.19)
We complete the proof.
2
It is now possible to establish an extended conclusion of Theorem 14.7.2.
Theorem 14.7.5 Let fi(x) (i = 1, · · · , m) be twice continuously diﬀeren-
tiable.
Suppose that Bk in Algorithm 14.7.1 is not given by (14.7.7) but
instead by (14.7.12) and that the sequence {xk} of the algorithm is bounded,
then there must exist an accumulation point x∗of {xk} which is a stationary
point of the problem (14.6.1).

14.7. TRUST REGION METHOD FOR COMPOSITE PROBLEMS
627
Proof.
Suppose that the theorem does not hold, so there exists a positive
constant δ > 0 such that
ψ1(xk) ≥δ, ∀k.
(14.7.20)
By use of 5) of Lemma 14.6.2, Lemma 14.7.4, inequality (14.7.20) and bound-
edness of ∆k, we deduce that
h(f(xk)) −φk(dk)
≥
c7 min

∆k,
1
∥Bk∥

≥
c7 min

∆k,
1
c5 + c6

k
i=1 ∆i

,
(14.7.21)
where c7 is a positive constant. Deﬁne a set
S = {k | rk ≥c2},
(14.7.22)
then we have
h(f(x1)) −min
x∈Ωh(f(x))
≥
∞

k=1
[h(f(xk)) −h(f(xk+1))]
≥

k∈S
[h(f(xk)) −h(f(xk+1))]
≥
c2

k∈S
[h(f(xk)) −φk(dk)].
(14.7.23)
By (14.7.23), (14.7.21), (14.7.12) and ∆k ≤¯∆, it follows that

k∈S
∆k/

c5 + c6
k

i=1
∆i

< +∞.
(14.7.24)
In view of deﬁnition of ∆k+1, we have
∆k+1 ≤c4∆k, ∀k /∈S,
(14.7.25)
which gives
k

i=1
∆i ≤
 
1 +
c1
1 −c4
!
⎡
⎢⎣
k

i=1
i∈S
∆i + ∆1
⎤
⎥⎦.
(14.7.26)
Combining (14.7.24) and (14.7.26) yields that 
i∈S ∆i converges, and fur-
ther that 
∞
k=1 ∆k converges by (14.7.26) again. Hence ∥Bk∥is uniformly

628
CHAPTER 14. NONSMOOTH OPTIMIZATION
bounded. So, by Corollary 14.7.3, we know that (14.7.20) cannot hold for all
k. The contradiction proves the theorem.
2
Similar to the analysis of the trust-region method for unconstrained op-
timization, the condition (14.7.12) can further be weakened to
∥Bk∥≤c8 + c9k.
(14.7.27)
However, for the nonsmooth trust-region method, no matter what choices
of Bk, there is only linear convergence. Several modiﬁcations are available to
avoid the Maratos eﬀect and enable the second-order rate to be established.
The interested reader can consult Fletcher [131] and Yuan [369] for details.
14.8
Nonsmooth Newton’s Method
Qi and Sun [283] extended the classical Newton’s method to a non-smooth
case by using the generalized Jacobian instead of the classical Jacobian. In
this section, following Qi and Sun [283], we discuss the non-smooth Newton’s
method.
First, we introduce the generalized Jacobian and semismooth function.
Suppose that F : Rn →Rm is a locally Lipschitzian function. Rademacher’s
theorem says that F is diﬀerentiable almost everywhere. Denote the set of
points at which F is diﬀerentiable by DF . We write JF(x) for the usual
m × n Jacobian matrix of partial derivatives whenever x is a point at which
the necessary partial derivatives exist.
The generalized Jacobian of F at x, denoted by ∂F(x), is a convex hull of
all m × n matrices V obtained as the limit of a sequence of the form JF(xi),
where xi →x and xi ∈DF . Then, we have
∂F(x) = co {lim JF(xi) | xi →x, xi ∈DF }.
(14.8.1)
Let F be Lipschitz on an open convex set U in Rn, and let x and y be
points in U. Then, by Proposition 2.6.5 of Clarke [60], one has
F(y) −F(x) ∈∂F([x, y])(y −x).
(14.8.2)
Assume that for any h ∈Rn,
lim
V ∈∂F (x+th)
t↓0
{V h}
(14.8.3)

14.8. NONSMOOTH NEWTON’S METHOD
629
exists. Then the classical directional derivative
F ′(x; h) = lim
t↓0
F(x + th) −F(x)
t
(14.8.4)
exists, and
F ′(x; h) =
lim
V ∈∂F (x+th)
t↓0
{V h}.
(14.8.5)
In fact, by (14.8.2), we have
F(x + tjh) −F(x)
tj
∈co ∂F([x, x + tjh])h.
By the Carath`eodory theorem, there exist t(k)
j
∈[0, tj], λ(k)
j
∈[0, 1], V (k)
j
∈
∂F([x, x + t(k)
j h]), for k = 0, 1, · · · , m, 
m
k=0 λ(k)
j
= 1, such that
F(x + tjh) −F(x)
tj
=
m

k=0
λ(k)
j V (k)
j
h.
By passing to a subsequence, we can assume that λ(k)
j
→λj as j →∞. We
have λj ∈[0, 1] for k = 0, · · · , m and 
m
k=0 λj = 1. Then there are tj ↓0 such
that
F ′(x; h)
=
lim
j→∞
F(x + tjh) −F(x)
tj
= lim
j→∞{
m

k=0
λ(k)
j V (k)
j
h}
=
m

k=0
lim
j→∞λ(k)
j
lim
j→∞{V (k)
j
h} =
m

k=0
λj
lim
V ∈∂F (x+th)
t↓0
{V h}
=
lim
V ∈∂F (x+th)
t↓0
{V h}.
F is called semismooth at x if F is locally Lipschitzian at x and
lim
V ∈∂F (x+th′)
h′→h,t↓0
{V h′}
(14.8.6)
exists for any h ∈Rn. It implies that
lim
h′→h
t↓0
F(x + th′) −F(x)
t
=
lim
V ∈∂F (x+th′)
h′→h,t↓0
{V h′}.
(14.8.7)

630
CHAPTER 14. NONSMOOTH OPTIMIZATION
Lemma 14.8.1 suppose that F : Rn →Rm is locally Lipschitzian and
F ′(x; h) exists for any h at x. Then
(1) F ′(x; h) is Lipschitzian;
(2) for any h, there exists a V ∈∂F(x) such that
F ′(x; h) = V h.
(14.8.8)
Proof.
For any h, h′ ∈Rn,
∥F ′(x; h) −F ′(x; h′)∥=
lim
t↓0
F(x + th) −F(x + th′)
t

≤
lim
t↓0
∥F(x + th) −F(x + th′)∥
t
≤L∥h −h′∥,
where L is the Lipschitzian constant near x. This proves (1).
By (14.8.2) and (14.8.4), there are a sequence {tk} and a sequence {Vk}
such that tk ↓0, Vk ∈co ∂F([x, x + tkh]),
F ′(x; h) = lim
k→∞{Vkh}.
Because of the local Lipschitzian property of F, {Vk} is bounded. By passing
to a subsequence, we may assume that Vk →V . Also since ∂F is closed,
V ∈∂F(x). So, (2) is proved.
2
If F is semismooth, then for any V ∈∂F(x + h) and h →0,
V h −F ′(x; h) = o(∥h∥)
(14.8.9)
and
lim
x+h∈Df
h→0
F ′(x + h; h) −F ′(x; h)
∥h∥
= 0.
(14.8.10)
In fact, if F is semismooth, we have a conclusion that the right-hand side of
(14.8.7) is uniformly convergent for all h. Suppose that this conclusion does
not hold. Then there exist ϵ > 0, {hk ∈Rn | ∥hk∥= 1, k = 1, 2, · · ·}, ∥¯hk −
hk∥→0, tk ↓0, Vk ∈∂F(x + tk¯hk) such that
∥Vk¯hk −F ′(x; hk)∥≥2ϵ,
(14.8.11)
for k = 1, 2, · · ·. By passing to a subsequence, we may assume that hk →h.
Thus, ¯hk →h too. By Lemma 14.8.1 (1) and (14.8.11), we can get
∥Vk¯hk −F ′(x; h)∥≥ϵ
(14.8.12)

14.8. NONSMOOTH NEWTON’S METHOD
631
for all suﬃciently large k. This contradicts the semismoothness assumption.
The uniform convergence of the right-hand side of (14.8.7) implies the
uniform convergence of the right-hand side of (14.8.5), which further implies
(14.8.9).
Also, it immediately follows from (14.8.9) and (14.8.8) that (14.8.10)
holds.
The Fr´echet derivative F ′(x) is said to be strong if
lim
y→x
z→x
F(z) −F(y) −F ′(x)(z −y)
∥z −y∥
= 0.
(14.8.13)
Clearly, if F has strong Fr´echet derivative at x, then F is semismooth at x.
If for any V ∈∂F(x + h) and h →0,
V h −F ′(x; h) = O(∥h∥1+p),
where 0 < p ≤1, then we call F p-order semismooth at x. Obviously, p-order
semismoothness (0 < p ≤1) implies semismoothness.
Note that, if F is semismooth at x, then for any h →0,
F(x + h) −F(x) −F ′(x; h) = o(∥h∥).
(14.8.14)
If F is p-order semismooth at x, then for any h →0,
F(x + h) −F(x) −F ′(x; h) = O(∥h∥1+p).
(14.8.15)
Now, we are in a position to give the nonsmooth Newton’s method.
It is well-known that for smooth function F : Rn →Rn, the Newton’s
method for solving the nonlinear equation
F(x) = 0
(14.8.16)
is
xk+1 = xk −[F ′(xk)]−1F(xk).
(14.8.17)
Now, suppose that F is not a smooth function, but a locally Lipschitzian
function. Then the formula (14.8.17) cannot be used. Let ∂F(xk) be the
generalized Jacobian of F at xk. Instead of (14.8.17), we may use
xk+1 = xk −V −1
k
F(xk),
(14.8.18)
where Vk ∈∂F(xk), to solve the nonsmooth equation
F(x) = 0.
(14.8.19)

632
CHAPTER 14. NONSMOOTH OPTIMIZATION
Lemma 14.8.2 If all V ∈∂F(x) are nonsingular, then there is a neigh-
borhood N(x) of x and a constant C such that for any y ∈N(x) and any
V ∈∂F(y), V is nonsingular and
∥V −1∥≤C.
(14.8.20)
Proof.
By contradiction. If the lemma is not true, there is a sequence
yk →x, Vk ∈∂F(yk) such that either all Vk are singular or ∥V −1
k
∥→∞.
Since F is locally Lipschitzian, ∂F is bounded in a neighborhood of x. By
passing to a subsequence, we may assume that Vk →V .
Then V must
be singular, a contradiction to the assumption for this proposition.
This
completes the proof.
2
Theorem 14.8.3 (Local Convergence) Suppose that x∗is a solution of non-
smooth equation (14.8.19), F is locally Lipschitzian and semismooth at x∗,
and all V ∈∂F(x∗) are nonsingular. Then the iterative method (14.8.18)
is well-deﬁned and convergent to x∗in a neighborhood of x∗. If in addition
F is p-order semismooth at x∗, then the convergence of (14.8.18) is of order
1 + p.
Proof.
By Lemma 14.8.2, the iteration (14.8.18) is well-deﬁned in the
neighborhood of x∗. By (14.8.18), (14.8.9) and (14.8.14), we have
∥xk+1 −x∗∥
=
∥xk −x∗−V −1
k
F(xk)∥
≤
∥V −1
k
[F(xk) −F(x∗) −F ′(x∗, xk −x∗)]∥
+∥V −1
k
[Vk(xk −x∗) −F ′(x∗; xk −x∗)]∥
=
o(∥xk −x∗∥).
(14.8.21)
The case that F is p-order semismooth at x is similar.
2
Finally we give the global convergence of nonsmooth Newton’s method.
Theorem 14.8.4 (Global Convergence) Suppose that F is locally Lipschitzian
and semismooth on S = {x ∈Rn : ∥x −x0∥≤r}. Also suppose that for
any V ∈∂F(x) and x, y ∈S, V is nonsingular,
∥V −1∥≤β, ∥V (y −x) −F ′(x; y −x)∥≤γ∥y −x∥,
and
∥F(y) −F(x) −F ′(x; y −x)∥≤δ∥y −x∥,

14.8. NONSMOOTH NEWTON’S METHOD
633
where α = β(γ+δ) < 1 and β∥F(x0)∥≤r(1−α). Then the iterates (14.8.18)
remain in S and converge to the unique solution x∗of (14.8.19). Moreover,
the error estimate
∥xk −x∗∥≤[α/(1 −α)]∥xk −xk−1∥
(14.8.22)
holds for k = 1, 2, · · ·
Proof.
Obviously,
∥x1 −x0∥= ∥V −1
0
F(x0)∥≤β∥F(x0)∥≤r(1 −α).
So x1 ∈S. Suppose now that x1, x2, · · · , xk ∈S. Then
∥xk+1 −xk∥
=
∥V −1
k
F(xk)∥≤β∥F(xk)∥
≤
β∥F(xk) −F(xk−1) −F ′(xk−1; xk −xk−1)∥
+β∥Vk−1(xk −xk−1) −F ′(xk−1, xk −xk−1)∥
≤
β(δ + γ)∥xk −xk−1∥= α∥xk −xk−1∥≤αk∥x1 −x0∥
≤
rαk(1 −α).
(14.8.23)
Hence
∥xk+1 −x0∥≤
k

j=0
∥xj+1 −xj∥≤
k

j=0
rαj(1 −α) ≤r.
(14.8.24)
So xk+1 ∈S, i.e., all the iterates (14.8.18) remain in S.
For any k and n,
∥xk+n+1 −xk∥≤
k+n

j=k
∥xj+1 −xj∥≤
k+n

j=k
rαj(1 −α) ≤rαk.
(14.8.25)
So the iterates (14.8.18) converge to a point x∗in S. Since F is Lipschitzian
in S, ∥Vk∥is uniformly bounded. Thus
∥F(x∗)∥= lim
k→∞∥F(xk)∥≤lim
k→∞∥Vk∥∥xk+1 −xk∥= 0,
i.e., F(x∗) = 0.

634
CHAPTER 14. NONSMOOTH OPTIMIZATION
Suppose that there are x∗, y∗∈S with F(x∗) = 0 and F(y∗) = 0. Let
V ∗∈∂F(x∗). Then
∥y∗−x∗∥
≤
β∥V ∗(y∗−x∗)∥
≤
β∥V ∗(y∗−x∗) −F ′(x∗; y∗−x∗)∥
+β∥F(y∗) −F(x∗) −F ′(x∗; y∗−x∗)∥
≤
β(δ + γ)∥y∗−x∗∥= α∥y∗−x∗∥.
(14.8.26)
This implies
∥y∗−x∗∥≤0,
i.e., x∗= y∗. This shows that x∗is the unique solution of (14.8.19).
Finally,
∥xk+n+1 −xk∥
≤
k+n

j=k
∥xj+1 −xj∥≤
n

j=0
αj+1∥xk −xk−1∥
≤
α
1 −α∥xk −xk−1∥.
Setting n →∞, we obtain the result (14.8.22).
2
Exercises
1. Describe directional derivative, Dini directional derivative, Clarke di-
rectional derivative of f at x in the direction d respectively, and their prop-
erties and relations.
2. Describe the deﬁnition and properties of semi-smoothness.
3. Assume that f(x) is continuously diﬀerentiable. Prove that
∂f(x) = ∇f(x).
4. Assume that ci(x)(i = 1, ..., m) are continuously diﬀerentiable. Let
f(x) = max1≤i≤m ci(x) and ¯f(x) = 
m
i=1 |ci(x)|. Compute ∂f(x) and ∂¯f(x).
5. Prove Theorem 14.3.3.

14.8. NONSMOOTH NEWTON’S METHOD
635
6. Assume that f(x) is a convex function. Prove
f(x) = sup
y
sup
g∈∂f(y)
[f(y) + gT (x −y)].
7. Prove Theorem 14.4.2.
8. Prove the global convergence of the bundle method for uniformly con-
vex functions.
9. Prove Lemma 14.6.2.
10. Apply the trust-region Algorithm 14.7.1 to problem
min f(x) = max{1 + x1 −x2
2, 1 −x1 + (1 + ϵ)x2
2}
where ϵ > 0 is a small positive number with the starting point (δ, δ2) and
initial trust-region radius ∆1 = 0.5δ, δ > 0 being a small positive number.
You should observe that the iterates converge only linearly if the trust-region
is chosen {d|
∥d∥∞≤∆k}.
11. Prove Theorem 14.7.2.
12. Modify Algorithm 14.7.1 to derive a nonmonotone algorithm.
13. Give a generalized Newton’s method for nonsmooth optimization and
establish its global and local convergence.

Appendix: Test Functions
§1.Test Functions for Unconstrained Optimization Problems
Problem 1.1 Rosenbrock function:
f(x) = 100(x2 −x2
1)2 + (1 −x1)2,
(1.1)
x0 = [−1.2, 1]T , x∗= [1, 1]T , f(x∗) = 0.
Problem 1.2 Extended Rosenbrock function:
f(x) =
n−1

i=1
[100(xi+1 −x2
i )2 + (1 −xi)2],
(1.2)
x0 = [−1.2, 1, · · · , −1.2, 1]T , x∗= [1, 1, · · · , 1, 1]T , f(x∗) = 0.
Problem 1.3 Wood function:
f(x)
=
100(x2
1 −x2)2 + (x1 −1)2 + (x3 −1)2 + 90(x2
3 −x4)2
+10.1[(x2 −1)2 + (x4 −1)2] + 19.8(x2 −1)(x4 −1),
(1.3)
x0 = [−3, −1, −3, −1]T , x∗= [1, 1, 1, 1]T , f(x∗) = 0.
Problem 1.4 Powell singular function:
f(x) = (x1 + 10x2)2 + 5(x3 −x4)2 + (x2 −2x3)4 + 10(x1 −x4)4,
(1.4)
x0 = [3, −1, 0, 1]T , x∗= [0, 0, 0, 0]T , f(x∗) = 0.
Problem 1.5 Cube function
f(x) = 100(x2 −x3
1)2 + (1 −x1)2,
(1.5)
x0 = [−1.2, −1]T , x∗= [1, 1]T , f(x∗) = 0.

638
APPENDIX: TEST FUNCTIONS
Problem 1.6 Trigonometric function
f(x) =
n

i=1
⎡
⎣n + i(1 −cos xi) −sin xi −
n

j=1
cos xj
⎤
⎦
2
,
(1.6)
x0 =
 1
5n, · · · , 1
5n
T
, x∗= [0, · · · , 0]T , f(x∗) = 0.
Problem 1.7 Helical valley function
f(x) = 100[(x3 −10θ)2 + (

x2
1 + x2
2 −1)2] + x2
3,
(1.7)
where
2πθ =

arctan(x1/x2)
if x1 > 0,
π + arctan(x2/x1)
if x1 < 0,
x0 = [−1, 0, 0]T , x∗= [1, 0, 0]T , f(x∗) = 0.
§2. Test Functions for Constrained Optimization Problems
The test functions for constrained optimization are selected from Hock
and Schittkowski [176].
Problem 2.1 (No. 14 in [176])
Number of Variables: n = 2
Objective Function:
f(x) = (x1 −2)2 + (x2 −1)2
Constraints:
−0.25x2
1 −x2
2 + 1 ≥0,
x1 −2x2 + 1 = 0.
Start: x0 = (2, 2), f(x0) = 1.
Solution: x∗= (0.5(
√
7 −1), 0.25(
√
7 + 1)),
f(x∗) = 9 −2.875
√
7.
Problem 2.2 (No. 22 in [176])

639
Number of Variables: n = 2
Objective Function:
f(x) = (x1 −2)2 + (x2 −1)2
Constraints:
−x1 −x2 + 2 ≥0
−x2
1 + x2 ≥0
Start: x0 = (2, 2), f(x0) = 1.
Solution: x∗= (1, 1), f(x∗) = 1.
Problem 2.3 (No. 59 in [176])
Number of Variables: n = 2
Objective Functions:
f(x)
= −75.196 + 3.8112x1 + 0.0020567x3
1 −1.0345E-5x4
1
+6.8306x2 −0.030234x1x2 + 1.28134E-3x2x2
1
+2.266E-7x4
1x2 −0.25645x2
2 + 0.0034604x3
2 −1.3514E-5x4
2
+28.106/(x2 + 1) + 5.2375E-6x2
1x2
2 + 6.3E-8x3
1x2
2
−7E-10x3
1x3
2 −3.405E-4x1x2
2 + 1.6638E-6x1x3
2
+2.8673 exp(0.0005x1x2) −3.5256E-5x3
1x2
Constraints:
x1x2 −700 ≥0,
x2 −x2
1/125 ≥0,
(x2 −50)2 −5(x1 −55) ≥0,
0 ≤x1 ≤75,
0 ≤x2 ≤65.
Start: x0 = (90, 10), f(x0) = 86.878639
Solution: x∗= (13.55010424, 51.66018129), f(x∗) = −7.804226324.
Problem 2.4 (No. 63 in [176])
Number of Variables: n = 3
Objective Function:
f(x) = 1000 −x2
1 −2x2
2 −x2
3 −x1x2 −x1x3

640
APPENDIX: TEST FUNCTIONS
Constraints:
8x1 + 14x2 + 7x3 −56 = 0,
x2
1 + x2
2 + x2
3 −25 = 0,
0 ≤xi, i = 1, 2, 3.
Start: x0 = (2, 2, 2), f(x0) = 976
Solution: x∗= (3.512118414, 0.2169881741, 3.552174034), f(x∗) = 961.7151721
Problem 2.5 (No. 25 in [176])
Number of Variables: n = 3
Objective Function:
f(x) =
99

i=1
(fi(x))2
where
fi(x)
=
−0.01i + exp(−1
x1
(ui −x2)x3)
ui
=
25 + (−50 ln(0.01i))2/3, i = 1, · · · , 99.
Constraints:
0.1 ≤
x1
≤100
0 ≤
x2
≤25.6
0 ≤
x3
≤5
Start: x0 = (100, 12.5, 3), f(x0) = 32.835
Solution: x∗= (50, 25, 1.5), f(x∗) = 0
Problem 2.6 (No. 35 in [176])
Number of Variables: n = 3
Objective Function:
f(x)
=
9 −8x1 −6x2 −4x3 + 2x2
1 + 2x2
2 + x2
3
+2x1x2 + 2x1x3
Constraints:
3 −x1 −x2 −2x3 ≥0
0 ≤xi, i = 1, 2, 3.

641
Start: x0 = (0.5, 0.5, 0.5), f(x0) = 2.25
Solution: x∗= (4/3, 7/9, 4/9), f(x∗) = 1/9.
Problem 2.7 (No. 38 in [176])
Number of Variables: n = 4
Objective Function:
f(x)
=
100(x2 −x2
1)2 + (1 −x1)2 + 90(x4 −x2
3)2 + (1 −x3)2
+10.1((x2 −1)2 + (x4 −1)2) + 19.8(x2 −1)(x4 −1)
Constraints:
−10 ≤xi ≤10, i = 1, · · · , 4
Start: x0 = (−3, −1, −3, −1), f(x0) = 19192
Solution: x∗= (1, 1, 1, 1), f(x∗) = 0.
Problem 2.8 (No. 43 in [176])
Number of Variables: n = 4
Objective Function:
f(x) = x2
1 + x2
2 + 2x2
3 + x2
4 −5x1 −5x2 −21x3 + 7x4
Constraints:
8 −x2
1 −x2
2 −x2
3 −x2
4 −x1 + x2 −x3 + x4 ≥0
10 −x2
1 −2x2
2 −x2
3 −2x2
4 + x1 + x4 ≥0
5 −2x2
1 −x2
2 −x2
3 −2x1 + x2 + x4 ≥0
Start: x0 = (0, 0, 0, 0), f(x0) = 0.
Solution: x∗= (0, 1, 2, −1), f(x∗) = −44
Problem 2.9 (No. 73 in [176])
Number of Variables: n = 4
Objective Function:
f(x) = 24.55x1 + 26.75x2 + 39x3 + 40.50x4
Constraints:
2.3x1 + 5.6x2 + 11.1x3 + 1.3x4 −5 ≥0
12x1 + 11.9x2 + 41.8x3 + 52.1x4 −21
−1.645(0.28x2
1 + 0.19x2
2 + 20.5x2
3 + 0.62x2
4)
1
2 ≥0
x1 + x2 + x3 + x4 −1 = 0
0 ≤xi, i = 1, · · · , 4.

642
APPENDIX: TEST FUNCTIONS
Start: x0 = (1, 1, 1, 1), f(x0) = 130.8
Solution:
x∗= (0.6355216, −0.12E-11, 0.3127019, 0.05177655),
f(x∗) = 29.894378
Problem 2.10 (No. 83 in [176])
Number of Variables: n = 5
Objective Function:
f(x) = 5.3578547x2
3 + 0.8356891x1x5 + 37.293239x1 −40792.141
Constraints:
92 ≥a1 + a2x2x5 + a3x1x4 −a4x3x5 ≥0
20 ≥a5 + a6x2x5 + a7x1x2 + a8x2
3 −90 ≥0
5 ≥a9 + a10x3x5 + a11x1x3 + a12x3x4 −20 ≥0
78 ≤x1 ≤102
33 ≤x2 ≤45
27 ≤xi ≤45, i = 3, 4, 5,
where
a1 = 85.334407, a2 = 0.0056858, a3 = 0.0006262,
a4 = 0.0022053, a5 = 80.51249, a6 = 0.0071317,
a7 = 0.0029955, a8 = 0.0021813, a9 = 9.300961,
a10 = 0.0047026, a11 = 0.0012547, a12 = 0.0019085
Start: x0 = (78, 33, 27, 27, 27), f(x0) = −32217
Solution: x∗= (78, 33, 29.99526, 45, 36.77581), f(x∗) = −30665.53867
Problem 2.11 (No. 86 in [176])
Number of Variables: n = 5
Objective Function:
f(x) =
5

j=1
ejxj +
5

i=1
5

j=1
cijxixj +
5

j=1
djx3
j

643
Constraints:
5

j=1
aijxj −bi ≥0, i = 1, · · · , 10,
0 ≤xi, i = 1, · · · , 5,
where
j
1
2
3
4
5
ej
-15
-27
-36
-18
-12
c1j
30
-20
-10
32
-10
c2j
-20
39
-6
-31
32
c3j
-10
-6
10
-6
-10
c4j
32
-31
-6
39
-20
c5j
-10
32
-10
-20
30
dj
4
8
10
6
2
a1j
-16
2
0
1
0
a2j
0
-2
0
4
2
a3j
-3.5
0
2
0
0
a4j
0
-2
0
-4
-1
a5j
0
-9
-2
1
-2.8
bj
-40
-2
-0.25
-4
-4
Start: x0 = (0, 0, 0, 0, 1), f(x0) = 20
Solution: x∗= (0.3, 0.33346761, 0.4, 0.42831010, 0.22396487), f(x∗) = −32.34867897
Problem 2.12 (No. 93 in [176])
Number of Variables: n = 6
Objective Function:
f(x)
=
0.0204x1x4(x1 + x2 + x3) + 0.0187x2x3(x1 + 1.57x2 + x4)
+0.0607x1x4x2
5(x1 + x2 + x3)
+0.0437x2x3x2
6(x1 + 1.57x2 + x4)
Constraints:
0.001x1x2x3x4x5x6 −2.07 ≥0,
1 −0.00062x1x4x2
5(x1 + x2 + x3),
−0.00058x2x3x2
6(x1 + 1.57x2 + x4) ≥0,
0 ≤xi, i = 1, · · · 6.

644
APPENDIX: TEST FUNCTIONS
Start: x0 = (5.54, 4.4, 12.02, 11.82, 0.702, 0.852), f(x0) = 137.066
Solution:
x∗= (5.332666, 4.656744, 10.43299,
12.08230, 0.7526074, 0.87865084),
f(x∗) = 135.075961
Problem 2.13 (No. 108 in [176])
Number of Variables: n = 9
Objective Function:
f(x) = −0.5(x1x4 −x2x3 + x3x9 −x5x9 + x5x8 −x6x7)
Constraints:
1 −x2
3 −x2
4 ≥0,
1 −x2
5 −x2
6 ≥0,
1 −x2
9 ≥0,
1 −x2
1 −(x2 −x9)2 ≥0,
1 −(x1 −x5)2 −(x2 −x6)2 ≥0,
1 −(x1 −x7)2 −(x2 −x8)2 ≥0,
1 −(x3 −x5)2 −(x4 −x6)2 ≥0,
1 −(x3 −x7)2 −(x4 −x8)2 ≥0,
1 −x2
7 −(x8 −x9)2 ≥0,
x1x4 −x2x3 ≥0,
x3x9 ≥0,
−x5x9 ≥0,
x5x8 −x6x7 ≥0,
0 ≤x9.
Start:
x0 = (1, 1, 1, 1, 1, 1, 1, 1, 1),
f(x0) = 0

645
Solution:
x∗= (0.8841292, 0.4672425, 0.03742076, 0.9992996,
0.8841292, 0.4672424, 0.03742076, 0.9992996,
0.26E-19),
f(x∗) = −0.8660254038
Problem 2.14 (No. 110 in [176])
Number of Variables: n = 10
Objective Function:
f(x) =
10

i=1
[(ln(xi −2))2 + (ln(10 −xi))2 −(
10
/
i=1
xi)2]
Constraints:
2.001 ≤xi ≤9.999, i = 1, · · · , 10.
Start: x0 = (9, · · · , 9), f(x0) = −43.134337
Solution: x∗= (9.35025655, · · · , 9.35025655), f(x∗) = −45.77846971
Problem 2.15 (No. 111 in [176])
Number of Variables: n = 10
Objective Function:
f(x) =
10

j=1
exp(xj)(cj + xj −ln(
10

k=1
exp(xk)))
where
c1 = −6.089, c2 = −17.164, c3 = −34.054,
c4 = −5.914, c5 = −24.721, c6 = −14.986,
c7 = −24.100, c8 = −10.708, c9 = −26.662, c10 = −22.179
Constraints:
exp(x1) + 2 exp(x2) + 2 exp(x3) + exp(x6) + exp(x10) −2 = 0,
exp(x4) + 2 exp(x5) + exp(x6) + exp(x7) −1 = 0,
exp(x3) + exp(x7) + exp(x8) + 2 exp(x9) + exp(x10) −1 = 0,
−100 ≤xi ≤100, i = 1, · · · , 10.

646
APPENDIX: TEST FUNCTIONS
Start: x0 = (−2.3, · · · , −2.3), f(x0) = −21.015
Solution:
x∗
=
(−3.201212, −1.912060, −0.2444413, −6.537489,
−0.7231524, −7.267738, −3.596711, −4.017769,
−3.287462, −2.335582),
f(x∗)
=
−47.76109026
Problem 2.16 (No. 112 in [176])
Number of Variables: n = 10
Objective Function:
f(x) =
10

j=1
xj(cj + ln
xj
x1 + · · · + x10
)
where cj are deﬁned in Problem 2.15.
Constraints:
x1 + 2x2 + 2x3 + x6 + x10 −2 = 0,
x4 + 2x5 + x6 + x7 −1 = 0,
x3 + x7 + x8 + 2x9 + x10 = 0,
1.E-6 ≤xi, i = 1, · · · , 10.
Start: x0 = (0.1, · · · , 0.1), f(x0) = −20.961
Solution:
x∗
=
(0.01773548, 0.08200180, 0.8825646, 0.7233256E-3,
0.4907851, 0.4335469E-3, 0.01727298,
0.007765639, 0.01984929, 0.05269826),
f(x∗)
=
−47.707579
Problem 2.17 (No. 117 in [176])
Number of Variables: n = 15
Objective Function:
f(x) = −
10

j=1
bjxj +
5

j=1
5

k=1
ckjx10+kx10+j + 2
5

j=1
djx3
10+j

647
Constraints:
2
5

k=1
ckjx10+k + 3djx2
10+j + ej −
10

k=1
akjxk ≥0, j = 1, · · · , 5,
0 ≤xi, i = 1, · · · , 15,
where
j
1
2
3
4
5
a6j
2
0
-4
0
0
a7j
-1
-1
-1
-1
-1
a8j
-1
-2
-3
-2
-1
a9j
1
2
3
4
5
a10j
1
1
1
1
1
b5+j
-1
-40
-60
5
1
and other parameters are deﬁned as in Problem 2.11.
Start:
x0 = 0.001(1, 1, 1, 1, 1, 1, 60000, 1, 1, 1, 1, 1, 1, 1, 1),
f(x0) = 2400.1053
Solution:
x∗
=
(0, 0, 5.174136, 0, 3.061093, 11.83968, 0, 0,
0.1039071, 0, 0.2999929, 0.3334709, 0.3999910,
0.4283145, 0.2239607)
f(x∗)
=
32.348679

Bibliography
[1] N. Abachi, On variable metric algorithms, J. Optimization Theory and
Methods 7 (1971) 391-410.
[2] M. Al-Baali, Descent property and global convergence of the Fletcher-
Reeves method with inexact line search, IMA J. Numerical Analysis 5
(1985) 121-124.
[3] K.A. Ariyawansa, Deriving collinear scaling algorithms as extensions of
quasi-Newton methods and the local convergence of DFP- and BFGS-
related collinear scaling algorithms, Mathematical Programming 49
(1990) 23-48.
[4] L. Armijo, Minimization of functions having Lipschitz continuous par-
tial derivatives, Paciﬁc J. Mathematics 16 (1966) 1-3.
[5] M. Avriel, Nonlinear Programming: Analysis and Methods, Prentice-
Hall, Inc., Englewood Cliﬀs, New Jersey, (1976).
[6] P. Baptist and J. Stoer, On the relation between quadratic termina-
tion and convergence properties of minimization algorithms, Part II:
Applications, Numerische Mathematik 28 (1977) 367-392.
[7] R. Bartels and A. Conn, An approach to nonlinear l1 data ﬁtting,
in: J.P. Hennart ed., Lecture Notes in Mathematics 909: Numerical
Analysis, Cocoyoc 1981 (Springer-Verlag, Berlin, 1982), 48-58.
[8] J. Barzilai and J.M. Borwein, Two-point step size gradient methods,
IMA Journal of Numerical Analysis 8 (1988) 141-148.
[9] M.S. Bazara and C.M. Shetty, Nonlinear Programming, Theory and
Algorithms, John Wiley and Sons, New York, (1979).

650
BIBLIOGRAPHY
[10] E.M.L. Beale, A derivative of conjugate gradients, in F.A. Lootsma
eds., Numerical Methods for Nonlinear Optimization, London, Aca-
demic Press, (1972), 39-43.
[11] C.S. Beighter, D.T. Phillips and D.J. Wilde, Foundations of Optimiza-
tion, Prentice-Hall, Englewood Cliﬀs, N.J., (1979).
[12] A. Ben-Israel and T.N.E. Greville, Generalized Inverses: Theory and
Applications, John Wiley & Sons, New York, (1974).
[13] D.P. Bertsekas, Constrained Optimization and Lagrange Multipler
Methods, Academic Press, New York, (1982).
[14] D.P. Bertsekas, Nonlinear Programming, Athena Scientiﬁc, Belmont,
Mass., (1995).
[15] M.C. Bartholomew-Biggs, The estimation of the Hessian matrix in non-
linear least squares problems with non-zero residuals, Mathematical
Programming 12 (1977) 67-80.
[16] M.C. Bartholomew-Biggs, Recursive quadratic programming meth-
ods based on the augmented Lagrangian, Mathematical Programming
Study 31 (1987) 21-24.
[17] M.C. Biggs, Minimization algorithms making use of non-quadratic
properties of the objective function, Institute of Mathematics and Its
Applications 8 (1971) 315-327.
[18] A. Bj¨ock, Numerical Methods for Least Squares Problems, SIAM Pub-
lications, Philadelphia, Penn, (1996).
[19] P.T. Boggs and J.W. Tolle, Merit function for nonlinear programming
problems, Operations Research and System Analysis Report No 81-2,
University of North Carolina at Capel Hill, (1981).
[20] P.T. Boggs and J.W. Tolle, Convergence properties of a class of rank-
two updates, SIAM Journal on Optimization 4 (1994) 262-287.
[21] P.T. Boggs and J.W. Tolle, Sequential quadratic programming, Acta
Numerica 4 (1996) 1-51.

BIBLIOGRAPHY
651
[22] P.T. Boggs, J.W. Tolle, and P. Wang, On the local convergence methods
for constrained optimization, SIAM J. Control and Optimization 20
(1982) 161-171.
[23] I. Bongartz, A.R. Conn, N.I.M. Gould, and Ph.L. Toint, CUTE: Con-
strained and unconstrained testing environment, ACM Transactions on
Mathematical Software 21 (1995) 123-160.
[24] C.A. Botsaris and D.H. Jacobson, A Newton-type curvilinear search
method for optimization, J. Mathematical Analysis and Applications
54 (1976) 217–229.
[25] A. Bouaricha and R.B. Schnabel, Tensor methods for large sparse non-
linear least-squares problems, SIAM J. Scientiﬁc Computing 21 (1999)
1199-1221.
[26] C.G. Broyden, A class of methods for solving nonlinear simultaneous
equations, Mathematics of Computation 19 (1965) 577-593.
[27] C.G. Broyden, The convergence of a class double-rank minimization al-
gorithms, Journal of the Institute of Mathematics and its Applications
6 (1970) 76-90.
[28] C.G. Broyden, J.E. Dennis and J.J. Mor´e, On the local and superlinear
convergence of quasi-Newton algorithm, J. Inst. Math. Appl. 12 (1973)
222-236.
[29] C.G. Broyden, J.E. Dennis, Jr., and J.J. Mor´e, On the local superlinear
convergence of quasi-Newton methods, J. Institute of Mathematics and
Applications 12 (1973) 223-246.
[30] A. Buckley, A combined conjugate gradient quasi-Newton minimization
algorithm, Mathematical Programming 15 (1978) 200-210.
[31] J.P. Buleau and J.Ph. Vial, Curvilinear path and trust region in un-
constrained optimization: a convergence analysis, Mathematical Pro-
gramming Study 30 (1987) 82-101.
[32] J.R. Bunch and L. Kaufman, Some stable methods for calculating iner-
tia and solving symmetric linear systems, Mathematics of Computation
31 (1977) 163-179.

652
BIBLIOGRAPHY
[33] J.R. Bunch and B.N. Parlett, Direct methods for solving symmetric in-
deﬁnite systems of linear equations, SIAM Journal on Numerical Anal-
ysis 8 (1971) 639-655.
[34] J.V. Burke, Descent methods for composite nondiﬀerential optimiza-
tion problems, Mathematical Programming 33 (1985) 260-279.
[35] J.V. Burke, Second order necessary and suﬃcient conditions for convex
composite NDO, Mathematical Programming 38 (1987) 287-302.
[36] J.V. Burke, A robust trust region method for constrained nonlinear
programming problems, SIAM J. Optimization 2 (1992) 325-347.
[37] J.V. Burke and J.J. Mor´e, On the identiﬁcation of active constraints,
SIAM J. Numerical Analysis 25 (1988) 1197-1211.
[38] J.V. Burke, J.J. Mor´e, and G. Toraldo, Convergence properties of trust
region methods for linear and convex constraints, Mathematical Pro-
gramming 47 (1990) 305-336.
[39] W. Burmeister, Die konvergenzordnung des Fletcher-Powell algorith-
mus, Z. Angew. Math. Mech. 53 (1973) 693-699.
[40] R.H. Byrd, An example of irregular convergence in some constrained
optimization methods that use projected Hessian, Mathematical Pro-
gramming 32 (1985) 232-237.
[41] R.H. Byrd, M.E. Hribar, and J. Nocedal, An interior-point algorithm
for large-scale nonlinear programming, Technical Report 97/05, Opti-
mization Technology Center, Argonne National Laboratory and North-
western University, July (1997).
[42] R.H. Byrd, H.F. Khalfan, and R.B. Schnabel, Analysis of symmetric
rank-one trust region method, SIAM Journal on Optimization 6 (1996)
1025-1039.
[43] R.H. Byrd, D.C. Liu, and J. Nocedal, On the behavior of Broydens class
of quasi-Newton methods, SIAM Journal on Optimization (1992).
[44] R.H. Byrd and J. Nocedal, A tool for the analysis of quasi-Newton
methods with application to unconstrained minimization, SIAM Jour-
nal on Numerical Analysis 26 (1989) 727-739.

BIBLIOGRAPHY
653
[45] R.H. Byrd and J. Nocedal, An analysis of reduced Hessian methods for
constrained optimization, Mathematical Programming 49 (1991) 285-
323.
[46] R.H. Byrd, J. Nocedal, and R.B. Schnabel, Representations of quasi-
Newton matrices and their use in limit-memory methods, Mathematical
Programming 63 (1994) 129-156.
[47] R.H. Byrd, J. Nocedal and Y. Yuan, Global convergence of a class
of variable metric algorithms, SIAM J. Numerical Analysis 24 (1987)
1171-1190.
[48] R.H. Byrd, R.B. Schnabel, and G.A. Schultz, A trust region algorithm
for nonlinearly constrained optimization, SIAM J. Numerical Analysis
24 (1987) 1152-1170.
[49] R.H. Byrd, R.B. Schnabel, and G.A. Schultz, Approximate solution of
the trust region problem by minimization over two-dimensional sub-
spaces, Mathematical Programming 40 (1988) 247-263.
[50] P.H. Calamai and J.J. Mor´e, Projected gradient methods for linearly
constrained problems, Mathematical Programming 39 (1987) 93-116.
[51] M.R. Celis, A trust region strategy for nonlinear equality constrained
optimization, Ph.D. thesis, Dept of Math. Sci., Rice University, Hous-
ton, (1985).
[52] M.R. Celis, J.E. Dennis and R.A. Tapia, A trust region algorithm for
nonlinear equality constrained optimization, in P.T. Boggs, R.H. Byrd
and R.B. Schnabel, eds., Numerical Optimization (SIAM Philadelphia,
1985), 71-82.
[53] R.M. Chamberlain, Some examples of cycling in variable metric meth-
ods for constrained minimization, Mathematical Programming 16
(1979) 378-383.
[54] R.M. Chamberlain, C. Lemarechal, H.C. Pedersen, and M.J.D. Pow-
ell, The watchdog techniques for forcing convergence in algorithms for
constrained optimization, Mathematical Programming Study 16 (1982)
1-17.

654
BIBLIOGRAPHY
[55] C. Charelambous, Unconstrained optimization based on homogeneous
models, Mathematical Programming 5 (1973) 189-198.
[56] C. Charelambous and A.R. Conn, An eﬃcient method to solve the
minimax problem directly, SIAM J. Numerical Analysis 15 (1978) 162-
187.
[57] X. Chen, Superlinear convergence of smoothing quasi-Newton methods
for nonsmooth equations, J. of Computational and Applied Mathemat-
ics 80 (1997) 105-126.
[58] E.W. Cheney and A.A. Goldstein, Newton’s method for convex pro-
gramming and Chebyshev approximation, Numerische Mathematik 1
(1959) 253-268.
[59] V. Chvat´al, Linear Programming, W.M. Freeman and Company, New
York, (1983).
[60] F.H. Clarke, Optimization and Nonsmooth Analysis, John Wiley and
Sons, New York, (1983).
[61] A. Cohen, Rate of convergence of several conjugate gradient algorithms,
SIAM J. Numer Anal. 9 (1972) 248-259.
[62] T.F. Coleman and A.R. Conn, Nonlinear programming via an exact
penalty function: asymptotic analysis, Mathematical Programming 24
(1982) 123-136.
[63] T.F. Coleman and A.R. Conn, On the local convergence of a quasi-
Newton method for the nonlinear programming problem, SIAM J. Nu-
merical Analysis 21 (1984) 755-769.
[64] A.R. Conn, N.I.M. Gould, D. Orban, and Ph.L. Toint, A primal-dual
trust region algorithm for nonconvex nonlinear programming, Mathe-
matical Programming 87 (2000) 215-249.
[65] A.R. Conn, N.I.M. Gould, and Ph.L. Toint, Global convergence of a
class of trust region algorithms for optimization with simple bounds,
SIAM J. on Numerical Analysis 25 (1988) 433-460.
[66] A.R. Conn, N.I.M. Gould, and Ph.L. Toint, Testing a class of algo-
rithms for solving minimization problems with simple bounds on the
variables, Mathematics of Computation 50 (1988) 399-430.

BIBLIOGRAPHY
655
[67] A.R. Conn, N.I.M. Gould, and Ph.L. Toint, Convergence of quasi-
Newton matrices generated by symmetric rank one update, Mathe-
matical Programming 50 (1991) 177-195.
[68] A.R. Conn, N.I.M. Gould, and Ph.L. Toint, LANCELOT: a FORTRAN
package for large-scale nonlinear optimization (Release A), No. 17 in
Springer Series in Computational Mathematics, Springer-Verlag, New
York, (1992).
[69] A.R. Conn, N.I.M. Gould, and Ph.L. Toint, Convergence properties of
minimization algorithms for convex constraints using a structured trust
region, SIAM Journal on Numerical Analysis 25 (1996) 1059-1086.
[70] A.R. Conn, N.I.M. Gould and Ph.L. Toint, Trust-Region Methods,
SIAM, (2000).
[71] G. Corradi, Quasi-Newton methods for nonlinear equations and un-
constrained optimization methods, International Journal of Computer
Mathematics 38 (1991) 71-89.
[72] C.W. Cryer, Numerical Functional Analysis, Clarendon Press, Oxford,
(1982).
[73] Y.H. Dai, New properties of a nonlinear conjugate gradient method,
Numerische Mathematik 89 (2001) 83-98.
[74] Y.H. Dai and Y. Yuan, Convergence properties of Fletcher–Reeves
method, IMA J. Numerical Analysis 16 (1996) 155-164.
[75] Y.H. Dai and Y. Yuan, A nonlinear conjugate gradient method with
a strong global convergence property, SIAM J. Optimization 10 (1999)
177-182.
[76] Y.H. Dai and Y. Yuan, An eﬃcient hybrid conjugate gradient method
for unconstrained optimization, Annals of Operations Research 103
(2001) 33-47.
[77] Y.H. Dai and Y. Yuan, A three-parameter family of nonlinear conjugate
gradient methods, Mathematics of Computation 70 (2001) 1155-1167.
[78] G.B. Dantzig, Linear Programming and Extensions, Princeton Univer-
sity Press, Princeton, New Jersey, (1963).

656
BIBLIOGRAPHY
[79] W.C. Davidon, Variable metric methods for minimization, Argonne
National Labs Report, ANL-5990, (1959).
[80] W.C. Davidon, Optimally conditioned optimization algorithms without
line searches, Mathematical Programming 9 (1975) 1-30.
[81] W.C. Davidon, Optimization by nonlinear scaling, in: D. Jacobs ed.,
Proceedings of the conference on Applications of Numerical Software
— Needs and Availability, Academic Press, New York, (1978), 377-383.
[82] W.C. Davidon, Conic approximation and Collinear scaling for optimiz-
ers, SIAM Numer. Anal. 17 (1980) 268-281.
[83] R.S.Dembo, S.C.Eisenstat, and T.Steihaug, Inexact Newton methods,
SIAM Journal on Numerical Analysis 19 (1982) 400-408.
[84] V.F. Demyanov and L.V. Vaselev, Nondiﬀerentiable Optimization, Op-
timization Software, Inc., New York, (1985).
[85] N.Y. Deng, Computational Methods for Unconstrained Optimization,
Science Press, Beijing, (1982).
[86] N.Y. Deng, Y. Xiao and F. Zhou, A nonmonotonic trust region al-
gorithm, Journal of Optimization Theory and Applications 76 (1993)
259-285.
[87] J.E. Dennis Jr., M. El-Alem, and M.C. Maciel, A global convergence
theory for general trust region based algorithms for equality constrained
optimization, SIAM J. Optimization 7 (1997) 177-207.
[88] J.E. Dennis Jr., D.M. Gay and R.E. Welsch, An adaptive nonlinear
least-squares algorithm, ACM Transactions on Math. Software 7 (1981)
348-368.
[89] J.E. Dennis Jr., S.B. Li, and R.A. Tapia, A uniﬁed approach to
global convergence of trust region methods for nonsmooth optimiza-
tion, Mathematical Programming 68 (1995) 319-346.
[90] J.E. Dennis and H.H.W. Mei, Two new unconstrained optimization
algorithms with use function and gradient values, Journal of Optimiza-
tion Theory and Applications 28 (1979) 453-482.

BIBLIOGRAPHY
657
[91] J.E. Dennis Jr., and J.J. Mor´e, A charaterization of superlinear con-
vergence and its application to quasi-Newton methods, Math. Comp.
28 (1974) 549-560.
[92] J.E. Dennis Jr., and J.J. Mor´e, Quasi-Newton Methods, motivation
and theory, SIAM Review 19 (1977) 46-89.
[93] J.E. Dennis Jr., and R.B. Schnabel, Least change secant updates for
quasi-Newton methods, SIAM Review 19 (1979) 443-459.
[94] J.E. Dennis Jr., and R.B. Schnabel, A new derivation of symmetric
positive deﬁnite secant updates, in: O.L. Mangasarian, R.R. Meyer and
S.M. Robinson eds., Nonlinear Programming vol. 4, Academic Press,
New York, (1980) 167-199.
[95] J.E. Dennis and R.B. Schnabel, Numerical Methods for Uncon-
strained Optimization and Nonlinear Equations, Prentice-Hall, Engle-
wood Cliﬀs, NJ, (1983).
[96] J.E. Dennis and R.B. Schnabel, A view of unconstrained optimization,
in: Optimization Vol. 1 of Handbooks in Operations Research and
Management, Elsevier Science Publishers, Amsterdam, (1989) 1–72.
[97] J.E. Dennis Jr. and K. Turner, Generalized conjugate directions, Re-
port 85-11, Dept of Mathmatics, Rice University, Houston, (1985).
[98] J.E.Dennis Jr. and H.F. Walker, Convergence theorems for least change
secant update methods, SIAM J. Numer. Anal. 18 (1981) 949-987; 19
(1982) 443-443.
[99] J.E.Dennis Jr. and H.F. Walker, Least-change sparse secant update
methods with inaccurate secant conditions, SIAM J. Numer. Anal. 22
(1985) 760-778.
[100] J.E. Dennis, Jr. and H. Wolkowicz, Sizing and least change secant meth-
ods, Research Report 90-02, Faculty of Mathematics, University of Wa-
terloo, Canada, (1990).
[101] S. Di and W. Sun, Trust region method for conic model to solve uncon-
strained optimization problems, Optimization Methods and Software 6
(1996) 237–263.

658
BIBLIOGRAPHY
[102] G. Di Pillo and L. Grippo, A new class of augmented Lagrangians in
nonlinear programming, SIAM J. Control and Optimization 17 (1979)
618-828.
[103] G. Di Pillo and L. Grippo, An exact penalty function method with
global convergence properties for nonlinear programming problem,
Math. Prog. 36 (1986) 1-18.
[104] G. Di Pillo, L. Grippo and F. Lampariello, A class of algorithms for
the solution of optimization problems with inequalities, CNR Inst. di
Anal. dei Sistemi ed Inf. Report R18, (1981).
[105] L.C.W. Dixon, The choice of step length, a crucial factor in the per-
formance of variable metric method, in: F.A. Lootsma, ed., Numerical
Methods for Nonlinear Optimization, (Academic Press, London, 1972)
149–170.
[106] L.C.W. Dixon, Variable metric algorithms: necessary and suﬃcient
conditions for identical behavior of nonquadratical functions, J. Opti-
mization Theory and Appl. 10 (1972) 34–40.
[107] L.C.W.Dixon, Quasi-Newton family generates identical points, Part I
and Part II, Math. Prog. 2 (1972) 383–387, 3 (1972) 345–358.
[108] L.C.W. Dixon, E. Spedicato and G.P. Szego, eds. Nonlinear Optimiza-
tion Birkhauser, Boston, (1980).
[109] L.C.W. Dixon and G.P. Szeg¨o, Towards Global Optimization, Vol. 1,
Vol. 2, North-Holland, Amsterdam, (1975), (1978).
[110] I.S. Duﬀ, J. Nocedal, and J.K. Reid, The use of linear programming
for the solution of sparse sets of nonlinear equations, SIAM J. Scientiﬁc
and Statistical Computing 8 (1987) 99–108.
[111] M. El-Alem, A Global Convergence Theory for a Class of Trust Re-
gion Algorithms for Constrained Optimization, Ph. D. Thesis, Dept of
Mathematical Sciences, Rice University, Houston, (1988).
[112] M. El-Alem, A global convergence theory for the Celis-Dennis-Tapia
trust region algorithm for constrained optimization, SIAM J. Numerical
Analysis 28 (1991) 266–290.

BIBLIOGRAPHY
659
[113] M. El-Alem, A robust trust region algorithm with nonmonotone
penalty parameter scheme for constrained optimization, SIAM J. Op-
timization 5 (1995) 348–378.
[114] M. El-Hallabi, A global convergence theory for a class of trust region
methods for nonsmooth optimization, Report MASC TR 90-16, Rice
University, USA.
[115] M. El-Hallabi and Tapia, A global convergence theory for arbitrary
norm trust-region methods for nonlinear equations, Report MASC TR
93-43, Rice University, Houston, USA.
[116] M. El-Hallabi and R.A. Tapia, An inexact trust regionfeasible-point
algorithm for nonlinear systems and inequalities, Report MASC TR
95-09, Rice University, Houston, USA.
[117] I.I. Eremin, A generalization of the Motzkin-Agmon relaxation method,
Soviet Math. Doklady 6 (1965) 219-221.
[118] Yu.M. Ermoliev, Method of solution of nonlinear extremal problems,
(in Russian), Kibernetika 2 (1966) 1-17.
[119] D.J. Evans, W. Sun, R.J.B. Sampaio, and J. Yuan, Restricted gener-
alized inverse corresponding to constrained quadratic system, Interna-
tional Journal of Computer Mathematics 62 (1996) 285-296.
[120] F. Facchinei and S. Lucidi, Nonmonotone bundle-type scheme for con-
vex nonsmooth minimization, J. Optimization Theory and Applications
76 (1993) 241-257.
[121] Shu-Cheng Fang and S. Puthenpura, Linear Programming and Exten-
sions, Theory and Algorithms, Prentice Hall, Inc., (1993).
[122] A.V. Fiacco and G.P. McCormick, Nonlinear Programming: Sequential
Unconstrained Mininization Techniques, (John Wiley, New York 1968).
[123] J. Flachs, On the convergence, invariance, and related aspects of a mod-
iﬁcation of Huang’s algorithm, J. Optimization Theory and Methods
37 (1982) 315-341.
[124] J. Flachs, On the generalization of updates for quasi-Newton method,
J. Optimization Theory and Applications 48 (1986) 379-418.

660
BIBLIOGRAPHY
[125] R. Fletcher, A new approach to variable metric algorithms, Computer
J. 13 (1970) 317-322.
[126] R. Fletcher, An exact penalty function for nonlinear programming with
inequalities, Math. Prog. 5 (1973) 129-150.
[127] R. Fletcher, An ideal penalty function for constrained optimization, J.
Inst. Math. Applications 15 (1975) 319-342.
[128] R. Fletcher, Practical Methods of Optimization, Vol. 1, Unconstrained
Optimization, (John Wiley and Sons, Chichester, 1980).
[129] R. Fletcher, Practical Methods of Optimization, Vol. 2, Constrained
Optimization, (John Wiley and Sons, Chichester, 1981).
[130] R. Fletcher, A model algorithm for composite NDO problem, Math.
Prog. Study 17 (1982) 67-76. (1982a)
[131] R. Fletcher, Second order correction for nondiﬀerentiable optimization,
in: G.A. Watson, ed., Numerical Analysis, (Springer-Verlag, Berlin,
1982), 85-115. (1982b).
[132] R. Fletcher, Penalty functions, in: A. Bachem, M. Gr¨otschel and B. Ko-
rte, eds., Mathematical Programming: The State of the Art, (Springer-
Verlag, Berlin, 1983), 87-114.
[133] R. Fletcher, Practical Methods of Optimization (second edition), (John
Wiley and Sons, Chichester, 1987).
[134] R. Fletcher, An optimal positive deﬁnite update for sparse hessian ma-
trices, SIAM Journal on Optimization 5 (1995) 192-218.
[135] R. Fletcher and T.L. Freeman, A modiﬁed Newton method for mini-
mization, J. Optimization Theory and Methods 23 (1977) 357-372.
[136] R. Fletcher, S. Leyﬀer, and Ph.L. Toint, On the global convergence of
a ﬁlter-SQP algorithm, SIAM J. Optimization No.1 (2002) 44-59.
[137] R. Fletcher and M.J.D. Powell, A rapid convergent descent method for
minimization, Computer Journal 6 (1963) 163-168.
[138] R. Fletcher and C.M. Reeves, Function minimization by conjugate gra-
dients, Computer Journal 7 (1964) 149-154.

BIBLIOGRAPHY
661
[139] R. Fletcher and C. Xu, Hybrid methods of nonlinear least squares, IMA
J. of Numerical Analysis 7 (1987).
[140] J.A. Ford and R.A. Ghundhari, On the use of curvature estimates in
quasi-Newton methods, J. Comput. Appl. Math. 35 (1991) 185-196.
[141] M. Fukushima, A descent algorithm for non-smooth convex program-
ming, Mathematical Programming 30 (1984) 163-175.
[142] M. Fukushima, A succesive quadratic programming algorithm with
global and superlinear convergence properties, Mathematical Program-
ming 35 (1986) 253-264.
[143] M. Fukushima and L. Qi, A globally and superlinearly convergent al-
gorithm for nonsmooth convex minimization, SIAM J. Optimization 6
(1996) 1106-1120.
[144] D.M. Gay, Computing optimal local constrained step, SIAM J. Sci.
Stat. Comp. 2 (1981) 186-197.
[145] D.M. Gay, A trust region approach to linearly constrained optimization,
in: D.F. Griﬃths, ed., Lecture Notes in Mathematics 1066: Numerical
Analysis, Springer-Verlag, Berlin, (1984) 72-105.
[146] J.C. Gilbert and J. Nocedal, Global convergence properties of conjugate
gradient methods for optimization, SIAM J. Optimization 2 (1992) 21-
42.
[147] P.E. Gill and W. Murray, Quasi-Newton methods for unconstrained
optimization, J. Inst. Maths. Appli. 9 (1972) 91-108.
[148] P.E. Gill, G.H. Golub, W. Murray, and M.A. Sauders, Methods
for modifying matrix factorizations, Mathematics of Computation 28
(1974) 505-535.
[149] P.E. Gill and W. Murray, Newton-type methods for unconstrained
and linearly constrained optimization, Mathematical Programming 28
(1974) 311-350.
[150] P.E. Gill and W. Murray, Numerically stable methods for quadratic
programming, Math. Prog. 14 (1978) 348-372.

662
BIBLIOGRAPHY
[151] P.E. Gill and W. Murray, Conjugate gradient methods for large-scale
nonlinear optimization, Technical Report SOL 79-15, Department of
Operations Research, Stanford University, Stanford, California, (1979).
[152] P.E. Gill and W. Murray and M.H. Wright, Practical Optimization,
Academic Press, London, (1981).
[153] D. Goldfarb, A family of variable metric methods derived by variation
mean, Mathematics of Computation 23 (1970) 23-26.
[154] D.Goldfarb, Curvilinear path steplength algorithms for minimization
which use directions of negative curvature, Mathematical Programming
18 (1980) 31-40.
[155] D. Goldfarb and A. Idinani, A numerical stable dual method for solving
strictly convex quadratic programs, Math. Prog. 27 (1983) 1-33.
[156] S.M. Goldfeld, R.E. Quandt, and H.F. Trotter, Maximisation by
quadratic hill-climbing, Econometrica 34 (1966) 541-551.
[157] A.A. Goldstein, On steepest descent, SIAM J. Control 3 (1965) 147-
151.
[158] A.A. Goldstein, Constructive Real Analysis, Harper & Row, New York,
(1967).
[159] A.A. Goldstein and J.F. Price, An eﬀective algorithm for minimization,
Numer. Math. 10 (1967) 184-189.
[160] G.H. Golub and C.F.Van Loan, Matrix Computations, The Johns Hop-
kins University Press, Baltimore, 3rd ed., (1996).
[161] N.I.M. Gould, D. Orban, Ph.L. Toint, CUTEr and SifDec: A con-
strained and unconstrained testing environment revisited, ACM Trans-
actions on Mathematical Software 29 (2003) 373-394.
[162] L. Grandinetti, Some investigation in a new algorithm for nonlinear op-
timization based on conic model of objective function, J. Optimization
Theory and Applications 43 (1984) 1-21.
[163] J. Greenstadt, Variations on variable metric methods, Mathematics of
Computation 24 (1970) 1-22.

BIBLIOGRAPHY
663
[164] L. Grippo, F. Lampariello and S. Lucidi, A nonmonotone line search
technique for Newton’s methods, SIAM J. Numer. Anal. 23 (1986) 707-
716.
[165] L. Grippo and S. Lucidi, A globally convergence version of the Polak–
Ribiere conjugate gradient method, Mathematical Programming 78
(1997) 375-391.
[166] J. Hald and K. Madsen, Combined LP and quasi-Newton methods for
minmax, Math. Prog. 20 (1981) 49-62.
[167] D. Han and W. Sun, New decomposition methods for solving variable
inequality problems, Mathematics and Computer Modeling 37 (2003)
408–418.
[168] Q. Han, W. Sun, J. Han and R.J.B. Sampaio, An adaptive conic trust-
region method for unconstrained optimization, Optimization Methods
and Software 20 (2005) 645-663.
[169] S.P. Han, A global convergent method for nonlinear programming, J.
Optimization Theory and Applications 22 (1977) 297-309.
[170] S.P. Han, J.S. Pang and N. Rangaraj, Globally convergent Newton
methods for nonsmooth equations, Mathematics of Operations Re-
search, 17 (1992) 586-607.
[171] X. He and W. Sun, Analysis on Greville’s method, Journal of Nanjing
University, Mathematical Biquarterly 5 (1988) 1-10.
[172] X. He and W. Sun, Introduction to Generalized Inverses of Matrices,
Jiangsu Sci. & Tech. Publishing House, Nanjing, (1991). (in Chinese).
[173] M.R. Hestenes and E. Stiefel, Method of conjugate gradient for solving
linear system, J. Res. Nat. Bur. Stand. 49 (1952) 409-436.
[174] D.M. Himmeblau, Applied Nonlinear Programming, McGraw-Hill,
(1972).
[175] J.B. Hiriart-Urruty and C. Lemar´echal, Convex Analysis and Mini-
mization Algorithms, Springer-Verlag, Berlin, New York, (1993).

664
BIBLIOGRAPHY
[176] W. Hock and K. Schittkowski, Test Examples for Nonlinear Program-
ming Codes, Lecture Notes in Economical and Mathematical Systems
187, Springer-Verlag, Berlin, (1981).
[177] E. H¨opﬁnger, On the solution of the unidimensional local minimization
problem, J. Optimization Theory and Appl. 18 (1976) 425-428.
[178] R. Hooke and T.A. Jeeves, Direct search solution of numerical and
statistical problems, J. ACM 8 (1961) 212-229.
[179] Yuda Hu, Nonlinear Programming, Higher Education Press, Beijing,
(1990). (in Chinese).
[180] H.Y. Huang, Uniﬁed approach to quadratically convergent algorithms
for function minimization, J. Optimization Theory and Appl. 5 (1970)
405-423.
[181] D.H. Jacobson and W. Oxman, An algorithm that minimizes homoge-
neous functions of n variables in n+2 iterations and rapidly minimizes
general functions, J. Math. Anal. Appl. 38 (1972) 533-552.
[182] D.H. Jacobson and L.M. Pels, A modiﬁed homogeneous algorithm for
function minimization, J. Math. Anal. Appl. 46 (1974) 533-541.
[183] F. John, Extremum problem with inequalities as subsidiary conditions,
in: F.D. Friedrichs, et al. (eds.) Studies and Essays, Courant Anniver-
sary Volume (Interscience Publishers, New York, 1948)
[184] N. Karmarkar, A new polynomial-time algorithm for linear program-
ming, Combinatorica, 4 (1984) 374-395.
[185] W. Karush, Minima of functions of several variables with inequalities
as side conditions, Master’s thesis, University of Chicago, Chicago,
Illinois, (1939).
[186] J.E. Kelley, The cutting plane method for solving convex programs, J.
of SIAM 8 (1960) 703-712.
[187] J.E. Kelley, Iterative Methods for Linear and Nonlinear Equations,
SIAM Publications, Philadelphia, Penn., (1995).
[188] L.G. Khachiyan, A polynomial algorithm in linear programming, Soviet
Mathematics Doklady 20 (1979) 191-194.

BIBLIOGRAPHY
665
[189] H.F.H. Khalfan, Topics in quasi-Newton methods for unconstrained
optimization, Ph D thesis, University of Colorado, (1989).
[190] K.C. Kiwiel, Methods of Descent for Nondiﬀerentiable Optimization,
Lecture Notes in Mathematics 1133, Springer-Verlag, Berlin, (1985).
[191] M. Kojima and S. Shindo, Extensions of Newton and quasi-Newton
methods to systems of PC1 equations, J. Oper. Res. Soc. Japan 29
(1986) 352-374.
[192] J. Kowalik and K. Ramakrishnan, A numerically stable optimization
method based on homogeneous function, Math. Prog. 11 (1976) 50-66.
[193] H.W. Kuhn and A.W. Tucker, Nonlinear programming, in: J. Neyman,
ed., Proceedings of the Second Berkeley Symposium on Mathemati-
cal Statistics and Probability (University of California Press, Berkeley,
California, 1951) 481-492.
[194] C.J.L. Lagrange, Essai dune nouvelle methods pour deteminer les max-
ima et les minima, Miscellanea Taurinensia 2 (1760-61) Oeuvres, 1, pp.
356-357, 360.
[195] C. Lemar´echal, Bundle methods in nonsmooth optimization, in: C.
Lemar´echal and R. Miﬄin, eds., Nonsmooth Optimization (Pergamon,
Oxford, 1978) 79-102.
[196] C. Lemar´echal, Nondiﬀerentiable optimization, in: L.C.W. Dixon, E.
Spedicato and G.P. Szego, eds., Nonlinear Optimization (Birkhauser,
Boston, 1980) 149-199.
[197] C.Lemarechal and C.Sagastizabal, Variational metric bundle methods:
From conceptual to implementable forms, Mathematical Programming
B, 76 (1997) 393-410.
[198] C. Lemarechal and C. Sagastizabal, Practical aspects of the Moreau
Yosida relarization: Theoretical preliminaries, SIAM J. Optimization
7 (1997).
[199] K. Levenberg, A method for the solution of certain nonlinear problems
in least squares, Qart. Appl. Math. 2 (1944) 164-166.

666
BIBLIOGRAPHY
[200] D.C. Liu and J. Noced´al, On the limited memory BFGS method for
large scale optimization, Mathematical Programming 45 (1989) 503-
528.
[201] D.G. Luenberger, Linear and Nonlinear Programming (2nd Edition),
(Addison-Wesley, Massachusetts, 1984).
[202] Z.Q. Luo, J.S. Pang and D. Ralph, Mathematical Programs with Equi-
librium Constraints, Cambridge University Press, Cambridge, (1996).
[203] G. McCormick, A modiﬁcation of Armijio’s step-size rule for negative
curvature, Mathematical Programming 13 (1977) 111-115.
[204] G. McCormick, Nonlinear Programming: Theory, Algorithms, and Ap-
plications, (John Wiley and Sons, New York, 1983)
[205] G. McCormick and K. Ritter, Alternative proofs of the convergence
properties of the conjugate gradient method, J. Optimization Theory
and Applications 13 (1974) 497-518.
[206] K. Madsen, An algorithm for the minimax solution of overdetermined
systems of nonlinear equations, J. Inst. Math. Appl. 16 (1975) 1-20.
[207] O.L Mangasarian, Nonlinear Programming, (McGraw-Hill, New York,
1969).
[208] O.L. Mangasarian and S. Fromowitz, The Fritz John necessary opti-
mality conditions in the presence of equality and inequality constraints,
J. Math. Anal. Appl. 17 (1967) 37-47.
[209] N. Maratos, Exact Penalty Function Algorithms for Finite Dimensional
and Control Optimization Problems, Ph. D. thesis, Imperial College
Sci. Tech., University of London, (1978).
[210] D.W. Marquardt, An algorithm for least-squares estimation of nonlin-
ear inequalities, SIAM J. Appl. Math. 11 (1963) 431-441.
[211] J.M. Martinez, Quasi-Newton methods with factorization scaling for
solving sparse nonlinear systems of equations, Computing 38 (1987)
133-144.

BIBLIOGRAPHY
667
[212] J.M. Martinez and A.C. Moretti, A trust region method for minimiza-
tion of nonsmooth functions with linear constraints, Mathematical Pro-
gramming 76 (1997) 431-449.
[213] J.M. Martinez and S.A. Santos, A trust-region strategy for minimiza-
tion on arbitrary domains, Mathematical Programming 68 (1995) 267-
301.
[214] D.Q. Mayne and E. Polak, A superlinearly convergent algorithm for
constrained optimization problems, Math. Prog. Study 16 (1982) 45-
61.
[215] R.R. Meyer, Theoretical and computational aspects of nonlinear re-
gression, in: J. Rosen, O. Mangasarian and K. Ritter eds., Nonlinear
Programming, (Academic Press, London, 1970), 465-486.
[216] R. Miﬄin, An algorithm for constrained optimization with semismooth
functions, Math. Oper.Research 2 (1977) 197-207.
[217] R. Miﬄin, Semismooth and semiconvex function in constrained opti-
mization, SIAM J. Control and Optimization 15 (1977) 957-972.
[218] J.J. Mor´e, The Levenberg-Marquardt algorithm: implementation and
theory, in: G.A. Watson, ed., Lecture Notes in Mathematics 630: Nu-
merical Analysis (Springer-Verlag, Berlin, 1978) 105-116.
[219] J.J. Mor´e, Recent developments in algorithms and software for trust
region methods, in: A. Bachem, M. Gr¨otschel and B. Korte, eds., Math-
ematical Programming: The State of the Art (Springer-Verlag, Berlin,
1983) 258-287.
[220] J.J. Mor´e, B.S. Garbow and K.E. Hilstrom, Testing unconstrained op-
timization software, ACM Transactions on Mathematical Software 7
(1983) 17-41; 9 (1983) 503-524.
[221] J.J. Mor´e and D.C.Sorensen, On the use of directions of negative curva-
ture in adiﬁed Newton method, Mathematical Programming 16 (1979)
1-20.
[222] J.J. Mor´e and D.C. Sorensen, Computing a trust region step, SIAM J.
Sci. Stat. Comp. 4 (1983) 553-572.

668
BIBLIOGRAPHY
[223] T. Motzkin and I.J. Schoenberg, The relaxation method for linear in-
equalities, Candian J. Math. 6 (1954) 393-404.
[224] W. Murray and M.L. Overton, A projected Lagrangian algorithm for
nonlinear minimax optimization, SIAM J. Sci. Stat. Comp. 1 (1980)
345-370.
[225] W. Murray and M.L. Overton, A projected Lagrangian algorithm for
nonlinear L1 optimization, SIAM J. Sci. Stat. Comp. 2 (1981) 207-224.
[226] B.A. Murtagh, and R.H.W. Sargent, A constrained minimization
method with quadratic convergence, in: R. Fletcher, ed., Optimiza-
tion (Academic Press, London, 1969) 215-346.
[227] S.G. Nash, Preconditioning of truncated-Newton methods, SIAM
J.Scientiﬁc Statistics and Computing 6 (1985) 599-616.
[228] S.G. Nash and J. Nocedal, A numerical study of the limited memory
BFGS method and truncated-Newton method for large-scale optimiza-
tion, SIAM J. Optimization 1 (1991) 358-372.
[229] S.G. Nash, A survey of truncated-Newton methods, Journal of Com-
putational and Applied Mathematics 124 (2000) 45-59.
[230] L. Nazareth, A relationship between the BFGS and conjugate gradient
algorithms and its implications for new algorithms, SIAM J. Numer.
Anal. 16 (1979) 794-800.
[231] L. Nazareth, Some recent approaches to solving large residual nonlinear
least squares problems, SIAM Review 22 (1980) 1-11.
[232] J. Nocedal and M.L. Overton, Projected Hessian update algorithms for
nonlinear constrained optimization, SIAM J. Numer. Anal. 22 (1985)
821-850.
[233] J. Nocedal and S.J. Wright, Numerical Optimization, Springer, New
York, (1999).
[234] J. Nocedal and Y. Yuan, Analysis of a self-scaling quasi-Newton
method, Math. Prog. 61 (1993) 19-37.
[235] E.A. Nurminski, (ed.) Progress on Nondiﬀerentiable Optimization
(IIASA, Laxenburg, 1982)

BIBLIOGRAPHY
669
[236] .O. Omojokun, Trust Region Algorithms for Optimization with Non-
linear Equality and Inequality Constraints, Ph. D. Thesis, University
of Colorado at Boulder, (1989).
[237] S.S. Oren, Self-scaling variable metric algorithm for unconstrained min-
imization, Ph.D. Dissertation, Computer Science Department, Stanford
University, USA, (1972).
[238] S.S. Oren, Self-scaling variable metric algorithm without line-search for
unconstrained minimization, Mathematics of Computation 27 (1973)
873-885.
[239] S.S. Oren, On the selection of parameters in self-scaling variable metric
algorithms, Mathematical Programming 7 (1974) 351-367.
[240] S.S. Oren, Self-scaling variable metric algorithm II: Implementation
and experiments, Management Science 20 (1974) 863-874.
[241] S.S. Oren, Perspectives on self-scaling variable metric algorithms, J.
Optimization Theory and Methods 37 (1982) 137-147.
[242] S.S. Oren, Planar quasi-Newton algorithm for unconstrained saddle
point problems, J. Optimization Theory and Methods 43 (1984) 167-
204.
[243] S.S. Oren and D.G. Luenberber, Self-scaling variable metric (SSVM)
algorithm I: Criteria and suﬃcient conditions for scaling a class of al-
gorithms, Management Science 20 (1974) 845-862.
[244] S.S. Oren and E. Spedicato, Optimal conditioning of self-scaling vari-
able metric algorithm, Math. Prog. 10 (1976) 70-90.
[245] J.M. Ortega and W.C. Rheinboldt, Iterative Solution of Nonlinear
Equations in Several Variables, (Academic Press, New York, 1970).
[246] M.L. Overton, Algorithms for nonlinear l1 and l∞ﬁtting, in: M.J.D.
Powell, ed., Nonlinear Optimization 1981 (Academic Press, London,
1982).
[247] J.S. Pang, Newton’s method for B-diﬀerentiable equations, Mathemat-
ics of Operations Research 15 (1990) 311-341.

670
BIBLIOGRAPHY
[248] J.S. Pang, S.H. Han and N. Rangaraj, Minimization of locally Lips-
chitzian functions, SIAM J. Optimization 1 (1991) 57-82.
[249] J.S, Pang, A B-diﬀerentiable equation based, globally and locally
quadratic convergent algorithm for nonlinear programs, complemen-
tarity and variable inequality problems, Mathematical Programming
51 (1991) 101-131.
[250] J.S. Pang and L. Qi, Nonsmooth equations: Motivation and Algo-
rithms, SIAM J. Optimization
[251] J.S. Pang and L. Qi, A globally convergent Newton method for convex
SC1 minimization problems, J. Optimization Theory and Applications
85 (1995) 633-648.
[252] J.D. Pearson, Variable metric methods of minimization, The Computer
J. 12 (1969) 171-178.
[253] E. Polak and G. Ribi`ere, Note sur la convergence de directions con-
jug´ees, Rev. Francaise Informat. Recherche Operationelle, 3e ann´ee 16
(1969) 35-43.
[254] B.T. Polyak, A general method of solving extremal problems, Soviet
Math. Doklady 8 (1967) 14-29.
[255] B.T. Polyak, The conjugate gradient method in extremum problems,
USSR Comp. Math. and Math. Phys. 9 (1969) 94-112.
[256] B.T. Polyak, Subgradient methods: A survey of Soviet research, in: C.
Lemarechal and R. Miﬄin, eds., Nonsmooth Optimization (Pergamon,
Oxford, 1978) 5-30.
[257] M.J.D. Powell, An eﬃcient method for ﬁnding the minum of a function
of several variables without calculating derivatives, The Computer J. 7
(1964) 155-162.
[258] M.J.D. Powell, On the calculation of orthogonal vectors, Computer J.
11 (1968) 302-304.
[259] M.J.D. Powell, A theory on rank one modiﬁcations to a matrix and its
inverse, The Computer J. 12 (1969) 288-290.

BIBLIOGRAPHY
671
[260] M.J.D. Powell, A new algorithm for unconstrained optimization, in:
J.B. Rosen, O.L. Mangasarian and K. Ritter, eds., Nonlinear Program-
ming (Academic Press, New York, 1970) 31-66.
[261] M.J.D. Powell, A hybird method for nonlinear equations, in: P. Robi-
nowitz, ed., Numerical Methods for Nonlinear Algebraic Equations
(Gordon and Breach Science, London, 1970) 87-144.
[262] M.J.D. Powell, On the convergence of the variable metric algorithm, J.
Inst. Maths. Appl. 7 (1971) 21-36.
[263] M.J.D. Powell, Quadratic termination properties of minimization algo-
rithms, Part I and Part II, J. Inst. Maths. Appl. 10 (1972) 332-357.
[264] M.J.D. Powell, Convergence properties of a class of minimization algo-
rithms, in: O.L. Mangasarian, R.R. Meyer and S.M. Robinson, eds.,
Nonlinear Programming 2 (Academic Press, New York, 1975) 1-27.
[265] M.J.D. Powell, Some global convergence properties of a variable metric
algorithm for minimization without exact line searches, in: R.W. Cottle
and C.E. Lemke, eds., Noninear Programming, SIAM-AMS Proceed-
ings vol. IX (SIAM publications, Philadelphia, 1976) 53-72. (1976a).
[266] M.J.D. Powell, Some convergence properties of the conjugate gradient
method, Math. Prog. 11 (1976) 42-49. (1976b)
[267] M.J.D. Powell, Restart procedure for the conjugate gradient method
Mathematical Programming 12 (1977) 241-254.
[268] M.J.D. Powell, A fast algorithm for nonlinearly constrained optimiza-
tion calculations, in: G.A. Watson, ed., Numerical Analysis (Springer-
Verlag, Berlin, 1978) 144-157.
[269] M.J.D. Powell, VMCWD: A FORTRAN subroutine for constrained
optimization, DAMTP Report 1982/NA4, University of Cambridge,
England (1982).
[270] M.J.D. Powell, Nonconvex minimization calculations and the conjugate
gradient method, in: D.F. Griﬃths, ed., Numerical Analysis Lecture
Notes in Mathematics 1066 (Springer-Verlag, Berlin, 1984) pp. 122-141.

672
BIBLIOGRAPHY
[271] M.J.D. Powell, On the rate of convergence of variable metric algorithms
for unconstrained optimization, in: Z. Ciesielki and C. Olech, eds.,
Proceeding of the International Congress of Mathematicians (Elsevier,
New York, 1984) 1525-1539.
[272] M.J.D. Powell, General algorithms for discrete nonlinear approxima-
tion calculations, in: L.L. Schumacher, ed., Approximation Theory IV
(Academic Press, New York, 1984) 187-218.
[273] M.J.D. Powell, On the global convergence of trust region algorithms
for unconstrained optimization, Math. Prog. 29 (1984) 297-303.
[274] M.J.D. Powell, On the quadratic programming algorithm of Goldfarb
and Idnani, Math. Prog. Study 25 (1985) 46-61.
[275] M.J.D. Powell, Updating conjugate directions by the BFGS formular,
Mathematical Programming 38 (1987) 29-46.
[276] M.J.D. Powell and Ph.L. Toint, On the estimation of sparse Hessian
matrices, SIAM J. Numer. Anal. 16 (1979) 1060-1074.
[277] M.J.D. Powell and Y. Yuan, A recursive quadratic programming al-
gorithm that uses diﬀerentiable exact penalty function, Mathematical
Programming 35 (1986) 265-278.
[278] M.J.D. Powell and Y. Yuan, A trust region algorithm for equality con-
strained optimization, Mathematical Programming 49 (1991) 189-211.
[279] L. Qi, Convergence analysis of some algorithms for solving nonsmoth
equations, Mathematics of Operations Research 18 (1993) 227-244.
[280] L. Qi, Trust region algorithms for solving nonsmooth equations, SIAM
J. Optimization 5 (1995) 219-230.
[281] L. Qi and X. Chen, A globally convergent successive approximation
method for severely nonsmooth equations, SIAM J. Control and Opti-
mization 33 (1995) 402-418.
[282] L. Qi and D. Sun, A survey of some nonsmooth equations and smooth-
ing Newton methods, in: A. Eberhard, R. Hill, D. Ralph and B.M.
Glover eds., Progress in Optimization, Kluwer Academic, Dordrecht,
(1999), 121-146.

BIBLIOGRAPHY
673
[283] L. Qi and J. Sun, A nonsmooth version of Newton’s method, Mathe-
matical Programming 58 (1993) 353-367.
[284] L. Qi and W. Sun, An iterative method for the minimax problem, in
D.Z. Du and P.M. Pardalos eds., Minimax and Applications, Kluwer
Academic Publisher, Boston, (1995), 55-67.
[285] D. Ralph, Global convergence of damped Newton’s method for nons-
mooth equations, via the path search, Mathematics of Operations Re-
search.
[286] F. Rendle and H. Wolkowicz, A semideﬁnite framework for trust region
subproblems with applications to large scale minimization, Mathemat-
ical Programming 77 (1997) 273-299.
[287] K. Ritter, On the rate of superlinear convergence of a class of variable
metric methods, Numerische Mathematik 35 (1980) 293-313.
[288] R.T. Rockafellar, Convex Analysis (Princeton University Press, Prince-
ton, 1970).
[289] R.T. Rockafellar, Augmented Lagrangians and applications of the prox-
imal point algorithm in convex programming, Mathematics of Opera-
tions Research 1 (1976) 97-116. (1976a).
[290] R.T. Rockafellar, Monotone operators and the proximal point algo-
rithm. SIAM J. Control and Optimization 14 (1976) 877-898.
[291] R.T. Rockafellar, The Theory of Subgradient and Its Application
to Problems of Optimization:
Convex and Not Convex Functions
(Heldermann-Verlag, West Berlin, 1981).
[292] R.T. Rockafellar, Computational scheme for solving large-scale prob-
lems in extended linear-quadratic programming, Mathematical Pro-
gramming 48 (1990) 447-474.
[293] R.T. Rockafellar and R.J.B. Wets, Variational Analysis, Springer-
Verlag, Berlin, (1998).
[294] J.B. Rosen, The gradient projection method for nonlinear program-
ming, Part 1: Linear constraints, J. SIAM 8 (1960) 181-217.

674
BIBLIOGRAPHY
[295] J.B. Rosen, The gradient projection method for nonlinear program-
ming, Part 2: Nonlinear constraints, J. SIAM 9 (1961) 514-532.
[296] R.J.B. Sampaio, W. Sun, and J. Yuan, On the trust region algorithm
for nonsmooth optimization, Applied Mathematics and Computation
85 (1997) 109-116.
[297] K. Schittkowski, The nonlinear programming method of Wilson, Han
and Powell with an augmented Lagragian type line search function,
Part 1: convergence analysis, Numerische Mathematik 38 (1981) 83-
114.
[298] K. Schittkowski, More test examples for nonlinear programming codes,
Lecture Notes in Economics and Mathematical System 282, Springer-
Verlag, Berlin, (1987).
[299] R.B. Schnabel, Analysing and improving quasi-Newton methods for
unconstrained optimization, PhD thesis, Department of Computer Sci-
ence, Cornell University, Ithaca, NY, (1977).
[300] R.B. Schnabel, Conic methods for unconstrained optimization and ten-
sor methods for nonlinear equations, in: A. Bachem, M. Grotschel
and B. Korte eds., Mathematical Programming, The State of the Art
(Springer-Verlag, Berlin, 1983) 417-438.
[301] R.B. Schnabel and Ta-Tung Chow, Tensor methods for unconstrained
optimization using second derivatives, SIAM J. Optimization 1 (1991)
293-315.
[302] R.B. Schnabel and P.D. Frank, Tensor methods for nonlinear equations,
SIAM J.Numer.Anal. 21 (1984) 815-843.
[303] L.K. Schubert, Modiﬁcation of a quasi-Newton method for nonlinear
equations with sparse Jacobian, Mathematics of Computation 24 (1970)
27-30.
[304] D.F. Shanno, Conditioning of quasi-Newton methods for function min-
imization, Math. Comput. 24 (1970) 647-656.
[305] D.F. Shanno, Conjugate gradient methods with inexact searches, Math.
Oper. Res. 3 (1978) 244-256.

BIBLIOGRAPHY
675
[306] D.F. Shanno and K.H. Phua, Matrix conditioning and nonlinear opti-
mization, Mathematical Programming 14 (1978) 149-160.
[307] D.F. Shanno and K.H. Phua, Remark on Algorithm 500: Minimization
of unconstrained multivariate functions, ACM Transactions on Mathe-
matical Software 6 (1980) 618-622.
[308] S. Sheng, A class of collinear scaling algorithm for unconstrained opti-
mization, Numerical Mathematics, A Journal of Chinese Universities 6
(1997) 219-230.
[309] N.Z. Shor, An application of the method of gradient descent to the
solution of the network transpotation problems (in Russian), in: Notes
Scientiﬁc Seminar on Theory and Application of Cybernetics and Op-
erations Research (Acdemy of Science, Ukrain, SSSR, 1 (1962) 9-17.
[310] N.Z. Shor, On the speed of convergence of the generalized gradient,
Kibernetika 3 (1968) 98-99.
[311] N.Z. Shor, An application of the operation of space dilation to the
problems of minimizing convex functions, Kibernetika 1 (1970) 6-12.
[312] N.Z. Shor, Generalized gradient methods of nondiﬀerentiable optimiza-
tion emplying space dilation operations, in: A. Bachem, M. Grotchel,
and B. Korte, eds., Mathematical Programming, The State of the Art,
Springer-Verlag, Berlin, (1983), 501-529.
[313] N.Z. Shor, Minimization Methods for Non-diﬀerentiable Functions,
Springer-Verlag, Berlin, (1985).
[314] G.A. Shultz, R.B. Schnable and R.H. Byrd, A family of trust region
based algorithms for unconstrained minimization with strong global
convergence properties, SIAM J. Numerical Analysis 22 (1985) 47-67.
[315] D.C. Sorensen, The q-superlinear convergence of a collinear scaling
algorithm for unconstrained optimization, SIAM J.Numer.Anal. 17
(1980) 84-114.
[316] D.C. Sorensen, Newton’s method with a model trust region modiﬁca-
tion, SIAM J. Numer. Anal. 20 (1982) 409-426.

676
BIBLIOGRAPHY
[317] D.C. Sorensen, Trust region methods for unconstrained optimization,
in: M.J.D. Powell, ed., Nonlinear Optimization 1981 (Academic Press,
London, 1982) 29-38. (1982b).
[318] E. Spedicato, A variable metric method for function minimization de-
rived from invariancy to nonlinear scaling, J. Optimization Theory and
its Applications (1976).
[319] E. Spedicato, A note on the determination of the scaling parameters
in a class of quasi-Newton methods for unconstrained optimization, J.
Inst. Maths. Applics. 21 (1978) 285-291.
[320] E. Spedicato and Zunquan Xia, Finding general solutions of the quasi-
Newton equation via the ABS approach, Optimization Methods and
Software 1 (1992) 243-252.
[321] T. Steihaug, Quasi-Newton methods for large scale optimization, Ph.
D. Dissertation, SOM Technical Report No.49, Yale University. (1980).
[322] T. Steihaug, The conjugate gradient and trust regions in large scale
optimization, SIAM Journal on Numerical Analysis 20 (1983) 626-637.
[323] G.W. Stewart, A modiﬁcation of Davidon’s method to accept diﬀerence
approximation of derivatives, J. ACM 14 (1967) 72-83.
[324] J. Stoer, On the convergence rate of imperfect minimization algorithms
in broyden’s β class”, Math. Prog., 9 (1975) 313-335.
[325] J. Stoer, On the relation between quadratic termination and con-
vergence properties of minimization algorithms, Part I: Theory, Nu-
merische Mathematik 28 (1977) 343-366.
[326] J. Stoer, Foundations of recusive quadratic programming methods for
solving nonlinear programs, in: K. Schittkowski, ed. Computational
Mathematical Programming (Springer-Verlag, Berlin, 1985) 165-207.
[327] J. Stoer and R. Bulirsch,
Introduction to Numerical Analysis,
(Springer-Verlag, New York, 1993).
[328] J. Stoer, High order long-step methods for solving linear complemen-
tarity problems, Annals of Operations Research 103 (2001) 149-159.

BIBLIOGRAPHY
677
[329] J. Stoer, M. Wechs and S. Mizuno, High order infeasible-interior-point
methods for solving suﬃcient linear complementarity problems, Math-
ematics of Operations Research 23 (1998) 832-862.
[330] W. Sun and X. Chang, An unconstrained minimization method based
on homogeneous functions, Journal of Applied Mathematics & Com-
putational Mathematics 3 (1989) 81-88.
[331] W. Sun and Z. Wu, Numerical research on self-scaling variable metric
algorithm, Numerical Mathematics, A Journal of Chinese Universities
11 (1989) 145-158.
[332] W. Sun, Generalized Newton method for LC1 unconstrained optimiza-
tion, Journal of Computational Mathematics 15 (1995) 502-508.
[333] W. Sun, On nonquadratic model optimization methods, Asia and Pa-
ciﬁc Journal of Operations Research 13 (1996) 43-63.
[334] W. Sun, On convergence of an iterative method for minimax problem,
Journal of Australian Mathematics Society, Series B, 39 (1997) 280-
292.
[335] W. Sun, Newton’s method and quasi-Newton-SQP method for general
LC1 constrained optimization, Applied Mathematics and Computa-
tion, 92 (1998) 69-84.
[336] W. Sun and Y. Wei, Inverse order rule for weighted generalized inverse,
SIAM Matrix Analysis and Applications 19 (1998) 772-775.
[337] W. Sun and Y. Yuan, A conic trust-region method for nonlinearly con-
strained optimization, Annals of Operations Research 103 (2001) 175-
191.
[338] W. Sun, J. Yuan and Y. Yuan, Conic trust-region method for linearly
constrained optimization, Journal of Computational Mathematics 21
(2003) 295-304.
[339] W. Sun, C. Xu and D. Zhu, Optimization Methods, Higher Education
Press, Beijing, (2004). (in Chinese).
[340] R.A. Tapia, Diagonalized multiplier methods and quasi-Newton meth-
ods for constrained optimization, J. Optimization Theory and Appli-
cations 22 (1977) 135-194.

678
BIBLIOGRAPHY
[341] Ph.L. Toint, Towards an eﬃcient sparsity exploiting Newton method
for minimization, in: I.S. Duﬀ, ed., Sparse Matrices and Their Uses
(Academic Press, London, 1981) 57-88.
[342] Ph.L. Toint, Global convergence of a class of trust region methods
for nonconvex minimization in Hilbert space, IMA J. Numer. Anal. 8
(1988) 231-252.
[343] Ph.L. Toint, An assessment of nonmonotone linesearch technique for
unconstrained optimization, SIAM J. Scientiﬁc Computing 17 (1996)
725-739.
[344] L. Vandenberghe and S. Boyd, Semideﬁnite programming, SIAM Re-
view 38 (1996) 49-95.
[345] A. Vardi, A trust region algorithm for equality constrained minimiza-
tion: convergence properties and implementation, SIAM J. Numer.
Anal. 22 (1985) 575-591.
[346] H. Wang and Y. Yuan, An second order convergent method for one-
dimensional optimization, Chinese Journal of Operations Research 11
(1992) 1-10.
[347] G.R. Walsch, An Introduction to Linear Programming, (John Wiley
and Sons, New York, 1985).
[348] G.A. Watson, Methods for best approximation and regression, in: A.
Iserles and M.J.D. Powell eds., The State of the Art in Numerical Anal-
ysis, (Clarendon Press, Oxford, 1987) 139-164.
[349] R.B. Wilson, A simplicial algorithm for concave programming, Ph.D.
thesis, Graduate School of Business administration, Harvard Univer-
sity, (1963).
[350] P. Wolfe, Methods of recent advances in mathematical programming,
in: R.L. Graves and P. Wolfe, eds., Recent Advances in Mathematical
Programming (McGraw-Hill, New York, 1963) 67-86.
[351] P. Wolfe, Another variable metric method, working paper, (1968).
[352] P. Wolfe, Convergence conditions for ascent methods, SIAM Review 11
(1969) 226-235.

BIBLIOGRAPHY
679
[353] P. Wolfe, Convergence conditions for ascent methods, (II): some cor-
rections, SIAM Review 13 (1971) 185-188.
[354] P. Wolfe, A method of conjugate subgradients for minimizing nondif-
ferentiable functions, Math. Prog. Study 3 (1975) 145-173.
[355] H. Wolkowitz, R. Saigal, and L. Vandenberghe, edc., Handbook of
Semideﬁnite Programming:
Theory, Algorithms and Applications,
Kluwer International Series in Operational Research and Management
Science, Kluwer, Boston, (2000).
[356] R. S. Womersley, Local properties of algorithms for minimizing nons-
mooth composite functions, Mathematical Programming 32 (1985) 69-
89.
[357] S.J. Wright, Local properties of inexact methods for minimizing nons-
mooth composite functions, Mathematical Programming 37 (1987) 232-
252.
[358] S.J. Wright, Primal-Dual Interior-Point Methods, SIAM Publications,
Philadelphia, PA, (1997).
[359] S. Xi, Nonlinear Optimization Methods, Higher Education Press, Bei-
jing, (1992). (in Chinese).
[360] S. Xi and F. Zhao, Computational Methods for Optimization, Shanghai
Sci & Tech Press, Shanghai, (1983). (in Chinese).
[361] Y. Xiao and F.J. Zhou, Nonmonotone trust region methods with curvi-
linear path in unconstrained optimization, Computing 48 (1992) 303-
317.
[362] C.X. Xu and J. Zhang, A survey of quasi-Newton equations and quasi-
Newton methods for optimization, Annals of Operations Research 103
(2001) 213-234.
[363] Y. Ye and M.J. Todd, Containing and shrinking ellipsoids in the path-
following algorithm, Math. Porg. 47 (1990) 1-9.
[364] Y. Ye and E. Tse, An extension of Karmarkar’s algorithm to convex
quadratic programming, Math. Prog. 44 (1989) 157-179.

680
BIBLIOGRAPHY
[365] T.J. Ypma, Local convergence of inexact Newton methods, SIAM J.
Numer. Anal. 21 (1984) 583-590.
[366] Y. Yuan, On the least Q-order of convergence of variable metric algo-
rithms, IMA J. Numerical Analysis 4 (1984) 233-239. (1984a).
[367] Y. Yuan, An example of only linearly convergence of trust region algo-
rithms for nonsmooth optimization, IMA J. Numerical Analysis 4(1984)
327-335. (1984b).
[368] Y. Yuan, Conditions for convergence of trust region algorithms for non-
smooth optimization, Mathematical Programming 31 (1985) 220-228.
(1985a).
[369] Y. Yuan, On the superlinear convergence of a trust region algorithm for
nonsmooth optimization, Mathematical Programming 31 (1985) 269-
285. (1985b).
[370] Y. Yuan, An only 2-step Q-superlinear convergence example for some
algorithms that use reduced Hessian approximation, Mathematical Pro-
gramming 32 (1985) 224-231. (1985c).
[371] Y. Yuan, On a subproblem of trust region algorithms for constrained
optimization, Mathematical Programming 47 (1990) 53-63.
[372] Y. Yuan, A modiﬁed BFGS algorithm for unconstrained optimization,
IMA Journal of Numerical Analysis 11 (1991) 325-332.
[373] Y. Yuan, A dual algorithm for minimizing a quadratic function with
two quadratic constraints, Journal of Computational Mathematics 9
(1991) 348-359.
[374] Y.Yuan, On self-dual update formulae in the Broyden family, Opti-
mization Methods and Software 1 (1992) 117-127.
[375] Y. Yuan, NumericaL Methods for Nonlinear Programming, Shanghai
Sci. & Tech Press, Shanghai, (1993).
[376] Y. Yuan and R. Byrd, Non-quasi-Newton updates for unconstrained
optimization, J. Comp. Math. 13 (1995) 95-107.
[377] Y. Yuan, On the truncated conjugate gradient method, Mathematical
Programming 87 (2000) 561-573.

BIBLIOGRAPHY
681
[378] W.I. Zangwill. Non-linear programming via penalty functions, Man-
agement Sci. 13 (1967) 344-358.
[379] J.Z. Zhang, N.Y. Deng, and L.H. Chen, A new quasi-Newton equa-
tion and related methods for unconstrained optimization, Journal of
Optimization Theory and applications 102 (1999) 147-167.
[380] J.Z. Zhang and C.X. Xu, A class of indeﬁnite dogleg path methods for
unconstrained minimization, SIAM J. on Optimization 9 (1994) 646-
667.
[381] Y. Zhang, Computing a Celis-Dennis-Tapia trust region step for equal-
ity constrained optimization, Mathematical Programming 55 (1992)
109-124.
[382] H.C. Zhou and W. Sun, Optimality and duality without a constraint
qualiﬁcation for minimax programming, Bulletin of the Australian
Mathematical Society 67 (2003) 121-130.
[383] H.C. Zhou and W. Sun, Nonmonotone descent algorithm for nonsmooth
unconstrained optimization problems, International Journal of Pure
and Applied Mathematics 9 (2003) 153-163.
[384] M. Zhu, Y. Xue, and F. Zhang, A quasi-Newton type trust region
method based on the conic model, Numerical Mathematics, A Journal
of Chinese Universities No.1 (1995) 36-47.
[385] G. Zoutendijk, Nonlinear programming, computational methods, in: J.
Abadie ed. Integer and Nonlinear Programming, North-Holland, Ams-
terdam, (1970), 37-86.
[386] J. Zowe, Nondiﬀerentiable optimization — a motivation and a short
introduction into the subgradient and the bundle concept, in: K. Schit-
tkowski, ed., Computational Mathematical Programming (Springer-
Verlag, Berlin, 1985) 321-356.

Index
Accumulation point: see Limit point,
23, 62, 76, 116, 120, 121,
151, 191, 193, 368, 463, 464,
478, 486, 520, 524, 545, 564,
621, 625
Active constraint, 387, 489, 582
strong, 393
weak, 393
Active set, 387, 394, 538
Active set method, 427, 428, 431,
433, 435
Actual reduction, 563
Approximate Newton’s method, 513
Armijo line search, 103, 509, 513
Augmented Lagrangian function, 460,
474, 480
Average Hessian, 214
Barrier function, 467
Barzilai-Borwein gradient method,
127
BFGS method, 217, 381, 536, 556
Bunch-Parlett factorization, 152, 163
Bundle method, 617, 619
Cauchy point, 316, 570
Cauchy sequence, 7, 9, 50, 51
Cauchy-Schwarz inequality, 7, 119,
212
Cholesky factorization, 14, 19–21,
136, 138, 148, 150, 373
Clarke directional derivative, 598
Collinear scaling, 325
Collinear scaling algorithm, 324
Collinear scaling BFGS algorithm,
334
Complementarity condition, 393
Composite nonsmooth optimization,
620, 623
Concave function, 36
Cone, 35
Conic model, 324, 325, 329
Conic model algorithm, 324
Conic trust-region method, 336
Conjugate direction, 175
Conjugate direction method, 176,
177
Conjugate gradient method, 1, 175,
178, 180
Beale, 186
convergence, 191, 193, 200
Crowder-Wolfe formula, 180
Dai-Yuan formula, 180
Dixon formula, 180
Fletcher-Reeves formula, 180
Hestenes-Stiefel formula, 180
Polak-Ribi`ere-Polyak formula,
180
preconditioned, 189
restart, 183
Conjugate subgradient method, 617

INDEX
683
Constraint qualiﬁcation (CQ), 391–
393, 401, 403
linear function (LFCQ), 394
linear independence (LICQ), 394,
396
Constraint violation function, 455,
462, 476, 482
Convergence, 524
Convex combination, 32
Convex cone, 35
Convex function, 31, 36, 62, 114,
134, 472, 482, 496, 621
geometry, 43
property, 40, 41, 43, 44
Convex hull, 34
Convex programming, 39, 58, 615
Convex set, 25–27, 31–34, 36, 37,
46, 47, 134, 358
separation and support, 50, 52,
54, 56
Convexity, 9, 38, 46, 377
Cutting plane method, 615, 616
Descent direction, 58, 64, 119, 131,
148, 156, 493
Descent pair, 156, 160
DFP method, 210, 211, 215
Dini directional derivative, 598
Directional derivative, 24
second order, 24
Dual method, 438
Dual problem, 417, 435
Duality, 406
Eigen-pair, 13
Eigenvalue, 4, 6, 12, 14, 17, 18,
181, 188, 366, 369, 371
Eigenvector, 12, 14
Epigraph, 37, 40
Exact penalty function, 570
Farkas Lemma, 53, 391, 401
Feasible descent direction, 493, 496,
502
Feasible direction, 388, 493, 496
linearized, 388
sequential, 388
Feasible direction method, 509, 515
Feasible point, 386, 456, 464
Feasible point Armijo step, 493, 495
Feasible point method, 493, 563
Feasible region, 2, 34, 457, 467, 469,
473, 474, 476, 485
Feasible set, 386, 493
Feasible steepest descent direction,
499
Finite-diﬀerence Newton’s method,
140, 146
First-order optimality condition, 59,
388, 391
Fr´echet derivative, 29
Strong, 631
Fritz John optimality condition, 397
Frobenius norm, 291, 380
Gateaux derivative, 29
Gauss-Newton equation, 356, 361
Gauss-Newton method, 355, 356,
359, 360, 363
Generalized elimination method, 422,
506
Generalized inverse, 9
Generalized Jacobian, 628
Generalized quasi-Newton equation,
326
Generalized reduced gradient method
(GRG method), 509
Gerschgorin circle, 17

684
INDEX
Global convergence, 369, 532, 552,
579, 593
Global minimizer, 57, 58, 62, 63,
134
Goldstein line search, 103
Gradient method, 119
Graph, 37
H¨older inequality, 8
Hypograph, 37
Inactive constraint, 387, 393
Indeﬁnite factorization, 152, 163
Indicator function, 40
Inexact Log-barrier function method,
473
Inexact Newton’s method, 163, 164,
169
Interior ellipsoid method, 443
Interior point, 473
Inverse barrier function, 457
Karmarkar’s algorithm, 441
Karush-Kuhn-Tucker conditions: see
KKT conditions, 393
Karush-Kuhn-Tucker Theorem, 391
KKT conditions, 393, 546
KKT matrix, 426
KKT point, 393, 401, 402, 460, 520,
526, 532, 564, 566, 576
Krylov subspace, 182
Lagrange multiplier, 391, 393, 460,
482, 523, 537
Lagrange-Newton method, 524, 554
Lagrangian dual problem, 406
Lagrangian function, 391, 398, 403,
416, 503, 523, 554
Least-squares problem, 353, 360, 373,
381
Level set, 47
Levenberg-Marquardt method, 362,
366
convergence, 367, 369
implementation, 372
Limit point, 63, 134
Limited memory BFGS method, 292
Line search, 71, 127, 133, 140, 150,
176–178, 200, 360, 362
Armijo rule: see Armijo line
search, 103
backtracking, 108
exact, 71, 75, 81, 120, 180, 184,
191, 192
Goldstein rule: see Goldstein
line search, 103
inexact, 72, 102, 109, 114, 121,
183, 185, 195
interpolation, 89
nonmonotone: see Nonmono-
tone line search, 115, 127
second order, 157, 160
Wolfe rule: see Wolfe rule or
Wolfe-Powell rule, 104
Linear convergence, 81
Linear programming, 34, 407, 616
Linearized feasible direction method,
515
Lipschitz condition, 597
Lipschitz continuous, 25–27, 112,
132, 143, 169, 210, 241, 262,
337, 357, 358
Lipschitzian function, 604
Local minimizer, 57–60, 357, 371,
488, 622
Logarithmic barrier function, 458

INDEX
685
Lower hemi-continuous, 26
Lower semi-continuous, 39
Maratos Eﬀect, 541, 550
Memoryless BFGS formula, 301
Merit function, 543, 550, 572
Minkowski inequality, 48
Modiﬁed Newton’s method, 136, 140,
147
Monotone mapping, 44
Negative curvature direction, 147
Newton point, 316
Newton’s method, 130, 131
Newton-Raphson step, 523
Nondiﬀerentiable function, 597
Nonmonotone line search, 116
Nonsmooth exact penalty method,
484
Nonsmooth function, 609
Nonsmooth Newton’s method, 628,
631
global convergence, 632
Nonsmooth optimization, 597, 608,
610
Norm, 3, 37
l1-norm, 3
l2-norm, 3
lp-norm, 3
l∞-norm, 3
consistency, 6
equivalence, 6
Frobenius norm, 4, 18
inequalities, 7
matrix norm, 4
orthogonally invariant matrix
norm, 5
vector norm, 3
weighted, 5
Null space, 538, 555, 562
Null space method, 571
Null space step, 573
Objective function, 1, 26, 62
Optimality condition, 59, 412, 620
Penalty function, 455, 456, 524
L1 exact penalty function, 484,
531, 572
L1 penalty function, 457
L∞exact penalty function, 484
L∞penalty function, 457
Courant penalty function, 456
Fletcher’s smooth exact penalty
function, 459
interior point penalty function,
457, 466
multiplier penalty function: see
Augmented Lagrangian func-
tion, 474
nonsmooth exact penalty func-
tion, 482
quadratic penalty function, 456
simple penalty function, 461
smooth exact penalty function,
480
Penalty function method, 455, 461,
462
Penalty parameter, 457
Powell-Yuan’s method, 551
Preconditioned, 190
Predicted reduction, 563
Primal problem, 407, 408
Primal-dual interior-point method,
445
central path, 447
Projected gradient method, 513, 516
Projection, 513

686
INDEX
Projection theorem, 50
PSB method, 219
Q-convergence, 65
QR factorization, 21, 361, 554
Quadratic convergence, 65, 100, 144,
146, 147, 169
Quadratic model, 26, 130, 163
Quadratic programming (QP), 34,
408, 411, 413, 419, 428, 441,
563
equality-constrained, 419, 425
necessary and suﬃcient condi-
tions, 412
Quadratic termination, 177
Quasi-Newton equation, 205, 220,
380
Quasi-Newton method, 204, 381,
535
BFGS: see BFGS method, 217
Broyden class, 226, 229
DFP: see DFP method, 211
global convergence, 231, 238
Huang class, 231
least change update, 223
local convergence, 240
PSB: see PSB method, 219
SR1: see Symmetric rank-one
update (SR1), 207
R-convergence, 66
Rademacher’s theorem, 628
Range space, 555
Range space step, 573
Rank-one tensor, 339
Rank-one update, 17
Rayleigh quotient, 15
Reduced gradient, 504
Reduced Hessian matrix, 554
Reduced Hessian matrix method,
554
one-side, 556
two-side, 557
Sawtooth phenomenon, 515
Second-order Armijo rule, 157
Second-order correction step, 545
Second-order optimality condition,
60, 61, 401
Second-order Wolfe-Powell rule, 160
Self-scaling variable metric method
(SSVM), 277
Semismooth, 629
Semismooth function, 630
p-order, 631
Separation theorem, 55, 56
Sequential quadratic programming
method (SQP), 530, 532
superlinear convergence, 537
Sherman-Morrison formula, 17, 217
Sherman-Morrison-Woodburg for-
mula, 17
Singular value, 13
Singular value decomposition (SVD),
11
Sparse PSB update, 286
Sparse quasi-Newton method, 282
Spectral radius, 13
Stationary point, 62, 120, 121, 134,
148, 151, 160, 191, 193, 212,
368, 621, 625
Clarke stationary point, 604
Dini stationary point, 604
Strong duality theorem, 406
Subdiﬀerential, 604
Subgradient method, 609
Superlinear convergence, 65, 93, 127,

INDEX
687
144, 165, 168, 169, 336, 529,
537, 553, 556, 594
Superlinearly convergent step, 538,
550, 552
Support function, 37, 40, 601, 604
Supporting hyperplane, 43, 54
Symmetric rank-one update (SR1),
208, 210
Tensor method, 337, 338
nonlinear equations, 337
optimization, 341, 348
Trust-region method, 304, 363, 380,
561, 563, 624
CDT subproblem, 580
dogleg method, 316
double dogleg method, 318
null space technique, 574
Powell-Yuan algorithm, 585
Steihaug-CG method, 320
Steihaug-Toint method, 320
Trust-region subproblem, 372, 562,
563, 568, 623
Upper hemi-continuous, 26
Variable elimination method, 420,
505
Variable metric method: see Quasi-
Newton method, 207
Von-Neumann Lemma, 9
Watchdog technique, 543, 544
Weak duality theorem, 408
Weighted Frobenius norm, 291
Wilson-Han-Powell method, 530
Wolfe rule or Wolfe-Powell rule, 104
Zigzagging, 514
Zoutendijk condition, 112


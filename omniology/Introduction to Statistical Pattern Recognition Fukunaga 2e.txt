% 
Introduction to Statistical 
Pattern 
Recogni-tion 
% 
Second Edition 
0 
0 
0 
0 
0 
n 

Keinosuke Fukunaga 
Introduction to 
Stas-tical 
Pattern 
Recognit ion 
Second Edition 
This completely revised second edition 
presents an introduction to statistical pat- 
tern recognition. Pattern recognition in 
general covers a wide range of problems: it 
is applied to engineering problems, such as 
character readers and wave form analysis, 
as well as to brain modeling in biology and 
psychology. Statistical decision and estima- 
tion, which are the main subjects of this 
book, are regarded as fimdamental to the 
study of pattern recognition. This book 
is appropriate as a text for introductory 
courses in pattern recognition and as a ref- 
erence book for people who work in the 
field. Each chapter also contains computer 
projects as well as exercises. 



Introduction to Statistical 
Pattern Recognition 
Second Edition 

This is a volume in 
COMPUTER SCIENCE AND SCIENTIFIC COMPUTING 
Editor: WERNER RHEINBOLDT 

Introduction to Statistical 
Pattern Recognition 
Second Edition 
Keinosuke F'ukunaga 
School of Electrical Engineering 
Purdue University 
West Lafa yet te, Indiana 
M K4 
Morgan K.ufmu\n is an imprint of Academic Rars 
A H a m r t  SaenccandTechlndogyCompony 
San Diego San Francisco New York Boston 
London Sydney Tokyo 

This book is printed on acid-free paper. 8 
Copyright 0 1990 by Academic Press 
All rights reserved. 
No part of this publication may be reproduced or 
transmitted in any form or by any means, electronic 
or mechanical, including photocopy, recording, or 
any information storage and retrieval system. without 
permission in writing from the publisher. 
ACADEMIC PRESS 
A Harcourt Science and Technology Company 
525 B Street, Suite 1900, San Diego, CA 92101-4495 
USA 
h ttp://www.academicpress.com 
Academic Press 
24-28 Oval Road, London N W 1  7DX United Kingdom 
h tt p:/lwww. hbuWap/ 
Morgan Kaufmann 
340 Pine Street, Sixth Floor, San Francisco, CA 94104-3205 
http://mkp.com 
Library of Congress Cataloging-in-Publication Data 
Fukunaga. Keinosuke. 
Fukunaga. - 2nd ed. 
Introduction to statistical pattern recognition I Keinosuke 
p, cm. 
Includes bibliographical references. 
1. Pattern perception - Statistical methods. 2. Decision-making - 
ISBN 0-12-269851-7 
- Mathematical models. 3. Mathematical statistics. I. Title. 
0327.F85 1990 
006.4 - dc20 
89-18195 
CIP 
PRINTF.D M THE UNITED STATES OF AMEIUCA 
03 
9 

To Reiko, Gen, and Nina 


Contents 
Preface ............................................. 
xi 
Acknowledgments .................................. 
x m  
Chapter 1 Introduction 
1 
Formulation of Pattern Recognition Problems ......... 1 
Process of Classifier Design ........................ 
7 
... 
1.1 
1.2 
Notation ........................................ 
9 
References ..................................... 
10 
Chapter2 Random Vectors and Their Properties 
Random Vectors and Their Distributions ............. 11 
Estimation of Parameters ......................... 
17 
2.3 Linear Transformation ........................... 
24 
Computer Projects ............................... 
47 
Problems ....................................... 
48 
11 
2.1 
2.2 
2.4 Various Properties of Eigenvalues and 
Eigenvectors ................................... 
35 
..................................... 
References 
50 
Vii 

Viii 
Contents 
Chapter 3 Hypothesis Testing 
51 
3.1 
3.2 
3.3 
3.4 
3.5 Sequential Hypothesis Testing .................... 
110 
Problems ...................................... 
120 
References .................................... 
122 
Hypothesis Tests for Two Classes ................... 51 
Other Hypothesis Tests ........................... 
65 
Error Probability in Hypothesis Testing ............. 85 
Upper Bounds on the Bayes Error .................. 97 
Computer Projects .............................. 
119 
Chapter 4 Parametric Classifiers 
124 
4.1 The Bayes Linear Classifier ....................... 
125 
4.2 Linear Classifier Design ......................... 
131 
4.3 Quadratic Classifier Design ...................... 
153 
4.4 Other Classifiers ................................ 
169 
Computer Projects .............................. 
176 
Problems ...................................... 
177 
References ..................................... 
180 
Chapter 5 Parameter Estimation 
181 
5.1 Effect of Sample Size in Estimation ................ 182 
5.2 Estimation of Classification Errors ................ 196 
5.3 Holdout. LeaveOneOut. and Resubstitution 
Methods ...................................... 
219 
5.4 Bootstrap Methods ............................. 
238 
Computer Projects .............................. 
250 
Problems ...................................... 
250 
References .................................... 
252 
Chapter 6 Nonparametric Density Estimation 
254 
6.1 
6.2 
6.3 
Parzen Density Estimate ........................ 
255 
kNearest Neighbor Density Estimate .............. 268 
Expansion by Basis Functions .................... 
287 
Computer Projects .............................. 
295 
Problems ..................................... 
296 
References .................................... 
297 

Contents 
ix 
Chapter 7 Nonparametric Classification and 
Error Estimation 
300 
7.1 General Discussion .............................. 
301 
7.2 Voting kNN Procedure - Asymptotic Analysis ...... 305 
7.3 Voting kNN Procedure - Finite Sample Analysis ..... 313 
7.4 Error Estimation ............................... 
322 
7.5 Miscellaneous Topics in the kNN Approach .......... 351 
Computer Projects .............................. 
362 
Problems ...................................... 
363 
References ..................................... 
364 
Chapter 8 Successive Parameter Estimation 
367 
8.1 Successive Adjustment of a Linear Classifier ........ 367 
8.2 Stochastic Approximation ....................... 
375 
8.3 Successive Bayes Estimation ..................... 
389 
Computer Projects ............................ 
395 
Problems .................................... 
396 
References ................................... 
397 
Chapter 9 Feature Extraction and Linear Mapping 
9.1 The Discrete Karhunen-Lokve Expansion ........... 400 
9.2 The Karhunen-LoBve Expansion for Random 
Processes ..................................... 
417 
9.3 
for Signal Representation 
399 
Estimation of Eigenvalues and Eigenvectors ........ 425 
Computer Projects .............................. 
435 
Problems ..................................... 
438 
References .................................... 
440 
Chapter 10 Feature Extraction and Linear Mapping 
for Classification 
441 
10.1 General Problem Formulation .................... 
442 
10.2 Discriminant Analysis ......................... 
445 
10.3 Generalized Criteria ............................ 
460 
10.4 Nonparametric Discriminant Analysis ............ 466 
10.5 Sequential Selection of Quadratic Features ......... 480 
10.6 Feature Subset Selection ........................ 
489 

X 
Contents 
Computer Projects ............................. 
503 
Problems ..................................... 
504 
References .................................... 
506 
Chapter 11 Clustering 
508 
11.1 Parametric Clustering .......................... 
509 
11.2 Nonparametric Clustering ....................... 
533 
11.3 Selection of Representatives ..................... 
549 
Computer Projects ............................. 
559 
Problems ..................................... 
560 
References .................................... 
562 
Appendix A DERIVATIVES OF MATRICES ............. 564 
Appendix B MATHEMATICAL FORMULAS ............ 572 
Appendix C NORMAL ERROR TABLE ................. 576 
Appendix D GAMMA FUNCTION TABLE .............. 578 
Index ................................................ 
579 

Preface 
This book presents an introduction to statistical pattern recogni- 
tion. Pattern recognition in general covers a wide range of problems, 
and it is hard to find a unified view or approach. It is applied to 
engineering problems, such as character readers and waveform analy- 
sis, as well as to brain modeling in biology and psychology. However, 
statistical decision and estimation, which are the subjects of this book, 
are regarded as fundamental to the study of pattern recognition. Statis- 
tical decision and estimation are covered in various texts on mathemati- 
cal statistics, statistical communication, control theory, and so on. But 
obviously each field has a different need and view. So that workers in 
pattern recognition need not look from one book to another, this book is 
organized to provide the basics of these statistical concepts from the 
viewpoint of pattern recognition. 
The material of this book has been taught in a graduate course at 
Purdue University and also in short courses offered in a number of 
locations. Therefore, it is the author’s hope that this book will serve as a 
text for introductory courses of pattern recognition as well as a refer- 
ence book for the workers in the field. 
xi 


Acknowledgments 
The author would like to express his gratitude for the support of 
the National Science Foundation for research in pattern recognition. 
Much of the material in this book was contributed by the author’s past 
co-workers, T. E Krile, D. R. Olsen, W. L. G. Koontz, D. L. Kessell, L. D. 
Hostetler, I? M. Narendra, R. D. Short, J. M. Mantock, T. E. Flick, D. 
M. Hummels, and R. R. Hayes. Working with these outstanding indi- 
viduals has been the author’s honor, pleasure, and delight. Also, the 
continuous discussion with W. H. Schoendorf, B. J. Burdick, A. C. 
Williams, and L. M. Novak has been stimulating. In addition, the 
author wishes to thank his wife Reiko for continuous support and 
encouragement. 
The author acknowledges those at the Institute of Electrical and 
Electronics Engineers, Inc., for their authorization to use material from 
its journals. 
xiii 


Chapter I 
INTRODUCTION 
This book presents and discusses the fundamental mathematical tools for 
statistical decision-making processes in pattern recognition. It is felt that the 
decision-making processes of a human being are somewhat related to the 
recognition of patterns; for example, the next move in a chess game is based 
upon the present pattern on the board, and buying or selling stocks is decided 
by a complex pattern of information. The goal of pattern recognition is to clar- 
ify these complicated mechanisms of decision-making processes and to 
automate these functions using computers. However, because of the complex 
nature of the problem, most pattern recognition research has been concentrated 
on more realistic problems, such as the recognition of Latin characters and the 
classification of waveforms. The purpose of this book is to cover the 
mathematical models of these practical problems and to provide the fundamen- 
tal mathematical tools necessary for solving them. Although many approaches 
have been proposed to formulate more complex decision-making processes, 
these are outside the scope of this book. 
1.1 Formulation of Pattern Recognition Problems 
Many important applications of pattern recognition can be characterized 
as either waveform classification or classification of geometric figures. For 
example, consider the problem of testing a machine for normal or abnormal 
I 

2 
Introduction to Statistical Pattern Recognition 
operation by observing the output voltage of a microphone over a period of 
time. This problem reduces to discrimination of waveforms from good and 
bad machines. On the other hand, recognition of printed English Characters 
corresponds to classification of geometric figures. In order to perform this type 
of classification, we must first measure the observable characteristics of the 
sample. The most primitive but assured way to extract all information con- 
tained in the sample is to measure the time-sampled values for a waveform, 
x ( t , ) ,  . . . , x(t,,), and the grey levels of pixels for a figure, x(1) , . . . , A-(n), as 
shown in Fig. 1-1. These n measurements form a vector X. Even under the 
normal machine condition, the observed waveforms are different each time the 
observation is made. Therefore, x(ri) is a random variable and will be 
expressed, using boldface, as x(fi). Likewise, X is called a random vector if its 
components are random variables and is expressed as X. Similar arguments 
hold for characters: the observation, x(i), varies from one A to another and 
therefore x(i) is a random variable, and X is a random vector. 
Thus, each waveform or character is expressed by a vector (or a sample) 
in an n-dimensional space, and many waveforms or characters form a distribu- 
tion of X in the n-dimensional space. Figure 1-2 shows a simple two- 
dimensional example of two distributions corresponding to normal and 
abnormal machine conditions, where points depict the locations of samples and 
solid lines are the contour lines of the probability density functions. If we 
know these two distributions of X from past experience, we can set up a boun- 
dary between these two distributions, g (I- ,, x2) = 0, which divides the two- 
dimensional space into two regions. Once the boundary is selected, we can 
classify a sample without a class label to a normal or abnormal machine, 
depending on g (x I ,  xz)< 0 or g ( x ,  , x 2 )  >O. We call g (x , x 2 )  a discriminant 
function, and a network which detects the sign of g (x 1, x2) is called a pattern 
I-ecognition network, a categorizer, or a classfier. Figure 1-3 shows a block 
diagram of a classifier in a general n-dimensional space. Thus, in order to 
design a classifier, we must study the characteristics of the distribution of X for 
each category and find a proper discriminant function. This process is called 
learning or training, and samples used to design a classifier are called learning 
or training samples. The discussion can be easily extended to multi-category 
cases. 
Thus, pattern recognition, or decision-making in a broader sense, may be 
considered as a problem of estimating density functions in a high-dimensional 
space and dividing the space into the regions of categories or classes. Because 

1 Introduction 
3 
,Pixel #1 
(b) 
Fig. 1-1 Two measurements of patterns: (a) waveform; (b) character. 
of this view, mathematical statistics forms the foundation of the subject. Also, 
since vectors and matrices are used to represent samples and linear operators, 
respectively, a basic knowledge of linear algebra is required to read this book. 
Chapter 2 presents a brief review of these two subjects. 
The first question we ask is what is the theoretically best classifier, 
assuming that the distributions of the random vectors are given. This problem 
is statistical hypothesis testing, and the Bayes classifier is the best classifier 
which minimizes the probability of classification error. Various hypothesis 
tests are discussed in Chapter 3. 
The probability of error is the key parameter in pattern recognition. The 
error due to the Bayes classifier (the Bayes error) gives the smallest error we 
can achieve from given distributions. In Chapter 3, we discuss how to calcu- 
late the Bayes error. We also consider a simpler problem of finding an upper 
bound of the Bayes error. 

4 
Introduction to Statistical Pattern Recognition 
Fig. 1-2 
Distributions of samples from normal and abnormal machines. 
Although the Bayes classifier is optimal, its implementation is often 
difficult in practice because of its complexity, particularly when the dimen- 
sionality is high. Therefore, we are often led to consider a simpler, parametric 
classifier. Parametric classifiers are based on assumed mathematical forms for 
either the density functions or the discriminant functions. Linear, quadratic, or 
piecewise classifiers are the simplest and most common choices. Various 
design procedures for these classifiers are discussed in Chapter 4. 
Even when the mathematical forms can be assumed, the values of the 
parameters are not given in practice and must be estimated from available sam- 
ples. With a finite number of samples, the estimates of the parameters and 
subsequently of the classifiers based on these estimates become random vari- 
ables. The resulting classification error also becomes a random variable and is 
biased with a variance. Therefore, it is important to understand how the 
number of samples affects classifier design and its performance. Chapter 5 
discusses this subject. 

1 Introduction 
5 
When no parametric structure can be assumed for the density functions, 
we must use nonparametric techniques such as the Parzen and k-nearest neigh- 
bor approaches for estimating density functions. In Chapter 6, we develop the 
basic statistical properties of these estimates. 
Then, in Chapter 7, the nonparametric density estimates are applied to 
classification problems. The main topic in Chapter 7 is the estimation of the 
Bayes error without assuming any mathematical form for the density functions. 
In general, nonparametric techniques are very sensitive to the number of con- 
trol parameters, and tend to give heavily biased results unless the values of 
these parameters are carefully chosen. Chapter 7 presents an extensive discus- 
sion of how to select these parameter values. 
In Fig. 1-2, we presented decision-making as dividing a high- 
dimensional space. An alternative view is to consider decision-making as a 
dictionary search. That is, all past experiences (learning samples) are stored in 
a memory (a dictionary), and a test sample is classified to the class of the 
closest sample in the dictionary. This process is called the nearest neighbor 
classification rule. This process is widely considered as a decision-making 
process close to the one of a human being. Figure 1-4 shows an example of 
the decision boundary due to this classifier. Again, the classifier divides the 
space into two regions, but in a somewhat more complex and sample- 
dependent way than the boundary of Fig. 1-2. This is a nonparametric 
classifier discussed in Chapter 7. 
From the very beginning of the computer age, researchers have been 
interested in how a human being learns, for example, to read English charac- 
ters. The study of neurons suggested that a single neuron operates like a linear 
classifier, and that a combination of many neurons may produce a complex, 
piecewise linear boundary. So, researchers came up with the idea of a learning 
machine as shown in Fig. 1-5. The structure of the classifier is given along 
with a number of unknown parameters wo, . . . ,wT. The input vector, for 
example an English character, is fed, one sample at a time, in sequence. A 
teacher stands beside the machine, observing both the input and output. When 
a discrepancy is observed between the input and output, the teacher notifies the 
machine, and the machine changes the parameters according to a predesigned 
algorithm. Chapter 8 discusses how to change these parameters and how the 
parameters converge to the desired values. However, changing a large number 
of parameters by observing one sample at a time turns out to be a very 
inefficient way of designing a classifier. 

6 
X 
> 
Introduction to Statistical Pattern Recognition 
classifier 
output * 
wo, w 1,"" '., wy 
+ 
+ 
+ 
+ 
+ 
class 1 
+ 
+ 
0
0
 
I 
0 
0 
0 
class 2 
0 
0 
I 
+ X I  
Fig. 1-4 Nearest neighbor decision boundary. 
f 
We started our discussion by choosing time-sampled values of 
waveforms or pixel values of geometric figures. Usually, the number of meas- 
urements n becomes high in order to ensure that the measurements carry all of 
the information contained in the original data. This high-dimensionality makes 
many pattern recognition problems difficult. On the other hand, classification 
by a human being is usually based on a small number of features such as the 
peak value, fundamental frequency, etc. Each of these measurements carries 
significant information for classification and is selected according to the physi- 
cal meaning of the problem. Obviously, as the number of inputs to a classifier 
becomes smaller, the design of the classifier becomes simpler. In order to 
enjoy this advantage, we have to find some way to select or extract important 

1 Introduction 
7 
features from the observed samples. This problem is calledfeature selection or 
extraction and is another important subject of pattern recognition. However, it 
should be noted that, as long as features are computed from the measurements, 
the set of features cannot carry more classification information than the meas- 
urements. As a result, the Bayes error in the feature space is always larger 
than that in the measurement space. 
Feature selection can be considered as a mapping from the n-dimensional 
space to a lower-dimensional feature space. The mapping should be carried 
out without severely reducing the class separability. Although most features 
that a human being selects are nonlinear functions of the measurements, finding 
the optimum nonlinear mapping functions is beyond our capability. So, the 
discussion in this book is limited to linear mappings. 
In Chapter 9, feature extraction for- signal representation is discussed in 
which the mapping is limited to orthonormal transformations and the mean- 
square error is minimized. On the other hand, in feature extruetion for- classif- 
cation, mapping is not limited to any specific form and the class separability is 
used as the criterion to be optimized. Feature extraction for classification is 
discussed in Chapter 10. 
It is sometimes important to decompose a given distribution into several 
clusters. This operation is called clustering or unsupervised classification (or 
learning). The subject is discussed in Chapter 1 1. 
1.2 Process of Classifier Design 
Figure 1-6 shows a flow chart of how a classifier is designed. After data 
is gathered, samples are normalized and registered. Normalization and regis- 
tration are very important processes for a successful classifier design. How- 
ever, different data requires different normalization and registration, and it is 
difficult to discuss these subjects in a generalized way. Therefore, these sub- 
jects are not included in this book. 
After normalization and registration, the class separability of the data is 
measured. This is done by estimating the Bayes error in the measurement 
space. Since it is not appropriate at this stage to assume a mathematical form 
for the data structure, the estimation procedure must be nonparametric. If the 
Bayes error is larger than the final classifier error we wish to achieve (denoted 
by E ~ ) ,  the data does not carry enough classification information to meet the 
specification. Selecting features and designing a classifier in the later stages 

8 
Introduction to Statistical Pattern Recognition 
SEARCH FOR 
NEW MEASUREMENTS 
NORMALIZATION 
REGISTRATION 
(NONPARAMETRIC) 
1 
& <  Eo 
ERROR ESTIMATION 
(NONPARAMETRIC) 
NONPARAMETRIC 
PROCESS 
STATISTICAL TESTS 
LINEAR CLASSIFIER 
QUADRATIC CLASSIFIER 
PIECEWISE CLASSIFIER 
NONPARAMETRIC CLASS1 FI E R 
PARAMETERIZATION 
PROCESS 
t 
Fig. 1-6 A flow chart of the process of classifier design. 
merely increase the classification error. Therefore, we must go back to data 
gathering and seek better measurements. 
Only when the estimate of the Bayes error is less than E,,, may we 
proceed to the next stage of data structure analysis in which we study the 
characteristics of the data. All kinds of data analysis techniques are used here 
which include feature extraction, clustering, statistical tests, modeling, and so 
on. Note that, each time a feature set is chosen, the Bayes error in the feature 
space is estimated and compared with the one in the measurement space. The 
difference between them indicates how much classification information is lost 
in the feature selection process. 

1 Introduction 
9 
Once the structure of the data is thoroughly understood, the data dictates 
which classifier must be adopted. Our choice is normally either a linear, qua- 
dratic, or piecewise classifier, and rarely a nonparametric classifier. Non- 
parametric techniques are necessary in off-line analyses to carry out many 
important operations such as the estimation of the Bayes error and data struc- 
ture analysis. However, they are not so popular for any on-line operation, 
because of their complexity. 
After a classifier is designed, the classifier must be evaluated by the pro- 
cedures discussed in Chapter 5. The resulting error is compared with the 
Bayes error in the feature space. The difference between these two errors indi- 
cates how much the error is increased by adopting the classifier. If the differ- 
ence is unacceptably high, we must reevaluate the design of the classifier. 
At last, the classifier is tested in the field. If the classifier does not 
perform as was expected, the data base used for designing the classifier is dif- 
ferent from the test data in the field. Therefore, we must expand the data base 
and design a new classifier. 
Notation 
n 
L 
N 
N, 
Oi 
Dimensionality 
Number of classes 
Number of total samples 
Number of class i samples 
Class i 
A priori probability of 0, 
Vector 
Random vector 
Conditional density function of O, 
Mixture density function 
A poster-iori probability of w, 
given X 
Expected vector of o, 
M, = E ( X I  w, I 

10 
Introduction to Statistical Pattern Recognition 
M = E  { X ) = EL 
PiM; 
Zi = E  {(X -M;)(X --Mi)‘ 
I O; } 
Expected vector of the mixture 
density 
Covariance matrix of O; 
r = l  
Z = E {  (X - M ) ( X  - M)’} 
( P J ,  +P,(M, -M)(M; -M)‘ 1 
Covariance matrix of the 
mixture density 
r = l  
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
K. Fukunaga, “Introduction to Statistical Pattern Recognition,” Academic 
Press, New York, 1972. 
R. 0. Duda and P. E. Hart, “Pattern Classification and Scene Analysis,” 
Wiley, New York, 1973. 
P. R. Devijver and J. Kittler, “Pattern Recognition: A Statistical 
Approach,” Prentice-Hall, Englewood Cliffs, New Jersey, 1982. 
A. K. Agrawala (ed.), “Machine Recognition of Patterns,” IEEE Press, 
New York, 1977. 
L. N. Kanal, Patterns in pattern recognition: 1968-1972, Trans. IEEE 
Inform. Theory, IT-20, pp. 697-722,1974. 
P. R. Krishnaiah and L. N. Kanal (eds.), “Handbook of Statistics 2: 
Classification, Pattern Recognition and Reduction of Dimensionality,” 
North-Holland, Amsterdam, 1982. 
T. Y. Young and K. S. Fu (eds.), “Handbook of Pattern Recognition and 
Image Processing,” Academic Press, New York, 1986. 

Chapter 2 
RANDOM VECTORS 
AND THEIR PROPERTIES 
In succeeding chapters, we often make use of the properties of random 
vectors. We also freely employ standard results from linear algebra. This 
chapter is a review of the basic properties of a random vector [1,2] and the 
related techniques of linear algebra [3-5). The reader who is familiar with 
these topics may omit this chapter, except for a quick reading to become fami- 
liar with the notation. 
2.1 Random Vectors and their Distributions 
Distribution and Density Functions 
As we discussed in Chapter 1, the input to a pattern recognition network 
is a random vector with n variables as 
x = [x,x* . . . X,]T , 
where T denotes the transpose of the vector. 
Distribution function: A random vector may be characterized by a pr-o- 
bahility distribution function, which is defined by 
P ( . Y , . . . . , ? c , l ) = P I . ~ x ,  
SX,, ..., x,, Ix,,] , 
(2.2) 
11 

12 
Introduction to Statistical Pattern Recognition 
where P r ( A )  is the probability of an event A. For convenience, we often write 
(2.2) as 
P ( X )  = P r ( X  5x1 . 
(2.3) 
Density function: Another expression for characterizing a random vector 
is the density function, which is defined as 
P r ( x l  < x l  < x l + A x , ,  . . . ,  ~ ~ < x , ~ ~ x , , + A x ~ ~ )  
p ( X )  = lim 
Av I +O 
 AX^ . . .AX,, 
Inversely, the distribution function can be expressed in terms of the density 
function as follows: 
X 
P ( X ) = j  p ( Y ) d Y  =I”’. 
. 
.-ca 
- 
1 Yr,) dY I ’ . 
where 
( .) dY is a shorthand notation for an n-dimensional integral, as 
shown. -?he density function p ( X )  is not a probability but must be multiplied 
by a certain region Ax I . . . Axrl (or AX ) to obtain a probability. 
In pattern recognition, we deal with random vectors drawn from different 
classes (or categories), each of which is characterized by its own density func- 
tion. This density function is called the class i density or conditional density of 
class i, and is expressed as 
p(X I 0,) or p,(X) 
(i=l, . . . , L )  , 
where 0, indicates class i and L is the number of classes. The unconditional 
density function of X, which is sometimes called the mixture densiry function, 
is given by 
(2.6) 
where Pi is a priori probability of class i. 
Aposteriori probability: The a posteriori probability of mi given X ,  
P ( w j  I X )  or qi(X), can be computed by using the Bayes theorem, as follows: 

2 Random Vectors and their Properties 
13 
This relation between qi(X) 
and pj(X) provides a basic tool in hypothesis test- 
ing which will be discussed in Chapter 3. 
Parameters of Distributions 
A random vector X is fully characterized by its distribution or density 
function. Often, however, these functions cannot be easily determined or they 
are mathematically too complex to be of practical use. Therefore, it is some- 
times preferable to adopt a less complete, but more computable, characteriza- 
tion. 
Expected vector: One of the most important parameters is the expected 
wctor’ or mean of a random vector X. The expected vector of a random vector 
X is defined by 
M = E { X J  = J X p ( X )  dX , 
(2.9) 
where the integration is taken over the entire X-space unless otherwise 
specified. 
The ith component of M ,  m,, can be calculated by 
+m 
rn, = J x , p ( ~ )  
dx = j x , p ( s , )  dx, , 
(2.10) 
-m 
where p (s,) 
is the marginal density of the ith component of X ,  given by 
p ( S I )  = I” 
. . . j+-p (X) dx I . . . d,Vl -, dx, +, . . . dx,, . 
-_ 
-m 
(2.1 1) 
I1 - I 
Thus, each component of M is actually calculated as the expected value of an 
individual variable with the marginal one-dimensional density. 
The conditional expected \vector of a random vector X for 0, 
is the 
integral 
M I  =/?{XI 0,) = J X p , ( X ) d X ,  
(2.12) 
where p , ( X )  is used instead of p ( X )  in (2.9). 
Covariance matrix: Another important set of parameters is that which 
indicates the dispersion of the distribution. The coiwriance mafrk of X is 

14 
Introduction to Statistical Pattern Recognition 
defined by 
X = E ( (X-M)(X-M)T) = E 
m , )  . . .  
m l )  . . .  
(2.13) 
The components cI, of this matrix are 
(2.14) 
Thus, the diagonal components of the covariance matrix are the \w.iunces of 
individual random variables, and the off-diagonal components are the c o i ~ ~ r i -  
unces of two random variables, xi and x,. Also, it should be noted that all 
covariance matrices are symmetric. This property allows us to employ results 
from the theory of symmetric matrices as an important analytical tool. 
Equation (2.13) is often converted into the following form: 

2 Random Vectors and their Properties 
R =  
1s 
1 
PI2 
' ' ' Pi,, 
PI2 
1 
' 
Pin 
. . .  
1 
C=E[XXT} - E { X ) M T - M E ( X T }  + M M T = S - M M T ,  (2.15) 
R =  
(2.16) 
1 
PI2 
' ' ' Pi,, 
PI2 
1 
' 
Pin 
. . .  
1 
Derivation of (2.15) is straightforward since M = E [  X ) .  The matrix S of 
(2.16) is called the autocorrelafion matri.r of X. Equation (2.15) gives the 
relation between the covariance and autocorrelation matrices, and shows that 
both essentially contain the same amount of information. 
Sometimes it is convenient to express cii by 
cII = (3, 2 
and c,, = p,ioioj , 
(2.17) 
where 0: is the variance of xi, Var(xi }, or (3/ is the standard deviation of xi, 
SD[xi}, and pi, is the correlation coefficient between xi and xi. Then 
z = r R r  
(2.18) 
where 
r =  
and 
J, 
0 
. . .  0 
0 
(32 
(2.19) 
(2.20) 
Thus, C can be expressed as the combination of two types of matrices: one is 
the diagonal matrix of standard deviations and the other is the matrix of the 

16 
Introduction to Statistical Pattern Recognition 
correlation coefficients. We will call R a correlation matrix. Since standard 
deviations depend on the scales of the coordinate system, the correlation matrix 
retains the essential information of the relation between random variables. 
Normal Distributions 
An explicit expression of p (X) for a normal distribution is 
(2.21) 
where Nx(M, C) is a shorthand notation for a normal distribution with the 
expected vector M and covariance matrix X, and 
(2.22) 
where h, is the i, j component of C-'. The term trA is the trace of a matrix A 
and is equal to the summation of the diagonal components of A. As shown in 
(2.21), a normal distribution is a simple exponential function of a distance 
function (2.22) that is a positive definite quadratic function of the x's. The 
coefficient ( 2 ~ ) ~ " ' ~  
IC I 
is selected to satisfy the probability condition 
l p ( X ) d X  = 1 . 
(2.23) 
Normal distributions are widely used because of their many important 
(1) Parameters that specify the distribution: The expected vector M and 
covariance matrix C are sufficient to characterize a normal distribution 
uniquely. All moments of a normal distribution can be calculated as functions 
of these parameters. 
they are also independent. 
(3) Normal marginal densities and normal conditional densities: The 
marginal densities and the conditional densities of a normal distribution are all 
normal. 
properties. Some of these are listed below. 
(2) Wncorrelated-independent: If the xi's are mutually uncorrelated, then 
(4) Normal characteristic functions: The characteristic function of a nor- 
mal distribution, Nx(M, C), has a normal form as 

2 Random Vectors and their Properties 
17 
where SZ = [o, . . . o , ] ~  
and O, is the ith frequency component. 
( 5 )  Linear- transformations: Under any nonsingular linear transformation, 
the distance function of (2.22) keeps its quadratic form and also does not lose 
its positive definiteness. Therefore, after a nonsingular linear transformation, a 
normal distribution becomes another normal distribution with different parame- 
ters. 
Also, it is always possible to find a nonsingular linear transformation 
which makes the new covariance matrix diagonal. Since a diagonal covariance 
matrix means uncorrelated variables (independent variables for a normal distri- 
bution), we can always find for a normal distribution a set of axes such that 
random variables are independent in the new coordinate system. These sub- 
jects will be discussed in detail in a later section. 
(6) Physical jusfification: The assumption of normality is a reasonable 
approximation for many real data sets. This is, in particular, true for processes 
where random variables are sums of many variables and the central limit 
theorem can be applied. However, normality should not be assumed without 
good justification. More often than not this leads to meaningless conclusions. 
2.2 Estimation of Parameters 
Sample Estimates 
Although the expected vector and autocorrelation matrix are important 
parameters for characterizing a distribution, they are unknown in practice and 
should be estimated from a set of available samples. This is normally done by 
using the sample estimation technique [6,7]. In this section, we will discuss 
the technique in a generalized form first, and later treat the estimations of the 
expected vector and autocorrelation matrix as the special cases. 
Sample estimates: Let y be a function of x, , . . . , x,, as 
s = f ' c x , , .  . ., x,,) 
with the expcctcd value rn, and variance 0:: 
(2.25) 

18 
Introduction to Statistical Pattern Recognition 
m, = E { y }  and 0; =Var(y) . 
(2.26) 
Note that all components of M and S of X are special cases of m,. More 
specifically, when y = x:’ . . . x: 
with positive integer ih’s, the corresponding 
m, is called the (i I + . . . + i,,)th order. moment. The components of M are the 
first order moments, and the components of S are the second order moments. 
In practice, the density function of y is unknown, or too complex for 
computing these expectations. Therefore, it is common practice to replace the 
expectation of (2.26) by the average over available samples as 
(2.27) 
where yh is computed by (2.25) from the kth sample x,. This estimate is 
called the sample estimate. Since all N samples X I , .  . . , XN are randomly 
drawn from a distribution, it is reasonable to assume that the Xk’s are mutually 
independent and identically distributed (iid). Therefore, yI , . . . , yN are also 
iid. 
,. 
l
N
 
m\ =-CY!, , 
h = l  
Moments of the estimates: Since the estimate m, is the summation of N 
random variables, it is also a random variable and characterized by an expected 
value and variance. The expected value of m, is 
l
h
 
(2.28) 
That is, the expected value of the estimate is the same as the expected value of 
y .  An estimate that satisfies this condition is called an unhiased estimate. 
Similarly, the variance of the estimate can be calculated as 

2 Random Vectors and their Properties 
19 
(2.29) 
Since 
yI , . . . , yN 
are 
mutually 
independent, 
E ( (yk - m,)(y, - my) 1 
= E (  yk - m, JE(y, - m, 1 = 0 for k&. The variance of the estimate is seen to 
be 11N times the variance of y. Thus, Var( m,.} can be reduced to zero by let- 
ting N go to m. An estimate that satisfies this condition is called a consisrent 
esrimare. All sample estimates are unbiased and consistent regardless of the 
functional form off. 
The above discussion can be extended to the covariance between two dif- 
ferent estimates. Let us introduce another random variable z = g (xl, . . . , x,~). 
Subsequently, m, and m, are obtained by (2.26) and (2.27) respectively. The 
covariance of m, and m, is 
,. 
n 
A 
n 
A
A
 
A 
n 
Cov(my,m,} = E ( ( m ,  -m,)(m, - m ) }  
(2.30) 
1 
N 
= - 
Cov(y,z) . 
Again, E{(yl -m,.)(z;-m,)} = E ( y k  -rn,)E(z, - m r }  = O  for k + L  because 
yl and z are independent due to the independence between XI and X, . 
In most applications, our attention is focused on the first and second 
order moments, the sample mean and sample autocor-relation matrix, respec- 
tively. These are defined by 
and 
(2.31) 
(2.32) 
Note that all components of (2.31) and (2.32) are special cases of (2.25). 
Therefore, M and 6 are unbiased and consistent estimates of M and S respec- 
tively. 

20 
Introduction to Statistical Pattern Recognition 
A 
A 
Example 1: For mi, the ith component of M, 
the corresponding y is xi. 
If the moments of xi are given as .!?(xi} = m i ,  V a r ( x i }  =of, and 
Cov(xj,xi} =pijojaj, then the moments of mi are computed by (2.28), (2.29), 
and 
(2.30), 
resulting 
in 
E { mi 1 = mi, 
Var( mi } = o?/N, 
and 
Cov( mi,mj) = pijaioj/N. They may be rewritten in vector and matrix forms 
,. 
*
A
 
as 
E(MJ = M ,  
(2.33) 
(2.34) 
1 
N 
Cov{M) = E ( ( M - M ) ( M - M ) T )  =-E, 
where Cov( M} is the covariance matrix of M. 
,. 
A 
Example 2: For sjj, the i, j component of S, the corresponding y is xix,;. 
Therefore, 
A 
E ( S j j }  = Sjj , 
(2.35) 
2 
(2.36) 
A 
Var(sij} = -Var(xixj} 
1 
= 1 
-[[E{xixj 
2
2
 } - E  ( x j x j ) l ,  
Cov( s;j,spt ) = -Cov( x;xj.xpx, ) 
N 
N 
1 
N 
A
,
.
 
= -[E{xixjxkx;} 
1 
-E(xixj}E{XkX;Il. 
N 
(2.37) 
Central Moments 
The situation is somewhat different when we discuss central moments 
such as variances and covariance matrices. If we could define y for the i, j 
component of 
as 
y = (xi - mi)(xi - m j )  
(2.38) 
with the given expected values mi and mi, then 
A 
E(m,} = E { y }  = pijoioj . 
(2.39) 
The sample estimate is unbiased. In practice, however, mi and mi are 
unknown, and they should be estimated from available samples. When the 
sample means are used, (2.38) must be changed to 

2 Random Vectors and their Properties 
21 
n 
n 
y = (xi - m,)(xj - mi) . 
(2.40) 
Then 
E(m,.} = E { y )  f pippi. 
(2.41) 
That is, the expectation of the sample estimate is still the same as the expected 
value of y given by (2.40). However, the expectation of (2.40) is not equal to 
that of (2.38) which we want to estimate. 
Sample covariance matrix: In order to study the expectation of (2.40) 
in a matrix form, let us define the sample estimate of a covariance matrix as 
I
N
 
c = ,C(X, 
- M)(X, - M ) T  
(2.42) 
Then 
h 
Thus, taking the expectation of C 
E {  i] 
= c - E {  (M - M)(M - M)T ) 
c .  
1 
N - 1  
=I:--E=- 
N 
N 
That is, (2.44) shows that C is a hiased estimate of C .  This bias can be elim- 
inated by using a modified estimate for the covariance matrix as 
(2.45) 
Both (2.42) and (2.45) are termed a sample co\qarianc.e muriiu. In this book, 
we use (2.45) as the estimate of a covariance matrix unless otherwise stated, 
because of its unbiasedness. When N is large, both are practically the same. 

22 
Introduction to Statistical Pattern Recognition 
,. 
Variances and covariances of cij: The variances and covariances of cij 
(the i, j component of i) are hard to commte exactly. However, approxima- 
tions may be obtained easily by using ?a = ( 1 / N )  c” (Xk - M ) ( X I  - M)T in 
place of 
of (2.42). The i, j component of ia as an approximation of iij is 
then given by 
I = I  
(2.46) 
where xik is the ith component of the kth sample Xg. The right hand side of 
(2.46) is the sample estimate of E { (xi - mi)(xj - m,)]. Therefore, the argu- 
ments used to derive (2.28). (2.29), and (2.30) can be applied without 
modification, resulting in 
E { cjj } Z cij , 
A 
I
N
 
cij E -Z(x;k - mi)(x,* - mj) , 
Nk=, 
(2.47) 
A 
(2.48) 
I 
N 
Var(cij } z -Var( (xi - mi)@ - m j )  t , 
and 
Note that the approximations are due to the use of mi on the left side and mi 
on the right side. Both sides are practically the same for a large N. 
Normal case with approximation: Let us assume that samples are 
drawn from a normal distribution, Nx(O,A), where A is a diagonal matrix with 
components A,,. . . , A,,. Since the covariance matrix is diagonal, xi and x,~ 
for 
ii’j are mutually independent. Therefore, (2.48) and (2.49) are further 
simplified to 
,. 
I 
Aih, 
Var(cij} = -Var(x,)Var(xjl = - 
N 
N
’
 
(2.50) 
(2.51) 

2 Random Vectors and their Properties 
23 
and 
= 0 
except (i=k and j=t) . 
(2.52) 
The reader may confirm (2.52) for all possible combinations of i, j ,  k, and 1. When 
i=kandj=t,Cov(cjj, ck.:) becomesVar(cjj), whichisgiven in (2.50). 
as follows: 
1 1 , .  
A 
L) 
A 
Also, the covariance between mi and ckt may be computed in approximation 
1 
Cov(m,,ck, 1 E - 
Cov{x,, xkx, 1 
N 
A
1
1
 
= --[E{X,X,R) 
1 
- E(Xj)E(XkX t1 
= 0 ,  
(2.53) 
N 
because E ( xjxkx, } = 0 and E { xi } = 0 for a zero-mean normal distribution. 
Normal case without approximation: When samples are drawn from a 
normal distribution, the variances and covariances for cjj of (2.45) are known 
without approximation. In order to see the effects of the approximation on (2.50)- 
(2.52), let us study the exact solutions here. Again, for simplicity, let us assume a 
zero expected vector and a diagonal covariance matrix A. 
It is known that the sample variance ijj 
= I/(N-l)E:=, (xi, -mi)* for a nor- 
L) 
mal xi has a gamma distribution as [2] 
where 
N-1 
p + l = -  N-' 
and ai=- 
2 
2hi ' 
(2.54) 
(2.55) 
and r(.) 
is the gamma function and u (-) is a step function. The expected value and 
variance of (2.54) are also known as 

24 
Introduction to Statistical Pattern Recognition 
E ( C ; ; ]  
* 
= - 
=I;, 
ai 
+I 
2hf 
Var{ ;.;; ) = pr = - 
ai 
N-1 
(2.56) 
(2.57) 
On the other hand, the moments of ljj = l/(N-l)xr=, (xjk-m;)(xjk-mj) for 
i#j can be computed as follows: 
I 
I 
(2.58) 
l
N
 
N-1 k = l  
E(c;,) = - x E ( x i k  
- mjlE(xjL- - mi) = 0, 
(2.59) 
The expectations can be broken into the product of two expectations because x, 
and x, are mutually independent. E ( (xlL-m,)(x,,-m,)] =h, S,, ( N - l ) / N ,  because 
Xn and X, are independent. Similarly, the covariances are 
,. 
,. 
,
.
A
 
COV{C,~,C~,) 
= O  
except {i=k and j = l ) ,  
(2.60) 
because some of the (x..-m.) terms are independent from the others and 
E{x..-m.) =O. 
Note that (2.59) and (2.60) are the same as (2.50) and (2.52) respectively. 
Equation (2.51) may be obtained from (2.57) by using the approximation of 
N-1 
N. This confirms that the approximations are good for a large N. 
A 
,. 
2.3 Linear Transformation 
Linear Transformation 
When an n-dimensional vector X is transformed linearly to another n- 
dimensional vector Y, Y is expressed as a function of X as 

2 Random Vectors and their Properties 
25 
Y = A T X ,  
(2.61) 
where A is an n x n matrix. Then, the expected vector and covariance matrix of Y 
are 
M y = E ( Y )  = A T E ( X ]  = A T M X ,  
(2.62) 
Xy = E ((Y - My)(Y - My)' ] 
=A'E((X - M,y)(X - Mx)T}A 
= A T
~
X
~
 
, 
(2.63) 
where the following rule of matrices (matrices need not be square) is used 
 AB)^ = B
~
A
~
 
. 
(2.64) 
A similar rule, which holds for the inversion of matrices, is 
This time, the existence of (AB)-' , A - ' ,  andB-' is required. 
Example 3: The distance function of (2.22) for Y can be calculated as 
d?(Y) = (Y - My)%;'(Y - M Y )  
= (x - M , ) ~ A A - ' C , ' ( A ~ ) - ' A ~ ( X  
- M ~ )  
= (X - MX)%iI (X - M,) 
= &(X) . 
(2.66) 
That is, the distance of (2.22) is invariant under any nonsingular ( I A I t 0) linear 
transformation. 
Example 4: If X is normal with M x  and &, Y is also normal with M Y  and 
Cy. Since the quadratic form in the exponential function is invariant, the density 
function of Y is 

26 
Introduction to Statistical Pattern Recognition 
(2.67) 
where IA I is the Jacobian of this linear transformation. Recalling (2.63) and a 
determinant rule 
C , = A ~ C ~ A +  
I ~ ~ I = I A ~ I I ~ , I I A I = I C ~ I I A I ~ ,  
(2.68) 
p (Y) becomes 
(2.69) 
Thus, Y is a normal distribution with the expected vector MY and covariance 
matrix Xy. 
Orthonormal Transformation 
Let us shift our coordinate system to bring the expected vector M to the ori- 
gin. We use Zfor the new coordinate system. 
Z = X - M .  
(2.70) 
Then the quadratic form of (2.22) becomes 
&Z) = ZTC-IZ . 
(2.71) 
Let us find a vector Z which maximizes d$(Z) subject to the condition ZTZ = 1 
(constant). This is obtained by 
(2.72) 
where p is a Lagrange multiplier. The term a/aZ consists of n partial derivatives 
a 
-lzTC-lZ 
- p(Z7Z - 1 ) )  = 2x-Iz - 2p.z = 0 ,  
az 
[a/az a/az2 . . . &az,,lT. The result is 
c-Iz=pz 01‘ zz = h Z  (h = I/p), 
(2.73) 
ZTZ= 1 .  
(2.74) 
In order that a nonnull Z may exist, h must be chosen to satisfy the determinant 
equation 
IC-hll = o .  
(2.75) 
This is called the char-ucter-istic equatiori of the matrix Z. Any value of h that 
satisfies this equation is called an eigenvalire, and the Z corresponding to a given h 

2 Random Vectors and their Properties 
27 
is called an eigenvector-. When X is a symmetric n x n matrix, we haven real eigen- 
values X I , .  . ., h, and n real eigenvectors $,, . . . , @,. The eigenvectors 
corresponding to two different eigenvalues are orthogonal. This can be proved as 
follows: For h,, @, 
and h,, @,(A, #A,), 
X@, =A,@, and 
XQJ = A,@, . 
(2.76) 
Multiplying the first equation by @;, 
the second by $7, and subtracting the second 
from the first gives 
('1 - 'J)@T@, = @;'$I 
- 
= 
9 
(2.77) 
since C is a symmetric matrix. Since h, +AJ, 
O T @ I  = 0 ' 
(2.78) 
Thus, (2.73), (2.74), and (2.78) can be rewritten as 
(2.79) 
(2.80) 
where Q, is an n x n matrix, consisting of n eigenvectors as 
@ = [@I 
' ' ' 4411 
(2.81) 
and A is a diagonal matrix of eigenvalues as 
0 
F' 
(2.82) 
and I is the identity matrix. The matrices 0 and A will be called the eigenvector 
matrix and the eigenvalue matrix, respectively. 
Let us use @ as the transformation matrix A of (2.61) as 
Y = O T X .  
(2.83) 
Then, from (2.63), 

28 
Introduction to Statistical Pattern Recognition 
where the following relationships are used: 
(@T)T = 0 , 
(2.85) 
@-I 
= @‘ 
[from (2.80)] 
(2.86) 
Equation (2.84) leads to the following important conclusions: 
(1) The transformation of (2.83) may be broken down to n separate equa- 
tions yi =$yX ( i = l ,  . . . ,n). Since @;X is lb$iIIIIXIlcosO= lIXIlcos0 where 0 is the 
angle between the two vectors $; and X ,  yi is the projected value of X on 0;. 
Thus, 
Y represents X in the new coordinate system spanned by 
. . . , qn, and (2.83) 
may be interpreted as a coordinate transformation. 
(2) We can find a linear transformation to diagonalize a covariance matrix in 
the new coordinate system. This means that we can obtain uncorrelated random 
variables in general and independent random variables for normal distributions. 
(3) The transformation matrix is the eigenvector matrix of E x .  Since the 
eigenvectors are the ones that maximize &Z), we are actually selecting the prin- 
cipal components of the distribution as the new coordinate axes. A two- 
dimensional example is given in Fig. 2- 1. 
(4) The eigenvalues are the variances of the transformed variables, y, ’s. 
(5) This transformation is called an orthonormal transformation, because 
(2.80) is satisfied. In orthonormal transformations, Euclidean distances are 
preserved since 
llY112 = YTY =XT@QTX =xTx = IIx112 . 
(2.87) 
Whitening Transformation 
After applying the orthonormal transformation of (2.83), we can add another 
that will make the covariance matrix equal to I. 
transformation 
y = A-”2@TX = (@A-1’2)TX , 
(2.88) 
(2.89) 
is called the whitening transformation or the 
E - A-I/2@TZx@A-l/2 = A-l/2AA-l/2 = I 
Y -  
This transformation 

2 Random Vectors and their Properties 
29 
- 
- ' I  
Fig. 2-1 Eigenvalues and eigenvectors of a distribution. 
whitening process. The purpose of the second transformation 
is to change 
the scales of the principal components in proportion to I/%. 
Figure 2-2 shows a 
two-dimensional example. 
A few properties of the whitening transformation are pointed out here as fol- 
lows. 
( 1) Whitening transformations are not orthonormal transformations because 
(@A-l/z)T(@A-l/2) = A-I/2@T@A-I/Z = A-1 
[ 
(2.90) 
Therefore, Euclidean distances are not preserved: 
llY112 = Y'Y = X'@A-'@'X 
= XTZilX # IIXII' . 
(2.91) 
(2) After a whitening transformation, the covariance matrix is invariant 
under any orthonormal transformation, because 
YTIY = YTY = I  . 
(2.92) 
This property will be used for simultaneous diagonalization of two matrices. 

Introduction to Statistical Pattern Recognition 
i l  
= XI 
Fig. 2-2 Whitening process. 
Sample generation: In pattern recognition experiments, it is often neces- 
sary to generate samples which are to be normally distributed according to a given 
expected vector M and covariance matrix X. In general, the variables are corre- 
lated and this makes the generation of samples complex. However, the generation 
of normal samples with the expected vector 0 and covariance matrix I is easy. 
Therefore, samples may be generated as follows: 
(1) From the given E, find the whitening transformation of Y =h-”2@TX. 
(2) Generate N independent, normally distributed numbers for each 
yi (i=l, . . . , n) with zero expected value and unit variance. Then, form N vectors 
In the transformed space, C y  =I. 
Y, 
, . . . ,YN. 
(3) Transform back the generated samples to the X-space by 
Xk =Oh”2Yk 
(k=l, . . . , N). 
(4) Add M to the samples in the X-space as Xk + M (k = 1, . . . , N). 

2 Random Vectors and their Properties 
31 
Simultaneous Diagonalization 
We can diagonalize two symmetric matrices El and C2 simultaneously by a 
( I )  First, we whiten C, by 
linear transformation. The process is as follows: 
y = @ - ” 2 @ T x  
(2.93) 
where 0 
and @ are the eigenvalue and eigenvector matrices of E I as 
E l @ = @ @  and 
@ ‘ @ = I .  
(2.94) 
Then, C, and C2 are transformed to 
@-1/*@TZ, QQ-1’2 
= 
(2.95) 
@-l/2@TC2@Q-’/2 
= K 
(2.96) 
In general, K is not a diagonal matrix. 
(2) Second, we apply the orthonormal transformation to diagonalize K. 
That is, 
Z = V Y ,  
(2.97) 
where ‘i’ and A are the eigenvector and eigenvalue matrices of K as 
K Y = Y A  and 
Y T Y = = I .  
(2.98) 
As shown in (2.92), the first matrix I of (2.95) is invariant under this transforma- 
tion. Thus, 
Y’IY =YTY = I ,  
(2.99) 
Y J ~ K Y  
= A .  
(2.100) 
Thus, both matrices are diagonalized. Figure 2-3 shows a two-dimensional 
example of this process. The combination of steps (1) and (2) gives the overall 
transformation matrix w - ” ~ Y ’ .  
Alternative approach: The matrices O@-”2\y and A can be calculated 
directly from X, and C2 without going through the two steps above. This is done 
as follows: 
Theorem We can diagonalize two symmetric matrices as 

32 
Introduction to Statistical Pattern Recognition 
(C 1 
Fig. 2-3 Simultaneous diagonalization. 
ATCIA = I  
and 
ATZ2A = A ,  
(2.101) 
ZT'C2A =AA . 
(2.102) 
where A and A are the eigenvalue and eigenvector matrices of X y ' C2. 
Proof Since A's are the eigenvalues of K from (2.98), 
I K - h l l  = o .  
(2.103) 
Replacing K and I by (2.95) and (2.96), 
I o-'/*oT 
I I Z* - hC' I I 
I = 0 , 
is nonsingular, I O-1/2@T 
I # 0 and 
(2.104) 
Since the transformation matrix 
I @O-112 
I # 0. Therefore, 
Iz,-hCII = o  or 
IC;'z2-hlI = o .  
(2.105) 
Thus, h's are the eigenvalues of Cy1X2. 
For the eigenvectors, inserting (2.96) into (2.98) yields 

2 Random Vectors and their Properties 
33 
or 
By (2.951, (@-1’20T)-1 
can be replaced by CI O@-1’2. 
or 
z~’z~(@@-’’~Y) 
= (w-”~Y)A . 
(2.109) 
Thus, the transformation matrix A = <D@-’/2yl is calculated as the eigenvector 
matrix  of^;'^^. 
One fact should be mentioned here. The eigenvectors $i of a symmetric 
matrix are orthogonal and satisfy @7qj = O  for i #j. However, IT’& is not sym- 
metric in general, and subsequently the eigenvectors ci are not mutually orthogo- 
nal. Instead, the s i ’ s  are orthogonal with respect to C I  : that is, <:XI cj = 0 for igj. 
Furthermore, in order to make the 5,’s orthonormal with respect to X I  to satisfy 
the first equation of (2.101), the scale of si must be adjusted by 
such that 
r T  
Y 
(2.1 10) 
Simultaneous diagonalization of two matrices is a very powerful tool in pat- 
tern recognition, because many problems of pattern recognition consider two dis- 
tributions for classification purposes. Also, there are many possible modifications 
of the above discussion. These depend on what kind of properties we are 
interested in, what kind of matrices are used, etc. In this section we will show one 
of the modifications that will be used in later chapters. 
Modification: 
Theorem Let a matrix Q be given by a linear combination of two symmetric 
matrices Q I and Q as 
Q =aiQi + ~ 2 Q 2  
(2.1 11) 
where a I and a 2  are positive constants. If we normalize the eigenvectors with 
respect to Q as the first equation of (2.101), Q l  and Q2 will share the same 

34 
Introduction to Statistical Pattern Recognition 
eigenvectors, and their eigenvalues will be reversely ordered as 
h\’) > hi1) > . . . > ha’) for Q ,  , 
(2.1 12) 
< hi2) < . . . < hi2) for Q 2  . 
(2.113) 
Proof Let Q and Q be diagonalized simultaneously such that 
ATQA = I  and 
ATQIA =A(’) , 
(2.114) 
or 
Then Q2 is also diagonalized because, from (2.1 11) and (2.1 14), 
(2.115) 
(2.1 16) 
(2.1 17) 
Therefore, Q ,  and Q2 share the same eigenvectors that are normalized with 
respect to Q because of the first equation of (2.1 14) and, if hf” > A:”, 
then 
hi2) < hj2) from (2.1 17). 
Example 5: Let S be the mixture autocorrelation matrix of two distributions 
whose autocorrelation matrices are S I and S 2. Then 
s = E{XXT) 
=PIE(XX*ICO~} 
+P2E(XXTI~2} 
= P  IS1 + P , S ; ! .  
(2.118) 
Thus, by the above theorem, we can diagonalize S I and S2 with the same set of 
eigenvectors. Since the eigenvalues are ordered in reverse, the eigenvector with 
the largest eigenvalue for the first distribution has the least eigenvalue for the 
second, and vice versa. This property can be used to extract features important to 
distinguish two distributions [8]. 

2 Random Vectors and their Properties 
35 
2.4 Various Properties of Eigenvalues and Eigenvectors 
As we saw in the diagonalization processes, the eigenvalues and eigenvec- 
tors of symmetric matrices play an important role. In this section, we review vari- 
ous properties of eigenvalues and eigenvectors, which will simplify discussions in 
later chapters. Most of the matrices we will be dealing with are covariance and 
autocorrelation matrices, which are symmetric. Therefore, unless specifically 
stated, we assume that matrices are symmetric, with real eigenvalues and eigen- 
vectors. 
Orthonormal Transformations 
Theorem An eigenvalue matrix A is invariant under any orthonormal linear 
transformation. 
Proof Let A be an orthonormal transformation matrix and let it satisfy 
A T A  = I  
or 
= A - '  . 
(2.1 19) 
By this transformation, Q is converted to A TQA [see (2.63)]. If the eigenvalue and 
eigenvector matrices of A 'QA are A and Q,, 
QT(ATQA)Q, = A , 
(2.120) 
(AQ,)TQ(AcD) = A . 
(2.121) 
Thus, A and A @  should be the eigenvalue and eigenvector matrices of Q. This 
transformation matrix A Q, satisfies the orthonormal condition as 
(AQ,~(AQ,) 
= Q,'A'AQ, 
= aTcg = I  . 
(2.122) 
Positive Definiteness 
Theorem If all eigenvalues are positive, Q is a positive definite matrix. 
Proof Consider a quadratic form 
d 2  =XTQX. 
(2.123) 
We can rewrite X as @Y, where Q, is the eigenvector matrix of Q. Then 

36 
Introduction to Statistical Pattern Recognition 
n 
i=l 
d 2  = (@Y)'Q(@Y) = Y'QTQ@Y = Y'AY = zh;y? , 
(2.124) 
where the hi's are the eigenvalues of Q. If these eigenvalues are all positive, then 
d is positive, unless Y is a zero vector. From the relation between Y andX, we see 
that d must be positive for all nonzeroX as well. Therefore, Q is positive definite. 
When Q is a covariance or autocorrelation matrix, the hi's are the variances 
or second order moments after the orthonormal transformation to diagonalize Q. 
Therefore, all hi's should be positive for both cases, and both covariance and auto- 
correlation matrices are positive definite. 
Trace 
Theorem The trace of Q is the summation of all eigenvalues and is invariant 
under any orthonormal transformation. That is, 
n 
trQ = 
. 
i=l 
Proof First for general rectangular matrices A,,,,, and B,,,,, , 
because 
(2.125) 
(2.126) 
(2.127) 
where aij and bji are the components of A,,,, and B,,,,, . Using (2.126), 
n 
z h i  = t r h  = tr(@Q@) = tr(QWD') = trQ . 
(2.128) 
As we proved before, the eigenvalues are invariant under any orthonormal 
transformation. Therefore, any function of eigenvalues is also invariant. 
When Q is a covariance or autocorrelation matrix, the above theorem states 
that the summation of the variances or second order moments of individual vari- 
ables is invariant under any orthonormal transformation. 
i=l 

2 Random Vectors and their Properties 
37 
Theorem If A and 0 
are the eigenvalue and eigenvector matrices of Q, the 
eigenvalue and eigenvector matrices of Q"' for any integer m are A"' and @ respec- 
tively. That is, 
Q @ = @ A  + Qm@ = @ A m .  
(2.129) 
Proof Using QQ =@A, 
Theorem The trace of Q" is the summation of hy's, and invariant under 
any orthonormal transformation. That is, 
I1 
trQ" = tr A"' = x h r  . 
i = l  
(2. I3 I )  
Example 6: Let us consider n eigenvalues, h I , . . . , h,, , as the samples drawn 
from the distribution of a random variable h. Then we can calculate all sample 
moments of the distribution of 1 by 
(2.132) 
1 
I1 
1 
n ;=I 
n 
E ( P }  =-XI:' = -trQn', 
where E { .  ) indicates the sample estimate of E { }. Particularly, we may use 
A 
(2.133) 
1 
1 
n 
n j = l  
E(1J 
= -trQ = -Cqji, 
1 
1 
n 
n 
car(X} = -trQ2 - (-trQ j2 
(2.134) 
Example 7: Equation (2.13 1 ) is used to find the largest eigenvalue because 
A;' + . , . + hr E h;' 
for m >>1 , 
(2.135) 
where h, is assumed to be the largest eigenvalue. For example, if we select 

38 
Introduction to Statistical Pattern Recognition 
m = 16, we need to multiply matrices four times as Q + Q 2  -+ Q4 + Q8 + Q 1 6 ,  
and take the trace of Q 
l6 to estimate the largest eigenvalue. 
Determinant and Rank 
Theorem The determinant of Q is equal to the product of all eigenvalues 
and is invariant under any orthonormal transformation. That is, 
n 
l Q  I = Ihl = nhi 
i=l 
(2.136) 
Proof Since the determinant of the product of matrices is the product of the 
determinants of the matrices, 
Ihl = IaTI lQl101 = l Q l  IQTI 101 = l Q l  . 
(2.137) 
Theorem The rank of Q is equal to the number of nonzero eigenvalues. 
Proof Q can be expressed by 
(2.138) 
where the +;'s are linearly independent vectors with mutually orthonormal rela- 
tions. Therefore, if we have (n - r )  zero hi's, we can express Q by r linearly 
independent vectors, which is the definition of rank r. 
Three applications of the above theorems are given as follows: 
Relation between I S I and I C I : We show the relation between the deter- 
minants of the covariance and autocorrelation matrices [from (2.15)]: 
IS1 = I C + M M T I .  
(2.139) 
Applying the simultaneous diagonalization of (2.101) for X I  =C and & = M M T ,  
wehaveAr(C+MMT)A =I +A. Therefore, 
f i ( l  + hi) 
I1 
(2.140) 
where I! I A I = I C I is obtained from (2.101). On the other hand, since the rank of 
MM' is one, the hi's should satisfy the following conditions 
i=l 
IZ + MMTI = 
= I n ( l  + h;)}lCl , 
IA I* 
i = l  

2 Random Vectors and their Properties 
39 
hI#O, hZ= . . . =  hn=O, 
(2.141) 
i h ; = h  
I - - tr(ATMMTA) = tr(M*AA*M) = M7X-'M , 
(2.142) 
where AA * = C-' is obtained from (2. IOl), and tr(MTZ-'M) = MrC-'M because 
MTZ-l M is a scalar. Thus, 
i=l 
IS1 = lZl(1 + M Y M ) .  
(2.143) 
Small sample size problem: When only m samples are available in an n- 
dimensional vector space with m < n, the sample autocorrelation matrix S is calcu- 
lated from the samples as 
A 
(2.144) 
That is, is a function of m or less linearly independent vectors. Therefore, the 
rank of ,? should be m or less. The same conclusion can be obtained for a sample 
covariance matrix. However, the (Xi - M)'s are not linearly independent, because 
they are related by E? (Xi - M )  
= 0. Therefore, the rank of a sample covariance 
matrix is (m - I )  or less. This problem, which is called a small sample size prob- 
lem, is often encountered in pattern recognition, particularly when n is very large. 
For this type of problem, instead of calculating eigenvalues and eigenvectors from 
an n x n matrix, the following procedure is more efficient [9]. 
Let X I ,  . . . , X, (m c n) be samples. The sample autocorrelation matrix of 
,. 
A 
I =I 
these samples is 
(2.145) 
where U,,, 
is called a sample matrix and defined by 
u = [XI . . * Xnilnxm . 
(2.146) 
Instead of using the n x n matrix i,lx, 
of (2.143, let us calculate the eigenvalues 
and eigenvectors of an m x m matrix (U TU)mml as 
1 
m 
(2.147) 
- ( ~ ~ ~ ) m m t Q m m i  
= Q'nixniAmxni . 
Multiplying U into (2.147) from the left side, we obtain 

40 
Introduction to Statistical Pattern Recognition 
Thus, (U@)nx,,, and A,,,,,,, 
are the m eigenvectors and eigenvalues of 
S = (UUT)nx,,/m. 
The other (n - m) eigenvalues are all zero and their eigenvectors 
are indefinite. The advantage of this calculation is that only an m xm matrix is 
used for calculating m eigenvalues and eigenvectors. The matrix (U@),,,,, 
represents orthogonal vectors but not orthonormal ones. In order to obtain ortho- 
normal vectors Vi, we have to divide each column vector of (U(D)f,ml 
by (mhi)1'2 
as 
n 
1 
v. = - 
UQi 
or 
V,,, = -(U@II-"~)~~,,, , 
(2.149) 
' 
(mhi)1'2 
112 
because, from (2.147), 
Near-singular matrix: in many pattern recognition problems, n may be 
very large, for example 100. However, only a few eigenvalues, such as 10, are 
dominant, so that 
h, + . . . + h, a1 + . . . + hp 
(k <<n) . 
(2.151) 
This means that in a practical sense we are handling X (or S ) with rank k, even 
though the mathematical rank of C is still n. Therefore, it is very inefficient to use 
an n x n matrix to find k eigenvalues and eigenvectors, even when we have a sam- 
ple size greater than n. In addition to this inefficiency, we face some computa- 
tional difficulty in handling a large, near-singular matrix. For example, let us con- 
sider the calculation of X-' or I C I .  The determinant 1 Z I is n:=, hi and (n - k) 
S;'sareveryclose tozero. If we haven = 100,k= 10, and hl + . . . +hIO=0.90ut 
ofhl+ ...+ hloo=l, IZlbecomes 
nfo hi x n!O0 
J = I I  h. 
J = n!' 
/ = I  hi x (0.1190)90 E n,!:,hi x 
r = l  
fortheassumptionofhI1 =hl2 = . . . =Aloo. 

2 Random Vectors and their Properties 
41 
Fortunately, in pattern recognition problems, I X I is rarely computed 
directly. Instead, In IC I is commonly used, which can be computed from the 
eigenvalues as 
n 
InICI = Zlnh,. 
i=l 
(2.152) 
Fortheaboveexample,InlCI =C,!!, Inh, +90ln (O.l/9O)=C,!!, Inhi -612.2. 
As far as the inverse is concerned, each element of I:-' is given by the ratio 
of a cofactor (the determinant of an (n -l)x(n - 1) matrix B ) and I X I . The cofactor 
is the product of (n -1) eigenvalues of B, while I 
I is the product of n eigenvalues 
of Z. Assuming that (n -I) eigenvalues of the denominator are, roughly speaking, 
cancelled out with (n -1) eigenvalues of the numerator, I B I / I C I is proportional to 
l/hk where hk is one of the eigenvalues of X. Therefore, although I C I becomes 
extremely small as the above example indicates, each element of C-l does not go 
up to an extremely large number. In order to avoid I B I / I I: I = 010 in computation, 
it is suggested to use the following formula to compute the inverse matrix. 
(2.153) 
Again, the eigenvalues and eigenvectors of C are computed first, and then C-' is 
obtained by (2.153). Recall from (2.129) that, if A and @ are the eigenvalue and 
eigenvector matrices of C, A-I and 0 are the eigenvalue and eigenvector matrices 
of C-' . Also, any matrix Q can be expressed by (2.138), using the eigenvalues and 
eigenvectors. 
Matrix Inversion 
Diagonalization of matrices is particularly useful when we need the inverse 
From (2.66), a distance function is expressed by 
of matrices. 
d$(X) = (X - M)'Z-'(X - M )  = (Y - D)'A-l(Y - D )  
(2.154) 
where D = [d, . . . d,,]' and A are the expected vector and diagonal covariance 

42 
Introduction to Statistical Pattern Recognition 
matrix of Y after the diagonizing transformation. For two distributions, the dis- 
tance functions are, by simultaneous diagonalization, 
(2.155) 
(2.156) 
When distance computations are heavily involved in practice, it is suggested to 
transform the original data samples X i  to Yj before processing the data. This saves 
a significant amount of computation time. 
Relation between S-' and C-': We show the inverse matrix of an auto- 
correlation matrix in terms of the covariance matrix and expected vector. From 
(2.15), 
s-I = (C + MM')-' 
. 
Applying the simultaneous diagonalization of (2.101) for X I  = X  and C2 = M M 7 ,  
we have A '(C + MMT)A = I  + A, or C + MM' = (A ')-I 
(I + A)A-'. Taking the 
(2.157) 
inverse, 
(Z + MM')-I 
= A(I + A)-'AT . 
where A is given in (2.141) and (2.142). Therefore, 
( I  + A)-' = 
. + A ,  0 
1 
0 
1 
1 
1 + h ,  
0 
(2.1 58) 
0 
1 
1 

2 Random Vectors and their Properties 
43 
0 
hl 
I + h ,  
I-- 
1 
0 
1 
= I - -  ' A .  
1 +h, 
where A, 
would like to calculateMT'S-'M in terms of MTC-'M, 
=X-' and AMT =Z-'MMTE-' are obtained from (2. 
(2.159) 
(2.160) 
01). If we 
(2.161) 
(2.162) 
Pseudoinverse: One way of calculating the pseudoinverse of a singular 
square matrix is as follows. Let Q be a singular matrix with rank r, then Q can be 
expressed by the eigenvalues and eigenvectors as 
If we express Q * by 
(2.163) 
(2.164) 
then 

44 
Introduction to Statistical Pattern Recognition 
0 
1 
0 
1 
0 
0 
0 
I' 
n - r -  
(2.165) 
Therefore, Q" is the inverse matrix of Q in the subspace spanned by I' eigenvec- 
tors, and satisfies 
QQ*Q = e .  
(2.166) 
Generalized inverse: Equation (2.166) suggests a general way to define the 
"inverse" of a rectangular (not square) singular matrix [IO]. The generalized 
inverse of an m x n matrix R of rank r is an n x m matrix R# satisfying 
R R ~ R  
= R  , 
(2.167) 
o# = o r .  
(2.168) 
The column vectors of R are seen to be eigenvectors of the m x m matrix (RR' ), 
among which the r's are linearly independent with eigenvalues equal to 1. Also, 
(rn - r )  eigenvalues of (RR') must be zero. The matrix (RR') has the properties 
of a projection matrix and is useful in linear regression analysis [6]. 
A particular form of R# is most often used. Let B be an m x r matrix whose 
columns are the linearly independent columns of R. Then R can be expressed by 

2 Random Vectors and their Properties 
45 
Rnrxn = BnixrCrxii . 
(2.169) 
Since BTB is an r x rnonsingular matrix, C can be obtained by 
c = ( B ~ B ) - I B ~ R  
. 
(2.170) 
From (2.170), C has rank r so that CCT is also an I' x I' nonsingular matrix. The 
pseudoinverse R * of R is defined by 
R* = CT(CCT)-'(BTB)-'BT 
. 
(2.171) 
It can be shown that R x  satisfies (2.167) and is therefore a generalized inverse. 
Further, R x  is unique. The pseudoinverse is the most often used generalized 
inverse. 
Standard Data and Experimental Procedure 
Throughout this book the following data will be used: 
Type of distribution: normal, 
Dimension: n = 8 unless specified otherwise, 
Number of classes: L = 2, 
Distribution parameters: 
M ,  = O = [ O .  
z, = I =  
017, 
M * = M = [ m 1  
Data 1-1: 
m l  =m, m 2 =  . . .  = m x = O ,  
1, 
= . . . = h* = 1 
. . .  
0 
hR 
In this data, both C, and C2 are 1. The value of m controls the overlap 
between the two distributions. Unless m (or .In/lTM or IIMz - M I  1)) is specified 

46 
Introduction to Statistical Pattern Recognition 
otherwise, we use m =2.56, which gives the Bayes error of 10%. Also, unless 
specified otherwise, we assume n = 8. Even when n changes, the Bayes error stays 
the same for a fixed m. 
Data 1-41: 
m 1  
= . . . =m8 = O ,  
h, = . . . = A 8  = 4  
In this data, the two expected vectors are the same, but the covariance 
matrices are different. The Bayes error varies depending on the value of the hi’s as 
well as n, and becomes about 9% for hl = . . . = h8 = 4. Again, unless specified 
otherwise, we use n = 8 for this data. 
Data I-A: 
I 
1 
2 
3 
4 
5 
6 
7 
8 
mi 
3.86 
3.10 
0.84 
0.84 
1.64 
1.08 
0.26 
0.01 
hi 
8.41 
12.06 
0.12 
0.22 
1.49 
1.77 
0.35 
2.73 
In this data [ 1 11, both the expected vectors and the covariance matrices 
differ, and the Bayes error is 1.9% as will be shown in Chapter 3. The dimen- 
sionality of this data is fixed and cannot be changed. 
Generally, parametric algorithms which work well for Data /-I will not work 
for Data 1-41, and vice versa. So, it is important to understand which algorithms fit 
which data. Any reasonable nonparametric algorithm must work for all types of 
data, since the algorithm should not depend on the structure of a particular data set. 
Even though the covariance matrices for these three data sets are diagonal, 
they still represent the general case, since any two non-diagonal covariances can 
be simultaneously diagonalized by a linear transformation. Also, a coordinate 
shift can bring M I  to the origin of the coordinate system without any loss of gen- 
erality. 
The dimensionality of 8 was selected for the following reasons. When the 
dimensionality is low (e.g., 1 or 2), all experimental results can be explained easily 
using an engineer’s intuition. Unfortunately, this is no longer true when the 
dimensionality becomes high (for example, 32 or 64). Often, experimental con- 

2 Random Vectors and their Properties 
47 
clusions obtained using low-dimensional data cannot be extended to high- 
dimensional cases. However, running experiments with high-dimensional data 
requires a large amount of memory and frequently consumes a lot of computer 
time. The dimensionality of 8 is a compromise; high-dimensional phenomena can 
be observed with relatively inexpensive data-handling costs. 
Experimental procedure: When an experiment is called for, a number of 
samples, N j  (i = 1,2), are generated according to the specified parameters. Nor- 
mally Ni = 100 is selected for n=8, unless specified otherwise. Using these N j  
samples per class, the planned experiment is conducted. This process is repeated T 
times. For each trial, N j  samples per class must be generated independently. Nor- 
mally T=IO is used in this book, unless specified otherwise. Then, the z experi- 
mental results are averaged and the standard deviation is computed. 
Data RADAR: In addition to the three standard data sets mentioned above, 
a set of millimeter-wave radar data is used in this book in order to test algorithms 
on high-dimensional real data. Each sample is a range profile of a target observed 
using a high resolution millimeter-wave radar. The samples were collected by 
rotating a Chevrolet Camaro and a Dodge Van on a turntable, taking approxi- 
mately 8,800 readings over a complete revolution. The magnitude of each range 
profile was time-sampled at 66 positions (range bins), and the resulting 66- 
dimensional vector was normalized by energy. Furthermore, each normalized 
time-sampled value, x i ,  was transformed to yi by y j  =xi 
(i = 1, . . . ,66). The 
justification of this transformation will be discussed in Chapter 3. The vectors 
were then selected at each half-degree of revolution to form 720 sample sets. 
These sets (720 samples from each class) are referred to in this book as Data 
RADAR. When a large number of samples is needed, 8,800 samples per class will 
be used. 
11.4 
Computer Projects 
1. 
Generate samples from a normal distribution specified by 
n = 2 ,  N=100, .=E], 
and .=I;]. 
2. 
Plot the generated samples. 

48 
Introduction to Statistical Pattern Recognition 
n 
A 
3. 
Compute the sample mean, M, 
and sample covariance matrix,,C 
4. 
Repeat 1 and 3, 10 times. Compute the sample mean and sample variance 
for each component of M and C over 10 trials. 
A 
,. 
5. 
Repeat 4 for N =lo, 20, and 40, and examine the effect of the sample size. 
6. 
Simultaneously diagonalize X and 
Compute the sample mean and sample covariance of V over I O  trials. 
and form a vector V = [XI &I7 
Problems 
1. 
Compute and plot q I (x) and q2(x) forp I (x) =N,,(O, 1) andp2(x) =A',( 1,2). 
2. 
Let p ( X )  be Nx(M, C) with 
Show that 
p (x I ) = N.,, (m 1 ,  0:) (a marginal density) , 
3. 
For the distribution of Problem 2, plot the contour lines for 
d2(X) = 1, 4, and 9 , 
where the parameters are selected as 
p = - l ,  -0.5,0,0.5, 1 .  

2 Random Vectors and their Properties 
49 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
A two-dimensional random vector becomes [a h IT. [-a -hIT, [-c dIT or 
[c -dIT with probability of 1/4 for each. 
(a) 
(b) 
(c) 
Compute the expected vector and covariance matrix. 
Find the condition for a, h, c, and d to satisfy in order to obtain p = 0. 
Find the conditions for (I, h, c and d to satisfy in order to obtain p = + 1 
andp=- 1. 
Let m be the sample mean of N samples, x1 , . . . , xN, drawn from N.,(rn, 02). 
Find the expected value and variance of ( m - ~ ) ~ ,  
and confirm that 
Var{(r;l-rn)2} - I / N ~ .  
Let 
I 
C1 = [ 1 OS] 
and C2 = [I +oy 
0.5 
0.5 
1 
1 - 6 1 4  
Diagonalize these two matrices simultaneously. 
Prove that S-' Mand C-'M are the same vector with different lengths. 
Express a non-zero eigenvalue and the corresponding eigenvector of 
C-'MMT in terms of C and M. (Hint: The rank of Z-IMM' 
is one.) 
Let S be an n x n  matrix, composed of two vectors M I  and M ,  as 
S = M I M Y  +M2MT. The lengths of M I  and M 2  are 1 and 2 respectively, 
and their mutual angle is 60 ". Compute the eigenvalues of S. 
After the mixture of two distributions is normalized by a shift and a linear 
transformation, the expected vectors and covariance matrices satisfy the fol- 
lowing equations. 
Calculate the followings in terms of Y I , P ?, and M I 

50 
Introduction to Statistical Pattern Recognition 
References 
1. 
R. V. Hogg and A. T. Craig, “Introduction to Mathematical Statistics 
(Second Edition),” Macmillan, New York, 1965. 
A. Papoulis, “Probability, Random Variables, and Stochastic Processes,” 
McGraw-Hill, New York, 1965. 
B. Noble and J. W. Daniel, “Applied Linear Algebra (Second Edition),” 
Prentice-Hall, Englewood Cliffs, New Jersey, 1977. 
S. R. Searle, “Matrix Algebra Useful for Statistics,” Wiley, New York, 
1982. 
J. H. Wilkinson, “Algebraic Eigenvalue Problem,” Oxford Univ. Press, 
London and New York, 1965. 
R. Deutsch, “Estimation Theory,” Prentice-Hall, Englewood Cliffs, New 
Jersey, 1965. 
A. Gelb (ed.), “Applied Optimal Estimation,” MIT Press, Cambridge, 1974. 
K. Fukunaga and W. L. G. Koontz, Application of the Karhunen-Lotve 
expansion to feature selection and ordering, Trans. IEEE Computers, C- 19, 
J. A. McLaughlin and J. Raviv, Nth order autocorrelations in pattern recogni- 
tion, inform. and Contr., 1 2, pp. 12 1 - 142,1968. 
10. R. Penrose, On the generalized inverse of a matrix, Proc. Cambridge Philos. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
pp.311-318,1970. 
9. 
S0~.,51,~~.406-413,1955. 
1 1. T. Marill and D. M. Green, On the effectiveness of receptors in recognition 
systems, Trans. IEEE Inform. Theory, IT-9, pp. 1 1-27, 1963. 

Chapter 3 
HYPOTHESIS TESTING 
The purpose of pattern recognition is to determine to which category or 
class a given sample belongs. Through an observation or measurement pro- 
cess, we obtain a set of numbers which make up the observation vector. The 
observation vector serves as the input to a decision rule by which we assign the 
sample to one of the given classes. Let us assume that the observation vector 
is a random vector whose conditional density function depends on its class. If 
the conditional density function for each class is known, then the pattern recog- 
nition problem becomes a problem in statistical hypothesis testing. 
3.1 Hypothesis Tests for Two Classes 
In this section, we discuss two-class problems, which arise because each 
sample belongs to one of two classes, o1 
or 0 2 .  The conditional density func- 
tions and the a priori probabilities are assumed to be known. 
The Bayes Decision Rule for Minimum Error 
Bayes test: Let X be an observation vector, and let it be our purpose to 
determine whether X belongs to o1 
or 02. 
A decision rule based simply on 
probabilities may be written as follows: 
51 

52 
Introduction to Statistical Pattern Recognition 
where qi(X) is a posteriori probability of 0; given X .  Equation (3.1) indicates 
that, if the probability of o1 given X is larger than the probability of 02, 
X is 
classified to o1 , and vice versa. The a posteriori probability q;(X) may be cal- 
culated from the a priori probability Pi and the conditional density function 
pi(X), using Bayes theorem, as 
(3.2) 
where p (X) is the mixture density function. Since p ( X )  is positive and com- 
mon to both sides of the inequality, the decision rule of (3.1) can be expressed 
as 
or 
(3.3) 
(3.4) 
The term [(X) is called the likelihood ratio and is the basic quantity in 
hypothesis testing. We call P21P the threshold value of the likelihood ratio 
for the decision. Sometimes it is more convenient to write the minus-log likeli- 
hood ratio rather than writing the likelihood ratio itself. In that case, the deci- 
sion rule of (3.4) becomes 
P ,  
0 2  
p2 
h ( X ) = - l n t ( X ) = - I n p I ( X ) + l n p 2 ( X )  
3 In - .  
(3.5) 
The direction of the inequality is reversed because we have used the negative 
logarithm. The term h ( X )  is called the discriminant function. Throughout this 
book, we assume P I = P 2 ,  and set the threshold In P IIP = 0 for simplicity, 
unless otherwise stated. 
Equation (3.1), (3.4), or (3.5) is called the Bayes test for minimum error. 
Bayes error: In general, the decision rule of (3.3, or any other decision 
rule, does not lead to perfect classification. In order to evaluate the perfor- 
mance of a decision rule, we must calculate the probability of error, that is, the 
probability that a sample is assigned to the wrong class. 
The conditional error given X ,  r(X), due to the decision rule of (3.1) is 
either 9 I (X) or q * ( X )  whichever smaller. That is, 

3 Hypothesis Testing 
53 
r-(X) = mink1(X),q2(X)I . 
(3.4) 
The total error, which is called the Bayes error, is computed by E { r ( X ) ] .  
where 
Equation (3.7) shows several ways to express the Bayes error, E. 
is the definition of E. The second line is obtained by inserting 
The first line 
(3.6) into the 
first line and applying the Bayes theorem of (3.2). The integral regions L and 
L 2  of the third line are the regions where X is classified to o1 and o2 by this 
decision rule, and they are called the ol- and o;?-regions. 
In L I ,  
P IpI (X) > P 2p2(X), and 
therefore r (X) = P2p2(X)/p (X). 
Likewise, 
r-(X) = P Ip 
I (X)/p (X) in L2 because P lp I (X) < P g 2 ( X )  in L2. In (3.8), we 
distinguish two types of errors: one results from misclassifying samples from 
w1 and the other results from misclassifying samples from 02. The total error 
is a weighted sum of these errors. 
Figure 3-1 shows an example of this decision rule for a simple one- 
dimensional case. 
The 
decision boundary 
is 
set 
at 
x = r  
where 
P lp I (x) = P 2p2(x), and s < r and x > t are designated to L I and L2 respec- 
tively. The resulting errors are P 
= R + C, P 2
~
2
 
= A, and E = A + B + C, 
where A, B, and C indicate the areas, for example, B = I' P Ip (8) dx. 
This decision rule gives the smallest probability of error. This may be 
demonstrated easily from the one-dimensional example of Fig. 3- 1. Suppose 
that the boundary is moved from r to t', setting up the new wI 
- and o2-regions 
as L; and L;. Then, the resulting errors are P ]E; = C, P 2 ~ i  
= A + B + D, and 
6 
= A  + B + C + D, which is larger than E by D. The same is true when the 

54 
Introduction to Statistical Pattern Recognition 
I *L2 
I 
b* 
+--1-->c2 
Fig. 3-1 Bayes decision rule for minimum error. 
boundary is shifted to the left. This argument can be extended to a general n- 
dimensional case. 
The computation of the Bayes error is a very complex problem except in 
some special cases. This is due to the fact that E is obtained by integrating 
high-dimensional density functions in complex regions as seen in (3.8). There- 
fore, it is sometimes more convenient to integrate the density function of 
h = h (X) of (3.5), which is one-dimensional: 
(3.9) 
(3.10) 
where ph(h I mi) is the conditional density of h for mi. However, in general, 
the density function of h is not available, and very difficult to compute. 
Example 1: When the pi(X)'s are normal with expected vectors Mi and 
covariance matrices C;, the decision rule of (3.5) becomes 
h (X) = - In 1(X) 
(3.1 1) 
Equation (3.1 1) shows that the decision boundary is given by a quadratic form 
in X. When CI = C2 = C, the boundary becomes a linear function of X as 

3 Hypothesis Testing 
55 
x2 
x2 
/ 
Fig. 3-2 Decision boundaries for normal distributions: 
(a) XI f &; (b) XI = &. 
Figure 3-2 shows two-dimensional examples for XI 
zC2 and C 
I =&. 
Example 2: Let us study a special case of (3.1 1 )  where 
Mi = O  and X i  = 
1 
p; 
. . .  py-' 
Pi 
1 
' 
Pi 
py-' 
. . . 
Pi 
1 
(3.12) 
(3.13) 
This type of covariance matrix is often seen, for example, when stationary run- 
dom processes are time-sampled to form random vectors. The explicit expres- 
sions for Z;' 
and I Zi 
I are known for this covariance matrix as 

56 
Introduction to Statistical Pattern Recognition 
-p; 
0 . . . 
1+p: -p; 
. .  , 
0 1 
2 
l+Pi -pi 
IZ; I = (1 - p
y
 
. 
(3.14) 
(3.15) 
Therefore, the quadratic equation of (3.11) becomes 
1-p: 
P1 
l-p2 o? 
p2 
- 
~ x i x i + l  
+ (n-1) In 7 
>< In - 
, 
(3.16) 
where the second term shows the edge effect of terminating the observation of 
random processes within a finite length, and this effect diminishes as n gets 
large. 
If we could ignore the second and fourth terms and make 
In ( P ,/P2) 
= 0 (P 1= P2), 
the decision rule becomes (CX;X~+~)/(CX~) 
>< t ; that 
is, the decision is made by estimating the correlation coefficient and threshold- 
ing the estimate. Since pl#p2 is the only difference between o1 and o2 in this 
case, this decision rule is reasonable. 
Example 3: When xk's are mutually independent and exponentially 
distributed, 
(3.17) 
where ajk is the parameter of the exponential distribution for xk and mi, and 
u (.) is the step function. Then, h (X) 
of (3.5) becomes 

3 Hypothesis Testing 
57 
(3.18) 
The Bayes decision rule becomes a linear function of xk’s. 
The Bayes Decision Rule for Minimum Cost 
Often in practice, minimizing the probability of error is not the best cri- 
terion to design a decision rule because the misclassifications of ol 
- and 02- 
samples may have different consequences. For example, the misclassification 
of a cancer patient to normal may have a more damaging effect than the 
misclassification of a normal patient to cancer. Therefore, it is appropriate to 
assign a cost to each situation as 
cIj = cost of deciding X E o, 
when X E o, . 
(3.19) 
Then, the conditional cost of deciding X E o, 
given X ,  r,(X), is 
i’i(X) = ci 14 
I ( X I  + cj2q2(X) . 
(3.20) 
The decision rule and the resulting conditional cost given X ,  i’ (X), are 
(3.21) 
and 
r ( X )  = min[rI(X), r.z(X)I . 
(3.22) 
The total cost of this decision is 

58 
Introduction to Statistical Pattern Recognition 
The boundary which minimizes r of (3.23) can be found as follows. 
First, rewrite (3.23) as a function of L l  alone. This is done by replacing 
[ pi(X)CUr with 1 - I, pi(X)dX, since L I and L2 do not overlap and cover the 
I 
entire domain. Thus, 
Now our problem becomes one of choosing L I such that r is minimized. Sup- 
pose, for a given value of X, that the integrand of (3.24) is negative. Then we 
can decrease r by assigning X to L l .  If the integrand is positive, we can 
decrease r by assigning X to L2. Thus the minimum cost decision rule is to 
assign to L l  those X’s and only those X’s, for which the integrand of (3.24) is 
negative. This decision rule can be stated by the following inequality: 
0 1  
0- 
(c 12-C22)P2P 2 W )  3 (C2I --c 
I I )P IP I (X) 
(3.25) 
or 
(3.26) 
This decision rule is called the Bayes test for- minimum cost. 
Comparing (3.26) with (3.4), we notice that the Bayes test for minimum 
cost is a likelihood ratio test with a different threshold from (3.4), and that the 
selection of the cost functions is equivalent to changing the a priori probabili- 
ties Pi. Equation (3.26) is equal to (3.4) for the special selection of the cost 
functions 
This is called a symmetrical cost function. For a symmetrical cost function, the 
cost becomes the probability of error, and the test of (3.26) minimizes the pro- 
bability of error. 
Different cost functions are used when a wrong decision for one class is 
more critical than one for the other class. 

3 Hypothesis Testing 
59 
The Neyman - 
Pearson Test 
The Neyman-Pearson test follows from a third formulation of the 
hypothesis test problem. Recall that we can commit two types of errors in a 
two-class decision problem. Let the probabilities of these two errors again be 
subject to E;! being equal to a constant, say Q. To determine this decision rule, 
we must find the minimum of 
and E ~ -  The Neyman-Pearson decision rule is the one which minimizes 
I' = E1 + ME2 - Eo) 7 
(3.28) 
where p is a Lagrange multiplier. Inserting 
and 
of (3.8) into (3.28), 
Using the same argument as in the derivation of (3.25) from (3.24), I' can be 
minimized by selecting L and L as 
or 
(3.30) 
(3.31) 
Comparing (3.3 1) with (3.26), we can conclude that the Neyman-Pearson test 
does not offer any new decision rule but relies on the likelihood ratio test, as 
did the Bayes test. However, the preceding discussion shows that the likeli- 
hood ratio test is the test which minimizes the error for one class, while main- 
taining the error for the other class constant. 
The threshold p is the solution, for a given Q, of the following equation: 
E2 = ,, p*(X)dX = Eo 
I 
Or, using the density function of h ( X )  of (3.10). 
(3.32) 

60 
Introduction to Statistical Pattern Recognition 
E2 = y p h ( h  l q ) d h  = 
. 
(3.33) 
-0 
However, an analytical solution is not possible in general. So, we must find p 
experimentally or numerically. Since ph(h 102) 2 0, c2 of (3.33) is a mono- 
tonic function of p, and increases as p increases. Therefore, after calculating 
E ~ ’ S  for several p’s, we can find the p which gives a specified 
as c2. 
Example 4: 
Let us consider two-dimensional normal distributions with 
Then, from 
M I  = [-l,0IT, M 2  = [+1,0]‘, XI = X2 = I, and P I  = P 2  = 0.5. 
(3.12) and (3.31), the decision boundary can be expressed by 
LI 
h (X) 
= { [+1 01 - [-1 01 1 
(3.34) 
The decision boundaries for various p’s are lines parallel to the x2-axis, as 
shown in Fig. 3-3, and the corresponding errors EZ’S are given in Table 3-1. 
For example, if we would like to maintain e2 = 0.09, then p becomes 2 from 
Table 3-1, and the decision boundary passes (-0.34) of x 
TABLE 3-1 
RELATION BETWEEN 
AND ~2 
1 - 
1 - 
l
2
 4 
p: 
4 
2 
~
2
:
 0.04 
0.09 
0.16 
0.25 
0.38 

3 Hypothesis Testing 
61 
--y$ 
-0.34’ 
p=4 2 
I 
1/2 114 
1 
Fig. 3-3 Neyman - 
Pearson boundaries. 
The Minimax Test 
In the Bayes test for minimum cost, we notice that the likelihood ratio is 
compared with a threshold value which is a function of P i .  Therefore, in order 
to design a decision rule which minimizes the cost, we need to know the 
values of Pi beforehand. After the design is completed, the decision rule stays 
optimum only if the Pj’s stay the same. Unfortunately in practice, the Pi’s 
vary after the decision rule is fixed. The minimax test is designed to protect 
the performance of the decision rule, even if the Pi’s vary unexpectedly. 
First, let us express the cost of (3.24) in terms of P I .  Since 
Inserting P 2  = 1-P I into 
P I  + P 2  = 1, P 2  is uniquely determined by P 
(3.24), and replacing [ p I (X)dX by 1 - I p I (X)dX, 
I 
L? 

62 
Introduction to Statistical Pattern Recognition 
(3.35) 
Equation (3.35) shows that, once L and L 2  are determined, r is a linear func- 
tion of P,. In Fig. 3-4, the curved Iine represents an example of the Bayes 
Fig. 3-4 Bayes cost vs. P 
cost plotted against P I ,  where L I  and L2 are selected optimally for each P I .  
If L I  and L2 are fixed for P I  = 0.3, for example, and if P I  varies later unex- 
pectedly, then I' changes according to (3.33, which is the equation for the 
straight line passing through A, as shown in Fig. 3-4. As the result, I' could 
become much larger than we expected when we design the decision rule (for 
example, I' can go up to B when P I  becomes 1). In order to prevent this 
deterioration of performance, we choose L 1 and L2 to make the coefficient of 
P zero in (3.35) regardless of the predicted value for P 
Then, the straight 
line becomes the tangent at the point C where the Bayes cost curve is max- 
imum. This selection of L and L 2  guarantees that the maximum Bayes cost is 
minimized after the threshold value is fixed, regardless of the change of P I .  
This decision rule is called the minimax rest. 
Thus, in the minimax test, the boundary is designed to satisfy 

3 Hypothesis Testing 
63 
If we select the special set of cost functions 
c I 1  = c 2 2  and 
c 1 2 = c 2 1 ,  
(3.37) 
(3.36) becomes 
I, Pl(X)dX =I P2(X)dX ' 
(3.38) 
That is, the decision boundary is still determined by the likelihood ratio, but 
the threshold is selected to satisfy el = e2. 
L ,  
Operating Characteristics 
So far, we have found that the likelihood ratio test is commonly used for 
various tests, and only the selection of the threshold varies depending on the 
test. Extending this, it is a common practice to plot the relation between el 
and 
by changing the value of the threshold continuously. This curve is 
called the operating characteristic [5]. Figure 5 shows an example of the 
operating characteristics where el and 1-e2 are used for the x- and y-axes in 
log scale. Three curves in Fig. 3-5 show the performance of the likelihood 
ratio test for 30, 20, and 9 features which are selected from the same data set. 
They indicate that 30 and 20 features give almost identical performance for a 
wide range of operating points, while 9 features give much poor performance. 
From such curves, the designer of the decision rule can select a proper operat- 
ing point and the corresponding threshold, depending on one's need. 
Burdick's chart: Various combinations of log and linear scales are used 
for operating characteristics. However, the following scale gives a straight line 
when h (X) 
of (3.5) is normally distributed for both o1 and o2 [6]. 
Let @(a) 
be a normal error function defined by 
(3.39) 
If h is distributed as Nh(rn,,o:) 
for o1 
and Nh(m2,0:) for w2, and r is the 
value of the threshold as shown in Fig. 3-6, then 
t-m2 
and 
e2 = @  [T] 
. 
(3.40) 
Or, taking the inverse operation, 

64 
0.999 
0.995 
0.990 
w" - 
0.95 
0.90 
0.50 
0 
introduction to Statistical Pattern Recognition 
30 DIMENSIONS 
I I l l  I 
I 
I 
I 1 1 1 1 1 1  
I 
I 
I I  
15 
0.01 
0.05 
0.10 
0.50 
&l 
Fig. 3-5 An example of operating characteristics. 
m '-t 
I-mZ - W ' ( E 2 )  . 
-- 
-@-'(E~) 
and - 
- 
01 
0 2  
(3.41) 
Eliminating t from these two equations, we can obtain the relation between 
@-'(el) and @-'(e2) as 

3 Hypothesis Testing 
65 
(3.42) 
That is, if @-'(E,) and 
are used as the x- and y-axes, we have a 
h 
E* 
E, 
Fig. 3-6 Normal distributions of h. 
straight line with -o1/o2 as the slope and (mI-rn2)/02 as the y-cross point. 
Figure 3-7 shows the chart, where both @-'(E) and E scales are shown. Note 
that @-'(E) =-2, -1, 0, 1, 2 correspond to E = 2.3, 15.9, 50.0, 84.1, 97.7 (%). 
For Data 1-1, h ( X )  becomes a linear function of X as shown in (3.12), 
and therefore h (X) becomes normal if X is normal. The straight line operating 
characteristic is shown in Fig. 3-7 with the corresponding threshold values. 
The advantage of using this scale is that we may see whether the distri- 
butions of h (X) for a, and w2 are close to normal or not. Also, we can meas- 
ure some of the parameters, -01/02 and (rn I-rn2)/02, from the line. 
3.2 Other Hypothesis Tests 
In this section, other hypothesis tests will be discussed. They are mul- 
tihypothesis tests, single hypothesis tests, reject option, and composite 
hypothesis tests. 

66 
a- 
1 
0 
-1 
-2 
-2 
Introduction to Statistical Pattern Recognition 
ip) €2% 
4 
€1 % 
I 
I 
I 
I 
I 
-3 
-2 
-1 
0 
1 
@?1(€,) 
Fig. 3-7 The operating characteristic of Data I-I on a special coordinate sys- 
tem. 
Multihypothesis Tests 
When the samples are known to come from L classes, we can generalize 
First, if our decision is simply based on probabilities, the decision rule is 
the binary hypothesis testing problem. 
q k ( X )  = max q i ( X )  + X E wk . 
(3.43) 
I 
Or, by the Bayes theorem, 
Pkpk(X) = max Pipi(X) + X E ok . 
(3.44) 
I 
Since X belongs to wj with the probability of q j ( X ) ,  the decision rule of (3.43) 
misclassifys X from oi 
( j  # k) to mk with the same probability. Summing up 
these, the conditional probability of error given X, due to (3.43), becomes 

3 Hypothesis Testing 
67 
I - @ )  = 4 1 ( x )  + . . . + qn.-~(X) + qk+l(X) + . . . + q L ( X )  = 1 - q k ( X ) ,  and the 
Bayes error is the expected value of r ( X )  over X. That is, 
r ( X )  = 1 - max q i ( X )  and 
E = E ( r - ( X ) }  . 
(3.45) 
I 
When cost functions are involved, the decision rule becomes 
r k ( X )  = min r i ( X )  + X E ok 
(3.46) 
I 
where r i ( X )  is a simple extension of (3.20) to L classes as 
L 
ri(X> = C c i j q j ( X )  
j = l  
(3.47) 
and ci, is the cost of deciding X E wi when X E mi. Substituting (3.47) into 
(3.46) and using the Bayes theorem, 
L 
L 
C c k j P j p , ( X )  = min x c i j P j p j ( X )  + X E ok . 
(3.48) 
j=l 
i 
j=1 
The resulting conditional cost given X and the total cost are 
r ( X )  = min r j ( X )  and 
I‘ = E ( r ( X ) }  . 
(3.49) 
I 
Example 5: When cii = 0 and cij = 1 for i#j, r i ( X )  of (3.47) becomes 
(3.50) 
Therefore, the decision rule of (3.46) and the resulting conditional cost of 
(3.49) become (3.43) and (3.43, respectively. 
Single Hypothesis Tests 
So far, we have assumed that our task is to classify an unknown sample 
to one of L classes. However, in practice, we often face the problem in which 
one class is well defined while the others are not. For example, when we want 
to distinguish targets from all other possible nontargets, the nontargets may 
include trucks, automobiles, and all kinds of other vehicles as well as trees and 
clutter discretes. Because of the wide variety, it is almost impossible to study 
the distributions of all possible nontargets before a decision rule is designed. 

68 
Introduction to Statistical Pattern Recognition 
Single hypothesis schemes have been proposed to solve this problem. 
Typically, they involve measuring the distance of the object from the target 
mean (normalized by the target covariance matrix), and applying a threshold to 
determine if it is or is not a target. This technique works well when the dimen- 
sionality of the data, n, is very low (such as 1 or 2). However, as n increases, 
the error of this technique increases significantly. The mapping from the origi- 
nal n-dimensional feature space to a one-dimensional distance space destroys 
valuable classification information which existed in the original feature space. 
In order to understand this phenomena, let us study here the statistics of the 
distance. 
Distribution of the distance: Let us consider a distribution of X with 
the expected vector M and the covariance matrix X. Then, the normalized dis- 
tance of X from M is 
n 
d2 = (X-M)TX-l(X-M) = ZTZ = ZZ? , 
(3.51) 
where Z = AT(X-M) and A is the whitening transformation. Since the 
expected vector and covariance matrix of Z are 0 and I respectively, the zi’s 
are uncorrelated, and E { z i ]  = 0 and Var(zi ] = 1. Thus, the expected value and 
variance of d2 are 
i = l  
E{d2] = n  E { z ? ]  = n  
(3.52) 
Var{d2] =E{(d2)2] - E 2 ( d 2 ]  
(3.53) 
When the 2;’s are uncorrelated (this is satisfied when the zi’s are independent), 
and E { 24 ] is independent of i, the variance of d2 can be further simplified to 
Var{d2) = n  y , 
(3.54) 
where 

3 Hypothesis Testing 
69 
~ = E { z : )  
- E 2 { z ? ]  = E ( z : ]  - 1 .  
(3.55) 
For normal distributions, when the zits are uncorrelated, they are also indepen- 
dent. Therefore, (3.55) can be used to compute Var{d2}, and y= 2. Figure 
3-8 shows the distribution of d2 with the mean n and the standard deviation 
G. 
Example 6: 
Let the xi's be mutually independent and identically distri- 
buted with a gamma densify function, which is characterized by two parameters 
a and p as in (2.54). Using m = E { xi } and 02 = Var{ xi 1, (3.51) becomes 
Then, y is 
E (xi-m14 - d4 
CY4 
p + l '  
Y= 
6 
= 2 + -  
(3.56) 
(3.57) 
where the second line is obtained by using the mth order moments of a gamma 
density as 
(3.58) 
An exponential distribution is a special case of a gamma distribution with 
p = 0, for which y becomes 8. On the other hand, y = 2 is obtained by letting 
p be 00. Recall from (3.55) that y for a normal distribution is 2. 
Example 7: 
In (3.52) and (3.54), only the first and second order 
moments of d2 are given. However, if the 2;'s are normal, the density function 
of d2 is known as [7]. 
(3.59) 
which is the gamma density with p = n /2 - 1 and a = 1/2. 

70 
Introduction to Statistical Pattern Recognition 
L 
yy 
I 
- r  
0 
n 
Fig. 3-8 The distribution of d2. 
The expected value and variance of the gamma distribution are computed from 
a and 
as 
Var(d2] = 
= 2n , 
a2 
(3.60) 
(3.61) 
which are identical to (352) and (3.54). Since the zi’s are obtained by a linear 
transformation from X, the zi’s are normal if X is normal. 
Also, note that (3.59) becomes an exponential distribution for n =2. It is 
known that coherent (complex) radar signatures have real and imaginary parts 
that tend to be independent. Therefore, if both parts are normally distributed, 
the magnitude-square of these two parts, (real)2 + 
will exhibit an 
exponential distribution. 
It is important to realize from Fig. 3-8 that, if samples are drawn from a 
normal distribution in a high-dimensional space, most samples fall in a 
doughnut-type ring and no samples fall in the center region where the value of 
the density function is largest. Because of this phenomena, two distributions 
could be classified with little error, even when they share the same expected 
vectors, as long as the covariance matrices are different. In order to understand 
why this happens, let us look at the example of Fig. 3-9. This figure shows the 
contour lines of a normal distribution with covariance matrix I .  The probabil- 
ity mass of region A, an n-dimensional hypersphere with radius a, is 
P r ( A  } = c ~
‘
~
p
 
(XA) where c is a constant and XA is located somewhere in A. 

3 Hypothesis Testing 
71 
2a 
Fig. 3-9 Probability coverage. 
On the other hand, for the outer ring, region B, with radius between a and 2a, 
Pr ( B  ] = c [(2a)" - a"] p (X,) = c (2"-l)a"p (XB), where X, is located some- 
where in B. Therefore, Pr { B )/Pr ( A  ) = (2"-l)p (X,)@ (XA). This becomes, 
for example, 2 x 10'' for n a 4  and p(X,)/p(X,) = 10. That is, the probability 
of having a sample in region A is so much smaller than the probability for 
region B, that we would never see samples in A by drawing a resonable 
number (say io4) of samples. 
Performance of a single hypothesis test: Suppose that two classes are 
distributed with expected vectors M I  = 0 and M 2  = M, 
and covariance matrices 
El = I and C2 = A  (a diagonal matrix with hi's as the components), respec- 
tively. Without loss of generality, any two covariance matrices can be simul- 
taneously diagonalized to I and A, and a coordinate shift can bring the 
expected vector of wI to zero. As shown in (3.52) and (3.54), E (d2 I o1 } = n 
and Var( d2 I wI 
] = yn, and y = 2 if the o1 -distribution is normal. On the other 
hand, the distance of an %-sample from the wI 
-expected vector, 0, is 
d2 = XTX = (X - M +M)'(X - M + M )  
= (X - M)T(X - M) + 2MT(X - M )  + MTM 
= tr [(X - M ) ( X  - M)T] + 2M7(X - M )  + M'M 
. 
(3.62) 
Taking the expectation with respect to 0 2 ,  

72 
Introduction to Statistical Pattern Recognition 
Likewise, the variance can be computed as 
When the w2-distribution is normal, 
E { (d2)2 1% ] = E ((X-M)7(X-M)(X-M)T(X-M) 
I O, ) 
+ 4 M'E ( (X-M>(X-Mf 1% }M 
+ (MTM)2 + 2 E {  (X-M)T(X-M) I 0 2  )MTM 
n 
n 
= 3 ~ h ?  
+ ~ ~ h i h j  
+ 4zhim; 
i = l  
i t j  
i=l 
(3.63) 
(3.64) 
(3.65) 
where mi is the ith component of M. Subtracting E 2 ( d 2 1 q )  of (3.63), we 
obtain 
(3.66) 
Example 8: For Data I-I with n variables, hi = 1. Therefore, 
E(d210,) = n  and 
Var{d2101] = 2 n ,  
(3.67) 
E ( d 2 1 w )  = n  +M'M 
and Var(d2102] = 2 n  +4M7M. (3.68) 
If we assume normal distributions for d2, we can design the Bayes classifier 
and compute the Bayes error in the d-space, E ~ .  The normality assumption for 
d2 is reasonable for high-dimensional data because d2 is the summation of n 
terms as seen in (3.51), and the central limit theorem can be applied. The &d is 
determined by n and MTM, 
while MTM specifies the Bayes error in the X- 
space, 
In order to show how much classification information is lost by 
mapping the n-dimensional X into the one-dimensional d2, the relation between 

3 Hypothesis Testing 
73 
-n 
0.1 
I 
I 
I 
I
1
1
1
1
1
1
 
, 
I I I I I I I I  
I 
I 
I
1
1
1
1
1
1
 
I 
, I I I I I I  
8 
16 
32 
64 
128 
256 
Fig. 3-10 Performance of a single hypothesis test for Data 1-1. 
ex and ed for various values of n is plotted in Fig. 3-10. For example, when 
n =64, &X = 0.1% is increased to &d = 8.4%. This is the price we must pay 
when we do not know where the second distribution is located relative to the 
first distribution for a fixed IlM 11. 
Ranking procedure: So far, we have pointed out that mapping the n- 
dimensional X into the one-dimensional d2 and classifying samples by thres- 
holding d2 produce a large increase in error. However, the error may be 
reduced significantly by using different approaches for different applications. 
For example, let our problem be to select one object for targeting out of many 
objects detected in a field. Then, we may rank the objects according to their 
distances from the selected target mean, and choose the closest one as the one 
to target. This ranking, instead of thresholding, reduces the classification error 
of the selected object. However, it must be noted that this problem is different 
from the conventional one, in which all objects are classified and the number 
of misclassified objects are counted as the error. 
Assuming that k l  ol-samples and k 2  02-samples are available for rank- 
ing, the probability of acquiring one of the k ,  ol-samples by this procedure 
(the probability of correct classification) can be expressed as [8-91 

74 
(%o) 
1.0 
5.0 
10.0 
20.0 
Introduction to Statistical Pattern Recognition 
Ed 
1 - P a  (%) 
(%) 
10.0 
0.9 
0.6 
24.0 
8.9 
4.4 
32.0 
17.6 
14.9 
42.0 
34.2 
32.0 
k1 = k 2  = 5  
k1 = k 2 = 2 0  
I 
0 
P, = j  kl(l-ul)kl-’(1-*2)kzdUI 
, 
where 
(3.69) 
u;(f) = [ p d ’ ( <  1 mi)d< 
(3.70) 
andpdz((lmi) is the density function of < = d2 for ai. 
As seen in (3.70), u&) 
is the probability of a sample from ai falling in 0 I < < t. Thus, u I (t) = l - - ~ ~  
and u2(r) =
E
~
 
in the d-space when the threshold is chosen at d2 = f. In (3.69), 
du,, (l-uIf’-’, and (1-u2$’ 
represent the probability of one of k l  ol- 
samples falling in f 5 < < r +At, kl-1 of al-samples falling in f + A t  I 
< < 00, and all k2 m2-samples falling in t + Ar I & < 00 respectively. The pro- 
duct of these three gives the probability of the combined event. Since the 
acquisition of any one of the k l  ol-samples is a correct classification, the pro- 
bability is multiplied by k I .  The integration is taken with respect to f from 0 
to 00, that is, with respect to u I from 0 to 1. 
TABLE 3-2 
Table 3-2 shows (l-Po)’s for Data I-I and n =20. Specifying &x as 1, 5, 
10, and 20 %, we computed the corresponding llkfll’s, from which E ~ ’ S  were 
obtained assuming that both pc,l((I~l) 
and p d z ( < l w )  in (3.70) are normal. 
Then, the integrations of (3.69) and (3.70) were carried out numerically for 
normal p d ’ ( <  I wj)’s. Table 3-2 indicates that the ranking procedure is effective, 
particularly for small E ~ ’ s .  Also, the errors are smaller for larger k I and k2 ’s. 

3 Hypothesis Testing 
75 
Test of normality: Despite its importance, it has been difficult to test 
whether a given data set is normal or not. If the dimensionality of the data is 
low, we could use a conventional chi-square test [lo]. But obviously the 
number of cells increases exponentially with the dimensionality and the test is 
impractical for high-dimensional data sets. Measuring the variance of d2 pro- 
vides an estimate of y in (3.54) which may be used to test for normality of a 
high-dimensional distribution. Also, the parameter p could be determined for a 
gamma density function according to (3.57). However, it must be cautioned 
that this procedure tests only one marginal aspect of the distribution, however 
important that aspect, and does not guarantee the overall normality of the dis- 
tribution even if the samples pass the test. 
are given, the density function of 
d2 = (X-M)TZc-'(X-M) 
is given in (3.59), which is a gamma distribution. 
This may be extended to the case where the sample mean and sample covari- 
ance matrix are used in place of M and C as 
When X is normal and M and 
A 
..-I 
(3.71) 
1 
N-1 
5 = -(X-M)TZ 
(X-M) , 
where 
When X is normal, 5 has the beta-distribution given by [l 11 
osys1. 
The expected value and variance of 
may be computed by using 
(3.73) 
(3.74) 
The results are 

76 
Introduction to Statistical Pattern Recognition 
2n 
l-(n+l)/N - 2n 
-- 
- 
Var(C,} = - 
(N-1)2 
1+1IN 
(N-1)2 . 
(3.75) 
(3.76) 
Because 6 of (3.71) is l/(N-1) times the distance, (3.75) and the right-most 
term of (3.76) correspond to (3.60) and (3.61) respectively. 
els. 
(1) 
Thus, the test of normality may be performed in the following two lev- 
Compute the sample variance of 6 of (3.71), and check whether it is 
close to (3.76) or not. When N>>n, 2r~4N-l)~ 
may be used to approxi- 
mate (3.76). 
Plot the empirical distribution function of C by using ((XI), 
, . . ,l,(XN) 
and the theoretical distribution function from (3.73), and apply the 
Kolmogorov-Smirnov test [IO]. 
(2) 
Variable transformation: When variables are causal (i.e. positive), the 
distribution of each variable may be approximated by a gamma density. In this 
case, it is advantageous to convert the distribution to a normal-like one by 
applying a transformation such as 
y =xv 
(0 < v < l ) ,  
(3.77) 
which is called the power transformation. The normal-like is achieved by 
making y of (3.54), E ( ( Y - ~ ) ~  
I - E2((y-Y)*}, close to 2 under the condition 
thatE((~-?)~] 
= 1, wherey=E{y). 
Assuming a gamma density function of (2.54) for x, let us compute the 
moments of y as 
Therefore, 
(3.78) 

3 Hypothesis Testing 
77 
Note in (3.79) that the a's of the numerator are cancelled out with the a's of 
the denominator, and y is a function of p and v only. Figure 3-1 1 shows plots 
of y as a function of v for various values of p. Figure 3-11 indicates that we 
b 
011 012 
013 0:4 
015 0:s 017 
018 of9 1.b- 
Fig. 3-11 Relation between y and a for various p. 
can make y close to 2 for a wide range of p by selecting v = 0.4. This transfor- 
mation with v=O.4 was applied to obtain Data RADAR of Chapter 2. 

78 
Introduction to Statistical Pattern Recognition 
Reject Option 
When r(X) of (3.6) is close to 0.5, the conditional error of making the 
decision given X is high. So, we could postpone decision-making and call for 
a further test. This option is called reject. Setting a threshold for r(X), we 
may define the reject region, LR(t), and reject probability, R (t), as 
L R ( f )  = (XI r(x) 2 f )  , 
(3.80) 
R ( t )  = Pr(r(X) 2 t )  =I p(X) dX . 
(3.81) 
LR(r) 
Then, the resulting error, &(t), is 
~ ( t )  
= &pnrp 
Ip l ~ x ) ,  
dx , 
(3.82) 
where zR is the complementary region of L R .  When the minus-log likelihood 
test is used, (3.80) can be converted to 
inequalities 
are 
obtained 
from 
2 t 
when PIPI(X) > Pzp2(X), and 
2 t when PIpI(X) c P2p2(X), respec- 
tively. Thus, any sample X which satisfies (3.83) is rejected. On the other 
hand, the oI 
-sample satisfying h (X) > In (1 -f)/t + In P l/P2 and the 9- 
sample satisfying h (X) < - In (1 -tyr + In P I /P are misclassified. 
Figure 3-12 shows the relationship between & ( f )  and R(r) for a simple 
one-dimensional example. As seen in Fig. 3-12, as t increases from 0 to 0.5, 
&(I) increases from 0 to the Bayes error, and R (t) decreases from 1 to 0. 
Error-reject curve: The relation between R ( t )  and &(t) resembles the 
operating characteristics in which 
and &2 are related with decision threshold 
as the parameter. Therefore, the error-reject curve, which plots &(t) as the 

3 Hypothesis Testing 
79 
I
I
 
* x  
+ 
L, (t) 
Fig. 3-12 Reject region and probability. 
function of R(t), could be used as another indicator of the system behavior 
For example, we can study, by using the error-reject curve, the effect of 
[ 12- 131. 
the sample size used to design a decision rule as follows: 
Experiment 1: The error-reject curve for Data I-I 
Data: I-I (Normal, MTM =32,42) 
Dimensionality: n =5, 20, 100 
Classifier: Linear classifier of (3.84) 
Sample size: N I = N 2  =kn, k = 2,10,50 (Design) 
N I  
=N2=100n (Test) 
No. of trials: T =  10 
Results: Fig. 3-13 1131 
In this experiment, the two covariance matrices are the same, and the 
Bayes classifier becomes linear as in (3.12). In addition, since the effect of 
design sample size on the error-reject curve is our concern in this experiment, 
the M, and C in (3.12) are replaced by their respective estimates hi and 2 as 

80 
Introduction to Statistical Pattern Recognition 
n=5,20. 100: k-50 
n-5, 20, 100: k-10 
I f  
t\ 
/ / / / - -  
I 
.075 
n=5, 20, 100: kz50 
n=5,20,100: k=10 
.025 
0 
0 
.2 
.4 
.6 
.a 
1 .o 
Reject Probability a 
Fig. 3-13 Error-reject curves for Data 1-1. 
where Gi is the sample mean and 5 is the sample covariance matrix estimated 
from ( N l + N 2 )  samples. The test sample, which was generated independently 
of the design samples, was classified by using (3.83) and (3.84), and labeled 
according to either "correct", "error", or "reject". The numbers of error and 
reject samples were counted and divided by (N1+N2) to give &> and i ( t > ,  
respectively. A large number of test samples was used to minimize the varia- 
tion of the result due to the finite number of test samples. Figure 3-13 shows 
the error-reject curves, which are the averages of the 10-trial results. The mean 
performance depends almost entirely on the ratio k = N/n. As a rule of thumb, 
it appears that k must be 10 or greater for the mean performance reasonably to 
approximate the asymptotic one. This conclusion for the whole of the error- 
reject curves is an extension of the same conclusion for the error without rejec- 
tion. 

3 Hypothesis Testing 
81 
Experiment 2: The error-reject curve for Data I-A 
Data: 
Classifier: Quadratic classifier of (3.1 1) with GI, ii 
Sample size: 
No. of trials: z= 10 
Results: Fig. 3-14 [13] 
Case I - I-A (Normal, n = 8) 
Case I1 - I-A except M I = M 2 
N I = N = kn, k = 2,4,8,50 
(Design) 
N I  = N 2  = lOOn = g o 0  (Test) 
'25
Case I (data I-A) 
/Jk=50 
.2 
.4 
.6 
.8 
1 .o 
Reject Probability h 
Fig. 3-14 Error-reject curves for Data I-A. 
In this experiment, the two covariance matrices are different, and the 
Bayes classifier becomes quadratic as in (3.11). Again, MI and X, in (3.1 1) are 
replaced by their estimates Mi and &.. The resuIting error-reject curves are 
shown in Fig. 3-14. The general dependency on the ratio k = N h  is present, 
but now a somewhat larger number of design samples is needed for good 
results. The effect of the design sample size on the classification error will be 
readdressed in Chapter 5 in more detail. 
As the error-reject curve suggests, ~ ( t )  
may be expressed explicitly in 
terms of R(r). When r is increased by Ar, the reject region is reduced by 
L. 

82 
Introduction to Statistical Pattern Recognition 
ALR(t), resulting in a reduction of R ( t )  and an increase of &(t). However, as 
Fig. 3-12 suggests, the change of ~ ( t )  
occurs in two different ways, depending 
on the right and left sides of L&). That is, on the right side the ol-error 
increases, while on the left side the w;?-error increases. Therefore, defining 
AL I and AL2 as the change of the reject region in the right and left sides, 
A& = &(t+At) - &(t) = 
P Ip I(X)dX + 
P 2 p 2 ( X )  dX . 
(3.85) 
MI 
ALL2 
On the other hand, 
- M = R ( t ) - R ( t + A t ) = I  
p ( X ) d X + I  p ( X ) d X .  
(3.86) 
ml 
m2 
Since t E P I p I ( X ) / p ( X )  
in A L I  and t z P 2 p 2 ( X ) / p ( X )  in AL2, (3.86) can be 
modified to 
=I, 
P l p l ( X )  dX + 
P2p2(X)dX = A & .  
(3.87) 
I 
AL? 
Therefore, integrating (3.87) from 0 to t, 
E ( t )  = -[@R(<) . 
(3.88) 
Thus, once we know R(t), E@) can be computed by (3.88) [12]. 
Model validity tests: In pattern recognition, we have a set of data, and 
often assume a system model (the mathematical form of distributions) from 
which the data were drawn. A typical example is the normality assumption. 
Then, we need a procedure to test the model validity in order to assure a rea- 
sonable fit of the model with the data. Since the description of the model is a 
specification of probability distributions in n dimensions, it at first appears that 
we face the difficult problem of multivariate goodness of fit tests. We avoid 
this problem by using a transformation of the data to univariate statistics and 
apply goodness of fit tests in one dimension. The reject probability is one of 
the transformations. 
The reject probability of (3.81) reveals that 1-R(t) is the distribution 
function of a random variable I’ (X), P,.(r). 
P,.(t) = PI’ { r(X) < t 1 = 1 - R ( t )  . 
(3.89) 
Also, we know that &(t) is determined from R ( t )  by (3.88). These facts 

3 Hypothesis Testing 
83 
suggest that appropriate statistics of r ( X )  can be used for model validity tests, 
and that the error-reject curve is one option among many possible choices. 
Three other possibilities are listed as follows [ 131. 
(1) A rest bused on the mean of r(X): Since the Bayes error is E { r ( X ) } ,  
the sample mean of r ( X )  from the data can be compared with the Bayes error 
obtained from the model. This tests only one moment of the distribution of 
r (X). Therefore, although simple, this does not provide sufficient information 
to compare two models. 
(2) Chi-square goodness-of-fiit test: The empirical distribution function, 
P,.(r), is obtained from r ( X , ) ,  . . . ,r(X,), and compared with 1-R(t) of the 
model by the chi-square test. This procedure divides the space into a finite 
number of bins according to the reject threshold values. The test is conducted 
to compare the empirical probability in each bin with the predicted one. 
( 3 )  Kolmogorov-Smirnov test for- R(t): The empirical distribution func- 
tion of r-(X) is compared with 1-R(t) by measuring the maximum difference 
between them. 
For details regarding the use, definition, and critical values of these tests, 
A 
the reader is refered to [lo]. 
Composite Hypothesis Tests 
Sometimes p;(X) is not given directly, but is given by the combination of 
p (X IO;) and p (0; 
I mi), where p (X IO;) is the conditional density function of 
X assuming a set of parameters or a parameter vector O;, and p (0; 
I mi) is the 
conditional density function of €3; assuming class mi. In this case, we can cal- 
culate p ; ( X )  by 
pj(X) = j p  (X I 0 ; ) p  (0; 
I mi) dOi . 
(3.90) 
Once p,(X) is obtained, the likelihood ratio test can be carried out for 
p I (X) and p 2 ( X ) .  as described in the previous sections. That is, 
This is the composite hypothesis test. 

84 
Introduction to Statistical Pattern Recognition 
Example 9: 
Two distributions are known to be normal, with fixed 
covariance matrices XI and X 2  for given expected vectors M I  and M 2 .  The 
expected vectors M, and M2 are also known to be normally distributed, with 
the expected vectors M 
and M20 and covariance matrices K I  and K 2 .  Then 
according to (3.90), 
(3.92) 
This can be calculated by diagonalizing X; and K; simultaneously. The result 
is 
1 
2 
- -(MI - M;JKT1(M; - M;o)]dM; . 
1 
P;(X) = ( 2 ~ ) " ' ~  
I Xj+Kj I 
Knowing that p ; ( X )  is normal when p (X I M i )  and p (Mi 
I 0;) are normal, 
we can simply calculate the expected vector and covariance matrix of X assum- 
ing o;: 
= jM;p (M; Io;)dM; 
= M ; o ,  
E { (X - M;o)(X - Mj0)T Io; 
} 
= JIJ(x-M;o)(x-M;o)~p (X 1M;)dXlp (M; Io;) dM; 
= jp; + (M;-M;o)(M;-M;o)7]p 
(M; lo;) 
dM; 
= Z i + K ; .  
(3.94) 
(3.95) 
The result is the same as (3.93). 

3 Hypothesis Testing 
85 
3.3 Error Probability in Hypothesis Testing 
Associated with any decision rule is a probability of error.. The proba- 
bility of error is the most effective measure of a decision rule's usefulness. In 
general, the calculation of error probability is very difficult, although the con- 
cept is quite simple. In order to evaluate (3.8), we must perform an n- 
dimensional integration in a complicated region. A more promising procedure 
is to determine the density function of the likelihood ratio, and integrate it in 
the one-dimensional h-space, as in (3.9) and (3.10). This is possible for nor- 
mal distributions, and will be discussed in this section. However, if the distri- 
butions are not normal, finding the density function of h is very difficult. 
Thus, in many practical problems, we either employ experimental techniques 
such as Monte Carlo simulation, or we seek bounds on the error probabilities. 
We will discuss error bounds in the next section. 
Linear Boundaries 
When the distributions are normal with equal covariance matrices, 
Zl 
= Z2 = Z, the minus-log likelihood ratio becomes a linear function of X as 
shown in (3.12). Since (3.12) is a linear transformation from an n-dimensional 
space to one-dimension, h ( X )  is a normal random variable when X i s  a nor- 
mally distributed random vector. The expected value and variance of h (X) can 
be calculated as follows: 
Since E { X I mi } = Mi, (3.96) becomes 
I 
2 
q, = --(A42 
- M I ) Y ( M 2  - M , )  = -q , 
1 
2 
q 2  =+-(A42 
- M , ) Y ( M 2  - M , ) = + q .  
(3.96) 
(3.97) 
(3.98) 
Also, 

86 
Introduction to Statistical Pattern Recognition 
0’ = E  [ {  h (X) - 
IO;] 
= E [ (  (M2 - M I ) Y ( X  - M ; ) ] 2  I Oil 
= (M2 - M 1 ) Y E { ( X  - M,)(X - M;)T IO; } X 4 ( M 2  - M I )  
= (M* - M 1 ) Y ( M *  - M I )  = 2q . 
(3.99) 
The above holds because E { (X - Mj)(X - Mi)‘ IO, ] is Z, (=Z), as was shown 
in (2.13). 
Figure 3-15 shows the density functions of h ( X )  for o1 and y, 
and the 
Fig. 3-15 
Density functions of h ( X )  for normal distributions with equal co- 
variances. 
hatched parts correspond to the error probabilities which are due to the Bayes 
test for minimum error. Therefore, 
(3.100) 
(3.101) 
where a(.) is the normal error function of (3.39), and 

3 Hypothesis Testing 
87 
(3.102) 
PI 
p2 
t = l n - - ,  
a* = a: = a: = 27 , 
(3.103) 
Thus, when the density function of the likelihood ratio is normal, the probabili- 
ties of error can be obtained from the table of @(.). 
General Error Expression 
Error expression: Before computing the error of the quadratic classifier 
for normal distributions, let us express the error of a classifier in a general 
form. Let a classifier be 
Then, the ol 
-error is 
(3.104) 
(3.105) 
where u(.) is the step function. The second line is obtained by using the fact 
that the Fourier transform of a step function, u(h), is [K&(O) + lijw]. Like- 
wise, the 02-error i s  
(3.106) 
Then, the total error becomes 

88 
Introduction to Statistical Pattern Recognition 
E = PIE, + P*E2 
(3.107) 
where 
P ( x ) = p l P l ( x ) - ~ 2 P 2 ( x ) ~  
(3.108) 
That is, the error is a function of h ( X )  and p(X), which specify the classifier 
and the test distributions, respectively. 
Another interpretation of (3.105) is given as follows. Let us define the 
characteristic function of h (X) for wl, 
F I (a), 
as 
= jeJ'ph(h lal) dh . 
(3.1 09) 
That is, F,(w) may be obtained through an n-dimensional integration using 
p I (X) or through a one-dimensional integration using ph(h I al 
). Since F I (a) 
is the Fourier transform of ph(h lol), except for the sign of jo, the inverse 
Fourier transform from F I (0) to ph(h 101 ) is given by 
or 
(3.1 1 1) 
ph(-h I 0 , )  = -jF 
(o)eJwhdw . 
Equation (3.1 11) indicates that F I (0) is the Fourier transform of ph(-h I wl). 
The multiplication by [@a) + l/jw] in the Fourier domain corresponds to an 
integration in the time domain from -00 to 1. Therefore, from the second line 
of (3.105) and the first line of (3.109) 
1 
2rc 

3 Hypothesis Testing 
89 
1 
1 
= -j[x&o) + - ] F , ( o ) d o  
27c 
JO 
2lt 
JW 
1 
1 
= -j[TC6(o) + -IF, 
(w)e@do I 
= jomph(h lol) dh . 
(3.1 12) 
Likewise, for 02, 
multiplying by [7c6(o) - l/jo] in the Fourier domain 
corresponds to an integration in the time domain from t to +ob. Therefore, 
Procedure to compute the error: Thus, when a classifier and test distri- 
butions are given, the error of the classifier can be computed as follows [ 141: 
(1) 
Compute the characteristic function, F,(o), by carrying out the n- 
dimensional integration of (3.109). For quadratic classifiers with normal 
test distributions, the explicit expression for F,(o) can be obtained, as 
will be discussed later. 
Carry out the inverse operations of (3.105) and (3.106) as 
(2) 
(3.1 14) 
where + and - are used for i = 1 and 2, respectively. The real and ima- 
ginary parts of Fi(o) are even and odd. Therefore, the real and ima- 
ginary parts of F,(o)lw are odd and even, which lead us to the second 
line of (3.1 14). Note that this integration is one-dimensional. Therefore, 

90 
Introduction to Statistical Pattern Recognition 
we can carry out the integration numerically, even if F,(o) is a compli- 
cated function and not integrable explicitly. 
Example 10: 
In order to confirm the above discussion, let us study a 
simple case with two normal distributions: Nx(O,I) and Nx(M,I). From (3.12), 
the Bayes classifier becomes 
1 
2 
h (X) = MTX - -MTM , 
and the ol-density function Nx(O,I) is expressed by 
(3.1 15) 
(3.1 16) 
1 
2 
exp[--xTx]. 
P l ( X )  = 
~ (2X)"'* 
1 
1 
1 
2 
= INx( joM,I)exp 
- - j o M T M  
dX 
= exp I--MTM 
o2 
- 
2 
(3.1 17) 
Equation (2.24) shows the characteristic function of a multivariate normal dis- 
tribution, and (3.117) is a special case for one-dimension. Therefore, taking 
the inversion, 
(3.1 18) 
1 
(h + MTM22)2 
GdiZii [- 
2MTM 
Ph(h 1 0 1 )  = 
That 
is, 
the 
w,-distribution 
of 
h 
has 
E(hlol ) = - M T M / 2  
and 
Var( h I ol 
1 = M T M ,  which are identical to (3.97) and (3.99) for M 1=0, &=M, 
and CI=C2=I, 

3 Hypothesis Testing 
91 
Quadratic Classifiers 
For a normal test distribution: When the quadratic classifier of (3.1 1) 
is designed and tested on a normal distribution pT(X), h ( X )  and p7(X) can be 
given as 
and 
where t of (3.1 19) is a threshold. Applying simultaneous diagonalization and a 
coordinate shift, Y = A T(X-M,), 
ATXTA = I  
and 
A7(&'--Cs1)-'A = A .  
(3.121) 
Then, M, and Zj are converted to 
AT(Mj - MT) 
= Di 
and 
A7&A = K j  
(i=1,2). 
(3.122) 
Thus, in the Y-space 
1 
2 
h ( Y )  = -YTA-'Y - V'Y + c , 
(3.123) 
(3.124) 
where 
V = KT'D, - KT1D2 , 
(3.125) 
t .  
(3.126) 
1 
1 
IKlI 
c = - ( D ~ K T ' D ~  - D : K : ~ D ~ )  
+ 2 in - 
- 
2 
IK,I 
The characteristic function F T ( a )  can be computed now as 

92 
Introduction to Statistical Pattern Recognition 
(3.127) 
where yi. vi, and hi are the components of Y, V, and A respectively. Depend- 
ing on whether the error is generated by h ( X )  > 0 or h ( X )  < 0, the error must 
be computed as 
& T =  { 
(3.128) 
The integration of (3.128) must be carried out numerically. This integration is 
not simple but is possible, because it is one-dimensional. 
The result of (3.128) is quite general, because we may select the test dis- 
tribution independently of the parameters used for design [15]. However, the 
cases most frequently encountered in practice are MT = Mi and ZT = X i  
(2=1,2). Therefore, let us find vi. hi, and c for these cases. 
MT = MI and ZT = 21: 
In this case, we apply simultaneous diagonali- 
zation and a coordinate shift such that 

3 Hypothesis Testing 
93 
ATZIA = I ,  A r Z 2 A  = p, and A T ( M 2 - M I )  = L . 
(3.129) 
Then, (Z.;’-Z;’)-’ 
is also transformed to a diagonal matrix A by A as 
A = AT(Z,T’-ZT1)-‘A = AT[A (l-p-’)AT]-’A 
-I 
= (I-p )-I 
. 
(3.130) 
Since M T = M I ,  D I  = O  and D 2  =AT(M2-MI)=L. from (3.122). Also, 
K I  = I  and K 2  = c1 from (3.122) and (3.129). Therefore, inserting these into 
(3.125) and (3.126), Vand c are 
v = -p-IL , 
(3.131) 
(3.132) 
That is, after computing p and L by (3.129), we replace hi and vi of (3.127) 
by 
(3.133) 
where p, andi, are the components of p and L. Then 
first equation of (3.128). 
is computed by the 
MT = Mz 
and ZT = Z,: 
After applying the transformation of 
Y = A T ( X - M 2 )  where A is determined by (3.129), a further transformation of 
2 = p-”’Y is applied. Then, (3.129) is modified to 
-1/2 
p-1/2A 
T(M I -Ma) = -p L . 
(3.134) 
Also, (Zyl-Z;l)-l 
is diagonalized as 
A = p-1/2(1-p-1)-1p-1’2 
= (p-[)-I 
, 
(3.135) 
-112 
- I  
Since MT = M 2  this time, D I = -p 
L, D z  = 0, K 
= p , and Kz = I from 
(3.122) and (3.134). Therefore, inserting them into (3.125) and (3.126), V and 

94 
Introduction to Statistical Pattern Recognition 
c are computed as 
(3.137) 
That is, after computing p and L by (3.129), we calculate hi, 
vi, and c of 
(3.127) by using (3.135), (3.136), and (3.137). Then 
is computed by the 
second equation of (3.128). 
Example 11: The technique was applied to Data I-A (n =8) in which 
two normal distributions have significantly different covariance matrices. First, 
the density functions of h for ol 
and o2 are numerically computed by using 
(3.110) and plotted in Fig. 3-16 [14]. Note that these density functions are 
skewed from a normal distribution. The Bayes error was computed using 
(3.128), resulting in 
= 1.6%, 
= 2.2%, and 
E = 1.9% , 
(3.138) 
where P I = P 2  = 0.5 and t = 0 are used. 
Approximations 
Since the quadratic equation of (3.1 1) represents the summation of many 
terms, the central limit theorem suggests that the distribution of h (X) could be 
close to normal. If that is true, we only need to compute E(h(X)loj) and 
Var{ h (X) 
I wi ). Then, the error can be calculated from a normal error table. 
Expected value of h(X): The expected values can be calculated easily 
regardless of the distributions of X as follows: 
1 
2 
E(h(X)Iwl) = - tr [ ~ . ; ' E ( ( x - M ~ ) ( x - M , ) ' I ~ ~  
11 

3 Hypothesis Testing 
95 
h 
Fig. 3-16 Density functions of h (X) for Data I-A. 
1 
- - 
2 tr [Z;IE ( (X-M I )(X-M 
)T 
t .  
+ - l n  
-- 
1 
2 
IC*l 
(3.139) 
Likewise, for w2 

96 
Introduction to Statistical Pattern Recognition 
t .  
+-ln-- 
1 
lCll 
2 
1x21 
(3.140) 
Or, after simultaneous diagonalization from Z1 and X 2  to I and A by Y = A'X, 
1 "  
2 
I 
2 i = l  
hi 
E ( h ( Y ) I q )  = - x [ ( h j - l )  + (d2;-dli) + In -1 
- r , 
(3.141) 
(3.142) 
where dki is the ith component of Dp = ATMp. 
t = 0, 
An interesting property emerges from (3.141) and (3.142). That is, if 
E(h(Y)lwl) 1 0  and E(h(Y)lw2] 20 
(3.143) 
regardless of the distributions of X. These inequalities may be proved by using 
In x I x-1. 
From (3.141), (l-l/k;) + In (1/3Li) I O  and -(d2;-d;i)2/ki I O  yield 
E(h(Y)lw, 
} I O  for t = 0. 
Also, from (3.142), (kj-l)- In hi 2 0 and 
(d2i-dli)2 2OyieIdE(h(Y)ly} 2Ofort=O. 
Variance of h(X): The computation of the variance is more involved. 
Therefore, only the results for normal distributions are presented here. The 
reader is encouraged to confirm these results. It is suggested to work in the Y- 
space where the two covariances are diagonalized to I and A. 

3 Hypothesis Testing 
97 
3.4 Upper Bounds on the Bayes Error 
It is evident from the preceding discussion that the calculation of the 
error probability is, in general, a difficult task. Even when observation vectors 
have a normal distribution, we must resort to numerical techniques. However, 
a closed-form expression for the error probability is the most desirable solution 
for a number of reasons. Not only is the computational effort greatly reduced, 
since we need only to evaluate a formula, but more importantly, the use of the 
closed-form solution provides insight into the mechanisms causing the errors. 
This information is useful later when we consider the problem of feature selec- 
tion. 
When we cannot obtain a closed-form expression for the error probabil- 
ity, we may take some other approach. We may seek either an approximate 
expression for the error probability, or an upper bound on the error probability. 
In this section, we will discuss some upper. hounds of error pr.ohahility. 
The Chernoff and Bhattacharyya Bounds 
Chernoff bound: The Bayes error is given in (3.7) as 

98 
Introduction to Statistical Pattern Recognition 
An upper bound of the integrand may be obtained by making use of the fact 
that 
min[a, b ]  I aSbl-’ 
OIsIl 
(3.147) 
for a, b20. Equation (3.147) simply states that the geometric mean of two 
positive numbers is larger than the smaller one. The statement can be proved 
as follows. If a d ,  the left side of (3.147) is a, and the right side can be 
rewritten as ax(bla)’-S. Since (bla) > 1 and 1-5 2 0 for 0 I 
s I 1, the right 
side becomes larger than the left side. Likewise, if a>b, the left side of 
(3.147) is b, and the right side is rewritten as bx(a/b)’, which is larger than b 
because (alh) > 1 and s 2 0. Using the inequality of (3.147), E can be 
bounded by 
where E, indicates an upper bound of E. This E,, is called the ChernofS bound 
[16]. The optimum s can be found by minimizing E~,. 
When two density functions are normal as Nx(MI,C,) 
and NX(M2,C2), 
the integration of (3.148) can be carried out to obtain a closed-form expression 
for E,. That is, 
where 
(3.150) 
This expression of p(s) is called the ChernofS disrance. For this case, the 
optimum s can be easily obtained by plotting p(s) for various s with given Mi 
and Cj. The optimum s is the one which gives the maximum value for p(s). 

3 Hypothesis Testing 
99 
Bhattacharyya bound: If we do not insist on the optimum selection of 
s, we may obtain a less complicated upper bound. One of the possibilities is to 
select s = 1/2. Then, the upper bound is 
E,, = G
j
m
 
dX = G e - p ( 1 i 2 )  
(3.151) 
in general, and for normal distributions 
I 
I- C I +c2 
(3.152) 
1 
2 
+ - In 
2 m. 
The term p(1/2) is called the Bhattacharyya distance, and will be used as an 
important measure of the separability of two distributions [ 171. 
When CI = C2 = C, the Chemoff distance, (3.150), becomes 
(3.153) 
s (1-s) 
2 
W) = ~ 
(M2-M1)Y(M*-M1) . 
In this case, the optimum s can be obtained by solving 
dp(s) 
1-2s 
ds 
2 
-- - - ( M 2 - M 1 ) Y ( M 2 - M 1 )  
= 0 .  
(3.154) 
The solution is s = O S .  That is, the Bhattacharyya distance is the optimum 
Chemoff distance when XI = C2. 
As seen in (3.151), 
E, = d f  1f2exp[-p(1/2)] or 
In E, =-p(1/2) 
- In .\lp,p,. Figure 3-17 shows the relation between p(1/2) and E, for 
PI = f 2  = O S .  
Throughout this book, we use the Bhattacharyya distance rather than the 
Chemoff because of its simplicity. However, all discussions about the Bhatta- 
charyya distance in this book could be extended to the Chemoff. 
As seen in (3.152), the Bhattacharyya distance consists of two terms. 
The first or second term disappears when M I  = M Z  or C1 =X2, respectively. 
Therefore, the first term gives the class separability due to the mean-difference, 
while the second term gives the class separability due to the covariance- 

100 
1.0 
Introduction to Statistical Pattern Recognition 
7 
difference. It is important to know which term is dominant, because it deter- 
mines what type of a classifier must be designed for given distributions. 
Example 12: Figure 3-18 shows the Chernoff distance p(s) for DATA 
I-A. Although the two covariance matrices are significantly different in this 
case, the optimum s is so = 0.58, which is close to 0.5. Also, the resulting 
bound of E, = 0.5e-pL(S") 
= 0.046 (assuming P 1  = P 2  = 0.5) is very close to the 

3 Hypothesis Testing 
101 
2 .o 
on 
t - 
1 .o 
I 
1 .o 
0 .a 
0.6 9 
w 
0.4 
0.2 
0.0 
0.2 
0.4 0.5 0.6 
0.8 
1.0 
So =O .58 
Fig. 3-18 Error bound vs. s. 
Bhattacharyya bound of E, = 0.5 
is E = 0.019. 
= 0.048. The Bayes error for this data 
Example 13: Let us compute the Bhattacharyya distance between two 
normal distributions, Nx(O,I) and N,(O,A), which share the same mean. Since 
the first term of J.I 
disappears in this case, 
(3.155) 
Since ( 1 + L i ) / 2 c  2 1 regardless of the value of h, (h, is the variance and 
positive), In ( l + h , ) / 2 q  2 0, where the equality holds only when h, = 1 .  
Therefore, as n goes to 00 with L , # l ,  ~(112) can go to -. 
This example shows 
that, even if M , = M 2 ,  
the Bayes error in a high-dimensional space could 
become very small with different covariance matrices. 
Example 14: 
Let x, ( i = l , .  . . ,n) be independent and identically distri- 
buted random variables. The density functions of xi for o1 
and 02 are uniform 
in [0.4, 0.61 for ol 
and [0, 11 for w2. The Bhattacharyya bound for these two 

102 
distributions is 
Introduction to Statistical Pattern Recognition 
= =hf;‘l3dxj 
i = l  
= G 0 . 4 4 7 ”  . 
(3.156) 
Thus, E,, becomes small as n increases. When n = 1 and P 1 = P  2 =OS, E, is 
0.224 while the Bayes error is 0.1. 
Other bounds: Many other bounds can be derived similarly. One of 
them is the asymptotic nearesf neighbor error, which is a tighter upper bound 
of the Bayes error than the Bhattacharyya bound, as given by 
E I 2 1  p1p1(x)p2p2(x) 
dX I IJP l p l ( x ) P 2 p 2 ( x )  dX . 
(3.157) 
P (X) 
The inequalities are verified by proving min[a,h] I 2ahl(a+b) 5 dab for any 
positive a and h. If a>b, the left inequality becomes b < 2b/(l+b/a). Since 
bla < 1, the inequality holds. The case for a<h can be proved similarly. The 
right inequality holds, because a+h-2&6 
= (& + dh)* 2 0. 
These measures of class separability have a common structure. In the 
Bayes error, P , p , ( X )  and P2p2(X) are integrated in L2 and L 1  respectively, 
thus measuring the overlap of two distributions exactly. In both the nearest 
neighbor error and the Bhattacharyya bound, this overlap was approximated by 
integrating the product of P p l ( X )  and P2p2(X). However, in order to ensure 
that the dimension of the integrand is one of a density function, 
P I p  I (X)P2p2(X) is divided by the mixture density p ( X )  in the nearest neigh- 
bor error while the product is square-rooted in the Bhattacharyya bound. The 
properties of the nearest neighbor error will be discussed extensively in 
Chapter 7. 

3 Hypothesis Testing 
Validity of the Bhattacharyya Distance 
103 
The Bhattacharyya distance for norma. distributions, (3.152). is a very 
convenient equation to evaluate class separability. Even for non-normal cases, 
(3.152) seems to be a reasonable equation, measuring in the first term the dis- 
tance between M I  and M 2  normalized by the average covariance matrix, and in 
the second term the distance due to the covariance-difference. The question 
here is how widely (3.152) can be used. Since we cannot examine all possible 
non-normal distributions, we limit our discussion to a family of gamma distri- 
butions. Also, in order to avoid complexity, we present only one-dimensional 
cases. Note that, if two diagonalized covariance matrices are used, ~(112) of 
(3.152) is the summation of the Bhattacharyya distances of individual vari- 
ables. 
p for gamma densities: When two one-dimensional distributions are 
gamma as shown in (2.54), 1-d~ 
can be computed as 
where ai and pi are the parameters of the gamma distribution for ai. 
Or, tak- 
ing the minus-log of (3.158), 

1 04 
Introduction to Statistical Pattern Recognition 
(3.159) 
On the other hand, when (3.152) is used to compute p, 
(3.160) 
1 
p = -  
R 
2 
This is based on E (x I ai } = (Bi+l)/ai 
and Var{ x I mi 
} = (Pi+l)/aF as in (2.56) 
and (2.57). 
In many applications, PI and P2 are equal or close to each other. There- 
fore, in order to simplify the discussion of comparing (3.159) and (3.160). let 
us assume PI = P 2  = P. Then, 
and 
r 
7 
(3.161) 
(3.162) 
Figure 3-19 shows the relation between 
and pLR 
for various values of p. For 
a given p and po, (3.161) is solved to find the corresponding aI/az, 
which is 
inserted to (3.162) to compute p,. 
The values for 
are selected between 0 
and 2 which corresponds to E" between 0.5 and 0.068 from Fig. 3-17. Figure 
3-19 indicates that pg could be significantly different from p0, particularly for 
smaller P's. 
Variable transformation: The power transformation of (3.77) tends to 
convert a gamma distribution to a normal-like one. Therefore, it must make pg 

3 Hypothesis Testing 
105 
I.lg 
3 
2 
1 
I -0.9 
i 
2 
Pc 
Fig. 3-19 Relation between po and pg for gamma densities. 
closer to po so that pR could be used for a wider range of distributions. The 
mth order moments of the transformed variable, y, are given in (3.78). Thus, 
(3.163) 
(3.164) 
Then, using PI = P2 = p for simplicity, p,? in the y-space is 

106 
Introduction to Statistical Pattern Recognition 
(al/adV 
+(a2/aI)v 
2 
]+i 
In+ I [2]"+ 
[?Iv 1. 
(3.165) 
L 
-I 
Figure 3-20 shows the relation between 
of (3.161) and pg of (3.165) for 
v = 112, 1/4 and p = 0, -0.5. For larger p's, the curves become very close to 
the pg =po line. These curves indicate that po and pg are now much closer 
than the ones of Fig. 3-19. Thus, pg of (3.165) may be used for a wider range 
Once variables are transformed to normal-like distributions, we can 
evaluate the class separability more easily. Also, the design of a classifier 
becomes easier, because a standard quadratic classifier could be adopted, rather 
than designing a complicated classifier depending on the underlying distribu- 
tions. 
Before leaving this subject, we would like to point out two important 
properties of this variable transformation. 
The first point is that the correlation coefficients are relatively unaffected 
by the transformation of (3.77). In order to see this, let us expand yi = xr 
around xi = E { xi 1 by a Taylor series up to the first order term. 
of p. 
-v 
-v-l - 
y; = xy 2 x; + v x; 
(xi-Xi) . 
(3.166) 
Then, 
(3.167) 
(3.168) 
and 

3 Hypothesis Testing 
107 
Fig. 3-20 Relation between po and pLR 
after transformation. 
That is, by the first order approximation, py,,, = p ,,,, . Whether the first order 
approximation is good enough or not depends on the distribution. 
Example 15: Let us study an exponential distribution and v =  1/4. In 
the exponential distribution, p = 0 and E' 
{ x } = Var{ x } . Therefore, the second 
order approximation of (3.167) becomes 

108 
Introduction to Statistical Pattern Recognition 
-v 
v( 1-v) -v-2 
2 
E ( y )  E x  + ~ 
x E{(X-X)2 I 
v(1-v) 
-v 
= [1+- 
IX 
2 - 1/4 
"= (1 - 0.094)~ . 
(3.170) 
This is reasonably close to 5 which is the first order approximation. Prob- 
ably, the first order approximation in this case would be acceptable for qualita- 
tive discussions. 
The second point is that, by changing v of the transformation, the 
weights of the first and second terms of the Bhattacharyya distance vary. The 
smaller v is, the more the first term tends to dominate. That is, the class separ- 
ability comes more from the mean-difference than the covariance-difference. 
This means that we may have a better chance to design a linear classifier after 
the transformation with a small v. 
Furthermore, when two gamma density functions of x share the same P, 
we can achieve Var( y I w1 } = Var( y I w2 ) by using another popular log- 
tr-ansformation y = In x [18]. Suppose that x has a gamma density of (2.54) 
and we apply y = In x, then 
(3.171) 
where l-"(P+l> = dr(x)/dx I ptl . Therefore, 
(3.173) 
The integrations of (3.171) and (3.172) are obtained from an integral table [19]. 
Note from (3.173) that Var(y) is independent of a. Therefore, if two classes 
have different a's but the same P, the variance-difference between the two 
classes disappears, and the class separability comes from the mean-difference 
only. Thus, after the transformation, the Bhattacharyya distance in the y-space 
becomes 

3 Hypothesis Testing 
109 
(3.174) 
,% (P+l+i)* 
The relation between po of (3.161) and pCq of (3.174) is plotted in Fig. 3-21. 
Figure 3-21 shows that pR tends to be smaller than po. This could be accept- 
able, because G e x p [ - y ]  still gives an upper bound of the Bayes error 
although the bound is not as good as po. 
2 
1 
1 
2 
/lo 
Fig. 3-21 Relation between po and pLS after log transformation. 
The above advantage of the log-transformation is often cancelled by the 
disadvantage that the distribution of y tends to have a long negative tail when x 
is distributed in [0, I]. The tail generates more error when a standard quadratic 
or linear classifier is used. 
Thus, when the application of a transformation is considered, a careful 
study must be conducted, and a proper transformation suitable to the given dis- 
tributions must be selected. 

110 
Introduction to Statistical Pattern Recognition 
3.5 Sequential Hypothesis Testing 
In the problems considered so far, all of the information about the sam- 
ple to be classified is presented at one instant. The classifier uses the single 
observation vector to make a decision via Bayes rule since no further observa- 
tions will be made, and, as a result, we essentially have no control over the 
error, unless we can modify the observation process. 
In many practical problems, however, the observations are sequential in 
nature, and more and more information becomes available as time procedes. 
For example, the vibration of a machine is observed to determine whether the 
machine is in good or bad condition. In this case, a sequence of observed 
waveforms should belong to the same category: either "good" or "bad' condi- 
tion. Another popular example is a radar detection problem. Again the 
sequence of return pulses over a certain period of time should be from the 
same class: either existence or nonexistence of a target. A basic approach to 
problems of this type is the averaging of the sequence of observation vectors. 
This has the effect of filtering the noise and reducing the observed vectors 
down to the expected vector. Thus, it is possible, at least theoretically, to 
achieve zero error, provided that the expected vectors of the two classes are not 
the same. However, since obtaining an infinite number of observation vectors 
is obviously not feasible, it is necessary to have a condition, or rule, which 
helps us decide when to terminate the observations. The sequential hypothesis 
test, the subject of this section, is a mathematical tool for this type of problem. 
The Sequential Test 
Let X I , .  . . ,X, be the random vectors observed in sequence. These are 
assumed to be drawn from the same distribution and thus to be independent 
and identically distributed. Using the joint density functions of these m vec- 
tors, p j ( X j , .  . . .Xnl) (i = 1,2), the minus-log likelihood ratio becomes 

3 Hypothesis Testing 
111 
(3.175) 
where h ( X )  = - In p I ( X ) / p 2 ( X )  is the likelihood ratio for an individual obser- 
vation vector. The s of (3.175) is compared with a threshold such as InPIIP2 
for the Bayes classifier, and the group of the samples ( X I ,  . . . , X m )  is 
classified to wI or 02, depending on s c 0 or s > 0 (assuming 1nP I/P2 = 0). 
The expected values and variances of s for w1 and w2 are 
m 
E(slw;} = ~ . E ( h ( X j ) l w j )  
= m  qi , 
j = l  
(3.176) 
since the h (X,)’s are also independent and identically distributed with mean q; 
and variance 0;. 
When the Bayes classifier is used for h ( X ) ,  it can be proved that q I I 0 
and q2 2 0 as follows: 
(3.178) 

112 
Introduction to Statistical Pattern Recognition 
(3.179) 
where the inequalities are derived from In x I x - 1. The equalities in (3.178) 
and (3.179) hold only when pl(X) = p 2 ( X ) .  
Thus, as m increases, E { s I o1 
} decreases and E { s I o2 } increases in pro- 
portion to m, while the standard deviations increase in proportion to &. This 
is true regardless of p I (X) and p2(X) as long as p I (X) # p 2 ( X ) .  Therefore, the 
density functions of s for o1 and o2 become more separable as m increases. 
Also, by the central limit theorem, the density function of s tends toward a nor- 
mal distribution for large m. 
Example 16: 
In order to see the effect of m easily, let us study a sim- 
ple example in which h (X) is distributed as Nh(-q, 1) for w, and Nh(+q, 1) for 
02. Then, s is distributed as N,(-rnq,m) for o1 and N,(+mT,m) for 02. 
Therefore, the Bayes error of the sequential classifier for P I = P 2  = 0.5 is 
(3.180) 
where @(.) is the normal error function. Figure 3-22 shows the relation 
between E and m for various q. 
In practice, the pi(X)’s are not known, and the Bayes classifier is hard to 
design. Therefore, in place of the Bayes classifier, some classifiers such as the 
quadratic classifier of (3.1 1) and the linear classifier of (3.12) are often used. 
These two classifiers satisfy 
E(h(X)lwl} 
1 0  and 
E{h(X)Io2} 
2 0  
(3.181) 
regardless of the distributions of X as shown in (3.143), (3.97), and (3.98) 
respectively. Note here that (3.97) and (3.98) can be derived from (3.96) 
regardless of the selection of C. Therefore, by increasing m, we can make the 
errors of these classifiers as small as we like. However, note from (3.97) and 
(3.98) that E { h (X) 
I o1 
} = E ( h  (X) 
1% 
1 = 0 for M I  
= Mz. Therefore, when 
M I  
= M 2 ,  we cannot use the linear classifier of (3.12) for sequential operation. 

3 Hypothesis Testing 
113 
@--'(€) 
E % 
4
4
 
0 -  
-1 - 
-2 - 
1 
0.5 
1 
2 
3 
4 
5 
6 
7 
8 
9 
1
0
6
 
Fig. 3-22 Effect of the number of observations. 
On the other hand, the E [ h ( X )  I wi 1's for the quadratic classifier do not become 
zero unless both means and covariance matrices are the same for o, 
and 02, 
as 
seen in (3.139) and (3.140). 
The effectiveness of m for reducing the error is significantly diminished 
when samples and subsequently h(Xj)'s are correlated. This can be observed 
by computing the variance of s for correlated h's as 
ni 
Var(sIoj) = zVar(h(X,i)loiJ 
;=I 
That is, the second term does not disappear and contributes to increase 
Var{ s I mi 1. 
Example 17: 
Suppose that Var{h(X,)Io,} =o:, and E ( ( h ( X , )  
-q,)(h (Xk)-q,) I w, J = p I '-I ' 02. 
Then, the second term of (3.182) becomes 
20![p,(m-l)+p,'(m-2) 
+ . . . +p?-']. When p, = O S  and m=10 are used, 
it becomes 160:. 
Therefore, Var( s I w, ] = 260; instead of lMf for p, = 0. 

114 
Introduction to Statistical Pattern Recognition 
When p i =  1, Var(sIoi} =mo?+2o?[(rn-l)+ . . . +  l]=rn20?. There- 
fore, the error of the sequential classifier is the same as the one of a classifier 
with a single observation, regardless of the value of in. 
Multi-sensor fusion: The multi-sensor fusion problem may be handled 
in a similar way as the sequential test. Suppose that in different sensors (such 
as radar, infrared, and so on) are used to gather data, and the ith sensor gen- 
erates a vector X i  with ki measurements. Then, we form a vector with 
( k ,  + . . . + k,) component‘s, concatinating X I ,  . . . ,X,. 
However, often in 
practice, X, , . . . ,X, are mutually independent. Therefore, the Bayes classifier 
becomes 
o1 
P ,  
03 
p2 
>< ln-, 
(3.183) 
where - In px, (Xi 
I o1 )/px, 
(Xi 
I 02) 
is the minus-log likelihood ratio for the irh 
sensor outputs. The Bayes classifier for the multi-sensor system can be 
designed by computing the minus-log likelihood ratio for each individual sen- 
sor outputs, adding these ratios, and thresholding the summation. Note that 
(3.183) is similar to (3.175). However, there is a difference between the 
multi-sensor and sequential classifiers in that each likelihood function is dif- 
ferent for the multi-sensor classifier, while it is the same for the sequential 
classifier. When the outputs of different sensors are correlated, we need to 
treat the problem in the (k I + . . . +k,)-dimensional space. 
The Wald Sequential Test 
Wald sequential test: Instead of fixing in, we may terminate the obser- 
vations when s of (3.175) reaches a certain threshold value. That is 
s,, 2 a 
h I s,, 
+X’s E 01 , 
+ X’s E w2 , 
a < s,, < h + take the (rn+l)th sample , 
(3.184) 

3 Hypothesis Testing 
115 
where s, 
is used instead of s to indicate the number of observations, and a and 
b are thresholds to determine wl and 02, 
respectively. This decision rule is 
called the Wald sequential test [20]. 
The error of the Wald sequential test is controlled by a and b; that is, as 
the absolute values of a and h increase, the error decreases, while the number 
of observations required to reach the decision increases. The relation between 
the threshold values and the error can be expressed by 
02)dsj . 
(3.186) 
Theoretically, we should be able to find u and b from (3.185) and (3.186) for 
any given 
and ~ 2 .  
A simpler way to find the threshold values was developed by Wald. The 
procedure is as follows: At the mth observation, the likelihood ratio is tested as 
I B  + X ‘ s  
E y . 
Therefore, 
p 1 ( X , ,  . . . , X , ) d X I .  . . dX, 
m=l 
(3.187) 
(3.188) 

116 
Introduction to Statistical Pattern Recognition 
(3.189) 
The left side of (3.188) includes all X's which belong to w1 and are classified 
correctly; hence, it should be l - ~ ~ .  
On the other hand, the right side of (3.188) 
includes all X's which belong to 0 2  and are misclassified as 01; 
hence, it 
should be E ~ .  By the same argument, the left and right sides of (3.189) 
become 
and l - ~ ~ ,  
respectively. Therefore, (3.188) and (3.189) are rewritten 
as 
or 
1 -E1 
E2 
> A ,  
I B .  
El 
1 - E2 
(3.190) 
(3.191) 
(3.192) 
(3.193) 
Thus, for any given 
When the minus-log likelihood ratio is used, A and B should be converted to 
and ~ 2 ,  
A and B are obtained by (3.192) and (3.193). 
1 -E1 
E2 
Q =- InA 2-In - 
, 
(3.194) 
El 
b = - 1 n B  <-In -. 
1 - €2 
(3.195) 
When the increments h (Xi) 
are small, the likelihood ratio will exceed the thres- 
hold values A and B by only a small amount at the stage where 0; is chosen. 

3 Hypothesis Testing 
117 
Thus, the inequalities of the above equations can be replaced by equalities, and 
A and B are approximately determined by (1 
and €2 can be expressed in terms of A and B as 
and &,/(I - E ~ ) .  Or, 
B ( A  - 1) 
E1 ”= 
E 2 Z - .  
A - B  ’ 
1 - B  
A - B  
(3.196) 
(3.197) 
A few remarks concerning the properties of the Wald sequential test are 
(1) For the derivation of (3.192) and (3.193), Xl,X2,. . . do not need to 
in order. 
be independent and identically distributed. 
[201. 
achieve a given set of errors, E !  and ~2 [21]. 
(2) It can be proved that the Wald test terminates with probability 1 
(3) The Wald test minimizes the average number of observations to 
Expected number of observations: In the Wald sequential test, the 
average number of observations varies, depending on the distributions and the 
error we would like to achieve. In order to discuss this subject, let m be the 
number of observations needed to reach the upper or lower threshold value. 
The term m is a random variable. Equation (3.175) is rewritten as 
m 
j = l  
s = C h ( X j )  
(3.198) 
Then s should be either a or b of (3.184), with 
s = a (accept q) 
with probability 1 - E ,  wlhen X’s E 01 , 
s = a (accept 0,) 
with probability e2 
when X’s E 02 , 
s = b (accept w2) with probability 
when X’s E w1 , 
s = b (accept 0 2 )  with probability 1 - ~2 when X’s E 02 . 
(3.199) 
Therefore, 
E ( s l w , )  = a(l -el) + he, , 
(3.200) 

118 
Introduction to Statistical Pattern Recognition 
E{sl*) = a E z  + b(1 - E ; ? ) .  
(3.201) 
On the other hand, since (3.198) is a random sum, it is known that 
E ( s l o ; }  = E { E ( s l m , o i } }  =E{mqiIoj} =E(mIoj}qj , 
(3.202) 
where E { h (Xi) 
I mi ] is equal to qj, regardless of j .  Thus, the average number 
of observations needed to reach the decisions is 
~ ( l  
- E * )  +bel 
r l l  
E(mlm,) = 
, 
U E ~  
+ b(l - ~
2
)
 
rl2 
E ( m l y )  = 
(3.203) 
(3.204) 
Example 18: 
Let us consider an example with normal distributions. 
Then, h (Xi) 
becomes the quadratic equation of (3.1 l), and qj = E 1 h (Xi) 
Io; } 
is given by (3.139) or (3.140). On the other hand, we can select 
and &2 as 
we like, and a, b, a (1 - E I ) + b ~ , ,  
and 
+ b (1 - E * )  are subsequently deter- 
mined, as shown in Table 3-3. 
TABLE 3-3 
AVERAGE NUMBER OF OBSERVATIONS 
el =e2: io-* 
io4 
1 0 - ~  
io4 
-a = b 
4.6 
6.9 
9.2 
11.5 
13.8 
a ( l - ~ I ) + h ~ l :  -4.6 
-6.9 
-9.2 
-11.5 
-13.8 
U E ~  
+ h(1 - ~ 2 ) :  
4.6 
6.9 
9.2 
11.5 
13.8 
In order to get an idea how many observations are needed, let us con- 
sider one-dimensional distributions with equal variances. In this case, (3.97) 
and (3.98) become 
(3.205) 
(1112 - m d 2  
202 
(m2 - m I)* 
202 
and 
q2 =+ 
q l = -  
If we assume (m2 - m l ) / a =  1, then we have heavy overlap with 
E] = E ~  
= 0.31 by the observation of one sample. However, we can achieve 

3 Hypothesis Testing 
119 
lo4 as 
and &2 by observing an average of 27.6 samples. This indicates how 
errors can be significantly reduced by using a relatively small number of obser- 
vations. 
Computer Projects 
Two normal distributions are specified by the following parameters. 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
Generate 100 samples from each class. 
Design the Bayes classifier for minimum error by using given Mi, Xi and 
Pi (the theoretical classifier). Classify the generated samples by the 
classifier, and count the number of misclassified samples. 
Plot the theoretical distribution function derived from (3.73) and the 
empirical distribution functions of (3.71), and test the normality of the 
generated samples. 
Plot the operating characteristics by classifying the generated samples 
with the theoretical classifier. 
Plot the error-reject curve by classifying the generated samples with the 
theoretical classifier. 
Compute the theoretical Bayes error for the given normal distributions. 
Changing the threshold value t in Project 6, plot the theoretical operating 
characteristics and error-reject curve, and compare them with the results 
of Projects 4 and 5. 
Plot the Chemoff bound as a function of s, and find the optimum s and 
the minimum Chemoff bound. 
Perform the sequential classification for m =9 and 25. Generate 100 m- 
sample-groups-from each class and count the number of misclassified 
rn-sample-groups. 

120 
Introduction to Statistical Pattern Recognition 
Problems 
1. 
Two one-dimensional distributions are uniform in [0, 21 for 01 
and [ 1, 41 
for q, 
and P I  = P2 = 0.5. 
(a) 
Find the Bayes boundary for minimum error, and compute the 
Bayes error. 
(b) 
Plot the operating characteristics. 
(c) 
Find the Neyrnan-Pearson boundary with 
= 0.25. 
(d) 
Find the minimax boundary. 
(e) 
(f) 
Compute the Bhattacharyya bound. 
Two normal distributions are characterized by 
Compute the Chernoff bound, and find the optimal s. 
2. 
Pi =P2 =0.5, 
(a) 
Draw the Bayes decision boundary to minimize the probability of 
error. 
Draw the Bayes decision boundary to minimize the cost with 
c I I  = ~ ~ ~ = O a n d c ~ ~ = 2 c ~ , .  
(b) 
3. 
Repeat Problem 2 for 
4. 
Assuming that c I I  = c22 = 0 and c 12 =c21 in Problem 2, plot the rela- 
tionship between the threshold values of the likelihood ratio and the pro- 
babilities of errors. 
(a) 
Plot the operating characteristics. 
(b) 
Find the total error when the Neyman-Pearson test is performed 
with E ]  = 0.05. 
Find the threshold value and the total error for the minimax test. 
(c) 

3 Hypothesis Testing 
121 
(d) 
Plot the error-reject curve. 
Two normal distributions are characterized by 
5. 
PI = 0.6, 
P2 = 0.4 , 
ComputetheBayeserrorforcll =c22 =OandcI2 = c 2 ] .  
Show how to derive the variances of (3.144) and (3.145) for normal dis- 
tributions. 
Let xi (i=l, . . . ,n) be independent and identically distributed random 
variables, whose distributions are exponential with the parameters a, for 
o1 and a2 for w;?. Find E [ h (X) 
I wj ) where h (X) is the quadratic equa- 
tion of (3.1 l). 
The equivocation is given by 
6. 
7. 
8. 
Prove that the equivocation is larger than the asymptotic nearest neighbor 
error but smaller than the Bhattacharyya error bound. 
When two distributions are normal with an equal covariance matrix, Z, 
both the Bayes error, E, and the Bhattacharyya bound, E,, are expressed 
as functions of 1 = (M2-M l)TZ-' (M2-M). Plot E and E, vs. 1. 
Three distributions are normal with 
9. 
10. 
The cost matrix is 

122 
Introduction to Statistical Pattern Recognition 
where 0 5 a < 1 and P 2  = P 3  = p .  
(a) 
(b) 
Find the Bayes boundary and plot it in the X-coordinate system. 
Write an expression for the probabilities of errors. (Do not evalu- 
ate the integrals.) 
11. 
Two distributions are normal with 
P I  = P 2 = 0 . 5 ,  
M I  = i], M 2 =  11, 
E , = , , = [  1 0.5 1. 
0.5 
1 
(a) 
Calculate the threshold values for the Wald sequential test for 
= E ~  
= 
and lo-’. 
(b) 
(c) 
Find the average number of observations required. 
Fixing the number of observations as obtained in (b), compute the 
error of the sequential classifier with fixed m. 
References 
1. 
2. 
3. 
4. 
5. 
6. 
C. K. Chow, An optimum character recognition system using decision func- 
tions, Trans. IRE Electronic Computers, EC-6, pp. 247-254,1957. 
A. Wald, “Statistical Decision Functions,” Wiley, New York, 1950. 
D. Blackwell and M. A. Girshick, “Theory of Games and Statistical Deci- 
sions,” Wiley, New York, 1954. 
T. S. Furguson, “Mathematical Statistics: A Decision Theoretic 
Approach,” Academic Press, New York, 1967. 
H. L. Van Trees, “Detection, Estimation, and Modulation Theory: Part I,” 
Wiley, New York, 1968. 
B. J. Burdick, Private communication, 1984. 

3 Hypothesis Testing 
123 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
20. 
21. 
A. Papoulis, “Probability, Random Variables, and Stochastic Processes,” 
p. 250, McGraw-Hill, New York, 1965. 
K. Fukunaga, R. R. Hayes, and L. M. Novak, The acquisition probability for 
a minimum distance one-class classifier, Trans. IEEE Aerospace and Elec- 
tronic Systems, AES-23, pp. 493-499,1987. 
R. R. Parenti and E. W. Tung, A statistical analysis of the multiple-target, 
multiple-shot target acquisition problem, Project Report ‘IT-43, Lincoln 
Laboratory,M.I.T., 1981. 
G. E. Noether, “Elements of Non-Parametric Statistics,” Wiley, New York, 
1967. 
K. Fukunaga and D. L. Kessell, Error evaluation and model validation in 
statistical pattern recognition, Purdue University, Technical report TR-EE- 
72-73, Chapter 6,1972. 
C. K. Chow, On optimum recognition error and reject tradeoff, Trans. IEEE 
Inform. Theory, IT-l6,pp. 41-46,1970. 
K. Fukunaga and D. L. Kessell, Application of optimum error-reject func- 
tions, Trans. IEEE Inform. Theory, IT-18, 
pp- 814-817,1972. 
K. Fukunaga and T. F. Krile, Calculation of Bayes recognition error for two 
multivariate Gaussian distributions, Trans. IEEE Computers, C- 18, pp. 
L. M. Novak, On the sensitivity of Bayes and Fisher classifiers in radar tar- 
get detection, Proc. 18th Asilomar Conference on Circuit, Systems, and 
Computers, Pacific Grove, CA, 1984. 
H. Chernoff, A measure of asymptotic efficiency for tests of a hypothesis 
based on the sum of observations, Ann. Math. Stat., 23, pp. 493-507,1952. 
A. Bhattacharyya, On a measure of divergence between two statistical popu- 
lations defined by their probability distributions, Bull. Calcutta Math. Soc., 
S .  D. Martinez, Private communication, 1988. 
I. S. Gradshteyn and I. M. Ryzhik, “Tables of Integrals, Series, and Pro- 
ducts,“ Academic Press, New York, 1980. 
A. Wald, “Sequential Analysis,” Wiley, New York, 1947. 
A. Wald and J. Wolfowitz, Optimum character of the sequential probability 
ratio test,Ann. Math. Stat., 19, pp. 326-339, 1948. 
220-229,1969. 
35, pp. 99- 1 10, 1943. 

Chapter 4 
PARAMETRIC CLASSIFIERS 
The Bayes likelihood ratio test has been shown to be optimal in the 
sense that it minimizes the cost or the probability of error. However, in order 
to construct the likelihood ratio, we must have the conditional probability den- 
sity function for each class. In most applications, we must estimate these den- 
sity functions using a finite number of sample observation vectors. Estimation 
procedures are available, and will be discussed in Chapters 6 and 7. However, 
they may be very complex or require a large number of samples to give accu- 
rate results. 
Even if we can obtain the densities, the likelihood ratio test may be 
difficult to implement; time and storage requirements for the classification pro- 
cess may be excessive. Therefore, we are often led to consider a simpler pro- 
cedure for designing a pattern classifier. In particular, we may specify the 
mathematical form of the classifier, leaving a finite set of parameters to be 
determined. The most common choices are linear, quadratic, or piecewise 
classifiers which we will discuss in this chapter. 
First, we will consider under what conditions the Bayes classifier 
becomes quadratic, linear, or piecewise. We will then develop alternative 
methods for deriving "good" parametric classifiers even when these conditions 
are not met. 
The reader should be reminded, however, that the Bayes classifier is the 
1 24 

4 Parametric Classifiers 
125 
best classifier in all cases. No parametric classifier will exceed the perfor- 
mance of the likelihood ratio test. 
4.1 The Bayes Linear Classifier 
For two normal distributions, the Bayes decision rule can be expressed as 
a quadratic function of the observation vector X as 
1 
1 
-(X 
2 
- MdTC;'(X - M I )  - -(X 
2 
- M2)7C,'(X - M 2 )  
(4.1) 
When both covariance matrices are equal, that is when C, =C2 =C, (4.1) 
reduces to a linear function of X as 
(4.2) 
Furthermore, if the covariance matrix is the identity matrix, I, then we can 
view X as an observation corrupted by white noise. The components of X are 
uncorrelated and have unit variance. The Bayes decision rule reduces to 
There have been a number of classifiers, such as the correlation classifier 
and the matched filter, developed in the communication field for signal detec- 
tion problems [l]. We will discuss here how these classifiers are related to the 
Bayes classifier. 
Correlation Classifier 
The product MTX is called the correlation between Mi 
and X. When X 
consists of time-sampled values taken from a continuous random process, x(f), 
we can write the correlation as 
M:X = &?i(fj)xq) . 
j = l  
In the continuous case, the correlation becomes an integral, that is 
(4.4) 

126 
Introduction to Statistical Pattern Recognition 
0 bserved 
at t = T  
m,(t) 
Fig. 4-1 Block diagram of a correlation classifier. 
We can see that the classifier (4.3) compares the difference in the correlations 
of X with M 1  
and M 2  with a threshold to make a decision. Thus, we may call 
it a correlation classifier. The structure of the correlation classifier is shown in 
Fig. 4-1, and is written as 
WI 
cu, 
MYX - MTX 5 c . 
(4.6) 
If c is selected as (MTM, 
- M;M2)/2 - In P I/P2, (4.6) becomes identical to 
(4.3). Thus, in order for the correlation classifier to be the Bayes classifier, the 
distributions must be normal with the equal covariance matrix I for both o1 
and 02. 
Matched Filter 
The correlation between M i  and X can also be considered as the output 
(4.7) 
of a linear filter. Suppose we construct functions g ; ( t )  such that 
gi(T - t )  = mi(r) . 
The relation between gi(t) and mi(f) is illustrated in Fig. 4-2. Then, clearly, 
Thus, the correlation is the output of a linear filter whose impulse response is 
gi(t). This filter is called a matchedflter. The matched filter classifier, which 

4 Parametric Classifiers 
127 
( 0 )  
( b )  
Fig. 4-2 Relation between mi@) and g j ( t ) .  
Observed 
at t = T  
- 1 - 
X(t)€W, 
Filter No 1 
g,(t) 
C 
Fig. 4-3 Block diagram of a matched filter classifier. 
performs the same function as the correlation classifier, is shown in Fig. 4-3. 
Again, the matched filter becomes the Bayes classifier with a proper threshold, 
when the distributions are normal with the equal covariance I. 
Distance Classifier 
The correlation and matched filter classifiers are directly related to 
Suppose we multiply (4.3) by 2, and then add and subtract XTX from the 
another popular classifier called a distance cfuss$er as follows. 
left-hand side. The resulting decision rule is 
(XTX - 2MTX + M:M ) - (XTX - 2M:X + M ; M 2 )  
(4.9) 
or 

128 
Introduction to Statistical Pattern Recognition 
Now the decision rule has the geometrical interpretation of comparing the 
Euclidean distances from X to M I ,  and M 2  according to a threshold. When 
P I  = P 2  = 0.5, the decision boundary is the perpendicular bisector of the line 
joining M I  and M2, as shown in Fig. 4-4. 
Nonwhite Observation Noise 
In the more general case when X I  = C2 + I, the observation noise is 
correlated and is often called colored noise. The Bayes classifier of (4.2) 
should be used in this case instead of (4.3). However, it is still useful to view 
the decision rule of (4.2) as a correlation classifier or a distance classifier. To 
see this, we introduce the "whitening" transformation, Y = ATX, where 
A ~ X A  = I .  
(4.1 1) 
It is important to note that as long as C is positive definite, A exists and is non- 
singular. Thus, the whitening transformation is reversible, and the observation 
Y can be classified as effectively as X. 
The expected vector of Y is 
Di = E ( Y l o ; }  =ATMi 
(i = 1,2) 
(4.12) 
for class ai, 
and the covariance of Y is I for both classes. Hence, all of the 
discussion of the preceding section applies to Y if we replace M; [or rni(t)] 
with Di 
[or d,(t)]. 
In the continuous time case, the transformation becomes an integral as 
7 
Y = A T X  + y(t)=ta(r,T)x(T)dT. 
(4.13) 
The kernel, a (r, T), can be viewed as the impulse response of a whitening filter. 
A possible structure for this classifier is shown in Fig. 4-5. We see that we 
have the correlation classifier of Fig. 4-1 modified by the addition of whitening 
filters. 
Example 1: 
Figure 4-6 shows a two-dimensional example in which a 
whitening transformation is effective. Although the two distributions of Fig. 

4 Parametric Classifiers 
X 
129 
Fig. 4-4 A classifier by Euclidean distances. 
4-6(a) are very much separable by the Bayes classifier of (4.2), the bisector 
classifier or simple correlation gives a poor classification. The narrow distribu- 
tions of Fig. 4-6(a) occur when x1 and x2 are highly correlated. Particularly, 
when xI,x2,. . . are the time-sampled values of waveforms, adjacent xi's are 
usually highly correlated and show this type of distribution. The whitening 
transformation changes these two distributions to the circular ones of Fig. 4- 
6(b) such that the Bayes classifier becomes the bisector. 
Other Bayes Linear Classifiers 
The Bayes classifier becomes linear for some other distributions such as 
independent exponential distributions and the distributions of independent 
binary variables. We will discuss these cases in this section. 
Independent exponential distributions: When the xi's are mutually 
independent and exponentially distributed for both a, and a2, 
the Bayes 
classifier becomes linear as shown in (3.18). 
Independent binary variables: When the x,'s are binary, either +1 or 
-1, the density function of xi for a, is expressed by 

130 
Introduction to Statistical Pattern Recognition 
x(t) 
z 
A Correlation classifier 
=- Cdt, T ) X ( T )  dT 
for white noise 
(Fig. I) - 
Fig. 4-5 A correlation classifier for colored noise. 
P,(Xi =xilo;} = Pjj 
'J 
, 
(4.14) 
( l+.Vj)12 ( - p ,  . )( I-.l, Y2 
where 
Pji +,(Xi 
= 1 I W j )  . 
(4.15) 
Note that (4.14) becomes P ,  for xj = + 1 ,  and (1 - Pii) for xi = -1. 
For 
x2 
y2 
= 
X I  
( 0 )  
Bisector = 
class if ier 
Fig. 4-6 Effect of a whitening process. 
independent xi's, the density function of the binary random vector X is 

4 Parametric Classifiers 
n 
P r { X = X l o i }  = ~ P r { x j = x j I o i )  
. 
/ = I  
Thus, the minus-log likelihood ratio of (4.16) becomes 
P, { x = x I 0 ,  ] 
P,{ x = x I o 2  ] 
h (X) = -In 
131 
(4.16) 
This is a linear function of xi. 
4.2 Linear Classifier Design 
Linear classifiers are the simplest ones as far as implementation is con- 
cerned, and are directly related to many known techniques such as correlations 
and Euclidean distances. However, in the Bayes sense, linear classifiers are 
optimum only for normal distributions with equal covariance matrices. In 
some applications such as signal detection in communication systems, the 
assumption of equal covariance is reasonable because the properties of the 
noise do not change very much from one signal to another. However, in many 
other applications of pattern recognition, the assumption of equal covariance is 
not appropriate. 
Many attempts have been made to design the best linear classifiers for 
normal distributions with unequal covariance matrices and non-noma1 distribu- 
tions. Of course, these are not optimum, but in many cases the simplicity and 
robustness of the linear classifier more than compensate for the loss in perfor- 
mance. In this section, we will discuss how linear classifiers are designed for 
these more complicated cases. 
Since it is predetermined that we use a linear classifier regardless of the 
given distributions, our decision rule should be 

132 
Introduction to Statistical Pattern Recognition 
0 1  
o? 
h ( X )  = vTx + v, >< 0 
(4.18) 
The term h ( X )  is a linear function of X and is called a linear- discriminant 
function. Our design work is to find the optimum coefficients V = [ V I  . . . v,lT 
and the threshold value v,, for given distributions under various criteria. The 
linear discriminant function becomes the minus-log likelihood ratio when the 
given distributions are normal with equal covariance matrices. 
However, the reader should be cautioned that no linear classifiers work 
well for the distributions which are not separated by the mean-difference but 
separated by the covariance-difference. In this case, we have no choice but to 
adopt a more complex classifier such as a quadratic one. The first and second 
terms of the Bhattacharyya distance, (3.152), will indicate where the class 
separability comes from, namely mean- or covariance-difference. 
Optimum Design Procedure 
Equation (4.18) indicates that an n-dimensional vector X is projected 
onto a vector V, and that the variable, y = VTX, in the projected one- 
dimensional h-space is classified to either o1 or %, depending on whether 
y < -v, or y > -v,,. 
Figure 4-7 shows an example in which distributions are 
projected onto two vectors, V and V’. On each mapped space, the threshold, 
vo, is chosen to separate the wI- 
and @*-regions, resulting in the hatched error 
probability. As seen in Fig. 4-7, the error on V is smaller than that on V’. 
Therefore, the optimum design procedure for a linear classifier is to select V 
and v, which give the smallest error in the projected h-space. 
When X is normally distributed, h (X) of (4.18) is also normal. There- 
fore, the error in the h-space is determined by qi = E { h ( X )  loi] and 
0’ = Var(h(X)Ioi), 
which are functions of V and v,. 
Thus, as will be dis- 
cussed later, the error may be minimized with respect to V and v,,. Even if X 
is not normally distributed, h (X) could be close to normal for large n, because 
h (X) is the summation of n terms and the central limit theorem may come into 
effect. In this case, a function of qi and 0’ could be a reasonable criterion to 
measure the class separability in the h-space. 

4 Parametric Classifiers 
133 
Fig. 4-7 An example of linear mapping. 
The expected values and variances of h (X) are 
q; = E ( h ( X ) l w ; }  = VTE(Xlw;} + V(] = VTM; + v, , 
(4.19) 
Let f (7-1 I ,q2,07,0i) be any criterion to be minimized or maximized for 
determining the optimum V and v,. Then, the derivatives off with respect to 
V and v, are 
(4.21) 
(4.22) 
On the other hand, from (4.19) and (4.20) 

134 
Introduction to Statistical Pattern Recognition 
ao: 
hi 
-- 
av -2x;v, 
--=Mi, 
av 
(4.23) 
(4.24) 
Substituting (4.23) and (4.24) into (4.21) and (4.22), and equating (4.21) and 
(4.22) to zero, 
(4.25) 
(4.26) 
Substituting (4.26) into (4.25), and solving (4.25) for V, the optimum V can be 
computed. However, it should be noted that the error in the h-space depends 
only on the direction of V, and not on the size of V. Therefore, for simplicity, 
we eliminate any constant term (not a function of Mi and E;) multiplying to V, 
resulting in 
v = [SEI + (l-S)Z2]--1(M2 - M , )  
, 
(4.27) 
where 
(4.28) 
Note that the optimum V has the form of (4.27) regardless of the selection off. 
The effect off appears only in s of (4.28). In (4.2), the Bayes classifier for 
normal distributions with the equal covariance matrix X has V = C-I(M2-MI). 
Replacing this C by the averaged covariance matrix [sCl+(l-s)Z2], we can 
obtain the optimum V of (4.27). 
Once the functional form off is selected, the optimum v, is obtained as 
the solution of (4.26). 
Example 2: Let us consider the Fisher criterion which is given by 

4 Parametric Classifiers 
(rll -112)2 
f =  o:+o; 
135 
(4.29) 
This criterion measures the difference of two means normalized by the aver- 
aged variance. The derivatives off with respect to 0: and 0; 
are 
(4.30) 
Therefore, s = 0.5 and the optimum V is 
(4.3 1) 
1 
1 
2 
2 
v = [--XI + -22]-'(M2 - M I )  . 
The h ( X )  with V of (4.31) and the resulting linear classifier are called the 
Fisher discriminant function and Fisher linear classifier, respectively [2]. The 
Fisher criterion does not depend on v,, because the subtraction of q2 from q l  
eliminates v, from (4.19). Therefore, we cannot determine the optimum v, by 
maximizing this criterion. 
Example 3: Another possible criterion is 
p 1 d  + p2rl; 
f =  P l o : + P , o ;  . 
(4.32) 
This criterion measures the between-class scatter (around zero) normalized by 
the within-class scatter, and will be discussed in Chapter 10. For this criterion, 
(4.33) 
Thus, s = P I and the optimum V is 
V = [ P , &  +P22J1(M2 -MI). 
(4.34) 
On the other hand, 
Substituting (4.35) into (4.26), and rewriting (4.26) by using (4.19) 
VT[PIMI + P 2 M 2 ] +  v, = 0 ,  
(4.35) 
(4.36) 
or 

136 
Introduction to Statistical Pattern Recognition 
v, = -VT[P,M, + P,M2]. 
(4.37) 
Equation (4.37) indicates that, if V is multiplied by a constant a, v, is also 
changed by a factor of a. The decision made by VTX + v, >< 0 is equivalent 
to the decision of aVTX + avo >< 0 for any positive a. This confirms that the 
scale of V is irrelevant in our discussion. 
Optimum Design for Normal Distributions 
Theoretical approach: When the distributions of h (X) are normal, we 
can find the V and v o  which minimize the Bayes error in the h-space. The 
Bayes error in the h-space is expressed as a function of qj and 0' as 
For this criterion, the derivatives of E are 
Therefore, from (4.26) 
(4.38) 
(4.39) 
(4.40) 
(4.41) 
(4.42) 
(4.43) 
That is, v, must be selected to make the two density functions of h (X) equal at 
h ( X )  = 0. Substituting (4.43) into (4.39) and (4.40), and using (4.28) 

4 Parametric Classifiers 
137 
(4.44) 
and 
[SC, + (I-S)C2l v = (M2 - M I ) ,  
(4.45) 
where s stays between 0 and 1 because q l  <O and q2>0. Thus, if we can find 
V and v, which satisfy (4.43) and (4.45), these V and v, minimize the error of 
(4.38) [3]. Unfortunately, since q i  and I$ 
are functions of V and v,, the expli- 
cit solution of these equations has not been found. Thus, we must use an itera- 
tive procedure to find the solution. 
Before discussing the iterative process, we need to develop one more 
equation to compute vo from s and V. This is done by substituting q 1  and q2 
of (4.19) into (4.44), and by solving (4.44) for v~,. The result is 
so:VTM* + (l-s)o:VTMI 
so: + (I-s)oZ 
v, = - 
(4.46) 
The iterative operation is carried out by changing the parameter s with an 
increment of As as follows [4]: 
Procedure I to find s (the theoretical method): 
(1) Calculatc V for a given s by 
(2) Using the V obtained, compute of by (4.20), 
by (4.19) in that sequence. 
v = [SC, + (I-s)C2]-I(M* 
- M I ) .  
by (4.46), and ql 
(3) Calculate E by (4.38). 
(4) Change s from 0 to 1. 
The s which minimizes E can be found from the E vs. s plot. 
The advantage of this process is that we have only one parameter s to 
adjust. This makes the process very much simpler than solving (4.43) and 
(4.45) with n + 1 variables. 
Example 4: 
Data I-A is used, and E vs. s is plotted in Fig. 4-8. As 
seen in Fig. 4-8, E is not particularly sensitive to s around the optimum point. 

138 
E 
0.15 
0.10 
0.05 
Introduction to Statistical Pattern Recognition 
I 
I 
0 
I 
I 
I 
I 
I 
I 
I 
I 
0 
0.2 
0.4 
0.6 0.67 0.8 
Fig. 4-8 Error vs. s. 
The optimized error is 5% by the best linear discriminant function, while the 
Bayes classifier with a quadratic form gives 1.996, as shown in Example 3- 1 1. 
Sample-based approach: The iterative process mentioned above is 
based on the closed-form expression of the error. Also, Mi and C j  are assumed 
to be given. However, if only a set of samples is available without any prior 
knowledge, Mi and Zi must be estimated. Furthermore, we could replace the 
error calculation by an empirical error-counting based on available samples. 
Assuming that N samples are available from each class, the procedure to find 
the optimum linear classifier is as follows. 

4 Parametric Classifiers 
139 
Procedure I1 to find s (the resubstitution method): 
A 
1 
(1) Compute the sample mean, M i ,  and sample covariance matrix, Zj. 
(2) Calculate V for a given s by V = [siI + (I-S)%J’($~-$~). 
(3) Using 
the 
V 
obtained, compute yy) = V T X f )  (i = 1,2; 
(4) The y;” 
and Y ) ~ ” S ,  which do not satisfy y;” c -v, 
and 
j = 1, . . . ,N), where Xy’ is the jth oj-sample. 
y j  
(2) > -v,, 
are counted as errors. Changing v, from -- to -tm, 
find the v, which gives the smallest error. 
(5) Change s from 0 to 1, and plot the error vs. s. 
Note that in this process no assumption is made on the distributions of X. 
Also, the criterion function, fi is never set up. Instead of using an equation for 
ft the empirical error-count is used. The procedure is based solely on our 
knowledge 
that 
the 
optimum 
V 
must 
have 
the 
form 
of 
In order to confirm the validity of the procedure, the following experi- 
[SEI + (l-s)x*]-yM*-M,). 
ment was conducted. 
Experiment 1: Finding the optimum s (Procedure 11) 
Data: I-A (Normal, n = 8, E = 1.9%) 
Sample size: N I = N 2  = 50, 200 
No. of trials: z = 10 
Results: Fig. 4-8 
Samples were generated and the error was counted according to Procedure 11. 
The averaged error over 10 trials vs. s is plotted in Fig. 4-8. Note that the 
error of Procedure I1 is smaller than the error of Procedure I. This is due to the 
fact that the same samples are used for both designing and testing the classifier. 
This method of using available samples is called the resubstitution merhod, and 
produces an optimistic bias. The bias is reduced by increasing the sample size 
as seen in Fig. 4-8. In order to avoid this bias, we need to assure independence 
between design and test samples, as follows: 

140 
Introduction to Statistical Pattern Recognition 
Procedure Ill to find s (the holdout method): 
(1) Divide the available samples into two groups: one is called the 
design sample set, and the other is called the test sample set. 
(2) Using the design samples, follow steps (1)-(4) of Procedure I1 to 
find the V and v, for a given s. 
(3) Using V and v, found in step (2), classify the test samples by 
(4.18), and count the number of misclassified samples. 
(4) Change s from 0 to 1, and plot the error vs. s. 
In order to confirm the validity of Procedure 111, the following experi- 
ment was conducted. 
Experiment 2: Calculation of the error (Procedure 111) 
Data: I-A (Normal, n = 8, E = 1.9%) 
Sample size: N ,  = N 2  = 50, 200 (Design) 
N ,  = N ,  = 50, 200 (Test) 
No. of trials: z = 10 
Results: Fig. 4-8 
Again, samples were generated and the error was counted according to Pro- 
cedure 111. The averaged error over 10 trials vs. s is plotted in Fig. 4-8. The 
error of this procedure is larger than the error of Procedure I at the optimum s. 
This method of using available samples is called the holdout method, and pro- 
duces a pessimistic bias. As N goes to =, both the optimistic and pessimistic 
biases are reduced to zero, and the errors of Procedures I1 and I11 converge to 
the error of Procedure I at the optimum s. Also, Fig. 4-8 shows that Procedure 
I does not give as good a performance as Procedures I1 and I11 when s is not 
optimum. This is due to the use of (4.46) to determine v o  for the entire region 
of s. Equation (4.46) is the condition for vo to satisfy at the optimum point. 
When s is not optimum, (4.46) may not be an appropriate equation to obtain 
the best y o .  In Data I-A, the two covariance matrices are significantly dif- 
ferent. Thus, the averaged covariance [sXI+(l-s)Xz] varies wildly with s. 
Despite this variation, both Procedures I1 and I11 keep the error curves flat for a 
wide range of s by adjusting the threshold V O .  This indicates that the proper 
selection of v o  is critical in classifier design. 

4 Parametric Classifiers 
141 
Since Procedures I1 and I11 produce different s’s, V’s, v,’s, and E’S, we 
need to know which s, V, v,, and E to use. Once a classifier has been designed 
by using N samples and implemented, the classifier is supposed to classify 
samples which were never used in design. Therefore, the error of Procedure I11 
is the one to indicate the performance of the classifier in operation. However, 
the error of Procedure 111 alone does not tell how much the error can be 
reduced if we use a larger number of design samples. The error of the ideal 
classifier, which is designed with an infinite number of design samples, lies 
somewhere between the errors of Procedures I1 and 111. Therefore, in order to 
predict the asymptotic error experimentally, it is common practice to run both 
Procedures I1 and 111. As far as the parameter selection of the classifier is con- 
cerned, we can get better estimates of these parameters by using a larger 
number of design samples. Therefore, if the available sample size is fixed, we 
had better use all samples to design the classifier. Thus, the s, V, and v ,  
obtained by Procedure I1 are the ones which must be used in classifier design. 
Before leaving this subject, the reader should be reminded that the cri- 
teria discussed in this section can be used to evaluate the performance of a 
linear classifier regardless of whether the classifier is optimum or not. For a 
given linear classifier and given test distributions, yi and of are computed 
from (4.19) and (4.20), and they are inserted into a chosen criterion to evaluate 
its performance. When the distributions of X are normal for both o1 
and ~
2
,
 
h (X) becomes normal. Thus, we can use the error of (4.38). 
Optimum Design of a Nonlinear Classifier 
So far, we have limited our discussion to a linear classifier. However, 
we can extend the previous discussion to a more general nonlinear classifier. 
General nonlinear classifier: Let y ( X )  be a general discriminant func- 
tion with X classified according to 
(4.47) 
Also, let f(y,,q2,s:,s3) be. the criterion to be optimized with respect to y ( X ) ,  
where 

142 
Introduction to Statistical Pattern Recognition 
Since s’=q’+o?, f (ql,q2,s:,s$) is a function of ql, q 2 ,  o:, and 
also. 
The reason why s’ is used instead of 0’ is that the second order moments are 
easier to handle than the central second order moments in which the means 
must be subtracted. The variation off, Sf, due to the variation of y(X), 6y(X), 
is expressed by 
where Sqi and 8s; are computed from (4.48) and (4.49) as 
(4.5 1) 
(4.52) 
Substituting (4.5 1) and (4.52) into (4.50), 6f becomes 
In order to make Sf = 0 regardless of Sy(X), the [.] term in the integrand must 
be zero. Thus 

4 Parametric Classifiers 
143 
where 
(4.56) 
Note that af/aof = (af/asf)(as;/ao;) = af/as; 
since sf = of + qf. The 
optimum solution, (4.54), may be interpreted as y ( X )  = a 1q I(X)+u2q2(X) = 
a2+(a I-a2)q I (X), where q I ( X )  is a posteriori probability function of ol with 
a priori probability of s. On the other hand, the Bayes classifier is 
q I ( X )  5 q2(X), and subsequently the Bayes discriminant function is 
h ( X )  = q2(X)-9, ( X )  = 1-2q I (X) >< 0. Therefore, if we seek the discriminant 
function by optimizing a criterion f (q1,q2,s:,sg), we obtain the Bayes 
discriminant function as the solution, except that different constants are multi- 
plied and added to q (X). The difference in the added constants can be elim- 
inated by adjusting the threshold, and the difference in the multiplied constants 
does not affect the decision rule, as was discussed previously. 
The above result further justifies the use of the criterion f (q 1 ,q2,of,og). 
The criterion not only provides a simple solution for linear classifier design, 
but also guarantees the best solution in the Bayes sense for general nonlinear 
classifier design. This guarantee enhances the validity of the criterion, 
although the above analysis does not directly reveal the procedure for obtaining 
the optimum nonlinear solution. 
Linear classifier: When we limit the mathematical form of y ( X )  to 
y = V‘X, the variation of y ( X )  comes from the variation of V. Therefore, 

144 
Introduction to Statistical Pattern Recognition 
6 y ( X )  = 6V'X 
. 
(4.57) 
Inserting (4.57) into (4.53), 
and using y (X) 
= V'X 
= X'V, 
1 
where Si = E(XXT lo,) is the autocorrelation matrix of 0,. 
Since 6f = 0 
regardless of 6V', [.] must be zero. Thus, 
where af/asy is replaced by af /do: in order to maintain the uniformity of 
expressions. Note that (4.59) is the same as (4.25), except that Si is used in 
(4.59) while Ci is used in (4.25). This is due to the difference in criteria we 
used; f (q1,q2,s:,s$) for (4.59) and f(q1,q2,0:,o~) 
for (4.25). Since 
s' =qy+o?, f (q1,q2,s:,s$) is also a function of ql, q2, o:, and 0:. There- 
fore, both (4.59) and (4.25) must give the same optimum V. 
In order to confirm the above argument, let us prove that 
V = [sS I+(l-s)S2]-1 [(af/&ll)Ml+(df/&12)M2] 
is the same vector as 
V = [sZI+(1-s)C2]-' [@f /&ll)Ml+(af 
/&12)M2] except for its length. Since 
the result must be independent of where the coordinate origin is, let us choose 
M as the coordinate origin for simplicity. Then, M I  and M2 are replaced by 
0 and M = M2-M I .  Ignoring the constant (af/&12), 
we start from 
v = [SS I + (l-s)S2]-IM . 
(4.60) 
Since SI 
= C 
I and S 2  = C2+MMT, 

4 Parametric Classifiers 
145 
SSI + (I-s)Sz = [SCI + (I-s)&] + (l-S)MMT . 
(4.61) 
Using (2. I60), 
--I 
--I 
( 1-s)2-'MMTt-' 
s = c  - 
(4.62) 
where 
= [sS ,+( 1-s)Sz] and 
= [sXI+( 1-s)C2]. 
Multiplying M from the 
right side, 
1 + (1-s)M'z-'M 
' 
5 ' M .  
(4.63) 
1 
- 
- 
1 + ( l-s)MTz-lM 
That is, s - ' M  and Z-IM are the same vector except for their lengths. 
Minimum Mean-Square Error 
The mean-square error is a popular criterion in optimization problems. 
Therefore, in this section, we will study how the concept of the mean-square 
error may be applied to linear classifier design. 
Let y(X) be the desired ourpur of the classifier which we woufd like to 
design. The possible functional forms for y(X) will be presented later. Then, 
the mean-square error between the actual and desired outputs is 
We minimize this criterion with respect to V and Y,,. Since the third term of 
(4.64) is not a function of V and tio, the minimization is carried out for the 
summation of the first and second terms only. 
Two different functional forms of y(X) are presented here as follows: 
(1) y(X) = -1 for- X E 0, and +I for- X E 02: 
Since h (X) is supposed 
to be either negative or positive, depending on X E wI or X E w2, -1 and +1 
for y(X) are a reasonable choice. Then 

146 
Introduction to Statistical Pattern Recognition 
= - P , q ,  +p2r12 ’ 
(4.65) 
On the other hand, the first term of (4.64), E ( h 2 ( X ) } ,  is the second order 
moment of h (X), and is expressed by 
Therefore, r2 is a function of ql, q2, o:, and 0;. Since ar2lao? = P i ,  the 
optimum V according to (4.27) is 
v = [ P I E ,  + P2C2]-I(M2 - MI) . 
(4.67) 
(2) y(X) = q z ( X )  - q , ( X ) :  This is the Bayes discriminant function 
(recalling q I ( X )  6 q 2 ( X )  or h ( X )  = q2(X)-q I ( X )  3 0). Therefore, if we can 
match the designed discriminant function with the Bayes one, the result must 
be desirable. For this y(X), 
which is identical to (4.65). Also, note that E ( h 2 ( X ) }  is not affected by y(X), 
and is equal to (4.66). 
Thus, for both cases, the mean-square error expressions become the same 
except for the constant term E { f ( X ) ) ,  and the resulting optimum classifiers 
are the same. Also, note that the mean-square errors for these y(X)’s are a spe- 
cial case of a general criterion function f(ql,q2,0:,o:). This is not surpris- 
ing, since the mean-square error consists of the first and second order moments 
of h ( X ) .  However, it is possible to make the mean-square error a different 
type of criterion than f(ql,qz,oy,o;) by selecting y(X) in a different way. 
This is the subject of the next section. 

4 Parametric Classifiers 
147 
Other Desired Outputs and Search Techniques 
In pattern recognition, the classifier should be designed by using samples 
near the decision boundary; samples far from the decision boundary are less 
important to the design. However, if we fix the desired output y(X) and try to 
minimize the mean-square error between h (X) and y(X), larger h (X)’s contri- 
bute more to the mean-square error. This has long been recognized as a disad- 
vantage of a mean-square error approach in pattern recognition. In this section, 
we discuss a modification which reduces this effect. 
New notation for the discriminant function: Before proceeding, let us 
introduce new notations which will simplify the discussion later. Instead of 
(4.18), we will write the linear discriminant function as 
h (x) = - V ~ X  - \io > o 
for x E o1 , 
(4.69) 
h ( ~ )  
= V ~ X  
+ v(l > o 
for x E o2 . 
(4.70) 
Furthermore, if we introduce a new vector to express a sample as 
z = [-I 
-XI . . . -xfflT for x E ol , 
(4.71) 
z = [+I s, . . . xf1lT for x E o2 , 
(4.72) 
then, the discriminant function becomes simply 
(4.73) 
where zo is either + I  or -1, and MJ, = I,, (i = 0,1, . . . ,n). 
Thus, our design procedure is 
( 1 )  to generate a new set of vectors Z’s from X’s, and 
(2) to find W T  so as to satisfy (4.73) for as many Z’s as possible. 
Desired outputs: Using the notation of (4.73), new desired outputs will 
be introduced. Also, the expectation in (4.64) is replaced by the sample mean 
to obtain the following mean-square errors: 

148 
Introduction to Statistical Pattern Recognition 
(4.74) 
(4.75) 
(4.76) 
y(Zj): a variable wirh constraint y(Zj) > 0 , 
where N is the total number of samples, and sign(.) is either +1 or -1 depend- 
ing on the sign of its argument. In (4.74), yCZj) is selected as I WTZj I so that, 
only when WTZjcO, the contribution to E 2  is made with (WTZj)2 weighting. 
On the other hand, (4.75) counts the number of samples which give WTZj<O. 
in the third criterion, we adjust y(Zj) as variables along with W. However, the 
y(Zj)'s are constrained to be positive. 
These criteria perform well, but, because of the nonlinear functions such 
as I I, sign (.), and y(Zj)>O, the explicit solutions of W which minimize these 
criteria are hard to obtain. Therefore, a search technique, such as the gradient 
method, must be used to find the optimum W. 
The gradient method for minimizing a criterion is given by 
w(t+l)=w(t)-p-$w(;), az 
(4.77) 
where 2 indicates the Lth iterative step, and p is a positive constant. 
Again, we cannot calculate aE2/aW because of the nonlinear functions 
involved in z2. However, in the linear case of (4.64), 
&*law can be obtained 
as follows. Replacing the expectation of (4.64) by the sample mean, 

4 Parametric Classifiers 
149 
1 
N 
= -(uW - r)T(uTw 
- r) , 
(4.78) 
where 
u = [ Z ,  ' . -ZNI(,,+l)xN 9 
(4.79) 
The U and r are called the sample matrix and the desired output vector, 
respectively. Taking the derivative of (4.78) with respect to W, 
(4.8 1 )  
By analogy to (4.8 l), the following correction terms have been suggested 
for the criteria [5]: 
(1) 
W ( & +  l)=W(u,)--U[UTW(t)- 
2P 
lu'W(t)lI], 
(4.82) 
N 
(3) 
W(Z + 1) = ~ ( t )  
- -u[uTw(t) 
2P 
- I-([)] , 
(4.84) 
N 
where 
(a) I U'W I is a vector whose components are the absolute values of the 
(b) sign(UTW) is a vector whose components are +I or -1 depending 
corresponding components of UTW 
on the signs of the corresponding components of U'W; 
(c) r, = [ I  1 . . . 11'; 

150 
Introduction to Statistical Pattern Recognition 
(d) L(S) is a penalty vector whose components are functions of the 
corresponding components of r(l). 
A different approach is to treat the problem of finding a feasible solution 
of (4.73) as a linear programming problem with an artificially created cost vec- 
tor. For this approach, it is suggested that the reader refers to a text in linear 
programming. 
A word of caution is in order here. In addition to its complexity, all of 
the above approaches have a more fundamental disadvantage. For examples of 
(4.82) and (4.83), the classifier is designed, based only on the misclassified 
samples in the boundary region. For a good classifier the number of the 
misclassified samples tends to be small, and sometimes it is questionable 
whether these samples represent the true statistics of the boundary structure. 
As the result, the resubstitution error, using the same sample set for both 
design and test, tends to be severely biased toward the optimistic side. There- 
fore, it is advisable that independent samples always be used to test the perfor- 
mance of the classifier. 
An iterative process and its convergence: In order to see how the itera- 
tive process works, let us consider the third criterion of (4.76), in which y(Zj) 
are adjusted along with W under the constraint y(Zj> > 0. Also, let us assume 
that our coordinate system has already been transformed to whiten the sample 
covariance matrix, such that 
UU' = NI . 
(4.86) 
Since the result of the procedure should not depend on the coordinate system, 
this transformation simplifies the discussion without loss of generality. Then 
the mean-square error becomes 
The gradients of E* with respect to W and r are 
1 
-- - 2(W - -un, 
aw 
N 
(4.88) 
(4.89) 
In order to satisfy the constraint y(Zi) > 0, a modification is made as follows: 

4 Parametric Classifiers 
151 
(1) Positiveness of the y's can be guaranteed if we start with positive 
numbers and never decrease their values. 
This can be done by modifying r in proportion to 
A r = C +  IC1 
(4.90) 
instead of C, where 
c = uTw - r .  
(4.91) 
Thus, the components of the vector Ar are positive or zero, depending on 
whether the corresponding components of C are positive or negative. Thus, 
r(t+ 
1) is 
r(e + 1) = r(e) + p A r  = r(u,) + p(c + I C  I )  , 
(4.92) 
where p is a properly selected positive constant. In this process, the y's are 
always increased at each iterative step, and W is adjusted to reduce the error 
between y(2,) and W'Z,. 
However, one should be reminded that the scale of 
y's and, subsequently, the scale of W does not change the essential structure of 
the classifier. That is, W'Zi is the same classifier as aWTZj where a is a posi- 
tive constant. 
(2) On the other hand, there are no restrictions on W. Therefore, for a 
given r, we can select W to satisfy &'.2/dW = 0 in (4.88). 
(4.93) 
1 
N 
w = -ur 
or. 
= W (Q + -U 
P 
AT([) . 
(4.94) 
N 
W(Z + 1) minimizes E' for a given r(l + 1) at each iterative step. 
In order to see how W converges by this optimization process, let us 
study the norm of C. The vector C makes the correction for both r and W. 
Also, from (4.91) and (4.87), 

152 
Introduction to Statistical Pattern Recognition 
-2 
llc1I2=o -+ uTw=r -+ E = o .  
(4.95) 
As U, increases, the change of IlC(1)112 can be calculated by substituting (4.92) 
and (4.94) into (4.91). The result is 
IIC(t+ 1)112 - Itc(e)1I2 
= -2pCT(e)[!-,~]AI'(t)+p2ArT(P$[l- -]Ar(t) 
UTU 
. 
N 
N 
(4.96) 
On the other hand, from (4.91), (4.93), and (4.86), 
CTUTU = (wTu - rT)uTu 
= (wT 
- rTuT)u 
= o 
(4.97) 
and 
2 C T A T = ( ( C +  ICI)'+(C- ICI)')(C+ ICI) 
= ( C +  lCl)'(C+ lCl)=ArTAr. 
(4.98) 
Therefore, (4.96) can be simplified as 
f o r O c p c l .  
(4.99) 
The equality holds only when llAr1I2 = 0. Thus, as t increases, )lC(U,)112 
decreases monotonically, until llAr112 equals zero. It means either IIC 112 = 0 or 
C =-IC I from (4.90). When llC11* = 0, we can achieve E2 = 0 [see (4.931. 
On the other hand, when C = - IC I, all components of C become negative or 
zero and the iteration stops with UTW I r satisfied from (4.91). 

4 Parametric Classifiers 
153 
Linearly separable cases: When there exists a linear classifier to 
separate two distributions without error, we call this linearly separable. We 
will prove here that C = - I C I never happens in linearly separable cases. This 
is done by establishing a contradiction as follows. 
For a linearly separable case, there exists a W* for a given U which 
satisfies 
UTW* > 0 .  
(4.100) 
Therefore, if C = - I C I (or C I 
0) occurs at the hh iterative step, 
cyuT'w*) = (UC)W* < 0 .  
(4.101) 
On the other hand, using (4.91), (4.86). and (4.93), UC can be obtained as 
= o .  
(4.102) 
This contradict (4.101), and C = - IC I cannot happen. 
Thus, the inequality of (4.99) holds only when IlCll' = 0. That is, 
IlC(t)112 continues to decrease monatonically with U, until llC112 equals zero. 
4.3 Quadratic Classifier Design 
When the distributions of X are normal for both oI 
and 02, 
the Bayes 
discriminant function becomes the quadratic equation of (4.1). Even for non- 
normal X, the quadratic classifier is a popular one: it works well for many 
applications. Conceptually, it is easy to accept that the classification be made 
by comparing the normalized distances (X-Mi)TX;' (X-Mi) with a proper 
threshold. 
However, very little is known about how to design a quadratic classifier, 
except for estimating Mi and I;; and inserting these estimates into (4.1). Also, 
quadratic classifiers may have a severe disadvantage in that they tend to have 
significantly larger biases than linear classifiers particularly when the number 

154 
Introduction to Statistical Pattern Recognition 
of design samples are relatively small. This problem will be addressed in 
Chapter 5. 
Design Procedure of a Quadratic Classifier 
The general quadratic classifier may be expressed as 
WI 
02 
h ( X ) = X T Q X + V T X + v ,  >< 0 ,  
(4.103) 
where Q, V ,  and v,, are a matrix, vector, and scalar, respectively. Therefore, 
we can optimizef(ql,l12,0T,0~) (qi =E(h(X)lwi) and O’ =Var(h(X)Iw;]) 
with respect to Q, V ,  and v, as was done in linear classifier design. Unfor- 
tunately, the number of parameters, [n (n +1)/2]+n +1, is too large, and O’ is the 
function of the third and fourth order moments of X. Therefore, it is not prac- 
tical to optimize f(ql ,q2,0:,0$). 
Linearization: Another possibility is to interpret (4.103) as a linear 
equation as 
l l ( I l + l )  
2 
I t  
= c. aiyi + Cvixi + v,) , 
(4.104) 
, = I  
i = l  
where qij and i t t  are the components of Q and V .  Each of the new variables, yi, 
represents the product of two x’s, and a is the corresponding q. Since (4.104) 
is a linear discriminant function, we can apply the optimum design procedure 
for a linear classifier, resulting in 
[ai. . . a , ( , , + 1 ) / 2 ~ ~ 1  
. . .~’,,IT = [S K I  + (~-S)K~I-*(D~ 
-01). 
(4.105) 
where Dj 
and Kj are the expected vector and covariance matrix of Z = [YTXTIT 
with [n(n+1)/2]+n variables. Since the y’s are the product of two x’s, Ki 
includes the third and fourth order moments of X. Again, the number of vari- 
ables is too large to compute (4.105) in practice. 
Data display: A practical solution to improve the quadratic classifier of 
(4.1) is 
to plot 
samples 
in 
a coordinate 
system, where 
d!(X) = 

4 Parametric Classifiers 
155 
(X-MI)'C;'(X-M~) 
and d;(X) = (X-Mz)'Z5'(X-M2) 
are used as the x- and 
y-axes respectively, and to draw the classifier boundary by using human judge- 
ment. Figure 4-9 shows an example where the data used for this plot was a 
40-dimensional radar signature. If the density functions of X are normal for 
both o1 and 6$, the Bayes classifier is a 45 line with the y-cross point deter- 
mined by In I X I  I / I & I (P 
I = P 2 = 0.5 in this data), as seen in (4.1). In Fig. 
4-9, it is seen that the Bayes classifier for normal distributions is not the best 
CLASS 1 = o  
CLASS 2 = * 
CLASS 2 DISTANCE 
Fig. 4-9 d2-display of a radar data. 
classifier for this data. Changing both the slope and y-cross point, we can set 
up a better boundary. Or, we could even adopt a curve (not a straight line) for 
the classifier. That is. 

156 
Introduction to Statistical Pattern Recognition 
di = a d :  + p  
(4.106) 
or 
d! = g ( d : ) .  
(4.107) 
In a high-dimensional space, we cannot plot samples to see the distribution. 
Therefore, we must rely on mathematical tools to guide us in finding a reason- 
able boundary. Once samples are mapped down to a two-dimensional space as 
in Fig. 4-9, we can see the distribution and use our own judgement to set up 
the boundary. However, the structure of the boundary should not be too com- 
plex, because the boundary must work not only for the current, existing sam- 
ples but also for samples which will come in the future. We can always draw 
a very complex boundary to classify the existing samples without error, but the 
boundary may misclassify many of the future samples. 
Stationary Processes 
The quadratic classifier for stationary processes: When xi is the ith 
time-sampled value of a stationary random process, x(t), the contribution of xi 
in the discriminant function must be independent of i. The same is true for x’ 
and X ; X ; + ~  for fixed j’s. Therefore, (4.104) may be simplified to 
n 
n-I 
h ( X )  = qo(xx?) + 2 q , ( ~ x ; x ; + 1 )  
+ . . . + 2qn4(xlxn) 
i = l  
i=I 
n 
+ v ( Z x ; )  + v, . 
i = l  
(4.108) 
This is a linear discriminant function of 
new variables, yo =Cx:, 
y I  = L Y ~ X ~ + ~ ,  
. . . ,yn-] =xlxn, yn = Zx;. However, now the number of vari- 
ables is reduced to n+l, and we need to find only n+2 coefficients, 
qo, . . . r4n-I, v, and vo. 
Orthonormality of the Fourier transform: Stationary processes have 
another desirable property, namely that the elements of the (discrefe) Fourier 
transform, F(k) and F(L), of the process are uncorrelated. In order to show this, 

4 Parametric Classifiers 
157 
let us define the discrete Fourier transform of the time-sampled values of a ran- 
dom process, x(O), . . . ,x(n -l), as 
I1 - I 
F(k) = xx(L)Wk' 
(k = 0,. . . ,n-1) , 
(4.109) 
i=o 
where W is 
2n 
-
J
T
 
W = e  
and W satisfies 
n for 
= 0 
k
4
 
0 for 5 # 0 .  
n-I 
(4.1 10) 
(4.111) 
Then, the inverse Fourier transform becomes 
111-1 
n k 4  
~ ( 5 )  = -xF(k)W-k' 
(t = 0,. . . ,n-1) . 
(4.1 12) 
In a stationary process, the first and second order moments of x(k) must 
satisfy 
R (k-4) = E (x(k)x(t)] , 
(4.1 14) 
where m and R (.) are called the mean and autocorrelation function of the pro- 
cess. We assume that the process is real. Note that m is independent of k, and 
R(.) depends only on the difference between k and 2 and is independent of k 
itself. 
Using (4.113) and (4.114), the expected values and second order 
moments of the F(k)'s can be computed as follows. 

158 
Introduction to Statistical Pattern Recognition 
nm for k = 0 
g-3 
0 f o r k # O ,  
11-1 
E(F(k)] = m z W k '  = { 
(4.1 15) 
n-l 
n-1 
= ( C R ( U ) W ~ " ) ( C , W ' " - " ~ )  
I I  =o 
s =o 
11-1 
nzR(u)W'" for k = t 
for k f 4 , 
II =o 
(4.116) 
0 
where R (-1.-s) = R (s--1.) for a real x ( t )  is used to derive the third line from the 
second, and F*(t) is the complex conjugate of F(2). The variances and covari- 
ances of the F(k)'s are 
Var(F(k)) =E(F(k)F*(k)} - E(F(k)}E(F*(k)J 
and 

4 Parametric Classifiers 
159 
Cov { F(k),F(t)) = E { F(k)F* (9) 
- E ( F(k) ) E  ( F*(&) 
) 
= O  
for k + L .  
(4.1 18) 
That is, F(k) and F(L) are uncorrelated. It means that the covariance matrices 
of X for all classes are simultaneously diagonalized by the Fourier transform, if 
the random processes are stationary. 
The quadratic classifier in the Fourier domain: Thus, if the F(k)'s are 
normally distributed, we can design a quadratic classifier in the Fourier domain 
as 
+ I),, 9 
where 
(4.1 19) 
(4.120) 
(4.121) 
Note in (4.119) that, since the covariance matrices of the F(j)'s for both w1 
and w2 are diagonal, all cross terms between F(j) and F(k) disappear. 
A modification of the quadratic classifier can be made by treating (4.119) 
as a linear classifier and finding the optimum vI,, v2,, and vo instead of using 
(4.120) and (4.121). In this approach, we need to optimize only 2n+l parame- 
ters, v,, (i = 1,2; j = 0, . . . ,n-l) and vo. 

160 
Introduction to Statistical Pattern Recognition 
Approximation of Covariance Matrices 
Most of the difficulty in designing quadratic classifiers comes from the 
covariance matrices. If we could impose some structure on the covariance 
matrix, it would become much easier to design a quadratic classifier. Also, the 
required design sample size would be reduced, and the classifier would become 
more insensitive to variations in the test distributions. One possible structure is 
the toeplitz form, based on the stationarity assumption. An example is seen in 
(3.13). However, the stationarity assumption is too restrictive and is not well 
suited to most applications in pattern recognition. 
Toeplitz approximation of a correlation matrix: Another possibility is 
to assume the toeplitz form only for the correlation matrices, allowing each 
individual variable to have its own mean and variance. That is, departing from 
(4.1 13) and (4.1 14) 
where 0: = Var(x(k)), and p l ~ . * ~  
is the correlation coefficient between x(k) 
and x(U,) which depends only on I k 4  I. Expressing the covariance matrix as 
C = TRT from (2.18), the inverse matrix and the determinant are 
11/01 
0 
1/01 
0 
0 
1 10, 
r 
1 
PI 
PI 
1 
Pn-1 
. . Pn-1 
' 
PI 
PI 
1 
-I 
1/01 
0 
0 
I /on 
, (4.124) 

4 Parametric Classifiers 
161 
In IC1 =In lrl IRI lrl 
n 
= x l n 0 , 2 + 1 n  I R I .  
(4.125) 
i=l 
Thus, we can focus our attention on R-‘ and In IR I. A particular form of the 
toeplitz matrix, (3.13), has the closed forms for the inverse and determinant as 
seen in (3.14) and (3.15). Rewritting these. 
1 
-P 
0 . . .  0 
-p 
l+p2 . . 
0
.
 
0 
1+p* -p 
0 . . .  0 
-P 
1 
I R I = (1 - p2)”-I 
(4.126) 
(4.127) 
(4.128) 
Thus, using (4.126) as the form to approximate the correlation matrix, 
the estimation process of an approximated covariance matrix is given as fol- 
lows: 
- 2  
(1) Estimate O’ by the sample variance, bi . 
(2) Estimate c;,;+~ = pi,i+loioi+l 
by the sample covariance, c;.;+~, 
and 
divide ci.i+l by oioi+l 
to obtain the estimate of 
A 
I 
A , .  
A 

162 
Introduction to Statistical Pattern Recognition 
,. 
(3) Average 
over i = 1,. . . ,n-1, to get p. 
(4) Insert 
into (4.126) to form k. 
Note that only (n+l) parameters, oi (i = 1,. . . ,n) and p, are used to approxi- 
mate a covariance matrix. 
Example 5: 
Figure 4-10 shows the correlation matrix for Camaro of 
Data RADAR. The radar transmitted a left circular electro-magnetic wave, and 
received left and right circular waves, depending on whether the wave bounced 
an even or odd number of times off the target surface. With 33 time-sampled 
values from each return, we form a vector with 66 variables. The first 33 are 
from the left-left and the latter 33 are from the left-right. The two triangular 
parts of Fig. 4-10 show the correlation matrices of the left-left and left-right. 
In both matrices, adjacent time-sampled variables are seen to be highly corre- 
lated, but the correlation disappears quickly as the intervals between two sam- 
pling points increase. The ith sampling point of the left-left and the ith sam- 
pling point of the left-right are the returns from the same target area. There- 
fore, they are somewhat correlated, which is seen in the rectangular part of Fig. 
The toeplitz form of (4.126) cannot approximate the rectangular part of 
Fig. 4-10 properly. Therefore, we need to modify the form of the approxima- 
tion. The structure of Fig. 4-10 is often seen in practice, whenever several sig- 
nals are observed from the same source. In the radar system of Example 5 ,  we 
have two returns. In infrared sensors, it is common to observe several wave- 
length components. Furthermore, if we extend our discussion to two- 
dimensional images, the need for the form of Fig. 4-10 becomes more evident 
as follows. 
4- 10. 
Block toeplitz: Let x ( i , j )  be a variable sampled at the i,j position in a 
two-dimensional random field as illustrated in Fig. 4-1 1. Also, let us assume 
toeplitz forms for correlation coefficients as 

€9 I 

164 
Introduction to Statistical Pattern Recognition 
p[x(i,j),x(i+k,j+t)] = pf'py) . 
(4.13 1) 
That is, the correlation coefficient between x(i,j) and x(i+k,j), which is called 
the column correlation coefficient, pf), depends only on k. The same is true 
for the row correlation corncient, py). The correlation coefficient between 
x(i,j) and x(i+k,j+!) is assumed to be the product of pf) and pf'')* 
If we form a vector with nm variables by stacking columns of the 
received image as 
x = [x(l,l). . .x(n, 1). . . x ( l , m ) .  . .x(n,m)lT 
(4.132) 
the correlation matrix has the block toeplitz form 
R, @R, = 
, 
(4.133) 

4 Parametric Classifiers 
where 
R, = 
R,. = 
165 
(4.134) 
(4.135) 
Many properties of the block toeplitz matrix are known, and listed as fol- 
lows without proof [6]. 
(1) 
A @ B # A  @ B ,  
(2) 
( 3 )  
(A @B)' = A T  @ B T ,  
(4) 
(5) 
(6) 
(A @B)(C @D) = AC @BD , 
(A @B)-' =A-' C3B-I , 
tr(A @B) = (tr A)(tr B )  , 
(A @B)(@ BY) = (a @")(A @ p) 
where A 0  =@A and BY = Y p  
e, 
Y: eigenvector matrices of A and B 
A, p: eigenvalue matrices of A and B, 
(7) 
I A  @Bl = I A I m l ~ I " =  
IAImlBI" 
where A and B are nxn and mxm matrices. 
(4.136) 
(4.137) 
(4.138) 
(4.139) 
(4.140) 
(4.141) 
(4.142) 

166 
Introduction to Statistical Pattern Recognition 
When a quadratic classifier is designed, Properties (4) and (7) are particularly 
useful. 
Example 6: 
For Data RADAR, n = 33 and m = 2. Both R,. and R,. 
matrices of (4.134) and (4.135) are formed based on the assumption that the 
toeplitz form of (4.126) holds. Then, the correlation matrix for mi is 
RCi 
pT)Rcl 
Rj = [ 
pT)RCj R,.; 
and 
RCi = 
The inverse matrix and determinant of R, are 
and 
. 
(4.143) 
(4.144) 
(4.145) 
(4.146) 
In order to verify the effectiveness of the above approximation, the fol- 
lowing two experiments were conducted. 

4 Parametric Classifiers 
167 
Experiment 3: Computation of the Bhattacharyya distance 
Data: RADAR 
Dimension: n = 66 
Sample size: N I = N 2  = 8800, 720, 360 
Approximation: Toeplitz approximation for Cj 
No. of trials: z = 1 
Results: Table 4-1 
n 
TABLE 4-1 
EFFECT OF TOEPLITZ APPROXIMATION 
NI = N 2  
8,800 
720 
360 
4,400 (Design) 
4,400 (Test) 
720 (Design) 
4,400 (Test) 
360 (Design) 
4,400 (Test) 
Without Approx. 
0.64 
1.57 
2.52 
20.2 
25.9 
30.1 
Toeplitz Approx. 
0.73 
0.77 
~~ 
0.8 1 
26.3 
26.6 
26.8 
n 
n 
In this experiment, the sample mean M, and sample covariance matrix C, 
were estimated from N, samples, and the correlation matrix of C, was approxi- 
mated by the toeplitz form of (4.143). Using M, and the approximated C,, the 
Bhattacharyya distance was computed and was compared with the one com- 
puted from h, and 
(without the approximation). Both are fairly close for 
N, = 8800, indicating the validity of the approximation. Furthermore, since the 
approximated covariance matrices depend on a smaller number of parameters, 
its estimates are less sensitive to the sample size. Without approximation, the 
effect of the sample size is evident. That is, p(1/2) increases as N, decreases. 
n 
n 
n 

168 
Introduction to Statistical Pattern Recognition 
However, with approximation, the effect of the sample size is significantly 
reduced. 
Experiment 4: Error of the quadratic classifier 
Data: RADAR 
Dimension: n = 66 
Sample size: N I = N 2  = 4400,720, 360 (Design) 
N I = N 2  = 4400 (Test) 
,. 
Approximation: Toeplitz approximation for Zj (Design only) 
No. of trials: z = 1 
Results: Table 4- 1 
A 
A 
In this experiment, Mi and the approximated C; were used to design the 
quadratic classifier of (4. l), and independent 4400 samples per class were 
tested. The results were compared with the error of the quadratic classifier 
designed without the approximation. The error of the approximated case is 
somewhat larger than the error without approximation. However, with approx- 
imation, the effect of the sample size is virtually eliminated. 
The performance evaluation of the toeplitz approximation can be carried 
out experimentally as seen in Experiments 3 and 4. That is, the means and the 
parameters of the covariance matrices are estimated from design samples, and 
the quadratic classifier based on these estimated parameters is tested by 
independent test samples. 
However, when the distributions of X are normal with given M i  
and C;, 
the performance of the quadratic classifier with the toeplitz approximation can 
be evaluated theoretically as follows. 
(1) Average the first off-diagonal terms of Ri from the given Xj and 
form the toeplitz approximation as in (4.143). 
(2) Using the given Mi and approximated Zj, design the quadratic 
classifier of (4.1). 
(3) Compute the error by testing the original distributions of 
Nx(M;,Z;)’s. 
Since X i ’ s  used for design (the toeplitz approximations) are dif- 

4 Parametric Classifiers 
169 
ferent from the ones used for test (given Xi's), the algorithm of (3.1 19)-(3.128) 
must be used to calculate the theoretical error. 
4.4 Other Classifiers 
In this section, we will discuss subjects which were left out in the previ- 
ous discussions. They are the piecewise classifiers and some of the properties 
in binary inputs. 
Piecewise Classifiers 
If we limit our discussion to two-class problems, quadratic or linear 
classifiers have wide applications. However, when we have to handle three or 
more classes, a single quadratic or linear classifier cannot be adopted effec- 
tively. Even in two-class problems, the same is true when each class consists 
of several clusters. For these cases, a set of classifiers, which is called a piece- 
wise classifier, gives increased flexibility. 
Piecewise quadratic for multiclass problems: For multiclass problems, 
the multihypothesis test in the Bayes sense gives the best classifier with regard 
to minimizing the error. That is, from (3.44) 
Pkpp(X) = max Pipi(X) + X E q . 
(4.147) 
I 
If the distributions of X for L classes are normal, (4.147) is replaced by 
(4.148) 
1 
1 
i
2
 
2 
min[-(X - M,)'z;'(x - M,) + - ln I 
I - w,] , 
where max is changed to min because of the minus-log operation. Note that 
the normalized distance of X from each class mean, Mi, 
must be adjusted by 
two constant terms, (112)ln ICi I and In Pi. Equation (4.148) forms a piecewise 
quadratic, boundary. 
Piecewise quadratic for multicluster problems: For multicluster prob- 
lems, the boundary is somewhat more complex. Assuming that L = 2, and that 
each distribution consists of m, normal clusters with the cluster probability of 
Pi, for the jth cluster, the Bayes classifier becomes 

170 
Introduction to Statistical Pattern Recognition 
(4.149) 
where Mij and Zjj are the expected vector and covariance matrix of the jth 
cluster in mi. Or, defining the distances as 
1 
1 
2 
2 
d*.(x) = -(x-M..)~c:' 
(x-M..) + - 
In I C, I -In P; -In pij , 
'J 
IJ 
the classifier becomes 
(4.150) 
(4.151) 
Note that the decision of (4.151) is different from min da(X), which is the 
Bayes decision if we treat this problem as an (rn I+m2)-class problem. Also, it 
should be realized that the distances are adjusted by lnP, as well as lnPi. 
Piecewise linear classifiers: When all covariance matrices are the same 
in multiclass problems, X T X T 1 X  and In I Ci I of (4.148) are common among all 
classes, and (4.148) is reduced to 
where C is the common covariance matrix, and min of (4.148) is changed to 
max in (4.152) because of the change of sign. That is, X is classified to the 
class with the highest correlation between X and Z-'Mj. Again, the correlation 
must be adjusted by constant terms. 
When covariance matrices are different among classes but close to each 
other, we may replace 
Another alternative, particularly when covariance matrices are not close 
to each other, is to set a linear discriminant function for each pair of classes, 
and to optimize the coefficients. Let each discriminant function be 
of (4.152) by the averaged covariance. 

4 Parametric Classifiers 
171 
hjj(X) = v;x + vjjo 
(i,j = 1,. . . ,L: i # j )  . 
(4.153) 
The signs of Vij are selected such that the distribution of oj is located on the 
positive side of hij(X) and pi on the negative side. Therefore, 
hij(X) = + ( X )  . 
(4.154) 
Let us assume that the region for each class is convex, as shown in Fig. 
4- 12. 
Fig. 4-12 A piecewise linear classifier. 
Then, the region of class i can be simply specified by 
hil(X) > 0,. . . ,hjL(X) > 0 -+ X E mi 
[hii(X) is excluded] . 
(4.155) 
As evidenced by the hatched part of Fig. 4-12, the L regions given by (4.155) 
do not necessarily cover the entire space. When a sample falls in this region, 
the piecewise linear classifier cannot decide the class of this sample; we call 
this a reject region. Implementation of (4.155) consists of (L - 1) linear 
discriminant functions and a logical AND circuit with (L - 1) inputs of 
sign{hij(X)), as shown in Fig. 4-13. Since the network has two cascaded 
circuits, the piecewise linear classifier is sometimes called a layered machine. 
When the assumption of convexity does not hold, we have to replace the AND 
gate by a more complex logic circuit. Consequently, the classifier becomes too 

172 
Introduction to Statistical Pattern Recognition 
complicated to be practical. Therefore, we will limit our discussion to convex 
regions here. 
The probability of error for each class, E;, can be expressed in terms of 
the (L - 1)-dimensional distribution function as 
E; = 1 - Pr(h;,(X) 
> 0,. . .,hiL(X) > OIX E 0 ; )  
= I - [- 
. . .[-p (h; 1 ,  . . . ,h;L I 0;)dh; 
I . . . dh, 
[hii(X) is excluded] . 
The total error is 
L 
E = ZP;E; . 
i = l  
(4.156) 
(4.157) 
Knowing the structure of piecewise linear classifiers, our problem is how 
to design the V’s and Y O ’ S  for a given set of L distributions. Because of the 
complexity involved, solutions for this problem are not as clear-cut as in a 
linear classifier. 
Three approaches are mentioned briefly: 

4 Parametric Classifiers 
173 
(1) We can adjust the V’s and vo’s so as to minimize E of (4.157). 
Since it is difficult to get an explicit mathematical expression for E, the error 
should be calculated numerically each time when we adjust the V’s and vo’s. 
When X is distributed normally for all classes, some simplification can be 
achieved, since the h’s are also normally distributed and p (h, I ,  . . . ,hjL Io;) 
is 
given by an explicit mathematical expression. Even for this case, the integra- 
tion of an (L - l)-dimensional normal distribution in the first quadrant must be 
carried out in a numerical way, using techniques such as the Monte Carlo 
method. 
(2) Design a linear discriminant function between a pair of classes 
according to one of the methods discussed previously for two-class problems. 
L
.
.
 
(2) discriminant functions are calculated. Then, use them as a piecewise linear 
discriminant function without further modification. When each class distribu- 
tion is quite different from the others, further modification can result in less 
error. However, in many applications, the decrease in error is found to be rela- 
tively minor by the further adjustment of V’s and vo’s. 
(3) We can assign the desired output y(X) for a piecewise linear 
discriminant function and minimize the mean-square error between the desired 
and actual outputs in order to find the optimum V’s and vo’s. The desired out- 
puts could be fixed or could be adjusted as variables with constraints. Unfor- 
tunately, even for piecewise linearly separable data, there is no proof of con- 
vergence. 
Binary Inputs 
In Section 4.1, we showed that for independent binary inputs the Bayes 
classifier becomes linear. In this section, we will discuss other properties of 
binary inputs. 
When we have n binary inputs forming an input vector X ,  the number of 
all possible inputs is 2”, { X o , .  . . ,X21j-I 1 [see Table 4-2 for example]. Then 
the components of X i ,  sk,(k = 1, . . . ,H), satisfy 

174 
Introduction to Statistical Pattern Recognition 
1 2'1-1 
- 
Z X k j  = 0, 
2" j* 
, 2'1-1 
- 
= 1 , 
2" 
J* 
(4.158) 
(4.159) 
(4.160) 
where xkj is either +1 or -1. Thus, if we define the sample matrix as 
(4.161) 
2" 
then the row vectors of U are mutually orthonormal, that is 
UUT = 2"1 . 
(4.162) 
Example 7: 
Table 4-2 shows an example of three binary inputs. We 
can easily see that (4.158)-(4.162) are all satisfied. 
Let y(X) be the desired output of a pattern recognition network for the 
input X. The y(X) is not necessarily a binary number. One of the design 
procedures which may be used to realize this network by a linear discriminant 
function is to minimize the mean-square error between y(X) and V'X + v,,. 
The mean-square error can be expressed by 
where W and r are the same as the ones used in (4.78). Therefore, the W 
which minimizes E' is 

4 Parametric Classifiers 
175 
TABLE 4-2 
ALL POSSIBLE BINARY INPUTS 
. U O I 1  
1
1
 1
1
 1
1
 1 
s 
I 
x2 
-r 3 
-1 
1 
-1 
1 
-1 
1 
-1 
1 
-1 
-1 
1 
1 
-1 
-1 
I 
1 
-1 
-1 
-1 
-1 
1 
1 
1 
1 
1 
-1 
-1 
1 
1 
-1 
-1 
1 
1 
-1 
1 
-1 
-1 
1 
-1 
1 
1 
1 
-1 
-1 
-1 
- 1  
1
1
 
-1 
1 
1 
-1 
1 
-1 
-1 
1 
aE2 
2 
1 
-- - -u(uTw - r) = 2(w - -ur) = 0 ,  
aw 
2" 
2" 
1 
2" 
w=-ur. 
(4.164) 
(4.165) 
Thus, the coefficients of the linear discriminant function are given by the corre- 
lation between the desired output and the input X .  The above discussion is 
identical to that of the general linear discriminant function. However, it should 
be noted that for binary inputs UUT = NI is automatically satisfied without 
transformation. 
As an example of y(X), let us use 
The term y(X) would be positive for P , p  I ( X )  < P * p 2 ( X )  or q I (X) < q 2(X), 
and be negative otherwise. Also, the absolute value of y(X) depends on p I ( X )  
and p2(X). When n is large but the number of observed samples N is far less 
than 2", the correlation of (4.165) can be computed only by N multiplications 
and additions, instead of 2" [7]. 
Table 4-2 suggests that we can extend our vector X = [xI . . . xnIT to 

176 
Introduction to Statistical Pattern Recognition 
Y = [l X I  . . . xn ( X I X 2 )  . . . ( X I X 2 . .  . x n ) ] ' .  
(4.167) 
2" 
Then, the sample matrix for this extended vector becomes a square matrix as 
Uy = [Yo Y 1 .  . . Y ~ L ~ ] } Y  
. 
(4.168) 
2" 
The row vectors of U y  are also orthonormal, such that 
uyu; = 2"1 . 
(4.169) 
A linear discriminant function for Y is 
2"- 1 
n 
j=O 
j = l  
i
i
 
wjyj = wo + x 
wjxj + C 
~ W ! X ; X ~  
+ . . . + ~ 2 , # - 1  
x I . . . X ,  . 
(4.170) 
In accordance with the reasoning applied to derive (4.165), we can determine 
W of (4.170) by 
1 
2" 
w = -uyr 
(4.171) 
The following should be noted here: 
(1) Any desired output is expressed by W'Y without error. 
- 2  
(2) Since yt's are mutually orthonormal, E due to the elimination of 
(3) The E2 determined by the linear discriminant function of V'X + V o  
wlyl from W ~ Y  
is w f .  
is 
2"- I 
-2 
E = x w ; .  
j=n+l 
Computer Projects 
1. 
Repeat Example 4, and obtain Fig. 4-8. 
(4.172) 
2. 
Repeat Experiment 1 for Ni = 50, 100, 200,400 and plot the error vs. s. 

4 Parametric Classifiers 
177 
3. 
4. 
5. 
Repeat Experiment 2 for Ni = 50, 100,200,400 and plot the error vs. s. 
Design the optimum linear classifier by minimizing the mean-square error 
of (4.76). Use 100 generated samples per class from Data Z-A for design, 
and test independently generated 100 samples per class. Observe the 
difference between this error (the error of the holdout method) and the one 
of the resubstitution method. 
Two 
8-dimensional 
normal 
distributions 
are 
characterized 
by 
P I  = P 2  = 0.5, M I  = M 2  = 0, Zj = o’Ri where Rj is given in (4.126) with 
(a) Compute the Bayes error theoretically. 
(b) Generate N, design samples per class, and compute the sample mean 
Mi and sample covariance Zi. 
(c) Approximate the correlation matrix of Cj by the toeplitz form of 
(4.126). 
(d) Design the quadratic classifier with M i  
and the approximated Zj. 
(e) Generate 1000 test samples, and classify them by the quadratic 
classifier designed in (d). 
Repeat (b)-(e) 10 times, and compute the average and standard devia- 
tion of the error. 
(g) Compare the error of (0 with the error of (a) for various N,. Sug- 
gested Nj’s are 10, 20, and 40. 
0 2 -  
2 -  
I - o2 - 1, pI = 0.5, and p2 = -0.5. 
,. 
.. 
,. 
6 
A 
(f) 
Problems 
1. 
Let x , ~  (i = 1,. . . ,n) be independent and identically distributed with an 
exponential density function 
1 
s j  
hi 
hi 
p-(r.) = - 
exp[--1 
u(sj) 
(i = 1,2) 
’ - . I  
where II (.) is the step function. 
(a) Find the density function of the Bayes discriminant function h (X). 

178 
Introduction to Statistical Pattern Recognition 
(b) Assuming that the density functions of h ( X )  for o1 and 0 2  can be 
approximated by normal densities, compute the approximated value 
of the Bayes error for n = 8, h2A, = 2.5, and P I  = P 2  = 0.5. 
2. 
Two normal distributions are characterized by 
Calculate the errors due to the Bayes classifier and the bisector. 
3. 
Using the same data as in Problem 2 except 
find the linear discriminant function which maximizes the Fisher criterion, 
and minimize the error by adjusting the threshold. 
4. 
Using the same data as in Problem 3, find the optimum linear discriminant 
function which minimizes the probability of error. Show that the error is 
smaller than the one of Problem 3. (Check the errors for s = 0, 0.02 and 
0.25.) 
5. 
Design the optimum linear classifier by minimizing the mean-square error 
of 
-2 
E = E { ( vTx + Yo - y(X))2 ) 
where y(X) = +1 for X E o2 and -1 for X E ol, Without using the pro- 
cedure discussed in this chapter, take the derivative of E2 with respect to 
V and 1'0, equate the derivative to zero, and solve the equation for V. Set- 
ting the mixture mean, M o  = P I M I  + P2M2, as the coordinate origin, 
confirm that the resulting optimum V is 

4 Parametric Classifiers 
v = [PIC, + P,C2]-1(M2 - M , )  
179 
6. 
Prove that E (  Fowl)F*ow2)) = 0 for wI 
# o2 where Fuw) is the Fourier 
transform of a stationary random process, x(r), as 
7. 
Two stationary normal distributions are characterized by P ,  = P 2  = 0.5, 
M I  =0, M2 = A [ l . .  . 1lT, and C = C ,  =C2 = 0 2 R  where R is given in 
(4.126). 
(a) Compute the Bayes error for n = 10, A = 2, o2 = 1, and p = 0.5. 
(b) Using the same numbers as in (a), compute the error when 
NX(Ml,02Z) 
and NX(M2,021) are used to design the classifier and 
N X ( M l , C )  and Nx(Mz,C) 
are used to test the classifier. 
8. 
Repeat Problem 7 for a two-dimensional random field of nxn. The verti- 
cal and horizontal correlation matrices are the same and specified by 
(4.126). 
9. 
Design a linear classifier by minimizing the mean-square error for the data 
given in the following Table, assuming P I = P ,  = 0.5. 
X I  
x2 
X3 
PlW) 
P 2 W  
-1 
-1 
-1 
1/3 
0 
+1 
-1 
-1 
1/24 
1/8 
-1 
+1 
-1 
1/24 
1/8 
+1 
+1 
-1 
0 
1/3 
-1 
-1 
+1 
1 /3 
0 
+1 
-1 
+1 
1/24 
1/8 
-1 
+I 
+1 
1/24 
1/8 
+1 
+1 
+1 
0 
1/3 

180 
Introduction to Statistical Pattern Recognition 
10. In the design of a piecewise linear classifier, propose a way to assign the 
desired output so that we can apply the technique of minimizing the 
mean-square error. 
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
H. L. VanTrees, “Detection, Estimation, and Modulation Theory: Part 
I,” Wiley, New York, 1968. 
A. Fisher, “The Mathematical Theory of Probabilities,” Vol. 1, Macmil- 
Ian, New York, 1923. 
T. W. Anderson and R. R. Buhadur, Classification into two multivariate 
normal distributions with different covariance matrices, Ann. Math. Stat., 
33, pp. 422-431, 1962. 
D. W. Peterson and R. L. Mattson, A method of finding linear discrim- 
inant functions for a class of performance criteria, Trans. IEEE Inform. 
Theory, IT-12, pp. 380-387, 1966. 
Y. C. Ho and R. L. Kashyap, An algorithm for linear inequalities and its 
applications, Trans. IEEE Electronic Computers, EC- 14, pp. 683-688, 
1965. 
C. W. Therrien and K. Fukunaga, Properties of separable covariance 
matrices and their associated Gaussian random processes, Trans. IEEE 
Pattern Anal. and Machine Intell., PAMI-6, pp. 652-656, 1984. 
K. Fukunaga and T. Ito, A design theory of recognition functions in 
self-organizing systems, Trans. IEEE Electronic Computers, EC- 14, pp. 
44-52, 1965. 

Chapter 5 
PARAMETER ESTIMATION 
As discussed in the previous chapters, once we express the density func- 
tions in terms of parameters such as expected vectors and covariance matrices, 
we can design the likelihood ratio classifier to partition the space. Another 
alternative is to express the discriminant function in terms of a number of 
parameters, assuming a mathematical form such as a linear or quadratic func- 
tion. Even in this case, the discriminant function often becomes a function of 
expected vectors and covariance matrices, as seen in Chapter 4. In either case, 
we call it the parametric approach. The parametric approach is generally con- 
sidered less complicated than its counterpart, the nonparametric approach, in 
which mathematical structures are not imposed on either the density functions 
or the discriminant function. 
In the previous chapters, we have assumed that the values of the parame- 
ters are given and fixed. Unfortunately, in practice their true values are never 
known, and must be estimated from a finite number of available samples. This 
is done by using the sample estimation technique presented in Section 2.2. 
However, the estimates are random variables and vary around the expected 
values. 
The statistical properties of sample estimates may be obtained easily as 
discussed in Section 2.2. However, in pattern recognition, we deal with func- 
tions of these estimates such as the discriminant function, the density function, 
181 

182 
Introduction to Statistical Pattern Recognition 
the classification error, and so on. Therefore, we need to know how the out- 
puts of these functions are affected by the random variations of parameters. 
More specifically, we are interested in the biases and variances of these func- 
tions. They depend on the functional form as well as the number of samples 
used to estimate the parameters. We will discuss this subject in this chapter. 
First, the problem will be addressed in a general form, and then the Bhatta- 
charyya distance will be studied. 
A more important quantity in pattern recognition is the probabi/ify of 
error, which is expressed as a complicated function of two sets of parameters: 
one is the set of parameters which specify a classifier, and the other is the set 
of parameters which specify the distributions to be tested. Because these two 
sets are involved, the estimation of the error is complex and difficult to discuss. 
In this chapter, we will show how the estimated error is affected by the design 
and test samples. Also, the discussion is extended to include several error esti- 
mation techniques such as the holdout, leave-one-out, and resubstitution 
methods as well as the bootsrr-ap method. 
5.1. Effect of Sample Size in Estimation 
General Formulation 
Expected value and variance: Let us consider the problem of estimat- 
ing f ( y  I , .  . . ,y4) by f ( y , ,  . . . ,y4), where f is a given function, the yj’s are 
the true parameter values, and the yI’s are their estimates. In this section, we 
will derive expressions for the expected value and variance off ( y l , .  . . ,y4), 
and discuss a method to estimate f ( y I ,  . . . ,y4). 
Assuming that the deviation of yi from yI is small, f (Y) can be 
,. 
,. 
A 
n 
,. 
,. 
,. 
expanded by a Taylor series up to the second order terms as 
A
,
.
 
where Y = bl . . . y,,lT, Y = [yI . . . ;,Ir, 
and AY = - Y .  

5 Parameter Estimation 
183 
If the estimates are unbiased, 
E(AY) = 0 
and subsequently the expecred value of T is 
Similarly, the variance of T can be derived as 
I- 
where the approximation from the first line to the second line is made by dis- 
carding terms higher than second order. 
Equation (5.3) shows that f is a biased estimate in general and that the 
bias depends on a2 flay2 and E { AYAYT ), where a2 flay2 is determined by the 
functional form off and E(AYAYT) is determined by the distribution of ?, 
p(Y), and the number of samples, N, used to compute Y. Likewise, the vari- 
ance depends on af/aY and E 1 AYAYT ). 
,. 
,. 
,. 
Estimation of f: For many estimates, the effects of p(Y) and N on 
E (AYAYT) can be separated as 
E ~ A Y A Y ~ )  
= g ( ~ )  
K@(?)), 
(5.5) 
,. 
where the scalar g and the matrix K are functions determined by how Y is 
computed. Substituting (5.5) into (5.3), 

184 
Introduction to Statistical Pattern Recognition 
E { i )  G f + v  g ( N )  , 
(5.6) 
where v = tr (a2ffaY2 K@(?))}/2 is independent of N and treated as a con- 
stant determined by the underlying problem. This leads to the following pro- 
cedure to estimate f. 
A 
Change the sample size N as N 1  , N 2 , .  . . , N t .  For each Ni, compute Y 
and subsequently f empirically. Repeat the experiment z times, and 
approximate E { f } by the sample mean of the z experimental results. 
.* 
n 
Plot these empirical points E { i} vs. g (N). Then, find the line best fitted 
to these points. The slope of this line is v and the y-cross point is the 
improved estimate off. There are many possible ways of selecting a 
line. The standard procedure would be the minimum mean-square error 
approach. 
Parametric Formulation 
Moments of parameters: In the parametric approach, most of the 
expressions we would like to estimate are functions of expected vectors and 
covariance matrices. In this section, we will show how the general discussion 
of the previous section can be applied to this particular family of parameters. 
mal distributions with expected vectors and covariance matrices given by 
Assume that N samples are drawn from each of two n-dimensional nor- 
(5.7) 
Without loss of generality, any two covariance matrices can be simultaneously 
diagonalized to I and A, and a coordinate shqt can bring the expected vector of 
one class to zero. Normality is assumed here for simplicity. However, the dis- 
cussion can be extended to non-normal cases easily. The extension will be 
presented at the end of this section. Also, N = N2 = N is assumed here. For 
N I  # NZ, a similar discussion could be developed, although the results are a 

5 Parameter Estimation 
185 
little more complex. In order to simplify the notation, A,. (r = 1,2) are used to 
indicate the diagonalized class covariances, where AI = I and h2 = A. 
mean and sample covariance matrix 
The parameters Mi and Ci can be estimated without bias by the sample 
,. 
where Xp) is the jth sample vector from class I-. Thus, the parameter vector Y 
of (1) consists of 2[n +n (n +1)/2] components 
where mi”) is the ith component of M,., and 
column component of X,.. 
(i 2j) is the ith row and jth 
n 
The random variables of (5.10) satisfy the following statistical properties,. 
where Amy-) = m:) 
- my) and Ac(r) = 
- ~ ( r ) :  
IJ 
rJ 
IJ 
(1) 
The sample mean and covariance matrix are unbiased: 
E(Am:’) = O  and E(Acjj)) = O .  
(5.1 1) 
(2) 
Samples from different classes are independent: 
E(Arnj1)Am)*’) =E(Amj”) E(Amj2)) = 0 ,  
(3) 
The covariance matrices of the sample means are diagonal [see (2.34)l: 

186 
Introduction to Statistical Pattern Recognition 
I 
E{M,.--M,.)(M,.-M,.)'} 
= -A 
N
'
 
or 
x p  
E { Amy)Am(!) ] = -&j 
(5.13) 
N 
J 
where hj'" is the ith diagonal component of A,.. 
(4) 
The third order central moments of a normal distribution are zero: 
E { Am?Acf;) } = 0 . 
(5.14) 
(5) 
The fourth order central moments of a normal distribution are [see 
(2.57), (2.59), and (2.60)]: 
0 
otherwise . 
Note that, in the equal index case of (5.15), N-1 is replaced by N for simpli- 
city. 
Moments off: Although we have not shown the higher order moments 
of yi's other than the second, it is not so difficult to generalize the discussion 
to obtain 
(5.16) 
i=l 
and 
E{O"'} = E { 0 ( 3 ' }  = . . . = o ,  
where 0") is the ith order term of the Taylor expansion in (5.1) [see Problem 

5 Parameter Estimation 
187 
2-51. Since N is large in general, we terminate the expansion at the second 
order throughout this book. 
Substituting (5.10) through (5.15) into (5.3), the bias term of the esti- 
mate, E { Af} = E { f }  - f, becomes 
(5.18) 
Note that the effect of N is successfully separated, and that g ( N )  of (5.5) 
becomes l/N. This is true for any functional form off, provided f is a function 
of the expected vectors and covariance matrices of two normal distributions. 
Similarly, the variance can be computed from (5.4), resulting in 
r 
(5.19) 
Note that, in order to calculate the bias and variance, we only need to compute 
af/amj’), df/acy, d2f/drnj’)*, and a2f/acj;)2 for I’ = 1,2. 
Non-normal cases: Even when the distributions of X are not normal, 
(5.11), (5.12), and (5.13) are valid as the first and second order moments. 

188 
Introduction to Statistical Pattern Recognition 
E { Ac$)Ac~~) 
} G 
However, the third and fourth order moments, (5.14) and (5.15), must be 
I 
1 
-Var{Axy)Axy)) for i # j ,  i = k, j = 
N 
1 
-Var 
{ Ax!'.)* } for i = j = k = 
N 
1 
-COV( A x ~ ) A x ~ ) ,  
A x ~ ) A x ~ ) }  
otherwise 
N 
, 
modified according to (2.53), (2.48), and (2.49), resulting in 
1 
N 
E {  Amy)Acfj) ] E -COV{ A x ~ , A x ~ A x ~ ~ ) ]  
, 
(5.20) 
(5.21) 
Subsequently, E(Af} of (5.18) and Var(Af] of (5.19) must be modified. How- 
ever, it must be noted that both (5.20) and (5.21) are proportional to 1/N. 
Thus, even for non-normal cases, we can isolate the effect of the sample size, 
and g ( N )  of (5.5) becomes 1/N. This means that we can adopt the estimation 
procedure off of (5.6). 
Bhattacharyya Distance 
A popular measure of similarity between two distributions is the Bhatta- 
charyya distance 
(5.22) 
Since p is a function of M I ,  M Z ,  
C,, and C 2 ,  it is a member of the family of 
functions discussed previously. 

5 Parameter Estimation 
189 
If two distributions are normal, the Bhattacharyya distance gives an 
upper bound of the Bayes error, E, as discussed in Chapter 3. The first and 
second terms of (5.22), pi and p2, measure the difference between the two dis- 
tributions due to the mean and covariance shifts respectively. 
When Mi and i, 
of (5.8) and (5.9) are used to compute p, the resulting 
i 
differs from its true value. The bias and variance of i 
can be obtained using 
(5.18) and (5.19). 
First term pI : From (5.22), the derivatives of pI with respect to M,. are 
(5.23) 
(5.24) 
where 
= (C, + C2)/2. The derivatives of pl with respect to e!;) can be 
obtained from (A.31) and (A.32). That is, 
where z, 
= (e!;) + cf))/2, x, = (h!” + h12’)/2, and m, = rn!” - m!’). 
hjl) = 1 and hj2’ =A,, 
Substituting (5.23) through (5.26) into (5.18) and (5.19), and noting that 
(5.28) 

190 
Introduction to Statistical Pattern Recognition 
Second term p2: Similarly, the derivatives of p2 can be obtained from 
(A.37) and (A.38). The results are 
(5.29) 
(5.30) 
(5.3 1) 
Substituting (5.29) through (5.31) into (5.18) and (5.19), and noting that 
hi') = 1 and hj2' =hi, 
(5.33) 
Discussions and experimental verification: Table 5- 1 shows the depen- 
dence of E ( Apl } and E ( Ap2 } on n and k (=N ln) for three different cases [4]. 
In the first case, two sets of samples are drawn from the same source Nx(U,I), 
a normal distribution with zero mean and identity covariance matrix. The 
second and third cases are Data 1-1 and Data I-41 (with variable n), respec- 
tively. As Table 5-1 indicates, for all three cases, E{Apl } is proportional to 
llk while E(Ap2) is proportional to (n+l)lk. Also, note that E(Apl ) is the 
same for the first and third cases because the sources have the same mean. 
Similarly, E{Ap2) is the same for the first and second cases because the 
sources share a covariance matrix. 
Since the trend is the same for all three cases, let us study the first case 
closely. Table 5-1 demonstrates that, in high-dimensional spaces (n >> I), 
E(Ap2J = 0.125(n+l)/k dominates 
E(Ap, } = 0.25lk. 
Also, 
E(Ap2) = 
0.125(n+l)/k indicates that an increasingly large value of k is required to main- 
tain a constant value of E 1 p 1 (= E { pl ) + E { p2 1) as the dimensionality 
increases. For example, Table 5-2 shows the value of k required to obtain 
,. 
1 
A 

5 Parameter Estimation 
TABLE 5-1 
191 
SAMPLE BIAS EXPRESSIONS FOR THE 
BHATTACHARYYA DISTANCE 
hi 
h; = 1 
hi = 1 
hi = 4  
PI 
0 
0.82 
0 
P2 
0 
0 
0.11 n 
E 
50% 
10% 
Depends on n 
0.25 
0.35 
0.25 
k 
k 
k 
- 
- 
- 
1 
n+l 
k 
E{ApZJ 
0.125- 
n+l 
k 
0.125- 
n+l 
0.08- k 
TABLE 5-2 
VALUES OF k AND N REQUIRED TO MAINTAIN E { i] 
I 
0.223 
I n 
I 
4 
8 
16 
32 
64 
128 1 
16 
50 
172 
628 
2407 
9396 
3.9 
6.2 
10.7 
19.6 
39.6 
l L = n k  1 
A 
E{pJ 
= 0.223 [4]. In this example, two sets of samples are drawn from the 
same source. Therefore, if an infinite number of samples is generated, the dis- 
tributions of two sets are identical (the Bayes error of 50% or p = 0). How- 
ever, with a finite number of samples, E ( p} is no longer equal to zero, indicat- 
ing that there exists a difference between two generated distributions. Accord- 
ing to (3.151), E ( p }  = 0.223 means that the overlap between them (the Bayes 
error) is less than 40%. The larger E { p) is, the smaller the overlap becomes. 

192 
Introduction to Statistical Pattern Recognition 
A 
Only 16 samples (3.9 times the dimensionality) are needed to achieve E (p) = 
0.223 in a 4 dimensional space, while 9396 samples (73.4 times the dimen- 
sionality) are needed in a 128 dimensional space. This result is sharply con- 
trasted with the common belief that a fixed multiple of the dimensionality, such 
as 10, could be used to determine the sample size. 
Since the theoretical results of (5.27) and (5.32) for biases and (5.28) and 
(5.33) for variances are approximations, we conducted three sets of experi- 
ments to verify the closeness of the approximations. 
Experiment 1: Computation of pI and p2 
Data: 1-1, I-41, I-A (Normal) 
Dimensionality: n = 4, 8, 16, 32, 64 (for I-I,1-4r) 
n = 8 (for I-A) 
Sample size: N I = N 2  = kn, k = 3, 5, 10, 20,40 
No. of trials: z = 10 
Results: Tables 5-3, 5-4, 5-5 [4] 
Tables 5-3 and 5-4 present a comparison of the theoretical predictions (first 
lines) and the means of the 10 trials (second lines) for Data 1-1 and Data 1-41 
respectively. These tables show that the theoretical predictions of the biases 
match the experimental results very closely. The third lines of Tables 5-3 and 
5-4 shows the theoretical predictions of the standard deviations from (5.28) and 
(5.33). The fourth lines present the experimental standard derivations from the 
10 trials. Again the theoretical predictions match the experimental results 
closely. It should be noted that the variances for i2 
of Data 1-1 and 
of Data 
1-41 are zero theoretically. This suggests that the variances for these cases 
come from the Taylor expansion terms higher than second order, and therefore 
are expected to be smaller than the variances for the other cases. This is 
confirmed by comparing the variances between pl and pz in each Table. Also, 
note that the variances of i2 
for Data 1-41 are independent of n. The similar 
results for Data I-A are presented in Table 5-5. The experimental results are 
well predicted by the theoretical equations for a wide range of k. 
A 
Verification of the estimation procedure: The estimation procedure of 
(5.6) was tested on Data RADAR as follows. 

5 Parameter Estimation 
193 
TABLE 5-3 
FOR DATA 1-1 
n 
4 
8 
16 
32 
64 
3 
1.1101 1.0758 1.0587 
1.0502 1.0459 
1.0730 0.9933 1.nsn2 1.0754 1.0825 
0.4688 0.3791 0.2221 
O.ISSI 0.09s~ 
0.3531 0.2497 0.1765 0.1248 0.0883 
5 0.9946 0.9740 0.9638 0.9586 0.9561 
1.0941 1.0702 1.03~6 0.9659 0.9764 
t 
10 0.9080 0.8977 0.8926 0.8900 
0.8887 
0.9593 0.9277 0.8421 0.9128 0.8~1 
I 
0.1368 0.0~67 0.0684 0.0483 
0.0342 
0.1356 0.1060 0.0929 0.0455 0.0387 
I1 
4 
8 
16 
32 
64 
3 0.2083 
0.3750 0.7083 
1.3750 2.7083 
0.2546 0.4106 0.8930 
1.7150 3.2875 
0 . m  0 . m  0 . m  0 . m  o.oo00 
0.0787 0.0653 0.0588 0.0776 0.1083 
5 0.1250 0.2250 0.4250 0.8250 
1.6250 
0.1 133 0.2791 0.5244 0.9252 
1.8035 
0 . m  0 . m  0 . m  0 . m  o.oo00 
0.0266 0.0785 0.058 I 
0.0302 0.0775 
f 
I O  0.0625 0.1 125 0.2125 0.4125 0.8125 
n.0803 
0.1 179 0.2280 0.4365 0.8578 
0 . m  0 . m  0 . m  O . m  0 . m  
0.0339 0.0191 0.0218 0.0279 0.0234 
20 0.0313 0.0563 0.1063 0.2063 0.4063 
0.0389 0.0566 0.1079 0.2099 0.4129 
0.0000 n.mn 0 . m  0 . m  o.0000 
O.OIOI 
0.0140 0.0132 0.0154 n.oos8 
40 0.0156 0.0281 0.0531 0.1031 0.2031 
0.0170 0.0282 n.ns61 0.1034 0.2061 
o.oo00 n.nwo 0 . m  n.oooo n.Ooo0 
0.0072 0.0084 0.0086 0.0046 0.0063 
(b) €{;?I 
(v? = O )  

194 
Introduction to Statistical Pattern Recognition 
TABLE 5-4 
FOR DATA 1-41 
n 
n 
4
8
 16 
32 
64 
4 
8 
16 
32 
64 
3 0.0833 0.0833 0.0833 0.0833 0.0833 
3 0.5796 
1.1326 2.2385 4.4503 8.8739 
0.1435 0.1212 0.1051 0.1118 0.1061 
0.7129 
1.0732 2.4527 4.7841 9.3263 
0 . m  0 . m  o.oo00 0 . m  0 . m  
0.1732 0.1732 0.1732 0.1732 0.1732 
0.0971 0.0633 0.0415 0.0385 0.0160 
0.1447 0.1653 0.2332 0.1893 0.1642 
5 0.0500 0.0500 0.0500 0.0500 0.0500 
5 0.5263 
1.0366 2.0572 
4.0983 8.1806 
0.0489 0.0709 0.0579 0.0545 0.0605 
0.5081 
1.0063 2.1341 4.1041 8.4000 
o.oo00 o.oo00 o.oo00 0 . m  o.oo00 
0. I342 0. I342 0. I342 0. I342 0. I342 
0.0284 0.0314 0.0141 0.0209 0.0071 
0.1119 o 1546 o 1129 0.0868 0.1209 
k 
IO 0.0250 0.0250 0.0250 0.0250 0.0250 
k 
IO 0.4863 0.9646 
1.9212 3.8343 7.6606 
0.0192 0.0267 0.0266 0.0276 0.0262 
0.4901 0.9463 
1.9345 3.8014 7.6630 
u . m  0 . m  o.oo00 0 . m  0 . m  
0.0949 0.0949 0.0949 0.0949 0.0949 
0.0151 0.0124 0.0066 0.0079 0.0035 
0.1016 0.0722 0.0759 0.0702 0.1206 
20 0.0125 0.0125 0.0125 0.0125 0.0125 
20 0.4663 0.9286 
1.8532 3.7023 7.4006 
0.0135 0.0156 0.0139 0.0120 0.0141 
0.4708 0.9331 
1.8277 3.7019 7.4049 
0.0000 o.ooO0 o.oo00 0 . m  o.oo00 
0.0671 0.0671 0.0671 0.0671 0.0671 
0.0055 0.0071 0.0036 0.0038 0.0025 
0.0658 0.0686 0.0966 0.0394 0.0672 
40 0.0063 0.0063 0.0063 0.0063 0.0063 
40 0.4473 0.9106 1.7886 3.5769 7.1536 
0.0066 0.0082 0.0056 0.0062 0.0065 
0.4713 0.8937 
1.8058 3.6374 7.2596 
0 . m  0 . m  o.oo00 0 . m  0 . m  
0.0474 0.0474 0.0474 0.0474 0.0474 
0.0045 0.0050 0.0021 
0.0014 0.0010 
0.0444 0.0328 0.0353 0.0563 0.0392 
(a) E I ~ ,  
I (pl = 0) 
(b) Eli21 
(p:=O.II n )  

5 Parameter Estimation 
k 
3 
5 
10 
20 
40 
195 
Mean 
Standard 
deviation 
Theor. 
Expt. 
Theor. 
Expt. 
1.6453 
1.5056 
0.3529 
0.4995 
1.4951 
1.5104 
0.2734 
0.1650 
1.3824 
1.3864 
0.1933 
0.1997 
1.3261 
1.3266 
0.1367 
0.1712 
1.2970 
1.3104 
0.0967 
0.0658 
TABLE 5-5 
i 
FOR DATA I-A 
1.443 1 
1.3002 
1.1929 
1.1393 
1.1 125 
1 S695 
0.1746 
1.2287 
0.1352 
1.1638 
0.0956 
1.1255 
0.0676 
1.1093 
0.0478 
10 
20 
Mean 
Standard 
deviation 
I I 
Theor. I Expt. I Theor. 
Expt. 
0.208 1 
0.1446 
0.0766 
0.0539 
0.0405 

196 
Introduction to Statistical Pattern Recognition 
Experiment 2: Estimation of p 
Data: RADAR (Real data, n = 66, E = unknown) 
pN: p estimated by using N I = N 2  = N samples 
N 
No. of sets per class 
pN 
8800 
1 
0.64 
720 
1 
1.57 
360 
2 
2.52* 
n 
n 
Estimation procedure: 
+ p = 0.62 
1.57 =p+v/720 
2.52 = p + v I360 
(*A set of 720 samples per class is divided to two sets of 360 
samples. With two sets from each class, there are 4 possible 
combinations of selecting one set from each class and forming a 
two-class problem. p3@ here is the average of the 4 cases.) 
A 
Although the radar data is not guaranteed to be normal, the above results 
indicate that the prediction of the true p from a relatively small number of 
samples (720 per class for the 66 dimensional space) seems possible. Also, 
note that p360, ~ 7 2 0 ,  and p88@) are significantly different. Without the compen- 
sation, p360 and ~ 7 2 0  could not provide a useful upper bound of the Bayes 
error. 
,
.
A
 
,. 
,. 
A 
5.2 Estimation of Classification Errors 
An even more important measurement in pattern recognition is the 
expected performance of a classifier. The discriminant functions for some 
popular classifiers, including the linear and quadratic classifiers, are functions 
of M I ,  
M 2 ,  XI, 
and X2. Thus, they are the members of the family of functions 
presented in the previous section. However, unlike the Bhattacharyya distance, 
the degradation of the expected classifier performance due to a finite sample 
size comes from two sources: the finite sample set used for design and the 
finite number of test samples. Thus, we need to study both their effects. 
For the two-class problem, a classifier can be expressed by 

5 Parameter Estimation 
197 
(5.34) 
where h ( X )  is the discriminant function of an n-dimensional vector X. The 
prohahiliries of error for this classifier from ol 
and o2 are from (3.105) and 
(3.106) 
(5.35) 
(5.36) 
where p i ( X )  represents the class i distribution to be tested. The roral prohahil- 
iry oj'error is 
where 
(5.37) 
(5.38) 
Effect of Test Samples 
Error expression: When a finite number of samples is available for test- 
ing a given classifier, an error-counting procedure is the only feasible possibil- 
ity in practice. That is, the samples are tested by the classifier, and the number 
of misclassified samples is counted. The other alternative is to estimate the test 
densities from the samples, and to integrate them in complicated regions. This 
procedure is, as seen in Chapter 3, complex and difficult even for normal distri- 
butions with known expected vectors and covariance matrices. 
In the error-counting procedure, pi(X) of (5.38) may be replaced by 

198 
Introduction to Statistical Pattern Recognition 
(5.39) 
where Xy), . . . ,X$! are Ni test samples drawn from pi(X), and 6(.) is a unit 
impulse function. 
Thus, the estimate of the error probability is 
(5.40) 
where 
Since af) 
is the inverse Fourier transform of l/jw, it becomes sign(h(Xy)))/2. 
That is, aft) is either +1/2 or -1/2, depending on h ( X 7 ) )  > 0 or h(X5;)) < 0. 
For i = 1, the a5I)’s are +1/2 for misclassified Xsl)’s and -1/2 for correctly 
classified X:”’s. Thus, summing up these 4)’)’s 
(5.42) 
1 
1 
= -(# 
of wl-errors) - - , 
N I  
2 
where (# of ul 
-corrects) = N I - (# of u1 -errors) is used to obtain the second 
line from the first. Likewise, for i = 2, 
(5.43) 
Substituting (5.42) and (5.43) into (5.40), 

5 Parameter Estimation 
199 
(# of ol-errors) 
(# of 02-errors) 
4- P2 
(5.44) 
& = P I  
N ,  
N2 
A 
That is, E of (5.40) is the error obtained by counting the number of 
misclassified samples with a given classifier. 
Moments of i: The expected value of aYi with respect to Xy' (w.r.t. the 
test sample) is 
(5.45) 
The second line of (5.45) can be obtained from (5.35) and (5.36) respectively. 
The second order moments are also computed as 
1 
4 '  
(5.46) 
- _  
- 
Equation (5.47) is obtained because a?' and 
independence between Xy) and Xlkj. 
are independent due to the 

200 
Introduction to Statistical Pattern Recognition 
From (5.40) and (5.45)-(5.47), 
a
1
 
E , { & }  = - 
2 + P I E l  - p 2 a 2  
= - 
1 + P*(E, - 2) 
1 - P 2 ( ,  1 - E21 = E ,  
2 
p: 
1 
1 
p: 
1 
1 
NI 4 
2 
N2 4 
= -[- 
- (E] - -)2] + -[- 
- (2 
- E2YI 
(5.48) 
(5.49) 
A 
That is, E is an unbiased and consistent estimate, no matter what h (X) is used. 
Error counting approach: When the error counting procedure is used, 
the effect of test samples can be analyzed in a more direct way. In order to 
estimate E ~ ,  
N j  samples are drawn from oj and tested by a given classifier. Let 
ij be the number of misclassified samples. Then, the random variables i, and 
q are independent, and each is binomially distributed as 
A 
A 
A 
2
A
 
P r { r ,  = 21,q = 2 2 }  = rIPr(2; 
= T i )  
i = l  
(5.50) 
I 
The 0;-error, E ~ ,  is estimated by q l N j  and, subsequently, the total probability 
of error is estimated by 
(5.51) 
The expected value and variance of the binomial distribution are known, and 
thus 

5 Parameter Estimation 
20 1 
(5.52) 
(5.53) 
These are the same as (5.48) and (5.49). 
Effect of Design Samples 
It is more difficult to discuss the effect of using afinite number of design 
samples. Although we would like to keep the formula as general as possible, 
in this section a specific family of discriminant functions is investigated to help 
determine which approximations should be used. 
Error expression: Assume that the discriminant function is a function of 
two expected vectors, M I  and M2, and two covariance matrices, X I  and C 2 .  
Typical examples are the quadratic and linear classifers as 
1 
2 
h ( X )  = -(X-M1)TC;I(X-M1) 
(5.54) 
(5.55) 
h ( X )  = ( M * - M 1 ) Y I X  + -(M:z-IM1-M;Z-lM2), 
1 
2 
where c = (Cl+X2)/2. When only a finite number of design samples is avail- 
able and M ,  and C; are estimated from them, h becomes a random variable and 
m 
Ah(X) = h(X) - h ( X )  = EO"!' 
, 
I=I 
(5.56) 
where h(X) = h(X,Ml,M,,%1,&), h ( X )  = h ( X , M I , M 2 , X l , X 2 ) ,  and 0"" is the 
kth order term of the Taylor series expansion in terms of the variations of M, 
and XI. If the design samples are drawn from normal distributions, and M, and 
Z, are unbiased estimates (e.g., the sample mean and sample covariance), 
,. 

202 
Introduction to Statistical Pattern Recognition 
(5.16) and (5.17) show 
where Ed indicates the expectation with respect to the design samples, and 97. is 
the number of design samples (while N indicates the number of test samples). 
Therefore, from (5.56) and (5.57), 
Assuming that ri is reasonably large, we can eliminate E ( Ahm(X) ] for rn larger 
than 2. 
From (5.37), the error of a random classifier for given test distributions is 
expressed by 
(5.59) 
When Ah is small, we may use the following approximation 
ejoh(X) = joh(X)ejwAh(X) E - ejoll(x)[ l+jmAh(x)+- tio)2 Ah2(X)] . 
e 
(5.60) 
2 
A 
Then, A& = 8: - E can be approximated by 
1 
A& 2- -jj{Ah(X) 
2x 
+ ~Ah2(X)Jc’W”‘X’p(X)dwdX 
2 
(5.61) 
Bayes classifier: When h ( X )  is the Bayes classifier for the given test 
distributions, F(X) = 0 at h (X) = 0. In this case, the Bayes error, E, is the 
minimum error and A& of (5.61) must be always positive. In order to confirm 
the validity of the error expression (5.59), let us prove AE 2 0 as an exercise. 
The first step to prove A& 2 0 is to show that the first order variation of 
(5.61) is zero regardless of Ah(X), as follows. 

5 Parameter Estimation 
203 
-~~Ah(X)rjwhcX)p”(X)dwdX 
1 
= jAh(X)G(h(X))i(X)dX 
2K 
= 
Ah(X)i(X)dX = 0 , 
(5.62) 
I1 ( X  )=o 
where we used the fact that the inverse Fourier transform of 1 is 6(.). Equation 
(5.62) becomes zero because F ( X )  = 0 at h ( X )  = 0. 
(5.61) is positive regardless of Ah(X). 
The second step involves showing that the second order variation of 
~ ~ ~ A h ’ ( X ) e i w h c x ) p ( X ) d ~ d X  
= -jAh2(X)- 
1 
dF(h) p(X)dX 
- 
. 
2n 
2 
dh 
(5.63) 
The derivative of the unit impulse, d6(h)/dh, is zero except in the region very 
close to h ( X )  = 0, where d6(h)/dh > 0 for h < 0 and dS(h)/dh < 0 for h > 0. 
On the other hand, i ( X )  > 0 for h < 0 and i ( X )  c 0 for h > 0. Since 
Ah2(X) > 0 regardless of Ah(X), (5.63) is always positive. 
Bias: The expected value of 2, E, with respect to the design samples is 
= & + E .  
Then, the bias, G, may be approximated by 
-jjEd 
1 
[ Ah(X) + -Ah2(X)}ejwh(X)i(X)dodX 
jo 
. 
2n 
2 
When h is a function of q parameters, y ,, . . . ,yq, Ah is, from (5.1) 
(5.64) 
(5.65) 
(5.66) 

204 
Introduction to Statistical Pattern Recognition 
Thus, discarding terms of higher order than 2, 
(5.67) 
(5.68) 
Note that Ed( Ayi ] = 0, assuming that unbiased estimates are used. Substitut- 
ing (5.67) and (5.68) into (5.65), 
Furthermore, if the parameters come from M , ,  M , ,  X I ,  and X2 of two normal 
distributions, as in (5. lo), G 
becomes 
A 
Equation (5.69) is a very general expression for the bias of the error, which is 
valid regardless of the selection of h ( X ) ,  PI, and p I  ( X ) .  The term E [ Ay,Ay, ] 
gives the effect of the sample size, YL. Therefore, if (5.5) is satisfied, 
can be 
expressed by vg ( E )  where v is determined by h (X), PI, and p,(X), and the 
estimation procedure of (5.6) can be applied. Furthermore, if h ( X )  is a func- 

5 Parameter Estimation 
205 
tion of M I ,  M , ,  E,, 
and &, g ( X )  becomes lln as is seen in (5.70). The qua- 
dratic and linear classifiers of (5.54) and (5.55) belong to this case. Therefore, 
for these classifiers. 
(5.71) 
The v of (5.71) is determined by the underlying problem, and stays constant 
for experiments with various sample sizes. Thus, we may choose various 
values of T, as Y , , .  . . , r u ,  and measure 2. Computing r as the average of 
several independent trials, we may solve (5.71) for E and v by a line fit tech- 
nique. 
Experiment 3: Estimation of the error for the quadratic classifier 
Data: RADAR (Real data, n = 66, E = unknown) 
Classifier: Quadratic classifier of (5.54) 
Test samples: N I = N 2  = 4400 (one set) 
Design samples: 
( L ,  = Yc2 = 4400, 720, 360 
E : The error of the quadratic classifier when 
per class are used. 
,. 
design samples 
,. 
r V  
No. of sets per class 
E (%) 
4400 
1 
20.2 
720 
1 
25.9 
360 
2 
30.1 * 
(*average of 4 possible combinations of 2 sets 
from each class - see Experiment 2.) 
Ectimation procedure: 
+ E =  21.7% 
25.9 = E + v 1720 
30.1 = ~ + ~ 1 3 6 0  
The estimated error by line fitting, 21.7%, is reasonably close to 
= 20.2%. 
This confirms that we can predict the potential performance of the quadratic 
classifier even if the available sample size is relatively small for a high- 
dimensional space (? ' , = ?? = 720 for n = 66.) Also, note that E
~
~
~
)
 
= 25.9% 
and E
~
~
~
)
 
= 30. I % are much higher than E ~ ~ ( ~ )  
= 20.2%. This suggests that nei- 
ther 
nor E ~ ~ ( ,  
can be uscd as reasonable estimates of the true performance 
of this quadratic classifier. 

206 
Introduction to Statistical Pattern Recognition 
Quadratic classifier: Although we do not need to know the value of v 
to conduct Experiment 3 to estimate E ,  v can be computed by obtaining the 
partial derivatives of h and carrying through the integration of (5.69) for a 
specific classifier. When the quadratic classifier of (5.54) with the parameters 
of (5.8) and (5.9) is adopted, the partial derivatives of h are, from (A.29)- 
(A.32) and (A.36)-(A.38), 
-- " - (-l)rA;'(X-A4r), 
aM,. 
(5.72) 
(5.73) 
(5.74) 
Substituting (5.72)-(5.75) into (5.70), and noting that 1:" = 1, hj2' = h- 
I +  
mil) = 0 and mj2' = m,, v of (5.71) becomes 
1 
v y  = -Jkq(X, 
2x 
o)ej""'X'p(x)dodX , 
(5.76) 
where 
1 
fJX, 0) = - 
2 
If { 
} 
(n+l)C .,f - 
I = I  
The integration of (5.76) can be carried out by using the procedure discussed in 

5 Parameter Estimation 
207 
Section 3.3. First, we get the closed-form solution for the integration with 
respect to X ,  and then take the one-dimensional integration with respect to o 
numerically. 
For the simplest case of Data 1-1, we can obtain the explicit expression 
for the integration of (5.76). In Data 1-1, p , ( X )  and p 2 ( X )  are normal NX(O,I) 
and N X ( M , I )  respectively. Then, e’oh(xip,(X) 
may be rewritten as 
(5.78) 
(5.79) 
where N,(a,h) and NX(D,K) 
are normal density functions of w and X with the 
expected value a and variance h for N,, and the expected vector D and covari- 
ance matrix K for N X .  
Since f,(X, o) 
is a linear combination of x;x, (k,L 5 4) as seen in (5.77), 
Ifq(X,o)NX(.;)dX 
is the linear combination of the moments of Nx(.,.). 
The 
result of the integration becomes a polynomial in o 
where - and + of T are for i = 1 and 2 respectively. 
Again, the 
JY,(o)N,(.,.)do is a linear combination of the moments of N,(.,.). Thus, vy 
for P I  = P2 = 0.5 is 

208 
Introduction to Statistical Pattern Recognition 
1 
e -M 'MI8 
vq = 
4 d s i i G  
(LW'M)~ M'M 
---1 
I] 
16 
2 
(5.81) 
In order to verify (5.81), the following experiment was conducted. 
Experiment 4: Error of the quadratic classifier 
Data: I-I (Normal, MTM = 2.56*, E = 10%) 
Dimensionality: n = 4, 8, 16, 32, 64 
Classifier: Quadratic classifier of (5.54) 
Design samples: ?I, 
= ?I2 = kn, k = 3, 5, 10, 20, 40 
Test: Theoretical using (3.119)-(3.128) 
No. of trials: z = 10 
Results: Table 5-6 [41 
In this experiment, kn samples are generated from each class, Mi and C j  are 
estimated by (5.8) and (5.9), and the quadratic classifier of (5.54) is designed. 
Testing was conducted by integrating the true normal distributions, 
p I(X) = Nx(O,I) and p 2 ( X )  = Nx(M,f), 
in the class 2 and 1 regions determined 
by this quadratic classifier, respectively [see (3.1 19)-(3.128)]. The first line of 
Table 5-6 shows the theoretical bias computed from (5.71) and (5.81), and the 
second and third lines are the average and standard deviation of the bias from 
the 10 trials of experiment. The theoretical prediction accurately reflects the 
experimental trends. Notice that v is proportional to n2 for n >> 1. Also, note 
that the standard deviations are very small. 
In theory, the Bayes error decreases monotonously, as the number of 
measurements, n, increases. However, in practice, when a fixed number of 
samples is used to design a classifier, the error of the classifier tends to 
increase as n gets large as shown in Fig. 5-1. This trend is called the Hughes 
phenomena [51. The difference between these two curves is the bias due to 
finite design samples, which is roughly proportional to n 2 / n  for a quadratic 
classifier. 
Linear classifier: The analysis of the linear classifier, (5.53, proceeds in 
a similar fashion. The partial derivatives of h may be obtained by using (A.30) 
and (A.33)-(A.35) as follows. 

5 Parameter Estimation 
209 
TABLE 5-6 
QUADRATIC CLASSIFIER DEGRADATION FOR DATA I-I (%) 
3 
5 
k 
IO 
20 
40 
4 
14.50 
16.68 
3.5 1 
12.70 
14.03 
2.1 1 
11.35 
11.52 
0.8 1 
10.67 
10.77 
0.2 1 
10.34 
10.37 
0.24 
8 
16.89 
20.4 1 
2.35 
14.14 
16.40 
1.86 
12.07 
12.40 
0.6 1 
11.03 
11.05 
0.23 
10.52 
10.57 
0.13 
n 
16 
21.15 
22.04 
2.89 
16.91 
17.34 
0.9 1 
13.45 
13.66 
0.70 
11.73 
11.90 
0.5 1 
10.86 
10.87 
0.13 
32 
30.67 
26.73 
1.95 
22.40 
20.8 1 
0.57 
16.20 
15.73 
0.54 
13.10 
13.93 
0.22 
11.55 
11.50 
0.13 
64 
48.94 
31.31 
1.33 
33.36 
25.54 
0.74 
2 1.68 
19.34 
0.85 
15.84 
15.13 
0.32 
12.92 
12.75 
0.18 
(5.82) 
(5.83) 

210 
Introduction to Statistical Pattern Recognition 
Classifier designed by 
(fixed) samples 
1 
* n  
Fig. 5-1 An example of Hughes phenomena. 
where 
(5.84) 
(5.85) 
(5.86) 
and El/ = (cf;) + cf))/2 and XI = (hf” + h;”)/2. 
(5.86) become 
In particular, when h;” = I ,  hj” = I J ,  
rn:” = 0, and rnj2) = rn, (5.84)- 
4 
-- - 
a2h 
a c y  
(I+h,)(l+h/) 
(5.87) 
(5.88) 
where 

5 Parameter Estimation 
21 1 
Substituting (5.82)-(5.89) into (5.70), v of (5.71) becomes 
where 
(5.89) 
(5.90) 
Since ,f,(X,w) is a linear combination of xf. (k I 2 ) ,  v,- can be theoretically 
computed for Data I-I, resulting in 
(5.92) 
Equation (5.92) was experimentally verified in the same manner as 
Experiment 5: Error of the linear classifier 
Data: 1-1 (Normal, M T M  = 2.562, E = 10%) 
Dimensionality: n = 4, 8, 16, 32, 64 
Classifier: Linear classifier of (5.55) 
Design samples: YI = y.2 = kn, k = 3, 5, 10, 20, 40 
Tcst: Theoretical using the normal error table 
No. of trials: z = 10 
Results: Table 5-7 [4] 
(5.81). The results are shown in Table 5-7. 

212 
Introduction to Statistical Pattern Recognition 
TABLE 5-7 
LINEAR CLASSIFIER DEGRADATION FOR DATA I-I (%) 
3 
5 
k 
10 
20 
40 
4 
12.73 
14.37 
3.65 
11.64 
11.65 
1.28 
10.82 
10.50 
.30 
10.4 1 
10.39 
.21 
10.20 
10.22 
.2 1 
8 
12.87 
14.36 
1.74 
11.72 
12.23 
1.53 
10.86 
10.89 
.4 1 
10.43 
10.39 
.18 
10.22 
10.27 
.I4 
n 
16 
12.94 
13.36 
1.35 
11.77 
12.07 
.7 1 
10.88 
10.93 
-24 
10.44 
10.58 
.26 
10.22 
10.21 
.09 
32 
12.98 
13.02 
.8 1 
1 1.79 
11.99 
.48 
10.89 
10.86 
.2 1 
10.45 
10.40 
.11 
10.22 
10.23 
.05 
64 
13.00 
13.19 
.40 
1.80 
2.07 
.4 1 
0.90 
10.92 
.I9 
10.45 
10.45 
.08 
10.22 
10.22 
-04 
Comparison of (5.8 1) and (5.92) reveals an important distinction between 
quadratic and linear classifiers. For Data 1-1, the two covariances are the same. 
Thus, if the true underlying parameters are used, the quadratic classifier of 
(5.54) becomes identical to the linear classifier of (5.55). However, when the 
estimated covariances, I:, f Cz, are used, the classifier of (5.54) differs from 
that of (5.55). As a result, E (A& 1 for the quadratic classifier is proportional to 
n2/n (= nlk) while E{Aa) for the linear classifier is proportional to 
nl?I(= Ilk) as in (5.81) and (5.92) when n >> 1. Although it depends on the 
a
n
 

5 Parameter Estimation 
213 
values of n and M T M ,  we may generally conclude that vg is larger than v, for n 
>> 1. This implies that many more samples are needed to properly design a 
quadratic classifier than a linear classifier. It is believed in general that the 
linear classifier is more robust (less sensitive to parameter estimation errors) 
than the quadratic classifier, particularly in high-dimensional spaces. The 
above results support this belief both theoretically and experimentally. 
Also note that for large n, v lY! is proportional to Ilk. This indicates 
that, as far as the design of a linear classifier is concerned, a fixed multiple 
could be used to determine the sample size from the dimensionality. However, 
(5.92) indicates that the value of the multiple depends on M T M ,  which meas- 
ures the separability between two distributions with a common covariance 
matrix 1. In particular, the less the separability between the two distributions, 
the greater k must be for a fixed bias. 
A 
Variance: The variance of E may be computed from (5.59) and (5.64) as 
1 
2 
- (E - -)2 
. 
(5.93) 
Applying the same approximation as (5.60) and keeping up to the second order 
terms of Ah, 
where 
jm, 
AC,(X) = Ah(X) + -Ah2(X) 
2 
. 
(5.95) 
Thus, (5.93) can be expanded to 

214 
Introduction to Statistical Pattern Recognition 
(5.96) 
The first line of (5.96) is ( ~ - 1 / 2 ) ~  
from (5.37), and the second and third lines 
are each ( ~ - 1 / 2 ) z  from (5.65). Furthermore, the summation of the first, 
second, third, and fifth lines is ( ~ - 1 / 2 ) ~  + 2(&--1/2)G - (E-1/2)2 = -s 
where 
E = E  + E. 
Since 
is proportional to E,(Ah(X) + joAh2(X)/2} (-1lrL) from 
(5.65) and (5.58), E2 is proportional to 1 / 9 *  and can be neglected for a large 
n. Thus, only the fourth line remains uncancelled. Thus, 
Vard(E) ZLjjjjEd(Ah(X)Ah(Y)} 
- 
1
2
 
-(&--) 
. 
2 
2 
- 
4x2 
= 
1 Ed(Ah(X)Ah(Y))p(X)F(Y)dXdY 
h ( X ) = o  h(Y)=O 
Equation (5.97) indicates that the integration is carried out along the 
classification boundary where h (X) = 0. When h ( X )  is the Bayes classifier, 
;(X) 
of (5.38) must be zero at the boundary. Thus, (5.97) becomes 0. Since 
we neglected the higher order terms of Ah(X) in the derivation of (5.97), the 
actual Vard{;) is not zero, but proportional to 1 P  according to (5.58). When 
h (X) is not the Bayes classifier, F(X) is no longer equal to zero at h ( X )  = 0. 
Thus, we may observe a variance dominated by a term proportional to 
E,(Ah(X)Ah(Y)). Since E,(Ah(X)Ah(Y)) is a second order term, it is propor- 
tional to 1/17. 

5 Parameter Estimation 
215 
8 -  
7 -  
6 -  
5 -  
4 -  
3 -  
2 -  
1 -  
In order to confirm the above theoretical conclusion, we can look at the 
third line of Table 5-6, which is the standard deviation of 10 trials in Experi- 
ment 4. Also, Fig. 5-2 shows the relationship between l/k(= ~ / T L )  
and the 
A standard 
deviation x 10 -3 
0 
X 
I' 
/ 
t/ 
/ 
$/ 
/ 
v
x
 
./ 
Ilk = n/N 
4 "  
/
e
 
o /  
I 
I 
I 
I 
I 
I 
I 
I 
I 
I - 
x n = 8  
n=64 
X 
Fig. 5-2 Quadratic classifier degradation for Data I-/. 
standard deviation [6]. From these results, we may confirm that the standard 
deviation is very small and roughly proportional to I/?,, except the far right- 
hand side where 77, is small and the approximation begins to break down. 
Thus, the variance is proportional to 1 K 2 .  
An intuitive reason why the standard deviation due to a finite number of 
design samples is proportional to l/r, may be observed as follows. When the 
Bayes classifier is implemented, A& is always positive and thus generates a 
positive bias. As (5.70) suggests, the bias is proportional to I/'C 
Since A& 
varies between 0 and some positive value with an expected value u/? (where Q 
is a positive number), we can expect that the standard deviation is also propor- 
tional to 1P:. 
In addition, it should be noted that design samples affect the variance of 
the error in a different way from test samples. When a classifier is fixed, the 
variations of two test distributions are independent. 
Thus, Var, ( E ]  = 
,. 

216 
Introduction to Statistical Pattern Recognition 
P:Var,(GI } + PiVar,{& 1 as seen in (5.49). On the other hand, when test dis- 
tributions are fixed and the classifier varies, 
and 
are strongly correlated 
with a correlation coefficient close to -1. 
That is, when 
decreases and vice versa. Thus, when P I = P2, Vard( 6 1 = 
Ed( A E ~  
} 
+ 
A&; } + 2(0.5)2 E d  { AEI A E ~  
} g(0.5)2 [Ed ( AEy I+ E d  { 
)' }+ 
2 E d { A ~ i  
(-AE~) 11 = 0. The covariance of 2, and G2 cancels the individual 
variances of 
and E ~ .  
* 
* 
* 
* 
increases, 
* 
* 
Effect of Independent Design and Test Samples 
When both design and test sample sizes are finite, the error is expressed 
as 
where 
(5.98) 
(5.99) 
I* 
That is, the randomness comes from h due to the finite design samples as well 
as from the test samples XI!'). 
The expected value and variance of E can be computed as follows: 
* 
* 
- 
*
1
 
2 
& = E ( & }  = E t E d { & }  = - + P I E l  - P 2 Z 2  
(5.100) 
where 
(5.101) 
Substituting (5.101) into (5.100), 

5 Parameter Estimation 
217 
- 
E =  PIE, + P2E2 . 
(5.102) 
This average error is the same as the error of (5.64). That is, the bias of the 
error due to finite design and test samples is identical to the bias due to finite 
design samples alone. Finite test samples do not contribute to the bias. 
The variance of 
can be obtained from (5.98) as 
- ( I )  
- ( 2 )  
- 2PlP2Cov{a, , a k  } , 
where 
(5.103) 
(5.104) 
1
-
1
 
4 
2 
- 
- - -  (E; - - ) 2  =E;(I - E ; ) ,  
- _  
-Cl;ak . 
(5.105) 
The second line of (5.104) can be derived from the first line as seen in (5.46). 
From (5.109, a portion of (5.103) can be expressed as 
P:cov(a, ,ak I +P;Cov{a, , a L  I - ~ P , P ~ c o v { ~ ,  
,ar I 
- ( I )  
- ( I )  
- ( 2 )  - ( 2 )  
- ( I )  
-(2) 
e 
= Var,{e) , 
(5.106) 
where Vard{i) is the same one as (5.97). On the other hand, (5.105) can be 
approximated as 

218 
Introduction to Statistical Pattern Recognition 
1 
n 
- -  
(5.107) 
Equation (5.107) can be derived by replacing ;(X) in (5.97) with pi(X). Qua- 
tion (5.107) is proportional to Iln because Ed(Ah(X)Ah(Y)) is proportional to 
11%. 
Substituting (5.104)-(5.107) into (5.103), and ignoring the terms propor- 
tional to l/Nin, 
As we discussed in the previous section, Vard { 61 is proportional to lln2 
when the Bayes classifier is used for normal distributions. Therefore, Var(;] 
of (5.108) is dominated by the first two terms which are due to the finite test 
set. A comparison of (5.108) and (5.49) shows that the effect of the finite 
design set appears in El and E2 of (5.108) instead of E I  and ~2 of (5.49). That 
is, the bias due to the finite design set increases the variance proportionally. 
However, since (Ei -&,)-I/%, 
this effect can be ignored. It should be noted 
that Vard( E }  could be proportional to 117l if the classifier is not the Bayes. 
When both design and test sets are finite, 
1. 
,. 
Thus, we can draw the following conclusions from (5.102) and (5.108). 
the bias of the classification error comes entirely from the finite design 
set, and 
the variance comes predominantly from the finite test set. 
2. 

5 Parameter Estimation 
219 
5.3 Holdout, Leave-One-Out, and Resubstitution Methods 
Bounds of the Bayes Error 
When a finite number of samples is given and the performance of a 
specified classifier is to be estimated, we need to decide which samples are 
used for designing the classifier and which samples are for testing the 
classifier. 
Upper and lower bounds of the Bayes error: In general, the 
classification error is a function of two sets of data, the design and test sets, 
and may be expressed by 
where, F is a set of two densities as 
lF = ( P  I(X), P2(X)I . 
(5.1 10) 
If the classifier is the Bayes for the given test distributions, the resulting error 
is minimum. Therefore, we have the following inequality 
E ( P T , P ; 7 )  I 
E ( P ' D , P T )  . 
(5.1 11) 
The Bayes error for the true I" is E(P,P). However, we never know the true 
5). One way to overcome this difficulty is to find upper and lower bounds of 
E(P,P) based on its estimate @' = (pI(X),p2(X)}. 
In order to accomplish this, 
let us introduce from (5.1 11) two inequalities as 
A
*
 ,. 
E(P,F) I 
E(;, 
P )  , 
(5.1 12) 
(5.1 13) 
1 
Equation (5.1 12) indicates that F is the better design set than 0' 
for testing F. 
Likewise, ; 
is the better design set than P for testing ;. 
Also, it is known 
from (5.48) that, if an error counting procedure is adopted, the error estimate is 
unbiased with respect to test samples. Therefore, the right-hand side of (5.112) 
can be modified to 
1 
1.. 
E( 5', F )  = E & ( E( 5', 
P T )  } , 
(5.1 14) 
where PT is another set generated from D independently of P. Also, after tak- 
ing the expectation of (5.1 13), the right-hand side may be replaced by 
1 
,. 

220 
Introduction to Statistical Pattern Recognition 
E{&(@,;)) = E(P,P). 
(5.115) 
Thus, combining (5.1 12)-(5.115), 
E{&(;,;)} I 
&(P,P) I 
E&T{&(;,&)] 
(5.1 16) 
That is, the Bayes error, e(@,@), is bounded by two sample-based estimates 
WI. 
The rightmost term 
is obtained by generating two independent 
sample sets, F and I&, from P, and using 
for designing the Bayes classifier 
and PT for testing. The expectation of this error with respect to PT gives the 
upper bound of the Bayes error. Furthermore, taking the expectation of this 
result with respect to 
does not change this inequality. Therefore, 
E >E ;T { &(P, 
PT) 
} also can be used as the upper bound. This procedure is 
called the holdout (H) method. On the other hand, E($,;) 
is obtained by 
using P for designing the Bayes classifier and the same P for testing. The 
expectation of this error with respect to 0' gives the lower bound of the Bayes 
error. This procedure is called resubstitution (R) method. 
The holdout method works well if the data sets are generated artificially 
by a computer. However, in practice, if only one set of data is available, in 
order to apply the holdout method, we need to divide the available sample set 
into two independent groups. This reduces the number of samples available 
for designing and testing. Also, how to divide samples is a serious and non- 
trivial problem. We must implement a proper dividing algorithm to assure that 
the distributions of design and test samples are very close. In addition, how to 
allocate samples to design and test is another problem. Since we know the 
effects of design and test sample sizes from the discussion of Section 5.2, we 
may determine how samples should be allocated to design and test by balanc- 
ing the resulting bias and variance. As seen in Section 5.2, the bias is deter- 
mined by the size of the design samples, while the variance is primarily deter- 
mined by the size of the test samples. 
A procedure, called the leave-one-out (L) method, alleviates the above 
difficulties of the H method [13]. In the L method, one sample is excluded, the 
classifier is designed on the remaining N-1 samples, and the excluded sample 
is tested by the classifier. This operation is repeated N times to test all N sam- 
ples. Then, the number of misclassified samples is counted to obtain the esti- 
,. 
,. 
A , .  
,. 
,. 
A 

5 Parameter Estimation 
22 1 
mate of the error. Since each test sample is excluded from the design sample 
set, the independence between the design and test sets is maintained. Also, all 
N samples are tested and N-1 samples are used for design. Thus, the available 
samples are, in this method, more effectively utilized. Furthermore, we do not 
need to worry about dissimilarity between the design and test distributions. 
One of the disadvantages of the L method is that N classifiers must be 
designed, one classifier for testing each sample. However, this problem is 
easily overcome by a procedure which will be discussed later. 
The H and L methods are supposed to give very similar, if not identical, 
estimates of the classification error, and both provide upper bounds of the 
Bayes error. In order to confirm this, an experiment was conducted as follows. 
Experiment 6: The H and L errors 
Data: 1-1 (Normal, MTM = 2.562, E = 10%) 
Dimensionality: n = 4, 8, 16, 32, 64 
Classifier: Quadratic classifier of (5.54) 
Sample size: n, 
= n2 = kn (Design) 
N I = N 2  = kn (Test) 
N I  = N ~ = k n f o r L  
k = 3, 5 ,  10, 20, 40 
for 
No. of trials: T =  10 
Results: Table 5-8 
The first and second lines of Table 5-8 show the average and standard devia- 
tion of the H error estimate, while the third and fourth lines are the average and 
standard deviation of the L error estimate. Both results are very close. 
Operation of the L method: In order to illustrate how the L method 
works, let us examine the simplest case in which two covariances are equal and 
known as 1. Then, the Bayes classifier is 
0 1  
w2 
( x - M , ) ~ ( x - M ~ )  - ( X - M , ) ~ ( X - M , )  >< r . 
(5.1 17) 
Assume that two sample sets, SI = (XI'), . . . , X g !  } from o1 and S 2  = {X\2), 
. . . ,X$: } from 02, are given. In the R method, all of these samples are used 
to design the classifier and also to test the classifier. With the given mathemat- 

222 
Introduction to Statistical Pattern Recognition 
TABLE 5-8 
COMPARISON OF THE H AND L ERRORS FOR DATA I-I (%) 
4 
3 
21.25 
7.97 
17.92 
1 1.63 
5 
13.25 
5.66 
12.25 
3.43 
k 
IO 
10.75 
2.30 
9.88 
3.79 
20 
11.25 
2.28 
11.13 
1.97 
40 
10.78 
1.39 
10.41 
2.1 1 
8 
20.00 
4.73 
17.50 
6.82 
16.50 
4.56 
17.75 
7.40 
10.63 
2.55 
11.50 
3.13 
10.19 
1.20 
12.47 
1.69 
10.14 
1.02 
10.05 
1.22 
n 
16 
20.42 
5.32 
23.33 
3.44 
17.63 
3.47 
15.56 
3.03 
14.13 
I .98 
13.63 
2.5 1 
11.75 
1.41 
12.55 
1.57 
1 1.44 
.89 
10.22 
.59 
32 
26.35 
2.25 
25.36 
4.0 1 
21.37 
2.7 1 
20.75 
2.72 
16.61 
1.17 
15.45 
2.09 
13.17 
.98 
12.75 
1.08 
12.01 
.8 1 
11.53 
.54 
64 
30.55 
2.53 
31.61 
2.57 
24.58 
1.89 
25.9 1 
1.70 
19.55 
1.28 
19.05 
1.52 
14.98 
.67 
15.25 
1.07 
12.59 
.48 
12.81 
.52 

5 Parameter Estimation 
223 
ical form of (5.117) for the classifier, designing means to estimate the neces- 
sary parameters - in this case two expected vectors by using the sample means, 
M i ,  of (5.8). Using the error counting procedure, each sample is tested by 
n 
01 
( X f G  J ( X f ’ - f i  
I )  - ( X f ) - k 2 ) 7 j X f ’ - M 2 )  >< I 
(5.1 18) 
0 2  
(i = 1,2 : k = 1 , .  . .,N;) . 
If Xi!’ 
does not satisfy <, the sample is labeled as an error. Likewise, Xp), 
which does not satisfy >, is labeled as an error. The R error is the number of 
errors divided by the total number of samples. 
On the other hand, in the L method, Xf) must be excluded from the 
design set when X f ’  is tested. The mean estimate without Xf’, Mik, may be 
computed as 
Therefore, testing an oI 
-sample, Xi’), can be carried out as follows. 
(XZ“-h Ik)T(Xp-h 1,) - (XiI’-M2) 
* 
7- ( X ,  
( 1 )  - fi 2 ) 
(5.120) 
(5.121) 
,. 
,. 
Note that, when Xi’) is tested, only M I  is changed and A 4 2  is not changed. 
Likewise, when an a2-sample, Xi2’, is tested, 
( X p h  ,7-(xp’-i I )  - (XP’-~2,)T(Xi2’-fi2k) 
* 
01 
(5.122) 
Equations (5.121) and (5.122) reveal that the modification from the R method 
to the L method is simply to multiply a scalar [N,l(Ni-1)]2 with a distance. 
Since the distance computation in a high-dimensional space needs much more 
1
2
 
= ( X p - M  i)T(Xp-M )-(-)2(x~2)-h~)7-(x~2’-M*) 
>< t . 
N2-1 
0 2  

224 
Introduction to Statistical Pattern Recognition 
computation time, the addition of a scalar multiplication is negligibly small. 
Thus, we can perform both R and L methods simultaneously within the compu- 
tation time needed to conduct the R method alone. In other words, (5.121) and 
(5.122) give a simple perturbation equation of the L method from the R method 
such that we do not need to design the classifier N times. 
The perturbation factor of N,l(Ni-l) is always larger than 1. This 
increases (Xi”-k 
I f(Xi’)--k 
I ) for an ol 
-sample, X i ’ ) ,  and (Xi2)-k2)T 
(Xi2)-k2) 
for an 02-sample, Xi2). For wI, 
Xi’) is misclassified if > is satisfied 
in (5.121). Therefore, increasing the (Xi1)--h1 )T(Xi’)-k 
I )  term by multiplying 
[N1/(Nl-1)I2 means that Xi” has more chance to be misclassified in the L 
method than in the R method. The same is true for Xi2) in (5.122). Thus, the 
L method gives a larger error than the R method. This is true even if the 
classifier of (5.117) is no longer the Bayes. That is, when the distance 
classifier of (5.1 17) is used, the L error is larger than the R error regardless of 
the test distributions. 
The above discussion may be illustrated in a one-dimensional example of 
Fig. 5-3, where m I and m2 are computed by the sample means of all available 
,. 
L. 
I 
I 
I 1
4
 
dlR =!= 
d2R 
I 
I 
I 
I 
I - 
d,L 
- 1 -  
d2L * 
I 
I 
Fig. 5-3 An example of the leave-one-out error estimation. 
samples, and each sample is classified according to the nearest mean. For 
example, x i ’ )  is correctly classified by the R method, because dlR<dZ and 
thus xi’) is classified to ol. 
On the other hand, in the L method, xi!) must be 
excluded from estimating the ol 
-mean. The new sample mean, m Ik, is shifted 
to the left side, thus increasing the distance between xi” and A I k ,  dlL, On the 
other hand, dZL is the same as d,. 
Since d l L  > d2L, xi’) is misclassified to o2 
in the L method. 
L. 

5 Parameter Estimation 
225 
The R and L Methods for the Quadratic Classifier 
Perturbation equation: The above discussion may be 
d to the 
more complex but more useful quadratic classifier [14]. In the quadratic 
classifier of (5.54), we need a perturbation equation for the covariance matrix 
in addition to the mean vector of (5.1 19) as follows. 
c, = - 
- (xZ"-h;p)(Xi!'-hjk) 
N,-2 
= E , + -  
1
-
 
N, 
(Xf)-h,)(xp-h,y 
. 
N,-2 " - (N,-1)(Nf-2) 
The inverse matrix of & can be obtained from (2.160) 
r 
xtend 
(5.124) 
- 2  
- 2  
where d, ( X p ) )  = (Xt'-h,f ?L'(XV)-h,). 
The L distance, d f a ( X f ' ) ,  is from 
(5.120) and (5.125) 
& X f ) )  = (Xp-hJi; (Xf'-h,,) 
-4 
Njd, ( X f ' )  
(N, - 1 )2-N, &xp, 
-2 
(N,2-3Nf+1 )iz ( X f  ))/(N, -l)+N,i:(X:')) 
= d, ( X f ' )  + 
(5.1 26) 
(N, - 1 )2-N, 2; (Xp ), 
Also, using (2.143), the determinant of 
can be calculated as follows. 

226 
Introduction to Statistical Pattern Recognition 
(5.127) 
Nj-1 
* 
N, 
-2 
I iik 
I = (-y 
I E; I [ 1 - -d; 
(Xi!’)] 
Nj-2 
(N, - 1 )2 
Or, taking the logarithm, 
(5.128) 
Let &(Xt)) and i L ( X f ) )  be the R and L discriminant functions with fi, and i, 
replacing M, and C, for & ( X f ) ) ,  and klL 
and ilk 
replacing MI and C, for 
& ( X t ’ ) ,  respectively. Then, substituting (5.126) and (5.128) into (5.54), 
+g(N ,$(Xi’’)) 
for ol 
(5.129) 
- g ( ~ ~ , i S ( x i ~ ) ) )  
for o2 , 
hL(Xf’) - i R ( X f ) )  = 
where 
When the R method is used to count the number of misclassified samples, 
d, ( X f ’ )  and hR(Xf)) must be computed for k = 1 , .  . . ,N, and i = 1,2. There- 
fore, the additional computation of the scalar function of (5.130) for each k is a 
negligible load for a computer in comparison with the computation of & ( X f ) )  
for each k, which includes vector-matrix operations in a high-dimensional 
space. Thus, the computation time of both the R and L methods becomes 
almost equivalent to the computation time of the R method alone. Remember 
that, in the R method, we are required to design only one classifier. Since hL 
can be obtained from hR by a trivial perturbation equation, we do not need to 
design the classifier N times in the L method. 
-2 
A 
- 

5 Parameter Estimation 
221 
Proof of g 2 0: The perturbation term, g, of (5.130) is always positive 
Assuming N, > 2, I &  
I of (5.127) should be positive because ilk 
is a 
- 2  
no matter what N,, d, ( X t ) ) ,  and n are. The proof is given as follows. 
sample covariance matrix and should be a positive definite matrix. Therefore, 
N, 
A2 
1 -  
d; (Xi!)) > 0 
(N, - 
(5.131) 
On the other hand, from (5.130) 
A2 
ag =1 
(N:-3N, + l)/(N, - 1 )+2N, d, 
- 
-2 
32; 
(N,-I)’ - N,d, 
1 [ (N;-3N,+ 1 ) i f / ( N ,  - 1 )+N, i:]N, 
2 
[(N,-l)’ - N,2f12 
+ -  
1 
-Nj/(Nj-1)’ 
-2 
+ -  
l-[Nj/(Nj-1)’]d; 
-4 
- 2  
1 -N’d; +N; (2N:-3N, +2)dj -(Nj- 1)(2N;- 1 ) 
- -  
- 
(5.132) 
2 
[(N;-l)’ - NI$l2 
A2 
The term &lad; is equal to zero when 
A2 
1 
3N: - 3Nj! + Ni 
d j  = - or 
Ni 
N; 
(5.133) 
The second solution of (5.133) does not satisfy the condition of (5.131). Since 
g and aglad; for dj = 0 are positive and negative, respectively, the first solu- 
tion of (5.133) gives the minimum g, which is 
-2 
- 2  
1 
+ - I n -  
+ - In[] - 
~ 
1 (Nj!-3Nj+I)/(N;-I)+l 
n 
Nj-1 
1 
1 
2 
Nj[(Ni-1)2-1] 
2 
Nj-2 
2 
(N;- 1 ) 2  
- 
1 
(N1-1Y-l N,-1 
n-1 
N,-1 
- _  
- 1
1
 + - I n [  
-1 
+ - 
2 
In- N,-2 
2 N,(N,-l) 
2 
(N,-l)’ 
N,-2 

228 
Introduction to Statistical Pattern Recognition 
1 
Ni 
n-1 
Ni-1 
1
1
 
2 Ni(N;-l) 
2 
Ni-1 
2 
Ni-2 
+ - ln- 
+ - 
In- 
> O  
- _  
- 
(5.134) 
for Ni > 2. The inequality of (5.134) holds since the numerators of the second 
and third terms are larger than the corresponding denominators. 
Comments: Since g of (5.130) is always positive, hL > hR for 01- 
samples and hL < hR for y-samples. Since h > 0 is the condition for ol- 
samples to be misclassified and h < 0 is the condition for 02-samples to be 
misclassified, the L method always gives a larger error than the R method. 
This is true for any test distributions, and is not necessarily limited to normal 
distributions. Note that this conclusion is a stronger statement than the ine- 
quality of (5.116), because the inequality of (5.1 16) holds only for the expecta- 
tion of errors, while the above statement is for individual samples of individual 
tests. 
Since we have the exact perturbation equation of (5.130), the use of this 
equation is recommended to conduct the R and L methods. However, for 
further theoretical analysis, (5.130) is a little too complex. An approximation 
of (5.130) may be obtained by assuming N; >> dj and Ni >> 1. When X is 
distributed normally, it is know that d2(X) has the chi-square distribution with 
an expected value of 
n and standard deviation of 6, 
where 
d 2 ( X )  = (X-M)TC-’(X-M) [see (3.59)-(3.61)]. Therefore, if N >> n, the 
approximation based on N >> d 2  is justified. Also, In( 1+6) Z6 for a small 6 is 
used to approximate the second and third terms of (5.130). The resulting 
approximation is 
A
,
.
 
- 2  
- 2  
1 
- 4  
g (Ni,di ( X f ) ) )  E -[d; 
(Xf’)+n] . 
2Ni 
(5.135) 
In order to confirm that the L and R methods give the upper and lower 
bounds of the Bayes error, the following experiment was conducted. 
Experiment 7: Error of the quadratic classifier, L and R 
Data: I-A (Normal, n = 8, E = 1.9%) 
Classifier: Quadratic classifier of (5.54) 
Sample size: N 1  = N 2  = 12, 50, 100, 200, 400 
No. of trials: z = 40 
Results: Table 5-9 [ 141 
As expected, the L and R methods bound the Bayes error. 

5 Parameter Estimation 
229 
TABLE 5-9 
THE BOUNDS OF THE BAYES ERROR BY THE L AND R METHODS 
No. 
Resubstitution method 
Leave-one-out method 
Bias 
of samples 
between 
N I = N 2  
Mean (%) 
deviation (%) 
Mean (%) 
deviation (%) 
means (%) 
12 
0.21 
1.3 
18.54 
7.6 
18.33 
so 
1.22 
0.9 
2.97 
1.7 
I .75 
100 
I .44 
0.8 
2.1s 
1 .o 
0.71 
200 
1.56 
0.7 
2.00 
0.7 
0.44 
400 
1.83 
0.5 
1.97 
0.5 
0.14 
per class 
Standard 
Standard 
two 
Effect of removing one sample: Generalizing the above discussion, let 
us study how the removal of one sample affects the estimate o f f  which is a 
function of M and Z. Let kR 
and iR 
be the sample mean and sample covari- 
ance computed from all available samples, and let ML and ZL be the 
corresponding estimates without a sample Y. From (5.119) and (5.124), we 
may express ML and C, in terms of MR, CR and Y as 
A 
A 
.. 
.. 
I
I
n
 
where N >> 1 is assumed. The terms associated with 1/N are small deviation 
terms. Therefore, we may approximate M R  and CR by the true parameters A4 
and C whenever they appear in [.]lN, resulting in 
A 

230 
Introduction to Statistical Pattern Recognition 
(5.138) 
(5.139) 
1 
N 
1 
N 
i f L  GIR - - ( Y - M ) ,  
iL 
E& + -[[c - (Y-M)(Y-M)T] . 
Now, let us consider the problem of estimating f (M,C) by f(if~,iz), 
where f is a given function and Z is either R or L, depending on which esti- 
mates are used. The estimate f (kz,&) 
may be expanded by a Taylor series 
around f (M, C) as 
afT 
A 
f (kz,&) 
Ef (M,C) + -(Mz 
- M )  + tr 
aM 
where 
Then, the difference between f (ifL,&) and f(ifR,&) is 
= f ( k L , % , )  - j ~ i f R , i R )  
E 
-- afr (AML-AMR) + tr 
aM 
(5.140) 
(5.141) 
(5.142) 
where AM, = if, - M and AXz = iZ 
- C (Z = R or L). 

5 Parameter Estimation 
23 1 
Example 1: Let f be 
(5.143) 
1 
1 
2 
2 
~ ( x , M , x )  
= - ( x - M ~ x - ' ( x - M )  + - In 1x1 . 
Then, 
(5.144) 
-- 
af' - -C-'(x-M) , 
-- 
a' 
- 1[Z-'-r-'(X-M)(X-M)7z-'] 
[from (A.41)-(A.46)] . 
aM 
(5.145) 
ax 
2 
If a sample Y is excluded, of (5.142) becomes 
h(X,Y) = - 
1 [ ( ( x - M ) ~ z - 1 ( Y - M ) p  + n + 2(x-M)7x-'(Y-M) 
2N 
- (X-M)TZ-'(X-M) - (Y-M)TX-'(Y-M)] . 
(5.146) 
Example 2: Iff is evaluated at X = Y ,  h of (5.146) becomes 
1 
2N 
h(Y,Y) = -[d4(Y) 
+ n ]  , 
(5.147) 
where d 2 ( Y )  = (Y-M)TX-'(Y-M). 
Equation (5.147) is the same as (5.135) 
except that the true parameters M and Z are used this time instead of Gj and i, 
for (5.135). 
Resubstitution Error for the Quadratic Classifier 
Error expression: When the L method is used, design and test samples 
are independent. Therefore, when the expectation is taken on the classification 
error of (5.98), we can isolate two expectations: one with respect to design 
samples and the other with respect to test samples. In (5.101), the randomness 
of h comes from design samples, and X is the test sample, independent of the 
design samples. Therefore, Ed [ doh(') 
} can be computed for a given X. The 
expectation with respect to test samples is obtained by computing j [ . ] p j ( X )  dX. 
On the other hand, when the R method is used, design and test simples are no 
longer independent. Thus, we cannot isolate two expectations. Since X is a 

232 
Introduction to Statistical Pattern Recognition 
,. 
design sample as well as a test sample, both contribute to the variation of h. 
Thus, we can no longer use the same argument as in Section 5.2 to compute 
Ed{ 
However, the relationship between iL 
and LR is given exactly by 
(5.129) and (5.130) and approximately by (5.147). Therefore, using the 
approximation of (5.147) for simplicity, 
(5.148) 
1 
iR(X$I)) G L ( X $ ' ) )  - -[d:(X$')) 
+ n ]  for 01 , 
2N I 
Applying (5.148), (5.149) and another approximation, eJwoiN : 
1 + j w / N  
(valid for large N), the classification error by the R method, eR, may be calcu- 
lated from (5.98) as 
* 
where 
and cL is the classification error by the L method. 
(5.150) 
(5.151) 
A 
,. 
Moments of the bias: By converting hR to hL, design and test samples 
are now independent, and the discussion of Section 5.2 can be directly applied. 

5 Parameter Estimation 
233 
A 
-
A
 
Therefore, the statistical properties of the bias, &h = E ~ - & ~ ,  
can be studied. 
The expected value of &h is 
,. 
where 
(5.152) 
(5.153) 
A 
And, the variance of &h is 
Example 3: The explicit expression for p, of (5.153) can be obtained by 
using the same technique used to compute (5.81), if two distributions are nor- 
mal with M I  
= 0, M 2  = M, and C, = Z2 = I ,  and the quadratic classifier of 
(5.54) is used. For N I = N 2  = N 
Ed{ c ’ ~ ” ‘ ~ )  l””‘x’[l+jwEd( 
Ah(X)+&Ah2(X) )]Ee/o”‘X’ 
. 
(5.155) 
2 
) &e 
The last approximation was made because E,, { Ah + jwAh2/2 ] is proportional 
to 1/N. Then, e/w”(x’p,(X) 
( i  = 1,2) are given in (5.78) and (5.79). Thus, the 
integration of (5.153) merely involves computing the moments of the normal 
distributions of (5.78) and (5.79), resulting in 
M’M 
( M 7 M ) 2  M T M  
---I 
I ] .  
16 
2 
.-.X’-. 
2 
117 +{ 
1 
2 d G z G  
Ir, 
(5.156) 
Note that PI of (5.156) is exactly twice v, of (5.81). That is, the bias between 

234 
Introduction to Statistical Pattern Recognition 
the L and R errors is twice the bias between the L and true errors. 
In order to confirm (5.156), the following experiment was conducted. 
Experiment 8: Bias between the L and R error 
Data: I-I (Normal, MTM = 2.562, E = 10%) 
Dimensionality: n = 4, 8, 16, 32, 64 
Classifier: Quadratic classifier of (5.54) 
Sample size: N I = N 2  = kn, k = 3, 5, 10, 20, 40 
No. of trials: z = 10 
Results: Table 5-10, Fig. 5-3 [6] 
The first line of Table 5-10 indicates the theoretical biases from (5.152) and 
(5.156), and the second and third lines are the average and standard deviation 
of the 10 trial experiment. Despite a series of approximations, the first and 
second lines are close except for small k's and large n's, confirming the vali- 
dity of our discussion. 
An important fact is that, from (5.152) and (5.156), E ( & ]  is roughly 
proportional to n2/N for large n. A simpler explanation for this fact can be 
obtained by examining (5.153) more closely. Assuming (5.155) and carrying 
through the integration of (5.153) with respect to O, 
(5.157) 
It is known that d:(X) is chi-square distributed with an expected value of n and 
standard deviation of d%, 
if X is normally distributed [see (3.59)-(3.61)]. 
This means that, when n is large, d?(X) is compactly distributed around the 
expected value n (Le. n >> 6.) 
Therefore, d;(X) on the classification boun- 
dary should be close to n2. Thus, 
The analysis of the variance (5.154) is more complex. Though the order 
of magnitude may not be immediately clear from (5.154), the experimental 
results, presented in Fig. 5-4 and the third line of Table 5-10, show that the 
standard deviation is roughly proportional to 1/N. The intuitive explanation 
should be the same as that presented in Section 5.2. 
should be roughly proportional to n 2 .  

5 Parameter Estimation 
235 
TABLE 5-10 
BIAS BETWEEN L AND R ERRORS FOR DATA 1-1 (%) 
4 
3 
9.00 
13.33 
7.03 
5 
5.40 
7.50 
4.56 
k 
10 
2.70 
2.2s 
1.84 
20 
1.35 
1.38 
1.05 
40 
0.67 
0.44 
0.30 
Effect of Outliers 
8 
13.79 
15.42 
5.22 
8.27 
9.25 
3.24 
4.14 
4.63 
2.02 
2.07 
2.09 
1 .oo 
1.03 
1.08 
0.39 
n 
16 
23.03 
19.69 
4.12 
13.82 
10.75 
2.28 
6.9 1 
6.34 
1.59 
3.45 
3.14 
0.64 
1.73 
1.55 
0.30 
32 
41.34 
22.86 
4.26 
24.80 
17.75 
2.69 
12.40 
9.58 
1.61 
6.20 
5.05 
0.53 
3.10 
2.96 
0.30 
64 
77.87 
30.29 
3.40 
46.72 
24.47 
1.53 
23.36 
16.01 
1.24 
1 1.68 
9.56 
0.45 
5.84 
5.21 
0.36 
It is widely believed in the pattern recognition field that classifier perfor- 
mance can be improved by removing outliers, points far from a class’s inferred 
mean which seem to distort the distribution. The approach used in this section, 
namely to analyze the difference between the R and L parameters, can be 

236 
5 
4 
3 
2 
1 
OO 
Introduction to Statistical Pattern Recognition 
- 
1 
2 
3 
4 
5 
standard 
deviation x 10 -2 
X 
/ 
/ 
/ 
/ 
/. 
/ 
/ 
i 
1/N x lo-* 
/ 
r 
I 
I 
I 
I 
I
,
 

5 Parameter Estimation 
237 
where 
- (X-M I )%;I 
(X-M 1 )  - (Y-M 
)%;I 
(Y-M , )] 
(5.159) 
Likewise, when Y comes from 02, 
n 
(5.160) 
where g2 is the same as (5.159) except that M 2  and X 2  are used instead of M ,  
and E l .  
When this modified classifier is used on an independent set of test sam- 
1 
h y ( X )  = h ( X )  - y g z ( X , Y )  
for  YEW^ , 
ples, the resulting error is, using (5.59), 
(5.161) 
where + and i = 1 are used for YEW, and - and i = 2 are for YEW?. The 
approximation in the last line involves replacing e/oh(X) by elo"(x). Unlike the 
case of the R error, (5.161) keeps F ( X )  in its integrand. This makes the 
integral in (5.161) particularly easy to handle. If the quadratic classifier is the 
Bayes classifier, the integration with respect to w results in 

238 
Introduction to Statistical Pattern Recognition 
(5.162) 
1 ?I 
= It 
-g, (X, Y ) F ( X )  dX = 0 . 
h (X)=O 
That is, as long as i ( X )  = 0 at h ( X )  = 0, the effect of an individual sample is 
negligible. Even if the quadratic classifier is not optimal, A E ~  
is dominated by 
a 1/7L term. Thus, as one would expect, as the number of design samples 
becomes larger, the effect of an individual sample diminishes. 
In order to confirm the above results, the following experiment was con- 
ducted. 
Experiment 9: Effect of removing one sample 
Data: I-I, I-41, I-A (Normal, n = 8) 
Classifier: Quadratic classifier of (5.54) 
Design samples: 7il = 712 = 24, 40, 80, 160, 320 
Test: Theoretical using (3.119)-(3.128) 
No. of trials: T = 10 
Results: Table 5-1 1 [6] 
Table 5-1 1 shows that, even if the squared distance of Y  EO^) from M I ,  d 2 ,  is 
much larger than n, the effect is still negligible. The expected value of d2 is n 
when X is distributed normally. 
5.4 Bootstrap Methods 
Bootstrap Errors 
Bootstrap method: So far, we have studied how to bound the Bayes 
error based on available sample sets. That is, we draw z sample sets 
S I , .  . . , S T ,  from the true distributions, P, 
as seen in Fig. 5-5, where each 
sample set contains N I ol-samples and N 2  m2-samples. For each S,, we can 
apply the L and R methods to obtain E ~ ,  
and E~,. The averages of these EL,’s 
and E ~ , ’ S  over r sets approximate the upper and lower bounds of the Bayes 
error, E ( E~ ] and E ( E~ }. The standard deviations of r EL,’S and E ~ , ’ s  indicate 
how E~ and E~ vary. However, in many cases in practice, only one sample set 
A 
I 
,. 
A 
,. 
L 

5 Parameter Estimation 
0.769 
0.279 
0.027 
0.014 
0.009 
1.45 1 
0.619 
0.09 1 
0.014 
0.012 
0.664 
0.110 
0.008 
0.001 
0.001 
239 
0.762 
0.274 
0.018 
0.013 
0.01 1 
1.356 
0.658 
0.083 
0.013 
0.015 
0.673 
0.103 
0.003 
0.00 1 
0.00 1 
TABLE 5-11 
EFFECT OF REMOVING ONE SAMPLE 
Case 
1-1 
(E = 10%) 
24 
40 
80 
160 
320 
1-41 
(E = 9%) 
I-A 
(E = 1.9%) 
Error without 
removing Y (%) 
20.18 
15.61 
12.04 
11.04 
10.53 
t 
23.53 
16.19 
11.79 
10.32 
9.52 
5.58 
3.70 
2.54 
2.25 
2.08 
Bias between errors with 
and without removing Y (%) 
d 2  = n  
0.689 
0.21 1 
0.035 
0.010 
0.006 
1.213 
0.423 
0.060 
0.005 
0.006 
0.555 
0.088 
0.007 
0.000 
0.000 
d 2  = 2 n  I d 2  =3n 
is available, say S I ,  from which we wish to learn as much about the statistical 
properties as possible that these S, ’s may have. 
One possible way of doing this is to generate t artificial sample sets 
S;, , . . . ,S; from S 1 ,  and study the statistical properties of these S;,’s 
(i = 1, . . . ,t), hoping that these statistical properties are close to the ones of the 
S,’s (i = 1, . . . ,T). This technique is called the bootstrap method, and the 
artificial samples are called the bootstrap samples [ 151. The bootstrap method 
may be applied to many estimation problems. However, in this book, we dis- 
cuss this technique only for the estimation of classification errors. 

240 
Introduction to Statistical Pattern Recognition 
Fig. 5-5 Bootstrap sample generation. 
There could be many possible ways to generate artificial samples. One 
is to generate samples normally around the existing samples. This leads to a 
nonparametric technique called the Parzen approach with the normal kernel 
function, which will be discussed extensively in Chapters 6 and 7. In this sec- 
tion, we discuss the case hi which samples are generated randomly at the 
points where the existing samples in S I are located. 
Bootstrap errors: Let us assume that S I consists of N I o1 -samples, 
X\'), . . . ,X#!, 
and N ,  m2-samples, XI2', . . . ,X$j. Then, we may express the 

5 Parameter Estimation 
24 1 
density function of mi by collection of Ni impulses which are located at the 
existing sample points, X ? ) ,  . . . ,X$l. That is, 
(5.163) 
where * indicates something related to the bootstrap operation. In the bootstrap 
operation, the density function of (5.163) is treated as the true density from 
which samples are generated. Therefore, in this section, X y )  is considered a 
given fixed vector and is not random as it was in the previous sections. 
When samples are drawn from p : ( X )  randomly, we select only the exist- 
ing sample points with random frequencies. Thus, the N, samples drawn from 
p r ( X )  form a random density function 
Within each class, the wy)’s are identically distributed under the condition 
E::, 
wj‘) = 1. Their statistical properties are known as 
1 
,’ 
Ni 
E{w‘”] = - 
, 
(5.165) 
(5.166) 
E{AwY)Aw(”] = 0 
fori # k , 
(5.167) 
where Awj‘) = w$’-l/N,. 
The H error in the bootstrap procedure, E~,, is obtained by generating 
samples, designing a classifier based on p, ( X ) ,  and testing p ; ( X )  of (5.163). 
On the other hand, the R error, E ~ ,  
is computed by testing p, (X). The bias 
between them can be expressed by 
..x 
A X  
..- 
A *  

242 
Introduction to Statistical Pattern Recognition 
where 
(5.168) 
(5.169) 
,.* 
Quadratic classifier: When a quadratic classifier is used, h ( X )  in 
h*(X) = f ( X , M , , Z , )  - f ( X , M 2 A ) ,  
(5.170) 
wheref(.,.,.) is defined in (5.143). The bootstrap parameters, M i ,  and Z, , are 
(5.169) becomes 
,,* 
,,* 
* *  r *  
,.* 
,.= 
(5.171) 
(5.172) 
,.* 
A ’  
Note that h, = ( X ~ ~ i X ~ ) ) / N ,  
instead of M, is used to compute E , .  In the con- 
ventional sample covariance matrix, the sample mean is used, because the true 
expected vector is not available. However, the true expected vector, M,, is 
available in the bootstrap operation, and the use of M, instead of M, simplifies 
the discussion significantly. Their expectations are 
A 
, . x  

5 Parameter Estimation 
243 
E* 
EX 
(5.174) 
where Ex indicates the expectation with respect to the w's. 
A
X
 ,* 
f(X, M, ,X, 
) can be expanded around f(X,k,,i,) by the Taylor series as 
A *  
A 
where AM; = My - k;, A$ = Z; - X i .  and df*/&; is defined in (5.141). 
Since h ( ~ )  
= f ( ~ , M ~ , i , )  
- f ( ~ , i i * , i * ) ,  
Ah(X) = h*(X) - h ( X )  
(5.176) 
The partial derivatives of (5.176) can be obtained by (5.144) and (5.145). 
Bootstrap Expectation 
Using the approximation of (5.60), (5.169) can be approximated as 
(5.177) 
The third term is a third order variation with the combination of AwY) and 
Ah , and can be ignored. Thus, our analysis will focus on the first and second 
terms only. With this in mind, substituting (5.171), (5.172), and (5.176) into 
(5.177) produces 
- 2  

244 
Introduction to Statistical Pattern Recognition 
af* 
(5.178) 
- 
Awj') A ~ i ~ ) ( X i ~ ) - f i ~ ) ~  
,-(Xl2)-h2)]do 
. 
N ?  
!,=I 
2x2 
Using the expectations in (5.165)-(5.167), E, [ yy)} becomes 
where an approximation of N-1 
N is used to obtain the second line from the 
first, Now, substituting the partial derivatives of (5.144) and (5.145) into 
(5.179), 

5 Parameter Estimation 
245 
(5.180) 
-2 
A 
--I 
A 
where d, ( X )  = (X-M,)TCj (X-M,). Thus, the expectation of the bootstrap bias 
for a quadratic classifier given a sample set S I  = {X\'), . . . ,X#;,X\*), . . . ,X$i } 
becomes 
where 
(5.181) 
(5.182) 
-2 
Note that (5.151) and (5.182) are very similar. The differences are d, vs. df 
and i 
vs. hL. The discriminant function h of (5.182) is designed with h, and 
C,, the sample mean and sample covariance of the sample set S I .  The test 
samples X y )  are the members of the same set, S I .  Therefore, h is the same as 
the R discriminant function hR, while hL of (5.151) is the L discriminant func- 
tion. For a given S I ,  hR is a fixed function. However, if a random set, S, 
replaces the fixed set, S I ,  the discriminant function becomes a random vari- 
able, h,. 
As shown in (5.148) and (5.149), the difference between h, and hR 
is proportional to l/N. Thus, the difference between dwhL 
and doh' is propor- 
tional to liN. Also, it can be shown that the difference between d, and d: is 
proportional to 1/N. Thus, ignoring terms with 1/N, E,, 
of (5.150) and 
E* {E,, I S }  of (5.18 1) (note that S is now a random set) become equal and have 
the same statistical properties. Practically, this means that estimating the 
expected error rate using the L and bootstrap methods should yield the same 
results. 
A 
A 
- 2  
..* 
These conclusions have been confirmed experimentally. 

246 
Introduction to Statistical Pattern Recognition 
Experiment 10: Bootstrap errors 
Data: 1-1, 1-41, I-A (Normal, n = 8) 
Classifier: Quadratic classifier of (5.54) 
No. of bootstrap sets: S = 100 
Sample size: N I = N 2  = 24, 40, 80, 160, 320 
No. of trials: z = 10 
Results: Table 5-12 [6] 
A 
#. 
A
A
 
In Table 5-12(a), the means and standard deviations of &L and &h (= E ~ - & ~ )  
of 
the 10 trials are presented for the conventional L and R methods. Table 5- 
12(b) shows the corresponding terms in the bootstrap method: ER + E* { &h IS ] 
and Ex { &h IS ] respectively. E* { &h IS, } is obtained by taking the average of 
&h, I , .  . . ,E,,, 
[see Fig. 5-51. This is the bootstrap estimation of the bias 
between the H and R errors given S,, and varies with S,. The random variable 
E,{;; I S ]  with a random S should be compared with ;,, of Table 5-12(a). If 
this bias estimate is close to the bias between E ~ ,  
and E ~ ,  
of SL,$ the bootstrap 
bias could be added to cR, to estimate 
. The term eR + E, { &h I S ] of Table 
5-12(b) shows this estimation of E ~ ,  
and should be cornpared with 
of Table 
5-12(a). The table shows that & of (a) and iR + Ex { 2, IS ] of (b) are close in 
both mean and standard deviation for a reasonable size of N .  The biases, &h of 
(a) and 
{ &,, 
I S ]  of (b), are also close in the mean, but the standard deviations 
of E* { &h I S 1 are consistently smaller than the ones of &h. 
A 
A *  
A X  
A *  
..* 
,* 
,. 
,. 
,. 
A 
A 
A 
A 
A 
A -  
A 
A 
Bootstrap Variance 
The variance with respect to the bootstrap samples can be evaluated in a 
fashion similar to (5.154) 
(5.183) 

5 Parameter Estimation 
Standard 
deviation (%) 
3.14 
3.88 
1.29 
1.09 
0.37 
3.86 
2.97 
1.56 
0.68 
0.40 
3.02 
1.21 
0.94 
0.36 
0.14 
247 
(%) 
0.1 1 
0.07 
0.04 
0.03 
0.0 1 
0.12 
0.06 
0.08 
0.01 
0.01 
0.0 1 
0.02 
0.01 
0.00 
0.00 
Data 
1-1 
1-41 
I-A 
TABLE 5-12 
COMPARISON BETWEEN CONVENTIONAL AND 
BOOTSTRAP METHODS 
N 
- 
24 
40 
80 
160 
320 
24 
40 
80 
160 
3 20 
24 
40 
80 
160 
320 
- 
Mean 
(%) 
17.08 
13.38 
11.19 
11.28 
10.67 
18.33 
13.75 
11.19 
9.88 
10.09 
5 .oo 
3.63 
2.3 1 
2.34 
2.17 
.. 
EL 
Standard 
deviation (%) 
4.89 
6.04 
2.47 
2.35 
0.80 
4.79 
3.23 
2.72 
1.58 
0.83 
3.43 
1.99 
1.10 
0.90 
0.48 
Mean 
(%) 
13.54 
7.63 
4.06 
2.16 
0.89 
14.79 
8.88 
4.00 
2.28 
0.98 
4.38 
1.75 
0.88 
0.4 1 
0.17 
(a) Conventional L and R error estimates. 
Because the samples from each class are bootstrapped independently, 
cOv*{yy),yp} = 0. 
Using a property of the inverse Fourier transform, 
(5.184) 
The variance of y)’’ is 

248 
4.17 
4.44 
2.50 
1.94 
0.9 1 
4.35 
2.27 
3.12 
1.23 
0.77 
1.69 
1.95 
1.08 
0.80 
0.53 
Introduction to Statistical Pattern Recognition 
9.23 
5.92 
3.55 
2.05 
1 .oo 
11.54 
7.22 
3.63 
1.96 
1.03 
3.52 
I .86 
0.82 
0.42 
0.18 
- 
Data 
1-1 
1-41 
- 
I-A 
TABLE 5-12 
COMPARISON BETWEEN CONVENTIONAL AND 
BOOTSTRAP METHODS 
- 
N 
24 
40 
80 
160 
320 
24 
40 
80 
160 
320 
24 
40 
80 
160 
3 20 
- 
Mean 
(%I 
12.77 
1 1.68 
10.67 
11.17 
10.78 
15.08 
12.10 
10.82 
9.56 
10.14 
4.14 
3.74 
2.26 
2.35 
2.18 
ZzJ-5 
Standard 
Standard 
deviation (%) 
1.38 
1.90 
0.56 
0.44 
0.1 1 
1.26 
0.92 
0.54 
0.33 
0. I5 
0.84 
0.67 
0.24 
0.17 
0.07 
(b) Bootstrap error estimates. 
A *  
Var, ( E,, IS) 
Mean 
0.18 
0.08 
0.04 
0.02 
0.0 1 
0.2 1 
0.12 
0.04 
0.02 
0.0 1 
0.10 
0.03 
0.0 1 
0.0 1 
0.00 
(5.185) 
where E:{yY)] is proportional to 1/NP from (5.180) and therefore can be 

5 Parameter Estimation 
249 
ignored. Cov-..{yy),yf) 
J may be approximated by using the first term only of 
(5.178). Again, using (5.184), 
COV- ( 7:),7f) } = Ex ( 7y)yy) ] - E* ( y:' ]E. { yf ) } 
=-sisn(h(X:"))sj~n(~(X1')))E 
1 
{ Awy)Awf) }-E.. { y:' 
]E* (yf' ] 
4 
(5.186) 
where E(Aw:"Aw~)] =-I/NP for j + k  by (5.166), and E*(yy)]E*{yf)} 
is 
proportional to I/Nf by (5.180) and therefore can be ignored. 
Thus, substituting (5.185) and (5.186) into (5.183) and using 
covx (y:",yp J = 0, 
r 
1 
[ V i  
i=I 
(5.187) 
Note that Csign(L(X,Y'))/Ni = (-1)' 
[(# of correctly classified mi-samples by 
.. 01 
,. 01 
h >< O)/N, - (# of misclassified mi-samples by h >< O)/Ni] = (-l)'[(1-qR) 
-eiR] ]= (-l)1(1-2~lR). Since h is the R discriminant function for the original 
sample set, the resulting error is the R error. The last column of Table 5-12(a) 
shows the variance of E ~ .  
which is computed by the 10 trials of the conven- 
tional R method. This should be compared with the last column of Table 5- 
12(b), Var,($IS). 
Both are close as (5.187) predicts. Var,{&,,ISi] is the 
variance of 
. . . 
[see Fig. 5-51. The last column is the average of 
Var( E,, I Si } over i. 
W? 
W? 
,. 
,. 
r *  
..* 
A X  
Note that (5.187) is the variance expression of the R error estimate. 

250 
Computer Projects 
Introduction to Statistical Pattern Recognition 
1. 
2. 
3. 
4. 
5. 
6. 
Repeat Experiment 1. 
Repeat Experiment 4. Also, estimate the asymptotic error for each n by 
using the line fitting procedure. 
Repeat Experiment 5. Also, estimate the asymptotic error for each n by 
using the line fitting procedure. 
Repeat Experiment 6. 
Repeat Experiment 9. 
Repeat Experiment 10. 
Problems 
1. 
2. 
3. 
4. 
The Fisher criterion, f = (m2-m l)2/(o:+o:), measures the class separa- 
bility between two one-dimensional distributions. Compute the bias and 
variance of f when these parameters are estimated by using N samples 
from N-,(O, 1) for o1 
and N samples from N,( 1.4) for 02. 
Let f (m) 
be a function of m, where m is the expected value of a one- 
dimensional normal distribution and is estimated by the sample mean m 
using N samples. Expand f (m) around f (m) up to the fourth order 
term, and confirm that E { 0'3' I = 0 and E { 0'4' ) - l I N 2 .  
,. 
.. 
A 
A 
,. 
Compute the bias and variance of p (not pI and p2 separately) for nor- 
mal distributions. 
0 1  
0 2  
In order for a linear classifier h (X) = VTX + v0 >< 0 to be optimum by 
minimizing the error of (5.37), prove that V and \io must satisfy 

5 Parameter Estimation 
25 1 
5. 
6. 
7. 
8. 
9. 
Derive v, of (5.92) for Data 1-1. 
A quadratic classifier for zero-mean distributions is given as 
01 
0 2  
h ( X )  =XT[C;I - ZC,']X >< t . 
In the design phase, C1 and C2 are estimated by 
* 
I
N
 
T 
I
N
 
T 
N 
; = I  
N 
; = I  
CI = - X i X ;  
and 
C2 = - Y,Y, , 
where XI and Y, are samples from wl and w2 respectively. For testing 
X k ,  Xk is excluded from design to get Elk. Prove 
n 
Modify the Procedure 111 of Section 3.2 to the leave-one-out method. 
Assuming M I  = 0, M 2  = M = [m 0. . . O]', 
and X I  = C 2  = I ,  compute 
the integral of (5.153) along the Bayes boundary (xI = -/2) 
to 
obtain PI. Use h ( X ) = = x ,  
-MTM/2, d':(X)=(C,:'=lx-f)2, and 
p (x) = (2~)-"'~exp[- 
-E ,:'= I x,' 1. 
1 
2 
N boxes have equal probability of getting a sample. When N samples 
are thrown, the ith box receives k, samples. Defining w, = k , / N ,  prove 
that 
( 1 )  
E { w , )  = 1/N, 
( 2 )  
E ( ( w ,  - l / N ) ( w ,  - l / N ) }  = 6 , / N 2  - l / N 3 .  

252 
Introduction to Statistical Pattern Recognition 
10. 
Let the bootstrap sample covariance matrix be 
. . N  
* *  
r *  
Z = Ewj(Xj - M )(Xj - M )T 
j=l 
instead of (5.172). Compute the expected value of i. 
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
G. J. McLachlan, Error rate estimation in discriminant analysis: recent 
advances, in “Advances in Multivariate Statistical Analysis,” ed. A. K. 
Gupta, D. Reidel Publishing Co., Dordrecht, 1987. 
D. J. Hand, Recent advances in error rate estimation, Patrern Recognition 
Letters, 4, pp. 335-346, 1986. 
A. K. Jain and B. Chandrasekaran, Dimensionality and sample size con- 
siderations in pattern recognition practice, in “Handbook of Statistics 
2,” ed. P. R. Krishnaiah and L. N. Kanal, North Holland, Amsterdam, 
1982. 
K. Fukunaga and R. R. Hayes, Effects of sample size in classifier design, 
Trans. IEEE Pattern Anal. and Machine Intell., PAMI-1 1, pp. 873-885, 
1989. 
G. F. Hughes, On the mean accuracy of statistical pattern recognizers, 
Trans. IEEE Inform. Theory, IT-14, pp. 55-63, 1968. 
K. Fukunaga and R. R. Hayes, Estimation of classifier performance, 
Trans. IEEE Pattern Anal. and Machine Intell., PAMI-11, pp. 1087- 
1101, 1989. 
S. Raudys and V. Pikelis, On dimensionality, sample size, classification 
error, and complexity of classification algorithm in pattern recognition, 
Trans. IEEE Pattern Anal. Machine Intell., PAMI-2, pp. 242-252, 1980. 
D. H. Foley, Considerations of sample and feature size, Trans. IEEE 
Inform. Theory, IT-18, pp. 618-626, 1972. 
S. John, Errors in discrimination, Ann. Math. Stat., 32, pp. 1125-1144, 
1961. 
C. P. Han, Distribution of discriminant function in circular models, Inst. 
Srat. Math. Ann., 22, pp. 1 17-125, 1970. 

5 Parameter Estimation 
253 
11. 
G. J. McLachlan, Some expected values for the error rates of the sample 
quadratic discriminant function, Ausfralian Journal of Statistics, 17(3), 
M. Hills, Allocation rules and their error rate, J. Royal Stat. Soc. Ser., 
P. A. Lachenbruch and R. M. Mickey, Estimation of error rates in 
discriminant analysis, Technometrics, 10, pp. 1 - 1 1, 1968. 
14. 
K. Fukunaga and D. L. Kessell, Estimation of classification errors, 
Trans. IEEE Computers, C-20, pp. 152 1 - 1527, 197 1. 
15. 
B. Efron, Bootstrap methods: Another look at the jackknife, Ann. Stat., 
pp. 161-165, 1975. 
12. 
B28, pp. 1-31, 1966. 
13. 
7, pp. 1-26, 1979. 

Chapter 6 
NONPARAMETRIC DENSITY ESTIMATION 
So far we have been discussing the estimation of parameters. Thus, if 
we can assume we have a density function that can be characterized by a set of 
parameters, we can design a classifier using estimates of the parameters. 
Unfortunately, we often cannot assume a parametric form for the density func- 
tion, and in order to apply the likelihood ratio test we somehow have to esti- 
mate the density functions using an unstructured approach. This type of 
approach is called nonparametric estimation, while the former is called 
parametric estimation. Since, in nonparametric approaches, the density func- 
tion is estimated locally by a small number of neighboring samples, the esti- 
mate is far less reliable with larger bias and variance than the parametric coun- 
terpart. 
There are two kinds of nonparametric estimation techniques available: 
one is called the Par-zen density estimate and the other is the k-nearest neigh- 
bor- densiry estimate. They are fundamentally very similar, but exhibit some 
different statistical properties. Both are discussed in this chapter. 
It is extremely difficult to obtain an accurate density estimate non- 
parametrically, particularly in high-dimensional spaces. However, our goal 
here is not to get an accurate estimate. Our goal is, by using these estimates, 
to design a classifier and evaluate its performance. For this reason, the accu- 
racy of the estimate is not necessarily a crucial issue. Classification and 
performance evaluation will be discussed in Chapter 7. The intention of this 
254 

6 Nonparametric Density Estimation 
255 
chapter is to make the reader familiar with the fundamental mathematical pro- 
perties related to nonparametric density estimation in preparation for the 
material presented in Chapter 7. 
6.1 Parzen Density Estimate 
Parzen Density Estimate 
In order to estimate the value of a density function at a point X, we may 
set up a small local region around X ,  L (X). Then, the probability coverage (or 
probability mass) of L(X) may be approximated by p(X)v where v is the 
volume of L(X). This probability may be estimated by drawing a large 
number of samples, N ,  from p(X), counting the number of samples, k, falling 
in L (X), and computing k / N .  Equating these two probabilities, we may obtain 
an estimate of the density function as 
n 
n 
k(X) 
or 
p(X)= - 
. 
k(X) 
p(X)v = - 
N 
Nv 
Note that, with a fixed v, k is a random variable and is dependent on X. A 
fixed v does not imply the same v throughout the entire space, and vv could still 
vary with X. However, v is a preset value and is not a random variable. 
Kernel expression: The estimate of (6.1) has another interpretation. 
Suppose that 3 samples, X3, X,, and X,, are found in L(X) as shown in Fig. 
6-1. With I' and N given, i ( X )  becomes 3/Nv. On the other hand, if we set up 
a uniform kernel function, IC(.), with volume v and height l/v around all exist- 
ing samples, the average of the values of these kernel functions at X is also 
3/Nv. That is, [ 1-41 
n 
(6.2) 
As seen in Fig. 6-1, only the kernel functions around the 3 samples, 
X3, X4, and X,, contribute to the summation of (6.2). 
Once (6.2) is adopted, the shape of the kernel function could be selected 
more freely, under the condition 
K(X) dX = 1 .  For one-dimensional cases, 
we may seek optimality and select a complex shape. However, in a high- 
dimensional space, because of its complexity, the practical selection of the ker- 
l
N
 
N 
; = I  
p(x) = - K(X - x;) . 

256 
Introduction to Statistical Pattern Recognition 
Fig. 6-1 Parzen kernel density estimate. 
ne1 function is very limited to either a normal or uniform kernel. In this book, 
we will use the following kernel which includes both normal and uniform ker- 
nels as special cases: 
where r(.) is the gamma function, and m is a parameter determining the shape 
of the kernel. It may be verified that, for any value of m, the covariance matrix 
of the kernel density (6.3) is r2A. The parameter rn determines the rate at 
which the kernel function drops off. For m = 1, (6.3) reduces to a simple nor- 
mal kernel. As m becomes large, (6.3) approaches a uniform (hyperelliptical) 
kernel, always with a smooth roll-off. The matrix A determines the shape of 
the hyperellipsoid, and I' controls the size or volume of the kernel. Other 
coefficients are selected to satisfy the two conditions mentioned previously: 

6 Nonparametric Density Estimation 
257 
j ~ ( x ) d X  = 1 and Z, = r’A where C, is the covariance matrix of K(X). 
Convolution expression: Equation (6.2) can be rewritten in convolution 
form as 
where p,T is an impulsive density function with impulses at the locations of 
existing N samples. 
That is, the estimated density p(X) is obtained by feeding p,(X) through a 
linear (noncausal) filter whose impulse response is given by K(X). Therefore, 
p(X) is a smoothed version of p,(X). 
n 
Moments of p(X): The first and second order moments of (6.4) can be 
n 
easily computed. First, let us compute the expected value of p,(X) as 
(6.6) 
That is, p,(X) is an unbiased estimate of p ( X ) .  Then, the expected value of 
p(X) of (6.4) may be computed as 
I
N
 
l
N
 
N,=I 
N ;=, 
E(P,(X)J = -Zj6(X-Z)P(Z)dZ 
= -ZP(X) = p ( X ) .  
Also, 

258 
Introduction to Statistical Pattern Recognition 
Therefore, the variance of p(X) is 
Approximations of moments: In order to approximate the moments of 
p ( X ) ,  let us expand p ( Y )  around X by a Taylor series up to the second order 
terms as 
p ( ~ )  
E ~ ( x )  
+ V~'(X)(Y-X) + -~~{v~~(x)(Y-x)(Y-x)'] 
. 
Then, p ( X ) * K ( X )  may be approximated by 
A 
(6.10) 
1 
2 
p ( X ) * K ( X )  = jp(Y)K(Y-x)dY 
gp (X)jK(Y-X)dY 
(6.1 1) 
+ -tr{ v2p ( X ) j ( Y  -X)(Y -X)'K(Y -X)dY ) , 
where the first order term disappears because K(.) is a symmetric function. 
Since ~ K ( Y - x ) ~ Y  = 1 and ~(Y-x)(Y-x)'K(Y-x)~Y 
= r . 2 ~  for K(.) of (6.3), 
(6.1 1) can be expressed by 
1 
2 
where 
(6.12) 
(6.13) 
Similarly, 
p ( X ) * d ( X )  Ep(X)jt?(Y-X)dY 
(6.14) 
Although K(.) is a density function, K*(.) is not. Therefore, ld(Y)dY has a 
value not equal to 1. Let 
+ - ~ ~ ( v ~ ~ ( x ) ~ ( Y - x ) ( Y - x ) ~ ( Y - x ) ~ Y  
1 
1 . 
2 

6 Nonparametric Density Estimation 
259 
w =jl?(Y)dY. 
Then, $(.)/w becomes a density function. Therefore, (6.14) becomes 
where 
and r-'B is the covariance matrix of ?(X)/w. 
(6.15) 
(6.16) 
(6.17) 
Substituting (6.12) and (6.16) into (6.7) and (6.9), the moments of p(X) 
are approximated by 
1 
E ( & X )  } 'I- p (X)[1 + y a ( X ) r 2 ]  2nd order approximation 
P (X) 
1st order approximation , 
(6.18) 
2nd order approximation 
(6.19) 
1 
N 
E -[wp(X) - p2(X)] 1st order approximation . 
Note that the variance is proportional to 1/N and thus can be reduced by 
increasing the sample size. On the other hand, the bias is independent of N, 
and is determined by V2p (X), A, and r 2 .  
Normal kernel: When the kernel function is normal with zero expected 
vector and covariance matrix r2A, Nx(0,r2A), $(X) becomes normal as 
cNx(0,r2A 12) where c = 2-"'2(2~)-"/2 
IA l-"2r-". Therefore, 

260 
Introduction to Statistical Pattern Recognition 
(6.20) 
Uniform kernel: For a uniform kernel with the covariance matrix r2A, 
l l v  
inside L ( X )  
0 
outside L ( X )  . 
K(Y) = 
(6.22) 
where 
L ( X )  = ( Y :  ~ ( Y , x )  
< r4n+2 1 , 
(6.23) 
d2(Y,X) = (Y-X)TA-'(Y-X), 
(6.24) 
and 
(6.25) 
Then, K ~ ( X )  is also uniform in L ( X )  with the height llv2. Therefore, 
w = [($Y)dY 
= - 
1 
V 
(6.26) 
Also, since the covariance matrix of K(X) is r2A, the covariance matrix of 
d ( X ) l w  is also r2A as 
1 
[(x,(Y -X)(Y -X)'-dY 
= r2A 
v 
(6.27) 
Therefore, for the uniform distribution of (6.22), 
B = A  and p ( X )  = a ( X )  . 
(6.28) 
Note that w's for both normal and uniform kernels are proportional to 
I' -n or v-' . In particular, w = l/v for the uniform kernel from (6.26). Using 
this relation, the first order approximation of the variance can be simplified 
further as follows: 

6 Nonparametric Density Estimation 
26 1 
r 
1 
(6.29) 
where p 'I- klNv and N >>k are used. This suggests that the second term of 
(6.19) is much smaller than the first term, and can be ignored. Also, (6.29) 
indicates that k+w is required along with N+- 
for the Parzen density esti- 
mate to be consistent. These are the known conditions for asymptotic unbias- 
ness and consistency [2]. 
Convolution of normal distributions: If p (X) is assumed to be normal 
and a normal kernel is selected for K(X), (6.7) and (6.9) become trivial to 
evaluate. When two normal densities Nx(O,A) and Nx(O,B) are convolved, the 
result is also a normal density of Nx(O,K), where 
In particular, if A = C and B = r2C 
K = (1 + r2)X 
(6.30) 
(6.31) 
Optimal Kernel Size 
Mean-square error criterion: In order to apply the density estimate of 
(6.1) (or (6.2) with the kernel function of (6.3)), we need to select a value for r 
[5-111. The optimal value of r may be determined by minimizing the mean- 
square error between p(X) and p (X) 
with respect to I-. 
.. 
MSE(P(X)J =El[P(X)-p(X)l21 . 
(6.32) 
This criterion is a function of X, and thus the optimal I' also must be a function 
of X. In order to make the optimal r independent of X, we may use the 
integral mean-square error 

262 
Introduction to Statistical Pattern Recognition 
IMSE = JMSE { P(X) IdX . 
(6.33) 
Another 
possible 
criterion to 
obtain 
the 
globally 
optimal 
r 
is 
Ex{ 
MSE { &X)) } = jMSE{ f i ( X ) ) p  (X)dX. The optimization of this criterion 
can be carried out in a similar way as the IMSE, and produces a similar but a 
slightly smaller I' than the IMSE. This criterion places more weight on the 
MSE in high density areas, where the locally optimal r's tend to be smaller. 
(6.19), M S E { ; ( X ) ]  may be expressed as 
n 
Since we have computed the bias and variance of p(X) in (6.18) and 
M S E { ~ % X ) J  = [E(P(x)I - p ( x ) 1 2  + v a r { i ( x ) ~  . 
(6.34) 
In this section, only the uniform kernel function is considered. This is 
because the Parzen density estimate with the uniform kernel is more directly 
related to the k nearest neighbor density estimate, and the comparison of these 
two is easier. Since both normal and uniform kernels share similar first and 
second order moments of fi(X), the normal kernel function may be treated in 
the same way as the uniform kernel, and both produce similar results. 
When the first order approximation is used, &X) is unbiased as in (6.18), 
and therefore MSE = Var = p I N v  - p21N as in (6.29). This criterion value is 
minimized by selecting v = m for a given N and p. That is, as long as the den- 
sity function is linear in L ( X ) ,  the variance dominates the MSE of the density 
estimate, and can be reduced by selecting larger v. However, as soon as L ( X )  
is expanded and picks up the second order term of (6.10), the bias starts to 
appear in the MSE and it grows with r2 (or v21n) as in (6.18). Therefore, in 
minimizing the MSE, we select the best compromise between the bias and the 
variance. In order to include the effect of the bias in our discussion, we have 
no choice but to seiect the second order approximation in (6.18). Otherwise, 
the MSE criterion does not depend on the bias term. On the other hand, the 
variance term is included in the MSE no matter which approximation of (6.19) 
is used, the first or second order. If the second order approximation is used, 
the accuracy of the variance may be improved. However, the degree of 
improvement may not warrant the extra complexity which the second order 
approximation brings in. Furthermore, it should be remembered that the 
optimal I' will be a function of p(X). Since we never know the true value of 

6 Nonparametric Density Estimation 
263 
p ( X )  accurately, it is futile to seek the more accurate but more complex 
expression for the variance. After all, what we can hope for is to get a rough 
estimate of I' to be used. 
Therefore, using the second order approximation of (6.18) and the first 
order approximation of (6.29) for simplicity, 
(6.35) 
Note that the first and second terms correspond to the variance and squared 
bias of p ( X ) ,  respectively. 
Minimization of MSE: Solving 3MSE/& = 0 [5], the resulting optimal 
I-, I'*, is 
I 
(6.36) 
N 
l1+4 
I" 
-7 
n +2 
2 
n u - )  
= [ 7 ~ " ~ ( n + 2 ) " ' ~ p  
IA I '12a2 
where 1' = CI'" and 
(6.37) 
The resulting mean-square error is obtained by substituting (6.36) into (6.35). 
When the integral mean-square error of (6.33) is computed, 1' and I' are 
supposed to be constant, being independent of X .  Therefore, from (6.35) 
1 
1 
Nv 
4 
IMSE = -jp 
(X)dX + - r 4 j a 2 ( X ) p 2 ( X ) d X  

264 
Introduction to Statistical Pattern Recognition 
1
1
 
Nv 
4 
= - 
+ -r4ja2(X)p2(X)dX . 
Again, by solving alMSElar = 0 [ 5 ] ,  
(6.39) 
The resulting criterion value is obtained by substituting (6.40) into (6.39), 
Optimal Metric 
Another important question in obtaining a good density estimate is how 
to select the metric, A of (6.3). The discussion of the optimal A is very com- 
plex unless the matrix is diagonalized. Therefore, we first need to study the 
effect of linear transformations on the various functions used in the previous 
sections. 
Linear transformation: Let @ be a non-singular matrix used to define a 
linear transformation. This transformation consists of a rotation and a scale 
change of the coordinate system. Under the transformation, a vector and 
metric become 
z=aTx, 
AZ = @'Ax@ . 
(6.42) 
(6.43) 
The distance of (6.24) is invariant since 
(Y-X)TAX'(Y-X) = (W-Z)'AZ'(W-Z), 
(6.44) 
where W = @'Y. 
The following is the list of effects of this transformation on 

6 Nonparametric Density Estimation 
265 
various functions. Proofs are not given but can be easily obtained by the 
reader. 
(1) p,(Z) = I O I-lpx(X) [Jacobian] , 
(6.45) 
(2) V?p,(Z) = I O I -‘@-I V*p,(X)@T-’ 
[from (6.10),(6.42), and (6.45)] , 
(6.46) 
(3) r ( Z )  = r-(X) 
[from (6.44)] , 
(6.47) 
(4) v(Z) = IO Iv(X) [from (6.25),(6.43), and (6.47)] , 
(6.48) 
(5) MSE {pz(Z) 1 = I O I-?MSE { px(X) 1 [from (6.32) and (6.45)] , 
(6.49) 
(6) IMSE, = I @ I -I IMSEx [from (6.33) and (6.42)] . 
(6.50) 
Note that both MSE and IMSE depend on @. The mean-square error is a coor- 
dinate dependent criterion. 
Minimization of IMSE: We will now use the above results to optimize 
the integral mean-square error criterion with respect to the matrix A. However, 
it is impossible to discuss the optimization for a general p ( X ) .  We need to 
limit the functional form of p ( X ) .  Here, we choose the following form for 
P ( X ) :  
p ( X )  = IB I~”’x((X-M)7B-I(X-M)) , 
(6.5 I )  
where x(.) does not involve B or M .  The p ( X )  of (6.51) covers a large family 
of density functions including the ones in (6.3). The expected vector, M ,  can 
be assumed to be zero, since all results should be independent of a mean shift. 
Now, we still have the freedom to choose the matrix A in some optimum 
manner. We will manipulate the two matrices B and A to simultaneously diag- 
onalize each, thus making the analysis easier. That is, 
Q7BQ = I 
and 
@ ‘ A 0  = A 
(6.52) 
and 
p ( Z )  = R (Z’Z) 
3 
(6.53) 
where A is a diagonal matrix with components h , ,  . . . ,A,!, 

266 
Introduction to Statistical Pattern Recognition 
In the transformed Z-space, IMSE$ of (6.41) becomes 
(6.54) 
where c1 and c2 are positive constants. IMSE; can be minimized by minimiz- 
ing t? 1. with respect to A. Since A is normalized by I A I I" such that 
the sca 
(6.55) 
: of the matrix has no effect. Thus, we will minimize t?(.) with 
respect to hi's with the constraint 
n 
i = l  
Ihl = Ilkj = 1 
Now, tr( .] can be evaluated as 
(6.56) 
(6.57) 
where 
Thus, the criterion to be optimized is 
where p is a Lagrange multiplier. Taking the derivative of J with respect to hk 
and setting the result equal to zero, 

6 Nonparametric Density Estimation 
267 
or 
n 
hi + hk(Chi) 
= - 
P 
(k = 1,. . . ,n) . 
i = l  
o2 
(6.60) 
(6.61) 
In order to satisfy (6.611, all hi's must be equal. Since IA I = 1, the solution 
of (6.61) must be 
A = I .  
(6.62) 
That is, in the transformed Z-space, the optimal matrix A, 
is I for B, = 1. 
Therefore, the optimal matrix A to use in the original X-space is identical to B 
of (6.51) [ 5 ] .  The neighborhoods should take the same ellipsoidal shape as the 
underlying distribution. For the normal distribution we see that the covariance 
matrix B = C is indeed optimal for A. 
It is important to notice that (6.62) is the locally optimal metric regard- 
less of the location, because IMSE* of (6.54) is minimized not after but before 
taking the integration. The same result can be obtained by minimizing MSE * 
of (6.38). 
Normal Case 
In order to get an idea of what kind of numbers should be used for I-, in 
this section let us compute the optimal I' for a normal distribution. The partial 
derivatives Vp (X) and V2p (X) for Nx(M, C) are 
Vp(X) = - p (x)C-'(x-M) , 
(6.63) 
v2p (X) = p (X)[C-' (X-M)(X-M)'C-' 
- C-'] . 
(6.64) 
For the simplest case in which M = 0 and I: = I, 
tr{V'p(X)) =~(x)(xTx - n) =P(x)(& 
- n) . 
(6.65) 
Note that the optimal A is also I in this case. It is easy to show that, if 
p (X) = Nx(OJ), then p2(X) = 2-"'2(2n)-"'2NX(0,1/2). Therefore, 
r = l  

268 
Introduction to Statistical Pattern Recognition 
1 
n(n+2) 
Jt?(V2p(X)}dX = 2"/2(27c)"/2 
4 
Accordingly, from (6.40) 
* 
I ' =  
TABLE 6-1 
OPTIMAL r OF THE UNIFORM KERNEL FUNCTION 
FOR NORMAL DISTRIBUTIONS 
(6.66) 
(6.67) 
Table 6-1 shows these r*'s for various values of n. Remember that the above 
discussion is for the uniform kernel, and that the radius of the hyperellipsoidal 
region is I.= 
according to (6.23). Therefore, I-*='s 
are also presented 
to demonstrate how large the local regions are. 
6.2 k Nearest Neighbor Density Estimate 
Statistical Properties 
RNN density estimate: In the Parzen density estimate of (6.1), we fix v 
and let k be a random variable. Another possibility is to fix k and let v be a 
random variable [12-161. That is, we extend the local region around X until 
the kth nearest neighbor is found. The local region, then, becomes random, 
L(X), and the volume becomes random, v(X). Also, both are now functions of 
X. This approach is called the k nearest neighbor (kNN) density estimate. The 
kNN approach can be interpreted as the Parzen approach with a uniform kernel 

6 Nonparametric Density Estimation 
269 
function whose size is adjusted automatically, depending on the location. That 
is, with k fixed throughout the entire space, 1' becomes larger in low density 
areas and smaller in high density areas. The kNN density estimate may be 
rewritten from (6.1) as [12-14] 
(6.68) 
The reason why ( k - I )  is used instead of k will be discussed later. 
Density of coverage: Although the density function of v is not available, 
the density function of the coverage (the probability mass in the local region), 
u, may be obtained as follows [ 171. 
Let L ( X )  and AL ( X )  be defined by 
L ( X )  = { Y :d(Y,X) I 
E }  and AL ( X )  = { Y :!<d(Y,X) I <+Ai} 
and 
(6.69) 
where d'(Y.X) = (Y-X)'A-'(Y-X). 
Also, let two events G and H be defined 
as 
G = [ ( k - I )  samples in L ( X ) )  , 
H = ( 1 sample in A L ( X ) }  . 
(6.71) 
(6.72) 
Then, the probability of the h-th NN in AL ( X )  is 
P r ( G  andH} = P r { G ) P r ( H I G ) ,  
(6.73) 
where 
(6.74) 
N 4. 
(6.75) 
Note that the coverage of A L ( X )  in the complementary domain of L ( X )  is 
Au/(l-u). 
Substituting 
(6.74) and 
(6.75) into 
(6.73) 
and 
using 
{ I-Au/(I-u)} + 1 as Air + 0, the probability of (6.73) becomes the product 
of Au and a function of u, pl,(u). Therefore, p,,(u) should be the density 

270 
Introduction to Statistical Pattern Recognition 
function of u, where u is the coverage of L(X) whose boundary is determined 
by the kth NN. 
(6.76) 
That is, p,(u) is a Beta distribution Be(k,N-k+l). Also, note that the distribu- 
tion of u is independent of the underlying distribution, p (X). 
More generally, the joint density function of uI , . . . ,uk may be obtained 
1171 
(6.77) 
where ui is the coverage of Li(X), the region extended until the ith NN is 
found. Note that the joint density depends on uk only. The marginal density 
of uk can be obtained by integrating (6.77) with respect to u I ,  . . . ,uk-l as 
N !  
(k - 1 ) ! (N 4) ! 
6"' . . . c 2 p  (u 1, . . . ,uk)duI . . . duk-l = 
ut-' ( I - u ~ ) ~ - ~  . 
(6.78) 
Equation (6.78) is the same as (6.76). 
over L ( X )  with respect to Y. That is, 
The relationship between u and v may be obtained by integrating (6.10) 
1 
u(X) Z p ( X ) v ( X )  + 2 f r ( V 2 p ( X ) [  
(Y-X)(Y-X)'dY} 
(X ) 
(6.79) 
where a is given in (6.13). Note that j(Y-X)(Y-X)TdY = vr2A from (6.27). 
The term [l+ar2/2] of (6.79) appeared in (6.18) in the Parzen case. Again, 
u =pv gives the first order approximation, and (6.79) is the second order 
approximation of u in terms of v. 
1 
2 
= p ( x ) v ( x ) [ 1  + - ~ ( x ) ~ . ~ ( x ) J  , 
Moments of p(X): When the first order approximation of u = pv is used, 
from (6.68) and (6.76) 
(6.80) 
where the following formula is used 

6 Nonparametric Density Estimation 
27 I 
(6.81) 
Equation (6.80) indicates that p = (k-I)/Nv is unbiased as long as u =pv 
holds. If k/Nv is used instead, the estimate becomes biased. This is the reason 
why (k-1) is used in (6.68) instead of k. The variance of i(X) also can be 
computed under the approximation of u = pv as 
(6.82) 
Comparison of (6.29) and (6.82) shows that the variance of the kNN density 
estimate is larger than the one for the Parzen density estimate. Also, (6.82) 
indicates that, in the kNN density estimate, k must be selected larger than 2. 
Otherwise, a large variance may result. 
Second order approximation: When the second order approximation is 
needed, (6.79) must be used to relate u and v. However, since r2 and 1' are 
related by v = cr", it is difficult to solve (6.79) for v and a series of approxima- 
tions is necessary. Since p = (k-l)/Nv, the computation of the first and second 
order moments of i(X) requires E { v-' ] and E { v - ~  
1. We start to derive v-l 
from (6.79) as 
A 
v-l - 
= p  [u-l 
+ Lc1c-2,n 
2/n - I  
2 
v
u
1
 
(6.83) 
1 
2 
z p [u-1 + --a(cp)-2'""2"-'] , 
where the approximation of u = p v  is applied to the second term to obtain the 
second line from the first. Note that the second term was ignored in the first 
order approximation and therefore is supposed to be much smaller than the first 
term. Thus, using u =pv to approximate the second term is justified. From 
(6.83) 

272 
Introduction to Statistical Pattern Recognition 
for 
k-m > 0 
T(k-m)T(N +1) 
E{u-"] = T( k)T(N + 1 -m ) 
Therefore, 
N(N-1) 
(k - 1 )(k -2) 
E(u-1) = - 
and 
E ( u - ~ }  
= 
k-1 
and 
r(k-l+&)r(N+l) 
N 
T(k-1+6) 
T(N) 
E { & ' }  = 
- 
T(k)T(N +6) 
k-I 
T(k-1) 
T(N+6) ' 
(6.85) 
(6.86) 
(6.87) 
(6.88) 
T(k-2+6)T(N+l) - N(N-1) 
T(k-2+6) 
T(N-1) 
- 
6 2  - 
E'u 
I-- 
T(k)T(N-1+6) 
(k-l)(k-2) 
T(k-2) T(N-1+6) 
where T(x+l) = xT(x) is used. It is known that 
(6.89) 
is a good approximation for large x and small 6. Therefore, applying this 
approximation, 
k-1 
N 
k-2 
N 
k-1 
N-2 
E(u61} E(-)&' 
and 
E(&'] 
z -(-)&I 
Combining (6.83), (6.84), (6.86), and (6.90), 
k-1 
1 
k-1 
N 
2 
N 
E { f j ( X ) )  = -E(v-' 
1 E ~(X)[1+-~(X)(cp(X))-2"'(-)2i'~l 
(6.90) 
(6.91) 
k-1 
k-2 
N 
N-1 
+ cL(cp)-2"-)(-)2",-1 

6 Nonparametric Density Estimation 
273 
where N >> k >> 1 is assumed. Therefore, the variance and mean-square error 
of &x) are 
(6.93) 
(6.94) 
Again, in (6.94) the first and second terms are the variance and the squared 
bias respectively. It must be pointed out that the series of approximations used 
to obtain (6.91)-(6.94) is valid only for large k. For small k, different and more 
complex approximations for E { p(X)) and Var( p(X)] must be derived by using 
(6.87) and (6.88) rather than (6.90). As in the Parzen case, the second order 
approximation for the bias and the first order approximation for the variance 
may be used for simplicity. Also, note that the MSE of (6.94) becomes zero as 
k+-= and klN+O. These are the conditions for the kNN density estimate to be 
asymptotically unbiased and consistent [ 141. 
A 
A 
Optimal Number of Neighbors 
Optimal k: In order to apply the kNN density estimate of (6.68), we 
need to know what value to select for k. The optimal k under the approxima- 
tion of 14 =PI’ is m, by minimizing (6.82) with respect to k. That is, when 
L ( X )  is small and u =PI’ holds, the variance dominates the MSE and can be 
reduced by selecting larger k or larger L ( X ) .  As L ( X )  becomes larger, the 
second order term produces the bias and the bias increases with L(X). The 
optimal k is determined by the rate of the variance decrease and the rate of bias 
increase. 

274 
Introduction to Statistical Pattern Recognition 
The optimal k, k*, may be found by minimizing the mean-square error of 
(6.94). That is, solving aMSE lak = 0 for k yields [5] 
L 
J 
(6.95) 
As in the Parzen case, the optimal k is a function of X. Equation (6.95) indi- 
cates that k* is invariant under any non-singular transformation. That is, 
k*(Z) = k*(X) . 
(6.96) 
Also, k* and I-* of (6.36) are related by 
(6.97) 
This indicates that both the Parzen and kNN density estimates become optimal 
in the same local range of L(X). The resulting mean-square error is obtained 
by substituting (6.95) into (6.94). 
4 
MSE* { &X)} = 
. 
(6.98) 
- 
IA 1'' 
Note that (6.98) and (6.38) are identical. That is, both the Parzen (with the 
uniform kernel) and kNN density estimates produce the same optimal MSE. 
The globally optimal k may be obtained by minimizing the integral 
mean-square error criterion. From (6.94), with a fixed k, 
I 
1 
IMSE = -Jp2(X)dX 
k 
+ - ~ ~ ' " ( ~ ) ~ ' ~ ' ~ ~ ~ ( X ) p ~ ~ ' " ( X ) d x  
4 
N 
. 
(6.99) 

6 Nonparametric Density Estimation 
275 
Solving alMSEIak = 0 generates [5] 
4 
-- 
x N  ’ l M  . 
(6.100) 
(6.101) 
It should be pointed out that Ex ( M S E  { p(X)} } can be minimized by a similar 
procedure to obtain the globally optimal k. The resulting k* is similar but 
slightly smaller than k* of (6.100). 
Optimal metric: The optimal metric also can be computed as in the Par- 
zen case. Again, a family of density functions with the form of (6.51) is stu- 
died with the metric of (6.24). In order to diagonalize both B and A to I and A 
respectively, X is linearly transformed to Z. In the transformed Z-space, 
IMSE; becomes, from (6.101) and (6.13), 
IMSE; = c I I 
c2jpP’”(Z)t? [ v2p~(Z)- A Idz]& , 
(6.102) 
where c I  and c2 are positive constants. IMSE; can be minimized with respect 
to A by minimizing 
n 
J = tr2(V2pZ(Z)A) - p( nh,-l) , 
(6.103) 
i = l  
which is identical to (6.59). 
Therefore, the optimal metric A for the kNN density estimate is identical 

276 
Introduction to Statistical Pattern Recognition 
to B. Also, note that the same optimal metric is obtained by minimizing MSE* 
of (6.98), and thus the metric is optimal locally as well as globally. 
Normal example: The optimal k for a normal distribution can be com- 
puted easily. For a normal distribution with zero expected vector and identity 
covariance matrix, 
(6.104) 
(6.105) 
Substituting (6.104) and (6.105) into (6. loo), and noting that the optimal 
metric A is I in this case, 
4 
. 
(6.106) 
N ll+4 
I;" 
- 
k* = 
TABLE 6-2 
OPTIMAL k FOR NORMAL DISTRIBUTIONS 
4 
1 
8 1 
16 1 
32 1 
64 1 
128 1 
0.75 N ' I 2  
0.94N 
0.62 N 
0.34 N ' I 9  
0.17 N "I7 
0.09 N 
for 
4.4~10 
1.5~10' 
3.4~10~ 3.2~10'" 
9 . 2 ~ 1 0 ~ ~  
3 . 8 ~ 1 0 ~ ~  
k*=5 
Table 6-2 shows k* for various values of n [ 5 ] .  Also, Table 6-2 shows how 
many samples are needed for k* to be 5. Note that N becomes very large after 
n = 16. This suggests how difficult it is to estimate a density function in a 
high-dimensional space, unless an extremely large number of samples is avail- 
able. 

6 Nonparametric Density Estimation 
277 
Distance to Neighbors 
Distance to W N :  
From (6.25), the distance to the kth nearest neighbor 
may be expressed in terms of the corresponding volume as 
The distance is a random variable due to v. Using the first order approxima- 
tion of u = p v  and knowing the density function of u as (6.76), the mth order 
moments of d f N N ( X )  can be obtained as [ 181 
(6.108) 
where 
p
/
N
 (__) 
n +2 
(6.109) 
Note that A = C is used as the optimal matrix. The overall average of this dis- 
tance in the entire space is 
2 
T(k+m/n) 
T(N+l) 
xn”2 , Z , m12n 
r(k) 
r(N+l+m/n) ’ 
V =  
E X E ( d ; b N ( X ) J  Z v E X { p - ” ’ ” ( X ) ~  . 
(6.110) 
Ex (p-”’’”(X) 1 for normal and uniform distributions can be expressed as 
(a) Normal: 
Ex {p-””” (X) = ( 2 ~ ) ” ’ ~  
I 
I n112f1 
(6.111) 
(b) Uniform [see (6.22)]: 
where both the normal and uniform distributions have zero expected vector and 
covariance matrix Z. Substituting (6.109), (6.1 1 I), and (6.1 12) into (6.1 IO), 
(a) Normal: 

27 8 
Introduction to Statistical Pattern Recognition 
(6.1 13) 
(b) Uniform: 
The reader may check that Tl” 2’12 ( l - l h ~ ) - ~ ’ ~  
for normal and (n+2)”’ for 
uniform with m = 1 are close for a large n (10.3 and 10.1 respectively for 
n = 100). 
Effect of parameters: Let us examine (6.108) and (6.109) with m = 1. 
These equations reveal how E { dkNN(X)) 
is affected by such parameters as n, k, 
N, and p ( X ) .  The effect of k appears only in the second term of (6.109). 
When m = 1 and n is large, T(k+I/n)lT(k) E k““ is close to 1 regardless of the 
value of k. This means that the average distance to the first NN is almost the 
same as the average distance to the second NN, and so on. The effect of N, 
which appears in the third term of (6.109), is also minimal, since 
T(N+l)lT(N+l+lln) N-”” 
1 for large n. The effect of the location is 
observed as p-””(X) in (6.108). When n is large, p-’”’(X) Z 1 regardless of 
the value of p ( X )  unless p ( X )  is either extremely large or small. Thus, 
E ( dkNN(X)] 
is highly influenced only by n and I E I. On the other hand, in the 
global kNN distance ExE{dkNN(X)), 
the I 1  I term in v cancels with the I Z I of 
Ex{p-””(X)), and only n determines the averaged kNN distance. This is true 
because the distances are normalized by C as in (6.24). Table 6-3 shows 
ExE(dkNN(X)} for various n, k, and N for normal and uniform distributions 
with covariance matrix I [18]. The parameter values are n = 10, k = 1, and 
N = 100, unless otherwise indicated. It can be observed from Table 6-3 that 
the effects of k and N are not significant for n = 10. This behavior is even 
more apparent for higher-dimensions. 
Although the above results are contrary to our intuition, they could be 
better understood by observing the volume to the kth NN, v ~ N N ,  instead of the 
distance. For example, dzNN/dNN 
= 2.551l2.319 El.1 and is close to 1 for a 
10-dimensional normal distribution from Table 6-3. However, the ratio of the 
corresponding volumes is V ~ N N I V N N  = ( ~ ~ N N / ~ N N ) ’ O  
E2.6, which is not close to 
1. That is, the effect of k on v ~ N N  is significant. The same is true for the 

6 Nonparametric Density Estimation 
279 
TABLE 6-3 
THE AVERAGE DISTANCE TO THE kth NEAREST NEIGHBOR 
n = 5  
10 
20 
~ 
N = 50 
100 
200 
400 
k = l  
2 
3 
Normal 
Uniform 
1.147 
0.966 
2.319 
2.078 
3.886 
3.626 
2.484 
2.226 
2.319 
2.078 
2.164 
1.940 
2.020 
1.810 
2.319 
2.078 
2.55 1 
2.286 
2.679 
2.400 
effects of N and p ( X )  on v ~ N N .  Since we estimate a density function by (6.68), 
if k or N is changed, v must be changed accordingly. Because of the nth 
power, a reasonable change of the volume is translated to a very small change 
of the distance for a large n. 
In order to show the effect of the location on E ( d k N N ( X ) } ,  
the following 
experiment was conducted. 
Experiment 1: NN distance 
Data: N(O,I), n = 10 
kNN: k = 1 
Sample size: N = 100 
No. of trials: 2 = 10 
Results: Fig. 6-2 [ 181 
Figure 6-2 shows the averaged NN distances and the standard deviations of 10 
trails vs. the distance from the center, !. Also, theoretical curves computed 
from (6.108) are plotted by dotted lines. The theoretical and experimental 
curves match closely until L = 4, where most samples are located. Also, note 
that the standard deviation is very small. This is predicted theoretically, 

280 
Introduction to Statistical Pattern Recognition 
- 
~ N N  
ONN 
10 
8 
6 
4 
2 
0 
experimental result 
1st order approx. 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
1 
2 
3 
4 
5 
6 
Fig. 6-2 Effect of location on the NN distance. 
because Var(d) =ExE(d2(X)) - [E, E(d(X))I2 E O  if T(x+6)/T(x) E x 6  can 
be used as an approximation. So, all dNN(X) are close to the expected value. 
As is expected from (6.108), E ( dNN(X)) does not change much from small S to 
large e. The marginal density, p (t), is also plotted in Fig. 6-2. 
Intrinsic Dimensionality 
Whenever we are confronted with high-dimensional data sets, it is usu- 
ally advantageous for us to discover or impose some structure on the data. 
Therefore, we might assume that the generation of the data is governed by a 
certain number of underlying parameters. The minimum number of parameters 
required to account for the observed properties of the data, n,, is called the 
intrinsic or effective dimensionality of the data set, or, equivalently, the data 
generating process. That is, when n random variables are functions of ne vari- 
ables such as xi = gi(yl, . . . ,y,J (i = 1, . . . ,n), the intrinsic dimensionality of 
the X-space is n,. The geometric interpretation is that the entire data set lies 
on a topological hypersurface of n,-dimension. 

6 Nonparametric Density Estimation 
28 I 
The conventional technique used to measure the dimensionality is to 
compute the eigenvalues and eigenvectors of the covariance matrix and count 
the number of dominant eigenvalues. The corresponding eigenvectors form the 
effective subspace. Although this technique is powerful, it is limited because it 
is based on a linear transformation. For example, in Fig. 6-3, a one- 
Fig. 6-3 Intrinsic dimensionality and linear mapping. 
dimensional distribution is shown by a solid line. The eigenvalues and eigen- 
vectors of this distribution are the same as the ones of the two-dimensional 
normal distribution of the dotted line. Thus, the conventional technique fails to 
demonstrate the intrinsic dimensionality, which is one for this example. 
The intrinsic dimensionality is, in essence, a local characteristic of the 
distribution, as shown in Fig. 6-4. If we establish small local regions around 
X I ,  Xa. X 3 ,  etc., the dimensionality within the local region must be close to 1 
[ 19],[20]. Because of this, the intrinsic dimensionality is sometimes called the 
local dimensionali5. This approach is similar to the local linearization of a 
nonlinear function. 
When k nearest neighbors are used to estimate dimensionality, the esti- 
mate relies on the local properties of the distribution and is not related to the 
global properties. Thus, the estimated dimensionality must be the intrinsic 
dimensionality. Keeping this in mind, let us compute the ratio of two NN dis- 
tances from (6.108)-(6.110) 

282 
Introduction to Statistical Pattern Recognition 
where use has been made of T(x+l) =xT(x). Measuring the left-hand side 
from the given data set and solving (6.1 15) for n, we can obtain the local 
dimensionality [18],[21]. 
In succeeding chapters, we will discuss the effect of dimensionality in 
various nonparametric operations. The dimensionality is the most important 
parameter in determining nonparametric properties as was already seen in 
E(dFN(X)) of (6.108). However, note that the dimensionality in non- 
parametric operations automatically means the intrinsic or local dimensionality. 
Without realizing this fact, readers may often find a discrepancy between 
theoretical and experimental results. 
Experiment 2: The Gaussian pulse is a popular waveform which reason- 
ably approximates many signals encountered in practice. The waveform is 
characterized by three parameters, a, m, and Q, as 
x(t) = a exp 1-91 
. 
(6.1 16) 
When these three parameters are random, the resulting random process x(t) has 
an intrinsic dimensionality of 3. In order to verify this, 250 waveforms were 
generated with uniform distributions for a, rn, and (T in the following ranges. 
0.7 5 a I 
1.3 , 
0.3 I 
m 50.7 , 
0.2 5 o 10.4 . 
(6.1 17) 
The waveforms were time-sampled at 8 points in 0 S r I 1.05 with increment 
0.15, forming eight-dimensional random vectors. These vectors lie on a three- 
dimensional warped surface in the eight-dimensional space. The kNN distances 
of each sample for k = I ,  2, 3, and 4 were computed, and averaged over 250 
samples. These averages were used to compute the intrinsic dimensionality of 
the data by (6.1 15). Table 6-4 shows the results. The procedure estimated the 
intrinsic dimensionality accurately. 

6 Nonparametric Density Estimation 
TABLE 6-4 
ESTIMATION OF INTRINSIC DIMENSIONALITY 
Averaged 
distance 
3NN 
4NN 
;insic i INN & ~ N N  
dimension 
2NN & 3NN 
estimated 
3NN & 4NN 
Average 
Gaussian 
pulse 
0.74 
0.99 
1.14 
1.26 
3.02 
3.19 
3.13 
3.1 1 
I 
Double 
exponential 
0.77 
1.02 
1.19 
1.31 
3.13 
2.97 
3.14 
3.08 
283 
- -1 
Fig. 6-4 Local subsets of data. 

284 
Introduction to Statistical Pattern Recognition 
Experiment 3: A similar experiment was conducted for a double 
exponential waveform as 
(6.1 18) 
where three parameters are uniformly distributed in 
0.7 5 a 5 1.3 , 
0.3 5 m 5 0.7 , 
0.3 5 z 10.6 . 
(6.1 19) 
Using eight sampling points and 250 waveforms, the intrinsic dimensionality 
of the data was estimated, and the results are shown in Table 6-4. Again, 
fairly accurate estimates of the intrinsic dimensionality (which is 3) were 
obtained. 
Experiment 4: The intrinsic dimensionalities of Data RADAR were 
estimated by (6.115). They were found to be 19.8 for Chevrolet Camaro and 
17.7 for Dodge Van, down from the original dimensionality of 66. This indi- 
cates that the number of features could be reduced significantly. Although this 
technique does not suggest how to reduce the number of features, the above 
numbers could serve as a guide to know how small the number of features 
should be. 
Very Large Number of Classes 
Another application in which the kNN distance is useful is a 
classification scenario where the number of classes is very large, perhaps in the 
hundreds. For simplicity, let us assume that we have N classes whose expected 
vectors Mi (i = l,,.,,N) are distributed uniformly with a covariance matrix I, 
and each class is distributed normally with the covariance matrix 0 ~ 1 .  
When only a pair of classes, 0; and ai, 
is considered, the Bayes 
classifier becomes a bisector between Mi 
and M,i, and the resulting error is 
m 
Ep = 
j 
1 
e-.'ZIZ 
dx 
(pairwise error) , 
(6.120) 
where d(Mj,Mj) is the Euclidean distance between Mi and Mj. Equation 
(6.120) indicates that E,, depends only on the signal-to-noise ratio, d ( M i , M j ) l ~ .  
When the number of classes is increased, Mi is surrounded by many neighbor- 
ing classes as seen in Fig. 6-5, where MmN is the center of the kth nearest 
d(M,.M,)iZo 

6 Nonparametric Density Estimation 
285 
x. 
M 
‘X 
5NN 
Pairwise classifier 
Fig. 6-5 Distribution of class centers and classifiers. 
neighbor class. If the distance to the closest neighbor, d(M,,MNN), is much 
smaller than the distances to the other neighbors, the pairwise error between 0, 
and o,, dominates the total error. However, (6.108) and (6.109) suggest that 
d(M,MLNN) 
is almost the same, regardless of k. The number of classes, N, and 
the distribution of the M,’s (uniform, normal and so on) have very little effect. 
Only the dimensionality, n, has a significant effect on d (M,MkNN). Since all 
neighboring M,’s are equally distanced from M I ,  the error from each pair, &,,, 
can be added up to produce a large total error, E,. Figure 6-6 shows expen- 
mental results indicating the relationship between E, and oIE,E { dNN(X)} for 
various values of n. Note that n is the intrinsic dimension of the distribution of 
the M,’s. 
Experiment 5: Error for N-class problem 
Data: 
Mi - uniform with mean 0 and covariance I 
x - N(M,,G*I) i = 1, ..., N 
Dimensionality: n = 5, 10, 20 
Sample size: N = 10n (10n classes) 
1 On samples/class 
No. of trials: T = 10 
Classifier: Bisectors between the generated M ,  ’s. 
Results: Fig. 6-6 [ 181 
Although the results are not shown here, the experiment confirmed that these 
curves are almost invariant for various values of N and distributions of M,’s. 
The theoretical saturation error for (3-00 
is ( l - I / N ) Z l  (100%) for N-class 
problem. 

286 
E %  A 
100 - 
80 - 
60 - 
40 - 
- 
20 - 
- 
Introduction to Statistical Pattern Recognition 
0.0 
0.2 
0.4 
0.6 
0.8 
1 .o 
1.2 
Fig. 6-6 Errors for a large number of classes. 
Figure 6-6 indicates that, when cVIEXE(d~~(X)) 
is larger than 0.2 for a 
large number of classes, it may not be feasible to identify individual classes. 
Therefore, before trying to design any classifier, it is advisable to confirm that 
the classes are well separated pairwise. One way to evade this difficulty is to 
assemble the classes into a smaller number (L) of groups, and to treat it as an 
L-class problem. The errors, which occur among classes within the same 
group, are considered as correct classification. Only the error from a class of 
one group to a class of another group is treated as the error between groups. 
Thus, the error between groups is normally much smaller than the error 
between classes. 
It has been found experimentally that E,, and E, are very roughly related 
by E, E(2 + 0.2n)~,, for small CY. That is, E, is about equivalent to the errors 
due to 4 and 6 neighbors for n = 10 and 20 respectively, assuming all distances 

6 Nonparametric Density Estimation 
287 
to these neighbors are equal and the pairwise errors are added without mutual 
interation. For large 0, E,, tends to saturate at 50% while E, does at 100%. 
Thus, the above empirical equation does not hold. 
When one class is surrounded by many other classes, we may design a 
circular, one-class classifier. That is, X is classified to of if d ( X , M , ) c  
d(Mf,MNN)/2 
[see Fig. 6-51. Then, the error from oi, E,, is 
P-’ e+”*dt (circular error) , 
cu 
(6.121) 
n 
n +2 
2 
E,. = 
I 
~(M,,M,,)/~o 2nQr(-) 
where the integrand is the marginal density function of the distance from the 
center and is derived from Nx(O,l). Note that the density function of the 
squared-distance, 6, is given in (3.59) for Nx(O,l). Therefore, the inte rand of 
E, computed from (6.121) is plotted (dotted lines) in Fig. 6-6. As is seen in 
Figs. 6-5 and 6-6, the circular classifier is worse than the pairwise bisector 
classifier. 
(6.121) may be obtained from (3.59) by applying a transformatione = ? 
5. The 
6.3 Expansion by Basis Functions 
Expansion of Density Functions 
Basis functions: Another approach to approximating a density function 
is to find an expansion in a set of husisfuncfions @;(X) as 
(6.122) 
If the basis functions satisfy 
IK(X)$;(X)l);(x)dx = hj6;j , 
(6.123) 
we say that the @;(X)’s are orthogonal with respect to the kernel K(X). The 
term l)T(X) is the complex conjugate of l);(X), and equals $;(X) 
when @;(X) is a 
real function. If the basis functions are orthogonal with respect to K(X), the 
coefficients of (6.122) are computed by 
h1c; = I.CX)p (X)@T(X)dX . 
(6.124) 
When K ( X )  is a density function, (6.123) and (6.124) may be expressed by 

288 
Introduction to Statistical Pattern Recognition 
E{p(X)&(X)) = h;c; . 
(6.126) 
When we terminate the expansion of (6.122) for i = m, the squared error 
is given by 
(6.127) 
Thus, hi IC; l 2  represents the error due to the elimination of the ith term in the 
expansion. This means that, if we can find a set of basis functions such that 
hi I ci I 
decreases quickly as i increases, the set of basis functions forms an 
economical representation of the density function. 
There is no known procedure for choosing a set of basis functions in the 
general multivariate case. Therefore, we will only consider special cases where 
the basis functions are well defined. 
Both the Fourier series and the Fourier transform are examples of 
expanding a function in a set of basis functions. The characteristic function of 
a density function is a Fourier transform and is thus one kind of expansion of a 
density function. Here we seek a simpler kind of expansion. 
One-dimensional case: When a density function is one-dimensional, we 
may try many well-known basis functions, such as Fourier series, Legendre, 
Gegenbauer, Jacohi, Hermite, and Leguerre polynomials, etc. [22]. Most of 
them have been developed for approximating a waveform, but obviously we 
can look at a one-dimensional density function as a waveform. 
As a typical example of the expansion, let us study the Hermite polyno- 
mial which is used to approximate a density function distorted from a normal 
distribution. That is, 

6 Nonparametric Density Estimation 
289 
The orthogonal condition is given by 
X 2  
202 
exp(-- 
)@,(~)@~(x)dx 
= i !6,, . 
1 
The coefficients L', can be obtained by 
where mi is the ith moment of p (x) as 
For example, if p ( x )  has zero-mean and has o2 as the variance, then 
mo 
- = o ,  
(3" 
(6.128) 
(6.129) 
(6.130) 
(6.1 31) 
(6.132) 
(6.133) 
(6.134) 
(6.135) 
(6.136) 

290 
Introduction to Statistical Pattern Recognition 
m4 
3 .  
-- 
4!c4 = - 
m4 - [*I7 
4 m2 + 1 . 3  [:IF 
= 
o4 
o4 
(6.137) 
(6.138) 
Therefore, terminating at i = 4, we have an approximation of a density function 
p (x) in terms of & ( X )  and the moments of p (x) as 
1 
X2 
(2n)1/20 exp -- 
- 
- - 
[ 202 
L 
m3 
I + -  3!03 
1 
X 
0 - 
1 
3 
- [:I 1: 
I 
. 
(6.139) 
Because of the complexity involved in the multivariate case, it is not as 
easy to find general basis functions or to calculate the coefficients. 
Density Function of Binary Inputs 
Basis functions for binary inputs: When the n inputs are binary 
numbers +I or -1, it is known that a linear combination of 2" independent 
basis functions can yield any density function without error. 
2"- I 
P ( X )  = ; =o c;+;(X) . 
(6.140) 
Table 6-5 shows the truth table that specifies p ( X ) .  Again, it is hard to say 
how we should select the 2" basis functions. However, a typical set of basis 

6 Nonparametric Density Estimation 
29 1 
TABLE 6-5 
SPECIFICATION OF A DENSITY FUNCTION 
OF BINARY VARIABLES ;
-1 
. .  . 
-I 
p ( X , )  
X2Il-1 
+1 
+1 
. . . 
+1 
P(X2"-1) 
functions is given as follows [23]: 
@O(X) = 1 9 
X I  -01 
1 1 7  - an 
(1 - a
y
 ' 
$2"- I (X) = ( 1 - a 2  
112 . . .  
I )  
which is a complete orthonormal set with the kernel 
(6.141) 
(6.142) 
That is, 

292 
Introduction to Statistical Pattern Recognition 
(6.143) 
The ai’s are control parameters and must be in the range 0 < ai < 1. The ci’s 
can be calculated by 
2“- 1 
b O  
Cj = I: K(XOp(X,)$i(XE) . 
Two special cases of the above expansion are well known. 
(6.144) 
The Walsh function: Selecting a; = 0 (i = I ,  . . . ,n), the basis functions 
become 
with the kernel 
1 
2” 
K(x) = - 
(6.146) 
This set of basis functions is known as the Wdsh funcfions and is used often 
for the expansions of binary functions. 
The Bahadur expansion: Let us introduce the following transformation: 
(6.147) 
That is, x, = + 1 and -1 correspond to yi = 1 and 0. Also, let Pi be the margi- 
nal probability of yi = 1, 
P, = PI-{Yi = +1) . 
(6.148) 
Then the expected value and variance of yi are given by 
E { y ; } = l x P ; + O x ( l - P ; ) = P ; ,  
(6.149) 
If we select ai as 

6 Nonparametric Density Estimation 
then the basis function of (6.141) becomes 
where 
which is the normalized y,. 
On the other hand, the kernel of (6.142) becomes 
If the y ,  's are mutually independent, p (Y) becomes equal to K(Y). 
Thus, we can find the expansion of p ( Y )  as 
293 
(6.151) 
(6.152) 
(6.153) 
(6.154) 
(6.155) 
where the first term K(Y) equals p (Y) under the independence assumption, and 
all other terms of [.] are the correction terms. The cj's are calculated by 
(6.156) 
Thus, (6.155) becomes 
where y's are the correlation coefficients of the associated variables. 
(6.158) 

294 
Introduction to Statistical Pattern Recognition 
This expansion is called the Bahadur- expansion [24]. In this expansion, we 
can see the effects of the correlations on the approximation of a density func- 
tion. In general, since the higher-order correlations are usually smaller than 
lower-order correlations, we may terminate the expansion with a reasonable 
number of terms and reasonable accuracy. 
Example 1: Let us calculate the Bahadur expansions for two density 
functions, p I ( Y )  and pz(Y), given in Fig. 6-7. We obtain the same basis func- 
tions and the same kernels for both p I(Y) and p 2 ( Y )  as 
(6.160) 
1 
1 
P I  = ?  
and 
P 2 = ? ,  
(6.161) 
(6.1 62) 
Y2 
Fig. 6-7 An example for the Bahadur expansion. 
The correlation coefficients of yI and y2 for p I (Y) and p z ( Y ) ,  y(,i’ and y(,;), are 
different and are calculated by 

6 Nonparametric Density Estimation 
295 
y g  = +{(2x0 - 1)(2xO - 1) + (2x1 - 1)(2xO - 1) 
+ (2x0 - 1)(2x1 - 1) + (2x1 - 1)(2x1 - 1)) = 0 ,  
I 
1 
6 
3 
y\': = -(2xO - 1)(2x0 - 1) + -(2x1 - 1)(2XO - 1) 
1 
3 
+ -(2x0 - 1)(2x1 - 
1 
1 
6 
3 
1) + -(2x1 - 1)(2x1 - 1) = -- . 
Therefore, substituting these results into (6.157), we obtain 
Computer Projects 
(6.163) 
(6.164) 
(6.165) 
(6.166) 
,. 
1. 
Estimate the mean and variance of the Parzen density estimate, p(X), as 
follows: 
Data: NX(OJ), n = 8 
Design samples: N = 100 
Test points: 
Procedure: Parzen 
Kernel: Uniform 
Kernel size: Optimal I' 
No. of trials: T = 10 
Results: Mean and variance vs. 1. 
[e 0. . . O]', 
. . . ,[O. . . 0 elT 
i =  1, 2, 3, 4, 5 
2. 
Repeat Project 1 for a normal kernel. 
3. 
Repeat Project 1 for the kNN density estimate with the optimal k. 

296 
Introduction to Statistical Pattern Recognition 
4. 
Repeat Projects 1 and 3 for various values of I' and k. Plot 1MSE vs. I' 
for the Parzen and ZMSE vs. k for the kNN. Determine the optimal I' and 
k experimentally. 
5. 
Repeat Experiment 1. 
6. 
Repeat Experiments 2 and 3. 
Problems 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Prove that 
(1) 
Equation (6.3) is a density function, and 
(2) 
the covariance matrix of (6.3) is r2A. 
Find w of (6.15) for the kernel function of (6.3). Inserting m = 1 and 00 
into the w obtained above, confirm that the w's for normal and uniform 
kernels are obtained. 
[Hint: r(E) + 1/& as E goes to zero.] 
Using a normal kernel, find the optimal r* and MSE*. Compare them 
with the optimal I-* and MSE* for a uniform kernel. 
Using Ex { MSE { p(X) } } instead of (MSE ( p ( X )  JdX, find the optimal I' 
and criterion value. Use p (X) = Nx(O,I) and the uniform kernel. 
Derive the joint density function of coverages, ul , . . . ,up. Compute the 
marginal density function of uk. 
Using Ex (MSE { p(X)} } instead of (MSE { p(X) )dX, find the optimal k 
and criterion value. Use p (X) = Nx(O,l). 
Derive E x E { d k N N ( X ) }  for the density function of (6.3). Inserting m = I 
and 00 to the above result, confirm that the averaged distances for normal 
and uniform distributions are obtained. 
Compute ExE ( dkNN(X) 
}, 
using the second order approximation 
u : 
pv(1 + ar2/2). 

6 Nonparametric Density Estimation 
297 
9. 
A density function is given in the figure. Find the Hermite expansion up 
to the fourth term and show how closely the expansion approximates the 
density function. 
p(x) 
t 
10. 
A density function of three binary inputs is given in the table. 
(a) Show the Walsh expansion. (b) Show the Bahadur expansion. 
-1 
-1 
+ I  
+ I  
+! 
References 
1. 
M. Rosenblatt, Remarks on some nonparametric estimates of a density 
function, Ann. Math. Stat., 27, pp. 832-837, 1956. 
E. Parzen, On the estimation of a probability density function and the 
mode, Ann. Math. Stat., 33, pp. 1065-1076, 1962. 
T. Cacoullos, Estimation of a multivariate density, Ann. Inst. Sfat. Math.. 
L. Devroye and L. Gyorfi, “Nonparametric Density Estimation: The L ,  
View,” John Wiley, New York, 1985. 
2. 
3. 
18, pp. 178-189, 1966. 
4. 

298 
Introduction to Statistical Pattern Recognition 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
K. Fukunaga and L. D. Hostetler, Optimization of k-nearest neighbor 
density estimates, Trans. IEEE Inform. Theory, IT- 19, pp. 320-326, 
1973. 
J. VanNess, On the dominance of non-parametric Bayes rule discrim- 
inant algorithms in high dimensions, Pattern Recognition, 12, pp. 355- 
368, 1980. 
P. Hall, Large sample optimality of least squares cross-validation in den- 
sity estimation, Ann. Star., 1 I, pp. 1156-1 174, 1983. 
Y. S. Chow, S. German, and L. D. Wu, Consistent cross-validated den- 
sity estimation, Ann. Star., 11, pp. 25-38, 1983. 
T. J. Wagner, Nonparametric estimates of probability densities, Trans. 
IEEE Inform. Theory, IT-21, pp. 438-440, 1975. 
B. W. Silverman, Choosing the window width when estimating a den- 
sity, Biornetrika, 65, pp. 1-11, 1978. 
D. J. Hand, “Kernel Discriminant Analysis,” Research Studies Press, 
Chichester, UK, 1982. 
E. Fix and L. J. Hodges, Discriminatory analysis, nonparametric discrim- 
ination, consistency properties, Report No. 4, Project 2 1-49-004, School 
of Aviation Medicine, Randolph Field, Texas, 195 1. 
E. Fix and L. J. Hodges, Nonparametric discrimination small sample per- 
formance, Report No. 11, Project 21-49-004, School of Aviation Medi- 
cine, Randolph Field, Texas, 1952. 
D. 0. Loftsgaarden and C. P. Quesenberry, A nonparametric estimate of 
a multivariate density function. Ann. Math. Stat., 36, pp. 1049-1051, 
1965. 
D. S. Moore and J. W. Yackel, Consistency properties of nearest neigh- 
bor density estimates, Ann. Stat., 5, pp. 143-154, 1977. 
L. Devroye and T. J. Wagner, The strong uniform consistency of nearest 
neighbor density estimates, Ann. Star., 5, pp. 536-540, 1977. 
D. A. S. Fraser, “Nonparametric Methods in Statistics,” John Wiley, 
New York, 1957. 
K. Fukunaga and T. E. Flick, Classification error for a very large number 
of classes, Trans. IEEE Pattern Anal. and Machine Intell., PAMI-6, pp. 
779-788, 1984. 

6 Nonparametric Density Estimation 
299 
19. 
20. 
21. 
22. 
23. 
24. 
G .  V. Trunk, Representation and analysis of signals: statistical estimation 
of intrinsic dimensionality and parameter identification, General System, 
K. Fukunaga and D. R. Olsen, An algorithm for finding intrinsic dimen- 
sionality of data, Trans. IEEE Computers, C-20, pp. 176- 183, 197 1. 
K. W. Pettis, T. A. Bailey, A. K. Jain, and R. C. Dubes, An intrinsic 
dimensionality estimator from near-neighbor information. Trans. IEEE 
Pattern Anal. and Machine Intell. PAMI-1, pp. 25-37, 1979. 
R. Deutsch, “System Analysis Techniques,” Prentice-Hall, Englewood 
Cliffs, NJ, 1969. 
T. Ito, A note on a general expansion of functions of binary variables, 
Inform. and Contr., 12, pp. 206-21 1, 1968. 
R. R. Bahadur, On classification based on responses to n dichotomous 
items, in “Studies in Item Analysis and Prediction,” ed. H. Solomon, 
Stanford University Press, Stanford, CA, 1967. 
13, pp. 49-76, 1968. 

Chapter 7 
NONPARAMETRIC CLASSIFICATION 
AND ERROR ESTIMATION 
After studying the nonparametric density estimates in Chapter 6, we are 
now ready to discuss the problem of how to design nonparumetric clussifiers 
and estimate their classification errors. 
A nonparametric classifier does not rely on any assumption concerning 
the structure of the underlying density function. Therefore, the classifier 
becomes the Bayes classifier if the density estimates converge to the true den- 
sities when an infinite number of samples are used. The resulting error is the 
Bayes error, the smallest achievable error given the underlying distributions. 
As was pointed out in Chapter 1, the Bayes error is a very important parameter 
in pattern recognition, assessing the classifiability of the data and measuring 
the discrimination capabilities of the features even before considering what 
type of classifier should be designed. The selection of features always results 
in a loss of classifiability. The amount of this loss may be measured by com- 
paring the Bayes error in the feature space with the Bayes error in the original 
data space. The same is true for a classifier. The performance of the classifier 
may be compared with the Bayes error in the original data space. However, in 
practice, we never have an infinite number of samples, and, due to the finite 
sample size, the density estimates and, subsequently, the estimate of the Bayes 
error have large biases and variances, particularly in a high-dimensional space. 
300 

7 Nonparametric Classification and Error Estimation 
301 
A similar trend was observed in the parametric cases of Chapter 5, but the 
trend is more severe with a nonparametric approach. These problems are 
addressed extensively in this chapter. 
Both Parzen and kNN approaches will be discussed. These two 
approaches offer similar algorithms for classification and error estimation, and 
give similar results. Also, the voting kNN procedure is included in this 
chapter, because the procedure is very popular, although this approach is 
slightly different from the kNN density estimation approach. 
7.1 General Discussion 
Parzen Approach 
Classifier: As we discussed in Chapter 3, the likelihood ratio classfier 
is given by -InpI(X)/p2(X) ><r, where the threshold t is determined in various 
ways depending on the type of classifier to be designed (e.g. Bayes, Neyman- 
Pearson, minimax, etc.). In this chapter, the true density functions are replaced 
by their estimates discussed in Chapter 6. When the Parzen density estimate 
with a kernel function IC,(.) 
is used, the likelihood ratio classifier becomes 
where S = {X\’), . . . ,X$!,X\2), . . . ,X$! } is the given data set. Equation (7.1) 
classifies a test sample X into either o1 or 02, 
depending on whether the left- 
hand side is smaller or larger than a threshold t. 
Error estimation: In order to estimate the error of this classifier from 
the given data set, S, we may use the resubstitution (R) and leave-one-out (L) 
methods to obtain the lower and upper bounds for the Bayes error. In the R 
method, all available samples are used to design the classifier, and the same 
sample set is tested. Therefore, when a sample Xi” from o1 is tested, the fol- 
lowing equation is used. 

302 
Introduction to Statistical Pattern Recognition 
If < is satisfied, Xi!) is correctly classified, and if > is satisfied, Xi” is 
misclassified. The R estimate of the q-error, cIR, is obtained by testing 
Xi’), . . . ,Xyl, counting the number of misclassified samples, and dividing the 
number by N I .  Similarly, &2R is estimated by testing xi2), . . . ,x$!. 
On the other hand, when the L method is applied to test Xi1), Xi’) must 
be excluded from the design set. Therefore, the numerator of (7.2) must be 
replaced by 
Again, Xi!) (k=l, . . . ,N I )  are tested and the misclassified samples are 
counted. Note that the amount subtracted in (7.3), K~ (0), does not depend on k. 
When an 02-sample is tested, the denominator of (7.2) is modified in the same 
way. 
Typical kernel functions, such as (6.3), generally satisfy ~ ~ ( 0 )  
2 K;(Y) 
(and subsequently ~ ~ ( 0 )  
2 pj(Y)). Then, 
That is, the L density estimate is always smaller than the R density estimate. 
Therefore, the left-hand side of (7.2) is larger in the L method than in the R 
method, and consequently Xi’) has more of a chance to be misclassified. Also, 
note that the L density estimate can be obtained from the R density estimate by 
simple scalar operations - subtracting K~ (0) and dividing by (N -1). 
There- 
fore, the computation time needed to obtain both the L and R density estimates 
is almost the same as that needed for the R density estimate alone. 

7 Nonparametric Classification and Error Estimation 
303 
H N  Approach 
Classifier: Using the kNN density estimate of Chapter 6, the likelihood 
ratio classifier becomes 
dz(xk:)N~.x) 
( k l - l ) N 2  lX2 I 112 
0, 
=-n In 
-In 
> < r ,  
(7.5) 
dI(Xil,)NN,X) 
(k2-1)NI IC, 
wz 
where 
11, =n”12r1(n/2+1)IC, l”2d:’ 
from 
(B.l), 
and 
df(Y,X) = 
(Y-X)TC;l(Y-X). 
In order to classify a test sample X, the k l t h  NN from oI 
and the k2th NN from o2 are found, the distances from X to these neighbors 
are measured, and these distances are inserted into (7.5) to test whether the 
left-hand side is smaller or larger than t. In order to avoid unnecessary com- 
plexity, k ,  = k2 is assumed in this chapter. 
Error estimation: The classification error based on a given data set S 
can be estimated by using the L and R methods. When Xi1) from o1 is tested 
by the R method, Xi1) must be included as a member of the design set. There- 
fore, when the kNN’s of Xi’) are found from the wI design set, Xi’’ itself is 
included among these kNN’s. Figure 7-1 shows how the kNN’s are selected 
and how the distances to the kth NN’s are measured for k = 2. Note in Fig. 7-1 
that the locus of points equidistant from Xi!) becomes ellipsoidal because the 
distance is normalized by E,. Also, since Cl # C2 in general, two different 
ellipsoids are used for o, and 02. In the R method, Xi1) and Xi,(, are the 
nearest and second nearest neighbors of Xi1) from o1 , while X,$, and X$& are 
the nearest and second nearest neighbors of Xi1) from 02. Thus, 
On the other hand, in the L method, Xi” is no longer considered a 
member of the design set. Therefore, X$h and X g N  are selected as the nearest 
and second nearest neighbors of Xi’) from 0,. The selection of o2 
neighbors 
is the same as before. Thus, 

304 
Introduction to Statistical Pattern Recognition 
I 
\ 
Fig. 7-1 Selection of neighbors. 
(7.7) 
Obviously, d I (X!&N,Xl')) 2 d ,  (X#k,Xl')), making the left-hand side of (7.7) 
larger than the left-hand side of (7.6). Thus, Xi') is more likely to be 
misclassified in the L method than in R method. 
Also, note that, in order to find the NN sample, the distances to all sam- 
ples must be computed and compared. Therefore, when d,(X$h,Xi!)) is 
obtained, d,(XgN,X&')) 
must also be available. This means that the computa- 
tion time needed to get both the L and R results is practically the same as the 
time needed for the R method alone. 
Voting RNN Procedure 
The kNN approach mentioned above can be modified as follows. Instead 
of selecting the kth NN from each class separately and comparing the distances, 
the kNN's of a test sample are selected from the mixture of classes, and the 

7 Nonparametric Classification and Error Estimation 
305 
number of neighbors from each class among the k selected samples is counted. 
The test sample is then classified to the class represented by a majority of the 
kNN’s. That is, 
kj=max{kl, . . . ,  k L )  + X E O ,  
(7.8) 
kI+. . .+k, = k , 
where kj is the number of neighbors from 
(i = 1,. . . , L )  among the kNN’s. 
In order to avoid confusion between these two rwVN procedures, we will call 
(7.8) the voting kNN procedure and (7.5) the volumetric kNN procedure. 
For the voting kNN procedure, it is common practice to use the same 
metric to measure the distances to samples from all classes, although each class 
could use its own metric. Since the kj’s are integers and a ranking procedure is 
used, it is hard to find a component of (7.8) analogous with the threshold of 
(7.5). 
It can be shown that the volumetric kiVN and voting (2k-1)NN pro- 
cedures give identical classification results for the two-class problem using the 
same metric for both classes. For example, let k and (2k-1) be 3 and 5 respec- 
tively. In the voting 5NN procedure, a test sample is classified to ol, 
if 3, 4, 
or S of the SNN’s belong to o,. This is equivalent to saying that the 3rd NN 
from o1 is closer to the test sample than the 3rd NN from 02. 
7.2 Voting kNN Procedure-Asymptotic 
Analysis 
In this section, let us study the expected performance of the voting kNN 
procedure. first for the asymptotic case (Nj = m) and later for the finite sample 
case. 
Twa-Class kNN 
NN: We start our discussion with the simplest case, setting k = 1 and 
L = 2 in (7.8). That is, in order to classify a test sample, X, the NN sample 
XNN is found. Then, X is classified to either o1 or 02, depending on the class 
membership of XNN. An error occurs when X E o1 
but XNN E 0 2 ,  or when 
X E o2 but XNN E ol. Therefore, the conditional risk given X and XNN is 
expressed by 

306 
Introduction to Statistical Pattern Recognition 
where 
qi(X) = Pr ( XEW, I X ) : a posteriori probability . 
(7.10) 
The 2nd line of (7.9) is obtained because the two events in the first line are 
mutually exclusive. The 3rd line is obtained because X and XNN are mutually 
independent. When an infinite number of samples is available, X N ~  
is located 
so close to X that qi(XNN) can be replaced by qi(X). Thus, the asymptotic con- 
ditional risk of the NN method is 
I.; (X) = 2q 1 ( X ) q 2 ( X )  = 2 W )  
(7.1 1) 
(7.12) 
2NN: When k is even, kl =k2 may occur and a decision cannot be 
made. In this case, we may set a rule that X be rejected and not counted as an 
error. In the simplest case of k = 2, the rejection occurs when XNN&wl and 
X 2 N N ~ ~ 2 ,  
or XNNeo2 and XZNN&al. On the other hand, X is misclassified, 
when X & o l  but X N N , X w N & o 2 ,  or X E W ~  but X N N , X Z N N & w I .  
Therefore, the con- 
ditional risk is 
For the asymptotic case with q i ( X )  = qi(XNN) 
= qi(XZNN), 
where q I ( X )  + q2(X) = 1 is used. 
RNN: Extending the above discussion to larger values of k, the asymp- 
totic conditional risks for odd k and even k are 

7 Nonparametric Classification and Error Estimation 
307 
On the other hand, the conditional Buyes risk given X is 
(7.15) 
(7.16) 
(7.17) 
where the 2nd line is the MacLaurin series expansion of the first line. Using 
(7.15)-(7.17), it is not difficult to prove that these conditional risks satisfy the 
following inequalities, regardless of 6 [ 11. 
(7.18) 
The proof for r* I 
r; was given in (3.157). Figure 7-2 shows these risks as 
functions of 5. The inequalities of (7.18) can also be seen in Fig. 7-2. In 
addition, 
is plotted in Fig. 7-2, because E
{
m
)
 is the Bhattacharyya 
bound of the Bayes error. Figure 7-2 shows that the kNN risks are better 
bounds than the Bhattacharyya bound. Taking the expectation of these risks 
with respect to X, the corresponding errors can be obtained. Therefore, these 
errors also satisfy the inequalities of (7.18). Thus, 
l
*
 * 
* 
*
*
 
--Y 
< r 2  <r4 I . .  
, <I-* I.. 
. 5 r 3  s r l  1 2 r * .  
2 
(7.19) 
1 ,  
* 
2 
where 
-E 
5 &2NN < &:&IN < . . . 5 E* 5 . . . 5 E j N N  5 
5 2E* , 
E* = E ( r * ( X ) }  and 
E;NN = E ( r ; ( X ) } .  
(7.20) 
Equation (7.19) indicates that the error of the voting NN procedure is 
less than twice the Bayes error. This is remarkable, considering that the pro- 
cedure does not use any information about the underlying distributions and 
only the class of the single nearest neighbor determines the outcome of the 
decision. 

308 
Introduction to Statistical Pattern Recognition 
0.5 
0.4 
0.3 
3 
.-r 
L 
0.2 
0. 
I 
0 
0 
0.05 
0.10 
0.15 
0.20 
0.25 
500 = ql(x)q*oo 
Fig. 7-2 Asymptotic risks vs. 5. 
Example 1: Figure 7-3 gives a simple example to demonstrate how the 
voting NN procedure produces an error between the Bayes error and twice the 
Bayes error. If the true Bayes classifier is known, samples 5 and 6 from wI 
and samples 1 and 3 from o2 are misclassified. By the voting NN procedure, 
these four samples are indeed misclassified, because their NN’s are from the 
other classes. However, some of these misclassified samples ( 1  from w2 and 5 
from 0 1 )  
become the NN’s of samples from the other classes ( 2  from wI and 4 
from a*), 
and produce additional errors (2 and 4). This may (for 1 and 5) or 
may not (for 3 and 6) occur, depending on the distribution of samples. There- 
fore, roughly speaking, the NN error is somewhere between the Bayes error 
and twice the Bayes error. Also, Fig. 7-3 shows that only 3 samples are 
misclassified by the voting 2NN procedure. For samples 3, 4, and 5, the votes 
are split and the samples are rejected. 

7 Nonparametric Classification and Error Estimation 
309 
t
t
 t
t
c
 
= 4/N 
t t  
t 
e2NN 
= 3/N 
Fig. 7-3 Example of kNN classification. 
Multiclass NN 
The voting NN procedure can also be applied to general L-class prob- 
lems, in which a test sample is classified to the class of the NN sample. The 
asymptotic conditional risk is 
L 
L 
4 (XI = 9 I ( X )  
s;(X)+. . . +9L(X) c q;(X) 
; = I  
; = I  
j t  I 
; # L  
L 
L 
= cq;(X)[l-qi(X)I = 1 - Z q ? ( X ) .  
i=l 
i = l  
On the other hand, the Bayes conditional risk is 
I-*(x) 
= I - max{q,(X)j = 1 - q,(x) 
. 
J 
Using the Schwanz's inequality, 
(7.21) 
(7.22) 
L 
L 
(L-1) 
q j ( X )  2[ 2 q;(X)I2 = [1-q;(X)I2 = r.*2(X) . 
(7.23) 
; = 1  
j =  I 
j ti 
j t; 
Adding (L -l)q2 (X) to both sides, 
L 
/ = I  
( L - l ) x q ; ( x )  >I-"*(X) + ( L - l ) [ I - , . * ( x ) p  
Substituting (7.24) into (7.21) [I], 
(7.24) 

3 10 
Introduction to Statistical Pattern Recognition 
(7.25) 
Equation (7.25) indicates that the NN error is still less than twice the Bayes 
error, but the upper bound becomes larger as L increases. 
Estimation of kNN Errors 
When samples are generated by a computer as in controlled experiments, 
it is generally desirable to have independent design and test sets. First, design 
samples are generated and stored. Then, test samples are independently gen- 
erated, their kNN’s are found from the design set, and the number of 
misclassified test samples is counted. This is the holdour (H) method for error 
estimation. 
However, with only one set of samples available in practice, we face the 
problem of deciding whether the sample set should be divided into two groups 
or used as one group for both design and test. In the parametric case, the latter 
approach, the R method, produces an optimistic bias, as was seen in Chapter 5. 
However, in the voting kNN procedure, we may get different results. 
Table 7-l(a) shows how the 3NN error can be estimated from a single 
sample set without dividing it into separate design and test sets. The data 
column, which lists the samples and their true classes, is given. The 3NNs of 
each sample are found and listed with their classes in the 1st-NN, 2nd-NN, and 
3rd-NN columns. Classification is the result of majority voting among the 
classes of the 3NN’s. Then, the classification column is compared with the 
true class of the data column. If the classification result matches the true class, 
the sample is labeled as correct. Otherwise, it is considered an error. The esti- 
mate of the 3NN error is obtained by counting the number of errors and divid- 
ing this by the total number of samples. 
Let us examine the first row. When X I  is tested, X I  is not included in 
the design set from which the 3NN’s of X I are selected. Therefore, this opera- 
tion utilizes the leave-one-out method. On the other hand, in the resubstitution 
method, X I  must be included in the design set. Since X I  is the closest neigh- 
bor of X I  itself, the 1st-NN column of Table 7-l(b) is identical to the data 
column, and thc 1st- and 2nd-NN columns of Table 7-l(a) are shifted to the 
2nd- and 3rd-NN columns in Table 7-l(b). Now, applying the voting 3NN 

7 Nonparametric Classification and Error Estimation 
1 
I 
2 
31 I 
or 
Error 
Correct 
Error 
Error 
TABLE 7-1 
3NN ERROR ESTIMATION PROCEDURES 
1st NN 
x3 
x 
1x 
x 3 5  
- 
0 - 
1 
1 
2 - 
2nd NN 
x IO 
25 
x536 
3rd NN 
23 
36 
x366 
- 
0 - 
2 
1 
2 - 
A 
# of errors 
EL = 
N 
(a) 3NN (or 3NN leave-one-out) 
# orerrors 
N 
&R = 
(b) 2NN (or 3NN resubstitution) 
procedure to Table 7-l(b), an error occurs only if the classes of the 2nd-NN 
and 3rd-NN agree and differ from the class of the 1st-NN (see X N ) ,  because the 
class of the 1st-NN is the same as the true class. Therefore, the number of 

312 
Introduction to Statistical Pattern Recognition 
2 -  
1 -  
errors counted in Table 7-l(b) is identical to that obtained from the voting 2NN 
procedure using the 2nd-NN and 3rd-NN columns. In the voting 2NN pro- 
cedure, a sample is rejected if the classes of the 2nd-NN and 3rd-NN columns 
disagree (see X 2 ) ,  and the case is not considered an error. Adding the 1st-NN 
column, this reject case ( X 2 )  becomes correct, but the error case (X,) still 
remains an error. Thus, the L method version of the voting 2NN procedure is 
the same as the R method version of the voting 3NN procedure. 
An experiment was conducted to compare the performances of two 
approaches: one is to divide the available sample set into design and test (the H 
method), and the other uses the procedure of Table 7- 1 (a) (the L method). 
E* 
I 
I 
I 
I 
- 
Experiment 1: NN error estimate, L and H 
Data: I-A (Normal, n = 8, E* = 1.9%) 
Sample size: N, = N2 = 50-400 (L) 
50-400 for design, 50-400 for test (H) 
No. of trials: z = 10 
Metric: A = I (Euclidean) 
Results: Fig. 7-4 
The voting NN procedure was applied. Nr indicates the number of test sam- 
ples per class. In the H method, NT is half of the available sample size, N, and 
NT = N for the L method. Note that the results are fairly close for the same 
value of NT. Although the performance of the L method is slightly worse, it is 
better to use the L method and double Nr than to divide the data into two 
groups. Also, note that the experimental curves are above twice the Bayes 

7 Nonparametric Classification and Error Estimation 
313 
error, and do not satisfy the inequality of (7.19). This is due to the large bias 
of this estimation technique, which will be discussed in the next section. 
7.3 Voting RNN Procedure-Finite Sample Analysis 
So far, the asymptotic performance of the voting kNN procedure has 
been studied. That is, the conditional risks and errors were derived based on 
the assumption that qi(XkNN) = q i ( X ) .  However, in the real world, the number 
of samples available is always finite. So, the question is how many samples 
are needed so that the observed performance is similar to the asymptotic one 
[2-71. In this section, finite sample analysis is presented for the voting NN and 
2NN procedures. 
Bias of the NN Error 
We start our analysis with (7.9). With a finite number of samples, we 
can no longer assume qj(XN,) = q j ( X ) .  Therefore, we define 6 as the differ- 
ence between q i ( X )  and q i ( X N N ) .  
9 1 ( X N N )  = 4 1 (X) + 6 and ~ ~ ( X N N )  
= q2(X) - 6 . 
(7.26) 
Equation (7.26) holds since q l ( X )  + q 2 ( X )  = 1 and ql(XNN)+ q2(XNN) = 1. 
Substituting (7.26) into (7.9), 
I ' ,  (X,X,N) = I'T ( X )  + [ q * ( X )  - q 1 (XI16 . 
(7.27) 
Thus, the bias between the finite sample and asymptotic NN errors may be 
computed by taking the expectation of the second term of (7.27) with respect 
to both XNN and X .  In order to accomplish this, ql(X,,) 
is expanded in a 
Taylor series around a given X .  Terms higher than second order are discarded 
and q ( X )  is subtracted to obtain 
(7.28) 
The metric used 
to 
measure the 
NN 
distances is 
specified by 
d 2 ( Y , X )  = (Y-X)TA-'(Y-X). In the case that A is held fixed, this is a global 
metric. However, in the more general case, A may be allowed to vary with X ,  
forming a local metric. The same metric is used for both w, and w2 in this 
section. However, a similar analysis could be done, even when two different 
rnetrics are adopted for w1 and 02. 
1 
6 zV7q I (x)(x,,-x) 
+ Ttr( V2q 
I (x)(x,,-x)(x,,-x)~ 
1 . 

3 14 
Introduction to Statistical Pattern Recognition 
The expectation of (7.28) can be computed in three stages as 
ExE,Ex,,(61p,X) where p is d ( X , X N N ) .  
The first expectation is taken with respect to XNN given X = X and 
p = p. That is, the bias is averaged over all XNN on the hyperellipsoidal sur- 
face, S (p) = ( Y :p = d (Y,X) ), specified by a constant p. Thus, 
( p ~ p  
(xNN MXNN 
(xNNMxNN 
Ex,(61P9X) = 
(7.29) 
where p ( X )  is the mixture density function, P ' p , ( X )  + P2p2(X). In order to 
obtain the second line of (7.291, the following formulas are used, along with 6 
of (7.28) [see (B.7)-(B.9)1 
S 
I,,, 
n 
(Y-X)(Y-X)TdY = -r2A . 
(7.3 1) 
(7.32) 
Note that all odd order terms of (XNN-X) disappear, since S(p) is symmetric 
around X .  
In order to take the expectation with respect to p, we can rewrite (7.29) 
in terms of u, since the density function of u is known in (6.76). Using the 
first order approximation of u Z pv, 
U Z  d"2 
p (X) 
I A I "*p" . 
n +2 
U T )  
(7.33) 
Therefore, 

7 Nonparametric Classification and Error Estimation 
315 
Finally, combining (7.27), (7.29), and (7.34), and taking the expectation 
with respect to X [SI, 
E { ; N N ~  
E E ~ N  
-tP,E,( IA l-””tr{ABI(X)]} 
(7.35) 
where 
1 
B 1 (XI = P-*”’(X)[q*(x)-q 
1 (X)l[Vp(X)VTq 
I ( X ) p - ? x ) + p 2 q  1 ( X ) ]  1 
(7.36) 
n +2 
r21n (-)r( 
1 +2in ) 
N-2111 
(7.37) 
- 
2 
- 
- 
nn 
The second line of (7.37) is obtained by approximating l-(x+a)lT(x) by x a  for 
a large integer x and a small a. 
Effect of Parameters 
Several observations may be made at this point. First, note that the 
value of PI is completely independent of the underlying densities. It depends 
only on the dimensionality of the data and the sample size, and does not 
depend on the particular distributions involved. The term inside the expecta- 
tion in (7.33, on the other hand, does not depend on the sample size. For any 
given set of distributions this term remains fixed regardless of the number of 
samples. This term does, however, depend heavily on the selection of the 
metric, A. These equations, therefore, yield much information about how the 

316 
Introduction to Statistical Pattern Recognition 
bias is effected by each of the parameters of interest (n, N ,  A, and p ( X ) ) .  Each 
of these parameters will be discussed separately as follows. 
Effect of sample size: Equation (7.37) gives an explicit expression 
showing how the sample size affects the size of the bias of the N N  error. Fig- 
ure 7-5 shows 
vs. N for various values of n [8]. The bias tends to drop off 
Fig. 7-5 PI vs. N .  
rather slowly as the sample size increases, particularly when the dimensionality 
of the data is high. This is not an encouraging result, since it tends to indicate 
that increasing the sample size N is not an effective means of reducing the bias. 
For example, with a dimensionality of 64, increasing the number of samples 
from 1,000 to 10,000 results in only a 6.9% reduction in the bias (PI from 
.0504 to .0469). Further reduction by 6.9% would require increasing the 
number of samples to over 100,000. Thus it does not appear that the asymp- 
totic NN error may be estimated simply by "choosing a large enough N" as 
generally believed, especially when the dimensionality of the data is high. The 
required value of N would be prohibitively large. 

7 Nonparametric Classification and Error Estimation 
317 
Effect of dimensionality: The dimensionality of the data appears to play 
an important role in determining the relationship between the size of the bias 
and the sample size. As is shown in Fig. 7-5, for small values of n (say, n I 
4), changing the sample size is an effective means of reducing the bias. For 
larger values of n, however, increasing the number of samples becomes a more 
and more futile means of improving the estimate. It is in these higher dimen- 
sional cases that improved techniques of accurately estimating the Bayes error 
are needed. It should be pointed out that, in the expression for the bias of the 
NN error, n represents the local or intrinsic dimensionality of the data as dis- 
cussed in Chapter 6. In many applications, the intrinsic dimensionality is 
much smaller than the dimensionality of the observation space. Therefore, in 
order to calculate PI, it is necessary that the intrinsic dimensionality be 
estimated from the data using (6.1 15). 
Effect of densities: The expectation term of (7.35) gives the effect of 
densities on the size of the bias. In general, it is very hard to determine the 
effect of this term because of its complexity. In order to investigate the gen- 
eral trends, however, we can compute the term numerically for a normal case. 
Experiment 2: Computation of Ex ( . ) of (7.35) 
Data: I-I (Normal) 
Dimensionality: n = 2, 4, 8, 16 
Sample size: N I = N 2  = 1600n 
Metric: A = I (Euclidean) 
Results: Table 7-2 [SI 
M adjusted to give E* = 2, 5, 10, 20, 30(%) 
In the experiment, B of (7.36) was evaluated at each generateL sample point 
where the mathematical formulas based on the normality assumption were used 
to compute p ( X )  and qi(X). The expectation of (7.35) was replaced by the 
sample mean taken over 160011 samples per class. 
Table 7-2 reveals many properties of the expectation term. But, special 
attention must be paid to the fact that, once n becomes large (n > 4), its value 
has little effect on the size of the expectation. This implies that PI of (7.37) 
dominates the effect of n on the bias. That is, the bias is much larger for 
high-dimensions. This coincides with the observation that, in practice, the NN 
error comes down, contrary to theoretical expectation, by selecting a smaller 

318 
30% 
Introduction to Statistical Pattern Recognition 
1'' order term 
3.4 
2"d order term 
2.2 
Sum 
5.6 
TABLE 7-2 
ESTIMATES OF THE EXPECTATION TERM 
IN (7.35) FOR NORMAL DATA 
20% 
10% 
5% 
2% 
Bayes 
Error 
1'' order term 
2.2 
2"d order term 
1.8 
Sum 
4.0 
1'' order term 
-1.3 
2'ld order term 
4.7 
Sum 
3.4 
1'' order term 
-1.9 
2"d order term 
3.8 
Sum 
1.9 
1" order term 
-2.0 
2'ld order term 
3.5 
Sum 
1.5 
1.3 
0.9 
1.2 
1.1 
2.5 
2.0 
-1.5 
-0.8 
2.3 
0.8 
0.5 
n = 16 
1.1 
0.3 
1.4 
0.8 
1 .o 
1.8 
-0.2 
1.6 
1.4 
-0.6 
1.5 
0.9 
-0.7 
1.1 
0.4 
number of features. This happens, because the bias is reduced more than the 
Bayes error is increased. In order to compare two sets of features in different 
dimensions, this dependency of the bias on n must be carefully examined. 
Also, note in Table 7-2 that the second order term due to V 2 q , ( X )  is compar- 
able to or even larger than the first order term due to Vq I ( X ) .  It is for this rea- 
son that the second order term is included in the Taylor series expansion of 
(7.28). 
Effect of metric: The expectation terms of (7.35) also indicates how the 
matrix, A, affects the bias. Certainly, proper selection of a metric may reduce 
the bias significantly. Unfortunately, B I  is a very complex function of X and 
very hard to estimate for any given set of data. As for optimization of A, 

7 Nonparametric Classification and Error Estimation 
319 
(6.54) shows that an expression of the form I A I-””tr{AB I } is minimized by 
setting A = BY‘, provided B I is a positive definite matrix. However, B might 
not be positive definite, because of the term [9*-9’] in (7.36). Thus, it is not 
immediately clear how to choose A to minimize the bias. Nevertheless, selec- 
tion of an appropriate metric remains an important topic in NN error estimation 
[9-lo]. 
Experimental Verification 
In order to verify the results mentioned above, the following experiment 
was run: 
Experiment 3: Voting NN error estimation, 
L method (Table 7-l(a)) 
Data: I-I (Normal, n = 8) 
Sample size: N 1  = N 2  = 20n, 40n, 80n, 160n 
No. of trials: z = 20 
Metric: A = I (Euclidean) 
Results: Fig. 7-6 [8] 
M adjusted to give E* = 2, 5, 10, 20, 30(%) 
In Fig. 7-6, the small circle indicates the average of the NN errors over 20 
trials, and the vertical bar represents f one standard deviation. According to 
(7.33, the bias of the NN error varies linearly with PI for any given set of dis- 
tributions. Therefore, if we know &LN and Ex(.}, 
we can predict the finite 
sample NN errors as linear functions of PI. The dotted lines of Fig. 7-6 show 
these predicted NN errors for various values of the Bayes error. The Ex { .}’s of 
(7.35) are tabulated in Table 7-2. The theoretical asymptotic error, E;,,,, 
was 
estimated by generating a large member (160011) of samples, calculating the 
risk at each sample point from (7.1 1) using the known mathematical expres- 
sions for si(X) in the normal case, and averaging the result. Note that the aver- 
ages of these measured E,,,~’s are reasonably close to the predicted values. 
While it may not be practical to obtain the asymptotic NN errors simply 
by increasing the sample size, it may be possible to use information concerning 
how the bias changes with sample size to our advantage. We could measure 
eNN empirically for several sample sizes, and obtain PI using either (7.37) or 
Fig. 7-5. These values could be used in conjunction with (7.35) to obtain an 
estimate of the asymptotic NN error as follows: 
A 

320 
Introduction to Statistical Pattern Recognition 
BAYES 
ZNN(%) 
ERROR 
-- 30% 
--- 
_---- 
--- 
I 
40 
I 
I 
I 
b 81 
0.0 
.01 
.02 
t
t
t
 t 
1280 640 320 
160 + N (for n=8) 
t 
00 
Fig. 7-6 NN errors for normal data. 
(1) 
Change the sample size N as N , ,  N 2 , .  . . ,NZ. 
For each Ni, 
calculate PI 
(using the intrinsic dimensionality) and measure ENN empirically. It is 
preferable to repeat the experiment several times independently and to 
average the measured ENN’s. 
A 
A 

7 Nonparametric Classification and Error Estimation 
32 1 
,. 
(2) 
Plot these t empirical points ENN vs. PI. Then, find the line best fitted to 
these 1 points. The slope of this line is Ex { .) and the y-intercept is &iN, 
which we would like to estimate. 
,. 
,. 
The reader must be aware that eNN varies widely (the standard deviation of ENN 
for each P, is large), and that PI = 0 is far away from the P-region where the 
actual experiments are conducted. Therefore, a small variation in 
tends to 
be amplified and causes a large shift in the estimate of E,&. 
,. 
Biases for Other Cases 
2NN: The bias of the 2NN error can be obtained from (7.13) in a similar 
fashion, resulting in [8] 
E(&,N) K&;NN + P2Ex([ IA I-1’”tr{AB2(X)}]2} , 
(7.38) 
where 
(7.39) 
1 
B 2 ( X )  =P-*’“(X)[VP(X)VTqI(X)p-I(X) 
+ ,v2qI(x)1 
1 
r( 1+4/n)N-4’f7 . 
1 +2/n 
(7.40) 
By comparing (7.40) with (7.37), it can be seen that P2 is roughly proportional 
to N41r1 while PI is proportional to N-2”1. That is, as N increases, the 2NN 
error converges to its asymptotic value more quickly than the NN error - as if 
the dimensionality, n, were half as large. Also, note that P2 is significantly 
smaller than P I ,  because r2’”/nx (.088 for n = X and .068 for n = 32) is 
squared. Many experiments also have revealed that the 2NN error is less 
biased than the NN error [I 11. Since their asymptotic errors are related by 
E;,,, 
= 2
~
;
~
~
 
from (7.1 1) and (7.14), a better estimate of &LN could be obtained 
by estimating &ZNN first and doubling it. 

322 
Introduction to Statistical Pattern Recognition 
Multiclass: The NN error for multiclass problems can also be obtained 
in a similar way, starting from (7.21) [8]. The result is 
E(&..,] :&iN 
+ PIEx( IA I-""tr(ABL(X))J , 
(7.41) 
where 
(7.42) 
Note that PI of (7.41) is the same as PI of (7.37). This means that the effect 
of sample size on the bias does not depend on the number of classes. 
7.4 Error Estimation 
In this section, we return to nonparametric density estimates, and use 
these estimates to design a classifier and estimate the classification error. Both 
the Parzen and volumetric kNN approaches will be discussed. 
However, 
because the analysis of the Parzen approach is simpler than the kNN approach, 
the Parzen approach will be presented first with detailed analysis, and then the 
kNN approach will be discussed through comparison with the Parzen approach. 
Classification and error estimation using the Parzen density estimate 
were discussed in Section 7.1. However, in order to effectively apply this 
technique to practical problems, we need to know how to determine the neces- 
sary parameter values, such as the kernel size, kernel shape, sample size, and 
threshold. 
Effect of the Kernel Size in the Parzen Approach 
As we discussed the optimal volume of the Parzen density estimate in 
Chapter 6, let us consider the problem of selecting the kernel size here. How- 
ever, density estimation and classification are different tasks, and the optimal 
solution for one might not be optimal for the other. For example, in density 
estimation, the mean-square error criterion was used to find the optimal 
volume. This criterion tends to weight the high density area more heavily than 
the low density area. On the other hand, in classification, the relationship 
between the tails of two densities is important. In this case, the mean-square 
error may not be an appropriate criterion. Despite significant efforts in the 
past, it is still unclear how to optimize the size of the kernel function for 

7 Nonparametric Classification and Error Estimation 
323 
classification. One of the ways to overcome this difficulty is to determine the 
optimal kernel size experimentally. Assuming that the kernel function of (6.3) 
is adopted with i' as the size control parameter, we may repeat the estimation of 
the classification error by both L and R methods for various values of I', and 
plot the results vs. I'. The major drawback of this approach is that the estima- 
tion procedure must be repeated completely for each value of i'. 
Experiment 4: Estimation of the Parzen errors, L and R 
Data: I-I, 1-41, I-A (Normal, n = 8) 
Sample size: N I = N2 = 100 
No. of trials: z = 10 
Kernel: Normal with A I = C 
Kernel size: I' = 0.6-3.0 
Threshold: t = 0 
Results: Fig. 7-7 11 21 
A 2 = C2 
In Fig. 7-7, the upper and lower bounds of the Bayes error were obtained by 
the L and R methods, respectively. As seen in Fig. 7-7, the error estimates are 
very sensitive to 1', except for the Data I-! case. Unless a proper I' is chosen, 
the estimates are heavily biased and do not necessarily bound the Bayes error. 
In order to understand why the error estimates behave as in Fig. 7-7 and 
to provide intelligent guidelines for parameter selection, we need a more 
detailed analysis of the Parzen error estimation procedure. 
Effect of the density estimate: In general, the likelihood ratio classifier 
is expressed by 
(7.43) 
where t is the threshold. When the estimates of p (X) and p2(X) are used, 
A 
PIG) 
P2W 
h(X) = -In- 
-I = h (X) + Ah(X) , 
(7.44) 
where is the adjusted threshold. The discriminant function i(X) is a random 
variable and deviates from h(X) by Ah(X). The effect of Ah(X) on the 
classification error can be evaluated from (5.65) as 

324 
Introduction to Statistical Pattern Recognition 
E (W 
A 
" 
1 .o 
2.0 
3.0 
(a) Data 1-1 
r 
(b) Data 1-41 
(c) Data I-A 
1 .o 
2.0 
3.0 
Fig. 7-7 Parzen error for various values of r. 

7 Nonparametric Classification and Error Estimation 
325 
E(A&) :-IIE(Ah(X) 
1 
+ -Ah 
jo 
2 (X)) 
2n 
2 
[PIP I (XI - P2P2(X)ldWX ' 
e J M X )  
From (7.44), Ah(X) and Ah2(X) are derived as follows. 
(7.45) 
(7.47) 
where Api(X) = PAX) - pi(X), Af = t - t, and the expansions are terminated at 
the second order terms. The bias of the error estimate may be obtained by tak- 
ing the expectations of (7.46) and (7.47), inserting them into (7.45), and c a y -  
ing out the integration. 
A 
,. 
Parzen kernel: When the Parzen kernel approach is used, E{;,(X)] 
Since 
- p,(X)I2 1 
and Var( if 
(X) 1 are available in (6.18) and (6.19) respectively. 
E { AP;(X> I = E { Pi(X) i - Pj(X) 
and 
= MSE{;i(X)l = Var{pi(X)} + E2{Api(X)), 
E 1 Ap! (XI I 
= E ( [ P i  (x) 
(7.48) 
(7.49) 
where MI, of (6.15) is expressed by sir-" and si is given in (6.20) and (6.26) for 

326 
Introduction to Statistical Pattern Recognition 
normal and uniform kernels respectively. The reason why the first and second 
order approximations are used for the variance and bias respectively was dis- 
cussed in Chapter 6. If the second order approximation for the variance is 
adopted, we can obtain a more accurate but complex expression for (7.49). 
Substituting (7.48) and (7.49) into (7.46) and (7.47), 
(7.50) 
I'-" 
2N isl 
PI i 
1 
1 
2 
8 
E(Ah(X)) S-r2(a2-a1)+ --~'~(a?-a;)-At + - - 
- - 
1 
At 
E (Ah2(X)) 3,r2(a2-a1 ) -All2 - yr4(a?-a;) 
L 
4 
(7.5 1) 
Note that from (6.18) and (6.19) the terms associated with r2ai are generated 
by the bias of the density estimate, and the terms associated with r-"lN come 
from the variance. The threshold adjustment At is a constant selected indepen- 
dently. 
Now, substituting (7.50) and (7.51) into (7.45) and carrying out the 
integration, the bias is expressed in terms of I' and N as 
E ( A & }  Zu1r2 + a 2 r 4  + a 3 ~ - 1 1 / N .  
(7.52) 
Here, the constants a l ,  a 2 ,  and a3 are obtained by evaluating the indicated 
integral expression in (7.45). Here, we assume, for simplicity, that the decision 
threshold t is set to zero. Because of the complexity of the expressions, expli- 
cit evaluation is not possible. However, the constants are only functions of the 
distributions and the kernel shapes, A;, and are completely independent of the 
sample size and the smoothing parameter, I'. Hence, (7.52) shows how changes 
in I' and N affect the error performance of the classifier. The a l r 2  and a2r4 
terms indicate how biases in the density estimates influence the performance of 
the classifier, while the a31'-"lN term reflects the role of the variance of the 
density estimates. For small values of I., the variance term dominates (7.52), 
and the observed error rates are significantly above the Bayes error. As I' 
grows, however, the variance term decreases while the u 1 r 2  and a2v4 terms 
play an increasingly significant role. Thus, for a typical plot of the observed 
error rate versus I', E decreases for small values of I' until a minimum point is 
A 

7 Nonparametric Classification and Error Estimation 
327 
reached, and then increases as the bias terms of the density estimates become 
more significant. This behavior is observed in Fig. 7-7 and is accurately 
predicted in the expression for E ( A E ] .  It should be noted that although expli- 
cit evaluation of a l  through a3 is not possible in general, it is reasonable to 
expect that these constants are positive. It is certainly true that E ( AE} must be 
positive for any value of r, since the Bayes decision rule is optimal in terms of 
error performance. 
Effect of Other Parameters in the Parzen Approach 
With the bias expression of the estimated error, (7.52), we can now dis- 
cuss the effect of important parameters such as N, t, and the shape of the kernel 
function. 
Effect of sample size: The role of the sample size, N, in (7.52) is seen 
as a means of reducing the term corresponding to the variance of the density 
estimates. Hence the primary effect of the sample size is seen at the smaller 
values of I; where the u3 term of (7.52) dominates. As I’ grows, and the a l  
and a 2  terms become dominant, changing the sample size has a decreasing 
effect on the resulting error rate. These observations were verified experimen- 
tally. 
Experiment 5: Estimation of the Parzen error, H 
Data: I-A (Normal, n = 8, E* = 1.9%) 
Sample size: N I = N 2  = 25, 50, 100, 200 (Design) 
N ,  = N 2  = 1000 (Test) 
No. of trial: T = 10 
Kernel: Normal with A I = I ,  A 2  = A 
Kernel size: I- = 0.6-2.4 
Threshold: f = 0 
Results: Fig. 7-8 
Figure 7-8 shows that, for each value of N, the Parzen classifier behaves as 
predicted by (7.52), decreasing to a minimum point, and then increasing as the 
biases of the density estimates become significant for larger values of r. Also 
note that the sample size plays its primary role for small values of 1’, where the 
u3 term is most significant, and has almost no effect at the larger values of I’. 

328 
Introduction to Statistical Pattern Recognition 
E(%) 
0 N=25 
7.0 .f 
0 N=50 
A N=100 
X N=200 
5.0 
9p 
H 
0 
0 
1 .o 
2.0 
Fig. 7-8 Effect of sample size on Parzen classification. 
In order to have E [A&} + 0 as N + 
00, the error expression implies that 
r must be chosen as a function of N such that I' + 0 and r-"/N + 0. This is 
the condition for the consistency of the Parzen density estimate [ 131, and vali- 
dates the approximations which we used to obtain (7.52). 
The optimal r may now be obtained from (7.52) by solving 
aE { A&)/& = 0. However, 2aI r +4a2r3 -na3r-"-'IN =O is not an easy equa- 
tion to solve, and the ai's are hard to obtain. Therefore, it seems better to find 
the minimum point of the error curve experimentally. 
Effect of the decision threshold: Increasing the sample size, N, is seen 
as a means of improving the performance of the Parzen classifier at small 
values of r. As n becomes large, however, increasing the sample size becomes 
more and more futile, and the designer is forced to resort to using larger values 
of r. This results in a reduction of the variance of the density estimates, at the 
cost of accepting a larger bias. On the other hand, (7.50) and (7.51) indicate 
that E (A&] of (7.45) could be reduced by selecting a proper threshold, At, and 
the kernel covariance matrix, A;, which determines a; [see (6.13)]. Here, we 
will study the effect of At, the adjustment of the decision threshold. Theoreti- 
cally speaking, the optimal At can be found by minimizing E (A&] with respect 
to Ar. However, in practice, it may not be possible to carry out the integration 
of (7.45) for such complicated functions of n variables. 

7 Nonparametric Classification and Error Estimation 
329 
The threshold for normal distributions: However, if the p,(X)’s are 
normal distributions, the effect of the threshold can be analyzed as follows. 
Recall from (6.7) that the expected value of & ( X )  in the Parzen density esti- 
mate is given by p ; ( x ) * ~ ; ( X ) .  When both p j ( X )  and K;(X) are normal with 
covariance matrices C; and r2Cj respectively, this convolution yields another 
normal density with mean Mi and covariance (l+r2)Ci, 
as shown in (6.31). 
For larger values of r, the variance of p ; ( X )  decreases, and the estimate 
approaches its expected value. Substituting the expected values into the 
estimated likelihood ratio, one obtains 
A 
(7.53) 
Except for the 1/( l+r2) factors on the inverse covariance matrices, this expres- 
sion is identical to the true likelihood ratio, -In p I ( X ) / p 2 ( X ) .  In fact, the two 
may be related by 
The true Bayes decision rule is given by - lnpI(X)/p2(X) ><1nPIlP2. Using 
(7.54), an equivalent test may be expressed in terms of the estimated densities: 
where 
1 
P I  
1 
/.? 
IC,I 
I+/.* 
P2 
2 
I+/-, 
IC, I 
t = -  
(ln-) 
+ -(-)In- 
. 
(7.56) 
In all of our experiments, we assume P I  = P 2  = 0.5, so the first term of (7.56) 
may be neglected. Equation (7.56) gives the appropriate threshold to use when 
the Parzen classifier with a normal kernel function is used on normal data. 
This indicates that t can be kept at zero if Cl = X I ,  but t should be adjusted for 
each value of I’ if Cl # C2. Otherwise, the classifier based on the Parzen 

330 
Introduction to Statistical Pattern Recognition 
density estimate classifies samples from the original normal distributions with 
an improper threshold. Figure 7-7 shows exactly that. In Fig. 7-7(a), with 
C1 = X2 = I, good performance was obtained even for large values of r without 
adjusting the threshold. When lEl I and IC2 I are different, as with Data 1-41 
and !-A, the performance of the Parzen classifier degrades sharply for larger 
values of r without adjusting the threshold, as evidenced in Fig. 7-7(b) and (c). 
Figure 7-9 shows the behavior of the Parzen classifier for these three data sets 
with t given by (7.56) (Option 1). For low values of 1’, the classifiers give 
similar performance to that shown in Fig. 7-7, since the appropriate value of t 
given in (7.56) is close to zero. As I’ increases, good performance is obtained 
for ail values of r. Thus, by allowing the decision threshold to vary with I-, we 
are able to make the Parzen classifier much less sensitive to the value of r. 
The threshold for non-normal distributions: The decision threshold as 
used here is simply a means of compensating for the bias inherent in the den- 
sity estimation procedure. When the data and the kernel functions are normal, 
we have shown that the bias may be completely compensated for by choosing 
the value of t given in (7.56). In the non-normal case, we cannot hope to 
obtain a decision rule equivalent to the Bayes classifier simply by varying 1. 
However, by choosing an appropriate value of t, we can hope to compensate, 
to some extent, for the bias of the density estimates in a region close to the 
Bayes decision boundary, providing significant improvement in the perfor- 
mance of the Parzen classifier. Therefore, procedures are needed for determin- 
ing the best value of t to use when non-normal data is encountered. We 
present here four possible options. These options, and a brief discussion of 
their motivation, are given below. 
Option I: Use the threshold as calculated under the normality assumption 
(7.56). Since for larger values of I’ the decision rule is dominated by the func- 
tional form of the kernels, this procedure may give satisfactory results when 
the kernels are normal. even if the data is not normal. 
Option 2: For each value of I-, find the value of t which minimizes the leave- 
one-out error, and find the optimal t for the resubstitution error separately. 
This option involves finding and sorting the L and R estimates of the likelihood 
ratio, and incrementing the values o f t  through these sorted lists. The error rate 
used as the estimate is the minimum error rate obtained over all values of t. 

7 Nonparametric Classification and Error Estimation 
33 1 
THRESHOLD SELECTION METHOD 
o OPTlON 1 
A OPTION 3 
0 OPTION 4 
(a) Data 1-1 
r 
1 .o 
2.0 
3.0 
E (%) 
- 
(b) Data 1-41 
r 
1 .o 
2.0 
3.0 
4.0 
3.0 
2.0 
1 .o 
(c) Data I-A 
r 
1 .o 
2.0 
3.0 
Fig. 7-9 Effect of threshold on Parzen classification. 

332 
Introduction to Statistical Pattern Recognition 
This option makes no assumptions about the densities of the data or the shape 
of the kernel function. However, since the value of the threshold is customized 
to the data being tested, using this option will consistently bias the results low. 
This is not objectionable in the case of R errors, since the R error is used as a 
lower bound of the Bayes error. However, using this procedure can give 
erroneous results for the L error. Options 3 and 4 are designed to alleviate this 
problem. 
Oprion 3: For each value of r, find the value of t which minimizes the R error, 
and then use this value oft to find the L error. Since the selection of the thres- 
hold has been isolated from the actual values of the L estimates of the likeli- 
hood ratio, using this method does in fact help reduce the bias encountered in 
Option 2. Experimental results will show that this method does give reliable 
results as long as r is relatively large. When r is small, however, the L esti- 
mates of the likelihood ratio are heavily biased as is seen in Fig. 7-9(b), and 
use of these estimates to determine the threshold may give far from optimal 
results. An advantage of this option is that it requires no more computation 
time than Option 2. 
Option 4: Under this option, the R error is found exactly as in Option 2, by 
finding the value oft which minimizes the R error, and using this error rate. In 
order to find the L error, we use an L procedure to determine the value of t to 
use for each sample. Hence, under Option 4, we use a different threshold to 
test each of the N l + N 2  samples, determining the threshold for each sample 
from the other NI+N2-I samples in the design set. The exact procedure is as 
follows. 
(1) 
Find the L density estimates at all samples, 
i # r  
(7.57) 
(2) 
To test sample X f ) :  
Modify the density estimates by removing the effect of X f )  from 
all estimates 
(a) 

7 Nonparametric Classification and Error Estimation 
333 
Let us assume e =  1 for example. The test sample, Xi!), was used 
to compute p (.) as in (7.57), but never used for p 2 ( . ) .  Therefore, 
the removal of Xi') does not change ;2(.) which is the case of the 
first line of (7.58). The removal of X i 1 ) ,  however, affects 
in 
two different ways, depending on whether 
(.) is evaluated at Xj') 
or X y .  i l ( X j " )  is the summation of NI-1 kernels excluding 
K ~ ( X ~ ' ) - X ~ ' ) )  
as is seen in the first line of (7.57). Therefore, the 
further removal of K~(X~')-X~')) 
can be computed by the second 
line of (7.58). On the other hand, since i 
I (X12)) is the summation 
of N I  kernels as in the second line of (7.57), the removal of 
K~(X:~)-XZI)) 
can be computed by the third line of (7.58). The 
case with E = 2 may be discussed similarly. 
Calculate the likelihood ratio estimates at all samples X y )  # X f )  
based on the modified density estimates. 
II 
(b) 
(7.59) 
(c) 
Find the value o f t  which minimizes the error among the NI+N2-I 
samples (without including X f ) ) ,  under the decision rule 
(7.60) 
This is best accomplished by first sorting the likelihood ratio esti- 

334 
Introduction to Statistical Pattern Recognition 
mates [(XY)), and then incrementing the value of t through this list, 
keeping track of the number of errors for each value oft. 
Classify the sample X f ’  using the original density estimates of 
(7.57) and the value of r found in Step (c): 
(d) 
(7.61) 
Count an error if the decided class is not 0,. 
(3) 
Repeat Step (2) for each sample, counting the resulting number of 
classification errors. 
Although this procedure is by far the most complex computationally, it is the 
only true L procedure, and gives the most reliable results, particularly for small 
values of r. 
Figure 7-9 shows the results of applying Options 1, 3, and 4 to the three 
test cases. 
Experiment 6: Estimation of the Parzen error, L and R 
Same as Experiment 4 except 
Threshold: r - Options 1, 3, 4 
Results: Fig. 7-9 [ 121 
In all of the experiments, using the threshold calculated under the normality 
assumption (Option 1) gave the best performance. This was expected, since 
both the data and the kernel function are, in fact, normal. It is notable, how- 
ever, that the use of Option 4 gave performance nearly equal to that of Option 
1. Option 3 gave good results also, but performance degraded sharply for 
small I’, particularly for Data 1-41, where the covariance determinants are 
extremely different. 
Non-normal example: It is of interest to examine the behavior of the 
Parzen classifier in a non-normal case where the quadratic classifier is no 
longer the Bayes classifier. Toward this end, the following experiment was 
conducted. 

7 Nonparametric Classification and Error Estimation 
335 
Experiment 7: Estimation of the Parzen error, L and R 
Data: 
p ( X )  = 0.5Nx(M1 ,I)+0.5Nx(M2,1) 
M I  = [OO. . . OlT, M 2  = [6.58 0 .  . . 0IT 
M 3  = I3.29 0 .  . . O]*, 
M4 = (9.87 0 .  . . OI7 
n = 8, E~ = 7.5% 
p 2 ( X )  = OSNx(M3 J)+0.5Nx(M4 , I )  
Sample size: N ,  = N ,  = 100 
No. of trials: T = 10 
Kernel: Normal with A I = A 2  = I 
Kernel size: 0.6-6.0 
Threshold: Option 4 
Results: Fig. 7-10 [I21 
30. 
20. 
IO. 
1.0 
2.0 3.0 4.0 
5.0 6.0 
Fig. 7-10 Parzen error for a non-normal test set. 
Figure 7-10 shows the results for this experiment. With a moderate 
value of 1', the Bayes error of 7.5% is bounded properly, when the Parzen den- 
sity estimate for each class represents the given two-modal distribution. On 
the other hand, as I' grows, the density estimate converges to the kernel func- 
tion itself. Thus, with normal kernels, the likelihood ratio of the estimated 

336 
Introduction to Statistical Pattern Recognition 
densities becomes a quadratic classifier, resulting in the error much higher than 
the Bayes error. As the result, the curves of Fig. 7-10 is significantly different 
from the ones of Fig. 7-9, indicating that the selection of a proper r for non- 
normal cases could be more critical than the one for normal cases. Neverthe- 
less, the Parzen classification does provide usable bounds on the Bayes error. 
Selection of the kernel shape: An alternative way of compensating for 
the biases of the error estimate is the selection of the kernel shape. Equations 
(7.50) and (7.51) suggest that, if the kernel covariances are selected such that 
a, 
(X) = a2(X), all terms which are independent of the sample size may be 
eliminated from the bias expression. Hence, we must find positive definite 
matrices A I and A2 such that, from (6.13), 
In general, V2pi(X)/pi(X)'s are hard to obtain. However, 
mal, 
(7.62) 
when pi(X) is nor- 
(7.63) 
Therefore, we may obtain a solution of (7.62) in terms of these expected vec- 
tors and covariance matrices. 
Before going to the general solution of (7.62), let us look at the simplest 
case, where Z, = C2 = C and A = A2 = C. Using (7.63), (7.62) becomes 
(X-M I )Y 
(X-M ' ) = ( X - M 2 ) Y  (X-M2) 
(7.64) 
which is satisfied by the X's located on the Bayes boundary. On the other 
hand, since the integration of (7.45) with respect to w results in 
J[E(AhJbh)+(1/2)E(Ah2 
1 d6(h)/dhl (P,p,-f 
*p2)dX, the bias is generated 
only by E { Ah(X) I and E {  Ah2(X) 1 on the boundary. Therefore, the selection 
of A = A 2  = C seems to be a reasonable choice. Indeed, the error curve of 
Fig. 7-7(a) shows little bias for large r, without adjusting the threshold. 
The general solution of (7.62) is very hard to obtain. However, since 
(7.62) is a scaler equation, there are many possible solutions. Let us select a 
solution of the form 

7 Nonparametric Classification and Error Estimation 
337 
A; = C; + y;(X-M;)(X-M;)' 
, 
(7.65) 
where y; is a constant to be determined by solving (7.62). Substituting (7.63) 
and (7.65) into (7.62) and simplifying give 
v: (X,M I 1-1 I[ 1+Y Id: (X,M 1 )I = v; (X,M2 )- 1 1 [ I+y& 
(X,M* )I 7 
(7.66) 
where d;(X,Mi) = (X-M;)'C;I(X-M;). 
If we could select yidf(X,Mj) = -1, 
(7.66) 
is 
satisfied. 
However, 
since 
(X-Mi)'A;' (X-Mi) = d?(X,M,)/ 
[ l+yid?(X,M;)] for A; of (7.65) from (2.160), yid?(X,Mi) > -1 must be satisfied 
for A; to be positive definite. A simple compromise to overcome this incon- 
sistency is to select a number slightly larger than -1 for yid?(X,Mj). This 
makes a1 
( X )  - a2(X) small, although not zero. This selection of the kernel 
covariance was tested in the following experiment. 
Experiment 8: Estimation of the Parzen error, H 
Data: I-I,1-41, I-A (Normal, n = 8) 
Sample size: N I = N 2  = 100 (Design) 
N I  = N 2  = 1000 (Test) 
Kernel: Normal, A; of (7.65), yid! = -0.8 
Kernel size: I' = 0.6-2.4 
Threshold: t = 0 
Results: Fig. 7-1 1 
The optimal kernels given in (7.65) were scaled to satisfy IAi I = I C j  I, allow- 
ing direct comparison with the results obtained using the more conventional 
kernel A; =E; (also shown in Fig. 7-1 1). The results for Data 1-41 and I-A 
indicate that although the estimates seem less stable at smaller values of 1', as I' 
grows the results using (7.65) remain close to the Bayes error while the results 
using A; = C j  degrade rapidly. This implies that the I" and r4 terms of (7.50) 
and (7.51) have been effectively reduced. Note that for Data 1-1 (Fig. 7-1 l(a)), 
the distributions were chosen so that aI(X) 
= a , ( X )  on the Bayes decision 
boundary. As a result the I" and r4 terms of (7.50) and (7.51) are already 
zero. and no improvement is observed by changing the kernel. These experi- 
mental results indicate the potential importance of the kernel covariance in 
designing Parzen classifiers. 

338 
20. 
Introduction to Statistical Pattern Recognition 
- 
x Aiof (7.65) 
A Ai = X i  
(a) Data 1-1 
(b) Data 1-41 
(c) Data I-A 

7 Nonparametric Classification and Error Estimation 
339 
Estimated kernel covariance: So far, we have assumed that the class 
covariance matrices are known and given. However, is practice, these covari- 
ance matrices are unknown and must be estimated from a finite number of 
samples. This may lead to an optimistically biased error estimate. For exam- 
ple, in Data RADAR, upper and lower bounds of the Bayes error are estimated 
by the L and R methods. If 720 samples per class are used in this operation 
with the class covariance matrices estimated from 8800 samples per class, the 
resulting upper and lower bounds are 17.8% and 16.2% respectively. On the 
other hand, if the same 720 samples are used to estimate the covariance 
matrices, bounds of 8.4% and 7.1% result. These bounds are further lowered 
to 5.2% and 3.8%, when 360 samples per class are used in both the error esti- 
mation procedure and the covariance estimation. These results demonstrate 
that the upper bound of the Bayes error in the L method may be severely 
biased. Thus, the estimate may no longer give the upper bound of the Bayes 
error, if the class covariances are estimated from the same data used to form 
the error estimates. If possible, then, to avoid this bias, one should estimate 
the class covariances using a large number of independent samples. Once the 
covariances are estimated accurately, we may use a relatively small sample size 
for the nonparametric procedures to produce reliable results. However, if addi- 
tional samples for estimation of the covariance matrices are not available, in 
order to obtain reliable upper bounds on the Bayes error, one must use leave- 
one-out type estimates of the kernel covariances when forming the L error esti- 
mate. This implies the use of a different covariance matrix for each sample 
being tested. 
In order to show how the kernel covariance can be estimated by the L 
method, let us study the kernel function of (6.3). In Parzen error estimation, 
this kernel function is inserted into (7.2) and (7.3) to test a sample Xi') from 
o, 
in the L method. Using A; = C j ,  which is a good choice in many applica- 
tions, we need to compute I C j  I and d?(Xi!',Xy)) = (Xil)-X(j))T 
J 
E;' (Xi!)-X'j)). 
J 
When the covariance matrix C j  is not known and needs to be estimated from 
;y(Xl'),Xy)) = (X~."-Xy))' Xj (Xi.')-X?)). 
The L type estimate of the kernel 
covariance means that, when X i ' )  is tested, X i ' )  must be excluded from the 
sample set used to estimate 1'. Letting Elk be the resulting estimate, C, and 
d; now must be replaced by ilk 
and dlk, while & and d2 are kept unchanged. 
When the sample covariance of (5.9) is used, I Z I P  I and dlk can be easily 
,. 
X"' , , . . , ,Xb,), Cj is replaced by its estimate, X i ,  and subsequently & by 
. 
,.-I 
,. 
-7 
- 2  
- 2  
.. 
- 2  


7 Nonparametric Classification and Error Estimation 
Data 
Bayes 
Covariance 
Leave-one-out 
c r ~  Resubstitution 
Error(%) 
Used 
Error(%) 
(%) 
Error(%) 
1-1 
10 
True 
11.0 
I .8 
6.4 
Estimated 
12.6 
2.6 
5.8 
1-41 
9 
True 
10.6 
2.9 
4.8 
Estimated 
11.0 
3.2 
4.5 
I-A 
1.9 
True 
2.0 
1.2 
1.1 
Estimated 
2.3 
0.9 
0.8 
34 1 
OR 
(%) 
1.3 
1.3 
1 .o 
1.3 
0.9 
0.6 
TABLE 7-3 
Case 
Cov. 
Nco,, 
N 
Leave-one-out 
Resubstitution 
- 
used 
(%I 
(%) 
1 
5; 
8800 
720 
17.8 
16.2 
8800 
360 
19.5 
15.6 
2 
&(L) 
720 
720 
23.3 
7.1 
360 
360 
27.4 
3.8 
3 
E, 
720 
720 
8.4 
7.1 
360 
360 
5.2 
3.8 
,. 
EFFECT OF ESTIMATED COVARIANCE MATRICES 
FOR THE PARZEN APPROACH 
(a) Standard data sets 
(b) Data RADAR 
used shows that the estimated covariance always gives a larger upper bound 
and smaller lower bound than the true covariance, although the differences are 
small. The standard deviations of 10 trials, 0, and o,, are also presented in 
Table 7-3. 

342 
Introduction to Statistical Pattern Recognition 
Experiment 10: Estimation of the Parzen error, L and R 
Data: RADAR (Real data, n = 66, E* = unknown) 
Sample size: N I = N 2  = 720, 360 
No. of trials: T =  1 
Kernel: 
A 
A; = Zj for N,,,. = 8800 (Case 1) 
A; = Zjk for N,,,, = 720, 360 (Case 2) 
A; = Z j  for N,,, = 720, 360 (Case 3) 
Nc0, - No. of samples to estimate Z 
A 
A 
Kernel size: I' = 9.0 
Threshold: Option 4 
Results: Table 7-3(b) [ 141 
This experiment demonstrates more clearly the importance of the selection of 
the kernel covariance. Note that even as the sample size used to estimate the 
covariance matrices becomes small, the L error rates continue to provide rea- 
sonable and consistent bounds in Case 2 of Table 7-3(b). This is in contrast to 
the results given in Case 3 in which the estimated covariances are blindly used 
without employing the L type covariance. As expected, the bounds become 
worse as the sample sizes decrease. 
Effect of m: Finally, in kernel selection, we need to decide which is 
better, a normal or uniform kernel. More generally, we may address the selec- 
tion of m in (6.3). The results using normal kernels (m = 1) are shown in Fig. 
7-12, in which the upper bounds of the Bayes error are observed to be excel- 
lent, but the lower bounds seem much too conservative. This tends to indicate 
that the normal kernel function places too much weight on the sample being 
tested in the R error estimate, Hence, one possible approach to improving the 
lower bound of the Parzen estimate is to use a non-normal kernel function 
which places less weight on the test sample and more weight on the neighbor- 
ing samples. The uniform kernel function, with constant value inside a 
specified region, is one such kernel function. However, if a uniform kernel 
function is employed, one must decide which decision be made when the den- 
sity estimates from the two classes are equal, making the Parzen procedure 
even more complex. A smooth transition from a normal kernel to a uniform 
kernel may be obtained by using the kernel function of (6.3) and changing m. 
The parameter m determines the rate at which the kernel function drops off. 
For m = 1, (6.3) reduces to a simple normal kernel. As m becomes large, (6.3) 

7 Nonparametric Classification and Error Estimation 
343 
o m-1 
A m-2 
rn-4 
E (Yo) 
(c) Data 1- A 
4.0 
3.0 
1.0 - 
I 
I b r  
1 .Q 
2.0 
3.0 
Fig. 7-12 Effect of kernel shape on Parzen classification. 

344 
Introduction to Statistical Pattern Recognition 
approaches a uniform (hyperelliptical) kernel, always with a smooth roll-off 
(for finite m), and always with covariance r2Ai. Using this kernel allows us to 
use kernel functions close to the uniform kernel, without having to worry about 
the problem of equal density estimates. 
(normal kernel), 2, and 4. 
Figure 7-12 shows the performance of the Parzen estimates with m = 1 
Experiment 11: Estimation of the Parzen error, L and R 
Same as Experiment 4 except 
Kernel: (6.3), m = 1, 2, 4 
Threshold: Option 4 
Results: Fig. 7-12 [12] 
In all cases, using higher values of rn (more uniform-like kernel functions) does 
improve the lower bound while having little effect on the upper bounds of the 
error. 
Estimation of the Bayes Error in the Parzen Approach 
So far, we have discussed how to obtain the upper and lower bounds of 
the Bayes error. In this section, we address the estimation of the Bayes error 
itself. From (7.52), we can write the expected error rate in terms of r and N as 
(7.70) 
Here, the constants u l ,  u2, a3, and the desired value of E* are unknown and 
must be determined experimentally. An estimate of E* may be obtained by 
observing the Parzen error rate, E, for a variety of values of r, and finding the 
set of constants which best fit the experimental results. Any data fitting tech- 
nique could be used. However, the linear least-square approach is straightfor- 
ward and easy to implement. This approach has several intuitive advantages 
over the procedure of accepting the lowest error rate over the various values of 
r. First, it provides a direct estimate of E* rather than an upper bound on the 
value. Another advantage is that this procedure provides a means of combin- 
ing the observed error rates for a variety of values of r. Hence, we may be 
utilizing certain information concerning the higher order properties of the dis- 
tributions which is ignored by the previous procedures. 
A 

7 Nonparametric Classification and Error Estimation 
345 
As mentioned earlier, it is reasonable to expect that all four of the con- 
stants in (7.70) are positive, since the observed error must remain above &* for 
any value of r. In order to ensure stability in the estimate of E*, it is advisable 
to restrict the constants to positive values during the curve fit procedure. 
The result of this procedure is illustrated in Fig. 7-13. 
Experiment 12: Estimation of the Bayes error, L 
Same as Experiment 4 except 
Data: I-A (Normal, n = 8, E* = 1.9%) 
Results: Fig. 7-13 
1.01 
I 
I 
) r  
0’ 
1 .o 
2.0 
Fig. 7-13 Estimation of the Bayes error. 
The best fit of the form given in (7.70) is drawn as a solid line. The resulting 
estimate of the Bayes error was 1.96% which is extremely close to the true 
Bayes error of 1.9%. Note the closeness of the fit, indicating that the observed 
error rates are in fact following the trends predicted. 
In order for (7.70) to be valid, the decision threshold t should be selected 
so that the estimated Bayes decision boundary is relatively close to the actual 
Bayes decision boundary. As shown in the threshold adjustment section, the 
optimal value of t may be highly dependent on the value of I-, particularly if 
the covariance determinants for the two classes are very different. Generally, it 

346 
Introduction to Statistical Pattern Recognition 
is advisable to evaluate the Parzen classifier for a variety of values of f, and 
then apply the curve fit procedure for each value of t. This results in a negligi- 
ble increase of the computational burden, since the bulk of the time is spent 
calculating the density estimates, and only a very small percentage comparing 
the estimates to the thresholds. 
Experiment 13: Estimation of the Bayes error, L 
Same as Experiment 12 except 
Data: 1-1, 1-41, I-A (Normal, n = 8) 
Threshold: t of (7.56), I' = 0.6-2.4 in steps of 0.2 
Results: Table 7-4 
TABLE 7-4 
ESTIMATION OF THE BAYES ERROR FOR VARIOUS VALUES OF r 
Date I-I 
(E* = 10%) 
t 
E*(%) 
0 
11.0 
Data 1-41 
(E* = 9%) 
t 
E*(%) 
- 1.47 
3.3 
-2.16 
7.8 
-2.77 
8.9 
-3.27 
9.8 
-3.67 
9.5 
-4.00 
9.7 
-4.24 
9.1 
-4.43 
11.5 
-4.59 
10.0 
-4.72 
9.1 
Data I-A 
(E* = 1.9%) 
t 
E (%) 
r *  
-.252 
1.99 
-.373 
1.93 
-.477 
1.93 
-.563 
1.99 
-.632 
2.05 
-.686 
2.05 
-.729 
2.07 
-.764 
2.08 
-.791 
2.1 1 
-.813 
2.12 
For Data 1-1, t = 0 is the optimal choice for any value of r, and hence only one 
error estimate (E = 11.0%) is listed. Comparison of the estimated error rates 
in Table 7-4 with the true error rates indicates that the procedure is providing 
reasonable estimates of the Bayes error for a wide range of decision thresholds. 
A particularly bad estimate is obtained for Data 1-41 with t > -2. Data 1-41 is 
A *  

7 Nonparametric Classification and Error Estimation 
347 
the case in which the covariance determinants of the two classes are very dif- 
ferent. Also, it is observed in Fig. 7-7(b) that the error curve increases very 
quickly. With t ranging from -3 to -5, the error curve becomes more like the 
one of Fig. 7-7(c), having a down-slope, a minimum at a higher I-, and a slower 
up-slope. This means that smaller biases exist in a wider range of I' and that 
(7.70) fits the actual errors more accurately. 
W N  Approach 
So far, we have discussed error estimation based on the Parzen density 
estimate. Similarly, we can develop the argument using the kNN density esti- 
mation. These two approaches are closely related in all aspects of the error 
estimation problem, and give similar results. In this section, the kNN approach 
will be presented. However, in order to avoid lengthy duplication, our discus- 
sion will be limited. 
Bias of 
the 
kNN 
error: 
When 
the 
kNN 
density 
estimate 
f;i(X) = (k-l)/Nv;(X) is used, E (  pi(X)] and MSE { ii(X)] are available in 
(6.91) and (6.94) respectively. Therefore, substituting E { Api(X) 1 = E { &(X) 1 
-p;(X), and E{Ap? ( X ) )  = E{[pi(X) 
- p i ( X ) I 2 )  = MSE {pj(X)] 
into (7.46) 
and (7.47), 
2 
k 
k 
N 
E(Ah2(X)] G- + Ar2 - 2At (y2 - y,) (-)2'n 
(7.71) 
(7.72) 
where 
(7.73) 
1 
2 
y; ( X )  = - a;(X) r2 (v;p;)-2/" . 
Substituting (7.71) and (7.72) into (7.45) and carring out the integration 

348 
Introduction to Statistical Pattern Recognition 
1 
k 
k 
k 
N 
N 
E(A&} 3
1
 - + b2 (-)un + b, (-)4'n 
, 
(7.74) 
where the hi's are constant. 
Several conclusions can be drawn from (7.74). In order to make 
E ( A & )  4 0, two conditions, k += and k / N  + 0 as N +=. must be 
satisfied. These conditions are identical to the conditions required for asymp- 
totic consistency of the kNN density estimate [15]. Since klN Epivi, klN is 
proportional to r", and therefore (klN)"" is proportional to r 2 .  That is, 
E { A & }  Z b , / k  + b; r2 + b; r4 where b; and b; are constants different from b2 
and h3. We may compare this with (7.52), E { A&] of the Parzen approach. In 
both cases, we have terms with r2 and r4 which are generated by the bias of 
the density estimate. However, the bias of the kNN error does not have an 
r-"/N term. This is due to the fact that (7.74) describes the behavior of the 
kNN error only for large values of k. A series of approximations based on N 
>> k >> 1 was applied to obtain E (&(X)) 
and MSE { &X)). If the analysis of 
the kNN error for small values of k is needed, more complicated expressions 
must be used. In addition to the r2 and r4 terms, the kNN bias has a constant 
term bilk, which the Parzen bias does not have. This may be related to the 
fact that the voting kNN error with a finite k does not converge to the Bayes 
error, even under asymptotic conditions. This term can be reduced only by 
increasing k. 
Effect of parameters: In the Parzen approach, the most effective way to 
reduce the bias of the error estimate is to adjust the threshold properly. Also, 
in order to assure that the L method gives an upper bound of the Bayes error, 
the kernel covariance matrix must be estimated either from a large number of 
independent samples or by an L type estimation technique. 
In terms of threshold selection, the method developed for the Parzen 
approach may be directly applied to the kNN approach. The kNN density esti- 
mates are known to be biased when the size of the design set is limited, and, 
by choosing an appropriate threshold, one might hope to reduce or eliminate 
the effect of that bias when classification is performed. There are no usable 
expressions for t even in the normal case. However, each of the non-normal 
methods for threshold selection (Options 2, 3, and 4) are directly applicable to 
the kNN problem. 

7 Nonparametric Classification and Error Estimation 
349 
One comment is in order regarding the application of Option 4 to kNN 
estimation. In Step 2 of Option 4 of the Parzen case, it is fairly simple to 
remove the effect of X f )  (the test sample) from the density estimates of all the 
other samples using (7.58). There is no analogous simple modification in the 
kNN case. In order to remove the effect of X f )  from all other density esti- 
mates, one must remove Xf’ from the table of nearest neighbors, rearrange the 
NN table, and recalculate all of the density estimates. This procedure would 
have to be repeated to test each of the samples in the design set, resulting in a 
fairly drastic increase in computation time. In practice, modifying each of the 
density estimates to remove the effect of X f )  is not nearly as important as 
finding the threshold by minimizing the error among the remaining N ,  + Nz-1 
samples. That is, modifying the estimates of the likelihood ratios in Step 2 is 
not necessary to get reliable results - we do it in the Parzen case primarily 
because it is easy. Thus for kNN estimation, Step 2 of Option 4 involves 
finding and sorting L(X:)) for all samples X y )  # X f ) ,  finding the value of t 
which minimizes the error among these Nl+N2-1 samples, and using this 
value oft to classify xj,?. 
Figure 7-14 shows the results of applying Option 4 to the kNN estima- 
tion problem. For comparison, the results obtained using t = 0 are also shown. 
Experiment 14: Estimation of the kNN error, L and R 
Same as Experiment 4, except 
Metric: A I = C 
I and A = C2 (Instead of kernel) 
No. of neighbors: k = 1-30 (Instead of kernel size) 
Threshold: Option 4 and t = 0 
Results: Fig. 7-14 [12] 
As in the Parzen case, the threshold plays its most significant role when the 
covariances of the data are different, and particularly when the covariance 
determinants are different. In Data I-I, the bias of the density estimates for o, 
and w2 are nearly equal near the Bayes decision boundary, and hence good 
results are obtained without adjusting the threshold. However, for Data I-41 
and I-A, the kNN errors are heavily biased and unusable without adjusting the 
threshold. 

350 
Introduction to Statistical Pattern Recognition 
20. f' 
0 t = O  
A t FOUND USING OPTION 4 
5 
10 
15 
20 
25 
30 
(b) Data 1-41 
k 
E 
8.0 
6.0 
4.0 
2.0 
(c) Data I-A 
I 
I 
I 
I 
I 
' 
5 
10 
15 
20 
25 
30 
b
k
 
Fig. 7-14 Effect of threshold on kNN classification. 

7 Nonparametric Classification and Error Estimation 
35 1 
As far as the L type estimation of the kernel covariance matrix is con- 
cerned, the same procedure used in the Parzen approach can be applied to the 
kNN approach. 
Experiment 15: Estimation of the kNN error, L and R 
Same as Experiment 9, except 
No. of neighbors: k = 10 
Results: Table 7-5(a) [ 141 
Experiment 16: Estimation of the kNN error, L and R 
Same as Experiment 10, except 
No. of neighbors: k = 10 
Results: Table 7-5(b) [ 141 
The conclusions from these experiments are similar to the ones from 
Experiments 9 and 10. 
7.5 Miscellaneous Topics in the kNN Approach 
In this section, miscellaneous topics related to the kNN approach, which 
were not discussed in the previous sections, will be studied. They are the error 
control problem (the Neyman-Pearson and minimax tests), data display, pro- 
cedures to save computational time, and the procedure to reduce the 
classification error. 
Two-Dimensional Data Display 
Error control: So far, we have discussed the Bayes classifier for 
minimum error by using the kNN approach. However, the discussion can be 
easily extended to other hypothesis tests such as the Bayes classifier for 
minimum cost, the Neyman-Pearson 
and minimax tests, as long as the 
volumetric kNN is used. As (7.5) indicates, in the likelihood ratio test of the 
kNN approach, two distances, d (Xil,)NN,X) 
and d2(XithN,X), 
are measured, and 
their ratio is compared with a threshold as 
where t must be determined according to which test is performed. For exam- 
ple, t is selected to achieve 
( E ~ :  a preassigned value) for the Neyman- 
Pearson test, and 
= E~ for the minimax test. 
= 

352 
Data 
I-I 
1-4 
I-A 
Introduction to Statistical Pattern Recognition 
Bayes 
Covariance 
Leave-one-out 
a~ 
Resubstitution 
oR 
Error (%) 
Used 
Error(%) 
(%) 
Error(%) 
(%) 
IO 
True 
11.9 
2.2 
8.7 
1.8 
Estimated 
13.6 
3.2 
8.2 
I .8 
9 
True 
13.6 
2.8 
9.2 
2.6 
Estimated 
17.7 
5.0 
9.0 
2. I 
1.9 
True 
2.7 
I .o 
1.4 
0.7 
Estimated 
3.2 
I .3 
I .3 
0.6 
TABLE 7-5 
Case 
1 
2 
3 
EFFECT OF ESTIMATED COVARIANCE MATRICES 
FOR THE lONN APPROACH 
Cov 
N,,,. 
N 
Leave-one-out 
Resubstitution 
used 
(%) 
(%) 
!& 
8800 
720 
22.5 
17.8 
8800 
360 
22.1 
18.6 
&(L) 
720 
720 
24.3 
10.0 
i; 
720 
720 
11.5 
10.0 
360 
360 
29.5 
6.3 
360 
360 
7.9 
6.3 
(a) Standard data sets 
(b) Data RADAR 
Equation (7.75) also shows how other parameters, k;, N;, 
and I C; I, affect 
the threshold. For example, when two sample sizes, N I and N 2 ,  are different, 
we cannot compare two distances simply as d l  ><d2 even if k l  = k 2 ,  
ICI I = I C2 I, and t = 0. Instead, we need either to set a threshold as 
(d2Idl) 5 (N IIN2)1/n, 
or weight two distances differently as N4'"d2 5 Nl/"dl. 
The above argument suggests that thresholding the distance ratio is 
equivalent to weighting two distances differently. Then, the concept could be 

7 Nonparametric Classification and Error Estimation 
353 
applied to the voting kNN approach too. If we adopt aiA (a I t u2) as the 
metric to measure the distance to mi-neighbors in the voting kNN procedure, 
we can control the decision threshold by adjusting the ratio of a l  and a2. 
Furthermore, using aiAi (a I t a2, 
A # A2), we could make the voting kNN 
virtually equivalent to the volumetric kNN. In this case, Ai could be Z j  or a 
more complex matrix like (7.65), and the ratio of a l  and a2 still determines 
the threshold. 
Data display: Equation (7.5) also suggests that we may plot data using 
y I = nlnd, (X) and y2 = nlnd2(X) as the x- and y-axes, in the same way as we 
selected y, = (X - Mj)*Z;'(X - Mi) 
for the parametric case in Fig. 4-9 [ 161. In 
fact, nlndi(X) is the nonparametric version of the normalized distance 
(X-Mj)TC;l (X-M,), as the following comparison shows: 
1 
1 
n 
2 
2 
2 
-lnpj(x) = -(x-M~)*z;'(x-M;) + { -1n IC, I + -1n2n:) 
for a normal distribution , 
(7.76) 
A 
Nico I Zj I 
-lnpj(X) = nlnd,(X) + 
for the kNN density estimate , 
(7.77) 
where co is the constant relating vi and d, as in (B.1). Note that 
&X) = (kj-l)/Nico I C, 
I "2dl is used in (7.77). Using two new variables, 
y I = nlndl ( X )  and y2 = nlnd2(X), the Bayes classifier becomes a 45 'line as 
nlnd2(X) Snlndl(X) - 
(7.78) 
where 1. ) gives the y-cross point. 
In order to show what the display of data looks like, the following exper- 
iment was conducted. 
Experiment 17: Display of data 
Data: I-A (Normal, n = 8, E* = 1.9%) 
Sample size: N I = N 2  = 100 (L method) 
No. of neighbors: k l  = k2 = 5 

354 
Introduction to Statistical Pattern Recognition 
0 
5 
10 
15 
20 
X 
Fig. 7-15 A nonparametric data display with five NN distances. 
No. of trials: z = 1 
Result: Fig. 7-15 
This experiment is the same as the one of Fig. 7-14(c) (Experiment 14), except 
that the number of trial is one for this experiment while Fig. 7-14(c) shows the 
average of 10 trials. The solid line shows the 45 "line with the y-cross point of 
(1/2)ln I C, I / I C2 I where Z j  is the given oj-covariance matrix. From (7.78), 
this is the threshold value for t = 0. The performance of this classifier 
(71200 = 3.5%) corresponds to the L-curve without threshold adjustment in Fig. 
7-14(c), which is about 5.5% for k = 5. The dotted 45 line was selected by 
using human judgement, minimizing the error among the existing samples. 
The resulting error is 2/200 = 1%. Since the distance to the 5th NN for each 
class is measured according to (7.7), this error is the L error. However, the 
threshold is fine-tuned to the existing samples (Option 2). Thus, the error is 

7 Nonparametric Classification and Error Estimation 
355 
expected to be much smaller than the L-curve of Fig. 7-14(c) (5.5% at k = 5), 
but larger than the R-curve (1.3% at k = 5). Note again that the curves of Fig. 
7-14(c) were obtained as the 10 trial average by using the threshold of Option 
4, while the dotted line of Fig. 7-15 was determined by human judgement for 
one trial. 
In order to apply the Neyman-Pearson test with 
= 2% for example, we 
maintain the 45 .,slope of the line, and shift it until we find 2 misclassified 02- 
samples out of 100. 
Constant risk contours: It is frequently desirable to have risk informa- 
tion about a point in a data display. For the two-class case, the risk at X, r (X), 
is given by 
Substituting pj(X) = (k,-l)/Njco I Cj I li2dy(X), and taking logarithms, 
(k I -1)Nz I z2 I 
+ l
n
~
}
 
+ 1n'0 
nlnd2(X) = n lnd I (X) - In I (kZ--l)NI 1x1 I 
P2 
1 -r (X) 
(7.80) 
Thus, for a given r-(X), a contour can be drawn on the display. The resulting 
contour is a 45' line shifted by lnr(X)/[l-r(X)] 
[16]. Similarly, for 
P I p I ( X )  > P2p2(X), the numerator of (7.79) is replaced by P2p2(X), and 
(7.80) is modified to 
(k I -1)lVz I x* 
I 
nlnd2(X) = n lnd I (X) - In 
+ 
1 - In- 
I (k2-1)N1 1x1 I 
p2 
1 -r- (X) 
(7.8 1) 
Comparison of (7.80) and (7.81) indicates that the constant risk lines are sym- 
metric about the Bayes decision line where r-(X) = 0.5. As r ( X )  is decreased, 
the constant risk lines move farther from the Bayes decision line. This result 
indicates that points mapped near the Bayes decision line on the display do in 
fact have a high degree of risk associated with them, while points mapped far 
from the Bayes decision line have a low degree of risk. This desirable pro- 

356 
Introduction to Statistical Pattern Recognition 
perty will be true regardless of the underlying distribution. Table 7-6 shows 
the amounts of shift for various values of r(X). However, it must be noted 
that these risk lines should not be drawn around the theoretical Bayes risk line 
(the solid line of Fig. 7-15). The kNN density estimates and subsequently the 
estimate of r(X) are heavily biased as discussed in the previous sections. In 
order to compensate these biases, the threshold terms of (7.80) and (7.81) must 
be adjusted and will differ from the theoretical values indicated in (. ). Further 
shift due to lnr (X)l( 1-r (X)) must start from the adjusted threshold. 
TABLE 7-6 
SHIFT OF THRESHOLD DUE TO r 
r 
0.5 
0.4 
0.3 
0.2 
0.1 
kAt 
0 
0.405 
0.847 
1.386 
2.197 
These constant risk lines allow the analyst to identify samples in a reject 
region easily [17-181. For a given reject threshold z, the reject region on the 
display is the area between two 45 lines specified by r(X) = 2, in which 
r ( X )  > z is satisfied and accordingly samples are rejected. 
Grouped error estimate: An obvious method of error estimation in 
display is to count the number of ol- 
and w2-samples in the 02- and wI- 
regions, respectively. Another possible method is to read r(Xj) for each Xi, 
and to compute the sample mean as 
A 
(7.82) 
because the Bayes error is expressed by E* = E ( r ( X ) ] .  This estimate is called 
the grouped estimate [19-201. The randomness of E comes from two sources: 
one from the estimation of r, r, and the other from Xi. When the conventional 
error-counting process is used, we design a classifier by estimating the density 
A 

7 Nonparametric Classification and Error Estimation 
357 
functions using design samples (design phase), and count the number of 
misclassified test samples (test phase). As discussed in Chapter 5, with a finite 
number of samples, the estimated error is biased due to the design samples and 
has a variance due to the test samples. In the grouped estimate, estimating I’ 
corresponds to the design phase and computing the sample mean of r ( X i )  
corresponds to the test phase. The performance of the grouped estimate has 
not been fully studied in comparison with the error-counting result. However, 
if the risk function I’ (X) is given, the test phase of the grouped estimate has the 
following advantages. 
A 
(1) We can use test samples without knowing their true class assign- 
ments. This property could be useful, when a classifier is tested in the field. 
(2) The variance due to this test process is less than a half of the vari- 
ance due to error-counting. 
In order to prove that the second property is true, let us compute the 
variance of (7.82) with the assumption that I’ (X) is given and E { r ( X )  1 = E .  
Since the Xi’s are independent and identically distributed, 
1 
N 
1 
N 
1 
1 
N 
2 
~ a r ( L 1  
= -~ar{r(~)l 
= - [ E - E ~ - E ( I ’ ( X ) [ I  
5 - [ E  - E* - -E{r(X) 
= -(E 
- 2&2), 
1 
2N 
- I’(W1 I 1  
1 
(7.83) 
where r(1-r) 2 1’/2 for 0 I r 50.5 is used to obtain the inequality. Note from 
(5.49) that the variance of error-counting is E(I-E)/N, which is larger than 
twice (7.83). When the design phase is included, we must add the bias and 
variance of r to evaluate the total performance of the grouped estimate. Also, 
note that the bias of 
should be removed by the threshold adjustment, as dis- 
cussed in the previous section. That is, r ( X )  must be estimated by solving 
(7.80) or (7.81) for r ( X )  with the adjusted ( ‘ 1  term. 
A 

358 
Introduction to Statistical Pattern Recognition 
Edited Nearest Neighbor 
Edited kNN: As seen in Fig. 7-3 for the voting kNN approach, the ai- 
samples in the a,-region (i # J) could become close neighbors of some of the 
a,-samples, and this produces extra errors. This is the reason why the kNN 
error is larger than the Bayes error. These extra errors could be reduced by 
removing ai-samples in the oj-region (samples 1, 3, 5 and 6 of Fig. 7-3) from 
the design sample set. In practice, since the exact location of the Bayes boun- 
dary is never known, the above operation is replaced by removing the 
misclassified samples (1, 2, 3, 4, 5 and 6 of Fig. 7-3) by the kNN classification 
[21]. The resulting design set is called the edited set. Test samples are 
classified by the kNN’s from the edited set. This algorithm is called the edited 
kNN procedure. It should be noted, however, that some of wi-samples in the 
oj-region are correctly classified and not removed, and some of oj-samples in 
the ai-region are misclassified and removed. Therefore, the resulting error is 
not the same as the Bayes error. 
Asymptotic analysis: In order to analyze the edited kNN procedure, let 
us study the simplest case of the asymptotic NN. In the original sample set, 
the populations of aI 
and o, given X are represented by a posteriori probabili- 
ties q I ( X )  and q2(X). Applying the voting NN classification to the oi-samples, 
q?(X) (qitX)qj(XNN)) are correctly classified, and qi(X)qj(X) (qitX)qitXNN)) 
0’ f i )  are misclassified. Therefore, keeping only correctly classified samples, 
a posteriori probabilities in the edited set, q ; ( X ) ,  become 
(7.84) 
Samples from the original set are classified according to the class of the NN 
from the edited set. The resulting error is 

7 Nonparametric Classification and Error Estimation 
359 
(7.85) 
where qi(X,,) Eqj(X) is used for the asymptotic analysis. The last line of 
(7.85) is the expression of r ( X )  in terms of 4 I ( X ) q 2 ( X ) .  
It is easy to show that (7.85) is bounded by the N N  risk from the upper 
side and the Bayes risk from the lower side in the range of 0 I 
4 I q2 I 
0.25 as 
The proof is left for the reader. Also, (7.85) can be generalized to the case 
where the k I NN is used for removing samples and the k2NN for testing. When 
both k l  and k2 are odd, the resulting error becomes larger than the Bayes error. 
When k2 is even, the resulting error becomes smaller than the Bayes error. 
Also, the edited N N  procedure can be applied repeatedly to remove the 
o,-samples in the o,-region. The asymptotic analysis also can be carried out 
by applying (7.85) repeatedly [22]. Figure 7-16 shows the results of the 
repeated edited NN procedure. 
Reduction of Computation Time 
One of the severe drawbacks of any nonparametric approach is that it 
requires a large amount of computation time. For the kNN approach, this is 
due to the fact that, in order to find the kNN’s, we must compute the distances 
to all samples. The same is true for the Parzen approach. Because of this 
computational burden, nonparametric approaches are not popular as a classifier 
operated on-line. In the off-line operation of the Bayes error estimation, each 
sample must be tested by computing the distances to other samples. This 
means that all possible pairwise distances among samples must be computed. 
This becomes a burden to a computer, slows down the turn-around time, and 
limits the number of samples we can use even when more samples are 
available. 

360 
Introduction to Statistical Pattern Recognition 
r 
4 
No Editing- 
/ 
I // 
0 
.05 
.IO 
.I 
5 
.20 
.25 
Fig. 7-16 Performance of repeated edited NN operations. 
In this section, we touch briefly on some past efforts to overcome the 
above difficulty as follows. 
Condensed NN: As seen in Fig. 7-3 for the voting kNN approach, the 
samples near the Bayes decision boundary are crucial to the kNN decision, but 
the samples far from the boundary do not affect the decision. Therefore, a sys- 
tematic removal of these ineffective samples helps to reduce both computation 
time and storage requirements. This procedure is called the condensed kNN 
decision rule [23]. 
The risk value, r ( X ) ,  of each sample can be used as an indicator of how 
close the sample is located to the boundary. Therefore, we may set a threshold 
z, and remove samples with I' (X) < z. In addition, we had better remove all 
misclassified samples regardless of the value of r(X), in order to avoid extra 
errors as was discussed in the edited kNN procedure. Since the effect of 
removing samples on the kNN error is hard to predict, it is suggested to clas- 
sify test samples based on the condensed design set and confirm that the result- 
ing error is close to the one based on the original design set. 
Branch and bound procedure: The kNN computation time could be 
reduced significantly by applying the brunch and bound technique, if design 

7 Nonparametric Classification and Error Estimation 
36 1 
samples can be hierarchically decomposed to subsets, sub-subsets and so on 
[24]. In order to describe the procedure, let us set up a tree structure as in Fig. 
7-17 where each node represents a subset, Sk. Each subset is decomposed into 
Fig. 7-17 A solution tree in the branch and bound procedure. 
several other subsets, and at the bottom of the tree each node represents an 
individual sample. Each subset (or node) is characterized by the mean vector 
of the samples in Sp, Mk, and the furthest distance from Mk to a sample in Sk, 
dk . 
When the NN sample of an unknown X is sought, the search begins from 
the leftmost branch. After comparing the distances from X to X 8 ,  X I 3 ,  X,,, 
and Xl0, X 2 s  is found as the closest one to X .  The computer back-tracks the 
tree from S 2  to S I  and then moves to S 3 .  However, before testing the 
members of S3, 
it is tested whether or not the following inequality is satisfied. 
d ( X , M x )  > d(X.Xz5) + d3 . 
(7.87) 
If the inequality is satisfied, there is no possibility that the distance between X 
and any member of S 3  is smaller than d ( X , X l 5 )  [see Fig. 7-18]. Therefore, the 
search moves to S4 without testing the members of S3. 
The procedure works well in a low-dimensional space. An experimental 
result for a uniform distribution in a two-dimensional space shows that only 46 
distance computations are needed to find the NN among 1000 samples. In this 

362 
Introduction to Statistical Pattern Recognition 
Fig. 7-18 A criterion to eliminate a group of samples. 
experiment, the tree consists of 3 levels with each node decomposed to 3 
nodes. At the bottom of the tree, there are 27 subsets containing 1000 sam- 
ples. However, for an 8-dimensional uniform distribution, 45 1 distance com- 
putations are needed to find the NN from 3000 samples. The tree is formed 
with 4 levels and 4 decomposition, which yields 256 subsets at the bottom 
housing 3000 samples. As discussed in Chapter 6, all pairwise distances 
among samples become close, as the dimensionality gets high. Therefore, the 
effectiveness of (7.87) to eliminate subsets diminishes, and only a smaller 
number of subsets are rejected by satisfying (7.87). 
Another problem of this method is how to divide samples into subsets. 
We will discuss this problem, which is called clustering, in Chapter 1 1. Again, 
finding clusters becomes more difficult, as the dimensionality goes up. 
Computer Projects 
1. 
2. 
3. 
4. 
5. 
6. 
Repeat Experiment 3 for Data I-A. Use (a) I and (b) (I +A)/2 as the 
metric. 
Repeat Experiment 5. 
Repeat Experiment 6. 
Repeat Experiment 8. 
Repeat Experiment 9. 
Repeat Experiment 1 1. 

7 Nonparametric Classification and Error Estimation 
363 
7. 
8. 
Repeat Experiment 14. 
9. 
Repeat Experiment 17. 
Repeat Experiments 12 and 13. 
Problems 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Derive the asymptotic 3NN and 4NN errors for the voting kNN pro- 
cedure, and confirm that the results can also be obtained from (7.15) and 
(7.16). 
Two one-dimensional distributions are uniform in [0,2] for wI and [ 1,4] 
for 02. Assuming P I  = P 2  = 0.5, compute the asymptotic kNN errors 
for k = 1, 2, 3, and 4 in the voting kNN procedure, and compare them 
with the Bayes error. 
In Problem 2, {(x) is either 6/25 or 0 with the probability of 5/12 or 
7/12. With this much information, compute the asympotitic kNN errors 
for k = 1, 2, 3, and 4 and the Bayes error. The results must be identical 
to the ones of Problem 2. 
1
1
 
2
2
 
Prove that min[ql(X),q2(X)1 = - - --dl-4qI(X)q2(X). 
Assuming P I  = P 2  and N I = N 2  = N ,  the Parzen classifier is expressed 
by 
Prove that the leave-one-out error is larger than the resubstitution error. 
Assume K(O) >K(X). 
Express B I  of (7.36) in terms of M,,M2,CI, and Z2 for normal X. 
Derive the bias equation for the volumetric 2NN error for two-class prob- 
lems. 
Derive the bias equation for the volumetric NN error for multiclass prob- 
lems. 

364 
Introduction to Statistical Pattern Recognition 
9. 
The risk r ( X )  is assigned to either 0 or a, depending on whether the 
classes of X and its NN agree or not. The NN error is computed by 
E ( r ( X ) ) .  Find a which makes this NN error equal to the Bayes error for 
the uniform distribution of c(X) = q I (X)q*(X). 
10. 
The edited design samples are selected by using the NN voting pro- 
cedure, and test samples are classified by the voting 2NN procedure. 
Compute the asymptotic error of this operation. Prove that the resulting 
error is between the Bayes error and the 2NN error without the editing 
procedure. 
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
T. M. Cover and P. E. Hart, Nearest neighbor pattern classification, 
Trans. IEEE Inform. Theory, IT-13, pp. 21-27, 1967. 
T. M. Cover, Rates of convergence for nearest neighbor procedures, 
Proc. Hawaii Int. Conf. on System Sciences, pp. 413-415, 1968. 
T. J. Wagner, Convergence of the nearest neighbor rule, Trans. IEEE 
Inform. Theory, IT-17, pp. 566-571, 1971. 
J. Fritz, Distribution-free exponential error bounds for nearest neighbor 
pattern classification, Trans. IEEE Inform. Theory, IT-2 1, pp. 552-557, 
1975. 
C. J. Stone, Consistent nonparametric regression, Ann. Stat., 5 ,  pp. 595- 
645, 1977. 
L. Devroye, On the almost everywhere convergence of nonparametric 
regression function estimates, Ann. Stat., 9, pp. 13 10- 13 19, 198 1. 
L. Devroye, On the inequality of Cover and Hart in nearest neighbor 
discrimination, Trans. IEEE Pattern Anal. and Machine Intell., PAMI-3, 
pp. 75-78, 1981. 
K. Fukunaga and D. M. Hummels, Bias of nearest neighbor estimates, 
Trans. IEEE on Pattern Anal. and Machine Intell., PAMI-9, pp. 103-1 12, 
1987. 
R. D. Short and K. Fukunaga, The optimal distance measure for nearest 
neighbor classification, Trans. IEEE Inform. Theory, IT-27, pp. 622-627, 
1981. 

7 Nonparametric Classification and Error Estimation 
365 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
20. 
21. 
K. Fukunaga and T. E. Flick, An optimal global nearest neighbor metric, 
Trans. IEEE Pattern Anal. and Machine Intell., PAMI-6, pp. 314-318, 
1984. 
K. Fukunaga and T. E. Flick, The 2-NN rule for more accurate NN risk 
estimation, Trans. IEEE on Pattern Anal. and Machine Intell., PAMI-7, 
K. Fukunaga and D. H. Hummels, Bayes error estimation using Parzen 
and k-NN procedures, Trans. IEEE Pattern Anal. and Machine Intell., 
E. Parzen, On the estimation of a probability density function and the 
mode, Ann. Math. Stat., 33, pp. 1065-1076, 1962. 
K. Fukunaga and D. H. Hummels, Leave-one-out procedures for non- 
parametric error estimates, Trans. IEEE Pattern Anal. and Machine 
Intell., PAMI- 1 1, pp. 42 1-423, 1989. 
D. 0. Loftsgaarden and C. P. Quesenberry, A nonparametric estimate of 
a multivariate density function, Ann. Math. Stat., 36, pp. 1049-1051, 
1965. 
K. Fukunaga and J. M. Mantock, A Nonparametric two-dimensional 
display for classification, Trans. IEEE on Pattern Anal. and Machine 
Intell., PAMI-4, pp. 427-436, 1982. 
M. Hellman, The nearest neighbor classification rule with a reject option, 
Trans. IEEE Systems Sci. Cyhernet., SSC-6, pp. 179-185, 1970. 
P. A. Devijver, New error bounds with the nearest neighbor rule, Trans. 
IEEE Inform. Theory, IT-25, pp. 749-753, 1979. 
K. Fukunaga and D. L. Kessell, Nonparametric Bayes error estimation 
using unclassified samples, Trans. IEEE on Inform. Theory, IT-19, pp. 
K. Fukunaga and L. D. Hostetler, k-nearest neighbor Bayes-risk estima- 
tion, Trans. IEEE on Inform. Theory, IT-21, pp. 258-293, 1975. 
D. L. Wilson, Asymptotic properties of nearest neighbor rules using 
edited data, Trans. IEEE Systems Man Cybernet., SMC-2, pp. 408-420, 
1972. 
pp. 107-112, 1985. 
PAMI-9, pp. 634-643, 1987. 
434-440, 1973. 

366 
Introduction to Statistical Pattern Recognition 
22. 
P. A. Devijver and J. Kittler, On the edited nearest neighbor rule, Proc. 
Fifth Internat. Con$ on Pattern Recognition, Miami Beach, FL, pp. 72- 
80, 1980. 
P. E. Hart, The condensed nearest neighbor rule, Trans. IEEE Inform. 
Theory, IT-14, pp. 515-516, 1968. 
K. Fukunaga and P. M. Narendra, A branch and bound algorithm for 
computing k-nearest neighbors, Trans. IEEE Computers, C-24, pp. 750- 
753, 1975. 
J. H. Friedman, F. Baskett and L. J. Shustek, An algorithm for finding 
nearest neighbors, Trans. IEEE Computers, C-24, pp. 1000- 1006, 1975. 
T. P. Yunck, A technique to identify nearest neighbors, Trans. IEEE Sys- 
tems Man Cybernet., SMC-6, pp. 678-683, 1976. 
23. 
24. 
25. 
26. 

Chapter 8 
SUCCESSIVE PARAMETER ESTIMATION 
In the approaches to parameter estimation presented so far, estimates 
have been determined from all of the observable data in a single calculation. 
Sometimes, however, it is required to use a procedure which is based on a 
sequence of observations. In this case, the parameters are first approximated 
by an initial "guess." Then each observation vector is used in turn to update 
the estimate. Hopefully, as the number of observations increases, the estimate 
will converge in some sense toward the true parameters. A major advantage of 
this successive approach is the fact that an infinite number of observations can 
be accounted for, using a finite amount of storage. 
8.1 Successive Adjustment of a Linear Classifier 
When each of the conditional density functions corresponding to the 
classes to be separated belongs to a known parametric family, the classifier 
design is fairly straightforward. After the estimates of the unknown parameters 
have been obtained, the Bayes decision rule is determined. Quite often, how- 
ever, even the functional form of the conditional densities cannot be assumed. 
The densities could be approximated by the techniques described in Chapter 6, 
but, on the other hand, it may be possible to avoid the calculation of the 
367 

368 
Introduction to Statistical Pattern Recognition 
densities completely. Suppose it is decided a priori that the decision rule 
belongs to a parametric family of decision rules. The problem then reduces to 
that of estimating the parameters of the decision rule. By this approach we 
sacrifice, perhaps, a certain amount of insight into the nature of the classifier 
problem. However, this sacrifice is often more than justified by savings in 
computation. 
Suppose, for a two-class problem, we decide to use a linear classifier of 
the form 
Then, our problem is that of estimating the parameters V and vo. The linear 
classifier is chosen because of its simplicity, but it should be recalled from 
Chapter 4 that (8.1) can include a broad range of nonlinear functions. This is 
the case if X is considered a vector in a functional space instead of being 
thought of as the original variable space. 
In Chapter 4, we discussed the design of V and v o  from a given set of 
observations, provided these are all simultaneously available. In the present 
case, however, we would rather not store all of the observation vectors simul- 
taneously. Instead, we store only the current parameter estimate and update 
this estimate each time a single observation vector is presented to the estima- 
tion system. This type of system was first developed as a simplified model of 
learning and decision-making in early pattern recognition research, and has 
since been called a perceprron [ 1],[2]. In this model, we have to have an algo- 
rithm which modifies the parameters on the basis of the present observation 
vector and the present values of these parameters. 
Linear Classifier for Two-Class Problems 
Let us rewrite (8.1) using new notations Z and Was in (4.73). 
n 
h ( 2 )  = WTZ = 
w; 
z; > 0 ,  
i=o 
where zo =-1 or +1 and zi =-xi or +xi (i = 1,2,. . . ,n) depending on X E o1 
or X E 02. 
Then, the current value of W is updated to the new value of W ’ ,  as 
follows [3]. 

8 Successive Parameter Estimation 
369 
(1) w ' =  w 
if wTz > 0 ,  
(8.3) 
(2) w ' =  W + C Z  if 
W ~ Z S O  (c > 01. 
(8.4) 
Since W of (8.3) correctly classifies the sample, we have no reason to change 
W. In (8.4), W should be modified to increase WTZ. The updated W '  of (8.4) 
satisfies this condition because 
w'Tz = WTZ + c z T z  > wTz . 
(8.5) 
The reader may try to find a function, W '  = f ( W , Z ) ,  which satisfies 
fT(W,Z)Z >WTZ for all Z's. Then, it will be realized that there are no other 
simple, practical solutions than the procedure of (8.3) and (8.4). In (8.4), we 
still have one control parameter, c, to determine. 
There are three common ways to choose c. These are as follows: 
(1) Fixed increment rule: c = constant 
(2) Absolute correction rule: Select c large enough to obtain W'TZ > 0. 
That is, 
W"Z = (W + CZ)'Z > o 
for 
W ~ Z  
50. 
(8.6) 
In order to satisfy (8.6), c should satisfy 
I wTz I 
ZTZ 
c > -  
(8.7) 
(3) Gradient correction rule: When we maximize or minimize a cri- 
terion such as the mean-square error between the desired and actual outputs, 
y(Z) and W'Z, we can determined c by the gradient of the criterion. For 
example, by analogy with (4.84) we may select c as 
c = p[y(Z) - WTZl . 
(8.8) 
Obviously, c depends on the criterion we select, and p should be a positive 
constant properly chosen. 
Example 1: Let us design a classifier which separates the four samples 
of Fig. 8-1. The samples are presented to the machine in the order 
Z,,Z,,Z,,Z,.Zo 
. . . . The fixed increment rule is used with c = 1. The 
sequence of W in the training period is shown in Table 8-1. The term W 
converges to 

370 
Introduction to Statistical Pattern Recognition 
I w1 
w2 
Fig. 8-1 Linearly separable example. 
TABLE 8-1 
SEQUENCE OF W FOR A LINEARLY SEPARABLE CASE 
Iteration 
Input 
zo z 1  z2 
wo w I  w2 
WTZ 
Adjustment 
1 
2 0  
- 1 0 0  
0
0
0
 
0 
w+zo 
z1 
1 
1 
0 
-1 
0 
0 
-1 
w+z, 
22 
-1 
0 -1 
0 
1 0 
0 
w+z, 
2 3  
1
1
 1 
-1 
1 - 1  
-1 
w + z 3  
2 
zo 
- 1 0 0  
0
2
0
 
0 
w+z, 
ZI 
1
1
0
 -
1
2
0
 
1 
No 
2 2  
-1 
0 -1 
-1 
2 
0 
1 
No 
2 3  
1
1
1
 -
1
2
0
 
1 
No 
3 
zo 
-1 
0 0 
-1 
2 
0 
1 
No 
2z, - 1 > 0 .  
Or, in the original X-space 
(8.9) 
(8.10) 
As seen in Fig. 8-1, this classifier separates the given four samples correctly. 

8 Successive Parameter Estimation 
37 1 
Example 2: This is an example of a linearly nonseparable case, in which 
there exists no linear classifier that separates samples without error. Six sam- 
ples are given in Fig. 8-2, and they are fed into the machine in the order 
Fig. 8-2 Linearly nonseparable example. 
Zo,ZI,. . . ,Z5,Zo,. . . ,Z5,Zo,. . . . The fixed increment rule is used again, 
with c = 1. The sequence of W in the training period is shown in Table 8-2. 
We have a cyclic sequence of W without convergence, although the best linear 
classifier [wo w 
I w,] = [0 2 21 is included in the sequence of W. 
Whenever we have an iterative process like this, there is a question of 
whether the convergence of the process is guaranteed. When two distributions 
are linearly separable, the convergence of the above process for the fixed 
increment rule with a proper range of p can be proved. 
Convergence Proof for the Linearly Separable Case 
The convergence proof of the fixed increment rule is as follows 131. 
Let us eliminate, from a training sequence Z I ,Zz, . . . , the Z’s which do 
not change W, and call it the reduced training sequence Z;,Z,, 
. . . . Since the 
eliminated Z’s do not affect W ,  they have no influence on the proof of conver- 
gence. 
Also, let us assume c = 1. This assumption does not reduce the general- 
ity of the proof, because a change of c merely corresponds to a change of the 
coordinate scale. The scale of the coordinate does not affect the structure of 
the data and the linear classifier. 

312 
Introduction to Statistical Pattern Recognition 
TABLE 8-2 
SEQUENCE OF W FOR A LINEARLY NONSEPARABLE CASE 
Iteration 
Input 
zo z l  z2 
w o  w I  w 2  
WTZ 
Adjustment 
1 
zo 
1
0
2
 0
0
0
 
0 
w+zo 
z, 
-1 -1 -1 
1 
0 
2 
-3 
w+z, 
2 2  
1 2 0 
0 - 1  
1 
-2 
w+z2 
2 4  
1 - 1 - 1  
1
1
1
 
-1 
w+z4 
z5 
-1 
2 0 
2 
0 
0 
-2 
w+z5 
ZI 
-1 -1 -1 
1 
2 
0 
-3 
W + Z l  
2 3  
-1 
0 2 
0 
1 -1 
-2 
w+z3 
2 4  
1 -1 -1 
-1 
1 
1 
-3 
w+z4 
z5 
- 1 2 0  
0
0
0
 
0 
w+z, 
2 3  
- 1 0 2  
1
1
1
 
1 
No 
2 
ZO 
1
0
2
 
1
2
0
 
1 
No 
2 2  
1
2
0
 
0 
1 - 1  
2 
No 
3 
ZO 
1 
0 2 
-1 
2 
0 
-1 
w+zo 
z1 
-1 -1 -1 
0 
2 
2 
-4 
W + Z I  
z2 
1
2
0
 -1 
1
1
 
1 
No 
23 
-1 
0 2 
-1 
1 
1 
3 
No 
2 4  
1 -1 -1 
-1 
1 
1 
-3 
w+z4 
z5 
- 1 2 0  
0
0
0
 
0 
w+z, 
A reduced sequence of samples generates the corresponding sequence of 
W* given by 
w;+, 
= w; +z; , 
(8.1 1) 
where 
W;TZ; 
SO. 
(8.12) 
Since we assume that the two distributions are linearly separable, we should be 
able to find W, such that 
WTZ~ > o 
for all k’s . 
(8.13) 
Selecting 

8 Successive Parameter Estimation 
373 
a =maxZIZk and 
h > 0 ,  
(8.14) 
k 
we can select the scale of W so that 
WYZk > - 
a+b > o .  
(8.15) 
2 
The change of scale of W does not change the decision rule (8.2). 
Now we calculate the distance between W, and W; as 
II w, - w; 112 = II w, 112 + II w; 112 - 2w;w; . 
(8.16) 
Hence, using (8.1 l), 
Ilw, - will2 - Ilw, - w;,, (I2 = IIwf II' - llw;+l (I2 
-2w3w; - w;+l ) = -2W;TZ; - zyz; + 2w;z; 
, 
(8.17) 
Recalling WZ'Z; 10 from (8.12) and the inequalities of (8.14) and (8.15), 
(8.17) becomes 
(8.18) 
Equation (8.18) shows that, whenever a new Zf is applied, IlW, - Will2 
decreases by a finite amount (larger than b). Therefore, after a finite number of 
samples, W; should converge to W,. 
The above convergence proof may be extended to the absolute correction 
rule by converting it to an equivalent fixed increment rule. This is done by 
generating artificially a sequence of samples Z, = Zk+I = . . . = Zk+, whenever 
WiZk 10 occurs, and applying the fixed increment rule until W[+a+lZk 
becomes positive. That is, u+l corresponds to the value of c which realizes 
(8.6). 
a +b 
2 
IIw, - will2 - IIw, - w:+, (I2 > -a + 2- 
> O .  
Linear Classifier for Multiclass Problems 
The algorithm of designing a linear classifier for the two-class problem 
can be extended to linear classifiers for the multiclass problem. 
For L classes, we set up L linear discriminant functions Wi. The deci- 
sion rule then becomes 

314 
Introduction to Statistical Pattern Recognition 
W:Y>W;Y u = l ,  ..., L :  j # i )  + Y E W ; ,  
(8.19) 
where Y and W are defined, in terms of X, V, and v o  of (8.1), by 
y o  = 1 and 
yi = x i  
(i = 1,. . . , n )  , 
wi = vi 
(i = 0,1,. . .,n) . 
(8.20) 
When all Y E ai (i = 1,. . . ,L) satisfy (8.19), we call these L classes linearly 
separable. An algorithm to adjust these W’s is given as follows: 
(1) 
If Wry > WTY 0’ = 1,. . . ,L: j f i )  for Y E mi, then 
w; = w, ( k =  1,. . . , L ) .  
(8.21) 
(2) If W:Y > WTY and WTY > WTY for Y E mi, then 
wT’ 
= w? 
-CY, w,: 
= wi 
+CY, wj = wj 0’ # i,e). 
(8.22) 
This multiclass problem can be reformulated as a two-class problem, if 
we extend our dimension to n x L as 
w = [WT . * . WT-1 WTWT+, 
. . . w;, WBWL, . . . w y  , 
(8.23) 
z = [OT. . .oT YT or. . .OT -YTOT. . . O T I T ,  
(8.24) 
for the Y of (8.22). Then, for a reduced sequence of samples, Z;,Z;, . . . , we 
can obtain a corresponding sequence of W ,  Wt,W;, 
. . . . These Wi’s are 
related by 
w;+, 
= w; + cz; . 
(8.25) 
Equation (8.25) is equivalent to (8.22). Since (8.25) is the same as (8.1 I), the 
convergence of (8.25) and, consequently, the convergence of (8.22) is 
guaranteed by the proof presented in the previous subsection. 
As discussed in Chapter 4, a piecewise linear classifier is often used for 
separating many classes. Unfortunately, the convergence proof for a piecewise 
linear classifier is not known. However, similar algorithms to adjust the W’s 
can be found in some references [3],[4]. 

8 Successive Parameter Estimation 
375 
8.2 Stochastic Approximation 
The successive estimation algorithm of the last section does not always 
converge when the observation vectors are not linearly separable, as seen in 
Example 2. This fact leads us to look for an estimation algorithm for which 
convergence is guaranteed. Stochastic approximation is a technique that has 
been developed to find the root or the optimum point of a regression function 
in random environments [5],[6]. Stochastic approximation can be used for 
parameter estimation in pattern recognition, and convergence is guaranteed 
under very general circumstances. It is usually difficult, however, to discuss 
the rate of convergence. 
Before we begin a detailed discussion, let us examine a simple example. 
Suppose we want to estimate the expected vector from a finite number of 
observation vectors. Suppose, further, that we want to use a successive esti- 
mate. Now the nonsuccessive estimate M N  of the expected vector, based on N 
observation vectors, X I ,  . . . ,XN, is given by 
CI 
The equation can be modified to 
(8.26) 
(8.27) 
CI 
That is, M N  can be calculated with a new sample X N  if we store only MN-I 
and N .  Also, the effect of the new sample on the sample mean vector should 
decrease, with an increase in N ,  as follows: 
(8.28) 
- x N .  
The sequence of coefficients 1, 112, 113, . . . , IIN, . . . is known as a har- 
monic sequence. 
1 
N 
1 
1 
XI, -x2, -x 
2 
3 
3 " " '  
The above simple example suggests the following basic approach to suc- 
cessive estimation. 

376 
Introduction to Statistical Pattern Recognition 
(1) When the mathematical expression for an estimate is available, we 
may obtain the successive expression of the estimate by separating the estimate 
calculated from (N - 1) samples and the contribution of the Nth sample. 
(2) Even when we have to use a search process, in order to minimize or 
maximize a certain criterion, we may diminish the effect of the Nth sample by 
using a coefficient which is a decreasing function of N. 
Root-Finding Problem 
The simplest form of stochastic approximation is seen in finding a root 
of a regression function. This process is also called the Robbins-Monro 
z 
f 
Fig. 8-3 Root-finding problem. 
method [7]. Let 8 and z be two random variables with some correlation, as 
shown in Fig. 8-3. Our problem is to find the root of the regression function 
f (e), which is defined by 
f ( e )  = E { z i e i  . 
(8.29) 
If we can collect all samples for a fixed 0 and estimate E { z I e), then 
finding the root off (e) can be carried out by a root-finding technique for a 
deterministic function such as the Newton method. However, when it is 

8 Successive Parameter Estimation 
377 
predetermined that only one sample is observed for a given 8 and we try to 
change 8 accordingly, the observation off (8) is very noisy and may introduce 
an erroneous adjustment of 8, particularly around the root. 
In the Robbins-Monro method, the new successive estimate ON+, based 
on the present estimate ON and a new observation zN is given by 
eN+I = ON - aNZN 7 
(8.30) 
where we assume, without losing any generality, that 8 approaches eo, the root 
of (8.29), from the high side; that is, f(8) > 0 for 8 > eo and f ( 8 )  < 0 for 
8 < Elo, as shown in Fig. 8-3. Also, uN is assumed to be a sequence of positive 
numbers which satisfy the following conditions: 
(1) 
l i m u N = O ,  
N +- 
m 
(3) 
u; < =  
(8.31) 
(8.32) 
(8.33) 
N=l 
Although we will see later how these conditions for uN are used for the conver- 
gence proof, the physical meaning of these equations can be described as fol- 
lows. Equation (8.31) is similar to the l / N  term discussed earlier and allows 
the process to settle down in the limit. On the other hand, (8.32) insures that 
there is enough corrective action to avoid stopping short of the root. Equation 
(8.33) guarantees the variance of the accumulated noise to be finite so that we 
can correct for the effect of noise. 
With a sequence of uN satisfying (8.31) through (8.33), ON of (8.30) con- 
verges toward Bo in the mean-square sense and with probability 1, that is, 
IimPr(ON =e,) = I . 
N +m 
(8.35) 
The harmonic sequence of (8.28) is a suitable candidate for ( uN ). More 
generally, a sequence of the form 

378 
Delay 
- E L  
Introduction to Statistical Pattern Recognition 
qe) 
(8.36) 
4 
satisfies (8.31) through (8.33), although it is not the only possible sequence. 
Before discussing the convergence of the Robbins-Monro method. let us 
consider a feedback system analogous to this process. 
Figure 8-4 shows an equivalent feedback circuit, where y(t) is a noise 
process. Instead of a fixed feedback gain, we have a time-decreasing feedback 
gain a (t). From the conventional design concept of a feedback circuit, one can 
notice that the decreasing a @ )  could guarantee the stability of the circuit 
without hunting but could also result in a slower response. 
Convergence Proof of the Robbins-Monro Method 
The convergence of the Robbins-Monro method is proved as follows. 
First let us divide zN into two parts: the regression function f(0,) and noise 
yN. Then, (8.30) is rewritten as 
(8.37) 
O N + ,  = e, - a N f  (0,) - ~ N Y ,  
where 
YN = ZN - f ( O N )  . 
(8.38) 
Then, from the definition of the regression function f (0) in (8.29), yN is a ran- 
dom variable with zero mean as 

8 Successive Parameter Estimation 
379 
E{Y,I~,,I = E { Z N i e N j  - f ( e N ) = o .  
(8.39) 
Also, it is reasonable to assume that the variance of yN is bounded, that is, 
E ( y i )  So2 
(8.40) 
and that yN and 6, are statistically independent. 
Next, let us study the difference between 8, and 0,. From (8.37), we 
have 
Taking the expectation of the square of (8.41) 
We assume that the regression function is also bounded in the region of 
interest as 
E ( f 2 @ N ) J  a 
' 
(8.44) 
Then, (8.43) is bounded by 
w % , - w 2 i - w e I  
-e,)*i 
Let us examine (8.45) term by term. First, since E {  (8, - €lo)* ) is posi- 
tive and assuming 8 ,  is selected so that E { @ ,  - €io)2 } is finite, the left-hand 
side of (8.45) is bounded from below. The first term on the right-hand side of 
(8.45) is finite because of (8.33). 
Recall from Fig. 8-3 that the regression function satisfies: 

380 
Introduction to Statistical Pattern Recognition 
Therefore, 
and 
f ( e )  > 0 if (e-eo)>o, 
f ( 0 )  = 0 if (8-eo)=0, 
f ( 0 )  < 0 if (e-e,)<o. 
(8.46) 
Now consider the following proposition: 
limE{ (ei - 0,)f (ei) 
1 = 0 . 
(8.49) 
I -F- 
If (8.49) does not hold, then, because of (8.32), the last term of (8.45) tends 
toward -. 
But this contradicts the fact that the left-hand side of (8.45) is 
bounded from below. Hence, (8.49) must hold. Since (8.47) holds for all e’s, 
(8.49) 
is equivalent to 
limPr { ei = e,)) = 1 . 
(8.50) 
i+=- 
Thus, the convergence with probability 1 is proved. The convergence in 
mean-square sense has also been proved but this proof is omitted here. 
Minimum-Point-Finding Problem 
The Robbins-Monro method can be easily modified to seek the minimum 
point of a regression function instead of the root. As is well known, the 
minimum point or the optimum point of a function f (0) is a root of df (B)ld0. 
Therefore, if we can measure df(e)lde, we can apply the Robbins-Monro 
method directly. Unfortunately, in most applications, the measurement of 
df (B)/d0 is not available. Therefore, we measure the derivative experimentally 
and modify 
as 
This successive equation is called the Kiefer-Wolfowitz method [8]. Figure 8-5 
illustrates the Kiefer-Wolfowitz method. 

8 Successive Parameter Estimation 
38 1 
, 
‘ I  
I 
. x  
fx I 
t 
1 
I 
I 
I 
1 
,e 
eN-cN 
eN +cN 
Fig. 8-5 Minimum-point-finding problem. 
Both uN and cN are sequences of positive numbers. They must vanish in 
the limit, that is, 
limaN = 0 ,  
(8.52) 
N - w  
limcN = 0 , 
N+- 
(8.53) 
so that the process eventually converges. In order to make sure that we have 
enough corrective action to avoid stopping short of the minimum point, uN 
should satisfy 
m 
N = l  
Also, to cancel the accumulated noise effect we must have 
2 +] 
e = .  
N = l  
(8.54) 
(8.55) 
With aN and cN satisfying these conditions, it has been proven that 0, of 
(8.51) converges to e,, both in the mean-square sense and with probability I ,  
provided that we have a bounded variance for the noise and a bounded slope 
for the regression function. The proof is similar to the one for root-finding but 
is omitted here. 

382 
Introduction to Statistical Pattern Recognition 
Multidimensional Extension 
So far, we have discussed stochastic approximation for a single variable. 
This has been done mainly for simplicity's sake. The conclusions and the cri- 
terion for the selection of uN are all valid for the multivariate case. The previ- 
ous discussion, including the convergence proof, can be repeated simply by 
replacing x2 by IIX11* [91. 
Thus, for the Robbins-Monro method, (8.30) can be rewritten as 
e N + l  = eN - aNZN 
9 
(8.56) 
where both 8 and Z are random vectors with n components. With aN satisfy- 
ing (8.3 1) through (8.33), the convergence in the mean-square sense and with 
probability 1 is guaranteed, provided that both the noise variances and the 
regression function are bounded. 
For the Kiefer-Wolfowitz method, the partial derivatives should be 
approximated by 
(8.57) 
or 
az(@N) - z (ON + CNE;) - z (0, - C N E ; )  
(8.58) 
where E; is the ith unit coordinate vector [0 . . . 0 1 0 . . . 0IT. Then (8.51) 
-- 
- 
aei 
2CN 
(8.59) 
Figure 8-6 shows how the partial derivatives are measured; n + 1 observations 
for (8.57) and 2n observations for (8.58) are needed. Again, the convergence 
in the mean-square sense and with probability 1 is guaranteed, provided that 
both the noise variances and the slope of the regression function are bounded. 
Now we can relate stochastic approximation to the design of a classifier. 
Let us design a classifier which minimizes the mean-square error between the 
desired output y(Z,) and the actual output WTZ, as in (4.78). Although we dis- 

8 Successive Parameter Estimation 
383 
x 
(a) 
(bl 
Fig. 8-6 Minimum-point-finding problem for a multivariate case. 
cuss only one criterion, the same discussion can be applied to other criteria. 
The successive adjustment of W of (4.84) can be rewritten as 
2P 
(8.60) 
That is, W(t) is modified by the sample mean vector of (WT(t)Z -y(Z)]Z. 
Therefore, when we can use all N samples to calculate the sample mean vector 
for a given W, the successive adjustment becomes a simple optimization pro- 
cess for the regression function. When we can use only one sample at a time 
to modify W(t), (8.60) is converted to 
a2 
W(t+l) = W(t) - p- 
= W(Z) - -E( WT(t)Z; - y(Z;)]Z; 
. 
aw 
N 
w(e + 1) = W(L) - 2p{ WT(1)Zt - y(Z,)]Z, 
. 
(8.61) 
This is identical to (8.56), the Robbins-Monro method for the multivariate 
case, where 2( WT(E)Z, - r ( Z , ) ] Z ,  corresponds to ZN of (8.56). Thus, although 
this is a minimum-point-finding problem, we can calculate the partial deriva- 
tives from W(I) and Ze. Therefore, we can apply the simpler Robbins-Monro 
method rather than the Kiefer-Wolfowitz method. 

3 84 
Introduction to Statistical Pattern Recognition 
Example 3: Let us apply the Robbins-Monro method to the six samples, 
given in Example 2, which are not linearly separable and for which the method 
of the previous section could not make the sequence of W converge. The result 
is shown in Table 8-3, where the starting W is [0 0 O l T ,  r(Zj) = 3 for 
TABLE 8-3 
AN EXAMPLE OF CLASSIFIER DESIGN 
BY USING INDIVIDUAL SAMPLES 
Iteration 
Input 
io I 1 '2 
"'0 
w ' I  
w 2  
Y-W'Z 
p 
2p(y-wTz)z 
I 
Zo 
1 0 2 
0 
0 
0 
3.0 
1/10 
0.6 
0 1.2 
21 
-1 -1 
- I  
0.6 
0 
1.2 
4.8 
1/11 
-0.9 -0.9 -0.9 
2 2  
1 
2 
0 
-0.3 -0.9 0.3 
5.0 
1/12 
0.8 
1.7 
0 
2 3  
-1 
0 
2 
0.6 0.8 0.3 
2.9 
1/13 
-0.4 
0 0.9 
2 4  
1 - I  
- I  
0.1 0.8 1.2 
4.9 
1/14 
0.7 4 . 7  -0.7 
Zs 
-1 
2 0 
0.8 0.1 0.5 
3.6 
1/15 
-0.5 
1.0 
0 
2 
Zo 
1 0 2 
0.3 
1.1 0.5 
1.6 
1/16 
0.2 
0 0.4 
Zs 
- I  
2 0 
0.4 
0.4 0.3 
2.5 
1/21 
-0.2 
0.5 
0 
3 
zo 
I 
n 2 
0.1 0.9 0.3 
2.2 
1/22 
0.2 
o 0.4 
2 s  
-1 
2 0 
0.2 
0.5 0.3 
2.3 
1/27 
4 . 2  0.3 
0 
4 
Zo 
1 0 2 
0.1 0.8 0.3 
2.3 
1/28 
0.2 
0 0.3 
Zs 
-1 
2 0 
0.2 
0.5 0.4 
2.2 
1/33 
-0.1 
0.3 
0 
5 
Z, 
-1 
2 0 
0.1 0.5 0.4 
2.1 
1/39 
4 . 1  0.2 
0 
IO 
2 5  
-1 
2 0 
0.1 
0.5 0.4 
2.1 
1/69 
- 0 . 1  
0.1 
0 
15 
Z, 
- I  
2 0 
0 
0.5 0.5 
2.0 
1/99 
0 0.1 
0 
j =0,1,. . . ,5, and p is sequenced from 1/10 as 1/10, 1/11, . . . . This 
sequence is selected mainly because we felt that all six samples should contri- 
bute to the design of the classifier equally, at least at the initial stage, and that 
the sequence 1, 1/2, 1/3, . . . places too much weight on Zo. The sequence 
1/10, 1/11, . . . does not violate the conditions (8.31) through (8.33). Table 
8-3 shows that, after 15 iterations, the sequence of W converges to the 
optimum classifier, which is W = [0 0.5 0.5IT in this example. 

8 Successive Parameter Estimation 
385 
Example 4: For comparison, let us find the sequence of W by using 
(8.60) instead of (8.61) for the same six samples. This time, W is changed 
after six samples are received. Table 8-4 shows the result. Starting with 
TABLE 8-4 
AN EXAMPLE OF CLASSIFIER DESIGN 
BY USING THE REGRESSION FUNCTION 
z, 
- 1 - 1 - 1  
0 
z 2 
7 
I/? -3.5-3.5-3.5 
Z ?  
I 
2 
0 
0 
2 
z 
-I 
I/? -0.5 
-I 
0 
z3 
-I 
0 2 
0 
2 
z 
-I 
I/? 
0.5 
0 
-I 
z, 
I -1 
-I 
0 2 
z 
7 
112 
3.5 -35 - 3 5  
zi 
- I  
2 
0 
0 
2 
z 
-I 
1/2 
0.5 
-I 
0 
0 -I 5 
-I 5 
1 
Zl, 
I 
0 
2 
0 0.5 0.5 
2 
113 
1 3  
0 2 7  
z I  
- I  
-I -I 
0 o s  0.5 
J 
111 
-27 -2.7 -27 
z2 
I 
2 
0 
0 0 . 5 0 . 5  
2 
113 
1.3 2.7 
0 
23 
-I 
0 
2 
0 0 5 0.5 
2 
1/3 
-I 3 
0 
2.7 
z5 
-I 
2 
0 
0 0 . 5 o . s  
2 
113 
-1.3 
2.7 
0 
11 
0 
0 
ZJ 
I -I -I 
0 0.5 0.5 
4 
l/7 
2.7 -2.7 -2.7 
4 
z,, 
0 
0.5 o r  
I I4 
W=[OOO]', y(Z,)=3 (j=O,l, . . . ,  51, and the sequence of p =  l,l/2 , . . _ ,  
the optimum classifier is obtained in two iterations. This is a much faster pro- 
cess than the one of Example 3, in which 15 iterations were needed. 
The Method of Potential Functions 
In stochastic approximation, successive approximation is applied to esti- 
mate a set of parameters which gives either the root or the extrema1 point of a 
regression function. The results can be extended to the estimation of the 
regression function itself [IO]. 
Let us express a regression function f ( X )  by an expansion of a given set 
of basis functions as 

386 
Introduction to Statistical Pattern Recognition 
(8.62) 
where (0; ] should satisfy 
m xei’<-. 
(8.63) 
Assuming that the 5;’s are know and given, the regression function f ( X )  is 
characterized by a set of parameters 0 = (e,, . . . ,e, ). This is somewhat the 
same as designing a linear classifier in which si is used instead of e i ( X ) .  The 
selection of the basis functions is very much problem-oriented and depends on 
the functional form o f f  (X). In general, we have to look for 5; so that 8; 
decreases quickly as i increases. Also, it is common practice to select an 
orthonormal set of 5; for theoretical convenience. 
i=l 
In a noisy environment, for a given X ,  our observation is a random variable 
z(X) whose expected value is f ( X ) .  Therefore, if we want to determine the 
8;’s to minimize the mean-square error between z(X) and CeJ,;(X), then we 
solve 
r 
L 
J 
where the 5;’s are assumed to be orthonormal. Therefore, 8; is determined by 
When a successive approximation is required, 
can be estimated by the 
sequence 

8 Successive Parameter Estimation 
387 
(8.67) 
Or, in vector form, 
0(i! + 1) = o(q - 2a, { OT(S)E(X ) - z (X,))Z(X 
) , 
(8.68) 
where 0 = [e, . . . 8,IT and Z = [t, . . . tmlT. 
This equation suggests the use 
of the Robbins-Monro method with the sequence a, satisfying (8.31) through 
(8.33). 
A successive approximation of a functional form f(X), rather than 0, is 
also obtained by multiplying (8.68) by Z(X). That is, 
f,+l (X) = OT(i + l)Z(X) 
= @({)Z(X) - 2a { OT(Z)Z(X,) - z (X,) ] 2 ( X  )Z(X) 
where 
This K(Y,X) is called the potenrial funcrion, and the successive approximation 
of (8.69) is called the method of potential functions. 
Although we have derived the method of the potential functions from a 
stochastic approximation point of view, the method can be stated in a more 
general form as follows. 
A function f ( X )  which is either deterministic or stochastic can be suc- 
cessively approximated by 
f,+l(X) =f.(X) - Y , W  9x1 7 
(8.72) 
where f (X), its observation z (X), and K(Y,X) are all bounded. The potential 
function satisfies 

388 
Introduction to Statistical Pattern Recognition 
K( Y,x) = K(X, Y) 
(8.73) 
and 
(8.74) 
The basis functions are orthogonal as 
(8.75) 
where k ( X )  is a general kernel function, and k(X) = p (X) for stochastic cases 
as in (8.64). Selecting y, as 
6 j 
hj! 
jci(x)cj(x)k(x)dx = - 
9 
Yr = a , V , ( X , ) - z ( X ) t  9 
(8.76) 
where a, satisfies (8.31) through (8.33), the successive approximation of (8.72) 
converges in probability. The proof is omitted [IO]. 
Although we may select a broad range of potential functions which 
satisfy the above conditions, we can be a little more specific by using the fact 
that the potential functions are symmetric with respect to two vectors X and Y. 
It has been suggested that the distance between X and Y be used as a sym- 
metric function, that is, 
K(y,x) = g(lly - Xll) . 
(8.77) 
Two typical examples of g (.) are 
K(Y,X) = exp(-c.llY - xl12 I 
(8.78) 
and 
K(Y,X) = (1 + IlY - 
. 
(8.79) 
Acceleration of Convergence 
As we stated previously, a stochastic approximation converges very 
slowly. This is the price for guaranteed convergence. There have been many 
proposals to improve this disadvantage. In this subsection, we discuss two of 
them. 

8 Successive Parameter Estimation 
389 
(1) Flatter Sequence of aN: Since the primary cause of slow convergence is 
the choice of the decreasing sequence aN, we can make the sequence decrease 
more slowly and still guarantees convergence. One way to do this is to change 
aN to the next value only when a sign change of zN is observed in root-finding. 
As long as the sign of zN remains the same, we are not close to the root and 
convergence speed is more important than guaranteed convergence. When a 
sign change of zN is observed, we have to start worrying about convergence. 
The same argument holds for the minimum-point-finding problem, where the 
sign of slope should be observed instead of the sign of zN. Table 8-5 shows 
this altered sequence uN. Note that the altered sequence satisfies (8.31) 
through (8.33). 
TABLE 8-5 
ACCELERATED SEQUENCE OF UN 
Trial: 
1 
2 
3 
4 
5 
6 
7 
8 
Sign of zN: 
+ 
+ 
+ 
- 
- 
+
+
-
 
Conventional aN: 
1 
1/2 
1/3 
1/4 
1/5 
1/6 
1/7 
1/8 
Accelerated aN: 
1 
1 
1 
1/2 
112 
1/3 
1/3 
114 
(2) More Ohsenrations for a Given 8: If we can take many observations for a 
given 8 and calculate the mean, we can obtain the regression function. There- 
fore, the problem becomes that of the convergence of a deterministic function. 
As a compromise between this deterministic approach and stochastic approxi- 
mation, we may select a few observations for a given 8 rather than one, take 
the average of these observations, and use it as zN. An analogy can be found 
in a conventional feedback circuit where a filter is used to eliminate noise from 
the observation signal. Determining how many observations should be aver- 
aged to eliminate noise corresponds to the selection of the time constant in 
filter design. 
8.3 Successive Bayes Estimation 
Since the estimates of parameters are random vectors, complete 
knowledge of the statistical properties of the estimates is obtained from their 
joint density, distribution, or characteristic functions. In this section, we show 

390 
Introduction to Statistical Pattern Recognition 
how the density function of the estimate can be calculated by a successive pro- 
cess. 
Supervised Estimation 
Let X I , .  . . ,XN be N samples which are used to estimate the density 
function of a parameter vector 8. The samples X i  are given successively one 
by one. Thus, using the Bayes theorem, we can obtain a recursive expression 
for the a posteriori density function of 8, given X . . . ,XN, as 
p ( X ~ I x 1 , -  . .9X,v-l,Q)~(01X~,. 
. .,XN-I) 
P(XNIXI.. . .,X,-l) 
p(QlXI,. . .,XN)= 
(8.80) 
where the a priori density function of X,, p (X, IX I ,  . . . .XN-, ,e), is assumed 
to be known. If the numerator of (8.80) is available, the denominator can be 
calculated by integrating the numerator as follows: 
p (X, I X 1 ,  . . . ,XN-I ) = Jp (XN 
I X 1 ,  . . . ,XN-l ,O)p (0 I X I ,  . . . ,XN-I )dO . (8.8 1) 
Thus (8.80) shows that p (0 IX I ,  . . . ,XN) may be calculated from 
p(OIXl, . . . ,XNVI). Repeating the same operation N times, we may start this 
sequence of calculations from p (0). The term p (0) is called the inirial density 
function of 8, and reflects our initial knowledge about 8. 
Estimation of an expected vector with a known covariance matrix: 
Let us estimate the expected vector M of a normal distribution with a known 
covariance matrix X. The initial density function p ( M )  is assumed to be nor- 
mal with the expected vector M, and covariance matrix &,. 
Then, after 
observing the first sample X I ,  
where 
(8.82) 

8 Successive Parameter Estimation 
39 1 
MI = eo(&) + c)-lxI + z(e" + c)-IMo , 
(8.83) 
e, = &(eo + X)-Ic . 
(8.84) 
That is, p ( M  I X l )  is also a normal distribution and its expected vector and 
covariance matrix are given by (8.83) and (8.84). Since Jp (X I I M ) p  (M)dM of 
(8.82) is independent of M, c I ,  and c 2  are independent of M and are constants 
such that j p  (M I X I  )dM = 1. 
We repeat the same process, replacing Mo and eo of p ( M )  by M I and 
CI of p ( M I X l ) .  The resulting density p ( M I X 1 , X 2 )  is also normal, and M 2  
and C2 are calculated by (8.83) and (8.84) with M I  and X I  instead of Mo and 
Eo. Thus, after N iterations, 
P ( M I X I , .  . . , X N ) = N M ( M N , & ~ ) ,  
(8.85) 
where NM(MN,ZN) denotes a normal distribution of M with expected vector 
MN and covariance matrix C N .  They are given by 
(8.87) 
As N increases, the effect of the initial knowledge of M, M o ,  and eo, decreases 
and finally 
lime, = 0 .  
N+- 
(8.88) 
(8.89) 
Thus, MN for large N is estimated by the sample mean vector, and XN is X/N. 
Throughout this process, we notice that both a priori and a posteriori 
density functions are always normal. Because of this fact, we calculated only 
MN and X,,, recursively instead of calculating the density function. In general, 
when the a posteriori density function after each iteration is a member of the 
same family as the a priori density function and only the parameters of the 

392 
Introduction to Statistical Pattern Recognition 
density function change, we call the density functions a conjugate or reproduc- 
ing pair. 
In addition to its simplicity of computation, it has been shown that the 
reproducing density function of 9 becomes more concentrated and converges 
toward the true parameter vector 0 
in some appropriate sense as N+ 
[ 1 11. 
Many well-known density functions which are reproducing pairs are 
listed in a reference [l 11. 
Estimation of a covariance matrix with a given expected vector: The 
successive estimation of a covariance matrix for a normally distributed random 
vector X can be discussed in the same manner as that of the expected vector. 
Here we assume that the expected vector is known and, without further loss of 
generality, that it is equal to zero. As we assumed, the a priori density func- 
tion of p(X1E) is normal. On the other hand, it is known that the sample 
covariance matrix has a Wisharr distriiburion. Therefore, we start from the dis- 
tribution of a sample covariance matrix p (C I Co,No) with No as the number of 
samples used to compute &,. The term N o  may be considered as a confidence 
constant about the initial estimate of Zo. Furthermore, instead of calculating 
p(ZIEo,No), let us compute p ( K  ICo,No) where K =Z-'. 
The reason for 
doing this is that the covariance matrix is always used in the inverse form for a 
normal distribution. Then, p (K I Zo,No) is given by 
where 
Using (8.90) as p ( 0 )  and applying (8.80) repeatedly, p(K I X I , .  . . ,XN) 
also 
becomes the Wishart distribution and the parameters of the Wishart distribu- 
tion, Zo and No, are updated as follows [ 121: 

8 Successive Parameter Estimation 
393 
(8.92) 
N N = N o + N .  
(8.93) 
Thus, as N increases, CN approaches the sample covariance matrix ( I / N ) C X , X ;  
with zero-mean. 
Estimation of an expected vector and a covariance matrix: When 
both the expected vector and covariance matrix are to be estimated succes- 
sively, we have to calculate the joint a posteriori density function 
p (M, Z I X 
. . . ,XN). When M and C. are estimated by the sample mean vector 
and 
sample 
covariance 
matrix 
and 
X is 
normally 
distributed, 
p (M,K I M o , C o , ~ , N o )  (K = X-') is known to be the Gauss-Wisharr disrrihu- 
lion as 
(8.94) 
where c(n,N") is given in (8.91). The term po is the confidence constant 
about the initial estimate of M ,  as N o  for C,. Again, using (8.94) as p (0) and 
applying (8.80) repeatedly, p (M,K I X I ,  . . . , X N )  becomes the Gauss-Wishart 
distribution and the parameters of the distribution, M,, Co, p,, and N o ,  are 
updated as follows [ 121: 
(8.95) 
I + -  N 

394 
Introduction to Statistical Pattern Recognition 
(8.96) 
(8.97) 
(8.98) 
Unsupervised Estimation 
Suppose that we have two distributions characterized by O1 and 02. In 
successive unsupervised esfimation, our task is to estimate O1 and O2 succes- 
sively, assuming that we do not know the true distributions from which the 
incoming samples are taken. This is also termed learning without a teacher-. 
Because of the additional ambiguity we impose, the computation of unsuper- 
vised estimation becomes more complex. However, the development of this 
kind of technique is motivated by the hope that the machine may improve the 
performance without any outside supervision after initial learning in a super- 
vised mode. 
Since we do not know the class of XN, our guess is that X N  may belong 
to mi with probability Pi (i = 1,2), provided that we know Pi. Therefore, the (I 
priori density function of (8.80) becomes 
2 
i=l 
p (x, I x 1, . . . ,x,- I,@1, e?)= 
cp(xN I x I, . . . , X N - l  ,e;, 0;)P; . 
(8.99) 
Hence, if we know the a priori density function of each class p (XN IX I ,  
. . , ,XN-l ,Oi,oi) and the a priori class probability P i g  (XN IX I, . . . , X N - I ,  
01,02) 
can be computed by (8.99) and subsequently p(XN IX1,. . . , X N - ~ )  by 
(8.81). 

8 Successive Parameter Estimation 
395 
Thus we obtain a recursive expression for the a posteriori density func- 
tion as 
P(OI,O* 1 x 1 , .  , . , X N )  
2 
(8.100) 
Therefore, as a concept, successive unsupervised estimation of (8.100) is the 
same as successive supervised estimation of (8.80). However, because of the 
summation involved in the calculation of a priori density function, the repro- 
ducing property is lost for all density functions listed previously, including the 
normal distribution. This means that updating parameters is no longer ade- 
quate and we must deal with the recursive estimation of multivariate functions. 
Using all available samples at a time, there are more practical techniques 
available for unsupervised estimation and classification. The problem is stated 
as the method of finding the clusters of given samples and finding the natural 
boundaries of these clusters without knowing the classes of the samples. This 
problem will be discussed in Chapter 11. 
Computer Projects 
1. 
Generate samples one by one, according to Data I-A with P I = P 2  = 0.5. 
Find the linear classifier by using three successive adjustments: fixed 
increment, absolute correction, and gradient correction rules. Find a way 
to detect the oscillation of the classifier around the steady state and show 
which samples cause this oscillation. 
Repeat 1 by using the stochastic approximation of (8.61). 
Repeat 2 by using the average of k samples in the second term of (8.61). 
Choose k = 2, 4, 8, and 16. 
Attach the acceleration program of Table 8-5 to Project 2. 
Attach the acceleration program of Table 8-5 to Project 3. 
2. 
3. 
4. 
5. 

396 
Problems 
Introduction to Statistical Pattern Recognition 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Repeat Examples 2 and 3 by using 
(a) 
(b) 
Suppose that we have six samples from three classes as (+1,0) and 
(O,+I) for ol, 
(-I,+]) 
and (-1,O) 
for w2, and (0,-1) and (+I,-I) 
for w3. 
Find a linear classifier to separate these three classes by a successive 
method. 
In Chapter 4, we discussed a piecewise linear classifier. Propose an 
algorithm of successive adjustment for a piecewise linear classifier in a 
multiclass problem. 
A regression function is given by f = (I3. Find the root of the regression 
function by the Robbins-Monro method starting from 6 = 2. Assume 
that the 4h observation is zq = 63 + (-0.3)’, where (-0.3)’ is an additive 
noise. 
A regression function is given by f =-e2. Find the maximum point of 
the regression function by the Kiefer-Wolfowitz method. Assume that 
the observation is z = -e2 + 0.3a, where a is either +I or -1 depending 
on the face of a tossed coin. 
Repeat the convergence proof of the Robbins-Monro method for the mul- 
tivariate case. 
Repeat Example 4 by using (4.82) so as to minimize (4.74). If the linear 
classifier does not converge to a (x I + x 2 )  = 0 (a is a positive constant), 
point out the problem of this procedure. 
The term x is a random variable +1 or 0 with probability P or (1 - P), 
respectively. Let y be the number of 1’s out of N observations of x. 
The a priori density function of y, given P and N, is given by 
the absolute correction rule of (8.7), and 
the gradient correction rule of (8.8). 
P r ( y  = y I P , N }  = 
P”1 - P p - !  , 
i‘l 
which is a binomial distribution. Find the successive Bayes estimate of 
P. Also show that the binomial distribution is a reproducing pair. (Hint: 
Start from 

8 Successive Parameter Estimation 
P ( P  lYo,No) = 
397 
9. 
Let x be normally distributed with zero-mean and variance 02. Find the 
successive Bayes estimate of 02, assuming oi as the initial estimate of 
o2 with the confidence constant No. (Hint: The sample variance with 
zero mean has the chi-square distribution.) 
Let xi be the ith sample from the mixture of two normal distributions 
whose means and variances are 0 and o2 for ol 
and m and o2 for 02. 
Assuming P I = P 2  = 0.5, find the successive unsupervised estimate of m 
when the first sample xI is received. Is p ( m  Ixl) normal? The mean of 
o,, 
0, and both variances o2 are assumed to be known. The initial den- 
sity function p ( m )  is normal with the expected value mo and variance 
10. 
0;. 
References 
1. 
2. 
3. 
4. 
5 .  
6. 
7. 
8. 
9. 
F. Rosenblatt, “Principles of Neurodynamics,” Spartan Books, Washing- 
ton, DC, 1962. 
M. Minsky and S. Papert, “Perceptrons,” MIT Press, Cambridge, 1969. 
N. J. Nilsson, “Learning Machines,” McGraw-Hill, New York, 1966. 
R. 0. Duda and H. Fossum, Pattern classification by iteratively deter- 
mined linear and piecewise linear discriminant functions, Trans. IEEE 
Electronic Computers, EC- 15, pp. 220-232, 1966. 
D. J. Walde, “Optimum Seeking Methods,” Prentice-Hall, Englewood 
Cliffs, New Jersey, 1964. 
K. S. Fu, “Sequential Methods in Pattern Recognition and Machine 
Learning,” Academic Press, New York, 1968. 
H. Robbins and S. Monro, A stochastic approximation method, Ann. 
Math. Stat., 22, pp. 400-407, 195 1 .  
J. Kiefer and J. Wolfowitz, Stochastic estimation of the maximum of a 
regression function, Ann. Math. Stat., 23, pp. 462-466, 1952. 
J. A. Blum, Multidimensional stochastic approximation procedures, Ann. 
Math. Stat., 25, pp. 737-744, 1965. 

398 
Introduction to Statistical Pattern Recognition 
10. 
E. M. Braverman, The method of potential functions in the problem of 
training machines to recognize patterns without a teacher, Aurom. and 
Remote Contr-. [USSR], 27, pp. 1748-1771, 1966. 
J. Spragins, A note on the iterative application of Bayes’ rule, Trans. 
IEEE Inform. Theory, IT-1 1, pp. 544-549, 1965. 
D. G. Keehn, A note on learning for Gaussian properties, Trans. IEEE 
Inform. Theory, IT-1 1, pp. 126-132, 1965. 
11. 
12. 

Chapter 9 
FEATURE EXTRACTION AND LINEAR 
MAPPING FOR SIGNAL REPRESENTATION 
Up to now we have discussed how to design a classifier to separate sam- 
ples into two or more classes, assuming that the variables of these samples are 
already selected and given. Obviously, the selection of these variables is 
important and strongly affects classifier design. That is, if the variables show 
significant differences from one class to another, the classifier can be designed 
more easily with better performance. Therefore, the selection of variables is a 
key problem in pattern recognition and is termed feature selection or feature 
extraction. 
Feature selection is generally considered a process of mapping the origi- 
nal measurements into more effective features. If the mapping is linear, the 
mapping function is well defined and our task is simply to find the coefficients 
of the linear function so as to maximize or minimize a criterion. Therefore, if 
we have a proper criterion for evaluating the effectiveness of features, we can 
use the well-developed techniques of linear algebra for simple criteria, or, in 
the case of a complex criterion, we can apply iterative techniques to determine 
these mapping coefficients. Unfortunately, in many applications of pattern 
recognition, there are important features which are not linear functions of the 
original measurements, but are highly nonlinear functions. Then, the basic 
problem is to find a proper nonlinear mapping function for the given data. 
399 

400 
Introduction to Statistical Pattern Recognition 
Since we do not have any general algorithm to generate nonlinear mapping 
functions systematically, the selection of features becomes very much 
problem-oriented. 
In this and the next chapter, we will discuss criteria for measuring 
feature effectiveness. Since linear mappings are based on these criteria, we 
discuss linear mappings as well as the criteria. In this chapter, we deal with 
features for signal representation. Since the evaluation of eigenvalues and 
eigenvectors is a central problem for signal representation, we will discuss their 
estimation in this chapter. In the next chapter we will extend the discussion to 
classification, and features will be evaluated by their effectiveness on class 
separability. 
9.1 The Discrete Karhunen-Mve Expansion 
First, let us discuss feature selection for signal representation. That is, 
we discuss how closely we can represent samples of a distribution with a set of 
features. If a small set of features is found to represent the samples accurately, 
we may say that these features are effective. Although this problem is not 
directly related to pattern classification, knowledge of the characteristics of 
individual distributions should help to separate one distribution from others. 
Also, feature selection for signal representation has wide applications in other 
areas such as data compression in communication systems. 
Another limitation stems from the fact that we seek only features which 
can be obtained by a linear transformation of the original variables. Figure 9-1 
shows that a new feature y is very effective in representing the given samples, 
but that y is a nonlinear function of XI and x2. 
Minimum Mean-Square Error 
Discrete Karhunen-Lokve expansion: Let X be an n-dimensional ran- 
dom vector. Then, X can be represented without error by the summation of n 
linearly independent vectors as 
n 
x = Cy;$; = @Y , 
i = l  
where 
(9.1) 

9 Featun: Extraction and Linear Mapping for Signal Representation 
40 1 
I 
1 
=
x
 
Fig. 9-1 Nonlinear mapping. 
and 
Y = [yI . . .y,,i7 
(9.3) 
The matrix 
vectors. Thus, 
is deterministic and is made up of n linearly independent column 
I@l # O .  
(9.4) 
The columns of 0 span the n-dimensional space containing X and are called 
basis vectors. Furthermore, we may assume that the columns of @ form an 
orthonormal set, that is, 
If the orthonormality condition is satisfied, the components of Y can be calcu- 
lated by 
9; = Q ~ X  (i = I , .  . . ,n) . 
(9.6) 
Therefore, Y is simply an orthonormal transformation of the random vector X, 

402 
Introduction to Statistical Pattern Recognition 
and is itself a random vector. We may call $; the ith feature or feature vector, 
and yi the ith component of the sample in the feature (or mapped) space. 
Suppose that we choose only m ( < n )  of 4,’s and that we still want, at 
least, to approximate X well. We can do this by replacing those components 
of Y, which we do not calculate, with preselected constants and form the fol- 
lowing approximation: 
(9.7) 
We lose no generality in assuming that only the first m y’s are calculated. The 
resulting representation error is 
n 
ni 
I 1  
I1 
AX(m) = X - X(m) = X - Cyi$; - C hi$; = C (yi -hi)$; . 
(9.8) 
~ 
i=l 
i=ni+l 
i=ni+l 
Note that both X and AX are random vectors. We will use the mean-square 
magnitude of AX as a criterion to measure the effectiveness of the subset of m 
features. We have 
For every choice of basis vectors and constant terms, we obtain a value for 
E (m). We would like to make the choice which minimizes E2(m). 
The optimum choice for h, is obtained by minimizing E2(m) with respect 
-2 
to h, as follows: 
(9.10) 
a 
ah, 
- E {  
(y, - hf)2 ) = -2[E{ yf ) - h,] = 0 . 
Solving (9.10) for h, 
h, = E{Y, 1 = 0,7E{Xl ‘ 
(9.1 1) 
That is, we should replace those y,’s, which we do not measure, by their 
expected values. 

9 Feature Extraction and Linear Mapping for Signal Representation 
403 
Now, the mean-square error can be written as 
(9.12) 
where Cx is, by definition, the covariance matrix of X. We shall show that the 
optimum choice for the 4;’s is those which satisfy 
ZxQ; = Ql 
, 
(9.13) 
that is, the eigenvectors of &. Thus, inserting (9.13) into (9.12), the minimum 
mean-square error becomes 
(9.14) 
The expansion of a random vector in the eigenvectors of the covariance 
matrix is called the discrete version of the Kat-hunen-Loeiv (K-L) expansion. 
Proof of (9.13): Since we minimize (9.12) under the constraint of ortho- 
normality among the Qi’s, let us rewrite the criterion as 
where the plj’s are Langrange multipliers and @n-nl and ~ l , - n l  are nx(n-m) 
and (n -m)x(n -m) matrices defined by 

404 
Introduction to Statistical Pattern Recognition 
(9.17) 
The derivative of J with respect to a,,-,,, 
can be calculated by using (A.13) and 
(A.14), resulting in 
Equating aJla@,, to zero, 
(9.18) 
(9.19) 
Certainly, (9.19) is satisfied, if pR-m 
is the diagonal matrix with the (n-m) 
eigenvalues of &, h,+,, . , . ,Anr and 
is the matrix of the corresponding 
eigenvectors. Thus, the eigenvalues and eigenvectors give a particular solution 
of (9.19). The minimization of E*(*) can be achieved by selecting the smal- 
lest (n -m) eigenvalues and the corresponding eigenvectors. 
However, there exist many other @ , , - n l ' ~  and pn-n,'s 
which satisfy 
(9.19). Therefore, we need to show why we choose the eigenvalue and eigen- 
vector matrices as the solution of (9.19). 
Multiplying a;-,,, 
to (9.19) from the left side and using the orthonormal- 
ity of the 
P n - m  = @ L C X @ n - m  . 
(9.20) 
Therefore, pn-n, 
is the covariance matrix of the (n-m)-dimensional vector Y 
after the transformation of Y = @)-,,,X, and is not necessarily diagonal in gen- 
eral. In the mapped space, pn-,,, 
has its eigenvalue and eigenvector matrices 
A,,-,,, and '€',,-",. Therefore, an additional transformation Z = Y k , Y  diago- 
nalizes the covariance matrix p,,-nl 
as 

9 Feature Extraction and Linear Mapping for Signal Representation 
405 
(9.21) 
exists 
& = An-m = Yn,@n-mYn-m 
7 
. 
Note that both Y,,-,, and A,,-m are (n-m)x(n-m) matrices, and 
and is equal to Y:-,,,. Furthermore, substituting (9.20) into (9.21), 
An-", = ( @ n - m y n - m  
) T x x ( @ n - m y n  -ni ) . 
(9.22) 
That is, A,,-,,, contains the (n-rn) eigenvalues of Ex, and (@,,-,,, 
Ylt-n,),7x(,,-n7) 
consists of the corresponding (n -m) eigenvectors. Let us denote this eigenvec- 
tor matrix by ai-,,,. Then, from (9.22) 
(9.23) 
That is, any @ll-,,, 
which is the solution of (9.19), is obtained from @:-,, 
by 
an (n -rn)x(n -m) orthonormal transformation. We call this 
the member 
of the ai-,,, family. 
follows: 
@;-m = @n-mY"-ni -+ 
= Q);-nlYn-m 
T 
. 
The mean-square error is invariant among all members of the family as 
That is, once X is mapped down to the (n-m)-dimensional subspace spanned 
by (n -m) eigenvectors of &, further application of an (n -m)x(n -m) orthonor- 
mal transformation would not change the mean-square error. Thus, although 
any member of the ai-n7 
family is a legitimate solution of (9.19). we select 
@E-,,, 
as the representative of the family. 
Properties of the K-L expansion: Figure 9-2 shows how the K-L expan- 
sion works for a simple two-dimensional example. A distribution is shown by 
a contour line of one standard deviation. The eigenvectors 91 and (p2 of the 
covariance matrix are the principal axes of the distribution, and the eigenvalues 
A, and h2 are the variances of the distribution along the $,- and $*-axes. 
Since yi =$pX, y, and y2 are the projected values of X on the $1- 
and Q 2 -  

406 
Introduction to Statistical Pattern Recognition 
Fig. 9-2 A two-dimensional example of the Karhunen-LoCve expansion. 
axes. If h2 = E ( ( y 2 - h 2 ) 2 ]  is small, y2 stays close to h 2 .  Thus, X can be 
approximated by yI+I + h2$9, which is X in Fig. 9-2. 
,. 
Conventionally, the expected vector M is subtracted from X before the 
K-L expansion is applied. This simplifies the discussion. Therefore, from now 
on in this chapter, we will assume that the expected vector of X is zero. Then, 
the second term of (9.7) disappears, and the expansion is terminated at the mth 
term. When X has a nonzero mean, the deviation of X from the mean is 
approximated by a summation of m eigenvectors, and the mean is added to the 
approximation to represent X. 
In the context of pattern recognition, the coefficients yl, . . . ,y,! in the 
expansion are viewed as feature values representing the observed vector X in 
the feature space. The feature space has several attractive properties which we 
can list. 
(1) The effectiveness of each feature, in terms of representing X, is 
determined by its corresponding eigenvalue. If a feature, say +;, is deleted, the 
mean-square error increases by hi. Therefore, the feature with the smallest 
eigenvalue should be deleted first, and so on. If the eigenvalues are indexed as 
hl 2h2 2. . . 2h,, 20, the features should be ordered in the same manner. 

9 Feature Extraction and Linear Mapping for Signal Representation 
407 
(2) The feature values are mutually uncorrelated, that is, the covariance 
matrix of Y is diagonal. This follows since 
(9.25) 
In the special case where X is normally distributed, the yi’s are mutually 
independent. 
(3) The set of m eigenvectors of Cx. which correspond to the m largest 
eigenvalues, minimizes E (m) 
over all choices of m orthonormal basis vectors. 
Linear transformations which are not orthonormal are not considered in this 
chapter. in the case of representing a signal distribution, we are concerned 
only with transformations which preserve the structure of the distribution. 
-2 
In order to show how the K-L expansion is applied, two simple examples 
are given here. 
Example 1: Let us examine two impulsive distributions as shown in Fig. 
9-3(a) and (b), where each impulse carries the probability of 1/4. The expected 
(a) 
(b) 
(C) 
Fig. 9-3 Examples of the Karhunen-Mve expansion. 
vectors are zero in both cases. First, we calculate the covariance matrix &. 

408 
Introduction to Statistical Pattern Recognition 
.xa=-,.xr=+[~]~l 
l 4  
1]+ 1112 2]+ [1;][-' 
-11 
4 ;=I 
+ [$2 
-2l} 
r 
1 
= [K/4 lO/J 
for Data (a) , 
+ p ] [ + l  -11 + [:;I[-2 
-2l} 
= 1:; 
r:d 
for Data (b) . 
(9.26) 
(9.27) 
Secondly, we calculate the eigenvalues and eigenvectors of Zx. 
A,, = 5, 
hf, = 0 1 
@ l a  = klz] , 
$20 = [ ""1 
for Data (a) , 
(9.28) 
-116 
A,h =4, xu, = I .  
Qlh = I;:] 
, 
t$2h = [ ' I G ]  
for Data (b) . 
(9.29) 
-1/G 
Thus, for both cases, the basis vectors become 45 and -45 "lines, as shown by 
$, and $2 of Fig. 9-3. 
Finally, let us consider the effect of eliminating one of these basis vec- 
tors. For Data (a), h20 = 0. Therefore, even if we eliminate 9 2  in the K-L 

9 Feature Extraction and Linear Mapping for Signal Representation 
409 
expansion, the mean-square error is zero. Figure 9-3(a) shows that all four 
points can be expressed by the first basis vector OI without error. On the other 
hand, for Data (b), h2h = 1. Therefore, we expect a mean-square error of I by 
eliminating $2 
in the expansion. From Fig. 9-3(b), we see that only 
X2 = [2 2IT and X4 = [-2 -2IT can be expressed by QI without error, but 
X I  = [-I 
11’ 
and X 3  = [l - ] I T  
have errors of ‘6. Therefore, the mean- 
square error is (02 + O2 + (‘6)2 
+ (6)2)/4 = 1, which equals 1 2 h .  
Example 2: In this example, we show why non-orthonormal transforma- 
tions should not be allowed for signal representation. Suppose that we apply to 
the distribution of Fig. 9-3(b) the transformation of 
(9.30) 
That is, after applying the orthonormal transformation of (9.29), the scales of 
the new axes are changed by factors of 1/2 and 2 to get the distribution of Fig. 
9-3(c). Since the distributions of Fig. 9-3(b) and (c) are different, any conclu- 
sion about the properties of (c) cannot be applied to (b) directly. For example, 
if we conclude that the y2-axis (or Q2) is important from (c), it contradicts the 
conclusion from (b) that the y -axis (or Q, ) is important. Since feature extrac- 
tion for signal representation finds a small number of effective features to 
approximate a given distribution, any transformation which alters the structure 
of the distribution should not be allowed. 
Data compression: One of the popular applications of the K-L expan- 
sion is data compression in communication. Suppose we want to send a ran- 
dom process x(t). If we time-sample this waveform with n sampling points at 
the transmitter, we need to send n numbers, x(rl), . . . ,x(ff,). However, if we 
study the properties of the distribution of X = [x(r I ) .  . . x(rff)IT and find out 
that X can be approximated by a smaller number (m) 
of yi’s and Qi’L, 
F we can 
compute these m yi ’s at the transmitter and send them through the communica- 
tion channel. At the receiving end, we can reconstruct X by E:;, yi@; as in Fig. 
9-4. Thus, we need to send only m numbers instead of n. As seen in Fig. 9-4, 
both the transmitter and receiver must have the information of QI , . . . 

410 
X 
* 
yi=+:x 
Introduction to Statistical Pattern Recognition 
A 
X 
E Y p i  
__f 
Yl... .*Ym 
i-1 
t -.. t 
Qi 
Om 
01 
b m  
Fig. 9-4 Data compression in communication. 
Normalization problems: In the K-L expansion, we decide whether or 
not we select an eigenvector by observing the corresponding eigenvalue. How- 
ever, the absolute value of the eigenvalue does not give adequate information 
for selection. The ratio of the eigenvalue to the summation of all eigenvalues 
expresses the percentage of the mean-square error introduced by eliminating 
the corresponding eigenvector. Thus, we may use 
hi 
- 
hi 
p i = - - -  
trZx 
k h j  
j =  I 
as a criterion for retaining or deleting the ith eigenvector. Note that 
I1 
x p i = l .  
i = l  
(9.31) 
(9.32) 
Sometimes samples are normalized prior to application of the K-L expan- 
sion. The magnitude-normalized vector Z is given by 
X 
Z=- llxll 
(9.33) 
so that 
llzll = 1 . 
(9.34) 
Let Zz and hi’s be the covariance matrix of Z and its eigenvalues. Then the 
summation of h;.’s is 
(9.35) 
where E(Z) 
= 0 is assumed. That is, the 1;’s are the normalized eigenvalues. 

9 Feature Extraction and Linear Mapping for Signal Representation 
41 1 
However, it must be noted that the statistical properties of Z, including the 
covariance matrix, are entirely different from those of X. Thus, application of 
the K-L expansion to Z produces completely different eigenvectors and, there- 
fore, completely different features, than for X. 
A word of caution is in order for another normalization of 
xi 
z ; = y .  
X i  
j=I 
(9.36) 
In this normalization, the 2;’s are no longer linearly independent, because 
X:=,zi = 1. Therefore, the covariance matrix of Z = [z, . . . z , ] ~  becomes 
singular. 
Other Criteria for Signal Representation 
In addition to the mean-square error of approximation, there are some 
other criteria for evaluating features for signal representation. In this section, 
we will discuss two typical criteria: scatier measure and enfi-opy. 
Scatter measure: One measure of scatter is the expected value of the 
squared between-sample distance, which is given by 
2 ,  = E {  llx; - Xjll’} = E {  XTX; + Xi’Xj} - E{XTXj + xi’x; 1 
(9.37) 
where X i  and X, are mutually independent sample vectors taken from a single 
distribution. By virtue of the independence property, (9.37) becomes 
ax2 
= 2E(XTX) - 2E(XT}E{X) 
= 2tr[E{XXT} -MMT1 
= 2 tr(S - M M ~ )  
= 2 tr zx , 
(9.38) 
where S and Zx are the autocorrelation and covariance matrices and M is the 
expected vector of the distribution. 
transformation as 
Let Y be an rn-dimensional vector mapped from X by an orthonormal 
Y = a ; # x ,  
(9.39) 
where @,,, = [$, . . . Q,],,,, consists of rn column vectors Qi’s, and satisfies 

412 
Introduction to Statistical Pattern Recognition 
@La,,, 
= I 
or 
@T@j = 6, . 
(9.40) 
Then, the scatter measure in the mapped Y-space, z:, 
is 
m 
= 2 
* 
i=l 
(9.41) 
Now the feature selection problem may be stated in terms of choosing 
orthonormal vectors @,, . . . ,$,,, so as to maximize dy . Equation (9.41) is the 
same as (9.12) except that in (9.41) the summation of chosen terms is maxim- 
ized while in (9.12) the summation of discarded terms is minimized. There- 
fore, the optimalization discussion of the K-L expansion can be directly applied 
to this case, and $,, . . . ,qm should be the eigenvectors of Zx, whose eigen- 
values are rn largest. Thus we can conclude: 
- 2  
(1) The eigenvectors of Zx, which correspond to the dominant eigen- 
values, are the optimum features among all orthonormal transformations with 
respect to the scatter criterion 2 * . 
(2) From (9.41), the contribution of each feature to the total scatter is 
twice the value of the corresponding eigenvalue. 
Population entropy: The population entropy can be used as a measure 
of diversity of a distribution, and is defined by 
hx = -E(Inp(X)) . 
(9.42) 
The entropy is a far more complex criterion than the previous two criteria 
because the density function of X is involved. 
Again, feature selection consists of finding features so as to maximize h 
for a given m ( m a ) .  As with the scatter measure, we should limit ourselves 
to structure-preserving, orthonormal transformations. 
When the distribution of X is normal, h of (9.42) becomes 

9 Feature Extraction and Linear Mapping for Signal Representation 
413 
(9.43) 
n
1
 
n 
2
2
 
2 
- _  
- + - In I Zx 
I + - ln(2n) 
which is simply a function of 1 Zx 
I . 
When an orthonormal transformation Y = @T,X of (9.39) is applied, the 
entropy of 
(9.43) in the mapped Y-space becomes hy(m) = m/2 + 
(l/2)lnl@~Zx@m 
I + (m/2)ln(2a). Since the first and third terms of the 
entropy are not a function of am, 
we can maximize the entropy under the con- 
straint of orthonormality for am 
by maximizing 
J = In I @:z~@, I - tr$Lm(Q:@n7 - 111 , 
(9.44) 
where pn, is a Lagrange multiplier matrix (mxm) as in (9.15). Using (A.27) 
and (A.14), the derivative of J with respect to Om 
is 
(9.45) 
where @:ZX@,,, is the covariance matrix of Y, and generally can be assumed 
to be nonsingular. Equating (9.45) to zero, 
XxQn7 = @ n i p n , Z Y  . 
Multiplying @z7 from the left side 
(9.46) 
Q:cx@ni = P m C Y  . 
(9.47) 
Thus, pm must be equal to I. Furthermore, we can express Zy by YmAn,Yi 
where Yn7 
and An7 are the eigenvector and eigenvalue matrices of Zy. 
Then, 
(9.46) can be rewritten as 
@ H ~ x @ n 7  = y n 7 ~ n i v i  
(9.48) 
(9.49) 
That is, Ani contains the m eigenvalues of Cx, 
and (@,,,Yn1) 
consists of the m 
corresponding eigenvectors. Note that 

414 
Introduction to Statistical Pattern Recognition 
where IYL I I Y,,, I = I 'PLYm I = I I I = 1 is used, Therefore, as in the trace 
criterion of (9.24), each m-dimensional subspace cames a different value of the 
entropy. But, within a subspace, the entropy of (9.43) is invariant under any 
orthonormal transformation. It is appropriate to select a subset of eigenvectors 
of Zx as the basis vectors to specify the subspace. 
When a,,, 
consists of rn eigenvectors of Zx, 
m 
lnI@~CxQmI =InIh,,,I = C lnh, . 
i = l  
(9.5 1) 
In order to maximize (9.5 l), we must select m largest eigenvalues. 
So far, we have found the best linear mapping by maximizing the 
entropy for a normal distribution, (9.43). However, the optimization of (9.43) 
may offer a wider implication [l]. Note that the second line of (9.43) can be 
obtained by taking the expectation of the first line with respect to any distribu- 
tion (not limited to a normal distribution) as long as the distribution has the 
mean M and covariance matrix C. We call this family of density functions Gx. 
That is, 
G X = { p ( X ) : E ( X ] = M ,  
C O V ( X } = C ] .  
(9.52) 
On the other hand, the following inequality holds for any two density 
functions p I (X) and p2(X). 
=~P1(x)dx-Ip2(x)dX=1- 
1 = 0 ,  
(9.53) 
where Ins 5 s  - 1 is used. The equality holds only when pl(X) 
= p 2 ( X ) .  
Replacing p l ( X )  and p2(X) by NX(M,Z) 
and p ( X )  E Gx respectively, (9.53) 
can be rewritten as 

9 Feature Extraction and Linear Mapping for Signal Representation 
415 
I[-lnNx(M, VIP (X)dX &-lnp 
(X)lp (X)dX . 
(9.54) 
Since (9.43) is equal to the left side of (9.54), we may conclude that (9.43) is 
the maximum entropy among all density functions in Gx. In other words, the 
entropy for a normal distribution is the largest for given M and E. 
When X is mapped onto Y by y, = @TX ( i  = 1 , .  . . ,m), the remaining 
yi = (ITX (i = m+l, . . . ,n) form the complementary (n-m)-dimensional space 
of Y, which we will call the Y-space. Let the feature extraction problem be to 
minimize the entropy in the Y-space, instead of maximizing the one in the Y- 
space as previously discussed. Furthermore, let us assume that the true distri- 
bution is not known. Then, often in practice, it is risky to assume a density 
function and to find the best mapping based on that assumption. If the true 
distribution is different from the assumed one, the mapping functions might not 
give a good set of features. Therefore, as a safety precaution, we may find the 
maximum entropy among all density functions in Gg, and select the best linear 
mapping which minimizes the maximum entropy in the Y-space. This is the 
minimax procedure applied to feature extraction. Thus, we may claim that the 
selected features are most reliable, although they might not be the best. 
Restating the above in a mathematical form, 
min 
max hg(n-m) . 
9",*13.. ..$. 
P(P)EGg 
Using (9.43), max hp(n -m) can be obtained. Thus, (9.55) becomes 
(9.55) 
where Qn-m = 
. . . (I,,]. Applying the same argument as before, the 
optimum 0; (i = m+l, . . . ,n) are the eigenvectors of Ex corresponding to the 
smallest (n -m) eigenvalues of Xx. 
Thus, the discussion of (9.43) through (9.51) is not only for the optimi- 
zation of the entropy for a normal distribution, but also for the selection of 
most reliable linear features regardless of the true distribution. 

416 
Introduction to Statistical Pattern Recognition 
In addition, the inequality of (9.53) can be used to measure the closeness 
of two distributions. That is, the closer (9.53) is to zero, the more similar two 
distributions are. One of the applications of this similarity measurement is to 
approximate a given density function p ( X )  ( = p 2 ( X )  in (9.53)) by i ( X )  
( = p I ( X )  in (9.53)). In this problem, we characterize i ( X )  by a number of 
parameters. In order to make p ( X )  as close as possible to p ( X ) ,  we may 
minimize -J[l&X)]p 
(X)dX by adjusting the control parameters of &X). This 
procedure is called the entropy minimization. Often in practice, p ( X )  is not 
known, but samples, X I , .  . . ,XN, 
from p ( X )  are available. Then, we may 
approximate the expectation part of -J[lni(X)]p (X)dX by the sample mean as 
( l/N)X:=l [-ln;(Xi)]. 
Note that the approximate criterion does no longer 
include p (X). 
A 
Example 3: When the components of X are binary, +1 or -1, and 
independent, hx of (9.42) becomes 
n 
i=I 
hx = -x(PilnPj + (1 - Pi)ln(l - Pi)) , 
(9.57) 
where Pi and (14;) are the probabilities of xi = +1 and -1 respectively. Thus, 
individual variables xi’s are evaluated by -(Piln Pi +( 1 - Pi) In( 1 - P i ) ) .  
When the inputs are not independent, we may use the Bahadur expansion of 
Chapter 6 as the approximation of p ( X )  of (9.42). Obviously, hx becomes 
much more complex. 
General remarks: In the foregoing discussion, we have dealt 
exclusively with orthonormal linear transformations. This is necessary in order 
to maintain the structure of the distribution. As seen in Example 2, for a given 
distribution, we can cause one eigenvalue to dominate the others by an arbi- 
trary amount simply by changing the scales. However, unless we are given 
some physical reason to introduce such distortion, we would only be selecting 
the features that are created by the transformation and not related to the origi- 
nal distribution. 
Let us also consider the type of criteria we have used. Both the mean- 
square error and scatter measure are the expected values of some quadratic 
functions of variables. For this reason, our features are all given in terms of 

9 Feature Extraction and Linear Mapping for Signal Representation 
417 
the second-order statistics of the distribution, the covariance or autocorrelation 
matrix. The eigenvalues of these matrices are invariant under linear orthonor- 
mal transformations. 
In the statistical literature, the above is generally called factor analysis or 
On the other hand, when we discuss feature selection for classifying two 
or more distributions, we will allow a more general class of transformations. 
This is because the class separability, for example the probability of error due 
to the Bayes classifier, is invariant under any nonsingular transformations. 
These transformations preserve the structure of these distributions as far as 
classification is concerned. 
principle components analysis. 
In this section, we concluded that the optimum basis vectors of the K-L 
expansion are the eigenvectors of the covariance matrix of a given distribution. 
However, it should be pointed out that even if we select the eigenvectors of the 
autocorrelation matrix as the basis vectors of the expansion for some reason, 
the discussion is exactly the same as for the covariance matrix. The eigenvalue 
of the autocorrelation matrix represents the mean-square error due to the elimi- 
nation of the corresponding eigenvector from the expansion. 
9.2 The Karhunen-Loeve Expansion for Random Processes 
Continuous K-L expansion: Since the K-L expansion was originally 
developed and discussed to represent a random process [2], in this section we 
relate our previous discussion to the case of random processes, and also add 
some specific properties of the expansion for random processes. 
A random process x(t), defined in a time domain [O,T], can be expressed 
as a linear combination of basis functions. 
m 
xu) = CYISI(t) (0 9 IT) 
1 
(9.58) 
where the hasis functions c1 ( t )  are deterministic time functions and the 
coefficients y, are random variables. An infinite number of & ( t )  is required in 
order to form a complete set. Therefore, the summation is taken to -. 
The 
orthonormal condition of cl(t) is given by 
I =o 

418 
Introduction to Statistical Pattern Recognition 
fci(r)t;(t)dr = 6, , 
(9.59) 
where tJ(r) 
is the complex conjugate of cj(t). If cj(t) is a real function, 
becomes cj(t). The inverse operation to calculate yi from x(r) is 
(9.60) 
The expected value, autocorrelation and covariance functions of x ( r )  are 
defined by 
R(r,z) = E ( x ( r ) x * ( z ) ]  , 
(9.62) 
For simplicity’s sake, let us assume m (t) = 0 for 0 I t  IT. If the c i ( t ) ’ s  are the 
eigenfunctions of R (t, z), they must satisfy the following integral equation: 
C R  (r. z)ci(z) dz = h,E,;(t) 
(i = 1,2, . . . ) , 
(9.64) 
where the 1;’s are the eigenvalues of R (t. 2). 
These equations are exactly the same as the ones for random vectors. 
Suppose we take n time-sampled values of these time functions and convert 
them to vectors as 
where each time-sampled value of x(t). x(ti), is a random variable. Then, for 
example, (9.59) and (9.64) can be rewritten as follows: 
(9.67) 
and 

9 Feature Extraction and Linear Mapping for Signal Representation 
419 
I1 
C R ( r  ,tr)5,(tn)=hl(,(r.) kt= 1,2,. . .,n). 
(9.68) 
Equation (9.68) can be rewritten in a matrix form to define the eigenvalues and 
eigenvectors as 
X = l  
so, = A;$; 
(i = 1, 2, . . . ,n) , 
where S is 
(9.69) 
(9.70) 
Since S is an n x n matrix, we can obtain only n eigenvalues and eigenvectors 
instead of an infinite number. 
Minimum mean-square error: In order to minimize the mean-square 
error in the continuous version, we can follow a procedure similar to the one 
for the discrete case. For orthonormal ci(t)’s, 
m 
= 
E(YiYL1 
t=ni+l 
(9.7 1) 
From (9.60), E {yiy* } can be calculated by 

420 
Introduction to Statistical Pattern Recognition 
Therefore, if the ci(t)’s are the eigenfunctions of R (1, z), 
Hence 
-2 
- 
E = c A;. 
;=ni+l 
(9.73) 
(9.74) 
Recalling our assumption that E { x(f) 1 = 0 and therefore E { yi } = 0, the result 
is the same as the one for the discrete version of the K-L expansion. 
The difficulty in the continuous K-L expansion is that we have to solve 
the integral equation of (9.64) in order to obtain eigenvalues and eigenfunc- 
tions. Except in very special cases, explicit solutions are hard to obtain. 
Therefore, in order to get the solution numerically, we have to go to the 
discrete version; that is, take time-sampled values, calculate the autocorrelation 
matrix, and find the eigenvalues and eigenvectors. 
Stationary Process 
K-L expansion for an infinite time duration: For simplicity, the sta- 
tionary condition is often imposed in many cases when discussing random 
processes. A random process is called sfufionury in rhe wide sense if the fol- 
lowing two conditions are satisfied: 
m ( t )  = rn 
(constant) , 
(9.75) 
R(t,T) = R ( t  - 2 ) .  
(9.76) 
Equation (9.76) means that the autocorrelation function depends only on t-7. 
The functional form of R in the left side with two arguments is not the same as 
that of R in the right side with one argument. Also, we will continue to 
assume that m = 0. Since our discussion is quite specific, the reader should 
consult more general texts on random processes for background [3]. 

9 Feature Extraction and Linear Mapping for Signal Representation 
42 1 
For stationary processes, the integral equation of (9.64) becomes 
j T ” R ( i  - z)ki(~)dz 
= h;<;(t) (-T/2 st s T / 2 )  , 
(9.77) 
-TI2 
where the time region is shifted from [O,T] to [-T/2,T/2]. Let us extend T to 
-. Then (9.77) becomes 
J*R (t - T)&j(T)dT = hjCi(t) 
(- 
I t  I+..) . 
(9.78) 
Since (9.78) is the convolution integral of R ( I )  and Si@), the Fourier transform 
of this equation becomes 
-00 
$o’o)E;o’o) = h;E;o’o), 
(9.79) 
where A(jw) and E,(jo) are the Fourier transforms of R (I) and ci(t). Particu- 
larly, A(jo), the Fourier transform of the autocorrelation function of a random 
process, is known as the power spectrum of the random process x(t). 
In order to solve (9.79) for E;Uo) and hi given Ujw), we must find a 
function of jo which becomes the same function, except its size, after it is 
multiplied by 8(jo). Assuming that $0’0) is nowhere flat, such a function 
must be an impulsive function as 
(9.80) 
- 
=;(jo) 
= 6(o - 0;) 
which corresponds to, in the time domain, 
< ; ( I )  = pt 
. 
(9.81) 
Then, (9.79) becomes 
bo’o;) = hi . 
(9.82) 
Since (9.80) with any value of oi is the solution of (9.79), we may vary oi 
from --m to 00 continuously (or with an extremely small increment). As a 
result, we have an infinite number of eigenvalues and eigenfunctions. Also, 
because o; 
takes negative values as well as positive values, the K-L expansion 
of (9.58) must be modified to 
+== 
x(t) = 
y;e’”;‘ 
, 
, 
=-m 
(9.83) 
where mi = -0;. With o, 
changing continuously form - 
to +oo, the summa- 
tion of (9.83) is replaced with an integration as 

422 
Introduction to Statistical Pattern Recognition 
(9.84) 
where yi = y(oi)Ao/2n. Note that (9.84) is the inverse Fourier transform. 
Thus, when a random process is stationary, the basis functions of the K-L 
expansion become the complex exponential, dW, 
and the coefficient, y(o) (or 
y;), is the Fourier transform of x(t). 
Equation (9.83) may be modified to an expansion with real basis func- 
tions and real coefficients as follows. Since the ith and 4th basis functions, 
, are mutually conjugate, their coefficients, y-; and y;, are also 
conjugate for a real x ( t )  from (9.60). Therefore, combining these two terms, 
(9.83) becomes 
eJw and e-Jmi' 
m 
x(r) = yo + c 2  I y; Icos(o;t + Ly;) , 
(9.85) 
i=l 
where I yi I and Lyi are the magnitude and angle of a complex variable yi. 
As far as the eigenvalue is concerned, two eigenfunctions, eJw" and 
, carry the same eigenvalue, because the power spectrum of a real random 
e-Jw,r 
process is an even and real function with A(joi) = h(-joi). Therefore, after 
combining these two eigenfunctions, cos(oit + Lyi) of (9.85) carries the eigen- 
values of A&). 
K-L expansion for a finite time duration: When the time domain is 
limited to a finite duration, the above conclusion is no longer true but still 
approximately true. When ~ ( t )  
is time-sampled at n points, 0, At,. . . ,(n-1)At 
for the duration of T = nAt, the autocorrelation function is obtained from 
(9.62) and (9.76) at (2n-1) points in the time domain [-T,T] with the same 
sampling rate, as shown in Fig. 9-5. Even if x ( t )  is shifted along the time axis, 
there is no effect on R(I). As known in the discrete Fourier transform, these 
(2n-1) sampling points in the time domain induce (2n-1) sampling points in 
the frequency domain. Since the duration of R ( f )  is (2n-l)Ar, the sampling 
rate in the frequency domain is 

9 Feature Extraction and Linear Mapping for Signal Representation 
423 
0 
At 
2At 
(n-1 )At ntt 
T 
' 
' t  
-T 
-(n-1)At 
-2At 
-At 
0 At 
2At 
(n-1)At T 
I 
c
,
 > W  
-nuo 
-(n-l)oo 
-2wo 
-00 0 wo 20, 
(n-110, 
n0, 
Fig. 9-5 Relationship between the sampling points of x(t), R ( t )  and $(jo). 
2nn 
- n  
(2n-I)At 
(2n-l)T 
T 
-- 
-
.
 
2n 
- 
wo = 
- 
(9.86) 
Since two eigenfunctions eJoCr and 
for i z 0 are combined to form 
a real eigenfunction as in (9.85), only n real eigenfunctions and eigenvalues are 
obtained as 

424 
- 
in 
sin- n+l 
in 71 
sin- n+l 
Introduction to Statistical Pattern Recognition 
ei(t) zCos(i%t + ei) , 
(9.87) 
h; "=$(jio()) 
(9.88) 
(i =0, 1 , .  . .,n-I) . 
Example 4: The autocorrelation matrix of a stationary process has a toe- 
plitz form. A special toeplitz form has known eigenvalues and eigenvectors 
expressible in closed form as follows [4]. 
x 1  
l a  1 
0 
1
.
 
1 
3 
l a  
in 
n+l 
= [a + 2cos-] 
i = 1,2,. . . ,n 
in 
sin- n+l 
in n 
sin- n+l 
(9.89) 
In order to see how closely (9.87) and (9.88) approximate (9.89), let us 
compute the eigenvalues and eigenvectors of the given toeplitz matrix by 
(9.88) and (9.87). Since R ( k )  of (9.89) is a for k = 0, 1 for k = -1 and +1, 
and 0 for other k's, the discrete Fourier transform of R (k) with A? = 1 or T = n 
becomes, from (9.86) and (9.88) 
in 
= a + 2cos- n-112 
(9.90) 
which is close to the eigenvalues of (9.89) particularly for a large n. Since we 
see a difference between (9.89) and (9.90) in the denominators of the cosine 
function, let us set up the eigenvalues and eigenfunctions from (9.90) and 

9 Feature Extraction and Linear Mapping for Signal Representation 
425 
(9.87) as hi =a+ 2cosin/n* and &(k) = cos(ikn/n* +e,), where n* is an 
unspecified number closed to n. Then, it is easy to confirm that the left and 
right sides of (9.89) for k = 2, . . . ,n-1 are equal regardless of n* and 8, as 
(9.91) 
However, at the edges of the matrix of (9.89), the first and nth rows, different 
equations must be satisfied as 
a cos in 
n 7 
+ 0i 
i (n - 1)n 
n 
+ cos 
+ Cxcos 
n 
n 
for k = 1 , 
(9.92) 
= [a + 2cosT] 
ix COS 
n 
n 
for k = n . 
(9.93) 
Both (9.92) and (9.93) are satisfied by selecting n x  = n+l and 8, = -90 
O. 
Thus, the eigenvalues and eigenvectors become h, = a + 2 cosin/(n +1) and 
& ( k )  = sinikx/(n+l) as in (9.89). 
9.3 Estimation of Eigenvalues and Eigenvectors 
From the previous discussion, we realize that selecting features for signal 
representation by linear transformations requires a considerable amount of 
eigenvalue and eigenvector calculation. Theoretically, this is the end of this 
subject. However, there are many problems to be solved in order to apply the 
technique to real-life data. Some of the problems, which will be discussed in 
this section, are the following. 
( 1 )  The number. of sampling points .for- a rmdom pi.oces.7: In some 
applications of pattern recognition, the number of variables I I  is very much 

426 
Introduction to Statistical Pattern Recognition 
predetermined and cannot be controlled. However, in some other areas, partic- 
ularly in waveform analysis, we have to determine the number of sampling 
points n. Furthermore, in waveform analysis, n becomes fairly large, say in the 
hundreds, and computer time grows rapidly with n. Therefore, proper pro- 
cedures are needed to select the minimum possible number of sampling points 
while maintaining sufficient accuracy for representing the random process. 
(2) The number of samples: We always have to know how many sam- 
ples (or waveforms) are needed to ensure the accuracy of estimation of eigen- 
values and eigenvectors. 
Determining the Dimensionality 
In this subsection, we will develop a procedure for determining n, the 
number of time samples taken from a random process x(t). Before going into 
the subject, we need to discuss the perturbation theory of eigenvalues and 
eigenvectors. 
Perturbation theory: Let us derive first-order approximations for the 
eigenvectors and eigenvalues of a perturbed matrix in terms of those of the 
unperturbed matrix. 
Let Qo be a real, symmetric n x n matrix and let AQ be a real, sym- 
metric perturbation matrix. Let $; and hi, i = 1, . . . ,n, be the eigenvectors and 
eigenvalues, respectively, of Qo. Assume that the h,’s are distinct. We wish 
to obtain a first-order approximation of the eigenvectors and eigenvalues of Q 
in terms of the $,’s and hi’s, where 
Q = Q o + A Q  
(9.94) 
These may be obtained by retaining the terms of first order or lower of the 
equation 
(Qo + AQ)($; + A@;) = (A; + Ah,)($; + A$) 
3 
(9.95) 
where 
The resulting equation is 
(9.96) 
(9.97) 

9 Feature Extraction and Linear Mapping for Signal Representation 
427 
To calculate AA,, we premultiply (9.97) by $: 
and, since $:eo = A,$: 
and $:$J = 6,,, we have 
A h  q T A Q $ ,  . 
(9.98) 
Since n $,’s form a complete set of basis vectors, we can write A$, as a linear 
combination of the q J ’ 5  as follows: 
n 
(9.99) 
where 
hjj = $;A$, 
. 
(9.100) 
If we premultiply (9.97) by $: 
and rearrange, we have for i # j 
(9.101) 
To determine h,, we impose a first-order normalization condition on 4, + A$,, 
that is, we require 
11$, + A$,112 = 1 Zll$,112 + 24)TA$, = 1 + 2$TA$, 
(9.102) 
and it follows that 
$;A$, 
= h,, 
. 
(9.103) 
Noting that $TQ0$, = h, and 4)TQ0qJ = 0 for i # j ,  we summarize this 
section as follows: 
A, + A i ,  .$TQ$, 
(9.104) 
and 
(9.105) 
o 
for i = j .  
Effect of doubling sampling rate: Let us begin our discussion with a 
simple example. Let x ( t )  be sampled at four instants, t2,t4,th, and t g ,  as 
shown by the solid vertical lines in Fig. 9-6(a). Then, the autocorrelation 
matrix S has four eigenvalues whose magnitudes are indicated by the solid 

428 
Introduction to Statistical Pattern Recognition 
lines in Fig. 9-6(b). Let the number of sampling points be doubled to include 
t l ,  t 3 ,  t 5 ,  and t,. Now S has eight eigenvalues, as shown by the dashed lines 
in Fig. 9-6(b). The first four eigenvalues are close to the eigenvalues derived 
in the four-sampling case. The remaining four eigenvalues are those which 
result from doubling the sampling rate. If these four eigenvalues are small, the 
( b l  
T 
( a )  
Fig. 9-6 A typical (a) waveform and (b) its eigenvalues. 
error committed by deleting the corresponding features is also small. If the 
new features are unimportant, the features derived in the four-sampling case 
adequately represent x(f). 
In general, suppose the dimension is 2n. Then, we would have a 2n x 2n 
autocorrelation matrix, S2". If the summation of some n of the 2n eigenvalues 
of SZJJ is small compared to the summation of all 2n eigenvalues, n is 
sufficiently large. 
However, the computation of eigenvalues is time- 
consuming. Therefore, we introduce a simpler test for n by using the perturba- 
tion result. 
Let the elements of the 2n-dimensional vector X2" be ordered so that 
X2" =[x(t2)x(t4). . . X ( t 2 " ) X ( t I ) X ( t 3 ) .  . .x(t2JJ-,)1T=[X:~X~TlT 
, 
(9.106) 
where e stands for even and d for odd. Then, S2" is given by 
For large n, S7, , Sy2, and ST2 are nearly equal. We write S2" as 
Now if 

9 Feature Extraction and Linear Mapping for Signal Representation 
429 
a" 
= [$;. . . $3 
(9.109) 
is the eigenvector matrix of S;, , then 
(9.1 10) 
is the eigenvector matrix of Sp. Since AS2" is small for large n, the eigen- 
values of 5," are given approximately from (9.104) by the diagonal elements 
of 
r 
1 
where 
I 
2 
G;, = -@"T(s;, 
+ s;, 
+ s;: 
+ S$2)Qf1 , 
1 
2 
G;* = -@'"T(S;, - s;, 
+ s;; - S!,)@I1 , 
(9.1 1 1 )  
(9.1 12) 
(9.1 13) 
and 
(9.1 14) 
1 
2 
G22 = -@"T(S;, 
- s;, 
- s;: 
+ S%)@," . 
We define a criterion J,, as [5] 
trG!j2 
tr(G7, + G ; 2 )  
Jll = 
(9.1 15) 
That is, J,, is approximately the ratio of the sum of the n smaller eigenvalues of 
S'" 
to the sum of all 2n eigenvalues. If J, << 1, n samples are sufficient, and 
the assumption concerning the smallness of AS2" is reinforced. Using (9.1 12), 
(9.1 14), and the orthonormality of W, we can rewrite J,, as 
(9.1 16) 
Or, recalling (9.106) and (9.1071, we can write J,, in terms of the autocorrela- 
tion function, R (r, z), of x(t) as 

430 
Introduction to Statistical Pattern Recognition 
(9.117) 
The advantage of this procedure is that Jn can be obtained from the autocorre- 
lation matrix directly without computing the eigenvalues. 
In stationary processes, R(t2i,t2i)=R(t2i-I,t2i-I)=R(0), 
and R(t2;-l,t2;) 
= R (t2i-t2i-l) = R(TI(2n)). Therefore, 
(9.1 18) 
Example 5: Let us calculate J,, of (9.1 18) for R(T) = exp(-lTI) and 
T = 1. In this example 
(9.1 19) 
If 2n = 8, we have J,,a.06 which indicates that four eigenvalues of S are very 
small compared to the other four. Thus, we would expect to gain little addi- 
tional characterization of x(r) by increasing n further. 
Example 6: Let R (T) be a triangular function as 
(121 > T o ) .  
If T <To, J,, is given by 
T 
4nT0 
J,, = - 
. 
(9.120) 
(9.12 1) 
The result of this example is useful in problems where stationarity may be 
assumed, but R (T) is unknown. We assume the triangular form and determine 
the necessary n for a given T. 

9 Feature Extraction and Linear Mapping for Signal Representation 
43 1 
Estimation of Eigenvalues and Eigenvectors 
Moments of eigenvalues and eigenvectors: Having chosen n, our next 
task is to estimate the eigenvalues and eigenvectors hi and Qj (j = 1,. . . , n )  of 
the autocorrelation matrix S. To do this, we calculate the sample autocorrela- 
tion matrix S by 
(9.122) 
A 
A 
and calculate the eigenvalues and eigenvectors S and $j (j = 1, . . . ,n) of S. 
,. 
It is important to note that A,, 
and +j are estimates of hj and Qj and that 
they are random variables and vectors. They are functions of X I ,  . . . , X N .  In 
this section, we shall show approximate formulas for the expected values and 
variances of these estimates. Using these formulas, we can determine a value 
of N such that the estimates are sufficiently accurate. 
The statistics of the eigenvectors and eigenvalues of a matrix of random 
variables have been studied previously [6-71. The general approach is to calcu- 
late the distribution of 6, and from this find the distribution of the eigenvectors 
and eigenvalues. 
tions (9.104) and (9.105) to express 6; and A,,, that is, 
A 
However, since S 3 for N sufficiently large, we may use the approxima- 
(9.123) 
jti 
and 
First, we consider the expected value of the estimate. 
Since 
S = E (  XXT}, the expected value of i 
of (9.122) becomes 
(9.125) 
Therefore, 

432 
Introduction to Statistical Pattern Recognition 
It follows from (9.123) and (9.124) that 
E { & }  3; and E(L) E X j .  
(9.127) 
Thus, the estimates are unbiased when only the first order approximations are 
used. This is due to the fact that the estimate of S is unbiased. However, as 
seen in (5.3) and (5.4), the bias comes from the second order term of the 
approximation, while the variance comes from the first order. Furthermore, 
since the eigenvalues and eigenvectors are functions of S, (5.18) suggests that 
the biases must be proportional to 1/N. Thus, 
(9.128) 
where yr; are pi are the functions of S, although the functional forms are too 
complex to determine. The asymptotic values can be obtained experimentally 
by the procedure of (5.6). 
From (5.19), the variances are also proportional to l/N. Since the vari- 
ances can be obtained from the first order approximation as in (5.4), their 
approximated values may be computed as follows. The variance of &, and the 
covariance matrix of 0; are given by 
1 
jti k t i  
(9.130) 
Or, as a simpler alternative of (9.130), we may look at the mean-square error 
between 
and $; as 

9 Feature Extraction and Linear Mapping for Signal Representation 
433 
(9. I3 1) 
/ t i  
Note that both (9.129) and (9.13 1) are expressed in terms of E { 
1. 
Normal cases: When X is normally distributed, E{(@:$@,)*) can be 
computed as follows. 
Since k is given by (9. I22), 
where 
Yiic = 
(9.132) 
(9.133) 
If both sides of (9.132) are squared and the expectation is taken, the result is 
I* 
since the X,’s are independent. Now 
and (9.134) may be rewritten as 
(9.136) 
The second subscript on y is dropped since the Xi's are identically distributed. 
When X is normally distributed with zero mean, the y,’s are also normal 
with E { y , }  = O  from (9.133) and E ( y , y , )  =A,&,, from (9.135). This means 
that the y, ‘s are mutually independent. Therefore, 

434 
Introduction to Statistical Pattern Recognition 
(9.137) 
Inserting (9.137) into (9.136), 
(9.138) 
1 
N 
E { ( @ ; s @ j ) 2 ]  
= h$, + -(h,26, + h,hj) . 
Substituting (9.138) into (9.129) and (9.13 l), 
(9.139) 
2 
N 
Var(&j :-A?, 
(9.140) 
The variance of the eigenvalue, normalized by h:, is approximately 2/N 
regardless of the value of h,. On the other hand, the variation of 6, depends 
on how close some of other eigenvalues are to h,. When h//h,Zl, y, of (9.140) 
can become very large. This seems to be a serious problem. However, this 
problem is not as critical as it appears to be. Let us consider a simple 3- 
dimensional example in which h, = h2 = 1 and h3 = 0.1. Applying the K-L 
expansion, we discard Q3 and map down X onto the two-dimensional subspace 
spanned by @ I  and Q2. The covariance matrix of y I  = @TX and y2 = @;X is I, 
and we cannot identify the principal axes of the distribution in the Y-space. In 
fact, @ I  and 
are indefinite, and (9.140) indicates that the variations of $I and 
$2 are infinite. However, in this case, we do not need to obtain the accurate 
estimates of @ I  and q2 individually, as long as the subspace spanned by @ I  and 
$2 is accurately estimated. 
,. 
,. 
,. 
Example 7: We now present a numerical example that illustrates some 
interesting points. 
Let x ( f )  be a stationary, normal random process with 
R ( T )  = exp(-a Irl) . 
(9.141) 
If x(f) is time-sampled at t = ,T/n (, = 0, . . . ,/?-I). 
the autocorrelation matrix 
S becomes a matrix whose element s ,,, is 

9 Feature Extraction and Linear Mapping for Signal Representation 
435 
(9.142) 
where 
p = exp(-aT) . 
(9.143) 
The error coefficients of (9.140) are determined by the eigenvalues of S. 
By varying p and n of (9.142), we have a family of S matrices. Let us then 
examine the error coefficients of various matrices in the family. 
(1) Fixed p: For each value of n, we have n error coefficients, 
y,, . . . ,yfI. We order y; according to decreasing magnitude of hi. 
Figure 9-7 is a plot of yi for p = 0.1 [ 5 ] .  The variation of the eigenvec- 
tor estimate tends to increase as the value of the corresponding eigenvalue 
decreases. Therefore, a larger number of samples is needed to estimate these 
eigenvectors with smaller eigenvalues. Fortunately, the eigenvectors with 
smaller eigenvalues are less important for signal representation, and thus the 
accuracy of the estimates is not as critical as the one for the eigenvectors with 
larger eigenvalues. The range of the error coefficients is on the order of 100. 
(2) Vuriarion ofrhe largest coef3riicient: Suppose we must measure all of 
the eigenvectors with a certain accuracy. Then, when the dimension is n, 
ymax = max( y, , . . . ,yf, } is the constraining factor. Thus, the variation of ymax 
with n indicates how the sample size must grow with n to maintain a fixed 
accuracy. 
Figure 9-8 shows the variation of ymax with II for various values of p. 
We see that ymax grows roughly as n 2  [5]. 
Computer Projects 
1. 
Generate 50, 100, and 200 samples from a normal distribution with zero 
mean and the covariance matrix A of Data I-A. For each sample size, 
calculate the sample covariance matrix and their eigenvalues. Repeat 
this experiment IO times and average the estimated eigenvalues. Using 
the estimation technique of (5.6), find the asymptotic values for these 
eigenvalues. 
(a) 
2- 
Compute the eigenvalues and eigenvectors of (4.126) where n = 10 
and p = exp(-0. 1 ). 

436 
100.0 
qo.0 
1.0 
Introduction to Statistical Pattern Recognition 
0. 0 
2 
4 
6 
a 
!O 
42 
14 
Number of dimensions, n 
Fig. 9-7 Error coefficients for p = 0.1. 

9 Feature Extraction and Linear Mapping for Signal Representation 
437 
100.0 
I 
Number of dimensions, n 
Fig. 9-8 Value of the maximum error coefficient. 

438 
Introduction to Statistical Pattern Recognition 
(b) 
Compute the power spectrum of an autocorrelation function 
R (t) = exp(- I t I ). Assuming that the random process is time- 
sampled in 0 I t  < 1 with increment of 0.1, determine the sampling 
points in the power spectrum. Compare the eigenvalues and eigen- 
vectors obtained in (b) with the ones obtained in (a). 
3. 
4. 
Repeat Project 2 for R ( t )  = 2/(20r2 + 1). 
Repeat Example 7 to obtain Figs. 9-7 and 9-8. 
Problems 
1. 
The density function of a two-dimensional random vector X consists of 
four impulses at (0,3), (O,l), (l,O), and (3,O) with probability of 1/4 for 
each. 
(a) 
Find the K-L expansion. Compute the mean-square error when 
one feature is eliminated. Compute the contribution of each 
impulse point to the mean-square error. 
Repeat (a) without subtracting the mean vector. That is, express X 
by the summation of two basis vectors, optimize the basis vectors 
without subtracting the mean vector, and compute the mean-square 
error when one vector is eliminated. 
2. 
Let X"' and X"' 
be samples from ol and 0 2  respectively. The 
(b) 
between-class scatter matrix is defined by 
Sh = E((X"' - X(?')(X(1) - X(Z))T} 
Find Sh in terms of the expected vectors and covariance matrices of two 
classes. 
Find the linear orthonormal transformation from an n-dimensional X 
to an m-dimensional Y by maximizing trS,,. 
Calculate E { -Inp I ( X )  I o2 
] where p,(X) (i = 1.2) is a normal distribu- 
tion with the expected vector M, and covariance matrix Z,. 
Assuming M I  = M 2  and C, = I ,  find the linear orthonormal transfor- 
mation from an n-dimensional X to an m-dimensional Y which maxim- 
izes E {  -InpI (X) 
I * 
}. 
3. 

9 Feature Extraction and Linear Mapping for Signal Representation 
439 
x ( t )  
Linear filter 
- - 
h (t,Z 1 
s(t) 
4. 
A stationary process x ( t )  is observed in 0 If < 1. The mean and auto- 
correlation function of x(r) are given by 0 and exp(-4 I t I). Find the 
necessary sampling points which assure approximately that the eigen- 
values larger than 10% of the largest eigenvalue are retained. 
The figure shows a block diagram of a linear filter problem, where s (t) is 
the original signal, x ( f )  is the observed signal with s(r) and noise n(t) 
added, and i(t) is the estimate of s (t). The impulse response h (t, Z) of 
the optimum linear filter is found by minimizing 
5. 
r 
k ( t )  
r 
6. 
The autocorrelation function of a Poisson process is given by 
ht + h2tT for T >t and h~ + h2tZ for t >z. 
Assuming 0 I t ,  z IT, find the sampling interval or the number of 
sampling points to assure that, even if we double the sampling rate, the 
summation of the newly generated eigenvalues is less than E% of the 
summation of total eigenvalues. Find the necessary number of sampling 
points to assure E = 0.1 % for h = 1 and T = 1. 
A stationary process is normally distributed in [0, I ]  with the covariance 
function of exp(-l t I). Assuming that the process is time-sampled at 10 
points, find the necessary number of samples (waveforms) to assure 
7. 
E(ll$l" -@l"1l21 9 . 1 .  

440 
References 
Introduction to Statistical Pattern Recognition 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
T. Y. Young, The reliability of linear feature extractor, Trans. IEEE 
Computers, C-20, pp. 967-97 1, 197 1. 
H. L. VanTrees, “Detection, Estimation, and Modulation Theory: Part 
I,” Wiley, New York, 1968. 
A. 
Papoulis, 
“Probability, 
Random 
Variables, and 
Stochastic 
Processes,” McGraw-Hill, New York, 1965. 
B. Noble and J. W. Daniel, “Applied Linear Algebra (Second Edition),” 
Prentice-Hall, Englewood Cliffs, New Jersey, 1977. 
K. Fukunaga and W. L. G. Koontz, Representation of random processes 
using the finite Karhunen-Mve expansion, Inform. and Contr., 16, pp. 
J. H. Wilkinson, “Algebraic Eigenvalue Problem,” Oxford Univ. Press, 
London and New York, 1965. 
T. W. Anderson, “An Introduction to Multivariate Statistical Analysis,” 
Wiley, New York, 1962. 
85-101, 1970. 

Chapter 10 
FEATURE EXTRACTION AND LINEAR 
MAPPING FOR CLASSIFICATION 
When we have two or more classes, feature extraction consists of choos- 
ing those features which are most effective for preserving class separability. 
Class separability criteria are essentially independent of coordinate systems, 
and are completely different from the criteria for signal representation. Furth- 
ermore, class separability depends not only on the class distributions but also 
on the classifier to be used. For example, the optimum feature set for a linear 
classifier may not be the optimum set for other classifiers for the same distribu- 
tions. In order to avoid this additional complexity, let us assume that we seek 
the optimum feature set with reference to the Bayes classifier; this will result in 
the minimum error for the given distributions. Then, class separability 
becomes equivalent to the probability of error due to the Bayes classifier, 
which is the best we can expect. 
Therefore, theoretically speaking, the Bayes error is the optimum meas- 
ure of feature effectiveness. Also, in practice, the Bayes error calculated exper- 
imentally is one of the most popular criteria. That is, having selected a set of 
features intuitively from given data, estimate the Bayes error in the feature 
space by the procedures discussed in Chapter 7. 
A major disadvantage of the Bayes error as a criterion is the fact that an 
explicit mathematical expression is not available except for a very few special 
44 1 

442 
Introduction to Statistical Pattern Recognition 
cases and, therefore, we cannot expect a great deal of theoretical development. 
In Chapter 3, we showed that, even for normal distributions, the calculation of 
the Bayes error requires a numerical integration, except for the equal covari- 
ance case. 
In this chapter, several criteria which have explicit mathematical expres- 
sions will be discussed. These expressions are derived from some physical 
notions. However, the reader should be reminded that, whenever a criterion is 
proposed, the performance of the criterion has to be discussed in relation to the 
Bayes error. 
10.1 General Problem Formulation 
Difference Between Signal Representation and Classification 
Feature extraction for classification which will be discussed in this 
chapter is different in many respects from feature extraction for signal 
representation in Chapter 9, particularly in the criteria to be used and in the 
transformations to be allowed. 
Criteria: As an example, let us look at the distribution of height and 
weight for males as in Fig. 10-1. Since these two variables are highly 
correlated (a taller person tends to be heavier and vice versa), the distribution 
shows a football shape. As discussed in Chapter 9, the principal axis 
with a 
larger eigenvalue is a better vector than $2 to represent the vectors of this dis- 
tribution. That is, the selection of $I produces a smaller mean-square error of 
representation than the selection of q2. The same is true for the female distri- 
bution, and even for the mixture of two distributions. However, as seen in Fig. 
10-1, if the two distributions are mapped onto $,, the marginal density func- 
tions are heavily overlapped. On the other hand, if they are mapped onto 0 2 ,  
the marginal densities are well separated with little overlap. Therefore, for 
classification purposes, $2 is a better feature than 
preserving more of the 
classification information. 
The same argument can be applied to the problem of classifying two dif- 
ferent human races. In order to describe a human being, we may use such 
characteristics as two eyes, one mouth, two hands, two legs and so on. How- 
ever, none of these features are useful for classifying Caucasians and orientals. 

10 Feature Extraction and Linear Mapping for Classification 
x p :  weight 
t 
443 
Fig. 10-1 An example of feature extraction for classification. 
Thus, feature extraction for classification cannot be discussed as a simple 
extension of Chapter 9. In particular, the criteria to evaluate the effectiveness 
of features must be a measure of the overlap or class separability among distri- 
butions, and not a measure of fit such as the mean-square error. 
Transformations: When feature extraction for signal representation was 
discussed, we limited our discussion only to orthonormal transformations, 
because the shape of the distribution had to be preserved. The mean-square 
error is a coordinate-dependent criterion except for orthonormal transforma- 
tions. On the other hand, the overlap or class separability among distributions 
is invariant under any nonsingular transformation, including both linear and 
nonlinear mappings. However, any singular transformation maps X onto a 
lower-dimensional Y, losing some of the classification information. Therefore, 
we can present feature extraction for classification as a search, among all possi- 
ble singular transformations, for the best subspace which preserves class separ- 
ability as much as possible in the lowest possible dimensional space. 

444 
Introduction to Statistical Pattern Recognition 
Ideal Features for Classification 
It is always advantageous to know what are the best features for 
classification, even though those features might be too complex to obtain in 
practice. Since the Bayes classifier for the L-class problem compares a pos- 
teriori probabilities, q I (X), . . . ,qL(X), and classifies X to the class whose a 
posteriori probability is the largest, these L functions carry sufficient informa- 
tion to set up the Bayes classifier. Furthermore, since xf.=,qj(X) = l, only 
(L-1) of these L functions are linearly independent. Thus, these (L-1) 
features are the smallest set needed to classify L classes. Also, the Bayes error 
in this (L-1)-dimensional feature space is identical to the Bayes error in the 
original X-space. That is, by the transformation of y j  = qi(X), i = 1, 
. . . ,(L-1), from an n-dimensional space to an (L-1)-dimensional space, no 
classification information is lost. Thus, we call { q I (X), 
. . . ,qL-l (X)) the ideal 
feature set for- classification. Figure 10-2 shows the distributions of three 
classes in the original X-space as well as in the ideal feature space. No matter 
Fig. 10-2 Mapping to the ideal feature space. 
how the distributions are in the X-space, the three classes of the ideal features 
are distributed in the two-dimensional plane to satisfy y I  + y2 + y3 = 1. Since 
the Bayes classifier classifies X according to the largest qi(X), 
the Bayes 
classifier in the feature space becomes a piecewise bisector classifier which is 
the simplest form of a classifier for the three-class problem. 
Figure 10-3 shows how the ideal features are translated to other forms, 
depending on the conditions. The features of the second line, Inpj(X), are 
obtained by taking the logarithm of the first line. Note that the term Inp(X) is 
common for all classes and is irrelevant to classification. Also, in feature 

10 Feature Extraction and Linear Mapping for Classification 
445 
FEATURE 1 
FEATURE 2 
FEATURE 3 
1 T - 1  
1 
T - 1  
Fig. 10-3 The ideal features. 
+,XC 
X+7M,CM1 
extraction, additive and multiplicative constants do not alter the subspace onto 
which distributions are mapped. Therefore, the term lnPi is not included in the 
feature function. If the distributions are normal, the quadratic equations of X in 
the third line are the ideal features. Furthermore, if the covariance matrices of 
all classes are the same, the linear functions of X in the fourth line become the 
ideal features. 
10.2 Discriminant Analysis 
The Bayes error is the best criterion to evaluate feature sets, and a pos- 
teriori probability functions are the ideal features. However, in practice, a pos- 
teriori probability functions are hard to obtain, and their estimates, obtained 
through nonparametric density estimation techniques of Chapter 6, normally 
have severe biases and variances. As for the Bayes error, we can estimate it 
and use it to evaluate given feature sets, as discussed in Chapter 7; however 
this is time-consuming. Unfortunately, the Bayes error is just too complex and 
useless as an analytical tool to extract features systematically. Therefore, we 
need simpler criteria associated with systematic feature extraction algorithms. 
There are two types of criteria which are frequently used in practice. 
One is based on a family of functions of scatter matrices, which are conceptu- 
ally simple and give systematic feature extraction algorithms. The criteria used 
measure the class separability of L classes, but do not relate to the Bayes error 
directly. The other is a family of criteria which give upper bounds of the 
Bayes error. The Bhattacharyya distance of (3.152) is one of these criteria. 

446 
Introduction to Statistical Pattern Recognition 
However, the criteria are only for two-class problems, and are based on nor- 
mality assumption. 
Scatter Matrices and Separability Criteria 
In discriminant analysis of statistics, within-class, between-class, and 
mixture scatter matrices are used to formulate criteria of class separability. 
A within-class scatter matrix shows the scatter of samples around their 
respective class expected vectors, and is expressed by 
L 
L 
S", = C P ; E  {(X-M,)(X-M;)T Io; ] = CP;Z; . 
(10.1) 
On the other hand, a between-class scatter man-ix is the scatter of the expected 
vectors around the mixture mean as 
i=l 
i=l 
L 
Sh = ~P;(M;-A4")(Mi-MlJ)~, 
(10.2) 
i=l 
where M o  represents the expected vector of the mixture distribution and is 
given by 
( 10.3) 
The mixture scatter matrix is the covariance matrix of all samples regardless of 
their class assignments, and is defined by 
s,, = E{(X-M,)(X-MJ) = s,. + Sh . 
All these scatter matrices are designed to be invariant under coordinate shifts. 
(10.4) 
In order to formulate criteria for class separability, we need to convert 
these matrices to a number. This number should be larger when the between- 
class scatter is larger or the within-class scatter is smaller. There are several 
ways to do this, and typical criteria are the following: 

10 Feature Extraction and Linear Mapping for Classification 
447 
(3) 
J 3  = trSl - p(trS2 - c) , 
(10.7) 
trS I 
trS2 ' 
(4) 54 = - 
(10.8) 
where S I  and S2 are one of s h ,  S,,., or S,,,. The following remarks pertain to 
these criteria: 
(1) Many combinations of s h ,  S,,., and S, for S I and S2 are possible. 
Typical examples are { s h , & } ,  [Sh,Sm}, 
and [SM,,Sm} 
for [SI,Sz ). 
(2) The optimization of J I  is equivalent to the optimization of 
&(A'S ,A) with respect to A under the constraint A'S2A = I where A is an 
nxrn transformation matrix [see Problem 21. The same is true for J 2 .  
(3) For J 2 ,  S h  cannot be used, because the rank of Sh is (,!-I) 
from 
(10.2)and(10.3)and 
=Ofor(,!-l) 
< n .  
(4) As we will discuss later, the optimization of J I  and J 2  results in the 
same linear features. Recall that the trace and determinant criteria produced 
the same linear features for signal representation in Chapter 9. Furthermore, 
these optimal features are the same no matter which combination of s h ,  Sw, and 
S, is used for SI and S2. Therefore, we may choose any combination for our 
discussion without worrying about which combination is better. 
(5) The logarithm of the determinant is used for J 2  in this book, 
although many references cite IS I I / I S I. By using the logarithm, J 2  in an n- 
dimensional space can be computed by adding the J 2  values of individual 
features, if the features are independent. This property is called the additive 
property of independent features. 
(6) When J 3  is used, trSl is optimized, subject to the constraint 
trS2 = c' [see Problem 31. That is, p is a Lagrange multiplier and c is a con- 
stant. 
(7) As we will discuss later, J I  and J 2  are invariant under any non- 
singular linear transformation, while J 3  and J4 are dependent on the coordinate 
system. 

448 
Introduction to Statistical Pattern Recognition 
In this book, we will discuss only the optimization of JI and J 2  in detail. 
Similar discussions are found in [I] for 53 and [2] for Jq. 
Optimum Linear Transformation 
Since it is very difficult, if not impossible, to discuss general nonlinear 
transformations, our discussion will be limited to linear transformations. A 
linear transformation from an n-dimensional X to an rn-dimensional Y (m < n) 
is expressed by 
Y =ATX, 
( 10.9) 
where A is an nxm rectangular matrix and the column vectors are linearly 
independent. However, contrary to the case for signal representation, these 
column vectors do not need to be orthonormal. Since all three scatter matrices, 
Sh, Sw, and S,, 
have the form of a covariance matrix, S I and S 2  in the Y-space 
can be calculated from S and S 2  in the X-space by 
sjY = A ~ S , ~ A  (i = 1,2) . 
(10.10) 
Thus, the problem of feature extraction for classification is to find the A 
which optimizes one of the J's in the Y-space. 
Optimization of J ,  : Let J I (rn) be the value of J I in an rn-dimensional 
Y-space. Then, 
Jl(rn) = tr(&$SIY) = tr((ATS2XA)-1(ATSIXA)) 
. 
(10.1 1) 
Taking the derivative of (10.1 1) with respect to A, by using (A.16), 
= -2 S2xA SSbS I YS& + 2 S IxA S$ . 
(IO. 12) 
aJlcrn> 
aA 
Equating (10.12) to zero, the optimum A must satisfy 
(SGS rx)A = A(S5b.S ly) . 
(IO. 13) 
Two matrices Sly and Szy can be simultaneously diagonalized to pLm 
and I,, 
by a linear transformation Z = BTY such that 
BTSlyB = Pn1 and 
BTS2YB = I,,, , 
(10.14) 
where B is an mxm nonsingular square matrix and B-' exists. 

10 Feature Extraction and Linear Mapping for Classification 
449 
It is easy to show that the criterion value is invariant under this non- 
singular transformation from Y to Z. 
Using (10.14), (10.13) is rewritten as 
(10.15) 
(10.16) 
or 
Equation (10.17) shows that the components of pnl and the column vectors of 
(AB) are the m eigenvalues and eigenvectors of S&Srx. Although both SIX 
and Sur are symmetric, S&SIx is not necessarily symmetric. However, the 
eigenvalues and eigenvectors of S&S Ix are obtained as the result of simultane- 
ous diagonalization of SIX and S Z x ,  as discussed in Chapter 2. As the result, 
the eigenvalues are real and positive, and the eigenvectors are real and ortho- 
normal with respect to SB. 
Since the trace of a matrix is the summation of the eigenvalues, 
where the h,’s and pi’s are the eigenvalues of S j i S  Ix and S$S I y  respectively. 
Since the pj’s are also the eigenvalues of S;iS Ix from (10.17), we can maxim- 
ize (or minimize) ./ I (m) 
by selecting the largest (or smallest) m eigenvalues. 
The corresponding m eigenvectors form the transformation matrix. 
The above argument indicates that, by projecting X onto the m eigenvec- 
tors of S&SIx, we can form an m-dimensional subspace which is spanned by 
these m eigenvectors. Then, J I (m) is the summation of the corresponding m 
eigenvalues. Further application of any mxm nonsingular linear transformation 
would not change the value of Jl(m). Therefore, we may conclude that the 

450 
Introduction to Statistical Pattern Recognition 
value of J,(rn) is attached to the subspace and is invariant regardless of the 
selection of the coordinate system in the subspace. Thus, it is appropriate to 
represent the subspace by the set of m eigenvectors of S&S 1
~
.
 
Comparing feature extraction for classification in this chapter with the 
one for signal representation in Chapter 9, it should be noted that both algo- 
rithms are identical, selecting the eigenvectors of a matrix as the optimum 
linear features. However, different matrices are used for different purposes: 
S;IS I for classification and the covariance matrix for signal representation. 
Two-class problems: Let J1 = tr(S,'Sb) 
be the criterion for two-class 
problems. For two-class problems, Sh becomes 
where P I M I  + P 2 M 2  = M o  is used to obtain the second line from the first. 
Since Sh of (10.20) is composed of one vector ( M 2 - M I ) ,  the rank of S h  is one. 
Since S,. is the averaged covariance matrix, it generally has full rank and S,' 
exists. Therefore, the rank of S,.'Sb is also one. That is, 
h l z O  and h 2 =  . . . =  h,,=O. 
(10.21) 
On the other hand, tr(SI;.'Sh) is the summation of these eigenvalues, 
hl = tr(S,lSh) = P , P ~ ( M ~ - M ~ ) ~ S ~ ' ( M ~ - M ~ )  
. 
(10.22) 
The corresponding eigenvector is 
(10.23) 
where the denominator is a constant selected to satisfy 11@, 11 = 1. The reader 
can confirm that S;!Sh@, = h l @ ,  is satisfied by using $I of (10.23), h, of 
(10.221, and Sh of (10.20). 
Equation (10.21) indicates that, for two-class problems, only one feature 
is needed, and the others do not contribute to the value of J I .  The mapping 
function is 
y, = @TX = ( . ( M ~ - M I ) T S , ! X ,  
(10.24) 

10 Feature Extraction and Linear Mapping for Classification 
45 1 
& ___ 
Feature 
extractor 
where L' is a constant. Note from (4.27) that (10.24) is the optimum linear 
classifier without including the threshold term, whenf(ql,q2,P '0: + P 2oz) is 
optimized with respect to V. If we were to select one vector to project two dis- 
tributions, the vector perpendicular to the optimum hyperplane should be the 
one. The above argument suggests that, by projecting two distributions onto 
V = Si! (M2-M I ), we can preserve all classification information, as long as the 
class separability is measured by tr(S;.' Sh). 
21 
Y1 ._ 
Discriminant 
* maximum 
> selector 
, 
selector 
function 
value 
L-class problems: Two-class problems can be extended to L-class prob- 
lems, still using J I  = tr(Si'Sh). Since the M,'s are related by (10.3), only 
(L-I) of them are linearly independent. Therefore, Sh of (10.2) has the rank 
(L-I), and subsequently the rank of S i l S h  is (L-I). This means that (L-1) 
eigenvalues of Si'sh are nonzero and the others are zero. Thus, without losing 
the criterion value, we can map X onto the (L-I)-dimensional subspace 
spanned by the (L -1) eigenvectors corresponding to these nonzero eigenvalues. 
Recall that the number of ideal features was (L-1). In order to classify 
L distributions, we need at least (15-1) features. The optimization of tr(S;'Sh) 
also produces (L -1) features without losing classifiability, as long as the 
classifiability is measured by tr(S;'S,). 
It is commonly considered that a pattern recognition system consists of 
two parts; a feature extractor and a classifier, as shown in Fig. 10-4. However. 
classifier 
A 
f 
\ 
Fig. 10-4 Structure of a classifier. 
when the classifier part is examined more closely, it is found to consist of a 
discriminant function selector, which chooses a posteriori probability functions 
for the Bayes classifier, and a maximum value selector, which decides the class 
assignment of the test sample. When we apply an optimization technique to 

452 
Introduction to Statistical Pattern Recognition 
find the smallest possible number of features with the largest possible value for 
a criterion, the mathematical procedure automatically combines the feature 
extractor and the discriminant function selector, and selects (L -1) features as 
the optimal solution. After all, the functions of these two blocks are the same, 
namely to reduce the number of variables. There is no theoretical reason to 
divide this operation into two stages. Particularly, when both blocks do linear 
transformations, the combination of two linear transformations is just another 
linear transformation. Generally, engineers like to see intermediate results 
showing how variables are transformed down step by step, and gain some phy- 
sical insights. However, we cannot expect an optimization procedure to divide 
the process into several steps and produce intermediate results. 
When an optimization procedure selects the (L -1) optimum features 
without reducing the criterion value, one would intuitively wonder why the 
number of classes alone should determine the number of features regardless of 
the distributions. However, once the mathematical process of optimization is 
understood, it may be recognized that the above conclusion is reasonable and 
sound after all. Indeed, we can select (L-1) features for the combination of 
the trace criterion, tr(S;!S,,), 
and linear transformation. Also, (15-1) a pos- 
teriori probability functions (the ideal features for classification) may be 
obtained by the nonlinear transformation, which minimizes the Bayes error. 
Since neither compromise nor approximation were used in the optimiza- 
tion process, the quality of the selected features depends solely on how accu- 
rately the criterion, tr(S;! Sh), measures the class separability. Generally speak- 
ing, tr(S;!S,,) 
is a good measure when the distributions are unimodal and 
separated by the scatter of means. However, if the distributions are multimodal 
and share the same mean as in Fig. 10-5(a), there is no scatter of M I  and M 2  
around M o .  Subsequently, tr(S;.'S,,) is not a good measure of class separabil- 
ity. For this case, we can find clusters, and treat this as a four-class problem 
rather than a two-class one. Then tr(S;!Sh) gives 3 good features. How to find 
clusters will be discussed in Chapter 11. The same is true for the case of Fig. 
10-5(b). This case is better handled as a six-class problem by finding three 
clusters from each class. However, the clustering technique is not so effective 
for the case of Fig. 10-5(c) in which unimodal distributions share the same 
mean. This case will be studied later in relation to the optimization of the 

10 Feature Extraction and Linear Mapping for Classification 
453 
(4 
(b) 
(C) 
Fig. 10-5 Examples for which tr(S;.]S,) does not work. 
Bhattacharyya distance. These examples suggest that, in order to utilize the 
feature extraction algorithm of tr(S;.] Sb) effectively, we need to study the data 
structure carefully and adapt our procedure accordingly. 
Same features for different matrices: So far, we have studied feature 
extraction by using the criterion of tr(S;'S I )  with S I = Sh and S 2  = Sw. In this 
section we will show that other combinations of S,,, S,,, and S,, for S I  and S 2  
give the same optimum features as the ones for S I = Sh and S 2  = S,.. 
Let us consider, as an example, 
J I = tr(S,l s ~ . )  
. 
(10.25) 
Since this criterion measures the averaged covariance matrix normalized by the 
mixture scatter matrix, the criterion must be minimized, contrary to the maxim- 
ization of tr(S;.]S,). 
According to (10.17), the minimization is achieved by 
selecting the eigenvectors of Si: S,. corresponding to the m smallest eigen- 
values. Let 0 and A be the eigenvector and eigenvalue matrices of S;:S,,. 
Then 
Si:S,,.@ 
= @ A  . 
( 10.26) 

454 
Introduction to Statistical Pattern Recognition 
SM,@ = S,,,@A . 
(10.27) 
Using S, = S,. + s h  from (10.4), (10.27) can be converted to 
(10.28) 
(10.29) 
(1 0.30) 
Equation (10.30) shows that @ is also the eigenvector matrix of S,'Sb, and its 
eigenvalue matrix is (A-' - I ) .  When the components of A, hi, are ordered 
from the largest to the smallest as 
A, 2.. . a,, 
, 
(10.31) 
the corresponding components of (A-' - I) are reversely ordered as 
1 
15 . . .  < - - l .  
1 -- 
AI 
A,, 
(10.32) 
That is, I$+ corresponds to the ith largest eigenvalue of S;/S,. as well as the ith 
smallest eigenvalue of si!&,. 
Therefore, the m eigenvectors of s;,'s,. 
corresponding to the m smallest eigenvalues are the same as the m eigenvectors 
of Si.' Sh corresponding to the m largest eigenvalues. 
for SI and Sz. 
Similar derivations are obtained for other combinations of S,,, SM,, 
and S,, 
Optimization of J z :  The optimization of 52 can be carried out in a simi- 
lar way as that for J I .  The criterion value in the m-dimensional Y-space is 
Jz(rn)=InlSlyl -lnISzYI 
= InIATSIxA I - InIATSzXA I . 
Taking the derivative of (10.33) with respect A by using (A.28), 
(10.33) 
( 10.34) 

10 Feature Extraction and Linear Mapping for Classification 
455 
Equating (10.34) to zero, the optimal A must satisfy 
(S&S 1x)A = A(S&S I * )  
. 
(10.35) 
Since (10.35) and (10.13) are identical, the feature extraction to optimize 
(10.35) gives the same result as the one to optimize (10.13). 
s ; $ ~ ~  
as 
The value of J 2 ( n )  can be expressed in terms of the eigenvalues of 
J 2 ( n )  = In hi + . . . + In h,, . 
(10.36) 
Therefore, in order to maximize (or minimize) J2(m), the m largest (or smal- 
lest) eigenvalues of S;,$ 
Ix and the rn corresponding eigenvectors are selected. 
Optimization of the Bhattacharyya Distance 
As discussed in Chapter 3, the Bhattacharyya distance of (3.152) is a 
convenient measure of class separability, if the number of classes is two. 
Furthermore, (3.152) gives an upper bound of the Bayes error, if the distribu- 
tions are normal. However, the optimization of this criterion is not so easy, 
because two different types of functions, trace and determinant, are combined 
in the criterion. Therefore, in this book, we present the optimum solutions 
only for special cases, and the suboptimum solutions for the general case. 
Optimization of p1: When CI =E2, the second term of the Bhatta- 
charyya distance disappears, and we need to maximize the first term only. 
From (3.152), p1 is 
(10.37) 
where E = (C, + C2)/2 and E is the same as S,. if P I  = P 2  = 0.5. Equation 
(10.37) is the same as tr(S,.'Sb) with Sh of (10.20), replacing S,. 
with 
and 
ignoring the multiplicative constant. Therefore, from the previous discussion, 
only one linear feature is needed to maximize this criterion, and the mapping 
function is, from (10.24) 

456 
Introduction to Statistical Pattern Recognition 
y = (M2-M1)T9X . 
(10.38) 
1 
= -{ln 
4 
1 
= -(ln 
4 
Optimization of pz: When M I = M2, the first term of the Bhattacharyya 
distance disappears, and we need to maximize the second term only. From 
(3.152), p2 is 
I 
l- XI +z2 
1 
2 
p2 = - In 
2 *= 
XT'(E~ + &)~5'(& +&)I - n In 4) 
E;'Zl + Xi1& + 21 I - n In 4) . 
(10.39) 
Therefore, the optimum linear transformation is found by maximizing 
J ( m )  = 1nI(ATX2A)-'(A7Z1A) + (ATZ,A))-'(ATZ2A) + 21, I 
( 10.40) 
where I, is the m-dimensional identity matrix while I of (10.39) is n- 
dimensional. As seen in the derivation of the third line of (10.39) from the 
second, I represents XT'Z;. Therefore, by the transformation, I is converted to 
(AT&A)-'(A7XjA) =I,. which is independent of A. 
Taking the derivative of (10.40) with respect A, 
where Xix = Z j ,  Eiy 
= ATXjxA, and [-] is the summation of three matrices in 
(10.40). Although it seems very complex to solve aJ(rn)/aA = 0 for A, the first 
( .) and second { .] of (10.41) can be made zero separately as follows. 

10 Feature Extraction and Linear Mapping for Classification 
457 
(C&CIX)A = A  ( Z & C I ~ )  for the first [ .) , 
(CiiZ2X)A = A (C;iC2y) for the second { .  ) . 
(1 0.42) 
(10.43) 
In order to satisfy both (10.42) and (10.43) simultaneously, A must be the 
eigenvectors of both C&CIX and C;i&. 
Since these two matrices are related 
by CifrCIX = (Ci;C=)-', 
they share the same eigenvector matrix and their 
eigenvalue matrices are A for Z&CIX and A-' for C;;Cz. 
Thus, by selecting 
m eigenvectors of C&CIX, we satisfy both (10.42) and (10.43) simultaneously, 
and can make dJ(m)/dA of (10.41) zero. The value of J ( n )  in the original n- 
dimensional space is 
I1 
(10.44) 
J ( n )  = Cln(h, + - 
+ 2), 
where the h,'s are the eigenvalues of C&ZIX. Therefore, in order to maximize 
J ( m ) ,  we must select the m eigenvectors of C&ZIX corresponding to the m 
largest (A, + l/h, + 2) terms. 
Each eigenvalue of C&CIx is the ratio of the 0,- and w2-variances along 
the corresponding eigenvector. If the o1 -variance is larger or smaller than the 
02-variance along $,, h, becomes larger or smaller than one. In either case, 
h, + l/h, + 2 becomes larger than 4, because x + 1I.v 22 for x 20. On the 
other hand, when two variances are the same, h, becomes one and 
h, + l/h, + 2 becomes 4 which is the smallest value for this term. Therefore, 
this algorithm is very effective in picking the features along which the vari- 
ances of two classes are different, as in Fig. 10-5(c). 
I 
I =I 
A, 
For dominant pl: When M I  f M 2  
and El f C 2 ,  there is no known pro- 
cedure available to optimize p of (3.152). Therefore, we must seek either an 
iterative process or a suboptimum solution to find A .  When the first term pI is 
dominant, we may use the following procedure to find the suboptimal solution. 
(1) 
Compute 
the 
eigenvalues 
and 
eigenvectors 
of 
~-I(M2-MI)(M2-MI)', 
hi and qj, where z = ( Z l  +Z2)/2. Since the rank of 
the matrix is one, only hl is nonzero and the other h,'s are zero. Use $I as the 
first feature and transform X to y I  = $TX. All information of class separability 
due to mean-difference is preserved in this feature. 

458 
Introduction to Statistical Pattern Recognition 
(2) By yi = $TX (i = 2, . . . ,n), map X onto the (n-1)-dimensional sub- 
space which is orthogonal to $, with respect to E. In this subspace, there is no 
information of class separability due to mean-difference. 
(3) In the (n-1)-dimensional Y-space, compute C&CIY and its eigen- 
values and eigenvectors, pj's and vi's. Note that the vi's are orthonormal 
with respect to CZy in the Y-space. 
terms, and transform Y to zi =vTY (i = 1,. . . m-1). 
(4) Select the vi's which correspond to the (m-1) largest (pi + l / p i  + 2) 
(5) Form an rn-dimensional vector as [y,z, . . . z ~ - ~ ] ' .  If desired, find 
the orthonormal basis vectors which are the eigenvectors of the covariance 
matrix of [yl z1 . . . z,,?-] 1'. 
For dominant pz: When the second term p2 is dominant, we may select 
the eigenvectors of C&:Clx as seen in the optimization of p2, while checking 
how much information of class separability due to the first term pI is distri- 
buted in these eigenvectors. The eigenvalue and eigenvector matrices of 
Z.;iZlx, A and a, 
satisfy 
@'CIX@ = A  and 
@Cur@ = I  . 
(10.45) 
Applying an nxn transformation Y = @'X, the Bhattacharyya distance can be 
rewritten as 
1 ibT(M2-M,)l2 
1 
1 
i=l 
4 
hi 
+ -(ln(hi + - 
+ 2) - ln4} 
( 10.46) 
The second term of (10.46) is the same as (10.44) except for a constant In4, 
and shows how the variance-difference of yi contributes to the Bhattacharyya 
distance. In addition, (10.46) includes the first term, the effect of the mean- 
difference in y j .  Thus, we can select the m eigenvectors of C&:Clx which 
correspond to the m largest [.I terms in (10.46). 
Divergence: The dhwgmce is another criterion of class separability 
similar to the Bhattacharyya distance. Most properties of the divergence can 
be discussed in terms similar to those used for the Bhattacharyya distance. 
In pattern recognition, the key variable is the likelihood ratio or minus- 
log-likelihood ratio of two density functions. Therefore, if we have some way 
to evaluate the density functions of the likelihood ratio for wI and w2, it is 

IO Feature Extraction and Linear Mapping for Classification 
459 
almost equivalent to evaluating the Bayes error. Unfortunately, this is not an 
easy task. The simplest version of this type of approach might be to use the 
expected values of the minus-log-likelihood ratio for w1 and 0 2  and to evaluate 
the class separability by the difference of the expected values of two classes. 
Thus, the divergence is defined as [3] 
Since we consider only expected values in the divergence, we cannot expect a 
close relationship between the divergence and the Bayes error. A closer rela- 
tionship can be obtained by normalizing the means by the standard deviations, 
but the expression might become very complicated. 
When two density functions are normal, the divergence becomes 
r 
- 
The form of (10.48) is close to that of the Bhattacharyya distance with first and 
second terms indicating class separabilities due to mean- and covariance- 
differences. The advantage of the divergence is that both the first and second 
terms are expressed by the trace of a matrix, while the Bhattacharyya distance 
is the combination of trace and determinant. Thus, it is sometimes easier to 
use the divergence for theoretical discussions. However, the weaker link to the 
Bayes error prevents the divergence from wider applications. 

460 
Introduction to Statistical Pattern Recognition 
10.3 Generalized Criteria 
The discussion of the previous section suggests that extracting features 
for classification is the same as finding the discriminant functions in classifier 
design. As seen in Fig. 10-4, the discriminant functions are the final analog 
outputs in a classifier before the decision-making logic is applied, and the 
number of discriminant functions, (L -l), is the smallest number needed for L- 
class classification problems. 
Furthermore, finding the optimum linear 
transformation is the same as designing the optimum linear classifier. Thus, 
the entire discussion of Chapter 4 can be directly applied to feature extraction. 
Two-Class Problems 
For two-class problems, we can map X onto a one-dimensional subspace 
of y1 = @:X, in which the criterion tr(S,!Sb) becomes 
where q i = E ( y l I o j ) = $ ~ M j ,  
q o = E ( y 1 ) = $ T M o ,  and 0' =Var(ylIwi) 
= $ T X ; ( I ~ .  The other criteria of scatter matrices also can be expressed as the 
functions of ql, q 2 ,  a!, and 0:. Therefore, as was already seen in Chapter 4, 
we may introduce the generalized criteria, f (ql,q2,0:,o;), and treat the cri- 
teria of scatter matrices as special cases. The optimization off (q1,q2,0:,oi) 
with respect to the mapping function was discussed extensively in Chapter 4, 
resulting in 
y1 = q I (X) for nonlinear mapping, 
(10.50) 
yI = ( M ~ - M ~ ) ~ [ S C ~  
+ (I-S)C~]-'X for linear mapping, 
(10.5 1) 
where 
(10.52) 
When f is a function of s h ,  S,,, and S,,, 
X I  and & never appear 
separately, but always appear in a combined form of S,, = P , X I  + P 222. Sub- 
sequently, after mapping to one-dimension, 0: and o; always appear in the 
same form of P ,o: + P 2 0 2  (= $:S,,J$~). 
Therefore, this particular family of 
criteria is expressed by f (q I ,q2,P lo: + P 20$), 
and s of (10.52) becomes P I .  

10 Feature Extraction and Linear Mapping for Classification 
46 1 
Thus, yI of (10.51) becomes (M2-M,)TS,’X which is the same as (10.24). 
L-Class Problems [4] 
The above argument can be extended to more general L-class problems 
[4-71. 
Class separability criteria: let f (M 
. . . ,ML,Sm) be a family of cri- 
teria for class separability. For simplicity, we assume in this section that the 
covariance matrices are always used in the combined form of Cf,,P,E, = S, . 
This criterion includes functions of Sh, S,,., and S, as special cases. The dis- 
cussion of this section can be extended to a more general criterion of 
f (M 
I ,  . . . ,ML,E~, 
. . . ,EL), but with more complexity. 
Feature extraction selects an (L-1)-dimensional 
Y (X) from an n- 
dimensional X by maximizing (or minimizing) the criterion. Without losing 
generality, we also assume for simplicity that the mixture means are zero vec- 
tors in both the X -  and Y-spaces. 
shift and nonsingular linear transformation, we whiten S,, such that 
Since our criterion of class separability is invariant under a coordinate 
f ( M 1 , .  . . , M L , S ~ ) = ~ ( D ~ , .  
. . , D L , ~ ) ,  
(10.53) 
where 
D, = 1 \ - 1 / 2 @ ~ ,  = s;//*~~ 
(10.54) 
and A and Q are the eigenvalue and eigenvector matrices of S,,. Further appli- 
cation of any orthonormal transformation keeps the value off the same as well 
as keeps the mixture scatter matrix I. This means that the value of fdoes not 
depend on the absolute position of each D,, but depends only on the relative 
positions of the D,’s. Since the relative positions of the D,’s may be specified 
by a set of DTD,’s for all possible combinations of i and j ,  the criterion can be 
rewritten as a function of the DTD,’s as 
( 1  0.55) 
where 
rfI = DTD, = MJS;:M, 
i 2j . 
( 10.56) 
Note that DTD, = DTD, and only the DTD,’s for i 2j are used. The criterion 

462 
Introduction to Statistical Pattern Recognition 
of (10.55) can be optimized by solving 
(10.57) 
where r; is the optimum value of rjJ. The same result can be obtained by 
optimizing another criterion J as 
L
i
 
; = l j = l  
J = x E  p . . ~ . .  
‘J 
‘J ’ 
(10.58) 
where 
Rewriting this criterion in matrix form 
where 
B =  
R =  
(10.59) 
(1 0.60) 
(10.61) 
( 10.62) 

10 Feature Extraction and Linear Mapping for Classification 
463 
6 = [D, 
. . .DJ, 
(10.63) 
T= 
[MI.. 
. M J ,  
(1 0.64) 
$h = VIB Vf . 
(10.65) 
Note from (10.60) that Ah is a generalized between-class scatter matrix, with B 
determined by the functional form of the criterion. 
Example 1: When f = tr(S,'Sh) is used, 
L 
f = tr(s,'Sh) = xPiM;S;;,'Mi . 
i=l 
Therefore, 
( 10.66) 
(10.67) 
( 10.68) 
Example 2: When f = tr(S;.'Sh) is used, we could find aglar, after 
replacing S,. by S,,-sh. 
However, it is simpler to replace the criterion 
f (MI,. . . ,ML,Sm) 
by another one f (M . . . ,ML,SM,) 
and repeat the same dis- 
cussion. The result becomes J = tr (S;! A h )  with the same A h  as (10.65). 
Optimum nonlinear features: When X is mapped onto an (L-1)- 
dimensional Y, the class separability in the Y-space is measured by 
Jy = tr(S;/y&y), where SmY and bhY are the (15-l)x(L-1) mixture and general- 
ized between-class scatter matrices in the Y-space. The optimization of the cri- 
terion with respect to the mapping function Y ( X )  can be carried out by solving, 
for Y ( X ) ,  
where aJ*,/aS,,,, is defined as in (A.40) for a symmetric matrix SnlY, and 
M i y  = jY(X)pi(X)dX , 6Miy = j6Y(X)pi(X)dX , 
(10.70) 

464 
J 
-, 
I 
0 
0 
PL - 
Introduction to Statistical Pattern Recognition 
SmY = j'y(x)y7(x)p (XMX 1 
a m y  = J[Y (X)6Y T ( X )  + 6Y (X)Y T(X)]p (X)dX . 
(10.71) 
Substituting (10.70) and (10.71) into (10.69), 
(10.72) 
1 
L aJy 
aJ; 
65, = J*Y7(X) [ ~ ~ P ; ( X I  
+ 2-YYX)p(X) 
dX = 0 . 
JS",Y 
In order to satisfy (10.72) regardless of 6Y (X), the [.] term in the integrand 
must be equal to zero. Therefore, 
aJ; 
1 
aJY PAX) 
- Y ( X )  
= --x-- 
a s m y  
2 i = l  JMiy p ( X )  
1 
1 
3JY 
= - - Z - - q ; ( X )  
2 
pi aMiy 
where 
P =  
(10.73) 
(10.74) 
(10.75) 
QW = [(I 
. . . ~ L W I ~  
. 
(10.76) 
Note that qi(X) = Pip;(X)/p (X) is the a posteriori probability function. The 

10 Feature Extraction and Linear Mapping for Classification 
465 
two derivatives aJ;/aS,, and aJYlaV& can be computed from (10.60) as 
(10.78) 
Substituting (10.77) and (10.78) into (10.73), 
$hy&/yY(X) = V&B P - ’ Q ( X )  . 
( 10.79) 
Since the dimensionality of Y ( X )  is (L-1), the (L-l)x(L-1) matrix of AhY is 
nonsingular in general and 
exists. Therefore, the y,(X)’s can be expressed 
as linear functions of the q,(X)’s. That is, the ideal features are the solutions to 
optimizing f (M 
I , . . . ,ML,Sn,), regardless of the functional form, f. This con- 
clusion enhances the use of f (M I ,  . . . ,ML,Snl) as the class separability cri- 
terion. Even when Ah)‘ is singular, we can solve (10.79) for linearly indepen- 
dent yi(X>’s such that the yi(X)’s become linear functions of the qi(X)’s. 
Optimum linear features: When (15 -1) 
linear features are sought, 
Y ( X )  = A T X  where A is an nx(L -1) transformation matrix, and 6Y (X) 
= 6A TX 
since the variation of Y comes from the variation of the mapping function A. 
Substituting this 6Y (X) into (10.72), 
where Mix = E (X 
I wi } and SnZx = E { XXT } . In order to satisfy (10.80) regard- 
less of 6A, the [.] term must be zero. Thus, 
(10.81) 
Substituting (10.77) and (10.78) into (IO.81), 

466 
Introduction to Statistical Pattern Recognition 
Or, taking the transpose, 
(10.83) 
(10.84) 
Note that (10.84) is the same as (10.13) except that the generalized between- 
class scatter matrix, $,,, is used in (10.84). Therefore, using the same argument 
to solve (10.13), we can conclude that the optimum linear features are the 
eigenvectors of S;&bhx which correspond to the largest (L-I) eigenvalues. 
10.4 Nonparametric Discriminant Analysis [8] 
So far we have stated repeatedly that only (15-1) features are needed for 
the classification of L classes. However, unless a posteriori probability func- 
tions are selected, (L-1) features are suboptimal in the Bayes sense, although 
they are optimal with regard to a criterion used. Therefore, if the estimate of 
the Bayes error in the feature space is much larger than the one in the original 
variable space, some method must be devised to augment the feature extraction 
process. 
One possibility is to artificially increase the number of classes. In this 
way we can increase the rank of Sh, and subsequently the number of features. 
This could be accomplished by dividing each class into a number of clusters as 
seen in Fig. 10-5(a) and (b). For those cases where multimodal behavior is 
present and a clustering algorithm can be found that "properly" identifies the 
clusters, this may work well. As a second possibility 191, after determining the 
(L -1) features, we could remove them leaving a subspace orthogonal to the 
extracted features. If tr(S,'Sb) 
is used as a criterion, the first (L-1)- 
dimensional subspace contains all classification information due to the scatter 
of mean vectors, while the second (n -L +I)-dimensional space contains the 
information due to covariance-differences. Therefore, in order to select addi- 
tional features from the (n -L +I )-dimensional subspace, tr(Si.1 s h )  is no longer 
an appropriate criterion to use. A different criterion such as the second term of 

10 Feature Extraction and Linear Mapping for Classification 
467 
the Bhattacharyya distance must be adopted. Unfortunately, such a criterion is 
available only for two-class cases, and we do not know how to handle general 
L-class problems, in which all means are the same [ 101. 
A more fundamental problem is the parametric nature of the scatter 
matrices. If the distributions are significantly non-normal, the use of such 
parametric forms cannot be expected to accurately indicate which features 
should be extracted to preserve complex structures needed for classification. 
In this section, a nonparametric form of discriminant analysis is 
presented which overcomes both of the previously mentioned problems. The 
basis of the extension is a nonparametric between-class scatter matrix which 
measures between-class scatter on a local basis, using k-nearest neighbor (kNN) 
techniques, and is generally of full rank. As the result, neither artificial class 
generation nor sequential methods are necessary. 
In addition, the non- 
parametric nature of the scatter matrix inherently leads to extracted features 
that preserve structure important for classification. 
Nonparametric Scatter Matrix and Linear Mapping 
In this section only the two-class problem will be discussed for simpli- 
city, although an extension to the general L-class problem is possible. 
Nonparametric between-class scatter matrix: We begin by defining a 
nonpar-ametr-ic between-class scatter- matsix, denoted 2 , as 
where X“’ refers to samples from a,, 
and Xi; is the NN of X(‘) from the a,- 
samples ( i  z j ) .  Note that the NN is selected from the other class, and that the 
vector, X k k  - X ( ’ )  ( i  # j ) ,  points to the other class locally at X ( ’ ) .  Equation 
(10.85) is the scatter matrix of these vectors. Recall that the between-class 
scatter matrix, Sh, for two-class was given in (10.20) as the scatter matrix of 
one vector, M2-M I ,  which points from M I  to M 2  (and also from M 2  to M I  if 
the sign is ignored). Therefore, J,, of (10.85) may be considered as a non- 
parametric version of s,,. 
Instead of using only the N N ,  we might prefer to include information 
about kNN’s. A natural approach to accomplish this would be the replacement 

468 
Introduction to Statistical Pattern Recognition 
of Xhh by (l/k)C;,,Xj&. 
We call the sample mean of the kNN's the local 
mean. For a given sample X,, the w,-local mean is computed as 
(10.86) 
where X?& is the jth NN from w; to the sample X , .  Observe that, as k 
approaches N,, the total number of samples in of, q.(,) 
approaches the wj- 
mean vector, Mi. Using (10.86), a more general form of (10.85) can be defined 
as 
It is of interest to study the behavior of Ah when k = N;. This 
corresponds to replacing v(.) 
with Mi. We denote the resulting between-class 
scatter matrix as s;. 
On the other hand, from (10.20), 
2 
S h  = xPj(M,-Mo)(M,-M,)T 
= P ~ P ~ ( M * - M I ) ( M ~ - M ~ ) ~  
. 
(10.89) 
i=l 
Therefore, S6 and S h  are related by 
Or, multiplying Si! from the left side 
(10.90) 
(10.91) 
That is, the features selected by maximizing tr(S;.'Sb) must be the same as the 
ones from tr(S;.'Sh). Thus, the parametric feature extraction obtained by max- 
imizing tr(S;!S,) 
is a special case of feature extraction with the more general 
nonparametric criterion tr(S;.' A,,). 

10 Feature Extraction and Linear Mapping for Classification 
469 
Further understanding of Ah is obtained by examining the vector 
( X ,  - Tq(Xr)). Figure 10-6 shows an example of these vectors for k = 1. 
Fig. 10-6 Nonparametric between-class scatter 
Pointing to the NN from the other class, each vector indicates the direction to 
the other class locally. If we select these vectors only from the samples 
located in the classification boundary ( V I ,  V 3 ,  V4, V,, etc.), the scatter matrix 
of these vectors should specify the subspace in which the boundary region is 
embedded. Samples which are far away from the boundary (V2, etc.) tend to 
have larger magnitudes. These large magnitudes can exert a considerable 
influence on the scatter matrix and distort the information of the boundary 
structure. Therefore, some method of deemphasizing samples far from the 
boundary seems appropriate. To accomplish this we can use a weighting func- 
tion for each ( X  - T v ( X  )). The value of the weighting function, denoted as 
MI , for X is defined as 

470 
Introduction to Statistical Pattern Recognition 
(10.92) 
where a is a control parameter between zero and infinity, and d(X,,Xr,;l) is 
the distance from X to its kNN from a,. 
Observe that if a is selected as n, MI, 
corresponds to the kNN risk estimate of Chapter 7. 
This weighting function has the property that near the classification 
boundary it takes on values close to 0.5 and drops off to zero as we move 
away from the classification boundary. The control parameter, a, adjusts how 
rapidly w falls to zero as we move away. 
The final discrete form for Ah is expressed by 
where N = N I + N , ,  and the expectations of (10.87) are replaced by the sam- 
ple means and P, by N,IN. 
Optimum linear mapping: We now turn to the problem of the choice 
for Sz. We could choose S 2  as either the parametric S, in (10.1), the 
parametric S,, in (10.41, or we could define a nonparametric extension of S 2  
based on one of the parametric forms. In this section, S, is selected as S 2 .  
This choice is based on the observation that some form of global normalization 
is appropriate. This is readily recognized when the Euclidean distance is used 
to determine the kNN’s. Intuitively, it is appropriate to apply the Euclidean 
metric to data whose covariance matrix is the identity matrix. However, 
transforming two data sets simultaneously so that both have C, = I is generally 
not possible. As a compromise, we transform the data so that the averaged 
covariance matrix, S, , becomes the identity matrix. Then, b h  is computed in 
the transformed space. In addition, using S, as S ? ,  the parametric scatter cri- 
terion tr(S,’Sh) becomes a special case of the nonparametric scatter criterion 
tr(.S;’L,) as in (10.91), when the local means approach the class means. 

10 Feature Extraction and Linear Mapping for Classification 
-2.0 
-1.0 
47 1 
1.0 
2.0 
We now present the algorithm in its entirety. 
Whiten the data with respect to S, . That is, transform X to Y by 
Y =I\-’12QTX where A and Q, are the eigenvalue and eigenvector 
matrices of S, . 
(2) 
Select k and a. 
(3) 
(4) 
( 1 )  
In the Y-space, compute h, of (10.93) using w of (10.92) for weighting. 
Select the m eigenvectors of A,, wI,. . . ,wnz, which correspond to the m 
largest eigenvalues. 
(5) Then, Z =‘l’ih-’’2Q,TX 
is the optimum 
linear mapping 
where 
Ynj = [w, . . . y~,,]. Note that this transformation matrix is orthonormal 
with respect to S ,  as 
\y~~A-”2@TS, 
Q,K”*’Pnl = I . 
(10.94) 
Experiment 1: In order to verify the algorithm, two groups of three- 
dimensional data were generated. The first two variables were generated uni- 
formly as shown in Fig. 10-7. The third variable, independent of other two, 
I x2 
Fig. 10-7 An example of distributions. 
was normally distributed with zero mean and unit variance for both classes. 
Each class was comprised of 100 samples. After applying the nonparametric 
feature extraction procedure with k = 3 and a = 2, the following eigenvalues 
and eigenvectors were obtained. 

472 
Introduction to Statistical Pattern Recognition 
(10.95) 
hi = 0.56 , 
h2 = 0.40 , 
h3 = 0.04 , 
0.28 
0.03 
where the eigenvalues are normalized so that hl + h2 + h3 = 1. The eigen- 
values clearly indicate that only two features are needed. Furthermore, it is 
observed that both $I and Q2 effectively exclude the third variable, which 
possesses no structure that would assist in classification. 
Experiment 2: As warped distributions, Gaussian pulses of (6.1 16) and 
double exponential waveforms of (6.1 18) were used. These two random 
processes were time-sampled at eight points, forming eight-dimensional ran- 
dom vectors. Since both processes are controlled by three random variables, as 
shown in Experiments 6-2 and 6-3, their intrinsic dimensionality is three, and 
samples from each class lie on a three-dimensional warped surface in the 
eight-dimensional space. The number of samples generated was 250 per class. 
The nonparametric feature extraction procedure with k = 3 and o! = 2 
was applied for the classification of these two classes. The resulting normal- 
ized eigenvalues were 0.45, 0.18, 0.1 1, 0.08, 0.06, 0.05, 0.04, and 0.03, indicat- 
ing that most of the classification information is concentrated on the first 
feature. Figure 10-8 shows the plot of samples in the subspace spanned by 
yl = $TX and y2 = $gX, where $I and Q2 are the eigenvectors corresponding to 
the eigenvalues hl = 0.45 and h2 = 0.18 respectively [8]. Figure 10-8 clearly 
shows that the first extracted feature embodies the majority of the classification 
structure, as its 0.45 eigenvalue predicts. The second extracted feature, while 
not effective if used alone (as its eigenvalue of 0.18 suggests), assists to show 
how two distributions are separated. 
For comparison, Fig. 10-9 shows the plot from the Karhunen-LoCve 
expansion [8]. That is, the covariance matrix of the mixture of two classes was 
computed, from which the eigenvalues and eigenvectors are obtained. The plot 
is the distribution of samples in the subspace spanned by yI =$:X 
and 
y2 = 4;X where $1 and $2 correspond to two largest eigenvalues. Figure 10-9 
shows that the K-L expansion gives a poor selection of two features for 
classification. 

10 Feature Extraction and Linear Mapping for Classification 
413 
. 
-5.000 
X 
x
x
 
x 
x
x
 
X 
X 
x x  x 
X X  
x 
X 
4 .m 
-9.m 
-3.m 
*.Mo 
-l.m 
.m P~ly 2.m 
3 . m  
4.m 
6.000 
e.m 
Fig. 10-8 Plot by the nonparametric discriminant analysis. 
Linear Classifier Design 
Linear classifier design is a special case of feature extraction, involving 
the selection of a linear mapping to reduce the dimensionality to one. 
Classification is then performed by specifying a single threshold. 
When L = 2, parametric discriminant analysis always results in the 
extraction of a single feature. Thus, parametric discriminant analysis for the 
two-class case is essentially linear classifier design. 
For linear classifier design using the nonparametric procedure, we select 
the eigenvector corresponding to the largest eigenvalue as our linear transfor- 
mation. 

414 
2.033 
2.m 
2.w 
Z.Xl0 
2.m 
N - 1.800 
E 
1 .Boo 
1 .ycD 
1 .m 
1 .OD0 
.m 
Introduction to Statistical Pattern Recognition 
I 
I 
I 
I 
I 
I 
I 
I 
I 
m 
-2.m 
-1.600 
-i.m 
-.Em 
PHimOl 
.6O0 
1.m 
1.mI 
2.m 
i 
Fig. 10-9 Plot by the Karhunen-LoCve expansion. 
00 
Experiment 3: Linear classifiers were designed to classify two normal 
distributions with expected vectors and covariance matrices as 
The theoretically optimal linear classifier was designed by Procedure I in Sec- 
tion 4.2 (just after (4.46)). The error was 9.3%. 
Then, both parametric and nonparametric discriminant analyses were 
applied by maximizing tr(S;.'S,) and tr(S;.' 6,). For these procedures, 100 
samples per class were generated for design. After a linear transformation 
y = $rX was found, the means and variances of y for o, and o2 were com- 
puted by $TM, and $:&$, 
(i = 1,2). Since y is normal for normal X, the 

10 Feature Extraction and Linear Mapping for Classification 
x 2  
w2 
I 
I 
I 
I 
I 
Irn 
475 
- 
- 
- 
- 
- 
W1 
1 
I 
I 
I 
I 
I 
I 
1 
minimum error in the y-space can be obtained by using a normal error table. 
This test process eliminates the variation of the estimated error due to a finite 
number of test samples. Also, k = 3 and a = 2 were used to determine the 
weighting coefficients in the nonparametric procedure. The resulting errors 
were 9.7% for parametric approach and 9.4% for nonparametric one. Thus, not 
only was the performance of the nonparametric procedure close to the optimal 
linear classifier, but it was in fact superior to the parametric procedure. 
Experiment 4: In this experiment, samples were uniformly distributed in 
the areas shown in Fig. 10-10. The true error of zero is achieved by selecting 
SAMPLES 
50 SAMPLES 
Fig. 10-10 An example of distributions. 
the s, -axis as the only feature. For comparison, all three mappings were com- 
puted as before. Since the data sets were not normally distributed, there is no 
reason to assume that the procedure of Section 4.2 would necessarily be 
optimal even if Procedures I1 and 111 are used. The average error of Procedures 
I1 and 111 was 5.4%. On the other hand, parametric and nonparametric discrim- 
inant analyses resulted in the errors of 5.6% and 3.4%, respectively. 

476 
Introduction to Statistical Pattern Recognition 
The above two experiments provide additional justification for the non- 
parametric feature extraction algorithm. 
Distributional Testing 
Another application of the nonparametric scatter matrices is in the testing 
of structural similarity of two distributions. The ability to compare two distri- 
butions has numerous applications. One can gather data sets at different times 
or under different conditions and distributional testing can then be used to 
determine whether they are similar or not. When only a single measurement is 
taken, the problem can be solved in a fairly straightforward manner. However, 
multiple measurements present a problem. 
One method of testing the similarity of two distributions in a high- 
dimensional space is to compare the mean vectors and covariance matrices of 
the distributions. Two covariance matrices can be more easily compared by 
simultaneously diagonalizing them and checking whether or not the diagonal 
terms of the second matrix are close to one. Also, the Bhattacharyya distance 
is used to measure the closeness of two distributions. Although these tests are 
simple and adequate for most applications, more sophisticated tests are needed 
for distributions severly distorted from normal. 
A second alternative is to estimate the degree of overlap of the two dis- 
tributions. The Bayes error could be estimated by nonparametric techniques 
such as the Parzen and kNN approaches as discussed in Chapter 7. However, 
this method fails to indicate the subspace in which the differences are most 
prominent, or what type of differences exists. 
The test of this section requires no distributional assumptions, and pro- 
duces an eigenvalue and eigenvector decomposition that is ranked by distribu- 
tional similarity. 
To develop the test we first separate $h into two parts 
(1 0.97) 
We can interpret Ah, as a nonparametric hetnwn-class scatter matrix computed 

10 Feature Extraction and Linear Mapping for Classification 
477 
with respect to mi. In addition we will define two nonparametric within-cIass 
scatter matrices, denoted by A,, I and 
as 
(10.100) 
The only difference between An,; and Ahi is the class of the local mean used in 
the computation. If the two distributions are identical, it is expected that 
EAh1 and hH.2 E&2. 
This suggests that the matrix products A;?lAbl and 
&;/2$h2 
should be close to I. To reduce the number of comparisons from n2 
for each matrix product we can diagonalize 
and 
The diago- 
nalization of $;.fAhi may be achieved by simultaneously diagonalizing AH.; and 
.ehi. The n diagonal elements of each matrix can then be compared. 
Before presenting the experimental results we make a final note about 
A,,.. For the sample X f ) ,  when the local mean rY?i(Xp)) is computed, we do not 
consider X!“ as its own NN. 
Experiment 5: The first experiment consists of two parts. First, two 
normal distributions with mean vectors equal to the zero vector and covariance 
matrices equal to the identity matrix were compared. The distributions were 
two-dimensional. One hundred samples per class were generated, k was 
chosen as three, and a was chosen as zero, Le. w; = 1 or no weighting. Two 
matrices 
and $;!2$h2 
were computed. The mean values and standard 
deviations of the eigenvalues for four trials are summarized in Table 10-1 [8]. 
The fact that the eigenvalues are less than one is not particularly surprising. 
Recall that when we computed 8”; it was necessary to exclude the sample X, 
from our kNN determination. As such, this tends to make the within-class dis- 
tances larger than the between-class distances, resulting in eigenvalues smaller 
than one. 
To complete the experiment a second comparison was performed. A 
normal distribution was compared with a uniform distribution, both two- 
dimensional with mean vector equal to the zero vector and covariance matrix 
equal to the identity matrix. As before, 100 samples per class were used, k 
was chosen as three, and a was chosen as zero. The mean value and standard 

478 
Introduction to Statistical Pattern Recognition 
TABLE 10-1 
EIGENVALUE RESULTS FOR FOUR TRIALS 
w2 - Normal 
B 
h 2 
h? 
hi 
h2 
Mean 
0.79 
0.61 
0.96 
0.66 
1.63 
0.95 
1.52 
0.89 
StandardDeviation 
0.06 
0.13 
0.17 
0.06 
0.22 
0.21 
0.20 
0.16 
deviation results of the eigenvalue calculations for four trials are presented in 
Table 10-1 [8]. When compared to the normal vs. normal results, a distribu- 
tional difference is evident. 
Experiment 6: In the second experiment the time sampled-gaussian 
pulse was compared with the time-sampled double exponential pulse. Refer to 
Experiments 6-2 and 6-3 for additional information about these data sets. 
To provide a reference, two eight-dimensional normal distributions (not 
time-sampled gaussian pulses) were generated, both with mean vector equal to 
the zero vector and covariance matrix equal to the identity matrix. The result- 
ing eigenvalues were 1.36, 1.21, 1.11, 1.06, 0.89, 0.88, 0.79, and 0.57 for 
S;.],A~~ 
and 1.29, 1.19, 1.11, 1.08, 0.93, 0.81, 0.70, and 0.57 forA;!2Ah2 for a 
single trial. 
To assure that we would be testing for structural differences, both time- 
sampled data sets were independently whitened, Le., mean vector transformed 
to the zero vector and covariance matrix transformed to the identity matrix. 
When the whitened time-sampled data sets were compared, the eigenvalues 
were 34.4, 17.3, 14.3, 10.6, 6.6, 3.9, 2.7, and 1.4 for hi!lhhl and 0.87, 0.75, 
0.67, 0.52, 0.41, 0.32, 0.23, and 0.14 for A;!2$h2. These results clearly indi- 
cate that significant distributional differences exist. In addition they indicate 
why the hj,.fbhi should not be combined. It is possible that if they are com- 
bined, the eigenvectors of the result may not exhibit the same level of discrimi- 
nation. This is due to the fact that the eigenvalues are averaged in some 
fashion. 

10 Feature Extraction and Linear Mapping for Classification 
479 
-+.om 
As well as having the ability to test distributional differences, if the 
eigenvectors are computed, the axes of major difference can be plotted. This is 
shown in Fig. 10-1 1 where we project the data down onto the two eigenvectors 
I 
I 
I 
-8.m 
+.m 
-q.m 
-3.m 
-1.baO 
.'* 
1.1m 
e.'m 
3.Im 
I 
P H I  1 
s . m  
6.m 
'I .m 
3 .m 
2 .om 
rJ 
I 
- 
a 1.m 
.om 
-1 .m 
-2 .an 
-3 .m 
Q 
Fig. 10-11 Plot of structure difference. 
m 
of A,llLhl with corresponding eigenvalues, 34.4 and 17.3 [8]. The two distri- 
butions of Fig. 10-11 seem to be very similar at a glance. This is due to the 
fact that they share the same mean vector as well as the same covariance 
matrix. Therefore, we must look for, in Fig. 10- 1 1, the more detailed structural 
difference beyond the first and second order statistics. 

480 
Introduction to Statistical Pattern Recognition 
10.5 Sequential Selection of Quadratic Features [ll] 
So far, we have limited our discussion to linear transformations, because 
general nonlinear transformations are hard to define and analyze. However, in 
practical classification problems, we need to focus our attention on a particular 
nonlinear function, which is the quadratic discriminant function. The quadratic 
discriminant functions are the ideal features for normal distributions, and are 
expected to be reasonably good features even for non-normal (but unimodal) 
distributions. Thus, the quadratic equation of 
(10.101) 
must be the first candidate of the most effective feature for the classification of 
two classes. If distributions are normal, h ( X )  is the sufficient statistic, and car- 
ries all of the classification information. No extra features are needed. How- 
ever, if the distributions are not normal, there exist other features carrying 
some of classification information. 
One systematic way of extracting quadratic features for non-normal cases 
is to select h ( X )  of (10.101) as the first feature, and then find the subspace, Y, 
orthogonal to h (X). In the next stage, the quadratic discriminant function h (Y) 
in the Y-subspace is selected as the second feature, and the sub-subspace 
orthogonal to h (Y) is found. This process is repeated until classification infor- 
mation is exhausted in the subspace. 
Orthogonal space to h(X): The or-rhogonal space to h(X) may be 
defined as an (n -1)-dimensional hypersurface on which a certain (possibly 
nonlinear) projection of two normal distributions are identical. It should be 
noted that this definition of orthogonality is not conventional. Without losing 
generality, let us assume that two covariance matrices are diagonal, I and A. 
Given two means M I  and M 2 ,  we can move the coordinate origin by C such 
that two mean vectors in the new coordinate system satisfy 
(M2 - C )  = A”2(Ml - C )  . 
(10.102) 
Such a C can be uniquely obtained by solving (10.102) for C as 

10 Feature Extraction and Linear Mapping for Classification 
48 1 
C = (I - A1'2)-'(M2 - A"2MI) , 
( 10.103) 
where none of the components of A, the hi's, are assumed to be one. When 
hi = 1, C of (10.103) cannot be determined. This case will be discussed later. 
Also, note that two covariance matrices I and A are related by 
A = AIi21 AIi2 . 
( 10.104) 
Equations (IO. 102) and (10.104) imply that, if two distributions are nor- 
mal, 
X(2) = 1\1/2X(I) 
(10.105) 
where X"' is X from oi. For non-normal distributions, (10.102) and (10.104) 
do not necessarily imply ( I  0.105). 
The significance of (10.105) is the fact that it allows us to relate two nor- 
mal distributions with CI = I and & = A to one another after performing a 
simple mean shift using (10.103). 
To proceed with the development we replace A"* in (10.105) with e-B 
where B is a matrix chosen to satisfy AIi2 = e-B. Rewriting (10.105) we get 
~ ( 2 )  
= e - B ~ ( I )  
(10.106) 
Using (10.106) it is possible to express the two random vectors in the follow- 
ing form with r and Xo as parameters: 
These equations suggest that corresponding samples of X"' and X"' 
are 
located on a trajectory of the following differential equation 
( 10.109) 
d 
- X ( t )  = BX(r) with X ( 0 )  = Xo . 
dr 
Given an Xo on a trajectory of (10.109), for any X on that trajectory there is a t 
such that 
x = x(r) = e-"X(o) = A'/~x(o) . 
(10.1 10) 
Figure 10-12 shows an example. 

482 
Introduction to Statistical Pattern Recognition 
Possible Trajectories 
/ 
I 
\-x(o) 
surface 
Fig. 10-12 Possible trajectories in two-dimensional space. 
The ith component of X(r), denoted by xi@), satisfies 
-- - hi12 . 
X;(O) 
(IO. 1 11) 
Raising to the (allnh;)th power, (10.1 11) can be converted to make the right- 
hand side independent of i, as 
where sign(-) is the sign function and a is introduced to scale the exponential 
term. Note that (10.1 12) holds for any value of a, and thus we can choose the 
value of a freely. Generally, it is suggested to select an a so that oc/lnh; is as 
close as possible to one for all i. This will minimize data distortion. 
Example 3: Let two normal distributions be characterized by 

10 Feature Extraction and Linear Mapping for Classification 
483 
0.25 0 
M ,  
= E] , C, = I  , and M 2  = I] 
, C2 = [ .] . 
(10.113) 
Substituting (10.1 13) into (10.103), C = [0 0IT. Also, from (10.1 12) 
(10.1 14) 
(10.115) 
Figure 10-13 shows the three trajectories of (10.1 15) for xI(0) = 8 and 
9 
8 
7 
6 
5 
v, 
$
4
 
X 
3 
2 
1 
-1 I 
0
1
2
3
4
5
6
7
8
9
 
X1 AXIS 
Fig. 10-13 Three trajectories for a normal example. 
x2(0) = 3, 2, 1, denoted as T I ,  T 2 ,  and T3, respectively. 
Note that if X(0) is always selected on an (n-1)-dimensional surface 
across which each trajectory passes through exactly once, then the n- 
dimensional space of X could be transformed to another n-dimensional space 
consisting of one variable r, or a function of r, and the (n-1)-dimensional X(0) 

484 
Introduction to Statistical Pattern Recognition 
surface. Since all corresponding samples of the two normal distributions are 
located on the same trajectories, the projection of the two normal distributions 
onto the X(0) surface would be identical. Thus, the X(0) surface is the sub- 
space orthogonal to h ( X )  of ( 10.10 1 ). 
Selection of the orthogonal subspace: It is desirable to select, regard- 
less of the distributions, the same (n -I)-dimensional surface to facilitate the 
transformation to the new n-dimensional space. This can achieved by introduc- 
ing the new variables yi as 
a - 
y i ( t )  = sign(xi(r)) I xi(r> I Inkt 
. 
(10.1 16) 
Then, (10.1 12) becomes 
(IO. I 17) 
In the Y-coordinate system all of the trajectories are lines beginning at the ori- 
gin. Since all the lines start at the origin, a natural selection of the Y(0) (form- 
erly X(0)) surface is a unir sphere around rhe origin. 
Example 4: Two normal distributions of Example 3 are transformed to 
y l  and y2. Selecting a = ln4, two means in the y-space, M I Y  and M z Y ,  are 
calculated from (10.1 16) as 
- 
- 
(10.118) 
( 1 0.1 1 9) 
Note that we can control the scales of the yi’s by a. Figure 10-14 shows two 
distributions as well as three trajectories in the Y-space. The Ti’s 
in Fig. 10-14 
correspond to the respective Ti’s 
in Fig. 10-13. 
This formulation standardizes the transformation to an 
(n -1)- 
dimensional Y(0) surface and the variable that is a function oft, regardless of 
distributions. By converting to multidimensional spherical coordinates r, 
el,. . . ,e,,-, as 

10 Feature Extraction and Linear Mapping for Classification 
485 
.OO 
.05 
.10 
.15 
.20 
.25 
.30 
.35 
.40 
.45 
50 
Y1 AXIS 
Fig. 10-14 Y coordinate representation. 
I1 
I’ = (CYf)”2 , 
I = )  
Y 2  
Y l  
01 = arctan(-) 
, 
r 
1 
(10.120) 

486 
Introduction to Statistical Pattern Recognition 
Note that the range of r is ( o , ~ ) ,  
the range of 
€I2, . . . ,8,7-1 are (0,n). 
is (0,271), and the ranges of 
Feature extraction algorithm: We now detail the systematic feature 
extraction procedure. 
Compute the sample mean vector and covariance matrix for each class. 
Compute h ( X )  in (10. IOI), and select this as a feature. 
If h ( X )  is an effective feature, retain it and continue. Otherwise, stop. 
Simultaneously diagonalize the data. 
Compute C in (IO.l03), and use it to translate the data sets. Select an a. 
For each sample X in the data set compute Y using (IO. 1 16). 
Convert Y to multidimensional spherical coordinates using (IO. 120). Do 
not compute the radial component, 
I'. 
The (n-1) 
components 
€I1,. 
. . ,en-, 
are orthogonal to h ( X ) .  
Go to Step (2). 
For two normal distributions, the algorithm will stop after testing the 
second extracted feature. This will occur because in the mapped 8-coordinate 
system, if we disregard I', the two distributions will have identical mean vectors 
and covariance matrices. Hence, the second extracted feature, h in the (n-1)- 
dimensional mapped space, will not have any capability of discrimination. 
We now address the case when C does not exist. From (10.103), C does 
not exist when any h, = 1. As an aid to the discussion, let us assume that there 
are 2 instances of h, = 1 and that the features are ordered so that hi + 1 
(j 
= 1,. . . ,n-l), and hi = 1 (i = n-I+I, . . . ,n). We can now deal with the 
two cases separately. The first (n-9 features are readily handled by the non- 
linear mapping algorithm. For the remaining t features we need to reexamine 
our process of finding the orthogonal space to h ( X ) .  Since the covariance 
matrices are both I for these t features (hi = I), the hypersurface and the pro- 
jection technique are simple to determine. The hypersurface is chosen as the 
hyperplane that bisects the mean-difference vector, and linear projection is 
used. Refer to Fig. 10-15, which illustrates this. It is seen that the mapping 

10 Feature Extraction and Linear Mapping for Classification 
487 
Fig. 10-15 Hypersurface and trajectories for features with h, = 1 
algorithm for the i features with hi = 1 is a linear transformation. Because of 
this linearity, there is no possibility that there will be any mean vector or 
covariance matrix differences in the (6- 1 )-dimensional subspace. This implies 
that the h computed in the (k1)-dimensional subspace will not exhibit any 
discriminatory ability. Specifically, the mean vectors will be identical and the 
covariance matrices will both be I .  As a result, there is no need to map the i 
features down to an (i-1)-dimensional space. All of our discriminatory ability 
is contained in the mean-difference, and this information is included in the 
h ( X )  computed for all n features, i.e., prior to the mapping procedure. This 
observation allows us to easily modify the nonlinear mapping algorithm. Add 
Step (4') after Step (4) and before Step (5) as follows: (4') Discard all features 
with h, = 1. 
One final topic deserves mention. In Step (7) we convert to a multidi- 
mensional spherical coordinate system. The retained axes are the angular coor- 
dinates. Since these angles have finite ranges, we should carefully consider 
where these angles are measured from. For example, if the positive T 2  line of 
Fig. 10-14 is chosen as the reference line, the distribution of the angle, e,, is 
centered at 0"as seen in Fig. 10-16(a). On the other hand, if the negative T2 
line of Fig. 10-14 is used, the distribution of 8 ,  is centered at T[: as in Fig. 10- 

488 
Introduction to Statistical Pattern Recognition 
(4 
(b) 
Fig. 10-16 Distributions of an angle. 
16(b). Since we will be using h ( X )  in the 8 coordinate system, and h ( X )  is 
essentially a normal classifier, it seems reasonable to try to avoid the situation 
in Fig. 10-16(a). This can be accomplished by rotating the 8 coordinate system 
so that the global mean vector is approximately in the midrange of 8 , .  
To demonstrate the performance of the nonlinear mapping algorithm, the 
results from two experiments follow. Note that for the experiments in these 
examples a was computed using the following formula: 
(IO. 12 1) 
Experiment 7: In the first experiment both data sets were drawn from 
normal distributions. Each class consisted of 100 three-dimensional samples. 
The mean vectors and covariances matrices were 
Mi = -3.75 
, C, = I  and M2 = 0 , C2 = 1.71. 
(10.122) 
[
:
I
 
61 
These values were chosen to provide a Bayes error of approximately 9%. 
A nonparametric error estimate of the data indicated an error rate of 9%. The 
first feature was computed using h (X). A nonparametric error estimate of the 
single feature indicated an error rate of 9%. This result is consistent with the 
fact that the original classes were actually jointly normal. The data set was 
reduced to two-dimension using the nonlinear mapping algorithm. 
A 

10 Feature Extraction and Linear Mapping for Classification 
489 
nonparametric error estimate in the two-dimensional subspace indicated an 
error rate of 49%, which implies the two distributions were essentially identi- 
cal. When a second feature was extracted from this subspace and combined 
with the first feature, there was no change in the nonparametric error estimate. 
Experiment 8: In the second experiment the first distribution was 100 
normal samples with M I  = [-1 
-4 -1IT and C1 = I .  The second distribution 
was formed using two normal distributions with parameters 
0.1 0 
0 
MZI = E], 
= [0 4.1 0 1, 
0 
0 4.1 
0.1 0 
0 
M22 = E] , 
Z22 = [ 0 4.1 0 1, 
0 
0 4.1 
(10.123) 
where the second subscript serves as an identifier for the two clusters. Fifty 
samples from each cluster were combined to provide 100 samples. A non- 
parametric error estimate of the data set indicated an error rate of 8%. The first 
feature was extracted, and a nonparametric error estimate indicated an 11% 
error rate. The data set was reduced to two-dimension using the nonlinear 
mapping algorithm. A nonparametric error estimate in the two-dimensional 
subspace produced an error rate of 40%, indicating improvement was possible. 
A second feature was extracted in the two-dimensional subspace. The resulting 
nonparametric error estimate of the two features was 8%. Since this was equal 
to the error rate of the original data, the feature extraction process was ter- 
minated. 
10.5 Feature Subset Selection 
Up to now, we have assumed that the features are functions of all the 
original variables, thus preserving as much classification information as possi- 
ble contained in the original variables. This assumption is reasonable in many 
applications of pattern recognition, particularly for the classification of random 
processes. A feature extractor for random processes may be interpreted as a 
filter, with the output time-sampled values to be functions of all input time- 
sampled values. However, in some other applications such as medical diag- 
nosis, we need to evaluate the effectiveness of individual tests (variables) or 

490 
Introduction to Statistical Pattern Recognition 
their combinations for classification, and select only the effective ones. This 
problem is called feature subset selection. The best subset of m variables out 
of n may be found by evaluating a criterion of class separability for all possible 
combinations of m variables. However, the number of all possible combina- 
tions, (m), 
becomes prohibitive even for modest values of m and n. For exam- 
ple, with n = 24 and m = 12, the number becomes 2,704,156. Therefore, we 
need some procedures to avoid the exhaustive search. 
n 
Backward and Forward Selections 
In this section, we will discuss two stepwise search techniques which 
avoid exhaustive enumeration [ 12- 141. Although the procedures are simple, 
they do not guarantee the selection of the best subset. 
Backward selection: Let us study a simple example in which two 
features are chosen out of four as in Fig. 10-17(a). The subset of features a, b, 
(a) Backward selection 
(b) Forward selection 
Fig. 10-17 Stepwise feature subset selection. 
and c is denoted by (a,b,c). A criterion of class separability is selected, and its 
value for (a,b,c) is expressed by J3(a,b,c) where the subscript indicates the 
number of features in the subset. The backward selection procedure starts 
from the full set (1,2,3,4). Then, eliminating one feature from four, all possi- 
ble subsets of three features are obtained and their criterion values are 
evaluated. If J3(1,3,4) is the largest among the four J 3 ’ s ,  (1,3,4) is selected as 
the best subset of three variables. Then, eliminating one more feature only 
from (1,3,4), we can obtain the subsets of two features, among which the sub- 
set with the largest J2 is chosen as the best solution of the problem. 

IO Feature Extraction and Linear Mapping for Classification 
49 1 
Forward selection: The forward selecrion procedure starts from the 
evaluation of individual features as shown in Fig. 10-17(b). Let the problem 
be to select three featurcs out of four. If J I (2) is the largest among all J 1  (.)’s, 
one feature is added to feature 2 to form the subset of two features. If J2(2,3) 
is the largest among all J2(2,.)’s, one more feature is added to (2,3) to form the 
subset of three features. Among all possible (2,3,.)’s, the best subset is the one 
which gives the largest J3(2,3,.). 
The reason why these stepwise searchs cannot necessarily select the 
optimum subset may be better understood by observing the forward selection 
procedure. Suppose that xl and x2 of Fig. 10-1 are two features among n, 
from which m features are to be selected. Since the wI- 
and 02-marginal den- 
sity functions of xl are heavily overlapped, xl drops out when individual 
features are evaluated. The same is true for x2. Thus, one of the other 
features, say xs, is selected as the best single feature. At the next stage, only 
combinations of xs and others are examined, and so on. As the result, the 
combination of features including both xI and x2 might not come up for 
evaluation at the later stages. As seen in Fig. 10-1, although xI and x2 are 
poor features individually, their combination gives a good feature space in 
which the o1 
- and q-distributions are well separated. The forward selection 
procedure could fail to pick that information. This phenomenon is observed 
frequently when two features are highly correlated. In general, it is true for 
signal representation that we can eliminate one feature when two features are 
highly correlated. This is due to the fact that the second feature gives little 
extra information for representing signals. For an example of Fig. 10-1, know- 
ing one’s height (xl) we can well guess the weight ( ~ 2 ) .  On the other hand, 
highly correlated features could enhance class separability significantly, as seen 
in Fig. 10-1. 
Eliminating one, we might lose vital information for 
classification. 
Thus, both backward and forward selection procedures give simple 
search techniques which avoid exhaustive enumeration. However, the selection 
of the optimal subset is not guaranteed. 
Branch and Bound Procedure [15] 
Branch and hound methods have been developed to obtain optimal solu- 
tions to combinatorial problems without involving exhaustive enumeration 
[16-19]. In this section, we formulate the feature subset selection as a 

492 
Introduction to Statistical Pattern Recognition 
combinatorial optimization problem, and develop an efficient algorithm for 
selecting feature subsets. The subset selected by this procedure is guaranteed 
to be the best among all possible combinations of features. 
Basic branch and bound procedure: Rather than enumerating the sub- 
sets of m features, we will find it more convenient to enumerate the subsets of 
m = n - m features discarded from the n feature set. Let ( z  I ,  . . . ,ZG) denote 
the set of those 
discarded features. Each variable zi can take on integer 
values in { 1,. . . ,n]. However, since the order of the zi’s is immaterial, every 
permutation of the sequence ( z I , .  . . ,z;} will yield the same value of the cri- 
terion. Moreover, all the zi’s should be different. Hence, it is sufficient to 
consider the sequences which satisfy 
ZI < z 2 <  . . .  <z,. 
(10.124) 
We will discuss more general enumeration of the subsets later. 
The feature selection criterion is a function of the m features obtained by 
deleting the m features from the n feature set. However, for notational con- 
venience, we write the criterion as J;(z I ,  . . . ,zi). Then the subset selection 
problem is to find the optimum sequence (z;, . . . ,z:) such that 
J,&;, 
. . . , z i )  = max J,(z 
I ,  . . . , z ~ )  
. 
(10.125) 
If the criterion were to be minimized instead, all the inequalities in the follow- 
ing discussion would be reversed. 
_- 
- I.. . . r i m  
Enumeration of the sequences (z I ,  . . . ,z,) 
satisfying (10.124) can be 
illustrated by a solution tree. Figure 10- 18 is a solution tree corresponding to 
n = 6 and m = 2 (m = 4). A node at level i is labeled by the value of zi. Also, 
each node can be identified by the sequence of discarded features; for example, 
(1,4) for node A in Fig. 10-18. At level 1 of Fig. 10-18, z I  can only assume 
values 1, 2, or 3, because, with z I  greater than 3, it would not be possible to 
have sequences ( z  I ,  . . . ,z4) satisfying (10.124). Similar considerations govern 
the enumeration at other levels of the tree, and the largest value for zi must be 
(m +i) in general. 
by 
Let us assume that the criterion J satisfies monotonicity, which is defined 

10 Feature Extraction and Linear Mapping for Classification 
493 
Fig. 10-18 
J I G 1  
n = 6, m = 2 (TA= 4) 
The solution tree for the basic algorithm. 
2 1 2 ( 2 , , z * )  2 . .  . U , - ( Z I , .  . .,=,). 
LEVEL 1 
LEVEL 2 
5 LEVEL3 
I': LEV EL 4 
6 
(10.126) 
The monotonicity is not particularly restrictive, as it merely implies that a sub- 
set of features should be no better than any larger set that contains the subset. 
Indeed, a large variety of feature selection criteria do satisfy the monotonicity 
relation. They are the Bayes error, asymptotic kNN error, distance measures 
such as the Bhattacharyya distance, and the functions of the scatter matrices. 
Let a be the best (maximum) value of J;(z I ,  . . . ,z,-) found so far in the 
search. If 
J k ( Z 1 , .  . . ,Zh) SCX 
for k < m , 
(10.127) 
then by (10.126) 

494 
Introduction to Statistical Pattern Recognition 
for all possible [ z ~ + ~ ,  
. . . ,z; }. 
This means that, whenever the criterion evaluated for any node is less than a, 
all nodes that are successors of that node also have criterion values less than a, 
and therefore cannot be optimal. This forms the basis for the branch and 
bound algorithm. 
The branch and bound algorithm successively generates portions of the 
solution tree and computes the criterion for the nodes explored. Whenever a 
suboptimal partial sequence of nodes is found to satisfy (10.128), the subtree 
under the node is implicitly rejected, and enumeration begins on partial 
sequences which have not yet been explored. 
We shall give a simple procedure for enumerating the partial sequences 
with z < . . . 
as follows. 
Basic algorithm: 
(1) Initialization: Set a= -00, the level i = 1, and z o  = 0. 
(2) Generate successors: Initialize LIST(i) which is the list of the featurc 
values that z, can assume given the values of ( 2  I ,  . . . , z ~ - ~ ) .  That is, 
- 
LIST(i)= { ~ ~ - ~ + l ,  
i1-,+2,. . . , m + i }  (i = 1,. . . , m ) .  (10.129) 
(3) Select new node: 
If LIST(i) is empty, go to (5). Otherwise, set z, = k, where 
J I ( z l , .  . .,z1-1,k) = max J l ( z I , .  . . , Z , - ~ , J ) .  
(10.130) 
J ELIST (1 ) 
Delete k from LIST(i). 
(4) Check bound: 
If J , ( z I , .  . . ,zl) < a, go to (5). If level i = 2, go to (6). Otherwise, set 
i = i + 1 (advance to a new level) and go to (2). 
(5) Backtrack to lolz~er level: 
Set i = i - 1. If i = 0, terminate the algorithm. Otherwise go to (3). 
(6) Last level: 

10 Feature Extraction and Linear Mapping for Classification 
495 
Set a = J,;(z I ,  . . . , z ~ )  
and set (z, , . . . , z k )  = ( z  I ,  . . . , z ~ ) .  Go to (5). 
The functioning of the algorithm is as follows. Starting from the root of 
the tree, the successors of the current node are enumerated in L/ST(i). The 
successor, for which the partial criterion J,(z I ,  . . . ,z,) is maximum (the most 
promising node), is picked as the new current node, and the algorithm moves 
on to the next higher level. The lists LIST (i) at each level i keep track of the 
nodes that have not been explored. Whenever the partial criterion is found to 
be less than a, the algorithm backtracks to the previous level and selects a 
hitherto unexplored node for expansion. Whenever the algorithm reaches the 
last level m, a is updated to be the current value of Jh(zI, . . . ,z;) and the 
current sequence (z I ,  . . . ,z;) is saved as ( z ; ,  . . . ,zk). When all the nodes in 
L/ST(i) for a given i are exhausted, the algorithm backtracks to the previous 
level. When the algorithm backtracks to level 0, it terminates. Upon termina- 
tion, the current value of (~7,. 
. . , z i )  gives the complement of the optimum set 
of m features, and the current value of a gives the optimum value of the cri- 
terion. The procedure guarantees that all sequences are either explicitly 
evaluated or implicitly rejected, and thus the sequence (z;, . . . , z i )  gives the 
best subset of features among all possible subsets. 
Alternate enumeration: The enumeration scheme of the basic algorithm 
is direct and simple. The partial sequences enumerated (see for example, Fig. 
10- 18) satisfy (10.124). Relation (10.124) ensures that no two equivalent 
sequences are enumerated. That is, a permutation of a previously enumerated 
sequence will not be enumerated again. 
On the other hand, we note that, with reference to Fig. 10-18, the nodes 
at a given level do not all have the same number of terminal nodes. Node 
(1,2) has 6 terminal successors, while node (1,4) has only one. As a result, if 
the suboptimality test (10.128) is satisfied for node (1,2) (i.e., if J2(1,2) < a), 
six sequences are rejected as being suboptimal, while for node (1,4) only the 
single sequence (1,4,5,6) would be rejected. Therefore, we would like nodes 
with more successors to have a greater probability of the suboptimality test 
(10.128) being satisfied: Le., those nodes should have smaller values of the cri- 
terion J I  than the ones with fewer successors. 
One way to accomplish the above is to initially rank the features from 
good to bad, and reorder them with 1 as the single feature whose removal from 
the full set of n features yields the smallest value of the criterion, and n as the 

496 
Introduction to Statistical Pattern Recognition 
worst single feature. The basic algorithm is then applied to the ordered 
features. Thus, at level 1, J I (I) e .I 
I (2) c J I (3). 
This reordering will obviously be effective at the first level, but there is 
no guarantee that, at successive levels, nodes with more successors will always 
have smaller values of the criterion. 
If we remove the restriction (IO. 124), we can order the features at each 
level to realize maximum advantage of the suboptimality test (10.128). This 
results in a slightly more involved enumeration procedure because we have to 
ensure that the sequences enumerated would still be unique to a permutation. 
The alternate enumeration scheme is based on the same tree structure as 
before. But, the successors of each node are ordered at each level so that suc- 
cessors with the smaller values of the partial criterion will be nodes which will 
have larger number of successors in turn. 
Following is the improved algorithm, the branch and bound algorithm 
employing the new enumeration scheme. 
Improved algorithm: The following notation will be used in the 
improved algorithm. 
LIST(i): an ordered list of the features enumerated at level i. 
POINTER(i): The pointer to the element of LIST(i) being currently con- 
sidered. For example, if the current element in LZST(i) is the kth, then 
POINTER ( i )  = k. 
SUCCESSOR(i,k): number of successors that the kth element in LIST(i) can 
have. 
AVAIL : a list of available feature values that LIST(i) can assume. 
(1) Initialization: 
Seta=-m,AVAIL=(1,2 , . . . ,  n],i=l,LIST(O)=(O), 
SUCCESSOR (0,l) = m + 1, POINTER (0) = 1. 
(2) Initialize LIST(i): 
Set NODE = POINTER (i -1). Compute J;(z . . . r ~ i - l  ,k) for all k in 
AVAIL. 
Rank the features in AVAIL 
in the increasing order of 
Ji(zl,. . . , z ~ - ~ & )  
and store the smallest r of these in LIST(i) in the increasing 
order (with the first element in LIST (i) being the feature in AVAIL yielding the 

10 Feature Extraction and Linear Mapping for Classification 
497 
smallest J,), where r = SUCCESSOR ( i  -1, NODE). Set SUCCESSOR (i, j )  
= I- - j + 1 for j = 1,2, . . . ,I-. Remove LIST(i) from AVAIL. 
( 3 )  Select new mode: 
If LIST(;) is empty, go to (5). Otherwise, set z, = k, where k is the last 
element in LIST(i). Set POINTER (i) = j ,  where j is the current number of 
elements in LIST(i). Delete k from LIST(i). 
(4) Check Bound: 
If J,(z I, . . . ,z,) < a, 
return zi to AVAIL and go to (5). 
If level i = k, go to (6). Otherwise set i = i + 1, and go to (2). 
( 5 )  Backtrack: 
Set i = i - 1. If i = 0, terminate the algorithm. Otherwise, return zi to 
AKAIL and go to (3). 
(6) Final level, Update hound: 
Set ct = J,;(: I, . . . ,z,;) and save ( z l ,  . . . ,z;) as (zy, . . . , z k ) .  Return z; 
At the conclusion of the algorithm, ( z y , .  . . ,z:) 
will give the comple- 
ment of the best feature set as before. Figure 10-19 illustrates, for a random 
example, the tree enumerated including the nodes which were rejected by the 
suboptimality test (10.128). At level 1, features 4, 3, and 6 were enumerated 
because J I (4) < J (3) < J I (6) < J I (l), J1 (2), J I (5). The SUCCESSOR vari- 
ables determine the number of successor nodes the current node will have at 
the next level. AVAIL keeps track of the feature values that can be enumerated 
at any level. The algorithm is thus totally independent of the ordering of the 
features. No sequence is enumerated more than once (even as a permutation) 
and all possible sequences are considered either explicitly or implicitly, guaran- 
teeing optimality of the subset sought. Moreover, the suboptimality test 
(10.128) is always used to the best advantage, rendering the algorithm very 
efficient. Also, the ordering of the nodes does not mean extra computation, 
because the partial criteria would be evaluated for the successors of each node 
anyway. 
to AVAIL. Go to (5). 

498 
=I 
Introduction to Statistical Pattern Recognition 
LEVEL 1 
LEVEL 2 
LEVEL 3 
LEVEL 4 
Fig. 10-19 The solution tree for the improved algorithm. 
Recursive computation of criteria: We noted in the previous section 
that the algorithms are implemented with the criterion evaluated for the partial 
sequences (z I ,  . . . ,zI). The nature of the enumeration schemes requires that 
the value of the criterion be computed successively as features are deleted from 
the full set. For a class of criteria, it is possible to evaluate the criterion recur- 
sively, as a new feature is deleted from the present partial set. Recursive com- 
putation results in substantial computational savings. We will derive recursive 
equations for the class of quadratic criteria. The function of the scatter 
matrices is a example of quadratic criteria. The criterion always takes the form 
of 
J I - x T  
- Ish 
-I XI 3 
(10.13 1) 
where X ,  is a k-dimensional vector and SI is a kxk scatter matrix, when k 
features are present. Also, the Bhattacharyya distance for the normal case has 
the first term in the quadratic form (10.131). The determinant term of the 

10 Feature Extraction and Linear Mapping for Classification 
499 
s,' = 
Bhattacharyya distance can also be computed recursively as we will see later. 
The inversion of Sk is the major computational effort in evaluating 
(10,131) as features are successively deleted from the full set of features. 
When the ith feature is deleted, it is necessary to compute the inverse of Sk 
with the ith row and column deleted. Without loss of generality, let the feature 
being deleted correspond to the kth row and column of Sk. 
1 
a 
a 
1 
1 
a 
a 
si!, + 'S,L,DDTS,L, 
--s-l 
D j 
> 
(10.133) 
- 
--DTS;!l 
- 
(10.132) 
A fundamental identity in matrix algebra gives S i '  in terms of Si!' as 
r 
1 
(10.135) 
then, by (10.133) and (10.135), it can be verified that 
(10.136) 
Hence, ST!' can be computed from S i '  with little computational effort. With 
reference to branch and bound algorithms, the inverse matrices Si' are stored 
for each level. The inverse at any level is computed from that at the previous 
level using the recursive equation (10.136). Whenever the algorithm back- 
tracks and proceeds down another branch, the inverse for the new SL can be 
recomputed from the inverse at the level at which the branching occurred. For 
example, in Fig. 10-18, after (2,4,5,6) is explored, the algorithm backtracks to 
the node (2.3). The value of S,' for the node (2,3) can be computed from the 
1 
h 
Si!\ = A  - - C C T .  

500 
Introduction to Statistical Pattern Recognition 
current value of S i 1  at level 1. The Si' for level 2 is updated to be this value 
as feature 3 is now chosen to be z2. 
It is also possible to recursively compute the quadratic XLSklXk for k-1 
features given its value with k features and the Si' matrix from the previous 
level. This is useful in Step (3) of branch and bound algorithms, where it is 
necessary to compute the criterion after deleting one feature at a time from a 
partial set. This avoids computation of Sa!l during this step. Once a node is 
selected however (Step (4)), the S i 1  for the new level has to be updated using 
(10.136). 
Let the criterion with k features be denoted by Jk = X r s i ' X k ,  and let us 
The criterion with k-1 features is now Jk-1 = X , ~ _ ] S ~ L ~ X ~ - ~ ,  
where 
assume that the kth feature is being deleted as before. 
x, = [X,T_]X,I7 , 
(10.137) 
k-I 
I 
and Sk-1 is defined as in (10.132). 
From ( 10.136), 
Consider 
=XL-'_IA Xk-1 + 2XkCTX,&1 + b xz . 
(IO. 139) 
Equations (10.138) and (10.139) together give 

10 Feature Extraction and Linear Mapping for Classification 
50 1 
Note that [CT b ]  is a row of S,' 
corresponding to the feature being deleted. 
Hence [C' bIX, is merely the inner product of that row with xh. Thus, the 
J,(z I ,  . . . ,=,-I , j )  in 
the 
algorithms 
can 
be 
directly 
evaluated 
from 
J,-l(zl,. . . ,z,-I) by (10.140) without actually having to compute 
for all 
the variables j .  Incidentally, (10.140) also furnishes proof that J is monotonic. 
can be computed recursively. 
Let Sk denote a matrix with k features. It is necessary to evaluate 
I ,%-[ 
I ,  when the kth feature is deleted from Sk. Recalling that at every level 
of the algorithm we have S i '  available (used in the quadratic term). Thus, 
ISk-, I can be directly computed from ISk I of (10.132) as follows: 
The determinant term in the Bhattacharyya distance for the normal case 
= ( s k h  -D'S;!]D)ISL-I 
1 , 
(10.141) 
where the third line is obtained from the second by (2.143). The ratio of two 
determinants, skk - DTS,!I D, is readily available from the (k,k) element of SLl 
as seen in (10.133) and (10.134). 
Experiment 9: The algorithms were tested on multispectral data 
acquired from airborne remote sensing scanners at the Laboratory for Applica- 
tions of Remote Sensing (LARS) at Purdue University. 
The data comprised of 423 sample vectors each from two classes; Soy- 
bean and Corn. There were 12 data channels, corresponding to 12 bands of the 
spectrum in which the sensing was performed. Each channel is a feature, and 
the problem was to select a subset of the channels which was best according to 
a given criterion. The criterion used was J = tr(S;.]S,,) with the estimated 
means and covariance matrices. 

502 
Introduction to Statistical Pattern Recognition 
Table 10-2 shows the number of subsets evaluated for three algorithms: 
two from the basic algorithm and one from the improved one. 
TABLE 10-2 
THE RESULTS OF THE BRANCH AND BOUND PROCEDURE 
Case 
kl 
[ 31 
NUMBER OF SUBSETS ENUMERATED 
Exhaustive 
search 
495 
2,704,156 
Basic algo- 
rithm 
without ini- 
tial ordering 
100 
Did not ter- 
minate after 
600 sec. 
CPU time 
(CDC 6500) 
Basic algo- 
rithm with 
initial order- 
ing 
42 
13,369 
(249 sec. 
CPU time) 
Improved 
algorithm 
32 
6,256 
(140 sec. 
CPU time) 
In the first algorithm, features are ordered according to the wave-lengths of the 
spectrum bands from the shortest to the longest, while in the second algorithm, 
features are ordered according to the class separability of individual feature. 
The basic algorithm was used for both cases. The third is the improved algo- 
rithm. Table 10-2 shows significant saving achieved by the basic algorithm 
with initial ordering and the improved algorithm [ 151. 
Experiment 10: To evaluate the performance of the algorithms for a 
larger problem, an additional set of 12 features was generated by taking the 
square of 12 features of Experiment 9. The covariance matrices and the means 
were computed for the 24 feature set. It is to be expected that the resulting 24 
feature set is very correlated, and there may be several subsets that yield very 
close values of the criterion. 

10 Feature Extraction and Linear Mapping for Classification 
503 
No. of node\ 
rejected 
n 
13 
90 
Again, significant saving was achieved by the basic algorithm with initial 
ordering and the improved algorithm. The superiority of the improved algo- 
rithm is to be expected, because in the large variable problem the initial order- 
ing scheme for the basis algorithm will not be very effective at higher levels. 
In fact, the basic algorithm without initial ordering did not even terminate after 
600 seconds of computation (by CDC6500). Table 10-2 summarizes the 
results [15]. Table 10-3 gives the number of nodes expanded (subsets 
127 
199 
452 
756 
ROO 
466 
633 
1x0 
16X 
TABLE 10-3 
SUMMARY OF BEHAVIOR OF ALGORITHMS 
enumerated) at each level and the number of nodes for which the inequality 
(10.128) was satisfied (subsets rejected) at each level [15]. Because of the 
reordering of the features, every node that was rejected at each level of the 
improved algorithm results in a large number of suboptimal sequences being 
discovered. Hence, fewer nodes are enumerated overall in the improved algo- 
rithm than in the basic algorithm. The additional complexity of the improved 
algorithm appears justified in the light of its efficiency. Also, the improved 
algorithm is independent of the initial ordering of the features. 
Computer Projects 
1. 
Find the suboptimal linear features from Data I-A by maximizing the 
Bhattacharyya distance. 
(a) 
Use the algorithm for pl dominant. 

504 
Introduction to Statistical Pattern Recognition 
2. 
3. 
4. 
5. 
6. 
(b) 
Repeat Experiment 2. Change the parameters N ,  n, a, and k to see the 
effects. 
Repeat Experiment 3 for Data I-A. 
Repeat Experiment 6. 
Repeat Experiment 8. 
Repeat Experiment 10 for Data I-A. 
Use the algorithm for p2 dominant. 
Problems 
1. 
The density functions of three classes consist of three impulses for each 
class as in the figure, each impulse carrying the probability of 1/3. 
(a) 
Select one linear feature which maximizes tr(S;.' Sb). 
(b) 
Select one linear feature which maximizes tr(S;.'S,) 
between mi 
and m2. 
\'J 
-2 

IO Feature Extraction and Linear Mapping for Classification 
505 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
Find the optimum linear features to maximize tr(A TS I A )  under a con- 
straint ATS2A = I, where A is an nxm ( m a )  transformation matrix. 
Find the optimum linear features to maximize tr (A ' S  I A )  under a con- 
straint tr(ATS2A) = c, where A is an nxm (m<n) matrix, and c is a con- 
stant. Point out the difference between Problems 2 and 3. 
Let A I , .  . . 
be the eigenvalues of S;'SI, and p I , .  . . ,p,, be the 
eigenvalues of (A 'S2A)-' (ATS l A )  where A is an nxm ( m a )  transforma- 
tion matrix. Prove hi >pi. 
Show how to find the suboptimum linear features to maximize the diver- 
gence, when the second term is dominant. Is it possible to obtain the 
explicit solutions to find linear features by optimizing the first term of 
the divergence? 
Let f ( M  I ,  . . . ,ML,Sm) be a class separability criterion. 
(a) 
Prove that Z:f.=, (df/aM,,) = 0 must be satisfied in order for f to be 
invariant under a coordinate shift Y ( X )  = Y * ( X )  + V where Y ( X )  
is the optimal solution. The mixture mean of Y (X) is assumed to 
be zero. 
Prove that Z ~ = l ( d f / d M j y ) M ~ y  
= -2(df/dSmy) SnlY must be satisfied 
in order for f to be invariant under nonsingular linear transforma- 
tions Y ( X )  = B ~ Y * ( x ) .  
Let J = tr( Si! ( M  ,M:+M2MT-M 
,M$-M,M:)J 
be the class separability 
criterion where 
(b) 
Compute A s h ,  and find one linear feature which maximizes tr(SiIV2 
h ) .  
Let f ( M l , .  . . ,ML,SnI) be the class separability criterion. Find the value 
off for the ideal features in terms of asymptotic NN errors. 
Define the nonparametric within-class and mixture scatter matrices, and 
prove that they are the same as the parametric ones, S, and S,,, when the 
local means approach the class means. 
Two normal distributions are characterized by 

506 
Introduction to Statistical Pattern Recognition 
MI=!], 
X l = l ,  and M 2 =  
(a) 
Plot the trajectories passing through (15, l), (15, 2), and (15, 3) in 
the original X-space. 
Plot the trajectories in the Y-space, corresponding to the three tra- 
jectories in the X-space. Use cx = In 2. 
(b) 
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
G. S. Sebestyen, “Decision-Making Process in Pattern Recognition,” 
Macmillan, New York, 1962. 
S. S. Wilks, “Mathematical Statistics,” Wiley, New York, 1963. 
S. Kullback, “Information Theory and Statistics,” Wiley, New York, 
1959. 
K. Fukunaga and R. D. Short, A Class of feature extraction criteria and 
its relation to the Bayes risk estimate, Trans. IEEE on Inform. Theory, 
K. Fukunaga and R. D. Short, Nonlinear feature extraction with a general 
criterion function, Trans. IEEE on Inform. Theory, IT-24, pp. 600-607, 
1978. 
K. Fukunaga and S. Ando, The optimum nonlinear features for a scatter 
criterion in discriminant analysis, Trans. IEEE on Inform. Theory, IT-23, 
P. A. Devijver, Relationships between statistical risks and least-mean- 
square-error design criterion in pattern recognition, Proc. 1st Intern. 
Joint Conf. Pattern Recog., Washington, DC, 1973. 
K. Fukunaga and J. M. Mantock, Nonparametric discriminant analysis, 
Trans. IEEE on Pattern Anal. and Machine Intell., PAMI-5, pp. 671-678, 
1983. 
D. H. Foley and J. W. Sammon, An optimal set of discriminant vectors, 
Trans. IEEE Computers, C-24, pp. 28 1-289, 1975. 
S. Watanabe and N. Takvasa, Subspace methods in pattern recognition, 
Proc. 1st Intern. Joint Cnnf Pattern Recop.. Washington. DC. 1973. 
IT-26, pp. 59-65, 1980. 
pp. 453-459, 1977. 

10 Feature Extraction and Linear Mapping for Classification 
507 
1 I .  
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
K. A. Brakke, J. M. Mantock, and K. Fukunaga, Systematic feature 
extraction, Trans. IEEE on Pattern Anal. and Machine Intell., PAMI-4, 
pp. 291-297, 1982. 
A. N. Mucciardi and E. E. Gose, A comparison of seven techniques for 
choosing subsets of pattern recognition properties, Trans. IEEE Comput- 
ers, C-20, pp. 1023- 103 1, 197 1. 
A. W. Whitney, A Direct method of nonparametric measurement selec- 
tion, Trans. IEEE Computers, C-20, pp. 1100-1 103, 1971. 
T. Marill and D. M. Green, On the effectiveness of receptors in recogni- 
tion systems, Trans. IEEE Inform. Theory, IT-9, pp. 11-27, 1963. 
P. M. Narendra and K. Fukunaga, A branch and bound algorithm for 
feature subset selection, Trans. IEEE Computers, C-26, pp. 9 17-922, 
1977. 
S. W. Golomb and L. D. Baumert, Backtrack programming, J .  ACM, 12, 
pp. 516-524, 1965. 
A. L. Chemyavskii, Algorithms for the solution of combinatorial prob- 
lems based on a method of implicit enumeration, Automat. and Remote 
Contr. 33, pp. 252-260, 1972. 
E. L. Lawler and D. E. Wood, Branch and bound methods: a survey, 
Oper. Res., 149, No. 4, 1966. 
N. J. Nilsson, “Problem Solving Methods in Artificial Intelligence,” 
McGraw-Hill, New York, 197 1. 

Chapter I 1  
CLUSTERING 
In the preceding chapters, we have presented a considerable body of 
design theory for pattern recognition. Procedures for classifier design, parame- 
ter estimation, and density estimation have been discussed in detail. We have 
consistently assumed the existence of a training set of classified samples. In 
this chapter, we will focus our attention on the classification of samples 
without the aid of a training set. We will refer to this kind of classification as 
clustering or unsupervised classification. 
There are many instances where classification must and can be per- 
formed without a priori knowledge. Consider, for example, the biological tax- 
onomy problem. Over the years, all known living things have been classified 
according to certain observable characteristics. Of course, plants and animals 
have never borne labels indicating their kingdoms, phylae, and so on. Rather, 
they have been categorized according to their observable characteristics without 
outside supervision. 
The clustering problem is not well defined unless the resulting classes of 
samples are required to exhibit certain properties. The choice of properties or, 
equivalently, the definition of a cluster, is the fundamental issue in the cluster- 
ing problem. Given a suitable definition of a cluster, it is possible to distin- 
guish between good and bad classifications of samples. 
508 

11 Clustering 
509 
In this chapter, two approaches to clustering will be addressed. One is 
called the parametric approach and the other is the nonparametric approach. 
In most parametric approaches, clustering criteria are defined, and given 
samples are classified to a number of clusters so as to optimize the criteria. 
The most commonly used criteria are the class separability measures which 
were introduced in Chapter 10. That is, the class assignment which maximizes 
the class separability measure is considered to be the best clustering result. In 
this approach, the structure (parametric form) of the classification boundary is 
determined by the criterion. The clustering algorithm, which determines 
efficiently the best classification with respect to the criterion, is normally an 
iterative algorithm. In another parametric approach, a mathematical form is 
assumed to express the distribution of the given data. A typical example is the 
summation of normal distributions. In this case, the clustering problem con- 
sists of finding the parameter values for this distribution which best fit the data. 
On the other hand, neither clustering criteria nor assumed mathematical 
forms for the distribution are used in the nonparametric approach. Instead, 
samples are separated according to the valley of the density function. The val- 
ley may be considered as the natural boundary which separates the modes of 
the distributions. This boundary could be complex and not expressible by any 
parametric form. 
In addition, clustering may be viewed as the selection of representatives. 
In general, a density function may be approximated by the Parzen density esti- 
mate around the representatives. Then, we may try to reduce the number of 
representatives while maintaining the degree of approximation. An iterative 
procedure to choose the representatives is discussed in this chapter. 
11.1 Parametric Clustering 
In this section, we will present, first, a general-purpose clustering algo- 
rithm based on a generalized criterion. Then, the discussion for a specific cri- 
terion follows. 

510 
Introduction to Statistical Pattern Recognition 
General Clustering Algorithm 
The clustering algorithm developed in this section applies to a wide 
range of criteria. However, it is necessary to specify the form of the criterion 
as well as some other details of the clustering problem at the outset. 
Clustering criterion: Assume that we want to classify N samples, 
X 
. . . ,XN. These vectors are not denoted as random vectors because, in the 
clustering problem, they are assumed to be fixed and known. Each sample is 
to be placed into one of L classes, ol, 
. . . ,aL, 
where L is assumed to be given. 
The class to which the ith sample is assigned is denoted ok, 
(i = 1, . . . ,N). 
For convenience, let the value of kj be an integer between 1 and L. A classif- 
cation R is a vector made up of the ok,’s, and a configuration X *  is a vector 
made up of the X;’s, that is, 
R = [Ok, . . . O k , ]  T 
(11.1) 
and 
x* = [ X T . .  . X i ] ’ .  
(11.2) 
The clustering criterion J is a function of R and X *  and can be written 
J =J(o,,, 
. . . ,o,,; 
X I , .  . .,XN) =J(R;x*). 
(11.3) 
By definition, the best classification Ro satisfies either 
J(R,;x*) = max or min J(R;x*) 
R 
a 
(1 1.4) 
depending on the criterion. For the remainder of this section, we will discuss 
only the minimization problem, since the maximization is similar. 
Example 1: Six vectors, X I , .  . . ,X6, of Fig. 11-1 are given. The 
problem is to find class assignments of these vectors to one of two classes so 
as to minimize a criterion. Let J = tr(S;lS,.) 
be the criterion where S,. and S,, 
are the within-class and mixture scatter matrices defined in (10.1) and (10.4). 
For 
each 
classification, 
for 
example 
( X l , X 2 , X s  1 E o1 and 
( X 3 , X 4 , X 6 }  E 0 2  as shown by dotted lines, or ( X l , X 2 , X 3 ]  E o1 and 
( X 4 , X 5 , X 6 )  E w2 as shown by solid lines, the mean vectors and covariance 
matrices for o1 and o2 are estimated, and S,., 
S,,, and J can be computed. 

1 1  Clustering 
51 1 
\ \ 
/ -- 
x6' 
Fig. 11-1 An example of clustering. 
Note that S,. varies depending on the class assignment but S, does not. The 
class assignment by the solid lines would give the minimum J (the smallest 
within-class scatter) among all possible combinations of class assignments. 
Clustering algorithm: For a given clustering problem, the configuration 
X *  is fixed. The clustering algorithm varies only the classification R. Ordi- 
nary steepest descent search techniques cannot be applied because of the 
discrete and unordered nature of R. Still, it is possible to define an iterative 
search algorithm based on variations in J with respect to variations in 52. 
Suppose, at the Eth iteration, the classification is R(t), where 
(1 1.5) 
If the ith sample is reclassified from its present class k,(PJ to class j ,  the cluster- 
ing criterion varies by an amount AJ (i,j, !), which is given by 
AJ(i,j,+J(w~.,(o, . . ..a,, 
,(,),aj,w~,+,(,), 
. . . .w,,,,);X*)-J(n(~);x"). 
(11.6) 
If AJ(I,J,S> is negative, reclassification of the ith sample to class j yields a 
classification that is improved in terms of J. This fact is the basis of the fol- 
lowing algorithm: 

5 12 
Introduction to Statistical Pattern Recognition 
(1) Choose an initial classification R(0). 
(2) 
Given the lth classification R(t), calculate 
Lw(i,j,Z) for 
(3) For i = 1,2,. . . ,N, reclassify the ith sample to class t, where 
AJ(i,r,t) = min AJ(i,j,t) . 
1 
j = 1,2,. . . ,Land i = 1,2,. . . ,N. 
(11.7) 
Decide ties involving j = ki(Y.) in favor of the present classification. Decide 
other ties arbitrarily. This step forms R(t + 1). 
(4) If R(E + 1) # R(Q, return to Step (2). Otherwise the algorithm is 
complete. 
The algorithm is simply the iterative application of a classification rule 
based on the clustering criterion. Here, we adopted a procedure in which all of 
the samples are reclassified simultaneously at each iteration. An alternative 
approach is to reclassify each sample one at a time, resulting in similar but 
slightly different clusters. In these iterative procedures, there is no guarantee 
that the algorithm converges. Even if it does converge, we cannot be certain 
that the absolute minimum of J has been obtained. Therefore, we must depend 
on empirical evidence to justify the algorithm. 
In contrast to these potential weaknesses, the algorithm described above 
is very efficient. Like any good search algorithm, it surveys a small subset of 
classifications in a systematic and adaptive manner. It is easily programmable 
for any criterion of the form of (1 1.3). 
Determining the number of clusters: So far, we have ignored the prob- 
lem of determining the number of classes L, and have assumed that L is given. 
However, in practice, L is rarely known. We not only need to determine L but 
also the proper class assignment. For that purpose, we may run the clustering 
procedure for the various values of L, and find the best classification for each 
value of L. Let JL be the optimal criterion value for a given L after the cluster- 
ing procedure has converged. If JL decreases as L increases, and either reaches 
the minimum point at LO or becomes flat after Lo, then we may use Lo as the 
proper number of classes. Unfortunately, many of the popular criteria do not 
have this favorable property. For example, consider J = tr(S,'S,.) 
of Example 
1. As L increases, samples are divided into smaller groups, and consequently 
the within-class scatter becomes smaller. This means that J might decrease 

11 Clustering 
513 
monotonically with L. Finally, when L becomes N ,  the total number of sam- 
ples, each class consists of one sample only, and there is no within-class scatter 
(J = 0). Although L = N minimizes this criterion, this is obviously not the 
solution we want. 
It appears, therefore, that some external method of controlling L is neces- 
sary. Unfortunately, no unified theory for determining L has been fully 
developed and accepted. 
Merging and splitting: After a number of classes is obtained, we may 
consider the merging of two classes into a single class or the splitting of a 
class into a number of classes. 
Basically, merging is desirable in two instances. The first is when two 
classes are very similar. The similarity may be measured in a number of ways. 
The Euclidean distance between two mean vectors is the simplest measure but 
is not an accurate one. The Bhattacharyya distance of (3.152), based on the 
normal assumption, could be a reasonable compromise between simplicity and 
accuracy. The second instance in which merging may be appropriate is when 
the population of a class is very small. In this case, the class may be merged 
with the most similar class, even when the similarity is not very close. 
Deciding whether or not a class is to be split is a more complex problem. 
Too large a population suggests that the class is a candidate for a split. Mul- 
timodal and nonsymmetric distributions as well as distributions with large vari- 
ances along one direction are also candidates for splitting. In order to identify 
these characteristics, various tests are necessary. Splitting a class may be car- 
ried out by applying the clustering procedure to the samples in the class. 
It goes without saying that this kind of merging and splitting is very 
heuristic. Its merit lies in the fact that it is efficient and requires a minimum of 
human interaction. 
Multiple dichotomy: It is somewhat more satisfying to adopt an 
approach which depends entirely on the clustering criterion J .  One such 
approach has been suggested [ I ]  and is outlined as follows. 
Suppose that for L = 2 there are several distinct classifications which 
yield a nearly minimal value of J .  If these classifications differ only in the 
classification of a few samples, there is no reason to suppose the existence of 

514 
Introduction to Statistical Pattern Recognition 
more than two classes. If the classifications are grossly different, however, 
then it is evident that several classes are actually present. 
2nd dichotomy 
/ 
/ 
/ 
Fig. 11-2 Multiple dichotomy of three classes of samples. 
Figure 11-2 illustrates two possible dichotomies of a collection of sam- 
ples apparently containing three classes A I ,  A*, and A 3 .  One dichotomy 
separates the samples into A ,  u A  and A 3 ,  while the other results in the two 
classes A I and A 2  u A 3 .  Thus, A 3 ,  A 
and A u A 3  = A 2  are seen to be dis- 
tinct classes (A is the complement of the set A.) 
Now let us consider the more general case where there are k dichotomies 
of a collection of samples containing L classes. Each dichotomy separates 
these samples into two groups. Let S,- be the set of all samples assigned to 
group j by the ith dichotomy for j = 1,2 and i = 1, . . . ,k. Assume that the fol- 
lowing two conditions hold. 
(a) A dichotomy places each class into only one group, that is, classes 
are not split by dichotomies. 
(b) For each pair of classes, there is at least one dichotomy which does 
not assign the two classes to the same group. 
Select one group from each of the k dichotomies and form a subset C as 
the intersection of these k groups. By condition (a), if C contains one sample, 
then it must contain all of the samples of that class. By condition (b), for any 
other class, there is at least one of the k selected groups to which that class 
does not belong. Therefore, if C is nonempty, then C contains one and only 
one class. Hence, in order to construct all the L classes, we consider the 2'' 
subsets of the form. 

11 Clustering 
515 
(1 1.8) 
where each j equals 1 or 2. Each nonempty C is a class. In our example, we 
have 
so that 
C(1, 1) = S l l  nSzl = A ,  , 
C(1, 2) = S I 1  n S 2 2  = A 2  , 
C(2, l)=S,,nSzl = 0 ,  
C(2, 2 ) = S 1 2 n S 2 2  = A 3  , 
(1 1.10) 
which is in agreement with our earlier argument. 
The multiple dichotomy approach has a stronger theoretical basis than 
the merging and splitting procedure. Further, it relies on no numerical criterion 
other than .I. 
However, implementation of the multiple dichotomy approach 
can be difficult, especially when the true number of classes is large. In addi- 
tion, the conditions (a) and (b) mentioned above are rarely satisfied in practice. 
These difficulties may be overcome somewhat by imposing a hierarchical 
structure on the classes. The samples are divided into a small number of 
classes, each class is divided further, and so on. Under this strategy, we need 
not find every possible dichotomy of the entire collection of samples. 
At this point, we depart from general discussion of the clustering algo- 
rithm. Obviously, the discussion is incomplete. We have a basis, however, to 
develop and implement clustering procedures. Therefore, let us turn our atten- 
tion to the detailed derivations of clustering procedures. 
Nearest Mean Reclassification Algorithm [2-51 
In this section, we will discuss clustering procedures based on parame- 
ters such as mean vectors and covariance matrices. We will show that the cri- 
teria of class separability discussed in Chapter 10 play an important role, and 
that the iterative algorithms of the previous section take on simple forms. 

516 
Introduction to Statistical Pattern Recognition 
Criteria: Clustering can be considered as a technique to group samples 
so as to maximize class separability. Then, all of the criteria which were dis- 
cussed in Chapter 10 may be used as clustering criteria. In this section only 
functions of scatter matrices are discussed due to the following reasons: 
(1) The extension to multiclass problems is straightforward. In this 
respect, the Bhattacharyya distance has a severe disadvantage, since it can be 
applied only to two-class problems. 
(2) Most clustering techniques are based on the scatter of mean vectors. 
Finding clusters based on covariance-differences is extremely difficult, unless 
some mathematical structure is imposed on the distribution. Therefore, the 
functions of scatter matrices fit well to clustering problems. 
(3) The simplicity of the criteria is a significant advantage, because in 
clustering we have the additional complexity of unknown class assignment. 
For feature extraction, we could choose any combination of S,, S,,,, and 
S, as S 
I and S2 to form a criterion J = tr(S;'S I ). However, for clustering it is 
preferable to use S,,, as S2, because S,,, is independent of class assignment. It 
would be too complicated if we had to recompute the inverse of S 2  each time 
the class assignment was altered in iteration. Therefore, our choice is limited 
to either tr(S;'S,) 
or tr(S;'Sw,). These two criteria are the same, because 
tr(S;'Sh) 
= tr{S;'(Sm-S,,,)} = n - tr(S;'S,,). 
In this chapter, we will use 
J = tr(S;'SH,). 
Another important consideration in selecting criteria for clustering is to 
ensure that the clustering procedures give the same classification for a given set 
of samples regardless of the coordinate system of these samples. The chosen 
criterion, J = tr(S;'S,,.), 
satisfies this condition, since the criterion is invariant 
under any nonsingular linear transformation. 
Clustering algorithm: Let us assume that M o  = 0 and S, = I  without 
losing generality. If the given samples do not satisfy these conditions, we can 
shift the coordinate origin and whiten the data with respect to S,. 
Then, using 
(10.1) the criterion is rewritten as 

11 Clustering 
517 
(11.11) 
Changing the class assignment of X ,  from the current class k, to class j at the 
?th iteration, we delete from (1 1.1 1) the term IIX, - M,,(t)11* and insert a new 
term IIX, - M,(L)1I2. Thus, 
(11.12) 
Since the second term of (1 1.12) is independent of j ,  the reclassification of X ,  
at the tth iteration can be carried out by 
llxi - M,(PJII 
= min llx, - M,(~)II 
+ x E o, . 
(11.13) 
i 
In words, the algorithm becomes: 
(1) 
Choose 
an 
initial 
classification, 
Q(O), 
and 
calculate 
M,(O), . . . M L ( 0 ) .  
(2) Having calculated sample mean vectors M I ( ! ) ,  . . . ,ML(%) at the 4th 
iteration, reclassify each X i  according to the nearest Mi(%). 
(3) If the classification of any X i  is changed, calculate the new sample 
mean vectors MI(,’ + 11,. . . ,ML(P + 1) for the new class assignment, and 
repeat from Step (2). Otherwise, stop. 
This algorithm is called the nearest mean reclassification rule. 
Figure 11-3 shows how the iterative process works. At the 4th step, 
samples are divided to three clusters, and their sample means, Mi(&)’s, are com- 
puted. All samples are now reclassified according to the nearest means. That 
is, the new boundary is piecewise linear, bisecting each pair of Mi(E)’s. In Fig. 
11-3, there are three clearly separated clusters. We can see that the boundary 
is indeed improved by this operation. 
From the above discussion, some properties of the nearest mean 
reclassification algorithm become evident. They are: 
( 1 )  Clusters are divided by piecewise linear bisectors. Only the means 
contribute to determine the boundary and covariance matrices do not affect the 
boundary. 

518 
Introduction to Statistical Pattern Recognition 
+ 
X 
X 
X 
J 
\ 
.()+l)-th 
- 
\ 
step 
Fig. 11-3 An example of the nearest mean reclassification algorithm. 
(2) The number of clusters must be preassigned. 
(3) The initial classification, Q(O), may be given randomly. No matter 
how Q(0) is given, the M,(O)’s are computed and the reclassification of sam- 
ples according to the nearest M,(O)’s results in a piecewise linear boundary. 
This is equivalent to selecting the number of vectors, M,(O)’s, initially accord- 
ing to the number of clusters. Random class assignment does not impose any 
extra instability on the algorithm. 
In order to verify the algorithm, the following experiment was con- 
ducted. 
Experiment 1: One hundred samples were generated from each of Data 
I-A, and mixed together to form 200 samples. Then, these samples were 
classified to two clusters. Table 11-1 shows the confusion matrix of this 
experiment [5]. All 100 samples from w, with 19 samples from w2 were 
assigned to one cluster, and 81 samples from q are assigned to the other clus- 
ter. The error rate is 19/200 = 0.095. Recall that we got 5% error for this data 
by designing the optimum linear classifier in Chapter 4. Considering the fact 
that any covariance information was not used in this clustering algorithm, the 
error rate of 9.5% is reasonable. Furthermore, since all error samples came 
from one class, we could improve the error rate simply by adjusting the deci- 
sion threshold. 
Convergence [5]: The nearest mean reclassification algorithm is not 
guaranteed to converge. In this section, we will discuss the conditions under 

11 Clustering 
TABLE 11-1 
519 
CONFUSION MATRIX FOR THE NEAREST MEAN 
RECLASSIFICATION ALGORITHM 
Assigned class 
class 
19 
81 
which the separating hyperplane converges for two normal distributions with 
equal covariance matrices. 
Let us assume that two normal distributions are N x ( M I , X I )  and 
Nx(M2,X2) after normalization and that XI = X 2  = Z. The normalization 
makes S, = I and M o  = 0. The Bayes classifier in this case becomes linear as 
( M ~  
- M ~ ) ~ C - ' X  
+ c = 0 ,  
(11.14) 
where c is a constant. 
Since S, = I  = C + P I M I M Y  + P2M2MT and 
M o = O = P , M ,  +P,M2, 
(11.15) 
Using (2.160), 
(11.17) 
Substituting (1 1.16) and (1 1.17) into (1 1.14), the Bayes classifier is 
M : X + c = o .  
P I-P2M;M2 
(11.18) 
Thus, the optimum hyperplane for the equal covariance case is always in the 

520 
Introduction to Statistical Pattern Recognition 
direction of M 2  which is the same as the mean-difference vector M2-M I .  
This property, which the original coordinate system does not have, is a 
significant advantage of the normalized coordinate system. 
For the equal covariance case, we can show that the algorithm converges 
to M2-M I from a wide range of initial classifications. After a given iteration, 
samples are separated by a hyperplane whose direction is, say V (IlVll = 1), as 
shown in Fig. 11-4. Also, the position of the hyperplane is specified by LI and 
Fig. 11-4 Separation of two distributions. 
which are the distances of M I  and M 2  from the hyperplane. Let D,, and D, 
be the centers of probability mass for the positive and negative sides of the 
hyperplane. Then, following the nearest mean reclassification rule, the direc- 
tion of the succeeding hyperplane will be D,-D,. 
So, our convergence proof 
is to show that the angle between D,,-D, and M2-M I is smaller than the angle 
between V and Mz-M 
Since the hyperplane separates each distribution into two parts, the posi- 
tive and negative sides, we have four probability masses Rij (i = 1,2; j = p,n), 
as shown in Fig. 11-4. Let Djj and 4jj be the mean vectors and populations of 
these probability masses. Then, 

1 1  Clustering 
52 1 
where 
(11.19) 
(1 1.20) 
(1 1.21) 
(1 I .22) 
(1 1.23) 
(1 1.24) 
of = VTC,V , 
(1 1.25) 
and @ is the normal error function. The sign + or - is selected, depending on 
whether M I  is located in R,,, or RIP. The details of the derivation of (1 1.19) 
through (1 1.25) are left to the reader. However, the following information 
could be helpful in deriving (1 1.19) through (1 1.25). Since X is normal, 
y = VTX is also normal with the variance of ( I  I .25). In the y-space, the proba- 
bility of mass for the positive side, h,, can be computed by (1 1.24). The vector 
D,, - M I  has the direction of C,V, and the projections of D,, - M I  on 
V 0 = p , n )  are VT(D,, - M I )  = a,o,/hf and 
VT(DIlI - M I )  = -a,o,/( 1 4 , ) .  
These are obtained by computing the expected values of y for the positive and 
negative sides. From D,, and q,,, D,, and D,l are obtained as 
qI,# 
I/’ + q2p2p 
D, = 
4 I p  + 4 2 p  
(1 1.26) 
(1 1.27) 
Substituting (1 1.19) through (1 1.22) into ( 1  1.26) and (1 1.27), 

522 
Introduction to Statistical Pattern Recognition 
P I P 2(b I-b2)(M 1 -M2)+{ P I (a 1\01 P
I
 +P 2(a 2/02)C2 I v 
(P 1 b I +P2b2) 1-(P I b I +P 2b 2 )  I 
D, -D, = 
(11.28) 
For the equal covariance case, 
C, = C2 = C and oI = o2 = 0 .  
(11.29) 
Furthermore, under the normalization of S, = I and Mo = 0, I: and M2-M 
are 
expressed as functions of M 2  as in (11.15) and (11.16). Therefore, (11.28) 
becomes 
D , - D , = c I M ~ + c ~ V ,  
( 1  1.30) 
where 
The normal of the new hyperplane has a component in the direction of V 
and another in the direction of M 2 .  If the coefficient of M 2 ,  c l ,  has the same 
sign as M:V, the successive hyperplane becomes more nearly parallel to M 2. 
Since c 2  and the denominator of c I  are positive, we need to show that the 
numerator of c I  and M:V have the same sign. We examine only the case 
where M i V  > 0. The discussion for M:V < 0 is similar to the one for 
M:V > 0. For M:V > 0, we see from Fig. 11-4 that 
(1 1.33) 
e, +& = (M,-M,) T
I
 
v = --M$V. 
PI 
Using ( 1  1.33), the condition for convergence becomes 
s, + t 2  
b Z - h l  >- 
( P , U l  + P 2 a 2 ) .  
( 1  1.34) 
0 
It is easily seen that the inequality of ( 1  1.34) is not satisfied for certain 
combinations of parameters. However, the region of parameters where (1 1.34) 
is satisfied can be calculated numerically. The result is shown in Fig. 11-5. 

11 Clustering 
523 
Y 
0.6 
0.4 
0.2 - 
O O  
0.2 
0.4 
0.6 
0.8 
1 .o 
Pl 
Fig. 11-5 Region of convergence. 
Equations (1 1.23) and (1 1.24) with (1 1.34) show that we have three parameters 
in (1 1.34), II /o, &/CY, and P I (P = 1 - P I ), or 
(1 1.35) 
In Fig. 11-5, the convergence regions of y and P 
are plotted for various 
values of k [5]. The figure shows that convergence is quite likely in practice, 
except for either extreme P I ’s or y’s. 
Branch and bound procedure [6]: The nearest mean reclassification 
algorithm works fine for many applications. However, there is no guarantee of 
the convergence of the iterative process. Also, the process might stop at a 
locally minimum point and fail to find the globally minimum point. 
Since assigning a class to each sample is a combinatorial problem, the 
brunch and bound procedure discussed in Chapter 10 may be applied to find 
the optimum class assignment. 
Figlfre 11-6 shows a solution tree for the clustering problem with four 
samples and three clusters. In general, there are L N  different Q’s for classify- 

5 24 
Introduction to Statistical Pattern Recognition 
A 
Fig. 11-6 A solution tree for clustering. 
ing N sample vectors into L clusters. However, since the label of each cluster 
may be chosen arbitrarily, each classification could have several different 
expressions for R. For example, Q = [l 1 2 21 and R = [2 2 3 31 are the same 
classification, both indicating that X I  and X 2  are in one cluster and X ,  and X4 
are in one of the other clusters. In order to eliminate this duplication, we 
assign the first sample X I  to cluster 1, the second sample X ,  to either cluster 1 
or 2, and so on, as shown in Fig. 11-6. 
In order for the branch and bound procedure to work effectively, we 
need to have a criterion which satisfies the monotonicity condition. Let J,(Q,) 
be the criterion to be minimized for R,, = [ak, 
. . . ak,,], 
where the subscript m 
indicates the number of sample vectors involved. Then, the monotonicity con- 
dition is stated as 
Jm+l (Qmmk",+, ) 2 1 , ( R m )  . 
(1 1.36) 
That is, the J of a node is smaller than the J's for all nodes which are succes- 
sors of the node. 
Let a be the value of JN which is the current smallest among all cases 
tested so far for the classification of N samples (for example, a = J ( A ) ) .  Then, 
the branch and bound procedure checks at each node (for example, at B) 
whether or not the following inequality is satisfied 

11 Clustering 
525 
J m ( Q m )  
. 
(11.37) 
If yes, then from (1 1.36), all successors of this node have J’s larger than a. It 
means that the optimum classification does not exist in the subtree under the 
node. Thus, the subtree is rejected without further tests, and the search back- 
tracks to the next node (for example, C). This elimination of subtrees makes 
the branch and bound procedure a very efficient tree search technique. When 
The 
(1 1.37) is not satisfied, the search moves down to a node in the next level. 
node selected for the next evaluation is determined by 
J,+l 
(Qni,t) = min Jm+, ( Q n i > l )  . 
(1 
< 
That is, Xm+, is assigned to cluster t, and the search goes on. 
.38) 
The criterion J = tr(S;’S, ) satisfies the monotonicity condition with a 
minor modification. Again, assuming S ,  = I, ( 1  1 . I  1) is the criterion to be 
minimized. Since the number of samples is determined by the level of the 
solution tree and is independent of Q, let us delete it from the criterion and 
rewrite the criterion for m samples, X . . . ,Xm, as 
L n1, 
Jm(nm) 
= c~llxy 
- M, 112 , 
(1 1.39) 
where m, is the number of a, 
-samples among X I , .  . . ,Xm. When Xn,+, 
is 
added into cluster 5, M, must be modified to M,’, 
including the effect of Xm+l, 
and IIXm+l - M,‘ \I2 must be added to the summation. Thus, 
I =IJ=l 
Jm+,(Qm?’) = J m ( Q n i )  +AJ(1) 
7 
(1 1.40) 
where 
m 
/ = I  
AJ(?)=lIXn,+~ 
-M,’\\’+E( 
\lX~’--Mf’\\*-\\X~)-M 
\ I 2 ) .  
(11.41) 
The new ;-class mean, M ’, can be obtained as 
(1 1.42) 

526 
Introduction to Statistical Pattern Recognition 
Substituting (1 1.43) into (1 1.41) and using M I  = ( l/mt)X~!lX~), 
m1 
me + 1 
N(L) = - 
IIXm+l - M,IP 20 . 
(11.43) 
(11.44) 
Since AJ(L) 20, from (1 1.40) the criterion has the monotonicity property. 
Note that (1 1.40), (1 1.44), and ( 1 1.42) provide recursive equations for 
computing Jm+,(Qm,E) and Mt’ from Jm(Qm), M t ,  and Xm+l. Also, (11.38), 
(1 1.40), and (1 1.44) indicate that the selection of the next node can be made by 
minimizing AJ(f) with respect to e. 
For a large N, the number of nodes is huge. Thus, the initial selection of 
a becomes critical. One way of selecting a reasonably low initial a is to apply 
the iterative nearest mean reclassification algorithm to get a suboptimal solu- 
tion and use the resulting criterion value as the initial a. The branch and 
bound procedure gives the final solution which is guaranteed to be optimum 
globally. 
Also, it is possible to make the procedure more efficient by reordering 
the samples [6]. 
Normal Decomposition 
Piecewise quadratic boundary: The nearest mean reclassification rule 
can be extended to include more complex boundaries such as quadratic ones. 
Following the same iterative process, the algorithm would be stated as follows: 
(1) Choose an initial classification, Q(O), and calculate P,(O), Mi(0) and 
(2) Having calculated class probabilities, Pi(!,), and sample means and 
covariance matrices, Mi(!.) and Xi(!), at the Lth iteration, reclassify each Xi 
according to the smallest ( 1/2)(Xj-Mj)T CT1 (Xi-Mi)+( 1/2) In I C j  I -In P,. The 
class probability for a; is estimated by the number of oj-samples divided by 
the total number of samples. 
(3) If the classification of any Xj is changed, calculate the Pi(t+I), 
Mj(E+l) and Zj(f+l) for the new class assignment, and repeat from Step (2). 
Ci(0) (i = 1,. . . ,L). 

11 Clustering 
527 
Otherwise stop. 
This process is identical to the nearest mean reclassification algorithm, 
but results in a piecewise quadratic boundary. Also, since the estimation of the 
covariance matrices is involved, the process is more computer-time consuming 
and more sensitive to parameters such as sample size, dimensionality, distribu- 
tions, and so on. 
More fundamentally, the clustering techniques mentioned above may 
have a serious shortcoming, particularly when a mixture distribution consists of 
several overlapping distributions. An important goal of finding clusters is to 
decompose a complex distribution into several normal-like distributions. If we 
could approximate a complex distribution by the summation of several normal 
distributions, it would be much easier to discuss all aspects of pattern recogni- 
tion, including feature extraction, classifier design, and so on. However, the 
clustering procedures discussed above decompose a mixture as in Fig. 11-7(b) 
rather than as in Fig. 11-7(a). The hatched distribution of cluster 1 in Fig. 1 1- 
7(b) includes the tail of the w2-distribution and does not include the tail of the 
ol-distribution. As a result, the mean and covariance matrix of the hatched 
distribution in Fig. ll-7(b) could be significantly different from the ones for 
the hatched distribution of Fig. 11-7(a). Thus, the representation of a complex 
distribution by the parameters obtained from the clustering procedures 
described above could be very poor. 
Decomposition of a distribution into a number of normal distributions 
has been studied extensively [7]. The two most common approaches are the 
method of moments and maximum likelihood estimation. In the former method, 
the parameters of normal distributions are estimated from the higher order 
moments of the mixture distribution (for example, the third and fourth order 
moments [8]). This approach is complex and not very reliable for high- 
dimensional cases. Therefore, in this chapter, only the latter approach is 
presented in detail. 
Maximum likelihood estimate [9-lo]: In order to obtain the hatched 
distribution of Fig. 11-7(a) from p ( X ) ,  it is necessary to impose a mathematical 
structure. Let us assume that p ( X )  consists of L normal distributions as 

528 
Introduction to Statistical Pattern Recognition 
(b) 
Fig. 11-7 An example of the shortcoming of the clustering technique. 
(1 1.45) 
where p, (X) is normal with the expected vector M, and covariance matrix C,. 
Under this assumption, our problem is to estimate P I ,  M I ,  and C, from N avail- 
able samples, X I ,  . . . ,XN, drawn from p (X). One way of solving this problem 
is to apply the maximum likelihood estimation technique. The maximum likeli- 
hood estimates may be obtained by maximizing llY=,p(X,) with respect to P I ,  
M I ,  and E, under a constraint Ef=,P, = 1. Taking the logarithm of lTy'lp(X,), 
the criterion to be maximized is 
(1 1.46) 
where p is a Lagrange multiplier. 

11 Clustering 
5 29 
First, computing the derivative of J with respect to Pi, 
( 1 1.47) 
where 4 , ( X )  = P,p,(X)/p ( X )  is the a posteriori probability of a,, 
and satisfies 
cf=,q,(x) 
= 1. 
Since 
c ~ = ~ P , ( ~ J / ~ P , )  
= c ; Y = ~ ( c ~ = ~ ~ , ( x , ) )  - ( ~ f = ~ ~ , ) p  
= N - 
0, 
CI=N 
3 
(1 1.48) 
and from ( 1 1.47) 
Next, the derivative of J with respect to M, can be computed as 
(1 1.49) 
Using Cy=lq,(X,) = N PI = N ,  where N, is the number of q-samples, (1 1.50) 
can be solved for M , ,  resulting in 
(1 1.51) 
l
N
 
M/ = - ql cxJ 
’ 
Nl /=I 
At last, the derivative of J with respect to X i  is, from (A.9) and (A.23), 
where diag[A] is a diagonal matrix, keeping only the diagonal terms of A. 
Equation (1 1 .52) can be solved for C, yielding 
(1 1.53) 
I
N
 
c, = --Cs,cx,)(X,-M,)(X/-M,)T 
. 
N! / = I  

530 
Introduction to Statistical Pattern Recognition 
By solving (11.49), (llSl), and (11.53), we can obtain the optimum 
solution. However, since q , ( X )  is a function of Pk, Mk, and Ck (k = 1 , .  . . ,L), 
it is very difficult to obtain the solution explicitly. Therefore, we must solve 
these equations iteratively. The process can be described as follows. 
(1) Choose an initial classification, Q(O), and calculate PI, M,, and 
C, (i = 1,. . . ,L). 
(2) Having calculated P?, M f ) ,  and q?(XJ), compute P r I ) ,  My’), and 
Cy” by ( I  1.49), (1 1.5 l), and (1 1.53), respectively. The new qyl)(XJ) 
can be 
calculated as 
(1 1.54) 
k = l  
where the superscript indicates the (t+l)st iteration, and p?”(X) is a normal 
density function with mean MI“’) and covariance matrix Cy1). Note that each 
sample XJ carries L probability values q I (X,), . . . ,qL(XJ) instead of being 
assigned to one of the L classes. 
(3) When qllfl)(X,) = qp)(XJ) for all i = 1, . . . ,L and j = 1, . . . ,N, then 
stop. Otherwise, increase by I and go to Step (2). 
In the maximum likelihood estimation technique, the criterion (the first 
term of (1 1.46)) may be used to determine the number of clusters. The max- 
imized criterion value, JL, is obtained for a given L, and the procedure is 
repeated for various values of L. The criterion JL tends to increase with 
increasing L, and reach a flat plateau at Lo. This means that, even if we use 
more normal distributions than LO, the mixture distribution cannot be better 
approximated. Therefore, Lo is the proper number of clusters to use in approx- 
imating the mixture distribution. 
In order to verify the procedure, the following two experiments were run. 
Experiment 2: One hundred samples per class were generated from 
two-dimensional normal distributions specified by 
1 0.7 
M ,  
= M * =  I] , XI = 
-:‘7] 
, and & =  L.7 1 ]  . 
(11.55) 
The sample means and covariance matrices of the generated samples were 

11 Clustering 
53 1 
= kl,6] , ,. [ 0.91 -0.651 
'I 
= -0.65 
1.17 ' 
These two hundred samples were mixed, 
w2 depending on x2 20 or x 2  < 0 (x2 is 
tions, the parameters became 
A 
PI ~ 0 . 6 1  , P2 ~ 0 . 3 9 ,  
::::I 
, 
(1 1.56) 
and initially assigned to either w1 or 
the second variable). After 10 itera- 
-0.471 
,. 
p . 0 4 1  , ,. 
1.12 0.661 
(1 1.57) 
M 2  = -0.06 
= 0.66 0.78 . 
Note that the two distributions share the same mean and are heavily over- 
lapped. This means that finding clusters must be very difficult. Despite the 
expected difficulty, the procedure found two reasonable clusters successfully. 
Without imposing the mathematical structure of (1 1.45), no other clustering 
technique works properly for this example. 
Experiment 3: One hundred samples per class were generated from 8- 
dimensional normal distributions of Data I-A, and initially assigned to either 
w1 or w2 depending on x8 50 or x8 > 0 (x8 is the eighth variable). After 20 
iterations, samples were classified to either w1 or w2, depending on whether 
q , ( X )  > q 2 ( X )  or q , ( X )  < q 2 ( X ) .  Table 11-2 shows the resulting confusion 
matrix. This error of 2.5% is very close to the Bayes error of 1.9%, and is 
much better than the 9.5% error of the nearest mean reclassification algorithm 
[see Table 11-11. 
In order to confirm that the mixture distribution was properly decom- 
posed into two normal distributions, a quadratic classifier was designed based 
on P,, 
M,, and I;, obtained from the two clusters. Independently, 100 samples 
per class were generated according to Data I-A, and classified by the quadratic 
classifier. The resulting error was 2.5%. Considering the fact that the holdout 
method (design and test samples are selected independently) always produces 
an error larger than the Bayes error, the designed classifier was very closed to 
the Bayes. 

532 
Introduction to Statistical Pattern Recognition 
TABLE 11-2 
CONFUSION MATRIX FOR THE MAXIMUM 
LIKELIHOOD ESTIMATION ALGORITHM 
Assigned class 
k 
In order to determine the proper number of clusters, the experiment was 
repeated for various values of L. For a given L, Pi and pi(X) (i = 1, . . . L) of 
(1 1.45) were estimated, and subsequently the first term of (1 1.46), 
J =CY=,lnp(Xj), was computed. Figure 11-8 shows JIN vs. L. The curves 
are flat for L 22 and N = 400, 800, indicating that two normal distributions are 
adequate to represent this data. The number of samples assigned to each clus- 
ter is NIL on the average. Therefore, when NIL becomes smaller (for example 
NIL = 50 for N = 200 and L = 4), each cluster may not have an adequate sam- 
ple size to estimate the covariance matrix properly. This is the reason that the 
curve decreases as L increases for N = 200 in Fig. 11-8. 
So far, we have presented the recursive equations to estimate a priori 
probabilities, mean vectors, and covariance matrices. However, in some appli- 
cations, we can assume some of the parameter values or the relationship among 
the parameters as follows: 
(1) all covariance matrices are equal [see Problem 21, 
(2) all mean vectors are equal, or 
(3) a priori probabilities are known [see Problem 31. 
With the above conditions, the maximum likelihood estimation technique can 
be applied to estimate the remaining parameters. Because of the additional 
information, we can obtain a better estimate with faster convergence. 

11 Clustering 
J/N 
-12.2 -- 
-12.4 -- 
-12.6 -- 
-12.8 -- 
-13.0 -- 
-13.2 -. 
-1 3.4 -- 
-13.6 -- 
533 
A 
I 
I 
I 
I 
I 
I 
I 
1 
I 
,L 
0 
1 
2 
3 
4 
Fig. 11-8 Criterion value vs. L for Data I-A. 
11.2 Nonparametric Clustering 
When a mixture density function has peaks and valleys as shown in Fig. 
11-9, it is most natural to divide the samples into clusters according to the 
valley. However, the valley may not have a parametric structure such as 
hyperplanes. quadratic surfaces, and so on. As discussed in the previous sec- 
tion, the parametric structure of the boundary comes from either the use of a 
parametric criterion or from the underlying assumption that the distribution 
consists of several normal distributions. For the distribution of Fig. 11-9, we 
cannot expect to get reasonable clusters by a parametric boundary. 
In order to find the valley of a density function in a high-dimensional 
space, we need a nonparametric technique to characterize the local structure of 
the valley. There are many nonparametric clustering procedures available. 
However, most of them are implicitly or explicitly based on the estimate of the 
density gradient. In Fig. 11-9, if we estimate the gradient of the density func- 

534 
Introduction to Statistical Pattern Recognition 
Fig. 11-9 Clusters separated by the valley. 
tion at each existing sample point (as indicated by mows) and move the sam- 
ple toward the direction of the gradient, samples move away from the valley 
area. Repeating this process, the valley becomes wider at each iteration, and 
samples form compact clusters. This procedure is called the valley-seeking 
procedure. 
Thus, the valley-seeking procedure consists of two problems: one is how 
to estimate the gradient of a density function, and the other is how to utilize 
the estimate to fom clusters. 
Estimation of Density Gradient 
In this section, we develop the estimation technique of the density gra- 
dient, and discuss how to apply this technique to pattern recognition problems. 
Estimation of density gradient [ll]: In order to estimate the gradient of 
a density function at X, let us select an ellipsoidal local region r(X) with radius 
r, specified by 
r(x) = {Y: 
~ ( Y , x )  I , 
(1 1.58) 
where 

1 1  Clustering 
535 
d 2 ( Y , X )  = (Y-X)TA-'(Y-X) 
(11.59) 
and A is the metric to measure the distance. The expected vector of Y in T ( X ) ,  
which is called the local mean, can be computed as 
M ( ~ ) = E  
( (y-x) I r(x) )=I ( Y - x ) ~ ~ Y  
, 
( 1  1.60) 
(X ) 
u0 
where 
uo = [,,,P 
( W Y  gp (X)V 
(1 1.61) 
and v is the volume of T ( X ) .  The term u o  is the coverage of r ( X ) ,  and 
p(Y)luo of (11.60) gives the conditional density function of Y given T ( X ) .  
Expanding p ( Y )  around X by a Taylor series 
p ( Y )  E p ( X )  + ( Y - X ) T V p ( X ) .  
( 1 1.62) 
Substituting (1 1.61) and (1 1.62) into (1 1.60), 
or 
(1 1.63) 
(1 1.64) 
where the integration of (I I .63) is obtained from (B.6). Equation (1 1.64) indi- 
cates that, by measuring the local mean M ( X )  in T ( X ) ,  V p  ( X ) / p  ( X )  can be 
estimated. Particularly, the formula becomes simpler if the Euclidean metric 
A = I is used. 
The normalization of V p  ( X )  by p ( X )  has an advantage, particularly in 
clustering. In clustering, it is desirable that samples around the valley area 
have stronger signal as to which direction the gradients point. Since p ( X )  is 
low at the valley, V p  ( X )  is amplified by being divided by p ( X ) .  On the other 
hand, at the peak area, p ( X )  is high and V p ( X )  is depressed by being divided 
by P (XI. 
Figure 11-10 illustrates how the local mean is related to the gradient of a 
density function. In T ( X ) ,  we tend to have more samples from the higher den- 
sity side than from the lower density side. As a result, the sample mean of 
local samples generally points in the direction of the higher density side. 

536 
Introduction to Statistical Pattern Recognition 
X 
X 
X 
X 
x
x
 
x
x
 
x
x
 X 
X x x  x 
X 
x
x
 
Mo() 
Fig. 11-10 Local mean as the gradient estimate. 
The estimate of the density gradient can be applied to many pattern 
recognition problems besides clustering. They are briefly discussed as follows. 
Gradient of qi(X): The Bayes classifier is the hypersurface on which X 
satisfies q I ( X )  = q 2 ( X )  = 0.5 for two-class problems. The vector perpendicular 
to this hypersurface at X is the gradient of q I ( X ) ,  Vq I ( X ) ,  which indicates the 
local linear classifier, classifying local samples Y around X as 
local linear 
0 - 0 2  
nx) 
Fig. 11-11 The gradient of q I ( X ) .  
(1 1.65) 
Figure 11-1 1 shows an example. 
Note that V q 2 ( X )  = -Vq I ( X ) ,  since 
4 1 ( X )  + q2W) = 1. 

11 Clustering 
The gradient Vq I (X) can be estimated from the local means as 
r 
1 
537 
(1 1.66) 
where the Euclidean metric A = I is used. Since q I(X)yz(X) is a scalar, the 
direction of the vector Vq I (XI is determined by M I  (X)-M2(X), as shown in 
Fig. 11-1 I .  
Normality test [12]: When p(X) is normal with zero mean and covari- 
ance matrix I, Vp(X)/p(X) can be obtained by differentiating Inp(X) with 
respect to X [see (B.l l)], resulting in 
( I  1.67) 
Equation ( I  1.67) suggests that, by adding the estimate of Vp (X)/p (X) to X, the 
resulting vector should point toward the coordinate origin if X is normally dis- 
tributed. This property can be used to test the normality of a given set of sam- 
ples. The procedure is described as follows. 
(1) Whiten the samples. After the whitening process, the samples have 
zero mean and covariance matrix I. 
( 2 )  Estimate Vp (X)/p (X) by the local mean M (X) of (1 1.60), and add it 
to X. Use A =I. 
(3) The data passes the normality test by satisfying 
( 1 1.68) 
where r is a threshold. Various propcrtics of this tcst as wcll as thc sclcction 
procedures of related parameters, including t, can be found in [ 121. 
Data filter [Ill: A data filter eliminates noise from a given set of sam- 
ples and produces the skeleton hypersurface. The filter could be an effective 
tool for determining the intrinsic dimensionality of samples. Figure 1 1 - 12 

538 
Introduction to Statistical Pattern Recognition 
Fig. 11-12 Noisy data set. 
shows a distribution of samples which, judged subjectively, is intrinsically one- 
or two-dimensional. Let us assume that the distribution is one-dimensional, 
and that unwanted noise is responsible for the two-dimensional scatter. As dis- 
cussed in Chapter 6, the intrinsic dimensionality is determined by observing 
the local dimensionalities. Selecting a local region, as shown in Fig. 11-12, the 
dimensionality in the local region is two, because the two-dimensional scatter 
of noise has the same order of magnitude as the size of the local region. In 
order to eliminate the noise, we can measure the density gradient at each sam- 
ple point Xi, and move X ,  toward the direction of the gradient. The amount of 
the move could be controlled by another parameter, which is multiplied to the 
local mean vector M ( X j ) .  Repeating this process, samples are merged to a 
curve having little two-dimensional scatter. This curve is the skeleton of the 
distribution. After obtaining the skeleton, the local dimensionality is measured, 
which is one in the example of Fig. 11-12. 
Clustering Algorithms 
After estimating the gradient of a density function, we now need an algo- 
rithm to find clusters. As discussed in data filter, one way of finding clusters is 
to move samples toward the direction of the gradient by p M ( X )  where p is a 
control parameter. The procedure must be repeated until all samples in each 
cluster converge to one vector. This is conceptually simple, but computation- 
ally cumbersome. So, if possible, we would like to change only class 

11 Clustering 
539 
assignment without altering sample vectors. Also, it is preferable to avoid 
iterative operations. There are many ways to accomplish this. However, in 
this section we present only two: one is a non-iterative process, and the other is 
an iterative one. 
Graph theoretic approach [13]: One way to avoid an iterative opera- 
tion is to form trees as shown in Fig. 11-13. In this figure a node representing 
I 
Fig. 11-13 Graph theoretic clustering. 
X 
initiates a branch (or an arrow) pointing another node X , ,  , which is called 
the predecessor of X 
Then, X , ,  initiates another branch to point to X,,, and 
so on. Thus, each sample becomes an initial node and leads into a final node 
through a series of branches, each branch pointing from one node to its prede- 
cessor. A series of branches is called a directed path. We will implement an 
algorithm such that there is no directed path from a node to itself (i.e. no 
cycle). At the top of a tree, thefinal node (such as X 3 2 )  does not have a prede- 
cessor and is called the root of a tree. Note that each node except the final 
node has one and only one predecessor, but each could be the predecessor of a 
number of nodes, including zero. This type of tree is called a directed tree. 
Since the concept of this tree-formation comes from graph theory, we call this 
the graph theoretic approach. 
In order to form directed trees for the clustering problem, we need an 
algorithm to select the predecessor of each sample. If we could select, as the 
predecessor of X ,  a sample along the steepest ascent path starting from X ,  sam- 
ples will be divided by the valley of the density function, and a tree is formed 
for each cluster as shown in Fig. 11-13. The quality of the result depends 
wholly on the quality of the estimation technique for the density gradient, par- 
ticularly in the low density area of the valley. 

540 
Introduction to Statistical Pattern Recognition 
The density gradient at X can be estimated by the local mean M(X) as in 
(1 1.64). Asymptotically, the sample at the local mean can be the predecessor 
of X. However, in practice, with a finite number of samples, none of the exist- 
ing local samples in T ( X )  is located exactly at the local mean. Therefore, we 
need a procedure to pick up an existing local sample which is closest to the 
local mean. 
When two samples are located close together, the steepness of the slope 
from Xj to X,! can be measured by 
(1 1.69) 
Then, the predecessor x k  is the one which has the steepest slope from X j  
among samples in r(Xj), satisfying 
max szJ 
5kJ = x.Er(x,) 
(11.70) 
Equation (1 1.69) has another interpretation. Expanding p ( X i )  around X j  
by a Taylor series 
Thus 
(11.71) 
(11.72) 
where 0, is the angle between the two vectors, V p ( X , )  and (X,-Xj). Since 
llVp (Xj)ll is independent of e, (1 1.70) and (1 1.72) suggest that Xk is the sample 
which gives the smallest angle between Vp(Xj) and (Xk - Xi) among all local 
samples in T(Xj). That is, Xk is the closest sample to the steepest ascent line 
from Xi. Thus, the predecessor of X j  can be determined by measuring the 
angle between the local mean and ( X ,  - Xj). 
In addition, when nodes approach the root of the tree and skj of (1 1.70) 
becomes either zero or negative, we need rules to identify the root as follows. 
(1) skj e 0 : X j  is a root. 

11 Clustering 
54 1 
( 2 )  sk, = 0 : Consider the set n(Xl) = ( X ,  IX, E r(X,),sk, 
= O } .  Elim- 
inate from n(X,) any X k ,  from which there exists a directed path to X,. If the 
resulting n(X,) is empty, X, is a root. Otherwise, the predecessor of X ,  is X ,  
which satisfies 
Xjll . 
(11.73) 
The similar result may be obtained without computing local means. The 
density values of (1 1.69) can be estimated by using any nonparametric tech- 
nique such as the Parzen or kNN approach as discussed in Chapter 6. For 
example, if the Parzen approach with a uniform kernel function is used, 
p(X,) = z(X,)/Nv, where z(X,) is the number of samples in T(X,), N is the total 
number of samples, and v is the volume of T(X,). Since N and 1.3 are indepen- 
dent of j ,  we may ignore them and replace i ( X , )  by GCX,). For the kNN 
approach, p(X,) = (k-l)/Nv(X,), where k is a preset number and v(X,) must be 
measured. Since k and N are independent of j in this case, p(X,) is replaced by 
l/v(X,) in (1 1.69). Thus, using p ( . )  in the place of p (.) in ( 1  1.69), (1 1.70) 
determines the predecessor of each sample. 
A 
A 
.. 
A 
A 
The graph theoretic approach has a number of advantages. It is a non- 
iterative process, and does not require an initial class assignment. Also, the 
number of clusters needs not be preassigned. After the predecessor of each 
sample is found, a computer keeps track of the connections of samples to iden- 
tify the number of isolated trees automatically. 
In the graph theoretic approach, a crucial parameter is the size of the 
local region T(X). A density function is not a smooth function with a few 
peaks, but a noisy function with many local peaks and valleys. With a small 
T(X), the algorithm tends to pick up many clusters separated by the local val- 
leys due to noise. On the other hand, if T(X) is too large, all peaks and valleys 
are smoothed out and the algorithm produces only one cluster. In order to find 
a proper size for T ( X ) ,  it is suggested to run the algorithm for various sizes of 
T(X), and observe the resulting number of clusters. Normally, as the size of 
T(X) is changed from a small value to a large one, the number of clusters starts 
from a large value, drops down and stays at a certain level for a while, and 
then starts to drop again. The plateau at the middle is a reasonable and stable 
operating range, from which we can determine the size of T ( X )  and the number 
of clusters. 

542 
Introduction to Statistical Pattern Recognition 
Iterative valley-seeking: A nonparametric version of the nearest mean 
reclassification algorithm can be developed by defining a nonparametric 
within-class scatter matrix as 
L 
1 Ni 
$ n  = cp; - 
~(xy'-r/l.(xy'))(xv)-r/l.(xy)))~ 
J 
, 
(11.74) 
;=I N; j = l  
where ?Q(Xf") is the sample mean of kNN's to Xy) from w; as 
(11.75) 
We will call W,(Xy)) the focal mi-mean of Xy). This is the kNN version of the 
local mean. On the other hand, the local o,-mean for the Parzen approach is 
the sample mean of wi-samples in the local region r(Xj) with a fixed radius. 
Comparing (11.74) with (10.99) and (10.100), we note that the weighting 
coefficients of (10.99) and (10.100) are not included in (11.74). Since w t  
requires knowledge of the true class assignment of the samples, their use is 
deemed inappropriate for clustering. 
The criterion for class separability is set as J = tr(S;'$,,) 
just as 
J = tr(S;'S,.) 
is used for the parametric counterpart. When k approaches N i ,  
the local q-mean becomes the global o;-mean Mi, and consequently (11.74) 
becomes the parametric within-class scatter matrix S,,.. Thus, the nearest mean 
reclassification algorithm is a special case of the optimization of J = tr(S;'$,,,). 
On the other hand, when k << N ; ,  we can develop the nonparametric 
reclassification algorithm by repeating the derivation of (1 1.13) with %.(Xi) 
this time instead of Mi then, resulting in 
Ilxj - TMX~)II 
= m:n Ilxj - T ~ ( X ~ > I I  + xi E o, . 
( 1 1.76) 
Note that (1 1.76) is applied only after the data is whitened with respect to S,. 
This procedure may be called the nearest local-mean reclassification algo- 
rithm. In this algorithm, the local 0;-means must be updated each time the 
l k  
k,I 
TG(xy)) = - P N N  . 
class assignment is changed. 
Another possible definition of 
matrix is 
L 
1 
N, 
where XfdN is the kth NN of X y )  from 
the nonparametric within-class scatter 
- xyAN)(xj - XfdN)' , 
(1 1.77) 
0;. This time, we use the kth NN itself 

11 Clustering 
543 
instead of the sample mean of the kNN's. Then, by a derivation similar to 
before, XI'' is reclassified to o, 
by 
Ik, - xj,&II = min I I x ,  - xfAN II -+ 
xJ E o, 
(1 1.78) 
i 
after whitening the data with respect to S,. 
Under the current class assignment, the density function of a, at X, can 
be estimated by using the kNN approach as 
(1 1.79) 
where c is a constant relating the radius to the volume. Selecting the smallest 
llX,-XfhN 11 means selecting the largest P,p,(X,). Therefore, the reclassification 
algorithm of (1 1.78) suggests that we evaluate 
;,(XI) 
by (1 1.79) at each X,, 
and classify X, to the class which has the largest ? pp(X,). 
? i,(X,) is estimated by 
A , .  
When the Parzen approach with a uniform kernel function is used, 
(1 1.80) 
where 1' is a fixed volume of the local region around X,, and k,(X,) is the 
number of a?-samples in the local region. Then, (1 1.78) is converted to 
k,(X,) = max k,(X,) + X, E o, 
. 
(11.81) 
The formulas of (1 1.78) and (1 1.8 1 ) have a computational advantage 
over the formula of (1 1.76). When (1 I .76) is used, we need to recompute the 
local means at each iteration. This is not required for (1 1.78) and (1 1.8 1). 
When (1 1.8 1) is used, we set the local region around each sample with a 
fixed volume v, and tabulate samples in the local regions with their current 
class assignments, as shown in Fig. 11-14. Then, finding the class with the 
highest population, each sample is reclassified to that class. For example, in 
Fig. 1 1-14, X is reclassified to o2 
because the local region of X I contains one 
o,-sample and two *-samples. 
After all samples are reclassified, the class 
labels of samples in the table are revised, and the same operation is repeated. 
In this iteration, only class labels are processed and no mean vector computa- 
tion is involved. The same is true for (1 1.78). In the operation of (11.78), 

544 
Introduction to Statistical Pattern Recognition 
Fig. 11-14 Iterative class assignment. 
after tabulating neighbors of each sample, the sample is classified to o, when 
the kth NN from o, appears first in the sequence of lNN, 2NN, . . . . For 
example, in Fig. 11-14 with k = 2, X I  has a sequence of neighbors as 
w 2 , q , o l , .  . . , and the second NN from q appears first in the sequence. 
Accordingly, X ,  is reclassified to 02. 
Again, no mean computation is involved 
in each iteration. 
Because of the above computational advantage, let us use (1 1.8 1) as the 
updating scheme of class assignment, and study the properties of the valley- 
seeking algorithm. The algorithm can be stated as follows [14-151. 
(1) Whiten the data with respect to Sn7. 
(2) Assign the number of clusters, L. Choose an initial classification, 
WO). 
(3) Set a local spherical region around each sample, r(Xj), 
with a fixed 
radius, and list samples in T(Xj) with the current class assignment, as in Fig. 
(4) Reclassify Xi according to the majority of classes among all neigh- 
(5) If any change in class assignment occurs, revise the class labels of 
In order to understand how this procedure works, let us study the one- 
dimensional example of Fig. 11-15. Suppose that, at the tth iteration, samples 
are divided into 7 clusters as shown. A sample X I  is not reclassified from w2 
because all neighboring samples of X I  in T(X,) belong to w2 currently. On 
the other hand, X 2  on the boundary between o5 and 0 6  is most likely 
reclassified to 0 6 ,  because the number of neighbors from 0 6  tends to be larger 
than the number of 05-neighbors. This is due to the fact that the density func- 
tion on the 06-side is higher than the one on the 05-side. Reclassifying X 2  
11-14. 
boring samples in r(Xj). 
all neighbors in the table and go to Step (4). Otherwise stop. 

11 Clustering 
545 
V
2
V
A
Y
 
0 2  
w3 
0 4  
0 5  
Fig. 11-15 An example of the valley-seeking clustering. 
into 06 results in a shift of the boundary toward the 05-side, as the arrow indi- 
cates. This is equivalent to saying that the direction of the density gradient is 
estimated by using the numbers of samples on both sides of the boundary, and 
the boundary is moved toward the lower density side. Applying this process to 
all boundaries repeatedly, the leftmost boundary of Fig. 11-15 moves out to 
-m, leaving the 0, 
-cluster empty, the second and third leftmost boundaries 
merge to one, making the 03-cluster empty, and so on. As a result, at the end 
of iteration, only 02, 04, 
and 0 6  keep many samples and the others become 
empty. The procedure works in the same way even in a high-dimensional 
space. 
A number of comments can be made about this iterative valley-seeking 
procedure. The procedure is nonparametric, and divides samples according to 
the valley of a density function. The density gradient is estimated, but in a 
crude way. The number of clusters must be preassigned, but we can always 
assign a larger number of clusters than we actually expect to have. Many of 
initial clusters could become empty, and only true clusters separated by the 
valleys will keep samples. As far as computation time is concerned, it takes a 
lot of computer time to find neighbors for each sample and form the table of 
Fig. 11-14. However, this operation is common for all nonparametric cluster- 
ing procedures, including the graph theoretic approach. The iterative process 
of this algorithm revises only the class assignment according to the majority of 
classes in r(X). This operation does not take much computation time. 
The volume of T(X) affects the performance of this algorithm, just as it 
did in the graph theoretic approach. The optimum volume should be deter- 
mined experimentally, as was done for the graph theoretic approach. 

546 
Introduction to Statistical Pattern Recognition 
Experiment 4: 
to 
Seventy five samples per class were generated according 
(11.82) 
where nl and n2 are independent and normally distributed with zero mean and 
unit variance for both o1 and 02, 
[ml m21 is [O 01 for o1 and [0 -201 for 02, 
and 8 is normally distributed with E { 8 I o1 ] = n, E { 8 I o2 
} = 0, and 
Var( 8 I o1 } = Var{ 8 1 %  
} = (n/4)'. After the data was whitened with respect 
to the mixture covariance matrix, both graph theoretic and iterative valley- 
seeking algorithms were applied, resulting in the same clustering result, as 
shown in Fig. 11-16 [13-141. 
I 
I 
I 
I 
I 
I 
I 
I 
1 
I 
I 
1 
I 
I 
1 
I I 
I 
I 
I 
I 
1 
I 
I 
I 
I 
I 
1 
I 
I 
1 
I 
1 
I 
I 
I 
1 
h 
A
A
 
A A 
A A  L 
9 h.i 
, 
A 6 4  
d 4 
A 
A 
A A  
A h  
4 a A A  4 
A 
ei3 
O H  
R f l  
HBH 
I: 
r 
n u  
n 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
1 
I 
I 
I 
I 
I 
Table 11-3 shows the selected radius for T(X), and the number of iterations 
required to reach the final clusters in the iterative valley-seeking algorithm 
[141. 
Experiment 5: Fifty samples per class were generated from three 
classes. Two of them were the same ones as Experiment 4 except 

11 Clustering 
547 
TABLE 11-3 
PERFORMANCE OF THE VALLEY-SEEKING ALGORITHM 
~ 
Experiment 
Number 
Number 
Radius 
Number 
of samples 
of clusters 
of T(X) 
of iterations 
- 
4 
5 
~ 
150 
2 
I .o 
I50 
3 
0.75 
8 
10 
[rn I rn2] = [20 01 for o2 
instead of [0 -201. 
The third distribution was normal 
with the mean and covariance matrix 
(1 1.83) 
Again, after the data was whitened with respect to the mixture covari- 
ance matrix, both the graph theoretic and iterative valley-seeking algorithms 
produced the same clustering result, as shown in Fig. 11-17 [13-141. The 
... 
I 
I 
? A  \ 
I 
4 
A
P
 
B U  P 
H 
C L C  
c c  
cc 
ccccc c c  
L cccccccccc 
L 
c
c
 
n 
el3 
R O  
la 
P 
e. 
R I  
H 
e 8  
P 
e 
F 
p n  
1' li 
e 
e r  
H 
R 
I 
I 
I 
1 
I I 
I 
I 
L 
I 
I 
I 
1 
I 
I 
I 
1 
I 
1 
I I 
1 
I 
1 
I 
o 
en 
I 
1 
I 
I 
I 
I 
Fig. 11-17 Classification of a three-class example. 

548 
Introduction to Statistical Pattern Recognition 
radius of T(X) and the number of iterations in the latter algorithm are listed in 
Table 11-3 [ 141. 
General comments: We have discussed both parametric and non- 
parametric clustering techniques. The question here is which technique is 
better under what conditions. Generally speaking, nonparametric techniques do 
not require the knowledge of the number of clusters beforehand. This is cer- 
tainly true for the graph theoretic approach. Even for the iterative valley- 
seeking procedure, we can preassign a larger number of clusters than what is 
actually needed, and let the procedure select automatically the necessary 
number of clusters. Therefore, if we do not have any a priori knowledge about 
the data structure, it is most natural to adopt a nonparametric clustering tech- 
nique and find out how the data is naturally divided into clusters. 
However, nonparametric procedures are in general very sensitive to the 
control parameters, especially the size of the local region. Therefore, it is 
necessary to run experiments for a wide range of sizes, and the results must be 
carefully examined. Also, nonparametric procedures are normally very compu- 
tationally intensive. 
Furthermore, nonparametric clustering techniques have two fundamental 
flaws described as follows. 
(1) We cannot divide a distribution into a number of clusters unless the 
valley actually exists. For example, the a1 
-distribution of Fig. 11-9 could be 
obtained as the result of the valley-seeking procedure, but the distribution can- 
not be divided into 2 or 3 clusters by any nonparametric method even if it is 
desirable to do so. When a distribution is wrapped as the al-distribution of 
Fig. 11-9, it is sometimes prefered to decompose the distribution into several 
normal distributions for further analysis of data structure or designing a 
classifier. 
On the other hand, the parametric procedures do not depend on the 
natural boundary of clusters, but depend only on the criterion. With a preas- 
signed number of clusters, the procedures seek the boundary to optimize the 
criterion value. Therefore, we can divide the oI 
-distribution of Fig. 11-9 into 
2, 3, or 4 clusters as we like. After examining the clustering results for various 
numbers of clusters, we may decide which number of clusters is most appropri- 
ate for a particular application. Previously, we stated that it is a disadvantage 

11 Clustering 
549 
of parametric clustering techniques to have to know the number of clusters 
beforehand. But, that property may work sometimes as an advantage as dis- 
cussed above. 
(2) As seen in Fig. 11-7(b), the resulting clusters of nonparametric clus- 
tering procedures contain the tails of other distributions and do not contain 
their own tails. Thus, the mean and covariance matrix of each cluster do not 
represent the true ones of the underlying distribution. In order to obtain proper 
parameters of the underlying distributions, we need to impose a parametric 
structure such as the summation of normal distributions. 
11.3 Selection of Representatives 
In the previous sections, we have discussed algorithms to find clusters 
from a distribution. A similar but slightly different problem is to reduce the 
number of samples while maintaining the structure of the distribution. This 
may be viewed as selecting a small number of representatives from a distribu- 
tion. Besides clustering, the reduction of sample size has a number of applica- 
tions. A typical example is to divide a set of available samples into design and 
test sets. In this case, we must assure that design and test distributions are 
similar. 
Nonparametric Data Reduction 
Generally, it is not appropriate to assume a mathematical structure when 
the sample size is reduced. Therefore, a nonparametric procedure must be 
adopted to guide through the operation. 
Data reduction algorithm [16-171: Let us study the Parzen approach to 
the problem of sample size reduction. Given N samples drawn from p ( X ) ,  we 
wish to select Q samples (Q < N) so that the Parzen density estimates for the N 
sample set and the Q sample set are close. 
Let i N ( X )  be the density estimate based on N samples. Then 
( 1  1.84) 
where IC(.) is the kernel function. 
Similarly, when Q representatives, 
Y , ,  . . . ,Yo, are selected, the density function is estimated by 

550 
Introduction to Statistical Pattern Recognition 
(1 1.85) 
n 
.+ 
In order to measure the similarity between pN(X) and pQ(X), the entropy cri- 
terion of Chapter 9 is used. Replacing pl(X) 
and p 2 ( X )  of (9.53) by &(X) 
and pN(X) respectively, we obtain the entropy inequality as 
n 
n 
n 
n 
where the equality holds only for ~ Q ( X )  
= p~(x). Thus, the best pa(X) may be 
found by minimizing the lefthand side of (11.86). The criterion may be 
simplified by approximating the expectation with respect to pN(X) in (1 1.86) 
by the sample mean over the existing samples X 
n 
. . . ,XN, 
as 
In order to find the best Q representatives from the existing samples 
X I ,  
. . . ,XN, we would like to minimize J over all possible Q element subsets 
N 
of the original N element set. Unfortunately, an exhaustive search of all ( ) 
Q 
subsets is not computationally feasible. Instead, we will settle for the 
minimum J for subset's formed by replacing one element of the representative 
set by the best candidate not yet selected. 
An iterative procedure is as follows: 
(1) Select an initial assignment of Q samples from the N sample data 
set. Call the Q sample set STORE and the remaining N -Q samples TEST. 
if the sample is transferred to STORE. 
(2) For each element, X,, in TEST, compute the change in J that results 
- In 
(1 1.88) 

11 Clustering 
55 1 
(3) Pick the element, X , ,  corresponding to the smallest A J I  (and call it 
X;). 
(4) For each element, X,, in STORE, compute the change in J that 
results if the sample is transferred to TEST. 
r 
(5) Find the element, X,, corresponding to the smallest AJ2 (and call it 
x: 1. 
(6) 
The 
change 
of 
J 
due 
to 
these 
two 
operations 
is 
AJ = A J , ( X j ' )  + AJ,(XZ). 
In order to minimize J ,  we would like to have 
AJ < 0. If X i  exists to satisfy AJ < 0, transfer X :  to TEST, transfer X; to 
STORE, and go to Step (2). 
A J I  (and call it XT). 
(7) Otherwise, find the element, X , ,  corresponding to the next smallest 
(8) If X r  exists, go to Step (4). 
(9) Otherwise, stop. 
Generally, this kind of iterative process produces a result which depends 
on the initial selection of Q representatives in STORE. However, Steps (7) and 
(8) allow us to search more possible combinations of X ,  and X ,  and thus insure 
that the final representative set is less dependent on the initial assignment. 
The kNN approach also can be applied in a similar way as the Parzen 
approach. The kNN density estimate is 
(1 1.90) 
where dN(X) is the distance from X to its kth NN among N samples. The same 

552 
Introduction to Statistical Pattern Recognition 
formula is used when Q samples are used to estimate the density function. 
Thus, the entropy criterion of (1 1.87) becomes 
J = -E[- 
l N  InpQ(Xi)] 
,. 
= - z [ n  
1 ”  In d,(X;) +In -1 CQ 
. 
N ;=I 
N ;=, 
k-1 
(11.91) 
The same iterative procedure can be used to find the best Q samples by minim- 
izing J of (1 1.9 1). 
As in most iterative techniques, a good initial guess usually is of consid- 
erable importance. The following is a method to perform the initial assignment 
when Q = N l 2 .  
An initial assignment procedure: We discuss a procedure to generate 
an initial assignment for the case with Q = N l 2 .  The basis of the procedure 
lies in an intuitive observation. If a subset with size N l 2 ,  called STORE, is 
optimally chosen, then we suspect that, on average, the NN of a sample, X ,  in 
STORE might be the second NN of X using all N samples. 
The procedure is best explained with the aid of Fig. 11-18. In Fig. 11- 
18(a) the NN of X I  is X 2 3 ,  the NN of X 2 3  is X I 7 ,  and X 17 and X ,  are mutual 
NN. Figures ll-l8(b) and (c) represent two other possible NN progressions. 
(a) 
(b) 
(C) 
Fig. 11-18 Three examples of NN progressions. 
We may form the initial assignment by simply assigning every other sample in 
a NN progression to the initial assignment of STORE. Thus for Fig. 11-18(a) 
the initial assignment for STORE would consist of either X I  and XI,, or X , ,  
and X40. 
We now state the procedure. 
(1) Get the first sample, and follow the complete progression of its 

1 1  Clustering 
553 
NN’s, assigning alternate samples to STORE. Flag every sample in the pro- 
gression. 
(2) Find the next unflagged sample. Follow its N N  progression, assign- 
ing alternate samples to STORE. This is done until the progression ends or 
leads to a previously flagged sample. Flag every sample in the progression. 
(3) Repeat Step (2) until all samples are flagged. 
If sufficient storage is available to store a table of kNN’s for each sample 
(k = 4 or 5), the entire process of initial assignment and criterion minimization 
can be done very efficiently. 
Reduced Parzen classifier [ 171: In pattern recognition, the quadratic 
classifier is very popular. However, in practice with non-normal distributions, 
it is frequently observed that the error of a quadratic classifier is much larger 
than the Bayes error estimated by a nonparametric technique. On the other 
hand, nonparametric classifiers are too complex and time-consuming for on- 
line operation. Thus, there is a need to fill the gap between these two kinds of 
classifiers. 
One possible solution is to divide each class into several clusters, and 
design a piecewise quadratic classifier as in (4.149). This approach contains 
both quadratic and nonparametric classifiers as special cases. If each class has 
only one cluster, it becomes a quadratic classifier, while, if each sample is 
viewed as a cluster, it becomes a Parzen classifier. 
The most elaborate procedure to design a piecewise quadratic classifier 
would be to decompose the distribution of each class into several normal distri- 
butions as shown in (1 1.45). Note that only this method gives reasonable esti- 
mates of a priori probabilities, means, and covariance matrices. The other clus- 
tering procedures fail to do so, because they divide samples as in Fig. 11-7(b) 
instead of Fig. 11-7(a). The entire clustering operation must be repeated by 
preassigning various numbers of clusters. The resulting classification error for 
each preassigned number of clusters is estimated and compared with the Bayes 
error estimated by a nonparametric technique. The final number of clusters 
must be as small as possible while maintaining an error close to the Bayes 
error. 
A somewhat simpler approach is to select Q representatives from each 
class as discussed in this section, and to set a kernel function at each represen- 
tative to form the Parzen density estimate as in ( 1  1.85). The classification can 

554 
Introduction to Statistical Pattern Recognition 
be carried out by comparing the density estimates for o1 and o2 
as 
(11.92) 
This classifier is called the reduced Par-zen classifier. It is generally too com- 
plex to select a different kernel function for each representative. So, we may 
use a normal kernel function with the covariance matrix r2&, where C j  is the 
global covariance matrix of oi and r is a parameter controlling the size of the 
kernel. This approach is simpler because a common kernel function is used 
within the same class and is prespecified, instead of being estimated and vary- 
ing locally, as in the approach of (1 1.45). As discussed in Chapter 7, the sensi- 
tive parameters are the size of the kernel, r, the decision threshold, t of (1 1.92), 
and the estimate of C;. 
In order to verify the above argument, two experimental results are 
presented. 
Experiment 6: The reduced Parzen classifier for Data I-A was designed 
and tested as follows: 
(1) One hundred samples per class were generated from Data I-A, and 
Experiment 7-6 was repeated. From Fig. 7-9, I' = 2 was chosen as the kernel 
size. 
(2) The sample reduction algorithm in this section was applied to select 
Q representatives from 100. 
(3) Using the Parzen density estimates of (1 1.85) for o1 
and o2 with 
r = 2, the original 100 samples per class were classified as in (1 1.92). The 
threshold t was selected so as to minimize the classification error. The 
optimum t is different for each different value of Q. 
(4) Independently, 100 samples per class were generated and tested by 
the classifier designed above. 
Figure 11-19 shows the plot of the error vs. Q [17]. The error curve is 
the average of 10 trials, and the standard deviations are shown by vertical bars. 
Note that the above error estimation is based on the holdout method, in which 
design and test samples are independent so that the error becomes larger than 
the Bayes error (1.9%). The error curve is almost flat up to one representative. 
For normal distributions, selecting the expected vector as the one representative 
from each class and the covariance matrix as the kernel covariance, the reduced 

11 Clustering 
555 
Parzen classifier becomes quadratic, which is the Bayes classifier. So, the error 
curve of Fig. 11-19 should be flat down to Q = 1. However, since we select 
representatives from the existing design samples, they may or may not be close 
to the expected vector. If not, we see that the error curve is flat up to Q = 2 or 
3 and starts to increase for smaller Q’s. 
6 
2 1
1
Q 
0
l
 
I 
I 
I 
I1 
I 
I 
I 
I 
I 
9 10’ 20 
40 
60 
80 
100 
m 
1
3
 
6 
# of representatives 
Fig. 11-19 The error of the reduced Parzen classifier for a normal case. 
Experiment 7: In order to test a non-normal case, the data of Experi- 
ment 7-7 was studied. The data is 8-dimensional, and each class consists of 
two normal distributions. From Fig. 7-10, I’ = 1.5 was chosen. The procedure 
of Experiment 6 was repeated for this data, and the result is shown in Fig. 
11-20 [17]. Figure 11-20 shows that the error curve is flat for Q 26. We 
found that, when Q = 6, three representatives are selected from each cluster. 
These experiments suggest an interesting fact. It has been believed that a 
nonparametric procedure needs a large number of samples for high-dimensional 
data, in order to reliably estimate the Bayes error. Any nonparametric opera- 
tion with a large number of samples requires a large amount of computer time. 
The results of the experiments in this section contradict these common beliefs, 
and suggest that we may need only a relatively small number of samples (or 
representatives) after all. However, as Experiment 7-10 and Table 7-3(b) sug- 
gest, we may still need a large number of samples to estimate the covariance 
matrices, which are used to form the kernel functions. 

Introduction to Statistical Pattern Recognition 
30 
25 
20 
15 
10 
5 -  
- 
- 
- 
- 
- 
Q 
Parametric Data Reduction 
In parametric approaches, our main concern is to maintain the expected 
vector and autocorrelation (or covariance) matrix while reducing the number of 
samples. 
Representation of a sample matrix: Let us express N samples in a 
matrix form as 
u = [ X , .  . .X,] 
(1 1.93) 
which is an n x N  rectangular matrix, called the sample matrix. Then, the sam- 
ple autocorrelation matrix can be expressed by 
(1 1.94) 
A 
A 
Since S is an nxn matrix; S has the nxn eigenvalue and eigenvector matrices, A 
and CD, such that 
i@=CDA. 
(1 1.95) 

11 Clustering 
557 
Or 
(UUT)@ = @ ( N h )  = @p, 
(1 1.96) 
where I-1 is the eigenvalue matrix of UUT and p = NA. Multiplying U T  from 
the left side, we can convert (1 1.96) to 
W T ~ ) N d W T @ ) N x n  = (uT@)Nx,,p,,xn 
9 
(11.97) 
where UTU is an NxN matrix, UT@ consists of n eigenvectors of UTU, and the 
components of P are the n eigenvalues of UTU. Since (UT@)T(UTcD) 
= 
QTUUT@ = p #I, we change the scales of the eigenvectors by multiplying 
I-1 
-1/2 
from the right side so that (1 1.97) can be rewritten as 
(UTU)Y = Yp , 
( 1 1.98) 
where 
(1 1.99) 
That is, 
condition as 
consists of the n eigenvectors of UTU, and satisfies the orthonormal 
-112 
-112 
Y T Y = P  @TuuT@p = I .  
( 1 1.100) 
From (1 1.99), U can be expressed in terms of @, Y ,  and A as 
112 
U = @k Y T  = 6 @ A 1 / 2 Y T .  
(1 1.101) 
This expression of a sample matrix is called singular- value decomposition [ 181. 
Singular value decomposition is an effective technique to represent a rectangu- 
lar (non-square) matrix by eigenvalues and eigenvectors. 
Data reduction: Now our problem is to reduce the number of samples 
while maintaining the expected vector and autocorrelation matrix. Let us intro- 
duce another sample matrix V which consists of Q sample vectors. In order 
that both U and V share the same autocorrelation matrix 
(1 1.102) 
Using ( 1  1.101), both U and V can be expressed by 

558 
Introduction to Statistical Pattern Recognition 
U = 'k@A1/*Y7- , 
(1 1.103) 
V = G@Ali2ZT , 
(11.104) 
where Y (Nxn) and Z (Qxn) are the eigenvector matrices of U'U and V'V 
respectively, and @ and A are the eigenvector and eigenvalue matrices of 5. 
Note that, although (UUT),xn and (WT),lx,, 
share the same eigenvector matrix, 
@, (U7-U)Nd and (VTV)QxP have different eigenvector matrices, Y and E. 
Furthermore, since 6 Y r  = J~-~'*@'U from (11.103), 6 Y T  is the sample 
matrix in the whitened space, where the sample autocorrelation matrix becomes 
I. Similarly, %e7- is the reduced sample matrix in that space. Then, (1 1.104) 
shows the transformation which converts GxT 
back to the original space. 
As in (1 1.100), the n column vectors of E are orthonormal to satisfy 
Y 
=T- = - I .  
- 
A 
(1 1.105) 
The second condition is that U and V must have the same mean vector. 
Without losing generality, let us assume that the mean is zero. Then, 
(1 1.106) 
where lk is a vector with all k components being equal to 1 as lk = [l . . . I]'. 
Or, substituting (1 1.103) and (1 1.104) into (1 1.106), 
(1 1.107) 
When the sample matrix U is given, we can compute @, Y, and A. 
Therefore, our problem is to find Z by solving (1 1.105) and ( 1  1.107). Then, V 
can be computed by (1 1.104). Now E consists of nxQ elements. To determine 
those nxQ unknowns we have nxn equations from (11.105) and n equations 
from (1 1.107). Therefore, in general when Q is smaller than n+l, there are 
many possible solutions. But when Q = n + l ,  we have a unique solution as 
follows: 
( 1  1.108) 
-7- 
= r l x ( r t + I )  = [FflxrlGrlxl 1 1 
F = l - a l f , l ; ,  
(11.109) 
G = - e f l l l l ,  
( 1 1.1 10) 

11 Clustering 
where 
559 
dn+l - I 
a =  
n
f
i
 ' 
1 
@ = -  GT 
( 1  1.1 11) 
(1 1 .I 12) 
The derivation of (1 1.108) through (1 1.1 12) has been omitted for brevity, but 
the reader may easily verify that 5 of (1 1.108) satisfies both (1 1.105) and 
( I 1.107). 
Substituting (11.109) and (11.110) into (11.108), 
r-* - a  
. . .  - a  -01 
- a  1-a 
- 8  
Note that ET of (1 1.1 13) is data independent, because =ST 
is the reduced 
sample matrix of (n+l) samples for the mean 0 and the covariance matrix I. 
This sample matrix is transformed by (1 1.104) into the space where the origi- 
nal data and its sample covariance matrix are given. 
Example 2: For n = 2, dn+l ZT becomes 
1 
1.366 
-0.366 
-1 
1.366 
-1 
' 
$ E T  
= 
Computer Projects 
1. 
Repeat Experiment 1. 
2. 
3. 
4. 
5. 
Repeat Experiments 2 and 3. 
Apply the graph theoretic valley-seeking technique to Data I-A. 
Apply the iterative valley-seeking technique to Data I-A. 
Repeat Experiments 6 and 7. 
(11.114) 

560 
Problems 
Introduction to Statistical Pattern Recognition 
1. 
2. 
3. 
4. 
5. 
A random variable x is uniformly distributed in [a, b]. From N samples 
x,, . . . ,xN drawn from the distribution, find the maximum likelihood 
estimates of a and b. 
Assume that a distribution can be decomposed to L normal clusters and 
all clusters share the same covariance matrix. Find the recursive equa- 
tions to compute the a priori probabilities, the mean vectors, and the 
common covariance matrix of L clusters by using the maximum likeli- 
hood estimation technique. Point out the difference between this pro- 
cedure and the nearest mean reclassification algorithm. 
Assume that a distribution consists of L normal clusters and 
P = . . . = PL. Find the recursive equations to compute the means and 
covariance matrices of L clusters by using the maximum likelihood esti- 
mation technique. 
A kurtosis matrix is defined by 
K = E { (XTX)XXT) - (n +2)1 . 
When a density function consists of L normal clusters with the mean 
vectors Mi and the common covariance matrix 2, K can be rewritten as 
L 
K = ~PiMjMT[tr(X+MiM[) - (n+2)] . 
i=l 
Since the rank of the above equation is (L-1), only (L-1) eigenvalues of 
K are non-zero. Using this property, we can estimate the number of 
modes when clusters are normal and share the same covariance matrix. 
Prove that the second equation is equal to the first one for Z = I. 
A density function of a random variable x can be estimated by the Par- 
Zen method with the kernel function 
A 
The derivative of the Parzen density estimate, dp(x)/dx, can be used as 
an estimate of the density gradient. Confirm that this method of gradient 
estimation coincides with (1 1.64). 

11 Clustering 
56 1 
6. 
7. 
8. 
9. 
The product of two normal distributions, N ( M , , X 1 )  and N(M,,C,), 
gives another normal distribution, N (Mo,&,), multiplied by a constant, c. 
(a) 
Derive the relations between the three means and covariance 
matrices. 
Samples are drawn from N(Ml,Xl), and a normal window func- 
tion N ( X , C * )  is set at X .  Using these samples and the given win- 
dow function, estimate the mean and covariance matrix of 
Using Go and C, and the given window function, estimate M 
and 
E,. This procedure could be used to estimate a local covariance 
matrix even when the samples are drawn from a non-normal distri- 
bution. 
(b) 
N (Mo, ZO). 
(c) 
Two density functions are estimated by the Parzen method. Find the 
expression for the class separability criterion j p  I ( X ) p  2 ( X ) d X .  
(a) 
(b) 
Use a normal kernel function with kernel covariance r2X, for w,. 
Use a hyperspherical uniform kernel with radius r for both ol 
and 
0 2 .  
Let class separability be measured by 
where 
1 when X I  and X ,  belong to different classes, 
0 when X I  and X ,  belong to the same class. 
‘?(CI,C,) = 
That is, J counts the number of pairs whose members are close and yet 
come from different classes. Prove that the minimization of J by an 
iterative process leads to the iterative valley-seeking procedure of 
(11.81). 
Formulate the selection of representatives by a branch and bound pro- 
cedure. 

562 
Introduction to Statistical Pattern Recognition 
10. 
By using the singular value decomposition technique, show how to 
decompose an nxm (m<n) two-dimensional image to m basis images, 
and how to evaluate the effectiveness of these basis images. 
References 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
S. Watanabe, “Knowing and Guessing,” Wiley, New York, 1969. 
D. J. Hall and G. B. Ball, ISODATA: A novel method of data analysis 
and pattern classification, Technical report, Stanford Research Institute, 
Menlo Park, CA, 1965. 
G. B. Ball, Data analysis in the social sciences: What about the details?, 
Proc. Fall Joint Computer Conference, pp. 533-559, Washington, DC, 
1965. 
J. MacQueen, Some methods for classification and analysis of multivari- 
ate observations, Proc. Fifth Berkeley Symposium on Math. Statist. and 
Prob., pp. 281-297, Berkeley, CA, 1967. 
K. Fukunaga and W. L. G. Koontz, A criterion and an algorithm for 
grouping data, Trans. IEEE Computers, C-19, pp. 917-923, 1970. 
W. L. G. Koontz, P. M. Narendra, and K. Fukunaga, A branch and 
bound clustering algorithm, Trans. IEEE Computers, C-24, pp. 908-9 15, 
1975. 
D. M. Titterington, A. F. M. Smith, and U. E. Makov, “Statistical 
Analysis of Finite Mixture Distributions,” Wiley, New York, 1985. 
K. Fukunaga and T. E. Flick, Estimation of the parameters of a Gaussian 
mixture using the method of moments, Trans. 1EEE.Pattel-n Anal. and 
Machine Intell., PAMI-5, pp. 410-416, 1983. 
N. E. Day, Estimating the components of a mixture of normal distribu- 
tions, Biometrika, 56, pp. 463-474, 1969. 
J. H. Wolfe, Pattern clustering by multivariate mixture analysis, Mul- 
tivar. Behav. Res., 5 ,  pp. 329-350, 1970. 
K. Fukunaga and L. D. Hostetler, The estimation of the gradient of a 
density function, with applications in pattern recognition, Trans. IEEE 
Inform. Theory, IT-21, pp. 32-40, 1975. 

11 Clustering 
563 
12. 
K. Fukunaga and T. E. Flick, A test of Gaussian-ness of a data set using 
clustering, IEEE Trans. Pattern Anal. and Machine Intell., PAMI-8, pp. 
W. L. G. Koontz, P. M. Narendra, and K. Fukunaga, A graph-theoretic 
approach to nonparametric cluster analysis, Trans. IEEE Computers, C- 
240-247, 1986. 
13. 
25, pp. 936-944, 1975. 
14. 
W. L. G. Koontz and K. Fukunaga, A nonparametric valley-seeking tech- 
nique for cluster analysis, Trans. IEEE Computers, C-21, pp. 17 1 - 178, 
1972. 
W. L. G. Koontz and K. Fukunaga, Asymptotic analysis of a non- 
parametric clustering technique, TrunJ. IEEE  computer^, C-2 1, pp. 967- 
974, 1972. 
K. Fukunaga and J. M. Mantock, Nonparametric data reduction, Trans. 
IEEE Pattern Anal. and Machine Intell., PAMI-6, pp. 115-1 18, 1984. 
K. Fukunaga and R. R. Hayes, The reduced Parzen classifier, Trans. 
IEEE Pattern Anal. and Machine Intell , PAMI-1 1, pp. 423-425, 1989. 
P. A. Wintz, Transform picture coding, Proc. IEEE, pp. 58-68, 1969. 
15. 
16. 
17. 
18. 

Appendix A 
DERIVATIVES OF MATRICES 
In this appendix, the derivatives of matrices will be discussed for various 
cases. The following notations are used throughout this appendix. 
R = [rjj] : nonsymmetric matrix of nxn 
S = [sjj] : symmetric matrix of nxn 
A = [ajj] : rectangular matrix of nxm 
B = [bji] : specified differently for each formula. 
Matrix Inversion 
When rij is a function of a scalar x, the derivative of R with respect to x 
is defined as the matrix of each element rij differentiated with respect to x. 
(A. 1 ) 
Applying this to the product RR-’ = I for a nonsingular R gives 
564 

Appendices 
565 
For x = rii, 
where ljj is a matrix with an i,j component of 1 and all other components 
equal to 0. 
For a symmetric matrix S, sjj = si;. Therefore, 
where 
the others. When i=j, I: = 1". Thus, from (A.2) and (A.4), 
for i#j is a matrix with 1's as the i,j and j,i components and 0's as 
(A.5) 
-- 
Furthermore, 
Trace 
('4.8) 
where Ojj is the i, j component of 0 = S-'BS-l. Therefore, 
3.f as - 
= -[O + 0' - diag[O]] 
= -[S-'(B + BT)S-' - diag[S-'BS-']] , 
('4.9) 
where diag[R] is a diagonal matrix, keeping only the diagonal terms of R .  
instead of 1; in (A.8), 
Similar formula can be obtained for a nonsymmetric R, by using I,, 

566 
Introduction to Statistical Pattern Recognition 
d 
(2) 
-tr(A'B 
dA 
d 
(3) 
-tr(A'R 
dA 
(A.lO) 
a 
--tr(R-IB) 
= -R-WR-I , 
aR 
where tr(ljj@) = Bjj is used. 
The derivatives of other trace functions are listed as follows: 
d 
d 
dA 
dA 
(1) 
-@(AB) 
= -tr(BA) 
= B T  
( B :  mxn) , 
d 
dA 
= -tr(BA 
T ,  = B 
(B : nxm) , 
) = (R + RT)A , 
d 
y t r ( A T R A )  =AT@ + RT) , 
dA 
Xtr{(ATSA)-'R) = -tr{R 
(A TSA)-') 
(4) 
d 
d 
dA 
(5) 
= -SA (A TSA )-' (R -t R T)(A TSA )-I , 
(A. 15) 
d 
d 
-tr{(ATS2A)-I(ATS 1A)I = xtr{(ATS IA)(ATS2A)-1) 
dA 
(6) 
d 
= -ttr{(A:S2A 
l)-l(A'SIA)) I A , = A  
dA I 
+ -tri(A 
'S2A )-I 
d 
,A 2)) 
1 A?=A 
dA 2 
= -2s 2A (A TS2A)-1 (A ' S  I A )(A ' S  2A)-' 
+ 2SIA(ATS2A)-' . 
When (A. 16) is set to be equal to zero, 
S ; ' S I A  =A(A7S2A)-'(ATSIA). 
(A.11) 
(A.12) 
(A.13) 
(A. 14) 
(A.16) 
(A.17) 

Appendices 
567 
Determinant 
The derivative of I R I with respect to r,/ can be obtained as 
-- - IR;; I , 
a l R l  
arIJ 
where I R,, I is the cofactor of r,,. Therefore, 
-- - (adj R f  = IR I R-" , 
a l R  I 
dR 
(A.18) 
(A.19) 
where (adj R )  is the adjoint matrix of R, and R-' = (adj R ) l  IR I .  Furthermore, 
(A.20) 
When a matrix, S, is symmetric, the above equations are modified to 
(A.21) 
-- 
ais I - IS,, I + IS,, I - F,, IS,, I = (2 - F,/) IS,, I , 
as,, 
-- 
a's I - IS I [2S-' - diag[S-']] , 
as 
Furthermore, using (AS), 
or 
(A.22) 
(A.23) 
(A.24) 
(A.25) 
where yij is the i,j component of S-' . 
The derivatives of other determinant functions are listed as follows: 
(A.26) 
d 
(1) 
x I A T S A  I = 21A7SA ISA(ATSA)-' , 

568 
Introduction to Statistical Pattern Recognition 
(A.27) 
d 
dA 
(2) -1n 
I A ~ S A  
I = ~ S A ( A ~ S A ) - '  
, 
d 
-{ln 
lATSIA I - In IATS2A I )  
dA 
(3) 
= 2{S IA (ATSIA)-' - S2A (ATS2A)-I) . 
When (A.28) is set to be equal to zero, (A.17) is satisfied. 
(A.28) 
Special Cases 
Let us consider a special case in which S = A  (a diagonal matrix with 
components hi), and 
sjj = cjj. 
The derivatives of three functions, 
f = tr(A-'MM'), f 2  = tr(A-'MXT), and f 3  = In IC I, with respect to M and cij 
are of particular interest, where both M and X are column vectors. These 
derivatives are listed here for the reader's convenience, because they are fre- 
quently used in Chapter 5. 
The first function to be considered is a trace function of (A.7) with a 
symmetric B or a squared distance as 
f l  = w(A-%M') = M%-'M 
. 
(A.29) 
The derivatives off are computed as follows: 
(A.30) 
mimj 
from (A.8) , 
af 1 
acij 
hihi 
- 
= -(2 - 6j,)[A-'MMTA-l]j, = -(2 - 6;j)- 
(A.31) 
(A.32) 
where mi is the ith component of M ,  and [KIij is the i,j component of K. 
The second function is a trace function of (A.7) with a nonsymmetric B 
as 

Appendices 
569 
f z  = tr(A-'MX') = XTA-'M = MTA-'X . 
The derivatives off are 
(mlxJ + mJxl - 61Jmlxl) from (A.8) , 
1 
- 
3f2 
a2f 2 
ac; 
- 
- -- 
k, 
' I  ' J  
- 
= 2MT[A-'I~JA-11T,A-']X 
2 
m l l l  
mJxj 
m1 4 
- 
- - 
j- 
+ - 
-6,,] 
from (A.6) 
' I h ]  
' I  
' J  
' I  
The third function is a determinant function as 
f 3  =InIAI . 
The derivatives of f 3  are 
- 
from (A.23) , 
acIJ 
'! 
from (A.25) . 
-- 
J2f 
3 - -(2 - 61J)- 
ac; 
hi', 
(A.33) 
(A.34) 
(A.35) 
(A.36) 
(A.37) 
(A.38) 
Special Derivatives 
Let f be a function of a mean vector M and covariance matrix C. When 
their estimates fi and X are used, f (k,Q 
can be expanded around f ( M ,  Z) by 
a Taylor series as 
a f T  
= f ( M , Z )  + -(M - M) + tr 
aM 
where 

570 
Introduction to Statistical Pattern Recognition 
-- 
af* - 
ax 
1 
2 
- _  
- 
a f  + diag [” 
ax 
ax 
. 
(A.40) 
That is, because of the symmetry of C, aj%E is not the same as df /ax whose 
i, j component is af lacij. 
Let us examine df*laC for two different types of f a s  
f l  = tr(PMMT) = M ~ Z - I M  
, 
f 2 = l n l Z l  . 
Then, from (A.9) and (A.23) 
(A.41) 
(A.42) 
(A.43) 
af I - 
az = -[2Z-1MMTC-1 - diag[Z-’MMTZ-’]] , 
Substituting (A.43) and (A.44) into (A.40), 
(A.45) 
(A.46) 
Comparing (A.45) and (A.46) with (A.lO) and (A.20), we can conclude that, in 
both cases, axlac is obtained by differentiating f ,  with respect to Z as though 
1 is a nonsymmetric matrix. 

Appendices 
57 1 
Reference 
S. R. Searle, “Matrix Algebra Useful for Statistics,” John Wiley, New York, 
1982. 

Appendix B 
MATHEMATICAL FORMULAS 
For quick reference, a list of formulas frequently used in this book is 
presented here as follows. 
Volume and Surface Area 
v = co IA I '"r" : volume with radius r 
L (X) 
= {Y : d(Y,X) I r] 
(B.3) 
d2(Y,X) = (Y-X)'A-'(Y-X) 
(B.4) 
vo = co I A I 112(i-G)" 
: the volume of a region L ( X )  which satisfies 
(B.5) 
(B.6) 
1 
V O  
1 
(x,(Y -X)(Y -X)T--dY 
= r 2A 
S = con IA I "*rn-' : surface area with radius r 
(B.7) 
572 

Appendices 
573 
So = co IA I 112rn-I : the area of a surface S ( X )  which satisfies 
(B.8) 
Properties Related to Normal Distributions 
V p ( X )  = - 
ap (x) - 
- -p ( X ) P  (X -M) 
(B.ll) 
ax 
V * P ( X )  = ~ a*p(x) = p (X)[z-l(X-M)(X-M)TC-' - c-I] 
(B.12) 
ax2 
Properties Related to Gamma and Beta Densities 
Gamma density: 
P+ 1 
a* 
Varjx} = - 
Beta density: 
a+ 1 
a+(k2 
E { x }  = ~ 
(B.14) 
(B.15) 
(B.16) 
(B.17) 
(B.18) 

514 
Introduction to Statistical Pattern Recognition 
(a+ 1 )(a+2) 
= (a+p+2)(a+p+3) 
Gamma function: 
T(x+l) = X !  for x = 0,1,2, . . 
1 
1.3.5.. . . . ( h - l )  
2 
2" 
r(x + -1 = 
7~ forx = 1,2,3,. . . 
r(X+s) 
U X )  
for a large x and a small 6 
Moments of the Parzen and KNN Density Estimates 
Parzen: 
I 
2 
E{p(X)] gp ( X ) [  1 + -a(X)r2] 
(2nd order approx.) 
1 
N 
Var{p(X)] Z--MY (X) 
(1st order approx.) 
(B. 19) 
(B.20) 
(B.21) 
(B.22) 
(B.23) 
(B.24) 
(B.25) 
(B.26) 
(B.27) 
(B.28) 
(B.29) 
(B.30) 

Appendices 
575 
(B.31) 
(B.32) 
(B.34) 
E{r;(X)] :p ( X ) [ l + p ( X ) ( c p  
1 
(x))-2'f' [ $1 
2"] 
(2nd order approx.) 
(B.35) 
L' = co IA I 'I2 
[co from (B.2)] 
(B.36) 
Var{p(X)] z- p 2 ( x )  
(1st order approx.) 
(B.37) 
k 

Appendix C 
NORMAL ERROR TABLE 
Each figure in the body of the table is preceded by a decimal point. 
576 

Appendices 
577 
- 
\ '0 
- 
0.0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
I .o 
1 . 1  
I .2 
I .3 
I .4 
I .5 
I .6 
I .7 
I .8 
I .9 
2.0 
2. I 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
3 .O 
3.5 
4.0 
4.5 
5.0 - 
0.00 
00000 
03983 
07926 
I1791 
15554 
19146 
22575 
25x04 
31594 
34 I 34 
36433 
38493 
40320 
41924 
43319 
44520 
45543 
46407 
47 I28 
47725 
48214 
48610 
48928 
49 180 
49379 
49534 
49653 
49744 
49813 
49865 
4997674 
4999683 
4999966 
4999997 I33 
28814 
0.0 I 
00399 
04380 
12172 
15910 
I 9497 
22907 
261 15 
29 103 
3 1859 
34375 
36650 
38686 
40490 
42073 
43448 
44630 
45637 
46485 
47193 
47778 
48257 
48645 
48956 
49202 
49396 
49547 
49664 
49752 
498 19 
- 
on3 I 7 
- 
0.02 
00798 
04776 
08706 
12552 
I6276 
19847 
23237 
26424 
29389 
32121 
346 14 
36864 
38877 
40658 
42220 
43574 
44738 
45728 
46562 
47257 
4783 I 
48300 
48679 
48983 
49224 
494 13 
49560 
49674 
49760 
49825 
- 
0.03 
01 197 
OS I72 
09095 
I2930 
16640 
20194 
23565 
26730 
29673 
3238 I 
34850 
37076 
39065 
40824 
42364 
43699 
44845 
458 18 
46638 
47320 
47882 
4834 I 
487 I3 
49010 
49245 
49430 
49573 
49683 
49767 
4983 I 
- 
0.04 
0 I595 
05567 
09483 
I3307 
I7003 
20450 
2389 I 
27035 
29955 
32639 
35083 
37286 
3925 I 
40988 
42507 
43822 
44950 
45907 
467 12 
4738 I 
47932 
48382 
48745 
49036 
49266 
49446 
49585 
49693 
49774 
49836 
- 
0.05 
0 I994 
05962 
0987 I 
I3683 
I7364 
20884 
23215 
27337 
30234 
32894 
35313 
37493 
39435 
41 149 
42647 
43943 
45053 
45994 
46784 
4744 1 
47982 
48422 
48778 
4906 I 
49286 
4946 I 
49598 
49702 
4978 I 
4984 1 
- - 
0.06 
02392 
06356 
10257 
14058 
I7724 
2 I226 
24537 
27637 
3051 I 
33 I47 
35543 
37698 
396 I7 
4 I308 
42786 
44062 
45154 
46080 
46856 
47500 
48030 
4846 I 
48809 
49086 
49305 
49477 
49609 
4971 I 
49788 
49846 
- - 
0.07 
02790 
06749 
10642 
14431 
18082 
21566 
24857 
27935 
30785 
33398 
35769 
37900 
39796 
4 I466 
42922 
41 179 
45254 
46164 
46926 
47558 
48077 
48500 
48840 
491 11 
49324 
49492 
4962 I 
49720 
49795 
4985 I 
- 
0.08 
03 188 
07142 
I 1026 
14803 
18439 
21904 
25 I75 
28230 
31057 
33646 
35993 
38100 
39973 
41621 
43056 
44295 
45352 
46246 
46995 
476 I5 
48 I24 
48537 
48870 
49134 
49343 
49506 
49632 
49728 
4980 I 
49856 
- 

Appendix D 
GAMMA FUNCTION TABLE 
4 
.9784 
.9364 
.9085 
.8922 
,8858 
,8882 
,8986 
,9168 
,9426 
,9761 
,9182 
,9156 
.8975 
,8960 
,8873 
,8868 
.8862 
,8866 
1.6 
.8935 
,8947 
,9086 
,9106 
1.8 
,9314 
,9341 
1.9 
,9618 
.9652 
5 
6 
7 
.9735 
,9687 
.9642 
,9330 
,9298 
.9267 
.9064 
.9044 
,9025 
,8912 
,8902 
,8893 
,8857 
,8856 
3856 
,8889 
,8896 
.8905 
.9001 
,9017 
.9033 
,9191 
,9214 
.9238 
.9456 
,9487 
,9518 
,9799 
.9837 
.9877 
2 
,9888 
,9436 
,9131 
,8946 
,8864 
,8870 
,8959 
.9 I26 
,9368 
,9688 
3 
,9835 
,9399 
,9108 
,8934 
.8860 
,8876 
,8972 
.9147 
.9397 
,9724 
8 
9 
.9597 
,9555 
.9237 
.9209 
.9007 
,8990 
.8885 
,8879 
,8857 
.8859 
,8914 
,8924 
.9050 
,9068 
,9262 
,9288 
,9551 
,9584 
,9917 
,9958 
In order to compute the gamma function outside the above range, apply 
T(s+l) = xT(x) for x > 0 recursively. For examples 
r(3.38) = 2.381-(2.38) = 2.38( i.38r(1.38)) = 2.918 
578 

INDEX 
A 
error, 52 
Absolute correction rule, 
see Correction rule 
Autocorrelation 
function, 418 
matrix, see Matrix 
mixture, see Scatter matrix, 
mixture 
B 
Bahadur expansion, see Expansion 
Basis 
complete set of, 417 
function, 287, 385, 417 
vector, 401 
classifier, see Classifier 
conditional 
cost. 57 
Bayes 
decision rule, see Decision rule 
error, see Error 
estimate, see Successive Bayes 
estimation 
linear classifier, see Linear 
classifier 
theorem, 12, 52 
Beta distribution, 
see Distribution 
Bhattacharyya 
bound, 99 
distance, see Distance 
estimate, see Estimate 
density function of, 290 
discriminant function for, 13 1 
orthonormality of, 174, 291 
see Distribution 
Binary input, 173, 290 
Binomial distribution, 
579 

580 
Introduction to Statistical Pattern Recognition 
Bisector, 128,444,517 
Block toeplitz matrix, 
see Matrix 
Bootstrap, 238 
bias, 243 
error, 240 
method, 238 
samples, 239 
variance, 246 
Branch and bound 
clustering, 523 
feature subset selection, 
basic algorithm, 494 
improved algorithm, 496 
for nearest neighbor, 361 
49 1 
Burdick’s chart, 63 
C 
Central limit theorem, 17 
Characteristic 
equation, 26 
function, 16,88 
of likelihood ratio, 88 
of normal distribution, 
16,91 
bound, 98 
distance, see Distance 
distribution, see 
goodness-of-fit test, 83 
Chernoff 
Chi-square 
Density function, gamma 
Circular error, see Error 
Class probability, see 
Probability 
Class separability, see 
Separability 
Classification, 5 10 
supervised, 51 
unsupervised, 508 
Classifier, see also 
Decision rule 
correlation, 125 
design, 7 
distance, 127 
linear, see Linear 
classifier 
piecewise, see Piecewise 
classifier 
quadratic, see Quadratic 
classifier 
Clustering, 508 
algorithm, 5 1 1 
criterion, 510 
graph theoretic approach, 539 
nearest local-mean 
reclassification rule, see 
Nearest local-mean 
reclassification rule 
nearest mean reclassification 
rule, see Nearest mean 
reclassification rule 
nonparametric approach, 533 
normal decomposition, see 
Normal decomposition 
parametric aproach, 510 
valley-seeking, 534,542 
Colored noise, 128 
Column correlation coefficient, 
164 

Index 
58 1 
Condensed nearest neighbor, 
D 
see kNN 
Configuration, 5 10 
Confusion matrix, 5 18 
Conjugate pair, see Density 
function 
Convergence 
acceleration of, 388 
for linearly separable case, 
in mean-square sense, 381 
of nearest mean reclassification 
rule, see Nearest mean 
reclassification rule 
with probability 1, 381 
of stochastic approximation, 
153,371 
378 
Correction rule 
absolute, 369 
fixed increment, 369 
gradient, 369 
Correlation, 125 
classifier, see Classifier 
coefficient, 15 
matrix, see Matrix 
conditional, 57 
of decision, 57 
symmetrical, 58 
Covariance, 14 
function, 4 18 
matrix, see Matrix 
cost 
Coverage, 255, 269 
Data 
compression, 409 
display 
nonparametric, 353 
parametric, 154 
risk contour, 355 
filter, 537 
reduction 
nonparametric, 549 
parametric, 556 
Decision rule 
Bayes 
for minimum error, 51 
for minimum risk, 57 
likelihood ratio, 52 
minimax, 61 
Neyman-Pearson, 59 
Density function, 12 
a posteriori, 390 
a priori, 390 
of binary inputs, see 
Binary input 
class, 12 
conditional, 12 
conjugate pair, 392 
of coverage, 269 
estimate of 
k nearest neighbor approach, 
Parzen approach, see Parzen 
see kNN 
expansion of, 287 
exponential, 56, 70 
gamma, 23,69,573 
gradient of, 534 

582 
Introduction to Statistical Pattern Recognition 
of likelihood ratio, 54 
marginal, 13 
mixture, 12 
reproducing pair, 392 
Design sample, see Sample 
Desired output, 145, 147 
Diagonal matrix, see Matrix 
Diagonalization, 27 
simultaneous, 3 1 
Dimensionality, 426 
intrinsic, 280, 537 
local, see Dimensionality, 
vector, 149 
intrinsic 
Directed 
path, 539 
tree, 539 
Discriminant analysis, 445 
nonparametric, 466 
Discriminant function, 52 
for binary inputs, see 
desired output of, see 
linear, see Linear classifier 
piecewise, see Piecewise 
quadratic, see Quadratic 
Binary input 
Desired output 
classifier 
classifier 
Dispersion, 13 
Distance 
between-sample, 41 1 
Bhattacharyya, 99, 188 
Chernoff, 98 
distribution of, see 
for feature extraction, 455 
Distribution 
normalized, 16 
Distance classifier, see 
Classifier 
Distribution 
beta, 75, 270, 573 
binomial, 200 
of distance, 68 
function, 11 
Gaussian-Wishart, 393 
normal, 16, 573 
characteristic function of, 
conditional density, 48 
entropy, 412 
generation of, 30 
likelihood ratio for, 54 
marginal density, 48 
probability of error for, 
test of, see Normality 
16 
91 
test 
Wishart, 392 
Distributional test, 476 
Divergence, 458 
Double exponential waveform, 
284,472 
E 
Edited k nearest neighbor, 
Effective dimensionality, 
Eigenfunction, 418 
Eigenvalues, 26 
see kNN 
see Dimensionality, intrinsic 

Index 
583 
of autocorrelation function, 41 8 
estimation of, 43 I 
matrix, see Matrix 
normalization of, 410 
perturbation of, 426 
estimation of, 43 1 
matrix, see Matrix 
perturbation of, 426 
for binary inputs, 416 
maximization, 4 13 
minimax, 415 
minimization, 416, 550 
for normal distributions, 
Eigenvectors, 27 
Entropy, 412, 550 
see Distribution, normal 
Error 
Bayes, 53 
circular, 287 
conditional, 52 
control, 351 
counting, 197, 200 
estimate, see Estimate 
expression, 87, 197 
function (normal), 63, 576 
of linear classifier, 85 
lower bound of, 220, 307 
mean-square, 145,402 
pairwise, 284 
probability of, 52, 85, 87, 197 
of quadratic classifier, 91 
reject curve, 79 
upper bound of, 97, 220, 307 
Bayes (successive), see 
Estimate 
Successive Bayes estimation 
of Bhattacharyya distance, 188 
bias, 189, 190 
variance, 189, 190 
biased, 21, 183, 187, 259, 
272, 313, 326, 347 
consistent, 19, 261, 273 
of density function, see 
of density gradient, 534 
of error, 196, 301, 303, 344 
grouped error, 356 
k nearest neighbor density, 
see kNN 
maximum likelihood, see 
Normal decomposition 
moment, see Moment 
Parzen density, see Parzen 
sample, see Sample 
unbiased, 18 
variance, 183, 187 
Bahadur, 292 
by basis functions, see Basis 
Karhunen-LoCve, 403,4 17 
kernel of, 287 
square error of, 288 
Walsh, 292 
for random process, 4 18 
value, 13 
vector, 13 
Density function 
Expansion 
Expected 

584 
Introduction to Statistical Pattern Recognition 
F 
Factor analysis, 417 
Feature 
extraction 
for classification, 442 
general critrion for, 460 
sequential, 480 
for signal representation, 400 
ideal, 444 
selection, see Feature 
space, 402 
subset selection, 489 
extraction 
backward selection, 490 
branch and bound, see 
Branch and bound 
forward selection, 49 1 
stepwise search technique, 
490 
vector, 402 
classifier, see Linear 
classifier 
criterion, 134 
Fixed increment rule, see 
Correction rule 
Fourier transform 
of likelihood ratio, 159 
orthonormality of, 156 
quadratic classifier of, 159 
for stational process, 421 
Fisher 
G 
Gamma 
density, see Density 
function, 23, 574, 578 
Gaussian pulse, 282, 472 
Gaussian-Wishart distribution, 
see Distribution 
Goodness-of-fit, see 
Chi-square 
Gradient 
of density function, see 
estimate of, see Estimate 
Gradient correction rule, see 
Correction rule 
Graph theoretic clustering, see 
Clustering 
Grouped error estimate, see 
Estimate 
function 
Density function 
H 
Harmonic sequence, see Sequence 
Hermite polynomial, 288 
Holdout method, 220, 310 
Hughes phenomena, 208 
Hyperellipsoid 
surface area, 314, 573 
volume, 260, 572 
Hypothesis test 
composite, 83 
multi. , 66 
sequential, see Sequential 
simple, 51 
(hypothesis) test 

Index 
585 
single, 67 
I 
Intrinsic dimensionality, see 
Inverse matrix, see Matrix 
Dimensionality 
K 
k nearest neighbor 
(NN) - volumetric, 305 
classification, 303 
likelihood ratio for, 303 
density estimation, 268, 575 
bias, 272 
consistent, 273 
metric, 275 
moments, 270 
optimal k, 273 
approximation of, 270 
minimum ZMSE, 275 
minimum MSE, 274 
unbias, 273 
variance, 273 
effect of parameters, 278 
bias, 347 
L estimate of a covariance, 
35 1 
leave-one-out method, 303 
metric, 303 
resubstitution method, 303 
progression, 552 
distance to kNN, 277 
error estimation 
k nearest neighbor (NN) 
approach - voting, 305 
asymptotic conditional risk 
and error, 307 
kNN, 306 
multiclass, 309 
NN, 305 
2NN, 306 
branch and bound, see 
Branch and bound 
condensed NN, 360 
edited kNN, 358 
finite sample analysis, 313 
bias 
multiclass, 322 
NN, 313 
2NN, 321 
see Expansion 
Karhunen-LoCve expansion, 
Kiefer-Wolfowitz method, 380 
Kolmogorov-Smirnov test, 76, 
83 
1 
Lagrange multiplier, 26, 59 
Large number of classes, 284 
Layered machine, 17 1 
Learning, 368 
machine, 5 
without teacher, 394 
Leave-one-out method, 220 
for k nearest neighbor 
approach, see kNN 
for Parzen approach, see Parzen 
characteristic function of, 
Likelihood ratio, 52 
see Characteristic function 

586 
Introduction to Statistical Pattern Recognition 
density function of, see 
Density function 
Fourier transform of, see 
Fourier transform 
for k nearest neighbor approach, 
see kNN 
minus-log, 52 
for normal distribution, see 
Distribution, normal 
for Parzen approach, see Parzen 
test (decision rule), see 
Decision rule 
threshold of, 52 
Bayes, 55, 57, 129 
effect of design samples, 208 
error of, see Error 
Fisher, 135 
iterative design, 150 
for minimum error, 136 
for minimum mean-square error, 
for multiclass, 373 
by nonparametric scatter matrix, 
successive adjustment, 367 
convergence for, see 
Linear classifier 
145, 147 
473 
Linearly separable, 153, 371 
Convergence 
dimensionality, see 
mean, 535, 542 
Transformation 
Local 
Dimensionality, intrinsic 
Log transformation, see 
M 
Mapped space, see Feature, 
space 
Mapping 
linear, 399, 448, 465, 470 
nonlinear, 463, 480 
Matched filter, 126 
Matrix 
autocorrelation, 15 
block toeplitz, 162 
correlation, 15 
covariance, 13 
derivatives, 564 
sample, see Sample 
sample, see Sample 
of determinant, 567 
of distance, 568 
of inverse, 564 
of trace, 565 
determinant, 38 
diagonal, 27 
eigenvalue, 27 
eigenvector, 27 
inversion of, 41 
generalized, 44 
pseudo-, 43 
near-singular, 40 
positive definite, 35 
rank, 38 
sample, 39, 149, 174, 556 
singular, 38 
toeplitz, 160 
trace, 36 
Mean, see Expected, value 
or vector 

Index 
5 87 
sample, see Sample 
Merging, 5 13 
Metric, 264, 275, 3 13 
global, 3 13 
local, 3 13 
for feature extraction, 
test, see Decision rule 
Minimum point finding problem, 
see Stochastic approximation 
Minus-log-likelihood ratio, 
see Likelihood ratio 
Mixture 
autocorrelation matrix, 
see Scatter matrix, mixture 
density function, see Density 
function 
normalization, 516, 519 
scatter matrix of, see 
Scatter matrix 
Model validity test, 82 
Moment, 18 
central, 20 
estimate, 18 
sample, 18 
Minimax 
see Feature, extraction 
Monotonicity, 492, 526 
Multiclass, 66, 169, 373 
Multicluster, 169 
Multihypotheses test, see 
Hypothesis test 
Multiple dichotomy, 5 13 
Multi-sensor fusion, 114 
N 
Nearest local-mean reclassification 
Nearest mean reclassification rule, 
rule, 542 
517 
convergence of, 5 18 
see kNN 
Nearest neighbor decision rule, 
Newton method, 376 
Neyman-Pearson test, see 
Decision rule 
Nonparametric 
clustering, see Clustering 
data reduction, see Data, 
density estimation 
see kNN 
reduction 
k nearest neighbor approach, 
Parzen approach, see Parzen 
discriminant analysis, see 
Discriminant analysis 
scatter matrix, see Scatter 
matrix 
Normal decomposition, 526 
maximum likelihood estimation, 
method of moments, 527 
piecewise quadratic boundary, 
527 
526 
Normal distribution, see 
Distribution, normal 
Normality test, 75, 537 
Normalization 
of eigenvalues, see 
Eigenvalues 

588 
Introduction to Statistical Pattern Recognition 
mixture, see Mixture 
Notation, 9 
0 
Operating characteristics, 63 
Orthogonal, 27,287 
Orthonormal, 27,386,401 
for binary inputs, see 
Binary input 
of Fourier transform, see 
Fourier transform 
transformation, see 
Transformation 
Outlier, 235 
P 
Pairwise error, see Error 
Parametric 
clustering, see Clustering 
data reduction, see Data, 
reduction 
estimation, 184 
classification, 301 
likelihood ratio, 301 
reduced, 553 
bias, 259 
consistent, 261 
convolution expression, 257 
kernel, 255 
Parzen 
density estimation, 255, 574 
metric, 264 
size, 261 
minimum ZMSE, 265 
minimum IMSE, 264 
minimum MSE, 263 
approximation of, 258 
for a normal kernel, 259 
for a uniform kernel, 260 
moments, 257 
unbias, 261 
variance, 259 
error estimation 
direct estimation of the 
kernel 
Bayes error, 344 
L estimate of the kernel 
shape, 336,342 
size, 322 
covariance, 339 
leave-one-out method, 301 
lower bound of the Bayes error, 
resubstitution method, 301 
sample size, 327 
threshold, 328 
upper bound of the Bayes 
error, 301 
Perceptron, 368 
Perturbation 
30 1 
of eigenvalues, see Eigenvalues 
of eigenvectors, see 
of a quadratic classifier, see 
Eigenvectors 
Quadratic classifier 
Piecewise classifier 
linear, 170 
quadratic, 169 
successive adjustment, 373 
Positive definiteness, see Matrix 
Potential function, 387 

Index 
5 89 
Power spectrum, 421 
Power transformation, see 
Transformation 
Principal component, 28 
analysis, 41 7 
Probability 
a posteriori, 12 
a priori, 12 
class, 12 
coverage, see Coverage 
of error, see Error 
reject, see Reject 
random, 4 17 
stationary, see Stationary 
process 
whitening, 28 
Process 
Q 
Quadratic classifier 
Bayes, 54 
bootstrap error for, 242 
design of, 153 
error of, see Error 
error of the resubstitution 
method, 231 
orthogonal subspace to, 480 
perturbation of, 225 
sequential selection, 480 
Quadratic form (function), 16, 
54, 125, 154 
recursive computation of, 498 
R 
Radar Data, 47 
Random process, see Process 
Random variable, 11 
Random vector, see Vector 
Rank of determinant, see Matrix 
Ranking procedure, 73 
Reduced Parzen classifier, 
Reduced training sequence, 
Regression function, 376 
Reject, 78 
see Parzen 
see Sequence 
probability, 78 
region, 78, 171 
threshold, 78 
Representative selection, 549 
Reproducing pair, see 
Density function 
Resubstitution method, 220 
error for a quadratic classifier, 
for k nearest neighbor approach, 
for Parzen approach, see Parzen 
23 1 
see kNN 
Robbins-Monro method, 376 
Root-finding problem, see 
Stochastic approximation 
Row correlation coefficient. 164 
S 
Sample 
autocorrelation matrix, 19 
covariance matrix, 21 
design 

590 
Introduction to Statistical Pattern Recognition 
bias due to, 203, 216 
effect of, 201 
variance due to, 213, 218 
estimate, 17 
generation, 30 
matrix, see Matrix 
mean vector, 19 
moment, see Moment 
test 
bias due to, 199, 216 
effect of, 197 
variance due to, 200, 218 
Scatter matrix 
between-class, 446 
generalized, 463 
nonparametric, 467 
of mixture, 446 
within-class, 446 
Scatter measure, 41 1 
Schwarz inequality, 309 
Separability criterion, 446 
Sequence 
nonparametric, 477, 542 
flatter, 389 
harmonic, 375 
reduced training, 37 1 
Wald, 114 
Hypothesis test 
Hypothesis test 
557 
Sequential (hypothesis) test, 110 
Simple hypothesis test, see 
Single hypothesis test, see 
Singular value decomposition, 
Skeleton hypersurface, 537 
Small sample size problem, 39 
Solution tree, 492, 523 
Spherical coordinate, 484 
Splitting, 5 I3 
Standard Data, 45 
Standard deviation, 15 
Stationary process, 55, 156,420 
autocorrelation function, 157, 
mean, 157,420 
convergence, see Convergence 
minimum point finding problem, 
multidimensional extension, 382 
root-finding problem, 376 
of density function, see 
Successive Bayes estimation 
linear classifier, see 
Linear classifier 
piecewise classifier, see 
Piecewise classifier 
potential function, 385 
Successive Bayes estimation, 389 
of covariance matrix, 392, 393 
of expected vector, 390, 393 
supervised estimation, 390 
unsupervised estimation, 394 
Surface area, see Hyperellipsoid 
420 
Stochastic approximation, 375 
3 80 
Successive adjustment 
T 
Taylor series, 182, 258, 270, 313 
Test sample, see Sample 
Toeplitz matrix, see Matrix 
Trace, see Matrix 

Index 
59 1 
Transformation 
linear, 24, 401,448,465, 470 
log, 108 
orthonormal, 28, 35,401,417 
power, 76, 104 
variable, 47 
whitening, 28, 128 
Truth table, 290 
U 
Unbiased 
asymptotic 
k nearest neighbor density 
Parzen density estimate, 
estimate, see kNN 
see Parzen 
estimate, see Estimate 
classification, see 
Classification 
estimation, see Successive 
Bayes estimation 
Unsupervised 
Valley-seeking technique, 
see Clustering 
Variable transformation, 
see Transformation 
Variance, 14 
Vector 
basis, see Basis 
conditional expected, 13 
desired output, see 
Desired output 
expected, see Expected 
feature, see Feature 
penalty, 150 
random, 11 
Volume, see Hyperellipsoid 
Volumetric k nearest neighbor, 
Voting k nearest neighbor, 
see kNN 
see kNN 
W 
Wald sequential test, see 
Sequential (hypothesis) test 
Walsh function, see Expansion 
Weighting function, 469 
White noise, 125 
Whitening 
filter, 128 
process, see Process 
transformation, see 
Transformation 
Wishart distribution, see 
Distribution 



ISBN 0-12-269851-7 



About the author: 
Keinosuke Fukunaga received a B.S. degree 
in electrical engineering from Kyoto Univer- 
sity, Kyoto, Japan, in 1953. In 1959 he earned 
an M.S.E.E. degree from the University of 
Pennsylvania in Philadelphia and then 
returned to Kyoto University to earn his 
Ph.D. in 1962. 
From 1953 to 1966 Dr. Fukunaga was 
employed by the Mitsubishi Electric Com- 
pany in Japan. His first pition was in the 
Central Research Laboratories where he 
worked on computer applications in 
control systems. Later, he became the 
Hardware Development Manager in the 
Computer Division. He is currently a pro- 
fessor of Electrical Engineering at Purdue 
University, where he has been on the fac- 
Dr. Fukunaga acted as an Associate Editor 
in pattern recognition for ZEEE Punsuc- 
tions on InfintzQtion Theory from 1977 to 
1980. He is a member ofEta Kappa Nu. 
ulty since 1966. 

Introduction to 
istical 
paner-n 
Recognition 
f b m  mviews oftheJirst editicm 
Contains an excellent review of the literature on 
idormation theory, automata, and pattern recognition.’’ 
-C)aoiae 
“Many topics are covered with dispatch. Also the book shm 
hints of the author’s own carefid and valuable research.’’ 
-IEEE Pansactions o 
n Theory 
I S B N  0-32-269853-7 
9 0 0 5 1  
9 780122 698514 

